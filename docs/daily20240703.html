<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240702.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SMERF: Streamable Memory Efficient Radiance Fields for Real-Time\n  Large-Scene Exploration", "author": "Daniel Duckworth and Peter Hedman and Christian Reiser and Peter Zhizhin and Jean-Fran\u00e7ois Thibert and Mario Lu\u010di\u0107 and Richard Szeliski and Jonathan T. Barron", "abstract": "  Recent techniques for real-time view synthesis have rapidly advanced in\nfidelity and speed, and modern methods are capable of rendering\nnear-photorealistic scenes at interactive frame rates. At the same time, a\ntension has arisen between explicit scene representations amenable to\nrasterization and neural fields built on ray marching, with state-of-the-art\ninstances of the latter surpassing the former in quality while being\nprohibitively expensive for real-time applications. In this work, we introduce\nSMERF, a view synthesis approach that achieves state-of-the-art accuracy among\nreal-time methods on large scenes with footprints up to 300 m$^2$ at a\nvolumetric resolution of 3.5 mm$^3$. Our method is built upon two primary\ncontributions: a hierarchical model partitioning scheme, which increases model\ncapacity while constraining compute and memory consumption, and a distillation\ntraining strategy that simultaneously yields high fidelity and internal\nconsistency. Our approach enables full six degrees of freedom (6DOF) navigation\nwithin a web browser and renders in real-time on commodity smartphones and\nlaptops. Extensive experiments show that our method exceeds the current\nstate-of-the-art in real-time novel view synthesis by 0.78 dB on standard\nbenchmarks and 1.78 dB on large scenes, renders frames three orders of\nmagnitude faster than state-of-the-art radiance field models, and achieves\nreal-time performance across a wide variety of commodity devices, including\nsmartphones. We encourage readers to explore these models interactively at our\nproject website: https://smerf-3d.github.io.\n", "link": "http://arxiv.org/abs/2312.07541v3", "date": "2024-07-02", "relevancy": 3.0692, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6415}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMERF%3A%20Streamable%20Memory%20Efficient%20Radiance%20Fields%20for%20Real-Time%0A%20%20Large-Scene%20Exploration&body=Title%3A%20SMERF%3A%20Streamable%20Memory%20Efficient%20Radiance%20Fields%20for%20Real-Time%0A%20%20Large-Scene%20Exploration%0AAuthor%3A%20Daniel%20Duckworth%20and%20Peter%20Hedman%20and%20Christian%20Reiser%20and%20Peter%20Zhizhin%20and%20Jean-Fran%C3%A7ois%20Thibert%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Richard%20Szeliski%20and%20Jonathan%20T.%20Barron%0AAbstract%3A%20%20%20Recent%20techniques%20for%20real-time%20view%20synthesis%20have%20rapidly%20advanced%20in%0Afidelity%20and%20speed%2C%20and%20modern%20methods%20are%20capable%20of%20rendering%0Anear-photorealistic%20scenes%20at%20interactive%20frame%20rates.%20At%20the%20same%20time%2C%20a%0Atension%20has%20arisen%20between%20explicit%20scene%20representations%20amenable%20to%0Arasterization%20and%20neural%20fields%20built%20on%20ray%20marching%2C%20with%20state-of-the-art%0Ainstances%20of%20the%20latter%20surpassing%20the%20former%20in%20quality%20while%20being%0Aprohibitively%20expensive%20for%20real-time%20applications.%20In%20this%20work%2C%20we%20introduce%0ASMERF%2C%20a%20view%20synthesis%20approach%20that%20achieves%20state-of-the-art%20accuracy%20among%0Areal-time%20methods%20on%20large%20scenes%20with%20footprints%20up%20to%20300%20m%24%5E2%24%20at%20a%0Avolumetric%20resolution%20of%203.5%20mm%24%5E3%24.%20Our%20method%20is%20built%20upon%20two%20primary%0Acontributions%3A%20a%20hierarchical%20model%20partitioning%20scheme%2C%20which%20increases%20model%0Acapacity%20while%20constraining%20compute%20and%20memory%20consumption%2C%20and%20a%20distillation%0Atraining%20strategy%20that%20simultaneously%20yields%20high%20fidelity%20and%20internal%0Aconsistency.%20Our%20approach%20enables%20full%20six%20degrees%20of%20freedom%20%286DOF%29%20navigation%0Awithin%20a%20web%20browser%20and%20renders%20in%20real-time%20on%20commodity%20smartphones%20and%0Alaptops.%20Extensive%20experiments%20show%20that%20our%20method%20exceeds%20the%20current%0Astate-of-the-art%20in%20real-time%20novel%20view%20synthesis%20by%200.78%20dB%20on%20standard%0Abenchmarks%20and%201.78%20dB%20on%20large%20scenes%2C%20renders%20frames%20three%20orders%20of%0Amagnitude%20faster%20than%20state-of-the-art%20radiance%20field%20models%2C%20and%20achieves%0Areal-time%20performance%20across%20a%20wide%20variety%20of%20commodity%20devices%2C%20including%0Asmartphones.%20We%20encourage%20readers%20to%20explore%20these%20models%20interactively%20at%20our%0Aproject%20website%3A%20https%3A//smerf-3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07541v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMERF%253A%2520Streamable%2520Memory%2520Efficient%2520Radiance%2520Fields%2520for%2520Real-Time%250A%2520%2520Large-Scene%2520Exploration%26entry.906535625%3DDaniel%2520Duckworth%2520and%2520Peter%2520Hedman%2520and%2520Christian%2520Reiser%2520and%2520Peter%2520Zhizhin%2520and%2520Jean-Fran%25C3%25A7ois%2520Thibert%2520and%2520Mario%2520Lu%25C4%258Di%25C4%2587%2520and%2520Richard%2520Szeliski%2520and%2520Jonathan%2520T.%2520Barron%26entry.1292438233%3D%2520%2520Recent%2520techniques%2520for%2520real-time%2520view%2520synthesis%2520have%2520rapidly%2520advanced%2520in%250Afidelity%2520and%2520speed%252C%2520and%2520modern%2520methods%2520are%2520capable%2520of%2520rendering%250Anear-photorealistic%2520scenes%2520at%2520interactive%2520frame%2520rates.%2520At%2520the%2520same%2520time%252C%2520a%250Atension%2520has%2520arisen%2520between%2520explicit%2520scene%2520representations%2520amenable%2520to%250Arasterization%2520and%2520neural%2520fields%2520built%2520on%2520ray%2520marching%252C%2520with%2520state-of-the-art%250Ainstances%2520of%2520the%2520latter%2520surpassing%2520the%2520former%2520in%2520quality%2520while%2520being%250Aprohibitively%2520expensive%2520for%2520real-time%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%250ASMERF%252C%2520a%2520view%2520synthesis%2520approach%2520that%2520achieves%2520state-of-the-art%2520accuracy%2520among%250Areal-time%2520methods%2520on%2520large%2520scenes%2520with%2520footprints%2520up%2520to%2520300%2520m%2524%255E2%2524%2520at%2520a%250Avolumetric%2520resolution%2520of%25203.5%2520mm%2524%255E3%2524.%2520Our%2520method%2520is%2520built%2520upon%2520two%2520primary%250Acontributions%253A%2520a%2520hierarchical%2520model%2520partitioning%2520scheme%252C%2520which%2520increases%2520model%250Acapacity%2520while%2520constraining%2520compute%2520and%2520memory%2520consumption%252C%2520and%2520a%2520distillation%250Atraining%2520strategy%2520that%2520simultaneously%2520yields%2520high%2520fidelity%2520and%2520internal%250Aconsistency.%2520Our%2520approach%2520enables%2520full%2520six%2520degrees%2520of%2520freedom%2520%25286DOF%2529%2520navigation%250Awithin%2520a%2520web%2520browser%2520and%2520renders%2520in%2520real-time%2520on%2520commodity%2520smartphones%2520and%250Alaptops.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520exceeds%2520the%2520current%250Astate-of-the-art%2520in%2520real-time%2520novel%2520view%2520synthesis%2520by%25200.78%2520dB%2520on%2520standard%250Abenchmarks%2520and%25201.78%2520dB%2520on%2520large%2520scenes%252C%2520renders%2520frames%2520three%2520orders%2520of%250Amagnitude%2520faster%2520than%2520state-of-the-art%2520radiance%2520field%2520models%252C%2520and%2520achieves%250Areal-time%2520performance%2520across%2520a%2520wide%2520variety%2520of%2520commodity%2520devices%252C%2520including%250Asmartphones.%2520We%2520encourage%2520readers%2520to%2520explore%2520these%2520models%2520interactively%2520at%2520our%250Aproject%2520website%253A%2520https%253A//smerf-3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07541v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMERF%3A%20Streamable%20Memory%20Efficient%20Radiance%20Fields%20for%20Real-Time%0A%20%20Large-Scene%20Exploration&entry.906535625=Daniel%20Duckworth%20and%20Peter%20Hedman%20and%20Christian%20Reiser%20and%20Peter%20Zhizhin%20and%20Jean-Fran%C3%A7ois%20Thibert%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Richard%20Szeliski%20and%20Jonathan%20T.%20Barron&entry.1292438233=%20%20Recent%20techniques%20for%20real-time%20view%20synthesis%20have%20rapidly%20advanced%20in%0Afidelity%20and%20speed%2C%20and%20modern%20methods%20are%20capable%20of%20rendering%0Anear-photorealistic%20scenes%20at%20interactive%20frame%20rates.%20At%20the%20same%20time%2C%20a%0Atension%20has%20arisen%20between%20explicit%20scene%20representations%20amenable%20to%0Arasterization%20and%20neural%20fields%20built%20on%20ray%20marching%2C%20with%20state-of-the-art%0Ainstances%20of%20the%20latter%20surpassing%20the%20former%20in%20quality%20while%20being%0Aprohibitively%20expensive%20for%20real-time%20applications.%20In%20this%20work%2C%20we%20introduce%0ASMERF%2C%20a%20view%20synthesis%20approach%20that%20achieves%20state-of-the-art%20accuracy%20among%0Areal-time%20methods%20on%20large%20scenes%20with%20footprints%20up%20to%20300%20m%24%5E2%24%20at%20a%0Avolumetric%20resolution%20of%203.5%20mm%24%5E3%24.%20Our%20method%20is%20built%20upon%20two%20primary%0Acontributions%3A%20a%20hierarchical%20model%20partitioning%20scheme%2C%20which%20increases%20model%0Acapacity%20while%20constraining%20compute%20and%20memory%20consumption%2C%20and%20a%20distillation%0Atraining%20strategy%20that%20simultaneously%20yields%20high%20fidelity%20and%20internal%0Aconsistency.%20Our%20approach%20enables%20full%20six%20degrees%20of%20freedom%20%286DOF%29%20navigation%0Awithin%20a%20web%20browser%20and%20renders%20in%20real-time%20on%20commodity%20smartphones%20and%0Alaptops.%20Extensive%20experiments%20show%20that%20our%20method%20exceeds%20the%20current%0Astate-of-the-art%20in%20real-time%20novel%20view%20synthesis%20by%200.78%20dB%20on%20standard%0Abenchmarks%20and%201.78%20dB%20on%20large%20scenes%2C%20renders%20frames%20three%20orders%20of%0Amagnitude%20faster%20than%20state-of-the-art%20radiance%20field%20models%2C%20and%20achieves%0Areal-time%20performance%20across%20a%20wide%20variety%20of%20commodity%20devices%2C%20including%0Asmartphones.%20We%20encourage%20readers%20to%20explore%20these%20models%20interactively%20at%20our%0Aproject%20website%3A%20https%3A//smerf-3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07541v3&entry.124074799=Read"},
{"title": "Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry,\n  Texture, and PBR Materials", "author": "Yawar Siddiqui and Tom Monnier and Filippos Kokkinos and Mahendra Kariya and Yanir Kleiman and Emilien Garreau and Oran Gafni and Natalia Neverova and Andrea Vedaldi and Roman Shapovalov and David Novotny", "abstract": "  We present Meta 3D AssetGen (AssetGen), a significant advancement in\ntext-to-3D generation which produces faithful, high-quality meshes with texture\nand material control. Compared to works that bake shading in the 3D object's\nappearance, AssetGen outputs physically-based rendering (PBR) materials,\nsupporting realistic relighting. AssetGen generates first several views of the\nobject with factored shaded and albedo appearance channels, and then\nreconstructs colours, metalness and roughness in 3D, using a deferred shading\nloss for efficient supervision. It also uses a sign-distance function to\nrepresent 3D shape more reliably and introduces a corresponding loss for direct\nshape supervision. This is implemented using fused kernels for high memory\nefficiency. After mesh extraction, a texture refinement transformer operating\nin UV space significantly improves sharpness and details. AssetGen achieves 17%\nimprovement in Chamfer Distance and 40% in LPIPS over the best concurrent work\nfor few-view reconstruction, and a human preference of 72% over the best\nindustry competitors of comparable speed, including those that support PBR.\nProject page with generated assets: https://assetgen.github.io\n", "link": "http://arxiv.org/abs/2407.02445v1", "date": "2024-07-02", "relevancy": 2.8244, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5679}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%203D%20AssetGen%3A%20Text-to-Mesh%20Generation%20with%20High-Quality%20Geometry%2C%0A%20%20Texture%2C%20and%20PBR%20Materials&body=Title%3A%20Meta%203D%20AssetGen%3A%20Text-to-Mesh%20Generation%20with%20High-Quality%20Geometry%2C%0A%20%20Texture%2C%20and%20PBR%20Materials%0AAuthor%3A%20Yawar%20Siddiqui%20and%20Tom%20Monnier%20and%20Filippos%20Kokkinos%20and%20Mahendra%20Kariya%20and%20Yanir%20Kleiman%20and%20Emilien%20Garreau%20and%20Oran%20Gafni%20and%20Natalia%20Neverova%20and%20Andrea%20Vedaldi%20and%20Roman%20Shapovalov%20and%20David%20Novotny%0AAbstract%3A%20%20%20We%20present%20Meta%203D%20AssetGen%20%28AssetGen%29%2C%20a%20significant%20advancement%20in%0Atext-to-3D%20generation%20which%20produces%20faithful%2C%20high-quality%20meshes%20with%20texture%0Aand%20material%20control.%20Compared%20to%20works%20that%20bake%20shading%20in%20the%203D%20object%27s%0Aappearance%2C%20AssetGen%20outputs%20physically-based%20rendering%20%28PBR%29%20materials%2C%0Asupporting%20realistic%20relighting.%20AssetGen%20generates%20first%20several%20views%20of%20the%0Aobject%20with%20factored%20shaded%20and%20albedo%20appearance%20channels%2C%20and%20then%0Areconstructs%20colours%2C%20metalness%20and%20roughness%20in%203D%2C%20using%20a%20deferred%20shading%0Aloss%20for%20efficient%20supervision.%20It%20also%20uses%20a%20sign-distance%20function%20to%0Arepresent%203D%20shape%20more%20reliably%20and%20introduces%20a%20corresponding%20loss%20for%20direct%0Ashape%20supervision.%20This%20is%20implemented%20using%20fused%20kernels%20for%20high%20memory%0Aefficiency.%20After%20mesh%20extraction%2C%20a%20texture%20refinement%20transformer%20operating%0Ain%20UV%20space%20significantly%20improves%20sharpness%20and%20details.%20AssetGen%20achieves%2017%25%0Aimprovement%20in%20Chamfer%20Distance%20and%2040%25%20in%20LPIPS%20over%20the%20best%20concurrent%20work%0Afor%20few-view%20reconstruction%2C%20and%20a%20human%20preference%20of%2072%25%20over%20the%20best%0Aindustry%20competitors%20of%20comparable%20speed%2C%20including%20those%20that%20support%20PBR.%0AProject%20page%20with%20generated%20assets%3A%20https%3A//assetgen.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%25203D%2520AssetGen%253A%2520Text-to-Mesh%2520Generation%2520with%2520High-Quality%2520Geometry%252C%250A%2520%2520Texture%252C%2520and%2520PBR%2520Materials%26entry.906535625%3DYawar%2520Siddiqui%2520and%2520Tom%2520Monnier%2520and%2520Filippos%2520Kokkinos%2520and%2520Mahendra%2520Kariya%2520and%2520Yanir%2520Kleiman%2520and%2520Emilien%2520Garreau%2520and%2520Oran%2520Gafni%2520and%2520Natalia%2520Neverova%2520and%2520Andrea%2520Vedaldi%2520and%2520Roman%2520Shapovalov%2520and%2520David%2520Novotny%26entry.1292438233%3D%2520%2520We%2520present%2520Meta%25203D%2520AssetGen%2520%2528AssetGen%2529%252C%2520a%2520significant%2520advancement%2520in%250Atext-to-3D%2520generation%2520which%2520produces%2520faithful%252C%2520high-quality%2520meshes%2520with%2520texture%250Aand%2520material%2520control.%2520Compared%2520to%2520works%2520that%2520bake%2520shading%2520in%2520the%25203D%2520object%2527s%250Aappearance%252C%2520AssetGen%2520outputs%2520physically-based%2520rendering%2520%2528PBR%2529%2520materials%252C%250Asupporting%2520realistic%2520relighting.%2520AssetGen%2520generates%2520first%2520several%2520views%2520of%2520the%250Aobject%2520with%2520factored%2520shaded%2520and%2520albedo%2520appearance%2520channels%252C%2520and%2520then%250Areconstructs%2520colours%252C%2520metalness%2520and%2520roughness%2520in%25203D%252C%2520using%2520a%2520deferred%2520shading%250Aloss%2520for%2520efficient%2520supervision.%2520It%2520also%2520uses%2520a%2520sign-distance%2520function%2520to%250Arepresent%25203D%2520shape%2520more%2520reliably%2520and%2520introduces%2520a%2520corresponding%2520loss%2520for%2520direct%250Ashape%2520supervision.%2520This%2520is%2520implemented%2520using%2520fused%2520kernels%2520for%2520high%2520memory%250Aefficiency.%2520After%2520mesh%2520extraction%252C%2520a%2520texture%2520refinement%2520transformer%2520operating%250Ain%2520UV%2520space%2520significantly%2520improves%2520sharpness%2520and%2520details.%2520AssetGen%2520achieves%252017%2525%250Aimprovement%2520in%2520Chamfer%2520Distance%2520and%252040%2525%2520in%2520LPIPS%2520over%2520the%2520best%2520concurrent%2520work%250Afor%2520few-view%2520reconstruction%252C%2520and%2520a%2520human%2520preference%2520of%252072%2525%2520over%2520the%2520best%250Aindustry%2520competitors%2520of%2520comparable%2520speed%252C%2520including%2520those%2520that%2520support%2520PBR.%250AProject%2520page%2520with%2520generated%2520assets%253A%2520https%253A//assetgen.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%203D%20AssetGen%3A%20Text-to-Mesh%20Generation%20with%20High-Quality%20Geometry%2C%0A%20%20Texture%2C%20and%20PBR%20Materials&entry.906535625=Yawar%20Siddiqui%20and%20Tom%20Monnier%20and%20Filippos%20Kokkinos%20and%20Mahendra%20Kariya%20and%20Yanir%20Kleiman%20and%20Emilien%20Garreau%20and%20Oran%20Gafni%20and%20Natalia%20Neverova%20and%20Andrea%20Vedaldi%20and%20Roman%20Shapovalov%20and%20David%20Novotny&entry.1292438233=%20%20We%20present%20Meta%203D%20AssetGen%20%28AssetGen%29%2C%20a%20significant%20advancement%20in%0Atext-to-3D%20generation%20which%20produces%20faithful%2C%20high-quality%20meshes%20with%20texture%0Aand%20material%20control.%20Compared%20to%20works%20that%20bake%20shading%20in%20the%203D%20object%27s%0Aappearance%2C%20AssetGen%20outputs%20physically-based%20rendering%20%28PBR%29%20materials%2C%0Asupporting%20realistic%20relighting.%20AssetGen%20generates%20first%20several%20views%20of%20the%0Aobject%20with%20factored%20shaded%20and%20albedo%20appearance%20channels%2C%20and%20then%0Areconstructs%20colours%2C%20metalness%20and%20roughness%20in%203D%2C%20using%20a%20deferred%20shading%0Aloss%20for%20efficient%20supervision.%20It%20also%20uses%20a%20sign-distance%20function%20to%0Arepresent%203D%20shape%20more%20reliably%20and%20introduces%20a%20corresponding%20loss%20for%20direct%0Ashape%20supervision.%20This%20is%20implemented%20using%20fused%20kernels%20for%20high%20memory%0Aefficiency.%20After%20mesh%20extraction%2C%20a%20texture%20refinement%20transformer%20operating%0Ain%20UV%20space%20significantly%20improves%20sharpness%20and%20details.%20AssetGen%20achieves%2017%25%0Aimprovement%20in%20Chamfer%20Distance%20and%2040%25%20in%20LPIPS%20over%20the%20best%20concurrent%20work%0Afor%20few-view%20reconstruction%2C%20and%20a%20human%20preference%20of%2072%25%20over%20the%20best%0Aindustry%20competitors%20of%20comparable%20speed%2C%20including%20those%20that%20support%20PBR.%0AProject%20page%20with%20generated%20assets%3A%20https%3A//assetgen.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02445v1&entry.124074799=Read"},
{"title": "WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation", "author": "Zihao Huang and ShouKang Hu and Guangcong Wang and Tianqi Liu and Yuhang Zang and Zhiguo Cao and Wei Li and Ziwei Liu", "abstract": "  Existing human datasets for avatar creation are typically limited to\nlaboratory environments, wherein high-quality annotations (e.g., SMPL\nestimation from 3D scans or multi-view images) can be ideally provided.\nHowever, their annotating requirements are impractical for real-world images or\nvideos, posing challenges toward real-world applications on current avatar\ncreation methods. To this end, we propose the WildAvatar dataset, a web-scale\nin-the-wild human avatar creation dataset extracted from YouTube, with\n$10,000+$ different human subjects and scenes. WildAvatar is at least\n$10\\times$ richer than previous datasets for 3D human avatar creation. We\nevaluate several state-of-the-art avatar creation methods on our dataset,\nhighlighting the unexplored challenges in real-world applications on avatar\ncreation. We also demonstrate the potential for generalizability of avatar\ncreation methods, when provided with data at scale. We will publicly release\nour data source links and annotations, to push forward 3D human avatar creation\nand other related fields for real-world applications.\n", "link": "http://arxiv.org/abs/2407.02165v1", "date": "2024-07-02", "relevancy": 2.8228, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5672}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5672}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildAvatar%3A%20Web-scale%20In-the-wild%20Video%20Dataset%20for%203D%20Avatar%20Creation&body=Title%3A%20WildAvatar%3A%20Web-scale%20In-the-wild%20Video%20Dataset%20for%203D%20Avatar%20Creation%0AAuthor%3A%20Zihao%20Huang%20and%20ShouKang%20Hu%20and%20Guangcong%20Wang%20and%20Tianqi%20Liu%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Existing%20human%20datasets%20for%20avatar%20creation%20are%20typically%20limited%20to%0Alaboratory%20environments%2C%20wherein%20high-quality%20annotations%20%28e.g.%2C%20SMPL%0Aestimation%20from%203D%20scans%20or%20multi-view%20images%29%20can%20be%20ideally%20provided.%0AHowever%2C%20their%20annotating%20requirements%20are%20impractical%20for%20real-world%20images%20or%0Avideos%2C%20posing%20challenges%20toward%20real-world%20applications%20on%20current%20avatar%0Acreation%20methods.%20To%20this%20end%2C%20we%20propose%20the%20WildAvatar%20dataset%2C%20a%20web-scale%0Ain-the-wild%20human%20avatar%20creation%20dataset%20extracted%20from%20YouTube%2C%20with%0A%2410%2C000%2B%24%20different%20human%20subjects%20and%20scenes.%20WildAvatar%20is%20at%20least%0A%2410%5Ctimes%24%20richer%20than%20previous%20datasets%20for%203D%20human%20avatar%20creation.%20We%0Aevaluate%20several%20state-of-the-art%20avatar%20creation%20methods%20on%20our%20dataset%2C%0Ahighlighting%20the%20unexplored%20challenges%20in%20real-world%20applications%20on%20avatar%0Acreation.%20We%20also%20demonstrate%20the%20potential%20for%20generalizability%20of%20avatar%0Acreation%20methods%2C%20when%20provided%20with%20data%20at%20scale.%20We%20will%20publicly%20release%0Aour%20data%20source%20links%20and%20annotations%2C%20to%20push%20forward%203D%20human%20avatar%20creation%0Aand%20other%20related%20fields%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildAvatar%253A%2520Web-scale%2520In-the-wild%2520Video%2520Dataset%2520for%25203D%2520Avatar%2520Creation%26entry.906535625%3DZihao%2520Huang%2520and%2520ShouKang%2520Hu%2520and%2520Guangcong%2520Wang%2520and%2520Tianqi%2520Liu%2520and%2520Yuhang%2520Zang%2520and%2520Zhiguo%2520Cao%2520and%2520Wei%2520Li%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520human%2520datasets%2520for%2520avatar%2520creation%2520are%2520typically%2520limited%2520to%250Alaboratory%2520environments%252C%2520wherein%2520high-quality%2520annotations%2520%2528e.g.%252C%2520SMPL%250Aestimation%2520from%25203D%2520scans%2520or%2520multi-view%2520images%2529%2520can%2520be%2520ideally%2520provided.%250AHowever%252C%2520their%2520annotating%2520requirements%2520are%2520impractical%2520for%2520real-world%2520images%2520or%250Avideos%252C%2520posing%2520challenges%2520toward%2520real-world%2520applications%2520on%2520current%2520avatar%250Acreation%2520methods.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520WildAvatar%2520dataset%252C%2520a%2520web-scale%250Ain-the-wild%2520human%2520avatar%2520creation%2520dataset%2520extracted%2520from%2520YouTube%252C%2520with%250A%252410%252C000%252B%2524%2520different%2520human%2520subjects%2520and%2520scenes.%2520WildAvatar%2520is%2520at%2520least%250A%252410%255Ctimes%2524%2520richer%2520than%2520previous%2520datasets%2520for%25203D%2520human%2520avatar%2520creation.%2520We%250Aevaluate%2520several%2520state-of-the-art%2520avatar%2520creation%2520methods%2520on%2520our%2520dataset%252C%250Ahighlighting%2520the%2520unexplored%2520challenges%2520in%2520real-world%2520applications%2520on%2520avatar%250Acreation.%2520We%2520also%2520demonstrate%2520the%2520potential%2520for%2520generalizability%2520of%2520avatar%250Acreation%2520methods%252C%2520when%2520provided%2520with%2520data%2520at%2520scale.%2520We%2520will%2520publicly%2520release%250Aour%2520data%2520source%2520links%2520and%2520annotations%252C%2520to%2520push%2520forward%25203D%2520human%2520avatar%2520creation%250Aand%2520other%2520related%2520fields%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildAvatar%3A%20Web-scale%20In-the-wild%20Video%20Dataset%20for%203D%20Avatar%20Creation&entry.906535625=Zihao%20Huang%20and%20ShouKang%20Hu%20and%20Guangcong%20Wang%20and%20Tianqi%20Liu%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu&entry.1292438233=%20%20Existing%20human%20datasets%20for%20avatar%20creation%20are%20typically%20limited%20to%0Alaboratory%20environments%2C%20wherein%20high-quality%20annotations%20%28e.g.%2C%20SMPL%0Aestimation%20from%203D%20scans%20or%20multi-view%20images%29%20can%20be%20ideally%20provided.%0AHowever%2C%20their%20annotating%20requirements%20are%20impractical%20for%20real-world%20images%20or%0Avideos%2C%20posing%20challenges%20toward%20real-world%20applications%20on%20current%20avatar%0Acreation%20methods.%20To%20this%20end%2C%20we%20propose%20the%20WildAvatar%20dataset%2C%20a%20web-scale%0Ain-the-wild%20human%20avatar%20creation%20dataset%20extracted%20from%20YouTube%2C%20with%0A%2410%2C000%2B%24%20different%20human%20subjects%20and%20scenes.%20WildAvatar%20is%20at%20least%0A%2410%5Ctimes%24%20richer%20than%20previous%20datasets%20for%203D%20human%20avatar%20creation.%20We%0Aevaluate%20several%20state-of-the-art%20avatar%20creation%20methods%20on%20our%20dataset%2C%0Ahighlighting%20the%20unexplored%20challenges%20in%20real-world%20applications%20on%20avatar%0Acreation.%20We%20also%20demonstrate%20the%20potential%20for%20generalizability%20of%20avatar%0Acreation%20methods%2C%20when%20provided%20with%20data%20at%20scale.%20We%20will%20publicly%20release%0Aour%20data%20source%20links%20and%20annotations%2C%20to%20push%20forward%203D%20human%20avatar%20creation%0Aand%20other%20related%20fields%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02165v1&entry.124074799=Read"},
{"title": "Reconstruction of Cardiac Cine MRI Using Motion-Guided Deformable\n  Alignment and Multi-Resolution Fusion", "author": "Xiaoxiang Han and Yang Chen and Qiaohong Liu and Yiman Liu and Keyan Chen and Yuanjie Lin and Weikun Zhang", "abstract": "  Cardiac cine magnetic resonance imaging (MRI) is one of the important means\nto assess cardiac functions and vascular abnormalities. Mitigating artifacts\narising during image reconstruction and accelerating cardiac cine MRI\nacquisition to obtain high-quality images is important. A novel end-to-end deep\nlearning network is developed to improve cardiac cine MRI reconstruction.\nFirst, a U-Net is adopted to obtain the initial reconstructed images in\nk-space. Further to remove the motion artifacts, the motion-guided deformable\nalignment (MGDA) module with second-order bidirectional propagation is\nintroduced to align the adjacent cine MRI frames by maximizing spatial-temporal\ninformation to alleviate motion artifacts. Finally, the multi-resolution fusion\n(MRF) module is designed to correct the blur and artifacts generated from\nalignment operation and obtain the last high-quality reconstructed cardiac\nimages. At an 8$\\times$ acceleration rate, the numerical measurements on the\nACDC dataset are structural similarity index (SSIM) of 78.40%$\\pm$.57%, peak\nsignal-to-noise ratio (PSNR) of 30.46$\\pm$1.22dB, and normalized mean squared\nerror (NMSE) of 0.0468$\\pm$0.0075. On the ACMRI dataset, the results are SSIM\nof 87.65%$\\pm$4.20%, PSNR of 30.04$\\pm$1.18dB, and NMSE of 0.0473$\\pm$0.0072.\nThe proposed method exhibits high-quality results with richer details and fewer\nartifacts for cardiac cine MRI reconstruction on different accelerations.\n", "link": "http://arxiv.org/abs/2303.04968v4", "date": "2024-07-02", "relevancy": 2.7273, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5478}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20of%20Cardiac%20Cine%20MRI%20Using%20Motion-Guided%20Deformable%0A%20%20Alignment%20and%20Multi-Resolution%20Fusion&body=Title%3A%20Reconstruction%20of%20Cardiac%20Cine%20MRI%20Using%20Motion-Guided%20Deformable%0A%20%20Alignment%20and%20Multi-Resolution%20Fusion%0AAuthor%3A%20Xiaoxiang%20Han%20and%20Yang%20Chen%20and%20Qiaohong%20Liu%20and%20Yiman%20Liu%20and%20Keyan%20Chen%20and%20Yuanjie%20Lin%20and%20Weikun%20Zhang%0AAbstract%3A%20%20%20Cardiac%20cine%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20one%20of%20the%20important%20means%0Ato%20assess%20cardiac%20functions%20and%20vascular%20abnormalities.%20Mitigating%20artifacts%0Aarising%20during%20image%20reconstruction%20and%20accelerating%20cardiac%20cine%20MRI%0Aacquisition%20to%20obtain%20high-quality%20images%20is%20important.%20A%20novel%20end-to-end%20deep%0Alearning%20network%20is%20developed%20to%20improve%20cardiac%20cine%20MRI%20reconstruction.%0AFirst%2C%20a%20U-Net%20is%20adopted%20to%20obtain%20the%20initial%20reconstructed%20images%20in%0Ak-space.%20Further%20to%20remove%20the%20motion%20artifacts%2C%20the%20motion-guided%20deformable%0Aalignment%20%28MGDA%29%20module%20with%20second-order%20bidirectional%20propagation%20is%0Aintroduced%20to%20align%20the%20adjacent%20cine%20MRI%20frames%20by%20maximizing%20spatial-temporal%0Ainformation%20to%20alleviate%20motion%20artifacts.%20Finally%2C%20the%20multi-resolution%20fusion%0A%28MRF%29%20module%20is%20designed%20to%20correct%20the%20blur%20and%20artifacts%20generated%20from%0Aalignment%20operation%20and%20obtain%20the%20last%20high-quality%20reconstructed%20cardiac%0Aimages.%20At%20an%208%24%5Ctimes%24%20acceleration%20rate%2C%20the%20numerical%20measurements%20on%20the%0AACDC%20dataset%20are%20structural%20similarity%20index%20%28SSIM%29%20of%2078.40%25%24%5Cpm%24.57%25%2C%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20of%2030.46%24%5Cpm%241.22dB%2C%20and%20normalized%20mean%20squared%0Aerror%20%28NMSE%29%20of%200.0468%24%5Cpm%240.0075.%20On%20the%20ACMRI%20dataset%2C%20the%20results%20are%20SSIM%0Aof%2087.65%25%24%5Cpm%244.20%25%2C%20PSNR%20of%2030.04%24%5Cpm%241.18dB%2C%20and%20NMSE%20of%200.0473%24%5Cpm%240.0072.%0AThe%20proposed%20method%20exhibits%20high-quality%20results%20with%20richer%20details%20and%20fewer%0Aartifacts%20for%20cardiac%20cine%20MRI%20reconstruction%20on%20different%20accelerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.04968v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520of%2520Cardiac%2520Cine%2520MRI%2520Using%2520Motion-Guided%2520Deformable%250A%2520%2520Alignment%2520and%2520Multi-Resolution%2520Fusion%26entry.906535625%3DXiaoxiang%2520Han%2520and%2520Yang%2520Chen%2520and%2520Qiaohong%2520Liu%2520and%2520Yiman%2520Liu%2520and%2520Keyan%2520Chen%2520and%2520Yuanjie%2520Lin%2520and%2520Weikun%2520Zhang%26entry.1292438233%3D%2520%2520Cardiac%2520cine%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520one%2520of%2520the%2520important%2520means%250Ato%2520assess%2520cardiac%2520functions%2520and%2520vascular%2520abnormalities.%2520Mitigating%2520artifacts%250Aarising%2520during%2520image%2520reconstruction%2520and%2520accelerating%2520cardiac%2520cine%2520MRI%250Aacquisition%2520to%2520obtain%2520high-quality%2520images%2520is%2520important.%2520A%2520novel%2520end-to-end%2520deep%250Alearning%2520network%2520is%2520developed%2520to%2520improve%2520cardiac%2520cine%2520MRI%2520reconstruction.%250AFirst%252C%2520a%2520U-Net%2520is%2520adopted%2520to%2520obtain%2520the%2520initial%2520reconstructed%2520images%2520in%250Ak-space.%2520Further%2520to%2520remove%2520the%2520motion%2520artifacts%252C%2520the%2520motion-guided%2520deformable%250Aalignment%2520%2528MGDA%2529%2520module%2520with%2520second-order%2520bidirectional%2520propagation%2520is%250Aintroduced%2520to%2520align%2520the%2520adjacent%2520cine%2520MRI%2520frames%2520by%2520maximizing%2520spatial-temporal%250Ainformation%2520to%2520alleviate%2520motion%2520artifacts.%2520Finally%252C%2520the%2520multi-resolution%2520fusion%250A%2528MRF%2529%2520module%2520is%2520designed%2520to%2520correct%2520the%2520blur%2520and%2520artifacts%2520generated%2520from%250Aalignment%2520operation%2520and%2520obtain%2520the%2520last%2520high-quality%2520reconstructed%2520cardiac%250Aimages.%2520At%2520an%25208%2524%255Ctimes%2524%2520acceleration%2520rate%252C%2520the%2520numerical%2520measurements%2520on%2520the%250AACDC%2520dataset%2520are%2520structural%2520similarity%2520index%2520%2528SSIM%2529%2520of%252078.40%2525%2524%255Cpm%2524.57%2525%252C%2520peak%250Asignal-to-noise%2520ratio%2520%2528PSNR%2529%2520of%252030.46%2524%255Cpm%25241.22dB%252C%2520and%2520normalized%2520mean%2520squared%250Aerror%2520%2528NMSE%2529%2520of%25200.0468%2524%255Cpm%25240.0075.%2520On%2520the%2520ACMRI%2520dataset%252C%2520the%2520results%2520are%2520SSIM%250Aof%252087.65%2525%2524%255Cpm%25244.20%2525%252C%2520PSNR%2520of%252030.04%2524%255Cpm%25241.18dB%252C%2520and%2520NMSE%2520of%25200.0473%2524%255Cpm%25240.0072.%250AThe%2520proposed%2520method%2520exhibits%2520high-quality%2520results%2520with%2520richer%2520details%2520and%2520fewer%250Aartifacts%2520for%2520cardiac%2520cine%2520MRI%2520reconstruction%2520on%2520different%2520accelerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.04968v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20of%20Cardiac%20Cine%20MRI%20Using%20Motion-Guided%20Deformable%0A%20%20Alignment%20and%20Multi-Resolution%20Fusion&entry.906535625=Xiaoxiang%20Han%20and%20Yang%20Chen%20and%20Qiaohong%20Liu%20and%20Yiman%20Liu%20and%20Keyan%20Chen%20and%20Yuanjie%20Lin%20and%20Weikun%20Zhang&entry.1292438233=%20%20Cardiac%20cine%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20one%20of%20the%20important%20means%0Ato%20assess%20cardiac%20functions%20and%20vascular%20abnormalities.%20Mitigating%20artifacts%0Aarising%20during%20image%20reconstruction%20and%20accelerating%20cardiac%20cine%20MRI%0Aacquisition%20to%20obtain%20high-quality%20images%20is%20important.%20A%20novel%20end-to-end%20deep%0Alearning%20network%20is%20developed%20to%20improve%20cardiac%20cine%20MRI%20reconstruction.%0AFirst%2C%20a%20U-Net%20is%20adopted%20to%20obtain%20the%20initial%20reconstructed%20images%20in%0Ak-space.%20Further%20to%20remove%20the%20motion%20artifacts%2C%20the%20motion-guided%20deformable%0Aalignment%20%28MGDA%29%20module%20with%20second-order%20bidirectional%20propagation%20is%0Aintroduced%20to%20align%20the%20adjacent%20cine%20MRI%20frames%20by%20maximizing%20spatial-temporal%0Ainformation%20to%20alleviate%20motion%20artifacts.%20Finally%2C%20the%20multi-resolution%20fusion%0A%28MRF%29%20module%20is%20designed%20to%20correct%20the%20blur%20and%20artifacts%20generated%20from%0Aalignment%20operation%20and%20obtain%20the%20last%20high-quality%20reconstructed%20cardiac%0Aimages.%20At%20an%208%24%5Ctimes%24%20acceleration%20rate%2C%20the%20numerical%20measurements%20on%20the%0AACDC%20dataset%20are%20structural%20similarity%20index%20%28SSIM%29%20of%2078.40%25%24%5Cpm%24.57%25%2C%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20of%2030.46%24%5Cpm%241.22dB%2C%20and%20normalized%20mean%20squared%0Aerror%20%28NMSE%29%20of%200.0468%24%5Cpm%240.0075.%20On%20the%20ACMRI%20dataset%2C%20the%20results%20are%20SSIM%0Aof%2087.65%25%24%5Cpm%244.20%25%2C%20PSNR%20of%2030.04%24%5Cpm%241.18dB%2C%20and%20NMSE%20of%200.0473%24%5Cpm%240.0072.%0AThe%20proposed%20method%20exhibits%20high-quality%20results%20with%20richer%20details%20and%20fewer%0Aartifacts%20for%20cardiac%20cine%20MRI%20reconstruction%20on%20different%20accelerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.04968v4&entry.124074799=Read"},
{"title": "MIGC++: Advanced Multi-Instance Generation Controller for Image\n  Synthesis", "author": "Dewei Zhou and You Li and Fan Ma and Zongxin Yang and Yi Yang", "abstract": "  We introduce the Multi-Instance Generation (MIG) task, which focuses on\ngenerating multiple instances within a single image, each accurately placed at\npredefined positions with attributes such as category, color, and shape,\nstrictly following user specifications. MIG faces three main challenges:\navoiding attribute leakage between instances, supporting diverse instance\ndescriptions, and maintaining consistency in iterative generation. To address\nattribute leakage, we propose the Multi-Instance Generation Controller (MIGC).\nMIGC generates multiple instances through a divide-and-conquer strategy,\nbreaking down multi-instance shading into single-instance tasks with singular\nattributes, later integrated. To provide more types of instance descriptions,\nwe developed MIGC++. MIGC++ allows attribute control through text \\& images and\nposition control through boxes \\& masks. Lastly, we introduced the\nConsistent-MIG algorithm to enhance the iterative MIG ability of MIGC and\nMIGC++. This algorithm ensures consistency in unmodified regions during the\naddition, deletion, or modification of instances, and preserves the identity of\ninstances when their attributes are changed. We introduce the COCO-MIG and\nMultimodal-MIG benchmarks to evaluate these methods. Extensive experiments on\nthese benchmarks, along with the COCO-Position benchmark and DrawBench,\ndemonstrate that our methods substantially outperform existing techniques,\nmaintaining precise control over aspects including position, attribute, and\nquantity. Project page: https://github.com/limuloo/MIGC.\n", "link": "http://arxiv.org/abs/2407.02329v1", "date": "2024-07-02", "relevancy": 2.7166, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5425}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIGC%2B%2B%3A%20Advanced%20Multi-Instance%20Generation%20Controller%20for%20Image%0A%20%20Synthesis&body=Title%3A%20MIGC%2B%2B%3A%20Advanced%20Multi-Instance%20Generation%20Controller%20for%20Image%0A%20%20Synthesis%0AAuthor%3A%20Dewei%20Zhou%20and%20You%20Li%20and%20Fan%20Ma%20and%20Zongxin%20Yang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20We%20introduce%20the%20Multi-Instance%20Generation%20%28MIG%29%20task%2C%20which%20focuses%20on%0Agenerating%20multiple%20instances%20within%20a%20single%20image%2C%20each%20accurately%20placed%20at%0Apredefined%20positions%20with%20attributes%20such%20as%20category%2C%20color%2C%20and%20shape%2C%0Astrictly%20following%20user%20specifications.%20MIG%20faces%20three%20main%20challenges%3A%0Aavoiding%20attribute%20leakage%20between%20instances%2C%20supporting%20diverse%20instance%0Adescriptions%2C%20and%20maintaining%20consistency%20in%20iterative%20generation.%20To%20address%0Aattribute%20leakage%2C%20we%20propose%20the%20Multi-Instance%20Generation%20Controller%20%28MIGC%29.%0AMIGC%20generates%20multiple%20instances%20through%20a%20divide-and-conquer%20strategy%2C%0Abreaking%20down%20multi-instance%20shading%20into%20single-instance%20tasks%20with%20singular%0Aattributes%2C%20later%20integrated.%20To%20provide%20more%20types%20of%20instance%20descriptions%2C%0Awe%20developed%20MIGC%2B%2B.%20MIGC%2B%2B%20allows%20attribute%20control%20through%20text%20%5C%26%20images%20and%0Aposition%20control%20through%20boxes%20%5C%26%20masks.%20Lastly%2C%20we%20introduced%20the%0AConsistent-MIG%20algorithm%20to%20enhance%20the%20iterative%20MIG%20ability%20of%20MIGC%20and%0AMIGC%2B%2B.%20This%20algorithm%20ensures%20consistency%20in%20unmodified%20regions%20during%20the%0Aaddition%2C%20deletion%2C%20or%20modification%20of%20instances%2C%20and%20preserves%20the%20identity%20of%0Ainstances%20when%20their%20attributes%20are%20changed.%20We%20introduce%20the%20COCO-MIG%20and%0AMultimodal-MIG%20benchmarks%20to%20evaluate%20these%20methods.%20Extensive%20experiments%20on%0Athese%20benchmarks%2C%20along%20with%20the%20COCO-Position%20benchmark%20and%20DrawBench%2C%0Ademonstrate%20that%20our%20methods%20substantially%20outperform%20existing%20techniques%2C%0Amaintaining%20precise%20control%20over%20aspects%20including%20position%2C%20attribute%2C%20and%0Aquantity.%20Project%20page%3A%20https%3A//github.com/limuloo/MIGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIGC%252B%252B%253A%2520Advanced%2520Multi-Instance%2520Generation%2520Controller%2520for%2520Image%250A%2520%2520Synthesis%26entry.906535625%3DDewei%2520Zhou%2520and%2520You%2520Li%2520and%2520Fan%2520Ma%2520and%2520Zongxin%2520Yang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Multi-Instance%2520Generation%2520%2528MIG%2529%2520task%252C%2520which%2520focuses%2520on%250Agenerating%2520multiple%2520instances%2520within%2520a%2520single%2520image%252C%2520each%2520accurately%2520placed%2520at%250Apredefined%2520positions%2520with%2520attributes%2520such%2520as%2520category%252C%2520color%252C%2520and%2520shape%252C%250Astrictly%2520following%2520user%2520specifications.%2520MIG%2520faces%2520three%2520main%2520challenges%253A%250Aavoiding%2520attribute%2520leakage%2520between%2520instances%252C%2520supporting%2520diverse%2520instance%250Adescriptions%252C%2520and%2520maintaining%2520consistency%2520in%2520iterative%2520generation.%2520To%2520address%250Aattribute%2520leakage%252C%2520we%2520propose%2520the%2520Multi-Instance%2520Generation%2520Controller%2520%2528MIGC%2529.%250AMIGC%2520generates%2520multiple%2520instances%2520through%2520a%2520divide-and-conquer%2520strategy%252C%250Abreaking%2520down%2520multi-instance%2520shading%2520into%2520single-instance%2520tasks%2520with%2520singular%250Aattributes%252C%2520later%2520integrated.%2520To%2520provide%2520more%2520types%2520of%2520instance%2520descriptions%252C%250Awe%2520developed%2520MIGC%252B%252B.%2520MIGC%252B%252B%2520allows%2520attribute%2520control%2520through%2520text%2520%255C%2526%2520images%2520and%250Aposition%2520control%2520through%2520boxes%2520%255C%2526%2520masks.%2520Lastly%252C%2520we%2520introduced%2520the%250AConsistent-MIG%2520algorithm%2520to%2520enhance%2520the%2520iterative%2520MIG%2520ability%2520of%2520MIGC%2520and%250AMIGC%252B%252B.%2520This%2520algorithm%2520ensures%2520consistency%2520in%2520unmodified%2520regions%2520during%2520the%250Aaddition%252C%2520deletion%252C%2520or%2520modification%2520of%2520instances%252C%2520and%2520preserves%2520the%2520identity%2520of%250Ainstances%2520when%2520their%2520attributes%2520are%2520changed.%2520We%2520introduce%2520the%2520COCO-MIG%2520and%250AMultimodal-MIG%2520benchmarks%2520to%2520evaluate%2520these%2520methods.%2520Extensive%2520experiments%2520on%250Athese%2520benchmarks%252C%2520along%2520with%2520the%2520COCO-Position%2520benchmark%2520and%2520DrawBench%252C%250Ademonstrate%2520that%2520our%2520methods%2520substantially%2520outperform%2520existing%2520techniques%252C%250Amaintaining%2520precise%2520control%2520over%2520aspects%2520including%2520position%252C%2520attribute%252C%2520and%250Aquantity.%2520Project%2520page%253A%2520https%253A//github.com/limuloo/MIGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIGC%2B%2B%3A%20Advanced%20Multi-Instance%20Generation%20Controller%20for%20Image%0A%20%20Synthesis&entry.906535625=Dewei%20Zhou%20and%20You%20Li%20and%20Fan%20Ma%20and%20Zongxin%20Yang%20and%20Yi%20Yang&entry.1292438233=%20%20We%20introduce%20the%20Multi-Instance%20Generation%20%28MIG%29%20task%2C%20which%20focuses%20on%0Agenerating%20multiple%20instances%20within%20a%20single%20image%2C%20each%20accurately%20placed%20at%0Apredefined%20positions%20with%20attributes%20such%20as%20category%2C%20color%2C%20and%20shape%2C%0Astrictly%20following%20user%20specifications.%20MIG%20faces%20three%20main%20challenges%3A%0Aavoiding%20attribute%20leakage%20between%20instances%2C%20supporting%20diverse%20instance%0Adescriptions%2C%20and%20maintaining%20consistency%20in%20iterative%20generation.%20To%20address%0Aattribute%20leakage%2C%20we%20propose%20the%20Multi-Instance%20Generation%20Controller%20%28MIGC%29.%0AMIGC%20generates%20multiple%20instances%20through%20a%20divide-and-conquer%20strategy%2C%0Abreaking%20down%20multi-instance%20shading%20into%20single-instance%20tasks%20with%20singular%0Aattributes%2C%20later%20integrated.%20To%20provide%20more%20types%20of%20instance%20descriptions%2C%0Awe%20developed%20MIGC%2B%2B.%20MIGC%2B%2B%20allows%20attribute%20control%20through%20text%20%5C%26%20images%20and%0Aposition%20control%20through%20boxes%20%5C%26%20masks.%20Lastly%2C%20we%20introduced%20the%0AConsistent-MIG%20algorithm%20to%20enhance%20the%20iterative%20MIG%20ability%20of%20MIGC%20and%0AMIGC%2B%2B.%20This%20algorithm%20ensures%20consistency%20in%20unmodified%20regions%20during%20the%0Aaddition%2C%20deletion%2C%20or%20modification%20of%20instances%2C%20and%20preserves%20the%20identity%20of%0Ainstances%20when%20their%20attributes%20are%20changed.%20We%20introduce%20the%20COCO-MIG%20and%0AMultimodal-MIG%20benchmarks%20to%20evaluate%20these%20methods.%20Extensive%20experiments%20on%0Athese%20benchmarks%2C%20along%20with%20the%20COCO-Position%20benchmark%20and%20DrawBench%2C%0Ademonstrate%20that%20our%20methods%20substantially%20outperform%20existing%20techniques%2C%0Amaintaining%20precise%20control%20over%20aspects%20including%20position%2C%20attribute%2C%20and%0Aquantity.%20Project%20page%3A%20https%3A//github.com/limuloo/MIGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02329v1&entry.124074799=Read"},
{"title": "TokenPacker: Efficient Visual Projector for Multimodal LLM", "author": "Wentong Li and Yuqian Yuan and Jian Liu and Dongqi Tang and Song Wang and Jianke Zhu and Lei Zhang", "abstract": "  The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.\n", "link": "http://arxiv.org/abs/2407.02392v1", "date": "2024-07-02", "relevancy": 2.69, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM&body=Title%3A%20TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM%0AAuthor%3A%20Wentong%20Li%20and%20Yuqian%20Yuan%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Song%20Wang%20and%20Jianke%20Zhu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20The%20visual%20projector%20serves%20as%20an%20essential%20bridge%20between%20the%20visual%20encoder%0Aand%20the%20Large%20Language%20Model%20%28LLM%29%20in%20a%20Multimodal%20LLM%20%28MLLM%29.%20Typically%2C%20MLLMs%0Aadopt%20a%20simple%20MLP%20to%20preserve%20all%20visual%20contexts%20via%20one-to-one%0Atransformation.%20However%2C%20the%20visual%20tokens%20are%20redundant%20and%20can%20be%0Aconsiderably%20increased%20when%20dealing%20with%20high-resolution%20images%2C%20impairing%20the%0Aefficiency%20of%20MLLMs%20significantly.%20Some%20recent%20works%20have%20introduced%20resampler%0Aor%20abstractor%20to%20reduce%20the%20number%20of%20resulting%20visual%20tokens.%20Unfortunately%2C%0Athey%20fail%20to%20capture%20finer%20details%20and%20undermine%20the%20visual%20reasoning%0Acapabilities%20of%20MLLMs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20visual%20projector%2C%20which%0Aadopts%20a%20coarse-to-fine%20scheme%20to%20inject%20the%20enriched%20characteristics%20to%0Agenerate%20the%20condensed%20visual%20tokens.%20In%20specific%2C%20we%20first%20interpolate%20the%0Avisual%20features%20as%20a%20low-resolution%20point%20query%2C%20providing%20the%20overall%20visual%0Arepresentation%20as%20the%20foundation.%20Then%2C%20we%20introduce%20a%20region-to-point%0Ainjection%20module%20that%20utilizes%20high-resolution%2C%20multi-level%20region-based%20cues%0Aas%20fine-grained%20reference%20keys%20and%20values%2C%20allowing%20them%20to%20be%20fully%20absorbed%0Awithin%20the%20corresponding%20local%20context%20region.%20This%20step%20effectively%20updates%0Athe%20coarse%20point%20query%2C%20transforming%20it%20into%20an%20enriched%20one%20for%20the%20subsequent%0ALLM%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20compresses%0Athe%20visual%20tokens%20by%2075%25~89%25%2C%20while%20achieves%20comparable%20or%20even%20better%0Aperformance%20across%20diverse%20benchmarks%20with%20significantly%20higher%20efficiency.%20The%0Asource%20codes%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/TokenPacker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenPacker%253A%2520Efficient%2520Visual%2520Projector%2520for%2520Multimodal%2520LLM%26entry.906535625%3DWentong%2520Li%2520and%2520Yuqian%2520Yuan%2520and%2520Jian%2520Liu%2520and%2520Dongqi%2520Tang%2520and%2520Song%2520Wang%2520and%2520Jianke%2520Zhu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520visual%2520projector%2520serves%2520as%2520an%2520essential%2520bridge%2520between%2520the%2520visual%2520encoder%250Aand%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520in%2520a%2520Multimodal%2520LLM%2520%2528MLLM%2529.%2520Typically%252C%2520MLLMs%250Aadopt%2520a%2520simple%2520MLP%2520to%2520preserve%2520all%2520visual%2520contexts%2520via%2520one-to-one%250Atransformation.%2520However%252C%2520the%2520visual%2520tokens%2520are%2520redundant%2520and%2520can%2520be%250Aconsiderably%2520increased%2520when%2520dealing%2520with%2520high-resolution%2520images%252C%2520impairing%2520the%250Aefficiency%2520of%2520MLLMs%2520significantly.%2520Some%2520recent%2520works%2520have%2520introduced%2520resampler%250Aor%2520abstractor%2520to%2520reduce%2520the%2520number%2520of%2520resulting%2520visual%2520tokens.%2520Unfortunately%252C%250Athey%2520fail%2520to%2520capture%2520finer%2520details%2520and%2520undermine%2520the%2520visual%2520reasoning%250Acapabilities%2520of%2520MLLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520visual%2520projector%252C%2520which%250Aadopts%2520a%2520coarse-to-fine%2520scheme%2520to%2520inject%2520the%2520enriched%2520characteristics%2520to%250Agenerate%2520the%2520condensed%2520visual%2520tokens.%2520In%2520specific%252C%2520we%2520first%2520interpolate%2520the%250Avisual%2520features%2520as%2520a%2520low-resolution%2520point%2520query%252C%2520providing%2520the%2520overall%2520visual%250Arepresentation%2520as%2520the%2520foundation.%2520Then%252C%2520we%2520introduce%2520a%2520region-to-point%250Ainjection%2520module%2520that%2520utilizes%2520high-resolution%252C%2520multi-level%2520region-based%2520cues%250Aas%2520fine-grained%2520reference%2520keys%2520and%2520values%252C%2520allowing%2520them%2520to%2520be%2520fully%2520absorbed%250Awithin%2520the%2520corresponding%2520local%2520context%2520region.%2520This%2520step%2520effectively%2520updates%250Athe%2520coarse%2520point%2520query%252C%2520transforming%2520it%2520into%2520an%2520enriched%2520one%2520for%2520the%2520subsequent%250ALLM%2520reasoning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520compresses%250Athe%2520visual%2520tokens%2520by%252075%2525~89%2525%252C%2520while%2520achieves%2520comparable%2520or%2520even%2520better%250Aperformance%2520across%2520diverse%2520benchmarks%2520with%2520significantly%2520higher%2520efficiency.%2520The%250Asource%2520codes%2520can%2520be%2520found%2520at%2520https%253A//github.com/CircleRadon/TokenPacker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenPacker%3A%20Efficient%20Visual%20Projector%20for%20Multimodal%20LLM&entry.906535625=Wentong%20Li%20and%20Yuqian%20Yuan%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Song%20Wang%20and%20Jianke%20Zhu%20and%20Lei%20Zhang&entry.1292438233=%20%20The%20visual%20projector%20serves%20as%20an%20essential%20bridge%20between%20the%20visual%20encoder%0Aand%20the%20Large%20Language%20Model%20%28LLM%29%20in%20a%20Multimodal%20LLM%20%28MLLM%29.%20Typically%2C%20MLLMs%0Aadopt%20a%20simple%20MLP%20to%20preserve%20all%20visual%20contexts%20via%20one-to-one%0Atransformation.%20However%2C%20the%20visual%20tokens%20are%20redundant%20and%20can%20be%0Aconsiderably%20increased%20when%20dealing%20with%20high-resolution%20images%2C%20impairing%20the%0Aefficiency%20of%20MLLMs%20significantly.%20Some%20recent%20works%20have%20introduced%20resampler%0Aor%20abstractor%20to%20reduce%20the%20number%20of%20resulting%20visual%20tokens.%20Unfortunately%2C%0Athey%20fail%20to%20capture%20finer%20details%20and%20undermine%20the%20visual%20reasoning%0Acapabilities%20of%20MLLMs.%20In%20this%20work%2C%20we%20propose%20a%20novel%20visual%20projector%2C%20which%0Aadopts%20a%20coarse-to-fine%20scheme%20to%20inject%20the%20enriched%20characteristics%20to%0Agenerate%20the%20condensed%20visual%20tokens.%20In%20specific%2C%20we%20first%20interpolate%20the%0Avisual%20features%20as%20a%20low-resolution%20point%20query%2C%20providing%20the%20overall%20visual%0Arepresentation%20as%20the%20foundation.%20Then%2C%20we%20introduce%20a%20region-to-point%0Ainjection%20module%20that%20utilizes%20high-resolution%2C%20multi-level%20region-based%20cues%0Aas%20fine-grained%20reference%20keys%20and%20values%2C%20allowing%20them%20to%20be%20fully%20absorbed%0Awithin%20the%20corresponding%20local%20context%20region.%20This%20step%20effectively%20updates%0Athe%20coarse%20point%20query%2C%20transforming%20it%20into%20an%20enriched%20one%20for%20the%20subsequent%0ALLM%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20compresses%0Athe%20visual%20tokens%20by%2075%25~89%25%2C%20while%20achieves%20comparable%20or%20even%20better%0Aperformance%20across%20diverse%20benchmarks%20with%20significantly%20higher%20efficiency.%20The%0Asource%20codes%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/TokenPacker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02392v1&entry.124074799=Read"},
{"title": "Homomorphism Autoencoder -- Learning Group Structured Representations\n  from Observed Transitions", "author": "Hamza Keurti and Hsiao-Ru Pan and Michel Besserve and Benjamin F. Grewe and Bernhard Sch\u00f6lkopf", "abstract": "  How can agents learn internal models that veridically represent interactions\nwith the real world is a largely open question. As machine learning is moving\ntowards representations containing not just observational but also\ninterventional knowledge, we study this problem using tools from representation\nlearning and group theory. We propose methods enabling an agent acting upon the\nworld to learn internal representations of sensory information that are\nconsistent with actions that modify it. We use an autoencoder equipped with a\ngroup representation acting on its latent space, trained using an\nequivariance-derived loss in order to enforce a suitable homomorphism property\non the group representation. In contrast to existing work, our approach does\nnot require prior knowledge of the group and does not restrict the set of\nactions the agent can perform. We motivate our method theoretically, and show\nempirically that it can learn a group representation of the actions, thereby\ncapturing the structure of the set of transformations applied to the\nenvironment. We further show that this allows agents to predict the effect of\nsequences of future actions with improved accuracy.\n", "link": "http://arxiv.org/abs/2207.12067v3", "date": "2024-07-02", "relevancy": 2.6857, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5553}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5451}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Homomorphism%20Autoencoder%20--%20Learning%20Group%20Structured%20Representations%0A%20%20from%20Observed%20Transitions&body=Title%3A%20Homomorphism%20Autoencoder%20--%20Learning%20Group%20Structured%20Representations%0A%20%20from%20Observed%20Transitions%0AAuthor%3A%20Hamza%20Keurti%20and%20Hsiao-Ru%20Pan%20and%20Michel%20Besserve%20and%20Benjamin%20F.%20Grewe%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20How%20can%20agents%20learn%20internal%20models%20that%20veridically%20represent%20interactions%0Awith%20the%20real%20world%20is%20a%20largely%20open%20question.%20As%20machine%20learning%20is%20moving%0Atowards%20representations%20containing%20not%20just%20observational%20but%20also%0Ainterventional%20knowledge%2C%20we%20study%20this%20problem%20using%20tools%20from%20representation%0Alearning%20and%20group%20theory.%20We%20propose%20methods%20enabling%20an%20agent%20acting%20upon%20the%0Aworld%20to%20learn%20internal%20representations%20of%20sensory%20information%20that%20are%0Aconsistent%20with%20actions%20that%20modify%20it.%20We%20use%20an%20autoencoder%20equipped%20with%20a%0Agroup%20representation%20acting%20on%20its%20latent%20space%2C%20trained%20using%20an%0Aequivariance-derived%20loss%20in%20order%20to%20enforce%20a%20suitable%20homomorphism%20property%0Aon%20the%20group%20representation.%20In%20contrast%20to%20existing%20work%2C%20our%20approach%20does%0Anot%20require%20prior%20knowledge%20of%20the%20group%20and%20does%20not%20restrict%20the%20set%20of%0Aactions%20the%20agent%20can%20perform.%20We%20motivate%20our%20method%20theoretically%2C%20and%20show%0Aempirically%20that%20it%20can%20learn%20a%20group%20representation%20of%20the%20actions%2C%20thereby%0Acapturing%20the%20structure%20of%20the%20set%20of%20transformations%20applied%20to%20the%0Aenvironment.%20We%20further%20show%20that%20this%20allows%20agents%20to%20predict%20the%20effect%20of%0Asequences%20of%20future%20actions%20with%20improved%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.12067v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHomomorphism%2520Autoencoder%2520--%2520Learning%2520Group%2520Structured%2520Representations%250A%2520%2520from%2520Observed%2520Transitions%26entry.906535625%3DHamza%2520Keurti%2520and%2520Hsiao-Ru%2520Pan%2520and%2520Michel%2520Besserve%2520and%2520Benjamin%2520F.%2520Grewe%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520How%2520can%2520agents%2520learn%2520internal%2520models%2520that%2520veridically%2520represent%2520interactions%250Awith%2520the%2520real%2520world%2520is%2520a%2520largely%2520open%2520question.%2520As%2520machine%2520learning%2520is%2520moving%250Atowards%2520representations%2520containing%2520not%2520just%2520observational%2520but%2520also%250Ainterventional%2520knowledge%252C%2520we%2520study%2520this%2520problem%2520using%2520tools%2520from%2520representation%250Alearning%2520and%2520group%2520theory.%2520We%2520propose%2520methods%2520enabling%2520an%2520agent%2520acting%2520upon%2520the%250Aworld%2520to%2520learn%2520internal%2520representations%2520of%2520sensory%2520information%2520that%2520are%250Aconsistent%2520with%2520actions%2520that%2520modify%2520it.%2520We%2520use%2520an%2520autoencoder%2520equipped%2520with%2520a%250Agroup%2520representation%2520acting%2520on%2520its%2520latent%2520space%252C%2520trained%2520using%2520an%250Aequivariance-derived%2520loss%2520in%2520order%2520to%2520enforce%2520a%2520suitable%2520homomorphism%2520property%250Aon%2520the%2520group%2520representation.%2520In%2520contrast%2520to%2520existing%2520work%252C%2520our%2520approach%2520does%250Anot%2520require%2520prior%2520knowledge%2520of%2520the%2520group%2520and%2520does%2520not%2520restrict%2520the%2520set%2520of%250Aactions%2520the%2520agent%2520can%2520perform.%2520We%2520motivate%2520our%2520method%2520theoretically%252C%2520and%2520show%250Aempirically%2520that%2520it%2520can%2520learn%2520a%2520group%2520representation%2520of%2520the%2520actions%252C%2520thereby%250Acapturing%2520the%2520structure%2520of%2520the%2520set%2520of%2520transformations%2520applied%2520to%2520the%250Aenvironment.%2520We%2520further%2520show%2520that%2520this%2520allows%2520agents%2520to%2520predict%2520the%2520effect%2520of%250Asequences%2520of%2520future%2520actions%2520with%2520improved%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.12067v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homomorphism%20Autoencoder%20--%20Learning%20Group%20Structured%20Representations%0A%20%20from%20Observed%20Transitions&entry.906535625=Hamza%20Keurti%20and%20Hsiao-Ru%20Pan%20and%20Michel%20Besserve%20and%20Benjamin%20F.%20Grewe%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20How%20can%20agents%20learn%20internal%20models%20that%20veridically%20represent%20interactions%0Awith%20the%20real%20world%20is%20a%20largely%20open%20question.%20As%20machine%20learning%20is%20moving%0Atowards%20representations%20containing%20not%20just%20observational%20but%20also%0Ainterventional%20knowledge%2C%20we%20study%20this%20problem%20using%20tools%20from%20representation%0Alearning%20and%20group%20theory.%20We%20propose%20methods%20enabling%20an%20agent%20acting%20upon%20the%0Aworld%20to%20learn%20internal%20representations%20of%20sensory%20information%20that%20are%0Aconsistent%20with%20actions%20that%20modify%20it.%20We%20use%20an%20autoencoder%20equipped%20with%20a%0Agroup%20representation%20acting%20on%20its%20latent%20space%2C%20trained%20using%20an%0Aequivariance-derived%20loss%20in%20order%20to%20enforce%20a%20suitable%20homomorphism%20property%0Aon%20the%20group%20representation.%20In%20contrast%20to%20existing%20work%2C%20our%20approach%20does%0Anot%20require%20prior%20knowledge%20of%20the%20group%20and%20does%20not%20restrict%20the%20set%20of%0Aactions%20the%20agent%20can%20perform.%20We%20motivate%20our%20method%20theoretically%2C%20and%20show%0Aempirically%20that%20it%20can%20learn%20a%20group%20representation%20of%20the%20actions%2C%20thereby%0Acapturing%20the%20structure%20of%20the%20set%20of%20transformations%20applied%20to%20the%0Aenvironment.%20We%20further%20show%20that%20this%20allows%20agents%20to%20predict%20the%20effect%20of%0Asequences%20of%20future%20actions%20with%20improved%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.12067v3&entry.124074799=Read"},
{"title": "SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and\n  Benchmark", "author": "Zhengdi Yu and Shaoli Huang and Yongkang Cheng and Tolga Birdal", "abstract": "  We present SignAvatars, the first large-scale, multi-prompt 3D sign language\n(SL) motion dataset designed to bridge the communication gap for Deaf and\nhard-of-hearing individuals. While there has been an exponentially growing\nnumber of research regarding digital communication, the majority of existing\ncommunication technologies primarily cater to spoken or written languages,\ninstead of SL, the essential communication method for Deaf and hard-of-hearing\ncommunities. Existing SL datasets, dictionaries, and sign language production\n(SLP) methods are typically limited to 2D as annotating 3D models and avatars\nfor SL is usually an entirely manual and labor-intensive process conducted by\nSL experts, often resulting in unnatural avatars. In response to these\nchallenges, we compile and curate the SignAvatars dataset, which comprises\n70,000 videos from 153 signers, totaling 8.34 million frames, covering both\nisolated signs and continuous, co-articulated signs, with multiple prompts\nincluding HamNoSys, spoken language, and words. To yield 3D holistic\nannotations, including meshes and biomechanically-valid poses of body, hands,\nand face, as well as 2D and 3D keypoints, we introduce an automated annotation\npipeline operating on our large corpus of SL videos. SignAvatars facilitates\nvarious tasks such as 3D sign language recognition (SLR) and the novel 3D SL\nproduction (SLP) from diverse inputs like text scripts, individual words, and\nHamNoSys notation. Hence, to evaluate the potential of SignAvatars, we further\npropose a unified benchmark of 3D SL holistic motion production. We believe\nthat this work is a significant step forward towards bringing the digital world\nto the Deaf and hard-of-hearing communities as well as people interacting with\nthem.\n", "link": "http://arxiv.org/abs/2310.20436v3", "date": "2024-07-02", "relevancy": 2.6842, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5588}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5259}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SignAvatars%3A%20A%20Large-scale%203D%20Sign%20Language%20Holistic%20Motion%20Dataset%20and%0A%20%20Benchmark&body=Title%3A%20SignAvatars%3A%20A%20Large-scale%203D%20Sign%20Language%20Holistic%20Motion%20Dataset%20and%0A%20%20Benchmark%0AAuthor%3A%20Zhengdi%20Yu%20and%20Shaoli%20Huang%20and%20Yongkang%20Cheng%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20We%20present%20SignAvatars%2C%20the%20first%20large-scale%2C%20multi-prompt%203D%20sign%20language%0A%28SL%29%20motion%20dataset%20designed%20to%20bridge%20the%20communication%20gap%20for%20Deaf%20and%0Ahard-of-hearing%20individuals.%20While%20there%20has%20been%20an%20exponentially%20growing%0Anumber%20of%20research%20regarding%20digital%20communication%2C%20the%20majority%20of%20existing%0Acommunication%20technologies%20primarily%20cater%20to%20spoken%20or%20written%20languages%2C%0Ainstead%20of%20SL%2C%20the%20essential%20communication%20method%20for%20Deaf%20and%20hard-of-hearing%0Acommunities.%20Existing%20SL%20datasets%2C%20dictionaries%2C%20and%20sign%20language%20production%0A%28SLP%29%20methods%20are%20typically%20limited%20to%202D%20as%20annotating%203D%20models%20and%20avatars%0Afor%20SL%20is%20usually%20an%20entirely%20manual%20and%20labor-intensive%20process%20conducted%20by%0ASL%20experts%2C%20often%20resulting%20in%20unnatural%20avatars.%20In%20response%20to%20these%0Achallenges%2C%20we%20compile%20and%20curate%20the%20SignAvatars%20dataset%2C%20which%20comprises%0A70%2C000%20videos%20from%20153%20signers%2C%20totaling%208.34%20million%20frames%2C%20covering%20both%0Aisolated%20signs%20and%20continuous%2C%20co-articulated%20signs%2C%20with%20multiple%20prompts%0Aincluding%20HamNoSys%2C%20spoken%20language%2C%20and%20words.%20To%20yield%203D%20holistic%0Aannotations%2C%20including%20meshes%20and%20biomechanically-valid%20poses%20of%20body%2C%20hands%2C%0Aand%20face%2C%20as%20well%20as%202D%20and%203D%20keypoints%2C%20we%20introduce%20an%20automated%20annotation%0Apipeline%20operating%20on%20our%20large%20corpus%20of%20SL%20videos.%20SignAvatars%20facilitates%0Avarious%20tasks%20such%20as%203D%20sign%20language%20recognition%20%28SLR%29%20and%20the%20novel%203D%20SL%0Aproduction%20%28SLP%29%20from%20diverse%20inputs%20like%20text%20scripts%2C%20individual%20words%2C%20and%0AHamNoSys%20notation.%20Hence%2C%20to%20evaluate%20the%20potential%20of%20SignAvatars%2C%20we%20further%0Apropose%20a%20unified%20benchmark%20of%203D%20SL%20holistic%20motion%20production.%20We%20believe%0Athat%20this%20work%20is%20a%20significant%20step%20forward%20towards%20bringing%20the%20digital%20world%0Ato%20the%20Deaf%20and%20hard-of-hearing%20communities%20as%20well%20as%20people%20interacting%20with%0Athem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20436v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignAvatars%253A%2520A%2520Large-scale%25203D%2520Sign%2520Language%2520Holistic%2520Motion%2520Dataset%2520and%250A%2520%2520Benchmark%26entry.906535625%3DZhengdi%2520Yu%2520and%2520Shaoli%2520Huang%2520and%2520Yongkang%2520Cheng%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520We%2520present%2520SignAvatars%252C%2520the%2520first%2520large-scale%252C%2520multi-prompt%25203D%2520sign%2520language%250A%2528SL%2529%2520motion%2520dataset%2520designed%2520to%2520bridge%2520the%2520communication%2520gap%2520for%2520Deaf%2520and%250Ahard-of-hearing%2520individuals.%2520While%2520there%2520has%2520been%2520an%2520exponentially%2520growing%250Anumber%2520of%2520research%2520regarding%2520digital%2520communication%252C%2520the%2520majority%2520of%2520existing%250Acommunication%2520technologies%2520primarily%2520cater%2520to%2520spoken%2520or%2520written%2520languages%252C%250Ainstead%2520of%2520SL%252C%2520the%2520essential%2520communication%2520method%2520for%2520Deaf%2520and%2520hard-of-hearing%250Acommunities.%2520Existing%2520SL%2520datasets%252C%2520dictionaries%252C%2520and%2520sign%2520language%2520production%250A%2528SLP%2529%2520methods%2520are%2520typically%2520limited%2520to%25202D%2520as%2520annotating%25203D%2520models%2520and%2520avatars%250Afor%2520SL%2520is%2520usually%2520an%2520entirely%2520manual%2520and%2520labor-intensive%2520process%2520conducted%2520by%250ASL%2520experts%252C%2520often%2520resulting%2520in%2520unnatural%2520avatars.%2520In%2520response%2520to%2520these%250Achallenges%252C%2520we%2520compile%2520and%2520curate%2520the%2520SignAvatars%2520dataset%252C%2520which%2520comprises%250A70%252C000%2520videos%2520from%2520153%2520signers%252C%2520totaling%25208.34%2520million%2520frames%252C%2520covering%2520both%250Aisolated%2520signs%2520and%2520continuous%252C%2520co-articulated%2520signs%252C%2520with%2520multiple%2520prompts%250Aincluding%2520HamNoSys%252C%2520spoken%2520language%252C%2520and%2520words.%2520To%2520yield%25203D%2520holistic%250Aannotations%252C%2520including%2520meshes%2520and%2520biomechanically-valid%2520poses%2520of%2520body%252C%2520hands%252C%250Aand%2520face%252C%2520as%2520well%2520as%25202D%2520and%25203D%2520keypoints%252C%2520we%2520introduce%2520an%2520automated%2520annotation%250Apipeline%2520operating%2520on%2520our%2520large%2520corpus%2520of%2520SL%2520videos.%2520SignAvatars%2520facilitates%250Avarious%2520tasks%2520such%2520as%25203D%2520sign%2520language%2520recognition%2520%2528SLR%2529%2520and%2520the%2520novel%25203D%2520SL%250Aproduction%2520%2528SLP%2529%2520from%2520diverse%2520inputs%2520like%2520text%2520scripts%252C%2520individual%2520words%252C%2520and%250AHamNoSys%2520notation.%2520Hence%252C%2520to%2520evaluate%2520the%2520potential%2520of%2520SignAvatars%252C%2520we%2520further%250Apropose%2520a%2520unified%2520benchmark%2520of%25203D%2520SL%2520holistic%2520motion%2520production.%2520We%2520believe%250Athat%2520this%2520work%2520is%2520a%2520significant%2520step%2520forward%2520towards%2520bringing%2520the%2520digital%2520world%250Ato%2520the%2520Deaf%2520and%2520hard-of-hearing%2520communities%2520as%2520well%2520as%2520people%2520interacting%2520with%250Athem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.20436v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SignAvatars%3A%20A%20Large-scale%203D%20Sign%20Language%20Holistic%20Motion%20Dataset%20and%0A%20%20Benchmark&entry.906535625=Zhengdi%20Yu%20and%20Shaoli%20Huang%20and%20Yongkang%20Cheng%20and%20Tolga%20Birdal&entry.1292438233=%20%20We%20present%20SignAvatars%2C%20the%20first%20large-scale%2C%20multi-prompt%203D%20sign%20language%0A%28SL%29%20motion%20dataset%20designed%20to%20bridge%20the%20communication%20gap%20for%20Deaf%20and%0Ahard-of-hearing%20individuals.%20While%20there%20has%20been%20an%20exponentially%20growing%0Anumber%20of%20research%20regarding%20digital%20communication%2C%20the%20majority%20of%20existing%0Acommunication%20technologies%20primarily%20cater%20to%20spoken%20or%20written%20languages%2C%0Ainstead%20of%20SL%2C%20the%20essential%20communication%20method%20for%20Deaf%20and%20hard-of-hearing%0Acommunities.%20Existing%20SL%20datasets%2C%20dictionaries%2C%20and%20sign%20language%20production%0A%28SLP%29%20methods%20are%20typically%20limited%20to%202D%20as%20annotating%203D%20models%20and%20avatars%0Afor%20SL%20is%20usually%20an%20entirely%20manual%20and%20labor-intensive%20process%20conducted%20by%0ASL%20experts%2C%20often%20resulting%20in%20unnatural%20avatars.%20In%20response%20to%20these%0Achallenges%2C%20we%20compile%20and%20curate%20the%20SignAvatars%20dataset%2C%20which%20comprises%0A70%2C000%20videos%20from%20153%20signers%2C%20totaling%208.34%20million%20frames%2C%20covering%20both%0Aisolated%20signs%20and%20continuous%2C%20co-articulated%20signs%2C%20with%20multiple%20prompts%0Aincluding%20HamNoSys%2C%20spoken%20language%2C%20and%20words.%20To%20yield%203D%20holistic%0Aannotations%2C%20including%20meshes%20and%20biomechanically-valid%20poses%20of%20body%2C%20hands%2C%0Aand%20face%2C%20as%20well%20as%202D%20and%203D%20keypoints%2C%20we%20introduce%20an%20automated%20annotation%0Apipeline%20operating%20on%20our%20large%20corpus%20of%20SL%20videos.%20SignAvatars%20facilitates%0Avarious%20tasks%20such%20as%203D%20sign%20language%20recognition%20%28SLR%29%20and%20the%20novel%203D%20SL%0Aproduction%20%28SLP%29%20from%20diverse%20inputs%20like%20text%20scripts%2C%20individual%20words%2C%20and%0AHamNoSys%20notation.%20Hence%2C%20to%20evaluate%20the%20potential%20of%20SignAvatars%2C%20we%20further%0Apropose%20a%20unified%20benchmark%20of%203D%20SL%20holistic%20motion%20production.%20We%20believe%0Athat%20this%20work%20is%20a%20significant%20step%20forward%20towards%20bringing%20the%20digital%20world%0Ato%20the%20Deaf%20and%20hard-of-hearing%20communities%20as%20well%20as%20people%20interacting%20with%0Athem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20436v3&entry.124074799=Read"},
{"title": "AXIAL: Attention-based eXplainability for Interpretable Alzheimer's\n  Localized Diagnosis using 2D CNNs on 3D MRI brain scans", "author": "Gabriele Lozupone and Alessandro Bria and Francesco Fontanella and Claudio De Stefano", "abstract": "  This study presents an innovative method for Alzheimer's disease diagnosis\nusing 3D MRI designed to enhance the explainability of model decisions. Our\napproach adopts a soft attention mechanism, enabling 2D CNNs to extract\nvolumetric representations. At the same time, the importance of each slice in\ndecision-making is learned, allowing the generation of a voxel-level attention\nmap to produces an explainable MRI. To test our method and ensure the\nreproducibility of our results, we chose a standardized collection of MRI data\nfrom the Alzheimer's Disease Neuroimaging Initiative (ADNI). On this dataset,\nour method significantly outperforms state-of-the-art methods in (i)\ndistinguishing AD from cognitive normal (CN) with an accuracy of 0.856 and\nMatthew's correlation coefficient (MCC) of 0.712, representing improvements of\n2.4\\% and 5.3\\% respectively over the second-best, and (ii) in the prognostic\ntask of discerning stable from progressive mild cognitive impairment (MCI) with\nan accuracy of 0.725 and MCC of 0.443, showing improvements of 10.2\\% and\n20.5\\% respectively over the second-best. We achieved this prognostic result by\nadopting a double transfer learning strategy, which enhanced sensitivity to\nmorphological changes and facilitated early-stage AD detection. With\nvoxel-level precision, our method identified which specific areas are being\npaid attention to, identifying these predominant brain regions: the\n\\emph{hippocampus}, the \\emph{amygdala}, the \\emph{parahippocampal}, and the\n\\emph{inferior lateral ventricles}. All these areas are clinically associated\nwith AD development. Furthermore, our approach consistently found the same\nAD-related areas across different cross-validation folds, proving its\nrobustness and precision in highlighting areas that align closely with known\npathological markers of the disease.\n", "link": "http://arxiv.org/abs/2407.02418v1", "date": "2024-07-02", "relevancy": 2.6471, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5252}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AXIAL%3A%20Attention-based%20eXplainability%20for%20Interpretable%20Alzheimer%27s%0A%20%20Localized%20Diagnosis%20using%202D%20CNNs%20on%203D%20MRI%20brain%20scans&body=Title%3A%20AXIAL%3A%20Attention-based%20eXplainability%20for%20Interpretable%20Alzheimer%27s%0A%20%20Localized%20Diagnosis%20using%202D%20CNNs%20on%203D%20MRI%20brain%20scans%0AAuthor%3A%20Gabriele%20Lozupone%20and%20Alessandro%20Bria%20and%20Francesco%20Fontanella%20and%20Claudio%20De%20Stefano%0AAbstract%3A%20%20%20This%20study%20presents%20an%20innovative%20method%20for%20Alzheimer%27s%20disease%20diagnosis%0Ausing%203D%20MRI%20designed%20to%20enhance%20the%20explainability%20of%20model%20decisions.%20Our%0Aapproach%20adopts%20a%20soft%20attention%20mechanism%2C%20enabling%202D%20CNNs%20to%20extract%0Avolumetric%20representations.%20At%20the%20same%20time%2C%20the%20importance%20of%20each%20slice%20in%0Adecision-making%20is%20learned%2C%20allowing%20the%20generation%20of%20a%20voxel-level%20attention%0Amap%20to%20produces%20an%20explainable%20MRI.%20To%20test%20our%20method%20and%20ensure%20the%0Areproducibility%20of%20our%20results%2C%20we%20chose%20a%20standardized%20collection%20of%20MRI%20data%0Afrom%20the%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29.%20On%20this%20dataset%2C%0Aour%20method%20significantly%20outperforms%20state-of-the-art%20methods%20in%20%28i%29%0Adistinguishing%20AD%20from%20cognitive%20normal%20%28CN%29%20with%20an%20accuracy%20of%200.856%20and%0AMatthew%27s%20correlation%20coefficient%20%28MCC%29%20of%200.712%2C%20representing%20improvements%20of%0A2.4%5C%25%20and%205.3%5C%25%20respectively%20over%20the%20second-best%2C%20and%20%28ii%29%20in%20the%20prognostic%0Atask%20of%20discerning%20stable%20from%20progressive%20mild%20cognitive%20impairment%20%28MCI%29%20with%0Aan%20accuracy%20of%200.725%20and%20MCC%20of%200.443%2C%20showing%20improvements%20of%2010.2%5C%25%20and%0A20.5%5C%25%20respectively%20over%20the%20second-best.%20We%20achieved%20this%20prognostic%20result%20by%0Aadopting%20a%20double%20transfer%20learning%20strategy%2C%20which%20enhanced%20sensitivity%20to%0Amorphological%20changes%20and%20facilitated%20early-stage%20AD%20detection.%20With%0Avoxel-level%20precision%2C%20our%20method%20identified%20which%20specific%20areas%20are%20being%0Apaid%20attention%20to%2C%20identifying%20these%20predominant%20brain%20regions%3A%20the%0A%5Cemph%7Bhippocampus%7D%2C%20the%20%5Cemph%7Bamygdala%7D%2C%20the%20%5Cemph%7Bparahippocampal%7D%2C%20and%20the%0A%5Cemph%7Binferior%20lateral%20ventricles%7D.%20All%20these%20areas%20are%20clinically%20associated%0Awith%20AD%20development.%20Furthermore%2C%20our%20approach%20consistently%20found%20the%20same%0AAD-related%20areas%20across%20different%20cross-validation%20folds%2C%20proving%20its%0Arobustness%20and%20precision%20in%20highlighting%20areas%20that%20align%20closely%20with%20known%0Apathological%20markers%20of%20the%20disease.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAXIAL%253A%2520Attention-based%2520eXplainability%2520for%2520Interpretable%2520Alzheimer%2527s%250A%2520%2520Localized%2520Diagnosis%2520using%25202D%2520CNNs%2520on%25203D%2520MRI%2520brain%2520scans%26entry.906535625%3DGabriele%2520Lozupone%2520and%2520Alessandro%2520Bria%2520and%2520Francesco%2520Fontanella%2520and%2520Claudio%2520De%2520Stefano%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520an%2520innovative%2520method%2520for%2520Alzheimer%2527s%2520disease%2520diagnosis%250Ausing%25203D%2520MRI%2520designed%2520to%2520enhance%2520the%2520explainability%2520of%2520model%2520decisions.%2520Our%250Aapproach%2520adopts%2520a%2520soft%2520attention%2520mechanism%252C%2520enabling%25202D%2520CNNs%2520to%2520extract%250Avolumetric%2520representations.%2520At%2520the%2520same%2520time%252C%2520the%2520importance%2520of%2520each%2520slice%2520in%250Adecision-making%2520is%2520learned%252C%2520allowing%2520the%2520generation%2520of%2520a%2520voxel-level%2520attention%250Amap%2520to%2520produces%2520an%2520explainable%2520MRI.%2520To%2520test%2520our%2520method%2520and%2520ensure%2520the%250Areproducibility%2520of%2520our%2520results%252C%2520we%2520chose%2520a%2520standardized%2520collection%2520of%2520MRI%2520data%250Afrom%2520the%2520Alzheimer%2527s%2520Disease%2520Neuroimaging%2520Initiative%2520%2528ADNI%2529.%2520On%2520this%2520dataset%252C%250Aour%2520method%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520%2528i%2529%250Adistinguishing%2520AD%2520from%2520cognitive%2520normal%2520%2528CN%2529%2520with%2520an%2520accuracy%2520of%25200.856%2520and%250AMatthew%2527s%2520correlation%2520coefficient%2520%2528MCC%2529%2520of%25200.712%252C%2520representing%2520improvements%2520of%250A2.4%255C%2525%2520and%25205.3%255C%2525%2520respectively%2520over%2520the%2520second-best%252C%2520and%2520%2528ii%2529%2520in%2520the%2520prognostic%250Atask%2520of%2520discerning%2520stable%2520from%2520progressive%2520mild%2520cognitive%2520impairment%2520%2528MCI%2529%2520with%250Aan%2520accuracy%2520of%25200.725%2520and%2520MCC%2520of%25200.443%252C%2520showing%2520improvements%2520of%252010.2%255C%2525%2520and%250A20.5%255C%2525%2520respectively%2520over%2520the%2520second-best.%2520We%2520achieved%2520this%2520prognostic%2520result%2520by%250Aadopting%2520a%2520double%2520transfer%2520learning%2520strategy%252C%2520which%2520enhanced%2520sensitivity%2520to%250Amorphological%2520changes%2520and%2520facilitated%2520early-stage%2520AD%2520detection.%2520With%250Avoxel-level%2520precision%252C%2520our%2520method%2520identified%2520which%2520specific%2520areas%2520are%2520being%250Apaid%2520attention%2520to%252C%2520identifying%2520these%2520predominant%2520brain%2520regions%253A%2520the%250A%255Cemph%257Bhippocampus%257D%252C%2520the%2520%255Cemph%257Bamygdala%257D%252C%2520the%2520%255Cemph%257Bparahippocampal%257D%252C%2520and%2520the%250A%255Cemph%257Binferior%2520lateral%2520ventricles%257D.%2520All%2520these%2520areas%2520are%2520clinically%2520associated%250Awith%2520AD%2520development.%2520Furthermore%252C%2520our%2520approach%2520consistently%2520found%2520the%2520same%250AAD-related%2520areas%2520across%2520different%2520cross-validation%2520folds%252C%2520proving%2520its%250Arobustness%2520and%2520precision%2520in%2520highlighting%2520areas%2520that%2520align%2520closely%2520with%2520known%250Apathological%2520markers%2520of%2520the%2520disease.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AXIAL%3A%20Attention-based%20eXplainability%20for%20Interpretable%20Alzheimer%27s%0A%20%20Localized%20Diagnosis%20using%202D%20CNNs%20on%203D%20MRI%20brain%20scans&entry.906535625=Gabriele%20Lozupone%20and%20Alessandro%20Bria%20and%20Francesco%20Fontanella%20and%20Claudio%20De%20Stefano&entry.1292438233=%20%20This%20study%20presents%20an%20innovative%20method%20for%20Alzheimer%27s%20disease%20diagnosis%0Ausing%203D%20MRI%20designed%20to%20enhance%20the%20explainability%20of%20model%20decisions.%20Our%0Aapproach%20adopts%20a%20soft%20attention%20mechanism%2C%20enabling%202D%20CNNs%20to%20extract%0Avolumetric%20representations.%20At%20the%20same%20time%2C%20the%20importance%20of%20each%20slice%20in%0Adecision-making%20is%20learned%2C%20allowing%20the%20generation%20of%20a%20voxel-level%20attention%0Amap%20to%20produces%20an%20explainable%20MRI.%20To%20test%20our%20method%20and%20ensure%20the%0Areproducibility%20of%20our%20results%2C%20we%20chose%20a%20standardized%20collection%20of%20MRI%20data%0Afrom%20the%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29.%20On%20this%20dataset%2C%0Aour%20method%20significantly%20outperforms%20state-of-the-art%20methods%20in%20%28i%29%0Adistinguishing%20AD%20from%20cognitive%20normal%20%28CN%29%20with%20an%20accuracy%20of%200.856%20and%0AMatthew%27s%20correlation%20coefficient%20%28MCC%29%20of%200.712%2C%20representing%20improvements%20of%0A2.4%5C%25%20and%205.3%5C%25%20respectively%20over%20the%20second-best%2C%20and%20%28ii%29%20in%20the%20prognostic%0Atask%20of%20discerning%20stable%20from%20progressive%20mild%20cognitive%20impairment%20%28MCI%29%20with%0Aan%20accuracy%20of%200.725%20and%20MCC%20of%200.443%2C%20showing%20improvements%20of%2010.2%5C%25%20and%0A20.5%5C%25%20respectively%20over%20the%20second-best.%20We%20achieved%20this%20prognostic%20result%20by%0Aadopting%20a%20double%20transfer%20learning%20strategy%2C%20which%20enhanced%20sensitivity%20to%0Amorphological%20changes%20and%20facilitated%20early-stage%20AD%20detection.%20With%0Avoxel-level%20precision%2C%20our%20method%20identified%20which%20specific%20areas%20are%20being%0Apaid%20attention%20to%2C%20identifying%20these%20predominant%20brain%20regions%3A%20the%0A%5Cemph%7Bhippocampus%7D%2C%20the%20%5Cemph%7Bamygdala%7D%2C%20the%20%5Cemph%7Bparahippocampal%7D%2C%20and%20the%0A%5Cemph%7Binferior%20lateral%20ventricles%7D.%20All%20these%20areas%20are%20clinically%20associated%0Awith%20AD%20development.%20Furthermore%2C%20our%20approach%20consistently%20found%20the%20same%0AAD-related%20areas%20across%20different%20cross-validation%20folds%2C%20proving%20its%0Arobustness%20and%20precision%20in%20highlighting%20areas%20that%20align%20closely%20with%20known%0Apathological%20markers%20of%20the%20disease.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02418v1&entry.124074799=Read"},
{"title": "OpenSlot: Mixed Open-set Recognition with Object-centric Learning", "author": "Xu Yin and Fei Pan and Guoyuan An and Yuchi Huo and Zixuan Xie and Sung-Eui Yoon", "abstract": "  Existing open-set recognition (OSR) studies typically assume that each image\ncontains only one class label, and the unknown test set (negative) has a\ndisjoint label space from the known test set (positive), a scenario termed\nfull-label shift. This paper introduces the mixed OSR problem, where test\nimages contain multiple class semantics, with known and unknown classes\nco-occurring in negatives, leading to a more challenging super-label shift.\nAddressing the mixed OSR requires classification models to accurately\ndistinguish different class semantics within images and measure their\n\"knowness\". In this study, we propose the OpenSlot framework, built upon\nobject-centric learning. OpenSlot utilizes slot features to represent diverse\nclass semantics and produce class predictions. Through our proposed\nanti-noise-slot (ANS) technique, we mitigate the impact of noise (invalid and\nbackground) slots during classification training, effectively addressing the\nsemantic misalignment between class predictions and the ground truth. We\nconduct extensive experiments with OpenSlot on mixed & conventional OSR\nbenchmarks. Without elaborate designs, OpenSlot not only exceeds existing OSR\nstudies in detecting super-label shifts across single & multi-label mixed OSR\ntasks but also achieves state-of-the-art performance on conventional\nbenchmarks. Remarkably, our method can localize class objects without using\nbounding boxes during training. The competitive performance in open-set object\ndetection demonstrates OpenSlot's ability to explicitly explain label shifts\nand benefits in computational efficiency and generalization.\n", "link": "http://arxiv.org/abs/2407.02386v1", "date": "2024-07-02", "relevancy": 2.6427, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5304}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSlot%3A%20Mixed%20Open-set%20Recognition%20with%20Object-centric%20Learning&body=Title%3A%20OpenSlot%3A%20Mixed%20Open-set%20Recognition%20with%20Object-centric%20Learning%0AAuthor%3A%20Xu%20Yin%20and%20Fei%20Pan%20and%20Guoyuan%20An%20and%20Yuchi%20Huo%20and%20Zixuan%20Xie%20and%20Sung-Eui%20Yoon%0AAbstract%3A%20%20%20Existing%20open-set%20recognition%20%28OSR%29%20studies%20typically%20assume%20that%20each%20image%0Acontains%20only%20one%20class%20label%2C%20and%20the%20unknown%20test%20set%20%28negative%29%20has%20a%0Adisjoint%20label%20space%20from%20the%20known%20test%20set%20%28positive%29%2C%20a%20scenario%20termed%0Afull-label%20shift.%20This%20paper%20introduces%20the%20mixed%20OSR%20problem%2C%20where%20test%0Aimages%20contain%20multiple%20class%20semantics%2C%20with%20known%20and%20unknown%20classes%0Aco-occurring%20in%20negatives%2C%20leading%20to%20a%20more%20challenging%20super-label%20shift.%0AAddressing%20the%20mixed%20OSR%20requires%20classification%20models%20to%20accurately%0Adistinguish%20different%20class%20semantics%20within%20images%20and%20measure%20their%0A%22knowness%22.%20In%20this%20study%2C%20we%20propose%20the%20OpenSlot%20framework%2C%20built%20upon%0Aobject-centric%20learning.%20OpenSlot%20utilizes%20slot%20features%20to%20represent%20diverse%0Aclass%20semantics%20and%20produce%20class%20predictions.%20Through%20our%20proposed%0Aanti-noise-slot%20%28ANS%29%20technique%2C%20we%20mitigate%20the%20impact%20of%20noise%20%28invalid%20and%0Abackground%29%20slots%20during%20classification%20training%2C%20effectively%20addressing%20the%0Asemantic%20misalignment%20between%20class%20predictions%20and%20the%20ground%20truth.%20We%0Aconduct%20extensive%20experiments%20with%20OpenSlot%20on%20mixed%20%26%20conventional%20OSR%0Abenchmarks.%20Without%20elaborate%20designs%2C%20OpenSlot%20not%20only%20exceeds%20existing%20OSR%0Astudies%20in%20detecting%20super-label%20shifts%20across%20single%20%26%20multi-label%20mixed%20OSR%0Atasks%20but%20also%20achieves%20state-of-the-art%20performance%20on%20conventional%0Abenchmarks.%20Remarkably%2C%20our%20method%20can%20localize%20class%20objects%20without%20using%0Abounding%20boxes%20during%20training.%20The%20competitive%20performance%20in%20open-set%20object%0Adetection%20demonstrates%20OpenSlot%27s%20ability%20to%20explicitly%20explain%20label%20shifts%0Aand%20benefits%20in%20computational%20efficiency%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSlot%253A%2520Mixed%2520Open-set%2520Recognition%2520with%2520Object-centric%2520Learning%26entry.906535625%3DXu%2520Yin%2520and%2520Fei%2520Pan%2520and%2520Guoyuan%2520An%2520and%2520Yuchi%2520Huo%2520and%2520Zixuan%2520Xie%2520and%2520Sung-Eui%2520Yoon%26entry.1292438233%3D%2520%2520Existing%2520open-set%2520recognition%2520%2528OSR%2529%2520studies%2520typically%2520assume%2520that%2520each%2520image%250Acontains%2520only%2520one%2520class%2520label%252C%2520and%2520the%2520unknown%2520test%2520set%2520%2528negative%2529%2520has%2520a%250Adisjoint%2520label%2520space%2520from%2520the%2520known%2520test%2520set%2520%2528positive%2529%252C%2520a%2520scenario%2520termed%250Afull-label%2520shift.%2520This%2520paper%2520introduces%2520the%2520mixed%2520OSR%2520problem%252C%2520where%2520test%250Aimages%2520contain%2520multiple%2520class%2520semantics%252C%2520with%2520known%2520and%2520unknown%2520classes%250Aco-occurring%2520in%2520negatives%252C%2520leading%2520to%2520a%2520more%2520challenging%2520super-label%2520shift.%250AAddressing%2520the%2520mixed%2520OSR%2520requires%2520classification%2520models%2520to%2520accurately%250Adistinguish%2520different%2520class%2520semantics%2520within%2520images%2520and%2520measure%2520their%250A%2522knowness%2522.%2520In%2520this%2520study%252C%2520we%2520propose%2520the%2520OpenSlot%2520framework%252C%2520built%2520upon%250Aobject-centric%2520learning.%2520OpenSlot%2520utilizes%2520slot%2520features%2520to%2520represent%2520diverse%250Aclass%2520semantics%2520and%2520produce%2520class%2520predictions.%2520Through%2520our%2520proposed%250Aanti-noise-slot%2520%2528ANS%2529%2520technique%252C%2520we%2520mitigate%2520the%2520impact%2520of%2520noise%2520%2528invalid%2520and%250Abackground%2529%2520slots%2520during%2520classification%2520training%252C%2520effectively%2520addressing%2520the%250Asemantic%2520misalignment%2520between%2520class%2520predictions%2520and%2520the%2520ground%2520truth.%2520We%250Aconduct%2520extensive%2520experiments%2520with%2520OpenSlot%2520on%2520mixed%2520%2526%2520conventional%2520OSR%250Abenchmarks.%2520Without%2520elaborate%2520designs%252C%2520OpenSlot%2520not%2520only%2520exceeds%2520existing%2520OSR%250Astudies%2520in%2520detecting%2520super-label%2520shifts%2520across%2520single%2520%2526%2520multi-label%2520mixed%2520OSR%250Atasks%2520but%2520also%2520achieves%2520state-of-the-art%2520performance%2520on%2520conventional%250Abenchmarks.%2520Remarkably%252C%2520our%2520method%2520can%2520localize%2520class%2520objects%2520without%2520using%250Abounding%2520boxes%2520during%2520training.%2520The%2520competitive%2520performance%2520in%2520open-set%2520object%250Adetection%2520demonstrates%2520OpenSlot%2527s%2520ability%2520to%2520explicitly%2520explain%2520label%2520shifts%250Aand%2520benefits%2520in%2520computational%2520efficiency%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSlot%3A%20Mixed%20Open-set%20Recognition%20with%20Object-centric%20Learning&entry.906535625=Xu%20Yin%20and%20Fei%20Pan%20and%20Guoyuan%20An%20and%20Yuchi%20Huo%20and%20Zixuan%20Xie%20and%20Sung-Eui%20Yoon&entry.1292438233=%20%20Existing%20open-set%20recognition%20%28OSR%29%20studies%20typically%20assume%20that%20each%20image%0Acontains%20only%20one%20class%20label%2C%20and%20the%20unknown%20test%20set%20%28negative%29%20has%20a%0Adisjoint%20label%20space%20from%20the%20known%20test%20set%20%28positive%29%2C%20a%20scenario%20termed%0Afull-label%20shift.%20This%20paper%20introduces%20the%20mixed%20OSR%20problem%2C%20where%20test%0Aimages%20contain%20multiple%20class%20semantics%2C%20with%20known%20and%20unknown%20classes%0Aco-occurring%20in%20negatives%2C%20leading%20to%20a%20more%20challenging%20super-label%20shift.%0AAddressing%20the%20mixed%20OSR%20requires%20classification%20models%20to%20accurately%0Adistinguish%20different%20class%20semantics%20within%20images%20and%20measure%20their%0A%22knowness%22.%20In%20this%20study%2C%20we%20propose%20the%20OpenSlot%20framework%2C%20built%20upon%0Aobject-centric%20learning.%20OpenSlot%20utilizes%20slot%20features%20to%20represent%20diverse%0Aclass%20semantics%20and%20produce%20class%20predictions.%20Through%20our%20proposed%0Aanti-noise-slot%20%28ANS%29%20technique%2C%20we%20mitigate%20the%20impact%20of%20noise%20%28invalid%20and%0Abackground%29%20slots%20during%20classification%20training%2C%20effectively%20addressing%20the%0Asemantic%20misalignment%20between%20class%20predictions%20and%20the%20ground%20truth.%20We%0Aconduct%20extensive%20experiments%20with%20OpenSlot%20on%20mixed%20%26%20conventional%20OSR%0Abenchmarks.%20Without%20elaborate%20designs%2C%20OpenSlot%20not%20only%20exceeds%20existing%20OSR%0Astudies%20in%20detecting%20super-label%20shifts%20across%20single%20%26%20multi-label%20mixed%20OSR%0Atasks%20but%20also%20achieves%20state-of-the-art%20performance%20on%20conventional%0Abenchmarks.%20Remarkably%2C%20our%20method%20can%20localize%20class%20objects%20without%20using%0Abounding%20boxes%20during%20training.%20The%20competitive%20performance%20in%20open-set%20object%0Adetection%20demonstrates%20OpenSlot%27s%20ability%20to%20explicitly%20explain%20label%20shifts%0Aand%20benefits%20in%20computational%20efficiency%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02386v1&entry.124074799=Read"},
{"title": "Characterizing the Interpretability of Attention Maps in Digital\n  Pathology", "author": "Tom\u00e9 Albuquerque and Anil Y\u00fcce and Markus D. Herrmann and Alvaro Gomariz", "abstract": "  Interpreting machine learning model decisions is crucial for high-risk\napplications like healthcare. In digital pathology, large whole slide images\n(WSIs) are decomposed into smaller tiles and tile-derived features are\nprocessed by attention-based multiple instance learning (ABMIL) models to\npredict WSI-level labels. These networks generate tile-specific attention\nweights, which can be visualized as attention maps for interpretability.\nHowever, a standardized evaluation framework for these maps is lacking,\nquestioning their reliability and ability to detect spurious correlations that\ncan mislead models. We herein propose a framework to assess the ability of\nattention networks to attend to relevant features in digital pathology by\ncreating artificial model confounders and using dedicated interpretability\nmetrics. Models are trained and evaluated on data with tile modifications\ncorrelated with WSI labels, enabling the analysis of model sensitivity to\nartificial confounders and the accuracy of attention maps in highlighting them.\nConfounders are introduced either through synthetic tile modifications or\nthrough tile ablations based on their specific image-based features, with the\nlatter being used to assess more clinically relevant scenarios. We also analyze\nthe impact of varying confounder quantities at both the tile and WSI levels.\nOur results show that ABMIL models perform as desired within our framework.\nWhile attention maps generally highlight relevant regions, their robustness is\naffected by the type and number of confounders. Our versatile framework has the\npotential to be used in the evaluation of various methods and the exploration\nof image-based features driving model predictions, which could aid in biomarker\ndiscovery.\n", "link": "http://arxiv.org/abs/2407.02484v1", "date": "2024-07-02", "relevancy": 2.6281, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20the%20Interpretability%20of%20Attention%20Maps%20in%20Digital%0A%20%20Pathology&body=Title%3A%20Characterizing%20the%20Interpretability%20of%20Attention%20Maps%20in%20Digital%0A%20%20Pathology%0AAuthor%3A%20Tom%C3%A9%20Albuquerque%20and%20Anil%20Y%C3%BCce%20and%20Markus%20D.%20Herrmann%20and%20Alvaro%20Gomariz%0AAbstract%3A%20%20%20Interpreting%20machine%20learning%20model%20decisions%20is%20crucial%20for%20high-risk%0Aapplications%20like%20healthcare.%20In%20digital%20pathology%2C%20large%20whole%20slide%20images%0A%28WSIs%29%20are%20decomposed%20into%20smaller%20tiles%20and%20tile-derived%20features%20are%0Aprocessed%20by%20attention-based%20multiple%20instance%20learning%20%28ABMIL%29%20models%20to%0Apredict%20WSI-level%20labels.%20These%20networks%20generate%20tile-specific%20attention%0Aweights%2C%20which%20can%20be%20visualized%20as%20attention%20maps%20for%20interpretability.%0AHowever%2C%20a%20standardized%20evaluation%20framework%20for%20these%20maps%20is%20lacking%2C%0Aquestioning%20their%20reliability%20and%20ability%20to%20detect%20spurious%20correlations%20that%0Acan%20mislead%20models.%20We%20herein%20propose%20a%20framework%20to%20assess%20the%20ability%20of%0Aattention%20networks%20to%20attend%20to%20relevant%20features%20in%20digital%20pathology%20by%0Acreating%20artificial%20model%20confounders%20and%20using%20dedicated%20interpretability%0Ametrics.%20Models%20are%20trained%20and%20evaluated%20on%20data%20with%20tile%20modifications%0Acorrelated%20with%20WSI%20labels%2C%20enabling%20the%20analysis%20of%20model%20sensitivity%20to%0Aartificial%20confounders%20and%20the%20accuracy%20of%20attention%20maps%20in%20highlighting%20them.%0AConfounders%20are%20introduced%20either%20through%20synthetic%20tile%20modifications%20or%0Athrough%20tile%20ablations%20based%20on%20their%20specific%20image-based%20features%2C%20with%20the%0Alatter%20being%20used%20to%20assess%20more%20clinically%20relevant%20scenarios.%20We%20also%20analyze%0Athe%20impact%20of%20varying%20confounder%20quantities%20at%20both%20the%20tile%20and%20WSI%20levels.%0AOur%20results%20show%20that%20ABMIL%20models%20perform%20as%20desired%20within%20our%20framework.%0AWhile%20attention%20maps%20generally%20highlight%20relevant%20regions%2C%20their%20robustness%20is%0Aaffected%20by%20the%20type%20and%20number%20of%20confounders.%20Our%20versatile%20framework%20has%20the%0Apotential%20to%20be%20used%20in%20the%20evaluation%20of%20various%20methods%20and%20the%20exploration%0Aof%20image-based%20features%20driving%20model%20predictions%2C%20which%20could%20aid%20in%20biomarker%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520the%2520Interpretability%2520of%2520Attention%2520Maps%2520in%2520Digital%250A%2520%2520Pathology%26entry.906535625%3DTom%25C3%25A9%2520Albuquerque%2520and%2520Anil%2520Y%25C3%25BCce%2520and%2520Markus%2520D.%2520Herrmann%2520and%2520Alvaro%2520Gomariz%26entry.1292438233%3D%2520%2520Interpreting%2520machine%2520learning%2520model%2520decisions%2520is%2520crucial%2520for%2520high-risk%250Aapplications%2520like%2520healthcare.%2520In%2520digital%2520pathology%252C%2520large%2520whole%2520slide%2520images%250A%2528WSIs%2529%2520are%2520decomposed%2520into%2520smaller%2520tiles%2520and%2520tile-derived%2520features%2520are%250Aprocessed%2520by%2520attention-based%2520multiple%2520instance%2520learning%2520%2528ABMIL%2529%2520models%2520to%250Apredict%2520WSI-level%2520labels.%2520These%2520networks%2520generate%2520tile-specific%2520attention%250Aweights%252C%2520which%2520can%2520be%2520visualized%2520as%2520attention%2520maps%2520for%2520interpretability.%250AHowever%252C%2520a%2520standardized%2520evaluation%2520framework%2520for%2520these%2520maps%2520is%2520lacking%252C%250Aquestioning%2520their%2520reliability%2520and%2520ability%2520to%2520detect%2520spurious%2520correlations%2520that%250Acan%2520mislead%2520models.%2520We%2520herein%2520propose%2520a%2520framework%2520to%2520assess%2520the%2520ability%2520of%250Aattention%2520networks%2520to%2520attend%2520to%2520relevant%2520features%2520in%2520digital%2520pathology%2520by%250Acreating%2520artificial%2520model%2520confounders%2520and%2520using%2520dedicated%2520interpretability%250Ametrics.%2520Models%2520are%2520trained%2520and%2520evaluated%2520on%2520data%2520with%2520tile%2520modifications%250Acorrelated%2520with%2520WSI%2520labels%252C%2520enabling%2520the%2520analysis%2520of%2520model%2520sensitivity%2520to%250Aartificial%2520confounders%2520and%2520the%2520accuracy%2520of%2520attention%2520maps%2520in%2520highlighting%2520them.%250AConfounders%2520are%2520introduced%2520either%2520through%2520synthetic%2520tile%2520modifications%2520or%250Athrough%2520tile%2520ablations%2520based%2520on%2520their%2520specific%2520image-based%2520features%252C%2520with%2520the%250Alatter%2520being%2520used%2520to%2520assess%2520more%2520clinically%2520relevant%2520scenarios.%2520We%2520also%2520analyze%250Athe%2520impact%2520of%2520varying%2520confounder%2520quantities%2520at%2520both%2520the%2520tile%2520and%2520WSI%2520levels.%250AOur%2520results%2520show%2520that%2520ABMIL%2520models%2520perform%2520as%2520desired%2520within%2520our%2520framework.%250AWhile%2520attention%2520maps%2520generally%2520highlight%2520relevant%2520regions%252C%2520their%2520robustness%2520is%250Aaffected%2520by%2520the%2520type%2520and%2520number%2520of%2520confounders.%2520Our%2520versatile%2520framework%2520has%2520the%250Apotential%2520to%2520be%2520used%2520in%2520the%2520evaluation%2520of%2520various%2520methods%2520and%2520the%2520exploration%250Aof%2520image-based%2520features%2520driving%2520model%2520predictions%252C%2520which%2520could%2520aid%2520in%2520biomarker%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20the%20Interpretability%20of%20Attention%20Maps%20in%20Digital%0A%20%20Pathology&entry.906535625=Tom%C3%A9%20Albuquerque%20and%20Anil%20Y%C3%BCce%20and%20Markus%20D.%20Herrmann%20and%20Alvaro%20Gomariz&entry.1292438233=%20%20Interpreting%20machine%20learning%20model%20decisions%20is%20crucial%20for%20high-risk%0Aapplications%20like%20healthcare.%20In%20digital%20pathology%2C%20large%20whole%20slide%20images%0A%28WSIs%29%20are%20decomposed%20into%20smaller%20tiles%20and%20tile-derived%20features%20are%0Aprocessed%20by%20attention-based%20multiple%20instance%20learning%20%28ABMIL%29%20models%20to%0Apredict%20WSI-level%20labels.%20These%20networks%20generate%20tile-specific%20attention%0Aweights%2C%20which%20can%20be%20visualized%20as%20attention%20maps%20for%20interpretability.%0AHowever%2C%20a%20standardized%20evaluation%20framework%20for%20these%20maps%20is%20lacking%2C%0Aquestioning%20their%20reliability%20and%20ability%20to%20detect%20spurious%20correlations%20that%0Acan%20mislead%20models.%20We%20herein%20propose%20a%20framework%20to%20assess%20the%20ability%20of%0Aattention%20networks%20to%20attend%20to%20relevant%20features%20in%20digital%20pathology%20by%0Acreating%20artificial%20model%20confounders%20and%20using%20dedicated%20interpretability%0Ametrics.%20Models%20are%20trained%20and%20evaluated%20on%20data%20with%20tile%20modifications%0Acorrelated%20with%20WSI%20labels%2C%20enabling%20the%20analysis%20of%20model%20sensitivity%20to%0Aartificial%20confounders%20and%20the%20accuracy%20of%20attention%20maps%20in%20highlighting%20them.%0AConfounders%20are%20introduced%20either%20through%20synthetic%20tile%20modifications%20or%0Athrough%20tile%20ablations%20based%20on%20their%20specific%20image-based%20features%2C%20with%20the%0Alatter%20being%20used%20to%20assess%20more%20clinically%20relevant%20scenarios.%20We%20also%20analyze%0Athe%20impact%20of%20varying%20confounder%20quantities%20at%20both%20the%20tile%20and%20WSI%20levels.%0AOur%20results%20show%20that%20ABMIL%20models%20perform%20as%20desired%20within%20our%20framework.%0AWhile%20attention%20maps%20generally%20highlight%20relevant%20regions%2C%20their%20robustness%20is%0Aaffected%20by%20the%20type%20and%20number%20of%20confounders.%20Our%20versatile%20framework%20has%20the%0Apotential%20to%20be%20used%20in%20the%20evaluation%20of%20various%20methods%20and%20the%20exploration%0Aof%20image-based%20features%20driving%20model%20predictions%2C%20which%20could%20aid%20in%20biomarker%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02484v1&entry.124074799=Read"},
{"title": "A reproducible 3D convolutional neural network with dual attention\n  module (3D-DAM) for Alzheimer's disease classification", "author": "Gia Minh Hoang and Youngjoo Lee and Jae Gwan Kim", "abstract": "  Alzheimer's disease is one of the most common types of neurodegenerative\ndisease, characterized by the accumulation of amyloid-beta plaque and tau\ntangles. Recently, deep learning approaches have shown promise in Alzheimer's\ndisease diagnosis. In this study, we propose a reproducible model that utilizes\na 3D convolutional neural network with a dual attention module for Alzheimer's\ndisease classification. We trained the model in the ADNI database and verified\nthe generalizability of our method in two independent datasets (AIBL and\nOASIS1). Our method achieved state-of-the-art classification performance, with\nan accuracy of 91.94% for MCI progression classification and 96.30% for\nAlzheimer's disease classification on the ADNI dataset. Furthermore, the model\ndemonstrated good generalizability, achieving an accuracy of 86.37% on the AIBL\ndataset and 83.42% on the OASIS1 dataset. These results indicate that our\nproposed approach has competitive performance and generalizability when\ncompared to recent studies in the field.\n", "link": "http://arxiv.org/abs/2310.12574v3", "date": "2024-07-02", "relevancy": 2.6219, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5022}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20reproducible%203D%20convolutional%20neural%20network%20with%20dual%20attention%0A%20%20module%20%283D-DAM%29%20for%20Alzheimer%27s%20disease%20classification&body=Title%3A%20A%20reproducible%203D%20convolutional%20neural%20network%20with%20dual%20attention%0A%20%20module%20%283D-DAM%29%20for%20Alzheimer%27s%20disease%20classification%0AAuthor%3A%20Gia%20Minh%20Hoang%20and%20Youngjoo%20Lee%20and%20Jae%20Gwan%20Kim%0AAbstract%3A%20%20%20Alzheimer%27s%20disease%20is%20one%20of%20the%20most%20common%20types%20of%20neurodegenerative%0Adisease%2C%20characterized%20by%20the%20accumulation%20of%20amyloid-beta%20plaque%20and%20tau%0Atangles.%20Recently%2C%20deep%20learning%20approaches%20have%20shown%20promise%20in%20Alzheimer%27s%0Adisease%20diagnosis.%20In%20this%20study%2C%20we%20propose%20a%20reproducible%20model%20that%20utilizes%0Aa%203D%20convolutional%20neural%20network%20with%20a%20dual%20attention%20module%20for%20Alzheimer%27s%0Adisease%20classification.%20We%20trained%20the%20model%20in%20the%20ADNI%20database%20and%20verified%0Athe%20generalizability%20of%20our%20method%20in%20two%20independent%20datasets%20%28AIBL%20and%0AOASIS1%29.%20Our%20method%20achieved%20state-of-the-art%20classification%20performance%2C%20with%0Aan%20accuracy%20of%2091.94%25%20for%20MCI%20progression%20classification%20and%2096.30%25%20for%0AAlzheimer%27s%20disease%20classification%20on%20the%20ADNI%20dataset.%20Furthermore%2C%20the%20model%0Ademonstrated%20good%20generalizability%2C%20achieving%20an%20accuracy%20of%2086.37%25%20on%20the%20AIBL%0Adataset%20and%2083.42%25%20on%20the%20OASIS1%20dataset.%20These%20results%20indicate%20that%20our%0Aproposed%20approach%20has%20competitive%20performance%20and%20generalizability%20when%0Acompared%20to%20recent%20studies%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12574v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520reproducible%25203D%2520convolutional%2520neural%2520network%2520with%2520dual%2520attention%250A%2520%2520module%2520%25283D-DAM%2529%2520for%2520Alzheimer%2527s%2520disease%2520classification%26entry.906535625%3DGia%2520Minh%2520Hoang%2520and%2520Youngjoo%2520Lee%2520and%2520Jae%2520Gwan%2520Kim%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520disease%2520is%2520one%2520of%2520the%2520most%2520common%2520types%2520of%2520neurodegenerative%250Adisease%252C%2520characterized%2520by%2520the%2520accumulation%2520of%2520amyloid-beta%2520plaque%2520and%2520tau%250Atangles.%2520Recently%252C%2520deep%2520learning%2520approaches%2520have%2520shown%2520promise%2520in%2520Alzheimer%2527s%250Adisease%2520diagnosis.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520reproducible%2520model%2520that%2520utilizes%250Aa%25203D%2520convolutional%2520neural%2520network%2520with%2520a%2520dual%2520attention%2520module%2520for%2520Alzheimer%2527s%250Adisease%2520classification.%2520We%2520trained%2520the%2520model%2520in%2520the%2520ADNI%2520database%2520and%2520verified%250Athe%2520generalizability%2520of%2520our%2520method%2520in%2520two%2520independent%2520datasets%2520%2528AIBL%2520and%250AOASIS1%2529.%2520Our%2520method%2520achieved%2520state-of-the-art%2520classification%2520performance%252C%2520with%250Aan%2520accuracy%2520of%252091.94%2525%2520for%2520MCI%2520progression%2520classification%2520and%252096.30%2525%2520for%250AAlzheimer%2527s%2520disease%2520classification%2520on%2520the%2520ADNI%2520dataset.%2520Furthermore%252C%2520the%2520model%250Ademonstrated%2520good%2520generalizability%252C%2520achieving%2520an%2520accuracy%2520of%252086.37%2525%2520on%2520the%2520AIBL%250Adataset%2520and%252083.42%2525%2520on%2520the%2520OASIS1%2520dataset.%2520These%2520results%2520indicate%2520that%2520our%250Aproposed%2520approach%2520has%2520competitive%2520performance%2520and%2520generalizability%2520when%250Acompared%2520to%2520recent%2520studies%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12574v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20reproducible%203D%20convolutional%20neural%20network%20with%20dual%20attention%0A%20%20module%20%283D-DAM%29%20for%20Alzheimer%27s%20disease%20classification&entry.906535625=Gia%20Minh%20Hoang%20and%20Youngjoo%20Lee%20and%20Jae%20Gwan%20Kim&entry.1292438233=%20%20Alzheimer%27s%20disease%20is%20one%20of%20the%20most%20common%20types%20of%20neurodegenerative%0Adisease%2C%20characterized%20by%20the%20accumulation%20of%20amyloid-beta%20plaque%20and%20tau%0Atangles.%20Recently%2C%20deep%20learning%20approaches%20have%20shown%20promise%20in%20Alzheimer%27s%0Adisease%20diagnosis.%20In%20this%20study%2C%20we%20propose%20a%20reproducible%20model%20that%20utilizes%0Aa%203D%20convolutional%20neural%20network%20with%20a%20dual%20attention%20module%20for%20Alzheimer%27s%0Adisease%20classification.%20We%20trained%20the%20model%20in%20the%20ADNI%20database%20and%20verified%0Athe%20generalizability%20of%20our%20method%20in%20two%20independent%20datasets%20%28AIBL%20and%0AOASIS1%29.%20Our%20method%20achieved%20state-of-the-art%20classification%20performance%2C%20with%0Aan%20accuracy%20of%2091.94%25%20for%20MCI%20progression%20classification%20and%2096.30%25%20for%0AAlzheimer%27s%20disease%20classification%20on%20the%20ADNI%20dataset.%20Furthermore%2C%20the%20model%0Ademonstrated%20good%20generalizability%2C%20achieving%20an%20accuracy%20of%2086.37%25%20on%20the%20AIBL%0Adataset%20and%2083.42%25%20on%20the%20OASIS1%20dataset.%20These%20results%20indicate%20that%20our%0Aproposed%20approach%20has%20competitive%20performance%20and%20generalizability%20when%0Acompared%20to%20recent%20studies%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12574v3&entry.124074799=Read"},
{"title": "Face Reconstruction Transfer Attack as Out-of-Distribution\n  Generalization", "author": "Yoon Gyo Jung and Jaewoo Park and Xingbo Dong and Hojin Park and Andrew Beng Jin Teoh and Octavia Camps", "abstract": "  Understanding the vulnerability of face recognition systems to malicious\nattacks is of critical importance. Previous works have focused on\nreconstructing face images that can penetrate a targeted verification system.\nEven in the white-box scenario, however, naively reconstructed images\nmisrepresent the identity information, hence the attacks are easily neutralized\nonce the face system is updated or changed. In this paper, we aim to\nreconstruct face images which are capable of transferring face attacks on\nunseen encoders. We term this problem as Face Reconstruction Transfer Attack\n(FRTA) and show that it can be formulated as an out-of-distribution (OOD)\ngeneralization problem. Inspired by its OOD nature, we propose to solve FRTA by\nAveraged Latent Search and Unsupervised Validation with pseudo target (ALSUV).\nTo strengthen the reconstruction attack on OOD unseen encoders, ALSUV\nreconstructs the face by searching the latent of amortized generator StyleGAN2\nthrough multiple latent optimization, latent optimization trajectory averaging,\nand unsupervised validation with a pseudo target. We demonstrate the efficacy\nand generalization of our method on widely used face datasets, accompanying it\nwith extensive ablation studies and visually, qualitatively, and quantitatively\nanalyses. The source code will be released.\n", "link": "http://arxiv.org/abs/2407.02403v1", "date": "2024-07-02", "relevancy": 2.6039, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5281}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5175}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Reconstruction%20Transfer%20Attack%20as%20Out-of-Distribution%0A%20%20Generalization&body=Title%3A%20Face%20Reconstruction%20Transfer%20Attack%20as%20Out-of-Distribution%0A%20%20Generalization%0AAuthor%3A%20Yoon%20Gyo%20Jung%20and%20Jaewoo%20Park%20and%20Xingbo%20Dong%20and%20Hojin%20Park%20and%20Andrew%20Beng%20Jin%20Teoh%20and%20Octavia%20Camps%0AAbstract%3A%20%20%20Understanding%20the%20vulnerability%20of%20face%20recognition%20systems%20to%20malicious%0Aattacks%20is%20of%20critical%20importance.%20Previous%20works%20have%20focused%20on%0Areconstructing%20face%20images%20that%20can%20penetrate%20a%20targeted%20verification%20system.%0AEven%20in%20the%20white-box%20scenario%2C%20however%2C%20naively%20reconstructed%20images%0Amisrepresent%20the%20identity%20information%2C%20hence%20the%20attacks%20are%20easily%20neutralized%0Aonce%20the%20face%20system%20is%20updated%20or%20changed.%20In%20this%20paper%2C%20we%20aim%20to%0Areconstruct%20face%20images%20which%20are%20capable%20of%20transferring%20face%20attacks%20on%0Aunseen%20encoders.%20We%20term%20this%20problem%20as%20Face%20Reconstruction%20Transfer%20Attack%0A%28FRTA%29%20and%20show%20that%20it%20can%20be%20formulated%20as%20an%20out-of-distribution%20%28OOD%29%0Ageneralization%20problem.%20Inspired%20by%20its%20OOD%20nature%2C%20we%20propose%20to%20solve%20FRTA%20by%0AAveraged%20Latent%20Search%20and%20Unsupervised%20Validation%20with%20pseudo%20target%20%28ALSUV%29.%0ATo%20strengthen%20the%20reconstruction%20attack%20on%20OOD%20unseen%20encoders%2C%20ALSUV%0Areconstructs%20the%20face%20by%20searching%20the%20latent%20of%20amortized%20generator%20StyleGAN2%0Athrough%20multiple%20latent%20optimization%2C%20latent%20optimization%20trajectory%20averaging%2C%0Aand%20unsupervised%20validation%20with%20a%20pseudo%20target.%20We%20demonstrate%20the%20efficacy%0Aand%20generalization%20of%20our%20method%20on%20widely%20used%20face%20datasets%2C%20accompanying%20it%0Awith%20extensive%20ablation%20studies%20and%20visually%2C%20qualitatively%2C%20and%20quantitatively%0Aanalyses.%20The%20source%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Reconstruction%2520Transfer%2520Attack%2520as%2520Out-of-Distribution%250A%2520%2520Generalization%26entry.906535625%3DYoon%2520Gyo%2520Jung%2520and%2520Jaewoo%2520Park%2520and%2520Xingbo%2520Dong%2520and%2520Hojin%2520Park%2520and%2520Andrew%2520Beng%2520Jin%2520Teoh%2520and%2520Octavia%2520Camps%26entry.1292438233%3D%2520%2520Understanding%2520the%2520vulnerability%2520of%2520face%2520recognition%2520systems%2520to%2520malicious%250Aattacks%2520is%2520of%2520critical%2520importance.%2520Previous%2520works%2520have%2520focused%2520on%250Areconstructing%2520face%2520images%2520that%2520can%2520penetrate%2520a%2520targeted%2520verification%2520system.%250AEven%2520in%2520the%2520white-box%2520scenario%252C%2520however%252C%2520naively%2520reconstructed%2520images%250Amisrepresent%2520the%2520identity%2520information%252C%2520hence%2520the%2520attacks%2520are%2520easily%2520neutralized%250Aonce%2520the%2520face%2520system%2520is%2520updated%2520or%2520changed.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Areconstruct%2520face%2520images%2520which%2520are%2520capable%2520of%2520transferring%2520face%2520attacks%2520on%250Aunseen%2520encoders.%2520We%2520term%2520this%2520problem%2520as%2520Face%2520Reconstruction%2520Transfer%2520Attack%250A%2528FRTA%2529%2520and%2520show%2520that%2520it%2520can%2520be%2520formulated%2520as%2520an%2520out-of-distribution%2520%2528OOD%2529%250Ageneralization%2520problem.%2520Inspired%2520by%2520its%2520OOD%2520nature%252C%2520we%2520propose%2520to%2520solve%2520FRTA%2520by%250AAveraged%2520Latent%2520Search%2520and%2520Unsupervised%2520Validation%2520with%2520pseudo%2520target%2520%2528ALSUV%2529.%250ATo%2520strengthen%2520the%2520reconstruction%2520attack%2520on%2520OOD%2520unseen%2520encoders%252C%2520ALSUV%250Areconstructs%2520the%2520face%2520by%2520searching%2520the%2520latent%2520of%2520amortized%2520generator%2520StyleGAN2%250Athrough%2520multiple%2520latent%2520optimization%252C%2520latent%2520optimization%2520trajectory%2520averaging%252C%250Aand%2520unsupervised%2520validation%2520with%2520a%2520pseudo%2520target.%2520We%2520demonstrate%2520the%2520efficacy%250Aand%2520generalization%2520of%2520our%2520method%2520on%2520widely%2520used%2520face%2520datasets%252C%2520accompanying%2520it%250Awith%2520extensive%2520ablation%2520studies%2520and%2520visually%252C%2520qualitatively%252C%2520and%2520quantitatively%250Aanalyses.%2520The%2520source%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Reconstruction%20Transfer%20Attack%20as%20Out-of-Distribution%0A%20%20Generalization&entry.906535625=Yoon%20Gyo%20Jung%20and%20Jaewoo%20Park%20and%20Xingbo%20Dong%20and%20Hojin%20Park%20and%20Andrew%20Beng%20Jin%20Teoh%20and%20Octavia%20Camps&entry.1292438233=%20%20Understanding%20the%20vulnerability%20of%20face%20recognition%20systems%20to%20malicious%0Aattacks%20is%20of%20critical%20importance.%20Previous%20works%20have%20focused%20on%0Areconstructing%20face%20images%20that%20can%20penetrate%20a%20targeted%20verification%20system.%0AEven%20in%20the%20white-box%20scenario%2C%20however%2C%20naively%20reconstructed%20images%0Amisrepresent%20the%20identity%20information%2C%20hence%20the%20attacks%20are%20easily%20neutralized%0Aonce%20the%20face%20system%20is%20updated%20or%20changed.%20In%20this%20paper%2C%20we%20aim%20to%0Areconstruct%20face%20images%20which%20are%20capable%20of%20transferring%20face%20attacks%20on%0Aunseen%20encoders.%20We%20term%20this%20problem%20as%20Face%20Reconstruction%20Transfer%20Attack%0A%28FRTA%29%20and%20show%20that%20it%20can%20be%20formulated%20as%20an%20out-of-distribution%20%28OOD%29%0Ageneralization%20problem.%20Inspired%20by%20its%20OOD%20nature%2C%20we%20propose%20to%20solve%20FRTA%20by%0AAveraged%20Latent%20Search%20and%20Unsupervised%20Validation%20with%20pseudo%20target%20%28ALSUV%29.%0ATo%20strengthen%20the%20reconstruction%20attack%20on%20OOD%20unseen%20encoders%2C%20ALSUV%0Areconstructs%20the%20face%20by%20searching%20the%20latent%20of%20amortized%20generator%20StyleGAN2%0Athrough%20multiple%20latent%20optimization%2C%20latent%20optimization%20trajectory%20averaging%2C%0Aand%20unsupervised%20validation%20with%20a%20pseudo%20target.%20We%20demonstrate%20the%20efficacy%0Aand%20generalization%20of%20our%20method%20on%20widely%20used%20face%20datasets%2C%20accompanying%20it%0Awith%20extensive%20ablation%20studies%20and%20visually%2C%20qualitatively%2C%20and%20quantitatively%0Aanalyses.%20The%20source%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02403v1&entry.124074799=Read"},
{"title": "Hybrid Feature Collaborative Reconstruction Network for Few-Shot\n  Fine-Grained Image Classification", "author": "Shulei Qiu and Wanqi Yang and Ming Yang", "abstract": "  Our research focuses on few-shot fine-grained image classification, which\nfaces two major challenges: appearance similarity of fine-grained objects and\nlimited number of samples. To preserve the appearance details of images,\ntraditional feature reconstruction networks usually enhance the representation\nability of key features by spatial feature reconstruction and minimizing the\nreconstruction error. However, we find that relying solely on a single type of\nfeature is insufficient for accurately capturing inter-class differences of\nfine-grained objects in scenarios with limited samples. In contrast, the\nintroduction of channel features provides additional information dimensions,\naiding in better understanding and distinguishing the inter-class differences\nof fine-grained objects. Therefore, in this paper, we design a new Hybrid\nFeature Collaborative Reconstruction Network (HFCR-Net) for few-shot\nfine-grained image classification, which includes a Hybrid Feature Fusion\nProcess (HFFP) and a Hybrid Feature Reconstruction Process (HFRP). In HFRP, we\nfuse the channel features and the spatial features. Through dynamic weight\nadjustment, we aggregate the spatial dependencies between arbitrary two\npositions and the correlations between different channels of each image to\nincrease the inter-class differences. Additionally, we introduce the\nreconstruction of channel dimension in HFRP. Through the collaborative\nreconstruction of channel dimension and spatial dimension, the inter-class\ndifferences are further increased in the process of support-to-query\nreconstruction, while the intra-class differences are reduced in the process of\nquery-to-support reconstruction. Ultimately, our extensive experiments on three\nwidely used fine-grained datasets demonstrate the effectiveness and superiority\nof our approach.\n", "link": "http://arxiv.org/abs/2407.02123v1", "date": "2024-07-02", "relevancy": 2.59, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5351}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5146}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Feature%20Collaborative%20Reconstruction%20Network%20for%20Few-Shot%0A%20%20Fine-Grained%20Image%20Classification&body=Title%3A%20Hybrid%20Feature%20Collaborative%20Reconstruction%20Network%20for%20Few-Shot%0A%20%20Fine-Grained%20Image%20Classification%0AAuthor%3A%20Shulei%20Qiu%20and%20Wanqi%20Yang%20and%20Ming%20Yang%0AAbstract%3A%20%20%20Our%20research%20focuses%20on%20few-shot%20fine-grained%20image%20classification%2C%20which%0Afaces%20two%20major%20challenges%3A%20appearance%20similarity%20of%20fine-grained%20objects%20and%0Alimited%20number%20of%20samples.%20To%20preserve%20the%20appearance%20details%20of%20images%2C%0Atraditional%20feature%20reconstruction%20networks%20usually%20enhance%20the%20representation%0Aability%20of%20key%20features%20by%20spatial%20feature%20reconstruction%20and%20minimizing%20the%0Areconstruction%20error.%20However%2C%20we%20find%20that%20relying%20solely%20on%20a%20single%20type%20of%0Afeature%20is%20insufficient%20for%20accurately%20capturing%20inter-class%20differences%20of%0Afine-grained%20objects%20in%20scenarios%20with%20limited%20samples.%20In%20contrast%2C%20the%0Aintroduction%20of%20channel%20features%20provides%20additional%20information%20dimensions%2C%0Aaiding%20in%20better%20understanding%20and%20distinguishing%20the%20inter-class%20differences%0Aof%20fine-grained%20objects.%20Therefore%2C%20in%20this%20paper%2C%20we%20design%20a%20new%20Hybrid%0AFeature%20Collaborative%20Reconstruction%20Network%20%28HFCR-Net%29%20for%20few-shot%0Afine-grained%20image%20classification%2C%20which%20includes%20a%20Hybrid%20Feature%20Fusion%0AProcess%20%28HFFP%29%20and%20a%20Hybrid%20Feature%20Reconstruction%20Process%20%28HFRP%29.%20In%20HFRP%2C%20we%0Afuse%20the%20channel%20features%20and%20the%20spatial%20features.%20Through%20dynamic%20weight%0Aadjustment%2C%20we%20aggregate%20the%20spatial%20dependencies%20between%20arbitrary%20two%0Apositions%20and%20the%20correlations%20between%20different%20channels%20of%20each%20image%20to%0Aincrease%20the%20inter-class%20differences.%20Additionally%2C%20we%20introduce%20the%0Areconstruction%20of%20channel%20dimension%20in%20HFRP.%20Through%20the%20collaborative%0Areconstruction%20of%20channel%20dimension%20and%20spatial%20dimension%2C%20the%20inter-class%0Adifferences%20are%20further%20increased%20in%20the%20process%20of%20support-to-query%0Areconstruction%2C%20while%20the%20intra-class%20differences%20are%20reduced%20in%20the%20process%20of%0Aquery-to-support%20reconstruction.%20Ultimately%2C%20our%20extensive%20experiments%20on%20three%0Awidely%20used%20fine-grained%20datasets%20demonstrate%20the%20effectiveness%20and%20superiority%0Aof%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Feature%2520Collaborative%2520Reconstruction%2520Network%2520for%2520Few-Shot%250A%2520%2520Fine-Grained%2520Image%2520Classification%26entry.906535625%3DShulei%2520Qiu%2520and%2520Wanqi%2520Yang%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520Our%2520research%2520focuses%2520on%2520few-shot%2520fine-grained%2520image%2520classification%252C%2520which%250Afaces%2520two%2520major%2520challenges%253A%2520appearance%2520similarity%2520of%2520fine-grained%2520objects%2520and%250Alimited%2520number%2520of%2520samples.%2520To%2520preserve%2520the%2520appearance%2520details%2520of%2520images%252C%250Atraditional%2520feature%2520reconstruction%2520networks%2520usually%2520enhance%2520the%2520representation%250Aability%2520of%2520key%2520features%2520by%2520spatial%2520feature%2520reconstruction%2520and%2520minimizing%2520the%250Areconstruction%2520error.%2520However%252C%2520we%2520find%2520that%2520relying%2520solely%2520on%2520a%2520single%2520type%2520of%250Afeature%2520is%2520insufficient%2520for%2520accurately%2520capturing%2520inter-class%2520differences%2520of%250Afine-grained%2520objects%2520in%2520scenarios%2520with%2520limited%2520samples.%2520In%2520contrast%252C%2520the%250Aintroduction%2520of%2520channel%2520features%2520provides%2520additional%2520information%2520dimensions%252C%250Aaiding%2520in%2520better%2520understanding%2520and%2520distinguishing%2520the%2520inter-class%2520differences%250Aof%2520fine-grained%2520objects.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520design%2520a%2520new%2520Hybrid%250AFeature%2520Collaborative%2520Reconstruction%2520Network%2520%2528HFCR-Net%2529%2520for%2520few-shot%250Afine-grained%2520image%2520classification%252C%2520which%2520includes%2520a%2520Hybrid%2520Feature%2520Fusion%250AProcess%2520%2528HFFP%2529%2520and%2520a%2520Hybrid%2520Feature%2520Reconstruction%2520Process%2520%2528HFRP%2529.%2520In%2520HFRP%252C%2520we%250Afuse%2520the%2520channel%2520features%2520and%2520the%2520spatial%2520features.%2520Through%2520dynamic%2520weight%250Aadjustment%252C%2520we%2520aggregate%2520the%2520spatial%2520dependencies%2520between%2520arbitrary%2520two%250Apositions%2520and%2520the%2520correlations%2520between%2520different%2520channels%2520of%2520each%2520image%2520to%250Aincrease%2520the%2520inter-class%2520differences.%2520Additionally%252C%2520we%2520introduce%2520the%250Areconstruction%2520of%2520channel%2520dimension%2520in%2520HFRP.%2520Through%2520the%2520collaborative%250Areconstruction%2520of%2520channel%2520dimension%2520and%2520spatial%2520dimension%252C%2520the%2520inter-class%250Adifferences%2520are%2520further%2520increased%2520in%2520the%2520process%2520of%2520support-to-query%250Areconstruction%252C%2520while%2520the%2520intra-class%2520differences%2520are%2520reduced%2520in%2520the%2520process%2520of%250Aquery-to-support%2520reconstruction.%2520Ultimately%252C%2520our%2520extensive%2520experiments%2520on%2520three%250Awidely%2520used%2520fine-grained%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%250Aof%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Feature%20Collaborative%20Reconstruction%20Network%20for%20Few-Shot%0A%20%20Fine-Grained%20Image%20Classification&entry.906535625=Shulei%20Qiu%20and%20Wanqi%20Yang%20and%20Ming%20Yang&entry.1292438233=%20%20Our%20research%20focuses%20on%20few-shot%20fine-grained%20image%20classification%2C%20which%0Afaces%20two%20major%20challenges%3A%20appearance%20similarity%20of%20fine-grained%20objects%20and%0Alimited%20number%20of%20samples.%20To%20preserve%20the%20appearance%20details%20of%20images%2C%0Atraditional%20feature%20reconstruction%20networks%20usually%20enhance%20the%20representation%0Aability%20of%20key%20features%20by%20spatial%20feature%20reconstruction%20and%20minimizing%20the%0Areconstruction%20error.%20However%2C%20we%20find%20that%20relying%20solely%20on%20a%20single%20type%20of%0Afeature%20is%20insufficient%20for%20accurately%20capturing%20inter-class%20differences%20of%0Afine-grained%20objects%20in%20scenarios%20with%20limited%20samples.%20In%20contrast%2C%20the%0Aintroduction%20of%20channel%20features%20provides%20additional%20information%20dimensions%2C%0Aaiding%20in%20better%20understanding%20and%20distinguishing%20the%20inter-class%20differences%0Aof%20fine-grained%20objects.%20Therefore%2C%20in%20this%20paper%2C%20we%20design%20a%20new%20Hybrid%0AFeature%20Collaborative%20Reconstruction%20Network%20%28HFCR-Net%29%20for%20few-shot%0Afine-grained%20image%20classification%2C%20which%20includes%20a%20Hybrid%20Feature%20Fusion%0AProcess%20%28HFFP%29%20and%20a%20Hybrid%20Feature%20Reconstruction%20Process%20%28HFRP%29.%20In%20HFRP%2C%20we%0Afuse%20the%20channel%20features%20and%20the%20spatial%20features.%20Through%20dynamic%20weight%0Aadjustment%2C%20we%20aggregate%20the%20spatial%20dependencies%20between%20arbitrary%20two%0Apositions%20and%20the%20correlations%20between%20different%20channels%20of%20each%20image%20to%0Aincrease%20the%20inter-class%20differences.%20Additionally%2C%20we%20introduce%20the%0Areconstruction%20of%20channel%20dimension%20in%20HFRP.%20Through%20the%20collaborative%0Areconstruction%20of%20channel%20dimension%20and%20spatial%20dimension%2C%20the%20inter-class%0Adifferences%20are%20further%20increased%20in%20the%20process%20of%20support-to-query%0Areconstruction%2C%20while%20the%20intra-class%20differences%20are%20reduced%20in%20the%20process%20of%0Aquery-to-support%20reconstruction.%20Ultimately%2C%20our%20extensive%20experiments%20on%20three%0Awidely%20used%20fine-grained%20datasets%20demonstrate%20the%20effectiveness%20and%20superiority%0Aof%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02123v1&entry.124074799=Read"},
{"title": "Conceptual Codebook Learning for Vision-Language Models", "author": "Yi Zhang and Ke Yu and Siqi Wu and Zhihai He", "abstract": "  In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.\n", "link": "http://arxiv.org/abs/2407.02350v1", "date": "2024-07-02", "relevancy": 2.5533, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Yi%20Zhang%20and%20Ke%20Yu%20and%20Siqi%20Wu%20and%20Zhihai%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Conceptual%20Codebook%20Learning%20%28CoCoLe%29%2C%20a%20novel%0Afine-tuning%20method%20for%20vision-language%20models%20%28VLMs%29%20to%20address%20the%20challenge%0Aof%20improving%20the%20generalization%20capability%20of%20VLMs%20while%20fine-tuning%20them%20on%0Adownstream%20tasks%20in%20a%20few-shot%20setting.%20We%20recognize%20that%20visual%20concepts%2C%20such%0Aas%20textures%2C%20shapes%2C%20and%20colors%20are%20naturally%20transferable%20across%20domains%20and%0Aplay%20a%20crucial%20role%20in%20generalization%20tasks.%20Motivated%20by%20this%20interesting%0Afinding%2C%20we%20learn%20a%20conceptual%20codebook%20consisting%20of%20visual%20concepts%20as%20keys%0Aand%20conceptual%20prompts%20as%20values%2C%20which%20serves%20as%20a%20link%20between%20the%20image%0Aencoder%27s%20outputs%20and%20the%20text%20encoder%27s%20inputs.%20Specifically%2C%20for%20a%20given%0Aimage%2C%20we%20leverage%20the%20codebook%20to%20identify%20the%20most%20relevant%20conceptual%0Aprompts%20associated%20with%20the%20class%20embeddings%20to%20perform%20the%20classification.%0AAdditionally%2C%20we%20incorporate%20a%20handcrafted%20concept%20cache%20as%20a%20regularization%20to%0Aalleviate%20the%20overfitting%20issues%20in%20low-shot%20scenarios.%20We%20observe%20that%20this%0Aconceptual%20codebook%20learning%20method%20is%20able%20to%20achieve%20enhanced%20alignment%0Abetween%20visual%20and%20linguistic%20modalities.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20CoCoLe%20method%20remarkably%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20across%20various%20evaluation%20settings%2C%20including%0Abase-to-new%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%20generalization%0Atasks.%20Detailed%20ablation%20studies%20further%20confirm%20the%20efficacy%20of%20each%20component%0Ain%20CoCoLe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptual%2520Codebook%2520Learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DYi%2520Zhang%2520and%2520Ke%2520Yu%2520and%2520Siqi%2520Wu%2520and%2520Zhihai%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Conceptual%2520Codebook%2520Learning%2520%2528CoCoLe%2529%252C%2520a%2520novel%250Afine-tuning%2520method%2520for%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520address%2520the%2520challenge%250Aof%2520improving%2520the%2520generalization%2520capability%2520of%2520VLMs%2520while%2520fine-tuning%2520them%2520on%250Adownstream%2520tasks%2520in%2520a%2520few-shot%2520setting.%2520We%2520recognize%2520that%2520visual%2520concepts%252C%2520such%250Aas%2520textures%252C%2520shapes%252C%2520and%2520colors%2520are%2520naturally%2520transferable%2520across%2520domains%2520and%250Aplay%2520a%2520crucial%2520role%2520in%2520generalization%2520tasks.%2520Motivated%2520by%2520this%2520interesting%250Afinding%252C%2520we%2520learn%2520a%2520conceptual%2520codebook%2520consisting%2520of%2520visual%2520concepts%2520as%2520keys%250Aand%2520conceptual%2520prompts%2520as%2520values%252C%2520which%2520serves%2520as%2520a%2520link%2520between%2520the%2520image%250Aencoder%2527s%2520outputs%2520and%2520the%2520text%2520encoder%2527s%2520inputs.%2520Specifically%252C%2520for%2520a%2520given%250Aimage%252C%2520we%2520leverage%2520the%2520codebook%2520to%2520identify%2520the%2520most%2520relevant%2520conceptual%250Aprompts%2520associated%2520with%2520the%2520class%2520embeddings%2520to%2520perform%2520the%2520classification.%250AAdditionally%252C%2520we%2520incorporate%2520a%2520handcrafted%2520concept%2520cache%2520as%2520a%2520regularization%2520to%250Aalleviate%2520the%2520overfitting%2520issues%2520in%2520low-shot%2520scenarios.%2520We%2520observe%2520that%2520this%250Aconceptual%2520codebook%2520learning%2520method%2520is%2520able%2520to%2520achieve%2520enhanced%2520alignment%250Abetween%2520visual%2520and%2520linguistic%2520modalities.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520CoCoLe%2520method%2520remarkably%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520methods%2520across%2520various%2520evaluation%2520settings%252C%2520including%250Abase-to-new%2520generalization%252C%2520cross-dataset%2520evaluation%252C%2520and%2520domain%2520generalization%250Atasks.%2520Detailed%2520ablation%2520studies%2520further%2520confirm%2520the%2520efficacy%2520of%2520each%2520component%250Ain%2520CoCoLe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models&entry.906535625=Yi%20Zhang%20and%20Ke%20Yu%20and%20Siqi%20Wu%20and%20Zhihai%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Conceptual%20Codebook%20Learning%20%28CoCoLe%29%2C%20a%20novel%0Afine-tuning%20method%20for%20vision-language%20models%20%28VLMs%29%20to%20address%20the%20challenge%0Aof%20improving%20the%20generalization%20capability%20of%20VLMs%20while%20fine-tuning%20them%20on%0Adownstream%20tasks%20in%20a%20few-shot%20setting.%20We%20recognize%20that%20visual%20concepts%2C%20such%0Aas%20textures%2C%20shapes%2C%20and%20colors%20are%20naturally%20transferable%20across%20domains%20and%0Aplay%20a%20crucial%20role%20in%20generalization%20tasks.%20Motivated%20by%20this%20interesting%0Afinding%2C%20we%20learn%20a%20conceptual%20codebook%20consisting%20of%20visual%20concepts%20as%20keys%0Aand%20conceptual%20prompts%20as%20values%2C%20which%20serves%20as%20a%20link%20between%20the%20image%0Aencoder%27s%20outputs%20and%20the%20text%20encoder%27s%20inputs.%20Specifically%2C%20for%20a%20given%0Aimage%2C%20we%20leverage%20the%20codebook%20to%20identify%20the%20most%20relevant%20conceptual%0Aprompts%20associated%20with%20the%20class%20embeddings%20to%20perform%20the%20classification.%0AAdditionally%2C%20we%20incorporate%20a%20handcrafted%20concept%20cache%20as%20a%20regularization%20to%0Aalleviate%20the%20overfitting%20issues%20in%20low-shot%20scenarios.%20We%20observe%20that%20this%0Aconceptual%20codebook%20learning%20method%20is%20able%20to%20achieve%20enhanced%20alignment%0Abetween%20visual%20and%20linguistic%20modalities.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20CoCoLe%20method%20remarkably%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20across%20various%20evaluation%20settings%2C%20including%0Abase-to-new%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%20generalization%0Atasks.%20Detailed%20ablation%20studies%20further%20confirm%20the%20efficacy%20of%20each%20component%0Ain%20CoCoLe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02350v1&entry.124074799=Read"},
{"title": "Close, But Not There: Boosting Geographic Distance Sensitivity in Visual\n  Place Recognition", "author": "Sergio Izquierdo and Javier Civera", "abstract": "  Visual Place Recognition (VPR) plays a critical role in many localization and\nmapping pipelines. It consists of retrieving the closest sample to a query\nimage, in a certain embedding space, from a database of geotagged references.\nThe image embedding is learned to effectively describe a place despite\nvariations in visual appearance, viewpoint, and geometric changes. In this\nwork, we formulate how limitations in the Geographic Distance Sensitivity of\ncurrent VPR embeddings result in a high probability of incorrectly sorting the\ntop-k retrievals, negatively impacting the recall. In order to address this\nissue in single-stage VPR, we propose a novel mining strategy, CliqueMining,\nthat selects positive and negative examples by sampling cliques from a graph of\nvisually similar images. Our approach boosts the sensitivity of VPR embeddings\nat small distance ranges, significantly improving the state of the art on\nrelevant benchmarks. In particular, we raise recall@1 from 75% to 82% in MSLS\nChallenge, and from 76% to 90% in Nordland. Models and code are available at\nhttps://github.com/serizba/cliquemining.\n", "link": "http://arxiv.org/abs/2407.02422v1", "date": "2024-07-02", "relevancy": 2.5498, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Close%2C%20But%20Not%20There%3A%20Boosting%20Geographic%20Distance%20Sensitivity%20in%20Visual%0A%20%20Place%20Recognition&body=Title%3A%20Close%2C%20But%20Not%20There%3A%20Boosting%20Geographic%20Distance%20Sensitivity%20in%20Visual%0A%20%20Place%20Recognition%0AAuthor%3A%20Sergio%20Izquierdo%20and%20Javier%20Civera%0AAbstract%3A%20%20%20Visual%20Place%20Recognition%20%28VPR%29%20plays%20a%20critical%20role%20in%20many%20localization%20and%0Amapping%20pipelines.%20It%20consists%20of%20retrieving%20the%20closest%20sample%20to%20a%20query%0Aimage%2C%20in%20a%20certain%20embedding%20space%2C%20from%20a%20database%20of%20geotagged%20references.%0AThe%20image%20embedding%20is%20learned%20to%20effectively%20describe%20a%20place%20despite%0Avariations%20in%20visual%20appearance%2C%20viewpoint%2C%20and%20geometric%20changes.%20In%20this%0Awork%2C%20we%20formulate%20how%20limitations%20in%20the%20Geographic%20Distance%20Sensitivity%20of%0Acurrent%20VPR%20embeddings%20result%20in%20a%20high%20probability%20of%20incorrectly%20sorting%20the%0Atop-k%20retrievals%2C%20negatively%20impacting%20the%20recall.%20In%20order%20to%20address%20this%0Aissue%20in%20single-stage%20VPR%2C%20we%20propose%20a%20novel%20mining%20strategy%2C%20CliqueMining%2C%0Athat%20selects%20positive%20and%20negative%20examples%20by%20sampling%20cliques%20from%20a%20graph%20of%0Avisually%20similar%20images.%20Our%20approach%20boosts%20the%20sensitivity%20of%20VPR%20embeddings%0Aat%20small%20distance%20ranges%2C%20significantly%20improving%20the%20state%20of%20the%20art%20on%0Arelevant%20benchmarks.%20In%20particular%2C%20we%20raise%20recall%401%20from%2075%25%20to%2082%25%20in%20MSLS%0AChallenge%2C%20and%20from%2076%25%20to%2090%25%20in%20Nordland.%20Models%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/serizba/cliquemining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClose%252C%2520But%2520Not%2520There%253A%2520Boosting%2520Geographic%2520Distance%2520Sensitivity%2520in%2520Visual%250A%2520%2520Place%2520Recognition%26entry.906535625%3DSergio%2520Izquierdo%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520plays%2520a%2520critical%2520role%2520in%2520many%2520localization%2520and%250Amapping%2520pipelines.%2520It%2520consists%2520of%2520retrieving%2520the%2520closest%2520sample%2520to%2520a%2520query%250Aimage%252C%2520in%2520a%2520certain%2520embedding%2520space%252C%2520from%2520a%2520database%2520of%2520geotagged%2520references.%250AThe%2520image%2520embedding%2520is%2520learned%2520to%2520effectively%2520describe%2520a%2520place%2520despite%250Avariations%2520in%2520visual%2520appearance%252C%2520viewpoint%252C%2520and%2520geometric%2520changes.%2520In%2520this%250Awork%252C%2520we%2520formulate%2520how%2520limitations%2520in%2520the%2520Geographic%2520Distance%2520Sensitivity%2520of%250Acurrent%2520VPR%2520embeddings%2520result%2520in%2520a%2520high%2520probability%2520of%2520incorrectly%2520sorting%2520the%250Atop-k%2520retrievals%252C%2520negatively%2520impacting%2520the%2520recall.%2520In%2520order%2520to%2520address%2520this%250Aissue%2520in%2520single-stage%2520VPR%252C%2520we%2520propose%2520a%2520novel%2520mining%2520strategy%252C%2520CliqueMining%252C%250Athat%2520selects%2520positive%2520and%2520negative%2520examples%2520by%2520sampling%2520cliques%2520from%2520a%2520graph%2520of%250Avisually%2520similar%2520images.%2520Our%2520approach%2520boosts%2520the%2520sensitivity%2520of%2520VPR%2520embeddings%250Aat%2520small%2520distance%2520ranges%252C%2520significantly%2520improving%2520the%2520state%2520of%2520the%2520art%2520on%250Arelevant%2520benchmarks.%2520In%2520particular%252C%2520we%2520raise%2520recall%25401%2520from%252075%2525%2520to%252082%2525%2520in%2520MSLS%250AChallenge%252C%2520and%2520from%252076%2525%2520to%252090%2525%2520in%2520Nordland.%2520Models%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/serizba/cliquemining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Close%2C%20But%20Not%20There%3A%20Boosting%20Geographic%20Distance%20Sensitivity%20in%20Visual%0A%20%20Place%20Recognition&entry.906535625=Sergio%20Izquierdo%20and%20Javier%20Civera&entry.1292438233=%20%20Visual%20Place%20Recognition%20%28VPR%29%20plays%20a%20critical%20role%20in%20many%20localization%20and%0Amapping%20pipelines.%20It%20consists%20of%20retrieving%20the%20closest%20sample%20to%20a%20query%0Aimage%2C%20in%20a%20certain%20embedding%20space%2C%20from%20a%20database%20of%20geotagged%20references.%0AThe%20image%20embedding%20is%20learned%20to%20effectively%20describe%20a%20place%20despite%0Avariations%20in%20visual%20appearance%2C%20viewpoint%2C%20and%20geometric%20changes.%20In%20this%0Awork%2C%20we%20formulate%20how%20limitations%20in%20the%20Geographic%20Distance%20Sensitivity%20of%0Acurrent%20VPR%20embeddings%20result%20in%20a%20high%20probability%20of%20incorrectly%20sorting%20the%0Atop-k%20retrievals%2C%20negatively%20impacting%20the%20recall.%20In%20order%20to%20address%20this%0Aissue%20in%20single-stage%20VPR%2C%20we%20propose%20a%20novel%20mining%20strategy%2C%20CliqueMining%2C%0Athat%20selects%20positive%20and%20negative%20examples%20by%20sampling%20cliques%20from%20a%20graph%20of%0Avisually%20similar%20images.%20Our%20approach%20boosts%20the%20sensitivity%20of%20VPR%20embeddings%0Aat%20small%20distance%20ranges%2C%20significantly%20improving%20the%20state%20of%20the%20art%20on%0Arelevant%20benchmarks.%20In%20particular%2C%20we%20raise%20recall%401%20from%2075%25%20to%2082%25%20in%20MSLS%0AChallenge%2C%20and%20from%2076%25%20to%2090%25%20in%20Nordland.%20Models%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/serizba/cliquemining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02422v1&entry.124074799=Read"},
{"title": "VRBiom: A New Periocular Dataset for Biometric Applications of HMD", "author": "Ketan Kotwal and Ibrahim Ulucan and Gokhan Ozbulak and Janani Selliah and Sebastien Marcel", "abstract": "  With advancements in hardware, high-quality HMD devices are being developed\nby numerous companies, driving increased consumer interest in AR, VR, and MR\napplications. In this work, we present a new dataset, called VRBiom, of\nperiocular videos acquired using a Virtual Reality headset. The VRBiom,\ntargeted at biometric applications, consists of 900 short videos acquired from\n25 individuals recorded in the NIR spectrum. These 10s long videos have been\ncaptured using the internal tracking cameras of Meta Quest Pro at 72 FPS. To\nencompass real-world variations, the dataset includes recordings under three\ngaze conditions: steady, moving, and partially closed eyes. We have also\nensured an equal split of recordings without and with glasses to facilitate the\nanalysis of eye-wear. These videos, characterized by non-frontal views of the\neye and relatively low spatial resolutions (400 x 400), can be instrumental in\nadvancing state-of-the-art research across various biometric applications. The\nVRBiom dataset can be utilized to evaluate, train, or adapt models for\nbiometric use-cases such as iris and/or periocular recognition and associated\nsub-tasks such as detection and semantic segmentation.\n  In addition to data from real individuals, we have included around 1100 PA\nconstructed from 92 PA instruments. These PAIs fall into six categories\nconstructed through combinations of print attacks (real and synthetic\nidentities), fake 3D eyeballs, plastic eyes, and various types of masks and\nmannequins. These PA videos, combined with genuine (bona-fide) data, can be\nutilized to address concerns related to spoofing, which is a significant threat\nif these devices are to be used for authentication.\n  The VRBiom dataset is publicly available for research purposes related to\nbiometric applications only.\n", "link": "http://arxiv.org/abs/2407.02150v1", "date": "2024-07-02", "relevancy": 2.512, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5186}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4943}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRBiom%3A%20A%20New%20Periocular%20Dataset%20for%20Biometric%20Applications%20of%20HMD&body=Title%3A%20VRBiom%3A%20A%20New%20Periocular%20Dataset%20for%20Biometric%20Applications%20of%20HMD%0AAuthor%3A%20Ketan%20Kotwal%20and%20Ibrahim%20Ulucan%20and%20Gokhan%20Ozbulak%20and%20Janani%20Selliah%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20With%20advancements%20in%20hardware%2C%20high-quality%20HMD%20devices%20are%20being%20developed%0Aby%20numerous%20companies%2C%20driving%20increased%20consumer%20interest%20in%20AR%2C%20VR%2C%20and%20MR%0Aapplications.%20In%20this%20work%2C%20we%20present%20a%20new%20dataset%2C%20called%20VRBiom%2C%20of%0Aperiocular%20videos%20acquired%20using%20a%20Virtual%20Reality%20headset.%20The%20VRBiom%2C%0Atargeted%20at%20biometric%20applications%2C%20consists%20of%20900%20short%20videos%20acquired%20from%0A25%20individuals%20recorded%20in%20the%20NIR%20spectrum.%20These%2010s%20long%20videos%20have%20been%0Acaptured%20using%20the%20internal%20tracking%20cameras%20of%20Meta%20Quest%20Pro%20at%2072%20FPS.%20To%0Aencompass%20real-world%20variations%2C%20the%20dataset%20includes%20recordings%20under%20three%0Agaze%20conditions%3A%20steady%2C%20moving%2C%20and%20partially%20closed%20eyes.%20We%20have%20also%0Aensured%20an%20equal%20split%20of%20recordings%20without%20and%20with%20glasses%20to%20facilitate%20the%0Aanalysis%20of%20eye-wear.%20These%20videos%2C%20characterized%20by%20non-frontal%20views%20of%20the%0Aeye%20and%20relatively%20low%20spatial%20resolutions%20%28400%20x%20400%29%2C%20can%20be%20instrumental%20in%0Aadvancing%20state-of-the-art%20research%20across%20various%20biometric%20applications.%20The%0AVRBiom%20dataset%20can%20be%20utilized%20to%20evaluate%2C%20train%2C%20or%20adapt%20models%20for%0Abiometric%20use-cases%20such%20as%20iris%20and/or%20periocular%20recognition%20and%20associated%0Asub-tasks%20such%20as%20detection%20and%20semantic%20segmentation.%0A%20%20In%20addition%20to%20data%20from%20real%20individuals%2C%20we%20have%20included%20around%201100%20PA%0Aconstructed%20from%2092%20PA%20instruments.%20These%20PAIs%20fall%20into%20six%20categories%0Aconstructed%20through%20combinations%20of%20print%20attacks%20%28real%20and%20synthetic%0Aidentities%29%2C%20fake%203D%20eyeballs%2C%20plastic%20eyes%2C%20and%20various%20types%20of%20masks%20and%0Amannequins.%20These%20PA%20videos%2C%20combined%20with%20genuine%20%28bona-fide%29%20data%2C%20can%20be%0Autilized%20to%20address%20concerns%20related%20to%20spoofing%2C%20which%20is%20a%20significant%20threat%0Aif%20these%20devices%20are%20to%20be%20used%20for%20authentication.%0A%20%20The%20VRBiom%20dataset%20is%20publicly%20available%20for%20research%20purposes%20related%20to%0Abiometric%20applications%20only.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRBiom%253A%2520A%2520New%2520Periocular%2520Dataset%2520for%2520Biometric%2520Applications%2520of%2520HMD%26entry.906535625%3DKetan%2520Kotwal%2520and%2520Ibrahim%2520Ulucan%2520and%2520Gokhan%2520Ozbulak%2520and%2520Janani%2520Selliah%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520hardware%252C%2520high-quality%2520HMD%2520devices%2520are%2520being%2520developed%250Aby%2520numerous%2520companies%252C%2520driving%2520increased%2520consumer%2520interest%2520in%2520AR%252C%2520VR%252C%2520and%2520MR%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520dataset%252C%2520called%2520VRBiom%252C%2520of%250Aperiocular%2520videos%2520acquired%2520using%2520a%2520Virtual%2520Reality%2520headset.%2520The%2520VRBiom%252C%250Atargeted%2520at%2520biometric%2520applications%252C%2520consists%2520of%2520900%2520short%2520videos%2520acquired%2520from%250A25%2520individuals%2520recorded%2520in%2520the%2520NIR%2520spectrum.%2520These%252010s%2520long%2520videos%2520have%2520been%250Acaptured%2520using%2520the%2520internal%2520tracking%2520cameras%2520of%2520Meta%2520Quest%2520Pro%2520at%252072%2520FPS.%2520To%250Aencompass%2520real-world%2520variations%252C%2520the%2520dataset%2520includes%2520recordings%2520under%2520three%250Agaze%2520conditions%253A%2520steady%252C%2520moving%252C%2520and%2520partially%2520closed%2520eyes.%2520We%2520have%2520also%250Aensured%2520an%2520equal%2520split%2520of%2520recordings%2520without%2520and%2520with%2520glasses%2520to%2520facilitate%2520the%250Aanalysis%2520of%2520eye-wear.%2520These%2520videos%252C%2520characterized%2520by%2520non-frontal%2520views%2520of%2520the%250Aeye%2520and%2520relatively%2520low%2520spatial%2520resolutions%2520%2528400%2520x%2520400%2529%252C%2520can%2520be%2520instrumental%2520in%250Aadvancing%2520state-of-the-art%2520research%2520across%2520various%2520biometric%2520applications.%2520The%250AVRBiom%2520dataset%2520can%2520be%2520utilized%2520to%2520evaluate%252C%2520train%252C%2520or%2520adapt%2520models%2520for%250Abiometric%2520use-cases%2520such%2520as%2520iris%2520and/or%2520periocular%2520recognition%2520and%2520associated%250Asub-tasks%2520such%2520as%2520detection%2520and%2520semantic%2520segmentation.%250A%2520%2520In%2520addition%2520to%2520data%2520from%2520real%2520individuals%252C%2520we%2520have%2520included%2520around%25201100%2520PA%250Aconstructed%2520from%252092%2520PA%2520instruments.%2520These%2520PAIs%2520fall%2520into%2520six%2520categories%250Aconstructed%2520through%2520combinations%2520of%2520print%2520attacks%2520%2528real%2520and%2520synthetic%250Aidentities%2529%252C%2520fake%25203D%2520eyeballs%252C%2520plastic%2520eyes%252C%2520and%2520various%2520types%2520of%2520masks%2520and%250Amannequins.%2520These%2520PA%2520videos%252C%2520combined%2520with%2520genuine%2520%2528bona-fide%2529%2520data%252C%2520can%2520be%250Autilized%2520to%2520address%2520concerns%2520related%2520to%2520spoofing%252C%2520which%2520is%2520a%2520significant%2520threat%250Aif%2520these%2520devices%2520are%2520to%2520be%2520used%2520for%2520authentication.%250A%2520%2520The%2520VRBiom%2520dataset%2520is%2520publicly%2520available%2520for%2520research%2520purposes%2520related%2520to%250Abiometric%2520applications%2520only.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRBiom%3A%20A%20New%20Periocular%20Dataset%20for%20Biometric%20Applications%20of%20HMD&entry.906535625=Ketan%20Kotwal%20and%20Ibrahim%20Ulucan%20and%20Gokhan%20Ozbulak%20and%20Janani%20Selliah%20and%20Sebastien%20Marcel&entry.1292438233=%20%20With%20advancements%20in%20hardware%2C%20high-quality%20HMD%20devices%20are%20being%20developed%0Aby%20numerous%20companies%2C%20driving%20increased%20consumer%20interest%20in%20AR%2C%20VR%2C%20and%20MR%0Aapplications.%20In%20this%20work%2C%20we%20present%20a%20new%20dataset%2C%20called%20VRBiom%2C%20of%0Aperiocular%20videos%20acquired%20using%20a%20Virtual%20Reality%20headset.%20The%20VRBiom%2C%0Atargeted%20at%20biometric%20applications%2C%20consists%20of%20900%20short%20videos%20acquired%20from%0A25%20individuals%20recorded%20in%20the%20NIR%20spectrum.%20These%2010s%20long%20videos%20have%20been%0Acaptured%20using%20the%20internal%20tracking%20cameras%20of%20Meta%20Quest%20Pro%20at%2072%20FPS.%20To%0Aencompass%20real-world%20variations%2C%20the%20dataset%20includes%20recordings%20under%20three%0Agaze%20conditions%3A%20steady%2C%20moving%2C%20and%20partially%20closed%20eyes.%20We%20have%20also%0Aensured%20an%20equal%20split%20of%20recordings%20without%20and%20with%20glasses%20to%20facilitate%20the%0Aanalysis%20of%20eye-wear.%20These%20videos%2C%20characterized%20by%20non-frontal%20views%20of%20the%0Aeye%20and%20relatively%20low%20spatial%20resolutions%20%28400%20x%20400%29%2C%20can%20be%20instrumental%20in%0Aadvancing%20state-of-the-art%20research%20across%20various%20biometric%20applications.%20The%0AVRBiom%20dataset%20can%20be%20utilized%20to%20evaluate%2C%20train%2C%20or%20adapt%20models%20for%0Abiometric%20use-cases%20such%20as%20iris%20and/or%20periocular%20recognition%20and%20associated%0Asub-tasks%20such%20as%20detection%20and%20semantic%20segmentation.%0A%20%20In%20addition%20to%20data%20from%20real%20individuals%2C%20we%20have%20included%20around%201100%20PA%0Aconstructed%20from%2092%20PA%20instruments.%20These%20PAIs%20fall%20into%20six%20categories%0Aconstructed%20through%20combinations%20of%20print%20attacks%20%28real%20and%20synthetic%0Aidentities%29%2C%20fake%203D%20eyeballs%2C%20plastic%20eyes%2C%20and%20various%20types%20of%20masks%20and%0Amannequins.%20These%20PA%20videos%2C%20combined%20with%20genuine%20%28bona-fide%29%20data%2C%20can%20be%0Autilized%20to%20address%20concerns%20related%20to%20spoofing%2C%20which%20is%20a%20significant%20threat%0Aif%20these%20devices%20are%20to%20be%20used%20for%20authentication.%0A%20%20The%20VRBiom%20dataset%20is%20publicly%20available%20for%20research%20purposes%20related%20to%0Abiometric%20applications%20only.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02150v1&entry.124074799=Read"},
{"title": "RETINA: a hardware-in-the-loop optical facility with reduced optical\n  aberrations", "author": "Paolo Panicucci and Fabio Ornati and Francesco Topputo", "abstract": "  The increasing interest in spacecraft autonomy and the complex tasks to be\naccomplished by the spacecraft raise the need for a trustworthy approach to\nperform Verification & Validation of Guidance, Navigation, and Control\nalgorithms. In the context of autonomous operations, vision-based navigation\nalgorithms have established themselves as effective solutions to determine the\nspacecraft state in orbit with low-cost and versatile sensors. Nevertheless,\ndetailed testing must be performed on ground to understand the algorithm's\nrobustness and performance on flight hardware. Given the impossibility of\ntesting directly on orbit these algorithms, a dedicated simulation framework\nmust be developed to emulate the orbital environment in a laboratory setup.\nThis paper presents the design of a low-aberration optical facility called\nRETINA to perform this task. RETINA is designed to accommodate cameras with\ndifferent characteristics (e.g., sensor size and focal length) while ensuring\nthe correct stimulation of the camera detector. A preliminary design is\nperformed to identify the range of possible components to be used in the\nfacility according to the facility requirements. Then, a detailed optical\ndesign is performed in Zemax OpticStudio to optimize the number and\ncharacteristics of the lenses composing the facility's optical systems. The\nfinal design is compared against the preliminary design to show the superiority\nof the optical performance achieved with this approach. This work presents also\na calibration procedure to estimate the misalignment and the centering errors\nin the facility. These estimated parameters are used in a dedicated\ncompensation algorithm, enabling the stimulation of the camera at tens of\narcseconds of precision. Finally, two different applications are presented to\nshow the versatility of RETINA in accommodating different cameras and in\nsimulating different mission scenarios.\n", "link": "http://arxiv.org/abs/2407.02172v1", "date": "2024-07-02", "relevancy": 2.5015, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5114}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4981}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RETINA%3A%20a%20hardware-in-the-loop%20optical%20facility%20with%20reduced%20optical%0A%20%20aberrations&body=Title%3A%20RETINA%3A%20a%20hardware-in-the-loop%20optical%20facility%20with%20reduced%20optical%0A%20%20aberrations%0AAuthor%3A%20Paolo%20Panicucci%20and%20Fabio%20Ornati%20and%20Francesco%20Topputo%0AAbstract%3A%20%20%20The%20increasing%20interest%20in%20spacecraft%20autonomy%20and%20the%20complex%20tasks%20to%20be%0Aaccomplished%20by%20the%20spacecraft%20raise%20the%20need%20for%20a%20trustworthy%20approach%20to%0Aperform%20Verification%20%26%20Validation%20of%20Guidance%2C%20Navigation%2C%20and%20Control%0Aalgorithms.%20In%20the%20context%20of%20autonomous%20operations%2C%20vision-based%20navigation%0Aalgorithms%20have%20established%20themselves%20as%20effective%20solutions%20to%20determine%20the%0Aspacecraft%20state%20in%20orbit%20with%20low-cost%20and%20versatile%20sensors.%20Nevertheless%2C%0Adetailed%20testing%20must%20be%20performed%20on%20ground%20to%20understand%20the%20algorithm%27s%0Arobustness%20and%20performance%20on%20flight%20hardware.%20Given%20the%20impossibility%20of%0Atesting%20directly%20on%20orbit%20these%20algorithms%2C%20a%20dedicated%20simulation%20framework%0Amust%20be%20developed%20to%20emulate%20the%20orbital%20environment%20in%20a%20laboratory%20setup.%0AThis%20paper%20presents%20the%20design%20of%20a%20low-aberration%20optical%20facility%20called%0ARETINA%20to%20perform%20this%20task.%20RETINA%20is%20designed%20to%20accommodate%20cameras%20with%0Adifferent%20characteristics%20%28e.g.%2C%20sensor%20size%20and%20focal%20length%29%20while%20ensuring%0Athe%20correct%20stimulation%20of%20the%20camera%20detector.%20A%20preliminary%20design%20is%0Aperformed%20to%20identify%20the%20range%20of%20possible%20components%20to%20be%20used%20in%20the%0Afacility%20according%20to%20the%20facility%20requirements.%20Then%2C%20a%20detailed%20optical%0Adesign%20is%20performed%20in%20Zemax%20OpticStudio%20to%20optimize%20the%20number%20and%0Acharacteristics%20of%20the%20lenses%20composing%20the%20facility%27s%20optical%20systems.%20The%0Afinal%20design%20is%20compared%20against%20the%20preliminary%20design%20to%20show%20the%20superiority%0Aof%20the%20optical%20performance%20achieved%20with%20this%20approach.%20This%20work%20presents%20also%0Aa%20calibration%20procedure%20to%20estimate%20the%20misalignment%20and%20the%20centering%20errors%0Ain%20the%20facility.%20These%20estimated%20parameters%20are%20used%20in%20a%20dedicated%0Acompensation%20algorithm%2C%20enabling%20the%20stimulation%20of%20the%20camera%20at%20tens%20of%0Aarcseconds%20of%20precision.%20Finally%2C%20two%20different%20applications%20are%20presented%20to%0Ashow%20the%20versatility%20of%20RETINA%20in%20accommodating%20different%20cameras%20and%20in%0Asimulating%20different%20mission%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRETINA%253A%2520a%2520hardware-in-the-loop%2520optical%2520facility%2520with%2520reduced%2520optical%250A%2520%2520aberrations%26entry.906535625%3DPaolo%2520Panicucci%2520and%2520Fabio%2520Ornati%2520and%2520Francesco%2520Topputo%26entry.1292438233%3D%2520%2520The%2520increasing%2520interest%2520in%2520spacecraft%2520autonomy%2520and%2520the%2520complex%2520tasks%2520to%2520be%250Aaccomplished%2520by%2520the%2520spacecraft%2520raise%2520the%2520need%2520for%2520a%2520trustworthy%2520approach%2520to%250Aperform%2520Verification%2520%2526%2520Validation%2520of%2520Guidance%252C%2520Navigation%252C%2520and%2520Control%250Aalgorithms.%2520In%2520the%2520context%2520of%2520autonomous%2520operations%252C%2520vision-based%2520navigation%250Aalgorithms%2520have%2520established%2520themselves%2520as%2520effective%2520solutions%2520to%2520determine%2520the%250Aspacecraft%2520state%2520in%2520orbit%2520with%2520low-cost%2520and%2520versatile%2520sensors.%2520Nevertheless%252C%250Adetailed%2520testing%2520must%2520be%2520performed%2520on%2520ground%2520to%2520understand%2520the%2520algorithm%2527s%250Arobustness%2520and%2520performance%2520on%2520flight%2520hardware.%2520Given%2520the%2520impossibility%2520of%250Atesting%2520directly%2520on%2520orbit%2520these%2520algorithms%252C%2520a%2520dedicated%2520simulation%2520framework%250Amust%2520be%2520developed%2520to%2520emulate%2520the%2520orbital%2520environment%2520in%2520a%2520laboratory%2520setup.%250AThis%2520paper%2520presents%2520the%2520design%2520of%2520a%2520low-aberration%2520optical%2520facility%2520called%250ARETINA%2520to%2520perform%2520this%2520task.%2520RETINA%2520is%2520designed%2520to%2520accommodate%2520cameras%2520with%250Adifferent%2520characteristics%2520%2528e.g.%252C%2520sensor%2520size%2520and%2520focal%2520length%2529%2520while%2520ensuring%250Athe%2520correct%2520stimulation%2520of%2520the%2520camera%2520detector.%2520A%2520preliminary%2520design%2520is%250Aperformed%2520to%2520identify%2520the%2520range%2520of%2520possible%2520components%2520to%2520be%2520used%2520in%2520the%250Afacility%2520according%2520to%2520the%2520facility%2520requirements.%2520Then%252C%2520a%2520detailed%2520optical%250Adesign%2520is%2520performed%2520in%2520Zemax%2520OpticStudio%2520to%2520optimize%2520the%2520number%2520and%250Acharacteristics%2520of%2520the%2520lenses%2520composing%2520the%2520facility%2527s%2520optical%2520systems.%2520The%250Afinal%2520design%2520is%2520compared%2520against%2520the%2520preliminary%2520design%2520to%2520show%2520the%2520superiority%250Aof%2520the%2520optical%2520performance%2520achieved%2520with%2520this%2520approach.%2520This%2520work%2520presents%2520also%250Aa%2520calibration%2520procedure%2520to%2520estimate%2520the%2520misalignment%2520and%2520the%2520centering%2520errors%250Ain%2520the%2520facility.%2520These%2520estimated%2520parameters%2520are%2520used%2520in%2520a%2520dedicated%250Acompensation%2520algorithm%252C%2520enabling%2520the%2520stimulation%2520of%2520the%2520camera%2520at%2520tens%2520of%250Aarcseconds%2520of%2520precision.%2520Finally%252C%2520two%2520different%2520applications%2520are%2520presented%2520to%250Ashow%2520the%2520versatility%2520of%2520RETINA%2520in%2520accommodating%2520different%2520cameras%2520and%2520in%250Asimulating%2520different%2520mission%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RETINA%3A%20a%20hardware-in-the-loop%20optical%20facility%20with%20reduced%20optical%0A%20%20aberrations&entry.906535625=Paolo%20Panicucci%20and%20Fabio%20Ornati%20and%20Francesco%20Topputo&entry.1292438233=%20%20The%20increasing%20interest%20in%20spacecraft%20autonomy%20and%20the%20complex%20tasks%20to%20be%0Aaccomplished%20by%20the%20spacecraft%20raise%20the%20need%20for%20a%20trustworthy%20approach%20to%0Aperform%20Verification%20%26%20Validation%20of%20Guidance%2C%20Navigation%2C%20and%20Control%0Aalgorithms.%20In%20the%20context%20of%20autonomous%20operations%2C%20vision-based%20navigation%0Aalgorithms%20have%20established%20themselves%20as%20effective%20solutions%20to%20determine%20the%0Aspacecraft%20state%20in%20orbit%20with%20low-cost%20and%20versatile%20sensors.%20Nevertheless%2C%0Adetailed%20testing%20must%20be%20performed%20on%20ground%20to%20understand%20the%20algorithm%27s%0Arobustness%20and%20performance%20on%20flight%20hardware.%20Given%20the%20impossibility%20of%0Atesting%20directly%20on%20orbit%20these%20algorithms%2C%20a%20dedicated%20simulation%20framework%0Amust%20be%20developed%20to%20emulate%20the%20orbital%20environment%20in%20a%20laboratory%20setup.%0AThis%20paper%20presents%20the%20design%20of%20a%20low-aberration%20optical%20facility%20called%0ARETINA%20to%20perform%20this%20task.%20RETINA%20is%20designed%20to%20accommodate%20cameras%20with%0Adifferent%20characteristics%20%28e.g.%2C%20sensor%20size%20and%20focal%20length%29%20while%20ensuring%0Athe%20correct%20stimulation%20of%20the%20camera%20detector.%20A%20preliminary%20design%20is%0Aperformed%20to%20identify%20the%20range%20of%20possible%20components%20to%20be%20used%20in%20the%0Afacility%20according%20to%20the%20facility%20requirements.%20Then%2C%20a%20detailed%20optical%0Adesign%20is%20performed%20in%20Zemax%20OpticStudio%20to%20optimize%20the%20number%20and%0Acharacteristics%20of%20the%20lenses%20composing%20the%20facility%27s%20optical%20systems.%20The%0Afinal%20design%20is%20compared%20against%20the%20preliminary%20design%20to%20show%20the%20superiority%0Aof%20the%20optical%20performance%20achieved%20with%20this%20approach.%20This%20work%20presents%20also%0Aa%20calibration%20procedure%20to%20estimate%20the%20misalignment%20and%20the%20centering%20errors%0Ain%20the%20facility.%20These%20estimated%20parameters%20are%20used%20in%20a%20dedicated%0Acompensation%20algorithm%2C%20enabling%20the%20stimulation%20of%20the%20camera%20at%20tens%20of%0Aarcseconds%20of%20precision.%20Finally%2C%20two%20different%20applications%20are%20presented%20to%0Ashow%20the%20versatility%20of%20RETINA%20in%20accommodating%20different%20cameras%20and%20in%0Asimulating%20different%20mission%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02172v1&entry.124074799=Read"},
{"title": "SERNet-Former: Semantic Segmentation by Efficient Residual Network with\n  Attention-Boosting Gates and Attention-Fusion Networks", "author": "Serdar Erisen", "abstract": "  Improving the efficiency of state-of-the-art methods in semantic segmentation\nrequires overcoming the increasing computational cost as well as issues such as\nfusing semantic information from global and local contexts. Based on the recent\nsuccess and problems that convolutional neural networks (CNNs) encounter in\nsemantic segmentation, this research proposes an encoder-decoder architecture\nwith a unique efficient residual network, Efficient-ResNet. Attention-boosting\ngates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to\nfuse the equivariant and feature-based semantic information with the equivalent\nsizes of the output of global context of the efficient residual network in the\nencoder. Respectively, the decoder network is developed with the additional\nattention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve\nthe efficiency in the one-to-one conversion of the semantic information by\ndeploying additional convolution layers in the decoder part. Our network is\ntested on the challenging CamVid and Cityscapes datasets, and the proposed\nmethods reveal significant improvements on the residual networks. To the best\nof our knowledge, the developed network, SERNet-Former, achieves\nstate-of-the-art results (84.62 % mean IoU) on CamVid dataset and challenging\nresults (87.35 % mean IoU) on Cityscapes validation dataset.\n", "link": "http://arxiv.org/abs/2401.15741v7", "date": "2024-07-02", "relevancy": 2.4937, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5082}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4986}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SERNet-Former%3A%20Semantic%20Segmentation%20by%20Efficient%20Residual%20Network%20with%0A%20%20Attention-Boosting%20Gates%20and%20Attention-Fusion%20Networks&body=Title%3A%20SERNet-Former%3A%20Semantic%20Segmentation%20by%20Efficient%20Residual%20Network%20with%0A%20%20Attention-Boosting%20Gates%20and%20Attention-Fusion%20Networks%0AAuthor%3A%20Serdar%20Erisen%0AAbstract%3A%20%20%20Improving%20the%20efficiency%20of%20state-of-the-art%20methods%20in%20semantic%20segmentation%0Arequires%20overcoming%20the%20increasing%20computational%20cost%20as%20well%20as%20issues%20such%20as%0Afusing%20semantic%20information%20from%20global%20and%20local%20contexts.%20Based%20on%20the%20recent%0Asuccess%20and%20problems%20that%20convolutional%20neural%20networks%20%28CNNs%29%20encounter%20in%0Asemantic%20segmentation%2C%20this%20research%20proposes%20an%20encoder-decoder%20architecture%0Awith%20a%20unique%20efficient%20residual%20network%2C%20Efficient-ResNet.%20Attention-boosting%0Agates%20%28AbGs%29%20and%20attention-boosting%20modules%20%28AbMs%29%20are%20deployed%20by%20aiming%20to%0Afuse%20the%20equivariant%20and%20feature-based%20semantic%20information%20with%20the%20equivalent%0Asizes%20of%20the%20output%20of%20global%20context%20of%20the%20efficient%20residual%20network%20in%20the%0Aencoder.%20Respectively%2C%20the%20decoder%20network%20is%20developed%20with%20the%20additional%0Aattention-fusion%20networks%20%28AfNs%29%20inspired%20by%20AbM.%20AfNs%20are%20designed%20to%20improve%0Athe%20efficiency%20in%20the%20one-to-one%20conversion%20of%20the%20semantic%20information%20by%0Adeploying%20additional%20convolution%20layers%20in%20the%20decoder%20part.%20Our%20network%20is%0Atested%20on%20the%20challenging%20CamVid%20and%20Cityscapes%20datasets%2C%20and%20the%20proposed%0Amethods%20reveal%20significant%20improvements%20on%20the%20residual%20networks.%20To%20the%20best%0Aof%20our%20knowledge%2C%20the%20developed%20network%2C%20SERNet-Former%2C%20achieves%0Astate-of-the-art%20results%20%2884.62%20%25%20mean%20IoU%29%20on%20CamVid%20dataset%20and%20challenging%0Aresults%20%2887.35%20%25%20mean%20IoU%29%20on%20Cityscapes%20validation%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15741v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSERNet-Former%253A%2520Semantic%2520Segmentation%2520by%2520Efficient%2520Residual%2520Network%2520with%250A%2520%2520Attention-Boosting%2520Gates%2520and%2520Attention-Fusion%2520Networks%26entry.906535625%3DSerdar%2520Erisen%26entry.1292438233%3D%2520%2520Improving%2520the%2520efficiency%2520of%2520state-of-the-art%2520methods%2520in%2520semantic%2520segmentation%250Arequires%2520overcoming%2520the%2520increasing%2520computational%2520cost%2520as%2520well%2520as%2520issues%2520such%2520as%250Afusing%2520semantic%2520information%2520from%2520global%2520and%2520local%2520contexts.%2520Based%2520on%2520the%2520recent%250Asuccess%2520and%2520problems%2520that%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520encounter%2520in%250Asemantic%2520segmentation%252C%2520this%2520research%2520proposes%2520an%2520encoder-decoder%2520architecture%250Awith%2520a%2520unique%2520efficient%2520residual%2520network%252C%2520Efficient-ResNet.%2520Attention-boosting%250Agates%2520%2528AbGs%2529%2520and%2520attention-boosting%2520modules%2520%2528AbMs%2529%2520are%2520deployed%2520by%2520aiming%2520to%250Afuse%2520the%2520equivariant%2520and%2520feature-based%2520semantic%2520information%2520with%2520the%2520equivalent%250Asizes%2520of%2520the%2520output%2520of%2520global%2520context%2520of%2520the%2520efficient%2520residual%2520network%2520in%2520the%250Aencoder.%2520Respectively%252C%2520the%2520decoder%2520network%2520is%2520developed%2520with%2520the%2520additional%250Aattention-fusion%2520networks%2520%2528AfNs%2529%2520inspired%2520by%2520AbM.%2520AfNs%2520are%2520designed%2520to%2520improve%250Athe%2520efficiency%2520in%2520the%2520one-to-one%2520conversion%2520of%2520the%2520semantic%2520information%2520by%250Adeploying%2520additional%2520convolution%2520layers%2520in%2520the%2520decoder%2520part.%2520Our%2520network%2520is%250Atested%2520on%2520the%2520challenging%2520CamVid%2520and%2520Cityscapes%2520datasets%252C%2520and%2520the%2520proposed%250Amethods%2520reveal%2520significant%2520improvements%2520on%2520the%2520residual%2520networks.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520the%2520developed%2520network%252C%2520SERNet-Former%252C%2520achieves%250Astate-of-the-art%2520results%2520%252884.62%2520%2525%2520mean%2520IoU%2529%2520on%2520CamVid%2520dataset%2520and%2520challenging%250Aresults%2520%252887.35%2520%2525%2520mean%2520IoU%2529%2520on%2520Cityscapes%2520validation%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15741v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SERNet-Former%3A%20Semantic%20Segmentation%20by%20Efficient%20Residual%20Network%20with%0A%20%20Attention-Boosting%20Gates%20and%20Attention-Fusion%20Networks&entry.906535625=Serdar%20Erisen&entry.1292438233=%20%20Improving%20the%20efficiency%20of%20state-of-the-art%20methods%20in%20semantic%20segmentation%0Arequires%20overcoming%20the%20increasing%20computational%20cost%20as%20well%20as%20issues%20such%20as%0Afusing%20semantic%20information%20from%20global%20and%20local%20contexts.%20Based%20on%20the%20recent%0Asuccess%20and%20problems%20that%20convolutional%20neural%20networks%20%28CNNs%29%20encounter%20in%0Asemantic%20segmentation%2C%20this%20research%20proposes%20an%20encoder-decoder%20architecture%0Awith%20a%20unique%20efficient%20residual%20network%2C%20Efficient-ResNet.%20Attention-boosting%0Agates%20%28AbGs%29%20and%20attention-boosting%20modules%20%28AbMs%29%20are%20deployed%20by%20aiming%20to%0Afuse%20the%20equivariant%20and%20feature-based%20semantic%20information%20with%20the%20equivalent%0Asizes%20of%20the%20output%20of%20global%20context%20of%20the%20efficient%20residual%20network%20in%20the%0Aencoder.%20Respectively%2C%20the%20decoder%20network%20is%20developed%20with%20the%20additional%0Aattention-fusion%20networks%20%28AfNs%29%20inspired%20by%20AbM.%20AfNs%20are%20designed%20to%20improve%0Athe%20efficiency%20in%20the%20one-to-one%20conversion%20of%20the%20semantic%20information%20by%0Adeploying%20additional%20convolution%20layers%20in%20the%20decoder%20part.%20Our%20network%20is%0Atested%20on%20the%20challenging%20CamVid%20and%20Cityscapes%20datasets%2C%20and%20the%20proposed%0Amethods%20reveal%20significant%20improvements%20on%20the%20residual%20networks.%20To%20the%20best%0Aof%20our%20knowledge%2C%20the%20developed%20network%2C%20SERNet-Former%2C%20achieves%0Astate-of-the-art%20results%20%2884.62%20%25%20mean%20IoU%29%20on%20CamVid%20dataset%20and%20challenging%0Aresults%20%2887.35%20%25%20mean%20IoU%29%20on%20Cityscapes%20validation%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15741v7&entry.124074799=Read"},
{"title": "OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video\n  Generation", "author": "Kepan Nan and Rui Xie and Penghao Zhou and Tiehan Fan and Zhenheng Yang and Zhijie Chen and Xiang Li and Jian Yang and Ying Tai", "abstract": "  Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.\n", "link": "http://arxiv.org/abs/2407.02371v1", "date": "2024-07-02", "relevancy": 2.4863, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6545}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6412}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenVid-1M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Text-to-video%0A%20%20Generation&body=Title%3A%20OpenVid-1M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Text-to-video%0A%20%20Generation%0AAuthor%3A%20Kepan%20Nan%20and%20Rui%20Xie%20and%20Penghao%20Zhou%20and%20Tiehan%20Fan%20and%20Zhenheng%20Yang%20and%20Zhijie%20Chen%20and%20Xiang%20Li%20and%20Jian%20Yang%20and%20Ying%20Tai%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generation%20has%20recently%20garnered%20significant%20attention%0Athanks%20to%20the%20large%20multi-modality%20model%20Sora.%20However%2C%20T2V%20generation%20still%0Afaces%20two%20important%20challenges%3A%201%29%20Lacking%20a%20precise%20open%20sourced%20high-quality%0Adataset.%20The%20previous%20popular%20video%20datasets%2C%20e.g.%20WebVid-10M%20and%20Panda-70M%2C%0Aare%20either%20with%20low%20quality%20or%20too%20large%20for%20most%20research%20institutions.%0ATherefore%2C%20it%20is%20challenging%20but%20crucial%20to%20collect%20a%20precise%20high-quality%0Atext-video%20pairs%20for%20T2V%20generation.%202%29%20Ignoring%20to%20fully%20utilize%20textual%0Ainformation.%20Recent%20T2V%20methods%20have%20focused%20on%20vision%20transformers%2C%20using%20a%0Asimple%20cross%20attention%20module%20for%20video%20generation%2C%20which%20falls%20short%20of%0Athoroughly%20extracting%20semantic%20information%20from%20text%20prompt.%20To%20address%20these%0Aissues%2C%20we%20introduce%20OpenVid-1M%2C%20a%20precise%20high-quality%20dataset%20with%20expressive%0Acaptions.%20This%20open-scenario%20dataset%20contains%20over%201%20million%20text-video%20pairs%2C%0Afacilitating%20research%20on%20T2V%20generation.%20Furthermore%2C%20we%20curate%20433K%201080p%0Avideos%20from%20OpenVid-1M%20to%20create%20OpenVidHD-0.4M%2C%20advancing%20high-definition%0Avideo%20generation.%20Additionally%2C%20we%20propose%20a%20novel%20Multi-modal%20Video%20Diffusion%0ATransformer%20%28MVDiT%29%20capable%20of%20mining%20both%20structure%20information%20from%20visual%0Atokens%20and%20semantic%20information%20from%20text%20tokens.%20Extensive%20experiments%20and%0Aablation%20studies%20verify%20the%20superiority%20of%20OpenVid-1M%20over%20previous%20datasets%0Aand%20the%20effectiveness%20of%20our%20MVDiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenVid-1M%253A%2520A%2520Large-Scale%2520High-Quality%2520Dataset%2520for%2520Text-to-video%250A%2520%2520Generation%26entry.906535625%3DKepan%2520Nan%2520and%2520Rui%2520Xie%2520and%2520Penghao%2520Zhou%2520and%2520Tiehan%2520Fan%2520and%2520Zhenheng%2520Yang%2520and%2520Zhijie%2520Chen%2520and%2520Xiang%2520Li%2520and%2520Jian%2520Yang%2520and%2520Ying%2520Tai%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generation%2520has%2520recently%2520garnered%2520significant%2520attention%250Athanks%2520to%2520the%2520large%2520multi-modality%2520model%2520Sora.%2520However%252C%2520T2V%2520generation%2520still%250Afaces%2520two%2520important%2520challenges%253A%25201%2529%2520Lacking%2520a%2520precise%2520open%2520sourced%2520high-quality%250Adataset.%2520The%2520previous%2520popular%2520video%2520datasets%252C%2520e.g.%2520WebVid-10M%2520and%2520Panda-70M%252C%250Aare%2520either%2520with%2520low%2520quality%2520or%2520too%2520large%2520for%2520most%2520research%2520institutions.%250ATherefore%252C%2520it%2520is%2520challenging%2520but%2520crucial%2520to%2520collect%2520a%2520precise%2520high-quality%250Atext-video%2520pairs%2520for%2520T2V%2520generation.%25202%2529%2520Ignoring%2520to%2520fully%2520utilize%2520textual%250Ainformation.%2520Recent%2520T2V%2520methods%2520have%2520focused%2520on%2520vision%2520transformers%252C%2520using%2520a%250Asimple%2520cross%2520attention%2520module%2520for%2520video%2520generation%252C%2520which%2520falls%2520short%2520of%250Athoroughly%2520extracting%2520semantic%2520information%2520from%2520text%2520prompt.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520OpenVid-1M%252C%2520a%2520precise%2520high-quality%2520dataset%2520with%2520expressive%250Acaptions.%2520This%2520open-scenario%2520dataset%2520contains%2520over%25201%2520million%2520text-video%2520pairs%252C%250Afacilitating%2520research%2520on%2520T2V%2520generation.%2520Furthermore%252C%2520we%2520curate%2520433K%25201080p%250Avideos%2520from%2520OpenVid-1M%2520to%2520create%2520OpenVidHD-0.4M%252C%2520advancing%2520high-definition%250Avideo%2520generation.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520Multi-modal%2520Video%2520Diffusion%250ATransformer%2520%2528MVDiT%2529%2520capable%2520of%2520mining%2520both%2520structure%2520information%2520from%2520visual%250Atokens%2520and%2520semantic%2520information%2520from%2520text%2520tokens.%2520Extensive%2520experiments%2520and%250Aablation%2520studies%2520verify%2520the%2520superiority%2520of%2520OpenVid-1M%2520over%2520previous%2520datasets%250Aand%2520the%2520effectiveness%2520of%2520our%2520MVDiT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenVid-1M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Text-to-video%0A%20%20Generation&entry.906535625=Kepan%20Nan%20and%20Rui%20Xie%20and%20Penghao%20Zhou%20and%20Tiehan%20Fan%20and%20Zhenheng%20Yang%20and%20Zhijie%20Chen%20and%20Xiang%20Li%20and%20Jian%20Yang%20and%20Ying%20Tai&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generation%20has%20recently%20garnered%20significant%20attention%0Athanks%20to%20the%20large%20multi-modality%20model%20Sora.%20However%2C%20T2V%20generation%20still%0Afaces%20two%20important%20challenges%3A%201%29%20Lacking%20a%20precise%20open%20sourced%20high-quality%0Adataset.%20The%20previous%20popular%20video%20datasets%2C%20e.g.%20WebVid-10M%20and%20Panda-70M%2C%0Aare%20either%20with%20low%20quality%20or%20too%20large%20for%20most%20research%20institutions.%0ATherefore%2C%20it%20is%20challenging%20but%20crucial%20to%20collect%20a%20precise%20high-quality%0Atext-video%20pairs%20for%20T2V%20generation.%202%29%20Ignoring%20to%20fully%20utilize%20textual%0Ainformation.%20Recent%20T2V%20methods%20have%20focused%20on%20vision%20transformers%2C%20using%20a%0Asimple%20cross%20attention%20module%20for%20video%20generation%2C%20which%20falls%20short%20of%0Athoroughly%20extracting%20semantic%20information%20from%20text%20prompt.%20To%20address%20these%0Aissues%2C%20we%20introduce%20OpenVid-1M%2C%20a%20precise%20high-quality%20dataset%20with%20expressive%0Acaptions.%20This%20open-scenario%20dataset%20contains%20over%201%20million%20text-video%20pairs%2C%0Afacilitating%20research%20on%20T2V%20generation.%20Furthermore%2C%20we%20curate%20433K%201080p%0Avideos%20from%20OpenVid-1M%20to%20create%20OpenVidHD-0.4M%2C%20advancing%20high-definition%0Avideo%20generation.%20Additionally%2C%20we%20propose%20a%20novel%20Multi-modal%20Video%20Diffusion%0ATransformer%20%28MVDiT%29%20capable%20of%20mining%20both%20structure%20information%20from%20visual%0Atokens%20and%20semantic%20information%20from%20text%20tokens.%20Extensive%20experiments%20and%0Aablation%20studies%20verify%20the%20superiority%20of%20OpenVid-1M%20over%20previous%20datasets%0Aand%20the%20effectiveness%20of%20our%20MVDiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02371v1&entry.124074799=Read"},
{"title": "Generative Monoculture in Large Language Models", "author": "Fan Wu and Emily Black and Varun Chandrasekaran", "abstract": "  We introduce {\\em generative monoculture}, a behavior observed in large\nlanguage models (LLMs) characterized by a significant narrowing of model output\ndiversity relative to available training data for a given task: for example,\ngenerating only positive book reviews for books with a mixed reception. While\nin some cases, generative monoculture enhances performance (e.g., LLMs more\noften produce efficient code), the dangers are exacerbated in others (e.g.,\nLLMs refuse to share diverse opinions). As LLMs are increasingly used in\nhigh-impact settings such as education and web search, careful maintenance of\nLLM output diversity is essential to ensure a variety of facts and perspectives\nare preserved over time. We experimentally demonstrate the prevalence of\ngenerative monoculture through analysis of book review and code generation\ntasks, and find that simple countermeasures such as altering sampling or\nprompting strategies are insufficient to mitigate the behavior. Moreover, our\nresults suggest that the root causes of generative monoculture are likely\nembedded within the LLM's alignment processes, suggesting a need for developing\nfine-tuning paradigms that preserve or promote diversity.\n", "link": "http://arxiv.org/abs/2407.02209v1", "date": "2024-07-02", "relevancy": 2.4829, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5322}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4814}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Monoculture%20in%20Large%20Language%20Models&body=Title%3A%20Generative%20Monoculture%20in%20Large%20Language%20Models%0AAuthor%3A%20Fan%20Wu%20and%20Emily%20Black%20and%20Varun%20Chandrasekaran%0AAbstract%3A%20%20%20We%20introduce%20%7B%5Cem%20generative%20monoculture%7D%2C%20a%20behavior%20observed%20in%20large%0Alanguage%20models%20%28LLMs%29%20characterized%20by%20a%20significant%20narrowing%20of%20model%20output%0Adiversity%20relative%20to%20available%20training%20data%20for%20a%20given%20task%3A%20for%20example%2C%0Agenerating%20only%20positive%20book%20reviews%20for%20books%20with%20a%20mixed%20reception.%20While%0Ain%20some%20cases%2C%20generative%20monoculture%20enhances%20performance%20%28e.g.%2C%20LLMs%20more%0Aoften%20produce%20efficient%20code%29%2C%20the%20dangers%20are%20exacerbated%20in%20others%20%28e.g.%2C%0ALLMs%20refuse%20to%20share%20diverse%20opinions%29.%20As%20LLMs%20are%20increasingly%20used%20in%0Ahigh-impact%20settings%20such%20as%20education%20and%20web%20search%2C%20careful%20maintenance%20of%0ALLM%20output%20diversity%20is%20essential%20to%20ensure%20a%20variety%20of%20facts%20and%20perspectives%0Aare%20preserved%20over%20time.%20We%20experimentally%20demonstrate%20the%20prevalence%20of%0Agenerative%20monoculture%20through%20analysis%20of%20book%20review%20and%20code%20generation%0Atasks%2C%20and%20find%20that%20simple%20countermeasures%20such%20as%20altering%20sampling%20or%0Aprompting%20strategies%20are%20insufficient%20to%20mitigate%20the%20behavior.%20Moreover%2C%20our%0Aresults%20suggest%20that%20the%20root%20causes%20of%20generative%20monoculture%20are%20likely%0Aembedded%20within%20the%20LLM%27s%20alignment%20processes%2C%20suggesting%20a%20need%20for%20developing%0Afine-tuning%20paradigms%20that%20preserve%20or%20promote%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Monoculture%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DFan%2520Wu%2520and%2520Emily%2520Black%2520and%2520Varun%2520Chandrasekaran%26entry.1292438233%3D%2520%2520We%2520introduce%2520%257B%255Cem%2520generative%2520monoculture%257D%252C%2520a%2520behavior%2520observed%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520characterized%2520by%2520a%2520significant%2520narrowing%2520of%2520model%2520output%250Adiversity%2520relative%2520to%2520available%2520training%2520data%2520for%2520a%2520given%2520task%253A%2520for%2520example%252C%250Agenerating%2520only%2520positive%2520book%2520reviews%2520for%2520books%2520with%2520a%2520mixed%2520reception.%2520While%250Ain%2520some%2520cases%252C%2520generative%2520monoculture%2520enhances%2520performance%2520%2528e.g.%252C%2520LLMs%2520more%250Aoften%2520produce%2520efficient%2520code%2529%252C%2520the%2520dangers%2520are%2520exacerbated%2520in%2520others%2520%2528e.g.%252C%250ALLMs%2520refuse%2520to%2520share%2520diverse%2520opinions%2529.%2520As%2520LLMs%2520are%2520increasingly%2520used%2520in%250Ahigh-impact%2520settings%2520such%2520as%2520education%2520and%2520web%2520search%252C%2520careful%2520maintenance%2520of%250ALLM%2520output%2520diversity%2520is%2520essential%2520to%2520ensure%2520a%2520variety%2520of%2520facts%2520and%2520perspectives%250Aare%2520preserved%2520over%2520time.%2520We%2520experimentally%2520demonstrate%2520the%2520prevalence%2520of%250Agenerative%2520monoculture%2520through%2520analysis%2520of%2520book%2520review%2520and%2520code%2520generation%250Atasks%252C%2520and%2520find%2520that%2520simple%2520countermeasures%2520such%2520as%2520altering%2520sampling%2520or%250Aprompting%2520strategies%2520are%2520insufficient%2520to%2520mitigate%2520the%2520behavior.%2520Moreover%252C%2520our%250Aresults%2520suggest%2520that%2520the%2520root%2520causes%2520of%2520generative%2520monoculture%2520are%2520likely%250Aembedded%2520within%2520the%2520LLM%2527s%2520alignment%2520processes%252C%2520suggesting%2520a%2520need%2520for%2520developing%250Afine-tuning%2520paradigms%2520that%2520preserve%2520or%2520promote%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Monoculture%20in%20Large%20Language%20Models&entry.906535625=Fan%20Wu%20and%20Emily%20Black%20and%20Varun%20Chandrasekaran&entry.1292438233=%20%20We%20introduce%20%7B%5Cem%20generative%20monoculture%7D%2C%20a%20behavior%20observed%20in%20large%0Alanguage%20models%20%28LLMs%29%20characterized%20by%20a%20significant%20narrowing%20of%20model%20output%0Adiversity%20relative%20to%20available%20training%20data%20for%20a%20given%20task%3A%20for%20example%2C%0Agenerating%20only%20positive%20book%20reviews%20for%20books%20with%20a%20mixed%20reception.%20While%0Ain%20some%20cases%2C%20generative%20monoculture%20enhances%20performance%20%28e.g.%2C%20LLMs%20more%0Aoften%20produce%20efficient%20code%29%2C%20the%20dangers%20are%20exacerbated%20in%20others%20%28e.g.%2C%0ALLMs%20refuse%20to%20share%20diverse%20opinions%29.%20As%20LLMs%20are%20increasingly%20used%20in%0Ahigh-impact%20settings%20such%20as%20education%20and%20web%20search%2C%20careful%20maintenance%20of%0ALLM%20output%20diversity%20is%20essential%20to%20ensure%20a%20variety%20of%20facts%20and%20perspectives%0Aare%20preserved%20over%20time.%20We%20experimentally%20demonstrate%20the%20prevalence%20of%0Agenerative%20monoculture%20through%20analysis%20of%20book%20review%20and%20code%20generation%0Atasks%2C%20and%20find%20that%20simple%20countermeasures%20such%20as%20altering%20sampling%20or%0Aprompting%20strategies%20are%20insufficient%20to%20mitigate%20the%20behavior.%20Moreover%2C%20our%0Aresults%20suggest%20that%20the%20root%20causes%20of%20generative%20monoculture%20are%20likely%0Aembedded%20within%20the%20LLM%27s%20alignment%20processes%2C%20suggesting%20a%20need%20for%20developing%0Afine-tuning%20paradigms%20that%20preserve%20or%20promote%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02209v1&entry.124074799=Read"},
{"title": "Synthetic Multimodal Question Generation", "author": "Ian Wu and Sravan Jayanthi and Vijay Viswanathan and Simon Rosenberg and Sina Pakazad and Tongshuang Wu and Graham Neubig", "abstract": "  Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to\nquestion-answering over multimodal documents. A key challenge with evaluating\nMMRAG is the paucity of high-quality datasets matching the question styles and\nmodalities of interest. In light of this, we propose SMMQG, a synthetic data\ngeneration framework. SMMQG leverages interplay between a retriever, large\nlanguage model (LLM) and large multimodal model (LMM) to generate question and\nanswer pairs directly from multimodal documents, with the questions conforming\nto specified styles and modalities. We use SMMQG to generate an MMRAG dataset\nof 1024 questions over Wikipedia documents and evaluate state-of-the-art models\nusing it, revealing insights into model performance that are attainable only\nthrough style- and modality-specific evaluation data. Next, we measure the\nquality of data produced by SMMQG via a human study. We find that the quality\nof our synthetic data is on par with the quality of the crowdsourced benchmark\nMMQA and that downstream evaluation results using both datasets strongly\nconcur.\n", "link": "http://arxiv.org/abs/2407.02233v1", "date": "2024-07-02", "relevancy": 2.4808, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5132}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4909}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Multimodal%20Question%20Generation&body=Title%3A%20Synthetic%20Multimodal%20Question%20Generation%0AAuthor%3A%20Ian%20Wu%20and%20Sravan%20Jayanthi%20and%20Vijay%20Viswanathan%20and%20Simon%20Rosenberg%20and%20Sina%20Pakazad%20and%20Tongshuang%20Wu%20and%20Graham%20Neubig%0AAbstract%3A%20%20%20Multimodal%20Retrieval%20Augmented%20Generation%20%28MMRAG%29%20is%20a%20powerful%20approach%20to%0Aquestion-answering%20over%20multimodal%20documents.%20A%20key%20challenge%20with%20evaluating%0AMMRAG%20is%20the%20paucity%20of%20high-quality%20datasets%20matching%20the%20question%20styles%20and%0Amodalities%20of%20interest.%20In%20light%20of%20this%2C%20we%20propose%20SMMQG%2C%20a%20synthetic%20data%0Ageneration%20framework.%20SMMQG%20leverages%20interplay%20between%20a%20retriever%2C%20large%0Alanguage%20model%20%28LLM%29%20and%20large%20multimodal%20model%20%28LMM%29%20to%20generate%20question%20and%0Aanswer%20pairs%20directly%20from%20multimodal%20documents%2C%20with%20the%20questions%20conforming%0Ato%20specified%20styles%20and%20modalities.%20We%20use%20SMMQG%20to%20generate%20an%20MMRAG%20dataset%0Aof%201024%20questions%20over%20Wikipedia%20documents%20and%20evaluate%20state-of-the-art%20models%0Ausing%20it%2C%20revealing%20insights%20into%20model%20performance%20that%20are%20attainable%20only%0Athrough%20style-%20and%20modality-specific%20evaluation%20data.%20Next%2C%20we%20measure%20the%0Aquality%20of%20data%20produced%20by%20SMMQG%20via%20a%20human%20study.%20We%20find%20that%20the%20quality%0Aof%20our%20synthetic%20data%20is%20on%20par%20with%20the%20quality%20of%20the%20crowdsourced%20benchmark%0AMMQA%20and%20that%20downstream%20evaluation%20results%20using%20both%20datasets%20strongly%0Aconcur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Multimodal%2520Question%2520Generation%26entry.906535625%3DIan%2520Wu%2520and%2520Sravan%2520Jayanthi%2520and%2520Vijay%2520Viswanathan%2520and%2520Simon%2520Rosenberg%2520and%2520Sina%2520Pakazad%2520and%2520Tongshuang%2520Wu%2520and%2520Graham%2520Neubig%26entry.1292438233%3D%2520%2520Multimodal%2520Retrieval%2520Augmented%2520Generation%2520%2528MMRAG%2529%2520is%2520a%2520powerful%2520approach%2520to%250Aquestion-answering%2520over%2520multimodal%2520documents.%2520A%2520key%2520challenge%2520with%2520evaluating%250AMMRAG%2520is%2520the%2520paucity%2520of%2520high-quality%2520datasets%2520matching%2520the%2520question%2520styles%2520and%250Amodalities%2520of%2520interest.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520SMMQG%252C%2520a%2520synthetic%2520data%250Ageneration%2520framework.%2520SMMQG%2520leverages%2520interplay%2520between%2520a%2520retriever%252C%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520and%2520large%2520multimodal%2520model%2520%2528LMM%2529%2520to%2520generate%2520question%2520and%250Aanswer%2520pairs%2520directly%2520from%2520multimodal%2520documents%252C%2520with%2520the%2520questions%2520conforming%250Ato%2520specified%2520styles%2520and%2520modalities.%2520We%2520use%2520SMMQG%2520to%2520generate%2520an%2520MMRAG%2520dataset%250Aof%25201024%2520questions%2520over%2520Wikipedia%2520documents%2520and%2520evaluate%2520state-of-the-art%2520models%250Ausing%2520it%252C%2520revealing%2520insights%2520into%2520model%2520performance%2520that%2520are%2520attainable%2520only%250Athrough%2520style-%2520and%2520modality-specific%2520evaluation%2520data.%2520Next%252C%2520we%2520measure%2520the%250Aquality%2520of%2520data%2520produced%2520by%2520SMMQG%2520via%2520a%2520human%2520study.%2520We%2520find%2520that%2520the%2520quality%250Aof%2520our%2520synthetic%2520data%2520is%2520on%2520par%2520with%2520the%2520quality%2520of%2520the%2520crowdsourced%2520benchmark%250AMMQA%2520and%2520that%2520downstream%2520evaluation%2520results%2520using%2520both%2520datasets%2520strongly%250Aconcur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Multimodal%20Question%20Generation&entry.906535625=Ian%20Wu%20and%20Sravan%20Jayanthi%20and%20Vijay%20Viswanathan%20and%20Simon%20Rosenberg%20and%20Sina%20Pakazad%20and%20Tongshuang%20Wu%20and%20Graham%20Neubig&entry.1292438233=%20%20Multimodal%20Retrieval%20Augmented%20Generation%20%28MMRAG%29%20is%20a%20powerful%20approach%20to%0Aquestion-answering%20over%20multimodal%20documents.%20A%20key%20challenge%20with%20evaluating%0AMMRAG%20is%20the%20paucity%20of%20high-quality%20datasets%20matching%20the%20question%20styles%20and%0Amodalities%20of%20interest.%20In%20light%20of%20this%2C%20we%20propose%20SMMQG%2C%20a%20synthetic%20data%0Ageneration%20framework.%20SMMQG%20leverages%20interplay%20between%20a%20retriever%2C%20large%0Alanguage%20model%20%28LLM%29%20and%20large%20multimodal%20model%20%28LMM%29%20to%20generate%20question%20and%0Aanswer%20pairs%20directly%20from%20multimodal%20documents%2C%20with%20the%20questions%20conforming%0Ato%20specified%20styles%20and%20modalities.%20We%20use%20SMMQG%20to%20generate%20an%20MMRAG%20dataset%0Aof%201024%20questions%20over%20Wikipedia%20documents%20and%20evaluate%20state-of-the-art%20models%0Ausing%20it%2C%20revealing%20insights%20into%20model%20performance%20that%20are%20attainable%20only%0Athrough%20style-%20and%20modality-specific%20evaluation%20data.%20Next%2C%20we%20measure%20the%0Aquality%20of%20data%20produced%20by%20SMMQG%20via%20a%20human%20study.%20We%20find%20that%20the%20quality%0Aof%20our%20synthetic%20data%20is%20on%20par%20with%20the%20quality%20of%20the%20crowdsourced%20benchmark%0AMMQA%20and%20that%20downstream%20evaluation%20results%20using%20both%20datasets%20strongly%0Aconcur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02233v1&entry.124074799=Read"},
{"title": "ColPali: Efficient Document Retrieval with Vision Language Models", "author": "Manuel Faysse and Hugues Sibille and Tony Wu and Bilel Omrani and Gautier Viaud and C\u00e9line Hudelot and Pierre Colombo", "abstract": "  Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.\n", "link": "http://arxiv.org/abs/2407.01449v2", "date": "2024-07-02", "relevancy": 2.4729, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColPali%3A%20Efficient%20Document%20Retrieval%20with%20Vision%20Language%20Models&body=Title%3A%20ColPali%3A%20Efficient%20Document%20Retrieval%20with%20Vision%20Language%20Models%0AAuthor%3A%20Manuel%20Faysse%20and%20Hugues%20Sibille%20and%20Tony%20Wu%20and%20Bilel%20Omrani%20and%20Gautier%20Viaud%20and%20C%C3%A9line%20Hudelot%20and%20Pierre%20Colombo%0AAbstract%3A%20%20%20Documents%20are%20visually%20rich%20structures%20that%20convey%20information%20through%20text%2C%0Aas%20well%20as%20tables%2C%20figures%2C%20page%20layouts%2C%20or%20fonts.%20While%20modern%20document%0Aretrieval%20systems%20exhibit%20strong%20performance%20on%20query-to-text%20matching%2C%20they%0Astruggle%20to%20exploit%20visual%20cues%20efficiently%2C%20hindering%20their%20performance%20on%0Apractical%20document%20retrieval%20applications%20such%20as%20Retrieval%20Augmented%0AGeneration.%20To%20benchmark%20current%20systems%20on%20visually%20rich%20document%20retrieval%2C%0Awe%20introduce%20the%20Visual%20Document%20Retrieval%20Benchmark%20ViDoRe%2C%20composed%20of%0Avarious%20page-level%20retrieving%20tasks%20spanning%20multiple%20domains%2C%20languages%2C%20and%0Asettings.%20The%20inherent%20shortcomings%20of%20modern%20systems%20motivate%20the%20introduction%0Aof%20a%20new%20retrieval%20model%20architecture%2C%20ColPali%2C%20which%20leverages%20the%20document%0Aunderstanding%20capabilities%20of%20recent%20Vision%20Language%20Models%20to%20produce%0Ahigh-quality%20contextualized%20embeddings%20solely%20from%20images%20of%20document%20pages.%0ACombined%20with%20a%20late%20interaction%20matching%20mechanism%2C%20ColPali%20largely%0Aoutperforms%20modern%20document%20retrieval%20pipelines%20while%20being%20drastically%20faster%0Aand%20end-to-end%20trainable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01449v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColPali%253A%2520Efficient%2520Document%2520Retrieval%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DManuel%2520Faysse%2520and%2520Hugues%2520Sibille%2520and%2520Tony%2520Wu%2520and%2520Bilel%2520Omrani%2520and%2520Gautier%2520Viaud%2520and%2520C%25C3%25A9line%2520Hudelot%2520and%2520Pierre%2520Colombo%26entry.1292438233%3D%2520%2520Documents%2520are%2520visually%2520rich%2520structures%2520that%2520convey%2520information%2520through%2520text%252C%250Aas%2520well%2520as%2520tables%252C%2520figures%252C%2520page%2520layouts%252C%2520or%2520fonts.%2520While%2520modern%2520document%250Aretrieval%2520systems%2520exhibit%2520strong%2520performance%2520on%2520query-to-text%2520matching%252C%2520they%250Astruggle%2520to%2520exploit%2520visual%2520cues%2520efficiently%252C%2520hindering%2520their%2520performance%2520on%250Apractical%2520document%2520retrieval%2520applications%2520such%2520as%2520Retrieval%2520Augmented%250AGeneration.%2520To%2520benchmark%2520current%2520systems%2520on%2520visually%2520rich%2520document%2520retrieval%252C%250Awe%2520introduce%2520the%2520Visual%2520Document%2520Retrieval%2520Benchmark%2520ViDoRe%252C%2520composed%2520of%250Avarious%2520page-level%2520retrieving%2520tasks%2520spanning%2520multiple%2520domains%252C%2520languages%252C%2520and%250Asettings.%2520The%2520inherent%2520shortcomings%2520of%2520modern%2520systems%2520motivate%2520the%2520introduction%250Aof%2520a%2520new%2520retrieval%2520model%2520architecture%252C%2520ColPali%252C%2520which%2520leverages%2520the%2520document%250Aunderstanding%2520capabilities%2520of%2520recent%2520Vision%2520Language%2520Models%2520to%2520produce%250Ahigh-quality%2520contextualized%2520embeddings%2520solely%2520from%2520images%2520of%2520document%2520pages.%250ACombined%2520with%2520a%2520late%2520interaction%2520matching%2520mechanism%252C%2520ColPali%2520largely%250Aoutperforms%2520modern%2520document%2520retrieval%2520pipelines%2520while%2520being%2520drastically%2520faster%250Aand%2520end-to-end%2520trainable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01449v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColPali%3A%20Efficient%20Document%20Retrieval%20with%20Vision%20Language%20Models&entry.906535625=Manuel%20Faysse%20and%20Hugues%20Sibille%20and%20Tony%20Wu%20and%20Bilel%20Omrani%20and%20Gautier%20Viaud%20and%20C%C3%A9line%20Hudelot%20and%20Pierre%20Colombo&entry.1292438233=%20%20Documents%20are%20visually%20rich%20structures%20that%20convey%20information%20through%20text%2C%0Aas%20well%20as%20tables%2C%20figures%2C%20page%20layouts%2C%20or%20fonts.%20While%20modern%20document%0Aretrieval%20systems%20exhibit%20strong%20performance%20on%20query-to-text%20matching%2C%20they%0Astruggle%20to%20exploit%20visual%20cues%20efficiently%2C%20hindering%20their%20performance%20on%0Apractical%20document%20retrieval%20applications%20such%20as%20Retrieval%20Augmented%0AGeneration.%20To%20benchmark%20current%20systems%20on%20visually%20rich%20document%20retrieval%2C%0Awe%20introduce%20the%20Visual%20Document%20Retrieval%20Benchmark%20ViDoRe%2C%20composed%20of%0Avarious%20page-level%20retrieving%20tasks%20spanning%20multiple%20domains%2C%20languages%2C%20and%0Asettings.%20The%20inherent%20shortcomings%20of%20modern%20systems%20motivate%20the%20introduction%0Aof%20a%20new%20retrieval%20model%20architecture%2C%20ColPali%2C%20which%20leverages%20the%20document%0Aunderstanding%20capabilities%20of%20recent%20Vision%20Language%20Models%20to%20produce%0Ahigh-quality%20contextualized%20embeddings%20solely%20from%20images%20of%20document%20pages.%0ACombined%20with%20a%20late%20interaction%20matching%20mechanism%2C%20ColPali%20largely%0Aoutperforms%20modern%20document%20retrieval%20pipelines%20while%20being%20drastically%20faster%0Aand%20end-to-end%20trainable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01449v2&entry.124074799=Read"},
{"title": "Structure-based drug design by denoising voxel grids", "author": "Pedro O. Pinheiro and Arian Jamasb and Omar Mahmood and Vishnu Sresht and Saeed Saremi", "abstract": "  We present VoxBind, a new score-based generative model for 3D molecules\nconditioned on protein structures. Our approach represents molecules as 3D\natomic density grids and leverages a 3D voxel-denoising network for learning\nand generation. We extend the neural empirical Bayes formalism (Saremi &\nHyvarinen, 2019) to the conditional setting and generate structure-conditioned\nmolecules with a two-step procedure: (i) sample noisy molecules from the\nGaussian-smoothed conditional distribution with underdamped Langevin MCMC using\nthe learned score function and (ii) estimate clean molecules from the noisy\nsamples with single-step denoising. Compared to the current state of the art,\nour model is simpler to train, significantly faster to sample from, and\nachieves better results on extensive in silico benchmarks -- the generated\nmolecules are more diverse, exhibit fewer steric clashes, and bind with higher\naffinity to protein pockets. The code is available at\nhttps://github.com/genentech/voxbind/.\n", "link": "http://arxiv.org/abs/2405.03961v2", "date": "2024-07-02", "relevancy": 2.4635, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4972}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4904}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-based%20drug%20design%20by%20denoising%20voxel%20grids&body=Title%3A%20Structure-based%20drug%20design%20by%20denoising%20voxel%20grids%0AAuthor%3A%20Pedro%20O.%20Pinheiro%20and%20Arian%20Jamasb%20and%20Omar%20Mahmood%20and%20Vishnu%20Sresht%20and%20Saeed%20Saremi%0AAbstract%3A%20%20%20We%20present%20VoxBind%2C%20a%20new%20score-based%20generative%20model%20for%203D%20molecules%0Aconditioned%20on%20protein%20structures.%20Our%20approach%20represents%20molecules%20as%203D%0Aatomic%20density%20grids%20and%20leverages%20a%203D%20voxel-denoising%20network%20for%20learning%0Aand%20generation.%20We%20extend%20the%20neural%20empirical%20Bayes%20formalism%20%28Saremi%20%26%0AHyvarinen%2C%202019%29%20to%20the%20conditional%20setting%20and%20generate%20structure-conditioned%0Amolecules%20with%20a%20two-step%20procedure%3A%20%28i%29%20sample%20noisy%20molecules%20from%20the%0AGaussian-smoothed%20conditional%20distribution%20with%20underdamped%20Langevin%20MCMC%20using%0Athe%20learned%20score%20function%20and%20%28ii%29%20estimate%20clean%20molecules%20from%20the%20noisy%0Asamples%20with%20single-step%20denoising.%20Compared%20to%20the%20current%20state%20of%20the%20art%2C%0Aour%20model%20is%20simpler%20to%20train%2C%20significantly%20faster%20to%20sample%20from%2C%20and%0Aachieves%20better%20results%20on%20extensive%20in%20silico%20benchmarks%20--%20the%20generated%0Amolecules%20are%20more%20diverse%2C%20exhibit%20fewer%20steric%20clashes%2C%20and%20bind%20with%20higher%0Aaffinity%20to%20protein%20pockets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/genentech/voxbind/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-based%2520drug%2520design%2520by%2520denoising%2520voxel%2520grids%26entry.906535625%3DPedro%2520O.%2520Pinheiro%2520and%2520Arian%2520Jamasb%2520and%2520Omar%2520Mahmood%2520and%2520Vishnu%2520Sresht%2520and%2520Saeed%2520Saremi%26entry.1292438233%3D%2520%2520We%2520present%2520VoxBind%252C%2520a%2520new%2520score-based%2520generative%2520model%2520for%25203D%2520molecules%250Aconditioned%2520on%2520protein%2520structures.%2520Our%2520approach%2520represents%2520molecules%2520as%25203D%250Aatomic%2520density%2520grids%2520and%2520leverages%2520a%25203D%2520voxel-denoising%2520network%2520for%2520learning%250Aand%2520generation.%2520We%2520extend%2520the%2520neural%2520empirical%2520Bayes%2520formalism%2520%2528Saremi%2520%2526%250AHyvarinen%252C%25202019%2529%2520to%2520the%2520conditional%2520setting%2520and%2520generate%2520structure-conditioned%250Amolecules%2520with%2520a%2520two-step%2520procedure%253A%2520%2528i%2529%2520sample%2520noisy%2520molecules%2520from%2520the%250AGaussian-smoothed%2520conditional%2520distribution%2520with%2520underdamped%2520Langevin%2520MCMC%2520using%250Athe%2520learned%2520score%2520function%2520and%2520%2528ii%2529%2520estimate%2520clean%2520molecules%2520from%2520the%2520noisy%250Asamples%2520with%2520single-step%2520denoising.%2520Compared%2520to%2520the%2520current%2520state%2520of%2520the%2520art%252C%250Aour%2520model%2520is%2520simpler%2520to%2520train%252C%2520significantly%2520faster%2520to%2520sample%2520from%252C%2520and%250Aachieves%2520better%2520results%2520on%2520extensive%2520in%2520silico%2520benchmarks%2520--%2520the%2520generated%250Amolecules%2520are%2520more%2520diverse%252C%2520exhibit%2520fewer%2520steric%2520clashes%252C%2520and%2520bind%2520with%2520higher%250Aaffinity%2520to%2520protein%2520pockets.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/genentech/voxbind/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-based%20drug%20design%20by%20denoising%20voxel%20grids&entry.906535625=Pedro%20O.%20Pinheiro%20and%20Arian%20Jamasb%20and%20Omar%20Mahmood%20and%20Vishnu%20Sresht%20and%20Saeed%20Saremi&entry.1292438233=%20%20We%20present%20VoxBind%2C%20a%20new%20score-based%20generative%20model%20for%203D%20molecules%0Aconditioned%20on%20protein%20structures.%20Our%20approach%20represents%20molecules%20as%203D%0Aatomic%20density%20grids%20and%20leverages%20a%203D%20voxel-denoising%20network%20for%20learning%0Aand%20generation.%20We%20extend%20the%20neural%20empirical%20Bayes%20formalism%20%28Saremi%20%26%0AHyvarinen%2C%202019%29%20to%20the%20conditional%20setting%20and%20generate%20structure-conditioned%0Amolecules%20with%20a%20two-step%20procedure%3A%20%28i%29%20sample%20noisy%20molecules%20from%20the%0AGaussian-smoothed%20conditional%20distribution%20with%20underdamped%20Langevin%20MCMC%20using%0Athe%20learned%20score%20function%20and%20%28ii%29%20estimate%20clean%20molecules%20from%20the%20noisy%0Asamples%20with%20single-step%20denoising.%20Compared%20to%20the%20current%20state%20of%20the%20art%2C%0Aour%20model%20is%20simpler%20to%20train%2C%20significantly%20faster%20to%20sample%20from%2C%20and%0Aachieves%20better%20results%20on%20extensive%20in%20silico%20benchmarks%20--%20the%20generated%0Amolecules%20are%20more%20diverse%2C%20exhibit%20fewer%20steric%20clashes%2C%20and%20bind%20with%20higher%0Aaffinity%20to%20protein%20pockets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/genentech/voxbind/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03961v2&entry.124074799=Read"},
{"title": "Structure-Aware Consensus Network on Graphs with Few Labeled Nodes", "author": "Shuaike Xu and Xiaolin Zhang and Peng Zhang and Kun Zhan", "abstract": "  Graph node classification with few labeled nodes presents significant\nchallenges due to limited supervision. Conventional methods often exploit the\ngraph in a transductive learning manner. They fail to effectively utilize the\nabundant unlabeled data and the structural information inherent in graphs. To\naddress these issues, we introduce a Structure-Aware Consensus Network (SACN)\nfrom three perspectives. Firstly, SACN leverages a novel structure-aware\nconsensus learning strategy between two strongly augmented views. The proposed\nstrategy can fully exploit the potentially useful information of the unlabeled\nnodes and the structural information of the entire graph. Secondly, SACN\nuniquely integrates the graph's structural information to achieve\nstrong-to-strong consensus learning, improving the utilization of unlabeled\ndata while maintaining multiview learning. Thirdly, unlike two-branch graph\nneural network-based methods, SACN is designed for multiview feature learning\nwithin a single-branch architecture. Furthermore, a class-aware pseudolabel\nselection strategy helps address class imbalance and achieve effective\nweak-to-strong supervision. Extensive experiments on three benchmark datasets\ndemonstrate SACN's superior performance in node classification tasks,\nparticularly at very low label rates, outperforming state-of-the-art methods\nwhile maintaining computational simplicity.The source code is available at\nhttps://github.com/kunzhan/SACN\n", "link": "http://arxiv.org/abs/2407.02188v1", "date": "2024-07-02", "relevancy": 2.4585, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.52}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Aware%20Consensus%20Network%20on%20Graphs%20with%20Few%20Labeled%20Nodes&body=Title%3A%20Structure-Aware%20Consensus%20Network%20on%20Graphs%20with%20Few%20Labeled%20Nodes%0AAuthor%3A%20Shuaike%20Xu%20and%20Xiaolin%20Zhang%20and%20Peng%20Zhang%20and%20Kun%20Zhan%0AAbstract%3A%20%20%20Graph%20node%20classification%20with%20few%20labeled%20nodes%20presents%20significant%0Achallenges%20due%20to%20limited%20supervision.%20Conventional%20methods%20often%20exploit%20the%0Agraph%20in%20a%20transductive%20learning%20manner.%20They%20fail%20to%20effectively%20utilize%20the%0Aabundant%20unlabeled%20data%20and%20the%20structural%20information%20inherent%20in%20graphs.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20a%20Structure-Aware%20Consensus%20Network%20%28SACN%29%0Afrom%20three%20perspectives.%20Firstly%2C%20SACN%20leverages%20a%20novel%20structure-aware%0Aconsensus%20learning%20strategy%20between%20two%20strongly%20augmented%20views.%20The%20proposed%0Astrategy%20can%20fully%20exploit%20the%20potentially%20useful%20information%20of%20the%20unlabeled%0Anodes%20and%20the%20structural%20information%20of%20the%20entire%20graph.%20Secondly%2C%20SACN%0Auniquely%20integrates%20the%20graph%27s%20structural%20information%20to%20achieve%0Astrong-to-strong%20consensus%20learning%2C%20improving%20the%20utilization%20of%20unlabeled%0Adata%20while%20maintaining%20multiview%20learning.%20Thirdly%2C%20unlike%20two-branch%20graph%0Aneural%20network-based%20methods%2C%20SACN%20is%20designed%20for%20multiview%20feature%20learning%0Awithin%20a%20single-branch%20architecture.%20Furthermore%2C%20a%20class-aware%20pseudolabel%0Aselection%20strategy%20helps%20address%20class%20imbalance%20and%20achieve%20effective%0Aweak-to-strong%20supervision.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%0Ademonstrate%20SACN%27s%20superior%20performance%20in%20node%20classification%20tasks%2C%0Aparticularly%20at%20very%20low%20label%20rates%2C%20outperforming%20state-of-the-art%20methods%0Awhile%20maintaining%20computational%20simplicity.The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/kunzhan/SACN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Aware%2520Consensus%2520Network%2520on%2520Graphs%2520with%2520Few%2520Labeled%2520Nodes%26entry.906535625%3DShuaike%2520Xu%2520and%2520Xiaolin%2520Zhang%2520and%2520Peng%2520Zhang%2520and%2520Kun%2520Zhan%26entry.1292438233%3D%2520%2520Graph%2520node%2520classification%2520with%2520few%2520labeled%2520nodes%2520presents%2520significant%250Achallenges%2520due%2520to%2520limited%2520supervision.%2520Conventional%2520methods%2520often%2520exploit%2520the%250Agraph%2520in%2520a%2520transductive%2520learning%2520manner.%2520They%2520fail%2520to%2520effectively%2520utilize%2520the%250Aabundant%2520unlabeled%2520data%2520and%2520the%2520structural%2520information%2520inherent%2520in%2520graphs.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520a%2520Structure-Aware%2520Consensus%2520Network%2520%2528SACN%2529%250Afrom%2520three%2520perspectives.%2520Firstly%252C%2520SACN%2520leverages%2520a%2520novel%2520structure-aware%250Aconsensus%2520learning%2520strategy%2520between%2520two%2520strongly%2520augmented%2520views.%2520The%2520proposed%250Astrategy%2520can%2520fully%2520exploit%2520the%2520potentially%2520useful%2520information%2520of%2520the%2520unlabeled%250Anodes%2520and%2520the%2520structural%2520information%2520of%2520the%2520entire%2520graph.%2520Secondly%252C%2520SACN%250Auniquely%2520integrates%2520the%2520graph%2527s%2520structural%2520information%2520to%2520achieve%250Astrong-to-strong%2520consensus%2520learning%252C%2520improving%2520the%2520utilization%2520of%2520unlabeled%250Adata%2520while%2520maintaining%2520multiview%2520learning.%2520Thirdly%252C%2520unlike%2520two-branch%2520graph%250Aneural%2520network-based%2520methods%252C%2520SACN%2520is%2520designed%2520for%2520multiview%2520feature%2520learning%250Awithin%2520a%2520single-branch%2520architecture.%2520Furthermore%252C%2520a%2520class-aware%2520pseudolabel%250Aselection%2520strategy%2520helps%2520address%2520class%2520imbalance%2520and%2520achieve%2520effective%250Aweak-to-strong%2520supervision.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%250Ademonstrate%2520SACN%2527s%2520superior%2520performance%2520in%2520node%2520classification%2520tasks%252C%250Aparticularly%2520at%2520very%2520low%2520label%2520rates%252C%2520outperforming%2520state-of-the-art%2520methods%250Awhile%2520maintaining%2520computational%2520simplicity.The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/kunzhan/SACN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Aware%20Consensus%20Network%20on%20Graphs%20with%20Few%20Labeled%20Nodes&entry.906535625=Shuaike%20Xu%20and%20Xiaolin%20Zhang%20and%20Peng%20Zhang%20and%20Kun%20Zhan&entry.1292438233=%20%20Graph%20node%20classification%20with%20few%20labeled%20nodes%20presents%20significant%0Achallenges%20due%20to%20limited%20supervision.%20Conventional%20methods%20often%20exploit%20the%0Agraph%20in%20a%20transductive%20learning%20manner.%20They%20fail%20to%20effectively%20utilize%20the%0Aabundant%20unlabeled%20data%20and%20the%20structural%20information%20inherent%20in%20graphs.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20a%20Structure-Aware%20Consensus%20Network%20%28SACN%29%0Afrom%20three%20perspectives.%20Firstly%2C%20SACN%20leverages%20a%20novel%20structure-aware%0Aconsensus%20learning%20strategy%20between%20two%20strongly%20augmented%20views.%20The%20proposed%0Astrategy%20can%20fully%20exploit%20the%20potentially%20useful%20information%20of%20the%20unlabeled%0Anodes%20and%20the%20structural%20information%20of%20the%20entire%20graph.%20Secondly%2C%20SACN%0Auniquely%20integrates%20the%20graph%27s%20structural%20information%20to%20achieve%0Astrong-to-strong%20consensus%20learning%2C%20improving%20the%20utilization%20of%20unlabeled%0Adata%20while%20maintaining%20multiview%20learning.%20Thirdly%2C%20unlike%20two-branch%20graph%0Aneural%20network-based%20methods%2C%20SACN%20is%20designed%20for%20multiview%20feature%20learning%0Awithin%20a%20single-branch%20architecture.%20Furthermore%2C%20a%20class-aware%20pseudolabel%0Aselection%20strategy%20helps%20address%20class%20imbalance%20and%20achieve%20effective%0Aweak-to-strong%20supervision.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%0Ademonstrate%20SACN%27s%20superior%20performance%20in%20node%20classification%20tasks%2C%0Aparticularly%20at%20very%20low%20label%20rates%2C%20outperforming%20state-of-the-art%20methods%0Awhile%20maintaining%20computational%20simplicity.The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/kunzhan/SACN%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02188v1&entry.124074799=Read"},
{"title": "StructLDM: Structured Latent Diffusion for 3D Human Generation", "author": "Tao Hu and Fangzhou Hong and Ziwei Liu", "abstract": "  Recent 3D human generative models have achieved remarkable progress by\nlearning 3D-aware GANs from 2D images. However, existing 3D human generative\nmethods model humans in a compact 1D latent space, ignoring the articulated\nstructure and semantics of human body topology. In this paper, we explore more\nexpressive and higher-dimensional latent space for 3D human modeling and\npropose StructLDM, a diffusion-based unconditional 3D human generative model,\nwhich is learned from 2D images. StructLDM solves the challenges imposed due to\nthe high-dimensional growth of latent space with three key designs: 1) A\nsemantic structured latent space defined on the dense surface manifold of a\nstatistical human body template. 2) A structured 3D-aware auto-decoder that\nfactorizes the global latent space into several semantic body parts\nparameterized by a set of conditional structured local NeRFs anchored to the\nbody template, which embeds the properties learned from the 2D training data\nand can be decoded to render view-consistent humans under different poses and\nclothing styles. 3) A structured latent diffusion model for generative human\nappearance sampling. Extensive experiments validate StructLDM's\nstate-of-the-art generation performance and illustrate the expressiveness of\nthe structured latent space over the well-adopted 1D latent space. Notably,\nStructLDM enables different levels of controllable 3D human generation and\nediting, including pose/view/shape control, and high-level tasks including\ncompositional generations, part-aware clothing editing, 3D virtual try-on, etc.\nOur project page is at: https://taohuumd.github.io/projects/StructLDM/.\n", "link": "http://arxiv.org/abs/2404.01241v3", "date": "2024-07-02", "relevancy": 2.3931, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5984}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5984}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructLDM%3A%20Structured%20Latent%20Diffusion%20for%203D%20Human%20Generation&body=Title%3A%20StructLDM%3A%20Structured%20Latent%20Diffusion%20for%203D%20Human%20Generation%0AAuthor%3A%20Tao%20Hu%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recent%203D%20human%20generative%20models%20have%20achieved%20remarkable%20progress%20by%0Alearning%203D-aware%20GANs%20from%202D%20images.%20However%2C%20existing%203D%20human%20generative%0Amethods%20model%20humans%20in%20a%20compact%201D%20latent%20space%2C%20ignoring%20the%20articulated%0Astructure%20and%20semantics%20of%20human%20body%20topology.%20In%20this%20paper%2C%20we%20explore%20more%0Aexpressive%20and%20higher-dimensional%20latent%20space%20for%203D%20human%20modeling%20and%0Apropose%20StructLDM%2C%20a%20diffusion-based%20unconditional%203D%20human%20generative%20model%2C%0Awhich%20is%20learned%20from%202D%20images.%20StructLDM%20solves%20the%20challenges%20imposed%20due%20to%0Athe%20high-dimensional%20growth%20of%20latent%20space%20with%20three%20key%20designs%3A%201%29%20A%0Asemantic%20structured%20latent%20space%20defined%20on%20the%20dense%20surface%20manifold%20of%20a%0Astatistical%20human%20body%20template.%202%29%20A%20structured%203D-aware%20auto-decoder%20that%0Afactorizes%20the%20global%20latent%20space%20into%20several%20semantic%20body%20parts%0Aparameterized%20by%20a%20set%20of%20conditional%20structured%20local%20NeRFs%20anchored%20to%20the%0Abody%20template%2C%20which%20embeds%20the%20properties%20learned%20from%20the%202D%20training%20data%0Aand%20can%20be%20decoded%20to%20render%20view-consistent%20humans%20under%20different%20poses%20and%0Aclothing%20styles.%203%29%20A%20structured%20latent%20diffusion%20model%20for%20generative%20human%0Aappearance%20sampling.%20Extensive%20experiments%20validate%20StructLDM%27s%0Astate-of-the-art%20generation%20performance%20and%20illustrate%20the%20expressiveness%20of%0Athe%20structured%20latent%20space%20over%20the%20well-adopted%201D%20latent%20space.%20Notably%2C%0AStructLDM%20enables%20different%20levels%20of%20controllable%203D%20human%20generation%20and%0Aediting%2C%20including%20pose/view/shape%20control%2C%20and%20high-level%20tasks%20including%0Acompositional%20generations%2C%20part-aware%20clothing%20editing%2C%203D%20virtual%20try-on%2C%20etc.%0AOur%20project%20page%20is%20at%3A%20https%3A//taohuumd.github.io/projects/StructLDM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01241v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructLDM%253A%2520Structured%2520Latent%2520Diffusion%2520for%25203D%2520Human%2520Generation%26entry.906535625%3DTao%2520Hu%2520and%2520Fangzhou%2520Hong%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recent%25203D%2520human%2520generative%2520models%2520have%2520achieved%2520remarkable%2520progress%2520by%250Alearning%25203D-aware%2520GANs%2520from%25202D%2520images.%2520However%252C%2520existing%25203D%2520human%2520generative%250Amethods%2520model%2520humans%2520in%2520a%2520compact%25201D%2520latent%2520space%252C%2520ignoring%2520the%2520articulated%250Astructure%2520and%2520semantics%2520of%2520human%2520body%2520topology.%2520In%2520this%2520paper%252C%2520we%2520explore%2520more%250Aexpressive%2520and%2520higher-dimensional%2520latent%2520space%2520for%25203D%2520human%2520modeling%2520and%250Apropose%2520StructLDM%252C%2520a%2520diffusion-based%2520unconditional%25203D%2520human%2520generative%2520model%252C%250Awhich%2520is%2520learned%2520from%25202D%2520images.%2520StructLDM%2520solves%2520the%2520challenges%2520imposed%2520due%2520to%250Athe%2520high-dimensional%2520growth%2520of%2520latent%2520space%2520with%2520three%2520key%2520designs%253A%25201%2529%2520A%250Asemantic%2520structured%2520latent%2520space%2520defined%2520on%2520the%2520dense%2520surface%2520manifold%2520of%2520a%250Astatistical%2520human%2520body%2520template.%25202%2529%2520A%2520structured%25203D-aware%2520auto-decoder%2520that%250Afactorizes%2520the%2520global%2520latent%2520space%2520into%2520several%2520semantic%2520body%2520parts%250Aparameterized%2520by%2520a%2520set%2520of%2520conditional%2520structured%2520local%2520NeRFs%2520anchored%2520to%2520the%250Abody%2520template%252C%2520which%2520embeds%2520the%2520properties%2520learned%2520from%2520the%25202D%2520training%2520data%250Aand%2520can%2520be%2520decoded%2520to%2520render%2520view-consistent%2520humans%2520under%2520different%2520poses%2520and%250Aclothing%2520styles.%25203%2529%2520A%2520structured%2520latent%2520diffusion%2520model%2520for%2520generative%2520human%250Aappearance%2520sampling.%2520Extensive%2520experiments%2520validate%2520StructLDM%2527s%250Astate-of-the-art%2520generation%2520performance%2520and%2520illustrate%2520the%2520expressiveness%2520of%250Athe%2520structured%2520latent%2520space%2520over%2520the%2520well-adopted%25201D%2520latent%2520space.%2520Notably%252C%250AStructLDM%2520enables%2520different%2520levels%2520of%2520controllable%25203D%2520human%2520generation%2520and%250Aediting%252C%2520including%2520pose/view/shape%2520control%252C%2520and%2520high-level%2520tasks%2520including%250Acompositional%2520generations%252C%2520part-aware%2520clothing%2520editing%252C%25203D%2520virtual%2520try-on%252C%2520etc.%250AOur%2520project%2520page%2520is%2520at%253A%2520https%253A//taohuumd.github.io/projects/StructLDM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01241v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructLDM%3A%20Structured%20Latent%20Diffusion%20for%203D%20Human%20Generation&entry.906535625=Tao%20Hu%20and%20Fangzhou%20Hong%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recent%203D%20human%20generative%20models%20have%20achieved%20remarkable%20progress%20by%0Alearning%203D-aware%20GANs%20from%202D%20images.%20However%2C%20existing%203D%20human%20generative%0Amethods%20model%20humans%20in%20a%20compact%201D%20latent%20space%2C%20ignoring%20the%20articulated%0Astructure%20and%20semantics%20of%20human%20body%20topology.%20In%20this%20paper%2C%20we%20explore%20more%0Aexpressive%20and%20higher-dimensional%20latent%20space%20for%203D%20human%20modeling%20and%0Apropose%20StructLDM%2C%20a%20diffusion-based%20unconditional%203D%20human%20generative%20model%2C%0Awhich%20is%20learned%20from%202D%20images.%20StructLDM%20solves%20the%20challenges%20imposed%20due%20to%0Athe%20high-dimensional%20growth%20of%20latent%20space%20with%20three%20key%20designs%3A%201%29%20A%0Asemantic%20structured%20latent%20space%20defined%20on%20the%20dense%20surface%20manifold%20of%20a%0Astatistical%20human%20body%20template.%202%29%20A%20structured%203D-aware%20auto-decoder%20that%0Afactorizes%20the%20global%20latent%20space%20into%20several%20semantic%20body%20parts%0Aparameterized%20by%20a%20set%20of%20conditional%20structured%20local%20NeRFs%20anchored%20to%20the%0Abody%20template%2C%20which%20embeds%20the%20properties%20learned%20from%20the%202D%20training%20data%0Aand%20can%20be%20decoded%20to%20render%20view-consistent%20humans%20under%20different%20poses%20and%0Aclothing%20styles.%203%29%20A%20structured%20latent%20diffusion%20model%20for%20generative%20human%0Aappearance%20sampling.%20Extensive%20experiments%20validate%20StructLDM%27s%0Astate-of-the-art%20generation%20performance%20and%20illustrate%20the%20expressiveness%20of%0Athe%20structured%20latent%20space%20over%20the%20well-adopted%201D%20latent%20space.%20Notably%2C%0AStructLDM%20enables%20different%20levels%20of%20controllable%203D%20human%20generation%20and%0Aediting%2C%20including%20pose/view/shape%20control%2C%20and%20high-level%20tasks%20including%0Acompositional%20generations%2C%20part-aware%20clothing%20editing%2C%203D%20virtual%20try-on%2C%20etc.%0AOur%20project%20page%20is%20at%3A%20https%3A//taohuumd.github.io/projects/StructLDM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01241v3&entry.124074799=Read"},
{"title": "Towards Training Music Taggers on Synthetic Data", "author": "Nadine Kroher and Steven Manangu and Aggelos Pikrakis", "abstract": "  Most contemporary music tagging systems rely on large volumes of annotated\ndata. As an alternative, we investigate the extent to which synthetically\ngenerated music excerpts can improve tagging systems when only small annotated\ncollections are available. To this end, we release GTZAN-synth, a synthetic\ndataset that follows the taxonomy of the well-known GTZAN dataset while being\nten times larger in data volume. We first observe that simply adding this\nsynthetic dataset to the training split of GTZAN does not result into\nperformance improvements. We then proceed to investigating domain adaptation,\ntransfer learning and fine-tuning strategies for the task at hand and draw the\nconclusion that the last two options yield an increase in accuracy. Overall,\nthe proposed approach can be considered as a first guide in a promising field\nfor future research.\n", "link": "http://arxiv.org/abs/2407.02156v1", "date": "2024-07-02", "relevancy": 2.3874, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4863}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4769}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Training%20Music%20Taggers%20on%20Synthetic%20Data&body=Title%3A%20Towards%20Training%20Music%20Taggers%20on%20Synthetic%20Data%0AAuthor%3A%20Nadine%20Kroher%20and%20Steven%20Manangu%20and%20Aggelos%20Pikrakis%0AAbstract%3A%20%20%20Most%20contemporary%20music%20tagging%20systems%20rely%20on%20large%20volumes%20of%20annotated%0Adata.%20As%20an%20alternative%2C%20we%20investigate%20the%20extent%20to%20which%20synthetically%0Agenerated%20music%20excerpts%20can%20improve%20tagging%20systems%20when%20only%20small%20annotated%0Acollections%20are%20available.%20To%20this%20end%2C%20we%20release%20GTZAN-synth%2C%20a%20synthetic%0Adataset%20that%20follows%20the%20taxonomy%20of%20the%20well-known%20GTZAN%20dataset%20while%20being%0Aten%20times%20larger%20in%20data%20volume.%20We%20first%20observe%20that%20simply%20adding%20this%0Asynthetic%20dataset%20to%20the%20training%20split%20of%20GTZAN%20does%20not%20result%20into%0Aperformance%20improvements.%20We%20then%20proceed%20to%20investigating%20domain%20adaptation%2C%0Atransfer%20learning%20and%20fine-tuning%20strategies%20for%20the%20task%20at%20hand%20and%20draw%20the%0Aconclusion%20that%20the%20last%20two%20options%20yield%20an%20increase%20in%20accuracy.%20Overall%2C%0Athe%20proposed%20approach%20can%20be%20considered%20as%20a%20first%20guide%20in%20a%20promising%20field%0Afor%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Training%2520Music%2520Taggers%2520on%2520Synthetic%2520Data%26entry.906535625%3DNadine%2520Kroher%2520and%2520Steven%2520Manangu%2520and%2520Aggelos%2520Pikrakis%26entry.1292438233%3D%2520%2520Most%2520contemporary%2520music%2520tagging%2520systems%2520rely%2520on%2520large%2520volumes%2520of%2520annotated%250Adata.%2520As%2520an%2520alternative%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520synthetically%250Agenerated%2520music%2520excerpts%2520can%2520improve%2520tagging%2520systems%2520when%2520only%2520small%2520annotated%250Acollections%2520are%2520available.%2520To%2520this%2520end%252C%2520we%2520release%2520GTZAN-synth%252C%2520a%2520synthetic%250Adataset%2520that%2520follows%2520the%2520taxonomy%2520of%2520the%2520well-known%2520GTZAN%2520dataset%2520while%2520being%250Aten%2520times%2520larger%2520in%2520data%2520volume.%2520We%2520first%2520observe%2520that%2520simply%2520adding%2520this%250Asynthetic%2520dataset%2520to%2520the%2520training%2520split%2520of%2520GTZAN%2520does%2520not%2520result%2520into%250Aperformance%2520improvements.%2520We%2520then%2520proceed%2520to%2520investigating%2520domain%2520adaptation%252C%250Atransfer%2520learning%2520and%2520fine-tuning%2520strategies%2520for%2520the%2520task%2520at%2520hand%2520and%2520draw%2520the%250Aconclusion%2520that%2520the%2520last%2520two%2520options%2520yield%2520an%2520increase%2520in%2520accuracy.%2520Overall%252C%250Athe%2520proposed%2520approach%2520can%2520be%2520considered%2520as%2520a%2520first%2520guide%2520in%2520a%2520promising%2520field%250Afor%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Training%20Music%20Taggers%20on%20Synthetic%20Data&entry.906535625=Nadine%20Kroher%20and%20Steven%20Manangu%20and%20Aggelos%20Pikrakis&entry.1292438233=%20%20Most%20contemporary%20music%20tagging%20systems%20rely%20on%20large%20volumes%20of%20annotated%0Adata.%20As%20an%20alternative%2C%20we%20investigate%20the%20extent%20to%20which%20synthetically%0Agenerated%20music%20excerpts%20can%20improve%20tagging%20systems%20when%20only%20small%20annotated%0Acollections%20are%20available.%20To%20this%20end%2C%20we%20release%20GTZAN-synth%2C%20a%20synthetic%0Adataset%20that%20follows%20the%20taxonomy%20of%20the%20well-known%20GTZAN%20dataset%20while%20being%0Aten%20times%20larger%20in%20data%20volume.%20We%20first%20observe%20that%20simply%20adding%20this%0Asynthetic%20dataset%20to%20the%20training%20split%20of%20GTZAN%20does%20not%20result%20into%0Aperformance%20improvements.%20We%20then%20proceed%20to%20investigating%20domain%20adaptation%2C%0Atransfer%20learning%20and%20fine-tuning%20strategies%20for%20the%20task%20at%20hand%20and%20draw%20the%0Aconclusion%20that%20the%20last%20two%20options%20yield%20an%20increase%20in%20accuracy.%20Overall%2C%0Athe%20proposed%20approach%20can%20be%20considered%20as%20a%20first%20guide%20in%20a%20promising%20field%0Afor%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02156v1&entry.124074799=Read"},
{"title": "SINCERE: Supervised Information Noise-Contrastive Estimation REvisited", "author": "Patrick Feeney and Michael C. Hughes", "abstract": "  The information noise-contrastive estimation (InfoNCE) loss function provides\nthe basis of many self-supervised deep learning methods due to its strong\nempirical results and theoretic motivation. Previous work suggests a supervised\ncontrastive (SupCon) loss to extend InfoNCE to learn from available class\nlabels. This SupCon loss has been widely-used due to reports of good empirical\nperformance. However, in this work we find that the prior SupCon loss\nformulation has questionable justification because it can encourage some images\nfrom the same class to repel one another in the learned embedding space. This\nproblematic intra-class repulsion gets worse as the number of images sharing\none class label increases. We propose the Supervised InfoNCE REvisited\n(SINCERE) loss as a theoretically-justified supervised extension of InfoNCE\nthat eliminates intra-class repulsion. Experiments show that SINCERE leads to\nbetter separation of embeddings from different classes and improves transfer\nlearning classification accuracy. We additionally utilize probabilistic\nmodeling to derive an information-theoretic bound that relates SINCERE loss to\nthe symmeterized KL divergence between data-generating distributions for a\ntarget class and all other classes.\n", "link": "http://arxiv.org/abs/2309.14277v3", "date": "2024-07-02", "relevancy": 2.383, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4868}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4731}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SINCERE%3A%20Supervised%20Information%20Noise-Contrastive%20Estimation%20REvisited&body=Title%3A%20SINCERE%3A%20Supervised%20Information%20Noise-Contrastive%20Estimation%20REvisited%0AAuthor%3A%20Patrick%20Feeney%20and%20Michael%20C.%20Hughes%0AAbstract%3A%20%20%20The%20information%20noise-contrastive%20estimation%20%28InfoNCE%29%20loss%20function%20provides%0Athe%20basis%20of%20many%20self-supervised%20deep%20learning%20methods%20due%20to%20its%20strong%0Aempirical%20results%20and%20theoretic%20motivation.%20Previous%20work%20suggests%20a%20supervised%0Acontrastive%20%28SupCon%29%20loss%20to%20extend%20InfoNCE%20to%20learn%20from%20available%20class%0Alabels.%20This%20SupCon%20loss%20has%20been%20widely-used%20due%20to%20reports%20of%20good%20empirical%0Aperformance.%20However%2C%20in%20this%20work%20we%20find%20that%20the%20prior%20SupCon%20loss%0Aformulation%20has%20questionable%20justification%20because%20it%20can%20encourage%20some%20images%0Afrom%20the%20same%20class%20to%20repel%20one%20another%20in%20the%20learned%20embedding%20space.%20This%0Aproblematic%20intra-class%20repulsion%20gets%20worse%20as%20the%20number%20of%20images%20sharing%0Aone%20class%20label%20increases.%20We%20propose%20the%20Supervised%20InfoNCE%20REvisited%0A%28SINCERE%29%20loss%20as%20a%20theoretically-justified%20supervised%20extension%20of%20InfoNCE%0Athat%20eliminates%20intra-class%20repulsion.%20Experiments%20show%20that%20SINCERE%20leads%20to%0Abetter%20separation%20of%20embeddings%20from%20different%20classes%20and%20improves%20transfer%0Alearning%20classification%20accuracy.%20We%20additionally%20utilize%20probabilistic%0Amodeling%20to%20derive%20an%20information-theoretic%20bound%20that%20relates%20SINCERE%20loss%20to%0Athe%20symmeterized%20KL%20divergence%20between%20data-generating%20distributions%20for%20a%0Atarget%20class%20and%20all%20other%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14277v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSINCERE%253A%2520Supervised%2520Information%2520Noise-Contrastive%2520Estimation%2520REvisited%26entry.906535625%3DPatrick%2520Feeney%2520and%2520Michael%2520C.%2520Hughes%26entry.1292438233%3D%2520%2520The%2520information%2520noise-contrastive%2520estimation%2520%2528InfoNCE%2529%2520loss%2520function%2520provides%250Athe%2520basis%2520of%2520many%2520self-supervised%2520deep%2520learning%2520methods%2520due%2520to%2520its%2520strong%250Aempirical%2520results%2520and%2520theoretic%2520motivation.%2520Previous%2520work%2520suggests%2520a%2520supervised%250Acontrastive%2520%2528SupCon%2529%2520loss%2520to%2520extend%2520InfoNCE%2520to%2520learn%2520from%2520available%2520class%250Alabels.%2520This%2520SupCon%2520loss%2520has%2520been%2520widely-used%2520due%2520to%2520reports%2520of%2520good%2520empirical%250Aperformance.%2520However%252C%2520in%2520this%2520work%2520we%2520find%2520that%2520the%2520prior%2520SupCon%2520loss%250Aformulation%2520has%2520questionable%2520justification%2520because%2520it%2520can%2520encourage%2520some%2520images%250Afrom%2520the%2520same%2520class%2520to%2520repel%2520one%2520another%2520in%2520the%2520learned%2520embedding%2520space.%2520This%250Aproblematic%2520intra-class%2520repulsion%2520gets%2520worse%2520as%2520the%2520number%2520of%2520images%2520sharing%250Aone%2520class%2520label%2520increases.%2520We%2520propose%2520the%2520Supervised%2520InfoNCE%2520REvisited%250A%2528SINCERE%2529%2520loss%2520as%2520a%2520theoretically-justified%2520supervised%2520extension%2520of%2520InfoNCE%250Athat%2520eliminates%2520intra-class%2520repulsion.%2520Experiments%2520show%2520that%2520SINCERE%2520leads%2520to%250Abetter%2520separation%2520of%2520embeddings%2520from%2520different%2520classes%2520and%2520improves%2520transfer%250Alearning%2520classification%2520accuracy.%2520We%2520additionally%2520utilize%2520probabilistic%250Amodeling%2520to%2520derive%2520an%2520information-theoretic%2520bound%2520that%2520relates%2520SINCERE%2520loss%2520to%250Athe%2520symmeterized%2520KL%2520divergence%2520between%2520data-generating%2520distributions%2520for%2520a%250Atarget%2520class%2520and%2520all%2520other%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14277v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SINCERE%3A%20Supervised%20Information%20Noise-Contrastive%20Estimation%20REvisited&entry.906535625=Patrick%20Feeney%20and%20Michael%20C.%20Hughes&entry.1292438233=%20%20The%20information%20noise-contrastive%20estimation%20%28InfoNCE%29%20loss%20function%20provides%0Athe%20basis%20of%20many%20self-supervised%20deep%20learning%20methods%20due%20to%20its%20strong%0Aempirical%20results%20and%20theoretic%20motivation.%20Previous%20work%20suggests%20a%20supervised%0Acontrastive%20%28SupCon%29%20loss%20to%20extend%20InfoNCE%20to%20learn%20from%20available%20class%0Alabels.%20This%20SupCon%20loss%20has%20been%20widely-used%20due%20to%20reports%20of%20good%20empirical%0Aperformance.%20However%2C%20in%20this%20work%20we%20find%20that%20the%20prior%20SupCon%20loss%0Aformulation%20has%20questionable%20justification%20because%20it%20can%20encourage%20some%20images%0Afrom%20the%20same%20class%20to%20repel%20one%20another%20in%20the%20learned%20embedding%20space.%20This%0Aproblematic%20intra-class%20repulsion%20gets%20worse%20as%20the%20number%20of%20images%20sharing%0Aone%20class%20label%20increases.%20We%20propose%20the%20Supervised%20InfoNCE%20REvisited%0A%28SINCERE%29%20loss%20as%20a%20theoretically-justified%20supervised%20extension%20of%20InfoNCE%0Athat%20eliminates%20intra-class%20repulsion.%20Experiments%20show%20that%20SINCERE%20leads%20to%0Abetter%20separation%20of%20embeddings%20from%20different%20classes%20and%20improves%20transfer%0Alearning%20classification%20accuracy.%20We%20additionally%20utilize%20probabilistic%0Amodeling%20to%20derive%20an%20information-theoretic%20bound%20that%20relates%20SINCERE%20loss%20to%0Athe%20symmeterized%20KL%20divergence%20between%20data-generating%20distributions%20for%20a%0Atarget%20class%20and%20all%20other%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14277v3&entry.124074799=Read"},
{"title": "GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object\n  Detection", "author": "Ziying Song and Lei Yang and Shaoqing Xu and Lin Liu and Dongyang Xu and Caiyan Jia and Feiyang Jia and Li Wang", "abstract": "  Integrating LiDAR and camera information into Bird's-Eye-View (BEV)\nrepresentation has emerged as a crucial aspect of 3D object detection in\nautonomous driving. However, existing methods are susceptible to the inaccurate\ncalibration relationship between LiDAR and the camera sensor. Such inaccuracies\nresult in errors in depth estimation for the camera branch, ultimately causing\nmisalignment between LiDAR and camera BEV features. In this work, we propose a\nrobust fusion framework called Graph BEV. Addressing errors caused by\ninaccurate point cloud projection, we introduce a Local Align module that\nemploys neighbor-aware depth features via Graph matching. Additionally, we\npropose a Global Align module to rectify the misalignment between LiDAR and\ncamera BEV features. Our Graph BEV framework achieves state-of-the-art\nperformance, with an mAP of 70.1\\%, surpassing BEV Fusion by 1.6\\% on the\nnuscenes validation set. Importantly, our Graph BEV outperforms BEV Fusion by\n8.3\\% under conditions with misalignment noise.\n", "link": "http://arxiv.org/abs/2403.11848v3", "date": "2024-07-02", "relevancy": 2.3531, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6105}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5729}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphBEV%3A%20Towards%20Robust%20BEV%20Feature%20Alignment%20for%20Multi-Modal%203D%20Object%0A%20%20Detection&body=Title%3A%20GraphBEV%3A%20Towards%20Robust%20BEV%20Feature%20Alignment%20for%20Multi-Modal%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Ziying%20Song%20and%20Lei%20Yang%20and%20Shaoqing%20Xu%20and%20Lin%20Liu%20and%20Dongyang%20Xu%20and%20Caiyan%20Jia%20and%20Feiyang%20Jia%20and%20Li%20Wang%0AAbstract%3A%20%20%20Integrating%20LiDAR%20and%20camera%20information%20into%20Bird%27s-Eye-View%20%28BEV%29%0Arepresentation%20has%20emerged%20as%20a%20crucial%20aspect%20of%203D%20object%20detection%20in%0Aautonomous%20driving.%20However%2C%20existing%20methods%20are%20susceptible%20to%20the%20inaccurate%0Acalibration%20relationship%20between%20LiDAR%20and%20the%20camera%20sensor.%20Such%20inaccuracies%0Aresult%20in%20errors%20in%20depth%20estimation%20for%20the%20camera%20branch%2C%20ultimately%20causing%0Amisalignment%20between%20LiDAR%20and%20camera%20BEV%20features.%20In%20this%20work%2C%20we%20propose%20a%0Arobust%20fusion%20framework%20called%20Graph%20BEV.%20Addressing%20errors%20caused%20by%0Ainaccurate%20point%20cloud%20projection%2C%20we%20introduce%20a%20Local%20Align%20module%20that%0Aemploys%20neighbor-aware%20depth%20features%20via%20Graph%20matching.%20Additionally%2C%20we%0Apropose%20a%20Global%20Align%20module%20to%20rectify%20the%20misalignment%20between%20LiDAR%20and%0Acamera%20BEV%20features.%20Our%20Graph%20BEV%20framework%20achieves%20state-of-the-art%0Aperformance%2C%20with%20an%20mAP%20of%2070.1%5C%25%2C%20surpassing%20BEV%20Fusion%20by%201.6%5C%25%20on%20the%0Anuscenes%20validation%20set.%20Importantly%2C%20our%20Graph%20BEV%20outperforms%20BEV%20Fusion%20by%0A8.3%5C%25%20under%20conditions%20with%20misalignment%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11848v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphBEV%253A%2520Towards%2520Robust%2520BEV%2520Feature%2520Alignment%2520for%2520Multi-Modal%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DZiying%2520Song%2520and%2520Lei%2520Yang%2520and%2520Shaoqing%2520Xu%2520and%2520Lin%2520Liu%2520and%2520Dongyang%2520Xu%2520and%2520Caiyan%2520Jia%2520and%2520Feiyang%2520Jia%2520and%2520Li%2520Wang%26entry.1292438233%3D%2520%2520Integrating%2520LiDAR%2520and%2520camera%2520information%2520into%2520Bird%2527s-Eye-View%2520%2528BEV%2529%250Arepresentation%2520has%2520emerged%2520as%2520a%2520crucial%2520aspect%2520of%25203D%2520object%2520detection%2520in%250Aautonomous%2520driving.%2520However%252C%2520existing%2520methods%2520are%2520susceptible%2520to%2520the%2520inaccurate%250Acalibration%2520relationship%2520between%2520LiDAR%2520and%2520the%2520camera%2520sensor.%2520Such%2520inaccuracies%250Aresult%2520in%2520errors%2520in%2520depth%2520estimation%2520for%2520the%2520camera%2520branch%252C%2520ultimately%2520causing%250Amisalignment%2520between%2520LiDAR%2520and%2520camera%2520BEV%2520features.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Arobust%2520fusion%2520framework%2520called%2520Graph%2520BEV.%2520Addressing%2520errors%2520caused%2520by%250Ainaccurate%2520point%2520cloud%2520projection%252C%2520we%2520introduce%2520a%2520Local%2520Align%2520module%2520that%250Aemploys%2520neighbor-aware%2520depth%2520features%2520via%2520Graph%2520matching.%2520Additionally%252C%2520we%250Apropose%2520a%2520Global%2520Align%2520module%2520to%2520rectify%2520the%2520misalignment%2520between%2520LiDAR%2520and%250Acamera%2520BEV%2520features.%2520Our%2520Graph%2520BEV%2520framework%2520achieves%2520state-of-the-art%250Aperformance%252C%2520with%2520an%2520mAP%2520of%252070.1%255C%2525%252C%2520surpassing%2520BEV%2520Fusion%2520by%25201.6%255C%2525%2520on%2520the%250Anuscenes%2520validation%2520set.%2520Importantly%252C%2520our%2520Graph%2520BEV%2520outperforms%2520BEV%2520Fusion%2520by%250A8.3%255C%2525%2520under%2520conditions%2520with%2520misalignment%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11848v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphBEV%3A%20Towards%20Robust%20BEV%20Feature%20Alignment%20for%20Multi-Modal%203D%20Object%0A%20%20Detection&entry.906535625=Ziying%20Song%20and%20Lei%20Yang%20and%20Shaoqing%20Xu%20and%20Lin%20Liu%20and%20Dongyang%20Xu%20and%20Caiyan%20Jia%20and%20Feiyang%20Jia%20and%20Li%20Wang&entry.1292438233=%20%20Integrating%20LiDAR%20and%20camera%20information%20into%20Bird%27s-Eye-View%20%28BEV%29%0Arepresentation%20has%20emerged%20as%20a%20crucial%20aspect%20of%203D%20object%20detection%20in%0Aautonomous%20driving.%20However%2C%20existing%20methods%20are%20susceptible%20to%20the%20inaccurate%0Acalibration%20relationship%20between%20LiDAR%20and%20the%20camera%20sensor.%20Such%20inaccuracies%0Aresult%20in%20errors%20in%20depth%20estimation%20for%20the%20camera%20branch%2C%20ultimately%20causing%0Amisalignment%20between%20LiDAR%20and%20camera%20BEV%20features.%20In%20this%20work%2C%20we%20propose%20a%0Arobust%20fusion%20framework%20called%20Graph%20BEV.%20Addressing%20errors%20caused%20by%0Ainaccurate%20point%20cloud%20projection%2C%20we%20introduce%20a%20Local%20Align%20module%20that%0Aemploys%20neighbor-aware%20depth%20features%20via%20Graph%20matching.%20Additionally%2C%20we%0Apropose%20a%20Global%20Align%20module%20to%20rectify%20the%20misalignment%20between%20LiDAR%20and%0Acamera%20BEV%20features.%20Our%20Graph%20BEV%20framework%20achieves%20state-of-the-art%0Aperformance%2C%20with%20an%20mAP%20of%2070.1%5C%25%2C%20surpassing%20BEV%20Fusion%20by%201.6%5C%25%20on%20the%0Anuscenes%20validation%20set.%20Importantly%2C%20our%20Graph%20BEV%20outperforms%20BEV%20Fusion%20by%0A8.3%5C%25%20under%20conditions%20with%20misalignment%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11848v3&entry.124074799=Read"},
{"title": "BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event\n  Stream", "author": "Wenpu Li and Pian Wan and Peng Wang and Jinhang Li and Yi Zhou and Peidong Liu", "abstract": "  Neural implicit representation of visual scenes has attracted a lot of\nattention in recent research of computer vision and graphics. Most prior\nmethods focus on how to reconstruct 3D scene representation from a set of\nimages. In this work, we demonstrate the possibility to recover the neural\nradiance fields (NeRF) from a single blurry image and its corresponding event\nstream. We model the camera motion with a cubic B-Spline in SE(3) space. Both\nthe blurry image and the brightness change within a time interval, can then be\nsynthesized from the 3D scene representation given the 6-DoF poses interpolated\nfrom the cubic B-Spline. Our method can jointly learn both the implicit neural\nscene representation and recover the camera motion by minimizing the\ndifferences between the synthesized data and the real measurements without\npre-computed camera poses from COLMAP. We evaluate the proposed method with\nboth synthetic and real datasets. The experimental results demonstrate that we\nare able to render view-consistent latent sharp images from the learned NeRF\nand bring a blurry image alive in high quality. Code and data are available at\nhttps://github.com/WU-CVGL/BeNeRF.\n", "link": "http://arxiv.org/abs/2407.02174v1", "date": "2024-07-02", "relevancy": 2.3252, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5839}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream&body=Title%3A%20BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream%0AAuthor%3A%20Wenpu%20Li%20and%20Pian%20Wan%20and%20Peng%20Wang%20and%20Jinhang%20Li%20and%20Yi%20Zhou%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Neural%20implicit%20representation%20of%20visual%20scenes%20has%20attracted%20a%20lot%20of%0Aattention%20in%20recent%20research%20of%20computer%20vision%20and%20graphics.%20Most%20prior%0Amethods%20focus%20on%20how%20to%20reconstruct%203D%20scene%20representation%20from%20a%20set%20of%0Aimages.%20In%20this%20work%2C%20we%20demonstrate%20the%20possibility%20to%20recover%20the%20neural%0Aradiance%20fields%20%28NeRF%29%20from%20a%20single%20blurry%20image%20and%20its%20corresponding%20event%0Astream.%20We%20model%20the%20camera%20motion%20with%20a%20cubic%20B-Spline%20in%20SE%283%29%20space.%20Both%0Athe%20blurry%20image%20and%20the%20brightness%20change%20within%20a%20time%20interval%2C%20can%20then%20be%0Asynthesized%20from%20the%203D%20scene%20representation%20given%20the%206-DoF%20poses%20interpolated%0Afrom%20the%20cubic%20B-Spline.%20Our%20method%20can%20jointly%20learn%20both%20the%20implicit%20neural%0Ascene%20representation%20and%20recover%20the%20camera%20motion%20by%20minimizing%20the%0Adifferences%20between%20the%20synthesized%20data%20and%20the%20real%20measurements%20without%0Apre-computed%20camera%20poses%20from%20COLMAP.%20We%20evaluate%20the%20proposed%20method%20with%0Aboth%20synthetic%20and%20real%20datasets.%20The%20experimental%20results%20demonstrate%20that%20we%0Aare%20able%20to%20render%20view-consistent%20latent%20sharp%20images%20from%20the%20learned%20NeRF%0Aand%20bring%20a%20blurry%20image%20alive%20in%20high%20quality.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/WU-CVGL/BeNeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeNeRF%253A%2520Neural%2520Radiance%2520Fields%2520from%2520a%2520Single%2520Blurry%2520Image%2520and%2520Event%250A%2520%2520Stream%26entry.906535625%3DWenpu%2520Li%2520and%2520Pian%2520Wan%2520and%2520Peng%2520Wang%2520and%2520Jinhang%2520Li%2520and%2520Yi%2520Zhou%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representation%2520of%2520visual%2520scenes%2520has%2520attracted%2520a%2520lot%2520of%250Aattention%2520in%2520recent%2520research%2520of%2520computer%2520vision%2520and%2520graphics.%2520Most%2520prior%250Amethods%2520focus%2520on%2520how%2520to%2520reconstruct%25203D%2520scene%2520representation%2520from%2520a%2520set%2520of%250Aimages.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520the%2520possibility%2520to%2520recover%2520the%2520neural%250Aradiance%2520fields%2520%2528NeRF%2529%2520from%2520a%2520single%2520blurry%2520image%2520and%2520its%2520corresponding%2520event%250Astream.%2520We%2520model%2520the%2520camera%2520motion%2520with%2520a%2520cubic%2520B-Spline%2520in%2520SE%25283%2529%2520space.%2520Both%250Athe%2520blurry%2520image%2520and%2520the%2520brightness%2520change%2520within%2520a%2520time%2520interval%252C%2520can%2520then%2520be%250Asynthesized%2520from%2520the%25203D%2520scene%2520representation%2520given%2520the%25206-DoF%2520poses%2520interpolated%250Afrom%2520the%2520cubic%2520B-Spline.%2520Our%2520method%2520can%2520jointly%2520learn%2520both%2520the%2520implicit%2520neural%250Ascene%2520representation%2520and%2520recover%2520the%2520camera%2520motion%2520by%2520minimizing%2520the%250Adifferences%2520between%2520the%2520synthesized%2520data%2520and%2520the%2520real%2520measurements%2520without%250Apre-computed%2520camera%2520poses%2520from%2520COLMAP.%2520We%2520evaluate%2520the%2520proposed%2520method%2520with%250Aboth%2520synthetic%2520and%2520real%2520datasets.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520we%250Aare%2520able%2520to%2520render%2520view-consistent%2520latent%2520sharp%2520images%2520from%2520the%2520learned%2520NeRF%250Aand%2520bring%2520a%2520blurry%2520image%2520alive%2520in%2520high%2520quality.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/WU-CVGL/BeNeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream&entry.906535625=Wenpu%20Li%20and%20Pian%20Wan%20and%20Peng%20Wang%20and%20Jinhang%20Li%20and%20Yi%20Zhou%20and%20Peidong%20Liu&entry.1292438233=%20%20Neural%20implicit%20representation%20of%20visual%20scenes%20has%20attracted%20a%20lot%20of%0Aattention%20in%20recent%20research%20of%20computer%20vision%20and%20graphics.%20Most%20prior%0Amethods%20focus%20on%20how%20to%20reconstruct%203D%20scene%20representation%20from%20a%20set%20of%0Aimages.%20In%20this%20work%2C%20we%20demonstrate%20the%20possibility%20to%20recover%20the%20neural%0Aradiance%20fields%20%28NeRF%29%20from%20a%20single%20blurry%20image%20and%20its%20corresponding%20event%0Astream.%20We%20model%20the%20camera%20motion%20with%20a%20cubic%20B-Spline%20in%20SE%283%29%20space.%20Both%0Athe%20blurry%20image%20and%20the%20brightness%20change%20within%20a%20time%20interval%2C%20can%20then%20be%0Asynthesized%20from%20the%203D%20scene%20representation%20given%20the%206-DoF%20poses%20interpolated%0Afrom%20the%20cubic%20B-Spline.%20Our%20method%20can%20jointly%20learn%20both%20the%20implicit%20neural%0Ascene%20representation%20and%20recover%20the%20camera%20motion%20by%20minimizing%20the%0Adifferences%20between%20the%20synthesized%20data%20and%20the%20real%20measurements%20without%0Apre-computed%20camera%20poses%20from%20COLMAP.%20We%20evaluate%20the%20proposed%20method%20with%0Aboth%20synthetic%20and%20real%20datasets.%20The%20experimental%20results%20demonstrate%20that%20we%0Aare%20able%20to%20render%20view-consistent%20latent%20sharp%20images%20from%20the%20learned%20NeRF%0Aand%20bring%20a%20blurry%20image%20alive%20in%20high%20quality.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/WU-CVGL/BeNeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02174v1&entry.124074799=Read"},
{"title": "Real HSI-MSI-PAN image dataset for the\n  hyperspectral/multi-spectral/panchromatic image fusion and super-resolution\n  fields", "author": "Shuangliang Li", "abstract": "  Nowadays, most of the hyperspectral image (HSI) fusion experiments are based\non simulated datasets to compare different fusion methods. However, most of the\nspectral response functions and spatial downsampling functions used to create\nthe simulated datasets are not entirely accurate, resulting in deviations in\nspatial and spectral features between the generated images for fusion and the\nreal images for fusion. This reduces the credibility of the fusion algorithm,\ncausing unfairness in the comparison between different algorithms and hindering\nthe development of the field of hyperspectral image fusion. Therefore, we\nrelease a real HSI/MSI/PAN image dataset to promote the development of the\nfield of hyperspectral image fusion. These three images are spatially\nregistered, meaning fusion can be performed between HSI and MSI, HSI and PAN\nimage, MSI and PAN image, as well as among HSI, MSI, and PAN image. This real\ndataset could be available at https://aistudio.baidu.com/datasetdetail/281612.\nThe related code to process the data could be available at\nhttps://github.com/rs-lsl/CSSNet.\n", "link": "http://arxiv.org/abs/2407.02387v1", "date": "2024-07-02", "relevancy": 2.3179, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.464}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.464}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real%20HSI-MSI-PAN%20image%20dataset%20for%20the%0A%20%20hyperspectral/multi-spectral/panchromatic%20image%20fusion%20and%20super-resolution%0A%20%20fields&body=Title%3A%20Real%20HSI-MSI-PAN%20image%20dataset%20for%20the%0A%20%20hyperspectral/multi-spectral/panchromatic%20image%20fusion%20and%20super-resolution%0A%20%20fields%0AAuthor%3A%20Shuangliang%20Li%0AAbstract%3A%20%20%20Nowadays%2C%20most%20of%20the%20hyperspectral%20image%20%28HSI%29%20fusion%20experiments%20are%20based%0Aon%20simulated%20datasets%20to%20compare%20different%20fusion%20methods.%20However%2C%20most%20of%20the%0Aspectral%20response%20functions%20and%20spatial%20downsampling%20functions%20used%20to%20create%0Athe%20simulated%20datasets%20are%20not%20entirely%20accurate%2C%20resulting%20in%20deviations%20in%0Aspatial%20and%20spectral%20features%20between%20the%20generated%20images%20for%20fusion%20and%20the%0Areal%20images%20for%20fusion.%20This%20reduces%20the%20credibility%20of%20the%20fusion%20algorithm%2C%0Acausing%20unfairness%20in%20the%20comparison%20between%20different%20algorithms%20and%20hindering%0Athe%20development%20of%20the%20field%20of%20hyperspectral%20image%20fusion.%20Therefore%2C%20we%0Arelease%20a%20real%20HSI/MSI/PAN%20image%20dataset%20to%20promote%20the%20development%20of%20the%0Afield%20of%20hyperspectral%20image%20fusion.%20These%20three%20images%20are%20spatially%0Aregistered%2C%20meaning%20fusion%20can%20be%20performed%20between%20HSI%20and%20MSI%2C%20HSI%20and%20PAN%0Aimage%2C%20MSI%20and%20PAN%20image%2C%20as%20well%20as%20among%20HSI%2C%20MSI%2C%20and%20PAN%20image.%20This%20real%0Adataset%20could%20be%20available%20at%20https%3A//aistudio.baidu.com/datasetdetail/281612.%0AThe%20related%20code%20to%20process%20the%20data%20could%20be%20available%20at%0Ahttps%3A//github.com/rs-lsl/CSSNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal%2520HSI-MSI-PAN%2520image%2520dataset%2520for%2520the%250A%2520%2520hyperspectral/multi-spectral/panchromatic%2520image%2520fusion%2520and%2520super-resolution%250A%2520%2520fields%26entry.906535625%3DShuangliang%2520Li%26entry.1292438233%3D%2520%2520Nowadays%252C%2520most%2520of%2520the%2520hyperspectral%2520image%2520%2528HSI%2529%2520fusion%2520experiments%2520are%2520based%250Aon%2520simulated%2520datasets%2520to%2520compare%2520different%2520fusion%2520methods.%2520However%252C%2520most%2520of%2520the%250Aspectral%2520response%2520functions%2520and%2520spatial%2520downsampling%2520functions%2520used%2520to%2520create%250Athe%2520simulated%2520datasets%2520are%2520not%2520entirely%2520accurate%252C%2520resulting%2520in%2520deviations%2520in%250Aspatial%2520and%2520spectral%2520features%2520between%2520the%2520generated%2520images%2520for%2520fusion%2520and%2520the%250Areal%2520images%2520for%2520fusion.%2520This%2520reduces%2520the%2520credibility%2520of%2520the%2520fusion%2520algorithm%252C%250Acausing%2520unfairness%2520in%2520the%2520comparison%2520between%2520different%2520algorithms%2520and%2520hindering%250Athe%2520development%2520of%2520the%2520field%2520of%2520hyperspectral%2520image%2520fusion.%2520Therefore%252C%2520we%250Arelease%2520a%2520real%2520HSI/MSI/PAN%2520image%2520dataset%2520to%2520promote%2520the%2520development%2520of%2520the%250Afield%2520of%2520hyperspectral%2520image%2520fusion.%2520These%2520three%2520images%2520are%2520spatially%250Aregistered%252C%2520meaning%2520fusion%2520can%2520be%2520performed%2520between%2520HSI%2520and%2520MSI%252C%2520HSI%2520and%2520PAN%250Aimage%252C%2520MSI%2520and%2520PAN%2520image%252C%2520as%2520well%2520as%2520among%2520HSI%252C%2520MSI%252C%2520and%2520PAN%2520image.%2520This%2520real%250Adataset%2520could%2520be%2520available%2520at%2520https%253A//aistudio.baidu.com/datasetdetail/281612.%250AThe%2520related%2520code%2520to%2520process%2520the%2520data%2520could%2520be%2520available%2520at%250Ahttps%253A//github.com/rs-lsl/CSSNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20HSI-MSI-PAN%20image%20dataset%20for%20the%0A%20%20hyperspectral/multi-spectral/panchromatic%20image%20fusion%20and%20super-resolution%0A%20%20fields&entry.906535625=Shuangliang%20Li&entry.1292438233=%20%20Nowadays%2C%20most%20of%20the%20hyperspectral%20image%20%28HSI%29%20fusion%20experiments%20are%20based%0Aon%20simulated%20datasets%20to%20compare%20different%20fusion%20methods.%20However%2C%20most%20of%20the%0Aspectral%20response%20functions%20and%20spatial%20downsampling%20functions%20used%20to%20create%0Athe%20simulated%20datasets%20are%20not%20entirely%20accurate%2C%20resulting%20in%20deviations%20in%0Aspatial%20and%20spectral%20features%20between%20the%20generated%20images%20for%20fusion%20and%20the%0Areal%20images%20for%20fusion.%20This%20reduces%20the%20credibility%20of%20the%20fusion%20algorithm%2C%0Acausing%20unfairness%20in%20the%20comparison%20between%20different%20algorithms%20and%20hindering%0Athe%20development%20of%20the%20field%20of%20hyperspectral%20image%20fusion.%20Therefore%2C%20we%0Arelease%20a%20real%20HSI/MSI/PAN%20image%20dataset%20to%20promote%20the%20development%20of%20the%0Afield%20of%20hyperspectral%20image%20fusion.%20These%20three%20images%20are%20spatially%0Aregistered%2C%20meaning%20fusion%20can%20be%20performed%20between%20HSI%20and%20MSI%2C%20HSI%20and%20PAN%0Aimage%2C%20MSI%20and%20PAN%20image%2C%20as%20well%20as%20among%20HSI%2C%20MSI%2C%20and%20PAN%20image.%20This%20real%0Adataset%20could%20be%20available%20at%20https%3A//aistudio.baidu.com/datasetdetail/281612.%0AThe%20related%20code%20to%20process%20the%20data%20could%20be%20available%20at%0Ahttps%3A//github.com/rs-lsl/CSSNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02387v1&entry.124074799=Read"},
{"title": "Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D\n  Objects", "author": "Raphael Bensadoun and Yanir Kleiman and Idan Azuri and Omri Harosh and Andrea Vedaldi and Natalia Neverova and Oran Gafni", "abstract": "  The recent availability and adaptability of text-to-image models has sparked\na new era in many related domains that benefit from the learned text priors as\nwell as high-quality and fast generation capabilities, one of which is texture\ngeneration for 3D objects. Although recent texture generation methods achieve\nimpressive results by using text-to-image networks, the combination of global\nconsistency, quality, and speed, which is crucial for advancing texture\ngeneration to real-world applications, remains elusive. To that end, we\nintroduce Meta 3D TextureGen: a new feedforward method comprised of two\nsequential networks aimed at generating high-quality and globally consistent\ntextures for arbitrary geometries of any complexity degree in less than 20\nseconds. Our method achieves state-of-the-art results in quality and speed by\nconditioning a text-to-image model on 3D semantics in 2D space and fusing them\ninto a complete and high-resolution UV texture map, as demonstrated by\nextensive qualitative and quantitative evaluations. In addition, we introduce a\ntexture enhancement network that is capable of up-scaling any texture by an\narbitrary ratio, producing 4k pixel resolution textures.\n", "link": "http://arxiv.org/abs/2407.02430v1", "date": "2024-07-02", "relevancy": 2.3138, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.579}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.579}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta%203D%20TextureGen%3A%20Fast%20and%20Consistent%20Texture%20Generation%20for%203D%0A%20%20Objects&body=Title%3A%20Meta%203D%20TextureGen%3A%20Fast%20and%20Consistent%20Texture%20Generation%20for%203D%0A%20%20Objects%0AAuthor%3A%20Raphael%20Bensadoun%20and%20Yanir%20Kleiman%20and%20Idan%20Azuri%20and%20Omri%20Harosh%20and%20Andrea%20Vedaldi%20and%20Natalia%20Neverova%20and%20Oran%20Gafni%0AAbstract%3A%20%20%20The%20recent%20availability%20and%20adaptability%20of%20text-to-image%20models%20has%20sparked%0Aa%20new%20era%20in%20many%20related%20domains%20that%20benefit%20from%20the%20learned%20text%20priors%20as%0Awell%20as%20high-quality%20and%20fast%20generation%20capabilities%2C%20one%20of%20which%20is%20texture%0Ageneration%20for%203D%20objects.%20Although%20recent%20texture%20generation%20methods%20achieve%0Aimpressive%20results%20by%20using%20text-to-image%20networks%2C%20the%20combination%20of%20global%0Aconsistency%2C%20quality%2C%20and%20speed%2C%20which%20is%20crucial%20for%20advancing%20texture%0Ageneration%20to%20real-world%20applications%2C%20remains%20elusive.%20To%20that%20end%2C%20we%0Aintroduce%20Meta%203D%20TextureGen%3A%20a%20new%20feedforward%20method%20comprised%20of%20two%0Asequential%20networks%20aimed%20at%20generating%20high-quality%20and%20globally%20consistent%0Atextures%20for%20arbitrary%20geometries%20of%20any%20complexity%20degree%20in%20less%20than%2020%0Aseconds.%20Our%20method%20achieves%20state-of-the-art%20results%20in%20quality%20and%20speed%20by%0Aconditioning%20a%20text-to-image%20model%20on%203D%20semantics%20in%202D%20space%20and%20fusing%20them%0Ainto%20a%20complete%20and%20high-resolution%20UV%20texture%20map%2C%20as%20demonstrated%20by%0Aextensive%20qualitative%20and%20quantitative%20evaluations.%20In%20addition%2C%20we%20introduce%20a%0Atexture%20enhancement%20network%20that%20is%20capable%20of%20up-scaling%20any%20texture%20by%20an%0Aarbitrary%20ratio%2C%20producing%204k%20pixel%20resolution%20textures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta%25203D%2520TextureGen%253A%2520Fast%2520and%2520Consistent%2520Texture%2520Generation%2520for%25203D%250A%2520%2520Objects%26entry.906535625%3DRaphael%2520Bensadoun%2520and%2520Yanir%2520Kleiman%2520and%2520Idan%2520Azuri%2520and%2520Omri%2520Harosh%2520and%2520Andrea%2520Vedaldi%2520and%2520Natalia%2520Neverova%2520and%2520Oran%2520Gafni%26entry.1292438233%3D%2520%2520The%2520recent%2520availability%2520and%2520adaptability%2520of%2520text-to-image%2520models%2520has%2520sparked%250Aa%2520new%2520era%2520in%2520many%2520related%2520domains%2520that%2520benefit%2520from%2520the%2520learned%2520text%2520priors%2520as%250Awell%2520as%2520high-quality%2520and%2520fast%2520generation%2520capabilities%252C%2520one%2520of%2520which%2520is%2520texture%250Ageneration%2520for%25203D%2520objects.%2520Although%2520recent%2520texture%2520generation%2520methods%2520achieve%250Aimpressive%2520results%2520by%2520using%2520text-to-image%2520networks%252C%2520the%2520combination%2520of%2520global%250Aconsistency%252C%2520quality%252C%2520and%2520speed%252C%2520which%2520is%2520crucial%2520for%2520advancing%2520texture%250Ageneration%2520to%2520real-world%2520applications%252C%2520remains%2520elusive.%2520To%2520that%2520end%252C%2520we%250Aintroduce%2520Meta%25203D%2520TextureGen%253A%2520a%2520new%2520feedforward%2520method%2520comprised%2520of%2520two%250Asequential%2520networks%2520aimed%2520at%2520generating%2520high-quality%2520and%2520globally%2520consistent%250Atextures%2520for%2520arbitrary%2520geometries%2520of%2520any%2520complexity%2520degree%2520in%2520less%2520than%252020%250Aseconds.%2520Our%2520method%2520achieves%2520state-of-the-art%2520results%2520in%2520quality%2520and%2520speed%2520by%250Aconditioning%2520a%2520text-to-image%2520model%2520on%25203D%2520semantics%2520in%25202D%2520space%2520and%2520fusing%2520them%250Ainto%2520a%2520complete%2520and%2520high-resolution%2520UV%2520texture%2520map%252C%2520as%2520demonstrated%2520by%250Aextensive%2520qualitative%2520and%2520quantitative%2520evaluations.%2520In%2520addition%252C%2520we%2520introduce%2520a%250Atexture%2520enhancement%2520network%2520that%2520is%2520capable%2520of%2520up-scaling%2520any%2520texture%2520by%2520an%250Aarbitrary%2520ratio%252C%2520producing%25204k%2520pixel%2520resolution%2520textures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta%203D%20TextureGen%3A%20Fast%20and%20Consistent%20Texture%20Generation%20for%203D%0A%20%20Objects&entry.906535625=Raphael%20Bensadoun%20and%20Yanir%20Kleiman%20and%20Idan%20Azuri%20and%20Omri%20Harosh%20and%20Andrea%20Vedaldi%20and%20Natalia%20Neverova%20and%20Oran%20Gafni&entry.1292438233=%20%20The%20recent%20availability%20and%20adaptability%20of%20text-to-image%20models%20has%20sparked%0Aa%20new%20era%20in%20many%20related%20domains%20that%20benefit%20from%20the%20learned%20text%20priors%20as%0Awell%20as%20high-quality%20and%20fast%20generation%20capabilities%2C%20one%20of%20which%20is%20texture%0Ageneration%20for%203D%20objects.%20Although%20recent%20texture%20generation%20methods%20achieve%0Aimpressive%20results%20by%20using%20text-to-image%20networks%2C%20the%20combination%20of%20global%0Aconsistency%2C%20quality%2C%20and%20speed%2C%20which%20is%20crucial%20for%20advancing%20texture%0Ageneration%20to%20real-world%20applications%2C%20remains%20elusive.%20To%20that%20end%2C%20we%0Aintroduce%20Meta%203D%20TextureGen%3A%20a%20new%20feedforward%20method%20comprised%20of%20two%0Asequential%20networks%20aimed%20at%20generating%20high-quality%20and%20globally%20consistent%0Atextures%20for%20arbitrary%20geometries%20of%20any%20complexity%20degree%20in%20less%20than%2020%0Aseconds.%20Our%20method%20achieves%20state-of-the-art%20results%20in%20quality%20and%20speed%20by%0Aconditioning%20a%20text-to-image%20model%20on%203D%20semantics%20in%202D%20space%20and%20fusing%20them%0Ainto%20a%20complete%20and%20high-resolution%20UV%20texture%20map%2C%20as%20demonstrated%20by%0Aextensive%20qualitative%20and%20quantitative%20evaluations.%20In%20addition%2C%20we%20introduce%20a%0Atexture%20enhancement%20network%20that%20is%20capable%20of%20up-scaling%20any%20texture%20by%20an%0Aarbitrary%20ratio%2C%20producing%204k%20pixel%20resolution%20textures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02430v1&entry.124074799=Read"},
{"title": "SparseSSP: 3D Subcellular Structure Prediction from Sparse-View\n  Transmitted Light Images", "author": "Jintu Zheng and YI Ding and Qizhe Liu and Yi Cao and Ying Hu and Zenan Wang", "abstract": "  Traditional fluorescence staining is phototoxic to live cells, slow, and\nexpensive; thus, the subcellular structure prediction (SSP) from transmitted\nlight (TL) images is emerging as a label-free, faster, low-cost alternative.\nHowever, existing approaches utilize 3D networks for one-to-one voxel level\ndense prediction, which necessitates a frequent and time-consuming Z-axis\nimaging process. Moreover, 3D convolutions inevitably lead to significant\ncomputation and GPU memory overhead. Therefore, we propose an efficient\nframework, SparseSSP, predicting fluorescent intensities within the target\nvoxel grid in an efficient paradigm instead of relying entirely on 3D\ntopologies. In particular, SparseSSP makes two pivotal improvements to prior\nworks. First, SparseSSP introduces a one-to-many voxel mapping paradigm, which\npermits the sparse TL slices to reconstruct the subcellular structure.\nSecondly, we propose a hybrid dimensions topology, which folds the Z-axis\ninformation into channel features, enabling the 2D network layers to tackle SSP\nunder low computational cost. We conduct extensive experiments to validate the\neffectiveness and advantages of SparseSSP on diverse sparse imaging ratios, and\nour approach achieves a leading performance compared to pure 3D topologies.\nSparseSSP reduces imaging frequencies compared to previous dense-view SSP\n(i.e., the number of imaging is reduced up to 87.5% at most), which is\nsignificant in visualizing rapid biological dynamics on low-cost devices and\nsamples.\n", "link": "http://arxiv.org/abs/2407.02159v1", "date": "2024-07-02", "relevancy": 2.3126, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6064}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5785}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseSSP%3A%203D%20Subcellular%20Structure%20Prediction%20from%20Sparse-View%0A%20%20Transmitted%20Light%20Images&body=Title%3A%20SparseSSP%3A%203D%20Subcellular%20Structure%20Prediction%20from%20Sparse-View%0A%20%20Transmitted%20Light%20Images%0AAuthor%3A%20Jintu%20Zheng%20and%20YI%20Ding%20and%20Qizhe%20Liu%20and%20Yi%20Cao%20and%20Ying%20Hu%20and%20Zenan%20Wang%0AAbstract%3A%20%20%20Traditional%20fluorescence%20staining%20is%20phototoxic%20to%20live%20cells%2C%20slow%2C%20and%0Aexpensive%3B%20thus%2C%20the%20subcellular%20structure%20prediction%20%28SSP%29%20from%20transmitted%0Alight%20%28TL%29%20images%20is%20emerging%20as%20a%20label-free%2C%20faster%2C%20low-cost%20alternative.%0AHowever%2C%20existing%20approaches%20utilize%203D%20networks%20for%20one-to-one%20voxel%20level%0Adense%20prediction%2C%20which%20necessitates%20a%20frequent%20and%20time-consuming%20Z-axis%0Aimaging%20process.%20Moreover%2C%203D%20convolutions%20inevitably%20lead%20to%20significant%0Acomputation%20and%20GPU%20memory%20overhead.%20Therefore%2C%20we%20propose%20an%20efficient%0Aframework%2C%20SparseSSP%2C%20predicting%20fluorescent%20intensities%20within%20the%20target%0Avoxel%20grid%20in%20an%20efficient%20paradigm%20instead%20of%20relying%20entirely%20on%203D%0Atopologies.%20In%20particular%2C%20SparseSSP%20makes%20two%20pivotal%20improvements%20to%20prior%0Aworks.%20First%2C%20SparseSSP%20introduces%20a%20one-to-many%20voxel%20mapping%20paradigm%2C%20which%0Apermits%20the%20sparse%20TL%20slices%20to%20reconstruct%20the%20subcellular%20structure.%0ASecondly%2C%20we%20propose%20a%20hybrid%20dimensions%20topology%2C%20which%20folds%20the%20Z-axis%0Ainformation%20into%20channel%20features%2C%20enabling%20the%202D%20network%20layers%20to%20tackle%20SSP%0Aunder%20low%20computational%20cost.%20We%20conduct%20extensive%20experiments%20to%20validate%20the%0Aeffectiveness%20and%20advantages%20of%20SparseSSP%20on%20diverse%20sparse%20imaging%20ratios%2C%20and%0Aour%20approach%20achieves%20a%20leading%20performance%20compared%20to%20pure%203D%20topologies.%0ASparseSSP%20reduces%20imaging%20frequencies%20compared%20to%20previous%20dense-view%20SSP%0A%28i.e.%2C%20the%20number%20of%20imaging%20is%20reduced%20up%20to%2087.5%25%20at%20most%29%2C%20which%20is%0Asignificant%20in%20visualizing%20rapid%20biological%20dynamics%20on%20low-cost%20devices%20and%0Asamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseSSP%253A%25203D%2520Subcellular%2520Structure%2520Prediction%2520from%2520Sparse-View%250A%2520%2520Transmitted%2520Light%2520Images%26entry.906535625%3DJintu%2520Zheng%2520and%2520YI%2520Ding%2520and%2520Qizhe%2520Liu%2520and%2520Yi%2520Cao%2520and%2520Ying%2520Hu%2520and%2520Zenan%2520Wang%26entry.1292438233%3D%2520%2520Traditional%2520fluorescence%2520staining%2520is%2520phototoxic%2520to%2520live%2520cells%252C%2520slow%252C%2520and%250Aexpensive%253B%2520thus%252C%2520the%2520subcellular%2520structure%2520prediction%2520%2528SSP%2529%2520from%2520transmitted%250Alight%2520%2528TL%2529%2520images%2520is%2520emerging%2520as%2520a%2520label-free%252C%2520faster%252C%2520low-cost%2520alternative.%250AHowever%252C%2520existing%2520approaches%2520utilize%25203D%2520networks%2520for%2520one-to-one%2520voxel%2520level%250Adense%2520prediction%252C%2520which%2520necessitates%2520a%2520frequent%2520and%2520time-consuming%2520Z-axis%250Aimaging%2520process.%2520Moreover%252C%25203D%2520convolutions%2520inevitably%2520lead%2520to%2520significant%250Acomputation%2520and%2520GPU%2520memory%2520overhead.%2520Therefore%252C%2520we%2520propose%2520an%2520efficient%250Aframework%252C%2520SparseSSP%252C%2520predicting%2520fluorescent%2520intensities%2520within%2520the%2520target%250Avoxel%2520grid%2520in%2520an%2520efficient%2520paradigm%2520instead%2520of%2520relying%2520entirely%2520on%25203D%250Atopologies.%2520In%2520particular%252C%2520SparseSSP%2520makes%2520two%2520pivotal%2520improvements%2520to%2520prior%250Aworks.%2520First%252C%2520SparseSSP%2520introduces%2520a%2520one-to-many%2520voxel%2520mapping%2520paradigm%252C%2520which%250Apermits%2520the%2520sparse%2520TL%2520slices%2520to%2520reconstruct%2520the%2520subcellular%2520structure.%250ASecondly%252C%2520we%2520propose%2520a%2520hybrid%2520dimensions%2520topology%252C%2520which%2520folds%2520the%2520Z-axis%250Ainformation%2520into%2520channel%2520features%252C%2520enabling%2520the%25202D%2520network%2520layers%2520to%2520tackle%2520SSP%250Aunder%2520low%2520computational%2520cost.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520validate%2520the%250Aeffectiveness%2520and%2520advantages%2520of%2520SparseSSP%2520on%2520diverse%2520sparse%2520imaging%2520ratios%252C%2520and%250Aour%2520approach%2520achieves%2520a%2520leading%2520performance%2520compared%2520to%2520pure%25203D%2520topologies.%250ASparseSSP%2520reduces%2520imaging%2520frequencies%2520compared%2520to%2520previous%2520dense-view%2520SSP%250A%2528i.e.%252C%2520the%2520number%2520of%2520imaging%2520is%2520reduced%2520up%2520to%252087.5%2525%2520at%2520most%2529%252C%2520which%2520is%250Asignificant%2520in%2520visualizing%2520rapid%2520biological%2520dynamics%2520on%2520low-cost%2520devices%2520and%250Asamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseSSP%3A%203D%20Subcellular%20Structure%20Prediction%20from%20Sparse-View%0A%20%20Transmitted%20Light%20Images&entry.906535625=Jintu%20Zheng%20and%20YI%20Ding%20and%20Qizhe%20Liu%20and%20Yi%20Cao%20and%20Ying%20Hu%20and%20Zenan%20Wang&entry.1292438233=%20%20Traditional%20fluorescence%20staining%20is%20phototoxic%20to%20live%20cells%2C%20slow%2C%20and%0Aexpensive%3B%20thus%2C%20the%20subcellular%20structure%20prediction%20%28SSP%29%20from%20transmitted%0Alight%20%28TL%29%20images%20is%20emerging%20as%20a%20label-free%2C%20faster%2C%20low-cost%20alternative.%0AHowever%2C%20existing%20approaches%20utilize%203D%20networks%20for%20one-to-one%20voxel%20level%0Adense%20prediction%2C%20which%20necessitates%20a%20frequent%20and%20time-consuming%20Z-axis%0Aimaging%20process.%20Moreover%2C%203D%20convolutions%20inevitably%20lead%20to%20significant%0Acomputation%20and%20GPU%20memory%20overhead.%20Therefore%2C%20we%20propose%20an%20efficient%0Aframework%2C%20SparseSSP%2C%20predicting%20fluorescent%20intensities%20within%20the%20target%0Avoxel%20grid%20in%20an%20efficient%20paradigm%20instead%20of%20relying%20entirely%20on%203D%0Atopologies.%20In%20particular%2C%20SparseSSP%20makes%20two%20pivotal%20improvements%20to%20prior%0Aworks.%20First%2C%20SparseSSP%20introduces%20a%20one-to-many%20voxel%20mapping%20paradigm%2C%20which%0Apermits%20the%20sparse%20TL%20slices%20to%20reconstruct%20the%20subcellular%20structure.%0ASecondly%2C%20we%20propose%20a%20hybrid%20dimensions%20topology%2C%20which%20folds%20the%20Z-axis%0Ainformation%20into%20channel%20features%2C%20enabling%20the%202D%20network%20layers%20to%20tackle%20SSP%0Aunder%20low%20computational%20cost.%20We%20conduct%20extensive%20experiments%20to%20validate%20the%0Aeffectiveness%20and%20advantages%20of%20SparseSSP%20on%20diverse%20sparse%20imaging%20ratios%2C%20and%0Aour%20approach%20achieves%20a%20leading%20performance%20compared%20to%20pure%203D%20topologies.%0ASparseSSP%20reduces%20imaging%20frequencies%20compared%20to%20previous%20dense-view%20SSP%0A%28i.e.%2C%20the%20number%20of%20imaging%20is%20reduced%20up%20to%2087.5%25%20at%20most%29%2C%20which%20is%0Asignificant%20in%20visualizing%20rapid%20biological%20dynamics%20on%20low-cost%20devices%20and%0Asamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02159v1&entry.124074799=Read"},
{"title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression\n  Recognition with AdaptERs", "author": "Haodong Chen and Haojian Huang and Junhao Dong and Mingzhe Zheng and Dian Shao", "abstract": "  Dynamic Facial Expression Recognition (DFER) is crucial for understanding\nhuman behavior. However, current methods exhibit limited performance mainly due\nto the scarcity of high-quality data, the insufficient utilization of facial\ndynamics, and the ambiguity of expression semantics, etc. To this end, we\npropose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic\nFacial Expression Recognition with AdaptERs (FineCLIPER), incorporating the\nfollowing novel designs: 1) To better distinguish between similar facial\nexpressions, we extend the class labels to textual descriptions from both\npositive and negative aspects, and obtain supervision by calculating the\ncross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a\nhierarchical manner to effectively mine useful cues from DFE videos.\nSpecifically, besides directly embedding video frames as input (low semantic\nlevel), we propose to extract the face segmentation masks and landmarks based\non each frame (middle semantic level) and utilize the Multi-modal Large\nLanguage Model (MLLM) to further generate detailed descriptions of facial\nchanges across frames with designed prompts (high semantic level).\nAdditionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable\nefficient adaptation of large pre-trained models (i.e., CLIP) for this task.\nOur FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW\ndatasets in both supervised and zero-shot settings with few tunable parameters.\nAnalysis and ablation studies further validate its effectiveness.\n", "link": "http://arxiv.org/abs/2407.02157v1", "date": "2024-07-02", "relevancy": 2.3072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6269}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineCLIPER%3A%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20with%20AdaptERs&body=Title%3A%20FineCLIPER%3A%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20with%20AdaptERs%0AAuthor%3A%20Haodong%20Chen%20and%20Haojian%20Huang%20and%20Junhao%20Dong%20and%20Mingzhe%20Zheng%20and%20Dian%20Shao%0AAbstract%3A%20%20%20Dynamic%20Facial%20Expression%20Recognition%20%28DFER%29%20is%20crucial%20for%20understanding%0Ahuman%20behavior.%20However%2C%20current%20methods%20exhibit%20limited%20performance%20mainly%20due%0Ato%20the%20scarcity%20of%20high-quality%20data%2C%20the%20insufficient%20utilization%20of%20facial%0Adynamics%2C%20and%20the%20ambiguity%20of%20expression%20semantics%2C%20etc.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20framework%2C%20named%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%0AFacial%20Expression%20Recognition%20with%20AdaptERs%20%28FineCLIPER%29%2C%20incorporating%20the%0Afollowing%20novel%20designs%3A%201%29%20To%20better%20distinguish%20between%20similar%20facial%0Aexpressions%2C%20we%20extend%20the%20class%20labels%20to%20textual%20descriptions%20from%20both%0Apositive%20and%20negative%20aspects%2C%20and%20obtain%20supervision%20by%20calculating%20the%0Across-modal%20similarity%20based%20on%20the%20CLIP%20model%3B%202%29%20Our%20FineCLIPER%20adopts%20a%0Ahierarchical%20manner%20to%20effectively%20mine%20useful%20cues%20from%20DFE%20videos.%0ASpecifically%2C%20besides%20directly%20embedding%20video%20frames%20as%20input%20%28low%20semantic%0Alevel%29%2C%20we%20propose%20to%20extract%20the%20face%20segmentation%20masks%20and%20landmarks%20based%0Aon%20each%20frame%20%28middle%20semantic%20level%29%20and%20utilize%20the%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%20to%20further%20generate%20detailed%20descriptions%20of%20facial%0Achanges%20across%20frames%20with%20designed%20prompts%20%28high%20semantic%20level%29.%0AAdditionally%2C%20we%20also%20adopt%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20to%20enable%0Aefficient%20adaptation%20of%20large%20pre-trained%20models%20%28i.e.%2C%20CLIP%29%20for%20this%20task.%0AOur%20FineCLIPER%20achieves%20SOTA%20performance%20on%20the%20DFEW%2C%20FERV39k%2C%20and%20MAFW%0Adatasets%20in%20both%20supervised%20and%20zero-shot%20settings%20with%20few%20tunable%20parameters.%0AAnalysis%20and%20ablation%20studies%20further%20validate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineCLIPER%253A%2520Multi-modal%2520Fine-grained%2520CLIP%2520for%2520Dynamic%2520Facial%2520Expression%250A%2520%2520Recognition%2520with%2520AdaptERs%26entry.906535625%3DHaodong%2520Chen%2520and%2520Haojian%2520Huang%2520and%2520Junhao%2520Dong%2520and%2520Mingzhe%2520Zheng%2520and%2520Dian%2520Shao%26entry.1292438233%3D%2520%2520Dynamic%2520Facial%2520Expression%2520Recognition%2520%2528DFER%2529%2520is%2520crucial%2520for%2520understanding%250Ahuman%2520behavior.%2520However%252C%2520current%2520methods%2520exhibit%2520limited%2520performance%2520mainly%2520due%250Ato%2520the%2520scarcity%2520of%2520high-quality%2520data%252C%2520the%2520insufficient%2520utilization%2520of%2520facial%250Adynamics%252C%2520and%2520the%2520ambiguity%2520of%2520expression%2520semantics%252C%2520etc.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520novel%2520framework%252C%2520named%2520Multi-modal%2520Fine-grained%2520CLIP%2520for%2520Dynamic%250AFacial%2520Expression%2520Recognition%2520with%2520AdaptERs%2520%2528FineCLIPER%2529%252C%2520incorporating%2520the%250Afollowing%2520novel%2520designs%253A%25201%2529%2520To%2520better%2520distinguish%2520between%2520similar%2520facial%250Aexpressions%252C%2520we%2520extend%2520the%2520class%2520labels%2520to%2520textual%2520descriptions%2520from%2520both%250Apositive%2520and%2520negative%2520aspects%252C%2520and%2520obtain%2520supervision%2520by%2520calculating%2520the%250Across-modal%2520similarity%2520based%2520on%2520the%2520CLIP%2520model%253B%25202%2529%2520Our%2520FineCLIPER%2520adopts%2520a%250Ahierarchical%2520manner%2520to%2520effectively%2520mine%2520useful%2520cues%2520from%2520DFE%2520videos.%250ASpecifically%252C%2520besides%2520directly%2520embedding%2520video%2520frames%2520as%2520input%2520%2528low%2520semantic%250Alevel%2529%252C%2520we%2520propose%2520to%2520extract%2520the%2520face%2520segmentation%2520masks%2520and%2520landmarks%2520based%250Aon%2520each%2520frame%2520%2528middle%2520semantic%2520level%2529%2520and%2520utilize%2520the%2520Multi-modal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%2520to%2520further%2520generate%2520detailed%2520descriptions%2520of%2520facial%250Achanges%2520across%2520frames%2520with%2520designed%2520prompts%2520%2528high%2520semantic%2520level%2529.%250AAdditionally%252C%2520we%2520also%2520adopt%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520to%2520enable%250Aefficient%2520adaptation%2520of%2520large%2520pre-trained%2520models%2520%2528i.e.%252C%2520CLIP%2529%2520for%2520this%2520task.%250AOur%2520FineCLIPER%2520achieves%2520SOTA%2520performance%2520on%2520the%2520DFEW%252C%2520FERV39k%252C%2520and%2520MAFW%250Adatasets%2520in%2520both%2520supervised%2520and%2520zero-shot%2520settings%2520with%2520few%2520tunable%2520parameters.%250AAnalysis%2520and%2520ablation%2520studies%2520further%2520validate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineCLIPER%3A%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20with%20AdaptERs&entry.906535625=Haodong%20Chen%20and%20Haojian%20Huang%20and%20Junhao%20Dong%20and%20Mingzhe%20Zheng%20and%20Dian%20Shao&entry.1292438233=%20%20Dynamic%20Facial%20Expression%20Recognition%20%28DFER%29%20is%20crucial%20for%20understanding%0Ahuman%20behavior.%20However%2C%20current%20methods%20exhibit%20limited%20performance%20mainly%20due%0Ato%20the%20scarcity%20of%20high-quality%20data%2C%20the%20insufficient%20utilization%20of%20facial%0Adynamics%2C%20and%20the%20ambiguity%20of%20expression%20semantics%2C%20etc.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20framework%2C%20named%20Multi-modal%20Fine-grained%20CLIP%20for%20Dynamic%0AFacial%20Expression%20Recognition%20with%20AdaptERs%20%28FineCLIPER%29%2C%20incorporating%20the%0Afollowing%20novel%20designs%3A%201%29%20To%20better%20distinguish%20between%20similar%20facial%0Aexpressions%2C%20we%20extend%20the%20class%20labels%20to%20textual%20descriptions%20from%20both%0Apositive%20and%20negative%20aspects%2C%20and%20obtain%20supervision%20by%20calculating%20the%0Across-modal%20similarity%20based%20on%20the%20CLIP%20model%3B%202%29%20Our%20FineCLIPER%20adopts%20a%0Ahierarchical%20manner%20to%20effectively%20mine%20useful%20cues%20from%20DFE%20videos.%0ASpecifically%2C%20besides%20directly%20embedding%20video%20frames%20as%20input%20%28low%20semantic%0Alevel%29%2C%20we%20propose%20to%20extract%20the%20face%20segmentation%20masks%20and%20landmarks%20based%0Aon%20each%20frame%20%28middle%20semantic%20level%29%20and%20utilize%20the%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%20to%20further%20generate%20detailed%20descriptions%20of%20facial%0Achanges%20across%20frames%20with%20designed%20prompts%20%28high%20semantic%20level%29.%0AAdditionally%2C%20we%20also%20adopt%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20to%20enable%0Aefficient%20adaptation%20of%20large%20pre-trained%20models%20%28i.e.%2C%20CLIP%29%20for%20this%20task.%0AOur%20FineCLIPER%20achieves%20SOTA%20performance%20on%20the%20DFEW%2C%20FERV39k%2C%20and%20MAFW%0Adatasets%20in%20both%20supervised%20and%20zero-shot%20settings%20with%20few%20tunable%20parameters.%0AAnalysis%20and%20ablation%20studies%20further%20validate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02157v1&entry.124074799=Read"},
{"title": "Matching domain experts by training from scratch on domain knowledge", "author": "Xiaoliang Luo and Guangzhi Sun and Bradley C. Love", "abstract": "  Recently, large language models (LLMs) have outperformed human experts in\npredicting the results of neuroscience experiments (Luo et al., 2024). What is\nthe basis for this performance? One possibility is that statistical patterns in\nthat specific scientific literature, as opposed to emergent reasoning abilities\narising from broader training, underlie LLMs' performance. To evaluate this\npossibility, we trained (next word prediction) a relatively small\n124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.\nDespite being orders of magnitude smaller than larger LLMs trained on trillions\nof tokens, small models achieved expert-level performance in predicting\nneuroscience results. Small models trained on the neuroscience literature\nsucceeded when they were trained from scratch using a tokenizer specifically\ntrained on neuroscience text or when the neuroscience literature was used to\nfinetune a pretrained GPT-2. Our results indicate that expert-level performance\nmay be attained by even small LLMs through domain-specific, auto-regressive\ntraining approaches.\n", "link": "http://arxiv.org/abs/2405.09395v2", "date": "2024-07-02", "relevancy": 2.299, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4614}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.46}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matching%20domain%20experts%20by%20training%20from%20scratch%20on%20domain%20knowledge&body=Title%3A%20Matching%20domain%20experts%20by%20training%20from%20scratch%20on%20domain%20knowledge%0AAuthor%3A%20Xiaoliang%20Luo%20and%20Guangzhi%20Sun%20and%20Bradley%20C.%20Love%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20outperformed%20human%20experts%20in%0Apredicting%20the%20results%20of%20neuroscience%20experiments%20%28Luo%20et%20al.%2C%202024%29.%20What%20is%0Athe%20basis%20for%20this%20performance%3F%20One%20possibility%20is%20that%20statistical%20patterns%20in%0Athat%20specific%20scientific%20literature%2C%20as%20opposed%20to%20emergent%20reasoning%20abilities%0Aarising%20from%20broader%20training%2C%20underlie%20LLMs%27%20performance.%20To%20evaluate%20this%0Apossibility%2C%20we%20trained%20%28next%20word%20prediction%29%20a%20relatively%20small%0A124M-parameter%20GPT-2%20model%20on%201.3%20billion%20tokens%20of%20domain-specific%20knowledge.%0ADespite%20being%20orders%20of%20magnitude%20smaller%20than%20larger%20LLMs%20trained%20on%20trillions%0Aof%20tokens%2C%20small%20models%20achieved%20expert-level%20performance%20in%20predicting%0Aneuroscience%20results.%20Small%20models%20trained%20on%20the%20neuroscience%20literature%0Asucceeded%20when%20they%20were%20trained%20from%20scratch%20using%20a%20tokenizer%20specifically%0Atrained%20on%20neuroscience%20text%20or%20when%20the%20neuroscience%20literature%20was%20used%20to%0Afinetune%20a%20pretrained%20GPT-2.%20Our%20results%20indicate%20that%20expert-level%20performance%0Amay%20be%20attained%20by%20even%20small%20LLMs%20through%20domain-specific%2C%20auto-regressive%0Atraining%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09395v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatching%2520domain%2520experts%2520by%2520training%2520from%2520scratch%2520on%2520domain%2520knowledge%26entry.906535625%3DXiaoliang%2520Luo%2520and%2520Guangzhi%2520Sun%2520and%2520Bradley%2520C.%2520Love%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520outperformed%2520human%2520experts%2520in%250Apredicting%2520the%2520results%2520of%2520neuroscience%2520experiments%2520%2528Luo%2520et%2520al.%252C%25202024%2529.%2520What%2520is%250Athe%2520basis%2520for%2520this%2520performance%253F%2520One%2520possibility%2520is%2520that%2520statistical%2520patterns%2520in%250Athat%2520specific%2520scientific%2520literature%252C%2520as%2520opposed%2520to%2520emergent%2520reasoning%2520abilities%250Aarising%2520from%2520broader%2520training%252C%2520underlie%2520LLMs%2527%2520performance.%2520To%2520evaluate%2520this%250Apossibility%252C%2520we%2520trained%2520%2528next%2520word%2520prediction%2529%2520a%2520relatively%2520small%250A124M-parameter%2520GPT-2%2520model%2520on%25201.3%2520billion%2520tokens%2520of%2520domain-specific%2520knowledge.%250ADespite%2520being%2520orders%2520of%2520magnitude%2520smaller%2520than%2520larger%2520LLMs%2520trained%2520on%2520trillions%250Aof%2520tokens%252C%2520small%2520models%2520achieved%2520expert-level%2520performance%2520in%2520predicting%250Aneuroscience%2520results.%2520Small%2520models%2520trained%2520on%2520the%2520neuroscience%2520literature%250Asucceeded%2520when%2520they%2520were%2520trained%2520from%2520scratch%2520using%2520a%2520tokenizer%2520specifically%250Atrained%2520on%2520neuroscience%2520text%2520or%2520when%2520the%2520neuroscience%2520literature%2520was%2520used%2520to%250Afinetune%2520a%2520pretrained%2520GPT-2.%2520Our%2520results%2520indicate%2520that%2520expert-level%2520performance%250Amay%2520be%2520attained%2520by%2520even%2520small%2520LLMs%2520through%2520domain-specific%252C%2520auto-regressive%250Atraining%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09395v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20domain%20experts%20by%20training%20from%20scratch%20on%20domain%20knowledge&entry.906535625=Xiaoliang%20Luo%20and%20Guangzhi%20Sun%20and%20Bradley%20C.%20Love&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20outperformed%20human%20experts%20in%0Apredicting%20the%20results%20of%20neuroscience%20experiments%20%28Luo%20et%20al.%2C%202024%29.%20What%20is%0Athe%20basis%20for%20this%20performance%3F%20One%20possibility%20is%20that%20statistical%20patterns%20in%0Athat%20specific%20scientific%20literature%2C%20as%20opposed%20to%20emergent%20reasoning%20abilities%0Aarising%20from%20broader%20training%2C%20underlie%20LLMs%27%20performance.%20To%20evaluate%20this%0Apossibility%2C%20we%20trained%20%28next%20word%20prediction%29%20a%20relatively%20small%0A124M-parameter%20GPT-2%20model%20on%201.3%20billion%20tokens%20of%20domain-specific%20knowledge.%0ADespite%20being%20orders%20of%20magnitude%20smaller%20than%20larger%20LLMs%20trained%20on%20trillions%0Aof%20tokens%2C%20small%20models%20achieved%20expert-level%20performance%20in%20predicting%0Aneuroscience%20results.%20Small%20models%20trained%20on%20the%20neuroscience%20literature%0Asucceeded%20when%20they%20were%20trained%20from%20scratch%20using%20a%20tokenizer%20specifically%0Atrained%20on%20neuroscience%20text%20or%20when%20the%20neuroscience%20literature%20was%20used%20to%0Afinetune%20a%20pretrained%20GPT-2.%20Our%20results%20indicate%20that%20expert-level%20performance%0Amay%20be%20attained%20by%20even%20small%20LLMs%20through%20domain-specific%2C%20auto-regressive%0Atraining%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09395v2&entry.124074799=Read"},
{"title": "Instant Photorealistic Neural Radiance Fields Stylization", "author": "Shaoxu Li and Ye Pan", "abstract": "  We present Instant Neural Radiance Fields Stylization, a novel approach for\nmulti-view image stylization for the 3D scene. Our approach models a neural\nradiance field based on neural graphics primitives, which use a hash\ntable-based position encoder for position embedding. We split the position\nencoder into two parts, the content and style sub-branches, and train the\nnetwork for normal novel view image synthesis with the content and style\ntargets. In the inference stage, we execute AdaIN to the output features of the\nposition encoder, with content and style voxel grid features as reference. With\nthe adjusted features, the stylization of novel view images could be obtained.\nOur method extends the style target from style images to image sets of scenes\nand does not require additional network training for stylization. Given a set\nof images of 3D scenes and a style target(a style image or another set of 3D\nscenes), our method can generate stylized novel views with a consistent\nappearance at various view angles in less than 10 minutes on modern GPU\nhardware. Extensive experimental results demonstrate the validity and\nsuperiority of our method.\n", "link": "http://arxiv.org/abs/2303.16884v2", "date": "2024-07-02", "relevancy": 2.2869, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instant%20Photorealistic%20Neural%20Radiance%20Fields%20Stylization&body=Title%3A%20Instant%20Photorealistic%20Neural%20Radiance%20Fields%20Stylization%0AAuthor%3A%20Shaoxu%20Li%20and%20Ye%20Pan%0AAbstract%3A%20%20%20We%20present%20Instant%20Neural%20Radiance%20Fields%20Stylization%2C%20a%20novel%20approach%20for%0Amulti-view%20image%20stylization%20for%20the%203D%20scene.%20Our%20approach%20models%20a%20neural%0Aradiance%20field%20based%20on%20neural%20graphics%20primitives%2C%20which%20use%20a%20hash%0Atable-based%20position%20encoder%20for%20position%20embedding.%20We%20split%20the%20position%0Aencoder%20into%20two%20parts%2C%20the%20content%20and%20style%20sub-branches%2C%20and%20train%20the%0Anetwork%20for%20normal%20novel%20view%20image%20synthesis%20with%20the%20content%20and%20style%0Atargets.%20In%20the%20inference%20stage%2C%20we%20execute%20AdaIN%20to%20the%20output%20features%20of%20the%0Aposition%20encoder%2C%20with%20content%20and%20style%20voxel%20grid%20features%20as%20reference.%20With%0Athe%20adjusted%20features%2C%20the%20stylization%20of%20novel%20view%20images%20could%20be%20obtained.%0AOur%20method%20extends%20the%20style%20target%20from%20style%20images%20to%20image%20sets%20of%20scenes%0Aand%20does%20not%20require%20additional%20network%20training%20for%20stylization.%20Given%20a%20set%0Aof%20images%20of%203D%20scenes%20and%20a%20style%20target%28a%20style%20image%20or%20another%20set%20of%203D%0Ascenes%29%2C%20our%20method%20can%20generate%20stylized%20novel%20views%20with%20a%20consistent%0Aappearance%20at%20various%20view%20angles%20in%20less%20than%2010%20minutes%20on%20modern%20GPU%0Ahardware.%20Extensive%20experimental%20results%20demonstrate%20the%20validity%20and%0Asuperiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.16884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstant%2520Photorealistic%2520Neural%2520Radiance%2520Fields%2520Stylization%26entry.906535625%3DShaoxu%2520Li%2520and%2520Ye%2520Pan%26entry.1292438233%3D%2520%2520We%2520present%2520Instant%2520Neural%2520Radiance%2520Fields%2520Stylization%252C%2520a%2520novel%2520approach%2520for%250Amulti-view%2520image%2520stylization%2520for%2520the%25203D%2520scene.%2520Our%2520approach%2520models%2520a%2520neural%250Aradiance%2520field%2520based%2520on%2520neural%2520graphics%2520primitives%252C%2520which%2520use%2520a%2520hash%250Atable-based%2520position%2520encoder%2520for%2520position%2520embedding.%2520We%2520split%2520the%2520position%250Aencoder%2520into%2520two%2520parts%252C%2520the%2520content%2520and%2520style%2520sub-branches%252C%2520and%2520train%2520the%250Anetwork%2520for%2520normal%2520novel%2520view%2520image%2520synthesis%2520with%2520the%2520content%2520and%2520style%250Atargets.%2520In%2520the%2520inference%2520stage%252C%2520we%2520execute%2520AdaIN%2520to%2520the%2520output%2520features%2520of%2520the%250Aposition%2520encoder%252C%2520with%2520content%2520and%2520style%2520voxel%2520grid%2520features%2520as%2520reference.%2520With%250Athe%2520adjusted%2520features%252C%2520the%2520stylization%2520of%2520novel%2520view%2520images%2520could%2520be%2520obtained.%250AOur%2520method%2520extends%2520the%2520style%2520target%2520from%2520style%2520images%2520to%2520image%2520sets%2520of%2520scenes%250Aand%2520does%2520not%2520require%2520additional%2520network%2520training%2520for%2520stylization.%2520Given%2520a%2520set%250Aof%2520images%2520of%25203D%2520scenes%2520and%2520a%2520style%2520target%2528a%2520style%2520image%2520or%2520another%2520set%2520of%25203D%250Ascenes%2529%252C%2520our%2520method%2520can%2520generate%2520stylized%2520novel%2520views%2520with%2520a%2520consistent%250Aappearance%2520at%2520various%2520view%2520angles%2520in%2520less%2520than%252010%2520minutes%2520on%2520modern%2520GPU%250Ahardware.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%2520validity%2520and%250Asuperiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.16884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instant%20Photorealistic%20Neural%20Radiance%20Fields%20Stylization&entry.906535625=Shaoxu%20Li%20and%20Ye%20Pan&entry.1292438233=%20%20We%20present%20Instant%20Neural%20Radiance%20Fields%20Stylization%2C%20a%20novel%20approach%20for%0Amulti-view%20image%20stylization%20for%20the%203D%20scene.%20Our%20approach%20models%20a%20neural%0Aradiance%20field%20based%20on%20neural%20graphics%20primitives%2C%20which%20use%20a%20hash%0Atable-based%20position%20encoder%20for%20position%20embedding.%20We%20split%20the%20position%0Aencoder%20into%20two%20parts%2C%20the%20content%20and%20style%20sub-branches%2C%20and%20train%20the%0Anetwork%20for%20normal%20novel%20view%20image%20synthesis%20with%20the%20content%20and%20style%0Atargets.%20In%20the%20inference%20stage%2C%20we%20execute%20AdaIN%20to%20the%20output%20features%20of%20the%0Aposition%20encoder%2C%20with%20content%20and%20style%20voxel%20grid%20features%20as%20reference.%20With%0Athe%20adjusted%20features%2C%20the%20stylization%20of%20novel%20view%20images%20could%20be%20obtained.%0AOur%20method%20extends%20the%20style%20target%20from%20style%20images%20to%20image%20sets%20of%20scenes%0Aand%20does%20not%20require%20additional%20network%20training%20for%20stylization.%20Given%20a%20set%0Aof%20images%20of%203D%20scenes%20and%20a%20style%20target%28a%20style%20image%20or%20another%20set%20of%203D%0Ascenes%29%2C%20our%20method%20can%20generate%20stylized%20novel%20views%20with%20a%20consistent%0Aappearance%20at%20various%20view%20angles%20in%20less%20than%2010%20minutes%20on%20modern%20GPU%0Ahardware.%20Extensive%20experimental%20results%20demonstrate%20the%20validity%20and%0Asuperiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.16884v2&entry.124074799=Read"},
{"title": "Equidistribution-based training of Free Knot Splines and ReLU Neural\n  Networks", "author": "Simone Appella and Simon Arridge and Chris Budd and Teo Deveney and Lisa Maria Kreusser", "abstract": "  We consider the problem of one-dimensional function approximation using\nshallow neural networks (NN) with a rectified linear unit (ReLU) activation\nfunction and compare their training with traditional methods such as univariate\nFree Knot Splines (FKS). ReLU NNs and FKS span the same function space, and\nthus have the same theoretical expressivity. In the case of ReLU NNs, we show\nthat their ill-conditioning degrades rapidly as the width of the network\nincreases. This often leads to significantly poorer approximation in contrast\nto the FKS representation, which remains well-conditioned as the number of\nknots increases. We leverage the theory of optimal piecewise linear\ninterpolants to improve the training procedure for a ReLU NN. Using the\nequidistribution principle, we propose a two-level procedure for training the\nFKS by first solving the nonlinear problem of finding the optimal knot\nlocations of the interpolating FKS. Determining the optimal knots then acts as\na good starting point for training the weights of the FKS. The training of the\nFKS gives insights into how we can train a ReLU NN effectively to give an\nequally accurate approximation. More precisely, we combine the training of the\nReLU NN with an equidistribution based loss to find the breakpoints of the ReLU\nfunctions, combined with preconditioning the ReLU NN approximation (to take an\nFKS form) to find the scalings of the ReLU functions, leads to a\nwell-conditioned and reliable method of finding an accurate ReLU NN\napproximation to a target function. We test this method on a series or regular,\nsingular, and rapidly varying target functions and obtain good results\nrealising the expressivity of the network in this case.\n", "link": "http://arxiv.org/abs/2407.02153v1", "date": "2024-07-02", "relevancy": 2.2686, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.46}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4523}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equidistribution-based%20training%20of%20Free%20Knot%20Splines%20and%20ReLU%20Neural%0A%20%20Networks&body=Title%3A%20Equidistribution-based%20training%20of%20Free%20Knot%20Splines%20and%20ReLU%20Neural%0A%20%20Networks%0AAuthor%3A%20Simone%20Appella%20and%20Simon%20Arridge%20and%20Chris%20Budd%20and%20Teo%20Deveney%20and%20Lisa%20Maria%20Kreusser%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20one-dimensional%20function%20approximation%20using%0Ashallow%20neural%20networks%20%28NN%29%20with%20a%20rectified%20linear%20unit%20%28ReLU%29%20activation%0Afunction%20and%20compare%20their%20training%20with%20traditional%20methods%20such%20as%20univariate%0AFree%20Knot%20Splines%20%28FKS%29.%20ReLU%20NNs%20and%20FKS%20span%20the%20same%20function%20space%2C%20and%0Athus%20have%20the%20same%20theoretical%20expressivity.%20In%20the%20case%20of%20ReLU%20NNs%2C%20we%20show%0Athat%20their%20ill-conditioning%20degrades%20rapidly%20as%20the%20width%20of%20the%20network%0Aincreases.%20This%20often%20leads%20to%20significantly%20poorer%20approximation%20in%20contrast%0Ato%20the%20FKS%20representation%2C%20which%20remains%20well-conditioned%20as%20the%20number%20of%0Aknots%20increases.%20We%20leverage%20the%20theory%20of%20optimal%20piecewise%20linear%0Ainterpolants%20to%20improve%20the%20training%20procedure%20for%20a%20ReLU%20NN.%20Using%20the%0Aequidistribution%20principle%2C%20we%20propose%20a%20two-level%20procedure%20for%20training%20the%0AFKS%20by%20first%20solving%20the%20nonlinear%20problem%20of%20finding%20the%20optimal%20knot%0Alocations%20of%20the%20interpolating%20FKS.%20Determining%20the%20optimal%20knots%20then%20acts%20as%0Aa%20good%20starting%20point%20for%20training%20the%20weights%20of%20the%20FKS.%20The%20training%20of%20the%0AFKS%20gives%20insights%20into%20how%20we%20can%20train%20a%20ReLU%20NN%20effectively%20to%20give%20an%0Aequally%20accurate%20approximation.%20More%20precisely%2C%20we%20combine%20the%20training%20of%20the%0AReLU%20NN%20with%20an%20equidistribution%20based%20loss%20to%20find%20the%20breakpoints%20of%20the%20ReLU%0Afunctions%2C%20combined%20with%20preconditioning%20the%20ReLU%20NN%20approximation%20%28to%20take%20an%0AFKS%20form%29%20to%20find%20the%20scalings%20of%20the%20ReLU%20functions%2C%20leads%20to%20a%0Awell-conditioned%20and%20reliable%20method%20of%20finding%20an%20accurate%20ReLU%20NN%0Aapproximation%20to%20a%20target%20function.%20We%20test%20this%20method%20on%20a%20series%20or%20regular%2C%0Asingular%2C%20and%20rapidly%20varying%20target%20functions%20and%20obtain%20good%20results%0Arealising%20the%20expressivity%20of%20the%20network%20in%20this%20case.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquidistribution-based%2520training%2520of%2520Free%2520Knot%2520Splines%2520and%2520ReLU%2520Neural%250A%2520%2520Networks%26entry.906535625%3DSimone%2520Appella%2520and%2520Simon%2520Arridge%2520and%2520Chris%2520Budd%2520and%2520Teo%2520Deveney%2520and%2520Lisa%2520Maria%2520Kreusser%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520one-dimensional%2520function%2520approximation%2520using%250Ashallow%2520neural%2520networks%2520%2528NN%2529%2520with%2520a%2520rectified%2520linear%2520unit%2520%2528ReLU%2529%2520activation%250Afunction%2520and%2520compare%2520their%2520training%2520with%2520traditional%2520methods%2520such%2520as%2520univariate%250AFree%2520Knot%2520Splines%2520%2528FKS%2529.%2520ReLU%2520NNs%2520and%2520FKS%2520span%2520the%2520same%2520function%2520space%252C%2520and%250Athus%2520have%2520the%2520same%2520theoretical%2520expressivity.%2520In%2520the%2520case%2520of%2520ReLU%2520NNs%252C%2520we%2520show%250Athat%2520their%2520ill-conditioning%2520degrades%2520rapidly%2520as%2520the%2520width%2520of%2520the%2520network%250Aincreases.%2520This%2520often%2520leads%2520to%2520significantly%2520poorer%2520approximation%2520in%2520contrast%250Ato%2520the%2520FKS%2520representation%252C%2520which%2520remains%2520well-conditioned%2520as%2520the%2520number%2520of%250Aknots%2520increases.%2520We%2520leverage%2520the%2520theory%2520of%2520optimal%2520piecewise%2520linear%250Ainterpolants%2520to%2520improve%2520the%2520training%2520procedure%2520for%2520a%2520ReLU%2520NN.%2520Using%2520the%250Aequidistribution%2520principle%252C%2520we%2520propose%2520a%2520two-level%2520procedure%2520for%2520training%2520the%250AFKS%2520by%2520first%2520solving%2520the%2520nonlinear%2520problem%2520of%2520finding%2520the%2520optimal%2520knot%250Alocations%2520of%2520the%2520interpolating%2520FKS.%2520Determining%2520the%2520optimal%2520knots%2520then%2520acts%2520as%250Aa%2520good%2520starting%2520point%2520for%2520training%2520the%2520weights%2520of%2520the%2520FKS.%2520The%2520training%2520of%2520the%250AFKS%2520gives%2520insights%2520into%2520how%2520we%2520can%2520train%2520a%2520ReLU%2520NN%2520effectively%2520to%2520give%2520an%250Aequally%2520accurate%2520approximation.%2520More%2520precisely%252C%2520we%2520combine%2520the%2520training%2520of%2520the%250AReLU%2520NN%2520with%2520an%2520equidistribution%2520based%2520loss%2520to%2520find%2520the%2520breakpoints%2520of%2520the%2520ReLU%250Afunctions%252C%2520combined%2520with%2520preconditioning%2520the%2520ReLU%2520NN%2520approximation%2520%2528to%2520take%2520an%250AFKS%2520form%2529%2520to%2520find%2520the%2520scalings%2520of%2520the%2520ReLU%2520functions%252C%2520leads%2520to%2520a%250Awell-conditioned%2520and%2520reliable%2520method%2520of%2520finding%2520an%2520accurate%2520ReLU%2520NN%250Aapproximation%2520to%2520a%2520target%2520function.%2520We%2520test%2520this%2520method%2520on%2520a%2520series%2520or%2520regular%252C%250Asingular%252C%2520and%2520rapidly%2520varying%2520target%2520functions%2520and%2520obtain%2520good%2520results%250Arealising%2520the%2520expressivity%2520of%2520the%2520network%2520in%2520this%2520case.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equidistribution-based%20training%20of%20Free%20Knot%20Splines%20and%20ReLU%20Neural%0A%20%20Networks&entry.906535625=Simone%20Appella%20and%20Simon%20Arridge%20and%20Chris%20Budd%20and%20Teo%20Deveney%20and%20Lisa%20Maria%20Kreusser&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20one-dimensional%20function%20approximation%20using%0Ashallow%20neural%20networks%20%28NN%29%20with%20a%20rectified%20linear%20unit%20%28ReLU%29%20activation%0Afunction%20and%20compare%20their%20training%20with%20traditional%20methods%20such%20as%20univariate%0AFree%20Knot%20Splines%20%28FKS%29.%20ReLU%20NNs%20and%20FKS%20span%20the%20same%20function%20space%2C%20and%0Athus%20have%20the%20same%20theoretical%20expressivity.%20In%20the%20case%20of%20ReLU%20NNs%2C%20we%20show%0Athat%20their%20ill-conditioning%20degrades%20rapidly%20as%20the%20width%20of%20the%20network%0Aincreases.%20This%20often%20leads%20to%20significantly%20poorer%20approximation%20in%20contrast%0Ato%20the%20FKS%20representation%2C%20which%20remains%20well-conditioned%20as%20the%20number%20of%0Aknots%20increases.%20We%20leverage%20the%20theory%20of%20optimal%20piecewise%20linear%0Ainterpolants%20to%20improve%20the%20training%20procedure%20for%20a%20ReLU%20NN.%20Using%20the%0Aequidistribution%20principle%2C%20we%20propose%20a%20two-level%20procedure%20for%20training%20the%0AFKS%20by%20first%20solving%20the%20nonlinear%20problem%20of%20finding%20the%20optimal%20knot%0Alocations%20of%20the%20interpolating%20FKS.%20Determining%20the%20optimal%20knots%20then%20acts%20as%0Aa%20good%20starting%20point%20for%20training%20the%20weights%20of%20the%20FKS.%20The%20training%20of%20the%0AFKS%20gives%20insights%20into%20how%20we%20can%20train%20a%20ReLU%20NN%20effectively%20to%20give%20an%0Aequally%20accurate%20approximation.%20More%20precisely%2C%20we%20combine%20the%20training%20of%20the%0AReLU%20NN%20with%20an%20equidistribution%20based%20loss%20to%20find%20the%20breakpoints%20of%20the%20ReLU%0Afunctions%2C%20combined%20with%20preconditioning%20the%20ReLU%20NN%20approximation%20%28to%20take%20an%0AFKS%20form%29%20to%20find%20the%20scalings%20of%20the%20ReLU%20functions%2C%20leads%20to%20a%0Awell-conditioned%20and%20reliable%20method%20of%20finding%20an%20accurate%20ReLU%20NN%0Aapproximation%20to%20a%20target%20function.%20We%20test%20this%20method%20on%20a%20series%20or%20regular%2C%0Asingular%2C%20and%20rapidly%20varying%20target%20functions%20and%20obtain%20good%20results%0Arealising%20the%20expressivity%20of%20the%20network%20in%20this%20case.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02153v1&entry.124074799=Read"},
{"title": "Sign Language Recognition Based On Facial Expression and Hand Skeleton", "author": "Zhiyu Long and Xingyou Liu and Jiaqi Qiao and Zhi Li", "abstract": "  Sign language is a visual language used by the deaf and dumb community to\ncommunicate. However, for most recognition methods based on monocular cameras,\nthe recognition accuracy is low and the robustness is poor. Even if the effect\nis good on some data, it may perform poorly in other data with different\ninterference due to the inability to extract effective features. To solve these\nproblems, we propose a sign language recognition network that integrates\nskeleton features of hands and facial expression. Among this, we propose a hand\nskeleton feature extraction based on coordinate transformation to describe the\nshape of the hand more accurately. Moreover, by incorporating facial expression\ninformation, the accuracy and robustness of sign language recognition are\nfinally improved, which was verified on A Dataset for Argentinian Sign Language\nand SEU's Chinese Sign Language Recognition Database (SEUCSLRD).\n", "link": "http://arxiv.org/abs/2407.02241v1", "date": "2024-07-02", "relevancy": 2.265, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.455}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign%20Language%20Recognition%20Based%20On%20Facial%20Expression%20and%20Hand%20Skeleton&body=Title%3A%20Sign%20Language%20Recognition%20Based%20On%20Facial%20Expression%20and%20Hand%20Skeleton%0AAuthor%3A%20Zhiyu%20Long%20and%20Xingyou%20Liu%20and%20Jiaqi%20Qiao%20and%20Zhi%20Li%0AAbstract%3A%20%20%20Sign%20language%20is%20a%20visual%20language%20used%20by%20the%20deaf%20and%20dumb%20community%20to%0Acommunicate.%20However%2C%20for%20most%20recognition%20methods%20based%20on%20monocular%20cameras%2C%0Athe%20recognition%20accuracy%20is%20low%20and%20the%20robustness%20is%20poor.%20Even%20if%20the%20effect%0Ais%20good%20on%20some%20data%2C%20it%20may%20perform%20poorly%20in%20other%20data%20with%20different%0Ainterference%20due%20to%20the%20inability%20to%20extract%20effective%20features.%20To%20solve%20these%0Aproblems%2C%20we%20propose%20a%20sign%20language%20recognition%20network%20that%20integrates%0Askeleton%20features%20of%20hands%20and%20facial%20expression.%20Among%20this%2C%20we%20propose%20a%20hand%0Askeleton%20feature%20extraction%20based%20on%20coordinate%20transformation%20to%20describe%20the%0Ashape%20of%20the%20hand%20more%20accurately.%20Moreover%2C%20by%20incorporating%20facial%20expression%0Ainformation%2C%20the%20accuracy%20and%20robustness%20of%20sign%20language%20recognition%20are%0Afinally%20improved%2C%20which%20was%20verified%20on%20A%20Dataset%20for%20Argentinian%20Sign%20Language%0Aand%20SEU%27s%20Chinese%20Sign%20Language%20Recognition%20Database%20%28SEUCSLRD%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign%2520Language%2520Recognition%2520Based%2520On%2520Facial%2520Expression%2520and%2520Hand%2520Skeleton%26entry.906535625%3DZhiyu%2520Long%2520and%2520Xingyou%2520Liu%2520and%2520Jiaqi%2520Qiao%2520and%2520Zhi%2520Li%26entry.1292438233%3D%2520%2520Sign%2520language%2520is%2520a%2520visual%2520language%2520used%2520by%2520the%2520deaf%2520and%2520dumb%2520community%2520to%250Acommunicate.%2520However%252C%2520for%2520most%2520recognition%2520methods%2520based%2520on%2520monocular%2520cameras%252C%250Athe%2520recognition%2520accuracy%2520is%2520low%2520and%2520the%2520robustness%2520is%2520poor.%2520Even%2520if%2520the%2520effect%250Ais%2520good%2520on%2520some%2520data%252C%2520it%2520may%2520perform%2520poorly%2520in%2520other%2520data%2520with%2520different%250Ainterference%2520due%2520to%2520the%2520inability%2520to%2520extract%2520effective%2520features.%2520To%2520solve%2520these%250Aproblems%252C%2520we%2520propose%2520a%2520sign%2520language%2520recognition%2520network%2520that%2520integrates%250Askeleton%2520features%2520of%2520hands%2520and%2520facial%2520expression.%2520Among%2520this%252C%2520we%2520propose%2520a%2520hand%250Askeleton%2520feature%2520extraction%2520based%2520on%2520coordinate%2520transformation%2520to%2520describe%2520the%250Ashape%2520of%2520the%2520hand%2520more%2520accurately.%2520Moreover%252C%2520by%2520incorporating%2520facial%2520expression%250Ainformation%252C%2520the%2520accuracy%2520and%2520robustness%2520of%2520sign%2520language%2520recognition%2520are%250Afinally%2520improved%252C%2520which%2520was%2520verified%2520on%2520A%2520Dataset%2520for%2520Argentinian%2520Sign%2520Language%250Aand%2520SEU%2527s%2520Chinese%2520Sign%2520Language%2520Recognition%2520Database%2520%2528SEUCSLRD%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign%20Language%20Recognition%20Based%20On%20Facial%20Expression%20and%20Hand%20Skeleton&entry.906535625=Zhiyu%20Long%20and%20Xingyou%20Liu%20and%20Jiaqi%20Qiao%20and%20Zhi%20Li&entry.1292438233=%20%20Sign%20language%20is%20a%20visual%20language%20used%20by%20the%20deaf%20and%20dumb%20community%20to%0Acommunicate.%20However%2C%20for%20most%20recognition%20methods%20based%20on%20monocular%20cameras%2C%0Athe%20recognition%20accuracy%20is%20low%20and%20the%20robustness%20is%20poor.%20Even%20if%20the%20effect%0Ais%20good%20on%20some%20data%2C%20it%20may%20perform%20poorly%20in%20other%20data%20with%20different%0Ainterference%20due%20to%20the%20inability%20to%20extract%20effective%20features.%20To%20solve%20these%0Aproblems%2C%20we%20propose%20a%20sign%20language%20recognition%20network%20that%20integrates%0Askeleton%20features%20of%20hands%20and%20facial%20expression.%20Among%20this%2C%20we%20propose%20a%20hand%0Askeleton%20feature%20extraction%20based%20on%20coordinate%20transformation%20to%20describe%20the%0Ashape%20of%20the%20hand%20more%20accurately.%20Moreover%2C%20by%20incorporating%20facial%20expression%0Ainformation%2C%20the%20accuracy%20and%20robustness%20of%20sign%20language%20recognition%20are%0Afinally%20improved%2C%20which%20was%20verified%20on%20A%20Dataset%20for%20Argentinian%20Sign%20Language%0Aand%20SEU%27s%20Chinese%20Sign%20Language%20Recognition%20Database%20%28SEUCSLRD%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02241v1&entry.124074799=Read"},
{"title": "On the Anatomy of Attention", "author": "Nikhil Khatri and Tuomas Laakkonen and Jonathon Liu and Vincent Wang-Ma\u015bcianica", "abstract": "  We introduce a category-theoretic diagrammatic formalism in order to\nsystematically relate and reason about machine learning models. Our diagrams\npresent architectures intuitively but without loss of essential detail, where\nnatural relationships between models are captured by graphical transformations,\nand important differences and similarities can be identified at a glance. In\nthis paper, we focus on attention mechanisms: translating folklore into\nmathematical derivations, and constructing a taxonomy of attention variants in\nthe literature. As a first example of an empirical investigation underpinned by\nour formalism, we identify recurring anatomical components of attention, which\nwe exhaustively recombine to explore a space of variations on the attention\nmechanism.\n", "link": "http://arxiv.org/abs/2407.02423v1", "date": "2024-07-02", "relevancy": 2.261, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Anatomy%20of%20Attention&body=Title%3A%20On%20the%20Anatomy%20of%20Attention%0AAuthor%3A%20Nikhil%20Khatri%20and%20Tuomas%20Laakkonen%20and%20Jonathon%20Liu%20and%20Vincent%20Wang-Ma%C5%9Bcianica%0AAbstract%3A%20%20%20We%20introduce%20a%20category-theoretic%20diagrammatic%20formalism%20in%20order%20to%0Asystematically%20relate%20and%20reason%20about%20machine%20learning%20models.%20Our%20diagrams%0Apresent%20architectures%20intuitively%20but%20without%20loss%20of%20essential%20detail%2C%20where%0Anatural%20relationships%20between%20models%20are%20captured%20by%20graphical%20transformations%2C%0Aand%20important%20differences%20and%20similarities%20can%20be%20identified%20at%20a%20glance.%20In%0Athis%20paper%2C%20we%20focus%20on%20attention%20mechanisms%3A%20translating%20folklore%20into%0Amathematical%20derivations%2C%20and%20constructing%20a%20taxonomy%20of%20attention%20variants%20in%0Athe%20literature.%20As%20a%20first%20example%20of%20an%20empirical%20investigation%20underpinned%20by%0Aour%20formalism%2C%20we%20identify%20recurring%20anatomical%20components%20of%20attention%2C%20which%0Awe%20exhaustively%20recombine%20to%20explore%20a%20space%20of%20variations%20on%20the%20attention%0Amechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Anatomy%2520of%2520Attention%26entry.906535625%3DNikhil%2520Khatri%2520and%2520Tuomas%2520Laakkonen%2520and%2520Jonathon%2520Liu%2520and%2520Vincent%2520Wang-Ma%25C5%259Bcianica%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520category-theoretic%2520diagrammatic%2520formalism%2520in%2520order%2520to%250Asystematically%2520relate%2520and%2520reason%2520about%2520machine%2520learning%2520models.%2520Our%2520diagrams%250Apresent%2520architectures%2520intuitively%2520but%2520without%2520loss%2520of%2520essential%2520detail%252C%2520where%250Anatural%2520relationships%2520between%2520models%2520are%2520captured%2520by%2520graphical%2520transformations%252C%250Aand%2520important%2520differences%2520and%2520similarities%2520can%2520be%2520identified%2520at%2520a%2520glance.%2520In%250Athis%2520paper%252C%2520we%2520focus%2520on%2520attention%2520mechanisms%253A%2520translating%2520folklore%2520into%250Amathematical%2520derivations%252C%2520and%2520constructing%2520a%2520taxonomy%2520of%2520attention%2520variants%2520in%250Athe%2520literature.%2520As%2520a%2520first%2520example%2520of%2520an%2520empirical%2520investigation%2520underpinned%2520by%250Aour%2520formalism%252C%2520we%2520identify%2520recurring%2520anatomical%2520components%2520of%2520attention%252C%2520which%250Awe%2520exhaustively%2520recombine%2520to%2520explore%2520a%2520space%2520of%2520variations%2520on%2520the%2520attention%250Amechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Anatomy%20of%20Attention&entry.906535625=Nikhil%20Khatri%20and%20Tuomas%20Laakkonen%20and%20Jonathon%20Liu%20and%20Vincent%20Wang-Ma%C5%9Bcianica&entry.1292438233=%20%20We%20introduce%20a%20category-theoretic%20diagrammatic%20formalism%20in%20order%20to%0Asystematically%20relate%20and%20reason%20about%20machine%20learning%20models.%20Our%20diagrams%0Apresent%20architectures%20intuitively%20but%20without%20loss%20of%20essential%20detail%2C%20where%0Anatural%20relationships%20between%20models%20are%20captured%20by%20graphical%20transformations%2C%0Aand%20important%20differences%20and%20similarities%20can%20be%20identified%20at%20a%20glance.%20In%0Athis%20paper%2C%20we%20focus%20on%20attention%20mechanisms%3A%20translating%20folklore%20into%0Amathematical%20derivations%2C%20and%20constructing%20a%20taxonomy%20of%20attention%20variants%20in%0Athe%20literature.%20As%20a%20first%20example%20of%20an%20empirical%20investigation%20underpinned%20by%0Aour%20formalism%2C%20we%20identify%20recurring%20anatomical%20components%20of%20attention%2C%20which%0Awe%20exhaustively%20recombine%20to%20explore%20a%20space%20of%20variations%20on%20the%20attention%0Amechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02423v1&entry.124074799=Read"},
{"title": "HRSAM: Efficiently Segment Anything in High-Resolution Images", "author": "You Huang and Wenbin Lai and Jiayi Ji and Liujuan Cao and Shengchuan Zhang and Rongrong Ji", "abstract": "  The Segment Anything Model (SAM) has significantly advanced interactive\nsegmentation but struggles with high-resolution images crucial for\nhigh-precision segmentation. This is primarily due to the quadratic space\ncomplexity of SAM-implemented attention and the length extrapolation issue in\ncommon global attention. This study proposes HRSAM that integrates Flash\nAttention and incorporates Plain, Shifted and newly proposed Cycle-scan Window\n(PSCWin) attention to address these issues. The shifted window attention is\nredesigned with padding to maintain consistent window sizes, enabling effective\nlength extrapolation. The cycle-scan window attention adopts the recently\ndeveloped State Space Models (SSMs) to ensure global information exchange with\nminimal computational overhead. Such window-based attention allows HRSAM to\nperform effective attention computations on scaled input images while\nmaintaining low latency. Moreover, we further propose HRSAM++ that additionally\nemploys a multi-scale strategy to enhance HRSAM's performance. The experiments\non the high-precision segmentation datasets HQSeg44K and DAVIS show that\nhigh-resolution inputs enable the SAM-distilled HRSAM models to outperform the\nteacher model while maintaining lower latency. Compared to the SOTAs, HRSAM\nachieves a 1.56 improvement in interactive segmentation's NoC95 metric with\nonly 31% of the latency. HRSAM++ further enhances the performance, achieving a\n1.63 improvement in NoC95 with just 38% of the latency.\n", "link": "http://arxiv.org/abs/2407.02109v1", "date": "2024-07-02", "relevancy": 2.2475, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5675}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5597}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HRSAM%3A%20Efficiently%20Segment%20Anything%20in%20High-Resolution%20Images&body=Title%3A%20HRSAM%3A%20Efficiently%20Segment%20Anything%20in%20High-Resolution%20Images%0AAuthor%3A%20You%20Huang%20and%20Wenbin%20Lai%20and%20Jiayi%20Ji%20and%20Liujuan%20Cao%20and%20Shengchuan%20Zhang%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20significantly%20advanced%20interactive%0Asegmentation%20but%20struggles%20with%20high-resolution%20images%20crucial%20for%0Ahigh-precision%20segmentation.%20This%20is%20primarily%20due%20to%20the%20quadratic%20space%0Acomplexity%20of%20SAM-implemented%20attention%20and%20the%20length%20extrapolation%20issue%20in%0Acommon%20global%20attention.%20This%20study%20proposes%20HRSAM%20that%20integrates%20Flash%0AAttention%20and%20incorporates%20Plain%2C%20Shifted%20and%20newly%20proposed%20Cycle-scan%20Window%0A%28PSCWin%29%20attention%20to%20address%20these%20issues.%20The%20shifted%20window%20attention%20is%0Aredesigned%20with%20padding%20to%20maintain%20consistent%20window%20sizes%2C%20enabling%20effective%0Alength%20extrapolation.%20The%20cycle-scan%20window%20attention%20adopts%20the%20recently%0Adeveloped%20State%20Space%20Models%20%28SSMs%29%20to%20ensure%20global%20information%20exchange%20with%0Aminimal%20computational%20overhead.%20Such%20window-based%20attention%20allows%20HRSAM%20to%0Aperform%20effective%20attention%20computations%20on%20scaled%20input%20images%20while%0Amaintaining%20low%20latency.%20Moreover%2C%20we%20further%20propose%20HRSAM%2B%2B%20that%20additionally%0Aemploys%20a%20multi-scale%20strategy%20to%20enhance%20HRSAM%27s%20performance.%20The%20experiments%0Aon%20the%20high-precision%20segmentation%20datasets%20HQSeg44K%20and%20DAVIS%20show%20that%0Ahigh-resolution%20inputs%20enable%20the%20SAM-distilled%20HRSAM%20models%20to%20outperform%20the%0Ateacher%20model%20while%20maintaining%20lower%20latency.%20Compared%20to%20the%20SOTAs%2C%20HRSAM%0Aachieves%20a%201.56%20improvement%20in%20interactive%20segmentation%27s%20NoC95%20metric%20with%0Aonly%2031%25%20of%20the%20latency.%20HRSAM%2B%2B%20further%20enhances%20the%20performance%2C%20achieving%20a%0A1.63%20improvement%20in%20NoC95%20with%20just%2038%25%20of%20the%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHRSAM%253A%2520Efficiently%2520Segment%2520Anything%2520in%2520High-Resolution%2520Images%26entry.906535625%3DYou%2520Huang%2520and%2520Wenbin%2520Lai%2520and%2520Jiayi%2520Ji%2520and%2520Liujuan%2520Cao%2520and%2520Shengchuan%2520Zhang%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520significantly%2520advanced%2520interactive%250Asegmentation%2520but%2520struggles%2520with%2520high-resolution%2520images%2520crucial%2520for%250Ahigh-precision%2520segmentation.%2520This%2520is%2520primarily%2520due%2520to%2520the%2520quadratic%2520space%250Acomplexity%2520of%2520SAM-implemented%2520attention%2520and%2520the%2520length%2520extrapolation%2520issue%2520in%250Acommon%2520global%2520attention.%2520This%2520study%2520proposes%2520HRSAM%2520that%2520integrates%2520Flash%250AAttention%2520and%2520incorporates%2520Plain%252C%2520Shifted%2520and%2520newly%2520proposed%2520Cycle-scan%2520Window%250A%2528PSCWin%2529%2520attention%2520to%2520address%2520these%2520issues.%2520The%2520shifted%2520window%2520attention%2520is%250Aredesigned%2520with%2520padding%2520to%2520maintain%2520consistent%2520window%2520sizes%252C%2520enabling%2520effective%250Alength%2520extrapolation.%2520The%2520cycle-scan%2520window%2520attention%2520adopts%2520the%2520recently%250Adeveloped%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520to%2520ensure%2520global%2520information%2520exchange%2520with%250Aminimal%2520computational%2520overhead.%2520Such%2520window-based%2520attention%2520allows%2520HRSAM%2520to%250Aperform%2520effective%2520attention%2520computations%2520on%2520scaled%2520input%2520images%2520while%250Amaintaining%2520low%2520latency.%2520Moreover%252C%2520we%2520further%2520propose%2520HRSAM%252B%252B%2520that%2520additionally%250Aemploys%2520a%2520multi-scale%2520strategy%2520to%2520enhance%2520HRSAM%2527s%2520performance.%2520The%2520experiments%250Aon%2520the%2520high-precision%2520segmentation%2520datasets%2520HQSeg44K%2520and%2520DAVIS%2520show%2520that%250Ahigh-resolution%2520inputs%2520enable%2520the%2520SAM-distilled%2520HRSAM%2520models%2520to%2520outperform%2520the%250Ateacher%2520model%2520while%2520maintaining%2520lower%2520latency.%2520Compared%2520to%2520the%2520SOTAs%252C%2520HRSAM%250Aachieves%2520a%25201.56%2520improvement%2520in%2520interactive%2520segmentation%2527s%2520NoC95%2520metric%2520with%250Aonly%252031%2525%2520of%2520the%2520latency.%2520HRSAM%252B%252B%2520further%2520enhances%2520the%2520performance%252C%2520achieving%2520a%250A1.63%2520improvement%2520in%2520NoC95%2520with%2520just%252038%2525%2520of%2520the%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRSAM%3A%20Efficiently%20Segment%20Anything%20in%20High-Resolution%20Images&entry.906535625=You%20Huang%20and%20Wenbin%20Lai%20and%20Jiayi%20Ji%20and%20Liujuan%20Cao%20and%20Shengchuan%20Zhang%20and%20Rongrong%20Ji&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20significantly%20advanced%20interactive%0Asegmentation%20but%20struggles%20with%20high-resolution%20images%20crucial%20for%0Ahigh-precision%20segmentation.%20This%20is%20primarily%20due%20to%20the%20quadratic%20space%0Acomplexity%20of%20SAM-implemented%20attention%20and%20the%20length%20extrapolation%20issue%20in%0Acommon%20global%20attention.%20This%20study%20proposes%20HRSAM%20that%20integrates%20Flash%0AAttention%20and%20incorporates%20Plain%2C%20Shifted%20and%20newly%20proposed%20Cycle-scan%20Window%0A%28PSCWin%29%20attention%20to%20address%20these%20issues.%20The%20shifted%20window%20attention%20is%0Aredesigned%20with%20padding%20to%20maintain%20consistent%20window%20sizes%2C%20enabling%20effective%0Alength%20extrapolation.%20The%20cycle-scan%20window%20attention%20adopts%20the%20recently%0Adeveloped%20State%20Space%20Models%20%28SSMs%29%20to%20ensure%20global%20information%20exchange%20with%0Aminimal%20computational%20overhead.%20Such%20window-based%20attention%20allows%20HRSAM%20to%0Aperform%20effective%20attention%20computations%20on%20scaled%20input%20images%20while%0Amaintaining%20low%20latency.%20Moreover%2C%20we%20further%20propose%20HRSAM%2B%2B%20that%20additionally%0Aemploys%20a%20multi-scale%20strategy%20to%20enhance%20HRSAM%27s%20performance.%20The%20experiments%0Aon%20the%20high-precision%20segmentation%20datasets%20HQSeg44K%20and%20DAVIS%20show%20that%0Ahigh-resolution%20inputs%20enable%20the%20SAM-distilled%20HRSAM%20models%20to%20outperform%20the%0Ateacher%20model%20while%20maintaining%20lower%20latency.%20Compared%20to%20the%20SOTAs%2C%20HRSAM%0Aachieves%20a%201.56%20improvement%20in%20interactive%20segmentation%27s%20NoC95%20metric%20with%0Aonly%2031%25%20of%20the%20latency.%20HRSAM%2B%2B%20further%20enhances%20the%20performance%2C%20achieving%20a%0A1.63%20improvement%20in%20NoC95%20with%20just%2038%25%20of%20the%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02109v1&entry.124074799=Read"},
{"title": "Masked Attribute Description Embedding for Cloth-Changing Person\n  Re-identification", "author": "Chunlei Peng and Boyu Wang and Decheng Liu and Nannan Wang and Ruimin Hu and Xinbo Gao", "abstract": "  Cloth-changing person re-identification (CC-ReID) aims to match persons who\nchange clothes over long periods. The key challenge in CC-ReID is to extract\nclothing-independent features, such as face, hairstyle, body shape, and gait.\nCurrent research mainly focuses on modeling body shape using multi-modal\nbiological features (such as silhouettes and sketches). However, it does not\nfully leverage the personal description information hidden in the original RGB\nimage. Considering that there are certain attribute descriptions which remain\nunchanged after the changing of cloth, we propose a Masked Attribute\nDescription Embedding (MADE) method that unifies personal visual appearance and\nattribute description for CC-ReID. Specifically, handling variable\nclothing-sensitive information, such as color and type, is challenging for\neffective modeling. To address this, we mask the clothing and color information\nin the personal attribute description extracted through an attribute detection\nmodel. The masked attribute description is then connected and embedded into\nTransformer blocks at various levels, fusing it with the low-level to\nhigh-level features of the image. This approach compels the model to discard\nclothing information. Experiments are conducted on several CC-ReID benchmarks,\nincluding PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE\neffectively utilizes attribute description, enhancing cloth-changing person\nre-identification performance, and compares favorably with state-of-the-art\nmethods. The code is available at https://github.com/moon-wh/MADE.\n", "link": "http://arxiv.org/abs/2401.05646v3", "date": "2024-07-02", "relevancy": 2.2363, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5759}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5594}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Attribute%20Description%20Embedding%20for%20Cloth-Changing%20Person%0A%20%20Re-identification&body=Title%3A%20Masked%20Attribute%20Description%20Embedding%20for%20Cloth-Changing%20Person%0A%20%20Re-identification%0AAuthor%3A%20Chunlei%20Peng%20and%20Boyu%20Wang%20and%20Decheng%20Liu%20and%20Nannan%20Wang%20and%20Ruimin%20Hu%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Cloth-changing%20person%20re-identification%20%28CC-ReID%29%20aims%20to%20match%20persons%20who%0Achange%20clothes%20over%20long%20periods.%20The%20key%20challenge%20in%20CC-ReID%20is%20to%20extract%0Aclothing-independent%20features%2C%20such%20as%20face%2C%20hairstyle%2C%20body%20shape%2C%20and%20gait.%0ACurrent%20research%20mainly%20focuses%20on%20modeling%20body%20shape%20using%20multi-modal%0Abiological%20features%20%28such%20as%20silhouettes%20and%20sketches%29.%20However%2C%20it%20does%20not%0Afully%20leverage%20the%20personal%20description%20information%20hidden%20in%20the%20original%20RGB%0Aimage.%20Considering%20that%20there%20are%20certain%20attribute%20descriptions%20which%20remain%0Aunchanged%20after%20the%20changing%20of%20cloth%2C%20we%20propose%20a%20Masked%20Attribute%0ADescription%20Embedding%20%28MADE%29%20method%20that%20unifies%20personal%20visual%20appearance%20and%0Aattribute%20description%20for%20CC-ReID.%20Specifically%2C%20handling%20variable%0Aclothing-sensitive%20information%2C%20such%20as%20color%20and%20type%2C%20is%20challenging%20for%0Aeffective%20modeling.%20To%20address%20this%2C%20we%20mask%20the%20clothing%20and%20color%20information%0Ain%20the%20personal%20attribute%20description%20extracted%20through%20an%20attribute%20detection%0Amodel.%20The%20masked%20attribute%20description%20is%20then%20connected%20and%20embedded%20into%0ATransformer%20blocks%20at%20various%20levels%2C%20fusing%20it%20with%20the%20low-level%20to%0Ahigh-level%20features%20of%20the%20image.%20This%20approach%20compels%20the%20model%20to%20discard%0Aclothing%20information.%20Experiments%20are%20conducted%20on%20several%20CC-ReID%20benchmarks%2C%0Aincluding%20PRCC%2C%20LTCC%2C%20Celeb-reID-light%2C%20and%20LaST.%20Results%20demonstrate%20that%20MADE%0Aeffectively%20utilizes%20attribute%20description%2C%20enhancing%20cloth-changing%20person%0Are-identification%20performance%2C%20and%20compares%20favorably%20with%20state-of-the-art%0Amethods.%20The%20code%20is%20available%20at%20https%3A//github.com/moon-wh/MADE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05646v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Attribute%2520Description%2520Embedding%2520for%2520Cloth-Changing%2520Person%250A%2520%2520Re-identification%26entry.906535625%3DChunlei%2520Peng%2520and%2520Boyu%2520Wang%2520and%2520Decheng%2520Liu%2520and%2520Nannan%2520Wang%2520and%2520Ruimin%2520Hu%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Cloth-changing%2520person%2520re-identification%2520%2528CC-ReID%2529%2520aims%2520to%2520match%2520persons%2520who%250Achange%2520clothes%2520over%2520long%2520periods.%2520The%2520key%2520challenge%2520in%2520CC-ReID%2520is%2520to%2520extract%250Aclothing-independent%2520features%252C%2520such%2520as%2520face%252C%2520hairstyle%252C%2520body%2520shape%252C%2520and%2520gait.%250ACurrent%2520research%2520mainly%2520focuses%2520on%2520modeling%2520body%2520shape%2520using%2520multi-modal%250Abiological%2520features%2520%2528such%2520as%2520silhouettes%2520and%2520sketches%2529.%2520However%252C%2520it%2520does%2520not%250Afully%2520leverage%2520the%2520personal%2520description%2520information%2520hidden%2520in%2520the%2520original%2520RGB%250Aimage.%2520Considering%2520that%2520there%2520are%2520certain%2520attribute%2520descriptions%2520which%2520remain%250Aunchanged%2520after%2520the%2520changing%2520of%2520cloth%252C%2520we%2520propose%2520a%2520Masked%2520Attribute%250ADescription%2520Embedding%2520%2528MADE%2529%2520method%2520that%2520unifies%2520personal%2520visual%2520appearance%2520and%250Aattribute%2520description%2520for%2520CC-ReID.%2520Specifically%252C%2520handling%2520variable%250Aclothing-sensitive%2520information%252C%2520such%2520as%2520color%2520and%2520type%252C%2520is%2520challenging%2520for%250Aeffective%2520modeling.%2520To%2520address%2520this%252C%2520we%2520mask%2520the%2520clothing%2520and%2520color%2520information%250Ain%2520the%2520personal%2520attribute%2520description%2520extracted%2520through%2520an%2520attribute%2520detection%250Amodel.%2520The%2520masked%2520attribute%2520description%2520is%2520then%2520connected%2520and%2520embedded%2520into%250ATransformer%2520blocks%2520at%2520various%2520levels%252C%2520fusing%2520it%2520with%2520the%2520low-level%2520to%250Ahigh-level%2520features%2520of%2520the%2520image.%2520This%2520approach%2520compels%2520the%2520model%2520to%2520discard%250Aclothing%2520information.%2520Experiments%2520are%2520conducted%2520on%2520several%2520CC-ReID%2520benchmarks%252C%250Aincluding%2520PRCC%252C%2520LTCC%252C%2520Celeb-reID-light%252C%2520and%2520LaST.%2520Results%2520demonstrate%2520that%2520MADE%250Aeffectively%2520utilizes%2520attribute%2520description%252C%2520enhancing%2520cloth-changing%2520person%250Are-identification%2520performance%252C%2520and%2520compares%2520favorably%2520with%2520state-of-the-art%250Amethods.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/moon-wh/MADE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05646v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Attribute%20Description%20Embedding%20for%20Cloth-Changing%20Person%0A%20%20Re-identification&entry.906535625=Chunlei%20Peng%20and%20Boyu%20Wang%20and%20Decheng%20Liu%20and%20Nannan%20Wang%20and%20Ruimin%20Hu%20and%20Xinbo%20Gao&entry.1292438233=%20%20Cloth-changing%20person%20re-identification%20%28CC-ReID%29%20aims%20to%20match%20persons%20who%0Achange%20clothes%20over%20long%20periods.%20The%20key%20challenge%20in%20CC-ReID%20is%20to%20extract%0Aclothing-independent%20features%2C%20such%20as%20face%2C%20hairstyle%2C%20body%20shape%2C%20and%20gait.%0ACurrent%20research%20mainly%20focuses%20on%20modeling%20body%20shape%20using%20multi-modal%0Abiological%20features%20%28such%20as%20silhouettes%20and%20sketches%29.%20However%2C%20it%20does%20not%0Afully%20leverage%20the%20personal%20description%20information%20hidden%20in%20the%20original%20RGB%0Aimage.%20Considering%20that%20there%20are%20certain%20attribute%20descriptions%20which%20remain%0Aunchanged%20after%20the%20changing%20of%20cloth%2C%20we%20propose%20a%20Masked%20Attribute%0ADescription%20Embedding%20%28MADE%29%20method%20that%20unifies%20personal%20visual%20appearance%20and%0Aattribute%20description%20for%20CC-ReID.%20Specifically%2C%20handling%20variable%0Aclothing-sensitive%20information%2C%20such%20as%20color%20and%20type%2C%20is%20challenging%20for%0Aeffective%20modeling.%20To%20address%20this%2C%20we%20mask%20the%20clothing%20and%20color%20information%0Ain%20the%20personal%20attribute%20description%20extracted%20through%20an%20attribute%20detection%0Amodel.%20The%20masked%20attribute%20description%20is%20then%20connected%20and%20embedded%20into%0ATransformer%20blocks%20at%20various%20levels%2C%20fusing%20it%20with%20the%20low-level%20to%0Ahigh-level%20features%20of%20the%20image.%20This%20approach%20compels%20the%20model%20to%20discard%0Aclothing%20information.%20Experiments%20are%20conducted%20on%20several%20CC-ReID%20benchmarks%2C%0Aincluding%20PRCC%2C%20LTCC%2C%20Celeb-reID-light%2C%20and%20LaST.%20Results%20demonstrate%20that%20MADE%0Aeffectively%20utilizes%20attribute%20description%2C%20enhancing%20cloth-changing%20person%0Are-identification%20performance%2C%20and%20compares%20favorably%20with%20state-of-the-art%0Amethods.%20The%20code%20is%20available%20at%20https%3A//github.com/moon-wh/MADE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05646v3&entry.124074799=Read"},
{"title": "Real Time Collision Avoidance with GPU-Computed Distance Maps", "author": "Wendwosen Bellete Bedada and Gianluca Palli", "abstract": "  This paper presents reactive obstacle and self-collision avoidance of\nredundant robotic manipulators within real time kinematic feedback control\nusing GPU-computed distance transform. The proposed framework utilizes\ndiscretized representation of the robot and the environment to calculate 3D\nEuclidean distance transform for task-priority based kinematic control. The\nenvironment scene is represented using a 3D GPU-voxel map created and updated\nfrom a live pointcloud data while the robotic link model is converted into a\nvoxels offline and inserted into the voxel map according to the joint state of\nthe robot to form the self-obstacle map. The proposed approach is evaluated\nusing the Tiago robot, showing that all obstacle and self collision avoidance\nconstraints are respected within one framework even with fast moving obstacles\nwhile the robot performs end-effector pose tracking in real time. A comparison\nof related works that depend on GPU and CPU computed distance fields is also\npresented to highlight the time performance as well as accuracy of the GPU\ndistance field.\n", "link": "http://arxiv.org/abs/2407.02363v1", "date": "2024-07-02", "relevancy": 2.2356, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real%20Time%20Collision%20Avoidance%20with%20GPU-Computed%20Distance%20Maps&body=Title%3A%20Real%20Time%20Collision%20Avoidance%20with%20GPU-Computed%20Distance%20Maps%0AAuthor%3A%20Wendwosen%20Bellete%20Bedada%20and%20Gianluca%20Palli%0AAbstract%3A%20%20%20This%20paper%20presents%20reactive%20obstacle%20and%20self-collision%20avoidance%20of%0Aredundant%20robotic%20manipulators%20within%20real%20time%20kinematic%20feedback%20control%0Ausing%20GPU-computed%20distance%20transform.%20The%20proposed%20framework%20utilizes%0Adiscretized%20representation%20of%20the%20robot%20and%20the%20environment%20to%20calculate%203D%0AEuclidean%20distance%20transform%20for%20task-priority%20based%20kinematic%20control.%20The%0Aenvironment%20scene%20is%20represented%20using%20a%203D%20GPU-voxel%20map%20created%20and%20updated%0Afrom%20a%20live%20pointcloud%20data%20while%20the%20robotic%20link%20model%20is%20converted%20into%20a%0Avoxels%20offline%20and%20inserted%20into%20the%20voxel%20map%20according%20to%20the%20joint%20state%20of%0Athe%20robot%20to%20form%20the%20self-obstacle%20map.%20The%20proposed%20approach%20is%20evaluated%0Ausing%20the%20Tiago%20robot%2C%20showing%20that%20all%20obstacle%20and%20self%20collision%20avoidance%0Aconstraints%20are%20respected%20within%20one%20framework%20even%20with%20fast%20moving%20obstacles%0Awhile%20the%20robot%20performs%20end-effector%20pose%20tracking%20in%20real%20time.%20A%20comparison%0Aof%20related%20works%20that%20depend%20on%20GPU%20and%20CPU%20computed%20distance%20fields%20is%20also%0Apresented%20to%20highlight%20the%20time%20performance%20as%20well%20as%20accuracy%20of%20the%20GPU%0Adistance%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal%2520Time%2520Collision%2520Avoidance%2520with%2520GPU-Computed%2520Distance%2520Maps%26entry.906535625%3DWendwosen%2520Bellete%2520Bedada%2520and%2520Gianluca%2520Palli%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520reactive%2520obstacle%2520and%2520self-collision%2520avoidance%2520of%250Aredundant%2520robotic%2520manipulators%2520within%2520real%2520time%2520kinematic%2520feedback%2520control%250Ausing%2520GPU-computed%2520distance%2520transform.%2520The%2520proposed%2520framework%2520utilizes%250Adiscretized%2520representation%2520of%2520the%2520robot%2520and%2520the%2520environment%2520to%2520calculate%25203D%250AEuclidean%2520distance%2520transform%2520for%2520task-priority%2520based%2520kinematic%2520control.%2520The%250Aenvironment%2520scene%2520is%2520represented%2520using%2520a%25203D%2520GPU-voxel%2520map%2520created%2520and%2520updated%250Afrom%2520a%2520live%2520pointcloud%2520data%2520while%2520the%2520robotic%2520link%2520model%2520is%2520converted%2520into%2520a%250Avoxels%2520offline%2520and%2520inserted%2520into%2520the%2520voxel%2520map%2520according%2520to%2520the%2520joint%2520state%2520of%250Athe%2520robot%2520to%2520form%2520the%2520self-obstacle%2520map.%2520The%2520proposed%2520approach%2520is%2520evaluated%250Ausing%2520the%2520Tiago%2520robot%252C%2520showing%2520that%2520all%2520obstacle%2520and%2520self%2520collision%2520avoidance%250Aconstraints%2520are%2520respected%2520within%2520one%2520framework%2520even%2520with%2520fast%2520moving%2520obstacles%250Awhile%2520the%2520robot%2520performs%2520end-effector%2520pose%2520tracking%2520in%2520real%2520time.%2520A%2520comparison%250Aof%2520related%2520works%2520that%2520depend%2520on%2520GPU%2520and%2520CPU%2520computed%2520distance%2520fields%2520is%2520also%250Apresented%2520to%2520highlight%2520the%2520time%2520performance%2520as%2520well%2520as%2520accuracy%2520of%2520the%2520GPU%250Adistance%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20Time%20Collision%20Avoidance%20with%20GPU-Computed%20Distance%20Maps&entry.906535625=Wendwosen%20Bellete%20Bedada%20and%20Gianluca%20Palli&entry.1292438233=%20%20This%20paper%20presents%20reactive%20obstacle%20and%20self-collision%20avoidance%20of%0Aredundant%20robotic%20manipulators%20within%20real%20time%20kinematic%20feedback%20control%0Ausing%20GPU-computed%20distance%20transform.%20The%20proposed%20framework%20utilizes%0Adiscretized%20representation%20of%20the%20robot%20and%20the%20environment%20to%20calculate%203D%0AEuclidean%20distance%20transform%20for%20task-priority%20based%20kinematic%20control.%20The%0Aenvironment%20scene%20is%20represented%20using%20a%203D%20GPU-voxel%20map%20created%20and%20updated%0Afrom%20a%20live%20pointcloud%20data%20while%20the%20robotic%20link%20model%20is%20converted%20into%20a%0Avoxels%20offline%20and%20inserted%20into%20the%20voxel%20map%20according%20to%20the%20joint%20state%20of%0Athe%20robot%20to%20form%20the%20self-obstacle%20map.%20The%20proposed%20approach%20is%20evaluated%0Ausing%20the%20Tiago%20robot%2C%20showing%20that%20all%20obstacle%20and%20self%20collision%20avoidance%0Aconstraints%20are%20respected%20within%20one%20framework%20even%20with%20fast%20moving%20obstacles%0Awhile%20the%20robot%20performs%20end-effector%20pose%20tracking%20in%20real%20time.%20A%20comparison%0Aof%20related%20works%20that%20depend%20on%20GPU%20and%20CPU%20computed%20distance%20fields%20is%20also%0Apresented%20to%20highlight%20the%20time%20performance%20as%20well%20as%20accuracy%20of%20the%20GPU%0Adistance%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02363v1&entry.124074799=Read"},
{"title": "Occlusion-Aware Seamless Segmentation", "author": "Yihong Cao and Jiaming Zhang and Hao Shi and Kunyu Peng and Yuhongxuan Zhang and Hui Zhang and Rainer Stiefelhagen and Kailun Yang", "abstract": "  Panoramic images can broaden the Field of View (FoV), occlusion-aware\nprediction can deepen the understanding of the scene, and domain adaptation can\ntransfer across viewing domains. In this work, we introduce a novel task,\nOcclusion-Aware Seamless Segmentation (OASS), which simultaneously tackles all\nthese three challenges. For benchmarking OASS, we establish a new\nhuman-annotated dataset for Blending Panoramic Amodal Seamless Segmentation,\ni.e., BlendPASS. Besides, we propose the first solution UnmaskFormer, aiming at\nunmasking the narrow FoV, occlusions, and domain gaps all at once.\nSpecifically, UnmaskFormer includes the crucial designs of Unmasking Attention\n(UA) and Amodal-oriented Mix (AoMix). Our method achieves state-of-the-art\nperformance on the BlendPASS dataset, reaching a remarkable mAPQ of 26.58% and\nmIoU of 43.66%. On public panoramic semantic segmentation datasets, i.e.,\nSynPASS and DensePASS, our method outperforms previous methods and obtains\n45.34% and 48.08% in mIoU, respectively. The fresh BlendPASS dataset and our\nsource code will be made publicly available at\nhttps://github.com/yihong-97/OASS.\n", "link": "http://arxiv.org/abs/2407.02182v1", "date": "2024-07-02", "relevancy": 2.2189, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5404}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion-Aware%20Seamless%20Segmentation&body=Title%3A%20Occlusion-Aware%20Seamless%20Segmentation%0AAuthor%3A%20Yihong%20Cao%20and%20Jiaming%20Zhang%20and%20Hao%20Shi%20and%20Kunyu%20Peng%20and%20Yuhongxuan%20Zhang%20and%20Hui%20Zhang%20and%20Rainer%20Stiefelhagen%20and%20Kailun%20Yang%0AAbstract%3A%20%20%20Panoramic%20images%20can%20broaden%20the%20Field%20of%20View%20%28FoV%29%2C%20occlusion-aware%0Aprediction%20can%20deepen%20the%20understanding%20of%20the%20scene%2C%20and%20domain%20adaptation%20can%0Atransfer%20across%20viewing%20domains.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20task%2C%0AOcclusion-Aware%20Seamless%20Segmentation%20%28OASS%29%2C%20which%20simultaneously%20tackles%20all%0Athese%20three%20challenges.%20For%20benchmarking%20OASS%2C%20we%20establish%20a%20new%0Ahuman-annotated%20dataset%20for%20Blending%20Panoramic%20Amodal%20Seamless%20Segmentation%2C%0Ai.e.%2C%20BlendPASS.%20Besides%2C%20we%20propose%20the%20first%20solution%20UnmaskFormer%2C%20aiming%20at%0Aunmasking%20the%20narrow%20FoV%2C%20occlusions%2C%20and%20domain%20gaps%20all%20at%20once.%0ASpecifically%2C%20UnmaskFormer%20includes%20the%20crucial%20designs%20of%20Unmasking%20Attention%0A%28UA%29%20and%20Amodal-oriented%20Mix%20%28AoMix%29.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20on%20the%20BlendPASS%20dataset%2C%20reaching%20a%20remarkable%20mAPQ%20of%2026.58%25%20and%0AmIoU%20of%2043.66%25.%20On%20public%20panoramic%20semantic%20segmentation%20datasets%2C%20i.e.%2C%0ASynPASS%20and%20DensePASS%2C%20our%20method%20outperforms%20previous%20methods%20and%20obtains%0A45.34%25%20and%2048.08%25%20in%20mIoU%2C%20respectively.%20The%20fresh%20BlendPASS%20dataset%20and%20our%0Asource%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/yihong-97/OASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion-Aware%2520Seamless%2520Segmentation%26entry.906535625%3DYihong%2520Cao%2520and%2520Jiaming%2520Zhang%2520and%2520Hao%2520Shi%2520and%2520Kunyu%2520Peng%2520and%2520Yuhongxuan%2520Zhang%2520and%2520Hui%2520Zhang%2520and%2520Rainer%2520Stiefelhagen%2520and%2520Kailun%2520Yang%26entry.1292438233%3D%2520%2520Panoramic%2520images%2520can%2520broaden%2520the%2520Field%2520of%2520View%2520%2528FoV%2529%252C%2520occlusion-aware%250Aprediction%2520can%2520deepen%2520the%2520understanding%2520of%2520the%2520scene%252C%2520and%2520domain%2520adaptation%2520can%250Atransfer%2520across%2520viewing%2520domains.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520task%252C%250AOcclusion-Aware%2520Seamless%2520Segmentation%2520%2528OASS%2529%252C%2520which%2520simultaneously%2520tackles%2520all%250Athese%2520three%2520challenges.%2520For%2520benchmarking%2520OASS%252C%2520we%2520establish%2520a%2520new%250Ahuman-annotated%2520dataset%2520for%2520Blending%2520Panoramic%2520Amodal%2520Seamless%2520Segmentation%252C%250Ai.e.%252C%2520BlendPASS.%2520Besides%252C%2520we%2520propose%2520the%2520first%2520solution%2520UnmaskFormer%252C%2520aiming%2520at%250Aunmasking%2520the%2520narrow%2520FoV%252C%2520occlusions%252C%2520and%2520domain%2520gaps%2520all%2520at%2520once.%250ASpecifically%252C%2520UnmaskFormer%2520includes%2520the%2520crucial%2520designs%2520of%2520Unmasking%2520Attention%250A%2528UA%2529%2520and%2520Amodal-oriented%2520Mix%2520%2528AoMix%2529.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520BlendPASS%2520dataset%252C%2520reaching%2520a%2520remarkable%2520mAPQ%2520of%252026.58%2525%2520and%250AmIoU%2520of%252043.66%2525.%2520On%2520public%2520panoramic%2520semantic%2520segmentation%2520datasets%252C%2520i.e.%252C%250ASynPASS%2520and%2520DensePASS%252C%2520our%2520method%2520outperforms%2520previous%2520methods%2520and%2520obtains%250A45.34%2525%2520and%252048.08%2525%2520in%2520mIoU%252C%2520respectively.%2520The%2520fresh%2520BlendPASS%2520dataset%2520and%2520our%250Asource%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yihong-97/OASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion-Aware%20Seamless%20Segmentation&entry.906535625=Yihong%20Cao%20and%20Jiaming%20Zhang%20and%20Hao%20Shi%20and%20Kunyu%20Peng%20and%20Yuhongxuan%20Zhang%20and%20Hui%20Zhang%20and%20Rainer%20Stiefelhagen%20and%20Kailun%20Yang&entry.1292438233=%20%20Panoramic%20images%20can%20broaden%20the%20Field%20of%20View%20%28FoV%29%2C%20occlusion-aware%0Aprediction%20can%20deepen%20the%20understanding%20of%20the%20scene%2C%20and%20domain%20adaptation%20can%0Atransfer%20across%20viewing%20domains.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20task%2C%0AOcclusion-Aware%20Seamless%20Segmentation%20%28OASS%29%2C%20which%20simultaneously%20tackles%20all%0Athese%20three%20challenges.%20For%20benchmarking%20OASS%2C%20we%20establish%20a%20new%0Ahuman-annotated%20dataset%20for%20Blending%20Panoramic%20Amodal%20Seamless%20Segmentation%2C%0Ai.e.%2C%20BlendPASS.%20Besides%2C%20we%20propose%20the%20first%20solution%20UnmaskFormer%2C%20aiming%20at%0Aunmasking%20the%20narrow%20FoV%2C%20occlusions%2C%20and%20domain%20gaps%20all%20at%20once.%0ASpecifically%2C%20UnmaskFormer%20includes%20the%20crucial%20designs%20of%20Unmasking%20Attention%0A%28UA%29%20and%20Amodal-oriented%20Mix%20%28AoMix%29.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20on%20the%20BlendPASS%20dataset%2C%20reaching%20a%20remarkable%20mAPQ%20of%2026.58%25%20and%0AmIoU%20of%2043.66%25.%20On%20public%20panoramic%20semantic%20segmentation%20datasets%2C%20i.e.%2C%0ASynPASS%20and%20DensePASS%2C%20our%20method%20outperforms%20previous%20methods%20and%20obtains%0A45.34%25%20and%2048.08%25%20in%20mIoU%2C%20respectively.%20The%20fresh%20BlendPASS%20dataset%20and%20our%0Asource%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/yihong-97/OASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02182v1&entry.124074799=Read"},
{"title": "Enhancing Deep Neural Network Training Efficiency and Performance\n  through Linear Prediction", "author": "Hejie Ying and Mengmeng Song and Yaohong Tang and Shungen Xiao and Zimin Xiao", "abstract": "  Deep neural networks (DNN) have achieved remarkable success in various\nfields, including computer vision and natural language processing. However,\ntraining an effective DNN model still poses challenges. This paper aims to\npropose a method to optimize the training effectiveness of DNN, with the goal\nof improving model performance. Firstly, based on the observation that the DNN\nparameters change in certain laws during training process, the potential of\nparameter prediction for improving model training efficiency and performance is\ndiscovered. Secondly, considering the magnitude of DNN model parameters,\nhardware limitations and characteristics of Stochastic Gradient Descent (SGD)\nfor noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to\nperform DNN parameter prediction. Finally, validations are carried out on some\nrepresentative backbones. Experiment results show that compare to the normal\ntraining ways, under the same training conditions and epochs, by employing\nproposed PLP method, the optimal model is able to obtain average about 1%\naccuracy improvement and 0.01 top-1/top-5 error reduction for Vgg16, Resnet18\nand GoogLeNet based on CIFAR-100 dataset, which shown the effectiveness of the\nproposed method on different DNN structures, and validated its capacity in\nenhancing DNN training efficiency and performance.\n", "link": "http://arxiv.org/abs/2310.10958v2", "date": "2024-07-02", "relevancy": 2.2099, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6033}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5359}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Deep%20Neural%20Network%20Training%20Efficiency%20and%20Performance%0A%20%20through%20Linear%20Prediction&body=Title%3A%20Enhancing%20Deep%20Neural%20Network%20Training%20Efficiency%20and%20Performance%0A%20%20through%20Linear%20Prediction%0AAuthor%3A%20Hejie%20Ying%20and%20Mengmeng%20Song%20and%20Yaohong%20Tang%20and%20Shungen%20Xiao%20and%20Zimin%20Xiao%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNN%29%20have%20achieved%20remarkable%20success%20in%20various%0Afields%2C%20including%20computer%20vision%20and%20natural%20language%20processing.%20However%2C%0Atraining%20an%20effective%20DNN%20model%20still%20poses%20challenges.%20This%20paper%20aims%20to%0Apropose%20a%20method%20to%20optimize%20the%20training%20effectiveness%20of%20DNN%2C%20with%20the%20goal%0Aof%20improving%20model%20performance.%20Firstly%2C%20based%20on%20the%20observation%20that%20the%20DNN%0Aparameters%20change%20in%20certain%20laws%20during%20training%20process%2C%20the%20potential%20of%0Aparameter%20prediction%20for%20improving%20model%20training%20efficiency%20and%20performance%20is%0Adiscovered.%20Secondly%2C%20considering%20the%20magnitude%20of%20DNN%20model%20parameters%2C%0Ahardware%20limitations%20and%20characteristics%20of%20Stochastic%20Gradient%20Descent%20%28SGD%29%0Afor%20noise%20tolerance%2C%20a%20Parameter%20Linear%20Prediction%20%28PLP%29%20method%20is%20exploit%20to%0Aperform%20DNN%20parameter%20prediction.%20Finally%2C%20validations%20are%20carried%20out%20on%20some%0Arepresentative%20backbones.%20Experiment%20results%20show%20that%20compare%20to%20the%20normal%0Atraining%20ways%2C%20under%20the%20same%20training%20conditions%20and%20epochs%2C%20by%20employing%0Aproposed%20PLP%20method%2C%20the%20optimal%20model%20is%20able%20to%20obtain%20average%20about%201%25%0Aaccuracy%20improvement%20and%200.01%20top-1/top-5%20error%20reduction%20for%20Vgg16%2C%20Resnet18%0Aand%20GoogLeNet%20based%20on%20CIFAR-100%20dataset%2C%20which%20shown%20the%20effectiveness%20of%20the%0Aproposed%20method%20on%20different%20DNN%20structures%2C%20and%20validated%20its%20capacity%20in%0Aenhancing%20DNN%20training%20efficiency%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Deep%2520Neural%2520Network%2520Training%2520Efficiency%2520and%2520Performance%250A%2520%2520through%2520Linear%2520Prediction%26entry.906535625%3DHejie%2520Ying%2520and%2520Mengmeng%2520Song%2520and%2520Yaohong%2520Tang%2520and%2520Shungen%2520Xiao%2520and%2520Zimin%2520Xiao%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNN%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520various%250Afields%252C%2520including%2520computer%2520vision%2520and%2520natural%2520language%2520processing.%2520However%252C%250Atraining%2520an%2520effective%2520DNN%2520model%2520still%2520poses%2520challenges.%2520This%2520paper%2520aims%2520to%250Apropose%2520a%2520method%2520to%2520optimize%2520the%2520training%2520effectiveness%2520of%2520DNN%252C%2520with%2520the%2520goal%250Aof%2520improving%2520model%2520performance.%2520Firstly%252C%2520based%2520on%2520the%2520observation%2520that%2520the%2520DNN%250Aparameters%2520change%2520in%2520certain%2520laws%2520during%2520training%2520process%252C%2520the%2520potential%2520of%250Aparameter%2520prediction%2520for%2520improving%2520model%2520training%2520efficiency%2520and%2520performance%2520is%250Adiscovered.%2520Secondly%252C%2520considering%2520the%2520magnitude%2520of%2520DNN%2520model%2520parameters%252C%250Ahardware%2520limitations%2520and%2520characteristics%2520of%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%250Afor%2520noise%2520tolerance%252C%2520a%2520Parameter%2520Linear%2520Prediction%2520%2528PLP%2529%2520method%2520is%2520exploit%2520to%250Aperform%2520DNN%2520parameter%2520prediction.%2520Finally%252C%2520validations%2520are%2520carried%2520out%2520on%2520some%250Arepresentative%2520backbones.%2520Experiment%2520results%2520show%2520that%2520compare%2520to%2520the%2520normal%250Atraining%2520ways%252C%2520under%2520the%2520same%2520training%2520conditions%2520and%2520epochs%252C%2520by%2520employing%250Aproposed%2520PLP%2520method%252C%2520the%2520optimal%2520model%2520is%2520able%2520to%2520obtain%2520average%2520about%25201%2525%250Aaccuracy%2520improvement%2520and%25200.01%2520top-1/top-5%2520error%2520reduction%2520for%2520Vgg16%252C%2520Resnet18%250Aand%2520GoogLeNet%2520based%2520on%2520CIFAR-100%2520dataset%252C%2520which%2520shown%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%2520on%2520different%2520DNN%2520structures%252C%2520and%2520validated%2520its%2520capacity%2520in%250Aenhancing%2520DNN%2520training%2520efficiency%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Deep%20Neural%20Network%20Training%20Efficiency%20and%20Performance%0A%20%20through%20Linear%20Prediction&entry.906535625=Hejie%20Ying%20and%20Mengmeng%20Song%20and%20Yaohong%20Tang%20and%20Shungen%20Xiao%20and%20Zimin%20Xiao&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNN%29%20have%20achieved%20remarkable%20success%20in%20various%0Afields%2C%20including%20computer%20vision%20and%20natural%20language%20processing.%20However%2C%0Atraining%20an%20effective%20DNN%20model%20still%20poses%20challenges.%20This%20paper%20aims%20to%0Apropose%20a%20method%20to%20optimize%20the%20training%20effectiveness%20of%20DNN%2C%20with%20the%20goal%0Aof%20improving%20model%20performance.%20Firstly%2C%20based%20on%20the%20observation%20that%20the%20DNN%0Aparameters%20change%20in%20certain%20laws%20during%20training%20process%2C%20the%20potential%20of%0Aparameter%20prediction%20for%20improving%20model%20training%20efficiency%20and%20performance%20is%0Adiscovered.%20Secondly%2C%20considering%20the%20magnitude%20of%20DNN%20model%20parameters%2C%0Ahardware%20limitations%20and%20characteristics%20of%20Stochastic%20Gradient%20Descent%20%28SGD%29%0Afor%20noise%20tolerance%2C%20a%20Parameter%20Linear%20Prediction%20%28PLP%29%20method%20is%20exploit%20to%0Aperform%20DNN%20parameter%20prediction.%20Finally%2C%20validations%20are%20carried%20out%20on%20some%0Arepresentative%20backbones.%20Experiment%20results%20show%20that%20compare%20to%20the%20normal%0Atraining%20ways%2C%20under%20the%20same%20training%20conditions%20and%20epochs%2C%20by%20employing%0Aproposed%20PLP%20method%2C%20the%20optimal%20model%20is%20able%20to%20obtain%20average%20about%201%25%0Aaccuracy%20improvement%20and%200.01%20top-1/top-5%20error%20reduction%20for%20Vgg16%2C%20Resnet18%0Aand%20GoogLeNet%20based%20on%20CIFAR-100%20dataset%2C%20which%20shown%20the%20effectiveness%20of%20the%0Aproposed%20method%20on%20different%20DNN%20structures%2C%20and%20validated%20its%20capacity%20in%0Aenhancing%20DNN%20training%20efficiency%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10958v2&entry.124074799=Read"},
{"title": "xLSTM-UNet can be an Effective 2D & 3D Medical Image Segmentation\n  Backbone with Vision-LSTM (ViL) better than its Mamba Counterpart", "author": "Tianrun Chen and Chaotao Ding and Lanyun Zhu and Tao Xu and Deyi Ji and Yan Wang and Ying Zang and Zejian Li", "abstract": "  Convolutional Neural Networks (CNNs) and Vision Transformers (ViT) have been\npivotal in biomedical image segmentation, yet their ability to manage\nlong-range dependencies remains constrained by inherent locality and\ncomputational overhead. To overcome these challenges, in this technical report,\nwe first propose xLSTM-UNet, a UNet structured deep learning neural network\nthat leverages Vision-LSTM (xLSTM) as its backbone for medical image\nsegmentation. xLSTM is a recently proposed as the successor of Long Short-Term\nMemory (LSTM) networks and have demonstrated superior performance compared to\nTransformers and State Space Models (SSMs) like Mamba in Neural Language\nProcessing (NLP) and image classification (as demonstrated in Vision-LSTM, or\nViL implementation). Here, xLSTM-UNet we designed extend the success in\nbiomedical image segmentation domain. By integrating the local feature\nextraction strengths of convolutional layers with the long-range dependency\ncapturing abilities of xLSTM, xLSTM-UNet offers a robust solution for\ncomprehensive image analysis. We validate the efficacy of xLSTM-UNet through\nexperiments. Our findings demonstrate that xLSTM-UNet consistently surpasses\nthe performance of leading CNN-based, Transformer-based, and Mamba-based\nsegmentation networks in multiple datasets in biomedical segmentation including\norgans in abdomen MRI, instruments in endoscopic images, and cells in\nmicroscopic images. With comprehensive experiments performed, this technical\nreport highlights the potential of xLSTM-based architectures in advancing\nbiomedical image analysis in both 2D and 3D. The code, models, and datasets are\npublicly available at http://tianrun-chen.github.io/xLSTM-UNet/\n", "link": "http://arxiv.org/abs/2407.01530v2", "date": "2024-07-02", "relevancy": 2.1869, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xLSTM-UNet%20can%20be%20an%20Effective%202D%20%26%203D%20Medical%20Image%20Segmentation%0A%20%20Backbone%20with%20Vision-LSTM%20%28ViL%29%20better%20than%20its%20Mamba%20Counterpart&body=Title%3A%20xLSTM-UNet%20can%20be%20an%20Effective%202D%20%26%203D%20Medical%20Image%20Segmentation%0A%20%20Backbone%20with%20Vision-LSTM%20%28ViL%29%20better%20than%20its%20Mamba%20Counterpart%0AAuthor%3A%20Tianrun%20Chen%20and%20Chaotao%20Ding%20and%20Lanyun%20Zhu%20and%20Tao%20Xu%20and%20Deyi%20Ji%20and%20Yan%20Wang%20and%20Ying%20Zang%20and%20Zejian%20Li%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViT%29%20have%20been%0Apivotal%20in%20biomedical%20image%20segmentation%2C%20yet%20their%20ability%20to%20manage%0Along-range%20dependencies%20remains%20constrained%20by%20inherent%20locality%20and%0Acomputational%20overhead.%20To%20overcome%20these%20challenges%2C%20in%20this%20technical%20report%2C%0Awe%20first%20propose%20xLSTM-UNet%2C%20a%20UNet%20structured%20deep%20learning%20neural%20network%0Athat%20leverages%20Vision-LSTM%20%28xLSTM%29%20as%20its%20backbone%20for%20medical%20image%0Asegmentation.%20xLSTM%20is%20a%20recently%20proposed%20as%20the%20successor%20of%20Long%20Short-Term%0AMemory%20%28LSTM%29%20networks%20and%20have%20demonstrated%20superior%20performance%20compared%20to%0ATransformers%20and%20State%20Space%20Models%20%28SSMs%29%20like%20Mamba%20in%20Neural%20Language%0AProcessing%20%28NLP%29%20and%20image%20classification%20%28as%20demonstrated%20in%20Vision-LSTM%2C%20or%0AViL%20implementation%29.%20Here%2C%20xLSTM-UNet%20we%20designed%20extend%20the%20success%20in%0Abiomedical%20image%20segmentation%20domain.%20By%20integrating%20the%20local%20feature%0Aextraction%20strengths%20of%20convolutional%20layers%20with%20the%20long-range%20dependency%0Acapturing%20abilities%20of%20xLSTM%2C%20xLSTM-UNet%20offers%20a%20robust%20solution%20for%0Acomprehensive%20image%20analysis.%20We%20validate%20the%20efficacy%20of%20xLSTM-UNet%20through%0Aexperiments.%20Our%20findings%20demonstrate%20that%20xLSTM-UNet%20consistently%20surpasses%0Athe%20performance%20of%20leading%20CNN-based%2C%20Transformer-based%2C%20and%20Mamba-based%0Asegmentation%20networks%20in%20multiple%20datasets%20in%20biomedical%20segmentation%20including%0Aorgans%20in%20abdomen%20MRI%2C%20instruments%20in%20endoscopic%20images%2C%20and%20cells%20in%0Amicroscopic%20images.%20With%20comprehensive%20experiments%20performed%2C%20this%20technical%0Areport%20highlights%20the%20potential%20of%20xLSTM-based%20architectures%20in%20advancing%0Abiomedical%20image%20analysis%20in%20both%202D%20and%203D.%20The%20code%2C%20models%2C%20and%20datasets%20are%0Apublicly%20available%20at%20http%3A//tianrun-chen.github.io/xLSTM-UNet/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxLSTM-UNet%2520can%2520be%2520an%2520Effective%25202D%2520%2526%25203D%2520Medical%2520Image%2520Segmentation%250A%2520%2520Backbone%2520with%2520Vision-LSTM%2520%2528ViL%2529%2520better%2520than%2520its%2520Mamba%2520Counterpart%26entry.906535625%3DTianrun%2520Chen%2520and%2520Chaotao%2520Ding%2520and%2520Lanyun%2520Zhu%2520and%2520Tao%2520Xu%2520and%2520Deyi%2520Ji%2520and%2520Yan%2520Wang%2520and%2520Ying%2520Zang%2520and%2520Zejian%2520Li%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Vision%2520Transformers%2520%2528ViT%2529%2520have%2520been%250Apivotal%2520in%2520biomedical%2520image%2520segmentation%252C%2520yet%2520their%2520ability%2520to%2520manage%250Along-range%2520dependencies%2520remains%2520constrained%2520by%2520inherent%2520locality%2520and%250Acomputational%2520overhead.%2520To%2520overcome%2520these%2520challenges%252C%2520in%2520this%2520technical%2520report%252C%250Awe%2520first%2520propose%2520xLSTM-UNet%252C%2520a%2520UNet%2520structured%2520deep%2520learning%2520neural%2520network%250Athat%2520leverages%2520Vision-LSTM%2520%2528xLSTM%2529%2520as%2520its%2520backbone%2520for%2520medical%2520image%250Asegmentation.%2520xLSTM%2520is%2520a%2520recently%2520proposed%2520as%2520the%2520successor%2520of%2520Long%2520Short-Term%250AMemory%2520%2528LSTM%2529%2520networks%2520and%2520have%2520demonstrated%2520superior%2520performance%2520compared%2520to%250ATransformers%2520and%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520like%2520Mamba%2520in%2520Neural%2520Language%250AProcessing%2520%2528NLP%2529%2520and%2520image%2520classification%2520%2528as%2520demonstrated%2520in%2520Vision-LSTM%252C%2520or%250AViL%2520implementation%2529.%2520Here%252C%2520xLSTM-UNet%2520we%2520designed%2520extend%2520the%2520success%2520in%250Abiomedical%2520image%2520segmentation%2520domain.%2520By%2520integrating%2520the%2520local%2520feature%250Aextraction%2520strengths%2520of%2520convolutional%2520layers%2520with%2520the%2520long-range%2520dependency%250Acapturing%2520abilities%2520of%2520xLSTM%252C%2520xLSTM-UNet%2520offers%2520a%2520robust%2520solution%2520for%250Acomprehensive%2520image%2520analysis.%2520We%2520validate%2520the%2520efficacy%2520of%2520xLSTM-UNet%2520through%250Aexperiments.%2520Our%2520findings%2520demonstrate%2520that%2520xLSTM-UNet%2520consistently%2520surpasses%250Athe%2520performance%2520of%2520leading%2520CNN-based%252C%2520Transformer-based%252C%2520and%2520Mamba-based%250Asegmentation%2520networks%2520in%2520multiple%2520datasets%2520in%2520biomedical%2520segmentation%2520including%250Aorgans%2520in%2520abdomen%2520MRI%252C%2520instruments%2520in%2520endoscopic%2520images%252C%2520and%2520cells%2520in%250Amicroscopic%2520images.%2520With%2520comprehensive%2520experiments%2520performed%252C%2520this%2520technical%250Areport%2520highlights%2520the%2520potential%2520of%2520xLSTM-based%2520architectures%2520in%2520advancing%250Abiomedical%2520image%2520analysis%2520in%2520both%25202D%2520and%25203D.%2520The%2520code%252C%2520models%252C%2520and%2520datasets%2520are%250Apublicly%2520available%2520at%2520http%253A//tianrun-chen.github.io/xLSTM-UNet/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xLSTM-UNet%20can%20be%20an%20Effective%202D%20%26%203D%20Medical%20Image%20Segmentation%0A%20%20Backbone%20with%20Vision-LSTM%20%28ViL%29%20better%20than%20its%20Mamba%20Counterpart&entry.906535625=Tianrun%20Chen%20and%20Chaotao%20Ding%20and%20Lanyun%20Zhu%20and%20Tao%20Xu%20and%20Deyi%20Ji%20and%20Yan%20Wang%20and%20Ying%20Zang%20and%20Zejian%20Li&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViT%29%20have%20been%0Apivotal%20in%20biomedical%20image%20segmentation%2C%20yet%20their%20ability%20to%20manage%0Along-range%20dependencies%20remains%20constrained%20by%20inherent%20locality%20and%0Acomputational%20overhead.%20To%20overcome%20these%20challenges%2C%20in%20this%20technical%20report%2C%0Awe%20first%20propose%20xLSTM-UNet%2C%20a%20UNet%20structured%20deep%20learning%20neural%20network%0Athat%20leverages%20Vision-LSTM%20%28xLSTM%29%20as%20its%20backbone%20for%20medical%20image%0Asegmentation.%20xLSTM%20is%20a%20recently%20proposed%20as%20the%20successor%20of%20Long%20Short-Term%0AMemory%20%28LSTM%29%20networks%20and%20have%20demonstrated%20superior%20performance%20compared%20to%0ATransformers%20and%20State%20Space%20Models%20%28SSMs%29%20like%20Mamba%20in%20Neural%20Language%0AProcessing%20%28NLP%29%20and%20image%20classification%20%28as%20demonstrated%20in%20Vision-LSTM%2C%20or%0AViL%20implementation%29.%20Here%2C%20xLSTM-UNet%20we%20designed%20extend%20the%20success%20in%0Abiomedical%20image%20segmentation%20domain.%20By%20integrating%20the%20local%20feature%0Aextraction%20strengths%20of%20convolutional%20layers%20with%20the%20long-range%20dependency%0Acapturing%20abilities%20of%20xLSTM%2C%20xLSTM-UNet%20offers%20a%20robust%20solution%20for%0Acomprehensive%20image%20analysis.%20We%20validate%20the%20efficacy%20of%20xLSTM-UNet%20through%0Aexperiments.%20Our%20findings%20demonstrate%20that%20xLSTM-UNet%20consistently%20surpasses%0Athe%20performance%20of%20leading%20CNN-based%2C%20Transformer-based%2C%20and%20Mamba-based%0Asegmentation%20networks%20in%20multiple%20datasets%20in%20biomedical%20segmentation%20including%0Aorgans%20in%20abdomen%20MRI%2C%20instruments%20in%20endoscopic%20images%2C%20and%20cells%20in%0Amicroscopic%20images.%20With%20comprehensive%20experiments%20performed%2C%20this%20technical%0Areport%20highlights%20the%20potential%20of%20xLSTM-based%20architectures%20in%20advancing%0Abiomedical%20image%20analysis%20in%20both%202D%20and%203D.%20The%20code%2C%20models%2C%20and%20datasets%20are%0Apublicly%20available%20at%20http%3A//tianrun-chen.github.io/xLSTM-UNet/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01530v2&entry.124074799=Read"},
{"title": "A Refreshed Similarity-based Upsampler for Direct High-Ratio Feature\n  Upsampling", "author": "Minghao Zhou and Hong Wang and Yefeng Zheng and Deyu Meng", "abstract": "  Feature upsampling is a fundamental and indispensable ingredient of almost\nall current network structures for image segmentation tasks. Recently, a\npopular similarity-based feature upsampling pipeline has been proposed, which\nutilizes a high-resolution feature as guidance to help upsample the\nlow-resolution deep feature based on their local similarity. Albeit achieving\npromising performance, this pipeline has specific limitations: 1) HR query and\nLR key features are not well aligned; 2) the similarity between query-key\nfeatures is computed based on the fixed inner product form; 3) neighbor\nselection is coarsely operated on LR features, resulting in mosaic artifacts.\nThese shortcomings make the existing methods along this pipeline primarily\napplicable to hierarchical network architectures with iterative features as\nguidance and they are not readily extended to a broader range of structures,\nespecially for a direct high-ratio upsampling. Against the issues, we\nmeticulously optimize every methodological design. Specifically, we firstly\npropose an explicitly controllable query-key feature alignment from both\nsemantic-aware and detail-aware perspectives, and then construct a\nparameterized paired central difference convolution block for flexibly\ncalculating the similarity between the well-aligned query-key features.\nBesides, we develop a fine-grained neighbor selection strategy on HR features,\nwhich is simple yet effective for alleviating mosaic artifacts. Based on these\ncareful designs, we systematically construct a refreshed similarity-based\nfeature upsampling framework named ReSFU. Extensive experiments substantiate\nthat our proposed ReSFU is finely applicable to various types of architectures\nin a direct high-ratio upsampling manner, and consistently achieves\nsatisfactory performance on different segmentation applications, showing\nsuperior generality and ease of deployment.\n", "link": "http://arxiv.org/abs/2407.02283v1", "date": "2024-07-02", "relevancy": 2.1836, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5691}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5332}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Refreshed%20Similarity-based%20Upsampler%20for%20Direct%20High-Ratio%20Feature%0A%20%20Upsampling&body=Title%3A%20A%20Refreshed%20Similarity-based%20Upsampler%20for%20Direct%20High-Ratio%20Feature%0A%20%20Upsampling%0AAuthor%3A%20Minghao%20Zhou%20and%20Hong%20Wang%20and%20Yefeng%20Zheng%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Feature%20upsampling%20is%20a%20fundamental%20and%20indispensable%20ingredient%20of%20almost%0Aall%20current%20network%20structures%20for%20image%20segmentation%20tasks.%20Recently%2C%20a%0Apopular%20similarity-based%20feature%20upsampling%20pipeline%20has%20been%20proposed%2C%20which%0Autilizes%20a%20high-resolution%20feature%20as%20guidance%20to%20help%20upsample%20the%0Alow-resolution%20deep%20feature%20based%20on%20their%20local%20similarity.%20Albeit%20achieving%0Apromising%20performance%2C%20this%20pipeline%20has%20specific%20limitations%3A%201%29%20HR%20query%20and%0ALR%20key%20features%20are%20not%20well%20aligned%3B%202%29%20the%20similarity%20between%20query-key%0Afeatures%20is%20computed%20based%20on%20the%20fixed%20inner%20product%20form%3B%203%29%20neighbor%0Aselection%20is%20coarsely%20operated%20on%20LR%20features%2C%20resulting%20in%20mosaic%20artifacts.%0AThese%20shortcomings%20make%20the%20existing%20methods%20along%20this%20pipeline%20primarily%0Aapplicable%20to%20hierarchical%20network%20architectures%20with%20iterative%20features%20as%0Aguidance%20and%20they%20are%20not%20readily%20extended%20to%20a%20broader%20range%20of%20structures%2C%0Aespecially%20for%20a%20direct%20high-ratio%20upsampling.%20Against%20the%20issues%2C%20we%0Ameticulously%20optimize%20every%20methodological%20design.%20Specifically%2C%20we%20firstly%0Apropose%20an%20explicitly%20controllable%20query-key%20feature%20alignment%20from%20both%0Asemantic-aware%20and%20detail-aware%20perspectives%2C%20and%20then%20construct%20a%0Aparameterized%20paired%20central%20difference%20convolution%20block%20for%20flexibly%0Acalculating%20the%20similarity%20between%20the%20well-aligned%20query-key%20features.%0ABesides%2C%20we%20develop%20a%20fine-grained%20neighbor%20selection%20strategy%20on%20HR%20features%2C%0Awhich%20is%20simple%20yet%20effective%20for%20alleviating%20mosaic%20artifacts.%20Based%20on%20these%0Acareful%20designs%2C%20we%20systematically%20construct%20a%20refreshed%20similarity-based%0Afeature%20upsampling%20framework%20named%20ReSFU.%20Extensive%20experiments%20substantiate%0Athat%20our%20proposed%20ReSFU%20is%20finely%20applicable%20to%20various%20types%20of%20architectures%0Ain%20a%20direct%20high-ratio%20upsampling%20manner%2C%20and%20consistently%20achieves%0Asatisfactory%20performance%20on%20different%20segmentation%20applications%2C%20showing%0Asuperior%20generality%20and%20ease%20of%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Refreshed%2520Similarity-based%2520Upsampler%2520for%2520Direct%2520High-Ratio%2520Feature%250A%2520%2520Upsampling%26entry.906535625%3DMinghao%2520Zhou%2520and%2520Hong%2520Wang%2520and%2520Yefeng%2520Zheng%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Feature%2520upsampling%2520is%2520a%2520fundamental%2520and%2520indispensable%2520ingredient%2520of%2520almost%250Aall%2520current%2520network%2520structures%2520for%2520image%2520segmentation%2520tasks.%2520Recently%252C%2520a%250Apopular%2520similarity-based%2520feature%2520upsampling%2520pipeline%2520has%2520been%2520proposed%252C%2520which%250Autilizes%2520a%2520high-resolution%2520feature%2520as%2520guidance%2520to%2520help%2520upsample%2520the%250Alow-resolution%2520deep%2520feature%2520based%2520on%2520their%2520local%2520similarity.%2520Albeit%2520achieving%250Apromising%2520performance%252C%2520this%2520pipeline%2520has%2520specific%2520limitations%253A%25201%2529%2520HR%2520query%2520and%250ALR%2520key%2520features%2520are%2520not%2520well%2520aligned%253B%25202%2529%2520the%2520similarity%2520between%2520query-key%250Afeatures%2520is%2520computed%2520based%2520on%2520the%2520fixed%2520inner%2520product%2520form%253B%25203%2529%2520neighbor%250Aselection%2520is%2520coarsely%2520operated%2520on%2520LR%2520features%252C%2520resulting%2520in%2520mosaic%2520artifacts.%250AThese%2520shortcomings%2520make%2520the%2520existing%2520methods%2520along%2520this%2520pipeline%2520primarily%250Aapplicable%2520to%2520hierarchical%2520network%2520architectures%2520with%2520iterative%2520features%2520as%250Aguidance%2520and%2520they%2520are%2520not%2520readily%2520extended%2520to%2520a%2520broader%2520range%2520of%2520structures%252C%250Aespecially%2520for%2520a%2520direct%2520high-ratio%2520upsampling.%2520Against%2520the%2520issues%252C%2520we%250Ameticulously%2520optimize%2520every%2520methodological%2520design.%2520Specifically%252C%2520we%2520firstly%250Apropose%2520an%2520explicitly%2520controllable%2520query-key%2520feature%2520alignment%2520from%2520both%250Asemantic-aware%2520and%2520detail-aware%2520perspectives%252C%2520and%2520then%2520construct%2520a%250Aparameterized%2520paired%2520central%2520difference%2520convolution%2520block%2520for%2520flexibly%250Acalculating%2520the%2520similarity%2520between%2520the%2520well-aligned%2520query-key%2520features.%250ABesides%252C%2520we%2520develop%2520a%2520fine-grained%2520neighbor%2520selection%2520strategy%2520on%2520HR%2520features%252C%250Awhich%2520is%2520simple%2520yet%2520effective%2520for%2520alleviating%2520mosaic%2520artifacts.%2520Based%2520on%2520these%250Acareful%2520designs%252C%2520we%2520systematically%2520construct%2520a%2520refreshed%2520similarity-based%250Afeature%2520upsampling%2520framework%2520named%2520ReSFU.%2520Extensive%2520experiments%2520substantiate%250Athat%2520our%2520proposed%2520ReSFU%2520is%2520finely%2520applicable%2520to%2520various%2520types%2520of%2520architectures%250Ain%2520a%2520direct%2520high-ratio%2520upsampling%2520manner%252C%2520and%2520consistently%2520achieves%250Asatisfactory%2520performance%2520on%2520different%2520segmentation%2520applications%252C%2520showing%250Asuperior%2520generality%2520and%2520ease%2520of%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Refreshed%20Similarity-based%20Upsampler%20for%20Direct%20High-Ratio%20Feature%0A%20%20Upsampling&entry.906535625=Minghao%20Zhou%20and%20Hong%20Wang%20and%20Yefeng%20Zheng%20and%20Deyu%20Meng&entry.1292438233=%20%20Feature%20upsampling%20is%20a%20fundamental%20and%20indispensable%20ingredient%20of%20almost%0Aall%20current%20network%20structures%20for%20image%20segmentation%20tasks.%20Recently%2C%20a%0Apopular%20similarity-based%20feature%20upsampling%20pipeline%20has%20been%20proposed%2C%20which%0Autilizes%20a%20high-resolution%20feature%20as%20guidance%20to%20help%20upsample%20the%0Alow-resolution%20deep%20feature%20based%20on%20their%20local%20similarity.%20Albeit%20achieving%0Apromising%20performance%2C%20this%20pipeline%20has%20specific%20limitations%3A%201%29%20HR%20query%20and%0ALR%20key%20features%20are%20not%20well%20aligned%3B%202%29%20the%20similarity%20between%20query-key%0Afeatures%20is%20computed%20based%20on%20the%20fixed%20inner%20product%20form%3B%203%29%20neighbor%0Aselection%20is%20coarsely%20operated%20on%20LR%20features%2C%20resulting%20in%20mosaic%20artifacts.%0AThese%20shortcomings%20make%20the%20existing%20methods%20along%20this%20pipeline%20primarily%0Aapplicable%20to%20hierarchical%20network%20architectures%20with%20iterative%20features%20as%0Aguidance%20and%20they%20are%20not%20readily%20extended%20to%20a%20broader%20range%20of%20structures%2C%0Aespecially%20for%20a%20direct%20high-ratio%20upsampling.%20Against%20the%20issues%2C%20we%0Ameticulously%20optimize%20every%20methodological%20design.%20Specifically%2C%20we%20firstly%0Apropose%20an%20explicitly%20controllable%20query-key%20feature%20alignment%20from%20both%0Asemantic-aware%20and%20detail-aware%20perspectives%2C%20and%20then%20construct%20a%0Aparameterized%20paired%20central%20difference%20convolution%20block%20for%20flexibly%0Acalculating%20the%20similarity%20between%20the%20well-aligned%20query-key%20features.%0ABesides%2C%20we%20develop%20a%20fine-grained%20neighbor%20selection%20strategy%20on%20HR%20features%2C%0Awhich%20is%20simple%20yet%20effective%20for%20alleviating%20mosaic%20artifacts.%20Based%20on%20these%0Acareful%20designs%2C%20we%20systematically%20construct%20a%20refreshed%20similarity-based%0Afeature%20upsampling%20framework%20named%20ReSFU.%20Extensive%20experiments%20substantiate%0Athat%20our%20proposed%20ReSFU%20is%20finely%20applicable%20to%20various%20types%20of%20architectures%0Ain%20a%20direct%20high-ratio%20upsampling%20manner%2C%20and%20consistently%20achieves%0Asatisfactory%20performance%20on%20different%20segmentation%20applications%2C%20showing%0Asuperior%20generality%20and%20ease%20of%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02283v1&entry.124074799=Read"},
{"title": "On the Robustness of Graph Reduction Against GNN Backdoor", "author": "Yuxuan Zhu and Michael Mandulak and Kerui Wu and George Slota and Yuseok Jeon and Ka-Ho Chow and Lei Yu", "abstract": "  Graph Neural Networks (GNNs) are gaining popularity across various domains\ndue to their effectiveness in learning graph-structured data. Nevertheless,\nthey have been shown to be susceptible to backdoor poisoning attacks, which\npose serious threats to real-world applications. Meanwhile, graph reduction\ntechniques, including coarsening and sparsification, which have long been\nemployed to improve the scalability of large graph computational tasks, have\nrecently emerged as effective methods for accelerating GNN training on\nlarge-scale graphs. However, the current development and deployment of graph\nreduction techniques for large graphs overlook the potential risks of data\npoisoning attacks against GNNs. It is not yet clear how graph reduction\ninteracts with existing backdoor attacks. This paper conducts a thorough\nexamination of the robustness of graph reduction methods in scalable GNN\ntraining in the presence of state-of-the-art backdoor attacks. We performed a\ncomprehensive robustness analysis across six coarsening methods and six\nsparsification methods for graph reduction, under three GNN backdoor attacks\nagainst three GNN architectures. Our findings indicate that the effectiveness\nof graph reduction methods in mitigating attack success rates varies\nsignificantly, with some methods even exacerbating the attacks. Through\ndetailed analyses of triggers and poisoned nodes, we interpret our findings and\nenhance our understanding of how graph reduction interacts with backdoor\nattacks. These results highlight the critical need for incorporating robustness\nconsiderations in graph reduction for GNN training, ensuring that enhancements\nin computational efficiency do not compromise the security of GNN systems.\n", "link": "http://arxiv.org/abs/2407.02431v1", "date": "2024-07-02", "relevancy": 2.1811, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4572}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.436}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Graph%20Reduction%20Against%20GNN%20Backdoor&body=Title%3A%20On%20the%20Robustness%20of%20Graph%20Reduction%20Against%20GNN%20Backdoor%0AAuthor%3A%20Yuxuan%20Zhu%20and%20Michael%20Mandulak%20and%20Kerui%20Wu%20and%20George%20Slota%20and%20Yuseok%20Jeon%20and%20Ka-Ho%20Chow%20and%20Lei%20Yu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20gaining%20popularity%20across%20various%20domains%0Adue%20to%20their%20effectiveness%20in%20learning%20graph-structured%20data.%20Nevertheless%2C%0Athey%20have%20been%20shown%20to%20be%20susceptible%20to%20backdoor%20poisoning%20attacks%2C%20which%0Apose%20serious%20threats%20to%20real-world%20applications.%20Meanwhile%2C%20graph%20reduction%0Atechniques%2C%20including%20coarsening%20and%20sparsification%2C%20which%20have%20long%20been%0Aemployed%20to%20improve%20the%20scalability%20of%20large%20graph%20computational%20tasks%2C%20have%0Arecently%20emerged%20as%20effective%20methods%20for%20accelerating%20GNN%20training%20on%0Alarge-scale%20graphs.%20However%2C%20the%20current%20development%20and%20deployment%20of%20graph%0Areduction%20techniques%20for%20large%20graphs%20overlook%20the%20potential%20risks%20of%20data%0Apoisoning%20attacks%20against%20GNNs.%20It%20is%20not%20yet%20clear%20how%20graph%20reduction%0Ainteracts%20with%20existing%20backdoor%20attacks.%20This%20paper%20conducts%20a%20thorough%0Aexamination%20of%20the%20robustness%20of%20graph%20reduction%20methods%20in%20scalable%20GNN%0Atraining%20in%20the%20presence%20of%20state-of-the-art%20backdoor%20attacks.%20We%20performed%20a%0Acomprehensive%20robustness%20analysis%20across%20six%20coarsening%20methods%20and%20six%0Asparsification%20methods%20for%20graph%20reduction%2C%20under%20three%20GNN%20backdoor%20attacks%0Aagainst%20three%20GNN%20architectures.%20Our%20findings%20indicate%20that%20the%20effectiveness%0Aof%20graph%20reduction%20methods%20in%20mitigating%20attack%20success%20rates%20varies%0Asignificantly%2C%20with%20some%20methods%20even%20exacerbating%20the%20attacks.%20Through%0Adetailed%20analyses%20of%20triggers%20and%20poisoned%20nodes%2C%20we%20interpret%20our%20findings%20and%0Aenhance%20our%20understanding%20of%20how%20graph%20reduction%20interacts%20with%20backdoor%0Aattacks.%20These%20results%20highlight%20the%20critical%20need%20for%20incorporating%20robustness%0Aconsiderations%20in%20graph%20reduction%20for%20GNN%20training%2C%20ensuring%20that%20enhancements%0Ain%20computational%20efficiency%20do%20not%20compromise%20the%20security%20of%20GNN%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520of%2520Graph%2520Reduction%2520Against%2520GNN%2520Backdoor%26entry.906535625%3DYuxuan%2520Zhu%2520and%2520Michael%2520Mandulak%2520and%2520Kerui%2520Wu%2520and%2520George%2520Slota%2520and%2520Yuseok%2520Jeon%2520and%2520Ka-Ho%2520Chow%2520and%2520Lei%2520Yu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520gaining%2520popularity%2520across%2520various%2520domains%250Adue%2520to%2520their%2520effectiveness%2520in%2520learning%2520graph-structured%2520data.%2520Nevertheless%252C%250Athey%2520have%2520been%2520shown%2520to%2520be%2520susceptible%2520to%2520backdoor%2520poisoning%2520attacks%252C%2520which%250Apose%2520serious%2520threats%2520to%2520real-world%2520applications.%2520Meanwhile%252C%2520graph%2520reduction%250Atechniques%252C%2520including%2520coarsening%2520and%2520sparsification%252C%2520which%2520have%2520long%2520been%250Aemployed%2520to%2520improve%2520the%2520scalability%2520of%2520large%2520graph%2520computational%2520tasks%252C%2520have%250Arecently%2520emerged%2520as%2520effective%2520methods%2520for%2520accelerating%2520GNN%2520training%2520on%250Alarge-scale%2520graphs.%2520However%252C%2520the%2520current%2520development%2520and%2520deployment%2520of%2520graph%250Areduction%2520techniques%2520for%2520large%2520graphs%2520overlook%2520the%2520potential%2520risks%2520of%2520data%250Apoisoning%2520attacks%2520against%2520GNNs.%2520It%2520is%2520not%2520yet%2520clear%2520how%2520graph%2520reduction%250Ainteracts%2520with%2520existing%2520backdoor%2520attacks.%2520This%2520paper%2520conducts%2520a%2520thorough%250Aexamination%2520of%2520the%2520robustness%2520of%2520graph%2520reduction%2520methods%2520in%2520scalable%2520GNN%250Atraining%2520in%2520the%2520presence%2520of%2520state-of-the-art%2520backdoor%2520attacks.%2520We%2520performed%2520a%250Acomprehensive%2520robustness%2520analysis%2520across%2520six%2520coarsening%2520methods%2520and%2520six%250Asparsification%2520methods%2520for%2520graph%2520reduction%252C%2520under%2520three%2520GNN%2520backdoor%2520attacks%250Aagainst%2520three%2520GNN%2520architectures.%2520Our%2520findings%2520indicate%2520that%2520the%2520effectiveness%250Aof%2520graph%2520reduction%2520methods%2520in%2520mitigating%2520attack%2520success%2520rates%2520varies%250Asignificantly%252C%2520with%2520some%2520methods%2520even%2520exacerbating%2520the%2520attacks.%2520Through%250Adetailed%2520analyses%2520of%2520triggers%2520and%2520poisoned%2520nodes%252C%2520we%2520interpret%2520our%2520findings%2520and%250Aenhance%2520our%2520understanding%2520of%2520how%2520graph%2520reduction%2520interacts%2520with%2520backdoor%250Aattacks.%2520These%2520results%2520highlight%2520the%2520critical%2520need%2520for%2520incorporating%2520robustness%250Aconsiderations%2520in%2520graph%2520reduction%2520for%2520GNN%2520training%252C%2520ensuring%2520that%2520enhancements%250Ain%2520computational%2520efficiency%2520do%2520not%2520compromise%2520the%2520security%2520of%2520GNN%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Graph%20Reduction%20Against%20GNN%20Backdoor&entry.906535625=Yuxuan%20Zhu%20and%20Michael%20Mandulak%20and%20Kerui%20Wu%20and%20George%20Slota%20and%20Yuseok%20Jeon%20and%20Ka-Ho%20Chow%20and%20Lei%20Yu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20gaining%20popularity%20across%20various%20domains%0Adue%20to%20their%20effectiveness%20in%20learning%20graph-structured%20data.%20Nevertheless%2C%0Athey%20have%20been%20shown%20to%20be%20susceptible%20to%20backdoor%20poisoning%20attacks%2C%20which%0Apose%20serious%20threats%20to%20real-world%20applications.%20Meanwhile%2C%20graph%20reduction%0Atechniques%2C%20including%20coarsening%20and%20sparsification%2C%20which%20have%20long%20been%0Aemployed%20to%20improve%20the%20scalability%20of%20large%20graph%20computational%20tasks%2C%20have%0Arecently%20emerged%20as%20effective%20methods%20for%20accelerating%20GNN%20training%20on%0Alarge-scale%20graphs.%20However%2C%20the%20current%20development%20and%20deployment%20of%20graph%0Areduction%20techniques%20for%20large%20graphs%20overlook%20the%20potential%20risks%20of%20data%0Apoisoning%20attacks%20against%20GNNs.%20It%20is%20not%20yet%20clear%20how%20graph%20reduction%0Ainteracts%20with%20existing%20backdoor%20attacks.%20This%20paper%20conducts%20a%20thorough%0Aexamination%20of%20the%20robustness%20of%20graph%20reduction%20methods%20in%20scalable%20GNN%0Atraining%20in%20the%20presence%20of%20state-of-the-art%20backdoor%20attacks.%20We%20performed%20a%0Acomprehensive%20robustness%20analysis%20across%20six%20coarsening%20methods%20and%20six%0Asparsification%20methods%20for%20graph%20reduction%2C%20under%20three%20GNN%20backdoor%20attacks%0Aagainst%20three%20GNN%20architectures.%20Our%20findings%20indicate%20that%20the%20effectiveness%0Aof%20graph%20reduction%20methods%20in%20mitigating%20attack%20success%20rates%20varies%0Asignificantly%2C%20with%20some%20methods%20even%20exacerbating%20the%20attacks.%20Through%0Adetailed%20analyses%20of%20triggers%20and%20poisoned%20nodes%2C%20we%20interpret%20our%20findings%20and%0Aenhance%20our%20understanding%20of%20how%20graph%20reduction%20interacts%20with%20backdoor%0Aattacks.%20These%20results%20highlight%20the%20critical%20need%20for%20incorporating%20robustness%0Aconsiderations%20in%20graph%20reduction%20for%20GNN%20training%2C%20ensuring%20that%20enhancements%0Ain%20computational%20efficiency%20do%20not%20compromise%20the%20security%20of%20GNN%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02431v1&entry.124074799=Read"},
{"title": "CALICO: Confident Active Learning with Integrated Calibration", "author": "Lorenzo S. Querol and Hajime Nagahara and Hideaki Hayashi", "abstract": "  The growing use of deep learning in safety-critical applications, such as\nmedical imaging, has raised concerns about limited labeled data, where this\ndemand is amplified as model complexity increases, posing hurdles for domain\nexperts to annotate data. In response to this, active learning (AL) is used to\nefficiently train models with limited annotation costs. In the context of deep\nneural networks (DNNs), AL often uses confidence or probability outputs as a\nscore for selecting the most informative samples. However, modern DNNs exhibit\nunreliable confidence outputs, making calibration essential. We propose an AL\nframework that self-calibrates the confidence used for sample selection during\nthe training process, referred to as Confident Active Learning with Integrated\nCalibratiOn (CALICO). CALICO incorporates the joint training of a classifier\nand an energy-based model, instead of the standard softmax-based classifier.\nThis approach allows for simultaneous estimation of the input data distribution\nand the class probabilities during training, improving calibration without\nneeding an additional labeled dataset. Experimental results showcase improved\nclassification performance compared to a softmax-based classifier with fewer\nlabeled samples. Furthermore, the calibration stability of the model is\nobserved to depend on the prior class distribution of the data.\n", "link": "http://arxiv.org/abs/2407.02335v1", "date": "2024-07-02", "relevancy": 2.1806, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5837}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5562}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CALICO%3A%20Confident%20Active%20Learning%20with%20Integrated%20Calibration&body=Title%3A%20CALICO%3A%20Confident%20Active%20Learning%20with%20Integrated%20Calibration%0AAuthor%3A%20Lorenzo%20S.%20Querol%20and%20Hajime%20Nagahara%20and%20Hideaki%20Hayashi%0AAbstract%3A%20%20%20The%20growing%20use%20of%20deep%20learning%20in%20safety-critical%20applications%2C%20such%20as%0Amedical%20imaging%2C%20has%20raised%20concerns%20about%20limited%20labeled%20data%2C%20where%20this%0Ademand%20is%20amplified%20as%20model%20complexity%20increases%2C%20posing%20hurdles%20for%20domain%0Aexperts%20to%20annotate%20data.%20In%20response%20to%20this%2C%20active%20learning%20%28AL%29%20is%20used%20to%0Aefficiently%20train%20models%20with%20limited%20annotation%20costs.%20In%20the%20context%20of%20deep%0Aneural%20networks%20%28DNNs%29%2C%20AL%20often%20uses%20confidence%20or%20probability%20outputs%20as%20a%0Ascore%20for%20selecting%20the%20most%20informative%20samples.%20However%2C%20modern%20DNNs%20exhibit%0Aunreliable%20confidence%20outputs%2C%20making%20calibration%20essential.%20We%20propose%20an%20AL%0Aframework%20that%20self-calibrates%20the%20confidence%20used%20for%20sample%20selection%20during%0Athe%20training%20process%2C%20referred%20to%20as%20Confident%20Active%20Learning%20with%20Integrated%0ACalibratiOn%20%28CALICO%29.%20CALICO%20incorporates%20the%20joint%20training%20of%20a%20classifier%0Aand%20an%20energy-based%20model%2C%20instead%20of%20the%20standard%20softmax-based%20classifier.%0AThis%20approach%20allows%20for%20simultaneous%20estimation%20of%20the%20input%20data%20distribution%0Aand%20the%20class%20probabilities%20during%20training%2C%20improving%20calibration%20without%0Aneeding%20an%20additional%20labeled%20dataset.%20Experimental%20results%20showcase%20improved%0Aclassification%20performance%20compared%20to%20a%20softmax-based%20classifier%20with%20fewer%0Alabeled%20samples.%20Furthermore%2C%20the%20calibration%20stability%20of%20the%20model%20is%0Aobserved%20to%20depend%20on%20the%20prior%20class%20distribution%20of%20the%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCALICO%253A%2520Confident%2520Active%2520Learning%2520with%2520Integrated%2520Calibration%26entry.906535625%3DLorenzo%2520S.%2520Querol%2520and%2520Hajime%2520Nagahara%2520and%2520Hideaki%2520Hayashi%26entry.1292438233%3D%2520%2520The%2520growing%2520use%2520of%2520deep%2520learning%2520in%2520safety-critical%2520applications%252C%2520such%2520as%250Amedical%2520imaging%252C%2520has%2520raised%2520concerns%2520about%2520limited%2520labeled%2520data%252C%2520where%2520this%250Ademand%2520is%2520amplified%2520as%2520model%2520complexity%2520increases%252C%2520posing%2520hurdles%2520for%2520domain%250Aexperts%2520to%2520annotate%2520data.%2520In%2520response%2520to%2520this%252C%2520active%2520learning%2520%2528AL%2529%2520is%2520used%2520to%250Aefficiently%2520train%2520models%2520with%2520limited%2520annotation%2520costs.%2520In%2520the%2520context%2520of%2520deep%250Aneural%2520networks%2520%2528DNNs%2529%252C%2520AL%2520often%2520uses%2520confidence%2520or%2520probability%2520outputs%2520as%2520a%250Ascore%2520for%2520selecting%2520the%2520most%2520informative%2520samples.%2520However%252C%2520modern%2520DNNs%2520exhibit%250Aunreliable%2520confidence%2520outputs%252C%2520making%2520calibration%2520essential.%2520We%2520propose%2520an%2520AL%250Aframework%2520that%2520self-calibrates%2520the%2520confidence%2520used%2520for%2520sample%2520selection%2520during%250Athe%2520training%2520process%252C%2520referred%2520to%2520as%2520Confident%2520Active%2520Learning%2520with%2520Integrated%250ACalibratiOn%2520%2528CALICO%2529.%2520CALICO%2520incorporates%2520the%2520joint%2520training%2520of%2520a%2520classifier%250Aand%2520an%2520energy-based%2520model%252C%2520instead%2520of%2520the%2520standard%2520softmax-based%2520classifier.%250AThis%2520approach%2520allows%2520for%2520simultaneous%2520estimation%2520of%2520the%2520input%2520data%2520distribution%250Aand%2520the%2520class%2520probabilities%2520during%2520training%252C%2520improving%2520calibration%2520without%250Aneeding%2520an%2520additional%2520labeled%2520dataset.%2520Experimental%2520results%2520showcase%2520improved%250Aclassification%2520performance%2520compared%2520to%2520a%2520softmax-based%2520classifier%2520with%2520fewer%250Alabeled%2520samples.%2520Furthermore%252C%2520the%2520calibration%2520stability%2520of%2520the%2520model%2520is%250Aobserved%2520to%2520depend%2520on%2520the%2520prior%2520class%2520distribution%2520of%2520the%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CALICO%3A%20Confident%20Active%20Learning%20with%20Integrated%20Calibration&entry.906535625=Lorenzo%20S.%20Querol%20and%20Hajime%20Nagahara%20and%20Hideaki%20Hayashi&entry.1292438233=%20%20The%20growing%20use%20of%20deep%20learning%20in%20safety-critical%20applications%2C%20such%20as%0Amedical%20imaging%2C%20has%20raised%20concerns%20about%20limited%20labeled%20data%2C%20where%20this%0Ademand%20is%20amplified%20as%20model%20complexity%20increases%2C%20posing%20hurdles%20for%20domain%0Aexperts%20to%20annotate%20data.%20In%20response%20to%20this%2C%20active%20learning%20%28AL%29%20is%20used%20to%0Aefficiently%20train%20models%20with%20limited%20annotation%20costs.%20In%20the%20context%20of%20deep%0Aneural%20networks%20%28DNNs%29%2C%20AL%20often%20uses%20confidence%20or%20probability%20outputs%20as%20a%0Ascore%20for%20selecting%20the%20most%20informative%20samples.%20However%2C%20modern%20DNNs%20exhibit%0Aunreliable%20confidence%20outputs%2C%20making%20calibration%20essential.%20We%20propose%20an%20AL%0Aframework%20that%20self-calibrates%20the%20confidence%20used%20for%20sample%20selection%20during%0Athe%20training%20process%2C%20referred%20to%20as%20Confident%20Active%20Learning%20with%20Integrated%0ACalibratiOn%20%28CALICO%29.%20CALICO%20incorporates%20the%20joint%20training%20of%20a%20classifier%0Aand%20an%20energy-based%20model%2C%20instead%20of%20the%20standard%20softmax-based%20classifier.%0AThis%20approach%20allows%20for%20simultaneous%20estimation%20of%20the%20input%20data%20distribution%0Aand%20the%20class%20probabilities%20during%20training%2C%20improving%20calibration%20without%0Aneeding%20an%20additional%20labeled%20dataset.%20Experimental%20results%20showcase%20improved%0Aclassification%20performance%20compared%20to%20a%20softmax-based%20classifier%20with%20fewer%0Alabeled%20samples.%20Furthermore%2C%20the%20calibration%20stability%20of%20the%20model%20is%0Aobserved%20to%20depend%20on%20the%20prior%20class%20distribution%20of%20the%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02335v1&entry.124074799=Read"},
{"title": "Characteristic Learning for Provable One Step Generation", "author": "Zhao Ding and Chenguang Duan and Yuling Jiao and Ruoxuan Li and Jerry Zhijian Yang and Pingwen Zhang", "abstract": "  We propose the characteristic generator, a novel one-step generative model\nthat combines the efficiency of sampling in Generative Adversarial Networks\n(GANs) with the stable performance of flow-based models. Our model is driven by\ncharacteristics, along which the probability density transport can be described\nby ordinary differential equations (ODEs). Specifically, We estimate the\nvelocity field through nonparametric regression and utilize Euler method to\nsolve the probability flow ODE, generating a series of discrete approximations\nto the characteristics. We then use a deep neural network to fit these\ncharacteristics, ensuring a one-step mapping that effectively pushes the prior\ndistribution towards the target distribution. In the theoretical aspect, we\nanalyze the errors in velocity matching, Euler discretization, and\ncharacteristic fitting to establish a non-asymptotic convergence rate for the\ncharacteristic generator in 2-Wasserstein distance. To the best of our\nknowledge, this is the first thorough analysis for simulation-free one step\ngenerative models. Additionally, our analysis refines the error analysis of\nflow-based generative models in prior works. We apply our method on both\nsynthetic and real datasets, and the results demonstrate that the\ncharacteristic generator achieves high generation quality with just a single\nevaluation of neural network.\n", "link": "http://arxiv.org/abs/2405.05512v3", "date": "2024-07-02", "relevancy": 2.1518, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5713}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characteristic%20Learning%20for%20Provable%20One%20Step%20Generation&body=Title%3A%20Characteristic%20Learning%20for%20Provable%20One%20Step%20Generation%0AAuthor%3A%20Zhao%20Ding%20and%20Chenguang%20Duan%20and%20Yuling%20Jiao%20and%20Ruoxuan%20Li%20and%20Jerry%20Zhijian%20Yang%20and%20Pingwen%20Zhang%0AAbstract%3A%20%20%20We%20propose%20the%20characteristic%20generator%2C%20a%20novel%20one-step%20generative%20model%0Athat%20combines%20the%20efficiency%20of%20sampling%20in%20Generative%20Adversarial%20Networks%0A%28GANs%29%20with%20the%20stable%20performance%20of%20flow-based%20models.%20Our%20model%20is%20driven%20by%0Acharacteristics%2C%20along%20which%20the%20probability%20density%20transport%20can%20be%20described%0Aby%20ordinary%20differential%20equations%20%28ODEs%29.%20Specifically%2C%20We%20estimate%20the%0Avelocity%20field%20through%20nonparametric%20regression%20and%20utilize%20Euler%20method%20to%0Asolve%20the%20probability%20flow%20ODE%2C%20generating%20a%20series%20of%20discrete%20approximations%0Ato%20the%20characteristics.%20We%20then%20use%20a%20deep%20neural%20network%20to%20fit%20these%0Acharacteristics%2C%20ensuring%20a%20one-step%20mapping%20that%20effectively%20pushes%20the%20prior%0Adistribution%20towards%20the%20target%20distribution.%20In%20the%20theoretical%20aspect%2C%20we%0Aanalyze%20the%20errors%20in%20velocity%20matching%2C%20Euler%20discretization%2C%20and%0Acharacteristic%20fitting%20to%20establish%20a%20non-asymptotic%20convergence%20rate%20for%20the%0Acharacteristic%20generator%20in%202-Wasserstein%20distance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20thorough%20analysis%20for%20simulation-free%20one%20step%0Agenerative%20models.%20Additionally%2C%20our%20analysis%20refines%20the%20error%20analysis%20of%0Aflow-based%20generative%20models%20in%20prior%20works.%20We%20apply%20our%20method%20on%20both%0Asynthetic%20and%20real%20datasets%2C%20and%20the%20results%20demonstrate%20that%20the%0Acharacteristic%20generator%20achieves%20high%20generation%20quality%20with%20just%20a%20single%0Aevaluation%20of%20neural%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05512v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacteristic%2520Learning%2520for%2520Provable%2520One%2520Step%2520Generation%26entry.906535625%3DZhao%2520Ding%2520and%2520Chenguang%2520Duan%2520and%2520Yuling%2520Jiao%2520and%2520Ruoxuan%2520Li%2520and%2520Jerry%2520Zhijian%2520Yang%2520and%2520Pingwen%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520characteristic%2520generator%252C%2520a%2520novel%2520one-step%2520generative%2520model%250Athat%2520combines%2520the%2520efficiency%2520of%2520sampling%2520in%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%2520with%2520the%2520stable%2520performance%2520of%2520flow-based%2520models.%2520Our%2520model%2520is%2520driven%2520by%250Acharacteristics%252C%2520along%2520which%2520the%2520probability%2520density%2520transport%2520can%2520be%2520described%250Aby%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529.%2520Specifically%252C%2520We%2520estimate%2520the%250Avelocity%2520field%2520through%2520nonparametric%2520regression%2520and%2520utilize%2520Euler%2520method%2520to%250Asolve%2520the%2520probability%2520flow%2520ODE%252C%2520generating%2520a%2520series%2520of%2520discrete%2520approximations%250Ato%2520the%2520characteristics.%2520We%2520then%2520use%2520a%2520deep%2520neural%2520network%2520to%2520fit%2520these%250Acharacteristics%252C%2520ensuring%2520a%2520one-step%2520mapping%2520that%2520effectively%2520pushes%2520the%2520prior%250Adistribution%2520towards%2520the%2520target%2520distribution.%2520In%2520the%2520theoretical%2520aspect%252C%2520we%250Aanalyze%2520the%2520errors%2520in%2520velocity%2520matching%252C%2520Euler%2520discretization%252C%2520and%250Acharacteristic%2520fitting%2520to%2520establish%2520a%2520non-asymptotic%2520convergence%2520rate%2520for%2520the%250Acharacteristic%2520generator%2520in%25202-Wasserstein%2520distance.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520thorough%2520analysis%2520for%2520simulation-free%2520one%2520step%250Agenerative%2520models.%2520Additionally%252C%2520our%2520analysis%2520refines%2520the%2520error%2520analysis%2520of%250Aflow-based%2520generative%2520models%2520in%2520prior%2520works.%2520We%2520apply%2520our%2520method%2520on%2520both%250Asynthetic%2520and%2520real%2520datasets%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520the%250Acharacteristic%2520generator%2520achieves%2520high%2520generation%2520quality%2520with%2520just%2520a%2520single%250Aevaluation%2520of%2520neural%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05512v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characteristic%20Learning%20for%20Provable%20One%20Step%20Generation&entry.906535625=Zhao%20Ding%20and%20Chenguang%20Duan%20and%20Yuling%20Jiao%20and%20Ruoxuan%20Li%20and%20Jerry%20Zhijian%20Yang%20and%20Pingwen%20Zhang&entry.1292438233=%20%20We%20propose%20the%20characteristic%20generator%2C%20a%20novel%20one-step%20generative%20model%0Athat%20combines%20the%20efficiency%20of%20sampling%20in%20Generative%20Adversarial%20Networks%0A%28GANs%29%20with%20the%20stable%20performance%20of%20flow-based%20models.%20Our%20model%20is%20driven%20by%0Acharacteristics%2C%20along%20which%20the%20probability%20density%20transport%20can%20be%20described%0Aby%20ordinary%20differential%20equations%20%28ODEs%29.%20Specifically%2C%20We%20estimate%20the%0Avelocity%20field%20through%20nonparametric%20regression%20and%20utilize%20Euler%20method%20to%0Asolve%20the%20probability%20flow%20ODE%2C%20generating%20a%20series%20of%20discrete%20approximations%0Ato%20the%20characteristics.%20We%20then%20use%20a%20deep%20neural%20network%20to%20fit%20these%0Acharacteristics%2C%20ensuring%20a%20one-step%20mapping%20that%20effectively%20pushes%20the%20prior%0Adistribution%20towards%20the%20target%20distribution.%20In%20the%20theoretical%20aspect%2C%20we%0Aanalyze%20the%20errors%20in%20velocity%20matching%2C%20Euler%20discretization%2C%20and%0Acharacteristic%20fitting%20to%20establish%20a%20non-asymptotic%20convergence%20rate%20for%20the%0Acharacteristic%20generator%20in%202-Wasserstein%20distance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20thorough%20analysis%20for%20simulation-free%20one%20step%0Agenerative%20models.%20Additionally%2C%20our%20analysis%20refines%20the%20error%20analysis%20of%0Aflow-based%20generative%20models%20in%20prior%20works.%20We%20apply%20our%20method%20on%20both%0Asynthetic%20and%20real%20datasets%2C%20and%20the%20results%20demonstrate%20that%20the%0Acharacteristic%20generator%20achieves%20high%20generation%20quality%20with%20just%20a%20single%0Aevaluation%20of%20neural%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05512v3&entry.124074799=Read"},
{"title": "VFIMamba: Video Frame Interpolation with State Space Models", "author": "Guozhen Zhang and Chunxu Liu and Yutao Cui and Xiaotong Zhao and Kai Ma and Limin Wang", "abstract": "  Inter-frame modeling is pivotal in generating intermediate frames for video\nframe interpolation (VFI). Current approaches predominantly rely on convolution\nor attention-based models, which often either lack sufficient receptive fields\nor entail significant computational overheads. Recently, Selective State Space\nModels (S6) have emerged, tailored specifically for long sequence modeling,\noffering both linear complexity and data-dependent modeling capabilities. In\nthis paper, we propose VFIMamba, a novel frame interpolation method for\nefficient and dynamic inter-frame modeling by harnessing the S6 model. Our\napproach introduces the Mixed-SSM Block (MSB), which initially rearranges\ntokens from adjacent frames in an interleaved fashion and subsequently applies\nmulti-directional S6 modeling. This design facilitates the efficient\ntransmission of information across frames while upholding linear complexity.\nFurthermore, we introduce a novel curriculum learning strategy that\nprogressively cultivates proficiency in modeling inter-frame dynamics across\nvarying motion magnitudes, fully unleashing the potential of the S6 model.\nExperimental findings showcase that our method attains state-of-the-art\nperformance across diverse benchmarks, particularly excelling in\nhigh-resolution scenarios. In particular, on the X-TEST dataset, VFIMamba\ndemonstrates a noteworthy improvement of 0.80 dB for 4K frames and 0.96 dB for\n2K frames.\n", "link": "http://arxiv.org/abs/2407.02315v1", "date": "2024-07-02", "relevancy": 2.1452, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5331}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VFIMamba%3A%20Video%20Frame%20Interpolation%20with%20State%20Space%20Models&body=Title%3A%20VFIMamba%3A%20Video%20Frame%20Interpolation%20with%20State%20Space%20Models%0AAuthor%3A%20Guozhen%20Zhang%20and%20Chunxu%20Liu%20and%20Yutao%20Cui%20and%20Xiaotong%20Zhao%20and%20Kai%20Ma%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Inter-frame%20modeling%20is%20pivotal%20in%20generating%20intermediate%20frames%20for%20video%0Aframe%20interpolation%20%28VFI%29.%20Current%20approaches%20predominantly%20rely%20on%20convolution%0Aor%20attention-based%20models%2C%20which%20often%20either%20lack%20sufficient%20receptive%20fields%0Aor%20entail%20significant%20computational%20overheads.%20Recently%2C%20Selective%20State%20Space%0AModels%20%28S6%29%20have%20emerged%2C%20tailored%20specifically%20for%20long%20sequence%20modeling%2C%0Aoffering%20both%20linear%20complexity%20and%20data-dependent%20modeling%20capabilities.%20In%0Athis%20paper%2C%20we%20propose%20VFIMamba%2C%20a%20novel%20frame%20interpolation%20method%20for%0Aefficient%20and%20dynamic%20inter-frame%20modeling%20by%20harnessing%20the%20S6%20model.%20Our%0Aapproach%20introduces%20the%20Mixed-SSM%20Block%20%28MSB%29%2C%20which%20initially%20rearranges%0Atokens%20from%20adjacent%20frames%20in%20an%20interleaved%20fashion%20and%20subsequently%20applies%0Amulti-directional%20S6%20modeling.%20This%20design%20facilitates%20the%20efficient%0Atransmission%20of%20information%20across%20frames%20while%20upholding%20linear%20complexity.%0AFurthermore%2C%20we%20introduce%20a%20novel%20curriculum%20learning%20strategy%20that%0Aprogressively%20cultivates%20proficiency%20in%20modeling%20inter-frame%20dynamics%20across%0Avarying%20motion%20magnitudes%2C%20fully%20unleashing%20the%20potential%20of%20the%20S6%20model.%0AExperimental%20findings%20showcase%20that%20our%20method%20attains%20state-of-the-art%0Aperformance%20across%20diverse%20benchmarks%2C%20particularly%20excelling%20in%0Ahigh-resolution%20scenarios.%20In%20particular%2C%20on%20the%20X-TEST%20dataset%2C%20VFIMamba%0Ademonstrates%20a%20noteworthy%20improvement%20of%200.80%20dB%20for%204K%20frames%20and%200.96%20dB%20for%0A2K%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVFIMamba%253A%2520Video%2520Frame%2520Interpolation%2520with%2520State%2520Space%2520Models%26entry.906535625%3DGuozhen%2520Zhang%2520and%2520Chunxu%2520Liu%2520and%2520Yutao%2520Cui%2520and%2520Xiaotong%2520Zhao%2520and%2520Kai%2520Ma%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Inter-frame%2520modeling%2520is%2520pivotal%2520in%2520generating%2520intermediate%2520frames%2520for%2520video%250Aframe%2520interpolation%2520%2528VFI%2529.%2520Current%2520approaches%2520predominantly%2520rely%2520on%2520convolution%250Aor%2520attention-based%2520models%252C%2520which%2520often%2520either%2520lack%2520sufficient%2520receptive%2520fields%250Aor%2520entail%2520significant%2520computational%2520overheads.%2520Recently%252C%2520Selective%2520State%2520Space%250AModels%2520%2528S6%2529%2520have%2520emerged%252C%2520tailored%2520specifically%2520for%2520long%2520sequence%2520modeling%252C%250Aoffering%2520both%2520linear%2520complexity%2520and%2520data-dependent%2520modeling%2520capabilities.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520VFIMamba%252C%2520a%2520novel%2520frame%2520interpolation%2520method%2520for%250Aefficient%2520and%2520dynamic%2520inter-frame%2520modeling%2520by%2520harnessing%2520the%2520S6%2520model.%2520Our%250Aapproach%2520introduces%2520the%2520Mixed-SSM%2520Block%2520%2528MSB%2529%252C%2520which%2520initially%2520rearranges%250Atokens%2520from%2520adjacent%2520frames%2520in%2520an%2520interleaved%2520fashion%2520and%2520subsequently%2520applies%250Amulti-directional%2520S6%2520modeling.%2520This%2520design%2520facilitates%2520the%2520efficient%250Atransmission%2520of%2520information%2520across%2520frames%2520while%2520upholding%2520linear%2520complexity.%250AFurthermore%252C%2520we%2520introduce%2520a%2520novel%2520curriculum%2520learning%2520strategy%2520that%250Aprogressively%2520cultivates%2520proficiency%2520in%2520modeling%2520inter-frame%2520dynamics%2520across%250Avarying%2520motion%2520magnitudes%252C%2520fully%2520unleashing%2520the%2520potential%2520of%2520the%2520S6%2520model.%250AExperimental%2520findings%2520showcase%2520that%2520our%2520method%2520attains%2520state-of-the-art%250Aperformance%2520across%2520diverse%2520benchmarks%252C%2520particularly%2520excelling%2520in%250Ahigh-resolution%2520scenarios.%2520In%2520particular%252C%2520on%2520the%2520X-TEST%2520dataset%252C%2520VFIMamba%250Ademonstrates%2520a%2520noteworthy%2520improvement%2520of%25200.80%2520dB%2520for%25204K%2520frames%2520and%25200.96%2520dB%2520for%250A2K%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFIMamba%3A%20Video%20Frame%20Interpolation%20with%20State%20Space%20Models&entry.906535625=Guozhen%20Zhang%20and%20Chunxu%20Liu%20and%20Yutao%20Cui%20and%20Xiaotong%20Zhao%20and%20Kai%20Ma%20and%20Limin%20Wang&entry.1292438233=%20%20Inter-frame%20modeling%20is%20pivotal%20in%20generating%20intermediate%20frames%20for%20video%0Aframe%20interpolation%20%28VFI%29.%20Current%20approaches%20predominantly%20rely%20on%20convolution%0Aor%20attention-based%20models%2C%20which%20often%20either%20lack%20sufficient%20receptive%20fields%0Aor%20entail%20significant%20computational%20overheads.%20Recently%2C%20Selective%20State%20Space%0AModels%20%28S6%29%20have%20emerged%2C%20tailored%20specifically%20for%20long%20sequence%20modeling%2C%0Aoffering%20both%20linear%20complexity%20and%20data-dependent%20modeling%20capabilities.%20In%0Athis%20paper%2C%20we%20propose%20VFIMamba%2C%20a%20novel%20frame%20interpolation%20method%20for%0Aefficient%20and%20dynamic%20inter-frame%20modeling%20by%20harnessing%20the%20S6%20model.%20Our%0Aapproach%20introduces%20the%20Mixed-SSM%20Block%20%28MSB%29%2C%20which%20initially%20rearranges%0Atokens%20from%20adjacent%20frames%20in%20an%20interleaved%20fashion%20and%20subsequently%20applies%0Amulti-directional%20S6%20modeling.%20This%20design%20facilitates%20the%20efficient%0Atransmission%20of%20information%20across%20frames%20while%20upholding%20linear%20complexity.%0AFurthermore%2C%20we%20introduce%20a%20novel%20curriculum%20learning%20strategy%20that%0Aprogressively%20cultivates%20proficiency%20in%20modeling%20inter-frame%20dynamics%20across%0Avarying%20motion%20magnitudes%2C%20fully%20unleashing%20the%20potential%20of%20the%20S6%20model.%0AExperimental%20findings%20showcase%20that%20our%20method%20attains%20state-of-the-art%0Aperformance%20across%20diverse%20benchmarks%2C%20particularly%20excelling%20in%0Ahigh-resolution%20scenarios.%20In%20particular%2C%20on%20the%20X-TEST%20dataset%2C%20VFIMamba%0Ademonstrates%20a%20noteworthy%20improvement%20of%200.80%20dB%20for%204K%20frames%20and%200.96%20dB%20for%0A2K%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02315v1&entry.124074799=Read"},
{"title": "Research on Reliable and Safe Occupancy Grid Prediction in Underground\n  Parking Lots", "author": "JiaQi Luo", "abstract": "  Against the backdrop of advancing science and technology, autonomous vehicle\ntechnology has emerged as a focal point of intense scrutiny within the academic\ncommunity. Nevertheless, the challenge persists in guaranteeing the safety and\nreliability of this technology when navigating intricate scenarios. While a\nsubstantial portion of autonomous driving research is dedicated to testing in\nopen-air environments, such as urban roads and highways, where the myriad\nvariables at play are meticulously examined, enclosed indoor spaces like\nunderground parking lots have, to a significant extent, been overlooked in the\nscholarly discourse. This discrepancy highlights a gap in derstanding the\nunique challenges these confined settings pose for autonomous navigation\nsystems.\n  This study tackles indoor autonomous driving, particularly in overlooked\nspaces like underground parking lots. Using CARLA's simulation platform, a\nrealistic parking model is created for data gathering. An occupancy grid\nnetwork then processes this data to predict vehicle paths and obstacles,\nenhancing the system's perception in complex indoor environments. Ultimately,\nthis strategy improves safety in autonomous parking operations. The paper\nmeticulously evaluates the model's predictive capabilities, validating its\nefficacy in the context of underground parking. Our findings confirm that the\nproposed strategy successfully enhances autonomous vehicle performance in these\ncomplex indoor settings. It equips autonomous systems with improved adaptation\nto underground lots, reinforcing safety measures and dependability. This work\npaves the way for future advancements and applications by addressing the\nresearch shortfall concerning indoor parking environments, serving as a pivotal\nreference point.\n", "link": "http://arxiv.org/abs/2407.02197v1", "date": "2024-07-02", "relevancy": 2.1403, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5715}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5465}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Reliable%20and%20Safe%20Occupancy%20Grid%20Prediction%20in%20Underground%0A%20%20Parking%20Lots&body=Title%3A%20Research%20on%20Reliable%20and%20Safe%20Occupancy%20Grid%20Prediction%20in%20Underground%0A%20%20Parking%20Lots%0AAuthor%3A%20JiaQi%20Luo%0AAbstract%3A%20%20%20Against%20the%20backdrop%20of%20advancing%20science%20and%20technology%2C%20autonomous%20vehicle%0Atechnology%20has%20emerged%20as%20a%20focal%20point%20of%20intense%20scrutiny%20within%20the%20academic%0Acommunity.%20Nevertheless%2C%20the%20challenge%20persists%20in%20guaranteeing%20the%20safety%20and%0Areliability%20of%20this%20technology%20when%20navigating%20intricate%20scenarios.%20While%20a%0Asubstantial%20portion%20of%20autonomous%20driving%20research%20is%20dedicated%20to%20testing%20in%0Aopen-air%20environments%2C%20such%20as%20urban%20roads%20and%20highways%2C%20where%20the%20myriad%0Avariables%20at%20play%20are%20meticulously%20examined%2C%20enclosed%20indoor%20spaces%20like%0Aunderground%20parking%20lots%20have%2C%20to%20a%20significant%20extent%2C%20been%20overlooked%20in%20the%0Ascholarly%20discourse.%20This%20discrepancy%20highlights%20a%20gap%20in%20derstanding%20the%0Aunique%20challenges%20these%20confined%20settings%20pose%20for%20autonomous%20navigation%0Asystems.%0A%20%20This%20study%20tackles%20indoor%20autonomous%20driving%2C%20particularly%20in%20overlooked%0Aspaces%20like%20underground%20parking%20lots.%20Using%20CARLA%27s%20simulation%20platform%2C%20a%0Arealistic%20parking%20model%20is%20created%20for%20data%20gathering.%20An%20occupancy%20grid%0Anetwork%20then%20processes%20this%20data%20to%20predict%20vehicle%20paths%20and%20obstacles%2C%0Aenhancing%20the%20system%27s%20perception%20in%20complex%20indoor%20environments.%20Ultimately%2C%0Athis%20strategy%20improves%20safety%20in%20autonomous%20parking%20operations.%20The%20paper%0Ameticulously%20evaluates%20the%20model%27s%20predictive%20capabilities%2C%20validating%20its%0Aefficacy%20in%20the%20context%20of%20underground%20parking.%20Our%20findings%20confirm%20that%20the%0Aproposed%20strategy%20successfully%20enhances%20autonomous%20vehicle%20performance%20in%20these%0Acomplex%20indoor%20settings.%20It%20equips%20autonomous%20systems%20with%20improved%20adaptation%0Ato%20underground%20lots%2C%20reinforcing%20safety%20measures%20and%20dependability.%20This%20work%0Apaves%20the%20way%20for%20future%20advancements%20and%20applications%20by%20addressing%20the%0Aresearch%20shortfall%20concerning%20indoor%20parking%20environments%2C%20serving%20as%20a%20pivotal%0Areference%20point.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Reliable%2520and%2520Safe%2520Occupancy%2520Grid%2520Prediction%2520in%2520Underground%250A%2520%2520Parking%2520Lots%26entry.906535625%3DJiaQi%2520Luo%26entry.1292438233%3D%2520%2520Against%2520the%2520backdrop%2520of%2520advancing%2520science%2520and%2520technology%252C%2520autonomous%2520vehicle%250Atechnology%2520has%2520emerged%2520as%2520a%2520focal%2520point%2520of%2520intense%2520scrutiny%2520within%2520the%2520academic%250Acommunity.%2520Nevertheless%252C%2520the%2520challenge%2520persists%2520in%2520guaranteeing%2520the%2520safety%2520and%250Areliability%2520of%2520this%2520technology%2520when%2520navigating%2520intricate%2520scenarios.%2520While%2520a%250Asubstantial%2520portion%2520of%2520autonomous%2520driving%2520research%2520is%2520dedicated%2520to%2520testing%2520in%250Aopen-air%2520environments%252C%2520such%2520as%2520urban%2520roads%2520and%2520highways%252C%2520where%2520the%2520myriad%250Avariables%2520at%2520play%2520are%2520meticulously%2520examined%252C%2520enclosed%2520indoor%2520spaces%2520like%250Aunderground%2520parking%2520lots%2520have%252C%2520to%2520a%2520significant%2520extent%252C%2520been%2520overlooked%2520in%2520the%250Ascholarly%2520discourse.%2520This%2520discrepancy%2520highlights%2520a%2520gap%2520in%2520derstanding%2520the%250Aunique%2520challenges%2520these%2520confined%2520settings%2520pose%2520for%2520autonomous%2520navigation%250Asystems.%250A%2520%2520This%2520study%2520tackles%2520indoor%2520autonomous%2520driving%252C%2520particularly%2520in%2520overlooked%250Aspaces%2520like%2520underground%2520parking%2520lots.%2520Using%2520CARLA%2527s%2520simulation%2520platform%252C%2520a%250Arealistic%2520parking%2520model%2520is%2520created%2520for%2520data%2520gathering.%2520An%2520occupancy%2520grid%250Anetwork%2520then%2520processes%2520this%2520data%2520to%2520predict%2520vehicle%2520paths%2520and%2520obstacles%252C%250Aenhancing%2520the%2520system%2527s%2520perception%2520in%2520complex%2520indoor%2520environments.%2520Ultimately%252C%250Athis%2520strategy%2520improves%2520safety%2520in%2520autonomous%2520parking%2520operations.%2520The%2520paper%250Ameticulously%2520evaluates%2520the%2520model%2527s%2520predictive%2520capabilities%252C%2520validating%2520its%250Aefficacy%2520in%2520the%2520context%2520of%2520underground%2520parking.%2520Our%2520findings%2520confirm%2520that%2520the%250Aproposed%2520strategy%2520successfully%2520enhances%2520autonomous%2520vehicle%2520performance%2520in%2520these%250Acomplex%2520indoor%2520settings.%2520It%2520equips%2520autonomous%2520systems%2520with%2520improved%2520adaptation%250Ato%2520underground%2520lots%252C%2520reinforcing%2520safety%2520measures%2520and%2520dependability.%2520This%2520work%250Apaves%2520the%2520way%2520for%2520future%2520advancements%2520and%2520applications%2520by%2520addressing%2520the%250Aresearch%2520shortfall%2520concerning%2520indoor%2520parking%2520environments%252C%2520serving%2520as%2520a%2520pivotal%250Areference%2520point.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Reliable%20and%20Safe%20Occupancy%20Grid%20Prediction%20in%20Underground%0A%20%20Parking%20Lots&entry.906535625=JiaQi%20Luo&entry.1292438233=%20%20Against%20the%20backdrop%20of%20advancing%20science%20and%20technology%2C%20autonomous%20vehicle%0Atechnology%20has%20emerged%20as%20a%20focal%20point%20of%20intense%20scrutiny%20within%20the%20academic%0Acommunity.%20Nevertheless%2C%20the%20challenge%20persists%20in%20guaranteeing%20the%20safety%20and%0Areliability%20of%20this%20technology%20when%20navigating%20intricate%20scenarios.%20While%20a%0Asubstantial%20portion%20of%20autonomous%20driving%20research%20is%20dedicated%20to%20testing%20in%0Aopen-air%20environments%2C%20such%20as%20urban%20roads%20and%20highways%2C%20where%20the%20myriad%0Avariables%20at%20play%20are%20meticulously%20examined%2C%20enclosed%20indoor%20spaces%20like%0Aunderground%20parking%20lots%20have%2C%20to%20a%20significant%20extent%2C%20been%20overlooked%20in%20the%0Ascholarly%20discourse.%20This%20discrepancy%20highlights%20a%20gap%20in%20derstanding%20the%0Aunique%20challenges%20these%20confined%20settings%20pose%20for%20autonomous%20navigation%0Asystems.%0A%20%20This%20study%20tackles%20indoor%20autonomous%20driving%2C%20particularly%20in%20overlooked%0Aspaces%20like%20underground%20parking%20lots.%20Using%20CARLA%27s%20simulation%20platform%2C%20a%0Arealistic%20parking%20model%20is%20created%20for%20data%20gathering.%20An%20occupancy%20grid%0Anetwork%20then%20processes%20this%20data%20to%20predict%20vehicle%20paths%20and%20obstacles%2C%0Aenhancing%20the%20system%27s%20perception%20in%20complex%20indoor%20environments.%20Ultimately%2C%0Athis%20strategy%20improves%20safety%20in%20autonomous%20parking%20operations.%20The%20paper%0Ameticulously%20evaluates%20the%20model%27s%20predictive%20capabilities%2C%20validating%20its%0Aefficacy%20in%20the%20context%20of%20underground%20parking.%20Our%20findings%20confirm%20that%20the%0Aproposed%20strategy%20successfully%20enhances%20autonomous%20vehicle%20performance%20in%20these%0Acomplex%20indoor%20settings.%20It%20equips%20autonomous%20systems%20with%20improved%20adaptation%0Ato%20underground%20lots%2C%20reinforcing%20safety%20measures%20and%20dependability.%20This%20work%0Apaves%20the%20way%20for%20future%20advancements%20and%20applications%20by%20addressing%20the%0Aresearch%20shortfall%20concerning%20indoor%20parking%20environments%2C%20serving%20as%20a%20pivotal%0Areference%20point.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02197v1&entry.124074799=Read"},
{"title": "MIREncoder: Multi-modal IR-based Pretrained Embeddings for Performance\n  Optimizations", "author": "Akash Dutta and Ali Jannesari", "abstract": "  One of the primary areas of interest in High Performance Computing is the\nimprovement of performance of parallel workloads. Nowadays, compilable source\ncode-based optimization tasks that employ deep learning often exploit LLVM\nIntermediate Representations (IRs) for extracting features from source code.\nMost such works target specific tasks, or are designed with a pre-defined set\nof heuristics. So far, pre-trained models are rare in this domain, but the\npossibilities have been widely discussed. Especially approaches mimicking\nlarge-language models (LLMs) have been proposed. But these have prohibitively\nlarge training costs. In this paper, we propose MIREncoder, a M}ulti-modal\nIR-based Auto-Encoder that can be pre-trained to generate a learned embedding\nspace to be used for downstream tasks by machine learning-based approaches. A\nmulti-modal approach enables us to better extract features from compilable\nprograms. It allows us to better model code syntax, semantics and structure.\nFor code-based performance optimizations, these features are very important\nwhile making optimization decisions. A pre-trained model/embedding implicitly\nenables the usage of transfer learning, and helps move away from task-specific\ntrained models. Additionally, a pre-trained model used for downstream\nperformance optimization should itself have reduced overhead, and be easily\nusable. These considerations have led us to propose a modeling approach that i)\nunderstands code semantics and structure, ii) enables use of transfer learning,\nand iii) is small and simple enough to be easily re-purposed or reused even\nwith low resource availability. Our evaluations will show that our proposed\napproach can outperform the state of the art while reducing overhead.\n", "link": "http://arxiv.org/abs/2407.02238v1", "date": "2024-07-02", "relevancy": 2.1378, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIREncoder%3A%20Multi-modal%20IR-based%20Pretrained%20Embeddings%20for%20Performance%0A%20%20Optimizations&body=Title%3A%20MIREncoder%3A%20Multi-modal%20IR-based%20Pretrained%20Embeddings%20for%20Performance%0A%20%20Optimizations%0AAuthor%3A%20Akash%20Dutta%20and%20Ali%20Jannesari%0AAbstract%3A%20%20%20One%20of%20the%20primary%20areas%20of%20interest%20in%20High%20Performance%20Computing%20is%20the%0Aimprovement%20of%20performance%20of%20parallel%20workloads.%20Nowadays%2C%20compilable%20source%0Acode-based%20optimization%20tasks%20that%20employ%20deep%20learning%20often%20exploit%20LLVM%0AIntermediate%20Representations%20%28IRs%29%20for%20extracting%20features%20from%20source%20code.%0AMost%20such%20works%20target%20specific%20tasks%2C%20or%20are%20designed%20with%20a%20pre-defined%20set%0Aof%20heuristics.%20So%20far%2C%20pre-trained%20models%20are%20rare%20in%20this%20domain%2C%20but%20the%0Apossibilities%20have%20been%20widely%20discussed.%20Especially%20approaches%20mimicking%0Alarge-language%20models%20%28LLMs%29%20have%20been%20proposed.%20But%20these%20have%20prohibitively%0Alarge%20training%20costs.%20In%20this%20paper%2C%20we%20propose%20MIREncoder%2C%20a%20M%7Dulti-modal%0AIR-based%20Auto-Encoder%20that%20can%20be%20pre-trained%20to%20generate%20a%20learned%20embedding%0Aspace%20to%20be%20used%20for%20downstream%20tasks%20by%20machine%20learning-based%20approaches.%20A%0Amulti-modal%20approach%20enables%20us%20to%20better%20extract%20features%20from%20compilable%0Aprograms.%20It%20allows%20us%20to%20better%20model%20code%20syntax%2C%20semantics%20and%20structure.%0AFor%20code-based%20performance%20optimizations%2C%20these%20features%20are%20very%20important%0Awhile%20making%20optimization%20decisions.%20A%20pre-trained%20model/embedding%20implicitly%0Aenables%20the%20usage%20of%20transfer%20learning%2C%20and%20helps%20move%20away%20from%20task-specific%0Atrained%20models.%20Additionally%2C%20a%20pre-trained%20model%20used%20for%20downstream%0Aperformance%20optimization%20should%20itself%20have%20reduced%20overhead%2C%20and%20be%20easily%0Ausable.%20These%20considerations%20have%20led%20us%20to%20propose%20a%20modeling%20approach%20that%20i%29%0Aunderstands%20code%20semantics%20and%20structure%2C%20ii%29%20enables%20use%20of%20transfer%20learning%2C%0Aand%20iii%29%20is%20small%20and%20simple%20enough%20to%20be%20easily%20re-purposed%20or%20reused%20even%0Awith%20low%20resource%20availability.%20Our%20evaluations%20will%20show%20that%20our%20proposed%0Aapproach%20can%20outperform%20the%20state%20of%20the%20art%20while%20reducing%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIREncoder%253A%2520Multi-modal%2520IR-based%2520Pretrained%2520Embeddings%2520for%2520Performance%250A%2520%2520Optimizations%26entry.906535625%3DAkash%2520Dutta%2520and%2520Ali%2520Jannesari%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520primary%2520areas%2520of%2520interest%2520in%2520High%2520Performance%2520Computing%2520is%2520the%250Aimprovement%2520of%2520performance%2520of%2520parallel%2520workloads.%2520Nowadays%252C%2520compilable%2520source%250Acode-based%2520optimization%2520tasks%2520that%2520employ%2520deep%2520learning%2520often%2520exploit%2520LLVM%250AIntermediate%2520Representations%2520%2528IRs%2529%2520for%2520extracting%2520features%2520from%2520source%2520code.%250AMost%2520such%2520works%2520target%2520specific%2520tasks%252C%2520or%2520are%2520designed%2520with%2520a%2520pre-defined%2520set%250Aof%2520heuristics.%2520So%2520far%252C%2520pre-trained%2520models%2520are%2520rare%2520in%2520this%2520domain%252C%2520but%2520the%250Apossibilities%2520have%2520been%2520widely%2520discussed.%2520Especially%2520approaches%2520mimicking%250Alarge-language%2520models%2520%2528LLMs%2529%2520have%2520been%2520proposed.%2520But%2520these%2520have%2520prohibitively%250Alarge%2520training%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MIREncoder%252C%2520a%2520M%257Dulti-modal%250AIR-based%2520Auto-Encoder%2520that%2520can%2520be%2520pre-trained%2520to%2520generate%2520a%2520learned%2520embedding%250Aspace%2520to%2520be%2520used%2520for%2520downstream%2520tasks%2520by%2520machine%2520learning-based%2520approaches.%2520A%250Amulti-modal%2520approach%2520enables%2520us%2520to%2520better%2520extract%2520features%2520from%2520compilable%250Aprograms.%2520It%2520allows%2520us%2520to%2520better%2520model%2520code%2520syntax%252C%2520semantics%2520and%2520structure.%250AFor%2520code-based%2520performance%2520optimizations%252C%2520these%2520features%2520are%2520very%2520important%250Awhile%2520making%2520optimization%2520decisions.%2520A%2520pre-trained%2520model/embedding%2520implicitly%250Aenables%2520the%2520usage%2520of%2520transfer%2520learning%252C%2520and%2520helps%2520move%2520away%2520from%2520task-specific%250Atrained%2520models.%2520Additionally%252C%2520a%2520pre-trained%2520model%2520used%2520for%2520downstream%250Aperformance%2520optimization%2520should%2520itself%2520have%2520reduced%2520overhead%252C%2520and%2520be%2520easily%250Ausable.%2520These%2520considerations%2520have%2520led%2520us%2520to%2520propose%2520a%2520modeling%2520approach%2520that%2520i%2529%250Aunderstands%2520code%2520semantics%2520and%2520structure%252C%2520ii%2529%2520enables%2520use%2520of%2520transfer%2520learning%252C%250Aand%2520iii%2529%2520is%2520small%2520and%2520simple%2520enough%2520to%2520be%2520easily%2520re-purposed%2520or%2520reused%2520even%250Awith%2520low%2520resource%2520availability.%2520Our%2520evaluations%2520will%2520show%2520that%2520our%2520proposed%250Aapproach%2520can%2520outperform%2520the%2520state%2520of%2520the%2520art%2520while%2520reducing%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIREncoder%3A%20Multi-modal%20IR-based%20Pretrained%20Embeddings%20for%20Performance%0A%20%20Optimizations&entry.906535625=Akash%20Dutta%20and%20Ali%20Jannesari&entry.1292438233=%20%20One%20of%20the%20primary%20areas%20of%20interest%20in%20High%20Performance%20Computing%20is%20the%0Aimprovement%20of%20performance%20of%20parallel%20workloads.%20Nowadays%2C%20compilable%20source%0Acode-based%20optimization%20tasks%20that%20employ%20deep%20learning%20often%20exploit%20LLVM%0AIntermediate%20Representations%20%28IRs%29%20for%20extracting%20features%20from%20source%20code.%0AMost%20such%20works%20target%20specific%20tasks%2C%20or%20are%20designed%20with%20a%20pre-defined%20set%0Aof%20heuristics.%20So%20far%2C%20pre-trained%20models%20are%20rare%20in%20this%20domain%2C%20but%20the%0Apossibilities%20have%20been%20widely%20discussed.%20Especially%20approaches%20mimicking%0Alarge-language%20models%20%28LLMs%29%20have%20been%20proposed.%20But%20these%20have%20prohibitively%0Alarge%20training%20costs.%20In%20this%20paper%2C%20we%20propose%20MIREncoder%2C%20a%20M%7Dulti-modal%0AIR-based%20Auto-Encoder%20that%20can%20be%20pre-trained%20to%20generate%20a%20learned%20embedding%0Aspace%20to%20be%20used%20for%20downstream%20tasks%20by%20machine%20learning-based%20approaches.%20A%0Amulti-modal%20approach%20enables%20us%20to%20better%20extract%20features%20from%20compilable%0Aprograms.%20It%20allows%20us%20to%20better%20model%20code%20syntax%2C%20semantics%20and%20structure.%0AFor%20code-based%20performance%20optimizations%2C%20these%20features%20are%20very%20important%0Awhile%20making%20optimization%20decisions.%20A%20pre-trained%20model/embedding%20implicitly%0Aenables%20the%20usage%20of%20transfer%20learning%2C%20and%20helps%20move%20away%20from%20task-specific%0Atrained%20models.%20Additionally%2C%20a%20pre-trained%20model%20used%20for%20downstream%0Aperformance%20optimization%20should%20itself%20have%20reduced%20overhead%2C%20and%20be%20easily%0Ausable.%20These%20considerations%20have%20led%20us%20to%20propose%20a%20modeling%20approach%20that%20i%29%0Aunderstands%20code%20semantics%20and%20structure%2C%20ii%29%20enables%20use%20of%20transfer%20learning%2C%0Aand%20iii%29%20is%20small%20and%20simple%20enough%20to%20be%20easily%20re-purposed%20or%20reused%20even%0Awith%20low%20resource%20availability.%20Our%20evaluations%20will%20show%20that%20our%20proposed%0Aapproach%20can%20outperform%20the%20state%20of%20the%20art%20while%20reducing%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02238v1&entry.124074799=Read"},
{"title": "Cradle: Empowering Foundation Agents Towards General Computer Control", "author": "Weihao Tan and Wentao Zhang and Xinrun Xu and Haochong Xia and Ziluo Ding and Boyu Li and Bohan Zhou and Junpeng Yue and Jiechuan Jiang and Yewen Li and Ruyi An and Molei Qin and Chuqiao Zong and Longtao Zheng and Yujie Wu and Xiaoqiang Chai and Yifei Bi and Tianbao Xie and Pengjie Gu and Xiyun Li and Ceyao Zhang and Long Tian and Chaojie Wang and Xinrun Wang and B\u00f6rje F. Karlsson and Bo An and Shuicheng Yan and Zongqing Lu", "abstract": "  Despite the success in specific scenarios, existing foundation agents still\nstruggle to generalize across various virtual scenarios, mainly due to the\ndramatically different encapsulations of environments with manually designed\nobservation and action spaces. To handle this issue, we propose the General\nComputer Control (GCC) setting to restrict foundation agents to interact with\nsoftware through the most unified and standardized interface, i.e., using\nscreenshots as input and keyboard and mouse actions as output. We introduce\nCradle, a modular and flexible LMM-powered framework, as a preliminary attempt\ntowards GCC. Enhanced by six key modules, Cradle can understand input\nscreenshots and output executable code for low-level keyboard and mouse control\nafter high-level planning, so that Cradle can interact with any software and\ncomplete long-horizon complex tasks without relying on any built-in APIs.\nExperimental results show that Cradle exhibits remarkable generalizability and\nimpressive performance across four previously unexplored commercial video\ngames, five software applications, and a comprehensive benchmark, OSWorld.\nCradle is the first to enable foundation agents to follow the main storyline\nand complete 40-minute-long real missions in the complex AAA game Red Dead\nRedemption 2 (RDR2). Cradle can also create a city of a thousand people in\nCities: Skylines, farm and harvest parsnips in Stardew Valley, and trade and\nbargain with a maximal weekly total profit of 87% in Dealer's Life 2. Cradle\ncan not only operate daily software, like Chrome, Outlook, and Feishu, but also\nedit images and videos using Meitu and CapCut. Cradle greatly extends the reach\nof foundation agents by enabling the easy conversion of any software,\nespecially complex games, into benchmarks to evaluate agents' various abilities\nand facilitate further data collection, thus paving the way for generalist\nagents.\n", "link": "http://arxiv.org/abs/2403.03186v3", "date": "2024-07-02", "relevancy": 2.1339, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5793}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5355}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cradle%3A%20Empowering%20Foundation%20Agents%20Towards%20General%20Computer%20Control&body=Title%3A%20Cradle%3A%20Empowering%20Foundation%20Agents%20Towards%20General%20Computer%20Control%0AAuthor%3A%20Weihao%20Tan%20and%20Wentao%20Zhang%20and%20Xinrun%20Xu%20and%20Haochong%20Xia%20and%20Ziluo%20Ding%20and%20Boyu%20Li%20and%20Bohan%20Zhou%20and%20Junpeng%20Yue%20and%20Jiechuan%20Jiang%20and%20Yewen%20Li%20and%20Ruyi%20An%20and%20Molei%20Qin%20and%20Chuqiao%20Zong%20and%20Longtao%20Zheng%20and%20Yujie%20Wu%20and%20Xiaoqiang%20Chai%20and%20Yifei%20Bi%20and%20Tianbao%20Xie%20and%20Pengjie%20Gu%20and%20Xiyun%20Li%20and%20Ceyao%20Zhang%20and%20Long%20Tian%20and%20Chaojie%20Wang%20and%20Xinrun%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Bo%20An%20and%20Shuicheng%20Yan%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Despite%20the%20success%20in%20specific%20scenarios%2C%20existing%20foundation%20agents%20still%0Astruggle%20to%20generalize%20across%20various%20virtual%20scenarios%2C%20mainly%20due%20to%20the%0Adramatically%20different%20encapsulations%20of%20environments%20with%20manually%20designed%0Aobservation%20and%20action%20spaces.%20To%20handle%20this%20issue%2C%20we%20propose%20the%20General%0AComputer%20Control%20%28GCC%29%20setting%20to%20restrict%20foundation%20agents%20to%20interact%20with%0Asoftware%20through%20the%20most%20unified%20and%20standardized%20interface%2C%20i.e.%2C%20using%0Ascreenshots%20as%20input%20and%20keyboard%20and%20mouse%20actions%20as%20output.%20We%20introduce%0ACradle%2C%20a%20modular%20and%20flexible%20LMM-powered%20framework%2C%20as%20a%20preliminary%20attempt%0Atowards%20GCC.%20Enhanced%20by%20six%20key%20modules%2C%20Cradle%20can%20understand%20input%0Ascreenshots%20and%20output%20executable%20code%20for%20low-level%20keyboard%20and%20mouse%20control%0Aafter%20high-level%20planning%2C%20so%20that%20Cradle%20can%20interact%20with%20any%20software%20and%0Acomplete%20long-horizon%20complex%20tasks%20without%20relying%20on%20any%20built-in%20APIs.%0AExperimental%20results%20show%20that%20Cradle%20exhibits%20remarkable%20generalizability%20and%0Aimpressive%20performance%20across%20four%20previously%20unexplored%20commercial%20video%0Agames%2C%20five%20software%20applications%2C%20and%20a%20comprehensive%20benchmark%2C%20OSWorld.%0ACradle%20is%20the%20first%20to%20enable%20foundation%20agents%20to%20follow%20the%20main%20storyline%0Aand%20complete%2040-minute-long%20real%20missions%20in%20the%20complex%20AAA%20game%20Red%20Dead%0ARedemption%202%20%28RDR2%29.%20Cradle%20can%20also%20create%20a%20city%20of%20a%20thousand%20people%20in%0ACities%3A%20Skylines%2C%20farm%20and%20harvest%20parsnips%20in%20Stardew%20Valley%2C%20and%20trade%20and%0Abargain%20with%20a%20maximal%20weekly%20total%20profit%20of%2087%25%20in%20Dealer%27s%20Life%202.%20Cradle%0Acan%20not%20only%20operate%20daily%20software%2C%20like%20Chrome%2C%20Outlook%2C%20and%20Feishu%2C%20but%20also%0Aedit%20images%20and%20videos%20using%20Meitu%20and%20CapCut.%20Cradle%20greatly%20extends%20the%20reach%0Aof%20foundation%20agents%20by%20enabling%20the%20easy%20conversion%20of%20any%20software%2C%0Aespecially%20complex%20games%2C%20into%20benchmarks%20to%20evaluate%20agents%27%20various%20abilities%0Aand%20facilitate%20further%20data%20collection%2C%20thus%20paving%20the%20way%20for%20generalist%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03186v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCradle%253A%2520Empowering%2520Foundation%2520Agents%2520Towards%2520General%2520Computer%2520Control%26entry.906535625%3DWeihao%2520Tan%2520and%2520Wentao%2520Zhang%2520and%2520Xinrun%2520Xu%2520and%2520Haochong%2520Xia%2520and%2520Ziluo%2520Ding%2520and%2520Boyu%2520Li%2520and%2520Bohan%2520Zhou%2520and%2520Junpeng%2520Yue%2520and%2520Jiechuan%2520Jiang%2520and%2520Yewen%2520Li%2520and%2520Ruyi%2520An%2520and%2520Molei%2520Qin%2520and%2520Chuqiao%2520Zong%2520and%2520Longtao%2520Zheng%2520and%2520Yujie%2520Wu%2520and%2520Xiaoqiang%2520Chai%2520and%2520Yifei%2520Bi%2520and%2520Tianbao%2520Xie%2520and%2520Pengjie%2520Gu%2520and%2520Xiyun%2520Li%2520and%2520Ceyao%2520Zhang%2520and%2520Long%2520Tian%2520and%2520Chaojie%2520Wang%2520and%2520Xinrun%2520Wang%2520and%2520B%25C3%25B6rje%2520F.%2520Karlsson%2520and%2520Bo%2520An%2520and%2520Shuicheng%2520Yan%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520in%2520specific%2520scenarios%252C%2520existing%2520foundation%2520agents%2520still%250Astruggle%2520to%2520generalize%2520across%2520various%2520virtual%2520scenarios%252C%2520mainly%2520due%2520to%2520the%250Adramatically%2520different%2520encapsulations%2520of%2520environments%2520with%2520manually%2520designed%250Aobservation%2520and%2520action%2520spaces.%2520To%2520handle%2520this%2520issue%252C%2520we%2520propose%2520the%2520General%250AComputer%2520Control%2520%2528GCC%2529%2520setting%2520to%2520restrict%2520foundation%2520agents%2520to%2520interact%2520with%250Asoftware%2520through%2520the%2520most%2520unified%2520and%2520standardized%2520interface%252C%2520i.e.%252C%2520using%250Ascreenshots%2520as%2520input%2520and%2520keyboard%2520and%2520mouse%2520actions%2520as%2520output.%2520We%2520introduce%250ACradle%252C%2520a%2520modular%2520and%2520flexible%2520LMM-powered%2520framework%252C%2520as%2520a%2520preliminary%2520attempt%250Atowards%2520GCC.%2520Enhanced%2520by%2520six%2520key%2520modules%252C%2520Cradle%2520can%2520understand%2520input%250Ascreenshots%2520and%2520output%2520executable%2520code%2520for%2520low-level%2520keyboard%2520and%2520mouse%2520control%250Aafter%2520high-level%2520planning%252C%2520so%2520that%2520Cradle%2520can%2520interact%2520with%2520any%2520software%2520and%250Acomplete%2520long-horizon%2520complex%2520tasks%2520without%2520relying%2520on%2520any%2520built-in%2520APIs.%250AExperimental%2520results%2520show%2520that%2520Cradle%2520exhibits%2520remarkable%2520generalizability%2520and%250Aimpressive%2520performance%2520across%2520four%2520previously%2520unexplored%2520commercial%2520video%250Agames%252C%2520five%2520software%2520applications%252C%2520and%2520a%2520comprehensive%2520benchmark%252C%2520OSWorld.%250ACradle%2520is%2520the%2520first%2520to%2520enable%2520foundation%2520agents%2520to%2520follow%2520the%2520main%2520storyline%250Aand%2520complete%252040-minute-long%2520real%2520missions%2520in%2520the%2520complex%2520AAA%2520game%2520Red%2520Dead%250ARedemption%25202%2520%2528RDR2%2529.%2520Cradle%2520can%2520also%2520create%2520a%2520city%2520of%2520a%2520thousand%2520people%2520in%250ACities%253A%2520Skylines%252C%2520farm%2520and%2520harvest%2520parsnips%2520in%2520Stardew%2520Valley%252C%2520and%2520trade%2520and%250Abargain%2520with%2520a%2520maximal%2520weekly%2520total%2520profit%2520of%252087%2525%2520in%2520Dealer%2527s%2520Life%25202.%2520Cradle%250Acan%2520not%2520only%2520operate%2520daily%2520software%252C%2520like%2520Chrome%252C%2520Outlook%252C%2520and%2520Feishu%252C%2520but%2520also%250Aedit%2520images%2520and%2520videos%2520using%2520Meitu%2520and%2520CapCut.%2520Cradle%2520greatly%2520extends%2520the%2520reach%250Aof%2520foundation%2520agents%2520by%2520enabling%2520the%2520easy%2520conversion%2520of%2520any%2520software%252C%250Aespecially%2520complex%2520games%252C%2520into%2520benchmarks%2520to%2520evaluate%2520agents%2527%2520various%2520abilities%250Aand%2520facilitate%2520further%2520data%2520collection%252C%2520thus%2520paving%2520the%2520way%2520for%2520generalist%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03186v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cradle%3A%20Empowering%20Foundation%20Agents%20Towards%20General%20Computer%20Control&entry.906535625=Weihao%20Tan%20and%20Wentao%20Zhang%20and%20Xinrun%20Xu%20and%20Haochong%20Xia%20and%20Ziluo%20Ding%20and%20Boyu%20Li%20and%20Bohan%20Zhou%20and%20Junpeng%20Yue%20and%20Jiechuan%20Jiang%20and%20Yewen%20Li%20and%20Ruyi%20An%20and%20Molei%20Qin%20and%20Chuqiao%20Zong%20and%20Longtao%20Zheng%20and%20Yujie%20Wu%20and%20Xiaoqiang%20Chai%20and%20Yifei%20Bi%20and%20Tianbao%20Xie%20and%20Pengjie%20Gu%20and%20Xiyun%20Li%20and%20Ceyao%20Zhang%20and%20Long%20Tian%20and%20Chaojie%20Wang%20and%20Xinrun%20Wang%20and%20B%C3%B6rje%20F.%20Karlsson%20and%20Bo%20An%20and%20Shuicheng%20Yan%20and%20Zongqing%20Lu&entry.1292438233=%20%20Despite%20the%20success%20in%20specific%20scenarios%2C%20existing%20foundation%20agents%20still%0Astruggle%20to%20generalize%20across%20various%20virtual%20scenarios%2C%20mainly%20due%20to%20the%0Adramatically%20different%20encapsulations%20of%20environments%20with%20manually%20designed%0Aobservation%20and%20action%20spaces.%20To%20handle%20this%20issue%2C%20we%20propose%20the%20General%0AComputer%20Control%20%28GCC%29%20setting%20to%20restrict%20foundation%20agents%20to%20interact%20with%0Asoftware%20through%20the%20most%20unified%20and%20standardized%20interface%2C%20i.e.%2C%20using%0Ascreenshots%20as%20input%20and%20keyboard%20and%20mouse%20actions%20as%20output.%20We%20introduce%0ACradle%2C%20a%20modular%20and%20flexible%20LMM-powered%20framework%2C%20as%20a%20preliminary%20attempt%0Atowards%20GCC.%20Enhanced%20by%20six%20key%20modules%2C%20Cradle%20can%20understand%20input%0Ascreenshots%20and%20output%20executable%20code%20for%20low-level%20keyboard%20and%20mouse%20control%0Aafter%20high-level%20planning%2C%20so%20that%20Cradle%20can%20interact%20with%20any%20software%20and%0Acomplete%20long-horizon%20complex%20tasks%20without%20relying%20on%20any%20built-in%20APIs.%0AExperimental%20results%20show%20that%20Cradle%20exhibits%20remarkable%20generalizability%20and%0Aimpressive%20performance%20across%20four%20previously%20unexplored%20commercial%20video%0Agames%2C%20five%20software%20applications%2C%20and%20a%20comprehensive%20benchmark%2C%20OSWorld.%0ACradle%20is%20the%20first%20to%20enable%20foundation%20agents%20to%20follow%20the%20main%20storyline%0Aand%20complete%2040-minute-long%20real%20missions%20in%20the%20complex%20AAA%20game%20Red%20Dead%0ARedemption%202%20%28RDR2%29.%20Cradle%20can%20also%20create%20a%20city%20of%20a%20thousand%20people%20in%0ACities%3A%20Skylines%2C%20farm%20and%20harvest%20parsnips%20in%20Stardew%20Valley%2C%20and%20trade%20and%0Abargain%20with%20a%20maximal%20weekly%20total%20profit%20of%2087%25%20in%20Dealer%27s%20Life%202.%20Cradle%0Acan%20not%20only%20operate%20daily%20software%2C%20like%20Chrome%2C%20Outlook%2C%20and%20Feishu%2C%20but%20also%0Aedit%20images%20and%20videos%20using%20Meitu%20and%20CapCut.%20Cradle%20greatly%20extends%20the%20reach%0Aof%20foundation%20agents%20by%20enabling%20the%20easy%20conversion%20of%20any%20software%2C%0Aespecially%20complex%20games%2C%20into%20benchmarks%20to%20evaluate%20agents%27%20various%20abilities%0Aand%20facilitate%20further%20data%20collection%2C%20thus%20paving%20the%20way%20for%20generalist%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03186v3&entry.124074799=Read"},
{"title": "Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs", "author": "Zander W. Blasingame and Chen Liu", "abstract": "  Morphing attacks are an emerging threat to state-of-the-art Face Recognition\n(FR) systems, which aim to create a single image that contains the biometric\ninformation of multiple identities. Diffusion Morphs (DiM) are a recently\nproposed morphing attack that has achieved state-of-the-art performance for\nrepresentation-based morphing attacks. However, none of the existing research\non DiMs have leveraged the iterative nature of DiMs and left the DiM model as a\nblack box, treating it no differently than one would a Generative Adversarial\nNetwork (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy on\nthe iterative sampling process of DiM models which searches for an optimal step\nguided by an identity-based heuristic function. We compare our proposed\nalgorithm against ten other state-of-the-art morphing algorithms using the\nopen-source SYN-MAD 2022 competition dataset. We find that our proposed\nalgorithm is unreasonably effective, fooling all of the tested FR systems with\nan MMPMR of 100%, outperforming all other morphing algorithms compared.\n", "link": "http://arxiv.org/abs/2404.06025v2", "date": "2024-07-02", "relevancy": 2.1251, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5474}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Greedy-DiM%3A%20Greedy%20Algorithms%20for%20Unreasonably%20Effective%20Face%20Morphs&body=Title%3A%20Greedy-DiM%3A%20Greedy%20Algorithms%20for%20Unreasonably%20Effective%20Face%20Morphs%0AAuthor%3A%20Zander%20W.%20Blasingame%20and%20Chen%20Liu%0AAbstract%3A%20%20%20Morphing%20attacks%20are%20an%20emerging%20threat%20to%20state-of-the-art%20Face%20Recognition%0A%28FR%29%20systems%2C%20which%20aim%20to%20create%20a%20single%20image%20that%20contains%20the%20biometric%0Ainformation%20of%20multiple%20identities.%20Diffusion%20Morphs%20%28DiM%29%20are%20a%20recently%0Aproposed%20morphing%20attack%20that%20has%20achieved%20state-of-the-art%20performance%20for%0Arepresentation-based%20morphing%20attacks.%20However%2C%20none%20of%20the%20existing%20research%0Aon%20DiMs%20have%20leveraged%20the%20iterative%20nature%20of%20DiMs%20and%20left%20the%20DiM%20model%20as%20a%0Ablack%20box%2C%20treating%20it%20no%20differently%20than%20one%20would%20a%20Generative%20Adversarial%0ANetwork%20%28GAN%29%20or%20Varational%20AutoEncoder%20%28VAE%29.%20We%20propose%20a%20greedy%20strategy%20on%0Athe%20iterative%20sampling%20process%20of%20DiM%20models%20which%20searches%20for%20an%20optimal%20step%0Aguided%20by%20an%20identity-based%20heuristic%20function.%20We%20compare%20our%20proposed%0Aalgorithm%20against%20ten%20other%20state-of-the-art%20morphing%20algorithms%20using%20the%0Aopen-source%20SYN-MAD%202022%20competition%20dataset.%20We%20find%20that%20our%20proposed%0Aalgorithm%20is%20unreasonably%20effective%2C%20fooling%20all%20of%20the%20tested%20FR%20systems%20with%0Aan%20MMPMR%20of%20100%25%2C%20outperforming%20all%20other%20morphing%20algorithms%20compared.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreedy-DiM%253A%2520Greedy%2520Algorithms%2520for%2520Unreasonably%2520Effective%2520Face%2520Morphs%26entry.906535625%3DZander%2520W.%2520Blasingame%2520and%2520Chen%2520Liu%26entry.1292438233%3D%2520%2520Morphing%2520attacks%2520are%2520an%2520emerging%2520threat%2520to%2520state-of-the-art%2520Face%2520Recognition%250A%2528FR%2529%2520systems%252C%2520which%2520aim%2520to%2520create%2520a%2520single%2520image%2520that%2520contains%2520the%2520biometric%250Ainformation%2520of%2520multiple%2520identities.%2520Diffusion%2520Morphs%2520%2528DiM%2529%2520are%2520a%2520recently%250Aproposed%2520morphing%2520attack%2520that%2520has%2520achieved%2520state-of-the-art%2520performance%2520for%250Arepresentation-based%2520morphing%2520attacks.%2520However%252C%2520none%2520of%2520the%2520existing%2520research%250Aon%2520DiMs%2520have%2520leveraged%2520the%2520iterative%2520nature%2520of%2520DiMs%2520and%2520left%2520the%2520DiM%2520model%2520as%2520a%250Ablack%2520box%252C%2520treating%2520it%2520no%2520differently%2520than%2520one%2520would%2520a%2520Generative%2520Adversarial%250ANetwork%2520%2528GAN%2529%2520or%2520Varational%2520AutoEncoder%2520%2528VAE%2529.%2520We%2520propose%2520a%2520greedy%2520strategy%2520on%250Athe%2520iterative%2520sampling%2520process%2520of%2520DiM%2520models%2520which%2520searches%2520for%2520an%2520optimal%2520step%250Aguided%2520by%2520an%2520identity-based%2520heuristic%2520function.%2520We%2520compare%2520our%2520proposed%250Aalgorithm%2520against%2520ten%2520other%2520state-of-the-art%2520morphing%2520algorithms%2520using%2520the%250Aopen-source%2520SYN-MAD%25202022%2520competition%2520dataset.%2520We%2520find%2520that%2520our%2520proposed%250Aalgorithm%2520is%2520unreasonably%2520effective%252C%2520fooling%2520all%2520of%2520the%2520tested%2520FR%2520systems%2520with%250Aan%2520MMPMR%2520of%2520100%2525%252C%2520outperforming%2520all%2520other%2520morphing%2520algorithms%2520compared.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Greedy-DiM%3A%20Greedy%20Algorithms%20for%20Unreasonably%20Effective%20Face%20Morphs&entry.906535625=Zander%20W.%20Blasingame%20and%20Chen%20Liu&entry.1292438233=%20%20Morphing%20attacks%20are%20an%20emerging%20threat%20to%20state-of-the-art%20Face%20Recognition%0A%28FR%29%20systems%2C%20which%20aim%20to%20create%20a%20single%20image%20that%20contains%20the%20biometric%0Ainformation%20of%20multiple%20identities.%20Diffusion%20Morphs%20%28DiM%29%20are%20a%20recently%0Aproposed%20morphing%20attack%20that%20has%20achieved%20state-of-the-art%20performance%20for%0Arepresentation-based%20morphing%20attacks.%20However%2C%20none%20of%20the%20existing%20research%0Aon%20DiMs%20have%20leveraged%20the%20iterative%20nature%20of%20DiMs%20and%20left%20the%20DiM%20model%20as%20a%0Ablack%20box%2C%20treating%20it%20no%20differently%20than%20one%20would%20a%20Generative%20Adversarial%0ANetwork%20%28GAN%29%20or%20Varational%20AutoEncoder%20%28VAE%29.%20We%20propose%20a%20greedy%20strategy%20on%0Athe%20iterative%20sampling%20process%20of%20DiM%20models%20which%20searches%20for%20an%20optimal%20step%0Aguided%20by%20an%20identity-based%20heuristic%20function.%20We%20compare%20our%20proposed%0Aalgorithm%20against%20ten%20other%20state-of-the-art%20morphing%20algorithms%20using%20the%0Aopen-source%20SYN-MAD%202022%20competition%20dataset.%20We%20find%20that%20our%20proposed%0Aalgorithm%20is%20unreasonably%20effective%2C%20fooling%20all%20of%20the%20tested%20FR%20systems%20with%0Aan%20MMPMR%20of%20100%25%2C%20outperforming%20all%20other%20morphing%20algorithms%20compared.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06025v2&entry.124074799=Read"},
{"title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention", "author": "Huiqiang Jiang and Yucheng Li and Chengruidong Zhang and Qianhui Wu and Xufang Luo and Surin Ahn and Zhenhua Han and Amir H. Abdi and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu", "abstract": "  The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.\n", "link": "http://arxiv.org/abs/2407.02490v1", "date": "2024-07-02", "relevancy": 2.1158, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5487}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MInference%201.0%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20LLMs%20via%0A%20%20Dynamic%20Sparse%20Attention&body=Title%3A%20MInference%201.0%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20LLMs%20via%0A%20%20Dynamic%20Sparse%20Attention%0AAuthor%3A%20Huiqiang%20Jiang%20and%20Yucheng%20Li%20and%20Chengruidong%20Zhang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Zhenhua%20Han%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Chin-Yew%20Lin%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20The%20computational%20challenges%20of%20Large%20Language%20Model%20%28LLM%29%20inference%20remain%20a%0Asignificant%20barrier%20to%20their%20widespread%20deployment%2C%20especially%20as%20prompt%0Alengths%20continue%20to%20increase.%20Due%20to%20the%20quadratic%20complexity%20of%20the%20attention%0Acomputation%2C%20it%20takes%2030%20minutes%20for%20an%208B%20LLM%20to%20process%20a%20prompt%20of%201M%20tokens%0A%28i.e.%2C%20the%20pre-filling%20stage%29%20on%20a%20single%20A100%20GPU.%20Existing%20methods%20for%0Aspeeding%20up%20prefilling%20often%20fail%20to%20maintain%20acceptable%20accuracy%20or%20efficiency%0Awhen%20applied%20to%20long-context%20LLMs.%20To%20address%20this%20gap%2C%20we%20introduce%20MInference%0A%28Milliontokens%20Inference%29%2C%20a%20sparse%20calculation%20method%20designed%20to%20accelerate%0Apre-filling%20of%20long-sequence%20processing.%20Specifically%2C%20we%20identify%20three%20unique%0Apatterns%20in%20long-context%20attention%20matrices-the%20A-shape%2C%20Vertical-Slash%2C%20and%0ABlock-Sparsethat%20can%20be%20leveraged%20for%20efficient%20sparse%20computation%20on%20GPUs.%20We%0Adetermine%20the%20optimal%20pattern%20for%20each%20attention%20head%20offline%20and%20dynamically%0Abuild%20sparse%20indices%20based%20on%20the%20assigned%20pattern%20during%20inference.%20With%20the%0Apattern%20and%20sparse%20indices%2C%20we%20perform%20efficient%20sparse%20attention%20calculations%0Avia%20our%20optimized%20GPU%20kernels%20to%20significantly%20reduce%20the%20latency%20in%20the%0Apre-filling%20stage%20of%20long-context%20LLMs.%20Our%20proposed%20technique%20can%20be%20directly%0Aapplied%20to%20existing%20LLMs%20without%20any%20modifications%20to%20the%20pre-training%20setup%20or%0Aadditional%20fine-tuning.%20By%20evaluating%20on%20a%20wide%20range%20of%20downstream%20tasks%2C%0Aincluding%20InfiniteBench%2C%20RULER%2C%20PG-19%2C%20and%20Needle%20In%20A%20Haystack%2C%20and%20models%0Aincluding%20LLaMA-3-1M%2C%20GLM4-1M%2C%20Yi-200K%2C%20Phi-3-128K%2C%20and%20Qwen2-128K%2C%20we%0Ademonstrate%20that%20MInference%20effectively%20reduces%20inference%20latency%20by%20up%20to%2010x%0Afor%20pre-filling%20on%20an%20A100%2C%20while%20maintaining%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//aka.ms/MInference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMInference%25201.0%253A%2520Accelerating%2520Pre-filling%2520for%2520Long-Context%2520LLMs%2520via%250A%2520%2520Dynamic%2520Sparse%2520Attention%26entry.906535625%3DHuiqiang%2520Jiang%2520and%2520Yucheng%2520Li%2520and%2520Chengruidong%2520Zhang%2520and%2520Qianhui%2520Wu%2520and%2520Xufang%2520Luo%2520and%2520Surin%2520Ahn%2520and%2520Zhenhua%2520Han%2520and%2520Amir%2520H.%2520Abdi%2520and%2520Dongsheng%2520Li%2520and%2520Chin-Yew%2520Lin%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520The%2520computational%2520challenges%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520inference%2520remain%2520a%250Asignificant%2520barrier%2520to%2520their%2520widespread%2520deployment%252C%2520especially%2520as%2520prompt%250Alengths%2520continue%2520to%2520increase.%2520Due%2520to%2520the%2520quadratic%2520complexity%2520of%2520the%2520attention%250Acomputation%252C%2520it%2520takes%252030%2520minutes%2520for%2520an%25208B%2520LLM%2520to%2520process%2520a%2520prompt%2520of%25201M%2520tokens%250A%2528i.e.%252C%2520the%2520pre-filling%2520stage%2529%2520on%2520a%2520single%2520A100%2520GPU.%2520Existing%2520methods%2520for%250Aspeeding%2520up%2520prefilling%2520often%2520fail%2520to%2520maintain%2520acceptable%2520accuracy%2520or%2520efficiency%250Awhen%2520applied%2520to%2520long-context%2520LLMs.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520MInference%250A%2528Milliontokens%2520Inference%2529%252C%2520a%2520sparse%2520calculation%2520method%2520designed%2520to%2520accelerate%250Apre-filling%2520of%2520long-sequence%2520processing.%2520Specifically%252C%2520we%2520identify%2520three%2520unique%250Apatterns%2520in%2520long-context%2520attention%2520matrices-the%2520A-shape%252C%2520Vertical-Slash%252C%2520and%250ABlock-Sparsethat%2520can%2520be%2520leveraged%2520for%2520efficient%2520sparse%2520computation%2520on%2520GPUs.%2520We%250Adetermine%2520the%2520optimal%2520pattern%2520for%2520each%2520attention%2520head%2520offline%2520and%2520dynamically%250Abuild%2520sparse%2520indices%2520based%2520on%2520the%2520assigned%2520pattern%2520during%2520inference.%2520With%2520the%250Apattern%2520and%2520sparse%2520indices%252C%2520we%2520perform%2520efficient%2520sparse%2520attention%2520calculations%250Avia%2520our%2520optimized%2520GPU%2520kernels%2520to%2520significantly%2520reduce%2520the%2520latency%2520in%2520the%250Apre-filling%2520stage%2520of%2520long-context%2520LLMs.%2520Our%2520proposed%2520technique%2520can%2520be%2520directly%250Aapplied%2520to%2520existing%2520LLMs%2520without%2520any%2520modifications%2520to%2520the%2520pre-training%2520setup%2520or%250Aadditional%2520fine-tuning.%2520By%2520evaluating%2520on%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%252C%250Aincluding%2520InfiniteBench%252C%2520RULER%252C%2520PG-19%252C%2520and%2520Needle%2520In%2520A%2520Haystack%252C%2520and%2520models%250Aincluding%2520LLaMA-3-1M%252C%2520GLM4-1M%252C%2520Yi-200K%252C%2520Phi-3-128K%252C%2520and%2520Qwen2-128K%252C%2520we%250Ademonstrate%2520that%2520MInference%2520effectively%2520reduces%2520inference%2520latency%2520by%2520up%2520to%252010x%250Afor%2520pre-filling%2520on%2520an%2520A100%252C%2520while%2520maintaining%2520accuracy.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//aka.ms/MInference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MInference%201.0%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20LLMs%20via%0A%20%20Dynamic%20Sparse%20Attention&entry.906535625=Huiqiang%20Jiang%20and%20Yucheng%20Li%20and%20Chengruidong%20Zhang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Zhenhua%20Han%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Chin-Yew%20Lin%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20The%20computational%20challenges%20of%20Large%20Language%20Model%20%28LLM%29%20inference%20remain%20a%0Asignificant%20barrier%20to%20their%20widespread%20deployment%2C%20especially%20as%20prompt%0Alengths%20continue%20to%20increase.%20Due%20to%20the%20quadratic%20complexity%20of%20the%20attention%0Acomputation%2C%20it%20takes%2030%20minutes%20for%20an%208B%20LLM%20to%20process%20a%20prompt%20of%201M%20tokens%0A%28i.e.%2C%20the%20pre-filling%20stage%29%20on%20a%20single%20A100%20GPU.%20Existing%20methods%20for%0Aspeeding%20up%20prefilling%20often%20fail%20to%20maintain%20acceptable%20accuracy%20or%20efficiency%0Awhen%20applied%20to%20long-context%20LLMs.%20To%20address%20this%20gap%2C%20we%20introduce%20MInference%0A%28Milliontokens%20Inference%29%2C%20a%20sparse%20calculation%20method%20designed%20to%20accelerate%0Apre-filling%20of%20long-sequence%20processing.%20Specifically%2C%20we%20identify%20three%20unique%0Apatterns%20in%20long-context%20attention%20matrices-the%20A-shape%2C%20Vertical-Slash%2C%20and%0ABlock-Sparsethat%20can%20be%20leveraged%20for%20efficient%20sparse%20computation%20on%20GPUs.%20We%0Adetermine%20the%20optimal%20pattern%20for%20each%20attention%20head%20offline%20and%20dynamically%0Abuild%20sparse%20indices%20based%20on%20the%20assigned%20pattern%20during%20inference.%20With%20the%0Apattern%20and%20sparse%20indices%2C%20we%20perform%20efficient%20sparse%20attention%20calculations%0Avia%20our%20optimized%20GPU%20kernels%20to%20significantly%20reduce%20the%20latency%20in%20the%0Apre-filling%20stage%20of%20long-context%20LLMs.%20Our%20proposed%20technique%20can%20be%20directly%0Aapplied%20to%20existing%20LLMs%20without%20any%20modifications%20to%20the%20pre-training%20setup%20or%0Aadditional%20fine-tuning.%20By%20evaluating%20on%20a%20wide%20range%20of%20downstream%20tasks%2C%0Aincluding%20InfiniteBench%2C%20RULER%2C%20PG-19%2C%20and%20Needle%20In%20A%20Haystack%2C%20and%20models%0Aincluding%20LLaMA-3-1M%2C%20GLM4-1M%2C%20Yi-200K%2C%20Phi-3-128K%2C%20and%20Qwen2-128K%2C%20we%0Ademonstrate%20that%20MInference%20effectively%20reduces%20inference%20latency%20by%20up%20to%2010x%0Afor%20pre-filling%20on%20an%20A100%2C%20while%20maintaining%20accuracy.%20Our%20code%20is%20available%0Aat%20https%3A//aka.ms/MInference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02490v1&entry.124074799=Read"},
{"title": "SUPER: Seated Upper Body Pose Estimation using mmWave Radars", "author": "Bo Zhang and Zimeng Zhou and Boyu Jiang and Rong Zheng", "abstract": "  In industrial countries, adults spend a considerable amount of time sedentary\neach day at work, driving and during activities of daily living. Characterizing\nthe seated upper body human poses using mmWave radars is an important, yet\nunder-studied topic with many applications in human-machine interaction,\ntransportation and road safety. In this work, we devise SUPER, a framework for\nseated upper body human pose estimation that utilizes dual-mmWave radars in\nclose proximity. A novel masking algorithm is proposed to coherently fuse data\nfrom the radars to generate intensity and Doppler point clouds with\ncomplementary information for high-motion but small radar cross section areas\n(e.g., upper extremities) and low-motion but large RCS areas (e.g. torso). A\nlightweight neural network extracts both global and local features of upper\nbody and output pose parameters for the Skinned Multi-Person Linear (SMPL)\nmodel. Extensive leave-one-subject-out experiments on various motion sequences\nfrom multiple subjects show that SUPER outperforms a state-of-the-art baseline\nmethod by 30 -- 184%. We also demonstrate its utility in a simple downstream\ntask for hand-object interaction.\n", "link": "http://arxiv.org/abs/2407.02455v1", "date": "2024-07-02", "relevancy": 2.109, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5653}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5198}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUPER%3A%20Seated%20Upper%20Body%20Pose%20Estimation%20using%20mmWave%20Radars&body=Title%3A%20SUPER%3A%20Seated%20Upper%20Body%20Pose%20Estimation%20using%20mmWave%20Radars%0AAuthor%3A%20Bo%20Zhang%20and%20Zimeng%20Zhou%20and%20Boyu%20Jiang%20and%20Rong%20Zheng%0AAbstract%3A%20%20%20In%20industrial%20countries%2C%20adults%20spend%20a%20considerable%20amount%20of%20time%20sedentary%0Aeach%20day%20at%20work%2C%20driving%20and%20during%20activities%20of%20daily%20living.%20Characterizing%0Athe%20seated%20upper%20body%20human%20poses%20using%20mmWave%20radars%20is%20an%20important%2C%20yet%0Aunder-studied%20topic%20with%20many%20applications%20in%20human-machine%20interaction%2C%0Atransportation%20and%20road%20safety.%20In%20this%20work%2C%20we%20devise%20SUPER%2C%20a%20framework%20for%0Aseated%20upper%20body%20human%20pose%20estimation%20that%20utilizes%20dual-mmWave%20radars%20in%0Aclose%20proximity.%20A%20novel%20masking%20algorithm%20is%20proposed%20to%20coherently%20fuse%20data%0Afrom%20the%20radars%20to%20generate%20intensity%20and%20Doppler%20point%20clouds%20with%0Acomplementary%20information%20for%20high-motion%20but%20small%20radar%20cross%20section%20areas%0A%28e.g.%2C%20upper%20extremities%29%20and%20low-motion%20but%20large%20RCS%20areas%20%28e.g.%20torso%29.%20A%0Alightweight%20neural%20network%20extracts%20both%20global%20and%20local%20features%20of%20upper%0Abody%20and%20output%20pose%20parameters%20for%20the%20Skinned%20Multi-Person%20Linear%20%28SMPL%29%0Amodel.%20Extensive%20leave-one-subject-out%20experiments%20on%20various%20motion%20sequences%0Afrom%20multiple%20subjects%20show%20that%20SUPER%20outperforms%20a%20state-of-the-art%20baseline%0Amethod%20by%2030%20--%20184%25.%20We%20also%20demonstrate%20its%20utility%20in%20a%20simple%20downstream%0Atask%20for%20hand-object%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUPER%253A%2520Seated%2520Upper%2520Body%2520Pose%2520Estimation%2520using%2520mmWave%2520Radars%26entry.906535625%3DBo%2520Zhang%2520and%2520Zimeng%2520Zhou%2520and%2520Boyu%2520Jiang%2520and%2520Rong%2520Zheng%26entry.1292438233%3D%2520%2520In%2520industrial%2520countries%252C%2520adults%2520spend%2520a%2520considerable%2520amount%2520of%2520time%2520sedentary%250Aeach%2520day%2520at%2520work%252C%2520driving%2520and%2520during%2520activities%2520of%2520daily%2520living.%2520Characterizing%250Athe%2520seated%2520upper%2520body%2520human%2520poses%2520using%2520mmWave%2520radars%2520is%2520an%2520important%252C%2520yet%250Aunder-studied%2520topic%2520with%2520many%2520applications%2520in%2520human-machine%2520interaction%252C%250Atransportation%2520and%2520road%2520safety.%2520In%2520this%2520work%252C%2520we%2520devise%2520SUPER%252C%2520a%2520framework%2520for%250Aseated%2520upper%2520body%2520human%2520pose%2520estimation%2520that%2520utilizes%2520dual-mmWave%2520radars%2520in%250Aclose%2520proximity.%2520A%2520novel%2520masking%2520algorithm%2520is%2520proposed%2520to%2520coherently%2520fuse%2520data%250Afrom%2520the%2520radars%2520to%2520generate%2520intensity%2520and%2520Doppler%2520point%2520clouds%2520with%250Acomplementary%2520information%2520for%2520high-motion%2520but%2520small%2520radar%2520cross%2520section%2520areas%250A%2528e.g.%252C%2520upper%2520extremities%2529%2520and%2520low-motion%2520but%2520large%2520RCS%2520areas%2520%2528e.g.%2520torso%2529.%2520A%250Alightweight%2520neural%2520network%2520extracts%2520both%2520global%2520and%2520local%2520features%2520of%2520upper%250Abody%2520and%2520output%2520pose%2520parameters%2520for%2520the%2520Skinned%2520Multi-Person%2520Linear%2520%2528SMPL%2529%250Amodel.%2520Extensive%2520leave-one-subject-out%2520experiments%2520on%2520various%2520motion%2520sequences%250Afrom%2520multiple%2520subjects%2520show%2520that%2520SUPER%2520outperforms%2520a%2520state-of-the-art%2520baseline%250Amethod%2520by%252030%2520--%2520184%2525.%2520We%2520also%2520demonstrate%2520its%2520utility%2520in%2520a%2520simple%2520downstream%250Atask%2520for%2520hand-object%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUPER%3A%20Seated%20Upper%20Body%20Pose%20Estimation%20using%20mmWave%20Radars&entry.906535625=Bo%20Zhang%20and%20Zimeng%20Zhou%20and%20Boyu%20Jiang%20and%20Rong%20Zheng&entry.1292438233=%20%20In%20industrial%20countries%2C%20adults%20spend%20a%20considerable%20amount%20of%20time%20sedentary%0Aeach%20day%20at%20work%2C%20driving%20and%20during%20activities%20of%20daily%20living.%20Characterizing%0Athe%20seated%20upper%20body%20human%20poses%20using%20mmWave%20radars%20is%20an%20important%2C%20yet%0Aunder-studied%20topic%20with%20many%20applications%20in%20human-machine%20interaction%2C%0Atransportation%20and%20road%20safety.%20In%20this%20work%2C%20we%20devise%20SUPER%2C%20a%20framework%20for%0Aseated%20upper%20body%20human%20pose%20estimation%20that%20utilizes%20dual-mmWave%20radars%20in%0Aclose%20proximity.%20A%20novel%20masking%20algorithm%20is%20proposed%20to%20coherently%20fuse%20data%0Afrom%20the%20radars%20to%20generate%20intensity%20and%20Doppler%20point%20clouds%20with%0Acomplementary%20information%20for%20high-motion%20but%20small%20radar%20cross%20section%20areas%0A%28e.g.%2C%20upper%20extremities%29%20and%20low-motion%20but%20large%20RCS%20areas%20%28e.g.%20torso%29.%20A%0Alightweight%20neural%20network%20extracts%20both%20global%20and%20local%20features%20of%20upper%0Abody%20and%20output%20pose%20parameters%20for%20the%20Skinned%20Multi-Person%20Linear%20%28SMPL%29%0Amodel.%20Extensive%20leave-one-subject-out%20experiments%20on%20various%20motion%20sequences%0Afrom%20multiple%20subjects%20show%20that%20SUPER%20outperforms%20a%20state-of-the-art%20baseline%0Amethod%20by%2030%20--%20184%25.%20We%20also%20demonstrate%20its%20utility%20in%20a%20simple%20downstream%0Atask%20for%20hand-object%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02455v1&entry.124074799=Read"},
{"title": "A Pattern Language for Machine Learning Tasks", "author": "Benjamin Rodatz and Ian Fan and Tuomas Laakkonen and Neil John Ortega and Thomas Hoffman and Vincent Wang-Mascianica", "abstract": "  Idealised as universal approximators, learners such as neural networks can be\nviewed as \"variable functions\" that may become one of a range of concrete\nfunctions after training. In the same way that equations constrain the possible\nvalues of variables in algebra, we may view objective functions as constraints\non the behaviour of learners. We extract the equivalences perfectly optimised\nobjective functions impose, calling them \"tasks\". For these tasks, we develop a\nformal graphical language that allows us to: (1) separate the core tasks of a\nbehaviour from its implementation details; (2) reason about and design\nbehaviours model-agnostically; and (3) simply describe and unify approaches in\nmachine learning across domains.\n  As proof-of-concept, we design a novel task that enables converting\nclassifiers into generative models we call \"manipulators\", which we implement\nby directly translating task specifications into code. The resulting models\nexhibit capabilities such as style transfer and interpretable latent-space\nediting, without the need for custom architectures, adversarial training or\nrandom sampling. We formally relate the behaviour of manipulators to GANs, and\nempirically demonstrate their competitive performance with VAEs. We report on\nexperiments across vision and language domains aiming to characterise\nmanipulators as approximate Bayesian inversions of discriminative classifiers.\n", "link": "http://arxiv.org/abs/2407.02424v1", "date": "2024-07-02", "relevancy": 2.1019, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5322}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pattern%20Language%20for%20Machine%20Learning%20Tasks&body=Title%3A%20A%20Pattern%20Language%20for%20Machine%20Learning%20Tasks%0AAuthor%3A%20Benjamin%20Rodatz%20and%20Ian%20Fan%20and%20Tuomas%20Laakkonen%20and%20Neil%20John%20Ortega%20and%20Thomas%20Hoffman%20and%20Vincent%20Wang-Mascianica%0AAbstract%3A%20%20%20Idealised%20as%20universal%20approximators%2C%20learners%20such%20as%20neural%20networks%20can%20be%0Aviewed%20as%20%22variable%20functions%22%20that%20may%20become%20one%20of%20a%20range%20of%20concrete%0Afunctions%20after%20training.%20In%20the%20same%20way%20that%20equations%20constrain%20the%20possible%0Avalues%20of%20variables%20in%20algebra%2C%20we%20may%20view%20objective%20functions%20as%20constraints%0Aon%20the%20behaviour%20of%20learners.%20We%20extract%20the%20equivalences%20perfectly%20optimised%0Aobjective%20functions%20impose%2C%20calling%20them%20%22tasks%22.%20For%20these%20tasks%2C%20we%20develop%20a%0Aformal%20graphical%20language%20that%20allows%20us%20to%3A%20%281%29%20separate%20the%20core%20tasks%20of%20a%0Abehaviour%20from%20its%20implementation%20details%3B%20%282%29%20reason%20about%20and%20design%0Abehaviours%20model-agnostically%3B%20and%20%283%29%20simply%20describe%20and%20unify%20approaches%20in%0Amachine%20learning%20across%20domains.%0A%20%20As%20proof-of-concept%2C%20we%20design%20a%20novel%20task%20that%20enables%20converting%0Aclassifiers%20into%20generative%20models%20we%20call%20%22manipulators%22%2C%20which%20we%20implement%0Aby%20directly%20translating%20task%20specifications%20into%20code.%20The%20resulting%20models%0Aexhibit%20capabilities%20such%20as%20style%20transfer%20and%20interpretable%20latent-space%0Aediting%2C%20without%20the%20need%20for%20custom%20architectures%2C%20adversarial%20training%20or%0Arandom%20sampling.%20We%20formally%20relate%20the%20behaviour%20of%20manipulators%20to%20GANs%2C%20and%0Aempirically%20demonstrate%20their%20competitive%20performance%20with%20VAEs.%20We%20report%20on%0Aexperiments%20across%20vision%20and%20language%20domains%20aiming%20to%20characterise%0Amanipulators%20as%20approximate%20Bayesian%20inversions%20of%20discriminative%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pattern%2520Language%2520for%2520Machine%2520Learning%2520Tasks%26entry.906535625%3DBenjamin%2520Rodatz%2520and%2520Ian%2520Fan%2520and%2520Tuomas%2520Laakkonen%2520and%2520Neil%2520John%2520Ortega%2520and%2520Thomas%2520Hoffman%2520and%2520Vincent%2520Wang-Mascianica%26entry.1292438233%3D%2520%2520Idealised%2520as%2520universal%2520approximators%252C%2520learners%2520such%2520as%2520neural%2520networks%2520can%2520be%250Aviewed%2520as%2520%2522variable%2520functions%2522%2520that%2520may%2520become%2520one%2520of%2520a%2520range%2520of%2520concrete%250Afunctions%2520after%2520training.%2520In%2520the%2520same%2520way%2520that%2520equations%2520constrain%2520the%2520possible%250Avalues%2520of%2520variables%2520in%2520algebra%252C%2520we%2520may%2520view%2520objective%2520functions%2520as%2520constraints%250Aon%2520the%2520behaviour%2520of%2520learners.%2520We%2520extract%2520the%2520equivalences%2520perfectly%2520optimised%250Aobjective%2520functions%2520impose%252C%2520calling%2520them%2520%2522tasks%2522.%2520For%2520these%2520tasks%252C%2520we%2520develop%2520a%250Aformal%2520graphical%2520language%2520that%2520allows%2520us%2520to%253A%2520%25281%2529%2520separate%2520the%2520core%2520tasks%2520of%2520a%250Abehaviour%2520from%2520its%2520implementation%2520details%253B%2520%25282%2529%2520reason%2520about%2520and%2520design%250Abehaviours%2520model-agnostically%253B%2520and%2520%25283%2529%2520simply%2520describe%2520and%2520unify%2520approaches%2520in%250Amachine%2520learning%2520across%2520domains.%250A%2520%2520As%2520proof-of-concept%252C%2520we%2520design%2520a%2520novel%2520task%2520that%2520enables%2520converting%250Aclassifiers%2520into%2520generative%2520models%2520we%2520call%2520%2522manipulators%2522%252C%2520which%2520we%2520implement%250Aby%2520directly%2520translating%2520task%2520specifications%2520into%2520code.%2520The%2520resulting%2520models%250Aexhibit%2520capabilities%2520such%2520as%2520style%2520transfer%2520and%2520interpretable%2520latent-space%250Aediting%252C%2520without%2520the%2520need%2520for%2520custom%2520architectures%252C%2520adversarial%2520training%2520or%250Arandom%2520sampling.%2520We%2520formally%2520relate%2520the%2520behaviour%2520of%2520manipulators%2520to%2520GANs%252C%2520and%250Aempirically%2520demonstrate%2520their%2520competitive%2520performance%2520with%2520VAEs.%2520We%2520report%2520on%250Aexperiments%2520across%2520vision%2520and%2520language%2520domains%2520aiming%2520to%2520characterise%250Amanipulators%2520as%2520approximate%2520Bayesian%2520inversions%2520of%2520discriminative%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pattern%20Language%20for%20Machine%20Learning%20Tasks&entry.906535625=Benjamin%20Rodatz%20and%20Ian%20Fan%20and%20Tuomas%20Laakkonen%20and%20Neil%20John%20Ortega%20and%20Thomas%20Hoffman%20and%20Vincent%20Wang-Mascianica&entry.1292438233=%20%20Idealised%20as%20universal%20approximators%2C%20learners%20such%20as%20neural%20networks%20can%20be%0Aviewed%20as%20%22variable%20functions%22%20that%20may%20become%20one%20of%20a%20range%20of%20concrete%0Afunctions%20after%20training.%20In%20the%20same%20way%20that%20equations%20constrain%20the%20possible%0Avalues%20of%20variables%20in%20algebra%2C%20we%20may%20view%20objective%20functions%20as%20constraints%0Aon%20the%20behaviour%20of%20learners.%20We%20extract%20the%20equivalences%20perfectly%20optimised%0Aobjective%20functions%20impose%2C%20calling%20them%20%22tasks%22.%20For%20these%20tasks%2C%20we%20develop%20a%0Aformal%20graphical%20language%20that%20allows%20us%20to%3A%20%281%29%20separate%20the%20core%20tasks%20of%20a%0Abehaviour%20from%20its%20implementation%20details%3B%20%282%29%20reason%20about%20and%20design%0Abehaviours%20model-agnostically%3B%20and%20%283%29%20simply%20describe%20and%20unify%20approaches%20in%0Amachine%20learning%20across%20domains.%0A%20%20As%20proof-of-concept%2C%20we%20design%20a%20novel%20task%20that%20enables%20converting%0Aclassifiers%20into%20generative%20models%20we%20call%20%22manipulators%22%2C%20which%20we%20implement%0Aby%20directly%20translating%20task%20specifications%20into%20code.%20The%20resulting%20models%0Aexhibit%20capabilities%20such%20as%20style%20transfer%20and%20interpretable%20latent-space%0Aediting%2C%20without%20the%20need%20for%20custom%20architectures%2C%20adversarial%20training%20or%0Arandom%20sampling.%20We%20formally%20relate%20the%20behaviour%20of%20manipulators%20to%20GANs%2C%20and%0Aempirically%20demonstrate%20their%20competitive%20performance%20with%20VAEs.%20We%20report%20on%0Aexperiments%20across%20vision%20and%20language%20domains%20aiming%20to%20characterise%0Amanipulators%20as%20approximate%20Bayesian%20inversions%20of%20discriminative%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02424v1&entry.124074799=Read"},
{"title": "GSQA: An End-to-End Model for Generative Spoken Question Answering", "author": "Min-Han Shih and Ho-Lam Chung and Yu-Chi Pai and Ming-Hao Hsu and Guan-Ting Lin and Shang-Wen Li and Hung-yi Lee", "abstract": "  In recent advancements in spoken question answering (QA), end-to-end models\nhave made significant strides. However, previous research has primarily focused\non extractive span selection. While this extractive-based approach is effective\nwhen answers are present directly within the input, it falls short in\naddressing abstractive questions, where answers are not directly extracted but\ninferred from the given information. To bridge this gap, we introduce the first\nend-to-end Generative Spoken Question Answering (GSQA) model that empowers the\nsystem to engage in abstractive reasoning. The challenge in training our GSQA\nmodel lies in the absence of a spoken abstractive QA dataset. We propose using\ntext models for initialization and leveraging the extractive QA dataset to\ntransfer knowledge from the text generative model to the spoken generative\nmodel. Experimental results indicate that our model surpasses the previous\nextractive model by 3% on extractive QA datasets. Furthermore, the GSQA model\nhas only been fine-tuned on the spoken extractive QA dataset. Despite not\nhaving seen any spoken abstractive QA data, it can still closely match the\nperformance of the cascade model. In conclusion, our GSQA model shows the\npotential to generalize to a broad spectrum of questions, thus further\nexpanding the spoken question answering capabilities of abstractive QA. Our\ncode is available at https://voidful.github.io/GSQA\n", "link": "http://arxiv.org/abs/2312.09781v3", "date": "2024-07-02", "relevancy": 2.0999, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5365}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5326}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSQA%3A%20An%20End-to-End%20Model%20for%20Generative%20Spoken%20Question%20Answering&body=Title%3A%20GSQA%3A%20An%20End-to-End%20Model%20for%20Generative%20Spoken%20Question%20Answering%0AAuthor%3A%20Min-Han%20Shih%20and%20Ho-Lam%20Chung%20and%20Yu-Chi%20Pai%20and%20Ming-Hao%20Hsu%20and%20Guan-Ting%20Lin%20and%20Shang-Wen%20Li%20and%20Hung-yi%20Lee%0AAbstract%3A%20%20%20In%20recent%20advancements%20in%20spoken%20question%20answering%20%28QA%29%2C%20end-to-end%20models%0Ahave%20made%20significant%20strides.%20However%2C%20previous%20research%20has%20primarily%20focused%0Aon%20extractive%20span%20selection.%20While%20this%20extractive-based%20approach%20is%20effective%0Awhen%20answers%20are%20present%20directly%20within%20the%20input%2C%20it%20falls%20short%20in%0Aaddressing%20abstractive%20questions%2C%20where%20answers%20are%20not%20directly%20extracted%20but%0Ainferred%20from%20the%20given%20information.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20first%0Aend-to-end%20Generative%20Spoken%20Question%20Answering%20%28GSQA%29%20model%20that%20empowers%20the%0Asystem%20to%20engage%20in%20abstractive%20reasoning.%20The%20challenge%20in%20training%20our%20GSQA%0Amodel%20lies%20in%20the%20absence%20of%20a%20spoken%20abstractive%20QA%20dataset.%20We%20propose%20using%0Atext%20models%20for%20initialization%20and%20leveraging%20the%20extractive%20QA%20dataset%20to%0Atransfer%20knowledge%20from%20the%20text%20generative%20model%20to%20the%20spoken%20generative%0Amodel.%20Experimental%20results%20indicate%20that%20our%20model%20surpasses%20the%20previous%0Aextractive%20model%20by%203%25%20on%20extractive%20QA%20datasets.%20Furthermore%2C%20the%20GSQA%20model%0Ahas%20only%20been%20fine-tuned%20on%20the%20spoken%20extractive%20QA%20dataset.%20Despite%20not%0Ahaving%20seen%20any%20spoken%20abstractive%20QA%20data%2C%20it%20can%20still%20closely%20match%20the%0Aperformance%20of%20the%20cascade%20model.%20In%20conclusion%2C%20our%20GSQA%20model%20shows%20the%0Apotential%20to%20generalize%20to%20a%20broad%20spectrum%20of%20questions%2C%20thus%20further%0Aexpanding%20the%20spoken%20question%20answering%20capabilities%20of%20abstractive%20QA.%20Our%0Acode%20is%20available%20at%20https%3A//voidful.github.io/GSQA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09781v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSQA%253A%2520An%2520End-to-End%2520Model%2520for%2520Generative%2520Spoken%2520Question%2520Answering%26entry.906535625%3DMin-Han%2520Shih%2520and%2520Ho-Lam%2520Chung%2520and%2520Yu-Chi%2520Pai%2520and%2520Ming-Hao%2520Hsu%2520and%2520Guan-Ting%2520Lin%2520and%2520Shang-Wen%2520Li%2520and%2520Hung-yi%2520Lee%26entry.1292438233%3D%2520%2520In%2520recent%2520advancements%2520in%2520spoken%2520question%2520answering%2520%2528QA%2529%252C%2520end-to-end%2520models%250Ahave%2520made%2520significant%2520strides.%2520However%252C%2520previous%2520research%2520has%2520primarily%2520focused%250Aon%2520extractive%2520span%2520selection.%2520While%2520this%2520extractive-based%2520approach%2520is%2520effective%250Awhen%2520answers%2520are%2520present%2520directly%2520within%2520the%2520input%252C%2520it%2520falls%2520short%2520in%250Aaddressing%2520abstractive%2520questions%252C%2520where%2520answers%2520are%2520not%2520directly%2520extracted%2520but%250Ainferred%2520from%2520the%2520given%2520information.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%2520first%250Aend-to-end%2520Generative%2520Spoken%2520Question%2520Answering%2520%2528GSQA%2529%2520model%2520that%2520empowers%2520the%250Asystem%2520to%2520engage%2520in%2520abstractive%2520reasoning.%2520The%2520challenge%2520in%2520training%2520our%2520GSQA%250Amodel%2520lies%2520in%2520the%2520absence%2520of%2520a%2520spoken%2520abstractive%2520QA%2520dataset.%2520We%2520propose%2520using%250Atext%2520models%2520for%2520initialization%2520and%2520leveraging%2520the%2520extractive%2520QA%2520dataset%2520to%250Atransfer%2520knowledge%2520from%2520the%2520text%2520generative%2520model%2520to%2520the%2520spoken%2520generative%250Amodel.%2520Experimental%2520results%2520indicate%2520that%2520our%2520model%2520surpasses%2520the%2520previous%250Aextractive%2520model%2520by%25203%2525%2520on%2520extractive%2520QA%2520datasets.%2520Furthermore%252C%2520the%2520GSQA%2520model%250Ahas%2520only%2520been%2520fine-tuned%2520on%2520the%2520spoken%2520extractive%2520QA%2520dataset.%2520Despite%2520not%250Ahaving%2520seen%2520any%2520spoken%2520abstractive%2520QA%2520data%252C%2520it%2520can%2520still%2520closely%2520match%2520the%250Aperformance%2520of%2520the%2520cascade%2520model.%2520In%2520conclusion%252C%2520our%2520GSQA%2520model%2520shows%2520the%250Apotential%2520to%2520generalize%2520to%2520a%2520broad%2520spectrum%2520of%2520questions%252C%2520thus%2520further%250Aexpanding%2520the%2520spoken%2520question%2520answering%2520capabilities%2520of%2520abstractive%2520QA.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//voidful.github.io/GSQA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09781v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSQA%3A%20An%20End-to-End%20Model%20for%20Generative%20Spoken%20Question%20Answering&entry.906535625=Min-Han%20Shih%20and%20Ho-Lam%20Chung%20and%20Yu-Chi%20Pai%20and%20Ming-Hao%20Hsu%20and%20Guan-Ting%20Lin%20and%20Shang-Wen%20Li%20and%20Hung-yi%20Lee&entry.1292438233=%20%20In%20recent%20advancements%20in%20spoken%20question%20answering%20%28QA%29%2C%20end-to-end%20models%0Ahave%20made%20significant%20strides.%20However%2C%20previous%20research%20has%20primarily%20focused%0Aon%20extractive%20span%20selection.%20While%20this%20extractive-based%20approach%20is%20effective%0Awhen%20answers%20are%20present%20directly%20within%20the%20input%2C%20it%20falls%20short%20in%0Aaddressing%20abstractive%20questions%2C%20where%20answers%20are%20not%20directly%20extracted%20but%0Ainferred%20from%20the%20given%20information.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20first%0Aend-to-end%20Generative%20Spoken%20Question%20Answering%20%28GSQA%29%20model%20that%20empowers%20the%0Asystem%20to%20engage%20in%20abstractive%20reasoning.%20The%20challenge%20in%20training%20our%20GSQA%0Amodel%20lies%20in%20the%20absence%20of%20a%20spoken%20abstractive%20QA%20dataset.%20We%20propose%20using%0Atext%20models%20for%20initialization%20and%20leveraging%20the%20extractive%20QA%20dataset%20to%0Atransfer%20knowledge%20from%20the%20text%20generative%20model%20to%20the%20spoken%20generative%0Amodel.%20Experimental%20results%20indicate%20that%20our%20model%20surpasses%20the%20previous%0Aextractive%20model%20by%203%25%20on%20extractive%20QA%20datasets.%20Furthermore%2C%20the%20GSQA%20model%0Ahas%20only%20been%20fine-tuned%20on%20the%20spoken%20extractive%20QA%20dataset.%20Despite%20not%0Ahaving%20seen%20any%20spoken%20abstractive%20QA%20data%2C%20it%20can%20still%20closely%20match%20the%0Aperformance%20of%20the%20cascade%20model.%20In%20conclusion%2C%20our%20GSQA%20model%20shows%20the%0Apotential%20to%20generalize%20to%20a%20broad%20spectrum%20of%20questions%2C%20thus%20further%0Aexpanding%20the%20spoken%20question%20answering%20capabilities%20of%20abstractive%20QA.%20Our%0Acode%20is%20available%20at%20https%3A//voidful.github.io/GSQA%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09781v3&entry.124074799=Read"},
{"title": "QSync: Quantization-Minimized Synchronous Distributed Training Across\n  Hybrid Devices", "author": "Juntao Zhao and Borui Wan and Yanghua Peng and Haibin Lin and Yibo Zhu and Chuan Wu", "abstract": "  A number of production deep learning clusters have attempted to explore\ninference hardware for DNN training, at the off-peak serving hours with many\ninference GPUs idling. Conducting DNN training with a combination of\nheterogeneous training and inference GPUs, known as hybrid device training,\npresents considerable challenges due to disparities in compute capability and\nsignificant differences in memory capacity. We propose QSync, a training system\nthat enables efficient synchronous data-parallel DNN training over hybrid\ndevices by strategically exploiting quantized operators. According to each\ndevice's available resource capacity, QSync selects a quantization-minimized\nsetting for operators in the distributed DNN training graph, minimizing model\naccuracy degradation but keeping the training efficiency brought by\nquantization. We carefully design a predictor with a bi-directional\nmixed-precision indicator to reflect the sensitivity of DNN layers on\nfixed-point and floating-point low-precision operators, a replayer with a\nneighborhood-aware cost mapper to accurately estimate the latency of\ndistributed hybrid mixed-precision training, and then an allocator that\nefficiently synchronizes workers with minimized model accuracy degradation.\nQSync bridges the computational graph on PyTorch to an optimized backend for\nquantization kernel performance and flexible support for various GPU\narchitectures. Extensive experiments show that QSync's predictor can accurately\nsimulate distributed mixed-precision training with <5% error, with a consistent\n0.27-1.03% accuracy improvement over the from-scratch training tasks compared\nto uniform precision.\n", "link": "http://arxiv.org/abs/2407.02327v1", "date": "2024-07-02", "relevancy": 2.087, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5356}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5119}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QSync%3A%20Quantization-Minimized%20Synchronous%20Distributed%20Training%20Across%0A%20%20Hybrid%20Devices&body=Title%3A%20QSync%3A%20Quantization-Minimized%20Synchronous%20Distributed%20Training%20Across%0A%20%20Hybrid%20Devices%0AAuthor%3A%20Juntao%20Zhao%20and%20Borui%20Wan%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Yibo%20Zhu%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20A%20number%20of%20production%20deep%20learning%20clusters%20have%20attempted%20to%20explore%0Ainference%20hardware%20for%20DNN%20training%2C%20at%20the%20off-peak%20serving%20hours%20with%20many%0Ainference%20GPUs%20idling.%20Conducting%20DNN%20training%20with%20a%20combination%20of%0Aheterogeneous%20training%20and%20inference%20GPUs%2C%20known%20as%20hybrid%20device%20training%2C%0Apresents%20considerable%20challenges%20due%20to%20disparities%20in%20compute%20capability%20and%0Asignificant%20differences%20in%20memory%20capacity.%20We%20propose%20QSync%2C%20a%20training%20system%0Athat%20enables%20efficient%20synchronous%20data-parallel%20DNN%20training%20over%20hybrid%0Adevices%20by%20strategically%20exploiting%20quantized%20operators.%20According%20to%20each%0Adevice%27s%20available%20resource%20capacity%2C%20QSync%20selects%20a%20quantization-minimized%0Asetting%20for%20operators%20in%20the%20distributed%20DNN%20training%20graph%2C%20minimizing%20model%0Aaccuracy%20degradation%20but%20keeping%20the%20training%20efficiency%20brought%20by%0Aquantization.%20We%20carefully%20design%20a%20predictor%20with%20a%20bi-directional%0Amixed-precision%20indicator%20to%20reflect%20the%20sensitivity%20of%20DNN%20layers%20on%0Afixed-point%20and%20floating-point%20low-precision%20operators%2C%20a%20replayer%20with%20a%0Aneighborhood-aware%20cost%20mapper%20to%20accurately%20estimate%20the%20latency%20of%0Adistributed%20hybrid%20mixed-precision%20training%2C%20and%20then%20an%20allocator%20that%0Aefficiently%20synchronizes%20workers%20with%20minimized%20model%20accuracy%20degradation.%0AQSync%20bridges%20the%20computational%20graph%20on%20PyTorch%20to%20an%20optimized%20backend%20for%0Aquantization%20kernel%20performance%20and%20flexible%20support%20for%20various%20GPU%0Aarchitectures.%20Extensive%20experiments%20show%20that%20QSync%27s%20predictor%20can%20accurately%0Asimulate%20distributed%20mixed-precision%20training%20with%20%3C5%25%20error%2C%20with%20a%20consistent%0A0.27-1.03%25%20accuracy%20improvement%20over%20the%20from-scratch%20training%20tasks%20compared%0Ato%20uniform%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQSync%253A%2520Quantization-Minimized%2520Synchronous%2520Distributed%2520Training%2520Across%250A%2520%2520Hybrid%2520Devices%26entry.906535625%3DJuntao%2520Zhao%2520and%2520Borui%2520Wan%2520and%2520Yanghua%2520Peng%2520and%2520Haibin%2520Lin%2520and%2520Yibo%2520Zhu%2520and%2520Chuan%2520Wu%26entry.1292438233%3D%2520%2520A%2520number%2520of%2520production%2520deep%2520learning%2520clusters%2520have%2520attempted%2520to%2520explore%250Ainference%2520hardware%2520for%2520DNN%2520training%252C%2520at%2520the%2520off-peak%2520serving%2520hours%2520with%2520many%250Ainference%2520GPUs%2520idling.%2520Conducting%2520DNN%2520training%2520with%2520a%2520combination%2520of%250Aheterogeneous%2520training%2520and%2520inference%2520GPUs%252C%2520known%2520as%2520hybrid%2520device%2520training%252C%250Apresents%2520considerable%2520challenges%2520due%2520to%2520disparities%2520in%2520compute%2520capability%2520and%250Asignificant%2520differences%2520in%2520memory%2520capacity.%2520We%2520propose%2520QSync%252C%2520a%2520training%2520system%250Athat%2520enables%2520efficient%2520synchronous%2520data-parallel%2520DNN%2520training%2520over%2520hybrid%250Adevices%2520by%2520strategically%2520exploiting%2520quantized%2520operators.%2520According%2520to%2520each%250Adevice%2527s%2520available%2520resource%2520capacity%252C%2520QSync%2520selects%2520a%2520quantization-minimized%250Asetting%2520for%2520operators%2520in%2520the%2520distributed%2520DNN%2520training%2520graph%252C%2520minimizing%2520model%250Aaccuracy%2520degradation%2520but%2520keeping%2520the%2520training%2520efficiency%2520brought%2520by%250Aquantization.%2520We%2520carefully%2520design%2520a%2520predictor%2520with%2520a%2520bi-directional%250Amixed-precision%2520indicator%2520to%2520reflect%2520the%2520sensitivity%2520of%2520DNN%2520layers%2520on%250Afixed-point%2520and%2520floating-point%2520low-precision%2520operators%252C%2520a%2520replayer%2520with%2520a%250Aneighborhood-aware%2520cost%2520mapper%2520to%2520accurately%2520estimate%2520the%2520latency%2520of%250Adistributed%2520hybrid%2520mixed-precision%2520training%252C%2520and%2520then%2520an%2520allocator%2520that%250Aefficiently%2520synchronizes%2520workers%2520with%2520minimized%2520model%2520accuracy%2520degradation.%250AQSync%2520bridges%2520the%2520computational%2520graph%2520on%2520PyTorch%2520to%2520an%2520optimized%2520backend%2520for%250Aquantization%2520kernel%2520performance%2520and%2520flexible%2520support%2520for%2520various%2520GPU%250Aarchitectures.%2520Extensive%2520experiments%2520show%2520that%2520QSync%2527s%2520predictor%2520can%2520accurately%250Asimulate%2520distributed%2520mixed-precision%2520training%2520with%2520%253C5%2525%2520error%252C%2520with%2520a%2520consistent%250A0.27-1.03%2525%2520accuracy%2520improvement%2520over%2520the%2520from-scratch%2520training%2520tasks%2520compared%250Ato%2520uniform%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QSync%3A%20Quantization-Minimized%20Synchronous%20Distributed%20Training%20Across%0A%20%20Hybrid%20Devices&entry.906535625=Juntao%20Zhao%20and%20Borui%20Wan%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Yibo%20Zhu%20and%20Chuan%20Wu&entry.1292438233=%20%20A%20number%20of%20production%20deep%20learning%20clusters%20have%20attempted%20to%20explore%0Ainference%20hardware%20for%20DNN%20training%2C%20at%20the%20off-peak%20serving%20hours%20with%20many%0Ainference%20GPUs%20idling.%20Conducting%20DNN%20training%20with%20a%20combination%20of%0Aheterogeneous%20training%20and%20inference%20GPUs%2C%20known%20as%20hybrid%20device%20training%2C%0Apresents%20considerable%20challenges%20due%20to%20disparities%20in%20compute%20capability%20and%0Asignificant%20differences%20in%20memory%20capacity.%20We%20propose%20QSync%2C%20a%20training%20system%0Athat%20enables%20efficient%20synchronous%20data-parallel%20DNN%20training%20over%20hybrid%0Adevices%20by%20strategically%20exploiting%20quantized%20operators.%20According%20to%20each%0Adevice%27s%20available%20resource%20capacity%2C%20QSync%20selects%20a%20quantization-minimized%0Asetting%20for%20operators%20in%20the%20distributed%20DNN%20training%20graph%2C%20minimizing%20model%0Aaccuracy%20degradation%20but%20keeping%20the%20training%20efficiency%20brought%20by%0Aquantization.%20We%20carefully%20design%20a%20predictor%20with%20a%20bi-directional%0Amixed-precision%20indicator%20to%20reflect%20the%20sensitivity%20of%20DNN%20layers%20on%0Afixed-point%20and%20floating-point%20low-precision%20operators%2C%20a%20replayer%20with%20a%0Aneighborhood-aware%20cost%20mapper%20to%20accurately%20estimate%20the%20latency%20of%0Adistributed%20hybrid%20mixed-precision%20training%2C%20and%20then%20an%20allocator%20that%0Aefficiently%20synchronizes%20workers%20with%20minimized%20model%20accuracy%20degradation.%0AQSync%20bridges%20the%20computational%20graph%20on%20PyTorch%20to%20an%20optimized%20backend%20for%0Aquantization%20kernel%20performance%20and%20flexible%20support%20for%20various%20GPU%0Aarchitectures.%20Extensive%20experiments%20show%20that%20QSync%27s%20predictor%20can%20accurately%0Asimulate%20distributed%20mixed-precision%20training%20with%20%3C5%25%20error%2C%20with%20a%20consistent%0A0.27-1.03%25%20accuracy%20improvement%20over%20the%20from-scratch%20training%20tasks%20compared%0Ato%20uniform%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02327v1&entry.124074799=Read"},
{"title": "FreeCG: Free the Design Space of Clebsch-Gordan Transform for machine\n  learning force field", "author": "Shihao Shao and Haoran Geng and Qinghua Cui", "abstract": "  The Clebsch-Gordan Transform (CG transform) effectively encodes many-body\ninteractions. Many studies have proven its accuracy in depicting atomic\nenvironments, although this comes with high computational needs. The\ncomputational burden of this challenge is hard to reduce due to the need for\npermutation equivariance, which limits the design space of the CG transform\nlayer. We show that, implementing the CG transform layer on\npermutation-invariant inputs allows complete freedom in the design of this\nlayer without affecting symmetry. Developing further on this premise, our idea\nis to create a CG transform layer that operates on permutation-invariant\nabstract edges generated from real edge information. We bring in group CG\ntransform with sparse path, abstract edges shuffling, and attention enhancer to\nform a powerful and efficient CG transform layer. Our method, known as FreeCG,\nachieves State-of-The-Art (SoTA) results in force prediction for MD17, rMD17,\nMD22, and property prediction in QM9 datasets with notable enhancement. It\nintroduces a novel paradigm for carrying out efficient and expressive CG\ntransform in future geometric neural network designs.\n", "link": "http://arxiv.org/abs/2407.02263v1", "date": "2024-07-02", "relevancy": 2.0812, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5222}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeCG%3A%20Free%20the%20Design%20Space%20of%20Clebsch-Gordan%20Transform%20for%20machine%0A%20%20learning%20force%20field&body=Title%3A%20FreeCG%3A%20Free%20the%20Design%20Space%20of%20Clebsch-Gordan%20Transform%20for%20machine%0A%20%20learning%20force%20field%0AAuthor%3A%20Shihao%20Shao%20and%20Haoran%20Geng%20and%20Qinghua%20Cui%0AAbstract%3A%20%20%20The%20Clebsch-Gordan%20Transform%20%28CG%20transform%29%20effectively%20encodes%20many-body%0Ainteractions.%20Many%20studies%20have%20proven%20its%20accuracy%20in%20depicting%20atomic%0Aenvironments%2C%20although%20this%20comes%20with%20high%20computational%20needs.%20The%0Acomputational%20burden%20of%20this%20challenge%20is%20hard%20to%20reduce%20due%20to%20the%20need%20for%0Apermutation%20equivariance%2C%20which%20limits%20the%20design%20space%20of%20the%20CG%20transform%0Alayer.%20We%20show%20that%2C%20implementing%20the%20CG%20transform%20layer%20on%0Apermutation-invariant%20inputs%20allows%20complete%20freedom%20in%20the%20design%20of%20this%0Alayer%20without%20affecting%20symmetry.%20Developing%20further%20on%20this%20premise%2C%20our%20idea%0Ais%20to%20create%20a%20CG%20transform%20layer%20that%20operates%20on%20permutation-invariant%0Aabstract%20edges%20generated%20from%20real%20edge%20information.%20We%20bring%20in%20group%20CG%0Atransform%20with%20sparse%20path%2C%20abstract%20edges%20shuffling%2C%20and%20attention%20enhancer%20to%0Aform%20a%20powerful%20and%20efficient%20CG%20transform%20layer.%20Our%20method%2C%20known%20as%20FreeCG%2C%0Aachieves%20State-of-The-Art%20%28SoTA%29%20results%20in%20force%20prediction%20for%20MD17%2C%20rMD17%2C%0AMD22%2C%20and%20property%20prediction%20in%20QM9%20datasets%20with%20notable%20enhancement.%20It%0Aintroduces%20a%20novel%20paradigm%20for%20carrying%20out%20efficient%20and%20expressive%20CG%0Atransform%20in%20future%20geometric%20neural%20network%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeCG%253A%2520Free%2520the%2520Design%2520Space%2520of%2520Clebsch-Gordan%2520Transform%2520for%2520machine%250A%2520%2520learning%2520force%2520field%26entry.906535625%3DShihao%2520Shao%2520and%2520Haoran%2520Geng%2520and%2520Qinghua%2520Cui%26entry.1292438233%3D%2520%2520The%2520Clebsch-Gordan%2520Transform%2520%2528CG%2520transform%2529%2520effectively%2520encodes%2520many-body%250Ainteractions.%2520Many%2520studies%2520have%2520proven%2520its%2520accuracy%2520in%2520depicting%2520atomic%250Aenvironments%252C%2520although%2520this%2520comes%2520with%2520high%2520computational%2520needs.%2520The%250Acomputational%2520burden%2520of%2520this%2520challenge%2520is%2520hard%2520to%2520reduce%2520due%2520to%2520the%2520need%2520for%250Apermutation%2520equivariance%252C%2520which%2520limits%2520the%2520design%2520space%2520of%2520the%2520CG%2520transform%250Alayer.%2520We%2520show%2520that%252C%2520implementing%2520the%2520CG%2520transform%2520layer%2520on%250Apermutation-invariant%2520inputs%2520allows%2520complete%2520freedom%2520in%2520the%2520design%2520of%2520this%250Alayer%2520without%2520affecting%2520symmetry.%2520Developing%2520further%2520on%2520this%2520premise%252C%2520our%2520idea%250Ais%2520to%2520create%2520a%2520CG%2520transform%2520layer%2520that%2520operates%2520on%2520permutation-invariant%250Aabstract%2520edges%2520generated%2520from%2520real%2520edge%2520information.%2520We%2520bring%2520in%2520group%2520CG%250Atransform%2520with%2520sparse%2520path%252C%2520abstract%2520edges%2520shuffling%252C%2520and%2520attention%2520enhancer%2520to%250Aform%2520a%2520powerful%2520and%2520efficient%2520CG%2520transform%2520layer.%2520Our%2520method%252C%2520known%2520as%2520FreeCG%252C%250Aachieves%2520State-of-The-Art%2520%2528SoTA%2529%2520results%2520in%2520force%2520prediction%2520for%2520MD17%252C%2520rMD17%252C%250AMD22%252C%2520and%2520property%2520prediction%2520in%2520QM9%2520datasets%2520with%2520notable%2520enhancement.%2520It%250Aintroduces%2520a%2520novel%2520paradigm%2520for%2520carrying%2520out%2520efficient%2520and%2520expressive%2520CG%250Atransform%2520in%2520future%2520geometric%2520neural%2520network%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeCG%3A%20Free%20the%20Design%20Space%20of%20Clebsch-Gordan%20Transform%20for%20machine%0A%20%20learning%20force%20field&entry.906535625=Shihao%20Shao%20and%20Haoran%20Geng%20and%20Qinghua%20Cui&entry.1292438233=%20%20The%20Clebsch-Gordan%20Transform%20%28CG%20transform%29%20effectively%20encodes%20many-body%0Ainteractions.%20Many%20studies%20have%20proven%20its%20accuracy%20in%20depicting%20atomic%0Aenvironments%2C%20although%20this%20comes%20with%20high%20computational%20needs.%20The%0Acomputational%20burden%20of%20this%20challenge%20is%20hard%20to%20reduce%20due%20to%20the%20need%20for%0Apermutation%20equivariance%2C%20which%20limits%20the%20design%20space%20of%20the%20CG%20transform%0Alayer.%20We%20show%20that%2C%20implementing%20the%20CG%20transform%20layer%20on%0Apermutation-invariant%20inputs%20allows%20complete%20freedom%20in%20the%20design%20of%20this%0Alayer%20without%20affecting%20symmetry.%20Developing%20further%20on%20this%20premise%2C%20our%20idea%0Ais%20to%20create%20a%20CG%20transform%20layer%20that%20operates%20on%20permutation-invariant%0Aabstract%20edges%20generated%20from%20real%20edge%20information.%20We%20bring%20in%20group%20CG%0Atransform%20with%20sparse%20path%2C%20abstract%20edges%20shuffling%2C%20and%20attention%20enhancer%20to%0Aform%20a%20powerful%20and%20efficient%20CG%20transform%20layer.%20Our%20method%2C%20known%20as%20FreeCG%2C%0Aachieves%20State-of-The-Art%20%28SoTA%29%20results%20in%20force%20prediction%20for%20MD17%2C%20rMD17%2C%0AMD22%2C%20and%20property%20prediction%20in%20QM9%20datasets%20with%20notable%20enhancement.%20It%0Aintroduces%20a%20novel%20paradigm%20for%20carrying%20out%20efficient%20and%20expressive%20CG%0Atransform%20in%20future%20geometric%20neural%20network%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02263v1&entry.124074799=Read"},
{"title": "Multi-level Reliable Guidance for Unpaired Multi-view Clustering", "author": "Like Xin and Wanqi Yang and Lei Wang and Ming Yang", "abstract": "  In this paper, we address the challenging problem of unpaired multi-view\nclustering (UMC), aiming to perform effective joint clustering using unpaired\nobserved samples across multiple views. Commonly, traditional incomplete\nmulti-view clustering (IMC) methods often depend on paired samples to capture\ncomplementary information between views. However, the strategy becomes\nimpractical in UMC due to the absence of paired samples. Although some\nresearchers have attempted to tackle the issue by preserving consistent cluster\nstructures across views, they frequently neglect the confidence of these\ncluster structures, especially for boundary samples and uncertain cluster\nstructures during the initial training. Therefore, we propose a method called\nMulti-level Reliable Guidance for UMC (MRG-UMC), which leverages multi-level\nclustering to aid in learning a trustworthy cluster structure across\ninner-view, cross-view, and common-view, respectively. Specifically, within\neach view, multi-level clustering fosters a trustworthy cluster structure\nacross different levels and reduces clustering error. In cross-view learning,\nreliable view guidance enhances the confidence of the cluster structures in\nother views. Similarly, within the multi-level framework, the incorporation of\na common view aids in aligning different views, thereby reducing the clustering\nerror and uncertainty of cluster structure. Finally, as evidenced by extensive\nexperiments, our method for UMC demonstrates significant efficiency\nimprovements compared to 20 state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.01247v2", "date": "2024-07-02", "relevancy": 2.0805, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5241}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-level%20Reliable%20Guidance%20for%20Unpaired%20Multi-view%20Clustering&body=Title%3A%20Multi-level%20Reliable%20Guidance%20for%20Unpaired%20Multi-view%20Clustering%0AAuthor%3A%20Like%20Xin%20and%20Wanqi%20Yang%20and%20Lei%20Wang%20and%20Ming%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenging%20problem%20of%20unpaired%20multi-view%0Aclustering%20%28UMC%29%2C%20aiming%20to%20perform%20effective%20joint%20clustering%20using%20unpaired%0Aobserved%20samples%20across%20multiple%20views.%20Commonly%2C%20traditional%20incomplete%0Amulti-view%20clustering%20%28IMC%29%20methods%20often%20depend%20on%20paired%20samples%20to%20capture%0Acomplementary%20information%20between%20views.%20However%2C%20the%20strategy%20becomes%0Aimpractical%20in%20UMC%20due%20to%20the%20absence%20of%20paired%20samples.%20Although%20some%0Aresearchers%20have%20attempted%20to%20tackle%20the%20issue%20by%20preserving%20consistent%20cluster%0Astructures%20across%20views%2C%20they%20frequently%20neglect%20the%20confidence%20of%20these%0Acluster%20structures%2C%20especially%20for%20boundary%20samples%20and%20uncertain%20cluster%0Astructures%20during%20the%20initial%20training.%20Therefore%2C%20we%20propose%20a%20method%20called%0AMulti-level%20Reliable%20Guidance%20for%20UMC%20%28MRG-UMC%29%2C%20which%20leverages%20multi-level%0Aclustering%20to%20aid%20in%20learning%20a%20trustworthy%20cluster%20structure%20across%0Ainner-view%2C%20cross-view%2C%20and%20common-view%2C%20respectively.%20Specifically%2C%20within%0Aeach%20view%2C%20multi-level%20clustering%20fosters%20a%20trustworthy%20cluster%20structure%0Aacross%20different%20levels%20and%20reduces%20clustering%20error.%20In%20cross-view%20learning%2C%0Areliable%20view%20guidance%20enhances%20the%20confidence%20of%20the%20cluster%20structures%20in%0Aother%20views.%20Similarly%2C%20within%20the%20multi-level%20framework%2C%20the%20incorporation%20of%0Aa%20common%20view%20aids%20in%20aligning%20different%20views%2C%20thereby%20reducing%20the%20clustering%0Aerror%20and%20uncertainty%20of%20cluster%20structure.%20Finally%2C%20as%20evidenced%20by%20extensive%0Aexperiments%2C%20our%20method%20for%20UMC%20demonstrates%20significant%20efficiency%0Aimprovements%20compared%20to%2020%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-level%2520Reliable%2520Guidance%2520for%2520Unpaired%2520Multi-view%2520Clustering%26entry.906535625%3DLike%2520Xin%2520and%2520Wanqi%2520Yang%2520and%2520Lei%2520Wang%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenging%2520problem%2520of%2520unpaired%2520multi-view%250Aclustering%2520%2528UMC%2529%252C%2520aiming%2520to%2520perform%2520effective%2520joint%2520clustering%2520using%2520unpaired%250Aobserved%2520samples%2520across%2520multiple%2520views.%2520Commonly%252C%2520traditional%2520incomplete%250Amulti-view%2520clustering%2520%2528IMC%2529%2520methods%2520often%2520depend%2520on%2520paired%2520samples%2520to%2520capture%250Acomplementary%2520information%2520between%2520views.%2520However%252C%2520the%2520strategy%2520becomes%250Aimpractical%2520in%2520UMC%2520due%2520to%2520the%2520absence%2520of%2520paired%2520samples.%2520Although%2520some%250Aresearchers%2520have%2520attempted%2520to%2520tackle%2520the%2520issue%2520by%2520preserving%2520consistent%2520cluster%250Astructures%2520across%2520views%252C%2520they%2520frequently%2520neglect%2520the%2520confidence%2520of%2520these%250Acluster%2520structures%252C%2520especially%2520for%2520boundary%2520samples%2520and%2520uncertain%2520cluster%250Astructures%2520during%2520the%2520initial%2520training.%2520Therefore%252C%2520we%2520propose%2520a%2520method%2520called%250AMulti-level%2520Reliable%2520Guidance%2520for%2520UMC%2520%2528MRG-UMC%2529%252C%2520which%2520leverages%2520multi-level%250Aclustering%2520to%2520aid%2520in%2520learning%2520a%2520trustworthy%2520cluster%2520structure%2520across%250Ainner-view%252C%2520cross-view%252C%2520and%2520common-view%252C%2520respectively.%2520Specifically%252C%2520within%250Aeach%2520view%252C%2520multi-level%2520clustering%2520fosters%2520a%2520trustworthy%2520cluster%2520structure%250Aacross%2520different%2520levels%2520and%2520reduces%2520clustering%2520error.%2520In%2520cross-view%2520learning%252C%250Areliable%2520view%2520guidance%2520enhances%2520the%2520confidence%2520of%2520the%2520cluster%2520structures%2520in%250Aother%2520views.%2520Similarly%252C%2520within%2520the%2520multi-level%2520framework%252C%2520the%2520incorporation%2520of%250Aa%2520common%2520view%2520aids%2520in%2520aligning%2520different%2520views%252C%2520thereby%2520reducing%2520the%2520clustering%250Aerror%2520and%2520uncertainty%2520of%2520cluster%2520structure.%2520Finally%252C%2520as%2520evidenced%2520by%2520extensive%250Aexperiments%252C%2520our%2520method%2520for%2520UMC%2520demonstrates%2520significant%2520efficiency%250Aimprovements%2520compared%2520to%252020%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-level%20Reliable%20Guidance%20for%20Unpaired%20Multi-view%20Clustering&entry.906535625=Like%20Xin%20and%20Wanqi%20Yang%20and%20Lei%20Wang%20and%20Ming%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenging%20problem%20of%20unpaired%20multi-view%0Aclustering%20%28UMC%29%2C%20aiming%20to%20perform%20effective%20joint%20clustering%20using%20unpaired%0Aobserved%20samples%20across%20multiple%20views.%20Commonly%2C%20traditional%20incomplete%0Amulti-view%20clustering%20%28IMC%29%20methods%20often%20depend%20on%20paired%20samples%20to%20capture%0Acomplementary%20information%20between%20views.%20However%2C%20the%20strategy%20becomes%0Aimpractical%20in%20UMC%20due%20to%20the%20absence%20of%20paired%20samples.%20Although%20some%0Aresearchers%20have%20attempted%20to%20tackle%20the%20issue%20by%20preserving%20consistent%20cluster%0Astructures%20across%20views%2C%20they%20frequently%20neglect%20the%20confidence%20of%20these%0Acluster%20structures%2C%20especially%20for%20boundary%20samples%20and%20uncertain%20cluster%0Astructures%20during%20the%20initial%20training.%20Therefore%2C%20we%20propose%20a%20method%20called%0AMulti-level%20Reliable%20Guidance%20for%20UMC%20%28MRG-UMC%29%2C%20which%20leverages%20multi-level%0Aclustering%20to%20aid%20in%20learning%20a%20trustworthy%20cluster%20structure%20across%0Ainner-view%2C%20cross-view%2C%20and%20common-view%2C%20respectively.%20Specifically%2C%20within%0Aeach%20view%2C%20multi-level%20clustering%20fosters%20a%20trustworthy%20cluster%20structure%0Aacross%20different%20levels%20and%20reduces%20clustering%20error.%20In%20cross-view%20learning%2C%0Areliable%20view%20guidance%20enhances%20the%20confidence%20of%20the%20cluster%20structures%20in%0Aother%20views.%20Similarly%2C%20within%20the%20multi-level%20framework%2C%20the%20incorporation%20of%0Aa%20common%20view%20aids%20in%20aligning%20different%20views%2C%20thereby%20reducing%20the%20clustering%0Aerror%20and%20uncertainty%20of%20cluster%20structure.%20Finally%2C%20as%20evidenced%20by%20extensive%0Aexperiments%2C%20our%20method%20for%20UMC%20demonstrates%20significant%20efficiency%0Aimprovements%20compared%20to%2020%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01247v2&entry.124074799=Read"},
{"title": "EvolBA: Evolutionary Boundary Attack under Hard-label Black Box\n  condition", "author": "Ayane Tajima and Satoshi Ono", "abstract": "  Research has shown that deep neural networks (DNNs) have vulnerabilities that\ncan lead to the misrecognition of Adversarial Examples (AEs) with specifically\ndesigned perturbations. Various adversarial attack methods have been proposed\nto detect vulnerabilities under hard-label black box (HL-BB) conditions in the\nabsence of loss gradients and confidence scores.However, these methods fall\ninto local solutions because they search only local regions of the search\nspace. Therefore, this study proposes an adversarial attack method named EvolBA\nto generate AEs using Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\nunder the HL-BB condition, where only a class label predicted by the target DNN\nmodel is available. Inspired by formula-driven supervised learning, the\nproposed method introduces domain-independent operators for the initialization\nprocess and a jump that enhances search exploration. Experimental results\nconfirmed that the proposed method could determine AEs with smaller\nperturbations than previous methods in images where the previous methods have\ndifficulty.\n", "link": "http://arxiv.org/abs/2407.02248v1", "date": "2024-07-02", "relevancy": 2.0707, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5301}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvolBA%3A%20Evolutionary%20Boundary%20Attack%20under%20Hard-label%20Black%20Box%0A%20%20condition&body=Title%3A%20EvolBA%3A%20Evolutionary%20Boundary%20Attack%20under%20Hard-label%20Black%20Box%0A%20%20condition%0AAuthor%3A%20Ayane%20Tajima%20and%20Satoshi%20Ono%0AAbstract%3A%20%20%20Research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20have%20vulnerabilities%20that%0Acan%20lead%20to%20the%20misrecognition%20of%20Adversarial%20Examples%20%28AEs%29%20with%20specifically%0Adesigned%20perturbations.%20Various%20adversarial%20attack%20methods%20have%20been%20proposed%0Ato%20detect%20vulnerabilities%20under%20hard-label%20black%20box%20%28HL-BB%29%20conditions%20in%20the%0Aabsence%20of%20loss%20gradients%20and%20confidence%20scores.However%2C%20these%20methods%20fall%0Ainto%20local%20solutions%20because%20they%20search%20only%20local%20regions%20of%20the%20search%0Aspace.%20Therefore%2C%20this%20study%20proposes%20an%20adversarial%20attack%20method%20named%20EvolBA%0Ato%20generate%20AEs%20using%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20%28CMA-ES%29%0Aunder%20the%20HL-BB%20condition%2C%20where%20only%20a%20class%20label%20predicted%20by%20the%20target%20DNN%0Amodel%20is%20available.%20Inspired%20by%20formula-driven%20supervised%20learning%2C%20the%0Aproposed%20method%20introduces%20domain-independent%20operators%20for%20the%20initialization%0Aprocess%20and%20a%20jump%20that%20enhances%20search%20exploration.%20Experimental%20results%0Aconfirmed%20that%20the%20proposed%20method%20could%20determine%20AEs%20with%20smaller%0Aperturbations%20than%20previous%20methods%20in%20images%20where%20the%20previous%20methods%20have%0Adifficulty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolBA%253A%2520Evolutionary%2520Boundary%2520Attack%2520under%2520Hard-label%2520Black%2520Box%250A%2520%2520condition%26entry.906535625%3DAyane%2520Tajima%2520and%2520Satoshi%2520Ono%26entry.1292438233%3D%2520%2520Research%2520has%2520shown%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520vulnerabilities%2520that%250Acan%2520lead%2520to%2520the%2520misrecognition%2520of%2520Adversarial%2520Examples%2520%2528AEs%2529%2520with%2520specifically%250Adesigned%2520perturbations.%2520Various%2520adversarial%2520attack%2520methods%2520have%2520been%2520proposed%250Ato%2520detect%2520vulnerabilities%2520under%2520hard-label%2520black%2520box%2520%2528HL-BB%2529%2520conditions%2520in%2520the%250Aabsence%2520of%2520loss%2520gradients%2520and%2520confidence%2520scores.However%252C%2520these%2520methods%2520fall%250Ainto%2520local%2520solutions%2520because%2520they%2520search%2520only%2520local%2520regions%2520of%2520the%2520search%250Aspace.%2520Therefore%252C%2520this%2520study%2520proposes%2520an%2520adversarial%2520attack%2520method%2520named%2520EvolBA%250Ato%2520generate%2520AEs%2520using%2520Covariance%2520Matrix%2520Adaptation%2520Evolution%2520Strategy%2520%2528CMA-ES%2529%250Aunder%2520the%2520HL-BB%2520condition%252C%2520where%2520only%2520a%2520class%2520label%2520predicted%2520by%2520the%2520target%2520DNN%250Amodel%2520is%2520available.%2520Inspired%2520by%2520formula-driven%2520supervised%2520learning%252C%2520the%250Aproposed%2520method%2520introduces%2520domain-independent%2520operators%2520for%2520the%2520initialization%250Aprocess%2520and%2520a%2520jump%2520that%2520enhances%2520search%2520exploration.%2520Experimental%2520results%250Aconfirmed%2520that%2520the%2520proposed%2520method%2520could%2520determine%2520AEs%2520with%2520smaller%250Aperturbations%2520than%2520previous%2520methods%2520in%2520images%2520where%2520the%2520previous%2520methods%2520have%250Adifficulty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvolBA%3A%20Evolutionary%20Boundary%20Attack%20under%20Hard-label%20Black%20Box%0A%20%20condition&entry.906535625=Ayane%20Tajima%20and%20Satoshi%20Ono&entry.1292438233=%20%20Research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20have%20vulnerabilities%20that%0Acan%20lead%20to%20the%20misrecognition%20of%20Adversarial%20Examples%20%28AEs%29%20with%20specifically%0Adesigned%20perturbations.%20Various%20adversarial%20attack%20methods%20have%20been%20proposed%0Ato%20detect%20vulnerabilities%20under%20hard-label%20black%20box%20%28HL-BB%29%20conditions%20in%20the%0Aabsence%20of%20loss%20gradients%20and%20confidence%20scores.However%2C%20these%20methods%20fall%0Ainto%20local%20solutions%20because%20they%20search%20only%20local%20regions%20of%20the%20search%0Aspace.%20Therefore%2C%20this%20study%20proposes%20an%20adversarial%20attack%20method%20named%20EvolBA%0Ato%20generate%20AEs%20using%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20%28CMA-ES%29%0Aunder%20the%20HL-BB%20condition%2C%20where%20only%20a%20class%20label%20predicted%20by%20the%20target%20DNN%0Amodel%20is%20available.%20Inspired%20by%20formula-driven%20supervised%20learning%2C%20the%0Aproposed%20method%20introduces%20domain-independent%20operators%20for%20the%20initialization%0Aprocess%20and%20a%20jump%20that%20enhances%20search%20exploration.%20Experimental%20results%0Aconfirmed%20that%20the%20proposed%20method%20could%20determine%20AEs%20with%20smaller%0Aperturbations%20than%20previous%20methods%20in%20images%20where%20the%20previous%20methods%20have%0Adifficulty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02248v1&entry.124074799=Read"},
{"title": "FedIA: Federated Medical Image Segmentation with Heterogeneous\n  Annotation Completeness", "author": "Yangyang Xiang and Nannan Wu and Li Yu and Xin Yang and Kwang-Ting Cheng and Zengqiang Yan", "abstract": "  Federated learning has emerged as a compelling paradigm for medical image\nsegmentation, particularly in light of increasing privacy concerns. However,\nmost of the existing research relies on relatively stringent assumptions\nregarding the uniformity and completeness of annotations across clients.\nContrary to this, this paper highlights a prevalent challenge in medical\npractice: incomplete annotations. Such annotations can introduce incorrectly\nlabeled pixels, potentially undermining the performance of neural networks in\nsupervised learning. To tackle this issue, we introduce a novel solution, named\nFedIA. Our insight is to conceptualize incomplete annotations as noisy data\n(\\textit{i.e.}, low-quality data), with a focus on mitigating their adverse\neffects. We begin by evaluating the completeness of annotations at the client\nlevel using a designed indicator. Subsequently, we enhance the influence of\nclients with more comprehensive annotations and implement corrections for\nincomplete ones, thereby ensuring that models are trained on accurate data. Our\nmethod's effectiveness is validated through its superior performance on two\nextensively used medical image segmentation datasets, outperforming existing\nsolutions. The code is available at https://github.com/HUSTxyy/FedIA.\n", "link": "http://arxiv.org/abs/2407.02280v1", "date": "2024-07-02", "relevancy": 2.0646, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5775}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5093}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedIA%3A%20Federated%20Medical%20Image%20Segmentation%20with%20Heterogeneous%0A%20%20Annotation%20Completeness&body=Title%3A%20FedIA%3A%20Federated%20Medical%20Image%20Segmentation%20with%20Heterogeneous%0A%20%20Annotation%20Completeness%0AAuthor%3A%20Yangyang%20Xiang%20and%20Nannan%20Wu%20and%20Li%20Yu%20and%20Xin%20Yang%20and%20Kwang-Ting%20Cheng%20and%20Zengqiang%20Yan%0AAbstract%3A%20%20%20Federated%20learning%20has%20emerged%20as%20a%20compelling%20paradigm%20for%20medical%20image%0Asegmentation%2C%20particularly%20in%20light%20of%20increasing%20privacy%20concerns.%20However%2C%0Amost%20of%20the%20existing%20research%20relies%20on%20relatively%20stringent%20assumptions%0Aregarding%20the%20uniformity%20and%20completeness%20of%20annotations%20across%20clients.%0AContrary%20to%20this%2C%20this%20paper%20highlights%20a%20prevalent%20challenge%20in%20medical%0Apractice%3A%20incomplete%20annotations.%20Such%20annotations%20can%20introduce%20incorrectly%0Alabeled%20pixels%2C%20potentially%20undermining%20the%20performance%20of%20neural%20networks%20in%0Asupervised%20learning.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20novel%20solution%2C%20named%0AFedIA.%20Our%20insight%20is%20to%20conceptualize%20incomplete%20annotations%20as%20noisy%20data%0A%28%5Ctextit%7Bi.e.%7D%2C%20low-quality%20data%29%2C%20with%20a%20focus%20on%20mitigating%20their%20adverse%0Aeffects.%20We%20begin%20by%20evaluating%20the%20completeness%20of%20annotations%20at%20the%20client%0Alevel%20using%20a%20designed%20indicator.%20Subsequently%2C%20we%20enhance%20the%20influence%20of%0Aclients%20with%20more%20comprehensive%20annotations%20and%20implement%20corrections%20for%0Aincomplete%20ones%2C%20thereby%20ensuring%20that%20models%20are%20trained%20on%20accurate%20data.%20Our%0Amethod%27s%20effectiveness%20is%20validated%20through%20its%20superior%20performance%20on%20two%0Aextensively%20used%20medical%20image%20segmentation%20datasets%2C%20outperforming%20existing%0Asolutions.%20The%20code%20is%20available%20at%20https%3A//github.com/HUSTxyy/FedIA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedIA%253A%2520Federated%2520Medical%2520Image%2520Segmentation%2520with%2520Heterogeneous%250A%2520%2520Annotation%2520Completeness%26entry.906535625%3DYangyang%2520Xiang%2520and%2520Nannan%2520Wu%2520and%2520Li%2520Yu%2520and%2520Xin%2520Yang%2520and%2520Kwang-Ting%2520Cheng%2520and%2520Zengqiang%2520Yan%26entry.1292438233%3D%2520%2520Federated%2520learning%2520has%2520emerged%2520as%2520a%2520compelling%2520paradigm%2520for%2520medical%2520image%250Asegmentation%252C%2520particularly%2520in%2520light%2520of%2520increasing%2520privacy%2520concerns.%2520However%252C%250Amost%2520of%2520the%2520existing%2520research%2520relies%2520on%2520relatively%2520stringent%2520assumptions%250Aregarding%2520the%2520uniformity%2520and%2520completeness%2520of%2520annotations%2520across%2520clients.%250AContrary%2520to%2520this%252C%2520this%2520paper%2520highlights%2520a%2520prevalent%2520challenge%2520in%2520medical%250Apractice%253A%2520incomplete%2520annotations.%2520Such%2520annotations%2520can%2520introduce%2520incorrectly%250Alabeled%2520pixels%252C%2520potentially%2520undermining%2520the%2520performance%2520of%2520neural%2520networks%2520in%250Asupervised%2520learning.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520solution%252C%2520named%250AFedIA.%2520Our%2520insight%2520is%2520to%2520conceptualize%2520incomplete%2520annotations%2520as%2520noisy%2520data%250A%2528%255Ctextit%257Bi.e.%257D%252C%2520low-quality%2520data%2529%252C%2520with%2520a%2520focus%2520on%2520mitigating%2520their%2520adverse%250Aeffects.%2520We%2520begin%2520by%2520evaluating%2520the%2520completeness%2520of%2520annotations%2520at%2520the%2520client%250Alevel%2520using%2520a%2520designed%2520indicator.%2520Subsequently%252C%2520we%2520enhance%2520the%2520influence%2520of%250Aclients%2520with%2520more%2520comprehensive%2520annotations%2520and%2520implement%2520corrections%2520for%250Aincomplete%2520ones%252C%2520thereby%2520ensuring%2520that%2520models%2520are%2520trained%2520on%2520accurate%2520data.%2520Our%250Amethod%2527s%2520effectiveness%2520is%2520validated%2520through%2520its%2520superior%2520performance%2520on%2520two%250Aextensively%2520used%2520medical%2520image%2520segmentation%2520datasets%252C%2520outperforming%2520existing%250Asolutions.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/HUSTxyy/FedIA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedIA%3A%20Federated%20Medical%20Image%20Segmentation%20with%20Heterogeneous%0A%20%20Annotation%20Completeness&entry.906535625=Yangyang%20Xiang%20and%20Nannan%20Wu%20and%20Li%20Yu%20and%20Xin%20Yang%20and%20Kwang-Ting%20Cheng%20and%20Zengqiang%20Yan&entry.1292438233=%20%20Federated%20learning%20has%20emerged%20as%20a%20compelling%20paradigm%20for%20medical%20image%0Asegmentation%2C%20particularly%20in%20light%20of%20increasing%20privacy%20concerns.%20However%2C%0Amost%20of%20the%20existing%20research%20relies%20on%20relatively%20stringent%20assumptions%0Aregarding%20the%20uniformity%20and%20completeness%20of%20annotations%20across%20clients.%0AContrary%20to%20this%2C%20this%20paper%20highlights%20a%20prevalent%20challenge%20in%20medical%0Apractice%3A%20incomplete%20annotations.%20Such%20annotations%20can%20introduce%20incorrectly%0Alabeled%20pixels%2C%20potentially%20undermining%20the%20performance%20of%20neural%20networks%20in%0Asupervised%20learning.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20novel%20solution%2C%20named%0AFedIA.%20Our%20insight%20is%20to%20conceptualize%20incomplete%20annotations%20as%20noisy%20data%0A%28%5Ctextit%7Bi.e.%7D%2C%20low-quality%20data%29%2C%20with%20a%20focus%20on%20mitigating%20their%20adverse%0Aeffects.%20We%20begin%20by%20evaluating%20the%20completeness%20of%20annotations%20at%20the%20client%0Alevel%20using%20a%20designed%20indicator.%20Subsequently%2C%20we%20enhance%20the%20influence%20of%0Aclients%20with%20more%20comprehensive%20annotations%20and%20implement%20corrections%20for%0Aincomplete%20ones%2C%20thereby%20ensuring%20that%20models%20are%20trained%20on%20accurate%20data.%20Our%0Amethod%27s%20effectiveness%20is%20validated%20through%20its%20superior%20performance%20on%20two%0Aextensively%20used%20medical%20image%20segmentation%20datasets%2C%20outperforming%20existing%0Asolutions.%20The%20code%20is%20available%20at%20https%3A//github.com/HUSTxyy/FedIA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02280v1&entry.124074799=Read"},
{"title": "MALT Powers Up Adversarial Attacks", "author": "Odelia Melamed and Gilad Yehudai and Adi Shamir", "abstract": "  Current adversarial attacks for multi-class classifiers choose the target\nclass for a given input naively, based on the classifier's confidence levels\nfor various target classes. We present a novel adversarial targeting method,\n\\textit{MALT - Mesoscopic Almost Linearity Targeting}, based on medium-scale\nalmost linearity assumptions. Our attack wins over the current state of the art\nAutoAttack on the standard benchmark datasets CIFAR-100 and ImageNet and for a\nvariety of robust models. In particular, our attack is \\emph{five times faster}\nthan AutoAttack, while successfully matching all of AutoAttack's successes and\nattacking additional samples that were previously out of reach. We then prove\nformally and demonstrate empirically that our targeting method, although\ninspired by linear predictors, also applies to standard non-linear models.\n", "link": "http://arxiv.org/abs/2407.02240v1", "date": "2024-07-02", "relevancy": 2.062, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MALT%20Powers%20Up%20Adversarial%20Attacks&body=Title%3A%20MALT%20Powers%20Up%20Adversarial%20Attacks%0AAuthor%3A%20Odelia%20Melamed%20and%20Gilad%20Yehudai%20and%20Adi%20Shamir%0AAbstract%3A%20%20%20Current%20adversarial%20attacks%20for%20multi-class%20classifiers%20choose%20the%20target%0Aclass%20for%20a%20given%20input%20naively%2C%20based%20on%20the%20classifier%27s%20confidence%20levels%0Afor%20various%20target%20classes.%20We%20present%20a%20novel%20adversarial%20targeting%20method%2C%0A%5Ctextit%7BMALT%20-%20Mesoscopic%20Almost%20Linearity%20Targeting%7D%2C%20based%20on%20medium-scale%0Aalmost%20linearity%20assumptions.%20Our%20attack%20wins%20over%20the%20current%20state%20of%20the%20art%0AAutoAttack%20on%20the%20standard%20benchmark%20datasets%20CIFAR-100%20and%20ImageNet%20and%20for%20a%0Avariety%20of%20robust%20models.%20In%20particular%2C%20our%20attack%20is%20%5Cemph%7Bfive%20times%20faster%7D%0Athan%20AutoAttack%2C%20while%20successfully%20matching%20all%20of%20AutoAttack%27s%20successes%20and%0Aattacking%20additional%20samples%20that%20were%20previously%20out%20of%20reach.%20We%20then%20prove%0Aformally%20and%20demonstrate%20empirically%20that%20our%20targeting%20method%2C%20although%0Ainspired%20by%20linear%20predictors%2C%20also%20applies%20to%20standard%20non-linear%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMALT%2520Powers%2520Up%2520Adversarial%2520Attacks%26entry.906535625%3DOdelia%2520Melamed%2520and%2520Gilad%2520Yehudai%2520and%2520Adi%2520Shamir%26entry.1292438233%3D%2520%2520Current%2520adversarial%2520attacks%2520for%2520multi-class%2520classifiers%2520choose%2520the%2520target%250Aclass%2520for%2520a%2520given%2520input%2520naively%252C%2520based%2520on%2520the%2520classifier%2527s%2520confidence%2520levels%250Afor%2520various%2520target%2520classes.%2520We%2520present%2520a%2520novel%2520adversarial%2520targeting%2520method%252C%250A%255Ctextit%257BMALT%2520-%2520Mesoscopic%2520Almost%2520Linearity%2520Targeting%257D%252C%2520based%2520on%2520medium-scale%250Aalmost%2520linearity%2520assumptions.%2520Our%2520attack%2520wins%2520over%2520the%2520current%2520state%2520of%2520the%2520art%250AAutoAttack%2520on%2520the%2520standard%2520benchmark%2520datasets%2520CIFAR-100%2520and%2520ImageNet%2520and%2520for%2520a%250Avariety%2520of%2520robust%2520models.%2520In%2520particular%252C%2520our%2520attack%2520is%2520%255Cemph%257Bfive%2520times%2520faster%257D%250Athan%2520AutoAttack%252C%2520while%2520successfully%2520matching%2520all%2520of%2520AutoAttack%2527s%2520successes%2520and%250Aattacking%2520additional%2520samples%2520that%2520were%2520previously%2520out%2520of%2520reach.%2520We%2520then%2520prove%250Aformally%2520and%2520demonstrate%2520empirically%2520that%2520our%2520targeting%2520method%252C%2520although%250Ainspired%2520by%2520linear%2520predictors%252C%2520also%2520applies%2520to%2520standard%2520non-linear%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MALT%20Powers%20Up%20Adversarial%20Attacks&entry.906535625=Odelia%20Melamed%20and%20Gilad%20Yehudai%20and%20Adi%20Shamir&entry.1292438233=%20%20Current%20adversarial%20attacks%20for%20multi-class%20classifiers%20choose%20the%20target%0Aclass%20for%20a%20given%20input%20naively%2C%20based%20on%20the%20classifier%27s%20confidence%20levels%0Afor%20various%20target%20classes.%20We%20present%20a%20novel%20adversarial%20targeting%20method%2C%0A%5Ctextit%7BMALT%20-%20Mesoscopic%20Almost%20Linearity%20Targeting%7D%2C%20based%20on%20medium-scale%0Aalmost%20linearity%20assumptions.%20Our%20attack%20wins%20over%20the%20current%20state%20of%20the%20art%0AAutoAttack%20on%20the%20standard%20benchmark%20datasets%20CIFAR-100%20and%20ImageNet%20and%20for%20a%0Avariety%20of%20robust%20models.%20In%20particular%2C%20our%20attack%20is%20%5Cemph%7Bfive%20times%20faster%7D%0Athan%20AutoAttack%2C%20while%20successfully%20matching%20all%20of%20AutoAttack%27s%20successes%20and%0Aattacking%20additional%20samples%20that%20were%20previously%20out%20of%20reach.%20We%20then%20prove%0Aformally%20and%20demonstrate%20empirically%20that%20our%20targeting%20method%2C%20although%0Ainspired%20by%20linear%20predictors%2C%20also%20applies%20to%20standard%20non-linear%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02240v1&entry.124074799=Read"},
{"title": "Forward Learning for Gradient-based Black-box Saliency Map Generation", "author": "Zeliang Zhang and Mingqian Feng and Jinyang Jiang and Rongyi Zhu and Yijie Peng and Chenliang Xu", "abstract": "  Gradient-based saliency maps are widely used to explain deep neural network\ndecisions. However, as models become deeper and more black-box, such as in\nclosed-source APIs like ChatGPT, computing gradients become challenging,\nhindering conventional explanation methods. In this work, we introduce a novel\nunified framework for estimating gradients in black-box settings and generating\nsaliency maps to interpret model decisions. We employ the likelihood ratio\nmethod to estimate output-to-input gradients and utilize them for saliency map\ngeneration. Additionally, we propose blockwise computation techniques to\nenhance estimation accuracy. Extensive experiments in black-box settings\nvalidate the effectiveness of our method, demonstrating accurate gradient\nestimation and explainability of generated saliency maps. Furthermore, we\nshowcase the scalability of our approach by applying it to explain GPT-Vision,\nrevealing the continued relevance of gradient-based explanation methods in the\nera of large, closed-source, and black-box models.\n", "link": "http://arxiv.org/abs/2403.15603v2", "date": "2024-07-02", "relevancy": 2.0618, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5278}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.515}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forward%20Learning%20for%20Gradient-based%20Black-box%20Saliency%20Map%20Generation&body=Title%3A%20Forward%20Learning%20for%20Gradient-based%20Black-box%20Saliency%20Map%20Generation%0AAuthor%3A%20Zeliang%20Zhang%20and%20Mingqian%20Feng%20and%20Jinyang%20Jiang%20and%20Rongyi%20Zhu%20and%20Yijie%20Peng%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Gradient-based%20saliency%20maps%20are%20widely%20used%20to%20explain%20deep%20neural%20network%0Adecisions.%20However%2C%20as%20models%20become%20deeper%20and%20more%20black-box%2C%20such%20as%20in%0Aclosed-source%20APIs%20like%20ChatGPT%2C%20computing%20gradients%20become%20challenging%2C%0Ahindering%20conventional%20explanation%20methods.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aunified%20framework%20for%20estimating%20gradients%20in%20black-box%20settings%20and%20generating%0Asaliency%20maps%20to%20interpret%20model%20decisions.%20We%20employ%20the%20likelihood%20ratio%0Amethod%20to%20estimate%20output-to-input%20gradients%20and%20utilize%20them%20for%20saliency%20map%0Ageneration.%20Additionally%2C%20we%20propose%20blockwise%20computation%20techniques%20to%0Aenhance%20estimation%20accuracy.%20Extensive%20experiments%20in%20black-box%20settings%0Avalidate%20the%20effectiveness%20of%20our%20method%2C%20demonstrating%20accurate%20gradient%0Aestimation%20and%20explainability%20of%20generated%20saliency%20maps.%20Furthermore%2C%20we%0Ashowcase%20the%20scalability%20of%20our%20approach%20by%20applying%20it%20to%20explain%20GPT-Vision%2C%0Arevealing%20the%20continued%20relevance%20of%20gradient-based%20explanation%20methods%20in%20the%0Aera%20of%20large%2C%20closed-source%2C%20and%20black-box%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForward%2520Learning%2520for%2520Gradient-based%2520Black-box%2520Saliency%2520Map%2520Generation%26entry.906535625%3DZeliang%2520Zhang%2520and%2520Mingqian%2520Feng%2520and%2520Jinyang%2520Jiang%2520and%2520Rongyi%2520Zhu%2520and%2520Yijie%2520Peng%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Gradient-based%2520saliency%2520maps%2520are%2520widely%2520used%2520to%2520explain%2520deep%2520neural%2520network%250Adecisions.%2520However%252C%2520as%2520models%2520become%2520deeper%2520and%2520more%2520black-box%252C%2520such%2520as%2520in%250Aclosed-source%2520APIs%2520like%2520ChatGPT%252C%2520computing%2520gradients%2520become%2520challenging%252C%250Ahindering%2520conventional%2520explanation%2520methods.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aunified%2520framework%2520for%2520estimating%2520gradients%2520in%2520black-box%2520settings%2520and%2520generating%250Asaliency%2520maps%2520to%2520interpret%2520model%2520decisions.%2520We%2520employ%2520the%2520likelihood%2520ratio%250Amethod%2520to%2520estimate%2520output-to-input%2520gradients%2520and%2520utilize%2520them%2520for%2520saliency%2520map%250Ageneration.%2520Additionally%252C%2520we%2520propose%2520blockwise%2520computation%2520techniques%2520to%250Aenhance%2520estimation%2520accuracy.%2520Extensive%2520experiments%2520in%2520black-box%2520settings%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520demonstrating%2520accurate%2520gradient%250Aestimation%2520and%2520explainability%2520of%2520generated%2520saliency%2520maps.%2520Furthermore%252C%2520we%250Ashowcase%2520the%2520scalability%2520of%2520our%2520approach%2520by%2520applying%2520it%2520to%2520explain%2520GPT-Vision%252C%250Arevealing%2520the%2520continued%2520relevance%2520of%2520gradient-based%2520explanation%2520methods%2520in%2520the%250Aera%2520of%2520large%252C%2520closed-source%252C%2520and%2520black-box%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forward%20Learning%20for%20Gradient-based%20Black-box%20Saliency%20Map%20Generation&entry.906535625=Zeliang%20Zhang%20and%20Mingqian%20Feng%20and%20Jinyang%20Jiang%20and%20Rongyi%20Zhu%20and%20Yijie%20Peng%20and%20Chenliang%20Xu&entry.1292438233=%20%20Gradient-based%20saliency%20maps%20are%20widely%20used%20to%20explain%20deep%20neural%20network%0Adecisions.%20However%2C%20as%20models%20become%20deeper%20and%20more%20black-box%2C%20such%20as%20in%0Aclosed-source%20APIs%20like%20ChatGPT%2C%20computing%20gradients%20become%20challenging%2C%0Ahindering%20conventional%20explanation%20methods.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aunified%20framework%20for%20estimating%20gradients%20in%20black-box%20settings%20and%20generating%0Asaliency%20maps%20to%20interpret%20model%20decisions.%20We%20employ%20the%20likelihood%20ratio%0Amethod%20to%20estimate%20output-to-input%20gradients%20and%20utilize%20them%20for%20saliency%20map%0Ageneration.%20Additionally%2C%20we%20propose%20blockwise%20computation%20techniques%20to%0Aenhance%20estimation%20accuracy.%20Extensive%20experiments%20in%20black-box%20settings%0Avalidate%20the%20effectiveness%20of%20our%20method%2C%20demonstrating%20accurate%20gradient%0Aestimation%20and%20explainability%20of%20generated%20saliency%20maps.%20Furthermore%2C%20we%0Ashowcase%20the%20scalability%20of%20our%20approach%20by%20applying%20it%20to%20explain%20GPT-Vision%2C%0Arevealing%20the%20continued%20relevance%20of%20gradient-based%20explanation%20methods%20in%20the%0Aera%20of%20large%2C%20closed-source%2C%20and%20black-box%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15603v2&entry.124074799=Read"},
{"title": "Investigating Event-Based Cameras for Video Frame Interpolation in\n  Sports", "author": "Antoine Deckyvere and Anthony Cioppa and Silvio Giancola and Bernard Ghanem and Marc Van Droogenbroeck", "abstract": "  Slow-motion replays provide a thrilling perspective on pivotal moments within\nsports games, offering a fresh and captivating visual experience. However,\ncapturing slow-motion footage typically demands high-tech, expensive cameras\nand infrastructures. Deep learning Video Frame Interpolation (VFI) techniques\nhave emerged as a promising avenue, capable of generating high-speed footage\nfrom regular camera feeds. Moreover, the utilization of event-based cameras has\nrecently gathered attention as they provide valuable motion information between\nframes, further enhancing the VFI performances. In this work, we present a\nfirst investigation of event-based VFI models for generating sports slow-motion\nvideos. Particularly, we design and implement a bi-camera recording setup,\nincluding an RGB and an event-based camera to capture sports videos, to\ntemporally align and spatially register both cameras. Our experimental\nvalidation demonstrates that TimeLens, an off-the-shelf event-based VFI model,\ncan effectively generate slow-motion footage for sports videos. This first\ninvestigation underscores the practical utility of event-based cameras in\nproducing sports slow-motion content and lays the groundwork for future\nresearch endeavors in this domain.\n", "link": "http://arxiv.org/abs/2407.02370v1", "date": "2024-07-02", "relevancy": 2.0458, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.504}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Event-Based%20Cameras%20for%20Video%20Frame%20Interpolation%20in%0A%20%20Sports&body=Title%3A%20Investigating%20Event-Based%20Cameras%20for%20Video%20Frame%20Interpolation%20in%0A%20%20Sports%0AAuthor%3A%20Antoine%20Deckyvere%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem%20and%20Marc%20Van%20Droogenbroeck%0AAbstract%3A%20%20%20Slow-motion%20replays%20provide%20a%20thrilling%20perspective%20on%20pivotal%20moments%20within%0Asports%20games%2C%20offering%20a%20fresh%20and%20captivating%20visual%20experience.%20However%2C%0Acapturing%20slow-motion%20footage%20typically%20demands%20high-tech%2C%20expensive%20cameras%0Aand%20infrastructures.%20Deep%20learning%20Video%20Frame%20Interpolation%20%28VFI%29%20techniques%0Ahave%20emerged%20as%20a%20promising%20avenue%2C%20capable%20of%20generating%20high-speed%20footage%0Afrom%20regular%20camera%20feeds.%20Moreover%2C%20the%20utilization%20of%20event-based%20cameras%20has%0Arecently%20gathered%20attention%20as%20they%20provide%20valuable%20motion%20information%20between%0Aframes%2C%20further%20enhancing%20the%20VFI%20performances.%20In%20this%20work%2C%20we%20present%20a%0Afirst%20investigation%20of%20event-based%20VFI%20models%20for%20generating%20sports%20slow-motion%0Avideos.%20Particularly%2C%20we%20design%20and%20implement%20a%20bi-camera%20recording%20setup%2C%0Aincluding%20an%20RGB%20and%20an%20event-based%20camera%20to%20capture%20sports%20videos%2C%20to%0Atemporally%20align%20and%20spatially%20register%20both%20cameras.%20Our%20experimental%0Avalidation%20demonstrates%20that%20TimeLens%2C%20an%20off-the-shelf%20event-based%20VFI%20model%2C%0Acan%20effectively%20generate%20slow-motion%20footage%20for%20sports%20videos.%20This%20first%0Ainvestigation%20underscores%20the%20practical%20utility%20of%20event-based%20cameras%20in%0Aproducing%20sports%20slow-motion%20content%20and%20lays%20the%20groundwork%20for%20future%0Aresearch%20endeavors%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Event-Based%2520Cameras%2520for%2520Video%2520Frame%2520Interpolation%2520in%250A%2520%2520Sports%26entry.906535625%3DAntoine%2520Deckyvere%2520and%2520Anthony%2520Cioppa%2520and%2520Silvio%2520Giancola%2520and%2520Bernard%2520Ghanem%2520and%2520Marc%2520Van%2520Droogenbroeck%26entry.1292438233%3D%2520%2520Slow-motion%2520replays%2520provide%2520a%2520thrilling%2520perspective%2520on%2520pivotal%2520moments%2520within%250Asports%2520games%252C%2520offering%2520a%2520fresh%2520and%2520captivating%2520visual%2520experience.%2520However%252C%250Acapturing%2520slow-motion%2520footage%2520typically%2520demands%2520high-tech%252C%2520expensive%2520cameras%250Aand%2520infrastructures.%2520Deep%2520learning%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520techniques%250Ahave%2520emerged%2520as%2520a%2520promising%2520avenue%252C%2520capable%2520of%2520generating%2520high-speed%2520footage%250Afrom%2520regular%2520camera%2520feeds.%2520Moreover%252C%2520the%2520utilization%2520of%2520event-based%2520cameras%2520has%250Arecently%2520gathered%2520attention%2520as%2520they%2520provide%2520valuable%2520motion%2520information%2520between%250Aframes%252C%2520further%2520enhancing%2520the%2520VFI%2520performances.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Afirst%2520investigation%2520of%2520event-based%2520VFI%2520models%2520for%2520generating%2520sports%2520slow-motion%250Avideos.%2520Particularly%252C%2520we%2520design%2520and%2520implement%2520a%2520bi-camera%2520recording%2520setup%252C%250Aincluding%2520an%2520RGB%2520and%2520an%2520event-based%2520camera%2520to%2520capture%2520sports%2520videos%252C%2520to%250Atemporally%2520align%2520and%2520spatially%2520register%2520both%2520cameras.%2520Our%2520experimental%250Avalidation%2520demonstrates%2520that%2520TimeLens%252C%2520an%2520off-the-shelf%2520event-based%2520VFI%2520model%252C%250Acan%2520effectively%2520generate%2520slow-motion%2520footage%2520for%2520sports%2520videos.%2520This%2520first%250Ainvestigation%2520underscores%2520the%2520practical%2520utility%2520of%2520event-based%2520cameras%2520in%250Aproducing%2520sports%2520slow-motion%2520content%2520and%2520lays%2520the%2520groundwork%2520for%2520future%250Aresearch%2520endeavors%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Event-Based%20Cameras%20for%20Video%20Frame%20Interpolation%20in%0A%20%20Sports&entry.906535625=Antoine%20Deckyvere%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem%20and%20Marc%20Van%20Droogenbroeck&entry.1292438233=%20%20Slow-motion%20replays%20provide%20a%20thrilling%20perspective%20on%20pivotal%20moments%20within%0Asports%20games%2C%20offering%20a%20fresh%20and%20captivating%20visual%20experience.%20However%2C%0Acapturing%20slow-motion%20footage%20typically%20demands%20high-tech%2C%20expensive%20cameras%0Aand%20infrastructures.%20Deep%20learning%20Video%20Frame%20Interpolation%20%28VFI%29%20techniques%0Ahave%20emerged%20as%20a%20promising%20avenue%2C%20capable%20of%20generating%20high-speed%20footage%0Afrom%20regular%20camera%20feeds.%20Moreover%2C%20the%20utilization%20of%20event-based%20cameras%20has%0Arecently%20gathered%20attention%20as%20they%20provide%20valuable%20motion%20information%20between%0Aframes%2C%20further%20enhancing%20the%20VFI%20performances.%20In%20this%20work%2C%20we%20present%20a%0Afirst%20investigation%20of%20event-based%20VFI%20models%20for%20generating%20sports%20slow-motion%0Avideos.%20Particularly%2C%20we%20design%20and%20implement%20a%20bi-camera%20recording%20setup%2C%0Aincluding%20an%20RGB%20and%20an%20event-based%20camera%20to%20capture%20sports%20videos%2C%20to%0Atemporally%20align%20and%20spatially%20register%20both%20cameras.%20Our%20experimental%0Avalidation%20demonstrates%20that%20TimeLens%2C%20an%20off-the-shelf%20event-based%20VFI%20model%2C%0Acan%20effectively%20generate%20slow-motion%20footage%20for%20sports%20videos.%20This%20first%0Ainvestigation%20underscores%20the%20practical%20utility%20of%20event-based%20cameras%20in%0Aproducing%20sports%20slow-motion%20content%20and%20lays%20the%20groundwork%20for%20future%0Aresearch%20endeavors%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02370v1&entry.124074799=Read"},
{"title": "SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring\n  Expression Segmentation", "author": "Sayan Nag and Koustava Goswami and Srikrishna Karanam", "abstract": "  Referring Expression Segmentation (RES) aims to provide a segmentation mask\nof the target object in an image referred to by the text (i.e., referring\nexpression). Existing methods require large-scale mask annotations. Moreover,\nsuch approaches do not generalize well to unseen/zero-shot scenarios. To\naddress the aforementioned issues, we propose a weakly-supervised bootstrapping\narchitecture for RES with several new algorithmic innovations. To the best of\nour knowledge, ours is the first approach that considers only a fraction of\nboth mask and box annotations (shown in Figure 1 and Table 1) for training. To\nenable principled training of models in such low-annotation settings, improve\nimage-text region-level alignment, and further enhance spatial localization of\nthe target object in the image, we propose Cross-modal Fusion with Attention\nConsistency module. For automatic pseudo-labeling of unlabeled samples, we\nintroduce a novel Mask Validity Filtering routine based on a spatially aware\nzero-shot proposal scoring approach. Extensive experiments show that with just\n30% annotations, our model SafaRi achieves 59.31 and 48.26 mIoUs as compared to\n58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTR\nrespectively on RefCOCO+@testA and RefCOCO+testB datasets. SafaRi also\noutperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in a\nfully-supervised setting and demonstrates strong generalization capabilities in\nunseen/zero-shot tasks.\n", "link": "http://arxiv.org/abs/2407.02389v1", "date": "2024-07-02", "relevancy": 2.0449, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5084}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafaRi%3AAdaptive%20Sequence%20Transformer%20for%20Weakly%20Supervised%20Referring%0A%20%20Expression%20Segmentation&body=Title%3A%20SafaRi%3AAdaptive%20Sequence%20Transformer%20for%20Weakly%20Supervised%20Referring%0A%20%20Expression%20Segmentation%0AAuthor%3A%20Sayan%20Nag%20and%20Koustava%20Goswami%20and%20Srikrishna%20Karanam%0AAbstract%3A%20%20%20Referring%20Expression%20Segmentation%20%28RES%29%20aims%20to%20provide%20a%20segmentation%20mask%0Aof%20the%20target%20object%20in%20an%20image%20referred%20to%20by%20the%20text%20%28i.e.%2C%20referring%0Aexpression%29.%20Existing%20methods%20require%20large-scale%20mask%20annotations.%20Moreover%2C%0Asuch%20approaches%20do%20not%20generalize%20well%20to%20unseen/zero-shot%20scenarios.%20To%0Aaddress%20the%20aforementioned%20issues%2C%20we%20propose%20a%20weakly-supervised%20bootstrapping%0Aarchitecture%20for%20RES%20with%20several%20new%20algorithmic%20innovations.%20To%20the%20best%20of%0Aour%20knowledge%2C%20ours%20is%20the%20first%20approach%20that%20considers%20only%20a%20fraction%20of%0Aboth%20mask%20and%20box%20annotations%20%28shown%20in%20Figure%201%20and%20Table%201%29%20for%20training.%20To%0Aenable%20principled%20training%20of%20models%20in%20such%20low-annotation%20settings%2C%20improve%0Aimage-text%20region-level%20alignment%2C%20and%20further%20enhance%20spatial%20localization%20of%0Athe%20target%20object%20in%20the%20image%2C%20we%20propose%20Cross-modal%20Fusion%20with%20Attention%0AConsistency%20module.%20For%20automatic%20pseudo-labeling%20of%20unlabeled%20samples%2C%20we%0Aintroduce%20a%20novel%20Mask%20Validity%20Filtering%20routine%20based%20on%20a%20spatially%20aware%0Azero-shot%20proposal%20scoring%20approach.%20Extensive%20experiments%20show%20that%20with%20just%0A30%25%20annotations%2C%20our%20model%20SafaRi%20achieves%2059.31%20and%2048.26%20mIoUs%20as%20compared%20to%0A58.93%20and%2048.19%20mIoUs%20obtained%20by%20the%20fully-supervised%20SOTA%20method%20SeqTR%0Arespectively%20on%20RefCOCO%2B%40testA%20and%20RefCOCO%2BtestB%20datasets.%20SafaRi%20also%0Aoutperforms%20SeqTR%20by%2011.7%25%20%28on%20RefCOCO%2BtestA%29%20and%2019.6%25%20%28on%20RefCOCO%2BtestB%29%20in%20a%0Afully-supervised%20setting%20and%20demonstrates%20strong%20generalization%20capabilities%20in%0Aunseen/zero-shot%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafaRi%253AAdaptive%2520Sequence%2520Transformer%2520for%2520Weakly%2520Supervised%2520Referring%250A%2520%2520Expression%2520Segmentation%26entry.906535625%3DSayan%2520Nag%2520and%2520Koustava%2520Goswami%2520and%2520Srikrishna%2520Karanam%26entry.1292438233%3D%2520%2520Referring%2520Expression%2520Segmentation%2520%2528RES%2529%2520aims%2520to%2520provide%2520a%2520segmentation%2520mask%250Aof%2520the%2520target%2520object%2520in%2520an%2520image%2520referred%2520to%2520by%2520the%2520text%2520%2528i.e.%252C%2520referring%250Aexpression%2529.%2520Existing%2520methods%2520require%2520large-scale%2520mask%2520annotations.%2520Moreover%252C%250Asuch%2520approaches%2520do%2520not%2520generalize%2520well%2520to%2520unseen/zero-shot%2520scenarios.%2520To%250Aaddress%2520the%2520aforementioned%2520issues%252C%2520we%2520propose%2520a%2520weakly-supervised%2520bootstrapping%250Aarchitecture%2520for%2520RES%2520with%2520several%2520new%2520algorithmic%2520innovations.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520ours%2520is%2520the%2520first%2520approach%2520that%2520considers%2520only%2520a%2520fraction%2520of%250Aboth%2520mask%2520and%2520box%2520annotations%2520%2528shown%2520in%2520Figure%25201%2520and%2520Table%25201%2529%2520for%2520training.%2520To%250Aenable%2520principled%2520training%2520of%2520models%2520in%2520such%2520low-annotation%2520settings%252C%2520improve%250Aimage-text%2520region-level%2520alignment%252C%2520and%2520further%2520enhance%2520spatial%2520localization%2520of%250Athe%2520target%2520object%2520in%2520the%2520image%252C%2520we%2520propose%2520Cross-modal%2520Fusion%2520with%2520Attention%250AConsistency%2520module.%2520For%2520automatic%2520pseudo-labeling%2520of%2520unlabeled%2520samples%252C%2520we%250Aintroduce%2520a%2520novel%2520Mask%2520Validity%2520Filtering%2520routine%2520based%2520on%2520a%2520spatially%2520aware%250Azero-shot%2520proposal%2520scoring%2520approach.%2520Extensive%2520experiments%2520show%2520that%2520with%2520just%250A30%2525%2520annotations%252C%2520our%2520model%2520SafaRi%2520achieves%252059.31%2520and%252048.26%2520mIoUs%2520as%2520compared%2520to%250A58.93%2520and%252048.19%2520mIoUs%2520obtained%2520by%2520the%2520fully-supervised%2520SOTA%2520method%2520SeqTR%250Arespectively%2520on%2520RefCOCO%252B%2540testA%2520and%2520RefCOCO%252BtestB%2520datasets.%2520SafaRi%2520also%250Aoutperforms%2520SeqTR%2520by%252011.7%2525%2520%2528on%2520RefCOCO%252BtestA%2529%2520and%252019.6%2525%2520%2528on%2520RefCOCO%252BtestB%2529%2520in%2520a%250Afully-supervised%2520setting%2520and%2520demonstrates%2520strong%2520generalization%2520capabilities%2520in%250Aunseen/zero-shot%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafaRi%3AAdaptive%20Sequence%20Transformer%20for%20Weakly%20Supervised%20Referring%0A%20%20Expression%20Segmentation&entry.906535625=Sayan%20Nag%20and%20Koustava%20Goswami%20and%20Srikrishna%20Karanam&entry.1292438233=%20%20Referring%20Expression%20Segmentation%20%28RES%29%20aims%20to%20provide%20a%20segmentation%20mask%0Aof%20the%20target%20object%20in%20an%20image%20referred%20to%20by%20the%20text%20%28i.e.%2C%20referring%0Aexpression%29.%20Existing%20methods%20require%20large-scale%20mask%20annotations.%20Moreover%2C%0Asuch%20approaches%20do%20not%20generalize%20well%20to%20unseen/zero-shot%20scenarios.%20To%0Aaddress%20the%20aforementioned%20issues%2C%20we%20propose%20a%20weakly-supervised%20bootstrapping%0Aarchitecture%20for%20RES%20with%20several%20new%20algorithmic%20innovations.%20To%20the%20best%20of%0Aour%20knowledge%2C%20ours%20is%20the%20first%20approach%20that%20considers%20only%20a%20fraction%20of%0Aboth%20mask%20and%20box%20annotations%20%28shown%20in%20Figure%201%20and%20Table%201%29%20for%20training.%20To%0Aenable%20principled%20training%20of%20models%20in%20such%20low-annotation%20settings%2C%20improve%0Aimage-text%20region-level%20alignment%2C%20and%20further%20enhance%20spatial%20localization%20of%0Athe%20target%20object%20in%20the%20image%2C%20we%20propose%20Cross-modal%20Fusion%20with%20Attention%0AConsistency%20module.%20For%20automatic%20pseudo-labeling%20of%20unlabeled%20samples%2C%20we%0Aintroduce%20a%20novel%20Mask%20Validity%20Filtering%20routine%20based%20on%20a%20spatially%20aware%0Azero-shot%20proposal%20scoring%20approach.%20Extensive%20experiments%20show%20that%20with%20just%0A30%25%20annotations%2C%20our%20model%20SafaRi%20achieves%2059.31%20and%2048.26%20mIoUs%20as%20compared%20to%0A58.93%20and%2048.19%20mIoUs%20obtained%20by%20the%20fully-supervised%20SOTA%20method%20SeqTR%0Arespectively%20on%20RefCOCO%2B%40testA%20and%20RefCOCO%2BtestB%20datasets.%20SafaRi%20also%0Aoutperforms%20SeqTR%20by%2011.7%25%20%28on%20RefCOCO%2BtestA%29%20and%2019.6%25%20%28on%20RefCOCO%2BtestB%29%20in%20a%0Afully-supervised%20setting%20and%20demonstrates%20strong%20generalization%20capabilities%20in%0Aunseen/zero-shot%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02389v1&entry.124074799=Read"},
{"title": "Improving Explainability of Softmax Classifiers Using a Prototype-Based\n  Joint Embedding Method", "author": "Hilarie Sit and Brendan Keith and Karianne Bergen", "abstract": "  We propose a prototype-based approach for improving explainability of softmax\nclassifiers that provides an understandable prediction confidence, generated\nthrough stochastic sampling of prototypes, and demonstrates potential for out\nof distribution detection (OOD). By modifying the model architecture and\ntraining to make predictions using similarities to any set of class examples\nfrom the training dataset, we acquire the ability to sample for prototypical\nexamples that contributed to the prediction, which provide an instance-based\nexplanation for the model's decision. Furthermore, by learning relationships\nbetween images from the training dataset through relative distances within the\nmodel's latent space, we obtain a metric for uncertainty that is better able to\ndetect out of distribution data than softmax confidence.\n", "link": "http://arxiv.org/abs/2407.02271v1", "date": "2024-07-02", "relevancy": 2.0394, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5088}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Explainability%20of%20Softmax%20Classifiers%20Using%20a%20Prototype-Based%0A%20%20Joint%20Embedding%20Method&body=Title%3A%20Improving%20Explainability%20of%20Softmax%20Classifiers%20Using%20a%20Prototype-Based%0A%20%20Joint%20Embedding%20Method%0AAuthor%3A%20Hilarie%20Sit%20and%20Brendan%20Keith%20and%20Karianne%20Bergen%0AAbstract%3A%20%20%20We%20propose%20a%20prototype-based%20approach%20for%20improving%20explainability%20of%20softmax%0Aclassifiers%20that%20provides%20an%20understandable%20prediction%20confidence%2C%20generated%0Athrough%20stochastic%20sampling%20of%20prototypes%2C%20and%20demonstrates%20potential%20for%20out%0Aof%20distribution%20detection%20%28OOD%29.%20By%20modifying%20the%20model%20architecture%20and%0Atraining%20to%20make%20predictions%20using%20similarities%20to%20any%20set%20of%20class%20examples%0Afrom%20the%20training%20dataset%2C%20we%20acquire%20the%20ability%20to%20sample%20for%20prototypical%0Aexamples%20that%20contributed%20to%20the%20prediction%2C%20which%20provide%20an%20instance-based%0Aexplanation%20for%20the%20model%27s%20decision.%20Furthermore%2C%20by%20learning%20relationships%0Abetween%20images%20from%20the%20training%20dataset%20through%20relative%20distances%20within%20the%0Amodel%27s%20latent%20space%2C%20we%20obtain%20a%20metric%20for%20uncertainty%20that%20is%20better%20able%20to%0Adetect%20out%20of%20distribution%20data%20than%20softmax%20confidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Explainability%2520of%2520Softmax%2520Classifiers%2520Using%2520a%2520Prototype-Based%250A%2520%2520Joint%2520Embedding%2520Method%26entry.906535625%3DHilarie%2520Sit%2520and%2520Brendan%2520Keith%2520and%2520Karianne%2520Bergen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520prototype-based%2520approach%2520for%2520improving%2520explainability%2520of%2520softmax%250Aclassifiers%2520that%2520provides%2520an%2520understandable%2520prediction%2520confidence%252C%2520generated%250Athrough%2520stochastic%2520sampling%2520of%2520prototypes%252C%2520and%2520demonstrates%2520potential%2520for%2520out%250Aof%2520distribution%2520detection%2520%2528OOD%2529.%2520By%2520modifying%2520the%2520model%2520architecture%2520and%250Atraining%2520to%2520make%2520predictions%2520using%2520similarities%2520to%2520any%2520set%2520of%2520class%2520examples%250Afrom%2520the%2520training%2520dataset%252C%2520we%2520acquire%2520the%2520ability%2520to%2520sample%2520for%2520prototypical%250Aexamples%2520that%2520contributed%2520to%2520the%2520prediction%252C%2520which%2520provide%2520an%2520instance-based%250Aexplanation%2520for%2520the%2520model%2527s%2520decision.%2520Furthermore%252C%2520by%2520learning%2520relationships%250Abetween%2520images%2520from%2520the%2520training%2520dataset%2520through%2520relative%2520distances%2520within%2520the%250Amodel%2527s%2520latent%2520space%252C%2520we%2520obtain%2520a%2520metric%2520for%2520uncertainty%2520that%2520is%2520better%2520able%2520to%250Adetect%2520out%2520of%2520distribution%2520data%2520than%2520softmax%2520confidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Explainability%20of%20Softmax%20Classifiers%20Using%20a%20Prototype-Based%0A%20%20Joint%20Embedding%20Method&entry.906535625=Hilarie%20Sit%20and%20Brendan%20Keith%20and%20Karianne%20Bergen&entry.1292438233=%20%20We%20propose%20a%20prototype-based%20approach%20for%20improving%20explainability%20of%20softmax%0Aclassifiers%20that%20provides%20an%20understandable%20prediction%20confidence%2C%20generated%0Athrough%20stochastic%20sampling%20of%20prototypes%2C%20and%20demonstrates%20potential%20for%20out%0Aof%20distribution%20detection%20%28OOD%29.%20By%20modifying%20the%20model%20architecture%20and%0Atraining%20to%20make%20predictions%20using%20similarities%20to%20any%20set%20of%20class%20examples%0Afrom%20the%20training%20dataset%2C%20we%20acquire%20the%20ability%20to%20sample%20for%20prototypical%0Aexamples%20that%20contributed%20to%20the%20prediction%2C%20which%20provide%20an%20instance-based%0Aexplanation%20for%20the%20model%27s%20decision.%20Furthermore%2C%20by%20learning%20relationships%0Abetween%20images%20from%20the%20training%20dataset%20through%20relative%20distances%20within%20the%0Amodel%27s%20latent%20space%2C%20we%20obtain%20a%20metric%20for%20uncertainty%20that%20is%20better%20able%20to%0Adetect%20out%20of%20distribution%20data%20than%20softmax%20confidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02271v1&entry.124074799=Read"},
{"title": "Enabling Large Batch Size Training for DNN Models Beyond the Memory\n  Limit While Maintaining Performance", "author": "XinYu Piao and DoangJoo Synn and JooYoung Park and Jong-Kook Kim", "abstract": "  Recent deep learning models are difficult to train using a large batch size,\nbecause commodity machines may not have enough memory to accommodate both the\nmodel and a large data batch size. The batch size is one of the\nhyper-parameters used in the training model, and it is dependent on and is\nlimited by the target machine memory capacity because the batch size can only\nfit into the remaining memory after the model is uploaded. Moreover, the data\nitem size is also an important factor because if each data item size is larger\nthen the batch size that can fit into the remaining memory becomes smaller.\nThis paper proposes a method called Micro-Batch Processing (MBP) to address\nthis problem. This method helps deep learning models to train by providing a\nbatch processing method that splits a batch into a size that can fit in the\nremaining memory and processes them sequentially. After processing the small\nbatches individually, a loss normalization algorithm based on the gradient\naccumulation is used to maintain the performance. The purpose of our method is\nto allow deep learning models to train using larger batch sizes that exceed the\nmemory capacity of a system without increasing the memory size or using\nmultiple devices (GPUs).\n", "link": "http://arxiv.org/abs/2110.12484v3", "date": "2024-07-02", "relevancy": 2.0311, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5158}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Large%20Batch%20Size%20Training%20for%20DNN%20Models%20Beyond%20the%20Memory%0A%20%20Limit%20While%20Maintaining%20Performance&body=Title%3A%20Enabling%20Large%20Batch%20Size%20Training%20for%20DNN%20Models%20Beyond%20the%20Memory%0A%20%20Limit%20While%20Maintaining%20Performance%0AAuthor%3A%20XinYu%20Piao%20and%20DoangJoo%20Synn%20and%20JooYoung%20Park%20and%20Jong-Kook%20Kim%0AAbstract%3A%20%20%20Recent%20deep%20learning%20models%20are%20difficult%20to%20train%20using%20a%20large%20batch%20size%2C%0Abecause%20commodity%20machines%20may%20not%20have%20enough%20memory%20to%20accommodate%20both%20the%0Amodel%20and%20a%20large%20data%20batch%20size.%20The%20batch%20size%20is%20one%20of%20the%0Ahyper-parameters%20used%20in%20the%20training%20model%2C%20and%20it%20is%20dependent%20on%20and%20is%0Alimited%20by%20the%20target%20machine%20memory%20capacity%20because%20the%20batch%20size%20can%20only%0Afit%20into%20the%20remaining%20memory%20after%20the%20model%20is%20uploaded.%20Moreover%2C%20the%20data%0Aitem%20size%20is%20also%20an%20important%20factor%20because%20if%20each%20data%20item%20size%20is%20larger%0Athen%20the%20batch%20size%20that%20can%20fit%20into%20the%20remaining%20memory%20becomes%20smaller.%0AThis%20paper%20proposes%20a%20method%20called%20Micro-Batch%20Processing%20%28MBP%29%20to%20address%0Athis%20problem.%20This%20method%20helps%20deep%20learning%20models%20to%20train%20by%20providing%20a%0Abatch%20processing%20method%20that%20splits%20a%20batch%20into%20a%20size%20that%20can%20fit%20in%20the%0Aremaining%20memory%20and%20processes%20them%20sequentially.%20After%20processing%20the%20small%0Abatches%20individually%2C%20a%20loss%20normalization%20algorithm%20based%20on%20the%20gradient%0Aaccumulation%20is%20used%20to%20maintain%20the%20performance.%20The%20purpose%20of%20our%20method%20is%0Ato%20allow%20deep%20learning%20models%20to%20train%20using%20larger%20batch%20sizes%20that%20exceed%20the%0Amemory%20capacity%20of%20a%20system%20without%20increasing%20the%20memory%20size%20or%20using%0Amultiple%20devices%20%28GPUs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2110.12484v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Large%2520Batch%2520Size%2520Training%2520for%2520DNN%2520Models%2520Beyond%2520the%2520Memory%250A%2520%2520Limit%2520While%2520Maintaining%2520Performance%26entry.906535625%3DXinYu%2520Piao%2520and%2520DoangJoo%2520Synn%2520and%2520JooYoung%2520Park%2520and%2520Jong-Kook%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520deep%2520learning%2520models%2520are%2520difficult%2520to%2520train%2520using%2520a%2520large%2520batch%2520size%252C%250Abecause%2520commodity%2520machines%2520may%2520not%2520have%2520enough%2520memory%2520to%2520accommodate%2520both%2520the%250Amodel%2520and%2520a%2520large%2520data%2520batch%2520size.%2520The%2520batch%2520size%2520is%2520one%2520of%2520the%250Ahyper-parameters%2520used%2520in%2520the%2520training%2520model%252C%2520and%2520it%2520is%2520dependent%2520on%2520and%2520is%250Alimited%2520by%2520the%2520target%2520machine%2520memory%2520capacity%2520because%2520the%2520batch%2520size%2520can%2520only%250Afit%2520into%2520the%2520remaining%2520memory%2520after%2520the%2520model%2520is%2520uploaded.%2520Moreover%252C%2520the%2520data%250Aitem%2520size%2520is%2520also%2520an%2520important%2520factor%2520because%2520if%2520each%2520data%2520item%2520size%2520is%2520larger%250Athen%2520the%2520batch%2520size%2520that%2520can%2520fit%2520into%2520the%2520remaining%2520memory%2520becomes%2520smaller.%250AThis%2520paper%2520proposes%2520a%2520method%2520called%2520Micro-Batch%2520Processing%2520%2528MBP%2529%2520to%2520address%250Athis%2520problem.%2520This%2520method%2520helps%2520deep%2520learning%2520models%2520to%2520train%2520by%2520providing%2520a%250Abatch%2520processing%2520method%2520that%2520splits%2520a%2520batch%2520into%2520a%2520size%2520that%2520can%2520fit%2520in%2520the%250Aremaining%2520memory%2520and%2520processes%2520them%2520sequentially.%2520After%2520processing%2520the%2520small%250Abatches%2520individually%252C%2520a%2520loss%2520normalization%2520algorithm%2520based%2520on%2520the%2520gradient%250Aaccumulation%2520is%2520used%2520to%2520maintain%2520the%2520performance.%2520The%2520purpose%2520of%2520our%2520method%2520is%250Ato%2520allow%2520deep%2520learning%2520models%2520to%2520train%2520using%2520larger%2520batch%2520sizes%2520that%2520exceed%2520the%250Amemory%2520capacity%2520of%2520a%2520system%2520without%2520increasing%2520the%2520memory%2520size%2520or%2520using%250Amultiple%2520devices%2520%2528GPUs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2110.12484v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Large%20Batch%20Size%20Training%20for%20DNN%20Models%20Beyond%20the%20Memory%0A%20%20Limit%20While%20Maintaining%20Performance&entry.906535625=XinYu%20Piao%20and%20DoangJoo%20Synn%20and%20JooYoung%20Park%20and%20Jong-Kook%20Kim&entry.1292438233=%20%20Recent%20deep%20learning%20models%20are%20difficult%20to%20train%20using%20a%20large%20batch%20size%2C%0Abecause%20commodity%20machines%20may%20not%20have%20enough%20memory%20to%20accommodate%20both%20the%0Amodel%20and%20a%20large%20data%20batch%20size.%20The%20batch%20size%20is%20one%20of%20the%0Ahyper-parameters%20used%20in%20the%20training%20model%2C%20and%20it%20is%20dependent%20on%20and%20is%0Alimited%20by%20the%20target%20machine%20memory%20capacity%20because%20the%20batch%20size%20can%20only%0Afit%20into%20the%20remaining%20memory%20after%20the%20model%20is%20uploaded.%20Moreover%2C%20the%20data%0Aitem%20size%20is%20also%20an%20important%20factor%20because%20if%20each%20data%20item%20size%20is%20larger%0Athen%20the%20batch%20size%20that%20can%20fit%20into%20the%20remaining%20memory%20becomes%20smaller.%0AThis%20paper%20proposes%20a%20method%20called%20Micro-Batch%20Processing%20%28MBP%29%20to%20address%0Athis%20problem.%20This%20method%20helps%20deep%20learning%20models%20to%20train%20by%20providing%20a%0Abatch%20processing%20method%20that%20splits%20a%20batch%20into%20a%20size%20that%20can%20fit%20in%20the%0Aremaining%20memory%20and%20processes%20them%20sequentially.%20After%20processing%20the%20small%0Abatches%20individually%2C%20a%20loss%20normalization%20algorithm%20based%20on%20the%20gradient%0Aaccumulation%20is%20used%20to%20maintain%20the%20performance.%20The%20purpose%20of%20our%20method%20is%0Ato%20allow%20deep%20learning%20models%20to%20train%20using%20larger%20batch%20sizes%20that%20exceed%20the%0Amemory%20capacity%20of%20a%20system%20without%20increasing%20the%20memory%20size%20or%20using%0Amultiple%20devices%20%28GPUs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2110.12484v3&entry.124074799=Read"},
{"title": "Similarity Distance-Based Label Assignment for Tiny Object Detection", "author": "Shuohao Shi and Qiang Fang and Tong Zhao and Xin Xu", "abstract": "  Tiny object detection is becoming one of the most challenging tasks in\ncomputer vision because of the limited object size and lack of information. The\nlabel assignment strategy is a key factor affecting the accuracy of object\ndetection. Although there are some effective label assignment strategies for\ntiny objects, most of them focus on reducing the sensitivity to the bounding\nboxes to increase the number of positive samples and have some fixed\nhyperparameters need to set. However, more positive samples may not necessarily\nlead to better detection results, in fact, excessive positive samples may lead\nto more false positives. In this paper, we introduce a simple but effective\nstrategy named the Similarity Distance (SimD) to evaluate the similarity\nbetween bounding boxes. This proposed strategy not only considers both location\nand shape similarity but also learns hyperparameters adaptively, ensuring that\nit can adapt to different datasets and various object sizes in a dataset. Our\napproach can be simply applied in common anchor-based detectors in place of the\nIoU for label assignment and Non Maximum Suppression (NMS). Extensive\nexperiments on four mainstream tiny object detection datasets demonstrate\nsuperior performance of our method, especially, 1.8 AP points and 4.1 AP points\nof very tiny higher than the state-of-the-art competitors on AI-TOD. Code is\navailable at: \\url{https://github.com/cszzshi/SimD}.\n", "link": "http://arxiv.org/abs/2407.02394v1", "date": "2024-07-02", "relevancy": 2.0188, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5091}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Similarity%20Distance-Based%20Label%20Assignment%20for%20Tiny%20Object%20Detection&body=Title%3A%20Similarity%20Distance-Based%20Label%20Assignment%20for%20Tiny%20Object%20Detection%0AAuthor%3A%20Shuohao%20Shi%20and%20Qiang%20Fang%20and%20Tong%20Zhao%20and%20Xin%20Xu%0AAbstract%3A%20%20%20Tiny%20object%20detection%20is%20becoming%20one%20of%20the%20most%20challenging%20tasks%20in%0Acomputer%20vision%20because%20of%20the%20limited%20object%20size%20and%20lack%20of%20information.%20The%0Alabel%20assignment%20strategy%20is%20a%20key%20factor%20affecting%20the%20accuracy%20of%20object%0Adetection.%20Although%20there%20are%20some%20effective%20label%20assignment%20strategies%20for%0Atiny%20objects%2C%20most%20of%20them%20focus%20on%20reducing%20the%20sensitivity%20to%20the%20bounding%0Aboxes%20to%20increase%20the%20number%20of%20positive%20samples%20and%20have%20some%20fixed%0Ahyperparameters%20need%20to%20set.%20However%2C%20more%20positive%20samples%20may%20not%20necessarily%0Alead%20to%20better%20detection%20results%2C%20in%20fact%2C%20excessive%20positive%20samples%20may%20lead%0Ato%20more%20false%20positives.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%20but%20effective%0Astrategy%20named%20the%20Similarity%20Distance%20%28SimD%29%20to%20evaluate%20the%20similarity%0Abetween%20bounding%20boxes.%20This%20proposed%20strategy%20not%20only%20considers%20both%20location%0Aand%20shape%20similarity%20but%20also%20learns%20hyperparameters%20adaptively%2C%20ensuring%20that%0Ait%20can%20adapt%20to%20different%20datasets%20and%20various%20object%20sizes%20in%20a%20dataset.%20Our%0Aapproach%20can%20be%20simply%20applied%20in%20common%20anchor-based%20detectors%20in%20place%20of%20the%0AIoU%20for%20label%20assignment%20and%20Non%20Maximum%20Suppression%20%28NMS%29.%20Extensive%0Aexperiments%20on%20four%20mainstream%20tiny%20object%20detection%20datasets%20demonstrate%0Asuperior%20performance%20of%20our%20method%2C%20especially%2C%201.8%20AP%20points%20and%204.1%20AP%20points%0Aof%20very%20tiny%20higher%20than%20the%20state-of-the-art%20competitors%20on%20AI-TOD.%20Code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/cszzshi/SimD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimilarity%2520Distance-Based%2520Label%2520Assignment%2520for%2520Tiny%2520Object%2520Detection%26entry.906535625%3DShuohao%2520Shi%2520and%2520Qiang%2520Fang%2520and%2520Tong%2520Zhao%2520and%2520Xin%2520Xu%26entry.1292438233%3D%2520%2520Tiny%2520object%2520detection%2520is%2520becoming%2520one%2520of%2520the%2520most%2520challenging%2520tasks%2520in%250Acomputer%2520vision%2520because%2520of%2520the%2520limited%2520object%2520size%2520and%2520lack%2520of%2520information.%2520The%250Alabel%2520assignment%2520strategy%2520is%2520a%2520key%2520factor%2520affecting%2520the%2520accuracy%2520of%2520object%250Adetection.%2520Although%2520there%2520are%2520some%2520effective%2520label%2520assignment%2520strategies%2520for%250Atiny%2520objects%252C%2520most%2520of%2520them%2520focus%2520on%2520reducing%2520the%2520sensitivity%2520to%2520the%2520bounding%250Aboxes%2520to%2520increase%2520the%2520number%2520of%2520positive%2520samples%2520and%2520have%2520some%2520fixed%250Ahyperparameters%2520need%2520to%2520set.%2520However%252C%2520more%2520positive%2520samples%2520may%2520not%2520necessarily%250Alead%2520to%2520better%2520detection%2520results%252C%2520in%2520fact%252C%2520excessive%2520positive%2520samples%2520may%2520lead%250Ato%2520more%2520false%2520positives.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520simple%2520but%2520effective%250Astrategy%2520named%2520the%2520Similarity%2520Distance%2520%2528SimD%2529%2520to%2520evaluate%2520the%2520similarity%250Abetween%2520bounding%2520boxes.%2520This%2520proposed%2520strategy%2520not%2520only%2520considers%2520both%2520location%250Aand%2520shape%2520similarity%2520but%2520also%2520learns%2520hyperparameters%2520adaptively%252C%2520ensuring%2520that%250Ait%2520can%2520adapt%2520to%2520different%2520datasets%2520and%2520various%2520object%2520sizes%2520in%2520a%2520dataset.%2520Our%250Aapproach%2520can%2520be%2520simply%2520applied%2520in%2520common%2520anchor-based%2520detectors%2520in%2520place%2520of%2520the%250AIoU%2520for%2520label%2520assignment%2520and%2520Non%2520Maximum%2520Suppression%2520%2528NMS%2529.%2520Extensive%250Aexperiments%2520on%2520four%2520mainstream%2520tiny%2520object%2520detection%2520datasets%2520demonstrate%250Asuperior%2520performance%2520of%2520our%2520method%252C%2520especially%252C%25201.8%2520AP%2520points%2520and%25204.1%2520AP%2520points%250Aof%2520very%2520tiny%2520higher%2520than%2520the%2520state-of-the-art%2520competitors%2520on%2520AI-TOD.%2520Code%2520is%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/cszzshi/SimD%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity%20Distance-Based%20Label%20Assignment%20for%20Tiny%20Object%20Detection&entry.906535625=Shuohao%20Shi%20and%20Qiang%20Fang%20and%20Tong%20Zhao%20and%20Xin%20Xu&entry.1292438233=%20%20Tiny%20object%20detection%20is%20becoming%20one%20of%20the%20most%20challenging%20tasks%20in%0Acomputer%20vision%20because%20of%20the%20limited%20object%20size%20and%20lack%20of%20information.%20The%0Alabel%20assignment%20strategy%20is%20a%20key%20factor%20affecting%20the%20accuracy%20of%20object%0Adetection.%20Although%20there%20are%20some%20effective%20label%20assignment%20strategies%20for%0Atiny%20objects%2C%20most%20of%20them%20focus%20on%20reducing%20the%20sensitivity%20to%20the%20bounding%0Aboxes%20to%20increase%20the%20number%20of%20positive%20samples%20and%20have%20some%20fixed%0Ahyperparameters%20need%20to%20set.%20However%2C%20more%20positive%20samples%20may%20not%20necessarily%0Alead%20to%20better%20detection%20results%2C%20in%20fact%2C%20excessive%20positive%20samples%20may%20lead%0Ato%20more%20false%20positives.%20In%20this%20paper%2C%20we%20introduce%20a%20simple%20but%20effective%0Astrategy%20named%20the%20Similarity%20Distance%20%28SimD%29%20to%20evaluate%20the%20similarity%0Abetween%20bounding%20boxes.%20This%20proposed%20strategy%20not%20only%20considers%20both%20location%0Aand%20shape%20similarity%20but%20also%20learns%20hyperparameters%20adaptively%2C%20ensuring%20that%0Ait%20can%20adapt%20to%20different%20datasets%20and%20various%20object%20sizes%20in%20a%20dataset.%20Our%0Aapproach%20can%20be%20simply%20applied%20in%20common%20anchor-based%20detectors%20in%20place%20of%20the%0AIoU%20for%20label%20assignment%20and%20Non%20Maximum%20Suppression%20%28NMS%29.%20Extensive%0Aexperiments%20on%20four%20mainstream%20tiny%20object%20detection%20datasets%20demonstrate%0Asuperior%20performance%20of%20our%20method%2C%20especially%2C%201.8%20AP%20points%20and%204.1%20AP%20points%0Aof%20very%20tiny%20higher%20than%20the%20state-of-the-art%20competitors%20on%20AI-TOD.%20Code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/cszzshi/SimD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02394v1&entry.124074799=Read"},
{"title": "Universal Semi-Supervised Learning for Medical Image Classification", "author": "Lie Ju and Yicheng Wu and Wei Feng and Zhen Yu and Lin Wang and Zhuoting Zhu and Zongyuan Ge", "abstract": "  Semi-supervised learning (SSL) has attracted much attention since it reduces\nthe expensive costs of collecting adequate well-labeled training data,\nespecially for deep learning methods. However, traditional SSL is built upon an\nassumption that labeled and unlabeled data should be from the same distribution\n\\textit{e.g.,} classes and domains. However, in practical scenarios, unlabeled\ndata would be from unseen classes or unseen domains, and it is still\nchallenging to exploit them by existing SSL methods. Therefore, in this paper,\nwe proposed a unified framework to leverage these unseen unlabeled data for\nopen-scenario semi-supervised medical image classification. We first design a\nnovel scoring mechanism, called dual-path outliers estimation, to identify\nsamples from unseen classes. Meanwhile, to extract unseen-domain samples, we\nthen apply an effective variational autoencoder (VAE) pre-training. After that,\nwe conduct domain adaptation to fully exploit the value of the detected\nunseen-domain samples to boost semi-supervised training. We evaluated our\nproposed framework on dermatology and ophthalmology tasks. Extensive\nexperiments demonstrate our model can achieve superior classification\nperformance in various medical SSL scenarios. The code implementations are\naccessible at: https://github.com/PyJulie/USSL4MIC.\n", "link": "http://arxiv.org/abs/2304.04059v2", "date": "2024-07-02", "relevancy": 2.0073, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5499}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Semi-Supervised%20Learning%20for%20Medical%20Image%20Classification&body=Title%3A%20Universal%20Semi-Supervised%20Learning%20for%20Medical%20Image%20Classification%0AAuthor%3A%20Lie%20Ju%20and%20Yicheng%20Wu%20and%20Wei%20Feng%20and%20Zhen%20Yu%20and%20Lin%20Wang%20and%20Zhuoting%20Zhu%20and%20Zongyuan%20Ge%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20has%20attracted%20much%20attention%20since%20it%20reduces%0Athe%20expensive%20costs%20of%20collecting%20adequate%20well-labeled%20training%20data%2C%0Aespecially%20for%20deep%20learning%20methods.%20However%2C%20traditional%20SSL%20is%20built%20upon%20an%0Aassumption%20that%20labeled%20and%20unlabeled%20data%20should%20be%20from%20the%20same%20distribution%0A%5Ctextit%7Be.g.%2C%7D%20classes%20and%20domains.%20However%2C%20in%20practical%20scenarios%2C%20unlabeled%0Adata%20would%20be%20from%20unseen%20classes%20or%20unseen%20domains%2C%20and%20it%20is%20still%0Achallenging%20to%20exploit%20them%20by%20existing%20SSL%20methods.%20Therefore%2C%20in%20this%20paper%2C%0Awe%20proposed%20a%20unified%20framework%20to%20leverage%20these%20unseen%20unlabeled%20data%20for%0Aopen-scenario%20semi-supervised%20medical%20image%20classification.%20We%20first%20design%20a%0Anovel%20scoring%20mechanism%2C%20called%20dual-path%20outliers%20estimation%2C%20to%20identify%0Asamples%20from%20unseen%20classes.%20Meanwhile%2C%20to%20extract%20unseen-domain%20samples%2C%20we%0Athen%20apply%20an%20effective%20variational%20autoencoder%20%28VAE%29%20pre-training.%20After%20that%2C%0Awe%20conduct%20domain%20adaptation%20to%20fully%20exploit%20the%20value%20of%20the%20detected%0Aunseen-domain%20samples%20to%20boost%20semi-supervised%20training.%20We%20evaluated%20our%0Aproposed%20framework%20on%20dermatology%20and%20ophthalmology%20tasks.%20Extensive%0Aexperiments%20demonstrate%20our%20model%20can%20achieve%20superior%20classification%0Aperformance%20in%20various%20medical%20SSL%20scenarios.%20The%20code%20implementations%20are%0Aaccessible%20at%3A%20https%3A//github.com/PyJulie/USSL4MIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.04059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Semi-Supervised%2520Learning%2520for%2520Medical%2520Image%2520Classification%26entry.906535625%3DLie%2520Ju%2520and%2520Yicheng%2520Wu%2520and%2520Wei%2520Feng%2520and%2520Zhen%2520Yu%2520and%2520Lin%2520Wang%2520and%2520Zhuoting%2520Zhu%2520and%2520Zongyuan%2520Ge%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520has%2520attracted%2520much%2520attention%2520since%2520it%2520reduces%250Athe%2520expensive%2520costs%2520of%2520collecting%2520adequate%2520well-labeled%2520training%2520data%252C%250Aespecially%2520for%2520deep%2520learning%2520methods.%2520However%252C%2520traditional%2520SSL%2520is%2520built%2520upon%2520an%250Aassumption%2520that%2520labeled%2520and%2520unlabeled%2520data%2520should%2520be%2520from%2520the%2520same%2520distribution%250A%255Ctextit%257Be.g.%252C%257D%2520classes%2520and%2520domains.%2520However%252C%2520in%2520practical%2520scenarios%252C%2520unlabeled%250Adata%2520would%2520be%2520from%2520unseen%2520classes%2520or%2520unseen%2520domains%252C%2520and%2520it%2520is%2520still%250Achallenging%2520to%2520exploit%2520them%2520by%2520existing%2520SSL%2520methods.%2520Therefore%252C%2520in%2520this%2520paper%252C%250Awe%2520proposed%2520a%2520unified%2520framework%2520to%2520leverage%2520these%2520unseen%2520unlabeled%2520data%2520for%250Aopen-scenario%2520semi-supervised%2520medical%2520image%2520classification.%2520We%2520first%2520design%2520a%250Anovel%2520scoring%2520mechanism%252C%2520called%2520dual-path%2520outliers%2520estimation%252C%2520to%2520identify%250Asamples%2520from%2520unseen%2520classes.%2520Meanwhile%252C%2520to%2520extract%2520unseen-domain%2520samples%252C%2520we%250Athen%2520apply%2520an%2520effective%2520variational%2520autoencoder%2520%2528VAE%2529%2520pre-training.%2520After%2520that%252C%250Awe%2520conduct%2520domain%2520adaptation%2520to%2520fully%2520exploit%2520the%2520value%2520of%2520the%2520detected%250Aunseen-domain%2520samples%2520to%2520boost%2520semi-supervised%2520training.%2520We%2520evaluated%2520our%250Aproposed%2520framework%2520on%2520dermatology%2520and%2520ophthalmology%2520tasks.%2520Extensive%250Aexperiments%2520demonstrate%2520our%2520model%2520can%2520achieve%2520superior%2520classification%250Aperformance%2520in%2520various%2520medical%2520SSL%2520scenarios.%2520The%2520code%2520implementations%2520are%250Aaccessible%2520at%253A%2520https%253A//github.com/PyJulie/USSL4MIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.04059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Semi-Supervised%20Learning%20for%20Medical%20Image%20Classification&entry.906535625=Lie%20Ju%20and%20Yicheng%20Wu%20and%20Wei%20Feng%20and%20Zhen%20Yu%20and%20Lin%20Wang%20and%20Zhuoting%20Zhu%20and%20Zongyuan%20Ge&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20has%20attracted%20much%20attention%20since%20it%20reduces%0Athe%20expensive%20costs%20of%20collecting%20adequate%20well-labeled%20training%20data%2C%0Aespecially%20for%20deep%20learning%20methods.%20However%2C%20traditional%20SSL%20is%20built%20upon%20an%0Aassumption%20that%20labeled%20and%20unlabeled%20data%20should%20be%20from%20the%20same%20distribution%0A%5Ctextit%7Be.g.%2C%7D%20classes%20and%20domains.%20However%2C%20in%20practical%20scenarios%2C%20unlabeled%0Adata%20would%20be%20from%20unseen%20classes%20or%20unseen%20domains%2C%20and%20it%20is%20still%0Achallenging%20to%20exploit%20them%20by%20existing%20SSL%20methods.%20Therefore%2C%20in%20this%20paper%2C%0Awe%20proposed%20a%20unified%20framework%20to%20leverage%20these%20unseen%20unlabeled%20data%20for%0Aopen-scenario%20semi-supervised%20medical%20image%20classification.%20We%20first%20design%20a%0Anovel%20scoring%20mechanism%2C%20called%20dual-path%20outliers%20estimation%2C%20to%20identify%0Asamples%20from%20unseen%20classes.%20Meanwhile%2C%20to%20extract%20unseen-domain%20samples%2C%20we%0Athen%20apply%20an%20effective%20variational%20autoencoder%20%28VAE%29%20pre-training.%20After%20that%2C%0Awe%20conduct%20domain%20adaptation%20to%20fully%20exploit%20the%20value%20of%20the%20detected%0Aunseen-domain%20samples%20to%20boost%20semi-supervised%20training.%20We%20evaluated%20our%0Aproposed%20framework%20on%20dermatology%20and%20ophthalmology%20tasks.%20Extensive%0Aexperiments%20demonstrate%20our%20model%20can%20achieve%20superior%20classification%0Aperformance%20in%20various%20medical%20SSL%20scenarios.%20The%20code%20implementations%20are%0Aaccessible%20at%3A%20https%3A//github.com/PyJulie/USSL4MIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.04059v2&entry.124074799=Read"},
{"title": "Federated Distillation for Medical Image Classification: Towards\n  Trustworthy Computer-Aided Diagnosis", "author": "Sufen Ren and Yule Hu and Shengchao Chen and Guanjun Wang", "abstract": "  Medical image classification plays a crucial role in computer-aided clinical\ndiagnosis. While deep learning techniques have significantly enhanced\nefficiency and reduced costs, the privacy-sensitive nature of medical imaging\ndata complicates centralized storage and model training. Furthermore,\nlow-resource healthcare organizations face challenges related to communication\noverhead and efficiency due to increasing data and model scales. This paper\nproposes a novel privacy-preserving medical image classification framework\nbased on federated learning to address these issues, named FedMIC. The\nframework enables healthcare organizations to learn from both global and local\nknowledge, enhancing local representation of private data despite statistical\nheterogeneity. It provides customized models for organizations with diverse\ndata distributions while minimizing communication overhead and improving\nefficiency without compromising performance. Our FedMIC enhances robustness and\npractical applicability under resource-constrained conditions. We demonstrate\nFedMIC's effectiveness using four public medical image datasets for classical\nmedical image classification tasks.\n", "link": "http://arxiv.org/abs/2407.02261v1", "date": "2024-07-02", "relevancy": 2.0048, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Distillation%20for%20Medical%20Image%20Classification%3A%20Towards%0A%20%20Trustworthy%20Computer-Aided%20Diagnosis&body=Title%3A%20Federated%20Distillation%20for%20Medical%20Image%20Classification%3A%20Towards%0A%20%20Trustworthy%20Computer-Aided%20Diagnosis%0AAuthor%3A%20Sufen%20Ren%20and%20Yule%20Hu%20and%20Shengchao%20Chen%20and%20Guanjun%20Wang%0AAbstract%3A%20%20%20Medical%20image%20classification%20plays%20a%20crucial%20role%20in%20computer-aided%20clinical%0Adiagnosis.%20While%20deep%20learning%20techniques%20have%20significantly%20enhanced%0Aefficiency%20and%20reduced%20costs%2C%20the%20privacy-sensitive%20nature%20of%20medical%20imaging%0Adata%20complicates%20centralized%20storage%20and%20model%20training.%20Furthermore%2C%0Alow-resource%20healthcare%20organizations%20face%20challenges%20related%20to%20communication%0Aoverhead%20and%20efficiency%20due%20to%20increasing%20data%20and%20model%20scales.%20This%20paper%0Aproposes%20a%20novel%20privacy-preserving%20medical%20image%20classification%20framework%0Abased%20on%20federated%20learning%20to%20address%20these%20issues%2C%20named%20FedMIC.%20The%0Aframework%20enables%20healthcare%20organizations%20to%20learn%20from%20both%20global%20and%20local%0Aknowledge%2C%20enhancing%20local%20representation%20of%20private%20data%20despite%20statistical%0Aheterogeneity.%20It%20provides%20customized%20models%20for%20organizations%20with%20diverse%0Adata%20distributions%20while%20minimizing%20communication%20overhead%20and%20improving%0Aefficiency%20without%20compromising%20performance.%20Our%20FedMIC%20enhances%20robustness%20and%0Apractical%20applicability%20under%20resource-constrained%20conditions.%20We%20demonstrate%0AFedMIC%27s%20effectiveness%20using%20four%20public%20medical%20image%20datasets%20for%20classical%0Amedical%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Distillation%2520for%2520Medical%2520Image%2520Classification%253A%2520Towards%250A%2520%2520Trustworthy%2520Computer-Aided%2520Diagnosis%26entry.906535625%3DSufen%2520Ren%2520and%2520Yule%2520Hu%2520and%2520Shengchao%2520Chen%2520and%2520Guanjun%2520Wang%26entry.1292438233%3D%2520%2520Medical%2520image%2520classification%2520plays%2520a%2520crucial%2520role%2520in%2520computer-aided%2520clinical%250Adiagnosis.%2520While%2520deep%2520learning%2520techniques%2520have%2520significantly%2520enhanced%250Aefficiency%2520and%2520reduced%2520costs%252C%2520the%2520privacy-sensitive%2520nature%2520of%2520medical%2520imaging%250Adata%2520complicates%2520centralized%2520storage%2520and%2520model%2520training.%2520Furthermore%252C%250Alow-resource%2520healthcare%2520organizations%2520face%2520challenges%2520related%2520to%2520communication%250Aoverhead%2520and%2520efficiency%2520due%2520to%2520increasing%2520data%2520and%2520model%2520scales.%2520This%2520paper%250Aproposes%2520a%2520novel%2520privacy-preserving%2520medical%2520image%2520classification%2520framework%250Abased%2520on%2520federated%2520learning%2520to%2520address%2520these%2520issues%252C%2520named%2520FedMIC.%2520The%250Aframework%2520enables%2520healthcare%2520organizations%2520to%2520learn%2520from%2520both%2520global%2520and%2520local%250Aknowledge%252C%2520enhancing%2520local%2520representation%2520of%2520private%2520data%2520despite%2520statistical%250Aheterogeneity.%2520It%2520provides%2520customized%2520models%2520for%2520organizations%2520with%2520diverse%250Adata%2520distributions%2520while%2520minimizing%2520communication%2520overhead%2520and%2520improving%250Aefficiency%2520without%2520compromising%2520performance.%2520Our%2520FedMIC%2520enhances%2520robustness%2520and%250Apractical%2520applicability%2520under%2520resource-constrained%2520conditions.%2520We%2520demonstrate%250AFedMIC%2527s%2520effectiveness%2520using%2520four%2520public%2520medical%2520image%2520datasets%2520for%2520classical%250Amedical%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Distillation%20for%20Medical%20Image%20Classification%3A%20Towards%0A%20%20Trustworthy%20Computer-Aided%20Diagnosis&entry.906535625=Sufen%20Ren%20and%20Yule%20Hu%20and%20Shengchao%20Chen%20and%20Guanjun%20Wang&entry.1292438233=%20%20Medical%20image%20classification%20plays%20a%20crucial%20role%20in%20computer-aided%20clinical%0Adiagnosis.%20While%20deep%20learning%20techniques%20have%20significantly%20enhanced%0Aefficiency%20and%20reduced%20costs%2C%20the%20privacy-sensitive%20nature%20of%20medical%20imaging%0Adata%20complicates%20centralized%20storage%20and%20model%20training.%20Furthermore%2C%0Alow-resource%20healthcare%20organizations%20face%20challenges%20related%20to%20communication%0Aoverhead%20and%20efficiency%20due%20to%20increasing%20data%20and%20model%20scales.%20This%20paper%0Aproposes%20a%20novel%20privacy-preserving%20medical%20image%20classification%20framework%0Abased%20on%20federated%20learning%20to%20address%20these%20issues%2C%20named%20FedMIC.%20The%0Aframework%20enables%20healthcare%20organizations%20to%20learn%20from%20both%20global%20and%20local%0Aknowledge%2C%20enhancing%20local%20representation%20of%20private%20data%20despite%20statistical%0Aheterogeneity.%20It%20provides%20customized%20models%20for%20organizations%20with%20diverse%0Adata%20distributions%20while%20minimizing%20communication%20overhead%20and%20improving%0Aefficiency%20without%20compromising%20performance.%20Our%20FedMIC%20enhances%20robustness%20and%0Apractical%20applicability%20under%20resource-constrained%20conditions.%20We%20demonstrate%0AFedMIC%27s%20effectiveness%20using%20four%20public%20medical%20image%20datasets%20for%20classical%0Amedical%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02261v1&entry.124074799=Read"},
{"title": "Comparative Evaluation of Learning Models for Bionic Robots: Non-Linear\n  Transfer Function Identifications", "author": "Po-Yu Hsieh and June-Hao Hou", "abstract": "  The control and modeling of bionic robot dynamics have increasingly adopted\nmodel-free control strategies using machine learning methods. Given the\nnon-linear elastic nature of bionic robotic systems, learning-based methods\nprovide reliable alternatives by utilizing numerical data to establish a direct\nmapping from actuation inputs to robot trajectories without complex kinematics\nmodels. However, for developers, the method of identifying an appropriate\nlearning model for their specific bionic robots and further constructing the\ntransfer function has not been thoroughly discussed. Thus, this research trains\nfour types of models, including ensemble learning models, regularization-based\nmodels, kernel-based models, and neural network models, suitable for\nmulti-input multi-output (MIMO) data and non-linear transfer function\nidentification, in order to evaluate their (1) accuracy, (2) computation\ncomplexity, and (3) performance of capturing biological movements. This\nresearch encompasses data collection methods for control inputs and action\noutputs, selection of machine learning models, comparative analysis of training\nresults, and transfer function identifications. The main objective is to\nprovide a comprehensive evaluation strategy and framework for the application\nof model-free control.\n", "link": "http://arxiv.org/abs/2407.02428v1", "date": "2024-07-02", "relevancy": 1.9896, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5751}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4831}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Evaluation%20of%20Learning%20Models%20for%20Bionic%20Robots%3A%20Non-Linear%0A%20%20Transfer%20Function%20Identifications&body=Title%3A%20Comparative%20Evaluation%20of%20Learning%20Models%20for%20Bionic%20Robots%3A%20Non-Linear%0A%20%20Transfer%20Function%20Identifications%0AAuthor%3A%20Po-Yu%20Hsieh%20and%20June-Hao%20Hou%0AAbstract%3A%20%20%20The%20control%20and%20modeling%20of%20bionic%20robot%20dynamics%20have%20increasingly%20adopted%0Amodel-free%20control%20strategies%20using%20machine%20learning%20methods.%20Given%20the%0Anon-linear%20elastic%20nature%20of%20bionic%20robotic%20systems%2C%20learning-based%20methods%0Aprovide%20reliable%20alternatives%20by%20utilizing%20numerical%20data%20to%20establish%20a%20direct%0Amapping%20from%20actuation%20inputs%20to%20robot%20trajectories%20without%20complex%20kinematics%0Amodels.%20However%2C%20for%20developers%2C%20the%20method%20of%20identifying%20an%20appropriate%0Alearning%20model%20for%20their%20specific%20bionic%20robots%20and%20further%20constructing%20the%0Atransfer%20function%20has%20not%20been%20thoroughly%20discussed.%20Thus%2C%20this%20research%20trains%0Afour%20types%20of%20models%2C%20including%20ensemble%20learning%20models%2C%20regularization-based%0Amodels%2C%20kernel-based%20models%2C%20and%20neural%20network%20models%2C%20suitable%20for%0Amulti-input%20multi-output%20%28MIMO%29%20data%20and%20non-linear%20transfer%20function%0Aidentification%2C%20in%20order%20to%20evaluate%20their%20%281%29%20accuracy%2C%20%282%29%20computation%0Acomplexity%2C%20and%20%283%29%20performance%20of%20capturing%20biological%20movements.%20This%0Aresearch%20encompasses%20data%20collection%20methods%20for%20control%20inputs%20and%20action%0Aoutputs%2C%20selection%20of%20machine%20learning%20models%2C%20comparative%20analysis%20of%20training%0Aresults%2C%20and%20transfer%20function%20identifications.%20The%20main%20objective%20is%20to%0Aprovide%20a%20comprehensive%20evaluation%20strategy%20and%20framework%20for%20the%20application%0Aof%20model-free%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Evaluation%2520of%2520Learning%2520Models%2520for%2520Bionic%2520Robots%253A%2520Non-Linear%250A%2520%2520Transfer%2520Function%2520Identifications%26entry.906535625%3DPo-Yu%2520Hsieh%2520and%2520June-Hao%2520Hou%26entry.1292438233%3D%2520%2520The%2520control%2520and%2520modeling%2520of%2520bionic%2520robot%2520dynamics%2520have%2520increasingly%2520adopted%250Amodel-free%2520control%2520strategies%2520using%2520machine%2520learning%2520methods.%2520Given%2520the%250Anon-linear%2520elastic%2520nature%2520of%2520bionic%2520robotic%2520systems%252C%2520learning-based%2520methods%250Aprovide%2520reliable%2520alternatives%2520by%2520utilizing%2520numerical%2520data%2520to%2520establish%2520a%2520direct%250Amapping%2520from%2520actuation%2520inputs%2520to%2520robot%2520trajectories%2520without%2520complex%2520kinematics%250Amodels.%2520However%252C%2520for%2520developers%252C%2520the%2520method%2520of%2520identifying%2520an%2520appropriate%250Alearning%2520model%2520for%2520their%2520specific%2520bionic%2520robots%2520and%2520further%2520constructing%2520the%250Atransfer%2520function%2520has%2520not%2520been%2520thoroughly%2520discussed.%2520Thus%252C%2520this%2520research%2520trains%250Afour%2520types%2520of%2520models%252C%2520including%2520ensemble%2520learning%2520models%252C%2520regularization-based%250Amodels%252C%2520kernel-based%2520models%252C%2520and%2520neural%2520network%2520models%252C%2520suitable%2520for%250Amulti-input%2520multi-output%2520%2528MIMO%2529%2520data%2520and%2520non-linear%2520transfer%2520function%250Aidentification%252C%2520in%2520order%2520to%2520evaluate%2520their%2520%25281%2529%2520accuracy%252C%2520%25282%2529%2520computation%250Acomplexity%252C%2520and%2520%25283%2529%2520performance%2520of%2520capturing%2520biological%2520movements.%2520This%250Aresearch%2520encompasses%2520data%2520collection%2520methods%2520for%2520control%2520inputs%2520and%2520action%250Aoutputs%252C%2520selection%2520of%2520machine%2520learning%2520models%252C%2520comparative%2520analysis%2520of%2520training%250Aresults%252C%2520and%2520transfer%2520function%2520identifications.%2520The%2520main%2520objective%2520is%2520to%250Aprovide%2520a%2520comprehensive%2520evaluation%2520strategy%2520and%2520framework%2520for%2520the%2520application%250Aof%2520model-free%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Evaluation%20of%20Learning%20Models%20for%20Bionic%20Robots%3A%20Non-Linear%0A%20%20Transfer%20Function%20Identifications&entry.906535625=Po-Yu%20Hsieh%20and%20June-Hao%20Hou&entry.1292438233=%20%20The%20control%20and%20modeling%20of%20bionic%20robot%20dynamics%20have%20increasingly%20adopted%0Amodel-free%20control%20strategies%20using%20machine%20learning%20methods.%20Given%20the%0Anon-linear%20elastic%20nature%20of%20bionic%20robotic%20systems%2C%20learning-based%20methods%0Aprovide%20reliable%20alternatives%20by%20utilizing%20numerical%20data%20to%20establish%20a%20direct%0Amapping%20from%20actuation%20inputs%20to%20robot%20trajectories%20without%20complex%20kinematics%0Amodels.%20However%2C%20for%20developers%2C%20the%20method%20of%20identifying%20an%20appropriate%0Alearning%20model%20for%20their%20specific%20bionic%20robots%20and%20further%20constructing%20the%0Atransfer%20function%20has%20not%20been%20thoroughly%20discussed.%20Thus%2C%20this%20research%20trains%0Afour%20types%20of%20models%2C%20including%20ensemble%20learning%20models%2C%20regularization-based%0Amodels%2C%20kernel-based%20models%2C%20and%20neural%20network%20models%2C%20suitable%20for%0Amulti-input%20multi-output%20%28MIMO%29%20data%20and%20non-linear%20transfer%20function%0Aidentification%2C%20in%20order%20to%20evaluate%20their%20%281%29%20accuracy%2C%20%282%29%20computation%0Acomplexity%2C%20and%20%283%29%20performance%20of%20capturing%20biological%20movements.%20This%0Aresearch%20encompasses%20data%20collection%20methods%20for%20control%20inputs%20and%20action%0Aoutputs%2C%20selection%20of%20machine%20learning%20models%2C%20comparative%20analysis%20of%20training%0Aresults%2C%20and%20transfer%20function%20identifications.%20The%20main%20objective%20is%20to%0Aprovide%20a%20comprehensive%20evaluation%20strategy%20and%20framework%20for%20the%20application%0Aof%20model-free%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02428v1&entry.124074799=Read"},
{"title": "Steerable Pyramid Transform Enables Robust Left Ventricle Quantification", "author": "Xiangyang Zhu and Kede Ma and Wufeng Xue", "abstract": "  Predicting cardiac indices has long been a focal point in the medical imaging\ncommunity. While various deep learning models have demonstrated success in\nquantifying cardiac indices, they remain susceptible to mild input\nperturbations, e.g., spatial transformations, image distortions, and\nadversarial attacks. This vulnerability undermines confidence in using\nlearning-based automated systems for diagnosing cardiovascular diseases. In\nthis work, we describe a simple yet effective method to learn robust models for\nleft ventricle (LV) quantification, encompassing cavity and myocardium areas,\ndirectional dimensions, and regional wall thicknesses. Our success hinges on\nemploying the biologically inspired steerable pyramid transform (SPT) for fixed\nfront-end processing, which offers three main benefits. First, the basis\nfunctions of SPT align with the anatomical structure of LV and the geometric\nfeatures of the measured indices. Second, SPT facilitates weight sharing across\ndifferent orientations as a form of parameter regularization and naturally\ncaptures the scale variations of LV. Third, the residual highpass subband can\nbe conveniently discarded, promoting robust feature learning. Extensive\nexperiments on the Cardiac-Dig benchmark show that our SPT-augmented model not\nonly achieves reasonable prediction accuracy compared to state-of-the-art\nmethods, but also exhibits significantly improved robustness against input\nperturbations.\n", "link": "http://arxiv.org/abs/2201.08388v2", "date": "2024-07-02", "relevancy": 1.9746, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5129}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steerable%20Pyramid%20Transform%20Enables%20Robust%20Left%20Ventricle%20Quantification&body=Title%3A%20Steerable%20Pyramid%20Transform%20Enables%20Robust%20Left%20Ventricle%20Quantification%0AAuthor%3A%20Xiangyang%20Zhu%20and%20Kede%20Ma%20and%20Wufeng%20Xue%0AAbstract%3A%20%20%20Predicting%20cardiac%20indices%20has%20long%20been%20a%20focal%20point%20in%20the%20medical%20imaging%0Acommunity.%20While%20various%20deep%20learning%20models%20have%20demonstrated%20success%20in%0Aquantifying%20cardiac%20indices%2C%20they%20remain%20susceptible%20to%20mild%20input%0Aperturbations%2C%20e.g.%2C%20spatial%20transformations%2C%20image%20distortions%2C%20and%0Aadversarial%20attacks.%20This%20vulnerability%20undermines%20confidence%20in%20using%0Alearning-based%20automated%20systems%20for%20diagnosing%20cardiovascular%20diseases.%20In%0Athis%20work%2C%20we%20describe%20a%20simple%20yet%20effective%20method%20to%20learn%20robust%20models%20for%0Aleft%20ventricle%20%28LV%29%20quantification%2C%20encompassing%20cavity%20and%20myocardium%20areas%2C%0Adirectional%20dimensions%2C%20and%20regional%20wall%20thicknesses.%20Our%20success%20hinges%20on%0Aemploying%20the%20biologically%20inspired%20steerable%20pyramid%20transform%20%28SPT%29%20for%20fixed%0Afront-end%20processing%2C%20which%20offers%20three%20main%20benefits.%20First%2C%20the%20basis%0Afunctions%20of%20SPT%20align%20with%20the%20anatomical%20structure%20of%20LV%20and%20the%20geometric%0Afeatures%20of%20the%20measured%20indices.%20Second%2C%20SPT%20facilitates%20weight%20sharing%20across%0Adifferent%20orientations%20as%20a%20form%20of%20parameter%20regularization%20and%20naturally%0Acaptures%20the%20scale%20variations%20of%20LV.%20Third%2C%20the%20residual%20highpass%20subband%20can%0Abe%20conveniently%20discarded%2C%20promoting%20robust%20feature%20learning.%20Extensive%0Aexperiments%20on%20the%20Cardiac-Dig%20benchmark%20show%20that%20our%20SPT-augmented%20model%20not%0Aonly%20achieves%20reasonable%20prediction%20accuracy%20compared%20to%20state-of-the-art%0Amethods%2C%20but%20also%20exhibits%20significantly%20improved%20robustness%20against%20input%0Aperturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.08388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteerable%2520Pyramid%2520Transform%2520Enables%2520Robust%2520Left%2520Ventricle%2520Quantification%26entry.906535625%3DXiangyang%2520Zhu%2520and%2520Kede%2520Ma%2520and%2520Wufeng%2520Xue%26entry.1292438233%3D%2520%2520Predicting%2520cardiac%2520indices%2520has%2520long%2520been%2520a%2520focal%2520point%2520in%2520the%2520medical%2520imaging%250Acommunity.%2520While%2520various%2520deep%2520learning%2520models%2520have%2520demonstrated%2520success%2520in%250Aquantifying%2520cardiac%2520indices%252C%2520they%2520remain%2520susceptible%2520to%2520mild%2520input%250Aperturbations%252C%2520e.g.%252C%2520spatial%2520transformations%252C%2520image%2520distortions%252C%2520and%250Aadversarial%2520attacks.%2520This%2520vulnerability%2520undermines%2520confidence%2520in%2520using%250Alearning-based%2520automated%2520systems%2520for%2520diagnosing%2520cardiovascular%2520diseases.%2520In%250Athis%2520work%252C%2520we%2520describe%2520a%2520simple%2520yet%2520effective%2520method%2520to%2520learn%2520robust%2520models%2520for%250Aleft%2520ventricle%2520%2528LV%2529%2520quantification%252C%2520encompassing%2520cavity%2520and%2520myocardium%2520areas%252C%250Adirectional%2520dimensions%252C%2520and%2520regional%2520wall%2520thicknesses.%2520Our%2520success%2520hinges%2520on%250Aemploying%2520the%2520biologically%2520inspired%2520steerable%2520pyramid%2520transform%2520%2528SPT%2529%2520for%2520fixed%250Afront-end%2520processing%252C%2520which%2520offers%2520three%2520main%2520benefits.%2520First%252C%2520the%2520basis%250Afunctions%2520of%2520SPT%2520align%2520with%2520the%2520anatomical%2520structure%2520of%2520LV%2520and%2520the%2520geometric%250Afeatures%2520of%2520the%2520measured%2520indices.%2520Second%252C%2520SPT%2520facilitates%2520weight%2520sharing%2520across%250Adifferent%2520orientations%2520as%2520a%2520form%2520of%2520parameter%2520regularization%2520and%2520naturally%250Acaptures%2520the%2520scale%2520variations%2520of%2520LV.%2520Third%252C%2520the%2520residual%2520highpass%2520subband%2520can%250Abe%2520conveniently%2520discarded%252C%2520promoting%2520robust%2520feature%2520learning.%2520Extensive%250Aexperiments%2520on%2520the%2520Cardiac-Dig%2520benchmark%2520show%2520that%2520our%2520SPT-augmented%2520model%2520not%250Aonly%2520achieves%2520reasonable%2520prediction%2520accuracy%2520compared%2520to%2520state-of-the-art%250Amethods%252C%2520but%2520also%2520exhibits%2520significantly%2520improved%2520robustness%2520against%2520input%250Aperturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.08388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steerable%20Pyramid%20Transform%20Enables%20Robust%20Left%20Ventricle%20Quantification&entry.906535625=Xiangyang%20Zhu%20and%20Kede%20Ma%20and%20Wufeng%20Xue&entry.1292438233=%20%20Predicting%20cardiac%20indices%20has%20long%20been%20a%20focal%20point%20in%20the%20medical%20imaging%0Acommunity.%20While%20various%20deep%20learning%20models%20have%20demonstrated%20success%20in%0Aquantifying%20cardiac%20indices%2C%20they%20remain%20susceptible%20to%20mild%20input%0Aperturbations%2C%20e.g.%2C%20spatial%20transformations%2C%20image%20distortions%2C%20and%0Aadversarial%20attacks.%20This%20vulnerability%20undermines%20confidence%20in%20using%0Alearning-based%20automated%20systems%20for%20diagnosing%20cardiovascular%20diseases.%20In%0Athis%20work%2C%20we%20describe%20a%20simple%20yet%20effective%20method%20to%20learn%20robust%20models%20for%0Aleft%20ventricle%20%28LV%29%20quantification%2C%20encompassing%20cavity%20and%20myocardium%20areas%2C%0Adirectional%20dimensions%2C%20and%20regional%20wall%20thicknesses.%20Our%20success%20hinges%20on%0Aemploying%20the%20biologically%20inspired%20steerable%20pyramid%20transform%20%28SPT%29%20for%20fixed%0Afront-end%20processing%2C%20which%20offers%20three%20main%20benefits.%20First%2C%20the%20basis%0Afunctions%20of%20SPT%20align%20with%20the%20anatomical%20structure%20of%20LV%20and%20the%20geometric%0Afeatures%20of%20the%20measured%20indices.%20Second%2C%20SPT%20facilitates%20weight%20sharing%20across%0Adifferent%20orientations%20as%20a%20form%20of%20parameter%20regularization%20and%20naturally%0Acaptures%20the%20scale%20variations%20of%20LV.%20Third%2C%20the%20residual%20highpass%20subband%20can%0Abe%20conveniently%20discarded%2C%20promoting%20robust%20feature%20learning.%20Extensive%0Aexperiments%20on%20the%20Cardiac-Dig%20benchmark%20show%20that%20our%20SPT-augmented%20model%20not%0Aonly%20achieves%20reasonable%20prediction%20accuracy%20compared%20to%20state-of-the-art%0Amethods%2C%20but%20also%20exhibits%20significantly%20improved%20robustness%20against%20input%0Aperturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.08388v2&entry.124074799=Read"},
{"title": "Why do LLaVA Vision-Language Models Reply to Images in English?", "author": "Musashi Hinck and Carolin Holtermann and Matthew Lyle Olson and Florian Schneider and Sungduk Yu and Anahita Bhiwandiwalla and Anne Lauscher and Shaoyen Tseng and Vasudev Lal", "abstract": "  We uncover a surprising multilingual bias occurring in a popular class of\nmultimodal vision-language models (VLMs). Including an image in the query to a\nLLaVA-style VLM significantly increases the likelihood of the model returning\nan English response, regardless of the language of the query. This paper\ninvestigates the causes of this loss with a two-pronged approach that combines\nextensive ablation of the design space with a mechanistic analysis of the\nmodels' internal representations of image and text inputs. Both approaches\nindicate that the issue stems in the language modelling component of the LLaVA\nmodel. Statistically, we find that switching the language backbone for a\nbilingual language model has the strongest effect on reducing this error.\nMechanistically, we provide compelling evidence that visual inputs are not\nmapped to a similar space as text ones, and that intervening on intermediary\nattention layers can reduce this bias. Our findings provide important insights\nto researchers and engineers seeking to understand the crossover between\nmultimodal and multilingual spaces, and contribute to the goal of developing\ncapable and inclusive VLMs for non-English contexts.\n", "link": "http://arxiv.org/abs/2407.02333v1", "date": "2024-07-02", "relevancy": 1.9744, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5059}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4998}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20do%20LLaVA%20Vision-Language%20Models%20Reply%20to%20Images%20in%20English%3F&body=Title%3A%20Why%20do%20LLaVA%20Vision-Language%20Models%20Reply%20to%20Images%20in%20English%3F%0AAuthor%3A%20Musashi%20Hinck%20and%20Carolin%20Holtermann%20and%20Matthew%20Lyle%20Olson%20and%20Florian%20Schneider%20and%20Sungduk%20Yu%20and%20Anahita%20Bhiwandiwalla%20and%20Anne%20Lauscher%20and%20Shaoyen%20Tseng%20and%20Vasudev%20Lal%0AAbstract%3A%20%20%20We%20uncover%20a%20surprising%20multilingual%20bias%20occurring%20in%20a%20popular%20class%20of%0Amultimodal%20vision-language%20models%20%28VLMs%29.%20Including%20an%20image%20in%20the%20query%20to%20a%0ALLaVA-style%20VLM%20significantly%20increases%20the%20likelihood%20of%20the%20model%20returning%0Aan%20English%20response%2C%20regardless%20of%20the%20language%20of%20the%20query.%20This%20paper%0Ainvestigates%20the%20causes%20of%20this%20loss%20with%20a%20two-pronged%20approach%20that%20combines%0Aextensive%20ablation%20of%20the%20design%20space%20with%20a%20mechanistic%20analysis%20of%20the%0Amodels%27%20internal%20representations%20of%20image%20and%20text%20inputs.%20Both%20approaches%0Aindicate%20that%20the%20issue%20stems%20in%20the%20language%20modelling%20component%20of%20the%20LLaVA%0Amodel.%20Statistically%2C%20we%20find%20that%20switching%20the%20language%20backbone%20for%20a%0Abilingual%20language%20model%20has%20the%20strongest%20effect%20on%20reducing%20this%20error.%0AMechanistically%2C%20we%20provide%20compelling%20evidence%20that%20visual%20inputs%20are%20not%0Amapped%20to%20a%20similar%20space%20as%20text%20ones%2C%20and%20that%20intervening%20on%20intermediary%0Aattention%20layers%20can%20reduce%20this%20bias.%20Our%20findings%20provide%20important%20insights%0Ato%20researchers%20and%20engineers%20seeking%20to%20understand%20the%20crossover%20between%0Amultimodal%20and%20multilingual%20spaces%2C%20and%20contribute%20to%20the%20goal%20of%20developing%0Acapable%20and%20inclusive%20VLMs%20for%20non-English%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520do%2520LLaVA%2520Vision-Language%2520Models%2520Reply%2520to%2520Images%2520in%2520English%253F%26entry.906535625%3DMusashi%2520Hinck%2520and%2520Carolin%2520Holtermann%2520and%2520Matthew%2520Lyle%2520Olson%2520and%2520Florian%2520Schneider%2520and%2520Sungduk%2520Yu%2520and%2520Anahita%2520Bhiwandiwalla%2520and%2520Anne%2520Lauscher%2520and%2520Shaoyen%2520Tseng%2520and%2520Vasudev%2520Lal%26entry.1292438233%3D%2520%2520We%2520uncover%2520a%2520surprising%2520multilingual%2520bias%2520occurring%2520in%2520a%2520popular%2520class%2520of%250Amultimodal%2520vision-language%2520models%2520%2528VLMs%2529.%2520Including%2520an%2520image%2520in%2520the%2520query%2520to%2520a%250ALLaVA-style%2520VLM%2520significantly%2520increases%2520the%2520likelihood%2520of%2520the%2520model%2520returning%250Aan%2520English%2520response%252C%2520regardless%2520of%2520the%2520language%2520of%2520the%2520query.%2520This%2520paper%250Ainvestigates%2520the%2520causes%2520of%2520this%2520loss%2520with%2520a%2520two-pronged%2520approach%2520that%2520combines%250Aextensive%2520ablation%2520of%2520the%2520design%2520space%2520with%2520a%2520mechanistic%2520analysis%2520of%2520the%250Amodels%2527%2520internal%2520representations%2520of%2520image%2520and%2520text%2520inputs.%2520Both%2520approaches%250Aindicate%2520that%2520the%2520issue%2520stems%2520in%2520the%2520language%2520modelling%2520component%2520of%2520the%2520LLaVA%250Amodel.%2520Statistically%252C%2520we%2520find%2520that%2520switching%2520the%2520language%2520backbone%2520for%2520a%250Abilingual%2520language%2520model%2520has%2520the%2520strongest%2520effect%2520on%2520reducing%2520this%2520error.%250AMechanistically%252C%2520we%2520provide%2520compelling%2520evidence%2520that%2520visual%2520inputs%2520are%2520not%250Amapped%2520to%2520a%2520similar%2520space%2520as%2520text%2520ones%252C%2520and%2520that%2520intervening%2520on%2520intermediary%250Aattention%2520layers%2520can%2520reduce%2520this%2520bias.%2520Our%2520findings%2520provide%2520important%2520insights%250Ato%2520researchers%2520and%2520engineers%2520seeking%2520to%2520understand%2520the%2520crossover%2520between%250Amultimodal%2520and%2520multilingual%2520spaces%252C%2520and%2520contribute%2520to%2520the%2520goal%2520of%2520developing%250Acapable%2520and%2520inclusive%2520VLMs%2520for%2520non-English%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20do%20LLaVA%20Vision-Language%20Models%20Reply%20to%20Images%20in%20English%3F&entry.906535625=Musashi%20Hinck%20and%20Carolin%20Holtermann%20and%20Matthew%20Lyle%20Olson%20and%20Florian%20Schneider%20and%20Sungduk%20Yu%20and%20Anahita%20Bhiwandiwalla%20and%20Anne%20Lauscher%20and%20Shaoyen%20Tseng%20and%20Vasudev%20Lal&entry.1292438233=%20%20We%20uncover%20a%20surprising%20multilingual%20bias%20occurring%20in%20a%20popular%20class%20of%0Amultimodal%20vision-language%20models%20%28VLMs%29.%20Including%20an%20image%20in%20the%20query%20to%20a%0ALLaVA-style%20VLM%20significantly%20increases%20the%20likelihood%20of%20the%20model%20returning%0Aan%20English%20response%2C%20regardless%20of%20the%20language%20of%20the%20query.%20This%20paper%0Ainvestigates%20the%20causes%20of%20this%20loss%20with%20a%20two-pronged%20approach%20that%20combines%0Aextensive%20ablation%20of%20the%20design%20space%20with%20a%20mechanistic%20analysis%20of%20the%0Amodels%27%20internal%20representations%20of%20image%20and%20text%20inputs.%20Both%20approaches%0Aindicate%20that%20the%20issue%20stems%20in%20the%20language%20modelling%20component%20of%20the%20LLaVA%0Amodel.%20Statistically%2C%20we%20find%20that%20switching%20the%20language%20backbone%20for%20a%0Abilingual%20language%20model%20has%20the%20strongest%20effect%20on%20reducing%20this%20error.%0AMechanistically%2C%20we%20provide%20compelling%20evidence%20that%20visual%20inputs%20are%20not%0Amapped%20to%20a%20similar%20space%20as%20text%20ones%2C%20and%20that%20intervening%20on%20intermediary%0Aattention%20layers%20can%20reduce%20this%20bias.%20Our%20findings%20provide%20important%20insights%0Ato%20researchers%20and%20engineers%20seeking%20to%20understand%20the%20crossover%20between%0Amultimodal%20and%20multilingual%20spaces%2C%20and%20contribute%20to%20the%20goal%20of%20developing%0Acapable%20and%20inclusive%20VLMs%20for%20non-English%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02333v1&entry.124074799=Read"},
{"title": "Ensemble of pre-trained language models and data augmentation for hate\n  speech detection from Arabic tweets", "author": "Kheir Eddine Daouadi and Yaakoub Boualleg and Kheir Eddine Haouaouchi", "abstract": "  Today, hate speech classification from Arabic tweets has drawn the attention\nof several researchers. Many systems and techniques have been developed to\nresolve this classification task. Nevertheless, two of the major challenges\nfaced in this context are the limited performance and the problem of imbalanced\ndata. In this study, we propose a novel approach that leverages ensemble\nlearning and semi-supervised learning based on previously manually labeled. We\nconducted experiments on a benchmark dataset by classifying Arabic tweets into\n5 distinct classes: non-hate, general hate, racial, religious, or sexism.\nExperimental results show that: (1) ensemble learning based on pre-trained\nlanguage models outperforms existing related works; (2) Our proposed data\naugmentation improves the accuracy results of hate speech detection from Arabic\ntweets and outperforms existing related works. Our main contribution is the\nachievement of encouraging results in Arabic hate speech detection.\n", "link": "http://arxiv.org/abs/2407.02448v1", "date": "2024-07-02", "relevancy": 1.9734, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5122}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4879}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20of%20pre-trained%20language%20models%20and%20data%20augmentation%20for%20hate%0A%20%20speech%20detection%20from%20Arabic%20tweets&body=Title%3A%20Ensemble%20of%20pre-trained%20language%20models%20and%20data%20augmentation%20for%20hate%0A%20%20speech%20detection%20from%20Arabic%20tweets%0AAuthor%3A%20Kheir%20Eddine%20Daouadi%20and%20Yaakoub%20Boualleg%20and%20Kheir%20Eddine%20Haouaouchi%0AAbstract%3A%20%20%20Today%2C%20hate%20speech%20classification%20from%20Arabic%20tweets%20has%20drawn%20the%20attention%0Aof%20several%20researchers.%20Many%20systems%20and%20techniques%20have%20been%20developed%20to%0Aresolve%20this%20classification%20task.%20Nevertheless%2C%20two%20of%20the%20major%20challenges%0Afaced%20in%20this%20context%20are%20the%20limited%20performance%20and%20the%20problem%20of%20imbalanced%0Adata.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20ensemble%0Alearning%20and%20semi-supervised%20learning%20based%20on%20previously%20manually%20labeled.%20We%0Aconducted%20experiments%20on%20a%20benchmark%20dataset%20by%20classifying%20Arabic%20tweets%20into%0A5%20distinct%20classes%3A%20non-hate%2C%20general%20hate%2C%20racial%2C%20religious%2C%20or%20sexism.%0AExperimental%20results%20show%20that%3A%20%281%29%20ensemble%20learning%20based%20on%20pre-trained%0Alanguage%20models%20outperforms%20existing%20related%20works%3B%20%282%29%20Our%20proposed%20data%0Aaugmentation%20improves%20the%20accuracy%20results%20of%20hate%20speech%20detection%20from%20Arabic%0Atweets%20and%20outperforms%20existing%20related%20works.%20Our%20main%20contribution%20is%20the%0Aachievement%20of%20encouraging%20results%20in%20Arabic%20hate%20speech%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520of%2520pre-trained%2520language%2520models%2520and%2520data%2520augmentation%2520for%2520hate%250A%2520%2520speech%2520detection%2520from%2520Arabic%2520tweets%26entry.906535625%3DKheir%2520Eddine%2520Daouadi%2520and%2520Yaakoub%2520Boualleg%2520and%2520Kheir%2520Eddine%2520Haouaouchi%26entry.1292438233%3D%2520%2520Today%252C%2520hate%2520speech%2520classification%2520from%2520Arabic%2520tweets%2520has%2520drawn%2520the%2520attention%250Aof%2520several%2520researchers.%2520Many%2520systems%2520and%2520techniques%2520have%2520been%2520developed%2520to%250Aresolve%2520this%2520classification%2520task.%2520Nevertheless%252C%2520two%2520of%2520the%2520major%2520challenges%250Afaced%2520in%2520this%2520context%2520are%2520the%2520limited%2520performance%2520and%2520the%2520problem%2520of%2520imbalanced%250Adata.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520leverages%2520ensemble%250Alearning%2520and%2520semi-supervised%2520learning%2520based%2520on%2520previously%2520manually%2520labeled.%2520We%250Aconducted%2520experiments%2520on%2520a%2520benchmark%2520dataset%2520by%2520classifying%2520Arabic%2520tweets%2520into%250A5%2520distinct%2520classes%253A%2520non-hate%252C%2520general%2520hate%252C%2520racial%252C%2520religious%252C%2520or%2520sexism.%250AExperimental%2520results%2520show%2520that%253A%2520%25281%2529%2520ensemble%2520learning%2520based%2520on%2520pre-trained%250Alanguage%2520models%2520outperforms%2520existing%2520related%2520works%253B%2520%25282%2529%2520Our%2520proposed%2520data%250Aaugmentation%2520improves%2520the%2520accuracy%2520results%2520of%2520hate%2520speech%2520detection%2520from%2520Arabic%250Atweets%2520and%2520outperforms%2520existing%2520related%2520works.%2520Our%2520main%2520contribution%2520is%2520the%250Aachievement%2520of%2520encouraging%2520results%2520in%2520Arabic%2520hate%2520speech%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20of%20pre-trained%20language%20models%20and%20data%20augmentation%20for%20hate%0A%20%20speech%20detection%20from%20Arabic%20tweets&entry.906535625=Kheir%20Eddine%20Daouadi%20and%20Yaakoub%20Boualleg%20and%20Kheir%20Eddine%20Haouaouchi&entry.1292438233=%20%20Today%2C%20hate%20speech%20classification%20from%20Arabic%20tweets%20has%20drawn%20the%20attention%0Aof%20several%20researchers.%20Many%20systems%20and%20techniques%20have%20been%20developed%20to%0Aresolve%20this%20classification%20task.%20Nevertheless%2C%20two%20of%20the%20major%20challenges%0Afaced%20in%20this%20context%20are%20the%20limited%20performance%20and%20the%20problem%20of%20imbalanced%0Adata.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20ensemble%0Alearning%20and%20semi-supervised%20learning%20based%20on%20previously%20manually%20labeled.%20We%0Aconducted%20experiments%20on%20a%20benchmark%20dataset%20by%20classifying%20Arabic%20tweets%20into%0A5%20distinct%20classes%3A%20non-hate%2C%20general%20hate%2C%20racial%2C%20religious%2C%20or%20sexism.%0AExperimental%20results%20show%20that%3A%20%281%29%20ensemble%20learning%20based%20on%20pre-trained%0Alanguage%20models%20outperforms%20existing%20related%20works%3B%20%282%29%20Our%20proposed%20data%0Aaugmentation%20improves%20the%20accuracy%20results%20of%20hate%20speech%20detection%20from%20Arabic%0Atweets%20and%20outperforms%20existing%20related%20works.%20Our%20main%20contribution%20is%20the%0Aachievement%20of%20encouraging%20results%20in%20Arabic%20hate%20speech%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02448v1&entry.124074799=Read"},
{"title": "Selective Pre-training for Private Fine-tuning", "author": "Da Yu and Sivakanth Gopi and Janardhan Kulkarni and Zinan Lin and Saurabh Naik and Tomasz Lukasz Religa and Jian Yin and Huishuai Zhang", "abstract": "  Text prediction models, when used in applications like email clients or word\nprocessors, must protect user data privacy and adhere to model size\nconstraints. These constraints are crucial to meet memory and inference time\nrequirements, as well as to reduce inference costs. Building small, fast, and\nprivate domain-specific language models is a thriving area of research. In this\nwork, we show that a careful pre-training on a \\emph{subset} of the public\ndataset that is guided by the private dataset is crucial to train small\nlanguage models with differential privacy. On standard benchmarks, small models\ntrained with our new framework achieve state-of-the-art performance. In\naddition to performance improvements, our results demonstrate that smaller\nmodels, through careful pre-training and private fine-tuning, can match the\nperformance of much larger models that do not have access to private data. This\nunderscores the potential of private learning for model compression and\nenhanced efficiency.\n", "link": "http://arxiv.org/abs/2305.13865v3", "date": "2024-07-02", "relevancy": 1.9553, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.492}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4912}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Pre-training%20for%20Private%20Fine-tuning&body=Title%3A%20Selective%20Pre-training%20for%20Private%20Fine-tuning%0AAuthor%3A%20Da%20Yu%20and%20Sivakanth%20Gopi%20and%20Janardhan%20Kulkarni%20and%20Zinan%20Lin%20and%20Saurabh%20Naik%20and%20Tomasz%20Lukasz%20Religa%20and%20Jian%20Yin%20and%20Huishuai%20Zhang%0AAbstract%3A%20%20%20Text%20prediction%20models%2C%20when%20used%20in%20applications%20like%20email%20clients%20or%20word%0Aprocessors%2C%20must%20protect%20user%20data%20privacy%20and%20adhere%20to%20model%20size%0Aconstraints.%20These%20constraints%20are%20crucial%20to%20meet%20memory%20and%20inference%20time%0Arequirements%2C%20as%20well%20as%20to%20reduce%20inference%20costs.%20Building%20small%2C%20fast%2C%20and%0Aprivate%20domain-specific%20language%20models%20is%20a%20thriving%20area%20of%20research.%20In%20this%0Awork%2C%20we%20show%20that%20a%20careful%20pre-training%20on%20a%20%5Cemph%7Bsubset%7D%20of%20the%20public%0Adataset%20that%20is%20guided%20by%20the%20private%20dataset%20is%20crucial%20to%20train%20small%0Alanguage%20models%20with%20differential%20privacy.%20On%20standard%20benchmarks%2C%20small%20models%0Atrained%20with%20our%20new%20framework%20achieve%20state-of-the-art%20performance.%20In%0Aaddition%20to%20performance%20improvements%2C%20our%20results%20demonstrate%20that%20smaller%0Amodels%2C%20through%20careful%20pre-training%20and%20private%20fine-tuning%2C%20can%20match%20the%0Aperformance%20of%20much%20larger%20models%20that%20do%20not%20have%20access%20to%20private%20data.%20This%0Aunderscores%20the%20potential%20of%20private%20learning%20for%20model%20compression%20and%0Aenhanced%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13865v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Pre-training%2520for%2520Private%2520Fine-tuning%26entry.906535625%3DDa%2520Yu%2520and%2520Sivakanth%2520Gopi%2520and%2520Janardhan%2520Kulkarni%2520and%2520Zinan%2520Lin%2520and%2520Saurabh%2520Naik%2520and%2520Tomasz%2520Lukasz%2520Religa%2520and%2520Jian%2520Yin%2520and%2520Huishuai%2520Zhang%26entry.1292438233%3D%2520%2520Text%2520prediction%2520models%252C%2520when%2520used%2520in%2520applications%2520like%2520email%2520clients%2520or%2520word%250Aprocessors%252C%2520must%2520protect%2520user%2520data%2520privacy%2520and%2520adhere%2520to%2520model%2520size%250Aconstraints.%2520These%2520constraints%2520are%2520crucial%2520to%2520meet%2520memory%2520and%2520inference%2520time%250Arequirements%252C%2520as%2520well%2520as%2520to%2520reduce%2520inference%2520costs.%2520Building%2520small%252C%2520fast%252C%2520and%250Aprivate%2520domain-specific%2520language%2520models%2520is%2520a%2520thriving%2520area%2520of%2520research.%2520In%2520this%250Awork%252C%2520we%2520show%2520that%2520a%2520careful%2520pre-training%2520on%2520a%2520%255Cemph%257Bsubset%257D%2520of%2520the%2520public%250Adataset%2520that%2520is%2520guided%2520by%2520the%2520private%2520dataset%2520is%2520crucial%2520to%2520train%2520small%250Alanguage%2520models%2520with%2520differential%2520privacy.%2520On%2520standard%2520benchmarks%252C%2520small%2520models%250Atrained%2520with%2520our%2520new%2520framework%2520achieve%2520state-of-the-art%2520performance.%2520In%250Aaddition%2520to%2520performance%2520improvements%252C%2520our%2520results%2520demonstrate%2520that%2520smaller%250Amodels%252C%2520through%2520careful%2520pre-training%2520and%2520private%2520fine-tuning%252C%2520can%2520match%2520the%250Aperformance%2520of%2520much%2520larger%2520models%2520that%2520do%2520not%2520have%2520access%2520to%2520private%2520data.%2520This%250Aunderscores%2520the%2520potential%2520of%2520private%2520learning%2520for%2520model%2520compression%2520and%250Aenhanced%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.13865v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Pre-training%20for%20Private%20Fine-tuning&entry.906535625=Da%20Yu%20and%20Sivakanth%20Gopi%20and%20Janardhan%20Kulkarni%20and%20Zinan%20Lin%20and%20Saurabh%20Naik%20and%20Tomasz%20Lukasz%20Religa%20and%20Jian%20Yin%20and%20Huishuai%20Zhang&entry.1292438233=%20%20Text%20prediction%20models%2C%20when%20used%20in%20applications%20like%20email%20clients%20or%20word%0Aprocessors%2C%20must%20protect%20user%20data%20privacy%20and%20adhere%20to%20model%20size%0Aconstraints.%20These%20constraints%20are%20crucial%20to%20meet%20memory%20and%20inference%20time%0Arequirements%2C%20as%20well%20as%20to%20reduce%20inference%20costs.%20Building%20small%2C%20fast%2C%20and%0Aprivate%20domain-specific%20language%20models%20is%20a%20thriving%20area%20of%20research.%20In%20this%0Awork%2C%20we%20show%20that%20a%20careful%20pre-training%20on%20a%20%5Cemph%7Bsubset%7D%20of%20the%20public%0Adataset%20that%20is%20guided%20by%20the%20private%20dataset%20is%20crucial%20to%20train%20small%0Alanguage%20models%20with%20differential%20privacy.%20On%20standard%20benchmarks%2C%20small%20models%0Atrained%20with%20our%20new%20framework%20achieve%20state-of-the-art%20performance.%20In%0Aaddition%20to%20performance%20improvements%2C%20our%20results%20demonstrate%20that%20smaller%0Amodels%2C%20through%20careful%20pre-training%20and%20private%20fine-tuning%2C%20can%20match%20the%0Aperformance%20of%20much%20larger%20models%20that%20do%20not%20have%20access%20to%20private%20data.%20This%0Aunderscores%20the%20potential%20of%20private%20learning%20for%20model%20compression%20and%0Aenhanced%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13865v3&entry.124074799=Read"},
{"title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to\n  Solve Cross-lingual Emotion Detection", "author": "Long Cheng and Qihao Shao and Christine Zhao and Sheng Bi and Gina-Anne Levow", "abstract": "  Cross-lingual emotion detection allows us to analyze global trends, public\nopinion, and social phenomena at scale. We participated in the Explainability\nof Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score\nof 0.6046 on the evaluation set for the emotion detection sub-task. Our system\noutperformed the baseline by more than 0.16 F1-score absolute, and ranked\nsecond amongst competing systems. We conducted experiments using fine-tuning,\nzero-shot learning, and few-shot learning for Large Language Model (LLM)-based\nmodels as well as embedding-based BiLSTM and KNN for non-LLM-based techniques.\nAdditionally, we introduced two novel methods: the Multi-Iteration Agentic\nWorkflow and the Multi-Binary-Classifier Agentic Workflow. We found that\nLLM-based approaches provided good performance on multilingual emotion\ndetection. Furthermore, ensembles combining all our experimented models yielded\nhigher F1-scores than any single approach alone.\n", "link": "http://arxiv.org/abs/2405.17129v2", "date": "2024-07-02", "relevancy": 1.955, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5124}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEII%3A%20Think%2C%20Explain%2C%20Interact%20and%20Iterate%20with%20Large%20Language%20Models%20to%0A%20%20Solve%20Cross-lingual%20Emotion%20Detection&body=Title%3A%20TEII%3A%20Think%2C%20Explain%2C%20Interact%20and%20Iterate%20with%20Large%20Language%20Models%20to%0A%20%20Solve%20Cross-lingual%20Emotion%20Detection%0AAuthor%3A%20Long%20Cheng%20and%20Qihao%20Shao%20and%20Christine%20Zhao%20and%20Sheng%20Bi%20and%20Gina-Anne%20Levow%0AAbstract%3A%20%20%20Cross-lingual%20emotion%20detection%20allows%20us%20to%20analyze%20global%20trends%2C%20public%0Aopinion%2C%20and%20social%20phenomena%20at%20scale.%20We%20participated%20in%20the%20Explainability%0Aof%20Cross-lingual%20Emotion%20Detection%20%28EXALT%29%20shared%20task%2C%20achieving%20an%20F1-score%0Aof%200.6046%20on%20the%20evaluation%20set%20for%20the%20emotion%20detection%20sub-task.%20Our%20system%0Aoutperformed%20the%20baseline%20by%20more%20than%200.16%20F1-score%20absolute%2C%20and%20ranked%0Asecond%20amongst%20competing%20systems.%20We%20conducted%20experiments%20using%20fine-tuning%2C%0Azero-shot%20learning%2C%20and%20few-shot%20learning%20for%20Large%20Language%20Model%20%28LLM%29-based%0Amodels%20as%20well%20as%20embedding-based%20BiLSTM%20and%20KNN%20for%20non-LLM-based%20techniques.%0AAdditionally%2C%20we%20introduced%20two%20novel%20methods%3A%20the%20Multi-Iteration%20Agentic%0AWorkflow%20and%20the%20Multi-Binary-Classifier%20Agentic%20Workflow.%20We%20found%20that%0ALLM-based%20approaches%20provided%20good%20performance%20on%20multilingual%20emotion%0Adetection.%20Furthermore%2C%20ensembles%20combining%20all%20our%20experimented%20models%20yielded%0Ahigher%20F1-scores%20than%20any%20single%20approach%20alone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEII%253A%2520Think%252C%2520Explain%252C%2520Interact%2520and%2520Iterate%2520with%2520Large%2520Language%2520Models%2520to%250A%2520%2520Solve%2520Cross-lingual%2520Emotion%2520Detection%26entry.906535625%3DLong%2520Cheng%2520and%2520Qihao%2520Shao%2520and%2520Christine%2520Zhao%2520and%2520Sheng%2520Bi%2520and%2520Gina-Anne%2520Levow%26entry.1292438233%3D%2520%2520Cross-lingual%2520emotion%2520detection%2520allows%2520us%2520to%2520analyze%2520global%2520trends%252C%2520public%250Aopinion%252C%2520and%2520social%2520phenomena%2520at%2520scale.%2520We%2520participated%2520in%2520the%2520Explainability%250Aof%2520Cross-lingual%2520Emotion%2520Detection%2520%2528EXALT%2529%2520shared%2520task%252C%2520achieving%2520an%2520F1-score%250Aof%25200.6046%2520on%2520the%2520evaluation%2520set%2520for%2520the%2520emotion%2520detection%2520sub-task.%2520Our%2520system%250Aoutperformed%2520the%2520baseline%2520by%2520more%2520than%25200.16%2520F1-score%2520absolute%252C%2520and%2520ranked%250Asecond%2520amongst%2520competing%2520systems.%2520We%2520conducted%2520experiments%2520using%2520fine-tuning%252C%250Azero-shot%2520learning%252C%2520and%2520few-shot%2520learning%2520for%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%250Amodels%2520as%2520well%2520as%2520embedding-based%2520BiLSTM%2520and%2520KNN%2520for%2520non-LLM-based%2520techniques.%250AAdditionally%252C%2520we%2520introduced%2520two%2520novel%2520methods%253A%2520the%2520Multi-Iteration%2520Agentic%250AWorkflow%2520and%2520the%2520Multi-Binary-Classifier%2520Agentic%2520Workflow.%2520We%2520found%2520that%250ALLM-based%2520approaches%2520provided%2520good%2520performance%2520on%2520multilingual%2520emotion%250Adetection.%2520Furthermore%252C%2520ensembles%2520combining%2520all%2520our%2520experimented%2520models%2520yielded%250Ahigher%2520F1-scores%2520than%2520any%2520single%2520approach%2520alone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEII%3A%20Think%2C%20Explain%2C%20Interact%20and%20Iterate%20with%20Large%20Language%20Models%20to%0A%20%20Solve%20Cross-lingual%20Emotion%20Detection&entry.906535625=Long%20Cheng%20and%20Qihao%20Shao%20and%20Christine%20Zhao%20and%20Sheng%20Bi%20and%20Gina-Anne%20Levow&entry.1292438233=%20%20Cross-lingual%20emotion%20detection%20allows%20us%20to%20analyze%20global%20trends%2C%20public%0Aopinion%2C%20and%20social%20phenomena%20at%20scale.%20We%20participated%20in%20the%20Explainability%0Aof%20Cross-lingual%20Emotion%20Detection%20%28EXALT%29%20shared%20task%2C%20achieving%20an%20F1-score%0Aof%200.6046%20on%20the%20evaluation%20set%20for%20the%20emotion%20detection%20sub-task.%20Our%20system%0Aoutperformed%20the%20baseline%20by%20more%20than%200.16%20F1-score%20absolute%2C%20and%20ranked%0Asecond%20amongst%20competing%20systems.%20We%20conducted%20experiments%20using%20fine-tuning%2C%0Azero-shot%20learning%2C%20and%20few-shot%20learning%20for%20Large%20Language%20Model%20%28LLM%29-based%0Amodels%20as%20well%20as%20embedding-based%20BiLSTM%20and%20KNN%20for%20non-LLM-based%20techniques.%0AAdditionally%2C%20we%20introduced%20two%20novel%20methods%3A%20the%20Multi-Iteration%20Agentic%0AWorkflow%20and%20the%20Multi-Binary-Classifier%20Agentic%20Workflow.%20We%20found%20that%0ALLM-based%20approaches%20provided%20good%20performance%20on%20multilingual%20emotion%0Adetection.%20Furthermore%2C%20ensembles%20combining%20all%20our%20experimented%20models%20yielded%0Ahigher%20F1-scores%20than%20any%20single%20approach%20alone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17129v2&entry.124074799=Read"},
{"title": "RVISA: Reasoning and Verification for Implicit Sentiment Analysis", "author": "Wenna Lai and Haoran Xie and Guandong Xu and Qing Li", "abstract": "  With an increasing social demand for fine-grained sentiment analysis (SA),\nimplicit sentiment analysis (ISA) poses a significant challenge with the\nabsence of salient cue words in expressions. It necessitates reliable reasoning\nto understand how the sentiment is aroused and thus determine implicit\nsentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)\nLLMs have gained popularity to serve as backbone models for SA applications,\nconsidering impressive text comprehension and reasoning ability among diverse\ntasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural\nlanguage generation and in-context learning capabilities. However, their\nresponses may contain misleading or inaccurate information. To identify\nimplicit sentiment with reliable reasoning, this study proposes RVISA, a\ntwo-stage reasoning framework that harnesses the generation ability of DO LLMs\nand the reasoning ability of ED LLMs to train an enhanced reasoner.\nSpecifically, we adopt three-hop reasoning prompting to explicitly furnish\nsentiment elements as cues. The generated rationales are utilized to fine-tune\nan ED LLM into a skilled reasoner. Additionally, we develop a straightforward\nyet effective verification mechanism to ensure the reliability of the reasoning\nlearning. We evaluated the proposed method on two benchmark datasets and\nachieved state-of-the-art results in ISA performance.\n", "link": "http://arxiv.org/abs/2407.02340v1", "date": "2024-07-02", "relevancy": 1.9529, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4946}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4843}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RVISA%3A%20Reasoning%20and%20Verification%20for%20Implicit%20Sentiment%20Analysis&body=Title%3A%20RVISA%3A%20Reasoning%20and%20Verification%20for%20Implicit%20Sentiment%20Analysis%0AAuthor%3A%20Wenna%20Lai%20and%20Haoran%20Xie%20and%20Guandong%20Xu%20and%20Qing%20Li%0AAbstract%3A%20%20%20With%20an%20increasing%20social%20demand%20for%20fine-grained%20sentiment%20analysis%20%28SA%29%2C%0Aimplicit%20sentiment%20analysis%20%28ISA%29%20poses%20a%20significant%20challenge%20with%20the%0Aabsence%20of%20salient%20cue%20words%20in%20expressions.%20It%20necessitates%20reliable%20reasoning%0Ato%20understand%20how%20the%20sentiment%20is%20aroused%20and%20thus%20determine%20implicit%0Asentiments.%20In%20the%20era%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20Encoder-Decoder%20%28ED%29%0ALLMs%20have%20gained%20popularity%20to%20serve%20as%20backbone%20models%20for%20SA%20applications%2C%0Aconsidering%20impressive%20text%20comprehension%20and%20reasoning%20ability%20among%20diverse%0Atasks.%20On%20the%20other%20hand%2C%20Decoder-only%20%28DO%29%20LLMs%20exhibit%20superior%20natural%0Alanguage%20generation%20and%20in-context%20learning%20capabilities.%20However%2C%20their%0Aresponses%20may%20contain%20misleading%20or%20inaccurate%20information.%20To%20identify%0Aimplicit%20sentiment%20with%20reliable%20reasoning%2C%20this%20study%20proposes%20RVISA%2C%20a%0Atwo-stage%20reasoning%20framework%20that%20harnesses%20the%20generation%20ability%20of%20DO%20LLMs%0Aand%20the%20reasoning%20ability%20of%20ED%20LLMs%20to%20train%20an%20enhanced%20reasoner.%0ASpecifically%2C%20we%20adopt%20three-hop%20reasoning%20prompting%20to%20explicitly%20furnish%0Asentiment%20elements%20as%20cues.%20The%20generated%20rationales%20are%20utilized%20to%20fine-tune%0Aan%20ED%20LLM%20into%20a%20skilled%20reasoner.%20Additionally%2C%20we%20develop%20a%20straightforward%0Ayet%20effective%20verification%20mechanism%20to%20ensure%20the%20reliability%20of%20the%20reasoning%0Alearning.%20We%20evaluated%20the%20proposed%20method%20on%20two%20benchmark%20datasets%20and%0Aachieved%20state-of-the-art%20results%20in%20ISA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRVISA%253A%2520Reasoning%2520and%2520Verification%2520for%2520Implicit%2520Sentiment%2520Analysis%26entry.906535625%3DWenna%2520Lai%2520and%2520Haoran%2520Xie%2520and%2520Guandong%2520Xu%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520With%2520an%2520increasing%2520social%2520demand%2520for%2520fine-grained%2520sentiment%2520analysis%2520%2528SA%2529%252C%250Aimplicit%2520sentiment%2520analysis%2520%2528ISA%2529%2520poses%2520a%2520significant%2520challenge%2520with%2520the%250Aabsence%2520of%2520salient%2520cue%2520words%2520in%2520expressions.%2520It%2520necessitates%2520reliable%2520reasoning%250Ato%2520understand%2520how%2520the%2520sentiment%2520is%2520aroused%2520and%2520thus%2520determine%2520implicit%250Asentiments.%2520In%2520the%2520era%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Encoder-Decoder%2520%2528ED%2529%250ALLMs%2520have%2520gained%2520popularity%2520to%2520serve%2520as%2520backbone%2520models%2520for%2520SA%2520applications%252C%250Aconsidering%2520impressive%2520text%2520comprehension%2520and%2520reasoning%2520ability%2520among%2520diverse%250Atasks.%2520On%2520the%2520other%2520hand%252C%2520Decoder-only%2520%2528DO%2529%2520LLMs%2520exhibit%2520superior%2520natural%250Alanguage%2520generation%2520and%2520in-context%2520learning%2520capabilities.%2520However%252C%2520their%250Aresponses%2520may%2520contain%2520misleading%2520or%2520inaccurate%2520information.%2520To%2520identify%250Aimplicit%2520sentiment%2520with%2520reliable%2520reasoning%252C%2520this%2520study%2520proposes%2520RVISA%252C%2520a%250Atwo-stage%2520reasoning%2520framework%2520that%2520harnesses%2520the%2520generation%2520ability%2520of%2520DO%2520LLMs%250Aand%2520the%2520reasoning%2520ability%2520of%2520ED%2520LLMs%2520to%2520train%2520an%2520enhanced%2520reasoner.%250ASpecifically%252C%2520we%2520adopt%2520three-hop%2520reasoning%2520prompting%2520to%2520explicitly%2520furnish%250Asentiment%2520elements%2520as%2520cues.%2520The%2520generated%2520rationales%2520are%2520utilized%2520to%2520fine-tune%250Aan%2520ED%2520LLM%2520into%2520a%2520skilled%2520reasoner.%2520Additionally%252C%2520we%2520develop%2520a%2520straightforward%250Ayet%2520effective%2520verification%2520mechanism%2520to%2520ensure%2520the%2520reliability%2520of%2520the%2520reasoning%250Alearning.%2520We%2520evaluated%2520the%2520proposed%2520method%2520on%2520two%2520benchmark%2520datasets%2520and%250Aachieved%2520state-of-the-art%2520results%2520in%2520ISA%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RVISA%3A%20Reasoning%20and%20Verification%20for%20Implicit%20Sentiment%20Analysis&entry.906535625=Wenna%20Lai%20and%20Haoran%20Xie%20and%20Guandong%20Xu%20and%20Qing%20Li&entry.1292438233=%20%20With%20an%20increasing%20social%20demand%20for%20fine-grained%20sentiment%20analysis%20%28SA%29%2C%0Aimplicit%20sentiment%20analysis%20%28ISA%29%20poses%20a%20significant%20challenge%20with%20the%0Aabsence%20of%20salient%20cue%20words%20in%20expressions.%20It%20necessitates%20reliable%20reasoning%0Ato%20understand%20how%20the%20sentiment%20is%20aroused%20and%20thus%20determine%20implicit%0Asentiments.%20In%20the%20era%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20Encoder-Decoder%20%28ED%29%0ALLMs%20have%20gained%20popularity%20to%20serve%20as%20backbone%20models%20for%20SA%20applications%2C%0Aconsidering%20impressive%20text%20comprehension%20and%20reasoning%20ability%20among%20diverse%0Atasks.%20On%20the%20other%20hand%2C%20Decoder-only%20%28DO%29%20LLMs%20exhibit%20superior%20natural%0Alanguage%20generation%20and%20in-context%20learning%20capabilities.%20However%2C%20their%0Aresponses%20may%20contain%20misleading%20or%20inaccurate%20information.%20To%20identify%0Aimplicit%20sentiment%20with%20reliable%20reasoning%2C%20this%20study%20proposes%20RVISA%2C%20a%0Atwo-stage%20reasoning%20framework%20that%20harnesses%20the%20generation%20ability%20of%20DO%20LLMs%0Aand%20the%20reasoning%20ability%20of%20ED%20LLMs%20to%20train%20an%20enhanced%20reasoner.%0ASpecifically%2C%20we%20adopt%20three-hop%20reasoning%20prompting%20to%20explicitly%20furnish%0Asentiment%20elements%20as%20cues.%20The%20generated%20rationales%20are%20utilized%20to%20fine-tune%0Aan%20ED%20LLM%20into%20a%20skilled%20reasoner.%20Additionally%2C%20we%20develop%20a%20straightforward%0Ayet%20effective%20verification%20mechanism%20to%20ensure%20the%20reliability%20of%20the%20reasoning%0Alearning.%20We%20evaluated%20the%20proposed%20method%20on%20two%20benchmark%20datasets%20and%0Aachieved%20state-of-the-art%20results%20in%20ISA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02340v1&entry.124074799=Read"},
{"title": "SOAF: Scene Occlusion-aware Neural Acoustic Field", "author": "Huiyu Gao and Jiahao Ma and David Ahmedt-Aristizabal and Chuong Nguyen and Miaomiao Liu", "abstract": "  This paper tackles the problem of novel view audio-visual synthesis along an\narbitrary trajectory in an indoor scene, given the audio-video recordings from\nother known trajectories of the scene. Existing methods often overlook the\neffect of room geometry, particularly wall occlusion to sound propagation,\nmaking them less accurate in multi-room environments. In this work, we propose\na new approach called Scene Occlusion-aware Acoustic Field (SOAF) for accurate\nsound generation. Our approach derives a prior for sound energy field using\ndistance-aware parametric sound-propagation modelling and then transforms it\nbased on scene transmittance learned from the input video. We extract features\nfrom the local acoustic field centred around the receiver using a Fibonacci\nSphere to generate binaural audio for novel views with a direction-aware\nattention mechanism. Extensive experiments on the real dataset~\\emph{RWAVS} and\nthe synthetic dataset~\\emph{SoundSpaces} demonstrate that our method\noutperforms previous state-of-the-art techniques in audio generation. Project\npage: https://github.com/huiyu-gao/SOAF/.\n", "link": "http://arxiv.org/abs/2407.02264v1", "date": "2024-07-02", "relevancy": 1.9491, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4845}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOAF%3A%20Scene%20Occlusion-aware%20Neural%20Acoustic%20Field&body=Title%3A%20SOAF%3A%20Scene%20Occlusion-aware%20Neural%20Acoustic%20Field%0AAuthor%3A%20Huiyu%20Gao%20and%20Jiahao%20Ma%20and%20David%20Ahmedt-Aristizabal%20and%20Chuong%20Nguyen%20and%20Miaomiao%20Liu%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20problem%20of%20novel%20view%20audio-visual%20synthesis%20along%20an%0Aarbitrary%20trajectory%20in%20an%20indoor%20scene%2C%20given%20the%20audio-video%20recordings%20from%0Aother%20known%20trajectories%20of%20the%20scene.%20Existing%20methods%20often%20overlook%20the%0Aeffect%20of%20room%20geometry%2C%20particularly%20wall%20occlusion%20to%20sound%20propagation%2C%0Amaking%20them%20less%20accurate%20in%20multi-room%20environments.%20In%20this%20work%2C%20we%20propose%0Aa%20new%20approach%20called%20Scene%20Occlusion-aware%20Acoustic%20Field%20%28SOAF%29%20for%20accurate%0Asound%20generation.%20Our%20approach%20derives%20a%20prior%20for%20sound%20energy%20field%20using%0Adistance-aware%20parametric%20sound-propagation%20modelling%20and%20then%20transforms%20it%0Abased%20on%20scene%20transmittance%20learned%20from%20the%20input%20video.%20We%20extract%20features%0Afrom%20the%20local%20acoustic%20field%20centred%20around%20the%20receiver%20using%20a%20Fibonacci%0ASphere%20to%20generate%20binaural%20audio%20for%20novel%20views%20with%20a%20direction-aware%0Aattention%20mechanism.%20Extensive%20experiments%20on%20the%20real%20dataset~%5Cemph%7BRWAVS%7D%20and%0Athe%20synthetic%20dataset~%5Cemph%7BSoundSpaces%7D%20demonstrate%20that%20our%20method%0Aoutperforms%20previous%20state-of-the-art%20techniques%20in%20audio%20generation.%20Project%0Apage%3A%20https%3A//github.com/huiyu-gao/SOAF/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOAF%253A%2520Scene%2520Occlusion-aware%2520Neural%2520Acoustic%2520Field%26entry.906535625%3DHuiyu%2520Gao%2520and%2520Jiahao%2520Ma%2520and%2520David%2520Ahmedt-Aristizabal%2520and%2520Chuong%2520Nguyen%2520and%2520Miaomiao%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520problem%2520of%2520novel%2520view%2520audio-visual%2520synthesis%2520along%2520an%250Aarbitrary%2520trajectory%2520in%2520an%2520indoor%2520scene%252C%2520given%2520the%2520audio-video%2520recordings%2520from%250Aother%2520known%2520trajectories%2520of%2520the%2520scene.%2520Existing%2520methods%2520often%2520overlook%2520the%250Aeffect%2520of%2520room%2520geometry%252C%2520particularly%2520wall%2520occlusion%2520to%2520sound%2520propagation%252C%250Amaking%2520them%2520less%2520accurate%2520in%2520multi-room%2520environments.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520new%2520approach%2520called%2520Scene%2520Occlusion-aware%2520Acoustic%2520Field%2520%2528SOAF%2529%2520for%2520accurate%250Asound%2520generation.%2520Our%2520approach%2520derives%2520a%2520prior%2520for%2520sound%2520energy%2520field%2520using%250Adistance-aware%2520parametric%2520sound-propagation%2520modelling%2520and%2520then%2520transforms%2520it%250Abased%2520on%2520scene%2520transmittance%2520learned%2520from%2520the%2520input%2520video.%2520We%2520extract%2520features%250Afrom%2520the%2520local%2520acoustic%2520field%2520centred%2520around%2520the%2520receiver%2520using%2520a%2520Fibonacci%250ASphere%2520to%2520generate%2520binaural%2520audio%2520for%2520novel%2520views%2520with%2520a%2520direction-aware%250Aattention%2520mechanism.%2520Extensive%2520experiments%2520on%2520the%2520real%2520dataset~%255Cemph%257BRWAVS%257D%2520and%250Athe%2520synthetic%2520dataset~%255Cemph%257BSoundSpaces%257D%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520previous%2520state-of-the-art%2520techniques%2520in%2520audio%2520generation.%2520Project%250Apage%253A%2520https%253A//github.com/huiyu-gao/SOAF/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOAF%3A%20Scene%20Occlusion-aware%20Neural%20Acoustic%20Field&entry.906535625=Huiyu%20Gao%20and%20Jiahao%20Ma%20and%20David%20Ahmedt-Aristizabal%20and%20Chuong%20Nguyen%20and%20Miaomiao%20Liu&entry.1292438233=%20%20This%20paper%20tackles%20the%20problem%20of%20novel%20view%20audio-visual%20synthesis%20along%20an%0Aarbitrary%20trajectory%20in%20an%20indoor%20scene%2C%20given%20the%20audio-video%20recordings%20from%0Aother%20known%20trajectories%20of%20the%20scene.%20Existing%20methods%20often%20overlook%20the%0Aeffect%20of%20room%20geometry%2C%20particularly%20wall%20occlusion%20to%20sound%20propagation%2C%0Amaking%20them%20less%20accurate%20in%20multi-room%20environments.%20In%20this%20work%2C%20we%20propose%0Aa%20new%20approach%20called%20Scene%20Occlusion-aware%20Acoustic%20Field%20%28SOAF%29%20for%20accurate%0Asound%20generation.%20Our%20approach%20derives%20a%20prior%20for%20sound%20energy%20field%20using%0Adistance-aware%20parametric%20sound-propagation%20modelling%20and%20then%20transforms%20it%0Abased%20on%20scene%20transmittance%20learned%20from%20the%20input%20video.%20We%20extract%20features%0Afrom%20the%20local%20acoustic%20field%20centred%20around%20the%20receiver%20using%20a%20Fibonacci%0ASphere%20to%20generate%20binaural%20audio%20for%20novel%20views%20with%20a%20direction-aware%0Aattention%20mechanism.%20Extensive%20experiments%20on%20the%20real%20dataset~%5Cemph%7BRWAVS%7D%20and%0Athe%20synthetic%20dataset~%5Cemph%7BSoundSpaces%7D%20demonstrate%20that%20our%20method%0Aoutperforms%20previous%20state-of-the-art%20techniques%20in%20audio%20generation.%20Project%0Apage%3A%20https%3A//github.com/huiyu-gao/SOAF/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02264v1&entry.124074799=Read"},
{"title": "Counterfactual Data Augmentation with Denoising Diffusion for Graph\n  Anomaly Detection", "author": "Chunjing Xiao and Shikang Pang and Xovee Xu and Xuan Li and Goce Trajcevski and Fan Zhou", "abstract": "  A critical aspect of Graph Neural Networks (GNNs) is to enhance the node\nrepresentations by aggregating node neighborhood information. However, when\ndetecting anomalies, the representations of abnormal nodes are prone to be\naveraged by normal neighbors, making the learned anomaly representations less\ndistinguishable. To tackle this issue, we propose CAGAD -- an unsupervised\nCounterfactual data Augmentation method for Graph Anomaly Detection -- which\nintroduces a graph pointer neural network as the heterophilic node detector to\nidentify potential anomalies whose neighborhoods are normal-node-dominant. For\neach identified potential anomaly, we design a graph-specific diffusion model\nto translate a part of its neighbors, which are probably normal, into anomalous\nones. At last, we involve these translated neighbors in GNN neighborhood\naggregation to produce counterfactual representations of anomalies. Through\naggregating the translated anomalous neighbors, counterfactual representations\nbecome more distinguishable and further advocate detection performance. The\nexperimental results on four datasets demonstrate that CAGAD significantly\noutperforms strong baselines, with an average improvement of 2.35% on F1, 2.53%\non AUC-ROC, and 2.79% on AUC-PR.\n", "link": "http://arxiv.org/abs/2407.02143v1", "date": "2024-07-02", "relevancy": 1.9486, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Data%20Augmentation%20with%20Denoising%20Diffusion%20for%20Graph%0A%20%20Anomaly%20Detection&body=Title%3A%20Counterfactual%20Data%20Augmentation%20with%20Denoising%20Diffusion%20for%20Graph%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Chunjing%20Xiao%20and%20Shikang%20Pang%20and%20Xovee%20Xu%20and%20Xuan%20Li%20and%20Goce%20Trajcevski%20and%20Fan%20Zhou%0AAbstract%3A%20%20%20A%20critical%20aspect%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20is%20to%20enhance%20the%20node%0Arepresentations%20by%20aggregating%20node%20neighborhood%20information.%20However%2C%20when%0Adetecting%20anomalies%2C%20the%20representations%20of%20abnormal%20nodes%20are%20prone%20to%20be%0Aaveraged%20by%20normal%20neighbors%2C%20making%20the%20learned%20anomaly%20representations%20less%0Adistinguishable.%20To%20tackle%20this%20issue%2C%20we%20propose%20CAGAD%20--%20an%20unsupervised%0ACounterfactual%20data%20Augmentation%20method%20for%20Graph%20Anomaly%20Detection%20--%20which%0Aintroduces%20a%20graph%20pointer%20neural%20network%20as%20the%20heterophilic%20node%20detector%20to%0Aidentify%20potential%20anomalies%20whose%20neighborhoods%20are%20normal-node-dominant.%20For%0Aeach%20identified%20potential%20anomaly%2C%20we%20design%20a%20graph-specific%20diffusion%20model%0Ato%20translate%20a%20part%20of%20its%20neighbors%2C%20which%20are%20probably%20normal%2C%20into%20anomalous%0Aones.%20At%20last%2C%20we%20involve%20these%20translated%20neighbors%20in%20GNN%20neighborhood%0Aaggregation%20to%20produce%20counterfactual%20representations%20of%20anomalies.%20Through%0Aaggregating%20the%20translated%20anomalous%20neighbors%2C%20counterfactual%20representations%0Abecome%20more%20distinguishable%20and%20further%20advocate%20detection%20performance.%20The%0Aexperimental%20results%20on%20four%20datasets%20demonstrate%20that%20CAGAD%20significantly%0Aoutperforms%20strong%20baselines%2C%20with%20an%20average%20improvement%20of%202.35%25%20on%20F1%2C%202.53%25%0Aon%20AUC-ROC%2C%20and%202.79%25%20on%20AUC-PR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Data%2520Augmentation%2520with%2520Denoising%2520Diffusion%2520for%2520Graph%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DChunjing%2520Xiao%2520and%2520Shikang%2520Pang%2520and%2520Xovee%2520Xu%2520and%2520Xuan%2520Li%2520and%2520Goce%2520Trajcevski%2520and%2520Fan%2520Zhou%26entry.1292438233%3D%2520%2520A%2520critical%2520aspect%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520is%2520to%2520enhance%2520the%2520node%250Arepresentations%2520by%2520aggregating%2520node%2520neighborhood%2520information.%2520However%252C%2520when%250Adetecting%2520anomalies%252C%2520the%2520representations%2520of%2520abnormal%2520nodes%2520are%2520prone%2520to%2520be%250Aaveraged%2520by%2520normal%2520neighbors%252C%2520making%2520the%2520learned%2520anomaly%2520representations%2520less%250Adistinguishable.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520CAGAD%2520--%2520an%2520unsupervised%250ACounterfactual%2520data%2520Augmentation%2520method%2520for%2520Graph%2520Anomaly%2520Detection%2520--%2520which%250Aintroduces%2520a%2520graph%2520pointer%2520neural%2520network%2520as%2520the%2520heterophilic%2520node%2520detector%2520to%250Aidentify%2520potential%2520anomalies%2520whose%2520neighborhoods%2520are%2520normal-node-dominant.%2520For%250Aeach%2520identified%2520potential%2520anomaly%252C%2520we%2520design%2520a%2520graph-specific%2520diffusion%2520model%250Ato%2520translate%2520a%2520part%2520of%2520its%2520neighbors%252C%2520which%2520are%2520probably%2520normal%252C%2520into%2520anomalous%250Aones.%2520At%2520last%252C%2520we%2520involve%2520these%2520translated%2520neighbors%2520in%2520GNN%2520neighborhood%250Aaggregation%2520to%2520produce%2520counterfactual%2520representations%2520of%2520anomalies.%2520Through%250Aaggregating%2520the%2520translated%2520anomalous%2520neighbors%252C%2520counterfactual%2520representations%250Abecome%2520more%2520distinguishable%2520and%2520further%2520advocate%2520detection%2520performance.%2520The%250Aexperimental%2520results%2520on%2520four%2520datasets%2520demonstrate%2520that%2520CAGAD%2520significantly%250Aoutperforms%2520strong%2520baselines%252C%2520with%2520an%2520average%2520improvement%2520of%25202.35%2525%2520on%2520F1%252C%25202.53%2525%250Aon%2520AUC-ROC%252C%2520and%25202.79%2525%2520on%2520AUC-PR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Data%20Augmentation%20with%20Denoising%20Diffusion%20for%20Graph%0A%20%20Anomaly%20Detection&entry.906535625=Chunjing%20Xiao%20and%20Shikang%20Pang%20and%20Xovee%20Xu%20and%20Xuan%20Li%20and%20Goce%20Trajcevski%20and%20Fan%20Zhou&entry.1292438233=%20%20A%20critical%20aspect%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20is%20to%20enhance%20the%20node%0Arepresentations%20by%20aggregating%20node%20neighborhood%20information.%20However%2C%20when%0Adetecting%20anomalies%2C%20the%20representations%20of%20abnormal%20nodes%20are%20prone%20to%20be%0Aaveraged%20by%20normal%20neighbors%2C%20making%20the%20learned%20anomaly%20representations%20less%0Adistinguishable.%20To%20tackle%20this%20issue%2C%20we%20propose%20CAGAD%20--%20an%20unsupervised%0ACounterfactual%20data%20Augmentation%20method%20for%20Graph%20Anomaly%20Detection%20--%20which%0Aintroduces%20a%20graph%20pointer%20neural%20network%20as%20the%20heterophilic%20node%20detector%20to%0Aidentify%20potential%20anomalies%20whose%20neighborhoods%20are%20normal-node-dominant.%20For%0Aeach%20identified%20potential%20anomaly%2C%20we%20design%20a%20graph-specific%20diffusion%20model%0Ato%20translate%20a%20part%20of%20its%20neighbors%2C%20which%20are%20probably%20normal%2C%20into%20anomalous%0Aones.%20At%20last%2C%20we%20involve%20these%20translated%20neighbors%20in%20GNN%20neighborhood%0Aaggregation%20to%20produce%20counterfactual%20representations%20of%20anomalies.%20Through%0Aaggregating%20the%20translated%20anomalous%20neighbors%2C%20counterfactual%20representations%0Abecome%20more%20distinguishable%20and%20further%20advocate%20detection%20performance.%20The%0Aexperimental%20results%20on%20four%20datasets%20demonstrate%20that%20CAGAD%20significantly%0Aoutperforms%20strong%20baselines%2C%20with%20an%20average%20improvement%20of%202.35%25%20on%20F1%2C%202.53%25%0Aon%20AUC-ROC%2C%20and%202.79%25%20on%20AUC-PR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02143v1&entry.124074799=Read"},
{"title": "Parameter-Selective Continual Test-Time Adaptation", "author": "Jiaxu Tian and Fan Lyu", "abstract": "  Continual Test-Time Adaptation (CTTA) aims to adapt a pretrained model to\never-changing environments during the test time under continuous domain shifts.\nMost existing CTTA approaches are based on the Mean Teacher (MT) structure,\nwhich contains a student and a teacher model, where the student is updated\nusing the pseudo-labels from the teacher model, and the teacher is then updated\nby exponential moving average strategy. However, these methods update the MT\nmodel indiscriminately on all parameters of the model. That is, some critical\nparameters involving sharing knowledge across different domains may be erased,\nintensifying error accumulation and catastrophic forgetting. In this paper, we\nintroduce Parameter-Selective Mean Teacher (PSMT) method, which is capable of\neffectively updating the critical parameters within the MT network under domain\nshifts. First, we introduce a selective distillation mechanism in the student\nmodel, which utilizes past knowledge to regularize novel knowledge, thereby\nmitigating the impact of error accumulation. Second, to avoid catastrophic\nforgetting, in the teacher model, we create a mask through Fisher information\nto selectively update parameters via exponential moving average, with\npreservation measures applied to crucial parameters. Extensive experimental\nresults verify that PSMT outperforms state-of-the-art methods across multiple\nbenchmark datasets. Our code is available at\n\\url{https://github.com/JiaxuTian/PSMT}.\n", "link": "http://arxiv.org/abs/2407.02253v1", "date": "2024-07-02", "relevancy": 1.9462, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4695}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Selective%20Continual%20Test-Time%20Adaptation&body=Title%3A%20Parameter-Selective%20Continual%20Test-Time%20Adaptation%0AAuthor%3A%20Jiaxu%20Tian%20and%20Fan%20Lyu%0AAbstract%3A%20%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20aims%20to%20adapt%20a%20pretrained%20model%20to%0Aever-changing%20environments%20during%20the%20test%20time%20under%20continuous%20domain%20shifts.%0AMost%20existing%20CTTA%20approaches%20are%20based%20on%20the%20Mean%20Teacher%20%28MT%29%20structure%2C%0Awhich%20contains%20a%20student%20and%20a%20teacher%20model%2C%20where%20the%20student%20is%20updated%0Ausing%20the%20pseudo-labels%20from%20the%20teacher%20model%2C%20and%20the%20teacher%20is%20then%20updated%0Aby%20exponential%20moving%20average%20strategy.%20However%2C%20these%20methods%20update%20the%20MT%0Amodel%20indiscriminately%20on%20all%20parameters%20of%20the%20model.%20That%20is%2C%20some%20critical%0Aparameters%20involving%20sharing%20knowledge%20across%20different%20domains%20may%20be%20erased%2C%0Aintensifying%20error%20accumulation%20and%20catastrophic%20forgetting.%20In%20this%20paper%2C%20we%0Aintroduce%20Parameter-Selective%20Mean%20Teacher%20%28PSMT%29%20method%2C%20which%20is%20capable%20of%0Aeffectively%20updating%20the%20critical%20parameters%20within%20the%20MT%20network%20under%20domain%0Ashifts.%20First%2C%20we%20introduce%20a%20selective%20distillation%20mechanism%20in%20the%20student%0Amodel%2C%20which%20utilizes%20past%20knowledge%20to%20regularize%20novel%20knowledge%2C%20thereby%0Amitigating%20the%20impact%20of%20error%20accumulation.%20Second%2C%20to%20avoid%20catastrophic%0Aforgetting%2C%20in%20the%20teacher%20model%2C%20we%20create%20a%20mask%20through%20Fisher%20information%0Ato%20selectively%20update%20parameters%20via%20exponential%20moving%20average%2C%20with%0Apreservation%20measures%20applied%20to%20crucial%20parameters.%20Extensive%20experimental%0Aresults%20verify%20that%20PSMT%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Abenchmark%20datasets.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/JiaxuTian/PSMT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Selective%2520Continual%2520Test-Time%2520Adaptation%26entry.906535625%3DJiaxu%2520Tian%2520and%2520Fan%2520Lyu%26entry.1292438233%3D%2520%2520Continual%2520Test-Time%2520Adaptation%2520%2528CTTA%2529%2520aims%2520to%2520adapt%2520a%2520pretrained%2520model%2520to%250Aever-changing%2520environments%2520during%2520the%2520test%2520time%2520under%2520continuous%2520domain%2520shifts.%250AMost%2520existing%2520CTTA%2520approaches%2520are%2520based%2520on%2520the%2520Mean%2520Teacher%2520%2528MT%2529%2520structure%252C%250Awhich%2520contains%2520a%2520student%2520and%2520a%2520teacher%2520model%252C%2520where%2520the%2520student%2520is%2520updated%250Ausing%2520the%2520pseudo-labels%2520from%2520the%2520teacher%2520model%252C%2520and%2520the%2520teacher%2520is%2520then%2520updated%250Aby%2520exponential%2520moving%2520average%2520strategy.%2520However%252C%2520these%2520methods%2520update%2520the%2520MT%250Amodel%2520indiscriminately%2520on%2520all%2520parameters%2520of%2520the%2520model.%2520That%2520is%252C%2520some%2520critical%250Aparameters%2520involving%2520sharing%2520knowledge%2520across%2520different%2520domains%2520may%2520be%2520erased%252C%250Aintensifying%2520error%2520accumulation%2520and%2520catastrophic%2520forgetting.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Parameter-Selective%2520Mean%2520Teacher%2520%2528PSMT%2529%2520method%252C%2520which%2520is%2520capable%2520of%250Aeffectively%2520updating%2520the%2520critical%2520parameters%2520within%2520the%2520MT%2520network%2520under%2520domain%250Ashifts.%2520First%252C%2520we%2520introduce%2520a%2520selective%2520distillation%2520mechanism%2520in%2520the%2520student%250Amodel%252C%2520which%2520utilizes%2520past%2520knowledge%2520to%2520regularize%2520novel%2520knowledge%252C%2520thereby%250Amitigating%2520the%2520impact%2520of%2520error%2520accumulation.%2520Second%252C%2520to%2520avoid%2520catastrophic%250Aforgetting%252C%2520in%2520the%2520teacher%2520model%252C%2520we%2520create%2520a%2520mask%2520through%2520Fisher%2520information%250Ato%2520selectively%2520update%2520parameters%2520via%2520exponential%2520moving%2520average%252C%2520with%250Apreservation%2520measures%2520applied%2520to%2520crucial%2520parameters.%2520Extensive%2520experimental%250Aresults%2520verify%2520that%2520PSMT%2520outperforms%2520state-of-the-art%2520methods%2520across%2520multiple%250Abenchmark%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/JiaxuTian/PSMT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Selective%20Continual%20Test-Time%20Adaptation&entry.906535625=Jiaxu%20Tian%20and%20Fan%20Lyu&entry.1292438233=%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20aims%20to%20adapt%20a%20pretrained%20model%20to%0Aever-changing%20environments%20during%20the%20test%20time%20under%20continuous%20domain%20shifts.%0AMost%20existing%20CTTA%20approaches%20are%20based%20on%20the%20Mean%20Teacher%20%28MT%29%20structure%2C%0Awhich%20contains%20a%20student%20and%20a%20teacher%20model%2C%20where%20the%20student%20is%20updated%0Ausing%20the%20pseudo-labels%20from%20the%20teacher%20model%2C%20and%20the%20teacher%20is%20then%20updated%0Aby%20exponential%20moving%20average%20strategy.%20However%2C%20these%20methods%20update%20the%20MT%0Amodel%20indiscriminately%20on%20all%20parameters%20of%20the%20model.%20That%20is%2C%20some%20critical%0Aparameters%20involving%20sharing%20knowledge%20across%20different%20domains%20may%20be%20erased%2C%0Aintensifying%20error%20accumulation%20and%20catastrophic%20forgetting.%20In%20this%20paper%2C%20we%0Aintroduce%20Parameter-Selective%20Mean%20Teacher%20%28PSMT%29%20method%2C%20which%20is%20capable%20of%0Aeffectively%20updating%20the%20critical%20parameters%20within%20the%20MT%20network%20under%20domain%0Ashifts.%20First%2C%20we%20introduce%20a%20selective%20distillation%20mechanism%20in%20the%20student%0Amodel%2C%20which%20utilizes%20past%20knowledge%20to%20regularize%20novel%20knowledge%2C%20thereby%0Amitigating%20the%20impact%20of%20error%20accumulation.%20Second%2C%20to%20avoid%20catastrophic%0Aforgetting%2C%20in%20the%20teacher%20model%2C%20we%20create%20a%20mask%20through%20Fisher%20information%0Ato%20selectively%20update%20parameters%20via%20exponential%20moving%20average%2C%20with%0Apreservation%20measures%20applied%20to%20crucial%20parameters.%20Extensive%20experimental%0Aresults%20verify%20that%20PSMT%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Abenchmark%20datasets.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/JiaxuTian/PSMT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02253v1&entry.124074799=Read"},
{"title": "Vision-LSTM: xLSTM as Generic Vision Backbone", "author": "Benedikt Alkin and Maximilian Beck and Korbinian P\u00f6ppel and Sepp Hochreiter and Johannes Brandstetter", "abstract": "  Transformers are widely used as generic backbones in computer vision, despite\ninitially introduced for natural language processing. Recently, the Long\nShort-Term Memory (LSTM) has been extended to a scalable and performant\narchitecture - the xLSTM - which overcomes long-standing LSTM limitations via\nexponential gating and parallelizable matrix memory structure. In this report,\nwe introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to\ncomputer vision. ViL comprises a stack of xLSTM blocks where odd blocks process\nthe sequence of patch tokens from top to bottom while even blocks go from\nbottom to top. Experiments show that ViL holds promise to be further deployed\nas new generic backbone for computer vision architectures.\n", "link": "http://arxiv.org/abs/2406.04303v2", "date": "2024-07-02", "relevancy": 1.9462, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-LSTM%3A%20xLSTM%20as%20Generic%20Vision%20Backbone&body=Title%3A%20Vision-LSTM%3A%20xLSTM%20as%20Generic%20Vision%20Backbone%0AAuthor%3A%20Benedikt%20Alkin%20and%20Maximilian%20Beck%20and%20Korbinian%20P%C3%B6ppel%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20Transformers%20are%20widely%20used%20as%20generic%20backbones%20in%20computer%20vision%2C%20despite%0Ainitially%20introduced%20for%20natural%20language%20processing.%20Recently%2C%20the%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20been%20extended%20to%20a%20scalable%20and%20performant%0Aarchitecture%20-%20the%20xLSTM%20-%20which%20overcomes%20long-standing%20LSTM%20limitations%20via%0Aexponential%20gating%20and%20parallelizable%20matrix%20memory%20structure.%20In%20this%20report%2C%0Awe%20introduce%20Vision-LSTM%20%28ViL%29%2C%20an%20adaption%20of%20the%20xLSTM%20building%20blocks%20to%0Acomputer%20vision.%20ViL%20comprises%20a%20stack%20of%20xLSTM%20blocks%20where%20odd%20blocks%20process%0Athe%20sequence%20of%20patch%20tokens%20from%20top%20to%20bottom%20while%20even%20blocks%20go%20from%0Abottom%20to%20top.%20Experiments%20show%20that%20ViL%20holds%20promise%20to%20be%20further%20deployed%0Aas%20new%20generic%20backbone%20for%20computer%20vision%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-LSTM%253A%2520xLSTM%2520as%2520Generic%2520Vision%2520Backbone%26entry.906535625%3DBenedikt%2520Alkin%2520and%2520Maximilian%2520Beck%2520and%2520Korbinian%2520P%25C3%25B6ppel%2520and%2520Sepp%2520Hochreiter%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520Transformers%2520are%2520widely%2520used%2520as%2520generic%2520backbones%2520in%2520computer%2520vision%252C%2520despite%250Ainitially%2520introduced%2520for%2520natural%2520language%2520processing.%2520Recently%252C%2520the%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%2520has%2520been%2520extended%2520to%2520a%2520scalable%2520and%2520performant%250Aarchitecture%2520-%2520the%2520xLSTM%2520-%2520which%2520overcomes%2520long-standing%2520LSTM%2520limitations%2520via%250Aexponential%2520gating%2520and%2520parallelizable%2520matrix%2520memory%2520structure.%2520In%2520this%2520report%252C%250Awe%2520introduce%2520Vision-LSTM%2520%2528ViL%2529%252C%2520an%2520adaption%2520of%2520the%2520xLSTM%2520building%2520blocks%2520to%250Acomputer%2520vision.%2520ViL%2520comprises%2520a%2520stack%2520of%2520xLSTM%2520blocks%2520where%2520odd%2520blocks%2520process%250Athe%2520sequence%2520of%2520patch%2520tokens%2520from%2520top%2520to%2520bottom%2520while%2520even%2520blocks%2520go%2520from%250Abottom%2520to%2520top.%2520Experiments%2520show%2520that%2520ViL%2520holds%2520promise%2520to%2520be%2520further%2520deployed%250Aas%2520new%2520generic%2520backbone%2520for%2520computer%2520vision%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-LSTM%3A%20xLSTM%20as%20Generic%20Vision%20Backbone&entry.906535625=Benedikt%20Alkin%20and%20Maximilian%20Beck%20and%20Korbinian%20P%C3%B6ppel%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20Transformers%20are%20widely%20used%20as%20generic%20backbones%20in%20computer%20vision%2C%20despite%0Ainitially%20introduced%20for%20natural%20language%20processing.%20Recently%2C%20the%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20been%20extended%20to%20a%20scalable%20and%20performant%0Aarchitecture%20-%20the%20xLSTM%20-%20which%20overcomes%20long-standing%20LSTM%20limitations%20via%0Aexponential%20gating%20and%20parallelizable%20matrix%20memory%20structure.%20In%20this%20report%2C%0Awe%20introduce%20Vision-LSTM%20%28ViL%29%2C%20an%20adaption%20of%20the%20xLSTM%20building%20blocks%20to%0Acomputer%20vision.%20ViL%20comprises%20a%20stack%20of%20xLSTM%20blocks%20where%20odd%20blocks%20process%0Athe%20sequence%20of%20patch%20tokens%20from%20top%20to%20bottom%20while%20even%20blocks%20go%20from%0Abottom%20to%20top.%20Experiments%20show%20that%20ViL%20holds%20promise%20to%20be%20further%20deployed%0Aas%20new%20generic%20backbone%20for%20computer%20vision%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04303v2&entry.124074799=Read"},
{"title": "SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech\n  Recognition", "author": "Hao Wang and Shuhei Kurita and Shuichiro Shimizu and Daisuke Kawahara", "abstract": "  Audio-visual speech recognition (AVSR) is a multimodal extension of automatic\nspeech recognition (ASR), using video as a complement to audio. In AVSR,\nconsiderable efforts have been directed at datasets for facial features such as\nlip-readings, while they often fall short in evaluating the image comprehension\ncapabilities in broader contexts. In this paper, we construct SlideAVSR, an\nAVSR dataset using scientific paper explanation videos. SlideAVSR provides a\nnew benchmark where models transcribe speech utterances with texts on the\nslides on the presentation recordings. As technical terminologies that are\nfrequent in paper explanations are notoriously challenging to transcribe\nwithout reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR\nproblems. As a simple yet effective baseline, we propose DocWhisper, an AVSR\nmodel that can refer to textual information from slides, and confirm its\neffectiveness on SlideAVSR.\n", "link": "http://arxiv.org/abs/2401.09759v2", "date": "2024-07-02", "relevancy": 1.9446, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5081}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4943}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlideAVSR%3A%20A%20Dataset%20of%20Paper%20Explanation%20Videos%20for%20Audio-Visual%20Speech%0A%20%20Recognition&body=Title%3A%20SlideAVSR%3A%20A%20Dataset%20of%20Paper%20Explanation%20Videos%20for%20Audio-Visual%20Speech%0A%20%20Recognition%0AAuthor%3A%20Hao%20Wang%20and%20Shuhei%20Kurita%20and%20Shuichiro%20Shimizu%20and%20Daisuke%20Kawahara%0AAbstract%3A%20%20%20Audio-visual%20speech%20recognition%20%28AVSR%29%20is%20a%20multimodal%20extension%20of%20automatic%0Aspeech%20recognition%20%28ASR%29%2C%20using%20video%20as%20a%20complement%20to%20audio.%20In%20AVSR%2C%0Aconsiderable%20efforts%20have%20been%20directed%20at%20datasets%20for%20facial%20features%20such%20as%0Alip-readings%2C%20while%20they%20often%20fall%20short%20in%20evaluating%20the%20image%20comprehension%0Acapabilities%20in%20broader%20contexts.%20In%20this%20paper%2C%20we%20construct%20SlideAVSR%2C%20an%0AAVSR%20dataset%20using%20scientific%20paper%20explanation%20videos.%20SlideAVSR%20provides%20a%0Anew%20benchmark%20where%20models%20transcribe%20speech%20utterances%20with%20texts%20on%20the%0Aslides%20on%20the%20presentation%20recordings.%20As%20technical%20terminologies%20that%20are%0Afrequent%20in%20paper%20explanations%20are%20notoriously%20challenging%20to%20transcribe%0Awithout%20reference%20texts%2C%20our%20SlideAVSR%20dataset%20spotlights%20a%20new%20aspect%20of%20AVSR%0Aproblems.%20As%20a%20simple%20yet%20effective%20baseline%2C%20we%20propose%20DocWhisper%2C%20an%20AVSR%0Amodel%20that%20can%20refer%20to%20textual%20information%20from%20slides%2C%20and%20confirm%20its%0Aeffectiveness%20on%20SlideAVSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlideAVSR%253A%2520A%2520Dataset%2520of%2520Paper%2520Explanation%2520Videos%2520for%2520Audio-Visual%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DHao%2520Wang%2520and%2520Shuhei%2520Kurita%2520and%2520Shuichiro%2520Shimizu%2520and%2520Daisuke%2520Kawahara%26entry.1292438233%3D%2520%2520Audio-visual%2520speech%2520recognition%2520%2528AVSR%2529%2520is%2520a%2520multimodal%2520extension%2520of%2520automatic%250Aspeech%2520recognition%2520%2528ASR%2529%252C%2520using%2520video%2520as%2520a%2520complement%2520to%2520audio.%2520In%2520AVSR%252C%250Aconsiderable%2520efforts%2520have%2520been%2520directed%2520at%2520datasets%2520for%2520facial%2520features%2520such%2520as%250Alip-readings%252C%2520while%2520they%2520often%2520fall%2520short%2520in%2520evaluating%2520the%2520image%2520comprehension%250Acapabilities%2520in%2520broader%2520contexts.%2520In%2520this%2520paper%252C%2520we%2520construct%2520SlideAVSR%252C%2520an%250AAVSR%2520dataset%2520using%2520scientific%2520paper%2520explanation%2520videos.%2520SlideAVSR%2520provides%2520a%250Anew%2520benchmark%2520where%2520models%2520transcribe%2520speech%2520utterances%2520with%2520texts%2520on%2520the%250Aslides%2520on%2520the%2520presentation%2520recordings.%2520As%2520technical%2520terminologies%2520that%2520are%250Afrequent%2520in%2520paper%2520explanations%2520are%2520notoriously%2520challenging%2520to%2520transcribe%250Awithout%2520reference%2520texts%252C%2520our%2520SlideAVSR%2520dataset%2520spotlights%2520a%2520new%2520aspect%2520of%2520AVSR%250Aproblems.%2520As%2520a%2520simple%2520yet%2520effective%2520baseline%252C%2520we%2520propose%2520DocWhisper%252C%2520an%2520AVSR%250Amodel%2520that%2520can%2520refer%2520to%2520textual%2520information%2520from%2520slides%252C%2520and%2520confirm%2520its%250Aeffectiveness%2520on%2520SlideAVSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlideAVSR%3A%20A%20Dataset%20of%20Paper%20Explanation%20Videos%20for%20Audio-Visual%20Speech%0A%20%20Recognition&entry.906535625=Hao%20Wang%20and%20Shuhei%20Kurita%20and%20Shuichiro%20Shimizu%20and%20Daisuke%20Kawahara&entry.1292438233=%20%20Audio-visual%20speech%20recognition%20%28AVSR%29%20is%20a%20multimodal%20extension%20of%20automatic%0Aspeech%20recognition%20%28ASR%29%2C%20using%20video%20as%20a%20complement%20to%20audio.%20In%20AVSR%2C%0Aconsiderable%20efforts%20have%20been%20directed%20at%20datasets%20for%20facial%20features%20such%20as%0Alip-readings%2C%20while%20they%20often%20fall%20short%20in%20evaluating%20the%20image%20comprehension%0Acapabilities%20in%20broader%20contexts.%20In%20this%20paper%2C%20we%20construct%20SlideAVSR%2C%20an%0AAVSR%20dataset%20using%20scientific%20paper%20explanation%20videos.%20SlideAVSR%20provides%20a%0Anew%20benchmark%20where%20models%20transcribe%20speech%20utterances%20with%20texts%20on%20the%0Aslides%20on%20the%20presentation%20recordings.%20As%20technical%20terminologies%20that%20are%0Afrequent%20in%20paper%20explanations%20are%20notoriously%20challenging%20to%20transcribe%0Awithout%20reference%20texts%2C%20our%20SlideAVSR%20dataset%20spotlights%20a%20new%20aspect%20of%20AVSR%0Aproblems.%20As%20a%20simple%20yet%20effective%20baseline%2C%20we%20propose%20DocWhisper%2C%20an%20AVSR%0Amodel%20that%20can%20refer%20to%20textual%20information%20from%20slides%2C%20and%20confirm%20its%0Aeffectiveness%20on%20SlideAVSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09759v2&entry.124074799=Read"},
{"title": "Scalable Multi-Output Gaussian Processes with Stochastic Variational\n  Inference", "author": "Xiaoyu Jiang and Sokratia Georgaka and Magnus Rattray and Mauricio A. Alvarez", "abstract": "  The Multi-Output Gaussian Process is is a popular tool for modelling data\nfrom multiple sources. A typical choice to build a covariance function for a\nMOGP is the Linear Model of Coregionalization (LMC) which parametrically models\nthe covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises\nthis idea by modelling the covariance between outputs using a kernel applied to\nlatent variables, one per output, leading to a flexible MOGP model that allows\nefficient generalization to new outputs with few data points. Computational\ncomplexity in LV-MOGP grows linearly with the number of outputs, which makes it\nunsuitable for problems with a large number of outputs. In this paper, we\npropose a stochastic variational inference approach for the LV-MOGP that allows\nmini-batches for both inputs and outputs, making computational complexity per\ntraining iteration independent of the number of outputs.\n", "link": "http://arxiv.org/abs/2407.02476v1", "date": "2024-07-02", "relevancy": 1.943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5071}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4889}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-Output%20Gaussian%20Processes%20with%20Stochastic%20Variational%0A%20%20Inference&body=Title%3A%20Scalable%20Multi-Output%20Gaussian%20Processes%20with%20Stochastic%20Variational%0A%20%20Inference%0AAuthor%3A%20Xiaoyu%20Jiang%20and%20Sokratia%20Georgaka%20and%20Magnus%20Rattray%20and%20Mauricio%20A.%20Alvarez%0AAbstract%3A%20%20%20The%20Multi-Output%20Gaussian%20Process%20is%20is%20a%20popular%20tool%20for%20modelling%20data%0Afrom%20multiple%20sources.%20A%20typical%20choice%20to%20build%20a%20covariance%20function%20for%20a%0AMOGP%20is%20the%20Linear%20Model%20of%20Coregionalization%20%28LMC%29%20which%20parametrically%20models%0Athe%20covariance%20between%20outputs.%20The%20Latent%20Variable%20MOGP%20%28LV-MOGP%29%20generalises%0Athis%20idea%20by%20modelling%20the%20covariance%20between%20outputs%20using%20a%20kernel%20applied%20to%0Alatent%20variables%2C%20one%20per%20output%2C%20leading%20to%20a%20flexible%20MOGP%20model%20that%20allows%0Aefficient%20generalization%20to%20new%20outputs%20with%20few%20data%20points.%20Computational%0Acomplexity%20in%20LV-MOGP%20grows%20linearly%20with%20the%20number%20of%20outputs%2C%20which%20makes%20it%0Aunsuitable%20for%20problems%20with%20a%20large%20number%20of%20outputs.%20In%20this%20paper%2C%20we%0Apropose%20a%20stochastic%20variational%20inference%20approach%20for%20the%20LV-MOGP%20that%20allows%0Amini-batches%20for%20both%20inputs%20and%20outputs%2C%20making%20computational%20complexity%20per%0Atraining%20iteration%20independent%20of%20the%20number%20of%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-Output%2520Gaussian%2520Processes%2520with%2520Stochastic%2520Variational%250A%2520%2520Inference%26entry.906535625%3DXiaoyu%2520Jiang%2520and%2520Sokratia%2520Georgaka%2520and%2520Magnus%2520Rattray%2520and%2520Mauricio%2520A.%2520Alvarez%26entry.1292438233%3D%2520%2520The%2520Multi-Output%2520Gaussian%2520Process%2520is%2520is%2520a%2520popular%2520tool%2520for%2520modelling%2520data%250Afrom%2520multiple%2520sources.%2520A%2520typical%2520choice%2520to%2520build%2520a%2520covariance%2520function%2520for%2520a%250AMOGP%2520is%2520the%2520Linear%2520Model%2520of%2520Coregionalization%2520%2528LMC%2529%2520which%2520parametrically%2520models%250Athe%2520covariance%2520between%2520outputs.%2520The%2520Latent%2520Variable%2520MOGP%2520%2528LV-MOGP%2529%2520generalises%250Athis%2520idea%2520by%2520modelling%2520the%2520covariance%2520between%2520outputs%2520using%2520a%2520kernel%2520applied%2520to%250Alatent%2520variables%252C%2520one%2520per%2520output%252C%2520leading%2520to%2520a%2520flexible%2520MOGP%2520model%2520that%2520allows%250Aefficient%2520generalization%2520to%2520new%2520outputs%2520with%2520few%2520data%2520points.%2520Computational%250Acomplexity%2520in%2520LV-MOGP%2520grows%2520linearly%2520with%2520the%2520number%2520of%2520outputs%252C%2520which%2520makes%2520it%250Aunsuitable%2520for%2520problems%2520with%2520a%2520large%2520number%2520of%2520outputs.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520stochastic%2520variational%2520inference%2520approach%2520for%2520the%2520LV-MOGP%2520that%2520allows%250Amini-batches%2520for%2520both%2520inputs%2520and%2520outputs%252C%2520making%2520computational%2520complexity%2520per%250Atraining%2520iteration%2520independent%2520of%2520the%2520number%2520of%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-Output%20Gaussian%20Processes%20with%20Stochastic%20Variational%0A%20%20Inference&entry.906535625=Xiaoyu%20Jiang%20and%20Sokratia%20Georgaka%20and%20Magnus%20Rattray%20and%20Mauricio%20A.%20Alvarez&entry.1292438233=%20%20The%20Multi-Output%20Gaussian%20Process%20is%20is%20a%20popular%20tool%20for%20modelling%20data%0Afrom%20multiple%20sources.%20A%20typical%20choice%20to%20build%20a%20covariance%20function%20for%20a%0AMOGP%20is%20the%20Linear%20Model%20of%20Coregionalization%20%28LMC%29%20which%20parametrically%20models%0Athe%20covariance%20between%20outputs.%20The%20Latent%20Variable%20MOGP%20%28LV-MOGP%29%20generalises%0Athis%20idea%20by%20modelling%20the%20covariance%20between%20outputs%20using%20a%20kernel%20applied%20to%0Alatent%20variables%2C%20one%20per%20output%2C%20leading%20to%20a%20flexible%20MOGP%20model%20that%20allows%0Aefficient%20generalization%20to%20new%20outputs%20with%20few%20data%20points.%20Computational%0Acomplexity%20in%20LV-MOGP%20grows%20linearly%20with%20the%20number%20of%20outputs%2C%20which%20makes%20it%0Aunsuitable%20for%20problems%20with%20a%20large%20number%20of%20outputs.%20In%20this%20paper%2C%20we%0Apropose%20a%20stochastic%20variational%20inference%20approach%20for%20the%20LV-MOGP%20that%20allows%0Amini-batches%20for%20both%20inputs%20and%20outputs%2C%20making%20computational%20complexity%20per%0Atraining%20iteration%20independent%20of%20the%20number%20of%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02476v1&entry.124074799=Read"},
{"title": "Task-Synchronized Recurrent Neural Networks", "author": "Mantas Luko\u0161evi\u010dius and Arnas Uselis", "abstract": "  Data are often sampled irregularly in time. Dealing with this using Recurrent\nNeural Networks (RNNs) traditionally involved ignoring the fact, feeding the\ntime differences as additional inputs, or resampling the data. All these\nmethods have their shortcomings. We propose an elegant straightforward\nalternative approach where instead the RNN is in effect resampled in time to\nmatch the time of the data or the task at hand. We use Echo State Network (ESN)\nand Gated Recurrent Unit (GRU) as the basis for our solution. Such RNNs can be\nseen as discretizations of continuous-time dynamical systems, which gives a\nsolid theoretical ground to our approach. Our Task-Synchronized ESN (TSESN) and\nGRU (TSGRU) models allow for a direct model time setting and require no\nadditional training, parameter tuning, or computation (solving differential\nequations or interpolating data) compared to their regular counterparts, thus\nretaining their original efficiency. We confirm empirically that our models can\neffectively compensate for the time-non-uniformity of the data and demonstrate\nthat they compare favorably to data resampling, classical RNN methods, and\nalternative RNN models proposed to deal with time irregularities on several\nreal-world nonuniform-time datasets. We open-source the code at\nhttps://github.com/oshapio/task-synchronized-RNNs .\n", "link": "http://arxiv.org/abs/2204.05192v2", "date": "2024-07-02", "relevancy": 1.938, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4894}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4869}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Synchronized%20Recurrent%20Neural%20Networks&body=Title%3A%20Task-Synchronized%20Recurrent%20Neural%20Networks%0AAuthor%3A%20Mantas%20Luko%C5%A1evi%C4%8Dius%20and%20Arnas%20Uselis%0AAbstract%3A%20%20%20Data%20are%20often%20sampled%20irregularly%20in%20time.%20Dealing%20with%20this%20using%20Recurrent%0ANeural%20Networks%20%28RNNs%29%20traditionally%20involved%20ignoring%20the%20fact%2C%20feeding%20the%0Atime%20differences%20as%20additional%20inputs%2C%20or%20resampling%20the%20data.%20All%20these%0Amethods%20have%20their%20shortcomings.%20We%20propose%20an%20elegant%20straightforward%0Aalternative%20approach%20where%20instead%20the%20RNN%20is%20in%20effect%20resampled%20in%20time%20to%0Amatch%20the%20time%20of%20the%20data%20or%20the%20task%20at%20hand.%20We%20use%20Echo%20State%20Network%20%28ESN%29%0Aand%20Gated%20Recurrent%20Unit%20%28GRU%29%20as%20the%20basis%20for%20our%20solution.%20Such%20RNNs%20can%20be%0Aseen%20as%20discretizations%20of%20continuous-time%20dynamical%20systems%2C%20which%20gives%20a%0Asolid%20theoretical%20ground%20to%20our%20approach.%20Our%20Task-Synchronized%20ESN%20%28TSESN%29%20and%0AGRU%20%28TSGRU%29%20models%20allow%20for%20a%20direct%20model%20time%20setting%20and%20require%20no%0Aadditional%20training%2C%20parameter%20tuning%2C%20or%20computation%20%28solving%20differential%0Aequations%20or%20interpolating%20data%29%20compared%20to%20their%20regular%20counterparts%2C%20thus%0Aretaining%20their%20original%20efficiency.%20We%20confirm%20empirically%20that%20our%20models%20can%0Aeffectively%20compensate%20for%20the%20time-non-uniformity%20of%20the%20data%20and%20demonstrate%0Athat%20they%20compare%20favorably%20to%20data%20resampling%2C%20classical%20RNN%20methods%2C%20and%0Aalternative%20RNN%20models%20proposed%20to%20deal%20with%20time%20irregularities%20on%20several%0Areal-world%20nonuniform-time%20datasets.%20We%20open-source%20the%20code%20at%0Ahttps%3A//github.com/oshapio/task-synchronized-RNNs%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.05192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Synchronized%2520Recurrent%2520Neural%2520Networks%26entry.906535625%3DMantas%2520Luko%25C5%25A1evi%25C4%258Dius%2520and%2520Arnas%2520Uselis%26entry.1292438233%3D%2520%2520Data%2520are%2520often%2520sampled%2520irregularly%2520in%2520time.%2520Dealing%2520with%2520this%2520using%2520Recurrent%250ANeural%2520Networks%2520%2528RNNs%2529%2520traditionally%2520involved%2520ignoring%2520the%2520fact%252C%2520feeding%2520the%250Atime%2520differences%2520as%2520additional%2520inputs%252C%2520or%2520resampling%2520the%2520data.%2520All%2520these%250Amethods%2520have%2520their%2520shortcomings.%2520We%2520propose%2520an%2520elegant%2520straightforward%250Aalternative%2520approach%2520where%2520instead%2520the%2520RNN%2520is%2520in%2520effect%2520resampled%2520in%2520time%2520to%250Amatch%2520the%2520time%2520of%2520the%2520data%2520or%2520the%2520task%2520at%2520hand.%2520We%2520use%2520Echo%2520State%2520Network%2520%2528ESN%2529%250Aand%2520Gated%2520Recurrent%2520Unit%2520%2528GRU%2529%2520as%2520the%2520basis%2520for%2520our%2520solution.%2520Such%2520RNNs%2520can%2520be%250Aseen%2520as%2520discretizations%2520of%2520continuous-time%2520dynamical%2520systems%252C%2520which%2520gives%2520a%250Asolid%2520theoretical%2520ground%2520to%2520our%2520approach.%2520Our%2520Task-Synchronized%2520ESN%2520%2528TSESN%2529%2520and%250AGRU%2520%2528TSGRU%2529%2520models%2520allow%2520for%2520a%2520direct%2520model%2520time%2520setting%2520and%2520require%2520no%250Aadditional%2520training%252C%2520parameter%2520tuning%252C%2520or%2520computation%2520%2528solving%2520differential%250Aequations%2520or%2520interpolating%2520data%2529%2520compared%2520to%2520their%2520regular%2520counterparts%252C%2520thus%250Aretaining%2520their%2520original%2520efficiency.%2520We%2520confirm%2520empirically%2520that%2520our%2520models%2520can%250Aeffectively%2520compensate%2520for%2520the%2520time-non-uniformity%2520of%2520the%2520data%2520and%2520demonstrate%250Athat%2520they%2520compare%2520favorably%2520to%2520data%2520resampling%252C%2520classical%2520RNN%2520methods%252C%2520and%250Aalternative%2520RNN%2520models%2520proposed%2520to%2520deal%2520with%2520time%2520irregularities%2520on%2520several%250Areal-world%2520nonuniform-time%2520datasets.%2520We%2520open-source%2520the%2520code%2520at%250Ahttps%253A//github.com/oshapio/task-synchronized-RNNs%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.05192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Synchronized%20Recurrent%20Neural%20Networks&entry.906535625=Mantas%20Luko%C5%A1evi%C4%8Dius%20and%20Arnas%20Uselis&entry.1292438233=%20%20Data%20are%20often%20sampled%20irregularly%20in%20time.%20Dealing%20with%20this%20using%20Recurrent%0ANeural%20Networks%20%28RNNs%29%20traditionally%20involved%20ignoring%20the%20fact%2C%20feeding%20the%0Atime%20differences%20as%20additional%20inputs%2C%20or%20resampling%20the%20data.%20All%20these%0Amethods%20have%20their%20shortcomings.%20We%20propose%20an%20elegant%20straightforward%0Aalternative%20approach%20where%20instead%20the%20RNN%20is%20in%20effect%20resampled%20in%20time%20to%0Amatch%20the%20time%20of%20the%20data%20or%20the%20task%20at%20hand.%20We%20use%20Echo%20State%20Network%20%28ESN%29%0Aand%20Gated%20Recurrent%20Unit%20%28GRU%29%20as%20the%20basis%20for%20our%20solution.%20Such%20RNNs%20can%20be%0Aseen%20as%20discretizations%20of%20continuous-time%20dynamical%20systems%2C%20which%20gives%20a%0Asolid%20theoretical%20ground%20to%20our%20approach.%20Our%20Task-Synchronized%20ESN%20%28TSESN%29%20and%0AGRU%20%28TSGRU%29%20models%20allow%20for%20a%20direct%20model%20time%20setting%20and%20require%20no%0Aadditional%20training%2C%20parameter%20tuning%2C%20or%20computation%20%28solving%20differential%0Aequations%20or%20interpolating%20data%29%20compared%20to%20their%20regular%20counterparts%2C%20thus%0Aretaining%20their%20original%20efficiency.%20We%20confirm%20empirically%20that%20our%20models%20can%0Aeffectively%20compensate%20for%20the%20time-non-uniformity%20of%20the%20data%20and%20demonstrate%0Athat%20they%20compare%20favorably%20to%20data%20resampling%2C%20classical%20RNN%20methods%2C%20and%0Aalternative%20RNN%20models%20proposed%20to%20deal%20with%20time%20irregularities%20on%20several%0Areal-world%20nonuniform-time%20datasets.%20We%20open-source%20the%20code%20at%0Ahttps%3A//github.com/oshapio/task-synchronized-RNNs%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.05192v2&entry.124074799=Read"},
{"title": "Sparse Variational Contaminated Noise Gaussian Process Regression with\n  Applications in Geomagnetic Perturbations Forecasting", "author": "Daniel Iong and Matthew McAnear and Yuezhou Qu and Shasha Zou and Gabor Toth and Yang Chen", "abstract": "  Gaussian Processes (GP) have become popular machine-learning methods for\nkernel-based learning on datasets with complicated covariance structures. In\nthis paper, we present a novel extension to the GP framework using a\ncontaminated normal likelihood function to better account for heteroscedastic\nvariance and outlier noise. We propose a scalable inference algorithm based on\nthe Sparse Variational Gaussian Process (SVGP) method for fitting sparse\nGaussian process regression models with contaminated normal noise on large\ndatasets. We examine an application to geomagnetic ground perturbations, where\nthe state-of-the-art prediction model is based on neural networks. We show that\nour approach yields shorter prediction intervals for similar coverage and\naccuracy when compared to an artificial dense neural network baseline.\n", "link": "http://arxiv.org/abs/2402.17570v3", "date": "2024-07-02", "relevancy": 1.9367, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Variational%20Contaminated%20Noise%20Gaussian%20Process%20Regression%20with%0A%20%20Applications%20in%20Geomagnetic%20Perturbations%20Forecasting&body=Title%3A%20Sparse%20Variational%20Contaminated%20Noise%20Gaussian%20Process%20Regression%20with%0A%20%20Applications%20in%20Geomagnetic%20Perturbations%20Forecasting%0AAuthor%3A%20Daniel%20Iong%20and%20Matthew%20McAnear%20and%20Yuezhou%20Qu%20and%20Shasha%20Zou%20and%20Gabor%20Toth%20and%20Yang%20Chen%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GP%29%20have%20become%20popular%20machine-learning%20methods%20for%0Akernel-based%20learning%20on%20datasets%20with%20complicated%20covariance%20structures.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20extension%20to%20the%20GP%20framework%20using%20a%0Acontaminated%20normal%20likelihood%20function%20to%20better%20account%20for%20heteroscedastic%0Avariance%20and%20outlier%20noise.%20We%20propose%20a%20scalable%20inference%20algorithm%20based%20on%0Athe%20Sparse%20Variational%20Gaussian%20Process%20%28SVGP%29%20method%20for%20fitting%20sparse%0AGaussian%20process%20regression%20models%20with%20contaminated%20normal%20noise%20on%20large%0Adatasets.%20We%20examine%20an%20application%20to%20geomagnetic%20ground%20perturbations%2C%20where%0Athe%20state-of-the-art%20prediction%20model%20is%20based%20on%20neural%20networks.%20We%20show%20that%0Aour%20approach%20yields%20shorter%20prediction%20intervals%20for%20similar%20coverage%20and%0Aaccuracy%20when%20compared%20to%20an%20artificial%20dense%20neural%20network%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17570v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Variational%2520Contaminated%2520Noise%2520Gaussian%2520Process%2520Regression%2520with%250A%2520%2520Applications%2520in%2520Geomagnetic%2520Perturbations%2520Forecasting%26entry.906535625%3DDaniel%2520Iong%2520and%2520Matthew%2520McAnear%2520and%2520Yuezhou%2520Qu%2520and%2520Shasha%2520Zou%2520and%2520Gabor%2520Toth%2520and%2520Yang%2520Chen%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GP%2529%2520have%2520become%2520popular%2520machine-learning%2520methods%2520for%250Akernel-based%2520learning%2520on%2520datasets%2520with%2520complicated%2520covariance%2520structures.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520novel%2520extension%2520to%2520the%2520GP%2520framework%2520using%2520a%250Acontaminated%2520normal%2520likelihood%2520function%2520to%2520better%2520account%2520for%2520heteroscedastic%250Avariance%2520and%2520outlier%2520noise.%2520We%2520propose%2520a%2520scalable%2520inference%2520algorithm%2520based%2520on%250Athe%2520Sparse%2520Variational%2520Gaussian%2520Process%2520%2528SVGP%2529%2520method%2520for%2520fitting%2520sparse%250AGaussian%2520process%2520regression%2520models%2520with%2520contaminated%2520normal%2520noise%2520on%2520large%250Adatasets.%2520We%2520examine%2520an%2520application%2520to%2520geomagnetic%2520ground%2520perturbations%252C%2520where%250Athe%2520state-of-the-art%2520prediction%2520model%2520is%2520based%2520on%2520neural%2520networks.%2520We%2520show%2520that%250Aour%2520approach%2520yields%2520shorter%2520prediction%2520intervals%2520for%2520similar%2520coverage%2520and%250Aaccuracy%2520when%2520compared%2520to%2520an%2520artificial%2520dense%2520neural%2520network%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17570v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Variational%20Contaminated%20Noise%20Gaussian%20Process%20Regression%20with%0A%20%20Applications%20in%20Geomagnetic%20Perturbations%20Forecasting&entry.906535625=Daniel%20Iong%20and%20Matthew%20McAnear%20and%20Yuezhou%20Qu%20and%20Shasha%20Zou%20and%20Gabor%20Toth%20and%20Yang%20Chen&entry.1292438233=%20%20Gaussian%20Processes%20%28GP%29%20have%20become%20popular%20machine-learning%20methods%20for%0Akernel-based%20learning%20on%20datasets%20with%20complicated%20covariance%20structures.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20extension%20to%20the%20GP%20framework%20using%20a%0Acontaminated%20normal%20likelihood%20function%20to%20better%20account%20for%20heteroscedastic%0Avariance%20and%20outlier%20noise.%20We%20propose%20a%20scalable%20inference%20algorithm%20based%20on%0Athe%20Sparse%20Variational%20Gaussian%20Process%20%28SVGP%29%20method%20for%20fitting%20sparse%0AGaussian%20process%20regression%20models%20with%20contaminated%20normal%20noise%20on%20large%0Adatasets.%20We%20examine%20an%20application%20to%20geomagnetic%20ground%20perturbations%2C%20where%0Athe%20state-of-the-art%20prediction%20model%20is%20based%20on%20neural%20networks.%20We%20show%20that%0Aour%20approach%20yields%20shorter%20prediction%20intervals%20for%20similar%20coverage%20and%0Aaccuracy%20when%20compared%20to%20an%20artificial%20dense%20neural%20network%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17570v3&entry.124074799=Read"},
{"title": "Explaining Deep Learning for ECG Analysis: Building Blocks for Auditing\n  and Knowledge Discovery", "author": "Patrick Wagner and Temesgen Mehari and Wilhelm Haverkamp and Nils Strodthoff", "abstract": "  Deep neural networks have become increasingly popular for analyzing ECG data\nbecause of their ability to accurately identify cardiac conditions and hidden\nclinical factors. However, the lack of transparency due to the black box nature\nof these models is a common concern. To address this issue, explainable AI\n(XAI) methods can be employed. In this study, we present a comprehensive\nanalysis of post-hoc XAI methods, investigating the local (attributions per\nsample) and global (based on domain expert concepts) perspectives. We have\nestablished a set of sanity checks to identify sensible attribution methods,\nand we provide quantitative evidence in accordance with expert rules. This\ndataset-wide analysis goes beyond anecdotal evidence by aggregating data across\npatient subgroups. Furthermore, we demonstrate how these XAI techniques can be\nutilized for knowledge discovery, such as identifying subtypes of myocardial\ninfarction. We believe that these proposed methods can serve as building blocks\nfor a complementary assessment of the internal validity during a certification\nprocess, as well as for knowledge discovery in the field of ECG analysis.\n", "link": "http://arxiv.org/abs/2305.17043v2", "date": "2024-07-02", "relevancy": 1.9209, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5188}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4786}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Deep%20Learning%20for%20ECG%20Analysis%3A%20Building%20Blocks%20for%20Auditing%0A%20%20and%20Knowledge%20Discovery&body=Title%3A%20Explaining%20Deep%20Learning%20for%20ECG%20Analysis%3A%20Building%20Blocks%20for%20Auditing%0A%20%20and%20Knowledge%20Discovery%0AAuthor%3A%20Patrick%20Wagner%20and%20Temesgen%20Mehari%20and%20Wilhelm%20Haverkamp%20and%20Nils%20Strodthoff%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20become%20increasingly%20popular%20for%20analyzing%20ECG%20data%0Abecause%20of%20their%20ability%20to%20accurately%20identify%20cardiac%20conditions%20and%20hidden%0Aclinical%20factors.%20However%2C%20the%20lack%20of%20transparency%20due%20to%20the%20black%20box%20nature%0Aof%20these%20models%20is%20a%20common%20concern.%20To%20address%20this%20issue%2C%20explainable%20AI%0A%28XAI%29%20methods%20can%20be%20employed.%20In%20this%20study%2C%20we%20present%20a%20comprehensive%0Aanalysis%20of%20post-hoc%20XAI%20methods%2C%20investigating%20the%20local%20%28attributions%20per%0Asample%29%20and%20global%20%28based%20on%20domain%20expert%20concepts%29%20perspectives.%20We%20have%0Aestablished%20a%20set%20of%20sanity%20checks%20to%20identify%20sensible%20attribution%20methods%2C%0Aand%20we%20provide%20quantitative%20evidence%20in%20accordance%20with%20expert%20rules.%20This%0Adataset-wide%20analysis%20goes%20beyond%20anecdotal%20evidence%20by%20aggregating%20data%20across%0Apatient%20subgroups.%20Furthermore%2C%20we%20demonstrate%20how%20these%20XAI%20techniques%20can%20be%0Autilized%20for%20knowledge%20discovery%2C%20such%20as%20identifying%20subtypes%20of%20myocardial%0Ainfarction.%20We%20believe%20that%20these%20proposed%20methods%20can%20serve%20as%20building%20blocks%0Afor%20a%20complementary%20assessment%20of%20the%20internal%20validity%20during%20a%20certification%0Aprocess%2C%20as%20well%20as%20for%20knowledge%20discovery%20in%20the%20field%20of%20ECG%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Deep%2520Learning%2520for%2520ECG%2520Analysis%253A%2520Building%2520Blocks%2520for%2520Auditing%250A%2520%2520and%2520Knowledge%2520Discovery%26entry.906535625%3DPatrick%2520Wagner%2520and%2520Temesgen%2520Mehari%2520and%2520Wilhelm%2520Haverkamp%2520and%2520Nils%2520Strodthoff%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520become%2520increasingly%2520popular%2520for%2520analyzing%2520ECG%2520data%250Abecause%2520of%2520their%2520ability%2520to%2520accurately%2520identify%2520cardiac%2520conditions%2520and%2520hidden%250Aclinical%2520factors.%2520However%252C%2520the%2520lack%2520of%2520transparency%2520due%2520to%2520the%2520black%2520box%2520nature%250Aof%2520these%2520models%2520is%2520a%2520common%2520concern.%2520To%2520address%2520this%2520issue%252C%2520explainable%2520AI%250A%2528XAI%2529%2520methods%2520can%2520be%2520employed.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520comprehensive%250Aanalysis%2520of%2520post-hoc%2520XAI%2520methods%252C%2520investigating%2520the%2520local%2520%2528attributions%2520per%250Asample%2529%2520and%2520global%2520%2528based%2520on%2520domain%2520expert%2520concepts%2529%2520perspectives.%2520We%2520have%250Aestablished%2520a%2520set%2520of%2520sanity%2520checks%2520to%2520identify%2520sensible%2520attribution%2520methods%252C%250Aand%2520we%2520provide%2520quantitative%2520evidence%2520in%2520accordance%2520with%2520expert%2520rules.%2520This%250Adataset-wide%2520analysis%2520goes%2520beyond%2520anecdotal%2520evidence%2520by%2520aggregating%2520data%2520across%250Apatient%2520subgroups.%2520Furthermore%252C%2520we%2520demonstrate%2520how%2520these%2520XAI%2520techniques%2520can%2520be%250Autilized%2520for%2520knowledge%2520discovery%252C%2520such%2520as%2520identifying%2520subtypes%2520of%2520myocardial%250Ainfarction.%2520We%2520believe%2520that%2520these%2520proposed%2520methods%2520can%2520serve%2520as%2520building%2520blocks%250Afor%2520a%2520complementary%2520assessment%2520of%2520the%2520internal%2520validity%2520during%2520a%2520certification%250Aprocess%252C%2520as%2520well%2520as%2520for%2520knowledge%2520discovery%2520in%2520the%2520field%2520of%2520ECG%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Deep%20Learning%20for%20ECG%20Analysis%3A%20Building%20Blocks%20for%20Auditing%0A%20%20and%20Knowledge%20Discovery&entry.906535625=Patrick%20Wagner%20and%20Temesgen%20Mehari%20and%20Wilhelm%20Haverkamp%20and%20Nils%20Strodthoff&entry.1292438233=%20%20Deep%20neural%20networks%20have%20become%20increasingly%20popular%20for%20analyzing%20ECG%20data%0Abecause%20of%20their%20ability%20to%20accurately%20identify%20cardiac%20conditions%20and%20hidden%0Aclinical%20factors.%20However%2C%20the%20lack%20of%20transparency%20due%20to%20the%20black%20box%20nature%0Aof%20these%20models%20is%20a%20common%20concern.%20To%20address%20this%20issue%2C%20explainable%20AI%0A%28XAI%29%20methods%20can%20be%20employed.%20In%20this%20study%2C%20we%20present%20a%20comprehensive%0Aanalysis%20of%20post-hoc%20XAI%20methods%2C%20investigating%20the%20local%20%28attributions%20per%0Asample%29%20and%20global%20%28based%20on%20domain%20expert%20concepts%29%20perspectives.%20We%20have%0Aestablished%20a%20set%20of%20sanity%20checks%20to%20identify%20sensible%20attribution%20methods%2C%0Aand%20we%20provide%20quantitative%20evidence%20in%20accordance%20with%20expert%20rules.%20This%0Adataset-wide%20analysis%20goes%20beyond%20anecdotal%20evidence%20by%20aggregating%20data%20across%0Apatient%20subgroups.%20Furthermore%2C%20we%20demonstrate%20how%20these%20XAI%20techniques%20can%20be%0Autilized%20for%20knowledge%20discovery%2C%20such%20as%20identifying%20subtypes%20of%20myocardial%0Ainfarction.%20We%20believe%20that%20these%20proposed%20methods%20can%20serve%20as%20building%20blocks%0Afor%20a%20complementary%20assessment%20of%20the%20internal%20validity%20during%20a%20certification%0Aprocess%2C%20as%20well%20as%20for%20knowledge%20discovery%20in%20the%20field%20of%20ECG%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17043v2&entry.124074799=Read"},
{"title": "DextrAH-G: Pixels-to-Action Dexterous Arm-Hand Grasping with Geometric\n  Fabrics", "author": "Tyler Ga Wei Lum and Martin Matak and Viktor Makoviychuk and Ankur Handa and Arthur Allshire and Tucker Hermans and Nathan D. Ratliff and Karl Van Wyk", "abstract": "  A pivotal challenge in robotics is achieving fast, safe, and robust dexterous\ngrasping across a diverse range of objects, an important goal within industrial\napplications. However, existing methods often have very limited speed,\ndexterity, and generality, along with limited or no hardware safety guarantees.\nIn this work, we introduce DextrAH-G, a depth-based dexterous grasping policy\ntrained entirely in simulation that combines reinforcement learning, geometric\nfabrics, and teacher-student distillation. We address key challenges in joint\narm-hand policy learning, such as high-dimensional observation and action\nspaces, the sim2real gap, collision avoidance, and hardware constraints.\nDextrAH-G enables a 23 motor arm-hand robot to safely and continuously grasp\nand transport a large variety of objects at high speed using multi-modal inputs\nincluding depth images, allowing generalization across object geometry. Videos\nat https://sites.google.com/view/dextrah-g.\n", "link": "http://arxiv.org/abs/2407.02274v1", "date": "2024-07-02", "relevancy": 1.9205, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6779}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DextrAH-G%3A%20Pixels-to-Action%20Dexterous%20Arm-Hand%20Grasping%20with%20Geometric%0A%20%20Fabrics&body=Title%3A%20DextrAH-G%3A%20Pixels-to-Action%20Dexterous%20Arm-Hand%20Grasping%20with%20Geometric%0A%20%20Fabrics%0AAuthor%3A%20Tyler%20Ga%20Wei%20Lum%20and%20Martin%20Matak%20and%20Viktor%20Makoviychuk%20and%20Ankur%20Handa%20and%20Arthur%20Allshire%20and%20Tucker%20Hermans%20and%20Nathan%20D.%20Ratliff%20and%20Karl%20Van%20Wyk%0AAbstract%3A%20%20%20A%20pivotal%20challenge%20in%20robotics%20is%20achieving%20fast%2C%20safe%2C%20and%20robust%20dexterous%0Agrasping%20across%20a%20diverse%20range%20of%20objects%2C%20an%20important%20goal%20within%20industrial%0Aapplications.%20However%2C%20existing%20methods%20often%20have%20very%20limited%20speed%2C%0Adexterity%2C%20and%20generality%2C%20along%20with%20limited%20or%20no%20hardware%20safety%20guarantees.%0AIn%20this%20work%2C%20we%20introduce%20DextrAH-G%2C%20a%20depth-based%20dexterous%20grasping%20policy%0Atrained%20entirely%20in%20simulation%20that%20combines%20reinforcement%20learning%2C%20geometric%0Afabrics%2C%20and%20teacher-student%20distillation.%20We%20address%20key%20challenges%20in%20joint%0Aarm-hand%20policy%20learning%2C%20such%20as%20high-dimensional%20observation%20and%20action%0Aspaces%2C%20the%20sim2real%20gap%2C%20collision%20avoidance%2C%20and%20hardware%20constraints.%0ADextrAH-G%20enables%20a%2023%20motor%20arm-hand%20robot%20to%20safely%20and%20continuously%20grasp%0Aand%20transport%20a%20large%20variety%20of%20objects%20at%20high%20speed%20using%20multi-modal%20inputs%0Aincluding%20depth%20images%2C%20allowing%20generalization%20across%20object%20geometry.%20Videos%0Aat%20https%3A//sites.google.com/view/dextrah-g.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDextrAH-G%253A%2520Pixels-to-Action%2520Dexterous%2520Arm-Hand%2520Grasping%2520with%2520Geometric%250A%2520%2520Fabrics%26entry.906535625%3DTyler%2520Ga%2520Wei%2520Lum%2520and%2520Martin%2520Matak%2520and%2520Viktor%2520Makoviychuk%2520and%2520Ankur%2520Handa%2520and%2520Arthur%2520Allshire%2520and%2520Tucker%2520Hermans%2520and%2520Nathan%2520D.%2520Ratliff%2520and%2520Karl%2520Van%2520Wyk%26entry.1292438233%3D%2520%2520A%2520pivotal%2520challenge%2520in%2520robotics%2520is%2520achieving%2520fast%252C%2520safe%252C%2520and%2520robust%2520dexterous%250Agrasping%2520across%2520a%2520diverse%2520range%2520of%2520objects%252C%2520an%2520important%2520goal%2520within%2520industrial%250Aapplications.%2520However%252C%2520existing%2520methods%2520often%2520have%2520very%2520limited%2520speed%252C%250Adexterity%252C%2520and%2520generality%252C%2520along%2520with%2520limited%2520or%2520no%2520hardware%2520safety%2520guarantees.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520DextrAH-G%252C%2520a%2520depth-based%2520dexterous%2520grasping%2520policy%250Atrained%2520entirely%2520in%2520simulation%2520that%2520combines%2520reinforcement%2520learning%252C%2520geometric%250Afabrics%252C%2520and%2520teacher-student%2520distillation.%2520We%2520address%2520key%2520challenges%2520in%2520joint%250Aarm-hand%2520policy%2520learning%252C%2520such%2520as%2520high-dimensional%2520observation%2520and%2520action%250Aspaces%252C%2520the%2520sim2real%2520gap%252C%2520collision%2520avoidance%252C%2520and%2520hardware%2520constraints.%250ADextrAH-G%2520enables%2520a%252023%2520motor%2520arm-hand%2520robot%2520to%2520safely%2520and%2520continuously%2520grasp%250Aand%2520transport%2520a%2520large%2520variety%2520of%2520objects%2520at%2520high%2520speed%2520using%2520multi-modal%2520inputs%250Aincluding%2520depth%2520images%252C%2520allowing%2520generalization%2520across%2520object%2520geometry.%2520Videos%250Aat%2520https%253A//sites.google.com/view/dextrah-g.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DextrAH-G%3A%20Pixels-to-Action%20Dexterous%20Arm-Hand%20Grasping%20with%20Geometric%0A%20%20Fabrics&entry.906535625=Tyler%20Ga%20Wei%20Lum%20and%20Martin%20Matak%20and%20Viktor%20Makoviychuk%20and%20Ankur%20Handa%20and%20Arthur%20Allshire%20and%20Tucker%20Hermans%20and%20Nathan%20D.%20Ratliff%20and%20Karl%20Van%20Wyk&entry.1292438233=%20%20A%20pivotal%20challenge%20in%20robotics%20is%20achieving%20fast%2C%20safe%2C%20and%20robust%20dexterous%0Agrasping%20across%20a%20diverse%20range%20of%20objects%2C%20an%20important%20goal%20within%20industrial%0Aapplications.%20However%2C%20existing%20methods%20often%20have%20very%20limited%20speed%2C%0Adexterity%2C%20and%20generality%2C%20along%20with%20limited%20or%20no%20hardware%20safety%20guarantees.%0AIn%20this%20work%2C%20we%20introduce%20DextrAH-G%2C%20a%20depth-based%20dexterous%20grasping%20policy%0Atrained%20entirely%20in%20simulation%20that%20combines%20reinforcement%20learning%2C%20geometric%0Afabrics%2C%20and%20teacher-student%20distillation.%20We%20address%20key%20challenges%20in%20joint%0Aarm-hand%20policy%20learning%2C%20such%20as%20high-dimensional%20observation%20and%20action%0Aspaces%2C%20the%20sim2real%20gap%2C%20collision%20avoidance%2C%20and%20hardware%20constraints.%0ADextrAH-G%20enables%20a%2023%20motor%20arm-hand%20robot%20to%20safely%20and%20continuously%20grasp%0Aand%20transport%20a%20large%20variety%20of%20objects%20at%20high%20speed%20using%20multi-modal%20inputs%0Aincluding%20depth%20images%2C%20allowing%20generalization%20across%20object%20geometry.%20Videos%0Aat%20https%3A//sites.google.com/view/dextrah-g.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02274v1&entry.124074799=Read"},
{"title": "Stochastic Differential Equations models for Least-Squares Stochastic\n  Gradient Descent", "author": "Adrien Schertzer and Loucas Pillaud-Vivien", "abstract": "  We study the dynamics of a continuous-time model of the Stochastic Gradient\nDescent (SGD) for the least-square problem. Indeed, pursuing the work of Li et\nal. (2019), we analyze Stochastic Differential Equations (SDEs) that model SGD\neither in the case of the training loss (finite samples) or the population one\n(online setting). A key qualitative feature of the dynamics is the existence of\na perfect interpolator of the data, irrespective of the sample size. In both\nscenarios, we provide precise, non-asymptotic rates of convergence to the\n(possibly degenerate) stationary distribution. Additionally, we describe this\nasymptotic distribution, offering estimates of its mean, deviations from it,\nand a proof of the emergence of heavy-tails related to the step-size magnitude.\nNumerical simulations supporting our findings are also presented.\n", "link": "http://arxiv.org/abs/2407.02322v1", "date": "2024-07-02", "relevancy": 1.9109, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5111}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4711}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Differential%20Equations%20models%20for%20Least-Squares%20Stochastic%0A%20%20Gradient%20Descent&body=Title%3A%20Stochastic%20Differential%20Equations%20models%20for%20Least-Squares%20Stochastic%0A%20%20Gradient%20Descent%0AAuthor%3A%20Adrien%20Schertzer%20and%20Loucas%20Pillaud-Vivien%0AAbstract%3A%20%20%20We%20study%20the%20dynamics%20of%20a%20continuous-time%20model%20of%20the%20Stochastic%20Gradient%0ADescent%20%28SGD%29%20for%20the%20least-square%20problem.%20Indeed%2C%20pursuing%20the%20work%20of%20Li%20et%0Aal.%20%282019%29%2C%20we%20analyze%20Stochastic%20Differential%20Equations%20%28SDEs%29%20that%20model%20SGD%0Aeither%20in%20the%20case%20of%20the%20training%20loss%20%28finite%20samples%29%20or%20the%20population%20one%0A%28online%20setting%29.%20A%20key%20qualitative%20feature%20of%20the%20dynamics%20is%20the%20existence%20of%0Aa%20perfect%20interpolator%20of%20the%20data%2C%20irrespective%20of%20the%20sample%20size.%20In%20both%0Ascenarios%2C%20we%20provide%20precise%2C%20non-asymptotic%20rates%20of%20convergence%20to%20the%0A%28possibly%20degenerate%29%20stationary%20distribution.%20Additionally%2C%20we%20describe%20this%0Aasymptotic%20distribution%2C%20offering%20estimates%20of%20its%20mean%2C%20deviations%20from%20it%2C%0Aand%20a%20proof%20of%20the%20emergence%20of%20heavy-tails%20related%20to%20the%20step-size%20magnitude.%0ANumerical%20simulations%20supporting%20our%20findings%20are%20also%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Differential%2520Equations%2520models%2520for%2520Least-Squares%2520Stochastic%250A%2520%2520Gradient%2520Descent%26entry.906535625%3DAdrien%2520Schertzer%2520and%2520Loucas%2520Pillaud-Vivien%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520dynamics%2520of%2520a%2520continuous-time%2520model%2520of%2520the%2520Stochastic%2520Gradient%250ADescent%2520%2528SGD%2529%2520for%2520the%2520least-square%2520problem.%2520Indeed%252C%2520pursuing%2520the%2520work%2520of%2520Li%2520et%250Aal.%2520%25282019%2529%252C%2520we%2520analyze%2520Stochastic%2520Differential%2520Equations%2520%2528SDEs%2529%2520that%2520model%2520SGD%250Aeither%2520in%2520the%2520case%2520of%2520the%2520training%2520loss%2520%2528finite%2520samples%2529%2520or%2520the%2520population%2520one%250A%2528online%2520setting%2529.%2520A%2520key%2520qualitative%2520feature%2520of%2520the%2520dynamics%2520is%2520the%2520existence%2520of%250Aa%2520perfect%2520interpolator%2520of%2520the%2520data%252C%2520irrespective%2520of%2520the%2520sample%2520size.%2520In%2520both%250Ascenarios%252C%2520we%2520provide%2520precise%252C%2520non-asymptotic%2520rates%2520of%2520convergence%2520to%2520the%250A%2528possibly%2520degenerate%2529%2520stationary%2520distribution.%2520Additionally%252C%2520we%2520describe%2520this%250Aasymptotic%2520distribution%252C%2520offering%2520estimates%2520of%2520its%2520mean%252C%2520deviations%2520from%2520it%252C%250Aand%2520a%2520proof%2520of%2520the%2520emergence%2520of%2520heavy-tails%2520related%2520to%2520the%2520step-size%2520magnitude.%250ANumerical%2520simulations%2520supporting%2520our%2520findings%2520are%2520also%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Differential%20Equations%20models%20for%20Least-Squares%20Stochastic%0A%20%20Gradient%20Descent&entry.906535625=Adrien%20Schertzer%20and%20Loucas%20Pillaud-Vivien&entry.1292438233=%20%20We%20study%20the%20dynamics%20of%20a%20continuous-time%20model%20of%20the%20Stochastic%20Gradient%0ADescent%20%28SGD%29%20for%20the%20least-square%20problem.%20Indeed%2C%20pursuing%20the%20work%20of%20Li%20et%0Aal.%20%282019%29%2C%20we%20analyze%20Stochastic%20Differential%20Equations%20%28SDEs%29%20that%20model%20SGD%0Aeither%20in%20the%20case%20of%20the%20training%20loss%20%28finite%20samples%29%20or%20the%20population%20one%0A%28online%20setting%29.%20A%20key%20qualitative%20feature%20of%20the%20dynamics%20is%20the%20existence%20of%0Aa%20perfect%20interpolator%20of%20the%20data%2C%20irrespective%20of%20the%20sample%20size.%20In%20both%0Ascenarios%2C%20we%20provide%20precise%2C%20non-asymptotic%20rates%20of%20convergence%20to%20the%0A%28possibly%20degenerate%29%20stationary%20distribution.%20Additionally%2C%20we%20describe%20this%0Aasymptotic%20distribution%2C%20offering%20estimates%20of%20its%20mean%2C%20deviations%20from%20it%2C%0Aand%20a%20proof%20of%20the%20emergence%20of%20heavy-tails%20related%20to%20the%20step-size%20magnitude.%0ANumerical%20simulations%20supporting%20our%20findings%20are%20also%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02322v1&entry.124074799=Read"},
{"title": "Evaluating the Robustness of Adverse Drug Event Classification Models\n  Using Templates", "author": "Dorothea MacPhail and David Harbecke and Lisa Raithel and Sebastian M\u00f6ller", "abstract": "  An adverse drug effect (ADE) is any harmful event resulting from medical drug\ntreatment. Despite their importance, ADEs are often under-reported in official\nchannels. Some research has therefore turned to detecting discussions of ADEs\nin social media. Impressive results have been achieved in various attempts to\ndetect ADEs. In a high-stakes domain such as medicine, however, an in-depth\nevaluation of a model's abilities is crucial. We address the issue of thorough\nperformance evaluation in English-language ADE detection with hand-crafted\ntemplates for four capabilities: Temporal order, negation, sentiment, and\nbeneficial effect. We find that models with similar performance on held-out\ntest sets have varying results on these capabilities.\n", "link": "http://arxiv.org/abs/2407.02432v1", "date": "2024-07-02", "relevancy": 1.2829, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.421}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Robustness%20of%20Adverse%20Drug%20Event%20Classification%20Models%0A%20%20Using%20Templates&body=Title%3A%20Evaluating%20the%20Robustness%20of%20Adverse%20Drug%20Event%20Classification%20Models%0A%20%20Using%20Templates%0AAuthor%3A%20Dorothea%20MacPhail%20and%20David%20Harbecke%20and%20Lisa%20Raithel%20and%20Sebastian%20M%C3%B6ller%0AAbstract%3A%20%20%20An%20adverse%20drug%20effect%20%28ADE%29%20is%20any%20harmful%20event%20resulting%20from%20medical%20drug%0Atreatment.%20Despite%20their%20importance%2C%20ADEs%20are%20often%20under-reported%20in%20official%0Achannels.%20Some%20research%20has%20therefore%20turned%20to%20detecting%20discussions%20of%20ADEs%0Ain%20social%20media.%20Impressive%20results%20have%20been%20achieved%20in%20various%20attempts%20to%0Adetect%20ADEs.%20In%20a%20high-stakes%20domain%20such%20as%20medicine%2C%20however%2C%20an%20in-depth%0Aevaluation%20of%20a%20model%27s%20abilities%20is%20crucial.%20We%20address%20the%20issue%20of%20thorough%0Aperformance%20evaluation%20in%20English-language%20ADE%20detection%20with%20hand-crafted%0Atemplates%20for%20four%20capabilities%3A%20Temporal%20order%2C%20negation%2C%20sentiment%2C%20and%0Abeneficial%20effect.%20We%20find%20that%20models%20with%20similar%20performance%20on%20held-out%0Atest%20sets%20have%20varying%20results%20on%20these%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Robustness%2520of%2520Adverse%2520Drug%2520Event%2520Classification%2520Models%250A%2520%2520Using%2520Templates%26entry.906535625%3DDorothea%2520MacPhail%2520and%2520David%2520Harbecke%2520and%2520Lisa%2520Raithel%2520and%2520Sebastian%2520M%25C3%25B6ller%26entry.1292438233%3D%2520%2520An%2520adverse%2520drug%2520effect%2520%2528ADE%2529%2520is%2520any%2520harmful%2520event%2520resulting%2520from%2520medical%2520drug%250Atreatment.%2520Despite%2520their%2520importance%252C%2520ADEs%2520are%2520often%2520under-reported%2520in%2520official%250Achannels.%2520Some%2520research%2520has%2520therefore%2520turned%2520to%2520detecting%2520discussions%2520of%2520ADEs%250Ain%2520social%2520media.%2520Impressive%2520results%2520have%2520been%2520achieved%2520in%2520various%2520attempts%2520to%250Adetect%2520ADEs.%2520In%2520a%2520high-stakes%2520domain%2520such%2520as%2520medicine%252C%2520however%252C%2520an%2520in-depth%250Aevaluation%2520of%2520a%2520model%2527s%2520abilities%2520is%2520crucial.%2520We%2520address%2520the%2520issue%2520of%2520thorough%250Aperformance%2520evaluation%2520in%2520English-language%2520ADE%2520detection%2520with%2520hand-crafted%250Atemplates%2520for%2520four%2520capabilities%253A%2520Temporal%2520order%252C%2520negation%252C%2520sentiment%252C%2520and%250Abeneficial%2520effect.%2520We%2520find%2520that%2520models%2520with%2520similar%2520performance%2520on%2520held-out%250Atest%2520sets%2520have%2520varying%2520results%2520on%2520these%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Robustness%20of%20Adverse%20Drug%20Event%20Classification%20Models%0A%20%20Using%20Templates&entry.906535625=Dorothea%20MacPhail%20and%20David%20Harbecke%20and%20Lisa%20Raithel%20and%20Sebastian%20M%C3%B6ller&entry.1292438233=%20%20An%20adverse%20drug%20effect%20%28ADE%29%20is%20any%20harmful%20event%20resulting%20from%20medical%20drug%0Atreatment.%20Despite%20their%20importance%2C%20ADEs%20are%20often%20under-reported%20in%20official%0Achannels.%20Some%20research%20has%20therefore%20turned%20to%20detecting%20discussions%20of%20ADEs%0Ain%20social%20media.%20Impressive%20results%20have%20been%20achieved%20in%20various%20attempts%20to%0Adetect%20ADEs.%20In%20a%20high-stakes%20domain%20such%20as%20medicine%2C%20however%2C%20an%20in-depth%0Aevaluation%20of%20a%20model%27s%20abilities%20is%20crucial.%20We%20address%20the%20issue%20of%20thorough%0Aperformance%20evaluation%20in%20English-language%20ADE%20detection%20with%20hand-crafted%0Atemplates%20for%20four%20capabilities%3A%20Temporal%20order%2C%20negation%2C%20sentiment%2C%20and%0Abeneficial%20effect.%20We%20find%20that%20models%20with%20similar%20performance%20on%20held-out%0Atest%20sets%20have%20varying%20results%20on%20these%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02432v1&entry.124074799=Read"},
{"title": "How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise\n  on Machine Translation", "author": "Yan Meng and Di Wu and Christof Monz", "abstract": "  The massive amounts of web-mined parallel data contain large amounts of\nnoise. Semantic misalignment, as the primary source of the noise, poses a\nchallenge for training machine translation systems. In this paper, we first\nstudy the impact of real-world hard-to-detect misalignment noise by proposing a\nprocess to simulate the realistic misalignment controlled by semantic\nsimilarity. After quantitatively analyzing the impact of simulated misalignment\non machine translation, we show the limited effectiveness of widely used\npre-filters to improve the translation performance, underscoring the necessity\nof more fine-grained ways to handle data noise. By observing the increasing\nreliability of the model's self-knowledge for distinguishing misaligned and\nclean data at the token-level, we propose a self-correction approach which\nleverages the model's prediction distribution to revise the training\nsupervision from the ground-truth data over training time. Through\ncomprehensive experiments, we show that our self-correction method not only\nimproves translation performance in the presence of simulated misalignment\nnoise but also proves effective for real-world noisy web-mined datasets across\neight translation tasks.\n", "link": "http://arxiv.org/abs/2407.02208v1", "date": "2024-07-02", "relevancy": 1.5545, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5258}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5124}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Learn%20in%20a%20Noisy%20World%3F%20Self-Correcting%20the%20Real-World%20Data%20Noise%0A%20%20on%20Machine%20Translation&body=Title%3A%20How%20to%20Learn%20in%20a%20Noisy%20World%3F%20Self-Correcting%20the%20Real-World%20Data%20Noise%0A%20%20on%20Machine%20Translation%0AAuthor%3A%20Yan%20Meng%20and%20Di%20Wu%20and%20Christof%20Monz%0AAbstract%3A%20%20%20The%20massive%20amounts%20of%20web-mined%20parallel%20data%20contain%20large%20amounts%20of%0Anoise.%20Semantic%20misalignment%2C%20as%20the%20primary%20source%20of%20the%20noise%2C%20poses%20a%0Achallenge%20for%20training%20machine%20translation%20systems.%20In%20this%20paper%2C%20we%20first%0Astudy%20the%20impact%20of%20real-world%20hard-to-detect%20misalignment%20noise%20by%20proposing%20a%0Aprocess%20to%20simulate%20the%20realistic%20misalignment%20controlled%20by%20semantic%0Asimilarity.%20After%20quantitatively%20analyzing%20the%20impact%20of%20simulated%20misalignment%0Aon%20machine%20translation%2C%20we%20show%20the%20limited%20effectiveness%20of%20widely%20used%0Apre-filters%20to%20improve%20the%20translation%20performance%2C%20underscoring%20the%20necessity%0Aof%20more%20fine-grained%20ways%20to%20handle%20data%20noise.%20By%20observing%20the%20increasing%0Areliability%20of%20the%20model%27s%20self-knowledge%20for%20distinguishing%20misaligned%20and%0Aclean%20data%20at%20the%20token-level%2C%20we%20propose%20a%20self-correction%20approach%20which%0Aleverages%20the%20model%27s%20prediction%20distribution%20to%20revise%20the%20training%0Asupervision%20from%20the%20ground-truth%20data%20over%20training%20time.%20Through%0Acomprehensive%20experiments%2C%20we%20show%20that%20our%20self-correction%20method%20not%20only%0Aimproves%20translation%20performance%20in%20the%20presence%20of%20simulated%20misalignment%0Anoise%20but%20also%20proves%20effective%20for%20real-world%20noisy%20web-mined%20datasets%20across%0Aeight%20translation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Learn%2520in%2520a%2520Noisy%2520World%253F%2520Self-Correcting%2520the%2520Real-World%2520Data%2520Noise%250A%2520%2520on%2520Machine%2520Translation%26entry.906535625%3DYan%2520Meng%2520and%2520Di%2520Wu%2520and%2520Christof%2520Monz%26entry.1292438233%3D%2520%2520The%2520massive%2520amounts%2520of%2520web-mined%2520parallel%2520data%2520contain%2520large%2520amounts%2520of%250Anoise.%2520Semantic%2520misalignment%252C%2520as%2520the%2520primary%2520source%2520of%2520the%2520noise%252C%2520poses%2520a%250Achallenge%2520for%2520training%2520machine%2520translation%2520systems.%2520In%2520this%2520paper%252C%2520we%2520first%250Astudy%2520the%2520impact%2520of%2520real-world%2520hard-to-detect%2520misalignment%2520noise%2520by%2520proposing%2520a%250Aprocess%2520to%2520simulate%2520the%2520realistic%2520misalignment%2520controlled%2520by%2520semantic%250Asimilarity.%2520After%2520quantitatively%2520analyzing%2520the%2520impact%2520of%2520simulated%2520misalignment%250Aon%2520machine%2520translation%252C%2520we%2520show%2520the%2520limited%2520effectiveness%2520of%2520widely%2520used%250Apre-filters%2520to%2520improve%2520the%2520translation%2520performance%252C%2520underscoring%2520the%2520necessity%250Aof%2520more%2520fine-grained%2520ways%2520to%2520handle%2520data%2520noise.%2520By%2520observing%2520the%2520increasing%250Areliability%2520of%2520the%2520model%2527s%2520self-knowledge%2520for%2520distinguishing%2520misaligned%2520and%250Aclean%2520data%2520at%2520the%2520token-level%252C%2520we%2520propose%2520a%2520self-correction%2520approach%2520which%250Aleverages%2520the%2520model%2527s%2520prediction%2520distribution%2520to%2520revise%2520the%2520training%250Asupervision%2520from%2520the%2520ground-truth%2520data%2520over%2520training%2520time.%2520Through%250Acomprehensive%2520experiments%252C%2520we%2520show%2520that%2520our%2520self-correction%2520method%2520not%2520only%250Aimproves%2520translation%2520performance%2520in%2520the%2520presence%2520of%2520simulated%2520misalignment%250Anoise%2520but%2520also%2520proves%2520effective%2520for%2520real-world%2520noisy%2520web-mined%2520datasets%2520across%250Aeight%2520translation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Learn%20in%20a%20Noisy%20World%3F%20Self-Correcting%20the%20Real-World%20Data%20Noise%0A%20%20on%20Machine%20Translation&entry.906535625=Yan%20Meng%20and%20Di%20Wu%20and%20Christof%20Monz&entry.1292438233=%20%20The%20massive%20amounts%20of%20web-mined%20parallel%20data%20contain%20large%20amounts%20of%0Anoise.%20Semantic%20misalignment%2C%20as%20the%20primary%20source%20of%20the%20noise%2C%20poses%20a%0Achallenge%20for%20training%20machine%20translation%20systems.%20In%20this%20paper%2C%20we%20first%0Astudy%20the%20impact%20of%20real-world%20hard-to-detect%20misalignment%20noise%20by%20proposing%20a%0Aprocess%20to%20simulate%20the%20realistic%20misalignment%20controlled%20by%20semantic%0Asimilarity.%20After%20quantitatively%20analyzing%20the%20impact%20of%20simulated%20misalignment%0Aon%20machine%20translation%2C%20we%20show%20the%20limited%20effectiveness%20of%20widely%20used%0Apre-filters%20to%20improve%20the%20translation%20performance%2C%20underscoring%20the%20necessity%0Aof%20more%20fine-grained%20ways%20to%20handle%20data%20noise.%20By%20observing%20the%20increasing%0Areliability%20of%20the%20model%27s%20self-knowledge%20for%20distinguishing%20misaligned%20and%0Aclean%20data%20at%20the%20token-level%2C%20we%20propose%20a%20self-correction%20approach%20which%0Aleverages%20the%20model%27s%20prediction%20distribution%20to%20revise%20the%20training%0Asupervision%20from%20the%20ground-truth%20data%20over%20training%20time.%20Through%0Acomprehensive%20experiments%2C%20we%20show%20that%20our%20self-correction%20method%20not%20only%0Aimproves%20translation%20performance%20in%20the%20presence%20of%20simulated%20misalignment%0Anoise%20but%20also%20proves%20effective%20for%20real-world%20noisy%20web-mined%20datasets%20across%0Aeight%20translation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02208v1&entry.124074799=Read"},
{"title": "Contractual Reinforcement Learning: Pulling Arms with Invisible Hands", "author": "Jibang Wu and Siyu Chen and Mengdi Wang and Huazheng Wang and Haifeng Xu", "abstract": "  The agency problem emerges in today's large scale machine learning tasks,\nwhere the learners are unable to direct content creation or enforce data\ncollection. In this work, we propose a theoretical framework for aligning\neconomic interests of different stakeholders in the online learning problems\nthrough contract design. The problem, termed \\emph{contractual reinforcement\nlearning}, naturally arises from the classic model of Markov decision\nprocesses, where a learning principal seeks to optimally influence the agent's\naction policy for their common interests through a set of payment rules\ncontingent on the realization of next state. For the planning problem, we\ndesign an efficient dynamic programming algorithm to determine the optimal\ncontracts against the far-sighted agent. For the learning problem, we introduce\na generic design of no-regret learning algorithms to untangle the challenges\nfrom robust design of contracts to the balance of exploration and exploitation,\nreducing the complexity analysis to the construction of efficient search\nalgorithms. For several natural classes of problems, we design tailored search\nalgorithms that provably achieve $\\tilde{O}(\\sqrt{T})$ regret. We also present\nan algorithm with $\\tilde{O}(T^{2/3})$ for the general problem that improves\nthe existing analysis in online contract design with mild technical\nassumptions.\n", "link": "http://arxiv.org/abs/2407.01458v2", "date": "2024-07-02", "relevancy": 1.503, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5193}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5017}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contractual%20Reinforcement%20Learning%3A%20Pulling%20Arms%20with%20Invisible%20Hands&body=Title%3A%20Contractual%20Reinforcement%20Learning%3A%20Pulling%20Arms%20with%20Invisible%20Hands%0AAuthor%3A%20Jibang%20Wu%20and%20Siyu%20Chen%20and%20Mengdi%20Wang%20and%20Huazheng%20Wang%20and%20Haifeng%20Xu%0AAbstract%3A%20%20%20The%20agency%20problem%20emerges%20in%20today%27s%20large%20scale%20machine%20learning%20tasks%2C%0Awhere%20the%20learners%20are%20unable%20to%20direct%20content%20creation%20or%20enforce%20data%0Acollection.%20In%20this%20work%2C%20we%20propose%20a%20theoretical%20framework%20for%20aligning%0Aeconomic%20interests%20of%20different%20stakeholders%20in%20the%20online%20learning%20problems%0Athrough%20contract%20design.%20The%20problem%2C%20termed%20%5Cemph%7Bcontractual%20reinforcement%0Alearning%7D%2C%20naturally%20arises%20from%20the%20classic%20model%20of%20Markov%20decision%0Aprocesses%2C%20where%20a%20learning%20principal%20seeks%20to%20optimally%20influence%20the%20agent%27s%0Aaction%20policy%20for%20their%20common%20interests%20through%20a%20set%20of%20payment%20rules%0Acontingent%20on%20the%20realization%20of%20next%20state.%20For%20the%20planning%20problem%2C%20we%0Adesign%20an%20efficient%20dynamic%20programming%20algorithm%20to%20determine%20the%20optimal%0Acontracts%20against%20the%20far-sighted%20agent.%20For%20the%20learning%20problem%2C%20we%20introduce%0Aa%20generic%20design%20of%20no-regret%20learning%20algorithms%20to%20untangle%20the%20challenges%0Afrom%20robust%20design%20of%20contracts%20to%20the%20balance%20of%20exploration%20and%20exploitation%2C%0Areducing%20the%20complexity%20analysis%20to%20the%20construction%20of%20efficient%20search%0Aalgorithms.%20For%20several%20natural%20classes%20of%20problems%2C%20we%20design%20tailored%20search%0Aalgorithms%20that%20provably%20achieve%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret.%20We%20also%20present%0Aan%20algorithm%20with%20%24%5Ctilde%7BO%7D%28T%5E%7B2/3%7D%29%24%20for%20the%20general%20problem%20that%20improves%0Athe%20existing%20analysis%20in%20online%20contract%20design%20with%20mild%20technical%0Aassumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContractual%2520Reinforcement%2520Learning%253A%2520Pulling%2520Arms%2520with%2520Invisible%2520Hands%26entry.906535625%3DJibang%2520Wu%2520and%2520Siyu%2520Chen%2520and%2520Mengdi%2520Wang%2520and%2520Huazheng%2520Wang%2520and%2520Haifeng%2520Xu%26entry.1292438233%3D%2520%2520The%2520agency%2520problem%2520emerges%2520in%2520today%2527s%2520large%2520scale%2520machine%2520learning%2520tasks%252C%250Awhere%2520the%2520learners%2520are%2520unable%2520to%2520direct%2520content%2520creation%2520or%2520enforce%2520data%250Acollection.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520theoretical%2520framework%2520for%2520aligning%250Aeconomic%2520interests%2520of%2520different%2520stakeholders%2520in%2520the%2520online%2520learning%2520problems%250Athrough%2520contract%2520design.%2520The%2520problem%252C%2520termed%2520%255Cemph%257Bcontractual%2520reinforcement%250Alearning%257D%252C%2520naturally%2520arises%2520from%2520the%2520classic%2520model%2520of%2520Markov%2520decision%250Aprocesses%252C%2520where%2520a%2520learning%2520principal%2520seeks%2520to%2520optimally%2520influence%2520the%2520agent%2527s%250Aaction%2520policy%2520for%2520their%2520common%2520interests%2520through%2520a%2520set%2520of%2520payment%2520rules%250Acontingent%2520on%2520the%2520realization%2520of%2520next%2520state.%2520For%2520the%2520planning%2520problem%252C%2520we%250Adesign%2520an%2520efficient%2520dynamic%2520programming%2520algorithm%2520to%2520determine%2520the%2520optimal%250Acontracts%2520against%2520the%2520far-sighted%2520agent.%2520For%2520the%2520learning%2520problem%252C%2520we%2520introduce%250Aa%2520generic%2520design%2520of%2520no-regret%2520learning%2520algorithms%2520to%2520untangle%2520the%2520challenges%250Afrom%2520robust%2520design%2520of%2520contracts%2520to%2520the%2520balance%2520of%2520exploration%2520and%2520exploitation%252C%250Areducing%2520the%2520complexity%2520analysis%2520to%2520the%2520construction%2520of%2520efficient%2520search%250Aalgorithms.%2520For%2520several%2520natural%2520classes%2520of%2520problems%252C%2520we%2520design%2520tailored%2520search%250Aalgorithms%2520that%2520provably%2520achieve%2520%2524%255Ctilde%257BO%257D%2528%255Csqrt%257BT%257D%2529%2524%2520regret.%2520We%2520also%2520present%250Aan%2520algorithm%2520with%2520%2524%255Ctilde%257BO%257D%2528T%255E%257B2/3%257D%2529%2524%2520for%2520the%2520general%2520problem%2520that%2520improves%250Athe%2520existing%2520analysis%2520in%2520online%2520contract%2520design%2520with%2520mild%2520technical%250Aassumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contractual%20Reinforcement%20Learning%3A%20Pulling%20Arms%20with%20Invisible%20Hands&entry.906535625=Jibang%20Wu%20and%20Siyu%20Chen%20and%20Mengdi%20Wang%20and%20Huazheng%20Wang%20and%20Haifeng%20Xu&entry.1292438233=%20%20The%20agency%20problem%20emerges%20in%20today%27s%20large%20scale%20machine%20learning%20tasks%2C%0Awhere%20the%20learners%20are%20unable%20to%20direct%20content%20creation%20or%20enforce%20data%0Acollection.%20In%20this%20work%2C%20we%20propose%20a%20theoretical%20framework%20for%20aligning%0Aeconomic%20interests%20of%20different%20stakeholders%20in%20the%20online%20learning%20problems%0Athrough%20contract%20design.%20The%20problem%2C%20termed%20%5Cemph%7Bcontractual%20reinforcement%0Alearning%7D%2C%20naturally%20arises%20from%20the%20classic%20model%20of%20Markov%20decision%0Aprocesses%2C%20where%20a%20learning%20principal%20seeks%20to%20optimally%20influence%20the%20agent%27s%0Aaction%20policy%20for%20their%20common%20interests%20through%20a%20set%20of%20payment%20rules%0Acontingent%20on%20the%20realization%20of%20next%20state.%20For%20the%20planning%20problem%2C%20we%0Adesign%20an%20efficient%20dynamic%20programming%20algorithm%20to%20determine%20the%20optimal%0Acontracts%20against%20the%20far-sighted%20agent.%20For%20the%20learning%20problem%2C%20we%20introduce%0Aa%20generic%20design%20of%20no-regret%20learning%20algorithms%20to%20untangle%20the%20challenges%0Afrom%20robust%20design%20of%20contracts%20to%20the%20balance%20of%20exploration%20and%20exploitation%2C%0Areducing%20the%20complexity%20analysis%20to%20the%20construction%20of%20efficient%20search%0Aalgorithms.%20For%20several%20natural%20classes%20of%20problems%2C%20we%20design%20tailored%20search%0Aalgorithms%20that%20provably%20achieve%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret.%20We%20also%20present%0Aan%20algorithm%20with%20%24%5Ctilde%7BO%7D%28T%5E%7B2/3%7D%29%24%20for%20the%20general%20problem%20that%20improves%0Athe%20existing%20analysis%20in%20online%20contract%20design%20with%20mild%20technical%0Aassumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01458v2&entry.124074799=Read"},
{"title": "Multi-Modal Video Dialog State Tracking in the Wild", "author": "Adnen Abdessaied and Lei Shi and Andreas Bulling", "abstract": "  We present MST-MIXER - a novel video dialog model operating over a generic\nmulti-modal state tracking scheme. Current models that claim to perform\nmulti-modal state tracking fall short of two major aspects: (1) They either\ntrack only one modality (mostly the visual input) or (2) they target synthetic\ndatasets that do not reflect the complexity of real-world in the wild\nscenarios. Our model addresses these two limitations in an attempt to close\nthis crucial research gap. Specifically, MST-MIXER first tracks the most\nimportant constituents of each input modality. Then, it predicts the missing\nunderlying structure of the selected constituents of each modality by learning\nlocal latent graphs using a novel multi-modal graph structure learning method.\nSubsequently, the learned local graphs and features are parsed together to form\na global graph operating on the mix of all modalities which further refines its\nstructure and node embeddings. Finally, the fine-grained graph node features\nare used to enhance the hidden states of the backbone Vision-Language Model\n(VLM). MST-MIXER achieves new state-of-the-art results on five challenging\nbenchmarks.\n", "link": "http://arxiv.org/abs/2407.02218v1", "date": "2024-07-02", "relevancy": 1.7048, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5756}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Video%20Dialog%20State%20Tracking%20in%20the%20Wild&body=Title%3A%20Multi-Modal%20Video%20Dialog%20State%20Tracking%20in%20the%20Wild%0AAuthor%3A%20Adnen%20Abdessaied%20and%20Lei%20Shi%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20present%20MST-MIXER%20-%20a%20novel%20video%20dialog%20model%20operating%20over%20a%20generic%0Amulti-modal%20state%20tracking%20scheme.%20Current%20models%20that%20claim%20to%20perform%0Amulti-modal%20state%20tracking%20fall%20short%20of%20two%20major%20aspects%3A%20%281%29%20They%20either%0Atrack%20only%20one%20modality%20%28mostly%20the%20visual%20input%29%20or%20%282%29%20they%20target%20synthetic%0Adatasets%20that%20do%20not%20reflect%20the%20complexity%20of%20real-world%20in%20the%20wild%0Ascenarios.%20Our%20model%20addresses%20these%20two%20limitations%20in%20an%20attempt%20to%20close%0Athis%20crucial%20research%20gap.%20Specifically%2C%20MST-MIXER%20first%20tracks%20the%20most%0Aimportant%20constituents%20of%20each%20input%20modality.%20Then%2C%20it%20predicts%20the%20missing%0Aunderlying%20structure%20of%20the%20selected%20constituents%20of%20each%20modality%20by%20learning%0Alocal%20latent%20graphs%20using%20a%20novel%20multi-modal%20graph%20structure%20learning%20method.%0ASubsequently%2C%20the%20learned%20local%20graphs%20and%20features%20are%20parsed%20together%20to%20form%0Aa%20global%20graph%20operating%20on%20the%20mix%20of%20all%20modalities%20which%20further%20refines%20its%0Astructure%20and%20node%20embeddings.%20Finally%2C%20the%20fine-grained%20graph%20node%20features%0Aare%20used%20to%20enhance%20the%20hidden%20states%20of%20the%20backbone%20Vision-Language%20Model%0A%28VLM%29.%20MST-MIXER%20achieves%20new%20state-of-the-art%20results%20on%20five%20challenging%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Video%2520Dialog%2520State%2520Tracking%2520in%2520the%2520Wild%26entry.906535625%3DAdnen%2520Abdessaied%2520and%2520Lei%2520Shi%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520We%2520present%2520MST-MIXER%2520-%2520a%2520novel%2520video%2520dialog%2520model%2520operating%2520over%2520a%2520generic%250Amulti-modal%2520state%2520tracking%2520scheme.%2520Current%2520models%2520that%2520claim%2520to%2520perform%250Amulti-modal%2520state%2520tracking%2520fall%2520short%2520of%2520two%2520major%2520aspects%253A%2520%25281%2529%2520They%2520either%250Atrack%2520only%2520one%2520modality%2520%2528mostly%2520the%2520visual%2520input%2529%2520or%2520%25282%2529%2520they%2520target%2520synthetic%250Adatasets%2520that%2520do%2520not%2520reflect%2520the%2520complexity%2520of%2520real-world%2520in%2520the%2520wild%250Ascenarios.%2520Our%2520model%2520addresses%2520these%2520two%2520limitations%2520in%2520an%2520attempt%2520to%2520close%250Athis%2520crucial%2520research%2520gap.%2520Specifically%252C%2520MST-MIXER%2520first%2520tracks%2520the%2520most%250Aimportant%2520constituents%2520of%2520each%2520input%2520modality.%2520Then%252C%2520it%2520predicts%2520the%2520missing%250Aunderlying%2520structure%2520of%2520the%2520selected%2520constituents%2520of%2520each%2520modality%2520by%2520learning%250Alocal%2520latent%2520graphs%2520using%2520a%2520novel%2520multi-modal%2520graph%2520structure%2520learning%2520method.%250ASubsequently%252C%2520the%2520learned%2520local%2520graphs%2520and%2520features%2520are%2520parsed%2520together%2520to%2520form%250Aa%2520global%2520graph%2520operating%2520on%2520the%2520mix%2520of%2520all%2520modalities%2520which%2520further%2520refines%2520its%250Astructure%2520and%2520node%2520embeddings.%2520Finally%252C%2520the%2520fine-grained%2520graph%2520node%2520features%250Aare%2520used%2520to%2520enhance%2520the%2520hidden%2520states%2520of%2520the%2520backbone%2520Vision-Language%2520Model%250A%2528VLM%2529.%2520MST-MIXER%2520achieves%2520new%2520state-of-the-art%2520results%2520on%2520five%2520challenging%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Video%20Dialog%20State%20Tracking%20in%20the%20Wild&entry.906535625=Adnen%20Abdessaied%20and%20Lei%20Shi%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20present%20MST-MIXER%20-%20a%20novel%20video%20dialog%20model%20operating%20over%20a%20generic%0Amulti-modal%20state%20tracking%20scheme.%20Current%20models%20that%20claim%20to%20perform%0Amulti-modal%20state%20tracking%20fall%20short%20of%20two%20major%20aspects%3A%20%281%29%20They%20either%0Atrack%20only%20one%20modality%20%28mostly%20the%20visual%20input%29%20or%20%282%29%20they%20target%20synthetic%0Adatasets%20that%20do%20not%20reflect%20the%20complexity%20of%20real-world%20in%20the%20wild%0Ascenarios.%20Our%20model%20addresses%20these%20two%20limitations%20in%20an%20attempt%20to%20close%0Athis%20crucial%20research%20gap.%20Specifically%2C%20MST-MIXER%20first%20tracks%20the%20most%0Aimportant%20constituents%20of%20each%20input%20modality.%20Then%2C%20it%20predicts%20the%20missing%0Aunderlying%20structure%20of%20the%20selected%20constituents%20of%20each%20modality%20by%20learning%0Alocal%20latent%20graphs%20using%20a%20novel%20multi-modal%20graph%20structure%20learning%20method.%0ASubsequently%2C%20the%20learned%20local%20graphs%20and%20features%20are%20parsed%20together%20to%20form%0Aa%20global%20graph%20operating%20on%20the%20mix%20of%20all%20modalities%20which%20further%20refines%20its%0Astructure%20and%20node%20embeddings.%20Finally%2C%20the%20fine-grained%20graph%20node%20features%0Aare%20used%20to%20enhance%20the%20hidden%20states%20of%20the%20backbone%20Vision-Language%20Model%0A%28VLM%29.%20MST-MIXER%20achieves%20new%20state-of-the-art%20results%20on%20five%20challenging%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02218v1&entry.124074799=Read"},
{"title": "ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image\n  and Text", "author": "Dingkun Yan and Liang Yuan and Erwin Wu and Yuma Nishioka and Issei Fujishiro and Suguru Saito", "abstract": "  Diffusion models have recently demonstrated their effectiveness in generating\nextremely high-quality images and are now utilized in a wide range of\napplications, including automatic sketch colorization. Although many methods\nhave been developed for guided sketch colorization, there has been limited\nexploration of the potential conflicts between image prompts and sketch inputs,\nwhich can lead to severe deterioration in the results. Therefore, this paper\nexhaustively investigates reference-based sketch colorization models that aim\nto colorize sketch images using reference color images. We specifically\ninvestigate two critical aspects of reference-based diffusion models: the\n\"distribution problem\", which is a major shortcoming compared to text-based\ncounterparts, and the capability in zero-shot sequential text-based\nmanipulation. We introduce two variations of an image-guided latent diffusion\nmodel utilizing different image tokens from the pre-trained CLIP image encoder\nand propose corresponding manipulation methods to adjust their results\nsequentially using weighted text inputs. We conduct comprehensive evaluations\nof our models through qualitative and quantitative experiments as well as a\nuser study.\n", "link": "http://arxiv.org/abs/2401.01456v2", "date": "2024-07-02", "relevancy": 1.2802, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6546}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6426}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColorizeDiffusion%3A%20Adjustable%20Sketch%20Colorization%20with%20Reference%20Image%0A%20%20and%20Text&body=Title%3A%20ColorizeDiffusion%3A%20Adjustable%20Sketch%20Colorization%20with%20Reference%20Image%0A%20%20and%20Text%0AAuthor%3A%20Dingkun%20Yan%20and%20Liang%20Yuan%20and%20Erwin%20Wu%20and%20Yuma%20Nishioka%20and%20Issei%20Fujishiro%20and%20Suguru%20Saito%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20demonstrated%20their%20effectiveness%20in%20generating%0Aextremely%20high-quality%20images%20and%20are%20now%20utilized%20in%20a%20wide%20range%20of%0Aapplications%2C%20including%20automatic%20sketch%20colorization.%20Although%20many%20methods%0Ahave%20been%20developed%20for%20guided%20sketch%20colorization%2C%20there%20has%20been%20limited%0Aexploration%20of%20the%20potential%20conflicts%20between%20image%20prompts%20and%20sketch%20inputs%2C%0Awhich%20can%20lead%20to%20severe%20deterioration%20in%20the%20results.%20Therefore%2C%20this%20paper%0Aexhaustively%20investigates%20reference-based%20sketch%20colorization%20models%20that%20aim%0Ato%20colorize%20sketch%20images%20using%20reference%20color%20images.%20We%20specifically%0Ainvestigate%20two%20critical%20aspects%20of%20reference-based%20diffusion%20models%3A%20the%0A%22distribution%20problem%22%2C%20which%20is%20a%20major%20shortcoming%20compared%20to%20text-based%0Acounterparts%2C%20and%20the%20capability%20in%20zero-shot%20sequential%20text-based%0Amanipulation.%20We%20introduce%20two%20variations%20of%20an%20image-guided%20latent%20diffusion%0Amodel%20utilizing%20different%20image%20tokens%20from%20the%20pre-trained%20CLIP%20image%20encoder%0Aand%20propose%20corresponding%20manipulation%20methods%20to%20adjust%20their%20results%0Asequentially%20using%20weighted%20text%20inputs.%20We%20conduct%20comprehensive%20evaluations%0Aof%20our%20models%20through%20qualitative%20and%20quantitative%20experiments%20as%20well%20as%20a%0Auser%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorizeDiffusion%253A%2520Adjustable%2520Sketch%2520Colorization%2520with%2520Reference%2520Image%250A%2520%2520and%2520Text%26entry.906535625%3DDingkun%2520Yan%2520and%2520Liang%2520Yuan%2520and%2520Erwin%2520Wu%2520and%2520Yuma%2520Nishioka%2520and%2520Issei%2520Fujishiro%2520and%2520Suguru%2520Saito%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520recently%2520demonstrated%2520their%2520effectiveness%2520in%2520generating%250Aextremely%2520high-quality%2520images%2520and%2520are%2520now%2520utilized%2520in%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520including%2520automatic%2520sketch%2520colorization.%2520Although%2520many%2520methods%250Ahave%2520been%2520developed%2520for%2520guided%2520sketch%2520colorization%252C%2520there%2520has%2520been%2520limited%250Aexploration%2520of%2520the%2520potential%2520conflicts%2520between%2520image%2520prompts%2520and%2520sketch%2520inputs%252C%250Awhich%2520can%2520lead%2520to%2520severe%2520deterioration%2520in%2520the%2520results.%2520Therefore%252C%2520this%2520paper%250Aexhaustively%2520investigates%2520reference-based%2520sketch%2520colorization%2520models%2520that%2520aim%250Ato%2520colorize%2520sketch%2520images%2520using%2520reference%2520color%2520images.%2520We%2520specifically%250Ainvestigate%2520two%2520critical%2520aspects%2520of%2520reference-based%2520diffusion%2520models%253A%2520the%250A%2522distribution%2520problem%2522%252C%2520which%2520is%2520a%2520major%2520shortcoming%2520compared%2520to%2520text-based%250Acounterparts%252C%2520and%2520the%2520capability%2520in%2520zero-shot%2520sequential%2520text-based%250Amanipulation.%2520We%2520introduce%2520two%2520variations%2520of%2520an%2520image-guided%2520latent%2520diffusion%250Amodel%2520utilizing%2520different%2520image%2520tokens%2520from%2520the%2520pre-trained%2520CLIP%2520image%2520encoder%250Aand%2520propose%2520corresponding%2520manipulation%2520methods%2520to%2520adjust%2520their%2520results%250Asequentially%2520using%2520weighted%2520text%2520inputs.%2520We%2520conduct%2520comprehensive%2520evaluations%250Aof%2520our%2520models%2520through%2520qualitative%2520and%2520quantitative%2520experiments%2520as%2520well%2520as%2520a%250Auser%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorizeDiffusion%3A%20Adjustable%20Sketch%20Colorization%20with%20Reference%20Image%0A%20%20and%20Text&entry.906535625=Dingkun%20Yan%20and%20Liang%20Yuan%20and%20Erwin%20Wu%20and%20Yuma%20Nishioka%20and%20Issei%20Fujishiro%20and%20Suguru%20Saito&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20demonstrated%20their%20effectiveness%20in%20generating%0Aextremely%20high-quality%20images%20and%20are%20now%20utilized%20in%20a%20wide%20range%20of%0Aapplications%2C%20including%20automatic%20sketch%20colorization.%20Although%20many%20methods%0Ahave%20been%20developed%20for%20guided%20sketch%20colorization%2C%20there%20has%20been%20limited%0Aexploration%20of%20the%20potential%20conflicts%20between%20image%20prompts%20and%20sketch%20inputs%2C%0Awhich%20can%20lead%20to%20severe%20deterioration%20in%20the%20results.%20Therefore%2C%20this%20paper%0Aexhaustively%20investigates%20reference-based%20sketch%20colorization%20models%20that%20aim%0Ato%20colorize%20sketch%20images%20using%20reference%20color%20images.%20We%20specifically%0Ainvestigate%20two%20critical%20aspects%20of%20reference-based%20diffusion%20models%3A%20the%0A%22distribution%20problem%22%2C%20which%20is%20a%20major%20shortcoming%20compared%20to%20text-based%0Acounterparts%2C%20and%20the%20capability%20in%20zero-shot%20sequential%20text-based%0Amanipulation.%20We%20introduce%20two%20variations%20of%20an%20image-guided%20latent%20diffusion%0Amodel%20utilizing%20different%20image%20tokens%20from%20the%20pre-trained%20CLIP%20image%20encoder%0Aand%20propose%20corresponding%20manipulation%20methods%20to%20adjust%20their%20results%0Asequentially%20using%20weighted%20text%20inputs.%20We%20conduct%20comprehensive%20evaluations%0Aof%20our%20models%20through%20qualitative%20and%20quantitative%20experiments%20as%20well%20as%20a%0Auser%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01456v2&entry.124074799=Read"},
{"title": "Automatic Adaptation Rule Optimization via Large Language Models", "author": "Yusei Ishimizu and Jialong Li and Jinglue Xu and Jinyu Cai and Hitoshi Iba and Kenji Tei", "abstract": "  Rule-based adaptation is a foundational approach to self-adaptation,\ncharacterized by its human readability and rapid response. However, building\nhigh-performance and robust adaptation rules is often a challenge because it\nessentially involves searching the optimal design in a complex (variables)\nspace. In response, this paper attempt to employ large language models (LLMs)\nas a optimizer to construct and optimize adaptation rules, leveraging the\ncommon sense and reasoning capabilities inherent in LLMs. Preliminary\nexperiments conducted in SWIM have validated the effectiveness and limitation\nof our method.\n", "link": "http://arxiv.org/abs/2407.02203v1", "date": "2024-07-02", "relevancy": 1.4203, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4809}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4702}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Adaptation%20Rule%20Optimization%20via%20Large%20Language%20Models&body=Title%3A%20Automatic%20Adaptation%20Rule%20Optimization%20via%20Large%20Language%20Models%0AAuthor%3A%20Yusei%20Ishimizu%20and%20Jialong%20Li%20and%20Jinglue%20Xu%20and%20Jinyu%20Cai%20and%20Hitoshi%20Iba%20and%20Kenji%20Tei%0AAbstract%3A%20%20%20Rule-based%20adaptation%20is%20a%20foundational%20approach%20to%20self-adaptation%2C%0Acharacterized%20by%20its%20human%20readability%20and%20rapid%20response.%20However%2C%20building%0Ahigh-performance%20and%20robust%20adaptation%20rules%20is%20often%20a%20challenge%20because%20it%0Aessentially%20involves%20searching%20the%20optimal%20design%20in%20a%20complex%20%28variables%29%0Aspace.%20In%20response%2C%20this%20paper%20attempt%20to%20employ%20large%20language%20models%20%28LLMs%29%0Aas%20a%20optimizer%20to%20construct%20and%20optimize%20adaptation%20rules%2C%20leveraging%20the%0Acommon%20sense%20and%20reasoning%20capabilities%20inherent%20in%20LLMs.%20Preliminary%0Aexperiments%20conducted%20in%20SWIM%20have%20validated%20the%20effectiveness%20and%20limitation%0Aof%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Adaptation%2520Rule%2520Optimization%2520via%2520Large%2520Language%2520Models%26entry.906535625%3DYusei%2520Ishimizu%2520and%2520Jialong%2520Li%2520and%2520Jinglue%2520Xu%2520and%2520Jinyu%2520Cai%2520and%2520Hitoshi%2520Iba%2520and%2520Kenji%2520Tei%26entry.1292438233%3D%2520%2520Rule-based%2520adaptation%2520is%2520a%2520foundational%2520approach%2520to%2520self-adaptation%252C%250Acharacterized%2520by%2520its%2520human%2520readability%2520and%2520rapid%2520response.%2520However%252C%2520building%250Ahigh-performance%2520and%2520robust%2520adaptation%2520rules%2520is%2520often%2520a%2520challenge%2520because%2520it%250Aessentially%2520involves%2520searching%2520the%2520optimal%2520design%2520in%2520a%2520complex%2520%2528variables%2529%250Aspace.%2520In%2520response%252C%2520this%2520paper%2520attempt%2520to%2520employ%2520large%2520language%2520models%2520%2528LLMs%2529%250Aas%2520a%2520optimizer%2520to%2520construct%2520and%2520optimize%2520adaptation%2520rules%252C%2520leveraging%2520the%250Acommon%2520sense%2520and%2520reasoning%2520capabilities%2520inherent%2520in%2520LLMs.%2520Preliminary%250Aexperiments%2520conducted%2520in%2520SWIM%2520have%2520validated%2520the%2520effectiveness%2520and%2520limitation%250Aof%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Adaptation%20Rule%20Optimization%20via%20Large%20Language%20Models&entry.906535625=Yusei%20Ishimizu%20and%20Jialong%20Li%20and%20Jinglue%20Xu%20and%20Jinyu%20Cai%20and%20Hitoshi%20Iba%20and%20Kenji%20Tei&entry.1292438233=%20%20Rule-based%20adaptation%20is%20a%20foundational%20approach%20to%20self-adaptation%2C%0Acharacterized%20by%20its%20human%20readability%20and%20rapid%20response.%20However%2C%20building%0Ahigh-performance%20and%20robust%20adaptation%20rules%20is%20often%20a%20challenge%20because%20it%0Aessentially%20involves%20searching%20the%20optimal%20design%20in%20a%20complex%20%28variables%29%0Aspace.%20In%20response%2C%20this%20paper%20attempt%20to%20employ%20large%20language%20models%20%28LLMs%29%0Aas%20a%20optimizer%20to%20construct%20and%20optimize%20adaptation%20rules%2C%20leveraging%20the%0Acommon%20sense%20and%20reasoning%20capabilities%20inherent%20in%20LLMs.%20Preliminary%0Aexperiments%20conducted%20in%20SWIM%20have%20validated%20the%20effectiveness%20and%20limitation%0Aof%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02203v1&entry.124074799=Read"},
{"title": "Survey of Simulators for Aerial Robots", "author": "Cora A. Dimmig and Giuseppe Silano and Kimberly McGuire and Chiara Gabellieri and Wolfgang H\u00f6nig and Joseph Moore and Marin Kobilarov", "abstract": "  Uncrewed Aerial Vehicle (UAV) research faces challenges with safety,\nscalability, costs, and ecological impact when conducting hardware testing.\nHigh-fidelity simulators offer a vital solution by replicating real-world\nconditions to enable the development and evaluation of novel perception and\ncontrol algorithms. However, the large number of available simulators poses a\nsignificant challenge for researchers to determine which simulator best suits\ntheir specific use-case, based on each simulator's limitations and\ncustomization readiness. In this paper we present an overview of 43 UAV\nsimulators, including in-depth, systematic comparisons for 17 of the\nsimulators. Additionally, we present a set of decision factors for selection of\nsimulators, aiming to enhance the efficiency and safety of research endeavors.\n", "link": "http://arxiv.org/abs/2311.02296v4", "date": "2024-07-02", "relevancy": 1.3321, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4472}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20of%20Simulators%20for%20Aerial%20Robots&body=Title%3A%20Survey%20of%20Simulators%20for%20Aerial%20Robots%0AAuthor%3A%20Cora%20A.%20Dimmig%20and%20Giuseppe%20Silano%20and%20Kimberly%20McGuire%20and%20Chiara%20Gabellieri%20and%20Wolfgang%20H%C3%B6nig%20and%20Joseph%20Moore%20and%20Marin%20Kobilarov%0AAbstract%3A%20%20%20Uncrewed%20Aerial%20Vehicle%20%28UAV%29%20research%20faces%20challenges%20with%20safety%2C%0Ascalability%2C%20costs%2C%20and%20ecological%20impact%20when%20conducting%20hardware%20testing.%0AHigh-fidelity%20simulators%20offer%20a%20vital%20solution%20by%20replicating%20real-world%0Aconditions%20to%20enable%20the%20development%20and%20evaluation%20of%20novel%20perception%20and%0Acontrol%20algorithms.%20However%2C%20the%20large%20number%20of%20available%20simulators%20poses%20a%0Asignificant%20challenge%20for%20researchers%20to%20determine%20which%20simulator%20best%20suits%0Atheir%20specific%20use-case%2C%20based%20on%20each%20simulator%27s%20limitations%20and%0Acustomization%20readiness.%20In%20this%20paper%20we%20present%20an%20overview%20of%2043%20UAV%0Asimulators%2C%20including%20in-depth%2C%20systematic%20comparisons%20for%2017%20of%20the%0Asimulators.%20Additionally%2C%20we%20present%20a%20set%20of%20decision%20factors%20for%20selection%20of%0Asimulators%2C%20aiming%20to%20enhance%20the%20efficiency%20and%20safety%20of%20research%20endeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02296v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520of%2520Simulators%2520for%2520Aerial%2520Robots%26entry.906535625%3DCora%2520A.%2520Dimmig%2520and%2520Giuseppe%2520Silano%2520and%2520Kimberly%2520McGuire%2520and%2520Chiara%2520Gabellieri%2520and%2520Wolfgang%2520H%25C3%25B6nig%2520and%2520Joseph%2520Moore%2520and%2520Marin%2520Kobilarov%26entry.1292438233%3D%2520%2520Uncrewed%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520research%2520faces%2520challenges%2520with%2520safety%252C%250Ascalability%252C%2520costs%252C%2520and%2520ecological%2520impact%2520when%2520conducting%2520hardware%2520testing.%250AHigh-fidelity%2520simulators%2520offer%2520a%2520vital%2520solution%2520by%2520replicating%2520real-world%250Aconditions%2520to%2520enable%2520the%2520development%2520and%2520evaluation%2520of%2520novel%2520perception%2520and%250Acontrol%2520algorithms.%2520However%252C%2520the%2520large%2520number%2520of%2520available%2520simulators%2520poses%2520a%250Asignificant%2520challenge%2520for%2520researchers%2520to%2520determine%2520which%2520simulator%2520best%2520suits%250Atheir%2520specific%2520use-case%252C%2520based%2520on%2520each%2520simulator%2527s%2520limitations%2520and%250Acustomization%2520readiness.%2520In%2520this%2520paper%2520we%2520present%2520an%2520overview%2520of%252043%2520UAV%250Asimulators%252C%2520including%2520in-depth%252C%2520systematic%2520comparisons%2520for%252017%2520of%2520the%250Asimulators.%2520Additionally%252C%2520we%2520present%2520a%2520set%2520of%2520decision%2520factors%2520for%2520selection%2520of%250Asimulators%252C%2520aiming%2520to%2520enhance%2520the%2520efficiency%2520and%2520safety%2520of%2520research%2520endeavors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02296v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20of%20Simulators%20for%20Aerial%20Robots&entry.906535625=Cora%20A.%20Dimmig%20and%20Giuseppe%20Silano%20and%20Kimberly%20McGuire%20and%20Chiara%20Gabellieri%20and%20Wolfgang%20H%C3%B6nig%20and%20Joseph%20Moore%20and%20Marin%20Kobilarov&entry.1292438233=%20%20Uncrewed%20Aerial%20Vehicle%20%28UAV%29%20research%20faces%20challenges%20with%20safety%2C%0Ascalability%2C%20costs%2C%20and%20ecological%20impact%20when%20conducting%20hardware%20testing.%0AHigh-fidelity%20simulators%20offer%20a%20vital%20solution%20by%20replicating%20real-world%0Aconditions%20to%20enable%20the%20development%20and%20evaluation%20of%20novel%20perception%20and%0Acontrol%20algorithms.%20However%2C%20the%20large%20number%20of%20available%20simulators%20poses%20a%0Asignificant%20challenge%20for%20researchers%20to%20determine%20which%20simulator%20best%20suits%0Atheir%20specific%20use-case%2C%20based%20on%20each%20simulator%27s%20limitations%20and%0Acustomization%20readiness.%20In%20this%20paper%20we%20present%20an%20overview%20of%2043%20UAV%0Asimulators%2C%20including%20in-depth%2C%20systematic%20comparisons%20for%2017%20of%20the%0Asimulators.%20Additionally%2C%20we%20present%20a%20set%20of%20decision%20factors%20for%20selection%20of%0Asimulators%2C%20aiming%20to%20enhance%20the%20efficiency%20and%20safety%20of%20research%20endeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02296v4&entry.124074799=Read"},
{"title": "Embedded FPGA Developments in 130nm and 28nm CMOS for Machine Learning\n  in Particle Detector Readout", "author": "Julia Gonski and Aseem Gupta and Haoyi Jia and Hyunjoon Kim and Lorenzo Rota and Larry Ruckman and Angelo Dragone and Ryan Herbst", "abstract": "  Embedded field programmable gate array (eFPGA) technology allows the\nimplementation of reconfigurable logic within the design of an\napplication-specific integrated circuit (ASIC). This approach offers the low\npower and efficiency of an ASIC along with the ease of FPGA configuration,\nparticularly beneficial for the use case of machine learning in the data\npipeline of next-generation collider experiments. An open-source framework\ncalled \"FABulous\" was used to design eFPGAs using 130 nm and 28 nm CMOS\ntechnology nodes, which were subsequently fabricated and verified through\ntesting. The capability of an eFPGA to act as a front-end readout chip was\nassessed using simulation of high energy particles passing through a silicon\npixel sensor. A machine learning-based classifier, designed for reduction of\nsensor data at the source, was synthesized and configured onto the eFPGA. A\nsuccessful proof-of-concept was demonstrated through reproduction of the\nexpected algorithm result on the eFPGA with perfect accuracy. Further\ndevelopment of the eFPGA technology and its application to collider detector\nreadout is discussed.\n", "link": "http://arxiv.org/abs/2404.17701v3", "date": "2024-07-02", "relevancy": 1.2544, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4332}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20FPGA%20Developments%20in%20130nm%20and%2028nm%20CMOS%20for%20Machine%20Learning%0A%20%20in%20Particle%20Detector%20Readout&body=Title%3A%20Embedded%20FPGA%20Developments%20in%20130nm%20and%2028nm%20CMOS%20for%20Machine%20Learning%0A%20%20in%20Particle%20Detector%20Readout%0AAuthor%3A%20Julia%20Gonski%20and%20Aseem%20Gupta%20and%20Haoyi%20Jia%20and%20Hyunjoon%20Kim%20and%20Lorenzo%20Rota%20and%20Larry%20Ruckman%20and%20Angelo%20Dragone%20and%20Ryan%20Herbst%0AAbstract%3A%20%20%20Embedded%20field%20programmable%20gate%20array%20%28eFPGA%29%20technology%20allows%20the%0Aimplementation%20of%20reconfigurable%20logic%20within%20the%20design%20of%20an%0Aapplication-specific%20integrated%20circuit%20%28ASIC%29.%20This%20approach%20offers%20the%20low%0Apower%20and%20efficiency%20of%20an%20ASIC%20along%20with%20the%20ease%20of%20FPGA%20configuration%2C%0Aparticularly%20beneficial%20for%20the%20use%20case%20of%20machine%20learning%20in%20the%20data%0Apipeline%20of%20next-generation%20collider%20experiments.%20An%20open-source%20framework%0Acalled%20%22FABulous%22%20was%20used%20to%20design%20eFPGAs%20using%20130%20nm%20and%2028%20nm%20CMOS%0Atechnology%20nodes%2C%20which%20were%20subsequently%20fabricated%20and%20verified%20through%0Atesting.%20The%20capability%20of%20an%20eFPGA%20to%20act%20as%20a%20front-end%20readout%20chip%20was%0Aassessed%20using%20simulation%20of%20high%20energy%20particles%20passing%20through%20a%20silicon%0Apixel%20sensor.%20A%20machine%20learning-based%20classifier%2C%20designed%20for%20reduction%20of%0Asensor%20data%20at%20the%20source%2C%20was%20synthesized%20and%20configured%20onto%20the%20eFPGA.%20A%0Asuccessful%20proof-of-concept%20was%20demonstrated%20through%20reproduction%20of%20the%0Aexpected%20algorithm%20result%20on%20the%20eFPGA%20with%20perfect%20accuracy.%20Further%0Adevelopment%20of%20the%20eFPGA%20technology%20and%20its%20application%20to%20collider%20detector%0Areadout%20is%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17701v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520FPGA%2520Developments%2520in%2520130nm%2520and%252028nm%2520CMOS%2520for%2520Machine%2520Learning%250A%2520%2520in%2520Particle%2520Detector%2520Readout%26entry.906535625%3DJulia%2520Gonski%2520and%2520Aseem%2520Gupta%2520and%2520Haoyi%2520Jia%2520and%2520Hyunjoon%2520Kim%2520and%2520Lorenzo%2520Rota%2520and%2520Larry%2520Ruckman%2520and%2520Angelo%2520Dragone%2520and%2520Ryan%2520Herbst%26entry.1292438233%3D%2520%2520Embedded%2520field%2520programmable%2520gate%2520array%2520%2528eFPGA%2529%2520technology%2520allows%2520the%250Aimplementation%2520of%2520reconfigurable%2520logic%2520within%2520the%2520design%2520of%2520an%250Aapplication-specific%2520integrated%2520circuit%2520%2528ASIC%2529.%2520This%2520approach%2520offers%2520the%2520low%250Apower%2520and%2520efficiency%2520of%2520an%2520ASIC%2520along%2520with%2520the%2520ease%2520of%2520FPGA%2520configuration%252C%250Aparticularly%2520beneficial%2520for%2520the%2520use%2520case%2520of%2520machine%2520learning%2520in%2520the%2520data%250Apipeline%2520of%2520next-generation%2520collider%2520experiments.%2520An%2520open-source%2520framework%250Acalled%2520%2522FABulous%2522%2520was%2520used%2520to%2520design%2520eFPGAs%2520using%2520130%2520nm%2520and%252028%2520nm%2520CMOS%250Atechnology%2520nodes%252C%2520which%2520were%2520subsequently%2520fabricated%2520and%2520verified%2520through%250Atesting.%2520The%2520capability%2520of%2520an%2520eFPGA%2520to%2520act%2520as%2520a%2520front-end%2520readout%2520chip%2520was%250Aassessed%2520using%2520simulation%2520of%2520high%2520energy%2520particles%2520passing%2520through%2520a%2520silicon%250Apixel%2520sensor.%2520A%2520machine%2520learning-based%2520classifier%252C%2520designed%2520for%2520reduction%2520of%250Asensor%2520data%2520at%2520the%2520source%252C%2520was%2520synthesized%2520and%2520configured%2520onto%2520the%2520eFPGA.%2520A%250Asuccessful%2520proof-of-concept%2520was%2520demonstrated%2520through%2520reproduction%2520of%2520the%250Aexpected%2520algorithm%2520result%2520on%2520the%2520eFPGA%2520with%2520perfect%2520accuracy.%2520Further%250Adevelopment%2520of%2520the%2520eFPGA%2520technology%2520and%2520its%2520application%2520to%2520collider%2520detector%250Areadout%2520is%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17701v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20FPGA%20Developments%20in%20130nm%20and%2028nm%20CMOS%20for%20Machine%20Learning%0A%20%20in%20Particle%20Detector%20Readout&entry.906535625=Julia%20Gonski%20and%20Aseem%20Gupta%20and%20Haoyi%20Jia%20and%20Hyunjoon%20Kim%20and%20Lorenzo%20Rota%20and%20Larry%20Ruckman%20and%20Angelo%20Dragone%20and%20Ryan%20Herbst&entry.1292438233=%20%20Embedded%20field%20programmable%20gate%20array%20%28eFPGA%29%20technology%20allows%20the%0Aimplementation%20of%20reconfigurable%20logic%20within%20the%20design%20of%20an%0Aapplication-specific%20integrated%20circuit%20%28ASIC%29.%20This%20approach%20offers%20the%20low%0Apower%20and%20efficiency%20of%20an%20ASIC%20along%20with%20the%20ease%20of%20FPGA%20configuration%2C%0Aparticularly%20beneficial%20for%20the%20use%20case%20of%20machine%20learning%20in%20the%20data%0Apipeline%20of%20next-generation%20collider%20experiments.%20An%20open-source%20framework%0Acalled%20%22FABulous%22%20was%20used%20to%20design%20eFPGAs%20using%20130%20nm%20and%2028%20nm%20CMOS%0Atechnology%20nodes%2C%20which%20were%20subsequently%20fabricated%20and%20verified%20through%0Atesting.%20The%20capability%20of%20an%20eFPGA%20to%20act%20as%20a%20front-end%20readout%20chip%20was%0Aassessed%20using%20simulation%20of%20high%20energy%20particles%20passing%20through%20a%20silicon%0Apixel%20sensor.%20A%20machine%20learning-based%20classifier%2C%20designed%20for%20reduction%20of%0Asensor%20data%20at%20the%20source%2C%20was%20synthesized%20and%20configured%20onto%20the%20eFPGA.%20A%0Asuccessful%20proof-of-concept%20was%20demonstrated%20through%20reproduction%20of%20the%0Aexpected%20algorithm%20result%20on%20the%20eFPGA%20with%20perfect%20accuracy.%20Further%0Adevelopment%20of%20the%20eFPGA%20technology%20and%20its%20application%20to%20collider%20detector%0Areadout%20is%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17701v3&entry.124074799=Read"},
{"title": "Proceedings of the 2nd International Workshop on Adaptive Cyber Defense", "author": "Marco Carvalho and Damian Marriott and Mark Bilinski and Ahmad Ridley", "abstract": "  The 2nd International Workshop on Adaptive Cyber Defense was held at the\nFlorida Institute of Technology, Florida. This workshop was organized to share\nresearch that explores unique applications of Artificial Intelligence (AI) and\nMachine Learning (ML) as foundational capabilities for the pursuit of adaptive\ncyber defense. The cyber domain cannot currently be reliably and effectively\ndefended without extensive reliance on human experts. Skilled cyber defenders\nare in short supply and often cannot respond fast enough to cyber threats.\n  Building on recent advances in AI and ML the Cyber defense research community\nhas been motivated to develop new dynamic and sustainable defenses through the\nadoption of AI and ML techniques to cyber settings. Bridging critical gaps\nbetween AI and Cyber researchers and practitioners can accelerate efforts to\ncreate semi-autonomous cyber defenses that can learn to recognize and respond\nto cyber attacks or discover and mitigate weaknesses in cooperation with other\ncyber operation systems and human experts. Furthermore, these defenses are\nexpected to be adaptive and able to evolve over time to thwart changes in\nattacker behavior, changes in the system health and readiness, and natural\nshifts in user behavior over time.\n  The workshop was comprised of invited keynote talks, technical presentations\nand a panel discussion about how AI/ML can enable autonomous mitigation of\ncurrent and future cyber attacks. Workshop submissions were peer reviewed by a\npanel of domain experts with a proceedings consisting of six technical articles\nexploring challenging problems of critical importance to national and global\nsecurity. Participation in this workshop offered new opportunities to stimulate\nresearch and innovation in the emerging domain of adaptive and autonomous cyber\ndefense.\n", "link": "http://arxiv.org/abs/2308.09520v5", "date": "2024-07-02", "relevancy": 1.5269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3935}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3798}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proceedings%20of%20the%202nd%20International%20Workshop%20on%20Adaptive%20Cyber%20Defense&body=Title%3A%20Proceedings%20of%20the%202nd%20International%20Workshop%20on%20Adaptive%20Cyber%20Defense%0AAuthor%3A%20Marco%20Carvalho%20and%20Damian%20Marriott%20and%20Mark%20Bilinski%20and%20Ahmad%20Ridley%0AAbstract%3A%20%20%20The%202nd%20International%20Workshop%20on%20Adaptive%20Cyber%20Defense%20was%20held%20at%20the%0AFlorida%20Institute%20of%20Technology%2C%20Florida.%20This%20workshop%20was%20organized%20to%20share%0Aresearch%20that%20explores%20unique%20applications%20of%20Artificial%20Intelligence%20%28AI%29%20and%0AMachine%20Learning%20%28ML%29%20as%20foundational%20capabilities%20for%20the%20pursuit%20of%20adaptive%0Acyber%20defense.%20The%20cyber%20domain%20cannot%20currently%20be%20reliably%20and%20effectively%0Adefended%20without%20extensive%20reliance%20on%20human%20experts.%20Skilled%20cyber%20defenders%0Aare%20in%20short%20supply%20and%20often%20cannot%20respond%20fast%20enough%20to%20cyber%20threats.%0A%20%20Building%20on%20recent%20advances%20in%20AI%20and%20ML%20the%20Cyber%20defense%20research%20community%0Ahas%20been%20motivated%20to%20develop%20new%20dynamic%20and%20sustainable%20defenses%20through%20the%0Aadoption%20of%20AI%20and%20ML%20techniques%20to%20cyber%20settings.%20Bridging%20critical%20gaps%0Abetween%20AI%20and%20Cyber%20researchers%20and%20practitioners%20can%20accelerate%20efforts%20to%0Acreate%20semi-autonomous%20cyber%20defenses%20that%20can%20learn%20to%20recognize%20and%20respond%0Ato%20cyber%20attacks%20or%20discover%20and%20mitigate%20weaknesses%20in%20cooperation%20with%20other%0Acyber%20operation%20systems%20and%20human%20experts.%20Furthermore%2C%20these%20defenses%20are%0Aexpected%20to%20be%20adaptive%20and%20able%20to%20evolve%20over%20time%20to%20thwart%20changes%20in%0Aattacker%20behavior%2C%20changes%20in%20the%20system%20health%20and%20readiness%2C%20and%20natural%0Ashifts%20in%20user%20behavior%20over%20time.%0A%20%20The%20workshop%20was%20comprised%20of%20invited%20keynote%20talks%2C%20technical%20presentations%0Aand%20a%20panel%20discussion%20about%20how%20AI/ML%20can%20enable%20autonomous%20mitigation%20of%0Acurrent%20and%20future%20cyber%20attacks.%20Workshop%20submissions%20were%20peer%20reviewed%20by%20a%0Apanel%20of%20domain%20experts%20with%20a%20proceedings%20consisting%20of%20six%20technical%20articles%0Aexploring%20challenging%20problems%20of%20critical%20importance%20to%20national%20and%20global%0Asecurity.%20Participation%20in%20this%20workshop%20offered%20new%20opportunities%20to%20stimulate%0Aresearch%20and%20innovation%20in%20the%20emerging%20domain%20of%20adaptive%20and%20autonomous%20cyber%0Adefense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09520v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProceedings%2520of%2520the%25202nd%2520International%2520Workshop%2520on%2520Adaptive%2520Cyber%2520Defense%26entry.906535625%3DMarco%2520Carvalho%2520and%2520Damian%2520Marriott%2520and%2520Mark%2520Bilinski%2520and%2520Ahmad%2520Ridley%26entry.1292438233%3D%2520%2520The%25202nd%2520International%2520Workshop%2520on%2520Adaptive%2520Cyber%2520Defense%2520was%2520held%2520at%2520the%250AFlorida%2520Institute%2520of%2520Technology%252C%2520Florida.%2520This%2520workshop%2520was%2520organized%2520to%2520share%250Aresearch%2520that%2520explores%2520unique%2520applications%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%250AMachine%2520Learning%2520%2528ML%2529%2520as%2520foundational%2520capabilities%2520for%2520the%2520pursuit%2520of%2520adaptive%250Acyber%2520defense.%2520The%2520cyber%2520domain%2520cannot%2520currently%2520be%2520reliably%2520and%2520effectively%250Adefended%2520without%2520extensive%2520reliance%2520on%2520human%2520experts.%2520Skilled%2520cyber%2520defenders%250Aare%2520in%2520short%2520supply%2520and%2520often%2520cannot%2520respond%2520fast%2520enough%2520to%2520cyber%2520threats.%250A%2520%2520Building%2520on%2520recent%2520advances%2520in%2520AI%2520and%2520ML%2520the%2520Cyber%2520defense%2520research%2520community%250Ahas%2520been%2520motivated%2520to%2520develop%2520new%2520dynamic%2520and%2520sustainable%2520defenses%2520through%2520the%250Aadoption%2520of%2520AI%2520and%2520ML%2520techniques%2520to%2520cyber%2520settings.%2520Bridging%2520critical%2520gaps%250Abetween%2520AI%2520and%2520Cyber%2520researchers%2520and%2520practitioners%2520can%2520accelerate%2520efforts%2520to%250Acreate%2520semi-autonomous%2520cyber%2520defenses%2520that%2520can%2520learn%2520to%2520recognize%2520and%2520respond%250Ato%2520cyber%2520attacks%2520or%2520discover%2520and%2520mitigate%2520weaknesses%2520in%2520cooperation%2520with%2520other%250Acyber%2520operation%2520systems%2520and%2520human%2520experts.%2520Furthermore%252C%2520these%2520defenses%2520are%250Aexpected%2520to%2520be%2520adaptive%2520and%2520able%2520to%2520evolve%2520over%2520time%2520to%2520thwart%2520changes%2520in%250Aattacker%2520behavior%252C%2520changes%2520in%2520the%2520system%2520health%2520and%2520readiness%252C%2520and%2520natural%250Ashifts%2520in%2520user%2520behavior%2520over%2520time.%250A%2520%2520The%2520workshop%2520was%2520comprised%2520of%2520invited%2520keynote%2520talks%252C%2520technical%2520presentations%250Aand%2520a%2520panel%2520discussion%2520about%2520how%2520AI/ML%2520can%2520enable%2520autonomous%2520mitigation%2520of%250Acurrent%2520and%2520future%2520cyber%2520attacks.%2520Workshop%2520submissions%2520were%2520peer%2520reviewed%2520by%2520a%250Apanel%2520of%2520domain%2520experts%2520with%2520a%2520proceedings%2520consisting%2520of%2520six%2520technical%2520articles%250Aexploring%2520challenging%2520problems%2520of%2520critical%2520importance%2520to%2520national%2520and%2520global%250Asecurity.%2520Participation%2520in%2520this%2520workshop%2520offered%2520new%2520opportunities%2520to%2520stimulate%250Aresearch%2520and%2520innovation%2520in%2520the%2520emerging%2520domain%2520of%2520adaptive%2520and%2520autonomous%2520cyber%250Adefense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09520v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proceedings%20of%20the%202nd%20International%20Workshop%20on%20Adaptive%20Cyber%20Defense&entry.906535625=Marco%20Carvalho%20and%20Damian%20Marriott%20and%20Mark%20Bilinski%20and%20Ahmad%20Ridley&entry.1292438233=%20%20The%202nd%20International%20Workshop%20on%20Adaptive%20Cyber%20Defense%20was%20held%20at%20the%0AFlorida%20Institute%20of%20Technology%2C%20Florida.%20This%20workshop%20was%20organized%20to%20share%0Aresearch%20that%20explores%20unique%20applications%20of%20Artificial%20Intelligence%20%28AI%29%20and%0AMachine%20Learning%20%28ML%29%20as%20foundational%20capabilities%20for%20the%20pursuit%20of%20adaptive%0Acyber%20defense.%20The%20cyber%20domain%20cannot%20currently%20be%20reliably%20and%20effectively%0Adefended%20without%20extensive%20reliance%20on%20human%20experts.%20Skilled%20cyber%20defenders%0Aare%20in%20short%20supply%20and%20often%20cannot%20respond%20fast%20enough%20to%20cyber%20threats.%0A%20%20Building%20on%20recent%20advances%20in%20AI%20and%20ML%20the%20Cyber%20defense%20research%20community%0Ahas%20been%20motivated%20to%20develop%20new%20dynamic%20and%20sustainable%20defenses%20through%20the%0Aadoption%20of%20AI%20and%20ML%20techniques%20to%20cyber%20settings.%20Bridging%20critical%20gaps%0Abetween%20AI%20and%20Cyber%20researchers%20and%20practitioners%20can%20accelerate%20efforts%20to%0Acreate%20semi-autonomous%20cyber%20defenses%20that%20can%20learn%20to%20recognize%20and%20respond%0Ato%20cyber%20attacks%20or%20discover%20and%20mitigate%20weaknesses%20in%20cooperation%20with%20other%0Acyber%20operation%20systems%20and%20human%20experts.%20Furthermore%2C%20these%20defenses%20are%0Aexpected%20to%20be%20adaptive%20and%20able%20to%20evolve%20over%20time%20to%20thwart%20changes%20in%0Aattacker%20behavior%2C%20changes%20in%20the%20system%20health%20and%20readiness%2C%20and%20natural%0Ashifts%20in%20user%20behavior%20over%20time.%0A%20%20The%20workshop%20was%20comprised%20of%20invited%20keynote%20talks%2C%20technical%20presentations%0Aand%20a%20panel%20discussion%20about%20how%20AI/ML%20can%20enable%20autonomous%20mitigation%20of%0Acurrent%20and%20future%20cyber%20attacks.%20Workshop%20submissions%20were%20peer%20reviewed%20by%20a%0Apanel%20of%20domain%20experts%20with%20a%20proceedings%20consisting%20of%20six%20technical%20articles%0Aexploring%20challenging%20problems%20of%20critical%20importance%20to%20national%20and%20global%0Asecurity.%20Participation%20in%20this%20workshop%20offered%20new%20opportunities%20to%20stimulate%0Aresearch%20and%20innovation%20in%20the%20emerging%20domain%20of%20adaptive%20and%20autonomous%20cyber%0Adefense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09520v5&entry.124074799=Read"},
{"title": "Detecting Driver Fatigue With Eye Blink Behavior", "author": "Ali Akin and Habil Kalkan", "abstract": "  Traffic accidents, causing millions of deaths and billions of dollars in\neconomic losses each year globally, have become a significant issue. One of the\nmain causes of these accidents is drivers being sleepy or fatigued. Recently,\nvarious studies have focused on detecting drivers' sleep/wake states using\ncamera-based solutions that do not require physical contact with the driver,\nthereby enhancing ease of use. In this study, besides the eye blink frequency,\na driver adaptive eye blink behavior feature set have been evaluated to detect\nthe fatigue status. It is observed from the results that behavior of eye blink\ncarries useful information on fatigue detection. The developed image-based\nsystem provides a solution that can work adaptively to the physical\ncharacteristics of the drivers and their positions in the vehicle\n", "link": "http://arxiv.org/abs/2407.02222v1", "date": "2024-07-02", "relevancy": 1.7466, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.457}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4265}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Driver%20Fatigue%20With%20Eye%20Blink%20Behavior&body=Title%3A%20Detecting%20Driver%20Fatigue%20With%20Eye%20Blink%20Behavior%0AAuthor%3A%20Ali%20Akin%20and%20Habil%20Kalkan%0AAbstract%3A%20%20%20Traffic%20accidents%2C%20causing%20millions%20of%20deaths%20and%20billions%20of%20dollars%20in%0Aeconomic%20losses%20each%20year%20globally%2C%20have%20become%20a%20significant%20issue.%20One%20of%20the%0Amain%20causes%20of%20these%20accidents%20is%20drivers%20being%20sleepy%20or%20fatigued.%20Recently%2C%0Avarious%20studies%20have%20focused%20on%20detecting%20drivers%27%20sleep/wake%20states%20using%0Acamera-based%20solutions%20that%20do%20not%20require%20physical%20contact%20with%20the%20driver%2C%0Athereby%20enhancing%20ease%20of%20use.%20In%20this%20study%2C%20besides%20the%20eye%20blink%20frequency%2C%0Aa%20driver%20adaptive%20eye%20blink%20behavior%20feature%20set%20have%20been%20evaluated%20to%20detect%0Athe%20fatigue%20status.%20It%20is%20observed%20from%20the%20results%20that%20behavior%20of%20eye%20blink%0Acarries%20useful%20information%20on%20fatigue%20detection.%20The%20developed%20image-based%0Asystem%20provides%20a%20solution%20that%20can%20work%20adaptively%20to%20the%20physical%0Acharacteristics%20of%20the%20drivers%20and%20their%20positions%20in%20the%20vehicle%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Driver%2520Fatigue%2520With%2520Eye%2520Blink%2520Behavior%26entry.906535625%3DAli%2520Akin%2520and%2520Habil%2520Kalkan%26entry.1292438233%3D%2520%2520Traffic%2520accidents%252C%2520causing%2520millions%2520of%2520deaths%2520and%2520billions%2520of%2520dollars%2520in%250Aeconomic%2520losses%2520each%2520year%2520globally%252C%2520have%2520become%2520a%2520significant%2520issue.%2520One%2520of%2520the%250Amain%2520causes%2520of%2520these%2520accidents%2520is%2520drivers%2520being%2520sleepy%2520or%2520fatigued.%2520Recently%252C%250Avarious%2520studies%2520have%2520focused%2520on%2520detecting%2520drivers%2527%2520sleep/wake%2520states%2520using%250Acamera-based%2520solutions%2520that%2520do%2520not%2520require%2520physical%2520contact%2520with%2520the%2520driver%252C%250Athereby%2520enhancing%2520ease%2520of%2520use.%2520In%2520this%2520study%252C%2520besides%2520the%2520eye%2520blink%2520frequency%252C%250Aa%2520driver%2520adaptive%2520eye%2520blink%2520behavior%2520feature%2520set%2520have%2520been%2520evaluated%2520to%2520detect%250Athe%2520fatigue%2520status.%2520It%2520is%2520observed%2520from%2520the%2520results%2520that%2520behavior%2520of%2520eye%2520blink%250Acarries%2520useful%2520information%2520on%2520fatigue%2520detection.%2520The%2520developed%2520image-based%250Asystem%2520provides%2520a%2520solution%2520that%2520can%2520work%2520adaptively%2520to%2520the%2520physical%250Acharacteristics%2520of%2520the%2520drivers%2520and%2520their%2520positions%2520in%2520the%2520vehicle%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Driver%20Fatigue%20With%20Eye%20Blink%20Behavior&entry.906535625=Ali%20Akin%20and%20Habil%20Kalkan&entry.1292438233=%20%20Traffic%20accidents%2C%20causing%20millions%20of%20deaths%20and%20billions%20of%20dollars%20in%0Aeconomic%20losses%20each%20year%20globally%2C%20have%20become%20a%20significant%20issue.%20One%20of%20the%0Amain%20causes%20of%20these%20accidents%20is%20drivers%20being%20sleepy%20or%20fatigued.%20Recently%2C%0Avarious%20studies%20have%20focused%20on%20detecting%20drivers%27%20sleep/wake%20states%20using%0Acamera-based%20solutions%20that%20do%20not%20require%20physical%20contact%20with%20the%20driver%2C%0Athereby%20enhancing%20ease%20of%20use.%20In%20this%20study%2C%20besides%20the%20eye%20blink%20frequency%2C%0Aa%20driver%20adaptive%20eye%20blink%20behavior%20feature%20set%20have%20been%20evaluated%20to%20detect%0Athe%20fatigue%20status.%20It%20is%20observed%20from%20the%20results%20that%20behavior%20of%20eye%20blink%0Acarries%20useful%20information%20on%20fatigue%20detection.%20The%20developed%20image-based%0Asystem%20provides%20a%20solution%20that%20can%20work%20adaptively%20to%20the%20physical%0Acharacteristics%20of%20the%20drivers%20and%20their%20positions%20in%20the%20vehicle%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02222v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


