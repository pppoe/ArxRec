<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250730.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near\n  Real-Time 3DGS Optimization", "author": "Yiwei Xu and Yifei Yu and Wentian Gan and Tengfei Wang and Zongqian Zhan and Hao Cheng and Xin Wang", "abstract": "  3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nGaussian on-the-fly Splatting (abbreviated as On-the-Fly GS), a progressive\nframework enabling near real-time 3DGS optimization during image capture. As\neach image arrives, its pose and sparse points are updated via On-the-Fly SfM,\nand newly optimized Gaussians are immediately integrated into the 3DGS field.\nTo achieve this, we propose a progressive Local & Semi-Global optimization to\nprioritize the new image and its neighbors by their corresponding overlapping\nrelationship, allowing the new image and its overlapping images to get more\ntraining. To further stabilize training across previous and new images, an\nadaptive learning rate schedule balances the iterations and the learning rate.\nExtensive experiments on multiple benchmarks show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering one of the first practical steps toward rapid,\nprogressive 3DGS reconstruction.\n", "link": "http://arxiv.org/abs/2503.13086v2", "date": "2025-07-30", "relevancy": 3.591, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7714}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7077}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20On-the-Fly%20Splatting%3A%20A%20Progressive%20Framework%20for%20Robust%20Near%0A%20%20Real-Time%203DGS%20Optimization&body=Title%3A%20Gaussian%20On-the-Fly%20Splatting%3A%20A%20Progressive%20Framework%20for%20Robust%20Near%0A%20%20Real-Time%203DGS%20Optimization%0AAuthor%3A%20Yiwei%20Xu%20and%20Yifei%20Yu%20and%20Wentian%20Gan%20and%20Tengfei%20Wang%20and%20Zongqian%20Zhan%20and%20Hao%20Cheng%20and%20Xin%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20achieves%20high-fidelity%20rendering%20with%20fast%0Areal-time%20performance%2C%20but%20existing%20methods%20rely%20on%20offline%20training%20after%20full%0AStructure-from-Motion%20%28SfM%29%20processing.%20In%20contrast%2C%20this%20work%20introduces%0AGaussian%20on-the-fly%20Splatting%20%28abbreviated%20as%20On-the-Fly%20GS%29%2C%20a%20progressive%0Aframework%20enabling%20near%20real-time%203DGS%20optimization%20during%20image%20capture.%20As%0Aeach%20image%20arrives%2C%20its%20pose%20and%20sparse%20points%20are%20updated%20via%20On-the-Fly%20SfM%2C%0Aand%20newly%20optimized%20Gaussians%20are%20immediately%20integrated%20into%20the%203DGS%20field.%0ATo%20achieve%20this%2C%20we%20propose%20a%20progressive%20Local%20%26%20Semi-Global%20optimization%20to%0Aprioritize%20the%20new%20image%20and%20its%20neighbors%20by%20their%20corresponding%20overlapping%0Arelationship%2C%20allowing%20the%20new%20image%20and%20its%20overlapping%20images%20to%20get%20more%0Atraining.%20To%20further%20stabilize%20training%20across%20previous%20and%20new%20images%2C%20an%0Aadaptive%20learning%20rate%20schedule%20balances%20the%20iterations%20and%20the%20learning%20rate.%0AExtensive%20experiments%20on%20multiple%20benchmarks%20show%20that%20our%20On-the-Fly%20GS%0Areduces%20training%20time%20significantly%2C%20optimizing%20each%20new%20image%20in%20seconds%20with%0Aminimal%20rendering%20loss%2C%20offering%20one%20of%20the%20first%20practical%20steps%20toward%20rapid%2C%0Aprogressive%203DGS%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520On-the-Fly%2520Splatting%253A%2520A%2520Progressive%2520Framework%2520for%2520Robust%2520Near%250A%2520%2520Real-Time%25203DGS%2520Optimization%26entry.906535625%3DYiwei%2520Xu%2520and%2520Yifei%2520Yu%2520and%2520Wentian%2520Gan%2520and%2520Tengfei%2520Wang%2520and%2520Zongqian%2520Zhan%2520and%2520Hao%2520Cheng%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520achieves%2520high-fidelity%2520rendering%2520with%2520fast%250Areal-time%2520performance%252C%2520but%2520existing%2520methods%2520rely%2520on%2520offline%2520training%2520after%2520full%250AStructure-from-Motion%2520%2528SfM%2529%2520processing.%2520In%2520contrast%252C%2520this%2520work%2520introduces%250AGaussian%2520on-the-fly%2520Splatting%2520%2528abbreviated%2520as%2520On-the-Fly%2520GS%2529%252C%2520a%2520progressive%250Aframework%2520enabling%2520near%2520real-time%25203DGS%2520optimization%2520during%2520image%2520capture.%2520As%250Aeach%2520image%2520arrives%252C%2520its%2520pose%2520and%2520sparse%2520points%2520are%2520updated%2520via%2520On-the-Fly%2520SfM%252C%250Aand%2520newly%2520optimized%2520Gaussians%2520are%2520immediately%2520integrated%2520into%2520the%25203DGS%2520field.%250ATo%2520achieve%2520this%252C%2520we%2520propose%2520a%2520progressive%2520Local%2520%2526%2520Semi-Global%2520optimization%2520to%250Aprioritize%2520the%2520new%2520image%2520and%2520its%2520neighbors%2520by%2520their%2520corresponding%2520overlapping%250Arelationship%252C%2520allowing%2520the%2520new%2520image%2520and%2520its%2520overlapping%2520images%2520to%2520get%2520more%250Atraining.%2520To%2520further%2520stabilize%2520training%2520across%2520previous%2520and%2520new%2520images%252C%2520an%250Aadaptive%2520learning%2520rate%2520schedule%2520balances%2520the%2520iterations%2520and%2520the%2520learning%2520rate.%250AExtensive%2520experiments%2520on%2520multiple%2520benchmarks%2520show%2520that%2520our%2520On-the-Fly%2520GS%250Areduces%2520training%2520time%2520significantly%252C%2520optimizing%2520each%2520new%2520image%2520in%2520seconds%2520with%250Aminimal%2520rendering%2520loss%252C%2520offering%2520one%2520of%2520the%2520first%2520practical%2520steps%2520toward%2520rapid%252C%250Aprogressive%25203DGS%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20On-the-Fly%20Splatting%3A%20A%20Progressive%20Framework%20for%20Robust%20Near%0A%20%20Real-Time%203DGS%20Optimization&entry.906535625=Yiwei%20Xu%20and%20Yifei%20Yu%20and%20Wentian%20Gan%20and%20Tengfei%20Wang%20and%20Zongqian%20Zhan%20and%20Hao%20Cheng%20and%20Xin%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20achieves%20high-fidelity%20rendering%20with%20fast%0Areal-time%20performance%2C%20but%20existing%20methods%20rely%20on%20offline%20training%20after%20full%0AStructure-from-Motion%20%28SfM%29%20processing.%20In%20contrast%2C%20this%20work%20introduces%0AGaussian%20on-the-fly%20Splatting%20%28abbreviated%20as%20On-the-Fly%20GS%29%2C%20a%20progressive%0Aframework%20enabling%20near%20real-time%203DGS%20optimization%20during%20image%20capture.%20As%0Aeach%20image%20arrives%2C%20its%20pose%20and%20sparse%20points%20are%20updated%20via%20On-the-Fly%20SfM%2C%0Aand%20newly%20optimized%20Gaussians%20are%20immediately%20integrated%20into%20the%203DGS%20field.%0ATo%20achieve%20this%2C%20we%20propose%20a%20progressive%20Local%20%26%20Semi-Global%20optimization%20to%0Aprioritize%20the%20new%20image%20and%20its%20neighbors%20by%20their%20corresponding%20overlapping%0Arelationship%2C%20allowing%20the%20new%20image%20and%20its%20overlapping%20images%20to%20get%20more%0Atraining.%20To%20further%20stabilize%20training%20across%20previous%20and%20new%20images%2C%20an%0Aadaptive%20learning%20rate%20schedule%20balances%20the%20iterations%20and%20the%20learning%20rate.%0AExtensive%20experiments%20on%20multiple%20benchmarks%20show%20that%20our%20On-the-Fly%20GS%0Areduces%20training%20time%20significantly%2C%20optimizing%20each%20new%20image%20in%20seconds%20with%0Aminimal%20rendering%20loss%2C%20offering%20one%20of%20the%20first%20practical%20steps%20toward%20rapid%2C%0Aprogressive%203DGS%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13086v2&entry.124074799=Read"},
{"title": "Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale\n  Super-Resolution", "author": "Du Chen and Liyi Chen and Zhengqiang Zhang and Lei Zhang", "abstract": "  Implicit Neural Representations (INR) have been successfully employed for\nArbitrary-scale Super-Resolution (ASR). However, INR-based models need to query\nthe multi-layer perceptron module numerous times and render a pixel in each\nquery, resulting in insufficient representation capability and low\ncomputational efficiency. Recently, Gaussian Splatting (GS) has shown its\nadvantages over INR in both visual quality and rendering speed in 3D tasks,\nwhich motivates us to explore whether GS can be employed for the ASR task.\nHowever, directly applying GS to ASR is exceptionally challenging because the\noriginal GS is an optimization-based method through overfitting each single\nscene, while in ASR we aim to learn a single model that can generalize to\ndifferent images and scaling factors. We overcome these challenges by\ndeveloping two novel techniques. Firstly, to generalize GS for ASR, we\nelaborately design an architecture to predict the corresponding\nimage-conditioned Gaussians of the input low-resolution image in a feed-forward\nmanner. Each Gaussian can fit the shape and direction of an area of complex\ntextures, showing powerful representation capability. Secondly, we implement an\nefficient differentiable 2D GPU/CUDA-based scale-aware rasterization to render\nsuper-resolved images by sampling discrete RGB values from the predicted\ncontinuous Gaussians. Via end-to-end training, our optimized network, namely\nGSASR, can perform ASR for any image and unseen scaling factors. Extensive\nexperiments validate the effectiveness of our proposed method. The code and\nmodels are available at https://github.com/ChrisDud0257/GSASR.\n", "link": "http://arxiv.org/abs/2501.06838v5", "date": "2025-07-30", "relevancy": 3.3331, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7097}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.646}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20and%20Efficient%202D%20Gaussian%20Splatting%20for%20Arbitrary-scale%0A%20%20Super-Resolution&body=Title%3A%20Generalized%20and%20Efficient%202D%20Gaussian%20Splatting%20for%20Arbitrary-scale%0A%20%20Super-Resolution%0AAuthor%3A%20Du%20Chen%20and%20Liyi%20Chen%20and%20Zhengqiang%20Zhang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INR%29%20have%20been%20successfully%20employed%20for%0AArbitrary-scale%20Super-Resolution%20%28ASR%29.%20However%2C%20INR-based%20models%20need%20to%20query%0Athe%20multi-layer%20perceptron%20module%20numerous%20times%20and%20render%20a%20pixel%20in%20each%0Aquery%2C%20resulting%20in%20insufficient%20representation%20capability%20and%20low%0Acomputational%20efficiency.%20Recently%2C%20Gaussian%20Splatting%20%28GS%29%20has%20shown%20its%0Aadvantages%20over%20INR%20in%20both%20visual%20quality%20and%20rendering%20speed%20in%203D%20tasks%2C%0Awhich%20motivates%20us%20to%20explore%20whether%20GS%20can%20be%20employed%20for%20the%20ASR%20task.%0AHowever%2C%20directly%20applying%20GS%20to%20ASR%20is%20exceptionally%20challenging%20because%20the%0Aoriginal%20GS%20is%20an%20optimization-based%20method%20through%20overfitting%20each%20single%0Ascene%2C%20while%20in%20ASR%20we%20aim%20to%20learn%20a%20single%20model%20that%20can%20generalize%20to%0Adifferent%20images%20and%20scaling%20factors.%20We%20overcome%20these%20challenges%20by%0Adeveloping%20two%20novel%20techniques.%20Firstly%2C%20to%20generalize%20GS%20for%20ASR%2C%20we%0Aelaborately%20design%20an%20architecture%20to%20predict%20the%20corresponding%0Aimage-conditioned%20Gaussians%20of%20the%20input%20low-resolution%20image%20in%20a%20feed-forward%0Amanner.%20Each%20Gaussian%20can%20fit%20the%20shape%20and%20direction%20of%20an%20area%20of%20complex%0Atextures%2C%20showing%20powerful%20representation%20capability.%20Secondly%2C%20we%20implement%20an%0Aefficient%20differentiable%202D%20GPU/CUDA-based%20scale-aware%20rasterization%20to%20render%0Asuper-resolved%20images%20by%20sampling%20discrete%20RGB%20values%20from%20the%20predicted%0Acontinuous%20Gaussians.%20Via%20end-to-end%20training%2C%20our%20optimized%20network%2C%20namely%0AGSASR%2C%20can%20perform%20ASR%20for%20any%20image%20and%20unseen%20scaling%20factors.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20proposed%20method.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/ChrisDud0257/GSASR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06838v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520and%2520Efficient%25202D%2520Gaussian%2520Splatting%2520for%2520Arbitrary-scale%250A%2520%2520Super-Resolution%26entry.906535625%3DDu%2520Chen%2520and%2520Liyi%2520Chen%2520and%2520Zhengqiang%2520Zhang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INR%2529%2520have%2520been%2520successfully%2520employed%2520for%250AArbitrary-scale%2520Super-Resolution%2520%2528ASR%2529.%2520However%252C%2520INR-based%2520models%2520need%2520to%2520query%250Athe%2520multi-layer%2520perceptron%2520module%2520numerous%2520times%2520and%2520render%2520a%2520pixel%2520in%2520each%250Aquery%252C%2520resulting%2520in%2520insufficient%2520representation%2520capability%2520and%2520low%250Acomputational%2520efficiency.%2520Recently%252C%2520Gaussian%2520Splatting%2520%2528GS%2529%2520has%2520shown%2520its%250Aadvantages%2520over%2520INR%2520in%2520both%2520visual%2520quality%2520and%2520rendering%2520speed%2520in%25203D%2520tasks%252C%250Awhich%2520motivates%2520us%2520to%2520explore%2520whether%2520GS%2520can%2520be%2520employed%2520for%2520the%2520ASR%2520task.%250AHowever%252C%2520directly%2520applying%2520GS%2520to%2520ASR%2520is%2520exceptionally%2520challenging%2520because%2520the%250Aoriginal%2520GS%2520is%2520an%2520optimization-based%2520method%2520through%2520overfitting%2520each%2520single%250Ascene%252C%2520while%2520in%2520ASR%2520we%2520aim%2520to%2520learn%2520a%2520single%2520model%2520that%2520can%2520generalize%2520to%250Adifferent%2520images%2520and%2520scaling%2520factors.%2520We%2520overcome%2520these%2520challenges%2520by%250Adeveloping%2520two%2520novel%2520techniques.%2520Firstly%252C%2520to%2520generalize%2520GS%2520for%2520ASR%252C%2520we%250Aelaborately%2520design%2520an%2520architecture%2520to%2520predict%2520the%2520corresponding%250Aimage-conditioned%2520Gaussians%2520of%2520the%2520input%2520low-resolution%2520image%2520in%2520a%2520feed-forward%250Amanner.%2520Each%2520Gaussian%2520can%2520fit%2520the%2520shape%2520and%2520direction%2520of%2520an%2520area%2520of%2520complex%250Atextures%252C%2520showing%2520powerful%2520representation%2520capability.%2520Secondly%252C%2520we%2520implement%2520an%250Aefficient%2520differentiable%25202D%2520GPU/CUDA-based%2520scale-aware%2520rasterization%2520to%2520render%250Asuper-resolved%2520images%2520by%2520sampling%2520discrete%2520RGB%2520values%2520from%2520the%2520predicted%250Acontinuous%2520Gaussians.%2520Via%2520end-to-end%2520training%252C%2520our%2520optimized%2520network%252C%2520namely%250AGSASR%252C%2520can%2520perform%2520ASR%2520for%2520any%2520image%2520and%2520unseen%2520scaling%2520factors.%2520Extensive%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%2520The%2520code%2520and%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/ChrisDud0257/GSASR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06838v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20and%20Efficient%202D%20Gaussian%20Splatting%20for%20Arbitrary-scale%0A%20%20Super-Resolution&entry.906535625=Du%20Chen%20and%20Liyi%20Chen%20and%20Zhengqiang%20Zhang%20and%20Lei%20Zhang&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INR%29%20have%20been%20successfully%20employed%20for%0AArbitrary-scale%20Super-Resolution%20%28ASR%29.%20However%2C%20INR-based%20models%20need%20to%20query%0Athe%20multi-layer%20perceptron%20module%20numerous%20times%20and%20render%20a%20pixel%20in%20each%0Aquery%2C%20resulting%20in%20insufficient%20representation%20capability%20and%20low%0Acomputational%20efficiency.%20Recently%2C%20Gaussian%20Splatting%20%28GS%29%20has%20shown%20its%0Aadvantages%20over%20INR%20in%20both%20visual%20quality%20and%20rendering%20speed%20in%203D%20tasks%2C%0Awhich%20motivates%20us%20to%20explore%20whether%20GS%20can%20be%20employed%20for%20the%20ASR%20task.%0AHowever%2C%20directly%20applying%20GS%20to%20ASR%20is%20exceptionally%20challenging%20because%20the%0Aoriginal%20GS%20is%20an%20optimization-based%20method%20through%20overfitting%20each%20single%0Ascene%2C%20while%20in%20ASR%20we%20aim%20to%20learn%20a%20single%20model%20that%20can%20generalize%20to%0Adifferent%20images%20and%20scaling%20factors.%20We%20overcome%20these%20challenges%20by%0Adeveloping%20two%20novel%20techniques.%20Firstly%2C%20to%20generalize%20GS%20for%20ASR%2C%20we%0Aelaborately%20design%20an%20architecture%20to%20predict%20the%20corresponding%0Aimage-conditioned%20Gaussians%20of%20the%20input%20low-resolution%20image%20in%20a%20feed-forward%0Amanner.%20Each%20Gaussian%20can%20fit%20the%20shape%20and%20direction%20of%20an%20area%20of%20complex%0Atextures%2C%20showing%20powerful%20representation%20capability.%20Secondly%2C%20we%20implement%20an%0Aefficient%20differentiable%202D%20GPU/CUDA-based%20scale-aware%20rasterization%20to%20render%0Asuper-resolved%20images%20by%20sampling%20discrete%20RGB%20values%20from%20the%20predicted%0Acontinuous%20Gaussians.%20Via%20end-to-end%20training%2C%20our%20optimized%20network%2C%20namely%0AGSASR%2C%20can%20perform%20ASR%20for%20any%20image%20and%20unseen%20scaling%20factors.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20proposed%20method.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/ChrisDud0257/GSASR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06838v5&entry.124074799=Read"},
{"title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous\n  Driving with Gaussian Splatting", "author": "Baijun Ye and Minghui Qin and Saining Zhang and Moonjun Gong and Shaoting Zhu and Zebang Shen and Luan Zhang and Lu Zhang and Hao Zhao and Hang Zhao", "abstract": "  Occupancy is crucial for autonomous driving, providing essential geometric\npriors for perception and planning. However, existing methods predominantly\nrely on LiDAR-based occupancy annotations, which limits scalability and\nprevents leveraging vast amounts of potential crowdsourced data for\nauto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only\nframework that directly reconstructs occupancy. Vision-only occupancy\nreconstruction poses significant challenges due to sparse viewpoints, dynamic\nscene elements, severe occlusions, and long-horizon motion. Existing\nvision-based methods primarily rely on mesh representation, which suffer from\nincomplete geometry and additional post-processing, limiting scalability. To\novercome these issues, GS-Occ3D optimizes an explicit occupancy representation\nusing an Octree-based Gaussian Surfel formulation, ensuring efficiency and\nscalability. Additionally, we decompose scenes into static background, ground,\nand dynamic objects, enabling tailored modeling strategies: (1) Ground is\nexplicitly reconstructed as a dominant structural element, significantly\nimproving large-area consistency; (2) Dynamic vehicles are separately modeled\nto better capture motion-related occupancy patterns. Extensive experiments on\nthe Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry\nreconstruction results. By curating vision-only binary occupancy labels from\ndiverse urban scenes, we show their effectiveness for downstream occupancy\nmodels on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.\nIt highlights the potential of large-scale vision-based occupancy\nreconstruction as a new paradigm for scalable auto-labeling. Project Page:\nhttps://gs-occ3d.github.io/\n", "link": "http://arxiv.org/abs/2507.19451v2", "date": "2025-07-30", "relevancy": 3.269, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6851}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6416}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Occ3D%3A%20Scaling%20Vision-only%20Occupancy%20Reconstruction%20for%20Autonomous%0A%20%20Driving%20with%20Gaussian%20Splatting&body=Title%3A%20GS-Occ3D%3A%20Scaling%20Vision-only%20Occupancy%20Reconstruction%20for%20Autonomous%0A%20%20Driving%20with%20Gaussian%20Splatting%0AAuthor%3A%20Baijun%20Ye%20and%20Minghui%20Qin%20and%20Saining%20Zhang%20and%20Moonjun%20Gong%20and%20Shaoting%20Zhu%20and%20Zebang%20Shen%20and%20Luan%20Zhang%20and%20Lu%20Zhang%20and%20Hao%20Zhao%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20Occupancy%20is%20crucial%20for%20autonomous%20driving%2C%20providing%20essential%20geometric%0Apriors%20for%20perception%20and%20planning.%20However%2C%20existing%20methods%20predominantly%0Arely%20on%20LiDAR-based%20occupancy%20annotations%2C%20which%20limits%20scalability%20and%0Aprevents%20leveraging%20vast%20amounts%20of%20potential%20crowdsourced%20data%20for%0Aauto-labeling.%20To%20address%20this%2C%20we%20propose%20GS-Occ3D%2C%20a%20scalable%20vision-only%0Aframework%20that%20directly%20reconstructs%20occupancy.%20Vision-only%20occupancy%0Areconstruction%20poses%20significant%20challenges%20due%20to%20sparse%20viewpoints%2C%20dynamic%0Ascene%20elements%2C%20severe%20occlusions%2C%20and%20long-horizon%20motion.%20Existing%0Avision-based%20methods%20primarily%20rely%20on%20mesh%20representation%2C%20which%20suffer%20from%0Aincomplete%20geometry%20and%20additional%20post-processing%2C%20limiting%20scalability.%20To%0Aovercome%20these%20issues%2C%20GS-Occ3D%20optimizes%20an%20explicit%20occupancy%20representation%0Ausing%20an%20Octree-based%20Gaussian%20Surfel%20formulation%2C%20ensuring%20efficiency%20and%0Ascalability.%20Additionally%2C%20we%20decompose%20scenes%20into%20static%20background%2C%20ground%2C%0Aand%20dynamic%20objects%2C%20enabling%20tailored%20modeling%20strategies%3A%20%281%29%20Ground%20is%0Aexplicitly%20reconstructed%20as%20a%20dominant%20structural%20element%2C%20significantly%0Aimproving%20large-area%20consistency%3B%20%282%29%20Dynamic%20vehicles%20are%20separately%20modeled%0Ato%20better%20capture%20motion-related%20occupancy%20patterns.%20Extensive%20experiments%20on%0Athe%20Waymo%20dataset%20demonstrate%20that%20GS-Occ3D%20achieves%20state-of-the-art%20geometry%0Areconstruction%20results.%20By%20curating%20vision-only%20binary%20occupancy%20labels%20from%0Adiverse%20urban%20scenes%2C%20we%20show%20their%20effectiveness%20for%20downstream%20occupancy%0Amodels%20on%20Occ3D-Waymo%20and%20superior%20zero-shot%20generalization%20on%20Occ3D-nuScenes.%0AIt%20highlights%20the%20potential%20of%20large-scale%20vision-based%20occupancy%0Areconstruction%20as%20a%20new%20paradigm%20for%20scalable%20auto-labeling.%20Project%20Page%3A%0Ahttps%3A//gs-occ3d.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Occ3D%253A%2520Scaling%2520Vision-only%2520Occupancy%2520Reconstruction%2520for%2520Autonomous%250A%2520%2520Driving%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DBaijun%2520Ye%2520and%2520Minghui%2520Qin%2520and%2520Saining%2520Zhang%2520and%2520Moonjun%2520Gong%2520and%2520Shaoting%2520Zhu%2520and%2520Zebang%2520Shen%2520and%2520Luan%2520Zhang%2520and%2520Lu%2520Zhang%2520and%2520Hao%2520Zhao%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520Occupancy%2520is%2520crucial%2520for%2520autonomous%2520driving%252C%2520providing%2520essential%2520geometric%250Apriors%2520for%2520perception%2520and%2520planning.%2520However%252C%2520existing%2520methods%2520predominantly%250Arely%2520on%2520LiDAR-based%2520occupancy%2520annotations%252C%2520which%2520limits%2520scalability%2520and%250Aprevents%2520leveraging%2520vast%2520amounts%2520of%2520potential%2520crowdsourced%2520data%2520for%250Aauto-labeling.%2520To%2520address%2520this%252C%2520we%2520propose%2520GS-Occ3D%252C%2520a%2520scalable%2520vision-only%250Aframework%2520that%2520directly%2520reconstructs%2520occupancy.%2520Vision-only%2520occupancy%250Areconstruction%2520poses%2520significant%2520challenges%2520due%2520to%2520sparse%2520viewpoints%252C%2520dynamic%250Ascene%2520elements%252C%2520severe%2520occlusions%252C%2520and%2520long-horizon%2520motion.%2520Existing%250Avision-based%2520methods%2520primarily%2520rely%2520on%2520mesh%2520representation%252C%2520which%2520suffer%2520from%250Aincomplete%2520geometry%2520and%2520additional%2520post-processing%252C%2520limiting%2520scalability.%2520To%250Aovercome%2520these%2520issues%252C%2520GS-Occ3D%2520optimizes%2520an%2520explicit%2520occupancy%2520representation%250Ausing%2520an%2520Octree-based%2520Gaussian%2520Surfel%2520formulation%252C%2520ensuring%2520efficiency%2520and%250Ascalability.%2520Additionally%252C%2520we%2520decompose%2520scenes%2520into%2520static%2520background%252C%2520ground%252C%250Aand%2520dynamic%2520objects%252C%2520enabling%2520tailored%2520modeling%2520strategies%253A%2520%25281%2529%2520Ground%2520is%250Aexplicitly%2520reconstructed%2520as%2520a%2520dominant%2520structural%2520element%252C%2520significantly%250Aimproving%2520large-area%2520consistency%253B%2520%25282%2529%2520Dynamic%2520vehicles%2520are%2520separately%2520modeled%250Ato%2520better%2520capture%2520motion-related%2520occupancy%2520patterns.%2520Extensive%2520experiments%2520on%250Athe%2520Waymo%2520dataset%2520demonstrate%2520that%2520GS-Occ3D%2520achieves%2520state-of-the-art%2520geometry%250Areconstruction%2520results.%2520By%2520curating%2520vision-only%2520binary%2520occupancy%2520labels%2520from%250Adiverse%2520urban%2520scenes%252C%2520we%2520show%2520their%2520effectiveness%2520for%2520downstream%2520occupancy%250Amodels%2520on%2520Occ3D-Waymo%2520and%2520superior%2520zero-shot%2520generalization%2520on%2520Occ3D-nuScenes.%250AIt%2520highlights%2520the%2520potential%2520of%2520large-scale%2520vision-based%2520occupancy%250Areconstruction%2520as%2520a%2520new%2520paradigm%2520for%2520scalable%2520auto-labeling.%2520Project%2520Page%253A%250Ahttps%253A//gs-occ3d.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Occ3D%3A%20Scaling%20Vision-only%20Occupancy%20Reconstruction%20for%20Autonomous%0A%20%20Driving%20with%20Gaussian%20Splatting&entry.906535625=Baijun%20Ye%20and%20Minghui%20Qin%20and%20Saining%20Zhang%20and%20Moonjun%20Gong%20and%20Shaoting%20Zhu%20and%20Zebang%20Shen%20and%20Luan%20Zhang%20and%20Lu%20Zhang%20and%20Hao%20Zhao%20and%20Hang%20Zhao&entry.1292438233=%20%20Occupancy%20is%20crucial%20for%20autonomous%20driving%2C%20providing%20essential%20geometric%0Apriors%20for%20perception%20and%20planning.%20However%2C%20existing%20methods%20predominantly%0Arely%20on%20LiDAR-based%20occupancy%20annotations%2C%20which%20limits%20scalability%20and%0Aprevents%20leveraging%20vast%20amounts%20of%20potential%20crowdsourced%20data%20for%0Aauto-labeling.%20To%20address%20this%2C%20we%20propose%20GS-Occ3D%2C%20a%20scalable%20vision-only%0Aframework%20that%20directly%20reconstructs%20occupancy.%20Vision-only%20occupancy%0Areconstruction%20poses%20significant%20challenges%20due%20to%20sparse%20viewpoints%2C%20dynamic%0Ascene%20elements%2C%20severe%20occlusions%2C%20and%20long-horizon%20motion.%20Existing%0Avision-based%20methods%20primarily%20rely%20on%20mesh%20representation%2C%20which%20suffer%20from%0Aincomplete%20geometry%20and%20additional%20post-processing%2C%20limiting%20scalability.%20To%0Aovercome%20these%20issues%2C%20GS-Occ3D%20optimizes%20an%20explicit%20occupancy%20representation%0Ausing%20an%20Octree-based%20Gaussian%20Surfel%20formulation%2C%20ensuring%20efficiency%20and%0Ascalability.%20Additionally%2C%20we%20decompose%20scenes%20into%20static%20background%2C%20ground%2C%0Aand%20dynamic%20objects%2C%20enabling%20tailored%20modeling%20strategies%3A%20%281%29%20Ground%20is%0Aexplicitly%20reconstructed%20as%20a%20dominant%20structural%20element%2C%20significantly%0Aimproving%20large-area%20consistency%3B%20%282%29%20Dynamic%20vehicles%20are%20separately%20modeled%0Ato%20better%20capture%20motion-related%20occupancy%20patterns.%20Extensive%20experiments%20on%0Athe%20Waymo%20dataset%20demonstrate%20that%20GS-Occ3D%20achieves%20state-of-the-art%20geometry%0Areconstruction%20results.%20By%20curating%20vision-only%20binary%20occupancy%20labels%20from%0Adiverse%20urban%20scenes%2C%20we%20show%20their%20effectiveness%20for%20downstream%20occupancy%0Amodels%20on%20Occ3D-Waymo%20and%20superior%20zero-shot%20generalization%20on%20Occ3D-nuScenes.%0AIt%20highlights%20the%20potential%20of%20large-scale%20vision-based%20occupancy%0Areconstruction%20as%20a%20new%20paradigm%20for%20scalable%20auto-labeling.%20Project%20Page%3A%0Ahttps%3A//gs-occ3d.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19451v2&entry.124074799=Read"},
{"title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention", "author": "Yiwen Chen and Zhihao Li and Yikai Wang and Hu Zhang and Qin Li and Chi Zhang and Guosheng Lin", "abstract": "  Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.\n", "link": "http://arxiv.org/abs/2507.17745v2", "date": "2025-07-30", "relevancy": 3.1257, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6322}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention&body=Title%3A%20Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention%0AAuthor%3A%20Yiwen%20Chen%20and%20Zhihao%20Li%20and%20Yikai%20Wang%20and%20Hu%20Zhang%20and%20Qin%20Li%20and%20Chi%20Zhang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Recent%20advances%20in%20sparse%20voxel%20representations%20have%20significantly%20improved%0Athe%20quality%20of%203D%20content%20generation%2C%20enabling%20high-resolution%20modeling%20with%0Afine-grained%20geometry.%20However%2C%20existing%20frameworks%20suffer%20from%20severe%0Acomputational%20inefficiencies%20due%20to%20the%20quadratic%20complexity%20of%20attention%0Amechanisms%20in%20their%20two-stage%20diffusion%20pipelines.%20In%20this%20work%2C%20we%20propose%0AUltra3D%2C%20an%20efficient%203D%20generation%20framework%20that%20significantly%20accelerates%0Asparse%20voxel%20modeling%20without%20compromising%20quality.%20Our%20method%20leverages%20the%0Acompact%20VecSet%20representation%20to%20efficiently%20generate%20a%20coarse%20object%20layout%20in%0Athe%20first%20stage%2C%20reducing%20token%20count%20and%20accelerating%20voxel%20coordinate%0Aprediction.%20To%20refine%20per-voxel%20latent%20features%20in%20the%20second%20stage%2C%20we%0Aintroduce%20Part%20Attention%2C%20a%20geometry-aware%20localized%20attention%20mechanism%20that%0Arestricts%20attention%20computation%20within%20semantically%20consistent%20part%20regions.%0AThis%20design%20preserves%20structural%20continuity%20while%20avoiding%20unnecessary%20global%0Aattention%2C%20achieving%20up%20to%206.7x%20speed-up%20in%20latent%20generation.%20To%20support%20this%0Amechanism%2C%20we%20construct%20a%20scalable%20part%20annotation%20pipeline%20that%20converts%20raw%0Ameshes%20into%20part-labeled%20sparse%20voxels.%20Extensive%20experiments%20demonstrate%20that%0AUltra3D%20supports%20high-resolution%203D%20generation%20at%201024%20resolution%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20visual%20fidelity%20and%20user%20preference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra3D%253A%2520Efficient%2520and%2520High-Fidelity%25203D%2520Generation%2520with%2520Part%2520Attention%26entry.906535625%3DYiwen%2520Chen%2520and%2520Zhihao%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Hu%2520Zhang%2520and%2520Qin%2520Li%2520and%2520Chi%2520Zhang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520sparse%2520voxel%2520representations%2520have%2520significantly%2520improved%250Athe%2520quality%2520of%25203D%2520content%2520generation%252C%2520enabling%2520high-resolution%2520modeling%2520with%250Afine-grained%2520geometry.%2520However%252C%2520existing%2520frameworks%2520suffer%2520from%2520severe%250Acomputational%2520inefficiencies%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520attention%250Amechanisms%2520in%2520their%2520two-stage%2520diffusion%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520propose%250AUltra3D%252C%2520an%2520efficient%25203D%2520generation%2520framework%2520that%2520significantly%2520accelerates%250Asparse%2520voxel%2520modeling%2520without%2520compromising%2520quality.%2520Our%2520method%2520leverages%2520the%250Acompact%2520VecSet%2520representation%2520to%2520efficiently%2520generate%2520a%2520coarse%2520object%2520layout%2520in%250Athe%2520first%2520stage%252C%2520reducing%2520token%2520count%2520and%2520accelerating%2520voxel%2520coordinate%250Aprediction.%2520To%2520refine%2520per-voxel%2520latent%2520features%2520in%2520the%2520second%2520stage%252C%2520we%250Aintroduce%2520Part%2520Attention%252C%2520a%2520geometry-aware%2520localized%2520attention%2520mechanism%2520that%250Arestricts%2520attention%2520computation%2520within%2520semantically%2520consistent%2520part%2520regions.%250AThis%2520design%2520preserves%2520structural%2520continuity%2520while%2520avoiding%2520unnecessary%2520global%250Aattention%252C%2520achieving%2520up%2520to%25206.7x%2520speed-up%2520in%2520latent%2520generation.%2520To%2520support%2520this%250Amechanism%252C%2520we%2520construct%2520a%2520scalable%2520part%2520annotation%2520pipeline%2520that%2520converts%2520raw%250Ameshes%2520into%2520part-labeled%2520sparse%2520voxels.%2520Extensive%2520experiments%2520demonstrate%2520that%250AUltra3D%2520supports%2520high-resolution%25203D%2520generation%2520at%25201024%2520resolution%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520visual%2520fidelity%2520and%2520user%2520preference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention&entry.906535625=Yiwen%20Chen%20and%20Zhihao%20Li%20and%20Yikai%20Wang%20and%20Hu%20Zhang%20and%20Qin%20Li%20and%20Chi%20Zhang%20and%20Guosheng%20Lin&entry.1292438233=%20%20Recent%20advances%20in%20sparse%20voxel%20representations%20have%20significantly%20improved%0Athe%20quality%20of%203D%20content%20generation%2C%20enabling%20high-resolution%20modeling%20with%0Afine-grained%20geometry.%20However%2C%20existing%20frameworks%20suffer%20from%20severe%0Acomputational%20inefficiencies%20due%20to%20the%20quadratic%20complexity%20of%20attention%0Amechanisms%20in%20their%20two-stage%20diffusion%20pipelines.%20In%20this%20work%2C%20we%20propose%0AUltra3D%2C%20an%20efficient%203D%20generation%20framework%20that%20significantly%20accelerates%0Asparse%20voxel%20modeling%20without%20compromising%20quality.%20Our%20method%20leverages%20the%0Acompact%20VecSet%20representation%20to%20efficiently%20generate%20a%20coarse%20object%20layout%20in%0Athe%20first%20stage%2C%20reducing%20token%20count%20and%20accelerating%20voxel%20coordinate%0Aprediction.%20To%20refine%20per-voxel%20latent%20features%20in%20the%20second%20stage%2C%20we%0Aintroduce%20Part%20Attention%2C%20a%20geometry-aware%20localized%20attention%20mechanism%20that%0Arestricts%20attention%20computation%20within%20semantically%20consistent%20part%20regions.%0AThis%20design%20preserves%20structural%20continuity%20while%20avoiding%20unnecessary%20global%0Aattention%2C%20achieving%20up%20to%206.7x%20speed-up%20in%20latent%20generation.%20To%20support%20this%0Amechanism%2C%20we%20construct%20a%20scalable%20part%20annotation%20pipeline%20that%20converts%20raw%0Ameshes%20into%20part-labeled%20sparse%20voxels.%20Extensive%20experiments%20demonstrate%20that%0AUltra3D%20supports%20high-resolution%203D%20generation%20at%201024%20resolution%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20visual%20fidelity%20and%20user%20preference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17745v2&entry.124074799=Read"},
{"title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and\n  Hierarchical Group Attention", "author": "Yuqi Pang and Bowen Yang and Yun Cao and Fan Rong and Xiaoyu Li and Chen He", "abstract": "  Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.\n", "link": "http://arxiv.org/abs/2507.22805v1", "date": "2025-07-30", "relevancy": 3.1133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoCHA%3A%20Advanced%20Vision-Language%20Reasoning%20with%20MoE%20Connector%20and%0A%20%20Hierarchical%20Group%20Attention&body=Title%3A%20MoCHA%3A%20Advanced%20Vision-Language%20Reasoning%20with%20MoE%20Connector%20and%0A%20%20Hierarchical%20Group%20Attention%0AAuthor%3A%20Yuqi%20Pang%20and%20Bowen%20Yang%20and%20Yun%20Cao%20and%20Fan%20Rong%20and%20Xiaoyu%20Li%20and%20Chen%20He%0AAbstract%3A%20%20%20Vision%20large%20language%20models%20%28VLLMs%29%20are%20focusing%20primarily%20on%20handling%0Acomplex%20and%20fine-grained%20visual%20information%20by%20incorporating%20advanced%20vision%0Aencoders%20and%20scaling%20up%20visual%20models.%20However%2C%20these%20approaches%20face%20high%0Atraining%20and%20inference%20costs%2C%20as%20well%20as%20challenges%20in%20extracting%20visual%0Adetails%2C%20effectively%20bridging%20across%20modalities.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20visual%20framework%2C%20MoCHA%2C%20to%20address%20these%20issues.%20Our%20framework%0Aintegrates%20four%20vision%20backbones%20%28i.e.%2C%20CLIP%2C%20SigLIP%2C%20DINOv2%20and%20ConvNeXt%29%20to%0Aextract%20complementary%20visual%20features%20and%20is%20equipped%20with%20a%20sparse%20Mixture%20of%0AExperts%20Connectors%20%28MoECs%29%20module%20to%20dynamically%20select%20experts%20tailored%20to%0Adifferent%20visual%20dimensions.%20To%20mitigate%20redundant%20or%20insufficient%20use%20of%20the%0Avisual%20information%20encoded%20by%20the%20MoECs%20module%2C%20we%20further%20design%20a%0AHierarchical%20Group%20Attention%20%28HGA%29%20with%20intra-%20and%20inter-group%20operations%20and%0Aan%20adaptive%20gating%20strategy%20for%20encoded%20visual%20features.%20We%20train%20MoCHA%20on%20two%0Amainstream%20LLMs%20%28e.g.%2C%20Phi2-2.7B%20and%20Vicuna-7B%29%20and%20evaluate%20their%20performance%0Aacross%20various%20benchmarks.%20Notably%2C%20MoCHA%20outperforms%20state-of-the-art%0Aopen-weight%20models%20on%20various%20tasks.%20For%20example%2C%20compared%20to%20CuMo%0A%28Mistral-7B%29%2C%20our%20MoCHA%20%28Phi2-2.7B%29%20presents%20outstanding%20abilities%20to%20mitigate%0Ahallucination%20by%20showing%20improvements%20of%203.25%25%20in%20POPE%20and%20to%20follow%20visual%0Ainstructions%20by%20raising%20153%20points%20on%20MME.%20Finally%2C%20ablation%20studies%20further%0Aconfirm%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20MoECs%20and%20HGA%20in%0Aimproving%20the%20overall%20performance%20of%20MoCHA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoCHA%253A%2520Advanced%2520Vision-Language%2520Reasoning%2520with%2520MoE%2520Connector%2520and%250A%2520%2520Hierarchical%2520Group%2520Attention%26entry.906535625%3DYuqi%2520Pang%2520and%2520Bowen%2520Yang%2520and%2520Yun%2520Cao%2520and%2520Fan%2520Rong%2520and%2520Xiaoyu%2520Li%2520and%2520Chen%2520He%26entry.1292438233%3D%2520%2520Vision%2520large%2520language%2520models%2520%2528VLLMs%2529%2520are%2520focusing%2520primarily%2520on%2520handling%250Acomplex%2520and%2520fine-grained%2520visual%2520information%2520by%2520incorporating%2520advanced%2520vision%250Aencoders%2520and%2520scaling%2520up%2520visual%2520models.%2520However%252C%2520these%2520approaches%2520face%2520high%250Atraining%2520and%2520inference%2520costs%252C%2520as%2520well%2520as%2520challenges%2520in%2520extracting%2520visual%250Adetails%252C%2520effectively%2520bridging%2520across%2520modalities.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520visual%2520framework%252C%2520MoCHA%252C%2520to%2520address%2520these%2520issues.%2520Our%2520framework%250Aintegrates%2520four%2520vision%2520backbones%2520%2528i.e.%252C%2520CLIP%252C%2520SigLIP%252C%2520DINOv2%2520and%2520ConvNeXt%2529%2520to%250Aextract%2520complementary%2520visual%2520features%2520and%2520is%2520equipped%2520with%2520a%2520sparse%2520Mixture%2520of%250AExperts%2520Connectors%2520%2528MoECs%2529%2520module%2520to%2520dynamically%2520select%2520experts%2520tailored%2520to%250Adifferent%2520visual%2520dimensions.%2520To%2520mitigate%2520redundant%2520or%2520insufficient%2520use%2520of%2520the%250Avisual%2520information%2520encoded%2520by%2520the%2520MoECs%2520module%252C%2520we%2520further%2520design%2520a%250AHierarchical%2520Group%2520Attention%2520%2528HGA%2529%2520with%2520intra-%2520and%2520inter-group%2520operations%2520and%250Aan%2520adaptive%2520gating%2520strategy%2520for%2520encoded%2520visual%2520features.%2520We%2520train%2520MoCHA%2520on%2520two%250Amainstream%2520LLMs%2520%2528e.g.%252C%2520Phi2-2.7B%2520and%2520Vicuna-7B%2529%2520and%2520evaluate%2520their%2520performance%250Aacross%2520various%2520benchmarks.%2520Notably%252C%2520MoCHA%2520outperforms%2520state-of-the-art%250Aopen-weight%2520models%2520on%2520various%2520tasks.%2520For%2520example%252C%2520compared%2520to%2520CuMo%250A%2528Mistral-7B%2529%252C%2520our%2520MoCHA%2520%2528Phi2-2.7B%2529%2520presents%2520outstanding%2520abilities%2520to%2520mitigate%250Ahallucination%2520by%2520showing%2520improvements%2520of%25203.25%2525%2520in%2520POPE%2520and%2520to%2520follow%2520visual%250Ainstructions%2520by%2520raising%2520153%2520points%2520on%2520MME.%2520Finally%252C%2520ablation%2520studies%2520further%250Aconfirm%2520the%2520effectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520MoECs%2520and%2520HGA%2520in%250Aimproving%2520the%2520overall%2520performance%2520of%2520MoCHA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCHA%3A%20Advanced%20Vision-Language%20Reasoning%20with%20MoE%20Connector%20and%0A%20%20Hierarchical%20Group%20Attention&entry.906535625=Yuqi%20Pang%20and%20Bowen%20Yang%20and%20Yun%20Cao%20and%20Fan%20Rong%20and%20Xiaoyu%20Li%20and%20Chen%20He&entry.1292438233=%20%20Vision%20large%20language%20models%20%28VLLMs%29%20are%20focusing%20primarily%20on%20handling%0Acomplex%20and%20fine-grained%20visual%20information%20by%20incorporating%20advanced%20vision%0Aencoders%20and%20scaling%20up%20visual%20models.%20However%2C%20these%20approaches%20face%20high%0Atraining%20and%20inference%20costs%2C%20as%20well%20as%20challenges%20in%20extracting%20visual%0Adetails%2C%20effectively%20bridging%20across%20modalities.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20visual%20framework%2C%20MoCHA%2C%20to%20address%20these%20issues.%20Our%20framework%0Aintegrates%20four%20vision%20backbones%20%28i.e.%2C%20CLIP%2C%20SigLIP%2C%20DINOv2%20and%20ConvNeXt%29%20to%0Aextract%20complementary%20visual%20features%20and%20is%20equipped%20with%20a%20sparse%20Mixture%20of%0AExperts%20Connectors%20%28MoECs%29%20module%20to%20dynamically%20select%20experts%20tailored%20to%0Adifferent%20visual%20dimensions.%20To%20mitigate%20redundant%20or%20insufficient%20use%20of%20the%0Avisual%20information%20encoded%20by%20the%20MoECs%20module%2C%20we%20further%20design%20a%0AHierarchical%20Group%20Attention%20%28HGA%29%20with%20intra-%20and%20inter-group%20operations%20and%0Aan%20adaptive%20gating%20strategy%20for%20encoded%20visual%20features.%20We%20train%20MoCHA%20on%20two%0Amainstream%20LLMs%20%28e.g.%2C%20Phi2-2.7B%20and%20Vicuna-7B%29%20and%20evaluate%20their%20performance%0Aacross%20various%20benchmarks.%20Notably%2C%20MoCHA%20outperforms%20state-of-the-art%0Aopen-weight%20models%20on%20various%20tasks.%20For%20example%2C%20compared%20to%20CuMo%0A%28Mistral-7B%29%2C%20our%20MoCHA%20%28Phi2-2.7B%29%20presents%20outstanding%20abilities%20to%20mitigate%0Ahallucination%20by%20showing%20improvements%20of%203.25%25%20in%20POPE%20and%20to%20follow%20visual%0Ainstructions%20by%20raising%20153%20points%20on%20MME.%20Finally%2C%20ablation%20studies%20further%0Aconfirm%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20MoECs%20and%20HGA%20in%0Aimproving%20the%20overall%20performance%20of%20MoCHA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22805v1&entry.124074799=Read"},
{"title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via\n  Local Density-Aware Spatial Occupancy", "author": "Jicheng Yuan and Manh Nguyen Duc and Qian Liu and Manfred Hauswirth and Danh Le Phuoc", "abstract": "  Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.\n", "link": "http://arxiv.org/abs/2507.21358v2", "date": "2025-07-30", "relevancy": 3.0555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6186}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Perceiver%3A%20Elevating%20Vision-based%203D%20Object%20Detection%20via%0A%20%20Local%20Density-Aware%20Spatial%20Occupancy&body=Title%3A%20Collaborative%20Perceiver%3A%20Elevating%20Vision-based%203D%20Object%20Detection%20via%0A%20%20Local%20Density-Aware%20Spatial%20Occupancy%0AAuthor%3A%20Jicheng%20Yuan%20and%20Manh%20Nguyen%20Duc%20and%20Qian%20Liu%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc%0AAbstract%3A%20%20%20Vision-based%20bird%27s-eye-view%20%28BEV%29%203D%20object%20detection%20has%20advanced%0Asignificantly%20in%20autonomous%20driving%20by%20offering%20cost-effectiveness%20and%20rich%0Acontextual%20information.%20However%2C%20existing%20methods%20often%20construct%20BEV%0Arepresentations%20by%20collapsing%20extracted%20object%20features%2C%20neglecting%20intrinsic%0Aenvironmental%20contexts%2C%20such%20as%20roads%20and%20pavements.%20This%20hinders%20detectors%0Afrom%20comprehensively%20perceiving%20the%20characteristics%20of%20the%20physical%20world.%20To%0Aalleviate%20this%2C%20we%20introduce%20a%20multi-task%20learning%20framework%2C%20Collaborative%0APerceiver%20%28CoP%29%2C%20that%20leverages%20spatial%20occupancy%20as%20auxiliary%20information%20to%0Amine%20consistent%20structural%20and%20conceptual%20similarities%20shared%20between%203D%20object%0Adetection%20and%20occupancy%20prediction%20tasks%2C%20bridging%20gaps%20in%20spatial%0Arepresentations%20and%20feature%20refinement.%20To%20this%20end%2C%20we%20first%20propose%20a%0Apipeline%20to%20generate%20dense%20occupancy%20ground%20truths%20incorporating%20local%20density%0Ainformation%20%28LDO%29%20for%20reconstructing%20detailed%20environmental%20information.%20Next%2C%0Awe%20employ%20a%20voxel-height-guided%20sampling%20%28VHS%29%20strategy%20to%20distill%20fine-grained%0Alocal%20features%20according%20to%20distinct%20object%20properties.%20Furthermore%2C%20we%20develop%0Aa%20global-local%20collaborative%20feature%20fusion%20%28CFF%29%20module%20that%20seamlessly%0Aintegrates%20complementary%20knowledge%20between%20both%20tasks%2C%20thus%20composing%20more%0Arobust%20BEV%20representations.%20Extensive%20experiments%20on%20the%20nuScenes%20benchmark%0Ademonstrate%20that%20CoP%20outperforms%20existing%20vision-based%20frameworks%2C%20achieving%0A49.5%5C%25%20mAP%20and%2059.2%5C%25%20NDS%20on%20the%20test%20set.%20Code%20and%20supplementary%20materials%20are%0Aavailable%20at%20this%20link%20https%3A//github.com/jichengyuan/Collaborative-Perceiver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Perceiver%253A%2520Elevating%2520Vision-based%25203D%2520Object%2520Detection%2520via%250A%2520%2520Local%2520Density-Aware%2520Spatial%2520Occupancy%26entry.906535625%3DJicheng%2520Yuan%2520and%2520Manh%2520Nguyen%2520Duc%2520and%2520Qian%2520Liu%2520and%2520Manfred%2520Hauswirth%2520and%2520Danh%2520Le%2520Phuoc%26entry.1292438233%3D%2520%2520Vision-based%2520bird%2527s-eye-view%2520%2528BEV%2529%25203D%2520object%2520detection%2520has%2520advanced%250Asignificantly%2520in%2520autonomous%2520driving%2520by%2520offering%2520cost-effectiveness%2520and%2520rich%250Acontextual%2520information.%2520However%252C%2520existing%2520methods%2520often%2520construct%2520BEV%250Arepresentations%2520by%2520collapsing%2520extracted%2520object%2520features%252C%2520neglecting%2520intrinsic%250Aenvironmental%2520contexts%252C%2520such%2520as%2520roads%2520and%2520pavements.%2520This%2520hinders%2520detectors%250Afrom%2520comprehensively%2520perceiving%2520the%2520characteristics%2520of%2520the%2520physical%2520world.%2520To%250Aalleviate%2520this%252C%2520we%2520introduce%2520a%2520multi-task%2520learning%2520framework%252C%2520Collaborative%250APerceiver%2520%2528CoP%2529%252C%2520that%2520leverages%2520spatial%2520occupancy%2520as%2520auxiliary%2520information%2520to%250Amine%2520consistent%2520structural%2520and%2520conceptual%2520similarities%2520shared%2520between%25203D%2520object%250Adetection%2520and%2520occupancy%2520prediction%2520tasks%252C%2520bridging%2520gaps%2520in%2520spatial%250Arepresentations%2520and%2520feature%2520refinement.%2520To%2520this%2520end%252C%2520we%2520first%2520propose%2520a%250Apipeline%2520to%2520generate%2520dense%2520occupancy%2520ground%2520truths%2520incorporating%2520local%2520density%250Ainformation%2520%2528LDO%2529%2520for%2520reconstructing%2520detailed%2520environmental%2520information.%2520Next%252C%250Awe%2520employ%2520a%2520voxel-height-guided%2520sampling%2520%2528VHS%2529%2520strategy%2520to%2520distill%2520fine-grained%250Alocal%2520features%2520according%2520to%2520distinct%2520object%2520properties.%2520Furthermore%252C%2520we%2520develop%250Aa%2520global-local%2520collaborative%2520feature%2520fusion%2520%2528CFF%2529%2520module%2520that%2520seamlessly%250Aintegrates%2520complementary%2520knowledge%2520between%2520both%2520tasks%252C%2520thus%2520composing%2520more%250Arobust%2520BEV%2520representations.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520benchmark%250Ademonstrate%2520that%2520CoP%2520outperforms%2520existing%2520vision-based%2520frameworks%252C%2520achieving%250A49.5%255C%2525%2520mAP%2520and%252059.2%255C%2525%2520NDS%2520on%2520the%2520test%2520set.%2520Code%2520and%2520supplementary%2520materials%2520are%250Aavailable%2520at%2520this%2520link%2520https%253A//github.com/jichengyuan/Collaborative-Perceiver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Perceiver%3A%20Elevating%20Vision-based%203D%20Object%20Detection%20via%0A%20%20Local%20Density-Aware%20Spatial%20Occupancy&entry.906535625=Jicheng%20Yuan%20and%20Manh%20Nguyen%20Duc%20and%20Qian%20Liu%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc&entry.1292438233=%20%20Vision-based%20bird%27s-eye-view%20%28BEV%29%203D%20object%20detection%20has%20advanced%0Asignificantly%20in%20autonomous%20driving%20by%20offering%20cost-effectiveness%20and%20rich%0Acontextual%20information.%20However%2C%20existing%20methods%20often%20construct%20BEV%0Arepresentations%20by%20collapsing%20extracted%20object%20features%2C%20neglecting%20intrinsic%0Aenvironmental%20contexts%2C%20such%20as%20roads%20and%20pavements.%20This%20hinders%20detectors%0Afrom%20comprehensively%20perceiving%20the%20characteristics%20of%20the%20physical%20world.%20To%0Aalleviate%20this%2C%20we%20introduce%20a%20multi-task%20learning%20framework%2C%20Collaborative%0APerceiver%20%28CoP%29%2C%20that%20leverages%20spatial%20occupancy%20as%20auxiliary%20information%20to%0Amine%20consistent%20structural%20and%20conceptual%20similarities%20shared%20between%203D%20object%0Adetection%20and%20occupancy%20prediction%20tasks%2C%20bridging%20gaps%20in%20spatial%0Arepresentations%20and%20feature%20refinement.%20To%20this%20end%2C%20we%20first%20propose%20a%0Apipeline%20to%20generate%20dense%20occupancy%20ground%20truths%20incorporating%20local%20density%0Ainformation%20%28LDO%29%20for%20reconstructing%20detailed%20environmental%20information.%20Next%2C%0Awe%20employ%20a%20voxel-height-guided%20sampling%20%28VHS%29%20strategy%20to%20distill%20fine-grained%0Alocal%20features%20according%20to%20distinct%20object%20properties.%20Furthermore%2C%20we%20develop%0Aa%20global-local%20collaborative%20feature%20fusion%20%28CFF%29%20module%20that%20seamlessly%0Aintegrates%20complementary%20knowledge%20between%20both%20tasks%2C%20thus%20composing%20more%0Arobust%20BEV%20representations.%20Extensive%20experiments%20on%20the%20nuScenes%20benchmark%0Ademonstrate%20that%20CoP%20outperforms%20existing%20vision-based%20frameworks%2C%20achieving%0A49.5%5C%25%20mAP%20and%2059.2%5C%25%20NDS%20on%20the%20test%20set.%20Code%20and%20supplementary%20materials%20are%0Aavailable%20at%20this%20link%20https%3A//github.com/jichengyuan/Collaborative-Perceiver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21358v2&entry.124074799=Read"},
{"title": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions", "author": "Akash Ghosh and Arkadeep Acharya and Sriparna Saha and Vinija Jain and Aman Chadha", "abstract": "  The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.\n", "link": "http://arxiv.org/abs/2404.07214v3", "date": "2025-07-30", "relevancy": 3.0404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Frontier%20of%20Vision-Language%20Models%3A%20A%20Survey%20of%20Current%0A%20%20Methodologies%20and%20Future%20Directions&body=Title%3A%20Exploring%20the%20Frontier%20of%20Vision-Language%20Models%3A%20A%20Survey%20of%20Current%0A%20%20Methodologies%20and%20Future%20Directions%0AAuthor%3A%20Akash%20Ghosh%20and%20Arkadeep%20Acharya%20and%20Sriparna%20Saha%20and%20Vinija%20Jain%20and%20Aman%20Chadha%0AAbstract%3A%20%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%20reshaped%20the%0Atrajectory%20of%20the%20AI%20revolution.%20Nevertheless%2C%20these%20LLMs%20exhibit%20a%20notable%0Alimitation%2C%20as%20they%20are%20primarily%20adept%20at%20processing%20textual%20information.%20To%0Aaddress%20this%20constraint%2C%20researchers%20have%20endeavored%20to%20integrate%20visual%0Acapabilities%20with%20LLMs%2C%20resulting%20in%20the%20emergence%20of%20Vision-Language%20Models%0A%28VLMs%29.%20These%20advanced%20models%20are%20instrumental%20in%20tackling%20more%20intricate%20tasks%0Asuch%20as%20image%20captioning%20and%20visual%20question%20answering.%20In%20our%20comprehensive%0Asurvey%20paper%2C%20we%20delve%20into%20the%20key%20advancements%20within%20the%20realm%20of%20VLMs.%20Our%0Aclassification%20organizes%20VLMs%20into%20three%20distinct%20categories%3A%20models%20dedicated%0Ato%20vision-language%20understanding%2C%20models%20that%20process%20multimodal%20inputs%20to%0Agenerate%20unimodal%20%28textual%29%20outputs%20and%20models%20that%20both%20accept%20and%20produce%0Amultimodal%20inputs%20and%20outputs.This%20classification%20is%20based%20on%20their%20respective%0Acapabilities%20and%20functionalities%20in%20processing%20and%20generating%20various%0Amodalities%20of%20data.We%20meticulously%20dissect%20each%20model%2C%20offering%20an%20extensive%0Aanalysis%20of%20its%20foundational%20architecture%2C%20training%20data%20sources%2C%20as%20well%20as%0Aits%20strengths%20and%20limitations%20wherever%20possible%2C%20providing%20readers%20with%20a%0Acomprehensive%20understanding%20of%20its%20essential%20components.%20We%20also%20analyzed%20the%0Aperformance%20of%20VLMs%20in%20various%20benchmark%20datasets.%20By%20doing%20so%2C%20we%20aim%20to%20offer%0Aa%20nuanced%20understanding%20of%20the%20diverse%20landscape%20of%20VLMs.%20Additionally%2C%20we%0Aunderscore%20potential%20avenues%20for%20future%20research%20in%20this%20dynamic%20domain%2C%0Aanticipating%20further%20breakthroughs%20and%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07214v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Frontier%2520of%2520Vision-Language%2520Models%253A%2520A%2520Survey%2520of%2520Current%250A%2520%2520Methodologies%2520and%2520Future%2520Directions%26entry.906535625%3DAkash%2520Ghosh%2520and%2520Arkadeep%2520Acharya%2520and%2520Sriparna%2520Saha%2520and%2520Vinija%2520Jain%2520and%2520Aman%2520Chadha%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%2520reshaped%2520the%250Atrajectory%2520of%2520the%2520AI%2520revolution.%2520Nevertheless%252C%2520these%2520LLMs%2520exhibit%2520a%2520notable%250Alimitation%252C%2520as%2520they%2520are%2520primarily%2520adept%2520at%2520processing%2520textual%2520information.%2520To%250Aaddress%2520this%2520constraint%252C%2520researchers%2520have%2520endeavored%2520to%2520integrate%2520visual%250Acapabilities%2520with%2520LLMs%252C%2520resulting%2520in%2520the%2520emergence%2520of%2520Vision-Language%2520Models%250A%2528VLMs%2529.%2520These%2520advanced%2520models%2520are%2520instrumental%2520in%2520tackling%2520more%2520intricate%2520tasks%250Asuch%2520as%2520image%2520captioning%2520and%2520visual%2520question%2520answering.%2520In%2520our%2520comprehensive%250Asurvey%2520paper%252C%2520we%2520delve%2520into%2520the%2520key%2520advancements%2520within%2520the%2520realm%2520of%2520VLMs.%2520Our%250Aclassification%2520organizes%2520VLMs%2520into%2520three%2520distinct%2520categories%253A%2520models%2520dedicated%250Ato%2520vision-language%2520understanding%252C%2520models%2520that%2520process%2520multimodal%2520inputs%2520to%250Agenerate%2520unimodal%2520%2528textual%2529%2520outputs%2520and%2520models%2520that%2520both%2520accept%2520and%2520produce%250Amultimodal%2520inputs%2520and%2520outputs.This%2520classification%2520is%2520based%2520on%2520their%2520respective%250Acapabilities%2520and%2520functionalities%2520in%2520processing%2520and%2520generating%2520various%250Amodalities%2520of%2520data.We%2520meticulously%2520dissect%2520each%2520model%252C%2520offering%2520an%2520extensive%250Aanalysis%2520of%2520its%2520foundational%2520architecture%252C%2520training%2520data%2520sources%252C%2520as%2520well%2520as%250Aits%2520strengths%2520and%2520limitations%2520wherever%2520possible%252C%2520providing%2520readers%2520with%2520a%250Acomprehensive%2520understanding%2520of%2520its%2520essential%2520components.%2520We%2520also%2520analyzed%2520the%250Aperformance%2520of%2520VLMs%2520in%2520various%2520benchmark%2520datasets.%2520By%2520doing%2520so%252C%2520we%2520aim%2520to%2520offer%250Aa%2520nuanced%2520understanding%2520of%2520the%2520diverse%2520landscape%2520of%2520VLMs.%2520Additionally%252C%2520we%250Aunderscore%2520potential%2520avenues%2520for%2520future%2520research%2520in%2520this%2520dynamic%2520domain%252C%250Aanticipating%2520further%2520breakthroughs%2520and%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07214v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Frontier%20of%20Vision-Language%20Models%3A%20A%20Survey%20of%20Current%0A%20%20Methodologies%20and%20Future%20Directions&entry.906535625=Akash%20Ghosh%20and%20Arkadeep%20Acharya%20and%20Sriparna%20Saha%20and%20Vinija%20Jain%20and%20Aman%20Chadha&entry.1292438233=%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%20reshaped%20the%0Atrajectory%20of%20the%20AI%20revolution.%20Nevertheless%2C%20these%20LLMs%20exhibit%20a%20notable%0Alimitation%2C%20as%20they%20are%20primarily%20adept%20at%20processing%20textual%20information.%20To%0Aaddress%20this%20constraint%2C%20researchers%20have%20endeavored%20to%20integrate%20visual%0Acapabilities%20with%20LLMs%2C%20resulting%20in%20the%20emergence%20of%20Vision-Language%20Models%0A%28VLMs%29.%20These%20advanced%20models%20are%20instrumental%20in%20tackling%20more%20intricate%20tasks%0Asuch%20as%20image%20captioning%20and%20visual%20question%20answering.%20In%20our%20comprehensive%0Asurvey%20paper%2C%20we%20delve%20into%20the%20key%20advancements%20within%20the%20realm%20of%20VLMs.%20Our%0Aclassification%20organizes%20VLMs%20into%20three%20distinct%20categories%3A%20models%20dedicated%0Ato%20vision-language%20understanding%2C%20models%20that%20process%20multimodal%20inputs%20to%0Agenerate%20unimodal%20%28textual%29%20outputs%20and%20models%20that%20both%20accept%20and%20produce%0Amultimodal%20inputs%20and%20outputs.This%20classification%20is%20based%20on%20their%20respective%0Acapabilities%20and%20functionalities%20in%20processing%20and%20generating%20various%0Amodalities%20of%20data.We%20meticulously%20dissect%20each%20model%2C%20offering%20an%20extensive%0Aanalysis%20of%20its%20foundational%20architecture%2C%20training%20data%20sources%2C%20as%20well%20as%0Aits%20strengths%20and%20limitations%20wherever%20possible%2C%20providing%20readers%20with%20a%0Acomprehensive%20understanding%20of%20its%20essential%20components.%20We%20also%20analyzed%20the%0Aperformance%20of%20VLMs%20in%20various%20benchmark%20datasets.%20By%20doing%20so%2C%20we%20aim%20to%20offer%0Aa%20nuanced%20understanding%20of%20the%20diverse%20landscape%20of%20VLMs.%20Additionally%2C%20we%0Aunderscore%20potential%20avenues%20for%20future%20research%20in%20this%20dynamic%20domain%2C%0Aanticipating%20further%20breakthroughs%20and%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07214v3&entry.124074799=Read"},
{"title": "StoryTeller: Improving Long Video Description through Global\n  Audio-Visual Character Identification", "author": "Yichen He and Yuan Lin and Jianchao Wu and Hanchong Zhang and Yuchen Zhang and Ruicheng Le", "abstract": "  Existing large vision-language models (LVLMs) are largely limited to\nprocessing short, seconds-long videos and struggle with generating coherent\ndescriptions for extended video spanning minutes or more. Long video\ndescription introduces new challenges, such as consistent character\nidentification and plot-level descriptions incorporating both visual and audio\ninformation. To address these, we figure out audio-visual character\nidentification, matching character names to each dialogue, as a key factor. We\npropose StoryTeller, a system for generating dense descriptions of long videos,\nincorporating both low-level visual concepts and high-level plot information.\nStoryTeller uses a multimodal large language model that integrates visual,\naudio, and text modalities to perform audio-visual character identification on\nminute-long video clips. The results are then fed into a LVLM to enhance\nconsistency of video description. We validate our approach on movie description\ntasks and introduce MovieStory101, a dataset with dense descriptions for\nthree-minute movie clips. To evaluate long video descriptions, we create\nStoryQA, a large set of multiple-choice questions for MovieStory101 test set.\nWe assess descriptions by inputting them into GPT-4 to answer these questions,\nusing accuracy as an automatic evaluation metric. Experiments show that\nStoryTeller outperforms all open and closed-source baselines on StoryQA,\nachieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and\ndemonstrating a +15.56% advantage in human side-by-side evaluations.\nAdditionally, incorporating audio-visual character identification from\nStoryTeller improves the performance of all video description models, with\nGemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,\nrespectively, in accuracy on StoryQA.\n", "link": "http://arxiv.org/abs/2411.07076v3", "date": "2025-07-30", "relevancy": 2.9447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5966}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryTeller%3A%20Improving%20Long%20Video%20Description%20through%20Global%0A%20%20Audio-Visual%20Character%20Identification&body=Title%3A%20StoryTeller%3A%20Improving%20Long%20Video%20Description%20through%20Global%0A%20%20Audio-Visual%20Character%20Identification%0AAuthor%3A%20Yichen%20He%20and%20Yuan%20Lin%20and%20Jianchao%20Wu%20and%20Hanchong%20Zhang%20and%20Yuchen%20Zhang%20and%20Ruicheng%20Le%0AAbstract%3A%20%20%20Existing%20large%20vision-language%20models%20%28LVLMs%29%20are%20largely%20limited%20to%0Aprocessing%20short%2C%20seconds-long%20videos%20and%20struggle%20with%20generating%20coherent%0Adescriptions%20for%20extended%20video%20spanning%20minutes%20or%20more.%20Long%20video%0Adescription%20introduces%20new%20challenges%2C%20such%20as%20consistent%20character%0Aidentification%20and%20plot-level%20descriptions%20incorporating%20both%20visual%20and%20audio%0Ainformation.%20To%20address%20these%2C%20we%20figure%20out%20audio-visual%20character%0Aidentification%2C%20matching%20character%20names%20to%20each%20dialogue%2C%20as%20a%20key%20factor.%20We%0Apropose%20StoryTeller%2C%20a%20system%20for%20generating%20dense%20descriptions%20of%20long%20videos%2C%0Aincorporating%20both%20low-level%20visual%20concepts%20and%20high-level%20plot%20information.%0AStoryTeller%20uses%20a%20multimodal%20large%20language%20model%20that%20integrates%20visual%2C%0Aaudio%2C%20and%20text%20modalities%20to%20perform%20audio-visual%20character%20identification%20on%0Aminute-long%20video%20clips.%20The%20results%20are%20then%20fed%20into%20a%20LVLM%20to%20enhance%0Aconsistency%20of%20video%20description.%20We%20validate%20our%20approach%20on%20movie%20description%0Atasks%20and%20introduce%20MovieStory101%2C%20a%20dataset%20with%20dense%20descriptions%20for%0Athree-minute%20movie%20clips.%20To%20evaluate%20long%20video%20descriptions%2C%20we%20create%0AStoryQA%2C%20a%20large%20set%20of%20multiple-choice%20questions%20for%20MovieStory101%20test%20set.%0AWe%20assess%20descriptions%20by%20inputting%20them%20into%20GPT-4%20to%20answer%20these%20questions%2C%0Ausing%20accuracy%20as%20an%20automatic%20evaluation%20metric.%20Experiments%20show%20that%0AStoryTeller%20outperforms%20all%20open%20and%20closed-source%20baselines%20on%20StoryQA%2C%0Aachieving%209.5%25%20higher%20accuracy%20than%20the%20strongest%20baseline%2C%20Gemini-1.5-pro%2C%20and%0Ademonstrating%20a%20%2B15.56%25%20advantage%20in%20human%20side-by-side%20evaluations.%0AAdditionally%2C%20incorporating%20audio-visual%20character%20identification%20from%0AStoryTeller%20improves%20the%20performance%20of%20all%20video%20description%20models%2C%20with%0AGemini-1.5-pro%20and%20GPT-4o%20showing%20relative%20improvement%20of%205.5%25%20and%2013.0%25%2C%0Arespectively%2C%20in%20accuracy%20on%20StoryQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07076v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryTeller%253A%2520Improving%2520Long%2520Video%2520Description%2520through%2520Global%250A%2520%2520Audio-Visual%2520Character%2520Identification%26entry.906535625%3DYichen%2520He%2520and%2520Yuan%2520Lin%2520and%2520Jianchao%2520Wu%2520and%2520Hanchong%2520Zhang%2520and%2520Yuchen%2520Zhang%2520and%2520Ruicheng%2520Le%26entry.1292438233%3D%2520%2520Existing%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520largely%2520limited%2520to%250Aprocessing%2520short%252C%2520seconds-long%2520videos%2520and%2520struggle%2520with%2520generating%2520coherent%250Adescriptions%2520for%2520extended%2520video%2520spanning%2520minutes%2520or%2520more.%2520Long%2520video%250Adescription%2520introduces%2520new%2520challenges%252C%2520such%2520as%2520consistent%2520character%250Aidentification%2520and%2520plot-level%2520descriptions%2520incorporating%2520both%2520visual%2520and%2520audio%250Ainformation.%2520To%2520address%2520these%252C%2520we%2520figure%2520out%2520audio-visual%2520character%250Aidentification%252C%2520matching%2520character%2520names%2520to%2520each%2520dialogue%252C%2520as%2520a%2520key%2520factor.%2520We%250Apropose%2520StoryTeller%252C%2520a%2520system%2520for%2520generating%2520dense%2520descriptions%2520of%2520long%2520videos%252C%250Aincorporating%2520both%2520low-level%2520visual%2520concepts%2520and%2520high-level%2520plot%2520information.%250AStoryTeller%2520uses%2520a%2520multimodal%2520large%2520language%2520model%2520that%2520integrates%2520visual%252C%250Aaudio%252C%2520and%2520text%2520modalities%2520to%2520perform%2520audio-visual%2520character%2520identification%2520on%250Aminute-long%2520video%2520clips.%2520The%2520results%2520are%2520then%2520fed%2520into%2520a%2520LVLM%2520to%2520enhance%250Aconsistency%2520of%2520video%2520description.%2520We%2520validate%2520our%2520approach%2520on%2520movie%2520description%250Atasks%2520and%2520introduce%2520MovieStory101%252C%2520a%2520dataset%2520with%2520dense%2520descriptions%2520for%250Athree-minute%2520movie%2520clips.%2520To%2520evaluate%2520long%2520video%2520descriptions%252C%2520we%2520create%250AStoryQA%252C%2520a%2520large%2520set%2520of%2520multiple-choice%2520questions%2520for%2520MovieStory101%2520test%2520set.%250AWe%2520assess%2520descriptions%2520by%2520inputting%2520them%2520into%2520GPT-4%2520to%2520answer%2520these%2520questions%252C%250Ausing%2520accuracy%2520as%2520an%2520automatic%2520evaluation%2520metric.%2520Experiments%2520show%2520that%250AStoryTeller%2520outperforms%2520all%2520open%2520and%2520closed-source%2520baselines%2520on%2520StoryQA%252C%250Aachieving%25209.5%2525%2520higher%2520accuracy%2520than%2520the%2520strongest%2520baseline%252C%2520Gemini-1.5-pro%252C%2520and%250Ademonstrating%2520a%2520%252B15.56%2525%2520advantage%2520in%2520human%2520side-by-side%2520evaluations.%250AAdditionally%252C%2520incorporating%2520audio-visual%2520character%2520identification%2520from%250AStoryTeller%2520improves%2520the%2520performance%2520of%2520all%2520video%2520description%2520models%252C%2520with%250AGemini-1.5-pro%2520and%2520GPT-4o%2520showing%2520relative%2520improvement%2520of%25205.5%2525%2520and%252013.0%2525%252C%250Arespectively%252C%2520in%2520accuracy%2520on%2520StoryQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07076v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryTeller%3A%20Improving%20Long%20Video%20Description%20through%20Global%0A%20%20Audio-Visual%20Character%20Identification&entry.906535625=Yichen%20He%20and%20Yuan%20Lin%20and%20Jianchao%20Wu%20and%20Hanchong%20Zhang%20and%20Yuchen%20Zhang%20and%20Ruicheng%20Le&entry.1292438233=%20%20Existing%20large%20vision-language%20models%20%28LVLMs%29%20are%20largely%20limited%20to%0Aprocessing%20short%2C%20seconds-long%20videos%20and%20struggle%20with%20generating%20coherent%0Adescriptions%20for%20extended%20video%20spanning%20minutes%20or%20more.%20Long%20video%0Adescription%20introduces%20new%20challenges%2C%20such%20as%20consistent%20character%0Aidentification%20and%20plot-level%20descriptions%20incorporating%20both%20visual%20and%20audio%0Ainformation.%20To%20address%20these%2C%20we%20figure%20out%20audio-visual%20character%0Aidentification%2C%20matching%20character%20names%20to%20each%20dialogue%2C%20as%20a%20key%20factor.%20We%0Apropose%20StoryTeller%2C%20a%20system%20for%20generating%20dense%20descriptions%20of%20long%20videos%2C%0Aincorporating%20both%20low-level%20visual%20concepts%20and%20high-level%20plot%20information.%0AStoryTeller%20uses%20a%20multimodal%20large%20language%20model%20that%20integrates%20visual%2C%0Aaudio%2C%20and%20text%20modalities%20to%20perform%20audio-visual%20character%20identification%20on%0Aminute-long%20video%20clips.%20The%20results%20are%20then%20fed%20into%20a%20LVLM%20to%20enhance%0Aconsistency%20of%20video%20description.%20We%20validate%20our%20approach%20on%20movie%20description%0Atasks%20and%20introduce%20MovieStory101%2C%20a%20dataset%20with%20dense%20descriptions%20for%0Athree-minute%20movie%20clips.%20To%20evaluate%20long%20video%20descriptions%2C%20we%20create%0AStoryQA%2C%20a%20large%20set%20of%20multiple-choice%20questions%20for%20MovieStory101%20test%20set.%0AWe%20assess%20descriptions%20by%20inputting%20them%20into%20GPT-4%20to%20answer%20these%20questions%2C%0Ausing%20accuracy%20as%20an%20automatic%20evaluation%20metric.%20Experiments%20show%20that%0AStoryTeller%20outperforms%20all%20open%20and%20closed-source%20baselines%20on%20StoryQA%2C%0Aachieving%209.5%25%20higher%20accuracy%20than%20the%20strongest%20baseline%2C%20Gemini-1.5-pro%2C%20and%0Ademonstrating%20a%20%2B15.56%25%20advantage%20in%20human%20side-by-side%20evaluations.%0AAdditionally%2C%20incorporating%20audio-visual%20character%20identification%20from%0AStoryTeller%20improves%20the%20performance%20of%20all%20video%20description%20models%2C%20with%0AGemini-1.5-pro%20and%20GPT-4o%20showing%20relative%20improvement%20of%205.5%25%20and%2013.0%25%2C%0Arespectively%2C%20in%20accuracy%20on%20StoryQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07076v3&entry.124074799=Read"},
{"title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning,\n  Rendering, and Visual Feedback", "author": "Yang Chen and Yufan Shen and Wenxuan Huang and Sheng Zhou and Qunshu Lin and Xinyu Cai and Zhi Yu and Jiajun Bu and Botian Shi and Yu Qiao", "abstract": "  Multimodal Large Language Models (MLLMs) exhibit impressive performance\nacross various visual tasks. Subsequent investigations into enhancing their\nvisual reasoning abilities have significantly expanded their performance\nenvelope. However, a critical bottleneck in the advancement of MLLMs toward\ndeep visual reasoning is their heavy reliance on curated image-text\nsupervision. To solve this problem, we introduce a novel framework termed\n``Reasoning-Rendering-Visual-Feedback'' (RRVF), which enables MLLMs to learn\ncomplex visual reasoning from only raw images. This framework builds on the\n``Asymmetry of Verification'' principle to train MLLMs, i.e., verifying the\nrendered output against a source image is easier than generating it. We\ndemonstrate that this relative ease provides an ideal reward signal for\noptimization via Reinforcement Learning (RL) training, reducing reliance on the\nimage-text supervision. Guided by the above principle, RRVF implements a\nclosed-loop iterative process encompassing reasoning, rendering, and visual\nfeedback components, enabling the model to perform self-correction through\nmulti-turn interactions, while this pipeline can be optimized end-to-end by the\nGRPO algorithm. Extensive evaluations are conducted on image-to-code generation\nacross two diverse domains: data charts and web interfaces. The RRVF-trained\nmodel not only outperforms existing open-source MLLMs and supervised\nfine-tuning baselines but also exhibits superior generalization to unseen\ndatasets. Critically, the model's performance surpasses that of the more\nadvanced MLLM used to provide the feedback signal during training. This work\nestablishes a self-improvement paradigm that offers a viable path to robust,\ngeneralizable models without reliance on explicit supervision. Code will be\navailable at https://github.com/L-O-I/RRVF.\n", "link": "http://arxiv.org/abs/2507.20766v2", "date": "2025-07-30", "relevancy": 2.8805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback&body=Title%3A%20Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback%0AAuthor%3A%20Yang%20Chen%20and%20Yufan%20Shen%20and%20Wenxuan%20Huang%20and%20Sheng%20Zhou%20and%20Qunshu%20Lin%20and%20Xinyu%20Cai%20and%20Zhi%20Yu%20and%20Jiajun%20Bu%20and%20Botian%20Shi%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20exhibit%20impressive%20performance%0Aacross%20various%20visual%20tasks.%20Subsequent%20investigations%20into%20enhancing%20their%0Avisual%20reasoning%20abilities%20have%20significantly%20expanded%20their%20performance%0Aenvelope.%20However%2C%20a%20critical%20bottleneck%20in%20the%20advancement%20of%20MLLMs%20toward%0Adeep%20visual%20reasoning%20is%20their%20heavy%20reliance%20on%20curated%20image-text%0Asupervision.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%20framework%20termed%0A%60%60Reasoning-Rendering-Visual-Feedback%27%27%20%28RRVF%29%2C%20which%20enables%20MLLMs%20to%20learn%0Acomplex%20visual%20reasoning%20from%20only%20raw%20images.%20This%20framework%20builds%20on%20the%0A%60%60Asymmetry%20of%20Verification%27%27%20principle%20to%20train%20MLLMs%2C%20i.e.%2C%20verifying%20the%0Arendered%20output%20against%20a%20source%20image%20is%20easier%20than%20generating%20it.%20We%0Ademonstrate%20that%20this%20relative%20ease%20provides%20an%20ideal%20reward%20signal%20for%0Aoptimization%20via%20Reinforcement%20Learning%20%28RL%29%20training%2C%20reducing%20reliance%20on%20the%0Aimage-text%20supervision.%20Guided%20by%20the%20above%20principle%2C%20RRVF%20implements%20a%0Aclosed-loop%20iterative%20process%20encompassing%20reasoning%2C%20rendering%2C%20and%20visual%0Afeedback%20components%2C%20enabling%20the%20model%20to%20perform%20self-correction%20through%0Amulti-turn%20interactions%2C%20while%20this%20pipeline%20can%20be%20optimized%20end-to-end%20by%20the%0AGRPO%20algorithm.%20Extensive%20evaluations%20are%20conducted%20on%20image-to-code%20generation%0Aacross%20two%20diverse%20domains%3A%20data%20charts%20and%20web%20interfaces.%20The%20RRVF-trained%0Amodel%20not%20only%20outperforms%20existing%20open-source%20MLLMs%20and%20supervised%0Afine-tuning%20baselines%20but%20also%20exhibits%20superior%20generalization%20to%20unseen%0Adatasets.%20Critically%2C%20the%20model%27s%20performance%20surpasses%20that%20of%20the%20more%0Aadvanced%20MLLM%20used%20to%20provide%20the%20feedback%20signal%20during%20training.%20This%20work%0Aestablishes%20a%20self-improvement%20paradigm%20that%20offers%20a%20viable%20path%20to%20robust%2C%0Ageneralizable%20models%20without%20reliance%20on%20explicit%20supervision.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/L-O-I/RRVF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Only%2520with%2520Images%253A%2520Visual%2520Reinforcement%2520Learning%2520with%2520Reasoning%252C%250A%2520%2520Rendering%252C%2520and%2520Visual%2520Feedback%26entry.906535625%3DYang%2520Chen%2520and%2520Yufan%2520Shen%2520and%2520Wenxuan%2520Huang%2520and%2520Sheng%2520Zhou%2520and%2520Qunshu%2520Lin%2520and%2520Xinyu%2520Cai%2520and%2520Zhi%2520Yu%2520and%2520Jiajun%2520Bu%2520and%2520Botian%2520Shi%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520exhibit%2520impressive%2520performance%250Aacross%2520various%2520visual%2520tasks.%2520Subsequent%2520investigations%2520into%2520enhancing%2520their%250Avisual%2520reasoning%2520abilities%2520have%2520significantly%2520expanded%2520their%2520performance%250Aenvelope.%2520However%252C%2520a%2520critical%2520bottleneck%2520in%2520the%2520advancement%2520of%2520MLLMs%2520toward%250Adeep%2520visual%2520reasoning%2520is%2520their%2520heavy%2520reliance%2520on%2520curated%2520image-text%250Asupervision.%2520To%2520solve%2520this%2520problem%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520termed%250A%2560%2560Reasoning-Rendering-Visual-Feedback%2527%2527%2520%2528RRVF%2529%252C%2520which%2520enables%2520MLLMs%2520to%2520learn%250Acomplex%2520visual%2520reasoning%2520from%2520only%2520raw%2520images.%2520This%2520framework%2520builds%2520on%2520the%250A%2560%2560Asymmetry%2520of%2520Verification%2527%2527%2520principle%2520to%2520train%2520MLLMs%252C%2520i.e.%252C%2520verifying%2520the%250Arendered%2520output%2520against%2520a%2520source%2520image%2520is%2520easier%2520than%2520generating%2520it.%2520We%250Ademonstrate%2520that%2520this%2520relative%2520ease%2520provides%2520an%2520ideal%2520reward%2520signal%2520for%250Aoptimization%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%2520training%252C%2520reducing%2520reliance%2520on%2520the%250Aimage-text%2520supervision.%2520Guided%2520by%2520the%2520above%2520principle%252C%2520RRVF%2520implements%2520a%250Aclosed-loop%2520iterative%2520process%2520encompassing%2520reasoning%252C%2520rendering%252C%2520and%2520visual%250Afeedback%2520components%252C%2520enabling%2520the%2520model%2520to%2520perform%2520self-correction%2520through%250Amulti-turn%2520interactions%252C%2520while%2520this%2520pipeline%2520can%2520be%2520optimized%2520end-to-end%2520by%2520the%250AGRPO%2520algorithm.%2520Extensive%2520evaluations%2520are%2520conducted%2520on%2520image-to-code%2520generation%250Aacross%2520two%2520diverse%2520domains%253A%2520data%2520charts%2520and%2520web%2520interfaces.%2520The%2520RRVF-trained%250Amodel%2520not%2520only%2520outperforms%2520existing%2520open-source%2520MLLMs%2520and%2520supervised%250Afine-tuning%2520baselines%2520but%2520also%2520exhibits%2520superior%2520generalization%2520to%2520unseen%250Adatasets.%2520Critically%252C%2520the%2520model%2527s%2520performance%2520surpasses%2520that%2520of%2520the%2520more%250Aadvanced%2520MLLM%2520used%2520to%2520provide%2520the%2520feedback%2520signal%2520during%2520training.%2520This%2520work%250Aestablishes%2520a%2520self-improvement%2520paradigm%2520that%2520offers%2520a%2520viable%2520path%2520to%2520robust%252C%250Ageneralizable%2520models%2520without%2520reliance%2520on%2520explicit%2520supervision.%2520Code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/L-O-I/RRVF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback&entry.906535625=Yang%20Chen%20and%20Yufan%20Shen%20and%20Wenxuan%20Huang%20and%20Sheng%20Zhou%20and%20Qunshu%20Lin%20and%20Xinyu%20Cai%20and%20Zhi%20Yu%20and%20Jiajun%20Bu%20and%20Botian%20Shi%20and%20Yu%20Qiao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20exhibit%20impressive%20performance%0Aacross%20various%20visual%20tasks.%20Subsequent%20investigations%20into%20enhancing%20their%0Avisual%20reasoning%20abilities%20have%20significantly%20expanded%20their%20performance%0Aenvelope.%20However%2C%20a%20critical%20bottleneck%20in%20the%20advancement%20of%20MLLMs%20toward%0Adeep%20visual%20reasoning%20is%20their%20heavy%20reliance%20on%20curated%20image-text%0Asupervision.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%20framework%20termed%0A%60%60Reasoning-Rendering-Visual-Feedback%27%27%20%28RRVF%29%2C%20which%20enables%20MLLMs%20to%20learn%0Acomplex%20visual%20reasoning%20from%20only%20raw%20images.%20This%20framework%20builds%20on%20the%0A%60%60Asymmetry%20of%20Verification%27%27%20principle%20to%20train%20MLLMs%2C%20i.e.%2C%20verifying%20the%0Arendered%20output%20against%20a%20source%20image%20is%20easier%20than%20generating%20it.%20We%0Ademonstrate%20that%20this%20relative%20ease%20provides%20an%20ideal%20reward%20signal%20for%0Aoptimization%20via%20Reinforcement%20Learning%20%28RL%29%20training%2C%20reducing%20reliance%20on%20the%0Aimage-text%20supervision.%20Guided%20by%20the%20above%20principle%2C%20RRVF%20implements%20a%0Aclosed-loop%20iterative%20process%20encompassing%20reasoning%2C%20rendering%2C%20and%20visual%0Afeedback%20components%2C%20enabling%20the%20model%20to%20perform%20self-correction%20through%0Amulti-turn%20interactions%2C%20while%20this%20pipeline%20can%20be%20optimized%20end-to-end%20by%20the%0AGRPO%20algorithm.%20Extensive%20evaluations%20are%20conducted%20on%20image-to-code%20generation%0Aacross%20two%20diverse%20domains%3A%20data%20charts%20and%20web%20interfaces.%20The%20RRVF-trained%0Amodel%20not%20only%20outperforms%20existing%20open-source%20MLLMs%20and%20supervised%0Afine-tuning%20baselines%20but%20also%20exhibits%20superior%20generalization%20to%20unseen%0Adatasets.%20Critically%2C%20the%20model%27s%20performance%20surpasses%20that%20of%20the%20more%0Aadvanced%20MLLM%20used%20to%20provide%20the%20feedback%20signal%20during%20training.%20This%20work%0Aestablishes%20a%20self-improvement%20paradigm%20that%20offers%20a%20viable%20path%20to%20robust%2C%0Ageneralizable%20models%20without%20reliance%20on%20explicit%20supervision.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/L-O-I/RRVF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20766v2&entry.124074799=Read"},
{"title": "Modality-Aware Feature Matching: A Comprehensive Review of Single- and\n  Cross-Modality Techniques", "author": "Weide Liu and Wei Zhou and Jun Liu and Ping Hu and Jun Cheng and Jungong Han and Weisi Lin", "abstract": "  Feature matching is a cornerstone task in computer vision, essential for\napplications such as image retrieval, stereo matching, 3D reconstruction, and\nSLAM. This survey comprehensively reviews modality-based feature matching,\nexploring traditional handcrafted methods and emphasizing contemporary deep\nlearning approaches across various modalities, including RGB images, depth\nimages, 3D point clouds, LiDAR scans, medical images, and vision-language\ninteractions. Traditional methods, leveraging detectors like Harris corners and\ndescriptors such as SIFT and ORB, demonstrate robustness under moderate\nintra-modality variations but struggle with significant modality gaps.\nContemporary deep learning-based methods, exemplified by detector-free\nstrategies like CNN-based SuperPoint and transformer-based LoFTR, substantially\nimprove robustness and adaptability across modalities. We highlight\nmodality-aware advancements, such as geometric and depth-specific descriptors\nfor depth images, sparse and dense learning methods for 3D point clouds,\nattention-enhanced neural networks for LiDAR scans, and specialized solutions\nlike the MIND descriptor for complex medical image matching. Cross-modal\napplications, particularly in medical image registration and vision-language\ntasks, underscore the evolution of feature matching to handle increasingly\ndiverse data interactions.\n", "link": "http://arxiv.org/abs/2507.22791v1", "date": "2025-07-30", "relevancy": 2.8774, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5851}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-Aware%20Feature%20Matching%3A%20A%20Comprehensive%20Review%20of%20Single-%20and%0A%20%20Cross-Modality%20Techniques&body=Title%3A%20Modality-Aware%20Feature%20Matching%3A%20A%20Comprehensive%20Review%20of%20Single-%20and%0A%20%20Cross-Modality%20Techniques%0AAuthor%3A%20Weide%20Liu%20and%20Wei%20Zhou%20and%20Jun%20Liu%20and%20Ping%20Hu%20and%20Jun%20Cheng%20and%20Jungong%20Han%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20Feature%20matching%20is%20a%20cornerstone%20task%20in%20computer%20vision%2C%20essential%20for%0Aapplications%20such%20as%20image%20retrieval%2C%20stereo%20matching%2C%203D%20reconstruction%2C%20and%0ASLAM.%20This%20survey%20comprehensively%20reviews%20modality-based%20feature%20matching%2C%0Aexploring%20traditional%20handcrafted%20methods%20and%20emphasizing%20contemporary%20deep%0Alearning%20approaches%20across%20various%20modalities%2C%20including%20RGB%20images%2C%20depth%0Aimages%2C%203D%20point%20clouds%2C%20LiDAR%20scans%2C%20medical%20images%2C%20and%20vision-language%0Ainteractions.%20Traditional%20methods%2C%20leveraging%20detectors%20like%20Harris%20corners%20and%0Adescriptors%20such%20as%20SIFT%20and%20ORB%2C%20demonstrate%20robustness%20under%20moderate%0Aintra-modality%20variations%20but%20struggle%20with%20significant%20modality%20gaps.%0AContemporary%20deep%20learning-based%20methods%2C%20exemplified%20by%20detector-free%0Astrategies%20like%20CNN-based%20SuperPoint%20and%20transformer-based%20LoFTR%2C%20substantially%0Aimprove%20robustness%20and%20adaptability%20across%20modalities.%20We%20highlight%0Amodality-aware%20advancements%2C%20such%20as%20geometric%20and%20depth-specific%20descriptors%0Afor%20depth%20images%2C%20sparse%20and%20dense%20learning%20methods%20for%203D%20point%20clouds%2C%0Aattention-enhanced%20neural%20networks%20for%20LiDAR%20scans%2C%20and%20specialized%20solutions%0Alike%20the%20MIND%20descriptor%20for%20complex%20medical%20image%20matching.%20Cross-modal%0Aapplications%2C%20particularly%20in%20medical%20image%20registration%20and%20vision-language%0Atasks%2C%20underscore%20the%20evolution%20of%20feature%20matching%20to%20handle%20increasingly%0Adiverse%20data%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-Aware%2520Feature%2520Matching%253A%2520A%2520Comprehensive%2520Review%2520of%2520Single-%2520and%250A%2520%2520Cross-Modality%2520Techniques%26entry.906535625%3DWeide%2520Liu%2520and%2520Wei%2520Zhou%2520and%2520Jun%2520Liu%2520and%2520Ping%2520Hu%2520and%2520Jun%2520Cheng%2520and%2520Jungong%2520Han%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520Feature%2520matching%2520is%2520a%2520cornerstone%2520task%2520in%2520computer%2520vision%252C%2520essential%2520for%250Aapplications%2520such%2520as%2520image%2520retrieval%252C%2520stereo%2520matching%252C%25203D%2520reconstruction%252C%2520and%250ASLAM.%2520This%2520survey%2520comprehensively%2520reviews%2520modality-based%2520feature%2520matching%252C%250Aexploring%2520traditional%2520handcrafted%2520methods%2520and%2520emphasizing%2520contemporary%2520deep%250Alearning%2520approaches%2520across%2520various%2520modalities%252C%2520including%2520RGB%2520images%252C%2520depth%250Aimages%252C%25203D%2520point%2520clouds%252C%2520LiDAR%2520scans%252C%2520medical%2520images%252C%2520and%2520vision-language%250Ainteractions.%2520Traditional%2520methods%252C%2520leveraging%2520detectors%2520like%2520Harris%2520corners%2520and%250Adescriptors%2520such%2520as%2520SIFT%2520and%2520ORB%252C%2520demonstrate%2520robustness%2520under%2520moderate%250Aintra-modality%2520variations%2520but%2520struggle%2520with%2520significant%2520modality%2520gaps.%250AContemporary%2520deep%2520learning-based%2520methods%252C%2520exemplified%2520by%2520detector-free%250Astrategies%2520like%2520CNN-based%2520SuperPoint%2520and%2520transformer-based%2520LoFTR%252C%2520substantially%250Aimprove%2520robustness%2520and%2520adaptability%2520across%2520modalities.%2520We%2520highlight%250Amodality-aware%2520advancements%252C%2520such%2520as%2520geometric%2520and%2520depth-specific%2520descriptors%250Afor%2520depth%2520images%252C%2520sparse%2520and%2520dense%2520learning%2520methods%2520for%25203D%2520point%2520clouds%252C%250Aattention-enhanced%2520neural%2520networks%2520for%2520LiDAR%2520scans%252C%2520and%2520specialized%2520solutions%250Alike%2520the%2520MIND%2520descriptor%2520for%2520complex%2520medical%2520image%2520matching.%2520Cross-modal%250Aapplications%252C%2520particularly%2520in%2520medical%2520image%2520registration%2520and%2520vision-language%250Atasks%252C%2520underscore%2520the%2520evolution%2520of%2520feature%2520matching%2520to%2520handle%2520increasingly%250Adiverse%2520data%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-Aware%20Feature%20Matching%3A%20A%20Comprehensive%20Review%20of%20Single-%20and%0A%20%20Cross-Modality%20Techniques&entry.906535625=Weide%20Liu%20and%20Wei%20Zhou%20and%20Jun%20Liu%20and%20Ping%20Hu%20and%20Jun%20Cheng%20and%20Jungong%20Han%20and%20Weisi%20Lin&entry.1292438233=%20%20Feature%20matching%20is%20a%20cornerstone%20task%20in%20computer%20vision%2C%20essential%20for%0Aapplications%20such%20as%20image%20retrieval%2C%20stereo%20matching%2C%203D%20reconstruction%2C%20and%0ASLAM.%20This%20survey%20comprehensively%20reviews%20modality-based%20feature%20matching%2C%0Aexploring%20traditional%20handcrafted%20methods%20and%20emphasizing%20contemporary%20deep%0Alearning%20approaches%20across%20various%20modalities%2C%20including%20RGB%20images%2C%20depth%0Aimages%2C%203D%20point%20clouds%2C%20LiDAR%20scans%2C%20medical%20images%2C%20and%20vision-language%0Ainteractions.%20Traditional%20methods%2C%20leveraging%20detectors%20like%20Harris%20corners%20and%0Adescriptors%20such%20as%20SIFT%20and%20ORB%2C%20demonstrate%20robustness%20under%20moderate%0Aintra-modality%20variations%20but%20struggle%20with%20significant%20modality%20gaps.%0AContemporary%20deep%20learning-based%20methods%2C%20exemplified%20by%20detector-free%0Astrategies%20like%20CNN-based%20SuperPoint%20and%20transformer-based%20LoFTR%2C%20substantially%0Aimprove%20robustness%20and%20adaptability%20across%20modalities.%20We%20highlight%0Amodality-aware%20advancements%2C%20such%20as%20geometric%20and%20depth-specific%20descriptors%0Afor%20depth%20images%2C%20sparse%20and%20dense%20learning%20methods%20for%203D%20point%20clouds%2C%0Aattention-enhanced%20neural%20networks%20for%20LiDAR%20scans%2C%20and%20specialized%20solutions%0Alike%20the%20MIND%20descriptor%20for%20complex%20medical%20image%20matching.%20Cross-modal%0Aapplications%2C%20particularly%20in%20medical%20image%20registration%20and%20vision-language%0Atasks%2C%20underscore%20the%20evolution%20of%20feature%20matching%20to%20handle%20increasingly%0Adiverse%20data%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22791v1&entry.124074799=Read"},
{"title": "Language Driven Occupancy Prediction", "author": "Zhu Yu and Bowen Pang and Lizhe Liu and Runmin Zhang and Qiang Li and Si-Yuan Cao and Maochun Luo and Mingxia Chen and Sheng Yang and Hui-Liang Shen", "abstract": "  We introduce LOcc, an effective and generalizable framework for\nopen-vocabulary occupancy (OVO) prediction. Previous approaches typically\nsupervise the networks through coarse voxel-to-text correspondences via image\nfeatures as intermediates or noisy and sparse correspondences from voxel-based\nmodel-view projections. To alleviate the inaccurate supervision, we propose a\nsemantic transitive labeling pipeline to generate dense and fine-grained 3D\nlanguage occupancy ground truth. Our pipeline presents a feasible way to dig\ninto the valuable semantic information of images, transferring text labels from\nimages to LiDAR point clouds and ultimately to voxels, to establish precise\nvoxel-to-text correspondences. By replacing the original prediction head of\nsupervised occupancy models with a geometry head for binary occupancy states\nand a language head for language features, LOcc effectively uses the generated\nlanguage ground truth to guide the learning of 3D language volume. Through\nextensive experiments, we demonstrate that our transitive semantic labeling\npipeline can produce more accurate pseudo-labeled ground truth, diminishing\nlabor-intensive human annotations. Additionally, we validate LOcc across\nvarious architectures, where all models consistently outperform\nstate-of-the-art zero-shot occupancy prediction approaches on the\nOcc3D-nuScenes dataset.\n", "link": "http://arxiv.org/abs/2411.16072v3", "date": "2025-07-30", "relevancy": 2.8749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Driven%20Occupancy%20Prediction&body=Title%3A%20Language%20Driven%20Occupancy%20Prediction%0AAuthor%3A%20Zhu%20Yu%20and%20Bowen%20Pang%20and%20Lizhe%20Liu%20and%20Runmin%20Zhang%20and%20Qiang%20Li%20and%20Si-Yuan%20Cao%20and%20Maochun%20Luo%20and%20Mingxia%20Chen%20and%20Sheng%20Yang%20and%20Hui-Liang%20Shen%0AAbstract%3A%20%20%20We%20introduce%20LOcc%2C%20an%20effective%20and%20generalizable%20framework%20for%0Aopen-vocabulary%20occupancy%20%28OVO%29%20prediction.%20Previous%20approaches%20typically%0Asupervise%20the%20networks%20through%20coarse%20voxel-to-text%20correspondences%20via%20image%0Afeatures%20as%20intermediates%20or%20noisy%20and%20sparse%20correspondences%20from%20voxel-based%0Amodel-view%20projections.%20To%20alleviate%20the%20inaccurate%20supervision%2C%20we%20propose%20a%0Asemantic%20transitive%20labeling%20pipeline%20to%20generate%20dense%20and%20fine-grained%203D%0Alanguage%20occupancy%20ground%20truth.%20Our%20pipeline%20presents%20a%20feasible%20way%20to%20dig%0Ainto%20the%20valuable%20semantic%20information%20of%20images%2C%20transferring%20text%20labels%20from%0Aimages%20to%20LiDAR%20point%20clouds%20and%20ultimately%20to%20voxels%2C%20to%20establish%20precise%0Avoxel-to-text%20correspondences.%20By%20replacing%20the%20original%20prediction%20head%20of%0Asupervised%20occupancy%20models%20with%20a%20geometry%20head%20for%20binary%20occupancy%20states%0Aand%20a%20language%20head%20for%20language%20features%2C%20LOcc%20effectively%20uses%20the%20generated%0Alanguage%20ground%20truth%20to%20guide%20the%20learning%20of%203D%20language%20volume.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20our%20transitive%20semantic%20labeling%0Apipeline%20can%20produce%20more%20accurate%20pseudo-labeled%20ground%20truth%2C%20diminishing%0Alabor-intensive%20human%20annotations.%20Additionally%2C%20we%20validate%20LOcc%20across%0Avarious%20architectures%2C%20where%20all%20models%20consistently%20outperform%0Astate-of-the-art%20zero-shot%20occupancy%20prediction%20approaches%20on%20the%0AOcc3D-nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16072v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Driven%2520Occupancy%2520Prediction%26entry.906535625%3DZhu%2520Yu%2520and%2520Bowen%2520Pang%2520and%2520Lizhe%2520Liu%2520and%2520Runmin%2520Zhang%2520and%2520Qiang%2520Li%2520and%2520Si-Yuan%2520Cao%2520and%2520Maochun%2520Luo%2520and%2520Mingxia%2520Chen%2520and%2520Sheng%2520Yang%2520and%2520Hui-Liang%2520Shen%26entry.1292438233%3D%2520%2520We%2520introduce%2520LOcc%252C%2520an%2520effective%2520and%2520generalizable%2520framework%2520for%250Aopen-vocabulary%2520occupancy%2520%2528OVO%2529%2520prediction.%2520Previous%2520approaches%2520typically%250Asupervise%2520the%2520networks%2520through%2520coarse%2520voxel-to-text%2520correspondences%2520via%2520image%250Afeatures%2520as%2520intermediates%2520or%2520noisy%2520and%2520sparse%2520correspondences%2520from%2520voxel-based%250Amodel-view%2520projections.%2520To%2520alleviate%2520the%2520inaccurate%2520supervision%252C%2520we%2520propose%2520a%250Asemantic%2520transitive%2520labeling%2520pipeline%2520to%2520generate%2520dense%2520and%2520fine-grained%25203D%250Alanguage%2520occupancy%2520ground%2520truth.%2520Our%2520pipeline%2520presents%2520a%2520feasible%2520way%2520to%2520dig%250Ainto%2520the%2520valuable%2520semantic%2520information%2520of%2520images%252C%2520transferring%2520text%2520labels%2520from%250Aimages%2520to%2520LiDAR%2520point%2520clouds%2520and%2520ultimately%2520to%2520voxels%252C%2520to%2520establish%2520precise%250Avoxel-to-text%2520correspondences.%2520By%2520replacing%2520the%2520original%2520prediction%2520head%2520of%250Asupervised%2520occupancy%2520models%2520with%2520a%2520geometry%2520head%2520for%2520binary%2520occupancy%2520states%250Aand%2520a%2520language%2520head%2520for%2520language%2520features%252C%2520LOcc%2520effectively%2520uses%2520the%2520generated%250Alanguage%2520ground%2520truth%2520to%2520guide%2520the%2520learning%2520of%25203D%2520language%2520volume.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520transitive%2520semantic%2520labeling%250Apipeline%2520can%2520produce%2520more%2520accurate%2520pseudo-labeled%2520ground%2520truth%252C%2520diminishing%250Alabor-intensive%2520human%2520annotations.%2520Additionally%252C%2520we%2520validate%2520LOcc%2520across%250Avarious%2520architectures%252C%2520where%2520all%2520models%2520consistently%2520outperform%250Astate-of-the-art%2520zero-shot%2520occupancy%2520prediction%2520approaches%2520on%2520the%250AOcc3D-nuScenes%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16072v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Driven%20Occupancy%20Prediction&entry.906535625=Zhu%20Yu%20and%20Bowen%20Pang%20and%20Lizhe%20Liu%20and%20Runmin%20Zhang%20and%20Qiang%20Li%20and%20Si-Yuan%20Cao%20and%20Maochun%20Luo%20and%20Mingxia%20Chen%20and%20Sheng%20Yang%20and%20Hui-Liang%20Shen&entry.1292438233=%20%20We%20introduce%20LOcc%2C%20an%20effective%20and%20generalizable%20framework%20for%0Aopen-vocabulary%20occupancy%20%28OVO%29%20prediction.%20Previous%20approaches%20typically%0Asupervise%20the%20networks%20through%20coarse%20voxel-to-text%20correspondences%20via%20image%0Afeatures%20as%20intermediates%20or%20noisy%20and%20sparse%20correspondences%20from%20voxel-based%0Amodel-view%20projections.%20To%20alleviate%20the%20inaccurate%20supervision%2C%20we%20propose%20a%0Asemantic%20transitive%20labeling%20pipeline%20to%20generate%20dense%20and%20fine-grained%203D%0Alanguage%20occupancy%20ground%20truth.%20Our%20pipeline%20presents%20a%20feasible%20way%20to%20dig%0Ainto%20the%20valuable%20semantic%20information%20of%20images%2C%20transferring%20text%20labels%20from%0Aimages%20to%20LiDAR%20point%20clouds%20and%20ultimately%20to%20voxels%2C%20to%20establish%20precise%0Avoxel-to-text%20correspondences.%20By%20replacing%20the%20original%20prediction%20head%20of%0Asupervised%20occupancy%20models%20with%20a%20geometry%20head%20for%20binary%20occupancy%20states%0Aand%20a%20language%20head%20for%20language%20features%2C%20LOcc%20effectively%20uses%20the%20generated%0Alanguage%20ground%20truth%20to%20guide%20the%20learning%20of%203D%20language%20volume.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20that%20our%20transitive%20semantic%20labeling%0Apipeline%20can%20produce%20more%20accurate%20pseudo-labeled%20ground%20truth%2C%20diminishing%0Alabor-intensive%20human%20annotations.%20Additionally%2C%20we%20validate%20LOcc%20across%0Avarious%20architectures%2C%20where%20all%20models%20consistently%20outperform%0Astate-of-the-art%20zero-shot%20occupancy%20prediction%20approaches%20on%20the%0AOcc3D-nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16072v3&entry.124074799=Read"},
{"title": "Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints", "author": "Thuy Tran and Ruochen Chen and Shaifali Parashar", "abstract": "  Shape-from-Template (SfT) refers to the class of methods that reconstruct the\n3D shape of a deforming object from images/videos using a 3D template.\nTraditional SfT methods require point correspondences between images and the\ntexture of the 3D template in order to reconstruct 3D shapes from images/videos\nin real time. Their performance severely degrades when encountered with severe\nocclusions in the images because of the unavailability of correspondences. In\ncontrast, modern SfT methods use a correspondence-free approach by\nincorporating deep neural networks to reconstruct 3D objects, thus requiring\nhuge amounts of data for supervision. Recent advances use a fully unsupervised\nor self-supervised approach by combining differentiable physics and graphics to\ndeform 3D template to match input images. In this paper, we propose an\nunsupervised SfT which uses only image observations: color features, gradients\nand silhouettes along with a mesh inextensibility constraint to reconstruct at\na $400\\times$ faster pace than (best-performing) unsupervised SfT. Moreover,\nwhen it comes to generating finer details and severe occlusions, our method\noutperforms the existing methodologies by a large margin. Code is available at\nhttps://github.com/dvttran/nsft.\n", "link": "http://arxiv.org/abs/2507.22699v1", "date": "2025-07-30", "relevancy": 2.8727, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6089}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5598}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-Guided%20Shape-from-Template%20Using%20Mesh%20Inextensibility%20Constraints&body=Title%3A%20Image-Guided%20Shape-from-Template%20Using%20Mesh%20Inextensibility%20Constraints%0AAuthor%3A%20Thuy%20Tran%20and%20Ruochen%20Chen%20and%20Shaifali%20Parashar%0AAbstract%3A%20%20%20Shape-from-Template%20%28SfT%29%20refers%20to%20the%20class%20of%20methods%20that%20reconstruct%20the%0A3D%20shape%20of%20a%20deforming%20object%20from%20images/videos%20using%20a%203D%20template.%0ATraditional%20SfT%20methods%20require%20point%20correspondences%20between%20images%20and%20the%0Atexture%20of%20the%203D%20template%20in%20order%20to%20reconstruct%203D%20shapes%20from%20images/videos%0Ain%20real%20time.%20Their%20performance%20severely%20degrades%20when%20encountered%20with%20severe%0Aocclusions%20in%20the%20images%20because%20of%20the%20unavailability%20of%20correspondences.%20In%0Acontrast%2C%20modern%20SfT%20methods%20use%20a%20correspondence-free%20approach%20by%0Aincorporating%20deep%20neural%20networks%20to%20reconstruct%203D%20objects%2C%20thus%20requiring%0Ahuge%20amounts%20of%20data%20for%20supervision.%20Recent%20advances%20use%20a%20fully%20unsupervised%0Aor%20self-supervised%20approach%20by%20combining%20differentiable%20physics%20and%20graphics%20to%0Adeform%203D%20template%20to%20match%20input%20images.%20In%20this%20paper%2C%20we%20propose%20an%0Aunsupervised%20SfT%20which%20uses%20only%20image%20observations%3A%20color%20features%2C%20gradients%0Aand%20silhouettes%20along%20with%20a%20mesh%20inextensibility%20constraint%20to%20reconstruct%20at%0Aa%20%24400%5Ctimes%24%20faster%20pace%20than%20%28best-performing%29%20unsupervised%20SfT.%20Moreover%2C%0Awhen%20it%20comes%20to%20generating%20finer%20details%20and%20severe%20occlusions%2C%20our%20method%0Aoutperforms%20the%20existing%20methodologies%20by%20a%20large%20margin.%20Code%20is%20available%20at%0Ahttps%3A//github.com/dvttran/nsft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-Guided%2520Shape-from-Template%2520Using%2520Mesh%2520Inextensibility%2520Constraints%26entry.906535625%3DThuy%2520Tran%2520and%2520Ruochen%2520Chen%2520and%2520Shaifali%2520Parashar%26entry.1292438233%3D%2520%2520Shape-from-Template%2520%2528SfT%2529%2520refers%2520to%2520the%2520class%2520of%2520methods%2520that%2520reconstruct%2520the%250A3D%2520shape%2520of%2520a%2520deforming%2520object%2520from%2520images/videos%2520using%2520a%25203D%2520template.%250ATraditional%2520SfT%2520methods%2520require%2520point%2520correspondences%2520between%2520images%2520and%2520the%250Atexture%2520of%2520the%25203D%2520template%2520in%2520order%2520to%2520reconstruct%25203D%2520shapes%2520from%2520images/videos%250Ain%2520real%2520time.%2520Their%2520performance%2520severely%2520degrades%2520when%2520encountered%2520with%2520severe%250Aocclusions%2520in%2520the%2520images%2520because%2520of%2520the%2520unavailability%2520of%2520correspondences.%2520In%250Acontrast%252C%2520modern%2520SfT%2520methods%2520use%2520a%2520correspondence-free%2520approach%2520by%250Aincorporating%2520deep%2520neural%2520networks%2520to%2520reconstruct%25203D%2520objects%252C%2520thus%2520requiring%250Ahuge%2520amounts%2520of%2520data%2520for%2520supervision.%2520Recent%2520advances%2520use%2520a%2520fully%2520unsupervised%250Aor%2520self-supervised%2520approach%2520by%2520combining%2520differentiable%2520physics%2520and%2520graphics%2520to%250Adeform%25203D%2520template%2520to%2520match%2520input%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aunsupervised%2520SfT%2520which%2520uses%2520only%2520image%2520observations%253A%2520color%2520features%252C%2520gradients%250Aand%2520silhouettes%2520along%2520with%2520a%2520mesh%2520inextensibility%2520constraint%2520to%2520reconstruct%2520at%250Aa%2520%2524400%255Ctimes%2524%2520faster%2520pace%2520than%2520%2528best-performing%2529%2520unsupervised%2520SfT.%2520Moreover%252C%250Awhen%2520it%2520comes%2520to%2520generating%2520finer%2520details%2520and%2520severe%2520occlusions%252C%2520our%2520method%250Aoutperforms%2520the%2520existing%2520methodologies%2520by%2520a%2520large%2520margin.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/dvttran/nsft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Guided%20Shape-from-Template%20Using%20Mesh%20Inextensibility%20Constraints&entry.906535625=Thuy%20Tran%20and%20Ruochen%20Chen%20and%20Shaifali%20Parashar&entry.1292438233=%20%20Shape-from-Template%20%28SfT%29%20refers%20to%20the%20class%20of%20methods%20that%20reconstruct%20the%0A3D%20shape%20of%20a%20deforming%20object%20from%20images/videos%20using%20a%203D%20template.%0ATraditional%20SfT%20methods%20require%20point%20correspondences%20between%20images%20and%20the%0Atexture%20of%20the%203D%20template%20in%20order%20to%20reconstruct%203D%20shapes%20from%20images/videos%0Ain%20real%20time.%20Their%20performance%20severely%20degrades%20when%20encountered%20with%20severe%0Aocclusions%20in%20the%20images%20because%20of%20the%20unavailability%20of%20correspondences.%20In%0Acontrast%2C%20modern%20SfT%20methods%20use%20a%20correspondence-free%20approach%20by%0Aincorporating%20deep%20neural%20networks%20to%20reconstruct%203D%20objects%2C%20thus%20requiring%0Ahuge%20amounts%20of%20data%20for%20supervision.%20Recent%20advances%20use%20a%20fully%20unsupervised%0Aor%20self-supervised%20approach%20by%20combining%20differentiable%20physics%20and%20graphics%20to%0Adeform%203D%20template%20to%20match%20input%20images.%20In%20this%20paper%2C%20we%20propose%20an%0Aunsupervised%20SfT%20which%20uses%20only%20image%20observations%3A%20color%20features%2C%20gradients%0Aand%20silhouettes%20along%20with%20a%20mesh%20inextensibility%20constraint%20to%20reconstruct%20at%0Aa%20%24400%5Ctimes%24%20faster%20pace%20than%20%28best-performing%29%20unsupervised%20SfT.%20Moreover%2C%0Awhen%20it%20comes%20to%20generating%20finer%20details%20and%20severe%20occlusions%2C%20our%20method%0Aoutperforms%20the%20existing%20methodologies%20by%20a%20large%20margin.%20Code%20is%20available%20at%0Ahttps%3A//github.com/dvttran/nsft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22699v1&entry.124074799=Read"},
{"title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation", "author": "Kaining Ying and Henghui Ding and Guanquan Jie and Yu-Gang Jiang", "abstract": "  Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,098 videos and 59,458 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.\n", "link": "http://arxiv.org/abs/2507.22886v1", "date": "2025-07-30", "relevancy": 2.8576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Omnimodal%20Expressions%20and%20Reasoning%20in%20Referring%20Audio-Visual%0A%20%20Segmentation&body=Title%3A%20Towards%20Omnimodal%20Expressions%20and%20Reasoning%20in%20Referring%20Audio-Visual%0A%20%20Segmentation%0AAuthor%3A%20Kaining%20Ying%20and%20Henghui%20Ding%20and%20Guanquan%20Jie%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Referring%20audio-visual%20segmentation%20%28RAVS%29%20has%20recently%20seen%20significant%0Aadvancements%2C%20yet%20challenges%20remain%20in%20integrating%20multimodal%20information%20and%0Adeeply%20understanding%20and%20reasoning%20about%20audiovisual%20content.%20To%20extend%20the%0Aboundaries%20of%20RAVS%20and%20facilitate%20future%20research%20in%20this%20field%2C%20we%20propose%0AOmnimodal%20Referring%20Audio-Visual%20Segmentation%20%28OmniAVS%29%2C%20a%20new%20dataset%0Acontaining%202%2C098%20videos%20and%2059%2C458%20multimodal%20referring%20expressions.%20OmniAVS%0Astands%20out%20with%20three%20key%20innovations%3A%20%281%29%208%20types%20of%20multimodal%20expressions%0Athat%20flexibly%20combine%20text%2C%20speech%2C%20sound%2C%20and%20visual%20cues%3B%20%282%29%20an%20emphasis%20on%0Aunderstanding%20audio%20content%20beyond%20just%20detecting%20their%20presence%3B%20and%20%283%29%20the%0Ainclusion%20of%20complex%20reasoning%20and%20world%20knowledge%20in%20expressions.%20Furthermore%2C%0Awe%20introduce%20Omnimodal%20Instructed%20Segmentation%20Assistant%20%28OISA%29%2C%20to%20address%20the%0Achallenges%20of%20multimodal%20reasoning%20and%20fine-grained%20understanding%20of%0Aaudiovisual%20content%20in%20OmniAVS.%20OISA%20uses%20MLLM%20to%20comprehend%20complex%20cues%20and%0Aperform%20reasoning-based%20segmentation.%20Extensive%20experiments%20show%20that%20OISA%0Aoutperforms%20existing%20methods%20on%20OmniAVS%20and%20achieves%20competitive%20results%20on%0Aother%20related%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Omnimodal%2520Expressions%2520and%2520Reasoning%2520in%2520Referring%2520Audio-Visual%250A%2520%2520Segmentation%26entry.906535625%3DKaining%2520Ying%2520and%2520Henghui%2520Ding%2520and%2520Guanquan%2520Jie%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Referring%2520audio-visual%2520segmentation%2520%2528RAVS%2529%2520has%2520recently%2520seen%2520significant%250Aadvancements%252C%2520yet%2520challenges%2520remain%2520in%2520integrating%2520multimodal%2520information%2520and%250Adeeply%2520understanding%2520and%2520reasoning%2520about%2520audiovisual%2520content.%2520To%2520extend%2520the%250Aboundaries%2520of%2520RAVS%2520and%2520facilitate%2520future%2520research%2520in%2520this%2520field%252C%2520we%2520propose%250AOmnimodal%2520Referring%2520Audio-Visual%2520Segmentation%2520%2528OmniAVS%2529%252C%2520a%2520new%2520dataset%250Acontaining%25202%252C098%2520videos%2520and%252059%252C458%2520multimodal%2520referring%2520expressions.%2520OmniAVS%250Astands%2520out%2520with%2520three%2520key%2520innovations%253A%2520%25281%2529%25208%2520types%2520of%2520multimodal%2520expressions%250Athat%2520flexibly%2520combine%2520text%252C%2520speech%252C%2520sound%252C%2520and%2520visual%2520cues%253B%2520%25282%2529%2520an%2520emphasis%2520on%250Aunderstanding%2520audio%2520content%2520beyond%2520just%2520detecting%2520their%2520presence%253B%2520and%2520%25283%2529%2520the%250Ainclusion%2520of%2520complex%2520reasoning%2520and%2520world%2520knowledge%2520in%2520expressions.%2520Furthermore%252C%250Awe%2520introduce%2520Omnimodal%2520Instructed%2520Segmentation%2520Assistant%2520%2528OISA%2529%252C%2520to%2520address%2520the%250Achallenges%2520of%2520multimodal%2520reasoning%2520and%2520fine-grained%2520understanding%2520of%250Aaudiovisual%2520content%2520in%2520OmniAVS.%2520OISA%2520uses%2520MLLM%2520to%2520comprehend%2520complex%2520cues%2520and%250Aperform%2520reasoning-based%2520segmentation.%2520Extensive%2520experiments%2520show%2520that%2520OISA%250Aoutperforms%2520existing%2520methods%2520on%2520OmniAVS%2520and%2520achieves%2520competitive%2520results%2520on%250Aother%2520related%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Omnimodal%20Expressions%20and%20Reasoning%20in%20Referring%20Audio-Visual%0A%20%20Segmentation&entry.906535625=Kaining%20Ying%20and%20Henghui%20Ding%20and%20Guanquan%20Jie%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Referring%20audio-visual%20segmentation%20%28RAVS%29%20has%20recently%20seen%20significant%0Aadvancements%2C%20yet%20challenges%20remain%20in%20integrating%20multimodal%20information%20and%0Adeeply%20understanding%20and%20reasoning%20about%20audiovisual%20content.%20To%20extend%20the%0Aboundaries%20of%20RAVS%20and%20facilitate%20future%20research%20in%20this%20field%2C%20we%20propose%0AOmnimodal%20Referring%20Audio-Visual%20Segmentation%20%28OmniAVS%29%2C%20a%20new%20dataset%0Acontaining%202%2C098%20videos%20and%2059%2C458%20multimodal%20referring%20expressions.%20OmniAVS%0Astands%20out%20with%20three%20key%20innovations%3A%20%281%29%208%20types%20of%20multimodal%20expressions%0Athat%20flexibly%20combine%20text%2C%20speech%2C%20sound%2C%20and%20visual%20cues%3B%20%282%29%20an%20emphasis%20on%0Aunderstanding%20audio%20content%20beyond%20just%20detecting%20their%20presence%3B%20and%20%283%29%20the%0Ainclusion%20of%20complex%20reasoning%20and%20world%20knowledge%20in%20expressions.%20Furthermore%2C%0Awe%20introduce%20Omnimodal%20Instructed%20Segmentation%20Assistant%20%28OISA%29%2C%20to%20address%20the%0Achallenges%20of%20multimodal%20reasoning%20and%20fine-grained%20understanding%20of%0Aaudiovisual%20content%20in%20OmniAVS.%20OISA%20uses%20MLLM%20to%20comprehend%20complex%20cues%20and%0Aperform%20reasoning-based%20segmentation.%20Extensive%20experiments%20show%20that%20OISA%0Aoutperforms%20existing%20methods%20on%20OmniAVS%20and%20achieves%20competitive%20results%20on%0Aother%20related%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22886v1&entry.124074799=Read"},
{"title": "A Dual-Feature Extractor Framework for Accurate Back Depth and Spine\n  Morphology Estimation from Monocular RGB Images", "author": "Yuxin Wei and Yue Zhang and Moxin Zhao and Chang Shi and Jason P. Y. Cheung and Teng Zhang and Nan Meng", "abstract": "  Scoliosis is a prevalent condition that impacts both physical health and\nappearance, with adolescent idiopathic scoliosis (AIS) being the most common\nform. Currently, the main AIS assessment tool, X-rays, poses significant\nlimitations, including radiation exposure and limited accessibility in poor and\nremote areas. To address this problem, the current solutions are using RGB\nimages to analyze spine morphology. However, RGB images are highly susceptible\nto environmental factors, such as lighting conditions, compromising model\nstability and generalizability. Therefore, in this study, we propose a novel\npipeline to accurately estimate the depth information of the unclothed back,\ncompensating for the limitations of 2D information, and then estimate spine\nmorphology by integrating both depth and surface information. To capture the\nsubtle depth variations of the back surface with precision, we design an\nadaptive multiscale feature learning network named Grid-Aware Multiscale\nAdaptive Network (GAMA-Net). This model uses dual encoders to extract both\npatch-level and global features, which are then interacted by the Patch-Based\nHybrid Attention (PBHA) module. The Adaptive Multiscale Feature Fusion (AMFF)\nmodule is used to dynamically fuse information in the decoder. As a result, our\ndepth estimation model achieves remarkable accuracy across three different\nevaluation metrics, with scores of nearly 78.2%, 93.6%, and 97.5%,\nrespectively. To further validate the effectiveness of the predicted depth, we\nintegrate both surface and depth information for spine morphology estimation.\nThis integrated approach enhances the accuracy of spine curve generation,\nachieving an impressive performance of up to 97%.\n", "link": "http://arxiv.org/abs/2507.22691v1", "date": "2025-07-30", "relevancy": 2.8564, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5955}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5656}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dual-Feature%20Extractor%20Framework%20for%20Accurate%20Back%20Depth%20and%20Spine%0A%20%20Morphology%20Estimation%20from%20Monocular%20RGB%20Images&body=Title%3A%20A%20Dual-Feature%20Extractor%20Framework%20for%20Accurate%20Back%20Depth%20and%20Spine%0A%20%20Morphology%20Estimation%20from%20Monocular%20RGB%20Images%0AAuthor%3A%20Yuxin%20Wei%20and%20Yue%20Zhang%20and%20Moxin%20Zhao%20and%20Chang%20Shi%20and%20Jason%20P.%20Y.%20Cheung%20and%20Teng%20Zhang%20and%20Nan%20Meng%0AAbstract%3A%20%20%20Scoliosis%20is%20a%20prevalent%20condition%20that%20impacts%20both%20physical%20health%20and%0Aappearance%2C%20with%20adolescent%20idiopathic%20scoliosis%20%28AIS%29%20being%20the%20most%20common%0Aform.%20Currently%2C%20the%20main%20AIS%20assessment%20tool%2C%20X-rays%2C%20poses%20significant%0Alimitations%2C%20including%20radiation%20exposure%20and%20limited%20accessibility%20in%20poor%20and%0Aremote%20areas.%20To%20address%20this%20problem%2C%20the%20current%20solutions%20are%20using%20RGB%0Aimages%20to%20analyze%20spine%20morphology.%20However%2C%20RGB%20images%20are%20highly%20susceptible%0Ato%20environmental%20factors%2C%20such%20as%20lighting%20conditions%2C%20compromising%20model%0Astability%20and%20generalizability.%20Therefore%2C%20in%20this%20study%2C%20we%20propose%20a%20novel%0Apipeline%20to%20accurately%20estimate%20the%20depth%20information%20of%20the%20unclothed%20back%2C%0Acompensating%20for%20the%20limitations%20of%202D%20information%2C%20and%20then%20estimate%20spine%0Amorphology%20by%20integrating%20both%20depth%20and%20surface%20information.%20To%20capture%20the%0Asubtle%20depth%20variations%20of%20the%20back%20surface%20with%20precision%2C%20we%20design%20an%0Aadaptive%20multiscale%20feature%20learning%20network%20named%20Grid-Aware%20Multiscale%0AAdaptive%20Network%20%28GAMA-Net%29.%20This%20model%20uses%20dual%20encoders%20to%20extract%20both%0Apatch-level%20and%20global%20features%2C%20which%20are%20then%20interacted%20by%20the%20Patch-Based%0AHybrid%20Attention%20%28PBHA%29%20module.%20The%20Adaptive%20Multiscale%20Feature%20Fusion%20%28AMFF%29%0Amodule%20is%20used%20to%20dynamically%20fuse%20information%20in%20the%20decoder.%20As%20a%20result%2C%20our%0Adepth%20estimation%20model%20achieves%20remarkable%20accuracy%20across%20three%20different%0Aevaluation%20metrics%2C%20with%20scores%20of%20nearly%2078.2%25%2C%2093.6%25%2C%20and%2097.5%25%2C%0Arespectively.%20To%20further%20validate%20the%20effectiveness%20of%20the%20predicted%20depth%2C%20we%0Aintegrate%20both%20surface%20and%20depth%20information%20for%20spine%20morphology%20estimation.%0AThis%20integrated%20approach%20enhances%20the%20accuracy%20of%20spine%20curve%20generation%2C%0Aachieving%20an%20impressive%20performance%20of%20up%20to%2097%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dual-Feature%2520Extractor%2520Framework%2520for%2520Accurate%2520Back%2520Depth%2520and%2520Spine%250A%2520%2520Morphology%2520Estimation%2520from%2520Monocular%2520RGB%2520Images%26entry.906535625%3DYuxin%2520Wei%2520and%2520Yue%2520Zhang%2520and%2520Moxin%2520Zhao%2520and%2520Chang%2520Shi%2520and%2520Jason%2520P.%2520Y.%2520Cheung%2520and%2520Teng%2520Zhang%2520and%2520Nan%2520Meng%26entry.1292438233%3D%2520%2520Scoliosis%2520is%2520a%2520prevalent%2520condition%2520that%2520impacts%2520both%2520physical%2520health%2520and%250Aappearance%252C%2520with%2520adolescent%2520idiopathic%2520scoliosis%2520%2528AIS%2529%2520being%2520the%2520most%2520common%250Aform.%2520Currently%252C%2520the%2520main%2520AIS%2520assessment%2520tool%252C%2520X-rays%252C%2520poses%2520significant%250Alimitations%252C%2520including%2520radiation%2520exposure%2520and%2520limited%2520accessibility%2520in%2520poor%2520and%250Aremote%2520areas.%2520To%2520address%2520this%2520problem%252C%2520the%2520current%2520solutions%2520are%2520using%2520RGB%250Aimages%2520to%2520analyze%2520spine%2520morphology.%2520However%252C%2520RGB%2520images%2520are%2520highly%2520susceptible%250Ato%2520environmental%2520factors%252C%2520such%2520as%2520lighting%2520conditions%252C%2520compromising%2520model%250Astability%2520and%2520generalizability.%2520Therefore%252C%2520in%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250Apipeline%2520to%2520accurately%2520estimate%2520the%2520depth%2520information%2520of%2520the%2520unclothed%2520back%252C%250Acompensating%2520for%2520the%2520limitations%2520of%25202D%2520information%252C%2520and%2520then%2520estimate%2520spine%250Amorphology%2520by%2520integrating%2520both%2520depth%2520and%2520surface%2520information.%2520To%2520capture%2520the%250Asubtle%2520depth%2520variations%2520of%2520the%2520back%2520surface%2520with%2520precision%252C%2520we%2520design%2520an%250Aadaptive%2520multiscale%2520feature%2520learning%2520network%2520named%2520Grid-Aware%2520Multiscale%250AAdaptive%2520Network%2520%2528GAMA-Net%2529.%2520This%2520model%2520uses%2520dual%2520encoders%2520to%2520extract%2520both%250Apatch-level%2520and%2520global%2520features%252C%2520which%2520are%2520then%2520interacted%2520by%2520the%2520Patch-Based%250AHybrid%2520Attention%2520%2528PBHA%2529%2520module.%2520The%2520Adaptive%2520Multiscale%2520Feature%2520Fusion%2520%2528AMFF%2529%250Amodule%2520is%2520used%2520to%2520dynamically%2520fuse%2520information%2520in%2520the%2520decoder.%2520As%2520a%2520result%252C%2520our%250Adepth%2520estimation%2520model%2520achieves%2520remarkable%2520accuracy%2520across%2520three%2520different%250Aevaluation%2520metrics%252C%2520with%2520scores%2520of%2520nearly%252078.2%2525%252C%252093.6%2525%252C%2520and%252097.5%2525%252C%250Arespectively.%2520To%2520further%2520validate%2520the%2520effectiveness%2520of%2520the%2520predicted%2520depth%252C%2520we%250Aintegrate%2520both%2520surface%2520and%2520depth%2520information%2520for%2520spine%2520morphology%2520estimation.%250AThis%2520integrated%2520approach%2520enhances%2520the%2520accuracy%2520of%2520spine%2520curve%2520generation%252C%250Aachieving%2520an%2520impressive%2520performance%2520of%2520up%2520to%252097%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dual-Feature%20Extractor%20Framework%20for%20Accurate%20Back%20Depth%20and%20Spine%0A%20%20Morphology%20Estimation%20from%20Monocular%20RGB%20Images&entry.906535625=Yuxin%20Wei%20and%20Yue%20Zhang%20and%20Moxin%20Zhao%20and%20Chang%20Shi%20and%20Jason%20P.%20Y.%20Cheung%20and%20Teng%20Zhang%20and%20Nan%20Meng&entry.1292438233=%20%20Scoliosis%20is%20a%20prevalent%20condition%20that%20impacts%20both%20physical%20health%20and%0Aappearance%2C%20with%20adolescent%20idiopathic%20scoliosis%20%28AIS%29%20being%20the%20most%20common%0Aform.%20Currently%2C%20the%20main%20AIS%20assessment%20tool%2C%20X-rays%2C%20poses%20significant%0Alimitations%2C%20including%20radiation%20exposure%20and%20limited%20accessibility%20in%20poor%20and%0Aremote%20areas.%20To%20address%20this%20problem%2C%20the%20current%20solutions%20are%20using%20RGB%0Aimages%20to%20analyze%20spine%20morphology.%20However%2C%20RGB%20images%20are%20highly%20susceptible%0Ato%20environmental%20factors%2C%20such%20as%20lighting%20conditions%2C%20compromising%20model%0Astability%20and%20generalizability.%20Therefore%2C%20in%20this%20study%2C%20we%20propose%20a%20novel%0Apipeline%20to%20accurately%20estimate%20the%20depth%20information%20of%20the%20unclothed%20back%2C%0Acompensating%20for%20the%20limitations%20of%202D%20information%2C%20and%20then%20estimate%20spine%0Amorphology%20by%20integrating%20both%20depth%20and%20surface%20information.%20To%20capture%20the%0Asubtle%20depth%20variations%20of%20the%20back%20surface%20with%20precision%2C%20we%20design%20an%0Aadaptive%20multiscale%20feature%20learning%20network%20named%20Grid-Aware%20Multiscale%0AAdaptive%20Network%20%28GAMA-Net%29.%20This%20model%20uses%20dual%20encoders%20to%20extract%20both%0Apatch-level%20and%20global%20features%2C%20which%20are%20then%20interacted%20by%20the%20Patch-Based%0AHybrid%20Attention%20%28PBHA%29%20module.%20The%20Adaptive%20Multiscale%20Feature%20Fusion%20%28AMFF%29%0Amodule%20is%20used%20to%20dynamically%20fuse%20information%20in%20the%20decoder.%20As%20a%20result%2C%20our%0Adepth%20estimation%20model%20achieves%20remarkable%20accuracy%20across%20three%20different%0Aevaluation%20metrics%2C%20with%20scores%20of%20nearly%2078.2%25%2C%2093.6%25%2C%20and%2097.5%25%2C%0Arespectively.%20To%20further%20validate%20the%20effectiveness%20of%20the%20predicted%20depth%2C%20we%0Aintegrate%20both%20surface%20and%20depth%20information%20for%20spine%20morphology%20estimation.%0AThis%20integrated%20approach%20enhances%20the%20accuracy%20of%20spine%20curve%20generation%2C%0Aachieving%20an%20impressive%20performance%20of%20up%20to%2097%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22691v1&entry.124074799=Read"},
{"title": "Subtyping Breast Lesions via Generative Augmentation based Long-tailed\n  Recognition in Ultrasound", "author": "Shijing Chen and Xinrui Zhou and Yuhao Wang and Yuhao Huang and Ao Chang and Dong Ni and Ruobing Huang", "abstract": "  Accurate identification of breast lesion subtypes can facilitate personalized\ntreatment and interventions. Ultrasound (US), as a safe and accessible imaging\nmodality, is extensively employed in breast abnormality screening and\ndiagnosis. However, the incidence of different subtypes exhibits a skewed\nlong-tailed distribution, posing significant challenges for automated\nrecognition. Generative augmentation provides a promising solution to rectify\ndata distribution. Inspired by this, we propose a dual-phase framework for\nlong-tailed classification that mitigates distributional bias through\nhigh-fidelity data synthesis while avoiding overuse that corrupts holistic\nperformance. The framework incorporates a reinforcement learning-driven\nadaptive sampler, dynamically calibrating synthetic-real data ratios by\ntraining a strategic multi-agent to compensate for scarcities of real data\nwhile ensuring stable discriminative capability. Furthermore, our\nclass-controllable synthetic network integrates a sketch-grounded perception\nbranch that harnesses anatomical priors to maintain distinctive class features\nwhile enabling annotation-free inference. Extensive experiments on an in-house\nlong-tailed and a public imbalanced breast US datasets demonstrate that our\nmethod achieves promising performance compared to state-of-the-art approaches.\nMore synthetic images can be found at\nhttps://github.com/Stinalalala/Breast-LT-GenAug.\n", "link": "http://arxiv.org/abs/2507.22568v1", "date": "2025-07-30", "relevancy": 2.7973, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5973}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subtyping%20Breast%20Lesions%20via%20Generative%20Augmentation%20based%20Long-tailed%0A%20%20Recognition%20in%20Ultrasound&body=Title%3A%20Subtyping%20Breast%20Lesions%20via%20Generative%20Augmentation%20based%20Long-tailed%0A%20%20Recognition%20in%20Ultrasound%0AAuthor%3A%20Shijing%20Chen%20and%20Xinrui%20Zhou%20and%20Yuhao%20Wang%20and%20Yuhao%20Huang%20and%20Ao%20Chang%20and%20Dong%20Ni%20and%20Ruobing%20Huang%0AAbstract%3A%20%20%20Accurate%20identification%20of%20breast%20lesion%20subtypes%20can%20facilitate%20personalized%0Atreatment%20and%20interventions.%20Ultrasound%20%28US%29%2C%20as%20a%20safe%20and%20accessible%20imaging%0Amodality%2C%20is%20extensively%20employed%20in%20breast%20abnormality%20screening%20and%0Adiagnosis.%20However%2C%20the%20incidence%20of%20different%20subtypes%20exhibits%20a%20skewed%0Along-tailed%20distribution%2C%20posing%20significant%20challenges%20for%20automated%0Arecognition.%20Generative%20augmentation%20provides%20a%20promising%20solution%20to%20rectify%0Adata%20distribution.%20Inspired%20by%20this%2C%20we%20propose%20a%20dual-phase%20framework%20for%0Along-tailed%20classification%20that%20mitigates%20distributional%20bias%20through%0Ahigh-fidelity%20data%20synthesis%20while%20avoiding%20overuse%20that%20corrupts%20holistic%0Aperformance.%20The%20framework%20incorporates%20a%20reinforcement%20learning-driven%0Aadaptive%20sampler%2C%20dynamically%20calibrating%20synthetic-real%20data%20ratios%20by%0Atraining%20a%20strategic%20multi-agent%20to%20compensate%20for%20scarcities%20of%20real%20data%0Awhile%20ensuring%20stable%20discriminative%20capability.%20Furthermore%2C%20our%0Aclass-controllable%20synthetic%20network%20integrates%20a%20sketch-grounded%20perception%0Abranch%20that%20harnesses%20anatomical%20priors%20to%20maintain%20distinctive%20class%20features%0Awhile%20enabling%20annotation-free%20inference.%20Extensive%20experiments%20on%20an%20in-house%0Along-tailed%20and%20a%20public%20imbalanced%20breast%20US%20datasets%20demonstrate%20that%20our%0Amethod%20achieves%20promising%20performance%20compared%20to%20state-of-the-art%20approaches.%0AMore%20synthetic%20images%20can%20be%20found%20at%0Ahttps%3A//github.com/Stinalalala/Breast-LT-GenAug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubtyping%2520Breast%2520Lesions%2520via%2520Generative%2520Augmentation%2520based%2520Long-tailed%250A%2520%2520Recognition%2520in%2520Ultrasound%26entry.906535625%3DShijing%2520Chen%2520and%2520Xinrui%2520Zhou%2520and%2520Yuhao%2520Wang%2520and%2520Yuhao%2520Huang%2520and%2520Ao%2520Chang%2520and%2520Dong%2520Ni%2520and%2520Ruobing%2520Huang%26entry.1292438233%3D%2520%2520Accurate%2520identification%2520of%2520breast%2520lesion%2520subtypes%2520can%2520facilitate%2520personalized%250Atreatment%2520and%2520interventions.%2520Ultrasound%2520%2528US%2529%252C%2520as%2520a%2520safe%2520and%2520accessible%2520imaging%250Amodality%252C%2520is%2520extensively%2520employed%2520in%2520breast%2520abnormality%2520screening%2520and%250Adiagnosis.%2520However%252C%2520the%2520incidence%2520of%2520different%2520subtypes%2520exhibits%2520a%2520skewed%250Along-tailed%2520distribution%252C%2520posing%2520significant%2520challenges%2520for%2520automated%250Arecognition.%2520Generative%2520augmentation%2520provides%2520a%2520promising%2520solution%2520to%2520rectify%250Adata%2520distribution.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520dual-phase%2520framework%2520for%250Along-tailed%2520classification%2520that%2520mitigates%2520distributional%2520bias%2520through%250Ahigh-fidelity%2520data%2520synthesis%2520while%2520avoiding%2520overuse%2520that%2520corrupts%2520holistic%250Aperformance.%2520The%2520framework%2520incorporates%2520a%2520reinforcement%2520learning-driven%250Aadaptive%2520sampler%252C%2520dynamically%2520calibrating%2520synthetic-real%2520data%2520ratios%2520by%250Atraining%2520a%2520strategic%2520multi-agent%2520to%2520compensate%2520for%2520scarcities%2520of%2520real%2520data%250Awhile%2520ensuring%2520stable%2520discriminative%2520capability.%2520Furthermore%252C%2520our%250Aclass-controllable%2520synthetic%2520network%2520integrates%2520a%2520sketch-grounded%2520perception%250Abranch%2520that%2520harnesses%2520anatomical%2520priors%2520to%2520maintain%2520distinctive%2520class%2520features%250Awhile%2520enabling%2520annotation-free%2520inference.%2520Extensive%2520experiments%2520on%2520an%2520in-house%250Along-tailed%2520and%2520a%2520public%2520imbalanced%2520breast%2520US%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520promising%2520performance%2520compared%2520to%2520state-of-the-art%2520approaches.%250AMore%2520synthetic%2520images%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Stinalalala/Breast-LT-GenAug.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subtyping%20Breast%20Lesions%20via%20Generative%20Augmentation%20based%20Long-tailed%0A%20%20Recognition%20in%20Ultrasound&entry.906535625=Shijing%20Chen%20and%20Xinrui%20Zhou%20and%20Yuhao%20Wang%20and%20Yuhao%20Huang%20and%20Ao%20Chang%20and%20Dong%20Ni%20and%20Ruobing%20Huang&entry.1292438233=%20%20Accurate%20identification%20of%20breast%20lesion%20subtypes%20can%20facilitate%20personalized%0Atreatment%20and%20interventions.%20Ultrasound%20%28US%29%2C%20as%20a%20safe%20and%20accessible%20imaging%0Amodality%2C%20is%20extensively%20employed%20in%20breast%20abnormality%20screening%20and%0Adiagnosis.%20However%2C%20the%20incidence%20of%20different%20subtypes%20exhibits%20a%20skewed%0Along-tailed%20distribution%2C%20posing%20significant%20challenges%20for%20automated%0Arecognition.%20Generative%20augmentation%20provides%20a%20promising%20solution%20to%20rectify%0Adata%20distribution.%20Inspired%20by%20this%2C%20we%20propose%20a%20dual-phase%20framework%20for%0Along-tailed%20classification%20that%20mitigates%20distributional%20bias%20through%0Ahigh-fidelity%20data%20synthesis%20while%20avoiding%20overuse%20that%20corrupts%20holistic%0Aperformance.%20The%20framework%20incorporates%20a%20reinforcement%20learning-driven%0Aadaptive%20sampler%2C%20dynamically%20calibrating%20synthetic-real%20data%20ratios%20by%0Atraining%20a%20strategic%20multi-agent%20to%20compensate%20for%20scarcities%20of%20real%20data%0Awhile%20ensuring%20stable%20discriminative%20capability.%20Furthermore%2C%20our%0Aclass-controllable%20synthetic%20network%20integrates%20a%20sketch-grounded%20perception%0Abranch%20that%20harnesses%20anatomical%20priors%20to%20maintain%20distinctive%20class%20features%0Awhile%20enabling%20annotation-free%20inference.%20Extensive%20experiments%20on%20an%20in-house%0Along-tailed%20and%20a%20public%20imbalanced%20breast%20US%20datasets%20demonstrate%20that%20our%0Amethod%20achieves%20promising%20performance%20compared%20to%20state-of-the-art%20approaches.%0AMore%20synthetic%20images%20can%20be%20found%20at%0Ahttps%3A//github.com/Stinalalala/Breast-LT-GenAug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22568v1&entry.124074799=Read"},
{"title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation", "author": "Junhong Min and Youngpil Jeon and Jimin Kim and Minyong Choi", "abstract": "  The pursuit of a generalizable stereo matching model, capable of performing\nwell across varying resolutions and disparity ranges without dataset-specific\nfine-tuning, has revealed a fundamental trade-off. Iterative local search\nmethods achieve high scores on constrained benchmarks, but their core mechanism\ninherently limits the global consistency required for true generalization.\nHowever, global matching architectures, while theoretically more robust, have\nhistorically been rendered infeasible by prohibitive computational and memory\ncosts. We resolve this dilemma with $S^2M^2$: a global matching architecture\nthat achieves state-of-the-art accuracy and high efficiency without relying on\ncost volume filtering or deep refinement stacks. Our design integrates a\nmulti-resolution transformer for robust long-range correspondence, trained with\na novel loss function that concentrates probability on feasible matches. This\napproach enables a more robust joint estimation of disparity, occlusion, and\nconfidence. $S^2M^2$ establishes a new state of the art on Middlebury v3 and\nETH3D benchmarks, significantly outperforming prior methods in most metrics\nwhile reconstructing high-quality details with competitive efficiency.\n", "link": "http://arxiv.org/abs/2507.13229v3", "date": "2025-07-30", "relevancy": 2.7957, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5617}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5595}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24S%5E2M%5E2%24%3A%20Scalable%20Stereo%20Matching%20Model%20for%20Reliable%20Depth%20Estimation&body=Title%3A%20%24S%5E2M%5E2%24%3A%20Scalable%20Stereo%20Matching%20Model%20for%20Reliable%20Depth%20Estimation%0AAuthor%3A%20Junhong%20Min%20and%20Youngpil%20Jeon%20and%20Jimin%20Kim%20and%20Minyong%20Choi%0AAbstract%3A%20%20%20The%20pursuit%20of%20a%20generalizable%20stereo%20matching%20model%2C%20capable%20of%20performing%0Awell%20across%20varying%20resolutions%20and%20disparity%20ranges%20without%20dataset-specific%0Afine-tuning%2C%20has%20revealed%20a%20fundamental%20trade-off.%20Iterative%20local%20search%0Amethods%20achieve%20high%20scores%20on%20constrained%20benchmarks%2C%20but%20their%20core%20mechanism%0Ainherently%20limits%20the%20global%20consistency%20required%20for%20true%20generalization.%0AHowever%2C%20global%20matching%20architectures%2C%20while%20theoretically%20more%20robust%2C%20have%0Ahistorically%20been%20rendered%20infeasible%20by%20prohibitive%20computational%20and%20memory%0Acosts.%20We%20resolve%20this%20dilemma%20with%20%24S%5E2M%5E2%24%3A%20a%20global%20matching%20architecture%0Athat%20achieves%20state-of-the-art%20accuracy%20and%20high%20efficiency%20without%20relying%20on%0Acost%20volume%20filtering%20or%20deep%20refinement%20stacks.%20Our%20design%20integrates%20a%0Amulti-resolution%20transformer%20for%20robust%20long-range%20correspondence%2C%20trained%20with%0Aa%20novel%20loss%20function%20that%20concentrates%20probability%20on%20feasible%20matches.%20This%0Aapproach%20enables%20a%20more%20robust%20joint%20estimation%20of%20disparity%2C%20occlusion%2C%20and%0Aconfidence.%20%24S%5E2M%5E2%24%20establishes%20a%20new%20state%20of%20the%20art%20on%20Middlebury%20v3%20and%0AETH3D%20benchmarks%2C%20significantly%20outperforming%20prior%20methods%20in%20most%20metrics%0Awhile%20reconstructing%20high-quality%20details%20with%20competitive%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13229v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524S%255E2M%255E2%2524%253A%2520Scalable%2520Stereo%2520Matching%2520Model%2520for%2520Reliable%2520Depth%2520Estimation%26entry.906535625%3DJunhong%2520Min%2520and%2520Youngpil%2520Jeon%2520and%2520Jimin%2520Kim%2520and%2520Minyong%2520Choi%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520a%2520generalizable%2520stereo%2520matching%2520model%252C%2520capable%2520of%2520performing%250Awell%2520across%2520varying%2520resolutions%2520and%2520disparity%2520ranges%2520without%2520dataset-specific%250Afine-tuning%252C%2520has%2520revealed%2520a%2520fundamental%2520trade-off.%2520Iterative%2520local%2520search%250Amethods%2520achieve%2520high%2520scores%2520on%2520constrained%2520benchmarks%252C%2520but%2520their%2520core%2520mechanism%250Ainherently%2520limits%2520the%2520global%2520consistency%2520required%2520for%2520true%2520generalization.%250AHowever%252C%2520global%2520matching%2520architectures%252C%2520while%2520theoretically%2520more%2520robust%252C%2520have%250Ahistorically%2520been%2520rendered%2520infeasible%2520by%2520prohibitive%2520computational%2520and%2520memory%250Acosts.%2520We%2520resolve%2520this%2520dilemma%2520with%2520%2524S%255E2M%255E2%2524%253A%2520a%2520global%2520matching%2520architecture%250Athat%2520achieves%2520state-of-the-art%2520accuracy%2520and%2520high%2520efficiency%2520without%2520relying%2520on%250Acost%2520volume%2520filtering%2520or%2520deep%2520refinement%2520stacks.%2520Our%2520design%2520integrates%2520a%250Amulti-resolution%2520transformer%2520for%2520robust%2520long-range%2520correspondence%252C%2520trained%2520with%250Aa%2520novel%2520loss%2520function%2520that%2520concentrates%2520probability%2520on%2520feasible%2520matches.%2520This%250Aapproach%2520enables%2520a%2520more%2520robust%2520joint%2520estimation%2520of%2520disparity%252C%2520occlusion%252C%2520and%250Aconfidence.%2520%2524S%255E2M%255E2%2524%2520establishes%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520Middlebury%2520v3%2520and%250AETH3D%2520benchmarks%252C%2520significantly%2520outperforming%2520prior%2520methods%2520in%2520most%2520metrics%250Awhile%2520reconstructing%2520high-quality%2520details%2520with%2520competitive%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13229v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24S%5E2M%5E2%24%3A%20Scalable%20Stereo%20Matching%20Model%20for%20Reliable%20Depth%20Estimation&entry.906535625=Junhong%20Min%20and%20Youngpil%20Jeon%20and%20Jimin%20Kim%20and%20Minyong%20Choi&entry.1292438233=%20%20The%20pursuit%20of%20a%20generalizable%20stereo%20matching%20model%2C%20capable%20of%20performing%0Awell%20across%20varying%20resolutions%20and%20disparity%20ranges%20without%20dataset-specific%0Afine-tuning%2C%20has%20revealed%20a%20fundamental%20trade-off.%20Iterative%20local%20search%0Amethods%20achieve%20high%20scores%20on%20constrained%20benchmarks%2C%20but%20their%20core%20mechanism%0Ainherently%20limits%20the%20global%20consistency%20required%20for%20true%20generalization.%0AHowever%2C%20global%20matching%20architectures%2C%20while%20theoretically%20more%20robust%2C%20have%0Ahistorically%20been%20rendered%20infeasible%20by%20prohibitive%20computational%20and%20memory%0Acosts.%20We%20resolve%20this%20dilemma%20with%20%24S%5E2M%5E2%24%3A%20a%20global%20matching%20architecture%0Athat%20achieves%20state-of-the-art%20accuracy%20and%20high%20efficiency%20without%20relying%20on%0Acost%20volume%20filtering%20or%20deep%20refinement%20stacks.%20Our%20design%20integrates%20a%0Amulti-resolution%20transformer%20for%20robust%20long-range%20correspondence%2C%20trained%20with%0Aa%20novel%20loss%20function%20that%20concentrates%20probability%20on%20feasible%20matches.%20This%0Aapproach%20enables%20a%20more%20robust%20joint%20estimation%20of%20disparity%2C%20occlusion%2C%20and%0Aconfidence.%20%24S%5E2M%5E2%24%20establishes%20a%20new%20state%20of%20the%20art%20on%20Middlebury%20v3%20and%0AETH3D%20benchmarks%2C%20significantly%20outperforming%20prior%20methods%20in%20most%20metrics%0Awhile%20reconstructing%20high-quality%20details%20with%20competitive%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13229v3&entry.124074799=Read"},
{"title": "AstroLoc: Robust Space to Ground Image Localizer", "author": "Gabriele Berton and Alex Stoken and Carlo Masone", "abstract": "  Astronauts take thousands of photos of Earth per day from the International\nSpace Station, which, once localized on Earth's surface, are used for a\nmultitude of tasks, ranging from climate change research to disaster\nmanagement. The localization process, which has been performed manually for\ndecades, has recently been approached through image retrieval solutions: given\nan astronaut photo, find its most similar match among a large database of\ngeo-tagged satellite images, in a task called Astronaut Photography\nLocalization (APL). Yet, existing APL approaches are trained only using\nsatellite images, without taking advantage of the millions open-source\nastronaut photos. In this work we present the first APL pipeline capable of\nleveraging astronaut photos for training. We first produce full localization\ninformation for 300,000 manually weakly labeled astronaut photos through an\nautomated pipeline, and then use these images to train a model, called\nAstroLoc. AstroLoc learns a robust representation of Earth's surface features\nthrough two losses: astronaut photos paired with their matching satellite\ncounterparts in a pairwise loss, and a second loss on clusters of satellite\nimagery weighted by their relevance to astronaut photography via unsupervised\nmining. We find that AstroLoc achieves a staggering 35% average improvement in\nrecall@1 over previous SOTA, pushing the limits of existing datasets with a\nrecall@100 consistently over 99%. Finally, we note that AstroLoc, without any\nfine-tuning, provides excellent results for related tasks like the\nlost-in-space satellite problem and historical space imagery localization.\n", "link": "http://arxiv.org/abs/2502.07003v2", "date": "2025-07-30", "relevancy": 2.7832, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5902}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5427}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroLoc%3A%20Robust%20Space%20to%20Ground%20Image%20Localizer&body=Title%3A%20AstroLoc%3A%20Robust%20Space%20to%20Ground%20Image%20Localizer%0AAuthor%3A%20Gabriele%20Berton%20and%20Alex%20Stoken%20and%20Carlo%20Masone%0AAbstract%3A%20%20%20Astronauts%20take%20thousands%20of%20photos%20of%20Earth%20per%20day%20from%20the%20International%0ASpace%20Station%2C%20which%2C%20once%20localized%20on%20Earth%27s%20surface%2C%20are%20used%20for%20a%0Amultitude%20of%20tasks%2C%20ranging%20from%20climate%20change%20research%20to%20disaster%0Amanagement.%20The%20localization%20process%2C%20which%20has%20been%20performed%20manually%20for%0Adecades%2C%20has%20recently%20been%20approached%20through%20image%20retrieval%20solutions%3A%20given%0Aan%20astronaut%20photo%2C%20find%20its%20most%20similar%20match%20among%20a%20large%20database%20of%0Ageo-tagged%20satellite%20images%2C%20in%20a%20task%20called%20Astronaut%20Photography%0ALocalization%20%28APL%29.%20Yet%2C%20existing%20APL%20approaches%20are%20trained%20only%20using%0Asatellite%20images%2C%20without%20taking%20advantage%20of%20the%20millions%20open-source%0Aastronaut%20photos.%20In%20this%20work%20we%20present%20the%20first%20APL%20pipeline%20capable%20of%0Aleveraging%20astronaut%20photos%20for%20training.%20We%20first%20produce%20full%20localization%0Ainformation%20for%20300%2C000%20manually%20weakly%20labeled%20astronaut%20photos%20through%20an%0Aautomated%20pipeline%2C%20and%20then%20use%20these%20images%20to%20train%20a%20model%2C%20called%0AAstroLoc.%20AstroLoc%20learns%20a%20robust%20representation%20of%20Earth%27s%20surface%20features%0Athrough%20two%20losses%3A%20astronaut%20photos%20paired%20with%20their%20matching%20satellite%0Acounterparts%20in%20a%20pairwise%20loss%2C%20and%20a%20second%20loss%20on%20clusters%20of%20satellite%0Aimagery%20weighted%20by%20their%20relevance%20to%20astronaut%20photography%20via%20unsupervised%0Amining.%20We%20find%20that%20AstroLoc%20achieves%20a%20staggering%2035%25%20average%20improvement%20in%0Arecall%401%20over%20previous%20SOTA%2C%20pushing%20the%20limits%20of%20existing%20datasets%20with%20a%0Arecall%40100%20consistently%20over%2099%25.%20Finally%2C%20we%20note%20that%20AstroLoc%2C%20without%20any%0Afine-tuning%2C%20provides%20excellent%20results%20for%20related%20tasks%20like%20the%0Alost-in-space%20satellite%20problem%20and%20historical%20space%20imagery%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroLoc%253A%2520Robust%2520Space%2520to%2520Ground%2520Image%2520Localizer%26entry.906535625%3DGabriele%2520Berton%2520and%2520Alex%2520Stoken%2520and%2520Carlo%2520Masone%26entry.1292438233%3D%2520%2520Astronauts%2520take%2520thousands%2520of%2520photos%2520of%2520Earth%2520per%2520day%2520from%2520the%2520International%250ASpace%2520Station%252C%2520which%252C%2520once%2520localized%2520on%2520Earth%2527s%2520surface%252C%2520are%2520used%2520for%2520a%250Amultitude%2520of%2520tasks%252C%2520ranging%2520from%2520climate%2520change%2520research%2520to%2520disaster%250Amanagement.%2520The%2520localization%2520process%252C%2520which%2520has%2520been%2520performed%2520manually%2520for%250Adecades%252C%2520has%2520recently%2520been%2520approached%2520through%2520image%2520retrieval%2520solutions%253A%2520given%250Aan%2520astronaut%2520photo%252C%2520find%2520its%2520most%2520similar%2520match%2520among%2520a%2520large%2520database%2520of%250Ageo-tagged%2520satellite%2520images%252C%2520in%2520a%2520task%2520called%2520Astronaut%2520Photography%250ALocalization%2520%2528APL%2529.%2520Yet%252C%2520existing%2520APL%2520approaches%2520are%2520trained%2520only%2520using%250Asatellite%2520images%252C%2520without%2520taking%2520advantage%2520of%2520the%2520millions%2520open-source%250Aastronaut%2520photos.%2520In%2520this%2520work%2520we%2520present%2520the%2520first%2520APL%2520pipeline%2520capable%2520of%250Aleveraging%2520astronaut%2520photos%2520for%2520training.%2520We%2520first%2520produce%2520full%2520localization%250Ainformation%2520for%2520300%252C000%2520manually%2520weakly%2520labeled%2520astronaut%2520photos%2520through%2520an%250Aautomated%2520pipeline%252C%2520and%2520then%2520use%2520these%2520images%2520to%2520train%2520a%2520model%252C%2520called%250AAstroLoc.%2520AstroLoc%2520learns%2520a%2520robust%2520representation%2520of%2520Earth%2527s%2520surface%2520features%250Athrough%2520two%2520losses%253A%2520astronaut%2520photos%2520paired%2520with%2520their%2520matching%2520satellite%250Acounterparts%2520in%2520a%2520pairwise%2520loss%252C%2520and%2520a%2520second%2520loss%2520on%2520clusters%2520of%2520satellite%250Aimagery%2520weighted%2520by%2520their%2520relevance%2520to%2520astronaut%2520photography%2520via%2520unsupervised%250Amining.%2520We%2520find%2520that%2520AstroLoc%2520achieves%2520a%2520staggering%252035%2525%2520average%2520improvement%2520in%250Arecall%25401%2520over%2520previous%2520SOTA%252C%2520pushing%2520the%2520limits%2520of%2520existing%2520datasets%2520with%2520a%250Arecall%2540100%2520consistently%2520over%252099%2525.%2520Finally%252C%2520we%2520note%2520that%2520AstroLoc%252C%2520without%2520any%250Afine-tuning%252C%2520provides%2520excellent%2520results%2520for%2520related%2520tasks%2520like%2520the%250Alost-in-space%2520satellite%2520problem%2520and%2520historical%2520space%2520imagery%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroLoc%3A%20Robust%20Space%20to%20Ground%20Image%20Localizer&entry.906535625=Gabriele%20Berton%20and%20Alex%20Stoken%20and%20Carlo%20Masone&entry.1292438233=%20%20Astronauts%20take%20thousands%20of%20photos%20of%20Earth%20per%20day%20from%20the%20International%0ASpace%20Station%2C%20which%2C%20once%20localized%20on%20Earth%27s%20surface%2C%20are%20used%20for%20a%0Amultitude%20of%20tasks%2C%20ranging%20from%20climate%20change%20research%20to%20disaster%0Amanagement.%20The%20localization%20process%2C%20which%20has%20been%20performed%20manually%20for%0Adecades%2C%20has%20recently%20been%20approached%20through%20image%20retrieval%20solutions%3A%20given%0Aan%20astronaut%20photo%2C%20find%20its%20most%20similar%20match%20among%20a%20large%20database%20of%0Ageo-tagged%20satellite%20images%2C%20in%20a%20task%20called%20Astronaut%20Photography%0ALocalization%20%28APL%29.%20Yet%2C%20existing%20APL%20approaches%20are%20trained%20only%20using%0Asatellite%20images%2C%20without%20taking%20advantage%20of%20the%20millions%20open-source%0Aastronaut%20photos.%20In%20this%20work%20we%20present%20the%20first%20APL%20pipeline%20capable%20of%0Aleveraging%20astronaut%20photos%20for%20training.%20We%20first%20produce%20full%20localization%0Ainformation%20for%20300%2C000%20manually%20weakly%20labeled%20astronaut%20photos%20through%20an%0Aautomated%20pipeline%2C%20and%20then%20use%20these%20images%20to%20train%20a%20model%2C%20called%0AAstroLoc.%20AstroLoc%20learns%20a%20robust%20representation%20of%20Earth%27s%20surface%20features%0Athrough%20two%20losses%3A%20astronaut%20photos%20paired%20with%20their%20matching%20satellite%0Acounterparts%20in%20a%20pairwise%20loss%2C%20and%20a%20second%20loss%20on%20clusters%20of%20satellite%0Aimagery%20weighted%20by%20their%20relevance%20to%20astronaut%20photography%20via%20unsupervised%0Amining.%20We%20find%20that%20AstroLoc%20achieves%20a%20staggering%2035%25%20average%20improvement%20in%0Arecall%401%20over%20previous%20SOTA%2C%20pushing%20the%20limits%20of%20existing%20datasets%20with%20a%0Arecall%40100%20consistently%20over%2099%25.%20Finally%2C%20we%20note%20that%20AstroLoc%2C%20without%20any%0Afine-tuning%2C%20provides%20excellent%20results%20for%20related%20tasks%20like%20the%0Alost-in-space%20satellite%20problem%20and%20historical%20space%20imagery%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07003v2&entry.124074799=Read"},
{"title": "Bi-Level Optimization for Self-Supervised AI-Generated Face Detection", "author": "Mian Zou and Nan Zhong and Baosheng Yu and Yibing Zhan and Kede Ma", "abstract": "  AI-generated face detectors trained via supervised learning typically rely on\nsynthesized images from specific generators, limiting their generalization to\nemerging generative techniques. To overcome this limitation, we introduce a\nself-supervised method based on bi-level optimization. In the inner loop, we\npretrain a vision encoder only on photographic face images using a set of\nlinearly weighted pretext tasks: classification of categorical exchangeable\nimage file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of\nartificial face manipulations. The outer loop then optimizes the relative\nweights of these pretext tasks to enhance the coarse-grained detection of\nmanipulated faces, serving as a proxy task for identifying AI-generated faces.\nIn doing so, it aligns self-supervised learning more closely with the ultimate\ngoal of AI-generated face detection. Once pretrained, the encoder remains\nfixed, and AI-generated faces are detected either as anomalies under a Gaussian\nmixture model fitted to photographic face features or by a lightweight\ntwo-layer perceptron serving as a binary classifier. Extensive experiments\ndemonstrate that our detectors significantly outperform existing approaches in\nboth one-class and binary classification settings, exhibiting strong\ngeneralization to unseen generators.\n", "link": "http://arxiv.org/abs/2507.22824v1", "date": "2025-07-30", "relevancy": 2.7732, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5587}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-Level%20Optimization%20for%20Self-Supervised%20AI-Generated%20Face%20Detection&body=Title%3A%20Bi-Level%20Optimization%20for%20Self-Supervised%20AI-Generated%20Face%20Detection%0AAuthor%3A%20Mian%20Zou%20and%20Nan%20Zhong%20and%20Baosheng%20Yu%20and%20Yibing%20Zhan%20and%20Kede%20Ma%0AAbstract%3A%20%20%20AI-generated%20face%20detectors%20trained%20via%20supervised%20learning%20typically%20rely%20on%0Asynthesized%20images%20from%20specific%20generators%2C%20limiting%20their%20generalization%20to%0Aemerging%20generative%20techniques.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%0Aself-supervised%20method%20based%20on%20bi-level%20optimization.%20In%20the%20inner%20loop%2C%20we%0Apretrain%20a%20vision%20encoder%20only%20on%20photographic%20face%20images%20using%20a%20set%20of%0Alinearly%20weighted%20pretext%20tasks%3A%20classification%20of%20categorical%20exchangeable%0Aimage%20file%20format%20%28EXIF%29%20tags%2C%20ranking%20of%20ordinal%20EXIF%20tags%2C%20and%20detection%20of%0Aartificial%20face%20manipulations.%20The%20outer%20loop%20then%20optimizes%20the%20relative%0Aweights%20of%20these%20pretext%20tasks%20to%20enhance%20the%20coarse-grained%20detection%20of%0Amanipulated%20faces%2C%20serving%20as%20a%20proxy%20task%20for%20identifying%20AI-generated%20faces.%0AIn%20doing%20so%2C%20it%20aligns%20self-supervised%20learning%20more%20closely%20with%20the%20ultimate%0Agoal%20of%20AI-generated%20face%20detection.%20Once%20pretrained%2C%20the%20encoder%20remains%0Afixed%2C%20and%20AI-generated%20faces%20are%20detected%20either%20as%20anomalies%20under%20a%20Gaussian%0Amixture%20model%20fitted%20to%20photographic%20face%20features%20or%20by%20a%20lightweight%0Atwo-layer%20perceptron%20serving%20as%20a%20binary%20classifier.%20Extensive%20experiments%0Ademonstrate%20that%20our%20detectors%20significantly%20outperform%20existing%20approaches%20in%0Aboth%20one-class%20and%20binary%20classification%20settings%2C%20exhibiting%20strong%0Ageneralization%20to%20unseen%20generators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-Level%2520Optimization%2520for%2520Self-Supervised%2520AI-Generated%2520Face%2520Detection%26entry.906535625%3DMian%2520Zou%2520and%2520Nan%2520Zhong%2520and%2520Baosheng%2520Yu%2520and%2520Yibing%2520Zhan%2520and%2520Kede%2520Ma%26entry.1292438233%3D%2520%2520AI-generated%2520face%2520detectors%2520trained%2520via%2520supervised%2520learning%2520typically%2520rely%2520on%250Asynthesized%2520images%2520from%2520specific%2520generators%252C%2520limiting%2520their%2520generalization%2520to%250Aemerging%2520generative%2520techniques.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%250Aself-supervised%2520method%2520based%2520on%2520bi-level%2520optimization.%2520In%2520the%2520inner%2520loop%252C%2520we%250Apretrain%2520a%2520vision%2520encoder%2520only%2520on%2520photographic%2520face%2520images%2520using%2520a%2520set%2520of%250Alinearly%2520weighted%2520pretext%2520tasks%253A%2520classification%2520of%2520categorical%2520exchangeable%250Aimage%2520file%2520format%2520%2528EXIF%2529%2520tags%252C%2520ranking%2520of%2520ordinal%2520EXIF%2520tags%252C%2520and%2520detection%2520of%250Aartificial%2520face%2520manipulations.%2520The%2520outer%2520loop%2520then%2520optimizes%2520the%2520relative%250Aweights%2520of%2520these%2520pretext%2520tasks%2520to%2520enhance%2520the%2520coarse-grained%2520detection%2520of%250Amanipulated%2520faces%252C%2520serving%2520as%2520a%2520proxy%2520task%2520for%2520identifying%2520AI-generated%2520faces.%250AIn%2520doing%2520so%252C%2520it%2520aligns%2520self-supervised%2520learning%2520more%2520closely%2520with%2520the%2520ultimate%250Agoal%2520of%2520AI-generated%2520face%2520detection.%2520Once%2520pretrained%252C%2520the%2520encoder%2520remains%250Afixed%252C%2520and%2520AI-generated%2520faces%2520are%2520detected%2520either%2520as%2520anomalies%2520under%2520a%2520Gaussian%250Amixture%2520model%2520fitted%2520to%2520photographic%2520face%2520features%2520or%2520by%2520a%2520lightweight%250Atwo-layer%2520perceptron%2520serving%2520as%2520a%2520binary%2520classifier.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520detectors%2520significantly%2520outperform%2520existing%2520approaches%2520in%250Aboth%2520one-class%2520and%2520binary%2520classification%2520settings%252C%2520exhibiting%2520strong%250Ageneralization%2520to%2520unseen%2520generators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-Level%20Optimization%20for%20Self-Supervised%20AI-Generated%20Face%20Detection&entry.906535625=Mian%20Zou%20and%20Nan%20Zhong%20and%20Baosheng%20Yu%20and%20Yibing%20Zhan%20and%20Kede%20Ma&entry.1292438233=%20%20AI-generated%20face%20detectors%20trained%20via%20supervised%20learning%20typically%20rely%20on%0Asynthesized%20images%20from%20specific%20generators%2C%20limiting%20their%20generalization%20to%0Aemerging%20generative%20techniques.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%0Aself-supervised%20method%20based%20on%20bi-level%20optimization.%20In%20the%20inner%20loop%2C%20we%0Apretrain%20a%20vision%20encoder%20only%20on%20photographic%20face%20images%20using%20a%20set%20of%0Alinearly%20weighted%20pretext%20tasks%3A%20classification%20of%20categorical%20exchangeable%0Aimage%20file%20format%20%28EXIF%29%20tags%2C%20ranking%20of%20ordinal%20EXIF%20tags%2C%20and%20detection%20of%0Aartificial%20face%20manipulations.%20The%20outer%20loop%20then%20optimizes%20the%20relative%0Aweights%20of%20these%20pretext%20tasks%20to%20enhance%20the%20coarse-grained%20detection%20of%0Amanipulated%20faces%2C%20serving%20as%20a%20proxy%20task%20for%20identifying%20AI-generated%20faces.%0AIn%20doing%20so%2C%20it%20aligns%20self-supervised%20learning%20more%20closely%20with%20the%20ultimate%0Agoal%20of%20AI-generated%20face%20detection.%20Once%20pretrained%2C%20the%20encoder%20remains%0Afixed%2C%20and%20AI-generated%20faces%20are%20detected%20either%20as%20anomalies%20under%20a%20Gaussian%0Amixture%20model%20fitted%20to%20photographic%20face%20features%20or%20by%20a%20lightweight%0Atwo-layer%20perceptron%20serving%20as%20a%20binary%20classifier.%20Extensive%20experiments%0Ademonstrate%20that%20our%20detectors%20significantly%20outperform%20existing%20approaches%20in%0Aboth%20one-class%20and%20binary%20classification%20settings%2C%20exhibiting%20strong%0Ageneralization%20to%20unseen%20generators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22824v1&entry.124074799=Read"},
{"title": "Generative Active Learning for Long-tail Trajectory Prediction via\n  Controllable Diffusion Model", "author": "Daehee Park and Monu Surana and Pranav Desai and Ashish Mehta and Reuben MV John and Kuk-Jin Yoon", "abstract": "  While data-driven trajectory prediction has enhanced the reliability of\nautonomous driving systems, it still struggles with rarely observed long-tail\nscenarios. Prior works addressed this by modifying model architectures, such as\nusing hypernetworks. In contrast, we propose refining the training process to\nunlock each model's potential without altering its structure. We introduce\nGenerative Active Learning for Trajectory prediction (GALTraj), the first\nmethod to successfully deploy generative active learning into trajectory\nprediction. It actively identifies rare tail samples where the model fails and\naugments these samples with a controllable diffusion model during training. In\nour framework, generating scenarios that are diverse, realistic, and preserve\ntail-case characteristics is paramount. Accordingly, we design a tail-aware\ngeneration method that applies tailored diffusion guidance to generate\ntrajectories that both capture rare behaviors and respect traffic rules. Unlike\nprior simulation methods focused solely on scenario diversity, GALTraj is the\nfirst to show how simulator-driven augmentation benefits long-tail learning in\ntrajectory prediction. Experiments on multiple trajectory datasets (WOMD,\nArgoverse2) with popular backbones (QCNet, MTR) confirm that our method\nsignificantly boosts performance on tail samples and also enhances accuracy on\nhead samples.\n", "link": "http://arxiv.org/abs/2507.22615v1", "date": "2025-07-30", "relevancy": 2.7725, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5624}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5526}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Active%20Learning%20for%20Long-tail%20Trajectory%20Prediction%20via%0A%20%20Controllable%20Diffusion%20Model&body=Title%3A%20Generative%20Active%20Learning%20for%20Long-tail%20Trajectory%20Prediction%20via%0A%20%20Controllable%20Diffusion%20Model%0AAuthor%3A%20Daehee%20Park%20and%20Monu%20Surana%20and%20Pranav%20Desai%20and%20Ashish%20Mehta%20and%20Reuben%20MV%20John%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20While%20data-driven%20trajectory%20prediction%20has%20enhanced%20the%20reliability%20of%0Aautonomous%20driving%20systems%2C%20it%20still%20struggles%20with%20rarely%20observed%20long-tail%0Ascenarios.%20Prior%20works%20addressed%20this%20by%20modifying%20model%20architectures%2C%20such%20as%0Ausing%20hypernetworks.%20In%20contrast%2C%20we%20propose%20refining%20the%20training%20process%20to%0Aunlock%20each%20model%27s%20potential%20without%20altering%20its%20structure.%20We%20introduce%0AGenerative%20Active%20Learning%20for%20Trajectory%20prediction%20%28GALTraj%29%2C%20the%20first%0Amethod%20to%20successfully%20deploy%20generative%20active%20learning%20into%20trajectory%0Aprediction.%20It%20actively%20identifies%20rare%20tail%20samples%20where%20the%20model%20fails%20and%0Aaugments%20these%20samples%20with%20a%20controllable%20diffusion%20model%20during%20training.%20In%0Aour%20framework%2C%20generating%20scenarios%20that%20are%20diverse%2C%20realistic%2C%20and%20preserve%0Atail-case%20characteristics%20is%20paramount.%20Accordingly%2C%20we%20design%20a%20tail-aware%0Ageneration%20method%20that%20applies%20tailored%20diffusion%20guidance%20to%20generate%0Atrajectories%20that%20both%20capture%20rare%20behaviors%20and%20respect%20traffic%20rules.%20Unlike%0Aprior%20simulation%20methods%20focused%20solely%20on%20scenario%20diversity%2C%20GALTraj%20is%20the%0Afirst%20to%20show%20how%20simulator-driven%20augmentation%20benefits%20long-tail%20learning%20in%0Atrajectory%20prediction.%20Experiments%20on%20multiple%20trajectory%20datasets%20%28WOMD%2C%0AArgoverse2%29%20with%20popular%20backbones%20%28QCNet%2C%20MTR%29%20confirm%20that%20our%20method%0Asignificantly%20boosts%20performance%20on%20tail%20samples%20and%20also%20enhances%20accuracy%20on%0Ahead%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Active%2520Learning%2520for%2520Long-tail%2520Trajectory%2520Prediction%2520via%250A%2520%2520Controllable%2520Diffusion%2520Model%26entry.906535625%3DDaehee%2520Park%2520and%2520Monu%2520Surana%2520and%2520Pranav%2520Desai%2520and%2520Ashish%2520Mehta%2520and%2520Reuben%2520MV%2520John%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520While%2520data-driven%2520trajectory%2520prediction%2520has%2520enhanced%2520the%2520reliability%2520of%250Aautonomous%2520driving%2520systems%252C%2520it%2520still%2520struggles%2520with%2520rarely%2520observed%2520long-tail%250Ascenarios.%2520Prior%2520works%2520addressed%2520this%2520by%2520modifying%2520model%2520architectures%252C%2520such%2520as%250Ausing%2520hypernetworks.%2520In%2520contrast%252C%2520we%2520propose%2520refining%2520the%2520training%2520process%2520to%250Aunlock%2520each%2520model%2527s%2520potential%2520without%2520altering%2520its%2520structure.%2520We%2520introduce%250AGenerative%2520Active%2520Learning%2520for%2520Trajectory%2520prediction%2520%2528GALTraj%2529%252C%2520the%2520first%250Amethod%2520to%2520successfully%2520deploy%2520generative%2520active%2520learning%2520into%2520trajectory%250Aprediction.%2520It%2520actively%2520identifies%2520rare%2520tail%2520samples%2520where%2520the%2520model%2520fails%2520and%250Aaugments%2520these%2520samples%2520with%2520a%2520controllable%2520diffusion%2520model%2520during%2520training.%2520In%250Aour%2520framework%252C%2520generating%2520scenarios%2520that%2520are%2520diverse%252C%2520realistic%252C%2520and%2520preserve%250Atail-case%2520characteristics%2520is%2520paramount.%2520Accordingly%252C%2520we%2520design%2520a%2520tail-aware%250Ageneration%2520method%2520that%2520applies%2520tailored%2520diffusion%2520guidance%2520to%2520generate%250Atrajectories%2520that%2520both%2520capture%2520rare%2520behaviors%2520and%2520respect%2520traffic%2520rules.%2520Unlike%250Aprior%2520simulation%2520methods%2520focused%2520solely%2520on%2520scenario%2520diversity%252C%2520GALTraj%2520is%2520the%250Afirst%2520to%2520show%2520how%2520simulator-driven%2520augmentation%2520benefits%2520long-tail%2520learning%2520in%250Atrajectory%2520prediction.%2520Experiments%2520on%2520multiple%2520trajectory%2520datasets%2520%2528WOMD%252C%250AArgoverse2%2529%2520with%2520popular%2520backbones%2520%2528QCNet%252C%2520MTR%2529%2520confirm%2520that%2520our%2520method%250Asignificantly%2520boosts%2520performance%2520on%2520tail%2520samples%2520and%2520also%2520enhances%2520accuracy%2520on%250Ahead%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Active%20Learning%20for%20Long-tail%20Trajectory%20Prediction%20via%0A%20%20Controllable%20Diffusion%20Model&entry.906535625=Daehee%20Park%20and%20Monu%20Surana%20and%20Pranav%20Desai%20and%20Ashish%20Mehta%20and%20Reuben%20MV%20John%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20While%20data-driven%20trajectory%20prediction%20has%20enhanced%20the%20reliability%20of%0Aautonomous%20driving%20systems%2C%20it%20still%20struggles%20with%20rarely%20observed%20long-tail%0Ascenarios.%20Prior%20works%20addressed%20this%20by%20modifying%20model%20architectures%2C%20such%20as%0Ausing%20hypernetworks.%20In%20contrast%2C%20we%20propose%20refining%20the%20training%20process%20to%0Aunlock%20each%20model%27s%20potential%20without%20altering%20its%20structure.%20We%20introduce%0AGenerative%20Active%20Learning%20for%20Trajectory%20prediction%20%28GALTraj%29%2C%20the%20first%0Amethod%20to%20successfully%20deploy%20generative%20active%20learning%20into%20trajectory%0Aprediction.%20It%20actively%20identifies%20rare%20tail%20samples%20where%20the%20model%20fails%20and%0Aaugments%20these%20samples%20with%20a%20controllable%20diffusion%20model%20during%20training.%20In%0Aour%20framework%2C%20generating%20scenarios%20that%20are%20diverse%2C%20realistic%2C%20and%20preserve%0Atail-case%20characteristics%20is%20paramount.%20Accordingly%2C%20we%20design%20a%20tail-aware%0Ageneration%20method%20that%20applies%20tailored%20diffusion%20guidance%20to%20generate%0Atrajectories%20that%20both%20capture%20rare%20behaviors%20and%20respect%20traffic%20rules.%20Unlike%0Aprior%20simulation%20methods%20focused%20solely%20on%20scenario%20diversity%2C%20GALTraj%20is%20the%0Afirst%20to%20show%20how%20simulator-driven%20augmentation%20benefits%20long-tail%20learning%20in%0Atrajectory%20prediction.%20Experiments%20on%20multiple%20trajectory%20datasets%20%28WOMD%2C%0AArgoverse2%29%20with%20popular%20backbones%20%28QCNet%2C%20MTR%29%20confirm%20that%20our%20method%0Asignificantly%20boosts%20performance%20on%20tail%20samples%20and%20also%20enhances%20accuracy%20on%0Ahead%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22615v1&entry.124074799=Read"},
{"title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on\n  Vision Language Models", "author": "Kedong Xiu and Saiqian Zhang", "abstract": "  As Vision-Language Models (VLMs) are increasingly deployed in split-DNN\nconfigurations--with visual encoders (e.g., ResNet, ViT) operating on user\ndevices and sending intermediate features to the cloud--there is a growing\nprivacy risk from semantic information leakage. Existing approaches to\nreconstructing images from these intermediate features often result in blurry,\nsemantically ambiguous images. To directly address semantic leakage, we propose\nCapRecover, a cross-modality inversion framework that recovers high-level\nsemantic content, such as labels or captions, directly from intermediate\nfeatures without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating\nstrong performance in semantic recovery. Specifically, CapRecover achieves up\nto 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from\nResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis\nfurther reveals that deeper convolutional layers encode significantly more\nsemantic information compared to shallow layers. To mitigate semantic leakage,\nwe introduce a simple yet effective protection method: adding random noise to\nintermediate features at each layer and removing the noise in the next layer.\nExperimental results show that this approach prevents semantic leakage without\nadditional training costs.\n", "link": "http://arxiv.org/abs/2507.22828v1", "date": "2025-07-30", "relevancy": 2.7439, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CapRecover%3A%20A%20Cross-Modality%20Feature%20Inversion%20Attack%20Framework%20on%0A%20%20Vision%20Language%20Models&body=Title%3A%20CapRecover%3A%20A%20Cross-Modality%20Feature%20Inversion%20Attack%20Framework%20on%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Kedong%20Xiu%20and%20Saiqian%20Zhang%0AAbstract%3A%20%20%20As%20Vision-Language%20Models%20%28VLMs%29%20are%20increasingly%20deployed%20in%20split-DNN%0Aconfigurations--with%20visual%20encoders%20%28e.g.%2C%20ResNet%2C%20ViT%29%20operating%20on%20user%0Adevices%20and%20sending%20intermediate%20features%20to%20the%20cloud--there%20is%20a%20growing%0Aprivacy%20risk%20from%20semantic%20information%20leakage.%20Existing%20approaches%20to%0Areconstructing%20images%20from%20these%20intermediate%20features%20often%20result%20in%20blurry%2C%0Asemantically%20ambiguous%20images.%20To%20directly%20address%20semantic%20leakage%2C%20we%20propose%0ACapRecover%2C%20a%20cross-modality%20inversion%20framework%20that%20recovers%20high-level%0Asemantic%20content%2C%20such%20as%20labels%20or%20captions%2C%20directly%20from%20intermediate%0Afeatures%20without%20image%20reconstruction.%0A%20%20We%20evaluate%20CapRecover%20on%20multiple%20datasets%20and%20victim%20models%2C%20demonstrating%0Astrong%20performance%20in%20semantic%20recovery.%20Specifically%2C%20CapRecover%20achieves%20up%0Ato%2092.71%25%20Top-1%20label%20accuracy%20on%20CIFAR-10%20and%20generates%20fluent%20captions%20from%0AResNet50%20features%20on%20COCO2017%20with%20ROUGE-L%20scores%20up%20to%200.52.%20Our%20analysis%0Afurther%20reveals%20that%20deeper%20convolutional%20layers%20encode%20significantly%20more%0Asemantic%20information%20compared%20to%20shallow%20layers.%20To%20mitigate%20semantic%20leakage%2C%0Awe%20introduce%20a%20simple%20yet%20effective%20protection%20method%3A%20adding%20random%20noise%20to%0Aintermediate%20features%20at%20each%20layer%20and%20removing%20the%20noise%20in%20the%20next%20layer.%0AExperimental%20results%20show%20that%20this%20approach%20prevents%20semantic%20leakage%20without%0Aadditional%20training%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapRecover%253A%2520A%2520Cross-Modality%2520Feature%2520Inversion%2520Attack%2520Framework%2520on%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DKedong%2520Xiu%2520and%2520Saiqian%2520Zhang%26entry.1292438233%3D%2520%2520As%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520split-DNN%250Aconfigurations--with%2520visual%2520encoders%2520%2528e.g.%252C%2520ResNet%252C%2520ViT%2529%2520operating%2520on%2520user%250Adevices%2520and%2520sending%2520intermediate%2520features%2520to%2520the%2520cloud--there%2520is%2520a%2520growing%250Aprivacy%2520risk%2520from%2520semantic%2520information%2520leakage.%2520Existing%2520approaches%2520to%250Areconstructing%2520images%2520from%2520these%2520intermediate%2520features%2520often%2520result%2520in%2520blurry%252C%250Asemantically%2520ambiguous%2520images.%2520To%2520directly%2520address%2520semantic%2520leakage%252C%2520we%2520propose%250ACapRecover%252C%2520a%2520cross-modality%2520inversion%2520framework%2520that%2520recovers%2520high-level%250Asemantic%2520content%252C%2520such%2520as%2520labels%2520or%2520captions%252C%2520directly%2520from%2520intermediate%250Afeatures%2520without%2520image%2520reconstruction.%250A%2520%2520We%2520evaluate%2520CapRecover%2520on%2520multiple%2520datasets%2520and%2520victim%2520models%252C%2520demonstrating%250Astrong%2520performance%2520in%2520semantic%2520recovery.%2520Specifically%252C%2520CapRecover%2520achieves%2520up%250Ato%252092.71%2525%2520Top-1%2520label%2520accuracy%2520on%2520CIFAR-10%2520and%2520generates%2520fluent%2520captions%2520from%250AResNet50%2520features%2520on%2520COCO2017%2520with%2520ROUGE-L%2520scores%2520up%2520to%25200.52.%2520Our%2520analysis%250Afurther%2520reveals%2520that%2520deeper%2520convolutional%2520layers%2520encode%2520significantly%2520more%250Asemantic%2520information%2520compared%2520to%2520shallow%2520layers.%2520To%2520mitigate%2520semantic%2520leakage%252C%250Awe%2520introduce%2520a%2520simple%2520yet%2520effective%2520protection%2520method%253A%2520adding%2520random%2520noise%2520to%250Aintermediate%2520features%2520at%2520each%2520layer%2520and%2520removing%2520the%2520noise%2520in%2520the%2520next%2520layer.%250AExperimental%2520results%2520show%2520that%2520this%2520approach%2520prevents%2520semantic%2520leakage%2520without%250Aadditional%2520training%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CapRecover%3A%20A%20Cross-Modality%20Feature%20Inversion%20Attack%20Framework%20on%0A%20%20Vision%20Language%20Models&entry.906535625=Kedong%20Xiu%20and%20Saiqian%20Zhang&entry.1292438233=%20%20As%20Vision-Language%20Models%20%28VLMs%29%20are%20increasingly%20deployed%20in%20split-DNN%0Aconfigurations--with%20visual%20encoders%20%28e.g.%2C%20ResNet%2C%20ViT%29%20operating%20on%20user%0Adevices%20and%20sending%20intermediate%20features%20to%20the%20cloud--there%20is%20a%20growing%0Aprivacy%20risk%20from%20semantic%20information%20leakage.%20Existing%20approaches%20to%0Areconstructing%20images%20from%20these%20intermediate%20features%20often%20result%20in%20blurry%2C%0Asemantically%20ambiguous%20images.%20To%20directly%20address%20semantic%20leakage%2C%20we%20propose%0ACapRecover%2C%20a%20cross-modality%20inversion%20framework%20that%20recovers%20high-level%0Asemantic%20content%2C%20such%20as%20labels%20or%20captions%2C%20directly%20from%20intermediate%0Afeatures%20without%20image%20reconstruction.%0A%20%20We%20evaluate%20CapRecover%20on%20multiple%20datasets%20and%20victim%20models%2C%20demonstrating%0Astrong%20performance%20in%20semantic%20recovery.%20Specifically%2C%20CapRecover%20achieves%20up%0Ato%2092.71%25%20Top-1%20label%20accuracy%20on%20CIFAR-10%20and%20generates%20fluent%20captions%20from%0AResNet50%20features%20on%20COCO2017%20with%20ROUGE-L%20scores%20up%20to%200.52.%20Our%20analysis%0Afurther%20reveals%20that%20deeper%20convolutional%20layers%20encode%20significantly%20more%0Asemantic%20information%20compared%20to%20shallow%20layers.%20To%20mitigate%20semantic%20leakage%2C%0Awe%20introduce%20a%20simple%20yet%20effective%20protection%20method%3A%20adding%20random%20noise%20to%0Aintermediate%20features%20at%20each%20layer%20and%20removing%20the%20noise%20in%20the%20next%20layer.%0AExperimental%20results%20show%20that%20this%20approach%20prevents%20semantic%20leakage%20without%0Aadditional%20training%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22828v1&entry.124074799=Read"},
{"title": "CLIP-HandID: Vision-Language Model for Hand-Based Person Identification", "author": "Nathanael L. Baisa and Babu Pallam and Amudhavel Jayavel", "abstract": "  This paper introduces a novel approach to person identification using hand\nimages, designed specifically for criminal investigations. The method is\nparticularly valuable in serious crimes such as sexual abuse, where hand images\nare often the only identifiable evidence available. Our proposed method,\nCLIP-HandID, leverages a pre-trained foundational vision-language model - CLIP\n- to efficiently learn discriminative deep feature representations from hand\nimages (input to CLIP's image encoder) using textual prompts as semantic\nguidance. Since hand images are labeled with indexes rather than text\ndescriptions, we employ a textual inversion network to learn pseudo-tokens that\nencode specific visual contexts or appearance attributes. These learned\npseudo-tokens are then incorporated into textual prompts, which are fed into\nCLIP's text encoder to leverage its multi-modal reasoning and enhance\ngeneralization for identification. Through extensive evaluations on two large,\npublicly available hand datasets with multi-ethnic representation, we\ndemonstrate that our method significantly outperforms existing approaches.\n", "link": "http://arxiv.org/abs/2506.12447v3", "date": "2025-07-30", "relevancy": 2.7395, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5515}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-HandID%3A%20Vision-Language%20Model%20for%20Hand-Based%20Person%20Identification&body=Title%3A%20CLIP-HandID%3A%20Vision-Language%20Model%20for%20Hand-Based%20Person%20Identification%0AAuthor%3A%20Nathanael%20L.%20Baisa%20and%20Babu%20Pallam%20and%20Amudhavel%20Jayavel%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20person%20identification%20using%20hand%0Aimages%2C%20designed%20specifically%20for%20criminal%20investigations.%20The%20method%20is%0Aparticularly%20valuable%20in%20serious%20crimes%20such%20as%20sexual%20abuse%2C%20where%20hand%20images%0Aare%20often%20the%20only%20identifiable%20evidence%20available.%20Our%20proposed%20method%2C%0ACLIP-HandID%2C%20leverages%20a%20pre-trained%20foundational%20vision-language%20model%20-%20CLIP%0A-%20to%20efficiently%20learn%20discriminative%20deep%20feature%20representations%20from%20hand%0Aimages%20%28input%20to%20CLIP%27s%20image%20encoder%29%20using%20textual%20prompts%20as%20semantic%0Aguidance.%20Since%20hand%20images%20are%20labeled%20with%20indexes%20rather%20than%20text%0Adescriptions%2C%20we%20employ%20a%20textual%20inversion%20network%20to%20learn%20pseudo-tokens%20that%0Aencode%20specific%20visual%20contexts%20or%20appearance%20attributes.%20These%20learned%0Apseudo-tokens%20are%20then%20incorporated%20into%20textual%20prompts%2C%20which%20are%20fed%20into%0ACLIP%27s%20text%20encoder%20to%20leverage%20its%20multi-modal%20reasoning%20and%20enhance%0Ageneralization%20for%20identification.%20Through%20extensive%20evaluations%20on%20two%20large%2C%0Apublicly%20available%20hand%20datasets%20with%20multi-ethnic%20representation%2C%20we%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12447v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-HandID%253A%2520Vision-Language%2520Model%2520for%2520Hand-Based%2520Person%2520Identification%26entry.906535625%3DNathanael%2520L.%2520Baisa%2520and%2520Babu%2520Pallam%2520and%2520Amudhavel%2520Jayavel%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520person%2520identification%2520using%2520hand%250Aimages%252C%2520designed%2520specifically%2520for%2520criminal%2520investigations.%2520The%2520method%2520is%250Aparticularly%2520valuable%2520in%2520serious%2520crimes%2520such%2520as%2520sexual%2520abuse%252C%2520where%2520hand%2520images%250Aare%2520often%2520the%2520only%2520identifiable%2520evidence%2520available.%2520Our%2520proposed%2520method%252C%250ACLIP-HandID%252C%2520leverages%2520a%2520pre-trained%2520foundational%2520vision-language%2520model%2520-%2520CLIP%250A-%2520to%2520efficiently%2520learn%2520discriminative%2520deep%2520feature%2520representations%2520from%2520hand%250Aimages%2520%2528input%2520to%2520CLIP%2527s%2520image%2520encoder%2529%2520using%2520textual%2520prompts%2520as%2520semantic%250Aguidance.%2520Since%2520hand%2520images%2520are%2520labeled%2520with%2520indexes%2520rather%2520than%2520text%250Adescriptions%252C%2520we%2520employ%2520a%2520textual%2520inversion%2520network%2520to%2520learn%2520pseudo-tokens%2520that%250Aencode%2520specific%2520visual%2520contexts%2520or%2520appearance%2520attributes.%2520These%2520learned%250Apseudo-tokens%2520are%2520then%2520incorporated%2520into%2520textual%2520prompts%252C%2520which%2520are%2520fed%2520into%250ACLIP%2527s%2520text%2520encoder%2520to%2520leverage%2520its%2520multi-modal%2520reasoning%2520and%2520enhance%250Ageneralization%2520for%2520identification.%2520Through%2520extensive%2520evaluations%2520on%2520two%2520large%252C%250Apublicly%2520available%2520hand%2520datasets%2520with%2520multi-ethnic%2520representation%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12447v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-HandID%3A%20Vision-Language%20Model%20for%20Hand-Based%20Person%20Identification&entry.906535625=Nathanael%20L.%20Baisa%20and%20Babu%20Pallam%20and%20Amudhavel%20Jayavel&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20person%20identification%20using%20hand%0Aimages%2C%20designed%20specifically%20for%20criminal%20investigations.%20The%20method%20is%0Aparticularly%20valuable%20in%20serious%20crimes%20such%20as%20sexual%20abuse%2C%20where%20hand%20images%0Aare%20often%20the%20only%20identifiable%20evidence%20available.%20Our%20proposed%20method%2C%0ACLIP-HandID%2C%20leverages%20a%20pre-trained%20foundational%20vision-language%20model%20-%20CLIP%0A-%20to%20efficiently%20learn%20discriminative%20deep%20feature%20representations%20from%20hand%0Aimages%20%28input%20to%20CLIP%27s%20image%20encoder%29%20using%20textual%20prompts%20as%20semantic%0Aguidance.%20Since%20hand%20images%20are%20labeled%20with%20indexes%20rather%20than%20text%0Adescriptions%2C%20we%20employ%20a%20textual%20inversion%20network%20to%20learn%20pseudo-tokens%20that%0Aencode%20specific%20visual%20contexts%20or%20appearance%20attributes.%20These%20learned%0Apseudo-tokens%20are%20then%20incorporated%20into%20textual%20prompts%2C%20which%20are%20fed%20into%0ACLIP%27s%20text%20encoder%20to%20leverage%20its%20multi-modal%20reasoning%20and%20enhance%0Ageneralization%20for%20identification.%20Through%20extensive%20evaluations%20on%20two%20large%2C%0Apublicly%20available%20hand%20datasets%20with%20multi-ethnic%20representation%2C%20we%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12447v3&entry.124074799=Read"},
{"title": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration\n  for Efficient and Versatile Embodied Navigation", "author": "Ziyu Zhu and Xilin Wang and Yixuan Li and Zhuofan Zhang and Xiaojian Ma and Yixin Chen and Baoxiong Jia and Wei Liang and Qian Yu and Zhidong Deng and Siyuan Huang and Qing Li", "abstract": "  Embodied scene understanding requires not only comprehending visual-spatial\ninformation that has been observed but also determining where to explore next\nin the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily\nfocus on grounding objects in static observations from 3D reconstruction, such\nas meshes and point clouds, but lack the ability to actively perceive and\nexplore their environment. To address this limitation, we introduce\n\\underline{\\textbf{M}}ove \\underline{\\textbf{t}}o\n\\underline{\\textbf{U}}nderstand (\\textbf{\\model}), a unified framework that\nintegrates active perception with \\underline{\\textbf{3D}} vision-language\nlearning, enabling embodied agents to effectively explore and understand their\nenvironment. This is achieved by three key innovations: 1) Online query-based\nrepresentation learning, enabling direct spatial memory construction from RGB-D\nframes, eliminating the need for explicit 3D reconstruction. 2) A unified\nobjective for grounding and exploring, which represents unexplored locations as\nfrontier queries and jointly optimizes object grounding and frontier selection.\n3) End-to-end trajectory learning that combines\n\\textbf{V}ision-\\textbf{L}anguage-\\textbf{E}xploration pre-training over a\nmillion diverse trajectories collected from both simulated and real-world RGB-D\nsequences. Extensive evaluations across various embodied navigation and\nquestion-answering benchmarks show that MTU3D outperforms state-of-the-art\nreinforcement learning and modular navigation approaches by 14\\%, 23\\%, 9\\%,\nand 2\\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA,\nrespectively. \\model's versatility enables navigation using diverse input\nmodalities, including categories, language descriptions, and reference images.\nThese findings highlight the importance of bridging visual grounding and\nexploration for embodied intelligence.\n", "link": "http://arxiv.org/abs/2507.04047v2", "date": "2025-07-30", "relevancy": 2.737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6937}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Move%20to%20Understand%20a%203D%20Scene%3A%20Bridging%20Visual%20Grounding%20and%20Exploration%0A%20%20for%20Efficient%20and%20Versatile%20Embodied%20Navigation&body=Title%3A%20Move%20to%20Understand%20a%203D%20Scene%3A%20Bridging%20Visual%20Grounding%20and%20Exploration%0A%20%20for%20Efficient%20and%20Versatile%20Embodied%20Navigation%0AAuthor%3A%20Ziyu%20Zhu%20and%20Xilin%20Wang%20and%20Yixuan%20Li%20and%20Zhuofan%20Zhang%20and%20Xiaojian%20Ma%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Wei%20Liang%20and%20Qian%20Yu%20and%20Zhidong%20Deng%20and%20Siyuan%20Huang%20and%20Qing%20Li%0AAbstract%3A%20%20%20Embodied%20scene%20understanding%20requires%20not%20only%20comprehending%20visual-spatial%0Ainformation%20that%20has%20been%20observed%20but%20also%20determining%20where%20to%20explore%20next%0Ain%20the%203D%20physical%20world.%20Existing%203D%20Vision-Language%20%283D-VL%29%20models%20primarily%0Afocus%20on%20grounding%20objects%20in%20static%20observations%20from%203D%20reconstruction%2C%20such%0Aas%20meshes%20and%20point%20clouds%2C%20but%20lack%20the%20ability%20to%20actively%20perceive%20and%0Aexplore%20their%20environment.%20To%20address%20this%20limitation%2C%20we%20introduce%0A%5Cunderline%7B%5Ctextbf%7BM%7D%7Dove%20%5Cunderline%7B%5Ctextbf%7Bt%7D%7Do%0A%5Cunderline%7B%5Ctextbf%7BU%7D%7Dnderstand%20%28%5Ctextbf%7B%5Cmodel%7D%29%2C%20a%20unified%20framework%20that%0Aintegrates%20active%20perception%20with%20%5Cunderline%7B%5Ctextbf%7B3D%7D%7D%20vision-language%0Alearning%2C%20enabling%20embodied%20agents%20to%20effectively%20explore%20and%20understand%20their%0Aenvironment.%20This%20is%20achieved%20by%20three%20key%20innovations%3A%201%29%20Online%20query-based%0Arepresentation%20learning%2C%20enabling%20direct%20spatial%20memory%20construction%20from%20RGB-D%0Aframes%2C%20eliminating%20the%20need%20for%20explicit%203D%20reconstruction.%202%29%20A%20unified%0Aobjective%20for%20grounding%20and%20exploring%2C%20which%20represents%20unexplored%20locations%20as%0Afrontier%20queries%20and%20jointly%20optimizes%20object%20grounding%20and%20frontier%20selection.%0A3%29%20End-to-end%20trajectory%20learning%20that%20combines%0A%5Ctextbf%7BV%7Dision-%5Ctextbf%7BL%7Danguage-%5Ctextbf%7BE%7Dxploration%20pre-training%20over%20a%0Amillion%20diverse%20trajectories%20collected%20from%20both%20simulated%20and%20real-world%20RGB-D%0Asequences.%20Extensive%20evaluations%20across%20various%20embodied%20navigation%20and%0Aquestion-answering%20benchmarks%20show%20that%20MTU3D%20outperforms%20state-of-the-art%0Areinforcement%20learning%20and%20modular%20navigation%20approaches%20by%2014%5C%25%2C%2023%5C%25%2C%209%5C%25%2C%0Aand%202%5C%25%20in%20success%20rate%20on%20HM3D-OVON%2C%20GOAT-Bench%2C%20SG3D%2C%20and%20A-EQA%2C%0Arespectively.%20%5Cmodel%27s%20versatility%20enables%20navigation%20using%20diverse%20input%0Amodalities%2C%20including%20categories%2C%20language%20descriptions%2C%20and%20reference%20images.%0AThese%20findings%20highlight%20the%20importance%20of%20bridging%20visual%20grounding%20and%0Aexploration%20for%20embodied%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMove%2520to%2520Understand%2520a%25203D%2520Scene%253A%2520Bridging%2520Visual%2520Grounding%2520and%2520Exploration%250A%2520%2520for%2520Efficient%2520and%2520Versatile%2520Embodied%2520Navigation%26entry.906535625%3DZiyu%2520Zhu%2520and%2520Xilin%2520Wang%2520and%2520Yixuan%2520Li%2520and%2520Zhuofan%2520Zhang%2520and%2520Xiaojian%2520Ma%2520and%2520Yixin%2520Chen%2520and%2520Baoxiong%2520Jia%2520and%2520Wei%2520Liang%2520and%2520Qian%2520Yu%2520and%2520Zhidong%2520Deng%2520and%2520Siyuan%2520Huang%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Embodied%2520scene%2520understanding%2520requires%2520not%2520only%2520comprehending%2520visual-spatial%250Ainformation%2520that%2520has%2520been%2520observed%2520but%2520also%2520determining%2520where%2520to%2520explore%2520next%250Ain%2520the%25203D%2520physical%2520world.%2520Existing%25203D%2520Vision-Language%2520%25283D-VL%2529%2520models%2520primarily%250Afocus%2520on%2520grounding%2520objects%2520in%2520static%2520observations%2520from%25203D%2520reconstruction%252C%2520such%250Aas%2520meshes%2520and%2520point%2520clouds%252C%2520but%2520lack%2520the%2520ability%2520to%2520actively%2520perceive%2520and%250Aexplore%2520their%2520environment.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250A%255Cunderline%257B%255Ctextbf%257BM%257D%257Dove%2520%255Cunderline%257B%255Ctextbf%257Bt%257D%257Do%250A%255Cunderline%257B%255Ctextbf%257BU%257D%257Dnderstand%2520%2528%255Ctextbf%257B%255Cmodel%257D%2529%252C%2520a%2520unified%2520framework%2520that%250Aintegrates%2520active%2520perception%2520with%2520%255Cunderline%257B%255Ctextbf%257B3D%257D%257D%2520vision-language%250Alearning%252C%2520enabling%2520embodied%2520agents%2520to%2520effectively%2520explore%2520and%2520understand%2520their%250Aenvironment.%2520This%2520is%2520achieved%2520by%2520three%2520key%2520innovations%253A%25201%2529%2520Online%2520query-based%250Arepresentation%2520learning%252C%2520enabling%2520direct%2520spatial%2520memory%2520construction%2520from%2520RGB-D%250Aframes%252C%2520eliminating%2520the%2520need%2520for%2520explicit%25203D%2520reconstruction.%25202%2529%2520A%2520unified%250Aobjective%2520for%2520grounding%2520and%2520exploring%252C%2520which%2520represents%2520unexplored%2520locations%2520as%250Afrontier%2520queries%2520and%2520jointly%2520optimizes%2520object%2520grounding%2520and%2520frontier%2520selection.%250A3%2529%2520End-to-end%2520trajectory%2520learning%2520that%2520combines%250A%255Ctextbf%257BV%257Dision-%255Ctextbf%257BL%257Danguage-%255Ctextbf%257BE%257Dxploration%2520pre-training%2520over%2520a%250Amillion%2520diverse%2520trajectories%2520collected%2520from%2520both%2520simulated%2520and%2520real-world%2520RGB-D%250Asequences.%2520Extensive%2520evaluations%2520across%2520various%2520embodied%2520navigation%2520and%250Aquestion-answering%2520benchmarks%2520show%2520that%2520MTU3D%2520outperforms%2520state-of-the-art%250Areinforcement%2520learning%2520and%2520modular%2520navigation%2520approaches%2520by%252014%255C%2525%252C%252023%255C%2525%252C%25209%255C%2525%252C%250Aand%25202%255C%2525%2520in%2520success%2520rate%2520on%2520HM3D-OVON%252C%2520GOAT-Bench%252C%2520SG3D%252C%2520and%2520A-EQA%252C%250Arespectively.%2520%255Cmodel%2527s%2520versatility%2520enables%2520navigation%2520using%2520diverse%2520input%250Amodalities%252C%2520including%2520categories%252C%2520language%2520descriptions%252C%2520and%2520reference%2520images.%250AThese%2520findings%2520highlight%2520the%2520importance%2520of%2520bridging%2520visual%2520grounding%2520and%250Aexploration%2520for%2520embodied%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Move%20to%20Understand%20a%203D%20Scene%3A%20Bridging%20Visual%20Grounding%20and%20Exploration%0A%20%20for%20Efficient%20and%20Versatile%20Embodied%20Navigation&entry.906535625=Ziyu%20Zhu%20and%20Xilin%20Wang%20and%20Yixuan%20Li%20and%20Zhuofan%20Zhang%20and%20Xiaojian%20Ma%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Wei%20Liang%20and%20Qian%20Yu%20and%20Zhidong%20Deng%20and%20Siyuan%20Huang%20and%20Qing%20Li&entry.1292438233=%20%20Embodied%20scene%20understanding%20requires%20not%20only%20comprehending%20visual-spatial%0Ainformation%20that%20has%20been%20observed%20but%20also%20determining%20where%20to%20explore%20next%0Ain%20the%203D%20physical%20world.%20Existing%203D%20Vision-Language%20%283D-VL%29%20models%20primarily%0Afocus%20on%20grounding%20objects%20in%20static%20observations%20from%203D%20reconstruction%2C%20such%0Aas%20meshes%20and%20point%20clouds%2C%20but%20lack%20the%20ability%20to%20actively%20perceive%20and%0Aexplore%20their%20environment.%20To%20address%20this%20limitation%2C%20we%20introduce%0A%5Cunderline%7B%5Ctextbf%7BM%7D%7Dove%20%5Cunderline%7B%5Ctextbf%7Bt%7D%7Do%0A%5Cunderline%7B%5Ctextbf%7BU%7D%7Dnderstand%20%28%5Ctextbf%7B%5Cmodel%7D%29%2C%20a%20unified%20framework%20that%0Aintegrates%20active%20perception%20with%20%5Cunderline%7B%5Ctextbf%7B3D%7D%7D%20vision-language%0Alearning%2C%20enabling%20embodied%20agents%20to%20effectively%20explore%20and%20understand%20their%0Aenvironment.%20This%20is%20achieved%20by%20three%20key%20innovations%3A%201%29%20Online%20query-based%0Arepresentation%20learning%2C%20enabling%20direct%20spatial%20memory%20construction%20from%20RGB-D%0Aframes%2C%20eliminating%20the%20need%20for%20explicit%203D%20reconstruction.%202%29%20A%20unified%0Aobjective%20for%20grounding%20and%20exploring%2C%20which%20represents%20unexplored%20locations%20as%0Afrontier%20queries%20and%20jointly%20optimizes%20object%20grounding%20and%20frontier%20selection.%0A3%29%20End-to-end%20trajectory%20learning%20that%20combines%0A%5Ctextbf%7BV%7Dision-%5Ctextbf%7BL%7Danguage-%5Ctextbf%7BE%7Dxploration%20pre-training%20over%20a%0Amillion%20diverse%20trajectories%20collected%20from%20both%20simulated%20and%20real-world%20RGB-D%0Asequences.%20Extensive%20evaluations%20across%20various%20embodied%20navigation%20and%0Aquestion-answering%20benchmarks%20show%20that%20MTU3D%20outperforms%20state-of-the-art%0Areinforcement%20learning%20and%20modular%20navigation%20approaches%20by%2014%5C%25%2C%2023%5C%25%2C%209%5C%25%2C%0Aand%202%5C%25%20in%20success%20rate%20on%20HM3D-OVON%2C%20GOAT-Bench%2C%20SG3D%2C%20and%20A-EQA%2C%0Arespectively.%20%5Cmodel%27s%20versatility%20enables%20navigation%20using%20diverse%20input%0Amodalities%2C%20including%20categories%2C%20language%20descriptions%2C%20and%20reference%20images.%0AThese%20findings%20highlight%20the%20importance%20of%20bridging%20visual%20grounding%20and%0Aexploration%20for%20embodied%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04047v2&entry.124074799=Read"},
{"title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding", "author": "Shuquan Lian and Yuhang Wu and Jia Ma and Zihan Song and Bingqi Chen and Xiawu Zheng and Hui Li", "abstract": "  The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.\n", "link": "http://arxiv.org/abs/2507.22025v2", "date": "2025-07-30", "relevancy": 2.6922, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5433}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5418}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-AGILE%3A%20Advancing%20GUI%20Agents%20with%20Effective%20Reinforcement%20Learning%20and%0A%20%20Precise%20Inference-Time%20Grounding&body=Title%3A%20UI-AGILE%3A%20Advancing%20GUI%20Agents%20with%20Effective%20Reinforcement%20Learning%20and%0A%20%20Precise%20Inference-Time%20Grounding%0AAuthor%3A%20Shuquan%20Lian%20and%20Yuhang%20Wu%20and%20Jia%20Ma%20and%20Zihan%20Song%20and%20Bingqi%20Chen%20and%20Xiawu%20Zheng%20and%20Hui%20Li%0AAbstract%3A%20%20%20The%20emergence%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20driven%0Asignificant%20advances%20in%20Graphical%20User%20Interface%20%28GUI%29%20agent%20capabilities.%0ANevertheless%2C%20existing%20GUI%20agent%20training%20and%20inference%20techniques%20still%20suffer%0Afrom%20a%20dilemma%20for%20reasoning%20designs%2C%20ineffective%20reward%2C%20and%20visual%20noise.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20UI-AGILE%2C%20a%20comprehensive%20framework%0Aenhancing%20GUI%20agents%20at%20both%20the%20training%20and%20inference%20stages.%20For%20training%2C%0Awe%20propose%20a%20suite%20of%20improvements%20to%20the%20Supervised%20Fine-Tuning%20%28SFT%29%20process%3A%0A1%29%20a%20Continuous%20Reward%20function%20to%20incentivize%20high-precision%20grounding%3B%202%29%20a%0A%22Simple%20Thinking%22%20reward%20to%20balance%20planning%20with%20speed%20and%20grounding%20accuracy%3B%0Aand%203%29%20a%20Cropping-based%20Resampling%20strategy%20to%20mitigate%20the%20sparse%20reward%0Aproblem%20and%20improve%20learning%20on%20complex%20tasks.%20For%20inference%2C%20we%20present%0ADecomposed%20Grounding%20with%20Selection%2C%20a%20novel%20method%20that%20dramatically%20improves%0Agrounding%20accuracy%20on%20high-resolution%20displays%20by%20breaking%20the%20image%20into%0Asmaller%2C%20manageable%20parts.%20Experiments%20show%20that%20UI-AGILE%20achieves%20the%0Astate-of-the-art%20performance%20on%20two%20benchmarks%20ScreenSpot-Pro%20and%0AScreenSpot-v2.%20For%20instance%2C%20using%20both%20our%20proposed%20training%20and%20inference%0Aenhancement%20methods%20brings%2023%25%20grounding%20accuracy%20improvement%20over%20the%20best%0Abaseline%20on%20ScreenSpot-Pro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-AGILE%253A%2520Advancing%2520GUI%2520Agents%2520with%2520Effective%2520Reinforcement%2520Learning%2520and%250A%2520%2520Precise%2520Inference-Time%2520Grounding%26entry.906535625%3DShuquan%2520Lian%2520and%2520Yuhang%2520Wu%2520and%2520Jia%2520Ma%2520and%2520Zihan%2520Song%2520and%2520Bingqi%2520Chen%2520and%2520Xiawu%2520Zheng%2520and%2520Hui%2520Li%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520driven%250Asignificant%2520advances%2520in%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agent%2520capabilities.%250ANevertheless%252C%2520existing%2520GUI%2520agent%2520training%2520and%2520inference%2520techniques%2520still%2520suffer%250Afrom%2520a%2520dilemma%2520for%2520reasoning%2520designs%252C%2520ineffective%2520reward%252C%2520and%2520visual%2520noise.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520UI-AGILE%252C%2520a%2520comprehensive%2520framework%250Aenhancing%2520GUI%2520agents%2520at%2520both%2520the%2520training%2520and%2520inference%2520stages.%2520For%2520training%252C%250Awe%2520propose%2520a%2520suite%2520of%2520improvements%2520to%2520the%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520process%253A%250A1%2529%2520a%2520Continuous%2520Reward%2520function%2520to%2520incentivize%2520high-precision%2520grounding%253B%25202%2529%2520a%250A%2522Simple%2520Thinking%2522%2520reward%2520to%2520balance%2520planning%2520with%2520speed%2520and%2520grounding%2520accuracy%253B%250Aand%25203%2529%2520a%2520Cropping-based%2520Resampling%2520strategy%2520to%2520mitigate%2520the%2520sparse%2520reward%250Aproblem%2520and%2520improve%2520learning%2520on%2520complex%2520tasks.%2520For%2520inference%252C%2520we%2520present%250ADecomposed%2520Grounding%2520with%2520Selection%252C%2520a%2520novel%2520method%2520that%2520dramatically%2520improves%250Agrounding%2520accuracy%2520on%2520high-resolution%2520displays%2520by%2520breaking%2520the%2520image%2520into%250Asmaller%252C%2520manageable%2520parts.%2520Experiments%2520show%2520that%2520UI-AGILE%2520achieves%2520the%250Astate-of-the-art%2520performance%2520on%2520two%2520benchmarks%2520ScreenSpot-Pro%2520and%250AScreenSpot-v2.%2520For%2520instance%252C%2520using%2520both%2520our%2520proposed%2520training%2520and%2520inference%250Aenhancement%2520methods%2520brings%252023%2525%2520grounding%2520accuracy%2520improvement%2520over%2520the%2520best%250Abaseline%2520on%2520ScreenSpot-Pro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-AGILE%3A%20Advancing%20GUI%20Agents%20with%20Effective%20Reinforcement%20Learning%20and%0A%20%20Precise%20Inference-Time%20Grounding&entry.906535625=Shuquan%20Lian%20and%20Yuhang%20Wu%20and%20Jia%20Ma%20and%20Zihan%20Song%20and%20Bingqi%20Chen%20and%20Xiawu%20Zheng%20and%20Hui%20Li&entry.1292438233=%20%20The%20emergence%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20driven%0Asignificant%20advances%20in%20Graphical%20User%20Interface%20%28GUI%29%20agent%20capabilities.%0ANevertheless%2C%20existing%20GUI%20agent%20training%20and%20inference%20techniques%20still%20suffer%0Afrom%20a%20dilemma%20for%20reasoning%20designs%2C%20ineffective%20reward%2C%20and%20visual%20noise.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20UI-AGILE%2C%20a%20comprehensive%20framework%0Aenhancing%20GUI%20agents%20at%20both%20the%20training%20and%20inference%20stages.%20For%20training%2C%0Awe%20propose%20a%20suite%20of%20improvements%20to%20the%20Supervised%20Fine-Tuning%20%28SFT%29%20process%3A%0A1%29%20a%20Continuous%20Reward%20function%20to%20incentivize%20high-precision%20grounding%3B%202%29%20a%0A%22Simple%20Thinking%22%20reward%20to%20balance%20planning%20with%20speed%20and%20grounding%20accuracy%3B%0Aand%203%29%20a%20Cropping-based%20Resampling%20strategy%20to%20mitigate%20the%20sparse%20reward%0Aproblem%20and%20improve%20learning%20on%20complex%20tasks.%20For%20inference%2C%20we%20present%0ADecomposed%20Grounding%20with%20Selection%2C%20a%20novel%20method%20that%20dramatically%20improves%0Agrounding%20accuracy%20on%20high-resolution%20displays%20by%20breaking%20the%20image%20into%0Asmaller%2C%20manageable%20parts.%20Experiments%20show%20that%20UI-AGILE%20achieves%20the%0Astate-of-the-art%20performance%20on%20two%20benchmarks%20ScreenSpot-Pro%20and%0AScreenSpot-v2.%20For%20instance%2C%20using%20both%20our%20proposed%20training%20and%20inference%0Aenhancement%20methods%20brings%2023%25%20grounding%20accuracy%20improvement%20over%20the%20best%0Abaseline%20on%20ScreenSpot-Pro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22025v2&entry.124074799=Read"},
{"title": "Subgrid BoostCNN: Efficient Boosting of Convolutional Networks via\n  Gradient-Guided Feature Selection", "author": "Biyi Fang and Jean Utke and Truong Vo and Diego Klabjan", "abstract": "  Convolutional Neural Networks (CNNs) have achieved remarkable success across\na wide range of machine learning tasks by leveraging hierarchical feature\nlearning through deep architectures. However, the large number of layers and\nmillions of parameters often make CNNs computationally expensive to train,\nrequiring extensive time and manual tuning to discover optimal architectures.\nIn this paper, we introduce a novel framework for boosting CNN performance that\nintegrates dynamic feature selection with the principles of BoostCNN. Our\napproach incorporates two key strategies: subgrid selection and importance\nsampling, to guide training toward informative regions of the feature space. We\nfurther develop a family of algorithms that embed boosting weights directly\ninto the network training process using a least squares loss formulation. This\nintegration not only alleviates the burden of manual architecture design but\nalso enhances accuracy and efficiency. Experimental results across several\nfine-grained classification benchmarks demonstrate that our boosted CNN\nvariants consistently outperform conventional CNNs in both predictive\nperformance and training speed.\n", "link": "http://arxiv.org/abs/2507.22842v1", "date": "2025-07-30", "relevancy": 2.6899, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6271}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4977}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subgrid%20BoostCNN%3A%20Efficient%20Boosting%20of%20Convolutional%20Networks%20via%0A%20%20Gradient-Guided%20Feature%20Selection&body=Title%3A%20Subgrid%20BoostCNN%3A%20Efficient%20Boosting%20of%20Convolutional%20Networks%20via%0A%20%20Gradient-Guided%20Feature%20Selection%0AAuthor%3A%20Biyi%20Fang%20and%20Jean%20Utke%20and%20Truong%20Vo%20and%20Diego%20Klabjan%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20remarkable%20success%20across%0Aa%20wide%20range%20of%20machine%20learning%20tasks%20by%20leveraging%20hierarchical%20feature%0Alearning%20through%20deep%20architectures.%20However%2C%20the%20large%20number%20of%20layers%20and%0Amillions%20of%20parameters%20often%20make%20CNNs%20computationally%20expensive%20to%20train%2C%0Arequiring%20extensive%20time%20and%20manual%20tuning%20to%20discover%20optimal%20architectures.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20for%20boosting%20CNN%20performance%20that%0Aintegrates%20dynamic%20feature%20selection%20with%20the%20principles%20of%20BoostCNN.%20Our%0Aapproach%20incorporates%20two%20key%20strategies%3A%20subgrid%20selection%20and%20importance%0Asampling%2C%20to%20guide%20training%20toward%20informative%20regions%20of%20the%20feature%20space.%20We%0Afurther%20develop%20a%20family%20of%20algorithms%20that%20embed%20boosting%20weights%20directly%0Ainto%20the%20network%20training%20process%20using%20a%20least%20squares%20loss%20formulation.%20This%0Aintegration%20not%20only%20alleviates%20the%20burden%20of%20manual%20architecture%20design%20but%0Aalso%20enhances%20accuracy%20and%20efficiency.%20Experimental%20results%20across%20several%0Afine-grained%20classification%20benchmarks%20demonstrate%20that%20our%20boosted%20CNN%0Avariants%20consistently%20outperform%20conventional%20CNNs%20in%20both%20predictive%0Aperformance%20and%20training%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubgrid%2520BoostCNN%253A%2520Efficient%2520Boosting%2520of%2520Convolutional%2520Networks%2520via%250A%2520%2520Gradient-Guided%2520Feature%2520Selection%26entry.906535625%3DBiyi%2520Fang%2520and%2520Jean%2520Utke%2520and%2520Truong%2520Vo%2520and%2520Diego%2520Klabjan%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%250Aa%2520wide%2520range%2520of%2520machine%2520learning%2520tasks%2520by%2520leveraging%2520hierarchical%2520feature%250Alearning%2520through%2520deep%2520architectures.%2520However%252C%2520the%2520large%2520number%2520of%2520layers%2520and%250Amillions%2520of%2520parameters%2520often%2520make%2520CNNs%2520computationally%2520expensive%2520to%2520train%252C%250Arequiring%2520extensive%2520time%2520and%2520manual%2520tuning%2520to%2520discover%2520optimal%2520architectures.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520boosting%2520CNN%2520performance%2520that%250Aintegrates%2520dynamic%2520feature%2520selection%2520with%2520the%2520principles%2520of%2520BoostCNN.%2520Our%250Aapproach%2520incorporates%2520two%2520key%2520strategies%253A%2520subgrid%2520selection%2520and%2520importance%250Asampling%252C%2520to%2520guide%2520training%2520toward%2520informative%2520regions%2520of%2520the%2520feature%2520space.%2520We%250Afurther%2520develop%2520a%2520family%2520of%2520algorithms%2520that%2520embed%2520boosting%2520weights%2520directly%250Ainto%2520the%2520network%2520training%2520process%2520using%2520a%2520least%2520squares%2520loss%2520formulation.%2520This%250Aintegration%2520not%2520only%2520alleviates%2520the%2520burden%2520of%2520manual%2520architecture%2520design%2520but%250Aalso%2520enhances%2520accuracy%2520and%2520efficiency.%2520Experimental%2520results%2520across%2520several%250Afine-grained%2520classification%2520benchmarks%2520demonstrate%2520that%2520our%2520boosted%2520CNN%250Avariants%2520consistently%2520outperform%2520conventional%2520CNNs%2520in%2520both%2520predictive%250Aperformance%2520and%2520training%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subgrid%20BoostCNN%3A%20Efficient%20Boosting%20of%20Convolutional%20Networks%20via%0A%20%20Gradient-Guided%20Feature%20Selection&entry.906535625=Biyi%20Fang%20and%20Jean%20Utke%20and%20Truong%20Vo%20and%20Diego%20Klabjan&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20remarkable%20success%20across%0Aa%20wide%20range%20of%20machine%20learning%20tasks%20by%20leveraging%20hierarchical%20feature%0Alearning%20through%20deep%20architectures.%20However%2C%20the%20large%20number%20of%20layers%20and%0Amillions%20of%20parameters%20often%20make%20CNNs%20computationally%20expensive%20to%20train%2C%0Arequiring%20extensive%20time%20and%20manual%20tuning%20to%20discover%20optimal%20architectures.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20for%20boosting%20CNN%20performance%20that%0Aintegrates%20dynamic%20feature%20selection%20with%20the%20principles%20of%20BoostCNN.%20Our%0Aapproach%20incorporates%20two%20key%20strategies%3A%20subgrid%20selection%20and%20importance%0Asampling%2C%20to%20guide%20training%20toward%20informative%20regions%20of%20the%20feature%20space.%20We%0Afurther%20develop%20a%20family%20of%20algorithms%20that%20embed%20boosting%20weights%20directly%0Ainto%20the%20network%20training%20process%20using%20a%20least%20squares%20loss%20formulation.%20This%0Aintegration%20not%20only%20alleviates%20the%20burden%20of%20manual%20architecture%20design%20but%0Aalso%20enhances%20accuracy%20and%20efficiency.%20Experimental%20results%20across%20several%0Afine-grained%20classification%20benchmarks%20demonstrate%20that%20our%20boosted%20CNN%0Avariants%20consistently%20outperform%20conventional%20CNNs%20in%20both%20predictive%0Aperformance%20and%20training%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22842v1&entry.124074799=Read"},
{"title": "ComicsPAP: understanding comic strips by picking the correct panel", "author": "Emanuele Vivoli and Artemis Llabr\u00e9s and Mohamed Ali Souibgui and Marco Bertini and Ernest Valveny Llobet and Dimosthenis Karatzas", "abstract": "  Large multimodal models (LMMs) have made impressive strides in image\ncaptioning, VQA, and video comprehension, yet they still struggle with the\nintricate temporal and spatial cues found in comics. To address this gap, we\nintroduce ComicsPAP, a large-scale benchmark designed for comic strip\nunderstanding. Comprising over 100k samples and organized into 5 subtasks under\na Pick-a-Panel framework, ComicsPAP demands models to identify the missing\npanel in a sequence. Our evaluations, conducted under both multi-image and\nsingle-image protocols, reveal that current state-of-the-art LMMs perform near\nchance on these tasks, underscoring significant limitations in capturing\nsequential and contextual dependencies. To close the gap, we adapted LMMs for\ncomic strip understanding, obtaining better results on ComicsPAP than 10x\nbigger models, demonstrating that ComicsPAP offers a robust resource to drive\nfuture research in multimodal comic comprehension.\n", "link": "http://arxiv.org/abs/2503.08561v3", "date": "2025-07-30", "relevancy": 2.6786, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComicsPAP%3A%20understanding%20comic%20strips%20by%20picking%20the%20correct%20panel&body=Title%3A%20ComicsPAP%3A%20understanding%20comic%20strips%20by%20picking%20the%20correct%20panel%0AAuthor%3A%20Emanuele%20Vivoli%20and%20Artemis%20Llabr%C3%A9s%20and%20Mohamed%20Ali%20Souibgui%20and%20Marco%20Bertini%20and%20Ernest%20Valveny%20Llobet%20and%20Dimosthenis%20Karatzas%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20made%20impressive%20strides%20in%20image%0Acaptioning%2C%20VQA%2C%20and%20video%20comprehension%2C%20yet%20they%20still%20struggle%20with%20the%0Aintricate%20temporal%20and%20spatial%20cues%20found%20in%20comics.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20ComicsPAP%2C%20a%20large-scale%20benchmark%20designed%20for%20comic%20strip%0Aunderstanding.%20Comprising%20over%20100k%20samples%20and%20organized%20into%205%20subtasks%20under%0Aa%20Pick-a-Panel%20framework%2C%20ComicsPAP%20demands%20models%20to%20identify%20the%20missing%0Apanel%20in%20a%20sequence.%20Our%20evaluations%2C%20conducted%20under%20both%20multi-image%20and%0Asingle-image%20protocols%2C%20reveal%20that%20current%20state-of-the-art%20LMMs%20perform%20near%0Achance%20on%20these%20tasks%2C%20underscoring%20significant%20limitations%20in%20capturing%0Asequential%20and%20contextual%20dependencies.%20To%20close%20the%20gap%2C%20we%20adapted%20LMMs%20for%0Acomic%20strip%20understanding%2C%20obtaining%20better%20results%20on%20ComicsPAP%20than%2010x%0Abigger%20models%2C%20demonstrating%20that%20ComicsPAP%20offers%20a%20robust%20resource%20to%20drive%0Afuture%20research%20in%20multimodal%20comic%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08561v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComicsPAP%253A%2520understanding%2520comic%2520strips%2520by%2520picking%2520the%2520correct%2520panel%26entry.906535625%3DEmanuele%2520Vivoli%2520and%2520Artemis%2520Llabr%25C3%25A9s%2520and%2520Mohamed%2520Ali%2520Souibgui%2520and%2520Marco%2520Bertini%2520and%2520Ernest%2520Valveny%2520Llobet%2520and%2520Dimosthenis%2520Karatzas%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520made%2520impressive%2520strides%2520in%2520image%250Acaptioning%252C%2520VQA%252C%2520and%2520video%2520comprehension%252C%2520yet%2520they%2520still%2520struggle%2520with%2520the%250Aintricate%2520temporal%2520and%2520spatial%2520cues%2520found%2520in%2520comics.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520ComicsPAP%252C%2520a%2520large-scale%2520benchmark%2520designed%2520for%2520comic%2520strip%250Aunderstanding.%2520Comprising%2520over%2520100k%2520samples%2520and%2520organized%2520into%25205%2520subtasks%2520under%250Aa%2520Pick-a-Panel%2520framework%252C%2520ComicsPAP%2520demands%2520models%2520to%2520identify%2520the%2520missing%250Apanel%2520in%2520a%2520sequence.%2520Our%2520evaluations%252C%2520conducted%2520under%2520both%2520multi-image%2520and%250Asingle-image%2520protocols%252C%2520reveal%2520that%2520current%2520state-of-the-art%2520LMMs%2520perform%2520near%250Achance%2520on%2520these%2520tasks%252C%2520underscoring%2520significant%2520limitations%2520in%2520capturing%250Asequential%2520and%2520contextual%2520dependencies.%2520To%2520close%2520the%2520gap%252C%2520we%2520adapted%2520LMMs%2520for%250Acomic%2520strip%2520understanding%252C%2520obtaining%2520better%2520results%2520on%2520ComicsPAP%2520than%252010x%250Abigger%2520models%252C%2520demonstrating%2520that%2520ComicsPAP%2520offers%2520a%2520robust%2520resource%2520to%2520drive%250Afuture%2520research%2520in%2520multimodal%2520comic%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08561v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComicsPAP%3A%20understanding%20comic%20strips%20by%20picking%20the%20correct%20panel&entry.906535625=Emanuele%20Vivoli%20and%20Artemis%20Llabr%C3%A9s%20and%20Mohamed%20Ali%20Souibgui%20and%20Marco%20Bertini%20and%20Ernest%20Valveny%20Llobet%20and%20Dimosthenis%20Karatzas&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20made%20impressive%20strides%20in%20image%0Acaptioning%2C%20VQA%2C%20and%20video%20comprehension%2C%20yet%20they%20still%20struggle%20with%20the%0Aintricate%20temporal%20and%20spatial%20cues%20found%20in%20comics.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20ComicsPAP%2C%20a%20large-scale%20benchmark%20designed%20for%20comic%20strip%0Aunderstanding.%20Comprising%20over%20100k%20samples%20and%20organized%20into%205%20subtasks%20under%0Aa%20Pick-a-Panel%20framework%2C%20ComicsPAP%20demands%20models%20to%20identify%20the%20missing%0Apanel%20in%20a%20sequence.%20Our%20evaluations%2C%20conducted%20under%20both%20multi-image%20and%0Asingle-image%20protocols%2C%20reveal%20that%20current%20state-of-the-art%20LMMs%20perform%20near%0Achance%20on%20these%20tasks%2C%20underscoring%20significant%20limitations%20in%20capturing%0Asequential%20and%20contextual%20dependencies.%20To%20close%20the%20gap%2C%20we%20adapted%20LMMs%20for%0Acomic%20strip%20understanding%2C%20obtaining%20better%20results%20on%20ComicsPAP%20than%2010x%0Abigger%20models%2C%20demonstrating%20that%20ComicsPAP%20offers%20a%20robust%20resource%20to%20drive%0Afuture%20research%20in%20multimodal%20comic%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08561v3&entry.124074799=Read"},
{"title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text\n  Pairing", "author": "Federico Girella and Davide Talon and Ziyue Liu and Zanxi Ruan and Yiming Wang and Marco Cristani", "abstract": "  Fashion design is a complex creative process that blends visual and textual\nexpressions. Designers convey ideas through sketches, which define spatial\nstructure and design elements, and textual descriptions, capturing material,\ntexture, and stylistic details. In this paper, we present LOcalized Text and\nSketch for fashion image generation (LOTS), an approach for compositional\nsketch-text based generation of complete fashion outlooks. LOTS leverages a\nglobal description with paired localized sketch + text information for\nconditioning and introduces a novel step-based merging strategy for diffusion\nadaptation. First, a Modularized Pair-Centric representation encodes sketches\nand text into a shared latent space while preserving independent localized\nfeatures; then, a Diffusion Pair Guidance phase integrates both local and\nglobal conditioning via attention-based guidance within the diffusion model's\nmulti-step denoising process. To validate our method, we build on Fashionpedia\nto release Sketchy, the first fashion dataset where multiple text-sketch pairs\nare provided per image. Quantitative results show LOTS achieves\nstate-of-the-art image generation performance on both global and localized\nmetrics, while qualitative examples and a human evaluation study highlight its\nunprecedented level of design customization.\n", "link": "http://arxiv.org/abs/2507.22627v1", "date": "2025-07-30", "relevancy": 2.664, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7659}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6603}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOTS%20of%20Fashion%21%20Multi-Conditioning%20for%20Image%20Generation%20via%20Sketch-Text%0A%20%20Pairing&body=Title%3A%20LOTS%20of%20Fashion%21%20Multi-Conditioning%20for%20Image%20Generation%20via%20Sketch-Text%0A%20%20Pairing%0AAuthor%3A%20Federico%20Girella%20and%20Davide%20Talon%20and%20Ziyue%20Liu%20and%20Zanxi%20Ruan%20and%20Yiming%20Wang%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20Fashion%20design%20is%20a%20complex%20creative%20process%20that%20blends%20visual%20and%20textual%0Aexpressions.%20Designers%20convey%20ideas%20through%20sketches%2C%20which%20define%20spatial%0Astructure%20and%20design%20elements%2C%20and%20textual%20descriptions%2C%20capturing%20material%2C%0Atexture%2C%20and%20stylistic%20details.%20In%20this%20paper%2C%20we%20present%20LOcalized%20Text%20and%0ASketch%20for%20fashion%20image%20generation%20%28LOTS%29%2C%20an%20approach%20for%20compositional%0Asketch-text%20based%20generation%20of%20complete%20fashion%20outlooks.%20LOTS%20leverages%20a%0Aglobal%20description%20with%20paired%20localized%20sketch%20%2B%20text%20information%20for%0Aconditioning%20and%20introduces%20a%20novel%20step-based%20merging%20strategy%20for%20diffusion%0Aadaptation.%20First%2C%20a%20Modularized%20Pair-Centric%20representation%20encodes%20sketches%0Aand%20text%20into%20a%20shared%20latent%20space%20while%20preserving%20independent%20localized%0Afeatures%3B%20then%2C%20a%20Diffusion%20Pair%20Guidance%20phase%20integrates%20both%20local%20and%0Aglobal%20conditioning%20via%20attention-based%20guidance%20within%20the%20diffusion%20model%27s%0Amulti-step%20denoising%20process.%20To%20validate%20our%20method%2C%20we%20build%20on%20Fashionpedia%0Ato%20release%20Sketchy%2C%20the%20first%20fashion%20dataset%20where%20multiple%20text-sketch%20pairs%0Aare%20provided%20per%20image.%20Quantitative%20results%20show%20LOTS%20achieves%0Astate-of-the-art%20image%20generation%20performance%20on%20both%20global%20and%20localized%0Ametrics%2C%20while%20qualitative%20examples%20and%20a%20human%20evaluation%20study%20highlight%20its%0Aunprecedented%20level%20of%20design%20customization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOTS%2520of%2520Fashion%2521%2520Multi-Conditioning%2520for%2520Image%2520Generation%2520via%2520Sketch-Text%250A%2520%2520Pairing%26entry.906535625%3DFederico%2520Girella%2520and%2520Davide%2520Talon%2520and%2520Ziyue%2520Liu%2520and%2520Zanxi%2520Ruan%2520and%2520Yiming%2520Wang%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520Fashion%2520design%2520is%2520a%2520complex%2520creative%2520process%2520that%2520blends%2520visual%2520and%2520textual%250Aexpressions.%2520Designers%2520convey%2520ideas%2520through%2520sketches%252C%2520which%2520define%2520spatial%250Astructure%2520and%2520design%2520elements%252C%2520and%2520textual%2520descriptions%252C%2520capturing%2520material%252C%250Atexture%252C%2520and%2520stylistic%2520details.%2520In%2520this%2520paper%252C%2520we%2520present%2520LOcalized%2520Text%2520and%250ASketch%2520for%2520fashion%2520image%2520generation%2520%2528LOTS%2529%252C%2520an%2520approach%2520for%2520compositional%250Asketch-text%2520based%2520generation%2520of%2520complete%2520fashion%2520outlooks.%2520LOTS%2520leverages%2520a%250Aglobal%2520description%2520with%2520paired%2520localized%2520sketch%2520%252B%2520text%2520information%2520for%250Aconditioning%2520and%2520introduces%2520a%2520novel%2520step-based%2520merging%2520strategy%2520for%2520diffusion%250Aadaptation.%2520First%252C%2520a%2520Modularized%2520Pair-Centric%2520representation%2520encodes%2520sketches%250Aand%2520text%2520into%2520a%2520shared%2520latent%2520space%2520while%2520preserving%2520independent%2520localized%250Afeatures%253B%2520then%252C%2520a%2520Diffusion%2520Pair%2520Guidance%2520phase%2520integrates%2520both%2520local%2520and%250Aglobal%2520conditioning%2520via%2520attention-based%2520guidance%2520within%2520the%2520diffusion%2520model%2527s%250Amulti-step%2520denoising%2520process.%2520To%2520validate%2520our%2520method%252C%2520we%2520build%2520on%2520Fashionpedia%250Ato%2520release%2520Sketchy%252C%2520the%2520first%2520fashion%2520dataset%2520where%2520multiple%2520text-sketch%2520pairs%250Aare%2520provided%2520per%2520image.%2520Quantitative%2520results%2520show%2520LOTS%2520achieves%250Astate-of-the-art%2520image%2520generation%2520performance%2520on%2520both%2520global%2520and%2520localized%250Ametrics%252C%2520while%2520qualitative%2520examples%2520and%2520a%2520human%2520evaluation%2520study%2520highlight%2520its%250Aunprecedented%2520level%2520of%2520design%2520customization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOTS%20of%20Fashion%21%20Multi-Conditioning%20for%20Image%20Generation%20via%20Sketch-Text%0A%20%20Pairing&entry.906535625=Federico%20Girella%20and%20Davide%20Talon%20and%20Ziyue%20Liu%20and%20Zanxi%20Ruan%20and%20Yiming%20Wang%20and%20Marco%20Cristani&entry.1292438233=%20%20Fashion%20design%20is%20a%20complex%20creative%20process%20that%20blends%20visual%20and%20textual%0Aexpressions.%20Designers%20convey%20ideas%20through%20sketches%2C%20which%20define%20spatial%0Astructure%20and%20design%20elements%2C%20and%20textual%20descriptions%2C%20capturing%20material%2C%0Atexture%2C%20and%20stylistic%20details.%20In%20this%20paper%2C%20we%20present%20LOcalized%20Text%20and%0ASketch%20for%20fashion%20image%20generation%20%28LOTS%29%2C%20an%20approach%20for%20compositional%0Asketch-text%20based%20generation%20of%20complete%20fashion%20outlooks.%20LOTS%20leverages%20a%0Aglobal%20description%20with%20paired%20localized%20sketch%20%2B%20text%20information%20for%0Aconditioning%20and%20introduces%20a%20novel%20step-based%20merging%20strategy%20for%20diffusion%0Aadaptation.%20First%2C%20a%20Modularized%20Pair-Centric%20representation%20encodes%20sketches%0Aand%20text%20into%20a%20shared%20latent%20space%20while%20preserving%20independent%20localized%0Afeatures%3B%20then%2C%20a%20Diffusion%20Pair%20Guidance%20phase%20integrates%20both%20local%20and%0Aglobal%20conditioning%20via%20attention-based%20guidance%20within%20the%20diffusion%20model%27s%0Amulti-step%20denoising%20process.%20To%20validate%20our%20method%2C%20we%20build%20on%20Fashionpedia%0Ato%20release%20Sketchy%2C%20the%20first%20fashion%20dataset%20where%20multiple%20text-sketch%20pairs%0Aare%20provided%20per%20image.%20Quantitative%20results%20show%20LOTS%20achieves%0Astate-of-the-art%20image%20generation%20performance%20on%20both%20global%20and%20localized%0Ametrics%2C%20while%20qualitative%20examples%20and%20a%20human%20evaluation%20study%20highlight%20its%0Aunprecedented%20level%20of%20design%20customization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22627v1&entry.124074799=Read"},
{"title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\n  Synthesis", "author": "Xinyi Liu and Xiaoyi Zhang and Ziyun Zhang and Yan Lu", "abstract": "  Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://microsoft.github.io/FIVE-UI-Evol/ .\n", "link": "http://arxiv.org/abs/2504.11257v4", "date": "2025-07-30", "relevancy": 2.6252, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5321}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5304}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI-E2I-Synth%3A%20Advancing%20GUI%20Grounding%20with%20Large-Scale%20Instruction%0A%20%20Synthesis&body=Title%3A%20UI-E2I-Synth%3A%20Advancing%20GUI%20Grounding%20with%20Large-Scale%20Instruction%0A%20%20Synthesis%0AAuthor%3A%20Xinyi%20Liu%20and%20Xiaoyi%20Zhang%20and%20Ziyun%20Zhang%20and%20Yan%20Lu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20are%20accelerating%20the%0Adevelopment%20of%20Graphical%20User%20Interface%20%28GUI%29%20agents%20that%20utilize%20human-like%0Avision%20perception%20capabilities%20to%20enhance%20productivity%20on%20digital%20devices.%0ACompared%20to%20approaches%20predicated%20on%20GUI%20metadata%2C%20which%20are%20platform-dependent%0Aand%20vulnerable%20to%20implementation%20variations%2C%20vision-based%20approaches%20offer%0Abroader%20applicability.%20In%20this%20vision-based%20paradigm%2C%20the%20GUI%20instruction%0Agrounding%2C%20which%20maps%20user%20instruction%20to%20the%20location%20of%20corresponding%20element%0Aon%20the%20given%20screenshot%2C%20remains%20a%20critical%20challenge%2C%20particularly%20due%20to%0Alimited%20public%20training%20dataset%20and%20resource-intensive%20manual%20instruction%20data%0Aannotation.%20In%20this%20paper%2C%20we%20delve%20into%20unexplored%20challenges%20in%20this%20task%0Aincluding%20element-to-screen%20ratio%2C%20unbalanced%20element%20type%2C%20and%20implicit%0Ainstruction.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20large-scale%20data%0Asynthesis%20pipeline%20UI-E2I-Synth%20for%20generating%20varying%20complex%20instruction%0Adatasets%20using%20GPT-4o%20instead%20of%20human%20annotators.%20Furthermore%2C%20we%20propose%20a%0Anew%20GUI%20instruction%20grounding%20benchmark%20UI-I2E-Bench%2C%20which%20is%20designed%20to%0Aaddress%20the%20limitations%20of%20existing%20benchmarks%20by%20incorporating%20diverse%0Aannotation%20aspects.%20Our%20model%2C%20trained%20on%20the%20synthesized%20data%2C%20achieves%0Asuperior%20performance%20in%20GUI%20instruction%20grounding%2C%20demonstrating%20the%0Aadvancements%20of%20proposed%20data%20synthesis%20pipeline.%20The%20proposed%20benchmark%2C%0Aaccompanied%20by%20extensive%20analyses%2C%20provides%20practical%20insights%20for%20future%0Aresearch%20in%20GUI%20grounding.%20We%20will%20release%20corresponding%20artifacts%20at%0Ahttps%3A//microsoft.github.io/FIVE-UI-Evol/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11257v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI-E2I-Synth%253A%2520Advancing%2520GUI%2520Grounding%2520with%2520Large-Scale%2520Instruction%250A%2520%2520Synthesis%26entry.906535625%3DXinyi%2520Liu%2520and%2520Xiaoyi%2520Zhang%2520and%2520Ziyun%2520Zhang%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%2520are%2520accelerating%2520the%250Adevelopment%2520of%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520that%2520utilize%2520human-like%250Avision%2520perception%2520capabilities%2520to%2520enhance%2520productivity%2520on%2520digital%2520devices.%250ACompared%2520to%2520approaches%2520predicated%2520on%2520GUI%2520metadata%252C%2520which%2520are%2520platform-dependent%250Aand%2520vulnerable%2520to%2520implementation%2520variations%252C%2520vision-based%2520approaches%2520offer%250Abroader%2520applicability.%2520In%2520this%2520vision-based%2520paradigm%252C%2520the%2520GUI%2520instruction%250Agrounding%252C%2520which%2520maps%2520user%2520instruction%2520to%2520the%2520location%2520of%2520corresponding%2520element%250Aon%2520the%2520given%2520screenshot%252C%2520remains%2520a%2520critical%2520challenge%252C%2520particularly%2520due%2520to%250Alimited%2520public%2520training%2520dataset%2520and%2520resource-intensive%2520manual%2520instruction%2520data%250Aannotation.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520unexplored%2520challenges%2520in%2520this%2520task%250Aincluding%2520element-to-screen%2520ratio%252C%2520unbalanced%2520element%2520type%252C%2520and%2520implicit%250Ainstruction.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520large-scale%2520data%250Asynthesis%2520pipeline%2520UI-E2I-Synth%2520for%2520generating%2520varying%2520complex%2520instruction%250Adatasets%2520using%2520GPT-4o%2520instead%2520of%2520human%2520annotators.%2520Furthermore%252C%2520we%2520propose%2520a%250Anew%2520GUI%2520instruction%2520grounding%2520benchmark%2520UI-I2E-Bench%252C%2520which%2520is%2520designed%2520to%250Aaddress%2520the%2520limitations%2520of%2520existing%2520benchmarks%2520by%2520incorporating%2520diverse%250Aannotation%2520aspects.%2520Our%2520model%252C%2520trained%2520on%2520the%2520synthesized%2520data%252C%2520achieves%250Asuperior%2520performance%2520in%2520GUI%2520instruction%2520grounding%252C%2520demonstrating%2520the%250Aadvancements%2520of%2520proposed%2520data%2520synthesis%2520pipeline.%2520The%2520proposed%2520benchmark%252C%250Aaccompanied%2520by%2520extensive%2520analyses%252C%2520provides%2520practical%2520insights%2520for%2520future%250Aresearch%2520in%2520GUI%2520grounding.%2520We%2520will%2520release%2520corresponding%2520artifacts%2520at%250Ahttps%253A//microsoft.github.io/FIVE-UI-Evol/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11257v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI-E2I-Synth%3A%20Advancing%20GUI%20Grounding%20with%20Large-Scale%20Instruction%0A%20%20Synthesis&entry.906535625=Xinyi%20Liu%20and%20Xiaoyi%20Zhang%20and%20Ziyun%20Zhang%20and%20Yan%20Lu&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20are%20accelerating%20the%0Adevelopment%20of%20Graphical%20User%20Interface%20%28GUI%29%20agents%20that%20utilize%20human-like%0Avision%20perception%20capabilities%20to%20enhance%20productivity%20on%20digital%20devices.%0ACompared%20to%20approaches%20predicated%20on%20GUI%20metadata%2C%20which%20are%20platform-dependent%0Aand%20vulnerable%20to%20implementation%20variations%2C%20vision-based%20approaches%20offer%0Abroader%20applicability.%20In%20this%20vision-based%20paradigm%2C%20the%20GUI%20instruction%0Agrounding%2C%20which%20maps%20user%20instruction%20to%20the%20location%20of%20corresponding%20element%0Aon%20the%20given%20screenshot%2C%20remains%20a%20critical%20challenge%2C%20particularly%20due%20to%0Alimited%20public%20training%20dataset%20and%20resource-intensive%20manual%20instruction%20data%0Aannotation.%20In%20this%20paper%2C%20we%20delve%20into%20unexplored%20challenges%20in%20this%20task%0Aincluding%20element-to-screen%20ratio%2C%20unbalanced%20element%20type%2C%20and%20implicit%0Ainstruction.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20large-scale%20data%0Asynthesis%20pipeline%20UI-E2I-Synth%20for%20generating%20varying%20complex%20instruction%0Adatasets%20using%20GPT-4o%20instead%20of%20human%20annotators.%20Furthermore%2C%20we%20propose%20a%0Anew%20GUI%20instruction%20grounding%20benchmark%20UI-I2E-Bench%2C%20which%20is%20designed%20to%0Aaddress%20the%20limitations%20of%20existing%20benchmarks%20by%20incorporating%20diverse%0Aannotation%20aspects.%20Our%20model%2C%20trained%20on%20the%20synthesized%20data%2C%20achieves%0Asuperior%20performance%20in%20GUI%20instruction%20grounding%2C%20demonstrating%20the%0Aadvancements%20of%20proposed%20data%20synthesis%20pipeline.%20The%20proposed%20benchmark%2C%0Aaccompanied%20by%20extensive%20analyses%2C%20provides%20practical%20insights%20for%20future%0Aresearch%20in%20GUI%20grounding.%20We%20will%20release%20corresponding%20artifacts%20at%0Ahttps%3A//microsoft.github.io/FIVE-UI-Evol/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11257v4&entry.124074799=Read"},
{"title": "Neutral Residues: Revisiting Adapters for Model Extension", "author": "Franck Signe Talla and Edouard Grave and Herv\u00e9 J\u00e9gou", "abstract": "  We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.\n", "link": "http://arxiv.org/abs/2410.02744v2", "date": "2025-07-30", "relevancy": 2.6155, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neutral%20Residues%3A%20Revisiting%20Adapters%20for%20Model%20Extension&body=Title%3A%20Neutral%20Residues%3A%20Revisiting%20Adapters%20for%20Model%20Extension%0AAuthor%3A%20Franck%20Signe%20Talla%20and%20Edouard%20Grave%20and%20Herv%C3%A9%20J%C3%A9gou%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20extending%20a%20pretrained%20large%20language%20model%20to%20a%0Anew%20domain%20that%20was%20not%20seen%20during%20training.%20Standard%20techniques%2C%20such%20as%0Afinetuning%20or%20low-rank%20adaptation%20%28LoRA%29%20are%20successful%20at%20domain%20adaptation%2C%0Abut%20do%20not%20formally%20add%20capacity%20to%20the%20model.%20This%20often%20leads%20to%20a%20trade-off%2C%0Abetween%20performing%20well%20on%20the%20new%20domain%20vs.%20degrading%20performance%20on%20the%0Aoriginal%20domain.%20Here%2C%20we%20revisit%20and%20improve%20adapters%20to%20extend%20LLMs%20from%0Athree%20angles%3A%20data%2C%20architecture%20and%20training%20procedure%2C%20which%20are%0Aadvantageously%20considered%20jointly.%20The%20resulting%20method%2C%20called%20neutral%0Aresidues%2C%20modifies%20adapters%20in%20a%20way%20that%20leads%20each%20new%20residual%20block%20to%0Aoutput%20near-zeros%20on%20the%20original%20domain.%20This%20solution%20leads%20to%20strong%20results%0Awhen%20adapting%20a%20state-of-the-art%20model%20originally%20trained%20on%20English%20to%20a%20new%0Alanguage.%20Neutral%20residues%20significantly%20outperform%20competing%20approaches%20such%0Aas%20finetuning%2C%20LoRA%20or%20vanilla%20adapters%20in%20terms%20of%20the%20trade-off%20between%0Alearning%20the%20new%20language%20and%20not%20forgetting%20English.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeutral%2520Residues%253A%2520Revisiting%2520Adapters%2520for%2520Model%2520Extension%26entry.906535625%3DFranck%2520Signe%2520Talla%2520and%2520Edouard%2520Grave%2520and%2520Herv%25C3%25A9%2520J%25C3%25A9gou%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520extending%2520a%2520pretrained%2520large%2520language%2520model%2520to%2520a%250Anew%2520domain%2520that%2520was%2520not%2520seen%2520during%2520training.%2520Standard%2520techniques%252C%2520such%2520as%250Afinetuning%2520or%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520are%2520successful%2520at%2520domain%2520adaptation%252C%250Abut%2520do%2520not%2520formally%2520add%2520capacity%2520to%2520the%2520model.%2520This%2520often%2520leads%2520to%2520a%2520trade-off%252C%250Abetween%2520performing%2520well%2520on%2520the%2520new%2520domain%2520vs.%2520degrading%2520performance%2520on%2520the%250Aoriginal%2520domain.%2520Here%252C%2520we%2520revisit%2520and%2520improve%2520adapters%2520to%2520extend%2520LLMs%2520from%250Athree%2520angles%253A%2520data%252C%2520architecture%2520and%2520training%2520procedure%252C%2520which%2520are%250Aadvantageously%2520considered%2520jointly.%2520The%2520resulting%2520method%252C%2520called%2520neutral%250Aresidues%252C%2520modifies%2520adapters%2520in%2520a%2520way%2520that%2520leads%2520each%2520new%2520residual%2520block%2520to%250Aoutput%2520near-zeros%2520on%2520the%2520original%2520domain.%2520This%2520solution%2520leads%2520to%2520strong%2520results%250Awhen%2520adapting%2520a%2520state-of-the-art%2520model%2520originally%2520trained%2520on%2520English%2520to%2520a%2520new%250Alanguage.%2520Neutral%2520residues%2520significantly%2520outperform%2520competing%2520approaches%2520such%250Aas%2520finetuning%252C%2520LoRA%2520or%2520vanilla%2520adapters%2520in%2520terms%2520of%2520the%2520trade-off%2520between%250Alearning%2520the%2520new%2520language%2520and%2520not%2520forgetting%2520English.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neutral%20Residues%3A%20Revisiting%20Adapters%20for%20Model%20Extension&entry.906535625=Franck%20Signe%20Talla%20and%20Edouard%20Grave%20and%20Herv%C3%A9%20J%C3%A9gou&entry.1292438233=%20%20We%20address%20the%20problem%20of%20extending%20a%20pretrained%20large%20language%20model%20to%20a%0Anew%20domain%20that%20was%20not%20seen%20during%20training.%20Standard%20techniques%2C%20such%20as%0Afinetuning%20or%20low-rank%20adaptation%20%28LoRA%29%20are%20successful%20at%20domain%20adaptation%2C%0Abut%20do%20not%20formally%20add%20capacity%20to%20the%20model.%20This%20often%20leads%20to%20a%20trade-off%2C%0Abetween%20performing%20well%20on%20the%20new%20domain%20vs.%20degrading%20performance%20on%20the%0Aoriginal%20domain.%20Here%2C%20we%20revisit%20and%20improve%20adapters%20to%20extend%20LLMs%20from%0Athree%20angles%3A%20data%2C%20architecture%20and%20training%20procedure%2C%20which%20are%0Aadvantageously%20considered%20jointly.%20The%20resulting%20method%2C%20called%20neutral%0Aresidues%2C%20modifies%20adapters%20in%20a%20way%20that%20leads%20each%20new%20residual%20block%20to%0Aoutput%20near-zeros%20on%20the%20original%20domain.%20This%20solution%20leads%20to%20strong%20results%0Awhen%20adapting%20a%20state-of-the-art%20model%20originally%20trained%20on%20English%20to%20a%20new%0Alanguage.%20Neutral%20residues%20significantly%20outperform%20competing%20approaches%20such%0Aas%20finetuning%2C%20LoRA%20or%20vanilla%20adapters%20in%20terms%20of%20the%20trade-off%20between%0Alearning%20the%20new%20language%20and%20not%20forgetting%20English.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02744v2&entry.124074799=Read"},
{"title": "Scaling RL to Long Videos", "author": "Yukang Chen and Wei Huang and Baifeng Shi and Qinghao Hu and Hanrong Ye and Ligeng Zhu and Zhijian Liu and Pavlo Molchanov and Jan Kautz and Xiaojuan Qi and Sifei Liu and Hongxu Yin and Yao Lu and Song Han", "abstract": "  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).\n", "link": "http://arxiv.org/abs/2507.07966v3", "date": "2025-07-30", "relevancy": 2.6116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5289}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20RL%20to%20Long%20Videos&body=Title%3A%20Scaling%20RL%20to%20Long%20Videos%0AAuthor%3A%20Yukang%20Chen%20and%20Wei%20Huang%20and%20Baifeng%20Shi%20and%20Qinghao%20Hu%20and%20Hanrong%20Ye%20and%20Ligeng%20Zhu%20and%20Zhijian%20Liu%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Xiaojuan%20Qi%20and%20Sifei%20Liu%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20We%20introduce%20a%20full-stack%20framework%20that%20scales%20up%20reasoning%20in%0Avision-language%20models%20%28VLMs%29%20to%20long%20videos%2C%20leveraging%20reinforcement%0Alearning.%20We%20address%20the%20unique%20challenges%20of%20long%20video%20reasoning%20by%0Aintegrating%20three%20critical%20components%3A%20%281%29%20a%20large-scale%20dataset%2C%0ALongVideo-Reason%2C%20comprising%20104K%20long%20video%20QA%20pairs%20with%20high-quality%0Areasoning%20annotations%20across%20diverse%20domains%20such%20as%20sports%2C%20games%2C%20and%20vlogs%3B%0A%282%29%20a%20two-stage%20training%20pipeline%20that%20extends%20VLMs%20with%20chain-of-thought%0Asupervised%20fine-tuning%20%28CoT-SFT%29%20and%20reinforcement%20learning%20%28RL%29%3B%20and%20%283%29%20a%0Atraining%20infrastructure%20for%20long%20video%20RL%2C%20named%20Multi-modal%20Reinforcement%0ASequence%20Parallelism%20%28MR-SP%29%2C%20which%20incorporates%20sequence%20parallelism%20and%20a%0AvLLM-based%20engine%20tailored%20for%20long%20video%2C%20using%20cached%20video%20embeddings%20for%0Aefficient%20rollout%20and%20prefilling.%20In%20our%20experiments%2C%20LongVILA-R1-7B%20achieves%0Astrong%20performance%20on%20video%20benchmarks%2C%20reaching%2065.1%25%20and%2071.1%25%20accuracy%20on%0AVideoMME%20without%20and%20with%20subtitles%2C%20respectively%2C%20and%20consistently%0Aoutperforming%20LongVILA-7B%20across%20multiple%20benchmarks.%20Moreover%2C%20LongVILA-R1-7B%0Asupports%20processing%20up%20to%208%2C192%20video%20frames%20per%20video%2C%20and%20configurable%20FPS%0Asettings.%20Notably%2C%20our%20MR-SP%20system%20achieves%20up%20to%202.1x%20speedup%20on%20long%20video%0ARL%20training.%20In%20addition%2C%20we%20release%20our%20training%20system%20for%20public%0Aavailability%20that%20supports%20RL%20training%20on%20various%20modalities%20%28video%2C%20text%2C%20and%0Aaudio%29%2C%20various%20models%20%28VILA%20and%20Qwen%20series%29%2C%20and%20even%20image%20and%20video%0Ageneration%20models.%20On%20a%20single%20A100%20node%20%288%20GPUs%29%2C%20it%20supports%20RL%20training%20on%0Ahour-long%20videos%20%28e.g.%2C%203%2C600%20frames%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520RL%2520to%2520Long%2520Videos%26entry.906535625%3DYukang%2520Chen%2520and%2520Wei%2520Huang%2520and%2520Baifeng%2520Shi%2520and%2520Qinghao%2520Hu%2520and%2520Hanrong%2520Ye%2520and%2520Ligeng%2520Zhu%2520and%2520Zhijian%2520Liu%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Xiaojuan%2520Qi%2520and%2520Sifei%2520Liu%2520and%2520Hongxu%2520Yin%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520full-stack%2520framework%2520that%2520scales%2520up%2520reasoning%2520in%250Avision-language%2520models%2520%2528VLMs%2529%2520to%2520long%2520videos%252C%2520leveraging%2520reinforcement%250Alearning.%2520We%2520address%2520the%2520unique%2520challenges%2520of%2520long%2520video%2520reasoning%2520by%250Aintegrating%2520three%2520critical%2520components%253A%2520%25281%2529%2520a%2520large-scale%2520dataset%252C%250ALongVideo-Reason%252C%2520comprising%2520104K%2520long%2520video%2520QA%2520pairs%2520with%2520high-quality%250Areasoning%2520annotations%2520across%2520diverse%2520domains%2520such%2520as%2520sports%252C%2520games%252C%2520and%2520vlogs%253B%250A%25282%2529%2520a%2520two-stage%2520training%2520pipeline%2520that%2520extends%2520VLMs%2520with%2520chain-of-thought%250Asupervised%2520fine-tuning%2520%2528CoT-SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529%253B%2520and%2520%25283%2529%2520a%250Atraining%2520infrastructure%2520for%2520long%2520video%2520RL%252C%2520named%2520Multi-modal%2520Reinforcement%250ASequence%2520Parallelism%2520%2528MR-SP%2529%252C%2520which%2520incorporates%2520sequence%2520parallelism%2520and%2520a%250AvLLM-based%2520engine%2520tailored%2520for%2520long%2520video%252C%2520using%2520cached%2520video%2520embeddings%2520for%250Aefficient%2520rollout%2520and%2520prefilling.%2520In%2520our%2520experiments%252C%2520LongVILA-R1-7B%2520achieves%250Astrong%2520performance%2520on%2520video%2520benchmarks%252C%2520reaching%252065.1%2525%2520and%252071.1%2525%2520accuracy%2520on%250AVideoMME%2520without%2520and%2520with%2520subtitles%252C%2520respectively%252C%2520and%2520consistently%250Aoutperforming%2520LongVILA-7B%2520across%2520multiple%2520benchmarks.%2520Moreover%252C%2520LongVILA-R1-7B%250Asupports%2520processing%2520up%2520to%25208%252C192%2520video%2520frames%2520per%2520video%252C%2520and%2520configurable%2520FPS%250Asettings.%2520Notably%252C%2520our%2520MR-SP%2520system%2520achieves%2520up%2520to%25202.1x%2520speedup%2520on%2520long%2520video%250ARL%2520training.%2520In%2520addition%252C%2520we%2520release%2520our%2520training%2520system%2520for%2520public%250Aavailability%2520that%2520supports%2520RL%2520training%2520on%2520various%2520modalities%2520%2528video%252C%2520text%252C%2520and%250Aaudio%2529%252C%2520various%2520models%2520%2528VILA%2520and%2520Qwen%2520series%2529%252C%2520and%2520even%2520image%2520and%2520video%250Ageneration%2520models.%2520On%2520a%2520single%2520A100%2520node%2520%25288%2520GPUs%2529%252C%2520it%2520supports%2520RL%2520training%2520on%250Ahour-long%2520videos%2520%2528e.g.%252C%25203%252C600%2520frames%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20RL%20to%20Long%20Videos&entry.906535625=Yukang%20Chen%20and%20Wei%20Huang%20and%20Baifeng%20Shi%20and%20Qinghao%20Hu%20and%20Hanrong%20Ye%20and%20Ligeng%20Zhu%20and%20Zhijian%20Liu%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Xiaojuan%20Qi%20and%20Sifei%20Liu%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20We%20introduce%20a%20full-stack%20framework%20that%20scales%20up%20reasoning%20in%0Avision-language%20models%20%28VLMs%29%20to%20long%20videos%2C%20leveraging%20reinforcement%0Alearning.%20We%20address%20the%20unique%20challenges%20of%20long%20video%20reasoning%20by%0Aintegrating%20three%20critical%20components%3A%20%281%29%20a%20large-scale%20dataset%2C%0ALongVideo-Reason%2C%20comprising%20104K%20long%20video%20QA%20pairs%20with%20high-quality%0Areasoning%20annotations%20across%20diverse%20domains%20such%20as%20sports%2C%20games%2C%20and%20vlogs%3B%0A%282%29%20a%20two-stage%20training%20pipeline%20that%20extends%20VLMs%20with%20chain-of-thought%0Asupervised%20fine-tuning%20%28CoT-SFT%29%20and%20reinforcement%20learning%20%28RL%29%3B%20and%20%283%29%20a%0Atraining%20infrastructure%20for%20long%20video%20RL%2C%20named%20Multi-modal%20Reinforcement%0ASequence%20Parallelism%20%28MR-SP%29%2C%20which%20incorporates%20sequence%20parallelism%20and%20a%0AvLLM-based%20engine%20tailored%20for%20long%20video%2C%20using%20cached%20video%20embeddings%20for%0Aefficient%20rollout%20and%20prefilling.%20In%20our%20experiments%2C%20LongVILA-R1-7B%20achieves%0Astrong%20performance%20on%20video%20benchmarks%2C%20reaching%2065.1%25%20and%2071.1%25%20accuracy%20on%0AVideoMME%20without%20and%20with%20subtitles%2C%20respectively%2C%20and%20consistently%0Aoutperforming%20LongVILA-7B%20across%20multiple%20benchmarks.%20Moreover%2C%20LongVILA-R1-7B%0Asupports%20processing%20up%20to%208%2C192%20video%20frames%20per%20video%2C%20and%20configurable%20FPS%0Asettings.%20Notably%2C%20our%20MR-SP%20system%20achieves%20up%20to%202.1x%20speedup%20on%20long%20video%0ARL%20training.%20In%20addition%2C%20we%20release%20our%20training%20system%20for%20public%0Aavailability%20that%20supports%20RL%20training%20on%20various%20modalities%20%28video%2C%20text%2C%20and%0Aaudio%29%2C%20various%20models%20%28VILA%20and%20Qwen%20series%29%2C%20and%20even%20image%20and%20video%0Ageneration%20models.%20On%20a%20single%20A100%20node%20%288%20GPUs%29%2C%20it%20supports%20RL%20training%20on%0Ahour-long%20videos%20%28e.g.%2C%203%2C600%20frames%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07966v3&entry.124074799=Read"},
{"title": "FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation", "author": "Yasser Benigmim and Mohammad Fahes and Tuan-Hung Vu and Andrei Bursuc and Raoul de Charette", "abstract": "  In this paper, we challenge the conventional practice in Open-Vocabulary\nSemantic Segmentation (OVSS) of using averaged class-wise text embeddings,\nwhich are typically obtained by encoding each class name with multiple\ntemplates (e.g., a photo of <class>, a sketch of a <class>). We investigate the\nimpact of templates for OVSS, and find that for each class, there exist\nsingle-template classifiers--which we refer to as class-experts--that\nsignificantly outperform the conventional averaged classifier. First, to\nidentify these class-experts, we introduce a novel approach that estimates them\nwithout any labeled data or training. By leveraging the class-wise prediction\nentropy of single-template classifiers, we select those yielding the lowest\nentropy as the most reliable class-experts. Second, we combine the outputs of\nclass-experts in a new fusion process. Our plug-and-play method, coined FLOSS,\nis orthogonal and complementary to existing OVSS methods, offering an\nimprovement without the need for additional labels or training. Extensive\nexperiments show that FLOSS consistently enhances state-of-the-art OVSS models,\ngeneralizes well across datasets with different distribution shifts, and\ndelivers substantial improvements in low-data scenarios where only a few\nunlabeled images are available. Our code is available at\nhttps://github.com/yasserben/FLOSS .\n", "link": "http://arxiv.org/abs/2504.10487v2", "date": "2025-07-30", "relevancy": 2.6031, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5271}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLOSS%3A%20Free%20Lunch%20in%20Open-vocabulary%20Semantic%20Segmentation&body=Title%3A%20FLOSS%3A%20Free%20Lunch%20in%20Open-vocabulary%20Semantic%20Segmentation%0AAuthor%3A%20Yasser%20Benigmim%20and%20Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Raoul%20de%20Charette%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20challenge%20the%20conventional%20practice%20in%20Open-Vocabulary%0ASemantic%20Segmentation%20%28OVSS%29%20of%20using%20averaged%20class-wise%20text%20embeddings%2C%0Awhich%20are%20typically%20obtained%20by%20encoding%20each%20class%20name%20with%20multiple%0Atemplates%20%28e.g.%2C%20a%20photo%20of%20%3Cclass%3E%2C%20a%20sketch%20of%20a%20%3Cclass%3E%29.%20We%20investigate%20the%0Aimpact%20of%20templates%20for%20OVSS%2C%20and%20find%20that%20for%20each%20class%2C%20there%20exist%0Asingle-template%20classifiers--which%20we%20refer%20to%20as%20class-experts--that%0Asignificantly%20outperform%20the%20conventional%20averaged%20classifier.%20First%2C%20to%0Aidentify%20these%20class-experts%2C%20we%20introduce%20a%20novel%20approach%20that%20estimates%20them%0Awithout%20any%20labeled%20data%20or%20training.%20By%20leveraging%20the%20class-wise%20prediction%0Aentropy%20of%20single-template%20classifiers%2C%20we%20select%20those%20yielding%20the%20lowest%0Aentropy%20as%20the%20most%20reliable%20class-experts.%20Second%2C%20we%20combine%20the%20outputs%20of%0Aclass-experts%20in%20a%20new%20fusion%20process.%20Our%20plug-and-play%20method%2C%20coined%20FLOSS%2C%0Ais%20orthogonal%20and%20complementary%20to%20existing%20OVSS%20methods%2C%20offering%20an%0Aimprovement%20without%20the%20need%20for%20additional%20labels%20or%20training.%20Extensive%0Aexperiments%20show%20that%20FLOSS%20consistently%20enhances%20state-of-the-art%20OVSS%20models%2C%0Ageneralizes%20well%20across%20datasets%20with%20different%20distribution%20shifts%2C%20and%0Adelivers%20substantial%20improvements%20in%20low-data%20scenarios%20where%20only%20a%20few%0Aunlabeled%20images%20are%20available.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yasserben/FLOSS%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10487v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLOSS%253A%2520Free%2520Lunch%2520in%2520Open-vocabulary%2520Semantic%2520Segmentation%26entry.906535625%3DYasser%2520Benigmim%2520and%2520Mohammad%2520Fahes%2520and%2520Tuan-Hung%2520Vu%2520and%2520Andrei%2520Bursuc%2520and%2520Raoul%2520de%2520Charette%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520challenge%2520the%2520conventional%2520practice%2520in%2520Open-Vocabulary%250ASemantic%2520Segmentation%2520%2528OVSS%2529%2520of%2520using%2520averaged%2520class-wise%2520text%2520embeddings%252C%250Awhich%2520are%2520typically%2520obtained%2520by%2520encoding%2520each%2520class%2520name%2520with%2520multiple%250Atemplates%2520%2528e.g.%252C%2520a%2520photo%2520of%2520%253Cclass%253E%252C%2520a%2520sketch%2520of%2520a%2520%253Cclass%253E%2529.%2520We%2520investigate%2520the%250Aimpact%2520of%2520templates%2520for%2520OVSS%252C%2520and%2520find%2520that%2520for%2520each%2520class%252C%2520there%2520exist%250Asingle-template%2520classifiers--which%2520we%2520refer%2520to%2520as%2520class-experts--that%250Asignificantly%2520outperform%2520the%2520conventional%2520averaged%2520classifier.%2520First%252C%2520to%250Aidentify%2520these%2520class-experts%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%2520estimates%2520them%250Awithout%2520any%2520labeled%2520data%2520or%2520training.%2520By%2520leveraging%2520the%2520class-wise%2520prediction%250Aentropy%2520of%2520single-template%2520classifiers%252C%2520we%2520select%2520those%2520yielding%2520the%2520lowest%250Aentropy%2520as%2520the%2520most%2520reliable%2520class-experts.%2520Second%252C%2520we%2520combine%2520the%2520outputs%2520of%250Aclass-experts%2520in%2520a%2520new%2520fusion%2520process.%2520Our%2520plug-and-play%2520method%252C%2520coined%2520FLOSS%252C%250Ais%2520orthogonal%2520and%2520complementary%2520to%2520existing%2520OVSS%2520methods%252C%2520offering%2520an%250Aimprovement%2520without%2520the%2520need%2520for%2520additional%2520labels%2520or%2520training.%2520Extensive%250Aexperiments%2520show%2520that%2520FLOSS%2520consistently%2520enhances%2520state-of-the-art%2520OVSS%2520models%252C%250Ageneralizes%2520well%2520across%2520datasets%2520with%2520different%2520distribution%2520shifts%252C%2520and%250Adelivers%2520substantial%2520improvements%2520in%2520low-data%2520scenarios%2520where%2520only%2520a%2520few%250Aunlabeled%2520images%2520are%2520available.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/yasserben/FLOSS%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10487v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLOSS%3A%20Free%20Lunch%20in%20Open-vocabulary%20Semantic%20Segmentation&entry.906535625=Yasser%20Benigmim%20and%20Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Raoul%20de%20Charette&entry.1292438233=%20%20In%20this%20paper%2C%20we%20challenge%20the%20conventional%20practice%20in%20Open-Vocabulary%0ASemantic%20Segmentation%20%28OVSS%29%20of%20using%20averaged%20class-wise%20text%20embeddings%2C%0Awhich%20are%20typically%20obtained%20by%20encoding%20each%20class%20name%20with%20multiple%0Atemplates%20%28e.g.%2C%20a%20photo%20of%20%3Cclass%3E%2C%20a%20sketch%20of%20a%20%3Cclass%3E%29.%20We%20investigate%20the%0Aimpact%20of%20templates%20for%20OVSS%2C%20and%20find%20that%20for%20each%20class%2C%20there%20exist%0Asingle-template%20classifiers--which%20we%20refer%20to%20as%20class-experts--that%0Asignificantly%20outperform%20the%20conventional%20averaged%20classifier.%20First%2C%20to%0Aidentify%20these%20class-experts%2C%20we%20introduce%20a%20novel%20approach%20that%20estimates%20them%0Awithout%20any%20labeled%20data%20or%20training.%20By%20leveraging%20the%20class-wise%20prediction%0Aentropy%20of%20single-template%20classifiers%2C%20we%20select%20those%20yielding%20the%20lowest%0Aentropy%20as%20the%20most%20reliable%20class-experts.%20Second%2C%20we%20combine%20the%20outputs%20of%0Aclass-experts%20in%20a%20new%20fusion%20process.%20Our%20plug-and-play%20method%2C%20coined%20FLOSS%2C%0Ais%20orthogonal%20and%20complementary%20to%20existing%20OVSS%20methods%2C%20offering%20an%0Aimprovement%20without%20the%20need%20for%20additional%20labels%20or%20training.%20Extensive%0Aexperiments%20show%20that%20FLOSS%20consistently%20enhances%20state-of-the-art%20OVSS%20models%2C%0Ageneralizes%20well%20across%20datasets%20with%20different%20distribution%20shifts%2C%20and%0Adelivers%20substantial%20improvements%20in%20low-data%20scenarios%20where%20only%20a%20few%0Aunlabeled%20images%20are%20available.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yasserben/FLOSS%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10487v2&entry.124074799=Read"},
{"title": "The Cooperative Network Architecture: Learning Structured Networks as\n  Representation of Sensory Patterns", "author": "Pascal J. Sager and Jan M. Deriu and Benjamin F. Grewe and Thilo Stadelmann and Christoph von der Malsburg", "abstract": "  We introduce the Cooperative Network Architecture (CNA), a model that\nrepresents sensory signals using structured, recurrently connected networks of\nneurons, termed \"nets.\" Nets are dynamically assembled from overlapping net\nfragments, which are learned based on statistical regularities in sensory\ninput. This architecture offers robustness to noise, deformation, and\nout-of-distribution data, addressing challenges in current vision systems from\na novel perspective. We demonstrate that net fragments can be learned without\nsupervision and flexibly recombined to encode novel patterns, enabling figure\ncompletion and resilience to noise. Our findings establish CNA as a promising\nparadigm for developing neural representations that integrate local feature\nprocessing with global structure formation, providing a foundation for future\nresearch on invariant object recognition.\n", "link": "http://arxiv.org/abs/2407.05650v4", "date": "2025-07-30", "relevancy": 2.5619, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Cooperative%20Network%20Architecture%3A%20Learning%20Structured%20Networks%20as%0A%20%20Representation%20of%20Sensory%20Patterns&body=Title%3A%20The%20Cooperative%20Network%20Architecture%3A%20Learning%20Structured%20Networks%20as%0A%20%20Representation%20of%20Sensory%20Patterns%0AAuthor%3A%20Pascal%20J.%20Sager%20and%20Jan%20M.%20Deriu%20and%20Benjamin%20F.%20Grewe%20and%20Thilo%20Stadelmann%20and%20Christoph%20von%20der%20Malsburg%0AAbstract%3A%20%20%20We%20introduce%20the%20Cooperative%20Network%20Architecture%20%28CNA%29%2C%20a%20model%20that%0Arepresents%20sensory%20signals%20using%20structured%2C%20recurrently%20connected%20networks%20of%0Aneurons%2C%20termed%20%22nets.%22%20Nets%20are%20dynamically%20assembled%20from%20overlapping%20net%0Afragments%2C%20which%20are%20learned%20based%20on%20statistical%20regularities%20in%20sensory%0Ainput.%20This%20architecture%20offers%20robustness%20to%20noise%2C%20deformation%2C%20and%0Aout-of-distribution%20data%2C%20addressing%20challenges%20in%20current%20vision%20systems%20from%0Aa%20novel%20perspective.%20We%20demonstrate%20that%20net%20fragments%20can%20be%20learned%20without%0Asupervision%20and%20flexibly%20recombined%20to%20encode%20novel%20patterns%2C%20enabling%20figure%0Acompletion%20and%20resilience%20to%20noise.%20Our%20findings%20establish%20CNA%20as%20a%20promising%0Aparadigm%20for%20developing%20neural%20representations%20that%20integrate%20local%20feature%0Aprocessing%20with%20global%20structure%20formation%2C%20providing%20a%20foundation%20for%20future%0Aresearch%20on%20invariant%20object%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05650v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Cooperative%2520Network%2520Architecture%253A%2520Learning%2520Structured%2520Networks%2520as%250A%2520%2520Representation%2520of%2520Sensory%2520Patterns%26entry.906535625%3DPascal%2520J.%2520Sager%2520and%2520Jan%2520M.%2520Deriu%2520and%2520Benjamin%2520F.%2520Grewe%2520and%2520Thilo%2520Stadelmann%2520and%2520Christoph%2520von%2520der%2520Malsburg%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Cooperative%2520Network%2520Architecture%2520%2528CNA%2529%252C%2520a%2520model%2520that%250Arepresents%2520sensory%2520signals%2520using%2520structured%252C%2520recurrently%2520connected%2520networks%2520of%250Aneurons%252C%2520termed%2520%2522nets.%2522%2520Nets%2520are%2520dynamically%2520assembled%2520from%2520overlapping%2520net%250Afragments%252C%2520which%2520are%2520learned%2520based%2520on%2520statistical%2520regularities%2520in%2520sensory%250Ainput.%2520This%2520architecture%2520offers%2520robustness%2520to%2520noise%252C%2520deformation%252C%2520and%250Aout-of-distribution%2520data%252C%2520addressing%2520challenges%2520in%2520current%2520vision%2520systems%2520from%250Aa%2520novel%2520perspective.%2520We%2520demonstrate%2520that%2520net%2520fragments%2520can%2520be%2520learned%2520without%250Asupervision%2520and%2520flexibly%2520recombined%2520to%2520encode%2520novel%2520patterns%252C%2520enabling%2520figure%250Acompletion%2520and%2520resilience%2520to%2520noise.%2520Our%2520findings%2520establish%2520CNA%2520as%2520a%2520promising%250Aparadigm%2520for%2520developing%2520neural%2520representations%2520that%2520integrate%2520local%2520feature%250Aprocessing%2520with%2520global%2520structure%2520formation%252C%2520providing%2520a%2520foundation%2520for%2520future%250Aresearch%2520on%2520invariant%2520object%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05650v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Cooperative%20Network%20Architecture%3A%20Learning%20Structured%20Networks%20as%0A%20%20Representation%20of%20Sensory%20Patterns&entry.906535625=Pascal%20J.%20Sager%20and%20Jan%20M.%20Deriu%20and%20Benjamin%20F.%20Grewe%20and%20Thilo%20Stadelmann%20and%20Christoph%20von%20der%20Malsburg&entry.1292438233=%20%20We%20introduce%20the%20Cooperative%20Network%20Architecture%20%28CNA%29%2C%20a%20model%20that%0Arepresents%20sensory%20signals%20using%20structured%2C%20recurrently%20connected%20networks%20of%0Aneurons%2C%20termed%20%22nets.%22%20Nets%20are%20dynamically%20assembled%20from%20overlapping%20net%0Afragments%2C%20which%20are%20learned%20based%20on%20statistical%20regularities%20in%20sensory%0Ainput.%20This%20architecture%20offers%20robustness%20to%20noise%2C%20deformation%2C%20and%0Aout-of-distribution%20data%2C%20addressing%20challenges%20in%20current%20vision%20systems%20from%0Aa%20novel%20perspective.%20We%20demonstrate%20that%20net%20fragments%20can%20be%20learned%20without%0Asupervision%20and%20flexibly%20recombined%20to%20encode%20novel%20patterns%2C%20enabling%20figure%0Acompletion%20and%20resilience%20to%20noise.%20Our%20findings%20establish%20CNA%20as%20a%20promising%0Aparadigm%20for%20developing%20neural%20representations%20that%20integrate%20local%20feature%0Aprocessing%20with%20global%20structure%20formation%2C%20providing%20a%20foundation%20for%20future%0Aresearch%20on%20invariant%20object%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05650v4&entry.124074799=Read"},
{"title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "author": "Inaya Rahmanisa and Lyzander Marciano Andrylie and Krisna Mahardika Ihsani and Alfan Farizki Wicaksono and Haryo Akbarianto Wibowo and Alham Fikri Aji", "abstract": "  Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.\n", "link": "http://arxiv.org/abs/2507.22581v1", "date": "2025-07-30", "relevancy": 2.5534, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Influence%20of%20Amplifying%20Language-Specific%20Neurons&body=Title%3A%20Unveiling%20the%20Influence%20of%20Amplifying%20Language-Specific%20Neurons%0AAuthor%3A%20Inaya%20Rahmanisa%20and%20Lyzander%20Marciano%20Andrylie%20and%20Krisna%20Mahardika%20Ihsani%20and%20Alfan%20Farizki%20Wicaksono%20and%20Haryo%20Akbarianto%20Wibowo%20and%20Alham%20Fikri%20Aji%0AAbstract%3A%20%20%20Language-specific%20neurons%20in%20LLMs%20that%20strongly%20correlate%20with%20individual%0Alanguages%20have%20been%20shown%20to%20influence%20model%20behavior%20by%20deactivating%20them.%0AHowever%2C%20their%20role%20in%20amplification%20remains%20underexplored.%20This%20work%0Ainvestigates%20the%20effect%20of%20amplifying%20language-specific%20neurons%20through%0Ainterventions%20across%2018%20languages%2C%20including%20low-resource%20ones%2C%20using%20three%0Amodels%20primarily%20trained%20in%20different%20languages.%20We%20compare%20amplification%0Afactors%20by%20their%20effectiveness%20in%20steering%20to%20the%20target%20language%20using%20a%0Aproposed%20Language%20Steering%20Shift%20%28LSS%29%20evaluation%20score%2C%20then%20evaluate%20it%20on%0Adownstream%20tasks%3A%20commonsense%20reasoning%20%28XCOPA%2C%20XWinograd%29%2C%20knowledge%0A%28Include%29%2C%20and%20translation%20%28FLORES%29.%20The%20optimal%20amplification%20factors%0Aeffectively%20steer%20output%20toward%20nearly%20all%20tested%20languages.%20Intervention%20using%0Athis%20factor%20on%20downstream%20tasks%20improves%20self-language%20performance%20in%20some%0Acases%20but%20generally%20degrades%20cross-language%20results.%20These%20findings%20highlight%0Athe%20effect%20of%20language-specific%20neurons%20in%20multilingual%20behavior%2C%20where%0Aamplification%20can%20be%20beneficial%20especially%20for%20low-resource%20languages%2C%20but%0Aprovides%20limited%20advantage%20for%20cross-lingual%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Influence%2520of%2520Amplifying%2520Language-Specific%2520Neurons%26entry.906535625%3DInaya%2520Rahmanisa%2520and%2520Lyzander%2520Marciano%2520Andrylie%2520and%2520Krisna%2520Mahardika%2520Ihsani%2520and%2520Alfan%2520Farizki%2520Wicaksono%2520and%2520Haryo%2520Akbarianto%2520Wibowo%2520and%2520Alham%2520Fikri%2520Aji%26entry.1292438233%3D%2520%2520Language-specific%2520neurons%2520in%2520LLMs%2520that%2520strongly%2520correlate%2520with%2520individual%250Alanguages%2520have%2520been%2520shown%2520to%2520influence%2520model%2520behavior%2520by%2520deactivating%2520them.%250AHowever%252C%2520their%2520role%2520in%2520amplification%2520remains%2520underexplored.%2520This%2520work%250Ainvestigates%2520the%2520effect%2520of%2520amplifying%2520language-specific%2520neurons%2520through%250Ainterventions%2520across%252018%2520languages%252C%2520including%2520low-resource%2520ones%252C%2520using%2520three%250Amodels%2520primarily%2520trained%2520in%2520different%2520languages.%2520We%2520compare%2520amplification%250Afactors%2520by%2520their%2520effectiveness%2520in%2520steering%2520to%2520the%2520target%2520language%2520using%2520a%250Aproposed%2520Language%2520Steering%2520Shift%2520%2528LSS%2529%2520evaluation%2520score%252C%2520then%2520evaluate%2520it%2520on%250Adownstream%2520tasks%253A%2520commonsense%2520reasoning%2520%2528XCOPA%252C%2520XWinograd%2529%252C%2520knowledge%250A%2528Include%2529%252C%2520and%2520translation%2520%2528FLORES%2529.%2520The%2520optimal%2520amplification%2520factors%250Aeffectively%2520steer%2520output%2520toward%2520nearly%2520all%2520tested%2520languages.%2520Intervention%2520using%250Athis%2520factor%2520on%2520downstream%2520tasks%2520improves%2520self-language%2520performance%2520in%2520some%250Acases%2520but%2520generally%2520degrades%2520cross-language%2520results.%2520These%2520findings%2520highlight%250Athe%2520effect%2520of%2520language-specific%2520neurons%2520in%2520multilingual%2520behavior%252C%2520where%250Aamplification%2520can%2520be%2520beneficial%2520especially%2520for%2520low-resource%2520languages%252C%2520but%250Aprovides%2520limited%2520advantage%2520for%2520cross-lingual%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Influence%20of%20Amplifying%20Language-Specific%20Neurons&entry.906535625=Inaya%20Rahmanisa%20and%20Lyzander%20Marciano%20Andrylie%20and%20Krisna%20Mahardika%20Ihsani%20and%20Alfan%20Farizki%20Wicaksono%20and%20Haryo%20Akbarianto%20Wibowo%20and%20Alham%20Fikri%20Aji&entry.1292438233=%20%20Language-specific%20neurons%20in%20LLMs%20that%20strongly%20correlate%20with%20individual%0Alanguages%20have%20been%20shown%20to%20influence%20model%20behavior%20by%20deactivating%20them.%0AHowever%2C%20their%20role%20in%20amplification%20remains%20underexplored.%20This%20work%0Ainvestigates%20the%20effect%20of%20amplifying%20language-specific%20neurons%20through%0Ainterventions%20across%2018%20languages%2C%20including%20low-resource%20ones%2C%20using%20three%0Amodels%20primarily%20trained%20in%20different%20languages.%20We%20compare%20amplification%0Afactors%20by%20their%20effectiveness%20in%20steering%20to%20the%20target%20language%20using%20a%0Aproposed%20Language%20Steering%20Shift%20%28LSS%29%20evaluation%20score%2C%20then%20evaluate%20it%20on%0Adownstream%20tasks%3A%20commonsense%20reasoning%20%28XCOPA%2C%20XWinograd%29%2C%20knowledge%0A%28Include%29%2C%20and%20translation%20%28FLORES%29.%20The%20optimal%20amplification%20factors%0Aeffectively%20steer%20output%20toward%20nearly%20all%20tested%20languages.%20Intervention%20using%0Athis%20factor%20on%20downstream%20tasks%20improves%20self-language%20performance%20in%20some%0Acases%20but%20generally%20degrades%20cross-language%20results.%20These%20findings%20highlight%0Athe%20effect%20of%20language-specific%20neurons%20in%20multilingual%20behavior%2C%20where%0Aamplification%20can%20be%20beneficial%20especially%20for%20low-resource%20languages%2C%20but%0Aprovides%20limited%20advantage%20for%20cross-lingual%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22581v1&entry.124074799=Read"},
{"title": "Hyperbolic Graph Learning: A Comprehensive Review", "author": "Menglin Yang and Min Zhou and Tong Zhang and Jiahong Liu and Zhihao Li and Lujia Pan and Hui Xiong and Irwin King", "abstract": "  Graph representation learning in Euclidean space, despite its widespread\nadoption and proven utility in many domains, often struggles to effectively\ncapture the inherent hierarchical and complex relational structures prevalent\nin real-world data, particularly for datasets exhibiting a highly non-Euclidean\nlatent anatomy or power-law distributions. Hyperbolic geometry, with its\nconstant negative curvature and exponential growth property, naturally\naccommodates such structures, offering a promising alternative for learning\nrich graph representations. This survey paper provides a comprehensive review\nof the rapidly evolving field of Hyperbolic Graph Learning (HGL). We\nsystematically categorize and analyze existing methods broadly dividing them\ninto (1) hyperbolic graph embedding-based techniques, (2) graph neural\nnetwork-based hyperbolic models, and (3) emerging paradigms. Beyond\nmethodologies, we extensively discuss diverse applications of HGL across\nmultiple domains, including recommender systems, knowledge graphs,\nbioinformatics, and other relevant scenarios, demonstrating the broad\napplicability and effectiveness of hyperbolic geometry in real-world graph\nlearning tasks. Most importantly, we identify several key challenges that serve\nas directions for advancing HGL, including handling complex data structures,\ndeveloping geometry-aware learning objectives, ensuring trustworthy and\nscalable implementations, and integrating with foundation models, e.g., large\nlanguage models. We highlight promising research opportunities in this exciting\ninterdisciplinary area. A comprehensive repository can be found at\nhttps://github.com/digailab/awesome-hyperbolic-graph-learning.\n", "link": "http://arxiv.org/abs/2202.13852v3", "date": "2025-07-30", "relevancy": 2.541, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5458}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.507}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Graph%20Learning%3A%20A%20Comprehensive%20Review&body=Title%3A%20Hyperbolic%20Graph%20Learning%3A%20A%20Comprehensive%20Review%0AAuthor%3A%20Menglin%20Yang%20and%20Min%20Zhou%20and%20Tong%20Zhang%20and%20Jiahong%20Liu%20and%20Zhihao%20Li%20and%20Lujia%20Pan%20and%20Hui%20Xiong%20and%20Irwin%20King%0AAbstract%3A%20%20%20Graph%20representation%20learning%20in%20Euclidean%20space%2C%20despite%20its%20widespread%0Aadoption%20and%20proven%20utility%20in%20many%20domains%2C%20often%20struggles%20to%20effectively%0Acapture%20the%20inherent%20hierarchical%20and%20complex%20relational%20structures%20prevalent%0Ain%20real-world%20data%2C%20particularly%20for%20datasets%20exhibiting%20a%20highly%20non-Euclidean%0Alatent%20anatomy%20or%20power-law%20distributions.%20Hyperbolic%20geometry%2C%20with%20its%0Aconstant%20negative%20curvature%20and%20exponential%20growth%20property%2C%20naturally%0Aaccommodates%20such%20structures%2C%20offering%20a%20promising%20alternative%20for%20learning%0Arich%20graph%20representations.%20This%20survey%20paper%20provides%20a%20comprehensive%20review%0Aof%20the%20rapidly%20evolving%20field%20of%20Hyperbolic%20Graph%20Learning%20%28HGL%29.%20We%0Asystematically%20categorize%20and%20analyze%20existing%20methods%20broadly%20dividing%20them%0Ainto%20%281%29%20hyperbolic%20graph%20embedding-based%20techniques%2C%20%282%29%20graph%20neural%0Anetwork-based%20hyperbolic%20models%2C%20and%20%283%29%20emerging%20paradigms.%20Beyond%0Amethodologies%2C%20we%20extensively%20discuss%20diverse%20applications%20of%20HGL%20across%0Amultiple%20domains%2C%20including%20recommender%20systems%2C%20knowledge%20graphs%2C%0Abioinformatics%2C%20and%20other%20relevant%20scenarios%2C%20demonstrating%20the%20broad%0Aapplicability%20and%20effectiveness%20of%20hyperbolic%20geometry%20in%20real-world%20graph%0Alearning%20tasks.%20Most%20importantly%2C%20we%20identify%20several%20key%20challenges%20that%20serve%0Aas%20directions%20for%20advancing%20HGL%2C%20including%20handling%20complex%20data%20structures%2C%0Adeveloping%20geometry-aware%20learning%20objectives%2C%20ensuring%20trustworthy%20and%0Ascalable%20implementations%2C%20and%20integrating%20with%20foundation%20models%2C%20e.g.%2C%20large%0Alanguage%20models.%20We%20highlight%20promising%20research%20opportunities%20in%20this%20exciting%0Ainterdisciplinary%20area.%20A%20comprehensive%20repository%20can%20be%20found%20at%0Ahttps%3A//github.com/digailab/awesome-hyperbolic-graph-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.13852v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperbolic%2520Graph%2520Learning%253A%2520A%2520Comprehensive%2520Review%26entry.906535625%3DMenglin%2520Yang%2520and%2520Min%2520Zhou%2520and%2520Tong%2520Zhang%2520and%2520Jiahong%2520Liu%2520and%2520Zhihao%2520Li%2520and%2520Lujia%2520Pan%2520and%2520Hui%2520Xiong%2520and%2520Irwin%2520King%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520in%2520Euclidean%2520space%252C%2520despite%2520its%2520widespread%250Aadoption%2520and%2520proven%2520utility%2520in%2520many%2520domains%252C%2520often%2520struggles%2520to%2520effectively%250Acapture%2520the%2520inherent%2520hierarchical%2520and%2520complex%2520relational%2520structures%2520prevalent%250Ain%2520real-world%2520data%252C%2520particularly%2520for%2520datasets%2520exhibiting%2520a%2520highly%2520non-Euclidean%250Alatent%2520anatomy%2520or%2520power-law%2520distributions.%2520Hyperbolic%2520geometry%252C%2520with%2520its%250Aconstant%2520negative%2520curvature%2520and%2520exponential%2520growth%2520property%252C%2520naturally%250Aaccommodates%2520such%2520structures%252C%2520offering%2520a%2520promising%2520alternative%2520for%2520learning%250Arich%2520graph%2520representations.%2520This%2520survey%2520paper%2520provides%2520a%2520comprehensive%2520review%250Aof%2520the%2520rapidly%2520evolving%2520field%2520of%2520Hyperbolic%2520Graph%2520Learning%2520%2528HGL%2529.%2520We%250Asystematically%2520categorize%2520and%2520analyze%2520existing%2520methods%2520broadly%2520dividing%2520them%250Ainto%2520%25281%2529%2520hyperbolic%2520graph%2520embedding-based%2520techniques%252C%2520%25282%2529%2520graph%2520neural%250Anetwork-based%2520hyperbolic%2520models%252C%2520and%2520%25283%2529%2520emerging%2520paradigms.%2520Beyond%250Amethodologies%252C%2520we%2520extensively%2520discuss%2520diverse%2520applications%2520of%2520HGL%2520across%250Amultiple%2520domains%252C%2520including%2520recommender%2520systems%252C%2520knowledge%2520graphs%252C%250Abioinformatics%252C%2520and%2520other%2520relevant%2520scenarios%252C%2520demonstrating%2520the%2520broad%250Aapplicability%2520and%2520effectiveness%2520of%2520hyperbolic%2520geometry%2520in%2520real-world%2520graph%250Alearning%2520tasks.%2520Most%2520importantly%252C%2520we%2520identify%2520several%2520key%2520challenges%2520that%2520serve%250Aas%2520directions%2520for%2520advancing%2520HGL%252C%2520including%2520handling%2520complex%2520data%2520structures%252C%250Adeveloping%2520geometry-aware%2520learning%2520objectives%252C%2520ensuring%2520trustworthy%2520and%250Ascalable%2520implementations%252C%2520and%2520integrating%2520with%2520foundation%2520models%252C%2520e.g.%252C%2520large%250Alanguage%2520models.%2520We%2520highlight%2520promising%2520research%2520opportunities%2520in%2520this%2520exciting%250Ainterdisciplinary%2520area.%2520A%2520comprehensive%2520repository%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/digailab/awesome-hyperbolic-graph-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.13852v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Graph%20Learning%3A%20A%20Comprehensive%20Review&entry.906535625=Menglin%20Yang%20and%20Min%20Zhou%20and%20Tong%20Zhang%20and%20Jiahong%20Liu%20and%20Zhihao%20Li%20and%20Lujia%20Pan%20and%20Hui%20Xiong%20and%20Irwin%20King&entry.1292438233=%20%20Graph%20representation%20learning%20in%20Euclidean%20space%2C%20despite%20its%20widespread%0Aadoption%20and%20proven%20utility%20in%20many%20domains%2C%20often%20struggles%20to%20effectively%0Acapture%20the%20inherent%20hierarchical%20and%20complex%20relational%20structures%20prevalent%0Ain%20real-world%20data%2C%20particularly%20for%20datasets%20exhibiting%20a%20highly%20non-Euclidean%0Alatent%20anatomy%20or%20power-law%20distributions.%20Hyperbolic%20geometry%2C%20with%20its%0Aconstant%20negative%20curvature%20and%20exponential%20growth%20property%2C%20naturally%0Aaccommodates%20such%20structures%2C%20offering%20a%20promising%20alternative%20for%20learning%0Arich%20graph%20representations.%20This%20survey%20paper%20provides%20a%20comprehensive%20review%0Aof%20the%20rapidly%20evolving%20field%20of%20Hyperbolic%20Graph%20Learning%20%28HGL%29.%20We%0Asystematically%20categorize%20and%20analyze%20existing%20methods%20broadly%20dividing%20them%0Ainto%20%281%29%20hyperbolic%20graph%20embedding-based%20techniques%2C%20%282%29%20graph%20neural%0Anetwork-based%20hyperbolic%20models%2C%20and%20%283%29%20emerging%20paradigms.%20Beyond%0Amethodologies%2C%20we%20extensively%20discuss%20diverse%20applications%20of%20HGL%20across%0Amultiple%20domains%2C%20including%20recommender%20systems%2C%20knowledge%20graphs%2C%0Abioinformatics%2C%20and%20other%20relevant%20scenarios%2C%20demonstrating%20the%20broad%0Aapplicability%20and%20effectiveness%20of%20hyperbolic%20geometry%20in%20real-world%20graph%0Alearning%20tasks.%20Most%20importantly%2C%20we%20identify%20several%20key%20challenges%20that%20serve%0Aas%20directions%20for%20advancing%20HGL%2C%20including%20handling%20complex%20data%20structures%2C%0Adeveloping%20geometry-aware%20learning%20objectives%2C%20ensuring%20trustworthy%20and%0Ascalable%20implementations%2C%20and%20integrating%20with%20foundation%20models%2C%20e.g.%2C%20large%0Alanguage%20models.%20We%20highlight%20promising%20research%20opportunities%20in%20this%20exciting%0Ainterdisciplinary%20area.%20A%20comprehensive%20repository%20can%20be%20found%20at%0Ahttps%3A//github.com/digailab/awesome-hyperbolic-graph-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.13852v3&entry.124074799=Read"},
{"title": "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt\n  Learning for Vision-Language Models", "author": "Xinting Liao and Weiming Liu and Jiaming Qian and Pengyang Zhou and Jiahe Xu and Wenjie Wang and Chaochao Chen and Xiaolin Zheng and Tat-Seng Chua", "abstract": "  Federated prompt learning (FPL) for vision-language models is a powerful\napproach to collaboratively adapt models across distributed clients while\npreserving data privacy. However, existing FPL approaches suffer from a\ntrade-off between performance and robustness, particularly in\nout-of-distribution (OOD) shifts, limiting their reliability in real-world\nscenarios. The inherent in-distribution (ID) data heterogeneity among different\nclients makes it more challenging to maintain this trade-off. To fill this gap,\nwe introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,\nwhich captures diverse distributions among clients using ID global prompts,\nlocal prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of\nprompts to create both class-level and distribution-level separations, which\nadapt to OOD shifts through bi-level distributionally robust optimization.\nAdditionally, FOCoOp improves the discrimination consistency among clients,\ni.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by\nsemi-unbalanced optimal transport. The extensive experiments on real-world\ndatasets demonstrate that FOCoOp effectively captures decentralized\nheterogeneous distributions and enhances robustness of different OOD shifts.\nThe project is available at GitHub.\n", "link": "http://arxiv.org/abs/2506.16218v3", "date": "2025-07-30", "relevancy": 2.5266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCoOp%3A%20Enhancing%20Out-of-Distribution%20Robustness%20in%20Federated%20Prompt%0A%20%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20FOCoOp%3A%20Enhancing%20Out-of-Distribution%20Robustness%20in%20Federated%20Prompt%0A%20%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Xinting%20Liao%20and%20Weiming%20Liu%20and%20Jiaming%20Qian%20and%20Pengyang%20Zhou%20and%20Jiahe%20Xu%20and%20Wenjie%20Wang%20and%20Chaochao%20Chen%20and%20Xiaolin%20Zheng%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Federated%20prompt%20learning%20%28FPL%29%20for%20vision-language%20models%20is%20a%20powerful%0Aapproach%20to%20collaboratively%20adapt%20models%20across%20distributed%20clients%20while%0Apreserving%20data%20privacy.%20However%2C%20existing%20FPL%20approaches%20suffer%20from%20a%0Atrade-off%20between%20performance%20and%20robustness%2C%20particularly%20in%0Aout-of-distribution%20%28OOD%29%20shifts%2C%20limiting%20their%20reliability%20in%20real-world%0Ascenarios.%20The%20inherent%20in-distribution%20%28ID%29%20data%20heterogeneity%20among%20different%0Aclients%20makes%20it%20more%20challenging%20to%20maintain%20this%20trade-off.%20To%20fill%20this%20gap%2C%0Awe%20introduce%20a%20Federated%20OOD-aware%20Context%20Optimization%20%28FOCoOp%29%20framework%2C%0Awhich%20captures%20diverse%20distributions%20among%20clients%20using%20ID%20global%20prompts%2C%0Alocal%20prompts%2C%20and%20OOD%20prompts.%20Specifically%2C%20FOCoOp%20leverages%20three%20sets%20of%0Aprompts%20to%20create%20both%20class-level%20and%20distribution-level%20separations%2C%20which%0Aadapt%20to%20OOD%20shifts%20through%20bi-level%20distributionally%20robust%20optimization.%0AAdditionally%2C%20FOCoOp%20improves%20the%20discrimination%20consistency%20among%20clients%2C%0Ai.e.%2C%20calibrating%20global%20prompts%2C%20seemingly%20OOD%20prompts%2C%20and%20OOD%20prompts%20by%0Asemi-unbalanced%20optimal%20transport.%20The%20extensive%20experiments%20on%20real-world%0Adatasets%20demonstrate%20that%20FOCoOp%20effectively%20captures%20decentralized%0Aheterogeneous%20distributions%20and%20enhances%20robustness%20of%20different%20OOD%20shifts.%0AThe%20project%20is%20available%20at%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16218v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCoOp%253A%2520Enhancing%2520Out-of-Distribution%2520Robustness%2520in%2520Federated%2520Prompt%250A%2520%2520Learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DXinting%2520Liao%2520and%2520Weiming%2520Liu%2520and%2520Jiaming%2520Qian%2520and%2520Pengyang%2520Zhou%2520and%2520Jiahe%2520Xu%2520and%2520Wenjie%2520Wang%2520and%2520Chaochao%2520Chen%2520and%2520Xiaolin%2520Zheng%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Federated%2520prompt%2520learning%2520%2528FPL%2529%2520for%2520vision-language%2520models%2520is%2520a%2520powerful%250Aapproach%2520to%2520collaboratively%2520adapt%2520models%2520across%2520distributed%2520clients%2520while%250Apreserving%2520data%2520privacy.%2520However%252C%2520existing%2520FPL%2520approaches%2520suffer%2520from%2520a%250Atrade-off%2520between%2520performance%2520and%2520robustness%252C%2520particularly%2520in%250Aout-of-distribution%2520%2528OOD%2529%2520shifts%252C%2520limiting%2520their%2520reliability%2520in%2520real-world%250Ascenarios.%2520The%2520inherent%2520in-distribution%2520%2528ID%2529%2520data%2520heterogeneity%2520among%2520different%250Aclients%2520makes%2520it%2520more%2520challenging%2520to%2520maintain%2520this%2520trade-off.%2520To%2520fill%2520this%2520gap%252C%250Awe%2520introduce%2520a%2520Federated%2520OOD-aware%2520Context%2520Optimization%2520%2528FOCoOp%2529%2520framework%252C%250Awhich%2520captures%2520diverse%2520distributions%2520among%2520clients%2520using%2520ID%2520global%2520prompts%252C%250Alocal%2520prompts%252C%2520and%2520OOD%2520prompts.%2520Specifically%252C%2520FOCoOp%2520leverages%2520three%2520sets%2520of%250Aprompts%2520to%2520create%2520both%2520class-level%2520and%2520distribution-level%2520separations%252C%2520which%250Aadapt%2520to%2520OOD%2520shifts%2520through%2520bi-level%2520distributionally%2520robust%2520optimization.%250AAdditionally%252C%2520FOCoOp%2520improves%2520the%2520discrimination%2520consistency%2520among%2520clients%252C%250Ai.e.%252C%2520calibrating%2520global%2520prompts%252C%2520seemingly%2520OOD%2520prompts%252C%2520and%2520OOD%2520prompts%2520by%250Asemi-unbalanced%2520optimal%2520transport.%2520The%2520extensive%2520experiments%2520on%2520real-world%250Adatasets%2520demonstrate%2520that%2520FOCoOp%2520effectively%2520captures%2520decentralized%250Aheterogeneous%2520distributions%2520and%2520enhances%2520robustness%2520of%2520different%2520OOD%2520shifts.%250AThe%2520project%2520is%2520available%2520at%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16218v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCoOp%3A%20Enhancing%20Out-of-Distribution%20Robustness%20in%20Federated%20Prompt%0A%20%20Learning%20for%20Vision-Language%20Models&entry.906535625=Xinting%20Liao%20and%20Weiming%20Liu%20and%20Jiaming%20Qian%20and%20Pengyang%20Zhou%20and%20Jiahe%20Xu%20and%20Wenjie%20Wang%20and%20Chaochao%20Chen%20and%20Xiaolin%20Zheng%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Federated%20prompt%20learning%20%28FPL%29%20for%20vision-language%20models%20is%20a%20powerful%0Aapproach%20to%20collaboratively%20adapt%20models%20across%20distributed%20clients%20while%0Apreserving%20data%20privacy.%20However%2C%20existing%20FPL%20approaches%20suffer%20from%20a%0Atrade-off%20between%20performance%20and%20robustness%2C%20particularly%20in%0Aout-of-distribution%20%28OOD%29%20shifts%2C%20limiting%20their%20reliability%20in%20real-world%0Ascenarios.%20The%20inherent%20in-distribution%20%28ID%29%20data%20heterogeneity%20among%20different%0Aclients%20makes%20it%20more%20challenging%20to%20maintain%20this%20trade-off.%20To%20fill%20this%20gap%2C%0Awe%20introduce%20a%20Federated%20OOD-aware%20Context%20Optimization%20%28FOCoOp%29%20framework%2C%0Awhich%20captures%20diverse%20distributions%20among%20clients%20using%20ID%20global%20prompts%2C%0Alocal%20prompts%2C%20and%20OOD%20prompts.%20Specifically%2C%20FOCoOp%20leverages%20three%20sets%20of%0Aprompts%20to%20create%20both%20class-level%20and%20distribution-level%20separations%2C%20which%0Aadapt%20to%20OOD%20shifts%20through%20bi-level%20distributionally%20robust%20optimization.%0AAdditionally%2C%20FOCoOp%20improves%20the%20discrimination%20consistency%20among%20clients%2C%0Ai.e.%2C%20calibrating%20global%20prompts%2C%20seemingly%20OOD%20prompts%2C%20and%20OOD%20prompts%20by%0Asemi-unbalanced%20optimal%20transport.%20The%20extensive%20experiments%20on%20real-world%0Adatasets%20demonstrate%20that%20FOCoOp%20effectively%20captures%20decentralized%0Aheterogeneous%20distributions%20and%20enhances%20robustness%20of%20different%20OOD%20shifts.%0AThe%20project%20is%20available%20at%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16218v3&entry.124074799=Read"},
{"title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG", "author": "Roie Kazoom and Raz Lapid and Moshe Sipper and Ofer Hadar", "abstract": "  Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks.\n", "link": "http://arxiv.org/abs/2504.04858v3", "date": "2025-07-30", "relevancy": 2.5093, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5052}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5014}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Lag%2C%20RAG%3A%20Training-Free%20Adversarial%20Detection%20Using%20RAG&body=Title%3A%20Don%27t%20Lag%2C%20RAG%3A%20Training-Free%20Adversarial%20Detection%20Using%20RAG%0AAuthor%3A%20Roie%20Kazoom%20and%20Raz%20Lapid%20and%20Moshe%20Sipper%20and%20Ofer%20Hadar%0AAbstract%3A%20%20%20Adversarial%20patch%20attacks%20pose%20a%20major%20threat%20to%20vision%20systems%20by%20embedding%0Alocalized%20perturbations%20that%20mislead%20deep%20models.%20Traditional%20defense%20methods%0Aoften%20require%20retraining%20or%20fine-tuning%2C%20making%20them%20impractical%20for%20real-world%0Adeployment.%20We%20propose%20a%20training-free%20Visual%20Retrieval-Augmented%20Generation%0A%28VRAG%29%20framework%20that%20integrates%20Vision-Language%20Models%20%28VLMs%29%20for%20adversarial%0Apatch%20detection.%20By%20retrieving%20visually%20similar%20patches%20and%20images%20that%0Aresemble%20stored%20attacks%20in%20a%20continuously%20expanding%20database%2C%20VRAG%20performs%0Agenerative%20reasoning%20to%20identify%20diverse%20attack%20types%2C%20all%20without%20additional%0Atraining%20or%20fine-tuning.%20We%20extensively%20evaluate%20open-source%20large-scale%20VLMs%2C%0Aincluding%20Qwen-VL-Plus%2C%20Qwen2.5-VL-72B%2C%20and%20UI-TARS-72B-DPO%2C%20alongside%0AGemini-2.0%2C%20a%20closed-source%20model.%20Notably%2C%20the%20open-source%20UI-TARS-72B-DPO%0Amodel%20achieves%20up%20to%2095%20percent%20classification%20accuracy%2C%20setting%20a%20new%0Astate-of-the-art%20for%20open-source%20adversarial%20patch%20detection.%20Gemini-2.0%0Aattains%20the%20highest%20overall%20accuracy%2C%2098%20percent%2C%20but%20remains%20closed-source.%0AExperimental%20results%20demonstrate%20VRAG%27s%20effectiveness%20in%20identifying%20a%20variety%0Aof%20adversarial%20patches%20with%20minimal%20human%20annotation%2C%20paving%20the%20way%20for%0Arobust%2C%20practical%20defenses%20against%20evolving%20adversarial%20patch%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04858v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Lag%252C%2520RAG%253A%2520Training-Free%2520Adversarial%2520Detection%2520Using%2520RAG%26entry.906535625%3DRoie%2520Kazoom%2520and%2520Raz%2520Lapid%2520and%2520Moshe%2520Sipper%2520and%2520Ofer%2520Hadar%26entry.1292438233%3D%2520%2520Adversarial%2520patch%2520attacks%2520pose%2520a%2520major%2520threat%2520to%2520vision%2520systems%2520by%2520embedding%250Alocalized%2520perturbations%2520that%2520mislead%2520deep%2520models.%2520Traditional%2520defense%2520methods%250Aoften%2520require%2520retraining%2520or%2520fine-tuning%252C%2520making%2520them%2520impractical%2520for%2520real-world%250Adeployment.%2520We%2520propose%2520a%2520training-free%2520Visual%2520Retrieval-Augmented%2520Generation%250A%2528VRAG%2529%2520framework%2520that%2520integrates%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520adversarial%250Apatch%2520detection.%2520By%2520retrieving%2520visually%2520similar%2520patches%2520and%2520images%2520that%250Aresemble%2520stored%2520attacks%2520in%2520a%2520continuously%2520expanding%2520database%252C%2520VRAG%2520performs%250Agenerative%2520reasoning%2520to%2520identify%2520diverse%2520attack%2520types%252C%2520all%2520without%2520additional%250Atraining%2520or%2520fine-tuning.%2520We%2520extensively%2520evaluate%2520open-source%2520large-scale%2520VLMs%252C%250Aincluding%2520Qwen-VL-Plus%252C%2520Qwen2.5-VL-72B%252C%2520and%2520UI-TARS-72B-DPO%252C%2520alongside%250AGemini-2.0%252C%2520a%2520closed-source%2520model.%2520Notably%252C%2520the%2520open-source%2520UI-TARS-72B-DPO%250Amodel%2520achieves%2520up%2520to%252095%2520percent%2520classification%2520accuracy%252C%2520setting%2520a%2520new%250Astate-of-the-art%2520for%2520open-source%2520adversarial%2520patch%2520detection.%2520Gemini-2.0%250Aattains%2520the%2520highest%2520overall%2520accuracy%252C%252098%2520percent%252C%2520but%2520remains%2520closed-source.%250AExperimental%2520results%2520demonstrate%2520VRAG%2527s%2520effectiveness%2520in%2520identifying%2520a%2520variety%250Aof%2520adversarial%2520patches%2520with%2520minimal%2520human%2520annotation%252C%2520paving%2520the%2520way%2520for%250Arobust%252C%2520practical%2520defenses%2520against%2520evolving%2520adversarial%2520patch%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04858v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Lag%2C%20RAG%3A%20Training-Free%20Adversarial%20Detection%20Using%20RAG&entry.906535625=Roie%20Kazoom%20and%20Raz%20Lapid%20and%20Moshe%20Sipper%20and%20Ofer%20Hadar&entry.1292438233=%20%20Adversarial%20patch%20attacks%20pose%20a%20major%20threat%20to%20vision%20systems%20by%20embedding%0Alocalized%20perturbations%20that%20mislead%20deep%20models.%20Traditional%20defense%20methods%0Aoften%20require%20retraining%20or%20fine-tuning%2C%20making%20them%20impractical%20for%20real-world%0Adeployment.%20We%20propose%20a%20training-free%20Visual%20Retrieval-Augmented%20Generation%0A%28VRAG%29%20framework%20that%20integrates%20Vision-Language%20Models%20%28VLMs%29%20for%20adversarial%0Apatch%20detection.%20By%20retrieving%20visually%20similar%20patches%20and%20images%20that%0Aresemble%20stored%20attacks%20in%20a%20continuously%20expanding%20database%2C%20VRAG%20performs%0Agenerative%20reasoning%20to%20identify%20diverse%20attack%20types%2C%20all%20without%20additional%0Atraining%20or%20fine-tuning.%20We%20extensively%20evaluate%20open-source%20large-scale%20VLMs%2C%0Aincluding%20Qwen-VL-Plus%2C%20Qwen2.5-VL-72B%2C%20and%20UI-TARS-72B-DPO%2C%20alongside%0AGemini-2.0%2C%20a%20closed-source%20model.%20Notably%2C%20the%20open-source%20UI-TARS-72B-DPO%0Amodel%20achieves%20up%20to%2095%20percent%20classification%20accuracy%2C%20setting%20a%20new%0Astate-of-the-art%20for%20open-source%20adversarial%20patch%20detection.%20Gemini-2.0%0Aattains%20the%20highest%20overall%20accuracy%2C%2098%20percent%2C%20but%20remains%20closed-source.%0AExperimental%20results%20demonstrate%20VRAG%27s%20effectiveness%20in%20identifying%20a%20variety%0Aof%20adversarial%20patches%20with%20minimal%20human%20annotation%2C%20paving%20the%20way%20for%0Arobust%2C%20practical%20defenses%20against%20evolving%20adversarial%20patch%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04858v3&entry.124074799=Read"},
{"title": "DepR: Depth Guided Single-view Scene Reconstruction with Instance-level\n  Diffusion", "author": "Qingcheng Zhao and Xiang Zhang and Haiyang Xu and Zeyuan Chen and Jianwen Xie and Yuan Gao and Zhuowen Tu", "abstract": "  We propose DepR, a depth-guided single-view scene reconstruction framework\nthat integrates instance-level diffusion within a compositional paradigm.\nInstead of reconstructing the entire scene holistically, DepR generates\nindividual objects and subsequently composes them into a coherent 3D layout.\nUnlike previous methods that use depth solely for object layout estimation\nduring inference and therefore fail to fully exploit its rich geometric\ninformation, DepR leverages depth throughout both training and inference.\nSpecifically, we introduce depth-guided conditioning to effectively encode\nshape priors into diffusion models. During inference, depth further guides DDIM\nsampling and layout optimization, enhancing alignment between the\nreconstruction and the input image. Despite being trained on limited synthetic\ndata, DepR achieves state-of-the-art performance and demonstrates strong\ngeneralization in single-view scene reconstruction, as shown through\nevaluations on both synthetic and real-world datasets.\n", "link": "http://arxiv.org/abs/2507.22825v1", "date": "2025-07-30", "relevancy": 2.5086, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6294}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6294}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepR%3A%20Depth%20Guided%20Single-view%20Scene%20Reconstruction%20with%20Instance-level%0A%20%20Diffusion&body=Title%3A%20DepR%3A%20Depth%20Guided%20Single-view%20Scene%20Reconstruction%20with%20Instance-level%0A%20%20Diffusion%0AAuthor%3A%20Qingcheng%20Zhao%20and%20Xiang%20Zhang%20and%20Haiyang%20Xu%20and%20Zeyuan%20Chen%20and%20Jianwen%20Xie%20and%20Yuan%20Gao%20and%20Zhuowen%20Tu%0AAbstract%3A%20%20%20We%20propose%20DepR%2C%20a%20depth-guided%20single-view%20scene%20reconstruction%20framework%0Athat%20integrates%20instance-level%20diffusion%20within%20a%20compositional%20paradigm.%0AInstead%20of%20reconstructing%20the%20entire%20scene%20holistically%2C%20DepR%20generates%0Aindividual%20objects%20and%20subsequently%20composes%20them%20into%20a%20coherent%203D%20layout.%0AUnlike%20previous%20methods%20that%20use%20depth%20solely%20for%20object%20layout%20estimation%0Aduring%20inference%20and%20therefore%20fail%20to%20fully%20exploit%20its%20rich%20geometric%0Ainformation%2C%20DepR%20leverages%20depth%20throughout%20both%20training%20and%20inference.%0ASpecifically%2C%20we%20introduce%20depth-guided%20conditioning%20to%20effectively%20encode%0Ashape%20priors%20into%20diffusion%20models.%20During%20inference%2C%20depth%20further%20guides%20DDIM%0Asampling%20and%20layout%20optimization%2C%20enhancing%20alignment%20between%20the%0Areconstruction%20and%20the%20input%20image.%20Despite%20being%20trained%20on%20limited%20synthetic%0Adata%2C%20DepR%20achieves%20state-of-the-art%20performance%20and%20demonstrates%20strong%0Ageneralization%20in%20single-view%20scene%20reconstruction%2C%20as%20shown%20through%0Aevaluations%20on%20both%20synthetic%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepR%253A%2520Depth%2520Guided%2520Single-view%2520Scene%2520Reconstruction%2520with%2520Instance-level%250A%2520%2520Diffusion%26entry.906535625%3DQingcheng%2520Zhao%2520and%2520Xiang%2520Zhang%2520and%2520Haiyang%2520Xu%2520and%2520Zeyuan%2520Chen%2520and%2520Jianwen%2520Xie%2520and%2520Yuan%2520Gao%2520and%2520Zhuowen%2520Tu%26entry.1292438233%3D%2520%2520We%2520propose%2520DepR%252C%2520a%2520depth-guided%2520single-view%2520scene%2520reconstruction%2520framework%250Athat%2520integrates%2520instance-level%2520diffusion%2520within%2520a%2520compositional%2520paradigm.%250AInstead%2520of%2520reconstructing%2520the%2520entire%2520scene%2520holistically%252C%2520DepR%2520generates%250Aindividual%2520objects%2520and%2520subsequently%2520composes%2520them%2520into%2520a%2520coherent%25203D%2520layout.%250AUnlike%2520previous%2520methods%2520that%2520use%2520depth%2520solely%2520for%2520object%2520layout%2520estimation%250Aduring%2520inference%2520and%2520therefore%2520fail%2520to%2520fully%2520exploit%2520its%2520rich%2520geometric%250Ainformation%252C%2520DepR%2520leverages%2520depth%2520throughout%2520both%2520training%2520and%2520inference.%250ASpecifically%252C%2520we%2520introduce%2520depth-guided%2520conditioning%2520to%2520effectively%2520encode%250Ashape%2520priors%2520into%2520diffusion%2520models.%2520During%2520inference%252C%2520depth%2520further%2520guides%2520DDIM%250Asampling%2520and%2520layout%2520optimization%252C%2520enhancing%2520alignment%2520between%2520the%250Areconstruction%2520and%2520the%2520input%2520image.%2520Despite%2520being%2520trained%2520on%2520limited%2520synthetic%250Adata%252C%2520DepR%2520achieves%2520state-of-the-art%2520performance%2520and%2520demonstrates%2520strong%250Ageneralization%2520in%2520single-view%2520scene%2520reconstruction%252C%2520as%2520shown%2520through%250Aevaluations%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepR%3A%20Depth%20Guided%20Single-view%20Scene%20Reconstruction%20with%20Instance-level%0A%20%20Diffusion&entry.906535625=Qingcheng%20Zhao%20and%20Xiang%20Zhang%20and%20Haiyang%20Xu%20and%20Zeyuan%20Chen%20and%20Jianwen%20Xie%20and%20Yuan%20Gao%20and%20Zhuowen%20Tu&entry.1292438233=%20%20We%20propose%20DepR%2C%20a%20depth-guided%20single-view%20scene%20reconstruction%20framework%0Athat%20integrates%20instance-level%20diffusion%20within%20a%20compositional%20paradigm.%0AInstead%20of%20reconstructing%20the%20entire%20scene%20holistically%2C%20DepR%20generates%0Aindividual%20objects%20and%20subsequently%20composes%20them%20into%20a%20coherent%203D%20layout.%0AUnlike%20previous%20methods%20that%20use%20depth%20solely%20for%20object%20layout%20estimation%0Aduring%20inference%20and%20therefore%20fail%20to%20fully%20exploit%20its%20rich%20geometric%0Ainformation%2C%20DepR%20leverages%20depth%20throughout%20both%20training%20and%20inference.%0ASpecifically%2C%20we%20introduce%20depth-guided%20conditioning%20to%20effectively%20encode%0Ashape%20priors%20into%20diffusion%20models.%20During%20inference%2C%20depth%20further%20guides%20DDIM%0Asampling%20and%20layout%20optimization%2C%20enhancing%20alignment%20between%20the%0Areconstruction%20and%20the%20input%20image.%20Despite%20being%20trained%20on%20limited%20synthetic%0Adata%2C%20DepR%20achieves%20state-of-the-art%20performance%20and%20demonstrates%20strong%0Ageneralization%20in%20single-view%20scene%20reconstruction%2C%20as%20shown%20through%0Aevaluations%20on%20both%20synthetic%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22825v1&entry.124074799=Read"},
{"title": "Application of Vision-Language Model to Pedestrians Behavior and Scene\n  Understanding in Autonomous Driving", "author": "Haoxiang Gao and Li Zhang and Yu Zhao and Zhou Yang and Jinghan Cao", "abstract": "  Vision-language models (VLMs) have become a promising approach to enhancing\nperception and decision-making in autonomous driving. The gap remains in\napplying VLMs to understand complex scenarios interacting with pedestrians and\nefficient vehicle deployment. In this paper, we propose a knowledge\ndistillation method that transfers knowledge from large-scale vision-language\nfoundation models to efficient vision networks, and we apply it to pedestrian\nbehavior prediction and scene understanding tasks, achieving promising results\nin generating more diverse and comprehensive semantic attributes. We also\nutilize multiple pre-trained models and ensemble techniques to boost the\nmodel's performance. We further examined the effectiveness of the model after\nknowledge distillation; the results show significant metric improvements in\nopen-vocabulary perception and trajectory prediction tasks, which can\npotentially enhance the end-to-end performance of autonomous driving.\n", "link": "http://arxiv.org/abs/2501.06680v2", "date": "2025-07-30", "relevancy": 2.4999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6309}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Vision-Language%20Model%20to%20Pedestrians%20Behavior%20and%20Scene%0A%20%20Understanding%20in%20Autonomous%20Driving&body=Title%3A%20Application%20of%20Vision-Language%20Model%20to%20Pedestrians%20Behavior%20and%20Scene%0A%20%20Understanding%20in%20Autonomous%20Driving%0AAuthor%3A%20Haoxiang%20Gao%20and%20Li%20Zhang%20and%20Yu%20Zhao%20and%20Zhou%20Yang%20and%20Jinghan%20Cao%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20become%20a%20promising%20approach%20to%20enhancing%0Aperception%20and%20decision-making%20in%20autonomous%20driving.%20The%20gap%20remains%20in%0Aapplying%20VLMs%20to%20understand%20complex%20scenarios%20interacting%20with%20pedestrians%20and%0Aefficient%20vehicle%20deployment.%20In%20this%20paper%2C%20we%20propose%20a%20knowledge%0Adistillation%20method%20that%20transfers%20knowledge%20from%20large-scale%20vision-language%0Afoundation%20models%20to%20efficient%20vision%20networks%2C%20and%20we%20apply%20it%20to%20pedestrian%0Abehavior%20prediction%20and%20scene%20understanding%20tasks%2C%20achieving%20promising%20results%0Ain%20generating%20more%20diverse%20and%20comprehensive%20semantic%20attributes.%20We%20also%0Autilize%20multiple%20pre-trained%20models%20and%20ensemble%20techniques%20to%20boost%20the%0Amodel%27s%20performance.%20We%20further%20examined%20the%20effectiveness%20of%20the%20model%20after%0Aknowledge%20distillation%3B%20the%20results%20show%20significant%20metric%20improvements%20in%0Aopen-vocabulary%20perception%20and%20trajectory%20prediction%20tasks%2C%20which%20can%0Apotentially%20enhance%20the%20end-to-end%20performance%20of%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Vision-Language%2520Model%2520to%2520Pedestrians%2520Behavior%2520and%2520Scene%250A%2520%2520Understanding%2520in%2520Autonomous%2520Driving%26entry.906535625%3DHaoxiang%2520Gao%2520and%2520Li%2520Zhang%2520and%2520Yu%2520Zhao%2520and%2520Zhou%2520Yang%2520and%2520Jinghan%2520Cao%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520become%2520a%2520promising%2520approach%2520to%2520enhancing%250Aperception%2520and%2520decision-making%2520in%2520autonomous%2520driving.%2520The%2520gap%2520remains%2520in%250Aapplying%2520VLMs%2520to%2520understand%2520complex%2520scenarios%2520interacting%2520with%2520pedestrians%2520and%250Aefficient%2520vehicle%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520knowledge%250Adistillation%2520method%2520that%2520transfers%2520knowledge%2520from%2520large-scale%2520vision-language%250Afoundation%2520models%2520to%2520efficient%2520vision%2520networks%252C%2520and%2520we%2520apply%2520it%2520to%2520pedestrian%250Abehavior%2520prediction%2520and%2520scene%2520understanding%2520tasks%252C%2520achieving%2520promising%2520results%250Ain%2520generating%2520more%2520diverse%2520and%2520comprehensive%2520semantic%2520attributes.%2520We%2520also%250Autilize%2520multiple%2520pre-trained%2520models%2520and%2520ensemble%2520techniques%2520to%2520boost%2520the%250Amodel%2527s%2520performance.%2520We%2520further%2520examined%2520the%2520effectiveness%2520of%2520the%2520model%2520after%250Aknowledge%2520distillation%253B%2520the%2520results%2520show%2520significant%2520metric%2520improvements%2520in%250Aopen-vocabulary%2520perception%2520and%2520trajectory%2520prediction%2520tasks%252C%2520which%2520can%250Apotentially%2520enhance%2520the%2520end-to-end%2520performance%2520of%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Vision-Language%20Model%20to%20Pedestrians%20Behavior%20and%20Scene%0A%20%20Understanding%20in%20Autonomous%20Driving&entry.906535625=Haoxiang%20Gao%20and%20Li%20Zhang%20and%20Yu%20Zhao%20and%20Zhou%20Yang%20and%20Jinghan%20Cao&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20become%20a%20promising%20approach%20to%20enhancing%0Aperception%20and%20decision-making%20in%20autonomous%20driving.%20The%20gap%20remains%20in%0Aapplying%20VLMs%20to%20understand%20complex%20scenarios%20interacting%20with%20pedestrians%20and%0Aefficient%20vehicle%20deployment.%20In%20this%20paper%2C%20we%20propose%20a%20knowledge%0Adistillation%20method%20that%20transfers%20knowledge%20from%20large-scale%20vision-language%0Afoundation%20models%20to%20efficient%20vision%20networks%2C%20and%20we%20apply%20it%20to%20pedestrian%0Abehavior%20prediction%20and%20scene%20understanding%20tasks%2C%20achieving%20promising%20results%0Ain%20generating%20more%20diverse%20and%20comprehensive%20semantic%20attributes.%20We%20also%0Autilize%20multiple%20pre-trained%20models%20and%20ensemble%20techniques%20to%20boost%20the%0Amodel%27s%20performance.%20We%20further%20examined%20the%20effectiveness%20of%20the%20model%20after%0Aknowledge%20distillation%3B%20the%20results%20show%20significant%20metric%20improvements%20in%0Aopen-vocabulary%20perception%20and%20trajectory%20prediction%20tasks%2C%20which%20can%0Apotentially%20enhance%20the%20end-to-end%20performance%20of%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06680v2&entry.124074799=Read"},
{"title": "Federated Learning on Riemannian Manifolds: A Gradient-Free\n  Projection-Based Approach", "author": "Hongye Wang and Zhaoye Pan and Chang He and Jiaxiang Li and Bo Jiang", "abstract": "  Federated learning (FL) has emerged as a powerful paradigm for collaborative\nmodel training across distributed clients while preserving data privacy.\nHowever, existing FL algorithms predominantly focus on unconstrained\noptimization problems with exact gradient information, limiting its\napplicability in scenarios where only noisy function evaluations are accessible\nor where model parameters are constrained. To address these challenges, we\npropose a novel zeroth-order projection-based algorithm on Riemannian manifolds\nfor FL. By leveraging the projection operator, we introduce a computationally\nefficient zeroth-order Riemannian gradient estimator. Unlike existing\nestimators, ours requires only a simple Euclidean random perturbation,\neliminating the need to sample random vectors in the tangent space, thus\nreducing computational cost. Theoretically, we first prove the approximation\nproperties of the estimator and then establish the sublinear convergence of the\nproposed algorithm, matching the rate of its first-order counterpart.\nNumerically, we first assess the efficiency of our estimator using kernel\nprincipal component analysis. Furthermore, we apply the proposed algorithm to\ntwo real-world scenarios: zeroth-order attacks on deep neural networks and\nlow-rank neural network training to validate the theoretical findings.\n", "link": "http://arxiv.org/abs/2507.22855v1", "date": "2025-07-30", "relevancy": 2.495, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4957}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20on%20Riemannian%20Manifolds%3A%20A%20Gradient-Free%0A%20%20Projection-Based%20Approach&body=Title%3A%20Federated%20Learning%20on%20Riemannian%20Manifolds%3A%20A%20Gradient-Free%0A%20%20Projection-Based%20Approach%0AAuthor%3A%20Hongye%20Wang%20and%20Zhaoye%20Pan%20and%20Chang%20He%20and%20Jiaxiang%20Li%20and%20Bo%20Jiang%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20collaborative%0Amodel%20training%20across%20distributed%20clients%20while%20preserving%20data%20privacy.%0AHowever%2C%20existing%20FL%20algorithms%20predominantly%20focus%20on%20unconstrained%0Aoptimization%20problems%20with%20exact%20gradient%20information%2C%20limiting%20its%0Aapplicability%20in%20scenarios%20where%20only%20noisy%20function%20evaluations%20are%20accessible%0Aor%20where%20model%20parameters%20are%20constrained.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20zeroth-order%20projection-based%20algorithm%20on%20Riemannian%20manifolds%0Afor%20FL.%20By%20leveraging%20the%20projection%20operator%2C%20we%20introduce%20a%20computationally%0Aefficient%20zeroth-order%20Riemannian%20gradient%20estimator.%20Unlike%20existing%0Aestimators%2C%20ours%20requires%20only%20a%20simple%20Euclidean%20random%20perturbation%2C%0Aeliminating%20the%20need%20to%20sample%20random%20vectors%20in%20the%20tangent%20space%2C%20thus%0Areducing%20computational%20cost.%20Theoretically%2C%20we%20first%20prove%20the%20approximation%0Aproperties%20of%20the%20estimator%20and%20then%20establish%20the%20sublinear%20convergence%20of%20the%0Aproposed%20algorithm%2C%20matching%20the%20rate%20of%20its%20first-order%20counterpart.%0ANumerically%2C%20we%20first%20assess%20the%20efficiency%20of%20our%20estimator%20using%20kernel%0Aprincipal%20component%20analysis.%20Furthermore%2C%20we%20apply%20the%20proposed%20algorithm%20to%0Atwo%20real-world%20scenarios%3A%20zeroth-order%20attacks%20on%20deep%20neural%20networks%20and%0Alow-rank%20neural%20network%20training%20to%20validate%20the%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520on%2520Riemannian%2520Manifolds%253A%2520A%2520Gradient-Free%250A%2520%2520Projection-Based%2520Approach%26entry.906535625%3DHongye%2520Wang%2520and%2520Zhaoye%2520Pan%2520and%2520Chang%2520He%2520and%2520Jiaxiang%2520Li%2520and%2520Bo%2520Jiang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520collaborative%250Amodel%2520training%2520across%2520distributed%2520clients%2520while%2520preserving%2520data%2520privacy.%250AHowever%252C%2520existing%2520FL%2520algorithms%2520predominantly%2520focus%2520on%2520unconstrained%250Aoptimization%2520problems%2520with%2520exact%2520gradient%2520information%252C%2520limiting%2520its%250Aapplicability%2520in%2520scenarios%2520where%2520only%2520noisy%2520function%2520evaluations%2520are%2520accessible%250Aor%2520where%2520model%2520parameters%2520are%2520constrained.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520novel%2520zeroth-order%2520projection-based%2520algorithm%2520on%2520Riemannian%2520manifolds%250Afor%2520FL.%2520By%2520leveraging%2520the%2520projection%2520operator%252C%2520we%2520introduce%2520a%2520computationally%250Aefficient%2520zeroth-order%2520Riemannian%2520gradient%2520estimator.%2520Unlike%2520existing%250Aestimators%252C%2520ours%2520requires%2520only%2520a%2520simple%2520Euclidean%2520random%2520perturbation%252C%250Aeliminating%2520the%2520need%2520to%2520sample%2520random%2520vectors%2520in%2520the%2520tangent%2520space%252C%2520thus%250Areducing%2520computational%2520cost.%2520Theoretically%252C%2520we%2520first%2520prove%2520the%2520approximation%250Aproperties%2520of%2520the%2520estimator%2520and%2520then%2520establish%2520the%2520sublinear%2520convergence%2520of%2520the%250Aproposed%2520algorithm%252C%2520matching%2520the%2520rate%2520of%2520its%2520first-order%2520counterpart.%250ANumerically%252C%2520we%2520first%2520assess%2520the%2520efficiency%2520of%2520our%2520estimator%2520using%2520kernel%250Aprincipal%2520component%2520analysis.%2520Furthermore%252C%2520we%2520apply%2520the%2520proposed%2520algorithm%2520to%250Atwo%2520real-world%2520scenarios%253A%2520zeroth-order%2520attacks%2520on%2520deep%2520neural%2520networks%2520and%250Alow-rank%2520neural%2520network%2520training%2520to%2520validate%2520the%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20on%20Riemannian%20Manifolds%3A%20A%20Gradient-Free%0A%20%20Projection-Based%20Approach&entry.906535625=Hongye%20Wang%20and%20Zhaoye%20Pan%20and%20Chang%20He%20and%20Jiaxiang%20Li%20and%20Bo%20Jiang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20collaborative%0Amodel%20training%20across%20distributed%20clients%20while%20preserving%20data%20privacy.%0AHowever%2C%20existing%20FL%20algorithms%20predominantly%20focus%20on%20unconstrained%0Aoptimization%20problems%20with%20exact%20gradient%20information%2C%20limiting%20its%0Aapplicability%20in%20scenarios%20where%20only%20noisy%20function%20evaluations%20are%20accessible%0Aor%20where%20model%20parameters%20are%20constrained.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20zeroth-order%20projection-based%20algorithm%20on%20Riemannian%20manifolds%0Afor%20FL.%20By%20leveraging%20the%20projection%20operator%2C%20we%20introduce%20a%20computationally%0Aefficient%20zeroth-order%20Riemannian%20gradient%20estimator.%20Unlike%20existing%0Aestimators%2C%20ours%20requires%20only%20a%20simple%20Euclidean%20random%20perturbation%2C%0Aeliminating%20the%20need%20to%20sample%20random%20vectors%20in%20the%20tangent%20space%2C%20thus%0Areducing%20computational%20cost.%20Theoretically%2C%20we%20first%20prove%20the%20approximation%0Aproperties%20of%20the%20estimator%20and%20then%20establish%20the%20sublinear%20convergence%20of%20the%0Aproposed%20algorithm%2C%20matching%20the%20rate%20of%20its%20first-order%20counterpart.%0ANumerically%2C%20we%20first%20assess%20the%20efficiency%20of%20our%20estimator%20using%20kernel%0Aprincipal%20component%20analysis.%20Furthermore%2C%20we%20apply%20the%20proposed%20algorithm%20to%0Atwo%20real-world%20scenarios%3A%20zeroth-order%20attacks%20on%20deep%20neural%20networks%20and%0Alow-rank%20neural%20network%20training%20to%20validate%20the%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22855v1&entry.124074799=Read"},
{"title": "Viser: Imperative, Web-based 3D Visualization in Python", "author": "Brent Yi and Chung Min Kim and Justin Kerr and Gina Wu and Rebecca Feng and Anthony Zhang and Jonas Kulhanek and Hongsuk Choi and Yi Ma and Matthew Tancik and Angjoo Kanazawa", "abstract": "  We present Viser, a 3D visualization library for computer vision and\nrobotics. Viser aims to bring easy and extensible 3D visualization to Python:\nwe provide a comprehensive set of 3D scene and 2D GUI primitives, which can be\nused independently with minimal setup or composed to build specialized\ninterfaces. This technical report describes Viser's features, interface, and\nimplementation. Key design choices include an imperative-style API and a\nweb-based viewer, which improve compatibility with modern programming patterns\nand workflows.\n", "link": "http://arxiv.org/abs/2507.22885v1", "date": "2025-07-30", "relevancy": 2.4751, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5139}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Viser%3A%20Imperative%2C%20Web-based%203D%20Visualization%20in%20Python&body=Title%3A%20Viser%3A%20Imperative%2C%20Web-based%203D%20Visualization%20in%20Python%0AAuthor%3A%20Brent%20Yi%20and%20Chung%20Min%20Kim%20and%20Justin%20Kerr%20and%20Gina%20Wu%20and%20Rebecca%20Feng%20and%20Anthony%20Zhang%20and%20Jonas%20Kulhanek%20and%20Hongsuk%20Choi%20and%20Yi%20Ma%20and%20Matthew%20Tancik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20We%20present%20Viser%2C%20a%203D%20visualization%20library%20for%20computer%20vision%20and%0Arobotics.%20Viser%20aims%20to%20bring%20easy%20and%20extensible%203D%20visualization%20to%20Python%3A%0Awe%20provide%20a%20comprehensive%20set%20of%203D%20scene%20and%202D%20GUI%20primitives%2C%20which%20can%20be%0Aused%20independently%20with%20minimal%20setup%20or%20composed%20to%20build%20specialized%0Ainterfaces.%20This%20technical%20report%20describes%20Viser%27s%20features%2C%20interface%2C%20and%0Aimplementation.%20Key%20design%20choices%20include%20an%20imperative-style%20API%20and%20a%0Aweb-based%20viewer%2C%20which%20improve%20compatibility%20with%20modern%20programming%20patterns%0Aand%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViser%253A%2520Imperative%252C%2520Web-based%25203D%2520Visualization%2520in%2520Python%26entry.906535625%3DBrent%2520Yi%2520and%2520Chung%2520Min%2520Kim%2520and%2520Justin%2520Kerr%2520and%2520Gina%2520Wu%2520and%2520Rebecca%2520Feng%2520and%2520Anthony%2520Zhang%2520and%2520Jonas%2520Kulhanek%2520and%2520Hongsuk%2520Choi%2520and%2520Yi%2520Ma%2520and%2520Matthew%2520Tancik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520We%2520present%2520Viser%252C%2520a%25203D%2520visualization%2520library%2520for%2520computer%2520vision%2520and%250Arobotics.%2520Viser%2520aims%2520to%2520bring%2520easy%2520and%2520extensible%25203D%2520visualization%2520to%2520Python%253A%250Awe%2520provide%2520a%2520comprehensive%2520set%2520of%25203D%2520scene%2520and%25202D%2520GUI%2520primitives%252C%2520which%2520can%2520be%250Aused%2520independently%2520with%2520minimal%2520setup%2520or%2520composed%2520to%2520build%2520specialized%250Ainterfaces.%2520This%2520technical%2520report%2520describes%2520Viser%2527s%2520features%252C%2520interface%252C%2520and%250Aimplementation.%2520Key%2520design%2520choices%2520include%2520an%2520imperative-style%2520API%2520and%2520a%250Aweb-based%2520viewer%252C%2520which%2520improve%2520compatibility%2520with%2520modern%2520programming%2520patterns%250Aand%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Viser%3A%20Imperative%2C%20Web-based%203D%20Visualization%20in%20Python&entry.906535625=Brent%20Yi%20and%20Chung%20Min%20Kim%20and%20Justin%20Kerr%20and%20Gina%20Wu%20and%20Rebecca%20Feng%20and%20Anthony%20Zhang%20and%20Jonas%20Kulhanek%20and%20Hongsuk%20Choi%20and%20Yi%20Ma%20and%20Matthew%20Tancik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20We%20present%20Viser%2C%20a%203D%20visualization%20library%20for%20computer%20vision%20and%0Arobotics.%20Viser%20aims%20to%20bring%20easy%20and%20extensible%203D%20visualization%20to%20Python%3A%0Awe%20provide%20a%20comprehensive%20set%20of%203D%20scene%20and%202D%20GUI%20primitives%2C%20which%20can%20be%0Aused%20independently%20with%20minimal%20setup%20or%20composed%20to%20build%20specialized%0Ainterfaces.%20This%20technical%20report%20describes%20Viser%27s%20features%2C%20interface%2C%20and%0Aimplementation.%20Key%20design%20choices%20include%20an%20imperative-style%20API%20and%20a%0Aweb-based%20viewer%2C%20which%20improve%20compatibility%20with%20modern%20programming%20patterns%0Aand%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22885v1&entry.124074799=Read"},
{"title": "VAR: Visual Analysis for Rashomon Set of Machine Learning Models'\n  Performance", "author": "Yuanzhe Jin", "abstract": "  Evaluating the performance of closely matched machine learning(ML) models\nunder specific conditions has long been a focus of researchers in the field of\nmachine learning. The Rashomon set is a collection of closely matched ML\nmodels, encompassing a wide range of models with similar accuracies but\ndifferent structures. Traditionally, the analysis of these sets has focused on\nvertical structural analysis, which involves comparing the corresponding\nfeatures at various levels within the ML models. However, there has been a lack\nof effective visualization methods for horizontally comparing multiple models\nwith specific features. We propose the VAR visualization solution. VAR uses\nvisualization to perform comparisons of ML models within the Rashomon set. This\nsolution combines heatmaps and scatter plots to facilitate the comparison. With\nthe help of VAR, ML model developers can identify the optimal model under\nspecific conditions and better understand the Rashomon set's overall\ncharacteristics.\n", "link": "http://arxiv.org/abs/2507.22556v1", "date": "2025-07-30", "relevancy": 2.4603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAR%3A%20Visual%20Analysis%20for%20Rashomon%20Set%20of%20Machine%20Learning%20Models%27%0A%20%20Performance&body=Title%3A%20VAR%3A%20Visual%20Analysis%20for%20Rashomon%20Set%20of%20Machine%20Learning%20Models%27%0A%20%20Performance%0AAuthor%3A%20Yuanzhe%20Jin%0AAbstract%3A%20%20%20Evaluating%20the%20performance%20of%20closely%20matched%20machine%20learning%28ML%29%20models%0Aunder%20specific%20conditions%20has%20long%20been%20a%20focus%20of%20researchers%20in%20the%20field%20of%0Amachine%20learning.%20The%20Rashomon%20set%20is%20a%20collection%20of%20closely%20matched%20ML%0Amodels%2C%20encompassing%20a%20wide%20range%20of%20models%20with%20similar%20accuracies%20but%0Adifferent%20structures.%20Traditionally%2C%20the%20analysis%20of%20these%20sets%20has%20focused%20on%0Avertical%20structural%20analysis%2C%20which%20involves%20comparing%20the%20corresponding%0Afeatures%20at%20various%20levels%20within%20the%20ML%20models.%20However%2C%20there%20has%20been%20a%20lack%0Aof%20effective%20visualization%20methods%20for%20horizontally%20comparing%20multiple%20models%0Awith%20specific%20features.%20We%20propose%20the%20VAR%20visualization%20solution.%20VAR%20uses%0Avisualization%20to%20perform%20comparisons%20of%20ML%20models%20within%20the%20Rashomon%20set.%20This%0Asolution%20combines%20heatmaps%20and%20scatter%20plots%20to%20facilitate%20the%20comparison.%20With%0Athe%20help%20of%20VAR%2C%20ML%20model%20developers%20can%20identify%20the%20optimal%20model%20under%0Aspecific%20conditions%20and%20better%20understand%20the%20Rashomon%20set%27s%20overall%0Acharacteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAR%253A%2520Visual%2520Analysis%2520for%2520Rashomon%2520Set%2520of%2520Machine%2520Learning%2520Models%2527%250A%2520%2520Performance%26entry.906535625%3DYuanzhe%2520Jin%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520performance%2520of%2520closely%2520matched%2520machine%2520learning%2528ML%2529%2520models%250Aunder%2520specific%2520conditions%2520has%2520long%2520been%2520a%2520focus%2520of%2520researchers%2520in%2520the%2520field%2520of%250Amachine%2520learning.%2520The%2520Rashomon%2520set%2520is%2520a%2520collection%2520of%2520closely%2520matched%2520ML%250Amodels%252C%2520encompassing%2520a%2520wide%2520range%2520of%2520models%2520with%2520similar%2520accuracies%2520but%250Adifferent%2520structures.%2520Traditionally%252C%2520the%2520analysis%2520of%2520these%2520sets%2520has%2520focused%2520on%250Avertical%2520structural%2520analysis%252C%2520which%2520involves%2520comparing%2520the%2520corresponding%250Afeatures%2520at%2520various%2520levels%2520within%2520the%2520ML%2520models.%2520However%252C%2520there%2520has%2520been%2520a%2520lack%250Aof%2520effective%2520visualization%2520methods%2520for%2520horizontally%2520comparing%2520multiple%2520models%250Awith%2520specific%2520features.%2520We%2520propose%2520the%2520VAR%2520visualization%2520solution.%2520VAR%2520uses%250Avisualization%2520to%2520perform%2520comparisons%2520of%2520ML%2520models%2520within%2520the%2520Rashomon%2520set.%2520This%250Asolution%2520combines%2520heatmaps%2520and%2520scatter%2520plots%2520to%2520facilitate%2520the%2520comparison.%2520With%250Athe%2520help%2520of%2520VAR%252C%2520ML%2520model%2520developers%2520can%2520identify%2520the%2520optimal%2520model%2520under%250Aspecific%2520conditions%2520and%2520better%2520understand%2520the%2520Rashomon%2520set%2527s%2520overall%250Acharacteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAR%3A%20Visual%20Analysis%20for%20Rashomon%20Set%20of%20Machine%20Learning%20Models%27%0A%20%20Performance&entry.906535625=Yuanzhe%20Jin&entry.1292438233=%20%20Evaluating%20the%20performance%20of%20closely%20matched%20machine%20learning%28ML%29%20models%0Aunder%20specific%20conditions%20has%20long%20been%20a%20focus%20of%20researchers%20in%20the%20field%20of%0Amachine%20learning.%20The%20Rashomon%20set%20is%20a%20collection%20of%20closely%20matched%20ML%0Amodels%2C%20encompassing%20a%20wide%20range%20of%20models%20with%20similar%20accuracies%20but%0Adifferent%20structures.%20Traditionally%2C%20the%20analysis%20of%20these%20sets%20has%20focused%20on%0Avertical%20structural%20analysis%2C%20which%20involves%20comparing%20the%20corresponding%0Afeatures%20at%20various%20levels%20within%20the%20ML%20models.%20However%2C%20there%20has%20been%20a%20lack%0Aof%20effective%20visualization%20methods%20for%20horizontally%20comparing%20multiple%20models%0Awith%20specific%20features.%20We%20propose%20the%20VAR%20visualization%20solution.%20VAR%20uses%0Avisualization%20to%20perform%20comparisons%20of%20ML%20models%20within%20the%20Rashomon%20set.%20This%0Asolution%20combines%20heatmaps%20and%20scatter%20plots%20to%20facilitate%20the%20comparison.%20With%0Athe%20help%20of%20VAR%2C%20ML%20model%20developers%20can%20identify%20the%20optimal%20model%20under%0Aspecific%20conditions%20and%20better%20understand%20the%20Rashomon%20set%27s%20overall%0Acharacteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22556v1&entry.124074799=Read"},
{"title": "Social-Pose: Enhancing Trajectory Prediction with Human Body Pose", "author": "Yang Gao and Saeed Saadatnejad and Alexandre Alahi", "abstract": "  Accurate human trajectory prediction is one of the most crucial tasks for\nautonomous driving, ensuring its safety. Yet, existing models often fail to\nfully leverage the visual cues that humans subconsciously communicate when\nnavigating the space. In this work, we study the benefits of predicting human\ntrajectories using human body poses instead of solely their Cartesian space\nlocations in time. We propose `Social-pose', an attention-based pose encoder\nthat effectively captures the poses of all humans in a scene and their social\nrelations. Our method can be integrated into various trajectory prediction\narchitectures. We have conducted extensive experiments on state-of-the-art\nmodels (based on LSTM, GAN, MLP, and Transformer), and showed improvements over\nall of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians\nand Cyclists in Road Traffic, and JRDB) datasets. We also explored the\nadvantages of using 2D versus 3D poses, as well as the effect of noisy poses\nand the application of our pose-based predictor in robot navigation scenarios.\n", "link": "http://arxiv.org/abs/2507.22742v1", "date": "2025-07-30", "relevancy": 2.4423, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5977}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social-Pose%3A%20Enhancing%20Trajectory%20Prediction%20with%20Human%20Body%20Pose&body=Title%3A%20Social-Pose%3A%20Enhancing%20Trajectory%20Prediction%20with%20Human%20Body%20Pose%0AAuthor%3A%20Yang%20Gao%20and%20Saeed%20Saadatnejad%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Accurate%20human%20trajectory%20prediction%20is%20one%20of%20the%20most%20crucial%20tasks%20for%0Aautonomous%20driving%2C%20ensuring%20its%20safety.%20Yet%2C%20existing%20models%20often%20fail%20to%0Afully%20leverage%20the%20visual%20cues%20that%20humans%20subconsciously%20communicate%20when%0Anavigating%20the%20space.%20In%20this%20work%2C%20we%20study%20the%20benefits%20of%20predicting%20human%0Atrajectories%20using%20human%20body%20poses%20instead%20of%20solely%20their%20Cartesian%20space%0Alocations%20in%20time.%20We%20propose%20%60Social-pose%27%2C%20an%20attention-based%20pose%20encoder%0Athat%20effectively%20captures%20the%20poses%20of%20all%20humans%20in%20a%20scene%20and%20their%20social%0Arelations.%20Our%20method%20can%20be%20integrated%20into%20various%20trajectory%20prediction%0Aarchitectures.%20We%20have%20conducted%20extensive%20experiments%20on%20state-of-the-art%0Amodels%20%28based%20on%20LSTM%2C%20GAN%2C%20MLP%2C%20and%20Transformer%29%2C%20and%20showed%20improvements%20over%0Aall%20of%20them%20on%20synthetic%20%28Joint%20Track%20Auto%29%20and%20real%20%28Human3.6M%2C%20Pedestrians%0Aand%20Cyclists%20in%20Road%20Traffic%2C%20and%20JRDB%29%20datasets.%20We%20also%20explored%20the%0Aadvantages%20of%20using%202D%20versus%203D%20poses%2C%20as%20well%20as%20the%20effect%20of%20noisy%20poses%0Aand%20the%20application%20of%20our%20pose-based%20predictor%20in%20robot%20navigation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial-Pose%253A%2520Enhancing%2520Trajectory%2520Prediction%2520with%2520Human%2520Body%2520Pose%26entry.906535625%3DYang%2520Gao%2520and%2520Saeed%2520Saadatnejad%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Accurate%2520human%2520trajectory%2520prediction%2520is%2520one%2520of%2520the%2520most%2520crucial%2520tasks%2520for%250Aautonomous%2520driving%252C%2520ensuring%2520its%2520safety.%2520Yet%252C%2520existing%2520models%2520often%2520fail%2520to%250Afully%2520leverage%2520the%2520visual%2520cues%2520that%2520humans%2520subconsciously%2520communicate%2520when%250Anavigating%2520the%2520space.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520benefits%2520of%2520predicting%2520human%250Atrajectories%2520using%2520human%2520body%2520poses%2520instead%2520of%2520solely%2520their%2520Cartesian%2520space%250Alocations%2520in%2520time.%2520We%2520propose%2520%2560Social-pose%2527%252C%2520an%2520attention-based%2520pose%2520encoder%250Athat%2520effectively%2520captures%2520the%2520poses%2520of%2520all%2520humans%2520in%2520a%2520scene%2520and%2520their%2520social%250Arelations.%2520Our%2520method%2520can%2520be%2520integrated%2520into%2520various%2520trajectory%2520prediction%250Aarchitectures.%2520We%2520have%2520conducted%2520extensive%2520experiments%2520on%2520state-of-the-art%250Amodels%2520%2528based%2520on%2520LSTM%252C%2520GAN%252C%2520MLP%252C%2520and%2520Transformer%2529%252C%2520and%2520showed%2520improvements%2520over%250Aall%2520of%2520them%2520on%2520synthetic%2520%2528Joint%2520Track%2520Auto%2529%2520and%2520real%2520%2528Human3.6M%252C%2520Pedestrians%250Aand%2520Cyclists%2520in%2520Road%2520Traffic%252C%2520and%2520JRDB%2529%2520datasets.%2520We%2520also%2520explored%2520the%250Aadvantages%2520of%2520using%25202D%2520versus%25203D%2520poses%252C%2520as%2520well%2520as%2520the%2520effect%2520of%2520noisy%2520poses%250Aand%2520the%2520application%2520of%2520our%2520pose-based%2520predictor%2520in%2520robot%2520navigation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social-Pose%3A%20Enhancing%20Trajectory%20Prediction%20with%20Human%20Body%20Pose&entry.906535625=Yang%20Gao%20and%20Saeed%20Saadatnejad%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Accurate%20human%20trajectory%20prediction%20is%20one%20of%20the%20most%20crucial%20tasks%20for%0Aautonomous%20driving%2C%20ensuring%20its%20safety.%20Yet%2C%20existing%20models%20often%20fail%20to%0Afully%20leverage%20the%20visual%20cues%20that%20humans%20subconsciously%20communicate%20when%0Anavigating%20the%20space.%20In%20this%20work%2C%20we%20study%20the%20benefits%20of%20predicting%20human%0Atrajectories%20using%20human%20body%20poses%20instead%20of%20solely%20their%20Cartesian%20space%0Alocations%20in%20time.%20We%20propose%20%60Social-pose%27%2C%20an%20attention-based%20pose%20encoder%0Athat%20effectively%20captures%20the%20poses%20of%20all%20humans%20in%20a%20scene%20and%20their%20social%0Arelations.%20Our%20method%20can%20be%20integrated%20into%20various%20trajectory%20prediction%0Aarchitectures.%20We%20have%20conducted%20extensive%20experiments%20on%20state-of-the-art%0Amodels%20%28based%20on%20LSTM%2C%20GAN%2C%20MLP%2C%20and%20Transformer%29%2C%20and%20showed%20improvements%20over%0Aall%20of%20them%20on%20synthetic%20%28Joint%20Track%20Auto%29%20and%20real%20%28Human3.6M%2C%20Pedestrians%0Aand%20Cyclists%20in%20Road%20Traffic%2C%20and%20JRDB%29%20datasets.%20We%20also%20explored%20the%0Aadvantages%20of%20using%202D%20versus%203D%20poses%2C%20as%20well%20as%20the%20effect%20of%20noisy%20poses%0Aand%20the%20application%20of%20our%20pose-based%20predictor%20in%20robot%20navigation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22742v1&entry.124074799=Read"},
{"title": "A Unified Analysis of Generalization and Sample Complexity for\n  Semi-Supervised Domain Adaptation", "author": "Elif Vural and Huseyin Karaca", "abstract": "  Domain adaptation seeks to leverage the abundant label information in a\nsource domain to improve classification performance in a target domain with\nlimited labels. While the field has seen extensive methodological development,\nits theoretical foundations remain relatively underexplored. Most existing\ntheoretical analyses focus on simplified settings where the source and target\ndomains share the same input space and relate target-domain performance to\nmeasures of domain discrepancy. Although insightful, these analyses may not\nfully capture the behavior of modern approaches that align domains into a\nshared space via feature transformations. In this paper, we present a\ncomprehensive theoretical study of domain adaptation algorithms based on domain\nalignment. We consider the joint learning of domain-aligning feature\ntransformations and a shared classifier in a semi-supervised setting. We first\nderive generalization bounds in a broad setting, in terms of covering numbers\nof the relevant function classes. We then extend our analysis to characterize\nthe sample complexity of domain-adaptive neural networks employing maximum mean\ndiscrepancy (MMD) or adversarial objectives. Our results rely on a rigorous\nanalysis of the covering numbers of these architectures. We show that, for both\nMMD-based and adversarial models, the sample complexity admits an upper bound\nthat scales quadratically with network depth and width. Furthermore, our\nanalysis suggests that in semi-supervised settings, robustness to limited\nlabeled target data can be achieved by scaling the target loss proportionally\nto the square root of the number of labeled target samples. Experimental\nevaluation in both shallow and deep settings lends support to our theoretical\nfindings.\n", "link": "http://arxiv.org/abs/2507.22632v1", "date": "2025-07-30", "relevancy": 2.4335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4971}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4946}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Analysis%20of%20Generalization%20and%20Sample%20Complexity%20for%0A%20%20Semi-Supervised%20Domain%20Adaptation&body=Title%3A%20A%20Unified%20Analysis%20of%20Generalization%20and%20Sample%20Complexity%20for%0A%20%20Semi-Supervised%20Domain%20Adaptation%0AAuthor%3A%20Elif%20Vural%20and%20Huseyin%20Karaca%0AAbstract%3A%20%20%20Domain%20adaptation%20seeks%20to%20leverage%20the%20abundant%20label%20information%20in%20a%0Asource%20domain%20to%20improve%20classification%20performance%20in%20a%20target%20domain%20with%0Alimited%20labels.%20While%20the%20field%20has%20seen%20extensive%20methodological%20development%2C%0Aits%20theoretical%20foundations%20remain%20relatively%20underexplored.%20Most%20existing%0Atheoretical%20analyses%20focus%20on%20simplified%20settings%20where%20the%20source%20and%20target%0Adomains%20share%20the%20same%20input%20space%20and%20relate%20target-domain%20performance%20to%0Ameasures%20of%20domain%20discrepancy.%20Although%20insightful%2C%20these%20analyses%20may%20not%0Afully%20capture%20the%20behavior%20of%20modern%20approaches%20that%20align%20domains%20into%20a%0Ashared%20space%20via%20feature%20transformations.%20In%20this%20paper%2C%20we%20present%20a%0Acomprehensive%20theoretical%20study%20of%20domain%20adaptation%20algorithms%20based%20on%20domain%0Aalignment.%20We%20consider%20the%20joint%20learning%20of%20domain-aligning%20feature%0Atransformations%20and%20a%20shared%20classifier%20in%20a%20semi-supervised%20setting.%20We%20first%0Aderive%20generalization%20bounds%20in%20a%20broad%20setting%2C%20in%20terms%20of%20covering%20numbers%0Aof%20the%20relevant%20function%20classes.%20We%20then%20extend%20our%20analysis%20to%20characterize%0Athe%20sample%20complexity%20of%20domain-adaptive%20neural%20networks%20employing%20maximum%20mean%0Adiscrepancy%20%28MMD%29%20or%20adversarial%20objectives.%20Our%20results%20rely%20on%20a%20rigorous%0Aanalysis%20of%20the%20covering%20numbers%20of%20these%20architectures.%20We%20show%20that%2C%20for%20both%0AMMD-based%20and%20adversarial%20models%2C%20the%20sample%20complexity%20admits%20an%20upper%20bound%0Athat%20scales%20quadratically%20with%20network%20depth%20and%20width.%20Furthermore%2C%20our%0Aanalysis%20suggests%20that%20in%20semi-supervised%20settings%2C%20robustness%20to%20limited%0Alabeled%20target%20data%20can%20be%20achieved%20by%20scaling%20the%20target%20loss%20proportionally%0Ato%20the%20square%20root%20of%20the%20number%20of%20labeled%20target%20samples.%20Experimental%0Aevaluation%20in%20both%20shallow%20and%20deep%20settings%20lends%20support%20to%20our%20theoretical%0Afindings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Analysis%2520of%2520Generalization%2520and%2520Sample%2520Complexity%2520for%250A%2520%2520Semi-Supervised%2520Domain%2520Adaptation%26entry.906535625%3DElif%2520Vural%2520and%2520Huseyin%2520Karaca%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520seeks%2520to%2520leverage%2520the%2520abundant%2520label%2520information%2520in%2520a%250Asource%2520domain%2520to%2520improve%2520classification%2520performance%2520in%2520a%2520target%2520domain%2520with%250Alimited%2520labels.%2520While%2520the%2520field%2520has%2520seen%2520extensive%2520methodological%2520development%252C%250Aits%2520theoretical%2520foundations%2520remain%2520relatively%2520underexplored.%2520Most%2520existing%250Atheoretical%2520analyses%2520focus%2520on%2520simplified%2520settings%2520where%2520the%2520source%2520and%2520target%250Adomains%2520share%2520the%2520same%2520input%2520space%2520and%2520relate%2520target-domain%2520performance%2520to%250Ameasures%2520of%2520domain%2520discrepancy.%2520Although%2520insightful%252C%2520these%2520analyses%2520may%2520not%250Afully%2520capture%2520the%2520behavior%2520of%2520modern%2520approaches%2520that%2520align%2520domains%2520into%2520a%250Ashared%2520space%2520via%2520feature%2520transformations.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Acomprehensive%2520theoretical%2520study%2520of%2520domain%2520adaptation%2520algorithms%2520based%2520on%2520domain%250Aalignment.%2520We%2520consider%2520the%2520joint%2520learning%2520of%2520domain-aligning%2520feature%250Atransformations%2520and%2520a%2520shared%2520classifier%2520in%2520a%2520semi-supervised%2520setting.%2520We%2520first%250Aderive%2520generalization%2520bounds%2520in%2520a%2520broad%2520setting%252C%2520in%2520terms%2520of%2520covering%2520numbers%250Aof%2520the%2520relevant%2520function%2520classes.%2520We%2520then%2520extend%2520our%2520analysis%2520to%2520characterize%250Athe%2520sample%2520complexity%2520of%2520domain-adaptive%2520neural%2520networks%2520employing%2520maximum%2520mean%250Adiscrepancy%2520%2528MMD%2529%2520or%2520adversarial%2520objectives.%2520Our%2520results%2520rely%2520on%2520a%2520rigorous%250Aanalysis%2520of%2520the%2520covering%2520numbers%2520of%2520these%2520architectures.%2520We%2520show%2520that%252C%2520for%2520both%250AMMD-based%2520and%2520adversarial%2520models%252C%2520the%2520sample%2520complexity%2520admits%2520an%2520upper%2520bound%250Athat%2520scales%2520quadratically%2520with%2520network%2520depth%2520and%2520width.%2520Furthermore%252C%2520our%250Aanalysis%2520suggests%2520that%2520in%2520semi-supervised%2520settings%252C%2520robustness%2520to%2520limited%250Alabeled%2520target%2520data%2520can%2520be%2520achieved%2520by%2520scaling%2520the%2520target%2520loss%2520proportionally%250Ato%2520the%2520square%2520root%2520of%2520the%2520number%2520of%2520labeled%2520target%2520samples.%2520Experimental%250Aevaluation%2520in%2520both%2520shallow%2520and%2520deep%2520settings%2520lends%2520support%2520to%2520our%2520theoretical%250Afindings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Analysis%20of%20Generalization%20and%20Sample%20Complexity%20for%0A%20%20Semi-Supervised%20Domain%20Adaptation&entry.906535625=Elif%20Vural%20and%20Huseyin%20Karaca&entry.1292438233=%20%20Domain%20adaptation%20seeks%20to%20leverage%20the%20abundant%20label%20information%20in%20a%0Asource%20domain%20to%20improve%20classification%20performance%20in%20a%20target%20domain%20with%0Alimited%20labels.%20While%20the%20field%20has%20seen%20extensive%20methodological%20development%2C%0Aits%20theoretical%20foundations%20remain%20relatively%20underexplored.%20Most%20existing%0Atheoretical%20analyses%20focus%20on%20simplified%20settings%20where%20the%20source%20and%20target%0Adomains%20share%20the%20same%20input%20space%20and%20relate%20target-domain%20performance%20to%0Ameasures%20of%20domain%20discrepancy.%20Although%20insightful%2C%20these%20analyses%20may%20not%0Afully%20capture%20the%20behavior%20of%20modern%20approaches%20that%20align%20domains%20into%20a%0Ashared%20space%20via%20feature%20transformations.%20In%20this%20paper%2C%20we%20present%20a%0Acomprehensive%20theoretical%20study%20of%20domain%20adaptation%20algorithms%20based%20on%20domain%0Aalignment.%20We%20consider%20the%20joint%20learning%20of%20domain-aligning%20feature%0Atransformations%20and%20a%20shared%20classifier%20in%20a%20semi-supervised%20setting.%20We%20first%0Aderive%20generalization%20bounds%20in%20a%20broad%20setting%2C%20in%20terms%20of%20covering%20numbers%0Aof%20the%20relevant%20function%20classes.%20We%20then%20extend%20our%20analysis%20to%20characterize%0Athe%20sample%20complexity%20of%20domain-adaptive%20neural%20networks%20employing%20maximum%20mean%0Adiscrepancy%20%28MMD%29%20or%20adversarial%20objectives.%20Our%20results%20rely%20on%20a%20rigorous%0Aanalysis%20of%20the%20covering%20numbers%20of%20these%20architectures.%20We%20show%20that%2C%20for%20both%0AMMD-based%20and%20adversarial%20models%2C%20the%20sample%20complexity%20admits%20an%20upper%20bound%0Athat%20scales%20quadratically%20with%20network%20depth%20and%20width.%20Furthermore%2C%20our%0Aanalysis%20suggests%20that%20in%20semi-supervised%20settings%2C%20robustness%20to%20limited%0Alabeled%20target%20data%20can%20be%20achieved%20by%20scaling%20the%20target%20loss%20proportionally%0Ato%20the%20square%20root%20of%20the%20number%20of%20labeled%20target%20samples.%20Experimental%0Aevaluation%20in%20both%20shallow%20and%20deep%20settings%20lends%20support%20to%20our%20theoretical%0Afindings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22632v1&entry.124074799=Read"},
{"title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation", "author": "Jiwoo Chung and Sangeek Hyun and Hyunjun Kim and Eunseo Koh and MinKyu Lee and Jae-Pil Heo", "abstract": "  Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive (VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, naive\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize minor\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage.\n", "link": "http://arxiv.org/abs/2504.02612v2", "date": "2025-07-30", "relevancy": 2.4319, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6393}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6047}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20Visual%20Autoregressive%20Models%20for%20Subject-Driven%20Generation&body=Title%3A%20Fine-Tuning%20Visual%20Autoregressive%20Models%20for%20Subject-Driven%20Generation%0AAuthor%3A%20Jiwoo%20Chung%20and%20Sangeek%20Hyun%20and%20Hyunjun%20Kim%20and%20Eunseo%20Koh%20and%20MinKyu%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-image%20generative%20models%20have%20enabled%20numerous%0Apractical%20applications%2C%20including%20subject-driven%20generation%2C%20which%20fine-tunes%0Apretrained%20models%20to%20capture%20subject%20semantics%20from%20only%20a%20few%20examples.%20While%0Adiffusion-based%20models%20produce%20high-quality%20images%2C%20their%20extensive%20denoising%0Asteps%20result%20in%20significant%20computational%20overhead%2C%20limiting%20real-world%0Aapplicability.%20Visual%20autoregressive%20%28VAR%29%20models%2C%20which%20predict%20next-scale%0Atokens%20rather%20than%20spatially%20adjacent%20ones%2C%20offer%20significantly%20faster%0Ainference%20suitable%20for%20practical%20deployment.%20In%20this%20paper%2C%20we%20propose%20the%0Afirst%20VAR-based%20approach%20for%20subject-driven%20generation.%20However%2C%20naive%0Afine-tuning%20VAR%20leads%20to%20computational%20overhead%2C%20language%20drift%2C%20and%20reduced%0Adiversity.%20To%20address%20these%20challenges%2C%20we%20introduce%20selective%20layer%20tuning%20to%0Areduce%20complexity%20and%20prior%20distillation%20to%20mitigate%20language%20drift.%0AAdditionally%2C%20we%20found%20that%20the%20early%20stages%20have%20a%20greater%20influence%20on%20the%0Ageneration%20of%20subject%20than%20the%20latter%20stages%2C%20which%20merely%20synthesize%20minor%0Adetails.%20Based%20on%20this%20finding%2C%20we%20propose%20scale-wise%20weighted%20tuning%2C%20which%0Aprioritizes%20coarser%20resolutions%20for%20promoting%20the%20model%20to%20focus%20on%20the%0Asubject-relevant%20information%20instead%20of%20local%20details.%20Extensive%20experiments%0Avalidate%20that%20our%20method%20significantly%20outperforms%20diffusion-based%20baselines%0Aacross%20various%20metrics%20and%20demonstrates%20its%20practical%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520Visual%2520Autoregressive%2520Models%2520for%2520Subject-Driven%2520Generation%26entry.906535625%3DJiwoo%2520Chung%2520and%2520Sangeek%2520Hyun%2520and%2520Hyunjun%2520Kim%2520and%2520Eunseo%2520Koh%2520and%2520MinKyu%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-image%2520generative%2520models%2520have%2520enabled%2520numerous%250Apractical%2520applications%252C%2520including%2520subject-driven%2520generation%252C%2520which%2520fine-tunes%250Apretrained%2520models%2520to%2520capture%2520subject%2520semantics%2520from%2520only%2520a%2520few%2520examples.%2520While%250Adiffusion-based%2520models%2520produce%2520high-quality%2520images%252C%2520their%2520extensive%2520denoising%250Asteps%2520result%2520in%2520significant%2520computational%2520overhead%252C%2520limiting%2520real-world%250Aapplicability.%2520Visual%2520autoregressive%2520%2528VAR%2529%2520models%252C%2520which%2520predict%2520next-scale%250Atokens%2520rather%2520than%2520spatially%2520adjacent%2520ones%252C%2520offer%2520significantly%2520faster%250Ainference%2520suitable%2520for%2520practical%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250Afirst%2520VAR-based%2520approach%2520for%2520subject-driven%2520generation.%2520However%252C%2520naive%250Afine-tuning%2520VAR%2520leads%2520to%2520computational%2520overhead%252C%2520language%2520drift%252C%2520and%2520reduced%250Adiversity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520selective%2520layer%2520tuning%2520to%250Areduce%2520complexity%2520and%2520prior%2520distillation%2520to%2520mitigate%2520language%2520drift.%250AAdditionally%252C%2520we%2520found%2520that%2520the%2520early%2520stages%2520have%2520a%2520greater%2520influence%2520on%2520the%250Ageneration%2520of%2520subject%2520than%2520the%2520latter%2520stages%252C%2520which%2520merely%2520synthesize%2520minor%250Adetails.%2520Based%2520on%2520this%2520finding%252C%2520we%2520propose%2520scale-wise%2520weighted%2520tuning%252C%2520which%250Aprioritizes%2520coarser%2520resolutions%2520for%2520promoting%2520the%2520model%2520to%2520focus%2520on%2520the%250Asubject-relevant%2520information%2520instead%2520of%2520local%2520details.%2520Extensive%2520experiments%250Avalidate%2520that%2520our%2520method%2520significantly%2520outperforms%2520diffusion-based%2520baselines%250Aacross%2520various%2520metrics%2520and%2520demonstrates%2520its%2520practical%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20Visual%20Autoregressive%20Models%20for%20Subject-Driven%20Generation&entry.906535625=Jiwoo%20Chung%20and%20Sangeek%20Hyun%20and%20Hyunjun%20Kim%20and%20Eunseo%20Koh%20and%20MinKyu%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Recent%20advances%20in%20text-to-image%20generative%20models%20have%20enabled%20numerous%0Apractical%20applications%2C%20including%20subject-driven%20generation%2C%20which%20fine-tunes%0Apretrained%20models%20to%20capture%20subject%20semantics%20from%20only%20a%20few%20examples.%20While%0Adiffusion-based%20models%20produce%20high-quality%20images%2C%20their%20extensive%20denoising%0Asteps%20result%20in%20significant%20computational%20overhead%2C%20limiting%20real-world%0Aapplicability.%20Visual%20autoregressive%20%28VAR%29%20models%2C%20which%20predict%20next-scale%0Atokens%20rather%20than%20spatially%20adjacent%20ones%2C%20offer%20significantly%20faster%0Ainference%20suitable%20for%20practical%20deployment.%20In%20this%20paper%2C%20we%20propose%20the%0Afirst%20VAR-based%20approach%20for%20subject-driven%20generation.%20However%2C%20naive%0Afine-tuning%20VAR%20leads%20to%20computational%20overhead%2C%20language%20drift%2C%20and%20reduced%0Adiversity.%20To%20address%20these%20challenges%2C%20we%20introduce%20selective%20layer%20tuning%20to%0Areduce%20complexity%20and%20prior%20distillation%20to%20mitigate%20language%20drift.%0AAdditionally%2C%20we%20found%20that%20the%20early%20stages%20have%20a%20greater%20influence%20on%20the%0Ageneration%20of%20subject%20than%20the%20latter%20stages%2C%20which%20merely%20synthesize%20minor%0Adetails.%20Based%20on%20this%20finding%2C%20we%20propose%20scale-wise%20weighted%20tuning%2C%20which%0Aprioritizes%20coarser%20resolutions%20for%20promoting%20the%20model%20to%20focus%20on%20the%0Asubject-relevant%20information%20instead%20of%20local%20details.%20Extensive%20experiments%0Avalidate%20that%20our%20method%20significantly%20outperforms%20diffusion-based%20baselines%0Aacross%20various%20metrics%20and%20demonstrates%20its%20practical%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02612v2&entry.124074799=Read"},
{"title": "I2VControl: Disentangled and Unified Video Motion Synthesis Control", "author": "Wanquan Feng and Tianhao Qi and Jiawei Liu and Mingzhen Sun and Pengqi Tu and Tianxiang Ma and Fei Dai and Songtao Zhao and Siyu Zhou and Qian He", "abstract": "  Motion controllability is crucial in video synthesis. However, most previous\nmethods are limited to single control types, and combining them often results\nin logical conflicts. In this paper, we propose a disentangled and unified\nframework, namely I2VControl, to overcome the logical conflicts. We rethink\ncamera control, object dragging, and motion brush, reformulating all tasks into\na consistent representation based on point trajectories, each managed by a\ndedicated formulation. Accordingly, we propose a spatial partitioning strategy,\nwhere each unit is assigned to a concomitant control category, enabling diverse\ncontrol types to be dynamically orchestrated within a single synthesis pipeline\nwithout conflicts. Furthermore, we design an adapter structure that functions\nas a plug-in for pre-trained models and is agnostic to specific model\narchitectures. We conduct extensive experiments, achieving excellent\nperformance on various control tasks, and our method further facilitates\nuser-driven creative combinations, enhancing innovation and creativity. Project\npage: https://wanquanf.github.io/I2VControl .\n", "link": "http://arxiv.org/abs/2411.17765v3", "date": "2025-07-30", "relevancy": 2.4254, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6251}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6118}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2VControl%3A%20Disentangled%20and%20Unified%20Video%20Motion%20Synthesis%20Control&body=Title%3A%20I2VControl%3A%20Disentangled%20and%20Unified%20Video%20Motion%20Synthesis%20Control%0AAuthor%3A%20Wanquan%20Feng%20and%20Tianhao%20Qi%20and%20Jiawei%20Liu%20and%20Mingzhen%20Sun%20and%20Pengqi%20Tu%20and%20Tianxiang%20Ma%20and%20Fei%20Dai%20and%20Songtao%20Zhao%20and%20Siyu%20Zhou%20and%20Qian%20He%0AAbstract%3A%20%20%20Motion%20controllability%20is%20crucial%20in%20video%20synthesis.%20However%2C%20most%20previous%0Amethods%20are%20limited%20to%20single%20control%20types%2C%20and%20combining%20them%20often%20results%0Ain%20logical%20conflicts.%20In%20this%20paper%2C%20we%20propose%20a%20disentangled%20and%20unified%0Aframework%2C%20namely%20I2VControl%2C%20to%20overcome%20the%20logical%20conflicts.%20We%20rethink%0Acamera%20control%2C%20object%20dragging%2C%20and%20motion%20brush%2C%20reformulating%20all%20tasks%20into%0Aa%20consistent%20representation%20based%20on%20point%20trajectories%2C%20each%20managed%20by%20a%0Adedicated%20formulation.%20Accordingly%2C%20we%20propose%20a%20spatial%20partitioning%20strategy%2C%0Awhere%20each%20unit%20is%20assigned%20to%20a%20concomitant%20control%20category%2C%20enabling%20diverse%0Acontrol%20types%20to%20be%20dynamically%20orchestrated%20within%20a%20single%20synthesis%20pipeline%0Awithout%20conflicts.%20Furthermore%2C%20we%20design%20an%20adapter%20structure%20that%20functions%0Aas%20a%20plug-in%20for%20pre-trained%20models%20and%20is%20agnostic%20to%20specific%20model%0Aarchitectures.%20We%20conduct%20extensive%20experiments%2C%20achieving%20excellent%0Aperformance%20on%20various%20control%20tasks%2C%20and%20our%20method%20further%20facilitates%0Auser-driven%20creative%20combinations%2C%20enhancing%20innovation%20and%20creativity.%20Project%0Apage%3A%20https%3A//wanquanf.github.io/I2VControl%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17765v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2VControl%253A%2520Disentangled%2520and%2520Unified%2520Video%2520Motion%2520Synthesis%2520Control%26entry.906535625%3DWanquan%2520Feng%2520and%2520Tianhao%2520Qi%2520and%2520Jiawei%2520Liu%2520and%2520Mingzhen%2520Sun%2520and%2520Pengqi%2520Tu%2520and%2520Tianxiang%2520Ma%2520and%2520Fei%2520Dai%2520and%2520Songtao%2520Zhao%2520and%2520Siyu%2520Zhou%2520and%2520Qian%2520He%26entry.1292438233%3D%2520%2520Motion%2520controllability%2520is%2520crucial%2520in%2520video%2520synthesis.%2520However%252C%2520most%2520previous%250Amethods%2520are%2520limited%2520to%2520single%2520control%2520types%252C%2520and%2520combining%2520them%2520often%2520results%250Ain%2520logical%2520conflicts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520disentangled%2520and%2520unified%250Aframework%252C%2520namely%2520I2VControl%252C%2520to%2520overcome%2520the%2520logical%2520conflicts.%2520We%2520rethink%250Acamera%2520control%252C%2520object%2520dragging%252C%2520and%2520motion%2520brush%252C%2520reformulating%2520all%2520tasks%2520into%250Aa%2520consistent%2520representation%2520based%2520on%2520point%2520trajectories%252C%2520each%2520managed%2520by%2520a%250Adedicated%2520formulation.%2520Accordingly%252C%2520we%2520propose%2520a%2520spatial%2520partitioning%2520strategy%252C%250Awhere%2520each%2520unit%2520is%2520assigned%2520to%2520a%2520concomitant%2520control%2520category%252C%2520enabling%2520diverse%250Acontrol%2520types%2520to%2520be%2520dynamically%2520orchestrated%2520within%2520a%2520single%2520synthesis%2520pipeline%250Awithout%2520conflicts.%2520Furthermore%252C%2520we%2520design%2520an%2520adapter%2520structure%2520that%2520functions%250Aas%2520a%2520plug-in%2520for%2520pre-trained%2520models%2520and%2520is%2520agnostic%2520to%2520specific%2520model%250Aarchitectures.%2520We%2520conduct%2520extensive%2520experiments%252C%2520achieving%2520excellent%250Aperformance%2520on%2520various%2520control%2520tasks%252C%2520and%2520our%2520method%2520further%2520facilitates%250Auser-driven%2520creative%2520combinations%252C%2520enhancing%2520innovation%2520and%2520creativity.%2520Project%250Apage%253A%2520https%253A//wanquanf.github.io/I2VControl%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17765v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2VControl%3A%20Disentangled%20and%20Unified%20Video%20Motion%20Synthesis%20Control&entry.906535625=Wanquan%20Feng%20and%20Tianhao%20Qi%20and%20Jiawei%20Liu%20and%20Mingzhen%20Sun%20and%20Pengqi%20Tu%20and%20Tianxiang%20Ma%20and%20Fei%20Dai%20and%20Songtao%20Zhao%20and%20Siyu%20Zhou%20and%20Qian%20He&entry.1292438233=%20%20Motion%20controllability%20is%20crucial%20in%20video%20synthesis.%20However%2C%20most%20previous%0Amethods%20are%20limited%20to%20single%20control%20types%2C%20and%20combining%20them%20often%20results%0Ain%20logical%20conflicts.%20In%20this%20paper%2C%20we%20propose%20a%20disentangled%20and%20unified%0Aframework%2C%20namely%20I2VControl%2C%20to%20overcome%20the%20logical%20conflicts.%20We%20rethink%0Acamera%20control%2C%20object%20dragging%2C%20and%20motion%20brush%2C%20reformulating%20all%20tasks%20into%0Aa%20consistent%20representation%20based%20on%20point%20trajectories%2C%20each%20managed%20by%20a%0Adedicated%20formulation.%20Accordingly%2C%20we%20propose%20a%20spatial%20partitioning%20strategy%2C%0Awhere%20each%20unit%20is%20assigned%20to%20a%20concomitant%20control%20category%2C%20enabling%20diverse%0Acontrol%20types%20to%20be%20dynamically%20orchestrated%20within%20a%20single%20synthesis%20pipeline%0Awithout%20conflicts.%20Furthermore%2C%20we%20design%20an%20adapter%20structure%20that%20functions%0Aas%20a%20plug-in%20for%20pre-trained%20models%20and%20is%20agnostic%20to%20specific%20model%0Aarchitectures.%20We%20conduct%20extensive%20experiments%2C%20achieving%20excellent%0Aperformance%20on%20various%20control%20tasks%2C%20and%20our%20method%20further%20facilitates%0Auser-driven%20creative%20combinations%2C%20enhancing%20innovation%20and%20creativity.%20Project%0Apage%3A%20https%3A//wanquanf.github.io/I2VControl%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17765v3&entry.124074799=Read"},
{"title": "Skull-stripping induces shortcut learning in MRI-based Alzheimer's\n  disease classification", "author": "Christian Tinauer and Maximilian Sackl and Rudolf Stollberger and Reinhold Schmidt and Stefan Ropele and Christian Langkammer", "abstract": "  Objectives: High classification accuracy of Alzheimer's disease (AD) from\nstructural MRI has been achieved using deep neural networks, yet the specific\nimage features contributing to these decisions remain unclear. In this study,\nthe contributions of T1-weighted (T1w) gray-white matter texture, volumetric\ninformation, and preprocessing -- particularly skull-stripping -- were\nsystematically assessed.\n  Methods: A dataset of 990 matched T1w MRIs from AD patients and cognitively\nnormal controls from the ADNI database were used. Preprocessing was varied\nthrough skull-stripping and intensity binarization to isolate texture and shape\ncontributions. A 3D convolutional neural network was trained on each\nconfiguration, and classification performance was compared using exact McNemar\ntests with discrete Bonferroni-Holm correction. Feature relevance was analyzed\nusing Layer-wise Relevance Propagation, image similarity metrics, and spectral\nclustering of relevance maps.\n  Results: Despite substantial differences in image content, classification\naccuracy, sensitivity, and specificity remained stable across preprocessing\nconditions. Models trained on binarized images preserved performance,\nindicating minimal reliance on gray-white matter texture. Instead, volumetric\nfeatures -- particularly brain contours introduced through skull-stripping --\nwere consistently used by the models.\n  Conclusions: This behavior reflects a shortcut learning phenomenon, where\npreprocessing artifacts act as potentially unintended cues. The resulting\nClever Hans effect emphasizes the critical importance of interpretability tools\nto reveal hidden biases and to ensure robust and trustworthy deep learning in\nmedical imaging.\n", "link": "http://arxiv.org/abs/2501.15831v3", "date": "2025-07-30", "relevancy": 2.4178, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4747}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skull-stripping%20induces%20shortcut%20learning%20in%20MRI-based%20Alzheimer%27s%0A%20%20disease%20classification&body=Title%3A%20Skull-stripping%20induces%20shortcut%20learning%20in%20MRI-based%20Alzheimer%27s%0A%20%20disease%20classification%0AAuthor%3A%20Christian%20Tinauer%20and%20Maximilian%20Sackl%20and%20Rudolf%20Stollberger%20and%20Reinhold%20Schmidt%20and%20Stefan%20Ropele%20and%20Christian%20Langkammer%0AAbstract%3A%20%20%20Objectives%3A%20High%20classification%20accuracy%20of%20Alzheimer%27s%20disease%20%28AD%29%20from%0Astructural%20MRI%20has%20been%20achieved%20using%20deep%20neural%20networks%2C%20yet%20the%20specific%0Aimage%20features%20contributing%20to%20these%20decisions%20remain%20unclear.%20In%20this%20study%2C%0Athe%20contributions%20of%20T1-weighted%20%28T1w%29%20gray-white%20matter%20texture%2C%20volumetric%0Ainformation%2C%20and%20preprocessing%20--%20particularly%20skull-stripping%20--%20were%0Asystematically%20assessed.%0A%20%20Methods%3A%20A%20dataset%20of%20990%20matched%20T1w%20MRIs%20from%20AD%20patients%20and%20cognitively%0Anormal%20controls%20from%20the%20ADNI%20database%20were%20used.%20Preprocessing%20was%20varied%0Athrough%20skull-stripping%20and%20intensity%20binarization%20to%20isolate%20texture%20and%20shape%0Acontributions.%20A%203D%20convolutional%20neural%20network%20was%20trained%20on%20each%0Aconfiguration%2C%20and%20classification%20performance%20was%20compared%20using%20exact%20McNemar%0Atests%20with%20discrete%20Bonferroni-Holm%20correction.%20Feature%20relevance%20was%20analyzed%0Ausing%20Layer-wise%20Relevance%20Propagation%2C%20image%20similarity%20metrics%2C%20and%20spectral%0Aclustering%20of%20relevance%20maps.%0A%20%20Results%3A%20Despite%20substantial%20differences%20in%20image%20content%2C%20classification%0Aaccuracy%2C%20sensitivity%2C%20and%20specificity%20remained%20stable%20across%20preprocessing%0Aconditions.%20Models%20trained%20on%20binarized%20images%20preserved%20performance%2C%0Aindicating%20minimal%20reliance%20on%20gray-white%20matter%20texture.%20Instead%2C%20volumetric%0Afeatures%20--%20particularly%20brain%20contours%20introduced%20through%20skull-stripping%20--%0Awere%20consistently%20used%20by%20the%20models.%0A%20%20Conclusions%3A%20This%20behavior%20reflects%20a%20shortcut%20learning%20phenomenon%2C%20where%0Apreprocessing%20artifacts%20act%20as%20potentially%20unintended%20cues.%20The%20resulting%0AClever%20Hans%20effect%20emphasizes%20the%20critical%20importance%20of%20interpretability%20tools%0Ato%20reveal%20hidden%20biases%20and%20to%20ensure%20robust%20and%20trustworthy%20deep%20learning%20in%0Amedical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15831v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkull-stripping%2520induces%2520shortcut%2520learning%2520in%2520MRI-based%2520Alzheimer%2527s%250A%2520%2520disease%2520classification%26entry.906535625%3DChristian%2520Tinauer%2520and%2520Maximilian%2520Sackl%2520and%2520Rudolf%2520Stollberger%2520and%2520Reinhold%2520Schmidt%2520and%2520Stefan%2520Ropele%2520and%2520Christian%2520Langkammer%26entry.1292438233%3D%2520%2520Objectives%253A%2520High%2520classification%2520accuracy%2520of%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520from%250Astructural%2520MRI%2520has%2520been%2520achieved%2520using%2520deep%2520neural%2520networks%252C%2520yet%2520the%2520specific%250Aimage%2520features%2520contributing%2520to%2520these%2520decisions%2520remain%2520unclear.%2520In%2520this%2520study%252C%250Athe%2520contributions%2520of%2520T1-weighted%2520%2528T1w%2529%2520gray-white%2520matter%2520texture%252C%2520volumetric%250Ainformation%252C%2520and%2520preprocessing%2520--%2520particularly%2520skull-stripping%2520--%2520were%250Asystematically%2520assessed.%250A%2520%2520Methods%253A%2520A%2520dataset%2520of%2520990%2520matched%2520T1w%2520MRIs%2520from%2520AD%2520patients%2520and%2520cognitively%250Anormal%2520controls%2520from%2520the%2520ADNI%2520database%2520were%2520used.%2520Preprocessing%2520was%2520varied%250Athrough%2520skull-stripping%2520and%2520intensity%2520binarization%2520to%2520isolate%2520texture%2520and%2520shape%250Acontributions.%2520A%25203D%2520convolutional%2520neural%2520network%2520was%2520trained%2520on%2520each%250Aconfiguration%252C%2520and%2520classification%2520performance%2520was%2520compared%2520using%2520exact%2520McNemar%250Atests%2520with%2520discrete%2520Bonferroni-Holm%2520correction.%2520Feature%2520relevance%2520was%2520analyzed%250Ausing%2520Layer-wise%2520Relevance%2520Propagation%252C%2520image%2520similarity%2520metrics%252C%2520and%2520spectral%250Aclustering%2520of%2520relevance%2520maps.%250A%2520%2520Results%253A%2520Despite%2520substantial%2520differences%2520in%2520image%2520content%252C%2520classification%250Aaccuracy%252C%2520sensitivity%252C%2520and%2520specificity%2520remained%2520stable%2520across%2520preprocessing%250Aconditions.%2520Models%2520trained%2520on%2520binarized%2520images%2520preserved%2520performance%252C%250Aindicating%2520minimal%2520reliance%2520on%2520gray-white%2520matter%2520texture.%2520Instead%252C%2520volumetric%250Afeatures%2520--%2520particularly%2520brain%2520contours%2520introduced%2520through%2520skull-stripping%2520--%250Awere%2520consistently%2520used%2520by%2520the%2520models.%250A%2520%2520Conclusions%253A%2520This%2520behavior%2520reflects%2520a%2520shortcut%2520learning%2520phenomenon%252C%2520where%250Apreprocessing%2520artifacts%2520act%2520as%2520potentially%2520unintended%2520cues.%2520The%2520resulting%250AClever%2520Hans%2520effect%2520emphasizes%2520the%2520critical%2520importance%2520of%2520interpretability%2520tools%250Ato%2520reveal%2520hidden%2520biases%2520and%2520to%2520ensure%2520robust%2520and%2520trustworthy%2520deep%2520learning%2520in%250Amedical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15831v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skull-stripping%20induces%20shortcut%20learning%20in%20MRI-based%20Alzheimer%27s%0A%20%20disease%20classification&entry.906535625=Christian%20Tinauer%20and%20Maximilian%20Sackl%20and%20Rudolf%20Stollberger%20and%20Reinhold%20Schmidt%20and%20Stefan%20Ropele%20and%20Christian%20Langkammer&entry.1292438233=%20%20Objectives%3A%20High%20classification%20accuracy%20of%20Alzheimer%27s%20disease%20%28AD%29%20from%0Astructural%20MRI%20has%20been%20achieved%20using%20deep%20neural%20networks%2C%20yet%20the%20specific%0Aimage%20features%20contributing%20to%20these%20decisions%20remain%20unclear.%20In%20this%20study%2C%0Athe%20contributions%20of%20T1-weighted%20%28T1w%29%20gray-white%20matter%20texture%2C%20volumetric%0Ainformation%2C%20and%20preprocessing%20--%20particularly%20skull-stripping%20--%20were%0Asystematically%20assessed.%0A%20%20Methods%3A%20A%20dataset%20of%20990%20matched%20T1w%20MRIs%20from%20AD%20patients%20and%20cognitively%0Anormal%20controls%20from%20the%20ADNI%20database%20were%20used.%20Preprocessing%20was%20varied%0Athrough%20skull-stripping%20and%20intensity%20binarization%20to%20isolate%20texture%20and%20shape%0Acontributions.%20A%203D%20convolutional%20neural%20network%20was%20trained%20on%20each%0Aconfiguration%2C%20and%20classification%20performance%20was%20compared%20using%20exact%20McNemar%0Atests%20with%20discrete%20Bonferroni-Holm%20correction.%20Feature%20relevance%20was%20analyzed%0Ausing%20Layer-wise%20Relevance%20Propagation%2C%20image%20similarity%20metrics%2C%20and%20spectral%0Aclustering%20of%20relevance%20maps.%0A%20%20Results%3A%20Despite%20substantial%20differences%20in%20image%20content%2C%20classification%0Aaccuracy%2C%20sensitivity%2C%20and%20specificity%20remained%20stable%20across%20preprocessing%0Aconditions.%20Models%20trained%20on%20binarized%20images%20preserved%20performance%2C%0Aindicating%20minimal%20reliance%20on%20gray-white%20matter%20texture.%20Instead%2C%20volumetric%0Afeatures%20--%20particularly%20brain%20contours%20introduced%20through%20skull-stripping%20--%0Awere%20consistently%20used%20by%20the%20models.%0A%20%20Conclusions%3A%20This%20behavior%20reflects%20a%20shortcut%20learning%20phenomenon%2C%20where%0Apreprocessing%20artifacts%20act%20as%20potentially%20unintended%20cues.%20The%20resulting%0AClever%20Hans%20effect%20emphasizes%20the%20critical%20importance%20of%20interpretability%20tools%0Ato%20reveal%20hidden%20biases%20and%20to%20ensure%20robust%20and%20trustworthy%20deep%20learning%20in%0Amedical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15831v3&entry.124074799=Read"},
{"title": "SpatialViz-Bench: Automatically Generated Spatial Visualization\n  Reasoning Tasks for MLLMs", "author": "Siting Wang and Luoyang Sun and Cheng Deng and Kun Shao and Minnan Pei and Zheng Tian and Haifeng Zhang and Jun Wang", "abstract": "  Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models show difficulty perception\nmisaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs,\ndefault to formulaic derivation over visualization, and paradoxically suffer\nperformance degradation from Chain-of-Thought prompting in open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench\ndemonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in\nspatial visualization tasks, thereby addressing a significant lacuna in the\nfield. The benchmark data and evaluation code are publicly available.\n", "link": "http://arxiv.org/abs/2507.07610v3", "date": "2025-07-30", "relevancy": 2.3768, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialViz-Bench%3A%20Automatically%20Generated%20Spatial%20Visualization%0A%20%20Reasoning%20Tasks%20for%20MLLMs&body=Title%3A%20SpatialViz-Bench%3A%20Automatically%20Generated%20Spatial%20Visualization%0A%20%20Reasoning%20Tasks%20for%20MLLMs%0AAuthor%3A%20Siting%20Wang%20and%20Luoyang%20Sun%20and%20Cheng%20Deng%20and%20Kun%20Shao%20and%20Minnan%20Pei%20and%20Zheng%20Tian%20and%20Haifeng%20Zhang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Humans%20can%20directly%20imagine%20and%20manipulate%20visual%20images%20in%20their%20minds%2C%20a%0Acapability%20known%20as%20spatial%20visualization.%20While%20multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20support%20imagination-based%20reasoning%2C%20spatial%20visualization%0Aremains%20insufficiently%20evaluated%2C%20typically%20embedded%20within%20broader%0Amathematical%20and%20logical%20assessments.%20Existing%20evaluations%20often%20rely%20on%20IQ%0Atests%20or%20math%20competitions%20that%20may%20overlap%20with%20training%20data%2C%20compromising%0Aassessment%20reliability.%20To%20this%20end%2C%20we%20introduce%20SpatialViz-Bench%2C%20a%0Acomprehensive%20multi-modal%20benchmark%20for%20spatial%20visualization%20with%2012%20tasks%0Aacross%204%20sub-abilities%2C%20comprising%201%2C180%20automatically%20generated%20problems.%20Our%0Aevaluation%20of%2033%20state-of-the-art%20MLLMs%20not%20only%20reveals%20wide%20performance%0Avariations%20and%20demonstrates%20the%20benchmark%27s%20strong%20discriminative%20power%2C%20but%0Aalso%20uncovers%20counter-intuitive%20findings%3A%20models%20show%20difficulty%20perception%0Amisaligned%20with%20human%20intuition%2C%20exhibit%20dramatic%202Dto-3D%20performance%20cliffs%2C%0Adefault%20to%20formulaic%20derivation%20over%20visualization%2C%20and%20paradoxically%20suffer%0Aperformance%20degradation%20from%20Chain-of-Thought%20prompting%20in%20open-source%20models.%0AThrough%20statistical%20and%20qualitative%20analysis%20of%20error%20types%2C%20SpatialViz-Bench%0Ademonstrates%20that%20state-of-the-art%20MLLMs%20continue%20to%20exhibit%20deficiencies%20in%0Aspatial%20visualization%20tasks%2C%20thereby%20addressing%20a%20significant%20lacuna%20in%20the%0Afield.%20The%20benchmark%20data%20and%20evaluation%20code%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07610v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialViz-Bench%253A%2520Automatically%2520Generated%2520Spatial%2520Visualization%250A%2520%2520Reasoning%2520Tasks%2520for%2520MLLMs%26entry.906535625%3DSiting%2520Wang%2520and%2520Luoyang%2520Sun%2520and%2520Cheng%2520Deng%2520and%2520Kun%2520Shao%2520and%2520Minnan%2520Pei%2520and%2520Zheng%2520Tian%2520and%2520Haifeng%2520Zhang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Humans%2520can%2520directly%2520imagine%2520and%2520manipulate%2520visual%2520images%2520in%2520their%2520minds%252C%2520a%250Acapability%2520known%2520as%2520spatial%2520visualization.%2520While%2520multi-modal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520support%2520imagination-based%2520reasoning%252C%2520spatial%2520visualization%250Aremains%2520insufficiently%2520evaluated%252C%2520typically%2520embedded%2520within%2520broader%250Amathematical%2520and%2520logical%2520assessments.%2520Existing%2520evaluations%2520often%2520rely%2520on%2520IQ%250Atests%2520or%2520math%2520competitions%2520that%2520may%2520overlap%2520with%2520training%2520data%252C%2520compromising%250Aassessment%2520reliability.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SpatialViz-Bench%252C%2520a%250Acomprehensive%2520multi-modal%2520benchmark%2520for%2520spatial%2520visualization%2520with%252012%2520tasks%250Aacross%25204%2520sub-abilities%252C%2520comprising%25201%252C180%2520automatically%2520generated%2520problems.%2520Our%250Aevaluation%2520of%252033%2520state-of-the-art%2520MLLMs%2520not%2520only%2520reveals%2520wide%2520performance%250Avariations%2520and%2520demonstrates%2520the%2520benchmark%2527s%2520strong%2520discriminative%2520power%252C%2520but%250Aalso%2520uncovers%2520counter-intuitive%2520findings%253A%2520models%2520show%2520difficulty%2520perception%250Amisaligned%2520with%2520human%2520intuition%252C%2520exhibit%2520dramatic%25202Dto-3D%2520performance%2520cliffs%252C%250Adefault%2520to%2520formulaic%2520derivation%2520over%2520visualization%252C%2520and%2520paradoxically%2520suffer%250Aperformance%2520degradation%2520from%2520Chain-of-Thought%2520prompting%2520in%2520open-source%2520models.%250AThrough%2520statistical%2520and%2520qualitative%2520analysis%2520of%2520error%2520types%252C%2520SpatialViz-Bench%250Ademonstrates%2520that%2520state-of-the-art%2520MLLMs%2520continue%2520to%2520exhibit%2520deficiencies%2520in%250Aspatial%2520visualization%2520tasks%252C%2520thereby%2520addressing%2520a%2520significant%2520lacuna%2520in%2520the%250Afield.%2520The%2520benchmark%2520data%2520and%2520evaluation%2520code%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07610v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialViz-Bench%3A%20Automatically%20Generated%20Spatial%20Visualization%0A%20%20Reasoning%20Tasks%20for%20MLLMs&entry.906535625=Siting%20Wang%20and%20Luoyang%20Sun%20and%20Cheng%20Deng%20and%20Kun%20Shao%20and%20Minnan%20Pei%20and%20Zheng%20Tian%20and%20Haifeng%20Zhang%20and%20Jun%20Wang&entry.1292438233=%20%20Humans%20can%20directly%20imagine%20and%20manipulate%20visual%20images%20in%20their%20minds%2C%20a%0Acapability%20known%20as%20spatial%20visualization.%20While%20multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20support%20imagination-based%20reasoning%2C%20spatial%20visualization%0Aremains%20insufficiently%20evaluated%2C%20typically%20embedded%20within%20broader%0Amathematical%20and%20logical%20assessments.%20Existing%20evaluations%20often%20rely%20on%20IQ%0Atests%20or%20math%20competitions%20that%20may%20overlap%20with%20training%20data%2C%20compromising%0Aassessment%20reliability.%20To%20this%20end%2C%20we%20introduce%20SpatialViz-Bench%2C%20a%0Acomprehensive%20multi-modal%20benchmark%20for%20spatial%20visualization%20with%2012%20tasks%0Aacross%204%20sub-abilities%2C%20comprising%201%2C180%20automatically%20generated%20problems.%20Our%0Aevaluation%20of%2033%20state-of-the-art%20MLLMs%20not%20only%20reveals%20wide%20performance%0Avariations%20and%20demonstrates%20the%20benchmark%27s%20strong%20discriminative%20power%2C%20but%0Aalso%20uncovers%20counter-intuitive%20findings%3A%20models%20show%20difficulty%20perception%0Amisaligned%20with%20human%20intuition%2C%20exhibit%20dramatic%202Dto-3D%20performance%20cliffs%2C%0Adefault%20to%20formulaic%20derivation%20over%20visualization%2C%20and%20paradoxically%20suffer%0Aperformance%20degradation%20from%20Chain-of-Thought%20prompting%20in%20open-source%20models.%0AThrough%20statistical%20and%20qualitative%20analysis%20of%20error%20types%2C%20SpatialViz-Bench%0Ademonstrates%20that%20state-of-the-art%20MLLMs%20continue%20to%20exhibit%20deficiencies%20in%0Aspatial%20visualization%20tasks%2C%20thereby%20addressing%20a%20significant%20lacuna%20in%20the%0Afield.%20The%20benchmark%20data%20and%20evaluation%20code%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07610v3&entry.124074799=Read"},
{"title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy\n  Protection", "author": "Liqin Wang and Qianyue Hu and Wei Lu and Xiangyang Luo", "abstract": "  The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun.\n", "link": "http://arxiv.org/abs/2504.21646v3", "date": "2025-07-30", "relevancy": 2.3696, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6295}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5716}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Adversarial%20Identity%20Manipulation%20for%20Facial%20Privacy%0A%20%20Protection&body=Title%3A%20Diffusion-based%20Adversarial%20Identity%20Manipulation%20for%20Facial%20Privacy%0A%20%20Protection%0AAuthor%3A%20Liqin%20Wang%20and%20Qianyue%20Hu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%0AAbstract%3A%20%20%20The%20success%20of%20face%20recognition%20%28FR%29%20systems%20has%20led%20to%20serious%20privacy%0Aconcerns%20due%20to%20potential%20unauthorized%20surveillance%20and%20user%20tracking%20on%20social%0Anetworks.%20Existing%20methods%20for%20enhancing%20privacy%20fail%20to%20generate%20natural%20face%0Aimages%20that%20can%20protect%20facial%20privacy.%20In%20this%20paper%2C%20we%20propose%0Adiffusion-based%20adversarial%20identity%20manipulation%20%28DiffAIM%29%20to%20generate%20natural%0Aand%20highly%20transferable%20adversarial%20faces%20against%20malicious%20FR%20systems.%20To%20be%0Aspecific%2C%20we%20manipulate%20facial%20identity%20within%20the%20low-dimensional%20latent%20space%0Aof%20a%20diffusion%20model.%20This%20involves%20iteratively%20injecting%20gradient-based%0Aadversarial%20identity%20guidance%20during%20the%20reverse%20diffusion%20process%2C%0Aprogressively%20steering%20the%20generation%20toward%20the%20desired%20adversarial%20faces.%20The%0Aguidance%20is%20optimized%20for%20identity%20convergence%20towards%20a%20target%20while%20promoting%0Asemantic%20divergence%20from%20the%20source%2C%20facilitating%20effective%20impersonation%20while%0Amaintaining%20visual%20naturalness.%20We%20further%20incorporate%20structure-preserving%0Aregularization%20to%20preserve%20facial%20structure%20consistency%20during%20manipulation.%0AExtensive%20experiments%20on%20both%20face%20verification%20and%20identification%20tasks%0Ademonstrate%20that%20compared%20with%20the%20state-of-the-art%2C%20DiffAIM%20achieves%20stronger%0Ablack-box%20attack%20transferability%20while%20maintaining%20superior%20visual%20quality.%20We%0Aalso%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20for%20commercial%20FR%0AAPIs%2C%20including%20Face%2B%2B%20and%20Aliyun.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21646v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Adversarial%2520Identity%2520Manipulation%2520for%2520Facial%2520Privacy%250A%2520%2520Protection%26entry.906535625%3DLiqin%2520Wang%2520and%2520Qianyue%2520Hu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520face%2520recognition%2520%2528FR%2529%2520systems%2520has%2520led%2520to%2520serious%2520privacy%250Aconcerns%2520due%2520to%2520potential%2520unauthorized%2520surveillance%2520and%2520user%2520tracking%2520on%2520social%250Anetworks.%2520Existing%2520methods%2520for%2520enhancing%2520privacy%2520fail%2520to%2520generate%2520natural%2520face%250Aimages%2520that%2520can%2520protect%2520facial%2520privacy.%2520In%2520this%2520paper%252C%2520we%2520propose%250Adiffusion-based%2520adversarial%2520identity%2520manipulation%2520%2528DiffAIM%2529%2520to%2520generate%2520natural%250Aand%2520highly%2520transferable%2520adversarial%2520faces%2520against%2520malicious%2520FR%2520systems.%2520To%2520be%250Aspecific%252C%2520we%2520manipulate%2520facial%2520identity%2520within%2520the%2520low-dimensional%2520latent%2520space%250Aof%2520a%2520diffusion%2520model.%2520This%2520involves%2520iteratively%2520injecting%2520gradient-based%250Aadversarial%2520identity%2520guidance%2520during%2520the%2520reverse%2520diffusion%2520process%252C%250Aprogressively%2520steering%2520the%2520generation%2520toward%2520the%2520desired%2520adversarial%2520faces.%2520The%250Aguidance%2520is%2520optimized%2520for%2520identity%2520convergence%2520towards%2520a%2520target%2520while%2520promoting%250Asemantic%2520divergence%2520from%2520the%2520source%252C%2520facilitating%2520effective%2520impersonation%2520while%250Amaintaining%2520visual%2520naturalness.%2520We%2520further%2520incorporate%2520structure-preserving%250Aregularization%2520to%2520preserve%2520facial%2520structure%2520consistency%2520during%2520manipulation.%250AExtensive%2520experiments%2520on%2520both%2520face%2520verification%2520and%2520identification%2520tasks%250Ademonstrate%2520that%2520compared%2520with%2520the%2520state-of-the-art%252C%2520DiffAIM%2520achieves%2520stronger%250Ablack-box%2520attack%2520transferability%2520while%2520maintaining%2520superior%2520visual%2520quality.%2520We%250Aalso%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520for%2520commercial%2520FR%250AAPIs%252C%2520including%2520Face%252B%252B%2520and%2520Aliyun.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21646v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Adversarial%20Identity%20Manipulation%20for%20Facial%20Privacy%0A%20%20Protection&entry.906535625=Liqin%20Wang%20and%20Qianyue%20Hu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo&entry.1292438233=%20%20The%20success%20of%20face%20recognition%20%28FR%29%20systems%20has%20led%20to%20serious%20privacy%0Aconcerns%20due%20to%20potential%20unauthorized%20surveillance%20and%20user%20tracking%20on%20social%0Anetworks.%20Existing%20methods%20for%20enhancing%20privacy%20fail%20to%20generate%20natural%20face%0Aimages%20that%20can%20protect%20facial%20privacy.%20In%20this%20paper%2C%20we%20propose%0Adiffusion-based%20adversarial%20identity%20manipulation%20%28DiffAIM%29%20to%20generate%20natural%0Aand%20highly%20transferable%20adversarial%20faces%20against%20malicious%20FR%20systems.%20To%20be%0Aspecific%2C%20we%20manipulate%20facial%20identity%20within%20the%20low-dimensional%20latent%20space%0Aof%20a%20diffusion%20model.%20This%20involves%20iteratively%20injecting%20gradient-based%0Aadversarial%20identity%20guidance%20during%20the%20reverse%20diffusion%20process%2C%0Aprogressively%20steering%20the%20generation%20toward%20the%20desired%20adversarial%20faces.%20The%0Aguidance%20is%20optimized%20for%20identity%20convergence%20towards%20a%20target%20while%20promoting%0Asemantic%20divergence%20from%20the%20source%2C%20facilitating%20effective%20impersonation%20while%0Amaintaining%20visual%20naturalness.%20We%20further%20incorporate%20structure-preserving%0Aregularization%20to%20preserve%20facial%20structure%20consistency%20during%20manipulation.%0AExtensive%20experiments%20on%20both%20face%20verification%20and%20identification%20tasks%0Ademonstrate%20that%20compared%20with%20the%20state-of-the-art%2C%20DiffAIM%20achieves%20stronger%0Ablack-box%20attack%20transferability%20while%20maintaining%20superior%20visual%20quality.%20We%0Aalso%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20for%20commercial%20FR%0AAPIs%2C%20including%20Face%2B%2B%20and%20Aliyun.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21646v3&entry.124074799=Read"},
{"title": "GATEAU: Selecting Influential Samples for Long Context Alignment", "author": "Shuzheng Si and Haozhe Zhao and Gang Chen and Yunshui Li and Kangyang Luo and Chuancheng Lv and Kaikai An and Fanchao Qi and Baobao Chang and Maosong Sun", "abstract": "  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies have attempted to\nscale up the available data volume by synthesizing long instruction-following\nsamples, as constructing such a dataset tends to be challenging for annotators.\nHowever, a lack of a well-defined strategy for ensuring data quality may\nintroduce low-quality samples and restrict the model's performance. Thus, we\npropose GATEAU, a novel framework to address the unique challenge of long\ncontext alignment by identifying the influential samples enriched with\nlong-range dependency relations. Specifically, GATEAU measures the long-range\ndependencies from two essential aspects: the difficulty of generating target\nresponses due to the long-range dependencies, and the difficulty of\nunderstanding long inputs due to such dependencies. Comprehensive experiments\nindicate that GATEAU effectively identifies influential samples, and the model\ntrained on these selected samples exhibits better instruction-following and\nlong-context understanding capabilities.\n", "link": "http://arxiv.org/abs/2410.15633v6", "date": "2025-07-30", "relevancy": 2.3688, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment&body=Title%3A%20GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment%0AAuthor%3A%20Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20have%20attempted%20to%0Ascale%20up%20the%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%0Asamples%2C%20as%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%0AHowever%2C%20a%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%0Aintroduce%20low-quality%20samples%20and%20restrict%20the%20model%27s%20performance.%20Thus%2C%20we%0Apropose%20GATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%0Acontext%20alignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%0Along-range%20dependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%0Adependencies%20from%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%0Aresponses%20due%20to%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%0Aunderstanding%20long%20inputs%20due%20to%20such%20dependencies.%20Comprehensive%20experiments%0Aindicate%20that%20GATEAU%20effectively%20identifies%20influential%20samples%2C%20and%20the%20model%0Atrained%20on%20these%20selected%20samples%20exhibits%20better%20instruction-following%20and%0Along-context%20understanding%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15633v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGATEAU%253A%2520Selecting%2520Influential%2520Samples%2520for%2520Long%2520Context%2520Alignment%26entry.906535625%3DShuzheng%2520Si%2520and%2520Haozhe%2520Zhao%2520and%2520Gang%2520Chen%2520and%2520Yunshui%2520Li%2520and%2520Kangyang%2520Luo%2520and%2520Chuancheng%2520Lv%2520and%2520Kaikai%2520An%2520and%2520Fanchao%2520Qi%2520and%2520Baobao%2520Chang%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Aligning%2520large%2520language%2520models%2520to%2520handle%2520instructions%2520with%2520extremely%2520long%250Acontexts%2520has%2520yet%2520to%2520be%2520fully%2520investigated.%2520Previous%2520studies%2520have%2520attempted%2520to%250Ascale%2520up%2520the%2520available%2520data%2520volume%2520by%2520synthesizing%2520long%2520instruction-following%250Asamples%252C%2520as%2520constructing%2520such%2520a%2520dataset%2520tends%2520to%2520be%2520challenging%2520for%2520annotators.%250AHowever%252C%2520a%2520lack%2520of%2520a%2520well-defined%2520strategy%2520for%2520ensuring%2520data%2520quality%2520may%250Aintroduce%2520low-quality%2520samples%2520and%2520restrict%2520the%2520model%2527s%2520performance.%2520Thus%252C%2520we%250Apropose%2520GATEAU%252C%2520a%2520novel%2520framework%2520to%2520address%2520the%2520unique%2520challenge%2520of%2520long%250Acontext%2520alignment%2520by%2520identifying%2520the%2520influential%2520samples%2520enriched%2520with%250Along-range%2520dependency%2520relations.%2520Specifically%252C%2520GATEAU%2520measures%2520the%2520long-range%250Adependencies%2520from%2520two%2520essential%2520aspects%253A%2520the%2520difficulty%2520of%2520generating%2520target%250Aresponses%2520due%2520to%2520the%2520long-range%2520dependencies%252C%2520and%2520the%2520difficulty%2520of%250Aunderstanding%2520long%2520inputs%2520due%2520to%2520such%2520dependencies.%2520Comprehensive%2520experiments%250Aindicate%2520that%2520GATEAU%2520effectively%2520identifies%2520influential%2520samples%252C%2520and%2520the%2520model%250Atrained%2520on%2520these%2520selected%2520samples%2520exhibits%2520better%2520instruction-following%2520and%250Along-context%2520understanding%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15633v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment&entry.906535625=Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun&entry.1292438233=%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20have%20attempted%20to%0Ascale%20up%20the%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%0Asamples%2C%20as%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%0AHowever%2C%20a%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%0Aintroduce%20low-quality%20samples%20and%20restrict%20the%20model%27s%20performance.%20Thus%2C%20we%0Apropose%20GATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%0Acontext%20alignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%0Along-range%20dependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%0Adependencies%20from%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%0Aresponses%20due%20to%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%0Aunderstanding%20long%20inputs%20due%20to%20such%20dependencies.%20Comprehensive%20experiments%0Aindicate%20that%20GATEAU%20effectively%20identifies%20influential%20samples%2C%20and%20the%20model%0Atrained%20on%20these%20selected%20samples%20exhibits%20better%20instruction-following%20and%0Along-context%20understanding%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15633v6&entry.124074799=Read"},
{"title": "Quantifying surprise in clinical care: Detecting highly informative\n  events in electronic health records with foundation models", "author": "Michael C. Burkhart and Bashar Ramadan and Luke Solo and William F. Parker and Brett K. Beaulieu-Jones", "abstract": "  We present a foundation model-derived method to identify highly informative\ntokens and events in electronic health records. Our approach considers incoming\ndata in the entire context of a patient's hospitalization and so can flag\nanomalous events that rule-based approaches would consider within a normal\nrange. We demonstrate that the events our model flags are significant for\npredicting downstream patient outcomes and that a fraction of events identified\nas carrying little information can safely be dropped. Additionally, we show how\ninformativeness can help interpret the predictions of prognostic models trained\non foundation model-derived representations.\n", "link": "http://arxiv.org/abs/2507.22798v1", "date": "2025-07-30", "relevancy": 2.3398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20surprise%20in%20clinical%20care%3A%20Detecting%20highly%20informative%0A%20%20events%20in%20electronic%20health%20records%20with%20foundation%20models&body=Title%3A%20Quantifying%20surprise%20in%20clinical%20care%3A%20Detecting%20highly%20informative%0A%20%20events%20in%20electronic%20health%20records%20with%20foundation%20models%0AAuthor%3A%20Michael%20C.%20Burkhart%20and%20Bashar%20Ramadan%20and%20Luke%20Solo%20and%20William%20F.%20Parker%20and%20Brett%20K.%20Beaulieu-Jones%0AAbstract%3A%20%20%20We%20present%20a%20foundation%20model-derived%20method%20to%20identify%20highly%20informative%0Atokens%20and%20events%20in%20electronic%20health%20records.%20Our%20approach%20considers%20incoming%0Adata%20in%20the%20entire%20context%20of%20a%20patient%27s%20hospitalization%20and%20so%20can%20flag%0Aanomalous%20events%20that%20rule-based%20approaches%20would%20consider%20within%20a%20normal%0Arange.%20We%20demonstrate%20that%20the%20events%20our%20model%20flags%20are%20significant%20for%0Apredicting%20downstream%20patient%20outcomes%20and%20that%20a%20fraction%20of%20events%20identified%0Aas%20carrying%20little%20information%20can%20safely%20be%20dropped.%20Additionally%2C%20we%20show%20how%0Ainformativeness%20can%20help%20interpret%20the%20predictions%20of%20prognostic%20models%20trained%0Aon%20foundation%20model-derived%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520surprise%2520in%2520clinical%2520care%253A%2520Detecting%2520highly%2520informative%250A%2520%2520events%2520in%2520electronic%2520health%2520records%2520with%2520foundation%2520models%26entry.906535625%3DMichael%2520C.%2520Burkhart%2520and%2520Bashar%2520Ramadan%2520and%2520Luke%2520Solo%2520and%2520William%2520F.%2520Parker%2520and%2520Brett%2520K.%2520Beaulieu-Jones%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520foundation%2520model-derived%2520method%2520to%2520identify%2520highly%2520informative%250Atokens%2520and%2520events%2520in%2520electronic%2520health%2520records.%2520Our%2520approach%2520considers%2520incoming%250Adata%2520in%2520the%2520entire%2520context%2520of%2520a%2520patient%2527s%2520hospitalization%2520and%2520so%2520can%2520flag%250Aanomalous%2520events%2520that%2520rule-based%2520approaches%2520would%2520consider%2520within%2520a%2520normal%250Arange.%2520We%2520demonstrate%2520that%2520the%2520events%2520our%2520model%2520flags%2520are%2520significant%2520for%250Apredicting%2520downstream%2520patient%2520outcomes%2520and%2520that%2520a%2520fraction%2520of%2520events%2520identified%250Aas%2520carrying%2520little%2520information%2520can%2520safely%2520be%2520dropped.%2520Additionally%252C%2520we%2520show%2520how%250Ainformativeness%2520can%2520help%2520interpret%2520the%2520predictions%2520of%2520prognostic%2520models%2520trained%250Aon%2520foundation%2520model-derived%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20surprise%20in%20clinical%20care%3A%20Detecting%20highly%20informative%0A%20%20events%20in%20electronic%20health%20records%20with%20foundation%20models&entry.906535625=Michael%20C.%20Burkhart%20and%20Bashar%20Ramadan%20and%20Luke%20Solo%20and%20William%20F.%20Parker%20and%20Brett%20K.%20Beaulieu-Jones&entry.1292438233=%20%20We%20present%20a%20foundation%20model-derived%20method%20to%20identify%20highly%20informative%0Atokens%20and%20events%20in%20electronic%20health%20records.%20Our%20approach%20considers%20incoming%0Adata%20in%20the%20entire%20context%20of%20a%20patient%27s%20hospitalization%20and%20so%20can%20flag%0Aanomalous%20events%20that%20rule-based%20approaches%20would%20consider%20within%20a%20normal%0Arange.%20We%20demonstrate%20that%20the%20events%20our%20model%20flags%20are%20significant%20for%0Apredicting%20downstream%20patient%20outcomes%20and%20that%20a%20fraction%20of%20events%20identified%0Aas%20carrying%20little%20information%20can%20safely%20be%20dropped.%20Additionally%2C%20we%20show%20how%0Ainformativeness%20can%20help%20interpret%20the%20predictions%20of%20prognostic%20models%20trained%0Aon%20foundation%20model-derived%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22798v1&entry.124074799=Read"},
{"title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness", "author": "Hong-Jun Yoon and Mariam Kiran and Danial Ebling and Joe Breen", "abstract": "  The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere.\n", "link": "http://arxiv.org/abs/2507.22711v1", "date": "2025-07-30", "relevancy": 2.3389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OFCnetLLM%3A%20Large%20Language%20Model%20for%20Network%20Monitoring%20and%20Alertness&body=Title%3A%20OFCnetLLM%3A%20Large%20Language%20Model%20for%20Network%20Monitoring%20and%20Alertness%0AAuthor%3A%20Hong-Jun%20Yoon%20and%20Mariam%20Kiran%20and%20Danial%20Ebling%20and%20Joe%20Breen%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20network%20infrastructure%20is%20bringing%20new%20challenges%20and%0Aopportunities%20for%20efficient%20network%20management%2C%20optimization%2C%20and%20security.%0AWith%20very%20large%20monitoring%20databases%20becoming%20expensive%20to%20explore%2C%20the%20use%20of%0AAI%20and%20Generative%20AI%20can%20help%20reduce%20costs%20of%20managing%20these%20datasets.%20This%0Apaper%20explores%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20revolutionize%20network%0Amonitoring%20management%20by%20addressing%20the%20limitations%20of%20query%20finding%20and%0Apattern%20analysis.%20We%20leverage%20LLMs%20to%20enhance%20anomaly%20detection%2C%20automate%0Aroot-cause%20analysis%2C%20and%20automate%20incident%20analysis%20to%20build%20a%20well-monitored%0Anetwork%20management%20team%20using%20AI.%20Through%20a%20real-world%20example%20of%20developing%0Aour%20own%20OFCNetLLM%2C%20based%20on%20the%20open-source%20LLM%20model%2C%20we%20demonstrate%20practical%0Aapplications%20of%20OFCnetLLM%20in%20the%20OFC%20conference%20network.%20Our%20model%20is%20developed%0Aas%20a%20multi-agent%20approach%20and%20is%20still%20evolving%2C%20and%20we%20present%20early%20results%0Ahere.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOFCnetLLM%253A%2520Large%2520Language%2520Model%2520for%2520Network%2520Monitoring%2520and%2520Alertness%26entry.906535625%3DHong-Jun%2520Yoon%2520and%2520Mariam%2520Kiran%2520and%2520Danial%2520Ebling%2520and%2520Joe%2520Breen%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520network%2520infrastructure%2520is%2520bringing%2520new%2520challenges%2520and%250Aopportunities%2520for%2520efficient%2520network%2520management%252C%2520optimization%252C%2520and%2520security.%250AWith%2520very%2520large%2520monitoring%2520databases%2520becoming%2520expensive%2520to%2520explore%252C%2520the%2520use%2520of%250AAI%2520and%2520Generative%2520AI%2520can%2520help%2520reduce%2520costs%2520of%2520managing%2520these%2520datasets.%2520This%250Apaper%2520explores%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520revolutionize%2520network%250Amonitoring%2520management%2520by%2520addressing%2520the%2520limitations%2520of%2520query%2520finding%2520and%250Apattern%2520analysis.%2520We%2520leverage%2520LLMs%2520to%2520enhance%2520anomaly%2520detection%252C%2520automate%250Aroot-cause%2520analysis%252C%2520and%2520automate%2520incident%2520analysis%2520to%2520build%2520a%2520well-monitored%250Anetwork%2520management%2520team%2520using%2520AI.%2520Through%2520a%2520real-world%2520example%2520of%2520developing%250Aour%2520own%2520OFCNetLLM%252C%2520based%2520on%2520the%2520open-source%2520LLM%2520model%252C%2520we%2520demonstrate%2520practical%250Aapplications%2520of%2520OFCnetLLM%2520in%2520the%2520OFC%2520conference%2520network.%2520Our%2520model%2520is%2520developed%250Aas%2520a%2520multi-agent%2520approach%2520and%2520is%2520still%2520evolving%252C%2520and%2520we%2520present%2520early%2520results%250Ahere.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OFCnetLLM%3A%20Large%20Language%20Model%20for%20Network%20Monitoring%20and%20Alertness&entry.906535625=Hong-Jun%20Yoon%20and%20Mariam%20Kiran%20and%20Danial%20Ebling%20and%20Joe%20Breen&entry.1292438233=%20%20The%20rapid%20evolution%20of%20network%20infrastructure%20is%20bringing%20new%20challenges%20and%0Aopportunities%20for%20efficient%20network%20management%2C%20optimization%2C%20and%20security.%0AWith%20very%20large%20monitoring%20databases%20becoming%20expensive%20to%20explore%2C%20the%20use%20of%0AAI%20and%20Generative%20AI%20can%20help%20reduce%20costs%20of%20managing%20these%20datasets.%20This%0Apaper%20explores%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20revolutionize%20network%0Amonitoring%20management%20by%20addressing%20the%20limitations%20of%20query%20finding%20and%0Apattern%20analysis.%20We%20leverage%20LLMs%20to%20enhance%20anomaly%20detection%2C%20automate%0Aroot-cause%20analysis%2C%20and%20automate%20incident%20analysis%20to%20build%20a%20well-monitored%0Anetwork%20management%20team%20using%20AI.%20Through%20a%20real-world%20example%20of%20developing%0Aour%20own%20OFCNetLLM%2C%20based%20on%20the%20open-source%20LLM%20model%2C%20we%20demonstrate%20practical%0Aapplications%20of%20OFCnetLLM%20in%20the%20OFC%20conference%20network.%20Our%20model%20is%20developed%0Aas%20a%20multi-agent%20approach%20and%20is%20still%20evolving%2C%20and%20we%20present%20early%20results%0Ahere.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22711v1&entry.124074799=Read"},
{"title": "Perception-aware Planning for Quadrotor Flight in Unknown and\n  Feature-limited Environments", "author": "Chenxin Yu and Zihong Lu and Jie Mei and Boyu Zhou", "abstract": "  Various studies on perception-aware planning have been proposed to enhance\nthe state estimation accuracy of quadrotors in visually degraded environments.\nHowever, many existing methods heavily rely on prior environmental knowledge\nand face significant limitations in previously unknown environments with sparse\nlocalization features, which greatly limits their practical application. In\nthis paper, we present a perception-aware planning method for quadrotor flight\nin unknown and feature-limited environments that properly allocates perception\nresources among environmental information during navigation. We introduce a\nviewpoint transition graph that allows for the adaptive selection of local\ntarget viewpoints, which guide the quadrotor to efficiently navigate to the\ngoal while maintaining sufficient localizability and without being trapped in\nfeature-limited regions. During the local planning, a novel yaw trajectory\ngeneration method that simultaneously considers exploration capability and\nlocalizability is presented. It constructs a localizable corridor via feature\nco-visibility evaluation to ensure localization robustness in a computationally\nefficient way. Through validations conducted in both simulation and real-world\nexperiments, we demonstrate the feasibility and real-time performance of the\nproposed method. The source code will be released to benefit the community.\n", "link": "http://arxiv.org/abs/2503.15273v2", "date": "2025-07-30", "relevancy": 2.3371, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6038}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5724}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception-aware%20Planning%20for%20Quadrotor%20Flight%20in%20Unknown%20and%0A%20%20Feature-limited%20Environments&body=Title%3A%20Perception-aware%20Planning%20for%20Quadrotor%20Flight%20in%20Unknown%20and%0A%20%20Feature-limited%20Environments%0AAuthor%3A%20Chenxin%20Yu%20and%20Zihong%20Lu%20and%20Jie%20Mei%20and%20Boyu%20Zhou%0AAbstract%3A%20%20%20Various%20studies%20on%20perception-aware%20planning%20have%20been%20proposed%20to%20enhance%0Athe%20state%20estimation%20accuracy%20of%20quadrotors%20in%20visually%20degraded%20environments.%0AHowever%2C%20many%20existing%20methods%20heavily%20rely%20on%20prior%20environmental%20knowledge%0Aand%20face%20significant%20limitations%20in%20previously%20unknown%20environments%20with%20sparse%0Alocalization%20features%2C%20which%20greatly%20limits%20their%20practical%20application.%20In%0Athis%20paper%2C%20we%20present%20a%20perception-aware%20planning%20method%20for%20quadrotor%20flight%0Ain%20unknown%20and%20feature-limited%20environments%20that%20properly%20allocates%20perception%0Aresources%20among%20environmental%20information%20during%20navigation.%20We%20introduce%20a%0Aviewpoint%20transition%20graph%20that%20allows%20for%20the%20adaptive%20selection%20of%20local%0Atarget%20viewpoints%2C%20which%20guide%20the%20quadrotor%20to%20efficiently%20navigate%20to%20the%0Agoal%20while%20maintaining%20sufficient%20localizability%20and%20without%20being%20trapped%20in%0Afeature-limited%20regions.%20During%20the%20local%20planning%2C%20a%20novel%20yaw%20trajectory%0Ageneration%20method%20that%20simultaneously%20considers%20exploration%20capability%20and%0Alocalizability%20is%20presented.%20It%20constructs%20a%20localizable%20corridor%20via%20feature%0Aco-visibility%20evaluation%20to%20ensure%20localization%20robustness%20in%20a%20computationally%0Aefficient%20way.%20Through%20validations%20conducted%20in%20both%20simulation%20and%20real-world%0Aexperiments%2C%20we%20demonstrate%20the%20feasibility%20and%20real-time%20performance%20of%20the%0Aproposed%20method.%20The%20source%20code%20will%20be%20released%20to%20benefit%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception-aware%2520Planning%2520for%2520Quadrotor%2520Flight%2520in%2520Unknown%2520and%250A%2520%2520Feature-limited%2520Environments%26entry.906535625%3DChenxin%2520Yu%2520and%2520Zihong%2520Lu%2520and%2520Jie%2520Mei%2520and%2520Boyu%2520Zhou%26entry.1292438233%3D%2520%2520Various%2520studies%2520on%2520perception-aware%2520planning%2520have%2520been%2520proposed%2520to%2520enhance%250Athe%2520state%2520estimation%2520accuracy%2520of%2520quadrotors%2520in%2520visually%2520degraded%2520environments.%250AHowever%252C%2520many%2520existing%2520methods%2520heavily%2520rely%2520on%2520prior%2520environmental%2520knowledge%250Aand%2520face%2520significant%2520limitations%2520in%2520previously%2520unknown%2520environments%2520with%2520sparse%250Alocalization%2520features%252C%2520which%2520greatly%2520limits%2520their%2520practical%2520application.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520perception-aware%2520planning%2520method%2520for%2520quadrotor%2520flight%250Ain%2520unknown%2520and%2520feature-limited%2520environments%2520that%2520properly%2520allocates%2520perception%250Aresources%2520among%2520environmental%2520information%2520during%2520navigation.%2520We%2520introduce%2520a%250Aviewpoint%2520transition%2520graph%2520that%2520allows%2520for%2520the%2520adaptive%2520selection%2520of%2520local%250Atarget%2520viewpoints%252C%2520which%2520guide%2520the%2520quadrotor%2520to%2520efficiently%2520navigate%2520to%2520the%250Agoal%2520while%2520maintaining%2520sufficient%2520localizability%2520and%2520without%2520being%2520trapped%2520in%250Afeature-limited%2520regions.%2520During%2520the%2520local%2520planning%252C%2520a%2520novel%2520yaw%2520trajectory%250Ageneration%2520method%2520that%2520simultaneously%2520considers%2520exploration%2520capability%2520and%250Alocalizability%2520is%2520presented.%2520It%2520constructs%2520a%2520localizable%2520corridor%2520via%2520feature%250Aco-visibility%2520evaluation%2520to%2520ensure%2520localization%2520robustness%2520in%2520a%2520computationally%250Aefficient%2520way.%2520Through%2520validations%2520conducted%2520in%2520both%2520simulation%2520and%2520real-world%250Aexperiments%252C%2520we%2520demonstrate%2520the%2520feasibility%2520and%2520real-time%2520performance%2520of%2520the%250Aproposed%2520method.%2520The%2520source%2520code%2520will%2520be%2520released%2520to%2520benefit%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception-aware%20Planning%20for%20Quadrotor%20Flight%20in%20Unknown%20and%0A%20%20Feature-limited%20Environments&entry.906535625=Chenxin%20Yu%20and%20Zihong%20Lu%20and%20Jie%20Mei%20and%20Boyu%20Zhou&entry.1292438233=%20%20Various%20studies%20on%20perception-aware%20planning%20have%20been%20proposed%20to%20enhance%0Athe%20state%20estimation%20accuracy%20of%20quadrotors%20in%20visually%20degraded%20environments.%0AHowever%2C%20many%20existing%20methods%20heavily%20rely%20on%20prior%20environmental%20knowledge%0Aand%20face%20significant%20limitations%20in%20previously%20unknown%20environments%20with%20sparse%0Alocalization%20features%2C%20which%20greatly%20limits%20their%20practical%20application.%20In%0Athis%20paper%2C%20we%20present%20a%20perception-aware%20planning%20method%20for%20quadrotor%20flight%0Ain%20unknown%20and%20feature-limited%20environments%20that%20properly%20allocates%20perception%0Aresources%20among%20environmental%20information%20during%20navigation.%20We%20introduce%20a%0Aviewpoint%20transition%20graph%20that%20allows%20for%20the%20adaptive%20selection%20of%20local%0Atarget%20viewpoints%2C%20which%20guide%20the%20quadrotor%20to%20efficiently%20navigate%20to%20the%0Agoal%20while%20maintaining%20sufficient%20localizability%20and%20without%20being%20trapped%20in%0Afeature-limited%20regions.%20During%20the%20local%20planning%2C%20a%20novel%20yaw%20trajectory%0Ageneration%20method%20that%20simultaneously%20considers%20exploration%20capability%20and%0Alocalizability%20is%20presented.%20It%20constructs%20a%20localizable%20corridor%20via%20feature%0Aco-visibility%20evaluation%20to%20ensure%20localization%20robustness%20in%20a%20computationally%0Aefficient%20way.%20Through%20validations%20conducted%20in%20both%20simulation%20and%20real-world%0Aexperiments%2C%20we%20demonstrate%20the%20feasibility%20and%20real-time%20performance%20of%20the%0Aproposed%20method.%20The%20source%20code%20will%20be%20released%20to%20benefit%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15273v2&entry.124074799=Read"},
{"title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model\n  Merging", "author": "Ryo Bertolissi and Jonas H\u00fcbotter and Ido Hakimi and Andreas Krause", "abstract": "  Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training.\n", "link": "http://arxiv.org/abs/2505.14136v2", "date": "2025-07-30", "relevancy": 2.3351, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4614}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Mixtures%20of%20Experts%3A%20Essentially%20Free%20Test-Time%20Training%20via%20Model%0A%20%20Merging&body=Title%3A%20Local%20Mixtures%20of%20Experts%3A%20Essentially%20Free%20Test-Time%20Training%20via%20Model%0A%20%20Merging%0AAuthor%3A%20Ryo%20Bertolissi%20and%20Jonas%20H%C3%BCbotter%20and%20Ido%20Hakimi%20and%20Andreas%20Krause%0AAbstract%3A%20%20%20Mixture%20of%20expert%20%28MoE%29%20models%20are%20a%20promising%20approach%20to%20increasing%20model%0Acapacity%20without%20increasing%20inference%20cost%2C%20and%20are%20core%20components%20of%20many%0Astate-of-the-art%20language%20models.%20However%2C%20current%20MoE%20models%20typically%20use%0Aonly%20few%20experts%20due%20to%20prohibitive%20training%20and%20inference%20cost.%20We%20propose%0ATest-Time%20Model%20Merging%20%28TTMM%29%20which%20scales%20the%20MoE%20paradigm%20to%20an%20order%20of%0Amagnitude%20more%20experts%20and%20uses%20model%20merging%20to%20avoid%20almost%20any%20test-time%0Aoverhead.%20We%20show%20that%20TTMM%20is%20an%20approximation%20of%20test-time%20training%20%28TTT%29%2C%0Awhich%20fine-tunes%20an%20expert%20model%20for%20each%20prediction%20task%2C%20i.e.%2C%20prompt.%20TTT%0Ahas%20recently%20been%20shown%20to%20significantly%20improve%20language%20models%2C%20but%20is%0Acomputationally%20expensive.%20We%20find%20that%20performance%20of%20TTMM%20improves%20with%20more%0Aexperts%20and%20approaches%20the%20performance%20of%20TTT.%20Moreover%2C%20we%20find%20that%20with%20a%201B%0Aparameter%20base%20model%2C%20TTMM%20is%20more%20than%20100x%20faster%20than%20TTT%20at%20test-time%20by%0Aamortizing%20the%20cost%20of%20TTT%20at%20train-time.%20Thus%2C%20TTMM%20offers%20a%20promising%0Acost-effective%20approach%20to%20scale%20test-time%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Mixtures%2520of%2520Experts%253A%2520Essentially%2520Free%2520Test-Time%2520Training%2520via%2520Model%250A%2520%2520Merging%26entry.906535625%3DRyo%2520Bertolissi%2520and%2520Jonas%2520H%25C3%25BCbotter%2520and%2520Ido%2520Hakimi%2520and%2520Andreas%2520Krause%26entry.1292438233%3D%2520%2520Mixture%2520of%2520expert%2520%2528MoE%2529%2520models%2520are%2520a%2520promising%2520approach%2520to%2520increasing%2520model%250Acapacity%2520without%2520increasing%2520inference%2520cost%252C%2520and%2520are%2520core%2520components%2520of%2520many%250Astate-of-the-art%2520language%2520models.%2520However%252C%2520current%2520MoE%2520models%2520typically%2520use%250Aonly%2520few%2520experts%2520due%2520to%2520prohibitive%2520training%2520and%2520inference%2520cost.%2520We%2520propose%250ATest-Time%2520Model%2520Merging%2520%2528TTMM%2529%2520which%2520scales%2520the%2520MoE%2520paradigm%2520to%2520an%2520order%2520of%250Amagnitude%2520more%2520experts%2520and%2520uses%2520model%2520merging%2520to%2520avoid%2520almost%2520any%2520test-time%250Aoverhead.%2520We%2520show%2520that%2520TTMM%2520is%2520an%2520approximation%2520of%2520test-time%2520training%2520%2528TTT%2529%252C%250Awhich%2520fine-tunes%2520an%2520expert%2520model%2520for%2520each%2520prediction%2520task%252C%2520i.e.%252C%2520prompt.%2520TTT%250Ahas%2520recently%2520been%2520shown%2520to%2520significantly%2520improve%2520language%2520models%252C%2520but%2520is%250Acomputationally%2520expensive.%2520We%2520find%2520that%2520performance%2520of%2520TTMM%2520improves%2520with%2520more%250Aexperts%2520and%2520approaches%2520the%2520performance%2520of%2520TTT.%2520Moreover%252C%2520we%2520find%2520that%2520with%2520a%25201B%250Aparameter%2520base%2520model%252C%2520TTMM%2520is%2520more%2520than%2520100x%2520faster%2520than%2520TTT%2520at%2520test-time%2520by%250Aamortizing%2520the%2520cost%2520of%2520TTT%2520at%2520train-time.%2520Thus%252C%2520TTMM%2520offers%2520a%2520promising%250Acost-effective%2520approach%2520to%2520scale%2520test-time%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Mixtures%20of%20Experts%3A%20Essentially%20Free%20Test-Time%20Training%20via%20Model%0A%20%20Merging&entry.906535625=Ryo%20Bertolissi%20and%20Jonas%20H%C3%BCbotter%20and%20Ido%20Hakimi%20and%20Andreas%20Krause&entry.1292438233=%20%20Mixture%20of%20expert%20%28MoE%29%20models%20are%20a%20promising%20approach%20to%20increasing%20model%0Acapacity%20without%20increasing%20inference%20cost%2C%20and%20are%20core%20components%20of%20many%0Astate-of-the-art%20language%20models.%20However%2C%20current%20MoE%20models%20typically%20use%0Aonly%20few%20experts%20due%20to%20prohibitive%20training%20and%20inference%20cost.%20We%20propose%0ATest-Time%20Model%20Merging%20%28TTMM%29%20which%20scales%20the%20MoE%20paradigm%20to%20an%20order%20of%0Amagnitude%20more%20experts%20and%20uses%20model%20merging%20to%20avoid%20almost%20any%20test-time%0Aoverhead.%20We%20show%20that%20TTMM%20is%20an%20approximation%20of%20test-time%20training%20%28TTT%29%2C%0Awhich%20fine-tunes%20an%20expert%20model%20for%20each%20prediction%20task%2C%20i.e.%2C%20prompt.%20TTT%0Ahas%20recently%20been%20shown%20to%20significantly%20improve%20language%20models%2C%20but%20is%0Acomputationally%20expensive.%20We%20find%20that%20performance%20of%20TTMM%20improves%20with%20more%0Aexperts%20and%20approaches%20the%20performance%20of%20TTT.%20Moreover%2C%20we%20find%20that%20with%20a%201B%0Aparameter%20base%20model%2C%20TTMM%20is%20more%20than%20100x%20faster%20than%20TTT%20at%20test-time%20by%0Aamortizing%20the%20cost%20of%20TTT%20at%20train-time.%20Thus%2C%20TTMM%20offers%20a%20promising%0Acost-effective%20approach%20to%20scale%20test-time%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14136v2&entry.124074799=Read"},
{"title": "Unsupervised Learning: Comparative Analysis of Clustering Techniques on\n  High-Dimensional Data", "author": "Vishnu Vardhan Baligodugula and Fathi Amsaad", "abstract": "  This paper presents a comprehensive comparative analysis of prominent\nclustering algorithms K-means, DBSCAN, and Spectral Clustering on\nhigh-dimensional datasets. We introduce a novel evaluation framework that\nassesses clustering performance across multiple dimensionality reduction\ntechniques (PCA, t-SNE, and UMAP) using diverse quantitative metrics.\nExperiments conducted on MNIST, Fashion-MNIST, and UCI HAR datasets reveal that\npreprocessing with UMAP consistently improves clustering quality across all\nalgorithms, with Spectral Clustering demonstrating superior performance on\ncomplex manifold structures. Our findings show that algorithm selection should\nbe guided by data characteristics, with Kmeans excelling in computational\nefficiency, DBSCAN in handling irregular clusters, and Spectral Clustering in\ncapturing complex relationships. This research contributes a systematic\napproach for evaluating and selecting clustering techniques for high\ndimensional data applications.\n", "link": "http://arxiv.org/abs/2503.23215v2", "date": "2025-07-30", "relevancy": 2.3279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%3A%20Comparative%20Analysis%20of%20Clustering%20Techniques%20on%0A%20%20High-Dimensional%20Data&body=Title%3A%20Unsupervised%20Learning%3A%20Comparative%20Analysis%20of%20Clustering%20Techniques%20on%0A%20%20High-Dimensional%20Data%0AAuthor%3A%20Vishnu%20Vardhan%20Baligodugula%20and%20Fathi%20Amsaad%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20comparative%20analysis%20of%20prominent%0Aclustering%20algorithms%20K-means%2C%20DBSCAN%2C%20and%20Spectral%20Clustering%20on%0Ahigh-dimensional%20datasets.%20We%20introduce%20a%20novel%20evaluation%20framework%20that%0Aassesses%20clustering%20performance%20across%20multiple%20dimensionality%20reduction%0Atechniques%20%28PCA%2C%20t-SNE%2C%20and%20UMAP%29%20using%20diverse%20quantitative%20metrics.%0AExperiments%20conducted%20on%20MNIST%2C%20Fashion-MNIST%2C%20and%20UCI%20HAR%20datasets%20reveal%20that%0Apreprocessing%20with%20UMAP%20consistently%20improves%20clustering%20quality%20across%20all%0Aalgorithms%2C%20with%20Spectral%20Clustering%20demonstrating%20superior%20performance%20on%0Acomplex%20manifold%20structures.%20Our%20findings%20show%20that%20algorithm%20selection%20should%0Abe%20guided%20by%20data%20characteristics%2C%20with%20Kmeans%20excelling%20in%20computational%0Aefficiency%2C%20DBSCAN%20in%20handling%20irregular%20clusters%2C%20and%20Spectral%20Clustering%20in%0Acapturing%20complex%20relationships.%20This%20research%20contributes%20a%20systematic%0Aapproach%20for%20evaluating%20and%20selecting%20clustering%20techniques%20for%20high%0Adimensional%20data%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Learning%253A%2520Comparative%2520Analysis%2520of%2520Clustering%2520Techniques%2520on%250A%2520%2520High-Dimensional%2520Data%26entry.906535625%3DVishnu%2520Vardhan%2520Baligodugula%2520and%2520Fathi%2520Amsaad%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520comparative%2520analysis%2520of%2520prominent%250Aclustering%2520algorithms%2520K-means%252C%2520DBSCAN%252C%2520and%2520Spectral%2520Clustering%2520on%250Ahigh-dimensional%2520datasets.%2520We%2520introduce%2520a%2520novel%2520evaluation%2520framework%2520that%250Aassesses%2520clustering%2520performance%2520across%2520multiple%2520dimensionality%2520reduction%250Atechniques%2520%2528PCA%252C%2520t-SNE%252C%2520and%2520UMAP%2529%2520using%2520diverse%2520quantitative%2520metrics.%250AExperiments%2520conducted%2520on%2520MNIST%252C%2520Fashion-MNIST%252C%2520and%2520UCI%2520HAR%2520datasets%2520reveal%2520that%250Apreprocessing%2520with%2520UMAP%2520consistently%2520improves%2520clustering%2520quality%2520across%2520all%250Aalgorithms%252C%2520with%2520Spectral%2520Clustering%2520demonstrating%2520superior%2520performance%2520on%250Acomplex%2520manifold%2520structures.%2520Our%2520findings%2520show%2520that%2520algorithm%2520selection%2520should%250Abe%2520guided%2520by%2520data%2520characteristics%252C%2520with%2520Kmeans%2520excelling%2520in%2520computational%250Aefficiency%252C%2520DBSCAN%2520in%2520handling%2520irregular%2520clusters%252C%2520and%2520Spectral%2520Clustering%2520in%250Acapturing%2520complex%2520relationships.%2520This%2520research%2520contributes%2520a%2520systematic%250Aapproach%2520for%2520evaluating%2520and%2520selecting%2520clustering%2520techniques%2520for%2520high%250Adimensional%2520data%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%3A%20Comparative%20Analysis%20of%20Clustering%20Techniques%20on%0A%20%20High-Dimensional%20Data&entry.906535625=Vishnu%20Vardhan%20Baligodugula%20and%20Fathi%20Amsaad&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20comparative%20analysis%20of%20prominent%0Aclustering%20algorithms%20K-means%2C%20DBSCAN%2C%20and%20Spectral%20Clustering%20on%0Ahigh-dimensional%20datasets.%20We%20introduce%20a%20novel%20evaluation%20framework%20that%0Aassesses%20clustering%20performance%20across%20multiple%20dimensionality%20reduction%0Atechniques%20%28PCA%2C%20t-SNE%2C%20and%20UMAP%29%20using%20diverse%20quantitative%20metrics.%0AExperiments%20conducted%20on%20MNIST%2C%20Fashion-MNIST%2C%20and%20UCI%20HAR%20datasets%20reveal%20that%0Apreprocessing%20with%20UMAP%20consistently%20improves%20clustering%20quality%20across%20all%0Aalgorithms%2C%20with%20Spectral%20Clustering%20demonstrating%20superior%20performance%20on%0Acomplex%20manifold%20structures.%20Our%20findings%20show%20that%20algorithm%20selection%20should%0Abe%20guided%20by%20data%20characteristics%2C%20with%20Kmeans%20excelling%20in%20computational%0Aefficiency%2C%20DBSCAN%20in%20handling%20irregular%20clusters%2C%20and%20Spectral%20Clustering%20in%0Acapturing%20complex%20relationships.%20This%20research%20contributes%20a%20systematic%0Aapproach%20for%20evaluating%20and%20selecting%20clustering%20techniques%20for%20high%0Adimensional%20data%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23215v2&entry.124074799=Read"},
{"title": "RocketStack: Level-aware deep recursive ensemble learning framework with\n  adaptive feature fusion and model pruning dynamics", "author": "\u00c7a\u011fatay Demirel", "abstract": "  Ensemble learning remains a cornerstone of machine learning, with stacking\nused to integrate predictions from multiple base learners through a meta-model.\nHowever, deep stacking remains rare, as most designs prioritize horizontal\ndiversity over recursive depth due to model complexity, feature redundancy, and\ncomputational burden. To address these challenges, RocketStack, a level-aware\nrecursive ensemble framework, is introduced and explored up to ten stacking\nlevels, extending beyond prior architectures. The framework incrementally\nprunes weaker learners at each level, enabling deeper stacking without\nexcessive complexity. To mitigate early performance saturation, mild Gaussian\nnoise is added to out-of-fold (OOF) scores before pruning, and compared against\nstrict OOF pruning. Further both per-level and periodic feature compressions\nare explored using attention-based selection, Simple, Fast, Efficient (SFE)\nfilter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),\nlinear-trend tests confirmed rising accuracy with depth in most variants, and\nthe top performing meta-model at each level increasingly outperformed the\nstrongest standalone ensemble. In the binary subset, periodic SFE with mild\nOOF-score randomization reached 97.08% at level 10, 5.14% above the\nstrict-pruning configuration and cut runtime by 10.5% relative to no\ncompression. In the multi-class subset, periodic attention selection reached\n98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing\nruntime by 56.1% and feature dimensionality by 74% compared to no compression.\nThese findings highlight mild randomization as an effective regularizer and\nperiodic compression as a stabilizer. Echoing the design of multistage rockets\nin aerospace (prune, compress, propel) RocketStack achieves deep recursive\nensembling with tractable complexity.\n", "link": "http://arxiv.org/abs/2506.16965v2", "date": "2025-07-30", "relevancy": 2.3266, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RocketStack%3A%20Level-aware%20deep%20recursive%20ensemble%20learning%20framework%20with%0A%20%20adaptive%20feature%20fusion%20and%20model%20pruning%20dynamics&body=Title%3A%20RocketStack%3A%20Level-aware%20deep%20recursive%20ensemble%20learning%20framework%20with%0A%20%20adaptive%20feature%20fusion%20and%20model%20pruning%20dynamics%0AAuthor%3A%20%C3%87a%C4%9Fatay%20Demirel%0AAbstract%3A%20%20%20Ensemble%20learning%20remains%20a%20cornerstone%20of%20machine%20learning%2C%20with%20stacking%0Aused%20to%20integrate%20predictions%20from%20multiple%20base%20learners%20through%20a%20meta-model.%0AHowever%2C%20deep%20stacking%20remains%20rare%2C%20as%20most%20designs%20prioritize%20horizontal%0Adiversity%20over%20recursive%20depth%20due%20to%20model%20complexity%2C%20feature%20redundancy%2C%20and%0Acomputational%20burden.%20To%20address%20these%20challenges%2C%20RocketStack%2C%20a%20level-aware%0Arecursive%20ensemble%20framework%2C%20is%20introduced%20and%20explored%20up%20to%20ten%20stacking%0Alevels%2C%20extending%20beyond%20prior%20architectures.%20The%20framework%20incrementally%0Aprunes%20weaker%20learners%20at%20each%20level%2C%20enabling%20deeper%20stacking%20without%0Aexcessive%20complexity.%20To%20mitigate%20early%20performance%20saturation%2C%20mild%20Gaussian%0Anoise%20is%20added%20to%20out-of-fold%20%28OOF%29%20scores%20before%20pruning%2C%20and%20compared%20against%0Astrict%20OOF%20pruning.%20Further%20both%20per-level%20and%20periodic%20feature%20compressions%0Aare%20explored%20using%20attention-based%20selection%2C%20Simple%2C%20Fast%2C%20Efficient%20%28SFE%29%0Afilter%2C%20and%20autoencoders.%20Across%2033%20datasets%20%2823%20binary%2C%2010%20multi-class%29%2C%0Alinear-trend%20tests%20confirmed%20rising%20accuracy%20with%20depth%20in%20most%20variants%2C%20and%0Athe%20top%20performing%20meta-model%20at%20each%20level%20increasingly%20outperformed%20the%0Astrongest%20standalone%20ensemble.%20In%20the%20binary%20subset%2C%20periodic%20SFE%20with%20mild%0AOOF-score%20randomization%20reached%2097.08%25%20at%20level%2010%2C%205.14%25%20above%20the%0Astrict-pruning%20configuration%20and%20cut%20runtime%20by%2010.5%25%20relative%20to%20no%0Acompression.%20In%20the%20multi-class%20subset%2C%20periodic%20attention%20selection%20reached%0A98.60%25%20at%20level%2010%2C%20exceeding%20the%20strongest%20baseline%20by%206.11%25%2C%20while%20reducing%0Aruntime%20by%2056.1%25%20and%20feature%20dimensionality%20by%2074%25%20compared%20to%20no%20compression.%0AThese%20findings%20highlight%20mild%20randomization%20as%20an%20effective%20regularizer%20and%0Aperiodic%20compression%20as%20a%20stabilizer.%20Echoing%20the%20design%20of%20multistage%20rockets%0Ain%20aerospace%20%28prune%2C%20compress%2C%20propel%29%20RocketStack%20achieves%20deep%20recursive%0Aensembling%20with%20tractable%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRocketStack%253A%2520Level-aware%2520deep%2520recursive%2520ensemble%2520learning%2520framework%2520with%250A%2520%2520adaptive%2520feature%2520fusion%2520and%2520model%2520pruning%2520dynamics%26entry.906535625%3D%25C3%2587a%25C4%259Fatay%2520Demirel%26entry.1292438233%3D%2520%2520Ensemble%2520learning%2520remains%2520a%2520cornerstone%2520of%2520machine%2520learning%252C%2520with%2520stacking%250Aused%2520to%2520integrate%2520predictions%2520from%2520multiple%2520base%2520learners%2520through%2520a%2520meta-model.%250AHowever%252C%2520deep%2520stacking%2520remains%2520rare%252C%2520as%2520most%2520designs%2520prioritize%2520horizontal%250Adiversity%2520over%2520recursive%2520depth%2520due%2520to%2520model%2520complexity%252C%2520feature%2520redundancy%252C%2520and%250Acomputational%2520burden.%2520To%2520address%2520these%2520challenges%252C%2520RocketStack%252C%2520a%2520level-aware%250Arecursive%2520ensemble%2520framework%252C%2520is%2520introduced%2520and%2520explored%2520up%2520to%2520ten%2520stacking%250Alevels%252C%2520extending%2520beyond%2520prior%2520architectures.%2520The%2520framework%2520incrementally%250Aprunes%2520weaker%2520learners%2520at%2520each%2520level%252C%2520enabling%2520deeper%2520stacking%2520without%250Aexcessive%2520complexity.%2520To%2520mitigate%2520early%2520performance%2520saturation%252C%2520mild%2520Gaussian%250Anoise%2520is%2520added%2520to%2520out-of-fold%2520%2528OOF%2529%2520scores%2520before%2520pruning%252C%2520and%2520compared%2520against%250Astrict%2520OOF%2520pruning.%2520Further%2520both%2520per-level%2520and%2520periodic%2520feature%2520compressions%250Aare%2520explored%2520using%2520attention-based%2520selection%252C%2520Simple%252C%2520Fast%252C%2520Efficient%2520%2528SFE%2529%250Afilter%252C%2520and%2520autoencoders.%2520Across%252033%2520datasets%2520%252823%2520binary%252C%252010%2520multi-class%2529%252C%250Alinear-trend%2520tests%2520confirmed%2520rising%2520accuracy%2520with%2520depth%2520in%2520most%2520variants%252C%2520and%250Athe%2520top%2520performing%2520meta-model%2520at%2520each%2520level%2520increasingly%2520outperformed%2520the%250Astrongest%2520standalone%2520ensemble.%2520In%2520the%2520binary%2520subset%252C%2520periodic%2520SFE%2520with%2520mild%250AOOF-score%2520randomization%2520reached%252097.08%2525%2520at%2520level%252010%252C%25205.14%2525%2520above%2520the%250Astrict-pruning%2520configuration%2520and%2520cut%2520runtime%2520by%252010.5%2525%2520relative%2520to%2520no%250Acompression.%2520In%2520the%2520multi-class%2520subset%252C%2520periodic%2520attention%2520selection%2520reached%250A98.60%2525%2520at%2520level%252010%252C%2520exceeding%2520the%2520strongest%2520baseline%2520by%25206.11%2525%252C%2520while%2520reducing%250Aruntime%2520by%252056.1%2525%2520and%2520feature%2520dimensionality%2520by%252074%2525%2520compared%2520to%2520no%2520compression.%250AThese%2520findings%2520highlight%2520mild%2520randomization%2520as%2520an%2520effective%2520regularizer%2520and%250Aperiodic%2520compression%2520as%2520a%2520stabilizer.%2520Echoing%2520the%2520design%2520of%2520multistage%2520rockets%250Ain%2520aerospace%2520%2528prune%252C%2520compress%252C%2520propel%2529%2520RocketStack%2520achieves%2520deep%2520recursive%250Aensembling%2520with%2520tractable%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RocketStack%3A%20Level-aware%20deep%20recursive%20ensemble%20learning%20framework%20with%0A%20%20adaptive%20feature%20fusion%20and%20model%20pruning%20dynamics&entry.906535625=%C3%87a%C4%9Fatay%20Demirel&entry.1292438233=%20%20Ensemble%20learning%20remains%20a%20cornerstone%20of%20machine%20learning%2C%20with%20stacking%0Aused%20to%20integrate%20predictions%20from%20multiple%20base%20learners%20through%20a%20meta-model.%0AHowever%2C%20deep%20stacking%20remains%20rare%2C%20as%20most%20designs%20prioritize%20horizontal%0Adiversity%20over%20recursive%20depth%20due%20to%20model%20complexity%2C%20feature%20redundancy%2C%20and%0Acomputational%20burden.%20To%20address%20these%20challenges%2C%20RocketStack%2C%20a%20level-aware%0Arecursive%20ensemble%20framework%2C%20is%20introduced%20and%20explored%20up%20to%20ten%20stacking%0Alevels%2C%20extending%20beyond%20prior%20architectures.%20The%20framework%20incrementally%0Aprunes%20weaker%20learners%20at%20each%20level%2C%20enabling%20deeper%20stacking%20without%0Aexcessive%20complexity.%20To%20mitigate%20early%20performance%20saturation%2C%20mild%20Gaussian%0Anoise%20is%20added%20to%20out-of-fold%20%28OOF%29%20scores%20before%20pruning%2C%20and%20compared%20against%0Astrict%20OOF%20pruning.%20Further%20both%20per-level%20and%20periodic%20feature%20compressions%0Aare%20explored%20using%20attention-based%20selection%2C%20Simple%2C%20Fast%2C%20Efficient%20%28SFE%29%0Afilter%2C%20and%20autoencoders.%20Across%2033%20datasets%20%2823%20binary%2C%2010%20multi-class%29%2C%0Alinear-trend%20tests%20confirmed%20rising%20accuracy%20with%20depth%20in%20most%20variants%2C%20and%0Athe%20top%20performing%20meta-model%20at%20each%20level%20increasingly%20outperformed%20the%0Astrongest%20standalone%20ensemble.%20In%20the%20binary%20subset%2C%20periodic%20SFE%20with%20mild%0AOOF-score%20randomization%20reached%2097.08%25%20at%20level%2010%2C%205.14%25%20above%20the%0Astrict-pruning%20configuration%20and%20cut%20runtime%20by%2010.5%25%20relative%20to%20no%0Acompression.%20In%20the%20multi-class%20subset%2C%20periodic%20attention%20selection%20reached%0A98.60%25%20at%20level%2010%2C%20exceeding%20the%20strongest%20baseline%20by%206.11%25%2C%20while%20reducing%0Aruntime%20by%2056.1%25%20and%20feature%20dimensionality%20by%2074%25%20compared%20to%20no%20compression.%0AThese%20findings%20highlight%20mild%20randomization%20as%20an%20effective%20regularizer%20and%0Aperiodic%20compression%20as%20a%20stabilizer.%20Echoing%20the%20design%20of%20multistage%20rockets%0Ain%20aerospace%20%28prune%2C%20compress%2C%20propel%29%20RocketStack%20achieves%20deep%20recursive%0Aensembling%20with%20tractable%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16965v2&entry.124074799=Read"},
{"title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents", "author": "Yilei Jiang and Yaozhi Zheng and Yuxuan Wan and Jiaming Han and Qunzhong Wang and Michael R. Lyu and Xiangyu Yue", "abstract": "  Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.\n", "link": "http://arxiv.org/abs/2507.22827v1", "date": "2025-07-30", "relevancy": 2.2906, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6156}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.546}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScreenCoder%3A%20Advancing%20Visual-to-Code%20Generation%20for%20Front-End%0A%20%20Automation%20via%20Modular%20Multimodal%20Agents&body=Title%3A%20ScreenCoder%3A%20Advancing%20Visual-to-Code%20Generation%20for%20Front-End%0A%20%20Automation%20via%20Modular%20Multimodal%20Agents%0AAuthor%3A%20Yilei%20Jiang%20and%20Yaozhi%20Zheng%20and%20Yuxuan%20Wan%20and%20Jiaming%20Han%20and%20Qunzhong%20Wang%20and%20Michael%20R.%20Lyu%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Automating%20the%20transformation%20of%20user%20interface%20%28UI%29%20designs%20into%20front-end%0Acode%20holds%20significant%20promise%20for%20accelerating%20software%20development%20and%0Ademocratizing%20design%20workflows.%20While%20recent%20large%20language%20models%20%28LLMs%29%20have%0Ademonstrated%20progress%20in%20text-to-code%20generation%2C%20many%20existing%20approaches%20rely%0Asolely%20on%20natural%20language%20prompts%2C%20limiting%20their%20effectiveness%20in%20capturing%0Aspatial%20layout%20and%20visual%20design%20intent.%20In%20contrast%2C%20UI%20development%20in%0Apractice%20is%20inherently%20multimodal%2C%20often%20starting%20from%20visual%20sketches%20or%0Amockups.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20modular%20multi-agent%20framework%20that%0Aperforms%20UI-to-code%20generation%20in%20three%20interpretable%20stages%3A%20grounding%2C%0Aplanning%2C%20and%20generation.%20The%20grounding%20agent%20uses%20a%20vision-language%20model%20to%0Adetect%20and%20label%20UI%20components%2C%20the%20planning%20agent%20constructs%20a%20hierarchical%0Alayout%20using%20front-end%20engineering%20priors%2C%20and%20the%20generation%20agent%20produces%0AHTML/CSS%20code%20via%20adaptive%20prompt-based%20synthesis.%20This%20design%20improves%0Arobustness%2C%20interpretability%2C%20and%20fidelity%20over%20end-to-end%20black-box%20methods.%0AFurthermore%2C%20we%20extend%20the%20framework%20into%20a%20scalable%20data%20engine%20that%0Aautomatically%20produces%20large-scale%20image-code%20pairs.%20Using%20these%20synthetic%0Aexamples%2C%20we%20fine-tune%20and%20reinforce%20an%20open-source%20VLM%2C%20yielding%20notable%20gains%0Ain%20UI%20understanding%20and%20code%20quality.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20state-of-the-art%20performance%20in%20layout%20accuracy%2C%0Astructural%20coherence%2C%20and%20code%20correctness.%20Our%20code%20is%20made%20publicly%20available%0Aat%20https%3A//github.com/leigest519/ScreenCoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScreenCoder%253A%2520Advancing%2520Visual-to-Code%2520Generation%2520for%2520Front-End%250A%2520%2520Automation%2520via%2520Modular%2520Multimodal%2520Agents%26entry.906535625%3DYilei%2520Jiang%2520and%2520Yaozhi%2520Zheng%2520and%2520Yuxuan%2520Wan%2520and%2520Jiaming%2520Han%2520and%2520Qunzhong%2520Wang%2520and%2520Michael%2520R.%2520Lyu%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Automating%2520the%2520transformation%2520of%2520user%2520interface%2520%2528UI%2529%2520designs%2520into%2520front-end%250Acode%2520holds%2520significant%2520promise%2520for%2520accelerating%2520software%2520development%2520and%250Ademocratizing%2520design%2520workflows.%2520While%2520recent%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Ademonstrated%2520progress%2520in%2520text-to-code%2520generation%252C%2520many%2520existing%2520approaches%2520rely%250Asolely%2520on%2520natural%2520language%2520prompts%252C%2520limiting%2520their%2520effectiveness%2520in%2520capturing%250Aspatial%2520layout%2520and%2520visual%2520design%2520intent.%2520In%2520contrast%252C%2520UI%2520development%2520in%250Apractice%2520is%2520inherently%2520multimodal%252C%2520often%2520starting%2520from%2520visual%2520sketches%2520or%250Amockups.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%2520modular%2520multi-agent%2520framework%2520that%250Aperforms%2520UI-to-code%2520generation%2520in%2520three%2520interpretable%2520stages%253A%2520grounding%252C%250Aplanning%252C%2520and%2520generation.%2520The%2520grounding%2520agent%2520uses%2520a%2520vision-language%2520model%2520to%250Adetect%2520and%2520label%2520UI%2520components%252C%2520the%2520planning%2520agent%2520constructs%2520a%2520hierarchical%250Alayout%2520using%2520front-end%2520engineering%2520priors%252C%2520and%2520the%2520generation%2520agent%2520produces%250AHTML/CSS%2520code%2520via%2520adaptive%2520prompt-based%2520synthesis.%2520This%2520design%2520improves%250Arobustness%252C%2520interpretability%252C%2520and%2520fidelity%2520over%2520end-to-end%2520black-box%2520methods.%250AFurthermore%252C%2520we%2520extend%2520the%2520framework%2520into%2520a%2520scalable%2520data%2520engine%2520that%250Aautomatically%2520produces%2520large-scale%2520image-code%2520pairs.%2520Using%2520these%2520synthetic%250Aexamples%252C%2520we%2520fine-tune%2520and%2520reinforce%2520an%2520open-source%2520VLM%252C%2520yielding%2520notable%2520gains%250Ain%2520UI%2520understanding%2520and%2520code%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520approach%2520achieves%2520state-of-the-art%2520performance%2520in%2520layout%2520accuracy%252C%250Astructural%2520coherence%252C%2520and%2520code%2520correctness.%2520Our%2520code%2520is%2520made%2520publicly%2520available%250Aat%2520https%253A//github.com/leigest519/ScreenCoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScreenCoder%3A%20Advancing%20Visual-to-Code%20Generation%20for%20Front-End%0A%20%20Automation%20via%20Modular%20Multimodal%20Agents&entry.906535625=Yilei%20Jiang%20and%20Yaozhi%20Zheng%20and%20Yuxuan%20Wan%20and%20Jiaming%20Han%20and%20Qunzhong%20Wang%20and%20Michael%20R.%20Lyu%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Automating%20the%20transformation%20of%20user%20interface%20%28UI%29%20designs%20into%20front-end%0Acode%20holds%20significant%20promise%20for%20accelerating%20software%20development%20and%0Ademocratizing%20design%20workflows.%20While%20recent%20large%20language%20models%20%28LLMs%29%20have%0Ademonstrated%20progress%20in%20text-to-code%20generation%2C%20many%20existing%20approaches%20rely%0Asolely%20on%20natural%20language%20prompts%2C%20limiting%20their%20effectiveness%20in%20capturing%0Aspatial%20layout%20and%20visual%20design%20intent.%20In%20contrast%2C%20UI%20development%20in%0Apractice%20is%20inherently%20multimodal%2C%20often%20starting%20from%20visual%20sketches%20or%0Amockups.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20modular%20multi-agent%20framework%20that%0Aperforms%20UI-to-code%20generation%20in%20three%20interpretable%20stages%3A%20grounding%2C%0Aplanning%2C%20and%20generation.%20The%20grounding%20agent%20uses%20a%20vision-language%20model%20to%0Adetect%20and%20label%20UI%20components%2C%20the%20planning%20agent%20constructs%20a%20hierarchical%0Alayout%20using%20front-end%20engineering%20priors%2C%20and%20the%20generation%20agent%20produces%0AHTML/CSS%20code%20via%20adaptive%20prompt-based%20synthesis.%20This%20design%20improves%0Arobustness%2C%20interpretability%2C%20and%20fidelity%20over%20end-to-end%20black-box%20methods.%0AFurthermore%2C%20we%20extend%20the%20framework%20into%20a%20scalable%20data%20engine%20that%0Aautomatically%20produces%20large-scale%20image-code%20pairs.%20Using%20these%20synthetic%0Aexamples%2C%20we%20fine-tune%20and%20reinforce%20an%20open-source%20VLM%2C%20yielding%20notable%20gains%0Ain%20UI%20understanding%20and%20code%20quality.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20state-of-the-art%20performance%20in%20layout%20accuracy%2C%0Astructural%20coherence%2C%20and%20code%20correctness.%20Our%20code%20is%20made%20publicly%20available%0Aat%20https%3A//github.com/leigest519/ScreenCoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22827v1&entry.124074799=Read"},
{"title": "Segment Anything for Video: A Comprehensive Review of Video Object\n  Segmentation and Tracking from Past to Future", "author": "Guoping Xu and Jayaram K. Udupa and Yajun Yu and Hua-Chieh Shao and Songlin Zhao and Wei Liu and You Zhang", "abstract": "  Video Object Segmentation and Tracking (VOST) presents a complex yet critical\nchallenge in computer vision, requiring robust integration of segmentation and\ntracking across temporally dynamic frames. Traditional methods have struggled\nwith domain generalization, temporal consistency, and computational efficiency.\nThe emergence of foundation models like the Segment Anything Model (SAM) and\nits successor, SAM2, has introduced a paradigm shift, enabling prompt-driven\nsegmentation with strong generalization capabilities. Building upon these\nadvances, this survey provides a comprehensive review of SAM/SAM2-based methods\nfor VOST, structured along three temporal dimensions: past, present, and\nfuture. We examine strategies for retaining and updating historical information\n(past), approaches for extracting and optimizing discriminative features from\nthe current frame (present), and motion prediction and trajectory estimation\nmechanisms for anticipating object dynamics in subsequent frames (future). In\ndoing so, we highlight the evolution from early memory-based architectures to\nthe streaming memory and real-time segmentation capabilities of SAM2. We also\ndiscuss recent innovations such as motion-aware memory selection and\ntrajectory-guided prompting, which aim to enhance both accuracy and efficiency.\nFinally, we identify remaining challenges including memory redundancy, error\naccumulation, and prompt inefficiency, and suggest promising directions for\nfuture research. This survey offers a timely and structured overview of the\nfield, aiming to guide researchers and practitioners in advancing the state of\nVOST through the lens of foundation models.\n", "link": "http://arxiv.org/abs/2507.22792v1", "date": "2025-07-30", "relevancy": 2.2882, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20for%20Video%3A%20A%20Comprehensive%20Review%20of%20Video%20Object%0A%20%20Segmentation%20and%20Tracking%20from%20Past%20to%20Future&body=Title%3A%20Segment%20Anything%20for%20Video%3A%20A%20Comprehensive%20Review%20of%20Video%20Object%0A%20%20Segmentation%20and%20Tracking%20from%20Past%20to%20Future%0AAuthor%3A%20Guoping%20Xu%20and%20Jayaram%20K.%20Udupa%20and%20Yajun%20Yu%20and%20Hua-Chieh%20Shao%20and%20Songlin%20Zhao%20and%20Wei%20Liu%20and%20You%20Zhang%0AAbstract%3A%20%20%20Video%20Object%20Segmentation%20and%20Tracking%20%28VOST%29%20presents%20a%20complex%20yet%20critical%0Achallenge%20in%20computer%20vision%2C%20requiring%20robust%20integration%20of%20segmentation%20and%0Atracking%20across%20temporally%20dynamic%20frames.%20Traditional%20methods%20have%20struggled%0Awith%20domain%20generalization%2C%20temporal%20consistency%2C%20and%20computational%20efficiency.%0AThe%20emergence%20of%20foundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%0Aits%20successor%2C%20SAM2%2C%20has%20introduced%20a%20paradigm%20shift%2C%20enabling%20prompt-driven%0Asegmentation%20with%20strong%20generalization%20capabilities.%20Building%20upon%20these%0Aadvances%2C%20this%20survey%20provides%20a%20comprehensive%20review%20of%20SAM/SAM2-based%20methods%0Afor%20VOST%2C%20structured%20along%20three%20temporal%20dimensions%3A%20past%2C%20present%2C%20and%0Afuture.%20We%20examine%20strategies%20for%20retaining%20and%20updating%20historical%20information%0A%28past%29%2C%20approaches%20for%20extracting%20and%20optimizing%20discriminative%20features%20from%0Athe%20current%20frame%20%28present%29%2C%20and%20motion%20prediction%20and%20trajectory%20estimation%0Amechanisms%20for%20anticipating%20object%20dynamics%20in%20subsequent%20frames%20%28future%29.%20In%0Adoing%20so%2C%20we%20highlight%20the%20evolution%20from%20early%20memory-based%20architectures%20to%0Athe%20streaming%20memory%20and%20real-time%20segmentation%20capabilities%20of%20SAM2.%20We%20also%0Adiscuss%20recent%20innovations%20such%20as%20motion-aware%20memory%20selection%20and%0Atrajectory-guided%20prompting%2C%20which%20aim%20to%20enhance%20both%20accuracy%20and%20efficiency.%0AFinally%2C%20we%20identify%20remaining%20challenges%20including%20memory%20redundancy%2C%20error%0Aaccumulation%2C%20and%20prompt%20inefficiency%2C%20and%20suggest%20promising%20directions%20for%0Afuture%20research.%20This%20survey%20offers%20a%20timely%20and%20structured%20overview%20of%20the%0Afield%2C%20aiming%20to%20guide%20researchers%20and%20practitioners%20in%20advancing%20the%20state%20of%0AVOST%20through%20the%20lens%20of%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520for%2520Video%253A%2520A%2520Comprehensive%2520Review%2520of%2520Video%2520Object%250A%2520%2520Segmentation%2520and%2520Tracking%2520from%2520Past%2520to%2520Future%26entry.906535625%3DGuoping%2520Xu%2520and%2520Jayaram%2520K.%2520Udupa%2520and%2520Yajun%2520Yu%2520and%2520Hua-Chieh%2520Shao%2520and%2520Songlin%2520Zhao%2520and%2520Wei%2520Liu%2520and%2520You%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520Object%2520Segmentation%2520and%2520Tracking%2520%2528VOST%2529%2520presents%2520a%2520complex%2520yet%2520critical%250Achallenge%2520in%2520computer%2520vision%252C%2520requiring%2520robust%2520integration%2520of%2520segmentation%2520and%250Atracking%2520across%2520temporally%2520dynamic%2520frames.%2520Traditional%2520methods%2520have%2520struggled%250Awith%2520domain%2520generalization%252C%2520temporal%2520consistency%252C%2520and%2520computational%2520efficiency.%250AThe%2520emergence%2520of%2520foundation%2520models%2520like%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%250Aits%2520successor%252C%2520SAM2%252C%2520has%2520introduced%2520a%2520paradigm%2520shift%252C%2520enabling%2520prompt-driven%250Asegmentation%2520with%2520strong%2520generalization%2520capabilities.%2520Building%2520upon%2520these%250Aadvances%252C%2520this%2520survey%2520provides%2520a%2520comprehensive%2520review%2520of%2520SAM/SAM2-based%2520methods%250Afor%2520VOST%252C%2520structured%2520along%2520three%2520temporal%2520dimensions%253A%2520past%252C%2520present%252C%2520and%250Afuture.%2520We%2520examine%2520strategies%2520for%2520retaining%2520and%2520updating%2520historical%2520information%250A%2528past%2529%252C%2520approaches%2520for%2520extracting%2520and%2520optimizing%2520discriminative%2520features%2520from%250Athe%2520current%2520frame%2520%2528present%2529%252C%2520and%2520motion%2520prediction%2520and%2520trajectory%2520estimation%250Amechanisms%2520for%2520anticipating%2520object%2520dynamics%2520in%2520subsequent%2520frames%2520%2528future%2529.%2520In%250Adoing%2520so%252C%2520we%2520highlight%2520the%2520evolution%2520from%2520early%2520memory-based%2520architectures%2520to%250Athe%2520streaming%2520memory%2520and%2520real-time%2520segmentation%2520capabilities%2520of%2520SAM2.%2520We%2520also%250Adiscuss%2520recent%2520innovations%2520such%2520as%2520motion-aware%2520memory%2520selection%2520and%250Atrajectory-guided%2520prompting%252C%2520which%2520aim%2520to%2520enhance%2520both%2520accuracy%2520and%2520efficiency.%250AFinally%252C%2520we%2520identify%2520remaining%2520challenges%2520including%2520memory%2520redundancy%252C%2520error%250Aaccumulation%252C%2520and%2520prompt%2520inefficiency%252C%2520and%2520suggest%2520promising%2520directions%2520for%250Afuture%2520research.%2520This%2520survey%2520offers%2520a%2520timely%2520and%2520structured%2520overview%2520of%2520the%250Afield%252C%2520aiming%2520to%2520guide%2520researchers%2520and%2520practitioners%2520in%2520advancing%2520the%2520state%2520of%250AVOST%2520through%2520the%2520lens%2520of%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20for%20Video%3A%20A%20Comprehensive%20Review%20of%20Video%20Object%0A%20%20Segmentation%20and%20Tracking%20from%20Past%20to%20Future&entry.906535625=Guoping%20Xu%20and%20Jayaram%20K.%20Udupa%20and%20Yajun%20Yu%20and%20Hua-Chieh%20Shao%20and%20Songlin%20Zhao%20and%20Wei%20Liu%20and%20You%20Zhang&entry.1292438233=%20%20Video%20Object%20Segmentation%20and%20Tracking%20%28VOST%29%20presents%20a%20complex%20yet%20critical%0Achallenge%20in%20computer%20vision%2C%20requiring%20robust%20integration%20of%20segmentation%20and%0Atracking%20across%20temporally%20dynamic%20frames.%20Traditional%20methods%20have%20struggled%0Awith%20domain%20generalization%2C%20temporal%20consistency%2C%20and%20computational%20efficiency.%0AThe%20emergence%20of%20foundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%0Aits%20successor%2C%20SAM2%2C%20has%20introduced%20a%20paradigm%20shift%2C%20enabling%20prompt-driven%0Asegmentation%20with%20strong%20generalization%20capabilities.%20Building%20upon%20these%0Aadvances%2C%20this%20survey%20provides%20a%20comprehensive%20review%20of%20SAM/SAM2-based%20methods%0Afor%20VOST%2C%20structured%20along%20three%20temporal%20dimensions%3A%20past%2C%20present%2C%20and%0Afuture.%20We%20examine%20strategies%20for%20retaining%20and%20updating%20historical%20information%0A%28past%29%2C%20approaches%20for%20extracting%20and%20optimizing%20discriminative%20features%20from%0Athe%20current%20frame%20%28present%29%2C%20and%20motion%20prediction%20and%20trajectory%20estimation%0Amechanisms%20for%20anticipating%20object%20dynamics%20in%20subsequent%20frames%20%28future%29.%20In%0Adoing%20so%2C%20we%20highlight%20the%20evolution%20from%20early%20memory-based%20architectures%20to%0Athe%20streaming%20memory%20and%20real-time%20segmentation%20capabilities%20of%20SAM2.%20We%20also%0Adiscuss%20recent%20innovations%20such%20as%20motion-aware%20memory%20selection%20and%0Atrajectory-guided%20prompting%2C%20which%20aim%20to%20enhance%20both%20accuracy%20and%20efficiency.%0AFinally%2C%20we%20identify%20remaining%20challenges%20including%20memory%20redundancy%2C%20error%0Aaccumulation%2C%20and%20prompt%20inefficiency%2C%20and%20suggest%20promising%20directions%20for%0Afuture%20research.%20This%20survey%20offers%20a%20timely%20and%20structured%20overview%20of%20the%0Afield%2C%20aiming%20to%20guide%20researchers%20and%20practitioners%20in%20advancing%20the%20state%20of%0AVOST%20through%20the%20lens%20of%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22792v1&entry.124074799=Read"},
{"title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models", "author": "Rawan Al-Matham and Kareem Darwish and Raghad Al-Rasheed and Waad Alshammari and Muneera Alhoshan and Amal Almazrua and Asma Al Wazrah and Mais Alheraki and Firoj Alam and Preslav Nakov and Norah Alzahrani and Eman alBilali and Nizar Habash and Abdelrahman El-Sheikh and Muhammad Elmallah and Haonan Li and Hamdy Mubarak and Mohamed Anwar and Zaid Alyafeai and Ahmed Abdelali and Nora Altwairesh and Maram Hasanain and Abdulmohsen Al Thubaity and Shady Shehata and Bashar Alhafni and Injy Hamed and Go Inoue and Khalid Elmadani and Ossama Obeid and Fatima Haouari and Tamer Elsayed and Emad Alghamdi and Khalid Almubarak and Saied Alshahrani and Ola Aljarrah and Safa Alajlan and Areej Alshaqarawi and Maryam Alshihri and Sultana Alghurabi and Atikah Alzeghayer and Afrah Altamimi and Abdullah Alfaifi and Abdulrahman AlOsaimy", "abstract": "  The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities.\n", "link": "http://arxiv.org/abs/2507.22603v1", "date": "2025-07-30", "relevancy": 2.2859, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BALSAM%3A%20A%20Platform%20for%20Benchmarking%20Arabic%20Large%20Language%20Models&body=Title%3A%20BALSAM%3A%20A%20Platform%20for%20Benchmarking%20Arabic%20Large%20Language%20Models%0AAuthor%3A%20Rawan%20Al-Matham%20and%20Kareem%20Darwish%20and%20Raghad%20Al-Rasheed%20and%20Waad%20Alshammari%20and%20Muneera%20Alhoshan%20and%20Amal%20Almazrua%20and%20Asma%20Al%20Wazrah%20and%20Mais%20Alheraki%20and%20Firoj%20Alam%20and%20Preslav%20Nakov%20and%20Norah%20Alzahrani%20and%20Eman%20alBilali%20and%20Nizar%20Habash%20and%20Abdelrahman%20El-Sheikh%20and%20Muhammad%20Elmallah%20and%20Haonan%20Li%20and%20Hamdy%20Mubarak%20and%20Mohamed%20Anwar%20and%20Zaid%20Alyafeai%20and%20Ahmed%20Abdelali%20and%20Nora%20Altwairesh%20and%20Maram%20Hasanain%20and%20Abdulmohsen%20Al%20Thubaity%20and%20Shady%20Shehata%20and%20Bashar%20Alhafni%20and%20Injy%20Hamed%20and%20Go%20Inoue%20and%20Khalid%20Elmadani%20and%20Ossama%20Obeid%20and%20Fatima%20Haouari%20and%20Tamer%20Elsayed%20and%20Emad%20Alghamdi%20and%20Khalid%20Almubarak%20and%20Saied%20Alshahrani%20and%20Ola%20Aljarrah%20and%20Safa%20Alajlan%20and%20Areej%20Alshaqarawi%20and%20Maryam%20Alshihri%20and%20Sultana%20Alghurabi%20and%20Atikah%20Alzeghayer%20and%20Afrah%20Altamimi%20and%20Abdullah%20Alfaifi%20and%20Abdulrahman%20AlOsaimy%0AAbstract%3A%20%20%20The%20impressive%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20English%20has%20not%0Abeen%20matched%20across%20all%20languages.%20In%20particular%2C%20LLM%20performance%20in%20Arabic%0Alags%20behind%2C%20due%20to%20data%20scarcity%2C%20linguistic%20diversity%20of%20Arabic%20and%20its%0Adialects%2C%20morphological%20complexity%2C%20etc.%20Progress%20is%20further%20hindered%20by%20the%0Aquality%20of%20Arabic%20benchmarks%2C%20which%20typically%20rely%20on%20static%2C%20publicly%0Aavailable%20data%2C%20lack%20comprehensive%20task%20coverage%2C%20or%20do%20not%20provide%20dedicated%0Aplatforms%20with%20blind%20test%20sets.%20This%20makes%20it%20challenging%20to%20measure%20actual%0Aprogress%20and%20to%20mitigate%20data%20contamination.%20Here%2C%20we%20aim%20to%20bridge%20these%20gaps.%0AIn%20particular%2C%20we%20introduce%20BALSAM%2C%20a%20comprehensive%2C%20community-driven%20benchmark%0Aaimed%20at%20advancing%20Arabic%20LLM%20development%20and%20evaluation.%20It%20includes%2078%20NLP%0Atasks%20from%2014%20broad%20categories%2C%20with%2052K%20examples%20divided%20into%2037K%20test%20and%2015K%0Adevelopment%2C%20and%20a%20centralized%2C%20transparent%20platform%20for%20blind%20evaluation.%20We%0Aenvision%20BALSAM%20as%20a%20unifying%20platform%20that%20sets%20standards%20and%20promotes%0Acollaborative%20research%20to%20advance%20Arabic%20LLM%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBALSAM%253A%2520A%2520Platform%2520for%2520Benchmarking%2520Arabic%2520Large%2520Language%2520Models%26entry.906535625%3DRawan%2520Al-Matham%2520and%2520Kareem%2520Darwish%2520and%2520Raghad%2520Al-Rasheed%2520and%2520Waad%2520Alshammari%2520and%2520Muneera%2520Alhoshan%2520and%2520Amal%2520Almazrua%2520and%2520Asma%2520Al%2520Wazrah%2520and%2520Mais%2520Alheraki%2520and%2520Firoj%2520Alam%2520and%2520Preslav%2520Nakov%2520and%2520Norah%2520Alzahrani%2520and%2520Eman%2520alBilali%2520and%2520Nizar%2520Habash%2520and%2520Abdelrahman%2520El-Sheikh%2520and%2520Muhammad%2520Elmallah%2520and%2520Haonan%2520Li%2520and%2520Hamdy%2520Mubarak%2520and%2520Mohamed%2520Anwar%2520and%2520Zaid%2520Alyafeai%2520and%2520Ahmed%2520Abdelali%2520and%2520Nora%2520Altwairesh%2520and%2520Maram%2520Hasanain%2520and%2520Abdulmohsen%2520Al%2520Thubaity%2520and%2520Shady%2520Shehata%2520and%2520Bashar%2520Alhafni%2520and%2520Injy%2520Hamed%2520and%2520Go%2520Inoue%2520and%2520Khalid%2520Elmadani%2520and%2520Ossama%2520Obeid%2520and%2520Fatima%2520Haouari%2520and%2520Tamer%2520Elsayed%2520and%2520Emad%2520Alghamdi%2520and%2520Khalid%2520Almubarak%2520and%2520Saied%2520Alshahrani%2520and%2520Ola%2520Aljarrah%2520and%2520Safa%2520Alajlan%2520and%2520Areej%2520Alshaqarawi%2520and%2520Maryam%2520Alshihri%2520and%2520Sultana%2520Alghurabi%2520and%2520Atikah%2520Alzeghayer%2520and%2520Afrah%2520Altamimi%2520and%2520Abdullah%2520Alfaifi%2520and%2520Abdulrahman%2520AlOsaimy%26entry.1292438233%3D%2520%2520The%2520impressive%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520English%2520has%2520not%250Abeen%2520matched%2520across%2520all%2520languages.%2520In%2520particular%252C%2520LLM%2520performance%2520in%2520Arabic%250Alags%2520behind%252C%2520due%2520to%2520data%2520scarcity%252C%2520linguistic%2520diversity%2520of%2520Arabic%2520and%2520its%250Adialects%252C%2520morphological%2520complexity%252C%2520etc.%2520Progress%2520is%2520further%2520hindered%2520by%2520the%250Aquality%2520of%2520Arabic%2520benchmarks%252C%2520which%2520typically%2520rely%2520on%2520static%252C%2520publicly%250Aavailable%2520data%252C%2520lack%2520comprehensive%2520task%2520coverage%252C%2520or%2520do%2520not%2520provide%2520dedicated%250Aplatforms%2520with%2520blind%2520test%2520sets.%2520This%2520makes%2520it%2520challenging%2520to%2520measure%2520actual%250Aprogress%2520and%2520to%2520mitigate%2520data%2520contamination.%2520Here%252C%2520we%2520aim%2520to%2520bridge%2520these%2520gaps.%250AIn%2520particular%252C%2520we%2520introduce%2520BALSAM%252C%2520a%2520comprehensive%252C%2520community-driven%2520benchmark%250Aaimed%2520at%2520advancing%2520Arabic%2520LLM%2520development%2520and%2520evaluation.%2520It%2520includes%252078%2520NLP%250Atasks%2520from%252014%2520broad%2520categories%252C%2520with%252052K%2520examples%2520divided%2520into%252037K%2520test%2520and%252015K%250Adevelopment%252C%2520and%2520a%2520centralized%252C%2520transparent%2520platform%2520for%2520blind%2520evaluation.%2520We%250Aenvision%2520BALSAM%2520as%2520a%2520unifying%2520platform%2520that%2520sets%2520standards%2520and%2520promotes%250Acollaborative%2520research%2520to%2520advance%2520Arabic%2520LLM%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BALSAM%3A%20A%20Platform%20for%20Benchmarking%20Arabic%20Large%20Language%20Models&entry.906535625=Rawan%20Al-Matham%20and%20Kareem%20Darwish%20and%20Raghad%20Al-Rasheed%20and%20Waad%20Alshammari%20and%20Muneera%20Alhoshan%20and%20Amal%20Almazrua%20and%20Asma%20Al%20Wazrah%20and%20Mais%20Alheraki%20and%20Firoj%20Alam%20and%20Preslav%20Nakov%20and%20Norah%20Alzahrani%20and%20Eman%20alBilali%20and%20Nizar%20Habash%20and%20Abdelrahman%20El-Sheikh%20and%20Muhammad%20Elmallah%20and%20Haonan%20Li%20and%20Hamdy%20Mubarak%20and%20Mohamed%20Anwar%20and%20Zaid%20Alyafeai%20and%20Ahmed%20Abdelali%20and%20Nora%20Altwairesh%20and%20Maram%20Hasanain%20and%20Abdulmohsen%20Al%20Thubaity%20and%20Shady%20Shehata%20and%20Bashar%20Alhafni%20and%20Injy%20Hamed%20and%20Go%20Inoue%20and%20Khalid%20Elmadani%20and%20Ossama%20Obeid%20and%20Fatima%20Haouari%20and%20Tamer%20Elsayed%20and%20Emad%20Alghamdi%20and%20Khalid%20Almubarak%20and%20Saied%20Alshahrani%20and%20Ola%20Aljarrah%20and%20Safa%20Alajlan%20and%20Areej%20Alshaqarawi%20and%20Maryam%20Alshihri%20and%20Sultana%20Alghurabi%20and%20Atikah%20Alzeghayer%20and%20Afrah%20Altamimi%20and%20Abdullah%20Alfaifi%20and%20Abdulrahman%20AlOsaimy&entry.1292438233=%20%20The%20impressive%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20English%20has%20not%0Abeen%20matched%20across%20all%20languages.%20In%20particular%2C%20LLM%20performance%20in%20Arabic%0Alags%20behind%2C%20due%20to%20data%20scarcity%2C%20linguistic%20diversity%20of%20Arabic%20and%20its%0Adialects%2C%20morphological%20complexity%2C%20etc.%20Progress%20is%20further%20hindered%20by%20the%0Aquality%20of%20Arabic%20benchmarks%2C%20which%20typically%20rely%20on%20static%2C%20publicly%0Aavailable%20data%2C%20lack%20comprehensive%20task%20coverage%2C%20or%20do%20not%20provide%20dedicated%0Aplatforms%20with%20blind%20test%20sets.%20This%20makes%20it%20challenging%20to%20measure%20actual%0Aprogress%20and%20to%20mitigate%20data%20contamination.%20Here%2C%20we%20aim%20to%20bridge%20these%20gaps.%0AIn%20particular%2C%20we%20introduce%20BALSAM%2C%20a%20comprehensive%2C%20community-driven%20benchmark%0Aaimed%20at%20advancing%20Arabic%20LLM%20development%20and%20evaluation.%20It%20includes%2078%20NLP%0Atasks%20from%2014%20broad%20categories%2C%20with%2052K%20examples%20divided%20into%2037K%20test%20and%2015K%0Adevelopment%2C%20and%20a%20centralized%2C%20transparent%20platform%20for%20blind%20evaluation.%20We%0Aenvision%20BALSAM%20as%20a%20unifying%20platform%20that%20sets%20standards%20and%20promotes%0Acollaborative%20research%20to%20advance%20Arabic%20LLM%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22603v1&entry.124074799=Read"},
{"title": "Multi-robot LiDAR SLAM: a practical case study in underground tunnel\n  environments", "author": "Federica Di Lauro and Domenico G. Sorrenti and Miguel Angel Sotelo", "abstract": "  Multi-robot SLAM aims at localizing and building a map with multiple robots,\ninteracting with each other. In the work described in this article, we analyze\nthe pipeline of a decentralized LiDAR SLAM system to study the current\nlimitations of the state of the art, and we discover a significant source of\nfailures, i.e., that the loop detection is the source of too many false\npositives. We therefore develop and propose a new heuristic to overcome these\nlimitations. The environment taken as reference in this work is the highly\nchallenging case of underground tunnels. We also highlight potential new\nresearch areas still under-explored.\n", "link": "http://arxiv.org/abs/2507.21553v2", "date": "2025-07-30", "relevancy": 2.2833, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5786}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-robot%20LiDAR%20SLAM%3A%20a%20practical%20case%20study%20in%20underground%20tunnel%0A%20%20environments&body=Title%3A%20Multi-robot%20LiDAR%20SLAM%3A%20a%20practical%20case%20study%20in%20underground%20tunnel%0A%20%20environments%0AAuthor%3A%20Federica%20Di%20Lauro%20and%20Domenico%20G.%20Sorrenti%20and%20Miguel%20Angel%20Sotelo%0AAbstract%3A%20%20%20Multi-robot%20SLAM%20aims%20at%20localizing%20and%20building%20a%20map%20with%20multiple%20robots%2C%0Ainteracting%20with%20each%20other.%20In%20the%20work%20described%20in%20this%20article%2C%20we%20analyze%0Athe%20pipeline%20of%20a%20decentralized%20LiDAR%20SLAM%20system%20to%20study%20the%20current%0Alimitations%20of%20the%20state%20of%20the%20art%2C%20and%20we%20discover%20a%20significant%20source%20of%0Afailures%2C%20i.e.%2C%20that%20the%20loop%20detection%20is%20the%20source%20of%20too%20many%20false%0Apositives.%20We%20therefore%20develop%20and%20propose%20a%20new%20heuristic%20to%20overcome%20these%0Alimitations.%20The%20environment%20taken%20as%20reference%20in%20this%20work%20is%20the%20highly%0Achallenging%20case%20of%20underground%20tunnels.%20We%20also%20highlight%20potential%20new%0Aresearch%20areas%20still%20under-explored.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-robot%2520LiDAR%2520SLAM%253A%2520a%2520practical%2520case%2520study%2520in%2520underground%2520tunnel%250A%2520%2520environments%26entry.906535625%3DFederica%2520Di%2520Lauro%2520and%2520Domenico%2520G.%2520Sorrenti%2520and%2520Miguel%2520Angel%2520Sotelo%26entry.1292438233%3D%2520%2520Multi-robot%2520SLAM%2520aims%2520at%2520localizing%2520and%2520building%2520a%2520map%2520with%2520multiple%2520robots%252C%250Ainteracting%2520with%2520each%2520other.%2520In%2520the%2520work%2520described%2520in%2520this%2520article%252C%2520we%2520analyze%250Athe%2520pipeline%2520of%2520a%2520decentralized%2520LiDAR%2520SLAM%2520system%2520to%2520study%2520the%2520current%250Alimitations%2520of%2520the%2520state%2520of%2520the%2520art%252C%2520and%2520we%2520discover%2520a%2520significant%2520source%2520of%250Afailures%252C%2520i.e.%252C%2520that%2520the%2520loop%2520detection%2520is%2520the%2520source%2520of%2520too%2520many%2520false%250Apositives.%2520We%2520therefore%2520develop%2520and%2520propose%2520a%2520new%2520heuristic%2520to%2520overcome%2520these%250Alimitations.%2520The%2520environment%2520taken%2520as%2520reference%2520in%2520this%2520work%2520is%2520the%2520highly%250Achallenging%2520case%2520of%2520underground%2520tunnels.%2520We%2520also%2520highlight%2520potential%2520new%250Aresearch%2520areas%2520still%2520under-explored.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-robot%20LiDAR%20SLAM%3A%20a%20practical%20case%20study%20in%20underground%20tunnel%0A%20%20environments&entry.906535625=Federica%20Di%20Lauro%20and%20Domenico%20G.%20Sorrenti%20and%20Miguel%20Angel%20Sotelo&entry.1292438233=%20%20Multi-robot%20SLAM%20aims%20at%20localizing%20and%20building%20a%20map%20with%20multiple%20robots%2C%0Ainteracting%20with%20each%20other.%20In%20the%20work%20described%20in%20this%20article%2C%20we%20analyze%0Athe%20pipeline%20of%20a%20decentralized%20LiDAR%20SLAM%20system%20to%20study%20the%20current%0Alimitations%20of%20the%20state%20of%20the%20art%2C%20and%20we%20discover%20a%20significant%20source%20of%0Afailures%2C%20i.e.%2C%20that%20the%20loop%20detection%20is%20the%20source%20of%20too%20many%20false%0Apositives.%20We%20therefore%20develop%20and%20propose%20a%20new%20heuristic%20to%20overcome%20these%0Alimitations.%20The%20environment%20taken%20as%20reference%20in%20this%20work%20is%20the%20highly%0Achallenging%20case%20of%20underground%20tunnels.%20We%20also%20highlight%20potential%20new%0Aresearch%20areas%20still%20under-explored.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21553v2&entry.124074799=Read"},
{"title": "Bridging the Gap in Missing Modalities: Leveraging Knowledge\n  Distillation and Style Matching for Brain Tumor Segmentation", "author": "Shenghao Zhu and Yifei Chen and Weihong Chen and Yuanhan Wang and Chang Liu and Shuo Jiang and Feiwei Qin and Changmiao Wang", "abstract": "  Accurate and reliable brain tumor segmentation, particularly when dealing\nwith missing modalities, remains a critical challenge in medical image\nanalysis. Previous studies have not fully resolved the challenges of tumor\nboundary segmentation insensitivity and feature transfer in the absence of key\nimaging modalities. In this study, we introduce MST-KDNet, aimed at addressing\nthese critical issues. Our model features Multi-Scale Transformer Knowledge\nDistillation to effectively capture attention weights at various resolutions,\nDual-Mode Logit Distillation to improve the transfer of knowledge, and a Global\nStyle Matching Module that integrates feature matching with adversarial\nlearning. Comprehensive experiments conducted on the BraTS and FeTS 2024\ndatasets demonstrate that MST-KDNet surpasses current leading methods in both\nDice and HD95 scores, particularly in conditions with substantial modality\nloss. Our approach shows exceptional robustness and generalization potential,\nmaking it a promising candidate for real-world clinical applications. Our\nsource code is available at https://github.com/Quanato607/MST-KDNet.\n", "link": "http://arxiv.org/abs/2507.22626v1", "date": "2025-07-30", "relevancy": 2.2746, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6033}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%20in%20Missing%20Modalities%3A%20Leveraging%20Knowledge%0A%20%20Distillation%20and%20Style%20Matching%20for%20Brain%20Tumor%20Segmentation&body=Title%3A%20Bridging%20the%20Gap%20in%20Missing%20Modalities%3A%20Leveraging%20Knowledge%0A%20%20Distillation%20and%20Style%20Matching%20for%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Shenghao%20Zhu%20and%20Yifei%20Chen%20and%20Weihong%20Chen%20and%20Yuanhan%20Wang%20and%20Chang%20Liu%20and%20Shuo%20Jiang%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%0AAbstract%3A%20%20%20Accurate%20and%20reliable%20brain%20tumor%20segmentation%2C%20particularly%20when%20dealing%0Awith%20missing%20modalities%2C%20remains%20a%20critical%20challenge%20in%20medical%20image%0Aanalysis.%20Previous%20studies%20have%20not%20fully%20resolved%20the%20challenges%20of%20tumor%0Aboundary%20segmentation%20insensitivity%20and%20feature%20transfer%20in%20the%20absence%20of%20key%0Aimaging%20modalities.%20In%20this%20study%2C%20we%20introduce%20MST-KDNet%2C%20aimed%20at%20addressing%0Athese%20critical%20issues.%20Our%20model%20features%20Multi-Scale%20Transformer%20Knowledge%0ADistillation%20to%20effectively%20capture%20attention%20weights%20at%20various%20resolutions%2C%0ADual-Mode%20Logit%20Distillation%20to%20improve%20the%20transfer%20of%20knowledge%2C%20and%20a%20Global%0AStyle%20Matching%20Module%20that%20integrates%20feature%20matching%20with%20adversarial%0Alearning.%20Comprehensive%20experiments%20conducted%20on%20the%20BraTS%20and%20FeTS%202024%0Adatasets%20demonstrate%20that%20MST-KDNet%20surpasses%20current%20leading%20methods%20in%20both%0ADice%20and%20HD95%20scores%2C%20particularly%20in%20conditions%20with%20substantial%20modality%0Aloss.%20Our%20approach%20shows%20exceptional%20robustness%20and%20generalization%20potential%2C%0Amaking%20it%20a%20promising%20candidate%20for%20real-world%20clinical%20applications.%20Our%0Asource%20code%20is%20available%20at%20https%3A//github.com/Quanato607/MST-KDNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Gap%2520in%2520Missing%2520Modalities%253A%2520Leveraging%2520Knowledge%250A%2520%2520Distillation%2520and%2520Style%2520Matching%2520for%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DShenghao%2520Zhu%2520and%2520Yifei%2520Chen%2520and%2520Weihong%2520Chen%2520and%2520Yuanhan%2520Wang%2520and%2520Chang%2520Liu%2520and%2520Shuo%2520Jiang%2520and%2520Feiwei%2520Qin%2520and%2520Changmiao%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520and%2520reliable%2520brain%2520tumor%2520segmentation%252C%2520particularly%2520when%2520dealing%250Awith%2520missing%2520modalities%252C%2520remains%2520a%2520critical%2520challenge%2520in%2520medical%2520image%250Aanalysis.%2520Previous%2520studies%2520have%2520not%2520fully%2520resolved%2520the%2520challenges%2520of%2520tumor%250Aboundary%2520segmentation%2520insensitivity%2520and%2520feature%2520transfer%2520in%2520the%2520absence%2520of%2520key%250Aimaging%2520modalities.%2520In%2520this%2520study%252C%2520we%2520introduce%2520MST-KDNet%252C%2520aimed%2520at%2520addressing%250Athese%2520critical%2520issues.%2520Our%2520model%2520features%2520Multi-Scale%2520Transformer%2520Knowledge%250ADistillation%2520to%2520effectively%2520capture%2520attention%2520weights%2520at%2520various%2520resolutions%252C%250ADual-Mode%2520Logit%2520Distillation%2520to%2520improve%2520the%2520transfer%2520of%2520knowledge%252C%2520and%2520a%2520Global%250AStyle%2520Matching%2520Module%2520that%2520integrates%2520feature%2520matching%2520with%2520adversarial%250Alearning.%2520Comprehensive%2520experiments%2520conducted%2520on%2520the%2520BraTS%2520and%2520FeTS%25202024%250Adatasets%2520demonstrate%2520that%2520MST-KDNet%2520surpasses%2520current%2520leading%2520methods%2520in%2520both%250ADice%2520and%2520HD95%2520scores%252C%2520particularly%2520in%2520conditions%2520with%2520substantial%2520modality%250Aloss.%2520Our%2520approach%2520shows%2520exceptional%2520robustness%2520and%2520generalization%2520potential%252C%250Amaking%2520it%2520a%2520promising%2520candidate%2520for%2520real-world%2520clinical%2520applications.%2520Our%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/Quanato607/MST-KDNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%20in%20Missing%20Modalities%3A%20Leveraging%20Knowledge%0A%20%20Distillation%20and%20Style%20Matching%20for%20Brain%20Tumor%20Segmentation&entry.906535625=Shenghao%20Zhu%20and%20Yifei%20Chen%20and%20Weihong%20Chen%20and%20Yuanhan%20Wang%20and%20Chang%20Liu%20and%20Shuo%20Jiang%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang&entry.1292438233=%20%20Accurate%20and%20reliable%20brain%20tumor%20segmentation%2C%20particularly%20when%20dealing%0Awith%20missing%20modalities%2C%20remains%20a%20critical%20challenge%20in%20medical%20image%0Aanalysis.%20Previous%20studies%20have%20not%20fully%20resolved%20the%20challenges%20of%20tumor%0Aboundary%20segmentation%20insensitivity%20and%20feature%20transfer%20in%20the%20absence%20of%20key%0Aimaging%20modalities.%20In%20this%20study%2C%20we%20introduce%20MST-KDNet%2C%20aimed%20at%20addressing%0Athese%20critical%20issues.%20Our%20model%20features%20Multi-Scale%20Transformer%20Knowledge%0ADistillation%20to%20effectively%20capture%20attention%20weights%20at%20various%20resolutions%2C%0ADual-Mode%20Logit%20Distillation%20to%20improve%20the%20transfer%20of%20knowledge%2C%20and%20a%20Global%0AStyle%20Matching%20Module%20that%20integrates%20feature%20matching%20with%20adversarial%0Alearning.%20Comprehensive%20experiments%20conducted%20on%20the%20BraTS%20and%20FeTS%202024%0Adatasets%20demonstrate%20that%20MST-KDNet%20surpasses%20current%20leading%20methods%20in%20both%0ADice%20and%20HD95%20scores%2C%20particularly%20in%20conditions%20with%20substantial%20modality%0Aloss.%20Our%20approach%20shows%20exceptional%20robustness%20and%20generalization%20potential%2C%0Amaking%20it%20a%20promising%20candidate%20for%20real-world%20clinical%20applications.%20Our%0Asource%20code%20is%20available%20at%20https%3A//github.com/Quanato607/MST-KDNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22626v1&entry.124074799=Read"},
{"title": "Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation", "author": "Hongbin Lin and Yifan Jiang and Juangui Xu and Jesse Jiaxi Xu and Yi Lu and Zhengyu Hu and Ying-Cong Chen and Hao Wang", "abstract": "  3D point cloud segmentation aims to assign semantic labels to individual\npoints in a scene for fine-grained spatial understanding. Existing methods\ntypically adopt data augmentation to alleviate the burden of large-scale\nannotation. However, most augmentation strategies only focus on local\ntransformations or semantic recomposition, lacking the consideration of global\nstructural dependencies within scenes. To address this limitation, we propose a\ngraph-guided data augmentation framework with dual-level constraints for\nrealistic 3D scene synthesis. Our method learns object relationship statistics\nfrom real-world data to construct guiding graphs for scene generation.\nLocal-level constraints enforce geometric plausibility and semantic consistency\nbetween objects, while global-level constraints maintain the topological\nstructure of the scene by aligning the generated layout with the guiding graph.\nExtensive experiments on indoor and outdoor datasets demonstrate that our\nframework generates diverse and high-quality augmented scenes, leading to\nconsistent improvements in point cloud segmentation performance across various\nmodels.\n", "link": "http://arxiv.org/abs/2507.22668v1", "date": "2025-07-30", "relevancy": 2.2742, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5712}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5675}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Guided%20Dual-Level%20Augmentation%20for%203D%20Scene%20Segmentation&body=Title%3A%20Graph-Guided%20Dual-Level%20Augmentation%20for%203D%20Scene%20Segmentation%0AAuthor%3A%20Hongbin%20Lin%20and%20Yifan%20Jiang%20and%20Juangui%20Xu%20and%20Jesse%20Jiaxi%20Xu%20and%20Yi%20Lu%20and%20Zhengyu%20Hu%20and%20Ying-Cong%20Chen%20and%20Hao%20Wang%0AAbstract%3A%20%20%203D%20point%20cloud%20segmentation%20aims%20to%20assign%20semantic%20labels%20to%20individual%0Apoints%20in%20a%20scene%20for%20fine-grained%20spatial%20understanding.%20Existing%20methods%0Atypically%20adopt%20data%20augmentation%20to%20alleviate%20the%20burden%20of%20large-scale%0Aannotation.%20However%2C%20most%20augmentation%20strategies%20only%20focus%20on%20local%0Atransformations%20or%20semantic%20recomposition%2C%20lacking%20the%20consideration%20of%20global%0Astructural%20dependencies%20within%20scenes.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Agraph-guided%20data%20augmentation%20framework%20with%20dual-level%20constraints%20for%0Arealistic%203D%20scene%20synthesis.%20Our%20method%20learns%20object%20relationship%20statistics%0Afrom%20real-world%20data%20to%20construct%20guiding%20graphs%20for%20scene%20generation.%0ALocal-level%20constraints%20enforce%20geometric%20plausibility%20and%20semantic%20consistency%0Abetween%20objects%2C%20while%20global-level%20constraints%20maintain%20the%20topological%0Astructure%20of%20the%20scene%20by%20aligning%20the%20generated%20layout%20with%20the%20guiding%20graph.%0AExtensive%20experiments%20on%20indoor%20and%20outdoor%20datasets%20demonstrate%20that%20our%0Aframework%20generates%20diverse%20and%20high-quality%20augmented%20scenes%2C%20leading%20to%0Aconsistent%20improvements%20in%20point%20cloud%20segmentation%20performance%20across%20various%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Guided%2520Dual-Level%2520Augmentation%2520for%25203D%2520Scene%2520Segmentation%26entry.906535625%3DHongbin%2520Lin%2520and%2520Yifan%2520Jiang%2520and%2520Juangui%2520Xu%2520and%2520Jesse%2520Jiaxi%2520Xu%2520and%2520Yi%2520Lu%2520and%2520Zhengyu%2520Hu%2520and%2520Ying-Cong%2520Chen%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%25203D%2520point%2520cloud%2520segmentation%2520aims%2520to%2520assign%2520semantic%2520labels%2520to%2520individual%250Apoints%2520in%2520a%2520scene%2520for%2520fine-grained%2520spatial%2520understanding.%2520Existing%2520methods%250Atypically%2520adopt%2520data%2520augmentation%2520to%2520alleviate%2520the%2520burden%2520of%2520large-scale%250Aannotation.%2520However%252C%2520most%2520augmentation%2520strategies%2520only%2520focus%2520on%2520local%250Atransformations%2520or%2520semantic%2520recomposition%252C%2520lacking%2520the%2520consideration%2520of%2520global%250Astructural%2520dependencies%2520within%2520scenes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%250Agraph-guided%2520data%2520augmentation%2520framework%2520with%2520dual-level%2520constraints%2520for%250Arealistic%25203D%2520scene%2520synthesis.%2520Our%2520method%2520learns%2520object%2520relationship%2520statistics%250Afrom%2520real-world%2520data%2520to%2520construct%2520guiding%2520graphs%2520for%2520scene%2520generation.%250ALocal-level%2520constraints%2520enforce%2520geometric%2520plausibility%2520and%2520semantic%2520consistency%250Abetween%2520objects%252C%2520while%2520global-level%2520constraints%2520maintain%2520the%2520topological%250Astructure%2520of%2520the%2520scene%2520by%2520aligning%2520the%2520generated%2520layout%2520with%2520the%2520guiding%2520graph.%250AExtensive%2520experiments%2520on%2520indoor%2520and%2520outdoor%2520datasets%2520demonstrate%2520that%2520our%250Aframework%2520generates%2520diverse%2520and%2520high-quality%2520augmented%2520scenes%252C%2520leading%2520to%250Aconsistent%2520improvements%2520in%2520point%2520cloud%2520segmentation%2520performance%2520across%2520various%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Guided%20Dual-Level%20Augmentation%20for%203D%20Scene%20Segmentation&entry.906535625=Hongbin%20Lin%20and%20Yifan%20Jiang%20and%20Juangui%20Xu%20and%20Jesse%20Jiaxi%20Xu%20and%20Yi%20Lu%20and%20Zhengyu%20Hu%20and%20Ying-Cong%20Chen%20and%20Hao%20Wang&entry.1292438233=%20%203D%20point%20cloud%20segmentation%20aims%20to%20assign%20semantic%20labels%20to%20individual%0Apoints%20in%20a%20scene%20for%20fine-grained%20spatial%20understanding.%20Existing%20methods%0Atypically%20adopt%20data%20augmentation%20to%20alleviate%20the%20burden%20of%20large-scale%0Aannotation.%20However%2C%20most%20augmentation%20strategies%20only%20focus%20on%20local%0Atransformations%20or%20semantic%20recomposition%2C%20lacking%20the%20consideration%20of%20global%0Astructural%20dependencies%20within%20scenes.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Agraph-guided%20data%20augmentation%20framework%20with%20dual-level%20constraints%20for%0Arealistic%203D%20scene%20synthesis.%20Our%20method%20learns%20object%20relationship%20statistics%0Afrom%20real-world%20data%20to%20construct%20guiding%20graphs%20for%20scene%20generation.%0ALocal-level%20constraints%20enforce%20geometric%20plausibility%20and%20semantic%20consistency%0Abetween%20objects%2C%20while%20global-level%20constraints%20maintain%20the%20topological%0Astructure%20of%20the%20scene%20by%20aligning%20the%20generated%20layout%20with%20the%20guiding%20graph.%0AExtensive%20experiments%20on%20indoor%20and%20outdoor%20datasets%20demonstrate%20that%20our%0Aframework%20generates%20diverse%20and%20high-quality%20augmented%20scenes%2C%20leading%20to%0Aconsistent%20improvements%20in%20point%20cloud%20segmentation%20performance%20across%20various%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22668v1&entry.124074799=Read"},
{"title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness\n  via dynamic curvature estimation", "author": "Peiran Sun", "abstract": "  Adversarial attack reveals the vulnerability of deep learning models. For\nabout a decade, countless attack and defense methods have been proposed,\nleading to robustified classifiers and better understanding of models. Among\nthese methods, curvature-based approaches have attracted attention because it\nis assumed that high curvature may give rise to rough decision boundary.\nHowever, the most commonly used \\textit{curvature} is the curvature of loss\nfunction, scores or other parameters from within the model as opposed to\ndecision boundary curvature, since the former can be relatively easily formed\nusing second order derivative. In this paper, we propose a new query-efficient\nmethod, dynamic curvature estimation(DCE), to estimate the decision boundary\ncurvature in a black-box setting. Our approach is based on CGBA, a black-box\nadversarial attack. By performing DCE on a wide range of classifiers, we\ndiscovered, statistically, a connection between decision boundary curvature and\nadversarial robustness. We also propose a new attack method, curvature dynamic\nblack-box attack(CDBA) with improved performance using the dynamically\nestimated curvature.\n", "link": "http://arxiv.org/abs/2505.19194v2", "date": "2025-07-30", "relevancy": 2.2646, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4586}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.455}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curvature%20Dynamic%20Black-box%20Attack%3A%20revisiting%20adversarial%20robustness%0A%20%20via%20dynamic%20curvature%20estimation&body=Title%3A%20Curvature%20Dynamic%20Black-box%20Attack%3A%20revisiting%20adversarial%20robustness%0A%20%20via%20dynamic%20curvature%20estimation%0AAuthor%3A%20Peiran%20Sun%0AAbstract%3A%20%20%20Adversarial%20attack%20reveals%20the%20vulnerability%20of%20deep%20learning%20models.%20For%0Aabout%20a%20decade%2C%20countless%20attack%20and%20defense%20methods%20have%20been%20proposed%2C%0Aleading%20to%20robustified%20classifiers%20and%20better%20understanding%20of%20models.%20Among%0Athese%20methods%2C%20curvature-based%20approaches%20have%20attracted%20attention%20because%20it%0Ais%20assumed%20that%20high%20curvature%20may%20give%20rise%20to%20rough%20decision%20boundary.%0AHowever%2C%20the%20most%20commonly%20used%20%5Ctextit%7Bcurvature%7D%20is%20the%20curvature%20of%20loss%0Afunction%2C%20scores%20or%20other%20parameters%20from%20within%20the%20model%20as%20opposed%20to%0Adecision%20boundary%20curvature%2C%20since%20the%20former%20can%20be%20relatively%20easily%20formed%0Ausing%20second%20order%20derivative.%20In%20this%20paper%2C%20we%20propose%20a%20new%20query-efficient%0Amethod%2C%20dynamic%20curvature%20estimation%28DCE%29%2C%20to%20estimate%20the%20decision%20boundary%0Acurvature%20in%20a%20black-box%20setting.%20Our%20approach%20is%20based%20on%20CGBA%2C%20a%20black-box%0Aadversarial%20attack.%20By%20performing%20DCE%20on%20a%20wide%20range%20of%20classifiers%2C%20we%0Adiscovered%2C%20statistically%2C%20a%20connection%20between%20decision%20boundary%20curvature%20and%0Aadversarial%20robustness.%20We%20also%20propose%20a%20new%20attack%20method%2C%20curvature%20dynamic%0Ablack-box%20attack%28CDBA%29%20with%20improved%20performance%20using%20the%20dynamically%0Aestimated%20curvature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19194v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurvature%2520Dynamic%2520Black-box%2520Attack%253A%2520revisiting%2520adversarial%2520robustness%250A%2520%2520via%2520dynamic%2520curvature%2520estimation%26entry.906535625%3DPeiran%2520Sun%26entry.1292438233%3D%2520%2520Adversarial%2520attack%2520reveals%2520the%2520vulnerability%2520of%2520deep%2520learning%2520models.%2520For%250Aabout%2520a%2520decade%252C%2520countless%2520attack%2520and%2520defense%2520methods%2520have%2520been%2520proposed%252C%250Aleading%2520to%2520robustified%2520classifiers%2520and%2520better%2520understanding%2520of%2520models.%2520Among%250Athese%2520methods%252C%2520curvature-based%2520approaches%2520have%2520attracted%2520attention%2520because%2520it%250Ais%2520assumed%2520that%2520high%2520curvature%2520may%2520give%2520rise%2520to%2520rough%2520decision%2520boundary.%250AHowever%252C%2520the%2520most%2520commonly%2520used%2520%255Ctextit%257Bcurvature%257D%2520is%2520the%2520curvature%2520of%2520loss%250Afunction%252C%2520scores%2520or%2520other%2520parameters%2520from%2520within%2520the%2520model%2520as%2520opposed%2520to%250Adecision%2520boundary%2520curvature%252C%2520since%2520the%2520former%2520can%2520be%2520relatively%2520easily%2520formed%250Ausing%2520second%2520order%2520derivative.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520query-efficient%250Amethod%252C%2520dynamic%2520curvature%2520estimation%2528DCE%2529%252C%2520to%2520estimate%2520the%2520decision%2520boundary%250Acurvature%2520in%2520a%2520black-box%2520setting.%2520Our%2520approach%2520is%2520based%2520on%2520CGBA%252C%2520a%2520black-box%250Aadversarial%2520attack.%2520By%2520performing%2520DCE%2520on%2520a%2520wide%2520range%2520of%2520classifiers%252C%2520we%250Adiscovered%252C%2520statistically%252C%2520a%2520connection%2520between%2520decision%2520boundary%2520curvature%2520and%250Aadversarial%2520robustness.%2520We%2520also%2520propose%2520a%2520new%2520attack%2520method%252C%2520curvature%2520dynamic%250Ablack-box%2520attack%2528CDBA%2529%2520with%2520improved%2520performance%2520using%2520the%2520dynamically%250Aestimated%2520curvature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19194v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curvature%20Dynamic%20Black-box%20Attack%3A%20revisiting%20adversarial%20robustness%0A%20%20via%20dynamic%20curvature%20estimation&entry.906535625=Peiran%20Sun&entry.1292438233=%20%20Adversarial%20attack%20reveals%20the%20vulnerability%20of%20deep%20learning%20models.%20For%0Aabout%20a%20decade%2C%20countless%20attack%20and%20defense%20methods%20have%20been%20proposed%2C%0Aleading%20to%20robustified%20classifiers%20and%20better%20understanding%20of%20models.%20Among%0Athese%20methods%2C%20curvature-based%20approaches%20have%20attracted%20attention%20because%20it%0Ais%20assumed%20that%20high%20curvature%20may%20give%20rise%20to%20rough%20decision%20boundary.%0AHowever%2C%20the%20most%20commonly%20used%20%5Ctextit%7Bcurvature%7D%20is%20the%20curvature%20of%20loss%0Afunction%2C%20scores%20or%20other%20parameters%20from%20within%20the%20model%20as%20opposed%20to%0Adecision%20boundary%20curvature%2C%20since%20the%20former%20can%20be%20relatively%20easily%20formed%0Ausing%20second%20order%20derivative.%20In%20this%20paper%2C%20we%20propose%20a%20new%20query-efficient%0Amethod%2C%20dynamic%20curvature%20estimation%28DCE%29%2C%20to%20estimate%20the%20decision%20boundary%0Acurvature%20in%20a%20black-box%20setting.%20Our%20approach%20is%20based%20on%20CGBA%2C%20a%20black-box%0Aadversarial%20attack.%20By%20performing%20DCE%20on%20a%20wide%20range%20of%20classifiers%2C%20we%0Adiscovered%2C%20statistically%2C%20a%20connection%20between%20decision%20boundary%20curvature%20and%0Aadversarial%20robustness.%20We%20also%20propose%20a%20new%20attack%20method%2C%20curvature%20dynamic%0Ablack-box%20attack%28CDBA%29%20with%20improved%20performance%20using%20the%20dynamically%0Aestimated%20curvature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19194v2&entry.124074799=Read"},
{"title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\n  Multimodal Reasoning", "author": "Ruifeng Yuan and Chenghao Xiao and Sicong Leng and Jianyu Wang and Long Li and Weiwen Xu and Hou Pong Chan and Deli Zhao and Tingyang Xu and Zhongyu Wei and Hao Zhang and Yu Rong", "abstract": "  Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2507.22607v1", "date": "2025-07-30", "relevancy": 2.2634, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-Cogito%3A%20Progressive%20Curriculum%20Reinforcement%20Learning%20for%20Advanced%0A%20%20Multimodal%20Reasoning&body=Title%3A%20VL-Cogito%3A%20Progressive%20Curriculum%20Reinforcement%20Learning%20for%20Advanced%0A%20%20Multimodal%20Reasoning%0AAuthor%3A%20Ruifeng%20Yuan%20and%20Chenghao%20Xiao%20and%20Sicong%20Leng%20and%20Jianyu%20Wang%20and%20Long%20Li%20and%20Weiwen%20Xu%20and%20Hou%20Pong%20Chan%20and%20Deli%20Zhao%20and%20Tingyang%20Xu%20and%20Zhongyu%20Wei%20and%20Hao%20Zhang%20and%20Yu%20Rong%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20proven%20its%20effectiveness%20in%20enhancing%20the%0Areasoning%20capabilities%20of%20large%20language%20models.%20Recent%20research%20efforts%20have%0Aprogressively%20extended%20this%20paradigm%20to%20multimodal%20reasoning%20tasks.%20Due%20to%20the%0Ainherent%20complexity%20and%20diversity%20of%20multimodal%20tasks%2C%20especially%20in%20semantic%0Acontent%20and%20problem%20formulations%2C%20existing%20models%20often%20exhibit%20unstable%0Aperformance%20across%20various%20domains%20and%20difficulty%20levels.%20To%20address%20these%0Alimitations%2C%20we%20propose%20VL-Cogito%2C%20an%20advanced%20multimodal%20reasoning%20model%0Atrained%20via%20a%20novel%20multi-stage%20Progressive%20Curriculum%20Reinforcement%20Learning%0A%28PCuRL%29%20framework.%20PCuRL%20systematically%20guides%20the%20model%20through%20tasks%20of%0Agradually%20increasing%20difficulty%2C%20substantially%20improving%20its%20reasoning%0Aabilities%20across%20diverse%20multimodal%20contexts.%20The%20framework%20introduces%20two%20key%0Ainnovations%3A%20%281%29%20an%20online%20difficulty%20soft%20weighting%20mechanism%2C%20dynamically%0Aadjusting%20training%20difficulty%20across%20successive%20RL%20training%20stages%3B%20and%20%282%29%20a%0Adynamic%20length%20reward%20mechanism%2C%20which%20encourages%20the%20model%20to%20adaptively%0Aregulate%20its%20reasoning%20path%20length%20according%20to%20task%20complexity%2C%20thus%20balancing%0Areasoning%20efficiency%20with%20correctness.%20Experimental%20evaluations%20demonstrate%0Athat%20VL-Cogito%20consistently%20matches%20or%20surpasses%20existing%20reasoning-oriented%0Amodels%20across%20mainstream%20multimodal%20benchmarks%20spanning%20mathematics%2C%20science%2C%0Alogic%2C%20and%20general%20understanding%2C%20validating%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-Cogito%253A%2520Progressive%2520Curriculum%2520Reinforcement%2520Learning%2520for%2520Advanced%250A%2520%2520Multimodal%2520Reasoning%26entry.906535625%3DRuifeng%2520Yuan%2520and%2520Chenghao%2520Xiao%2520and%2520Sicong%2520Leng%2520and%2520Jianyu%2520Wang%2520and%2520Long%2520Li%2520and%2520Weiwen%2520Xu%2520and%2520Hou%2520Pong%2520Chan%2520and%2520Deli%2520Zhao%2520and%2520Tingyang%2520Xu%2520and%2520Zhongyu%2520Wei%2520and%2520Hao%2520Zhang%2520and%2520Yu%2520Rong%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520proven%2520its%2520effectiveness%2520in%2520enhancing%2520the%250Areasoning%2520capabilities%2520of%2520large%2520language%2520models.%2520Recent%2520research%2520efforts%2520have%250Aprogressively%2520extended%2520this%2520paradigm%2520to%2520multimodal%2520reasoning%2520tasks.%2520Due%2520to%2520the%250Ainherent%2520complexity%2520and%2520diversity%2520of%2520multimodal%2520tasks%252C%2520especially%2520in%2520semantic%250Acontent%2520and%2520problem%2520formulations%252C%2520existing%2520models%2520often%2520exhibit%2520unstable%250Aperformance%2520across%2520various%2520domains%2520and%2520difficulty%2520levels.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520VL-Cogito%252C%2520an%2520advanced%2520multimodal%2520reasoning%2520model%250Atrained%2520via%2520a%2520novel%2520multi-stage%2520Progressive%2520Curriculum%2520Reinforcement%2520Learning%250A%2528PCuRL%2529%2520framework.%2520PCuRL%2520systematically%2520guides%2520the%2520model%2520through%2520tasks%2520of%250Agradually%2520increasing%2520difficulty%252C%2520substantially%2520improving%2520its%2520reasoning%250Aabilities%2520across%2520diverse%2520multimodal%2520contexts.%2520The%2520framework%2520introduces%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520an%2520online%2520difficulty%2520soft%2520weighting%2520mechanism%252C%2520dynamically%250Aadjusting%2520training%2520difficulty%2520across%2520successive%2520RL%2520training%2520stages%253B%2520and%2520%25282%2529%2520a%250Adynamic%2520length%2520reward%2520mechanism%252C%2520which%2520encourages%2520the%2520model%2520to%2520adaptively%250Aregulate%2520its%2520reasoning%2520path%2520length%2520according%2520to%2520task%2520complexity%252C%2520thus%2520balancing%250Areasoning%2520efficiency%2520with%2520correctness.%2520Experimental%2520evaluations%2520demonstrate%250Athat%2520VL-Cogito%2520consistently%2520matches%2520or%2520surpasses%2520existing%2520reasoning-oriented%250Amodels%2520across%2520mainstream%2520multimodal%2520benchmarks%2520spanning%2520mathematics%252C%2520science%252C%250Alogic%252C%2520and%2520general%2520understanding%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-Cogito%3A%20Progressive%20Curriculum%20Reinforcement%20Learning%20for%20Advanced%0A%20%20Multimodal%20Reasoning&entry.906535625=Ruifeng%20Yuan%20and%20Chenghao%20Xiao%20and%20Sicong%20Leng%20and%20Jianyu%20Wang%20and%20Long%20Li%20and%20Weiwen%20Xu%20and%20Hou%20Pong%20Chan%20and%20Deli%20Zhao%20and%20Tingyang%20Xu%20and%20Zhongyu%20Wei%20and%20Hao%20Zhang%20and%20Yu%20Rong&entry.1292438233=%20%20Reinforcement%20learning%20has%20proven%20its%20effectiveness%20in%20enhancing%20the%0Areasoning%20capabilities%20of%20large%20language%20models.%20Recent%20research%20efforts%20have%0Aprogressively%20extended%20this%20paradigm%20to%20multimodal%20reasoning%20tasks.%20Due%20to%20the%0Ainherent%20complexity%20and%20diversity%20of%20multimodal%20tasks%2C%20especially%20in%20semantic%0Acontent%20and%20problem%20formulations%2C%20existing%20models%20often%20exhibit%20unstable%0Aperformance%20across%20various%20domains%20and%20difficulty%20levels.%20To%20address%20these%0Alimitations%2C%20we%20propose%20VL-Cogito%2C%20an%20advanced%20multimodal%20reasoning%20model%0Atrained%20via%20a%20novel%20multi-stage%20Progressive%20Curriculum%20Reinforcement%20Learning%0A%28PCuRL%29%20framework.%20PCuRL%20systematically%20guides%20the%20model%20through%20tasks%20of%0Agradually%20increasing%20difficulty%2C%20substantially%20improving%20its%20reasoning%0Aabilities%20across%20diverse%20multimodal%20contexts.%20The%20framework%20introduces%20two%20key%0Ainnovations%3A%20%281%29%20an%20online%20difficulty%20soft%20weighting%20mechanism%2C%20dynamically%0Aadjusting%20training%20difficulty%20across%20successive%20RL%20training%20stages%3B%20and%20%282%29%20a%0Adynamic%20length%20reward%20mechanism%2C%20which%20encourages%20the%20model%20to%20adaptively%0Aregulate%20its%20reasoning%20path%20length%20according%20to%20task%20complexity%2C%20thus%20balancing%0Areasoning%20efficiency%20with%20correctness.%20Experimental%20evaluations%20demonstrate%0Athat%20VL-Cogito%20consistently%20matches%20or%20surpasses%20existing%20reasoning-oriented%0Amodels%20across%20mainstream%20multimodal%20benchmarks%20spanning%20mathematics%2C%20science%2C%0Alogic%2C%20and%20general%20understanding%2C%20validating%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22607v1&entry.124074799=Read"},
{"title": "trAIce3D: A Prompt-Driven Transformer Based U-Net for Semantic\n  Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images", "author": "MohammadAmin Alamalhoda and Arsalan Firoozi and Alessandro Venturino and Sandra Siegert", "abstract": "  The shape of a cell contains essential information about its function within\nthe biological system. Segmenting these structures from large-scale 3D\nmicroscopy images is challenging, limiting clinical insights especially for\nmicroglia, immune-associated cells involved in neurodegenerative diseases.\nExisting segmentation methods mainly focus on cell bodies, struggle with\noverlapping structures, perform poorly on noisy images, require hyperparameter\ntuning for each new dataset, or rely on tedious semi-automated approaches. We\nintroduce trAIce3D, a deep-learning architecture designed for precise microglia\nsegmentation, capturing both somas and branches. It employs a two-stage\napproach: first, a 3D U-Net with vision transformers in the encoder detects\nsomas using a sliding-window technique to cover the entire image. Then, the\nsame architecture, enhanced with cross-attention blocks in skip connections,\nrefines each soma and its branches by using soma coordinates as a prompt and a\n3D window around the target cell as input. Training occurs in two phases:\nself-supervised Soma Segmentation, followed by prompt-based Branch\nSegmentation, leveraging pre-trained weights from the first phase. Trained and\nevaluated on a dataset of 41,230 microglial cells, trAIce3D significantly\nimproves segmentation accuracy and generalization, enabling scalable analysis\nof complex cellular morphologies. While optimized for microglia, its\narchitecture can extend to other intricate cell types, such as neurons and\nastrocytes, broadening its impact on neurobiological research.\n", "link": "http://arxiv.org/abs/2507.22635v1", "date": "2025-07-30", "relevancy": 2.2541, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5941}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20trAIce3D%3A%20A%20Prompt-Driven%20Transformer%20Based%20U-Net%20for%20Semantic%0A%20%20Segmentation%20of%20Microglial%20Cells%20from%20Large-Scale%203D%20Microscopy%20Images&body=Title%3A%20trAIce3D%3A%20A%20Prompt-Driven%20Transformer%20Based%20U-Net%20for%20Semantic%0A%20%20Segmentation%20of%20Microglial%20Cells%20from%20Large-Scale%203D%20Microscopy%20Images%0AAuthor%3A%20MohammadAmin%20Alamalhoda%20and%20Arsalan%20Firoozi%20and%20Alessandro%20Venturino%20and%20Sandra%20Siegert%0AAbstract%3A%20%20%20The%20shape%20of%20a%20cell%20contains%20essential%20information%20about%20its%20function%20within%0Athe%20biological%20system.%20Segmenting%20these%20structures%20from%20large-scale%203D%0Amicroscopy%20images%20is%20challenging%2C%20limiting%20clinical%20insights%20especially%20for%0Amicroglia%2C%20immune-associated%20cells%20involved%20in%20neurodegenerative%20diseases.%0AExisting%20segmentation%20methods%20mainly%20focus%20on%20cell%20bodies%2C%20struggle%20with%0Aoverlapping%20structures%2C%20perform%20poorly%20on%20noisy%20images%2C%20require%20hyperparameter%0Atuning%20for%20each%20new%20dataset%2C%20or%20rely%20on%20tedious%20semi-automated%20approaches.%20We%0Aintroduce%20trAIce3D%2C%20a%20deep-learning%20architecture%20designed%20for%20precise%20microglia%0Asegmentation%2C%20capturing%20both%20somas%20and%20branches.%20It%20employs%20a%20two-stage%0Aapproach%3A%20first%2C%20a%203D%20U-Net%20with%20vision%20transformers%20in%20the%20encoder%20detects%0Asomas%20using%20a%20sliding-window%20technique%20to%20cover%20the%20entire%20image.%20Then%2C%20the%0Asame%20architecture%2C%20enhanced%20with%20cross-attention%20blocks%20in%20skip%20connections%2C%0Arefines%20each%20soma%20and%20its%20branches%20by%20using%20soma%20coordinates%20as%20a%20prompt%20and%20a%0A3D%20window%20around%20the%20target%20cell%20as%20input.%20Training%20occurs%20in%20two%20phases%3A%0Aself-supervised%20Soma%20Segmentation%2C%20followed%20by%20prompt-based%20Branch%0ASegmentation%2C%20leveraging%20pre-trained%20weights%20from%20the%20first%20phase.%20Trained%20and%0Aevaluated%20on%20a%20dataset%20of%2041%2C230%20microglial%20cells%2C%20trAIce3D%20significantly%0Aimproves%20segmentation%20accuracy%20and%20generalization%2C%20enabling%20scalable%20analysis%0Aof%20complex%20cellular%20morphologies.%20While%20optimized%20for%20microglia%2C%20its%0Aarchitecture%20can%20extend%20to%20other%20intricate%20cell%20types%2C%20such%20as%20neurons%20and%0Aastrocytes%2C%20broadening%20its%20impact%20on%20neurobiological%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DtrAIce3D%253A%2520A%2520Prompt-Driven%2520Transformer%2520Based%2520U-Net%2520for%2520Semantic%250A%2520%2520Segmentation%2520of%2520Microglial%2520Cells%2520from%2520Large-Scale%25203D%2520Microscopy%2520Images%26entry.906535625%3DMohammadAmin%2520Alamalhoda%2520and%2520Arsalan%2520Firoozi%2520and%2520Alessandro%2520Venturino%2520and%2520Sandra%2520Siegert%26entry.1292438233%3D%2520%2520The%2520shape%2520of%2520a%2520cell%2520contains%2520essential%2520information%2520about%2520its%2520function%2520within%250Athe%2520biological%2520system.%2520Segmenting%2520these%2520structures%2520from%2520large-scale%25203D%250Amicroscopy%2520images%2520is%2520challenging%252C%2520limiting%2520clinical%2520insights%2520especially%2520for%250Amicroglia%252C%2520immune-associated%2520cells%2520involved%2520in%2520neurodegenerative%2520diseases.%250AExisting%2520segmentation%2520methods%2520mainly%2520focus%2520on%2520cell%2520bodies%252C%2520struggle%2520with%250Aoverlapping%2520structures%252C%2520perform%2520poorly%2520on%2520noisy%2520images%252C%2520require%2520hyperparameter%250Atuning%2520for%2520each%2520new%2520dataset%252C%2520or%2520rely%2520on%2520tedious%2520semi-automated%2520approaches.%2520We%250Aintroduce%2520trAIce3D%252C%2520a%2520deep-learning%2520architecture%2520designed%2520for%2520precise%2520microglia%250Asegmentation%252C%2520capturing%2520both%2520somas%2520and%2520branches.%2520It%2520employs%2520a%2520two-stage%250Aapproach%253A%2520first%252C%2520a%25203D%2520U-Net%2520with%2520vision%2520transformers%2520in%2520the%2520encoder%2520detects%250Asomas%2520using%2520a%2520sliding-window%2520technique%2520to%2520cover%2520the%2520entire%2520image.%2520Then%252C%2520the%250Asame%2520architecture%252C%2520enhanced%2520with%2520cross-attention%2520blocks%2520in%2520skip%2520connections%252C%250Arefines%2520each%2520soma%2520and%2520its%2520branches%2520by%2520using%2520soma%2520coordinates%2520as%2520a%2520prompt%2520and%2520a%250A3D%2520window%2520around%2520the%2520target%2520cell%2520as%2520input.%2520Training%2520occurs%2520in%2520two%2520phases%253A%250Aself-supervised%2520Soma%2520Segmentation%252C%2520followed%2520by%2520prompt-based%2520Branch%250ASegmentation%252C%2520leveraging%2520pre-trained%2520weights%2520from%2520the%2520first%2520phase.%2520Trained%2520and%250Aevaluated%2520on%2520a%2520dataset%2520of%252041%252C230%2520microglial%2520cells%252C%2520trAIce3D%2520significantly%250Aimproves%2520segmentation%2520accuracy%2520and%2520generalization%252C%2520enabling%2520scalable%2520analysis%250Aof%2520complex%2520cellular%2520morphologies.%2520While%2520optimized%2520for%2520microglia%252C%2520its%250Aarchitecture%2520can%2520extend%2520to%2520other%2520intricate%2520cell%2520types%252C%2520such%2520as%2520neurons%2520and%250Aastrocytes%252C%2520broadening%2520its%2520impact%2520on%2520neurobiological%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=trAIce3D%3A%20A%20Prompt-Driven%20Transformer%20Based%20U-Net%20for%20Semantic%0A%20%20Segmentation%20of%20Microglial%20Cells%20from%20Large-Scale%203D%20Microscopy%20Images&entry.906535625=MohammadAmin%20Alamalhoda%20and%20Arsalan%20Firoozi%20and%20Alessandro%20Venturino%20and%20Sandra%20Siegert&entry.1292438233=%20%20The%20shape%20of%20a%20cell%20contains%20essential%20information%20about%20its%20function%20within%0Athe%20biological%20system.%20Segmenting%20these%20structures%20from%20large-scale%203D%0Amicroscopy%20images%20is%20challenging%2C%20limiting%20clinical%20insights%20especially%20for%0Amicroglia%2C%20immune-associated%20cells%20involved%20in%20neurodegenerative%20diseases.%0AExisting%20segmentation%20methods%20mainly%20focus%20on%20cell%20bodies%2C%20struggle%20with%0Aoverlapping%20structures%2C%20perform%20poorly%20on%20noisy%20images%2C%20require%20hyperparameter%0Atuning%20for%20each%20new%20dataset%2C%20or%20rely%20on%20tedious%20semi-automated%20approaches.%20We%0Aintroduce%20trAIce3D%2C%20a%20deep-learning%20architecture%20designed%20for%20precise%20microglia%0Asegmentation%2C%20capturing%20both%20somas%20and%20branches.%20It%20employs%20a%20two-stage%0Aapproach%3A%20first%2C%20a%203D%20U-Net%20with%20vision%20transformers%20in%20the%20encoder%20detects%0Asomas%20using%20a%20sliding-window%20technique%20to%20cover%20the%20entire%20image.%20Then%2C%20the%0Asame%20architecture%2C%20enhanced%20with%20cross-attention%20blocks%20in%20skip%20connections%2C%0Arefines%20each%20soma%20and%20its%20branches%20by%20using%20soma%20coordinates%20as%20a%20prompt%20and%20a%0A3D%20window%20around%20the%20target%20cell%20as%20input.%20Training%20occurs%20in%20two%20phases%3A%0Aself-supervised%20Soma%20Segmentation%2C%20followed%20by%20prompt-based%20Branch%0ASegmentation%2C%20leveraging%20pre-trained%20weights%20from%20the%20first%20phase.%20Trained%20and%0Aevaluated%20on%20a%20dataset%20of%2041%2C230%20microglial%20cells%2C%20trAIce3D%20significantly%0Aimproves%20segmentation%20accuracy%20and%20generalization%2C%20enabling%20scalable%20analysis%0Aof%20complex%20cellular%20morphologies.%20While%20optimized%20for%20microglia%2C%20its%0Aarchitecture%20can%20extend%20to%20other%20intricate%20cell%20types%2C%20such%20as%20neurons%20and%0Aastrocytes%2C%20broadening%20its%20impact%20on%20neurobiological%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22635v1&entry.124074799=Read"},
{"title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for\n  Multi-Modal Offensive Content Detection", "author": "Md. Mithun Hossain and Md. Shakil Hossain and Sudipto Chaki and M. F. Mridha", "abstract": "  Multi-modal learning has emerged as a crucial research direction, as\nintegrating textual and visual information can substantially enhance\nperformance in tasks such as classification, retrieval, and scene\nunderstanding. Despite advances with large pre-trained models, existing\napproaches often suffer from insufficient cross-modal interactions and rigid\nfusion strategies, failing to fully harness the complementary strengths of\ndifferent modalities. To address these limitations, we propose Co-AttenDWG,\nco-attention with dimension-wise gating, and expert fusion. Our approach first\nprojects textual and visual features into a shared embedding space, where a\ndedicated co-attention mechanism enables simultaneous, fine-grained\ninteractions between modalities. This is further strengthened by a\ndimension-wise gating network, which adaptively modulates feature contributions\nat the channel level to emphasize salient information. In parallel, dual-path\nencoders independently refine modality-specific representations, while an\nadditional cross-attention layer aligns the modalities further. The resulting\nfeatures are aggregated via an expert fusion module that integrates learned\ngating and self-attention, yielding a robust unified representation.\nExperimental results on the MIMIC and SemEval Memotion 1.0 datasets show that\nCo-AttenDWG achieves state-of-the-art performance and superior cross-modal\nalignment, highlighting its effectiveness for diverse multi-modal applications.\n", "link": "http://arxiv.org/abs/2505.19010v2", "date": "2025-07-30", "relevancy": 2.235, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5501}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-AttenDWG%3A%20Co-Attentive%20Dimension-Wise%20Gating%20and%20Expert%20Fusion%20for%0A%20%20Multi-Modal%20Offensive%20Content%20Detection&body=Title%3A%20Co-AttenDWG%3A%20Co-Attentive%20Dimension-Wise%20Gating%20and%20Expert%20Fusion%20for%0A%20%20Multi-Modal%20Offensive%20Content%20Detection%0AAuthor%3A%20Md.%20Mithun%20Hossain%20and%20Md.%20Shakil%20Hossain%20and%20Sudipto%20Chaki%20and%20M.%20F.%20Mridha%0AAbstract%3A%20%20%20Multi-modal%20learning%20has%20emerged%20as%20a%20crucial%20research%20direction%2C%20as%0Aintegrating%20textual%20and%20visual%20information%20can%20substantially%20enhance%0Aperformance%20in%20tasks%20such%20as%20classification%2C%20retrieval%2C%20and%20scene%0Aunderstanding.%20Despite%20advances%20with%20large%20pre-trained%20models%2C%20existing%0Aapproaches%20often%20suffer%20from%20insufficient%20cross-modal%20interactions%20and%20rigid%0Afusion%20strategies%2C%20failing%20to%20fully%20harness%20the%20complementary%20strengths%20of%0Adifferent%20modalities.%20To%20address%20these%20limitations%2C%20we%20propose%20Co-AttenDWG%2C%0Aco-attention%20with%20dimension-wise%20gating%2C%20and%20expert%20fusion.%20Our%20approach%20first%0Aprojects%20textual%20and%20visual%20features%20into%20a%20shared%20embedding%20space%2C%20where%20a%0Adedicated%20co-attention%20mechanism%20enables%20simultaneous%2C%20fine-grained%0Ainteractions%20between%20modalities.%20This%20is%20further%20strengthened%20by%20a%0Adimension-wise%20gating%20network%2C%20which%20adaptively%20modulates%20feature%20contributions%0Aat%20the%20channel%20level%20to%20emphasize%20salient%20information.%20In%20parallel%2C%20dual-path%0Aencoders%20independently%20refine%20modality-specific%20representations%2C%20while%20an%0Aadditional%20cross-attention%20layer%20aligns%20the%20modalities%20further.%20The%20resulting%0Afeatures%20are%20aggregated%20via%20an%20expert%20fusion%20module%20that%20integrates%20learned%0Agating%20and%20self-attention%2C%20yielding%20a%20robust%20unified%20representation.%0AExperimental%20results%20on%20the%20MIMIC%20and%20SemEval%20Memotion%201.0%20datasets%20show%20that%0ACo-AttenDWG%20achieves%20state-of-the-art%20performance%20and%20superior%20cross-modal%0Aalignment%2C%20highlighting%20its%20effectiveness%20for%20diverse%20multi-modal%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-AttenDWG%253A%2520Co-Attentive%2520Dimension-Wise%2520Gating%2520and%2520Expert%2520Fusion%2520for%250A%2520%2520Multi-Modal%2520Offensive%2520Content%2520Detection%26entry.906535625%3DMd.%2520Mithun%2520Hossain%2520and%2520Md.%2520Shakil%2520Hossain%2520and%2520Sudipto%2520Chaki%2520and%2520M.%2520F.%2520Mridha%26entry.1292438233%3D%2520%2520Multi-modal%2520learning%2520has%2520emerged%2520as%2520a%2520crucial%2520research%2520direction%252C%2520as%250Aintegrating%2520textual%2520and%2520visual%2520information%2520can%2520substantially%2520enhance%250Aperformance%2520in%2520tasks%2520such%2520as%2520classification%252C%2520retrieval%252C%2520and%2520scene%250Aunderstanding.%2520Despite%2520advances%2520with%2520large%2520pre-trained%2520models%252C%2520existing%250Aapproaches%2520often%2520suffer%2520from%2520insufficient%2520cross-modal%2520interactions%2520and%2520rigid%250Afusion%2520strategies%252C%2520failing%2520to%2520fully%2520harness%2520the%2520complementary%2520strengths%2520of%250Adifferent%2520modalities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Co-AttenDWG%252C%250Aco-attention%2520with%2520dimension-wise%2520gating%252C%2520and%2520expert%2520fusion.%2520Our%2520approach%2520first%250Aprojects%2520textual%2520and%2520visual%2520features%2520into%2520a%2520shared%2520embedding%2520space%252C%2520where%2520a%250Adedicated%2520co-attention%2520mechanism%2520enables%2520simultaneous%252C%2520fine-grained%250Ainteractions%2520between%2520modalities.%2520This%2520is%2520further%2520strengthened%2520by%2520a%250Adimension-wise%2520gating%2520network%252C%2520which%2520adaptively%2520modulates%2520feature%2520contributions%250Aat%2520the%2520channel%2520level%2520to%2520emphasize%2520salient%2520information.%2520In%2520parallel%252C%2520dual-path%250Aencoders%2520independently%2520refine%2520modality-specific%2520representations%252C%2520while%2520an%250Aadditional%2520cross-attention%2520layer%2520aligns%2520the%2520modalities%2520further.%2520The%2520resulting%250Afeatures%2520are%2520aggregated%2520via%2520an%2520expert%2520fusion%2520module%2520that%2520integrates%2520learned%250Agating%2520and%2520self-attention%252C%2520yielding%2520a%2520robust%2520unified%2520representation.%250AExperimental%2520results%2520on%2520the%2520MIMIC%2520and%2520SemEval%2520Memotion%25201.0%2520datasets%2520show%2520that%250ACo-AttenDWG%2520achieves%2520state-of-the-art%2520performance%2520and%2520superior%2520cross-modal%250Aalignment%252C%2520highlighting%2520its%2520effectiveness%2520for%2520diverse%2520multi-modal%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-AttenDWG%3A%20Co-Attentive%20Dimension-Wise%20Gating%20and%20Expert%20Fusion%20for%0A%20%20Multi-Modal%20Offensive%20Content%20Detection&entry.906535625=Md.%20Mithun%20Hossain%20and%20Md.%20Shakil%20Hossain%20and%20Sudipto%20Chaki%20and%20M.%20F.%20Mridha&entry.1292438233=%20%20Multi-modal%20learning%20has%20emerged%20as%20a%20crucial%20research%20direction%2C%20as%0Aintegrating%20textual%20and%20visual%20information%20can%20substantially%20enhance%0Aperformance%20in%20tasks%20such%20as%20classification%2C%20retrieval%2C%20and%20scene%0Aunderstanding.%20Despite%20advances%20with%20large%20pre-trained%20models%2C%20existing%0Aapproaches%20often%20suffer%20from%20insufficient%20cross-modal%20interactions%20and%20rigid%0Afusion%20strategies%2C%20failing%20to%20fully%20harness%20the%20complementary%20strengths%20of%0Adifferent%20modalities.%20To%20address%20these%20limitations%2C%20we%20propose%20Co-AttenDWG%2C%0Aco-attention%20with%20dimension-wise%20gating%2C%20and%20expert%20fusion.%20Our%20approach%20first%0Aprojects%20textual%20and%20visual%20features%20into%20a%20shared%20embedding%20space%2C%20where%20a%0Adedicated%20co-attention%20mechanism%20enables%20simultaneous%2C%20fine-grained%0Ainteractions%20between%20modalities.%20This%20is%20further%20strengthened%20by%20a%0Adimension-wise%20gating%20network%2C%20which%20adaptively%20modulates%20feature%20contributions%0Aat%20the%20channel%20level%20to%20emphasize%20salient%20information.%20In%20parallel%2C%20dual-path%0Aencoders%20independently%20refine%20modality-specific%20representations%2C%20while%20an%0Aadditional%20cross-attention%20layer%20aligns%20the%20modalities%20further.%20The%20resulting%0Afeatures%20are%20aggregated%20via%20an%20expert%20fusion%20module%20that%20integrates%20learned%0Agating%20and%20self-attention%2C%20yielding%20a%20robust%20unified%20representation.%0AExperimental%20results%20on%20the%20MIMIC%20and%20SemEval%20Memotion%201.0%20datasets%20show%20that%0ACo-AttenDWG%20achieves%20state-of-the-art%20performance%20and%20superior%20cross-modal%0Aalignment%2C%20highlighting%20its%20effectiveness%20for%20diverse%20multi-modal%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19010v2&entry.124074799=Read"},
{"title": "Decentralized Differentially Private Power Method", "author": "Andrew Campbell and Anna Scaglione and Sean Peisert", "abstract": "  We propose a novel Decentralized Differentially Private Power Method\n(D-DP-PM) for performing Principal Component Analysis (PCA) in networked\nmulti-agent settings. Unlike conventional decentralized PCA approaches where\neach agent accesses the full n-dimensional sample space, we address the\nchallenging scenario where each agent observes only a subset of dimensions\nthrough row-wise data partitioning. Our method ensures\n$(\\epsilon,\\delta)$-Differential Privacy (DP) while enabling collaborative\nestimation of global eigenvectors across the network without requiring a\ncentral aggregator. We achieve this by having agents share only local\nembeddings of the current eigenvector iterate, leveraging both the inherent\nprivacy from random initialization and carefully calibrated Gaussian noise\nadditions. We prove that our algorithm satisfies the prescribed\n$(\\epsilon,\\delta)$-DP guarantee and establish convergence rates that\nexplicitly characterize the impact of the network topology. Our theoretical\nanalysis, based on linear dynamics and high-dimensional probability theory,\nprovides tight bounds on both privacy and utility. Experiments on real-world\ndatasets demonstrate that D-DP-PM achieves superior privacy-utility tradeoffs\ncompared to naive local DP approaches, with particularly strong performance in\nmoderate privacy regimes ($\\epsilon\\in[2, 5]$). The method converges rapidly,\nallowing practitioners to trade iterations for enhanced privacy while\nmaintaining competitive utility.\n", "link": "http://arxiv.org/abs/2507.22849v1", "date": "2025-07-30", "relevancy": 2.2332, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4483}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Differentially%20Private%20Power%20Method&body=Title%3A%20Decentralized%20Differentially%20Private%20Power%20Method%0AAuthor%3A%20Andrew%20Campbell%20and%20Anna%20Scaglione%20and%20Sean%20Peisert%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20Decentralized%20Differentially%20Private%20Power%20Method%0A%28D-DP-PM%29%20for%20performing%20Principal%20Component%20Analysis%20%28PCA%29%20in%20networked%0Amulti-agent%20settings.%20Unlike%20conventional%20decentralized%20PCA%20approaches%20where%0Aeach%20agent%20accesses%20the%20full%20n-dimensional%20sample%20space%2C%20we%20address%20the%0Achallenging%20scenario%20where%20each%20agent%20observes%20only%20a%20subset%20of%20dimensions%0Athrough%20row-wise%20data%20partitioning.%20Our%20method%20ensures%0A%24%28%5Cepsilon%2C%5Cdelta%29%24-Differential%20Privacy%20%28DP%29%20while%20enabling%20collaborative%0Aestimation%20of%20global%20eigenvectors%20across%20the%20network%20without%20requiring%20a%0Acentral%20aggregator.%20We%20achieve%20this%20by%20having%20agents%20share%20only%20local%0Aembeddings%20of%20the%20current%20eigenvector%20iterate%2C%20leveraging%20both%20the%20inherent%0Aprivacy%20from%20random%20initialization%20and%20carefully%20calibrated%20Gaussian%20noise%0Aadditions.%20We%20prove%20that%20our%20algorithm%20satisfies%20the%20prescribed%0A%24%28%5Cepsilon%2C%5Cdelta%29%24-DP%20guarantee%20and%20establish%20convergence%20rates%20that%0Aexplicitly%20characterize%20the%20impact%20of%20the%20network%20topology.%20Our%20theoretical%0Aanalysis%2C%20based%20on%20linear%20dynamics%20and%20high-dimensional%20probability%20theory%2C%0Aprovides%20tight%20bounds%20on%20both%20privacy%20and%20utility.%20Experiments%20on%20real-world%0Adatasets%20demonstrate%20that%20D-DP-PM%20achieves%20superior%20privacy-utility%20tradeoffs%0Acompared%20to%20naive%20local%20DP%20approaches%2C%20with%20particularly%20strong%20performance%20in%0Amoderate%20privacy%20regimes%20%28%24%5Cepsilon%5Cin%5B2%2C%205%5D%24%29.%20The%20method%20converges%20rapidly%2C%0Aallowing%20practitioners%20to%20trade%20iterations%20for%20enhanced%20privacy%20while%0Amaintaining%20competitive%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Differentially%2520Private%2520Power%2520Method%26entry.906535625%3DAndrew%2520Campbell%2520and%2520Anna%2520Scaglione%2520and%2520Sean%2520Peisert%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520Decentralized%2520Differentially%2520Private%2520Power%2520Method%250A%2528D-DP-PM%2529%2520for%2520performing%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520in%2520networked%250Amulti-agent%2520settings.%2520Unlike%2520conventional%2520decentralized%2520PCA%2520approaches%2520where%250Aeach%2520agent%2520accesses%2520the%2520full%2520n-dimensional%2520sample%2520space%252C%2520we%2520address%2520the%250Achallenging%2520scenario%2520where%2520each%2520agent%2520observes%2520only%2520a%2520subset%2520of%2520dimensions%250Athrough%2520row-wise%2520data%2520partitioning.%2520Our%2520method%2520ensures%250A%2524%2528%255Cepsilon%252C%255Cdelta%2529%2524-Differential%2520Privacy%2520%2528DP%2529%2520while%2520enabling%2520collaborative%250Aestimation%2520of%2520global%2520eigenvectors%2520across%2520the%2520network%2520without%2520requiring%2520a%250Acentral%2520aggregator.%2520We%2520achieve%2520this%2520by%2520having%2520agents%2520share%2520only%2520local%250Aembeddings%2520of%2520the%2520current%2520eigenvector%2520iterate%252C%2520leveraging%2520both%2520the%2520inherent%250Aprivacy%2520from%2520random%2520initialization%2520and%2520carefully%2520calibrated%2520Gaussian%2520noise%250Aadditions.%2520We%2520prove%2520that%2520our%2520algorithm%2520satisfies%2520the%2520prescribed%250A%2524%2528%255Cepsilon%252C%255Cdelta%2529%2524-DP%2520guarantee%2520and%2520establish%2520convergence%2520rates%2520that%250Aexplicitly%2520characterize%2520the%2520impact%2520of%2520the%2520network%2520topology.%2520Our%2520theoretical%250Aanalysis%252C%2520based%2520on%2520linear%2520dynamics%2520and%2520high-dimensional%2520probability%2520theory%252C%250Aprovides%2520tight%2520bounds%2520on%2520both%2520privacy%2520and%2520utility.%2520Experiments%2520on%2520real-world%250Adatasets%2520demonstrate%2520that%2520D-DP-PM%2520achieves%2520superior%2520privacy-utility%2520tradeoffs%250Acompared%2520to%2520naive%2520local%2520DP%2520approaches%252C%2520with%2520particularly%2520strong%2520performance%2520in%250Amoderate%2520privacy%2520regimes%2520%2528%2524%255Cepsilon%255Cin%255B2%252C%25205%255D%2524%2529.%2520The%2520method%2520converges%2520rapidly%252C%250Aallowing%2520practitioners%2520to%2520trade%2520iterations%2520for%2520enhanced%2520privacy%2520while%250Amaintaining%2520competitive%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Differentially%20Private%20Power%20Method&entry.906535625=Andrew%20Campbell%20and%20Anna%20Scaglione%20and%20Sean%20Peisert&entry.1292438233=%20%20We%20propose%20a%20novel%20Decentralized%20Differentially%20Private%20Power%20Method%0A%28D-DP-PM%29%20for%20performing%20Principal%20Component%20Analysis%20%28PCA%29%20in%20networked%0Amulti-agent%20settings.%20Unlike%20conventional%20decentralized%20PCA%20approaches%20where%0Aeach%20agent%20accesses%20the%20full%20n-dimensional%20sample%20space%2C%20we%20address%20the%0Achallenging%20scenario%20where%20each%20agent%20observes%20only%20a%20subset%20of%20dimensions%0Athrough%20row-wise%20data%20partitioning.%20Our%20method%20ensures%0A%24%28%5Cepsilon%2C%5Cdelta%29%24-Differential%20Privacy%20%28DP%29%20while%20enabling%20collaborative%0Aestimation%20of%20global%20eigenvectors%20across%20the%20network%20without%20requiring%20a%0Acentral%20aggregator.%20We%20achieve%20this%20by%20having%20agents%20share%20only%20local%0Aembeddings%20of%20the%20current%20eigenvector%20iterate%2C%20leveraging%20both%20the%20inherent%0Aprivacy%20from%20random%20initialization%20and%20carefully%20calibrated%20Gaussian%20noise%0Aadditions.%20We%20prove%20that%20our%20algorithm%20satisfies%20the%20prescribed%0A%24%28%5Cepsilon%2C%5Cdelta%29%24-DP%20guarantee%20and%20establish%20convergence%20rates%20that%0Aexplicitly%20characterize%20the%20impact%20of%20the%20network%20topology.%20Our%20theoretical%0Aanalysis%2C%20based%20on%20linear%20dynamics%20and%20high-dimensional%20probability%20theory%2C%0Aprovides%20tight%20bounds%20on%20both%20privacy%20and%20utility.%20Experiments%20on%20real-world%0Adatasets%20demonstrate%20that%20D-DP-PM%20achieves%20superior%20privacy-utility%20tradeoffs%0Acompared%20to%20naive%20local%20DP%20approaches%2C%20with%20particularly%20strong%20performance%20in%0Amoderate%20privacy%20regimes%20%28%24%5Cepsilon%5Cin%5B2%2C%205%5D%24%29.%20The%20method%20converges%20rapidly%2C%0Aallowing%20practitioners%20to%20trade%20iterations%20for%20enhanced%20privacy%20while%0Amaintaining%20competitive%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22849v1&entry.124074799=Read"},
{"title": "Distance and Collision Probability Estimation from Gaussian Surface\n  Models", "author": "Kshitij Goel and Wennie Tabib", "abstract": "  This paper describes continuous-space methodologies to estimate the collision\nprobability, Euclidean distance and gradient between an ellipsoidal robot model\nand an environment surface modeled as a set of Gaussian distributions.\nContinuous-space collision probability estimation is critical for\nuncertainty-aware motion planning. Most collision detection and avoidance\napproaches assume the robot is modeled as a sphere, but ellipsoidal\nrepresentations provide tighter approximations and enable navigation in\ncluttered and narrow spaces. State-of-the-art methods derive the Euclidean\ndistance and gradient by processing raw point clouds, which is computationally\nexpensive for large workspaces. Recent advances in Gaussian surface modeling\n(e.g. mixture models, splatting) enable compressed and high-fidelity surface\nrepresentations. Few methods exist to estimate continuous-space occupancy from\nsuch models. They require Gaussians to model free space and are unable to\nestimate the collision probability, Euclidean distance and gradient for an\nellipsoidal robot. The proposed methods bridge this gap by extending prior work\nin ellipsoid-to-ellipsoid Euclidean distance and collision probability\nestimation to Gaussian surface models. A geometric blending approach is also\nproposed to improve collision probability estimation. The approaches are\nevaluated with numerical 2D and 3D experiments using real-world point cloud\ndata. Methods for efficient calculation of these quantities are demonstrated to\nexecute within a few microseconds per ellipsoid pair using a single-thread on\nlow-power CPUs of modern embedded computers\n", "link": "http://arxiv.org/abs/2402.00186v3", "date": "2025-07-30", "relevancy": 2.2301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.556}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance%20and%20Collision%20Probability%20Estimation%20from%20Gaussian%20Surface%0A%20%20Models&body=Title%3A%20Distance%20and%20Collision%20Probability%20Estimation%20from%20Gaussian%20Surface%0A%20%20Models%0AAuthor%3A%20Kshitij%20Goel%20and%20Wennie%20Tabib%0AAbstract%3A%20%20%20This%20paper%20describes%20continuous-space%20methodologies%20to%20estimate%20the%20collision%0Aprobability%2C%20Euclidean%20distance%20and%20gradient%20between%20an%20ellipsoidal%20robot%20model%0Aand%20an%20environment%20surface%20modeled%20as%20a%20set%20of%20Gaussian%20distributions.%0AContinuous-space%20collision%20probability%20estimation%20is%20critical%20for%0Auncertainty-aware%20motion%20planning.%20Most%20collision%20detection%20and%20avoidance%0Aapproaches%20assume%20the%20robot%20is%20modeled%20as%20a%20sphere%2C%20but%20ellipsoidal%0Arepresentations%20provide%20tighter%20approximations%20and%20enable%20navigation%20in%0Acluttered%20and%20narrow%20spaces.%20State-of-the-art%20methods%20derive%20the%20Euclidean%0Adistance%20and%20gradient%20by%20processing%20raw%20point%20clouds%2C%20which%20is%20computationally%0Aexpensive%20for%20large%20workspaces.%20Recent%20advances%20in%20Gaussian%20surface%20modeling%0A%28e.g.%20mixture%20models%2C%20splatting%29%20enable%20compressed%20and%20high-fidelity%20surface%0Arepresentations.%20Few%20methods%20exist%20to%20estimate%20continuous-space%20occupancy%20from%0Asuch%20models.%20They%20require%20Gaussians%20to%20model%20free%20space%20and%20are%20unable%20to%0Aestimate%20the%20collision%20probability%2C%20Euclidean%20distance%20and%20gradient%20for%20an%0Aellipsoidal%20robot.%20The%20proposed%20methods%20bridge%20this%20gap%20by%20extending%20prior%20work%0Ain%20ellipsoid-to-ellipsoid%20Euclidean%20distance%20and%20collision%20probability%0Aestimation%20to%20Gaussian%20surface%20models.%20A%20geometric%20blending%20approach%20is%20also%0Aproposed%20to%20improve%20collision%20probability%20estimation.%20The%20approaches%20are%0Aevaluated%20with%20numerical%202D%20and%203D%20experiments%20using%20real-world%20point%20cloud%0Adata.%20Methods%20for%20efficient%20calculation%20of%20these%20quantities%20are%20demonstrated%20to%0Aexecute%20within%20a%20few%20microseconds%20per%20ellipsoid%20pair%20using%20a%20single-thread%20on%0Alow-power%20CPUs%20of%20modern%20embedded%20computers%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00186v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance%2520and%2520Collision%2520Probability%2520Estimation%2520from%2520Gaussian%2520Surface%250A%2520%2520Models%26entry.906535625%3DKshitij%2520Goel%2520and%2520Wennie%2520Tabib%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520continuous-space%2520methodologies%2520to%2520estimate%2520the%2520collision%250Aprobability%252C%2520Euclidean%2520distance%2520and%2520gradient%2520between%2520an%2520ellipsoidal%2520robot%2520model%250Aand%2520an%2520environment%2520surface%2520modeled%2520as%2520a%2520set%2520of%2520Gaussian%2520distributions.%250AContinuous-space%2520collision%2520probability%2520estimation%2520is%2520critical%2520for%250Auncertainty-aware%2520motion%2520planning.%2520Most%2520collision%2520detection%2520and%2520avoidance%250Aapproaches%2520assume%2520the%2520robot%2520is%2520modeled%2520as%2520a%2520sphere%252C%2520but%2520ellipsoidal%250Arepresentations%2520provide%2520tighter%2520approximations%2520and%2520enable%2520navigation%2520in%250Acluttered%2520and%2520narrow%2520spaces.%2520State-of-the-art%2520methods%2520derive%2520the%2520Euclidean%250Adistance%2520and%2520gradient%2520by%2520processing%2520raw%2520point%2520clouds%252C%2520which%2520is%2520computationally%250Aexpensive%2520for%2520large%2520workspaces.%2520Recent%2520advances%2520in%2520Gaussian%2520surface%2520modeling%250A%2528e.g.%2520mixture%2520models%252C%2520splatting%2529%2520enable%2520compressed%2520and%2520high-fidelity%2520surface%250Arepresentations.%2520Few%2520methods%2520exist%2520to%2520estimate%2520continuous-space%2520occupancy%2520from%250Asuch%2520models.%2520They%2520require%2520Gaussians%2520to%2520model%2520free%2520space%2520and%2520are%2520unable%2520to%250Aestimate%2520the%2520collision%2520probability%252C%2520Euclidean%2520distance%2520and%2520gradient%2520for%2520an%250Aellipsoidal%2520robot.%2520The%2520proposed%2520methods%2520bridge%2520this%2520gap%2520by%2520extending%2520prior%2520work%250Ain%2520ellipsoid-to-ellipsoid%2520Euclidean%2520distance%2520and%2520collision%2520probability%250Aestimation%2520to%2520Gaussian%2520surface%2520models.%2520A%2520geometric%2520blending%2520approach%2520is%2520also%250Aproposed%2520to%2520improve%2520collision%2520probability%2520estimation.%2520The%2520approaches%2520are%250Aevaluated%2520with%2520numerical%25202D%2520and%25203D%2520experiments%2520using%2520real-world%2520point%2520cloud%250Adata.%2520Methods%2520for%2520efficient%2520calculation%2520of%2520these%2520quantities%2520are%2520demonstrated%2520to%250Aexecute%2520within%2520a%2520few%2520microseconds%2520per%2520ellipsoid%2520pair%2520using%2520a%2520single-thread%2520on%250Alow-power%2520CPUs%2520of%2520modern%2520embedded%2520computers%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00186v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance%20and%20Collision%20Probability%20Estimation%20from%20Gaussian%20Surface%0A%20%20Models&entry.906535625=Kshitij%20Goel%20and%20Wennie%20Tabib&entry.1292438233=%20%20This%20paper%20describes%20continuous-space%20methodologies%20to%20estimate%20the%20collision%0Aprobability%2C%20Euclidean%20distance%20and%20gradient%20between%20an%20ellipsoidal%20robot%20model%0Aand%20an%20environment%20surface%20modeled%20as%20a%20set%20of%20Gaussian%20distributions.%0AContinuous-space%20collision%20probability%20estimation%20is%20critical%20for%0Auncertainty-aware%20motion%20planning.%20Most%20collision%20detection%20and%20avoidance%0Aapproaches%20assume%20the%20robot%20is%20modeled%20as%20a%20sphere%2C%20but%20ellipsoidal%0Arepresentations%20provide%20tighter%20approximations%20and%20enable%20navigation%20in%0Acluttered%20and%20narrow%20spaces.%20State-of-the-art%20methods%20derive%20the%20Euclidean%0Adistance%20and%20gradient%20by%20processing%20raw%20point%20clouds%2C%20which%20is%20computationally%0Aexpensive%20for%20large%20workspaces.%20Recent%20advances%20in%20Gaussian%20surface%20modeling%0A%28e.g.%20mixture%20models%2C%20splatting%29%20enable%20compressed%20and%20high-fidelity%20surface%0Arepresentations.%20Few%20methods%20exist%20to%20estimate%20continuous-space%20occupancy%20from%0Asuch%20models.%20They%20require%20Gaussians%20to%20model%20free%20space%20and%20are%20unable%20to%0Aestimate%20the%20collision%20probability%2C%20Euclidean%20distance%20and%20gradient%20for%20an%0Aellipsoidal%20robot.%20The%20proposed%20methods%20bridge%20this%20gap%20by%20extending%20prior%20work%0Ain%20ellipsoid-to-ellipsoid%20Euclidean%20distance%20and%20collision%20probability%0Aestimation%20to%20Gaussian%20surface%20models.%20A%20geometric%20blending%20approach%20is%20also%0Aproposed%20to%20improve%20collision%20probability%20estimation.%20The%20approaches%20are%0Aevaluated%20with%20numerical%202D%20and%203D%20experiments%20using%20real-world%20point%20cloud%0Adata.%20Methods%20for%20efficient%20calculation%20of%20these%20quantities%20are%20demonstrated%20to%0Aexecute%20within%20a%20few%20microseconds%20per%20ellipsoid%20pair%20using%20a%20single-thread%20on%0Alow-power%20CPUs%20of%20modern%20embedded%20computers%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00186v3&entry.124074799=Read"},
{"title": "A Linear N-Point Solver for Structure and Motion from Asynchronous\n  Tracks", "author": "Hang Su and Yunlong Feng and Daniel Gehrig and Panfeng Jiang and Ling Gao and Xavier Lagorce and Laurent Kneip", "abstract": "  Structure and continuous motion estimation from point correspondences is a\nfundamental problem in computer vision that has been powered by well-known\nalgorithms such as the familiar 5-point or 8-point algorithm. However, despite\ntheir acclaim, these algorithms are limited to processing point correspondences\noriginating from a pair of views each one representing an instantaneous capture\nof the scene. Yet, in the case of rolling shutter cameras, or more recently,\nevent cameras, this synchronization breaks down. In this work, we present a\nunified approach for structure and linear motion estimation from 2D point\ncorrespondences with arbitrary timestamps, from an arbitrary set of views. By\nformulating the problem in terms of first-order dynamics and leveraging a\nconstant velocity motion model, we derive a novel, linear point incidence\nrelation allowing for the efficient recovery of both linear velocity and 3D\npoints with predictable degeneracies and solution multiplicities. Owing to its\ngeneral formulation, it can handle correspondences from a wide range of sensing\nmodalities such as global shutter, rolling shutter, and event cameras, and can\neven combine correspondences from different collocated sensors. We validate the\neffectiveness of our solver on both simulated and real-world data, where we\nshow consistent improvement across all modalities when compared to recent\napproaches. We believe our work opens the door to efficient structure and\nmotion estimation from asynchronous data. Code can be found at\nhttps://github.com/suhang99/AsyncTrack-Motion-Solver.\n", "link": "http://arxiv.org/abs/2507.22733v1", "date": "2025-07-30", "relevancy": 2.2258, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5687}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5592}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Linear%20N-Point%20Solver%20for%20Structure%20and%20Motion%20from%20Asynchronous%0A%20%20Tracks&body=Title%3A%20A%20Linear%20N-Point%20Solver%20for%20Structure%20and%20Motion%20from%20Asynchronous%0A%20%20Tracks%0AAuthor%3A%20Hang%20Su%20and%20Yunlong%20Feng%20and%20Daniel%20Gehrig%20and%20Panfeng%20Jiang%20and%20Ling%20Gao%20and%20Xavier%20Lagorce%20and%20Laurent%20Kneip%0AAbstract%3A%20%20%20Structure%20and%20continuous%20motion%20estimation%20from%20point%20correspondences%20is%20a%0Afundamental%20problem%20in%20computer%20vision%20that%20has%20been%20powered%20by%20well-known%0Aalgorithms%20such%20as%20the%20familiar%205-point%20or%208-point%20algorithm.%20However%2C%20despite%0Atheir%20acclaim%2C%20these%20algorithms%20are%20limited%20to%20processing%20point%20correspondences%0Aoriginating%20from%20a%20pair%20of%20views%20each%20one%20representing%20an%20instantaneous%20capture%0Aof%20the%20scene.%20Yet%2C%20in%20the%20case%20of%20rolling%20shutter%20cameras%2C%20or%20more%20recently%2C%0Aevent%20cameras%2C%20this%20synchronization%20breaks%20down.%20In%20this%20work%2C%20we%20present%20a%0Aunified%20approach%20for%20structure%20and%20linear%20motion%20estimation%20from%202D%20point%0Acorrespondences%20with%20arbitrary%20timestamps%2C%20from%20an%20arbitrary%20set%20of%20views.%20By%0Aformulating%20the%20problem%20in%20terms%20of%20first-order%20dynamics%20and%20leveraging%20a%0Aconstant%20velocity%20motion%20model%2C%20we%20derive%20a%20novel%2C%20linear%20point%20incidence%0Arelation%20allowing%20for%20the%20efficient%20recovery%20of%20both%20linear%20velocity%20and%203D%0Apoints%20with%20predictable%20degeneracies%20and%20solution%20multiplicities.%20Owing%20to%20its%0Ageneral%20formulation%2C%20it%20can%20handle%20correspondences%20from%20a%20wide%20range%20of%20sensing%0Amodalities%20such%20as%20global%20shutter%2C%20rolling%20shutter%2C%20and%20event%20cameras%2C%20and%20can%0Aeven%20combine%20correspondences%20from%20different%20collocated%20sensors.%20We%20validate%20the%0Aeffectiveness%20of%20our%20solver%20on%20both%20simulated%20and%20real-world%20data%2C%20where%20we%0Ashow%20consistent%20improvement%20across%20all%20modalities%20when%20compared%20to%20recent%0Aapproaches.%20We%20believe%20our%20work%20opens%20the%20door%20to%20efficient%20structure%20and%0Amotion%20estimation%20from%20asynchronous%20data.%20Code%20can%20be%20found%20at%0Ahttps%3A//github.com/suhang99/AsyncTrack-Motion-Solver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Linear%2520N-Point%2520Solver%2520for%2520Structure%2520and%2520Motion%2520from%2520Asynchronous%250A%2520%2520Tracks%26entry.906535625%3DHang%2520Su%2520and%2520Yunlong%2520Feng%2520and%2520Daniel%2520Gehrig%2520and%2520Panfeng%2520Jiang%2520and%2520Ling%2520Gao%2520and%2520Xavier%2520Lagorce%2520and%2520Laurent%2520Kneip%26entry.1292438233%3D%2520%2520Structure%2520and%2520continuous%2520motion%2520estimation%2520from%2520point%2520correspondences%2520is%2520a%250Afundamental%2520problem%2520in%2520computer%2520vision%2520that%2520has%2520been%2520powered%2520by%2520well-known%250Aalgorithms%2520such%2520as%2520the%2520familiar%25205-point%2520or%25208-point%2520algorithm.%2520However%252C%2520despite%250Atheir%2520acclaim%252C%2520these%2520algorithms%2520are%2520limited%2520to%2520processing%2520point%2520correspondences%250Aoriginating%2520from%2520a%2520pair%2520of%2520views%2520each%2520one%2520representing%2520an%2520instantaneous%2520capture%250Aof%2520the%2520scene.%2520Yet%252C%2520in%2520the%2520case%2520of%2520rolling%2520shutter%2520cameras%252C%2520or%2520more%2520recently%252C%250Aevent%2520cameras%252C%2520this%2520synchronization%2520breaks%2520down.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Aunified%2520approach%2520for%2520structure%2520and%2520linear%2520motion%2520estimation%2520from%25202D%2520point%250Acorrespondences%2520with%2520arbitrary%2520timestamps%252C%2520from%2520an%2520arbitrary%2520set%2520of%2520views.%2520By%250Aformulating%2520the%2520problem%2520in%2520terms%2520of%2520first-order%2520dynamics%2520and%2520leveraging%2520a%250Aconstant%2520velocity%2520motion%2520model%252C%2520we%2520derive%2520a%2520novel%252C%2520linear%2520point%2520incidence%250Arelation%2520allowing%2520for%2520the%2520efficient%2520recovery%2520of%2520both%2520linear%2520velocity%2520and%25203D%250Apoints%2520with%2520predictable%2520degeneracies%2520and%2520solution%2520multiplicities.%2520Owing%2520to%2520its%250Ageneral%2520formulation%252C%2520it%2520can%2520handle%2520correspondences%2520from%2520a%2520wide%2520range%2520of%2520sensing%250Amodalities%2520such%2520as%2520global%2520shutter%252C%2520rolling%2520shutter%252C%2520and%2520event%2520cameras%252C%2520and%2520can%250Aeven%2520combine%2520correspondences%2520from%2520different%2520collocated%2520sensors.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520solver%2520on%2520both%2520simulated%2520and%2520real-world%2520data%252C%2520where%2520we%250Ashow%2520consistent%2520improvement%2520across%2520all%2520modalities%2520when%2520compared%2520to%2520recent%250Aapproaches.%2520We%2520believe%2520our%2520work%2520opens%2520the%2520door%2520to%2520efficient%2520structure%2520and%250Amotion%2520estimation%2520from%2520asynchronous%2520data.%2520Code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/suhang99/AsyncTrack-Motion-Solver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Linear%20N-Point%20Solver%20for%20Structure%20and%20Motion%20from%20Asynchronous%0A%20%20Tracks&entry.906535625=Hang%20Su%20and%20Yunlong%20Feng%20and%20Daniel%20Gehrig%20and%20Panfeng%20Jiang%20and%20Ling%20Gao%20and%20Xavier%20Lagorce%20and%20Laurent%20Kneip&entry.1292438233=%20%20Structure%20and%20continuous%20motion%20estimation%20from%20point%20correspondences%20is%20a%0Afundamental%20problem%20in%20computer%20vision%20that%20has%20been%20powered%20by%20well-known%0Aalgorithms%20such%20as%20the%20familiar%205-point%20or%208-point%20algorithm.%20However%2C%20despite%0Atheir%20acclaim%2C%20these%20algorithms%20are%20limited%20to%20processing%20point%20correspondences%0Aoriginating%20from%20a%20pair%20of%20views%20each%20one%20representing%20an%20instantaneous%20capture%0Aof%20the%20scene.%20Yet%2C%20in%20the%20case%20of%20rolling%20shutter%20cameras%2C%20or%20more%20recently%2C%0Aevent%20cameras%2C%20this%20synchronization%20breaks%20down.%20In%20this%20work%2C%20we%20present%20a%0Aunified%20approach%20for%20structure%20and%20linear%20motion%20estimation%20from%202D%20point%0Acorrespondences%20with%20arbitrary%20timestamps%2C%20from%20an%20arbitrary%20set%20of%20views.%20By%0Aformulating%20the%20problem%20in%20terms%20of%20first-order%20dynamics%20and%20leveraging%20a%0Aconstant%20velocity%20motion%20model%2C%20we%20derive%20a%20novel%2C%20linear%20point%20incidence%0Arelation%20allowing%20for%20the%20efficient%20recovery%20of%20both%20linear%20velocity%20and%203D%0Apoints%20with%20predictable%20degeneracies%20and%20solution%20multiplicities.%20Owing%20to%20its%0Ageneral%20formulation%2C%20it%20can%20handle%20correspondences%20from%20a%20wide%20range%20of%20sensing%0Amodalities%20such%20as%20global%20shutter%2C%20rolling%20shutter%2C%20and%20event%20cameras%2C%20and%20can%0Aeven%20combine%20correspondences%20from%20different%20collocated%20sensors.%20We%20validate%20the%0Aeffectiveness%20of%20our%20solver%20on%20both%20simulated%20and%20real-world%20data%2C%20where%20we%0Ashow%20consistent%20improvement%20across%20all%20modalities%20when%20compared%20to%20recent%0Aapproaches.%20We%20believe%20our%20work%20opens%20the%20door%20to%20efficient%20structure%20and%0Amotion%20estimation%20from%20asynchronous%20data.%20Code%20can%20be%20found%20at%0Ahttps%3A//github.com/suhang99/AsyncTrack-Motion-Solver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22733v1&entry.124074799=Read"},
{"title": "UniLegs: Universal Multi-Legged Robot Control through\n  Morphology-Agnostic Policy Distillation", "author": "Weijie Xi and Zhanxiang Cao and Chenlin Ming and Jianying Zheng and Guyue Zhou", "abstract": "  Developing controllers that generalize across diverse robot morphologies\nremains a significant challenge in legged locomotion. Traditional approaches\neither create specialized controllers for each morphology or compromise\nperformance for generality. This paper introduces a two-stage teacher-student\nframework that bridges this gap through policy distillation. First, we train\nspecialized teacher policies optimized for individual morphologies, capturing\nthe unique optimal control strategies for each robot design. Then, we distill\nthis specialized expertise into a single Transformer-based student policy\ncapable of controlling robots with varying leg configurations. Our experiments\nacross five distinct legged morphologies demonstrate that our approach\npreserves morphology-specific optimal behaviors, with the Transformer\narchitecture achieving 94.47\\% of teacher performance on training morphologies\nand 72.64\\% on unseen robot designs. Comparative analysis reveals that\nTransformer-based architectures consistently outperform MLP baselines by\nleveraging attention mechanisms to effectively model joint relationships across\ndifferent kinematic structures. We validate our approach through successful\ndeployment on a physical quadruped robot, demonstrating the practical viability\nof our morphology-agnostic control framework. This work presents a scalable\nsolution for developing universal legged robot controllers that maintain\nnear-optimal performance while generalizing across diverse morphologies.\n", "link": "http://arxiv.org/abs/2507.22653v1", "date": "2025-07-30", "relevancy": 2.1881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5845}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5487}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLegs%3A%20Universal%20Multi-Legged%20Robot%20Control%20through%0A%20%20Morphology-Agnostic%20Policy%20Distillation&body=Title%3A%20UniLegs%3A%20Universal%20Multi-Legged%20Robot%20Control%20through%0A%20%20Morphology-Agnostic%20Policy%20Distillation%0AAuthor%3A%20Weijie%20Xi%20and%20Zhanxiang%20Cao%20and%20Chenlin%20Ming%20and%20Jianying%20Zheng%20and%20Guyue%20Zhou%0AAbstract%3A%20%20%20Developing%20controllers%20that%20generalize%20across%20diverse%20robot%20morphologies%0Aremains%20a%20significant%20challenge%20in%20legged%20locomotion.%20Traditional%20approaches%0Aeither%20create%20specialized%20controllers%20for%20each%20morphology%20or%20compromise%0Aperformance%20for%20generality.%20This%20paper%20introduces%20a%20two-stage%20teacher-student%0Aframework%20that%20bridges%20this%20gap%20through%20policy%20distillation.%20First%2C%20we%20train%0Aspecialized%20teacher%20policies%20optimized%20for%20individual%20morphologies%2C%20capturing%0Athe%20unique%20optimal%20control%20strategies%20for%20each%20robot%20design.%20Then%2C%20we%20distill%0Athis%20specialized%20expertise%20into%20a%20single%20Transformer-based%20student%20policy%0Acapable%20of%20controlling%20robots%20with%20varying%20leg%20configurations.%20Our%20experiments%0Aacross%20five%20distinct%20legged%20morphologies%20demonstrate%20that%20our%20approach%0Apreserves%20morphology-specific%20optimal%20behaviors%2C%20with%20the%20Transformer%0Aarchitecture%20achieving%2094.47%5C%25%20of%20teacher%20performance%20on%20training%20morphologies%0Aand%2072.64%5C%25%20on%20unseen%20robot%20designs.%20Comparative%20analysis%20reveals%20that%0ATransformer-based%20architectures%20consistently%20outperform%20MLP%20baselines%20by%0Aleveraging%20attention%20mechanisms%20to%20effectively%20model%20joint%20relationships%20across%0Adifferent%20kinematic%20structures.%20We%20validate%20our%20approach%20through%20successful%0Adeployment%20on%20a%20physical%20quadruped%20robot%2C%20demonstrating%20the%20practical%20viability%0Aof%20our%20morphology-agnostic%20control%20framework.%20This%20work%20presents%20a%20scalable%0Asolution%20for%20developing%20universal%20legged%20robot%20controllers%20that%20maintain%0Anear-optimal%20performance%20while%20generalizing%20across%20diverse%20morphologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLegs%253A%2520Universal%2520Multi-Legged%2520Robot%2520Control%2520through%250A%2520%2520Morphology-Agnostic%2520Policy%2520Distillation%26entry.906535625%3DWeijie%2520Xi%2520and%2520Zhanxiang%2520Cao%2520and%2520Chenlin%2520Ming%2520and%2520Jianying%2520Zheng%2520and%2520Guyue%2520Zhou%26entry.1292438233%3D%2520%2520Developing%2520controllers%2520that%2520generalize%2520across%2520diverse%2520robot%2520morphologies%250Aremains%2520a%2520significant%2520challenge%2520in%2520legged%2520locomotion.%2520Traditional%2520approaches%250Aeither%2520create%2520specialized%2520controllers%2520for%2520each%2520morphology%2520or%2520compromise%250Aperformance%2520for%2520generality.%2520This%2520paper%2520introduces%2520a%2520two-stage%2520teacher-student%250Aframework%2520that%2520bridges%2520this%2520gap%2520through%2520policy%2520distillation.%2520First%252C%2520we%2520train%250Aspecialized%2520teacher%2520policies%2520optimized%2520for%2520individual%2520morphologies%252C%2520capturing%250Athe%2520unique%2520optimal%2520control%2520strategies%2520for%2520each%2520robot%2520design.%2520Then%252C%2520we%2520distill%250Athis%2520specialized%2520expertise%2520into%2520a%2520single%2520Transformer-based%2520student%2520policy%250Acapable%2520of%2520controlling%2520robots%2520with%2520varying%2520leg%2520configurations.%2520Our%2520experiments%250Aacross%2520five%2520distinct%2520legged%2520morphologies%2520demonstrate%2520that%2520our%2520approach%250Apreserves%2520morphology-specific%2520optimal%2520behaviors%252C%2520with%2520the%2520Transformer%250Aarchitecture%2520achieving%252094.47%255C%2525%2520of%2520teacher%2520performance%2520on%2520training%2520morphologies%250Aand%252072.64%255C%2525%2520on%2520unseen%2520robot%2520designs.%2520Comparative%2520analysis%2520reveals%2520that%250ATransformer-based%2520architectures%2520consistently%2520outperform%2520MLP%2520baselines%2520by%250Aleveraging%2520attention%2520mechanisms%2520to%2520effectively%2520model%2520joint%2520relationships%2520across%250Adifferent%2520kinematic%2520structures.%2520We%2520validate%2520our%2520approach%2520through%2520successful%250Adeployment%2520on%2520a%2520physical%2520quadruped%2520robot%252C%2520demonstrating%2520the%2520practical%2520viability%250Aof%2520our%2520morphology-agnostic%2520control%2520framework.%2520This%2520work%2520presents%2520a%2520scalable%250Asolution%2520for%2520developing%2520universal%2520legged%2520robot%2520controllers%2520that%2520maintain%250Anear-optimal%2520performance%2520while%2520generalizing%2520across%2520diverse%2520morphologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLegs%3A%20Universal%20Multi-Legged%20Robot%20Control%20through%0A%20%20Morphology-Agnostic%20Policy%20Distillation&entry.906535625=Weijie%20Xi%20and%20Zhanxiang%20Cao%20and%20Chenlin%20Ming%20and%20Jianying%20Zheng%20and%20Guyue%20Zhou&entry.1292438233=%20%20Developing%20controllers%20that%20generalize%20across%20diverse%20robot%20morphologies%0Aremains%20a%20significant%20challenge%20in%20legged%20locomotion.%20Traditional%20approaches%0Aeither%20create%20specialized%20controllers%20for%20each%20morphology%20or%20compromise%0Aperformance%20for%20generality.%20This%20paper%20introduces%20a%20two-stage%20teacher-student%0Aframework%20that%20bridges%20this%20gap%20through%20policy%20distillation.%20First%2C%20we%20train%0Aspecialized%20teacher%20policies%20optimized%20for%20individual%20morphologies%2C%20capturing%0Athe%20unique%20optimal%20control%20strategies%20for%20each%20robot%20design.%20Then%2C%20we%20distill%0Athis%20specialized%20expertise%20into%20a%20single%20Transformer-based%20student%20policy%0Acapable%20of%20controlling%20robots%20with%20varying%20leg%20configurations.%20Our%20experiments%0Aacross%20five%20distinct%20legged%20morphologies%20demonstrate%20that%20our%20approach%0Apreserves%20morphology-specific%20optimal%20behaviors%2C%20with%20the%20Transformer%0Aarchitecture%20achieving%2094.47%5C%25%20of%20teacher%20performance%20on%20training%20morphologies%0Aand%2072.64%5C%25%20on%20unseen%20robot%20designs.%20Comparative%20analysis%20reveals%20that%0ATransformer-based%20architectures%20consistently%20outperform%20MLP%20baselines%20by%0Aleveraging%20attention%20mechanisms%20to%20effectively%20model%20joint%20relationships%20across%0Adifferent%20kinematic%20structures.%20We%20validate%20our%20approach%20through%20successful%0Adeployment%20on%20a%20physical%20quadruped%20robot%2C%20demonstrating%20the%20practical%20viability%0Aof%20our%20morphology-agnostic%20control%20framework.%20This%20work%20presents%20a%20scalable%0Asolution%20for%20developing%20universal%20legged%20robot%20controllers%20that%20maintain%0Anear-optimal%20performance%20while%20generalizing%20across%20diverse%20morphologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22653v1&entry.124074799=Read"},
{"title": "Mitigating loss of variance in ensemble data assimilation: machine\n  learning-based and distance-free localization", "author": "Vinicius L. S. Silva and Gabriel S. Seabra and Alexandre A. Emerick", "abstract": "  We propose two new methods based/inspired by machine learning for tabular\ndata and distance-free localization to enhance the covariance estimations in an\nensemble data assimilation. The main goal is to enhance the data assimilation\nresults by mitigating loss of variance due to sampling errors. We also analyze\nthe suitability of several machine learning models and the balance between\naccuracy and computational cost of the covariance estimations. We introduce two\ndistance-free localization techniques leveraging machine learning methods\nspecifically tailored for tabular data. The methods are integrated into the\nEnsemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The\nresults show that the proposed localizations improve covariance accuracy and\nenhance data assimilation and uncertainty quantification results. We observe\nreduced variance loss for the input variables using the proposed methods.\nFurthermore, we compare several machine learning models, assessing their\nsuitability for the problem in terms of computational cost, and quality of the\ncovariance estimation and data match. The influence of ensemble size is also\ninvestigated, providing insights into balancing accuracy and computational\nefficiency. Our findings demonstrate that certain machine learning models are\nmore suitable for this problem. This study introduces two novel methods that\nmitigate variance loss for model parameters in ensemble-based data\nassimilation, offering practical solutions that are easy to implement and do\nnot require any additional numerical simulation or hyperparameter tuning.\n", "link": "http://arxiv.org/abs/2506.13362v2", "date": "2025-07-30", "relevancy": 2.1811, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5535}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5445}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20loss%20of%20variance%20in%20ensemble%20data%20assimilation%3A%20machine%0A%20%20learning-based%20and%20distance-free%20localization&body=Title%3A%20Mitigating%20loss%20of%20variance%20in%20ensemble%20data%20assimilation%3A%20machine%0A%20%20learning-based%20and%20distance-free%20localization%0AAuthor%3A%20Vinicius%20L.%20S.%20Silva%20and%20Gabriel%20S.%20Seabra%20and%20Alexandre%20A.%20Emerick%0AAbstract%3A%20%20%20We%20propose%20two%20new%20methods%20based/inspired%20by%20machine%20learning%20for%20tabular%0Adata%20and%20distance-free%20localization%20to%20enhance%20the%20covariance%20estimations%20in%20an%0Aensemble%20data%20assimilation.%20The%20main%20goal%20is%20to%20enhance%20the%20data%20assimilation%0Aresults%20by%20mitigating%20loss%20of%20variance%20due%20to%20sampling%20errors.%20We%20also%20analyze%0Athe%20suitability%20of%20several%20machine%20learning%20models%20and%20the%20balance%20between%0Aaccuracy%20and%20computational%20cost%20of%20the%20covariance%20estimations.%20We%20introduce%20two%0Adistance-free%20localization%20techniques%20leveraging%20machine%20learning%20methods%0Aspecifically%20tailored%20for%20tabular%20data.%20The%20methods%20are%20integrated%20into%20the%0AEnsemble%20Smoother%20with%20Multiple%20Data%20Assimilation%20%28ES-MDA%29%20framework.%20The%0Aresults%20show%20that%20the%20proposed%20localizations%20improve%20covariance%20accuracy%20and%0Aenhance%20data%20assimilation%20and%20uncertainty%20quantification%20results.%20We%20observe%0Areduced%20variance%20loss%20for%20the%20input%20variables%20using%20the%20proposed%20methods.%0AFurthermore%2C%20we%20compare%20several%20machine%20learning%20models%2C%20assessing%20their%0Asuitability%20for%20the%20problem%20in%20terms%20of%20computational%20cost%2C%20and%20quality%20of%20the%0Acovariance%20estimation%20and%20data%20match.%20The%20influence%20of%20ensemble%20size%20is%20also%0Ainvestigated%2C%20providing%20insights%20into%20balancing%20accuracy%20and%20computational%0Aefficiency.%20Our%20findings%20demonstrate%20that%20certain%20machine%20learning%20models%20are%0Amore%20suitable%20for%20this%20problem.%20This%20study%20introduces%20two%20novel%20methods%20that%0Amitigate%20variance%20loss%20for%20model%20parameters%20in%20ensemble-based%20data%0Aassimilation%2C%20offering%20practical%20solutions%20that%20are%20easy%20to%20implement%20and%20do%0Anot%20require%20any%20additional%20numerical%20simulation%20or%20hyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520loss%2520of%2520variance%2520in%2520ensemble%2520data%2520assimilation%253A%2520machine%250A%2520%2520learning-based%2520and%2520distance-free%2520localization%26entry.906535625%3DVinicius%2520L.%2520S.%2520Silva%2520and%2520Gabriel%2520S.%2520Seabra%2520and%2520Alexandre%2520A.%2520Emerick%26entry.1292438233%3D%2520%2520We%2520propose%2520two%2520new%2520methods%2520based/inspired%2520by%2520machine%2520learning%2520for%2520tabular%250Adata%2520and%2520distance-free%2520localization%2520to%2520enhance%2520the%2520covariance%2520estimations%2520in%2520an%250Aensemble%2520data%2520assimilation.%2520The%2520main%2520goal%2520is%2520to%2520enhance%2520the%2520data%2520assimilation%250Aresults%2520by%2520mitigating%2520loss%2520of%2520variance%2520due%2520to%2520sampling%2520errors.%2520We%2520also%2520analyze%250Athe%2520suitability%2520of%2520several%2520machine%2520learning%2520models%2520and%2520the%2520balance%2520between%250Aaccuracy%2520and%2520computational%2520cost%2520of%2520the%2520covariance%2520estimations.%2520We%2520introduce%2520two%250Adistance-free%2520localization%2520techniques%2520leveraging%2520machine%2520learning%2520methods%250Aspecifically%2520tailored%2520for%2520tabular%2520data.%2520The%2520methods%2520are%2520integrated%2520into%2520the%250AEnsemble%2520Smoother%2520with%2520Multiple%2520Data%2520Assimilation%2520%2528ES-MDA%2529%2520framework.%2520The%250Aresults%2520show%2520that%2520the%2520proposed%2520localizations%2520improve%2520covariance%2520accuracy%2520and%250Aenhance%2520data%2520assimilation%2520and%2520uncertainty%2520quantification%2520results.%2520We%2520observe%250Areduced%2520variance%2520loss%2520for%2520the%2520input%2520variables%2520using%2520the%2520proposed%2520methods.%250AFurthermore%252C%2520we%2520compare%2520several%2520machine%2520learning%2520models%252C%2520assessing%2520their%250Asuitability%2520for%2520the%2520problem%2520in%2520terms%2520of%2520computational%2520cost%252C%2520and%2520quality%2520of%2520the%250Acovariance%2520estimation%2520and%2520data%2520match.%2520The%2520influence%2520of%2520ensemble%2520size%2520is%2520also%250Ainvestigated%252C%2520providing%2520insights%2520into%2520balancing%2520accuracy%2520and%2520computational%250Aefficiency.%2520Our%2520findings%2520demonstrate%2520that%2520certain%2520machine%2520learning%2520models%2520are%250Amore%2520suitable%2520for%2520this%2520problem.%2520This%2520study%2520introduces%2520two%2520novel%2520methods%2520that%250Amitigate%2520variance%2520loss%2520for%2520model%2520parameters%2520in%2520ensemble-based%2520data%250Aassimilation%252C%2520offering%2520practical%2520solutions%2520that%2520are%2520easy%2520to%2520implement%2520and%2520do%250Anot%2520require%2520any%2520additional%2520numerical%2520simulation%2520or%2520hyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20loss%20of%20variance%20in%20ensemble%20data%20assimilation%3A%20machine%0A%20%20learning-based%20and%20distance-free%20localization&entry.906535625=Vinicius%20L.%20S.%20Silva%20and%20Gabriel%20S.%20Seabra%20and%20Alexandre%20A.%20Emerick&entry.1292438233=%20%20We%20propose%20two%20new%20methods%20based/inspired%20by%20machine%20learning%20for%20tabular%0Adata%20and%20distance-free%20localization%20to%20enhance%20the%20covariance%20estimations%20in%20an%0Aensemble%20data%20assimilation.%20The%20main%20goal%20is%20to%20enhance%20the%20data%20assimilation%0Aresults%20by%20mitigating%20loss%20of%20variance%20due%20to%20sampling%20errors.%20We%20also%20analyze%0Athe%20suitability%20of%20several%20machine%20learning%20models%20and%20the%20balance%20between%0Aaccuracy%20and%20computational%20cost%20of%20the%20covariance%20estimations.%20We%20introduce%20two%0Adistance-free%20localization%20techniques%20leveraging%20machine%20learning%20methods%0Aspecifically%20tailored%20for%20tabular%20data.%20The%20methods%20are%20integrated%20into%20the%0AEnsemble%20Smoother%20with%20Multiple%20Data%20Assimilation%20%28ES-MDA%29%20framework.%20The%0Aresults%20show%20that%20the%20proposed%20localizations%20improve%20covariance%20accuracy%20and%0Aenhance%20data%20assimilation%20and%20uncertainty%20quantification%20results.%20We%20observe%0Areduced%20variance%20loss%20for%20the%20input%20variables%20using%20the%20proposed%20methods.%0AFurthermore%2C%20we%20compare%20several%20machine%20learning%20models%2C%20assessing%20their%0Asuitability%20for%20the%20problem%20in%20terms%20of%20computational%20cost%2C%20and%20quality%20of%20the%0Acovariance%20estimation%20and%20data%20match.%20The%20influence%20of%20ensemble%20size%20is%20also%0Ainvestigated%2C%20providing%20insights%20into%20balancing%20accuracy%20and%20computational%0Aefficiency.%20Our%20findings%20demonstrate%20that%20certain%20machine%20learning%20models%20are%0Amore%20suitable%20for%20this%20problem.%20This%20study%20introduces%20two%20novel%20methods%20that%0Amitigate%20variance%20loss%20for%20model%20parameters%20in%20ensemble-based%20data%0Aassimilation%2C%20offering%20practical%20solutions%20that%20are%20easy%20to%20implement%20and%20do%0Anot%20require%20any%20additional%20numerical%20simulation%20or%20hyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13362v2&entry.124074799=Read"},
{"title": "Bayesian Optimization applied for accelerated Virtual Validation of the\n  Autonomous Driving Function", "author": "Satyesh Shanker Awasthi and Mohammed Irshadh Ismaaeel Sathyamangalam Imran and Stefano Arrigoni and Francesco Braghin", "abstract": "  Rigorous Verification and Validation (V&V) of Autonomous Driving Functions\n(ADFs) is paramount for ensuring the safety and public acceptance of Autonomous\nVehicles (AVs). Current validation relies heavily on simulation to achieve\nsufficient test coverage within the Operational Design Domain (ODD) of a\nvehicle, but exhaustively exploring the vast parameter space of possible\nscenarios is computationally expensive and time-consuming. This work introduces\na framework based on Bayesian Optimization (BO) to accelerate the discovery of\ncritical scenarios. We demonstrate the effectiveness of the framework on an\nModel Predictive Controller (MPC)-based motion planner, showing that it\nidentifies hazardous situations, such as off-road events, using orders of\nmagnitude fewer simulations than brute-force Design of Experiments (DoE)\nmethods. Furthermore, this study investigates the scalability of the framework\nin higher-dimensional parameter spaces and its ability to identify multiple,\ndistinct critical regions within the ODD of the motion planner used as the case\nstudy .\n", "link": "http://arxiv.org/abs/2507.22769v1", "date": "2025-07-30", "relevancy": 2.1475, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5954}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20applied%20for%20accelerated%20Virtual%20Validation%20of%20the%0A%20%20Autonomous%20Driving%20Function&body=Title%3A%20Bayesian%20Optimization%20applied%20for%20accelerated%20Virtual%20Validation%20of%20the%0A%20%20Autonomous%20Driving%20Function%0AAuthor%3A%20Satyesh%20Shanker%20Awasthi%20and%20Mohammed%20Irshadh%20Ismaaeel%20Sathyamangalam%20Imran%20and%20Stefano%20Arrigoni%20and%20Francesco%20Braghin%0AAbstract%3A%20%20%20Rigorous%20Verification%20and%20Validation%20%28V%26V%29%20of%20Autonomous%20Driving%20Functions%0A%28ADFs%29%20is%20paramount%20for%20ensuring%20the%20safety%20and%20public%20acceptance%20of%20Autonomous%0AVehicles%20%28AVs%29.%20Current%20validation%20relies%20heavily%20on%20simulation%20to%20achieve%0Asufficient%20test%20coverage%20within%20the%20Operational%20Design%20Domain%20%28ODD%29%20of%20a%0Avehicle%2C%20but%20exhaustively%20exploring%20the%20vast%20parameter%20space%20of%20possible%0Ascenarios%20is%20computationally%20expensive%20and%20time-consuming.%20This%20work%20introduces%0Aa%20framework%20based%20on%20Bayesian%20Optimization%20%28BO%29%20to%20accelerate%20the%20discovery%20of%0Acritical%20scenarios.%20We%20demonstrate%20the%20effectiveness%20of%20the%20framework%20on%20an%0AModel%20Predictive%20Controller%20%28MPC%29-based%20motion%20planner%2C%20showing%20that%20it%0Aidentifies%20hazardous%20situations%2C%20such%20as%20off-road%20events%2C%20using%20orders%20of%0Amagnitude%20fewer%20simulations%20than%20brute-force%20Design%20of%20Experiments%20%28DoE%29%0Amethods.%20Furthermore%2C%20this%20study%20investigates%20the%20scalability%20of%20the%20framework%0Ain%20higher-dimensional%20parameter%20spaces%20and%20its%20ability%20to%20identify%20multiple%2C%0Adistinct%20critical%20regions%20within%20the%20ODD%20of%20the%20motion%20planner%20used%20as%20the%20case%0Astudy%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520applied%2520for%2520accelerated%2520Virtual%2520Validation%2520of%2520the%250A%2520%2520Autonomous%2520Driving%2520Function%26entry.906535625%3DSatyesh%2520Shanker%2520Awasthi%2520and%2520Mohammed%2520Irshadh%2520Ismaaeel%2520Sathyamangalam%2520Imran%2520and%2520Stefano%2520Arrigoni%2520and%2520Francesco%2520Braghin%26entry.1292438233%3D%2520%2520Rigorous%2520Verification%2520and%2520Validation%2520%2528V%2526V%2529%2520of%2520Autonomous%2520Driving%2520Functions%250A%2528ADFs%2529%2520is%2520paramount%2520for%2520ensuring%2520the%2520safety%2520and%2520public%2520acceptance%2520of%2520Autonomous%250AVehicles%2520%2528AVs%2529.%2520Current%2520validation%2520relies%2520heavily%2520on%2520simulation%2520to%2520achieve%250Asufficient%2520test%2520coverage%2520within%2520the%2520Operational%2520Design%2520Domain%2520%2528ODD%2529%2520of%2520a%250Avehicle%252C%2520but%2520exhaustively%2520exploring%2520the%2520vast%2520parameter%2520space%2520of%2520possible%250Ascenarios%2520is%2520computationally%2520expensive%2520and%2520time-consuming.%2520This%2520work%2520introduces%250Aa%2520framework%2520based%2520on%2520Bayesian%2520Optimization%2520%2528BO%2529%2520to%2520accelerate%2520the%2520discovery%2520of%250Acritical%2520scenarios.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520framework%2520on%2520an%250AModel%2520Predictive%2520Controller%2520%2528MPC%2529-based%2520motion%2520planner%252C%2520showing%2520that%2520it%250Aidentifies%2520hazardous%2520situations%252C%2520such%2520as%2520off-road%2520events%252C%2520using%2520orders%2520of%250Amagnitude%2520fewer%2520simulations%2520than%2520brute-force%2520Design%2520of%2520Experiments%2520%2528DoE%2529%250Amethods.%2520Furthermore%252C%2520this%2520study%2520investigates%2520the%2520scalability%2520of%2520the%2520framework%250Ain%2520higher-dimensional%2520parameter%2520spaces%2520and%2520its%2520ability%2520to%2520identify%2520multiple%252C%250Adistinct%2520critical%2520regions%2520within%2520the%2520ODD%2520of%2520the%2520motion%2520planner%2520used%2520as%2520the%2520case%250Astudy%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20applied%20for%20accelerated%20Virtual%20Validation%20of%20the%0A%20%20Autonomous%20Driving%20Function&entry.906535625=Satyesh%20Shanker%20Awasthi%20and%20Mohammed%20Irshadh%20Ismaaeel%20Sathyamangalam%20Imran%20and%20Stefano%20Arrigoni%20and%20Francesco%20Braghin&entry.1292438233=%20%20Rigorous%20Verification%20and%20Validation%20%28V%26V%29%20of%20Autonomous%20Driving%20Functions%0A%28ADFs%29%20is%20paramount%20for%20ensuring%20the%20safety%20and%20public%20acceptance%20of%20Autonomous%0AVehicles%20%28AVs%29.%20Current%20validation%20relies%20heavily%20on%20simulation%20to%20achieve%0Asufficient%20test%20coverage%20within%20the%20Operational%20Design%20Domain%20%28ODD%29%20of%20a%0Avehicle%2C%20but%20exhaustively%20exploring%20the%20vast%20parameter%20space%20of%20possible%0Ascenarios%20is%20computationally%20expensive%20and%20time-consuming.%20This%20work%20introduces%0Aa%20framework%20based%20on%20Bayesian%20Optimization%20%28BO%29%20to%20accelerate%20the%20discovery%20of%0Acritical%20scenarios.%20We%20demonstrate%20the%20effectiveness%20of%20the%20framework%20on%20an%0AModel%20Predictive%20Controller%20%28MPC%29-based%20motion%20planner%2C%20showing%20that%20it%0Aidentifies%20hazardous%20situations%2C%20such%20as%20off-road%20events%2C%20using%20orders%20of%0Amagnitude%20fewer%20simulations%20than%20brute-force%20Design%20of%20Experiments%20%28DoE%29%0Amethods.%20Furthermore%2C%20this%20study%20investigates%20the%20scalability%20of%20the%20framework%0Ain%20higher-dimensional%20parameter%20spaces%20and%20its%20ability%20to%20identify%20multiple%2C%0Adistinct%20critical%20regions%20within%20the%20ODD%20of%20the%20motion%20planner%20used%20as%20the%20case%0Astudy%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22769v1&entry.124074799=Read"},
{"title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User\n  Focused Roadside Perception", "author": "Jonas Mirlach and Lei Wan and Andreas Wiedholz and Hannan Ejaz Keen and Andreas Eich", "abstract": "  In autonomous driving, the integration of roadside perception systems is\nessential for overcoming occlusion challenges and enhancing the safety of\nVulnerable Road Users(VRUs). While LiDAR and visual (RGB) sensors are commonly\nused, thermal imaging remains underrepresented in datasets, despite its\nacknowledged advantages for VRU detection in extreme lighting conditions. In\nthis paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and\nthermal imaging from a roadside perspective, with a strong focus on VRUs.\nR-LiViT captures three intersections during both day and night, ensuring a\ndiverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and\nspatially aligned RGB and thermal images across 150 traffic scenarios, with 7\nand 8 annotated classes respectively, providing a comprehensive resource for\ntasks such as object detection and tracking. The dataset and the code for\nreproducing our evaluation results are made publicly available.\n", "link": "http://arxiv.org/abs/2503.17122v3", "date": "2025-07-30", "relevancy": 2.1376, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5654}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R-LiViT%3A%20A%20LiDAR-Visual-Thermal%20Dataset%20Enabling%20Vulnerable%20Road%20User%0A%20%20Focused%20Roadside%20Perception&body=Title%3A%20R-LiViT%3A%20A%20LiDAR-Visual-Thermal%20Dataset%20Enabling%20Vulnerable%20Road%20User%0A%20%20Focused%20Roadside%20Perception%0AAuthor%3A%20Jonas%20Mirlach%20and%20Lei%20Wan%20and%20Andreas%20Wiedholz%20and%20Hannan%20Ejaz%20Keen%20and%20Andreas%20Eich%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20the%20integration%20of%20roadside%20perception%20systems%20is%0Aessential%20for%20overcoming%20occlusion%20challenges%20and%20enhancing%20the%20safety%20of%0AVulnerable%20Road%20Users%28VRUs%29.%20While%20LiDAR%20and%20visual%20%28RGB%29%20sensors%20are%20commonly%0Aused%2C%20thermal%20imaging%20remains%20underrepresented%20in%20datasets%2C%20despite%20its%0Aacknowledged%20advantages%20for%20VRU%20detection%20in%20extreme%20lighting%20conditions.%20In%0Athis%20paper%2C%20we%20present%20R-LiViT%2C%20the%20first%20dataset%20to%20combine%20LiDAR%2C%20RGB%2C%20and%0Athermal%20imaging%20from%20a%20roadside%20perspective%2C%20with%20a%20strong%20focus%20on%20VRUs.%0AR-LiViT%20captures%20three%20intersections%20during%20both%20day%20and%20night%2C%20ensuring%20a%0Adiverse%20dataset.%20It%20includes%2010%2C000%20LiDAR%20frames%20and%202%2C400%20temporally%20and%0Aspatially%20aligned%20RGB%20and%20thermal%20images%20across%20150%20traffic%20scenarios%2C%20with%207%0Aand%208%20annotated%20classes%20respectively%2C%20providing%20a%20comprehensive%20resource%20for%0Atasks%20such%20as%20object%20detection%20and%20tracking.%20The%20dataset%20and%20the%20code%20for%0Areproducing%20our%20evaluation%20results%20are%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17122v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR-LiViT%253A%2520A%2520LiDAR-Visual-Thermal%2520Dataset%2520Enabling%2520Vulnerable%2520Road%2520User%250A%2520%2520Focused%2520Roadside%2520Perception%26entry.906535625%3DJonas%2520Mirlach%2520and%2520Lei%2520Wan%2520and%2520Andreas%2520Wiedholz%2520and%2520Hannan%2520Ejaz%2520Keen%2520and%2520Andreas%2520Eich%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520the%2520integration%2520of%2520roadside%2520perception%2520systems%2520is%250Aessential%2520for%2520overcoming%2520occlusion%2520challenges%2520and%2520enhancing%2520the%2520safety%2520of%250AVulnerable%2520Road%2520Users%2528VRUs%2529.%2520While%2520LiDAR%2520and%2520visual%2520%2528RGB%2529%2520sensors%2520are%2520commonly%250Aused%252C%2520thermal%2520imaging%2520remains%2520underrepresented%2520in%2520datasets%252C%2520despite%2520its%250Aacknowledged%2520advantages%2520for%2520VRU%2520detection%2520in%2520extreme%2520lighting%2520conditions.%2520In%250Athis%2520paper%252C%2520we%2520present%2520R-LiViT%252C%2520the%2520first%2520dataset%2520to%2520combine%2520LiDAR%252C%2520RGB%252C%2520and%250Athermal%2520imaging%2520from%2520a%2520roadside%2520perspective%252C%2520with%2520a%2520strong%2520focus%2520on%2520VRUs.%250AR-LiViT%2520captures%2520three%2520intersections%2520during%2520both%2520day%2520and%2520night%252C%2520ensuring%2520a%250Adiverse%2520dataset.%2520It%2520includes%252010%252C000%2520LiDAR%2520frames%2520and%25202%252C400%2520temporally%2520and%250Aspatially%2520aligned%2520RGB%2520and%2520thermal%2520images%2520across%2520150%2520traffic%2520scenarios%252C%2520with%25207%250Aand%25208%2520annotated%2520classes%2520respectively%252C%2520providing%2520a%2520comprehensive%2520resource%2520for%250Atasks%2520such%2520as%2520object%2520detection%2520and%2520tracking.%2520The%2520dataset%2520and%2520the%2520code%2520for%250Areproducing%2520our%2520evaluation%2520results%2520are%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17122v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R-LiViT%3A%20A%20LiDAR-Visual-Thermal%20Dataset%20Enabling%20Vulnerable%20Road%20User%0A%20%20Focused%20Roadside%20Perception&entry.906535625=Jonas%20Mirlach%20and%20Lei%20Wan%20and%20Andreas%20Wiedholz%20and%20Hannan%20Ejaz%20Keen%20and%20Andreas%20Eich&entry.1292438233=%20%20In%20autonomous%20driving%2C%20the%20integration%20of%20roadside%20perception%20systems%20is%0Aessential%20for%20overcoming%20occlusion%20challenges%20and%20enhancing%20the%20safety%20of%0AVulnerable%20Road%20Users%28VRUs%29.%20While%20LiDAR%20and%20visual%20%28RGB%29%20sensors%20are%20commonly%0Aused%2C%20thermal%20imaging%20remains%20underrepresented%20in%20datasets%2C%20despite%20its%0Aacknowledged%20advantages%20for%20VRU%20detection%20in%20extreme%20lighting%20conditions.%20In%0Athis%20paper%2C%20we%20present%20R-LiViT%2C%20the%20first%20dataset%20to%20combine%20LiDAR%2C%20RGB%2C%20and%0Athermal%20imaging%20from%20a%20roadside%20perspective%2C%20with%20a%20strong%20focus%20on%20VRUs.%0AR-LiViT%20captures%20three%20intersections%20during%20both%20day%20and%20night%2C%20ensuring%20a%0Adiverse%20dataset.%20It%20includes%2010%2C000%20LiDAR%20frames%20and%202%2C400%20temporally%20and%0Aspatially%20aligned%20RGB%20and%20thermal%20images%20across%20150%20traffic%20scenarios%2C%20with%207%0Aand%208%20annotated%20classes%20respectively%2C%20providing%20a%20comprehensive%20resource%20for%0Atasks%20such%20as%20object%20detection%20and%20tracking.%20The%20dataset%20and%20the%20code%20for%0Areproducing%20our%20evaluation%20results%20are%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17122v3&entry.124074799=Read"},
{"title": "Utilizing Evolution Strategies to Train Transformers in Reinforcement\n  Learning", "author": "Maty\u00e1\u0161 Lorenc and Roman Neruda", "abstract": "  We explore the capability of evolution strategies to train an agent with a\npolicy based on a transformer architecture in a reinforcement learning setting.\nWe performed experiments using OpenAI's highly parallelizable evolution\nstrategy to train Decision Transformer in the MuJoCo Humanoid locomotion\nenvironment and in the environment of Atari games, testing the ability of this\nblack-box optimization technique to train even such relatively large and\ncomplicated models (compared to those previously tested in the literature). The\nexamined evolution strategy proved to be, in general, capable of achieving\nstrong results and managed to produce high-performing agents, showcasing\nevolution's ability to tackle the training of even such complex models.\n", "link": "http://arxiv.org/abs/2501.13883v2", "date": "2025-07-30", "relevancy": 2.1369, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5502}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Evolution%20Strategies%20to%20Train%20Transformers%20in%20Reinforcement%0A%20%20Learning&body=Title%3A%20Utilizing%20Evolution%20Strategies%20to%20Train%20Transformers%20in%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Maty%C3%A1%C5%A1%20Lorenc%20and%20Roman%20Neruda%0AAbstract%3A%20%20%20We%20explore%20the%20capability%20of%20evolution%20strategies%20to%20train%20an%20agent%20with%20a%0Apolicy%20based%20on%20a%20transformer%20architecture%20in%20a%20reinforcement%20learning%20setting.%0AWe%20performed%20experiments%20using%20OpenAI%27s%20highly%20parallelizable%20evolution%0Astrategy%20to%20train%20Decision%20Transformer%20in%20the%20MuJoCo%20Humanoid%20locomotion%0Aenvironment%20and%20in%20the%20environment%20of%20Atari%20games%2C%20testing%20the%20ability%20of%20this%0Ablack-box%20optimization%20technique%20to%20train%20even%20such%20relatively%20large%20and%0Acomplicated%20models%20%28compared%20to%20those%20previously%20tested%20in%20the%20literature%29.%20The%0Aexamined%20evolution%20strategy%20proved%20to%20be%2C%20in%20general%2C%20capable%20of%20achieving%0Astrong%20results%20and%20managed%20to%20produce%20high-performing%20agents%2C%20showcasing%0Aevolution%27s%20ability%20to%20tackle%20the%20training%20of%20even%20such%20complex%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Evolution%2520Strategies%2520to%2520Train%2520Transformers%2520in%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMaty%25C3%25A1%25C5%25A1%2520Lorenc%2520and%2520Roman%2520Neruda%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520capability%2520of%2520evolution%2520strategies%2520to%2520train%2520an%2520agent%2520with%2520a%250Apolicy%2520based%2520on%2520a%2520transformer%2520architecture%2520in%2520a%2520reinforcement%2520learning%2520setting.%250AWe%2520performed%2520experiments%2520using%2520OpenAI%2527s%2520highly%2520parallelizable%2520evolution%250Astrategy%2520to%2520train%2520Decision%2520Transformer%2520in%2520the%2520MuJoCo%2520Humanoid%2520locomotion%250Aenvironment%2520and%2520in%2520the%2520environment%2520of%2520Atari%2520games%252C%2520testing%2520the%2520ability%2520of%2520this%250Ablack-box%2520optimization%2520technique%2520to%2520train%2520even%2520such%2520relatively%2520large%2520and%250Acomplicated%2520models%2520%2528compared%2520to%2520those%2520previously%2520tested%2520in%2520the%2520literature%2529.%2520The%250Aexamined%2520evolution%2520strategy%2520proved%2520to%2520be%252C%2520in%2520general%252C%2520capable%2520of%2520achieving%250Astrong%2520results%2520and%2520managed%2520to%2520produce%2520high-performing%2520agents%252C%2520showcasing%250Aevolution%2527s%2520ability%2520to%2520tackle%2520the%2520training%2520of%2520even%2520such%2520complex%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Evolution%20Strategies%20to%20Train%20Transformers%20in%20Reinforcement%0A%20%20Learning&entry.906535625=Maty%C3%A1%C5%A1%20Lorenc%20and%20Roman%20Neruda&entry.1292438233=%20%20We%20explore%20the%20capability%20of%20evolution%20strategies%20to%20train%20an%20agent%20with%20a%0Apolicy%20based%20on%20a%20transformer%20architecture%20in%20a%20reinforcement%20learning%20setting.%0AWe%20performed%20experiments%20using%20OpenAI%27s%20highly%20parallelizable%20evolution%0Astrategy%20to%20train%20Decision%20Transformer%20in%20the%20MuJoCo%20Humanoid%20locomotion%0Aenvironment%20and%20in%20the%20environment%20of%20Atari%20games%2C%20testing%20the%20ability%20of%20this%0Ablack-box%20optimization%20technique%20to%20train%20even%20such%20relatively%20large%20and%0Acomplicated%20models%20%28compared%20to%20those%20previously%20tested%20in%20the%20literature%29.%20The%0Aexamined%20evolution%20strategy%20proved%20to%20be%2C%20in%20general%2C%20capable%20of%20achieving%0Astrong%20results%20and%20managed%20to%20produce%20high-performing%20agents%2C%20showcasing%0Aevolution%27s%20ability%20to%20tackle%20the%20training%20of%20even%20such%20complex%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13883v2&entry.124074799=Read"},
{"title": "UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot\n  Ground Vehicle Repeat", "author": "Desiree Fisker and Alexander Krawciw and Sven Lilge and Melissa Greeff and Timothy D. Barfoot", "abstract": "  This paper presents Virtual Teach and Repeat (VirT&R): an extension of the\nTeach and Repeat (T&R) framework that enables GPS-denied, zero-shot autonomous\nground vehicle navigation in untraversed environments. VirT&R leverages aerial\nimagery captured for a target environment to train a Neural Radiance Field\n(NeRF) model so that dense point clouds and photo-textured meshes can be\nextracted. The NeRF mesh is used to create a high-fidelity simulation of the\nenvironment for piloting an unmanned ground vehicle (UGV) to virtually define a\ndesired path. The mission can then be executed in the actual target environment\nby using NeRF-generated point cloud submaps associated along the path and an\nexisting LiDAR Teach and Repeat (LT&R) framework. We benchmark the\nrepeatability of VirT&R on over 12 km of autonomous driving data using physical\nmarkings that allow a sim-to-real lateral path-tracking error to be obtained\nand compared with LT&R. VirT&R achieved measured root mean squared errors\n(RMSE) of 19.5 cm and 18.4 cm in two different environments, which are slightly\nless than one tire width (24 cm) on the robot used for testing, and respective\nmaximum errors were 39.4 cm and 47.6 cm. This was done using only the\nNeRF-derived teach map, demonstrating that VirT&R has similar closed-loop\npath-tracking performance to LT&R but does not require a human to manually\nteach the path to the UGV in the actual environment.\n", "link": "http://arxiv.org/abs/2505.16912v2", "date": "2025-07-30", "relevancy": 2.1364, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5337}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAV%20See%2C%20UGV%20Do%3A%20Aerial%20Imagery%20and%20Virtual%20Teach%20Enabling%20Zero-Shot%0A%20%20Ground%20Vehicle%20Repeat&body=Title%3A%20UAV%20See%2C%20UGV%20Do%3A%20Aerial%20Imagery%20and%20Virtual%20Teach%20Enabling%20Zero-Shot%0A%20%20Ground%20Vehicle%20Repeat%0AAuthor%3A%20Desiree%20Fisker%20and%20Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Melissa%20Greeff%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20This%20paper%20presents%20Virtual%20Teach%20and%20Repeat%20%28VirT%26R%29%3A%20an%20extension%20of%20the%0ATeach%20and%20Repeat%20%28T%26R%29%20framework%20that%20enables%20GPS-denied%2C%20zero-shot%20autonomous%0Aground%20vehicle%20navigation%20in%20untraversed%20environments.%20VirT%26R%20leverages%20aerial%0Aimagery%20captured%20for%20a%20target%20environment%20to%20train%20a%20Neural%20Radiance%20Field%0A%28NeRF%29%20model%20so%20that%20dense%20point%20clouds%20and%20photo-textured%20meshes%20can%20be%0Aextracted.%20The%20NeRF%20mesh%20is%20used%20to%20create%20a%20high-fidelity%20simulation%20of%20the%0Aenvironment%20for%20piloting%20an%20unmanned%20ground%20vehicle%20%28UGV%29%20to%20virtually%20define%20a%0Adesired%20path.%20The%20mission%20can%20then%20be%20executed%20in%20the%20actual%20target%20environment%0Aby%20using%20NeRF-generated%20point%20cloud%20submaps%20associated%20along%20the%20path%20and%20an%0Aexisting%20LiDAR%20Teach%20and%20Repeat%20%28LT%26R%29%20framework.%20We%20benchmark%20the%0Arepeatability%20of%20VirT%26R%20on%20over%2012%20km%20of%20autonomous%20driving%20data%20using%20physical%0Amarkings%20that%20allow%20a%20sim-to-real%20lateral%20path-tracking%20error%20to%20be%20obtained%0Aand%20compared%20with%20LT%26R.%20VirT%26R%20achieved%20measured%20root%20mean%20squared%20errors%0A%28RMSE%29%20of%2019.5%20cm%20and%2018.4%20cm%20in%20two%20different%20environments%2C%20which%20are%20slightly%0Aless%20than%20one%20tire%20width%20%2824%20cm%29%20on%20the%20robot%20used%20for%20testing%2C%20and%20respective%0Amaximum%20errors%20were%2039.4%20cm%20and%2047.6%20cm.%20This%20was%20done%20using%20only%20the%0ANeRF-derived%20teach%20map%2C%20demonstrating%20that%20VirT%26R%20has%20similar%20closed-loop%0Apath-tracking%20performance%20to%20LT%26R%20but%20does%20not%20require%20a%20human%20to%20manually%0Ateach%20the%20path%20to%20the%20UGV%20in%20the%20actual%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAV%2520See%252C%2520UGV%2520Do%253A%2520Aerial%2520Imagery%2520and%2520Virtual%2520Teach%2520Enabling%2520Zero-Shot%250A%2520%2520Ground%2520Vehicle%2520Repeat%26entry.906535625%3DDesiree%2520Fisker%2520and%2520Alexander%2520Krawciw%2520and%2520Sven%2520Lilge%2520and%2520Melissa%2520Greeff%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Virtual%2520Teach%2520and%2520Repeat%2520%2528VirT%2526R%2529%253A%2520an%2520extension%2520of%2520the%250ATeach%2520and%2520Repeat%2520%2528T%2526R%2529%2520framework%2520that%2520enables%2520GPS-denied%252C%2520zero-shot%2520autonomous%250Aground%2520vehicle%2520navigation%2520in%2520untraversed%2520environments.%2520VirT%2526R%2520leverages%2520aerial%250Aimagery%2520captured%2520for%2520a%2520target%2520environment%2520to%2520train%2520a%2520Neural%2520Radiance%2520Field%250A%2528NeRF%2529%2520model%2520so%2520that%2520dense%2520point%2520clouds%2520and%2520photo-textured%2520meshes%2520can%2520be%250Aextracted.%2520The%2520NeRF%2520mesh%2520is%2520used%2520to%2520create%2520a%2520high-fidelity%2520simulation%2520of%2520the%250Aenvironment%2520for%2520piloting%2520an%2520unmanned%2520ground%2520vehicle%2520%2528UGV%2529%2520to%2520virtually%2520define%2520a%250Adesired%2520path.%2520The%2520mission%2520can%2520then%2520be%2520executed%2520in%2520the%2520actual%2520target%2520environment%250Aby%2520using%2520NeRF-generated%2520point%2520cloud%2520submaps%2520associated%2520along%2520the%2520path%2520and%2520an%250Aexisting%2520LiDAR%2520Teach%2520and%2520Repeat%2520%2528LT%2526R%2529%2520framework.%2520We%2520benchmark%2520the%250Arepeatability%2520of%2520VirT%2526R%2520on%2520over%252012%2520km%2520of%2520autonomous%2520driving%2520data%2520using%2520physical%250Amarkings%2520that%2520allow%2520a%2520sim-to-real%2520lateral%2520path-tracking%2520error%2520to%2520be%2520obtained%250Aand%2520compared%2520with%2520LT%2526R.%2520VirT%2526R%2520achieved%2520measured%2520root%2520mean%2520squared%2520errors%250A%2528RMSE%2529%2520of%252019.5%2520cm%2520and%252018.4%2520cm%2520in%2520two%2520different%2520environments%252C%2520which%2520are%2520slightly%250Aless%2520than%2520one%2520tire%2520width%2520%252824%2520cm%2529%2520on%2520the%2520robot%2520used%2520for%2520testing%252C%2520and%2520respective%250Amaximum%2520errors%2520were%252039.4%2520cm%2520and%252047.6%2520cm.%2520This%2520was%2520done%2520using%2520only%2520the%250ANeRF-derived%2520teach%2520map%252C%2520demonstrating%2520that%2520VirT%2526R%2520has%2520similar%2520closed-loop%250Apath-tracking%2520performance%2520to%2520LT%2526R%2520but%2520does%2520not%2520require%2520a%2520human%2520to%2520manually%250Ateach%2520the%2520path%2520to%2520the%2520UGV%2520in%2520the%2520actual%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAV%20See%2C%20UGV%20Do%3A%20Aerial%20Imagery%20and%20Virtual%20Teach%20Enabling%20Zero-Shot%0A%20%20Ground%20Vehicle%20Repeat&entry.906535625=Desiree%20Fisker%20and%20Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Melissa%20Greeff%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20This%20paper%20presents%20Virtual%20Teach%20and%20Repeat%20%28VirT%26R%29%3A%20an%20extension%20of%20the%0ATeach%20and%20Repeat%20%28T%26R%29%20framework%20that%20enables%20GPS-denied%2C%20zero-shot%20autonomous%0Aground%20vehicle%20navigation%20in%20untraversed%20environments.%20VirT%26R%20leverages%20aerial%0Aimagery%20captured%20for%20a%20target%20environment%20to%20train%20a%20Neural%20Radiance%20Field%0A%28NeRF%29%20model%20so%20that%20dense%20point%20clouds%20and%20photo-textured%20meshes%20can%20be%0Aextracted.%20The%20NeRF%20mesh%20is%20used%20to%20create%20a%20high-fidelity%20simulation%20of%20the%0Aenvironment%20for%20piloting%20an%20unmanned%20ground%20vehicle%20%28UGV%29%20to%20virtually%20define%20a%0Adesired%20path.%20The%20mission%20can%20then%20be%20executed%20in%20the%20actual%20target%20environment%0Aby%20using%20NeRF-generated%20point%20cloud%20submaps%20associated%20along%20the%20path%20and%20an%0Aexisting%20LiDAR%20Teach%20and%20Repeat%20%28LT%26R%29%20framework.%20We%20benchmark%20the%0Arepeatability%20of%20VirT%26R%20on%20over%2012%20km%20of%20autonomous%20driving%20data%20using%20physical%0Amarkings%20that%20allow%20a%20sim-to-real%20lateral%20path-tracking%20error%20to%20be%20obtained%0Aand%20compared%20with%20LT%26R.%20VirT%26R%20achieved%20measured%20root%20mean%20squared%20errors%0A%28RMSE%29%20of%2019.5%20cm%20and%2018.4%20cm%20in%20two%20different%20environments%2C%20which%20are%20slightly%0Aless%20than%20one%20tire%20width%20%2824%20cm%29%20on%20the%20robot%20used%20for%20testing%2C%20and%20respective%0Amaximum%20errors%20were%2039.4%20cm%20and%2047.6%20cm.%20This%20was%20done%20using%20only%20the%0ANeRF-derived%20teach%20map%2C%20demonstrating%20that%20VirT%26R%20has%20similar%20closed-loop%0Apath-tracking%20performance%20to%20LT%26R%20but%20does%20not%20require%20a%20human%20to%20manually%0Ateach%20the%20path%20to%20the%20UGV%20in%20the%20actual%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16912v2&entry.124074799=Read"},
{"title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer", "author": "Junyu Wu and Weiming Chang and Xiaotao Liu and Guanyou He and Haoqiang Hong and Boqi Liu and Hongtao Tian and Tao Yang and Yunsheng Shi and Feng Lin and Ting Yao", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels.\n", "link": "http://arxiv.org/abs/2507.22789v1", "date": "2025-07-30", "relevancy": 2.1328, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5546}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5328}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Core%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer&body=Title%3A%20G-Core%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer%0AAuthor%3A%20Junyu%20Wu%20and%20Weiming%20Chang%20and%20Xiaotao%20Liu%20and%20Guanyou%20He%20and%20Haoqiang%20Hong%20and%20Boqi%20Liu%20and%20Hongtao%20Tian%20and%20Tao%20Yang%20and%20Yunsheng%20Shi%20and%20Feng%20Lin%20and%20Ting%20Yao%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20an%20increasingly%0Apopular%20paradigm%20for%20training%20large%20language%20models%20%28LLMs%29%20and%20diffusion%0Amodels.%20While%20existing%20RLHF%20training%20systems%20have%20enabled%20significant%20progress%2C%0Athey%20often%20face%20challenges%20in%20scaling%20to%20multi-modal%20and%20diffusion%20workflows%0Aand%20adapting%20to%20dynamic%20workloads.%20In%20particular%2C%20current%20approaches%20may%0Aencounter%20limitations%20in%20controller%20scalability%2C%20flexible%20resource%20placement%2C%0Aand%20efficient%20orchestration%20when%20handling%20complex%20RLHF%20pipelines%2C%20especially%20in%0Ascenarios%20involving%20dynamic%20sampling%20or%20generative%20reward%20modeling.%20In%20this%0Apaper%2C%20we%20present%20%5Ctextbf%7BG-Core%7D%2C%20a%20simple%2C%20scalable%2C%20and%20balanced%20RLHF%0Atraining%20framework%20designed%20to%20address%20these%20challenges.%20G-Core%20introduces%20a%0Aparallel%20controller%20programming%20model%2C%20enabling%20flexible%20and%20efficient%0Aorchestration%20of%20complex%20RLHF%20workflows%20without%20the%20bottlenecks%20of%20a%20single%0Acentralized%20controller.%20Furthermore%2C%20we%20propose%20a%20dynamic%20placement%20schema%20that%0Aadaptively%20partitions%20resources%20and%20schedules%20workloads%2C%20significantly%20reducing%0Ahardware%20idle%20time%20and%20improving%20utilization%2C%20even%20under%20highly%20variable%0Atraining%20conditions.%20G-Core%20has%20successfully%20trained%20models%20that%20support%20WeChat%0Aproduct%20features%20serving%20a%20large-scale%20user%20base%2C%20demonstrating%20its%0Aeffectiveness%20and%20robustness%20in%20real-world%20scenarios.%20Our%20results%20show%20that%0AG-Core%20advances%20the%20state%20of%20the%20art%20in%20RLHF%20training%2C%20providing%20a%20solid%0Afoundation%20for%20future%20research%20and%20deployment%20of%20large-scale%2C%20human-aligned%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Core%253A%2520A%2520Simple%252C%2520Scalable%2520and%2520Balanced%2520RLHF%2520Trainer%26entry.906535625%3DJunyu%2520Wu%2520and%2520Weiming%2520Chang%2520and%2520Xiaotao%2520Liu%2520and%2520Guanyou%2520He%2520and%2520Haoqiang%2520Hong%2520and%2520Boqi%2520Liu%2520and%2520Hongtao%2520Tian%2520and%2520Tao%2520Yang%2520and%2520Yunsheng%2520Shi%2520and%2520Feng%2520Lin%2520and%2520Ting%2520Yao%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520has%2520become%2520an%2520increasingly%250Apopular%2520paradigm%2520for%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520diffusion%250Amodels.%2520While%2520existing%2520RLHF%2520training%2520systems%2520have%2520enabled%2520significant%2520progress%252C%250Athey%2520often%2520face%2520challenges%2520in%2520scaling%2520to%2520multi-modal%2520and%2520diffusion%2520workflows%250Aand%2520adapting%2520to%2520dynamic%2520workloads.%2520In%2520particular%252C%2520current%2520approaches%2520may%250Aencounter%2520limitations%2520in%2520controller%2520scalability%252C%2520flexible%2520resource%2520placement%252C%250Aand%2520efficient%2520orchestration%2520when%2520handling%2520complex%2520RLHF%2520pipelines%252C%2520especially%2520in%250Ascenarios%2520involving%2520dynamic%2520sampling%2520or%2520generative%2520reward%2520modeling.%2520In%2520this%250Apaper%252C%2520we%2520present%2520%255Ctextbf%257BG-Core%257D%252C%2520a%2520simple%252C%2520scalable%252C%2520and%2520balanced%2520RLHF%250Atraining%2520framework%2520designed%2520to%2520address%2520these%2520challenges.%2520G-Core%2520introduces%2520a%250Aparallel%2520controller%2520programming%2520model%252C%2520enabling%2520flexible%2520and%2520efficient%250Aorchestration%2520of%2520complex%2520RLHF%2520workflows%2520without%2520the%2520bottlenecks%2520of%2520a%2520single%250Acentralized%2520controller.%2520Furthermore%252C%2520we%2520propose%2520a%2520dynamic%2520placement%2520schema%2520that%250Aadaptively%2520partitions%2520resources%2520and%2520schedules%2520workloads%252C%2520significantly%2520reducing%250Ahardware%2520idle%2520time%2520and%2520improving%2520utilization%252C%2520even%2520under%2520highly%2520variable%250Atraining%2520conditions.%2520G-Core%2520has%2520successfully%2520trained%2520models%2520that%2520support%2520WeChat%250Aproduct%2520features%2520serving%2520a%2520large-scale%2520user%2520base%252C%2520demonstrating%2520its%250Aeffectiveness%2520and%2520robustness%2520in%2520real-world%2520scenarios.%2520Our%2520results%2520show%2520that%250AG-Core%2520advances%2520the%2520state%2520of%2520the%2520art%2520in%2520RLHF%2520training%252C%2520providing%2520a%2520solid%250Afoundation%2520for%2520future%2520research%2520and%2520deployment%2520of%2520large-scale%252C%2520human-aligned%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Core%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer&entry.906535625=Junyu%20Wu%20and%20Weiming%20Chang%20and%20Xiaotao%20Liu%20and%20Guanyou%20He%20and%20Haoqiang%20Hong%20and%20Boqi%20Liu%20and%20Hongtao%20Tian%20and%20Tao%20Yang%20and%20Yunsheng%20Shi%20and%20Feng%20Lin%20and%20Ting%20Yao&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20an%20increasingly%0Apopular%20paradigm%20for%20training%20large%20language%20models%20%28LLMs%29%20and%20diffusion%0Amodels.%20While%20existing%20RLHF%20training%20systems%20have%20enabled%20significant%20progress%2C%0Athey%20often%20face%20challenges%20in%20scaling%20to%20multi-modal%20and%20diffusion%20workflows%0Aand%20adapting%20to%20dynamic%20workloads.%20In%20particular%2C%20current%20approaches%20may%0Aencounter%20limitations%20in%20controller%20scalability%2C%20flexible%20resource%20placement%2C%0Aand%20efficient%20orchestration%20when%20handling%20complex%20RLHF%20pipelines%2C%20especially%20in%0Ascenarios%20involving%20dynamic%20sampling%20or%20generative%20reward%20modeling.%20In%20this%0Apaper%2C%20we%20present%20%5Ctextbf%7BG-Core%7D%2C%20a%20simple%2C%20scalable%2C%20and%20balanced%20RLHF%0Atraining%20framework%20designed%20to%20address%20these%20challenges.%20G-Core%20introduces%20a%0Aparallel%20controller%20programming%20model%2C%20enabling%20flexible%20and%20efficient%0Aorchestration%20of%20complex%20RLHF%20workflows%20without%20the%20bottlenecks%20of%20a%20single%0Acentralized%20controller.%20Furthermore%2C%20we%20propose%20a%20dynamic%20placement%20schema%20that%0Aadaptively%20partitions%20resources%20and%20schedules%20workloads%2C%20significantly%20reducing%0Ahardware%20idle%20time%20and%20improving%20utilization%2C%20even%20under%20highly%20variable%0Atraining%20conditions.%20G-Core%20has%20successfully%20trained%20models%20that%20support%20WeChat%0Aproduct%20features%20serving%20a%20large-scale%20user%20base%2C%20demonstrating%20its%0Aeffectiveness%20and%20robustness%20in%20real-world%20scenarios.%20Our%20results%20show%20that%0AG-Core%20advances%20the%20state%20of%20the%20art%20in%20RLHF%20training%2C%20providing%20a%20solid%0Afoundation%20for%20future%20research%20and%20deployment%20of%20large-scale%2C%20human-aligned%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22789v1&entry.124074799=Read"},
{"title": "DeepC4: Deep Conditional Census-Constrained Clustering for Large-scale\n  Multitask Spatial Disaggregation of Urban Morphology", "author": "Joshua Dimasaka and Christian Gei\u00df and Emily So", "abstract": "  To understand our global progress for sustainable development and disaster\nrisk reduction in many developing economies, two recent major initiatives - the\nUniform African Exposure Dataset of the Global Earthquake Model (GEM)\nFoundation and the Modelling Exposure through Earth Observation Routines\n(METEOR) Project - implemented classical spatial disaggregation techniques to\ngenerate large-scale mapping of urban morphology using the information from\nvarious satellite imagery and its derivatives, geospatial datasets of the built\nenvironment, and subnational census statistics. However, the local discrepancy\nwith well-validated census statistics and the propagated model uncertainties\nremain a challenge in such coarse-to-fine-grained mapping problems,\nspecifically constrained by weak and conditional label supervision. Therefore,\nwe present Deep Conditional Census-Constrained Clustering (DeepC4), a novel\ndeep learning-based spatial disaggregation approach that incorporates local\ncensus statistics as cluster-level constraints while considering multiple\nconditional label relationships in a joint multitask learning of the patterns\nof satellite imagery. To demonstrate, compared to GEM and METEOR, we enhanced\nthe quality of Rwandan maps of urban morphology, specifically building exposure\nand physical vulnerability, at the third-level administrative unit from the\n2022 census. As the world approaches the conclusion of our global frameworks in\n2030, our work has offered a new deep learning-based mapping technique towards\na spatial auditing of our existing coarse-grained derived information at large\nscales.\n", "link": "http://arxiv.org/abs/2507.22554v1", "date": "2025-07-30", "relevancy": 2.1116, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepC4%3A%20Deep%20Conditional%20Census-Constrained%20Clustering%20for%20Large-scale%0A%20%20Multitask%20Spatial%20Disaggregation%20of%20Urban%20Morphology&body=Title%3A%20DeepC4%3A%20Deep%20Conditional%20Census-Constrained%20Clustering%20for%20Large-scale%0A%20%20Multitask%20Spatial%20Disaggregation%20of%20Urban%20Morphology%0AAuthor%3A%20Joshua%20Dimasaka%20and%20Christian%20Gei%C3%9F%20and%20Emily%20So%0AAbstract%3A%20%20%20To%20understand%20our%20global%20progress%20for%20sustainable%20development%20and%20disaster%0Arisk%20reduction%20in%20many%20developing%20economies%2C%20two%20recent%20major%20initiatives%20-%20the%0AUniform%20African%20Exposure%20Dataset%20of%20the%20Global%20Earthquake%20Model%20%28GEM%29%0AFoundation%20and%20the%20Modelling%20Exposure%20through%20Earth%20Observation%20Routines%0A%28METEOR%29%20Project%20-%20implemented%20classical%20spatial%20disaggregation%20techniques%20to%0Agenerate%20large-scale%20mapping%20of%20urban%20morphology%20using%20the%20information%20from%0Avarious%20satellite%20imagery%20and%20its%20derivatives%2C%20geospatial%20datasets%20of%20the%20built%0Aenvironment%2C%20and%20subnational%20census%20statistics.%20However%2C%20the%20local%20discrepancy%0Awith%20well-validated%20census%20statistics%20and%20the%20propagated%20model%20uncertainties%0Aremain%20a%20challenge%20in%20such%20coarse-to-fine-grained%20mapping%20problems%2C%0Aspecifically%20constrained%20by%20weak%20and%20conditional%20label%20supervision.%20Therefore%2C%0Awe%20present%20Deep%20Conditional%20Census-Constrained%20Clustering%20%28DeepC4%29%2C%20a%20novel%0Adeep%20learning-based%20spatial%20disaggregation%20approach%20that%20incorporates%20local%0Acensus%20statistics%20as%20cluster-level%20constraints%20while%20considering%20multiple%0Aconditional%20label%20relationships%20in%20a%20joint%20multitask%20learning%20of%20the%20patterns%0Aof%20satellite%20imagery.%20To%20demonstrate%2C%20compared%20to%20GEM%20and%20METEOR%2C%20we%20enhanced%0Athe%20quality%20of%20Rwandan%20maps%20of%20urban%20morphology%2C%20specifically%20building%20exposure%0Aand%20physical%20vulnerability%2C%20at%20the%20third-level%20administrative%20unit%20from%20the%0A2022%20census.%20As%20the%20world%20approaches%20the%20conclusion%20of%20our%20global%20frameworks%20in%0A2030%2C%20our%20work%20has%20offered%20a%20new%20deep%20learning-based%20mapping%20technique%20towards%0Aa%20spatial%20auditing%20of%20our%20existing%20coarse-grained%20derived%20information%20at%20large%0Ascales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepC4%253A%2520Deep%2520Conditional%2520Census-Constrained%2520Clustering%2520for%2520Large-scale%250A%2520%2520Multitask%2520Spatial%2520Disaggregation%2520of%2520Urban%2520Morphology%26entry.906535625%3DJoshua%2520Dimasaka%2520and%2520Christian%2520Gei%25C3%259F%2520and%2520Emily%2520So%26entry.1292438233%3D%2520%2520To%2520understand%2520our%2520global%2520progress%2520for%2520sustainable%2520development%2520and%2520disaster%250Arisk%2520reduction%2520in%2520many%2520developing%2520economies%252C%2520two%2520recent%2520major%2520initiatives%2520-%2520the%250AUniform%2520African%2520Exposure%2520Dataset%2520of%2520the%2520Global%2520Earthquake%2520Model%2520%2528GEM%2529%250AFoundation%2520and%2520the%2520Modelling%2520Exposure%2520through%2520Earth%2520Observation%2520Routines%250A%2528METEOR%2529%2520Project%2520-%2520implemented%2520classical%2520spatial%2520disaggregation%2520techniques%2520to%250Agenerate%2520large-scale%2520mapping%2520of%2520urban%2520morphology%2520using%2520the%2520information%2520from%250Avarious%2520satellite%2520imagery%2520and%2520its%2520derivatives%252C%2520geospatial%2520datasets%2520of%2520the%2520built%250Aenvironment%252C%2520and%2520subnational%2520census%2520statistics.%2520However%252C%2520the%2520local%2520discrepancy%250Awith%2520well-validated%2520census%2520statistics%2520and%2520the%2520propagated%2520model%2520uncertainties%250Aremain%2520a%2520challenge%2520in%2520such%2520coarse-to-fine-grained%2520mapping%2520problems%252C%250Aspecifically%2520constrained%2520by%2520weak%2520and%2520conditional%2520label%2520supervision.%2520Therefore%252C%250Awe%2520present%2520Deep%2520Conditional%2520Census-Constrained%2520Clustering%2520%2528DeepC4%2529%252C%2520a%2520novel%250Adeep%2520learning-based%2520spatial%2520disaggregation%2520approach%2520that%2520incorporates%2520local%250Acensus%2520statistics%2520as%2520cluster-level%2520constraints%2520while%2520considering%2520multiple%250Aconditional%2520label%2520relationships%2520in%2520a%2520joint%2520multitask%2520learning%2520of%2520the%2520patterns%250Aof%2520satellite%2520imagery.%2520To%2520demonstrate%252C%2520compared%2520to%2520GEM%2520and%2520METEOR%252C%2520we%2520enhanced%250Athe%2520quality%2520of%2520Rwandan%2520maps%2520of%2520urban%2520morphology%252C%2520specifically%2520building%2520exposure%250Aand%2520physical%2520vulnerability%252C%2520at%2520the%2520third-level%2520administrative%2520unit%2520from%2520the%250A2022%2520census.%2520As%2520the%2520world%2520approaches%2520the%2520conclusion%2520of%2520our%2520global%2520frameworks%2520in%250A2030%252C%2520our%2520work%2520has%2520offered%2520a%2520new%2520deep%2520learning-based%2520mapping%2520technique%2520towards%250Aa%2520spatial%2520auditing%2520of%2520our%2520existing%2520coarse-grained%2520derived%2520information%2520at%2520large%250Ascales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepC4%3A%20Deep%20Conditional%20Census-Constrained%20Clustering%20for%20Large-scale%0A%20%20Multitask%20Spatial%20Disaggregation%20of%20Urban%20Morphology&entry.906535625=Joshua%20Dimasaka%20and%20Christian%20Gei%C3%9F%20and%20Emily%20So&entry.1292438233=%20%20To%20understand%20our%20global%20progress%20for%20sustainable%20development%20and%20disaster%0Arisk%20reduction%20in%20many%20developing%20economies%2C%20two%20recent%20major%20initiatives%20-%20the%0AUniform%20African%20Exposure%20Dataset%20of%20the%20Global%20Earthquake%20Model%20%28GEM%29%0AFoundation%20and%20the%20Modelling%20Exposure%20through%20Earth%20Observation%20Routines%0A%28METEOR%29%20Project%20-%20implemented%20classical%20spatial%20disaggregation%20techniques%20to%0Agenerate%20large-scale%20mapping%20of%20urban%20morphology%20using%20the%20information%20from%0Avarious%20satellite%20imagery%20and%20its%20derivatives%2C%20geospatial%20datasets%20of%20the%20built%0Aenvironment%2C%20and%20subnational%20census%20statistics.%20However%2C%20the%20local%20discrepancy%0Awith%20well-validated%20census%20statistics%20and%20the%20propagated%20model%20uncertainties%0Aremain%20a%20challenge%20in%20such%20coarse-to-fine-grained%20mapping%20problems%2C%0Aspecifically%20constrained%20by%20weak%20and%20conditional%20label%20supervision.%20Therefore%2C%0Awe%20present%20Deep%20Conditional%20Census-Constrained%20Clustering%20%28DeepC4%29%2C%20a%20novel%0Adeep%20learning-based%20spatial%20disaggregation%20approach%20that%20incorporates%20local%0Acensus%20statistics%20as%20cluster-level%20constraints%20while%20considering%20multiple%0Aconditional%20label%20relationships%20in%20a%20joint%20multitask%20learning%20of%20the%20patterns%0Aof%20satellite%20imagery.%20To%20demonstrate%2C%20compared%20to%20GEM%20and%20METEOR%2C%20we%20enhanced%0Athe%20quality%20of%20Rwandan%20maps%20of%20urban%20morphology%2C%20specifically%20building%20exposure%0Aand%20physical%20vulnerability%2C%20at%20the%20third-level%20administrative%20unit%20from%20the%0A2022%20census.%20As%20the%20world%20approaches%20the%20conclusion%20of%20our%20global%20frameworks%20in%0A2030%2C%20our%20work%20has%20offered%20a%20new%20deep%20learning-based%20mapping%20technique%20towards%0Aa%20spatial%20auditing%20of%20our%20existing%20coarse-grained%20derived%20information%20at%20large%0Ascales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22554v1&entry.124074799=Read"},
{"title": "Human-Level Competitive Pok\u00e9mon via Scalable Offline Reinforcement\n  Learning with Transformers", "author": "Jake Grigsby and Yuqi Xie and Justin Sasek and Steven Zheng and Yuke Zhu", "abstract": "  Competitive Pok\\'emon Singles (CPS) is a popular strategy game where players\nlearn to exploit their opponent based on imperfect information in battles that\ncan last more than one hundred stochastic turns. AI research in CPS has been\nled by heuristic tree search and online self-play, but the game may also create\na platform to study adaptive policies trained offline on large datasets. We\ndevelop a pipeline to reconstruct the first-person perspective of an agent from\nlogs saved from the third-person perspective of a spectator, thereby unlocking\na dataset of real human battles spanning more than a decade that grows larger\nevery day. This dataset enables a black-box approach where we train large\nsequence models to adapt to their opponent based solely on their input\ntrajectory while selecting moves without explicit search of any kind. We study\na progression from imitation learning to offline RL and offline fine-tuning on\nself-play data in the hardcore competitive setting of Pok\\'emon's four oldest\n(and most partially observed) game generations. The resulting agents outperform\na recent LLM Agent approach and a strong heuristic search engine. While playing\nanonymously in online battles against humans, our best agents climb to rankings\ninside the top 10% of active players. All agent checkpoints, training details,\ndatasets, and baselines are available at https://metamon.tech.\n", "link": "http://arxiv.org/abs/2504.04395v2", "date": "2025-07-30", "relevancy": 2.1001, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5565}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5071}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Level%20Competitive%20Pok%C3%A9mon%20via%20Scalable%20Offline%20Reinforcement%0A%20%20Learning%20with%20Transformers&body=Title%3A%20Human-Level%20Competitive%20Pok%C3%A9mon%20via%20Scalable%20Offline%20Reinforcement%0A%20%20Learning%20with%20Transformers%0AAuthor%3A%20Jake%20Grigsby%20and%20Yuqi%20Xie%20and%20Justin%20Sasek%20and%20Steven%20Zheng%20and%20Yuke%20Zhu%0AAbstract%3A%20%20%20Competitive%20Pok%5C%27emon%20Singles%20%28CPS%29%20is%20a%20popular%20strategy%20game%20where%20players%0Alearn%20to%20exploit%20their%20opponent%20based%20on%20imperfect%20information%20in%20battles%20that%0Acan%20last%20more%20than%20one%20hundred%20stochastic%20turns.%20AI%20research%20in%20CPS%20has%20been%0Aled%20by%20heuristic%20tree%20search%20and%20online%20self-play%2C%20but%20the%20game%20may%20also%20create%0Aa%20platform%20to%20study%20adaptive%20policies%20trained%20offline%20on%20large%20datasets.%20We%0Adevelop%20a%20pipeline%20to%20reconstruct%20the%20first-person%20perspective%20of%20an%20agent%20from%0Alogs%20saved%20from%20the%20third-person%20perspective%20of%20a%20spectator%2C%20thereby%20unlocking%0Aa%20dataset%20of%20real%20human%20battles%20spanning%20more%20than%20a%20decade%20that%20grows%20larger%0Aevery%20day.%20This%20dataset%20enables%20a%20black-box%20approach%20where%20we%20train%20large%0Asequence%20models%20to%20adapt%20to%20their%20opponent%20based%20solely%20on%20their%20input%0Atrajectory%20while%20selecting%20moves%20without%20explicit%20search%20of%20any%20kind.%20We%20study%0Aa%20progression%20from%20imitation%20learning%20to%20offline%20RL%20and%20offline%20fine-tuning%20on%0Aself-play%20data%20in%20the%20hardcore%20competitive%20setting%20of%20Pok%5C%27emon%27s%20four%20oldest%0A%28and%20most%20partially%20observed%29%20game%20generations.%20The%20resulting%20agents%20outperform%0Aa%20recent%20LLM%20Agent%20approach%20and%20a%20strong%20heuristic%20search%20engine.%20While%20playing%0Aanonymously%20in%20online%20battles%20against%20humans%2C%20our%20best%20agents%20climb%20to%20rankings%0Ainside%20the%20top%2010%25%20of%20active%20players.%20All%20agent%20checkpoints%2C%20training%20details%2C%0Adatasets%2C%20and%20baselines%20are%20available%20at%20https%3A//metamon.tech.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04395v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Level%2520Competitive%2520Pok%25C3%25A9mon%2520via%2520Scalable%2520Offline%2520Reinforcement%250A%2520%2520Learning%2520with%2520Transformers%26entry.906535625%3DJake%2520Grigsby%2520and%2520Yuqi%2520Xie%2520and%2520Justin%2520Sasek%2520and%2520Steven%2520Zheng%2520and%2520Yuke%2520Zhu%26entry.1292438233%3D%2520%2520Competitive%2520Pok%255C%2527emon%2520Singles%2520%2528CPS%2529%2520is%2520a%2520popular%2520strategy%2520game%2520where%2520players%250Alearn%2520to%2520exploit%2520their%2520opponent%2520based%2520on%2520imperfect%2520information%2520in%2520battles%2520that%250Acan%2520last%2520more%2520than%2520one%2520hundred%2520stochastic%2520turns.%2520AI%2520research%2520in%2520CPS%2520has%2520been%250Aled%2520by%2520heuristic%2520tree%2520search%2520and%2520online%2520self-play%252C%2520but%2520the%2520game%2520may%2520also%2520create%250Aa%2520platform%2520to%2520study%2520adaptive%2520policies%2520trained%2520offline%2520on%2520large%2520datasets.%2520We%250Adevelop%2520a%2520pipeline%2520to%2520reconstruct%2520the%2520first-person%2520perspective%2520of%2520an%2520agent%2520from%250Alogs%2520saved%2520from%2520the%2520third-person%2520perspective%2520of%2520a%2520spectator%252C%2520thereby%2520unlocking%250Aa%2520dataset%2520of%2520real%2520human%2520battles%2520spanning%2520more%2520than%2520a%2520decade%2520that%2520grows%2520larger%250Aevery%2520day.%2520This%2520dataset%2520enables%2520a%2520black-box%2520approach%2520where%2520we%2520train%2520large%250Asequence%2520models%2520to%2520adapt%2520to%2520their%2520opponent%2520based%2520solely%2520on%2520their%2520input%250Atrajectory%2520while%2520selecting%2520moves%2520without%2520explicit%2520search%2520of%2520any%2520kind.%2520We%2520study%250Aa%2520progression%2520from%2520imitation%2520learning%2520to%2520offline%2520RL%2520and%2520offline%2520fine-tuning%2520on%250Aself-play%2520data%2520in%2520the%2520hardcore%2520competitive%2520setting%2520of%2520Pok%255C%2527emon%2527s%2520four%2520oldest%250A%2528and%2520most%2520partially%2520observed%2529%2520game%2520generations.%2520The%2520resulting%2520agents%2520outperform%250Aa%2520recent%2520LLM%2520Agent%2520approach%2520and%2520a%2520strong%2520heuristic%2520search%2520engine.%2520While%2520playing%250Aanonymously%2520in%2520online%2520battles%2520against%2520humans%252C%2520our%2520best%2520agents%2520climb%2520to%2520rankings%250Ainside%2520the%2520top%252010%2525%2520of%2520active%2520players.%2520All%2520agent%2520checkpoints%252C%2520training%2520details%252C%250Adatasets%252C%2520and%2520baselines%2520are%2520available%2520at%2520https%253A//metamon.tech.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04395v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Level%20Competitive%20Pok%C3%A9mon%20via%20Scalable%20Offline%20Reinforcement%0A%20%20Learning%20with%20Transformers&entry.906535625=Jake%20Grigsby%20and%20Yuqi%20Xie%20and%20Justin%20Sasek%20and%20Steven%20Zheng%20and%20Yuke%20Zhu&entry.1292438233=%20%20Competitive%20Pok%5C%27emon%20Singles%20%28CPS%29%20is%20a%20popular%20strategy%20game%20where%20players%0Alearn%20to%20exploit%20their%20opponent%20based%20on%20imperfect%20information%20in%20battles%20that%0Acan%20last%20more%20than%20one%20hundred%20stochastic%20turns.%20AI%20research%20in%20CPS%20has%20been%0Aled%20by%20heuristic%20tree%20search%20and%20online%20self-play%2C%20but%20the%20game%20may%20also%20create%0Aa%20platform%20to%20study%20adaptive%20policies%20trained%20offline%20on%20large%20datasets.%20We%0Adevelop%20a%20pipeline%20to%20reconstruct%20the%20first-person%20perspective%20of%20an%20agent%20from%0Alogs%20saved%20from%20the%20third-person%20perspective%20of%20a%20spectator%2C%20thereby%20unlocking%0Aa%20dataset%20of%20real%20human%20battles%20spanning%20more%20than%20a%20decade%20that%20grows%20larger%0Aevery%20day.%20This%20dataset%20enables%20a%20black-box%20approach%20where%20we%20train%20large%0Asequence%20models%20to%20adapt%20to%20their%20opponent%20based%20solely%20on%20their%20input%0Atrajectory%20while%20selecting%20moves%20without%20explicit%20search%20of%20any%20kind.%20We%20study%0Aa%20progression%20from%20imitation%20learning%20to%20offline%20RL%20and%20offline%20fine-tuning%20on%0Aself-play%20data%20in%20the%20hardcore%20competitive%20setting%20of%20Pok%5C%27emon%27s%20four%20oldest%0A%28and%20most%20partially%20observed%29%20game%20generations.%20The%20resulting%20agents%20outperform%0Aa%20recent%20LLM%20Agent%20approach%20and%20a%20strong%20heuristic%20search%20engine.%20While%20playing%0Aanonymously%20in%20online%20battles%20against%20humans%2C%20our%20best%20agents%20climb%20to%20rankings%0Ainside%20the%20top%2010%25%20of%20active%20players.%20All%20agent%20checkpoints%2C%20training%20details%2C%0Adatasets%2C%20and%20baselines%20are%20available%20at%20https%3A//metamon.tech.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04395v2&entry.124074799=Read"},
{"title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource\n  Settings", "author": "Dongli He and Hu Wang and Mohammad Yaqub", "abstract": "  Accurate fetal biometric measurements, such as abdominal circumference, play\na vital role in prenatal care. However, obtaining high-quality ultrasound\nimages for these measurements heavily depends on the expertise of sonographers,\nposing a significant challenge in low-income countries due to the scarcity of\ntrained personnel. To address this issue, we leverage FetalCLIP, a\nvision-language model pretrained on a curated dataset of over 210,000 fetal\nultrasound image-caption pairs, to perform automated fetal ultrasound image\nquality assessment (IQA) on blind-sweep ultrasound data. We introduce\nFetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank\nAdaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN\nand Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of\n0.757. Moreover, we show that an adapted segmentation model, when repurposed\nfor classification, further improves performance, achieving an F1 score of\n0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal\nultrasound foundation models can enable task-specific adaptations, advancing\nprenatal care in resource-limited settings. The experimental code is available\nat: https://github.com/donglihe-hub/FetalCLIP-IQA.\n", "link": "http://arxiv.org/abs/2507.22802v1", "date": "2025-07-30", "relevancy": 2.0975, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5225}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Fetal%20Ultrasound%20Image%20Quality%20Assessment%20in%20Low-Resource%0A%20%20Settings&body=Title%3A%20Advancing%20Fetal%20Ultrasound%20Image%20Quality%20Assessment%20in%20Low-Resource%0A%20%20Settings%0AAuthor%3A%20Dongli%20He%20and%20Hu%20Wang%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Accurate%20fetal%20biometric%20measurements%2C%20such%20as%20abdominal%20circumference%2C%20play%0Aa%20vital%20role%20in%20prenatal%20care.%20However%2C%20obtaining%20high-quality%20ultrasound%0Aimages%20for%20these%20measurements%20heavily%20depends%20on%20the%20expertise%20of%20sonographers%2C%0Aposing%20a%20significant%20challenge%20in%20low-income%20countries%20due%20to%20the%20scarcity%20of%0Atrained%20personnel.%20To%20address%20this%20issue%2C%20we%20leverage%20FetalCLIP%2C%20a%0Avision-language%20model%20pretrained%20on%20a%20curated%20dataset%20of%20over%20210%2C000%20fetal%0Aultrasound%20image-caption%20pairs%2C%20to%20perform%20automated%20fetal%20ultrasound%20image%0Aquality%20assessment%20%28IQA%29%20on%20blind-sweep%20ultrasound%20data.%20We%20introduce%0AFetalCLIP%24_%7BCLS%7D%24%2C%20an%20IQA%20model%20adapted%20from%20FetalCLIP%20using%20Low-Rank%0AAdaptation%20%28LoRA%29%2C%20and%20evaluate%20it%20on%20the%20ACOUSLIC-AI%20dataset%20against%20six%20CNN%0Aand%20Transformer%20baselines.%20FetalCLIP%24_%7BCLS%7D%24%20achieves%20the%20highest%20F1%20score%20of%0A0.757.%20Moreover%2C%20we%20show%20that%20an%20adapted%20segmentation%20model%2C%20when%20repurposed%0Afor%20classification%2C%20further%20improves%20performance%2C%20achieving%20an%20F1%20score%20of%0A0.771.%20Our%20work%20demonstrates%20how%20parameter-efficient%20fine-tuning%20of%20fetal%0Aultrasound%20foundation%20models%20can%20enable%20task-specific%20adaptations%2C%20advancing%0Aprenatal%20care%20in%20resource-limited%20settings.%20The%20experimental%20code%20is%20available%0Aat%3A%20https%3A//github.com/donglihe-hub/FetalCLIP-IQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Fetal%2520Ultrasound%2520Image%2520Quality%2520Assessment%2520in%2520Low-Resource%250A%2520%2520Settings%26entry.906535625%3DDongli%2520He%2520and%2520Hu%2520Wang%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Accurate%2520fetal%2520biometric%2520measurements%252C%2520such%2520as%2520abdominal%2520circumference%252C%2520play%250Aa%2520vital%2520role%2520in%2520prenatal%2520care.%2520However%252C%2520obtaining%2520high-quality%2520ultrasound%250Aimages%2520for%2520these%2520measurements%2520heavily%2520depends%2520on%2520the%2520expertise%2520of%2520sonographers%252C%250Aposing%2520a%2520significant%2520challenge%2520in%2520low-income%2520countries%2520due%2520to%2520the%2520scarcity%2520of%250Atrained%2520personnel.%2520To%2520address%2520this%2520issue%252C%2520we%2520leverage%2520FetalCLIP%252C%2520a%250Avision-language%2520model%2520pretrained%2520on%2520a%2520curated%2520dataset%2520of%2520over%2520210%252C000%2520fetal%250Aultrasound%2520image-caption%2520pairs%252C%2520to%2520perform%2520automated%2520fetal%2520ultrasound%2520image%250Aquality%2520assessment%2520%2528IQA%2529%2520on%2520blind-sweep%2520ultrasound%2520data.%2520We%2520introduce%250AFetalCLIP%2524_%257BCLS%257D%2524%252C%2520an%2520IQA%2520model%2520adapted%2520from%2520FetalCLIP%2520using%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%252C%2520and%2520evaluate%2520it%2520on%2520the%2520ACOUSLIC-AI%2520dataset%2520against%2520six%2520CNN%250Aand%2520Transformer%2520baselines.%2520FetalCLIP%2524_%257BCLS%257D%2524%2520achieves%2520the%2520highest%2520F1%2520score%2520of%250A0.757.%2520Moreover%252C%2520we%2520show%2520that%2520an%2520adapted%2520segmentation%2520model%252C%2520when%2520repurposed%250Afor%2520classification%252C%2520further%2520improves%2520performance%252C%2520achieving%2520an%2520F1%2520score%2520of%250A0.771.%2520Our%2520work%2520demonstrates%2520how%2520parameter-efficient%2520fine-tuning%2520of%2520fetal%250Aultrasound%2520foundation%2520models%2520can%2520enable%2520task-specific%2520adaptations%252C%2520advancing%250Aprenatal%2520care%2520in%2520resource-limited%2520settings.%2520The%2520experimental%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/donglihe-hub/FetalCLIP-IQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Fetal%20Ultrasound%20Image%20Quality%20Assessment%20in%20Low-Resource%0A%20%20Settings&entry.906535625=Dongli%20He%20and%20Hu%20Wang%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Accurate%20fetal%20biometric%20measurements%2C%20such%20as%20abdominal%20circumference%2C%20play%0Aa%20vital%20role%20in%20prenatal%20care.%20However%2C%20obtaining%20high-quality%20ultrasound%0Aimages%20for%20these%20measurements%20heavily%20depends%20on%20the%20expertise%20of%20sonographers%2C%0Aposing%20a%20significant%20challenge%20in%20low-income%20countries%20due%20to%20the%20scarcity%20of%0Atrained%20personnel.%20To%20address%20this%20issue%2C%20we%20leverage%20FetalCLIP%2C%20a%0Avision-language%20model%20pretrained%20on%20a%20curated%20dataset%20of%20over%20210%2C000%20fetal%0Aultrasound%20image-caption%20pairs%2C%20to%20perform%20automated%20fetal%20ultrasound%20image%0Aquality%20assessment%20%28IQA%29%20on%20blind-sweep%20ultrasound%20data.%20We%20introduce%0AFetalCLIP%24_%7BCLS%7D%24%2C%20an%20IQA%20model%20adapted%20from%20FetalCLIP%20using%20Low-Rank%0AAdaptation%20%28LoRA%29%2C%20and%20evaluate%20it%20on%20the%20ACOUSLIC-AI%20dataset%20against%20six%20CNN%0Aand%20Transformer%20baselines.%20FetalCLIP%24_%7BCLS%7D%24%20achieves%20the%20highest%20F1%20score%20of%0A0.757.%20Moreover%2C%20we%20show%20that%20an%20adapted%20segmentation%20model%2C%20when%20repurposed%0Afor%20classification%2C%20further%20improves%20performance%2C%20achieving%20an%20F1%20score%20of%0A0.771.%20Our%20work%20demonstrates%20how%20parameter-efficient%20fine-tuning%20of%20fetal%0Aultrasound%20foundation%20models%20can%20enable%20task-specific%20adaptations%2C%20advancing%0Aprenatal%20care%20in%20resource-limited%20settings.%20The%20experimental%20code%20is%20available%0Aat%3A%20https%3A//github.com/donglihe-hub/FetalCLIP-IQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22802v1&entry.124074799=Read"},
{"title": "SpectraSentinel: LightWeight Dual-Stream Real-Time Drone Detection,\n  Tracking and Payload Identification", "author": "Shahriar Kabir and Istiak Ahmmed Rifti and H. M. Shadman Tabib and Mushfiqur Rahman and Sadatul Islam Sadi and Hasnaen Adil and Ahmed Mahir Sultan Rumi and Ch Md Rakin Haider", "abstract": "  The proliferation of drones in civilian airspace has raised urgent security\nconcerns, necessitating robust real-time surveillance systems. In response to\nthe 2025 VIP Cup challenge tasks - drone detection, tracking, and payload\nidentification - we propose a dual-stream drone monitoring framework. Our\napproach deploys independent You Only Look Once v11-nano (YOLOv11n) object\ndetectors on parallel infrared (thermal) and visible (RGB) data streams,\ndeliberately avoiding early fusion. This separation allows each model to be\nspecifically optimized for the distinct characteristics of its input modality,\naddressing the unique challenges posed by small aerial objects in diverse\nenvironmental conditions. We customize data preprocessing and augmentation\nstrategies per domain - such as limiting color jitter for IR imagery - and\nfine-tune training hyperparameters to enhance detection performance under\nconditions of heavy noise, low light, and motion blur. The resulting\nlightweight YOLOv11n models demonstrate high accuracy in distinguishing drones\nfrom birds and in classifying payload types, all while maintaining real-time\nperformance. This report details the rationale for a dual-modality design, the\nspecialized training pipelines, and the architectural optimizations that\ncollectively enable efficient and accurate drone surveillance across RGB and IR\nchannels.\n", "link": "http://arxiv.org/abs/2507.22650v1", "date": "2025-07-30", "relevancy": 2.0953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectraSentinel%3A%20LightWeight%20Dual-Stream%20Real-Time%20Drone%20Detection%2C%0A%20%20Tracking%20and%20Payload%20Identification&body=Title%3A%20SpectraSentinel%3A%20LightWeight%20Dual-Stream%20Real-Time%20Drone%20Detection%2C%0A%20%20Tracking%20and%20Payload%20Identification%0AAuthor%3A%20Shahriar%20Kabir%20and%20Istiak%20Ahmmed%20Rifti%20and%20H.%20M.%20Shadman%20Tabib%20and%20Mushfiqur%20Rahman%20and%20Sadatul%20Islam%20Sadi%20and%20Hasnaen%20Adil%20and%20Ahmed%20Mahir%20Sultan%20Rumi%20and%20Ch%20Md%20Rakin%20Haider%0AAbstract%3A%20%20%20The%20proliferation%20of%20drones%20in%20civilian%20airspace%20has%20raised%20urgent%20security%0Aconcerns%2C%20necessitating%20robust%20real-time%20surveillance%20systems.%20In%20response%20to%0Athe%202025%20VIP%20Cup%20challenge%20tasks%20-%20drone%20detection%2C%20tracking%2C%20and%20payload%0Aidentification%20-%20we%20propose%20a%20dual-stream%20drone%20monitoring%20framework.%20Our%0Aapproach%20deploys%20independent%20You%20Only%20Look%20Once%20v11-nano%20%28YOLOv11n%29%20object%0Adetectors%20on%20parallel%20infrared%20%28thermal%29%20and%20visible%20%28RGB%29%20data%20streams%2C%0Adeliberately%20avoiding%20early%20fusion.%20This%20separation%20allows%20each%20model%20to%20be%0Aspecifically%20optimized%20for%20the%20distinct%20characteristics%20of%20its%20input%20modality%2C%0Aaddressing%20the%20unique%20challenges%20posed%20by%20small%20aerial%20objects%20in%20diverse%0Aenvironmental%20conditions.%20We%20customize%20data%20preprocessing%20and%20augmentation%0Astrategies%20per%20domain%20-%20such%20as%20limiting%20color%20jitter%20for%20IR%20imagery%20-%20and%0Afine-tune%20training%20hyperparameters%20to%20enhance%20detection%20performance%20under%0Aconditions%20of%20heavy%20noise%2C%20low%20light%2C%20and%20motion%20blur.%20The%20resulting%0Alightweight%20YOLOv11n%20models%20demonstrate%20high%20accuracy%20in%20distinguishing%20drones%0Afrom%20birds%20and%20in%20classifying%20payload%20types%2C%20all%20while%20maintaining%20real-time%0Aperformance.%20This%20report%20details%20the%20rationale%20for%20a%20dual-modality%20design%2C%20the%0Aspecialized%20training%20pipelines%2C%20and%20the%20architectural%20optimizations%20that%0Acollectively%20enable%20efficient%20and%20accurate%20drone%20surveillance%20across%20RGB%20and%20IR%0Achannels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectraSentinel%253A%2520LightWeight%2520Dual-Stream%2520Real-Time%2520Drone%2520Detection%252C%250A%2520%2520Tracking%2520and%2520Payload%2520Identification%26entry.906535625%3DShahriar%2520Kabir%2520and%2520Istiak%2520Ahmmed%2520Rifti%2520and%2520H.%2520M.%2520Shadman%2520Tabib%2520and%2520Mushfiqur%2520Rahman%2520and%2520Sadatul%2520Islam%2520Sadi%2520and%2520Hasnaen%2520Adil%2520and%2520Ahmed%2520Mahir%2520Sultan%2520Rumi%2520and%2520Ch%2520Md%2520Rakin%2520Haider%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520drones%2520in%2520civilian%2520airspace%2520has%2520raised%2520urgent%2520security%250Aconcerns%252C%2520necessitating%2520robust%2520real-time%2520surveillance%2520systems.%2520In%2520response%2520to%250Athe%25202025%2520VIP%2520Cup%2520challenge%2520tasks%2520-%2520drone%2520detection%252C%2520tracking%252C%2520and%2520payload%250Aidentification%2520-%2520we%2520propose%2520a%2520dual-stream%2520drone%2520monitoring%2520framework.%2520Our%250Aapproach%2520deploys%2520independent%2520You%2520Only%2520Look%2520Once%2520v11-nano%2520%2528YOLOv11n%2529%2520object%250Adetectors%2520on%2520parallel%2520infrared%2520%2528thermal%2529%2520and%2520visible%2520%2528RGB%2529%2520data%2520streams%252C%250Adeliberately%2520avoiding%2520early%2520fusion.%2520This%2520separation%2520allows%2520each%2520model%2520to%2520be%250Aspecifically%2520optimized%2520for%2520the%2520distinct%2520characteristics%2520of%2520its%2520input%2520modality%252C%250Aaddressing%2520the%2520unique%2520challenges%2520posed%2520by%2520small%2520aerial%2520objects%2520in%2520diverse%250Aenvironmental%2520conditions.%2520We%2520customize%2520data%2520preprocessing%2520and%2520augmentation%250Astrategies%2520per%2520domain%2520-%2520such%2520as%2520limiting%2520color%2520jitter%2520for%2520IR%2520imagery%2520-%2520and%250Afine-tune%2520training%2520hyperparameters%2520to%2520enhance%2520detection%2520performance%2520under%250Aconditions%2520of%2520heavy%2520noise%252C%2520low%2520light%252C%2520and%2520motion%2520blur.%2520The%2520resulting%250Alightweight%2520YOLOv11n%2520models%2520demonstrate%2520high%2520accuracy%2520in%2520distinguishing%2520drones%250Afrom%2520birds%2520and%2520in%2520classifying%2520payload%2520types%252C%2520all%2520while%2520maintaining%2520real-time%250Aperformance.%2520This%2520report%2520details%2520the%2520rationale%2520for%2520a%2520dual-modality%2520design%252C%2520the%250Aspecialized%2520training%2520pipelines%252C%2520and%2520the%2520architectural%2520optimizations%2520that%250Acollectively%2520enable%2520efficient%2520and%2520accurate%2520drone%2520surveillance%2520across%2520RGB%2520and%2520IR%250Achannels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectraSentinel%3A%20LightWeight%20Dual-Stream%20Real-Time%20Drone%20Detection%2C%0A%20%20Tracking%20and%20Payload%20Identification&entry.906535625=Shahriar%20Kabir%20and%20Istiak%20Ahmmed%20Rifti%20and%20H.%20M.%20Shadman%20Tabib%20and%20Mushfiqur%20Rahman%20and%20Sadatul%20Islam%20Sadi%20and%20Hasnaen%20Adil%20and%20Ahmed%20Mahir%20Sultan%20Rumi%20and%20Ch%20Md%20Rakin%20Haider&entry.1292438233=%20%20The%20proliferation%20of%20drones%20in%20civilian%20airspace%20has%20raised%20urgent%20security%0Aconcerns%2C%20necessitating%20robust%20real-time%20surveillance%20systems.%20In%20response%20to%0Athe%202025%20VIP%20Cup%20challenge%20tasks%20-%20drone%20detection%2C%20tracking%2C%20and%20payload%0Aidentification%20-%20we%20propose%20a%20dual-stream%20drone%20monitoring%20framework.%20Our%0Aapproach%20deploys%20independent%20You%20Only%20Look%20Once%20v11-nano%20%28YOLOv11n%29%20object%0Adetectors%20on%20parallel%20infrared%20%28thermal%29%20and%20visible%20%28RGB%29%20data%20streams%2C%0Adeliberately%20avoiding%20early%20fusion.%20This%20separation%20allows%20each%20model%20to%20be%0Aspecifically%20optimized%20for%20the%20distinct%20characteristics%20of%20its%20input%20modality%2C%0Aaddressing%20the%20unique%20challenges%20posed%20by%20small%20aerial%20objects%20in%20diverse%0Aenvironmental%20conditions.%20We%20customize%20data%20preprocessing%20and%20augmentation%0Astrategies%20per%20domain%20-%20such%20as%20limiting%20color%20jitter%20for%20IR%20imagery%20-%20and%0Afine-tune%20training%20hyperparameters%20to%20enhance%20detection%20performance%20under%0Aconditions%20of%20heavy%20noise%2C%20low%20light%2C%20and%20motion%20blur.%20The%20resulting%0Alightweight%20YOLOv11n%20models%20demonstrate%20high%20accuracy%20in%20distinguishing%20drones%0Afrom%20birds%20and%20in%20classifying%20payload%20types%2C%20all%20while%20maintaining%20real-time%0Aperformance.%20This%20report%20details%20the%20rationale%20for%20a%20dual-modality%20design%2C%20the%0Aspecialized%20training%20pipelines%2C%20and%20the%20architectural%20optimizations%20that%0Acollectively%20enable%20efficient%20and%20accurate%20drone%20surveillance%20across%20RGB%20and%20IR%0Achannels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22650v1&entry.124074799=Read"},
{"title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering", "author": "Zhongjian Hu and Peng Yang and Bing Li and Fengyuan Liu", "abstract": "  Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively.\n", "link": "http://arxiv.org/abs/2412.16936v2", "date": "2025-07-30", "relevancy": 2.0847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rationale-guided%20Prompting%20for%20Knowledge-based%20Visual%20Question%20Answering&body=Title%3A%20Rationale-guided%20Prompting%20for%20Knowledge-based%20Visual%20Question%20Answering%0AAuthor%3A%20Zhongjian%20Hu%20and%20Peng%20Yang%20and%20Bing%20Li%20and%20Fengyuan%20Liu%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20used%20for%20knowledge-based%0AVisual%20Question%20Answering%20%28VQA%29.%20Despite%20the%20encouraging%20results%20of%20previous%0Astudies%2C%20prior%20methods%20prompt%20LLMs%20to%20predict%20answers%20directly%2C%20neglecting%0Aintermediate%20thought%20processes.%20We%20argue%20that%20prior%20methods%20do%20not%20sufficiently%0Aactivate%20the%20capacities%20of%20LLMs.%20We%20propose%20a%20framework%20called%20PLRH%20that%0APrompts%20LLMs%20with%20Rationale%20Heuristics%20for%20knowledge-based%20VQA.%20The%20PLRH%0Aprompts%20LLMs%20with%20Chain%20of%20Thought%20%28CoT%29%20to%20generate%20rationale%20heuristics%2C%0Ai.e.%2C%20intermediate%20thought%20processes%2C%20and%20then%20leverages%20the%20rationale%0Aheuristics%20to%20inspire%20LLMs%20to%20predict%20answers.%20Experiments%20show%20that%20our%0Aapproach%20outperforms%20the%20existing%20baselines%20by%20more%20than%202.2%20and%202.1%20on%20OK-VQA%0Aand%20A-OKVQA%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRationale-guided%2520Prompting%2520for%2520Knowledge-based%2520Visual%2520Question%2520Answering%26entry.906535625%3DZhongjian%2520Hu%2520and%2520Peng%2520Yang%2520and%2520Bing%2520Li%2520and%2520Fengyuan%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520used%2520for%2520knowledge-based%250AVisual%2520Question%2520Answering%2520%2528VQA%2529.%2520Despite%2520the%2520encouraging%2520results%2520of%2520previous%250Astudies%252C%2520prior%2520methods%2520prompt%2520LLMs%2520to%2520predict%2520answers%2520directly%252C%2520neglecting%250Aintermediate%2520thought%2520processes.%2520We%2520argue%2520that%2520prior%2520methods%2520do%2520not%2520sufficiently%250Aactivate%2520the%2520capacities%2520of%2520LLMs.%2520We%2520propose%2520a%2520framework%2520called%2520PLRH%2520that%250APrompts%2520LLMs%2520with%2520Rationale%2520Heuristics%2520for%2520knowledge-based%2520VQA.%2520The%2520PLRH%250Aprompts%2520LLMs%2520with%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520to%2520generate%2520rationale%2520heuristics%252C%250Ai.e.%252C%2520intermediate%2520thought%2520processes%252C%2520and%2520then%2520leverages%2520the%2520rationale%250Aheuristics%2520to%2520inspire%2520LLMs%2520to%2520predict%2520answers.%2520Experiments%2520show%2520that%2520our%250Aapproach%2520outperforms%2520the%2520existing%2520baselines%2520by%2520more%2520than%25202.2%2520and%25202.1%2520on%2520OK-VQA%250Aand%2520A-OKVQA%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rationale-guided%20Prompting%20for%20Knowledge-based%20Visual%20Question%20Answering&entry.906535625=Zhongjian%20Hu%20and%20Peng%20Yang%20and%20Bing%20Li%20and%20Fengyuan%20Liu&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20used%20for%20knowledge-based%0AVisual%20Question%20Answering%20%28VQA%29.%20Despite%20the%20encouraging%20results%20of%20previous%0Astudies%2C%20prior%20methods%20prompt%20LLMs%20to%20predict%20answers%20directly%2C%20neglecting%0Aintermediate%20thought%20processes.%20We%20argue%20that%20prior%20methods%20do%20not%20sufficiently%0Aactivate%20the%20capacities%20of%20LLMs.%20We%20propose%20a%20framework%20called%20PLRH%20that%0APrompts%20LLMs%20with%20Rationale%20Heuristics%20for%20knowledge-based%20VQA.%20The%20PLRH%0Aprompts%20LLMs%20with%20Chain%20of%20Thought%20%28CoT%29%20to%20generate%20rationale%20heuristics%2C%0Ai.e.%2C%20intermediate%20thought%20processes%2C%20and%20then%20leverages%20the%20rationale%0Aheuristics%20to%20inspire%20LLMs%20to%20predict%20answers.%20Experiments%20show%20that%20our%0Aapproach%20outperforms%20the%20existing%20baselines%20by%20more%20than%202.2%20and%202.1%20on%20OK-VQA%0Aand%20A-OKVQA%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16936v2&entry.124074799=Read"},
{"title": "Equivariant Flow Matching for Point Cloud Assembly", "author": "Ziming Wang and Nan Xue and Rebecka J\u00f6rnsten", "abstract": "  The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped.\n", "link": "http://arxiv.org/abs/2505.21539v2", "date": "2025-07-30", "relevancy": 2.0782, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5347}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Flow%20Matching%20for%20Point%20Cloud%20Assembly&body=Title%3A%20Equivariant%20Flow%20Matching%20for%20Point%20Cloud%20Assembly%0AAuthor%3A%20Ziming%20Wang%20and%20Nan%20Xue%20and%20Rebecka%20J%C3%B6rnsten%0AAbstract%3A%20%20%20The%20goal%20of%20point%20cloud%20assembly%20is%20to%20reconstruct%20a%20complete%203D%20shape%20by%0Aaligning%20multiple%20point%20cloud%20pieces.%20This%20work%20presents%20a%20novel%20equivariant%0Asolver%20for%20assembly%20tasks%20based%20on%20flow%20matching%20models.%20We%20first%20theoretically%0Ashow%20that%20the%20key%20to%20learning%20equivariant%20distributions%20via%20flow%20matching%20is%20to%0Alearn%20related%20vector%20fields.%20Based%20on%20this%20result%2C%20we%20propose%20an%20assembly%0Amodel%2C%20called%20equivariant%20diffusion%20assembly%20%28Eda%29%2C%20which%20learns%20related%20vector%0Afields%20conditioned%20on%20the%20input%20pieces.%20We%20further%20construct%20an%20equivariant%0Apath%20for%20Eda%2C%20which%20guarantees%20high%20data%20efficiency%20of%20the%20training%20process.%0AOur%20numerical%20results%20show%20that%20Eda%20is%20highly%20competitive%20on%20practical%0Adatasets%2C%20and%20it%20can%20even%20handle%20the%20challenging%20situation%20where%20the%20input%0Apieces%20are%20non-overlapped.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Flow%2520Matching%2520for%2520Point%2520Cloud%2520Assembly%26entry.906535625%3DZiming%2520Wang%2520and%2520Nan%2520Xue%2520and%2520Rebecka%2520J%25C3%25B6rnsten%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520point%2520cloud%2520assembly%2520is%2520to%2520reconstruct%2520a%2520complete%25203D%2520shape%2520by%250Aaligning%2520multiple%2520point%2520cloud%2520pieces.%2520This%2520work%2520presents%2520a%2520novel%2520equivariant%250Asolver%2520for%2520assembly%2520tasks%2520based%2520on%2520flow%2520matching%2520models.%2520We%2520first%2520theoretically%250Ashow%2520that%2520the%2520key%2520to%2520learning%2520equivariant%2520distributions%2520via%2520flow%2520matching%2520is%2520to%250Alearn%2520related%2520vector%2520fields.%2520Based%2520on%2520this%2520result%252C%2520we%2520propose%2520an%2520assembly%250Amodel%252C%2520called%2520equivariant%2520diffusion%2520assembly%2520%2528Eda%2529%252C%2520which%2520learns%2520related%2520vector%250Afields%2520conditioned%2520on%2520the%2520input%2520pieces.%2520We%2520further%2520construct%2520an%2520equivariant%250Apath%2520for%2520Eda%252C%2520which%2520guarantees%2520high%2520data%2520efficiency%2520of%2520the%2520training%2520process.%250AOur%2520numerical%2520results%2520show%2520that%2520Eda%2520is%2520highly%2520competitive%2520on%2520practical%250Adatasets%252C%2520and%2520it%2520can%2520even%2520handle%2520the%2520challenging%2520situation%2520where%2520the%2520input%250Apieces%2520are%2520non-overlapped.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Flow%20Matching%20for%20Point%20Cloud%20Assembly&entry.906535625=Ziming%20Wang%20and%20Nan%20Xue%20and%20Rebecka%20J%C3%B6rnsten&entry.1292438233=%20%20The%20goal%20of%20point%20cloud%20assembly%20is%20to%20reconstruct%20a%20complete%203D%20shape%20by%0Aaligning%20multiple%20point%20cloud%20pieces.%20This%20work%20presents%20a%20novel%20equivariant%0Asolver%20for%20assembly%20tasks%20based%20on%20flow%20matching%20models.%20We%20first%20theoretically%0Ashow%20that%20the%20key%20to%20learning%20equivariant%20distributions%20via%20flow%20matching%20is%20to%0Alearn%20related%20vector%20fields.%20Based%20on%20this%20result%2C%20we%20propose%20an%20assembly%0Amodel%2C%20called%20equivariant%20diffusion%20assembly%20%28Eda%29%2C%20which%20learns%20related%20vector%0Afields%20conditioned%20on%20the%20input%20pieces.%20We%20further%20construct%20an%20equivariant%0Apath%20for%20Eda%2C%20which%20guarantees%20high%20data%20efficiency%20of%20the%20training%20process.%0AOur%20numerical%20results%20show%20that%20Eda%20is%20highly%20competitive%20on%20practical%0Adatasets%2C%20and%20it%20can%20even%20handle%20the%20challenging%20situation%20where%20the%20input%0Apieces%20are%20non-overlapped.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21539v2&entry.124074799=Read"},
{"title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence", "author": "Huan-ang Gao and Jiayi Geng and Wenyue Hua and Mengkang Hu and Xinzhe Juan and Hongzhang Liu and Shilong Liu and Jiahao Qiu and Xuan Qi and Yiran Wu and Hongru Wang and Han Xiao and Yuhang Zhou and Shaokun Zhang and Jiayi Zhang and Jinyu Xiang and Yixiong Fang and Qiwen Zhao and Dongrui Liu and Qihan Ren and Cheng Qian and Zhenghailong Wang and Minda Hu and Huazheng Wang and Qingyun Wu and Heng Ji and Mengdi Wang", "abstract": "  Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.\n", "link": "http://arxiv.org/abs/2507.21046v2", "date": "2025-07-30", "relevancy": 2.0711, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5337}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5195}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence&body=Title%3A%20A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence%0AAuthor%3A%20Huan-ang%20Gao%20and%20Jiayi%20Geng%20and%20Wenyue%20Hua%20and%20Mengkang%20Hu%20and%20Xinzhe%20Juan%20and%20Hongzhang%20Liu%20and%20Shilong%20Liu%20and%20Jiahao%20Qiu%20and%20Xuan%20Qi%20and%20Yiran%20Wu%20and%20Hongru%20Wang%20and%20Han%20Xiao%20and%20Yuhang%20Zhou%20and%20Shaokun%20Zhang%20and%20Jiayi%20Zhang%20and%20Jinyu%20Xiang%20and%20Yixiong%20Fang%20and%20Qiwen%20Zhao%20and%20Dongrui%20Liu%20and%20Qihan%20Ren%20and%20Cheng%20Qian%20and%20Zhenghailong%20Wang%20and%20Minda%20Hu%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%20and%20Heng%20Ji%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20but%20remain%0Afundamentally%20static%2C%20unable%20to%20adapt%20their%20internal%20parameters%20to%20novel%20tasks%2C%0Aevolving%20knowledge%20domains%2C%20or%20dynamic%20interaction%20contexts.%20As%20LLMs%20are%0Aincreasingly%20deployed%20in%20open-ended%2C%20interactive%20environments%2C%20this%20static%0Anature%20has%20become%20a%20critical%20bottleneck%2C%20necessitating%20agents%20that%20can%0Aadaptively%20reason%2C%20act%2C%20and%20evolve%20in%20real%20time.%20This%20paradigm%20shift%20--%20from%0Ascaling%20static%20models%20to%20developing%20self-evolving%20agents%20--%20has%20sparked%20growing%0Ainterest%20in%20architectures%20and%20methods%20enabling%20continual%20learning%20and%0Aadaptation%20from%20data%2C%20interactions%2C%20and%20experiences.%20This%20survey%20provides%20the%0Afirst%20systematic%20and%20comprehensive%20review%20of%20self-evolving%20agents%2C%20organized%0Aaround%20three%20foundational%20dimensions%20--%20what%20to%20evolve%2C%20when%20to%20evolve%2C%20and%20how%0Ato%20evolve.%20We%20examine%20evolutionary%20mechanisms%20across%20agent%20components%20%28e.g.%2C%0Amodels%2C%20memory%2C%20tools%2C%20architecture%29%2C%20categorize%20adaptation%20methods%20by%20stages%0A%28e.g.%2C%20intra-test-time%2C%20inter-test-time%29%2C%20and%20analyze%20the%20algorithmic%20and%0Aarchitectural%20designs%20that%20guide%20evolutionary%20adaptation%20%28e.g.%2C%20scalar%20rewards%2C%0Atextual%20feedback%2C%20single-agent%20and%20multi-agent%20systems%29.%20Additionally%2C%20we%0Aanalyze%20evaluation%20metrics%20and%20benchmarks%20tailored%20for%20self-evolving%20agents%2C%0Ahighlight%20applications%20in%20domains%20such%20as%20coding%2C%20education%2C%20and%20healthcare%2C%0Aand%20identify%20critical%20challenges%20and%20research%20directions%20in%20safety%2C%0Ascalability%2C%20and%20co-evolutionary%20dynamics.%20By%20providing%20a%20structured%20framework%0Afor%20understanding%20and%20designing%20self-evolving%20agents%2C%20this%20survey%20establishes%20a%0Aroadmap%20for%20advancing%20adaptive%20agentic%20systems%20in%20both%20research%20and%20real-world%0Adeployments%2C%20ultimately%20shedding%20lights%20to%20pave%20the%20way%20for%20the%20realization%20of%0AArtificial%20Super%20Intelligence%20%28ASI%29%2C%20where%20agents%20evolve%20autonomously%2C%0Aperforming%20at%20or%20beyond%20human-level%20intelligence%20across%20a%20wide%20array%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Self-Evolving%2520Agents%253A%2520On%2520Path%2520to%2520Artificial%2520Super%250A%2520%2520Intelligence%26entry.906535625%3DHuan-ang%2520Gao%2520and%2520Jiayi%2520Geng%2520and%2520Wenyue%2520Hua%2520and%2520Mengkang%2520Hu%2520and%2520Xinzhe%2520Juan%2520and%2520Hongzhang%2520Liu%2520and%2520Shilong%2520Liu%2520and%2520Jiahao%2520Qiu%2520and%2520Xuan%2520Qi%2520and%2520Yiran%2520Wu%2520and%2520Hongru%2520Wang%2520and%2520Han%2520Xiao%2520and%2520Yuhang%2520Zhou%2520and%2520Shaokun%2520Zhang%2520and%2520Jiayi%2520Zhang%2520and%2520Jinyu%2520Xiang%2520and%2520Yixiong%2520Fang%2520and%2520Qiwen%2520Zhao%2520and%2520Dongrui%2520Liu%2520and%2520Qihan%2520Ren%2520and%2520Cheng%2520Qian%2520and%2520Zhenghailong%2520Wang%2520and%2520Minda%2520Hu%2520and%2520Huazheng%2520Wang%2520and%2520Qingyun%2520Wu%2520and%2520Heng%2520Ji%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520but%2520remain%250Afundamentally%2520static%252C%2520unable%2520to%2520adapt%2520their%2520internal%2520parameters%2520to%2520novel%2520tasks%252C%250Aevolving%2520knowledge%2520domains%252C%2520or%2520dynamic%2520interaction%2520contexts.%2520As%2520LLMs%2520are%250Aincreasingly%2520deployed%2520in%2520open-ended%252C%2520interactive%2520environments%252C%2520this%2520static%250Anature%2520has%2520become%2520a%2520critical%2520bottleneck%252C%2520necessitating%2520agents%2520that%2520can%250Aadaptively%2520reason%252C%2520act%252C%2520and%2520evolve%2520in%2520real%2520time.%2520This%2520paradigm%2520shift%2520--%2520from%250Ascaling%2520static%2520models%2520to%2520developing%2520self-evolving%2520agents%2520--%2520has%2520sparked%2520growing%250Ainterest%2520in%2520architectures%2520and%2520methods%2520enabling%2520continual%2520learning%2520and%250Aadaptation%2520from%2520data%252C%2520interactions%252C%2520and%2520experiences.%2520This%2520survey%2520provides%2520the%250Afirst%2520systematic%2520and%2520comprehensive%2520review%2520of%2520self-evolving%2520agents%252C%2520organized%250Aaround%2520three%2520foundational%2520dimensions%2520--%2520what%2520to%2520evolve%252C%2520when%2520to%2520evolve%252C%2520and%2520how%250Ato%2520evolve.%2520We%2520examine%2520evolutionary%2520mechanisms%2520across%2520agent%2520components%2520%2528e.g.%252C%250Amodels%252C%2520memory%252C%2520tools%252C%2520architecture%2529%252C%2520categorize%2520adaptation%2520methods%2520by%2520stages%250A%2528e.g.%252C%2520intra-test-time%252C%2520inter-test-time%2529%252C%2520and%2520analyze%2520the%2520algorithmic%2520and%250Aarchitectural%2520designs%2520that%2520guide%2520evolutionary%2520adaptation%2520%2528e.g.%252C%2520scalar%2520rewards%252C%250Atextual%2520feedback%252C%2520single-agent%2520and%2520multi-agent%2520systems%2529.%2520Additionally%252C%2520we%250Aanalyze%2520evaluation%2520metrics%2520and%2520benchmarks%2520tailored%2520for%2520self-evolving%2520agents%252C%250Ahighlight%2520applications%2520in%2520domains%2520such%2520as%2520coding%252C%2520education%252C%2520and%2520healthcare%252C%250Aand%2520identify%2520critical%2520challenges%2520and%2520research%2520directions%2520in%2520safety%252C%250Ascalability%252C%2520and%2520co-evolutionary%2520dynamics.%2520By%2520providing%2520a%2520structured%2520framework%250Afor%2520understanding%2520and%2520designing%2520self-evolving%2520agents%252C%2520this%2520survey%2520establishes%2520a%250Aroadmap%2520for%2520advancing%2520adaptive%2520agentic%2520systems%2520in%2520both%2520research%2520and%2520real-world%250Adeployments%252C%2520ultimately%2520shedding%2520lights%2520to%2520pave%2520the%2520way%2520for%2520the%2520realization%2520of%250AArtificial%2520Super%2520Intelligence%2520%2528ASI%2529%252C%2520where%2520agents%2520evolve%2520autonomously%252C%250Aperforming%2520at%2520or%2520beyond%2520human-level%2520intelligence%2520across%2520a%2520wide%2520array%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence&entry.906535625=Huan-ang%20Gao%20and%20Jiayi%20Geng%20and%20Wenyue%20Hua%20and%20Mengkang%20Hu%20and%20Xinzhe%20Juan%20and%20Hongzhang%20Liu%20and%20Shilong%20Liu%20and%20Jiahao%20Qiu%20and%20Xuan%20Qi%20and%20Yiran%20Wu%20and%20Hongru%20Wang%20and%20Han%20Xiao%20and%20Yuhang%20Zhou%20and%20Shaokun%20Zhang%20and%20Jiayi%20Zhang%20and%20Jinyu%20Xiang%20and%20Yixiong%20Fang%20and%20Qiwen%20Zhao%20and%20Dongrui%20Liu%20and%20Qihan%20Ren%20and%20Cheng%20Qian%20and%20Zhenghailong%20Wang%20and%20Minda%20Hu%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%20and%20Heng%20Ji%20and%20Mengdi%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20but%20remain%0Afundamentally%20static%2C%20unable%20to%20adapt%20their%20internal%20parameters%20to%20novel%20tasks%2C%0Aevolving%20knowledge%20domains%2C%20or%20dynamic%20interaction%20contexts.%20As%20LLMs%20are%0Aincreasingly%20deployed%20in%20open-ended%2C%20interactive%20environments%2C%20this%20static%0Anature%20has%20become%20a%20critical%20bottleneck%2C%20necessitating%20agents%20that%20can%0Aadaptively%20reason%2C%20act%2C%20and%20evolve%20in%20real%20time.%20This%20paradigm%20shift%20--%20from%0Ascaling%20static%20models%20to%20developing%20self-evolving%20agents%20--%20has%20sparked%20growing%0Ainterest%20in%20architectures%20and%20methods%20enabling%20continual%20learning%20and%0Aadaptation%20from%20data%2C%20interactions%2C%20and%20experiences.%20This%20survey%20provides%20the%0Afirst%20systematic%20and%20comprehensive%20review%20of%20self-evolving%20agents%2C%20organized%0Aaround%20three%20foundational%20dimensions%20--%20what%20to%20evolve%2C%20when%20to%20evolve%2C%20and%20how%0Ato%20evolve.%20We%20examine%20evolutionary%20mechanisms%20across%20agent%20components%20%28e.g.%2C%0Amodels%2C%20memory%2C%20tools%2C%20architecture%29%2C%20categorize%20adaptation%20methods%20by%20stages%0A%28e.g.%2C%20intra-test-time%2C%20inter-test-time%29%2C%20and%20analyze%20the%20algorithmic%20and%0Aarchitectural%20designs%20that%20guide%20evolutionary%20adaptation%20%28e.g.%2C%20scalar%20rewards%2C%0Atextual%20feedback%2C%20single-agent%20and%20multi-agent%20systems%29.%20Additionally%2C%20we%0Aanalyze%20evaluation%20metrics%20and%20benchmarks%20tailored%20for%20self-evolving%20agents%2C%0Ahighlight%20applications%20in%20domains%20such%20as%20coding%2C%20education%2C%20and%20healthcare%2C%0Aand%20identify%20critical%20challenges%20and%20research%20directions%20in%20safety%2C%0Ascalability%2C%20and%20co-evolutionary%20dynamics.%20By%20providing%20a%20structured%20framework%0Afor%20understanding%20and%20designing%20self-evolving%20agents%2C%20this%20survey%20establishes%20a%0Aroadmap%20for%20advancing%20adaptive%20agentic%20systems%20in%20both%20research%20and%20real-world%0Adeployments%2C%20ultimately%20shedding%20lights%20to%20pave%20the%20way%20for%20the%20realization%20of%0AArtificial%20Super%20Intelligence%20%28ASI%29%2C%20where%20agents%20evolve%20autonomously%2C%0Aperforming%20at%20or%20beyond%20human-level%20intelligence%20across%20a%20wide%20array%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21046v2&entry.124074799=Read"},
{"title": "COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP", "author": "Galadrielle Humblot-Renaux and Gianni Franchi and Sergio Escalera and Thomas B. Moeslund", "abstract": "  Out-of-distribution (OOD) detection is an important building block in\ntrustworthy image recognition systems as unknown classes may arise at\ntest-time. OOD detection methods typically revolve around a single classifier,\nleading to a split in the research field between the classical supervised\nsetting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot\nsetting (class names fed as prompts to CLIP). In both cases, an overarching\nchallenge is that the OOD detection performance is implicitly constrained by\nthe classifier's capabilities on in-distribution (ID) data. In this work, we\nshow that given a little open-mindedness from both ends, remarkable OOD\ndetection can be achieved by instead creating a heterogeneous ensemble - COOkeD\ncombines the predictions of a closed-world classifier trained end-to-end on a\nspecific dataset, a zero-shot CLIP classifier, and a linear probe classifier\ntrained on CLIP image features. While bulky at first sight, this approach is\nmodular, post-hoc and leverages the availability of pre-trained VLMs, thus\nintroduces little overhead compared to training a single standard classifier.\nWe evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also\nconsider more challenging, realistic settings ranging from training-time label\nnoise, to test-time covariate shift, to zero-shot shift which has been\npreviously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art\nperformance and greater robustness compared to both classical and CLIP-based\nOOD detection methods. Code is available at https://github.com/glhr/COOkeD\n", "link": "http://arxiv.org/abs/2507.22576v1", "date": "2025-07-30", "relevancy": 2.0696, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5153}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COOkeD%3A%20Ensemble-based%20OOD%20detection%20in%20the%20era%20of%20zero-shot%20CLIP&body=Title%3A%20COOkeD%3A%20Ensemble-based%20OOD%20detection%20in%20the%20era%20of%20zero-shot%20CLIP%0AAuthor%3A%20Galadrielle%20Humblot-Renaux%20and%20Gianni%20Franchi%20and%20Sergio%20Escalera%20and%20Thomas%20B.%20Moeslund%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20an%20important%20building%20block%20in%0Atrustworthy%20image%20recognition%20systems%20as%20unknown%20classes%20may%20arise%20at%0Atest-time.%20OOD%20detection%20methods%20typically%20revolve%20around%20a%20single%20classifier%2C%0Aleading%20to%20a%20split%20in%20the%20research%20field%20between%20the%20classical%20supervised%0Asetting%20%28e.g.%20ResNet18%20classifier%20trained%20on%20CIFAR100%29%20vs.%20the%20zero-shot%0Asetting%20%28class%20names%20fed%20as%20prompts%20to%20CLIP%29.%20In%20both%20cases%2C%20an%20overarching%0Achallenge%20is%20that%20the%20OOD%20detection%20performance%20is%20implicitly%20constrained%20by%0Athe%20classifier%27s%20capabilities%20on%20in-distribution%20%28ID%29%20data.%20In%20this%20work%2C%20we%0Ashow%20that%20given%20a%20little%20open-mindedness%20from%20both%20ends%2C%20remarkable%20OOD%0Adetection%20can%20be%20achieved%20by%20instead%20creating%20a%20heterogeneous%20ensemble%20-%20COOkeD%0Acombines%20the%20predictions%20of%20a%20closed-world%20classifier%20trained%20end-to-end%20on%20a%0Aspecific%20dataset%2C%20a%20zero-shot%20CLIP%20classifier%2C%20and%20a%20linear%20probe%20classifier%0Atrained%20on%20CLIP%20image%20features.%20While%20bulky%20at%20first%20sight%2C%20this%20approach%20is%0Amodular%2C%20post-hoc%20and%20leverages%20the%20availability%20of%20pre-trained%20VLMs%2C%20thus%0Aintroduces%20little%20overhead%20compared%20to%20training%20a%20single%20standard%20classifier.%0AWe%20evaluate%20COOkeD%20on%20popular%20CIFAR100%20and%20ImageNet%20benchmarks%2C%20but%20also%0Aconsider%20more%20challenging%2C%20realistic%20settings%20ranging%20from%20training-time%20label%0Anoise%2C%20to%20test-time%20covariate%20shift%2C%20to%20zero-shot%20shift%20which%20has%20been%0Apreviously%20overlooked.%20Despite%20its%20simplicity%2C%20COOkeD%20achieves%20state-of-the-art%0Aperformance%20and%20greater%20robustness%20compared%20to%20both%20classical%20and%20CLIP-based%0AOOD%20detection%20methods.%20Code%20is%20available%20at%20https%3A//github.com/glhr/COOkeD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOOkeD%253A%2520Ensemble-based%2520OOD%2520detection%2520in%2520the%2520era%2520of%2520zero-shot%2520CLIP%26entry.906535625%3DGaladrielle%2520Humblot-Renaux%2520and%2520Gianni%2520Franchi%2520and%2520Sergio%2520Escalera%2520and%2520Thomas%2520B.%2520Moeslund%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520an%2520important%2520building%2520block%2520in%250Atrustworthy%2520image%2520recognition%2520systems%2520as%2520unknown%2520classes%2520may%2520arise%2520at%250Atest-time.%2520OOD%2520detection%2520methods%2520typically%2520revolve%2520around%2520a%2520single%2520classifier%252C%250Aleading%2520to%2520a%2520split%2520in%2520the%2520research%2520field%2520between%2520the%2520classical%2520supervised%250Asetting%2520%2528e.g.%2520ResNet18%2520classifier%2520trained%2520on%2520CIFAR100%2529%2520vs.%2520the%2520zero-shot%250Asetting%2520%2528class%2520names%2520fed%2520as%2520prompts%2520to%2520CLIP%2529.%2520In%2520both%2520cases%252C%2520an%2520overarching%250Achallenge%2520is%2520that%2520the%2520OOD%2520detection%2520performance%2520is%2520implicitly%2520constrained%2520by%250Athe%2520classifier%2527s%2520capabilities%2520on%2520in-distribution%2520%2528ID%2529%2520data.%2520In%2520this%2520work%252C%2520we%250Ashow%2520that%2520given%2520a%2520little%2520open-mindedness%2520from%2520both%2520ends%252C%2520remarkable%2520OOD%250Adetection%2520can%2520be%2520achieved%2520by%2520instead%2520creating%2520a%2520heterogeneous%2520ensemble%2520-%2520COOkeD%250Acombines%2520the%2520predictions%2520of%2520a%2520closed-world%2520classifier%2520trained%2520end-to-end%2520on%2520a%250Aspecific%2520dataset%252C%2520a%2520zero-shot%2520CLIP%2520classifier%252C%2520and%2520a%2520linear%2520probe%2520classifier%250Atrained%2520on%2520CLIP%2520image%2520features.%2520While%2520bulky%2520at%2520first%2520sight%252C%2520this%2520approach%2520is%250Amodular%252C%2520post-hoc%2520and%2520leverages%2520the%2520availability%2520of%2520pre-trained%2520VLMs%252C%2520thus%250Aintroduces%2520little%2520overhead%2520compared%2520to%2520training%2520a%2520single%2520standard%2520classifier.%250AWe%2520evaluate%2520COOkeD%2520on%2520popular%2520CIFAR100%2520and%2520ImageNet%2520benchmarks%252C%2520but%2520also%250Aconsider%2520more%2520challenging%252C%2520realistic%2520settings%2520ranging%2520from%2520training-time%2520label%250Anoise%252C%2520to%2520test-time%2520covariate%2520shift%252C%2520to%2520zero-shot%2520shift%2520which%2520has%2520been%250Apreviously%2520overlooked.%2520Despite%2520its%2520simplicity%252C%2520COOkeD%2520achieves%2520state-of-the-art%250Aperformance%2520and%2520greater%2520robustness%2520compared%2520to%2520both%2520classical%2520and%2520CLIP-based%250AOOD%2520detection%2520methods.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/glhr/COOkeD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COOkeD%3A%20Ensemble-based%20OOD%20detection%20in%20the%20era%20of%20zero-shot%20CLIP&entry.906535625=Galadrielle%20Humblot-Renaux%20and%20Gianni%20Franchi%20and%20Sergio%20Escalera%20and%20Thomas%20B.%20Moeslund&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20an%20important%20building%20block%20in%0Atrustworthy%20image%20recognition%20systems%20as%20unknown%20classes%20may%20arise%20at%0Atest-time.%20OOD%20detection%20methods%20typically%20revolve%20around%20a%20single%20classifier%2C%0Aleading%20to%20a%20split%20in%20the%20research%20field%20between%20the%20classical%20supervised%0Asetting%20%28e.g.%20ResNet18%20classifier%20trained%20on%20CIFAR100%29%20vs.%20the%20zero-shot%0Asetting%20%28class%20names%20fed%20as%20prompts%20to%20CLIP%29.%20In%20both%20cases%2C%20an%20overarching%0Achallenge%20is%20that%20the%20OOD%20detection%20performance%20is%20implicitly%20constrained%20by%0Athe%20classifier%27s%20capabilities%20on%20in-distribution%20%28ID%29%20data.%20In%20this%20work%2C%20we%0Ashow%20that%20given%20a%20little%20open-mindedness%20from%20both%20ends%2C%20remarkable%20OOD%0Adetection%20can%20be%20achieved%20by%20instead%20creating%20a%20heterogeneous%20ensemble%20-%20COOkeD%0Acombines%20the%20predictions%20of%20a%20closed-world%20classifier%20trained%20end-to-end%20on%20a%0Aspecific%20dataset%2C%20a%20zero-shot%20CLIP%20classifier%2C%20and%20a%20linear%20probe%20classifier%0Atrained%20on%20CLIP%20image%20features.%20While%20bulky%20at%20first%20sight%2C%20this%20approach%20is%0Amodular%2C%20post-hoc%20and%20leverages%20the%20availability%20of%20pre-trained%20VLMs%2C%20thus%0Aintroduces%20little%20overhead%20compared%20to%20training%20a%20single%20standard%20classifier.%0AWe%20evaluate%20COOkeD%20on%20popular%20CIFAR100%20and%20ImageNet%20benchmarks%2C%20but%20also%0Aconsider%20more%20challenging%2C%20realistic%20settings%20ranging%20from%20training-time%20label%0Anoise%2C%20to%20test-time%20covariate%20shift%2C%20to%20zero-shot%20shift%20which%20has%20been%0Apreviously%20overlooked.%20Despite%20its%20simplicity%2C%20COOkeD%20achieves%20state-of-the-art%0Aperformance%20and%20greater%20robustness%20compared%20to%20both%20classical%20and%20CLIP-based%0AOOD%20detection%20methods.%20Code%20is%20available%20at%20https%3A//github.com/glhr/COOkeD%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22576v1&entry.124074799=Read"},
{"title": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and\n  Efficient Attention", "author": "Syed Haider Ali and Asrar Ahmad and Muhammad Ali and Asifullah Khan and Nadeem Shaukat", "abstract": "  Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.\n", "link": "http://arxiv.org/abs/2506.15562v2", "date": "2025-07-30", "relevancy": 2.0606, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5314}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5121}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20MRI%20Tumor%20Segmentation%20using%20hybrid%20U-Net%20with%20Transformer%20and%0A%20%20Efficient%20Attention&body=Title%3A%20Automated%20MRI%20Tumor%20Segmentation%20using%20hybrid%20U-Net%20with%20Transformer%20and%0A%20%20Efficient%20Attention%0AAuthor%3A%20Syed%20Haider%20Ali%20and%20Asrar%20Ahmad%20and%20Muhammad%20Ali%20and%20Asifullah%20Khan%20and%20Nadeem%20Shaukat%0AAbstract%3A%20%20%20Cancer%20is%20an%20abnormal%20growth%20with%20potential%20to%20invade%20locally%20and%20metastasize%0Ato%20distant%20organs.%20Accurate%20auto-segmentation%20of%20the%20tumor%20and%20surrounding%0Anormal%20tissues%20is%20required%20for%20radiotherapy%20treatment%20plan%20optimization.%20Recent%0AAI-based%20segmentation%20models%20are%20generally%20trained%20on%20large%20public%20datasets%2C%0Awhich%20lack%20the%20heterogeneity%20of%20local%20patient%20populations.%20While%20these%20studies%0Aadvance%20AI-based%20medical%20image%20segmentation%2C%20research%20on%20local%20datasets%20is%0Anecessary%20to%20develop%20and%20integrate%20AI%20tumor%20segmentation%20models%20directly%20into%0Ahospital%20software%20for%20efficient%20and%20accurate%20oncology%20treatment%20planning%20and%0Aexecution.%20This%20study%20enhances%20tumor%20segmentation%20using%20computationally%0Aefficient%20hybrid%20UNet-Transformer%20models%20on%20magnetic%20resonance%20imaging%20%28MRI%29%0Adatasets%20acquired%20from%20a%20local%20hospital%20under%20strict%20privacy%20protection.%20We%0Adeveloped%20a%20robust%20data%20pipeline%20for%20seamless%20DICOM%20extraction%20and%0Apreprocessing%2C%20followed%20by%20extensive%20image%20augmentation%20to%20ensure%20model%0Ageneralization%20across%20diverse%20clinical%20settings%2C%20resulting%20in%20a%20total%20dataset%0Aof%206080%20images%20for%20training.%20Our%20novel%20architecture%20integrates%20UNet-based%0Aconvolutional%20neural%20networks%20with%20a%20transformer%20bottleneck%20and%20complementary%0Aattention%20modules%2C%20including%20efficient%20attention%2C%20Squeeze-and-Excitation%20%28SE%29%0Ablocks%2C%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20and%20ResNeXt%20blocks.%20To%0Aaccelerate%20convergence%20and%20reduce%20computational%20demands%2C%20we%20used%20a%20maximum%0Abatch%20size%20of%208%20and%20initialized%20the%20encoder%20with%20pretrained%20ImageNet%20weights%2C%0Atraining%20the%20model%20on%20dual%20NVIDIA%20T4%20GPUs%20via%20checkpointing%20to%20overcome%0AKaggle%27s%20runtime%20limits.%20Quantitative%20evaluation%20on%20the%20local%20MRI%20dataset%0Ayielded%20a%20Dice%20similarity%20coefficient%20of%200.764%20and%20an%20Intersection%20over%20Union%0A%28IoU%29%20of%200.736%2C%20demonstrating%20competitive%20performance%20despite%20limited%20data%20and%0Aunderscoring%20the%20importance%20of%20site-specific%20model%20development%20for%20clinical%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520MRI%2520Tumor%2520Segmentation%2520using%2520hybrid%2520U-Net%2520with%2520Transformer%2520and%250A%2520%2520Efficient%2520Attention%26entry.906535625%3DSyed%2520Haider%2520Ali%2520and%2520Asrar%2520Ahmad%2520and%2520Muhammad%2520Ali%2520and%2520Asifullah%2520Khan%2520and%2520Nadeem%2520Shaukat%26entry.1292438233%3D%2520%2520Cancer%2520is%2520an%2520abnormal%2520growth%2520with%2520potential%2520to%2520invade%2520locally%2520and%2520metastasize%250Ato%2520distant%2520organs.%2520Accurate%2520auto-segmentation%2520of%2520the%2520tumor%2520and%2520surrounding%250Anormal%2520tissues%2520is%2520required%2520for%2520radiotherapy%2520treatment%2520plan%2520optimization.%2520Recent%250AAI-based%2520segmentation%2520models%2520are%2520generally%2520trained%2520on%2520large%2520public%2520datasets%252C%250Awhich%2520lack%2520the%2520heterogeneity%2520of%2520local%2520patient%2520populations.%2520While%2520these%2520studies%250Aadvance%2520AI-based%2520medical%2520image%2520segmentation%252C%2520research%2520on%2520local%2520datasets%2520is%250Anecessary%2520to%2520develop%2520and%2520integrate%2520AI%2520tumor%2520segmentation%2520models%2520directly%2520into%250Ahospital%2520software%2520for%2520efficient%2520and%2520accurate%2520oncology%2520treatment%2520planning%2520and%250Aexecution.%2520This%2520study%2520enhances%2520tumor%2520segmentation%2520using%2520computationally%250Aefficient%2520hybrid%2520UNet-Transformer%2520models%2520on%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Adatasets%2520acquired%2520from%2520a%2520local%2520hospital%2520under%2520strict%2520privacy%2520protection.%2520We%250Adeveloped%2520a%2520robust%2520data%2520pipeline%2520for%2520seamless%2520DICOM%2520extraction%2520and%250Apreprocessing%252C%2520followed%2520by%2520extensive%2520image%2520augmentation%2520to%2520ensure%2520model%250Ageneralization%2520across%2520diverse%2520clinical%2520settings%252C%2520resulting%2520in%2520a%2520total%2520dataset%250Aof%25206080%2520images%2520for%2520training.%2520Our%2520novel%2520architecture%2520integrates%2520UNet-based%250Aconvolutional%2520neural%2520networks%2520with%2520a%2520transformer%2520bottleneck%2520and%2520complementary%250Aattention%2520modules%252C%2520including%2520efficient%2520attention%252C%2520Squeeze-and-Excitation%2520%2528SE%2529%250Ablocks%252C%2520Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%252C%2520and%2520ResNeXt%2520blocks.%2520To%250Aaccelerate%2520convergence%2520and%2520reduce%2520computational%2520demands%252C%2520we%2520used%2520a%2520maximum%250Abatch%2520size%2520of%25208%2520and%2520initialized%2520the%2520encoder%2520with%2520pretrained%2520ImageNet%2520weights%252C%250Atraining%2520the%2520model%2520on%2520dual%2520NVIDIA%2520T4%2520GPUs%2520via%2520checkpointing%2520to%2520overcome%250AKaggle%2527s%2520runtime%2520limits.%2520Quantitative%2520evaluation%2520on%2520the%2520local%2520MRI%2520dataset%250Ayielded%2520a%2520Dice%2520similarity%2520coefficient%2520of%25200.764%2520and%2520an%2520Intersection%2520over%2520Union%250A%2528IoU%2529%2520of%25200.736%252C%2520demonstrating%2520competitive%2520performance%2520despite%2520limited%2520data%2520and%250Aunderscoring%2520the%2520importance%2520of%2520site-specific%2520model%2520development%2520for%2520clinical%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20MRI%20Tumor%20Segmentation%20using%20hybrid%20U-Net%20with%20Transformer%20and%0A%20%20Efficient%20Attention&entry.906535625=Syed%20Haider%20Ali%20and%20Asrar%20Ahmad%20and%20Muhammad%20Ali%20and%20Asifullah%20Khan%20and%20Nadeem%20Shaukat&entry.1292438233=%20%20Cancer%20is%20an%20abnormal%20growth%20with%20potential%20to%20invade%20locally%20and%20metastasize%0Ato%20distant%20organs.%20Accurate%20auto-segmentation%20of%20the%20tumor%20and%20surrounding%0Anormal%20tissues%20is%20required%20for%20radiotherapy%20treatment%20plan%20optimization.%20Recent%0AAI-based%20segmentation%20models%20are%20generally%20trained%20on%20large%20public%20datasets%2C%0Awhich%20lack%20the%20heterogeneity%20of%20local%20patient%20populations.%20While%20these%20studies%0Aadvance%20AI-based%20medical%20image%20segmentation%2C%20research%20on%20local%20datasets%20is%0Anecessary%20to%20develop%20and%20integrate%20AI%20tumor%20segmentation%20models%20directly%20into%0Ahospital%20software%20for%20efficient%20and%20accurate%20oncology%20treatment%20planning%20and%0Aexecution.%20This%20study%20enhances%20tumor%20segmentation%20using%20computationally%0Aefficient%20hybrid%20UNet-Transformer%20models%20on%20magnetic%20resonance%20imaging%20%28MRI%29%0Adatasets%20acquired%20from%20a%20local%20hospital%20under%20strict%20privacy%20protection.%20We%0Adeveloped%20a%20robust%20data%20pipeline%20for%20seamless%20DICOM%20extraction%20and%0Apreprocessing%2C%20followed%20by%20extensive%20image%20augmentation%20to%20ensure%20model%0Ageneralization%20across%20diverse%20clinical%20settings%2C%20resulting%20in%20a%20total%20dataset%0Aof%206080%20images%20for%20training.%20Our%20novel%20architecture%20integrates%20UNet-based%0Aconvolutional%20neural%20networks%20with%20a%20transformer%20bottleneck%20and%20complementary%0Aattention%20modules%2C%20including%20efficient%20attention%2C%20Squeeze-and-Excitation%20%28SE%29%0Ablocks%2C%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20and%20ResNeXt%20blocks.%20To%0Aaccelerate%20convergence%20and%20reduce%20computational%20demands%2C%20we%20used%20a%20maximum%0Abatch%20size%20of%208%20and%20initialized%20the%20encoder%20with%20pretrained%20ImageNet%20weights%2C%0Atraining%20the%20model%20on%20dual%20NVIDIA%20T4%20GPUs%20via%20checkpointing%20to%20overcome%0AKaggle%27s%20runtime%20limits.%20Quantitative%20evaluation%20on%20the%20local%20MRI%20dataset%0Ayielded%20a%20Dice%20similarity%20coefficient%20of%200.764%20and%20an%20Intersection%20over%20Union%0A%28IoU%29%20of%200.736%2C%20demonstrating%20competitive%20performance%20despite%20limited%20data%20and%0Aunderscoring%20the%20importance%20of%20site-specific%20model%20development%20for%20clinical%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15562v2&entry.124074799=Read"},
{"title": "Tapping into the Black Box: Uncovering Aligned Representations in\n  Pretrained Neural Networks", "author": "Maciej Satkiewicz", "abstract": "  In this paper we argue that ReLU networks learn an implicit linear model we\ncan actually tap into. We describe that alleged model formally and show that we\ncan approximately pull its decision boundary back to the input space with\ncertain simple modification to the backward pass. The resulting gradients\n(called excitation pullbacks) reveal high-resolution input- and target-specific\nfeatures of remarkable perceptual alignment on a number of popular\nImageNet-pretrained deep architectures. This strongly suggests that neural\nnetworks do, in fact, rely on learned interpretable patterns that can be\nrecovered after training. Thus, our findings may have profound implications for\nknowledge discovery and the development of dependable artificial systems.\n", "link": "http://arxiv.org/abs/2507.22832v1", "date": "2025-07-30", "relevancy": 2.0529, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5206}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5187}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tapping%20into%20the%20Black%20Box%3A%20Uncovering%20Aligned%20Representations%20in%0A%20%20Pretrained%20Neural%20Networks&body=Title%3A%20Tapping%20into%20the%20Black%20Box%3A%20Uncovering%20Aligned%20Representations%20in%0A%20%20Pretrained%20Neural%20Networks%0AAuthor%3A%20Maciej%20Satkiewicz%0AAbstract%3A%20%20%20In%20this%20paper%20we%20argue%20that%20ReLU%20networks%20learn%20an%20implicit%20linear%20model%20we%0Acan%20actually%20tap%20into.%20We%20describe%20that%20alleged%20model%20formally%20and%20show%20that%20we%0Acan%20approximately%20pull%20its%20decision%20boundary%20back%20to%20the%20input%20space%20with%0Acertain%20simple%20modification%20to%20the%20backward%20pass.%20The%20resulting%20gradients%0A%28called%20excitation%20pullbacks%29%20reveal%20high-resolution%20input-%20and%20target-specific%0Afeatures%20of%20remarkable%20perceptual%20alignment%20on%20a%20number%20of%20popular%0AImageNet-pretrained%20deep%20architectures.%20This%20strongly%20suggests%20that%20neural%0Anetworks%20do%2C%20in%20fact%2C%20rely%20on%20learned%20interpretable%20patterns%20that%20can%20be%0Arecovered%20after%20training.%20Thus%2C%20our%20findings%20may%20have%20profound%20implications%20for%0Aknowledge%20discovery%20and%20the%20development%20of%20dependable%20artificial%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTapping%2520into%2520the%2520Black%2520Box%253A%2520Uncovering%2520Aligned%2520Representations%2520in%250A%2520%2520Pretrained%2520Neural%2520Networks%26entry.906535625%3DMaciej%2520Satkiewicz%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520argue%2520that%2520ReLU%2520networks%2520learn%2520an%2520implicit%2520linear%2520model%2520we%250Acan%2520actually%2520tap%2520into.%2520We%2520describe%2520that%2520alleged%2520model%2520formally%2520and%2520show%2520that%2520we%250Acan%2520approximately%2520pull%2520its%2520decision%2520boundary%2520back%2520to%2520the%2520input%2520space%2520with%250Acertain%2520simple%2520modification%2520to%2520the%2520backward%2520pass.%2520The%2520resulting%2520gradients%250A%2528called%2520excitation%2520pullbacks%2529%2520reveal%2520high-resolution%2520input-%2520and%2520target-specific%250Afeatures%2520of%2520remarkable%2520perceptual%2520alignment%2520on%2520a%2520number%2520of%2520popular%250AImageNet-pretrained%2520deep%2520architectures.%2520This%2520strongly%2520suggests%2520that%2520neural%250Anetworks%2520do%252C%2520in%2520fact%252C%2520rely%2520on%2520learned%2520interpretable%2520patterns%2520that%2520can%2520be%250Arecovered%2520after%2520training.%2520Thus%252C%2520our%2520findings%2520may%2520have%2520profound%2520implications%2520for%250Aknowledge%2520discovery%2520and%2520the%2520development%2520of%2520dependable%2520artificial%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tapping%20into%20the%20Black%20Box%3A%20Uncovering%20Aligned%20Representations%20in%0A%20%20Pretrained%20Neural%20Networks&entry.906535625=Maciej%20Satkiewicz&entry.1292438233=%20%20In%20this%20paper%20we%20argue%20that%20ReLU%20networks%20learn%20an%20implicit%20linear%20model%20we%0Acan%20actually%20tap%20into.%20We%20describe%20that%20alleged%20model%20formally%20and%20show%20that%20we%0Acan%20approximately%20pull%20its%20decision%20boundary%20back%20to%20the%20input%20space%20with%0Acertain%20simple%20modification%20to%20the%20backward%20pass.%20The%20resulting%20gradients%0A%28called%20excitation%20pullbacks%29%20reveal%20high-resolution%20input-%20and%20target-specific%0Afeatures%20of%20remarkable%20perceptual%20alignment%20on%20a%20number%20of%20popular%0AImageNet-pretrained%20deep%20architectures.%20This%20strongly%20suggests%20that%20neural%0Anetworks%20do%2C%20in%20fact%2C%20rely%20on%20learned%20interpretable%20patterns%20that%20can%20be%0Arecovered%20after%20training.%20Thus%2C%20our%20findings%20may%20have%20profound%20implications%20for%0Aknowledge%20discovery%20and%20the%20development%20of%20dependable%20artificial%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22832v1&entry.124074799=Read"},
{"title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment\n  Pancreatic Tumor in Endoscopic Ultrasound", "author": "Pascal Spiegler and Taha Koleilat and Arash Harirpoush and Corey S. Miller and Hassan Rivaz and Marta Kersten-Oertel and Yiming Xiao", "abstract": "  Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS .\n", "link": "http://arxiv.org/abs/2507.18082v3", "date": "2025-07-30", "relevancy": 2.0488, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5401}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5103}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextSAM-EUS%3A%20Text%20Prompt%20Learning%20for%20SAM%20to%20Accurately%20Segment%0A%20%20Pancreatic%20Tumor%20in%20Endoscopic%20Ultrasound&body=Title%3A%20TextSAM-EUS%3A%20Text%20Prompt%20Learning%20for%20SAM%20to%20Accurately%20Segment%0A%20%20Pancreatic%20Tumor%20in%20Endoscopic%20Ultrasound%0AAuthor%3A%20Pascal%20Spiegler%20and%20Taha%20Koleilat%20and%20Arash%20Harirpoush%20and%20Corey%20S.%20Miller%20and%20Hassan%20Rivaz%20and%20Marta%20Kersten-Oertel%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Pancreatic%20cancer%20carries%20a%20poor%20prognosis%20and%20relies%20on%20endoscopic%0Aultrasound%20%28EUS%29%20for%20targeted%20biopsy%20and%20radiotherapy.%20However%2C%20the%20speckle%0Anoise%2C%20low%20contrast%2C%20and%20unintuitive%20appearance%20of%20EUS%20make%20segmentation%20of%0Apancreatic%20tumors%20with%20fully%20supervised%20deep%20learning%20%28DL%29%20models%20both%0Aerror-prone%20and%20dependent%20on%20large%2C%20expert-curated%20annotation%20datasets.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20TextSAM-EUS%2C%20a%20novel%2C%20lightweight%2C%0Atext-driven%20adaptation%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20that%20requires%20no%0Amanual%20geometric%20prompts%20at%20inference.%20Our%20approach%20leverages%20text%20prompt%0Alearning%20%28context%20optimization%29%20through%20the%20BiomedCLIP%20text%20encoder%20in%0Aconjunction%20with%20a%20LoRA-based%20adaptation%20of%20SAM%27s%20architecture%20to%20enable%0Aautomatic%20pancreatic%20tumor%20segmentation%20in%20EUS%2C%20tuning%20only%200.86%25%20of%20the%20total%0Aparameters.%20On%20the%20public%20Endoscopic%20Ultrasound%20Database%20of%20the%20Pancreas%2C%0ATextSAM-EUS%20with%20automatic%20prompts%20attains%2082.69%25%20Dice%20and%2085.28%25%20normalized%0Asurface%20distance%20%28NSD%29%2C%20and%20with%20manual%20geometric%20prompts%20reaches%2083.10%25%20Dice%0Aand%2085.70%25%20NSD%2C%20outperforming%20both%20existing%20state-of-the-art%20%28SOTA%29%20supervised%0ADL%20models%20and%20foundation%20models%20%28e.g.%2C%20SAM%20and%20its%20variants%29.%20As%20the%20first%0Aattempt%20to%20incorporate%20prompt%20learning%20in%20SAM-based%20medical%20image%20segmentation%2C%0ATextSAM-EUS%20offers%20a%20practical%20option%20for%20efficient%20and%20robust%20automatic%20EUS%0Asegmentation.%20Code%20is%20available%20at%20https%3A//github.com/HealthX-Lab/TextSAM-EUS%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18082v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextSAM-EUS%253A%2520Text%2520Prompt%2520Learning%2520for%2520SAM%2520to%2520Accurately%2520Segment%250A%2520%2520Pancreatic%2520Tumor%2520in%2520Endoscopic%2520Ultrasound%26entry.906535625%3DPascal%2520Spiegler%2520and%2520Taha%2520Koleilat%2520and%2520Arash%2520Harirpoush%2520and%2520Corey%2520S.%2520Miller%2520and%2520Hassan%2520Rivaz%2520and%2520Marta%2520Kersten-Oertel%2520and%2520Yiming%2520Xiao%26entry.1292438233%3D%2520%2520Pancreatic%2520cancer%2520carries%2520a%2520poor%2520prognosis%2520and%2520relies%2520on%2520endoscopic%250Aultrasound%2520%2528EUS%2529%2520for%2520targeted%2520biopsy%2520and%2520radiotherapy.%2520However%252C%2520the%2520speckle%250Anoise%252C%2520low%2520contrast%252C%2520and%2520unintuitive%2520appearance%2520of%2520EUS%2520make%2520segmentation%2520of%250Apancreatic%2520tumors%2520with%2520fully%2520supervised%2520deep%2520learning%2520%2528DL%2529%2520models%2520both%250Aerror-prone%2520and%2520dependent%2520on%2520large%252C%2520expert-curated%2520annotation%2520datasets.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520present%2520TextSAM-EUS%252C%2520a%2520novel%252C%2520lightweight%252C%250Atext-driven%2520adaptation%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520that%2520requires%2520no%250Amanual%2520geometric%2520prompts%2520at%2520inference.%2520Our%2520approach%2520leverages%2520text%2520prompt%250Alearning%2520%2528context%2520optimization%2529%2520through%2520the%2520BiomedCLIP%2520text%2520encoder%2520in%250Aconjunction%2520with%2520a%2520LoRA-based%2520adaptation%2520of%2520SAM%2527s%2520architecture%2520to%2520enable%250Aautomatic%2520pancreatic%2520tumor%2520segmentation%2520in%2520EUS%252C%2520tuning%2520only%25200.86%2525%2520of%2520the%2520total%250Aparameters.%2520On%2520the%2520public%2520Endoscopic%2520Ultrasound%2520Database%2520of%2520the%2520Pancreas%252C%250ATextSAM-EUS%2520with%2520automatic%2520prompts%2520attains%252082.69%2525%2520Dice%2520and%252085.28%2525%2520normalized%250Asurface%2520distance%2520%2528NSD%2529%252C%2520and%2520with%2520manual%2520geometric%2520prompts%2520reaches%252083.10%2525%2520Dice%250Aand%252085.70%2525%2520NSD%252C%2520outperforming%2520both%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520supervised%250ADL%2520models%2520and%2520foundation%2520models%2520%2528e.g.%252C%2520SAM%2520and%2520its%2520variants%2529.%2520As%2520the%2520first%250Aattempt%2520to%2520incorporate%2520prompt%2520learning%2520in%2520SAM-based%2520medical%2520image%2520segmentation%252C%250ATextSAM-EUS%2520offers%2520a%2520practical%2520option%2520for%2520efficient%2520and%2520robust%2520automatic%2520EUS%250Asegmentation.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/HealthX-Lab/TextSAM-EUS%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18082v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextSAM-EUS%3A%20Text%20Prompt%20Learning%20for%20SAM%20to%20Accurately%20Segment%0A%20%20Pancreatic%20Tumor%20in%20Endoscopic%20Ultrasound&entry.906535625=Pascal%20Spiegler%20and%20Taha%20Koleilat%20and%20Arash%20Harirpoush%20and%20Corey%20S.%20Miller%20and%20Hassan%20Rivaz%20and%20Marta%20Kersten-Oertel%20and%20Yiming%20Xiao&entry.1292438233=%20%20Pancreatic%20cancer%20carries%20a%20poor%20prognosis%20and%20relies%20on%20endoscopic%0Aultrasound%20%28EUS%29%20for%20targeted%20biopsy%20and%20radiotherapy.%20However%2C%20the%20speckle%0Anoise%2C%20low%20contrast%2C%20and%20unintuitive%20appearance%20of%20EUS%20make%20segmentation%20of%0Apancreatic%20tumors%20with%20fully%20supervised%20deep%20learning%20%28DL%29%20models%20both%0Aerror-prone%20and%20dependent%20on%20large%2C%20expert-curated%20annotation%20datasets.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20TextSAM-EUS%2C%20a%20novel%2C%20lightweight%2C%0Atext-driven%20adaptation%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20that%20requires%20no%0Amanual%20geometric%20prompts%20at%20inference.%20Our%20approach%20leverages%20text%20prompt%0Alearning%20%28context%20optimization%29%20through%20the%20BiomedCLIP%20text%20encoder%20in%0Aconjunction%20with%20a%20LoRA-based%20adaptation%20of%20SAM%27s%20architecture%20to%20enable%0Aautomatic%20pancreatic%20tumor%20segmentation%20in%20EUS%2C%20tuning%20only%200.86%25%20of%20the%20total%0Aparameters.%20On%20the%20public%20Endoscopic%20Ultrasound%20Database%20of%20the%20Pancreas%2C%0ATextSAM-EUS%20with%20automatic%20prompts%20attains%2082.69%25%20Dice%20and%2085.28%25%20normalized%0Asurface%20distance%20%28NSD%29%2C%20and%20with%20manual%20geometric%20prompts%20reaches%2083.10%25%20Dice%0Aand%2085.70%25%20NSD%2C%20outperforming%20both%20existing%20state-of-the-art%20%28SOTA%29%20supervised%0ADL%20models%20and%20foundation%20models%20%28e.g.%2C%20SAM%20and%20its%20variants%29.%20As%20the%20first%0Aattempt%20to%20incorporate%20prompt%20learning%20in%20SAM-based%20medical%20image%20segmentation%2C%0ATextSAM-EUS%20offers%20a%20practical%20option%20for%20efficient%20and%20robust%20automatic%20EUS%0Asegmentation.%20Code%20is%20available%20at%20https%3A//github.com/HealthX-Lab/TextSAM-EUS%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18082v3&entry.124074799=Read"},
{"title": "HRVVS: A High-resolution Video Vasculature Segmentation Network via\n  Hierarchical Autoregressive Residual Priors", "author": "Xincheng Yao and Yijun Yang and Kangwei Guo and Ruiqiang Xiao and Haipeng Zhou and Haisu Tao and Jian Yang and Lei Zhu", "abstract": "  The segmentation of the hepatic vasculature in surgical videos holds\nsubstantial clinical significance in the context of hepatectomy procedures.\nHowever, owing to the dearth of an appropriate dataset and the inherently\ncomplex task characteristics, few researches have been reported in this domain.\nTo address this issue, we first introduce a high quality frame-by-frame\nannotated hepatic vasculature dataset containing 35 long hepatectomy videos and\n11442 high-resolution frames. On this basis, we propose a novel high-resolution\nvideo vasculature segmentation network, dubbed as HRVVS. We innovatively embed\na pretrained visual autoregressive modeling (VAR) model into different layers\nof the hierarchical encoder as prior information to reduce the information\ndegradation generated during the downsampling process. In addition, we designed\na dynamic memory decoder on a multi-view segmentation network to minimize the\ntransmission of redundant information while preserving more details between\nframes. Extensive experiments on surgical video datasets demonstrate that our\nproposed HRVVS significantly outperforms the state-of-the-art methods. The\nsource code and dataset will be publicly available at\n\\href{https://github.com/scott-yjyang/xx}{https://github.com/scott-yjyang/HRVVS}.\n", "link": "http://arxiv.org/abs/2507.22530v1", "date": "2025-07-30", "relevancy": 2.047, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5364}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5118}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HRVVS%3A%20A%20High-resolution%20Video%20Vasculature%20Segmentation%20Network%20via%0A%20%20Hierarchical%20Autoregressive%20Residual%20Priors&body=Title%3A%20HRVVS%3A%20A%20High-resolution%20Video%20Vasculature%20Segmentation%20Network%20via%0A%20%20Hierarchical%20Autoregressive%20Residual%20Priors%0AAuthor%3A%20Xincheng%20Yao%20and%20Yijun%20Yang%20and%20Kangwei%20Guo%20and%20Ruiqiang%20Xiao%20and%20Haipeng%20Zhou%20and%20Haisu%20Tao%20and%20Jian%20Yang%20and%20Lei%20Zhu%0AAbstract%3A%20%20%20The%20segmentation%20of%20the%20hepatic%20vasculature%20in%20surgical%20videos%20holds%0Asubstantial%20clinical%20significance%20in%20the%20context%20of%20hepatectomy%20procedures.%0AHowever%2C%20owing%20to%20the%20dearth%20of%20an%20appropriate%20dataset%20and%20the%20inherently%0Acomplex%20task%20characteristics%2C%20few%20researches%20have%20been%20reported%20in%20this%20domain.%0ATo%20address%20this%20issue%2C%20we%20first%20introduce%20a%20high%20quality%20frame-by-frame%0Aannotated%20hepatic%20vasculature%20dataset%20containing%2035%20long%20hepatectomy%20videos%20and%0A11442%20high-resolution%20frames.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20high-resolution%0Avideo%20vasculature%20segmentation%20network%2C%20dubbed%20as%20HRVVS.%20We%20innovatively%20embed%0Aa%20pretrained%20visual%20autoregressive%20modeling%20%28VAR%29%20model%20into%20different%20layers%0Aof%20the%20hierarchical%20encoder%20as%20prior%20information%20to%20reduce%20the%20information%0Adegradation%20generated%20during%20the%20downsampling%20process.%20In%20addition%2C%20we%20designed%0Aa%20dynamic%20memory%20decoder%20on%20a%20multi-view%20segmentation%20network%20to%20minimize%20the%0Atransmission%20of%20redundant%20information%20while%20preserving%20more%20details%20between%0Aframes.%20Extensive%20experiments%20on%20surgical%20video%20datasets%20demonstrate%20that%20our%0Aproposed%20HRVVS%20significantly%20outperforms%20the%20state-of-the-art%20methods.%20The%0Asource%20code%20and%20dataset%20will%20be%20publicly%20available%20at%0A%5Chref%7Bhttps%3A//github.com/scott-yjyang/xx%7D%7Bhttps%3A//github.com/scott-yjyang/HRVVS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHRVVS%253A%2520A%2520High-resolution%2520Video%2520Vasculature%2520Segmentation%2520Network%2520via%250A%2520%2520Hierarchical%2520Autoregressive%2520Residual%2520Priors%26entry.906535625%3DXincheng%2520Yao%2520and%2520Yijun%2520Yang%2520and%2520Kangwei%2520Guo%2520and%2520Ruiqiang%2520Xiao%2520and%2520Haipeng%2520Zhou%2520and%2520Haisu%2520Tao%2520and%2520Jian%2520Yang%2520and%2520Lei%2520Zhu%26entry.1292438233%3D%2520%2520The%2520segmentation%2520of%2520the%2520hepatic%2520vasculature%2520in%2520surgical%2520videos%2520holds%250Asubstantial%2520clinical%2520significance%2520in%2520the%2520context%2520of%2520hepatectomy%2520procedures.%250AHowever%252C%2520owing%2520to%2520the%2520dearth%2520of%2520an%2520appropriate%2520dataset%2520and%2520the%2520inherently%250Acomplex%2520task%2520characteristics%252C%2520few%2520researches%2520have%2520been%2520reported%2520in%2520this%2520domain.%250ATo%2520address%2520this%2520issue%252C%2520we%2520first%2520introduce%2520a%2520high%2520quality%2520frame-by-frame%250Aannotated%2520hepatic%2520vasculature%2520dataset%2520containing%252035%2520long%2520hepatectomy%2520videos%2520and%250A11442%2520high-resolution%2520frames.%2520On%2520this%2520basis%252C%2520we%2520propose%2520a%2520novel%2520high-resolution%250Avideo%2520vasculature%2520segmentation%2520network%252C%2520dubbed%2520as%2520HRVVS.%2520We%2520innovatively%2520embed%250Aa%2520pretrained%2520visual%2520autoregressive%2520modeling%2520%2528VAR%2529%2520model%2520into%2520different%2520layers%250Aof%2520the%2520hierarchical%2520encoder%2520as%2520prior%2520information%2520to%2520reduce%2520the%2520information%250Adegradation%2520generated%2520during%2520the%2520downsampling%2520process.%2520In%2520addition%252C%2520we%2520designed%250Aa%2520dynamic%2520memory%2520decoder%2520on%2520a%2520multi-view%2520segmentation%2520network%2520to%2520minimize%2520the%250Atransmission%2520of%2520redundant%2520information%2520while%2520preserving%2520more%2520details%2520between%250Aframes.%2520Extensive%2520experiments%2520on%2520surgical%2520video%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520HRVVS%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520methods.%2520The%250Asource%2520code%2520and%2520dataset%2520will%2520be%2520publicly%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/scott-yjyang/xx%257D%257Bhttps%253A//github.com/scott-yjyang/HRVVS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRVVS%3A%20A%20High-resolution%20Video%20Vasculature%20Segmentation%20Network%20via%0A%20%20Hierarchical%20Autoregressive%20Residual%20Priors&entry.906535625=Xincheng%20Yao%20and%20Yijun%20Yang%20and%20Kangwei%20Guo%20and%20Ruiqiang%20Xiao%20and%20Haipeng%20Zhou%20and%20Haisu%20Tao%20and%20Jian%20Yang%20and%20Lei%20Zhu&entry.1292438233=%20%20The%20segmentation%20of%20the%20hepatic%20vasculature%20in%20surgical%20videos%20holds%0Asubstantial%20clinical%20significance%20in%20the%20context%20of%20hepatectomy%20procedures.%0AHowever%2C%20owing%20to%20the%20dearth%20of%20an%20appropriate%20dataset%20and%20the%20inherently%0Acomplex%20task%20characteristics%2C%20few%20researches%20have%20been%20reported%20in%20this%20domain.%0ATo%20address%20this%20issue%2C%20we%20first%20introduce%20a%20high%20quality%20frame-by-frame%0Aannotated%20hepatic%20vasculature%20dataset%20containing%2035%20long%20hepatectomy%20videos%20and%0A11442%20high-resolution%20frames.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20high-resolution%0Avideo%20vasculature%20segmentation%20network%2C%20dubbed%20as%20HRVVS.%20We%20innovatively%20embed%0Aa%20pretrained%20visual%20autoregressive%20modeling%20%28VAR%29%20model%20into%20different%20layers%0Aof%20the%20hierarchical%20encoder%20as%20prior%20information%20to%20reduce%20the%20information%0Adegradation%20generated%20during%20the%20downsampling%20process.%20In%20addition%2C%20we%20designed%0Aa%20dynamic%20memory%20decoder%20on%20a%20multi-view%20segmentation%20network%20to%20minimize%20the%0Atransmission%20of%20redundant%20information%20while%20preserving%20more%20details%20between%0Aframes.%20Extensive%20experiments%20on%20surgical%20video%20datasets%20demonstrate%20that%20our%0Aproposed%20HRVVS%20significantly%20outperforms%20the%20state-of-the-art%20methods.%20The%0Asource%20code%20and%20dataset%20will%20be%20publicly%20available%20at%0A%5Chref%7Bhttps%3A//github.com/scott-yjyang/xx%7D%7Bhttps%3A//github.com/scott-yjyang/HRVVS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22530v1&entry.124074799=Read"},
{"title": "TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning", "author": "Siqi Luo and Haoran Yang and Yi Xin and Mingyang Yi and Guangyang Wu and Guangtao Zhai and Xiaohong Liu", "abstract": "  Large pre-trained models achieve remarkable performance in vision tasks but\nare impractical for fine-tuning due to high computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods mitigate this issue by updating\nonly a subset of parameters; however, most existing approaches are\ntask-agnostic, failing to fully exploit task-specific adaptations, which leads\nto suboptimal efficiency and performance. To address this limitation, we\npropose Task-Relevant Parameter and Token Selection (TR-PTS), a task-driven\nframework that enhances both computational efficiency and accuracy.\nSpecifically, we introduce Task-Relevant Parameter Selection, which utilizes\nthe Fisher Information Matrix (FIM) to identify and fine-tune only the most\ninformative parameters in a layer-wise manner, while keeping the remaining\nparameters frozen. Simultaneously, Task-Relevant Token Selection dynamically\npreserves the most informative tokens and merges redundant ones, reducing\ncomputational overhead. By jointly optimizing parameters and tokens, TR-PTS\nenables the model to concentrate on task-discriminative information. We\nevaluate TR-PTS on benchmark, including FGVC and VTAB-1k, where it achieves\nstate-of-the-art performance, surpassing full fine-tuning by 3.40% and 10.35%,\nrespectively. The code are available at https://github.com/synbol/TR-PTS.\n", "link": "http://arxiv.org/abs/2507.22872v1", "date": "2025-07-30", "relevancy": 2.0424, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4899}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TR-PTS%3A%20Task-Relevant%20Parameter%20and%20Token%20Selection%20for%20Efficient%20Tuning&body=Title%3A%20TR-PTS%3A%20Task-Relevant%20Parameter%20and%20Token%20Selection%20for%20Efficient%20Tuning%0AAuthor%3A%20Siqi%20Luo%20and%20Haoran%20Yang%20and%20Yi%20Xin%20and%20Mingyang%20Yi%20and%20Guangyang%20Wu%20and%20Guangtao%20Zhai%20and%20Xiaohong%20Liu%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20achieve%20remarkable%20performance%20in%20vision%20tasks%20but%0Aare%20impractical%20for%20fine-tuning%20due%20to%20high%20computational%20and%20storage%20costs.%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20mitigate%20this%20issue%20by%20updating%0Aonly%20a%20subset%20of%20parameters%3B%20however%2C%20most%20existing%20approaches%20are%0Atask-agnostic%2C%20failing%20to%20fully%20exploit%20task-specific%20adaptations%2C%20which%20leads%0Ato%20suboptimal%20efficiency%20and%20performance.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Task-Relevant%20Parameter%20and%20Token%20Selection%20%28TR-PTS%29%2C%20a%20task-driven%0Aframework%20that%20enhances%20both%20computational%20efficiency%20and%20accuracy.%0ASpecifically%2C%20we%20introduce%20Task-Relevant%20Parameter%20Selection%2C%20which%20utilizes%0Athe%20Fisher%20Information%20Matrix%20%28FIM%29%20to%20identify%20and%20fine-tune%20only%20the%20most%0Ainformative%20parameters%20in%20a%20layer-wise%20manner%2C%20while%20keeping%20the%20remaining%0Aparameters%20frozen.%20Simultaneously%2C%20Task-Relevant%20Token%20Selection%20dynamically%0Apreserves%20the%20most%20informative%20tokens%20and%20merges%20redundant%20ones%2C%20reducing%0Acomputational%20overhead.%20By%20jointly%20optimizing%20parameters%20and%20tokens%2C%20TR-PTS%0Aenables%20the%20model%20to%20concentrate%20on%20task-discriminative%20information.%20We%0Aevaluate%20TR-PTS%20on%20benchmark%2C%20including%20FGVC%20and%20VTAB-1k%2C%20where%20it%20achieves%0Astate-of-the-art%20performance%2C%20surpassing%20full%20fine-tuning%20by%203.40%25%20and%2010.35%25%2C%0Arespectively.%20The%20code%20are%20available%20at%20https%3A//github.com/synbol/TR-PTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTR-PTS%253A%2520Task-Relevant%2520Parameter%2520and%2520Token%2520Selection%2520for%2520Efficient%2520Tuning%26entry.906535625%3DSiqi%2520Luo%2520and%2520Haoran%2520Yang%2520and%2520Yi%2520Xin%2520and%2520Mingyang%2520Yi%2520and%2520Guangyang%2520Wu%2520and%2520Guangtao%2520Zhai%2520and%2520Xiaohong%2520Liu%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520models%2520achieve%2520remarkable%2520performance%2520in%2520vision%2520tasks%2520but%250Aare%2520impractical%2520for%2520fine-tuning%2520due%2520to%2520high%2520computational%2520and%2520storage%2520costs.%250AParameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520mitigate%2520this%2520issue%2520by%2520updating%250Aonly%2520a%2520subset%2520of%2520parameters%253B%2520however%252C%2520most%2520existing%2520approaches%2520are%250Atask-agnostic%252C%2520failing%2520to%2520fully%2520exploit%2520task-specific%2520adaptations%252C%2520which%2520leads%250Ato%2520suboptimal%2520efficiency%2520and%2520performance.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520Task-Relevant%2520Parameter%2520and%2520Token%2520Selection%2520%2528TR-PTS%2529%252C%2520a%2520task-driven%250Aframework%2520that%2520enhances%2520both%2520computational%2520efficiency%2520and%2520accuracy.%250ASpecifically%252C%2520we%2520introduce%2520Task-Relevant%2520Parameter%2520Selection%252C%2520which%2520utilizes%250Athe%2520Fisher%2520Information%2520Matrix%2520%2528FIM%2529%2520to%2520identify%2520and%2520fine-tune%2520only%2520the%2520most%250Ainformative%2520parameters%2520in%2520a%2520layer-wise%2520manner%252C%2520while%2520keeping%2520the%2520remaining%250Aparameters%2520frozen.%2520Simultaneously%252C%2520Task-Relevant%2520Token%2520Selection%2520dynamically%250Apreserves%2520the%2520most%2520informative%2520tokens%2520and%2520merges%2520redundant%2520ones%252C%2520reducing%250Acomputational%2520overhead.%2520By%2520jointly%2520optimizing%2520parameters%2520and%2520tokens%252C%2520TR-PTS%250Aenables%2520the%2520model%2520to%2520concentrate%2520on%2520task-discriminative%2520information.%2520We%250Aevaluate%2520TR-PTS%2520on%2520benchmark%252C%2520including%2520FGVC%2520and%2520VTAB-1k%252C%2520where%2520it%2520achieves%250Astate-of-the-art%2520performance%252C%2520surpassing%2520full%2520fine-tuning%2520by%25203.40%2525%2520and%252010.35%2525%252C%250Arespectively.%2520The%2520code%2520are%2520available%2520at%2520https%253A//github.com/synbol/TR-PTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TR-PTS%3A%20Task-Relevant%20Parameter%20and%20Token%20Selection%20for%20Efficient%20Tuning&entry.906535625=Siqi%20Luo%20and%20Haoran%20Yang%20and%20Yi%20Xin%20and%20Mingyang%20Yi%20and%20Guangyang%20Wu%20and%20Guangtao%20Zhai%20and%20Xiaohong%20Liu&entry.1292438233=%20%20Large%20pre-trained%20models%20achieve%20remarkable%20performance%20in%20vision%20tasks%20but%0Aare%20impractical%20for%20fine-tuning%20due%20to%20high%20computational%20and%20storage%20costs.%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20mitigate%20this%20issue%20by%20updating%0Aonly%20a%20subset%20of%20parameters%3B%20however%2C%20most%20existing%20approaches%20are%0Atask-agnostic%2C%20failing%20to%20fully%20exploit%20task-specific%20adaptations%2C%20which%20leads%0Ato%20suboptimal%20efficiency%20and%20performance.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Task-Relevant%20Parameter%20and%20Token%20Selection%20%28TR-PTS%29%2C%20a%20task-driven%0Aframework%20that%20enhances%20both%20computational%20efficiency%20and%20accuracy.%0ASpecifically%2C%20we%20introduce%20Task-Relevant%20Parameter%20Selection%2C%20which%20utilizes%0Athe%20Fisher%20Information%20Matrix%20%28FIM%29%20to%20identify%20and%20fine-tune%20only%20the%20most%0Ainformative%20parameters%20in%20a%20layer-wise%20manner%2C%20while%20keeping%20the%20remaining%0Aparameters%20frozen.%20Simultaneously%2C%20Task-Relevant%20Token%20Selection%20dynamically%0Apreserves%20the%20most%20informative%20tokens%20and%20merges%20redundant%20ones%2C%20reducing%0Acomputational%20overhead.%20By%20jointly%20optimizing%20parameters%20and%20tokens%2C%20TR-PTS%0Aenables%20the%20model%20to%20concentrate%20on%20task-discriminative%20information.%20We%0Aevaluate%20TR-PTS%20on%20benchmark%2C%20including%20FGVC%20and%20VTAB-1k%2C%20where%20it%20achieves%0Astate-of-the-art%20performance%2C%20surpassing%20full%20fine-tuning%20by%203.40%25%20and%2010.35%25%2C%0Arespectively.%20The%20code%20are%20available%20at%20https%3A//github.com/synbol/TR-PTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22872v1&entry.124074799=Read"},
{"title": "Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction\n  Determination", "author": "Weicheng Gao", "abstract": "  This work is completed on a whim after discussions with my junior colleague.\nThe motion direction angle affects the micro-Doppler spectrum width, thus\ndetermining the human motion direction can provide important prior information\nfor downstream tasks such as gait recognition. However, Doppler-Time map\n(DTM)-based methods still have room for improvement in achieving feature\naugmentation and motion determination simultaneously. In response, a low-cost\nbut accurate radar-based human motion direction determination (HMDD) method is\nexplored in this paper. In detail, the radar-based human gait DTMs are first\ngenerated, and then the feature augmentation is achieved using feature linking\nmodel. Subsequently, the HMDD is implemented through a lightweight and fast\nVision Transformer-Convolutional Neural Network hybrid model structure. The\neffectiveness of the proposed method is verified through open-source dataset.\nThe open-source code of this work is released at:\nhttps://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.\n", "link": "http://arxiv.org/abs/2507.22567v1", "date": "2025-07-30", "relevancy": 2.0421, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5193}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploration%20of%20Low-Cost%20but%20Accurate%20Radar-Based%20Human%20Motion%20Direction%0A%20%20Determination&body=Title%3A%20Exploration%20of%20Low-Cost%20but%20Accurate%20Radar-Based%20Human%20Motion%20Direction%0A%20%20Determination%0AAuthor%3A%20Weicheng%20Gao%0AAbstract%3A%20%20%20This%20work%20is%20completed%20on%20a%20whim%20after%20discussions%20with%20my%20junior%20colleague.%0AThe%20motion%20direction%20angle%20affects%20the%20micro-Doppler%20spectrum%20width%2C%20thus%0Adetermining%20the%20human%20motion%20direction%20can%20provide%20important%20prior%20information%0Afor%20downstream%20tasks%20such%20as%20gait%20recognition.%20However%2C%20Doppler-Time%20map%0A%28DTM%29-based%20methods%20still%20have%20room%20for%20improvement%20in%20achieving%20feature%0Aaugmentation%20and%20motion%20determination%20simultaneously.%20In%20response%2C%20a%20low-cost%0Abut%20accurate%20radar-based%20human%20motion%20direction%20determination%20%28HMDD%29%20method%20is%0Aexplored%20in%20this%20paper.%20In%20detail%2C%20the%20radar-based%20human%20gait%20DTMs%20are%20first%0Agenerated%2C%20and%20then%20the%20feature%20augmentation%20is%20achieved%20using%20feature%20linking%0Amodel.%20Subsequently%2C%20the%20HMDD%20is%20implemented%20through%20a%20lightweight%20and%20fast%0AVision%20Transformer-Convolutional%20Neural%20Network%20hybrid%20model%20structure.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20is%20verified%20through%20open-source%20dataset.%0AThe%20open-source%20code%20of%20this%20work%20is%20released%20at%3A%0Ahttps%3A//github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploration%2520of%2520Low-Cost%2520but%2520Accurate%2520Radar-Based%2520Human%2520Motion%2520Direction%250A%2520%2520Determination%26entry.906535625%3DWeicheng%2520Gao%26entry.1292438233%3D%2520%2520This%2520work%2520is%2520completed%2520on%2520a%2520whim%2520after%2520discussions%2520with%2520my%2520junior%2520colleague.%250AThe%2520motion%2520direction%2520angle%2520affects%2520the%2520micro-Doppler%2520spectrum%2520width%252C%2520thus%250Adetermining%2520the%2520human%2520motion%2520direction%2520can%2520provide%2520important%2520prior%2520information%250Afor%2520downstream%2520tasks%2520such%2520as%2520gait%2520recognition.%2520However%252C%2520Doppler-Time%2520map%250A%2528DTM%2529-based%2520methods%2520still%2520have%2520room%2520for%2520improvement%2520in%2520achieving%2520feature%250Aaugmentation%2520and%2520motion%2520determination%2520simultaneously.%2520In%2520response%252C%2520a%2520low-cost%250Abut%2520accurate%2520radar-based%2520human%2520motion%2520direction%2520determination%2520%2528HMDD%2529%2520method%2520is%250Aexplored%2520in%2520this%2520paper.%2520In%2520detail%252C%2520the%2520radar-based%2520human%2520gait%2520DTMs%2520are%2520first%250Agenerated%252C%2520and%2520then%2520the%2520feature%2520augmentation%2520is%2520achieved%2520using%2520feature%2520linking%250Amodel.%2520Subsequently%252C%2520the%2520HMDD%2520is%2520implemented%2520through%2520a%2520lightweight%2520and%2520fast%250AVision%2520Transformer-Convolutional%2520Neural%2520Network%2520hybrid%2520model%2520structure.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520is%2520verified%2520through%2520open-source%2520dataset.%250AThe%2520open-source%2520code%2520of%2520this%2520work%2520is%2520released%2520at%253A%250Ahttps%253A//github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploration%20of%20Low-Cost%20but%20Accurate%20Radar-Based%20Human%20Motion%20Direction%0A%20%20Determination&entry.906535625=Weicheng%20Gao&entry.1292438233=%20%20This%20work%20is%20completed%20on%20a%20whim%20after%20discussions%20with%20my%20junior%20colleague.%0AThe%20motion%20direction%20angle%20affects%20the%20micro-Doppler%20spectrum%20width%2C%20thus%0Adetermining%20the%20human%20motion%20direction%20can%20provide%20important%20prior%20information%0Afor%20downstream%20tasks%20such%20as%20gait%20recognition.%20However%2C%20Doppler-Time%20map%0A%28DTM%29-based%20methods%20still%20have%20room%20for%20improvement%20in%20achieving%20feature%0Aaugmentation%20and%20motion%20determination%20simultaneously.%20In%20response%2C%20a%20low-cost%0Abut%20accurate%20radar-based%20human%20motion%20direction%20determination%20%28HMDD%29%20method%20is%0Aexplored%20in%20this%20paper.%20In%20detail%2C%20the%20radar-based%20human%20gait%20DTMs%20are%20first%0Agenerated%2C%20and%20then%20the%20feature%20augmentation%20is%20achieved%20using%20feature%20linking%0Amodel.%20Subsequently%2C%20the%20HMDD%20is%20implemented%20through%20a%20lightweight%20and%20fast%0AVision%20Transformer-Convolutional%20Neural%20Network%20hybrid%20model%20structure.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20is%20verified%20through%20open-source%20dataset.%0AThe%20open-source%20code%20of%20this%20work%20is%20released%20at%3A%0Ahttps%3A//github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22567v1&entry.124074799=Read"},
{"title": "Effective Non-Random Extreme Learning Machine", "author": "Daniela De Canditiis and Fabiano Veglianti", "abstract": "  The Extreme Learning Machine (ELM) is a growing statistical technique widely\napplied to regression problems. In essence, ELMs are single-layer neural\nnetworks where the hidden layer weights are randomly sampled from a specific\ndistribution, while the output layer weights are learned from the data. Two of\nthe key challenges with this approach are the architecture design, specifically\ndetermining the optimal number of neurons in the hidden layer, and the method's\nsensitivity to the random initialization of hidden layer weights.\n  This paper introduces a new and enhanced learning algorithm for regression\ntasks, the Effective Non-Random ELM (ENR-ELM), which simplifies the\narchitecture design and eliminates the need for random hidden layer weight\nselection. The proposed method incorporates concepts from signal processing,\nsuch as basis functions and projections, into the ELM framework. We introduce\ntwo versions of the ENR-ELM: the approximated ENR-ELM and the incremental\nENR-ELM. Experimental results on both synthetic and real datasets demonstrate\nthat our method overcomes the problems of traditional ELM while maintaining\ncomparable predictive performance.\n", "link": "http://arxiv.org/abs/2411.16229v2", "date": "2025-07-30", "relevancy": 2.0393, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4931}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Non-Random%20Extreme%20Learning%20Machine&body=Title%3A%20Effective%20Non-Random%20Extreme%20Learning%20Machine%0AAuthor%3A%20Daniela%20De%20Canditiis%20and%20Fabiano%20Veglianti%0AAbstract%3A%20%20%20The%20Extreme%20Learning%20Machine%20%28ELM%29%20is%20a%20growing%20statistical%20technique%20widely%0Aapplied%20to%20regression%20problems.%20In%20essence%2C%20ELMs%20are%20single-layer%20neural%0Anetworks%20where%20the%20hidden%20layer%20weights%20are%20randomly%20sampled%20from%20a%20specific%0Adistribution%2C%20while%20the%20output%20layer%20weights%20are%20learned%20from%20the%20data.%20Two%20of%0Athe%20key%20challenges%20with%20this%20approach%20are%20the%20architecture%20design%2C%20specifically%0Adetermining%20the%20optimal%20number%20of%20neurons%20in%20the%20hidden%20layer%2C%20and%20the%20method%27s%0Asensitivity%20to%20the%20random%20initialization%20of%20hidden%20layer%20weights.%0A%20%20This%20paper%20introduces%20a%20new%20and%20enhanced%20learning%20algorithm%20for%20regression%0Atasks%2C%20the%20Effective%20Non-Random%20ELM%20%28ENR-ELM%29%2C%20which%20simplifies%20the%0Aarchitecture%20design%20and%20eliminates%20the%20need%20for%20random%20hidden%20layer%20weight%0Aselection.%20The%20proposed%20method%20incorporates%20concepts%20from%20signal%20processing%2C%0Asuch%20as%20basis%20functions%20and%20projections%2C%20into%20the%20ELM%20framework.%20We%20introduce%0Atwo%20versions%20of%20the%20ENR-ELM%3A%20the%20approximated%20ENR-ELM%20and%20the%20incremental%0AENR-ELM.%20Experimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%0Athat%20our%20method%20overcomes%20the%20problems%20of%20traditional%20ELM%20while%20maintaining%0Acomparable%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Non-Random%2520Extreme%2520Learning%2520Machine%26entry.906535625%3DDaniela%2520De%2520Canditiis%2520and%2520Fabiano%2520Veglianti%26entry.1292438233%3D%2520%2520The%2520Extreme%2520Learning%2520Machine%2520%2528ELM%2529%2520is%2520a%2520growing%2520statistical%2520technique%2520widely%250Aapplied%2520to%2520regression%2520problems.%2520In%2520essence%252C%2520ELMs%2520are%2520single-layer%2520neural%250Anetworks%2520where%2520the%2520hidden%2520layer%2520weights%2520are%2520randomly%2520sampled%2520from%2520a%2520specific%250Adistribution%252C%2520while%2520the%2520output%2520layer%2520weights%2520are%2520learned%2520from%2520the%2520data.%2520Two%2520of%250Athe%2520key%2520challenges%2520with%2520this%2520approach%2520are%2520the%2520architecture%2520design%252C%2520specifically%250Adetermining%2520the%2520optimal%2520number%2520of%2520neurons%2520in%2520the%2520hidden%2520layer%252C%2520and%2520the%2520method%2527s%250Asensitivity%2520to%2520the%2520random%2520initialization%2520of%2520hidden%2520layer%2520weights.%250A%2520%2520This%2520paper%2520introduces%2520a%2520new%2520and%2520enhanced%2520learning%2520algorithm%2520for%2520regression%250Atasks%252C%2520the%2520Effective%2520Non-Random%2520ELM%2520%2528ENR-ELM%2529%252C%2520which%2520simplifies%2520the%250Aarchitecture%2520design%2520and%2520eliminates%2520the%2520need%2520for%2520random%2520hidden%2520layer%2520weight%250Aselection.%2520The%2520proposed%2520method%2520incorporates%2520concepts%2520from%2520signal%2520processing%252C%250Asuch%2520as%2520basis%2520functions%2520and%2520projections%252C%2520into%2520the%2520ELM%2520framework.%2520We%2520introduce%250Atwo%2520versions%2520of%2520the%2520ENR-ELM%253A%2520the%2520approximated%2520ENR-ELM%2520and%2520the%2520incremental%250AENR-ELM.%2520Experimental%2520results%2520on%2520both%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%250Athat%2520our%2520method%2520overcomes%2520the%2520problems%2520of%2520traditional%2520ELM%2520while%2520maintaining%250Acomparable%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Non-Random%20Extreme%20Learning%20Machine&entry.906535625=Daniela%20De%20Canditiis%20and%20Fabiano%20Veglianti&entry.1292438233=%20%20The%20Extreme%20Learning%20Machine%20%28ELM%29%20is%20a%20growing%20statistical%20technique%20widely%0Aapplied%20to%20regression%20problems.%20In%20essence%2C%20ELMs%20are%20single-layer%20neural%0Anetworks%20where%20the%20hidden%20layer%20weights%20are%20randomly%20sampled%20from%20a%20specific%0Adistribution%2C%20while%20the%20output%20layer%20weights%20are%20learned%20from%20the%20data.%20Two%20of%0Athe%20key%20challenges%20with%20this%20approach%20are%20the%20architecture%20design%2C%20specifically%0Adetermining%20the%20optimal%20number%20of%20neurons%20in%20the%20hidden%20layer%2C%20and%20the%20method%27s%0Asensitivity%20to%20the%20random%20initialization%20of%20hidden%20layer%20weights.%0A%20%20This%20paper%20introduces%20a%20new%20and%20enhanced%20learning%20algorithm%20for%20regression%0Atasks%2C%20the%20Effective%20Non-Random%20ELM%20%28ENR-ELM%29%2C%20which%20simplifies%20the%0Aarchitecture%20design%20and%20eliminates%20the%20need%20for%20random%20hidden%20layer%20weight%0Aselection.%20The%20proposed%20method%20incorporates%20concepts%20from%20signal%20processing%2C%0Asuch%20as%20basis%20functions%20and%20projections%2C%20into%20the%20ELM%20framework.%20We%20introduce%0Atwo%20versions%20of%20the%20ENR-ELM%3A%20the%20approximated%20ENR-ELM%20and%20the%20incremental%0AENR-ELM.%20Experimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%0Athat%20our%20method%20overcomes%20the%20problems%20of%20traditional%20ELM%20while%20maintaining%0Acomparable%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16229v2&entry.124074799=Read"},
{"title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba", "author": "Juncan Deng and Shuaiting Li and Zeyu Wang and Kedong Xu and Hong Gu and Kejie Huang", "abstract": "  Visual Mamba networks (ViMs) extend the selective state space model (Mamba)\nto various vision tasks and demonstrate significant potential. As a promising\ncompression technique, vector quantization (VQ) decomposes network weights into\ncodebooks and assignments, significantly reducing memory usage and\ncomputational latency, thereby enabling the deployment of ViMs on edge devices.\nAlthough existing VQ methods have achieved extremely low-bit quantization\n(e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and\nTransformer-based networks, directly applying these methods to ViMs results in\nunsatisfactory accuracy. We identify several key challenges: 1) The weights of\nMamba-based blocks in ViMs contain numerous outliers, significantly amplifying\nquantization errors. 2) When applied to ViMs, the latest VQ methods suffer from\nexcessive memory consumption, lengthy calibration procedures, and suboptimal\nperformance in the search for optimal codewords. In this paper, we propose\nViM-VQ, an efficient post-training vector quantization method tailored for\nViMs. ViM-VQ consists of two innovative components: 1) a fast convex\ncombination optimization algorithm that efficiently updates both the convex\ncombinations and the convex hulls to search for optimal codewords, and 2) an\nincremental vector quantization strategy that incrementally confirms optimal\ncodewords to mitigate truncation errors. Experimental results demonstrate that\nViM-VQ achieves state-of-the-art performance in low-bit quantization across\nvarious visual tasks.\n", "link": "http://arxiv.org/abs/2503.09509v2", "date": "2025-07-30", "relevancy": 2.039, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.522}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5013}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViM-VQ%3A%20Efficient%20Post-Training%20Vector%20Quantization%20for%20Visual%20Mamba&body=Title%3A%20ViM-VQ%3A%20Efficient%20Post-Training%20Vector%20Quantization%20for%20Visual%20Mamba%0AAuthor%3A%20Juncan%20Deng%20and%20Shuaiting%20Li%20and%20Zeyu%20Wang%20and%20Kedong%20Xu%20and%20Hong%20Gu%20and%20Kejie%20Huang%0AAbstract%3A%20%20%20Visual%20Mamba%20networks%20%28ViMs%29%20extend%20the%20selective%20state%20space%20model%20%28Mamba%29%0Ato%20various%20vision%20tasks%20and%20demonstrate%20significant%20potential.%20As%20a%20promising%0Acompression%20technique%2C%20vector%20quantization%20%28VQ%29%20decomposes%20network%20weights%20into%0Acodebooks%20and%20assignments%2C%20significantly%20reducing%20memory%20usage%20and%0Acomputational%20latency%2C%20thereby%20enabling%20the%20deployment%20of%20ViMs%20on%20edge%20devices.%0AAlthough%20existing%20VQ%20methods%20have%20achieved%20extremely%20low-bit%20quantization%0A%28e.g.%2C%203-bit%2C%202-bit%2C%20and%201-bit%29%20in%20convolutional%20neural%20networks%20and%0ATransformer-based%20networks%2C%20directly%20applying%20these%20methods%20to%20ViMs%20results%20in%0Aunsatisfactory%20accuracy.%20We%20identify%20several%20key%20challenges%3A%201%29%20The%20weights%20of%0AMamba-based%20blocks%20in%20ViMs%20contain%20numerous%20outliers%2C%20significantly%20amplifying%0Aquantization%20errors.%202%29%20When%20applied%20to%20ViMs%2C%20the%20latest%20VQ%20methods%20suffer%20from%0Aexcessive%20memory%20consumption%2C%20lengthy%20calibration%20procedures%2C%20and%20suboptimal%0Aperformance%20in%20the%20search%20for%20optimal%20codewords.%20In%20this%20paper%2C%20we%20propose%0AViM-VQ%2C%20an%20efficient%20post-training%20vector%20quantization%20method%20tailored%20for%0AViMs.%20ViM-VQ%20consists%20of%20two%20innovative%20components%3A%201%29%20a%20fast%20convex%0Acombination%20optimization%20algorithm%20that%20efficiently%20updates%20both%20the%20convex%0Acombinations%20and%20the%20convex%20hulls%20to%20search%20for%20optimal%20codewords%2C%20and%202%29%20an%0Aincremental%20vector%20quantization%20strategy%20that%20incrementally%20confirms%20optimal%0Acodewords%20to%20mitigate%20truncation%20errors.%20Experimental%20results%20demonstrate%20that%0AViM-VQ%20achieves%20state-of-the-art%20performance%20in%20low-bit%20quantization%20across%0Avarious%20visual%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViM-VQ%253A%2520Efficient%2520Post-Training%2520Vector%2520Quantization%2520for%2520Visual%2520Mamba%26entry.906535625%3DJuncan%2520Deng%2520and%2520Shuaiting%2520Li%2520and%2520Zeyu%2520Wang%2520and%2520Kedong%2520Xu%2520and%2520Hong%2520Gu%2520and%2520Kejie%2520Huang%26entry.1292438233%3D%2520%2520Visual%2520Mamba%2520networks%2520%2528ViMs%2529%2520extend%2520the%2520selective%2520state%2520space%2520model%2520%2528Mamba%2529%250Ato%2520various%2520vision%2520tasks%2520and%2520demonstrate%2520significant%2520potential.%2520As%2520a%2520promising%250Acompression%2520technique%252C%2520vector%2520quantization%2520%2528VQ%2529%2520decomposes%2520network%2520weights%2520into%250Acodebooks%2520and%2520assignments%252C%2520significantly%2520reducing%2520memory%2520usage%2520and%250Acomputational%2520latency%252C%2520thereby%2520enabling%2520the%2520deployment%2520of%2520ViMs%2520on%2520edge%2520devices.%250AAlthough%2520existing%2520VQ%2520methods%2520have%2520achieved%2520extremely%2520low-bit%2520quantization%250A%2528e.g.%252C%25203-bit%252C%25202-bit%252C%2520and%25201-bit%2529%2520in%2520convolutional%2520neural%2520networks%2520and%250ATransformer-based%2520networks%252C%2520directly%2520applying%2520these%2520methods%2520to%2520ViMs%2520results%2520in%250Aunsatisfactory%2520accuracy.%2520We%2520identify%2520several%2520key%2520challenges%253A%25201%2529%2520The%2520weights%2520of%250AMamba-based%2520blocks%2520in%2520ViMs%2520contain%2520numerous%2520outliers%252C%2520significantly%2520amplifying%250Aquantization%2520errors.%25202%2529%2520When%2520applied%2520to%2520ViMs%252C%2520the%2520latest%2520VQ%2520methods%2520suffer%2520from%250Aexcessive%2520memory%2520consumption%252C%2520lengthy%2520calibration%2520procedures%252C%2520and%2520suboptimal%250Aperformance%2520in%2520the%2520search%2520for%2520optimal%2520codewords.%2520In%2520this%2520paper%252C%2520we%2520propose%250AViM-VQ%252C%2520an%2520efficient%2520post-training%2520vector%2520quantization%2520method%2520tailored%2520for%250AViMs.%2520ViM-VQ%2520consists%2520of%2520two%2520innovative%2520components%253A%25201%2529%2520a%2520fast%2520convex%250Acombination%2520optimization%2520algorithm%2520that%2520efficiently%2520updates%2520both%2520the%2520convex%250Acombinations%2520and%2520the%2520convex%2520hulls%2520to%2520search%2520for%2520optimal%2520codewords%252C%2520and%25202%2529%2520an%250Aincremental%2520vector%2520quantization%2520strategy%2520that%2520incrementally%2520confirms%2520optimal%250Acodewords%2520to%2520mitigate%2520truncation%2520errors.%2520Experimental%2520results%2520demonstrate%2520that%250AViM-VQ%2520achieves%2520state-of-the-art%2520performance%2520in%2520low-bit%2520quantization%2520across%250Avarious%2520visual%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViM-VQ%3A%20Efficient%20Post-Training%20Vector%20Quantization%20for%20Visual%20Mamba&entry.906535625=Juncan%20Deng%20and%20Shuaiting%20Li%20and%20Zeyu%20Wang%20and%20Kedong%20Xu%20and%20Hong%20Gu%20and%20Kejie%20Huang&entry.1292438233=%20%20Visual%20Mamba%20networks%20%28ViMs%29%20extend%20the%20selective%20state%20space%20model%20%28Mamba%29%0Ato%20various%20vision%20tasks%20and%20demonstrate%20significant%20potential.%20As%20a%20promising%0Acompression%20technique%2C%20vector%20quantization%20%28VQ%29%20decomposes%20network%20weights%20into%0Acodebooks%20and%20assignments%2C%20significantly%20reducing%20memory%20usage%20and%0Acomputational%20latency%2C%20thereby%20enabling%20the%20deployment%20of%20ViMs%20on%20edge%20devices.%0AAlthough%20existing%20VQ%20methods%20have%20achieved%20extremely%20low-bit%20quantization%0A%28e.g.%2C%203-bit%2C%202-bit%2C%20and%201-bit%29%20in%20convolutional%20neural%20networks%20and%0ATransformer-based%20networks%2C%20directly%20applying%20these%20methods%20to%20ViMs%20results%20in%0Aunsatisfactory%20accuracy.%20We%20identify%20several%20key%20challenges%3A%201%29%20The%20weights%20of%0AMamba-based%20blocks%20in%20ViMs%20contain%20numerous%20outliers%2C%20significantly%20amplifying%0Aquantization%20errors.%202%29%20When%20applied%20to%20ViMs%2C%20the%20latest%20VQ%20methods%20suffer%20from%0Aexcessive%20memory%20consumption%2C%20lengthy%20calibration%20procedures%2C%20and%20suboptimal%0Aperformance%20in%20the%20search%20for%20optimal%20codewords.%20In%20this%20paper%2C%20we%20propose%0AViM-VQ%2C%20an%20efficient%20post-training%20vector%20quantization%20method%20tailored%20for%0AViMs.%20ViM-VQ%20consists%20of%20two%20innovative%20components%3A%201%29%20a%20fast%20convex%0Acombination%20optimization%20algorithm%20that%20efficiently%20updates%20both%20the%20convex%0Acombinations%20and%20the%20convex%20hulls%20to%20search%20for%20optimal%20codewords%2C%20and%202%29%20an%0Aincremental%20vector%20quantization%20strategy%20that%20incrementally%20confirms%20optimal%0Acodewords%20to%20mitigate%20truncation%20errors.%20Experimental%20results%20demonstrate%20that%0AViM-VQ%20achieves%20state-of-the-art%20performance%20in%20low-bit%20quantization%20across%0Avarious%20visual%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09509v2&entry.124074799=Read"},
{"title": "Thermodynamics-Inspired Computing with Oscillatory Neural Networks for\n  Inverse Matrix Computation", "author": "George Tsormpatzoglou and Filip Sabo and Aida Todri-Sanial", "abstract": "  We describe a thermodynamic-inspired computing paradigm based on oscillatory\nneural networks (ONNs). While ONNs have been widely studied as Ising machines\nfor tackling complex combinatorial optimization problems, this work\ninvestigates their feasibility in solving linear algebra problems, specifically\nthe inverse matrix. Grounded in thermodynamic principles, we analytically\ndemonstrate that the linear approximation of the coupled Kuramoto oscillator\nmodel leads to the inverse matrix solution. Numerical simulations validate the\ntheoretical framework, and we examine the parameter regimes that computation\nhas the highest accuracy.\n", "link": "http://arxiv.org/abs/2507.22544v1", "date": "2025-07-30", "relevancy": 1.6703, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4258}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4168}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thermodynamics-Inspired%20Computing%20with%20Oscillatory%20Neural%20Networks%20for%0A%20%20Inverse%20Matrix%20Computation&body=Title%3A%20Thermodynamics-Inspired%20Computing%20with%20Oscillatory%20Neural%20Networks%20for%0A%20%20Inverse%20Matrix%20Computation%0AAuthor%3A%20George%20Tsormpatzoglou%20and%20Filip%20Sabo%20and%20Aida%20Todri-Sanial%0AAbstract%3A%20%20%20We%20describe%20a%20thermodynamic-inspired%20computing%20paradigm%20based%20on%20oscillatory%0Aneural%20networks%20%28ONNs%29.%20While%20ONNs%20have%20been%20widely%20studied%20as%20Ising%20machines%0Afor%20tackling%20complex%20combinatorial%20optimization%20problems%2C%20this%20work%0Ainvestigates%20their%20feasibility%20in%20solving%20linear%20algebra%20problems%2C%20specifically%0Athe%20inverse%20matrix.%20Grounded%20in%20thermodynamic%20principles%2C%20we%20analytically%0Ademonstrate%20that%20the%20linear%20approximation%20of%20the%20coupled%20Kuramoto%20oscillator%0Amodel%20leads%20to%20the%20inverse%20matrix%20solution.%20Numerical%20simulations%20validate%20the%0Atheoretical%20framework%2C%20and%20we%20examine%20the%20parameter%20regimes%20that%20computation%0Ahas%20the%20highest%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermodynamics-Inspired%2520Computing%2520with%2520Oscillatory%2520Neural%2520Networks%2520for%250A%2520%2520Inverse%2520Matrix%2520Computation%26entry.906535625%3DGeorge%2520Tsormpatzoglou%2520and%2520Filip%2520Sabo%2520and%2520Aida%2520Todri-Sanial%26entry.1292438233%3D%2520%2520We%2520describe%2520a%2520thermodynamic-inspired%2520computing%2520paradigm%2520based%2520on%2520oscillatory%250Aneural%2520networks%2520%2528ONNs%2529.%2520While%2520ONNs%2520have%2520been%2520widely%2520studied%2520as%2520Ising%2520machines%250Afor%2520tackling%2520complex%2520combinatorial%2520optimization%2520problems%252C%2520this%2520work%250Ainvestigates%2520their%2520feasibility%2520in%2520solving%2520linear%2520algebra%2520problems%252C%2520specifically%250Athe%2520inverse%2520matrix.%2520Grounded%2520in%2520thermodynamic%2520principles%252C%2520we%2520analytically%250Ademonstrate%2520that%2520the%2520linear%2520approximation%2520of%2520the%2520coupled%2520Kuramoto%2520oscillator%250Amodel%2520leads%2520to%2520the%2520inverse%2520matrix%2520solution.%2520Numerical%2520simulations%2520validate%2520the%250Atheoretical%2520framework%252C%2520and%2520we%2520examine%2520the%2520parameter%2520regimes%2520that%2520computation%250Ahas%2520the%2520highest%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thermodynamics-Inspired%20Computing%20with%20Oscillatory%20Neural%20Networks%20for%0A%20%20Inverse%20Matrix%20Computation&entry.906535625=George%20Tsormpatzoglou%20and%20Filip%20Sabo%20and%20Aida%20Todri-Sanial&entry.1292438233=%20%20We%20describe%20a%20thermodynamic-inspired%20computing%20paradigm%20based%20on%20oscillatory%0Aneural%20networks%20%28ONNs%29.%20While%20ONNs%20have%20been%20widely%20studied%20as%20Ising%20machines%0Afor%20tackling%20complex%20combinatorial%20optimization%20problems%2C%20this%20work%0Ainvestigates%20their%20feasibility%20in%20solving%20linear%20algebra%20problems%2C%20specifically%0Athe%20inverse%20matrix.%20Grounded%20in%20thermodynamic%20principles%2C%20we%20analytically%0Ademonstrate%20that%20the%20linear%20approximation%20of%20the%20coupled%20Kuramoto%20oscillator%0Amodel%20leads%20to%20the%20inverse%20matrix%20solution.%20Numerical%20simulations%20validate%20the%0Atheoretical%20framework%2C%20and%20we%20examine%20the%20parameter%20regimes%20that%20computation%0Ahas%20the%20highest%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22544v1&entry.124074799=Read"},
{"title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding,\n  and Analysis", "author": "Rafi Al Attrach and Pedro Moreira and Rajna Fani and Renato Umeton and Leo Anthony Celi", "abstract": "  As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.\n", "link": "http://arxiv.org/abs/2507.01053v2", "date": "2025-07-30", "relevancy": 1.8418, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4638}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4638}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conversational%20LLMs%20Simplify%20Secure%20Clinical%20Data%20Access%2C%20Understanding%2C%0A%20%20and%20Analysis&body=Title%3A%20Conversational%20LLMs%20Simplify%20Secure%20Clinical%20Data%20Access%2C%20Understanding%2C%0A%20%20and%20Analysis%0AAuthor%3A%20Rafi%20Al%20Attrach%20and%20Pedro%20Moreira%20and%20Rajna%20Fani%20and%20Renato%20Umeton%20and%20Leo%20Anthony%20Celi%0AAbstract%3A%20%20%20As%20ever-larger%20clinical%20datasets%20become%20available%2C%20they%20have%20the%20potential%20to%0Aunlock%20unprecedented%20opportunities%20for%20medical%20research.%20Foremost%20among%20them%20is%0AMedical%20Information%20Mart%20for%20Intensive%20Care%20%28MIMIC-IV%29%2C%20the%20world%27s%20largest%0Aopen-source%20EHR%20database.%20However%2C%20the%20inherent%20complexity%20of%20these%20datasets%2C%0Aparticularly%20the%20need%20for%20sophisticated%20querying%20skills%20and%20the%20need%20to%0Aunderstand%20the%20underlying%20clinical%20settings%2C%20often%20presents%20a%20significant%0Abarrier%20to%20their%20effective%20use.%20M3%20lowers%20the%20technical%20barrier%20to%0Aunderstanding%20and%20querying%20MIMIC-IV%20data.%20With%20a%20single%20command%20it%20retrieves%0AMIMIC-IV%20from%20PhysioNet%2C%20launches%20a%20local%20SQLite%20instance%20%28or%20hooks%20into%20the%0Ahosted%20BigQuery%29%2C%20and-via%20the%20Model%20Context%20Protocol%20%28MCP%29-lets%20researchers%0Aconverse%20with%20the%20database%20in%20plain%20English.%20Ask%20a%20clinical%20question%20in%20natural%0Alanguage%3B%20M3%20uses%20a%20language%20model%20to%20translate%20it%20into%20SQL%2C%20executes%20the%20query%0Aagainst%20the%20MIMIC-IV%20dataset%2C%20and%20returns%20structured%20results%20alongside%20the%0Aunderlying%20query%20for%20verifiability%20and%20reproducibility.%20Demonstrations%20show%0Athat%20minutes%20of%20dialogue%20with%20M3%20yield%20the%20kind%20of%20nuanced%20cohort%20analyses%20that%0Aonce%20demanded%20hours%20of%20handcrafted%20SQL%20and%20relied%20on%20understanding%20the%0Acomplexities%20of%20clinical%20workflows.%20By%20simplifying%20access%2C%20M3%20invites%20the%0Abroader%20research%20community%20to%20mine%20clinical%20critical-care%20data%20and%20accelerates%0Athe%20translation%20of%20raw%20records%20into%20actionable%20insight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConversational%2520LLMs%2520Simplify%2520Secure%2520Clinical%2520Data%2520Access%252C%2520Understanding%252C%250A%2520%2520and%2520Analysis%26entry.906535625%3DRafi%2520Al%2520Attrach%2520and%2520Pedro%2520Moreira%2520and%2520Rajna%2520Fani%2520and%2520Renato%2520Umeton%2520and%2520Leo%2520Anthony%2520Celi%26entry.1292438233%3D%2520%2520As%2520ever-larger%2520clinical%2520datasets%2520become%2520available%252C%2520they%2520have%2520the%2520potential%2520to%250Aunlock%2520unprecedented%2520opportunities%2520for%2520medical%2520research.%2520Foremost%2520among%2520them%2520is%250AMedical%2520Information%2520Mart%2520for%2520Intensive%2520Care%2520%2528MIMIC-IV%2529%252C%2520the%2520world%2527s%2520largest%250Aopen-source%2520EHR%2520database.%2520However%252C%2520the%2520inherent%2520complexity%2520of%2520these%2520datasets%252C%250Aparticularly%2520the%2520need%2520for%2520sophisticated%2520querying%2520skills%2520and%2520the%2520need%2520to%250Aunderstand%2520the%2520underlying%2520clinical%2520settings%252C%2520often%2520presents%2520a%2520significant%250Abarrier%2520to%2520their%2520effective%2520use.%2520M3%2520lowers%2520the%2520technical%2520barrier%2520to%250Aunderstanding%2520and%2520querying%2520MIMIC-IV%2520data.%2520With%2520a%2520single%2520command%2520it%2520retrieves%250AMIMIC-IV%2520from%2520PhysioNet%252C%2520launches%2520a%2520local%2520SQLite%2520instance%2520%2528or%2520hooks%2520into%2520the%250Ahosted%2520BigQuery%2529%252C%2520and-via%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529-lets%2520researchers%250Aconverse%2520with%2520the%2520database%2520in%2520plain%2520English.%2520Ask%2520a%2520clinical%2520question%2520in%2520natural%250Alanguage%253B%2520M3%2520uses%2520a%2520language%2520model%2520to%2520translate%2520it%2520into%2520SQL%252C%2520executes%2520the%2520query%250Aagainst%2520the%2520MIMIC-IV%2520dataset%252C%2520and%2520returns%2520structured%2520results%2520alongside%2520the%250Aunderlying%2520query%2520for%2520verifiability%2520and%2520reproducibility.%2520Demonstrations%2520show%250Athat%2520minutes%2520of%2520dialogue%2520with%2520M3%2520yield%2520the%2520kind%2520of%2520nuanced%2520cohort%2520analyses%2520that%250Aonce%2520demanded%2520hours%2520of%2520handcrafted%2520SQL%2520and%2520relied%2520on%2520understanding%2520the%250Acomplexities%2520of%2520clinical%2520workflows.%2520By%2520simplifying%2520access%252C%2520M3%2520invites%2520the%250Abroader%2520research%2520community%2520to%2520mine%2520clinical%2520critical-care%2520data%2520and%2520accelerates%250Athe%2520translation%2520of%2520raw%2520records%2520into%2520actionable%2520insight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conversational%20LLMs%20Simplify%20Secure%20Clinical%20Data%20Access%2C%20Understanding%2C%0A%20%20and%20Analysis&entry.906535625=Rafi%20Al%20Attrach%20and%20Pedro%20Moreira%20and%20Rajna%20Fani%20and%20Renato%20Umeton%20and%20Leo%20Anthony%20Celi&entry.1292438233=%20%20As%20ever-larger%20clinical%20datasets%20become%20available%2C%20they%20have%20the%20potential%20to%0Aunlock%20unprecedented%20opportunities%20for%20medical%20research.%20Foremost%20among%20them%20is%0AMedical%20Information%20Mart%20for%20Intensive%20Care%20%28MIMIC-IV%29%2C%20the%20world%27s%20largest%0Aopen-source%20EHR%20database.%20However%2C%20the%20inherent%20complexity%20of%20these%20datasets%2C%0Aparticularly%20the%20need%20for%20sophisticated%20querying%20skills%20and%20the%20need%20to%0Aunderstand%20the%20underlying%20clinical%20settings%2C%20often%20presents%20a%20significant%0Abarrier%20to%20their%20effective%20use.%20M3%20lowers%20the%20technical%20barrier%20to%0Aunderstanding%20and%20querying%20MIMIC-IV%20data.%20With%20a%20single%20command%20it%20retrieves%0AMIMIC-IV%20from%20PhysioNet%2C%20launches%20a%20local%20SQLite%20instance%20%28or%20hooks%20into%20the%0Ahosted%20BigQuery%29%2C%20and-via%20the%20Model%20Context%20Protocol%20%28MCP%29-lets%20researchers%0Aconverse%20with%20the%20database%20in%20plain%20English.%20Ask%20a%20clinical%20question%20in%20natural%0Alanguage%3B%20M3%20uses%20a%20language%20model%20to%20translate%20it%20into%20SQL%2C%20executes%20the%20query%0Aagainst%20the%20MIMIC-IV%20dataset%2C%20and%20returns%20structured%20results%20alongside%20the%0Aunderlying%20query%20for%20verifiability%20and%20reproducibility.%20Demonstrations%20show%0Athat%20minutes%20of%20dialogue%20with%20M3%20yield%20the%20kind%20of%20nuanced%20cohort%20analyses%20that%0Aonce%20demanded%20hours%20of%20handcrafted%20SQL%20and%20relied%20on%20understanding%20the%0Acomplexities%20of%20clinical%20workflows.%20By%20simplifying%20access%2C%20M3%20invites%20the%0Abroader%20research%20community%20to%20mine%20clinical%20critical-care%20data%20and%20accelerates%0Athe%20translation%20of%20raw%20records%20into%20actionable%20insight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01053v2&entry.124074799=Read"},
{"title": "Graph Collaborative Attention Network for Link Prediction in Knowledge\n  Graphs", "author": "Thanh Hoang-Minh", "abstract": "  Knowledge graphs offer a structured representation of real-world entities and\ntheir relationships, enabling a wide range of applications from information\nretrieval to automated reasoning. In this paper, we conduct a systematic\ncomparison between traditional rule-based approaches and modern deep learning\nmethods for link prediction. We focus on KBGAT, a graph neural network model\nthat leverages multi-head attention to jointly encode both entity and relation\nfeatures within local neighborhood structures. To advance this line of\nresearch, we introduce \\textbf{GCAT} (Graph Collaborative Attention Network), a\nrefined model that enhances context aggregation and interaction between\nheterogeneous nodes. Experimental results on four widely-used benchmark\ndatasets demonstrate that GCAT not only consistently outperforms rule-based\nmethods but also achieves competitive or superior performance compared to\nexisting neural embedding models. Our findings highlight the advantages of\nattention-based architectures in capturing complex relational patterns for\nknowledge graph completion tasks.\n", "link": "http://arxiv.org/abs/2507.03947v2", "date": "2025-07-30", "relevancy": 1.8432, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4702}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4558}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Collaborative%20Attention%20Network%20for%20Link%20Prediction%20in%20Knowledge%0A%20%20Graphs&body=Title%3A%20Graph%20Collaborative%20Attention%20Network%20for%20Link%20Prediction%20in%20Knowledge%0A%20%20Graphs%0AAuthor%3A%20Thanh%20Hoang-Minh%0AAbstract%3A%20%20%20Knowledge%20graphs%20offer%20a%20structured%20representation%20of%20real-world%20entities%20and%0Atheir%20relationships%2C%20enabling%20a%20wide%20range%20of%20applications%20from%20information%0Aretrieval%20to%20automated%20reasoning.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%0Acomparison%20between%20traditional%20rule-based%20approaches%20and%20modern%20deep%20learning%0Amethods%20for%20link%20prediction.%20We%20focus%20on%20KBGAT%2C%20a%20graph%20neural%20network%20model%0Athat%20leverages%20multi-head%20attention%20to%20jointly%20encode%20both%20entity%20and%20relation%0Afeatures%20within%20local%20neighborhood%20structures.%20To%20advance%20this%20line%20of%0Aresearch%2C%20we%20introduce%20%5Ctextbf%7BGCAT%7D%20%28Graph%20Collaborative%20Attention%20Network%29%2C%20a%0Arefined%20model%20that%20enhances%20context%20aggregation%20and%20interaction%20between%0Aheterogeneous%20nodes.%20Experimental%20results%20on%20four%20widely-used%20benchmark%0Adatasets%20demonstrate%20that%20GCAT%20not%20only%20consistently%20outperforms%20rule-based%0Amethods%20but%20also%20achieves%20competitive%20or%20superior%20performance%20compared%20to%0Aexisting%20neural%20embedding%20models.%20Our%20findings%20highlight%20the%20advantages%20of%0Aattention-based%20architectures%20in%20capturing%20complex%20relational%20patterns%20for%0Aknowledge%20graph%20completion%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Collaborative%2520Attention%2520Network%2520for%2520Link%2520Prediction%2520in%2520Knowledge%250A%2520%2520Graphs%26entry.906535625%3DThanh%2520Hoang-Minh%26entry.1292438233%3D%2520%2520Knowledge%2520graphs%2520offer%2520a%2520structured%2520representation%2520of%2520real-world%2520entities%2520and%250Atheir%2520relationships%252C%2520enabling%2520a%2520wide%2520range%2520of%2520applications%2520from%2520information%250Aretrieval%2520to%2520automated%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520systematic%250Acomparison%2520between%2520traditional%2520rule-based%2520approaches%2520and%2520modern%2520deep%2520learning%250Amethods%2520for%2520link%2520prediction.%2520We%2520focus%2520on%2520KBGAT%252C%2520a%2520graph%2520neural%2520network%2520model%250Athat%2520leverages%2520multi-head%2520attention%2520to%2520jointly%2520encode%2520both%2520entity%2520and%2520relation%250Afeatures%2520within%2520local%2520neighborhood%2520structures.%2520To%2520advance%2520this%2520line%2520of%250Aresearch%252C%2520we%2520introduce%2520%255Ctextbf%257BGCAT%257D%2520%2528Graph%2520Collaborative%2520Attention%2520Network%2529%252C%2520a%250Arefined%2520model%2520that%2520enhances%2520context%2520aggregation%2520and%2520interaction%2520between%250Aheterogeneous%2520nodes.%2520Experimental%2520results%2520on%2520four%2520widely-used%2520benchmark%250Adatasets%2520demonstrate%2520that%2520GCAT%2520not%2520only%2520consistently%2520outperforms%2520rule-based%250Amethods%2520but%2520also%2520achieves%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%250Aexisting%2520neural%2520embedding%2520models.%2520Our%2520findings%2520highlight%2520the%2520advantages%2520of%250Aattention-based%2520architectures%2520in%2520capturing%2520complex%2520relational%2520patterns%2520for%250Aknowledge%2520graph%2520completion%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Collaborative%20Attention%20Network%20for%20Link%20Prediction%20in%20Knowledge%0A%20%20Graphs&entry.906535625=Thanh%20Hoang-Minh&entry.1292438233=%20%20Knowledge%20graphs%20offer%20a%20structured%20representation%20of%20real-world%20entities%20and%0Atheir%20relationships%2C%20enabling%20a%20wide%20range%20of%20applications%20from%20information%0Aretrieval%20to%20automated%20reasoning.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%0Acomparison%20between%20traditional%20rule-based%20approaches%20and%20modern%20deep%20learning%0Amethods%20for%20link%20prediction.%20We%20focus%20on%20KBGAT%2C%20a%20graph%20neural%20network%20model%0Athat%20leverages%20multi-head%20attention%20to%20jointly%20encode%20both%20entity%20and%20relation%0Afeatures%20within%20local%20neighborhood%20structures.%20To%20advance%20this%20line%20of%0Aresearch%2C%20we%20introduce%20%5Ctextbf%7BGCAT%7D%20%28Graph%20Collaborative%20Attention%20Network%29%2C%20a%0Arefined%20model%20that%20enhances%20context%20aggregation%20and%20interaction%20between%0Aheterogeneous%20nodes.%20Experimental%20results%20on%20four%20widely-used%20benchmark%0Adatasets%20demonstrate%20that%20GCAT%20not%20only%20consistently%20outperforms%20rule-based%0Amethods%20but%20also%20achieves%20competitive%20or%20superior%20performance%20compared%20to%0Aexisting%20neural%20embedding%20models.%20Our%20findings%20highlight%20the%20advantages%20of%0Aattention-based%20architectures%20in%20capturing%20complex%20relational%20patterns%20for%0Aknowledge%20graph%20completion%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03947v2&entry.124074799=Read"},
{"title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated\n  Patch Correctness Assessment", "author": "Marcos Fuster-Pena and David de-Fitero-Dominguez and Antonio Garcia-Cabot and Eva Garcia-Lopez", "abstract": "  Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability.\n", "link": "http://arxiv.org/abs/2507.22580v1", "date": "2025-07-30", "relevancy": 1.3182, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4405}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RePaCA%3A%20Leveraging%20Reasoning%20Large%20Language%20Models%20for%20Static%20Automated%0A%20%20Patch%20Correctness%20Assessment&body=Title%3A%20RePaCA%3A%20Leveraging%20Reasoning%20Large%20Language%20Models%20for%20Static%20Automated%0A%20%20Patch%20Correctness%20Assessment%0AAuthor%3A%20Marcos%20Fuster-Pena%20and%20David%20de-Fitero-Dominguez%20and%20Antonio%20Garcia-Cabot%20and%20Eva%20Garcia-Lopez%0AAbstract%3A%20%20%20Automated%20Program%20Repair%20%28APR%29%20seeks%20to%20automatically%20correct%20software%20bugs%0Awithout%20requiring%20human%20intervention.%20However%2C%20existing%20tools%20tend%20to%20generate%0Apatches%20that%20satisfy%20test%20cases%20without%20fixing%20the%20underlying%20bug%2C%20those%20are%0Aknown%20as%20overfitting%20patches.%20To%20address%20this%20issue%2C%20Automated%20Patch%0ACorrectness%20Assessment%20%28APCA%29%20attempts%20to%20identify%20overfitting%20patches%0Agenerated%20by%20APR%20tools.%20It%20can%20be%20solved%20as%20a%20static%20approach%2C%20meaning%20that%20no%0Aadditional%20information%20is%20needed%20beyond%20the%20original%20and%20fixed%20code%20snippets.%0ACurrent%20static%20techniques%20often%20struggle%20with%20reliability%2C%20flexibility%20and%0Atransparency.%20To%20address%20these%20issues%2C%20we%20introduce%20RePaCA%2C%20a%20novel%20static%20APCA%0Atechnique%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20specialized%20in%20thinking%0Atasks.%20Our%20model%20is%20prompted%20with%20both%20buggy%20and%20fixed%20code%20snippets%20and%20guided%0Ato%20generate%20a%20Chain%20of%20Thought%20that%20analyses%20code%20differences%2C%20reasons%20about%0Ahow%20the%20patch%20addresses%20the%20root%20cause%2C%20and%20ultimately%20provides%20a%20binary%0Aclassification%3A%20correct%20or%20overfitting.%20To%20enhance%20these%20reasoning%20capabilities%0Afor%20the%20APCA%20task%20specifically%2C%20the%20LLM%20is%20finetuned%20using%20Reinforcement%0ALearning%20with%20the%20Group%20Relative%20Policy%20Optimization%20algorithm.%20When%20evaluated%0Aon%20a%20standard%20Defects4J-derived%20test%2C%20our%20approach%20achieves%20state-of-the-art%0Aperformance%2C%20with%2083.1%25%20accuracy%20and%20an%2084.8%25%20F1-score.%20Furthermore%2C%20our%20model%0Ademonstrates%20superior%20generalization%20capabilities%20when%20trained%20on%20different%0Adatasets%2C%20outperforming%20the%20leading%20technique.%20This%20reasoning%20capability%20also%0Aprovides%20enhanced%20explainability%20for%20the%20patch%20assessment.%20These%20findings%0Aunderscore%20the%20considerable%20promise%20of%20finetuned%2C%20reasoning%20LLMs%20to%20advance%0Astatic%20APCA%20by%20enhancing%20accuracy%2C%20generalization%2C%20and%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRePaCA%253A%2520Leveraging%2520Reasoning%2520Large%2520Language%2520Models%2520for%2520Static%2520Automated%250A%2520%2520Patch%2520Correctness%2520Assessment%26entry.906535625%3DMarcos%2520Fuster-Pena%2520and%2520David%2520de-Fitero-Dominguez%2520and%2520Antonio%2520Garcia-Cabot%2520and%2520Eva%2520Garcia-Lopez%26entry.1292438233%3D%2520%2520Automated%2520Program%2520Repair%2520%2528APR%2529%2520seeks%2520to%2520automatically%2520correct%2520software%2520bugs%250Awithout%2520requiring%2520human%2520intervention.%2520However%252C%2520existing%2520tools%2520tend%2520to%2520generate%250Apatches%2520that%2520satisfy%2520test%2520cases%2520without%2520fixing%2520the%2520underlying%2520bug%252C%2520those%2520are%250Aknown%2520as%2520overfitting%2520patches.%2520To%2520address%2520this%2520issue%252C%2520Automated%2520Patch%250ACorrectness%2520Assessment%2520%2528APCA%2529%2520attempts%2520to%2520identify%2520overfitting%2520patches%250Agenerated%2520by%2520APR%2520tools.%2520It%2520can%2520be%2520solved%2520as%2520a%2520static%2520approach%252C%2520meaning%2520that%2520no%250Aadditional%2520information%2520is%2520needed%2520beyond%2520the%2520original%2520and%2520fixed%2520code%2520snippets.%250ACurrent%2520static%2520techniques%2520often%2520struggle%2520with%2520reliability%252C%2520flexibility%2520and%250Atransparency.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520RePaCA%252C%2520a%2520novel%2520static%2520APCA%250Atechnique%2520that%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520specialized%2520in%2520thinking%250Atasks.%2520Our%2520model%2520is%2520prompted%2520with%2520both%2520buggy%2520and%2520fixed%2520code%2520snippets%2520and%2520guided%250Ato%2520generate%2520a%2520Chain%2520of%2520Thought%2520that%2520analyses%2520code%2520differences%252C%2520reasons%2520about%250Ahow%2520the%2520patch%2520addresses%2520the%2520root%2520cause%252C%2520and%2520ultimately%2520provides%2520a%2520binary%250Aclassification%253A%2520correct%2520or%2520overfitting.%2520To%2520enhance%2520these%2520reasoning%2520capabilities%250Afor%2520the%2520APCA%2520task%2520specifically%252C%2520the%2520LLM%2520is%2520finetuned%2520using%2520Reinforcement%250ALearning%2520with%2520the%2520Group%2520Relative%2520Policy%2520Optimization%2520algorithm.%2520When%2520evaluated%250Aon%2520a%2520standard%2520Defects4J-derived%2520test%252C%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%252C%2520with%252083.1%2525%2520accuracy%2520and%2520an%252084.8%2525%2520F1-score.%2520Furthermore%252C%2520our%2520model%250Ademonstrates%2520superior%2520generalization%2520capabilities%2520when%2520trained%2520on%2520different%250Adatasets%252C%2520outperforming%2520the%2520leading%2520technique.%2520This%2520reasoning%2520capability%2520also%250Aprovides%2520enhanced%2520explainability%2520for%2520the%2520patch%2520assessment.%2520These%2520findings%250Aunderscore%2520the%2520considerable%2520promise%2520of%2520finetuned%252C%2520reasoning%2520LLMs%2520to%2520advance%250Astatic%2520APCA%2520by%2520enhancing%2520accuracy%252C%2520generalization%252C%2520and%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RePaCA%3A%20Leveraging%20Reasoning%20Large%20Language%20Models%20for%20Static%20Automated%0A%20%20Patch%20Correctness%20Assessment&entry.906535625=Marcos%20Fuster-Pena%20and%20David%20de-Fitero-Dominguez%20and%20Antonio%20Garcia-Cabot%20and%20Eva%20Garcia-Lopez&entry.1292438233=%20%20Automated%20Program%20Repair%20%28APR%29%20seeks%20to%20automatically%20correct%20software%20bugs%0Awithout%20requiring%20human%20intervention.%20However%2C%20existing%20tools%20tend%20to%20generate%0Apatches%20that%20satisfy%20test%20cases%20without%20fixing%20the%20underlying%20bug%2C%20those%20are%0Aknown%20as%20overfitting%20patches.%20To%20address%20this%20issue%2C%20Automated%20Patch%0ACorrectness%20Assessment%20%28APCA%29%20attempts%20to%20identify%20overfitting%20patches%0Agenerated%20by%20APR%20tools.%20It%20can%20be%20solved%20as%20a%20static%20approach%2C%20meaning%20that%20no%0Aadditional%20information%20is%20needed%20beyond%20the%20original%20and%20fixed%20code%20snippets.%0ACurrent%20static%20techniques%20often%20struggle%20with%20reliability%2C%20flexibility%20and%0Atransparency.%20To%20address%20these%20issues%2C%20we%20introduce%20RePaCA%2C%20a%20novel%20static%20APCA%0Atechnique%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20specialized%20in%20thinking%0Atasks.%20Our%20model%20is%20prompted%20with%20both%20buggy%20and%20fixed%20code%20snippets%20and%20guided%0Ato%20generate%20a%20Chain%20of%20Thought%20that%20analyses%20code%20differences%2C%20reasons%20about%0Ahow%20the%20patch%20addresses%20the%20root%20cause%2C%20and%20ultimately%20provides%20a%20binary%0Aclassification%3A%20correct%20or%20overfitting.%20To%20enhance%20these%20reasoning%20capabilities%0Afor%20the%20APCA%20task%20specifically%2C%20the%20LLM%20is%20finetuned%20using%20Reinforcement%0ALearning%20with%20the%20Group%20Relative%20Policy%20Optimization%20algorithm.%20When%20evaluated%0Aon%20a%20standard%20Defects4J-derived%20test%2C%20our%20approach%20achieves%20state-of-the-art%0Aperformance%2C%20with%2083.1%25%20accuracy%20and%20an%2084.8%25%20F1-score.%20Furthermore%2C%20our%20model%0Ademonstrates%20superior%20generalization%20capabilities%20when%20trained%20on%20different%0Adatasets%2C%20outperforming%20the%20leading%20technique.%20This%20reasoning%20capability%20also%0Aprovides%20enhanced%20explainability%20for%20the%20patch%20assessment.%20These%20findings%0Aunderscore%20the%20considerable%20promise%20of%20finetuned%2C%20reasoning%20LLMs%20to%20advance%0Astatic%20APCA%20by%20enhancing%20accuracy%2C%20generalization%2C%20and%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22580v1&entry.124074799=Read"},
{"title": "Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite\n  Precision", "author": "Samuel Teuber and Debasmita Lohar and Bernhard Beckert", "abstract": "  As neural networks (NNs) become increasingly prevalent in safety-critical\nneural network-controlled cyber-physical systems (NNCSs), formally guaranteeing\ntheir safety becomes crucial. For these systems, safety must be ensured\nthroughout their entire operation, necessitating infinite-time horizon\nverification. To verify the infinite-time horizon safety of NNCSs, recent\napproaches leverage Differential Dynamic Logic (dL). However, these dL-based\nguarantees rely on idealized, real-valued NN semantics and fail to account for\nroundoff errors introduced by finite-precision implementations. This paper\nbridges the gap between theoretical guarantees and real-world implementations\nby incorporating robustness under finite-precision perturbations -- in sensing,\nactuation, and computation -- into the safety verification. We model the\nproblem as a hybrid game between a good Demon, responsible for control actions,\nand a bad Angel, introducing perturbations. This formulation enables formal\nproofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this\nbound, we employ state-of-the-art mixed-precision fixed-point tuners to\nsynthesize sound and efficient implementations, thus providing a complete\nend-to-end solution. We evaluate our approach on case studies from the\nautomotive and aeronautics domains, producing efficient NN implementations with\nrigorous infinite-time horizon safety guarantees.\n", "link": "http://arxiv.org/abs/2507.22760v1", "date": "2025-07-30", "relevancy": 1.9074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4921}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4765}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Of%20Good%20Demons%20and%20Bad%20Angels%3A%20Guaranteeing%20Safe%20Control%20under%20Finite%0A%20%20Precision&body=Title%3A%20Of%20Good%20Demons%20and%20Bad%20Angels%3A%20Guaranteeing%20Safe%20Control%20under%20Finite%0A%20%20Precision%0AAuthor%3A%20Samuel%20Teuber%20and%20Debasmita%20Lohar%20and%20Bernhard%20Beckert%0AAbstract%3A%20%20%20As%20neural%20networks%20%28NNs%29%20become%20increasingly%20prevalent%20in%20safety-critical%0Aneural%20network-controlled%20cyber-physical%20systems%20%28NNCSs%29%2C%20formally%20guaranteeing%0Atheir%20safety%20becomes%20crucial.%20For%20these%20systems%2C%20safety%20must%20be%20ensured%0Athroughout%20their%20entire%20operation%2C%20necessitating%20infinite-time%20horizon%0Averification.%20To%20verify%20the%20infinite-time%20horizon%20safety%20of%20NNCSs%2C%20recent%0Aapproaches%20leverage%20Differential%20Dynamic%20Logic%20%28dL%29.%20However%2C%20these%20dL-based%0Aguarantees%20rely%20on%20idealized%2C%20real-valued%20NN%20semantics%20and%20fail%20to%20account%20for%0Aroundoff%20errors%20introduced%20by%20finite-precision%20implementations.%20This%20paper%0Abridges%20the%20gap%20between%20theoretical%20guarantees%20and%20real-world%20implementations%0Aby%20incorporating%20robustness%20under%20finite-precision%20perturbations%20--%20in%20sensing%2C%0Aactuation%2C%20and%20computation%20--%20into%20the%20safety%20verification.%20We%20model%20the%0Aproblem%20as%20a%20hybrid%20game%20between%20a%20good%20Demon%2C%20responsible%20for%20control%20actions%2C%0Aand%20a%20bad%20Angel%2C%20introducing%20perturbations.%20This%20formulation%20enables%20formal%0Aproofs%20of%20robustness%20w.r.t.%20a%20given%20%28bounded%29%20perturbation.%20Leveraging%20this%0Abound%2C%20we%20employ%20state-of-the-art%20mixed-precision%20fixed-point%20tuners%20to%0Asynthesize%20sound%20and%20efficient%20implementations%2C%20thus%20providing%20a%20complete%0Aend-to-end%20solution.%20We%20evaluate%20our%20approach%20on%20case%20studies%20from%20the%0Aautomotive%20and%20aeronautics%20domains%2C%20producing%20efficient%20NN%20implementations%20with%0Arigorous%20infinite-time%20horizon%20safety%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOf%2520Good%2520Demons%2520and%2520Bad%2520Angels%253A%2520Guaranteeing%2520Safe%2520Control%2520under%2520Finite%250A%2520%2520Precision%26entry.906535625%3DSamuel%2520Teuber%2520and%2520Debasmita%2520Lohar%2520and%2520Bernhard%2520Beckert%26entry.1292438233%3D%2520%2520As%2520neural%2520networks%2520%2528NNs%2529%2520become%2520increasingly%2520prevalent%2520in%2520safety-critical%250Aneural%2520network-controlled%2520cyber-physical%2520systems%2520%2528NNCSs%2529%252C%2520formally%2520guaranteeing%250Atheir%2520safety%2520becomes%2520crucial.%2520For%2520these%2520systems%252C%2520safety%2520must%2520be%2520ensured%250Athroughout%2520their%2520entire%2520operation%252C%2520necessitating%2520infinite-time%2520horizon%250Averification.%2520To%2520verify%2520the%2520infinite-time%2520horizon%2520safety%2520of%2520NNCSs%252C%2520recent%250Aapproaches%2520leverage%2520Differential%2520Dynamic%2520Logic%2520%2528dL%2529.%2520However%252C%2520these%2520dL-based%250Aguarantees%2520rely%2520on%2520idealized%252C%2520real-valued%2520NN%2520semantics%2520and%2520fail%2520to%2520account%2520for%250Aroundoff%2520errors%2520introduced%2520by%2520finite-precision%2520implementations.%2520This%2520paper%250Abridges%2520the%2520gap%2520between%2520theoretical%2520guarantees%2520and%2520real-world%2520implementations%250Aby%2520incorporating%2520robustness%2520under%2520finite-precision%2520perturbations%2520--%2520in%2520sensing%252C%250Aactuation%252C%2520and%2520computation%2520--%2520into%2520the%2520safety%2520verification.%2520We%2520model%2520the%250Aproblem%2520as%2520a%2520hybrid%2520game%2520between%2520a%2520good%2520Demon%252C%2520responsible%2520for%2520control%2520actions%252C%250Aand%2520a%2520bad%2520Angel%252C%2520introducing%2520perturbations.%2520This%2520formulation%2520enables%2520formal%250Aproofs%2520of%2520robustness%2520w.r.t.%2520a%2520given%2520%2528bounded%2529%2520perturbation.%2520Leveraging%2520this%250Abound%252C%2520we%2520employ%2520state-of-the-art%2520mixed-precision%2520fixed-point%2520tuners%2520to%250Asynthesize%2520sound%2520and%2520efficient%2520implementations%252C%2520thus%2520providing%2520a%2520complete%250Aend-to-end%2520solution.%2520We%2520evaluate%2520our%2520approach%2520on%2520case%2520studies%2520from%2520the%250Aautomotive%2520and%2520aeronautics%2520domains%252C%2520producing%2520efficient%2520NN%2520implementations%2520with%250Arigorous%2520infinite-time%2520horizon%2520safety%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Of%20Good%20Demons%20and%20Bad%20Angels%3A%20Guaranteeing%20Safe%20Control%20under%20Finite%0A%20%20Precision&entry.906535625=Samuel%20Teuber%20and%20Debasmita%20Lohar%20and%20Bernhard%20Beckert&entry.1292438233=%20%20As%20neural%20networks%20%28NNs%29%20become%20increasingly%20prevalent%20in%20safety-critical%0Aneural%20network-controlled%20cyber-physical%20systems%20%28NNCSs%29%2C%20formally%20guaranteeing%0Atheir%20safety%20becomes%20crucial.%20For%20these%20systems%2C%20safety%20must%20be%20ensured%0Athroughout%20their%20entire%20operation%2C%20necessitating%20infinite-time%20horizon%0Averification.%20To%20verify%20the%20infinite-time%20horizon%20safety%20of%20NNCSs%2C%20recent%0Aapproaches%20leverage%20Differential%20Dynamic%20Logic%20%28dL%29.%20However%2C%20these%20dL-based%0Aguarantees%20rely%20on%20idealized%2C%20real-valued%20NN%20semantics%20and%20fail%20to%20account%20for%0Aroundoff%20errors%20introduced%20by%20finite-precision%20implementations.%20This%20paper%0Abridges%20the%20gap%20between%20theoretical%20guarantees%20and%20real-world%20implementations%0Aby%20incorporating%20robustness%20under%20finite-precision%20perturbations%20--%20in%20sensing%2C%0Aactuation%2C%20and%20computation%20--%20into%20the%20safety%20verification.%20We%20model%20the%0Aproblem%20as%20a%20hybrid%20game%20between%20a%20good%20Demon%2C%20responsible%20for%20control%20actions%2C%0Aand%20a%20bad%20Angel%2C%20introducing%20perturbations.%20This%20formulation%20enables%20formal%0Aproofs%20of%20robustness%20w.r.t.%20a%20given%20%28bounded%29%20perturbation.%20Leveraging%20this%0Abound%2C%20we%20employ%20state-of-the-art%20mixed-precision%20fixed-point%20tuners%20to%0Asynthesize%20sound%20and%20efficient%20implementations%2C%20thus%20providing%20a%20complete%0Aend-to-end%20solution.%20We%20evaluate%20our%20approach%20on%20case%20studies%20from%20the%0Aautomotive%20and%20aeronautics%20domains%2C%20producing%20efficient%20NN%20implementations%20with%0Arigorous%20infinite-time%20horizon%20safety%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22760v1&entry.124074799=Read"},
{"title": "Transductive Model Selection under Prior Probability Shift", "author": "Lorenzo Volpi and Alejandro Moreo and Fabrizio Sebastiani", "abstract": "  Transductive learning is a supervised machine learning task in which, unlike\nin traditional inductive learning, the unlabelled data that require labelling\nare a finite set and are available at training time. Similarly to inductive\nlearning contexts, transductive learning contexts may be affected by dataset\nshift, i.e., may be such that the IID assumption does not hold. We here propose\na method, tailored to transductive classification contexts, for performing\nmodel selection (i.e., hyperparameter optimisation) when the data exhibit prior\nprobability shift, an important type of dataset shift typical of anti-causal\nlearning problems. In our proposed method the hyperparameters can be optimised\ndirectly on the unlabelled data to which the trained classifier must be\napplied; this is unlike traditional model selection methods, that are based on\nperforming cross-validation on the labelled training data. We provide\nexperimental results that show the benefits brought about by our method.\n", "link": "http://arxiv.org/abs/2507.22647v1", "date": "2025-07-30", "relevancy": 1.3805, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transductive%20Model%20Selection%20under%20Prior%20Probability%20Shift&body=Title%3A%20Transductive%20Model%20Selection%20under%20Prior%20Probability%20Shift%0AAuthor%3A%20Lorenzo%20Volpi%20and%20Alejandro%20Moreo%20and%20Fabrizio%20Sebastiani%0AAbstract%3A%20%20%20Transductive%20learning%20is%20a%20supervised%20machine%20learning%20task%20in%20which%2C%20unlike%0Ain%20traditional%20inductive%20learning%2C%20the%20unlabelled%20data%20that%20require%20labelling%0Aare%20a%20finite%20set%20and%20are%20available%20at%20training%20time.%20Similarly%20to%20inductive%0Alearning%20contexts%2C%20transductive%20learning%20contexts%20may%20be%20affected%20by%20dataset%0Ashift%2C%20i.e.%2C%20may%20be%20such%20that%20the%20IID%20assumption%20does%20not%20hold.%20We%20here%20propose%0Aa%20method%2C%20tailored%20to%20transductive%20classification%20contexts%2C%20for%20performing%0Amodel%20selection%20%28i.e.%2C%20hyperparameter%20optimisation%29%20when%20the%20data%20exhibit%20prior%0Aprobability%20shift%2C%20an%20important%20type%20of%20dataset%20shift%20typical%20of%20anti-causal%0Alearning%20problems.%20In%20our%20proposed%20method%20the%20hyperparameters%20can%20be%20optimised%0Adirectly%20on%20the%20unlabelled%20data%20to%20which%20the%20trained%20classifier%20must%20be%0Aapplied%3B%20this%20is%20unlike%20traditional%20model%20selection%20methods%2C%20that%20are%20based%20on%0Aperforming%20cross-validation%20on%20the%20labelled%20training%20data.%20We%20provide%0Aexperimental%20results%20that%20show%20the%20benefits%20brought%20about%20by%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransductive%2520Model%2520Selection%2520under%2520Prior%2520Probability%2520Shift%26entry.906535625%3DLorenzo%2520Volpi%2520and%2520Alejandro%2520Moreo%2520and%2520Fabrizio%2520Sebastiani%26entry.1292438233%3D%2520%2520Transductive%2520learning%2520is%2520a%2520supervised%2520machine%2520learning%2520task%2520in%2520which%252C%2520unlike%250Ain%2520traditional%2520inductive%2520learning%252C%2520the%2520unlabelled%2520data%2520that%2520require%2520labelling%250Aare%2520a%2520finite%2520set%2520and%2520are%2520available%2520at%2520training%2520time.%2520Similarly%2520to%2520inductive%250Alearning%2520contexts%252C%2520transductive%2520learning%2520contexts%2520may%2520be%2520affected%2520by%2520dataset%250Ashift%252C%2520i.e.%252C%2520may%2520be%2520such%2520that%2520the%2520IID%2520assumption%2520does%2520not%2520hold.%2520We%2520here%2520propose%250Aa%2520method%252C%2520tailored%2520to%2520transductive%2520classification%2520contexts%252C%2520for%2520performing%250Amodel%2520selection%2520%2528i.e.%252C%2520hyperparameter%2520optimisation%2529%2520when%2520the%2520data%2520exhibit%2520prior%250Aprobability%2520shift%252C%2520an%2520important%2520type%2520of%2520dataset%2520shift%2520typical%2520of%2520anti-causal%250Alearning%2520problems.%2520In%2520our%2520proposed%2520method%2520the%2520hyperparameters%2520can%2520be%2520optimised%250Adirectly%2520on%2520the%2520unlabelled%2520data%2520to%2520which%2520the%2520trained%2520classifier%2520must%2520be%250Aapplied%253B%2520this%2520is%2520unlike%2520traditional%2520model%2520selection%2520methods%252C%2520that%2520are%2520based%2520on%250Aperforming%2520cross-validation%2520on%2520the%2520labelled%2520training%2520data.%2520We%2520provide%250Aexperimental%2520results%2520that%2520show%2520the%2520benefits%2520brought%2520about%2520by%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transductive%20Model%20Selection%20under%20Prior%20Probability%20Shift&entry.906535625=Lorenzo%20Volpi%20and%20Alejandro%20Moreo%20and%20Fabrizio%20Sebastiani&entry.1292438233=%20%20Transductive%20learning%20is%20a%20supervised%20machine%20learning%20task%20in%20which%2C%20unlike%0Ain%20traditional%20inductive%20learning%2C%20the%20unlabelled%20data%20that%20require%20labelling%0Aare%20a%20finite%20set%20and%20are%20available%20at%20training%20time.%20Similarly%20to%20inductive%0Alearning%20contexts%2C%20transductive%20learning%20contexts%20may%20be%20affected%20by%20dataset%0Ashift%2C%20i.e.%2C%20may%20be%20such%20that%20the%20IID%20assumption%20does%20not%20hold.%20We%20here%20propose%0Aa%20method%2C%20tailored%20to%20transductive%20classification%20contexts%2C%20for%20performing%0Amodel%20selection%20%28i.e.%2C%20hyperparameter%20optimisation%29%20when%20the%20data%20exhibit%20prior%0Aprobability%20shift%2C%20an%20important%20type%20of%20dataset%20shift%20typical%20of%20anti-causal%0Alearning%20problems.%20In%20our%20proposed%20method%20the%20hyperparameters%20can%20be%20optimised%0Adirectly%20on%20the%20unlabelled%20data%20to%20which%20the%20trained%20classifier%20must%20be%0Aapplied%3B%20this%20is%20unlike%20traditional%20model%20selection%20methods%2C%20that%20are%20based%20on%0Aperforming%20cross-validation%20on%20the%20labelled%20training%20data.%20We%20provide%0Aexperimental%20results%20that%20show%20the%20benefits%20brought%20about%20by%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22647v1&entry.124074799=Read"},
{"title": "The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL\n  Divergence Analysis", "author": "Bernardo P. Schaeffer and Ricardo M. S. Rosa and Glauco Valle", "abstract": "  Sampling in score-based diffusion models can be performed by solving either a\nreverse-time stochastic differential equation (SDE) parameterized by an\narbitrary time-dependent stochasticity parameter or a probability flow ODE,\ncorresponding to the stochasticity parameter set to zero. In this work, we\nstudy the effect of this stochasticity on the generation process through bounds\non the Kullback-Leibler (KL) divergence, complementing the analysis with\nnumerical and analytical examples. Our main results apply to linear forward\nSDEs with additive noise and Lipschitz-continuous score functions, and quantify\nhow errors from the prior distribution and score approximation propagate under\ndifferent choices of the stochasticity parameter. The theoretical bounds are\nderived using log-Sobolev inequalities for the marginals of the forward\nprocess, which enable a more effective control of the KL divergence decay along\nsampling. For exact score functions, we find that stochasticity acts as an\nerror-correcting mechanism, decreasing KL divergence along the sampling\ntrajectory. For an approximate score function, there is a trade-off between\nerror correction and score error amplification, so that stochasticity can\neither improve or worsen the performance, depending on the structure of the\nscore error. Numerical experiments on simple datasets and a fully analytical\nexample are included to illustrate and enlighten the theoretical results.\n", "link": "http://arxiv.org/abs/2506.11378v2", "date": "2025-07-30", "relevancy": 1.3798, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4917}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4563}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Effect%20of%20Stochasticity%20in%20Score-Based%20Diffusion%20Sampling%3A%20a%20KL%0A%20%20Divergence%20Analysis&body=Title%3A%20The%20Effect%20of%20Stochasticity%20in%20Score-Based%20Diffusion%20Sampling%3A%20a%20KL%0A%20%20Divergence%20Analysis%0AAuthor%3A%20Bernardo%20P.%20Schaeffer%20and%20Ricardo%20M.%20S.%20Rosa%20and%20Glauco%20Valle%0AAbstract%3A%20%20%20Sampling%20in%20score-based%20diffusion%20models%20can%20be%20performed%20by%20solving%20either%20a%0Areverse-time%20stochastic%20differential%20equation%20%28SDE%29%20parameterized%20by%20an%0Aarbitrary%20time-dependent%20stochasticity%20parameter%20or%20a%20probability%20flow%20ODE%2C%0Acorresponding%20to%20the%20stochasticity%20parameter%20set%20to%20zero.%20In%20this%20work%2C%20we%0Astudy%20the%20effect%20of%20this%20stochasticity%20on%20the%20generation%20process%20through%20bounds%0Aon%20the%20Kullback-Leibler%20%28KL%29%20divergence%2C%20complementing%20the%20analysis%20with%0Anumerical%20and%20analytical%20examples.%20Our%20main%20results%20apply%20to%20linear%20forward%0ASDEs%20with%20additive%20noise%20and%20Lipschitz-continuous%20score%20functions%2C%20and%20quantify%0Ahow%20errors%20from%20the%20prior%20distribution%20and%20score%20approximation%20propagate%20under%0Adifferent%20choices%20of%20the%20stochasticity%20parameter.%20The%20theoretical%20bounds%20are%0Aderived%20using%20log-Sobolev%20inequalities%20for%20the%20marginals%20of%20the%20forward%0Aprocess%2C%20which%20enable%20a%20more%20effective%20control%20of%20the%20KL%20divergence%20decay%20along%0Asampling.%20For%20exact%20score%20functions%2C%20we%20find%20that%20stochasticity%20acts%20as%20an%0Aerror-correcting%20mechanism%2C%20decreasing%20KL%20divergence%20along%20the%20sampling%0Atrajectory.%20For%20an%20approximate%20score%20function%2C%20there%20is%20a%20trade-off%20between%0Aerror%20correction%20and%20score%20error%20amplification%2C%20so%20that%20stochasticity%20can%0Aeither%20improve%20or%20worsen%20the%20performance%2C%20depending%20on%20the%20structure%20of%20the%0Ascore%20error.%20Numerical%20experiments%20on%20simple%20datasets%20and%20a%20fully%20analytical%0Aexample%20are%20included%20to%20illustrate%20and%20enlighten%20the%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Effect%2520of%2520Stochasticity%2520in%2520Score-Based%2520Diffusion%2520Sampling%253A%2520a%2520KL%250A%2520%2520Divergence%2520Analysis%26entry.906535625%3DBernardo%2520P.%2520Schaeffer%2520and%2520Ricardo%2520M.%2520S.%2520Rosa%2520and%2520Glauco%2520Valle%26entry.1292438233%3D%2520%2520Sampling%2520in%2520score-based%2520diffusion%2520models%2520can%2520be%2520performed%2520by%2520solving%2520either%2520a%250Areverse-time%2520stochastic%2520differential%2520equation%2520%2528SDE%2529%2520parameterized%2520by%2520an%250Aarbitrary%2520time-dependent%2520stochasticity%2520parameter%2520or%2520a%2520probability%2520flow%2520ODE%252C%250Acorresponding%2520to%2520the%2520stochasticity%2520parameter%2520set%2520to%2520zero.%2520In%2520this%2520work%252C%2520we%250Astudy%2520the%2520effect%2520of%2520this%2520stochasticity%2520on%2520the%2520generation%2520process%2520through%2520bounds%250Aon%2520the%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%252C%2520complementing%2520the%2520analysis%2520with%250Anumerical%2520and%2520analytical%2520examples.%2520Our%2520main%2520results%2520apply%2520to%2520linear%2520forward%250ASDEs%2520with%2520additive%2520noise%2520and%2520Lipschitz-continuous%2520score%2520functions%252C%2520and%2520quantify%250Ahow%2520errors%2520from%2520the%2520prior%2520distribution%2520and%2520score%2520approximation%2520propagate%2520under%250Adifferent%2520choices%2520of%2520the%2520stochasticity%2520parameter.%2520The%2520theoretical%2520bounds%2520are%250Aderived%2520using%2520log-Sobolev%2520inequalities%2520for%2520the%2520marginals%2520of%2520the%2520forward%250Aprocess%252C%2520which%2520enable%2520a%2520more%2520effective%2520control%2520of%2520the%2520KL%2520divergence%2520decay%2520along%250Asampling.%2520For%2520exact%2520score%2520functions%252C%2520we%2520find%2520that%2520stochasticity%2520acts%2520as%2520an%250Aerror-correcting%2520mechanism%252C%2520decreasing%2520KL%2520divergence%2520along%2520the%2520sampling%250Atrajectory.%2520For%2520an%2520approximate%2520score%2520function%252C%2520there%2520is%2520a%2520trade-off%2520between%250Aerror%2520correction%2520and%2520score%2520error%2520amplification%252C%2520so%2520that%2520stochasticity%2520can%250Aeither%2520improve%2520or%2520worsen%2520the%2520performance%252C%2520depending%2520on%2520the%2520structure%2520of%2520the%250Ascore%2520error.%2520Numerical%2520experiments%2520on%2520simple%2520datasets%2520and%2520a%2520fully%2520analytical%250Aexample%2520are%2520included%2520to%2520illustrate%2520and%2520enlighten%2520the%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Effect%20of%20Stochasticity%20in%20Score-Based%20Diffusion%20Sampling%3A%20a%20KL%0A%20%20Divergence%20Analysis&entry.906535625=Bernardo%20P.%20Schaeffer%20and%20Ricardo%20M.%20S.%20Rosa%20and%20Glauco%20Valle&entry.1292438233=%20%20Sampling%20in%20score-based%20diffusion%20models%20can%20be%20performed%20by%20solving%20either%20a%0Areverse-time%20stochastic%20differential%20equation%20%28SDE%29%20parameterized%20by%20an%0Aarbitrary%20time-dependent%20stochasticity%20parameter%20or%20a%20probability%20flow%20ODE%2C%0Acorresponding%20to%20the%20stochasticity%20parameter%20set%20to%20zero.%20In%20this%20work%2C%20we%0Astudy%20the%20effect%20of%20this%20stochasticity%20on%20the%20generation%20process%20through%20bounds%0Aon%20the%20Kullback-Leibler%20%28KL%29%20divergence%2C%20complementing%20the%20analysis%20with%0Anumerical%20and%20analytical%20examples.%20Our%20main%20results%20apply%20to%20linear%20forward%0ASDEs%20with%20additive%20noise%20and%20Lipschitz-continuous%20score%20functions%2C%20and%20quantify%0Ahow%20errors%20from%20the%20prior%20distribution%20and%20score%20approximation%20propagate%20under%0Adifferent%20choices%20of%20the%20stochasticity%20parameter.%20The%20theoretical%20bounds%20are%0Aderived%20using%20log-Sobolev%20inequalities%20for%20the%20marginals%20of%20the%20forward%0Aprocess%2C%20which%20enable%20a%20more%20effective%20control%20of%20the%20KL%20divergence%20decay%20along%0Asampling.%20For%20exact%20score%20functions%2C%20we%20find%20that%20stochasticity%20acts%20as%20an%0Aerror-correcting%20mechanism%2C%20decreasing%20KL%20divergence%20along%20the%20sampling%0Atrajectory.%20For%20an%20approximate%20score%20function%2C%20there%20is%20a%20trade-off%20between%0Aerror%20correction%20and%20score%20error%20amplification%2C%20so%20that%20stochasticity%20can%0Aeither%20improve%20or%20worsen%20the%20performance%2C%20depending%20on%20the%20structure%20of%20the%0Ascore%20error.%20Numerical%20experiments%20on%20simple%20datasets%20and%20a%20fully%20analytical%0Aexample%20are%20included%20to%20illustrate%20and%20enlighten%20the%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11378v2&entry.124074799=Read"},
{"title": "Towards the Law of Capacity Gap in Distilling Language Models", "author": "Chen Zhang and Qiuchi Li and Dawei Song and Zheyu Ye and Yan Gao and Yan Hu", "abstract": "  Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors.\n", "link": "http://arxiv.org/abs/2311.07052v4", "date": "2025-07-30", "relevancy": 1.7595, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20the%20Law%20of%20Capacity%20Gap%20in%20Distilling%20Language%20Models&body=Title%3A%20Towards%20the%20Law%20of%20Capacity%20Gap%20in%20Distilling%20Language%20Models%0AAuthor%3A%20Chen%20Zhang%20and%20Qiuchi%20Li%20and%20Dawei%20Song%20and%20Zheyu%20Ye%20and%20Yan%20Gao%20and%20Yan%20Hu%0AAbstract%3A%20%20%20Language%20model%20%28LM%29%20distillation%20aims%20at%20distilling%20the%20knowledge%20in%20a%20large%0Ateacher%20LM%20to%20a%20small%20student%20one.%20As%20a%20critical%20issue%20facing%20LM%20distillation%2C%0Aa%20superior%20student%20often%20arises%20from%20a%20teacher%20of%20a%20relatively%20small%20scale%0Ainstead%20of%20a%20larger%20one%2C%20especially%20in%20the%20presence%20of%20substantial%20capacity%20gap%0Abetween%20the%20teacher%20and%20student.%20This%20issue%2C%20often%20referred%20to%20as%20the%0A%5Ctextit%7Bcurse%20of%20capacity%20gap%7D%2C%20suggests%20that%20there%20is%20likely%20an%20optimal%0Ateacher%20yielding%20the%20best-performing%20student%20along%20the%20scaling%20course%20of%20the%0Ateacher.%20Consequently%2C%20distillation%20trials%20on%20teachers%20of%20a%20wide%20range%20of%0Ascales%20are%20called%20for%20to%20determine%20the%20optimal%20teacher%2C%20which%20becomes%0Acomputationally%20intensive%20in%20the%20context%20of%20large%20LMs%20%28LLMs%29.%20This%20paper%0Aaddresses%20this%20critical%20bottleneck%20by%20providing%20the%20%5Ctextit%7Blaw%20of%20capacity%0Agap%7D%20inducted%20from%20a%20preliminary%20study%20on%20distilling%20a%20broad%20range%20of%0Asmall-scale%20%28%3C3B%29%20LMs%2C%20where%20the%20optimal%20teacher%20consistently%20scales%20linearly%0Awith%20the%20student%20scale%20across%20different%20model%20and%20data%20scales.%20By%20extending%20the%0Alaw%20to%20LLM%20distillation%20on%20a%20larger%20scale%20%287B%29%2C%20we%20succeed%20in%20obtaining%0Aversatile%20LLMs%20that%20outperform%20a%20wide%20array%20of%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07052v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520the%2520Law%2520of%2520Capacity%2520Gap%2520in%2520Distilling%2520Language%2520Models%26entry.906535625%3DChen%2520Zhang%2520and%2520Qiuchi%2520Li%2520and%2520Dawei%2520Song%2520and%2520Zheyu%2520Ye%2520and%2520Yan%2520Gao%2520and%2520Yan%2520Hu%26entry.1292438233%3D%2520%2520Language%2520model%2520%2528LM%2529%2520distillation%2520aims%2520at%2520distilling%2520the%2520knowledge%2520in%2520a%2520large%250Ateacher%2520LM%2520to%2520a%2520small%2520student%2520one.%2520As%2520a%2520critical%2520issue%2520facing%2520LM%2520distillation%252C%250Aa%2520superior%2520student%2520often%2520arises%2520from%2520a%2520teacher%2520of%2520a%2520relatively%2520small%2520scale%250Ainstead%2520of%2520a%2520larger%2520one%252C%2520especially%2520in%2520the%2520presence%2520of%2520substantial%2520capacity%2520gap%250Abetween%2520the%2520teacher%2520and%2520student.%2520This%2520issue%252C%2520often%2520referred%2520to%2520as%2520the%250A%255Ctextit%257Bcurse%2520of%2520capacity%2520gap%257D%252C%2520suggests%2520that%2520there%2520is%2520likely%2520an%2520optimal%250Ateacher%2520yielding%2520the%2520best-performing%2520student%2520along%2520the%2520scaling%2520course%2520of%2520the%250Ateacher.%2520Consequently%252C%2520distillation%2520trials%2520on%2520teachers%2520of%2520a%2520wide%2520range%2520of%250Ascales%2520are%2520called%2520for%2520to%2520determine%2520the%2520optimal%2520teacher%252C%2520which%2520becomes%250Acomputationally%2520intensive%2520in%2520the%2520context%2520of%2520large%2520LMs%2520%2528LLMs%2529.%2520This%2520paper%250Aaddresses%2520this%2520critical%2520bottleneck%2520by%2520providing%2520the%2520%255Ctextit%257Blaw%2520of%2520capacity%250Agap%257D%2520inducted%2520from%2520a%2520preliminary%2520study%2520on%2520distilling%2520a%2520broad%2520range%2520of%250Asmall-scale%2520%2528%253C3B%2529%2520LMs%252C%2520where%2520the%2520optimal%2520teacher%2520consistently%2520scales%2520linearly%250Awith%2520the%2520student%2520scale%2520across%2520different%2520model%2520and%2520data%2520scales.%2520By%2520extending%2520the%250Alaw%2520to%2520LLM%2520distillation%2520on%2520a%2520larger%2520scale%2520%25287B%2529%252C%2520we%2520succeed%2520in%2520obtaining%250Aversatile%2520LLMs%2520that%2520outperform%2520a%2520wide%2520array%2520of%2520competitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07052v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20the%20Law%20of%20Capacity%20Gap%20in%20Distilling%20Language%20Models&entry.906535625=Chen%20Zhang%20and%20Qiuchi%20Li%20and%20Dawei%20Song%20and%20Zheyu%20Ye%20and%20Yan%20Gao%20and%20Yan%20Hu&entry.1292438233=%20%20Language%20model%20%28LM%29%20distillation%20aims%20at%20distilling%20the%20knowledge%20in%20a%20large%0Ateacher%20LM%20to%20a%20small%20student%20one.%20As%20a%20critical%20issue%20facing%20LM%20distillation%2C%0Aa%20superior%20student%20often%20arises%20from%20a%20teacher%20of%20a%20relatively%20small%20scale%0Ainstead%20of%20a%20larger%20one%2C%20especially%20in%20the%20presence%20of%20substantial%20capacity%20gap%0Abetween%20the%20teacher%20and%20student.%20This%20issue%2C%20often%20referred%20to%20as%20the%0A%5Ctextit%7Bcurse%20of%20capacity%20gap%7D%2C%20suggests%20that%20there%20is%20likely%20an%20optimal%0Ateacher%20yielding%20the%20best-performing%20student%20along%20the%20scaling%20course%20of%20the%0Ateacher.%20Consequently%2C%20distillation%20trials%20on%20teachers%20of%20a%20wide%20range%20of%0Ascales%20are%20called%20for%20to%20determine%20the%20optimal%20teacher%2C%20which%20becomes%0Acomputationally%20intensive%20in%20the%20context%20of%20large%20LMs%20%28LLMs%29.%20This%20paper%0Aaddresses%20this%20critical%20bottleneck%20by%20providing%20the%20%5Ctextit%7Blaw%20of%20capacity%0Agap%7D%20inducted%20from%20a%20preliminary%20study%20on%20distilling%20a%20broad%20range%20of%0Asmall-scale%20%28%3C3B%29%20LMs%2C%20where%20the%20optimal%20teacher%20consistently%20scales%20linearly%0Awith%20the%20student%20scale%20across%20different%20model%20and%20data%20scales.%20By%20extending%20the%0Alaw%20to%20LLM%20distillation%20on%20a%20larger%20scale%20%287B%29%2C%20we%20succeed%20in%20obtaining%0Aversatile%20LLMs%20that%20outperform%20a%20wide%20array%20of%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07052v4&entry.124074799=Read"},
{"title": "Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful\n  Illusions", "author": "Yiting Qu and Ziqing Yang and Yihan Ma and Michael Backes and Savvas Zannettou and Yang Zhang", "abstract": "  Recent advances in text-to-image diffusion models have enabled the creation\nof a new form of digital art: optical illusions--visual tricks that create\ndifferent perceptions of reality. However, adversaries may misuse such\ntechniques to generate hateful illusions, which embed specific hate messages\ninto harmless scenes and disseminate them across web communities. In this work,\nwe take the first step toward investigating the risks of scalable hateful\nillusion generation and the potential for bypassing current content moderation\nmodels. Specifically, we generate 1,860 optical illusions using Stable\nDiffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are\nhateful illusions that successfully embed hate messages, either overtly or\nsubtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate\nthe performance of six moderation classifiers and nine vision language models\n(VLMs) in identifying hateful illusions. Experimental results reveal\nsignificant vulnerabilities in existing moderation models: the detection\naccuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs.\nWe further identify a critical limitation in their vision encoders, which\nmainly focus on surface-level image details while overlooking the secondary\nlayer of information, i.e., hidden messages. To address this risk, we explore\npreliminary mitigation measures and identify the most effective approaches from\nthe perspectives of image transformations and training-level strategies.\n", "link": "http://arxiv.org/abs/2507.22617v1", "date": "2025-07-30", "relevancy": 1.6287, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5562}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5445}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hate%20in%20Plain%20Sight%3A%20On%20the%20Risks%20of%20Moderating%20AI-Generated%20Hateful%0A%20%20Illusions&body=Title%3A%20Hate%20in%20Plain%20Sight%3A%20On%20the%20Risks%20of%20Moderating%20AI-Generated%20Hateful%0A%20%20Illusions%0AAuthor%3A%20Yiting%20Qu%20and%20Ziqing%20Yang%20and%20Yihan%20Ma%20and%20Michael%20Backes%20and%20Savvas%20Zannettou%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-image%20diffusion%20models%20have%20enabled%20the%20creation%0Aof%20a%20new%20form%20of%20digital%20art%3A%20optical%20illusions--visual%20tricks%20that%20create%0Adifferent%20perceptions%20of%20reality.%20However%2C%20adversaries%20may%20misuse%20such%0Atechniques%20to%20generate%20hateful%20illusions%2C%20which%20embed%20specific%20hate%20messages%0Ainto%20harmless%20scenes%20and%20disseminate%20them%20across%20web%20communities.%20In%20this%20work%2C%0Awe%20take%20the%20first%20step%20toward%20investigating%20the%20risks%20of%20scalable%20hateful%0Aillusion%20generation%20and%20the%20potential%20for%20bypassing%20current%20content%20moderation%0Amodels.%20Specifically%2C%20we%20generate%201%2C860%20optical%20illusions%20using%20Stable%0ADiffusion%20and%20ControlNet%2C%20conditioned%20on%2062%20hate%20messages.%20Of%20these%2C%201%2C571%20are%0Ahateful%20illusions%20that%20successfully%20embed%20hate%20messages%2C%20either%20overtly%20or%0Asubtly%2C%20forming%20the%20Hateful%20Illusion%20dataset.%20Using%20this%20dataset%2C%20we%20evaluate%0Athe%20performance%20of%20six%20moderation%20classifiers%20and%20nine%20vision%20language%20models%0A%28VLMs%29%20in%20identifying%20hateful%20illusions.%20Experimental%20results%20reveal%0Asignificant%20vulnerabilities%20in%20existing%20moderation%20models%3A%20the%20detection%0Aaccuracy%20falls%20below%200.245%20for%20moderation%20classifiers%20and%20below%200.102%20for%20VLMs.%0AWe%20further%20identify%20a%20critical%20limitation%20in%20their%20vision%20encoders%2C%20which%0Amainly%20focus%20on%20surface-level%20image%20details%20while%20overlooking%20the%20secondary%0Alayer%20of%20information%2C%20i.e.%2C%20hidden%20messages.%20To%20address%20this%20risk%2C%20we%20explore%0Apreliminary%20mitigation%20measures%20and%20identify%20the%20most%20effective%20approaches%20from%0Athe%20perspectives%20of%20image%20transformations%20and%20training-level%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHate%2520in%2520Plain%2520Sight%253A%2520On%2520the%2520Risks%2520of%2520Moderating%2520AI-Generated%2520Hateful%250A%2520%2520Illusions%26entry.906535625%3DYiting%2520Qu%2520and%2520Ziqing%2520Yang%2520and%2520Yihan%2520Ma%2520and%2520Michael%2520Backes%2520and%2520Savvas%2520Zannettou%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-image%2520diffusion%2520models%2520have%2520enabled%2520the%2520creation%250Aof%2520a%2520new%2520form%2520of%2520digital%2520art%253A%2520optical%2520illusions--visual%2520tricks%2520that%2520create%250Adifferent%2520perceptions%2520of%2520reality.%2520However%252C%2520adversaries%2520may%2520misuse%2520such%250Atechniques%2520to%2520generate%2520hateful%2520illusions%252C%2520which%2520embed%2520specific%2520hate%2520messages%250Ainto%2520harmless%2520scenes%2520and%2520disseminate%2520them%2520across%2520web%2520communities.%2520In%2520this%2520work%252C%250Awe%2520take%2520the%2520first%2520step%2520toward%2520investigating%2520the%2520risks%2520of%2520scalable%2520hateful%250Aillusion%2520generation%2520and%2520the%2520potential%2520for%2520bypassing%2520current%2520content%2520moderation%250Amodels.%2520Specifically%252C%2520we%2520generate%25201%252C860%2520optical%2520illusions%2520using%2520Stable%250ADiffusion%2520and%2520ControlNet%252C%2520conditioned%2520on%252062%2520hate%2520messages.%2520Of%2520these%252C%25201%252C571%2520are%250Ahateful%2520illusions%2520that%2520successfully%2520embed%2520hate%2520messages%252C%2520either%2520overtly%2520or%250Asubtly%252C%2520forming%2520the%2520Hateful%2520Illusion%2520dataset.%2520Using%2520this%2520dataset%252C%2520we%2520evaluate%250Athe%2520performance%2520of%2520six%2520moderation%2520classifiers%2520and%2520nine%2520vision%2520language%2520models%250A%2528VLMs%2529%2520in%2520identifying%2520hateful%2520illusions.%2520Experimental%2520results%2520reveal%250Asignificant%2520vulnerabilities%2520in%2520existing%2520moderation%2520models%253A%2520the%2520detection%250Aaccuracy%2520falls%2520below%25200.245%2520for%2520moderation%2520classifiers%2520and%2520below%25200.102%2520for%2520VLMs.%250AWe%2520further%2520identify%2520a%2520critical%2520limitation%2520in%2520their%2520vision%2520encoders%252C%2520which%250Amainly%2520focus%2520on%2520surface-level%2520image%2520details%2520while%2520overlooking%2520the%2520secondary%250Alayer%2520of%2520information%252C%2520i.e.%252C%2520hidden%2520messages.%2520To%2520address%2520this%2520risk%252C%2520we%2520explore%250Apreliminary%2520mitigation%2520measures%2520and%2520identify%2520the%2520most%2520effective%2520approaches%2520from%250Athe%2520perspectives%2520of%2520image%2520transformations%2520and%2520training-level%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hate%20in%20Plain%20Sight%3A%20On%20the%20Risks%20of%20Moderating%20AI-Generated%20Hateful%0A%20%20Illusions&entry.906535625=Yiting%20Qu%20and%20Ziqing%20Yang%20and%20Yihan%20Ma%20and%20Michael%20Backes%20and%20Savvas%20Zannettou%20and%20Yang%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20text-to-image%20diffusion%20models%20have%20enabled%20the%20creation%0Aof%20a%20new%20form%20of%20digital%20art%3A%20optical%20illusions--visual%20tricks%20that%20create%0Adifferent%20perceptions%20of%20reality.%20However%2C%20adversaries%20may%20misuse%20such%0Atechniques%20to%20generate%20hateful%20illusions%2C%20which%20embed%20specific%20hate%20messages%0Ainto%20harmless%20scenes%20and%20disseminate%20them%20across%20web%20communities.%20In%20this%20work%2C%0Awe%20take%20the%20first%20step%20toward%20investigating%20the%20risks%20of%20scalable%20hateful%0Aillusion%20generation%20and%20the%20potential%20for%20bypassing%20current%20content%20moderation%0Amodels.%20Specifically%2C%20we%20generate%201%2C860%20optical%20illusions%20using%20Stable%0ADiffusion%20and%20ControlNet%2C%20conditioned%20on%2062%20hate%20messages.%20Of%20these%2C%201%2C571%20are%0Ahateful%20illusions%20that%20successfully%20embed%20hate%20messages%2C%20either%20overtly%20or%0Asubtly%2C%20forming%20the%20Hateful%20Illusion%20dataset.%20Using%20this%20dataset%2C%20we%20evaluate%0Athe%20performance%20of%20six%20moderation%20classifiers%20and%20nine%20vision%20language%20models%0A%28VLMs%29%20in%20identifying%20hateful%20illusions.%20Experimental%20results%20reveal%0Asignificant%20vulnerabilities%20in%20existing%20moderation%20models%3A%20the%20detection%0Aaccuracy%20falls%20below%200.245%20for%20moderation%20classifiers%20and%20below%200.102%20for%20VLMs.%0AWe%20further%20identify%20a%20critical%20limitation%20in%20their%20vision%20encoders%2C%20which%0Amainly%20focus%20on%20surface-level%20image%20details%20while%20overlooking%20the%20secondary%0Alayer%20of%20information%2C%20i.e.%2C%20hidden%20messages.%20To%20address%20this%20risk%2C%20we%20explore%0Apreliminary%20mitigation%20measures%20and%20identify%20the%20most%20effective%20approaches%20from%0Athe%20perspectives%20of%20image%20transformations%20and%20training-level%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22617v1&entry.124074799=Read"},
{"title": "Free-Gate: Planning, Control And Policy Composition via Free Energy\n  Gating", "author": "Francesca Rossi and \u00c9miland Garrab\u00e9 and Giovanni Russo", "abstract": "  We consider the problem of optimally composing a set of primitives to tackle\nplanning and control tasks. To address this problem, we introduce a free energy\ncomputational model for planning and control via policy composition: Free-Gate.\nWithin Free-Gate, control primitives are combined via a gating mechanism that\nminimizes variational free energy. This composition problem is formulated as a\nfinite-horizon optimal control problem, which we prove remains convex even when\nthe cost is not convex in states/actions and the environment is nonlinear,\nstochastic and non-stationary. We develop an algorithm that computes the\noptimal primitives composition and demonstrate its effectiveness via in-silico\nand hardware experiments on an application involving robot navigation in an\nenvironment with obstacles. The experiments highlight that Free-Gate enables\nthe robot to navigate to the destination despite only having available simple\nmotor primitives that, individually, could not fulfill the task.\n", "link": "http://arxiv.org/abs/2412.06636v3", "date": "2025-07-30", "relevancy": 1.3859, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4908}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4847}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-Gate%3A%20Planning%2C%20Control%20And%20Policy%20Composition%20via%20Free%20Energy%0A%20%20Gating&body=Title%3A%20Free-Gate%3A%20Planning%2C%20Control%20And%20Policy%20Composition%20via%20Free%20Energy%0A%20%20Gating%0AAuthor%3A%20Francesca%20Rossi%20and%20%C3%89miland%20Garrab%C3%A9%20and%20Giovanni%20Russo%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20optimally%20composing%20a%20set%20of%20primitives%20to%20tackle%0Aplanning%20and%20control%20tasks.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20free%20energy%0Acomputational%20model%20for%20planning%20and%20control%20via%20policy%20composition%3A%20Free-Gate.%0AWithin%20Free-Gate%2C%20control%20primitives%20are%20combined%20via%20a%20gating%20mechanism%20that%0Aminimizes%20variational%20free%20energy.%20This%20composition%20problem%20is%20formulated%20as%20a%0Afinite-horizon%20optimal%20control%20problem%2C%20which%20we%20prove%20remains%20convex%20even%20when%0Athe%20cost%20is%20not%20convex%20in%20states/actions%20and%20the%20environment%20is%20nonlinear%2C%0Astochastic%20and%20non-stationary.%20We%20develop%20an%20algorithm%20that%20computes%20the%0Aoptimal%20primitives%20composition%20and%20demonstrate%20its%20effectiveness%20via%20in-silico%0Aand%20hardware%20experiments%20on%20an%20application%20involving%20robot%20navigation%20in%20an%0Aenvironment%20with%20obstacles.%20The%20experiments%20highlight%20that%20Free-Gate%20enables%0Athe%20robot%20to%20navigate%20to%20the%20destination%20despite%20only%20having%20available%20simple%0Amotor%20primitives%20that%2C%20individually%2C%20could%20not%20fulfill%20the%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06636v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-Gate%253A%2520Planning%252C%2520Control%2520And%2520Policy%2520Composition%2520via%2520Free%2520Energy%250A%2520%2520Gating%26entry.906535625%3DFrancesca%2520Rossi%2520and%2520%25C3%2589miland%2520Garrab%25C3%25A9%2520and%2520Giovanni%2520Russo%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520optimally%2520composing%2520a%2520set%2520of%2520primitives%2520to%2520tackle%250Aplanning%2520and%2520control%2520tasks.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520free%2520energy%250Acomputational%2520model%2520for%2520planning%2520and%2520control%2520via%2520policy%2520composition%253A%2520Free-Gate.%250AWithin%2520Free-Gate%252C%2520control%2520primitives%2520are%2520combined%2520via%2520a%2520gating%2520mechanism%2520that%250Aminimizes%2520variational%2520free%2520energy.%2520This%2520composition%2520problem%2520is%2520formulated%2520as%2520a%250Afinite-horizon%2520optimal%2520control%2520problem%252C%2520which%2520we%2520prove%2520remains%2520convex%2520even%2520when%250Athe%2520cost%2520is%2520not%2520convex%2520in%2520states/actions%2520and%2520the%2520environment%2520is%2520nonlinear%252C%250Astochastic%2520and%2520non-stationary.%2520We%2520develop%2520an%2520algorithm%2520that%2520computes%2520the%250Aoptimal%2520primitives%2520composition%2520and%2520demonstrate%2520its%2520effectiveness%2520via%2520in-silico%250Aand%2520hardware%2520experiments%2520on%2520an%2520application%2520involving%2520robot%2520navigation%2520in%2520an%250Aenvironment%2520with%2520obstacles.%2520The%2520experiments%2520highlight%2520that%2520Free-Gate%2520enables%250Athe%2520robot%2520to%2520navigate%2520to%2520the%2520destination%2520despite%2520only%2520having%2520available%2520simple%250Amotor%2520primitives%2520that%252C%2520individually%252C%2520could%2520not%2520fulfill%2520the%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06636v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-Gate%3A%20Planning%2C%20Control%20And%20Policy%20Composition%20via%20Free%20Energy%0A%20%20Gating&entry.906535625=Francesca%20Rossi%20and%20%C3%89miland%20Garrab%C3%A9%20and%20Giovanni%20Russo&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20optimally%20composing%20a%20set%20of%20primitives%20to%20tackle%0Aplanning%20and%20control%20tasks.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20free%20energy%0Acomputational%20model%20for%20planning%20and%20control%20via%20policy%20composition%3A%20Free-Gate.%0AWithin%20Free-Gate%2C%20control%20primitives%20are%20combined%20via%20a%20gating%20mechanism%20that%0Aminimizes%20variational%20free%20energy.%20This%20composition%20problem%20is%20formulated%20as%20a%0Afinite-horizon%20optimal%20control%20problem%2C%20which%20we%20prove%20remains%20convex%20even%20when%0Athe%20cost%20is%20not%20convex%20in%20states/actions%20and%20the%20environment%20is%20nonlinear%2C%0Astochastic%20and%20non-stationary.%20We%20develop%20an%20algorithm%20that%20computes%20the%0Aoptimal%20primitives%20composition%20and%20demonstrate%20its%20effectiveness%20via%20in-silico%0Aand%20hardware%20experiments%20on%20an%20application%20involving%20robot%20navigation%20in%20an%0Aenvironment%20with%20obstacles.%20The%20experiments%20highlight%20that%20Free-Gate%20enables%0Athe%20robot%20to%20navigate%20to%20the%20destination%20despite%20only%20having%20available%20simple%0Amotor%20primitives%20that%2C%20individually%2C%20could%20not%20fulfill%20the%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06636v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


