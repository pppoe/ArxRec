<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240905.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model\n  Priors", "author": "Hanyang Yu and Xiaoxiao Long and Ping Tan", "abstract": "  We aim to address sparse-view reconstruction of a 3D scene by leveraging\npriors from large-scale vision models. While recent advancements such as 3D\nGaussian Splatting (3DGS) have demonstrated remarkable successes in 3D\nreconstruction, these methods typically necessitate hundreds of input images\nthat densely capture the underlying scene, making them time-consuming and\nimpractical for real-world applications. However, sparse-view reconstruction is\ninherently ill-posed and under-constrained, often resulting in inferior and\nincomplete outcomes. This is due to issues such as failed initialization,\noverfitting on input images, and a lack of details. To mitigate these\nchallenges, we introduce LM-Gaussian, a method capable of generating\nhigh-quality reconstructions from a limited number of images. Specifically, we\npropose a robust initialization module that leverages stereo priors to aid in\nthe recovery of camera poses and the reliable point clouds. Additionally, a\ndiffusion-based refinement is iteratively applied to incorporate image\ndiffusion priors into the Gaussian optimization process to preserve intricate\nscene details. Finally, we utilize video diffusion priors to further enhance\nthe rendered images for realistic visual effects. Overall, our approach\nsignificantly reduces the data acquisition requirements compared to previous\n3DGS methods. We validate the effectiveness of our framework through\nexperiments on various public datasets, demonstrating its potential for\nhigh-quality 360-degree scene reconstruction. Visual results are on our\nwebsite.\n", "link": "http://arxiv.org/abs/2409.03456v1", "date": "2024-09-05", "relevancy": 3.4461, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7469}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6982}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM-Gaussian%3A%20Boost%20Sparse-view%203D%20Gaussian%20Splatting%20with%20Large%20Model%0A%20%20Priors&body=Title%3A%20LM-Gaussian%3A%20Boost%20Sparse-view%203D%20Gaussian%20Splatting%20with%20Large%20Model%0A%20%20Priors%0AAuthor%3A%20Hanyang%20Yu%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan%0AAbstract%3A%20%20%20We%20aim%20to%20address%20sparse-view%20reconstruction%20of%20a%203D%20scene%20by%20leveraging%0Apriors%20from%20large-scale%20vision%20models.%20While%20recent%20advancements%20such%20as%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20remarkable%20successes%20in%203D%0Areconstruction%2C%20these%20methods%20typically%20necessitate%20hundreds%20of%20input%20images%0Athat%20densely%20capture%20the%20underlying%20scene%2C%20making%20them%20time-consuming%20and%0Aimpractical%20for%20real-world%20applications.%20However%2C%20sparse-view%20reconstruction%20is%0Ainherently%20ill-posed%20and%20under-constrained%2C%20often%20resulting%20in%20inferior%20and%0Aincomplete%20outcomes.%20This%20is%20due%20to%20issues%20such%20as%20failed%20initialization%2C%0Aoverfitting%20on%20input%20images%2C%20and%20a%20lack%20of%20details.%20To%20mitigate%20these%0Achallenges%2C%20we%20introduce%20LM-Gaussian%2C%20a%20method%20capable%20of%20generating%0Ahigh-quality%20reconstructions%20from%20a%20limited%20number%20of%20images.%20Specifically%2C%20we%0Apropose%20a%20robust%20initialization%20module%20that%20leverages%20stereo%20priors%20to%20aid%20in%0Athe%20recovery%20of%20camera%20poses%20and%20the%20reliable%20point%20clouds.%20Additionally%2C%20a%0Adiffusion-based%20refinement%20is%20iteratively%20applied%20to%20incorporate%20image%0Adiffusion%20priors%20into%20the%20Gaussian%20optimization%20process%20to%20preserve%20intricate%0Ascene%20details.%20Finally%2C%20we%20utilize%20video%20diffusion%20priors%20to%20further%20enhance%0Athe%20rendered%20images%20for%20realistic%20visual%20effects.%20Overall%2C%20our%20approach%0Asignificantly%20reduces%20the%20data%20acquisition%20requirements%20compared%20to%20previous%0A3DGS%20methods.%20We%20validate%20the%20effectiveness%20of%20our%20framework%20through%0Aexperiments%20on%20various%20public%20datasets%2C%20demonstrating%20its%20potential%20for%0Ahigh-quality%20360-degree%20scene%20reconstruction.%20Visual%20results%20are%20on%20our%0Awebsite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM-Gaussian%253A%2520Boost%2520Sparse-view%25203D%2520Gaussian%2520Splatting%2520with%2520Large%2520Model%250A%2520%2520Priors%26entry.906535625%3DHanyang%2520Yu%2520and%2520Xiaoxiao%2520Long%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520address%2520sparse-view%2520reconstruction%2520of%2520a%25203D%2520scene%2520by%2520leveraging%250Apriors%2520from%2520large-scale%2520vision%2520models.%2520While%2520recent%2520advancements%2520such%2520as%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520have%2520demonstrated%2520remarkable%2520successes%2520in%25203D%250Areconstruction%252C%2520these%2520methods%2520typically%2520necessitate%2520hundreds%2520of%2520input%2520images%250Athat%2520densely%2520capture%2520the%2520underlying%2520scene%252C%2520making%2520them%2520time-consuming%2520and%250Aimpractical%2520for%2520real-world%2520applications.%2520However%252C%2520sparse-view%2520reconstruction%2520is%250Ainherently%2520ill-posed%2520and%2520under-constrained%252C%2520often%2520resulting%2520in%2520inferior%2520and%250Aincomplete%2520outcomes.%2520This%2520is%2520due%2520to%2520issues%2520such%2520as%2520failed%2520initialization%252C%250Aoverfitting%2520on%2520input%2520images%252C%2520and%2520a%2520lack%2520of%2520details.%2520To%2520mitigate%2520these%250Achallenges%252C%2520we%2520introduce%2520LM-Gaussian%252C%2520a%2520method%2520capable%2520of%2520generating%250Ahigh-quality%2520reconstructions%2520from%2520a%2520limited%2520number%2520of%2520images.%2520Specifically%252C%2520we%250Apropose%2520a%2520robust%2520initialization%2520module%2520that%2520leverages%2520stereo%2520priors%2520to%2520aid%2520in%250Athe%2520recovery%2520of%2520camera%2520poses%2520and%2520the%2520reliable%2520point%2520clouds.%2520Additionally%252C%2520a%250Adiffusion-based%2520refinement%2520is%2520iteratively%2520applied%2520to%2520incorporate%2520image%250Adiffusion%2520priors%2520into%2520the%2520Gaussian%2520optimization%2520process%2520to%2520preserve%2520intricate%250Ascene%2520details.%2520Finally%252C%2520we%2520utilize%2520video%2520diffusion%2520priors%2520to%2520further%2520enhance%250Athe%2520rendered%2520images%2520for%2520realistic%2520visual%2520effects.%2520Overall%252C%2520our%2520approach%250Asignificantly%2520reduces%2520the%2520data%2520acquisition%2520requirements%2520compared%2520to%2520previous%250A3DGS%2520methods.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520framework%2520through%250Aexperiments%2520on%2520various%2520public%2520datasets%252C%2520demonstrating%2520its%2520potential%2520for%250Ahigh-quality%2520360-degree%2520scene%2520reconstruction.%2520Visual%2520results%2520are%2520on%2520our%250Awebsite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM-Gaussian%3A%20Boost%20Sparse-view%203D%20Gaussian%20Splatting%20with%20Large%20Model%0A%20%20Priors&entry.906535625=Hanyang%20Yu%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan&entry.1292438233=%20%20We%20aim%20to%20address%20sparse-view%20reconstruction%20of%20a%203D%20scene%20by%20leveraging%0Apriors%20from%20large-scale%20vision%20models.%20While%20recent%20advancements%20such%20as%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20remarkable%20successes%20in%203D%0Areconstruction%2C%20these%20methods%20typically%20necessitate%20hundreds%20of%20input%20images%0Athat%20densely%20capture%20the%20underlying%20scene%2C%20making%20them%20time-consuming%20and%0Aimpractical%20for%20real-world%20applications.%20However%2C%20sparse-view%20reconstruction%20is%0Ainherently%20ill-posed%20and%20under-constrained%2C%20often%20resulting%20in%20inferior%20and%0Aincomplete%20outcomes.%20This%20is%20due%20to%20issues%20such%20as%20failed%20initialization%2C%0Aoverfitting%20on%20input%20images%2C%20and%20a%20lack%20of%20details.%20To%20mitigate%20these%0Achallenges%2C%20we%20introduce%20LM-Gaussian%2C%20a%20method%20capable%20of%20generating%0Ahigh-quality%20reconstructions%20from%20a%20limited%20number%20of%20images.%20Specifically%2C%20we%0Apropose%20a%20robust%20initialization%20module%20that%20leverages%20stereo%20priors%20to%20aid%20in%0Athe%20recovery%20of%20camera%20poses%20and%20the%20reliable%20point%20clouds.%20Additionally%2C%20a%0Adiffusion-based%20refinement%20is%20iteratively%20applied%20to%20incorporate%20image%0Adiffusion%20priors%20into%20the%20Gaussian%20optimization%20process%20to%20preserve%20intricate%0Ascene%20details.%20Finally%2C%20we%20utilize%20video%20diffusion%20priors%20to%20further%20enhance%0Athe%20rendered%20images%20for%20realistic%20visual%20effects.%20Overall%2C%20our%20approach%0Asignificantly%20reduces%20the%20data%20acquisition%20requirements%20compared%20to%20previous%0A3DGS%20methods.%20We%20validate%20the%20effectiveness%20of%20our%20framework%20through%0Aexperiments%20on%20various%20public%20datasets%2C%20demonstrating%20its%20potential%20for%0Ahigh-quality%20360-degree%20scene%20reconstruction.%20Visual%20results%20are%20on%20our%0Awebsite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03456v1&entry.124074799=Read"},
{"title": "EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian\n  Splatting", "author": "Yuchen Weng and Zhengwen Shen and Ruofan Chen and Qi Wang and Jun Wang", "abstract": "  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.13520v3", "date": "2024-09-05", "relevancy": 3.0983, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6905}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6287}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EaDeblur-GS%3A%20Event%20assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%0A%20%20Splatting&body=Title%3A%20EaDeblur-GS%3A%20Event%20assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Yuchen%20Weng%20and%20Zhengwen%20Shen%20and%20Ruofan%20Chen%20and%20Qi%20Wang%20and%20Jun%20Wang%0AAbstract%3A%20%20%203D%20deblurring%20reconstruction%20techniques%20have%20recently%20seen%20significant%0Aadvancements%20with%20the%20development%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29.%20Although%20these%20techniques%20can%20recover%20relatively%0Aclear%203D%20reconstructions%20from%20blurry%20image%20inputs%2C%20they%20still%20face%20limitations%0Ain%20handling%20severe%20blurring%20and%20complex%20camera%20motion.%20To%20address%20these%20issues%2C%0Awe%20propose%20Event-assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%20Splatting%0A%28EaDeblur-GS%29%2C%20which%20integrates%20event%20camera%20data%20to%20enhance%20the%20robustness%20of%0A3DGS%20against%20motion%20blur.%20By%20employing%20an%20Adaptive%20Deviation%20Estimator%20%28ADE%29%0Anetwork%20to%20estimate%20Gaussian%20center%20deviations%20and%20using%20novel%20loss%20functions%2C%0AEaDeblur-GS%20achieves%20sharp%203D%20reconstructions%20in%20real-time%2C%20demonstrating%0Aperformance%20comparable%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13520v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEaDeblur-GS%253A%2520Event%2520assisted%25203D%2520Deblur%2520Reconstruction%2520with%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DYuchen%2520Weng%2520and%2520Zhengwen%2520Shen%2520and%2520Ruofan%2520Chen%2520and%2520Qi%2520Wang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%25203D%2520deblurring%2520reconstruction%2520techniques%2520have%2520recently%2520seen%2520significant%250Aadvancements%2520with%2520the%2520development%2520of%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529.%2520Although%2520these%2520techniques%2520can%2520recover%2520relatively%250Aclear%25203D%2520reconstructions%2520from%2520blurry%2520image%2520inputs%252C%2520they%2520still%2520face%2520limitations%250Ain%2520handling%2520severe%2520blurring%2520and%2520complex%2520camera%2520motion.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520Event-assisted%25203D%2520Deblur%2520Reconstruction%2520with%2520Gaussian%2520Splatting%250A%2528EaDeblur-GS%2529%252C%2520which%2520integrates%2520event%2520camera%2520data%2520to%2520enhance%2520the%2520robustness%2520of%250A3DGS%2520against%2520motion%2520blur.%2520By%2520employing%2520an%2520Adaptive%2520Deviation%2520Estimator%2520%2528ADE%2529%250Anetwork%2520to%2520estimate%2520Gaussian%2520center%2520deviations%2520and%2520using%2520novel%2520loss%2520functions%252C%250AEaDeblur-GS%2520achieves%2520sharp%25203D%2520reconstructions%2520in%2520real-time%252C%2520demonstrating%250Aperformance%2520comparable%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13520v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EaDeblur-GS%3A%20Event%20assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%0A%20%20Splatting&entry.906535625=Yuchen%20Weng%20and%20Zhengwen%20Shen%20and%20Ruofan%20Chen%20and%20Qi%20Wang%20and%20Jun%20Wang&entry.1292438233=%20%203D%20deblurring%20reconstruction%20techniques%20have%20recently%20seen%20significant%0Aadvancements%20with%20the%20development%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29.%20Although%20these%20techniques%20can%20recover%20relatively%0Aclear%203D%20reconstructions%20from%20blurry%20image%20inputs%2C%20they%20still%20face%20limitations%0Ain%20handling%20severe%20blurring%20and%20complex%20camera%20motion.%20To%20address%20these%20issues%2C%0Awe%20propose%20Event-assisted%203D%20Deblur%20Reconstruction%20with%20Gaussian%20Splatting%0A%28EaDeblur-GS%29%2C%20which%20integrates%20event%20camera%20data%20to%20enhance%20the%20robustness%20of%0A3DGS%20against%20motion%20blur.%20By%20employing%20an%20Adaptive%20Deviation%20Estimator%20%28ADE%29%0Anetwork%20to%20estimate%20Gaussian%20center%20deviations%20and%20using%20novel%20loss%20functions%2C%0AEaDeblur-GS%20achieves%20sharp%203D%20reconstructions%20in%20real-time%2C%20demonstrating%0Aperformance%20comparable%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13520v3&entry.124074799=Read"},
{"title": "Text-Guided Mixup Towards Long-Tailed Image Categorization", "author": "Richard Franklin and Jiawei Yao and Deyang Zhong and Qi Qian and Juhua Hu", "abstract": "  In many real-world applications, the frequency distribution of class labels\nfor training data can exhibit a long-tailed distribution, which challenges\ntraditional approaches of training deep neural networks that require heavy\namounts of balanced data. Gathering and labeling data to balance out the class\nlabel distribution can be both costly and time-consuming. Many existing\nsolutions that enable ensemble learning, re-balancing strategies, or\nfine-tuning applied to deep neural networks are limited by the inert problem of\nfew class samples across a subset of classes. Recently, vision-language models\nlike CLIP have been observed as effective solutions to zero-shot or few-shot\nlearning by grasping a similarity between vision and language features for\nimage and text pairs. Considering that large pre-trained vision-language models\nmay contain valuable side textual information for minor classes, we propose to\nleverage text supervision to tackle the challenge of long-tailed learning.\nConcretely, we propose a novel text-guided mixup technique that takes advantage\nof the semantic relations between classes recognized by the pre-trained text\nencoder to help alleviate the long-tailed problem. Our empirical study on\nbenchmark long-tailed tasks demonstrates the effectiveness of our proposal with\na theoretical guarantee. Our code is available at\nhttps://github.com/rsamf/text-guided-mixup.\n", "link": "http://arxiv.org/abs/2409.03583v1", "date": "2024-09-05", "relevancy": 2.9875, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6037}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Guided%20Mixup%20Towards%20Long-Tailed%20Image%20Categorization&body=Title%3A%20Text-Guided%20Mixup%20Towards%20Long-Tailed%20Image%20Categorization%0AAuthor%3A%20Richard%20Franklin%20and%20Jiawei%20Yao%20and%20Deyang%20Zhong%20and%20Qi%20Qian%20and%20Juhua%20Hu%0AAbstract%3A%20%20%20In%20many%20real-world%20applications%2C%20the%20frequency%20distribution%20of%20class%20labels%0Afor%20training%20data%20can%20exhibit%20a%20long-tailed%20distribution%2C%20which%20challenges%0Atraditional%20approaches%20of%20training%20deep%20neural%20networks%20that%20require%20heavy%0Aamounts%20of%20balanced%20data.%20Gathering%20and%20labeling%20data%20to%20balance%20out%20the%20class%0Alabel%20distribution%20can%20be%20both%20costly%20and%20time-consuming.%20Many%20existing%0Asolutions%20that%20enable%20ensemble%20learning%2C%20re-balancing%20strategies%2C%20or%0Afine-tuning%20applied%20to%20deep%20neural%20networks%20are%20limited%20by%20the%20inert%20problem%20of%0Afew%20class%20samples%20across%20a%20subset%20of%20classes.%20Recently%2C%20vision-language%20models%0Alike%20CLIP%20have%20been%20observed%20as%20effective%20solutions%20to%20zero-shot%20or%20few-shot%0Alearning%20by%20grasping%20a%20similarity%20between%20vision%20and%20language%20features%20for%0Aimage%20and%20text%20pairs.%20Considering%20that%20large%20pre-trained%20vision-language%20models%0Amay%20contain%20valuable%20side%20textual%20information%20for%20minor%20classes%2C%20we%20propose%20to%0Aleverage%20text%20supervision%20to%20tackle%20the%20challenge%20of%20long-tailed%20learning.%0AConcretely%2C%20we%20propose%20a%20novel%20text-guided%20mixup%20technique%20that%20takes%20advantage%0Aof%20the%20semantic%20relations%20between%20classes%20recognized%20by%20the%20pre-trained%20text%0Aencoder%20to%20help%20alleviate%20the%20long-tailed%20problem.%20Our%20empirical%20study%20on%0Abenchmark%20long-tailed%20tasks%20demonstrates%20the%20effectiveness%20of%20our%20proposal%20with%0Aa%20theoretical%20guarantee.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/rsamf/text-guided-mixup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Guided%2520Mixup%2520Towards%2520Long-Tailed%2520Image%2520Categorization%26entry.906535625%3DRichard%2520Franklin%2520and%2520Jiawei%2520Yao%2520and%2520Deyang%2520Zhong%2520and%2520Qi%2520Qian%2520and%2520Juhua%2520Hu%26entry.1292438233%3D%2520%2520In%2520many%2520real-world%2520applications%252C%2520the%2520frequency%2520distribution%2520of%2520class%2520labels%250Afor%2520training%2520data%2520can%2520exhibit%2520a%2520long-tailed%2520distribution%252C%2520which%2520challenges%250Atraditional%2520approaches%2520of%2520training%2520deep%2520neural%2520networks%2520that%2520require%2520heavy%250Aamounts%2520of%2520balanced%2520data.%2520Gathering%2520and%2520labeling%2520data%2520to%2520balance%2520out%2520the%2520class%250Alabel%2520distribution%2520can%2520be%2520both%2520costly%2520and%2520time-consuming.%2520Many%2520existing%250Asolutions%2520that%2520enable%2520ensemble%2520learning%252C%2520re-balancing%2520strategies%252C%2520or%250Afine-tuning%2520applied%2520to%2520deep%2520neural%2520networks%2520are%2520limited%2520by%2520the%2520inert%2520problem%2520of%250Afew%2520class%2520samples%2520across%2520a%2520subset%2520of%2520classes.%2520Recently%252C%2520vision-language%2520models%250Alike%2520CLIP%2520have%2520been%2520observed%2520as%2520effective%2520solutions%2520to%2520zero-shot%2520or%2520few-shot%250Alearning%2520by%2520grasping%2520a%2520similarity%2520between%2520vision%2520and%2520language%2520features%2520for%250Aimage%2520and%2520text%2520pairs.%2520Considering%2520that%2520large%2520pre-trained%2520vision-language%2520models%250Amay%2520contain%2520valuable%2520side%2520textual%2520information%2520for%2520minor%2520classes%252C%2520we%2520propose%2520to%250Aleverage%2520text%2520supervision%2520to%2520tackle%2520the%2520challenge%2520of%2520long-tailed%2520learning.%250AConcretely%252C%2520we%2520propose%2520a%2520novel%2520text-guided%2520mixup%2520technique%2520that%2520takes%2520advantage%250Aof%2520the%2520semantic%2520relations%2520between%2520classes%2520recognized%2520by%2520the%2520pre-trained%2520text%250Aencoder%2520to%2520help%2520alleviate%2520the%2520long-tailed%2520problem.%2520Our%2520empirical%2520study%2520on%250Abenchmark%2520long-tailed%2520tasks%2520demonstrates%2520the%2520effectiveness%2520of%2520our%2520proposal%2520with%250Aa%2520theoretical%2520guarantee.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/rsamf/text-guided-mixup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Guided%20Mixup%20Towards%20Long-Tailed%20Image%20Categorization&entry.906535625=Richard%20Franklin%20and%20Jiawei%20Yao%20and%20Deyang%20Zhong%20and%20Qi%20Qian%20and%20Juhua%20Hu&entry.1292438233=%20%20In%20many%20real-world%20applications%2C%20the%20frequency%20distribution%20of%20class%20labels%0Afor%20training%20data%20can%20exhibit%20a%20long-tailed%20distribution%2C%20which%20challenges%0Atraditional%20approaches%20of%20training%20deep%20neural%20networks%20that%20require%20heavy%0Aamounts%20of%20balanced%20data.%20Gathering%20and%20labeling%20data%20to%20balance%20out%20the%20class%0Alabel%20distribution%20can%20be%20both%20costly%20and%20time-consuming.%20Many%20existing%0Asolutions%20that%20enable%20ensemble%20learning%2C%20re-balancing%20strategies%2C%20or%0Afine-tuning%20applied%20to%20deep%20neural%20networks%20are%20limited%20by%20the%20inert%20problem%20of%0Afew%20class%20samples%20across%20a%20subset%20of%20classes.%20Recently%2C%20vision-language%20models%0Alike%20CLIP%20have%20been%20observed%20as%20effective%20solutions%20to%20zero-shot%20or%20few-shot%0Alearning%20by%20grasping%20a%20similarity%20between%20vision%20and%20language%20features%20for%0Aimage%20and%20text%20pairs.%20Considering%20that%20large%20pre-trained%20vision-language%20models%0Amay%20contain%20valuable%20side%20textual%20information%20for%20minor%20classes%2C%20we%20propose%20to%0Aleverage%20text%20supervision%20to%20tackle%20the%20challenge%20of%20long-tailed%20learning.%0AConcretely%2C%20we%20propose%20a%20novel%20text-guided%20mixup%20technique%20that%20takes%20advantage%0Aof%20the%20semantic%20relations%20between%20classes%20recognized%20by%20the%20pre-trained%20text%0Aencoder%20to%20help%20alleviate%20the%20long-tailed%20problem.%20Our%20empirical%20study%20on%0Abenchmark%20long-tailed%20tasks%20demonstrates%20the%20effectiveness%20of%20our%20proposal%20with%0Aa%20theoretical%20guarantee.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/rsamf/text-guided-mixup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03583v1&entry.124074799=Read"},
{"title": "Automatic occlusion removal from 3D maps for maritime situational\n  awareness", "author": "Felix Sattler and Borja Carrillo Perez and Maurice Stephan and Sarah Barnes", "abstract": "  We introduce a novel method for updating 3D geospatial models, specifically\ntargeting occlusion removal in large-scale maritime environments. Traditional\n3D reconstruction techniques often face problems with dynamic objects, like\ncars or vessels, that obscure the true environment, leading to inaccurate\nmodels or requiring extensive manual editing. Our approach leverages deep\nlearning techniques, including instance segmentation and generative inpainting,\nto directly modify both the texture and geometry of 3D meshes without the need\nfor costly reprocessing. By selectively targeting occluding objects and\npreserving static elements, the method enhances both geometric and visual\naccuracy. This approach not only preserves structural and textural details of\nmap data but also maintains compatibility with current geospatial standards,\nensuring robust performance across diverse datasets. The results demonstrate\nsignificant improvements in 3D model fidelity, making this method highly\napplicable for maritime situational awareness and the dynamic display of\nauxiliary information.\n", "link": "http://arxiv.org/abs/2409.03451v1", "date": "2024-09-05", "relevancy": 2.887, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5839}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5741}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20occlusion%20removal%20from%203D%20maps%20for%20maritime%20situational%0A%20%20awareness&body=Title%3A%20Automatic%20occlusion%20removal%20from%203D%20maps%20for%20maritime%20situational%0A%20%20awareness%0AAuthor%3A%20Felix%20Sattler%20and%20Borja%20Carrillo%20Perez%20and%20Maurice%20Stephan%20and%20Sarah%20Barnes%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20updating%203D%20geospatial%20models%2C%20specifically%0Atargeting%20occlusion%20removal%20in%20large-scale%20maritime%20environments.%20Traditional%0A3D%20reconstruction%20techniques%20often%20face%20problems%20with%20dynamic%20objects%2C%20like%0Acars%20or%20vessels%2C%20that%20obscure%20the%20true%20environment%2C%20leading%20to%20inaccurate%0Amodels%20or%20requiring%20extensive%20manual%20editing.%20Our%20approach%20leverages%20deep%0Alearning%20techniques%2C%20including%20instance%20segmentation%20and%20generative%20inpainting%2C%0Ato%20directly%20modify%20both%20the%20texture%20and%20geometry%20of%203D%20meshes%20without%20the%20need%0Afor%20costly%20reprocessing.%20By%20selectively%20targeting%20occluding%20objects%20and%0Apreserving%20static%20elements%2C%20the%20method%20enhances%20both%20geometric%20and%20visual%0Aaccuracy.%20This%20approach%20not%20only%20preserves%20structural%20and%20textural%20details%20of%0Amap%20data%20but%20also%20maintains%20compatibility%20with%20current%20geospatial%20standards%2C%0Aensuring%20robust%20performance%20across%20diverse%20datasets.%20The%20results%20demonstrate%0Asignificant%20improvements%20in%203D%20model%20fidelity%2C%20making%20this%20method%20highly%0Aapplicable%20for%20maritime%20situational%20awareness%20and%20the%20dynamic%20display%20of%0Aauxiliary%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520occlusion%2520removal%2520from%25203D%2520maps%2520for%2520maritime%2520situational%250A%2520%2520awareness%26entry.906535625%3DFelix%2520Sattler%2520and%2520Borja%2520Carrillo%2520Perez%2520and%2520Maurice%2520Stephan%2520and%2520Sarah%2520Barnes%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520updating%25203D%2520geospatial%2520models%252C%2520specifically%250Atargeting%2520occlusion%2520removal%2520in%2520large-scale%2520maritime%2520environments.%2520Traditional%250A3D%2520reconstruction%2520techniques%2520often%2520face%2520problems%2520with%2520dynamic%2520objects%252C%2520like%250Acars%2520or%2520vessels%252C%2520that%2520obscure%2520the%2520true%2520environment%252C%2520leading%2520to%2520inaccurate%250Amodels%2520or%2520requiring%2520extensive%2520manual%2520editing.%2520Our%2520approach%2520leverages%2520deep%250Alearning%2520techniques%252C%2520including%2520instance%2520segmentation%2520and%2520generative%2520inpainting%252C%250Ato%2520directly%2520modify%2520both%2520the%2520texture%2520and%2520geometry%2520of%25203D%2520meshes%2520without%2520the%2520need%250Afor%2520costly%2520reprocessing.%2520By%2520selectively%2520targeting%2520occluding%2520objects%2520and%250Apreserving%2520static%2520elements%252C%2520the%2520method%2520enhances%2520both%2520geometric%2520and%2520visual%250Aaccuracy.%2520This%2520approach%2520not%2520only%2520preserves%2520structural%2520and%2520textural%2520details%2520of%250Amap%2520data%2520but%2520also%2520maintains%2520compatibility%2520with%2520current%2520geospatial%2520standards%252C%250Aensuring%2520robust%2520performance%2520across%2520diverse%2520datasets.%2520The%2520results%2520demonstrate%250Asignificant%2520improvements%2520in%25203D%2520model%2520fidelity%252C%2520making%2520this%2520method%2520highly%250Aapplicable%2520for%2520maritime%2520situational%2520awareness%2520and%2520the%2520dynamic%2520display%2520of%250Aauxiliary%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20occlusion%20removal%20from%203D%20maps%20for%20maritime%20situational%0A%20%20awareness&entry.906535625=Felix%20Sattler%20and%20Borja%20Carrillo%20Perez%20and%20Maurice%20Stephan%20and%20Sarah%20Barnes&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20updating%203D%20geospatial%20models%2C%20specifically%0Atargeting%20occlusion%20removal%20in%20large-scale%20maritime%20environments.%20Traditional%0A3D%20reconstruction%20techniques%20often%20face%20problems%20with%20dynamic%20objects%2C%20like%0Acars%20or%20vessels%2C%20that%20obscure%20the%20true%20environment%2C%20leading%20to%20inaccurate%0Amodels%20or%20requiring%20extensive%20manual%20editing.%20Our%20approach%20leverages%20deep%0Alearning%20techniques%2C%20including%20instance%20segmentation%20and%20generative%20inpainting%2C%0Ato%20directly%20modify%20both%20the%20texture%20and%20geometry%20of%203D%20meshes%20without%20the%20need%0Afor%20costly%20reprocessing.%20By%20selectively%20targeting%20occluding%20objects%20and%0Apreserving%20static%20elements%2C%20the%20method%20enhances%20both%20geometric%20and%20visual%0Aaccuracy.%20This%20approach%20not%20only%20preserves%20structural%20and%20textural%20details%20of%0Amap%20data%20but%20also%20maintains%20compatibility%20with%20current%20geospatial%20standards%2C%0Aensuring%20robust%20performance%20across%20diverse%20datasets.%20The%20results%20demonstrate%0Asignificant%20improvements%20in%203D%20model%20fidelity%2C%20making%20this%20method%20highly%0Aapplicable%20for%20maritime%20situational%20awareness%20and%20the%20dynamic%20display%20of%0Aauxiliary%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03451v1&entry.124074799=Read"},
{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "author": "Yunze Man and Shuhong Zheng and Zhipeng Bao and Martial Hebert and Liang-Yan Gui and Yu-Xiong Wang", "abstract": "  Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks.\n", "link": "http://arxiv.org/abs/2409.03757v1", "date": "2024-09-05", "relevancy": 2.8678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5963}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lexicon3D%3A%20Probing%20Visual%20Foundation%20Models%20for%20Complex%203D%20Scene%0A%20%20Understanding&body=Title%3A%20Lexicon3D%3A%20Probing%20Visual%20Foundation%20Models%20for%20Complex%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Yunze%20Man%20and%20Shuhong%20Zheng%20and%20Zhipeng%20Bao%20and%20Martial%20Hebert%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Complex%203D%20scene%20understanding%20has%20gained%20increasing%20attention%2C%20with%20scene%0Aencoding%20strategies%20playing%20a%20crucial%20role%20in%20this%20success.%20However%2C%20the%0Aoptimal%20scene%20encoding%20strategies%20for%20various%20scenarios%20remain%20unclear%2C%0Aparticularly%20compared%20to%20their%20image-based%20counterparts.%20To%20address%20this%20issue%2C%0Awe%20present%20a%20comprehensive%20study%20that%20probes%20various%20visual%20encoding%20models%20for%0A3D%20scene%20understanding%2C%20identifying%20the%20strengths%20and%20limitations%20of%20each%20model%0Aacross%20different%20scenarios.%20Our%20evaluation%20spans%20seven%20vision%20foundation%0Aencoders%2C%20including%20image-based%2C%20video-based%2C%20and%203D%20foundation%20models.%20We%0Aevaluate%20these%20models%20in%20four%20tasks%3A%20Vision-Language%20Scene%20Reasoning%2C%20Visual%0AGrounding%2C%20Segmentation%2C%20and%20Registration%2C%20each%20focusing%20on%20different%20aspects%0Aof%20scene%20understanding.%20Our%20evaluations%20yield%20key%20findings%3A%20DINOv2%20demonstrates%0Asuperior%20performance%2C%20video%20models%20excel%20in%20object-level%20tasks%2C%20diffusion%0Amodels%20benefit%20geometric%20tasks%2C%20and%20language-pretrained%20models%20show%20unexpected%0Alimitations%20in%20language-related%20tasks.%20These%20insights%20challenge%20some%0Aconventional%20understandings%2C%20provide%20novel%20perspectives%20on%20leveraging%20visual%0Afoundation%20models%2C%20and%20highlight%20the%20need%20for%20more%20flexible%20encoder%20selection%0Ain%20future%20vision-language%20and%20scene-understanding%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLexicon3D%253A%2520Probing%2520Visual%2520Foundation%2520Models%2520for%2520Complex%25203D%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYunze%2520Man%2520and%2520Shuhong%2520Zheng%2520and%2520Zhipeng%2520Bao%2520and%2520Martial%2520Hebert%2520and%2520Liang-Yan%2520Gui%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520Complex%25203D%2520scene%2520understanding%2520has%2520gained%2520increasing%2520attention%252C%2520with%2520scene%250Aencoding%2520strategies%2520playing%2520a%2520crucial%2520role%2520in%2520this%2520success.%2520However%252C%2520the%250Aoptimal%2520scene%2520encoding%2520strategies%2520for%2520various%2520scenarios%2520remain%2520unclear%252C%250Aparticularly%2520compared%2520to%2520their%2520image-based%2520counterparts.%2520To%2520address%2520this%2520issue%252C%250Awe%2520present%2520a%2520comprehensive%2520study%2520that%2520probes%2520various%2520visual%2520encoding%2520models%2520for%250A3D%2520scene%2520understanding%252C%2520identifying%2520the%2520strengths%2520and%2520limitations%2520of%2520each%2520model%250Aacross%2520different%2520scenarios.%2520Our%2520evaluation%2520spans%2520seven%2520vision%2520foundation%250Aencoders%252C%2520including%2520image-based%252C%2520video-based%252C%2520and%25203D%2520foundation%2520models.%2520We%250Aevaluate%2520these%2520models%2520in%2520four%2520tasks%253A%2520Vision-Language%2520Scene%2520Reasoning%252C%2520Visual%250AGrounding%252C%2520Segmentation%252C%2520and%2520Registration%252C%2520each%2520focusing%2520on%2520different%2520aspects%250Aof%2520scene%2520understanding.%2520Our%2520evaluations%2520yield%2520key%2520findings%253A%2520DINOv2%2520demonstrates%250Asuperior%2520performance%252C%2520video%2520models%2520excel%2520in%2520object-level%2520tasks%252C%2520diffusion%250Amodels%2520benefit%2520geometric%2520tasks%252C%2520and%2520language-pretrained%2520models%2520show%2520unexpected%250Alimitations%2520in%2520language-related%2520tasks.%2520These%2520insights%2520challenge%2520some%250Aconventional%2520understandings%252C%2520provide%2520novel%2520perspectives%2520on%2520leveraging%2520visual%250Afoundation%2520models%252C%2520and%2520highlight%2520the%2520need%2520for%2520more%2520flexible%2520encoder%2520selection%250Ain%2520future%2520vision-language%2520and%2520scene-understanding%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lexicon3D%3A%20Probing%20Visual%20Foundation%20Models%20for%20Complex%203D%20Scene%0A%20%20Understanding&entry.906535625=Yunze%20Man%20and%20Shuhong%20Zheng%20and%20Zhipeng%20Bao%20and%20Martial%20Hebert%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Complex%203D%20scene%20understanding%20has%20gained%20increasing%20attention%2C%20with%20scene%0Aencoding%20strategies%20playing%20a%20crucial%20role%20in%20this%20success.%20However%2C%20the%0Aoptimal%20scene%20encoding%20strategies%20for%20various%20scenarios%20remain%20unclear%2C%0Aparticularly%20compared%20to%20their%20image-based%20counterparts.%20To%20address%20this%20issue%2C%0Awe%20present%20a%20comprehensive%20study%20that%20probes%20various%20visual%20encoding%20models%20for%0A3D%20scene%20understanding%2C%20identifying%20the%20strengths%20and%20limitations%20of%20each%20model%0Aacross%20different%20scenarios.%20Our%20evaluation%20spans%20seven%20vision%20foundation%0Aencoders%2C%20including%20image-based%2C%20video-based%2C%20and%203D%20foundation%20models.%20We%0Aevaluate%20these%20models%20in%20four%20tasks%3A%20Vision-Language%20Scene%20Reasoning%2C%20Visual%0AGrounding%2C%20Segmentation%2C%20and%20Registration%2C%20each%20focusing%20on%20different%20aspects%0Aof%20scene%20understanding.%20Our%20evaluations%20yield%20key%20findings%3A%20DINOv2%20demonstrates%0Asuperior%20performance%2C%20video%20models%20excel%20in%20object-level%20tasks%2C%20diffusion%0Amodels%20benefit%20geometric%20tasks%2C%20and%20language-pretrained%20models%20show%20unexpected%0Alimitations%20in%20language-related%20tasks.%20These%20insights%20challenge%20some%0Aconventional%20understandings%2C%20provide%20novel%20perspectives%20on%20leveraging%20visual%0Afoundation%20models%2C%20and%20highlight%20the%20need%20for%20more%20flexible%20encoder%20selection%0Ain%20future%20vision-language%20and%20scene-understanding%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03757v1&entry.124074799=Read"},
{"title": "EgoHDM: An Online Egocentric-Inertial Human Motion Capture,\n  Localization, and Dense Mapping System", "author": "Bonan Liu and Handi Yin and Manuel Kaufmann and Jinhao He and Sammy Christen and Jie Song and Pan Hui", "abstract": "  We present EgoHDM, an online egocentric-inertial human motion capture\n(mocap), localization, and dense mapping system. Our system uses 6 inertial\nmeasurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the\nfirst human mocap system that offers dense scene mapping in near real-time.\nFurther, it is fast and robust to initialize and fully closes the loop between\nphysically plausible map-aware global human motion estimation and mocap-aware\n3D scene reconstruction. Our key idea is integrating camera localization and\nmapping information with inertial human motion capture bidirectionally in our\nsystem. To achieve this, we design a tightly coupled mocap-aware dense bundle\nadjustment and physics-based body pose correction module leveraging a local\nbody-centric elevation map. The latter introduces a novel terrain-aware contact\nPD controller, which enables characters to physically contact the given local\nelevation map thereby reducing human floating or penetration. We demonstrate\nthe performance of our system on established synthetic and real-world\nbenchmarks. The results show that our method reduces human localization, camera\npose, and mapping accuracy error by 41%, 71%, 46%, respectively, compared to\nthe state of the art. Our qualitative evaluations on newly captured data\nfurther demonstrate that EgoHDM can cover challenging scenarios in non-flat\nterrain including stepping over stairs and outdoor scenes in the wild.\n", "link": "http://arxiv.org/abs/2409.00343v2", "date": "2024-09-05", "relevancy": 2.8675, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5775}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5732}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoHDM%3A%20An%20Online%20Egocentric-Inertial%20Human%20Motion%20Capture%2C%0A%20%20Localization%2C%20and%20Dense%20Mapping%20System&body=Title%3A%20EgoHDM%3A%20An%20Online%20Egocentric-Inertial%20Human%20Motion%20Capture%2C%0A%20%20Localization%2C%20and%20Dense%20Mapping%20System%0AAuthor%3A%20Bonan%20Liu%20and%20Handi%20Yin%20and%20Manuel%20Kaufmann%20and%20Jinhao%20He%20and%20Sammy%20Christen%20and%20Jie%20Song%20and%20Pan%20Hui%0AAbstract%3A%20%20%20We%20present%20EgoHDM%2C%20an%20online%20egocentric-inertial%20human%20motion%20capture%0A%28mocap%29%2C%20localization%2C%20and%20dense%20mapping%20system.%20Our%20system%20uses%206%20inertial%0Ameasurement%20units%20%28IMUs%29%20and%20a%20commodity%20head-mounted%20RGB%20camera.%20EgoHDM%20is%20the%0Afirst%20human%20mocap%20system%20that%20offers%20dense%20scene%20mapping%20in%20near%20real-time.%0AFurther%2C%20it%20is%20fast%20and%20robust%20to%20initialize%20and%20fully%20closes%20the%20loop%20between%0Aphysically%20plausible%20map-aware%20global%20human%20motion%20estimation%20and%20mocap-aware%0A3D%20scene%20reconstruction.%20Our%20key%20idea%20is%20integrating%20camera%20localization%20and%0Amapping%20information%20with%20inertial%20human%20motion%20capture%20bidirectionally%20in%20our%0Asystem.%20To%20achieve%20this%2C%20we%20design%20a%20tightly%20coupled%20mocap-aware%20dense%20bundle%0Aadjustment%20and%20physics-based%20body%20pose%20correction%20module%20leveraging%20a%20local%0Abody-centric%20elevation%20map.%20The%20latter%20introduces%20a%20novel%20terrain-aware%20contact%0APD%20controller%2C%20which%20enables%20characters%20to%20physically%20contact%20the%20given%20local%0Aelevation%20map%20thereby%20reducing%20human%20floating%20or%20penetration.%20We%20demonstrate%0Athe%20performance%20of%20our%20system%20on%20established%20synthetic%20and%20real-world%0Abenchmarks.%20The%20results%20show%20that%20our%20method%20reduces%20human%20localization%2C%20camera%0Apose%2C%20and%20mapping%20accuracy%20error%20by%2041%25%2C%2071%25%2C%2046%25%2C%20respectively%2C%20compared%20to%0Athe%20state%20of%20the%20art.%20Our%20qualitative%20evaluations%20on%20newly%20captured%20data%0Afurther%20demonstrate%20that%20EgoHDM%20can%20cover%20challenging%20scenarios%20in%20non-flat%0Aterrain%20including%20stepping%20over%20stairs%20and%20outdoor%20scenes%20in%20the%20wild.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoHDM%253A%2520An%2520Online%2520Egocentric-Inertial%2520Human%2520Motion%2520Capture%252C%250A%2520%2520Localization%252C%2520and%2520Dense%2520Mapping%2520System%26entry.906535625%3DBonan%2520Liu%2520and%2520Handi%2520Yin%2520and%2520Manuel%2520Kaufmann%2520and%2520Jinhao%2520He%2520and%2520Sammy%2520Christen%2520and%2520Jie%2520Song%2520and%2520Pan%2520Hui%26entry.1292438233%3D%2520%2520We%2520present%2520EgoHDM%252C%2520an%2520online%2520egocentric-inertial%2520human%2520motion%2520capture%250A%2528mocap%2529%252C%2520localization%252C%2520and%2520dense%2520mapping%2520system.%2520Our%2520system%2520uses%25206%2520inertial%250Ameasurement%2520units%2520%2528IMUs%2529%2520and%2520a%2520commodity%2520head-mounted%2520RGB%2520camera.%2520EgoHDM%2520is%2520the%250Afirst%2520human%2520mocap%2520system%2520that%2520offers%2520dense%2520scene%2520mapping%2520in%2520near%2520real-time.%250AFurther%252C%2520it%2520is%2520fast%2520and%2520robust%2520to%2520initialize%2520and%2520fully%2520closes%2520the%2520loop%2520between%250Aphysically%2520plausible%2520map-aware%2520global%2520human%2520motion%2520estimation%2520and%2520mocap-aware%250A3D%2520scene%2520reconstruction.%2520Our%2520key%2520idea%2520is%2520integrating%2520camera%2520localization%2520and%250Amapping%2520information%2520with%2520inertial%2520human%2520motion%2520capture%2520bidirectionally%2520in%2520our%250Asystem.%2520To%2520achieve%2520this%252C%2520we%2520design%2520a%2520tightly%2520coupled%2520mocap-aware%2520dense%2520bundle%250Aadjustment%2520and%2520physics-based%2520body%2520pose%2520correction%2520module%2520leveraging%2520a%2520local%250Abody-centric%2520elevation%2520map.%2520The%2520latter%2520introduces%2520a%2520novel%2520terrain-aware%2520contact%250APD%2520controller%252C%2520which%2520enables%2520characters%2520to%2520physically%2520contact%2520the%2520given%2520local%250Aelevation%2520map%2520thereby%2520reducing%2520human%2520floating%2520or%2520penetration.%2520We%2520demonstrate%250Athe%2520performance%2520of%2520our%2520system%2520on%2520established%2520synthetic%2520and%2520real-world%250Abenchmarks.%2520The%2520results%2520show%2520that%2520our%2520method%2520reduces%2520human%2520localization%252C%2520camera%250Apose%252C%2520and%2520mapping%2520accuracy%2520error%2520by%252041%2525%252C%252071%2525%252C%252046%2525%252C%2520respectively%252C%2520compared%2520to%250Athe%2520state%2520of%2520the%2520art.%2520Our%2520qualitative%2520evaluations%2520on%2520newly%2520captured%2520data%250Afurther%2520demonstrate%2520that%2520EgoHDM%2520can%2520cover%2520challenging%2520scenarios%2520in%2520non-flat%250Aterrain%2520including%2520stepping%2520over%2520stairs%2520and%2520outdoor%2520scenes%2520in%2520the%2520wild.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoHDM%3A%20An%20Online%20Egocentric-Inertial%20Human%20Motion%20Capture%2C%0A%20%20Localization%2C%20and%20Dense%20Mapping%20System&entry.906535625=Bonan%20Liu%20and%20Handi%20Yin%20and%20Manuel%20Kaufmann%20and%20Jinhao%20He%20and%20Sammy%20Christen%20and%20Jie%20Song%20and%20Pan%20Hui&entry.1292438233=%20%20We%20present%20EgoHDM%2C%20an%20online%20egocentric-inertial%20human%20motion%20capture%0A%28mocap%29%2C%20localization%2C%20and%20dense%20mapping%20system.%20Our%20system%20uses%206%20inertial%0Ameasurement%20units%20%28IMUs%29%20and%20a%20commodity%20head-mounted%20RGB%20camera.%20EgoHDM%20is%20the%0Afirst%20human%20mocap%20system%20that%20offers%20dense%20scene%20mapping%20in%20near%20real-time.%0AFurther%2C%20it%20is%20fast%20and%20robust%20to%20initialize%20and%20fully%20closes%20the%20loop%20between%0Aphysically%20plausible%20map-aware%20global%20human%20motion%20estimation%20and%20mocap-aware%0A3D%20scene%20reconstruction.%20Our%20key%20idea%20is%20integrating%20camera%20localization%20and%0Amapping%20information%20with%20inertial%20human%20motion%20capture%20bidirectionally%20in%20our%0Asystem.%20To%20achieve%20this%2C%20we%20design%20a%20tightly%20coupled%20mocap-aware%20dense%20bundle%0Aadjustment%20and%20physics-based%20body%20pose%20correction%20module%20leveraging%20a%20local%0Abody-centric%20elevation%20map.%20The%20latter%20introduces%20a%20novel%20terrain-aware%20contact%0APD%20controller%2C%20which%20enables%20characters%20to%20physically%20contact%20the%20given%20local%0Aelevation%20map%20thereby%20reducing%20human%20floating%20or%20penetration.%20We%20demonstrate%0Athe%20performance%20of%20our%20system%20on%20established%20synthetic%20and%20real-world%0Abenchmarks.%20The%20results%20show%20that%20our%20method%20reduces%20human%20localization%2C%20camera%0Apose%2C%20and%20mapping%20accuracy%20error%20by%2041%25%2C%2071%25%2C%2046%25%2C%20respectively%2C%20compared%20to%0Athe%20state%20of%20the%20art.%20Our%20qualitative%20evaluations%20on%20newly%20captured%20data%0Afurther%20demonstrate%20that%20EgoHDM%20can%20cover%20challenging%20scenarios%20in%20non-flat%0Aterrain%20including%20stepping%20over%20stairs%20and%20outdoor%20scenes%20in%20the%20wild.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00343v2&entry.124074799=Read"},
{"title": "Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface\n  Reconstruction", "author": "Rui Peng and Shihe Shen and Kaiqiang Xiong and Huachen Gao and Jianbo Jiao and Xiaodong Gu and Ronggang Wang", "abstract": "  Reconstructing the high-fidelity surface from multi-view images, especially\nsparse images, is a critical and practical task that has attracted widespread\nattention in recent years. However, existing methods are impeded by the memory\nconstraint or the requirement of ground-truth depths and cannot recover\nsatisfactory geometric details. To this end, we propose SuRF, a new\nSurface-centric framework that incorporates a new Region sparsification based\non a matching Field, achieving good trade-offs between performance, efficiency\nand scalability. To our knowledge, this is the first unsupervised method\nachieving end-to-end sparsification powered by the introduced matching field,\nwhich leverages the weight distribution to efficiently locate the boundary\nregions containing surface. Instead of predicting an SDF value for each voxel,\nwe present a new region sparsification approach to sparse the volume by judging\nwhether the voxel is inside the surface region. In this way, our model can\nexploit higher frequency features around the surface with less memory and\ncomputational consumption. Extensive experiments on multiple benchmarks\ncontaining complex large-scale scenes show that our reconstructions exhibit\nhigh-quality details and achieve new state-of-the-art performance, i.e., 46%\nimprovements with 80% less memory consumption. Code is available at\nhttps://github.com/prstrive/SuRF.\n", "link": "http://arxiv.org/abs/2409.03634v1", "date": "2024-09-05", "relevancy": 2.8581, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6109}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.552}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surface-Centric%20Modeling%20for%20High-Fidelity%20Generalizable%20Neural%20Surface%0A%20%20Reconstruction&body=Title%3A%20Surface-Centric%20Modeling%20for%20High-Fidelity%20Generalizable%20Neural%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Rui%20Peng%20and%20Shihe%20Shen%20and%20Kaiqiang%20Xiong%20and%20Huachen%20Gao%20and%20Jianbo%20Jiao%20and%20Xiaodong%20Gu%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%20Reconstructing%20the%20high-fidelity%20surface%20from%20multi-view%20images%2C%20especially%0Asparse%20images%2C%20is%20a%20critical%20and%20practical%20task%20that%20has%20attracted%20widespread%0Aattention%20in%20recent%20years.%20However%2C%20existing%20methods%20are%20impeded%20by%20the%20memory%0Aconstraint%20or%20the%20requirement%20of%20ground-truth%20depths%20and%20cannot%20recover%0Asatisfactory%20geometric%20details.%20To%20this%20end%2C%20we%20propose%20SuRF%2C%20a%20new%0ASurface-centric%20framework%20that%20incorporates%20a%20new%20Region%20sparsification%20based%0Aon%20a%20matching%20Field%2C%20achieving%20good%20trade-offs%20between%20performance%2C%20efficiency%0Aand%20scalability.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20unsupervised%20method%0Aachieving%20end-to-end%20sparsification%20powered%20by%20the%20introduced%20matching%20field%2C%0Awhich%20leverages%20the%20weight%20distribution%20to%20efficiently%20locate%20the%20boundary%0Aregions%20containing%20surface.%20Instead%20of%20predicting%20an%20SDF%20value%20for%20each%20voxel%2C%0Awe%20present%20a%20new%20region%20sparsification%20approach%20to%20sparse%20the%20volume%20by%20judging%0Awhether%20the%20voxel%20is%20inside%20the%20surface%20region.%20In%20this%20way%2C%20our%20model%20can%0Aexploit%20higher%20frequency%20features%20around%20the%20surface%20with%20less%20memory%20and%0Acomputational%20consumption.%20Extensive%20experiments%20on%20multiple%20benchmarks%0Acontaining%20complex%20large-scale%20scenes%20show%20that%20our%20reconstructions%20exhibit%0Ahigh-quality%20details%20and%20achieve%20new%20state-of-the-art%20performance%2C%20i.e.%2C%2046%25%0Aimprovements%20with%2080%25%20less%20memory%20consumption.%20Code%20is%20available%20at%0Ahttps%3A//github.com/prstrive/SuRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurface-Centric%2520Modeling%2520for%2520High-Fidelity%2520Generalizable%2520Neural%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DRui%2520Peng%2520and%2520Shihe%2520Shen%2520and%2520Kaiqiang%2520Xiong%2520and%2520Huachen%2520Gao%2520and%2520Jianbo%2520Jiao%2520and%2520Xiaodong%2520Gu%2520and%2520Ronggang%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520high-fidelity%2520surface%2520from%2520multi-view%2520images%252C%2520especially%250Asparse%2520images%252C%2520is%2520a%2520critical%2520and%2520practical%2520task%2520that%2520has%2520attracted%2520widespread%250Aattention%2520in%2520recent%2520years.%2520However%252C%2520existing%2520methods%2520are%2520impeded%2520by%2520the%2520memory%250Aconstraint%2520or%2520the%2520requirement%2520of%2520ground-truth%2520depths%2520and%2520cannot%2520recover%250Asatisfactory%2520geometric%2520details.%2520To%2520this%2520end%252C%2520we%2520propose%2520SuRF%252C%2520a%2520new%250ASurface-centric%2520framework%2520that%2520incorporates%2520a%2520new%2520Region%2520sparsification%2520based%250Aon%2520a%2520matching%2520Field%252C%2520achieving%2520good%2520trade-offs%2520between%2520performance%252C%2520efficiency%250Aand%2520scalability.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520unsupervised%2520method%250Aachieving%2520end-to-end%2520sparsification%2520powered%2520by%2520the%2520introduced%2520matching%2520field%252C%250Awhich%2520leverages%2520the%2520weight%2520distribution%2520to%2520efficiently%2520locate%2520the%2520boundary%250Aregions%2520containing%2520surface.%2520Instead%2520of%2520predicting%2520an%2520SDF%2520value%2520for%2520each%2520voxel%252C%250Awe%2520present%2520a%2520new%2520region%2520sparsification%2520approach%2520to%2520sparse%2520the%2520volume%2520by%2520judging%250Awhether%2520the%2520voxel%2520is%2520inside%2520the%2520surface%2520region.%2520In%2520this%2520way%252C%2520our%2520model%2520can%250Aexploit%2520higher%2520frequency%2520features%2520around%2520the%2520surface%2520with%2520less%2520memory%2520and%250Acomputational%2520consumption.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%250Acontaining%2520complex%2520large-scale%2520scenes%2520show%2520that%2520our%2520reconstructions%2520exhibit%250Ahigh-quality%2520details%2520and%2520achieve%2520new%2520state-of-the-art%2520performance%252C%2520i.e.%252C%252046%2525%250Aimprovements%2520with%252080%2525%2520less%2520memory%2520consumption.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/prstrive/SuRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surface-Centric%20Modeling%20for%20High-Fidelity%20Generalizable%20Neural%20Surface%0A%20%20Reconstruction&entry.906535625=Rui%20Peng%20and%20Shihe%20Shen%20and%20Kaiqiang%20Xiong%20and%20Huachen%20Gao%20and%20Jianbo%20Jiao%20and%20Xiaodong%20Gu%20and%20Ronggang%20Wang&entry.1292438233=%20%20Reconstructing%20the%20high-fidelity%20surface%20from%20multi-view%20images%2C%20especially%0Asparse%20images%2C%20is%20a%20critical%20and%20practical%20task%20that%20has%20attracted%20widespread%0Aattention%20in%20recent%20years.%20However%2C%20existing%20methods%20are%20impeded%20by%20the%20memory%0Aconstraint%20or%20the%20requirement%20of%20ground-truth%20depths%20and%20cannot%20recover%0Asatisfactory%20geometric%20details.%20To%20this%20end%2C%20we%20propose%20SuRF%2C%20a%20new%0ASurface-centric%20framework%20that%20incorporates%20a%20new%20Region%20sparsification%20based%0Aon%20a%20matching%20Field%2C%20achieving%20good%20trade-offs%20between%20performance%2C%20efficiency%0Aand%20scalability.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20unsupervised%20method%0Aachieving%20end-to-end%20sparsification%20powered%20by%20the%20introduced%20matching%20field%2C%0Awhich%20leverages%20the%20weight%20distribution%20to%20efficiently%20locate%20the%20boundary%0Aregions%20containing%20surface.%20Instead%20of%20predicting%20an%20SDF%20value%20for%20each%20voxel%2C%0Awe%20present%20a%20new%20region%20sparsification%20approach%20to%20sparse%20the%20volume%20by%20judging%0Awhether%20the%20voxel%20is%20inside%20the%20surface%20region.%20In%20this%20way%2C%20our%20model%20can%0Aexploit%20higher%20frequency%20features%20around%20the%20surface%20with%20less%20memory%20and%0Acomputational%20consumption.%20Extensive%20experiments%20on%20multiple%20benchmarks%0Acontaining%20complex%20large-scale%20scenes%20show%20that%20our%20reconstructions%20exhibit%0Ahigh-quality%20details%20and%20achieve%20new%20state-of-the-art%20performance%2C%20i.e.%2C%2046%25%0Aimprovements%20with%2080%25%20less%20memory%20consumption.%20Code%20is%20available%20at%0Ahttps%3A//github.com/prstrive/SuRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03634v1&entry.124074799=Read"},
{"title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field\n  Representation and Generation", "author": "Yujin Chen and Yinyu Nie and Benjamin Ummenhofer and Reiner Birkl and Michael Paulitsch and Matthias M\u00fcller and Matthias Nie\u00dfner", "abstract": "  We present Mesh2NeRF, an approach to derive ground-truth radiance fields from\ntextured meshes for 3D generation tasks. Many 3D generative approaches\nrepresent 3D scenes as radiance fields for training. Their ground-truth\nradiance fields are usually fitted from multi-view renderings from a\nlarge-scale synthetic 3D dataset, which often results in artifacts due to\nocclusions or under-fitting issues. In Mesh2NeRF, we propose an analytic\nsolution to directly obtain ground-truth radiance fields from 3D meshes,\ncharacterizing the density field with an occupancy function featuring a defined\nsurface thickness, and determining view-dependent color through a reflection\nfunction considering both the mesh and environment lighting. Mesh2NeRF extracts\naccurate radiance fields which provides direct supervision for training\ngenerative NeRFs and single scene representation. We validate the effectiveness\nof Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in\nPSNR for view synthesis in single scene representation on the ABO dataset, a\n0.69 PSNR enhancement in the single-view conditional generation of ShapeNet\nCars, and notably improved mesh extraction from NeRF in the unconditional\ngeneration of Objaverse Mugs.\n", "link": "http://arxiv.org/abs/2403.19319v2", "date": "2024-09-05", "relevancy": 2.7855, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5805}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.548}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh2NeRF%3A%20Direct%20Mesh%20Supervision%20for%20Neural%20Radiance%20Field%0A%20%20Representation%20and%20Generation&body=Title%3A%20Mesh2NeRF%3A%20Direct%20Mesh%20Supervision%20for%20Neural%20Radiance%20Field%0A%20%20Representation%20and%20Generation%0AAuthor%3A%20Yujin%20Chen%20and%20Yinyu%20Nie%20and%20Benjamin%20Ummenhofer%20and%20Reiner%20Birkl%20and%20Michael%20Paulitsch%20and%20Matthias%20M%C3%BCller%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20We%20present%20Mesh2NeRF%2C%20an%20approach%20to%20derive%20ground-truth%20radiance%20fields%20from%0Atextured%20meshes%20for%203D%20generation%20tasks.%20Many%203D%20generative%20approaches%0Arepresent%203D%20scenes%20as%20radiance%20fields%20for%20training.%20Their%20ground-truth%0Aradiance%20fields%20are%20usually%20fitted%20from%20multi-view%20renderings%20from%20a%0Alarge-scale%20synthetic%203D%20dataset%2C%20which%20often%20results%20in%20artifacts%20due%20to%0Aocclusions%20or%20under-fitting%20issues.%20In%20Mesh2NeRF%2C%20we%20propose%20an%20analytic%0Asolution%20to%20directly%20obtain%20ground-truth%20radiance%20fields%20from%203D%20meshes%2C%0Acharacterizing%20the%20density%20field%20with%20an%20occupancy%20function%20featuring%20a%20defined%0Asurface%20thickness%2C%20and%20determining%20view-dependent%20color%20through%20a%20reflection%0Afunction%20considering%20both%20the%20mesh%20and%20environment%20lighting.%20Mesh2NeRF%20extracts%0Aaccurate%20radiance%20fields%20which%20provides%20direct%20supervision%20for%20training%0Agenerative%20NeRFs%20and%20single%20scene%20representation.%20We%20validate%20the%20effectiveness%0Aof%20Mesh2NeRF%20across%20various%20tasks%2C%20achieving%20a%20noteworthy%203.12dB%20improvement%20in%0APSNR%20for%20view%20synthesis%20in%20single%20scene%20representation%20on%20the%20ABO%20dataset%2C%20a%0A0.69%20PSNR%20enhancement%20in%20the%20single-view%20conditional%20generation%20of%20ShapeNet%0ACars%2C%20and%20notably%20improved%20mesh%20extraction%20from%20NeRF%20in%20the%20unconditional%0Ageneration%20of%20Objaverse%20Mugs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh2NeRF%253A%2520Direct%2520Mesh%2520Supervision%2520for%2520Neural%2520Radiance%2520Field%250A%2520%2520Representation%2520and%2520Generation%26entry.906535625%3DYujin%2520Chen%2520and%2520Yinyu%2520Nie%2520and%2520Benjamin%2520Ummenhofer%2520and%2520Reiner%2520Birkl%2520and%2520Michael%2520Paulitsch%2520and%2520Matthias%2520M%25C3%25BCller%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520We%2520present%2520Mesh2NeRF%252C%2520an%2520approach%2520to%2520derive%2520ground-truth%2520radiance%2520fields%2520from%250Atextured%2520meshes%2520for%25203D%2520generation%2520tasks.%2520Many%25203D%2520generative%2520approaches%250Arepresent%25203D%2520scenes%2520as%2520radiance%2520fields%2520for%2520training.%2520Their%2520ground-truth%250Aradiance%2520fields%2520are%2520usually%2520fitted%2520from%2520multi-view%2520renderings%2520from%2520a%250Alarge-scale%2520synthetic%25203D%2520dataset%252C%2520which%2520often%2520results%2520in%2520artifacts%2520due%2520to%250Aocclusions%2520or%2520under-fitting%2520issues.%2520In%2520Mesh2NeRF%252C%2520we%2520propose%2520an%2520analytic%250Asolution%2520to%2520directly%2520obtain%2520ground-truth%2520radiance%2520fields%2520from%25203D%2520meshes%252C%250Acharacterizing%2520the%2520density%2520field%2520with%2520an%2520occupancy%2520function%2520featuring%2520a%2520defined%250Asurface%2520thickness%252C%2520and%2520determining%2520view-dependent%2520color%2520through%2520a%2520reflection%250Afunction%2520considering%2520both%2520the%2520mesh%2520and%2520environment%2520lighting.%2520Mesh2NeRF%2520extracts%250Aaccurate%2520radiance%2520fields%2520which%2520provides%2520direct%2520supervision%2520for%2520training%250Agenerative%2520NeRFs%2520and%2520single%2520scene%2520representation.%2520We%2520validate%2520the%2520effectiveness%250Aof%2520Mesh2NeRF%2520across%2520various%2520tasks%252C%2520achieving%2520a%2520noteworthy%25203.12dB%2520improvement%2520in%250APSNR%2520for%2520view%2520synthesis%2520in%2520single%2520scene%2520representation%2520on%2520the%2520ABO%2520dataset%252C%2520a%250A0.69%2520PSNR%2520enhancement%2520in%2520the%2520single-view%2520conditional%2520generation%2520of%2520ShapeNet%250ACars%252C%2520and%2520notably%2520improved%2520mesh%2520extraction%2520from%2520NeRF%2520in%2520the%2520unconditional%250Ageneration%2520of%2520Objaverse%2520Mugs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh2NeRF%3A%20Direct%20Mesh%20Supervision%20for%20Neural%20Radiance%20Field%0A%20%20Representation%20and%20Generation&entry.906535625=Yujin%20Chen%20and%20Yinyu%20Nie%20and%20Benjamin%20Ummenhofer%20and%20Reiner%20Birkl%20and%20Michael%20Paulitsch%20and%20Matthias%20M%C3%BCller%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20We%20present%20Mesh2NeRF%2C%20an%20approach%20to%20derive%20ground-truth%20radiance%20fields%20from%0Atextured%20meshes%20for%203D%20generation%20tasks.%20Many%203D%20generative%20approaches%0Arepresent%203D%20scenes%20as%20radiance%20fields%20for%20training.%20Their%20ground-truth%0Aradiance%20fields%20are%20usually%20fitted%20from%20multi-view%20renderings%20from%20a%0Alarge-scale%20synthetic%203D%20dataset%2C%20which%20often%20results%20in%20artifacts%20due%20to%0Aocclusions%20or%20under-fitting%20issues.%20In%20Mesh2NeRF%2C%20we%20propose%20an%20analytic%0Asolution%20to%20directly%20obtain%20ground-truth%20radiance%20fields%20from%203D%20meshes%2C%0Acharacterizing%20the%20density%20field%20with%20an%20occupancy%20function%20featuring%20a%20defined%0Asurface%20thickness%2C%20and%20determining%20view-dependent%20color%20through%20a%20reflection%0Afunction%20considering%20both%20the%20mesh%20and%20environment%20lighting.%20Mesh2NeRF%20extracts%0Aaccurate%20radiance%20fields%20which%20provides%20direct%20supervision%20for%20training%0Agenerative%20NeRFs%20and%20single%20scene%20representation.%20We%20validate%20the%20effectiveness%0Aof%20Mesh2NeRF%20across%20various%20tasks%2C%20achieving%20a%20noteworthy%203.12dB%20improvement%20in%0APSNR%20for%20view%20synthesis%20in%20single%20scene%20representation%20on%20the%20ABO%20dataset%2C%20a%0A0.69%20PSNR%20enhancement%20in%20the%20single-view%20conditional%20generation%20of%20ShapeNet%0ACars%2C%20and%20notably%20improved%20mesh%20extraction%20from%20NeRF%20in%20the%20unconditional%0Ageneration%20of%20Objaverse%20Mugs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19319v2&entry.124074799=Read"},
{"title": "Organized Grouped Discrete Representation for Object-Centric Learning", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Object-Centric Learning (OCL) represents dense image or video pixels as\nsparse object features. Representative methods utilize discrete representation\ncomposed of Variational Autoencoder (VAE) template features to suppress\npixel-level information redundancy and guide object-level feature aggregation.\nThe most recent advancement, Grouped Discrete Representation (GDR), further\ndecomposes these template features into attributes. However, its naive channel\ngrouping as decomposition may erroneously group channels belonging to different\nattributes together and discretize them as sub-optimal template attributes,\nwhich losses information and harms expressivity. We propose Organized GDR\n(OGDR) to organize channels belonging to the same attributes together for\ncorrect decomposition from features into attributes. In unsupervised\nsegmentation experiments, OGDR is fully superior to GDR in augmentating\nclassical transformer-based OCL methods; it even improves state-of-the-art\ndiffusion-based ones. Codebook PCA and representation similarity analyses show\nthat compared with GDR, our OGDR eliminates redundancy and preserves\ninformation better for guiding object representation learning. The source code\nis available in the supplementary material.\n", "link": "http://arxiv.org/abs/2409.03553v1", "date": "2024-09-05", "relevancy": 2.7846, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5754}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Organized%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning&body=Title%3A%20Organized%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Object-Centric%20Learning%20%28OCL%29%20represents%20dense%20image%20or%20video%20pixels%20as%0Asparse%20object%20features.%20Representative%20methods%20utilize%20discrete%20representation%0Acomposed%20of%20Variational%20Autoencoder%20%28VAE%29%20template%20features%20to%20suppress%0Apixel-level%20information%20redundancy%20and%20guide%20object-level%20feature%20aggregation.%0AThe%20most%20recent%20advancement%2C%20Grouped%20Discrete%20Representation%20%28GDR%29%2C%20further%0Adecomposes%20these%20template%20features%20into%20attributes.%20However%2C%20its%20naive%20channel%0Agrouping%20as%20decomposition%20may%20erroneously%20group%20channels%20belonging%20to%20different%0Aattributes%20together%20and%20discretize%20them%20as%20sub-optimal%20template%20attributes%2C%0Awhich%20losses%20information%20and%20harms%20expressivity.%20We%20propose%20Organized%20GDR%0A%28OGDR%29%20to%20organize%20channels%20belonging%20to%20the%20same%20attributes%20together%20for%0Acorrect%20decomposition%20from%20features%20into%20attributes.%20In%20unsupervised%0Asegmentation%20experiments%2C%20OGDR%20is%20fully%20superior%20to%20GDR%20in%20augmentating%0Aclassical%20transformer-based%20OCL%20methods%3B%20it%20even%20improves%20state-of-the-art%0Adiffusion-based%20ones.%20Codebook%20PCA%20and%20representation%20similarity%20analyses%20show%0Athat%20compared%20with%20GDR%2C%20our%20OGDR%20eliminates%20redundancy%20and%20preserves%0Ainformation%20better%20for%20guiding%20object%20representation%20learning.%20The%20source%20code%0Ais%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrganized%2520Grouped%2520Discrete%2520Representation%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Object-Centric%2520Learning%2520%2528OCL%2529%2520represents%2520dense%2520image%2520or%2520video%2520pixels%2520as%250Asparse%2520object%2520features.%2520Representative%2520methods%2520utilize%2520discrete%2520representation%250Acomposed%2520of%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520template%2520features%2520to%2520suppress%250Apixel-level%2520information%2520redundancy%2520and%2520guide%2520object-level%2520feature%2520aggregation.%250AThe%2520most%2520recent%2520advancement%252C%2520Grouped%2520Discrete%2520Representation%2520%2528GDR%2529%252C%2520further%250Adecomposes%2520these%2520template%2520features%2520into%2520attributes.%2520However%252C%2520its%2520naive%2520channel%250Agrouping%2520as%2520decomposition%2520may%2520erroneously%2520group%2520channels%2520belonging%2520to%2520different%250Aattributes%2520together%2520and%2520discretize%2520them%2520as%2520sub-optimal%2520template%2520attributes%252C%250Awhich%2520losses%2520information%2520and%2520harms%2520expressivity.%2520We%2520propose%2520Organized%2520GDR%250A%2528OGDR%2529%2520to%2520organize%2520channels%2520belonging%2520to%2520the%2520same%2520attributes%2520together%2520for%250Acorrect%2520decomposition%2520from%2520features%2520into%2520attributes.%2520In%2520unsupervised%250Asegmentation%2520experiments%252C%2520OGDR%2520is%2520fully%2520superior%2520to%2520GDR%2520in%2520augmentating%250Aclassical%2520transformer-based%2520OCL%2520methods%253B%2520it%2520even%2520improves%2520state-of-the-art%250Adiffusion-based%2520ones.%2520Codebook%2520PCA%2520and%2520representation%2520similarity%2520analyses%2520show%250Athat%2520compared%2520with%2520GDR%252C%2520our%2520OGDR%2520eliminates%2520redundancy%2520and%2520preserves%250Ainformation%2520better%2520for%2520guiding%2520object%2520representation%2520learning.%2520The%2520source%2520code%250Ais%2520available%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Organized%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Object-Centric%20Learning%20%28OCL%29%20represents%20dense%20image%20or%20video%20pixels%20as%0Asparse%20object%20features.%20Representative%20methods%20utilize%20discrete%20representation%0Acomposed%20of%20Variational%20Autoencoder%20%28VAE%29%20template%20features%20to%20suppress%0Apixel-level%20information%20redundancy%20and%20guide%20object-level%20feature%20aggregation.%0AThe%20most%20recent%20advancement%2C%20Grouped%20Discrete%20Representation%20%28GDR%29%2C%20further%0Adecomposes%20these%20template%20features%20into%20attributes.%20However%2C%20its%20naive%20channel%0Agrouping%20as%20decomposition%20may%20erroneously%20group%20channels%20belonging%20to%20different%0Aattributes%20together%20and%20discretize%20them%20as%20sub-optimal%20template%20attributes%2C%0Awhich%20losses%20information%20and%20harms%20expressivity.%20We%20propose%20Organized%20GDR%0A%28OGDR%29%20to%20organize%20channels%20belonging%20to%20the%20same%20attributes%20together%20for%0Acorrect%20decomposition%20from%20features%20into%20attributes.%20In%20unsupervised%0Asegmentation%20experiments%2C%20OGDR%20is%20fully%20superior%20to%20GDR%20in%20augmentating%0Aclassical%20transformer-based%20OCL%20methods%3B%20it%20even%20improves%20state-of-the-art%0Adiffusion-based%20ones.%20Codebook%20PCA%20and%20representation%20similarity%20analyses%20show%0Athat%20compared%20with%20GDR%2C%20our%20OGDR%20eliminates%20redundancy%20and%20preserves%0Ainformation%20better%20for%20guiding%20object%20representation%20learning.%20The%20source%20code%0Ais%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03553v1&entry.124074799=Read"},
{"title": "Hierarchical Generative Adversarial Imitation Learning with Mid-level\n  Input Generation for Autonomous Driving on Urban Environments", "author": "Gustavo Claudio Karl Couto and Eric Aislan Antonelo", "abstract": "  Deriving robust control policies for realistic urban navigation scenarios is\nnot a trivial task. In an end-to-end approach, these policies must map\nhigh-dimensional images from the vehicle's cameras to low-level actions such as\nsteering and throttle. While pure Reinforcement Learning (RL) approaches are\nbased exclusively on engineered rewards, Generative Adversarial Imitation\nLearning (GAIL) agents learn from expert demonstrations while interacting with\nthe environment, which favors GAIL on tasks for which a reward signal is\ndifficult to derive, such as autonomous driving. However, training deep\nnetworks directly from raw images on RL tasks is known to be unstable and\ntroublesome. To deal with that, this work proposes a hierarchical GAIL-based\narchitecture (hGAIL) which decouples representation learning from the driving\ntask to solve the autonomous navigation of a vehicle. The proposed architecture\nconsists of two modules: a GAN (Generative Adversarial Net) which generates an\nabstract mid-level input representation, which is the Bird's-Eye View (BEV)\nfrom the surroundings of the vehicle; and the GAIL which learns to control the\nvehicle based on the BEV predictions from the GAN as input. hGAIL is able to\nlearn both the policy and the mid-level representation simultaneously as the\nagent interacts with the environment. Our experiments made in the CARLA\nsimulation environment have shown that GAIL exclusively from cameras (without\nBEV) fails to even learn the task, while hGAIL, after training exclusively on\none city, was able to autonomously navigate successfully in 98% of the\nintersections of a new city not used in training phase. Videos and code\navailable at: https://sites.google.com/view/hgail\n", "link": "http://arxiv.org/abs/2302.04823v5", "date": "2024-09-05", "relevancy": 2.7826, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5973}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5374}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Generative%20Adversarial%20Imitation%20Learning%20with%20Mid-level%0A%20%20Input%20Generation%20for%20Autonomous%20Driving%20on%20Urban%20Environments&body=Title%3A%20Hierarchical%20Generative%20Adversarial%20Imitation%20Learning%20with%20Mid-level%0A%20%20Input%20Generation%20for%20Autonomous%20Driving%20on%20Urban%20Environments%0AAuthor%3A%20Gustavo%20Claudio%20Karl%20Couto%20and%20Eric%20Aislan%20Antonelo%0AAbstract%3A%20%20%20Deriving%20robust%20control%20policies%20for%20realistic%20urban%20navigation%20scenarios%20is%0Anot%20a%20trivial%20task.%20In%20an%20end-to-end%20approach%2C%20these%20policies%20must%20map%0Ahigh-dimensional%20images%20from%20the%20vehicle%27s%20cameras%20to%20low-level%20actions%20such%20as%0Asteering%20and%20throttle.%20While%20pure%20Reinforcement%20Learning%20%28RL%29%20approaches%20are%0Abased%20exclusively%20on%20engineered%20rewards%2C%20Generative%20Adversarial%20Imitation%0ALearning%20%28GAIL%29%20agents%20learn%20from%20expert%20demonstrations%20while%20interacting%20with%0Athe%20environment%2C%20which%20favors%20GAIL%20on%20tasks%20for%20which%20a%20reward%20signal%20is%0Adifficult%20to%20derive%2C%20such%20as%20autonomous%20driving.%20However%2C%20training%20deep%0Anetworks%20directly%20from%20raw%20images%20on%20RL%20tasks%20is%20known%20to%20be%20unstable%20and%0Atroublesome.%20To%20deal%20with%20that%2C%20this%20work%20proposes%20a%20hierarchical%20GAIL-based%0Aarchitecture%20%28hGAIL%29%20which%20decouples%20representation%20learning%20from%20the%20driving%0Atask%20to%20solve%20the%20autonomous%20navigation%20of%20a%20vehicle.%20The%20proposed%20architecture%0Aconsists%20of%20two%20modules%3A%20a%20GAN%20%28Generative%20Adversarial%20Net%29%20which%20generates%20an%0Aabstract%20mid-level%20input%20representation%2C%20which%20is%20the%20Bird%27s-Eye%20View%20%28BEV%29%0Afrom%20the%20surroundings%20of%20the%20vehicle%3B%20and%20the%20GAIL%20which%20learns%20to%20control%20the%0Avehicle%20based%20on%20the%20BEV%20predictions%20from%20the%20GAN%20as%20input.%20hGAIL%20is%20able%20to%0Alearn%20both%20the%20policy%20and%20the%20mid-level%20representation%20simultaneously%20as%20the%0Aagent%20interacts%20with%20the%20environment.%20Our%20experiments%20made%20in%20the%20CARLA%0Asimulation%20environment%20have%20shown%20that%20GAIL%20exclusively%20from%20cameras%20%28without%0ABEV%29%20fails%20to%20even%20learn%20the%20task%2C%20while%20hGAIL%2C%20after%20training%20exclusively%20on%0Aone%20city%2C%20was%20able%20to%20autonomously%20navigate%20successfully%20in%2098%25%20of%20the%0Aintersections%20of%20a%20new%20city%20not%20used%20in%20training%20phase.%20Videos%20and%20code%0Aavailable%20at%3A%20https%3A//sites.google.com/view/hgail%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.04823v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Generative%2520Adversarial%2520Imitation%2520Learning%2520with%2520Mid-level%250A%2520%2520Input%2520Generation%2520for%2520Autonomous%2520Driving%2520on%2520Urban%2520Environments%26entry.906535625%3DGustavo%2520Claudio%2520Karl%2520Couto%2520and%2520Eric%2520Aislan%2520Antonelo%26entry.1292438233%3D%2520%2520Deriving%2520robust%2520control%2520policies%2520for%2520realistic%2520urban%2520navigation%2520scenarios%2520is%250Anot%2520a%2520trivial%2520task.%2520In%2520an%2520end-to-end%2520approach%252C%2520these%2520policies%2520must%2520map%250Ahigh-dimensional%2520images%2520from%2520the%2520vehicle%2527s%2520cameras%2520to%2520low-level%2520actions%2520such%2520as%250Asteering%2520and%2520throttle.%2520While%2520pure%2520Reinforcement%2520Learning%2520%2528RL%2529%2520approaches%2520are%250Abased%2520exclusively%2520on%2520engineered%2520rewards%252C%2520Generative%2520Adversarial%2520Imitation%250ALearning%2520%2528GAIL%2529%2520agents%2520learn%2520from%2520expert%2520demonstrations%2520while%2520interacting%2520with%250Athe%2520environment%252C%2520which%2520favors%2520GAIL%2520on%2520tasks%2520for%2520which%2520a%2520reward%2520signal%2520is%250Adifficult%2520to%2520derive%252C%2520such%2520as%2520autonomous%2520driving.%2520However%252C%2520training%2520deep%250Anetworks%2520directly%2520from%2520raw%2520images%2520on%2520RL%2520tasks%2520is%2520known%2520to%2520be%2520unstable%2520and%250Atroublesome.%2520To%2520deal%2520with%2520that%252C%2520this%2520work%2520proposes%2520a%2520hierarchical%2520GAIL-based%250Aarchitecture%2520%2528hGAIL%2529%2520which%2520decouples%2520representation%2520learning%2520from%2520the%2520driving%250Atask%2520to%2520solve%2520the%2520autonomous%2520navigation%2520of%2520a%2520vehicle.%2520The%2520proposed%2520architecture%250Aconsists%2520of%2520two%2520modules%253A%2520a%2520GAN%2520%2528Generative%2520Adversarial%2520Net%2529%2520which%2520generates%2520an%250Aabstract%2520mid-level%2520input%2520representation%252C%2520which%2520is%2520the%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%250Afrom%2520the%2520surroundings%2520of%2520the%2520vehicle%253B%2520and%2520the%2520GAIL%2520which%2520learns%2520to%2520control%2520the%250Avehicle%2520based%2520on%2520the%2520BEV%2520predictions%2520from%2520the%2520GAN%2520as%2520input.%2520hGAIL%2520is%2520able%2520to%250Alearn%2520both%2520the%2520policy%2520and%2520the%2520mid-level%2520representation%2520simultaneously%2520as%2520the%250Aagent%2520interacts%2520with%2520the%2520environment.%2520Our%2520experiments%2520made%2520in%2520the%2520CARLA%250Asimulation%2520environment%2520have%2520shown%2520that%2520GAIL%2520exclusively%2520from%2520cameras%2520%2528without%250ABEV%2529%2520fails%2520to%2520even%2520learn%2520the%2520task%252C%2520while%2520hGAIL%252C%2520after%2520training%2520exclusively%2520on%250Aone%2520city%252C%2520was%2520able%2520to%2520autonomously%2520navigate%2520successfully%2520in%252098%2525%2520of%2520the%250Aintersections%2520of%2520a%2520new%2520city%2520not%2520used%2520in%2520training%2520phase.%2520Videos%2520and%2520code%250Aavailable%2520at%253A%2520https%253A//sites.google.com/view/hgail%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.04823v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Generative%20Adversarial%20Imitation%20Learning%20with%20Mid-level%0A%20%20Input%20Generation%20for%20Autonomous%20Driving%20on%20Urban%20Environments&entry.906535625=Gustavo%20Claudio%20Karl%20Couto%20and%20Eric%20Aislan%20Antonelo&entry.1292438233=%20%20Deriving%20robust%20control%20policies%20for%20realistic%20urban%20navigation%20scenarios%20is%0Anot%20a%20trivial%20task.%20In%20an%20end-to-end%20approach%2C%20these%20policies%20must%20map%0Ahigh-dimensional%20images%20from%20the%20vehicle%27s%20cameras%20to%20low-level%20actions%20such%20as%0Asteering%20and%20throttle.%20While%20pure%20Reinforcement%20Learning%20%28RL%29%20approaches%20are%0Abased%20exclusively%20on%20engineered%20rewards%2C%20Generative%20Adversarial%20Imitation%0ALearning%20%28GAIL%29%20agents%20learn%20from%20expert%20demonstrations%20while%20interacting%20with%0Athe%20environment%2C%20which%20favors%20GAIL%20on%20tasks%20for%20which%20a%20reward%20signal%20is%0Adifficult%20to%20derive%2C%20such%20as%20autonomous%20driving.%20However%2C%20training%20deep%0Anetworks%20directly%20from%20raw%20images%20on%20RL%20tasks%20is%20known%20to%20be%20unstable%20and%0Atroublesome.%20To%20deal%20with%20that%2C%20this%20work%20proposes%20a%20hierarchical%20GAIL-based%0Aarchitecture%20%28hGAIL%29%20which%20decouples%20representation%20learning%20from%20the%20driving%0Atask%20to%20solve%20the%20autonomous%20navigation%20of%20a%20vehicle.%20The%20proposed%20architecture%0Aconsists%20of%20two%20modules%3A%20a%20GAN%20%28Generative%20Adversarial%20Net%29%20which%20generates%20an%0Aabstract%20mid-level%20input%20representation%2C%20which%20is%20the%20Bird%27s-Eye%20View%20%28BEV%29%0Afrom%20the%20surroundings%20of%20the%20vehicle%3B%20and%20the%20GAIL%20which%20learns%20to%20control%20the%0Avehicle%20based%20on%20the%20BEV%20predictions%20from%20the%20GAN%20as%20input.%20hGAIL%20is%20able%20to%0Alearn%20both%20the%20policy%20and%20the%20mid-level%20representation%20simultaneously%20as%20the%0Aagent%20interacts%20with%20the%20environment.%20Our%20experiments%20made%20in%20the%20CARLA%0Asimulation%20environment%20have%20shown%20that%20GAIL%20exclusively%20from%20cameras%20%28without%0ABEV%29%20fails%20to%20even%20learn%20the%20task%2C%20while%20hGAIL%2C%20after%20training%20exclusively%20on%0Aone%20city%2C%20was%20able%20to%20autonomously%20navigate%20successfully%20in%2098%25%20of%20the%0Aintersections%20of%20a%20new%20city%20not%20used%20in%20training%20phase.%20Videos%20and%20code%0Aavailable%20at%3A%20https%3A//sites.google.com/view/hgail%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.04823v5&entry.124074799=Read"},
{"title": "Triple-domain Feature Learning with Frequency-aware Memory Enhancement\n  for Moving Infrared Small Target Detection", "author": "Weiwei Duan and Luping Ji and Shengjia Chen and Sicheng Zhu and Mao Ye", "abstract": "  As a sub-field of object detection, moving infrared small target detection\npresents significant challenges due to tiny target sizes and low contrast\nagainst backgrounds. Currently-existing methods primarily rely on the features\nextracted only from spatio-temporal domain. Frequency domain has hardly been\nconcerned yet, although it has been widely applied in image processing. To\nextend feature source domains and enhance feature representation, we propose a\nnew Triple-domain Strategy (Tridos) with the frequency-aware memory enhancement\non spatio-temporal domain for infrared small target detection. In this scheme,\nit effectively detaches and enhances frequency features by a local-global\nfrequency-aware module with Fourier transform. Inspired by human visual system,\nour memory enhancement is designed to capture the spatial relations of infrared\ntargets among video frames. Furthermore, it encodes temporal dynamics motion\nfeatures via differential learning and residual enhancing. Additionally, we\nfurther design a residual compensation to reconcile possible cross-domain\nfeature mismatches. To our best knowledge, proposed Tridos is the first work to\nexplore infrared target feature learning comprehensively in\nspatio-temporal-frequency domains. The extensive experiments on three datasets\n(i.e., DAUB, ITSDT-15K and IRDST) validate that our triple-domain infrared\nfeature learning scheme could often be obviously superior to state-of-the-art\nones. Source codes are available at https://github.com/UESTC-nnLab/Tridos.\n", "link": "http://arxiv.org/abs/2406.06949v2", "date": "2024-09-05", "relevancy": 2.7802, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5786}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triple-domain%20Feature%20Learning%20with%20Frequency-aware%20Memory%20Enhancement%0A%20%20for%20Moving%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Triple-domain%20Feature%20Learning%20with%20Frequency-aware%20Memory%20Enhancement%0A%20%20for%20Moving%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Weiwei%20Duan%20and%20Luping%20Ji%20and%20Shengjia%20Chen%20and%20Sicheng%20Zhu%20and%20Mao%20Ye%0AAbstract%3A%20%20%20As%20a%20sub-field%20of%20object%20detection%2C%20moving%20infrared%20small%20target%20detection%0Apresents%20significant%20challenges%20due%20to%20tiny%20target%20sizes%20and%20low%20contrast%0Aagainst%20backgrounds.%20Currently-existing%20methods%20primarily%20rely%20on%20the%20features%0Aextracted%20only%20from%20spatio-temporal%20domain.%20Frequency%20domain%20has%20hardly%20been%0Aconcerned%20yet%2C%20although%20it%20has%20been%20widely%20applied%20in%20image%20processing.%20To%0Aextend%20feature%20source%20domains%20and%20enhance%20feature%20representation%2C%20we%20propose%20a%0Anew%20Triple-domain%20Strategy%20%28Tridos%29%20with%20the%20frequency-aware%20memory%20enhancement%0Aon%20spatio-temporal%20domain%20for%20infrared%20small%20target%20detection.%20In%20this%20scheme%2C%0Ait%20effectively%20detaches%20and%20enhances%20frequency%20features%20by%20a%20local-global%0Afrequency-aware%20module%20with%20Fourier%20transform.%20Inspired%20by%20human%20visual%20system%2C%0Aour%20memory%20enhancement%20is%20designed%20to%20capture%20the%20spatial%20relations%20of%20infrared%0Atargets%20among%20video%20frames.%20Furthermore%2C%20it%20encodes%20temporal%20dynamics%20motion%0Afeatures%20via%20differential%20learning%20and%20residual%20enhancing.%20Additionally%2C%20we%0Afurther%20design%20a%20residual%20compensation%20to%20reconcile%20possible%20cross-domain%0Afeature%20mismatches.%20To%20our%20best%20knowledge%2C%20proposed%20Tridos%20is%20the%20first%20work%20to%0Aexplore%20infrared%20target%20feature%20learning%20comprehensively%20in%0Aspatio-temporal-frequency%20domains.%20The%20extensive%20experiments%20on%20three%20datasets%0A%28i.e.%2C%20DAUB%2C%20ITSDT-15K%20and%20IRDST%29%20validate%20that%20our%20triple-domain%20infrared%0Afeature%20learning%20scheme%20could%20often%20be%20obviously%20superior%20to%20state-of-the-art%0Aones.%20Source%20codes%20are%20available%20at%20https%3A//github.com/UESTC-nnLab/Tridos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriple-domain%2520Feature%2520Learning%2520with%2520Frequency-aware%2520Memory%2520Enhancement%250A%2520%2520for%2520Moving%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DWeiwei%2520Duan%2520and%2520Luping%2520Ji%2520and%2520Shengjia%2520Chen%2520and%2520Sicheng%2520Zhu%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520As%2520a%2520sub-field%2520of%2520object%2520detection%252C%2520moving%2520infrared%2520small%2520target%2520detection%250Apresents%2520significant%2520challenges%2520due%2520to%2520tiny%2520target%2520sizes%2520and%2520low%2520contrast%250Aagainst%2520backgrounds.%2520Currently-existing%2520methods%2520primarily%2520rely%2520on%2520the%2520features%250Aextracted%2520only%2520from%2520spatio-temporal%2520domain.%2520Frequency%2520domain%2520has%2520hardly%2520been%250Aconcerned%2520yet%252C%2520although%2520it%2520has%2520been%2520widely%2520applied%2520in%2520image%2520processing.%2520To%250Aextend%2520feature%2520source%2520domains%2520and%2520enhance%2520feature%2520representation%252C%2520we%2520propose%2520a%250Anew%2520Triple-domain%2520Strategy%2520%2528Tridos%2529%2520with%2520the%2520frequency-aware%2520memory%2520enhancement%250Aon%2520spatio-temporal%2520domain%2520for%2520infrared%2520small%2520target%2520detection.%2520In%2520this%2520scheme%252C%250Ait%2520effectively%2520detaches%2520and%2520enhances%2520frequency%2520features%2520by%2520a%2520local-global%250Afrequency-aware%2520module%2520with%2520Fourier%2520transform.%2520Inspired%2520by%2520human%2520visual%2520system%252C%250Aour%2520memory%2520enhancement%2520is%2520designed%2520to%2520capture%2520the%2520spatial%2520relations%2520of%2520infrared%250Atargets%2520among%2520video%2520frames.%2520Furthermore%252C%2520it%2520encodes%2520temporal%2520dynamics%2520motion%250Afeatures%2520via%2520differential%2520learning%2520and%2520residual%2520enhancing.%2520Additionally%252C%2520we%250Afurther%2520design%2520a%2520residual%2520compensation%2520to%2520reconcile%2520possible%2520cross-domain%250Afeature%2520mismatches.%2520To%2520our%2520best%2520knowledge%252C%2520proposed%2520Tridos%2520is%2520the%2520first%2520work%2520to%250Aexplore%2520infrared%2520target%2520feature%2520learning%2520comprehensively%2520in%250Aspatio-temporal-frequency%2520domains.%2520The%2520extensive%2520experiments%2520on%2520three%2520datasets%250A%2528i.e.%252C%2520DAUB%252C%2520ITSDT-15K%2520and%2520IRDST%2529%2520validate%2520that%2520our%2520triple-domain%2520infrared%250Afeature%2520learning%2520scheme%2520could%2520often%2520be%2520obviously%2520superior%2520to%2520state-of-the-art%250Aones.%2520Source%2520codes%2520are%2520available%2520at%2520https%253A//github.com/UESTC-nnLab/Tridos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triple-domain%20Feature%20Learning%20with%20Frequency-aware%20Memory%20Enhancement%0A%20%20for%20Moving%20Infrared%20Small%20Target%20Detection&entry.906535625=Weiwei%20Duan%20and%20Luping%20Ji%20and%20Shengjia%20Chen%20and%20Sicheng%20Zhu%20and%20Mao%20Ye&entry.1292438233=%20%20As%20a%20sub-field%20of%20object%20detection%2C%20moving%20infrared%20small%20target%20detection%0Apresents%20significant%20challenges%20due%20to%20tiny%20target%20sizes%20and%20low%20contrast%0Aagainst%20backgrounds.%20Currently-existing%20methods%20primarily%20rely%20on%20the%20features%0Aextracted%20only%20from%20spatio-temporal%20domain.%20Frequency%20domain%20has%20hardly%20been%0Aconcerned%20yet%2C%20although%20it%20has%20been%20widely%20applied%20in%20image%20processing.%20To%0Aextend%20feature%20source%20domains%20and%20enhance%20feature%20representation%2C%20we%20propose%20a%0Anew%20Triple-domain%20Strategy%20%28Tridos%29%20with%20the%20frequency-aware%20memory%20enhancement%0Aon%20spatio-temporal%20domain%20for%20infrared%20small%20target%20detection.%20In%20this%20scheme%2C%0Ait%20effectively%20detaches%20and%20enhances%20frequency%20features%20by%20a%20local-global%0Afrequency-aware%20module%20with%20Fourier%20transform.%20Inspired%20by%20human%20visual%20system%2C%0Aour%20memory%20enhancement%20is%20designed%20to%20capture%20the%20spatial%20relations%20of%20infrared%0Atargets%20among%20video%20frames.%20Furthermore%2C%20it%20encodes%20temporal%20dynamics%20motion%0Afeatures%20via%20differential%20learning%20and%20residual%20enhancing.%20Additionally%2C%20we%0Afurther%20design%20a%20residual%20compensation%20to%20reconcile%20possible%20cross-domain%0Afeature%20mismatches.%20To%20our%20best%20knowledge%2C%20proposed%20Tridos%20is%20the%20first%20work%20to%0Aexplore%20infrared%20target%20feature%20learning%20comprehensively%20in%0Aspatio-temporal-frequency%20domains.%20The%20extensive%20experiments%20on%20three%20datasets%0A%28i.e.%2C%20DAUB%2C%20ITSDT-15K%20and%20IRDST%29%20validate%20that%20our%20triple-domain%20infrared%0Afeature%20learning%20scheme%20could%20often%20be%20obviously%20superior%20to%20state-of-the-art%0Aones.%20Source%20codes%20are%20available%20at%20https%3A//github.com/UESTC-nnLab/Tridos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06949v2&entry.124074799=Read"},
{"title": "Neural HD Map Generation from Multiple Vectorized Tiles Locally Produced\n  by Autonomous Vehicles", "author": "Miao Fan and Yi Yao and Jianping Zhang and Xiangbo Song and Daihui Wu", "abstract": "  High-definition (HD) map is a fundamental component of autonomous driving\nsystems, as it can provide precise environmental information about driving\nscenes. Recent work on vectorized map generation could produce merely 65% local\nmap elements around the ego-vehicle at runtime by one tour with onboard\nsensors, leaving a puzzle of how to construct a global HD map projected in the\nworld coordinate system under high-quality standards. To address the issue, we\npresent GNMap as an end-to-end generative neural network to automatically\nconstruct HD maps with multiple vectorized tiles which are locally produced by\nautonomous vehicles through several tours. It leverages a multi-layer and\nattention-based autoencoder as the shared network, of which parameters are\nlearned from two different tasks (i.e., pretraining and finetuning,\nrespectively) to ensure both the completeness of generated maps and the\ncorrectness of element categories. Abundant qualitative evaluations are\nconducted on a real-world dataset and experimental results show that GNMap can\nsurpass the SOTA method by more than 5% F1 score, reaching the level of\nindustrial usage with a small amount of manual modification. We have already\ndeployed it at Navinfo Co., Ltd., serving as an indispensable software to\nautomatically build HD maps for autonomous driving systems.\n", "link": "http://arxiv.org/abs/2409.03445v1", "date": "2024-09-05", "relevancy": 2.7254, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5926}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5262}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20HD%20Map%20Generation%20from%20Multiple%20Vectorized%20Tiles%20Locally%20Produced%0A%20%20by%20Autonomous%20Vehicles&body=Title%3A%20Neural%20HD%20Map%20Generation%20from%20Multiple%20Vectorized%20Tiles%20Locally%20Produced%0A%20%20by%20Autonomous%20Vehicles%0AAuthor%3A%20Miao%20Fan%20and%20Yi%20Yao%20and%20Jianping%20Zhang%20and%20Xiangbo%20Song%20and%20Daihui%20Wu%0AAbstract%3A%20%20%20High-definition%20%28HD%29%20map%20is%20a%20fundamental%20component%20of%20autonomous%20driving%0Asystems%2C%20as%20it%20can%20provide%20precise%20environmental%20information%20about%20driving%0Ascenes.%20Recent%20work%20on%20vectorized%20map%20generation%20could%20produce%20merely%2065%25%20local%0Amap%20elements%20around%20the%20ego-vehicle%20at%20runtime%20by%20one%20tour%20with%20onboard%0Asensors%2C%20leaving%20a%20puzzle%20of%20how%20to%20construct%20a%20global%20HD%20map%20projected%20in%20the%0Aworld%20coordinate%20system%20under%20high-quality%20standards.%20To%20address%20the%20issue%2C%20we%0Apresent%20GNMap%20as%20an%20end-to-end%20generative%20neural%20network%20to%20automatically%0Aconstruct%20HD%20maps%20with%20multiple%20vectorized%20tiles%20which%20are%20locally%20produced%20by%0Aautonomous%20vehicles%20through%20several%20tours.%20It%20leverages%20a%20multi-layer%20and%0Aattention-based%20autoencoder%20as%20the%20shared%20network%2C%20of%20which%20parameters%20are%0Alearned%20from%20two%20different%20tasks%20%28i.e.%2C%20pretraining%20and%20finetuning%2C%0Arespectively%29%20to%20ensure%20both%20the%20completeness%20of%20generated%20maps%20and%20the%0Acorrectness%20of%20element%20categories.%20Abundant%20qualitative%20evaluations%20are%0Aconducted%20on%20a%20real-world%20dataset%20and%20experimental%20results%20show%20that%20GNMap%20can%0Asurpass%20the%20SOTA%20method%20by%20more%20than%205%25%20F1%20score%2C%20reaching%20the%20level%20of%0Aindustrial%20usage%20with%20a%20small%20amount%20of%20manual%20modification.%20We%20have%20already%0Adeployed%20it%20at%20Navinfo%20Co.%2C%20Ltd.%2C%20serving%20as%20an%20indispensable%20software%20to%0Aautomatically%20build%20HD%20maps%20for%20autonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520HD%2520Map%2520Generation%2520from%2520Multiple%2520Vectorized%2520Tiles%2520Locally%2520Produced%250A%2520%2520by%2520Autonomous%2520Vehicles%26entry.906535625%3DMiao%2520Fan%2520and%2520Yi%2520Yao%2520and%2520Jianping%2520Zhang%2520and%2520Xiangbo%2520Song%2520and%2520Daihui%2520Wu%26entry.1292438233%3D%2520%2520High-definition%2520%2528HD%2529%2520map%2520is%2520a%2520fundamental%2520component%2520of%2520autonomous%2520driving%250Asystems%252C%2520as%2520it%2520can%2520provide%2520precise%2520environmental%2520information%2520about%2520driving%250Ascenes.%2520Recent%2520work%2520on%2520vectorized%2520map%2520generation%2520could%2520produce%2520merely%252065%2525%2520local%250Amap%2520elements%2520around%2520the%2520ego-vehicle%2520at%2520runtime%2520by%2520one%2520tour%2520with%2520onboard%250Asensors%252C%2520leaving%2520a%2520puzzle%2520of%2520how%2520to%2520construct%2520a%2520global%2520HD%2520map%2520projected%2520in%2520the%250Aworld%2520coordinate%2520system%2520under%2520high-quality%2520standards.%2520To%2520address%2520the%2520issue%252C%2520we%250Apresent%2520GNMap%2520as%2520an%2520end-to-end%2520generative%2520neural%2520network%2520to%2520automatically%250Aconstruct%2520HD%2520maps%2520with%2520multiple%2520vectorized%2520tiles%2520which%2520are%2520locally%2520produced%2520by%250Aautonomous%2520vehicles%2520through%2520several%2520tours.%2520It%2520leverages%2520a%2520multi-layer%2520and%250Aattention-based%2520autoencoder%2520as%2520the%2520shared%2520network%252C%2520of%2520which%2520parameters%2520are%250Alearned%2520from%2520two%2520different%2520tasks%2520%2528i.e.%252C%2520pretraining%2520and%2520finetuning%252C%250Arespectively%2529%2520to%2520ensure%2520both%2520the%2520completeness%2520of%2520generated%2520maps%2520and%2520the%250Acorrectness%2520of%2520element%2520categories.%2520Abundant%2520qualitative%2520evaluations%2520are%250Aconducted%2520on%2520a%2520real-world%2520dataset%2520and%2520experimental%2520results%2520show%2520that%2520GNMap%2520can%250Asurpass%2520the%2520SOTA%2520method%2520by%2520more%2520than%25205%2525%2520F1%2520score%252C%2520reaching%2520the%2520level%2520of%250Aindustrial%2520usage%2520with%2520a%2520small%2520amount%2520of%2520manual%2520modification.%2520We%2520have%2520already%250Adeployed%2520it%2520at%2520Navinfo%2520Co.%252C%2520Ltd.%252C%2520serving%2520as%2520an%2520indispensable%2520software%2520to%250Aautomatically%2520build%2520HD%2520maps%2520for%2520autonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20HD%20Map%20Generation%20from%20Multiple%20Vectorized%20Tiles%20Locally%20Produced%0A%20%20by%20Autonomous%20Vehicles&entry.906535625=Miao%20Fan%20and%20Yi%20Yao%20and%20Jianping%20Zhang%20and%20Xiangbo%20Song%20and%20Daihui%20Wu&entry.1292438233=%20%20High-definition%20%28HD%29%20map%20is%20a%20fundamental%20component%20of%20autonomous%20driving%0Asystems%2C%20as%20it%20can%20provide%20precise%20environmental%20information%20about%20driving%0Ascenes.%20Recent%20work%20on%20vectorized%20map%20generation%20could%20produce%20merely%2065%25%20local%0Amap%20elements%20around%20the%20ego-vehicle%20at%20runtime%20by%20one%20tour%20with%20onboard%0Asensors%2C%20leaving%20a%20puzzle%20of%20how%20to%20construct%20a%20global%20HD%20map%20projected%20in%20the%0Aworld%20coordinate%20system%20under%20high-quality%20standards.%20To%20address%20the%20issue%2C%20we%0Apresent%20GNMap%20as%20an%20end-to-end%20generative%20neural%20network%20to%20automatically%0Aconstruct%20HD%20maps%20with%20multiple%20vectorized%20tiles%20which%20are%20locally%20produced%20by%0Aautonomous%20vehicles%20through%20several%20tours.%20It%20leverages%20a%20multi-layer%20and%0Aattention-based%20autoencoder%20as%20the%20shared%20network%2C%20of%20which%20parameters%20are%0Alearned%20from%20two%20different%20tasks%20%28i.e.%2C%20pretraining%20and%20finetuning%2C%0Arespectively%29%20to%20ensure%20both%20the%20completeness%20of%20generated%20maps%20and%20the%0Acorrectness%20of%20element%20categories.%20Abundant%20qualitative%20evaluations%20are%0Aconducted%20on%20a%20real-world%20dataset%20and%20experimental%20results%20show%20that%20GNMap%20can%0Asurpass%20the%20SOTA%20method%20by%20more%20than%205%25%20F1%20score%2C%20reaching%20the%20level%20of%0Aindustrial%20usage%20with%20a%20small%20amount%20of%20manual%20modification.%20We%20have%20already%0Adeployed%20it%20at%20Navinfo%20Co.%2C%20Ltd.%2C%20serving%20as%20an%20indispensable%20software%20to%0Aautomatically%20build%20HD%20maps%20for%20autonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03445v1&entry.124074799=Read"},
{"title": "HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual\n  Grounding", "author": "Linhui Xiao and Xiaoshan Yang and Fang Peng and Yaowei Wang and Changsheng Xu", "abstract": "  Visual grounding, which aims to ground a visual region via natural language,\nis a task that heavily relies on cross-modal alignment. Existing works utilized\nuni-modal pre-trained models to transfer visual or linguistic knowledge\nseparately while ignoring the multimodal corresponding information. Motivated\nby recent advancements in contrastive language-image pre-training and low-rank\nadaptation (LoRA) methods, we aim to solve the grounding task based on\nmultimodal pre-training. However, there exists significant task gaps between\npre-training and grounding. Therefore, to address these gaps, we propose a\nconcise and efficient hierarchical multimodal fine-grained modulation\nframework, namely HiVG. Specifically, HiVG consists of a multi-layer adaptive\ncross-modal bridge and a hierarchical multimodal low-rank adaptation (HiLoRA)\nparadigm. The cross-modal bridge can address the inconsistency between visual\nfeatures and those required for grounding, and establish a connection between\nmulti-level visual and text features. HiLoRA prevents the accumulation of\nperceptual errors by adapting the cross-modal features from shallow to deep\nlayers in a hierarchical manner. Experimental results on five datasets\ndemonstrate the effectiveness of our approach and showcase the significant\ngrounding capabilities as well as promising energy efficiency advantages. The\nproject page: https://github.com/linhuixiao/HiVG.\n", "link": "http://arxiv.org/abs/2404.13400v2", "date": "2024-09-05", "relevancy": 2.7227, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.564}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiVG%3A%20Hierarchical%20Multimodal%20Fine-grained%20Modulation%20for%20Visual%0A%20%20Grounding&body=Title%3A%20HiVG%3A%20Hierarchical%20Multimodal%20Fine-grained%20Modulation%20for%20Visual%0A%20%20Grounding%0AAuthor%3A%20Linhui%20Xiao%20and%20Xiaoshan%20Yang%20and%20Fang%20Peng%20and%20Yaowei%20Wang%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20Visual%20grounding%2C%20which%20aims%20to%20ground%20a%20visual%20region%20via%20natural%20language%2C%0Ais%20a%20task%20that%20heavily%20relies%20on%20cross-modal%20alignment.%20Existing%20works%20utilized%0Auni-modal%20pre-trained%20models%20to%20transfer%20visual%20or%20linguistic%20knowledge%0Aseparately%20while%20ignoring%20the%20multimodal%20corresponding%20information.%20Motivated%0Aby%20recent%20advancements%20in%20contrastive%20language-image%20pre-training%20and%20low-rank%0Aadaptation%20%28LoRA%29%20methods%2C%20we%20aim%20to%20solve%20the%20grounding%20task%20based%20on%0Amultimodal%20pre-training.%20However%2C%20there%20exists%20significant%20task%20gaps%20between%0Apre-training%20and%20grounding.%20Therefore%2C%20to%20address%20these%20gaps%2C%20we%20propose%20a%0Aconcise%20and%20efficient%20hierarchical%20multimodal%20fine-grained%20modulation%0Aframework%2C%20namely%20HiVG.%20Specifically%2C%20HiVG%20consists%20of%20a%20multi-layer%20adaptive%0Across-modal%20bridge%20and%20a%20hierarchical%20multimodal%20low-rank%20adaptation%20%28HiLoRA%29%0Aparadigm.%20The%20cross-modal%20bridge%20can%20address%20the%20inconsistency%20between%20visual%0Afeatures%20and%20those%20required%20for%20grounding%2C%20and%20establish%20a%20connection%20between%0Amulti-level%20visual%20and%20text%20features.%20HiLoRA%20prevents%20the%20accumulation%20of%0Aperceptual%20errors%20by%20adapting%20the%20cross-modal%20features%20from%20shallow%20to%20deep%0Alayers%20in%20a%20hierarchical%20manner.%20Experimental%20results%20on%20five%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20and%20showcase%20the%20significant%0Agrounding%20capabilities%20as%20well%20as%20promising%20energy%20efficiency%20advantages.%20The%0Aproject%20page%3A%20https%3A//github.com/linhuixiao/HiVG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiVG%253A%2520Hierarchical%2520Multimodal%2520Fine-grained%2520Modulation%2520for%2520Visual%250A%2520%2520Grounding%26entry.906535625%3DLinhui%2520Xiao%2520and%2520Xiaoshan%2520Yang%2520and%2520Fang%2520Peng%2520and%2520Yaowei%2520Wang%2520and%2520Changsheng%2520Xu%26entry.1292438233%3D%2520%2520Visual%2520grounding%252C%2520which%2520aims%2520to%2520ground%2520a%2520visual%2520region%2520via%2520natural%2520language%252C%250Ais%2520a%2520task%2520that%2520heavily%2520relies%2520on%2520cross-modal%2520alignment.%2520Existing%2520works%2520utilized%250Auni-modal%2520pre-trained%2520models%2520to%2520transfer%2520visual%2520or%2520linguistic%2520knowledge%250Aseparately%2520while%2520ignoring%2520the%2520multimodal%2520corresponding%2520information.%2520Motivated%250Aby%2520recent%2520advancements%2520in%2520contrastive%2520language-image%2520pre-training%2520and%2520low-rank%250Aadaptation%2520%2528LoRA%2529%2520methods%252C%2520we%2520aim%2520to%2520solve%2520the%2520grounding%2520task%2520based%2520on%250Amultimodal%2520pre-training.%2520However%252C%2520there%2520exists%2520significant%2520task%2520gaps%2520between%250Apre-training%2520and%2520grounding.%2520Therefore%252C%2520to%2520address%2520these%2520gaps%252C%2520we%2520propose%2520a%250Aconcise%2520and%2520efficient%2520hierarchical%2520multimodal%2520fine-grained%2520modulation%250Aframework%252C%2520namely%2520HiVG.%2520Specifically%252C%2520HiVG%2520consists%2520of%2520a%2520multi-layer%2520adaptive%250Across-modal%2520bridge%2520and%2520a%2520hierarchical%2520multimodal%2520low-rank%2520adaptation%2520%2528HiLoRA%2529%250Aparadigm.%2520The%2520cross-modal%2520bridge%2520can%2520address%2520the%2520inconsistency%2520between%2520visual%250Afeatures%2520and%2520those%2520required%2520for%2520grounding%252C%2520and%2520establish%2520a%2520connection%2520between%250Amulti-level%2520visual%2520and%2520text%2520features.%2520HiLoRA%2520prevents%2520the%2520accumulation%2520of%250Aperceptual%2520errors%2520by%2520adapting%2520the%2520cross-modal%2520features%2520from%2520shallow%2520to%2520deep%250Alayers%2520in%2520a%2520hierarchical%2520manner.%2520Experimental%2520results%2520on%2520five%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520and%2520showcase%2520the%2520significant%250Agrounding%2520capabilities%2520as%2520well%2520as%2520promising%2520energy%2520efficiency%2520advantages.%2520The%250Aproject%2520page%253A%2520https%253A//github.com/linhuixiao/HiVG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiVG%3A%20Hierarchical%20Multimodal%20Fine-grained%20Modulation%20for%20Visual%0A%20%20Grounding&entry.906535625=Linhui%20Xiao%20and%20Xiaoshan%20Yang%20and%20Fang%20Peng%20and%20Yaowei%20Wang%20and%20Changsheng%20Xu&entry.1292438233=%20%20Visual%20grounding%2C%20which%20aims%20to%20ground%20a%20visual%20region%20via%20natural%20language%2C%0Ais%20a%20task%20that%20heavily%20relies%20on%20cross-modal%20alignment.%20Existing%20works%20utilized%0Auni-modal%20pre-trained%20models%20to%20transfer%20visual%20or%20linguistic%20knowledge%0Aseparately%20while%20ignoring%20the%20multimodal%20corresponding%20information.%20Motivated%0Aby%20recent%20advancements%20in%20contrastive%20language-image%20pre-training%20and%20low-rank%0Aadaptation%20%28LoRA%29%20methods%2C%20we%20aim%20to%20solve%20the%20grounding%20task%20based%20on%0Amultimodal%20pre-training.%20However%2C%20there%20exists%20significant%20task%20gaps%20between%0Apre-training%20and%20grounding.%20Therefore%2C%20to%20address%20these%20gaps%2C%20we%20propose%20a%0Aconcise%20and%20efficient%20hierarchical%20multimodal%20fine-grained%20modulation%0Aframework%2C%20namely%20HiVG.%20Specifically%2C%20HiVG%20consists%20of%20a%20multi-layer%20adaptive%0Across-modal%20bridge%20and%20a%20hierarchical%20multimodal%20low-rank%20adaptation%20%28HiLoRA%29%0Aparadigm.%20The%20cross-modal%20bridge%20can%20address%20the%20inconsistency%20between%20visual%0Afeatures%20and%20those%20required%20for%20grounding%2C%20and%20establish%20a%20connection%20between%0Amulti-level%20visual%20and%20text%20features.%20HiLoRA%20prevents%20the%20accumulation%20of%0Aperceptual%20errors%20by%20adapting%20the%20cross-modal%20features%20from%20shallow%20to%20deep%0Alayers%20in%20a%20hierarchical%20manner.%20Experimental%20results%20on%20five%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20and%20showcase%20the%20significant%0Agrounding%20capabilities%20as%20well%20as%20promising%20energy%20efficiency%20advantages.%20The%0Aproject%20page%3A%20https%3A//github.com/linhuixiao/HiVG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13400v2&entry.124074799=Read"},
{"title": "FrozenSeg: Harmonizing Frozen Foundation Models for Open-Vocabulary\n  Segmentation", "author": "Xi Chen and Haosen Yang and Sheng Jin and Xiatian Zhu and Hongxun Yao", "abstract": "  Open-vocabulary segmentation poses significant challenges, as it requires\nsegmenting and recognizing objects across an open set of categories in\nunconstrained environments. Building on the success of powerful vision-language\n(ViL) foundation models, such as CLIP, recent efforts sought to harness their\nzero-short capabilities to recognize unseen categories. Despite notable\nperformance improvements, these models still encounter the critical issue of\ngenerating precise mask proposals for unseen categories and scenarios,\nresulting in inferior segmentation performance eventually. To address this\nchallenge, we introduce a novel approach, FrozenSeg, designed to integrate\nspatial knowledge from a localization foundation model (e.g., SAM) and semantic\nknowledge extracted from a ViL model (e.g., CLIP), in a synergistic framework.\nTaking the ViL model's visual encoder as the feature backbone, we inject the\nspace-aware feature into the learnable queries and CLIP features within the\ntransformer decoder. In addition, we devise a mask proposal ensemble strategy\nfor further improving the recall rate and mask quality. To fully exploit\npre-trained knowledge while minimizing training overhead, we freeze both\nfoundation models, focusing optimization efforts solely on a lightweight\ntransformer decoder for mask proposal generation-the performance bottleneck.\nExtensive experiments demonstrate that FrozenSeg advances state-of-the-art\nresults across various segmentation benchmarks, trained exclusively on COCO\npanoptic data, and tested in a zero-shot manner. Code is available at\nhttps://github.com/chenxi52/FrozenSeg.\n", "link": "http://arxiv.org/abs/2409.03525v1", "date": "2024-09-05", "relevancy": 2.7174, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5371}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FrozenSeg%3A%20Harmonizing%20Frozen%20Foundation%20Models%20for%20Open-Vocabulary%0A%20%20Segmentation&body=Title%3A%20FrozenSeg%3A%20Harmonizing%20Frozen%20Foundation%20Models%20for%20Open-Vocabulary%0A%20%20Segmentation%0AAuthor%3A%20Xi%20Chen%20and%20Haosen%20Yang%20and%20Sheng%20Jin%20and%20Xiatian%20Zhu%20and%20Hongxun%20Yao%0AAbstract%3A%20%20%20Open-vocabulary%20segmentation%20poses%20significant%20challenges%2C%20as%20it%20requires%0Asegmenting%20and%20recognizing%20objects%20across%20an%20open%20set%20of%20categories%20in%0Aunconstrained%20environments.%20Building%20on%20the%20success%20of%20powerful%20vision-language%0A%28ViL%29%20foundation%20models%2C%20such%20as%20CLIP%2C%20recent%20efforts%20sought%20to%20harness%20their%0Azero-short%20capabilities%20to%20recognize%20unseen%20categories.%20Despite%20notable%0Aperformance%20improvements%2C%20these%20models%20still%20encounter%20the%20critical%20issue%20of%0Agenerating%20precise%20mask%20proposals%20for%20unseen%20categories%20and%20scenarios%2C%0Aresulting%20in%20inferior%20segmentation%20performance%20eventually.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20approach%2C%20FrozenSeg%2C%20designed%20to%20integrate%0Aspatial%20knowledge%20from%20a%20localization%20foundation%20model%20%28e.g.%2C%20SAM%29%20and%20semantic%0Aknowledge%20extracted%20from%20a%20ViL%20model%20%28e.g.%2C%20CLIP%29%2C%20in%20a%20synergistic%20framework.%0ATaking%20the%20ViL%20model%27s%20visual%20encoder%20as%20the%20feature%20backbone%2C%20we%20inject%20the%0Aspace-aware%20feature%20into%20the%20learnable%20queries%20and%20CLIP%20features%20within%20the%0Atransformer%20decoder.%20In%20addition%2C%20we%20devise%20a%20mask%20proposal%20ensemble%20strategy%0Afor%20further%20improving%20the%20recall%20rate%20and%20mask%20quality.%20To%20fully%20exploit%0Apre-trained%20knowledge%20while%20minimizing%20training%20overhead%2C%20we%20freeze%20both%0Afoundation%20models%2C%20focusing%20optimization%20efforts%20solely%20on%20a%20lightweight%0Atransformer%20decoder%20for%20mask%20proposal%20generation-the%20performance%20bottleneck.%0AExtensive%20experiments%20demonstrate%20that%20FrozenSeg%20advances%20state-of-the-art%0Aresults%20across%20various%20segmentation%20benchmarks%2C%20trained%20exclusively%20on%20COCO%0Apanoptic%20data%2C%20and%20tested%20in%20a%20zero-shot%20manner.%20Code%20is%20available%20at%0Ahttps%3A//github.com/chenxi52/FrozenSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrozenSeg%253A%2520Harmonizing%2520Frozen%2520Foundation%2520Models%2520for%2520Open-Vocabulary%250A%2520%2520Segmentation%26entry.906535625%3DXi%2520Chen%2520and%2520Haosen%2520Yang%2520and%2520Sheng%2520Jin%2520and%2520Xiatian%2520Zhu%2520and%2520Hongxun%2520Yao%26entry.1292438233%3D%2520%2520Open-vocabulary%2520segmentation%2520poses%2520significant%2520challenges%252C%2520as%2520it%2520requires%250Asegmenting%2520and%2520recognizing%2520objects%2520across%2520an%2520open%2520set%2520of%2520categories%2520in%250Aunconstrained%2520environments.%2520Building%2520on%2520the%2520success%2520of%2520powerful%2520vision-language%250A%2528ViL%2529%2520foundation%2520models%252C%2520such%2520as%2520CLIP%252C%2520recent%2520efforts%2520sought%2520to%2520harness%2520their%250Azero-short%2520capabilities%2520to%2520recognize%2520unseen%2520categories.%2520Despite%2520notable%250Aperformance%2520improvements%252C%2520these%2520models%2520still%2520encounter%2520the%2520critical%2520issue%2520of%250Agenerating%2520precise%2520mask%2520proposals%2520for%2520unseen%2520categories%2520and%2520scenarios%252C%250Aresulting%2520in%2520inferior%2520segmentation%2520performance%2520eventually.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520a%2520novel%2520approach%252C%2520FrozenSeg%252C%2520designed%2520to%2520integrate%250Aspatial%2520knowledge%2520from%2520a%2520localization%2520foundation%2520model%2520%2528e.g.%252C%2520SAM%2529%2520and%2520semantic%250Aknowledge%2520extracted%2520from%2520a%2520ViL%2520model%2520%2528e.g.%252C%2520CLIP%2529%252C%2520in%2520a%2520synergistic%2520framework.%250ATaking%2520the%2520ViL%2520model%2527s%2520visual%2520encoder%2520as%2520the%2520feature%2520backbone%252C%2520we%2520inject%2520the%250Aspace-aware%2520feature%2520into%2520the%2520learnable%2520queries%2520and%2520CLIP%2520features%2520within%2520the%250Atransformer%2520decoder.%2520In%2520addition%252C%2520we%2520devise%2520a%2520mask%2520proposal%2520ensemble%2520strategy%250Afor%2520further%2520improving%2520the%2520recall%2520rate%2520and%2520mask%2520quality.%2520To%2520fully%2520exploit%250Apre-trained%2520knowledge%2520while%2520minimizing%2520training%2520overhead%252C%2520we%2520freeze%2520both%250Afoundation%2520models%252C%2520focusing%2520optimization%2520efforts%2520solely%2520on%2520a%2520lightweight%250Atransformer%2520decoder%2520for%2520mask%2520proposal%2520generation-the%2520performance%2520bottleneck.%250AExtensive%2520experiments%2520demonstrate%2520that%2520FrozenSeg%2520advances%2520state-of-the-art%250Aresults%2520across%2520various%2520segmentation%2520benchmarks%252C%2520trained%2520exclusively%2520on%2520COCO%250Apanoptic%2520data%252C%2520and%2520tested%2520in%2520a%2520zero-shot%2520manner.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/chenxi52/FrozenSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FrozenSeg%3A%20Harmonizing%20Frozen%20Foundation%20Models%20for%20Open-Vocabulary%0A%20%20Segmentation&entry.906535625=Xi%20Chen%20and%20Haosen%20Yang%20and%20Sheng%20Jin%20and%20Xiatian%20Zhu%20and%20Hongxun%20Yao&entry.1292438233=%20%20Open-vocabulary%20segmentation%20poses%20significant%20challenges%2C%20as%20it%20requires%0Asegmenting%20and%20recognizing%20objects%20across%20an%20open%20set%20of%20categories%20in%0Aunconstrained%20environments.%20Building%20on%20the%20success%20of%20powerful%20vision-language%0A%28ViL%29%20foundation%20models%2C%20such%20as%20CLIP%2C%20recent%20efforts%20sought%20to%20harness%20their%0Azero-short%20capabilities%20to%20recognize%20unseen%20categories.%20Despite%20notable%0Aperformance%20improvements%2C%20these%20models%20still%20encounter%20the%20critical%20issue%20of%0Agenerating%20precise%20mask%20proposals%20for%20unseen%20categories%20and%20scenarios%2C%0Aresulting%20in%20inferior%20segmentation%20performance%20eventually.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20approach%2C%20FrozenSeg%2C%20designed%20to%20integrate%0Aspatial%20knowledge%20from%20a%20localization%20foundation%20model%20%28e.g.%2C%20SAM%29%20and%20semantic%0Aknowledge%20extracted%20from%20a%20ViL%20model%20%28e.g.%2C%20CLIP%29%2C%20in%20a%20synergistic%20framework.%0ATaking%20the%20ViL%20model%27s%20visual%20encoder%20as%20the%20feature%20backbone%2C%20we%20inject%20the%0Aspace-aware%20feature%20into%20the%20learnable%20queries%20and%20CLIP%20features%20within%20the%0Atransformer%20decoder.%20In%20addition%2C%20we%20devise%20a%20mask%20proposal%20ensemble%20strategy%0Afor%20further%20improving%20the%20recall%20rate%20and%20mask%20quality.%20To%20fully%20exploit%0Apre-trained%20knowledge%20while%20minimizing%20training%20overhead%2C%20we%20freeze%20both%0Afoundation%20models%2C%20focusing%20optimization%20efforts%20solely%20on%20a%20lightweight%0Atransformer%20decoder%20for%20mask%20proposal%20generation-the%20performance%20bottleneck.%0AExtensive%20experiments%20demonstrate%20that%20FrozenSeg%20advances%20state-of-the-art%0Aresults%20across%20various%20segmentation%20benchmarks%2C%20trained%20exclusively%20on%20COCO%0Apanoptic%20data%2C%20and%20tested%20in%20a%20zero-shot%20manner.%20Code%20is%20available%20at%0Ahttps%3A//github.com/chenxi52/FrozenSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03525v1&entry.124074799=Read"},
{"title": "A Key-Driven Framework for Identity-Preserving Face Anonymization", "author": "Miaomiao Wang and Guang Hua and Sheng Li and Guorui Feng", "abstract": "  Virtual faces are crucial content in the metaverse. Recently, attempts have\nbeen made to generate virtual faces for privacy protection. Nevertheless, these\nvirtual faces either permanently remove the identifiable information or map the\noriginal identity into a virtual one, which loses the original identity\nforever. In this study, we first attempt to address the conflict between\nprivacy and identifiability in virtual faces, where a key-driven face\nanonymization and authentication recognition (KFAAR) framework is proposed.\nConcretely, the KFAAR framework consists of a head posture-preserving virtual\nface generation (HPVFG) module and a key-controllable virtual face\nauthentication (KVFA) module. The HPVFG module uses a user key to project the\nlatent vector of the original face into a virtual one. Then it maps the virtual\nvectors to obtain an extended encoding, based on which the virtual face is\ngenerated. By simultaneously adding a head posture and facial expression\ncorrection module, the virtual face has the same head posture and facial\nexpression as the original face. During the authentication, we propose a KVFA\nmodule to directly recognize the virtual faces using the correct user key,\nwhich can obtain the original identity without exposing the original face\nimage. We also propose a multi-task learning objective to train HPVFG and KVFA.\nExtensive experiments demonstrate the advantages of the proposed HPVFG and KVFA\nmodules, which effectively achieve both facial anonymity and identifiability.\n", "link": "http://arxiv.org/abs/2409.03434v1", "date": "2024-09-05", "relevancy": 2.6954, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5661}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5491}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Key-Driven%20Framework%20for%20Identity-Preserving%20Face%20Anonymization&body=Title%3A%20A%20Key-Driven%20Framework%20for%20Identity-Preserving%20Face%20Anonymization%0AAuthor%3A%20Miaomiao%20Wang%20and%20Guang%20Hua%20and%20Sheng%20Li%20and%20Guorui%20Feng%0AAbstract%3A%20%20%20Virtual%20faces%20are%20crucial%20content%20in%20the%20metaverse.%20Recently%2C%20attempts%20have%0Abeen%20made%20to%20generate%20virtual%20faces%20for%20privacy%20protection.%20Nevertheless%2C%20these%0Avirtual%20faces%20either%20permanently%20remove%20the%20identifiable%20information%20or%20map%20the%0Aoriginal%20identity%20into%20a%20virtual%20one%2C%20which%20loses%20the%20original%20identity%0Aforever.%20In%20this%20study%2C%20we%20first%20attempt%20to%20address%20the%20conflict%20between%0Aprivacy%20and%20identifiability%20in%20virtual%20faces%2C%20where%20a%20key-driven%20face%0Aanonymization%20and%20authentication%20recognition%20%28KFAAR%29%20framework%20is%20proposed.%0AConcretely%2C%20the%20KFAAR%20framework%20consists%20of%20a%20head%20posture-preserving%20virtual%0Aface%20generation%20%28HPVFG%29%20module%20and%20a%20key-controllable%20virtual%20face%0Aauthentication%20%28KVFA%29%20module.%20The%20HPVFG%20module%20uses%20a%20user%20key%20to%20project%20the%0Alatent%20vector%20of%20the%20original%20face%20into%20a%20virtual%20one.%20Then%20it%20maps%20the%20virtual%0Avectors%20to%20obtain%20an%20extended%20encoding%2C%20based%20on%20which%20the%20virtual%20face%20is%0Agenerated.%20By%20simultaneously%20adding%20a%20head%20posture%20and%20facial%20expression%0Acorrection%20module%2C%20the%20virtual%20face%20has%20the%20same%20head%20posture%20and%20facial%0Aexpression%20as%20the%20original%20face.%20During%20the%20authentication%2C%20we%20propose%20a%20KVFA%0Amodule%20to%20directly%20recognize%20the%20virtual%20faces%20using%20the%20correct%20user%20key%2C%0Awhich%20can%20obtain%20the%20original%20identity%20without%20exposing%20the%20original%20face%0Aimage.%20We%20also%20propose%20a%20multi-task%20learning%20objective%20to%20train%20HPVFG%20and%20KVFA.%0AExtensive%20experiments%20demonstrate%20the%20advantages%20of%20the%20proposed%20HPVFG%20and%20KVFA%0Amodules%2C%20which%20effectively%20achieve%20both%20facial%20anonymity%20and%20identifiability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Key-Driven%2520Framework%2520for%2520Identity-Preserving%2520Face%2520Anonymization%26entry.906535625%3DMiaomiao%2520Wang%2520and%2520Guang%2520Hua%2520and%2520Sheng%2520Li%2520and%2520Guorui%2520Feng%26entry.1292438233%3D%2520%2520Virtual%2520faces%2520are%2520crucial%2520content%2520in%2520the%2520metaverse.%2520Recently%252C%2520attempts%2520have%250Abeen%2520made%2520to%2520generate%2520virtual%2520faces%2520for%2520privacy%2520protection.%2520Nevertheless%252C%2520these%250Avirtual%2520faces%2520either%2520permanently%2520remove%2520the%2520identifiable%2520information%2520or%2520map%2520the%250Aoriginal%2520identity%2520into%2520a%2520virtual%2520one%252C%2520which%2520loses%2520the%2520original%2520identity%250Aforever.%2520In%2520this%2520study%252C%2520we%2520first%2520attempt%2520to%2520address%2520the%2520conflict%2520between%250Aprivacy%2520and%2520identifiability%2520in%2520virtual%2520faces%252C%2520where%2520a%2520key-driven%2520face%250Aanonymization%2520and%2520authentication%2520recognition%2520%2528KFAAR%2529%2520framework%2520is%2520proposed.%250AConcretely%252C%2520the%2520KFAAR%2520framework%2520consists%2520of%2520a%2520head%2520posture-preserving%2520virtual%250Aface%2520generation%2520%2528HPVFG%2529%2520module%2520and%2520a%2520key-controllable%2520virtual%2520face%250Aauthentication%2520%2528KVFA%2529%2520module.%2520The%2520HPVFG%2520module%2520uses%2520a%2520user%2520key%2520to%2520project%2520the%250Alatent%2520vector%2520of%2520the%2520original%2520face%2520into%2520a%2520virtual%2520one.%2520Then%2520it%2520maps%2520the%2520virtual%250Avectors%2520to%2520obtain%2520an%2520extended%2520encoding%252C%2520based%2520on%2520which%2520the%2520virtual%2520face%2520is%250Agenerated.%2520By%2520simultaneously%2520adding%2520a%2520head%2520posture%2520and%2520facial%2520expression%250Acorrection%2520module%252C%2520the%2520virtual%2520face%2520has%2520the%2520same%2520head%2520posture%2520and%2520facial%250Aexpression%2520as%2520the%2520original%2520face.%2520During%2520the%2520authentication%252C%2520we%2520propose%2520a%2520KVFA%250Amodule%2520to%2520directly%2520recognize%2520the%2520virtual%2520faces%2520using%2520the%2520correct%2520user%2520key%252C%250Awhich%2520can%2520obtain%2520the%2520original%2520identity%2520without%2520exposing%2520the%2520original%2520face%250Aimage.%2520We%2520also%2520propose%2520a%2520multi-task%2520learning%2520objective%2520to%2520train%2520HPVFG%2520and%2520KVFA.%250AExtensive%2520experiments%2520demonstrate%2520the%2520advantages%2520of%2520the%2520proposed%2520HPVFG%2520and%2520KVFA%250Amodules%252C%2520which%2520effectively%2520achieve%2520both%2520facial%2520anonymity%2520and%2520identifiability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Key-Driven%20Framework%20for%20Identity-Preserving%20Face%20Anonymization&entry.906535625=Miaomiao%20Wang%20and%20Guang%20Hua%20and%20Sheng%20Li%20and%20Guorui%20Feng&entry.1292438233=%20%20Virtual%20faces%20are%20crucial%20content%20in%20the%20metaverse.%20Recently%2C%20attempts%20have%0Abeen%20made%20to%20generate%20virtual%20faces%20for%20privacy%20protection.%20Nevertheless%2C%20these%0Avirtual%20faces%20either%20permanently%20remove%20the%20identifiable%20information%20or%20map%20the%0Aoriginal%20identity%20into%20a%20virtual%20one%2C%20which%20loses%20the%20original%20identity%0Aforever.%20In%20this%20study%2C%20we%20first%20attempt%20to%20address%20the%20conflict%20between%0Aprivacy%20and%20identifiability%20in%20virtual%20faces%2C%20where%20a%20key-driven%20face%0Aanonymization%20and%20authentication%20recognition%20%28KFAAR%29%20framework%20is%20proposed.%0AConcretely%2C%20the%20KFAAR%20framework%20consists%20of%20a%20head%20posture-preserving%20virtual%0Aface%20generation%20%28HPVFG%29%20module%20and%20a%20key-controllable%20virtual%20face%0Aauthentication%20%28KVFA%29%20module.%20The%20HPVFG%20module%20uses%20a%20user%20key%20to%20project%20the%0Alatent%20vector%20of%20the%20original%20face%20into%20a%20virtual%20one.%20Then%20it%20maps%20the%20virtual%0Avectors%20to%20obtain%20an%20extended%20encoding%2C%20based%20on%20which%20the%20virtual%20face%20is%0Agenerated.%20By%20simultaneously%20adding%20a%20head%20posture%20and%20facial%20expression%0Acorrection%20module%2C%20the%20virtual%20face%20has%20the%20same%20head%20posture%20and%20facial%0Aexpression%20as%20the%20original%20face.%20During%20the%20authentication%2C%20we%20propose%20a%20KVFA%0Amodule%20to%20directly%20recognize%20the%20virtual%20faces%20using%20the%20correct%20user%20key%2C%0Awhich%20can%20obtain%20the%20original%20identity%20without%20exposing%20the%20original%20face%0Aimage.%20We%20also%20propose%20a%20multi-task%20learning%20objective%20to%20train%20HPVFG%20and%20KVFA.%0AExtensive%20experiments%20demonstrate%20the%20advantages%20of%20the%20proposed%20HPVFG%20and%20KVFA%0Amodules%2C%20which%20effectively%20achieve%20both%20facial%20anonymity%20and%20identifiability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03434v1&entry.124074799=Read"},
{"title": "Enhancing Facial Expression Recognition through Dual-Direction Attention\n  Mixed Feature Networks: Application to 7th ABAW Challenge", "author": "Josep Cabacas-Maso and Elena Ortega-Beltr\u00e1n and Ismael Benito-Altamirano and Carles Ventura", "abstract": "  We present our contribution to the 7th ABAW challenge at ECCV 2024, by\nutilizing a Dual-Direction Attention Mixed Feature Network (DDAMFN) for\nmultitask facial expression recognition, we achieve results far beyond the\nproposed baseline for the Multi-Task ABAW challenge. Our proposal uses the\nwell-known DDAMFN architecture as base to effectively predict valence-arousal,\nemotion recognition, and facial action units. We demonstrate the architecture\nability to handle these tasks simultaneously, providing insights into its\narchitecture and the rationale behind its design. Additionally, we compare our\nresults for a multitask solution with independent single-task performance.\n", "link": "http://arxiv.org/abs/2407.12390v3", "date": "2024-09-05", "relevancy": 2.6632, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5345}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Facial%20Expression%20Recognition%20through%20Dual-Direction%20Attention%0A%20%20Mixed%20Feature%20Networks%3A%20Application%20to%207th%20ABAW%20Challenge&body=Title%3A%20Enhancing%20Facial%20Expression%20Recognition%20through%20Dual-Direction%20Attention%0A%20%20Mixed%20Feature%20Networks%3A%20Application%20to%207th%20ABAW%20Challenge%0AAuthor%3A%20Josep%20Cabacas-Maso%20and%20Elena%20Ortega-Beltr%C3%A1n%20and%20Ismael%20Benito-Altamirano%20and%20Carles%20Ventura%0AAbstract%3A%20%20%20We%20present%20our%20contribution%20to%20the%207th%20ABAW%20challenge%20at%20ECCV%202024%2C%20by%0Autilizing%20a%20Dual-Direction%20Attention%20Mixed%20Feature%20Network%20%28DDAMFN%29%20for%0Amultitask%20facial%20expression%20recognition%2C%20we%20achieve%20results%20far%20beyond%20the%0Aproposed%20baseline%20for%20the%20Multi-Task%20ABAW%20challenge.%20Our%20proposal%20uses%20the%0Awell-known%20DDAMFN%20architecture%20as%20base%20to%20effectively%20predict%20valence-arousal%2C%0Aemotion%20recognition%2C%20and%20facial%20action%20units.%20We%20demonstrate%20the%20architecture%0Aability%20to%20handle%20these%20tasks%20simultaneously%2C%20providing%20insights%20into%20its%0Aarchitecture%20and%20the%20rationale%20behind%20its%20design.%20Additionally%2C%20we%20compare%20our%0Aresults%20for%20a%20multitask%20solution%20with%20independent%20single-task%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12390v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Facial%2520Expression%2520Recognition%2520through%2520Dual-Direction%2520Attention%250A%2520%2520Mixed%2520Feature%2520Networks%253A%2520Application%2520to%25207th%2520ABAW%2520Challenge%26entry.906535625%3DJosep%2520Cabacas-Maso%2520and%2520Elena%2520Ortega-Beltr%25C3%25A1n%2520and%2520Ismael%2520Benito-Altamirano%2520and%2520Carles%2520Ventura%26entry.1292438233%3D%2520%2520We%2520present%2520our%2520contribution%2520to%2520the%25207th%2520ABAW%2520challenge%2520at%2520ECCV%25202024%252C%2520by%250Autilizing%2520a%2520Dual-Direction%2520Attention%2520Mixed%2520Feature%2520Network%2520%2528DDAMFN%2529%2520for%250Amultitask%2520facial%2520expression%2520recognition%252C%2520we%2520achieve%2520results%2520far%2520beyond%2520the%250Aproposed%2520baseline%2520for%2520the%2520Multi-Task%2520ABAW%2520challenge.%2520Our%2520proposal%2520uses%2520the%250Awell-known%2520DDAMFN%2520architecture%2520as%2520base%2520to%2520effectively%2520predict%2520valence-arousal%252C%250Aemotion%2520recognition%252C%2520and%2520facial%2520action%2520units.%2520We%2520demonstrate%2520the%2520architecture%250Aability%2520to%2520handle%2520these%2520tasks%2520simultaneously%252C%2520providing%2520insights%2520into%2520its%250Aarchitecture%2520and%2520the%2520rationale%2520behind%2520its%2520design.%2520Additionally%252C%2520we%2520compare%2520our%250Aresults%2520for%2520a%2520multitask%2520solution%2520with%2520independent%2520single-task%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12390v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Facial%20Expression%20Recognition%20through%20Dual-Direction%20Attention%0A%20%20Mixed%20Feature%20Networks%3A%20Application%20to%207th%20ABAW%20Challenge&entry.906535625=Josep%20Cabacas-Maso%20and%20Elena%20Ortega-Beltr%C3%A1n%20and%20Ismael%20Benito-Altamirano%20and%20Carles%20Ventura&entry.1292438233=%20%20We%20present%20our%20contribution%20to%20the%207th%20ABAW%20challenge%20at%20ECCV%202024%2C%20by%0Autilizing%20a%20Dual-Direction%20Attention%20Mixed%20Feature%20Network%20%28DDAMFN%29%20for%0Amultitask%20facial%20expression%20recognition%2C%20we%20achieve%20results%20far%20beyond%20the%0Aproposed%20baseline%20for%20the%20Multi-Task%20ABAW%20challenge.%20Our%20proposal%20uses%20the%0Awell-known%20DDAMFN%20architecture%20as%20base%20to%20effectively%20predict%20valence-arousal%2C%0Aemotion%20recognition%2C%20and%20facial%20action%20units.%20We%20demonstrate%20the%20architecture%0Aability%20to%20handle%20these%20tasks%20simultaneously%2C%20providing%20insights%20into%20its%0Aarchitecture%20and%20the%20rationale%20behind%20its%20design.%20Additionally%2C%20we%20compare%20our%0Aresults%20for%20a%20multitask%20solution%20with%20independent%20single-task%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12390v3&entry.124074799=Read"},
{"title": "Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with\n  Image-Based Surface Representation", "author": "Slava Elizarov and Ciara Rowles and Simon Donn\u00e9", "abstract": "  Generating high-quality 3D objects from textual descriptions remains a\nchallenging problem due to computational cost, the scarcity of 3D data, and\ncomplex 3D representations. We introduce Geometry Image Diffusion\n(GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to\nefficiently represent 3D shapes using 2D images, thereby avoiding the need for\ncomplex 3D-aware architectures. By integrating a Collaborative Control\nmechanism, we exploit the rich 2D priors of existing Text-to-Image models such\nas Stable Diffusion. This enables strong generalization even with limited 3D\ntraining data (allowing us to use only high-quality training data) as well as\nretaining compatibility with guidance techniques such as IPAdapter. In short,\nGIMDiffusion enables the generation of 3D assets at speeds comparable to\ncurrent Text-to-Image models. The generated objects consist of semantically\nmeaningful, separate parts and include internal structures, enhancing both\nusability and versatility.\n", "link": "http://arxiv.org/abs/2409.03718v1", "date": "2024-09-05", "relevancy": 2.6521, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6709}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6709}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Image%20Diffusion%3A%20Fast%20and%20Data-Efficient%20Text-to-3D%20with%0A%20%20Image-Based%20Surface%20Representation&body=Title%3A%20Geometry%20Image%20Diffusion%3A%20Fast%20and%20Data-Efficient%20Text-to-3D%20with%0A%20%20Image-Based%20Surface%20Representation%0AAuthor%3A%20Slava%20Elizarov%20and%20Ciara%20Rowles%20and%20Simon%20Donn%C3%A9%0AAbstract%3A%20%20%20Generating%20high-quality%203D%20objects%20from%20textual%20descriptions%20remains%20a%0Achallenging%20problem%20due%20to%20computational%20cost%2C%20the%20scarcity%20of%203D%20data%2C%20and%0Acomplex%203D%20representations.%20We%20introduce%20Geometry%20Image%20Diffusion%0A%28GIMDiffusion%29%2C%20a%20novel%20Text-to-3D%20model%20that%20utilizes%20geometry%20images%20to%0Aefficiently%20represent%203D%20shapes%20using%202D%20images%2C%20thereby%20avoiding%20the%20need%20for%0Acomplex%203D-aware%20architectures.%20By%20integrating%20a%20Collaborative%20Control%0Amechanism%2C%20we%20exploit%20the%20rich%202D%20priors%20of%20existing%20Text-to-Image%20models%20such%0Aas%20Stable%20Diffusion.%20This%20enables%20strong%20generalization%20even%20with%20limited%203D%0Atraining%20data%20%28allowing%20us%20to%20use%20only%20high-quality%20training%20data%29%20as%20well%20as%0Aretaining%20compatibility%20with%20guidance%20techniques%20such%20as%20IPAdapter.%20In%20short%2C%0AGIMDiffusion%20enables%20the%20generation%20of%203D%20assets%20at%20speeds%20comparable%20to%0Acurrent%20Text-to-Image%20models.%20The%20generated%20objects%20consist%20of%20semantically%0Ameaningful%2C%20separate%20parts%20and%20include%20internal%20structures%2C%20enhancing%20both%0Ausability%20and%20versatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Image%2520Diffusion%253A%2520Fast%2520and%2520Data-Efficient%2520Text-to-3D%2520with%250A%2520%2520Image-Based%2520Surface%2520Representation%26entry.906535625%3DSlava%2520Elizarov%2520and%2520Ciara%2520Rowles%2520and%2520Simon%2520Donn%25C3%25A9%26entry.1292438233%3D%2520%2520Generating%2520high-quality%25203D%2520objects%2520from%2520textual%2520descriptions%2520remains%2520a%250Achallenging%2520problem%2520due%2520to%2520computational%2520cost%252C%2520the%2520scarcity%2520of%25203D%2520data%252C%2520and%250Acomplex%25203D%2520representations.%2520We%2520introduce%2520Geometry%2520Image%2520Diffusion%250A%2528GIMDiffusion%2529%252C%2520a%2520novel%2520Text-to-3D%2520model%2520that%2520utilizes%2520geometry%2520images%2520to%250Aefficiently%2520represent%25203D%2520shapes%2520using%25202D%2520images%252C%2520thereby%2520avoiding%2520the%2520need%2520for%250Acomplex%25203D-aware%2520architectures.%2520By%2520integrating%2520a%2520Collaborative%2520Control%250Amechanism%252C%2520we%2520exploit%2520the%2520rich%25202D%2520priors%2520of%2520existing%2520Text-to-Image%2520models%2520such%250Aas%2520Stable%2520Diffusion.%2520This%2520enables%2520strong%2520generalization%2520even%2520with%2520limited%25203D%250Atraining%2520data%2520%2528allowing%2520us%2520to%2520use%2520only%2520high-quality%2520training%2520data%2529%2520as%2520well%2520as%250Aretaining%2520compatibility%2520with%2520guidance%2520techniques%2520such%2520as%2520IPAdapter.%2520In%2520short%252C%250AGIMDiffusion%2520enables%2520the%2520generation%2520of%25203D%2520assets%2520at%2520speeds%2520comparable%2520to%250Acurrent%2520Text-to-Image%2520models.%2520The%2520generated%2520objects%2520consist%2520of%2520semantically%250Ameaningful%252C%2520separate%2520parts%2520and%2520include%2520internal%2520structures%252C%2520enhancing%2520both%250Ausability%2520and%2520versatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Image%20Diffusion%3A%20Fast%20and%20Data-Efficient%20Text-to-3D%20with%0A%20%20Image-Based%20Surface%20Representation&entry.906535625=Slava%20Elizarov%20and%20Ciara%20Rowles%20and%20Simon%20Donn%C3%A9&entry.1292438233=%20%20Generating%20high-quality%203D%20objects%20from%20textual%20descriptions%20remains%20a%0Achallenging%20problem%20due%20to%20computational%20cost%2C%20the%20scarcity%20of%203D%20data%2C%20and%0Acomplex%203D%20representations.%20We%20introduce%20Geometry%20Image%20Diffusion%0A%28GIMDiffusion%29%2C%20a%20novel%20Text-to-3D%20model%20that%20utilizes%20geometry%20images%20to%0Aefficiently%20represent%203D%20shapes%20using%202D%20images%2C%20thereby%20avoiding%20the%20need%20for%0Acomplex%203D-aware%20architectures.%20By%20integrating%20a%20Collaborative%20Control%0Amechanism%2C%20we%20exploit%20the%20rich%202D%20priors%20of%20existing%20Text-to-Image%20models%20such%0Aas%20Stable%20Diffusion.%20This%20enables%20strong%20generalization%20even%20with%20limited%203D%0Atraining%20data%20%28allowing%20us%20to%20use%20only%20high-quality%20training%20data%29%20as%20well%20as%0Aretaining%20compatibility%20with%20guidance%20techniques%20such%20as%20IPAdapter.%20In%20short%2C%0AGIMDiffusion%20enables%20the%20generation%20of%203D%20assets%20at%20speeds%20comparable%20to%0Acurrent%20Text-to-Image%20models.%20The%20generated%20objects%20consist%20of%20semantically%0Ameaningful%2C%20separate%20parts%20and%20include%20internal%20structures%2C%20enhancing%20both%0Ausability%20and%20versatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03718v1&entry.124074799=Read"},
{"title": "The representation landscape of few-shot learning and fine-tuning in\n  large language models", "author": "Diego Doimo and Alessandro Serra and Alessio Ansuini and Alberto Cazzaniga", "abstract": "  In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models.\n", "link": "http://arxiv.org/abs/2409.03662v1", "date": "2024-09-05", "relevancy": 2.6082, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5257}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20representation%20landscape%20of%20few-shot%20learning%20and%20fine-tuning%20in%0A%20%20large%20language%20models&body=Title%3A%20The%20representation%20landscape%20of%20few-shot%20learning%20and%20fine-tuning%20in%0A%20%20large%20language%20models%0AAuthor%3A%20Diego%20Doimo%20and%20Alessandro%20Serra%20and%20Alessio%20Ansuini%20and%20Alberto%20Cazzaniga%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20and%20supervised%20fine-tuning%20%28SFT%29%20are%20two%20common%0Astrategies%20for%20improving%20the%20performance%20of%20modern%20large%20language%20models%20%28LLMs%29%0Aon%20specific%20tasks.%20Despite%20their%20different%20natures%2C%20these%20strategies%20often%20lead%0Ato%20comparable%20performance%20gains.%20However%2C%20little%20is%20known%20about%20whether%20they%0Ainduce%20similar%20representations%20inside%20LLMs.%20We%20approach%20this%20problem%20by%0Aanalyzing%20the%20probability%20landscape%20of%20their%20hidden%20representations%20in%20the%20two%0Acases.%20More%20specifically%2C%20we%20compare%20how%20LLMs%20solve%20the%20same%20question-answering%0Atask%2C%20finding%20that%20ICL%20and%20SFT%20create%20very%20different%20internal%20structures%2C%20in%0Aboth%20cases%20undergoing%20a%20sharp%20transition%20in%20the%20middle%20of%20the%20network.%20In%20the%0Afirst%20half%20of%20the%20network%2C%20ICL%20shapes%20interpretable%20representations%0Ahierarchically%20organized%20according%20to%20their%20semantic%20content.%20In%20contrast%2C%20the%0Aprobability%20landscape%20obtained%20with%20SFT%20is%20fuzzier%20and%20semantically%20mixed.%20In%0Athe%20second%20half%20of%20the%20model%2C%20the%20fine-tuned%20representations%20develop%0Aprobability%20modes%20that%20better%20encode%20the%20identity%20of%20answers%2C%20while%20the%0Alandscape%20of%20ICL%20representations%20is%20characterized%20by%20less%20defined%20peaks.%20Our%0Aapproach%20reveals%20the%20diverse%20computational%20strategies%20developed%20inside%20LLMs%20to%0Asolve%20the%20same%20task%20across%20different%20conditions%2C%20allowing%20us%20to%20make%20a%20step%0Atowards%20designing%20optimal%20methods%20to%20extract%20information%20from%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520representation%2520landscape%2520of%2520few-shot%2520learning%2520and%2520fine-tuning%2520in%250A%2520%2520large%2520language%2520models%26entry.906535625%3DDiego%2520Doimo%2520and%2520Alessandro%2520Serra%2520and%2520Alessio%2520Ansuini%2520and%2520Alberto%2520Cazzaniga%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520and%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520are%2520two%2520common%250Astrategies%2520for%2520improving%2520the%2520performance%2520of%2520modern%2520large%2520language%2520models%2520%2528LLMs%2529%250Aon%2520specific%2520tasks.%2520Despite%2520their%2520different%2520natures%252C%2520these%2520strategies%2520often%2520lead%250Ato%2520comparable%2520performance%2520gains.%2520However%252C%2520little%2520is%2520known%2520about%2520whether%2520they%250Ainduce%2520similar%2520representations%2520inside%2520LLMs.%2520We%2520approach%2520this%2520problem%2520by%250Aanalyzing%2520the%2520probability%2520landscape%2520of%2520their%2520hidden%2520representations%2520in%2520the%2520two%250Acases.%2520More%2520specifically%252C%2520we%2520compare%2520how%2520LLMs%2520solve%2520the%2520same%2520question-answering%250Atask%252C%2520finding%2520that%2520ICL%2520and%2520SFT%2520create%2520very%2520different%2520internal%2520structures%252C%2520in%250Aboth%2520cases%2520undergoing%2520a%2520sharp%2520transition%2520in%2520the%2520middle%2520of%2520the%2520network.%2520In%2520the%250Afirst%2520half%2520of%2520the%2520network%252C%2520ICL%2520shapes%2520interpretable%2520representations%250Ahierarchically%2520organized%2520according%2520to%2520their%2520semantic%2520content.%2520In%2520contrast%252C%2520the%250Aprobability%2520landscape%2520obtained%2520with%2520SFT%2520is%2520fuzzier%2520and%2520semantically%2520mixed.%2520In%250Athe%2520second%2520half%2520of%2520the%2520model%252C%2520the%2520fine-tuned%2520representations%2520develop%250Aprobability%2520modes%2520that%2520better%2520encode%2520the%2520identity%2520of%2520answers%252C%2520while%2520the%250Alandscape%2520of%2520ICL%2520representations%2520is%2520characterized%2520by%2520less%2520defined%2520peaks.%2520Our%250Aapproach%2520reveals%2520the%2520diverse%2520computational%2520strategies%2520developed%2520inside%2520LLMs%2520to%250Asolve%2520the%2520same%2520task%2520across%2520different%2520conditions%252C%2520allowing%2520us%2520to%2520make%2520a%2520step%250Atowards%2520designing%2520optimal%2520methods%2520to%2520extract%2520information%2520from%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20representation%20landscape%20of%20few-shot%20learning%20and%20fine-tuning%20in%0A%20%20large%20language%20models&entry.906535625=Diego%20Doimo%20and%20Alessandro%20Serra%20and%20Alessio%20Ansuini%20and%20Alberto%20Cazzaniga&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20and%20supervised%20fine-tuning%20%28SFT%29%20are%20two%20common%0Astrategies%20for%20improving%20the%20performance%20of%20modern%20large%20language%20models%20%28LLMs%29%0Aon%20specific%20tasks.%20Despite%20their%20different%20natures%2C%20these%20strategies%20often%20lead%0Ato%20comparable%20performance%20gains.%20However%2C%20little%20is%20known%20about%20whether%20they%0Ainduce%20similar%20representations%20inside%20LLMs.%20We%20approach%20this%20problem%20by%0Aanalyzing%20the%20probability%20landscape%20of%20their%20hidden%20representations%20in%20the%20two%0Acases.%20More%20specifically%2C%20we%20compare%20how%20LLMs%20solve%20the%20same%20question-answering%0Atask%2C%20finding%20that%20ICL%20and%20SFT%20create%20very%20different%20internal%20structures%2C%20in%0Aboth%20cases%20undergoing%20a%20sharp%20transition%20in%20the%20middle%20of%20the%20network.%20In%20the%0Afirst%20half%20of%20the%20network%2C%20ICL%20shapes%20interpretable%20representations%0Ahierarchically%20organized%20according%20to%20their%20semantic%20content.%20In%20contrast%2C%20the%0Aprobability%20landscape%20obtained%20with%20SFT%20is%20fuzzier%20and%20semantically%20mixed.%20In%0Athe%20second%20half%20of%20the%20model%2C%20the%20fine-tuned%20representations%20develop%0Aprobability%20modes%20that%20better%20encode%20the%20identity%20of%20answers%2C%20while%20the%0Alandscape%20of%20ICL%20representations%20is%20characterized%20by%20less%20defined%20peaks.%20Our%0Aapproach%20reveals%20the%20diverse%20computational%20strategies%20developed%20inside%20LLMs%20to%0Asolve%20the%20same%20task%20across%20different%20conditions%2C%20allowing%20us%20to%20make%20a%20step%0Atowards%20designing%20optimal%20methods%20to%20extract%20information%20from%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03662v1&entry.124074799=Read"},
{"title": "TCDiff: Triple Condition Diffusion Model with 3D Constraints for\n  Stylizing Synthetic Faces", "author": "Bernardo Biesseck and Pedro Vidal and Luiz Coelho and Roger Granada and David Menotti|", "abstract": "  A robust face recognition model must be trained using datasets that include a\nlarge number of subjects and numerous samples per subject under varying\nconditions (such as pose, expression, age, noise, and occlusion). Due to\nethical and privacy concerns, large-scale real face datasets have been\ndiscontinued, such as MS1MV3, and synthetic face generators have been proposed,\nutilizing GANs and Diffusion Models, such as SYNFace, SFace, DigiFace-1M,\nIDiff-Face, DCFace, and GANDiffFace, aiming to supply this demand. Some of\nthese methods can produce high-fidelity realistic faces, but with low\nintra-class variance, while others generate high-variance faces with low\nidentity consistency. In this paper, we propose a Triple Condition Diffusion\nModel (TCDiff) to improve face style transfer from real to synthetic faces\nthrough 2D and 3D facial constraints, enhancing face identity consistency while\nkeeping the necessary high intra-class variance. Face recognition experiments\nusing 1k, 2k, and 5k classes of our new dataset for training outperform\nstate-of-the-art synthetic datasets in real face benchmarks such as LFW,\nCFP-FP, AgeDB, and BUPT. Our source code is available at:\nhttps://github.com/BOVIFOCR/tcdiff.\n", "link": "http://arxiv.org/abs/2409.03600v1", "date": "2024-09-05", "relevancy": 2.5726, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6503}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TCDiff%3A%20Triple%20Condition%20Diffusion%20Model%20with%203D%20Constraints%20for%0A%20%20Stylizing%20Synthetic%20Faces&body=Title%3A%20TCDiff%3A%20Triple%20Condition%20Diffusion%20Model%20with%203D%20Constraints%20for%0A%20%20Stylizing%20Synthetic%20Faces%0AAuthor%3A%20Bernardo%20Biesseck%20and%20Pedro%20Vidal%20and%20Luiz%20Coelho%20and%20Roger%20Granada%20and%20David%20Menotti%7C%0AAbstract%3A%20%20%20A%20robust%20face%20recognition%20model%20must%20be%20trained%20using%20datasets%20that%20include%20a%0Alarge%20number%20of%20subjects%20and%20numerous%20samples%20per%20subject%20under%20varying%0Aconditions%20%28such%20as%20pose%2C%20expression%2C%20age%2C%20noise%2C%20and%20occlusion%29.%20Due%20to%0Aethical%20and%20privacy%20concerns%2C%20large-scale%20real%20face%20datasets%20have%20been%0Adiscontinued%2C%20such%20as%20MS1MV3%2C%20and%20synthetic%20face%20generators%20have%20been%20proposed%2C%0Autilizing%20GANs%20and%20Diffusion%20Models%2C%20such%20as%20SYNFace%2C%20SFace%2C%20DigiFace-1M%2C%0AIDiff-Face%2C%20DCFace%2C%20and%20GANDiffFace%2C%20aiming%20to%20supply%20this%20demand.%20Some%20of%0Athese%20methods%20can%20produce%20high-fidelity%20realistic%20faces%2C%20but%20with%20low%0Aintra-class%20variance%2C%20while%20others%20generate%20high-variance%20faces%20with%20low%0Aidentity%20consistency.%20In%20this%20paper%2C%20we%20propose%20a%20Triple%20Condition%20Diffusion%0AModel%20%28TCDiff%29%20to%20improve%20face%20style%20transfer%20from%20real%20to%20synthetic%20faces%0Athrough%202D%20and%203D%20facial%20constraints%2C%20enhancing%20face%20identity%20consistency%20while%0Akeeping%20the%20necessary%20high%20intra-class%20variance.%20Face%20recognition%20experiments%0Ausing%201k%2C%202k%2C%20and%205k%20classes%20of%20our%20new%20dataset%20for%20training%20outperform%0Astate-of-the-art%20synthetic%20datasets%20in%20real%20face%20benchmarks%20such%20as%20LFW%2C%0ACFP-FP%2C%20AgeDB%2C%20and%20BUPT.%20Our%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/BOVIFOCR/tcdiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTCDiff%253A%2520Triple%2520Condition%2520Diffusion%2520Model%2520with%25203D%2520Constraints%2520for%250A%2520%2520Stylizing%2520Synthetic%2520Faces%26entry.906535625%3DBernardo%2520Biesseck%2520and%2520Pedro%2520Vidal%2520and%2520Luiz%2520Coelho%2520and%2520Roger%2520Granada%2520and%2520David%2520Menotti%257C%26entry.1292438233%3D%2520%2520A%2520robust%2520face%2520recognition%2520model%2520must%2520be%2520trained%2520using%2520datasets%2520that%2520include%2520a%250Alarge%2520number%2520of%2520subjects%2520and%2520numerous%2520samples%2520per%2520subject%2520under%2520varying%250Aconditions%2520%2528such%2520as%2520pose%252C%2520expression%252C%2520age%252C%2520noise%252C%2520and%2520occlusion%2529.%2520Due%2520to%250Aethical%2520and%2520privacy%2520concerns%252C%2520large-scale%2520real%2520face%2520datasets%2520have%2520been%250Adiscontinued%252C%2520such%2520as%2520MS1MV3%252C%2520and%2520synthetic%2520face%2520generators%2520have%2520been%2520proposed%252C%250Autilizing%2520GANs%2520and%2520Diffusion%2520Models%252C%2520such%2520as%2520SYNFace%252C%2520SFace%252C%2520DigiFace-1M%252C%250AIDiff-Face%252C%2520DCFace%252C%2520and%2520GANDiffFace%252C%2520aiming%2520to%2520supply%2520this%2520demand.%2520Some%2520of%250Athese%2520methods%2520can%2520produce%2520high-fidelity%2520realistic%2520faces%252C%2520but%2520with%2520low%250Aintra-class%2520variance%252C%2520while%2520others%2520generate%2520high-variance%2520faces%2520with%2520low%250Aidentity%2520consistency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Triple%2520Condition%2520Diffusion%250AModel%2520%2528TCDiff%2529%2520to%2520improve%2520face%2520style%2520transfer%2520from%2520real%2520to%2520synthetic%2520faces%250Athrough%25202D%2520and%25203D%2520facial%2520constraints%252C%2520enhancing%2520face%2520identity%2520consistency%2520while%250Akeeping%2520the%2520necessary%2520high%2520intra-class%2520variance.%2520Face%2520recognition%2520experiments%250Ausing%25201k%252C%25202k%252C%2520and%25205k%2520classes%2520of%2520our%2520new%2520dataset%2520for%2520training%2520outperform%250Astate-of-the-art%2520synthetic%2520datasets%2520in%2520real%2520face%2520benchmarks%2520such%2520as%2520LFW%252C%250ACFP-FP%252C%2520AgeDB%252C%2520and%2520BUPT.%2520Our%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/BOVIFOCR/tcdiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCDiff%3A%20Triple%20Condition%20Diffusion%20Model%20with%203D%20Constraints%20for%0A%20%20Stylizing%20Synthetic%20Faces&entry.906535625=Bernardo%20Biesseck%20and%20Pedro%20Vidal%20and%20Luiz%20Coelho%20and%20Roger%20Granada%20and%20David%20Menotti%7C&entry.1292438233=%20%20A%20robust%20face%20recognition%20model%20must%20be%20trained%20using%20datasets%20that%20include%20a%0Alarge%20number%20of%20subjects%20and%20numerous%20samples%20per%20subject%20under%20varying%0Aconditions%20%28such%20as%20pose%2C%20expression%2C%20age%2C%20noise%2C%20and%20occlusion%29.%20Due%20to%0Aethical%20and%20privacy%20concerns%2C%20large-scale%20real%20face%20datasets%20have%20been%0Adiscontinued%2C%20such%20as%20MS1MV3%2C%20and%20synthetic%20face%20generators%20have%20been%20proposed%2C%0Autilizing%20GANs%20and%20Diffusion%20Models%2C%20such%20as%20SYNFace%2C%20SFace%2C%20DigiFace-1M%2C%0AIDiff-Face%2C%20DCFace%2C%20and%20GANDiffFace%2C%20aiming%20to%20supply%20this%20demand.%20Some%20of%0Athese%20methods%20can%20produce%20high-fidelity%20realistic%20faces%2C%20but%20with%20low%0Aintra-class%20variance%2C%20while%20others%20generate%20high-variance%20faces%20with%20low%0Aidentity%20consistency.%20In%20this%20paper%2C%20we%20propose%20a%20Triple%20Condition%20Diffusion%0AModel%20%28TCDiff%29%20to%20improve%20face%20style%20transfer%20from%20real%20to%20synthetic%20faces%0Athrough%202D%20and%203D%20facial%20constraints%2C%20enhancing%20face%20identity%20consistency%20while%0Akeeping%20the%20necessary%20high%20intra-class%20variance.%20Face%20recognition%20experiments%0Ausing%201k%2C%202k%2C%20and%205k%20classes%20of%20our%20new%20dataset%20for%20training%20outperform%0Astate-of-the-art%20synthetic%20datasets%20in%20real%20face%20benchmarks%20such%20as%20LFW%2C%0ACFP-FP%2C%20AgeDB%2C%20and%20BUPT.%20Our%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/BOVIFOCR/tcdiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03600v1&entry.124074799=Read"},
{"title": "Unsupervised Anomaly Detection and Localization with Generative\n  Adversarial Networks", "author": "Khouloud Abdelli and Matteo Lonardi and Jurgen Gripp and Samuel Olsson and Fabien Boitier and Patricia Layec", "abstract": "  We propose a novel unsupervised anomaly detection approach using generative\nadversarial networks and SOP-derived spectrograms. Demonstrating remarkable\nefficacy, our method achieves over 97% accuracy on SOP datasets from both\nsubmarine and terrestrial fiber links, all achieved without the need for\nlabelled data.\n", "link": "http://arxiv.org/abs/2409.03657v1", "date": "2024-09-05", "relevancy": 2.5087, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5066}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5021}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Anomaly%20Detection%20and%20Localization%20with%20Generative%0A%20%20Adversarial%20Networks&body=Title%3A%20Unsupervised%20Anomaly%20Detection%20and%20Localization%20with%20Generative%0A%20%20Adversarial%20Networks%0AAuthor%3A%20Khouloud%20Abdelli%20and%20Matteo%20Lonardi%20and%20Jurgen%20Gripp%20and%20Samuel%20Olsson%20and%20Fabien%20Boitier%20and%20Patricia%20Layec%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20unsupervised%20anomaly%20detection%20approach%20using%20generative%0Aadversarial%20networks%20and%20SOP-derived%20spectrograms.%20Demonstrating%20remarkable%0Aefficacy%2C%20our%20method%20achieves%20over%2097%25%20accuracy%20on%20SOP%20datasets%20from%20both%0Asubmarine%20and%20terrestrial%20fiber%20links%2C%20all%20achieved%20without%20the%20need%20for%0Alabelled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Anomaly%2520Detection%2520and%2520Localization%2520with%2520Generative%250A%2520%2520Adversarial%2520Networks%26entry.906535625%3DKhouloud%2520Abdelli%2520and%2520Matteo%2520Lonardi%2520and%2520Jurgen%2520Gripp%2520and%2520Samuel%2520Olsson%2520and%2520Fabien%2520Boitier%2520and%2520Patricia%2520Layec%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520unsupervised%2520anomaly%2520detection%2520approach%2520using%2520generative%250Aadversarial%2520networks%2520and%2520SOP-derived%2520spectrograms.%2520Demonstrating%2520remarkable%250Aefficacy%252C%2520our%2520method%2520achieves%2520over%252097%2525%2520accuracy%2520on%2520SOP%2520datasets%2520from%2520both%250Asubmarine%2520and%2520terrestrial%2520fiber%2520links%252C%2520all%2520achieved%2520without%2520the%2520need%2520for%250Alabelled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Anomaly%20Detection%20and%20Localization%20with%20Generative%0A%20%20Adversarial%20Networks&entry.906535625=Khouloud%20Abdelli%20and%20Matteo%20Lonardi%20and%20Jurgen%20Gripp%20and%20Samuel%20Olsson%20and%20Fabien%20Boitier%20and%20Patricia%20Layec&entry.1292438233=%20%20We%20propose%20a%20novel%20unsupervised%20anomaly%20detection%20approach%20using%20generative%0Aadversarial%20networks%20and%20SOP-derived%20spectrograms.%20Demonstrating%20remarkable%0Aefficacy%2C%20our%20method%20achieves%20over%2097%25%20accuracy%20on%20SOP%20datasets%20from%20both%0Asubmarine%20and%20terrestrial%20fiber%20links%2C%20all%20achieved%20without%20the%20need%20for%0Alabelled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03657v1&entry.124074799=Read"},
{"title": "GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing\n  Patterns", "author": "Maria Korosteleva and Timur Levent Kesdogan and Fabian Kemper and Stephan Wenninger and Jasmin Koller and Yuhan Zhang and Mario Botsch and Olga Sorkine-Hornung", "abstract": "  Recent research interest in the learning-based processing of garments, from\nvirtual fitting to generation and reconstruction, stumbles on a scarcity of\nhigh-quality public data in the domain. We contribute to resolving this need by\npresenting the first large-scale synthetic dataset of 3D made-to-measure\ngarments with sewing patterns, as well as its generation pipeline.\nGarmentCodeData contains 115,000 data points that cover a variety of designs in\nmany common garment categories: tops, shirts, dresses, jumpsuits, skirts,\npants, etc., fitted to a variety of body shapes sampled from a custom\nstatistical body model based on CAESAR, as well as a standard reference body\nshape, applying three different textile materials. To enable the creation of\ndatasets of such complexity, we introduce a set of algorithms for automatically\ntaking tailor's measures on sampled body shapes, sampling strategies for sewing\npattern design, and propose an automatic, open-source 3D garment draping\npipeline based on a fast XPBD simulator, while contributing several solutions\nfor collision resolution and drape correctness to enable scalability.\n  Project Page: https://igl.ethz.ch/projects/GarmentCodeData/\n", "link": "http://arxiv.org/abs/2405.17609v3", "date": "2024-09-05", "relevancy": 2.4782, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7725}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5586}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarmentCodeData%3A%20A%20Dataset%20of%203D%20Made-to-Measure%20Garments%20With%20Sewing%0A%20%20Patterns&body=Title%3A%20GarmentCodeData%3A%20A%20Dataset%20of%203D%20Made-to-Measure%20Garments%20With%20Sewing%0A%20%20Patterns%0AAuthor%3A%20Maria%20Korosteleva%20and%20Timur%20Levent%20Kesdogan%20and%20Fabian%20Kemper%20and%20Stephan%20Wenninger%20and%20Jasmin%20Koller%20and%20Yuhan%20Zhang%20and%20Mario%20Botsch%20and%20Olga%20Sorkine-Hornung%0AAbstract%3A%20%20%20Recent%20research%20interest%20in%20the%20learning-based%20processing%20of%20garments%2C%20from%0Avirtual%20fitting%20to%20generation%20and%20reconstruction%2C%20stumbles%20on%20a%20scarcity%20of%0Ahigh-quality%20public%20data%20in%20the%20domain.%20We%20contribute%20to%20resolving%20this%20need%20by%0Apresenting%20the%20first%20large-scale%20synthetic%20dataset%20of%203D%20made-to-measure%0Agarments%20with%20sewing%20patterns%2C%20as%20well%20as%20its%20generation%20pipeline.%0AGarmentCodeData%20contains%20115%2C000%20data%20points%20that%20cover%20a%20variety%20of%20designs%20in%0Amany%20common%20garment%20categories%3A%20tops%2C%20shirts%2C%20dresses%2C%20jumpsuits%2C%20skirts%2C%0Apants%2C%20etc.%2C%20fitted%20to%20a%20variety%20of%20body%20shapes%20sampled%20from%20a%20custom%0Astatistical%20body%20model%20based%20on%20CAESAR%2C%20as%20well%20as%20a%20standard%20reference%20body%0Ashape%2C%20applying%20three%20different%20textile%20materials.%20To%20enable%20the%20creation%20of%0Adatasets%20of%20such%20complexity%2C%20we%20introduce%20a%20set%20of%20algorithms%20for%20automatically%0Ataking%20tailor%27s%20measures%20on%20sampled%20body%20shapes%2C%20sampling%20strategies%20for%20sewing%0Apattern%20design%2C%20and%20propose%20an%20automatic%2C%20open-source%203D%20garment%20draping%0Apipeline%20based%20on%20a%20fast%20XPBD%20simulator%2C%20while%20contributing%20several%20solutions%0Afor%20collision%20resolution%20and%20drape%20correctness%20to%20enable%20scalability.%0A%20%20Project%20Page%3A%20https%3A//igl.ethz.ch/projects/GarmentCodeData/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17609v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarmentCodeData%253A%2520A%2520Dataset%2520of%25203D%2520Made-to-Measure%2520Garments%2520With%2520Sewing%250A%2520%2520Patterns%26entry.906535625%3DMaria%2520Korosteleva%2520and%2520Timur%2520Levent%2520Kesdogan%2520and%2520Fabian%2520Kemper%2520and%2520Stephan%2520Wenninger%2520and%2520Jasmin%2520Koller%2520and%2520Yuhan%2520Zhang%2520and%2520Mario%2520Botsch%2520and%2520Olga%2520Sorkine-Hornung%26entry.1292438233%3D%2520%2520Recent%2520research%2520interest%2520in%2520the%2520learning-based%2520processing%2520of%2520garments%252C%2520from%250Avirtual%2520fitting%2520to%2520generation%2520and%2520reconstruction%252C%2520stumbles%2520on%2520a%2520scarcity%2520of%250Ahigh-quality%2520public%2520data%2520in%2520the%2520domain.%2520We%2520contribute%2520to%2520resolving%2520this%2520need%2520by%250Apresenting%2520the%2520first%2520large-scale%2520synthetic%2520dataset%2520of%25203D%2520made-to-measure%250Agarments%2520with%2520sewing%2520patterns%252C%2520as%2520well%2520as%2520its%2520generation%2520pipeline.%250AGarmentCodeData%2520contains%2520115%252C000%2520data%2520points%2520that%2520cover%2520a%2520variety%2520of%2520designs%2520in%250Amany%2520common%2520garment%2520categories%253A%2520tops%252C%2520shirts%252C%2520dresses%252C%2520jumpsuits%252C%2520skirts%252C%250Apants%252C%2520etc.%252C%2520fitted%2520to%2520a%2520variety%2520of%2520body%2520shapes%2520sampled%2520from%2520a%2520custom%250Astatistical%2520body%2520model%2520based%2520on%2520CAESAR%252C%2520as%2520well%2520as%2520a%2520standard%2520reference%2520body%250Ashape%252C%2520applying%2520three%2520different%2520textile%2520materials.%2520To%2520enable%2520the%2520creation%2520of%250Adatasets%2520of%2520such%2520complexity%252C%2520we%2520introduce%2520a%2520set%2520of%2520algorithms%2520for%2520automatically%250Ataking%2520tailor%2527s%2520measures%2520on%2520sampled%2520body%2520shapes%252C%2520sampling%2520strategies%2520for%2520sewing%250Apattern%2520design%252C%2520and%2520propose%2520an%2520automatic%252C%2520open-source%25203D%2520garment%2520draping%250Apipeline%2520based%2520on%2520a%2520fast%2520XPBD%2520simulator%252C%2520while%2520contributing%2520several%2520solutions%250Afor%2520collision%2520resolution%2520and%2520drape%2520correctness%2520to%2520enable%2520scalability.%250A%2520%2520Project%2520Page%253A%2520https%253A//igl.ethz.ch/projects/GarmentCodeData/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17609v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarmentCodeData%3A%20A%20Dataset%20of%203D%20Made-to-Measure%20Garments%20With%20Sewing%0A%20%20Patterns&entry.906535625=Maria%20Korosteleva%20and%20Timur%20Levent%20Kesdogan%20and%20Fabian%20Kemper%20and%20Stephan%20Wenninger%20and%20Jasmin%20Koller%20and%20Yuhan%20Zhang%20and%20Mario%20Botsch%20and%20Olga%20Sorkine-Hornung&entry.1292438233=%20%20Recent%20research%20interest%20in%20the%20learning-based%20processing%20of%20garments%2C%20from%0Avirtual%20fitting%20to%20generation%20and%20reconstruction%2C%20stumbles%20on%20a%20scarcity%20of%0Ahigh-quality%20public%20data%20in%20the%20domain.%20We%20contribute%20to%20resolving%20this%20need%20by%0Apresenting%20the%20first%20large-scale%20synthetic%20dataset%20of%203D%20made-to-measure%0Agarments%20with%20sewing%20patterns%2C%20as%20well%20as%20its%20generation%20pipeline.%0AGarmentCodeData%20contains%20115%2C000%20data%20points%20that%20cover%20a%20variety%20of%20designs%20in%0Amany%20common%20garment%20categories%3A%20tops%2C%20shirts%2C%20dresses%2C%20jumpsuits%2C%20skirts%2C%0Apants%2C%20etc.%2C%20fitted%20to%20a%20variety%20of%20body%20shapes%20sampled%20from%20a%20custom%0Astatistical%20body%20model%20based%20on%20CAESAR%2C%20as%20well%20as%20a%20standard%20reference%20body%0Ashape%2C%20applying%20three%20different%20textile%20materials.%20To%20enable%20the%20creation%20of%0Adatasets%20of%20such%20complexity%2C%20we%20introduce%20a%20set%20of%20algorithms%20for%20automatically%0Ataking%20tailor%27s%20measures%20on%20sampled%20body%20shapes%2C%20sampling%20strategies%20for%20sewing%0Apattern%20design%2C%20and%20propose%20an%20automatic%2C%20open-source%203D%20garment%20draping%0Apipeline%20based%20on%20a%20fast%20XPBD%20simulator%2C%20while%20contributing%20several%20solutions%0Afor%20collision%20resolution%20and%20drape%20correctness%20to%20enable%20scalability.%0A%20%20Project%20Page%3A%20https%3A//igl.ethz.ch/projects/GarmentCodeData/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17609v3&entry.124074799=Read"},
{"title": "FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image", "author": "Xiang Feng and Chengkai Wang and Chengyu Wu and Yunxiang Li and Yongbo He and Shuai Wang and Yaiqi Wang", "abstract": "  Precise Tooth Cone Beam Computed Tomography (CBCT) image segmentation is\ncrucial for orthodontic treatment planning. In this paper, we propose FDNet, a\nFeature Decoupled Segmentation Network, to excel in the face of the variable\ndental conditions encountered in CBCT scans, such as complex artifacts and\nindistinct tooth boundaries. The Low-Frequency Wavelet Transform (LF-Wavelet)\nis employed to enrich the semantic content by emphasizing the global structural\nintegrity of the teeth, while the SAM encoder is leveraged to refine the\nboundary delineation, thus improving the contrast between adjacent dental\nstructures. By integrating these dual aspects, FDNet adeptly addresses the\nsemantic gap, providing a detailed and accurate segmentation. The framework's\neffectiveness is validated through rigorous benchmarks, achieving the top Dice\nand IoU scores of 85.28% and 75.23%, respectively. This innovative decoupling\nof semantic and boundary features capitalizes on the unique strengths of each\nelement to elevate the quality of segmentation performance.\n", "link": "http://arxiv.org/abs/2311.06551v2", "date": "2024-09-05", "relevancy": 2.4636, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5135}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4848}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FDNet%3A%20Feature%20Decoupled%20Segmentation%20Network%20for%20Tooth%20CBCT%20Image&body=Title%3A%20FDNet%3A%20Feature%20Decoupled%20Segmentation%20Network%20for%20Tooth%20CBCT%20Image%0AAuthor%3A%20Xiang%20Feng%20and%20Chengkai%20Wang%20and%20Chengyu%20Wu%20and%20Yunxiang%20Li%20and%20Yongbo%20He%20and%20Shuai%20Wang%20and%20Yaiqi%20Wang%0AAbstract%3A%20%20%20Precise%20Tooth%20Cone%20Beam%20Computed%20Tomography%20%28CBCT%29%20image%20segmentation%20is%0Acrucial%20for%20orthodontic%20treatment%20planning.%20In%20this%20paper%2C%20we%20propose%20FDNet%2C%20a%0AFeature%20Decoupled%20Segmentation%20Network%2C%20to%20excel%20in%20the%20face%20of%20the%20variable%0Adental%20conditions%20encountered%20in%20CBCT%20scans%2C%20such%20as%20complex%20artifacts%20and%0Aindistinct%20tooth%20boundaries.%20The%20Low-Frequency%20Wavelet%20Transform%20%28LF-Wavelet%29%0Ais%20employed%20to%20enrich%20the%20semantic%20content%20by%20emphasizing%20the%20global%20structural%0Aintegrity%20of%20the%20teeth%2C%20while%20the%20SAM%20encoder%20is%20leveraged%20to%20refine%20the%0Aboundary%20delineation%2C%20thus%20improving%20the%20contrast%20between%20adjacent%20dental%0Astructures.%20By%20integrating%20these%20dual%20aspects%2C%20FDNet%20adeptly%20addresses%20the%0Asemantic%20gap%2C%20providing%20a%20detailed%20and%20accurate%20segmentation.%20The%20framework%27s%0Aeffectiveness%20is%20validated%20through%20rigorous%20benchmarks%2C%20achieving%20the%20top%20Dice%0Aand%20IoU%20scores%20of%2085.28%25%20and%2075.23%25%2C%20respectively.%20This%20innovative%20decoupling%0Aof%20semantic%20and%20boundary%20features%20capitalizes%20on%20the%20unique%20strengths%20of%20each%0Aelement%20to%20elevate%20the%20quality%20of%20segmentation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFDNet%253A%2520Feature%2520Decoupled%2520Segmentation%2520Network%2520for%2520Tooth%2520CBCT%2520Image%26entry.906535625%3DXiang%2520Feng%2520and%2520Chengkai%2520Wang%2520and%2520Chengyu%2520Wu%2520and%2520Yunxiang%2520Li%2520and%2520Yongbo%2520He%2520and%2520Shuai%2520Wang%2520and%2520Yaiqi%2520Wang%26entry.1292438233%3D%2520%2520Precise%2520Tooth%2520Cone%2520Beam%2520Computed%2520Tomography%2520%2528CBCT%2529%2520image%2520segmentation%2520is%250Acrucial%2520for%2520orthodontic%2520treatment%2520planning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FDNet%252C%2520a%250AFeature%2520Decoupled%2520Segmentation%2520Network%252C%2520to%2520excel%2520in%2520the%2520face%2520of%2520the%2520variable%250Adental%2520conditions%2520encountered%2520in%2520CBCT%2520scans%252C%2520such%2520as%2520complex%2520artifacts%2520and%250Aindistinct%2520tooth%2520boundaries.%2520The%2520Low-Frequency%2520Wavelet%2520Transform%2520%2528LF-Wavelet%2529%250Ais%2520employed%2520to%2520enrich%2520the%2520semantic%2520content%2520by%2520emphasizing%2520the%2520global%2520structural%250Aintegrity%2520of%2520the%2520teeth%252C%2520while%2520the%2520SAM%2520encoder%2520is%2520leveraged%2520to%2520refine%2520the%250Aboundary%2520delineation%252C%2520thus%2520improving%2520the%2520contrast%2520between%2520adjacent%2520dental%250Astructures.%2520By%2520integrating%2520these%2520dual%2520aspects%252C%2520FDNet%2520adeptly%2520addresses%2520the%250Asemantic%2520gap%252C%2520providing%2520a%2520detailed%2520and%2520accurate%2520segmentation.%2520The%2520framework%2527s%250Aeffectiveness%2520is%2520validated%2520through%2520rigorous%2520benchmarks%252C%2520achieving%2520the%2520top%2520Dice%250Aand%2520IoU%2520scores%2520of%252085.28%2525%2520and%252075.23%2525%252C%2520respectively.%2520This%2520innovative%2520decoupling%250Aof%2520semantic%2520and%2520boundary%2520features%2520capitalizes%2520on%2520the%2520unique%2520strengths%2520of%2520each%250Aelement%2520to%2520elevate%2520the%2520quality%2520of%2520segmentation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FDNet%3A%20Feature%20Decoupled%20Segmentation%20Network%20for%20Tooth%20CBCT%20Image&entry.906535625=Xiang%20Feng%20and%20Chengkai%20Wang%20and%20Chengyu%20Wu%20and%20Yunxiang%20Li%20and%20Yongbo%20He%20and%20Shuai%20Wang%20and%20Yaiqi%20Wang&entry.1292438233=%20%20Precise%20Tooth%20Cone%20Beam%20Computed%20Tomography%20%28CBCT%29%20image%20segmentation%20is%0Acrucial%20for%20orthodontic%20treatment%20planning.%20In%20this%20paper%2C%20we%20propose%20FDNet%2C%20a%0AFeature%20Decoupled%20Segmentation%20Network%2C%20to%20excel%20in%20the%20face%20of%20the%20variable%0Adental%20conditions%20encountered%20in%20CBCT%20scans%2C%20such%20as%20complex%20artifacts%20and%0Aindistinct%20tooth%20boundaries.%20The%20Low-Frequency%20Wavelet%20Transform%20%28LF-Wavelet%29%0Ais%20employed%20to%20enrich%20the%20semantic%20content%20by%20emphasizing%20the%20global%20structural%0Aintegrity%20of%20the%20teeth%2C%20while%20the%20SAM%20encoder%20is%20leveraged%20to%20refine%20the%0Aboundary%20delineation%2C%20thus%20improving%20the%20contrast%20between%20adjacent%20dental%0Astructures.%20By%20integrating%20these%20dual%20aspects%2C%20FDNet%20adeptly%20addresses%20the%0Asemantic%20gap%2C%20providing%20a%20detailed%20and%20accurate%20segmentation.%20The%20framework%27s%0Aeffectiveness%20is%20validated%20through%20rigorous%20benchmarks%2C%20achieving%20the%20top%20Dice%0Aand%20IoU%20scores%20of%2085.28%25%20and%2075.23%25%2C%20respectively.%20This%20innovative%20decoupling%0Aof%20semantic%20and%20boundary%20features%20capitalizes%20on%20the%20unique%20strengths%20of%20each%0Aelement%20to%20elevate%20the%20quality%20of%20segmentation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06551v2&entry.124074799=Read"},
{"title": "Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge\n  Transfer Learning for ASR", "author": "Xugang Lu and Peng Shen and Yu Tsao and Hisashi Kawai", "abstract": "  Transferring linguistic knowledge from a pretrained language model (PLM) to\nan acoustic model has been shown to greatly improve the performance of\nautomatic speech recognition (ASR). However, due to the heterogeneous feature\ndistributions in cross-modalities, designing an effective model for feature\nalignment and knowledge transfer between linguistic and acoustic sequences\nremains a challenging task. Optimal transport (OT), which efficiently measures\nprobability distribution discrepancies, holds great potential for aligning and\ntransferring knowledge between acoustic and linguistic modalities. Nonetheless,\nthe original OT treats acoustic and linguistic feature sequences as two\nunordered sets in alignment and neglects temporal order information during OT\ncoupling estimation. Consequently, a time-consuming pretraining stage is\nrequired to learn a good alignment between the acoustic and linguistic\nrepresentations. In this paper, we propose a Temporal Order Preserved OT\n(TOT)-based Cross-modal Alignment and Knowledge Transfer (CAKT) (TOT-CAKT) for\nASR. In the TOT-CAKT, local neighboring frames of acoustic sequences are\nsmoothly mapped to neighboring regions of linguistic sequences, preserving\ntheir temporal order relationship in feature alignment and matching. With the\nTOT-CAKT model framework, we conduct Mandarin ASR experiments with a pretrained\nChinese PLM for linguistic knowledge transfer. Our results demonstrate that the\nproposed TOT-CAKT significantly improves ASR performance compared to several\nstate-of-the-art models employing linguistic knowledge transfer, and addresses\nthe weaknesses of the original OT-based method in sequential feature alignment\nfor ASR.\n", "link": "http://arxiv.org/abs/2409.02239v2", "date": "2024-09-05", "relevancy": 2.4425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5587}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4535}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Order%20Preserved%20Optimal%20Transport-based%20Cross-modal%20Knowledge%0A%20%20Transfer%20Learning%20for%20ASR&body=Title%3A%20Temporal%20Order%20Preserved%20Optimal%20Transport-based%20Cross-modal%20Knowledge%0A%20%20Transfer%20Learning%20for%20ASR%0AAuthor%3A%20Xugang%20Lu%20and%20Peng%20Shen%20and%20Yu%20Tsao%20and%20Hisashi%20Kawai%0AAbstract%3A%20%20%20Transferring%20linguistic%20knowledge%20from%20a%20pretrained%20language%20model%20%28PLM%29%20to%0Aan%20acoustic%20model%20has%20been%20shown%20to%20greatly%20improve%20the%20performance%20of%0Aautomatic%20speech%20recognition%20%28ASR%29.%20However%2C%20due%20to%20the%20heterogeneous%20feature%0Adistributions%20in%20cross-modalities%2C%20designing%20an%20effective%20model%20for%20feature%0Aalignment%20and%20knowledge%20transfer%20between%20linguistic%20and%20acoustic%20sequences%0Aremains%20a%20challenging%20task.%20Optimal%20transport%20%28OT%29%2C%20which%20efficiently%20measures%0Aprobability%20distribution%20discrepancies%2C%20holds%20great%20potential%20for%20aligning%20and%0Atransferring%20knowledge%20between%20acoustic%20and%20linguistic%20modalities.%20Nonetheless%2C%0Athe%20original%20OT%20treats%20acoustic%20and%20linguistic%20feature%20sequences%20as%20two%0Aunordered%20sets%20in%20alignment%20and%20neglects%20temporal%20order%20information%20during%20OT%0Acoupling%20estimation.%20Consequently%2C%20a%20time-consuming%20pretraining%20stage%20is%0Arequired%20to%20learn%20a%20good%20alignment%20between%20the%20acoustic%20and%20linguistic%0Arepresentations.%20In%20this%20paper%2C%20we%20propose%20a%20Temporal%20Order%20Preserved%20OT%0A%28TOT%29-based%20Cross-modal%20Alignment%20and%20Knowledge%20Transfer%20%28CAKT%29%20%28TOT-CAKT%29%20for%0AASR.%20In%20the%20TOT-CAKT%2C%20local%20neighboring%20frames%20of%20acoustic%20sequences%20are%0Asmoothly%20mapped%20to%20neighboring%20regions%20of%20linguistic%20sequences%2C%20preserving%0Atheir%20temporal%20order%20relationship%20in%20feature%20alignment%20and%20matching.%20With%20the%0ATOT-CAKT%20model%20framework%2C%20we%20conduct%20Mandarin%20ASR%20experiments%20with%20a%20pretrained%0AChinese%20PLM%20for%20linguistic%20knowledge%20transfer.%20Our%20results%20demonstrate%20that%20the%0Aproposed%20TOT-CAKT%20significantly%20improves%20ASR%20performance%20compared%20to%20several%0Astate-of-the-art%20models%20employing%20linguistic%20knowledge%20transfer%2C%20and%20addresses%0Athe%20weaknesses%20of%20the%20original%20OT-based%20method%20in%20sequential%20feature%20alignment%0Afor%20ASR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Order%2520Preserved%2520Optimal%2520Transport-based%2520Cross-modal%2520Knowledge%250A%2520%2520Transfer%2520Learning%2520for%2520ASR%26entry.906535625%3DXugang%2520Lu%2520and%2520Peng%2520Shen%2520and%2520Yu%2520Tsao%2520and%2520Hisashi%2520Kawai%26entry.1292438233%3D%2520%2520Transferring%2520linguistic%2520knowledge%2520from%2520a%2520pretrained%2520language%2520model%2520%2528PLM%2529%2520to%250Aan%2520acoustic%2520model%2520has%2520been%2520shown%2520to%2520greatly%2520improve%2520the%2520performance%2520of%250Aautomatic%2520speech%2520recognition%2520%2528ASR%2529.%2520However%252C%2520due%2520to%2520the%2520heterogeneous%2520feature%250Adistributions%2520in%2520cross-modalities%252C%2520designing%2520an%2520effective%2520model%2520for%2520feature%250Aalignment%2520and%2520knowledge%2520transfer%2520between%2520linguistic%2520and%2520acoustic%2520sequences%250Aremains%2520a%2520challenging%2520task.%2520Optimal%2520transport%2520%2528OT%2529%252C%2520which%2520efficiently%2520measures%250Aprobability%2520distribution%2520discrepancies%252C%2520holds%2520great%2520potential%2520for%2520aligning%2520and%250Atransferring%2520knowledge%2520between%2520acoustic%2520and%2520linguistic%2520modalities.%2520Nonetheless%252C%250Athe%2520original%2520OT%2520treats%2520acoustic%2520and%2520linguistic%2520feature%2520sequences%2520as%2520two%250Aunordered%2520sets%2520in%2520alignment%2520and%2520neglects%2520temporal%2520order%2520information%2520during%2520OT%250Acoupling%2520estimation.%2520Consequently%252C%2520a%2520time-consuming%2520pretraining%2520stage%2520is%250Arequired%2520to%2520learn%2520a%2520good%2520alignment%2520between%2520the%2520acoustic%2520and%2520linguistic%250Arepresentations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Temporal%2520Order%2520Preserved%2520OT%250A%2528TOT%2529-based%2520Cross-modal%2520Alignment%2520and%2520Knowledge%2520Transfer%2520%2528CAKT%2529%2520%2528TOT-CAKT%2529%2520for%250AASR.%2520In%2520the%2520TOT-CAKT%252C%2520local%2520neighboring%2520frames%2520of%2520acoustic%2520sequences%2520are%250Asmoothly%2520mapped%2520to%2520neighboring%2520regions%2520of%2520linguistic%2520sequences%252C%2520preserving%250Atheir%2520temporal%2520order%2520relationship%2520in%2520feature%2520alignment%2520and%2520matching.%2520With%2520the%250ATOT-CAKT%2520model%2520framework%252C%2520we%2520conduct%2520Mandarin%2520ASR%2520experiments%2520with%2520a%2520pretrained%250AChinese%2520PLM%2520for%2520linguistic%2520knowledge%2520transfer.%2520Our%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520TOT-CAKT%2520significantly%2520improves%2520ASR%2520performance%2520compared%2520to%2520several%250Astate-of-the-art%2520models%2520employing%2520linguistic%2520knowledge%2520transfer%252C%2520and%2520addresses%250Athe%2520weaknesses%2520of%2520the%2520original%2520OT-based%2520method%2520in%2520sequential%2520feature%2520alignment%250Afor%2520ASR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Order%20Preserved%20Optimal%20Transport-based%20Cross-modal%20Knowledge%0A%20%20Transfer%20Learning%20for%20ASR&entry.906535625=Xugang%20Lu%20and%20Peng%20Shen%20and%20Yu%20Tsao%20and%20Hisashi%20Kawai&entry.1292438233=%20%20Transferring%20linguistic%20knowledge%20from%20a%20pretrained%20language%20model%20%28PLM%29%20to%0Aan%20acoustic%20model%20has%20been%20shown%20to%20greatly%20improve%20the%20performance%20of%0Aautomatic%20speech%20recognition%20%28ASR%29.%20However%2C%20due%20to%20the%20heterogeneous%20feature%0Adistributions%20in%20cross-modalities%2C%20designing%20an%20effective%20model%20for%20feature%0Aalignment%20and%20knowledge%20transfer%20between%20linguistic%20and%20acoustic%20sequences%0Aremains%20a%20challenging%20task.%20Optimal%20transport%20%28OT%29%2C%20which%20efficiently%20measures%0Aprobability%20distribution%20discrepancies%2C%20holds%20great%20potential%20for%20aligning%20and%0Atransferring%20knowledge%20between%20acoustic%20and%20linguistic%20modalities.%20Nonetheless%2C%0Athe%20original%20OT%20treats%20acoustic%20and%20linguistic%20feature%20sequences%20as%20two%0Aunordered%20sets%20in%20alignment%20and%20neglects%20temporal%20order%20information%20during%20OT%0Acoupling%20estimation.%20Consequently%2C%20a%20time-consuming%20pretraining%20stage%20is%0Arequired%20to%20learn%20a%20good%20alignment%20between%20the%20acoustic%20and%20linguistic%0Arepresentations.%20In%20this%20paper%2C%20we%20propose%20a%20Temporal%20Order%20Preserved%20OT%0A%28TOT%29-based%20Cross-modal%20Alignment%20and%20Knowledge%20Transfer%20%28CAKT%29%20%28TOT-CAKT%29%20for%0AASR.%20In%20the%20TOT-CAKT%2C%20local%20neighboring%20frames%20of%20acoustic%20sequences%20are%0Asmoothly%20mapped%20to%20neighboring%20regions%20of%20linguistic%20sequences%2C%20preserving%0Atheir%20temporal%20order%20relationship%20in%20feature%20alignment%20and%20matching.%20With%20the%0ATOT-CAKT%20model%20framework%2C%20we%20conduct%20Mandarin%20ASR%20experiments%20with%20a%20pretrained%0AChinese%20PLM%20for%20linguistic%20knowledge%20transfer.%20Our%20results%20demonstrate%20that%20the%0Aproposed%20TOT-CAKT%20significantly%20improves%20ASR%20performance%20compared%20to%20several%0Astate-of-the-art%20models%20employing%20linguistic%20knowledge%20transfer%2C%20and%20addresses%0Athe%20weaknesses%20of%20the%20original%20OT-based%20method%20in%20sequential%20feature%20alignment%0Afor%20ASR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02239v2&entry.124074799=Read"},
{"title": "RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in\n  Generated Images", "author": "Benzhi Wang and Jingkai Zhou and Jingqi Bai and Yang Yang and Weihua Chen and Fan Wang and Zhen Lei", "abstract": "  In recent years, diffusion models have revolutionized visual generation,\noutperforming traditional frameworks like Generative Adversarial Networks\n(GANs). However, generating images of humans with realistic semantic parts,\nsuch as hands and faces, remains a significant challenge due to their intricate\nstructural complexity. To address this issue, we propose a novel\npost-processing solution named RealisHuman. The RealisHuman framework operates\nin two stages. First, it generates realistic human parts, such as hands or\nfaces, using the original malformed parts as references, ensuring consistent\ndetails with the original image. Second, it seamlessly integrates the rectified\nhuman parts back into their corresponding positions by repainting the\nsurrounding areas to ensure smooth and realistic blending. The RealisHuman\nframework significantly enhances the realism of human generation, as\ndemonstrated by notable improvements in both qualitative and quantitative\nmetrics. Code is available at https://github.com/Wangbenzhi/RealisHuman.\n", "link": "http://arxiv.org/abs/2409.03644v1", "date": "2024-09-05", "relevancy": 2.4309, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6278}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5965}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealisHuman%3A%20A%20Two-Stage%20Approach%20for%20Refining%20Malformed%20Human%20Parts%20in%0A%20%20Generated%20Images&body=Title%3A%20RealisHuman%3A%20A%20Two-Stage%20Approach%20for%20Refining%20Malformed%20Human%20Parts%20in%0A%20%20Generated%20Images%0AAuthor%3A%20Benzhi%20Wang%20and%20Jingkai%20Zhou%20and%20Jingqi%20Bai%20and%20Yang%20Yang%20and%20Weihua%20Chen%20and%20Fan%20Wang%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20In%20recent%20years%2C%20diffusion%20models%20have%20revolutionized%20visual%20generation%2C%0Aoutperforming%20traditional%20frameworks%20like%20Generative%20Adversarial%20Networks%0A%28GANs%29.%20However%2C%20generating%20images%20of%20humans%20with%20realistic%20semantic%20parts%2C%0Asuch%20as%20hands%20and%20faces%2C%20remains%20a%20significant%20challenge%20due%20to%20their%20intricate%0Astructural%20complexity.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Apost-processing%20solution%20named%20RealisHuman.%20The%20RealisHuman%20framework%20operates%0Ain%20two%20stages.%20First%2C%20it%20generates%20realistic%20human%20parts%2C%20such%20as%20hands%20or%0Afaces%2C%20using%20the%20original%20malformed%20parts%20as%20references%2C%20ensuring%20consistent%0Adetails%20with%20the%20original%20image.%20Second%2C%20it%20seamlessly%20integrates%20the%20rectified%0Ahuman%20parts%20back%20into%20their%20corresponding%20positions%20by%20repainting%20the%0Asurrounding%20areas%20to%20ensure%20smooth%20and%20realistic%20blending.%20The%20RealisHuman%0Aframework%20significantly%20enhances%20the%20realism%20of%20human%20generation%2C%20as%0Ademonstrated%20by%20notable%20improvements%20in%20both%20qualitative%20and%20quantitative%0Ametrics.%20Code%20is%20available%20at%20https%3A//github.com/Wangbenzhi/RealisHuman.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealisHuman%253A%2520A%2520Two-Stage%2520Approach%2520for%2520Refining%2520Malformed%2520Human%2520Parts%2520in%250A%2520%2520Generated%2520Images%26entry.906535625%3DBenzhi%2520Wang%2520and%2520Jingkai%2520Zhou%2520and%2520Jingqi%2520Bai%2520and%2520Yang%2520Yang%2520and%2520Weihua%2520Chen%2520and%2520Fan%2520Wang%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520diffusion%2520models%2520have%2520revolutionized%2520visual%2520generation%252C%250Aoutperforming%2520traditional%2520frameworks%2520like%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529.%2520However%252C%2520generating%2520images%2520of%2520humans%2520with%2520realistic%2520semantic%2520parts%252C%250Asuch%2520as%2520hands%2520and%2520faces%252C%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520their%2520intricate%250Astructural%2520complexity.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%250Apost-processing%2520solution%2520named%2520RealisHuman.%2520The%2520RealisHuman%2520framework%2520operates%250Ain%2520two%2520stages.%2520First%252C%2520it%2520generates%2520realistic%2520human%2520parts%252C%2520such%2520as%2520hands%2520or%250Afaces%252C%2520using%2520the%2520original%2520malformed%2520parts%2520as%2520references%252C%2520ensuring%2520consistent%250Adetails%2520with%2520the%2520original%2520image.%2520Second%252C%2520it%2520seamlessly%2520integrates%2520the%2520rectified%250Ahuman%2520parts%2520back%2520into%2520their%2520corresponding%2520positions%2520by%2520repainting%2520the%250Asurrounding%2520areas%2520to%2520ensure%2520smooth%2520and%2520realistic%2520blending.%2520The%2520RealisHuman%250Aframework%2520significantly%2520enhances%2520the%2520realism%2520of%2520human%2520generation%252C%2520as%250Ademonstrated%2520by%2520notable%2520improvements%2520in%2520both%2520qualitative%2520and%2520quantitative%250Ametrics.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Wangbenzhi/RealisHuman.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealisHuman%3A%20A%20Two-Stage%20Approach%20for%20Refining%20Malformed%20Human%20Parts%20in%0A%20%20Generated%20Images&entry.906535625=Benzhi%20Wang%20and%20Jingkai%20Zhou%20and%20Jingqi%20Bai%20and%20Yang%20Yang%20and%20Weihua%20Chen%20and%20Fan%20Wang%20and%20Zhen%20Lei&entry.1292438233=%20%20In%20recent%20years%2C%20diffusion%20models%20have%20revolutionized%20visual%20generation%2C%0Aoutperforming%20traditional%20frameworks%20like%20Generative%20Adversarial%20Networks%0A%28GANs%29.%20However%2C%20generating%20images%20of%20humans%20with%20realistic%20semantic%20parts%2C%0Asuch%20as%20hands%20and%20faces%2C%20remains%20a%20significant%20challenge%20due%20to%20their%20intricate%0Astructural%20complexity.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Apost-processing%20solution%20named%20RealisHuman.%20The%20RealisHuman%20framework%20operates%0Ain%20two%20stages.%20First%2C%20it%20generates%20realistic%20human%20parts%2C%20such%20as%20hands%20or%0Afaces%2C%20using%20the%20original%20malformed%20parts%20as%20references%2C%20ensuring%20consistent%0Adetails%20with%20the%20original%20image.%20Second%2C%20it%20seamlessly%20integrates%20the%20rectified%0Ahuman%20parts%20back%20into%20their%20corresponding%20positions%20by%20repainting%20the%0Asurrounding%20areas%20to%20ensure%20smooth%20and%20realistic%20blending.%20The%20RealisHuman%0Aframework%20significantly%20enhances%20the%20realism%20of%20human%20generation%2C%20as%0Ademonstrated%20by%20notable%20improvements%20in%20both%20qualitative%20and%20quantitative%0Ametrics.%20Code%20is%20available%20at%20https%3A//github.com/Wangbenzhi/RealisHuman.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03644v1&entry.124074799=Read"},
{"title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?", "author": "Jonathan Hayase and Alisa Liu and Yejin Choi and Sewoong Oh and Noah A. Smith", "abstract": "  The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation: byte-pair encoding (BPE) tokenizers, used by the vast majority of\nmodern language models. Our key insight is that the ordered list of merge rules\nlearned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data. Given a tokenizer's merge list along with\nexample data for each category of interest, we formulate a linear program that\nsolves for the proportion of each category in the tokenizer's training set. In\ncontrolled experiments, we show that our attack recovers mixture ratios with\nhigh precision for tokenizers trained on known mixtures of natural languages,\nprogramming languages, and data sources. We then apply our approach to\noff-the-shelf tokenizers released with recent LMs. We confirm much publicly\ndisclosed information about these models, and also make several new inferences:\nGPT-4o and Mistral NeMo's tokenizers are much more multilingual than their\npredecessors, training on 39% and 47% non-English language data, respectively;\nLlama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;\nGPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We\nhope our work sheds light on current design practices for pretraining data, and\ninspires continued research into data mixture inference for LMs.\n", "link": "http://arxiv.org/abs/2407.16607v3", "date": "2024-09-05", "relevancy": 2.4286, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.492}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Mixture%20Inference%3A%20What%20do%20BPE%20Tokenizers%20Reveal%20about%20their%0A%20%20Training%20Data%3F&body=Title%3A%20Data%20Mixture%20Inference%3A%20What%20do%20BPE%20Tokenizers%20Reveal%20about%20their%0A%20%20Training%20Data%3F%0AAuthor%3A%20Jonathan%20Hayase%20and%20Alisa%20Liu%20and%20Yejin%20Choi%20and%20Sewoong%20Oh%20and%20Noah%20A.%20Smith%0AAbstract%3A%20%20%20The%20pretraining%20data%20of%20today%27s%20strongest%20language%20models%20is%20opaque%3B%20in%0Aparticular%2C%20little%20is%20known%20about%20the%20proportions%20of%20various%20domains%20or%0Alanguages%20represented.%20In%20this%20work%2C%20we%20tackle%20a%20task%20which%20we%20call%20data%0Amixture%20inference%2C%20which%20aims%20to%20uncover%20the%20distributional%20make-up%20of%20training%0Adata.%20We%20introduce%20a%20novel%20attack%20based%20on%20a%20previously%20overlooked%20source%20of%0Ainformation%3A%20byte-pair%20encoding%20%28BPE%29%20tokenizers%2C%20used%20by%20the%20vast%20majority%20of%0Amodern%20language%20models.%20Our%20key%20insight%20is%20that%20the%20ordered%20list%20of%20merge%20rules%0Alearned%20by%20a%20BPE%20tokenizer%20naturally%20reveals%20information%20about%20the%20token%0Afrequencies%20in%20its%20training%20data.%20Given%20a%20tokenizer%27s%20merge%20list%20along%20with%0Aexample%20data%20for%20each%20category%20of%20interest%2C%20we%20formulate%20a%20linear%20program%20that%0Asolves%20for%20the%20proportion%20of%20each%20category%20in%20the%20tokenizer%27s%20training%20set.%20In%0Acontrolled%20experiments%2C%20we%20show%20that%20our%20attack%20recovers%20mixture%20ratios%20with%0Ahigh%20precision%20for%20tokenizers%20trained%20on%20known%20mixtures%20of%20natural%20languages%2C%0Aprogramming%20languages%2C%20and%20data%20sources.%20We%20then%20apply%20our%20approach%20to%0Aoff-the-shelf%20tokenizers%20released%20with%20recent%20LMs.%20We%20confirm%20much%20publicly%0Adisclosed%20information%20about%20these%20models%2C%20and%20also%20make%20several%20new%20inferences%3A%0AGPT-4o%20and%20Mistral%20NeMo%27s%20tokenizers%20are%20much%20more%20multilingual%20than%20their%0Apredecessors%2C%20training%20on%2039%25%20and%2047%25%20non-English%20language%20data%2C%20respectively%3B%0ALlama%203%20extends%20GPT-3.5%27s%20tokenizer%20primarily%20for%20multilingual%20%2848%25%29%20use%3B%0AGPT-3.5%27s%20and%20Claude%27s%20tokenizers%20are%20trained%20on%20predominantly%20code%20%28~60%25%29.%20We%0Ahope%20our%20work%20sheds%20light%20on%20current%20design%20practices%20for%20pretraining%20data%2C%20and%0Ainspires%20continued%20research%20into%20data%20mixture%20inference%20for%20LMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16607v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Mixture%2520Inference%253A%2520What%2520do%2520BPE%2520Tokenizers%2520Reveal%2520about%2520their%250A%2520%2520Training%2520Data%253F%26entry.906535625%3DJonathan%2520Hayase%2520and%2520Alisa%2520Liu%2520and%2520Yejin%2520Choi%2520and%2520Sewoong%2520Oh%2520and%2520Noah%2520A.%2520Smith%26entry.1292438233%3D%2520%2520The%2520pretraining%2520data%2520of%2520today%2527s%2520strongest%2520language%2520models%2520is%2520opaque%253B%2520in%250Aparticular%252C%2520little%2520is%2520known%2520about%2520the%2520proportions%2520of%2520various%2520domains%2520or%250Alanguages%2520represented.%2520In%2520this%2520work%252C%2520we%2520tackle%2520a%2520task%2520which%2520we%2520call%2520data%250Amixture%2520inference%252C%2520which%2520aims%2520to%2520uncover%2520the%2520distributional%2520make-up%2520of%2520training%250Adata.%2520We%2520introduce%2520a%2520novel%2520attack%2520based%2520on%2520a%2520previously%2520overlooked%2520source%2520of%250Ainformation%253A%2520byte-pair%2520encoding%2520%2528BPE%2529%2520tokenizers%252C%2520used%2520by%2520the%2520vast%2520majority%2520of%250Amodern%2520language%2520models.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520ordered%2520list%2520of%2520merge%2520rules%250Alearned%2520by%2520a%2520BPE%2520tokenizer%2520naturally%2520reveals%2520information%2520about%2520the%2520token%250Afrequencies%2520in%2520its%2520training%2520data.%2520Given%2520a%2520tokenizer%2527s%2520merge%2520list%2520along%2520with%250Aexample%2520data%2520for%2520each%2520category%2520of%2520interest%252C%2520we%2520formulate%2520a%2520linear%2520program%2520that%250Asolves%2520for%2520the%2520proportion%2520of%2520each%2520category%2520in%2520the%2520tokenizer%2527s%2520training%2520set.%2520In%250Acontrolled%2520experiments%252C%2520we%2520show%2520that%2520our%2520attack%2520recovers%2520mixture%2520ratios%2520with%250Ahigh%2520precision%2520for%2520tokenizers%2520trained%2520on%2520known%2520mixtures%2520of%2520natural%2520languages%252C%250Aprogramming%2520languages%252C%2520and%2520data%2520sources.%2520We%2520then%2520apply%2520our%2520approach%2520to%250Aoff-the-shelf%2520tokenizers%2520released%2520with%2520recent%2520LMs.%2520We%2520confirm%2520much%2520publicly%250Adisclosed%2520information%2520about%2520these%2520models%252C%2520and%2520also%2520make%2520several%2520new%2520inferences%253A%250AGPT-4o%2520and%2520Mistral%2520NeMo%2527s%2520tokenizers%2520are%2520much%2520more%2520multilingual%2520than%2520their%250Apredecessors%252C%2520training%2520on%252039%2525%2520and%252047%2525%2520non-English%2520language%2520data%252C%2520respectively%253B%250ALlama%25203%2520extends%2520GPT-3.5%2527s%2520tokenizer%2520primarily%2520for%2520multilingual%2520%252848%2525%2529%2520use%253B%250AGPT-3.5%2527s%2520and%2520Claude%2527s%2520tokenizers%2520are%2520trained%2520on%2520predominantly%2520code%2520%2528~60%2525%2529.%2520We%250Ahope%2520our%2520work%2520sheds%2520light%2520on%2520current%2520design%2520practices%2520for%2520pretraining%2520data%252C%2520and%250Ainspires%2520continued%2520research%2520into%2520data%2520mixture%2520inference%2520for%2520LMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16607v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Mixture%20Inference%3A%20What%20do%20BPE%20Tokenizers%20Reveal%20about%20their%0A%20%20Training%20Data%3F&entry.906535625=Jonathan%20Hayase%20and%20Alisa%20Liu%20and%20Yejin%20Choi%20and%20Sewoong%20Oh%20and%20Noah%20A.%20Smith&entry.1292438233=%20%20The%20pretraining%20data%20of%20today%27s%20strongest%20language%20models%20is%20opaque%3B%20in%0Aparticular%2C%20little%20is%20known%20about%20the%20proportions%20of%20various%20domains%20or%0Alanguages%20represented.%20In%20this%20work%2C%20we%20tackle%20a%20task%20which%20we%20call%20data%0Amixture%20inference%2C%20which%20aims%20to%20uncover%20the%20distributional%20make-up%20of%20training%0Adata.%20We%20introduce%20a%20novel%20attack%20based%20on%20a%20previously%20overlooked%20source%20of%0Ainformation%3A%20byte-pair%20encoding%20%28BPE%29%20tokenizers%2C%20used%20by%20the%20vast%20majority%20of%0Amodern%20language%20models.%20Our%20key%20insight%20is%20that%20the%20ordered%20list%20of%20merge%20rules%0Alearned%20by%20a%20BPE%20tokenizer%20naturally%20reveals%20information%20about%20the%20token%0Afrequencies%20in%20its%20training%20data.%20Given%20a%20tokenizer%27s%20merge%20list%20along%20with%0Aexample%20data%20for%20each%20category%20of%20interest%2C%20we%20formulate%20a%20linear%20program%20that%0Asolves%20for%20the%20proportion%20of%20each%20category%20in%20the%20tokenizer%27s%20training%20set.%20In%0Acontrolled%20experiments%2C%20we%20show%20that%20our%20attack%20recovers%20mixture%20ratios%20with%0Ahigh%20precision%20for%20tokenizers%20trained%20on%20known%20mixtures%20of%20natural%20languages%2C%0Aprogramming%20languages%2C%20and%20data%20sources.%20We%20then%20apply%20our%20approach%20to%0Aoff-the-shelf%20tokenizers%20released%20with%20recent%20LMs.%20We%20confirm%20much%20publicly%0Adisclosed%20information%20about%20these%20models%2C%20and%20also%20make%20several%20new%20inferences%3A%0AGPT-4o%20and%20Mistral%20NeMo%27s%20tokenizers%20are%20much%20more%20multilingual%20than%20their%0Apredecessors%2C%20training%20on%2039%25%20and%2047%25%20non-English%20language%20data%2C%20respectively%3B%0ALlama%203%20extends%20GPT-3.5%27s%20tokenizer%20primarily%20for%20multilingual%20%2848%25%29%20use%3B%0AGPT-3.5%27s%20and%20Claude%27s%20tokenizers%20are%20trained%20on%20predominantly%20code%20%28~60%25%29.%20We%0Ahope%20our%20work%20sheds%20light%20on%20current%20design%20practices%20for%20pretraining%20data%2C%20and%0Ainspires%20continued%20research%20into%20data%20mixture%20inference%20for%20LMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16607v3&entry.124074799=Read"},
{"title": "Region-aware Grasp Framework with Normalized Grasp Space for Efficient\n  6-DoF Grasping", "author": "Siang Chen and Pengwei Xie and Wei Tang and Dingchang Hu and Yixiang Dai and Guijin Wang", "abstract": "  A series of region-based methods succeed in extracting regional features and\nenhancing grasp detection quality. However, faced with a cluttered scene with\npotential collision, the definition of the grasp-relevant region stays\ninconsistent, and the relationship between grasps and regional spaces remains\nincompletely investigated. In this paper, we propose Normalized Grasp Space\n(NGS) from a novel region-aware viewpoint, unifying the grasp representation\nwithin a normalized regional space and benefiting the generalizability of\nmethods. Leveraging the NGS, we find that CNNs are underestimated for 3D\nfeature extraction and 6-DoF grasp detection in clutter scenes and build a\nhighly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on\nthe public benchmark show that our method achieves significant >20% performance\ngains while attaining a real-time inference speed of approximately 50 FPS.\nReal-world cluttered scene clearance experiments underscore the effectiveness\nof our method. Further, human-to-robot handover and dynamic object grasping\nexperiments demonstrate the potential of our proposed method for closed-loop\ngrasping in dynamic scenarios.\n", "link": "http://arxiv.org/abs/2406.01767v2", "date": "2024-09-05", "relevancy": 2.3953, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6136}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6006}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region-aware%20Grasp%20Framework%20with%20Normalized%20Grasp%20Space%20for%20Efficient%0A%20%206-DoF%20Grasping&body=Title%3A%20Region-aware%20Grasp%20Framework%20with%20Normalized%20Grasp%20Space%20for%20Efficient%0A%20%206-DoF%20Grasping%0AAuthor%3A%20Siang%20Chen%20and%20Pengwei%20Xie%20and%20Wei%20Tang%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Guijin%20Wang%0AAbstract%3A%20%20%20A%20series%20of%20region-based%20methods%20succeed%20in%20extracting%20regional%20features%20and%0Aenhancing%20grasp%20detection%20quality.%20However%2C%20faced%20with%20a%20cluttered%20scene%20with%0Apotential%20collision%2C%20the%20definition%20of%20the%20grasp-relevant%20region%20stays%0Ainconsistent%2C%20and%20the%20relationship%20between%20grasps%20and%20regional%20spaces%20remains%0Aincompletely%20investigated.%20In%20this%20paper%2C%20we%20propose%20Normalized%20Grasp%20Space%0A%28NGS%29%20from%20a%20novel%20region-aware%20viewpoint%2C%20unifying%20the%20grasp%20representation%0Awithin%20a%20normalized%20regional%20space%20and%20benefiting%20the%20generalizability%20of%0Amethods.%20Leveraging%20the%20NGS%2C%20we%20find%20that%20CNNs%20are%20underestimated%20for%203D%0Afeature%20extraction%20and%206-DoF%20grasp%20detection%20in%20clutter%20scenes%20and%20build%20a%0Ahighly%20efficient%20Region-aware%20Normalized%20Grasp%20Network%20%28RNGNet%29.%20Experiments%20on%0Athe%20public%20benchmark%20show%20that%20our%20method%20achieves%20significant%20%3E20%25%20performance%0Agains%20while%20attaining%20a%20real-time%20inference%20speed%20of%20approximately%2050%20FPS.%0AReal-world%20cluttered%20scene%20clearance%20experiments%20underscore%20the%20effectiveness%0Aof%20our%20method.%20Further%2C%20human-to-robot%20handover%20and%20dynamic%20object%20grasping%0Aexperiments%20demonstrate%20the%20potential%20of%20our%20proposed%20method%20for%20closed-loop%0Agrasping%20in%20dynamic%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion-aware%2520Grasp%2520Framework%2520with%2520Normalized%2520Grasp%2520Space%2520for%2520Efficient%250A%2520%25206-DoF%2520Grasping%26entry.906535625%3DSiang%2520Chen%2520and%2520Pengwei%2520Xie%2520and%2520Wei%2520Tang%2520and%2520Dingchang%2520Hu%2520and%2520Yixiang%2520Dai%2520and%2520Guijin%2520Wang%26entry.1292438233%3D%2520%2520A%2520series%2520of%2520region-based%2520methods%2520succeed%2520in%2520extracting%2520regional%2520features%2520and%250Aenhancing%2520grasp%2520detection%2520quality.%2520However%252C%2520faced%2520with%2520a%2520cluttered%2520scene%2520with%250Apotential%2520collision%252C%2520the%2520definition%2520of%2520the%2520grasp-relevant%2520region%2520stays%250Ainconsistent%252C%2520and%2520the%2520relationship%2520between%2520grasps%2520and%2520regional%2520spaces%2520remains%250Aincompletely%2520investigated.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Normalized%2520Grasp%2520Space%250A%2528NGS%2529%2520from%2520a%2520novel%2520region-aware%2520viewpoint%252C%2520unifying%2520the%2520grasp%2520representation%250Awithin%2520a%2520normalized%2520regional%2520space%2520and%2520benefiting%2520the%2520generalizability%2520of%250Amethods.%2520Leveraging%2520the%2520NGS%252C%2520we%2520find%2520that%2520CNNs%2520are%2520underestimated%2520for%25203D%250Afeature%2520extraction%2520and%25206-DoF%2520grasp%2520detection%2520in%2520clutter%2520scenes%2520and%2520build%2520a%250Ahighly%2520efficient%2520Region-aware%2520Normalized%2520Grasp%2520Network%2520%2528RNGNet%2529.%2520Experiments%2520on%250Athe%2520public%2520benchmark%2520show%2520that%2520our%2520method%2520achieves%2520significant%2520%253E20%2525%2520performance%250Agains%2520while%2520attaining%2520a%2520real-time%2520inference%2520speed%2520of%2520approximately%252050%2520FPS.%250AReal-world%2520cluttered%2520scene%2520clearance%2520experiments%2520underscore%2520the%2520effectiveness%250Aof%2520our%2520method.%2520Further%252C%2520human-to-robot%2520handover%2520and%2520dynamic%2520object%2520grasping%250Aexperiments%2520demonstrate%2520the%2520potential%2520of%2520our%2520proposed%2520method%2520for%2520closed-loop%250Agrasping%2520in%2520dynamic%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region-aware%20Grasp%20Framework%20with%20Normalized%20Grasp%20Space%20for%20Efficient%0A%20%206-DoF%20Grasping&entry.906535625=Siang%20Chen%20and%20Pengwei%20Xie%20and%20Wei%20Tang%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Guijin%20Wang&entry.1292438233=%20%20A%20series%20of%20region-based%20methods%20succeed%20in%20extracting%20regional%20features%20and%0Aenhancing%20grasp%20detection%20quality.%20However%2C%20faced%20with%20a%20cluttered%20scene%20with%0Apotential%20collision%2C%20the%20definition%20of%20the%20grasp-relevant%20region%20stays%0Ainconsistent%2C%20and%20the%20relationship%20between%20grasps%20and%20regional%20spaces%20remains%0Aincompletely%20investigated.%20In%20this%20paper%2C%20we%20propose%20Normalized%20Grasp%20Space%0A%28NGS%29%20from%20a%20novel%20region-aware%20viewpoint%2C%20unifying%20the%20grasp%20representation%0Awithin%20a%20normalized%20regional%20space%20and%20benefiting%20the%20generalizability%20of%0Amethods.%20Leveraging%20the%20NGS%2C%20we%20find%20that%20CNNs%20are%20underestimated%20for%203D%0Afeature%20extraction%20and%206-DoF%20grasp%20detection%20in%20clutter%20scenes%20and%20build%20a%0Ahighly%20efficient%20Region-aware%20Normalized%20Grasp%20Network%20%28RNGNet%29.%20Experiments%20on%0Athe%20public%20benchmark%20show%20that%20our%20method%20achieves%20significant%20%3E20%25%20performance%0Agains%20while%20attaining%20a%20real-time%20inference%20speed%20of%20approximately%2050%20FPS.%0AReal-world%20cluttered%20scene%20clearance%20experiments%20underscore%20the%20effectiveness%0Aof%20our%20method.%20Further%2C%20human-to-robot%20handover%20and%20dynamic%20object%20grasping%0Aexperiments%20demonstrate%20the%20potential%20of%20our%20proposed%20method%20for%20closed-loop%0Agrasping%20in%20dynamic%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01767v2&entry.124074799=Read"},
{"title": "CW-CNN & CW-AN: Convolutional Networks and Attention Networks for\n  CW-Complexes", "author": "Rahul Khorana", "abstract": "  We present a novel framework for learning on CW-complex structured data\npoints. Recent advances have discussed CW-complexes as ideal learning\nrepresentations for problems in cheminformatics. However, there is a lack of\navailable machine learning methods suitable for learning on CW-complexes. In\nthis paper we develop notions of convolution and attention that are well\ndefined for CW-complexes. These notions enable us to create the first Hodge\ninformed neural network that can receive a CW-complex as input. We illustrate\nand interpret this framework in the context of supervised prediction.\n", "link": "http://arxiv.org/abs/2408.16686v2", "date": "2024-09-05", "relevancy": 2.3784, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.486}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4762}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CW-CNN%20%26%20CW-AN%3A%20Convolutional%20Networks%20and%20Attention%20Networks%20for%0A%20%20CW-Complexes&body=Title%3A%20CW-CNN%20%26%20CW-AN%3A%20Convolutional%20Networks%20and%20Attention%20Networks%20for%0A%20%20CW-Complexes%0AAuthor%3A%20Rahul%20Khorana%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20for%20learning%20on%20CW-complex%20structured%20data%0Apoints.%20Recent%20advances%20have%20discussed%20CW-complexes%20as%20ideal%20learning%0Arepresentations%20for%20problems%20in%20cheminformatics.%20However%2C%20there%20is%20a%20lack%20of%0Aavailable%20machine%20learning%20methods%20suitable%20for%20learning%20on%20CW-complexes.%20In%0Athis%20paper%20we%20develop%20notions%20of%20convolution%20and%20attention%20that%20are%20well%0Adefined%20for%20CW-complexes.%20These%20notions%20enable%20us%20to%20create%20the%20first%20Hodge%0Ainformed%20neural%20network%20that%20can%20receive%20a%20CW-complex%20as%20input.%20We%20illustrate%0Aand%20interpret%20this%20framework%20in%20the%20context%20of%20supervised%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCW-CNN%2520%2526%2520CW-AN%253A%2520Convolutional%2520Networks%2520and%2520Attention%2520Networks%2520for%250A%2520%2520CW-Complexes%26entry.906535625%3DRahul%2520Khorana%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520for%2520learning%2520on%2520CW-complex%2520structured%2520data%250Apoints.%2520Recent%2520advances%2520have%2520discussed%2520CW-complexes%2520as%2520ideal%2520learning%250Arepresentations%2520for%2520problems%2520in%2520cheminformatics.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%250Aavailable%2520machine%2520learning%2520methods%2520suitable%2520for%2520learning%2520on%2520CW-complexes.%2520In%250Athis%2520paper%2520we%2520develop%2520notions%2520of%2520convolution%2520and%2520attention%2520that%2520are%2520well%250Adefined%2520for%2520CW-complexes.%2520These%2520notions%2520enable%2520us%2520to%2520create%2520the%2520first%2520Hodge%250Ainformed%2520neural%2520network%2520that%2520can%2520receive%2520a%2520CW-complex%2520as%2520input.%2520We%2520illustrate%250Aand%2520interpret%2520this%2520framework%2520in%2520the%2520context%2520of%2520supervised%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CW-CNN%20%26%20CW-AN%3A%20Convolutional%20Networks%20and%20Attention%20Networks%20for%0A%20%20CW-Complexes&entry.906535625=Rahul%20Khorana&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20for%20learning%20on%20CW-complex%20structured%20data%0Apoints.%20Recent%20advances%20have%20discussed%20CW-complexes%20as%20ideal%20learning%0Arepresentations%20for%20problems%20in%20cheminformatics.%20However%2C%20there%20is%20a%20lack%20of%0Aavailable%20machine%20learning%20methods%20suitable%20for%20learning%20on%20CW-complexes.%20In%0Athis%20paper%20we%20develop%20notions%20of%20convolution%20and%20attention%20that%20are%20well%0Adefined%20for%20CW-complexes.%20These%20notions%20enable%20us%20to%20create%20the%20first%20Hodge%0Ainformed%20neural%20network%20that%20can%20receive%20a%20CW-complex%20as%20input.%20We%20illustrate%0Aand%20interpret%20this%20framework%20in%20the%20context%20of%20supervised%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16686v2&entry.124074799=Read"},
{"title": "Automatic Robot Hand-Eye Calibration Enabled by Learning-Based 3D Vision", "author": "Leihui Li and Xingyu Yang and Riwei Wang and Xuping Zhang", "abstract": "  Hand-eye calibration, as a fundamental task in vision-based robotic systems,\naims to estimate the transformation matrix between the coordinate frame of the\ncamera and the robot flange. Most approaches to hand-eye calibration rely on\nexternal markers or human assistance. We proposed Look at Robot Base Once\n(LRBO), a novel methodology that addresses the hand-eye calibration problem\nwithout external calibration objects or human support, but with the robot base.\nUsing point clouds of the robot base, a transformation matrix from the\ncoordinate frame of the camera to the robot base is established as I=AXB. To\nthis end, we exploit learning-based 3D detection and registration algorithms to\nestimate the location and orientation of the robot base. The robustness and\naccuracy of the method are quantified by ground-truth-based evaluation, and the\naccuracy result is compared with other 3D vision-based calibration methods. To\nassess the feasibility of our methodology, we carried out experiments utilizing\na low-cost structured light scanner across varying joint configurations and\ngroups of experiments. The proposed hand-eye calibration method achieved a\ntranslation deviation of 0.930 mm and a rotation deviation of 0.265 degrees\naccording to the experimental results. Additionally, the 3D reconstruction\nexperiments demonstrated a rotation error of 0.994 degrees and a position error\nof 1.697 mm. Moreover, our method offers the potential to be completed in 1\nsecond, which is the fastest compared to other 3D hand-eye calibration methods.\nCode is released at github.com/leihui6/LRBO.\n", "link": "http://arxiv.org/abs/2311.01335v3", "date": "2024-09-05", "relevancy": 2.3748, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6158}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5848}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Robot%20Hand-Eye%20Calibration%20Enabled%20by%20Learning-Based%203D%20Vision&body=Title%3A%20Automatic%20Robot%20Hand-Eye%20Calibration%20Enabled%20by%20Learning-Based%203D%20Vision%0AAuthor%3A%20Leihui%20Li%20and%20Xingyu%20Yang%20and%20Riwei%20Wang%20and%20Xuping%20Zhang%0AAbstract%3A%20%20%20Hand-eye%20calibration%2C%20as%20a%20fundamental%20task%20in%20vision-based%20robotic%20systems%2C%0Aaims%20to%20estimate%20the%20transformation%20matrix%20between%20the%20coordinate%20frame%20of%20the%0Acamera%20and%20the%20robot%20flange.%20Most%20approaches%20to%20hand-eye%20calibration%20rely%20on%0Aexternal%20markers%20or%20human%20assistance.%20We%20proposed%20Look%20at%20Robot%20Base%20Once%0A%28LRBO%29%2C%20a%20novel%20methodology%20that%20addresses%20the%20hand-eye%20calibration%20problem%0Awithout%20external%20calibration%20objects%20or%20human%20support%2C%20but%20with%20the%20robot%20base.%0AUsing%20point%20clouds%20of%20the%20robot%20base%2C%20a%20transformation%20matrix%20from%20the%0Acoordinate%20frame%20of%20the%20camera%20to%20the%20robot%20base%20is%20established%20as%20I%3DAXB.%20To%0Athis%20end%2C%20we%20exploit%20learning-based%203D%20detection%20and%20registration%20algorithms%20to%0Aestimate%20the%20location%20and%20orientation%20of%20the%20robot%20base.%20The%20robustness%20and%0Aaccuracy%20of%20the%20method%20are%20quantified%20by%20ground-truth-based%20evaluation%2C%20and%20the%0Aaccuracy%20result%20is%20compared%20with%20other%203D%20vision-based%20calibration%20methods.%20To%0Aassess%20the%20feasibility%20of%20our%20methodology%2C%20we%20carried%20out%20experiments%20utilizing%0Aa%20low-cost%20structured%20light%20scanner%20across%20varying%20joint%20configurations%20and%0Agroups%20of%20experiments.%20The%20proposed%20hand-eye%20calibration%20method%20achieved%20a%0Atranslation%20deviation%20of%200.930%20mm%20and%20a%20rotation%20deviation%20of%200.265%20degrees%0Aaccording%20to%20the%20experimental%20results.%20Additionally%2C%20the%203D%20reconstruction%0Aexperiments%20demonstrated%20a%20rotation%20error%20of%200.994%20degrees%20and%20a%20position%20error%0Aof%201.697%20mm.%20Moreover%2C%20our%20method%20offers%20the%20potential%20to%20be%20completed%20in%201%0Asecond%2C%20which%20is%20the%20fastest%20compared%20to%20other%203D%20hand-eye%20calibration%20methods.%0ACode%20is%20released%20at%20github.com/leihui6/LRBO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Robot%2520Hand-Eye%2520Calibration%2520Enabled%2520by%2520Learning-Based%25203D%2520Vision%26entry.906535625%3DLeihui%2520Li%2520and%2520Xingyu%2520Yang%2520and%2520Riwei%2520Wang%2520and%2520Xuping%2520Zhang%26entry.1292438233%3D%2520%2520Hand-eye%2520calibration%252C%2520as%2520a%2520fundamental%2520task%2520in%2520vision-based%2520robotic%2520systems%252C%250Aaims%2520to%2520estimate%2520the%2520transformation%2520matrix%2520between%2520the%2520coordinate%2520frame%2520of%2520the%250Acamera%2520and%2520the%2520robot%2520flange.%2520Most%2520approaches%2520to%2520hand-eye%2520calibration%2520rely%2520on%250Aexternal%2520markers%2520or%2520human%2520assistance.%2520We%2520proposed%2520Look%2520at%2520Robot%2520Base%2520Once%250A%2528LRBO%2529%252C%2520a%2520novel%2520methodology%2520that%2520addresses%2520the%2520hand-eye%2520calibration%2520problem%250Awithout%2520external%2520calibration%2520objects%2520or%2520human%2520support%252C%2520but%2520with%2520the%2520robot%2520base.%250AUsing%2520point%2520clouds%2520of%2520the%2520robot%2520base%252C%2520a%2520transformation%2520matrix%2520from%2520the%250Acoordinate%2520frame%2520of%2520the%2520camera%2520to%2520the%2520robot%2520base%2520is%2520established%2520as%2520I%253DAXB.%2520To%250Athis%2520end%252C%2520we%2520exploit%2520learning-based%25203D%2520detection%2520and%2520registration%2520algorithms%2520to%250Aestimate%2520the%2520location%2520and%2520orientation%2520of%2520the%2520robot%2520base.%2520The%2520robustness%2520and%250Aaccuracy%2520of%2520the%2520method%2520are%2520quantified%2520by%2520ground-truth-based%2520evaluation%252C%2520and%2520the%250Aaccuracy%2520result%2520is%2520compared%2520with%2520other%25203D%2520vision-based%2520calibration%2520methods.%2520To%250Aassess%2520the%2520feasibility%2520of%2520our%2520methodology%252C%2520we%2520carried%2520out%2520experiments%2520utilizing%250Aa%2520low-cost%2520structured%2520light%2520scanner%2520across%2520varying%2520joint%2520configurations%2520and%250Agroups%2520of%2520experiments.%2520The%2520proposed%2520hand-eye%2520calibration%2520method%2520achieved%2520a%250Atranslation%2520deviation%2520of%25200.930%2520mm%2520and%2520a%2520rotation%2520deviation%2520of%25200.265%2520degrees%250Aaccording%2520to%2520the%2520experimental%2520results.%2520Additionally%252C%2520the%25203D%2520reconstruction%250Aexperiments%2520demonstrated%2520a%2520rotation%2520error%2520of%25200.994%2520degrees%2520and%2520a%2520position%2520error%250Aof%25201.697%2520mm.%2520Moreover%252C%2520our%2520method%2520offers%2520the%2520potential%2520to%2520be%2520completed%2520in%25201%250Asecond%252C%2520which%2520is%2520the%2520fastest%2520compared%2520to%2520other%25203D%2520hand-eye%2520calibration%2520methods.%250ACode%2520is%2520released%2520at%2520github.com/leihui6/LRBO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Robot%20Hand-Eye%20Calibration%20Enabled%20by%20Learning-Based%203D%20Vision&entry.906535625=Leihui%20Li%20and%20Xingyu%20Yang%20and%20Riwei%20Wang%20and%20Xuping%20Zhang&entry.1292438233=%20%20Hand-eye%20calibration%2C%20as%20a%20fundamental%20task%20in%20vision-based%20robotic%20systems%2C%0Aaims%20to%20estimate%20the%20transformation%20matrix%20between%20the%20coordinate%20frame%20of%20the%0Acamera%20and%20the%20robot%20flange.%20Most%20approaches%20to%20hand-eye%20calibration%20rely%20on%0Aexternal%20markers%20or%20human%20assistance.%20We%20proposed%20Look%20at%20Robot%20Base%20Once%0A%28LRBO%29%2C%20a%20novel%20methodology%20that%20addresses%20the%20hand-eye%20calibration%20problem%0Awithout%20external%20calibration%20objects%20or%20human%20support%2C%20but%20with%20the%20robot%20base.%0AUsing%20point%20clouds%20of%20the%20robot%20base%2C%20a%20transformation%20matrix%20from%20the%0Acoordinate%20frame%20of%20the%20camera%20to%20the%20robot%20base%20is%20established%20as%20I%3DAXB.%20To%0Athis%20end%2C%20we%20exploit%20learning-based%203D%20detection%20and%20registration%20algorithms%20to%0Aestimate%20the%20location%20and%20orientation%20of%20the%20robot%20base.%20The%20robustness%20and%0Aaccuracy%20of%20the%20method%20are%20quantified%20by%20ground-truth-based%20evaluation%2C%20and%20the%0Aaccuracy%20result%20is%20compared%20with%20other%203D%20vision-based%20calibration%20methods.%20To%0Aassess%20the%20feasibility%20of%20our%20methodology%2C%20we%20carried%20out%20experiments%20utilizing%0Aa%20low-cost%20structured%20light%20scanner%20across%20varying%20joint%20configurations%20and%0Agroups%20of%20experiments.%20The%20proposed%20hand-eye%20calibration%20method%20achieved%20a%0Atranslation%20deviation%20of%200.930%20mm%20and%20a%20rotation%20deviation%20of%200.265%20degrees%0Aaccording%20to%20the%20experimental%20results.%20Additionally%2C%20the%203D%20reconstruction%0Aexperiments%20demonstrated%20a%20rotation%20error%20of%200.994%20degrees%20and%20a%20position%20error%0Aof%201.697%20mm.%20Moreover%2C%20our%20method%20offers%20the%20potential%20to%20be%20completed%20in%201%0Asecond%2C%20which%20is%20the%20fastest%20compared%20to%20other%203D%20hand-eye%20calibration%20methods.%0ACode%20is%20released%20at%20github.com/leihui6/LRBO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01335v3&entry.124074799=Read"},
{"title": "Characterizing Massive Activations of Attention Mechanism in Graph\n  Neural Networks", "author": "Lorenzo Bini and Marco Sorbi and Stephane Marchand-Maillet", "abstract": "  Graph Neural Networks (GNNs) have become increasingly popular for effectively\nmodeling data with graph structures. Recently, attention mechanisms have been\nintegrated into GNNs to improve their ability to capture complex patterns. This\npaper presents the first comprehensive study revealing a critical, unexplored\nconsequence of this integration: the emergence of Massive Activations (MAs)\nwithin attention layers. We introduce a novel method for detecting and\nanalyzing MAs, focusing on edge features in different graph transformer\narchitectures. Our study assesses various GNN models using benchmark datasets,\nincluding ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing\nthe direct link between attention mechanisms and MAs generation in GNNs, (2)\ndeveloping a robust definition and detection method for MAs based on activation\nratio distributions, (3) introducing the Explicit Bias Term (EBT) as a\npotential countermeasure and exploring it as an adversarial framework to assess\nmodels robustness based on the presence or absence of MAs. Our findings\nhighlight the prevalence and impact of attention-induced MAs across different\narchitectures, such as GraphTransformer, GraphiT, and SAN. The study reveals\nthe complex interplay between attention mechanisms, model architecture, dataset\ncharacteristics, and MAs emergence, providing crucial insights for developing\nmore robust and reliable graph models.\n", "link": "http://arxiv.org/abs/2409.03463v1", "date": "2024-09-05", "relevancy": 2.3732, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4804}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Massive%20Activations%20of%20Attention%20Mechanism%20in%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Characterizing%20Massive%20Activations%20of%20Attention%20Mechanism%20in%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Lorenzo%20Bini%20and%20Marco%20Sorbi%20and%20Stephane%20Marchand-Maillet%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20increasingly%20popular%20for%20effectively%0Amodeling%20data%20with%20graph%20structures.%20Recently%2C%20attention%20mechanisms%20have%20been%0Aintegrated%20into%20GNNs%20to%20improve%20their%20ability%20to%20capture%20complex%20patterns.%20This%0Apaper%20presents%20the%20first%20comprehensive%20study%20revealing%20a%20critical%2C%20unexplored%0Aconsequence%20of%20this%20integration%3A%20the%20emergence%20of%20Massive%20Activations%20%28MAs%29%0Awithin%20attention%20layers.%20We%20introduce%20a%20novel%20method%20for%20detecting%20and%0Aanalyzing%20MAs%2C%20focusing%20on%20edge%20features%20in%20different%20graph%20transformer%0Aarchitectures.%20Our%20study%20assesses%20various%20GNN%20models%20using%20benchmark%20datasets%2C%0Aincluding%20ZINC%2C%20TOX21%2C%20and%20PROTEINS.%20Key%20contributions%20include%20%281%29%20establishing%0Athe%20direct%20link%20between%20attention%20mechanisms%20and%20MAs%20generation%20in%20GNNs%2C%20%282%29%0Adeveloping%20a%20robust%20definition%20and%20detection%20method%20for%20MAs%20based%20on%20activation%0Aratio%20distributions%2C%20%283%29%20introducing%20the%20Explicit%20Bias%20Term%20%28EBT%29%20as%20a%0Apotential%20countermeasure%20and%20exploring%20it%20as%20an%20adversarial%20framework%20to%20assess%0Amodels%20robustness%20based%20on%20the%20presence%20or%20absence%20of%20MAs.%20Our%20findings%0Ahighlight%20the%20prevalence%20and%20impact%20of%20attention-induced%20MAs%20across%20different%0Aarchitectures%2C%20such%20as%20GraphTransformer%2C%20GraphiT%2C%20and%20SAN.%20The%20study%20reveals%0Athe%20complex%20interplay%20between%20attention%20mechanisms%2C%20model%20architecture%2C%20dataset%0Acharacteristics%2C%20and%20MAs%20emergence%2C%20providing%20crucial%20insights%20for%20developing%0Amore%20robust%20and%20reliable%20graph%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Massive%2520Activations%2520of%2520Attention%2520Mechanism%2520in%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DLorenzo%2520Bini%2520and%2520Marco%2520Sorbi%2520and%2520Stephane%2520Marchand-Maillet%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520increasingly%2520popular%2520for%2520effectively%250Amodeling%2520data%2520with%2520graph%2520structures.%2520Recently%252C%2520attention%2520mechanisms%2520have%2520been%250Aintegrated%2520into%2520GNNs%2520to%2520improve%2520their%2520ability%2520to%2520capture%2520complex%2520patterns.%2520This%250Apaper%2520presents%2520the%2520first%2520comprehensive%2520study%2520revealing%2520a%2520critical%252C%2520unexplored%250Aconsequence%2520of%2520this%2520integration%253A%2520the%2520emergence%2520of%2520Massive%2520Activations%2520%2528MAs%2529%250Awithin%2520attention%2520layers.%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520detecting%2520and%250Aanalyzing%2520MAs%252C%2520focusing%2520on%2520edge%2520features%2520in%2520different%2520graph%2520transformer%250Aarchitectures.%2520Our%2520study%2520assesses%2520various%2520GNN%2520models%2520using%2520benchmark%2520datasets%252C%250Aincluding%2520ZINC%252C%2520TOX21%252C%2520and%2520PROTEINS.%2520Key%2520contributions%2520include%2520%25281%2529%2520establishing%250Athe%2520direct%2520link%2520between%2520attention%2520mechanisms%2520and%2520MAs%2520generation%2520in%2520GNNs%252C%2520%25282%2529%250Adeveloping%2520a%2520robust%2520definition%2520and%2520detection%2520method%2520for%2520MAs%2520based%2520on%2520activation%250Aratio%2520distributions%252C%2520%25283%2529%2520introducing%2520the%2520Explicit%2520Bias%2520Term%2520%2528EBT%2529%2520as%2520a%250Apotential%2520countermeasure%2520and%2520exploring%2520it%2520as%2520an%2520adversarial%2520framework%2520to%2520assess%250Amodels%2520robustness%2520based%2520on%2520the%2520presence%2520or%2520absence%2520of%2520MAs.%2520Our%2520findings%250Ahighlight%2520the%2520prevalence%2520and%2520impact%2520of%2520attention-induced%2520MAs%2520across%2520different%250Aarchitectures%252C%2520such%2520as%2520GraphTransformer%252C%2520GraphiT%252C%2520and%2520SAN.%2520The%2520study%2520reveals%250Athe%2520complex%2520interplay%2520between%2520attention%2520mechanisms%252C%2520model%2520architecture%252C%2520dataset%250Acharacteristics%252C%2520and%2520MAs%2520emergence%252C%2520providing%2520crucial%2520insights%2520for%2520developing%250Amore%2520robust%2520and%2520reliable%2520graph%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Massive%20Activations%20of%20Attention%20Mechanism%20in%20Graph%0A%20%20Neural%20Networks&entry.906535625=Lorenzo%20Bini%20and%20Marco%20Sorbi%20and%20Stephane%20Marchand-Maillet&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20increasingly%20popular%20for%20effectively%0Amodeling%20data%20with%20graph%20structures.%20Recently%2C%20attention%20mechanisms%20have%20been%0Aintegrated%20into%20GNNs%20to%20improve%20their%20ability%20to%20capture%20complex%20patterns.%20This%0Apaper%20presents%20the%20first%20comprehensive%20study%20revealing%20a%20critical%2C%20unexplored%0Aconsequence%20of%20this%20integration%3A%20the%20emergence%20of%20Massive%20Activations%20%28MAs%29%0Awithin%20attention%20layers.%20We%20introduce%20a%20novel%20method%20for%20detecting%20and%0Aanalyzing%20MAs%2C%20focusing%20on%20edge%20features%20in%20different%20graph%20transformer%0Aarchitectures.%20Our%20study%20assesses%20various%20GNN%20models%20using%20benchmark%20datasets%2C%0Aincluding%20ZINC%2C%20TOX21%2C%20and%20PROTEINS.%20Key%20contributions%20include%20%281%29%20establishing%0Athe%20direct%20link%20between%20attention%20mechanisms%20and%20MAs%20generation%20in%20GNNs%2C%20%282%29%0Adeveloping%20a%20robust%20definition%20and%20detection%20method%20for%20MAs%20based%20on%20activation%0Aratio%20distributions%2C%20%283%29%20introducing%20the%20Explicit%20Bias%20Term%20%28EBT%29%20as%20a%0Apotential%20countermeasure%20and%20exploring%20it%20as%20an%20adversarial%20framework%20to%20assess%0Amodels%20robustness%20based%20on%20the%20presence%20or%20absence%20of%20MAs.%20Our%20findings%0Ahighlight%20the%20prevalence%20and%20impact%20of%20attention-induced%20MAs%20across%20different%0Aarchitectures%2C%20such%20as%20GraphTransformer%2C%20GraphiT%2C%20and%20SAN.%20The%20study%20reveals%0Athe%20complex%20interplay%20between%20attention%20mechanisms%2C%20model%20architecture%2C%20dataset%0Acharacteristics%2C%20and%20MAs%20emergence%2C%20providing%20crucial%20insights%20for%20developing%0Amore%20robust%20and%20reliable%20graph%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03463v1&entry.124074799=Read"},
{"title": "ArtiFade: Learning to Generate High-quality Subject from Blemished\n  Images", "author": "Shuya Yang and Shaozhe Hao and Yukang Cao and Kwan-Yee K. Wong", "abstract": "  Subject-driven text-to-image generation has witnessed remarkable advancements\nin its ability to learn and capture characteristics of a subject using only a\nlimited number of images. However, existing methods commonly rely on\nhigh-quality images for training and may struggle to generate reasonable images\nwhen the input images are blemished by artifacts. This is primarily attributed\nto the inadequate capability of current techniques in distinguishing\nsubject-related features from disruptive artifacts. In this paper, we introduce\nArtiFade to tackle this issue and successfully generate high-quality\nartifact-free images from blemished datasets. Specifically, ArtiFade exploits\nfine-tuning of a pre-trained text-to-image model, aiming to remove artifacts.\nThe elimination of artifacts is achieved by utilizing a specialized dataset\nthat encompasses both unblemished images and their corresponding blemished\ncounterparts during fine-tuning. ArtiFade also ensures the preservation of the\noriginal generative capabilities inherent within the diffusion model, thereby\nenhancing the overall performance of subject-driven methods in generating\nhigh-quality and artifact-free images. We further devise evaluation benchmarks\ntailored for this task. Through extensive qualitative and quantitative\nexperiments, we demonstrate the generalizability of ArtiFade in effective\nartifact removal under both in-distribution and out-of-distribution scenarios.\n", "link": "http://arxiv.org/abs/2409.03745v1", "date": "2024-09-05", "relevancy": 2.3579, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5986}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5878}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtiFade%3A%20Learning%20to%20Generate%20High-quality%20Subject%20from%20Blemished%0A%20%20Images&body=Title%3A%20ArtiFade%3A%20Learning%20to%20Generate%20High-quality%20Subject%20from%20Blemished%0A%20%20Images%0AAuthor%3A%20Shuya%20Yang%20and%20Shaozhe%20Hao%20and%20Yukang%20Cao%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20%20%20Subject-driven%20text-to-image%20generation%20has%20witnessed%20remarkable%20advancements%0Ain%20its%20ability%20to%20learn%20and%20capture%20characteristics%20of%20a%20subject%20using%20only%20a%0Alimited%20number%20of%20images.%20However%2C%20existing%20methods%20commonly%20rely%20on%0Ahigh-quality%20images%20for%20training%20and%20may%20struggle%20to%20generate%20reasonable%20images%0Awhen%20the%20input%20images%20are%20blemished%20by%20artifacts.%20This%20is%20primarily%20attributed%0Ato%20the%20inadequate%20capability%20of%20current%20techniques%20in%20distinguishing%0Asubject-related%20features%20from%20disruptive%20artifacts.%20In%20this%20paper%2C%20we%20introduce%0AArtiFade%20to%20tackle%20this%20issue%20and%20successfully%20generate%20high-quality%0Aartifact-free%20images%20from%20blemished%20datasets.%20Specifically%2C%20ArtiFade%20exploits%0Afine-tuning%20of%20a%20pre-trained%20text-to-image%20model%2C%20aiming%20to%20remove%20artifacts.%0AThe%20elimination%20of%20artifacts%20is%20achieved%20by%20utilizing%20a%20specialized%20dataset%0Athat%20encompasses%20both%20unblemished%20images%20and%20their%20corresponding%20blemished%0Acounterparts%20during%20fine-tuning.%20ArtiFade%20also%20ensures%20the%20preservation%20of%20the%0Aoriginal%20generative%20capabilities%20inherent%20within%20the%20diffusion%20model%2C%20thereby%0Aenhancing%20the%20overall%20performance%20of%20subject-driven%20methods%20in%20generating%0Ahigh-quality%20and%20artifact-free%20images.%20We%20further%20devise%20evaluation%20benchmarks%0Atailored%20for%20this%20task.%20Through%20extensive%20qualitative%20and%20quantitative%0Aexperiments%2C%20we%20demonstrate%20the%20generalizability%20of%20ArtiFade%20in%20effective%0Aartifact%20removal%20under%20both%20in-distribution%20and%20out-of-distribution%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtiFade%253A%2520Learning%2520to%2520Generate%2520High-quality%2520Subject%2520from%2520Blemished%250A%2520%2520Images%26entry.906535625%3DShuya%2520Yang%2520and%2520Shaozhe%2520Hao%2520and%2520Yukang%2520Cao%2520and%2520Kwan-Yee%2520K.%2520Wong%26entry.1292438233%3D%2520%2520Subject-driven%2520text-to-image%2520generation%2520has%2520witnessed%2520remarkable%2520advancements%250Ain%2520its%2520ability%2520to%2520learn%2520and%2520capture%2520characteristics%2520of%2520a%2520subject%2520using%2520only%2520a%250Alimited%2520number%2520of%2520images.%2520However%252C%2520existing%2520methods%2520commonly%2520rely%2520on%250Ahigh-quality%2520images%2520for%2520training%2520and%2520may%2520struggle%2520to%2520generate%2520reasonable%2520images%250Awhen%2520the%2520input%2520images%2520are%2520blemished%2520by%2520artifacts.%2520This%2520is%2520primarily%2520attributed%250Ato%2520the%2520inadequate%2520capability%2520of%2520current%2520techniques%2520in%2520distinguishing%250Asubject-related%2520features%2520from%2520disruptive%2520artifacts.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AArtiFade%2520to%2520tackle%2520this%2520issue%2520and%2520successfully%2520generate%2520high-quality%250Aartifact-free%2520images%2520from%2520blemished%2520datasets.%2520Specifically%252C%2520ArtiFade%2520exploits%250Afine-tuning%2520of%2520a%2520pre-trained%2520text-to-image%2520model%252C%2520aiming%2520to%2520remove%2520artifacts.%250AThe%2520elimination%2520of%2520artifacts%2520is%2520achieved%2520by%2520utilizing%2520a%2520specialized%2520dataset%250Athat%2520encompasses%2520both%2520unblemished%2520images%2520and%2520their%2520corresponding%2520blemished%250Acounterparts%2520during%2520fine-tuning.%2520ArtiFade%2520also%2520ensures%2520the%2520preservation%2520of%2520the%250Aoriginal%2520generative%2520capabilities%2520inherent%2520within%2520the%2520diffusion%2520model%252C%2520thereby%250Aenhancing%2520the%2520overall%2520performance%2520of%2520subject-driven%2520methods%2520in%2520generating%250Ahigh-quality%2520and%2520artifact-free%2520images.%2520We%2520further%2520devise%2520evaluation%2520benchmarks%250Atailored%2520for%2520this%2520task.%2520Through%2520extensive%2520qualitative%2520and%2520quantitative%250Aexperiments%252C%2520we%2520demonstrate%2520the%2520generalizability%2520of%2520ArtiFade%2520in%2520effective%250Aartifact%2520removal%2520under%2520both%2520in-distribution%2520and%2520out-of-distribution%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtiFade%3A%20Learning%20to%20Generate%20High-quality%20Subject%20from%20Blemished%0A%20%20Images&entry.906535625=Shuya%20Yang%20and%20Shaozhe%20Hao%20and%20Yukang%20Cao%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=%20%20Subject-driven%20text-to-image%20generation%20has%20witnessed%20remarkable%20advancements%0Ain%20its%20ability%20to%20learn%20and%20capture%20characteristics%20of%20a%20subject%20using%20only%20a%0Alimited%20number%20of%20images.%20However%2C%20existing%20methods%20commonly%20rely%20on%0Ahigh-quality%20images%20for%20training%20and%20may%20struggle%20to%20generate%20reasonable%20images%0Awhen%20the%20input%20images%20are%20blemished%20by%20artifacts.%20This%20is%20primarily%20attributed%0Ato%20the%20inadequate%20capability%20of%20current%20techniques%20in%20distinguishing%0Asubject-related%20features%20from%20disruptive%20artifacts.%20In%20this%20paper%2C%20we%20introduce%0AArtiFade%20to%20tackle%20this%20issue%20and%20successfully%20generate%20high-quality%0Aartifact-free%20images%20from%20blemished%20datasets.%20Specifically%2C%20ArtiFade%20exploits%0Afine-tuning%20of%20a%20pre-trained%20text-to-image%20model%2C%20aiming%20to%20remove%20artifacts.%0AThe%20elimination%20of%20artifacts%20is%20achieved%20by%20utilizing%20a%20specialized%20dataset%0Athat%20encompasses%20both%20unblemished%20images%20and%20their%20corresponding%20blemished%0Acounterparts%20during%20fine-tuning.%20ArtiFade%20also%20ensures%20the%20preservation%20of%20the%0Aoriginal%20generative%20capabilities%20inherent%20within%20the%20diffusion%20model%2C%20thereby%0Aenhancing%20the%20overall%20performance%20of%20subject-driven%20methods%20in%20generating%0Ahigh-quality%20and%20artifact-free%20images.%20We%20further%20devise%20evaluation%20benchmarks%0Atailored%20for%20this%20task.%20Through%20extensive%20qualitative%20and%20quantitative%0Aexperiments%2C%20we%20demonstrate%20the%20generalizability%20of%20ArtiFade%20in%20effective%0Aartifact%20removal%20under%20both%20in-distribution%20and%20out-of-distribution%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03745v1&entry.124074799=Read"},
{"title": "Use of triplet loss for facial restoration in low-resolution images", "author": "Sebastian Pulgar and Domingo Mery", "abstract": "  In recent years, facial recognition (FR) models have become the most widely\nused biometric tool, achieving impressive results on numerous datasets.\nHowever, inherent hardware challenges or shooting distances often result in\nlow-resolution images, which significantly impact the performance of FR models.\nTo address this issue, several solutions have been proposed, including\nsuper-resolution (SR) models that generate highly realistic faces. Despite\nthese efforts, significant improvements in FR algorithms have not been\nachieved. We propose a novel SR model FTLGAN, which focuses on generating\nhigh-resolution images that preserve individual identities rather than merely\nimproving image quality, thereby maximizing the performance of FR models. The\nresults are compelling, demonstrating a mean value of d' 21% above the best\ncurrent state-of-the-art models, specifically having a value of d' = 1.099 and\nAUC = 0.78 for 14x14 pixels, d' = 2.112 and AUC = 0.92 for 28x28 pixels, and d'\n= 3.049 and AUC = 0.98 for 56x56 pixels. The contributions of this study are\nsignificant in several key areas. Firstly, a notable improvement in facial\nrecognition performance has been achieved in low-resolution images,\nspecifically at resolutions of 14x14, 28x28, and 56x56 pixels. Secondly, the\nenhancements demonstrated by FTLGAN show a consistent response across all\nresolutions, delivering outstanding performance uniformly, unlike other\ncomparative models. Thirdly, an innovative approach has been implemented using\ntriplet loss logic, enabling the training of the super-resolution model solely\nwith real images, contrasting with current models, and expanding potential\nreal-world applications. Lastly, this study introduces a novel model that\nspecifically addresses the challenge of improving classification performance in\nfacial recognition systems by integrating facial recognition quality as a loss\nduring model training.\n", "link": "http://arxiv.org/abs/2409.03530v1", "date": "2024-09-05", "relevancy": 2.3426, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6062}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5751}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Use%20of%20triplet%20loss%20for%20facial%20restoration%20in%20low-resolution%20images&body=Title%3A%20Use%20of%20triplet%20loss%20for%20facial%20restoration%20in%20low-resolution%20images%0AAuthor%3A%20Sebastian%20Pulgar%20and%20Domingo%20Mery%0AAbstract%3A%20%20%20In%20recent%20years%2C%20facial%20recognition%20%28FR%29%20models%20have%20become%20the%20most%20widely%0Aused%20biometric%20tool%2C%20achieving%20impressive%20results%20on%20numerous%20datasets.%0AHowever%2C%20inherent%20hardware%20challenges%20or%20shooting%20distances%20often%20result%20in%0Alow-resolution%20images%2C%20which%20significantly%20impact%20the%20performance%20of%20FR%20models.%0ATo%20address%20this%20issue%2C%20several%20solutions%20have%20been%20proposed%2C%20including%0Asuper-resolution%20%28SR%29%20models%20that%20generate%20highly%20realistic%20faces.%20Despite%0Athese%20efforts%2C%20significant%20improvements%20in%20FR%20algorithms%20have%20not%20been%0Aachieved.%20We%20propose%20a%20novel%20SR%20model%20FTLGAN%2C%20which%20focuses%20on%20generating%0Ahigh-resolution%20images%20that%20preserve%20individual%20identities%20rather%20than%20merely%0Aimproving%20image%20quality%2C%20thereby%20maximizing%20the%20performance%20of%20FR%20models.%20The%0Aresults%20are%20compelling%2C%20demonstrating%20a%20mean%20value%20of%20d%27%2021%25%20above%20the%20best%0Acurrent%20state-of-the-art%20models%2C%20specifically%20having%20a%20value%20of%20d%27%20%3D%201.099%20and%0AAUC%20%3D%200.78%20for%2014x14%20pixels%2C%20d%27%20%3D%202.112%20and%20AUC%20%3D%200.92%20for%2028x28%20pixels%2C%20and%20d%27%0A%3D%203.049%20and%20AUC%20%3D%200.98%20for%2056x56%20pixels.%20The%20contributions%20of%20this%20study%20are%0Asignificant%20in%20several%20key%20areas.%20Firstly%2C%20a%20notable%20improvement%20in%20facial%0Arecognition%20performance%20has%20been%20achieved%20in%20low-resolution%20images%2C%0Aspecifically%20at%20resolutions%20of%2014x14%2C%2028x28%2C%20and%2056x56%20pixels.%20Secondly%2C%20the%0Aenhancements%20demonstrated%20by%20FTLGAN%20show%20a%20consistent%20response%20across%20all%0Aresolutions%2C%20delivering%20outstanding%20performance%20uniformly%2C%20unlike%20other%0Acomparative%20models.%20Thirdly%2C%20an%20innovative%20approach%20has%20been%20implemented%20using%0Atriplet%20loss%20logic%2C%20enabling%20the%20training%20of%20the%20super-resolution%20model%20solely%0Awith%20real%20images%2C%20contrasting%20with%20current%20models%2C%20and%20expanding%20potential%0Areal-world%20applications.%20Lastly%2C%20this%20study%20introduces%20a%20novel%20model%20that%0Aspecifically%20addresses%20the%20challenge%20of%20improving%20classification%20performance%20in%0Afacial%20recognition%20systems%20by%20integrating%20facial%20recognition%20quality%20as%20a%20loss%0Aduring%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUse%2520of%2520triplet%2520loss%2520for%2520facial%2520restoration%2520in%2520low-resolution%2520images%26entry.906535625%3DSebastian%2520Pulgar%2520and%2520Domingo%2520Mery%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520facial%2520recognition%2520%2528FR%2529%2520models%2520have%2520become%2520the%2520most%2520widely%250Aused%2520biometric%2520tool%252C%2520achieving%2520impressive%2520results%2520on%2520numerous%2520datasets.%250AHowever%252C%2520inherent%2520hardware%2520challenges%2520or%2520shooting%2520distances%2520often%2520result%2520in%250Alow-resolution%2520images%252C%2520which%2520significantly%2520impact%2520the%2520performance%2520of%2520FR%2520models.%250ATo%2520address%2520this%2520issue%252C%2520several%2520solutions%2520have%2520been%2520proposed%252C%2520including%250Asuper-resolution%2520%2528SR%2529%2520models%2520that%2520generate%2520highly%2520realistic%2520faces.%2520Despite%250Athese%2520efforts%252C%2520significant%2520improvements%2520in%2520FR%2520algorithms%2520have%2520not%2520been%250Aachieved.%2520We%2520propose%2520a%2520novel%2520SR%2520model%2520FTLGAN%252C%2520which%2520focuses%2520on%2520generating%250Ahigh-resolution%2520images%2520that%2520preserve%2520individual%2520identities%2520rather%2520than%2520merely%250Aimproving%2520image%2520quality%252C%2520thereby%2520maximizing%2520the%2520performance%2520of%2520FR%2520models.%2520The%250Aresults%2520are%2520compelling%252C%2520demonstrating%2520a%2520mean%2520value%2520of%2520d%2527%252021%2525%2520above%2520the%2520best%250Acurrent%2520state-of-the-art%2520models%252C%2520specifically%2520having%2520a%2520value%2520of%2520d%2527%2520%253D%25201.099%2520and%250AAUC%2520%253D%25200.78%2520for%252014x14%2520pixels%252C%2520d%2527%2520%253D%25202.112%2520and%2520AUC%2520%253D%25200.92%2520for%252028x28%2520pixels%252C%2520and%2520d%2527%250A%253D%25203.049%2520and%2520AUC%2520%253D%25200.98%2520for%252056x56%2520pixels.%2520The%2520contributions%2520of%2520this%2520study%2520are%250Asignificant%2520in%2520several%2520key%2520areas.%2520Firstly%252C%2520a%2520notable%2520improvement%2520in%2520facial%250Arecognition%2520performance%2520has%2520been%2520achieved%2520in%2520low-resolution%2520images%252C%250Aspecifically%2520at%2520resolutions%2520of%252014x14%252C%252028x28%252C%2520and%252056x56%2520pixels.%2520Secondly%252C%2520the%250Aenhancements%2520demonstrated%2520by%2520FTLGAN%2520show%2520a%2520consistent%2520response%2520across%2520all%250Aresolutions%252C%2520delivering%2520outstanding%2520performance%2520uniformly%252C%2520unlike%2520other%250Acomparative%2520models.%2520Thirdly%252C%2520an%2520innovative%2520approach%2520has%2520been%2520implemented%2520using%250Atriplet%2520loss%2520logic%252C%2520enabling%2520the%2520training%2520of%2520the%2520super-resolution%2520model%2520solely%250Awith%2520real%2520images%252C%2520contrasting%2520with%2520current%2520models%252C%2520and%2520expanding%2520potential%250Areal-world%2520applications.%2520Lastly%252C%2520this%2520study%2520introduces%2520a%2520novel%2520model%2520that%250Aspecifically%2520addresses%2520the%2520challenge%2520of%2520improving%2520classification%2520performance%2520in%250Afacial%2520recognition%2520systems%2520by%2520integrating%2520facial%2520recognition%2520quality%2520as%2520a%2520loss%250Aduring%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Use%20of%20triplet%20loss%20for%20facial%20restoration%20in%20low-resolution%20images&entry.906535625=Sebastian%20Pulgar%20and%20Domingo%20Mery&entry.1292438233=%20%20In%20recent%20years%2C%20facial%20recognition%20%28FR%29%20models%20have%20become%20the%20most%20widely%0Aused%20biometric%20tool%2C%20achieving%20impressive%20results%20on%20numerous%20datasets.%0AHowever%2C%20inherent%20hardware%20challenges%20or%20shooting%20distances%20often%20result%20in%0Alow-resolution%20images%2C%20which%20significantly%20impact%20the%20performance%20of%20FR%20models.%0ATo%20address%20this%20issue%2C%20several%20solutions%20have%20been%20proposed%2C%20including%0Asuper-resolution%20%28SR%29%20models%20that%20generate%20highly%20realistic%20faces.%20Despite%0Athese%20efforts%2C%20significant%20improvements%20in%20FR%20algorithms%20have%20not%20been%0Aachieved.%20We%20propose%20a%20novel%20SR%20model%20FTLGAN%2C%20which%20focuses%20on%20generating%0Ahigh-resolution%20images%20that%20preserve%20individual%20identities%20rather%20than%20merely%0Aimproving%20image%20quality%2C%20thereby%20maximizing%20the%20performance%20of%20FR%20models.%20The%0Aresults%20are%20compelling%2C%20demonstrating%20a%20mean%20value%20of%20d%27%2021%25%20above%20the%20best%0Acurrent%20state-of-the-art%20models%2C%20specifically%20having%20a%20value%20of%20d%27%20%3D%201.099%20and%0AAUC%20%3D%200.78%20for%2014x14%20pixels%2C%20d%27%20%3D%202.112%20and%20AUC%20%3D%200.92%20for%2028x28%20pixels%2C%20and%20d%27%0A%3D%203.049%20and%20AUC%20%3D%200.98%20for%2056x56%20pixels.%20The%20contributions%20of%20this%20study%20are%0Asignificant%20in%20several%20key%20areas.%20Firstly%2C%20a%20notable%20improvement%20in%20facial%0Arecognition%20performance%20has%20been%20achieved%20in%20low-resolution%20images%2C%0Aspecifically%20at%20resolutions%20of%2014x14%2C%2028x28%2C%20and%2056x56%20pixels.%20Secondly%2C%20the%0Aenhancements%20demonstrated%20by%20FTLGAN%20show%20a%20consistent%20response%20across%20all%0Aresolutions%2C%20delivering%20outstanding%20performance%20uniformly%2C%20unlike%20other%0Acomparative%20models.%20Thirdly%2C%20an%20innovative%20approach%20has%20been%20implemented%20using%0Atriplet%20loss%20logic%2C%20enabling%20the%20training%20of%20the%20super-resolution%20model%20solely%0Awith%20real%20images%2C%20contrasting%20with%20current%20models%2C%20and%20expanding%20potential%0Areal-world%20applications.%20Lastly%2C%20this%20study%20introduces%20a%20novel%20model%20that%0Aspecifically%20addresses%20the%20challenge%20of%20improving%20classification%20performance%20in%0Afacial%20recognition%20systems%20by%20integrating%20facial%20recognition%20quality%20as%20a%20loss%0Aduring%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03530v1&entry.124074799=Read"},
{"title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild", "author": "Yuntian Deng and Wenting Zhao and Jack Hessel and Xiang Ren and Claire Cardie and Yejin Choi", "abstract": "  The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis's utility through three\ncase studies: facilitating chatbot misuse research, visualizing and comparing\ntopic distributions across datasets, and characterizing user-specific\nconversation patterns. WildVis is open-source and designed to be extendable,\nsupporting additional datasets and customized search and visualization\nfunctionalities.\n", "link": "http://arxiv.org/abs/2409.03753v1", "date": "2024-09-05", "relevancy": 2.2946, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4649}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4649}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildVis%3A%20Open%20Source%20Visualizer%20for%20Million-Scale%20Chat%20Logs%20in%20the%20Wild&body=Title%3A%20WildVis%3A%20Open%20Source%20Visualizer%20for%20Million-Scale%20Chat%20Logs%20in%20the%20Wild%0AAuthor%3A%20Yuntian%20Deng%20and%20Wenting%20Zhao%20and%20Jack%20Hessel%20and%20Xiang%20Ren%20and%20Claire%20Cardie%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20The%20increasing%20availability%20of%20real-world%20conversation%20data%20offers%20exciting%0Aopportunities%20for%20researchers%20to%20study%20user-chatbot%20interactions.%20However%2C%20the%0Asheer%20volume%20of%20this%20data%20makes%20manually%20examining%20individual%20conversations%0Aimpractical.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20WildVis%2C%20an%20interactive%0Atool%20that%20enables%20fast%2C%20versatile%2C%20and%20large-scale%20conversation%20analysis.%0AWildVis%20provides%20search%20and%20visualization%20capabilities%20in%20the%20text%20and%0Aembedding%20spaces%20based%20on%20a%20list%20of%20criteria.%20To%20manage%20million-scale%20datasets%2C%0Awe%20implemented%20optimizations%20including%20search%20index%20construction%2C%20embedding%0Aprecomputation%20and%20compression%2C%20and%20caching%20to%20ensure%20responsive%20user%0Ainteractions%20within%20seconds.%20We%20demonstrate%20WildVis%27s%20utility%20through%20three%0Acase%20studies%3A%20facilitating%20chatbot%20misuse%20research%2C%20visualizing%20and%20comparing%0Atopic%20distributions%20across%20datasets%2C%20and%20characterizing%20user-specific%0Aconversation%20patterns.%20WildVis%20is%20open-source%20and%20designed%20to%20be%20extendable%2C%0Asupporting%20additional%20datasets%20and%20customized%20search%20and%20visualization%0Afunctionalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildVis%253A%2520Open%2520Source%2520Visualizer%2520for%2520Million-Scale%2520Chat%2520Logs%2520in%2520the%2520Wild%26entry.906535625%3DYuntian%2520Deng%2520and%2520Wenting%2520Zhao%2520and%2520Jack%2520Hessel%2520and%2520Xiang%2520Ren%2520and%2520Claire%2520Cardie%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520The%2520increasing%2520availability%2520of%2520real-world%2520conversation%2520data%2520offers%2520exciting%250Aopportunities%2520for%2520researchers%2520to%2520study%2520user-chatbot%2520interactions.%2520However%252C%2520the%250Asheer%2520volume%2520of%2520this%2520data%2520makes%2520manually%2520examining%2520individual%2520conversations%250Aimpractical.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520WildVis%252C%2520an%2520interactive%250Atool%2520that%2520enables%2520fast%252C%2520versatile%252C%2520and%2520large-scale%2520conversation%2520analysis.%250AWildVis%2520provides%2520search%2520and%2520visualization%2520capabilities%2520in%2520the%2520text%2520and%250Aembedding%2520spaces%2520based%2520on%2520a%2520list%2520of%2520criteria.%2520To%2520manage%2520million-scale%2520datasets%252C%250Awe%2520implemented%2520optimizations%2520including%2520search%2520index%2520construction%252C%2520embedding%250Aprecomputation%2520and%2520compression%252C%2520and%2520caching%2520to%2520ensure%2520responsive%2520user%250Ainteractions%2520within%2520seconds.%2520We%2520demonstrate%2520WildVis%2527s%2520utility%2520through%2520three%250Acase%2520studies%253A%2520facilitating%2520chatbot%2520misuse%2520research%252C%2520visualizing%2520and%2520comparing%250Atopic%2520distributions%2520across%2520datasets%252C%2520and%2520characterizing%2520user-specific%250Aconversation%2520patterns.%2520WildVis%2520is%2520open-source%2520and%2520designed%2520to%2520be%2520extendable%252C%250Asupporting%2520additional%2520datasets%2520and%2520customized%2520search%2520and%2520visualization%250Afunctionalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildVis%3A%20Open%20Source%20Visualizer%20for%20Million-Scale%20Chat%20Logs%20in%20the%20Wild&entry.906535625=Yuntian%20Deng%20and%20Wenting%20Zhao%20and%20Jack%20Hessel%20and%20Xiang%20Ren%20and%20Claire%20Cardie%20and%20Yejin%20Choi&entry.1292438233=%20%20The%20increasing%20availability%20of%20real-world%20conversation%20data%20offers%20exciting%0Aopportunities%20for%20researchers%20to%20study%20user-chatbot%20interactions.%20However%2C%20the%0Asheer%20volume%20of%20this%20data%20makes%20manually%20examining%20individual%20conversations%0Aimpractical.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20WildVis%2C%20an%20interactive%0Atool%20that%20enables%20fast%2C%20versatile%2C%20and%20large-scale%20conversation%20analysis.%0AWildVis%20provides%20search%20and%20visualization%20capabilities%20in%20the%20text%20and%0Aembedding%20spaces%20based%20on%20a%20list%20of%20criteria.%20To%20manage%20million-scale%20datasets%2C%0Awe%20implemented%20optimizations%20including%20search%20index%20construction%2C%20embedding%0Aprecomputation%20and%20compression%2C%20and%20caching%20to%20ensure%20responsive%20user%0Ainteractions%20within%20seconds.%20We%20demonstrate%20WildVis%27s%20utility%20through%20three%0Acase%20studies%3A%20facilitating%20chatbot%20misuse%20research%2C%20visualizing%20and%20comparing%0Atopic%20distributions%20across%20datasets%2C%20and%20characterizing%20user-specific%0Aconversation%20patterns.%20WildVis%20is%20open-source%20and%20designed%20to%20be%20extendable%2C%0Asupporting%20additional%20datasets%20and%20customized%20search%20and%20visualization%0Afunctionalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03753v1&entry.124074799=Read"},
{"title": "Prediction Accuracy & Reliability: Classification and Object\n  Localization under Distribution Shift", "author": "Fabian Diet and Moussa Kassem Sbeyti and Michelle Karg", "abstract": "  Natural distribution shift causes a deterioration in the perception\nperformance of convolutional neural networks (CNNs). This comprehensive\nanalysis for real-world traffic data addresses: 1) investigating the effect of\nnatural distribution shift and weather augmentations on both detection quality\nand confidence estimation, 2) evaluating model performance for both\nclassification and object localization, and 3) benchmarking two common\nuncertainty quantification methods - Ensembles and different variants of\nMonte-Carlo (MC) Dropout - under natural and close-to-natural distribution\nshift. For this purpose, a novel dataset has been curated from publicly\navailable autonomous driving datasets. The in-distribution (ID) data is based\non cutouts of a single object, for which both class and bounding box\nannotations are available. The six distribution-shift datasets cover adverse\nweather scenarios, simulated rain and fog, corner cases, and\nout-of-distribution data. A granular analysis of CNNs under distribution shift\nallows to quantize the impact of different types of shifts on both, task\nperformance and confidence estimation: ConvNeXt-Tiny is more robust than\nEfficientNet-B0; heavy rain degrades classification stronger than localization,\ncontrary to heavy fog; integrating MC-Dropout into selected layers only has the\npotential to enhance task performance and confidence estimation, whereby the\nidentification of these layers depends on the type of distribution shift and\nthe considered task.\n", "link": "http://arxiv.org/abs/2409.03543v1", "date": "2024-09-05", "relevancy": 2.2885, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5963}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20Accuracy%20%26%20Reliability%3A%20Classification%20and%20Object%0A%20%20Localization%20under%20Distribution%20Shift&body=Title%3A%20Prediction%20Accuracy%20%26%20Reliability%3A%20Classification%20and%20Object%0A%20%20Localization%20under%20Distribution%20Shift%0AAuthor%3A%20Fabian%20Diet%20and%20Moussa%20Kassem%20Sbeyti%20and%20Michelle%20Karg%0AAbstract%3A%20%20%20Natural%20distribution%20shift%20causes%20a%20deterioration%20in%20the%20perception%0Aperformance%20of%20convolutional%20neural%20networks%20%28CNNs%29.%20This%20comprehensive%0Aanalysis%20for%20real-world%20traffic%20data%20addresses%3A%201%29%20investigating%20the%20effect%20of%0Anatural%20distribution%20shift%20and%20weather%20augmentations%20on%20both%20detection%20quality%0Aand%20confidence%20estimation%2C%202%29%20evaluating%20model%20performance%20for%20both%0Aclassification%20and%20object%20localization%2C%20and%203%29%20benchmarking%20two%20common%0Auncertainty%20quantification%20methods%20-%20Ensembles%20and%20different%20variants%20of%0AMonte-Carlo%20%28MC%29%20Dropout%20-%20under%20natural%20and%20close-to-natural%20distribution%0Ashift.%20For%20this%20purpose%2C%20a%20novel%20dataset%20has%20been%20curated%20from%20publicly%0Aavailable%20autonomous%20driving%20datasets.%20The%20in-distribution%20%28ID%29%20data%20is%20based%0Aon%20cutouts%20of%20a%20single%20object%2C%20for%20which%20both%20class%20and%20bounding%20box%0Aannotations%20are%20available.%20The%20six%20distribution-shift%20datasets%20cover%20adverse%0Aweather%20scenarios%2C%20simulated%20rain%20and%20fog%2C%20corner%20cases%2C%20and%0Aout-of-distribution%20data.%20A%20granular%20analysis%20of%20CNNs%20under%20distribution%20shift%0Aallows%20to%20quantize%20the%20impact%20of%20different%20types%20of%20shifts%20on%20both%2C%20task%0Aperformance%20and%20confidence%20estimation%3A%20ConvNeXt-Tiny%20is%20more%20robust%20than%0AEfficientNet-B0%3B%20heavy%20rain%20degrades%20classification%20stronger%20than%20localization%2C%0Acontrary%20to%20heavy%20fog%3B%20integrating%20MC-Dropout%20into%20selected%20layers%20only%20has%20the%0Apotential%20to%20enhance%20task%20performance%20and%20confidence%20estimation%2C%20whereby%20the%0Aidentification%20of%20these%20layers%20depends%20on%20the%20type%20of%20distribution%20shift%20and%0Athe%20considered%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520Accuracy%2520%2526%2520Reliability%253A%2520Classification%2520and%2520Object%250A%2520%2520Localization%2520under%2520Distribution%2520Shift%26entry.906535625%3DFabian%2520Diet%2520and%2520Moussa%2520Kassem%2520Sbeyti%2520and%2520Michelle%2520Karg%26entry.1292438233%3D%2520%2520Natural%2520distribution%2520shift%2520causes%2520a%2520deterioration%2520in%2520the%2520perception%250Aperformance%2520of%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520This%2520comprehensive%250Aanalysis%2520for%2520real-world%2520traffic%2520data%2520addresses%253A%25201%2529%2520investigating%2520the%2520effect%2520of%250Anatural%2520distribution%2520shift%2520and%2520weather%2520augmentations%2520on%2520both%2520detection%2520quality%250Aand%2520confidence%2520estimation%252C%25202%2529%2520evaluating%2520model%2520performance%2520for%2520both%250Aclassification%2520and%2520object%2520localization%252C%2520and%25203%2529%2520benchmarking%2520two%2520common%250Auncertainty%2520quantification%2520methods%2520-%2520Ensembles%2520and%2520different%2520variants%2520of%250AMonte-Carlo%2520%2528MC%2529%2520Dropout%2520-%2520under%2520natural%2520and%2520close-to-natural%2520distribution%250Ashift.%2520For%2520this%2520purpose%252C%2520a%2520novel%2520dataset%2520has%2520been%2520curated%2520from%2520publicly%250Aavailable%2520autonomous%2520driving%2520datasets.%2520The%2520in-distribution%2520%2528ID%2529%2520data%2520is%2520based%250Aon%2520cutouts%2520of%2520a%2520single%2520object%252C%2520for%2520which%2520both%2520class%2520and%2520bounding%2520box%250Aannotations%2520are%2520available.%2520The%2520six%2520distribution-shift%2520datasets%2520cover%2520adverse%250Aweather%2520scenarios%252C%2520simulated%2520rain%2520and%2520fog%252C%2520corner%2520cases%252C%2520and%250Aout-of-distribution%2520data.%2520A%2520granular%2520analysis%2520of%2520CNNs%2520under%2520distribution%2520shift%250Aallows%2520to%2520quantize%2520the%2520impact%2520of%2520different%2520types%2520of%2520shifts%2520on%2520both%252C%2520task%250Aperformance%2520and%2520confidence%2520estimation%253A%2520ConvNeXt-Tiny%2520is%2520more%2520robust%2520than%250AEfficientNet-B0%253B%2520heavy%2520rain%2520degrades%2520classification%2520stronger%2520than%2520localization%252C%250Acontrary%2520to%2520heavy%2520fog%253B%2520integrating%2520MC-Dropout%2520into%2520selected%2520layers%2520only%2520has%2520the%250Apotential%2520to%2520enhance%2520task%2520performance%2520and%2520confidence%2520estimation%252C%2520whereby%2520the%250Aidentification%2520of%2520these%2520layers%2520depends%2520on%2520the%2520type%2520of%2520distribution%2520shift%2520and%250Athe%2520considered%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20Accuracy%20%26%20Reliability%3A%20Classification%20and%20Object%0A%20%20Localization%20under%20Distribution%20Shift&entry.906535625=Fabian%20Diet%20and%20Moussa%20Kassem%20Sbeyti%20and%20Michelle%20Karg&entry.1292438233=%20%20Natural%20distribution%20shift%20causes%20a%20deterioration%20in%20the%20perception%0Aperformance%20of%20convolutional%20neural%20networks%20%28CNNs%29.%20This%20comprehensive%0Aanalysis%20for%20real-world%20traffic%20data%20addresses%3A%201%29%20investigating%20the%20effect%20of%0Anatural%20distribution%20shift%20and%20weather%20augmentations%20on%20both%20detection%20quality%0Aand%20confidence%20estimation%2C%202%29%20evaluating%20model%20performance%20for%20both%0Aclassification%20and%20object%20localization%2C%20and%203%29%20benchmarking%20two%20common%0Auncertainty%20quantification%20methods%20-%20Ensembles%20and%20different%20variants%20of%0AMonte-Carlo%20%28MC%29%20Dropout%20-%20under%20natural%20and%20close-to-natural%20distribution%0Ashift.%20For%20this%20purpose%2C%20a%20novel%20dataset%20has%20been%20curated%20from%20publicly%0Aavailable%20autonomous%20driving%20datasets.%20The%20in-distribution%20%28ID%29%20data%20is%20based%0Aon%20cutouts%20of%20a%20single%20object%2C%20for%20which%20both%20class%20and%20bounding%20box%0Aannotations%20are%20available.%20The%20six%20distribution-shift%20datasets%20cover%20adverse%0Aweather%20scenarios%2C%20simulated%20rain%20and%20fog%2C%20corner%20cases%2C%20and%0Aout-of-distribution%20data.%20A%20granular%20analysis%20of%20CNNs%20under%20distribution%20shift%0Aallows%20to%20quantize%20the%20impact%20of%20different%20types%20of%20shifts%20on%20both%2C%20task%0Aperformance%20and%20confidence%20estimation%3A%20ConvNeXt-Tiny%20is%20more%20robust%20than%0AEfficientNet-B0%3B%20heavy%20rain%20degrades%20classification%20stronger%20than%20localization%2C%0Acontrary%20to%20heavy%20fog%3B%20integrating%20MC-Dropout%20into%20selected%20layers%20only%20has%20the%0Apotential%20to%20enhance%20task%20performance%20and%20confidence%20estimation%2C%20whereby%20the%0Aidentification%20of%20these%20layers%20depends%20on%20the%20type%20of%20distribution%20shift%20and%0Athe%20considered%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03543v1&entry.124074799=Read"},
{"title": "SegTalker: Segmentation-based Talking Face Generation with Mask-guided\n  Local Editing", "author": "Lingyu Xiong and Xize Cheng and Jintao Tan and Xianjia Wu and Xiandong Li and Lei Zhu and Fei Ma and Minglei Li and Huang Xu and Zhihu Hu", "abstract": "  Audio-driven talking face generation aims to synthesize video with lip\nmovements synchronized to input audio. However, current generative techniques\nface challenges in preserving intricate regional textures (skin, teeth). To\naddress the aforementioned challenges, we propose a novel framework called\nSegTalker to decouple lip movements and image textures by introducing\nsegmentation as intermediate representation. Specifically, given the mask of\nimage employed by a parsing network, we first leverage the speech to drive the\nmask and generate talking segmentation. Then we disentangle semantic regions of\nimage into style codes using a mask-guided encoder. Ultimately, we inject the\npreviously generated talking segmentation and style codes into a mask-guided\nStyleGAN to synthesize video frame. In this way, most of textures are fully\npreserved. Moreover, our approach can inherently achieve background separation\nand facilitate mask-guided facial local editing. In particular, by editing the\nmask and swapping the region textures from a given reference image (e.g. hair,\nlip, eyebrows), our approach enables facial editing seamlessly when generating\ntalking face video. Experiments demonstrate that our proposed approach can\neffectively preserve texture details and generate temporally consistent video\nwhile remaining competitive in lip synchronization. Quantitative and\nqualitative results on the HDTF and MEAD datasets illustrate the superior\nperformance of our method over existing methods.\n", "link": "http://arxiv.org/abs/2409.03605v1", "date": "2024-09-05", "relevancy": 2.2807, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6033}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5658}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegTalker%3A%20Segmentation-based%20Talking%20Face%20Generation%20with%20Mask-guided%0A%20%20Local%20Editing&body=Title%3A%20SegTalker%3A%20Segmentation-based%20Talking%20Face%20Generation%20with%20Mask-guided%0A%20%20Local%20Editing%0AAuthor%3A%20Lingyu%20Xiong%20and%20Xize%20Cheng%20and%20Jintao%20Tan%20and%20Xianjia%20Wu%20and%20Xiandong%20Li%20and%20Lei%20Zhu%20and%20Fei%20Ma%20and%20Minglei%20Li%20and%20Huang%20Xu%20and%20Zhihu%20Hu%0AAbstract%3A%20%20%20Audio-driven%20talking%20face%20generation%20aims%20to%20synthesize%20video%20with%20lip%0Amovements%20synchronized%20to%20input%20audio.%20However%2C%20current%20generative%20techniques%0Aface%20challenges%20in%20preserving%20intricate%20regional%20textures%20%28skin%2C%20teeth%29.%20To%0Aaddress%20the%20aforementioned%20challenges%2C%20we%20propose%20a%20novel%20framework%20called%0ASegTalker%20to%20decouple%20lip%20movements%20and%20image%20textures%20by%20introducing%0Asegmentation%20as%20intermediate%20representation.%20Specifically%2C%20given%20the%20mask%20of%0Aimage%20employed%20by%20a%20parsing%20network%2C%20we%20first%20leverage%20the%20speech%20to%20drive%20the%0Amask%20and%20generate%20talking%20segmentation.%20Then%20we%20disentangle%20semantic%20regions%20of%0Aimage%20into%20style%20codes%20using%20a%20mask-guided%20encoder.%20Ultimately%2C%20we%20inject%20the%0Apreviously%20generated%20talking%20segmentation%20and%20style%20codes%20into%20a%20mask-guided%0AStyleGAN%20to%20synthesize%20video%20frame.%20In%20this%20way%2C%20most%20of%20textures%20are%20fully%0Apreserved.%20Moreover%2C%20our%20approach%20can%20inherently%20achieve%20background%20separation%0Aand%20facilitate%20mask-guided%20facial%20local%20editing.%20In%20particular%2C%20by%20editing%20the%0Amask%20and%20swapping%20the%20region%20textures%20from%20a%20given%20reference%20image%20%28e.g.%20hair%2C%0Alip%2C%20eyebrows%29%2C%20our%20approach%20enables%20facial%20editing%20seamlessly%20when%20generating%0Atalking%20face%20video.%20Experiments%20demonstrate%20that%20our%20proposed%20approach%20can%0Aeffectively%20preserve%20texture%20details%20and%20generate%20temporally%20consistent%20video%0Awhile%20remaining%20competitive%20in%20lip%20synchronization.%20Quantitative%20and%0Aqualitative%20results%20on%20the%20HDTF%20and%20MEAD%20datasets%20illustrate%20the%20superior%0Aperformance%20of%20our%20method%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegTalker%253A%2520Segmentation-based%2520Talking%2520Face%2520Generation%2520with%2520Mask-guided%250A%2520%2520Local%2520Editing%26entry.906535625%3DLingyu%2520Xiong%2520and%2520Xize%2520Cheng%2520and%2520Jintao%2520Tan%2520and%2520Xianjia%2520Wu%2520and%2520Xiandong%2520Li%2520and%2520Lei%2520Zhu%2520and%2520Fei%2520Ma%2520and%2520Minglei%2520Li%2520and%2520Huang%2520Xu%2520and%2520Zhihu%2520Hu%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520face%2520generation%2520aims%2520to%2520synthesize%2520video%2520with%2520lip%250Amovements%2520synchronized%2520to%2520input%2520audio.%2520However%252C%2520current%2520generative%2520techniques%250Aface%2520challenges%2520in%2520preserving%2520intricate%2520regional%2520textures%2520%2528skin%252C%2520teeth%2529.%2520To%250Aaddress%2520the%2520aforementioned%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%250ASegTalker%2520to%2520decouple%2520lip%2520movements%2520and%2520image%2520textures%2520by%2520introducing%250Asegmentation%2520as%2520intermediate%2520representation.%2520Specifically%252C%2520given%2520the%2520mask%2520of%250Aimage%2520employed%2520by%2520a%2520parsing%2520network%252C%2520we%2520first%2520leverage%2520the%2520speech%2520to%2520drive%2520the%250Amask%2520and%2520generate%2520talking%2520segmentation.%2520Then%2520we%2520disentangle%2520semantic%2520regions%2520of%250Aimage%2520into%2520style%2520codes%2520using%2520a%2520mask-guided%2520encoder.%2520Ultimately%252C%2520we%2520inject%2520the%250Apreviously%2520generated%2520talking%2520segmentation%2520and%2520style%2520codes%2520into%2520a%2520mask-guided%250AStyleGAN%2520to%2520synthesize%2520video%2520frame.%2520In%2520this%2520way%252C%2520most%2520of%2520textures%2520are%2520fully%250Apreserved.%2520Moreover%252C%2520our%2520approach%2520can%2520inherently%2520achieve%2520background%2520separation%250Aand%2520facilitate%2520mask-guided%2520facial%2520local%2520editing.%2520In%2520particular%252C%2520by%2520editing%2520the%250Amask%2520and%2520swapping%2520the%2520region%2520textures%2520from%2520a%2520given%2520reference%2520image%2520%2528e.g.%2520hair%252C%250Alip%252C%2520eyebrows%2529%252C%2520our%2520approach%2520enables%2520facial%2520editing%2520seamlessly%2520when%2520generating%250Atalking%2520face%2520video.%2520Experiments%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520can%250Aeffectively%2520preserve%2520texture%2520details%2520and%2520generate%2520temporally%2520consistent%2520video%250Awhile%2520remaining%2520competitive%2520in%2520lip%2520synchronization.%2520Quantitative%2520and%250Aqualitative%2520results%2520on%2520the%2520HDTF%2520and%2520MEAD%2520datasets%2520illustrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520over%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegTalker%3A%20Segmentation-based%20Talking%20Face%20Generation%20with%20Mask-guided%0A%20%20Local%20Editing&entry.906535625=Lingyu%20Xiong%20and%20Xize%20Cheng%20and%20Jintao%20Tan%20and%20Xianjia%20Wu%20and%20Xiandong%20Li%20and%20Lei%20Zhu%20and%20Fei%20Ma%20and%20Minglei%20Li%20and%20Huang%20Xu%20and%20Zhihu%20Hu&entry.1292438233=%20%20Audio-driven%20talking%20face%20generation%20aims%20to%20synthesize%20video%20with%20lip%0Amovements%20synchronized%20to%20input%20audio.%20However%2C%20current%20generative%20techniques%0Aface%20challenges%20in%20preserving%20intricate%20regional%20textures%20%28skin%2C%20teeth%29.%20To%0Aaddress%20the%20aforementioned%20challenges%2C%20we%20propose%20a%20novel%20framework%20called%0ASegTalker%20to%20decouple%20lip%20movements%20and%20image%20textures%20by%20introducing%0Asegmentation%20as%20intermediate%20representation.%20Specifically%2C%20given%20the%20mask%20of%0Aimage%20employed%20by%20a%20parsing%20network%2C%20we%20first%20leverage%20the%20speech%20to%20drive%20the%0Amask%20and%20generate%20talking%20segmentation.%20Then%20we%20disentangle%20semantic%20regions%20of%0Aimage%20into%20style%20codes%20using%20a%20mask-guided%20encoder.%20Ultimately%2C%20we%20inject%20the%0Apreviously%20generated%20talking%20segmentation%20and%20style%20codes%20into%20a%20mask-guided%0AStyleGAN%20to%20synthesize%20video%20frame.%20In%20this%20way%2C%20most%20of%20textures%20are%20fully%0Apreserved.%20Moreover%2C%20our%20approach%20can%20inherently%20achieve%20background%20separation%0Aand%20facilitate%20mask-guided%20facial%20local%20editing.%20In%20particular%2C%20by%20editing%20the%0Amask%20and%20swapping%20the%20region%20textures%20from%20a%20given%20reference%20image%20%28e.g.%20hair%2C%0Alip%2C%20eyebrows%29%2C%20our%20approach%20enables%20facial%20editing%20seamlessly%20when%20generating%0Atalking%20face%20video.%20Experiments%20demonstrate%20that%20our%20proposed%20approach%20can%0Aeffectively%20preserve%20texture%20details%20and%20generate%20temporally%20consistent%20video%0Awhile%20remaining%20competitive%20in%20lip%20synchronization.%20Quantitative%20and%0Aqualitative%20results%20on%20the%20HDTF%20and%20MEAD%20datasets%20illustrate%20the%20superior%0Aperformance%20of%20our%20method%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03605v1&entry.124074799=Read"},
{"title": "FLAF: Focal Line and Feature-constrained Active View Planning for Visual\n  Teach and Repeat", "author": "Changfei Fu and Weinan Chen and Wenjun Xu and Hong Zhang", "abstract": "  This paper presents FLAF, a focal line and feature-constrained active view\nplanning method for tracking failure avoidance in feature-based visual\nnavigation of mobile robots. Our FLAF-based visual navigation is built upon a\nfeature-based visual teach and repeat (VT\\&R) framework, which supports many\nrobotic applications by teaching a robot to navigate on various paths that\ncover a significant portion of daily autonomous navigation requirements.\nHowever, tracking failure in feature-based visual simultaneous localization and\nmapping (VSLAM) caused by textureless regions in human-made environments is\nstill limiting VT\\&R to be adopted in the real world. To address this problem,\nthe proposed view planner is integrated into a feature-based visual SLAM system\nto build up an active VT\\&R system that avoids tracking failure. In our system,\na pan-tilt unit (PTU)-based active camera is mounted on the mobile robot. Using\nFLAF, the active camera-based VSLAM operates during the teaching phase to\nconstruct a complete path map and in the repeat phase to maintain stable\nlocalization. FLAF orients the robot toward more map points to avoid mapping\nfailures during path learning and toward more feature-identifiable map points\nbeneficial for localization while following the learned trajectory. Experiments\nin real scenarios demonstrate that FLAF outperforms the methods that do not\nconsider feature-identifiability, and our active VT\\&R system performs well in\ncomplex environments by effectively dealing with low-texture regions.\n", "link": "http://arxiv.org/abs/2409.03457v1", "date": "2024-09-05", "relevancy": 2.2793, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6135}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5393}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAF%3A%20Focal%20Line%20and%20Feature-constrained%20Active%20View%20Planning%20for%20Visual%0A%20%20Teach%20and%20Repeat&body=Title%3A%20FLAF%3A%20Focal%20Line%20and%20Feature-constrained%20Active%20View%20Planning%20for%20Visual%0A%20%20Teach%20and%20Repeat%0AAuthor%3A%20Changfei%20Fu%20and%20Weinan%20Chen%20and%20Wenjun%20Xu%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20FLAF%2C%20a%20focal%20line%20and%20feature-constrained%20active%20view%0Aplanning%20method%20for%20tracking%20failure%20avoidance%20in%20feature-based%20visual%0Anavigation%20of%20mobile%20robots.%20Our%20FLAF-based%20visual%20navigation%20is%20built%20upon%20a%0Afeature-based%20visual%20teach%20and%20repeat%20%28VT%5C%26R%29%20framework%2C%20which%20supports%20many%0Arobotic%20applications%20by%20teaching%20a%20robot%20to%20navigate%20on%20various%20paths%20that%0Acover%20a%20significant%20portion%20of%20daily%20autonomous%20navigation%20requirements.%0AHowever%2C%20tracking%20failure%20in%20feature-based%20visual%20simultaneous%20localization%20and%0Amapping%20%28VSLAM%29%20caused%20by%20textureless%20regions%20in%20human-made%20environments%20is%0Astill%20limiting%20VT%5C%26R%20to%20be%20adopted%20in%20the%20real%20world.%20To%20address%20this%20problem%2C%0Athe%20proposed%20view%20planner%20is%20integrated%20into%20a%20feature-based%20visual%20SLAM%20system%0Ato%20build%20up%20an%20active%20VT%5C%26R%20system%20that%20avoids%20tracking%20failure.%20In%20our%20system%2C%0Aa%20pan-tilt%20unit%20%28PTU%29-based%20active%20camera%20is%20mounted%20on%20the%20mobile%20robot.%20Using%0AFLAF%2C%20the%20active%20camera-based%20VSLAM%20operates%20during%20the%20teaching%20phase%20to%0Aconstruct%20a%20complete%20path%20map%20and%20in%20the%20repeat%20phase%20to%20maintain%20stable%0Alocalization.%20FLAF%20orients%20the%20robot%20toward%20more%20map%20points%20to%20avoid%20mapping%0Afailures%20during%20path%20learning%20and%20toward%20more%20feature-identifiable%20map%20points%0Abeneficial%20for%20localization%20while%20following%20the%20learned%20trajectory.%20Experiments%0Ain%20real%20scenarios%20demonstrate%20that%20FLAF%20outperforms%20the%20methods%20that%20do%20not%0Aconsider%20feature-identifiability%2C%20and%20our%20active%20VT%5C%26R%20system%20performs%20well%20in%0Acomplex%20environments%20by%20effectively%20dealing%20with%20low-texture%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAF%253A%2520Focal%2520Line%2520and%2520Feature-constrained%2520Active%2520View%2520Planning%2520for%2520Visual%250A%2520%2520Teach%2520and%2520Repeat%26entry.906535625%3DChangfei%2520Fu%2520and%2520Weinan%2520Chen%2520and%2520Wenjun%2520Xu%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520FLAF%252C%2520a%2520focal%2520line%2520and%2520feature-constrained%2520active%2520view%250Aplanning%2520method%2520for%2520tracking%2520failure%2520avoidance%2520in%2520feature-based%2520visual%250Anavigation%2520of%2520mobile%2520robots.%2520Our%2520FLAF-based%2520visual%2520navigation%2520is%2520built%2520upon%2520a%250Afeature-based%2520visual%2520teach%2520and%2520repeat%2520%2528VT%255C%2526R%2529%2520framework%252C%2520which%2520supports%2520many%250Arobotic%2520applications%2520by%2520teaching%2520a%2520robot%2520to%2520navigate%2520on%2520various%2520paths%2520that%250Acover%2520a%2520significant%2520portion%2520of%2520daily%2520autonomous%2520navigation%2520requirements.%250AHowever%252C%2520tracking%2520failure%2520in%2520feature-based%2520visual%2520simultaneous%2520localization%2520and%250Amapping%2520%2528VSLAM%2529%2520caused%2520by%2520textureless%2520regions%2520in%2520human-made%2520environments%2520is%250Astill%2520limiting%2520VT%255C%2526R%2520to%2520be%2520adopted%2520in%2520the%2520real%2520world.%2520To%2520address%2520this%2520problem%252C%250Athe%2520proposed%2520view%2520planner%2520is%2520integrated%2520into%2520a%2520feature-based%2520visual%2520SLAM%2520system%250Ato%2520build%2520up%2520an%2520active%2520VT%255C%2526R%2520system%2520that%2520avoids%2520tracking%2520failure.%2520In%2520our%2520system%252C%250Aa%2520pan-tilt%2520unit%2520%2528PTU%2529-based%2520active%2520camera%2520is%2520mounted%2520on%2520the%2520mobile%2520robot.%2520Using%250AFLAF%252C%2520the%2520active%2520camera-based%2520VSLAM%2520operates%2520during%2520the%2520teaching%2520phase%2520to%250Aconstruct%2520a%2520complete%2520path%2520map%2520and%2520in%2520the%2520repeat%2520phase%2520to%2520maintain%2520stable%250Alocalization.%2520FLAF%2520orients%2520the%2520robot%2520toward%2520more%2520map%2520points%2520to%2520avoid%2520mapping%250Afailures%2520during%2520path%2520learning%2520and%2520toward%2520more%2520feature-identifiable%2520map%2520points%250Abeneficial%2520for%2520localization%2520while%2520following%2520the%2520learned%2520trajectory.%2520Experiments%250Ain%2520real%2520scenarios%2520demonstrate%2520that%2520FLAF%2520outperforms%2520the%2520methods%2520that%2520do%2520not%250Aconsider%2520feature-identifiability%252C%2520and%2520our%2520active%2520VT%255C%2526R%2520system%2520performs%2520well%2520in%250Acomplex%2520environments%2520by%2520effectively%2520dealing%2520with%2520low-texture%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAF%3A%20Focal%20Line%20and%20Feature-constrained%20Active%20View%20Planning%20for%20Visual%0A%20%20Teach%20and%20Repeat&entry.906535625=Changfei%20Fu%20and%20Weinan%20Chen%20and%20Wenjun%20Xu%20and%20Hong%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20FLAF%2C%20a%20focal%20line%20and%20feature-constrained%20active%20view%0Aplanning%20method%20for%20tracking%20failure%20avoidance%20in%20feature-based%20visual%0Anavigation%20of%20mobile%20robots.%20Our%20FLAF-based%20visual%20navigation%20is%20built%20upon%20a%0Afeature-based%20visual%20teach%20and%20repeat%20%28VT%5C%26R%29%20framework%2C%20which%20supports%20many%0Arobotic%20applications%20by%20teaching%20a%20robot%20to%20navigate%20on%20various%20paths%20that%0Acover%20a%20significant%20portion%20of%20daily%20autonomous%20navigation%20requirements.%0AHowever%2C%20tracking%20failure%20in%20feature-based%20visual%20simultaneous%20localization%20and%0Amapping%20%28VSLAM%29%20caused%20by%20textureless%20regions%20in%20human-made%20environments%20is%0Astill%20limiting%20VT%5C%26R%20to%20be%20adopted%20in%20the%20real%20world.%20To%20address%20this%20problem%2C%0Athe%20proposed%20view%20planner%20is%20integrated%20into%20a%20feature-based%20visual%20SLAM%20system%0Ato%20build%20up%20an%20active%20VT%5C%26R%20system%20that%20avoids%20tracking%20failure.%20In%20our%20system%2C%0Aa%20pan-tilt%20unit%20%28PTU%29-based%20active%20camera%20is%20mounted%20on%20the%20mobile%20robot.%20Using%0AFLAF%2C%20the%20active%20camera-based%20VSLAM%20operates%20during%20the%20teaching%20phase%20to%0Aconstruct%20a%20complete%20path%20map%20and%20in%20the%20repeat%20phase%20to%20maintain%20stable%0Alocalization.%20FLAF%20orients%20the%20robot%20toward%20more%20map%20points%20to%20avoid%20mapping%0Afailures%20during%20path%20learning%20and%20toward%20more%20feature-identifiable%20map%20points%0Abeneficial%20for%20localization%20while%20following%20the%20learned%20trajectory.%20Experiments%0Ain%20real%20scenarios%20demonstrate%20that%20FLAF%20outperforms%20the%20methods%20that%20do%20not%0Aconsider%20feature-identifiability%2C%20and%20our%20active%20VT%5C%26R%20system%20performs%20well%20in%0Acomplex%20environments%20by%20effectively%20dealing%20with%20low-texture%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03457v1&entry.124074799=Read"},
{"title": "Game On: Towards Language Models as RL Experimenters", "author": "Jingwei Zhang and Thomas Lampe and Abbas Abdolmaleki and Jost Tobias Springenberg and Martin Riedmiller", "abstract": "  We propose an agent architecture that automates parts of the common\nreinforcement learning experiment workflow, to enable automated mastery of\ncontrol domains for embodied agents. To do so, it leverages a VLM to perform\nsome of the capabilities normally required of a human experimenter, including\nthe monitoring and analysis of experiment progress, the proposition of new\ntasks based on past successes and failures of the agent, decomposing tasks into\na sequence of subtasks (skills), and retrieval of the skill to execute -\nenabling our system to build automated curricula for learning. We believe this\nis one of the first proposals for a system that leverages a VLM throughout the\nfull experiment cycle of reinforcement learning. We provide a first prototype\nof this system, and examine the feasibility of current models and techniques\nfor the desired level of automation. For this, we use a standard Gemini model,\nwithout additional fine-tuning, to provide a curriculum of skills to a\nlanguage-conditioned Actor-Critic algorithm, in order to steer data collection\nso as to aid learning new skills. Data collected in this way is shown to be\nuseful for learning and iteratively improving control policies in a robotics\ndomain. Additional examination of the ability of the system to build a growing\nlibrary of skills, and to judge the progress of the training of those skills,\nalso shows promising results, suggesting that the proposed architecture\nprovides a potential recipe for fully automated mastery of tasks and domains\nfor embodied agents.\n", "link": "http://arxiv.org/abs/2409.03402v1", "date": "2024-09-05", "relevancy": 2.2656, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6147}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5621}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Game%20On%3A%20Towards%20Language%20Models%20as%20RL%20Experimenters&body=Title%3A%20Game%20On%3A%20Towards%20Language%20Models%20as%20RL%20Experimenters%0AAuthor%3A%20Jingwei%20Zhang%20and%20Thomas%20Lampe%20and%20Abbas%20Abdolmaleki%20and%20Jost%20Tobias%20Springenberg%20and%20Martin%20Riedmiller%0AAbstract%3A%20%20%20We%20propose%20an%20agent%20architecture%20that%20automates%20parts%20of%20the%20common%0Areinforcement%20learning%20experiment%20workflow%2C%20to%20enable%20automated%20mastery%20of%0Acontrol%20domains%20for%20embodied%20agents.%20To%20do%20so%2C%20it%20leverages%20a%20VLM%20to%20perform%0Asome%20of%20the%20capabilities%20normally%20required%20of%20a%20human%20experimenter%2C%20including%0Athe%20monitoring%20and%20analysis%20of%20experiment%20progress%2C%20the%20proposition%20of%20new%0Atasks%20based%20on%20past%20successes%20and%20failures%20of%20the%20agent%2C%20decomposing%20tasks%20into%0Aa%20sequence%20of%20subtasks%20%28skills%29%2C%20and%20retrieval%20of%20the%20skill%20to%20execute%20-%0Aenabling%20our%20system%20to%20build%20automated%20curricula%20for%20learning.%20We%20believe%20this%0Ais%20one%20of%20the%20first%20proposals%20for%20a%20system%20that%20leverages%20a%20VLM%20throughout%20the%0Afull%20experiment%20cycle%20of%20reinforcement%20learning.%20We%20provide%20a%20first%20prototype%0Aof%20this%20system%2C%20and%20examine%20the%20feasibility%20of%20current%20models%20and%20techniques%0Afor%20the%20desired%20level%20of%20automation.%20For%20this%2C%20we%20use%20a%20standard%20Gemini%20model%2C%0Awithout%20additional%20fine-tuning%2C%20to%20provide%20a%20curriculum%20of%20skills%20to%20a%0Alanguage-conditioned%20Actor-Critic%20algorithm%2C%20in%20order%20to%20steer%20data%20collection%0Aso%20as%20to%20aid%20learning%20new%20skills.%20Data%20collected%20in%20this%20way%20is%20shown%20to%20be%0Auseful%20for%20learning%20and%20iteratively%20improving%20control%20policies%20in%20a%20robotics%0Adomain.%20Additional%20examination%20of%20the%20ability%20of%20the%20system%20to%20build%20a%20growing%0Alibrary%20of%20skills%2C%20and%20to%20judge%20the%20progress%20of%20the%20training%20of%20those%20skills%2C%0Aalso%20shows%20promising%20results%2C%20suggesting%20that%20the%20proposed%20architecture%0Aprovides%20a%20potential%20recipe%20for%20fully%20automated%20mastery%20of%20tasks%20and%20domains%0Afor%20embodied%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGame%2520On%253A%2520Towards%2520Language%2520Models%2520as%2520RL%2520Experimenters%26entry.906535625%3DJingwei%2520Zhang%2520and%2520Thomas%2520Lampe%2520and%2520Abbas%2520Abdolmaleki%2520and%2520Jost%2520Tobias%2520Springenberg%2520and%2520Martin%2520Riedmiller%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520agent%2520architecture%2520that%2520automates%2520parts%2520of%2520the%2520common%250Areinforcement%2520learning%2520experiment%2520workflow%252C%2520to%2520enable%2520automated%2520mastery%2520of%250Acontrol%2520domains%2520for%2520embodied%2520agents.%2520To%2520do%2520so%252C%2520it%2520leverages%2520a%2520VLM%2520to%2520perform%250Asome%2520of%2520the%2520capabilities%2520normally%2520required%2520of%2520a%2520human%2520experimenter%252C%2520including%250Athe%2520monitoring%2520and%2520analysis%2520of%2520experiment%2520progress%252C%2520the%2520proposition%2520of%2520new%250Atasks%2520based%2520on%2520past%2520successes%2520and%2520failures%2520of%2520the%2520agent%252C%2520decomposing%2520tasks%2520into%250Aa%2520sequence%2520of%2520subtasks%2520%2528skills%2529%252C%2520and%2520retrieval%2520of%2520the%2520skill%2520to%2520execute%2520-%250Aenabling%2520our%2520system%2520to%2520build%2520automated%2520curricula%2520for%2520learning.%2520We%2520believe%2520this%250Ais%2520one%2520of%2520the%2520first%2520proposals%2520for%2520a%2520system%2520that%2520leverages%2520a%2520VLM%2520throughout%2520the%250Afull%2520experiment%2520cycle%2520of%2520reinforcement%2520learning.%2520We%2520provide%2520a%2520first%2520prototype%250Aof%2520this%2520system%252C%2520and%2520examine%2520the%2520feasibility%2520of%2520current%2520models%2520and%2520techniques%250Afor%2520the%2520desired%2520level%2520of%2520automation.%2520For%2520this%252C%2520we%2520use%2520a%2520standard%2520Gemini%2520model%252C%250Awithout%2520additional%2520fine-tuning%252C%2520to%2520provide%2520a%2520curriculum%2520of%2520skills%2520to%2520a%250Alanguage-conditioned%2520Actor-Critic%2520algorithm%252C%2520in%2520order%2520to%2520steer%2520data%2520collection%250Aso%2520as%2520to%2520aid%2520learning%2520new%2520skills.%2520Data%2520collected%2520in%2520this%2520way%2520is%2520shown%2520to%2520be%250Auseful%2520for%2520learning%2520and%2520iteratively%2520improving%2520control%2520policies%2520in%2520a%2520robotics%250Adomain.%2520Additional%2520examination%2520of%2520the%2520ability%2520of%2520the%2520system%2520to%2520build%2520a%2520growing%250Alibrary%2520of%2520skills%252C%2520and%2520to%2520judge%2520the%2520progress%2520of%2520the%2520training%2520of%2520those%2520skills%252C%250Aalso%2520shows%2520promising%2520results%252C%2520suggesting%2520that%2520the%2520proposed%2520architecture%250Aprovides%2520a%2520potential%2520recipe%2520for%2520fully%2520automated%2520mastery%2520of%2520tasks%2520and%2520domains%250Afor%2520embodied%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Game%20On%3A%20Towards%20Language%20Models%20as%20RL%20Experimenters&entry.906535625=Jingwei%20Zhang%20and%20Thomas%20Lampe%20and%20Abbas%20Abdolmaleki%20and%20Jost%20Tobias%20Springenberg%20and%20Martin%20Riedmiller&entry.1292438233=%20%20We%20propose%20an%20agent%20architecture%20that%20automates%20parts%20of%20the%20common%0Areinforcement%20learning%20experiment%20workflow%2C%20to%20enable%20automated%20mastery%20of%0Acontrol%20domains%20for%20embodied%20agents.%20To%20do%20so%2C%20it%20leverages%20a%20VLM%20to%20perform%0Asome%20of%20the%20capabilities%20normally%20required%20of%20a%20human%20experimenter%2C%20including%0Athe%20monitoring%20and%20analysis%20of%20experiment%20progress%2C%20the%20proposition%20of%20new%0Atasks%20based%20on%20past%20successes%20and%20failures%20of%20the%20agent%2C%20decomposing%20tasks%20into%0Aa%20sequence%20of%20subtasks%20%28skills%29%2C%20and%20retrieval%20of%20the%20skill%20to%20execute%20-%0Aenabling%20our%20system%20to%20build%20automated%20curricula%20for%20learning.%20We%20believe%20this%0Ais%20one%20of%20the%20first%20proposals%20for%20a%20system%20that%20leverages%20a%20VLM%20throughout%20the%0Afull%20experiment%20cycle%20of%20reinforcement%20learning.%20We%20provide%20a%20first%20prototype%0Aof%20this%20system%2C%20and%20examine%20the%20feasibility%20of%20current%20models%20and%20techniques%0Afor%20the%20desired%20level%20of%20automation.%20For%20this%2C%20we%20use%20a%20standard%20Gemini%20model%2C%0Awithout%20additional%20fine-tuning%2C%20to%20provide%20a%20curriculum%20of%20skills%20to%20a%0Alanguage-conditioned%20Actor-Critic%20algorithm%2C%20in%20order%20to%20steer%20data%20collection%0Aso%20as%20to%20aid%20learning%20new%20skills.%20Data%20collected%20in%20this%20way%20is%20shown%20to%20be%0Auseful%20for%20learning%20and%20iteratively%20improving%20control%20policies%20in%20a%20robotics%0Adomain.%20Additional%20examination%20of%20the%20ability%20of%20the%20system%20to%20build%20a%20growing%0Alibrary%20of%20skills%2C%20and%20to%20judge%20the%20progress%20of%20the%20training%20of%20those%20skills%2C%0Aalso%20shows%20promising%20results%2C%20suggesting%20that%20the%20proposed%20architecture%0Aprovides%20a%20potential%20recipe%20for%20fully%20automated%20mastery%20of%20tasks%20and%20domains%0Afor%20embodied%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03402v1&entry.124074799=Read"},
{"title": "Serialized Speech Information Guidance with Overlapped Encoding\n  Separation for Multi-Speaker Automatic Speech Recognition", "author": "Hao Shi and Yuan Gao and Zhaoheng Ni and Tatsuya Kawahara", "abstract": "  Serialized output training (SOT) attracts increasing attention due to its\nconvenience and flexibility for multi-speaker automatic speech recognition\n(ASR). However, it is not easy to train with attention loss only. In this\npaper, we propose the overlapped encoding separation (EncSep) to fully utilize\nthe benefits of the connectionist temporal classification (CTC) and attention\nhybrid loss. This additional separator is inserted after the encoder to extract\nthe multi-speaker information with CTC losses. Furthermore, we propose the\nserialized speech information guidance SOT (GEncSep) to further utilize the\nseparated encodings. The separated streams are concatenated to provide\nsingle-speaker information to guide attention during decoding. The experimental\nresults on LibriMix show that the single-speaker encoding can be separated from\nthe overlapped encoding. The CTC loss helps to improve the encoder\nrepresentation under complex scenarios. GEncSep further improved performance.\n", "link": "http://arxiv.org/abs/2409.00815v2", "date": "2024-09-05", "relevancy": 2.2557, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Serialized%20Speech%20Information%20Guidance%20with%20Overlapped%20Encoding%0A%20%20Separation%20for%20Multi-Speaker%20Automatic%20Speech%20Recognition&body=Title%3A%20Serialized%20Speech%20Information%20Guidance%20with%20Overlapped%20Encoding%0A%20%20Separation%20for%20Multi-Speaker%20Automatic%20Speech%20Recognition%0AAuthor%3A%20Hao%20Shi%20and%20Yuan%20Gao%20and%20Zhaoheng%20Ni%20and%20Tatsuya%20Kawahara%0AAbstract%3A%20%20%20Serialized%20output%20training%20%28SOT%29%20attracts%20increasing%20attention%20due%20to%20its%0Aconvenience%20and%20flexibility%20for%20multi-speaker%20automatic%20speech%20recognition%0A%28ASR%29.%20However%2C%20it%20is%20not%20easy%20to%20train%20with%20attention%20loss%20only.%20In%20this%0Apaper%2C%20we%20propose%20the%20overlapped%20encoding%20separation%20%28EncSep%29%20to%20fully%20utilize%0Athe%20benefits%20of%20the%20connectionist%20temporal%20classification%20%28CTC%29%20and%20attention%0Ahybrid%20loss.%20This%20additional%20separator%20is%20inserted%20after%20the%20encoder%20to%20extract%0Athe%20multi-speaker%20information%20with%20CTC%20losses.%20Furthermore%2C%20we%20propose%20the%0Aserialized%20speech%20information%20guidance%20SOT%20%28GEncSep%29%20to%20further%20utilize%20the%0Aseparated%20encodings.%20The%20separated%20streams%20are%20concatenated%20to%20provide%0Asingle-speaker%20information%20to%20guide%20attention%20during%20decoding.%20The%20experimental%0Aresults%20on%20LibriMix%20show%20that%20the%20single-speaker%20encoding%20can%20be%20separated%20from%0Athe%20overlapped%20encoding.%20The%20CTC%20loss%20helps%20to%20improve%20the%20encoder%0Arepresentation%20under%20complex%20scenarios.%20GEncSep%20further%20improved%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSerialized%2520Speech%2520Information%2520Guidance%2520with%2520Overlapped%2520Encoding%250A%2520%2520Separation%2520for%2520Multi-Speaker%2520Automatic%2520Speech%2520Recognition%26entry.906535625%3DHao%2520Shi%2520and%2520Yuan%2520Gao%2520and%2520Zhaoheng%2520Ni%2520and%2520Tatsuya%2520Kawahara%26entry.1292438233%3D%2520%2520Serialized%2520output%2520training%2520%2528SOT%2529%2520attracts%2520increasing%2520attention%2520due%2520to%2520its%250Aconvenience%2520and%2520flexibility%2520for%2520multi-speaker%2520automatic%2520speech%2520recognition%250A%2528ASR%2529.%2520However%252C%2520it%2520is%2520not%2520easy%2520to%2520train%2520with%2520attention%2520loss%2520only.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520overlapped%2520encoding%2520separation%2520%2528EncSep%2529%2520to%2520fully%2520utilize%250Athe%2520benefits%2520of%2520the%2520connectionist%2520temporal%2520classification%2520%2528CTC%2529%2520and%2520attention%250Ahybrid%2520loss.%2520This%2520additional%2520separator%2520is%2520inserted%2520after%2520the%2520encoder%2520to%2520extract%250Athe%2520multi-speaker%2520information%2520with%2520CTC%2520losses.%2520Furthermore%252C%2520we%2520propose%2520the%250Aserialized%2520speech%2520information%2520guidance%2520SOT%2520%2528GEncSep%2529%2520to%2520further%2520utilize%2520the%250Aseparated%2520encodings.%2520The%2520separated%2520streams%2520are%2520concatenated%2520to%2520provide%250Asingle-speaker%2520information%2520to%2520guide%2520attention%2520during%2520decoding.%2520The%2520experimental%250Aresults%2520on%2520LibriMix%2520show%2520that%2520the%2520single-speaker%2520encoding%2520can%2520be%2520separated%2520from%250Athe%2520overlapped%2520encoding.%2520The%2520CTC%2520loss%2520helps%2520to%2520improve%2520the%2520encoder%250Arepresentation%2520under%2520complex%2520scenarios.%2520GEncSep%2520further%2520improved%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Serialized%20Speech%20Information%20Guidance%20with%20Overlapped%20Encoding%0A%20%20Separation%20for%20Multi-Speaker%20Automatic%20Speech%20Recognition&entry.906535625=Hao%20Shi%20and%20Yuan%20Gao%20and%20Zhaoheng%20Ni%20and%20Tatsuya%20Kawahara&entry.1292438233=%20%20Serialized%20output%20training%20%28SOT%29%20attracts%20increasing%20attention%20due%20to%20its%0Aconvenience%20and%20flexibility%20for%20multi-speaker%20automatic%20speech%20recognition%0A%28ASR%29.%20However%2C%20it%20is%20not%20easy%20to%20train%20with%20attention%20loss%20only.%20In%20this%0Apaper%2C%20we%20propose%20the%20overlapped%20encoding%20separation%20%28EncSep%29%20to%20fully%20utilize%0Athe%20benefits%20of%20the%20connectionist%20temporal%20classification%20%28CTC%29%20and%20attention%0Ahybrid%20loss.%20This%20additional%20separator%20is%20inserted%20after%20the%20encoder%20to%20extract%0Athe%20multi-speaker%20information%20with%20CTC%20losses.%20Furthermore%2C%20we%20propose%20the%0Aserialized%20speech%20information%20guidance%20SOT%20%28GEncSep%29%20to%20further%20utilize%20the%0Aseparated%20encodings.%20The%20separated%20streams%20are%20concatenated%20to%20provide%0Asingle-speaker%20information%20to%20guide%20attention%20during%20decoding.%20The%20experimental%0Aresults%20on%20LibriMix%20show%20that%20the%20single-speaker%20encoding%20can%20be%20separated%20from%0Athe%20overlapped%20encoding.%20The%20CTC%20loss%20helps%20to%20improve%20the%20encoder%0Arepresentation%20under%20complex%20scenarios.%20GEncSep%20further%20improved%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00815v2&entry.124074799=Read"},
{"title": "UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary\n  Identification in High-Resolution Remote Sensing Images", "author": "Lulin Li and Ben Chen and Xuechao Zou and Junliang Xing and Pin Tao", "abstract": "  Owing to the diverse geographical environments, intricate landscapes, and\nhigh-density settlements, the automatic identification of urban village\nboundaries using remote sensing images is a highly challenging task. This paper\nproposes a novel and efficient neural network model called UV-Mamba for\naccurate boundary detection in high-resolution remote sensing images. UV-Mamba\nmitigates the memory loss problem in long sequence modeling, which arises in\nstate space model (SSM) with increasing image size, by incorporating deformable\nconvolutions (DCN). Its architecture utilizes an encoder-decoder framework,\nincludes an encoder with four deformable state space augmentation (DSSA) blocks\nfor efficient multi-level semantic extraction and a decoder to integrate the\nextracted semantic information. We conducted experiments on the Beijing and\nXi'an datasets, and the results show that UV-Mamba achieves state-of-the-art\nperformance. Specifically, our model achieves 73.3% and 78.1% IoU on the\nBeijing and Xi'an datasets, respectively, representing improvements of 1.2% and\n3.4% IoU over the previous best model, while also being 6x faster in inference\nspeed and 40x smaller in parameter count. Source code and pre-trained models\nare available in the supplementary material.\n", "link": "http://arxiv.org/abs/2409.03431v1", "date": "2024-09-05", "relevancy": 2.1898, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5626}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UV-Mamba%3A%20A%20DCN-Enhanced%20State%20Space%20Model%20for%20Urban%20Village%20Boundary%0A%20%20Identification%20in%20High-Resolution%20Remote%20Sensing%20Images&body=Title%3A%20UV-Mamba%3A%20A%20DCN-Enhanced%20State%20Space%20Model%20for%20Urban%20Village%20Boundary%0A%20%20Identification%20in%20High-Resolution%20Remote%20Sensing%20Images%0AAuthor%3A%20Lulin%20Li%20and%20Ben%20Chen%20and%20Xuechao%20Zou%20and%20Junliang%20Xing%20and%20Pin%20Tao%0AAbstract%3A%20%20%20Owing%20to%20the%20diverse%20geographical%20environments%2C%20intricate%20landscapes%2C%20and%0Ahigh-density%20settlements%2C%20the%20automatic%20identification%20of%20urban%20village%0Aboundaries%20using%20remote%20sensing%20images%20is%20a%20highly%20challenging%20task.%20This%20paper%0Aproposes%20a%20novel%20and%20efficient%20neural%20network%20model%20called%20UV-Mamba%20for%0Aaccurate%20boundary%20detection%20in%20high-resolution%20remote%20sensing%20images.%20UV-Mamba%0Amitigates%20the%20memory%20loss%20problem%20in%20long%20sequence%20modeling%2C%20which%20arises%20in%0Astate%20space%20model%20%28SSM%29%20with%20increasing%20image%20size%2C%20by%20incorporating%20deformable%0Aconvolutions%20%28DCN%29.%20Its%20architecture%20utilizes%20an%20encoder-decoder%20framework%2C%0Aincludes%20an%20encoder%20with%20four%20deformable%20state%20space%20augmentation%20%28DSSA%29%20blocks%0Afor%20efficient%20multi-level%20semantic%20extraction%20and%20a%20decoder%20to%20integrate%20the%0Aextracted%20semantic%20information.%20We%20conducted%20experiments%20on%20the%20Beijing%20and%0AXi%27an%20datasets%2C%20and%20the%20results%20show%20that%20UV-Mamba%20achieves%20state-of-the-art%0Aperformance.%20Specifically%2C%20our%20model%20achieves%2073.3%25%20and%2078.1%25%20IoU%20on%20the%0ABeijing%20and%20Xi%27an%20datasets%2C%20respectively%2C%20representing%20improvements%20of%201.2%25%20and%0A3.4%25%20IoU%20over%20the%20previous%20best%20model%2C%20while%20also%20being%206x%20faster%20in%20inference%0Aspeed%20and%2040x%20smaller%20in%20parameter%20count.%20Source%20code%20and%20pre-trained%20models%0Aare%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUV-Mamba%253A%2520A%2520DCN-Enhanced%2520State%2520Space%2520Model%2520for%2520Urban%2520Village%2520Boundary%250A%2520%2520Identification%2520in%2520High-Resolution%2520Remote%2520Sensing%2520Images%26entry.906535625%3DLulin%2520Li%2520and%2520Ben%2520Chen%2520and%2520Xuechao%2520Zou%2520and%2520Junliang%2520Xing%2520and%2520Pin%2520Tao%26entry.1292438233%3D%2520%2520Owing%2520to%2520the%2520diverse%2520geographical%2520environments%252C%2520intricate%2520landscapes%252C%2520and%250Ahigh-density%2520settlements%252C%2520the%2520automatic%2520identification%2520of%2520urban%2520village%250Aboundaries%2520using%2520remote%2520sensing%2520images%2520is%2520a%2520highly%2520challenging%2520task.%2520This%2520paper%250Aproposes%2520a%2520novel%2520and%2520efficient%2520neural%2520network%2520model%2520called%2520UV-Mamba%2520for%250Aaccurate%2520boundary%2520detection%2520in%2520high-resolution%2520remote%2520sensing%2520images.%2520UV-Mamba%250Amitigates%2520the%2520memory%2520loss%2520problem%2520in%2520long%2520sequence%2520modeling%252C%2520which%2520arises%2520in%250Astate%2520space%2520model%2520%2528SSM%2529%2520with%2520increasing%2520image%2520size%252C%2520by%2520incorporating%2520deformable%250Aconvolutions%2520%2528DCN%2529.%2520Its%2520architecture%2520utilizes%2520an%2520encoder-decoder%2520framework%252C%250Aincludes%2520an%2520encoder%2520with%2520four%2520deformable%2520state%2520space%2520augmentation%2520%2528DSSA%2529%2520blocks%250Afor%2520efficient%2520multi-level%2520semantic%2520extraction%2520and%2520a%2520decoder%2520to%2520integrate%2520the%250Aextracted%2520semantic%2520information.%2520We%2520conducted%2520experiments%2520on%2520the%2520Beijing%2520and%250AXi%2527an%2520datasets%252C%2520and%2520the%2520results%2520show%2520that%2520UV-Mamba%2520achieves%2520state-of-the-art%250Aperformance.%2520Specifically%252C%2520our%2520model%2520achieves%252073.3%2525%2520and%252078.1%2525%2520IoU%2520on%2520the%250ABeijing%2520and%2520Xi%2527an%2520datasets%252C%2520respectively%252C%2520representing%2520improvements%2520of%25201.2%2525%2520and%250A3.4%2525%2520IoU%2520over%2520the%2520previous%2520best%2520model%252C%2520while%2520also%2520being%25206x%2520faster%2520in%2520inference%250Aspeed%2520and%252040x%2520smaller%2520in%2520parameter%2520count.%2520Source%2520code%2520and%2520pre-trained%2520models%250Aare%2520available%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UV-Mamba%3A%20A%20DCN-Enhanced%20State%20Space%20Model%20for%20Urban%20Village%20Boundary%0A%20%20Identification%20in%20High-Resolution%20Remote%20Sensing%20Images&entry.906535625=Lulin%20Li%20and%20Ben%20Chen%20and%20Xuechao%20Zou%20and%20Junliang%20Xing%20and%20Pin%20Tao&entry.1292438233=%20%20Owing%20to%20the%20diverse%20geographical%20environments%2C%20intricate%20landscapes%2C%20and%0Ahigh-density%20settlements%2C%20the%20automatic%20identification%20of%20urban%20village%0Aboundaries%20using%20remote%20sensing%20images%20is%20a%20highly%20challenging%20task.%20This%20paper%0Aproposes%20a%20novel%20and%20efficient%20neural%20network%20model%20called%20UV-Mamba%20for%0Aaccurate%20boundary%20detection%20in%20high-resolution%20remote%20sensing%20images.%20UV-Mamba%0Amitigates%20the%20memory%20loss%20problem%20in%20long%20sequence%20modeling%2C%20which%20arises%20in%0Astate%20space%20model%20%28SSM%29%20with%20increasing%20image%20size%2C%20by%20incorporating%20deformable%0Aconvolutions%20%28DCN%29.%20Its%20architecture%20utilizes%20an%20encoder-decoder%20framework%2C%0Aincludes%20an%20encoder%20with%20four%20deformable%20state%20space%20augmentation%20%28DSSA%29%20blocks%0Afor%20efficient%20multi-level%20semantic%20extraction%20and%20a%20decoder%20to%20integrate%20the%0Aextracted%20semantic%20information.%20We%20conducted%20experiments%20on%20the%20Beijing%20and%0AXi%27an%20datasets%2C%20and%20the%20results%20show%20that%20UV-Mamba%20achieves%20state-of-the-art%0Aperformance.%20Specifically%2C%20our%20model%20achieves%2073.3%25%20and%2078.1%25%20IoU%20on%20the%0ABeijing%20and%20Xi%27an%20datasets%2C%20respectively%2C%20representing%20improvements%20of%201.2%25%20and%0A3.4%25%20IoU%20over%20the%20previous%20best%20model%2C%20while%20also%20being%206x%20faster%20in%20inference%0Aspeed%20and%2040x%20smaller%20in%20parameter%20count.%20Source%20code%20and%20pre-trained%20models%0Aare%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03431v1&entry.124074799=Read"},
{"title": "Deep Neural Implicit Representation of Accessibility for Multi-Axis\n  Manufacturing", "author": "George P. Harabin and Amir Mirzendehdel and Morad Behandish", "abstract": "  One of the main concerns in design and process planning for multi-axis\nadditive and subtractive manufacturing is collision avoidance between moving\nobjects (e.g., tool assemblies) and stationary objects (e.g., a part unified\nwith fixtures). The collision measure for various pairs of relative rigid\ntranslations and rotations between the two pointsets can be conceptualized by a\ncompactly supported scalar field over the 6D non-Euclidean configuration space.\nExplicit representation and computation of this field is costly in both time\nand space. If we fix $O(m)$ sparsely sampled rotations (e.g., tool\norientations), computation of the collision measure field as a convolution of\nindicator functions of the 3D pointsets over a uniform grid (i.e., voxelized\ngeometry) of resolution $O(n^3)$ via fast Fourier transforms (FFTs) scales as\nin $O(mn^3 \\log n)$ in time and $O(mn^3)$ in space. In this paper, we develop\nan implicit representation of the collision measure field via deep neural\nnetworks (DNNs). We show that our approach is able to accurately interpolate\nthe collision measure from a sparse sampling of rotations, and can represent\nthe collision measure field with a small memory footprint. Moreover, we show\nthat this representation can be efficiently updated through fine-tuning to more\nefficiently train the network on multi-resolution data, as well as accommodate\nincremental changes to the geometry (such as might occur in iterative processes\nsuch as topology optimization of the part subject to CNC tool accessibility\nconstraints).\n", "link": "http://arxiv.org/abs/2409.02115v2", "date": "2024-09-05", "relevancy": 2.1531, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5429}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Neural%20Implicit%20Representation%20of%20Accessibility%20for%20Multi-Axis%0A%20%20Manufacturing&body=Title%3A%20Deep%20Neural%20Implicit%20Representation%20of%20Accessibility%20for%20Multi-Axis%0A%20%20Manufacturing%0AAuthor%3A%20George%20P.%20Harabin%20and%20Amir%20Mirzendehdel%20and%20Morad%20Behandish%0AAbstract%3A%20%20%20One%20of%20the%20main%20concerns%20in%20design%20and%20process%20planning%20for%20multi-axis%0Aadditive%20and%20subtractive%20manufacturing%20is%20collision%20avoidance%20between%20moving%0Aobjects%20%28e.g.%2C%20tool%20assemblies%29%20and%20stationary%20objects%20%28e.g.%2C%20a%20part%20unified%0Awith%20fixtures%29.%20The%20collision%20measure%20for%20various%20pairs%20of%20relative%20rigid%0Atranslations%20and%20rotations%20between%20the%20two%20pointsets%20can%20be%20conceptualized%20by%20a%0Acompactly%20supported%20scalar%20field%20over%20the%206D%20non-Euclidean%20configuration%20space.%0AExplicit%20representation%20and%20computation%20of%20this%20field%20is%20costly%20in%20both%20time%0Aand%20space.%20If%20we%20fix%20%24O%28m%29%24%20sparsely%20sampled%20rotations%20%28e.g.%2C%20tool%0Aorientations%29%2C%20computation%20of%20the%20collision%20measure%20field%20as%20a%20convolution%20of%0Aindicator%20functions%20of%20the%203D%20pointsets%20over%20a%20uniform%20grid%20%28i.e.%2C%20voxelized%0Ageometry%29%20of%20resolution%20%24O%28n%5E3%29%24%20via%20fast%20Fourier%20transforms%20%28FFTs%29%20scales%20as%0Ain%20%24O%28mn%5E3%20%5Clog%20n%29%24%20in%20time%20and%20%24O%28mn%5E3%29%24%20in%20space.%20In%20this%20paper%2C%20we%20develop%0Aan%20implicit%20representation%20of%20the%20collision%20measure%20field%20via%20deep%20neural%0Anetworks%20%28DNNs%29.%20We%20show%20that%20our%20approach%20is%20able%20to%20accurately%20interpolate%0Athe%20collision%20measure%20from%20a%20sparse%20sampling%20of%20rotations%2C%20and%20can%20represent%0Athe%20collision%20measure%20field%20with%20a%20small%20memory%20footprint.%20Moreover%2C%20we%20show%0Athat%20this%20representation%20can%20be%20efficiently%20updated%20through%20fine-tuning%20to%20more%0Aefficiently%20train%20the%20network%20on%20multi-resolution%20data%2C%20as%20well%20as%20accommodate%0Aincremental%20changes%20to%20the%20geometry%20%28such%20as%20might%20occur%20in%20iterative%20processes%0Asuch%20as%20topology%20optimization%20of%20the%20part%20subject%20to%20CNC%20tool%20accessibility%0Aconstraints%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Neural%2520Implicit%2520Representation%2520of%2520Accessibility%2520for%2520Multi-Axis%250A%2520%2520Manufacturing%26entry.906535625%3DGeorge%2520P.%2520Harabin%2520and%2520Amir%2520Mirzendehdel%2520and%2520Morad%2520Behandish%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520main%2520concerns%2520in%2520design%2520and%2520process%2520planning%2520for%2520multi-axis%250Aadditive%2520and%2520subtractive%2520manufacturing%2520is%2520collision%2520avoidance%2520between%2520moving%250Aobjects%2520%2528e.g.%252C%2520tool%2520assemblies%2529%2520and%2520stationary%2520objects%2520%2528e.g.%252C%2520a%2520part%2520unified%250Awith%2520fixtures%2529.%2520The%2520collision%2520measure%2520for%2520various%2520pairs%2520of%2520relative%2520rigid%250Atranslations%2520and%2520rotations%2520between%2520the%2520two%2520pointsets%2520can%2520be%2520conceptualized%2520by%2520a%250Acompactly%2520supported%2520scalar%2520field%2520over%2520the%25206D%2520non-Euclidean%2520configuration%2520space.%250AExplicit%2520representation%2520and%2520computation%2520of%2520this%2520field%2520is%2520costly%2520in%2520both%2520time%250Aand%2520space.%2520If%2520we%2520fix%2520%2524O%2528m%2529%2524%2520sparsely%2520sampled%2520rotations%2520%2528e.g.%252C%2520tool%250Aorientations%2529%252C%2520computation%2520of%2520the%2520collision%2520measure%2520field%2520as%2520a%2520convolution%2520of%250Aindicator%2520functions%2520of%2520the%25203D%2520pointsets%2520over%2520a%2520uniform%2520grid%2520%2528i.e.%252C%2520voxelized%250Ageometry%2529%2520of%2520resolution%2520%2524O%2528n%255E3%2529%2524%2520via%2520fast%2520Fourier%2520transforms%2520%2528FFTs%2529%2520scales%2520as%250Ain%2520%2524O%2528mn%255E3%2520%255Clog%2520n%2529%2524%2520in%2520time%2520and%2520%2524O%2528mn%255E3%2529%2524%2520in%2520space.%2520In%2520this%2520paper%252C%2520we%2520develop%250Aan%2520implicit%2520representation%2520of%2520the%2520collision%2520measure%2520field%2520via%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529.%2520We%2520show%2520that%2520our%2520approach%2520is%2520able%2520to%2520accurately%2520interpolate%250Athe%2520collision%2520measure%2520from%2520a%2520sparse%2520sampling%2520of%2520rotations%252C%2520and%2520can%2520represent%250Athe%2520collision%2520measure%2520field%2520with%2520a%2520small%2520memory%2520footprint.%2520Moreover%252C%2520we%2520show%250Athat%2520this%2520representation%2520can%2520be%2520efficiently%2520updated%2520through%2520fine-tuning%2520to%2520more%250Aefficiently%2520train%2520the%2520network%2520on%2520multi-resolution%2520data%252C%2520as%2520well%2520as%2520accommodate%250Aincremental%2520changes%2520to%2520the%2520geometry%2520%2528such%2520as%2520might%2520occur%2520in%2520iterative%2520processes%250Asuch%2520as%2520topology%2520optimization%2520of%2520the%2520part%2520subject%2520to%2520CNC%2520tool%2520accessibility%250Aconstraints%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Neural%20Implicit%20Representation%20of%20Accessibility%20for%20Multi-Axis%0A%20%20Manufacturing&entry.906535625=George%20P.%20Harabin%20and%20Amir%20Mirzendehdel%20and%20Morad%20Behandish&entry.1292438233=%20%20One%20of%20the%20main%20concerns%20in%20design%20and%20process%20planning%20for%20multi-axis%0Aadditive%20and%20subtractive%20manufacturing%20is%20collision%20avoidance%20between%20moving%0Aobjects%20%28e.g.%2C%20tool%20assemblies%29%20and%20stationary%20objects%20%28e.g.%2C%20a%20part%20unified%0Awith%20fixtures%29.%20The%20collision%20measure%20for%20various%20pairs%20of%20relative%20rigid%0Atranslations%20and%20rotations%20between%20the%20two%20pointsets%20can%20be%20conceptualized%20by%20a%0Acompactly%20supported%20scalar%20field%20over%20the%206D%20non-Euclidean%20configuration%20space.%0AExplicit%20representation%20and%20computation%20of%20this%20field%20is%20costly%20in%20both%20time%0Aand%20space.%20If%20we%20fix%20%24O%28m%29%24%20sparsely%20sampled%20rotations%20%28e.g.%2C%20tool%0Aorientations%29%2C%20computation%20of%20the%20collision%20measure%20field%20as%20a%20convolution%20of%0Aindicator%20functions%20of%20the%203D%20pointsets%20over%20a%20uniform%20grid%20%28i.e.%2C%20voxelized%0Ageometry%29%20of%20resolution%20%24O%28n%5E3%29%24%20via%20fast%20Fourier%20transforms%20%28FFTs%29%20scales%20as%0Ain%20%24O%28mn%5E3%20%5Clog%20n%29%24%20in%20time%20and%20%24O%28mn%5E3%29%24%20in%20space.%20In%20this%20paper%2C%20we%20develop%0Aan%20implicit%20representation%20of%20the%20collision%20measure%20field%20via%20deep%20neural%0Anetworks%20%28DNNs%29.%20We%20show%20that%20our%20approach%20is%20able%20to%20accurately%20interpolate%0Athe%20collision%20measure%20from%20a%20sparse%20sampling%20of%20rotations%2C%20and%20can%20represent%0Athe%20collision%20measure%20field%20with%20a%20small%20memory%20footprint.%20Moreover%2C%20we%20show%0Athat%20this%20representation%20can%20be%20efficiently%20updated%20through%20fine-tuning%20to%20more%0Aefficiently%20train%20the%20network%20on%20multi-resolution%20data%2C%20as%20well%20as%20accommodate%0Aincremental%20changes%20to%20the%20geometry%20%28such%20as%20might%20occur%20in%20iterative%20processes%0Asuch%20as%20topology%20optimization%20of%20the%20part%20subject%20to%20CNC%20tool%20accessibility%0Aconstraints%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02115v2&entry.124074799=Read"},
{"title": "View-Invariant Policy Learning via Zero-Shot Novel View Synthesis", "author": "Stephen Tian and Blake Wulfe and Kyle Sargent and Katherine Liu and Sergey Zakharov and Vitor Guizilini and Jiajun Wu", "abstract": "  Large-scale visuomotor policy learning is a promising approach toward\ndeveloping generalizable manipulation systems. Yet, policies that can be\ndeployed on diverse embodiments, environments, and observational modalities\nremain elusive. In this work, we investigate how knowledge from large-scale\nvisual data of the world may be used to address one axis of variation for\ngeneralizable manipulation: observational viewpoint. Specifically, we study\nsingle-image novel view synthesis models, which learn 3D-aware scene-level\npriors by rendering images of the same scene from alternate camera viewpoints\ngiven a single input image. For practical application to diverse robotic data,\nthese models must operate zero-shot, performing view synthesis on unseen tasks\nand environments. We empirically analyze view synthesis models within a simple\ndata-augmentation scheme that we call View Synthesis Augmentation (VISTA) to\nunderstand their capabilities for learning viewpoint-invariant policies from\nsingle-viewpoint demonstration data. Upon evaluating the robustness of policies\ntrained with our method to out-of-distribution camera viewpoints, we find that\nthey outperform baselines in both simulated and real-world manipulation tasks.\nVideos and additional visualizations are available at\nhttps://s-tian.github.io/projects/vista.\n", "link": "http://arxiv.org/abs/2409.03685v1", "date": "2024-09-05", "relevancy": 2.1403, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20View-Invariant%20Policy%20Learning%20via%20Zero-Shot%20Novel%20View%20Synthesis&body=Title%3A%20View-Invariant%20Policy%20Learning%20via%20Zero-Shot%20Novel%20View%20Synthesis%0AAuthor%3A%20Stephen%20Tian%20and%20Blake%20Wulfe%20and%20Kyle%20Sargent%20and%20Katherine%20Liu%20and%20Sergey%20Zakharov%20and%20Vitor%20Guizilini%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20Large-scale%20visuomotor%20policy%20learning%20is%20a%20promising%20approach%20toward%0Adeveloping%20generalizable%20manipulation%20systems.%20Yet%2C%20policies%20that%20can%20be%0Adeployed%20on%20diverse%20embodiments%2C%20environments%2C%20and%20observational%20modalities%0Aremain%20elusive.%20In%20this%20work%2C%20we%20investigate%20how%20knowledge%20from%20large-scale%0Avisual%20data%20of%20the%20world%20may%20be%20used%20to%20address%20one%20axis%20of%20variation%20for%0Ageneralizable%20manipulation%3A%20observational%20viewpoint.%20Specifically%2C%20we%20study%0Asingle-image%20novel%20view%20synthesis%20models%2C%20which%20learn%203D-aware%20scene-level%0Apriors%20by%20rendering%20images%20of%20the%20same%20scene%20from%20alternate%20camera%20viewpoints%0Agiven%20a%20single%20input%20image.%20For%20practical%20application%20to%20diverse%20robotic%20data%2C%0Athese%20models%20must%20operate%20zero-shot%2C%20performing%20view%20synthesis%20on%20unseen%20tasks%0Aand%20environments.%20We%20empirically%20analyze%20view%20synthesis%20models%20within%20a%20simple%0Adata-augmentation%20scheme%20that%20we%20call%20View%20Synthesis%20Augmentation%20%28VISTA%29%20to%0Aunderstand%20their%20capabilities%20for%20learning%20viewpoint-invariant%20policies%20from%0Asingle-viewpoint%20demonstration%20data.%20Upon%20evaluating%20the%20robustness%20of%20policies%0Atrained%20with%20our%20method%20to%20out-of-distribution%20camera%20viewpoints%2C%20we%20find%20that%0Athey%20outperform%20baselines%20in%20both%20simulated%20and%20real-world%20manipulation%20tasks.%0AVideos%20and%20additional%20visualizations%20are%20available%20at%0Ahttps%3A//s-tian.github.io/projects/vista.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DView-Invariant%2520Policy%2520Learning%2520via%2520Zero-Shot%2520Novel%2520View%2520Synthesis%26entry.906535625%3DStephen%2520Tian%2520and%2520Blake%2520Wulfe%2520and%2520Kyle%2520Sargent%2520and%2520Katherine%2520Liu%2520and%2520Sergey%2520Zakharov%2520and%2520Vitor%2520Guizilini%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520Large-scale%2520visuomotor%2520policy%2520learning%2520is%2520a%2520promising%2520approach%2520toward%250Adeveloping%2520generalizable%2520manipulation%2520systems.%2520Yet%252C%2520policies%2520that%2520can%2520be%250Adeployed%2520on%2520diverse%2520embodiments%252C%2520environments%252C%2520and%2520observational%2520modalities%250Aremain%2520elusive.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520knowledge%2520from%2520large-scale%250Avisual%2520data%2520of%2520the%2520world%2520may%2520be%2520used%2520to%2520address%2520one%2520axis%2520of%2520variation%2520for%250Ageneralizable%2520manipulation%253A%2520observational%2520viewpoint.%2520Specifically%252C%2520we%2520study%250Asingle-image%2520novel%2520view%2520synthesis%2520models%252C%2520which%2520learn%25203D-aware%2520scene-level%250Apriors%2520by%2520rendering%2520images%2520of%2520the%2520same%2520scene%2520from%2520alternate%2520camera%2520viewpoints%250Agiven%2520a%2520single%2520input%2520image.%2520For%2520practical%2520application%2520to%2520diverse%2520robotic%2520data%252C%250Athese%2520models%2520must%2520operate%2520zero-shot%252C%2520performing%2520view%2520synthesis%2520on%2520unseen%2520tasks%250Aand%2520environments.%2520We%2520empirically%2520analyze%2520view%2520synthesis%2520models%2520within%2520a%2520simple%250Adata-augmentation%2520scheme%2520that%2520we%2520call%2520View%2520Synthesis%2520Augmentation%2520%2528VISTA%2529%2520to%250Aunderstand%2520their%2520capabilities%2520for%2520learning%2520viewpoint-invariant%2520policies%2520from%250Asingle-viewpoint%2520demonstration%2520data.%2520Upon%2520evaluating%2520the%2520robustness%2520of%2520policies%250Atrained%2520with%2520our%2520method%2520to%2520out-of-distribution%2520camera%2520viewpoints%252C%2520we%2520find%2520that%250Athey%2520outperform%2520baselines%2520in%2520both%2520simulated%2520and%2520real-world%2520manipulation%2520tasks.%250AVideos%2520and%2520additional%2520visualizations%2520are%2520available%2520at%250Ahttps%253A//s-tian.github.io/projects/vista.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=View-Invariant%20Policy%20Learning%20via%20Zero-Shot%20Novel%20View%20Synthesis&entry.906535625=Stephen%20Tian%20and%20Blake%20Wulfe%20and%20Kyle%20Sargent%20and%20Katherine%20Liu%20and%20Sergey%20Zakharov%20and%20Vitor%20Guizilini%20and%20Jiajun%20Wu&entry.1292438233=%20%20Large-scale%20visuomotor%20policy%20learning%20is%20a%20promising%20approach%20toward%0Adeveloping%20generalizable%20manipulation%20systems.%20Yet%2C%20policies%20that%20can%20be%0Adeployed%20on%20diverse%20embodiments%2C%20environments%2C%20and%20observational%20modalities%0Aremain%20elusive.%20In%20this%20work%2C%20we%20investigate%20how%20knowledge%20from%20large-scale%0Avisual%20data%20of%20the%20world%20may%20be%20used%20to%20address%20one%20axis%20of%20variation%20for%0Ageneralizable%20manipulation%3A%20observational%20viewpoint.%20Specifically%2C%20we%20study%0Asingle-image%20novel%20view%20synthesis%20models%2C%20which%20learn%203D-aware%20scene-level%0Apriors%20by%20rendering%20images%20of%20the%20same%20scene%20from%20alternate%20camera%20viewpoints%0Agiven%20a%20single%20input%20image.%20For%20practical%20application%20to%20diverse%20robotic%20data%2C%0Athese%20models%20must%20operate%20zero-shot%2C%20performing%20view%20synthesis%20on%20unseen%20tasks%0Aand%20environments.%20We%20empirically%20analyze%20view%20synthesis%20models%20within%20a%20simple%0Adata-augmentation%20scheme%20that%20we%20call%20View%20Synthesis%20Augmentation%20%28VISTA%29%20to%0Aunderstand%20their%20capabilities%20for%20learning%20viewpoint-invariant%20policies%20from%0Asingle-viewpoint%20demonstration%20data.%20Upon%20evaluating%20the%20robustness%20of%20policies%0Atrained%20with%20our%20method%20to%20out-of-distribution%20camera%20viewpoints%2C%20we%20find%20that%0Athey%20outperform%20baselines%20in%20both%20simulated%20and%20real-world%20manipulation%20tasks.%0AVideos%20and%20additional%20visualizations%20are%20available%20at%0Ahttps%3A//s-tian.github.io/projects/vista.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03685v1&entry.124074799=Read"},
{"title": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page\n  Document Understanding", "author": "Anwen Hu and Haiyang Xu and Liang Zhang and Jiabo Ye and Ming Yan and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou", "abstract": "  Multimodel Large Language Models(MLLMs) have achieved promising OCR-free\nDocument Understanding performance by increasing the supported resolution of\ndocument images. However, this comes at the cost of generating thousands of\nvisual tokens for a single document image, leading to excessive GPU memory and\nslower inference times, particularly in multi-page document comprehension. In\nthis work, to address these challenges, we propose a High-resolution\nDocCompressor module to compress each high-resolution document image into 324\ntokens, guided by low-resolution global visual features. With this compression\nmodule, to strengthen multi-page document comprehension ability and balance\nboth token efficiency and question-answering performance, we develop the\nDocOwl2 under a three-stage training framework: Single-image Pretraining,\nMulti-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new\nstate-of-the-art across multi-page document understanding benchmarks and\nreduces first token latency by more than 50%, demonstrating advanced\ncapabilities in multi-page questioning answering, explanation with evidence\npages, and cross-page structure understanding. Additionally, compared to\nsingle-image MLLMs trained on similar data, our DocOwl2 achieves comparable\nsingle-page understanding performance with less than 20% of the visual tokens.\nOur codes, models, and data are publicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.\n", "link": "http://arxiv.org/abs/2409.03420v1", "date": "2024-09-05", "relevancy": 2.1348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5362}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mPLUG-DocOwl2%3A%20High-resolution%20Compressing%20for%20OCR-free%20Multi-page%0A%20%20Document%20Understanding&body=Title%3A%20mPLUG-DocOwl2%3A%20High-resolution%20Compressing%20for%20OCR-free%20Multi-page%0A%20%20Document%20Understanding%0AAuthor%3A%20Anwen%20Hu%20and%20Haiyang%20Xu%20and%20Liang%20Zhang%20and%20Jiabo%20Ye%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Qin%20Jin%20and%20Fei%20Huang%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Multimodel%20Large%20Language%20Models%28MLLMs%29%20have%20achieved%20promising%20OCR-free%0ADocument%20Understanding%20performance%20by%20increasing%20the%20supported%20resolution%20of%0Adocument%20images.%20However%2C%20this%20comes%20at%20the%20cost%20of%20generating%20thousands%20of%0Avisual%20tokens%20for%20a%20single%20document%20image%2C%20leading%20to%20excessive%20GPU%20memory%20and%0Aslower%20inference%20times%2C%20particularly%20in%20multi-page%20document%20comprehension.%20In%0Athis%20work%2C%20to%20address%20these%20challenges%2C%20we%20propose%20a%20High-resolution%0ADocCompressor%20module%20to%20compress%20each%20high-resolution%20document%20image%20into%20324%0Atokens%2C%20guided%20by%20low-resolution%20global%20visual%20features.%20With%20this%20compression%0Amodule%2C%20to%20strengthen%20multi-page%20document%20comprehension%20ability%20and%20balance%0Aboth%20token%20efficiency%20and%20question-answering%20performance%2C%20we%20develop%20the%0ADocOwl2%20under%20a%20three-stage%20training%20framework%3A%20Single-image%20Pretraining%2C%0AMulti-image%20Continue-pretraining%2C%20and%20Multi-task%20Finetuning.%20DocOwl2%20sets%20a%20new%0Astate-of-the-art%20across%20multi-page%20document%20understanding%20benchmarks%20and%0Areduces%20first%20token%20latency%20by%20more%20than%2050%25%2C%20demonstrating%20advanced%0Acapabilities%20in%20multi-page%20questioning%20answering%2C%20explanation%20with%20evidence%0Apages%2C%20and%20cross-page%20structure%20understanding.%20Additionally%2C%20compared%20to%0Asingle-image%20MLLMs%20trained%20on%20similar%20data%2C%20our%20DocOwl2%20achieves%20comparable%0Asingle-page%20understanding%20performance%20with%20less%20than%2020%25%20of%20the%20visual%20tokens.%0AOur%20codes%2C%20models%2C%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmPLUG-DocOwl2%253A%2520High-resolution%2520Compressing%2520for%2520OCR-free%2520Multi-page%250A%2520%2520Document%2520Understanding%26entry.906535625%3DAnwen%2520Hu%2520and%2520Haiyang%2520Xu%2520and%2520Liang%2520Zhang%2520and%2520Jiabo%2520Ye%2520and%2520Ming%2520Yan%2520and%2520Ji%2520Zhang%2520and%2520Qin%2520Jin%2520and%2520Fei%2520Huang%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Multimodel%2520Large%2520Language%2520Models%2528MLLMs%2529%2520have%2520achieved%2520promising%2520OCR-free%250ADocument%2520Understanding%2520performance%2520by%2520increasing%2520the%2520supported%2520resolution%2520of%250Adocument%2520images.%2520However%252C%2520this%2520comes%2520at%2520the%2520cost%2520of%2520generating%2520thousands%2520of%250Avisual%2520tokens%2520for%2520a%2520single%2520document%2520image%252C%2520leading%2520to%2520excessive%2520GPU%2520memory%2520and%250Aslower%2520inference%2520times%252C%2520particularly%2520in%2520multi-page%2520document%2520comprehension.%2520In%250Athis%2520work%252C%2520to%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520High-resolution%250ADocCompressor%2520module%2520to%2520compress%2520each%2520high-resolution%2520document%2520image%2520into%2520324%250Atokens%252C%2520guided%2520by%2520low-resolution%2520global%2520visual%2520features.%2520With%2520this%2520compression%250Amodule%252C%2520to%2520strengthen%2520multi-page%2520document%2520comprehension%2520ability%2520and%2520balance%250Aboth%2520token%2520efficiency%2520and%2520question-answering%2520performance%252C%2520we%2520develop%2520the%250ADocOwl2%2520under%2520a%2520three-stage%2520training%2520framework%253A%2520Single-image%2520Pretraining%252C%250AMulti-image%2520Continue-pretraining%252C%2520and%2520Multi-task%2520Finetuning.%2520DocOwl2%2520sets%2520a%2520new%250Astate-of-the-art%2520across%2520multi-page%2520document%2520understanding%2520benchmarks%2520and%250Areduces%2520first%2520token%2520latency%2520by%2520more%2520than%252050%2525%252C%2520demonstrating%2520advanced%250Acapabilities%2520in%2520multi-page%2520questioning%2520answering%252C%2520explanation%2520with%2520evidence%250Apages%252C%2520and%2520cross-page%2520structure%2520understanding.%2520Additionally%252C%2520compared%2520to%250Asingle-image%2520MLLMs%2520trained%2520on%2520similar%2520data%252C%2520our%2520DocOwl2%2520achieves%2520comparable%250Asingle-page%2520understanding%2520performance%2520with%2520less%2520than%252020%2525%2520of%2520the%2520visual%2520tokens.%250AOur%2520codes%252C%2520models%252C%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mPLUG-DocOwl2%3A%20High-resolution%20Compressing%20for%20OCR-free%20Multi-page%0A%20%20Document%20Understanding&entry.906535625=Anwen%20Hu%20and%20Haiyang%20Xu%20and%20Liang%20Zhang%20and%20Jiabo%20Ye%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Qin%20Jin%20and%20Fei%20Huang%20and%20Jingren%20Zhou&entry.1292438233=%20%20Multimodel%20Large%20Language%20Models%28MLLMs%29%20have%20achieved%20promising%20OCR-free%0ADocument%20Understanding%20performance%20by%20increasing%20the%20supported%20resolution%20of%0Adocument%20images.%20However%2C%20this%20comes%20at%20the%20cost%20of%20generating%20thousands%20of%0Avisual%20tokens%20for%20a%20single%20document%20image%2C%20leading%20to%20excessive%20GPU%20memory%20and%0Aslower%20inference%20times%2C%20particularly%20in%20multi-page%20document%20comprehension.%20In%0Athis%20work%2C%20to%20address%20these%20challenges%2C%20we%20propose%20a%20High-resolution%0ADocCompressor%20module%20to%20compress%20each%20high-resolution%20document%20image%20into%20324%0Atokens%2C%20guided%20by%20low-resolution%20global%20visual%20features.%20With%20this%20compression%0Amodule%2C%20to%20strengthen%20multi-page%20document%20comprehension%20ability%20and%20balance%0Aboth%20token%20efficiency%20and%20question-answering%20performance%2C%20we%20develop%20the%0ADocOwl2%20under%20a%20three-stage%20training%20framework%3A%20Single-image%20Pretraining%2C%0AMulti-image%20Continue-pretraining%2C%20and%20Multi-task%20Finetuning.%20DocOwl2%20sets%20a%20new%0Astate-of-the-art%20across%20multi-page%20document%20understanding%20benchmarks%20and%0Areduces%20first%20token%20latency%20by%20more%20than%2050%25%2C%20demonstrating%20advanced%0Acapabilities%20in%20multi-page%20questioning%20answering%2C%20explanation%20with%20evidence%0Apages%2C%20and%20cross-page%20structure%20understanding.%20Additionally%2C%20compared%20to%0Asingle-image%20MLLMs%20trained%20on%20similar%20data%2C%20our%20DocOwl2%20achieves%20comparable%0Asingle-page%20understanding%20performance%20with%20less%20than%2020%25%20of%20the%20visual%20tokens.%0AOur%20codes%2C%20models%2C%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03420v1&entry.124074799=Read"},
{"title": "A Deep Generative Learning Approach for Two-stage Adaptive Robust\n  Optimization", "author": "Aron Brenner and Rahman Khorramfar and Jennifer Sun and Saurabh Amin", "abstract": "  Two-stage adaptive robust optimization is a powerful approach for planning\nunder uncertainty that aims to balance costs of \"here-and-now\" first-stage\ndecisions with those of \"wait-and-see\" recourse decisions made after\nuncertainty is realized. To embed robustness against uncertainty, modelers\ntypically assume a simple polyhedral or ellipsoidal set over which\ncontingencies may be realized. However, these simple uncertainty sets tend to\nyield highly conservative decision-making when uncertainties are\nhigh-dimensional. In this work, we introduce AGRO, a column-and-constraint\ngeneration algorithm that performs adversarial generation for two-stage\nadaptive robust optimization using a variational autoencoder. AGRO identifies\nrealistic and cost-maximizing contingencies by optimizing over spherical\nuncertainty sets in a latent space using a projected gradient ascent approach\nthat differentiates the optimal recourse cost with respect to the latent\nvariable. To demonstrate the cost- and time-efficiency of our approach\nexperimentally, we apply AGRO to an adaptive robust capacity expansion problem\nfor a regional power system and show that AGRO is able to reduce costs by up to\n7.8% and runtimes by up to 77% in comparison to the conventional\ncolumn-and-constraint generation algorithm.\n", "link": "http://arxiv.org/abs/2409.03731v1", "date": "2024-09-05", "relevancy": 2.1173, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.543}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5271}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Generative%20Learning%20Approach%20for%20Two-stage%20Adaptive%20Robust%0A%20%20Optimization&body=Title%3A%20A%20Deep%20Generative%20Learning%20Approach%20for%20Two-stage%20Adaptive%20Robust%0A%20%20Optimization%0AAuthor%3A%20Aron%20Brenner%20and%20Rahman%20Khorramfar%20and%20Jennifer%20Sun%20and%20Saurabh%20Amin%0AAbstract%3A%20%20%20Two-stage%20adaptive%20robust%20optimization%20is%20a%20powerful%20approach%20for%20planning%0Aunder%20uncertainty%20that%20aims%20to%20balance%20costs%20of%20%22here-and-now%22%20first-stage%0Adecisions%20with%20those%20of%20%22wait-and-see%22%20recourse%20decisions%20made%20after%0Auncertainty%20is%20realized.%20To%20embed%20robustness%20against%20uncertainty%2C%20modelers%0Atypically%20assume%20a%20simple%20polyhedral%20or%20ellipsoidal%20set%20over%20which%0Acontingencies%20may%20be%20realized.%20However%2C%20these%20simple%20uncertainty%20sets%20tend%20to%0Ayield%20highly%20conservative%20decision-making%20when%20uncertainties%20are%0Ahigh-dimensional.%20In%20this%20work%2C%20we%20introduce%20AGRO%2C%20a%20column-and-constraint%0Ageneration%20algorithm%20that%20performs%20adversarial%20generation%20for%20two-stage%0Aadaptive%20robust%20optimization%20using%20a%20variational%20autoencoder.%20AGRO%20identifies%0Arealistic%20and%20cost-maximizing%20contingencies%20by%20optimizing%20over%20spherical%0Auncertainty%20sets%20in%20a%20latent%20space%20using%20a%20projected%20gradient%20ascent%20approach%0Athat%20differentiates%20the%20optimal%20recourse%20cost%20with%20respect%20to%20the%20latent%0Avariable.%20To%20demonstrate%20the%20cost-%20and%20time-efficiency%20of%20our%20approach%0Aexperimentally%2C%20we%20apply%20AGRO%20to%20an%20adaptive%20robust%20capacity%20expansion%20problem%0Afor%20a%20regional%20power%20system%20and%20show%20that%20AGRO%20is%20able%20to%20reduce%20costs%20by%20up%20to%0A7.8%25%20and%20runtimes%20by%20up%20to%2077%25%20in%20comparison%20to%20the%20conventional%0Acolumn-and-constraint%20generation%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Generative%2520Learning%2520Approach%2520for%2520Two-stage%2520Adaptive%2520Robust%250A%2520%2520Optimization%26entry.906535625%3DAron%2520Brenner%2520and%2520Rahman%2520Khorramfar%2520and%2520Jennifer%2520Sun%2520and%2520Saurabh%2520Amin%26entry.1292438233%3D%2520%2520Two-stage%2520adaptive%2520robust%2520optimization%2520is%2520a%2520powerful%2520approach%2520for%2520planning%250Aunder%2520uncertainty%2520that%2520aims%2520to%2520balance%2520costs%2520of%2520%2522here-and-now%2522%2520first-stage%250Adecisions%2520with%2520those%2520of%2520%2522wait-and-see%2522%2520recourse%2520decisions%2520made%2520after%250Auncertainty%2520is%2520realized.%2520To%2520embed%2520robustness%2520against%2520uncertainty%252C%2520modelers%250Atypically%2520assume%2520a%2520simple%2520polyhedral%2520or%2520ellipsoidal%2520set%2520over%2520which%250Acontingencies%2520may%2520be%2520realized.%2520However%252C%2520these%2520simple%2520uncertainty%2520sets%2520tend%2520to%250Ayield%2520highly%2520conservative%2520decision-making%2520when%2520uncertainties%2520are%250Ahigh-dimensional.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AGRO%252C%2520a%2520column-and-constraint%250Ageneration%2520algorithm%2520that%2520performs%2520adversarial%2520generation%2520for%2520two-stage%250Aadaptive%2520robust%2520optimization%2520using%2520a%2520variational%2520autoencoder.%2520AGRO%2520identifies%250Arealistic%2520and%2520cost-maximizing%2520contingencies%2520by%2520optimizing%2520over%2520spherical%250Auncertainty%2520sets%2520in%2520a%2520latent%2520space%2520using%2520a%2520projected%2520gradient%2520ascent%2520approach%250Athat%2520differentiates%2520the%2520optimal%2520recourse%2520cost%2520with%2520respect%2520to%2520the%2520latent%250Avariable.%2520To%2520demonstrate%2520the%2520cost-%2520and%2520time-efficiency%2520of%2520our%2520approach%250Aexperimentally%252C%2520we%2520apply%2520AGRO%2520to%2520an%2520adaptive%2520robust%2520capacity%2520expansion%2520problem%250Afor%2520a%2520regional%2520power%2520system%2520and%2520show%2520that%2520AGRO%2520is%2520able%2520to%2520reduce%2520costs%2520by%2520up%2520to%250A7.8%2525%2520and%2520runtimes%2520by%2520up%2520to%252077%2525%2520in%2520comparison%2520to%2520the%2520conventional%250Acolumn-and-constraint%2520generation%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Generative%20Learning%20Approach%20for%20Two-stage%20Adaptive%20Robust%0A%20%20Optimization&entry.906535625=Aron%20Brenner%20and%20Rahman%20Khorramfar%20and%20Jennifer%20Sun%20and%20Saurabh%20Amin&entry.1292438233=%20%20Two-stage%20adaptive%20robust%20optimization%20is%20a%20powerful%20approach%20for%20planning%0Aunder%20uncertainty%20that%20aims%20to%20balance%20costs%20of%20%22here-and-now%22%20first-stage%0Adecisions%20with%20those%20of%20%22wait-and-see%22%20recourse%20decisions%20made%20after%0Auncertainty%20is%20realized.%20To%20embed%20robustness%20against%20uncertainty%2C%20modelers%0Atypically%20assume%20a%20simple%20polyhedral%20or%20ellipsoidal%20set%20over%20which%0Acontingencies%20may%20be%20realized.%20However%2C%20these%20simple%20uncertainty%20sets%20tend%20to%0Ayield%20highly%20conservative%20decision-making%20when%20uncertainties%20are%0Ahigh-dimensional.%20In%20this%20work%2C%20we%20introduce%20AGRO%2C%20a%20column-and-constraint%0Ageneration%20algorithm%20that%20performs%20adversarial%20generation%20for%20two-stage%0Aadaptive%20robust%20optimization%20using%20a%20variational%20autoencoder.%20AGRO%20identifies%0Arealistic%20and%20cost-maximizing%20contingencies%20by%20optimizing%20over%20spherical%0Auncertainty%20sets%20in%20a%20latent%20space%20using%20a%20projected%20gradient%20ascent%20approach%0Athat%20differentiates%20the%20optimal%20recourse%20cost%20with%20respect%20to%20the%20latent%0Avariable.%20To%20demonstrate%20the%20cost-%20and%20time-efficiency%20of%20our%20approach%0Aexperimentally%2C%20we%20apply%20AGRO%20to%20an%20adaptive%20robust%20capacity%20expansion%20problem%0Afor%20a%20regional%20power%20system%20and%20show%20that%20AGRO%20is%20able%20to%20reduce%20costs%20by%20up%20to%0A7.8%25%20and%20runtimes%20by%20up%20to%2077%25%20in%20comparison%20to%20the%20conventional%0Acolumn-and-constraint%20generation%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03731v1&entry.124074799=Read"},
{"title": "Towards Neural Network based Cognitive Models of Dynamic Decision-Making\n  by Humans", "author": "Changyu Chen and Shashank Reddy Chirra and Maria Jos\u00e9 Ferreira and Cleotilde Gonzalez and Arunesh Sinha and Pradeep Varakantham", "abstract": "  Modeling human cognitive processes in dynamic decision-making tasks has been\nan endeavor in AI for a long time because such models can help make AI systems\nmore intuitive, personalized, mitigate any human biases, and enhance training\nin simulation. Some initial work has attempted to utilize neural networks (and\nlarge language models) but often assumes one common model for all humans and\naims to emulate human behavior in aggregate. However, the behavior of each\nhuman is distinct, heterogeneous, and relies on specific past experiences in\ncertain tasks. For instance, consider two individuals responding to a phishing\nemail: one who has previously encountered and identified similar threats may\nrecognize it quickly, while another without such experience might fall for the\nscam. In this work, we build on Instance Based Learning (IBL) that posits that\nhuman decisions are based on similar situations encountered in the past.\nHowever, IBL relies on simple fixed form functions to capture the mapping from\npast situations to current decisions. To that end, we propose two new\nattention-based neural network models to have open form non-linear functions to\nmodel distinct and heterogeneous human decision-making in dynamic settings. We\nexperiment with two distinct datasets gathered from human subject experiment\ndata, one focusing on detection of phishing email by humans and another where\nhumans act as attackers in a cybersecurity setting and decide on an attack\noption. We conducted extensive experiments with our two neural network models,\nIBL, and GPT3.5, and demonstrate that the neural network models outperform IBL\nsignificantly in representing human decision-making, while providing similar\ninterpretability of human decisions as IBL. Overall, our work yields promising\nresults for further use of neural networks in cognitive modeling of human\ndecision making.\n", "link": "http://arxiv.org/abs/2407.17622v2", "date": "2024-09-05", "relevancy": 2.1134, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5774}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Neural%20Network%20based%20Cognitive%20Models%20of%20Dynamic%20Decision-Making%0A%20%20by%20Humans&body=Title%3A%20Towards%20Neural%20Network%20based%20Cognitive%20Models%20of%20Dynamic%20Decision-Making%0A%20%20by%20Humans%0AAuthor%3A%20Changyu%20Chen%20and%20Shashank%20Reddy%20Chirra%20and%20Maria%20Jos%C3%A9%20Ferreira%20and%20Cleotilde%20Gonzalez%20and%20Arunesh%20Sinha%20and%20Pradeep%20Varakantham%0AAbstract%3A%20%20%20Modeling%20human%20cognitive%20processes%20in%20dynamic%20decision-making%20tasks%20has%20been%0Aan%20endeavor%20in%20AI%20for%20a%20long%20time%20because%20such%20models%20can%20help%20make%20AI%20systems%0Amore%20intuitive%2C%20personalized%2C%20mitigate%20any%20human%20biases%2C%20and%20enhance%20training%0Ain%20simulation.%20Some%20initial%20work%20has%20attempted%20to%20utilize%20neural%20networks%20%28and%0Alarge%20language%20models%29%20but%20often%20assumes%20one%20common%20model%20for%20all%20humans%20and%0Aaims%20to%20emulate%20human%20behavior%20in%20aggregate.%20However%2C%20the%20behavior%20of%20each%0Ahuman%20is%20distinct%2C%20heterogeneous%2C%20and%20relies%20on%20specific%20past%20experiences%20in%0Acertain%20tasks.%20For%20instance%2C%20consider%20two%20individuals%20responding%20to%20a%20phishing%0Aemail%3A%20one%20who%20has%20previously%20encountered%20and%20identified%20similar%20threats%20may%0Arecognize%20it%20quickly%2C%20while%20another%20without%20such%20experience%20might%20fall%20for%20the%0Ascam.%20In%20this%20work%2C%20we%20build%20on%20Instance%20Based%20Learning%20%28IBL%29%20that%20posits%20that%0Ahuman%20decisions%20are%20based%20on%20similar%20situations%20encountered%20in%20the%20past.%0AHowever%2C%20IBL%20relies%20on%20simple%20fixed%20form%20functions%20to%20capture%20the%20mapping%20from%0Apast%20situations%20to%20current%20decisions.%20To%20that%20end%2C%20we%20propose%20two%20new%0Aattention-based%20neural%20network%20models%20to%20have%20open%20form%20non-linear%20functions%20to%0Amodel%20distinct%20and%20heterogeneous%20human%20decision-making%20in%20dynamic%20settings.%20We%0Aexperiment%20with%20two%20distinct%20datasets%20gathered%20from%20human%20subject%20experiment%0Adata%2C%20one%20focusing%20on%20detection%20of%20phishing%20email%20by%20humans%20and%20another%20where%0Ahumans%20act%20as%20attackers%20in%20a%20cybersecurity%20setting%20and%20decide%20on%20an%20attack%0Aoption.%20We%20conducted%20extensive%20experiments%20with%20our%20two%20neural%20network%20models%2C%0AIBL%2C%20and%20GPT3.5%2C%20and%20demonstrate%20that%20the%20neural%20network%20models%20outperform%20IBL%0Asignificantly%20in%20representing%20human%20decision-making%2C%20while%20providing%20similar%0Ainterpretability%20of%20human%20decisions%20as%20IBL.%20Overall%2C%20our%20work%20yields%20promising%0Aresults%20for%20further%20use%20of%20neural%20networks%20in%20cognitive%20modeling%20of%20human%0Adecision%20making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Neural%2520Network%2520based%2520Cognitive%2520Models%2520of%2520Dynamic%2520Decision-Making%250A%2520%2520by%2520Humans%26entry.906535625%3DChangyu%2520Chen%2520and%2520Shashank%2520Reddy%2520Chirra%2520and%2520Maria%2520Jos%25C3%25A9%2520Ferreira%2520and%2520Cleotilde%2520Gonzalez%2520and%2520Arunesh%2520Sinha%2520and%2520Pradeep%2520Varakantham%26entry.1292438233%3D%2520%2520Modeling%2520human%2520cognitive%2520processes%2520in%2520dynamic%2520decision-making%2520tasks%2520has%2520been%250Aan%2520endeavor%2520in%2520AI%2520for%2520a%2520long%2520time%2520because%2520such%2520models%2520can%2520help%2520make%2520AI%2520systems%250Amore%2520intuitive%252C%2520personalized%252C%2520mitigate%2520any%2520human%2520biases%252C%2520and%2520enhance%2520training%250Ain%2520simulation.%2520Some%2520initial%2520work%2520has%2520attempted%2520to%2520utilize%2520neural%2520networks%2520%2528and%250Alarge%2520language%2520models%2529%2520but%2520often%2520assumes%2520one%2520common%2520model%2520for%2520all%2520humans%2520and%250Aaims%2520to%2520emulate%2520human%2520behavior%2520in%2520aggregate.%2520However%252C%2520the%2520behavior%2520of%2520each%250Ahuman%2520is%2520distinct%252C%2520heterogeneous%252C%2520and%2520relies%2520on%2520specific%2520past%2520experiences%2520in%250Acertain%2520tasks.%2520For%2520instance%252C%2520consider%2520two%2520individuals%2520responding%2520to%2520a%2520phishing%250Aemail%253A%2520one%2520who%2520has%2520previously%2520encountered%2520and%2520identified%2520similar%2520threats%2520may%250Arecognize%2520it%2520quickly%252C%2520while%2520another%2520without%2520such%2520experience%2520might%2520fall%2520for%2520the%250Ascam.%2520In%2520this%2520work%252C%2520we%2520build%2520on%2520Instance%2520Based%2520Learning%2520%2528IBL%2529%2520that%2520posits%2520that%250Ahuman%2520decisions%2520are%2520based%2520on%2520similar%2520situations%2520encountered%2520in%2520the%2520past.%250AHowever%252C%2520IBL%2520relies%2520on%2520simple%2520fixed%2520form%2520functions%2520to%2520capture%2520the%2520mapping%2520from%250Apast%2520situations%2520to%2520current%2520decisions.%2520To%2520that%2520end%252C%2520we%2520propose%2520two%2520new%250Aattention-based%2520neural%2520network%2520models%2520to%2520have%2520open%2520form%2520non-linear%2520functions%2520to%250Amodel%2520distinct%2520and%2520heterogeneous%2520human%2520decision-making%2520in%2520dynamic%2520settings.%2520We%250Aexperiment%2520with%2520two%2520distinct%2520datasets%2520gathered%2520from%2520human%2520subject%2520experiment%250Adata%252C%2520one%2520focusing%2520on%2520detection%2520of%2520phishing%2520email%2520by%2520humans%2520and%2520another%2520where%250Ahumans%2520act%2520as%2520attackers%2520in%2520a%2520cybersecurity%2520setting%2520and%2520decide%2520on%2520an%2520attack%250Aoption.%2520We%2520conducted%2520extensive%2520experiments%2520with%2520our%2520two%2520neural%2520network%2520models%252C%250AIBL%252C%2520and%2520GPT3.5%252C%2520and%2520demonstrate%2520that%2520the%2520neural%2520network%2520models%2520outperform%2520IBL%250Asignificantly%2520in%2520representing%2520human%2520decision-making%252C%2520while%2520providing%2520similar%250Ainterpretability%2520of%2520human%2520decisions%2520as%2520IBL.%2520Overall%252C%2520our%2520work%2520yields%2520promising%250Aresults%2520for%2520further%2520use%2520of%2520neural%2520networks%2520in%2520cognitive%2520modeling%2520of%2520human%250Adecision%2520making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Neural%20Network%20based%20Cognitive%20Models%20of%20Dynamic%20Decision-Making%0A%20%20by%20Humans&entry.906535625=Changyu%20Chen%20and%20Shashank%20Reddy%20Chirra%20and%20Maria%20Jos%C3%A9%20Ferreira%20and%20Cleotilde%20Gonzalez%20and%20Arunesh%20Sinha%20and%20Pradeep%20Varakantham&entry.1292438233=%20%20Modeling%20human%20cognitive%20processes%20in%20dynamic%20decision-making%20tasks%20has%20been%0Aan%20endeavor%20in%20AI%20for%20a%20long%20time%20because%20such%20models%20can%20help%20make%20AI%20systems%0Amore%20intuitive%2C%20personalized%2C%20mitigate%20any%20human%20biases%2C%20and%20enhance%20training%0Ain%20simulation.%20Some%20initial%20work%20has%20attempted%20to%20utilize%20neural%20networks%20%28and%0Alarge%20language%20models%29%20but%20often%20assumes%20one%20common%20model%20for%20all%20humans%20and%0Aaims%20to%20emulate%20human%20behavior%20in%20aggregate.%20However%2C%20the%20behavior%20of%20each%0Ahuman%20is%20distinct%2C%20heterogeneous%2C%20and%20relies%20on%20specific%20past%20experiences%20in%0Acertain%20tasks.%20For%20instance%2C%20consider%20two%20individuals%20responding%20to%20a%20phishing%0Aemail%3A%20one%20who%20has%20previously%20encountered%20and%20identified%20similar%20threats%20may%0Arecognize%20it%20quickly%2C%20while%20another%20without%20such%20experience%20might%20fall%20for%20the%0Ascam.%20In%20this%20work%2C%20we%20build%20on%20Instance%20Based%20Learning%20%28IBL%29%20that%20posits%20that%0Ahuman%20decisions%20are%20based%20on%20similar%20situations%20encountered%20in%20the%20past.%0AHowever%2C%20IBL%20relies%20on%20simple%20fixed%20form%20functions%20to%20capture%20the%20mapping%20from%0Apast%20situations%20to%20current%20decisions.%20To%20that%20end%2C%20we%20propose%20two%20new%0Aattention-based%20neural%20network%20models%20to%20have%20open%20form%20non-linear%20functions%20to%0Amodel%20distinct%20and%20heterogeneous%20human%20decision-making%20in%20dynamic%20settings.%20We%0Aexperiment%20with%20two%20distinct%20datasets%20gathered%20from%20human%20subject%20experiment%0Adata%2C%20one%20focusing%20on%20detection%20of%20phishing%20email%20by%20humans%20and%20another%20where%0Ahumans%20act%20as%20attackers%20in%20a%20cybersecurity%20setting%20and%20decide%20on%20an%20attack%0Aoption.%20We%20conducted%20extensive%20experiments%20with%20our%20two%20neural%20network%20models%2C%0AIBL%2C%20and%20GPT3.5%2C%20and%20demonstrate%20that%20the%20neural%20network%20models%20outperform%20IBL%0Asignificantly%20in%20representing%20human%20decision-making%2C%20while%20providing%20similar%0Ainterpretability%20of%20human%20decisions%20as%20IBL.%20Overall%2C%20our%20work%20yields%20promising%0Aresults%20for%20further%20use%20of%20neural%20networks%20in%20cognitive%20modeling%20of%20human%0Adecision%20making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17622v2&entry.124074799=Read"},
{"title": "Efficient Incremental Penetration Depth Estimation between Convex\n  Geometries", "author": "Wei Gao", "abstract": "  Penetration depth (PD) is essential for robotics due to its extensive\napplications in dynamic simulation, motion planning, haptic rendering, etc. The\nExpanding Polytope Algorithm (EPA) is the de facto standard for this problem,\nwhich estimates PD by expanding an inner polyhedral approximation of an\nimplicit set. In this paper, we propose a novel optimization-based algorithm\nthat incrementally estimates minimum penetration depth and its direction. One\nmajor advantage of our method is that it can be warm-started by exploiting the\nspatial and temporal coherence, which emerges naturally in many robotic\napplications (e.g., the temporal coherence between adjacent simulation time\nknots). As a result, our algorithm achieves substantial speedup -- we\ndemonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our\napproach is built upon the same implicit geometry representation as EPA, which\nenables easy integration and deployment into existing software stacks. We also\nprovide an open-source implementation on: https://github.com/weigao95/mind-fcl\n", "link": "http://arxiv.org/abs/2304.07357v2", "date": "2024-09-05", "relevancy": 2.1049, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Incremental%20Penetration%20Depth%20Estimation%20between%20Convex%0A%20%20Geometries&body=Title%3A%20Efficient%20Incremental%20Penetration%20Depth%20Estimation%20between%20Convex%0A%20%20Geometries%0AAuthor%3A%20Wei%20Gao%0AAbstract%3A%20%20%20Penetration%20depth%20%28PD%29%20is%20essential%20for%20robotics%20due%20to%20its%20extensive%0Aapplications%20in%20dynamic%20simulation%2C%20motion%20planning%2C%20haptic%20rendering%2C%20etc.%20The%0AExpanding%20Polytope%20Algorithm%20%28EPA%29%20is%20the%20de%20facto%20standard%20for%20this%20problem%2C%0Awhich%20estimates%20PD%20by%20expanding%20an%20inner%20polyhedral%20approximation%20of%20an%0Aimplicit%20set.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20optimization-based%20algorithm%0Athat%20incrementally%20estimates%20minimum%20penetration%20depth%20and%20its%20direction.%20One%0Amajor%20advantage%20of%20our%20method%20is%20that%20it%20can%20be%20warm-started%20by%20exploiting%20the%0Aspatial%20and%20temporal%20coherence%2C%20which%20emerges%20naturally%20in%20many%20robotic%0Aapplications%20%28e.g.%2C%20the%20temporal%20coherence%20between%20adjacent%20simulation%20time%0Aknots%29.%20As%20a%20result%2C%20our%20algorithm%20achieves%20substantial%20speedup%20--%20we%0Ademonstrate%20it%20is%205-30x%20faster%20than%20EPA%20on%20several%20benchmarks.%20Moreover%2C%20our%0Aapproach%20is%20built%20upon%20the%20same%20implicit%20geometry%20representation%20as%20EPA%2C%20which%0Aenables%20easy%20integration%20and%20deployment%20into%20existing%20software%20stacks.%20We%20also%0Aprovide%20an%20open-source%20implementation%20on%3A%20https%3A//github.com/weigao95/mind-fcl%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.07357v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Incremental%2520Penetration%2520Depth%2520Estimation%2520between%2520Convex%250A%2520%2520Geometries%26entry.906535625%3DWei%2520Gao%26entry.1292438233%3D%2520%2520Penetration%2520depth%2520%2528PD%2529%2520is%2520essential%2520for%2520robotics%2520due%2520to%2520its%2520extensive%250Aapplications%2520in%2520dynamic%2520simulation%252C%2520motion%2520planning%252C%2520haptic%2520rendering%252C%2520etc.%2520The%250AExpanding%2520Polytope%2520Algorithm%2520%2528EPA%2529%2520is%2520the%2520de%2520facto%2520standard%2520for%2520this%2520problem%252C%250Awhich%2520estimates%2520PD%2520by%2520expanding%2520an%2520inner%2520polyhedral%2520approximation%2520of%2520an%250Aimplicit%2520set.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520optimization-based%2520algorithm%250Athat%2520incrementally%2520estimates%2520minimum%2520penetration%2520depth%2520and%2520its%2520direction.%2520One%250Amajor%2520advantage%2520of%2520our%2520method%2520is%2520that%2520it%2520can%2520be%2520warm-started%2520by%2520exploiting%2520the%250Aspatial%2520and%2520temporal%2520coherence%252C%2520which%2520emerges%2520naturally%2520in%2520many%2520robotic%250Aapplications%2520%2528e.g.%252C%2520the%2520temporal%2520coherence%2520between%2520adjacent%2520simulation%2520time%250Aknots%2529.%2520As%2520a%2520result%252C%2520our%2520algorithm%2520achieves%2520substantial%2520speedup%2520--%2520we%250Ademonstrate%2520it%2520is%25205-30x%2520faster%2520than%2520EPA%2520on%2520several%2520benchmarks.%2520Moreover%252C%2520our%250Aapproach%2520is%2520built%2520upon%2520the%2520same%2520implicit%2520geometry%2520representation%2520as%2520EPA%252C%2520which%250Aenables%2520easy%2520integration%2520and%2520deployment%2520into%2520existing%2520software%2520stacks.%2520We%2520also%250Aprovide%2520an%2520open-source%2520implementation%2520on%253A%2520https%253A//github.com/weigao95/mind-fcl%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.07357v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Incremental%20Penetration%20Depth%20Estimation%20between%20Convex%0A%20%20Geometries&entry.906535625=Wei%20Gao&entry.1292438233=%20%20Penetration%20depth%20%28PD%29%20is%20essential%20for%20robotics%20due%20to%20its%20extensive%0Aapplications%20in%20dynamic%20simulation%2C%20motion%20planning%2C%20haptic%20rendering%2C%20etc.%20The%0AExpanding%20Polytope%20Algorithm%20%28EPA%29%20is%20the%20de%20facto%20standard%20for%20this%20problem%2C%0Awhich%20estimates%20PD%20by%20expanding%20an%20inner%20polyhedral%20approximation%20of%20an%0Aimplicit%20set.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20optimization-based%20algorithm%0Athat%20incrementally%20estimates%20minimum%20penetration%20depth%20and%20its%20direction.%20One%0Amajor%20advantage%20of%20our%20method%20is%20that%20it%20can%20be%20warm-started%20by%20exploiting%20the%0Aspatial%20and%20temporal%20coherence%2C%20which%20emerges%20naturally%20in%20many%20robotic%0Aapplications%20%28e.g.%2C%20the%20temporal%20coherence%20between%20adjacent%20simulation%20time%0Aknots%29.%20As%20a%20result%2C%20our%20algorithm%20achieves%20substantial%20speedup%20--%20we%0Ademonstrate%20it%20is%205-30x%20faster%20than%20EPA%20on%20several%20benchmarks.%20Moreover%2C%20our%0Aapproach%20is%20built%20upon%20the%20same%20implicit%20geometry%20representation%20as%20EPA%2C%20which%0Aenables%20easy%20integration%20and%20deployment%20into%20existing%20software%20stacks.%20We%20also%0Aprovide%20an%20open-source%20implementation%20on%3A%20https%3A//github.com/weigao95/mind-fcl%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07357v2&entry.124074799=Read"},
{"title": "Limited but consistent gains in adversarial robustness by co-training\n  object recognition models with human EEG", "author": "Manshan Guo and Bhavin Choksi and Sari Sadiya and Alessandro T. Gifford and Martina G. Vilas and Radoslaw M. Cichy and Gemma Roig", "abstract": "  In contrast to human vision, artificial neural networks (ANNs) remain\nrelatively susceptible to adversarial attacks. To address this vulnerability,\nefforts have been made to transfer inductive bias from human brains to ANNs,\noften by training the ANN representations to match their biological\ncounterparts. Previous works relied on brain data acquired in rodents or\nprimates using invasive techniques, from specific regions of the brain, under\nnon-natural conditions (anesthetized animals), and with stimulus datasets\nlacking diversity and naturalness. In this work, we explored whether aligning\nmodel representations to human EEG responses to a rich set of real-world images\nincreases robustness to ANNs. Specifically, we trained ResNet50-backbone models\non a dual task of classification and EEG prediction; and evaluated their EEG\nprediction accuracy and robustness to adversarial attacks. We observed\nsignificant correlation between the networks' EEG prediction accuracy, often\nhighest around 100 ms post stimulus onset, and their gains in adversarial\nrobustness. Although effect size was limited, effects were consistent across\ndifferent random initializations and robust for architectural variants. We\nfurther teased apart the data from individual EEG channels and observed\nstrongest contribution from electrodes in the parieto-occipital regions. The\ndemonstrated utility of human EEG for such tasks opens up avenues for future\nefforts that scale to larger datasets under diverse stimuli conditions with the\npromise of stronger effects.\n", "link": "http://arxiv.org/abs/2409.03646v1", "date": "2024-09-05", "relevancy": 2.1037, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5106}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limited%20but%20consistent%20gains%20in%20adversarial%20robustness%20by%20co-training%0A%20%20object%20recognition%20models%20with%20human%20EEG&body=Title%3A%20Limited%20but%20consistent%20gains%20in%20adversarial%20robustness%20by%20co-training%0A%20%20object%20recognition%20models%20with%20human%20EEG%0AAuthor%3A%20Manshan%20Guo%20and%20Bhavin%20Choksi%20and%20Sari%20Sadiya%20and%20Alessandro%20T.%20Gifford%20and%20Martina%20G.%20Vilas%20and%20Radoslaw%20M.%20Cichy%20and%20Gemma%20Roig%0AAbstract%3A%20%20%20In%20contrast%20to%20human%20vision%2C%20artificial%20neural%20networks%20%28ANNs%29%20remain%0Arelatively%20susceptible%20to%20adversarial%20attacks.%20To%20address%20this%20vulnerability%2C%0Aefforts%20have%20been%20made%20to%20transfer%20inductive%20bias%20from%20human%20brains%20to%20ANNs%2C%0Aoften%20by%20training%20the%20ANN%20representations%20to%20match%20their%20biological%0Acounterparts.%20Previous%20works%20relied%20on%20brain%20data%20acquired%20in%20rodents%20or%0Aprimates%20using%20invasive%20techniques%2C%20from%20specific%20regions%20of%20the%20brain%2C%20under%0Anon-natural%20conditions%20%28anesthetized%20animals%29%2C%20and%20with%20stimulus%20datasets%0Alacking%20diversity%20and%20naturalness.%20In%20this%20work%2C%20we%20explored%20whether%20aligning%0Amodel%20representations%20to%20human%20EEG%20responses%20to%20a%20rich%20set%20of%20real-world%20images%0Aincreases%20robustness%20to%20ANNs.%20Specifically%2C%20we%20trained%20ResNet50-backbone%20models%0Aon%20a%20dual%20task%20of%20classification%20and%20EEG%20prediction%3B%20and%20evaluated%20their%20EEG%0Aprediction%20accuracy%20and%20robustness%20to%20adversarial%20attacks.%20We%20observed%0Asignificant%20correlation%20between%20the%20networks%27%20EEG%20prediction%20accuracy%2C%20often%0Ahighest%20around%20100%20ms%20post%20stimulus%20onset%2C%20and%20their%20gains%20in%20adversarial%0Arobustness.%20Although%20effect%20size%20was%20limited%2C%20effects%20were%20consistent%20across%0Adifferent%20random%20initializations%20and%20robust%20for%20architectural%20variants.%20We%0Afurther%20teased%20apart%20the%20data%20from%20individual%20EEG%20channels%20and%20observed%0Astrongest%20contribution%20from%20electrodes%20in%20the%20parieto-occipital%20regions.%20The%0Ademonstrated%20utility%20of%20human%20EEG%20for%20such%20tasks%20opens%20up%20avenues%20for%20future%0Aefforts%20that%20scale%20to%20larger%20datasets%20under%20diverse%20stimuli%20conditions%20with%20the%0Apromise%20of%20stronger%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimited%2520but%2520consistent%2520gains%2520in%2520adversarial%2520robustness%2520by%2520co-training%250A%2520%2520object%2520recognition%2520models%2520with%2520human%2520EEG%26entry.906535625%3DManshan%2520Guo%2520and%2520Bhavin%2520Choksi%2520and%2520Sari%2520Sadiya%2520and%2520Alessandro%2520T.%2520Gifford%2520and%2520Martina%2520G.%2520Vilas%2520and%2520Radoslaw%2520M.%2520Cichy%2520and%2520Gemma%2520Roig%26entry.1292438233%3D%2520%2520In%2520contrast%2520to%2520human%2520vision%252C%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%2520remain%250Arelatively%2520susceptible%2520to%2520adversarial%2520attacks.%2520To%2520address%2520this%2520vulnerability%252C%250Aefforts%2520have%2520been%2520made%2520to%2520transfer%2520inductive%2520bias%2520from%2520human%2520brains%2520to%2520ANNs%252C%250Aoften%2520by%2520training%2520the%2520ANN%2520representations%2520to%2520match%2520their%2520biological%250Acounterparts.%2520Previous%2520works%2520relied%2520on%2520brain%2520data%2520acquired%2520in%2520rodents%2520or%250Aprimates%2520using%2520invasive%2520techniques%252C%2520from%2520specific%2520regions%2520of%2520the%2520brain%252C%2520under%250Anon-natural%2520conditions%2520%2528anesthetized%2520animals%2529%252C%2520and%2520with%2520stimulus%2520datasets%250Alacking%2520diversity%2520and%2520naturalness.%2520In%2520this%2520work%252C%2520we%2520explored%2520whether%2520aligning%250Amodel%2520representations%2520to%2520human%2520EEG%2520responses%2520to%2520a%2520rich%2520set%2520of%2520real-world%2520images%250Aincreases%2520robustness%2520to%2520ANNs.%2520Specifically%252C%2520we%2520trained%2520ResNet50-backbone%2520models%250Aon%2520a%2520dual%2520task%2520of%2520classification%2520and%2520EEG%2520prediction%253B%2520and%2520evaluated%2520their%2520EEG%250Aprediction%2520accuracy%2520and%2520robustness%2520to%2520adversarial%2520attacks.%2520We%2520observed%250Asignificant%2520correlation%2520between%2520the%2520networks%2527%2520EEG%2520prediction%2520accuracy%252C%2520often%250Ahighest%2520around%2520100%2520ms%2520post%2520stimulus%2520onset%252C%2520and%2520their%2520gains%2520in%2520adversarial%250Arobustness.%2520Although%2520effect%2520size%2520was%2520limited%252C%2520effects%2520were%2520consistent%2520across%250Adifferent%2520random%2520initializations%2520and%2520robust%2520for%2520architectural%2520variants.%2520We%250Afurther%2520teased%2520apart%2520the%2520data%2520from%2520individual%2520EEG%2520channels%2520and%2520observed%250Astrongest%2520contribution%2520from%2520electrodes%2520in%2520the%2520parieto-occipital%2520regions.%2520The%250Ademonstrated%2520utility%2520of%2520human%2520EEG%2520for%2520such%2520tasks%2520opens%2520up%2520avenues%2520for%2520future%250Aefforts%2520that%2520scale%2520to%2520larger%2520datasets%2520under%2520diverse%2520stimuli%2520conditions%2520with%2520the%250Apromise%2520of%2520stronger%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limited%20but%20consistent%20gains%20in%20adversarial%20robustness%20by%20co-training%0A%20%20object%20recognition%20models%20with%20human%20EEG&entry.906535625=Manshan%20Guo%20and%20Bhavin%20Choksi%20and%20Sari%20Sadiya%20and%20Alessandro%20T.%20Gifford%20and%20Martina%20G.%20Vilas%20and%20Radoslaw%20M.%20Cichy%20and%20Gemma%20Roig&entry.1292438233=%20%20In%20contrast%20to%20human%20vision%2C%20artificial%20neural%20networks%20%28ANNs%29%20remain%0Arelatively%20susceptible%20to%20adversarial%20attacks.%20To%20address%20this%20vulnerability%2C%0Aefforts%20have%20been%20made%20to%20transfer%20inductive%20bias%20from%20human%20brains%20to%20ANNs%2C%0Aoften%20by%20training%20the%20ANN%20representations%20to%20match%20their%20biological%0Acounterparts.%20Previous%20works%20relied%20on%20brain%20data%20acquired%20in%20rodents%20or%0Aprimates%20using%20invasive%20techniques%2C%20from%20specific%20regions%20of%20the%20brain%2C%20under%0Anon-natural%20conditions%20%28anesthetized%20animals%29%2C%20and%20with%20stimulus%20datasets%0Alacking%20diversity%20and%20naturalness.%20In%20this%20work%2C%20we%20explored%20whether%20aligning%0Amodel%20representations%20to%20human%20EEG%20responses%20to%20a%20rich%20set%20of%20real-world%20images%0Aincreases%20robustness%20to%20ANNs.%20Specifically%2C%20we%20trained%20ResNet50-backbone%20models%0Aon%20a%20dual%20task%20of%20classification%20and%20EEG%20prediction%3B%20and%20evaluated%20their%20EEG%0Aprediction%20accuracy%20and%20robustness%20to%20adversarial%20attacks.%20We%20observed%0Asignificant%20correlation%20between%20the%20networks%27%20EEG%20prediction%20accuracy%2C%20often%0Ahighest%20around%20100%20ms%20post%20stimulus%20onset%2C%20and%20their%20gains%20in%20adversarial%0Arobustness.%20Although%20effect%20size%20was%20limited%2C%20effects%20were%20consistent%20across%0Adifferent%20random%20initializations%20and%20robust%20for%20architectural%20variants.%20We%0Afurther%20teased%20apart%20the%20data%20from%20individual%20EEG%20channels%20and%20observed%0Astrongest%20contribution%20from%20electrodes%20in%20the%20parieto-occipital%20regions.%20The%0Ademonstrated%20utility%20of%20human%20EEG%20for%20such%20tasks%20opens%20up%20avenues%20for%20future%0Aefforts%20that%20scale%20to%20larger%20datasets%20under%20diverse%20stimuli%20conditions%20with%20the%0Apromise%20of%20stronger%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03646v1&entry.124074799=Read"},
{"title": "Segment Beyond View: Handling Partially Missing Modality for\n  Audio-Visual Semantic Segmentation", "author": "Renjie Wu and Hu Wang and Feras Dayoub and Hsiang-Ting Chen", "abstract": "  Augmented Reality (AR) devices, emerging as prominent mobile interaction\nplatforms, face challenges in user safety, particularly concerning oncoming\nvehicles. While some solutions leverage onboard camera arrays, these cameras\noften have limited field-of-view (FoV) with front or downward perspectives.\nAddressing this, we propose a new out-of-view semantic segmentation task and\nSegment Beyond View (SBV), a novel audio-visual semantic segmentation method.\nSBV supplements the visual modality, which miss the information beyond FoV,\nwith the auditory information using a teacher-student distillation model\n(Omni2Ego). The model consists of a vision teacher utilising panoramic\ninformation, an auditory teacher with 8-channel audio, and an audio-visual\nstudent that takes views with limited FoV and binaural audio as input and\nproduce semantic segmentation for objects outside FoV. SBV outperforms existing\nmodels in comparative evaluations and shows a consistent performance across\nvarying FoV ranges and in monaural audio settings.\n", "link": "http://arxiv.org/abs/2312.08673v3", "date": "2024-09-05", "relevancy": 2.1009, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5893}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Beyond%20View%3A%20Handling%20Partially%20Missing%20Modality%20for%0A%20%20Audio-Visual%20Semantic%20Segmentation&body=Title%3A%20Segment%20Beyond%20View%3A%20Handling%20Partially%20Missing%20Modality%20for%0A%20%20Audio-Visual%20Semantic%20Segmentation%0AAuthor%3A%20Renjie%20Wu%20and%20Hu%20Wang%20and%20Feras%20Dayoub%20and%20Hsiang-Ting%20Chen%0AAbstract%3A%20%20%20Augmented%20Reality%20%28AR%29%20devices%2C%20emerging%20as%20prominent%20mobile%20interaction%0Aplatforms%2C%20face%20challenges%20in%20user%20safety%2C%20particularly%20concerning%20oncoming%0Avehicles.%20While%20some%20solutions%20leverage%20onboard%20camera%20arrays%2C%20these%20cameras%0Aoften%20have%20limited%20field-of-view%20%28FoV%29%20with%20front%20or%20downward%20perspectives.%0AAddressing%20this%2C%20we%20propose%20a%20new%20out-of-view%20semantic%20segmentation%20task%20and%0ASegment%20Beyond%20View%20%28SBV%29%2C%20a%20novel%20audio-visual%20semantic%20segmentation%20method.%0ASBV%20supplements%20the%20visual%20modality%2C%20which%20miss%20the%20information%20beyond%20FoV%2C%0Awith%20the%20auditory%20information%20using%20a%20teacher-student%20distillation%20model%0A%28Omni2Ego%29.%20The%20model%20consists%20of%20a%20vision%20teacher%20utilising%20panoramic%0Ainformation%2C%20an%20auditory%20teacher%20with%208-channel%20audio%2C%20and%20an%20audio-visual%0Astudent%20that%20takes%20views%20with%20limited%20FoV%20and%20binaural%20audio%20as%20input%20and%0Aproduce%20semantic%20segmentation%20for%20objects%20outside%20FoV.%20SBV%20outperforms%20existing%0Amodels%20in%20comparative%20evaluations%20and%20shows%20a%20consistent%20performance%20across%0Avarying%20FoV%20ranges%20and%20in%20monaural%20audio%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08673v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Beyond%2520View%253A%2520Handling%2520Partially%2520Missing%2520Modality%2520for%250A%2520%2520Audio-Visual%2520Semantic%2520Segmentation%26entry.906535625%3DRenjie%2520Wu%2520and%2520Hu%2520Wang%2520and%2520Feras%2520Dayoub%2520and%2520Hsiang-Ting%2520Chen%26entry.1292438233%3D%2520%2520Augmented%2520Reality%2520%2528AR%2529%2520devices%252C%2520emerging%2520as%2520prominent%2520mobile%2520interaction%250Aplatforms%252C%2520face%2520challenges%2520in%2520user%2520safety%252C%2520particularly%2520concerning%2520oncoming%250Avehicles.%2520While%2520some%2520solutions%2520leverage%2520onboard%2520camera%2520arrays%252C%2520these%2520cameras%250Aoften%2520have%2520limited%2520field-of-view%2520%2528FoV%2529%2520with%2520front%2520or%2520downward%2520perspectives.%250AAddressing%2520this%252C%2520we%2520propose%2520a%2520new%2520out-of-view%2520semantic%2520segmentation%2520task%2520and%250ASegment%2520Beyond%2520View%2520%2528SBV%2529%252C%2520a%2520novel%2520audio-visual%2520semantic%2520segmentation%2520method.%250ASBV%2520supplements%2520the%2520visual%2520modality%252C%2520which%2520miss%2520the%2520information%2520beyond%2520FoV%252C%250Awith%2520the%2520auditory%2520information%2520using%2520a%2520teacher-student%2520distillation%2520model%250A%2528Omni2Ego%2529.%2520The%2520model%2520consists%2520of%2520a%2520vision%2520teacher%2520utilising%2520panoramic%250Ainformation%252C%2520an%2520auditory%2520teacher%2520with%25208-channel%2520audio%252C%2520and%2520an%2520audio-visual%250Astudent%2520that%2520takes%2520views%2520with%2520limited%2520FoV%2520and%2520binaural%2520audio%2520as%2520input%2520and%250Aproduce%2520semantic%2520segmentation%2520for%2520objects%2520outside%2520FoV.%2520SBV%2520outperforms%2520existing%250Amodels%2520in%2520comparative%2520evaluations%2520and%2520shows%2520a%2520consistent%2520performance%2520across%250Avarying%2520FoV%2520ranges%2520and%2520in%2520monaural%2520audio%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08673v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Beyond%20View%3A%20Handling%20Partially%20Missing%20Modality%20for%0A%20%20Audio-Visual%20Semantic%20Segmentation&entry.906535625=Renjie%20Wu%20and%20Hu%20Wang%20and%20Feras%20Dayoub%20and%20Hsiang-Ting%20Chen&entry.1292438233=%20%20Augmented%20Reality%20%28AR%29%20devices%2C%20emerging%20as%20prominent%20mobile%20interaction%0Aplatforms%2C%20face%20challenges%20in%20user%20safety%2C%20particularly%20concerning%20oncoming%0Avehicles.%20While%20some%20solutions%20leverage%20onboard%20camera%20arrays%2C%20these%20cameras%0Aoften%20have%20limited%20field-of-view%20%28FoV%29%20with%20front%20or%20downward%20perspectives.%0AAddressing%20this%2C%20we%20propose%20a%20new%20out-of-view%20semantic%20segmentation%20task%20and%0ASegment%20Beyond%20View%20%28SBV%29%2C%20a%20novel%20audio-visual%20semantic%20segmentation%20method.%0ASBV%20supplements%20the%20visual%20modality%2C%20which%20miss%20the%20information%20beyond%20FoV%2C%0Awith%20the%20auditory%20information%20using%20a%20teacher-student%20distillation%20model%0A%28Omni2Ego%29.%20The%20model%20consists%20of%20a%20vision%20teacher%20utilising%20panoramic%0Ainformation%2C%20an%20auditory%20teacher%20with%208-channel%20audio%2C%20and%20an%20audio-visual%0Astudent%20that%20takes%20views%20with%20limited%20FoV%20and%20binaural%20audio%20as%20input%20and%0Aproduce%20semantic%20segmentation%20for%20objects%20outside%20FoV.%20SBV%20outperforms%20existing%0Amodels%20in%20comparative%20evaluations%20and%20shows%20a%20consistent%20performance%20across%0Avarying%20FoV%20ranges%20and%20in%20monaural%20audio%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08673v3&entry.124074799=Read"},
{"title": "Non-Uniform Illumination Attack for Fooling Convolutional Neural\n  Networks", "author": "Akshay Jain and Shiv Ram Dubey and Satish Kumar Singh and KC Santosh and Bidyut Baran Chaudhuri", "abstract": "  Convolutional Neural Networks (CNNs) have made remarkable strides; however,\nthey remain susceptible to vulnerabilities, particularly in the face of minor\nimage perturbations that humans can easily recognize. This weakness, often\ntermed as 'attacks', underscores the limited robustness of CNNs and the need\nfor research into fortifying their resistance against such manipulations. This\nstudy introduces a novel Non-Uniform Illumination (NUI) attack technique, where\nimages are subtly altered using varying NUI masks. Extensive experiments are\nconducted on widely-accepted datasets including CIFAR10, TinyImageNet, and\nCalTech256, focusing on image classification with 12 different NUI attack\nmodels. The resilience of VGG, ResNet, MobilenetV3-small and InceptionV3 models\nagainst NUI attacks are evaluated. Our results show a substantial decline in\nthe CNN models' classification accuracy when subjected to NUI attacks,\nindicating their vulnerability under non-uniform illumination. To mitigate\nthis, a defense strategy is proposed, including NUI-attacked images, generated\nthrough the new NUI transformation, into the training set. The results\ndemonstrate a significant enhancement in CNN model performance when confronted\nwith perturbed images affected by NUI attacks. This strategy seeks to bolster\nCNN models' resilience against NUI attacks.\n", "link": "http://arxiv.org/abs/2409.03458v1", "date": "2024-09-05", "relevancy": 2.0843, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5203}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Uniform%20Illumination%20Attack%20for%20Fooling%20Convolutional%20Neural%0A%20%20Networks&body=Title%3A%20Non-Uniform%20Illumination%20Attack%20for%20Fooling%20Convolutional%20Neural%0A%20%20Networks%0AAuthor%3A%20Akshay%20Jain%20and%20Shiv%20Ram%20Dubey%20and%20Satish%20Kumar%20Singh%20and%20KC%20Santosh%20and%20Bidyut%20Baran%20Chaudhuri%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20made%20remarkable%20strides%3B%20however%2C%0Athey%20remain%20susceptible%20to%20vulnerabilities%2C%20particularly%20in%20the%20face%20of%20minor%0Aimage%20perturbations%20that%20humans%20can%20easily%20recognize.%20This%20weakness%2C%20often%0Atermed%20as%20%27attacks%27%2C%20underscores%20the%20limited%20robustness%20of%20CNNs%20and%20the%20need%0Afor%20research%20into%20fortifying%20their%20resistance%20against%20such%20manipulations.%20This%0Astudy%20introduces%20a%20novel%20Non-Uniform%20Illumination%20%28NUI%29%20attack%20technique%2C%20where%0Aimages%20are%20subtly%20altered%20using%20varying%20NUI%20masks.%20Extensive%20experiments%20are%0Aconducted%20on%20widely-accepted%20datasets%20including%20CIFAR10%2C%20TinyImageNet%2C%20and%0ACalTech256%2C%20focusing%20on%20image%20classification%20with%2012%20different%20NUI%20attack%0Amodels.%20The%20resilience%20of%20VGG%2C%20ResNet%2C%20MobilenetV3-small%20and%20InceptionV3%20models%0Aagainst%20NUI%20attacks%20are%20evaluated.%20Our%20results%20show%20a%20substantial%20decline%20in%0Athe%20CNN%20models%27%20classification%20accuracy%20when%20subjected%20to%20NUI%20attacks%2C%0Aindicating%20their%20vulnerability%20under%20non-uniform%20illumination.%20To%20mitigate%0Athis%2C%20a%20defense%20strategy%20is%20proposed%2C%20including%20NUI-attacked%20images%2C%20generated%0Athrough%20the%20new%20NUI%20transformation%2C%20into%20the%20training%20set.%20The%20results%0Ademonstrate%20a%20significant%20enhancement%20in%20CNN%20model%20performance%20when%20confronted%0Awith%20perturbed%20images%20affected%20by%20NUI%20attacks.%20This%20strategy%20seeks%20to%20bolster%0ACNN%20models%27%20resilience%20against%20NUI%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Uniform%2520Illumination%2520Attack%2520for%2520Fooling%2520Convolutional%2520Neural%250A%2520%2520Networks%26entry.906535625%3DAkshay%2520Jain%2520and%2520Shiv%2520Ram%2520Dubey%2520and%2520Satish%2520Kumar%2520Singh%2520and%2520KC%2520Santosh%2520and%2520Bidyut%2520Baran%2520Chaudhuri%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520made%2520remarkable%2520strides%253B%2520however%252C%250Athey%2520remain%2520susceptible%2520to%2520vulnerabilities%252C%2520particularly%2520in%2520the%2520face%2520of%2520minor%250Aimage%2520perturbations%2520that%2520humans%2520can%2520easily%2520recognize.%2520This%2520weakness%252C%2520often%250Atermed%2520as%2520%2527attacks%2527%252C%2520underscores%2520the%2520limited%2520robustness%2520of%2520CNNs%2520and%2520the%2520need%250Afor%2520research%2520into%2520fortifying%2520their%2520resistance%2520against%2520such%2520manipulations.%2520This%250Astudy%2520introduces%2520a%2520novel%2520Non-Uniform%2520Illumination%2520%2528NUI%2529%2520attack%2520technique%252C%2520where%250Aimages%2520are%2520subtly%2520altered%2520using%2520varying%2520NUI%2520masks.%2520Extensive%2520experiments%2520are%250Aconducted%2520on%2520widely-accepted%2520datasets%2520including%2520CIFAR10%252C%2520TinyImageNet%252C%2520and%250ACalTech256%252C%2520focusing%2520on%2520image%2520classification%2520with%252012%2520different%2520NUI%2520attack%250Amodels.%2520The%2520resilience%2520of%2520VGG%252C%2520ResNet%252C%2520MobilenetV3-small%2520and%2520InceptionV3%2520models%250Aagainst%2520NUI%2520attacks%2520are%2520evaluated.%2520Our%2520results%2520show%2520a%2520substantial%2520decline%2520in%250Athe%2520CNN%2520models%2527%2520classification%2520accuracy%2520when%2520subjected%2520to%2520NUI%2520attacks%252C%250Aindicating%2520their%2520vulnerability%2520under%2520non-uniform%2520illumination.%2520To%2520mitigate%250Athis%252C%2520a%2520defense%2520strategy%2520is%2520proposed%252C%2520including%2520NUI-attacked%2520images%252C%2520generated%250Athrough%2520the%2520new%2520NUI%2520transformation%252C%2520into%2520the%2520training%2520set.%2520The%2520results%250Ademonstrate%2520a%2520significant%2520enhancement%2520in%2520CNN%2520model%2520performance%2520when%2520confronted%250Awith%2520perturbed%2520images%2520affected%2520by%2520NUI%2520attacks.%2520This%2520strategy%2520seeks%2520to%2520bolster%250ACNN%2520models%2527%2520resilience%2520against%2520NUI%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Uniform%20Illumination%20Attack%20for%20Fooling%20Convolutional%20Neural%0A%20%20Networks&entry.906535625=Akshay%20Jain%20and%20Shiv%20Ram%20Dubey%20and%20Satish%20Kumar%20Singh%20and%20KC%20Santosh%20and%20Bidyut%20Baran%20Chaudhuri&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20made%20remarkable%20strides%3B%20however%2C%0Athey%20remain%20susceptible%20to%20vulnerabilities%2C%20particularly%20in%20the%20face%20of%20minor%0Aimage%20perturbations%20that%20humans%20can%20easily%20recognize.%20This%20weakness%2C%20often%0Atermed%20as%20%27attacks%27%2C%20underscores%20the%20limited%20robustness%20of%20CNNs%20and%20the%20need%0Afor%20research%20into%20fortifying%20their%20resistance%20against%20such%20manipulations.%20This%0Astudy%20introduces%20a%20novel%20Non-Uniform%20Illumination%20%28NUI%29%20attack%20technique%2C%20where%0Aimages%20are%20subtly%20altered%20using%20varying%20NUI%20masks.%20Extensive%20experiments%20are%0Aconducted%20on%20widely-accepted%20datasets%20including%20CIFAR10%2C%20TinyImageNet%2C%20and%0ACalTech256%2C%20focusing%20on%20image%20classification%20with%2012%20different%20NUI%20attack%0Amodels.%20The%20resilience%20of%20VGG%2C%20ResNet%2C%20MobilenetV3-small%20and%20InceptionV3%20models%0Aagainst%20NUI%20attacks%20are%20evaluated.%20Our%20results%20show%20a%20substantial%20decline%20in%0Athe%20CNN%20models%27%20classification%20accuracy%20when%20subjected%20to%20NUI%20attacks%2C%0Aindicating%20their%20vulnerability%20under%20non-uniform%20illumination.%20To%20mitigate%0Athis%2C%20a%20defense%20strategy%20is%20proposed%2C%20including%20NUI-attacked%20images%2C%20generated%0Athrough%20the%20new%20NUI%20transformation%2C%20into%20the%20training%20set.%20The%20results%0Ademonstrate%20a%20significant%20enhancement%20in%20CNN%20model%20performance%20when%20confronted%0Awith%20perturbed%20images%20affected%20by%20NUI%20attacks.%20This%20strategy%20seeks%20to%20bolster%0ACNN%20models%27%20resilience%20against%20NUI%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03458v1&entry.124074799=Read"},
{"title": "Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole\n  Slide Image Classification", "author": "Renao Yan and Qiehe Sun and Cheng Jin and Yiqing Liu and Yonghong He and Tian Guan and Hao Chen", "abstract": "  In computational pathology, whole-slide image (WSI) classification presents a\nformidable challenge due to its gigapixel resolution and limited fine-grained\nannotations. Multiple-instance learning (MIL) offers a weakly supervised\nsolution, yet refining instance-level information from bag-level labels remains\nchallenging. While most of the conventional MIL methods use attention scores to\nestimate instance importance scores (IIS) which contribute to the prediction of\nthe slide labels, these often lead to skewed attention distributions and\ninaccuracies in identifying crucial instances. To address these issues, we\npropose a new approach inspired by cooperative game theory: employing Shapley\nvalues to assess each instance's contribution, thereby improving IIS\nestimation. The computation of the Shapley value is then accelerated using\nattention, meanwhile retaining the enhanced instance identification and\nprioritization. We further introduce a framework for the progressive assignment\nof pseudo bags based on estimated IIS, encouraging more balanced attention\ndistributions in MIL models. Our extensive experiments on CAMELYON-16, BRACS,\nTCGA-LUNG, and TCGA-BRCA datasets show our method's superiority over existing\nstate-of-the-art approaches, offering enhanced interpretability and class-wise\ninsights. Our source code is available at https://github.com/RenaoYan/PMIL.\n", "link": "http://arxiv.org/abs/2312.05490v4", "date": "2024-09-05", "relevancy": 2.07, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5206}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shapley%20Values-enabled%20Progressive%20Pseudo%20Bag%20Augmentation%20for%20Whole%0A%20%20Slide%20Image%20Classification&body=Title%3A%20Shapley%20Values-enabled%20Progressive%20Pseudo%20Bag%20Augmentation%20for%20Whole%0A%20%20Slide%20Image%20Classification%0AAuthor%3A%20Renao%20Yan%20and%20Qiehe%20Sun%20and%20Cheng%20Jin%20and%20Yiqing%20Liu%20and%20Yonghong%20He%20and%20Tian%20Guan%20and%20Hao%20Chen%0AAbstract%3A%20%20%20In%20computational%20pathology%2C%20whole-slide%20image%20%28WSI%29%20classification%20presents%20a%0Aformidable%20challenge%20due%20to%20its%20gigapixel%20resolution%20and%20limited%20fine-grained%0Aannotations.%20Multiple-instance%20learning%20%28MIL%29%20offers%20a%20weakly%20supervised%0Asolution%2C%20yet%20refining%20instance-level%20information%20from%20bag-level%20labels%20remains%0Achallenging.%20While%20most%20of%20the%20conventional%20MIL%20methods%20use%20attention%20scores%20to%0Aestimate%20instance%20importance%20scores%20%28IIS%29%20which%20contribute%20to%20the%20prediction%20of%0Athe%20slide%20labels%2C%20these%20often%20lead%20to%20skewed%20attention%20distributions%20and%0Ainaccuracies%20in%20identifying%20crucial%20instances.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20new%20approach%20inspired%20by%20cooperative%20game%20theory%3A%20employing%20Shapley%0Avalues%20to%20assess%20each%20instance%27s%20contribution%2C%20thereby%20improving%20IIS%0Aestimation.%20The%20computation%20of%20the%20Shapley%20value%20is%20then%20accelerated%20using%0Aattention%2C%20meanwhile%20retaining%20the%20enhanced%20instance%20identification%20and%0Aprioritization.%20We%20further%20introduce%20a%20framework%20for%20the%20progressive%20assignment%0Aof%20pseudo%20bags%20based%20on%20estimated%20IIS%2C%20encouraging%20more%20balanced%20attention%0Adistributions%20in%20MIL%20models.%20Our%20extensive%20experiments%20on%20CAMELYON-16%2C%20BRACS%2C%0ATCGA-LUNG%2C%20and%20TCGA-BRCA%20datasets%20show%20our%20method%27s%20superiority%20over%20existing%0Astate-of-the-art%20approaches%2C%20offering%20enhanced%20interpretability%20and%20class-wise%0Ainsights.%20Our%20source%20code%20is%20available%20at%20https%3A//github.com/RenaoYan/PMIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05490v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapley%2520Values-enabled%2520Progressive%2520Pseudo%2520Bag%2520Augmentation%2520for%2520Whole%250A%2520%2520Slide%2520Image%2520Classification%26entry.906535625%3DRenao%2520Yan%2520and%2520Qiehe%2520Sun%2520and%2520Cheng%2520Jin%2520and%2520Yiqing%2520Liu%2520and%2520Yonghong%2520He%2520and%2520Tian%2520Guan%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520In%2520computational%2520pathology%252C%2520whole-slide%2520image%2520%2528WSI%2529%2520classification%2520presents%2520a%250Aformidable%2520challenge%2520due%2520to%2520its%2520gigapixel%2520resolution%2520and%2520limited%2520fine-grained%250Aannotations.%2520Multiple-instance%2520learning%2520%2528MIL%2529%2520offers%2520a%2520weakly%2520supervised%250Asolution%252C%2520yet%2520refining%2520instance-level%2520information%2520from%2520bag-level%2520labels%2520remains%250Achallenging.%2520While%2520most%2520of%2520the%2520conventional%2520MIL%2520methods%2520use%2520attention%2520scores%2520to%250Aestimate%2520instance%2520importance%2520scores%2520%2528IIS%2529%2520which%2520contribute%2520to%2520the%2520prediction%2520of%250Athe%2520slide%2520labels%252C%2520these%2520often%2520lead%2520to%2520skewed%2520attention%2520distributions%2520and%250Ainaccuracies%2520in%2520identifying%2520crucial%2520instances.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520new%2520approach%2520inspired%2520by%2520cooperative%2520game%2520theory%253A%2520employing%2520Shapley%250Avalues%2520to%2520assess%2520each%2520instance%2527s%2520contribution%252C%2520thereby%2520improving%2520IIS%250Aestimation.%2520The%2520computation%2520of%2520the%2520Shapley%2520value%2520is%2520then%2520accelerated%2520using%250Aattention%252C%2520meanwhile%2520retaining%2520the%2520enhanced%2520instance%2520identification%2520and%250Aprioritization.%2520We%2520further%2520introduce%2520a%2520framework%2520for%2520the%2520progressive%2520assignment%250Aof%2520pseudo%2520bags%2520based%2520on%2520estimated%2520IIS%252C%2520encouraging%2520more%2520balanced%2520attention%250Adistributions%2520in%2520MIL%2520models.%2520Our%2520extensive%2520experiments%2520on%2520CAMELYON-16%252C%2520BRACS%252C%250ATCGA-LUNG%252C%2520and%2520TCGA-BRCA%2520datasets%2520show%2520our%2520method%2527s%2520superiority%2520over%2520existing%250Astate-of-the-art%2520approaches%252C%2520offering%2520enhanced%2520interpretability%2520and%2520class-wise%250Ainsights.%2520Our%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/RenaoYan/PMIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05490v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shapley%20Values-enabled%20Progressive%20Pseudo%20Bag%20Augmentation%20for%20Whole%0A%20%20Slide%20Image%20Classification&entry.906535625=Renao%20Yan%20and%20Qiehe%20Sun%20and%20Cheng%20Jin%20and%20Yiqing%20Liu%20and%20Yonghong%20He%20and%20Tian%20Guan%20and%20Hao%20Chen&entry.1292438233=%20%20In%20computational%20pathology%2C%20whole-slide%20image%20%28WSI%29%20classification%20presents%20a%0Aformidable%20challenge%20due%20to%20its%20gigapixel%20resolution%20and%20limited%20fine-grained%0Aannotations.%20Multiple-instance%20learning%20%28MIL%29%20offers%20a%20weakly%20supervised%0Asolution%2C%20yet%20refining%20instance-level%20information%20from%20bag-level%20labels%20remains%0Achallenging.%20While%20most%20of%20the%20conventional%20MIL%20methods%20use%20attention%20scores%20to%0Aestimate%20instance%20importance%20scores%20%28IIS%29%20which%20contribute%20to%20the%20prediction%20of%0Athe%20slide%20labels%2C%20these%20often%20lead%20to%20skewed%20attention%20distributions%20and%0Ainaccuracies%20in%20identifying%20crucial%20instances.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20new%20approach%20inspired%20by%20cooperative%20game%20theory%3A%20employing%20Shapley%0Avalues%20to%20assess%20each%20instance%27s%20contribution%2C%20thereby%20improving%20IIS%0Aestimation.%20The%20computation%20of%20the%20Shapley%20value%20is%20then%20accelerated%20using%0Aattention%2C%20meanwhile%20retaining%20the%20enhanced%20instance%20identification%20and%0Aprioritization.%20We%20further%20introduce%20a%20framework%20for%20the%20progressive%20assignment%0Aof%20pseudo%20bags%20based%20on%20estimated%20IIS%2C%20encouraging%20more%20balanced%20attention%0Adistributions%20in%20MIL%20models.%20Our%20extensive%20experiments%20on%20CAMELYON-16%2C%20BRACS%2C%0ATCGA-LUNG%2C%20and%20TCGA-BRCA%20datasets%20show%20our%20method%27s%20superiority%20over%20existing%0Astate-of-the-art%20approaches%2C%20offering%20enhanced%20interpretability%20and%20class-wise%0Ainsights.%20Our%20source%20code%20is%20available%20at%20https%3A//github.com/RenaoYan/PMIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05490v4&entry.124074799=Read"},
{"title": "Fast Decentralized State Estimation for Legged Robot Locomotion via EKF\n  and MHE", "author": "Jiarong Kang and Yi Wang and Xiaobin Xiong", "abstract": "  In this paper, we present a fast and decentralized state estimation framework\nfor the control of legged locomotion. The nonlinear estimation of the floating\nbase states is decentralized to an orientation estimation via Extended Kalman\nFilter (EKF) and a linear velocity estimation via Moving Horizon Estimation\n(MHE). The EKF fuses the inertia sensor with vision to estimate the floating\nbase orientation. The MHE uses the estimated orientation with all the sensors\nwithin a time window in the past to estimate the linear velocities based on a\ntime-varying linear dynamics formulation of the interested states with state\nconstraints. More importantly, a marginalization method based on the\noptimization structure of the full information filter (FIF) is proposed to\nconvert the equality-constrained FIF to an equivalent MHE. This decoupling of\nstate estimation promotes the desired balance of computation efficiency,\naccuracy of estimation, and the inclusion of state constraints. The proposed\nmethod is shown to be capable of providing accurate state estimation to several\nlegged robots, including the highly dynamic hopping robot PogoX, the bipedal\nrobot Cassie, and the quadrupedal robot Unitree Go1, with a frequency at 200 Hz\nand a window interval of 0.1s.\n", "link": "http://arxiv.org/abs/2405.20567v2", "date": "2024-09-05", "relevancy": 2.0652, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5864}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5384}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Decentralized%20State%20Estimation%20for%20Legged%20Robot%20Locomotion%20via%20EKF%0A%20%20and%20MHE&body=Title%3A%20Fast%20Decentralized%20State%20Estimation%20for%20Legged%20Robot%20Locomotion%20via%20EKF%0A%20%20and%20MHE%0AAuthor%3A%20Jiarong%20Kang%20and%20Yi%20Wang%20and%20Xiaobin%20Xiong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20fast%20and%20decentralized%20state%20estimation%20framework%0Afor%20the%20control%20of%20legged%20locomotion.%20The%20nonlinear%20estimation%20of%20the%20floating%0Abase%20states%20is%20decentralized%20to%20an%20orientation%20estimation%20via%20Extended%20Kalman%0AFilter%20%28EKF%29%20and%20a%20linear%20velocity%20estimation%20via%20Moving%20Horizon%20Estimation%0A%28MHE%29.%20The%20EKF%20fuses%20the%20inertia%20sensor%20with%20vision%20to%20estimate%20the%20floating%0Abase%20orientation.%20The%20MHE%20uses%20the%20estimated%20orientation%20with%20all%20the%20sensors%0Awithin%20a%20time%20window%20in%20the%20past%20to%20estimate%20the%20linear%20velocities%20based%20on%20a%0Atime-varying%20linear%20dynamics%20formulation%20of%20the%20interested%20states%20with%20state%0Aconstraints.%20More%20importantly%2C%20a%20marginalization%20method%20based%20on%20the%0Aoptimization%20structure%20of%20the%20full%20information%20filter%20%28FIF%29%20is%20proposed%20to%0Aconvert%20the%20equality-constrained%20FIF%20to%20an%20equivalent%20MHE.%20This%20decoupling%20of%0Astate%20estimation%20promotes%20the%20desired%20balance%20of%20computation%20efficiency%2C%0Aaccuracy%20of%20estimation%2C%20and%20the%20inclusion%20of%20state%20constraints.%20The%20proposed%0Amethod%20is%20shown%20to%20be%20capable%20of%20providing%20accurate%20state%20estimation%20to%20several%0Alegged%20robots%2C%20including%20the%20highly%20dynamic%20hopping%20robot%20PogoX%2C%20the%20bipedal%0Arobot%20Cassie%2C%20and%20the%20quadrupedal%20robot%20Unitree%20Go1%2C%20with%20a%20frequency%20at%20200%20Hz%0Aand%20a%20window%20interval%20of%200.1s.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Decentralized%2520State%2520Estimation%2520for%2520Legged%2520Robot%2520Locomotion%2520via%2520EKF%250A%2520%2520and%2520MHE%26entry.906535625%3DJiarong%2520Kang%2520and%2520Yi%2520Wang%2520and%2520Xiaobin%2520Xiong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520fast%2520and%2520decentralized%2520state%2520estimation%2520framework%250Afor%2520the%2520control%2520of%2520legged%2520locomotion.%2520The%2520nonlinear%2520estimation%2520of%2520the%2520floating%250Abase%2520states%2520is%2520decentralized%2520to%2520an%2520orientation%2520estimation%2520via%2520Extended%2520Kalman%250AFilter%2520%2528EKF%2529%2520and%2520a%2520linear%2520velocity%2520estimation%2520via%2520Moving%2520Horizon%2520Estimation%250A%2528MHE%2529.%2520The%2520EKF%2520fuses%2520the%2520inertia%2520sensor%2520with%2520vision%2520to%2520estimate%2520the%2520floating%250Abase%2520orientation.%2520The%2520MHE%2520uses%2520the%2520estimated%2520orientation%2520with%2520all%2520the%2520sensors%250Awithin%2520a%2520time%2520window%2520in%2520the%2520past%2520to%2520estimate%2520the%2520linear%2520velocities%2520based%2520on%2520a%250Atime-varying%2520linear%2520dynamics%2520formulation%2520of%2520the%2520interested%2520states%2520with%2520state%250Aconstraints.%2520More%2520importantly%252C%2520a%2520marginalization%2520method%2520based%2520on%2520the%250Aoptimization%2520structure%2520of%2520the%2520full%2520information%2520filter%2520%2528FIF%2529%2520is%2520proposed%2520to%250Aconvert%2520the%2520equality-constrained%2520FIF%2520to%2520an%2520equivalent%2520MHE.%2520This%2520decoupling%2520of%250Astate%2520estimation%2520promotes%2520the%2520desired%2520balance%2520of%2520computation%2520efficiency%252C%250Aaccuracy%2520of%2520estimation%252C%2520and%2520the%2520inclusion%2520of%2520state%2520constraints.%2520The%2520proposed%250Amethod%2520is%2520shown%2520to%2520be%2520capable%2520of%2520providing%2520accurate%2520state%2520estimation%2520to%2520several%250Alegged%2520robots%252C%2520including%2520the%2520highly%2520dynamic%2520hopping%2520robot%2520PogoX%252C%2520the%2520bipedal%250Arobot%2520Cassie%252C%2520and%2520the%2520quadrupedal%2520robot%2520Unitree%2520Go1%252C%2520with%2520a%2520frequency%2520at%2520200%2520Hz%250Aand%2520a%2520window%2520interval%2520of%25200.1s.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Decentralized%20State%20Estimation%20for%20Legged%20Robot%20Locomotion%20via%20EKF%0A%20%20and%20MHE&entry.906535625=Jiarong%20Kang%20and%20Yi%20Wang%20and%20Xiaobin%20Xiong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20fast%20and%20decentralized%20state%20estimation%20framework%0Afor%20the%20control%20of%20legged%20locomotion.%20The%20nonlinear%20estimation%20of%20the%20floating%0Abase%20states%20is%20decentralized%20to%20an%20orientation%20estimation%20via%20Extended%20Kalman%0AFilter%20%28EKF%29%20and%20a%20linear%20velocity%20estimation%20via%20Moving%20Horizon%20Estimation%0A%28MHE%29.%20The%20EKF%20fuses%20the%20inertia%20sensor%20with%20vision%20to%20estimate%20the%20floating%0Abase%20orientation.%20The%20MHE%20uses%20the%20estimated%20orientation%20with%20all%20the%20sensors%0Awithin%20a%20time%20window%20in%20the%20past%20to%20estimate%20the%20linear%20velocities%20based%20on%20a%0Atime-varying%20linear%20dynamics%20formulation%20of%20the%20interested%20states%20with%20state%0Aconstraints.%20More%20importantly%2C%20a%20marginalization%20method%20based%20on%20the%0Aoptimization%20structure%20of%20the%20full%20information%20filter%20%28FIF%29%20is%20proposed%20to%0Aconvert%20the%20equality-constrained%20FIF%20to%20an%20equivalent%20MHE.%20This%20decoupling%20of%0Astate%20estimation%20promotes%20the%20desired%20balance%20of%20computation%20efficiency%2C%0Aaccuracy%20of%20estimation%2C%20and%20the%20inclusion%20of%20state%20constraints.%20The%20proposed%0Amethod%20is%20shown%20to%20be%20capable%20of%20providing%20accurate%20state%20estimation%20to%20several%0Alegged%20robots%2C%20including%20the%20highly%20dynamic%20hopping%20robot%20PogoX%2C%20the%20bipedal%0Arobot%20Cassie%2C%20and%20the%20quadrupedal%20robot%20Unitree%20Go1%2C%20with%20a%20frequency%20at%20200%20Hz%0Aand%20a%20window%20interval%20of%200.1s.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20567v2&entry.124074799=Read"},
{"title": "Tissue Concepts: supervised foundation models in computational pathology", "author": "Till Nicke and Jan Raphael Schaefer and Henning Hoefener and Friedrich Feuerhake and Dorit Merhof and Fabian Kiessling and Johannes Lotz", "abstract": "  Due to the increasing workload of pathologists, the need for automation to\nsupport diagnostic tasks and quantitative biomarker evaluation is becoming more\nand more apparent. Foundation models have the potential to improve\ngeneralizability within and across centers and serve as starting points for\ndata efficient development of specialized yet robust AI models. However, the\ntraining foundation models themselves is usually very expensive in terms of\ndata, computation, and time. This paper proposes a supervised training method\nthat drastically reduces these expenses. The proposed method is based on\nmulti-task learning to train a joint encoder, by combining 16 different\nclassification, segmentation, and detection tasks on a total of 912,000\npatches. Since the encoder is capable of capturing the properties of the\nsamples, we term it the Tissue Concepts encoder. To evaluate the performance\nand generalizability of the Tissue Concepts encoder across centers,\nclassification of whole slide images from four of the most prevalent solid\ncancers - breast, colon, lung, and prostate - was used. The experiments show\nthat the Tissue Concepts model achieve comparable performance to models trained\nwith self-supervision, while requiring only 6% of the amount of training\npatches. Furthermore, the Tissue Concepts encoder outperforms an ImageNet\npre-trained encoder on both in-domain and out-of-domain data.\n", "link": "http://arxiv.org/abs/2409.03519v1", "date": "2024-09-05", "relevancy": 2.0571, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tissue%20Concepts%3A%20supervised%20foundation%20models%20in%20computational%20pathology&body=Title%3A%20Tissue%20Concepts%3A%20supervised%20foundation%20models%20in%20computational%20pathology%0AAuthor%3A%20Till%20Nicke%20and%20Jan%20Raphael%20Schaefer%20and%20Henning%20Hoefener%20and%20Friedrich%20Feuerhake%20and%20Dorit%20Merhof%20and%20Fabian%20Kiessling%20and%20Johannes%20Lotz%0AAbstract%3A%20%20%20Due%20to%20the%20increasing%20workload%20of%20pathologists%2C%20the%20need%20for%20automation%20to%0Asupport%20diagnostic%20tasks%20and%20quantitative%20biomarker%20evaluation%20is%20becoming%20more%0Aand%20more%20apparent.%20Foundation%20models%20have%20the%20potential%20to%20improve%0Ageneralizability%20within%20and%20across%20centers%20and%20serve%20as%20starting%20points%20for%0Adata%20efficient%20development%20of%20specialized%20yet%20robust%20AI%20models.%20However%2C%20the%0Atraining%20foundation%20models%20themselves%20is%20usually%20very%20expensive%20in%20terms%20of%0Adata%2C%20computation%2C%20and%20time.%20This%20paper%20proposes%20a%20supervised%20training%20method%0Athat%20drastically%20reduces%20these%20expenses.%20The%20proposed%20method%20is%20based%20on%0Amulti-task%20learning%20to%20train%20a%20joint%20encoder%2C%20by%20combining%2016%20different%0Aclassification%2C%20segmentation%2C%20and%20detection%20tasks%20on%20a%20total%20of%20912%2C000%0Apatches.%20Since%20the%20encoder%20is%20capable%20of%20capturing%20the%20properties%20of%20the%0Asamples%2C%20we%20term%20it%20the%20Tissue%20Concepts%20encoder.%20To%20evaluate%20the%20performance%0Aand%20generalizability%20of%20the%20Tissue%20Concepts%20encoder%20across%20centers%2C%0Aclassification%20of%20whole%20slide%20images%20from%20four%20of%20the%20most%20prevalent%20solid%0Acancers%20-%20breast%2C%20colon%2C%20lung%2C%20and%20prostate%20-%20was%20used.%20The%20experiments%20show%0Athat%20the%20Tissue%20Concepts%20model%20achieve%20comparable%20performance%20to%20models%20trained%0Awith%20self-supervision%2C%20while%20requiring%20only%206%25%20of%20the%20amount%20of%20training%0Apatches.%20Furthermore%2C%20the%20Tissue%20Concepts%20encoder%20outperforms%20an%20ImageNet%0Apre-trained%20encoder%20on%20both%20in-domain%20and%20out-of-domain%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTissue%2520Concepts%253A%2520supervised%2520foundation%2520models%2520in%2520computational%2520pathology%26entry.906535625%3DTill%2520Nicke%2520and%2520Jan%2520Raphael%2520Schaefer%2520and%2520Henning%2520Hoefener%2520and%2520Friedrich%2520Feuerhake%2520and%2520Dorit%2520Merhof%2520and%2520Fabian%2520Kiessling%2520and%2520Johannes%2520Lotz%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520increasing%2520workload%2520of%2520pathologists%252C%2520the%2520need%2520for%2520automation%2520to%250Asupport%2520diagnostic%2520tasks%2520and%2520quantitative%2520biomarker%2520evaluation%2520is%2520becoming%2520more%250Aand%2520more%2520apparent.%2520Foundation%2520models%2520have%2520the%2520potential%2520to%2520improve%250Ageneralizability%2520within%2520and%2520across%2520centers%2520and%2520serve%2520as%2520starting%2520points%2520for%250Adata%2520efficient%2520development%2520of%2520specialized%2520yet%2520robust%2520AI%2520models.%2520However%252C%2520the%250Atraining%2520foundation%2520models%2520themselves%2520is%2520usually%2520very%2520expensive%2520in%2520terms%2520of%250Adata%252C%2520computation%252C%2520and%2520time.%2520This%2520paper%2520proposes%2520a%2520supervised%2520training%2520method%250Athat%2520drastically%2520reduces%2520these%2520expenses.%2520The%2520proposed%2520method%2520is%2520based%2520on%250Amulti-task%2520learning%2520to%2520train%2520a%2520joint%2520encoder%252C%2520by%2520combining%252016%2520different%250Aclassification%252C%2520segmentation%252C%2520and%2520detection%2520tasks%2520on%2520a%2520total%2520of%2520912%252C000%250Apatches.%2520Since%2520the%2520encoder%2520is%2520capable%2520of%2520capturing%2520the%2520properties%2520of%2520the%250Asamples%252C%2520we%2520term%2520it%2520the%2520Tissue%2520Concepts%2520encoder.%2520To%2520evaluate%2520the%2520performance%250Aand%2520generalizability%2520of%2520the%2520Tissue%2520Concepts%2520encoder%2520across%2520centers%252C%250Aclassification%2520of%2520whole%2520slide%2520images%2520from%2520four%2520of%2520the%2520most%2520prevalent%2520solid%250Acancers%2520-%2520breast%252C%2520colon%252C%2520lung%252C%2520and%2520prostate%2520-%2520was%2520used.%2520The%2520experiments%2520show%250Athat%2520the%2520Tissue%2520Concepts%2520model%2520achieve%2520comparable%2520performance%2520to%2520models%2520trained%250Awith%2520self-supervision%252C%2520while%2520requiring%2520only%25206%2525%2520of%2520the%2520amount%2520of%2520training%250Apatches.%2520Furthermore%252C%2520the%2520Tissue%2520Concepts%2520encoder%2520outperforms%2520an%2520ImageNet%250Apre-trained%2520encoder%2520on%2520both%2520in-domain%2520and%2520out-of-domain%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tissue%20Concepts%3A%20supervised%20foundation%20models%20in%20computational%20pathology&entry.906535625=Till%20Nicke%20and%20Jan%20Raphael%20Schaefer%20and%20Henning%20Hoefener%20and%20Friedrich%20Feuerhake%20and%20Dorit%20Merhof%20and%20Fabian%20Kiessling%20and%20Johannes%20Lotz&entry.1292438233=%20%20Due%20to%20the%20increasing%20workload%20of%20pathologists%2C%20the%20need%20for%20automation%20to%0Asupport%20diagnostic%20tasks%20and%20quantitative%20biomarker%20evaluation%20is%20becoming%20more%0Aand%20more%20apparent.%20Foundation%20models%20have%20the%20potential%20to%20improve%0Ageneralizability%20within%20and%20across%20centers%20and%20serve%20as%20starting%20points%20for%0Adata%20efficient%20development%20of%20specialized%20yet%20robust%20AI%20models.%20However%2C%20the%0Atraining%20foundation%20models%20themselves%20is%20usually%20very%20expensive%20in%20terms%20of%0Adata%2C%20computation%2C%20and%20time.%20This%20paper%20proposes%20a%20supervised%20training%20method%0Athat%20drastically%20reduces%20these%20expenses.%20The%20proposed%20method%20is%20based%20on%0Amulti-task%20learning%20to%20train%20a%20joint%20encoder%2C%20by%20combining%2016%20different%0Aclassification%2C%20segmentation%2C%20and%20detection%20tasks%20on%20a%20total%20of%20912%2C000%0Apatches.%20Since%20the%20encoder%20is%20capable%20of%20capturing%20the%20properties%20of%20the%0Asamples%2C%20we%20term%20it%20the%20Tissue%20Concepts%20encoder.%20To%20evaluate%20the%20performance%0Aand%20generalizability%20of%20the%20Tissue%20Concepts%20encoder%20across%20centers%2C%0Aclassification%20of%20whole%20slide%20images%20from%20four%20of%20the%20most%20prevalent%20solid%0Acancers%20-%20breast%2C%20colon%2C%20lung%2C%20and%20prostate%20-%20was%20used.%20The%20experiments%20show%0Athat%20the%20Tissue%20Concepts%20model%20achieve%20comparable%20performance%20to%20models%20trained%0Awith%20self-supervision%2C%20while%20requiring%20only%206%25%20of%20the%20amount%20of%20training%0Apatches.%20Furthermore%2C%20the%20Tissue%20Concepts%20encoder%20outperforms%20an%20ImageNet%0Apre-trained%20encoder%20on%20both%20in-domain%20and%20out-of-domain%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03519v1&entry.124074799=Read"},
{"title": "TSFool: Crafting Highly-Imperceptible Adversarial Time Series through\n  Multi-Objective Attack", "author": "Yanyun Wang and Dehui Du and Haibo Hu and Zi Liang and Yuanhao Liu", "abstract": "  Recent years have witnessed the success of recurrent neural network (RNN)\nmodels in time series classification (TSC). However, neural networks (NNs) are\nvulnerable to adversarial samples, which cause real-life adversarial attacks\nthat undermine the robustness of AI models. To date, most existing attacks\ntarget at feed-forward NNs and image recognition tasks, but they cannot perform\nwell on RNN-based TSC. This is due to the cyclical computation of RNN, which\nprevents direct model differentiation. In addition, the high visual sensitivity\nof time series to perturbations also poses challenges to local objective\noptimization of adversarial samples. In this paper, we propose an efficient\nmethod called TSFool to craft highly-imperceptible adversarial time series for\nRNN-based TSC. The core idea is a new global optimization objective known as\n\"Camouflage Coefficient\" that captures the imperceptibility of adversarial\nsamples from the class distribution. Based on this, we reduce the adversarial\nattack problem to a multi-objective optimization problem that enhances the\nperturbation quality. Furthermore, to speed up the optimization process, we\npropose to use a representation model for RNN to capture deeply embedded\nvulnerable samples whose features deviate from the latent manifold. Experiments\non 11 UCR and UEA datasets showcase that TSFool significantly outperforms six\nwhite-box and three black-box benchmark attacks in terms of effectiveness,\nefficiency and imperceptibility from various perspectives including standard\nmeasure, human study and real-world defense.\n", "link": "http://arxiv.org/abs/2209.06388v4", "date": "2024-09-05", "relevancy": 2.0407, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5324}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4948}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSFool%3A%20Crafting%20Highly-Imperceptible%20Adversarial%20Time%20Series%20through%0A%20%20Multi-Objective%20Attack&body=Title%3A%20TSFool%3A%20Crafting%20Highly-Imperceptible%20Adversarial%20Time%20Series%20through%0A%20%20Multi-Objective%20Attack%0AAuthor%3A%20Yanyun%20Wang%20and%20Dehui%20Du%20and%20Haibo%20Hu%20and%20Zi%20Liang%20and%20Yuanhao%20Liu%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20the%20success%20of%20recurrent%20neural%20network%20%28RNN%29%0Amodels%20in%20time%20series%20classification%20%28TSC%29.%20However%2C%20neural%20networks%20%28NNs%29%20are%0Avulnerable%20to%20adversarial%20samples%2C%20which%20cause%20real-life%20adversarial%20attacks%0Athat%20undermine%20the%20robustness%20of%20AI%20models.%20To%20date%2C%20most%20existing%20attacks%0Atarget%20at%20feed-forward%20NNs%20and%20image%20recognition%20tasks%2C%20but%20they%20cannot%20perform%0Awell%20on%20RNN-based%20TSC.%20This%20is%20due%20to%20the%20cyclical%20computation%20of%20RNN%2C%20which%0Aprevents%20direct%20model%20differentiation.%20In%20addition%2C%20the%20high%20visual%20sensitivity%0Aof%20time%20series%20to%20perturbations%20also%20poses%20challenges%20to%20local%20objective%0Aoptimization%20of%20adversarial%20samples.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%0Amethod%20called%20TSFool%20to%20craft%20highly-imperceptible%20adversarial%20time%20series%20for%0ARNN-based%20TSC.%20The%20core%20idea%20is%20a%20new%20global%20optimization%20objective%20known%20as%0A%22Camouflage%20Coefficient%22%20that%20captures%20the%20imperceptibility%20of%20adversarial%0Asamples%20from%20the%20class%20distribution.%20Based%20on%20this%2C%20we%20reduce%20the%20adversarial%0Aattack%20problem%20to%20a%20multi-objective%20optimization%20problem%20that%20enhances%20the%0Aperturbation%20quality.%20Furthermore%2C%20to%20speed%20up%20the%20optimization%20process%2C%20we%0Apropose%20to%20use%20a%20representation%20model%20for%20RNN%20to%20capture%20deeply%20embedded%0Avulnerable%20samples%20whose%20features%20deviate%20from%20the%20latent%20manifold.%20Experiments%0Aon%2011%20UCR%20and%20UEA%20datasets%20showcase%20that%20TSFool%20significantly%20outperforms%20six%0Awhite-box%20and%20three%20black-box%20benchmark%20attacks%20in%20terms%20of%20effectiveness%2C%0Aefficiency%20and%20imperceptibility%20from%20various%20perspectives%20including%20standard%0Ameasure%2C%20human%20study%20and%20real-world%20defense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.06388v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSFool%253A%2520Crafting%2520Highly-Imperceptible%2520Adversarial%2520Time%2520Series%2520through%250A%2520%2520Multi-Objective%2520Attack%26entry.906535625%3DYanyun%2520Wang%2520and%2520Dehui%2520Du%2520and%2520Haibo%2520Hu%2520and%2520Zi%2520Liang%2520and%2520Yuanhao%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520the%2520success%2520of%2520recurrent%2520neural%2520network%2520%2528RNN%2529%250Amodels%2520in%2520time%2520series%2520classification%2520%2528TSC%2529.%2520However%252C%2520neural%2520networks%2520%2528NNs%2529%2520are%250Avulnerable%2520to%2520adversarial%2520samples%252C%2520which%2520cause%2520real-life%2520adversarial%2520attacks%250Athat%2520undermine%2520the%2520robustness%2520of%2520AI%2520models.%2520To%2520date%252C%2520most%2520existing%2520attacks%250Atarget%2520at%2520feed-forward%2520NNs%2520and%2520image%2520recognition%2520tasks%252C%2520but%2520they%2520cannot%2520perform%250Awell%2520on%2520RNN-based%2520TSC.%2520This%2520is%2520due%2520to%2520the%2520cyclical%2520computation%2520of%2520RNN%252C%2520which%250Aprevents%2520direct%2520model%2520differentiation.%2520In%2520addition%252C%2520the%2520high%2520visual%2520sensitivity%250Aof%2520time%2520series%2520to%2520perturbations%2520also%2520poses%2520challenges%2520to%2520local%2520objective%250Aoptimization%2520of%2520adversarial%2520samples.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%250Amethod%2520called%2520TSFool%2520to%2520craft%2520highly-imperceptible%2520adversarial%2520time%2520series%2520for%250ARNN-based%2520TSC.%2520The%2520core%2520idea%2520is%2520a%2520new%2520global%2520optimization%2520objective%2520known%2520as%250A%2522Camouflage%2520Coefficient%2522%2520that%2520captures%2520the%2520imperceptibility%2520of%2520adversarial%250Asamples%2520from%2520the%2520class%2520distribution.%2520Based%2520on%2520this%252C%2520we%2520reduce%2520the%2520adversarial%250Aattack%2520problem%2520to%2520a%2520multi-objective%2520optimization%2520problem%2520that%2520enhances%2520the%250Aperturbation%2520quality.%2520Furthermore%252C%2520to%2520speed%2520up%2520the%2520optimization%2520process%252C%2520we%250Apropose%2520to%2520use%2520a%2520representation%2520model%2520for%2520RNN%2520to%2520capture%2520deeply%2520embedded%250Avulnerable%2520samples%2520whose%2520features%2520deviate%2520from%2520the%2520latent%2520manifold.%2520Experiments%250Aon%252011%2520UCR%2520and%2520UEA%2520datasets%2520showcase%2520that%2520TSFool%2520significantly%2520outperforms%2520six%250Awhite-box%2520and%2520three%2520black-box%2520benchmark%2520attacks%2520in%2520terms%2520of%2520effectiveness%252C%250Aefficiency%2520and%2520imperceptibility%2520from%2520various%2520perspectives%2520including%2520standard%250Ameasure%252C%2520human%2520study%2520and%2520real-world%2520defense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.06388v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSFool%3A%20Crafting%20Highly-Imperceptible%20Adversarial%20Time%20Series%20through%0A%20%20Multi-Objective%20Attack&entry.906535625=Yanyun%20Wang%20and%20Dehui%20Du%20and%20Haibo%20Hu%20and%20Zi%20Liang%20and%20Yuanhao%20Liu&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20the%20success%20of%20recurrent%20neural%20network%20%28RNN%29%0Amodels%20in%20time%20series%20classification%20%28TSC%29.%20However%2C%20neural%20networks%20%28NNs%29%20are%0Avulnerable%20to%20adversarial%20samples%2C%20which%20cause%20real-life%20adversarial%20attacks%0Athat%20undermine%20the%20robustness%20of%20AI%20models.%20To%20date%2C%20most%20existing%20attacks%0Atarget%20at%20feed-forward%20NNs%20and%20image%20recognition%20tasks%2C%20but%20they%20cannot%20perform%0Awell%20on%20RNN-based%20TSC.%20This%20is%20due%20to%20the%20cyclical%20computation%20of%20RNN%2C%20which%0Aprevents%20direct%20model%20differentiation.%20In%20addition%2C%20the%20high%20visual%20sensitivity%0Aof%20time%20series%20to%20perturbations%20also%20poses%20challenges%20to%20local%20objective%0Aoptimization%20of%20adversarial%20samples.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%0Amethod%20called%20TSFool%20to%20craft%20highly-imperceptible%20adversarial%20time%20series%20for%0ARNN-based%20TSC.%20The%20core%20idea%20is%20a%20new%20global%20optimization%20objective%20known%20as%0A%22Camouflage%20Coefficient%22%20that%20captures%20the%20imperceptibility%20of%20adversarial%0Asamples%20from%20the%20class%20distribution.%20Based%20on%20this%2C%20we%20reduce%20the%20adversarial%0Aattack%20problem%20to%20a%20multi-objective%20optimization%20problem%20that%20enhances%20the%0Aperturbation%20quality.%20Furthermore%2C%20to%20speed%20up%20the%20optimization%20process%2C%20we%0Apropose%20to%20use%20a%20representation%20model%20for%20RNN%20to%20capture%20deeply%20embedded%0Avulnerable%20samples%20whose%20features%20deviate%20from%20the%20latent%20manifold.%20Experiments%0Aon%2011%20UCR%20and%20UEA%20datasets%20showcase%20that%20TSFool%20significantly%20outperforms%20six%0Awhite-box%20and%20three%20black-box%20benchmark%20attacks%20in%20terms%20of%20effectiveness%2C%0Aefficiency%20and%20imperceptibility%20from%20various%20perspectives%20including%20standard%0Ameasure%2C%20human%20study%20and%20real-world%20defense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.06388v4&entry.124074799=Read"},
{"title": "Have Large Vision-Language Models Mastered Art History?", "author": "Ombretta Strafforello and Derya Soydaner and Michiel Willems and Anne-Sofie Maerten and Stefanie De Winter", "abstract": "  The emergence of large Vision-Language Models (VLMs) has recently established\nnew baselines in image classification across multiple domains. However, the\nperformance of VLMs in the specific task of artwork classification,\nparticularly art style classification of paintings - a domain traditionally\nmastered by art historians - has not been explored yet. Artworks pose a unique\nchallenge compared to natural images due to their inherently complex and\ndiverse structures, characterized by variable compositions and styles. Art\nhistorians have long studied the unique aspects of artworks, with style\nprediction being a crucial component of their discipline. This paper\ninvestigates whether large VLMs, which integrate visual and textual data, can\neffectively predict the art historical attributes of paintings. We conduct an\nin-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o,\nfocusing on zero-shot classification of art style, author and time period using\ntwo public benchmarks of artworks. Additionally, we present ArTest, a\nwell-curated test set of artworks, including pivotal paintings studied by art\nhistorians.\n", "link": "http://arxiv.org/abs/2409.03521v1", "date": "2024-09-05", "relevancy": 2.032, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5193}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Have%20Large%20Vision-Language%20Models%20Mastered%20Art%20History%3F&body=Title%3A%20Have%20Large%20Vision-Language%20Models%20Mastered%20Art%20History%3F%0AAuthor%3A%20Ombretta%20Strafforello%20and%20Derya%20Soydaner%20and%20Michiel%20Willems%20and%20Anne-Sofie%20Maerten%20and%20Stefanie%20De%20Winter%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20has%20recently%20established%0Anew%20baselines%20in%20image%20classification%20across%20multiple%20domains.%20However%2C%20the%0Aperformance%20of%20VLMs%20in%20the%20specific%20task%20of%20artwork%20classification%2C%0Aparticularly%20art%20style%20classification%20of%20paintings%20-%20a%20domain%20traditionally%0Amastered%20by%20art%20historians%20-%20has%20not%20been%20explored%20yet.%20Artworks%20pose%20a%20unique%0Achallenge%20compared%20to%20natural%20images%20due%20to%20their%20inherently%20complex%20and%0Adiverse%20structures%2C%20characterized%20by%20variable%20compositions%20and%20styles.%20Art%0Ahistorians%20have%20long%20studied%20the%20unique%20aspects%20of%20artworks%2C%20with%20style%0Aprediction%20being%20a%20crucial%20component%20of%20their%20discipline.%20This%20paper%0Ainvestigates%20whether%20large%20VLMs%2C%20which%20integrate%20visual%20and%20textual%20data%2C%20can%0Aeffectively%20predict%20the%20art%20historical%20attributes%20of%20paintings.%20We%20conduct%20an%0Ain-depth%20analysis%20of%20four%20VLMs%2C%20namely%20CLIP%2C%20LLaVA%2C%20OpenFlamingo%2C%20and%20GPT-4o%2C%0Afocusing%20on%20zero-shot%20classification%20of%20art%20style%2C%20author%20and%20time%20period%20using%0Atwo%20public%20benchmarks%20of%20artworks.%20Additionally%2C%20we%20present%20ArTest%2C%20a%0Awell-curated%20test%20set%20of%20artworks%2C%20including%20pivotal%20paintings%20studied%20by%20art%0Ahistorians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHave%2520Large%2520Vision-Language%2520Models%2520Mastered%2520Art%2520History%253F%26entry.906535625%3DOmbretta%2520Strafforello%2520and%2520Derya%2520Soydaner%2520and%2520Michiel%2520Willems%2520and%2520Anne-Sofie%2520Maerten%2520and%2520Stefanie%2520De%2520Winter%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520recently%2520established%250Anew%2520baselines%2520in%2520image%2520classification%2520across%2520multiple%2520domains.%2520However%252C%2520the%250Aperformance%2520of%2520VLMs%2520in%2520the%2520specific%2520task%2520of%2520artwork%2520classification%252C%250Aparticularly%2520art%2520style%2520classification%2520of%2520paintings%2520-%2520a%2520domain%2520traditionally%250Amastered%2520by%2520art%2520historians%2520-%2520has%2520not%2520been%2520explored%2520yet.%2520Artworks%2520pose%2520a%2520unique%250Achallenge%2520compared%2520to%2520natural%2520images%2520due%2520to%2520their%2520inherently%2520complex%2520and%250Adiverse%2520structures%252C%2520characterized%2520by%2520variable%2520compositions%2520and%2520styles.%2520Art%250Ahistorians%2520have%2520long%2520studied%2520the%2520unique%2520aspects%2520of%2520artworks%252C%2520with%2520style%250Aprediction%2520being%2520a%2520crucial%2520component%2520of%2520their%2520discipline.%2520This%2520paper%250Ainvestigates%2520whether%2520large%2520VLMs%252C%2520which%2520integrate%2520visual%2520and%2520textual%2520data%252C%2520can%250Aeffectively%2520predict%2520the%2520art%2520historical%2520attributes%2520of%2520paintings.%2520We%2520conduct%2520an%250Ain-depth%2520analysis%2520of%2520four%2520VLMs%252C%2520namely%2520CLIP%252C%2520LLaVA%252C%2520OpenFlamingo%252C%2520and%2520GPT-4o%252C%250Afocusing%2520on%2520zero-shot%2520classification%2520of%2520art%2520style%252C%2520author%2520and%2520time%2520period%2520using%250Atwo%2520public%2520benchmarks%2520of%2520artworks.%2520Additionally%252C%2520we%2520present%2520ArTest%252C%2520a%250Awell-curated%2520test%2520set%2520of%2520artworks%252C%2520including%2520pivotal%2520paintings%2520studied%2520by%2520art%250Ahistorians.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Have%20Large%20Vision-Language%20Models%20Mastered%20Art%20History%3F&entry.906535625=Ombretta%20Strafforello%20and%20Derya%20Soydaner%20and%20Michiel%20Willems%20and%20Anne-Sofie%20Maerten%20and%20Stefanie%20De%20Winter&entry.1292438233=%20%20The%20emergence%20of%20large%20Vision-Language%20Models%20%28VLMs%29%20has%20recently%20established%0Anew%20baselines%20in%20image%20classification%20across%20multiple%20domains.%20However%2C%20the%0Aperformance%20of%20VLMs%20in%20the%20specific%20task%20of%20artwork%20classification%2C%0Aparticularly%20art%20style%20classification%20of%20paintings%20-%20a%20domain%20traditionally%0Amastered%20by%20art%20historians%20-%20has%20not%20been%20explored%20yet.%20Artworks%20pose%20a%20unique%0Achallenge%20compared%20to%20natural%20images%20due%20to%20their%20inherently%20complex%20and%0Adiverse%20structures%2C%20characterized%20by%20variable%20compositions%20and%20styles.%20Art%0Ahistorians%20have%20long%20studied%20the%20unique%20aspects%20of%20artworks%2C%20with%20style%0Aprediction%20being%20a%20crucial%20component%20of%20their%20discipline.%20This%20paper%0Ainvestigates%20whether%20large%20VLMs%2C%20which%20integrate%20visual%20and%20textual%20data%2C%20can%0Aeffectively%20predict%20the%20art%20historical%20attributes%20of%20paintings.%20We%20conduct%20an%0Ain-depth%20analysis%20of%20four%20VLMs%2C%20namely%20CLIP%2C%20LLaVA%2C%20OpenFlamingo%2C%20and%20GPT-4o%2C%0Afocusing%20on%20zero-shot%20classification%20of%20art%20style%2C%20author%20and%20time%20period%20using%0Atwo%20public%20benchmarks%20of%20artworks.%20Additionally%2C%20we%20present%20ArTest%2C%20a%0Awell-curated%20test%20set%20of%20artworks%2C%20including%20pivotal%20paintings%20studied%20by%20art%0Ahistorians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03521v1&entry.124074799=Read"},
{"title": "Robust Clustering on High-Dimensional Data with Stochastic Quantization", "author": "Anton Kozyriev and Vladimir Norkin", "abstract": "  This paper addresses the limitations of traditional vector quantization\n(clustering) algorithms, particularly K-Means and its variant K-Means++, and\nexplores the Stochastic Quantization (SQ) algorithm as a scalable alternative\nfor high-dimensional unsupervised and semi-supervised learning problems. Some\ntraditional clustering algorithms suffer from inefficient memory utilization\nduring computation, necessitating the loading of all data samples into memory,\nwhich becomes impractical for large-scale datasets. While variants such as\nMini-Batch K-Means partially mitigate this issue by reducing memory usage, they\nlack robust theoretical convergence guarantees due to the non-convex nature of\nclustering problems. In contrast, the Stochastic Quantization algorithm\nprovides strong theoretical convergence guarantees, making it a robust\nalternative for clustering tasks. We demonstrate the computational efficiency\nand rapid convergence of the algorithm on an image classification problem with\npartially labeled data, comparing model accuracy across various ratios of\nlabeled to unlabeled data. To address the challenge of high dimensionality, we\ntrained Triplet Network to encode images into low-dimensional representations\nin a latent space, which serve as a basis for comparing the efficiency of both\nthe Stochastic Quantization algorithm and traditional quantization algorithms.\nFurthermore, we enhance the algorithm's convergence speed by introducing\nmodifications with an adaptive learning rate.\n", "link": "http://arxiv.org/abs/2409.02066v2", "date": "2024-09-05", "relevancy": 2.0233, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5173}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Clustering%20on%20High-Dimensional%20Data%20with%20Stochastic%20Quantization&body=Title%3A%20Robust%20Clustering%20on%20High-Dimensional%20Data%20with%20Stochastic%20Quantization%0AAuthor%3A%20Anton%20Kozyriev%20and%20Vladimir%20Norkin%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20limitations%20of%20traditional%20vector%20quantization%0A%28clustering%29%20algorithms%2C%20particularly%20K-Means%20and%20its%20variant%20K-Means%2B%2B%2C%20and%0Aexplores%20the%20Stochastic%20Quantization%20%28SQ%29%20algorithm%20as%20a%20scalable%20alternative%0Afor%20high-dimensional%20unsupervised%20and%20semi-supervised%20learning%20problems.%20Some%0Atraditional%20clustering%20algorithms%20suffer%20from%20inefficient%20memory%20utilization%0Aduring%20computation%2C%20necessitating%20the%20loading%20of%20all%20data%20samples%20into%20memory%2C%0Awhich%20becomes%20impractical%20for%20large-scale%20datasets.%20While%20variants%20such%20as%0AMini-Batch%20K-Means%20partially%20mitigate%20this%20issue%20by%20reducing%20memory%20usage%2C%20they%0Alack%20robust%20theoretical%20convergence%20guarantees%20due%20to%20the%20non-convex%20nature%20of%0Aclustering%20problems.%20In%20contrast%2C%20the%20Stochastic%20Quantization%20algorithm%0Aprovides%20strong%20theoretical%20convergence%20guarantees%2C%20making%20it%20a%20robust%0Aalternative%20for%20clustering%20tasks.%20We%20demonstrate%20the%20computational%20efficiency%0Aand%20rapid%20convergence%20of%20the%20algorithm%20on%20an%20image%20classification%20problem%20with%0Apartially%20labeled%20data%2C%20comparing%20model%20accuracy%20across%20various%20ratios%20of%0Alabeled%20to%20unlabeled%20data.%20To%20address%20the%20challenge%20of%20high%20dimensionality%2C%20we%0Atrained%20Triplet%20Network%20to%20encode%20images%20into%20low-dimensional%20representations%0Ain%20a%20latent%20space%2C%20which%20serve%20as%20a%20basis%20for%20comparing%20the%20efficiency%20of%20both%0Athe%20Stochastic%20Quantization%20algorithm%20and%20traditional%20quantization%20algorithms.%0AFurthermore%2C%20we%20enhance%20the%20algorithm%27s%20convergence%20speed%20by%20introducing%0Amodifications%20with%20an%20adaptive%20learning%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Clustering%2520on%2520High-Dimensional%2520Data%2520with%2520Stochastic%2520Quantization%26entry.906535625%3DAnton%2520Kozyriev%2520and%2520Vladimir%2520Norkin%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520limitations%2520of%2520traditional%2520vector%2520quantization%250A%2528clustering%2529%2520algorithms%252C%2520particularly%2520K-Means%2520and%2520its%2520variant%2520K-Means%252B%252B%252C%2520and%250Aexplores%2520the%2520Stochastic%2520Quantization%2520%2528SQ%2529%2520algorithm%2520as%2520a%2520scalable%2520alternative%250Afor%2520high-dimensional%2520unsupervised%2520and%2520semi-supervised%2520learning%2520problems.%2520Some%250Atraditional%2520clustering%2520algorithms%2520suffer%2520from%2520inefficient%2520memory%2520utilization%250Aduring%2520computation%252C%2520necessitating%2520the%2520loading%2520of%2520all%2520data%2520samples%2520into%2520memory%252C%250Awhich%2520becomes%2520impractical%2520for%2520large-scale%2520datasets.%2520While%2520variants%2520such%2520as%250AMini-Batch%2520K-Means%2520partially%2520mitigate%2520this%2520issue%2520by%2520reducing%2520memory%2520usage%252C%2520they%250Alack%2520robust%2520theoretical%2520convergence%2520guarantees%2520due%2520to%2520the%2520non-convex%2520nature%2520of%250Aclustering%2520problems.%2520In%2520contrast%252C%2520the%2520Stochastic%2520Quantization%2520algorithm%250Aprovides%2520strong%2520theoretical%2520convergence%2520guarantees%252C%2520making%2520it%2520a%2520robust%250Aalternative%2520for%2520clustering%2520tasks.%2520We%2520demonstrate%2520the%2520computational%2520efficiency%250Aand%2520rapid%2520convergence%2520of%2520the%2520algorithm%2520on%2520an%2520image%2520classification%2520problem%2520with%250Apartially%2520labeled%2520data%252C%2520comparing%2520model%2520accuracy%2520across%2520various%2520ratios%2520of%250Alabeled%2520to%2520unlabeled%2520data.%2520To%2520address%2520the%2520challenge%2520of%2520high%2520dimensionality%252C%2520we%250Atrained%2520Triplet%2520Network%2520to%2520encode%2520images%2520into%2520low-dimensional%2520representations%250Ain%2520a%2520latent%2520space%252C%2520which%2520serve%2520as%2520a%2520basis%2520for%2520comparing%2520the%2520efficiency%2520of%2520both%250Athe%2520Stochastic%2520Quantization%2520algorithm%2520and%2520traditional%2520quantization%2520algorithms.%250AFurthermore%252C%2520we%2520enhance%2520the%2520algorithm%2527s%2520convergence%2520speed%2520by%2520introducing%250Amodifications%2520with%2520an%2520adaptive%2520learning%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Clustering%20on%20High-Dimensional%20Data%20with%20Stochastic%20Quantization&entry.906535625=Anton%20Kozyriev%20and%20Vladimir%20Norkin&entry.1292438233=%20%20This%20paper%20addresses%20the%20limitations%20of%20traditional%20vector%20quantization%0A%28clustering%29%20algorithms%2C%20particularly%20K-Means%20and%20its%20variant%20K-Means%2B%2B%2C%20and%0Aexplores%20the%20Stochastic%20Quantization%20%28SQ%29%20algorithm%20as%20a%20scalable%20alternative%0Afor%20high-dimensional%20unsupervised%20and%20semi-supervised%20learning%20problems.%20Some%0Atraditional%20clustering%20algorithms%20suffer%20from%20inefficient%20memory%20utilization%0Aduring%20computation%2C%20necessitating%20the%20loading%20of%20all%20data%20samples%20into%20memory%2C%0Awhich%20becomes%20impractical%20for%20large-scale%20datasets.%20While%20variants%20such%20as%0AMini-Batch%20K-Means%20partially%20mitigate%20this%20issue%20by%20reducing%20memory%20usage%2C%20they%0Alack%20robust%20theoretical%20convergence%20guarantees%20due%20to%20the%20non-convex%20nature%20of%0Aclustering%20problems.%20In%20contrast%2C%20the%20Stochastic%20Quantization%20algorithm%0Aprovides%20strong%20theoretical%20convergence%20guarantees%2C%20making%20it%20a%20robust%0Aalternative%20for%20clustering%20tasks.%20We%20demonstrate%20the%20computational%20efficiency%0Aand%20rapid%20convergence%20of%20the%20algorithm%20on%20an%20image%20classification%20problem%20with%0Apartially%20labeled%20data%2C%20comparing%20model%20accuracy%20across%20various%20ratios%20of%0Alabeled%20to%20unlabeled%20data.%20To%20address%20the%20challenge%20of%20high%20dimensionality%2C%20we%0Atrained%20Triplet%20Network%20to%20encode%20images%20into%20low-dimensional%20representations%0Ain%20a%20latent%20space%2C%20which%20serve%20as%20a%20basis%20for%20comparing%20the%20efficiency%20of%20both%0Athe%20Stochastic%20Quantization%20algorithm%20and%20traditional%20quantization%20algorithms.%0AFurthermore%2C%20we%20enhance%20the%20algorithm%27s%20convergence%20speed%20by%20introducing%0Amodifications%20with%20an%20adaptive%20learning%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02066v2&entry.124074799=Read"},
{"title": "Rethinking Molecular Design: Integrating Latent Variable and\n  Auto-Regressive Models for Goal Directed Generation", "author": "Heath Arthur-Loui and Amina Mollaysa and Michael Krauthammer", "abstract": "  De novo molecule design has become a highly active research area, advanced\nsignificantly through the use of state-of-the-art generative models. Despite\nthese advances, several fundamental questions remain unanswered as the field\nincreasingly focuses on more complex generative models and sophisticated\nmolecular representations as an answer to the challenges of drug design. In\nthis paper, we return to the simplest representation of molecules, and\ninvestigate overlooked limitations of classical generative approaches,\nparticularly Variational Autoencoders (VAEs) and auto-regressive models. We\npropose a hybrid model in the form of a novel regularizer that leverages the\nstrengths of both to improve validity, conditional generation, and style\ntransfer of molecular sequences. Additionally, we provide an in depth\ndiscussion of overlooked assumptions of these models' behaviour.\n", "link": "http://arxiv.org/abs/2409.00046v2", "date": "2024-09-05", "relevancy": 2.0168, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.523}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4936}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Molecular%20Design%3A%20Integrating%20Latent%20Variable%20and%0A%20%20Auto-Regressive%20Models%20for%20Goal%20Directed%20Generation&body=Title%3A%20Rethinking%20Molecular%20Design%3A%20Integrating%20Latent%20Variable%20and%0A%20%20Auto-Regressive%20Models%20for%20Goal%20Directed%20Generation%0AAuthor%3A%20Heath%20Arthur-Loui%20and%20Amina%20Mollaysa%20and%20Michael%20Krauthammer%0AAbstract%3A%20%20%20De%20novo%20molecule%20design%20has%20become%20a%20highly%20active%20research%20area%2C%20advanced%0Asignificantly%20through%20the%20use%20of%20state-of-the-art%20generative%20models.%20Despite%0Athese%20advances%2C%20several%20fundamental%20questions%20remain%20unanswered%20as%20the%20field%0Aincreasingly%20focuses%20on%20more%20complex%20generative%20models%20and%20sophisticated%0Amolecular%20representations%20as%20an%20answer%20to%20the%20challenges%20of%20drug%20design.%20In%0Athis%20paper%2C%20we%20return%20to%20the%20simplest%20representation%20of%20molecules%2C%20and%0Ainvestigate%20overlooked%20limitations%20of%20classical%20generative%20approaches%2C%0Aparticularly%20Variational%20Autoencoders%20%28VAEs%29%20and%20auto-regressive%20models.%20We%0Apropose%20a%20hybrid%20model%20in%20the%20form%20of%20a%20novel%20regularizer%20that%20leverages%20the%0Astrengths%20of%20both%20to%20improve%20validity%2C%20conditional%20generation%2C%20and%20style%0Atransfer%20of%20molecular%20sequences.%20Additionally%2C%20we%20provide%20an%20in%20depth%0Adiscussion%20of%20overlooked%20assumptions%20of%20these%20models%27%20behaviour.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Molecular%2520Design%253A%2520Integrating%2520Latent%2520Variable%2520and%250A%2520%2520Auto-Regressive%2520Models%2520for%2520Goal%2520Directed%2520Generation%26entry.906535625%3DHeath%2520Arthur-Loui%2520and%2520Amina%2520Mollaysa%2520and%2520Michael%2520Krauthammer%26entry.1292438233%3D%2520%2520De%2520novo%2520molecule%2520design%2520has%2520become%2520a%2520highly%2520active%2520research%2520area%252C%2520advanced%250Asignificantly%2520through%2520the%2520use%2520of%2520state-of-the-art%2520generative%2520models.%2520Despite%250Athese%2520advances%252C%2520several%2520fundamental%2520questions%2520remain%2520unanswered%2520as%2520the%2520field%250Aincreasingly%2520focuses%2520on%2520more%2520complex%2520generative%2520models%2520and%2520sophisticated%250Amolecular%2520representations%2520as%2520an%2520answer%2520to%2520the%2520challenges%2520of%2520drug%2520design.%2520In%250Athis%2520paper%252C%2520we%2520return%2520to%2520the%2520simplest%2520representation%2520of%2520molecules%252C%2520and%250Ainvestigate%2520overlooked%2520limitations%2520of%2520classical%2520generative%2520approaches%252C%250Aparticularly%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520and%2520auto-regressive%2520models.%2520We%250Apropose%2520a%2520hybrid%2520model%2520in%2520the%2520form%2520of%2520a%2520novel%2520regularizer%2520that%2520leverages%2520the%250Astrengths%2520of%2520both%2520to%2520improve%2520validity%252C%2520conditional%2520generation%252C%2520and%2520style%250Atransfer%2520of%2520molecular%2520sequences.%2520Additionally%252C%2520we%2520provide%2520an%2520in%2520depth%250Adiscussion%2520of%2520overlooked%2520assumptions%2520of%2520these%2520models%2527%2520behaviour.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Molecular%20Design%3A%20Integrating%20Latent%20Variable%20and%0A%20%20Auto-Regressive%20Models%20for%20Goal%20Directed%20Generation&entry.906535625=Heath%20Arthur-Loui%20and%20Amina%20Mollaysa%20and%20Michael%20Krauthammer&entry.1292438233=%20%20De%20novo%20molecule%20design%20has%20become%20a%20highly%20active%20research%20area%2C%20advanced%0Asignificantly%20through%20the%20use%20of%20state-of-the-art%20generative%20models.%20Despite%0Athese%20advances%2C%20several%20fundamental%20questions%20remain%20unanswered%20as%20the%20field%0Aincreasingly%20focuses%20on%20more%20complex%20generative%20models%20and%20sophisticated%0Amolecular%20representations%20as%20an%20answer%20to%20the%20challenges%20of%20drug%20design.%20In%0Athis%20paper%2C%20we%20return%20to%20the%20simplest%20representation%20of%20molecules%2C%20and%0Ainvestigate%20overlooked%20limitations%20of%20classical%20generative%20approaches%2C%0Aparticularly%20Variational%20Autoencoders%20%28VAEs%29%20and%20auto-regressive%20models.%20We%0Apropose%20a%20hybrid%20model%20in%20the%20form%20of%20a%20novel%20regularizer%20that%20leverages%20the%0Astrengths%20of%20both%20to%20improve%20validity%2C%20conditional%20generation%2C%20and%20style%0Atransfer%20of%20molecular%20sequences.%20Additionally%2C%20we%20provide%20an%20in%20depth%0Adiscussion%20of%20overlooked%20assumptions%20of%20these%20models%27%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00046v2&entry.124074799=Read"},
{"title": "A DNN Biophysics Model with Topological and Electrostatic Features", "author": "Elyssa Sliheet and Md Abu Talha and Weihua Geng", "abstract": "  In this project, we provide a deep-learning neural network (DNN) based\nbiophysics model to predict protein properties. The model uses multi-scale and\nuniform topological and electrostatic features generated with protein\nstructural information and force field, which governs the molecular mechanics.\nThe topological features are generated using the element specified persistent\nhomology (ESPH) while the electrostatic features are fast computed using a\nCartesian treecode. These features are uniform in number for proteins with\nvarious sizes thus the broadly available protein structure database can be used\nin training the network. These features are also multi-scale thus the\nresolution and computational cost can be balanced by the users. The machine\nlearning simulation on over 4000 protein structures shows the efficiency and\nfidelity of these features in representing the protein structure and force\nfield for the predication of their biophysical properties such as electrostatic\nsolvation energy. Tests on topological or electrostatic features alone and the\ncombination of both showed the optimal performance when both features are used.\nThis model shows its potential as a general tool in assisting biophysical\nproperties and function prediction for the broad biomolecules using data from\nboth theoretical computing and experiments.\n", "link": "http://arxiv.org/abs/2409.03658v1", "date": "2024-09-05", "relevancy": 1.9949, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20DNN%20Biophysics%20Model%20with%20Topological%20and%20Electrostatic%20Features&body=Title%3A%20A%20DNN%20Biophysics%20Model%20with%20Topological%20and%20Electrostatic%20Features%0AAuthor%3A%20Elyssa%20Sliheet%20and%20Md%20Abu%20Talha%20and%20Weihua%20Geng%0AAbstract%3A%20%20%20In%20this%20project%2C%20we%20provide%20a%20deep-learning%20neural%20network%20%28DNN%29%20based%0Abiophysics%20model%20to%20predict%20protein%20properties.%20The%20model%20uses%20multi-scale%20and%0Auniform%20topological%20and%20electrostatic%20features%20generated%20with%20protein%0Astructural%20information%20and%20force%20field%2C%20which%20governs%20the%20molecular%20mechanics.%0AThe%20topological%20features%20are%20generated%20using%20the%20element%20specified%20persistent%0Ahomology%20%28ESPH%29%20while%20the%20electrostatic%20features%20are%20fast%20computed%20using%20a%0ACartesian%20treecode.%20These%20features%20are%20uniform%20in%20number%20for%20proteins%20with%0Avarious%20sizes%20thus%20the%20broadly%20available%20protein%20structure%20database%20can%20be%20used%0Ain%20training%20the%20network.%20These%20features%20are%20also%20multi-scale%20thus%20the%0Aresolution%20and%20computational%20cost%20can%20be%20balanced%20by%20the%20users.%20The%20machine%0Alearning%20simulation%20on%20over%204000%20protein%20structures%20shows%20the%20efficiency%20and%0Afidelity%20of%20these%20features%20in%20representing%20the%20protein%20structure%20and%20force%0Afield%20for%20the%20predication%20of%20their%20biophysical%20properties%20such%20as%20electrostatic%0Asolvation%20energy.%20Tests%20on%20topological%20or%20electrostatic%20features%20alone%20and%20the%0Acombination%20of%20both%20showed%20the%20optimal%20performance%20when%20both%20features%20are%20used.%0AThis%20model%20shows%20its%20potential%20as%20a%20general%20tool%20in%20assisting%20biophysical%0Aproperties%20and%20function%20prediction%20for%20the%20broad%20biomolecules%20using%20data%20from%0Aboth%20theoretical%20computing%20and%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520DNN%2520Biophysics%2520Model%2520with%2520Topological%2520and%2520Electrostatic%2520Features%26entry.906535625%3DElyssa%2520Sliheet%2520and%2520Md%2520Abu%2520Talha%2520and%2520Weihua%2520Geng%26entry.1292438233%3D%2520%2520In%2520this%2520project%252C%2520we%2520provide%2520a%2520deep-learning%2520neural%2520network%2520%2528DNN%2529%2520based%250Abiophysics%2520model%2520to%2520predict%2520protein%2520properties.%2520The%2520model%2520uses%2520multi-scale%2520and%250Auniform%2520topological%2520and%2520electrostatic%2520features%2520generated%2520with%2520protein%250Astructural%2520information%2520and%2520force%2520field%252C%2520which%2520governs%2520the%2520molecular%2520mechanics.%250AThe%2520topological%2520features%2520are%2520generated%2520using%2520the%2520element%2520specified%2520persistent%250Ahomology%2520%2528ESPH%2529%2520while%2520the%2520electrostatic%2520features%2520are%2520fast%2520computed%2520using%2520a%250ACartesian%2520treecode.%2520These%2520features%2520are%2520uniform%2520in%2520number%2520for%2520proteins%2520with%250Avarious%2520sizes%2520thus%2520the%2520broadly%2520available%2520protein%2520structure%2520database%2520can%2520be%2520used%250Ain%2520training%2520the%2520network.%2520These%2520features%2520are%2520also%2520multi-scale%2520thus%2520the%250Aresolution%2520and%2520computational%2520cost%2520can%2520be%2520balanced%2520by%2520the%2520users.%2520The%2520machine%250Alearning%2520simulation%2520on%2520over%25204000%2520protein%2520structures%2520shows%2520the%2520efficiency%2520and%250Afidelity%2520of%2520these%2520features%2520in%2520representing%2520the%2520protein%2520structure%2520and%2520force%250Afield%2520for%2520the%2520predication%2520of%2520their%2520biophysical%2520properties%2520such%2520as%2520electrostatic%250Asolvation%2520energy.%2520Tests%2520on%2520topological%2520or%2520electrostatic%2520features%2520alone%2520and%2520the%250Acombination%2520of%2520both%2520showed%2520the%2520optimal%2520performance%2520when%2520both%2520features%2520are%2520used.%250AThis%2520model%2520shows%2520its%2520potential%2520as%2520a%2520general%2520tool%2520in%2520assisting%2520biophysical%250Aproperties%2520and%2520function%2520prediction%2520for%2520the%2520broad%2520biomolecules%2520using%2520data%2520from%250Aboth%2520theoretical%2520computing%2520and%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20DNN%20Biophysics%20Model%20with%20Topological%20and%20Electrostatic%20Features&entry.906535625=Elyssa%20Sliheet%20and%20Md%20Abu%20Talha%20and%20Weihua%20Geng&entry.1292438233=%20%20In%20this%20project%2C%20we%20provide%20a%20deep-learning%20neural%20network%20%28DNN%29%20based%0Abiophysics%20model%20to%20predict%20protein%20properties.%20The%20model%20uses%20multi-scale%20and%0Auniform%20topological%20and%20electrostatic%20features%20generated%20with%20protein%0Astructural%20information%20and%20force%20field%2C%20which%20governs%20the%20molecular%20mechanics.%0AThe%20topological%20features%20are%20generated%20using%20the%20element%20specified%20persistent%0Ahomology%20%28ESPH%29%20while%20the%20electrostatic%20features%20are%20fast%20computed%20using%20a%0ACartesian%20treecode.%20These%20features%20are%20uniform%20in%20number%20for%20proteins%20with%0Avarious%20sizes%20thus%20the%20broadly%20available%20protein%20structure%20database%20can%20be%20used%0Ain%20training%20the%20network.%20These%20features%20are%20also%20multi-scale%20thus%20the%0Aresolution%20and%20computational%20cost%20can%20be%20balanced%20by%20the%20users.%20The%20machine%0Alearning%20simulation%20on%20over%204000%20protein%20structures%20shows%20the%20efficiency%20and%0Afidelity%20of%20these%20features%20in%20representing%20the%20protein%20structure%20and%20force%0Afield%20for%20the%20predication%20of%20their%20biophysical%20properties%20such%20as%20electrostatic%0Asolvation%20energy.%20Tests%20on%20topological%20or%20electrostatic%20features%20alone%20and%20the%0Acombination%20of%20both%20showed%20the%20optimal%20performance%20when%20both%20features%20are%20used.%0AThis%20model%20shows%20its%20potential%20as%20a%20general%20tool%20in%20assisting%20biophysical%0Aproperties%20and%20function%20prediction%20for%20the%20broad%20biomolecules%20using%20data%20from%0Aboth%20theoretical%20computing%20and%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03658v1&entry.124074799=Read"},
{"title": "Differentiable Discrete Event Simulation for Queuing Network Control", "author": "Ethan Che and Jing Dong and Hongseok Namkoong", "abstract": "  Queuing network control is essential for managing congestion in\njob-processing systems such as service systems, communication networks, and\nmanufacturing processes. Despite growing interest in applying reinforcement\nlearning (RL) techniques, queueing network control poses distinct challenges,\nincluding high stochasticity, large state and action spaces, and lack of\nstability. To tackle these challenges, we propose a scalable framework for\npolicy optimization based on differentiable discrete event simulation. Our main\ninsight is that by implementing a well-designed smoothing technique for\ndiscrete event dynamics, we can compute pathwise policy gradients for\nlarge-scale queueing networks using auto-differentiation software (e.g.,\nTensorflow, PyTorch) and GPU parallelization. Through extensive empirical\nexperiments, we observe that our policy gradient estimators are several orders\nof magnitude more accurate than typical REINFORCE-based estimators. In\naddition, We propose a new policy architecture, which drastically improves\nstability while maintaining the flexibility of neural-network policies. In a\nwide variety of scheduling and admission control tasks, we demonstrate that\ntraining control policies with pathwise gradients leads to a 50-1000x\nimprovement in sample efficiency over state-of-the-art RL methods. Unlike prior\ntailored approaches to queueing, our methods can flexibly handle realistic\nscenarios, including systems operating in non-stationary environments and those\nwith non-exponential interarrival/service times.\n", "link": "http://arxiv.org/abs/2409.03740v1", "date": "2024-09-05", "relevancy": 1.9838, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4867}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Discrete%20Event%20Simulation%20for%20Queuing%20Network%20Control&body=Title%3A%20Differentiable%20Discrete%20Event%20Simulation%20for%20Queuing%20Network%20Control%0AAuthor%3A%20Ethan%20Che%20and%20Jing%20Dong%20and%20Hongseok%20Namkoong%0AAbstract%3A%20%20%20Queuing%20network%20control%20is%20essential%20for%20managing%20congestion%20in%0Ajob-processing%20systems%20such%20as%20service%20systems%2C%20communication%20networks%2C%20and%0Amanufacturing%20processes.%20Despite%20growing%20interest%20in%20applying%20reinforcement%0Alearning%20%28RL%29%20techniques%2C%20queueing%20network%20control%20poses%20distinct%20challenges%2C%0Aincluding%20high%20stochasticity%2C%20large%20state%20and%20action%20spaces%2C%20and%20lack%20of%0Astability.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20scalable%20framework%20for%0Apolicy%20optimization%20based%20on%20differentiable%20discrete%20event%20simulation.%20Our%20main%0Ainsight%20is%20that%20by%20implementing%20a%20well-designed%20smoothing%20technique%20for%0Adiscrete%20event%20dynamics%2C%20we%20can%20compute%20pathwise%20policy%20gradients%20for%0Alarge-scale%20queueing%20networks%20using%20auto-differentiation%20software%20%28e.g.%2C%0ATensorflow%2C%20PyTorch%29%20and%20GPU%20parallelization.%20Through%20extensive%20empirical%0Aexperiments%2C%20we%20observe%20that%20our%20policy%20gradient%20estimators%20are%20several%20orders%0Aof%20magnitude%20more%20accurate%20than%20typical%20REINFORCE-based%20estimators.%20In%0Aaddition%2C%20We%20propose%20a%20new%20policy%20architecture%2C%20which%20drastically%20improves%0Astability%20while%20maintaining%20the%20flexibility%20of%20neural-network%20policies.%20In%20a%0Awide%20variety%20of%20scheduling%20and%20admission%20control%20tasks%2C%20we%20demonstrate%20that%0Atraining%20control%20policies%20with%20pathwise%20gradients%20leads%20to%20a%2050-1000x%0Aimprovement%20in%20sample%20efficiency%20over%20state-of-the-art%20RL%20methods.%20Unlike%20prior%0Atailored%20approaches%20to%20queueing%2C%20our%20methods%20can%20flexibly%20handle%20realistic%0Ascenarios%2C%20including%20systems%20operating%20in%20non-stationary%20environments%20and%20those%0Awith%20non-exponential%20interarrival/service%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Discrete%2520Event%2520Simulation%2520for%2520Queuing%2520Network%2520Control%26entry.906535625%3DEthan%2520Che%2520and%2520Jing%2520Dong%2520and%2520Hongseok%2520Namkoong%26entry.1292438233%3D%2520%2520Queuing%2520network%2520control%2520is%2520essential%2520for%2520managing%2520congestion%2520in%250Ajob-processing%2520systems%2520such%2520as%2520service%2520systems%252C%2520communication%2520networks%252C%2520and%250Amanufacturing%2520processes.%2520Despite%2520growing%2520interest%2520in%2520applying%2520reinforcement%250Alearning%2520%2528RL%2529%2520techniques%252C%2520queueing%2520network%2520control%2520poses%2520distinct%2520challenges%252C%250Aincluding%2520high%2520stochasticity%252C%2520large%2520state%2520and%2520action%2520spaces%252C%2520and%2520lack%2520of%250Astability.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520scalable%2520framework%2520for%250Apolicy%2520optimization%2520based%2520on%2520differentiable%2520discrete%2520event%2520simulation.%2520Our%2520main%250Ainsight%2520is%2520that%2520by%2520implementing%2520a%2520well-designed%2520smoothing%2520technique%2520for%250Adiscrete%2520event%2520dynamics%252C%2520we%2520can%2520compute%2520pathwise%2520policy%2520gradients%2520for%250Alarge-scale%2520queueing%2520networks%2520using%2520auto-differentiation%2520software%2520%2528e.g.%252C%250ATensorflow%252C%2520PyTorch%2529%2520and%2520GPU%2520parallelization.%2520Through%2520extensive%2520empirical%250Aexperiments%252C%2520we%2520observe%2520that%2520our%2520policy%2520gradient%2520estimators%2520are%2520several%2520orders%250Aof%2520magnitude%2520more%2520accurate%2520than%2520typical%2520REINFORCE-based%2520estimators.%2520In%250Aaddition%252C%2520We%2520propose%2520a%2520new%2520policy%2520architecture%252C%2520which%2520drastically%2520improves%250Astability%2520while%2520maintaining%2520the%2520flexibility%2520of%2520neural-network%2520policies.%2520In%2520a%250Awide%2520variety%2520of%2520scheduling%2520and%2520admission%2520control%2520tasks%252C%2520we%2520demonstrate%2520that%250Atraining%2520control%2520policies%2520with%2520pathwise%2520gradients%2520leads%2520to%2520a%252050-1000x%250Aimprovement%2520in%2520sample%2520efficiency%2520over%2520state-of-the-art%2520RL%2520methods.%2520Unlike%2520prior%250Atailored%2520approaches%2520to%2520queueing%252C%2520our%2520methods%2520can%2520flexibly%2520handle%2520realistic%250Ascenarios%252C%2520including%2520systems%2520operating%2520in%2520non-stationary%2520environments%2520and%2520those%250Awith%2520non-exponential%2520interarrival/service%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Discrete%20Event%20Simulation%20for%20Queuing%20Network%20Control&entry.906535625=Ethan%20Che%20and%20Jing%20Dong%20and%20Hongseok%20Namkoong&entry.1292438233=%20%20Queuing%20network%20control%20is%20essential%20for%20managing%20congestion%20in%0Ajob-processing%20systems%20such%20as%20service%20systems%2C%20communication%20networks%2C%20and%0Amanufacturing%20processes.%20Despite%20growing%20interest%20in%20applying%20reinforcement%0Alearning%20%28RL%29%20techniques%2C%20queueing%20network%20control%20poses%20distinct%20challenges%2C%0Aincluding%20high%20stochasticity%2C%20large%20state%20and%20action%20spaces%2C%20and%20lack%20of%0Astability.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20scalable%20framework%20for%0Apolicy%20optimization%20based%20on%20differentiable%20discrete%20event%20simulation.%20Our%20main%0Ainsight%20is%20that%20by%20implementing%20a%20well-designed%20smoothing%20technique%20for%0Adiscrete%20event%20dynamics%2C%20we%20can%20compute%20pathwise%20policy%20gradients%20for%0Alarge-scale%20queueing%20networks%20using%20auto-differentiation%20software%20%28e.g.%2C%0ATensorflow%2C%20PyTorch%29%20and%20GPU%20parallelization.%20Through%20extensive%20empirical%0Aexperiments%2C%20we%20observe%20that%20our%20policy%20gradient%20estimators%20are%20several%20orders%0Aof%20magnitude%20more%20accurate%20than%20typical%20REINFORCE-based%20estimators.%20In%0Aaddition%2C%20We%20propose%20a%20new%20policy%20architecture%2C%20which%20drastically%20improves%0Astability%20while%20maintaining%20the%20flexibility%20of%20neural-network%20policies.%20In%20a%0Awide%20variety%20of%20scheduling%20and%20admission%20control%20tasks%2C%20we%20demonstrate%20that%0Atraining%20control%20policies%20with%20pathwise%20gradients%20leads%20to%20a%2050-1000x%0Aimprovement%20in%20sample%20efficiency%20over%20state-of-the-art%20RL%20methods.%20Unlike%20prior%0Atailored%20approaches%20to%20queueing%2C%20our%20methods%20can%20flexibly%20handle%20realistic%0Ascenarios%2C%20including%20systems%20operating%20in%20non-stationary%20environments%20and%20those%0Awith%20non-exponential%20interarrival/service%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03740v1&entry.124074799=Read"},
{"title": "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization", "author": "Navid Ansari and Alireza Javanmardi and Eyke H\u00fcllermeier and Hans-Peter Seidel and Vahid Babaei", "abstract": "  Bayesian optimization (BO) provides a powerful framework for optimizing\nblack-box, expensive-to-evaluate functions. It is therefore an attractive tool\nfor engineering design problems, typically involving multiple objectives.\nThanks to the rapid advances in fabrication and measurement methods as well as\nparallel computing infrastructure, querying many design problems can be heavily\nparallelized. This class of problems challenges BO with an unprecedented setup\nwhere it has to deal with very large batches, shifting its focus from sample\nefficiency to iteration efficiency. We present a novel Bayesian optimization\nframework specifically tailored to address these limitations. Our key\ncontribution is a highly scalable, sample-based acquisition function that\nperforms a non-dominated sorting of not only the objectives but also their\nassociated uncertainty. We show that our acquisition function in combination\nwith different Bayesian neural network surrogates is effective in\ndata-intensive environments with a minimal number of iterations. We demonstrate\nthe superiority of our method by comparing it with state-of-the-art\nmulti-objective optimizations. We perform our evaluation on two real-world\nproblems -- airfoil design and 3D printing -- showcasing the applicability and\nefficiency of our approach. Our code is available at:\nhttps://github.com/an-on-ym-ous/lbn_mobo\n", "link": "http://arxiv.org/abs/2306.01095v4", "date": "2024-09-05", "relevancy": 1.9797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5241}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5095}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Batch%2C%20Iteration-Efficient%20Neural%20Bayesian%20Design%20Optimization&body=Title%3A%20Large-Batch%2C%20Iteration-Efficient%20Neural%20Bayesian%20Design%20Optimization%0AAuthor%3A%20Navid%20Ansari%20and%20Alireza%20Javanmardi%20and%20Eyke%20H%C3%BCllermeier%20and%20Hans-Peter%20Seidel%20and%20Vahid%20Babaei%0AAbstract%3A%20%20%20Bayesian%20optimization%20%28BO%29%20provides%20a%20powerful%20framework%20for%20optimizing%0Ablack-box%2C%20expensive-to-evaluate%20functions.%20It%20is%20therefore%20an%20attractive%20tool%0Afor%20engineering%20design%20problems%2C%20typically%20involving%20multiple%20objectives.%0AThanks%20to%20the%20rapid%20advances%20in%20fabrication%20and%20measurement%20methods%20as%20well%20as%0Aparallel%20computing%20infrastructure%2C%20querying%20many%20design%20problems%20can%20be%20heavily%0Aparallelized.%20This%20class%20of%20problems%20challenges%20BO%20with%20an%20unprecedented%20setup%0Awhere%20it%20has%20to%20deal%20with%20very%20large%20batches%2C%20shifting%20its%20focus%20from%20sample%0Aefficiency%20to%20iteration%20efficiency.%20We%20present%20a%20novel%20Bayesian%20optimization%0Aframework%20specifically%20tailored%20to%20address%20these%20limitations.%20Our%20key%0Acontribution%20is%20a%20highly%20scalable%2C%20sample-based%20acquisition%20function%20that%0Aperforms%20a%20non-dominated%20sorting%20of%20not%20only%20the%20objectives%20but%20also%20their%0Aassociated%20uncertainty.%20We%20show%20that%20our%20acquisition%20function%20in%20combination%0Awith%20different%20Bayesian%20neural%20network%20surrogates%20is%20effective%20in%0Adata-intensive%20environments%20with%20a%20minimal%20number%20of%20iterations.%20We%20demonstrate%0Athe%20superiority%20of%20our%20method%20by%20comparing%20it%20with%20state-of-the-art%0Amulti-objective%20optimizations.%20We%20perform%20our%20evaluation%20on%20two%20real-world%0Aproblems%20--%20airfoil%20design%20and%203D%20printing%20--%20showcasing%20the%20applicability%20and%0Aefficiency%20of%20our%20approach.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/an-on-ym-ous/lbn_mobo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.01095v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Batch%252C%2520Iteration-Efficient%2520Neural%2520Bayesian%2520Design%2520Optimization%26entry.906535625%3DNavid%2520Ansari%2520and%2520Alireza%2520Javanmardi%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Hans-Peter%2520Seidel%2520and%2520Vahid%2520Babaei%26entry.1292438233%3D%2520%2520Bayesian%2520optimization%2520%2528BO%2529%2520provides%2520a%2520powerful%2520framework%2520for%2520optimizing%250Ablack-box%252C%2520expensive-to-evaluate%2520functions.%2520It%2520is%2520therefore%2520an%2520attractive%2520tool%250Afor%2520engineering%2520design%2520problems%252C%2520typically%2520involving%2520multiple%2520objectives.%250AThanks%2520to%2520the%2520rapid%2520advances%2520in%2520fabrication%2520and%2520measurement%2520methods%2520as%2520well%2520as%250Aparallel%2520computing%2520infrastructure%252C%2520querying%2520many%2520design%2520problems%2520can%2520be%2520heavily%250Aparallelized.%2520This%2520class%2520of%2520problems%2520challenges%2520BO%2520with%2520an%2520unprecedented%2520setup%250Awhere%2520it%2520has%2520to%2520deal%2520with%2520very%2520large%2520batches%252C%2520shifting%2520its%2520focus%2520from%2520sample%250Aefficiency%2520to%2520iteration%2520efficiency.%2520We%2520present%2520a%2520novel%2520Bayesian%2520optimization%250Aframework%2520specifically%2520tailored%2520to%2520address%2520these%2520limitations.%2520Our%2520key%250Acontribution%2520is%2520a%2520highly%2520scalable%252C%2520sample-based%2520acquisition%2520function%2520that%250Aperforms%2520a%2520non-dominated%2520sorting%2520of%2520not%2520only%2520the%2520objectives%2520but%2520also%2520their%250Aassociated%2520uncertainty.%2520We%2520show%2520that%2520our%2520acquisition%2520function%2520in%2520combination%250Awith%2520different%2520Bayesian%2520neural%2520network%2520surrogates%2520is%2520effective%2520in%250Adata-intensive%2520environments%2520with%2520a%2520minimal%2520number%2520of%2520iterations.%2520We%2520demonstrate%250Athe%2520superiority%2520of%2520our%2520method%2520by%2520comparing%2520it%2520with%2520state-of-the-art%250Amulti-objective%2520optimizations.%2520We%2520perform%2520our%2520evaluation%2520on%2520two%2520real-world%250Aproblems%2520--%2520airfoil%2520design%2520and%25203D%2520printing%2520--%2520showcasing%2520the%2520applicability%2520and%250Aefficiency%2520of%2520our%2520approach.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/an-on-ym-ous/lbn_mobo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.01095v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Batch%2C%20Iteration-Efficient%20Neural%20Bayesian%20Design%20Optimization&entry.906535625=Navid%20Ansari%20and%20Alireza%20Javanmardi%20and%20Eyke%20H%C3%BCllermeier%20and%20Hans-Peter%20Seidel%20and%20Vahid%20Babaei&entry.1292438233=%20%20Bayesian%20optimization%20%28BO%29%20provides%20a%20powerful%20framework%20for%20optimizing%0Ablack-box%2C%20expensive-to-evaluate%20functions.%20It%20is%20therefore%20an%20attractive%20tool%0Afor%20engineering%20design%20problems%2C%20typically%20involving%20multiple%20objectives.%0AThanks%20to%20the%20rapid%20advances%20in%20fabrication%20and%20measurement%20methods%20as%20well%20as%0Aparallel%20computing%20infrastructure%2C%20querying%20many%20design%20problems%20can%20be%20heavily%0Aparallelized.%20This%20class%20of%20problems%20challenges%20BO%20with%20an%20unprecedented%20setup%0Awhere%20it%20has%20to%20deal%20with%20very%20large%20batches%2C%20shifting%20its%20focus%20from%20sample%0Aefficiency%20to%20iteration%20efficiency.%20We%20present%20a%20novel%20Bayesian%20optimization%0Aframework%20specifically%20tailored%20to%20address%20these%20limitations.%20Our%20key%0Acontribution%20is%20a%20highly%20scalable%2C%20sample-based%20acquisition%20function%20that%0Aperforms%20a%20non-dominated%20sorting%20of%20not%20only%20the%20objectives%20but%20also%20their%0Aassociated%20uncertainty.%20We%20show%20that%20our%20acquisition%20function%20in%20combination%0Awith%20different%20Bayesian%20neural%20network%20surrogates%20is%20effective%20in%0Adata-intensive%20environments%20with%20a%20minimal%20number%20of%20iterations.%20We%20demonstrate%0Athe%20superiority%20of%20our%20method%20by%20comparing%20it%20with%20state-of-the-art%0Amulti-objective%20optimizations.%20We%20perform%20our%20evaluation%20on%20two%20real-world%0Aproblems%20--%20airfoil%20design%20and%203D%20printing%20--%20showcasing%20the%20applicability%20and%0Aefficiency%20of%20our%20approach.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/an-on-ym-ous/lbn_mobo%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.01095v4&entry.124074799=Read"},
{"title": "VFLGAN-TS: Vertical Federated Learning-based Generative Adversarial\n  Networks for Publication of Vertically Partitioned Time-Series Data", "author": "Xun Yuan and Zilong Zhao and Prosanta Gope and Biplab Sikdar", "abstract": "  In the current artificial intelligence (AI) era, the scale and quality of the\ndataset play a crucial role in training a high-quality AI model. However, often\noriginal data cannot be shared due to privacy concerns and regulations. A\npotential solution is to release a synthetic dataset with a similar\ndistribution to the private dataset. Nevertheless, in some scenarios, the\nattributes required to train an AI model are distributed among different\nparties, and the parties cannot share the local data for synthetic data\nconstruction due to privacy regulations. In PETS 2024, we recently introduced\nthe first Vertical Federated Learning-based Generative Adversarial Network\n(VFLGAN) for publishing vertically partitioned static data. However, VFLGAN\ncannot effectively handle time-series data, presenting both temporal and\nattribute dimensions. In this article, we proposed VFLGAN-TS, which combines\nthe ideas of attribute discriminator and vertical federated learning to\ngenerate synthetic time-series data in the vertically partitioned scenario. The\nperformance of VFLGAN-TS is close to that of its counterpart, which is trained\nin a centralized manner and represents the upper limit for VFLGAN-TS. To\nfurther protect privacy, we apply a Gaussian mechanism to make VFLGAN-TS\nsatisfy an $(\\epsilon,\\delta)$-differential privacy. Besides, we develop an\nenhanced privacy auditing scheme to evaluate the potential privacy breach\nthrough the framework of VFLGAN-TS and synthetic datasets.\n", "link": "http://arxiv.org/abs/2409.03612v1", "date": "2024-09-05", "relevancy": 1.9794, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5087}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4916}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VFLGAN-TS%3A%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%0A%20%20Networks%20for%20Publication%20of%20Vertically%20Partitioned%20Time-Series%20Data&body=Title%3A%20VFLGAN-TS%3A%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%0A%20%20Networks%20for%20Publication%20of%20Vertically%20Partitioned%20Time-Series%20Data%0AAuthor%3A%20Xun%20Yuan%20and%20Zilong%20Zhao%20and%20Prosanta%20Gope%20and%20Biplab%20Sikdar%0AAbstract%3A%20%20%20In%20the%20current%20artificial%20intelligence%20%28AI%29%20era%2C%20the%20scale%20and%20quality%20of%20the%0Adataset%20play%20a%20crucial%20role%20in%20training%20a%20high-quality%20AI%20model.%20However%2C%20often%0Aoriginal%20data%20cannot%20be%20shared%20due%20to%20privacy%20concerns%20and%20regulations.%20A%0Apotential%20solution%20is%20to%20release%20a%20synthetic%20dataset%20with%20a%20similar%0Adistribution%20to%20the%20private%20dataset.%20Nevertheless%2C%20in%20some%20scenarios%2C%20the%0Aattributes%20required%20to%20train%20an%20AI%20model%20are%20distributed%20among%20different%0Aparties%2C%20and%20the%20parties%20cannot%20share%20the%20local%20data%20for%20synthetic%20data%0Aconstruction%20due%20to%20privacy%20regulations.%20In%20PETS%202024%2C%20we%20recently%20introduced%0Athe%20first%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%20Network%0A%28VFLGAN%29%20for%20publishing%20vertically%20partitioned%20static%20data.%20However%2C%20VFLGAN%0Acannot%20effectively%20handle%20time-series%20data%2C%20presenting%20both%20temporal%20and%0Aattribute%20dimensions.%20In%20this%20article%2C%20we%20proposed%20VFLGAN-TS%2C%20which%20combines%0Athe%20ideas%20of%20attribute%20discriminator%20and%20vertical%20federated%20learning%20to%0Agenerate%20synthetic%20time-series%20data%20in%20the%20vertically%20partitioned%20scenario.%20The%0Aperformance%20of%20VFLGAN-TS%20is%20close%20to%20that%20of%20its%20counterpart%2C%20which%20is%20trained%0Ain%20a%20centralized%20manner%20and%20represents%20the%20upper%20limit%20for%20VFLGAN-TS.%20To%0Afurther%20protect%20privacy%2C%20we%20apply%20a%20Gaussian%20mechanism%20to%20make%20VFLGAN-TS%0Asatisfy%20an%20%24%28%5Cepsilon%2C%5Cdelta%29%24-differential%20privacy.%20Besides%2C%20we%20develop%20an%0Aenhanced%20privacy%20auditing%20scheme%20to%20evaluate%20the%20potential%20privacy%20breach%0Athrough%20the%20framework%20of%20VFLGAN-TS%20and%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVFLGAN-TS%253A%2520Vertical%2520Federated%2520Learning-based%2520Generative%2520Adversarial%250A%2520%2520Networks%2520for%2520Publication%2520of%2520Vertically%2520Partitioned%2520Time-Series%2520Data%26entry.906535625%3DXun%2520Yuan%2520and%2520Zilong%2520Zhao%2520and%2520Prosanta%2520Gope%2520and%2520Biplab%2520Sikdar%26entry.1292438233%3D%2520%2520In%2520the%2520current%2520artificial%2520intelligence%2520%2528AI%2529%2520era%252C%2520the%2520scale%2520and%2520quality%2520of%2520the%250Adataset%2520play%2520a%2520crucial%2520role%2520in%2520training%2520a%2520high-quality%2520AI%2520model.%2520However%252C%2520often%250Aoriginal%2520data%2520cannot%2520be%2520shared%2520due%2520to%2520privacy%2520concerns%2520and%2520regulations.%2520A%250Apotential%2520solution%2520is%2520to%2520release%2520a%2520synthetic%2520dataset%2520with%2520a%2520similar%250Adistribution%2520to%2520the%2520private%2520dataset.%2520Nevertheless%252C%2520in%2520some%2520scenarios%252C%2520the%250Aattributes%2520required%2520to%2520train%2520an%2520AI%2520model%2520are%2520distributed%2520among%2520different%250Aparties%252C%2520and%2520the%2520parties%2520cannot%2520share%2520the%2520local%2520data%2520for%2520synthetic%2520data%250Aconstruction%2520due%2520to%2520privacy%2520regulations.%2520In%2520PETS%25202024%252C%2520we%2520recently%2520introduced%250Athe%2520first%2520Vertical%2520Federated%2520Learning-based%2520Generative%2520Adversarial%2520Network%250A%2528VFLGAN%2529%2520for%2520publishing%2520vertically%2520partitioned%2520static%2520data.%2520However%252C%2520VFLGAN%250Acannot%2520effectively%2520handle%2520time-series%2520data%252C%2520presenting%2520both%2520temporal%2520and%250Aattribute%2520dimensions.%2520In%2520this%2520article%252C%2520we%2520proposed%2520VFLGAN-TS%252C%2520which%2520combines%250Athe%2520ideas%2520of%2520attribute%2520discriminator%2520and%2520vertical%2520federated%2520learning%2520to%250Agenerate%2520synthetic%2520time-series%2520data%2520in%2520the%2520vertically%2520partitioned%2520scenario.%2520The%250Aperformance%2520of%2520VFLGAN-TS%2520is%2520close%2520to%2520that%2520of%2520its%2520counterpart%252C%2520which%2520is%2520trained%250Ain%2520a%2520centralized%2520manner%2520and%2520represents%2520the%2520upper%2520limit%2520for%2520VFLGAN-TS.%2520To%250Afurther%2520protect%2520privacy%252C%2520we%2520apply%2520a%2520Gaussian%2520mechanism%2520to%2520make%2520VFLGAN-TS%250Asatisfy%2520an%2520%2524%2528%255Cepsilon%252C%255Cdelta%2529%2524-differential%2520privacy.%2520Besides%252C%2520we%2520develop%2520an%250Aenhanced%2520privacy%2520auditing%2520scheme%2520to%2520evaluate%2520the%2520potential%2520privacy%2520breach%250Athrough%2520the%2520framework%2520of%2520VFLGAN-TS%2520and%2520synthetic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFLGAN-TS%3A%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%0A%20%20Networks%20for%20Publication%20of%20Vertically%20Partitioned%20Time-Series%20Data&entry.906535625=Xun%20Yuan%20and%20Zilong%20Zhao%20and%20Prosanta%20Gope%20and%20Biplab%20Sikdar&entry.1292438233=%20%20In%20the%20current%20artificial%20intelligence%20%28AI%29%20era%2C%20the%20scale%20and%20quality%20of%20the%0Adataset%20play%20a%20crucial%20role%20in%20training%20a%20high-quality%20AI%20model.%20However%2C%20often%0Aoriginal%20data%20cannot%20be%20shared%20due%20to%20privacy%20concerns%20and%20regulations.%20A%0Apotential%20solution%20is%20to%20release%20a%20synthetic%20dataset%20with%20a%20similar%0Adistribution%20to%20the%20private%20dataset.%20Nevertheless%2C%20in%20some%20scenarios%2C%20the%0Aattributes%20required%20to%20train%20an%20AI%20model%20are%20distributed%20among%20different%0Aparties%2C%20and%20the%20parties%20cannot%20share%20the%20local%20data%20for%20synthetic%20data%0Aconstruction%20due%20to%20privacy%20regulations.%20In%20PETS%202024%2C%20we%20recently%20introduced%0Athe%20first%20Vertical%20Federated%20Learning-based%20Generative%20Adversarial%20Network%0A%28VFLGAN%29%20for%20publishing%20vertically%20partitioned%20static%20data.%20However%2C%20VFLGAN%0Acannot%20effectively%20handle%20time-series%20data%2C%20presenting%20both%20temporal%20and%0Aattribute%20dimensions.%20In%20this%20article%2C%20we%20proposed%20VFLGAN-TS%2C%20which%20combines%0Athe%20ideas%20of%20attribute%20discriminator%20and%20vertical%20federated%20learning%20to%0Agenerate%20synthetic%20time-series%20data%20in%20the%20vertically%20partitioned%20scenario.%20The%0Aperformance%20of%20VFLGAN-TS%20is%20close%20to%20that%20of%20its%20counterpart%2C%20which%20is%20trained%0Ain%20a%20centralized%20manner%20and%20represents%20the%20upper%20limit%20for%20VFLGAN-TS.%20To%0Afurther%20protect%20privacy%2C%20we%20apply%20a%20Gaussian%20mechanism%20to%20make%20VFLGAN-TS%0Asatisfy%20an%20%24%28%5Cepsilon%2C%5Cdelta%29%24-differential%20privacy.%20Besides%2C%20we%20develop%20an%0Aenhanced%20privacy%20auditing%20scheme%20to%20evaluate%20the%20potential%20privacy%20breach%0Athrough%20the%20framework%20of%20VFLGAN-TS%20and%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03612v1&entry.124074799=Read"},
{"title": "A Graph-based Adversarial Imitation Learning Framework for Reliable &\n  Realtime Fleet Scheduling in Urban Air Mobility", "author": "Prithvi Poddar and Steve Paul and Souma Chowdhury", "abstract": "  The advent of Urban Air Mobility (UAM) presents the scope for a\ntransformative shift in the domain of urban transportation. However, its\nwidespread adoption and economic viability depends in part on the ability to\noptimally schedule the fleet of aircraft across vertiports in a UAM network,\nunder uncertainties attributed to airspace congestion, changing weather\nconditions, and varying demands. This paper presents a comprehensive\noptimization formulation of the fleet scheduling problem, while also\nidentifying the need for alternate solution approaches, since directly solving\nthe resulting integer nonlinear programming problem is computationally\nprohibitive for daily fleet scheduling. Previous work has shown the\neffectiveness of using (graph) reinforcement learning (RL) approaches to train\nreal-time executable policy models for fleet scheduling. However, such policies\ncan often be brittle on out-of-distribution scenarios or edge cases. Moreover,\ntraining performance also deteriorates as the complexity (e.g., number of\nconstraints) of the problem increases. To address these issues, this paper\npresents an imitation learning approach where the RL-based policy exploits\nexpert demonstrations yielded by solving the exact optimization using a Genetic\nAlgorithm. The policy model comprises Graph Neural Network (GNN) based encoders\nthat embed the space of vertiports and aircraft, Transformer networks to encode\ndemand, passenger fare, and transport cost profiles, and a Multi-head attention\n(MHA) based decoder. Expert demonstrations are used through the Generative\nAdversarial Imitation Learning (GAIL) algorithm. Interfaced with a UAM\nsimulation environment involving 8 vertiports and 40 aircrafts, in terms of the\ndaily profits earned reward, the new imitative approach achieves better mean\nperformance and remarkable improvement in the case of unseen worst-case\nscenarios, compared to pure RL results.\n", "link": "http://arxiv.org/abs/2407.12113v2", "date": "2024-09-05", "relevancy": 1.9774, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4939}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Graph-based%20Adversarial%20Imitation%20Learning%20Framework%20for%20Reliable%20%26%0A%20%20Realtime%20Fleet%20Scheduling%20in%20Urban%20Air%20Mobility&body=Title%3A%20A%20Graph-based%20Adversarial%20Imitation%20Learning%20Framework%20for%20Reliable%20%26%0A%20%20Realtime%20Fleet%20Scheduling%20in%20Urban%20Air%20Mobility%0AAuthor%3A%20Prithvi%20Poddar%20and%20Steve%20Paul%20and%20Souma%20Chowdhury%0AAbstract%3A%20%20%20The%20advent%20of%20Urban%20Air%20Mobility%20%28UAM%29%20presents%20the%20scope%20for%20a%0Atransformative%20shift%20in%20the%20domain%20of%20urban%20transportation.%20However%2C%20its%0Awidespread%20adoption%20and%20economic%20viability%20depends%20in%20part%20on%20the%20ability%20to%0Aoptimally%20schedule%20the%20fleet%20of%20aircraft%20across%20vertiports%20in%20a%20UAM%20network%2C%0Aunder%20uncertainties%20attributed%20to%20airspace%20congestion%2C%20changing%20weather%0Aconditions%2C%20and%20varying%20demands.%20This%20paper%20presents%20a%20comprehensive%0Aoptimization%20formulation%20of%20the%20fleet%20scheduling%20problem%2C%20while%20also%0Aidentifying%20the%20need%20for%20alternate%20solution%20approaches%2C%20since%20directly%20solving%0Athe%20resulting%20integer%20nonlinear%20programming%20problem%20is%20computationally%0Aprohibitive%20for%20daily%20fleet%20scheduling.%20Previous%20work%20has%20shown%20the%0Aeffectiveness%20of%20using%20%28graph%29%20reinforcement%20learning%20%28RL%29%20approaches%20to%20train%0Areal-time%20executable%20policy%20models%20for%20fleet%20scheduling.%20However%2C%20such%20policies%0Acan%20often%20be%20brittle%20on%20out-of-distribution%20scenarios%20or%20edge%20cases.%20Moreover%2C%0Atraining%20performance%20also%20deteriorates%20as%20the%20complexity%20%28e.g.%2C%20number%20of%0Aconstraints%29%20of%20the%20problem%20increases.%20To%20address%20these%20issues%2C%20this%20paper%0Apresents%20an%20imitation%20learning%20approach%20where%20the%20RL-based%20policy%20exploits%0Aexpert%20demonstrations%20yielded%20by%20solving%20the%20exact%20optimization%20using%20a%20Genetic%0AAlgorithm.%20The%20policy%20model%20comprises%20Graph%20Neural%20Network%20%28GNN%29%20based%20encoders%0Athat%20embed%20the%20space%20of%20vertiports%20and%20aircraft%2C%20Transformer%20networks%20to%20encode%0Ademand%2C%20passenger%20fare%2C%20and%20transport%20cost%20profiles%2C%20and%20a%20Multi-head%20attention%0A%28MHA%29%20based%20decoder.%20Expert%20demonstrations%20are%20used%20through%20the%20Generative%0AAdversarial%20Imitation%20Learning%20%28GAIL%29%20algorithm.%20Interfaced%20with%20a%20UAM%0Asimulation%20environment%20involving%208%20vertiports%20and%2040%20aircrafts%2C%20in%20terms%20of%20the%0Adaily%20profits%20earned%20reward%2C%20the%20new%20imitative%20approach%20achieves%20better%20mean%0Aperformance%20and%20remarkable%20improvement%20in%20the%20case%20of%20unseen%20worst-case%0Ascenarios%2C%20compared%20to%20pure%20RL%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Graph-based%2520Adversarial%2520Imitation%2520Learning%2520Framework%2520for%2520Reliable%2520%2526%250A%2520%2520Realtime%2520Fleet%2520Scheduling%2520in%2520Urban%2520Air%2520Mobility%26entry.906535625%3DPrithvi%2520Poddar%2520and%2520Steve%2520Paul%2520and%2520Souma%2520Chowdhury%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Urban%2520Air%2520Mobility%2520%2528UAM%2529%2520presents%2520the%2520scope%2520for%2520a%250Atransformative%2520shift%2520in%2520the%2520domain%2520of%2520urban%2520transportation.%2520However%252C%2520its%250Awidespread%2520adoption%2520and%2520economic%2520viability%2520depends%2520in%2520part%2520on%2520the%2520ability%2520to%250Aoptimally%2520schedule%2520the%2520fleet%2520of%2520aircraft%2520across%2520vertiports%2520in%2520a%2520UAM%2520network%252C%250Aunder%2520uncertainties%2520attributed%2520to%2520airspace%2520congestion%252C%2520changing%2520weather%250Aconditions%252C%2520and%2520varying%2520demands.%2520This%2520paper%2520presents%2520a%2520comprehensive%250Aoptimization%2520formulation%2520of%2520the%2520fleet%2520scheduling%2520problem%252C%2520while%2520also%250Aidentifying%2520the%2520need%2520for%2520alternate%2520solution%2520approaches%252C%2520since%2520directly%2520solving%250Athe%2520resulting%2520integer%2520nonlinear%2520programming%2520problem%2520is%2520computationally%250Aprohibitive%2520for%2520daily%2520fleet%2520scheduling.%2520Previous%2520work%2520has%2520shown%2520the%250Aeffectiveness%2520of%2520using%2520%2528graph%2529%2520reinforcement%2520learning%2520%2528RL%2529%2520approaches%2520to%2520train%250Areal-time%2520executable%2520policy%2520models%2520for%2520fleet%2520scheduling.%2520However%252C%2520such%2520policies%250Acan%2520often%2520be%2520brittle%2520on%2520out-of-distribution%2520scenarios%2520or%2520edge%2520cases.%2520Moreover%252C%250Atraining%2520performance%2520also%2520deteriorates%2520as%2520the%2520complexity%2520%2528e.g.%252C%2520number%2520of%250Aconstraints%2529%2520of%2520the%2520problem%2520increases.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%250Apresents%2520an%2520imitation%2520learning%2520approach%2520where%2520the%2520RL-based%2520policy%2520exploits%250Aexpert%2520demonstrations%2520yielded%2520by%2520solving%2520the%2520exact%2520optimization%2520using%2520a%2520Genetic%250AAlgorithm.%2520The%2520policy%2520model%2520comprises%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520based%2520encoders%250Athat%2520embed%2520the%2520space%2520of%2520vertiports%2520and%2520aircraft%252C%2520Transformer%2520networks%2520to%2520encode%250Ademand%252C%2520passenger%2520fare%252C%2520and%2520transport%2520cost%2520profiles%252C%2520and%2520a%2520Multi-head%2520attention%250A%2528MHA%2529%2520based%2520decoder.%2520Expert%2520demonstrations%2520are%2520used%2520through%2520the%2520Generative%250AAdversarial%2520Imitation%2520Learning%2520%2528GAIL%2529%2520algorithm.%2520Interfaced%2520with%2520a%2520UAM%250Asimulation%2520environment%2520involving%25208%2520vertiports%2520and%252040%2520aircrafts%252C%2520in%2520terms%2520of%2520the%250Adaily%2520profits%2520earned%2520reward%252C%2520the%2520new%2520imitative%2520approach%2520achieves%2520better%2520mean%250Aperformance%2520and%2520remarkable%2520improvement%2520in%2520the%2520case%2520of%2520unseen%2520worst-case%250Ascenarios%252C%2520compared%2520to%2520pure%2520RL%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Graph-based%20Adversarial%20Imitation%20Learning%20Framework%20for%20Reliable%20%26%0A%20%20Realtime%20Fleet%20Scheduling%20in%20Urban%20Air%20Mobility&entry.906535625=Prithvi%20Poddar%20and%20Steve%20Paul%20and%20Souma%20Chowdhury&entry.1292438233=%20%20The%20advent%20of%20Urban%20Air%20Mobility%20%28UAM%29%20presents%20the%20scope%20for%20a%0Atransformative%20shift%20in%20the%20domain%20of%20urban%20transportation.%20However%2C%20its%0Awidespread%20adoption%20and%20economic%20viability%20depends%20in%20part%20on%20the%20ability%20to%0Aoptimally%20schedule%20the%20fleet%20of%20aircraft%20across%20vertiports%20in%20a%20UAM%20network%2C%0Aunder%20uncertainties%20attributed%20to%20airspace%20congestion%2C%20changing%20weather%0Aconditions%2C%20and%20varying%20demands.%20This%20paper%20presents%20a%20comprehensive%0Aoptimization%20formulation%20of%20the%20fleet%20scheduling%20problem%2C%20while%20also%0Aidentifying%20the%20need%20for%20alternate%20solution%20approaches%2C%20since%20directly%20solving%0Athe%20resulting%20integer%20nonlinear%20programming%20problem%20is%20computationally%0Aprohibitive%20for%20daily%20fleet%20scheduling.%20Previous%20work%20has%20shown%20the%0Aeffectiveness%20of%20using%20%28graph%29%20reinforcement%20learning%20%28RL%29%20approaches%20to%20train%0Areal-time%20executable%20policy%20models%20for%20fleet%20scheduling.%20However%2C%20such%20policies%0Acan%20often%20be%20brittle%20on%20out-of-distribution%20scenarios%20or%20edge%20cases.%20Moreover%2C%0Atraining%20performance%20also%20deteriorates%20as%20the%20complexity%20%28e.g.%2C%20number%20of%0Aconstraints%29%20of%20the%20problem%20increases.%20To%20address%20these%20issues%2C%20this%20paper%0Apresents%20an%20imitation%20learning%20approach%20where%20the%20RL-based%20policy%20exploits%0Aexpert%20demonstrations%20yielded%20by%20solving%20the%20exact%20optimization%20using%20a%20Genetic%0AAlgorithm.%20The%20policy%20model%20comprises%20Graph%20Neural%20Network%20%28GNN%29%20based%20encoders%0Athat%20embed%20the%20space%20of%20vertiports%20and%20aircraft%2C%20Transformer%20networks%20to%20encode%0Ademand%2C%20passenger%20fare%2C%20and%20transport%20cost%20profiles%2C%20and%20a%20Multi-head%20attention%0A%28MHA%29%20based%20decoder.%20Expert%20demonstrations%20are%20used%20through%20the%20Generative%0AAdversarial%20Imitation%20Learning%20%28GAIL%29%20algorithm.%20Interfaced%20with%20a%20UAM%0Asimulation%20environment%20involving%208%20vertiports%20and%2040%20aircrafts%2C%20in%20terms%20of%20the%0Adaily%20profits%20earned%20reward%2C%20the%20new%20imitative%20approach%20achieves%20better%20mean%0Aperformance%20and%20remarkable%20improvement%20in%20the%20case%20of%20unseen%20worst-case%0Ascenarios%2C%20compared%20to%20pure%20RL%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12113v2&entry.124074799=Read"},
{"title": "CyclicFL: A Cyclic Model Pre-Training Approach to Efficient Federated\n  Learning", "author": "Pengyu Zhang and Yingbo Zhou and Ming Hu and Xian Wei and Mingsong Chen", "abstract": "  Federated learning (FL) has been proposed to enable distributed learning on\nArtificial Intelligence Internet of Things (AIoT) devices with guarantees of\nhigh-level data privacy. Since random initial models in FL can easily result in\nunregulated Stochastic Gradient Descent (SGD) processes, existing FL methods\ngreatly suffer from both slow convergence and poor accuracy, especially in\nnon-IID scenarios. To address this problem, we propose a novel method named\nCyclicFL, which can quickly derive effective initial models to guide the SGD\nprocesses, thus improving the overall FL training performance. We formally\nanalyze the significance of data consistency between the pre-training and\ntraining stages of CyclicFL, showing the limited Lipschitzness of loss for the\npre-trained models by CyclicFL. Moreover, we systematically prove that our\nmethod can achieve faster convergence speed under various convexity\nassumptions. Unlike traditional centralized pre-training methods that require\npublic proxy data, CyclicFL pre-trains initial models on selected AIoT devices\ncyclically without exposing their local data. Therefore, they can be easily\nintegrated into any security-critical FL methods. Comprehensive experimental\nresults show that CyclicFL can not only improve the maximum classification\naccuracy by up to $14.11\\%$ but also significantly accelerate the overall FL\ntraining process.\n", "link": "http://arxiv.org/abs/2301.12193v2", "date": "2024-09-05", "relevancy": 1.9536, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4877}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CyclicFL%3A%20A%20Cyclic%20Model%20Pre-Training%20Approach%20to%20Efficient%20Federated%0A%20%20Learning&body=Title%3A%20CyclicFL%3A%20A%20Cyclic%20Model%20Pre-Training%20Approach%20to%20Efficient%20Federated%0A%20%20Learning%0AAuthor%3A%20Pengyu%20Zhang%20and%20Yingbo%20Zhou%20and%20Ming%20Hu%20and%20Xian%20Wei%20and%20Mingsong%20Chen%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20been%20proposed%20to%20enable%20distributed%20learning%20on%0AArtificial%20Intelligence%20Internet%20of%20Things%20%28AIoT%29%20devices%20with%20guarantees%20of%0Ahigh-level%20data%20privacy.%20Since%20random%20initial%20models%20in%20FL%20can%20easily%20result%20in%0Aunregulated%20Stochastic%20Gradient%20Descent%20%28SGD%29%20processes%2C%20existing%20FL%20methods%0Agreatly%20suffer%20from%20both%20slow%20convergence%20and%20poor%20accuracy%2C%20especially%20in%0Anon-IID%20scenarios.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20method%20named%0ACyclicFL%2C%20which%20can%20quickly%20derive%20effective%20initial%20models%20to%20guide%20the%20SGD%0Aprocesses%2C%20thus%20improving%20the%20overall%20FL%20training%20performance.%20We%20formally%0Aanalyze%20the%20significance%20of%20data%20consistency%20between%20the%20pre-training%20and%0Atraining%20stages%20of%20CyclicFL%2C%20showing%20the%20limited%20Lipschitzness%20of%20loss%20for%20the%0Apre-trained%20models%20by%20CyclicFL.%20Moreover%2C%20we%20systematically%20prove%20that%20our%0Amethod%20can%20achieve%20faster%20convergence%20speed%20under%20various%20convexity%0Aassumptions.%20Unlike%20traditional%20centralized%20pre-training%20methods%20that%20require%0Apublic%20proxy%20data%2C%20CyclicFL%20pre-trains%20initial%20models%20on%20selected%20AIoT%20devices%0Acyclically%20without%20exposing%20their%20local%20data.%20Therefore%2C%20they%20can%20be%20easily%0Aintegrated%20into%20any%20security-critical%20FL%20methods.%20Comprehensive%20experimental%0Aresults%20show%20that%20CyclicFL%20can%20not%20only%20improve%20the%20maximum%20classification%0Aaccuracy%20by%20up%20to%20%2414.11%5C%25%24%20but%20also%20significantly%20accelerate%20the%20overall%20FL%0Atraining%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.12193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyclicFL%253A%2520A%2520Cyclic%2520Model%2520Pre-Training%2520Approach%2520to%2520Efficient%2520Federated%250A%2520%2520Learning%26entry.906535625%3DPengyu%2520Zhang%2520and%2520Yingbo%2520Zhou%2520and%2520Ming%2520Hu%2520and%2520Xian%2520Wei%2520and%2520Mingsong%2520Chen%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520been%2520proposed%2520to%2520enable%2520distributed%2520learning%2520on%250AArtificial%2520Intelligence%2520Internet%2520of%2520Things%2520%2528AIoT%2529%2520devices%2520with%2520guarantees%2520of%250Ahigh-level%2520data%2520privacy.%2520Since%2520random%2520initial%2520models%2520in%2520FL%2520can%2520easily%2520result%2520in%250Aunregulated%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%2520processes%252C%2520existing%2520FL%2520methods%250Agreatly%2520suffer%2520from%2520both%2520slow%2520convergence%2520and%2520poor%2520accuracy%252C%2520especially%2520in%250Anon-IID%2520scenarios.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520method%2520named%250ACyclicFL%252C%2520which%2520can%2520quickly%2520derive%2520effective%2520initial%2520models%2520to%2520guide%2520the%2520SGD%250Aprocesses%252C%2520thus%2520improving%2520the%2520overall%2520FL%2520training%2520performance.%2520We%2520formally%250Aanalyze%2520the%2520significance%2520of%2520data%2520consistency%2520between%2520the%2520pre-training%2520and%250Atraining%2520stages%2520of%2520CyclicFL%252C%2520showing%2520the%2520limited%2520Lipschitzness%2520of%2520loss%2520for%2520the%250Apre-trained%2520models%2520by%2520CyclicFL.%2520Moreover%252C%2520we%2520systematically%2520prove%2520that%2520our%250Amethod%2520can%2520achieve%2520faster%2520convergence%2520speed%2520under%2520various%2520convexity%250Aassumptions.%2520Unlike%2520traditional%2520centralized%2520pre-training%2520methods%2520that%2520require%250Apublic%2520proxy%2520data%252C%2520CyclicFL%2520pre-trains%2520initial%2520models%2520on%2520selected%2520AIoT%2520devices%250Acyclically%2520without%2520exposing%2520their%2520local%2520data.%2520Therefore%252C%2520they%2520can%2520be%2520easily%250Aintegrated%2520into%2520any%2520security-critical%2520FL%2520methods.%2520Comprehensive%2520experimental%250Aresults%2520show%2520that%2520CyclicFL%2520can%2520not%2520only%2520improve%2520the%2520maximum%2520classification%250Aaccuracy%2520by%2520up%2520to%2520%252414.11%255C%2525%2524%2520but%2520also%2520significantly%2520accelerate%2520the%2520overall%2520FL%250Atraining%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.12193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CyclicFL%3A%20A%20Cyclic%20Model%20Pre-Training%20Approach%20to%20Efficient%20Federated%0A%20%20Learning&entry.906535625=Pengyu%20Zhang%20and%20Yingbo%20Zhou%20and%20Ming%20Hu%20and%20Xian%20Wei%20and%20Mingsong%20Chen&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20been%20proposed%20to%20enable%20distributed%20learning%20on%0AArtificial%20Intelligence%20Internet%20of%20Things%20%28AIoT%29%20devices%20with%20guarantees%20of%0Ahigh-level%20data%20privacy.%20Since%20random%20initial%20models%20in%20FL%20can%20easily%20result%20in%0Aunregulated%20Stochastic%20Gradient%20Descent%20%28SGD%29%20processes%2C%20existing%20FL%20methods%0Agreatly%20suffer%20from%20both%20slow%20convergence%20and%20poor%20accuracy%2C%20especially%20in%0Anon-IID%20scenarios.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20method%20named%0ACyclicFL%2C%20which%20can%20quickly%20derive%20effective%20initial%20models%20to%20guide%20the%20SGD%0Aprocesses%2C%20thus%20improving%20the%20overall%20FL%20training%20performance.%20We%20formally%0Aanalyze%20the%20significance%20of%20data%20consistency%20between%20the%20pre-training%20and%0Atraining%20stages%20of%20CyclicFL%2C%20showing%20the%20limited%20Lipschitzness%20of%20loss%20for%20the%0Apre-trained%20models%20by%20CyclicFL.%20Moreover%2C%20we%20systematically%20prove%20that%20our%0Amethod%20can%20achieve%20faster%20convergence%20speed%20under%20various%20convexity%0Aassumptions.%20Unlike%20traditional%20centralized%20pre-training%20methods%20that%20require%0Apublic%20proxy%20data%2C%20CyclicFL%20pre-trains%20initial%20models%20on%20selected%20AIoT%20devices%0Acyclically%20without%20exposing%20their%20local%20data.%20Therefore%2C%20they%20can%20be%20easily%0Aintegrated%20into%20any%20security-critical%20FL%20methods.%20Comprehensive%20experimental%0Aresults%20show%20that%20CyclicFL%20can%20not%20only%20improve%20the%20maximum%20classification%0Aaccuracy%20by%20up%20to%20%2414.11%5C%25%24%20but%20also%20significantly%20accelerate%20the%20overall%20FL%0Atraining%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.12193v2&entry.124074799=Read"},
{"title": "Large Scale Training of Graph Neural Networks for Optimal Markov-Chain\n  Partitioning Using the Kemeny Constant", "author": "Sam Alexander Martino and Jo\u00e3o Morado and Chenghao Li and Zhenghao Lu and Edina Rosta", "abstract": "  Traditional clustering algorithms often struggle to capture the complex\nrelationships within graphs and generalise to arbitrary clustering criteria.\nThe emergence of graph neural networks (GNNs) as a powerful framework for\nlearning representations of graph data provides new approaches to solving the\nproblem. Previous work has shown GNNs to be capable of proposing partitionings\nusing a variety of criteria, however, these approaches have not yet been\nextended to work on Markov chains or kinetic networks. These arise frequently\nin the study of molecular systems and are of particular interest to the\nbiochemical modelling community. In this work, we propose several GNN-based\narchitectures to tackle the graph partitioning problem for Markov Chains\ndescribed as kinetic networks. This approach aims to minimize how much a\nproposed partitioning changes the Kemeny constant. We propose using an\nencoder-decoder architecture and show how simple GraphSAGE-based GNNs with\nlinear layers can outperform much larger and more expressive attention-based\nmodels in this context. As a proof of concept, we first demonstrate the\nmethod's ability to cluster randomly connected graphs. We also use a linear\nchain architecture corresponding to a 1D free energy profile as our kinetic\nnetwork. Subsequently, we demonstrate the effectiveness of our method through\nexperiments on a data set derived from molecular dynamics. We compare the\nperformance of our method to other partitioning techniques such as PCCA+. We\nexplore the importance of feature and hyperparameter selection and propose a\ngeneral strategy for large-scale parallel training of GNNs for discovering\noptimal graph partitionings.\n", "link": "http://arxiv.org/abs/2312.14847v3", "date": "2024-09-05", "relevancy": 1.9522, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4906}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4893}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Scale%20Training%20of%20Graph%20Neural%20Networks%20for%20Optimal%20Markov-Chain%0A%20%20Partitioning%20Using%20the%20Kemeny%20Constant&body=Title%3A%20Large%20Scale%20Training%20of%20Graph%20Neural%20Networks%20for%20Optimal%20Markov-Chain%0A%20%20Partitioning%20Using%20the%20Kemeny%20Constant%0AAuthor%3A%20Sam%20Alexander%20Martino%20and%20Jo%C3%A3o%20Morado%20and%20Chenghao%20Li%20and%20Zhenghao%20Lu%20and%20Edina%20Rosta%0AAbstract%3A%20%20%20Traditional%20clustering%20algorithms%20often%20struggle%20to%20capture%20the%20complex%0Arelationships%20within%20graphs%20and%20generalise%20to%20arbitrary%20clustering%20criteria.%0AThe%20emergence%20of%20graph%20neural%20networks%20%28GNNs%29%20as%20a%20powerful%20framework%20for%0Alearning%20representations%20of%20graph%20data%20provides%20new%20approaches%20to%20solving%20the%0Aproblem.%20Previous%20work%20has%20shown%20GNNs%20to%20be%20capable%20of%20proposing%20partitionings%0Ausing%20a%20variety%20of%20criteria%2C%20however%2C%20these%20approaches%20have%20not%20yet%20been%0Aextended%20to%20work%20on%20Markov%20chains%20or%20kinetic%20networks.%20These%20arise%20frequently%0Ain%20the%20study%20of%20molecular%20systems%20and%20are%20of%20particular%20interest%20to%20the%0Abiochemical%20modelling%20community.%20In%20this%20work%2C%20we%20propose%20several%20GNN-based%0Aarchitectures%20to%20tackle%20the%20graph%20partitioning%20problem%20for%20Markov%20Chains%0Adescribed%20as%20kinetic%20networks.%20This%20approach%20aims%20to%20minimize%20how%20much%20a%0Aproposed%20partitioning%20changes%20the%20Kemeny%20constant.%20We%20propose%20using%20an%0Aencoder-decoder%20architecture%20and%20show%20how%20simple%20GraphSAGE-based%20GNNs%20with%0Alinear%20layers%20can%20outperform%20much%20larger%20and%20more%20expressive%20attention-based%0Amodels%20in%20this%20context.%20As%20a%20proof%20of%20concept%2C%20we%20first%20demonstrate%20the%0Amethod%27s%20ability%20to%20cluster%20randomly%20connected%20graphs.%20We%20also%20use%20a%20linear%0Achain%20architecture%20corresponding%20to%20a%201D%20free%20energy%20profile%20as%20our%20kinetic%0Anetwork.%20Subsequently%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%0Aexperiments%20on%20a%20data%20set%20derived%20from%20molecular%20dynamics.%20We%20compare%20the%0Aperformance%20of%20our%20method%20to%20other%20partitioning%20techniques%20such%20as%20PCCA%2B.%20We%0Aexplore%20the%20importance%20of%20feature%20and%20hyperparameter%20selection%20and%20propose%20a%0Ageneral%20strategy%20for%20large-scale%20parallel%20training%20of%20GNNs%20for%20discovering%0Aoptimal%20graph%20partitionings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14847v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Scale%2520Training%2520of%2520Graph%2520Neural%2520Networks%2520for%2520Optimal%2520Markov-Chain%250A%2520%2520Partitioning%2520Using%2520the%2520Kemeny%2520Constant%26entry.906535625%3DSam%2520Alexander%2520Martino%2520and%2520Jo%25C3%25A3o%2520Morado%2520and%2520Chenghao%2520Li%2520and%2520Zhenghao%2520Lu%2520and%2520Edina%2520Rosta%26entry.1292438233%3D%2520%2520Traditional%2520clustering%2520algorithms%2520often%2520struggle%2520to%2520capture%2520the%2520complex%250Arelationships%2520within%2520graphs%2520and%2520generalise%2520to%2520arbitrary%2520clustering%2520criteria.%250AThe%2520emergence%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520as%2520a%2520powerful%2520framework%2520for%250Alearning%2520representations%2520of%2520graph%2520data%2520provides%2520new%2520approaches%2520to%2520solving%2520the%250Aproblem.%2520Previous%2520work%2520has%2520shown%2520GNNs%2520to%2520be%2520capable%2520of%2520proposing%2520partitionings%250Ausing%2520a%2520variety%2520of%2520criteria%252C%2520however%252C%2520these%2520approaches%2520have%2520not%2520yet%2520been%250Aextended%2520to%2520work%2520on%2520Markov%2520chains%2520or%2520kinetic%2520networks.%2520These%2520arise%2520frequently%250Ain%2520the%2520study%2520of%2520molecular%2520systems%2520and%2520are%2520of%2520particular%2520interest%2520to%2520the%250Abiochemical%2520modelling%2520community.%2520In%2520this%2520work%252C%2520we%2520propose%2520several%2520GNN-based%250Aarchitectures%2520to%2520tackle%2520the%2520graph%2520partitioning%2520problem%2520for%2520Markov%2520Chains%250Adescribed%2520as%2520kinetic%2520networks.%2520This%2520approach%2520aims%2520to%2520minimize%2520how%2520much%2520a%250Aproposed%2520partitioning%2520changes%2520the%2520Kemeny%2520constant.%2520We%2520propose%2520using%2520an%250Aencoder-decoder%2520architecture%2520and%2520show%2520how%2520simple%2520GraphSAGE-based%2520GNNs%2520with%250Alinear%2520layers%2520can%2520outperform%2520much%2520larger%2520and%2520more%2520expressive%2520attention-based%250Amodels%2520in%2520this%2520context.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520first%2520demonstrate%2520the%250Amethod%2527s%2520ability%2520to%2520cluster%2520randomly%2520connected%2520graphs.%2520We%2520also%2520use%2520a%2520linear%250Achain%2520architecture%2520corresponding%2520to%2520a%25201D%2520free%2520energy%2520profile%2520as%2520our%2520kinetic%250Anetwork.%2520Subsequently%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520through%250Aexperiments%2520on%2520a%2520data%2520set%2520derived%2520from%2520molecular%2520dynamics.%2520We%2520compare%2520the%250Aperformance%2520of%2520our%2520method%2520to%2520other%2520partitioning%2520techniques%2520such%2520as%2520PCCA%252B.%2520We%250Aexplore%2520the%2520importance%2520of%2520feature%2520and%2520hyperparameter%2520selection%2520and%2520propose%2520a%250Ageneral%2520strategy%2520for%2520large-scale%2520parallel%2520training%2520of%2520GNNs%2520for%2520discovering%250Aoptimal%2520graph%2520partitionings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14847v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Scale%20Training%20of%20Graph%20Neural%20Networks%20for%20Optimal%20Markov-Chain%0A%20%20Partitioning%20Using%20the%20Kemeny%20Constant&entry.906535625=Sam%20Alexander%20Martino%20and%20Jo%C3%A3o%20Morado%20and%20Chenghao%20Li%20and%20Zhenghao%20Lu%20and%20Edina%20Rosta&entry.1292438233=%20%20Traditional%20clustering%20algorithms%20often%20struggle%20to%20capture%20the%20complex%0Arelationships%20within%20graphs%20and%20generalise%20to%20arbitrary%20clustering%20criteria.%0AThe%20emergence%20of%20graph%20neural%20networks%20%28GNNs%29%20as%20a%20powerful%20framework%20for%0Alearning%20representations%20of%20graph%20data%20provides%20new%20approaches%20to%20solving%20the%0Aproblem.%20Previous%20work%20has%20shown%20GNNs%20to%20be%20capable%20of%20proposing%20partitionings%0Ausing%20a%20variety%20of%20criteria%2C%20however%2C%20these%20approaches%20have%20not%20yet%20been%0Aextended%20to%20work%20on%20Markov%20chains%20or%20kinetic%20networks.%20These%20arise%20frequently%0Ain%20the%20study%20of%20molecular%20systems%20and%20are%20of%20particular%20interest%20to%20the%0Abiochemical%20modelling%20community.%20In%20this%20work%2C%20we%20propose%20several%20GNN-based%0Aarchitectures%20to%20tackle%20the%20graph%20partitioning%20problem%20for%20Markov%20Chains%0Adescribed%20as%20kinetic%20networks.%20This%20approach%20aims%20to%20minimize%20how%20much%20a%0Aproposed%20partitioning%20changes%20the%20Kemeny%20constant.%20We%20propose%20using%20an%0Aencoder-decoder%20architecture%20and%20show%20how%20simple%20GraphSAGE-based%20GNNs%20with%0Alinear%20layers%20can%20outperform%20much%20larger%20and%20more%20expressive%20attention-based%0Amodels%20in%20this%20context.%20As%20a%20proof%20of%20concept%2C%20we%20first%20demonstrate%20the%0Amethod%27s%20ability%20to%20cluster%20randomly%20connected%20graphs.%20We%20also%20use%20a%20linear%0Achain%20architecture%20corresponding%20to%20a%201D%20free%20energy%20profile%20as%20our%20kinetic%0Anetwork.%20Subsequently%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%0Aexperiments%20on%20a%20data%20set%20derived%20from%20molecular%20dynamics.%20We%20compare%20the%0Aperformance%20of%20our%20method%20to%20other%20partitioning%20techniques%20such%20as%20PCCA%2B.%20We%0Aexplore%20the%20importance%20of%20feature%20and%20hyperparameter%20selection%20and%20propose%20a%0Ageneral%20strategy%20for%20large-scale%20parallel%20training%20of%20GNNs%20for%20discovering%0Aoptimal%20graph%20partitionings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14847v3&entry.124074799=Read"},
{"title": "Finite Sample Frequency Domain Identification", "author": "Anastasios Tsiamis and Mohamed Abdalmoaty and Roy S. Smith and John Lygeros", "abstract": "  We study non-parametric frequency-domain system identification from a\nfinite-sample perspective. We assume an open loop scenario where the excitation\ninput is periodic and consider the Empirical Transfer Function Estimate (ETFE),\nwhere the goal is to estimate the frequency response at certain desired\n(evenly-spaced) frequencies, given input-output samples. We show that under\nsub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE\nestimates are concentrated around the true values. The error rate is of the\norder of\n$\\mathcal{O}((d_{\\mathrm{u}}+\\sqrt{d_{\\mathrm{u}}d_{\\mathrm{y}}})\\sqrt{M/N_{\\mathrm{tot}}})$,\nwhere $N_{\\mathrm{tot}}$ is the total number of samples, $M$ is the number of\ndesired frequencies, and $d_{\\mathrm{u}},\\,d_{\\mathrm{y}}$ are the dimensions\nof the input and output signals respectively. This rate remains valid for\ngeneral irrational transfer functions and does not require a finite order\nstate-space representation. By tuning $M$, we obtain a\n$N_{\\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency\nresponse over all frequencies in the $ \\mathcal{H}_{\\infty}$ norm. Our result\ndraws upon an extension of the Hanson-Wright inequality to semi-infinite\nmatrices. We study the finite-sample behavior of ETFE in simulations.\n", "link": "http://arxiv.org/abs/2404.01100v2", "date": "2024-09-05", "relevancy": 1.9491, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4157}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3784}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite%20Sample%20Frequency%20Domain%20Identification&body=Title%3A%20Finite%20Sample%20Frequency%20Domain%20Identification%0AAuthor%3A%20Anastasios%20Tsiamis%20and%20Mohamed%20Abdalmoaty%20and%20Roy%20S.%20Smith%20and%20John%20Lygeros%0AAbstract%3A%20%20%20We%20study%20non-parametric%20frequency-domain%20system%20identification%20from%20a%0Afinite-sample%20perspective.%20We%20assume%20an%20open%20loop%20scenario%20where%20the%20excitation%0Ainput%20is%20periodic%20and%20consider%20the%20Empirical%20Transfer%20Function%20Estimate%20%28ETFE%29%2C%0Awhere%20the%20goal%20is%20to%20estimate%20the%20frequency%20response%20at%20certain%20desired%0A%28evenly-spaced%29%20frequencies%2C%20given%20input-output%20samples.%20We%20show%20that%20under%0Asub-Gaussian%20colored%20noise%20%28in%20time-domain%29%20and%20stability%20assumptions%2C%20the%20ETFE%0Aestimates%20are%20concentrated%20around%20the%20true%20values.%20The%20error%20rate%20is%20of%20the%0Aorder%20of%0A%24%5Cmathcal%7BO%7D%28%28d_%7B%5Cmathrm%7Bu%7D%7D%2B%5Csqrt%7Bd_%7B%5Cmathrm%7Bu%7D%7Dd_%7B%5Cmathrm%7By%7D%7D%7D%29%5Csqrt%7BM/N_%7B%5Cmathrm%7Btot%7D%7D%7D%29%24%2C%0Awhere%20%24N_%7B%5Cmathrm%7Btot%7D%7D%24%20is%20the%20total%20number%20of%20samples%2C%20%24M%24%20is%20the%20number%20of%0Adesired%20frequencies%2C%20and%20%24d_%7B%5Cmathrm%7Bu%7D%7D%2C%5C%2Cd_%7B%5Cmathrm%7By%7D%7D%24%20are%20the%20dimensions%0Aof%20the%20input%20and%20output%20signals%20respectively.%20This%20rate%20remains%20valid%20for%0Ageneral%20irrational%20transfer%20functions%20and%20does%20not%20require%20a%20finite%20order%0Astate-space%20representation.%20By%20tuning%20%24M%24%2C%20we%20obtain%20a%0A%24N_%7B%5Cmathrm%7Btot%7D%7D%5E%7B-1/3%7D%24%20finite-sample%20rate%20for%20learning%20the%20frequency%0Aresponse%20over%20all%20frequencies%20in%20the%20%24%20%5Cmathcal%7BH%7D_%7B%5Cinfty%7D%24%20norm.%20Our%20result%0Adraws%20upon%20an%20extension%20of%20the%20Hanson-Wright%20inequality%20to%20semi-infinite%0Amatrices.%20We%20study%20the%20finite-sample%20behavior%20of%20ETFE%20in%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite%2520Sample%2520Frequency%2520Domain%2520Identification%26entry.906535625%3DAnastasios%2520Tsiamis%2520and%2520Mohamed%2520Abdalmoaty%2520and%2520Roy%2520S.%2520Smith%2520and%2520John%2520Lygeros%26entry.1292438233%3D%2520%2520We%2520study%2520non-parametric%2520frequency-domain%2520system%2520identification%2520from%2520a%250Afinite-sample%2520perspective.%2520We%2520assume%2520an%2520open%2520loop%2520scenario%2520where%2520the%2520excitation%250Ainput%2520is%2520periodic%2520and%2520consider%2520the%2520Empirical%2520Transfer%2520Function%2520Estimate%2520%2528ETFE%2529%252C%250Awhere%2520the%2520goal%2520is%2520to%2520estimate%2520the%2520frequency%2520response%2520at%2520certain%2520desired%250A%2528evenly-spaced%2529%2520frequencies%252C%2520given%2520input-output%2520samples.%2520We%2520show%2520that%2520under%250Asub-Gaussian%2520colored%2520noise%2520%2528in%2520time-domain%2529%2520and%2520stability%2520assumptions%252C%2520the%2520ETFE%250Aestimates%2520are%2520concentrated%2520around%2520the%2520true%2520values.%2520The%2520error%2520rate%2520is%2520of%2520the%250Aorder%2520of%250A%2524%255Cmathcal%257BO%257D%2528%2528d_%257B%255Cmathrm%257Bu%257D%257D%252B%255Csqrt%257Bd_%257B%255Cmathrm%257Bu%257D%257Dd_%257B%255Cmathrm%257By%257D%257D%257D%2529%255Csqrt%257BM/N_%257B%255Cmathrm%257Btot%257D%257D%257D%2529%2524%252C%250Awhere%2520%2524N_%257B%255Cmathrm%257Btot%257D%257D%2524%2520is%2520the%2520total%2520number%2520of%2520samples%252C%2520%2524M%2524%2520is%2520the%2520number%2520of%250Adesired%2520frequencies%252C%2520and%2520%2524d_%257B%255Cmathrm%257Bu%257D%257D%252C%255C%252Cd_%257B%255Cmathrm%257By%257D%257D%2524%2520are%2520the%2520dimensions%250Aof%2520the%2520input%2520and%2520output%2520signals%2520respectively.%2520This%2520rate%2520remains%2520valid%2520for%250Ageneral%2520irrational%2520transfer%2520functions%2520and%2520does%2520not%2520require%2520a%2520finite%2520order%250Astate-space%2520representation.%2520By%2520tuning%2520%2524M%2524%252C%2520we%2520obtain%2520a%250A%2524N_%257B%255Cmathrm%257Btot%257D%257D%255E%257B-1/3%257D%2524%2520finite-sample%2520rate%2520for%2520learning%2520the%2520frequency%250Aresponse%2520over%2520all%2520frequencies%2520in%2520the%2520%2524%2520%255Cmathcal%257BH%257D_%257B%255Cinfty%257D%2524%2520norm.%2520Our%2520result%250Adraws%2520upon%2520an%2520extension%2520of%2520the%2520Hanson-Wright%2520inequality%2520to%2520semi-infinite%250Amatrices.%2520We%2520study%2520the%2520finite-sample%2520behavior%2520of%2520ETFE%2520in%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite%20Sample%20Frequency%20Domain%20Identification&entry.906535625=Anastasios%20Tsiamis%20and%20Mohamed%20Abdalmoaty%20and%20Roy%20S.%20Smith%20and%20John%20Lygeros&entry.1292438233=%20%20We%20study%20non-parametric%20frequency-domain%20system%20identification%20from%20a%0Afinite-sample%20perspective.%20We%20assume%20an%20open%20loop%20scenario%20where%20the%20excitation%0Ainput%20is%20periodic%20and%20consider%20the%20Empirical%20Transfer%20Function%20Estimate%20%28ETFE%29%2C%0Awhere%20the%20goal%20is%20to%20estimate%20the%20frequency%20response%20at%20certain%20desired%0A%28evenly-spaced%29%20frequencies%2C%20given%20input-output%20samples.%20We%20show%20that%20under%0Asub-Gaussian%20colored%20noise%20%28in%20time-domain%29%20and%20stability%20assumptions%2C%20the%20ETFE%0Aestimates%20are%20concentrated%20around%20the%20true%20values.%20The%20error%20rate%20is%20of%20the%0Aorder%20of%0A%24%5Cmathcal%7BO%7D%28%28d_%7B%5Cmathrm%7Bu%7D%7D%2B%5Csqrt%7Bd_%7B%5Cmathrm%7Bu%7D%7Dd_%7B%5Cmathrm%7By%7D%7D%7D%29%5Csqrt%7BM/N_%7B%5Cmathrm%7Btot%7D%7D%7D%29%24%2C%0Awhere%20%24N_%7B%5Cmathrm%7Btot%7D%7D%24%20is%20the%20total%20number%20of%20samples%2C%20%24M%24%20is%20the%20number%20of%0Adesired%20frequencies%2C%20and%20%24d_%7B%5Cmathrm%7Bu%7D%7D%2C%5C%2Cd_%7B%5Cmathrm%7By%7D%7D%24%20are%20the%20dimensions%0Aof%20the%20input%20and%20output%20signals%20respectively.%20This%20rate%20remains%20valid%20for%0Ageneral%20irrational%20transfer%20functions%20and%20does%20not%20require%20a%20finite%20order%0Astate-space%20representation.%20By%20tuning%20%24M%24%2C%20we%20obtain%20a%0A%24N_%7B%5Cmathrm%7Btot%7D%7D%5E%7B-1/3%7D%24%20finite-sample%20rate%20for%20learning%20the%20frequency%0Aresponse%20over%20all%20frequencies%20in%20the%20%24%20%5Cmathcal%7BH%7D_%7B%5Cinfty%7D%24%20norm.%20Our%20result%0Adraws%20upon%20an%20extension%20of%20the%20Hanson-Wright%20inequality%20to%20semi-infinite%0Amatrices.%20We%20study%20the%20finite-sample%20behavior%20of%20ETFE%20in%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01100v2&entry.124074799=Read"},
{"title": "Dynamics of Supervised and Reinforcement Learning in the Non-Linear\n  Perceptron", "author": "Christian Schmid and James M. Murray", "abstract": "  The ability of a brain or a neural network to efficiently learn depends\ncrucially on both the task structure and the learning rule. Previous works have\nanalyzed the dynamical equations describing learning in the relatively\nsimplified context of the perceptron under assumptions of a student-teacher\nframework or a linearized output. While these assumptions have facilitated\ntheoretical understanding, they have precluded a detailed understanding of the\nroles of the nonlinearity and input-data distribution in determining the\nlearning dynamics, limiting the applicability of the theories to real\nbiological or artificial neural networks. Here, we use a stochastic-process\napproach to derive flow equations describing learning, applying this framework\nto the case of a nonlinear perceptron performing binary classification. We\ncharacterize the effects of the learning rule (supervised or reinforcement\nlearning, SL/RL) and input-data distribution on the perceptron's learning curve\nand the forgetting curve as subsequent tasks are learned. In particular, we\nfind that the input-data noise differently affects the learning speed under SL\nvs. RL, as well as determines how quickly learning of a task is overwritten by\nsubsequent learning. Additionally, we verify our approach with real data using\nthe MNIST dataset. This approach points a way toward analyzing learning\ndynamics for more-complex circuit architectures.\n", "link": "http://arxiv.org/abs/2409.03749v1", "date": "2024-09-05", "relevancy": 1.9244, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamics%20of%20Supervised%20and%20Reinforcement%20Learning%20in%20the%20Non-Linear%0A%20%20Perceptron&body=Title%3A%20Dynamics%20of%20Supervised%20and%20Reinforcement%20Learning%20in%20the%20Non-Linear%0A%20%20Perceptron%0AAuthor%3A%20Christian%20Schmid%20and%20James%20M.%20Murray%0AAbstract%3A%20%20%20The%20ability%20of%20a%20brain%20or%20a%20neural%20network%20to%20efficiently%20learn%20depends%0Acrucially%20on%20both%20the%20task%20structure%20and%20the%20learning%20rule.%20Previous%20works%20have%0Aanalyzed%20the%20dynamical%20equations%20describing%20learning%20in%20the%20relatively%0Asimplified%20context%20of%20the%20perceptron%20under%20assumptions%20of%20a%20student-teacher%0Aframework%20or%20a%20linearized%20output.%20While%20these%20assumptions%20have%20facilitated%0Atheoretical%20understanding%2C%20they%20have%20precluded%20a%20detailed%20understanding%20of%20the%0Aroles%20of%20the%20nonlinearity%20and%20input-data%20distribution%20in%20determining%20the%0Alearning%20dynamics%2C%20limiting%20the%20applicability%20of%20the%20theories%20to%20real%0Abiological%20or%20artificial%20neural%20networks.%20Here%2C%20we%20use%20a%20stochastic-process%0Aapproach%20to%20derive%20flow%20equations%20describing%20learning%2C%20applying%20this%20framework%0Ato%20the%20case%20of%20a%20nonlinear%20perceptron%20performing%20binary%20classification.%20We%0Acharacterize%20the%20effects%20of%20the%20learning%20rule%20%28supervised%20or%20reinforcement%0Alearning%2C%20SL/RL%29%20and%20input-data%20distribution%20on%20the%20perceptron%27s%20learning%20curve%0Aand%20the%20forgetting%20curve%20as%20subsequent%20tasks%20are%20learned.%20In%20particular%2C%20we%0Afind%20that%20the%20input-data%20noise%20differently%20affects%20the%20learning%20speed%20under%20SL%0Avs.%20RL%2C%20as%20well%20as%20determines%20how%20quickly%20learning%20of%20a%20task%20is%20overwritten%20by%0Asubsequent%20learning.%20Additionally%2C%20we%20verify%20our%20approach%20with%20real%20data%20using%0Athe%20MNIST%20dataset.%20This%20approach%20points%20a%20way%20toward%20analyzing%20learning%0Adynamics%20for%20more-complex%20circuit%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamics%2520of%2520Supervised%2520and%2520Reinforcement%2520Learning%2520in%2520the%2520Non-Linear%250A%2520%2520Perceptron%26entry.906535625%3DChristian%2520Schmid%2520and%2520James%2520M.%2520Murray%26entry.1292438233%3D%2520%2520The%2520ability%2520of%2520a%2520brain%2520or%2520a%2520neural%2520network%2520to%2520efficiently%2520learn%2520depends%250Acrucially%2520on%2520both%2520the%2520task%2520structure%2520and%2520the%2520learning%2520rule.%2520Previous%2520works%2520have%250Aanalyzed%2520the%2520dynamical%2520equations%2520describing%2520learning%2520in%2520the%2520relatively%250Asimplified%2520context%2520of%2520the%2520perceptron%2520under%2520assumptions%2520of%2520a%2520student-teacher%250Aframework%2520or%2520a%2520linearized%2520output.%2520While%2520these%2520assumptions%2520have%2520facilitated%250Atheoretical%2520understanding%252C%2520they%2520have%2520precluded%2520a%2520detailed%2520understanding%2520of%2520the%250Aroles%2520of%2520the%2520nonlinearity%2520and%2520input-data%2520distribution%2520in%2520determining%2520the%250Alearning%2520dynamics%252C%2520limiting%2520the%2520applicability%2520of%2520the%2520theories%2520to%2520real%250Abiological%2520or%2520artificial%2520neural%2520networks.%2520Here%252C%2520we%2520use%2520a%2520stochastic-process%250Aapproach%2520to%2520derive%2520flow%2520equations%2520describing%2520learning%252C%2520applying%2520this%2520framework%250Ato%2520the%2520case%2520of%2520a%2520nonlinear%2520perceptron%2520performing%2520binary%2520classification.%2520We%250Acharacterize%2520the%2520effects%2520of%2520the%2520learning%2520rule%2520%2528supervised%2520or%2520reinforcement%250Alearning%252C%2520SL/RL%2529%2520and%2520input-data%2520distribution%2520on%2520the%2520perceptron%2527s%2520learning%2520curve%250Aand%2520the%2520forgetting%2520curve%2520as%2520subsequent%2520tasks%2520are%2520learned.%2520In%2520particular%252C%2520we%250Afind%2520that%2520the%2520input-data%2520noise%2520differently%2520affects%2520the%2520learning%2520speed%2520under%2520SL%250Avs.%2520RL%252C%2520as%2520well%2520as%2520determines%2520how%2520quickly%2520learning%2520of%2520a%2520task%2520is%2520overwritten%2520by%250Asubsequent%2520learning.%2520Additionally%252C%2520we%2520verify%2520our%2520approach%2520with%2520real%2520data%2520using%250Athe%2520MNIST%2520dataset.%2520This%2520approach%2520points%2520a%2520way%2520toward%2520analyzing%2520learning%250Adynamics%2520for%2520more-complex%2520circuit%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics%20of%20Supervised%20and%20Reinforcement%20Learning%20in%20the%20Non-Linear%0A%20%20Perceptron&entry.906535625=Christian%20Schmid%20and%20James%20M.%20Murray&entry.1292438233=%20%20The%20ability%20of%20a%20brain%20or%20a%20neural%20network%20to%20efficiently%20learn%20depends%0Acrucially%20on%20both%20the%20task%20structure%20and%20the%20learning%20rule.%20Previous%20works%20have%0Aanalyzed%20the%20dynamical%20equations%20describing%20learning%20in%20the%20relatively%0Asimplified%20context%20of%20the%20perceptron%20under%20assumptions%20of%20a%20student-teacher%0Aframework%20or%20a%20linearized%20output.%20While%20these%20assumptions%20have%20facilitated%0Atheoretical%20understanding%2C%20they%20have%20precluded%20a%20detailed%20understanding%20of%20the%0Aroles%20of%20the%20nonlinearity%20and%20input-data%20distribution%20in%20determining%20the%0Alearning%20dynamics%2C%20limiting%20the%20applicability%20of%20the%20theories%20to%20real%0Abiological%20or%20artificial%20neural%20networks.%20Here%2C%20we%20use%20a%20stochastic-process%0Aapproach%20to%20derive%20flow%20equations%20describing%20learning%2C%20applying%20this%20framework%0Ato%20the%20case%20of%20a%20nonlinear%20perceptron%20performing%20binary%20classification.%20We%0Acharacterize%20the%20effects%20of%20the%20learning%20rule%20%28supervised%20or%20reinforcement%0Alearning%2C%20SL/RL%29%20and%20input-data%20distribution%20on%20the%20perceptron%27s%20learning%20curve%0Aand%20the%20forgetting%20curve%20as%20subsequent%20tasks%20are%20learned.%20In%20particular%2C%20we%0Afind%20that%20the%20input-data%20noise%20differently%20affects%20the%20learning%20speed%20under%20SL%0Avs.%20RL%2C%20as%20well%20as%20determines%20how%20quickly%20learning%20of%20a%20task%20is%20overwritten%20by%0Asubsequent%20learning.%20Additionally%2C%20we%20verify%20our%20approach%20with%20real%20data%20using%0Athe%20MNIST%20dataset.%20This%20approach%20points%20a%20way%20toward%20analyzing%20learning%0Adynamics%20for%20more-complex%20circuit%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03749v1&entry.124074799=Read"},
{"title": "A review on the use of large language models as virtual tutors", "author": "Silvia Garc\u00eda-M\u00e9ndez and Francisco de Arriba-P\u00e9rez and Mar\u00eda del Carmen Somoza-L\u00f3pez", "abstract": "  Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.\n", "link": "http://arxiv.org/abs/2405.11983v2", "date": "2024-09-05", "relevancy": 1.9222, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4993}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20review%20on%20the%20use%20of%20large%20language%20models%20as%20virtual%20tutors&body=Title%3A%20A%20review%20on%20the%20use%20of%20large%20language%20models%20as%20virtual%20tutors%0AAuthor%3A%20Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20Francisco%20de%20Arriba-P%C3%A9rez%20and%20Mar%C3%ADa%20del%20Carmen%20Somoza-L%C3%B3pez%0AAbstract%3A%20%20%20Transformer%20architectures%20contribute%20to%20managing%20long-term%20dependencies%20for%0ANatural%20Language%20Processing%2C%20representing%20one%20of%20the%20most%20recent%20changes%20in%20the%0Afield.%20These%20architectures%20are%20the%20basis%20of%20the%20innovative%2C%20cutting-edge%20Large%0ALanguage%20Models%20%28LLMs%29%20that%20have%20produced%20a%20huge%20buzz%20in%20several%20fields%20and%0Aindustrial%20sectors%2C%20among%20the%20ones%20education%20stands%20out.%20Accordingly%2C%20these%0Agenerative%20Artificial%20Intelligence-based%20solutions%20have%20directed%20the%20change%20in%0Atechniques%20and%20the%20evolution%20in%20educational%20methods%20and%20contents%2C%20along%20with%0Anetwork%20infrastructure%2C%20towards%20high-quality%20learning.%20Given%20the%20popularity%20of%0ALLMs%2C%20this%20review%20seeks%20to%20provide%20a%20comprehensive%20overview%20of%20those%20solutions%0Adesigned%20specifically%20to%20generate%20and%20evaluate%20educational%20materials%20and%20which%0Ainvolve%20students%20and%20teachers%20in%20their%20design%20or%20experimental%20plan.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20review%20of%20educational%20applications%20%28e.g.%2C%0Astudent%20assessment%29%20of%20LLMs.%20As%20expected%2C%20the%20most%20common%20role%20of%20these%20systems%0Ais%20as%20virtual%20tutors%20for%20automatic%20question%20generation.%20Moreover%2C%20the%20most%0Apopular%20models%20are%20GTP-3%20and%20BERT.%20However%2C%20due%20to%20the%20continuous%20launch%20of%20new%0Agenerative%20models%2C%20new%20works%20are%20expected%20to%20be%20published%20shortly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520review%2520on%2520the%2520use%2520of%2520large%2520language%2520models%2520as%2520virtual%2520tutors%26entry.906535625%3DSilvia%2520Garc%25C3%25ADa-M%25C3%25A9ndez%2520and%2520Francisco%2520de%2520Arriba-P%25C3%25A9rez%2520and%2520Mar%25C3%25ADa%2520del%2520Carmen%2520Somoza-L%25C3%25B3pez%26entry.1292438233%3D%2520%2520Transformer%2520architectures%2520contribute%2520to%2520managing%2520long-term%2520dependencies%2520for%250ANatural%2520Language%2520Processing%252C%2520representing%2520one%2520of%2520the%2520most%2520recent%2520changes%2520in%2520the%250Afield.%2520These%2520architectures%2520are%2520the%2520basis%2520of%2520the%2520innovative%252C%2520cutting-edge%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520that%2520have%2520produced%2520a%2520huge%2520buzz%2520in%2520several%2520fields%2520and%250Aindustrial%2520sectors%252C%2520among%2520the%2520ones%2520education%2520stands%2520out.%2520Accordingly%252C%2520these%250Agenerative%2520Artificial%2520Intelligence-based%2520solutions%2520have%2520directed%2520the%2520change%2520in%250Atechniques%2520and%2520the%2520evolution%2520in%2520educational%2520methods%2520and%2520contents%252C%2520along%2520with%250Anetwork%2520infrastructure%252C%2520towards%2520high-quality%2520learning.%2520Given%2520the%2520popularity%2520of%250ALLMs%252C%2520this%2520review%2520seeks%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520those%2520solutions%250Adesigned%2520specifically%2520to%2520generate%2520and%2520evaluate%2520educational%2520materials%2520and%2520which%250Ainvolve%2520students%2520and%2520teachers%2520in%2520their%2520design%2520or%2520experimental%2520plan.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520review%2520of%2520educational%2520applications%2520%2528e.g.%252C%250Astudent%2520assessment%2529%2520of%2520LLMs.%2520As%2520expected%252C%2520the%2520most%2520common%2520role%2520of%2520these%2520systems%250Ais%2520as%2520virtual%2520tutors%2520for%2520automatic%2520question%2520generation.%2520Moreover%252C%2520the%2520most%250Apopular%2520models%2520are%2520GTP-3%2520and%2520BERT.%2520However%252C%2520due%2520to%2520the%2520continuous%2520launch%2520of%2520new%250Agenerative%2520models%252C%2520new%2520works%2520are%2520expected%2520to%2520be%2520published%2520shortly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20review%20on%20the%20use%20of%20large%20language%20models%20as%20virtual%20tutors&entry.906535625=Silvia%20Garc%C3%ADa-M%C3%A9ndez%20and%20Francisco%20de%20Arriba-P%C3%A9rez%20and%20Mar%C3%ADa%20del%20Carmen%20Somoza-L%C3%B3pez&entry.1292438233=%20%20Transformer%20architectures%20contribute%20to%20managing%20long-term%20dependencies%20for%0ANatural%20Language%20Processing%2C%20representing%20one%20of%20the%20most%20recent%20changes%20in%20the%0Afield.%20These%20architectures%20are%20the%20basis%20of%20the%20innovative%2C%20cutting-edge%20Large%0ALanguage%20Models%20%28LLMs%29%20that%20have%20produced%20a%20huge%20buzz%20in%20several%20fields%20and%0Aindustrial%20sectors%2C%20among%20the%20ones%20education%20stands%20out.%20Accordingly%2C%20these%0Agenerative%20Artificial%20Intelligence-based%20solutions%20have%20directed%20the%20change%20in%0Atechniques%20and%20the%20evolution%20in%20educational%20methods%20and%20contents%2C%20along%20with%0Anetwork%20infrastructure%2C%20towards%20high-quality%20learning.%20Given%20the%20popularity%20of%0ALLMs%2C%20this%20review%20seeks%20to%20provide%20a%20comprehensive%20overview%20of%20those%20solutions%0Adesigned%20specifically%20to%20generate%20and%20evaluate%20educational%20materials%20and%20which%0Ainvolve%20students%20and%20teachers%20in%20their%20design%20or%20experimental%20plan.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20review%20of%20educational%20applications%20%28e.g.%2C%0Astudent%20assessment%29%20of%20LLMs.%20As%20expected%2C%20the%20most%20common%20role%20of%20these%20systems%0Ais%20as%20virtual%20tutors%20for%20automatic%20question%20generation.%20Moreover%2C%20the%20most%0Apopular%20models%20are%20GTP-3%20and%20BERT.%20However%2C%20due%20to%20the%20continuous%20launch%20of%20new%0Agenerative%20models%2C%20new%20works%20are%20expected%20to%20be%20published%20shortly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11983v2&entry.124074799=Read"},
{"title": "ScreenMark: Watermarking Arbitrary Visual Content on Screen", "author": "Xiujian Liang and Gaozhi Liu and Yichao Si and Xiaoxiao Hu and Zhenxing Qian and Xinpeng Zhang", "abstract": "  Digital watermarking has demonstrated its effectiveness in protecting\nmultimedia content. However, existing watermarking are predominantly tailored\nfor specific media types, rendering them less effective for the protection of\ncontent displayed on computer screens, which is often multimodal and dynamic.\nVisual Screen Content (VSC), is particularly susceptible to theft and leakage\nvia screenshots, a vulnerability that current watermarking methods fail to\nadequately address.To tackle these challenges, we propose ScreenMark, a robust\nand practical watermarking method designed specifically for arbitrary VSC\nprotection. ScreenMark utilizes a three-stage progressive watermarking\nframework. Initially, inspired by diffusion principles, we initialize the\nmutual transformation between regular watermark information and irregular\nwatermark patterns. Subsequently, these patterns are integrated with screen\ncontent using a pre-multiplication alpha blending technique, supported by a\npre-trained screen decoder for accurate watermark retrieval. The progressively\ncomplex distorter enhances the robustness of the watermark in real-world\nscreenshot scenarios. Finally, the model undergoes fine-tuning guided by a\njoint-level distorter to ensure optimal performance.To validate the\neffectiveness of ScreenMark, we compiled a dataset comprising 100,000\nscreenshots from various devices and resolutions. Extensive experiments across\ndifferent datasets confirm the method's superior robustness, imperceptibility,\nand practical applicability.\n", "link": "http://arxiv.org/abs/2409.03487v1", "date": "2024-09-05", "relevancy": 1.9204, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5037}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4785}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScreenMark%3A%20Watermarking%20Arbitrary%20Visual%20Content%20on%20Screen&body=Title%3A%20ScreenMark%3A%20Watermarking%20Arbitrary%20Visual%20Content%20on%20Screen%0AAuthor%3A%20Xiujian%20Liang%20and%20Gaozhi%20Liu%20and%20Yichao%20Si%20and%20Xiaoxiao%20Hu%20and%20Zhenxing%20Qian%20and%20Xinpeng%20Zhang%0AAbstract%3A%20%20%20Digital%20watermarking%20has%20demonstrated%20its%20effectiveness%20in%20protecting%0Amultimedia%20content.%20However%2C%20existing%20watermarking%20are%20predominantly%20tailored%0Afor%20specific%20media%20types%2C%20rendering%20them%20less%20effective%20for%20the%20protection%20of%0Acontent%20displayed%20on%20computer%20screens%2C%20which%20is%20often%20multimodal%20and%20dynamic.%0AVisual%20Screen%20Content%20%28VSC%29%2C%20is%20particularly%20susceptible%20to%20theft%20and%20leakage%0Avia%20screenshots%2C%20a%20vulnerability%20that%20current%20watermarking%20methods%20fail%20to%0Aadequately%20address.To%20tackle%20these%20challenges%2C%20we%20propose%20ScreenMark%2C%20a%20robust%0Aand%20practical%20watermarking%20method%20designed%20specifically%20for%20arbitrary%20VSC%0Aprotection.%20ScreenMark%20utilizes%20a%20three-stage%20progressive%20watermarking%0Aframework.%20Initially%2C%20inspired%20by%20diffusion%20principles%2C%20we%20initialize%20the%0Amutual%20transformation%20between%20regular%20watermark%20information%20and%20irregular%0Awatermark%20patterns.%20Subsequently%2C%20these%20patterns%20are%20integrated%20with%20screen%0Acontent%20using%20a%20pre-multiplication%20alpha%20blending%20technique%2C%20supported%20by%20a%0Apre-trained%20screen%20decoder%20for%20accurate%20watermark%20retrieval.%20The%20progressively%0Acomplex%20distorter%20enhances%20the%20robustness%20of%20the%20watermark%20in%20real-world%0Ascreenshot%20scenarios.%20Finally%2C%20the%20model%20undergoes%20fine-tuning%20guided%20by%20a%0Ajoint-level%20distorter%20to%20ensure%20optimal%20performance.To%20validate%20the%0Aeffectiveness%20of%20ScreenMark%2C%20we%20compiled%20a%20dataset%20comprising%20100%2C000%0Ascreenshots%20from%20various%20devices%20and%20resolutions.%20Extensive%20experiments%20across%0Adifferent%20datasets%20confirm%20the%20method%27s%20superior%20robustness%2C%20imperceptibility%2C%0Aand%20practical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScreenMark%253A%2520Watermarking%2520Arbitrary%2520Visual%2520Content%2520on%2520Screen%26entry.906535625%3DXiujian%2520Liang%2520and%2520Gaozhi%2520Liu%2520and%2520Yichao%2520Si%2520and%2520Xiaoxiao%2520Hu%2520and%2520Zhenxing%2520Qian%2520and%2520Xinpeng%2520Zhang%26entry.1292438233%3D%2520%2520Digital%2520watermarking%2520has%2520demonstrated%2520its%2520effectiveness%2520in%2520protecting%250Amultimedia%2520content.%2520However%252C%2520existing%2520watermarking%2520are%2520predominantly%2520tailored%250Afor%2520specific%2520media%2520types%252C%2520rendering%2520them%2520less%2520effective%2520for%2520the%2520protection%2520of%250Acontent%2520displayed%2520on%2520computer%2520screens%252C%2520which%2520is%2520often%2520multimodal%2520and%2520dynamic.%250AVisual%2520Screen%2520Content%2520%2528VSC%2529%252C%2520is%2520particularly%2520susceptible%2520to%2520theft%2520and%2520leakage%250Avia%2520screenshots%252C%2520a%2520vulnerability%2520that%2520current%2520watermarking%2520methods%2520fail%2520to%250Aadequately%2520address.To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520ScreenMark%252C%2520a%2520robust%250Aand%2520practical%2520watermarking%2520method%2520designed%2520specifically%2520for%2520arbitrary%2520VSC%250Aprotection.%2520ScreenMark%2520utilizes%2520a%2520three-stage%2520progressive%2520watermarking%250Aframework.%2520Initially%252C%2520inspired%2520by%2520diffusion%2520principles%252C%2520we%2520initialize%2520the%250Amutual%2520transformation%2520between%2520regular%2520watermark%2520information%2520and%2520irregular%250Awatermark%2520patterns.%2520Subsequently%252C%2520these%2520patterns%2520are%2520integrated%2520with%2520screen%250Acontent%2520using%2520a%2520pre-multiplication%2520alpha%2520blending%2520technique%252C%2520supported%2520by%2520a%250Apre-trained%2520screen%2520decoder%2520for%2520accurate%2520watermark%2520retrieval.%2520The%2520progressively%250Acomplex%2520distorter%2520enhances%2520the%2520robustness%2520of%2520the%2520watermark%2520in%2520real-world%250Ascreenshot%2520scenarios.%2520Finally%252C%2520the%2520model%2520undergoes%2520fine-tuning%2520guided%2520by%2520a%250Ajoint-level%2520distorter%2520to%2520ensure%2520optimal%2520performance.To%2520validate%2520the%250Aeffectiveness%2520of%2520ScreenMark%252C%2520we%2520compiled%2520a%2520dataset%2520comprising%2520100%252C000%250Ascreenshots%2520from%2520various%2520devices%2520and%2520resolutions.%2520Extensive%2520experiments%2520across%250Adifferent%2520datasets%2520confirm%2520the%2520method%2527s%2520superior%2520robustness%252C%2520imperceptibility%252C%250Aand%2520practical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScreenMark%3A%20Watermarking%20Arbitrary%20Visual%20Content%20on%20Screen&entry.906535625=Xiujian%20Liang%20and%20Gaozhi%20Liu%20and%20Yichao%20Si%20and%20Xiaoxiao%20Hu%20and%20Zhenxing%20Qian%20and%20Xinpeng%20Zhang&entry.1292438233=%20%20Digital%20watermarking%20has%20demonstrated%20its%20effectiveness%20in%20protecting%0Amultimedia%20content.%20However%2C%20existing%20watermarking%20are%20predominantly%20tailored%0Afor%20specific%20media%20types%2C%20rendering%20them%20less%20effective%20for%20the%20protection%20of%0Acontent%20displayed%20on%20computer%20screens%2C%20which%20is%20often%20multimodal%20and%20dynamic.%0AVisual%20Screen%20Content%20%28VSC%29%2C%20is%20particularly%20susceptible%20to%20theft%20and%20leakage%0Avia%20screenshots%2C%20a%20vulnerability%20that%20current%20watermarking%20methods%20fail%20to%0Aadequately%20address.To%20tackle%20these%20challenges%2C%20we%20propose%20ScreenMark%2C%20a%20robust%0Aand%20practical%20watermarking%20method%20designed%20specifically%20for%20arbitrary%20VSC%0Aprotection.%20ScreenMark%20utilizes%20a%20three-stage%20progressive%20watermarking%0Aframework.%20Initially%2C%20inspired%20by%20diffusion%20principles%2C%20we%20initialize%20the%0Amutual%20transformation%20between%20regular%20watermark%20information%20and%20irregular%0Awatermark%20patterns.%20Subsequently%2C%20these%20patterns%20are%20integrated%20with%20screen%0Acontent%20using%20a%20pre-multiplication%20alpha%20blending%20technique%2C%20supported%20by%20a%0Apre-trained%20screen%20decoder%20for%20accurate%20watermark%20retrieval.%20The%20progressively%0Acomplex%20distorter%20enhances%20the%20robustness%20of%20the%20watermark%20in%20real-world%0Ascreenshot%20scenarios.%20Finally%2C%20the%20model%20undergoes%20fine-tuning%20guided%20by%20a%0Ajoint-level%20distorter%20to%20ensure%20optimal%20performance.To%20validate%20the%0Aeffectiveness%20of%20ScreenMark%2C%20we%20compiled%20a%20dataset%20comprising%20100%2C000%0Ascreenshots%20from%20various%20devices%20and%20resolutions.%20Extensive%20experiments%20across%0Adifferent%20datasets%20confirm%20the%20method%27s%20superior%20robustness%2C%20imperceptibility%2C%0Aand%20practical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03487v1&entry.124074799=Read"},
{"title": "UniMERNet: A Universal Network for Real-World Mathematical Expression\n  Recognition", "author": "Bin Wang and Zhuangcheng Gu and Guang Liang and Chao Xu and Bo Zhang and Botian Shi and Conghui He", "abstract": "  The paper introduces the UniMER dataset, marking the first study on\nMathematical Expression Recognition (MER) targeting complex real-world\nscenarios. The UniMER dataset includes a large-scale training set, UniMER-1M,\nwhich offers unprecedented scale and diversity with one million training\ninstances to train high-quality, robust models. Additionally, UniMER features a\nmeticulously designed, diverse test set, UniMER-Test, which covers a variety of\nformula distributions found in real-world scenarios, providing a more\ncomprehensive and fair evaluation. To better utilize the UniMER dataset, the\npaper proposes a Universal Mathematical Expression Recognition Network\n(UniMERNet), tailored to the characteristics of formula recognition. UniMERNet\nconsists of a carefully designed encoder that incorporates detail-aware and\nlocal context features, and an optimized decoder for accelerated performance.\nExtensive experiments conducted using the UniMER-1M dataset and UniMERNet\ndemonstrate that training on the large-scale UniMER-1M dataset can produce a\nmore generalizable formula recognition model, significantly outperforming all\nprevious datasets. Furthermore, the introduction of UniMERNet enhances the\nmodel's performance in formula recognition, achieving higher accuracy and\nspeeds. All data, models, and code are available at\nhttps://github.com/opendatalab/UniMERNet.\n", "link": "http://arxiv.org/abs/2404.15254v2", "date": "2024-09-05", "relevancy": 1.9184, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMERNet%3A%20A%20Universal%20Network%20for%20Real-World%20Mathematical%20Expression%0A%20%20Recognition&body=Title%3A%20UniMERNet%3A%20A%20Universal%20Network%20for%20Real-World%20Mathematical%20Expression%0A%20%20Recognition%0AAuthor%3A%20Bin%20Wang%20and%20Zhuangcheng%20Gu%20and%20Guang%20Liang%20and%20Chao%20Xu%20and%20Bo%20Zhang%20and%20Botian%20Shi%20and%20Conghui%20He%0AAbstract%3A%20%20%20The%20paper%20introduces%20the%20UniMER%20dataset%2C%20marking%20the%20first%20study%20on%0AMathematical%20Expression%20Recognition%20%28MER%29%20targeting%20complex%20real-world%0Ascenarios.%20The%20UniMER%20dataset%20includes%20a%20large-scale%20training%20set%2C%20UniMER-1M%2C%0Awhich%20offers%20unprecedented%20scale%20and%20diversity%20with%20one%20million%20training%0Ainstances%20to%20train%20high-quality%2C%20robust%20models.%20Additionally%2C%20UniMER%20features%20a%0Ameticulously%20designed%2C%20diverse%20test%20set%2C%20UniMER-Test%2C%20which%20covers%20a%20variety%20of%0Aformula%20distributions%20found%20in%20real-world%20scenarios%2C%20providing%20a%20more%0Acomprehensive%20and%20fair%20evaluation.%20To%20better%20utilize%20the%20UniMER%20dataset%2C%20the%0Apaper%20proposes%20a%20Universal%20Mathematical%20Expression%20Recognition%20Network%0A%28UniMERNet%29%2C%20tailored%20to%20the%20characteristics%20of%20formula%20recognition.%20UniMERNet%0Aconsists%20of%20a%20carefully%20designed%20encoder%20that%20incorporates%20detail-aware%20and%0Alocal%20context%20features%2C%20and%20an%20optimized%20decoder%20for%20accelerated%20performance.%0AExtensive%20experiments%20conducted%20using%20the%20UniMER-1M%20dataset%20and%20UniMERNet%0Ademonstrate%20that%20training%20on%20the%20large-scale%20UniMER-1M%20dataset%20can%20produce%20a%0Amore%20generalizable%20formula%20recognition%20model%2C%20significantly%20outperforming%20all%0Aprevious%20datasets.%20Furthermore%2C%20the%20introduction%20of%20UniMERNet%20enhances%20the%0Amodel%27s%20performance%20in%20formula%20recognition%2C%20achieving%20higher%20accuracy%20and%0Aspeeds.%20All%20data%2C%20models%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/opendatalab/UniMERNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMERNet%253A%2520A%2520Universal%2520Network%2520for%2520Real-World%2520Mathematical%2520Expression%250A%2520%2520Recognition%26entry.906535625%3DBin%2520Wang%2520and%2520Zhuangcheng%2520Gu%2520and%2520Guang%2520Liang%2520and%2520Chao%2520Xu%2520and%2520Bo%2520Zhang%2520and%2520Botian%2520Shi%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520The%2520paper%2520introduces%2520the%2520UniMER%2520dataset%252C%2520marking%2520the%2520first%2520study%2520on%250AMathematical%2520Expression%2520Recognition%2520%2528MER%2529%2520targeting%2520complex%2520real-world%250Ascenarios.%2520The%2520UniMER%2520dataset%2520includes%2520a%2520large-scale%2520training%2520set%252C%2520UniMER-1M%252C%250Awhich%2520offers%2520unprecedented%2520scale%2520and%2520diversity%2520with%2520one%2520million%2520training%250Ainstances%2520to%2520train%2520high-quality%252C%2520robust%2520models.%2520Additionally%252C%2520UniMER%2520features%2520a%250Ameticulously%2520designed%252C%2520diverse%2520test%2520set%252C%2520UniMER-Test%252C%2520which%2520covers%2520a%2520variety%2520of%250Aformula%2520distributions%2520found%2520in%2520real-world%2520scenarios%252C%2520providing%2520a%2520more%250Acomprehensive%2520and%2520fair%2520evaluation.%2520To%2520better%2520utilize%2520the%2520UniMER%2520dataset%252C%2520the%250Apaper%2520proposes%2520a%2520Universal%2520Mathematical%2520Expression%2520Recognition%2520Network%250A%2528UniMERNet%2529%252C%2520tailored%2520to%2520the%2520characteristics%2520of%2520formula%2520recognition.%2520UniMERNet%250Aconsists%2520of%2520a%2520carefully%2520designed%2520encoder%2520that%2520incorporates%2520detail-aware%2520and%250Alocal%2520context%2520features%252C%2520and%2520an%2520optimized%2520decoder%2520for%2520accelerated%2520performance.%250AExtensive%2520experiments%2520conducted%2520using%2520the%2520UniMER-1M%2520dataset%2520and%2520UniMERNet%250Ademonstrate%2520that%2520training%2520on%2520the%2520large-scale%2520UniMER-1M%2520dataset%2520can%2520produce%2520a%250Amore%2520generalizable%2520formula%2520recognition%2520model%252C%2520significantly%2520outperforming%2520all%250Aprevious%2520datasets.%2520Furthermore%252C%2520the%2520introduction%2520of%2520UniMERNet%2520enhances%2520the%250Amodel%2527s%2520performance%2520in%2520formula%2520recognition%252C%2520achieving%2520higher%2520accuracy%2520and%250Aspeeds.%2520All%2520data%252C%2520models%252C%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/opendatalab/UniMERNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMERNet%3A%20A%20Universal%20Network%20for%20Real-World%20Mathematical%20Expression%0A%20%20Recognition&entry.906535625=Bin%20Wang%20and%20Zhuangcheng%20Gu%20and%20Guang%20Liang%20and%20Chao%20Xu%20and%20Bo%20Zhang%20and%20Botian%20Shi%20and%20Conghui%20He&entry.1292438233=%20%20The%20paper%20introduces%20the%20UniMER%20dataset%2C%20marking%20the%20first%20study%20on%0AMathematical%20Expression%20Recognition%20%28MER%29%20targeting%20complex%20real-world%0Ascenarios.%20The%20UniMER%20dataset%20includes%20a%20large-scale%20training%20set%2C%20UniMER-1M%2C%0Awhich%20offers%20unprecedented%20scale%20and%20diversity%20with%20one%20million%20training%0Ainstances%20to%20train%20high-quality%2C%20robust%20models.%20Additionally%2C%20UniMER%20features%20a%0Ameticulously%20designed%2C%20diverse%20test%20set%2C%20UniMER-Test%2C%20which%20covers%20a%20variety%20of%0Aformula%20distributions%20found%20in%20real-world%20scenarios%2C%20providing%20a%20more%0Acomprehensive%20and%20fair%20evaluation.%20To%20better%20utilize%20the%20UniMER%20dataset%2C%20the%0Apaper%20proposes%20a%20Universal%20Mathematical%20Expression%20Recognition%20Network%0A%28UniMERNet%29%2C%20tailored%20to%20the%20characteristics%20of%20formula%20recognition.%20UniMERNet%0Aconsists%20of%20a%20carefully%20designed%20encoder%20that%20incorporates%20detail-aware%20and%0Alocal%20context%20features%2C%20and%20an%20optimized%20decoder%20for%20accelerated%20performance.%0AExtensive%20experiments%20conducted%20using%20the%20UniMER-1M%20dataset%20and%20UniMERNet%0Ademonstrate%20that%20training%20on%20the%20large-scale%20UniMER-1M%20dataset%20can%20produce%20a%0Amore%20generalizable%20formula%20recognition%20model%2C%20significantly%20outperforming%20all%0Aprevious%20datasets.%20Furthermore%2C%20the%20introduction%20of%20UniMERNet%20enhances%20the%0Amodel%27s%20performance%20in%20formula%20recognition%2C%20achieving%20higher%20accuracy%20and%0Aspeeds.%20All%20data%2C%20models%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/opendatalab/UniMERNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15254v2&entry.124074799=Read"},
{"title": "Panopticon: a novel deep learning model to detect single transit events\n  with no prior data filtering in PLATO light curves", "author": "H. G. Vivien and M. Deleuil and N. Jannsen and J. De Ridder and D. Seynaeve and M. -A. Carpine and Y. Zerah", "abstract": "  To prepare for the analyses of the future PLATO light curves, we develop a\ndeep learning model, Panopticon, to detect transits in high precision\nphotometric light curves. Since PLATO's main objective is the detection of\ntemperate Earth-size planets around solar-type stars, the code is designed to\ndetect individual transit events. The filtering step, required by conventional\ndetection methods, can affect the transit, which could be an issue for long and\nshallow transits. To protect transit shape and depth, the code is also designed\nto work on unfiltered light curves. We trained the model on a set of simulated\nPLATO light curves in which we injected, at pixel level, either planetary,\neclipsing binary, or background eclipsing binary signals. We also include a\nvariety of noises in our data, such as granulation, stellar spots or cosmic\nrays. The approach is able to recover 90% of our test population, including\nmore than 25% of the Earth-analogs, even in the unfiltered light curves. The\nmodel also recovers the transits irrespective of the orbital period, and is\nable to retrieve transits on a unique event basis. These figures are obtained\nwhen accepting a false alarm rate of 1%. When keeping the false alarm rate low\n(<0.01%), it is still able to recover more than 85% of the transit signals. Any\ntransit deeper than 180ppm is essentially guaranteed to be recovered. This\nmethod is able to recover transits on a unique event basis, and does so with a\nlow false alarm rate. Thanks to light curves being one-dimensional, model\ntraining is fast, on the order of a few hours per model. This speed in training\nand inference, coupled to the recovery effectiveness and precision of the model\nmake it an ideal tool to complement, or be used ahead of, classical approaches.\n", "link": "http://arxiv.org/abs/2409.03466v1", "date": "2024-09-05", "relevancy": 1.9083, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5031}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4622}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panopticon%3A%20a%20novel%20deep%20learning%20model%20to%20detect%20single%20transit%20events%0A%20%20with%20no%20prior%20data%20filtering%20in%20PLATO%20light%20curves&body=Title%3A%20Panopticon%3A%20a%20novel%20deep%20learning%20model%20to%20detect%20single%20transit%20events%0A%20%20with%20no%20prior%20data%20filtering%20in%20PLATO%20light%20curves%0AAuthor%3A%20H.%20G.%20Vivien%20and%20M.%20Deleuil%20and%20N.%20Jannsen%20and%20J.%20De%20Ridder%20and%20D.%20Seynaeve%20and%20M.%20-A.%20Carpine%20and%20Y.%20Zerah%0AAbstract%3A%20%20%20To%20prepare%20for%20the%20analyses%20of%20the%20future%20PLATO%20light%20curves%2C%20we%20develop%20a%0Adeep%20learning%20model%2C%20Panopticon%2C%20to%20detect%20transits%20in%20high%20precision%0Aphotometric%20light%20curves.%20Since%20PLATO%27s%20main%20objective%20is%20the%20detection%20of%0Atemperate%20Earth-size%20planets%20around%20solar-type%20stars%2C%20the%20code%20is%20designed%20to%0Adetect%20individual%20transit%20events.%20The%20filtering%20step%2C%20required%20by%20conventional%0Adetection%20methods%2C%20can%20affect%20the%20transit%2C%20which%20could%20be%20an%20issue%20for%20long%20and%0Ashallow%20transits.%20To%20protect%20transit%20shape%20and%20depth%2C%20the%20code%20is%20also%20designed%0Ato%20work%20on%20unfiltered%20light%20curves.%20We%20trained%20the%20model%20on%20a%20set%20of%20simulated%0APLATO%20light%20curves%20in%20which%20we%20injected%2C%20at%20pixel%20level%2C%20either%20planetary%2C%0Aeclipsing%20binary%2C%20or%20background%20eclipsing%20binary%20signals.%20We%20also%20include%20a%0Avariety%20of%20noises%20in%20our%20data%2C%20such%20as%20granulation%2C%20stellar%20spots%20or%20cosmic%0Arays.%20The%20approach%20is%20able%20to%20recover%2090%25%20of%20our%20test%20population%2C%20including%0Amore%20than%2025%25%20of%20the%20Earth-analogs%2C%20even%20in%20the%20unfiltered%20light%20curves.%20The%0Amodel%20also%20recovers%20the%20transits%20irrespective%20of%20the%20orbital%20period%2C%20and%20is%0Aable%20to%20retrieve%20transits%20on%20a%20unique%20event%20basis.%20These%20figures%20are%20obtained%0Awhen%20accepting%20a%20false%20alarm%20rate%20of%201%25.%20When%20keeping%20the%20false%20alarm%20rate%20low%0A%28%3C0.01%25%29%2C%20it%20is%20still%20able%20to%20recover%20more%20than%2085%25%20of%20the%20transit%20signals.%20Any%0Atransit%20deeper%20than%20180ppm%20is%20essentially%20guaranteed%20to%20be%20recovered.%20This%0Amethod%20is%20able%20to%20recover%20transits%20on%20a%20unique%20event%20basis%2C%20and%20does%20so%20with%20a%0Alow%20false%20alarm%20rate.%20Thanks%20to%20light%20curves%20being%20one-dimensional%2C%20model%0Atraining%20is%20fast%2C%20on%20the%20order%20of%20a%20few%20hours%20per%20model.%20This%20speed%20in%20training%0Aand%20inference%2C%20coupled%20to%20the%20recovery%20effectiveness%20and%20precision%20of%20the%20model%0Amake%20it%20an%20ideal%20tool%20to%20complement%2C%20or%20be%20used%20ahead%20of%2C%20classical%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanopticon%253A%2520a%2520novel%2520deep%2520learning%2520model%2520to%2520detect%2520single%2520transit%2520events%250A%2520%2520with%2520no%2520prior%2520data%2520filtering%2520in%2520PLATO%2520light%2520curves%26entry.906535625%3DH.%2520G.%2520Vivien%2520and%2520M.%2520Deleuil%2520and%2520N.%2520Jannsen%2520and%2520J.%2520De%2520Ridder%2520and%2520D.%2520Seynaeve%2520and%2520M.%2520-A.%2520Carpine%2520and%2520Y.%2520Zerah%26entry.1292438233%3D%2520%2520To%2520prepare%2520for%2520the%2520analyses%2520of%2520the%2520future%2520PLATO%2520light%2520curves%252C%2520we%2520develop%2520a%250Adeep%2520learning%2520model%252C%2520Panopticon%252C%2520to%2520detect%2520transits%2520in%2520high%2520precision%250Aphotometric%2520light%2520curves.%2520Since%2520PLATO%2527s%2520main%2520objective%2520is%2520the%2520detection%2520of%250Atemperate%2520Earth-size%2520planets%2520around%2520solar-type%2520stars%252C%2520the%2520code%2520is%2520designed%2520to%250Adetect%2520individual%2520transit%2520events.%2520The%2520filtering%2520step%252C%2520required%2520by%2520conventional%250Adetection%2520methods%252C%2520can%2520affect%2520the%2520transit%252C%2520which%2520could%2520be%2520an%2520issue%2520for%2520long%2520and%250Ashallow%2520transits.%2520To%2520protect%2520transit%2520shape%2520and%2520depth%252C%2520the%2520code%2520is%2520also%2520designed%250Ato%2520work%2520on%2520unfiltered%2520light%2520curves.%2520We%2520trained%2520the%2520model%2520on%2520a%2520set%2520of%2520simulated%250APLATO%2520light%2520curves%2520in%2520which%2520we%2520injected%252C%2520at%2520pixel%2520level%252C%2520either%2520planetary%252C%250Aeclipsing%2520binary%252C%2520or%2520background%2520eclipsing%2520binary%2520signals.%2520We%2520also%2520include%2520a%250Avariety%2520of%2520noises%2520in%2520our%2520data%252C%2520such%2520as%2520granulation%252C%2520stellar%2520spots%2520or%2520cosmic%250Arays.%2520The%2520approach%2520is%2520able%2520to%2520recover%252090%2525%2520of%2520our%2520test%2520population%252C%2520including%250Amore%2520than%252025%2525%2520of%2520the%2520Earth-analogs%252C%2520even%2520in%2520the%2520unfiltered%2520light%2520curves.%2520The%250Amodel%2520also%2520recovers%2520the%2520transits%2520irrespective%2520of%2520the%2520orbital%2520period%252C%2520and%2520is%250Aable%2520to%2520retrieve%2520transits%2520on%2520a%2520unique%2520event%2520basis.%2520These%2520figures%2520are%2520obtained%250Awhen%2520accepting%2520a%2520false%2520alarm%2520rate%2520of%25201%2525.%2520When%2520keeping%2520the%2520false%2520alarm%2520rate%2520low%250A%2528%253C0.01%2525%2529%252C%2520it%2520is%2520still%2520able%2520to%2520recover%2520more%2520than%252085%2525%2520of%2520the%2520transit%2520signals.%2520Any%250Atransit%2520deeper%2520than%2520180ppm%2520is%2520essentially%2520guaranteed%2520to%2520be%2520recovered.%2520This%250Amethod%2520is%2520able%2520to%2520recover%2520transits%2520on%2520a%2520unique%2520event%2520basis%252C%2520and%2520does%2520so%2520with%2520a%250Alow%2520false%2520alarm%2520rate.%2520Thanks%2520to%2520light%2520curves%2520being%2520one-dimensional%252C%2520model%250Atraining%2520is%2520fast%252C%2520on%2520the%2520order%2520of%2520a%2520few%2520hours%2520per%2520model.%2520This%2520speed%2520in%2520training%250Aand%2520inference%252C%2520coupled%2520to%2520the%2520recovery%2520effectiveness%2520and%2520precision%2520of%2520the%2520model%250Amake%2520it%2520an%2520ideal%2520tool%2520to%2520complement%252C%2520or%2520be%2520used%2520ahead%2520of%252C%2520classical%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panopticon%3A%20a%20novel%20deep%20learning%20model%20to%20detect%20single%20transit%20events%0A%20%20with%20no%20prior%20data%20filtering%20in%20PLATO%20light%20curves&entry.906535625=H.%20G.%20Vivien%20and%20M.%20Deleuil%20and%20N.%20Jannsen%20and%20J.%20De%20Ridder%20and%20D.%20Seynaeve%20and%20M.%20-A.%20Carpine%20and%20Y.%20Zerah&entry.1292438233=%20%20To%20prepare%20for%20the%20analyses%20of%20the%20future%20PLATO%20light%20curves%2C%20we%20develop%20a%0Adeep%20learning%20model%2C%20Panopticon%2C%20to%20detect%20transits%20in%20high%20precision%0Aphotometric%20light%20curves.%20Since%20PLATO%27s%20main%20objective%20is%20the%20detection%20of%0Atemperate%20Earth-size%20planets%20around%20solar-type%20stars%2C%20the%20code%20is%20designed%20to%0Adetect%20individual%20transit%20events.%20The%20filtering%20step%2C%20required%20by%20conventional%0Adetection%20methods%2C%20can%20affect%20the%20transit%2C%20which%20could%20be%20an%20issue%20for%20long%20and%0Ashallow%20transits.%20To%20protect%20transit%20shape%20and%20depth%2C%20the%20code%20is%20also%20designed%0Ato%20work%20on%20unfiltered%20light%20curves.%20We%20trained%20the%20model%20on%20a%20set%20of%20simulated%0APLATO%20light%20curves%20in%20which%20we%20injected%2C%20at%20pixel%20level%2C%20either%20planetary%2C%0Aeclipsing%20binary%2C%20or%20background%20eclipsing%20binary%20signals.%20We%20also%20include%20a%0Avariety%20of%20noises%20in%20our%20data%2C%20such%20as%20granulation%2C%20stellar%20spots%20or%20cosmic%0Arays.%20The%20approach%20is%20able%20to%20recover%2090%25%20of%20our%20test%20population%2C%20including%0Amore%20than%2025%25%20of%20the%20Earth-analogs%2C%20even%20in%20the%20unfiltered%20light%20curves.%20The%0Amodel%20also%20recovers%20the%20transits%20irrespective%20of%20the%20orbital%20period%2C%20and%20is%0Aable%20to%20retrieve%20transits%20on%20a%20unique%20event%20basis.%20These%20figures%20are%20obtained%0Awhen%20accepting%20a%20false%20alarm%20rate%20of%201%25.%20When%20keeping%20the%20false%20alarm%20rate%20low%0A%28%3C0.01%25%29%2C%20it%20is%20still%20able%20to%20recover%20more%20than%2085%25%20of%20the%20transit%20signals.%20Any%0Atransit%20deeper%20than%20180ppm%20is%20essentially%20guaranteed%20to%20be%20recovered.%20This%0Amethod%20is%20able%20to%20recover%20transits%20on%20a%20unique%20event%20basis%2C%20and%20does%20so%20with%20a%0Alow%20false%20alarm%20rate.%20Thanks%20to%20light%20curves%20being%20one-dimensional%2C%20model%0Atraining%20is%20fast%2C%20on%20the%20order%20of%20a%20few%20hours%20per%20model.%20This%20speed%20in%20training%0Aand%20inference%2C%20coupled%20to%20the%20recovery%20effectiveness%20and%20precision%20of%20the%20model%0Amake%20it%20an%20ideal%20tool%20to%20complement%2C%20or%20be%20used%20ahead%20of%2C%20classical%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03466v1&entry.124074799=Read"},
{"title": "CDM: A Reliable Metric for Fair and Accurate Formula Recognition\n  Evaluation", "author": "Bin Wang and Fan Wu and Linke Ouyang and Zhuangcheng Gu and Rui Zhang and Renqiu Xia and Bo Zhang and Conghui He", "abstract": "  Formula recognition presents significant challenges due to the complicated\nstructure and varied notation of mathematical expressions. Despite continuous\nadvancements in formula recognition models, the evaluation metrics employed by\nthese models, such as BLEU and Edit Distance, still exhibit notable\nlimitations. They overlook the fact that the same formula has diverse\nrepresentations and is highly sensitive to the distribution of training data,\nthereby causing the unfairness in formula recognition evaluation. To this end,\nwe propose a Character Detection Matching (CDM) metric, ensuring the evaluation\nobjectivity by designing a image-level rather than LaTex-level metric score.\nSpecifically, CDM renders both the model-predicted LaTeX and the ground-truth\nLaTeX formulas into image-formatted formulas, then employs visual feature\nextraction and localization techniques for precise character-level matching,\nincorporating spatial position information. Such a spatially-aware and\ncharacter-matching method offers a more accurate and equitable evaluation\ncompared with previous BLEU and Edit Distance metrics that rely solely on\ntext-based character matching. Experimentally, we evaluated various formula\nrecognition models using CDM, BLEU, and ExpRate metrics. Their results\ndemonstrate that the CDM aligns more closely with human evaluation standards\nand provides a fairer comparison across different models by eliminating\ndiscrepancies caused by diverse formula representations.\n", "link": "http://arxiv.org/abs/2409.03643v1", "date": "2024-09-05", "relevancy": 1.9027, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4732}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDM%3A%20A%20Reliable%20Metric%20for%20Fair%20and%20Accurate%20Formula%20Recognition%0A%20%20Evaluation&body=Title%3A%20CDM%3A%20A%20Reliable%20Metric%20for%20Fair%20and%20Accurate%20Formula%20Recognition%0A%20%20Evaluation%0AAuthor%3A%20Bin%20Wang%20and%20Fan%20Wu%20and%20Linke%20Ouyang%20and%20Zhuangcheng%20Gu%20and%20Rui%20Zhang%20and%20Renqiu%20Xia%20and%20Bo%20Zhang%20and%20Conghui%20He%0AAbstract%3A%20%20%20Formula%20recognition%20presents%20significant%20challenges%20due%20to%20the%20complicated%0Astructure%20and%20varied%20notation%20of%20mathematical%20expressions.%20Despite%20continuous%0Aadvancements%20in%20formula%20recognition%20models%2C%20the%20evaluation%20metrics%20employed%20by%0Athese%20models%2C%20such%20as%20BLEU%20and%20Edit%20Distance%2C%20still%20exhibit%20notable%0Alimitations.%20They%20overlook%20the%20fact%20that%20the%20same%20formula%20has%20diverse%0Arepresentations%20and%20is%20highly%20sensitive%20to%20the%20distribution%20of%20training%20data%2C%0Athereby%20causing%20the%20unfairness%20in%20formula%20recognition%20evaluation.%20To%20this%20end%2C%0Awe%20propose%20a%20Character%20Detection%20Matching%20%28CDM%29%20metric%2C%20ensuring%20the%20evaluation%0Aobjectivity%20by%20designing%20a%20image-level%20rather%20than%20LaTex-level%20metric%20score.%0ASpecifically%2C%20CDM%20renders%20both%20the%20model-predicted%20LaTeX%20and%20the%20ground-truth%0ALaTeX%20formulas%20into%20image-formatted%20formulas%2C%20then%20employs%20visual%20feature%0Aextraction%20and%20localization%20techniques%20for%20precise%20character-level%20matching%2C%0Aincorporating%20spatial%20position%20information.%20Such%20a%20spatially-aware%20and%0Acharacter-matching%20method%20offers%20a%20more%20accurate%20and%20equitable%20evaluation%0Acompared%20with%20previous%20BLEU%20and%20Edit%20Distance%20metrics%20that%20rely%20solely%20on%0Atext-based%20character%20matching.%20Experimentally%2C%20we%20evaluated%20various%20formula%0Arecognition%20models%20using%20CDM%2C%20BLEU%2C%20and%20ExpRate%20metrics.%20Their%20results%0Ademonstrate%20that%20the%20CDM%20aligns%20more%20closely%20with%20human%20evaluation%20standards%0Aand%20provides%20a%20fairer%20comparison%20across%20different%20models%20by%20eliminating%0Adiscrepancies%20caused%20by%20diverse%20formula%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDM%253A%2520A%2520Reliable%2520Metric%2520for%2520Fair%2520and%2520Accurate%2520Formula%2520Recognition%250A%2520%2520Evaluation%26entry.906535625%3DBin%2520Wang%2520and%2520Fan%2520Wu%2520and%2520Linke%2520Ouyang%2520and%2520Zhuangcheng%2520Gu%2520and%2520Rui%2520Zhang%2520and%2520Renqiu%2520Xia%2520and%2520Bo%2520Zhang%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520Formula%2520recognition%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520complicated%250Astructure%2520and%2520varied%2520notation%2520of%2520mathematical%2520expressions.%2520Despite%2520continuous%250Aadvancements%2520in%2520formula%2520recognition%2520models%252C%2520the%2520evaluation%2520metrics%2520employed%2520by%250Athese%2520models%252C%2520such%2520as%2520BLEU%2520and%2520Edit%2520Distance%252C%2520still%2520exhibit%2520notable%250Alimitations.%2520They%2520overlook%2520the%2520fact%2520that%2520the%2520same%2520formula%2520has%2520diverse%250Arepresentations%2520and%2520is%2520highly%2520sensitive%2520to%2520the%2520distribution%2520of%2520training%2520data%252C%250Athereby%2520causing%2520the%2520unfairness%2520in%2520formula%2520recognition%2520evaluation.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520Character%2520Detection%2520Matching%2520%2528CDM%2529%2520metric%252C%2520ensuring%2520the%2520evaluation%250Aobjectivity%2520by%2520designing%2520a%2520image-level%2520rather%2520than%2520LaTex-level%2520metric%2520score.%250ASpecifically%252C%2520CDM%2520renders%2520both%2520the%2520model-predicted%2520LaTeX%2520and%2520the%2520ground-truth%250ALaTeX%2520formulas%2520into%2520image-formatted%2520formulas%252C%2520then%2520employs%2520visual%2520feature%250Aextraction%2520and%2520localization%2520techniques%2520for%2520precise%2520character-level%2520matching%252C%250Aincorporating%2520spatial%2520position%2520information.%2520Such%2520a%2520spatially-aware%2520and%250Acharacter-matching%2520method%2520offers%2520a%2520more%2520accurate%2520and%2520equitable%2520evaluation%250Acompared%2520with%2520previous%2520BLEU%2520and%2520Edit%2520Distance%2520metrics%2520that%2520rely%2520solely%2520on%250Atext-based%2520character%2520matching.%2520Experimentally%252C%2520we%2520evaluated%2520various%2520formula%250Arecognition%2520models%2520using%2520CDM%252C%2520BLEU%252C%2520and%2520ExpRate%2520metrics.%2520Their%2520results%250Ademonstrate%2520that%2520the%2520CDM%2520aligns%2520more%2520closely%2520with%2520human%2520evaluation%2520standards%250Aand%2520provides%2520a%2520fairer%2520comparison%2520across%2520different%2520models%2520by%2520eliminating%250Adiscrepancies%2520caused%2520by%2520diverse%2520formula%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDM%3A%20A%20Reliable%20Metric%20for%20Fair%20and%20Accurate%20Formula%20Recognition%0A%20%20Evaluation&entry.906535625=Bin%20Wang%20and%20Fan%20Wu%20and%20Linke%20Ouyang%20and%20Zhuangcheng%20Gu%20and%20Rui%20Zhang%20and%20Renqiu%20Xia%20and%20Bo%20Zhang%20and%20Conghui%20He&entry.1292438233=%20%20Formula%20recognition%20presents%20significant%20challenges%20due%20to%20the%20complicated%0Astructure%20and%20varied%20notation%20of%20mathematical%20expressions.%20Despite%20continuous%0Aadvancements%20in%20formula%20recognition%20models%2C%20the%20evaluation%20metrics%20employed%20by%0Athese%20models%2C%20such%20as%20BLEU%20and%20Edit%20Distance%2C%20still%20exhibit%20notable%0Alimitations.%20They%20overlook%20the%20fact%20that%20the%20same%20formula%20has%20diverse%0Arepresentations%20and%20is%20highly%20sensitive%20to%20the%20distribution%20of%20training%20data%2C%0Athereby%20causing%20the%20unfairness%20in%20formula%20recognition%20evaluation.%20To%20this%20end%2C%0Awe%20propose%20a%20Character%20Detection%20Matching%20%28CDM%29%20metric%2C%20ensuring%20the%20evaluation%0Aobjectivity%20by%20designing%20a%20image-level%20rather%20than%20LaTex-level%20metric%20score.%0ASpecifically%2C%20CDM%20renders%20both%20the%20model-predicted%20LaTeX%20and%20the%20ground-truth%0ALaTeX%20formulas%20into%20image-formatted%20formulas%2C%20then%20employs%20visual%20feature%0Aextraction%20and%20localization%20techniques%20for%20precise%20character-level%20matching%2C%0Aincorporating%20spatial%20position%20information.%20Such%20a%20spatially-aware%20and%0Acharacter-matching%20method%20offers%20a%20more%20accurate%20and%20equitable%20evaluation%0Acompared%20with%20previous%20BLEU%20and%20Edit%20Distance%20metrics%20that%20rely%20solely%20on%0Atext-based%20character%20matching.%20Experimentally%2C%20we%20evaluated%20various%20formula%0Arecognition%20models%20using%20CDM%2C%20BLEU%2C%20and%20ExpRate%20metrics.%20Their%20results%0Ademonstrate%20that%20the%20CDM%20aligns%20more%20closely%20with%20human%20evaluation%20standards%0Aand%20provides%20a%20fairer%20comparison%20across%20different%20models%20by%20eliminating%0Adiscrepancies%20caused%20by%20diverse%20formula%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03643v1&entry.124074799=Read"},
{"title": "A New First-Order Meta-Learning Algorithm with Convergence Guarantees", "author": "El Mahdi Chayti and Martin Jaggi", "abstract": "  Learning new tasks by drawing on prior experience gathered from other\n(related) tasks is a core property of any intelligent system. Gradient-based\nmeta-learning, especially MAML and its variants, has emerged as a viable\nsolution to accomplish this goal. One problem MAML encounters is its\ncomputational and memory burdens needed to compute the meta-gradients. We\npropose a new first-order variant of MAML that we prove converges to a\nstationary point of the MAML objective, unlike other first-order variants. We\nalso show that the MAML objective does not satisfy the smoothness assumption\nassumed in previous works; we show instead that its smoothness constant grows\nwith the norm of the meta-gradient, which theoretically suggests the use of\nnormalized or clipped-gradient methods compared to the plain gradient method\nused in previous works. We validate our theory on a synthetic experiment.\n", "link": "http://arxiv.org/abs/2409.03682v1", "date": "2024-09-05", "relevancy": 1.8793, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.472}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20First-Order%20Meta-Learning%20Algorithm%20with%20Convergence%20Guarantees&body=Title%3A%20A%20New%20First-Order%20Meta-Learning%20Algorithm%20with%20Convergence%20Guarantees%0AAuthor%3A%20El%20Mahdi%20Chayti%20and%20Martin%20Jaggi%0AAbstract%3A%20%20%20Learning%20new%20tasks%20by%20drawing%20on%20prior%20experience%20gathered%20from%20other%0A%28related%29%20tasks%20is%20a%20core%20property%20of%20any%20intelligent%20system.%20Gradient-based%0Ameta-learning%2C%20especially%20MAML%20and%20its%20variants%2C%20has%20emerged%20as%20a%20viable%0Asolution%20to%20accomplish%20this%20goal.%20One%20problem%20MAML%20encounters%20is%20its%0Acomputational%20and%20memory%20burdens%20needed%20to%20compute%20the%20meta-gradients.%20We%0Apropose%20a%20new%20first-order%20variant%20of%20MAML%20that%20we%20prove%20converges%20to%20a%0Astationary%20point%20of%20the%20MAML%20objective%2C%20unlike%20other%20first-order%20variants.%20We%0Aalso%20show%20that%20the%20MAML%20objective%20does%20not%20satisfy%20the%20smoothness%20assumption%0Aassumed%20in%20previous%20works%3B%20we%20show%20instead%20that%20its%20smoothness%20constant%20grows%0Awith%20the%20norm%20of%20the%20meta-gradient%2C%20which%20theoretically%20suggests%20the%20use%20of%0Anormalized%20or%20clipped-gradient%20methods%20compared%20to%20the%20plain%20gradient%20method%0Aused%20in%20previous%20works.%20We%20validate%20our%20theory%20on%20a%20synthetic%20experiment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520First-Order%2520Meta-Learning%2520Algorithm%2520with%2520Convergence%2520Guarantees%26entry.906535625%3DEl%2520Mahdi%2520Chayti%2520and%2520Martin%2520Jaggi%26entry.1292438233%3D%2520%2520Learning%2520new%2520tasks%2520by%2520drawing%2520on%2520prior%2520experience%2520gathered%2520from%2520other%250A%2528related%2529%2520tasks%2520is%2520a%2520core%2520property%2520of%2520any%2520intelligent%2520system.%2520Gradient-based%250Ameta-learning%252C%2520especially%2520MAML%2520and%2520its%2520variants%252C%2520has%2520emerged%2520as%2520a%2520viable%250Asolution%2520to%2520accomplish%2520this%2520goal.%2520One%2520problem%2520MAML%2520encounters%2520is%2520its%250Acomputational%2520and%2520memory%2520burdens%2520needed%2520to%2520compute%2520the%2520meta-gradients.%2520We%250Apropose%2520a%2520new%2520first-order%2520variant%2520of%2520MAML%2520that%2520we%2520prove%2520converges%2520to%2520a%250Astationary%2520point%2520of%2520the%2520MAML%2520objective%252C%2520unlike%2520other%2520first-order%2520variants.%2520We%250Aalso%2520show%2520that%2520the%2520MAML%2520objective%2520does%2520not%2520satisfy%2520the%2520smoothness%2520assumption%250Aassumed%2520in%2520previous%2520works%253B%2520we%2520show%2520instead%2520that%2520its%2520smoothness%2520constant%2520grows%250Awith%2520the%2520norm%2520of%2520the%2520meta-gradient%252C%2520which%2520theoretically%2520suggests%2520the%2520use%2520of%250Anormalized%2520or%2520clipped-gradient%2520methods%2520compared%2520to%2520the%2520plain%2520gradient%2520method%250Aused%2520in%2520previous%2520works.%2520We%2520validate%2520our%2520theory%2520on%2520a%2520synthetic%2520experiment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20First-Order%20Meta-Learning%20Algorithm%20with%20Convergence%20Guarantees&entry.906535625=El%20Mahdi%20Chayti%20and%20Martin%20Jaggi&entry.1292438233=%20%20Learning%20new%20tasks%20by%20drawing%20on%20prior%20experience%20gathered%20from%20other%0A%28related%29%20tasks%20is%20a%20core%20property%20of%20any%20intelligent%20system.%20Gradient-based%0Ameta-learning%2C%20especially%20MAML%20and%20its%20variants%2C%20has%20emerged%20as%20a%20viable%0Asolution%20to%20accomplish%20this%20goal.%20One%20problem%20MAML%20encounters%20is%20its%0Acomputational%20and%20memory%20burdens%20needed%20to%20compute%20the%20meta-gradients.%20We%0Apropose%20a%20new%20first-order%20variant%20of%20MAML%20that%20we%20prove%20converges%20to%20a%0Astationary%20point%20of%20the%20MAML%20objective%2C%20unlike%20other%20first-order%20variants.%20We%0Aalso%20show%20that%20the%20MAML%20objective%20does%20not%20satisfy%20the%20smoothness%20assumption%0Aassumed%20in%20previous%20works%3B%20we%20show%20instead%20that%20its%20smoothness%20constant%20grows%0Awith%20the%20norm%20of%20the%20meta-gradient%2C%20which%20theoretically%20suggests%20the%20use%20of%0Anormalized%20or%20clipped-gradient%20methods%20compared%20to%20the%20plain%20gradient%20method%0Aused%20in%20previous%20works.%20We%20validate%20our%20theory%20on%20a%20synthetic%20experiment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03682v1&entry.124074799=Read"},
{"title": "Risk-based Calibration for Probabilistic Classifiers", "author": "Aritz P\u00e9rez and Carlos Echegoyen and Guzm\u00e1n Santaf\u00e9", "abstract": "  We introduce a general iterative procedure called risk-based calibration (RC)\ndesigned to minimize the empirical risk under the 0-1 loss (empirical error)\nfor probabilistic classifiers. These classifiers are based on modeling\nprobability distributions, including those constructed from the joint\ndistribution (generative) and those based on the class conditional distribution\n(conditional). RC can be particularized to any probabilistic classifier\nprovided a specific learning algorithm that computes the classifier's\nparameters in closed form using data statistics. RC reinforces the statistics\naligned with the true class while penalizing those associated with other\nclasses, guided by the 0-1 loss. The proposed method has been empirically\ntested on 30 datasets using na\\\"ive Bayes, quadratic discriminant analysis, and\nlogistic regression classifiers. RC improves the empirical error of the\noriginal closed-form learning algorithms and, more notably, consistently\noutperforms the gradient descent approach with the three classifiers.\n", "link": "http://arxiv.org/abs/2409.03542v1", "date": "2024-09-05", "relevancy": 1.8785, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.463}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk-based%20Calibration%20for%20Probabilistic%20Classifiers&body=Title%3A%20Risk-based%20Calibration%20for%20Probabilistic%20Classifiers%0AAuthor%3A%20Aritz%20P%C3%A9rez%20and%20Carlos%20Echegoyen%20and%20Guzm%C3%A1n%20Santaf%C3%A9%0AAbstract%3A%20%20%20We%20introduce%20a%20general%20iterative%20procedure%20called%20risk-based%20calibration%20%28RC%29%0Adesigned%20to%20minimize%20the%20empirical%20risk%20under%20the%200-1%20loss%20%28empirical%20error%29%0Afor%20probabilistic%20classifiers.%20These%20classifiers%20are%20based%20on%20modeling%0Aprobability%20distributions%2C%20including%20those%20constructed%20from%20the%20joint%0Adistribution%20%28generative%29%20and%20those%20based%20on%20the%20class%20conditional%20distribution%0A%28conditional%29.%20RC%20can%20be%20particularized%20to%20any%20probabilistic%20classifier%0Aprovided%20a%20specific%20learning%20algorithm%20that%20computes%20the%20classifier%27s%0Aparameters%20in%20closed%20form%20using%20data%20statistics.%20RC%20reinforces%20the%20statistics%0Aaligned%20with%20the%20true%20class%20while%20penalizing%20those%20associated%20with%20other%0Aclasses%2C%20guided%20by%20the%200-1%20loss.%20The%20proposed%20method%20has%20been%20empirically%0Atested%20on%2030%20datasets%20using%20na%5C%22ive%20Bayes%2C%20quadratic%20discriminant%20analysis%2C%20and%0Alogistic%20regression%20classifiers.%20RC%20improves%20the%20empirical%20error%20of%20the%0Aoriginal%20closed-form%20learning%20algorithms%20and%2C%20more%20notably%2C%20consistently%0Aoutperforms%20the%20gradient%20descent%20approach%20with%20the%20three%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk-based%2520Calibration%2520for%2520Probabilistic%2520Classifiers%26entry.906535625%3DAritz%2520P%25C3%25A9rez%2520and%2520Carlos%2520Echegoyen%2520and%2520Guzm%25C3%25A1n%2520Santaf%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520general%2520iterative%2520procedure%2520called%2520risk-based%2520calibration%2520%2528RC%2529%250Adesigned%2520to%2520minimize%2520the%2520empirical%2520risk%2520under%2520the%25200-1%2520loss%2520%2528empirical%2520error%2529%250Afor%2520probabilistic%2520classifiers.%2520These%2520classifiers%2520are%2520based%2520on%2520modeling%250Aprobability%2520distributions%252C%2520including%2520those%2520constructed%2520from%2520the%2520joint%250Adistribution%2520%2528generative%2529%2520and%2520those%2520based%2520on%2520the%2520class%2520conditional%2520distribution%250A%2528conditional%2529.%2520RC%2520can%2520be%2520particularized%2520to%2520any%2520probabilistic%2520classifier%250Aprovided%2520a%2520specific%2520learning%2520algorithm%2520that%2520computes%2520the%2520classifier%2527s%250Aparameters%2520in%2520closed%2520form%2520using%2520data%2520statistics.%2520RC%2520reinforces%2520the%2520statistics%250Aaligned%2520with%2520the%2520true%2520class%2520while%2520penalizing%2520those%2520associated%2520with%2520other%250Aclasses%252C%2520guided%2520by%2520the%25200-1%2520loss.%2520The%2520proposed%2520method%2520has%2520been%2520empirically%250Atested%2520on%252030%2520datasets%2520using%2520na%255C%2522ive%2520Bayes%252C%2520quadratic%2520discriminant%2520analysis%252C%2520and%250Alogistic%2520regression%2520classifiers.%2520RC%2520improves%2520the%2520empirical%2520error%2520of%2520the%250Aoriginal%2520closed-form%2520learning%2520algorithms%2520and%252C%2520more%2520notably%252C%2520consistently%250Aoutperforms%2520the%2520gradient%2520descent%2520approach%2520with%2520the%2520three%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-based%20Calibration%20for%20Probabilistic%20Classifiers&entry.906535625=Aritz%20P%C3%A9rez%20and%20Carlos%20Echegoyen%20and%20Guzm%C3%A1n%20Santaf%C3%A9&entry.1292438233=%20%20We%20introduce%20a%20general%20iterative%20procedure%20called%20risk-based%20calibration%20%28RC%29%0Adesigned%20to%20minimize%20the%20empirical%20risk%20under%20the%200-1%20loss%20%28empirical%20error%29%0Afor%20probabilistic%20classifiers.%20These%20classifiers%20are%20based%20on%20modeling%0Aprobability%20distributions%2C%20including%20those%20constructed%20from%20the%20joint%0Adistribution%20%28generative%29%20and%20those%20based%20on%20the%20class%20conditional%20distribution%0A%28conditional%29.%20RC%20can%20be%20particularized%20to%20any%20probabilistic%20classifier%0Aprovided%20a%20specific%20learning%20algorithm%20that%20computes%20the%20classifier%27s%0Aparameters%20in%20closed%20form%20using%20data%20statistics.%20RC%20reinforces%20the%20statistics%0Aaligned%20with%20the%20true%20class%20while%20penalizing%20those%20associated%20with%20other%0Aclasses%2C%20guided%20by%20the%200-1%20loss.%20The%20proposed%20method%20has%20been%20empirically%0Atested%20on%2030%20datasets%20using%20na%5C%22ive%20Bayes%2C%20quadratic%20discriminant%20analysis%2C%20and%0Alogistic%20regression%20classifiers.%20RC%20improves%20the%20empirical%20error%20of%20the%0Aoriginal%20closed-form%20learning%20algorithms%20and%2C%20more%20notably%2C%20consistently%0Aoutperforms%20the%20gradient%20descent%20approach%20with%20the%20three%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03542v1&entry.124074799=Read"},
{"title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation", "author": "Tianyu Zheng and Shuyue Guo and Xingwei Qu and Jiawei Guo and Xinrun Du and Qi Jia and Chenghua Lin and Wenhao Huang and Jie Fu and Ge Zhang", "abstract": "  In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun\n", "link": "http://arxiv.org/abs/2401.06477v3", "date": "2024-09-05", "relevancy": 1.8602, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4684}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation&body=Title%3A%20Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation%0AAuthor%3A%20Tianyu%20Zheng%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Jiawei%20Guo%20and%20Xinrun%20Du%20and%20Qi%20Jia%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu%20and%20Ge%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Kun%2C%20a%20novel%20approach%20for%20creating%20high-quality%0Ainstruction-tuning%20datasets%20for%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%0Amanual%20annotations.%20Adapting%20a%20self-training%20algorithm%20based%20on%20instruction%0Aback-translation%20and%20answer%20polishment%2C%20Kun%20leverages%20unlabelled%20data%20from%0Adiverse%20sources%20such%20as%20Wudao%2C%20Wanjuan%2C%20and%20SkyPile%20to%20generate%20a%20substantial%0Adataset%20of%20over%20a%20million%20Chinese%20instructional%20data%20points.%20This%20approach%0Asignificantly%20deviates%20from%20traditional%20methods%20by%20using%20a%20self-curation%0Aprocess%20to%20refine%20and%20select%20the%20most%20effective%20instruction-output%20pairs.%20Our%0Aexperiments%20with%20the%206B-parameter%20Yi%20model%20across%20various%20benchmarks%0Ademonstrate%20Kun%27s%20robustness%20and%20scalability.%20Our%20method%27s%20core%20contributions%0Alie%20in%20its%20algorithmic%20advancement%2C%20which%20enhances%20data%20retention%20and%20clarity%2C%0Aand%20its%20innovative%20data%20generation%20approach%20that%20substantially%20reduces%20the%0Areliance%20on%20costly%20and%20time-consuming%20manual%20annotations.%20This%20methodology%0Apresents%20a%20scalable%20and%20efficient%20solution%20for%20improving%20the%0Ainstruction-following%20capabilities%20of%20LLMs%2C%20with%20significant%20implications%20for%0Atheir%20application%20across%20diverse%20fields.%20The%20code%20and%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/Zheng0428/COIG-Kun%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06477v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKun%253A%2520Answer%2520Polishment%2520for%2520Chinese%2520Self-Alignment%2520with%2520Instruction%250A%2520%2520Back-Translation%26entry.906535625%3DTianyu%2520Zheng%2520and%2520Shuyue%2520Guo%2520and%2520Xingwei%2520Qu%2520and%2520Jiawei%2520Guo%2520and%2520Xinrun%2520Du%2520and%2520Qi%2520Jia%2520and%2520Chenghua%2520Lin%2520and%2520Wenhao%2520Huang%2520and%2520Jie%2520Fu%2520and%2520Ge%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Kun%252C%2520a%2520novel%2520approach%2520for%2520creating%2520high-quality%250Ainstruction-tuning%2520datasets%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520without%2520relying%2520on%250Amanual%2520annotations.%2520Adapting%2520a%2520self-training%2520algorithm%2520based%2520on%2520instruction%250Aback-translation%2520and%2520answer%2520polishment%252C%2520Kun%2520leverages%2520unlabelled%2520data%2520from%250Adiverse%2520sources%2520such%2520as%2520Wudao%252C%2520Wanjuan%252C%2520and%2520SkyPile%2520to%2520generate%2520a%2520substantial%250Adataset%2520of%2520over%2520a%2520million%2520Chinese%2520instructional%2520data%2520points.%2520This%2520approach%250Asignificantly%2520deviates%2520from%2520traditional%2520methods%2520by%2520using%2520a%2520self-curation%250Aprocess%2520to%2520refine%2520and%2520select%2520the%2520most%2520effective%2520instruction-output%2520pairs.%2520Our%250Aexperiments%2520with%2520the%25206B-parameter%2520Yi%2520model%2520across%2520various%2520benchmarks%250Ademonstrate%2520Kun%2527s%2520robustness%2520and%2520scalability.%2520Our%2520method%2527s%2520core%2520contributions%250Alie%2520in%2520its%2520algorithmic%2520advancement%252C%2520which%2520enhances%2520data%2520retention%2520and%2520clarity%252C%250Aand%2520its%2520innovative%2520data%2520generation%2520approach%2520that%2520substantially%2520reduces%2520the%250Areliance%2520on%2520costly%2520and%2520time-consuming%2520manual%2520annotations.%2520This%2520methodology%250Apresents%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520improving%2520the%250Ainstruction-following%2520capabilities%2520of%2520LLMs%252C%2520with%2520significant%2520implications%2520for%250Atheir%2520application%2520across%2520diverse%2520fields.%2520The%2520code%2520and%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Zheng0428/COIG-Kun%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06477v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kun%3A%20Answer%20Polishment%20for%20Chinese%20Self-Alignment%20with%20Instruction%0A%20%20Back-Translation&entry.906535625=Tianyu%20Zheng%20and%20Shuyue%20Guo%20and%20Xingwei%20Qu%20and%20Jiawei%20Guo%20and%20Xinrun%20Du%20and%20Qi%20Jia%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu%20and%20Ge%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Kun%2C%20a%20novel%20approach%20for%20creating%20high-quality%0Ainstruction-tuning%20datasets%20for%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%0Amanual%20annotations.%20Adapting%20a%20self-training%20algorithm%20based%20on%20instruction%0Aback-translation%20and%20answer%20polishment%2C%20Kun%20leverages%20unlabelled%20data%20from%0Adiverse%20sources%20such%20as%20Wudao%2C%20Wanjuan%2C%20and%20SkyPile%20to%20generate%20a%20substantial%0Adataset%20of%20over%20a%20million%20Chinese%20instructional%20data%20points.%20This%20approach%0Asignificantly%20deviates%20from%20traditional%20methods%20by%20using%20a%20self-curation%0Aprocess%20to%20refine%20and%20select%20the%20most%20effective%20instruction-output%20pairs.%20Our%0Aexperiments%20with%20the%206B-parameter%20Yi%20model%20across%20various%20benchmarks%0Ademonstrate%20Kun%27s%20robustness%20and%20scalability.%20Our%20method%27s%20core%20contributions%0Alie%20in%20its%20algorithmic%20advancement%2C%20which%20enhances%20data%20retention%20and%20clarity%2C%0Aand%20its%20innovative%20data%20generation%20approach%20that%20substantially%20reduces%20the%0Areliance%20on%20costly%20and%20time-consuming%20manual%20annotations.%20This%20methodology%0Apresents%20a%20scalable%20and%20efficient%20solution%20for%20improving%20the%0Ainstruction-following%20capabilities%20of%20LLMs%2C%20with%20significant%20implications%20for%0Atheir%20application%20across%20diverse%20fields.%20The%20code%20and%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/Zheng0428/COIG-Kun%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06477v3&entry.124074799=Read"},
{"title": "How to Train your Antivirus: RL-based Hardening through the\n  Problem-Space", "author": "Ilias Tsingenopoulos and Jacopo Cortellazzi and Branislav Bo\u0161ansk\u00fd and Simone Aonzo and Davy Preuveneers and Wouter Joosen and Fabio Pierazzi and Lorenzo Cavallaro", "abstract": "  ML-based malware detection on dynamic analysis reports is vulnerable to both\nevasion and spurious correlations. In this work, we investigate a specific ML\narchitecture employed in the pipeline of a widely-known commercial antivirus\ncompany, with the goal to harden it against adversarial malware. Adversarial\ntraining, the sole defensive technique that can confer empirical robustness, is\nnot applicable out of the box in this domain, for the principal reason that\ngradient-based perturbations rarely map back to feasible problem-space\nprograms. We introduce a novel Reinforcement Learning approach for constructing\nadversarial examples, a constituent part of adversarially training a model\nagainst evasion. Our approach comes with multiple advantages. It performs\nmodifications that are feasible in the problem-space, and only those; thus it\ncircumvents the inverse mapping problem. It also makes possible to provide\ntheoretical guarantees on the robustness of the model against a particular set\nof adversarial capabilities. Our empirical exploration validates our\ntheoretical insights, where we can consistently reach 0% Attack Success Rate\nafter a few adversarial retraining iterations.\n", "link": "http://arxiv.org/abs/2402.19027v2", "date": "2024-09-05", "relevancy": 1.8578, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4682}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4653}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Train%20your%20Antivirus%3A%20RL-based%20Hardening%20through%20the%0A%20%20Problem-Space&body=Title%3A%20How%20to%20Train%20your%20Antivirus%3A%20RL-based%20Hardening%20through%20the%0A%20%20Problem-Space%0AAuthor%3A%20Ilias%20Tsingenopoulos%20and%20Jacopo%20Cortellazzi%20and%20Branislav%20Bo%C5%A1ansk%C3%BD%20and%20Simone%20Aonzo%20and%20Davy%20Preuveneers%20and%20Wouter%20Joosen%20and%20Fabio%20Pierazzi%20and%20Lorenzo%20Cavallaro%0AAbstract%3A%20%20%20ML-based%20malware%20detection%20on%20dynamic%20analysis%20reports%20is%20vulnerable%20to%20both%0Aevasion%20and%20spurious%20correlations.%20In%20this%20work%2C%20we%20investigate%20a%20specific%20ML%0Aarchitecture%20employed%20in%20the%20pipeline%20of%20a%20widely-known%20commercial%20antivirus%0Acompany%2C%20with%20the%20goal%20to%20harden%20it%20against%20adversarial%20malware.%20Adversarial%0Atraining%2C%20the%20sole%20defensive%20technique%20that%20can%20confer%20empirical%20robustness%2C%20is%0Anot%20applicable%20out%20of%20the%20box%20in%20this%20domain%2C%20for%20the%20principal%20reason%20that%0Agradient-based%20perturbations%20rarely%20map%20back%20to%20feasible%20problem-space%0Aprograms.%20We%20introduce%20a%20novel%20Reinforcement%20Learning%20approach%20for%20constructing%0Aadversarial%20examples%2C%20a%20constituent%20part%20of%20adversarially%20training%20a%20model%0Aagainst%20evasion.%20Our%20approach%20comes%20with%20multiple%20advantages.%20It%20performs%0Amodifications%20that%20are%20feasible%20in%20the%20problem-space%2C%20and%20only%20those%3B%20thus%20it%0Acircumvents%20the%20inverse%20mapping%20problem.%20It%20also%20makes%20possible%20to%20provide%0Atheoretical%20guarantees%20on%20the%20robustness%20of%20the%20model%20against%20a%20particular%20set%0Aof%20adversarial%20capabilities.%20Our%20empirical%20exploration%20validates%20our%0Atheoretical%20insights%2C%20where%20we%20can%20consistently%20reach%200%25%20Attack%20Success%20Rate%0Aafter%20a%20few%20adversarial%20retraining%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Train%2520your%2520Antivirus%253A%2520RL-based%2520Hardening%2520through%2520the%250A%2520%2520Problem-Space%26entry.906535625%3DIlias%2520Tsingenopoulos%2520and%2520Jacopo%2520Cortellazzi%2520and%2520Branislav%2520Bo%25C5%25A1ansk%25C3%25BD%2520and%2520Simone%2520Aonzo%2520and%2520Davy%2520Preuveneers%2520and%2520Wouter%2520Joosen%2520and%2520Fabio%2520Pierazzi%2520and%2520Lorenzo%2520Cavallaro%26entry.1292438233%3D%2520%2520ML-based%2520malware%2520detection%2520on%2520dynamic%2520analysis%2520reports%2520is%2520vulnerable%2520to%2520both%250Aevasion%2520and%2520spurious%2520correlations.%2520In%2520this%2520work%252C%2520we%2520investigate%2520a%2520specific%2520ML%250Aarchitecture%2520employed%2520in%2520the%2520pipeline%2520of%2520a%2520widely-known%2520commercial%2520antivirus%250Acompany%252C%2520with%2520the%2520goal%2520to%2520harden%2520it%2520against%2520adversarial%2520malware.%2520Adversarial%250Atraining%252C%2520the%2520sole%2520defensive%2520technique%2520that%2520can%2520confer%2520empirical%2520robustness%252C%2520is%250Anot%2520applicable%2520out%2520of%2520the%2520box%2520in%2520this%2520domain%252C%2520for%2520the%2520principal%2520reason%2520that%250Agradient-based%2520perturbations%2520rarely%2520map%2520back%2520to%2520feasible%2520problem-space%250Aprograms.%2520We%2520introduce%2520a%2520novel%2520Reinforcement%2520Learning%2520approach%2520for%2520constructing%250Aadversarial%2520examples%252C%2520a%2520constituent%2520part%2520of%2520adversarially%2520training%2520a%2520model%250Aagainst%2520evasion.%2520Our%2520approach%2520comes%2520with%2520multiple%2520advantages.%2520It%2520performs%250Amodifications%2520that%2520are%2520feasible%2520in%2520the%2520problem-space%252C%2520and%2520only%2520those%253B%2520thus%2520it%250Acircumvents%2520the%2520inverse%2520mapping%2520problem.%2520It%2520also%2520makes%2520possible%2520to%2520provide%250Atheoretical%2520guarantees%2520on%2520the%2520robustness%2520of%2520the%2520model%2520against%2520a%2520particular%2520set%250Aof%2520adversarial%2520capabilities.%2520Our%2520empirical%2520exploration%2520validates%2520our%250Atheoretical%2520insights%252C%2520where%2520we%2520can%2520consistently%2520reach%25200%2525%2520Attack%2520Success%2520Rate%250Aafter%2520a%2520few%2520adversarial%2520retraining%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Train%20your%20Antivirus%3A%20RL-based%20Hardening%20through%20the%0A%20%20Problem-Space&entry.906535625=Ilias%20Tsingenopoulos%20and%20Jacopo%20Cortellazzi%20and%20Branislav%20Bo%C5%A1ansk%C3%BD%20and%20Simone%20Aonzo%20and%20Davy%20Preuveneers%20and%20Wouter%20Joosen%20and%20Fabio%20Pierazzi%20and%20Lorenzo%20Cavallaro&entry.1292438233=%20%20ML-based%20malware%20detection%20on%20dynamic%20analysis%20reports%20is%20vulnerable%20to%20both%0Aevasion%20and%20spurious%20correlations.%20In%20this%20work%2C%20we%20investigate%20a%20specific%20ML%0Aarchitecture%20employed%20in%20the%20pipeline%20of%20a%20widely-known%20commercial%20antivirus%0Acompany%2C%20with%20the%20goal%20to%20harden%20it%20against%20adversarial%20malware.%20Adversarial%0Atraining%2C%20the%20sole%20defensive%20technique%20that%20can%20confer%20empirical%20robustness%2C%20is%0Anot%20applicable%20out%20of%20the%20box%20in%20this%20domain%2C%20for%20the%20principal%20reason%20that%0Agradient-based%20perturbations%20rarely%20map%20back%20to%20feasible%20problem-space%0Aprograms.%20We%20introduce%20a%20novel%20Reinforcement%20Learning%20approach%20for%20constructing%0Aadversarial%20examples%2C%20a%20constituent%20part%20of%20adversarially%20training%20a%20model%0Aagainst%20evasion.%20Our%20approach%20comes%20with%20multiple%20advantages.%20It%20performs%0Amodifications%20that%20are%20feasible%20in%20the%20problem-space%2C%20and%20only%20those%3B%20thus%20it%0Acircumvents%20the%20inverse%20mapping%20problem.%20It%20also%20makes%20possible%20to%20provide%0Atheoretical%20guarantees%20on%20the%20robustness%20of%20the%20model%20against%20a%20particular%20set%0Aof%20adversarial%20capabilities.%20Our%20empirical%20exploration%20validates%20our%0Atheoretical%20insights%2C%20where%20we%20can%20consistently%20reach%200%25%20Attack%20Success%20Rate%0Aafter%20a%20few%20adversarial%20retraining%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19027v2&entry.124074799=Read"},
{"title": "Multifidelity Covariance Estimation via Regression on the Manifold of\n  Symmetric Positive Definite Matrices", "author": "Aimee Maurais and Terrence Alsup and Benjamin Peherstorfer and Youssef Marzouk", "abstract": "  We introduce a multifidelity estimator of covariance matrices formulated as\nthe solution to a regression problem on the manifold of symmetric positive\ndefinite matrices. The estimator is positive definite by construction, and the\nMahalanobis distance minimized to obtain it possesses properties enabling\npractical computation. We show that our manifold regression multifidelity\n(MRMF) covariance estimator is a maximum likelihood estimator under a certain\nerror model on manifold tangent space. More broadly, we show that our\nRiemannian regression framework encompasses existing multifidelity covariance\nestimators constructed from control variates. We demonstrate via numerical\nexamples that the MRMF estimator can provide significant decreases, up to one\norder of magnitude, in squared estimation error relative to both\nsingle-fidelity and other multifidelity covariance estimators. Furthermore,\npreservation of positive definiteness ensures that our estimator is compatible\nwith downstream tasks, such as data assimilation and metric learning, in which\nthis property is essential.\n", "link": "http://arxiv.org/abs/2307.12438v3", "date": "2024-09-05", "relevancy": 1.8255, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4493}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multifidelity%20Covariance%20Estimation%20via%20Regression%20on%20the%20Manifold%20of%0A%20%20Symmetric%20Positive%20Definite%20Matrices&body=Title%3A%20Multifidelity%20Covariance%20Estimation%20via%20Regression%20on%20the%20Manifold%20of%0A%20%20Symmetric%20Positive%20Definite%20Matrices%0AAuthor%3A%20Aimee%20Maurais%20and%20Terrence%20Alsup%20and%20Benjamin%20Peherstorfer%20and%20Youssef%20Marzouk%0AAbstract%3A%20%20%20We%20introduce%20a%20multifidelity%20estimator%20of%20covariance%20matrices%20formulated%20as%0Athe%20solution%20to%20a%20regression%20problem%20on%20the%20manifold%20of%20symmetric%20positive%0Adefinite%20matrices.%20The%20estimator%20is%20positive%20definite%20by%20construction%2C%20and%20the%0AMahalanobis%20distance%20minimized%20to%20obtain%20it%20possesses%20properties%20enabling%0Apractical%20computation.%20We%20show%20that%20our%20manifold%20regression%20multifidelity%0A%28MRMF%29%20covariance%20estimator%20is%20a%20maximum%20likelihood%20estimator%20under%20a%20certain%0Aerror%20model%20on%20manifold%20tangent%20space.%20More%20broadly%2C%20we%20show%20that%20our%0ARiemannian%20regression%20framework%20encompasses%20existing%20multifidelity%20covariance%0Aestimators%20constructed%20from%20control%20variates.%20We%20demonstrate%20via%20numerical%0Aexamples%20that%20the%20MRMF%20estimator%20can%20provide%20significant%20decreases%2C%20up%20to%20one%0Aorder%20of%20magnitude%2C%20in%20squared%20estimation%20error%20relative%20to%20both%0Asingle-fidelity%20and%20other%20multifidelity%20covariance%20estimators.%20Furthermore%2C%0Apreservation%20of%20positive%20definiteness%20ensures%20that%20our%20estimator%20is%20compatible%0Awith%20downstream%20tasks%2C%20such%20as%20data%20assimilation%20and%20metric%20learning%2C%20in%20which%0Athis%20property%20is%20essential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12438v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultifidelity%2520Covariance%2520Estimation%2520via%2520Regression%2520on%2520the%2520Manifold%2520of%250A%2520%2520Symmetric%2520Positive%2520Definite%2520Matrices%26entry.906535625%3DAimee%2520Maurais%2520and%2520Terrence%2520Alsup%2520and%2520Benjamin%2520Peherstorfer%2520and%2520Youssef%2520Marzouk%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520multifidelity%2520estimator%2520of%2520covariance%2520matrices%2520formulated%2520as%250Athe%2520solution%2520to%2520a%2520regression%2520problem%2520on%2520the%2520manifold%2520of%2520symmetric%2520positive%250Adefinite%2520matrices.%2520The%2520estimator%2520is%2520positive%2520definite%2520by%2520construction%252C%2520and%2520the%250AMahalanobis%2520distance%2520minimized%2520to%2520obtain%2520it%2520possesses%2520properties%2520enabling%250Apractical%2520computation.%2520We%2520show%2520that%2520our%2520manifold%2520regression%2520multifidelity%250A%2528MRMF%2529%2520covariance%2520estimator%2520is%2520a%2520maximum%2520likelihood%2520estimator%2520under%2520a%2520certain%250Aerror%2520model%2520on%2520manifold%2520tangent%2520space.%2520More%2520broadly%252C%2520we%2520show%2520that%2520our%250ARiemannian%2520regression%2520framework%2520encompasses%2520existing%2520multifidelity%2520covariance%250Aestimators%2520constructed%2520from%2520control%2520variates.%2520We%2520demonstrate%2520via%2520numerical%250Aexamples%2520that%2520the%2520MRMF%2520estimator%2520can%2520provide%2520significant%2520decreases%252C%2520up%2520to%2520one%250Aorder%2520of%2520magnitude%252C%2520in%2520squared%2520estimation%2520error%2520relative%2520to%2520both%250Asingle-fidelity%2520and%2520other%2520multifidelity%2520covariance%2520estimators.%2520Furthermore%252C%250Apreservation%2520of%2520positive%2520definiteness%2520ensures%2520that%2520our%2520estimator%2520is%2520compatible%250Awith%2520downstream%2520tasks%252C%2520such%2520as%2520data%2520assimilation%2520and%2520metric%2520learning%252C%2520in%2520which%250Athis%2520property%2520is%2520essential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.12438v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multifidelity%20Covariance%20Estimation%20via%20Regression%20on%20the%20Manifold%20of%0A%20%20Symmetric%20Positive%20Definite%20Matrices&entry.906535625=Aimee%20Maurais%20and%20Terrence%20Alsup%20and%20Benjamin%20Peherstorfer%20and%20Youssef%20Marzouk&entry.1292438233=%20%20We%20introduce%20a%20multifidelity%20estimator%20of%20covariance%20matrices%20formulated%20as%0Athe%20solution%20to%20a%20regression%20problem%20on%20the%20manifold%20of%20symmetric%20positive%0Adefinite%20matrices.%20The%20estimator%20is%20positive%20definite%20by%20construction%2C%20and%20the%0AMahalanobis%20distance%20minimized%20to%20obtain%20it%20possesses%20properties%20enabling%0Apractical%20computation.%20We%20show%20that%20our%20manifold%20regression%20multifidelity%0A%28MRMF%29%20covariance%20estimator%20is%20a%20maximum%20likelihood%20estimator%20under%20a%20certain%0Aerror%20model%20on%20manifold%20tangent%20space.%20More%20broadly%2C%20we%20show%20that%20our%0ARiemannian%20regression%20framework%20encompasses%20existing%20multifidelity%20covariance%0Aestimators%20constructed%20from%20control%20variates.%20We%20demonstrate%20via%20numerical%0Aexamples%20that%20the%20MRMF%20estimator%20can%20provide%20significant%20decreases%2C%20up%20to%20one%0Aorder%20of%20magnitude%2C%20in%20squared%20estimation%20error%20relative%20to%20both%0Asingle-fidelity%20and%20other%20multifidelity%20covariance%20estimators.%20Furthermore%2C%0Apreservation%20of%20positive%20definiteness%20ensures%20that%20our%20estimator%20is%20compatible%0Awith%20downstream%20tasks%2C%20such%20as%20data%20assimilation%20and%20metric%20learning%2C%20in%20which%0Athis%20property%20is%20essential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12438v3&entry.124074799=Read"},
{"title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture", "author": "Qianlong Xiang and Miao Zhang and Yuzhang Shang and Jianlong Wu and Yan Yan and Liqiang Nie", "abstract": "  Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs.\n", "link": "http://arxiv.org/abs/2409.03550v1", "date": "2024-09-05", "relevancy": 1.8232, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6396}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.606}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DKDM%3A%20Data-Free%20Knowledge%20Distillation%20for%20Diffusion%20Models%20with%20Any%0A%20%20Architecture&body=Title%3A%20DKDM%3A%20Data-Free%20Knowledge%20Distillation%20for%20Diffusion%20Models%20with%20Any%0A%20%20Architecture%0AAuthor%3A%20Qianlong%20Xiang%20and%20Miao%20Zhang%20and%20Yuzhang%20Shang%20and%20Jianlong%20Wu%20and%20Yan%20Yan%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20exceptional%20generative%20capabilities%0Aacross%20various%20areas%2C%20while%20they%20are%20hindered%20by%20slow%20inference%20speeds%20and%20high%0Acomputational%20demands%20during%20deployment.%20The%20most%20common%20way%20to%20accelerate%20DMs%0Ainvolves%20reducing%20the%20number%20of%20denoising%20steps%20during%20generation%2C%20achieved%0Athrough%20faster%20sampling%20solvers%20or%20knowledge%20distillation%20%28KD%29.%20In%20contrast%20to%0Aprior%20approaches%2C%20we%20propose%20a%20novel%20method%20that%20transfers%20the%20capability%20of%0Alarge%20pretrained%20DMs%20to%20faster%20architectures.%20Specifically%2C%20we%20employ%20KD%20in%20a%0Adistinct%20manner%20to%20compress%20DMs%20by%20distilling%20their%20generative%20ability%20into%0Amore%20rapid%20variants.%20Furthermore%2C%20considering%20that%20the%20source%20data%20is%20either%0Aunaccessible%20or%20too%20enormous%20to%20store%20for%20current%20generative%20models%2C%20we%0Aintroduce%20a%20new%20paradigm%20for%20their%20distillation%20without%20source%20data%2C%20termed%0AData-Free%20Knowledge%20Distillation%20for%20Diffusion%20Models%20%28DKDM%29.%20Generally%2C%20our%0Aestablished%20DKDM%20framework%20comprises%20two%20main%20components%3A%201%29%20a%20DKDM%20objective%0Athat%20uses%20synthetic%20denoising%20data%20produced%20by%20pretrained%20DMs%20to%20optimize%0Afaster%20DMs%20without%20source%20data%2C%20and%202%29%20a%20dynamic%20iterative%20distillation%20method%0Athat%20flexibly%20organizes%20the%20synthesis%20of%20denoising%20data%2C%20preventing%20it%20from%0Aslowing%20down%20the%20optimization%20process%20as%20the%20generation%20is%20slow.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20attempt%20at%20using%20KD%20to%20distill%20DMs%20into%20any%0Aarchitecture%20in%20a%20data-free%20manner.%20Importantly%2C%20our%20DKDM%20is%20orthogonal%20to%20most%0Aexisting%20acceleration%20methods%2C%20such%20as%20denoising%20step%20reduction%2C%20quantization%0Aand%20pruning.%20Experiments%20show%20that%20our%20DKDM%20is%20capable%20of%20deriving%202x%20faster%0ADMs%20with%20performance%20remaining%20on%20par%20with%20the%20baseline.%20Notably%2C%20our%20DKDM%0Aenables%20pretrained%20DMs%20to%20function%20as%20%22datasets%22%20for%20training%20new%20DMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDKDM%253A%2520Data-Free%2520Knowledge%2520Distillation%2520for%2520Diffusion%2520Models%2520with%2520Any%250A%2520%2520Architecture%26entry.906535625%3DQianlong%2520Xiang%2520and%2520Miao%2520Zhang%2520and%2520Yuzhang%2520Shang%2520and%2520Jianlong%2520Wu%2520and%2520Yan%2520Yan%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520demonstrated%2520exceptional%2520generative%2520capabilities%250Aacross%2520various%2520areas%252C%2520while%2520they%2520are%2520hindered%2520by%2520slow%2520inference%2520speeds%2520and%2520high%250Acomputational%2520demands%2520during%2520deployment.%2520The%2520most%2520common%2520way%2520to%2520accelerate%2520DMs%250Ainvolves%2520reducing%2520the%2520number%2520of%2520denoising%2520steps%2520during%2520generation%252C%2520achieved%250Athrough%2520faster%2520sampling%2520solvers%2520or%2520knowledge%2520distillation%2520%2528KD%2529.%2520In%2520contrast%2520to%250Aprior%2520approaches%252C%2520we%2520propose%2520a%2520novel%2520method%2520that%2520transfers%2520the%2520capability%2520of%250Alarge%2520pretrained%2520DMs%2520to%2520faster%2520architectures.%2520Specifically%252C%2520we%2520employ%2520KD%2520in%2520a%250Adistinct%2520manner%2520to%2520compress%2520DMs%2520by%2520distilling%2520their%2520generative%2520ability%2520into%250Amore%2520rapid%2520variants.%2520Furthermore%252C%2520considering%2520that%2520the%2520source%2520data%2520is%2520either%250Aunaccessible%2520or%2520too%2520enormous%2520to%2520store%2520for%2520current%2520generative%2520models%252C%2520we%250Aintroduce%2520a%2520new%2520paradigm%2520for%2520their%2520distillation%2520without%2520source%2520data%252C%2520termed%250AData-Free%2520Knowledge%2520Distillation%2520for%2520Diffusion%2520Models%2520%2528DKDM%2529.%2520Generally%252C%2520our%250Aestablished%2520DKDM%2520framework%2520comprises%2520two%2520main%2520components%253A%25201%2529%2520a%2520DKDM%2520objective%250Athat%2520uses%2520synthetic%2520denoising%2520data%2520produced%2520by%2520pretrained%2520DMs%2520to%2520optimize%250Afaster%2520DMs%2520without%2520source%2520data%252C%2520and%25202%2529%2520a%2520dynamic%2520iterative%2520distillation%2520method%250Athat%2520flexibly%2520organizes%2520the%2520synthesis%2520of%2520denoising%2520data%252C%2520preventing%2520it%2520from%250Aslowing%2520down%2520the%2520optimization%2520process%2520as%2520the%2520generation%2520is%2520slow.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520at%2520using%2520KD%2520to%2520distill%2520DMs%2520into%2520any%250Aarchitecture%2520in%2520a%2520data-free%2520manner.%2520Importantly%252C%2520our%2520DKDM%2520is%2520orthogonal%2520to%2520most%250Aexisting%2520acceleration%2520methods%252C%2520such%2520as%2520denoising%2520step%2520reduction%252C%2520quantization%250Aand%2520pruning.%2520Experiments%2520show%2520that%2520our%2520DKDM%2520is%2520capable%2520of%2520deriving%25202x%2520faster%250ADMs%2520with%2520performance%2520remaining%2520on%2520par%2520with%2520the%2520baseline.%2520Notably%252C%2520our%2520DKDM%250Aenables%2520pretrained%2520DMs%2520to%2520function%2520as%2520%2522datasets%2522%2520for%2520training%2520new%2520DMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DKDM%3A%20Data-Free%20Knowledge%20Distillation%20for%20Diffusion%20Models%20with%20Any%0A%20%20Architecture&entry.906535625=Qianlong%20Xiang%20and%20Miao%20Zhang%20and%20Yuzhang%20Shang%20and%20Jianlong%20Wu%20and%20Yan%20Yan%20and%20Liqiang%20Nie&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20exceptional%20generative%20capabilities%0Aacross%20various%20areas%2C%20while%20they%20are%20hindered%20by%20slow%20inference%20speeds%20and%20high%0Acomputational%20demands%20during%20deployment.%20The%20most%20common%20way%20to%20accelerate%20DMs%0Ainvolves%20reducing%20the%20number%20of%20denoising%20steps%20during%20generation%2C%20achieved%0Athrough%20faster%20sampling%20solvers%20or%20knowledge%20distillation%20%28KD%29.%20In%20contrast%20to%0Aprior%20approaches%2C%20we%20propose%20a%20novel%20method%20that%20transfers%20the%20capability%20of%0Alarge%20pretrained%20DMs%20to%20faster%20architectures.%20Specifically%2C%20we%20employ%20KD%20in%20a%0Adistinct%20manner%20to%20compress%20DMs%20by%20distilling%20their%20generative%20ability%20into%0Amore%20rapid%20variants.%20Furthermore%2C%20considering%20that%20the%20source%20data%20is%20either%0Aunaccessible%20or%20too%20enormous%20to%20store%20for%20current%20generative%20models%2C%20we%0Aintroduce%20a%20new%20paradigm%20for%20their%20distillation%20without%20source%20data%2C%20termed%0AData-Free%20Knowledge%20Distillation%20for%20Diffusion%20Models%20%28DKDM%29.%20Generally%2C%20our%0Aestablished%20DKDM%20framework%20comprises%20two%20main%20components%3A%201%29%20a%20DKDM%20objective%0Athat%20uses%20synthetic%20denoising%20data%20produced%20by%20pretrained%20DMs%20to%20optimize%0Afaster%20DMs%20without%20source%20data%2C%20and%202%29%20a%20dynamic%20iterative%20distillation%20method%0Athat%20flexibly%20organizes%20the%20synthesis%20of%20denoising%20data%2C%20preventing%20it%20from%0Aslowing%20down%20the%20optimization%20process%20as%20the%20generation%20is%20slow.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20attempt%20at%20using%20KD%20to%20distill%20DMs%20into%20any%0Aarchitecture%20in%20a%20data-free%20manner.%20Importantly%2C%20our%20DKDM%20is%20orthogonal%20to%20most%0Aexisting%20acceleration%20methods%2C%20such%20as%20denoising%20step%20reduction%2C%20quantization%0Aand%20pruning.%20Experiments%20show%20that%20our%20DKDM%20is%20capable%20of%20deriving%202x%20faster%0ADMs%20with%20performance%20remaining%20on%20par%20with%20the%20baseline.%20Notably%2C%20our%20DKDM%0Aenables%20pretrained%20DMs%20to%20function%20as%20%22datasets%22%20for%20training%20new%20DMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03550v1&entry.124074799=Read"},
{"title": "Threat Classification on Deployed Optical Networks Using MIMO Digital\n  Fiber Sensing, Wavelets, and Machine Learning", "author": "Khouloud Abdelli and Henrique Pavani and Christian Dorize and Sterenn Guerrier and Haik Mardoyan and Patricia Layec and Jeremie Renaudier", "abstract": "  We demonstrate mechanical threats classification including jackhammers and\nexcavators, leveraging wavelet transform of MIMO-DFS output data across a 57-km\noperational network link. Our machine learning framework incorporates transfer\nlearning and shows 93% classification accuracy from field data, with benefits\nfor optical network supervision.\n", "link": "http://arxiv.org/abs/2409.03667v1", "date": "2024-09-05", "relevancy": 1.8128, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4713}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Threat%20Classification%20on%20Deployed%20Optical%20Networks%20Using%20MIMO%20Digital%0A%20%20Fiber%20Sensing%2C%20Wavelets%2C%20and%20Machine%20Learning&body=Title%3A%20Threat%20Classification%20on%20Deployed%20Optical%20Networks%20Using%20MIMO%20Digital%0A%20%20Fiber%20Sensing%2C%20Wavelets%2C%20and%20Machine%20Learning%0AAuthor%3A%20Khouloud%20Abdelli%20and%20Henrique%20Pavani%20and%20Christian%20Dorize%20and%20Sterenn%20Guerrier%20and%20Haik%20Mardoyan%20and%20Patricia%20Layec%20and%20Jeremie%20Renaudier%0AAbstract%3A%20%20%20We%20demonstrate%20mechanical%20threats%20classification%20including%20jackhammers%20and%0Aexcavators%2C%20leveraging%20wavelet%20transform%20of%20MIMO-DFS%20output%20data%20across%20a%2057-km%0Aoperational%20network%20link.%20Our%20machine%20learning%20framework%20incorporates%20transfer%0Alearning%20and%20shows%2093%25%20classification%20accuracy%20from%20field%20data%2C%20with%20benefits%0Afor%20optical%20network%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThreat%2520Classification%2520on%2520Deployed%2520Optical%2520Networks%2520Using%2520MIMO%2520Digital%250A%2520%2520Fiber%2520Sensing%252C%2520Wavelets%252C%2520and%2520Machine%2520Learning%26entry.906535625%3DKhouloud%2520Abdelli%2520and%2520Henrique%2520Pavani%2520and%2520Christian%2520Dorize%2520and%2520Sterenn%2520Guerrier%2520and%2520Haik%2520Mardoyan%2520and%2520Patricia%2520Layec%2520and%2520Jeremie%2520Renaudier%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520mechanical%2520threats%2520classification%2520including%2520jackhammers%2520and%250Aexcavators%252C%2520leveraging%2520wavelet%2520transform%2520of%2520MIMO-DFS%2520output%2520data%2520across%2520a%252057-km%250Aoperational%2520network%2520link.%2520Our%2520machine%2520learning%2520framework%2520incorporates%2520transfer%250Alearning%2520and%2520shows%252093%2525%2520classification%2520accuracy%2520from%2520field%2520data%252C%2520with%2520benefits%250Afor%2520optical%2520network%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Threat%20Classification%20on%20Deployed%20Optical%20Networks%20Using%20MIMO%20Digital%0A%20%20Fiber%20Sensing%2C%20Wavelets%2C%20and%20Machine%20Learning&entry.906535625=Khouloud%20Abdelli%20and%20Henrique%20Pavani%20and%20Christian%20Dorize%20and%20Sterenn%20Guerrier%20and%20Haik%20Mardoyan%20and%20Patricia%20Layec%20and%20Jeremie%20Renaudier&entry.1292438233=%20%20We%20demonstrate%20mechanical%20threats%20classification%20including%20jackhammers%20and%0Aexcavators%2C%20leveraging%20wavelet%20transform%20of%20MIMO-DFS%20output%20data%20across%20a%2057-km%0Aoperational%20network%20link.%20Our%20machine%20learning%20framework%20incorporates%20transfer%0Alearning%20and%20shows%2093%25%20classification%20accuracy%20from%20field%20data%2C%20with%20benefits%0Afor%20optical%20network%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03667v1&entry.124074799=Read"},
{"title": "Commute-Time-Optimised Graphs for GNNs", "author": "Igor Sterner and Shiye Su and Petar Veli\u010dkovi\u0107", "abstract": "  We explore graph rewiring methods that optimise commute time. Recent graph\nrewiring approaches facilitate long-range interactions in sparse graphs, making\nsuch rewirings commute-time-optimal on average. However, when an expert prior\nexists on which node pairs should or should not interact, a superior rewiring\nwould favour short commute times between these privileged node pairs. We\nconstruct two synthetic datasets with known priors reflecting realistic\nsettings, and use these to motivate two bespoke rewiring methods that\nincorporate the known prior. We investigate the regimes where our rewiring\nimproves test performance on the synthetic datasets. Finally, we perform a case\nstudy on a real-world citation graph to investigate the practical implications\nof our work.\n", "link": "http://arxiv.org/abs/2407.08762v3", "date": "2024-09-05", "relevancy": 1.8119, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4662}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4467}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commute-Time-Optimised%20Graphs%20for%20GNNs&body=Title%3A%20Commute-Time-Optimised%20Graphs%20for%20GNNs%0AAuthor%3A%20Igor%20Sterner%20and%20Shiye%20Su%20and%20Petar%20Veli%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20We%20explore%20graph%20rewiring%20methods%20that%20optimise%20commute%20time.%20Recent%20graph%0Arewiring%20approaches%20facilitate%20long-range%20interactions%20in%20sparse%20graphs%2C%20making%0Asuch%20rewirings%20commute-time-optimal%20on%20average.%20However%2C%20when%20an%20expert%20prior%0Aexists%20on%20which%20node%20pairs%20should%20or%20should%20not%20interact%2C%20a%20superior%20rewiring%0Awould%20favour%20short%20commute%20times%20between%20these%20privileged%20node%20pairs.%20We%0Aconstruct%20two%20synthetic%20datasets%20with%20known%20priors%20reflecting%20realistic%0Asettings%2C%20and%20use%20these%20to%20motivate%20two%20bespoke%20rewiring%20methods%20that%0Aincorporate%20the%20known%20prior.%20We%20investigate%20the%20regimes%20where%20our%20rewiring%0Aimproves%20test%20performance%20on%20the%20synthetic%20datasets.%20Finally%2C%20we%20perform%20a%20case%0Astudy%20on%20a%20real-world%20citation%20graph%20to%20investigate%20the%20practical%20implications%0Aof%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommute-Time-Optimised%2520Graphs%2520for%2520GNNs%26entry.906535625%3DIgor%2520Sterner%2520and%2520Shiye%2520Su%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%26entry.1292438233%3D%2520%2520We%2520explore%2520graph%2520rewiring%2520methods%2520that%2520optimise%2520commute%2520time.%2520Recent%2520graph%250Arewiring%2520approaches%2520facilitate%2520long-range%2520interactions%2520in%2520sparse%2520graphs%252C%2520making%250Asuch%2520rewirings%2520commute-time-optimal%2520on%2520average.%2520However%252C%2520when%2520an%2520expert%2520prior%250Aexists%2520on%2520which%2520node%2520pairs%2520should%2520or%2520should%2520not%2520interact%252C%2520a%2520superior%2520rewiring%250Awould%2520favour%2520short%2520commute%2520times%2520between%2520these%2520privileged%2520node%2520pairs.%2520We%250Aconstruct%2520two%2520synthetic%2520datasets%2520with%2520known%2520priors%2520reflecting%2520realistic%250Asettings%252C%2520and%2520use%2520these%2520to%2520motivate%2520two%2520bespoke%2520rewiring%2520methods%2520that%250Aincorporate%2520the%2520known%2520prior.%2520We%2520investigate%2520the%2520regimes%2520where%2520our%2520rewiring%250Aimproves%2520test%2520performance%2520on%2520the%2520synthetic%2520datasets.%2520Finally%252C%2520we%2520perform%2520a%2520case%250Astudy%2520on%2520a%2520real-world%2520citation%2520graph%2520to%2520investigate%2520the%2520practical%2520implications%250Aof%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commute-Time-Optimised%20Graphs%20for%20GNNs&entry.906535625=Igor%20Sterner%20and%20Shiye%20Su%20and%20Petar%20Veli%C4%8Dkovi%C4%87&entry.1292438233=%20%20We%20explore%20graph%20rewiring%20methods%20that%20optimise%20commute%20time.%20Recent%20graph%0Arewiring%20approaches%20facilitate%20long-range%20interactions%20in%20sparse%20graphs%2C%20making%0Asuch%20rewirings%20commute-time-optimal%20on%20average.%20However%2C%20when%20an%20expert%20prior%0Aexists%20on%20which%20node%20pairs%20should%20or%20should%20not%20interact%2C%20a%20superior%20rewiring%0Awould%20favour%20short%20commute%20times%20between%20these%20privileged%20node%20pairs.%20We%0Aconstruct%20two%20synthetic%20datasets%20with%20known%20priors%20reflecting%20realistic%0Asettings%2C%20and%20use%20these%20to%20motivate%20two%20bespoke%20rewiring%20methods%20that%0Aincorporate%20the%20known%20prior.%20We%20investigate%20the%20regimes%20where%20our%20rewiring%0Aimproves%20test%20performance%20on%20the%20synthetic%20datasets.%20Finally%2C%20we%20perform%20a%20case%0Astudy%20on%20a%20real-world%20citation%20graph%20to%20investigate%20the%20practical%20implications%0Aof%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08762v3&entry.124074799=Read"},
{"title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning", "author": "Yuqiang Sun and Daoyuan Wu and Yue Xue and Han Liu and Wei Ma and Lyuye Zhang and Yang Liu and Yingjiu Li", "abstract": "  Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties.\n", "link": "http://arxiv.org/abs/2401.16185v2", "date": "2024-09-05", "relevancy": 1.806, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM4Vuln%3A%20A%20Unified%20Evaluation%20Framework%20for%20Decoupling%20and%20Enhancing%0A%20%20LLMs%27%20Vulnerability%20Reasoning&body=Title%3A%20LLM4Vuln%3A%20A%20Unified%20Evaluation%20Framework%20for%20Decoupling%20and%20Enhancing%0A%20%20LLMs%27%20Vulnerability%20Reasoning%0AAuthor%3A%20Yuqiang%20Sun%20and%20Daoyuan%20Wu%20and%20Yue%20Xue%20and%20Han%20Liu%20and%20Wei%20Ma%20and%20Lyuye%20Zhang%20and%20Yang%20Liu%20and%20Yingjiu%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%0Avarious%20tasks%2C%20including%20vulnerability%20detection.%20However%2C%20current%20efforts%20in%0Athis%20area%20are%20preliminary%2C%20lacking%20clarity%20on%20whether%20LLMs%27%20vulnerability%0Areasoning%20capabilities%20stem%20from%20the%20models%20themselves%20or%20external%20aids%20such%20as%0Aknowledge%20retrieval%20and%20tooling%20support.%0A%20%20This%20paper%20aims%20to%20isolate%20LLMs%27%20vulnerability%20reasoning%20from%20other%0Acapabilities%2C%20such%20as%20vulnerability%20knowledge%20adoption%2C%20context%20information%0Aretrieval%2C%20and%20structured%20output%20generation.%20We%20introduce%20LLM4Vuln%2C%20a%20unified%0Aevaluation%20framework%20that%20separates%20and%20assesses%20LLMs%27%20vulnerability%20reasoning%0Acapabilities%20and%20examines%20improvements%20when%20combined%20with%20other%20enhancements.%0A%20%20We%20conducted%20controlled%20experiments%20with%2097%20ground-truth%20vulnerabilities%20and%0A97%20non-vulnerable%20cases%20in%20Solidity%20and%20Java%2C%20testing%20them%20in%20a%20total%20of%209%2C312%0Ascenarios%20across%20four%20LLMs%20%28GPT-4%2C%20GPT-3.5%2C%20Mixtral%2C%20and%20Llama%203%29.%20Our%20findings%0Areveal%20the%20varying%20impacts%20of%20knowledge%20enhancement%2C%20context%20supplementation%2C%0Aprompt%20schemes%2C%20and%20models.%20Additionally%2C%20we%20identified%2014%20zero-day%0Avulnerabilities%20in%20four%20pilot%20bug%20bounty%20programs%2C%20resulting%20in%20%5C%243%2C576%20in%0Abounties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM4Vuln%253A%2520A%2520Unified%2520Evaluation%2520Framework%2520for%2520Decoupling%2520and%2520Enhancing%250A%2520%2520LLMs%2527%2520Vulnerability%2520Reasoning%26entry.906535625%3DYuqiang%2520Sun%2520and%2520Daoyuan%2520Wu%2520and%2520Yue%2520Xue%2520and%2520Han%2520Liu%2520and%2520Wei%2520Ma%2520and%2520Lyuye%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Yingjiu%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520potential%2520in%250Avarious%2520tasks%252C%2520including%2520vulnerability%2520detection.%2520However%252C%2520current%2520efforts%2520in%250Athis%2520area%2520are%2520preliminary%252C%2520lacking%2520clarity%2520on%2520whether%2520LLMs%2527%2520vulnerability%250Areasoning%2520capabilities%2520stem%2520from%2520the%2520models%2520themselves%2520or%2520external%2520aids%2520such%2520as%250Aknowledge%2520retrieval%2520and%2520tooling%2520support.%250A%2520%2520This%2520paper%2520aims%2520to%2520isolate%2520LLMs%2527%2520vulnerability%2520reasoning%2520from%2520other%250Acapabilities%252C%2520such%2520as%2520vulnerability%2520knowledge%2520adoption%252C%2520context%2520information%250Aretrieval%252C%2520and%2520structured%2520output%2520generation.%2520We%2520introduce%2520LLM4Vuln%252C%2520a%2520unified%250Aevaluation%2520framework%2520that%2520separates%2520and%2520assesses%2520LLMs%2527%2520vulnerability%2520reasoning%250Acapabilities%2520and%2520examines%2520improvements%2520when%2520combined%2520with%2520other%2520enhancements.%250A%2520%2520We%2520conducted%2520controlled%2520experiments%2520with%252097%2520ground-truth%2520vulnerabilities%2520and%250A97%2520non-vulnerable%2520cases%2520in%2520Solidity%2520and%2520Java%252C%2520testing%2520them%2520in%2520a%2520total%2520of%25209%252C312%250Ascenarios%2520across%2520four%2520LLMs%2520%2528GPT-4%252C%2520GPT-3.5%252C%2520Mixtral%252C%2520and%2520Llama%25203%2529.%2520Our%2520findings%250Areveal%2520the%2520varying%2520impacts%2520of%2520knowledge%2520enhancement%252C%2520context%2520supplementation%252C%250Aprompt%2520schemes%252C%2520and%2520models.%2520Additionally%252C%2520we%2520identified%252014%2520zero-day%250Avulnerabilities%2520in%2520four%2520pilot%2520bug%2520bounty%2520programs%252C%2520resulting%2520in%2520%255C%25243%252C576%2520in%250Abounties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM4Vuln%3A%20A%20Unified%20Evaluation%20Framework%20for%20Decoupling%20and%20Enhancing%0A%20%20LLMs%27%20Vulnerability%20Reasoning&entry.906535625=Yuqiang%20Sun%20and%20Daoyuan%20Wu%20and%20Yue%20Xue%20and%20Han%20Liu%20and%20Wei%20Ma%20and%20Lyuye%20Zhang%20and%20Yang%20Liu%20and%20Yingjiu%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%0Avarious%20tasks%2C%20including%20vulnerability%20detection.%20However%2C%20current%20efforts%20in%0Athis%20area%20are%20preliminary%2C%20lacking%20clarity%20on%20whether%20LLMs%27%20vulnerability%0Areasoning%20capabilities%20stem%20from%20the%20models%20themselves%20or%20external%20aids%20such%20as%0Aknowledge%20retrieval%20and%20tooling%20support.%0A%20%20This%20paper%20aims%20to%20isolate%20LLMs%27%20vulnerability%20reasoning%20from%20other%0Acapabilities%2C%20such%20as%20vulnerability%20knowledge%20adoption%2C%20context%20information%0Aretrieval%2C%20and%20structured%20output%20generation.%20We%20introduce%20LLM4Vuln%2C%20a%20unified%0Aevaluation%20framework%20that%20separates%20and%20assesses%20LLMs%27%20vulnerability%20reasoning%0Acapabilities%20and%20examines%20improvements%20when%20combined%20with%20other%20enhancements.%0A%20%20We%20conducted%20controlled%20experiments%20with%2097%20ground-truth%20vulnerabilities%20and%0A97%20non-vulnerable%20cases%20in%20Solidity%20and%20Java%2C%20testing%20them%20in%20a%20total%20of%209%2C312%0Ascenarios%20across%20four%20LLMs%20%28GPT-4%2C%20GPT-3.5%2C%20Mixtral%2C%20and%20Llama%203%29.%20Our%20findings%0Areveal%20the%20varying%20impacts%20of%20knowledge%20enhancement%2C%20context%20supplementation%2C%0Aprompt%20schemes%2C%20and%20models.%20Additionally%2C%20we%20identified%2014%20zero-day%0Avulnerabilities%20in%20four%20pilot%20bug%20bounty%20programs%2C%20resulting%20in%20%5C%243%2C576%20in%0Abounties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16185v2&entry.124074799=Read"},
{"title": "Improving Uncertainty-Error Correspondence in Deep Bayesian Medical\n  Image Segmentation", "author": "Prerak Mody and Nicolas F. Chaves-de-Plaza and Chinmay Rao and Eleftheria Astrenidou and Mischa de Ridder and Nienke Hoekstra and Klaus Hildebrandt and Marius Staring", "abstract": "  Increased usage of automated tools like deep learning in medical image\nsegmentation has alleviated the bottleneck of manual contouring. This has\nshifted manual labour to quality assessment (QA) of automated contours which\ninvolves detecting errors and correcting them. A potential solution to\nsemi-automated QA is to use deep Bayesian uncertainty to recommend potentially\nerroneous regions, thus reducing time spent on error detection. Previous work\nhas investigated the correspondence between uncertainty and error, however, no\nwork has been done on improving the \"utility\" of Bayesian uncertainty maps such\nthat it is only present in inaccurate regions and not in the accurate ones. Our\nwork trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which\npromotes uncertainty to be present only in inaccurate regions. We apply this\nmethod on datasets of two radiotherapy body sites, c.f. head-and-neck CT and\nprostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated\nagainst voxel inaccuracies using Receiver Operating Characteristic (ROC) and\nPrecision-Recall (PR) curves. Numerical results show that when compared to the\nBayesian baseline the proposed method successfully suppresses uncertainty for\naccurate voxels, with similar presence of uncertainty for inaccurate voxels.\nCode to reproduce experiments is available at\nhttps://github.com/prerakmody/bayesuncertainty-error-correspondence\n", "link": "http://arxiv.org/abs/2409.03470v1", "date": "2024-09-05", "relevancy": 1.8023, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6369}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6121}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Uncertainty-Error%20Correspondence%20in%20Deep%20Bayesian%20Medical%0A%20%20Image%20Segmentation&body=Title%3A%20Improving%20Uncertainty-Error%20Correspondence%20in%20Deep%20Bayesian%20Medical%0A%20%20Image%20Segmentation%0AAuthor%3A%20Prerak%20Mody%20and%20Nicolas%20F.%20Chaves-de-Plaza%20and%20Chinmay%20Rao%20and%20Eleftheria%20Astrenidou%20and%20Mischa%20de%20Ridder%20and%20Nienke%20Hoekstra%20and%20Klaus%20Hildebrandt%20and%20Marius%20Staring%0AAbstract%3A%20%20%20Increased%20usage%20of%20automated%20tools%20like%20deep%20learning%20in%20medical%20image%0Asegmentation%20has%20alleviated%20the%20bottleneck%20of%20manual%20contouring.%20This%20has%0Ashifted%20manual%20labour%20to%20quality%20assessment%20%28QA%29%20of%20automated%20contours%20which%0Ainvolves%20detecting%20errors%20and%20correcting%20them.%20A%20potential%20solution%20to%0Asemi-automated%20QA%20is%20to%20use%20deep%20Bayesian%20uncertainty%20to%20recommend%20potentially%0Aerroneous%20regions%2C%20thus%20reducing%20time%20spent%20on%20error%20detection.%20Previous%20work%0Ahas%20investigated%20the%20correspondence%20between%20uncertainty%20and%20error%2C%20however%2C%20no%0Awork%20has%20been%20done%20on%20improving%20the%20%22utility%22%20of%20Bayesian%20uncertainty%20maps%20such%0Athat%20it%20is%20only%20present%20in%20inaccurate%20regions%20and%20not%20in%20the%20accurate%20ones.%20Our%0Awork%20trains%20the%20FlipOut%20model%20with%20the%20Accuracy-vs-Uncertainty%20%28AvU%29%20loss%20which%0Apromotes%20uncertainty%20to%20be%20present%20only%20in%20inaccurate%20regions.%20We%20apply%20this%0Amethod%20on%20datasets%20of%20two%20radiotherapy%20body%20sites%2C%20c.f.%20head-and-neck%20CT%20and%0Aprostate%20MR%20scans.%20Uncertainty%20heatmaps%20%28i.e.%20predictive%20entropy%29%20are%20evaluated%0Aagainst%20voxel%20inaccuracies%20using%20Receiver%20Operating%20Characteristic%20%28ROC%29%20and%0APrecision-Recall%20%28PR%29%20curves.%20Numerical%20results%20show%20that%20when%20compared%20to%20the%0ABayesian%20baseline%20the%20proposed%20method%20successfully%20suppresses%20uncertainty%20for%0Aaccurate%20voxels%2C%20with%20similar%20presence%20of%20uncertainty%20for%20inaccurate%20voxels.%0ACode%20to%20reproduce%20experiments%20is%20available%20at%0Ahttps%3A//github.com/prerakmody/bayesuncertainty-error-correspondence%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Uncertainty-Error%2520Correspondence%2520in%2520Deep%2520Bayesian%2520Medical%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DPrerak%2520Mody%2520and%2520Nicolas%2520F.%2520Chaves-de-Plaza%2520and%2520Chinmay%2520Rao%2520and%2520Eleftheria%2520Astrenidou%2520and%2520Mischa%2520de%2520Ridder%2520and%2520Nienke%2520Hoekstra%2520and%2520Klaus%2520Hildebrandt%2520and%2520Marius%2520Staring%26entry.1292438233%3D%2520%2520Increased%2520usage%2520of%2520automated%2520tools%2520like%2520deep%2520learning%2520in%2520medical%2520image%250Asegmentation%2520has%2520alleviated%2520the%2520bottleneck%2520of%2520manual%2520contouring.%2520This%2520has%250Ashifted%2520manual%2520labour%2520to%2520quality%2520assessment%2520%2528QA%2529%2520of%2520automated%2520contours%2520which%250Ainvolves%2520detecting%2520errors%2520and%2520correcting%2520them.%2520A%2520potential%2520solution%2520to%250Asemi-automated%2520QA%2520is%2520to%2520use%2520deep%2520Bayesian%2520uncertainty%2520to%2520recommend%2520potentially%250Aerroneous%2520regions%252C%2520thus%2520reducing%2520time%2520spent%2520on%2520error%2520detection.%2520Previous%2520work%250Ahas%2520investigated%2520the%2520correspondence%2520between%2520uncertainty%2520and%2520error%252C%2520however%252C%2520no%250Awork%2520has%2520been%2520done%2520on%2520improving%2520the%2520%2522utility%2522%2520of%2520Bayesian%2520uncertainty%2520maps%2520such%250Athat%2520it%2520is%2520only%2520present%2520in%2520inaccurate%2520regions%2520and%2520not%2520in%2520the%2520accurate%2520ones.%2520Our%250Awork%2520trains%2520the%2520FlipOut%2520model%2520with%2520the%2520Accuracy-vs-Uncertainty%2520%2528AvU%2529%2520loss%2520which%250Apromotes%2520uncertainty%2520to%2520be%2520present%2520only%2520in%2520inaccurate%2520regions.%2520We%2520apply%2520this%250Amethod%2520on%2520datasets%2520of%2520two%2520radiotherapy%2520body%2520sites%252C%2520c.f.%2520head-and-neck%2520CT%2520and%250Aprostate%2520MR%2520scans.%2520Uncertainty%2520heatmaps%2520%2528i.e.%2520predictive%2520entropy%2529%2520are%2520evaluated%250Aagainst%2520voxel%2520inaccuracies%2520using%2520Receiver%2520Operating%2520Characteristic%2520%2528ROC%2529%2520and%250APrecision-Recall%2520%2528PR%2529%2520curves.%2520Numerical%2520results%2520show%2520that%2520when%2520compared%2520to%2520the%250ABayesian%2520baseline%2520the%2520proposed%2520method%2520successfully%2520suppresses%2520uncertainty%2520for%250Aaccurate%2520voxels%252C%2520with%2520similar%2520presence%2520of%2520uncertainty%2520for%2520inaccurate%2520voxels.%250ACode%2520to%2520reproduce%2520experiments%2520is%2520available%2520at%250Ahttps%253A//github.com/prerakmody/bayesuncertainty-error-correspondence%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Uncertainty-Error%20Correspondence%20in%20Deep%20Bayesian%20Medical%0A%20%20Image%20Segmentation&entry.906535625=Prerak%20Mody%20and%20Nicolas%20F.%20Chaves-de-Plaza%20and%20Chinmay%20Rao%20and%20Eleftheria%20Astrenidou%20and%20Mischa%20de%20Ridder%20and%20Nienke%20Hoekstra%20and%20Klaus%20Hildebrandt%20and%20Marius%20Staring&entry.1292438233=%20%20Increased%20usage%20of%20automated%20tools%20like%20deep%20learning%20in%20medical%20image%0Asegmentation%20has%20alleviated%20the%20bottleneck%20of%20manual%20contouring.%20This%20has%0Ashifted%20manual%20labour%20to%20quality%20assessment%20%28QA%29%20of%20automated%20contours%20which%0Ainvolves%20detecting%20errors%20and%20correcting%20them.%20A%20potential%20solution%20to%0Asemi-automated%20QA%20is%20to%20use%20deep%20Bayesian%20uncertainty%20to%20recommend%20potentially%0Aerroneous%20regions%2C%20thus%20reducing%20time%20spent%20on%20error%20detection.%20Previous%20work%0Ahas%20investigated%20the%20correspondence%20between%20uncertainty%20and%20error%2C%20however%2C%20no%0Awork%20has%20been%20done%20on%20improving%20the%20%22utility%22%20of%20Bayesian%20uncertainty%20maps%20such%0Athat%20it%20is%20only%20present%20in%20inaccurate%20regions%20and%20not%20in%20the%20accurate%20ones.%20Our%0Awork%20trains%20the%20FlipOut%20model%20with%20the%20Accuracy-vs-Uncertainty%20%28AvU%29%20loss%20which%0Apromotes%20uncertainty%20to%20be%20present%20only%20in%20inaccurate%20regions.%20We%20apply%20this%0Amethod%20on%20datasets%20of%20two%20radiotherapy%20body%20sites%2C%20c.f.%20head-and-neck%20CT%20and%0Aprostate%20MR%20scans.%20Uncertainty%20heatmaps%20%28i.e.%20predictive%20entropy%29%20are%20evaluated%0Aagainst%20voxel%20inaccuracies%20using%20Receiver%20Operating%20Characteristic%20%28ROC%29%20and%0APrecision-Recall%20%28PR%29%20curves.%20Numerical%20results%20show%20that%20when%20compared%20to%20the%0ABayesian%20baseline%20the%20proposed%20method%20successfully%20suppresses%20uncertainty%20for%0Aaccurate%20voxels%2C%20with%20similar%20presence%20of%20uncertainty%20for%20inaccurate%20voxels.%0ACode%20to%20reproduce%20experiments%20is%20available%20at%0Ahttps%3A//github.com/prerakmody/bayesuncertainty-error-correspondence%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03470v1&entry.124074799=Read"},
{"title": "F3T: A soft tactile unit with 3D force and temperature mathematical\n  decoupling ability for robots", "author": "Xiong Yang and Hao Ren and Dong Guo and Zhengrong Ling and Tieshan Zhang and Gen Li and Yifeng Tang and Haoxiang Zhao and Jiale Wang and Hongyuan Chang and Jia Dong and Yajing Shen", "abstract": "  The human skin exhibits remarkable capability to perceive contact forces and\nenvironmental temperatures, providing intricate information essential for\nnuanced manipulation. Despite recent advancements in soft tactile sensors, a\nsignificant challenge remains in accurately decoupling signals - specifically,\nseparating force from directional orientation and temperature - resulting in\nfail to meet the advanced application requirements of robots. This research\nproposes a multi-layered soft sensor unit (F3T) designed to achieve isolated\nmeasurements and mathematical decoupling of normal pressure, omnidirectional\ntangential forces, and temperature. We developed a circular coaxial magnetic\nfilm featuring a floating-mountain multi-layer capacitor, facilitating the\nphysical decoupling of normal and tangential forces in all directions.\nAdditionally, we incorporated an ion gel-based temperature sensing film atop\nthe tactile sensor. This sensor is resilient to external pressure and\ndeformation, enabling it to measure temperature and, crucially, eliminate\ncapacitor errors induced by environmental temperature changes. This innovative\ndesign allows for the decoupled measurement of multiple signals, paving the way\nfor advancements in higher-level robot motion control, autonomous\ndecision-making, and task planning.\n", "link": "http://arxiv.org/abs/2409.03421v1", "date": "2024-09-05", "relevancy": 1.7813, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4419}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F3T%3A%20A%20soft%20tactile%20unit%20with%203D%20force%20and%20temperature%20mathematical%0A%20%20decoupling%20ability%20for%20robots&body=Title%3A%20F3T%3A%20A%20soft%20tactile%20unit%20with%203D%20force%20and%20temperature%20mathematical%0A%20%20decoupling%20ability%20for%20robots%0AAuthor%3A%20Xiong%20Yang%20and%20Hao%20Ren%20and%20Dong%20Guo%20and%20Zhengrong%20Ling%20and%20Tieshan%20Zhang%20and%20Gen%20Li%20and%20Yifeng%20Tang%20and%20Haoxiang%20Zhao%20and%20Jiale%20Wang%20and%20Hongyuan%20Chang%20and%20Jia%20Dong%20and%20Yajing%20Shen%0AAbstract%3A%20%20%20The%20human%20skin%20exhibits%20remarkable%20capability%20to%20perceive%20contact%20forces%20and%0Aenvironmental%20temperatures%2C%20providing%20intricate%20information%20essential%20for%0Anuanced%20manipulation.%20Despite%20recent%20advancements%20in%20soft%20tactile%20sensors%2C%20a%0Asignificant%20challenge%20remains%20in%20accurately%20decoupling%20signals%20-%20specifically%2C%0Aseparating%20force%20from%20directional%20orientation%20and%20temperature%20-%20resulting%20in%0Afail%20to%20meet%20the%20advanced%20application%20requirements%20of%20robots.%20This%20research%0Aproposes%20a%20multi-layered%20soft%20sensor%20unit%20%28F3T%29%20designed%20to%20achieve%20isolated%0Ameasurements%20and%20mathematical%20decoupling%20of%20normal%20pressure%2C%20omnidirectional%0Atangential%20forces%2C%20and%20temperature.%20We%20developed%20a%20circular%20coaxial%20magnetic%0Afilm%20featuring%20a%20floating-mountain%20multi-layer%20capacitor%2C%20facilitating%20the%0Aphysical%20decoupling%20of%20normal%20and%20tangential%20forces%20in%20all%20directions.%0AAdditionally%2C%20we%20incorporated%20an%20ion%20gel-based%20temperature%20sensing%20film%20atop%0Athe%20tactile%20sensor.%20This%20sensor%20is%20resilient%20to%20external%20pressure%20and%0Adeformation%2C%20enabling%20it%20to%20measure%20temperature%20and%2C%20crucially%2C%20eliminate%0Acapacitor%20errors%20induced%20by%20environmental%20temperature%20changes.%20This%20innovative%0Adesign%20allows%20for%20the%20decoupled%20measurement%20of%20multiple%20signals%2C%20paving%20the%20way%0Afor%20advancements%20in%20higher-level%20robot%20motion%20control%2C%20autonomous%0Adecision-making%2C%20and%20task%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF3T%253A%2520A%2520soft%2520tactile%2520unit%2520with%25203D%2520force%2520and%2520temperature%2520mathematical%250A%2520%2520decoupling%2520ability%2520for%2520robots%26entry.906535625%3DXiong%2520Yang%2520and%2520Hao%2520Ren%2520and%2520Dong%2520Guo%2520and%2520Zhengrong%2520Ling%2520and%2520Tieshan%2520Zhang%2520and%2520Gen%2520Li%2520and%2520Yifeng%2520Tang%2520and%2520Haoxiang%2520Zhao%2520and%2520Jiale%2520Wang%2520and%2520Hongyuan%2520Chang%2520and%2520Jia%2520Dong%2520and%2520Yajing%2520Shen%26entry.1292438233%3D%2520%2520The%2520human%2520skin%2520exhibits%2520remarkable%2520capability%2520to%2520perceive%2520contact%2520forces%2520and%250Aenvironmental%2520temperatures%252C%2520providing%2520intricate%2520information%2520essential%2520for%250Anuanced%2520manipulation.%2520Despite%2520recent%2520advancements%2520in%2520soft%2520tactile%2520sensors%252C%2520a%250Asignificant%2520challenge%2520remains%2520in%2520accurately%2520decoupling%2520signals%2520-%2520specifically%252C%250Aseparating%2520force%2520from%2520directional%2520orientation%2520and%2520temperature%2520-%2520resulting%2520in%250Afail%2520to%2520meet%2520the%2520advanced%2520application%2520requirements%2520of%2520robots.%2520This%2520research%250Aproposes%2520a%2520multi-layered%2520soft%2520sensor%2520unit%2520%2528F3T%2529%2520designed%2520to%2520achieve%2520isolated%250Ameasurements%2520and%2520mathematical%2520decoupling%2520of%2520normal%2520pressure%252C%2520omnidirectional%250Atangential%2520forces%252C%2520and%2520temperature.%2520We%2520developed%2520a%2520circular%2520coaxial%2520magnetic%250Afilm%2520featuring%2520a%2520floating-mountain%2520multi-layer%2520capacitor%252C%2520facilitating%2520the%250Aphysical%2520decoupling%2520of%2520normal%2520and%2520tangential%2520forces%2520in%2520all%2520directions.%250AAdditionally%252C%2520we%2520incorporated%2520an%2520ion%2520gel-based%2520temperature%2520sensing%2520film%2520atop%250Athe%2520tactile%2520sensor.%2520This%2520sensor%2520is%2520resilient%2520to%2520external%2520pressure%2520and%250Adeformation%252C%2520enabling%2520it%2520to%2520measure%2520temperature%2520and%252C%2520crucially%252C%2520eliminate%250Acapacitor%2520errors%2520induced%2520by%2520environmental%2520temperature%2520changes.%2520This%2520innovative%250Adesign%2520allows%2520for%2520the%2520decoupled%2520measurement%2520of%2520multiple%2520signals%252C%2520paving%2520the%2520way%250Afor%2520advancements%2520in%2520higher-level%2520robot%2520motion%2520control%252C%2520autonomous%250Adecision-making%252C%2520and%2520task%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F3T%3A%20A%20soft%20tactile%20unit%20with%203D%20force%20and%20temperature%20mathematical%0A%20%20decoupling%20ability%20for%20robots&entry.906535625=Xiong%20Yang%20and%20Hao%20Ren%20and%20Dong%20Guo%20and%20Zhengrong%20Ling%20and%20Tieshan%20Zhang%20and%20Gen%20Li%20and%20Yifeng%20Tang%20and%20Haoxiang%20Zhao%20and%20Jiale%20Wang%20and%20Hongyuan%20Chang%20and%20Jia%20Dong%20and%20Yajing%20Shen&entry.1292438233=%20%20The%20human%20skin%20exhibits%20remarkable%20capability%20to%20perceive%20contact%20forces%20and%0Aenvironmental%20temperatures%2C%20providing%20intricate%20information%20essential%20for%0Anuanced%20manipulation.%20Despite%20recent%20advancements%20in%20soft%20tactile%20sensors%2C%20a%0Asignificant%20challenge%20remains%20in%20accurately%20decoupling%20signals%20-%20specifically%2C%0Aseparating%20force%20from%20directional%20orientation%20and%20temperature%20-%20resulting%20in%0Afail%20to%20meet%20the%20advanced%20application%20requirements%20of%20robots.%20This%20research%0Aproposes%20a%20multi-layered%20soft%20sensor%20unit%20%28F3T%29%20designed%20to%20achieve%20isolated%0Ameasurements%20and%20mathematical%20decoupling%20of%20normal%20pressure%2C%20omnidirectional%0Atangential%20forces%2C%20and%20temperature.%20We%20developed%20a%20circular%20coaxial%20magnetic%0Afilm%20featuring%20a%20floating-mountain%20multi-layer%20capacitor%2C%20facilitating%20the%0Aphysical%20decoupling%20of%20normal%20and%20tangential%20forces%20in%20all%20directions.%0AAdditionally%2C%20we%20incorporated%20an%20ion%20gel-based%20temperature%20sensing%20film%20atop%0Athe%20tactile%20sensor.%20This%20sensor%20is%20resilient%20to%20external%20pressure%20and%0Adeformation%2C%20enabling%20it%20to%20measure%20temperature%20and%2C%20crucially%2C%20eliminate%0Acapacitor%20errors%20induced%20by%20environmental%20temperature%20changes.%20This%20innovative%0Adesign%20allows%20for%20the%20decoupled%20measurement%20of%20multiple%20signals%2C%20paving%20the%20way%0Afor%20advancements%20in%20higher-level%20robot%20motion%20control%2C%20autonomous%0Adecision-making%2C%20and%20task%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03421v1&entry.124074799=Read"},
{"title": "The Role of Transformer Models in Advancing Blockchain Technology: A\n  Systematic Survey", "author": "Tianxu Liu and Yanbin Wang and Jianguo Sun and Ye Tian and Yanyu Huang and Tao Xue and Peiyue Li and Yiwei Liu", "abstract": "  As blockchain technology rapidly evolves, the demand for enhanced efficiency,\nsecurity, and scalability grows.Transformer models, as powerful deep learning\narchitectures,have shown unprecedented potential in addressing various\nblockchain challenges. However, a systematic review of Transformer applications\nin blockchain is lacking. This paper aims to fill this research gap by\nsurveying over 200 relevant papers, comprehensively reviewing practical cases\nand research progress of Transformers in blockchain applications. Our survey\ncovers key areas including anomaly detection, smart contract security analysis,\ncryptocurrency prediction and trend analysis, and code summary generation. To\nclearly articulate the advancements of Transformers across various blockchain\ndomains, we adopt a domain-oriented classification system, organizing and\nintroducing representative methods based on major challenges in current\nblockchain research. For each research domain,we first introduce its background\nand objectives, then review previous representative methods and analyze their\nlimitations,and finally introduce the advancements brought by Transformer\nmodels. Furthermore, we explore the challenges of utilizing Transformer, such\nas data privacy, model complexity, and real-time processing requirements.\nFinally, this article proposes future research directions, emphasizing the\nimportance of exploring the Transformer architecture in depth to adapt it to\nspecific blockchain applications, and discusses its potential role in promoting\nthe development of blockchain technology. This review aims to provide new\nperspectives and a research foundation for the integrated development of\nblockchain technology and machine learning, supporting further innovation and\napplication expansion of blockchain technology.\n", "link": "http://arxiv.org/abs/2409.02139v2", "date": "2024-09-05", "relevancy": 1.7635, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5628}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4286}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Transformer%20Models%20in%20Advancing%20Blockchain%20Technology%3A%20A%0A%20%20Systematic%20Survey&body=Title%3A%20The%20Role%20of%20Transformer%20Models%20in%20Advancing%20Blockchain%20Technology%3A%20A%0A%20%20Systematic%20Survey%0AAuthor%3A%20Tianxu%20Liu%20and%20Yanbin%20Wang%20and%20Jianguo%20Sun%20and%20Ye%20Tian%20and%20Yanyu%20Huang%20and%20Tao%20Xue%20and%20Peiyue%20Li%20and%20Yiwei%20Liu%0AAbstract%3A%20%20%20As%20blockchain%20technology%20rapidly%20evolves%2C%20the%20demand%20for%20enhanced%20efficiency%2C%0Asecurity%2C%20and%20scalability%20grows.Transformer%20models%2C%20as%20powerful%20deep%20learning%0Aarchitectures%2Chave%20shown%20unprecedented%20potential%20in%20addressing%20various%0Ablockchain%20challenges.%20However%2C%20a%20systematic%20review%20of%20Transformer%20applications%0Ain%20blockchain%20is%20lacking.%20This%20paper%20aims%20to%20fill%20this%20research%20gap%20by%0Asurveying%20over%20200%20relevant%20papers%2C%20comprehensively%20reviewing%20practical%20cases%0Aand%20research%20progress%20of%20Transformers%20in%20blockchain%20applications.%20Our%20survey%0Acovers%20key%20areas%20including%20anomaly%20detection%2C%20smart%20contract%20security%20analysis%2C%0Acryptocurrency%20prediction%20and%20trend%20analysis%2C%20and%20code%20summary%20generation.%20To%0Aclearly%20articulate%20the%20advancements%20of%20Transformers%20across%20various%20blockchain%0Adomains%2C%20we%20adopt%20a%20domain-oriented%20classification%20system%2C%20organizing%20and%0Aintroducing%20representative%20methods%20based%20on%20major%20challenges%20in%20current%0Ablockchain%20research.%20For%20each%20research%20domain%2Cwe%20first%20introduce%20its%20background%0Aand%20objectives%2C%20then%20review%20previous%20representative%20methods%20and%20analyze%20their%0Alimitations%2Cand%20finally%20introduce%20the%20advancements%20brought%20by%20Transformer%0Amodels.%20Furthermore%2C%20we%20explore%20the%20challenges%20of%20utilizing%20Transformer%2C%20such%0Aas%20data%20privacy%2C%20model%20complexity%2C%20and%20real-time%20processing%20requirements.%0AFinally%2C%20this%20article%20proposes%20future%20research%20directions%2C%20emphasizing%20the%0Aimportance%20of%20exploring%20the%20Transformer%20architecture%20in%20depth%20to%20adapt%20it%20to%0Aspecific%20blockchain%20applications%2C%20and%20discusses%20its%20potential%20role%20in%20promoting%0Athe%20development%20of%20blockchain%20technology.%20This%20review%20aims%20to%20provide%20new%0Aperspectives%20and%20a%20research%20foundation%20for%20the%20integrated%20development%20of%0Ablockchain%20technology%20and%20machine%20learning%2C%20supporting%20further%20innovation%20and%0Aapplication%20expansion%20of%20blockchain%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Transformer%2520Models%2520in%2520Advancing%2520Blockchain%2520Technology%253A%2520A%250A%2520%2520Systematic%2520Survey%26entry.906535625%3DTianxu%2520Liu%2520and%2520Yanbin%2520Wang%2520and%2520Jianguo%2520Sun%2520and%2520Ye%2520Tian%2520and%2520Yanyu%2520Huang%2520and%2520Tao%2520Xue%2520and%2520Peiyue%2520Li%2520and%2520Yiwei%2520Liu%26entry.1292438233%3D%2520%2520As%2520blockchain%2520technology%2520rapidly%2520evolves%252C%2520the%2520demand%2520for%2520enhanced%2520efficiency%252C%250Asecurity%252C%2520and%2520scalability%2520grows.Transformer%2520models%252C%2520as%2520powerful%2520deep%2520learning%250Aarchitectures%252Chave%2520shown%2520unprecedented%2520potential%2520in%2520addressing%2520various%250Ablockchain%2520challenges.%2520However%252C%2520a%2520systematic%2520review%2520of%2520Transformer%2520applications%250Ain%2520blockchain%2520is%2520lacking.%2520This%2520paper%2520aims%2520to%2520fill%2520this%2520research%2520gap%2520by%250Asurveying%2520over%2520200%2520relevant%2520papers%252C%2520comprehensively%2520reviewing%2520practical%2520cases%250Aand%2520research%2520progress%2520of%2520Transformers%2520in%2520blockchain%2520applications.%2520Our%2520survey%250Acovers%2520key%2520areas%2520including%2520anomaly%2520detection%252C%2520smart%2520contract%2520security%2520analysis%252C%250Acryptocurrency%2520prediction%2520and%2520trend%2520analysis%252C%2520and%2520code%2520summary%2520generation.%2520To%250Aclearly%2520articulate%2520the%2520advancements%2520of%2520Transformers%2520across%2520various%2520blockchain%250Adomains%252C%2520we%2520adopt%2520a%2520domain-oriented%2520classification%2520system%252C%2520organizing%2520and%250Aintroducing%2520representative%2520methods%2520based%2520on%2520major%2520challenges%2520in%2520current%250Ablockchain%2520research.%2520For%2520each%2520research%2520domain%252Cwe%2520first%2520introduce%2520its%2520background%250Aand%2520objectives%252C%2520then%2520review%2520previous%2520representative%2520methods%2520and%2520analyze%2520their%250Alimitations%252Cand%2520finally%2520introduce%2520the%2520advancements%2520brought%2520by%2520Transformer%250Amodels.%2520Furthermore%252C%2520we%2520explore%2520the%2520challenges%2520of%2520utilizing%2520Transformer%252C%2520such%250Aas%2520data%2520privacy%252C%2520model%2520complexity%252C%2520and%2520real-time%2520processing%2520requirements.%250AFinally%252C%2520this%2520article%2520proposes%2520future%2520research%2520directions%252C%2520emphasizing%2520the%250Aimportance%2520of%2520exploring%2520the%2520Transformer%2520architecture%2520in%2520depth%2520to%2520adapt%2520it%2520to%250Aspecific%2520blockchain%2520applications%252C%2520and%2520discusses%2520its%2520potential%2520role%2520in%2520promoting%250Athe%2520development%2520of%2520blockchain%2520technology.%2520This%2520review%2520aims%2520to%2520provide%2520new%250Aperspectives%2520and%2520a%2520research%2520foundation%2520for%2520the%2520integrated%2520development%2520of%250Ablockchain%2520technology%2520and%2520machine%2520learning%252C%2520supporting%2520further%2520innovation%2520and%250Aapplication%2520expansion%2520of%2520blockchain%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Transformer%20Models%20in%20Advancing%20Blockchain%20Technology%3A%20A%0A%20%20Systematic%20Survey&entry.906535625=Tianxu%20Liu%20and%20Yanbin%20Wang%20and%20Jianguo%20Sun%20and%20Ye%20Tian%20and%20Yanyu%20Huang%20and%20Tao%20Xue%20and%20Peiyue%20Li%20and%20Yiwei%20Liu&entry.1292438233=%20%20As%20blockchain%20technology%20rapidly%20evolves%2C%20the%20demand%20for%20enhanced%20efficiency%2C%0Asecurity%2C%20and%20scalability%20grows.Transformer%20models%2C%20as%20powerful%20deep%20learning%0Aarchitectures%2Chave%20shown%20unprecedented%20potential%20in%20addressing%20various%0Ablockchain%20challenges.%20However%2C%20a%20systematic%20review%20of%20Transformer%20applications%0Ain%20blockchain%20is%20lacking.%20This%20paper%20aims%20to%20fill%20this%20research%20gap%20by%0Asurveying%20over%20200%20relevant%20papers%2C%20comprehensively%20reviewing%20practical%20cases%0Aand%20research%20progress%20of%20Transformers%20in%20blockchain%20applications.%20Our%20survey%0Acovers%20key%20areas%20including%20anomaly%20detection%2C%20smart%20contract%20security%20analysis%2C%0Acryptocurrency%20prediction%20and%20trend%20analysis%2C%20and%20code%20summary%20generation.%20To%0Aclearly%20articulate%20the%20advancements%20of%20Transformers%20across%20various%20blockchain%0Adomains%2C%20we%20adopt%20a%20domain-oriented%20classification%20system%2C%20organizing%20and%0Aintroducing%20representative%20methods%20based%20on%20major%20challenges%20in%20current%0Ablockchain%20research.%20For%20each%20research%20domain%2Cwe%20first%20introduce%20its%20background%0Aand%20objectives%2C%20then%20review%20previous%20representative%20methods%20and%20analyze%20their%0Alimitations%2Cand%20finally%20introduce%20the%20advancements%20brought%20by%20Transformer%0Amodels.%20Furthermore%2C%20we%20explore%20the%20challenges%20of%20utilizing%20Transformer%2C%20such%0Aas%20data%20privacy%2C%20model%20complexity%2C%20and%20real-time%20processing%20requirements.%0AFinally%2C%20this%20article%20proposes%20future%20research%20directions%2C%20emphasizing%20the%0Aimportance%20of%20exploring%20the%20Transformer%20architecture%20in%20depth%20to%20adapt%20it%20to%0Aspecific%20blockchain%20applications%2C%20and%20discusses%20its%20potential%20role%20in%20promoting%0Athe%20development%20of%20blockchain%20technology.%20This%20review%20aims%20to%20provide%20new%0Aperspectives%20and%20a%20research%20foundation%20for%20the%20integrated%20development%20of%0Ablockchain%20technology%20and%20machine%20learning%2C%20supporting%20further%20innovation%20and%0Aapplication%20expansion%20of%20blockchain%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02139v2&entry.124074799=Read"},
{"title": "Unified Convergence Theory of Stochastic and Variance-Reduced Cubic\n  Newton Methods", "author": "El Mahdi Chayti and Nikita Doikov and Martin Jaggi", "abstract": "  We study stochastic Cubic Newton methods for solving general possibly\nnon-convex minimization problems. We propose a new framework, which we call the\nhelper framework, that provides a unified view of the stochastic and\nvariance-reduced second-order algorithms equipped with global complexity\nguarantees. It can also be applied to learning with auxiliary information. Our\nhelper framework offers the algorithm designer high flexibility for\nconstructing and analyzing the stochastic Cubic Newton methods, allowing\narbitrary size batches, and the use of noisy and possibly biased estimates of\nthe gradients and Hessians, incorporating both the variance reduction and the\nlazy Hessian updates. We recover the best-known complexities for the stochastic\nand variance-reduced Cubic Newton, under weak assumptions on the noise. A\ndirect consequence of our theory is the new lazy stochastic second-order\nmethod, which significantly improves the arithmetic complexity for large\ndimension problems. We also establish complexity bounds for the classes of\ngradient-dominated objectives, that include convex and strongly convex\nproblems. For Auxiliary Learning, we show that using a helper (auxiliary\nfunction) can outperform training alone if a given similarity measure is small.\n", "link": "http://arxiv.org/abs/2302.11962v4", "date": "2024-09-05", "relevancy": 1.7617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4417}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Convergence%20Theory%20of%20Stochastic%20and%20Variance-Reduced%20Cubic%0A%20%20Newton%20Methods&body=Title%3A%20Unified%20Convergence%20Theory%20of%20Stochastic%20and%20Variance-Reduced%20Cubic%0A%20%20Newton%20Methods%0AAuthor%3A%20El%20Mahdi%20Chayti%20and%20Nikita%20Doikov%20and%20Martin%20Jaggi%0AAbstract%3A%20%20%20We%20study%20stochastic%20Cubic%20Newton%20methods%20for%20solving%20general%20possibly%0Anon-convex%20minimization%20problems.%20We%20propose%20a%20new%20framework%2C%20which%20we%20call%20the%0Ahelper%20framework%2C%20that%20provides%20a%20unified%20view%20of%20the%20stochastic%20and%0Avariance-reduced%20second-order%20algorithms%20equipped%20with%20global%20complexity%0Aguarantees.%20It%20can%20also%20be%20applied%20to%20learning%20with%20auxiliary%20information.%20Our%0Ahelper%20framework%20offers%20the%20algorithm%20designer%20high%20flexibility%20for%0Aconstructing%20and%20analyzing%20the%20stochastic%20Cubic%20Newton%20methods%2C%20allowing%0Aarbitrary%20size%20batches%2C%20and%20the%20use%20of%20noisy%20and%20possibly%20biased%20estimates%20of%0Athe%20gradients%20and%20Hessians%2C%20incorporating%20both%20the%20variance%20reduction%20and%20the%0Alazy%20Hessian%20updates.%20We%20recover%20the%20best-known%20complexities%20for%20the%20stochastic%0Aand%20variance-reduced%20Cubic%20Newton%2C%20under%20weak%20assumptions%20on%20the%20noise.%20A%0Adirect%20consequence%20of%20our%20theory%20is%20the%20new%20lazy%20stochastic%20second-order%0Amethod%2C%20which%20significantly%20improves%20the%20arithmetic%20complexity%20for%20large%0Adimension%20problems.%20We%20also%20establish%20complexity%20bounds%20for%20the%20classes%20of%0Agradient-dominated%20objectives%2C%20that%20include%20convex%20and%20strongly%20convex%0Aproblems.%20For%20Auxiliary%20Learning%2C%20we%20show%20that%20using%20a%20helper%20%28auxiliary%0Afunction%29%20can%20outperform%20training%20alone%20if%20a%20given%20similarity%20measure%20is%20small.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.11962v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Convergence%2520Theory%2520of%2520Stochastic%2520and%2520Variance-Reduced%2520Cubic%250A%2520%2520Newton%2520Methods%26entry.906535625%3DEl%2520Mahdi%2520Chayti%2520and%2520Nikita%2520Doikov%2520and%2520Martin%2520Jaggi%26entry.1292438233%3D%2520%2520We%2520study%2520stochastic%2520Cubic%2520Newton%2520methods%2520for%2520solving%2520general%2520possibly%250Anon-convex%2520minimization%2520problems.%2520We%2520propose%2520a%2520new%2520framework%252C%2520which%2520we%2520call%2520the%250Ahelper%2520framework%252C%2520that%2520provides%2520a%2520unified%2520view%2520of%2520the%2520stochastic%2520and%250Avariance-reduced%2520second-order%2520algorithms%2520equipped%2520with%2520global%2520complexity%250Aguarantees.%2520It%2520can%2520also%2520be%2520applied%2520to%2520learning%2520with%2520auxiliary%2520information.%2520Our%250Ahelper%2520framework%2520offers%2520the%2520algorithm%2520designer%2520high%2520flexibility%2520for%250Aconstructing%2520and%2520analyzing%2520the%2520stochastic%2520Cubic%2520Newton%2520methods%252C%2520allowing%250Aarbitrary%2520size%2520batches%252C%2520and%2520the%2520use%2520of%2520noisy%2520and%2520possibly%2520biased%2520estimates%2520of%250Athe%2520gradients%2520and%2520Hessians%252C%2520incorporating%2520both%2520the%2520variance%2520reduction%2520and%2520the%250Alazy%2520Hessian%2520updates.%2520We%2520recover%2520the%2520best-known%2520complexities%2520for%2520the%2520stochastic%250Aand%2520variance-reduced%2520Cubic%2520Newton%252C%2520under%2520weak%2520assumptions%2520on%2520the%2520noise.%2520A%250Adirect%2520consequence%2520of%2520our%2520theory%2520is%2520the%2520new%2520lazy%2520stochastic%2520second-order%250Amethod%252C%2520which%2520significantly%2520improves%2520the%2520arithmetic%2520complexity%2520for%2520large%250Adimension%2520problems.%2520We%2520also%2520establish%2520complexity%2520bounds%2520for%2520the%2520classes%2520of%250Agradient-dominated%2520objectives%252C%2520that%2520include%2520convex%2520and%2520strongly%2520convex%250Aproblems.%2520For%2520Auxiliary%2520Learning%252C%2520we%2520show%2520that%2520using%2520a%2520helper%2520%2528auxiliary%250Afunction%2529%2520can%2520outperform%2520training%2520alone%2520if%2520a%2520given%2520similarity%2520measure%2520is%2520small.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.11962v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Convergence%20Theory%20of%20Stochastic%20and%20Variance-Reduced%20Cubic%0A%20%20Newton%20Methods&entry.906535625=El%20Mahdi%20Chayti%20and%20Nikita%20Doikov%20and%20Martin%20Jaggi&entry.1292438233=%20%20We%20study%20stochastic%20Cubic%20Newton%20methods%20for%20solving%20general%20possibly%0Anon-convex%20minimization%20problems.%20We%20propose%20a%20new%20framework%2C%20which%20we%20call%20the%0Ahelper%20framework%2C%20that%20provides%20a%20unified%20view%20of%20the%20stochastic%20and%0Avariance-reduced%20second-order%20algorithms%20equipped%20with%20global%20complexity%0Aguarantees.%20It%20can%20also%20be%20applied%20to%20learning%20with%20auxiliary%20information.%20Our%0Ahelper%20framework%20offers%20the%20algorithm%20designer%20high%20flexibility%20for%0Aconstructing%20and%20analyzing%20the%20stochastic%20Cubic%20Newton%20methods%2C%20allowing%0Aarbitrary%20size%20batches%2C%20and%20the%20use%20of%20noisy%20and%20possibly%20biased%20estimates%20of%0Athe%20gradients%20and%20Hessians%2C%20incorporating%20both%20the%20variance%20reduction%20and%20the%0Alazy%20Hessian%20updates.%20We%20recover%20the%20best-known%20complexities%20for%20the%20stochastic%0Aand%20variance-reduced%20Cubic%20Newton%2C%20under%20weak%20assumptions%20on%20the%20noise.%20A%0Adirect%20consequence%20of%20our%20theory%20is%20the%20new%20lazy%20stochastic%20second-order%0Amethod%2C%20which%20significantly%20improves%20the%20arithmetic%20complexity%20for%20large%0Adimension%20problems.%20We%20also%20establish%20complexity%20bounds%20for%20the%20classes%20of%0Agradient-dominated%20objectives%2C%20that%20include%20convex%20and%20strongly%20convex%0Aproblems.%20For%20Auxiliary%20Learning%2C%20we%20show%20that%20using%20a%20helper%20%28auxiliary%0Afunction%29%20can%20outperform%20training%20alone%20if%20a%20given%20similarity%20measure%20is%20small.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.11962v4&entry.124074799=Read"},
{"title": "Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach", "author": "Narim Jeong and Donghwan Lee", "abstract": "  Soft Q-learning is a variation of Q-learning designed to solve entropy\nregularized Markov decision problems where an agent aims to maximize the\nentropy regularized value function. Despite its empirical success, there have\nbeen limited theoretical studies of soft Q-learning to date. This paper aims to\noffer a novel and unified finite-time, control-theoretic analysis of soft\nQ-learning algorithms. We focus on two types of soft Q-learning algorithms: one\nutilizing the log-sum-exp operator and the other employing the Boltzmann\noperator. By using dynamical switching system models, we derive novel\nfinite-time error bounds for both soft Q-learning algorithms. We hope that our\nanalysis will deepen the current understanding of soft Q-learning by\nestablishing connections with switching system models and may even pave the way\nfor new frameworks in the finite-time analysis of other reinforcement learning\nalgorithms.\n", "link": "http://arxiv.org/abs/2403.06366v3", "date": "2024-09-05", "relevancy": 1.7537, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4507}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4305}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite-Time%20Error%20Analysis%20of%20Soft%20Q-Learning%3A%20Switching%20System%20Approach&body=Title%3A%20Finite-Time%20Error%20Analysis%20of%20Soft%20Q-Learning%3A%20Switching%20System%20Approach%0AAuthor%3A%20Narim%20Jeong%20and%20Donghwan%20Lee%0AAbstract%3A%20%20%20Soft%20Q-learning%20is%20a%20variation%20of%20Q-learning%20designed%20to%20solve%20entropy%0Aregularized%20Markov%20decision%20problems%20where%20an%20agent%20aims%20to%20maximize%20the%0Aentropy%20regularized%20value%20function.%20Despite%20its%20empirical%20success%2C%20there%20have%0Abeen%20limited%20theoretical%20studies%20of%20soft%20Q-learning%20to%20date.%20This%20paper%20aims%20to%0Aoffer%20a%20novel%20and%20unified%20finite-time%2C%20control-theoretic%20analysis%20of%20soft%0AQ-learning%20algorithms.%20We%20focus%20on%20two%20types%20of%20soft%20Q-learning%20algorithms%3A%20one%0Autilizing%20the%20log-sum-exp%20operator%20and%20the%20other%20employing%20the%20Boltzmann%0Aoperator.%20By%20using%20dynamical%20switching%20system%20models%2C%20we%20derive%20novel%0Afinite-time%20error%20bounds%20for%20both%20soft%20Q-learning%20algorithms.%20We%20hope%20that%20our%0Aanalysis%20will%20deepen%20the%20current%20understanding%20of%20soft%20Q-learning%20by%0Aestablishing%20connections%20with%20switching%20system%20models%20and%20may%20even%20pave%20the%20way%0Afor%20new%20frameworks%20in%20the%20finite-time%20analysis%20of%20other%20reinforcement%20learning%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06366v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite-Time%2520Error%2520Analysis%2520of%2520Soft%2520Q-Learning%253A%2520Switching%2520System%2520Approach%26entry.906535625%3DNarim%2520Jeong%2520and%2520Donghwan%2520Lee%26entry.1292438233%3D%2520%2520Soft%2520Q-learning%2520is%2520a%2520variation%2520of%2520Q-learning%2520designed%2520to%2520solve%2520entropy%250Aregularized%2520Markov%2520decision%2520problems%2520where%2520an%2520agent%2520aims%2520to%2520maximize%2520the%250Aentropy%2520regularized%2520value%2520function.%2520Despite%2520its%2520empirical%2520success%252C%2520there%2520have%250Abeen%2520limited%2520theoretical%2520studies%2520of%2520soft%2520Q-learning%2520to%2520date.%2520This%2520paper%2520aims%2520to%250Aoffer%2520a%2520novel%2520and%2520unified%2520finite-time%252C%2520control-theoretic%2520analysis%2520of%2520soft%250AQ-learning%2520algorithms.%2520We%2520focus%2520on%2520two%2520types%2520of%2520soft%2520Q-learning%2520algorithms%253A%2520one%250Autilizing%2520the%2520log-sum-exp%2520operator%2520and%2520the%2520other%2520employing%2520the%2520Boltzmann%250Aoperator.%2520By%2520using%2520dynamical%2520switching%2520system%2520models%252C%2520we%2520derive%2520novel%250Afinite-time%2520error%2520bounds%2520for%2520both%2520soft%2520Q-learning%2520algorithms.%2520We%2520hope%2520that%2520our%250Aanalysis%2520will%2520deepen%2520the%2520current%2520understanding%2520of%2520soft%2520Q-learning%2520by%250Aestablishing%2520connections%2520with%2520switching%2520system%2520models%2520and%2520may%2520even%2520pave%2520the%2520way%250Afor%2520new%2520frameworks%2520in%2520the%2520finite-time%2520analysis%2520of%2520other%2520reinforcement%2520learning%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06366v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite-Time%20Error%20Analysis%20of%20Soft%20Q-Learning%3A%20Switching%20System%20Approach&entry.906535625=Narim%20Jeong%20and%20Donghwan%20Lee&entry.1292438233=%20%20Soft%20Q-learning%20is%20a%20variation%20of%20Q-learning%20designed%20to%20solve%20entropy%0Aregularized%20Markov%20decision%20problems%20where%20an%20agent%20aims%20to%20maximize%20the%0Aentropy%20regularized%20value%20function.%20Despite%20its%20empirical%20success%2C%20there%20have%0Abeen%20limited%20theoretical%20studies%20of%20soft%20Q-learning%20to%20date.%20This%20paper%20aims%20to%0Aoffer%20a%20novel%20and%20unified%20finite-time%2C%20control-theoretic%20analysis%20of%20soft%0AQ-learning%20algorithms.%20We%20focus%20on%20two%20types%20of%20soft%20Q-learning%20algorithms%3A%20one%0Autilizing%20the%20log-sum-exp%20operator%20and%20the%20other%20employing%20the%20Boltzmann%0Aoperator.%20By%20using%20dynamical%20switching%20system%20models%2C%20we%20derive%20novel%0Afinite-time%20error%20bounds%20for%20both%20soft%20Q-learning%20algorithms.%20We%20hope%20that%20our%0Aanalysis%20will%20deepen%20the%20current%20understanding%20of%20soft%20Q-learning%20by%0Aestablishing%20connections%20with%20switching%20system%20models%20and%20may%20even%20pave%20the%20way%0Afor%20new%20frameworks%20in%20the%20finite-time%20analysis%20of%20other%20reinforcement%20learning%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06366v3&entry.124074799=Read"},
{"title": "TG-LMM: Enhancing Medical Image Segmentation Accuracy through\n  Text-Guided Large Multi-Modal Model", "author": "Yihao Zhao and Enhao Zhong and Cuiyun Yuan and Yang Li and Man Zhao and Chunxia Li and Jun Hu and Chenbin Liu", "abstract": "  We propose TG-LMM (Text-Guided Large Multi-Modal Model), a novel approach\nthat leverages textual descriptions of organs to enhance segmentation accuracy\nin medical images. Existing medical image segmentation methods face several\nchallenges: current medical automatic segmentation models do not effectively\nutilize prior knowledge, such as descriptions of organ locations; previous\ntext-visual models focus on identifying the target rather than improving the\nsegmentation accuracy; prior models attempt to use prior knowledge to enhance\naccuracy but do not incorporate pre-trained models. To address these issues,\nTG-LMM integrates prior knowledge, specifically expert descriptions of the\nspatial locations of organs, into the segmentation process. Our model utilizes\npre-trained image and text encoders to reduce the number of training parameters\nand accelerate the training process. Additionally, we designed a comprehensive\nimage-text information fusion structure to ensure thorough integration of the\ntwo modalities of data. We evaluated TG-LMM on three authoritative medical\nimage datasets, encompassing the segmentation of various parts of the human\nbody. Our method demonstrated superior performance compared to existing\napproaches, such as MedSAM, SAM and nnUnet.\n", "link": "http://arxiv.org/abs/2409.03412v1", "date": "2024-09-05", "relevancy": 1.7517, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.608}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5671}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TG-LMM%3A%20Enhancing%20Medical%20Image%20Segmentation%20Accuracy%20through%0A%20%20Text-Guided%20Large%20Multi-Modal%20Model&body=Title%3A%20TG-LMM%3A%20Enhancing%20Medical%20Image%20Segmentation%20Accuracy%20through%0A%20%20Text-Guided%20Large%20Multi-Modal%20Model%0AAuthor%3A%20Yihao%20Zhao%20and%20Enhao%20Zhong%20and%20Cuiyun%20Yuan%20and%20Yang%20Li%20and%20Man%20Zhao%20and%20Chunxia%20Li%20and%20Jun%20Hu%20and%20Chenbin%20Liu%0AAbstract%3A%20%20%20We%20propose%20TG-LMM%20%28Text-Guided%20Large%20Multi-Modal%20Model%29%2C%20a%20novel%20approach%0Athat%20leverages%20textual%20descriptions%20of%20organs%20to%20enhance%20segmentation%20accuracy%0Ain%20medical%20images.%20Existing%20medical%20image%20segmentation%20methods%20face%20several%0Achallenges%3A%20current%20medical%20automatic%20segmentation%20models%20do%20not%20effectively%0Autilize%20prior%20knowledge%2C%20such%20as%20descriptions%20of%20organ%20locations%3B%20previous%0Atext-visual%20models%20focus%20on%20identifying%20the%20target%20rather%20than%20improving%20the%0Asegmentation%20accuracy%3B%20prior%20models%20attempt%20to%20use%20prior%20knowledge%20to%20enhance%0Aaccuracy%20but%20do%20not%20incorporate%20pre-trained%20models.%20To%20address%20these%20issues%2C%0ATG-LMM%20integrates%20prior%20knowledge%2C%20specifically%20expert%20descriptions%20of%20the%0Aspatial%20locations%20of%20organs%2C%20into%20the%20segmentation%20process.%20Our%20model%20utilizes%0Apre-trained%20image%20and%20text%20encoders%20to%20reduce%20the%20number%20of%20training%20parameters%0Aand%20accelerate%20the%20training%20process.%20Additionally%2C%20we%20designed%20a%20comprehensive%0Aimage-text%20information%20fusion%20structure%20to%20ensure%20thorough%20integration%20of%20the%0Atwo%20modalities%20of%20data.%20We%20evaluated%20TG-LMM%20on%20three%20authoritative%20medical%0Aimage%20datasets%2C%20encompassing%20the%20segmentation%20of%20various%20parts%20of%20the%20human%0Abody.%20Our%20method%20demonstrated%20superior%20performance%20compared%20to%20existing%0Aapproaches%2C%20such%20as%20MedSAM%2C%20SAM%20and%20nnUnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTG-LMM%253A%2520Enhancing%2520Medical%2520Image%2520Segmentation%2520Accuracy%2520through%250A%2520%2520Text-Guided%2520Large%2520Multi-Modal%2520Model%26entry.906535625%3DYihao%2520Zhao%2520and%2520Enhao%2520Zhong%2520and%2520Cuiyun%2520Yuan%2520and%2520Yang%2520Li%2520and%2520Man%2520Zhao%2520and%2520Chunxia%2520Li%2520and%2520Jun%2520Hu%2520and%2520Chenbin%2520Liu%26entry.1292438233%3D%2520%2520We%2520propose%2520TG-LMM%2520%2528Text-Guided%2520Large%2520Multi-Modal%2520Model%2529%252C%2520a%2520novel%2520approach%250Athat%2520leverages%2520textual%2520descriptions%2520of%2520organs%2520to%2520enhance%2520segmentation%2520accuracy%250Ain%2520medical%2520images.%2520Existing%2520medical%2520image%2520segmentation%2520methods%2520face%2520several%250Achallenges%253A%2520current%2520medical%2520automatic%2520segmentation%2520models%2520do%2520not%2520effectively%250Autilize%2520prior%2520knowledge%252C%2520such%2520as%2520descriptions%2520of%2520organ%2520locations%253B%2520previous%250Atext-visual%2520models%2520focus%2520on%2520identifying%2520the%2520target%2520rather%2520than%2520improving%2520the%250Asegmentation%2520accuracy%253B%2520prior%2520models%2520attempt%2520to%2520use%2520prior%2520knowledge%2520to%2520enhance%250Aaccuracy%2520but%2520do%2520not%2520incorporate%2520pre-trained%2520models.%2520To%2520address%2520these%2520issues%252C%250ATG-LMM%2520integrates%2520prior%2520knowledge%252C%2520specifically%2520expert%2520descriptions%2520of%2520the%250Aspatial%2520locations%2520of%2520organs%252C%2520into%2520the%2520segmentation%2520process.%2520Our%2520model%2520utilizes%250Apre-trained%2520image%2520and%2520text%2520encoders%2520to%2520reduce%2520the%2520number%2520of%2520training%2520parameters%250Aand%2520accelerate%2520the%2520training%2520process.%2520Additionally%252C%2520we%2520designed%2520a%2520comprehensive%250Aimage-text%2520information%2520fusion%2520structure%2520to%2520ensure%2520thorough%2520integration%2520of%2520the%250Atwo%2520modalities%2520of%2520data.%2520We%2520evaluated%2520TG-LMM%2520on%2520three%2520authoritative%2520medical%250Aimage%2520datasets%252C%2520encompassing%2520the%2520segmentation%2520of%2520various%2520parts%2520of%2520the%2520human%250Abody.%2520Our%2520method%2520demonstrated%2520superior%2520performance%2520compared%2520to%2520existing%250Aapproaches%252C%2520such%2520as%2520MedSAM%252C%2520SAM%2520and%2520nnUnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TG-LMM%3A%20Enhancing%20Medical%20Image%20Segmentation%20Accuracy%20through%0A%20%20Text-Guided%20Large%20Multi-Modal%20Model&entry.906535625=Yihao%20Zhao%20and%20Enhao%20Zhong%20and%20Cuiyun%20Yuan%20and%20Yang%20Li%20and%20Man%20Zhao%20and%20Chunxia%20Li%20and%20Jun%20Hu%20and%20Chenbin%20Liu&entry.1292438233=%20%20We%20propose%20TG-LMM%20%28Text-Guided%20Large%20Multi-Modal%20Model%29%2C%20a%20novel%20approach%0Athat%20leverages%20textual%20descriptions%20of%20organs%20to%20enhance%20segmentation%20accuracy%0Ain%20medical%20images.%20Existing%20medical%20image%20segmentation%20methods%20face%20several%0Achallenges%3A%20current%20medical%20automatic%20segmentation%20models%20do%20not%20effectively%0Autilize%20prior%20knowledge%2C%20such%20as%20descriptions%20of%20organ%20locations%3B%20previous%0Atext-visual%20models%20focus%20on%20identifying%20the%20target%20rather%20than%20improving%20the%0Asegmentation%20accuracy%3B%20prior%20models%20attempt%20to%20use%20prior%20knowledge%20to%20enhance%0Aaccuracy%20but%20do%20not%20incorporate%20pre-trained%20models.%20To%20address%20these%20issues%2C%0ATG-LMM%20integrates%20prior%20knowledge%2C%20specifically%20expert%20descriptions%20of%20the%0Aspatial%20locations%20of%20organs%2C%20into%20the%20segmentation%20process.%20Our%20model%20utilizes%0Apre-trained%20image%20and%20text%20encoders%20to%20reduce%20the%20number%20of%20training%20parameters%0Aand%20accelerate%20the%20training%20process.%20Additionally%2C%20we%20designed%20a%20comprehensive%0Aimage-text%20information%20fusion%20structure%20to%20ensure%20thorough%20integration%20of%20the%0Atwo%20modalities%20of%20data.%20We%20evaluated%20TG-LMM%20on%20three%20authoritative%20medical%0Aimage%20datasets%2C%20encompassing%20the%20segmentation%20of%20various%20parts%20of%20the%20human%0Abody.%20Our%20method%20demonstrated%20superior%20performance%20compared%20to%20existing%0Aapproaches%2C%20such%20as%20MedSAM%2C%20SAM%20and%20nnUnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03412v1&entry.124074799=Read"},
{"title": "Reprogrammable sequencing for physically intelligent under-actuated\n  robots", "author": "Leon M. Kamp and Mohamed Zanaty and Ahmad Zareei and Benjamin Gorissen and Robert J. Wood and Katia Bertoldi", "abstract": "  Programming physical intelligence into mechanisms holds great promise for\nmachines that can accomplish tasks such as navigation of unstructured\nenvironments while utilizing a minimal amount of computational resources and\nelectronic components. In this study, we introduce a novel design approach for\nphysically intelligent under-actuated mechanisms capable of autonomously\nadjusting their motion in response to environmental interactions. Specifically,\nmultistability is harnessed to sequence the motion of different degrees of\nfreedom in a programmed order. A key aspect of this approach is that these\nsequences can be passively reprogrammed through mechanical stimuli that arise\nfrom interactions with the environment. To showcase our approach, we construct\na four degree of freedom robot capable of autonomously navigating mazes and\nmoving away from obstacles. Remarkably, this robot operates without relying on\ntraditional computational architectures and utilizes only a single linear\nactuator.\n", "link": "http://arxiv.org/abs/2409.03737v1", "date": "2024-09-05", "relevancy": 1.744, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6099}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reprogrammable%20sequencing%20for%20physically%20intelligent%20under-actuated%0A%20%20robots&body=Title%3A%20Reprogrammable%20sequencing%20for%20physically%20intelligent%20under-actuated%0A%20%20robots%0AAuthor%3A%20Leon%20M.%20Kamp%20and%20Mohamed%20Zanaty%20and%20Ahmad%20Zareei%20and%20Benjamin%20Gorissen%20and%20Robert%20J.%20Wood%20and%20Katia%20Bertoldi%0AAbstract%3A%20%20%20Programming%20physical%20intelligence%20into%20mechanisms%20holds%20great%20promise%20for%0Amachines%20that%20can%20accomplish%20tasks%20such%20as%20navigation%20of%20unstructured%0Aenvironments%20while%20utilizing%20a%20minimal%20amount%20of%20computational%20resources%20and%0Aelectronic%20components.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20design%20approach%20for%0Aphysically%20intelligent%20under-actuated%20mechanisms%20capable%20of%20autonomously%0Aadjusting%20their%20motion%20in%20response%20to%20environmental%20interactions.%20Specifically%2C%0Amultistability%20is%20harnessed%20to%20sequence%20the%20motion%20of%20different%20degrees%20of%0Afreedom%20in%20a%20programmed%20order.%20A%20key%20aspect%20of%20this%20approach%20is%20that%20these%0Asequences%20can%20be%20passively%20reprogrammed%20through%20mechanical%20stimuli%20that%20arise%0Afrom%20interactions%20with%20the%20environment.%20To%20showcase%20our%20approach%2C%20we%20construct%0Aa%20four%20degree%20of%20freedom%20robot%20capable%20of%20autonomously%20navigating%20mazes%20and%0Amoving%20away%20from%20obstacles.%20Remarkably%2C%20this%20robot%20operates%20without%20relying%20on%0Atraditional%20computational%20architectures%20and%20utilizes%20only%20a%20single%20linear%0Aactuator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReprogrammable%2520sequencing%2520for%2520physically%2520intelligent%2520under-actuated%250A%2520%2520robots%26entry.906535625%3DLeon%2520M.%2520Kamp%2520and%2520Mohamed%2520Zanaty%2520and%2520Ahmad%2520Zareei%2520and%2520Benjamin%2520Gorissen%2520and%2520Robert%2520J.%2520Wood%2520and%2520Katia%2520Bertoldi%26entry.1292438233%3D%2520%2520Programming%2520physical%2520intelligence%2520into%2520mechanisms%2520holds%2520great%2520promise%2520for%250Amachines%2520that%2520can%2520accomplish%2520tasks%2520such%2520as%2520navigation%2520of%2520unstructured%250Aenvironments%2520while%2520utilizing%2520a%2520minimal%2520amount%2520of%2520computational%2520resources%2520and%250Aelectronic%2520components.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520design%2520approach%2520for%250Aphysically%2520intelligent%2520under-actuated%2520mechanisms%2520capable%2520of%2520autonomously%250Aadjusting%2520their%2520motion%2520in%2520response%2520to%2520environmental%2520interactions.%2520Specifically%252C%250Amultistability%2520is%2520harnessed%2520to%2520sequence%2520the%2520motion%2520of%2520different%2520degrees%2520of%250Afreedom%2520in%2520a%2520programmed%2520order.%2520A%2520key%2520aspect%2520of%2520this%2520approach%2520is%2520that%2520these%250Asequences%2520can%2520be%2520passively%2520reprogrammed%2520through%2520mechanical%2520stimuli%2520that%2520arise%250Afrom%2520interactions%2520with%2520the%2520environment.%2520To%2520showcase%2520our%2520approach%252C%2520we%2520construct%250Aa%2520four%2520degree%2520of%2520freedom%2520robot%2520capable%2520of%2520autonomously%2520navigating%2520mazes%2520and%250Amoving%2520away%2520from%2520obstacles.%2520Remarkably%252C%2520this%2520robot%2520operates%2520without%2520relying%2520on%250Atraditional%2520computational%2520architectures%2520and%2520utilizes%2520only%2520a%2520single%2520linear%250Aactuator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reprogrammable%20sequencing%20for%20physically%20intelligent%20under-actuated%0A%20%20robots&entry.906535625=Leon%20M.%20Kamp%20and%20Mohamed%20Zanaty%20and%20Ahmad%20Zareei%20and%20Benjamin%20Gorissen%20and%20Robert%20J.%20Wood%20and%20Katia%20Bertoldi&entry.1292438233=%20%20Programming%20physical%20intelligence%20into%20mechanisms%20holds%20great%20promise%20for%0Amachines%20that%20can%20accomplish%20tasks%20such%20as%20navigation%20of%20unstructured%0Aenvironments%20while%20utilizing%20a%20minimal%20amount%20of%20computational%20resources%20and%0Aelectronic%20components.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20design%20approach%20for%0Aphysically%20intelligent%20under-actuated%20mechanisms%20capable%20of%20autonomously%0Aadjusting%20their%20motion%20in%20response%20to%20environmental%20interactions.%20Specifically%2C%0Amultistability%20is%20harnessed%20to%20sequence%20the%20motion%20of%20different%20degrees%20of%0Afreedom%20in%20a%20programmed%20order.%20A%20key%20aspect%20of%20this%20approach%20is%20that%20these%0Asequences%20can%20be%20passively%20reprogrammed%20through%20mechanical%20stimuli%20that%20arise%0Afrom%20interactions%20with%20the%20environment.%20To%20showcase%20our%20approach%2C%20we%20construct%0Aa%20four%20degree%20of%20freedom%20robot%20capable%20of%20autonomously%20navigating%20mazes%20and%0Amoving%20away%20from%20obstacles.%20Remarkably%2C%20this%20robot%20operates%20without%20relying%20on%0Atraditional%20computational%20architectures%20and%20utilizes%20only%20a%20single%20linear%0Aactuator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03737v1&entry.124074799=Read"},
{"title": "MaskVal: Simple but Effective Uncertainty Quantification for 6D Pose\n  Estimation", "author": "Philipp Quentin and Daniel Goehring", "abstract": "  For the use of 6D pose estimation in robotic applications, reliable poses are\nof utmost importance to ensure a safe, reliable and predictable operational\nperformance. Despite these requirements, state-of-the-art 6D pose estimators\noften do not provide any uncertainty quantification for their pose estimates at\nall, or if they do, it has been shown that the uncertainty provided is only\nweakly correlated with the actual true error. To address this issue, we\ninvestigate a simple but effective uncertainty quantification, that we call\nMaskVal, which compares the pose estimates with their corresponding instance\nsegmentations by rendering and does not require any modification of the pose\nestimator itself. Despite its simplicity, MaskVal significantly outperforms a\nstate-of-the-art ensemble method on both a dataset and a robotic setup. We show\nthat by using MaskVal, the performance of a state-of-the-art 6D pose estimator\nis significantly improved towards a safe and reliable operation. In addition,\nwe propose a new and specific approach to compare and evaluate uncertainty\nquantification methods for 6D pose estimation in the context of robotic\nmanipulation.\n", "link": "http://arxiv.org/abs/2409.03556v1", "date": "2024-09-05", "relevancy": 1.7356, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5864}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskVal%3A%20Simple%20but%20Effective%20Uncertainty%20Quantification%20for%206D%20Pose%0A%20%20Estimation&body=Title%3A%20MaskVal%3A%20Simple%20but%20Effective%20Uncertainty%20Quantification%20for%206D%20Pose%0A%20%20Estimation%0AAuthor%3A%20Philipp%20Quentin%20and%20Daniel%20Goehring%0AAbstract%3A%20%20%20For%20the%20use%20of%206D%20pose%20estimation%20in%20robotic%20applications%2C%20reliable%20poses%20are%0Aof%20utmost%20importance%20to%20ensure%20a%20safe%2C%20reliable%20and%20predictable%20operational%0Aperformance.%20Despite%20these%20requirements%2C%20state-of-the-art%206D%20pose%20estimators%0Aoften%20do%20not%20provide%20any%20uncertainty%20quantification%20for%20their%20pose%20estimates%20at%0Aall%2C%20or%20if%20they%20do%2C%20it%20has%20been%20shown%20that%20the%20uncertainty%20provided%20is%20only%0Aweakly%20correlated%20with%20the%20actual%20true%20error.%20To%20address%20this%20issue%2C%20we%0Ainvestigate%20a%20simple%20but%20effective%20uncertainty%20quantification%2C%20that%20we%20call%0AMaskVal%2C%20which%20compares%20the%20pose%20estimates%20with%20their%20corresponding%20instance%0Asegmentations%20by%20rendering%20and%20does%20not%20require%20any%20modification%20of%20the%20pose%0Aestimator%20itself.%20Despite%20its%20simplicity%2C%20MaskVal%20significantly%20outperforms%20a%0Astate-of-the-art%20ensemble%20method%20on%20both%20a%20dataset%20and%20a%20robotic%20setup.%20We%20show%0Athat%20by%20using%20MaskVal%2C%20the%20performance%20of%20a%20state-of-the-art%206D%20pose%20estimator%0Ais%20significantly%20improved%20towards%20a%20safe%20and%20reliable%20operation.%20In%20addition%2C%0Awe%20propose%20a%20new%20and%20specific%20approach%20to%20compare%20and%20evaluate%20uncertainty%0Aquantification%20methods%20for%206D%20pose%20estimation%20in%20the%20context%20of%20robotic%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskVal%253A%2520Simple%2520but%2520Effective%2520Uncertainty%2520Quantification%2520for%25206D%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DPhilipp%2520Quentin%2520and%2520Daniel%2520Goehring%26entry.1292438233%3D%2520%2520For%2520the%2520use%2520of%25206D%2520pose%2520estimation%2520in%2520robotic%2520applications%252C%2520reliable%2520poses%2520are%250Aof%2520utmost%2520importance%2520to%2520ensure%2520a%2520safe%252C%2520reliable%2520and%2520predictable%2520operational%250Aperformance.%2520Despite%2520these%2520requirements%252C%2520state-of-the-art%25206D%2520pose%2520estimators%250Aoften%2520do%2520not%2520provide%2520any%2520uncertainty%2520quantification%2520for%2520their%2520pose%2520estimates%2520at%250Aall%252C%2520or%2520if%2520they%2520do%252C%2520it%2520has%2520been%2520shown%2520that%2520the%2520uncertainty%2520provided%2520is%2520only%250Aweakly%2520correlated%2520with%2520the%2520actual%2520true%2520error.%2520To%2520address%2520this%2520issue%252C%2520we%250Ainvestigate%2520a%2520simple%2520but%2520effective%2520uncertainty%2520quantification%252C%2520that%2520we%2520call%250AMaskVal%252C%2520which%2520compares%2520the%2520pose%2520estimates%2520with%2520their%2520corresponding%2520instance%250Asegmentations%2520by%2520rendering%2520and%2520does%2520not%2520require%2520any%2520modification%2520of%2520the%2520pose%250Aestimator%2520itself.%2520Despite%2520its%2520simplicity%252C%2520MaskVal%2520significantly%2520outperforms%2520a%250Astate-of-the-art%2520ensemble%2520method%2520on%2520both%2520a%2520dataset%2520and%2520a%2520robotic%2520setup.%2520We%2520show%250Athat%2520by%2520using%2520MaskVal%252C%2520the%2520performance%2520of%2520a%2520state-of-the-art%25206D%2520pose%2520estimator%250Ais%2520significantly%2520improved%2520towards%2520a%2520safe%2520and%2520reliable%2520operation.%2520In%2520addition%252C%250Awe%2520propose%2520a%2520new%2520and%2520specific%2520approach%2520to%2520compare%2520and%2520evaluate%2520uncertainty%250Aquantification%2520methods%2520for%25206D%2520pose%2520estimation%2520in%2520the%2520context%2520of%2520robotic%250Amanipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskVal%3A%20Simple%20but%20Effective%20Uncertainty%20Quantification%20for%206D%20Pose%0A%20%20Estimation&entry.906535625=Philipp%20Quentin%20and%20Daniel%20Goehring&entry.1292438233=%20%20For%20the%20use%20of%206D%20pose%20estimation%20in%20robotic%20applications%2C%20reliable%20poses%20are%0Aof%20utmost%20importance%20to%20ensure%20a%20safe%2C%20reliable%20and%20predictable%20operational%0Aperformance.%20Despite%20these%20requirements%2C%20state-of-the-art%206D%20pose%20estimators%0Aoften%20do%20not%20provide%20any%20uncertainty%20quantification%20for%20their%20pose%20estimates%20at%0Aall%2C%20or%20if%20they%20do%2C%20it%20has%20been%20shown%20that%20the%20uncertainty%20provided%20is%20only%0Aweakly%20correlated%20with%20the%20actual%20true%20error.%20To%20address%20this%20issue%2C%20we%0Ainvestigate%20a%20simple%20but%20effective%20uncertainty%20quantification%2C%20that%20we%20call%0AMaskVal%2C%20which%20compares%20the%20pose%20estimates%20with%20their%20corresponding%20instance%0Asegmentations%20by%20rendering%20and%20does%20not%20require%20any%20modification%20of%20the%20pose%0Aestimator%20itself.%20Despite%20its%20simplicity%2C%20MaskVal%20significantly%20outperforms%20a%0Astate-of-the-art%20ensemble%20method%20on%20both%20a%20dataset%20and%20a%20robotic%20setup.%20We%20show%0Athat%20by%20using%20MaskVal%2C%20the%20performance%20of%20a%20state-of-the-art%206D%20pose%20estimator%0Ais%20significantly%20improved%20towards%20a%20safe%20and%20reliable%20operation.%20In%20addition%2C%0Awe%20propose%20a%20new%20and%20specific%20approach%20to%20compare%20and%20evaluate%20uncertainty%0Aquantification%20methods%20for%206D%20pose%20estimation%20in%20the%20context%20of%20robotic%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03556v1&entry.124074799=Read"},
{"title": "MimicTouch: Leveraging Multi-modal Human Tactile Demonstrations for\n  Contact-rich Manipulation", "author": "Kelin Yu and Yunhai Han and Qixian Wang and Vaibhav Saxena and Danfei Xu and Ye Zhao", "abstract": "  Tactile sensing is critical to fine-grained, contact-rich manipulation tasks,\nsuch as insertion and assembly. Prior research has shown the possibility of\nlearning tactile-guided policy from teleoperated demonstration data. However,\nto provide the demonstration, human users often rely on visual feedback to\ncontrol the robot. This creates a gap between the sensing modality used for\ncontrolling the robot (visual) and the modality of interest (tactile). To\nbridge this gap, we introduce \"MimicTouch\", a novel framework for learning\npolicies directly from demonstrations provided by human users with their hands.\nThe key innovations are i) a human tactile data collection system which\ncollects multi-modal tactile dataset for learning human's tactile-guided\ncontrol strategy, ii) an imitation learning-based framework for learning\nhuman's tactile-guided control strategy through such data, and iii) an online\nresidual RL framework to bridge the embodiment gap between the human hand and\nthe robot gripper. Through comprehensive experiments, we highlight the efficacy\nof utilizing human's tactile-guided control strategy to resolve contact-rich\nmanipulation tasks. The project website is at\nhttps://sites.google.com/view/MimicTouch.\n", "link": "http://arxiv.org/abs/2310.16917v3", "date": "2024-09-05", "relevancy": 1.7277, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6121}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5775}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MimicTouch%3A%20Leveraging%20Multi-modal%20Human%20Tactile%20Demonstrations%20for%0A%20%20Contact-rich%20Manipulation&body=Title%3A%20MimicTouch%3A%20Leveraging%20Multi-modal%20Human%20Tactile%20Demonstrations%20for%0A%20%20Contact-rich%20Manipulation%0AAuthor%3A%20Kelin%20Yu%20and%20Yunhai%20Han%20and%20Qixian%20Wang%20and%20Vaibhav%20Saxena%20and%20Danfei%20Xu%20and%20Ye%20Zhao%0AAbstract%3A%20%20%20Tactile%20sensing%20is%20critical%20to%20fine-grained%2C%20contact-rich%20manipulation%20tasks%2C%0Asuch%20as%20insertion%20and%20assembly.%20Prior%20research%20has%20shown%20the%20possibility%20of%0Alearning%20tactile-guided%20policy%20from%20teleoperated%20demonstration%20data.%20However%2C%0Ato%20provide%20the%20demonstration%2C%20human%20users%20often%20rely%20on%20visual%20feedback%20to%0Acontrol%20the%20robot.%20This%20creates%20a%20gap%20between%20the%20sensing%20modality%20used%20for%0Acontrolling%20the%20robot%20%28visual%29%20and%20the%20modality%20of%20interest%20%28tactile%29.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20%22MimicTouch%22%2C%20a%20novel%20framework%20for%20learning%0Apolicies%20directly%20from%20demonstrations%20provided%20by%20human%20users%20with%20their%20hands.%0AThe%20key%20innovations%20are%20i%29%20a%20human%20tactile%20data%20collection%20system%20which%0Acollects%20multi-modal%20tactile%20dataset%20for%20learning%20human%27s%20tactile-guided%0Acontrol%20strategy%2C%20ii%29%20an%20imitation%20learning-based%20framework%20for%20learning%0Ahuman%27s%20tactile-guided%20control%20strategy%20through%20such%20data%2C%20and%20iii%29%20an%20online%0Aresidual%20RL%20framework%20to%20bridge%20the%20embodiment%20gap%20between%20the%20human%20hand%20and%0Athe%20robot%20gripper.%20Through%20comprehensive%20experiments%2C%20we%20highlight%20the%20efficacy%0Aof%20utilizing%20human%27s%20tactile-guided%20control%20strategy%20to%20resolve%20contact-rich%0Amanipulation%20tasks.%20The%20project%20website%20is%20at%0Ahttps%3A//sites.google.com/view/MimicTouch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16917v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimicTouch%253A%2520Leveraging%2520Multi-modal%2520Human%2520Tactile%2520Demonstrations%2520for%250A%2520%2520Contact-rich%2520Manipulation%26entry.906535625%3DKelin%2520Yu%2520and%2520Yunhai%2520Han%2520and%2520Qixian%2520Wang%2520and%2520Vaibhav%2520Saxena%2520and%2520Danfei%2520Xu%2520and%2520Ye%2520Zhao%26entry.1292438233%3D%2520%2520Tactile%2520sensing%2520is%2520critical%2520to%2520fine-grained%252C%2520contact-rich%2520manipulation%2520tasks%252C%250Asuch%2520as%2520insertion%2520and%2520assembly.%2520Prior%2520research%2520has%2520shown%2520the%2520possibility%2520of%250Alearning%2520tactile-guided%2520policy%2520from%2520teleoperated%2520demonstration%2520data.%2520However%252C%250Ato%2520provide%2520the%2520demonstration%252C%2520human%2520users%2520often%2520rely%2520on%2520visual%2520feedback%2520to%250Acontrol%2520the%2520robot.%2520This%2520creates%2520a%2520gap%2520between%2520the%2520sensing%2520modality%2520used%2520for%250Acontrolling%2520the%2520robot%2520%2528visual%2529%2520and%2520the%2520modality%2520of%2520interest%2520%2528tactile%2529.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520%2522MimicTouch%2522%252C%2520a%2520novel%2520framework%2520for%2520learning%250Apolicies%2520directly%2520from%2520demonstrations%2520provided%2520by%2520human%2520users%2520with%2520their%2520hands.%250AThe%2520key%2520innovations%2520are%2520i%2529%2520a%2520human%2520tactile%2520data%2520collection%2520system%2520which%250Acollects%2520multi-modal%2520tactile%2520dataset%2520for%2520learning%2520human%2527s%2520tactile-guided%250Acontrol%2520strategy%252C%2520ii%2529%2520an%2520imitation%2520learning-based%2520framework%2520for%2520learning%250Ahuman%2527s%2520tactile-guided%2520control%2520strategy%2520through%2520such%2520data%252C%2520and%2520iii%2529%2520an%2520online%250Aresidual%2520RL%2520framework%2520to%2520bridge%2520the%2520embodiment%2520gap%2520between%2520the%2520human%2520hand%2520and%250Athe%2520robot%2520gripper.%2520Through%2520comprehensive%2520experiments%252C%2520we%2520highlight%2520the%2520efficacy%250Aof%2520utilizing%2520human%2527s%2520tactile-guided%2520control%2520strategy%2520to%2520resolve%2520contact-rich%250Amanipulation%2520tasks.%2520The%2520project%2520website%2520is%2520at%250Ahttps%253A//sites.google.com/view/MimicTouch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16917v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MimicTouch%3A%20Leveraging%20Multi-modal%20Human%20Tactile%20Demonstrations%20for%0A%20%20Contact-rich%20Manipulation&entry.906535625=Kelin%20Yu%20and%20Yunhai%20Han%20and%20Qixian%20Wang%20and%20Vaibhav%20Saxena%20and%20Danfei%20Xu%20and%20Ye%20Zhao&entry.1292438233=%20%20Tactile%20sensing%20is%20critical%20to%20fine-grained%2C%20contact-rich%20manipulation%20tasks%2C%0Asuch%20as%20insertion%20and%20assembly.%20Prior%20research%20has%20shown%20the%20possibility%20of%0Alearning%20tactile-guided%20policy%20from%20teleoperated%20demonstration%20data.%20However%2C%0Ato%20provide%20the%20demonstration%2C%20human%20users%20often%20rely%20on%20visual%20feedback%20to%0Acontrol%20the%20robot.%20This%20creates%20a%20gap%20between%20the%20sensing%20modality%20used%20for%0Acontrolling%20the%20robot%20%28visual%29%20and%20the%20modality%20of%20interest%20%28tactile%29.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20%22MimicTouch%22%2C%20a%20novel%20framework%20for%20learning%0Apolicies%20directly%20from%20demonstrations%20provided%20by%20human%20users%20with%20their%20hands.%0AThe%20key%20innovations%20are%20i%29%20a%20human%20tactile%20data%20collection%20system%20which%0Acollects%20multi-modal%20tactile%20dataset%20for%20learning%20human%27s%20tactile-guided%0Acontrol%20strategy%2C%20ii%29%20an%20imitation%20learning-based%20framework%20for%20learning%0Ahuman%27s%20tactile-guided%20control%20strategy%20through%20such%20data%2C%20and%20iii%29%20an%20online%0Aresidual%20RL%20framework%20to%20bridge%20the%20embodiment%20gap%20between%20the%20human%20hand%20and%0Athe%20robot%20gripper.%20Through%20comprehensive%20experiments%2C%20we%20highlight%20the%20efficacy%0Aof%20utilizing%20human%27s%20tactile-guided%20control%20strategy%20to%20resolve%20contact-rich%0Amanipulation%20tasks.%20The%20project%20website%20is%20at%0Ahttps%3A//sites.google.com/view/MimicTouch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16917v3&entry.124074799=Read"},
{"title": "Interactive Surgical Liver Phantom for Cholecystectomy Training", "author": "Alexander Schuessler and Rayan Younis and Jamie Paik and Martin Wagner and Franziska Mathis-Ullrich and Christian Kunz", "abstract": "  Training and prototype development in robot-assisted surgery requires\nappropriate and safe environments for the execution of surgical procedures.\nCurrent dry lab laparoscopy phantoms often lack the ability to mimic complex,\ninteractive surgical tasks. This work presents an interactive surgical phantom\nfor the cholecystectomy. The phantom enables the removal of the gallbladder\nduring cholecystectomy by allowing manipulations and cutting interactions with\nthe synthetic tissue. The force-displacement behavior of the gallbladder is\nmodelled based on retraction demonstrations. The force model is compared to the\nforce model of ex-vivo porcine gallbladders and evaluated on its ability to\nestimate retraction forces.\n", "link": "http://arxiv.org/abs/2409.03535v1", "date": "2024-09-05", "relevancy": 1.7227, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4785}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4219}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Surgical%20Liver%20Phantom%20for%20Cholecystectomy%20Training&body=Title%3A%20Interactive%20Surgical%20Liver%20Phantom%20for%20Cholecystectomy%20Training%0AAuthor%3A%20Alexander%20Schuessler%20and%20Rayan%20Younis%20and%20Jamie%20Paik%20and%20Martin%20Wagner%20and%20Franziska%20Mathis-Ullrich%20and%20Christian%20Kunz%0AAbstract%3A%20%20%20Training%20and%20prototype%20development%20in%20robot-assisted%20surgery%20requires%0Aappropriate%20and%20safe%20environments%20for%20the%20execution%20of%20surgical%20procedures.%0ACurrent%20dry%20lab%20laparoscopy%20phantoms%20often%20lack%20the%20ability%20to%20mimic%20complex%2C%0Ainteractive%20surgical%20tasks.%20This%20work%20presents%20an%20interactive%20surgical%20phantom%0Afor%20the%20cholecystectomy.%20The%20phantom%20enables%20the%20removal%20of%20the%20gallbladder%0Aduring%20cholecystectomy%20by%20allowing%20manipulations%20and%20cutting%20interactions%20with%0Athe%20synthetic%20tissue.%20The%20force-displacement%20behavior%20of%20the%20gallbladder%20is%0Amodelled%20based%20on%20retraction%20demonstrations.%20The%20force%20model%20is%20compared%20to%20the%0Aforce%20model%20of%20ex-vivo%20porcine%20gallbladders%20and%20evaluated%20on%20its%20ability%20to%0Aestimate%20retraction%20forces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Surgical%2520Liver%2520Phantom%2520for%2520Cholecystectomy%2520Training%26entry.906535625%3DAlexander%2520Schuessler%2520and%2520Rayan%2520Younis%2520and%2520Jamie%2520Paik%2520and%2520Martin%2520Wagner%2520and%2520Franziska%2520Mathis-Ullrich%2520and%2520Christian%2520Kunz%26entry.1292438233%3D%2520%2520Training%2520and%2520prototype%2520development%2520in%2520robot-assisted%2520surgery%2520requires%250Aappropriate%2520and%2520safe%2520environments%2520for%2520the%2520execution%2520of%2520surgical%2520procedures.%250ACurrent%2520dry%2520lab%2520laparoscopy%2520phantoms%2520often%2520lack%2520the%2520ability%2520to%2520mimic%2520complex%252C%250Ainteractive%2520surgical%2520tasks.%2520This%2520work%2520presents%2520an%2520interactive%2520surgical%2520phantom%250Afor%2520the%2520cholecystectomy.%2520The%2520phantom%2520enables%2520the%2520removal%2520of%2520the%2520gallbladder%250Aduring%2520cholecystectomy%2520by%2520allowing%2520manipulations%2520and%2520cutting%2520interactions%2520with%250Athe%2520synthetic%2520tissue.%2520The%2520force-displacement%2520behavior%2520of%2520the%2520gallbladder%2520is%250Amodelled%2520based%2520on%2520retraction%2520demonstrations.%2520The%2520force%2520model%2520is%2520compared%2520to%2520the%250Aforce%2520model%2520of%2520ex-vivo%2520porcine%2520gallbladders%2520and%2520evaluated%2520on%2520its%2520ability%2520to%250Aestimate%2520retraction%2520forces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Surgical%20Liver%20Phantom%20for%20Cholecystectomy%20Training&entry.906535625=Alexander%20Schuessler%20and%20Rayan%20Younis%20and%20Jamie%20Paik%20and%20Martin%20Wagner%20and%20Franziska%20Mathis-Ullrich%20and%20Christian%20Kunz&entry.1292438233=%20%20Training%20and%20prototype%20development%20in%20robot-assisted%20surgery%20requires%0Aappropriate%20and%20safe%20environments%20for%20the%20execution%20of%20surgical%20procedures.%0ACurrent%20dry%20lab%20laparoscopy%20phantoms%20often%20lack%20the%20ability%20to%20mimic%20complex%2C%0Ainteractive%20surgical%20tasks.%20This%20work%20presents%20an%20interactive%20surgical%20phantom%0Afor%20the%20cholecystectomy.%20The%20phantom%20enables%20the%20removal%20of%20the%20gallbladder%0Aduring%20cholecystectomy%20by%20allowing%20manipulations%20and%20cutting%20interactions%20with%0Athe%20synthetic%20tissue.%20The%20force-displacement%20behavior%20of%20the%20gallbladder%20is%0Amodelled%20based%20on%20retraction%20demonstrations.%20The%20force%20model%20is%20compared%20to%20the%0Aforce%20model%20of%20ex-vivo%20porcine%20gallbladders%20and%20evaluated%20on%20its%20ability%20to%0Aestimate%20retraction%20forces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03535v1&entry.124074799=Read"},
{"title": "LowFormer: Hardware Efficient Design for Convolutional Transformer\n  Backbones", "author": "Moritz Nottebaum and Matteo Dunnhofer and Christian Micheloni", "abstract": "  Research in efficient vision backbones is evolving into models that are a\nmixture of convolutions and transformer blocks. A smart combination of both,\narchitecture-wise and component-wise is mandatory to excel in the speedaccuracy\ntrade-off. Most publications focus on maximizing accuracy and utilize MACs\n(multiply accumulate operations) as an efficiency metric. The latter however\noften do not measure accurately how fast a model actually is due to factors\nlike memory access cost and degree of parallelism. We analyzed common modules\nand architectural design choices for backbones not in terms of MACs, but rather\nin actual throughput and latency, as the combination of the latter two is a\nbetter representation of the efficiency of models in real applications. We\napplied the conclusions taken from that analysis to create a recipe for\nincreasing hardware-efficiency in macro design. Additionally we introduce a\nsimple slimmed-down version of MultiHead Self-Attention, that aligns with our\nanalysis. We combine both macro and micro design to create a new family of\nhardware-efficient backbone networks called LowFormer. LowFormer achieves a\nremarkable speedup in terms of throughput and latency, while achieving similar\nor better accuracy than current state-of-the-art efficient backbones. In order\nto prove the generalizability of our hardware-efficient design, we evaluate our\nmethod on GPU, mobile GPU and ARM CPU. We further show that the downstream\ntasks object detection and semantic segmentation profit from our\nhardware-efficient architecture. Code and models are available at\nhttps://github.com/ altair199797/LowFormer.\n", "link": "http://arxiv.org/abs/2409.03460v1", "date": "2024-09-05", "relevancy": 1.7177, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5793}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5718}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LowFormer%3A%20Hardware%20Efficient%20Design%20for%20Convolutional%20Transformer%0A%20%20Backbones&body=Title%3A%20LowFormer%3A%20Hardware%20Efficient%20Design%20for%20Convolutional%20Transformer%0A%20%20Backbones%0AAuthor%3A%20Moritz%20Nottebaum%20and%20Matteo%20Dunnhofer%20and%20Christian%20Micheloni%0AAbstract%3A%20%20%20Research%20in%20efficient%20vision%20backbones%20is%20evolving%20into%20models%20that%20are%20a%0Amixture%20of%20convolutions%20and%20transformer%20blocks.%20A%20smart%20combination%20of%20both%2C%0Aarchitecture-wise%20and%20component-wise%20is%20mandatory%20to%20excel%20in%20the%20speedaccuracy%0Atrade-off.%20Most%20publications%20focus%20on%20maximizing%20accuracy%20and%20utilize%20MACs%0A%28multiply%20accumulate%20operations%29%20as%20an%20efficiency%20metric.%20The%20latter%20however%0Aoften%20do%20not%20measure%20accurately%20how%20fast%20a%20model%20actually%20is%20due%20to%20factors%0Alike%20memory%20access%20cost%20and%20degree%20of%20parallelism.%20We%20analyzed%20common%20modules%0Aand%20architectural%20design%20choices%20for%20backbones%20not%20in%20terms%20of%20MACs%2C%20but%20rather%0Ain%20actual%20throughput%20and%20latency%2C%20as%20the%20combination%20of%20the%20latter%20two%20is%20a%0Abetter%20representation%20of%20the%20efficiency%20of%20models%20in%20real%20applications.%20We%0Aapplied%20the%20conclusions%20taken%20from%20that%20analysis%20to%20create%20a%20recipe%20for%0Aincreasing%20hardware-efficiency%20in%20macro%20design.%20Additionally%20we%20introduce%20a%0Asimple%20slimmed-down%20version%20of%20MultiHead%20Self-Attention%2C%20that%20aligns%20with%20our%0Aanalysis.%20We%20combine%20both%20macro%20and%20micro%20design%20to%20create%20a%20new%20family%20of%0Ahardware-efficient%20backbone%20networks%20called%20LowFormer.%20LowFormer%20achieves%20a%0Aremarkable%20speedup%20in%20terms%20of%20throughput%20and%20latency%2C%20while%20achieving%20similar%0Aor%20better%20accuracy%20than%20current%20state-of-the-art%20efficient%20backbones.%20In%20order%0Ato%20prove%20the%20generalizability%20of%20our%20hardware-efficient%20design%2C%20we%20evaluate%20our%0Amethod%20on%20GPU%2C%20mobile%20GPU%20and%20ARM%20CPU.%20We%20further%20show%20that%20the%20downstream%0Atasks%20object%20detection%20and%20semantic%20segmentation%20profit%20from%20our%0Ahardware-efficient%20architecture.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/%20altair199797/LowFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLowFormer%253A%2520Hardware%2520Efficient%2520Design%2520for%2520Convolutional%2520Transformer%250A%2520%2520Backbones%26entry.906535625%3DMoritz%2520Nottebaum%2520and%2520Matteo%2520Dunnhofer%2520and%2520Christian%2520Micheloni%26entry.1292438233%3D%2520%2520Research%2520in%2520efficient%2520vision%2520backbones%2520is%2520evolving%2520into%2520models%2520that%2520are%2520a%250Amixture%2520of%2520convolutions%2520and%2520transformer%2520blocks.%2520A%2520smart%2520combination%2520of%2520both%252C%250Aarchitecture-wise%2520and%2520component-wise%2520is%2520mandatory%2520to%2520excel%2520in%2520the%2520speedaccuracy%250Atrade-off.%2520Most%2520publications%2520focus%2520on%2520maximizing%2520accuracy%2520and%2520utilize%2520MACs%250A%2528multiply%2520accumulate%2520operations%2529%2520as%2520an%2520efficiency%2520metric.%2520The%2520latter%2520however%250Aoften%2520do%2520not%2520measure%2520accurately%2520how%2520fast%2520a%2520model%2520actually%2520is%2520due%2520to%2520factors%250Alike%2520memory%2520access%2520cost%2520and%2520degree%2520of%2520parallelism.%2520We%2520analyzed%2520common%2520modules%250Aand%2520architectural%2520design%2520choices%2520for%2520backbones%2520not%2520in%2520terms%2520of%2520MACs%252C%2520but%2520rather%250Ain%2520actual%2520throughput%2520and%2520latency%252C%2520as%2520the%2520combination%2520of%2520the%2520latter%2520two%2520is%2520a%250Abetter%2520representation%2520of%2520the%2520efficiency%2520of%2520models%2520in%2520real%2520applications.%2520We%250Aapplied%2520the%2520conclusions%2520taken%2520from%2520that%2520analysis%2520to%2520create%2520a%2520recipe%2520for%250Aincreasing%2520hardware-efficiency%2520in%2520macro%2520design.%2520Additionally%2520we%2520introduce%2520a%250Asimple%2520slimmed-down%2520version%2520of%2520MultiHead%2520Self-Attention%252C%2520that%2520aligns%2520with%2520our%250Aanalysis.%2520We%2520combine%2520both%2520macro%2520and%2520micro%2520design%2520to%2520create%2520a%2520new%2520family%2520of%250Ahardware-efficient%2520backbone%2520networks%2520called%2520LowFormer.%2520LowFormer%2520achieves%2520a%250Aremarkable%2520speedup%2520in%2520terms%2520of%2520throughput%2520and%2520latency%252C%2520while%2520achieving%2520similar%250Aor%2520better%2520accuracy%2520than%2520current%2520state-of-the-art%2520efficient%2520backbones.%2520In%2520order%250Ato%2520prove%2520the%2520generalizability%2520of%2520our%2520hardware-efficient%2520design%252C%2520we%2520evaluate%2520our%250Amethod%2520on%2520GPU%252C%2520mobile%2520GPU%2520and%2520ARM%2520CPU.%2520We%2520further%2520show%2520that%2520the%2520downstream%250Atasks%2520object%2520detection%2520and%2520semantic%2520segmentation%2520profit%2520from%2520our%250Ahardware-efficient%2520architecture.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/%2520altair199797/LowFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LowFormer%3A%20Hardware%20Efficient%20Design%20for%20Convolutional%20Transformer%0A%20%20Backbones&entry.906535625=Moritz%20Nottebaum%20and%20Matteo%20Dunnhofer%20and%20Christian%20Micheloni&entry.1292438233=%20%20Research%20in%20efficient%20vision%20backbones%20is%20evolving%20into%20models%20that%20are%20a%0Amixture%20of%20convolutions%20and%20transformer%20blocks.%20A%20smart%20combination%20of%20both%2C%0Aarchitecture-wise%20and%20component-wise%20is%20mandatory%20to%20excel%20in%20the%20speedaccuracy%0Atrade-off.%20Most%20publications%20focus%20on%20maximizing%20accuracy%20and%20utilize%20MACs%0A%28multiply%20accumulate%20operations%29%20as%20an%20efficiency%20metric.%20The%20latter%20however%0Aoften%20do%20not%20measure%20accurately%20how%20fast%20a%20model%20actually%20is%20due%20to%20factors%0Alike%20memory%20access%20cost%20and%20degree%20of%20parallelism.%20We%20analyzed%20common%20modules%0Aand%20architectural%20design%20choices%20for%20backbones%20not%20in%20terms%20of%20MACs%2C%20but%20rather%0Ain%20actual%20throughput%20and%20latency%2C%20as%20the%20combination%20of%20the%20latter%20two%20is%20a%0Abetter%20representation%20of%20the%20efficiency%20of%20models%20in%20real%20applications.%20We%0Aapplied%20the%20conclusions%20taken%20from%20that%20analysis%20to%20create%20a%20recipe%20for%0Aincreasing%20hardware-efficiency%20in%20macro%20design.%20Additionally%20we%20introduce%20a%0Asimple%20slimmed-down%20version%20of%20MultiHead%20Self-Attention%2C%20that%20aligns%20with%20our%0Aanalysis.%20We%20combine%20both%20macro%20and%20micro%20design%20to%20create%20a%20new%20family%20of%0Ahardware-efficient%20backbone%20networks%20called%20LowFormer.%20LowFormer%20achieves%20a%0Aremarkable%20speedup%20in%20terms%20of%20throughput%20and%20latency%2C%20while%20achieving%20similar%0Aor%20better%20accuracy%20than%20current%20state-of-the-art%20efficient%20backbones.%20In%20order%0Ato%20prove%20the%20generalizability%20of%20our%20hardware-efficient%20design%2C%20we%20evaluate%20our%0Amethod%20on%20GPU%2C%20mobile%20GPU%20and%20ARM%20CPU.%20We%20further%20show%20that%20the%20downstream%0Atasks%20object%20detection%20and%20semantic%20segmentation%20profit%20from%20our%0Ahardware-efficient%20architecture.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/%20altair199797/LowFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03460v1&entry.124074799=Read"},
{"title": "RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot\n  Learning", "author": "Lawrence Yunliang Chen and Chenfeng Xu and Karthik Dharmarajan and Zubair Irshad and Richard Cheng and Kurt Keutzer and Masayoshi Tomizuka and Quan Vuong and Ken Goldberg", "abstract": "  Scaling up robot learning requires large and diverse datasets, and how to\nefficiently reuse collected data and transfer policies to new embodiments\nremains an open question. Emerging research such as the Open-X Embodiment (OXE)\nproject has shown promise in leveraging skills by combining datasets including\ndifferent robots. However, imbalances in the distribution of robot types and\ncamera angles in many datasets make policies prone to overfit. To mitigate this\nissue, we propose RoVi-Aug, which leverages state-of-the-art image-to-image\ngenerative models to augment robot data by synthesizing demonstrations with\ndifferent robots and camera views. Through extensive physical experiments, we\nshow that, by training on robot- and viewpoint-augmented data, RoVi-Aug can\nzero-shot deploy on an unseen robot with significantly different camera angles.\nCompared to test-time adaptation algorithms such as Mirage, RoVi-Aug requires\nno extra processing at test time, does not assume known camera angles, and\nallows policy fine-tuning. Moreover, by co-training on both the original and\naugmented robot datasets, RoVi-Aug can learn multi-robot and multi-task\npolicies, enabling more efficient transfer between robots and skills and\nimproving success rates by up to 30%.\n", "link": "http://arxiv.org/abs/2409.03403v1", "date": "2024-09-05", "relevancy": 1.7164, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6083}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5743}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoVi-Aug%3A%20Robot%20and%20Viewpoint%20Augmentation%20for%20Cross-Embodiment%20Robot%0A%20%20Learning&body=Title%3A%20RoVi-Aug%3A%20Robot%20and%20Viewpoint%20Augmentation%20for%20Cross-Embodiment%20Robot%0A%20%20Learning%0AAuthor%3A%20Lawrence%20Yunliang%20Chen%20and%20Chenfeng%20Xu%20and%20Karthik%20Dharmarajan%20and%20Zubair%20Irshad%20and%20Richard%20Cheng%20and%20Kurt%20Keutzer%20and%20Masayoshi%20Tomizuka%20and%20Quan%20Vuong%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20Scaling%20up%20robot%20learning%20requires%20large%20and%20diverse%20datasets%2C%20and%20how%20to%0Aefficiently%20reuse%20collected%20data%20and%20transfer%20policies%20to%20new%20embodiments%0Aremains%20an%20open%20question.%20Emerging%20research%20such%20as%20the%20Open-X%20Embodiment%20%28OXE%29%0Aproject%20has%20shown%20promise%20in%20leveraging%20skills%20by%20combining%20datasets%20including%0Adifferent%20robots.%20However%2C%20imbalances%20in%20the%20distribution%20of%20robot%20types%20and%0Acamera%20angles%20in%20many%20datasets%20make%20policies%20prone%20to%20overfit.%20To%20mitigate%20this%0Aissue%2C%20we%20propose%20RoVi-Aug%2C%20which%20leverages%20state-of-the-art%20image-to-image%0Agenerative%20models%20to%20augment%20robot%20data%20by%20synthesizing%20demonstrations%20with%0Adifferent%20robots%20and%20camera%20views.%20Through%20extensive%20physical%20experiments%2C%20we%0Ashow%20that%2C%20by%20training%20on%20robot-%20and%20viewpoint-augmented%20data%2C%20RoVi-Aug%20can%0Azero-shot%20deploy%20on%20an%20unseen%20robot%20with%20significantly%20different%20camera%20angles.%0ACompared%20to%20test-time%20adaptation%20algorithms%20such%20as%20Mirage%2C%20RoVi-Aug%20requires%0Ano%20extra%20processing%20at%20test%20time%2C%20does%20not%20assume%20known%20camera%20angles%2C%20and%0Aallows%20policy%20fine-tuning.%20Moreover%2C%20by%20co-training%20on%20both%20the%20original%20and%0Aaugmented%20robot%20datasets%2C%20RoVi-Aug%20can%20learn%20multi-robot%20and%20multi-task%0Apolicies%2C%20enabling%20more%20efficient%20transfer%20between%20robots%20and%20skills%20and%0Aimproving%20success%20rates%20by%20up%20to%2030%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoVi-Aug%253A%2520Robot%2520and%2520Viewpoint%2520Augmentation%2520for%2520Cross-Embodiment%2520Robot%250A%2520%2520Learning%26entry.906535625%3DLawrence%2520Yunliang%2520Chen%2520and%2520Chenfeng%2520Xu%2520and%2520Karthik%2520Dharmarajan%2520and%2520Zubair%2520Irshad%2520and%2520Richard%2520Cheng%2520and%2520Kurt%2520Keutzer%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Quan%2520Vuong%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520Scaling%2520up%2520robot%2520learning%2520requires%2520large%2520and%2520diverse%2520datasets%252C%2520and%2520how%2520to%250Aefficiently%2520reuse%2520collected%2520data%2520and%2520transfer%2520policies%2520to%2520new%2520embodiments%250Aremains%2520an%2520open%2520question.%2520Emerging%2520research%2520such%2520as%2520the%2520Open-X%2520Embodiment%2520%2528OXE%2529%250Aproject%2520has%2520shown%2520promise%2520in%2520leveraging%2520skills%2520by%2520combining%2520datasets%2520including%250Adifferent%2520robots.%2520However%252C%2520imbalances%2520in%2520the%2520distribution%2520of%2520robot%2520types%2520and%250Acamera%2520angles%2520in%2520many%2520datasets%2520make%2520policies%2520prone%2520to%2520overfit.%2520To%2520mitigate%2520this%250Aissue%252C%2520we%2520propose%2520RoVi-Aug%252C%2520which%2520leverages%2520state-of-the-art%2520image-to-image%250Agenerative%2520models%2520to%2520augment%2520robot%2520data%2520by%2520synthesizing%2520demonstrations%2520with%250Adifferent%2520robots%2520and%2520camera%2520views.%2520Through%2520extensive%2520physical%2520experiments%252C%2520we%250Ashow%2520that%252C%2520by%2520training%2520on%2520robot-%2520and%2520viewpoint-augmented%2520data%252C%2520RoVi-Aug%2520can%250Azero-shot%2520deploy%2520on%2520an%2520unseen%2520robot%2520with%2520significantly%2520different%2520camera%2520angles.%250ACompared%2520to%2520test-time%2520adaptation%2520algorithms%2520such%2520as%2520Mirage%252C%2520RoVi-Aug%2520requires%250Ano%2520extra%2520processing%2520at%2520test%2520time%252C%2520does%2520not%2520assume%2520known%2520camera%2520angles%252C%2520and%250Aallows%2520policy%2520fine-tuning.%2520Moreover%252C%2520by%2520co-training%2520on%2520both%2520the%2520original%2520and%250Aaugmented%2520robot%2520datasets%252C%2520RoVi-Aug%2520can%2520learn%2520multi-robot%2520and%2520multi-task%250Apolicies%252C%2520enabling%2520more%2520efficient%2520transfer%2520between%2520robots%2520and%2520skills%2520and%250Aimproving%2520success%2520rates%2520by%2520up%2520to%252030%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoVi-Aug%3A%20Robot%20and%20Viewpoint%20Augmentation%20for%20Cross-Embodiment%20Robot%0A%20%20Learning&entry.906535625=Lawrence%20Yunliang%20Chen%20and%20Chenfeng%20Xu%20and%20Karthik%20Dharmarajan%20and%20Zubair%20Irshad%20and%20Richard%20Cheng%20and%20Kurt%20Keutzer%20and%20Masayoshi%20Tomizuka%20and%20Quan%20Vuong%20and%20Ken%20Goldberg&entry.1292438233=%20%20Scaling%20up%20robot%20learning%20requires%20large%20and%20diverse%20datasets%2C%20and%20how%20to%0Aefficiently%20reuse%20collected%20data%20and%20transfer%20policies%20to%20new%20embodiments%0Aremains%20an%20open%20question.%20Emerging%20research%20such%20as%20the%20Open-X%20Embodiment%20%28OXE%29%0Aproject%20has%20shown%20promise%20in%20leveraging%20skills%20by%20combining%20datasets%20including%0Adifferent%20robots.%20However%2C%20imbalances%20in%20the%20distribution%20of%20robot%20types%20and%0Acamera%20angles%20in%20many%20datasets%20make%20policies%20prone%20to%20overfit.%20To%20mitigate%20this%0Aissue%2C%20we%20propose%20RoVi-Aug%2C%20which%20leverages%20state-of-the-art%20image-to-image%0Agenerative%20models%20to%20augment%20robot%20data%20by%20synthesizing%20demonstrations%20with%0Adifferent%20robots%20and%20camera%20views.%20Through%20extensive%20physical%20experiments%2C%20we%0Ashow%20that%2C%20by%20training%20on%20robot-%20and%20viewpoint-augmented%20data%2C%20RoVi-Aug%20can%0Azero-shot%20deploy%20on%20an%20unseen%20robot%20with%20significantly%20different%20camera%20angles.%0ACompared%20to%20test-time%20adaptation%20algorithms%20such%20as%20Mirage%2C%20RoVi-Aug%20requires%0Ano%20extra%20processing%20at%20test%20time%2C%20does%20not%20assume%20known%20camera%20angles%2C%20and%0Aallows%20policy%20fine-tuning.%20Moreover%2C%20by%20co-training%20on%20both%20the%20original%20and%0Aaugmented%20robot%20datasets%2C%20RoVi-Aug%20can%20learn%20multi-robot%20and%20multi-task%0Apolicies%2C%20enabling%20more%20efficient%20transfer%20between%20robots%20and%20skills%20and%0Aimproving%20success%20rates%20by%20up%20to%2030%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03403v1&entry.124074799=Read"},
{"title": "Sparsifying Parametric Models with L0 Regularization", "author": "Nicol\u00f2 Botteghi and Urban Fasel", "abstract": "  This document contains an educational introduction to the problem of\nsparsifying parametric models with L0 regularization. We utilize this approach\ntogether with dictionary learning to learn sparse polynomial policies for deep\nreinforcement learning to control parametric partial differential equations.\nThe code and a tutorial are provided here:\nhttps://github.com/nicob15/Sparsifying-Parametric-Models-with-L0.\n", "link": "http://arxiv.org/abs/2409.03489v1", "date": "2024-09-05", "relevancy": 1.7003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4392}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsifying%20Parametric%20Models%20with%20L0%20Regularization&body=Title%3A%20Sparsifying%20Parametric%20Models%20with%20L0%20Regularization%0AAuthor%3A%20Nicol%C3%B2%20Botteghi%20and%20Urban%20Fasel%0AAbstract%3A%20%20%20This%20document%20contains%20an%20educational%20introduction%20to%20the%20problem%20of%0Asparsifying%20parametric%20models%20with%20L0%20regularization.%20We%20utilize%20this%20approach%0Atogether%20with%20dictionary%20learning%20to%20learn%20sparse%20polynomial%20policies%20for%20deep%0Areinforcement%20learning%20to%20control%20parametric%20partial%20differential%20equations.%0AThe%20code%20and%20a%20tutorial%20are%20provided%20here%3A%0Ahttps%3A//github.com/nicob15/Sparsifying-Parametric-Models-with-L0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsifying%2520Parametric%2520Models%2520with%2520L0%2520Regularization%26entry.906535625%3DNicol%25C3%25B2%2520Botteghi%2520and%2520Urban%2520Fasel%26entry.1292438233%3D%2520%2520This%2520document%2520contains%2520an%2520educational%2520introduction%2520to%2520the%2520problem%2520of%250Asparsifying%2520parametric%2520models%2520with%2520L0%2520regularization.%2520We%2520utilize%2520this%2520approach%250Atogether%2520with%2520dictionary%2520learning%2520to%2520learn%2520sparse%2520polynomial%2520policies%2520for%2520deep%250Areinforcement%2520learning%2520to%2520control%2520parametric%2520partial%2520differential%2520equations.%250AThe%2520code%2520and%2520a%2520tutorial%2520are%2520provided%2520here%253A%250Ahttps%253A//github.com/nicob15/Sparsifying-Parametric-Models-with-L0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsifying%20Parametric%20Models%20with%20L0%20Regularization&entry.906535625=Nicol%C3%B2%20Botteghi%20and%20Urban%20Fasel&entry.1292438233=%20%20This%20document%20contains%20an%20educational%20introduction%20to%20the%20problem%20of%0Asparsifying%20parametric%20models%20with%20L0%20regularization.%20We%20utilize%20this%20approach%0Atogether%20with%20dictionary%20learning%20to%20learn%20sparse%20polynomial%20policies%20for%20deep%0Areinforcement%20learning%20to%20control%20parametric%20partial%20differential%20equations.%0AThe%20code%20and%20a%20tutorial%20are%20provided%20here%3A%0Ahttps%3A//github.com/nicob15/Sparsifying-Parametric-Models-with-L0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03489v1&entry.124074799=Read"},
{"title": "Decision Theoretic Foundations for Experiments Evaluating Human\n  Decisions", "author": "Jessica Hullman and Alex Kale and Jason Hartline", "abstract": "  DeHow well people use information displays to make decisions is of primary\ninterest in human-centered AI, model explainability, data visualization, and\nrelated areas. However, what constitutes a decision problem, and what is\nrequired for a study to establish that human decisions could be improved remain\nopen to speculation. We propose a widely applicable definition of a decision\nproblem synthesized from statistical decision theory and information economics\nas a standard for establishing when human decisions can be improved in HCI. We\nargue that to attribute loss in human performance to forms of bias, an\nexperiment must provide participants with the information that a rational agent\nwould need to identify the utility-maximizing decision. As a demonstration, we\nevaluate the extent to which recent evaluations of decision-making from the\nliterature on AI-assisted decisions achieve these criteria. We find that only\n10 (26\\%) of 39 studies that claim to identify biased behavior present\nparticipants with sufficient information to characterize their behavior as\ndeviating from good decision-making in at least one treatment condition. We\nmotivate the value of studying well-defined decision problems by describing a\ncharacterization of performance losses they allow us to conceive. In contrast,\nthe ambiguities of a poorly communicated decision problem preclude normative\ninterpretation. We conclude with recommendations for practice.\n", "link": "http://arxiv.org/abs/2401.15106v3", "date": "2024-09-05", "relevancy": 1.3544, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4567}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision%20Theoretic%20Foundations%20for%20Experiments%20Evaluating%20Human%0A%20%20Decisions&body=Title%3A%20Decision%20Theoretic%20Foundations%20for%20Experiments%20Evaluating%20Human%0A%20%20Decisions%0AAuthor%3A%20Jessica%20Hullman%20and%20Alex%20Kale%20and%20Jason%20Hartline%0AAbstract%3A%20%20%20DeHow%20well%20people%20use%20information%20displays%20to%20make%20decisions%20is%20of%20primary%0Ainterest%20in%20human-centered%20AI%2C%20model%20explainability%2C%20data%20visualization%2C%20and%0Arelated%20areas.%20However%2C%20what%20constitutes%20a%20decision%20problem%2C%20and%20what%20is%0Arequired%20for%20a%20study%20to%20establish%20that%20human%20decisions%20could%20be%20improved%20remain%0Aopen%20to%20speculation.%20We%20propose%20a%20widely%20applicable%20definition%20of%20a%20decision%0Aproblem%20synthesized%20from%20statistical%20decision%20theory%20and%20information%20economics%0Aas%20a%20standard%20for%20establishing%20when%20human%20decisions%20can%20be%20improved%20in%20HCI.%20We%0Aargue%20that%20to%20attribute%20loss%20in%20human%20performance%20to%20forms%20of%20bias%2C%20an%0Aexperiment%20must%20provide%20participants%20with%20the%20information%20that%20a%20rational%20agent%0Awould%20need%20to%20identify%20the%20utility-maximizing%20decision.%20As%20a%20demonstration%2C%20we%0Aevaluate%20the%20extent%20to%20which%20recent%20evaluations%20of%20decision-making%20from%20the%0Aliterature%20on%20AI-assisted%20decisions%20achieve%20these%20criteria.%20We%20find%20that%20only%0A10%20%2826%5C%25%29%20of%2039%20studies%20that%20claim%20to%20identify%20biased%20behavior%20present%0Aparticipants%20with%20sufficient%20information%20to%20characterize%20their%20behavior%20as%0Adeviating%20from%20good%20decision-making%20in%20at%20least%20one%20treatment%20condition.%20We%0Amotivate%20the%20value%20of%20studying%20well-defined%20decision%20problems%20by%20describing%20a%0Acharacterization%20of%20performance%20losses%20they%20allow%20us%20to%20conceive.%20In%20contrast%2C%0Athe%20ambiguities%20of%20a%20poorly%20communicated%20decision%20problem%20preclude%20normative%0Ainterpretation.%20We%20conclude%20with%20recommendations%20for%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15106v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision%2520Theoretic%2520Foundations%2520for%2520Experiments%2520Evaluating%2520Human%250A%2520%2520Decisions%26entry.906535625%3DJessica%2520Hullman%2520and%2520Alex%2520Kale%2520and%2520Jason%2520Hartline%26entry.1292438233%3D%2520%2520DeHow%2520well%2520people%2520use%2520information%2520displays%2520to%2520make%2520decisions%2520is%2520of%2520primary%250Ainterest%2520in%2520human-centered%2520AI%252C%2520model%2520explainability%252C%2520data%2520visualization%252C%2520and%250Arelated%2520areas.%2520However%252C%2520what%2520constitutes%2520a%2520decision%2520problem%252C%2520and%2520what%2520is%250Arequired%2520for%2520a%2520study%2520to%2520establish%2520that%2520human%2520decisions%2520could%2520be%2520improved%2520remain%250Aopen%2520to%2520speculation.%2520We%2520propose%2520a%2520widely%2520applicable%2520definition%2520of%2520a%2520decision%250Aproblem%2520synthesized%2520from%2520statistical%2520decision%2520theory%2520and%2520information%2520economics%250Aas%2520a%2520standard%2520for%2520establishing%2520when%2520human%2520decisions%2520can%2520be%2520improved%2520in%2520HCI.%2520We%250Aargue%2520that%2520to%2520attribute%2520loss%2520in%2520human%2520performance%2520to%2520forms%2520of%2520bias%252C%2520an%250Aexperiment%2520must%2520provide%2520participants%2520with%2520the%2520information%2520that%2520a%2520rational%2520agent%250Awould%2520need%2520to%2520identify%2520the%2520utility-maximizing%2520decision.%2520As%2520a%2520demonstration%252C%2520we%250Aevaluate%2520the%2520extent%2520to%2520which%2520recent%2520evaluations%2520of%2520decision-making%2520from%2520the%250Aliterature%2520on%2520AI-assisted%2520decisions%2520achieve%2520these%2520criteria.%2520We%2520find%2520that%2520only%250A10%2520%252826%255C%2525%2529%2520of%252039%2520studies%2520that%2520claim%2520to%2520identify%2520biased%2520behavior%2520present%250Aparticipants%2520with%2520sufficient%2520information%2520to%2520characterize%2520their%2520behavior%2520as%250Adeviating%2520from%2520good%2520decision-making%2520in%2520at%2520least%2520one%2520treatment%2520condition.%2520We%250Amotivate%2520the%2520value%2520of%2520studying%2520well-defined%2520decision%2520problems%2520by%2520describing%2520a%250Acharacterization%2520of%2520performance%2520losses%2520they%2520allow%2520us%2520to%2520conceive.%2520In%2520contrast%252C%250Athe%2520ambiguities%2520of%2520a%2520poorly%2520communicated%2520decision%2520problem%2520preclude%2520normative%250Ainterpretation.%2520We%2520conclude%2520with%2520recommendations%2520for%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15106v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision%20Theoretic%20Foundations%20for%20Experiments%20Evaluating%20Human%0A%20%20Decisions&entry.906535625=Jessica%20Hullman%20and%20Alex%20Kale%20and%20Jason%20Hartline&entry.1292438233=%20%20DeHow%20well%20people%20use%20information%20displays%20to%20make%20decisions%20is%20of%20primary%0Ainterest%20in%20human-centered%20AI%2C%20model%20explainability%2C%20data%20visualization%2C%20and%0Arelated%20areas.%20However%2C%20what%20constitutes%20a%20decision%20problem%2C%20and%20what%20is%0Arequired%20for%20a%20study%20to%20establish%20that%20human%20decisions%20could%20be%20improved%20remain%0Aopen%20to%20speculation.%20We%20propose%20a%20widely%20applicable%20definition%20of%20a%20decision%0Aproblem%20synthesized%20from%20statistical%20decision%20theory%20and%20information%20economics%0Aas%20a%20standard%20for%20establishing%20when%20human%20decisions%20can%20be%20improved%20in%20HCI.%20We%0Aargue%20that%20to%20attribute%20loss%20in%20human%20performance%20to%20forms%20of%20bias%2C%20an%0Aexperiment%20must%20provide%20participants%20with%20the%20information%20that%20a%20rational%20agent%0Awould%20need%20to%20identify%20the%20utility-maximizing%20decision.%20As%20a%20demonstration%2C%20we%0Aevaluate%20the%20extent%20to%20which%20recent%20evaluations%20of%20decision-making%20from%20the%0Aliterature%20on%20AI-assisted%20decisions%20achieve%20these%20criteria.%20We%20find%20that%20only%0A10%20%2826%5C%25%29%20of%2039%20studies%20that%20claim%20to%20identify%20biased%20behavior%20present%0Aparticipants%20with%20sufficient%20information%20to%20characterize%20their%20behavior%20as%0Adeviating%20from%20good%20decision-making%20in%20at%20least%20one%20treatment%20condition.%20We%0Amotivate%20the%20value%20of%20studying%20well-defined%20decision%20problems%20by%20describing%20a%0Acharacterization%20of%20performance%20losses%20they%20allow%20us%20to%20conceive.%20In%20contrast%2C%0Athe%20ambiguities%20of%20a%20poorly%20communicated%20decision%20problem%20preclude%20normative%0Ainterpretation.%20We%20conclude%20with%20recommendations%20for%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15106v3&entry.124074799=Read"},
{"title": "Planning In Natural Language Improves LLM Search For Code Generation", "author": "Evan Wang and Federico Cassano and Catherine Wu and Yunfeng Bai and Will Song and Vaskar Nath and Ziwen Han and Sean Hendryx and Summer Yue and Hugh Zhang", "abstract": "  While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas.\n", "link": "http://arxiv.org/abs/2409.03733v1", "date": "2024-09-05", "relevancy": 1.3799, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning%20In%20Natural%20Language%20Improves%20LLM%20Search%20For%20Code%20Generation&body=Title%3A%20Planning%20In%20Natural%20Language%20Improves%20LLM%20Search%20For%20Code%20Generation%0AAuthor%3A%20Evan%20Wang%20and%20Federico%20Cassano%20and%20Catherine%20Wu%20and%20Yunfeng%20Bai%20and%20Will%20Song%20and%20Vaskar%20Nath%20and%20Ziwen%20Han%20and%20Sean%20Hendryx%20and%20Summer%20Yue%20and%20Hugh%20Zhang%0AAbstract%3A%20%20%20While%20scaling%20training%20compute%20has%20led%20to%20remarkable%20improvements%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20scaling%20inference%20compute%20has%20not%20yet%20yielded%20analogous%0Agains.%20We%20hypothesize%20that%20a%20core%20missing%20component%20is%20a%20lack%20of%20diverse%20LLM%0Aoutputs%2C%20leading%20to%20inefficient%20search%20due%20to%20models%20repeatedly%20sampling%20highly%0Asimilar%2C%20yet%20incorrect%20generations.%20We%20empirically%20demonstrate%20that%20this%20lack%0Aof%20diversity%20can%20be%20mitigated%20by%20searching%20over%20candidate%20plans%20for%20solving%20a%0Aproblem%20in%20natural%20language.%20Based%20on%20this%20insight%2C%20we%20propose%20PLANSEARCH%2C%20a%0Anovel%20search%20algorithm%20which%20shows%20strong%20results%20across%20HumanEval%2B%2C%20MBPP%2B%2C%20and%0ALiveCodeBench%20%28a%20contamination-free%20benchmark%20for%20competitive%20coding%29.%0APLANSEARCH%20generates%20a%20diverse%20set%20of%20observations%20about%20the%20problem%20and%20then%0Auses%20these%20observations%20to%20construct%20plans%20for%20solving%20the%20problem.%20By%0Asearching%20over%20plans%20in%20natural%20language%20rather%20than%20directly%20over%20code%0Asolutions%2C%20PLANSEARCH%20explores%20a%20significantly%20more%20diverse%20range%20of%20potential%0Asolutions%20compared%20to%20baseline%20search%20methods.%20Using%20PLANSEARCH%20on%20top%20of%0AClaude%203.5%20Sonnet%20achieves%20a%20state-of-the-art%20pass%40200%20of%2077.0%25%20on%0ALiveCodeBench%2C%20outperforming%20both%20the%20best%20score%20achieved%20without%20search%0A%28pass%401%20%3D%2041.4%25%29%20and%20using%20standard%20repeated%20sampling%20%28pass%40200%20%3D%2060.6%25%29.%0AFinally%2C%20we%20show%20that%2C%20across%20all%20models%2C%20search%20algorithms%2C%20and%20benchmarks%0Aanalyzed%2C%20we%20can%20accurately%20predict%20performance%20gains%20due%20to%20search%20as%20a%20direct%0Afunction%20of%20the%20diversity%20over%20generated%20ideas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning%2520In%2520Natural%2520Language%2520Improves%2520LLM%2520Search%2520For%2520Code%2520Generation%26entry.906535625%3DEvan%2520Wang%2520and%2520Federico%2520Cassano%2520and%2520Catherine%2520Wu%2520and%2520Yunfeng%2520Bai%2520and%2520Will%2520Song%2520and%2520Vaskar%2520Nath%2520and%2520Ziwen%2520Han%2520and%2520Sean%2520Hendryx%2520and%2520Summer%2520Yue%2520and%2520Hugh%2520Zhang%26entry.1292438233%3D%2520%2520While%2520scaling%2520training%2520compute%2520has%2520led%2520to%2520remarkable%2520improvements%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520scaling%2520inference%2520compute%2520has%2520not%2520yet%2520yielded%2520analogous%250Agains.%2520We%2520hypothesize%2520that%2520a%2520core%2520missing%2520component%2520is%2520a%2520lack%2520of%2520diverse%2520LLM%250Aoutputs%252C%2520leading%2520to%2520inefficient%2520search%2520due%2520to%2520models%2520repeatedly%2520sampling%2520highly%250Asimilar%252C%2520yet%2520incorrect%2520generations.%2520We%2520empirically%2520demonstrate%2520that%2520this%2520lack%250Aof%2520diversity%2520can%2520be%2520mitigated%2520by%2520searching%2520over%2520candidate%2520plans%2520for%2520solving%2520a%250Aproblem%2520in%2520natural%2520language.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520PLANSEARCH%252C%2520a%250Anovel%2520search%2520algorithm%2520which%2520shows%2520strong%2520results%2520across%2520HumanEval%252B%252C%2520MBPP%252B%252C%2520and%250ALiveCodeBench%2520%2528a%2520contamination-free%2520benchmark%2520for%2520competitive%2520coding%2529.%250APLANSEARCH%2520generates%2520a%2520diverse%2520set%2520of%2520observations%2520about%2520the%2520problem%2520and%2520then%250Auses%2520these%2520observations%2520to%2520construct%2520plans%2520for%2520solving%2520the%2520problem.%2520By%250Asearching%2520over%2520plans%2520in%2520natural%2520language%2520rather%2520than%2520directly%2520over%2520code%250Asolutions%252C%2520PLANSEARCH%2520explores%2520a%2520significantly%2520more%2520diverse%2520range%2520of%2520potential%250Asolutions%2520compared%2520to%2520baseline%2520search%2520methods.%2520Using%2520PLANSEARCH%2520on%2520top%2520of%250AClaude%25203.5%2520Sonnet%2520achieves%2520a%2520state-of-the-art%2520pass%2540200%2520of%252077.0%2525%2520on%250ALiveCodeBench%252C%2520outperforming%2520both%2520the%2520best%2520score%2520achieved%2520without%2520search%250A%2528pass%25401%2520%253D%252041.4%2525%2529%2520and%2520using%2520standard%2520repeated%2520sampling%2520%2528pass%2540200%2520%253D%252060.6%2525%2529.%250AFinally%252C%2520we%2520show%2520that%252C%2520across%2520all%2520models%252C%2520search%2520algorithms%252C%2520and%2520benchmarks%250Aanalyzed%252C%2520we%2520can%2520accurately%2520predict%2520performance%2520gains%2520due%2520to%2520search%2520as%2520a%2520direct%250Afunction%2520of%2520the%2520diversity%2520over%2520generated%2520ideas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning%20In%20Natural%20Language%20Improves%20LLM%20Search%20For%20Code%20Generation&entry.906535625=Evan%20Wang%20and%20Federico%20Cassano%20and%20Catherine%20Wu%20and%20Yunfeng%20Bai%20and%20Will%20Song%20and%20Vaskar%20Nath%20and%20Ziwen%20Han%20and%20Sean%20Hendryx%20and%20Summer%20Yue%20and%20Hugh%20Zhang&entry.1292438233=%20%20While%20scaling%20training%20compute%20has%20led%20to%20remarkable%20improvements%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20scaling%20inference%20compute%20has%20not%20yet%20yielded%20analogous%0Agains.%20We%20hypothesize%20that%20a%20core%20missing%20component%20is%20a%20lack%20of%20diverse%20LLM%0Aoutputs%2C%20leading%20to%20inefficient%20search%20due%20to%20models%20repeatedly%20sampling%20highly%0Asimilar%2C%20yet%20incorrect%20generations.%20We%20empirically%20demonstrate%20that%20this%20lack%0Aof%20diversity%20can%20be%20mitigated%20by%20searching%20over%20candidate%20plans%20for%20solving%20a%0Aproblem%20in%20natural%20language.%20Based%20on%20this%20insight%2C%20we%20propose%20PLANSEARCH%2C%20a%0Anovel%20search%20algorithm%20which%20shows%20strong%20results%20across%20HumanEval%2B%2C%20MBPP%2B%2C%20and%0ALiveCodeBench%20%28a%20contamination-free%20benchmark%20for%20competitive%20coding%29.%0APLANSEARCH%20generates%20a%20diverse%20set%20of%20observations%20about%20the%20problem%20and%20then%0Auses%20these%20observations%20to%20construct%20plans%20for%20solving%20the%20problem.%20By%0Asearching%20over%20plans%20in%20natural%20language%20rather%20than%20directly%20over%20code%0Asolutions%2C%20PLANSEARCH%20explores%20a%20significantly%20more%20diverse%20range%20of%20potential%0Asolutions%20compared%20to%20baseline%20search%20methods.%20Using%20PLANSEARCH%20on%20top%20of%0AClaude%203.5%20Sonnet%20achieves%20a%20state-of-the-art%20pass%40200%20of%2077.0%25%20on%0ALiveCodeBench%2C%20outperforming%20both%20the%20best%20score%20achieved%20without%20search%0A%28pass%401%20%3D%2041.4%25%29%20and%20using%20standard%20repeated%20sampling%20%28pass%40200%20%3D%2060.6%25%29.%0AFinally%2C%20we%20show%20that%2C%20across%20all%20models%2C%20search%20algorithms%2C%20and%20benchmarks%0Aanalyzed%2C%20we%20can%20accurately%20predict%20performance%20gains%20due%20to%20search%20as%20a%20direct%0Afunction%20of%20the%20diversity%20over%20generated%20ideas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03733v1&entry.124074799=Read"},
{"title": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review", "author": "Banghao Chen and Zhaofeng Zhang and Nicolas Langren\u00e9 and Shengxin Zhu", "abstract": "  This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application.\n", "link": "http://arxiv.org/abs/2310.14735v5", "date": "2024-09-05", "relevancy": 1.3658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4578}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20potential%20of%20prompt%20engineering%20in%20Large%20Language%20Models%3A%0A%20%20a%20comprehensive%20review&body=Title%3A%20Unleashing%20the%20potential%20of%20prompt%20engineering%20in%20Large%20Language%20Models%3A%0A%20%20a%20comprehensive%20review%0AAuthor%3A%20Banghao%20Chen%20and%20Zhaofeng%20Zhang%20and%20Nicolas%20Langren%C3%A9%20and%20Shengxin%20Zhu%0AAbstract%3A%20%20%20This%20comprehensive%20review%20delves%20into%20the%20pivotal%20role%20of%20prompt%20engineering%0Ain%20unleashing%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20development%0Aof%20Artificial%20Intelligence%20%28AI%29%2C%20from%20its%20inception%20in%20the%201950s%20to%20the%0Aemergence%20of%20advanced%20neural%20networks%20and%20deep%20learning%20architectures%2C%20has%20made%0Aa%20breakthrough%20in%20LLMs%2C%20with%20models%20such%20as%20GPT-4o%20and%20Claude-3%2C%20and%20in%0AVision-Language%20Models%20%28VLMs%29%2C%20with%20models%20such%20as%20CLIP%20and%20ALIGN.%20Prompt%0Aengineering%20is%20the%20process%20of%20structuring%20inputs%2C%20which%20has%20emerged%20as%20a%0Acrucial%20technique%20to%20maximize%20the%20utility%20and%20accuracy%20of%20these%20models.%20This%0Apaper%20explores%20both%20foundational%20and%20advanced%20methodologies%20of%20prompt%0Aengineering%2C%20including%20techniques%20such%20as%20self-consistency%2C%20chain-of-thought%2C%0Aand%20generated%20knowledge%2C%20which%20significantly%20enhance%20model%20performance.%0AAdditionally%2C%20it%20examines%20the%20prompt%20method%20of%20VLMs%20through%20innovative%0Aapproaches%20such%20as%20Context%20Optimization%20%28CoOp%29%2C%20Conditional%20Context%0AOptimization%20%28CoCoOp%29%2C%20and%20Multimodal%20Prompt%20Learning%20%28MaPLe%29.%20Critical%20to%20this%0Adiscussion%20is%20the%20aspect%20of%20AI%20security%2C%20particularly%20adversarial%20attacks%20that%0Aexploit%20vulnerabilities%20in%20prompt%20engineering.%20Strategies%20to%20mitigate%20these%0Arisks%20and%20enhance%20model%20robustness%20are%20thoroughly%20reviewed.%20The%20evaluation%20of%0Aprompt%20methods%20is%20also%20addressed%2C%20through%20both%20subjective%20and%20objective%0Ametrics%2C%20ensuring%20a%20robust%20analysis%20of%20their%20efficacy.%20This%20review%20also%0Areflects%20the%20essential%20role%20of%20prompt%20engineering%20in%20advancing%20AI%20capabilities%2C%0Aproviding%20a%20structured%20framework%20for%20future%20research%20and%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14735v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520potential%2520of%2520prompt%2520engineering%2520in%2520Large%2520Language%2520Models%253A%250A%2520%2520a%2520comprehensive%2520review%26entry.906535625%3DBanghao%2520Chen%2520and%2520Zhaofeng%2520Zhang%2520and%2520Nicolas%2520Langren%25C3%25A9%2520and%2520Shengxin%2520Zhu%26entry.1292438233%3D%2520%2520This%2520comprehensive%2520review%2520delves%2520into%2520the%2520pivotal%2520role%2520of%2520prompt%2520engineering%250Ain%2520unleashing%2520the%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520development%250Aof%2520Artificial%2520Intelligence%2520%2528AI%2529%252C%2520from%2520its%2520inception%2520in%2520the%25201950s%2520to%2520the%250Aemergence%2520of%2520advanced%2520neural%2520networks%2520and%2520deep%2520learning%2520architectures%252C%2520has%2520made%250Aa%2520breakthrough%2520in%2520LLMs%252C%2520with%2520models%2520such%2520as%2520GPT-4o%2520and%2520Claude-3%252C%2520and%2520in%250AVision-Language%2520Models%2520%2528VLMs%2529%252C%2520with%2520models%2520such%2520as%2520CLIP%2520and%2520ALIGN.%2520Prompt%250Aengineering%2520is%2520the%2520process%2520of%2520structuring%2520inputs%252C%2520which%2520has%2520emerged%2520as%2520a%250Acrucial%2520technique%2520to%2520maximize%2520the%2520utility%2520and%2520accuracy%2520of%2520these%2520models.%2520This%250Apaper%2520explores%2520both%2520foundational%2520and%2520advanced%2520methodologies%2520of%2520prompt%250Aengineering%252C%2520including%2520techniques%2520such%2520as%2520self-consistency%252C%2520chain-of-thought%252C%250Aand%2520generated%2520knowledge%252C%2520which%2520significantly%2520enhance%2520model%2520performance.%250AAdditionally%252C%2520it%2520examines%2520the%2520prompt%2520method%2520of%2520VLMs%2520through%2520innovative%250Aapproaches%2520such%2520as%2520Context%2520Optimization%2520%2528CoOp%2529%252C%2520Conditional%2520Context%250AOptimization%2520%2528CoCoOp%2529%252C%2520and%2520Multimodal%2520Prompt%2520Learning%2520%2528MaPLe%2529.%2520Critical%2520to%2520this%250Adiscussion%2520is%2520the%2520aspect%2520of%2520AI%2520security%252C%2520particularly%2520adversarial%2520attacks%2520that%250Aexploit%2520vulnerabilities%2520in%2520prompt%2520engineering.%2520Strategies%2520to%2520mitigate%2520these%250Arisks%2520and%2520enhance%2520model%2520robustness%2520are%2520thoroughly%2520reviewed.%2520The%2520evaluation%2520of%250Aprompt%2520methods%2520is%2520also%2520addressed%252C%2520through%2520both%2520subjective%2520and%2520objective%250Ametrics%252C%2520ensuring%2520a%2520robust%2520analysis%2520of%2520their%2520efficacy.%2520This%2520review%2520also%250Areflects%2520the%2520essential%2520role%2520of%2520prompt%2520engineering%2520in%2520advancing%2520AI%2520capabilities%252C%250Aproviding%2520a%2520structured%2520framework%2520for%2520future%2520research%2520and%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14735v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20potential%20of%20prompt%20engineering%20in%20Large%20Language%20Models%3A%0A%20%20a%20comprehensive%20review&entry.906535625=Banghao%20Chen%20and%20Zhaofeng%20Zhang%20and%20Nicolas%20Langren%C3%A9%20and%20Shengxin%20Zhu&entry.1292438233=%20%20This%20comprehensive%20review%20delves%20into%20the%20pivotal%20role%20of%20prompt%20engineering%0Ain%20unleashing%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20development%0Aof%20Artificial%20Intelligence%20%28AI%29%2C%20from%20its%20inception%20in%20the%201950s%20to%20the%0Aemergence%20of%20advanced%20neural%20networks%20and%20deep%20learning%20architectures%2C%20has%20made%0Aa%20breakthrough%20in%20LLMs%2C%20with%20models%20such%20as%20GPT-4o%20and%20Claude-3%2C%20and%20in%0AVision-Language%20Models%20%28VLMs%29%2C%20with%20models%20such%20as%20CLIP%20and%20ALIGN.%20Prompt%0Aengineering%20is%20the%20process%20of%20structuring%20inputs%2C%20which%20has%20emerged%20as%20a%0Acrucial%20technique%20to%20maximize%20the%20utility%20and%20accuracy%20of%20these%20models.%20This%0Apaper%20explores%20both%20foundational%20and%20advanced%20methodologies%20of%20prompt%0Aengineering%2C%20including%20techniques%20such%20as%20self-consistency%2C%20chain-of-thought%2C%0Aand%20generated%20knowledge%2C%20which%20significantly%20enhance%20model%20performance.%0AAdditionally%2C%20it%20examines%20the%20prompt%20method%20of%20VLMs%20through%20innovative%0Aapproaches%20such%20as%20Context%20Optimization%20%28CoOp%29%2C%20Conditional%20Context%0AOptimization%20%28CoCoOp%29%2C%20and%20Multimodal%20Prompt%20Learning%20%28MaPLe%29.%20Critical%20to%20this%0Adiscussion%20is%20the%20aspect%20of%20AI%20security%2C%20particularly%20adversarial%20attacks%20that%0Aexploit%20vulnerabilities%20in%20prompt%20engineering.%20Strategies%20to%20mitigate%20these%0Arisks%20and%20enhance%20model%20robustness%20are%20thoroughly%20reviewed.%20The%20evaluation%20of%0Aprompt%20methods%20is%20also%20addressed%2C%20through%20both%20subjective%20and%20objective%0Ametrics%2C%20ensuring%20a%20robust%20analysis%20of%20their%20efficacy.%20This%20review%20also%0Areflects%20the%20essential%20role%20of%20prompt%20engineering%20in%20advancing%20AI%20capabilities%2C%0Aproviding%20a%20structured%20framework%20for%20future%20research%20and%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14735v5&entry.124074799=Read"},
{"title": "Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to\n  Market Entry", "author": "Meena Jagadeesan and Michael I. Jordan and Jacob Steinhardt", "abstract": "  Emerging marketplaces for large language models and other large-scale machine\nlearning (ML) models appear to exhibit market concentration, which has raised\nconcerns about whether there are insurmountable barriers to entry in such\nmarkets. In this work, we study this issue from both an economic and an\nalgorithmic point of view, focusing on a phenomenon that reduces barriers to\nentry. Specifically, an incumbent company risks reputational damage unless its\nmodel is sufficiently aligned with safety objectives, whereas a new company can\nmore easily avoid reputational damage. To study this issue formally, we define\na multi-objective high-dimensional regression framework that captures\nreputational damage, and we characterize the number of data points that a new\ncompany needs to enter the market. Our results demonstrate how multi-objective\nconsiderations can fundamentally reduce barriers to entry -- the required\nnumber of data points can be significantly smaller than the incumbent company's\ndataset size. En route to proving these results, we develop scaling laws for\nhigh-dimensional linear regression in multi-objective environments, showing\nthat the scaling rate becomes slower when the dataset size is large, which\ncould be of independent interest.\n", "link": "http://arxiv.org/abs/2409.03734v1", "date": "2024-09-05", "relevancy": 1.4673, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5188}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20vs.%20Performance%3A%20How%20Multi-Objective%20Learning%20Reduces%20Barriers%20to%0A%20%20Market%20Entry&body=Title%3A%20Safety%20vs.%20Performance%3A%20How%20Multi-Objective%20Learning%20Reduces%20Barriers%20to%0A%20%20Market%20Entry%0AAuthor%3A%20Meena%20Jagadeesan%20and%20Michael%20I.%20Jordan%20and%20Jacob%20Steinhardt%0AAbstract%3A%20%20%20Emerging%20marketplaces%20for%20large%20language%20models%20and%20other%20large-scale%20machine%0Alearning%20%28ML%29%20models%20appear%20to%20exhibit%20market%20concentration%2C%20which%20has%20raised%0Aconcerns%20about%20whether%20there%20are%20insurmountable%20barriers%20to%20entry%20in%20such%0Amarkets.%20In%20this%20work%2C%20we%20study%20this%20issue%20from%20both%20an%20economic%20and%20an%0Aalgorithmic%20point%20of%20view%2C%20focusing%20on%20a%20phenomenon%20that%20reduces%20barriers%20to%0Aentry.%20Specifically%2C%20an%20incumbent%20company%20risks%20reputational%20damage%20unless%20its%0Amodel%20is%20sufficiently%20aligned%20with%20safety%20objectives%2C%20whereas%20a%20new%20company%20can%0Amore%20easily%20avoid%20reputational%20damage.%20To%20study%20this%20issue%20formally%2C%20we%20define%0Aa%20multi-objective%20high-dimensional%20regression%20framework%20that%20captures%0Areputational%20damage%2C%20and%20we%20characterize%20the%20number%20of%20data%20points%20that%20a%20new%0Acompany%20needs%20to%20enter%20the%20market.%20Our%20results%20demonstrate%20how%20multi-objective%0Aconsiderations%20can%20fundamentally%20reduce%20barriers%20to%20entry%20--%20the%20required%0Anumber%20of%20data%20points%20can%20be%20significantly%20smaller%20than%20the%20incumbent%20company%27s%0Adataset%20size.%20En%20route%20to%20proving%20these%20results%2C%20we%20develop%20scaling%20laws%20for%0Ahigh-dimensional%20linear%20regression%20in%20multi-objective%20environments%2C%20showing%0Athat%20the%20scaling%20rate%20becomes%20slower%20when%20the%20dataset%20size%20is%20large%2C%20which%0Acould%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520vs.%2520Performance%253A%2520How%2520Multi-Objective%2520Learning%2520Reduces%2520Barriers%2520to%250A%2520%2520Market%2520Entry%26entry.906535625%3DMeena%2520Jagadeesan%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Jacob%2520Steinhardt%26entry.1292438233%3D%2520%2520Emerging%2520marketplaces%2520for%2520large%2520language%2520models%2520and%2520other%2520large-scale%2520machine%250Alearning%2520%2528ML%2529%2520models%2520appear%2520to%2520exhibit%2520market%2520concentration%252C%2520which%2520has%2520raised%250Aconcerns%2520about%2520whether%2520there%2520are%2520insurmountable%2520barriers%2520to%2520entry%2520in%2520such%250Amarkets.%2520In%2520this%2520work%252C%2520we%2520study%2520this%2520issue%2520from%2520both%2520an%2520economic%2520and%2520an%250Aalgorithmic%2520point%2520of%2520view%252C%2520focusing%2520on%2520a%2520phenomenon%2520that%2520reduces%2520barriers%2520to%250Aentry.%2520Specifically%252C%2520an%2520incumbent%2520company%2520risks%2520reputational%2520damage%2520unless%2520its%250Amodel%2520is%2520sufficiently%2520aligned%2520with%2520safety%2520objectives%252C%2520whereas%2520a%2520new%2520company%2520can%250Amore%2520easily%2520avoid%2520reputational%2520damage.%2520To%2520study%2520this%2520issue%2520formally%252C%2520we%2520define%250Aa%2520multi-objective%2520high-dimensional%2520regression%2520framework%2520that%2520captures%250Areputational%2520damage%252C%2520and%2520we%2520characterize%2520the%2520number%2520of%2520data%2520points%2520that%2520a%2520new%250Acompany%2520needs%2520to%2520enter%2520the%2520market.%2520Our%2520results%2520demonstrate%2520how%2520multi-objective%250Aconsiderations%2520can%2520fundamentally%2520reduce%2520barriers%2520to%2520entry%2520--%2520the%2520required%250Anumber%2520of%2520data%2520points%2520can%2520be%2520significantly%2520smaller%2520than%2520the%2520incumbent%2520company%2527s%250Adataset%2520size.%2520En%2520route%2520to%2520proving%2520these%2520results%252C%2520we%2520develop%2520scaling%2520laws%2520for%250Ahigh-dimensional%2520linear%2520regression%2520in%2520multi-objective%2520environments%252C%2520showing%250Athat%2520the%2520scaling%2520rate%2520becomes%2520slower%2520when%2520the%2520dataset%2520size%2520is%2520large%252C%2520which%250Acould%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20vs.%20Performance%3A%20How%20Multi-Objective%20Learning%20Reduces%20Barriers%20to%0A%20%20Market%20Entry&entry.906535625=Meena%20Jagadeesan%20and%20Michael%20I.%20Jordan%20and%20Jacob%20Steinhardt&entry.1292438233=%20%20Emerging%20marketplaces%20for%20large%20language%20models%20and%20other%20large-scale%20machine%0Alearning%20%28ML%29%20models%20appear%20to%20exhibit%20market%20concentration%2C%20which%20has%20raised%0Aconcerns%20about%20whether%20there%20are%20insurmountable%20barriers%20to%20entry%20in%20such%0Amarkets.%20In%20this%20work%2C%20we%20study%20this%20issue%20from%20both%20an%20economic%20and%20an%0Aalgorithmic%20point%20of%20view%2C%20focusing%20on%20a%20phenomenon%20that%20reduces%20barriers%20to%0Aentry.%20Specifically%2C%20an%20incumbent%20company%20risks%20reputational%20damage%20unless%20its%0Amodel%20is%20sufficiently%20aligned%20with%20safety%20objectives%2C%20whereas%20a%20new%20company%20can%0Amore%20easily%20avoid%20reputational%20damage.%20To%20study%20this%20issue%20formally%2C%20we%20define%0Aa%20multi-objective%20high-dimensional%20regression%20framework%20that%20captures%0Areputational%20damage%2C%20and%20we%20characterize%20the%20number%20of%20data%20points%20that%20a%20new%0Acompany%20needs%20to%20enter%20the%20market.%20Our%20results%20demonstrate%20how%20multi-objective%0Aconsiderations%20can%20fundamentally%20reduce%20barriers%20to%20entry%20--%20the%20required%0Anumber%20of%20data%20points%20can%20be%20significantly%20smaller%20than%20the%20incumbent%20company%27s%0Adataset%20size.%20En%20route%20to%20proving%20these%20results%2C%20we%20develop%20scaling%20laws%20for%0Ahigh-dimensional%20linear%20regression%20in%20multi-objective%20environments%2C%20showing%0Athat%20the%20scaling%20rate%20becomes%20slower%20when%20the%20dataset%20size%20is%20large%2C%20which%0Acould%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03734v1&entry.124074799=Read"},
{"title": "A Different Level Text Protection Mechanism With Differential Privacy", "author": "Qingwen Fu", "abstract": "  The article introduces a method for extracting words of different degrees of\nimportance based on the BERT pre-training model and proves the effectiveness of\nthis method. The article also discusses the impact of maintaining the same\nperturbation results for words of different importance on the overall text\nutility. This method can be applied to long text protection.\n", "link": "http://arxiv.org/abs/2409.03707v1", "date": "2024-09-05", "relevancy": 1.6434, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.422}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4044}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Different%20Level%20Text%20Protection%20Mechanism%20With%20Differential%20Privacy&body=Title%3A%20A%20Different%20Level%20Text%20Protection%20Mechanism%20With%20Differential%20Privacy%0AAuthor%3A%20Qingwen%20Fu%0AAbstract%3A%20%20%20The%20article%20introduces%20a%20method%20for%20extracting%20words%20of%20different%20degrees%20of%0Aimportance%20based%20on%20the%20BERT%20pre-training%20model%20and%20proves%20the%20effectiveness%20of%0Athis%20method.%20The%20article%20also%20discusses%20the%20impact%20of%20maintaining%20the%20same%0Aperturbation%20results%20for%20words%20of%20different%20importance%20on%20the%20overall%20text%0Autility.%20This%20method%20can%20be%20applied%20to%20long%20text%20protection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Different%2520Level%2520Text%2520Protection%2520Mechanism%2520With%2520Differential%2520Privacy%26entry.906535625%3DQingwen%2520Fu%26entry.1292438233%3D%2520%2520The%2520article%2520introduces%2520a%2520method%2520for%2520extracting%2520words%2520of%2520different%2520degrees%2520of%250Aimportance%2520based%2520on%2520the%2520BERT%2520pre-training%2520model%2520and%2520proves%2520the%2520effectiveness%2520of%250Athis%2520method.%2520The%2520article%2520also%2520discusses%2520the%2520impact%2520of%2520maintaining%2520the%2520same%250Aperturbation%2520results%2520for%2520words%2520of%2520different%2520importance%2520on%2520the%2520overall%2520text%250Autility.%2520This%2520method%2520can%2520be%2520applied%2520to%2520long%2520text%2520protection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Different%20Level%20Text%20Protection%20Mechanism%20With%20Differential%20Privacy&entry.906535625=Qingwen%20Fu&entry.1292438233=%20%20The%20article%20introduces%20a%20method%20for%20extracting%20words%20of%20different%20degrees%20of%0Aimportance%20based%20on%20the%20BERT%20pre-training%20model%20and%20proves%20the%20effectiveness%20of%0Athis%20method.%20The%20article%20also%20discusses%20the%20impact%20of%20maintaining%20the%20same%0Aperturbation%20results%20for%20words%20of%20different%20importance%20on%20the%20overall%20text%0Autility.%20This%20method%20can%20be%20applied%20to%20long%20text%20protection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03707v1&entry.124074799=Read"},
{"title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models", "author": "Yan Shvartzshnaider and Vasisht Duddu and John Lacalamita", "abstract": "  Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization).\n", "link": "http://arxiv.org/abs/2409.03735v1", "date": "2024-09-05", "relevancy": 1.419, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4998}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4853}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-CI%3A%20Assessing%20Contextual%20Integrity%20Norms%20in%20Language%20Models&body=Title%3A%20LLM-CI%3A%20Assessing%20Contextual%20Integrity%20Norms%20in%20Language%20Models%0AAuthor%3A%20Yan%20Shvartzshnaider%20and%20Vasisht%20Duddu%20and%20John%20Lacalamita%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20while%20memorizing%20parts%20of%20their%20training%20data%0Ascraped%20from%20the%20Internet%2C%20may%20also%20inadvertently%20encode%20societal%20preferences%0Aand%20norms.%20As%20these%20models%20are%20integrated%20into%20sociotechnical%20systems%2C%20it%20is%0Acrucial%20that%20the%20norms%20they%20encode%20align%20with%20societal%20expectations.%20These%0Anorms%20could%20vary%20across%20models%2C%20hyperparameters%2C%20optimization%20techniques%2C%20and%0Adatasets.%20This%20is%20especially%20challenging%20due%20to%20prompt%20sensitivity%24-%24small%0Avariations%20in%20prompts%20yield%20different%20responses%2C%20rendering%20existing%20assessment%0Amethodologies%20unreliable.%20There%20is%20a%20need%20for%20a%20comprehensive%20framework%0Acovering%20various%20models%2C%20optimization%2C%20and%20datasets%2C%20along%20with%20a%20reliable%0Amethodology%20to%20assess%20encoded%20norms.%0A%20%20We%20present%20LLM-CI%2C%20the%20first%20open-sourced%20framework%20to%20assess%20privacy%20norms%0Aencoded%20in%20LLMs.%20LLM-CI%20uses%20a%20Contextual%20Integrity-based%20factorial%20vignette%0Amethodology%20to%20assess%20the%20encoded%20norms%20across%20different%20contexts%20and%20LLMs.%20We%0Apropose%20the%20multi-prompt%20assessment%20methodology%20to%20address%20prompt%20sensitivity%0Aby%20assessing%20the%20norms%20from%20only%20the%20prompts%20that%20yield%20consistent%20responses%0Aacross%20multiple%20variants.%20Using%20LLM-CI%20and%20our%20proposed%20methodology%2C%20we%0Acomprehensively%20evaluate%20LLMs%20using%20IoT%20and%20COPPA%20vignettes%20datasets%20from%20prior%0Awork%2C%20examining%20the%20impact%20of%20model%20properties%20%28e.g.%2C%20hyperparameters%2C%0Acapacity%29%20and%20optimization%20strategies%20%28e.g.%2C%20alignment%2C%20quantization%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-CI%253A%2520Assessing%2520Contextual%2520Integrity%2520Norms%2520in%2520Language%2520Models%26entry.906535625%3DYan%2520Shvartzshnaider%2520and%2520Vasisht%2520Duddu%2520and%2520John%2520Lacalamita%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520while%2520memorizing%2520parts%2520of%2520their%2520training%2520data%250Ascraped%2520from%2520the%2520Internet%252C%2520may%2520also%2520inadvertently%2520encode%2520societal%2520preferences%250Aand%2520norms.%2520As%2520these%2520models%2520are%2520integrated%2520into%2520sociotechnical%2520systems%252C%2520it%2520is%250Acrucial%2520that%2520the%2520norms%2520they%2520encode%2520align%2520with%2520societal%2520expectations.%2520These%250Anorms%2520could%2520vary%2520across%2520models%252C%2520hyperparameters%252C%2520optimization%2520techniques%252C%2520and%250Adatasets.%2520This%2520is%2520especially%2520challenging%2520due%2520to%2520prompt%2520sensitivity%2524-%2524small%250Avariations%2520in%2520prompts%2520yield%2520different%2520responses%252C%2520rendering%2520existing%2520assessment%250Amethodologies%2520unreliable.%2520There%2520is%2520a%2520need%2520for%2520a%2520comprehensive%2520framework%250Acovering%2520various%2520models%252C%2520optimization%252C%2520and%2520datasets%252C%2520along%2520with%2520a%2520reliable%250Amethodology%2520to%2520assess%2520encoded%2520norms.%250A%2520%2520We%2520present%2520LLM-CI%252C%2520the%2520first%2520open-sourced%2520framework%2520to%2520assess%2520privacy%2520norms%250Aencoded%2520in%2520LLMs.%2520LLM-CI%2520uses%2520a%2520Contextual%2520Integrity-based%2520factorial%2520vignette%250Amethodology%2520to%2520assess%2520the%2520encoded%2520norms%2520across%2520different%2520contexts%2520and%2520LLMs.%2520We%250Apropose%2520the%2520multi-prompt%2520assessment%2520methodology%2520to%2520address%2520prompt%2520sensitivity%250Aby%2520assessing%2520the%2520norms%2520from%2520only%2520the%2520prompts%2520that%2520yield%2520consistent%2520responses%250Aacross%2520multiple%2520variants.%2520Using%2520LLM-CI%2520and%2520our%2520proposed%2520methodology%252C%2520we%250Acomprehensively%2520evaluate%2520LLMs%2520using%2520IoT%2520and%2520COPPA%2520vignettes%2520datasets%2520from%2520prior%250Awork%252C%2520examining%2520the%2520impact%2520of%2520model%2520properties%2520%2528e.g.%252C%2520hyperparameters%252C%250Acapacity%2529%2520and%2520optimization%2520strategies%2520%2528e.g.%252C%2520alignment%252C%2520quantization%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-CI%3A%20Assessing%20Contextual%20Integrity%20Norms%20in%20Language%20Models&entry.906535625=Yan%20Shvartzshnaider%20and%20Vasisht%20Duddu%20and%20John%20Lacalamita&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20while%20memorizing%20parts%20of%20their%20training%20data%0Ascraped%20from%20the%20Internet%2C%20may%20also%20inadvertently%20encode%20societal%20preferences%0Aand%20norms.%20As%20these%20models%20are%20integrated%20into%20sociotechnical%20systems%2C%20it%20is%0Acrucial%20that%20the%20norms%20they%20encode%20align%20with%20societal%20expectations.%20These%0Anorms%20could%20vary%20across%20models%2C%20hyperparameters%2C%20optimization%20techniques%2C%20and%0Adatasets.%20This%20is%20especially%20challenging%20due%20to%20prompt%20sensitivity%24-%24small%0Avariations%20in%20prompts%20yield%20different%20responses%2C%20rendering%20existing%20assessment%0Amethodologies%20unreliable.%20There%20is%20a%20need%20for%20a%20comprehensive%20framework%0Acovering%20various%20models%2C%20optimization%2C%20and%20datasets%2C%20along%20with%20a%20reliable%0Amethodology%20to%20assess%20encoded%20norms.%0A%20%20We%20present%20LLM-CI%2C%20the%20first%20open-sourced%20framework%20to%20assess%20privacy%20norms%0Aencoded%20in%20LLMs.%20LLM-CI%20uses%20a%20Contextual%20Integrity-based%20factorial%20vignette%0Amethodology%20to%20assess%20the%20encoded%20norms%20across%20different%20contexts%20and%20LLMs.%20We%0Apropose%20the%20multi-prompt%20assessment%20methodology%20to%20address%20prompt%20sensitivity%0Aby%20assessing%20the%20norms%20from%20only%20the%20prompts%20that%20yield%20consistent%20responses%0Aacross%20multiple%20variants.%20Using%20LLM-CI%20and%20our%20proposed%20methodology%2C%20we%0Acomprehensively%20evaluate%20LLMs%20using%20IoT%20and%20COPPA%20vignettes%20datasets%20from%20prior%0Awork%2C%20examining%20the%20impact%20of%20model%20properties%20%28e.g.%2C%20hyperparameters%2C%0Acapacity%29%20and%20optimization%20strategies%20%28e.g.%2C%20alignment%2C%20quantization%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03735v1&entry.124074799=Read"},
{"title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner", "author": "Xunguang Wang and Daoyuan Wu and Zhenlan Ji and Zongjie Li and Pingchuan Ma and Shuai Wang and Yingjiu Li and Yang Liu and Ning Liu and Juergen Rahmel", "abstract": "  Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections.\n", "link": "http://arxiv.org/abs/2406.05498v2", "date": "2024-09-05", "relevancy": 1.602, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4092}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4042}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfDefend%3A%20LLMs%20Can%20Defend%20Themselves%20against%20Jailbreaking%20in%20a%0A%20%20Practical%20Manner&body=Title%3A%20SelfDefend%3A%20LLMs%20Can%20Defend%20Themselves%20against%20Jailbreaking%20in%20a%0A%20%20Practical%20Manner%0AAuthor%3A%20Xunguang%20Wang%20and%20Daoyuan%20Wu%20and%20Zhenlan%20Ji%20and%20Zongjie%20Li%20and%20Pingchuan%20Ma%20and%20Shuai%20Wang%20and%20Yingjiu%20Li%20and%20Yang%20Liu%20and%20Ning%20Liu%20and%20Juergen%20Rahmel%0AAbstract%3A%20%20%20Jailbreaking%20is%20an%20emerging%20adversarial%20attack%20that%20bypasses%20the%20safety%0Aalignment%20deployed%20in%20off-the-shelf%20large%20language%20models%20%28LLMs%29%20and%20has%0Aevolved%20into%20multiple%20categories%3A%20human-based%2C%20optimization-based%2C%0Ageneration-based%2C%20and%20the%20recent%20indirect%20and%20multilingual%20jailbreaks.%20However%2C%0Adelivering%20a%20practical%20jailbreak%20defense%20is%20challenging%20because%20it%20needs%20to%20not%0Aonly%20handle%20all%20the%20above%20jailbreak%20attacks%20but%20also%20incur%20negligible%20delays%20to%0Auser%20prompts%2C%20as%20well%20as%20be%20compatible%20with%20both%20open-source%20and%20closed-source%0ALLMs.%20Inspired%20by%20how%20the%20traditional%20security%20concept%20of%20shadow%20stacks%20defends%0Aagainst%20memory%20overflow%20attacks%2C%20this%20paper%20introduces%20a%20generic%20LLM%20jailbreak%0Adefense%20framework%20called%20SelfDefend%2C%20which%20establishes%20a%20shadow%20LLM%20as%20a%0Adefense%20instance%20to%20concurrently%20protect%20the%20target%20LLM%20instance%20in%20the%20normal%0Astack%20and%20collaborate%20with%20it%20for%20checkpoint-based%20access%20control.%20The%0Aeffectiveness%20of%20SelfDefend%20builds%20upon%20our%20observation%20that%20existing%20LLMs%0A%28both%20target%20and%20defense%20LLMs%29%20have%20the%20capability%20to%20identify%20harmful%20prompts%0Aor%20intentions%20in%20user%20queries%2C%20which%20we%20empirically%20validate%20using%20the%20commonly%0Aused%20GPT-3.5/4%20models%20across%20all%20major%20jailbreak%20attacks.%20To%20further%20improve%0Athe%20defense%27s%20robustness%20and%20minimize%20costs%2C%20we%20employ%20a%20data%20distillation%0Aapproach%20to%20tune%20dedicated%20open-source%20defense%20models.%20These%20models%20outperform%0Asix%20state-of-the-art%20defenses%20and%20match%20the%20performance%20of%20GPT-4-based%0ASelfDefend%2C%20with%20significantly%20lower%20extra%20delays.%20We%20also%20empirically%20show%0Athat%20the%20tuned%20models%20are%20robust%20to%20adaptive%20jailbreaks%20and%20prompt%20injections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfDefend%253A%2520LLMs%2520Can%2520Defend%2520Themselves%2520against%2520Jailbreaking%2520in%2520a%250A%2520%2520Practical%2520Manner%26entry.906535625%3DXunguang%2520Wang%2520and%2520Daoyuan%2520Wu%2520and%2520Zhenlan%2520Ji%2520and%2520Zongjie%2520Li%2520and%2520Pingchuan%2520Ma%2520and%2520Shuai%2520Wang%2520and%2520Yingjiu%2520Li%2520and%2520Yang%2520Liu%2520and%2520Ning%2520Liu%2520and%2520Juergen%2520Rahmel%26entry.1292438233%3D%2520%2520Jailbreaking%2520is%2520an%2520emerging%2520adversarial%2520attack%2520that%2520bypasses%2520the%2520safety%250Aalignment%2520deployed%2520in%2520off-the-shelf%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520has%250Aevolved%2520into%2520multiple%2520categories%253A%2520human-based%252C%2520optimization-based%252C%250Ageneration-based%252C%2520and%2520the%2520recent%2520indirect%2520and%2520multilingual%2520jailbreaks.%2520However%252C%250Adelivering%2520a%2520practical%2520jailbreak%2520defense%2520is%2520challenging%2520because%2520it%2520needs%2520to%2520not%250Aonly%2520handle%2520all%2520the%2520above%2520jailbreak%2520attacks%2520but%2520also%2520incur%2520negligible%2520delays%2520to%250Auser%2520prompts%252C%2520as%2520well%2520as%2520be%2520compatible%2520with%2520both%2520open-source%2520and%2520closed-source%250ALLMs.%2520Inspired%2520by%2520how%2520the%2520traditional%2520security%2520concept%2520of%2520shadow%2520stacks%2520defends%250Aagainst%2520memory%2520overflow%2520attacks%252C%2520this%2520paper%2520introduces%2520a%2520generic%2520LLM%2520jailbreak%250Adefense%2520framework%2520called%2520SelfDefend%252C%2520which%2520establishes%2520a%2520shadow%2520LLM%2520as%2520a%250Adefense%2520instance%2520to%2520concurrently%2520protect%2520the%2520target%2520LLM%2520instance%2520in%2520the%2520normal%250Astack%2520and%2520collaborate%2520with%2520it%2520for%2520checkpoint-based%2520access%2520control.%2520The%250Aeffectiveness%2520of%2520SelfDefend%2520builds%2520upon%2520our%2520observation%2520that%2520existing%2520LLMs%250A%2528both%2520target%2520and%2520defense%2520LLMs%2529%2520have%2520the%2520capability%2520to%2520identify%2520harmful%2520prompts%250Aor%2520intentions%2520in%2520user%2520queries%252C%2520which%2520we%2520empirically%2520validate%2520using%2520the%2520commonly%250Aused%2520GPT-3.5/4%2520models%2520across%2520all%2520major%2520jailbreak%2520attacks.%2520To%2520further%2520improve%250Athe%2520defense%2527s%2520robustness%2520and%2520minimize%2520costs%252C%2520we%2520employ%2520a%2520data%2520distillation%250Aapproach%2520to%2520tune%2520dedicated%2520open-source%2520defense%2520models.%2520These%2520models%2520outperform%250Asix%2520state-of-the-art%2520defenses%2520and%2520match%2520the%2520performance%2520of%2520GPT-4-based%250ASelfDefend%252C%2520with%2520significantly%2520lower%2520extra%2520delays.%2520We%2520also%2520empirically%2520show%250Athat%2520the%2520tuned%2520models%2520are%2520robust%2520to%2520adaptive%2520jailbreaks%2520and%2520prompt%2520injections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfDefend%3A%20LLMs%20Can%20Defend%20Themselves%20against%20Jailbreaking%20in%20a%0A%20%20Practical%20Manner&entry.906535625=Xunguang%20Wang%20and%20Daoyuan%20Wu%20and%20Zhenlan%20Ji%20and%20Zongjie%20Li%20and%20Pingchuan%20Ma%20and%20Shuai%20Wang%20and%20Yingjiu%20Li%20and%20Yang%20Liu%20and%20Ning%20Liu%20and%20Juergen%20Rahmel&entry.1292438233=%20%20Jailbreaking%20is%20an%20emerging%20adversarial%20attack%20that%20bypasses%20the%20safety%0Aalignment%20deployed%20in%20off-the-shelf%20large%20language%20models%20%28LLMs%29%20and%20has%0Aevolved%20into%20multiple%20categories%3A%20human-based%2C%20optimization-based%2C%0Ageneration-based%2C%20and%20the%20recent%20indirect%20and%20multilingual%20jailbreaks.%20However%2C%0Adelivering%20a%20practical%20jailbreak%20defense%20is%20challenging%20because%20it%20needs%20to%20not%0Aonly%20handle%20all%20the%20above%20jailbreak%20attacks%20but%20also%20incur%20negligible%20delays%20to%0Auser%20prompts%2C%20as%20well%20as%20be%20compatible%20with%20both%20open-source%20and%20closed-source%0ALLMs.%20Inspired%20by%20how%20the%20traditional%20security%20concept%20of%20shadow%20stacks%20defends%0Aagainst%20memory%20overflow%20attacks%2C%20this%20paper%20introduces%20a%20generic%20LLM%20jailbreak%0Adefense%20framework%20called%20SelfDefend%2C%20which%20establishes%20a%20shadow%20LLM%20as%20a%0Adefense%20instance%20to%20concurrently%20protect%20the%20target%20LLM%20instance%20in%20the%20normal%0Astack%20and%20collaborate%20with%20it%20for%20checkpoint-based%20access%20control.%20The%0Aeffectiveness%20of%20SelfDefend%20builds%20upon%20our%20observation%20that%20existing%20LLMs%0A%28both%20target%20and%20defense%20LLMs%29%20have%20the%20capability%20to%20identify%20harmful%20prompts%0Aor%20intentions%20in%20user%20queries%2C%20which%20we%20empirically%20validate%20using%20the%20commonly%0Aused%20GPT-3.5/4%20models%20across%20all%20major%20jailbreak%20attacks.%20To%20further%20improve%0Athe%20defense%27s%20robustness%20and%20minimize%20costs%2C%20we%20employ%20a%20data%20distillation%0Aapproach%20to%20tune%20dedicated%20open-source%20defense%20models.%20These%20models%20outperform%0Asix%20state-of-the-art%20defenses%20and%20match%20the%20performance%20of%20GPT-4-based%0ASelfDefend%2C%20with%20significantly%20lower%20extra%20delays.%20We%20also%20empirically%20show%0Athat%20the%20tuned%20models%20are%20robust%20to%20adaptive%20jailbreaks%20and%20prompt%20injections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05498v2&entry.124074799=Read"},
{"title": "Shuffle Vision Transformer: Lightweight, Fast and Efficient Recognition\n  of Driver Facial Expression", "author": "Ibtissam Saadi and Douglas W. Cunningham and Taleb-ahmed Abdelmalik and Abdenour Hadid and Yassin El Hillali", "abstract": "  Existing methods for driver facial expression recognition (DFER) are often\ncomputationally intensive, rendering them unsuitable for real-time\napplications. In this work, we introduce a novel transfer learning-based dual\narchitecture, named ShuffViT-DFER, which elegantly combines computational\nefficiency and accuracy. This is achieved by harnessing the strengths of two\nlightweight and efficient models using convolutional neural network (CNN) and\nvision transformers (ViT). We efficiently fuse the extracted features to\nenhance the performance of the model in accurately recognizing the facial\nexpressions of the driver. Our experimental results on two benchmarking and\npublic datasets, KMU-FED and KDEF, highlight the validity of our proposed\nmethod for real-time application with superior performance when compared to\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.03438v1", "date": "2024-09-05", "relevancy": 1.56, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5254}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5151}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shuffle%20Vision%20Transformer%3A%20Lightweight%2C%20Fast%20and%20Efficient%20Recognition%0A%20%20of%20Driver%20Facial%20Expression&body=Title%3A%20Shuffle%20Vision%20Transformer%3A%20Lightweight%2C%20Fast%20and%20Efficient%20Recognition%0A%20%20of%20Driver%20Facial%20Expression%0AAuthor%3A%20Ibtissam%20Saadi%20and%20Douglas%20W.%20Cunningham%20and%20Taleb-ahmed%20Abdelmalik%20and%20Abdenour%20Hadid%20and%20Yassin%20El%20Hillali%0AAbstract%3A%20%20%20Existing%20methods%20for%20driver%20facial%20expression%20recognition%20%28DFER%29%20are%20often%0Acomputationally%20intensive%2C%20rendering%20them%20unsuitable%20for%20real-time%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20transfer%20learning-based%20dual%0Aarchitecture%2C%20named%20ShuffViT-DFER%2C%20which%20elegantly%20combines%20computational%0Aefficiency%20and%20accuracy.%20This%20is%20achieved%20by%20harnessing%20the%20strengths%20of%20two%0Alightweight%20and%20efficient%20models%20using%20convolutional%20neural%20network%20%28CNN%29%20and%0Avision%20transformers%20%28ViT%29.%20We%20efficiently%20fuse%20the%20extracted%20features%20to%0Aenhance%20the%20performance%20of%20the%20model%20in%20accurately%20recognizing%20the%20facial%0Aexpressions%20of%20the%20driver.%20Our%20experimental%20results%20on%20two%20benchmarking%20and%0Apublic%20datasets%2C%20KMU-FED%20and%20KDEF%2C%20highlight%20the%20validity%20of%20our%20proposed%0Amethod%20for%20real-time%20application%20with%20superior%20performance%20when%20compared%20to%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShuffle%2520Vision%2520Transformer%253A%2520Lightweight%252C%2520Fast%2520and%2520Efficient%2520Recognition%250A%2520%2520of%2520Driver%2520Facial%2520Expression%26entry.906535625%3DIbtissam%2520Saadi%2520and%2520Douglas%2520W.%2520Cunningham%2520and%2520Taleb-ahmed%2520Abdelmalik%2520and%2520Abdenour%2520Hadid%2520and%2520Yassin%2520El%2520Hillali%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520driver%2520facial%2520expression%2520recognition%2520%2528DFER%2529%2520are%2520often%250Acomputationally%2520intensive%252C%2520rendering%2520them%2520unsuitable%2520for%2520real-time%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520transfer%2520learning-based%2520dual%250Aarchitecture%252C%2520named%2520ShuffViT-DFER%252C%2520which%2520elegantly%2520combines%2520computational%250Aefficiency%2520and%2520accuracy.%2520This%2520is%2520achieved%2520by%2520harnessing%2520the%2520strengths%2520of%2520two%250Alightweight%2520and%2520efficient%2520models%2520using%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520and%250Avision%2520transformers%2520%2528ViT%2529.%2520We%2520efficiently%2520fuse%2520the%2520extracted%2520features%2520to%250Aenhance%2520the%2520performance%2520of%2520the%2520model%2520in%2520accurately%2520recognizing%2520the%2520facial%250Aexpressions%2520of%2520the%2520driver.%2520Our%2520experimental%2520results%2520on%2520two%2520benchmarking%2520and%250Apublic%2520datasets%252C%2520KMU-FED%2520and%2520KDEF%252C%2520highlight%2520the%2520validity%2520of%2520our%2520proposed%250Amethod%2520for%2520real-time%2520application%2520with%2520superior%2520performance%2520when%2520compared%2520to%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shuffle%20Vision%20Transformer%3A%20Lightweight%2C%20Fast%20and%20Efficient%20Recognition%0A%20%20of%20Driver%20Facial%20Expression&entry.906535625=Ibtissam%20Saadi%20and%20Douglas%20W.%20Cunningham%20and%20Taleb-ahmed%20Abdelmalik%20and%20Abdenour%20Hadid%20and%20Yassin%20El%20Hillali&entry.1292438233=%20%20Existing%20methods%20for%20driver%20facial%20expression%20recognition%20%28DFER%29%20are%20often%0Acomputationally%20intensive%2C%20rendering%20them%20unsuitable%20for%20real-time%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20transfer%20learning-based%20dual%0Aarchitecture%2C%20named%20ShuffViT-DFER%2C%20which%20elegantly%20combines%20computational%0Aefficiency%20and%20accuracy.%20This%20is%20achieved%20by%20harnessing%20the%20strengths%20of%20two%0Alightweight%20and%20efficient%20models%20using%20convolutional%20neural%20network%20%28CNN%29%20and%0Avision%20transformers%20%28ViT%29.%20We%20efficiently%20fuse%20the%20extracted%20features%20to%0Aenhance%20the%20performance%20of%20the%20model%20in%20accurately%20recognizing%20the%20facial%0Aexpressions%20of%20the%20driver.%20Our%20experimental%20results%20on%20two%20benchmarking%20and%0Apublic%20datasets%2C%20KMU-FED%20and%20KDEF%2C%20highlight%20the%20validity%20of%20our%20proposed%0Amethod%20for%20real-time%20application%20with%20superior%20performance%20when%20compared%20to%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03438v1&entry.124074799=Read"},
{"title": "On the Impact of Data Heterogeneity in Federated Learning Environments\n  with Application to Healthcare Networks", "author": "Usevalad Milasheuski and Luca Barbieri and Bernardo Camajori Tedeschini and Monica Nicoli and Stefano Savazzi", "abstract": "  Federated Learning (FL) allows multiple privacy-sensitive applications to\nleverage their dataset for a global model construction without any disclosure\nof the information. One of those domains is healthcare, where groups of silos\ncollaborate in order to generate a global predictor with improved accuracy and\ngeneralization. However, the inherent challenge lies in the high heterogeneity\nof medical data, necessitating sophisticated techniques for assessment and\ncompensation. This paper presents a comprehensive exploration of the\nmathematical formalization and taxonomy of heterogeneity within FL\nenvironments, focusing on the intricacies of medical data. In particular, we\naddress the evaluation and comparison of the most popular FL algorithms with\nrespect to their ability to cope with quantity-based, feature and label\ndistribution-based heterogeneity. The goal is to provide a quantitative\nevaluation of the impact of data heterogeneity in FL systems for healthcare\nnetworks as well as a guideline on FL algorithm selection. Our research extends\nbeyond existing studies by benchmarking seven of the most common FL algorithms\nagainst the unique challenges posed by medical data use cases. The paper\ntargets the prediction of the risk of stroke recurrence through a set of\ntabular clinical reports collected by different federated hospital silos: data\nheterogeneity frequently encountered in this scenario and its impact on FL\nperformance are discussed.\n", "link": "http://arxiv.org/abs/2404.18519v3", "date": "2024-09-05", "relevancy": 1.413, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Impact%20of%20Data%20Heterogeneity%20in%20Federated%20Learning%20Environments%0A%20%20with%20Application%20to%20Healthcare%20Networks&body=Title%3A%20On%20the%20Impact%20of%20Data%20Heterogeneity%20in%20Federated%20Learning%20Environments%0A%20%20with%20Application%20to%20Healthcare%20Networks%0AAuthor%3A%20Usevalad%20Milasheuski%20and%20Luca%20Barbieri%20and%20Bernardo%20Camajori%20Tedeschini%20and%20Monica%20Nicoli%20and%20Stefano%20Savazzi%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20allows%20multiple%20privacy-sensitive%20applications%20to%0Aleverage%20their%20dataset%20for%20a%20global%20model%20construction%20without%20any%20disclosure%0Aof%20the%20information.%20One%20of%20those%20domains%20is%20healthcare%2C%20where%20groups%20of%20silos%0Acollaborate%20in%20order%20to%20generate%20a%20global%20predictor%20with%20improved%20accuracy%20and%0Ageneralization.%20However%2C%20the%20inherent%20challenge%20lies%20in%20the%20high%20heterogeneity%0Aof%20medical%20data%2C%20necessitating%20sophisticated%20techniques%20for%20assessment%20and%0Acompensation.%20This%20paper%20presents%20a%20comprehensive%20exploration%20of%20the%0Amathematical%20formalization%20and%20taxonomy%20of%20heterogeneity%20within%20FL%0Aenvironments%2C%20focusing%20on%20the%20intricacies%20of%20medical%20data.%20In%20particular%2C%20we%0Aaddress%20the%20evaluation%20and%20comparison%20of%20the%20most%20popular%20FL%20algorithms%20with%0Arespect%20to%20their%20ability%20to%20cope%20with%20quantity-based%2C%20feature%20and%20label%0Adistribution-based%20heterogeneity.%20The%20goal%20is%20to%20provide%20a%20quantitative%0Aevaluation%20of%20the%20impact%20of%20data%20heterogeneity%20in%20FL%20systems%20for%20healthcare%0Anetworks%20as%20well%20as%20a%20guideline%20on%20FL%20algorithm%20selection.%20Our%20research%20extends%0Abeyond%20existing%20studies%20by%20benchmarking%20seven%20of%20the%20most%20common%20FL%20algorithms%0Aagainst%20the%20unique%20challenges%20posed%20by%20medical%20data%20use%20cases.%20The%20paper%0Atargets%20the%20prediction%20of%20the%20risk%20of%20stroke%20recurrence%20through%20a%20set%20of%0Atabular%20clinical%20reports%20collected%20by%20different%20federated%20hospital%20silos%3A%20data%0Aheterogeneity%20frequently%20encountered%20in%20this%20scenario%20and%20its%20impact%20on%20FL%0Aperformance%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18519v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Impact%2520of%2520Data%2520Heterogeneity%2520in%2520Federated%2520Learning%2520Environments%250A%2520%2520with%2520Application%2520to%2520Healthcare%2520Networks%26entry.906535625%3DUsevalad%2520Milasheuski%2520and%2520Luca%2520Barbieri%2520and%2520Bernardo%2520Camajori%2520Tedeschini%2520and%2520Monica%2520Nicoli%2520and%2520Stefano%2520Savazzi%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520allows%2520multiple%2520privacy-sensitive%2520applications%2520to%250Aleverage%2520their%2520dataset%2520for%2520a%2520global%2520model%2520construction%2520without%2520any%2520disclosure%250Aof%2520the%2520information.%2520One%2520of%2520those%2520domains%2520is%2520healthcare%252C%2520where%2520groups%2520of%2520silos%250Acollaborate%2520in%2520order%2520to%2520generate%2520a%2520global%2520predictor%2520with%2520improved%2520accuracy%2520and%250Ageneralization.%2520However%252C%2520the%2520inherent%2520challenge%2520lies%2520in%2520the%2520high%2520heterogeneity%250Aof%2520medical%2520data%252C%2520necessitating%2520sophisticated%2520techniques%2520for%2520assessment%2520and%250Acompensation.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520exploration%2520of%2520the%250Amathematical%2520formalization%2520and%2520taxonomy%2520of%2520heterogeneity%2520within%2520FL%250Aenvironments%252C%2520focusing%2520on%2520the%2520intricacies%2520of%2520medical%2520data.%2520In%2520particular%252C%2520we%250Aaddress%2520the%2520evaluation%2520and%2520comparison%2520of%2520the%2520most%2520popular%2520FL%2520algorithms%2520with%250Arespect%2520to%2520their%2520ability%2520to%2520cope%2520with%2520quantity-based%252C%2520feature%2520and%2520label%250Adistribution-based%2520heterogeneity.%2520The%2520goal%2520is%2520to%2520provide%2520a%2520quantitative%250Aevaluation%2520of%2520the%2520impact%2520of%2520data%2520heterogeneity%2520in%2520FL%2520systems%2520for%2520healthcare%250Anetworks%2520as%2520well%2520as%2520a%2520guideline%2520on%2520FL%2520algorithm%2520selection.%2520Our%2520research%2520extends%250Abeyond%2520existing%2520studies%2520by%2520benchmarking%2520seven%2520of%2520the%2520most%2520common%2520FL%2520algorithms%250Aagainst%2520the%2520unique%2520challenges%2520posed%2520by%2520medical%2520data%2520use%2520cases.%2520The%2520paper%250Atargets%2520the%2520prediction%2520of%2520the%2520risk%2520of%2520stroke%2520recurrence%2520through%2520a%2520set%2520of%250Atabular%2520clinical%2520reports%2520collected%2520by%2520different%2520federated%2520hospital%2520silos%253A%2520data%250Aheterogeneity%2520frequently%2520encountered%2520in%2520this%2520scenario%2520and%2520its%2520impact%2520on%2520FL%250Aperformance%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18519v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Impact%20of%20Data%20Heterogeneity%20in%20Federated%20Learning%20Environments%0A%20%20with%20Application%20to%20Healthcare%20Networks&entry.906535625=Usevalad%20Milasheuski%20and%20Luca%20Barbieri%20and%20Bernardo%20Camajori%20Tedeschini%20and%20Monica%20Nicoli%20and%20Stefano%20Savazzi&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20allows%20multiple%20privacy-sensitive%20applications%20to%0Aleverage%20their%20dataset%20for%20a%20global%20model%20construction%20without%20any%20disclosure%0Aof%20the%20information.%20One%20of%20those%20domains%20is%20healthcare%2C%20where%20groups%20of%20silos%0Acollaborate%20in%20order%20to%20generate%20a%20global%20predictor%20with%20improved%20accuracy%20and%0Ageneralization.%20However%2C%20the%20inherent%20challenge%20lies%20in%20the%20high%20heterogeneity%0Aof%20medical%20data%2C%20necessitating%20sophisticated%20techniques%20for%20assessment%20and%0Acompensation.%20This%20paper%20presents%20a%20comprehensive%20exploration%20of%20the%0Amathematical%20formalization%20and%20taxonomy%20of%20heterogeneity%20within%20FL%0Aenvironments%2C%20focusing%20on%20the%20intricacies%20of%20medical%20data.%20In%20particular%2C%20we%0Aaddress%20the%20evaluation%20and%20comparison%20of%20the%20most%20popular%20FL%20algorithms%20with%0Arespect%20to%20their%20ability%20to%20cope%20with%20quantity-based%2C%20feature%20and%20label%0Adistribution-based%20heterogeneity.%20The%20goal%20is%20to%20provide%20a%20quantitative%0Aevaluation%20of%20the%20impact%20of%20data%20heterogeneity%20in%20FL%20systems%20for%20healthcare%0Anetworks%20as%20well%20as%20a%20guideline%20on%20FL%20algorithm%20selection.%20Our%20research%20extends%0Abeyond%20existing%20studies%20by%20benchmarking%20seven%20of%20the%20most%20common%20FL%20algorithms%0Aagainst%20the%20unique%20challenges%20posed%20by%20medical%20data%20use%20cases.%20The%20paper%0Atargets%20the%20prediction%20of%20the%20risk%20of%20stroke%20recurrence%20through%20a%20set%20of%0Atabular%20clinical%20reports%20collected%20by%20different%20federated%20hospital%20silos%3A%20data%0Aheterogeneity%20frequently%20encountered%20in%20this%20scenario%20and%20its%20impact%20on%20FL%0Aperformance%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18519v3&entry.124074799=Read"},
{"title": "Foundation Model or Finetune? Evaluation of few-shot semantic\n  segmentation for river pollution", "author": "Marga Don and Stijn Pinson and Blanca Guillen Cebrian and Yuki M. Asano", "abstract": "  Foundation models (FMs) are a popular topic of research in AI. Their ability\nto generalize to new tasks and datasets without retraining or needing an\nabundance of data makes them an appealing candidate for applications on\nspecialist datasets. In this work, we compare the performance of FMs to\nfinetuned pre-trained supervised models in the task of semantic segmentation on\nan entirely new dataset. We see that finetuned models consistently outperform\nthe FMs tested, even in cases were data is scarce. We release the code and\ndataset for this work on GitHub.\n", "link": "http://arxiv.org/abs/2409.03754v1", "date": "2024-09-05", "relevancy": 1.5164, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5129}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5024}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model%20or%20Finetune%3F%20Evaluation%20of%20few-shot%20semantic%0A%20%20segmentation%20for%20river%20pollution&body=Title%3A%20Foundation%20Model%20or%20Finetune%3F%20Evaluation%20of%20few-shot%20semantic%0A%20%20segmentation%20for%20river%20pollution%0AAuthor%3A%20Marga%20Don%20and%20Stijn%20Pinson%20and%20Blanca%20Guillen%20Cebrian%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20a%20popular%20topic%20of%20research%20in%20AI.%20Their%20ability%0Ato%20generalize%20to%20new%20tasks%20and%20datasets%20without%20retraining%20or%20needing%20an%0Aabundance%20of%20data%20makes%20them%20an%20appealing%20candidate%20for%20applications%20on%0Aspecialist%20datasets.%20In%20this%20work%2C%20we%20compare%20the%20performance%20of%20FMs%20to%0Afinetuned%20pre-trained%20supervised%20models%20in%20the%20task%20of%20semantic%20segmentation%20on%0Aan%20entirely%20new%20dataset.%20We%20see%20that%20finetuned%20models%20consistently%20outperform%0Athe%20FMs%20tested%2C%20even%20in%20cases%20were%20data%20is%20scarce.%20We%20release%20the%20code%20and%0Adataset%20for%20this%20work%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Model%2520or%2520Finetune%253F%2520Evaluation%2520of%2520few-shot%2520semantic%250A%2520%2520segmentation%2520for%2520river%2520pollution%26entry.906535625%3DMarga%2520Don%2520and%2520Stijn%2520Pinson%2520and%2520Blanca%2520Guillen%2520Cebrian%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520a%2520popular%2520topic%2520of%2520research%2520in%2520AI.%2520Their%2520ability%250Ato%2520generalize%2520to%2520new%2520tasks%2520and%2520datasets%2520without%2520retraining%2520or%2520needing%2520an%250Aabundance%2520of%2520data%2520makes%2520them%2520an%2520appealing%2520candidate%2520for%2520applications%2520on%250Aspecialist%2520datasets.%2520In%2520this%2520work%252C%2520we%2520compare%2520the%2520performance%2520of%2520FMs%2520to%250Afinetuned%2520pre-trained%2520supervised%2520models%2520in%2520the%2520task%2520of%2520semantic%2520segmentation%2520on%250Aan%2520entirely%2520new%2520dataset.%2520We%2520see%2520that%2520finetuned%2520models%2520consistently%2520outperform%250Athe%2520FMs%2520tested%252C%2520even%2520in%2520cases%2520were%2520data%2520is%2520scarce.%2520We%2520release%2520the%2520code%2520and%250Adataset%2520for%2520this%2520work%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model%20or%20Finetune%3F%20Evaluation%20of%20few-shot%20semantic%0A%20%20segmentation%20for%20river%20pollution&entry.906535625=Marga%20Don%20and%20Stijn%20Pinson%20and%20Blanca%20Guillen%20Cebrian%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20a%20popular%20topic%20of%20research%20in%20AI.%20Their%20ability%0Ato%20generalize%20to%20new%20tasks%20and%20datasets%20without%20retraining%20or%20needing%20an%0Aabundance%20of%20data%20makes%20them%20an%20appealing%20candidate%20for%20applications%20on%0Aspecialist%20datasets.%20In%20this%20work%2C%20we%20compare%20the%20performance%20of%20FMs%20to%0Afinetuned%20pre-trained%20supervised%20models%20in%20the%20task%20of%20semantic%20segmentation%20on%0Aan%20entirely%20new%20dataset.%20We%20see%20that%20finetuned%20models%20consistently%20outperform%0Athe%20FMs%20tested%2C%20even%20in%20cases%20were%20data%20is%20scarce.%20We%20release%20the%20code%20and%0Adataset%20for%20this%20work%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03754v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


