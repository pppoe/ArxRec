<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241029.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting", "author": "Sunghwan Hong and Jaewoo Jung and Heeseong Shin and Jisang Han and Jiaolong Yang and Chong Luo and Seungryong Kim", "abstract": "  We consider the problem of novel view synthesis from unposed images in a\nsingle feed-forward. Our framework capitalizes on fast speed, scalability, and\nhigh-quality 3D reconstruction and view synthesis capabilities of 3DGS, where\nwe further extend it to offer a practical solution that relaxes common\nassumptions such as dense image views, accurate camera poses, and substantial\nimage overlaps. We achieve this through identifying and addressing unique\nchallenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians\nacross different views induce noisy or sparse gradients that destabilize\ntraining and hinder convergence, especially when above assumptions are not met.\nTo mitigate this, we employ pre-trained monocular depth estimation and visual\ncorrespondence models to achieve coarse alignments of 3D Gaussians. We then\nintroduce lightweight, learnable modules to refine depth and pose estimates\nfrom the coarse alignments, improving the quality of 3D reconstruction and\nnovel view synthesis. Furthermore, the refined estimates are leveraged to\nestimate geometry confidence scores, which assess the reliability of 3D\nGaussian centers and condition the prediction of Gaussian parameters\naccordingly. Extensive evaluations on large-scale real-world datasets\ndemonstrate that PF3plat sets a new state-of-the-art across all benchmarks,\nsupported by comprehensive ablation studies validating our design choices.\n", "link": "http://arxiv.org/abs/2410.22128v1", "date": "2024-10-29", "relevancy": 3.5359, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7377}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7201}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PF3plat%3A%20Pose-Free%20Feed-Forward%203D%20Gaussian%20Splatting&body=Title%3A%20PF3plat%3A%20Pose-Free%20Feed-Forward%203D%20Gaussian%20Splatting%0AAuthor%3A%20Sunghwan%20Hong%20and%20Jaewoo%20Jung%20and%20Heeseong%20Shin%20and%20Jisang%20Han%20and%20Jiaolong%20Yang%20and%20Chong%20Luo%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20novel%20view%20synthesis%20from%20unposed%20images%20in%20a%0Asingle%20feed-forward.%20Our%20framework%20capitalizes%20on%20fast%20speed%2C%20scalability%2C%20and%0Ahigh-quality%203D%20reconstruction%20and%20view%20synthesis%20capabilities%20of%203DGS%2C%20where%0Awe%20further%20extend%20it%20to%20offer%20a%20practical%20solution%20that%20relaxes%20common%0Aassumptions%20such%20as%20dense%20image%20views%2C%20accurate%20camera%20poses%2C%20and%20substantial%0Aimage%20overlaps.%20We%20achieve%20this%20through%20identifying%20and%20addressing%20unique%0Achallenges%20arising%20from%20the%20use%20of%20pixel-aligned%203DGS%3A%20misaligned%203D%20Gaussians%0Aacross%20different%20views%20induce%20noisy%20or%20sparse%20gradients%20that%20destabilize%0Atraining%20and%20hinder%20convergence%2C%20especially%20when%20above%20assumptions%20are%20not%20met.%0ATo%20mitigate%20this%2C%20we%20employ%20pre-trained%20monocular%20depth%20estimation%20and%20visual%0Acorrespondence%20models%20to%20achieve%20coarse%20alignments%20of%203D%20Gaussians.%20We%20then%0Aintroduce%20lightweight%2C%20learnable%20modules%20to%20refine%20depth%20and%20pose%20estimates%0Afrom%20the%20coarse%20alignments%2C%20improving%20the%20quality%20of%203D%20reconstruction%20and%0Anovel%20view%20synthesis.%20Furthermore%2C%20the%20refined%20estimates%20are%20leveraged%20to%0Aestimate%20geometry%20confidence%20scores%2C%20which%20assess%20the%20reliability%20of%203D%0AGaussian%20centers%20and%20condition%20the%20prediction%20of%20Gaussian%20parameters%0Aaccordingly.%20Extensive%20evaluations%20on%20large-scale%20real-world%20datasets%0Ademonstrate%20that%20PF3plat%20sets%20a%20new%20state-of-the-art%20across%20all%20benchmarks%2C%0Asupported%20by%20comprehensive%20ablation%20studies%20validating%20our%20design%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPF3plat%253A%2520Pose-Free%2520Feed-Forward%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DSunghwan%2520Hong%2520and%2520Jaewoo%2520Jung%2520and%2520Heeseong%2520Shin%2520and%2520Jisang%2520Han%2520and%2520Jiaolong%2520Yang%2520and%2520Chong%2520Luo%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520novel%2520view%2520synthesis%2520from%2520unposed%2520images%2520in%2520a%250Asingle%2520feed-forward.%2520Our%2520framework%2520capitalizes%2520on%2520fast%2520speed%252C%2520scalability%252C%2520and%250Ahigh-quality%25203D%2520reconstruction%2520and%2520view%2520synthesis%2520capabilities%2520of%25203DGS%252C%2520where%250Awe%2520further%2520extend%2520it%2520to%2520offer%2520a%2520practical%2520solution%2520that%2520relaxes%2520common%250Aassumptions%2520such%2520as%2520dense%2520image%2520views%252C%2520accurate%2520camera%2520poses%252C%2520and%2520substantial%250Aimage%2520overlaps.%2520We%2520achieve%2520this%2520through%2520identifying%2520and%2520addressing%2520unique%250Achallenges%2520arising%2520from%2520the%2520use%2520of%2520pixel-aligned%25203DGS%253A%2520misaligned%25203D%2520Gaussians%250Aacross%2520different%2520views%2520induce%2520noisy%2520or%2520sparse%2520gradients%2520that%2520destabilize%250Atraining%2520and%2520hinder%2520convergence%252C%2520especially%2520when%2520above%2520assumptions%2520are%2520not%2520met.%250ATo%2520mitigate%2520this%252C%2520we%2520employ%2520pre-trained%2520monocular%2520depth%2520estimation%2520and%2520visual%250Acorrespondence%2520models%2520to%2520achieve%2520coarse%2520alignments%2520of%25203D%2520Gaussians.%2520We%2520then%250Aintroduce%2520lightweight%252C%2520learnable%2520modules%2520to%2520refine%2520depth%2520and%2520pose%2520estimates%250Afrom%2520the%2520coarse%2520alignments%252C%2520improving%2520the%2520quality%2520of%25203D%2520reconstruction%2520and%250Anovel%2520view%2520synthesis.%2520Furthermore%252C%2520the%2520refined%2520estimates%2520are%2520leveraged%2520to%250Aestimate%2520geometry%2520confidence%2520scores%252C%2520which%2520assess%2520the%2520reliability%2520of%25203D%250AGaussian%2520centers%2520and%2520condition%2520the%2520prediction%2520of%2520Gaussian%2520parameters%250Aaccordingly.%2520Extensive%2520evaluations%2520on%2520large-scale%2520real-world%2520datasets%250Ademonstrate%2520that%2520PF3plat%2520sets%2520a%2520new%2520state-of-the-art%2520across%2520all%2520benchmarks%252C%250Asupported%2520by%2520comprehensive%2520ablation%2520studies%2520validating%2520our%2520design%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PF3plat%3A%20Pose-Free%20Feed-Forward%203D%20Gaussian%20Splatting&entry.906535625=Sunghwan%20Hong%20and%20Jaewoo%20Jung%20and%20Heeseong%20Shin%20and%20Jisang%20Han%20and%20Jiaolong%20Yang%20and%20Chong%20Luo%20and%20Seungryong%20Kim&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20novel%20view%20synthesis%20from%20unposed%20images%20in%20a%0Asingle%20feed-forward.%20Our%20framework%20capitalizes%20on%20fast%20speed%2C%20scalability%2C%20and%0Ahigh-quality%203D%20reconstruction%20and%20view%20synthesis%20capabilities%20of%203DGS%2C%20where%0Awe%20further%20extend%20it%20to%20offer%20a%20practical%20solution%20that%20relaxes%20common%0Aassumptions%20such%20as%20dense%20image%20views%2C%20accurate%20camera%20poses%2C%20and%20substantial%0Aimage%20overlaps.%20We%20achieve%20this%20through%20identifying%20and%20addressing%20unique%0Achallenges%20arising%20from%20the%20use%20of%20pixel-aligned%203DGS%3A%20misaligned%203D%20Gaussians%0Aacross%20different%20views%20induce%20noisy%20or%20sparse%20gradients%20that%20destabilize%0Atraining%20and%20hinder%20convergence%2C%20especially%20when%20above%20assumptions%20are%20not%20met.%0ATo%20mitigate%20this%2C%20we%20employ%20pre-trained%20monocular%20depth%20estimation%20and%20visual%0Acorrespondence%20models%20to%20achieve%20coarse%20alignments%20of%203D%20Gaussians.%20We%20then%0Aintroduce%20lightweight%2C%20learnable%20modules%20to%20refine%20depth%20and%20pose%20estimates%0Afrom%20the%20coarse%20alignments%2C%20improving%20the%20quality%20of%203D%20reconstruction%20and%0Anovel%20view%20synthesis.%20Furthermore%2C%20the%20refined%20estimates%20are%20leveraged%20to%0Aestimate%20geometry%20confidence%20scores%2C%20which%20assess%20the%20reliability%20of%203D%0AGaussian%20centers%20and%20condition%20the%20prediction%20of%20Gaussian%20parameters%0Aaccordingly.%20Extensive%20evaluations%20on%20large-scale%20real-world%20datasets%0Ademonstrate%20that%20PF3plat%20sets%20a%20new%20state-of-the-art%20across%20all%20benchmarks%2C%0Asupported%20by%20comprehensive%20ablation%20studies%20validating%20our%20design%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22128v1&entry.124074799=Read"},
{"title": "GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction", "author": "Yuxuan Mu and Xinxin Zuo and Chuan Guo and Yilin Wang and Juwei Lu and Xiaofeng Wu and Songcen Xu and Peng Dai and Youliang Yan and Li Cheng", "abstract": "  We present GSD, a diffusion model approach based on Gaussian Splatting (GS)\nrepresentation for 3D object reconstruction from a single view. Prior works\nsuffer from inconsistent 3D geometry or mediocre rendering quality due to\nimproper representations. We take a step towards resolving these shortcomings\nby utilizing the recent state-of-the-art 3D explicit representation, Gaussian\nSplatting, and an unconditional diffusion model. This model learns to generate\n3D objects represented by sets of GS ellipsoids. With these strong generative\n3D priors, though learning unconditionally, the diffusion model is ready for\nview-guided reconstruction without further model fine-tuning. This is achieved\nby propagating fine-grained 2D features through the efficient yet flexible\nsplatting function and the guided denoising sampling process. In addition, a 2D\ndiffusion model is further employed to enhance rendering fidelity, and improve\nreconstructed GS quality by polishing and re-using the rendered images. The\nfinal reconstructed objects explicitly come with high-quality 3D structure and\ntexture, and can be efficiently rendered in arbitrary views. Experiments on the\nchallenging real-world CO3D dataset demonstrate the superiority of our\napproach. Project page: https://yxmu.foo/GSD/\n", "link": "http://arxiv.org/abs/2407.04237v4", "date": "2024-10-29", "relevancy": 3.5271, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7205}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7194}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSD%3A%20View-Guided%20Gaussian%20Splatting%20Diffusion%20for%203D%20Reconstruction&body=Title%3A%20GSD%3A%20View-Guided%20Gaussian%20Splatting%20Diffusion%20for%203D%20Reconstruction%0AAuthor%3A%20Yuxuan%20Mu%20and%20Xinxin%20Zuo%20and%20Chuan%20Guo%20and%20Yilin%20Wang%20and%20Juwei%20Lu%20and%20Xiaofeng%20Wu%20and%20Songcen%20Xu%20and%20Peng%20Dai%20and%20Youliang%20Yan%20and%20Li%20Cheng%0AAbstract%3A%20%20%20We%20present%20GSD%2C%20a%20diffusion%20model%20approach%20based%20on%20Gaussian%20Splatting%20%28GS%29%0Arepresentation%20for%203D%20object%20reconstruction%20from%20a%20single%20view.%20Prior%20works%0Asuffer%20from%20inconsistent%203D%20geometry%20or%20mediocre%20rendering%20quality%20due%20to%0Aimproper%20representations.%20We%20take%20a%20step%20towards%20resolving%20these%20shortcomings%0Aby%20utilizing%20the%20recent%20state-of-the-art%203D%20explicit%20representation%2C%20Gaussian%0ASplatting%2C%20and%20an%20unconditional%20diffusion%20model.%20This%20model%20learns%20to%20generate%0A3D%20objects%20represented%20by%20sets%20of%20GS%20ellipsoids.%20With%20these%20strong%20generative%0A3D%20priors%2C%20though%20learning%20unconditionally%2C%20the%20diffusion%20model%20is%20ready%20for%0Aview-guided%20reconstruction%20without%20further%20model%20fine-tuning.%20This%20is%20achieved%0Aby%20propagating%20fine-grained%202D%20features%20through%20the%20efficient%20yet%20flexible%0Asplatting%20function%20and%20the%20guided%20denoising%20sampling%20process.%20In%20addition%2C%20a%202D%0Adiffusion%20model%20is%20further%20employed%20to%20enhance%20rendering%20fidelity%2C%20and%20improve%0Areconstructed%20GS%20quality%20by%20polishing%20and%20re-using%20the%20rendered%20images.%20The%0Afinal%20reconstructed%20objects%20explicitly%20come%20with%20high-quality%203D%20structure%20and%0Atexture%2C%20and%20can%20be%20efficiently%20rendered%20in%20arbitrary%20views.%20Experiments%20on%20the%0Achallenging%20real-world%20CO3D%20dataset%20demonstrate%20the%20superiority%20of%20our%0Aapproach.%20Project%20page%3A%20https%3A//yxmu.foo/GSD/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04237v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSD%253A%2520View-Guided%2520Gaussian%2520Splatting%2520Diffusion%2520for%25203D%2520Reconstruction%26entry.906535625%3DYuxuan%2520Mu%2520and%2520Xinxin%2520Zuo%2520and%2520Chuan%2520Guo%2520and%2520Yilin%2520Wang%2520and%2520Juwei%2520Lu%2520and%2520Xiaofeng%2520Wu%2520and%2520Songcen%2520Xu%2520and%2520Peng%2520Dai%2520and%2520Youliang%2520Yan%2520and%2520Li%2520Cheng%26entry.1292438233%3D%2520%2520We%2520present%2520GSD%252C%2520a%2520diffusion%2520model%2520approach%2520based%2520on%2520Gaussian%2520Splatting%2520%2528GS%2529%250Arepresentation%2520for%25203D%2520object%2520reconstruction%2520from%2520a%2520single%2520view.%2520Prior%2520works%250Asuffer%2520from%2520inconsistent%25203D%2520geometry%2520or%2520mediocre%2520rendering%2520quality%2520due%2520to%250Aimproper%2520representations.%2520We%2520take%2520a%2520step%2520towards%2520resolving%2520these%2520shortcomings%250Aby%2520utilizing%2520the%2520recent%2520state-of-the-art%25203D%2520explicit%2520representation%252C%2520Gaussian%250ASplatting%252C%2520and%2520an%2520unconditional%2520diffusion%2520model.%2520This%2520model%2520learns%2520to%2520generate%250A3D%2520objects%2520represented%2520by%2520sets%2520of%2520GS%2520ellipsoids.%2520With%2520these%2520strong%2520generative%250A3D%2520priors%252C%2520though%2520learning%2520unconditionally%252C%2520the%2520diffusion%2520model%2520is%2520ready%2520for%250Aview-guided%2520reconstruction%2520without%2520further%2520model%2520fine-tuning.%2520This%2520is%2520achieved%250Aby%2520propagating%2520fine-grained%25202D%2520features%2520through%2520the%2520efficient%2520yet%2520flexible%250Asplatting%2520function%2520and%2520the%2520guided%2520denoising%2520sampling%2520process.%2520In%2520addition%252C%2520a%25202D%250Adiffusion%2520model%2520is%2520further%2520employed%2520to%2520enhance%2520rendering%2520fidelity%252C%2520and%2520improve%250Areconstructed%2520GS%2520quality%2520by%2520polishing%2520and%2520re-using%2520the%2520rendered%2520images.%2520The%250Afinal%2520reconstructed%2520objects%2520explicitly%2520come%2520with%2520high-quality%25203D%2520structure%2520and%250Atexture%252C%2520and%2520can%2520be%2520efficiently%2520rendered%2520in%2520arbitrary%2520views.%2520Experiments%2520on%2520the%250Achallenging%2520real-world%2520CO3D%2520dataset%2520demonstrate%2520the%2520superiority%2520of%2520our%250Aapproach.%2520Project%2520page%253A%2520https%253A//yxmu.foo/GSD/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04237v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSD%3A%20View-Guided%20Gaussian%20Splatting%20Diffusion%20for%203D%20Reconstruction&entry.906535625=Yuxuan%20Mu%20and%20Xinxin%20Zuo%20and%20Chuan%20Guo%20and%20Yilin%20Wang%20and%20Juwei%20Lu%20and%20Xiaofeng%20Wu%20and%20Songcen%20Xu%20and%20Peng%20Dai%20and%20Youliang%20Yan%20and%20Li%20Cheng&entry.1292438233=%20%20We%20present%20GSD%2C%20a%20diffusion%20model%20approach%20based%20on%20Gaussian%20Splatting%20%28GS%29%0Arepresentation%20for%203D%20object%20reconstruction%20from%20a%20single%20view.%20Prior%20works%0Asuffer%20from%20inconsistent%203D%20geometry%20or%20mediocre%20rendering%20quality%20due%20to%0Aimproper%20representations.%20We%20take%20a%20step%20towards%20resolving%20these%20shortcomings%0Aby%20utilizing%20the%20recent%20state-of-the-art%203D%20explicit%20representation%2C%20Gaussian%0ASplatting%2C%20and%20an%20unconditional%20diffusion%20model.%20This%20model%20learns%20to%20generate%0A3D%20objects%20represented%20by%20sets%20of%20GS%20ellipsoids.%20With%20these%20strong%20generative%0A3D%20priors%2C%20though%20learning%20unconditionally%2C%20the%20diffusion%20model%20is%20ready%20for%0Aview-guided%20reconstruction%20without%20further%20model%20fine-tuning.%20This%20is%20achieved%0Aby%20propagating%20fine-grained%202D%20features%20through%20the%20efficient%20yet%20flexible%0Asplatting%20function%20and%20the%20guided%20denoising%20sampling%20process.%20In%20addition%2C%20a%202D%0Adiffusion%20model%20is%20further%20employed%20to%20enhance%20rendering%20fidelity%2C%20and%20improve%0Areconstructed%20GS%20quality%20by%20polishing%20and%20re-using%20the%20rendered%20images.%20The%0Afinal%20reconstructed%20objects%20explicitly%20come%20with%20high-quality%203D%20structure%20and%0Atexture%2C%20and%20can%20be%20efficiently%20rendered%20in%20arbitrary%20views.%20Experiments%20on%20the%0Achallenging%20real-world%20CO3D%20dataset%20demonstrate%20the%20superiority%20of%20our%0Aapproach.%20Project%20page%3A%20https%3A//yxmu.foo/GSD/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04237v4&entry.124074799=Read"},
{"title": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D\n  Reconstruction Via Gaussian Consensus", "author": "Yu Chen and Gim Hee Lee", "abstract": "  The recent advances in 3D Gaussian Splatting (3DGS) show promising results on\nthe novel view synthesis (NVS) task. With its superior rendering performance\nand high-fidelity rendering quality, 3DGS is excelling at its previous NeRF\ncounterparts. The most recent 3DGS method focuses either on improving the\ninstability of rendering efficiency or reducing the model size. On the other\nhand, the training efficiency of 3DGS on large-scale scenes has not gained much\nattention. In this work, we propose DoGaussian, a method that trains 3DGS\ndistributedly. Our method first decomposes a scene into K blocks and then\nintroduces the Alternating Direction Method of Multipliers (ADMM) into the\ntraining procedure of 3DGS. During training, our DOGS maintains one global 3DGS\nmodel on the master node and K local 3DGS models on the slave nodes. The K\nlocal 3DGS models are dropped after training and we only query the global 3DGS\nmodel during inference. The training time is reduced by scene decomposition,\nand the training convergence and stability are guaranteed through the consensus\non the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+\ntimes when evaluated on large-scale scenes while concurrently achieving\nstate-of-the-art rendering quality. Our code is publicly available at\nhttps://github.com/AIBluefisher/DOGS.\n", "link": "http://arxiv.org/abs/2405.13943v2", "date": "2024-10-29", "relevancy": 3.4982, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7242}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6904}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOGS%3A%20Distributed-Oriented%20Gaussian%20Splatting%20for%20Large-Scale%203D%0A%20%20Reconstruction%20Via%20Gaussian%20Consensus&body=Title%3A%20DOGS%3A%20Distributed-Oriented%20Gaussian%20Splatting%20for%20Large-Scale%203D%0A%20%20Reconstruction%20Via%20Gaussian%20Consensus%0AAuthor%3A%20Yu%20Chen%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20The%20recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20show%20promising%20results%20on%0Athe%20novel%20view%20synthesis%20%28NVS%29%20task.%20With%20its%20superior%20rendering%20performance%0Aand%20high-fidelity%20rendering%20quality%2C%203DGS%20is%20excelling%20at%20its%20previous%20NeRF%0Acounterparts.%20The%20most%20recent%203DGS%20method%20focuses%20either%20on%20improving%20the%0Ainstability%20of%20rendering%20efficiency%20or%20reducing%20the%20model%20size.%20On%20the%20other%0Ahand%2C%20the%20training%20efficiency%20of%203DGS%20on%20large-scale%20scenes%20has%20not%20gained%20much%0Aattention.%20In%20this%20work%2C%20we%20propose%20DoGaussian%2C%20a%20method%20that%20trains%203DGS%0Adistributedly.%20Our%20method%20first%20decomposes%20a%20scene%20into%20K%20blocks%20and%20then%0Aintroduces%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20into%20the%0Atraining%20procedure%20of%203DGS.%20During%20training%2C%20our%20DOGS%20maintains%20one%20global%203DGS%0Amodel%20on%20the%20master%20node%20and%20K%20local%203DGS%20models%20on%20the%20slave%20nodes.%20The%20K%0Alocal%203DGS%20models%20are%20dropped%20after%20training%20and%20we%20only%20query%20the%20global%203DGS%0Amodel%20during%20inference.%20The%20training%20time%20is%20reduced%20by%20scene%20decomposition%2C%0Aand%20the%20training%20convergence%20and%20stability%20are%20guaranteed%20through%20the%20consensus%0Aon%20the%20shared%203D%20Gaussians.%20Our%20method%20accelerates%20the%20training%20of%203DGS%20by%206%2B%0Atimes%20when%20evaluated%20on%20large-scale%20scenes%20while%20concurrently%20achieving%0Astate-of-the-art%20rendering%20quality.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/AIBluefisher/DOGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOGS%253A%2520Distributed-Oriented%2520Gaussian%2520Splatting%2520for%2520Large-Scale%25203D%250A%2520%2520Reconstruction%2520Via%2520Gaussian%2520Consensus%26entry.906535625%3DYu%2520Chen%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520The%2520recent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520show%2520promising%2520results%2520on%250Athe%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520task.%2520With%2520its%2520superior%2520rendering%2520performance%250Aand%2520high-fidelity%2520rendering%2520quality%252C%25203DGS%2520is%2520excelling%2520at%2520its%2520previous%2520NeRF%250Acounterparts.%2520The%2520most%2520recent%25203DGS%2520method%2520focuses%2520either%2520on%2520improving%2520the%250Ainstability%2520of%2520rendering%2520efficiency%2520or%2520reducing%2520the%2520model%2520size.%2520On%2520the%2520other%250Ahand%252C%2520the%2520training%2520efficiency%2520of%25203DGS%2520on%2520large-scale%2520scenes%2520has%2520not%2520gained%2520much%250Aattention.%2520In%2520this%2520work%252C%2520we%2520propose%2520DoGaussian%252C%2520a%2520method%2520that%2520trains%25203DGS%250Adistributedly.%2520Our%2520method%2520first%2520decomposes%2520a%2520scene%2520into%2520K%2520blocks%2520and%2520then%250Aintroduces%2520the%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528ADMM%2529%2520into%2520the%250Atraining%2520procedure%2520of%25203DGS.%2520During%2520training%252C%2520our%2520DOGS%2520maintains%2520one%2520global%25203DGS%250Amodel%2520on%2520the%2520master%2520node%2520and%2520K%2520local%25203DGS%2520models%2520on%2520the%2520slave%2520nodes.%2520The%2520K%250Alocal%25203DGS%2520models%2520are%2520dropped%2520after%2520training%2520and%2520we%2520only%2520query%2520the%2520global%25203DGS%250Amodel%2520during%2520inference.%2520The%2520training%2520time%2520is%2520reduced%2520by%2520scene%2520decomposition%252C%250Aand%2520the%2520training%2520convergence%2520and%2520stability%2520are%2520guaranteed%2520through%2520the%2520consensus%250Aon%2520the%2520shared%25203D%2520Gaussians.%2520Our%2520method%2520accelerates%2520the%2520training%2520of%25203DGS%2520by%25206%252B%250Atimes%2520when%2520evaluated%2520on%2520large-scale%2520scenes%2520while%2520concurrently%2520achieving%250Astate-of-the-art%2520rendering%2520quality.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/AIBluefisher/DOGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOGS%3A%20Distributed-Oriented%20Gaussian%20Splatting%20for%20Large-Scale%203D%0A%20%20Reconstruction%20Via%20Gaussian%20Consensus&entry.906535625=Yu%20Chen%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20The%20recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20show%20promising%20results%20on%0Athe%20novel%20view%20synthesis%20%28NVS%29%20task.%20With%20its%20superior%20rendering%20performance%0Aand%20high-fidelity%20rendering%20quality%2C%203DGS%20is%20excelling%20at%20its%20previous%20NeRF%0Acounterparts.%20The%20most%20recent%203DGS%20method%20focuses%20either%20on%20improving%20the%0Ainstability%20of%20rendering%20efficiency%20or%20reducing%20the%20model%20size.%20On%20the%20other%0Ahand%2C%20the%20training%20efficiency%20of%203DGS%20on%20large-scale%20scenes%20has%20not%20gained%20much%0Aattention.%20In%20this%20work%2C%20we%20propose%20DoGaussian%2C%20a%20method%20that%20trains%203DGS%0Adistributedly.%20Our%20method%20first%20decomposes%20a%20scene%20into%20K%20blocks%20and%20then%0Aintroduces%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20into%20the%0Atraining%20procedure%20of%203DGS.%20During%20training%2C%20our%20DOGS%20maintains%20one%20global%203DGS%0Amodel%20on%20the%20master%20node%20and%20K%20local%203DGS%20models%20on%20the%20slave%20nodes.%20The%20K%0Alocal%203DGS%20models%20are%20dropped%20after%20training%20and%20we%20only%20query%20the%20global%203DGS%0Amodel%20during%20inference.%20The%20training%20time%20is%20reduced%20by%20scene%20decomposition%2C%0Aand%20the%20training%20convergence%20and%20stability%20are%20guaranteed%20through%20the%20consensus%0Aon%20the%20shared%203D%20Gaussians.%20Our%20method%20accelerates%20the%20training%20of%203DGS%20by%206%2B%0Atimes%20when%20evaluated%20on%20large-scale%20scenes%20while%20concurrently%20achieving%0Astate-of-the-art%20rendering%20quality.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/AIBluefisher/DOGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13943v2&entry.124074799=Read"},
{"title": "ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian\n  Splatting", "author": "Yuetao Li and Zijia Kuang and Ting Li and Guyue Zhou and Shaohui Zhang and Zike Yan", "abstract": "  We propose ActiveSplat, an autonomous high-fidelity reconstruction system\nleveraging Gaussian splatting. Taking advantage of efficient and realistic\nrendering, the system establishes a unified framework for online mapping,\nviewpoint selection, and path planning. The key to ActiveSplat is a hybrid map\nrepresentation that integrates both dense information about the environment and\na sparse abstraction of the workspace. Therefore, the system leverages sparse\ntopology for efficient viewpoint sampling and path planning, while exploiting\nview-dependent dense prediction for viewpoint selection, facilitating efficient\ndecision-making with promising accuracy and completeness. A hierarchical\nplanning strategy based on the topological map is adopted to mitigate\nrepetitive trajectories and improve local granularity given limited budgets,\nensuring high-fidelity reconstruction with photorealistic view synthesis.\nExtensive experiments and ablation studies validate the efficacy of the\nproposed method in terms of reconstruction accuracy, data coverage, and\nexploration efficiency. Project page: https://li-yuetao.github.io/ActiveSplat/.\n", "link": "http://arxiv.org/abs/2410.21955v1", "date": "2024-10-29", "relevancy": 3.3789, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7436}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActiveSplat%3A%20High-Fidelity%20Scene%20Reconstruction%20through%20Active%20Gaussian%0A%20%20Splatting&body=Title%3A%20ActiveSplat%3A%20High-Fidelity%20Scene%20Reconstruction%20through%20Active%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Yuetao%20Li%20and%20Zijia%20Kuang%20and%20Ting%20Li%20and%20Guyue%20Zhou%20and%20Shaohui%20Zhang%20and%20Zike%20Yan%0AAbstract%3A%20%20%20We%20propose%20ActiveSplat%2C%20an%20autonomous%20high-fidelity%20reconstruction%20system%0Aleveraging%20Gaussian%20splatting.%20Taking%20advantage%20of%20efficient%20and%20realistic%0Arendering%2C%20the%20system%20establishes%20a%20unified%20framework%20for%20online%20mapping%2C%0Aviewpoint%20selection%2C%20and%20path%20planning.%20The%20key%20to%20ActiveSplat%20is%20a%20hybrid%20map%0Arepresentation%20that%20integrates%20both%20dense%20information%20about%20the%20environment%20and%0Aa%20sparse%20abstraction%20of%20the%20workspace.%20Therefore%2C%20the%20system%20leverages%20sparse%0Atopology%20for%20efficient%20viewpoint%20sampling%20and%20path%20planning%2C%20while%20exploiting%0Aview-dependent%20dense%20prediction%20for%20viewpoint%20selection%2C%20facilitating%20efficient%0Adecision-making%20with%20promising%20accuracy%20and%20completeness.%20A%20hierarchical%0Aplanning%20strategy%20based%20on%20the%20topological%20map%20is%20adopted%20to%20mitigate%0Arepetitive%20trajectories%20and%20improve%20local%20granularity%20given%20limited%20budgets%2C%0Aensuring%20high-fidelity%20reconstruction%20with%20photorealistic%20view%20synthesis.%0AExtensive%20experiments%20and%20ablation%20studies%20validate%20the%20efficacy%20of%20the%0Aproposed%20method%20in%20terms%20of%20reconstruction%20accuracy%2C%20data%20coverage%2C%20and%0Aexploration%20efficiency.%20Project%20page%3A%20https%3A//li-yuetao.github.io/ActiveSplat/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActiveSplat%253A%2520High-Fidelity%2520Scene%2520Reconstruction%2520through%2520Active%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DYuetao%2520Li%2520and%2520Zijia%2520Kuang%2520and%2520Ting%2520Li%2520and%2520Guyue%2520Zhou%2520and%2520Shaohui%2520Zhang%2520and%2520Zike%2520Yan%26entry.1292438233%3D%2520%2520We%2520propose%2520ActiveSplat%252C%2520an%2520autonomous%2520high-fidelity%2520reconstruction%2520system%250Aleveraging%2520Gaussian%2520splatting.%2520Taking%2520advantage%2520of%2520efficient%2520and%2520realistic%250Arendering%252C%2520the%2520system%2520establishes%2520a%2520unified%2520framework%2520for%2520online%2520mapping%252C%250Aviewpoint%2520selection%252C%2520and%2520path%2520planning.%2520The%2520key%2520to%2520ActiveSplat%2520is%2520a%2520hybrid%2520map%250Arepresentation%2520that%2520integrates%2520both%2520dense%2520information%2520about%2520the%2520environment%2520and%250Aa%2520sparse%2520abstraction%2520of%2520the%2520workspace.%2520Therefore%252C%2520the%2520system%2520leverages%2520sparse%250Atopology%2520for%2520efficient%2520viewpoint%2520sampling%2520and%2520path%2520planning%252C%2520while%2520exploiting%250Aview-dependent%2520dense%2520prediction%2520for%2520viewpoint%2520selection%252C%2520facilitating%2520efficient%250Adecision-making%2520with%2520promising%2520accuracy%2520and%2520completeness.%2520A%2520hierarchical%250Aplanning%2520strategy%2520based%2520on%2520the%2520topological%2520map%2520is%2520adopted%2520to%2520mitigate%250Arepetitive%2520trajectories%2520and%2520improve%2520local%2520granularity%2520given%2520limited%2520budgets%252C%250Aensuring%2520high-fidelity%2520reconstruction%2520with%2520photorealistic%2520view%2520synthesis.%250AExtensive%2520experiments%2520and%2520ablation%2520studies%2520validate%2520the%2520efficacy%2520of%2520the%250Aproposed%2520method%2520in%2520terms%2520of%2520reconstruction%2520accuracy%252C%2520data%2520coverage%252C%2520and%250Aexploration%2520efficiency.%2520Project%2520page%253A%2520https%253A//li-yuetao.github.io/ActiveSplat/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActiveSplat%3A%20High-Fidelity%20Scene%20Reconstruction%20through%20Active%20Gaussian%0A%20%20Splatting&entry.906535625=Yuetao%20Li%20and%20Zijia%20Kuang%20and%20Ting%20Li%20and%20Guyue%20Zhou%20and%20Shaohui%20Zhang%20and%20Zike%20Yan&entry.1292438233=%20%20We%20propose%20ActiveSplat%2C%20an%20autonomous%20high-fidelity%20reconstruction%20system%0Aleveraging%20Gaussian%20splatting.%20Taking%20advantage%20of%20efficient%20and%20realistic%0Arendering%2C%20the%20system%20establishes%20a%20unified%20framework%20for%20online%20mapping%2C%0Aviewpoint%20selection%2C%20and%20path%20planning.%20The%20key%20to%20ActiveSplat%20is%20a%20hybrid%20map%0Arepresentation%20that%20integrates%20both%20dense%20information%20about%20the%20environment%20and%0Aa%20sparse%20abstraction%20of%20the%20workspace.%20Therefore%2C%20the%20system%20leverages%20sparse%0Atopology%20for%20efficient%20viewpoint%20sampling%20and%20path%20planning%2C%20while%20exploiting%0Aview-dependent%20dense%20prediction%20for%20viewpoint%20selection%2C%20facilitating%20efficient%0Adecision-making%20with%20promising%20accuracy%20and%20completeness.%20A%20hierarchical%0Aplanning%20strategy%20based%20on%20the%20topological%20map%20is%20adopted%20to%20mitigate%0Arepetitive%20trajectories%20and%20improve%20local%20granularity%20given%20limited%20budgets%2C%0Aensuring%20high-fidelity%20reconstruction%20with%20photorealistic%20view%20synthesis.%0AExtensive%20experiments%20and%20ablation%20studies%20validate%20the%20efficacy%20of%20the%0Aproposed%20method%20in%20terms%20of%20reconstruction%20accuracy%2C%20data%20coverage%2C%20and%0Aexploration%20efficiency.%20Project%20page%3A%20https%3A//li-yuetao.github.io/ActiveSplat/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21955v1&entry.124074799=Read"},
{"title": "FreeGaussian: Guidance-free Controllable 3D Gaussian Splats with Flow\n  Derivatives", "author": "Qizhi Chen and Delin Qu and Yiwen Tang and Haoming Song and Yiting Zhang and Dong Wang and Bin Zhao and Xuelong Li", "abstract": "  Reconstructing controllable Gaussian splats from monocular video is a\nchallenging task due to its inherently insufficient constraints. Widely adopted\napproaches supervise complex interactions with additional masks and control\nsignal annotations, limiting their real-world applications. In this paper, we\npropose an annotation guidance-free method, dubbed FreeGaussian, that\nmathematically derives dynamic Gaussian motion from optical flow and camera\nmotion using novel dynamic Gaussian constraints. By establishing a connection\nbetween 2D flows and 3D Gaussian dynamic control, our method enables\nself-supervised optimization and continuity of dynamic Gaussian motions from\nflow priors. Furthermore, we introduce a 3D spherical vector controlling\nscheme, which represents the state with a 3D Gaussian trajectory, thereby\neliminating the need for complex 1D control signal calculations and simplifying\ncontrollable Gaussian modeling. Quantitative and qualitative evaluations on\nextensive experiments demonstrate the state-of-the-art visual performance and\ncontrol capability of our method. Project page: https://freegaussian.github.io.\n", "link": "http://arxiv.org/abs/2410.22070v1", "date": "2024-10-29", "relevancy": 3.3046, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6705}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.658}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeGaussian%3A%20Guidance-free%20Controllable%203D%20Gaussian%20Splats%20with%20Flow%0A%20%20Derivatives&body=Title%3A%20FreeGaussian%3A%20Guidance-free%20Controllable%203D%20Gaussian%20Splats%20with%20Flow%0A%20%20Derivatives%0AAuthor%3A%20Qizhi%20Chen%20and%20Delin%20Qu%20and%20Yiwen%20Tang%20and%20Haoming%20Song%20and%20Yiting%20Zhang%20and%20Dong%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Reconstructing%20controllable%20Gaussian%20splats%20from%20monocular%20video%20is%20a%0Achallenging%20task%20due%20to%20its%20inherently%20insufficient%20constraints.%20Widely%20adopted%0Aapproaches%20supervise%20complex%20interactions%20with%20additional%20masks%20and%20control%0Asignal%20annotations%2C%20limiting%20their%20real-world%20applications.%20In%20this%20paper%2C%20we%0Apropose%20an%20annotation%20guidance-free%20method%2C%20dubbed%20FreeGaussian%2C%20that%0Amathematically%20derives%20dynamic%20Gaussian%20motion%20from%20optical%20flow%20and%20camera%0Amotion%20using%20novel%20dynamic%20Gaussian%20constraints.%20By%20establishing%20a%20connection%0Abetween%202D%20flows%20and%203D%20Gaussian%20dynamic%20control%2C%20our%20method%20enables%0Aself-supervised%20optimization%20and%20continuity%20of%20dynamic%20Gaussian%20motions%20from%0Aflow%20priors.%20Furthermore%2C%20we%20introduce%20a%203D%20spherical%20vector%20controlling%0Ascheme%2C%20which%20represents%20the%20state%20with%20a%203D%20Gaussian%20trajectory%2C%20thereby%0Aeliminating%20the%20need%20for%20complex%201D%20control%20signal%20calculations%20and%20simplifying%0Acontrollable%20Gaussian%20modeling.%20Quantitative%20and%20qualitative%20evaluations%20on%0Aextensive%20experiments%20demonstrate%20the%20state-of-the-art%20visual%20performance%20and%0Acontrol%20capability%20of%20our%20method.%20Project%20page%3A%20https%3A//freegaussian.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeGaussian%253A%2520Guidance-free%2520Controllable%25203D%2520Gaussian%2520Splats%2520with%2520Flow%250A%2520%2520Derivatives%26entry.906535625%3DQizhi%2520Chen%2520and%2520Delin%2520Qu%2520and%2520Yiwen%2520Tang%2520and%2520Haoming%2520Song%2520and%2520Yiting%2520Zhang%2520and%2520Dong%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Reconstructing%2520controllable%2520Gaussian%2520splats%2520from%2520monocular%2520video%2520is%2520a%250Achallenging%2520task%2520due%2520to%2520its%2520inherently%2520insufficient%2520constraints.%2520Widely%2520adopted%250Aapproaches%2520supervise%2520complex%2520interactions%2520with%2520additional%2520masks%2520and%2520control%250Asignal%2520annotations%252C%2520limiting%2520their%2520real-world%2520applications.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520annotation%2520guidance-free%2520method%252C%2520dubbed%2520FreeGaussian%252C%2520that%250Amathematically%2520derives%2520dynamic%2520Gaussian%2520motion%2520from%2520optical%2520flow%2520and%2520camera%250Amotion%2520using%2520novel%2520dynamic%2520Gaussian%2520constraints.%2520By%2520establishing%2520a%2520connection%250Abetween%25202D%2520flows%2520and%25203D%2520Gaussian%2520dynamic%2520control%252C%2520our%2520method%2520enables%250Aself-supervised%2520optimization%2520and%2520continuity%2520of%2520dynamic%2520Gaussian%2520motions%2520from%250Aflow%2520priors.%2520Furthermore%252C%2520we%2520introduce%2520a%25203D%2520spherical%2520vector%2520controlling%250Ascheme%252C%2520which%2520represents%2520the%2520state%2520with%2520a%25203D%2520Gaussian%2520trajectory%252C%2520thereby%250Aeliminating%2520the%2520need%2520for%2520complex%25201D%2520control%2520signal%2520calculations%2520and%2520simplifying%250Acontrollable%2520Gaussian%2520modeling.%2520Quantitative%2520and%2520qualitative%2520evaluations%2520on%250Aextensive%2520experiments%2520demonstrate%2520the%2520state-of-the-art%2520visual%2520performance%2520and%250Acontrol%2520capability%2520of%2520our%2520method.%2520Project%2520page%253A%2520https%253A//freegaussian.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeGaussian%3A%20Guidance-free%20Controllable%203D%20Gaussian%20Splats%20with%20Flow%0A%20%20Derivatives&entry.906535625=Qizhi%20Chen%20and%20Delin%20Qu%20and%20Yiwen%20Tang%20and%20Haoming%20Song%20and%20Yiting%20Zhang%20and%20Dong%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Reconstructing%20controllable%20Gaussian%20splats%20from%20monocular%20video%20is%20a%0Achallenging%20task%20due%20to%20its%20inherently%20insufficient%20constraints.%20Widely%20adopted%0Aapproaches%20supervise%20complex%20interactions%20with%20additional%20masks%20and%20control%0Asignal%20annotations%2C%20limiting%20their%20real-world%20applications.%20In%20this%20paper%2C%20we%0Apropose%20an%20annotation%20guidance-free%20method%2C%20dubbed%20FreeGaussian%2C%20that%0Amathematically%20derives%20dynamic%20Gaussian%20motion%20from%20optical%20flow%20and%20camera%0Amotion%20using%20novel%20dynamic%20Gaussian%20constraints.%20By%20establishing%20a%20connection%0Abetween%202D%20flows%20and%203D%20Gaussian%20dynamic%20control%2C%20our%20method%20enables%0Aself-supervised%20optimization%20and%20continuity%20of%20dynamic%20Gaussian%20motions%20from%0Aflow%20priors.%20Furthermore%2C%20we%20introduce%20a%203D%20spherical%20vector%20controlling%0Ascheme%2C%20which%20represents%20the%20state%20with%20a%203D%20Gaussian%20trajectory%2C%20thereby%0Aeliminating%20the%20need%20for%20complex%201D%20control%20signal%20calculations%20and%20simplifying%0Acontrollable%20Gaussian%20modeling.%20Quantitative%20and%20qualitative%20evaluations%20on%0Aextensive%20experiments%20demonstrate%20the%20state-of-the-art%20visual%20performance%20and%0Acontrol%20capability%20of%20our%20method.%20Project%20page%3A%20https%3A//freegaussian.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22070v1&entry.124074799=Read"},
{"title": "Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human\n  Reconstruction from Occluded Images", "author": "Donghwan Kim and Tae-Kyun Kim", "abstract": "  3D human shape reconstruction under severe occlusion due to human-object or\nhuman-human interaction is a challenging problem. Parametric models i.e.,\nSMPL(-X), which are based on the statistics across human shapes, can represent\nwhole human body shapes but are limited to minimally-clothed human shapes.\nImplicit-function-based methods extract features from the parametric models to\nemploy prior knowledge of human bodies and can capture geometric details such\nas clothing and hair. However, they often struggle to handle misaligned\nparametric models and inpaint occluded regions given a single RGB image. In\nthis work, we propose a novel pipeline, MHCDIFF, Multi-hypotheses Conditioned\nPoint Cloud Diffusion, composed of point cloud diffusion conditioned on\nprobabilistic distributions for pixel-aligned detailed 3D human reconstruction\nunder occlusion. Compared to previous implicit-function-based methods, the\npoint cloud diffusion model can capture the global consistent features to\ngenerate the occluded regions, and the denoising process corrects the\nmisaligned SMPL meshes. The core of MHCDIFF is extracting local features from\nmultiple hypothesized SMPL(-X) meshes and aggregating the set of features to\ncondition the diffusion model. In the experiments on CAPE and MultiHuman\ndatasets, the proposed method outperforms various SOTA methods based on SMPL,\nimplicit functions, point cloud diffusion, and their combined, under synthetic\nand real occlusions. Our code is publicly available at\nhttps://donghwankim0101.github.io/projects/mhcdiff/ .\n", "link": "http://arxiv.org/abs/2409.18364v3", "date": "2024-10-29", "relevancy": 3.2316, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6512}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6512}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-hypotheses%20Conditioned%20Point%20Cloud%20Diffusion%20for%203D%20Human%0A%20%20Reconstruction%20from%20Occluded%20Images&body=Title%3A%20Multi-hypotheses%20Conditioned%20Point%20Cloud%20Diffusion%20for%203D%20Human%0A%20%20Reconstruction%20from%20Occluded%20Images%0AAuthor%3A%20Donghwan%20Kim%20and%20Tae-Kyun%20Kim%0AAbstract%3A%20%20%203D%20human%20shape%20reconstruction%20under%20severe%20occlusion%20due%20to%20human-object%20or%0Ahuman-human%20interaction%20is%20a%20challenging%20problem.%20Parametric%20models%20i.e.%2C%0ASMPL%28-X%29%2C%20which%20are%20based%20on%20the%20statistics%20across%20human%20shapes%2C%20can%20represent%0Awhole%20human%20body%20shapes%20but%20are%20limited%20to%20minimally-clothed%20human%20shapes.%0AImplicit-function-based%20methods%20extract%20features%20from%20the%20parametric%20models%20to%0Aemploy%20prior%20knowledge%20of%20human%20bodies%20and%20can%20capture%20geometric%20details%20such%0Aas%20clothing%20and%20hair.%20However%2C%20they%20often%20struggle%20to%20handle%20misaligned%0Aparametric%20models%20and%20inpaint%20occluded%20regions%20given%20a%20single%20RGB%20image.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20pipeline%2C%20MHCDIFF%2C%20Multi-hypotheses%20Conditioned%0APoint%20Cloud%20Diffusion%2C%20composed%20of%20point%20cloud%20diffusion%20conditioned%20on%0Aprobabilistic%20distributions%20for%20pixel-aligned%20detailed%203D%20human%20reconstruction%0Aunder%20occlusion.%20Compared%20to%20previous%20implicit-function-based%20methods%2C%20the%0Apoint%20cloud%20diffusion%20model%20can%20capture%20the%20global%20consistent%20features%20to%0Agenerate%20the%20occluded%20regions%2C%20and%20the%20denoising%20process%20corrects%20the%0Amisaligned%20SMPL%20meshes.%20The%20core%20of%20MHCDIFF%20is%20extracting%20local%20features%20from%0Amultiple%20hypothesized%20SMPL%28-X%29%20meshes%20and%20aggregating%20the%20set%20of%20features%20to%0Acondition%20the%20diffusion%20model.%20In%20the%20experiments%20on%20CAPE%20and%20MultiHuman%0Adatasets%2C%20the%20proposed%20method%20outperforms%20various%20SOTA%20methods%20based%20on%20SMPL%2C%0Aimplicit%20functions%2C%20point%20cloud%20diffusion%2C%20and%20their%20combined%2C%20under%20synthetic%0Aand%20real%20occlusions.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//donghwankim0101.github.io/projects/mhcdiff/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18364v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-hypotheses%2520Conditioned%2520Point%2520Cloud%2520Diffusion%2520for%25203D%2520Human%250A%2520%2520Reconstruction%2520from%2520Occluded%2520Images%26entry.906535625%3DDonghwan%2520Kim%2520and%2520Tae-Kyun%2520Kim%26entry.1292438233%3D%2520%25203D%2520human%2520shape%2520reconstruction%2520under%2520severe%2520occlusion%2520due%2520to%2520human-object%2520or%250Ahuman-human%2520interaction%2520is%2520a%2520challenging%2520problem.%2520Parametric%2520models%2520i.e.%252C%250ASMPL%2528-X%2529%252C%2520which%2520are%2520based%2520on%2520the%2520statistics%2520across%2520human%2520shapes%252C%2520can%2520represent%250Awhole%2520human%2520body%2520shapes%2520but%2520are%2520limited%2520to%2520minimally-clothed%2520human%2520shapes.%250AImplicit-function-based%2520methods%2520extract%2520features%2520from%2520the%2520parametric%2520models%2520to%250Aemploy%2520prior%2520knowledge%2520of%2520human%2520bodies%2520and%2520can%2520capture%2520geometric%2520details%2520such%250Aas%2520clothing%2520and%2520hair.%2520However%252C%2520they%2520often%2520struggle%2520to%2520handle%2520misaligned%250Aparametric%2520models%2520and%2520inpaint%2520occluded%2520regions%2520given%2520a%2520single%2520RGB%2520image.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520pipeline%252C%2520MHCDIFF%252C%2520Multi-hypotheses%2520Conditioned%250APoint%2520Cloud%2520Diffusion%252C%2520composed%2520of%2520point%2520cloud%2520diffusion%2520conditioned%2520on%250Aprobabilistic%2520distributions%2520for%2520pixel-aligned%2520detailed%25203D%2520human%2520reconstruction%250Aunder%2520occlusion.%2520Compared%2520to%2520previous%2520implicit-function-based%2520methods%252C%2520the%250Apoint%2520cloud%2520diffusion%2520model%2520can%2520capture%2520the%2520global%2520consistent%2520features%2520to%250Agenerate%2520the%2520occluded%2520regions%252C%2520and%2520the%2520denoising%2520process%2520corrects%2520the%250Amisaligned%2520SMPL%2520meshes.%2520The%2520core%2520of%2520MHCDIFF%2520is%2520extracting%2520local%2520features%2520from%250Amultiple%2520hypothesized%2520SMPL%2528-X%2529%2520meshes%2520and%2520aggregating%2520the%2520set%2520of%2520features%2520to%250Acondition%2520the%2520diffusion%2520model.%2520In%2520the%2520experiments%2520on%2520CAPE%2520and%2520MultiHuman%250Adatasets%252C%2520the%2520proposed%2520method%2520outperforms%2520various%2520SOTA%2520methods%2520based%2520on%2520SMPL%252C%250Aimplicit%2520functions%252C%2520point%2520cloud%2520diffusion%252C%2520and%2520their%2520combined%252C%2520under%2520synthetic%250Aand%2520real%2520occlusions.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//donghwankim0101.github.io/projects/mhcdiff/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18364v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-hypotheses%20Conditioned%20Point%20Cloud%20Diffusion%20for%203D%20Human%0A%20%20Reconstruction%20from%20Occluded%20Images&entry.906535625=Donghwan%20Kim%20and%20Tae-Kyun%20Kim&entry.1292438233=%20%203D%20human%20shape%20reconstruction%20under%20severe%20occlusion%20due%20to%20human-object%20or%0Ahuman-human%20interaction%20is%20a%20challenging%20problem.%20Parametric%20models%20i.e.%2C%0ASMPL%28-X%29%2C%20which%20are%20based%20on%20the%20statistics%20across%20human%20shapes%2C%20can%20represent%0Awhole%20human%20body%20shapes%20but%20are%20limited%20to%20minimally-clothed%20human%20shapes.%0AImplicit-function-based%20methods%20extract%20features%20from%20the%20parametric%20models%20to%0Aemploy%20prior%20knowledge%20of%20human%20bodies%20and%20can%20capture%20geometric%20details%20such%0Aas%20clothing%20and%20hair.%20However%2C%20they%20often%20struggle%20to%20handle%20misaligned%0Aparametric%20models%20and%20inpaint%20occluded%20regions%20given%20a%20single%20RGB%20image.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20pipeline%2C%20MHCDIFF%2C%20Multi-hypotheses%20Conditioned%0APoint%20Cloud%20Diffusion%2C%20composed%20of%20point%20cloud%20diffusion%20conditioned%20on%0Aprobabilistic%20distributions%20for%20pixel-aligned%20detailed%203D%20human%20reconstruction%0Aunder%20occlusion.%20Compared%20to%20previous%20implicit-function-based%20methods%2C%20the%0Apoint%20cloud%20diffusion%20model%20can%20capture%20the%20global%20consistent%20features%20to%0Agenerate%20the%20occluded%20regions%2C%20and%20the%20denoising%20process%20corrects%20the%0Amisaligned%20SMPL%20meshes.%20The%20core%20of%20MHCDIFF%20is%20extracting%20local%20features%20from%0Amultiple%20hypothesized%20SMPL%28-X%29%20meshes%20and%20aggregating%20the%20set%20of%20features%20to%0Acondition%20the%20diffusion%20model.%20In%20the%20experiments%20on%20CAPE%20and%20MultiHuman%0Adatasets%2C%20the%20proposed%20method%20outperforms%20various%20SOTA%20methods%20based%20on%20SMPL%2C%0Aimplicit%20functions%2C%20point%20cloud%20diffusion%2C%20and%20their%20combined%2C%20under%20synthetic%0Aand%20real%20occlusions.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//donghwankim0101.github.io/projects/mhcdiff/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18364v3&entry.124074799=Read"},
{"title": "Multi-Object 3D Grounding with Dynamic Modules and Language-Informed\n  Spatial Attention", "author": "Haomeng Zhang and Chiao-An Yang and Raymond A. Yeh", "abstract": "  Multi-object 3D Grounding involves locating 3D boxes based on a given query\nphrase from a point cloud. It is a challenging and significant task with\nnumerous applications in visual understanding, human-computer interaction, and\nrobotics. To tackle this challenge, we introduce D-LISA, a two-stage approach\nincorporating three innovations. First, a dynamic vision module that enables a\nvariable and learnable number of box proposals. Second, a dynamic camera\npositioning that extracts features for each proposal. Third, a\nlanguage-informed spatial attention module that better reasons over the\nproposals to output the final prediction. Empirically, experiments show that\nour method outperforms the state-of-the-art methods on multi-object 3D\ngrounding by 12.8% (absolute) and is competitive in single-object 3D grounding.\n", "link": "http://arxiv.org/abs/2410.22306v1", "date": "2024-10-29", "relevancy": 3.0582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6152}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Object%203D%20Grounding%20with%20Dynamic%20Modules%20and%20Language-Informed%0A%20%20Spatial%20Attention&body=Title%3A%20Multi-Object%203D%20Grounding%20with%20Dynamic%20Modules%20and%20Language-Informed%0A%20%20Spatial%20Attention%0AAuthor%3A%20Haomeng%20Zhang%20and%20Chiao-An%20Yang%20and%20Raymond%20A.%20Yeh%0AAbstract%3A%20%20%20Multi-object%203D%20Grounding%20involves%20locating%203D%20boxes%20based%20on%20a%20given%20query%0Aphrase%20from%20a%20point%20cloud.%20It%20is%20a%20challenging%20and%20significant%20task%20with%0Anumerous%20applications%20in%20visual%20understanding%2C%20human-computer%20interaction%2C%20and%0Arobotics.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20D-LISA%2C%20a%20two-stage%20approach%0Aincorporating%20three%20innovations.%20First%2C%20a%20dynamic%20vision%20module%20that%20enables%20a%0Avariable%20and%20learnable%20number%20of%20box%20proposals.%20Second%2C%20a%20dynamic%20camera%0Apositioning%20that%20extracts%20features%20for%20each%20proposal.%20Third%2C%20a%0Alanguage-informed%20spatial%20attention%20module%20that%20better%20reasons%20over%20the%0Aproposals%20to%20output%20the%20final%20prediction.%20Empirically%2C%20experiments%20show%20that%0Aour%20method%20outperforms%20the%20state-of-the-art%20methods%20on%20multi-object%203D%0Agrounding%20by%2012.8%25%20%28absolute%29%20and%20is%20competitive%20in%20single-object%203D%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Object%25203D%2520Grounding%2520with%2520Dynamic%2520Modules%2520and%2520Language-Informed%250A%2520%2520Spatial%2520Attention%26entry.906535625%3DHaomeng%2520Zhang%2520and%2520Chiao-An%2520Yang%2520and%2520Raymond%2520A.%2520Yeh%26entry.1292438233%3D%2520%2520Multi-object%25203D%2520Grounding%2520involves%2520locating%25203D%2520boxes%2520based%2520on%2520a%2520given%2520query%250Aphrase%2520from%2520a%2520point%2520cloud.%2520It%2520is%2520a%2520challenging%2520and%2520significant%2520task%2520with%250Anumerous%2520applications%2520in%2520visual%2520understanding%252C%2520human-computer%2520interaction%252C%2520and%250Arobotics.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520introduce%2520D-LISA%252C%2520a%2520two-stage%2520approach%250Aincorporating%2520three%2520innovations.%2520First%252C%2520a%2520dynamic%2520vision%2520module%2520that%2520enables%2520a%250Avariable%2520and%2520learnable%2520number%2520of%2520box%2520proposals.%2520Second%252C%2520a%2520dynamic%2520camera%250Apositioning%2520that%2520extracts%2520features%2520for%2520each%2520proposal.%2520Third%252C%2520a%250Alanguage-informed%2520spatial%2520attention%2520module%2520that%2520better%2520reasons%2520over%2520the%250Aproposals%2520to%2520output%2520the%2520final%2520prediction.%2520Empirically%252C%2520experiments%2520show%2520that%250Aour%2520method%2520outperforms%2520the%2520state-of-the-art%2520methods%2520on%2520multi-object%25203D%250Agrounding%2520by%252012.8%2525%2520%2528absolute%2529%2520and%2520is%2520competitive%2520in%2520single-object%25203D%2520grounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Object%203D%20Grounding%20with%20Dynamic%20Modules%20and%20Language-Informed%0A%20%20Spatial%20Attention&entry.906535625=Haomeng%20Zhang%20and%20Chiao-An%20Yang%20and%20Raymond%20A.%20Yeh&entry.1292438233=%20%20Multi-object%203D%20Grounding%20involves%20locating%203D%20boxes%20based%20on%20a%20given%20query%0Aphrase%20from%20a%20point%20cloud.%20It%20is%20a%20challenging%20and%20significant%20task%20with%0Anumerous%20applications%20in%20visual%20understanding%2C%20human-computer%20interaction%2C%20and%0Arobotics.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20D-LISA%2C%20a%20two-stage%20approach%0Aincorporating%20three%20innovations.%20First%2C%20a%20dynamic%20vision%20module%20that%20enables%20a%0Avariable%20and%20learnable%20number%20of%20box%20proposals.%20Second%2C%20a%20dynamic%20camera%0Apositioning%20that%20extracts%20features%20for%20each%20proposal.%20Third%2C%20a%0Alanguage-informed%20spatial%20attention%20module%20that%20better%20reasons%20over%20the%0Aproposals%20to%20output%20the%20final%20prediction.%20Empirically%2C%20experiments%20show%20that%0Aour%20method%20outperforms%20the%20state-of-the-art%20methods%20on%20multi-object%203D%0Agrounding%20by%2012.8%25%20%28absolute%29%20and%20is%20competitive%20in%20single-object%203D%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22306v1&entry.124074799=Read"},
{"title": "Are VLMs Really Blind", "author": "Ayush Singh and Mansi Gupta and Shivank Garg", "abstract": "  Vision Language Models excel in handling a wide range of complex tasks,\nincluding Optical Character Recognition (OCR), Visual Question Answering (VQA),\nand advanced geometric reasoning. However, these models fail to perform well on\nlow-level basic visual tasks which are especially easy for humans. Our goal in\nthis work was to determine if these models are truly \"blind\" to geometric\nreasoning or if there are ways to enhance their capabilities in this area. Our\nwork presents a novel automatic pipeline designed to extract key information\nfrom images in response to specific questions. Instead of just relying on\ndirect VQA, we use question-derived keywords to create a caption that\nhighlights important details in the image related to the question. This caption\nis then used by a language model to provide a precise answer to the question\nwithout requiring external fine-tuning.\n", "link": "http://arxiv.org/abs/2410.22029v1", "date": "2024-10-29", "relevancy": 2.9747, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6245}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20VLMs%20Really%20Blind&body=Title%3A%20Are%20VLMs%20Really%20Blind%0AAuthor%3A%20Ayush%20Singh%20and%20Mansi%20Gupta%20and%20Shivank%20Garg%0AAbstract%3A%20%20%20Vision%20Language%20Models%20excel%20in%20handling%20a%20wide%20range%20of%20complex%20tasks%2C%0Aincluding%20Optical%20Character%20Recognition%20%28OCR%29%2C%20Visual%20Question%20Answering%20%28VQA%29%2C%0Aand%20advanced%20geometric%20reasoning.%20However%2C%20these%20models%20fail%20to%20perform%20well%20on%0Alow-level%20basic%20visual%20tasks%20which%20are%20especially%20easy%20for%20humans.%20Our%20goal%20in%0Athis%20work%20was%20to%20determine%20if%20these%20models%20are%20truly%20%22blind%22%20to%20geometric%0Areasoning%20or%20if%20there%20are%20ways%20to%20enhance%20their%20capabilities%20in%20this%20area.%20Our%0Awork%20presents%20a%20novel%20automatic%20pipeline%20designed%20to%20extract%20key%20information%0Afrom%20images%20in%20response%20to%20specific%20questions.%20Instead%20of%20just%20relying%20on%0Adirect%20VQA%2C%20we%20use%20question-derived%20keywords%20to%20create%20a%20caption%20that%0Ahighlights%20important%20details%20in%20the%20image%20related%20to%20the%20question.%20This%20caption%0Ais%20then%20used%20by%20a%20language%20model%20to%20provide%20a%20precise%20answer%20to%20the%20question%0Awithout%20requiring%20external%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520VLMs%2520Really%2520Blind%26entry.906535625%3DAyush%2520Singh%2520and%2520Mansi%2520Gupta%2520and%2520Shivank%2520Garg%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520excel%2520in%2520handling%2520a%2520wide%2520range%2520of%2520complex%2520tasks%252C%250Aincluding%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%250Aand%2520advanced%2520geometric%2520reasoning.%2520However%252C%2520these%2520models%2520fail%2520to%2520perform%2520well%2520on%250Alow-level%2520basic%2520visual%2520tasks%2520which%2520are%2520especially%2520easy%2520for%2520humans.%2520Our%2520goal%2520in%250Athis%2520work%2520was%2520to%2520determine%2520if%2520these%2520models%2520are%2520truly%2520%2522blind%2522%2520to%2520geometric%250Areasoning%2520or%2520if%2520there%2520are%2520ways%2520to%2520enhance%2520their%2520capabilities%2520in%2520this%2520area.%2520Our%250Awork%2520presents%2520a%2520novel%2520automatic%2520pipeline%2520designed%2520to%2520extract%2520key%2520information%250Afrom%2520images%2520in%2520response%2520to%2520specific%2520questions.%2520Instead%2520of%2520just%2520relying%2520on%250Adirect%2520VQA%252C%2520we%2520use%2520question-derived%2520keywords%2520to%2520create%2520a%2520caption%2520that%250Ahighlights%2520important%2520details%2520in%2520the%2520image%2520related%2520to%2520the%2520question.%2520This%2520caption%250Ais%2520then%2520used%2520by%2520a%2520language%2520model%2520to%2520provide%2520a%2520precise%2520answer%2520to%2520the%2520question%250Awithout%2520requiring%2520external%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20VLMs%20Really%20Blind&entry.906535625=Ayush%20Singh%20and%20Mansi%20Gupta%20and%20Shivank%20Garg&entry.1292438233=%20%20Vision%20Language%20Models%20excel%20in%20handling%20a%20wide%20range%20of%20complex%20tasks%2C%0Aincluding%20Optical%20Character%20Recognition%20%28OCR%29%2C%20Visual%20Question%20Answering%20%28VQA%29%2C%0Aand%20advanced%20geometric%20reasoning.%20However%2C%20these%20models%20fail%20to%20perform%20well%20on%0Alow-level%20basic%20visual%20tasks%20which%20are%20especially%20easy%20for%20humans.%20Our%20goal%20in%0Athis%20work%20was%20to%20determine%20if%20these%20models%20are%20truly%20%22blind%22%20to%20geometric%0Areasoning%20or%20if%20there%20are%20ways%20to%20enhance%20their%20capabilities%20in%20this%20area.%20Our%0Awork%20presents%20a%20novel%20automatic%20pipeline%20designed%20to%20extract%20key%20information%0Afrom%20images%20in%20response%20to%20specific%20questions.%20Instead%20of%20just%20relying%20on%0Adirect%20VQA%2C%20we%20use%20question-derived%20keywords%20to%20create%20a%20caption%20that%0Ahighlights%20important%20details%20in%20the%20image%20related%20to%20the%20question.%20This%20caption%0Ais%20then%20used%20by%20a%20language%20model%20to%20provide%20a%20precise%20answer%20to%20the%20question%0Awithout%20requiring%20external%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22029v1&entry.124074799=Read"},
{"title": "Guide3D: A Bi-planar X-ray Dataset for 3D Shape Reconstruction", "author": "Tudor Jianu and Baoru Huang and Hoan Nguyen and Binod Bhattarai and Tuong Do and Erman Tjiputra and Quang Tran and Pierre Berthet-Rayne and Ngan Le and Sebastiano Fichera and Anh Nguyen", "abstract": "  Endovascular surgical tool reconstruction represents an important factor in\nadvancing endovascular tool navigation, which is an important step in\nendovascular surgery. However, the lack of publicly available datasets\nsignificantly restricts the development and validation of novel machine\nlearning approaches. Moreover, due to the need for specialized equipment such\nas biplanar scanners, most of the previous research employs monoplanar\nfluoroscopic technologies, hence only capturing the data from a single view and\nsignificantly limiting the reconstruction accuracy. To bridge this gap, we\nintroduce Guide3D, a bi-planar X-ray dataset for 3D reconstruction. The dataset\nrepresents a collection of high resolution bi-planar, manually annotated\nfluoroscopic videos, captured in real-world settings. Validating our dataset\nwithin a simulated environment reflective of clinical settings confirms its\napplicability for real-world applications. Furthermore, we propose a new\nbenchmark for guidewrite shape prediction, serving as a strong baseline for\nfuture work. Guide3D not only addresses an essential need by offering a\nplatform for advancing segmentation and 3D reconstruction techniques but also\naids the development of more accurate and efficient endovascular surgery\ninterventions. Our project is available at https://airvlab.github.io/guide3d/.\n", "link": "http://arxiv.org/abs/2410.22224v1", "date": "2024-10-29", "relevancy": 2.8883, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5946}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guide3D%3A%20A%20Bi-planar%20X-ray%20Dataset%20for%203D%20Shape%20Reconstruction&body=Title%3A%20Guide3D%3A%20A%20Bi-planar%20X-ray%20Dataset%20for%203D%20Shape%20Reconstruction%0AAuthor%3A%20Tudor%20Jianu%20and%20Baoru%20Huang%20and%20Hoan%20Nguyen%20and%20Binod%20Bhattarai%20and%20Tuong%20Do%20and%20Erman%20Tjiputra%20and%20Quang%20Tran%20and%20Pierre%20Berthet-Rayne%20and%20Ngan%20Le%20and%20Sebastiano%20Fichera%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20Endovascular%20surgical%20tool%20reconstruction%20represents%20an%20important%20factor%20in%0Aadvancing%20endovascular%20tool%20navigation%2C%20which%20is%20an%20important%20step%20in%0Aendovascular%20surgery.%20However%2C%20the%20lack%20of%20publicly%20available%20datasets%0Asignificantly%20restricts%20the%20development%20and%20validation%20of%20novel%20machine%0Alearning%20approaches.%20Moreover%2C%20due%20to%20the%20need%20for%20specialized%20equipment%20such%0Aas%20biplanar%20scanners%2C%20most%20of%20the%20previous%20research%20employs%20monoplanar%0Afluoroscopic%20technologies%2C%20hence%20only%20capturing%20the%20data%20from%20a%20single%20view%20and%0Asignificantly%20limiting%20the%20reconstruction%20accuracy.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20Guide3D%2C%20a%20bi-planar%20X-ray%20dataset%20for%203D%20reconstruction.%20The%20dataset%0Arepresents%20a%20collection%20of%20high%20resolution%20bi-planar%2C%20manually%20annotated%0Afluoroscopic%20videos%2C%20captured%20in%20real-world%20settings.%20Validating%20our%20dataset%0Awithin%20a%20simulated%20environment%20reflective%20of%20clinical%20settings%20confirms%20its%0Aapplicability%20for%20real-world%20applications.%20Furthermore%2C%20we%20propose%20a%20new%0Abenchmark%20for%20guidewrite%20shape%20prediction%2C%20serving%20as%20a%20strong%20baseline%20for%0Afuture%20work.%20Guide3D%20not%20only%20addresses%20an%20essential%20need%20by%20offering%20a%0Aplatform%20for%20advancing%20segmentation%20and%203D%20reconstruction%20techniques%20but%20also%0Aaids%20the%20development%20of%20more%20accurate%20and%20efficient%20endovascular%20surgery%0Ainterventions.%20Our%20project%20is%20available%20at%20https%3A//airvlab.github.io/guide3d/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuide3D%253A%2520A%2520Bi-planar%2520X-ray%2520Dataset%2520for%25203D%2520Shape%2520Reconstruction%26entry.906535625%3DTudor%2520Jianu%2520and%2520Baoru%2520Huang%2520and%2520Hoan%2520Nguyen%2520and%2520Binod%2520Bhattarai%2520and%2520Tuong%2520Do%2520and%2520Erman%2520Tjiputra%2520and%2520Quang%2520Tran%2520and%2520Pierre%2520Berthet-Rayne%2520and%2520Ngan%2520Le%2520and%2520Sebastiano%2520Fichera%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Endovascular%2520surgical%2520tool%2520reconstruction%2520represents%2520an%2520important%2520factor%2520in%250Aadvancing%2520endovascular%2520tool%2520navigation%252C%2520which%2520is%2520an%2520important%2520step%2520in%250Aendovascular%2520surgery.%2520However%252C%2520the%2520lack%2520of%2520publicly%2520available%2520datasets%250Asignificantly%2520restricts%2520the%2520development%2520and%2520validation%2520of%2520novel%2520machine%250Alearning%2520approaches.%2520Moreover%252C%2520due%2520to%2520the%2520need%2520for%2520specialized%2520equipment%2520such%250Aas%2520biplanar%2520scanners%252C%2520most%2520of%2520the%2520previous%2520research%2520employs%2520monoplanar%250Afluoroscopic%2520technologies%252C%2520hence%2520only%2520capturing%2520the%2520data%2520from%2520a%2520single%2520view%2520and%250Asignificantly%2520limiting%2520the%2520reconstruction%2520accuracy.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520Guide3D%252C%2520a%2520bi-planar%2520X-ray%2520dataset%2520for%25203D%2520reconstruction.%2520The%2520dataset%250Arepresents%2520a%2520collection%2520of%2520high%2520resolution%2520bi-planar%252C%2520manually%2520annotated%250Afluoroscopic%2520videos%252C%2520captured%2520in%2520real-world%2520settings.%2520Validating%2520our%2520dataset%250Awithin%2520a%2520simulated%2520environment%2520reflective%2520of%2520clinical%2520settings%2520confirms%2520its%250Aapplicability%2520for%2520real-world%2520applications.%2520Furthermore%252C%2520we%2520propose%2520a%2520new%250Abenchmark%2520for%2520guidewrite%2520shape%2520prediction%252C%2520serving%2520as%2520a%2520strong%2520baseline%2520for%250Afuture%2520work.%2520Guide3D%2520not%2520only%2520addresses%2520an%2520essential%2520need%2520by%2520offering%2520a%250Aplatform%2520for%2520advancing%2520segmentation%2520and%25203D%2520reconstruction%2520techniques%2520but%2520also%250Aaids%2520the%2520development%2520of%2520more%2520accurate%2520and%2520efficient%2520endovascular%2520surgery%250Ainterventions.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//airvlab.github.io/guide3d/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guide3D%3A%20A%20Bi-planar%20X-ray%20Dataset%20for%203D%20Shape%20Reconstruction&entry.906535625=Tudor%20Jianu%20and%20Baoru%20Huang%20and%20Hoan%20Nguyen%20and%20Binod%20Bhattarai%20and%20Tuong%20Do%20and%20Erman%20Tjiputra%20and%20Quang%20Tran%20and%20Pierre%20Berthet-Rayne%20and%20Ngan%20Le%20and%20Sebastiano%20Fichera%20and%20Anh%20Nguyen&entry.1292438233=%20%20Endovascular%20surgical%20tool%20reconstruction%20represents%20an%20important%20factor%20in%0Aadvancing%20endovascular%20tool%20navigation%2C%20which%20is%20an%20important%20step%20in%0Aendovascular%20surgery.%20However%2C%20the%20lack%20of%20publicly%20available%20datasets%0Asignificantly%20restricts%20the%20development%20and%20validation%20of%20novel%20machine%0Alearning%20approaches.%20Moreover%2C%20due%20to%20the%20need%20for%20specialized%20equipment%20such%0Aas%20biplanar%20scanners%2C%20most%20of%20the%20previous%20research%20employs%20monoplanar%0Afluoroscopic%20technologies%2C%20hence%20only%20capturing%20the%20data%20from%20a%20single%20view%20and%0Asignificantly%20limiting%20the%20reconstruction%20accuracy.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20Guide3D%2C%20a%20bi-planar%20X-ray%20dataset%20for%203D%20reconstruction.%20The%20dataset%0Arepresents%20a%20collection%20of%20high%20resolution%20bi-planar%2C%20manually%20annotated%0Afluoroscopic%20videos%2C%20captured%20in%20real-world%20settings.%20Validating%20our%20dataset%0Awithin%20a%20simulated%20environment%20reflective%20of%20clinical%20settings%20confirms%20its%0Aapplicability%20for%20real-world%20applications.%20Furthermore%2C%20we%20propose%20a%20new%0Abenchmark%20for%20guidewrite%20shape%20prediction%2C%20serving%20as%20a%20strong%20baseline%20for%0Afuture%20work.%20Guide3D%20not%20only%20addresses%20an%20essential%20need%20by%20offering%20a%0Aplatform%20for%20advancing%20segmentation%20and%203D%20reconstruction%20techniques%20but%20also%0Aaids%20the%20development%20of%20more%20accurate%20and%20efficient%20endovascular%20surgery%0Ainterventions.%20Our%20project%20is%20available%20at%20https%3A//airvlab.github.io/guide3d/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22224v1&entry.124074799=Read"},
{"title": "Leveraging Reverberation and Visual Depth Cues for Sound Event\n  Localization and Detection with Distance Estimation", "author": "Davide Berghi and Philip J. B. Jackson", "abstract": "  This report describes our systems submitted for the DCASE2024 Task 3\nchallenge: Audio and Audiovisual Sound Event Localization and Detection with\nSource Distance Estimation (Track B). Our main model is based on the\naudio-visual (AV) Conformer, which processes video and audio embeddings\nextracted with ResNet50 and with an audio encoder pre-trained on SELD,\nrespectively. This model outperformed the audio-visual baseline of the\ndevelopment set of the STARSS23 dataset by a wide margin, halving its DOAE and\nimproving the F1 by more than 3x. Our second system performs a temporal\nensemble from the outputs of the AV-Conformer. We then extended the model with\nfeatures for distance estimation, such as direct and reverberant signal\ncomponents extracted from the omnidirectional audio channel, and depth maps\nextracted from the video frames. While the new system improved the RDE of our\nprevious model by about 3 percentage points, it achieved a lower F1 score. This\nmay be caused by sound classes that rarely appear in the training set and that\nthe more complex system does not detect, as analysis can determine. To overcome\nthis problem, our fourth and final system consists of an ensemble strategy\ncombining the predictions of the other three. Many opportunities to refine the\nsystem and training strategy can be tested in future ablation experiments, and\nlikely achieve incremental performance gains for this audio-visual task.\n", "link": "http://arxiv.org/abs/2410.22271v1", "date": "2024-10-29", "relevancy": 2.8824, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Reverberation%20and%20Visual%20Depth%20Cues%20for%20Sound%20Event%0A%20%20Localization%20and%20Detection%20with%20Distance%20Estimation&body=Title%3A%20Leveraging%20Reverberation%20and%20Visual%20Depth%20Cues%20for%20Sound%20Event%0A%20%20Localization%20and%20Detection%20with%20Distance%20Estimation%0AAuthor%3A%20Davide%20Berghi%20and%20Philip%20J.%20B.%20Jackson%0AAbstract%3A%20%20%20This%20report%20describes%20our%20systems%20submitted%20for%20the%20DCASE2024%20Task%203%0Achallenge%3A%20Audio%20and%20Audiovisual%20Sound%20Event%20Localization%20and%20Detection%20with%0ASource%20Distance%20Estimation%20%28Track%20B%29.%20Our%20main%20model%20is%20based%20on%20the%0Aaudio-visual%20%28AV%29%20Conformer%2C%20which%20processes%20video%20and%20audio%20embeddings%0Aextracted%20with%20ResNet50%20and%20with%20an%20audio%20encoder%20pre-trained%20on%20SELD%2C%0Arespectively.%20This%20model%20outperformed%20the%20audio-visual%20baseline%20of%20the%0Adevelopment%20set%20of%20the%20STARSS23%20dataset%20by%20a%20wide%20margin%2C%20halving%20its%20DOAE%20and%0Aimproving%20the%20F1%20by%20more%20than%203x.%20Our%20second%20system%20performs%20a%20temporal%0Aensemble%20from%20the%20outputs%20of%20the%20AV-Conformer.%20We%20then%20extended%20the%20model%20with%0Afeatures%20for%20distance%20estimation%2C%20such%20as%20direct%20and%20reverberant%20signal%0Acomponents%20extracted%20from%20the%20omnidirectional%20audio%20channel%2C%20and%20depth%20maps%0Aextracted%20from%20the%20video%20frames.%20While%20the%20new%20system%20improved%20the%20RDE%20of%20our%0Aprevious%20model%20by%20about%203%20percentage%20points%2C%20it%20achieved%20a%20lower%20F1%20score.%20This%0Amay%20be%20caused%20by%20sound%20classes%20that%20rarely%20appear%20in%20the%20training%20set%20and%20that%0Athe%20more%20complex%20system%20does%20not%20detect%2C%20as%20analysis%20can%20determine.%20To%20overcome%0Athis%20problem%2C%20our%20fourth%20and%20final%20system%20consists%20of%20an%20ensemble%20strategy%0Acombining%20the%20predictions%20of%20the%20other%20three.%20Many%20opportunities%20to%20refine%20the%0Asystem%20and%20training%20strategy%20can%20be%20tested%20in%20future%20ablation%20experiments%2C%20and%0Alikely%20achieve%20incremental%20performance%20gains%20for%20this%20audio-visual%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Reverberation%2520and%2520Visual%2520Depth%2520Cues%2520for%2520Sound%2520Event%250A%2520%2520Localization%2520and%2520Detection%2520with%2520Distance%2520Estimation%26entry.906535625%3DDavide%2520Berghi%2520and%2520Philip%2520J.%2520B.%2520Jackson%26entry.1292438233%3D%2520%2520This%2520report%2520describes%2520our%2520systems%2520submitted%2520for%2520the%2520DCASE2024%2520Task%25203%250Achallenge%253A%2520Audio%2520and%2520Audiovisual%2520Sound%2520Event%2520Localization%2520and%2520Detection%2520with%250ASource%2520Distance%2520Estimation%2520%2528Track%2520B%2529.%2520Our%2520main%2520model%2520is%2520based%2520on%2520the%250Aaudio-visual%2520%2528AV%2529%2520Conformer%252C%2520which%2520processes%2520video%2520and%2520audio%2520embeddings%250Aextracted%2520with%2520ResNet50%2520and%2520with%2520an%2520audio%2520encoder%2520pre-trained%2520on%2520SELD%252C%250Arespectively.%2520This%2520model%2520outperformed%2520the%2520audio-visual%2520baseline%2520of%2520the%250Adevelopment%2520set%2520of%2520the%2520STARSS23%2520dataset%2520by%2520a%2520wide%2520margin%252C%2520halving%2520its%2520DOAE%2520and%250Aimproving%2520the%2520F1%2520by%2520more%2520than%25203x.%2520Our%2520second%2520system%2520performs%2520a%2520temporal%250Aensemble%2520from%2520the%2520outputs%2520of%2520the%2520AV-Conformer.%2520We%2520then%2520extended%2520the%2520model%2520with%250Afeatures%2520for%2520distance%2520estimation%252C%2520such%2520as%2520direct%2520and%2520reverberant%2520signal%250Acomponents%2520extracted%2520from%2520the%2520omnidirectional%2520audio%2520channel%252C%2520and%2520depth%2520maps%250Aextracted%2520from%2520the%2520video%2520frames.%2520While%2520the%2520new%2520system%2520improved%2520the%2520RDE%2520of%2520our%250Aprevious%2520model%2520by%2520about%25203%2520percentage%2520points%252C%2520it%2520achieved%2520a%2520lower%2520F1%2520score.%2520This%250Amay%2520be%2520caused%2520by%2520sound%2520classes%2520that%2520rarely%2520appear%2520in%2520the%2520training%2520set%2520and%2520that%250Athe%2520more%2520complex%2520system%2520does%2520not%2520detect%252C%2520as%2520analysis%2520can%2520determine.%2520To%2520overcome%250Athis%2520problem%252C%2520our%2520fourth%2520and%2520final%2520system%2520consists%2520of%2520an%2520ensemble%2520strategy%250Acombining%2520the%2520predictions%2520of%2520the%2520other%2520three.%2520Many%2520opportunities%2520to%2520refine%2520the%250Asystem%2520and%2520training%2520strategy%2520can%2520be%2520tested%2520in%2520future%2520ablation%2520experiments%252C%2520and%250Alikely%2520achieve%2520incremental%2520performance%2520gains%2520for%2520this%2520audio-visual%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Reverberation%20and%20Visual%20Depth%20Cues%20for%20Sound%20Event%0A%20%20Localization%20and%20Detection%20with%20Distance%20Estimation&entry.906535625=Davide%20Berghi%20and%20Philip%20J.%20B.%20Jackson&entry.1292438233=%20%20This%20report%20describes%20our%20systems%20submitted%20for%20the%20DCASE2024%20Task%203%0Achallenge%3A%20Audio%20and%20Audiovisual%20Sound%20Event%20Localization%20and%20Detection%20with%0ASource%20Distance%20Estimation%20%28Track%20B%29.%20Our%20main%20model%20is%20based%20on%20the%0Aaudio-visual%20%28AV%29%20Conformer%2C%20which%20processes%20video%20and%20audio%20embeddings%0Aextracted%20with%20ResNet50%20and%20with%20an%20audio%20encoder%20pre-trained%20on%20SELD%2C%0Arespectively.%20This%20model%20outperformed%20the%20audio-visual%20baseline%20of%20the%0Adevelopment%20set%20of%20the%20STARSS23%20dataset%20by%20a%20wide%20margin%2C%20halving%20its%20DOAE%20and%0Aimproving%20the%20F1%20by%20more%20than%203x.%20Our%20second%20system%20performs%20a%20temporal%0Aensemble%20from%20the%20outputs%20of%20the%20AV-Conformer.%20We%20then%20extended%20the%20model%20with%0Afeatures%20for%20distance%20estimation%2C%20such%20as%20direct%20and%20reverberant%20signal%0Acomponents%20extracted%20from%20the%20omnidirectional%20audio%20channel%2C%20and%20depth%20maps%0Aextracted%20from%20the%20video%20frames.%20While%20the%20new%20system%20improved%20the%20RDE%20of%20our%0Aprevious%20model%20by%20about%203%20percentage%20points%2C%20it%20achieved%20a%20lower%20F1%20score.%20This%0Amay%20be%20caused%20by%20sound%20classes%20that%20rarely%20appear%20in%20the%20training%20set%20and%20that%0Athe%20more%20complex%20system%20does%20not%20detect%2C%20as%20analysis%20can%20determine.%20To%20overcome%0Athis%20problem%2C%20our%20fourth%20and%20final%20system%20consists%20of%20an%20ensemble%20strategy%0Acombining%20the%20predictions%20of%20the%20other%20three.%20Many%20opportunities%20to%20refine%20the%0Asystem%20and%20training%20strategy%20can%20be%20tested%20in%20future%20ablation%20experiments%2C%20and%0Alikely%20achieve%20incremental%20performance%20gains%20for%20this%20audio-visual%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22271v1&entry.124074799=Read"},
{"title": "Task Vectors are Cross-Modal", "author": "Grace Luo and Trevor Darrell and Amir Bar", "abstract": "  We investigate the internal representations of vision-and-language models\n(VLMs) and how they encode task representations. We consider tasks specified\nthrough examples or instructions, using either text or image inputs.\nSurprisingly, we find that conceptually similar tasks are mapped to similar\ntask vector representations, regardless of how they are specified. Our findings\nsuggest that to output answers, tokens in VLMs undergo three distinct phases:\ninput, task, and answer, a process which is consistent across different\nmodalities and specifications. The task vectors we identify in VLMs are general\nenough to be derived in one modality (e.g., text) and transferred to another\n(e.g., image). Additionally, we find that ensembling exemplar and instruction\nbased task vectors produce better task representations. Taken together, these\ninsights shed light on the underlying mechanisms of VLMs, particularly their\nability to represent tasks in a shared manner across different modalities and\ntask specifications. Project page:\nhttps://task-vectors-are-cross-modal.github.io.\n", "link": "http://arxiv.org/abs/2410.22330v1", "date": "2024-10-29", "relevancy": 2.8618, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Vectors%20are%20Cross-Modal&body=Title%3A%20Task%20Vectors%20are%20Cross-Modal%0AAuthor%3A%20Grace%20Luo%20and%20Trevor%20Darrell%20and%20Amir%20Bar%0AAbstract%3A%20%20%20We%20investigate%20the%20internal%20representations%20of%20vision-and-language%20models%0A%28VLMs%29%20and%20how%20they%20encode%20task%20representations.%20We%20consider%20tasks%20specified%0Athrough%20examples%20or%20instructions%2C%20using%20either%20text%20or%20image%20inputs.%0ASurprisingly%2C%20we%20find%20that%20conceptually%20similar%20tasks%20are%20mapped%20to%20similar%0Atask%20vector%20representations%2C%20regardless%20of%20how%20they%20are%20specified.%20Our%20findings%0Asuggest%20that%20to%20output%20answers%2C%20tokens%20in%20VLMs%20undergo%20three%20distinct%20phases%3A%0Ainput%2C%20task%2C%20and%20answer%2C%20a%20process%20which%20is%20consistent%20across%20different%0Amodalities%20and%20specifications.%20The%20task%20vectors%20we%20identify%20in%20VLMs%20are%20general%0Aenough%20to%20be%20derived%20in%20one%20modality%20%28e.g.%2C%20text%29%20and%20transferred%20to%20another%0A%28e.g.%2C%20image%29.%20Additionally%2C%20we%20find%20that%20ensembling%20exemplar%20and%20instruction%0Abased%20task%20vectors%20produce%20better%20task%20representations.%20Taken%20together%2C%20these%0Ainsights%20shed%20light%20on%20the%20underlying%20mechanisms%20of%20VLMs%2C%20particularly%20their%0Aability%20to%20represent%20tasks%20in%20a%20shared%20manner%20across%20different%20modalities%20and%0Atask%20specifications.%20Project%20page%3A%0Ahttps%3A//task-vectors-are-cross-modal.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Vectors%2520are%2520Cross-Modal%26entry.906535625%3DGrace%2520Luo%2520and%2520Trevor%2520Darrell%2520and%2520Amir%2520Bar%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520internal%2520representations%2520of%2520vision-and-language%2520models%250A%2528VLMs%2529%2520and%2520how%2520they%2520encode%2520task%2520representations.%2520We%2520consider%2520tasks%2520specified%250Athrough%2520examples%2520or%2520instructions%252C%2520using%2520either%2520text%2520or%2520image%2520inputs.%250ASurprisingly%252C%2520we%2520find%2520that%2520conceptually%2520similar%2520tasks%2520are%2520mapped%2520to%2520similar%250Atask%2520vector%2520representations%252C%2520regardless%2520of%2520how%2520they%2520are%2520specified.%2520Our%2520findings%250Asuggest%2520that%2520to%2520output%2520answers%252C%2520tokens%2520in%2520VLMs%2520undergo%2520three%2520distinct%2520phases%253A%250Ainput%252C%2520task%252C%2520and%2520answer%252C%2520a%2520process%2520which%2520is%2520consistent%2520across%2520different%250Amodalities%2520and%2520specifications.%2520The%2520task%2520vectors%2520we%2520identify%2520in%2520VLMs%2520are%2520general%250Aenough%2520to%2520be%2520derived%2520in%2520one%2520modality%2520%2528e.g.%252C%2520text%2529%2520and%2520transferred%2520to%2520another%250A%2528e.g.%252C%2520image%2529.%2520Additionally%252C%2520we%2520find%2520that%2520ensembling%2520exemplar%2520and%2520instruction%250Abased%2520task%2520vectors%2520produce%2520better%2520task%2520representations.%2520Taken%2520together%252C%2520these%250Ainsights%2520shed%2520light%2520on%2520the%2520underlying%2520mechanisms%2520of%2520VLMs%252C%2520particularly%2520their%250Aability%2520to%2520represent%2520tasks%2520in%2520a%2520shared%2520manner%2520across%2520different%2520modalities%2520and%250Atask%2520specifications.%2520Project%2520page%253A%250Ahttps%253A//task-vectors-are-cross-modal.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Vectors%20are%20Cross-Modal&entry.906535625=Grace%20Luo%20and%20Trevor%20Darrell%20and%20Amir%20Bar&entry.1292438233=%20%20We%20investigate%20the%20internal%20representations%20of%20vision-and-language%20models%0A%28VLMs%29%20and%20how%20they%20encode%20task%20representations.%20We%20consider%20tasks%20specified%0Athrough%20examples%20or%20instructions%2C%20using%20either%20text%20or%20image%20inputs.%0ASurprisingly%2C%20we%20find%20that%20conceptually%20similar%20tasks%20are%20mapped%20to%20similar%0Atask%20vector%20representations%2C%20regardless%20of%20how%20they%20are%20specified.%20Our%20findings%0Asuggest%20that%20to%20output%20answers%2C%20tokens%20in%20VLMs%20undergo%20three%20distinct%20phases%3A%0Ainput%2C%20task%2C%20and%20answer%2C%20a%20process%20which%20is%20consistent%20across%20different%0Amodalities%20and%20specifications.%20The%20task%20vectors%20we%20identify%20in%20VLMs%20are%20general%0Aenough%20to%20be%20derived%20in%20one%20modality%20%28e.g.%2C%20text%29%20and%20transferred%20to%20another%0A%28e.g.%2C%20image%29.%20Additionally%2C%20we%20find%20that%20ensembling%20exemplar%20and%20instruction%0Abased%20task%20vectors%20produce%20better%20task%20representations.%20Taken%20together%2C%20these%0Ainsights%20shed%20light%20on%20the%20underlying%20mechanisms%20of%20VLMs%2C%20particularly%20their%0Aability%20to%20represent%20tasks%20in%20a%20shared%20manner%20across%20different%20modalities%20and%0Atask%20specifications.%20Project%20page%3A%0Ahttps%3A//task-vectors-are-cross-modal.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22330v1&entry.124074799=Read"},
{"title": "EEG-Driven 3D Object Reconstruction with Color Consistency and Diffusion\n  Prior", "author": "Xin Xiang and Wenhui Zhou and Guojun Dai", "abstract": "  EEG-based visual perception reconstruction has become a current research\nhotspot. Neuroscientific studies have shown that humans can perceive various\ntypes of visual information, such as color, shape, and texture, when observing\nobjects. However, existing technical methods often face issues such as\ninconsistencies in texture, shape, and color between the visual stimulus images\nand the reconstructed images. In this paper, we propose a method for\nreconstructing 3D objects with color consistency based on EEG signals. The\nmethod adopts a two-stage strategy: in the first stage, we train an implicit\nneural EEG encoder with the capability of perceiving 3D objects, enabling it to\ncapture regional semantic features; in the second stage, based on the latent\nEEG codes obtained in the first stage, we integrate a diffusion model, neural\nstyle loss, and NeRF to implicitly decode the 3D objects. Finally, through\nexperimental validation, we demonstrate that our method can reconstruct 3D\nobjects with color consistency using EEG.\n", "link": "http://arxiv.org/abs/2410.20981v2", "date": "2024-10-29", "relevancy": 2.8612, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.575}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-Driven%203D%20Object%20Reconstruction%20with%20Color%20Consistency%20and%20Diffusion%0A%20%20Prior&body=Title%3A%20EEG-Driven%203D%20Object%20Reconstruction%20with%20Color%20Consistency%20and%20Diffusion%0A%20%20Prior%0AAuthor%3A%20Xin%20Xiang%20and%20Wenhui%20Zhou%20and%20Guojun%20Dai%0AAbstract%3A%20%20%20EEG-based%20visual%20perception%20reconstruction%20has%20become%20a%20current%20research%0Ahotspot.%20Neuroscientific%20studies%20have%20shown%20that%20humans%20can%20perceive%20various%0Atypes%20of%20visual%20information%2C%20such%20as%20color%2C%20shape%2C%20and%20texture%2C%20when%20observing%0Aobjects.%20However%2C%20existing%20technical%20methods%20often%20face%20issues%20such%20as%0Ainconsistencies%20in%20texture%2C%20shape%2C%20and%20color%20between%20the%20visual%20stimulus%20images%0Aand%20the%20reconstructed%20images.%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%0Areconstructing%203D%20objects%20with%20color%20consistency%20based%20on%20EEG%20signals.%20The%0Amethod%20adopts%20a%20two-stage%20strategy%3A%20in%20the%20first%20stage%2C%20we%20train%20an%20implicit%0Aneural%20EEG%20encoder%20with%20the%20capability%20of%20perceiving%203D%20objects%2C%20enabling%20it%20to%0Acapture%20regional%20semantic%20features%3B%20in%20the%20second%20stage%2C%20based%20on%20the%20latent%0AEEG%20codes%20obtained%20in%20the%20first%20stage%2C%20we%20integrate%20a%20diffusion%20model%2C%20neural%0Astyle%20loss%2C%20and%20NeRF%20to%20implicitly%20decode%20the%203D%20objects.%20Finally%2C%20through%0Aexperimental%20validation%2C%20we%20demonstrate%20that%20our%20method%20can%20reconstruct%203D%0Aobjects%20with%20color%20consistency%20using%20EEG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-Driven%25203D%2520Object%2520Reconstruction%2520with%2520Color%2520Consistency%2520and%2520Diffusion%250A%2520%2520Prior%26entry.906535625%3DXin%2520Xiang%2520and%2520Wenhui%2520Zhou%2520and%2520Guojun%2520Dai%26entry.1292438233%3D%2520%2520EEG-based%2520visual%2520perception%2520reconstruction%2520has%2520become%2520a%2520current%2520research%250Ahotspot.%2520Neuroscientific%2520studies%2520have%2520shown%2520that%2520humans%2520can%2520perceive%2520various%250Atypes%2520of%2520visual%2520information%252C%2520such%2520as%2520color%252C%2520shape%252C%2520and%2520texture%252C%2520when%2520observing%250Aobjects.%2520However%252C%2520existing%2520technical%2520methods%2520often%2520face%2520issues%2520such%2520as%250Ainconsistencies%2520in%2520texture%252C%2520shape%252C%2520and%2520color%2520between%2520the%2520visual%2520stimulus%2520images%250Aand%2520the%2520reconstructed%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%250Areconstructing%25203D%2520objects%2520with%2520color%2520consistency%2520based%2520on%2520EEG%2520signals.%2520The%250Amethod%2520adopts%2520a%2520two-stage%2520strategy%253A%2520in%2520the%2520first%2520stage%252C%2520we%2520train%2520an%2520implicit%250Aneural%2520EEG%2520encoder%2520with%2520the%2520capability%2520of%2520perceiving%25203D%2520objects%252C%2520enabling%2520it%2520to%250Acapture%2520regional%2520semantic%2520features%253B%2520in%2520the%2520second%2520stage%252C%2520based%2520on%2520the%2520latent%250AEEG%2520codes%2520obtained%2520in%2520the%2520first%2520stage%252C%2520we%2520integrate%2520a%2520diffusion%2520model%252C%2520neural%250Astyle%2520loss%252C%2520and%2520NeRF%2520to%2520implicitly%2520decode%2520the%25203D%2520objects.%2520Finally%252C%2520through%250Aexperimental%2520validation%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520can%2520reconstruct%25203D%250Aobjects%2520with%2520color%2520consistency%2520using%2520EEG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-Driven%203D%20Object%20Reconstruction%20with%20Color%20Consistency%20and%20Diffusion%0A%20%20Prior&entry.906535625=Xin%20Xiang%20and%20Wenhui%20Zhou%20and%20Guojun%20Dai&entry.1292438233=%20%20EEG-based%20visual%20perception%20reconstruction%20has%20become%20a%20current%20research%0Ahotspot.%20Neuroscientific%20studies%20have%20shown%20that%20humans%20can%20perceive%20various%0Atypes%20of%20visual%20information%2C%20such%20as%20color%2C%20shape%2C%20and%20texture%2C%20when%20observing%0Aobjects.%20However%2C%20existing%20technical%20methods%20often%20face%20issues%20such%20as%0Ainconsistencies%20in%20texture%2C%20shape%2C%20and%20color%20between%20the%20visual%20stimulus%20images%0Aand%20the%20reconstructed%20images.%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%0Areconstructing%203D%20objects%20with%20color%20consistency%20based%20on%20EEG%20signals.%20The%0Amethod%20adopts%20a%20two-stage%20strategy%3A%20in%20the%20first%20stage%2C%20we%20train%20an%20implicit%0Aneural%20EEG%20encoder%20with%20the%20capability%20of%20perceiving%203D%20objects%2C%20enabling%20it%20to%0Acapture%20regional%20semantic%20features%3B%20in%20the%20second%20stage%2C%20based%20on%20the%20latent%0AEEG%20codes%20obtained%20in%20the%20first%20stage%2C%20we%20integrate%20a%20diffusion%20model%2C%20neural%0Astyle%20loss%2C%20and%20NeRF%20to%20implicitly%20decode%20the%203D%20objects.%20Finally%2C%20through%0Aexperimental%20validation%2C%20we%20demonstrate%20that%20our%20method%20can%20reconstruct%203D%0Aobjects%20with%20color%20consistency%20using%20EEG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20981v2&entry.124074799=Read"},
{"title": "DINeuro: Distilling Knowledge from 2D Natural Images via Deformable\n  Tubular Transferring Strategy for 3D Neuron Reconstruction", "author": "Yik San Cheng and Runkai Zhao and Heng Wang and Hanchuan Peng and Yui Lo and Yuqian Chen and Lauren J. O'Donnell and Weidong Cai", "abstract": "  Reconstructing neuron morphology from 3D light microscope imaging data is\ncritical to aid neuroscientists in analyzing brain networks and neuroanatomy.\nWith the boost from deep learning techniques, a variety of learning-based\nsegmentation models have been developed to enhance the signal-to-noise ratio of\nraw neuron images as a pre-processing step in the reconstruction workflow.\nHowever, most existing models directly encode the latent representative\nfeatures of volumetric neuron data but neglect their intrinsic morphological\nknowledge. To address this limitation, we design a novel framework that\ndistills the prior knowledge from a 2D Vision Transformer pre-trained on\nextensive 2D natural images to facilitate neuronal morphological learning of\nour 3D Vision Transformer. To bridge the knowledge gap between the 2D natural\nimage and 3D microscopic morphologic domains, we propose a deformable tubular\ntransferring strategy that adapts the pre-trained 2D natural knowledge to the\ninherent tubular characteristics of neuronal structure in the latent embedding\nspace. The experimental results on the Janelia dataset of the BigNeuron project\ndemonstrate that our method achieves a segmentation performance improvement of\n4.53% in mean Dice and 3.56% in mean 95% Hausdorff distance.\n", "link": "http://arxiv.org/abs/2410.22078v1", "date": "2024-10-29", "relevancy": 2.8557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINeuro%3A%20Distilling%20Knowledge%20from%202D%20Natural%20Images%20via%20Deformable%0A%20%20Tubular%20Transferring%20Strategy%20for%203D%20Neuron%20Reconstruction&body=Title%3A%20DINeuro%3A%20Distilling%20Knowledge%20from%202D%20Natural%20Images%20via%20Deformable%0A%20%20Tubular%20Transferring%20Strategy%20for%203D%20Neuron%20Reconstruction%0AAuthor%3A%20Yik%20San%20Cheng%20and%20Runkai%20Zhao%20and%20Heng%20Wang%20and%20Hanchuan%20Peng%20and%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Lauren%20J.%20O%27Donnell%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Reconstructing%20neuron%20morphology%20from%203D%20light%20microscope%20imaging%20data%20is%0Acritical%20to%20aid%20neuroscientists%20in%20analyzing%20brain%20networks%20and%20neuroanatomy.%0AWith%20the%20boost%20from%20deep%20learning%20techniques%2C%20a%20variety%20of%20learning-based%0Asegmentation%20models%20have%20been%20developed%20to%20enhance%20the%20signal-to-noise%20ratio%20of%0Araw%20neuron%20images%20as%20a%20pre-processing%20step%20in%20the%20reconstruction%20workflow.%0AHowever%2C%20most%20existing%20models%20directly%20encode%20the%20latent%20representative%0Afeatures%20of%20volumetric%20neuron%20data%20but%20neglect%20their%20intrinsic%20morphological%0Aknowledge.%20To%20address%20this%20limitation%2C%20we%20design%20a%20novel%20framework%20that%0Adistills%20the%20prior%20knowledge%20from%20a%202D%20Vision%20Transformer%20pre-trained%20on%0Aextensive%202D%20natural%20images%20to%20facilitate%20neuronal%20morphological%20learning%20of%0Aour%203D%20Vision%20Transformer.%20To%20bridge%20the%20knowledge%20gap%20between%20the%202D%20natural%0Aimage%20and%203D%20microscopic%20morphologic%20domains%2C%20we%20propose%20a%20deformable%20tubular%0Atransferring%20strategy%20that%20adapts%20the%20pre-trained%202D%20natural%20knowledge%20to%20the%0Ainherent%20tubular%20characteristics%20of%20neuronal%20structure%20in%20the%20latent%20embedding%0Aspace.%20The%20experimental%20results%20on%20the%20Janelia%20dataset%20of%20the%20BigNeuron%20project%0Ademonstrate%20that%20our%20method%20achieves%20a%20segmentation%20performance%20improvement%20of%0A4.53%25%20in%20mean%20Dice%20and%203.56%25%20in%20mean%2095%25%20Hausdorff%20distance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINeuro%253A%2520Distilling%2520Knowledge%2520from%25202D%2520Natural%2520Images%2520via%2520Deformable%250A%2520%2520Tubular%2520Transferring%2520Strategy%2520for%25203D%2520Neuron%2520Reconstruction%26entry.906535625%3DYik%2520San%2520Cheng%2520and%2520Runkai%2520Zhao%2520and%2520Heng%2520Wang%2520and%2520Hanchuan%2520Peng%2520and%2520Yui%2520Lo%2520and%2520Yuqian%2520Chen%2520and%2520Lauren%2520J.%2520O%2527Donnell%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Reconstructing%2520neuron%2520morphology%2520from%25203D%2520light%2520microscope%2520imaging%2520data%2520is%250Acritical%2520to%2520aid%2520neuroscientists%2520in%2520analyzing%2520brain%2520networks%2520and%2520neuroanatomy.%250AWith%2520the%2520boost%2520from%2520deep%2520learning%2520techniques%252C%2520a%2520variety%2520of%2520learning-based%250Asegmentation%2520models%2520have%2520been%2520developed%2520to%2520enhance%2520the%2520signal-to-noise%2520ratio%2520of%250Araw%2520neuron%2520images%2520as%2520a%2520pre-processing%2520step%2520in%2520the%2520reconstruction%2520workflow.%250AHowever%252C%2520most%2520existing%2520models%2520directly%2520encode%2520the%2520latent%2520representative%250Afeatures%2520of%2520volumetric%2520neuron%2520data%2520but%2520neglect%2520their%2520intrinsic%2520morphological%250Aknowledge.%2520To%2520address%2520this%2520limitation%252C%2520we%2520design%2520a%2520novel%2520framework%2520that%250Adistills%2520the%2520prior%2520knowledge%2520from%2520a%25202D%2520Vision%2520Transformer%2520pre-trained%2520on%250Aextensive%25202D%2520natural%2520images%2520to%2520facilitate%2520neuronal%2520morphological%2520learning%2520of%250Aour%25203D%2520Vision%2520Transformer.%2520To%2520bridge%2520the%2520knowledge%2520gap%2520between%2520the%25202D%2520natural%250Aimage%2520and%25203D%2520microscopic%2520morphologic%2520domains%252C%2520we%2520propose%2520a%2520deformable%2520tubular%250Atransferring%2520strategy%2520that%2520adapts%2520the%2520pre-trained%25202D%2520natural%2520knowledge%2520to%2520the%250Ainherent%2520tubular%2520characteristics%2520of%2520neuronal%2520structure%2520in%2520the%2520latent%2520embedding%250Aspace.%2520The%2520experimental%2520results%2520on%2520the%2520Janelia%2520dataset%2520of%2520the%2520BigNeuron%2520project%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520a%2520segmentation%2520performance%2520improvement%2520of%250A4.53%2525%2520in%2520mean%2520Dice%2520and%25203.56%2525%2520in%2520mean%252095%2525%2520Hausdorff%2520distance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINeuro%3A%20Distilling%20Knowledge%20from%202D%20Natural%20Images%20via%20Deformable%0A%20%20Tubular%20Transferring%20Strategy%20for%203D%20Neuron%20Reconstruction&entry.906535625=Yik%20San%20Cheng%20and%20Runkai%20Zhao%20and%20Heng%20Wang%20and%20Hanchuan%20Peng%20and%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Lauren%20J.%20O%27Donnell%20and%20Weidong%20Cai&entry.1292438233=%20%20Reconstructing%20neuron%20morphology%20from%203D%20light%20microscope%20imaging%20data%20is%0Acritical%20to%20aid%20neuroscientists%20in%20analyzing%20brain%20networks%20and%20neuroanatomy.%0AWith%20the%20boost%20from%20deep%20learning%20techniques%2C%20a%20variety%20of%20learning-based%0Asegmentation%20models%20have%20been%20developed%20to%20enhance%20the%20signal-to-noise%20ratio%20of%0Araw%20neuron%20images%20as%20a%20pre-processing%20step%20in%20the%20reconstruction%20workflow.%0AHowever%2C%20most%20existing%20models%20directly%20encode%20the%20latent%20representative%0Afeatures%20of%20volumetric%20neuron%20data%20but%20neglect%20their%20intrinsic%20morphological%0Aknowledge.%20To%20address%20this%20limitation%2C%20we%20design%20a%20novel%20framework%20that%0Adistills%20the%20prior%20knowledge%20from%20a%202D%20Vision%20Transformer%20pre-trained%20on%0Aextensive%202D%20natural%20images%20to%20facilitate%20neuronal%20morphological%20learning%20of%0Aour%203D%20Vision%20Transformer.%20To%20bridge%20the%20knowledge%20gap%20between%20the%202D%20natural%0Aimage%20and%203D%20microscopic%20morphologic%20domains%2C%20we%20propose%20a%20deformable%20tubular%0Atransferring%20strategy%20that%20adapts%20the%20pre-trained%202D%20natural%20knowledge%20to%20the%0Ainherent%20tubular%20characteristics%20of%20neuronal%20structure%20in%20the%20latent%20embedding%0Aspace.%20The%20experimental%20results%20on%20the%20Janelia%20dataset%20of%20the%20BigNeuron%20project%0Ademonstrate%20that%20our%20method%20achieves%20a%20segmentation%20performance%20improvement%20of%0A4.53%25%20in%20mean%20Dice%20and%203.56%25%20in%20mean%2095%25%20Hausdorff%20distance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22078v1&entry.124074799=Read"},
{"title": "Non-rigid Relative Placement through 3D Dense Diffusion", "author": "Eric Cai and Octavian Donca and Ben Eisner and David Held", "abstract": "  The task of \"relative placement\" is to predict the placement of one object in\nrelation to another, e.g. placing a mug onto a mug rack. Through explicit\nobject-centric geometric reasoning, recent methods for relative placement have\nmade tremendous progress towards data-efficient learning for robot manipulation\nwhile generalizing to unseen task variations. However, they have yet to\nrepresent deformable transformations, despite the ubiquity of non-rigid bodies\nin real world settings. As a first step towards bridging this gap, we propose\n``cross-displacement\" - an extension of the principles of relative placement to\ngeometric relationships between deformable objects - and present a novel\nvision-based method to learn cross-displacement through dense diffusion. To\nthis end, we demonstrate our method's ability to generalize to unseen object\ninstances, out-of-distribution scene configurations, and multimodal goals on\nmultiple highly deformable tasks (both in simulation and in the real world)\nbeyond the scope of prior works. Supplementary information and videos can be\nfound at https://sites.google.com/view/tax3d-corl-2024 .\n", "link": "http://arxiv.org/abs/2410.19247v2", "date": "2024-10-29", "relevancy": 2.8548, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5719}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5719}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-rigid%20Relative%20Placement%20through%203D%20Dense%20Diffusion&body=Title%3A%20Non-rigid%20Relative%20Placement%20through%203D%20Dense%20Diffusion%0AAuthor%3A%20Eric%20Cai%20and%20Octavian%20Donca%20and%20Ben%20Eisner%20and%20David%20Held%0AAbstract%3A%20%20%20The%20task%20of%20%22relative%20placement%22%20is%20to%20predict%20the%20placement%20of%20one%20object%20in%0Arelation%20to%20another%2C%20e.g.%20placing%20a%20mug%20onto%20a%20mug%20rack.%20Through%20explicit%0Aobject-centric%20geometric%20reasoning%2C%20recent%20methods%20for%20relative%20placement%20have%0Amade%20tremendous%20progress%20towards%20data-efficient%20learning%20for%20robot%20manipulation%0Awhile%20generalizing%20to%20unseen%20task%20variations.%20However%2C%20they%20have%20yet%20to%0Arepresent%20deformable%20transformations%2C%20despite%20the%20ubiquity%20of%20non-rigid%20bodies%0Ain%20real%20world%20settings.%20As%20a%20first%20step%20towards%20bridging%20this%20gap%2C%20we%20propose%0A%60%60cross-displacement%22%20-%20an%20extension%20of%20the%20principles%20of%20relative%20placement%20to%0Ageometric%20relationships%20between%20deformable%20objects%20-%20and%20present%20a%20novel%0Avision-based%20method%20to%20learn%20cross-displacement%20through%20dense%20diffusion.%20To%0Athis%20end%2C%20we%20demonstrate%20our%20method%27s%20ability%20to%20generalize%20to%20unseen%20object%0Ainstances%2C%20out-of-distribution%20scene%20configurations%2C%20and%20multimodal%20goals%20on%0Amultiple%20highly%20deformable%20tasks%20%28both%20in%20simulation%20and%20in%20the%20real%20world%29%0Abeyond%20the%20scope%20of%20prior%20works.%20Supplementary%20information%20and%20videos%20can%20be%0Afound%20at%20https%3A//sites.google.com/view/tax3d-corl-2024%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-rigid%2520Relative%2520Placement%2520through%25203D%2520Dense%2520Diffusion%26entry.906535625%3DEric%2520Cai%2520and%2520Octavian%2520Donca%2520and%2520Ben%2520Eisner%2520and%2520David%2520Held%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520%2522relative%2520placement%2522%2520is%2520to%2520predict%2520the%2520placement%2520of%2520one%2520object%2520in%250Arelation%2520to%2520another%252C%2520e.g.%2520placing%2520a%2520mug%2520onto%2520a%2520mug%2520rack.%2520Through%2520explicit%250Aobject-centric%2520geometric%2520reasoning%252C%2520recent%2520methods%2520for%2520relative%2520placement%2520have%250Amade%2520tremendous%2520progress%2520towards%2520data-efficient%2520learning%2520for%2520robot%2520manipulation%250Awhile%2520generalizing%2520to%2520unseen%2520task%2520variations.%2520However%252C%2520they%2520have%2520yet%2520to%250Arepresent%2520deformable%2520transformations%252C%2520despite%2520the%2520ubiquity%2520of%2520non-rigid%2520bodies%250Ain%2520real%2520world%2520settings.%2520As%2520a%2520first%2520step%2520towards%2520bridging%2520this%2520gap%252C%2520we%2520propose%250A%2560%2560cross-displacement%2522%2520-%2520an%2520extension%2520of%2520the%2520principles%2520of%2520relative%2520placement%2520to%250Ageometric%2520relationships%2520between%2520deformable%2520objects%2520-%2520and%2520present%2520a%2520novel%250Avision-based%2520method%2520to%2520learn%2520cross-displacement%2520through%2520dense%2520diffusion.%2520To%250Athis%2520end%252C%2520we%2520demonstrate%2520our%2520method%2527s%2520ability%2520to%2520generalize%2520to%2520unseen%2520object%250Ainstances%252C%2520out-of-distribution%2520scene%2520configurations%252C%2520and%2520multimodal%2520goals%2520on%250Amultiple%2520highly%2520deformable%2520tasks%2520%2528both%2520in%2520simulation%2520and%2520in%2520the%2520real%2520world%2529%250Abeyond%2520the%2520scope%2520of%2520prior%2520works.%2520Supplementary%2520information%2520and%2520videos%2520can%2520be%250Afound%2520at%2520https%253A//sites.google.com/view/tax3d-corl-2024%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-rigid%20Relative%20Placement%20through%203D%20Dense%20Diffusion&entry.906535625=Eric%20Cai%20and%20Octavian%20Donca%20and%20Ben%20Eisner%20and%20David%20Held&entry.1292438233=%20%20The%20task%20of%20%22relative%20placement%22%20is%20to%20predict%20the%20placement%20of%20one%20object%20in%0Arelation%20to%20another%2C%20e.g.%20placing%20a%20mug%20onto%20a%20mug%20rack.%20Through%20explicit%0Aobject-centric%20geometric%20reasoning%2C%20recent%20methods%20for%20relative%20placement%20have%0Amade%20tremendous%20progress%20towards%20data-efficient%20learning%20for%20robot%20manipulation%0Awhile%20generalizing%20to%20unseen%20task%20variations.%20However%2C%20they%20have%20yet%20to%0Arepresent%20deformable%20transformations%2C%20despite%20the%20ubiquity%20of%20non-rigid%20bodies%0Ain%20real%20world%20settings.%20As%20a%20first%20step%20towards%20bridging%20this%20gap%2C%20we%20propose%0A%60%60cross-displacement%22%20-%20an%20extension%20of%20the%20principles%20of%20relative%20placement%20to%0Ageometric%20relationships%20between%20deformable%20objects%20-%20and%20present%20a%20novel%0Avision-based%20method%20to%20learn%20cross-displacement%20through%20dense%20diffusion.%20To%0Athis%20end%2C%20we%20demonstrate%20our%20method%27s%20ability%20to%20generalize%20to%20unseen%20object%0Ainstances%2C%20out-of-distribution%20scene%20configurations%2C%20and%20multimodal%20goals%20on%0Amultiple%20highly%20deformable%20tasks%20%28both%20in%20simulation%20and%20in%20the%20real%20world%29%0Abeyond%20the%20scope%20of%20prior%20works.%20Supplementary%20information%20and%20videos%20can%20be%0Afound%20at%20https%3A//sites.google.com/view/tax3d-corl-2024%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19247v2&entry.124074799=Read"},
{"title": "AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?", "author": "Han Bao and Yue Huang and Yanbo Wang and Jiayi Ye and Xiangqi Wang and Xiuying Chen and Mohamed Elhoseiny and Xiangliang Zhang", "abstract": "  Large Vision-Language Models (LVLMs) have become essential for advancing the\nintegration of visual and linguistic information, facilitating a wide range of\ncomplex applications and tasks. However, the evaluation of LVLMs presents\nsignificant challenges as the evaluation benchmark always demands lots of human\ncost for its construction, and remains static, lacking flexibility once\nconstructed. Even though automatic evaluation has been explored in textual\nmodality, the visual modality remains under-explored. As a result, in this\nwork, we address a question: \"Can LVLMs serve as a path to automatic\nbenchmarking?\". We introduce AutoBench-V, an automated framework for serving\nevaluation on demand, i.e., benchmarking LVLMs based on specific aspects of\nmodel capability. Upon receiving an evaluation capability, AutoBench-V\nleverages text-to-image models to generate relevant image samples and then\nutilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing\nthe evaluation process efficiently and flexibly. Through an extensive\nevaluation of seven popular LVLMs across five demanded user inputs (i.e.,\nevaluation capabilities), the framework shows effectiveness and reliability. We\nobserve the following: (1) Our constructed benchmark accurately reflects\nvarying task difficulties; (2) As task difficulty rises, the performance gap\nbetween models widens; (3) While models exhibit strong performance in abstract\nlevel understanding, they underperform in details reasoning tasks; and (4)\nConstructing a dataset with varying levels of difficulties is critical for a\ncomprehensive and exhaustive evaluation. Overall, AutoBench-V not only\nsuccessfully utilizes LVLMs for automated benchmarking but also reveals that\nLVLMs as judges have significant potential in various domains.\n", "link": "http://arxiv.org/abs/2410.21259v2", "date": "2024-10-29", "relevancy": 2.8464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoBench-V%3A%20Can%20Large%20Vision-Language%20Models%20Benchmark%20Themselves%3F&body=Title%3A%20AutoBench-V%3A%20Can%20Large%20Vision-Language%20Models%20Benchmark%20Themselves%3F%0AAuthor%3A%20Han%20Bao%20and%20Yue%20Huang%20and%20Yanbo%20Wang%20and%20Jiayi%20Ye%20and%20Xiangqi%20Wang%20and%20Xiuying%20Chen%20and%20Mohamed%20Elhoseiny%20and%20Xiangliang%20Zhang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20become%20essential%20for%20advancing%20the%0Aintegration%20of%20visual%20and%20linguistic%20information%2C%20facilitating%20a%20wide%20range%20of%0Acomplex%20applications%20and%20tasks.%20However%2C%20the%20evaluation%20of%20LVLMs%20presents%0Asignificant%20challenges%20as%20the%20evaluation%20benchmark%20always%20demands%20lots%20of%20human%0Acost%20for%20its%20construction%2C%20and%20remains%20static%2C%20lacking%20flexibility%20once%0Aconstructed.%20Even%20though%20automatic%20evaluation%20has%20been%20explored%20in%20textual%0Amodality%2C%20the%20visual%20modality%20remains%20under-explored.%20As%20a%20result%2C%20in%20this%0Awork%2C%20we%20address%20a%20question%3A%20%22Can%20LVLMs%20serve%20as%20a%20path%20to%20automatic%0Abenchmarking%3F%22.%20We%20introduce%20AutoBench-V%2C%20an%20automated%20framework%20for%20serving%0Aevaluation%20on%20demand%2C%20i.e.%2C%20benchmarking%20LVLMs%20based%20on%20specific%20aspects%20of%0Amodel%20capability.%20Upon%20receiving%20an%20evaluation%20capability%2C%20AutoBench-V%0Aleverages%20text-to-image%20models%20to%20generate%20relevant%20image%20samples%20and%20then%0Autilizes%20LVLMs%20to%20orchestrate%20visual%20question-answering%20%28VQA%29%20tasks%2C%20completing%0Athe%20evaluation%20process%20efficiently%20and%20flexibly.%20Through%20an%20extensive%0Aevaluation%20of%20seven%20popular%20LVLMs%20across%20five%20demanded%20user%20inputs%20%28i.e.%2C%0Aevaluation%20capabilities%29%2C%20the%20framework%20shows%20effectiveness%20and%20reliability.%20We%0Aobserve%20the%20following%3A%20%281%29%20Our%20constructed%20benchmark%20accurately%20reflects%0Avarying%20task%20difficulties%3B%20%282%29%20As%20task%20difficulty%20rises%2C%20the%20performance%20gap%0Abetween%20models%20widens%3B%20%283%29%20While%20models%20exhibit%20strong%20performance%20in%20abstract%0Alevel%20understanding%2C%20they%20underperform%20in%20details%20reasoning%20tasks%3B%20and%20%284%29%0AConstructing%20a%20dataset%20with%20varying%20levels%20of%20difficulties%20is%20critical%20for%20a%0Acomprehensive%20and%20exhaustive%20evaluation.%20Overall%2C%20AutoBench-V%20not%20only%0Asuccessfully%20utilizes%20LVLMs%20for%20automated%20benchmarking%20but%20also%20reveals%20that%0ALVLMs%20as%20judges%20have%20significant%20potential%20in%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoBench-V%253A%2520Can%2520Large%2520Vision-Language%2520Models%2520Benchmark%2520Themselves%253F%26entry.906535625%3DHan%2520Bao%2520and%2520Yue%2520Huang%2520and%2520Yanbo%2520Wang%2520and%2520Jiayi%2520Ye%2520and%2520Xiangqi%2520Wang%2520and%2520Xiuying%2520Chen%2520and%2520Mohamed%2520Elhoseiny%2520and%2520Xiangliang%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520become%2520essential%2520for%2520advancing%2520the%250Aintegration%2520of%2520visual%2520and%2520linguistic%2520information%252C%2520facilitating%2520a%2520wide%2520range%2520of%250Acomplex%2520applications%2520and%2520tasks.%2520However%252C%2520the%2520evaluation%2520of%2520LVLMs%2520presents%250Asignificant%2520challenges%2520as%2520the%2520evaluation%2520benchmark%2520always%2520demands%2520lots%2520of%2520human%250Acost%2520for%2520its%2520construction%252C%2520and%2520remains%2520static%252C%2520lacking%2520flexibility%2520once%250Aconstructed.%2520Even%2520though%2520automatic%2520evaluation%2520has%2520been%2520explored%2520in%2520textual%250Amodality%252C%2520the%2520visual%2520modality%2520remains%2520under-explored.%2520As%2520a%2520result%252C%2520in%2520this%250Awork%252C%2520we%2520address%2520a%2520question%253A%2520%2522Can%2520LVLMs%2520serve%2520as%2520a%2520path%2520to%2520automatic%250Abenchmarking%253F%2522.%2520We%2520introduce%2520AutoBench-V%252C%2520an%2520automated%2520framework%2520for%2520serving%250Aevaluation%2520on%2520demand%252C%2520i.e.%252C%2520benchmarking%2520LVLMs%2520based%2520on%2520specific%2520aspects%2520of%250Amodel%2520capability.%2520Upon%2520receiving%2520an%2520evaluation%2520capability%252C%2520AutoBench-V%250Aleverages%2520text-to-image%2520models%2520to%2520generate%2520relevant%2520image%2520samples%2520and%2520then%250Autilizes%2520LVLMs%2520to%2520orchestrate%2520visual%2520question-answering%2520%2528VQA%2529%2520tasks%252C%2520completing%250Athe%2520evaluation%2520process%2520efficiently%2520and%2520flexibly.%2520Through%2520an%2520extensive%250Aevaluation%2520of%2520seven%2520popular%2520LVLMs%2520across%2520five%2520demanded%2520user%2520inputs%2520%2528i.e.%252C%250Aevaluation%2520capabilities%2529%252C%2520the%2520framework%2520shows%2520effectiveness%2520and%2520reliability.%2520We%250Aobserve%2520the%2520following%253A%2520%25281%2529%2520Our%2520constructed%2520benchmark%2520accurately%2520reflects%250Avarying%2520task%2520difficulties%253B%2520%25282%2529%2520As%2520task%2520difficulty%2520rises%252C%2520the%2520performance%2520gap%250Abetween%2520models%2520widens%253B%2520%25283%2529%2520While%2520models%2520exhibit%2520strong%2520performance%2520in%2520abstract%250Alevel%2520understanding%252C%2520they%2520underperform%2520in%2520details%2520reasoning%2520tasks%253B%2520and%2520%25284%2529%250AConstructing%2520a%2520dataset%2520with%2520varying%2520levels%2520of%2520difficulties%2520is%2520critical%2520for%2520a%250Acomprehensive%2520and%2520exhaustive%2520evaluation.%2520Overall%252C%2520AutoBench-V%2520not%2520only%250Asuccessfully%2520utilizes%2520LVLMs%2520for%2520automated%2520benchmarking%2520but%2520also%2520reveals%2520that%250ALVLMs%2520as%2520judges%2520have%2520significant%2520potential%2520in%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoBench-V%3A%20Can%20Large%20Vision-Language%20Models%20Benchmark%20Themselves%3F&entry.906535625=Han%20Bao%20and%20Yue%20Huang%20and%20Yanbo%20Wang%20and%20Jiayi%20Ye%20and%20Xiangqi%20Wang%20and%20Xiuying%20Chen%20and%20Mohamed%20Elhoseiny%20and%20Xiangliang%20Zhang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20become%20essential%20for%20advancing%20the%0Aintegration%20of%20visual%20and%20linguistic%20information%2C%20facilitating%20a%20wide%20range%20of%0Acomplex%20applications%20and%20tasks.%20However%2C%20the%20evaluation%20of%20LVLMs%20presents%0Asignificant%20challenges%20as%20the%20evaluation%20benchmark%20always%20demands%20lots%20of%20human%0Acost%20for%20its%20construction%2C%20and%20remains%20static%2C%20lacking%20flexibility%20once%0Aconstructed.%20Even%20though%20automatic%20evaluation%20has%20been%20explored%20in%20textual%0Amodality%2C%20the%20visual%20modality%20remains%20under-explored.%20As%20a%20result%2C%20in%20this%0Awork%2C%20we%20address%20a%20question%3A%20%22Can%20LVLMs%20serve%20as%20a%20path%20to%20automatic%0Abenchmarking%3F%22.%20We%20introduce%20AutoBench-V%2C%20an%20automated%20framework%20for%20serving%0Aevaluation%20on%20demand%2C%20i.e.%2C%20benchmarking%20LVLMs%20based%20on%20specific%20aspects%20of%0Amodel%20capability.%20Upon%20receiving%20an%20evaluation%20capability%2C%20AutoBench-V%0Aleverages%20text-to-image%20models%20to%20generate%20relevant%20image%20samples%20and%20then%0Autilizes%20LVLMs%20to%20orchestrate%20visual%20question-answering%20%28VQA%29%20tasks%2C%20completing%0Athe%20evaluation%20process%20efficiently%20and%20flexibly.%20Through%20an%20extensive%0Aevaluation%20of%20seven%20popular%20LVLMs%20across%20five%20demanded%20user%20inputs%20%28i.e.%2C%0Aevaluation%20capabilities%29%2C%20the%20framework%20shows%20effectiveness%20and%20reliability.%20We%0Aobserve%20the%20following%3A%20%281%29%20Our%20constructed%20benchmark%20accurately%20reflects%0Avarying%20task%20difficulties%3B%20%282%29%20As%20task%20difficulty%20rises%2C%20the%20performance%20gap%0Abetween%20models%20widens%3B%20%283%29%20While%20models%20exhibit%20strong%20performance%20in%20abstract%0Alevel%20understanding%2C%20they%20underperform%20in%20details%20reasoning%20tasks%3B%20and%20%284%29%0AConstructing%20a%20dataset%20with%20varying%20levels%20of%20difficulties%20is%20critical%20for%20a%0Acomprehensive%20and%20exhaustive%20evaluation.%20Overall%2C%20AutoBench-V%20not%20only%0Asuccessfully%20utilizes%20LVLMs%20for%20automated%20benchmarking%20but%20also%20reveals%20that%0ALVLMs%20as%20judges%20have%20significant%20potential%20in%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21259v2&entry.124074799=Read"},
{"title": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds", "author": "Yui Lo and Yuqian Chen and Dongnan Liu and Jon Haitz Legarreta and Leo Zekelman and Fan Zhang and Jarrett Rushmore and Yogesh Rathi and Nikos Makris and Alexandra J. Golby and Weidong Cai and Lauren J. O'Donnell", "abstract": "  Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet.\n", "link": "http://arxiv.org/abs/2410.22099v1", "date": "2024-10-29", "relevancy": 2.8262, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6016}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TractShapeNet%3A%20Efficient%20Multi-Shape%20Learning%20with%203D%20Tractography%20Point%0A%20%20Clouds&body=Title%3A%20TractShapeNet%3A%20Efficient%20Multi-Shape%20Learning%20with%203D%20Tractography%20Point%0A%20%20Clouds%0AAuthor%3A%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Jon%20Haitz%20Legarreta%20and%20Leo%20Zekelman%20and%20Fan%20Zhang%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell%0AAbstract%3A%20%20%20Brain%20imaging%20studies%20have%20demonstrated%20that%20diffusion%20MRI%20tractography%0Ageometric%20shape%20descriptors%20can%20inform%20the%20study%20of%20the%20brain%27s%20white%20matter%0Apathways%20and%20their%20relationship%20to%20brain%20function.%20In%20this%20work%2C%20we%20investigate%0Athe%20possibility%20of%20utilizing%20a%20deep%20learning%20model%20to%20compute%20shape%20measures%20of%0Athe%20brain%27s%20white%20matter%20connections.%20We%20introduce%20a%20novel%20framework%2C%0ATractShapeNet%2C%20that%20leverages%20a%20point%20cloud%20representation%20of%20tractography%20to%0Acompute%20five%20shape%20measures%3A%20length%2C%20span%2C%20volume%2C%20total%20surface%20area%2C%20and%0Airregularity.%20We%20assess%20the%20performance%20of%20the%20method%20on%20a%20large%20dataset%0Aincluding%201065%20healthy%20young%20adults.%20Experiments%20for%20shape%20measure%20computation%0Ademonstrate%20that%20our%20proposed%20TractShapeNet%20outperforms%20other%20point%20cloud-based%0Aneural%20network%20models%20in%20both%20the%20Pearson%20correlation%20coefficient%20and%0Anormalized%20error%20metrics.%20We%20compare%20the%20inference%20runtime%20results%20with%20the%0Aconventional%20shape%20computation%20tool%20DSI-Studio.%20Our%20results%20demonstrate%20that%20a%0Adeep%20learning%20approach%20enables%20faster%20and%20more%20efficient%20shape%20measure%0Acomputation.%20We%20also%20conduct%20experiments%20on%20two%20downstream%20language%20cognition%0Aprediction%20tasks%2C%20showing%20that%20shape%20measures%20from%20TractShapeNet%20perform%0Asimilarly%20to%20those%20computed%20by%20DSI-Studio.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/SlicerDMRI/TractShapeNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTractShapeNet%253A%2520Efficient%2520Multi-Shape%2520Learning%2520with%25203D%2520Tractography%2520Point%250A%2520%2520Clouds%26entry.906535625%3DYui%2520Lo%2520and%2520Yuqian%2520Chen%2520and%2520Dongnan%2520Liu%2520and%2520Jon%2520Haitz%2520Legarreta%2520and%2520Leo%2520Zekelman%2520and%2520Fan%2520Zhang%2520and%2520Jarrett%2520Rushmore%2520and%2520Yogesh%2520Rathi%2520and%2520Nikos%2520Makris%2520and%2520Alexandra%2520J.%2520Golby%2520and%2520Weidong%2520Cai%2520and%2520Lauren%2520J.%2520O%2527Donnell%26entry.1292438233%3D%2520%2520Brain%2520imaging%2520studies%2520have%2520demonstrated%2520that%2520diffusion%2520MRI%2520tractography%250Ageometric%2520shape%2520descriptors%2520can%2520inform%2520the%2520study%2520of%2520the%2520brain%2527s%2520white%2520matter%250Apathways%2520and%2520their%2520relationship%2520to%2520brain%2520function.%2520In%2520this%2520work%252C%2520we%2520investigate%250Athe%2520possibility%2520of%2520utilizing%2520a%2520deep%2520learning%2520model%2520to%2520compute%2520shape%2520measures%2520of%250Athe%2520brain%2527s%2520white%2520matter%2520connections.%2520We%2520introduce%2520a%2520novel%2520framework%252C%250ATractShapeNet%252C%2520that%2520leverages%2520a%2520point%2520cloud%2520representation%2520of%2520tractography%2520to%250Acompute%2520five%2520shape%2520measures%253A%2520length%252C%2520span%252C%2520volume%252C%2520total%2520surface%2520area%252C%2520and%250Airregularity.%2520We%2520assess%2520the%2520performance%2520of%2520the%2520method%2520on%2520a%2520large%2520dataset%250Aincluding%25201065%2520healthy%2520young%2520adults.%2520Experiments%2520for%2520shape%2520measure%2520computation%250Ademonstrate%2520that%2520our%2520proposed%2520TractShapeNet%2520outperforms%2520other%2520point%2520cloud-based%250Aneural%2520network%2520models%2520in%2520both%2520the%2520Pearson%2520correlation%2520coefficient%2520and%250Anormalized%2520error%2520metrics.%2520We%2520compare%2520the%2520inference%2520runtime%2520results%2520with%2520the%250Aconventional%2520shape%2520computation%2520tool%2520DSI-Studio.%2520Our%2520results%2520demonstrate%2520that%2520a%250Adeep%2520learning%2520approach%2520enables%2520faster%2520and%2520more%2520efficient%2520shape%2520measure%250Acomputation.%2520We%2520also%2520conduct%2520experiments%2520on%2520two%2520downstream%2520language%2520cognition%250Aprediction%2520tasks%252C%2520showing%2520that%2520shape%2520measures%2520from%2520TractShapeNet%2520perform%250Asimilarly%2520to%2520those%2520computed%2520by%2520DSI-Studio.%2520Our%2520code%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/SlicerDMRI/TractShapeNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TractShapeNet%3A%20Efficient%20Multi-Shape%20Learning%20with%203D%20Tractography%20Point%0A%20%20Clouds&entry.906535625=Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Jon%20Haitz%20Legarreta%20and%20Leo%20Zekelman%20and%20Fan%20Zhang%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell&entry.1292438233=%20%20Brain%20imaging%20studies%20have%20demonstrated%20that%20diffusion%20MRI%20tractography%0Ageometric%20shape%20descriptors%20can%20inform%20the%20study%20of%20the%20brain%27s%20white%20matter%0Apathways%20and%20their%20relationship%20to%20brain%20function.%20In%20this%20work%2C%20we%20investigate%0Athe%20possibility%20of%20utilizing%20a%20deep%20learning%20model%20to%20compute%20shape%20measures%20of%0Athe%20brain%27s%20white%20matter%20connections.%20We%20introduce%20a%20novel%20framework%2C%0ATractShapeNet%2C%20that%20leverages%20a%20point%20cloud%20representation%20of%20tractography%20to%0Acompute%20five%20shape%20measures%3A%20length%2C%20span%2C%20volume%2C%20total%20surface%20area%2C%20and%0Airregularity.%20We%20assess%20the%20performance%20of%20the%20method%20on%20a%20large%20dataset%0Aincluding%201065%20healthy%20young%20adults.%20Experiments%20for%20shape%20measure%20computation%0Ademonstrate%20that%20our%20proposed%20TractShapeNet%20outperforms%20other%20point%20cloud-based%0Aneural%20network%20models%20in%20both%20the%20Pearson%20correlation%20coefficient%20and%0Anormalized%20error%20metrics.%20We%20compare%20the%20inference%20runtime%20results%20with%20the%0Aconventional%20shape%20computation%20tool%20DSI-Studio.%20Our%20results%20demonstrate%20that%20a%0Adeep%20learning%20approach%20enables%20faster%20and%20more%20efficient%20shape%20measure%0Acomputation.%20We%20also%20conduct%20experiments%20on%20two%20downstream%20language%20cognition%0Aprediction%20tasks%2C%20showing%20that%20shape%20measures%20from%20TractShapeNet%20perform%0Asimilarly%20to%20those%20computed%20by%20DSI-Studio.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/SlicerDMRI/TractShapeNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22099v1&entry.124074799=Read"},
{"title": "ContextIQ: A Multimodal Expert-Based Video Retrieval System for\n  Contextual Advertising", "author": "Ashutosh Chaubey and Anoubhav Agarwaal and Sartaki Sinha Roy and Aayush Agarwal and Susmita Ghose", "abstract": "  Contextual advertising serves ads that are aligned to the content that the\nuser is viewing. The rapid growth of video content on social platforms and\nstreaming services, along with privacy concerns, has increased the need for\ncontextual advertising. Placing the right ad in the right context creates a\nseamless and pleasant ad viewing experience, resulting in higher audience\nengagement and, ultimately, better ad monetization. From a technology\nstandpoint, effective contextual advertising requires a video retrieval system\ncapable of understanding complex video content at a very granular level.\nCurrent text-to-video retrieval models based on joint multimodal training\ndemand large datasets and computational resources, limiting their practicality\nand lacking the key functionalities required for ad ecosystem integration. We\nintroduce ContextIQ, a multimodal expert-based video retrieval system designed\nspecifically for contextual advertising. ContextIQ utilizes modality-specific\nexperts-video, audio, transcript (captions), and metadata such as objects,\nactions, emotion, etc.-to create semantically rich video representations. We\nshow that our system, without joint training, achieves better or comparable\nresults to state-of-the-art models and commercial solutions on multiple\ntext-to-video retrieval benchmarks. Our ablation studies highlight the benefits\nof leveraging multiple modalities for enhanced video retrieval accuracy instead\nof using a vision-language model alone. Furthermore, we show how video\nretrieval systems such as ContextIQ can be used for contextual advertising in\nan ad ecosystem while also addressing concerns related to brand safety and\nfiltering inappropriate content.\n", "link": "http://arxiv.org/abs/2410.22233v1", "date": "2024-10-29", "relevancy": 2.8078, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5636}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextIQ%3A%20A%20Multimodal%20Expert-Based%20Video%20Retrieval%20System%20for%0A%20%20Contextual%20Advertising&body=Title%3A%20ContextIQ%3A%20A%20Multimodal%20Expert-Based%20Video%20Retrieval%20System%20for%0A%20%20Contextual%20Advertising%0AAuthor%3A%20Ashutosh%20Chaubey%20and%20Anoubhav%20Agarwaal%20and%20Sartaki%20Sinha%20Roy%20and%20Aayush%20Agarwal%20and%20Susmita%20Ghose%0AAbstract%3A%20%20%20Contextual%20advertising%20serves%20ads%20that%20are%20aligned%20to%20the%20content%20that%20the%0Auser%20is%20viewing.%20The%20rapid%20growth%20of%20video%20content%20on%20social%20platforms%20and%0Astreaming%20services%2C%20along%20with%20privacy%20concerns%2C%20has%20increased%20the%20need%20for%0Acontextual%20advertising.%20Placing%20the%20right%20ad%20in%20the%20right%20context%20creates%20a%0Aseamless%20and%20pleasant%20ad%20viewing%20experience%2C%20resulting%20in%20higher%20audience%0Aengagement%20and%2C%20ultimately%2C%20better%20ad%20monetization.%20From%20a%20technology%0Astandpoint%2C%20effective%20contextual%20advertising%20requires%20a%20video%20retrieval%20system%0Acapable%20of%20understanding%20complex%20video%20content%20at%20a%20very%20granular%20level.%0ACurrent%20text-to-video%20retrieval%20models%20based%20on%20joint%20multimodal%20training%0Ademand%20large%20datasets%20and%20computational%20resources%2C%20limiting%20their%20practicality%0Aand%20lacking%20the%20key%20functionalities%20required%20for%20ad%20ecosystem%20integration.%20We%0Aintroduce%20ContextIQ%2C%20a%20multimodal%20expert-based%20video%20retrieval%20system%20designed%0Aspecifically%20for%20contextual%20advertising.%20ContextIQ%20utilizes%20modality-specific%0Aexperts-video%2C%20audio%2C%20transcript%20%28captions%29%2C%20and%20metadata%20such%20as%20objects%2C%0Aactions%2C%20emotion%2C%20etc.-to%20create%20semantically%20rich%20video%20representations.%20We%0Ashow%20that%20our%20system%2C%20without%20joint%20training%2C%20achieves%20better%20or%20comparable%0Aresults%20to%20state-of-the-art%20models%20and%20commercial%20solutions%20on%20multiple%0Atext-to-video%20retrieval%20benchmarks.%20Our%20ablation%20studies%20highlight%20the%20benefits%0Aof%20leveraging%20multiple%20modalities%20for%20enhanced%20video%20retrieval%20accuracy%20instead%0Aof%20using%20a%20vision-language%20model%20alone.%20Furthermore%2C%20we%20show%20how%20video%0Aretrieval%20systems%20such%20as%20ContextIQ%20can%20be%20used%20for%20contextual%20advertising%20in%0Aan%20ad%20ecosystem%20while%20also%20addressing%20concerns%20related%20to%20brand%20safety%20and%0Afiltering%20inappropriate%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextIQ%253A%2520A%2520Multimodal%2520Expert-Based%2520Video%2520Retrieval%2520System%2520for%250A%2520%2520Contextual%2520Advertising%26entry.906535625%3DAshutosh%2520Chaubey%2520and%2520Anoubhav%2520Agarwaal%2520and%2520Sartaki%2520Sinha%2520Roy%2520and%2520Aayush%2520Agarwal%2520and%2520Susmita%2520Ghose%26entry.1292438233%3D%2520%2520Contextual%2520advertising%2520serves%2520ads%2520that%2520are%2520aligned%2520to%2520the%2520content%2520that%2520the%250Auser%2520is%2520viewing.%2520The%2520rapid%2520growth%2520of%2520video%2520content%2520on%2520social%2520platforms%2520and%250Astreaming%2520services%252C%2520along%2520with%2520privacy%2520concerns%252C%2520has%2520increased%2520the%2520need%2520for%250Acontextual%2520advertising.%2520Placing%2520the%2520right%2520ad%2520in%2520the%2520right%2520context%2520creates%2520a%250Aseamless%2520and%2520pleasant%2520ad%2520viewing%2520experience%252C%2520resulting%2520in%2520higher%2520audience%250Aengagement%2520and%252C%2520ultimately%252C%2520better%2520ad%2520monetization.%2520From%2520a%2520technology%250Astandpoint%252C%2520effective%2520contextual%2520advertising%2520requires%2520a%2520video%2520retrieval%2520system%250Acapable%2520of%2520understanding%2520complex%2520video%2520content%2520at%2520a%2520very%2520granular%2520level.%250ACurrent%2520text-to-video%2520retrieval%2520models%2520based%2520on%2520joint%2520multimodal%2520training%250Ademand%2520large%2520datasets%2520and%2520computational%2520resources%252C%2520limiting%2520their%2520practicality%250Aand%2520lacking%2520the%2520key%2520functionalities%2520required%2520for%2520ad%2520ecosystem%2520integration.%2520We%250Aintroduce%2520ContextIQ%252C%2520a%2520multimodal%2520expert-based%2520video%2520retrieval%2520system%2520designed%250Aspecifically%2520for%2520contextual%2520advertising.%2520ContextIQ%2520utilizes%2520modality-specific%250Aexperts-video%252C%2520audio%252C%2520transcript%2520%2528captions%2529%252C%2520and%2520metadata%2520such%2520as%2520objects%252C%250Aactions%252C%2520emotion%252C%2520etc.-to%2520create%2520semantically%2520rich%2520video%2520representations.%2520We%250Ashow%2520that%2520our%2520system%252C%2520without%2520joint%2520training%252C%2520achieves%2520better%2520or%2520comparable%250Aresults%2520to%2520state-of-the-art%2520models%2520and%2520commercial%2520solutions%2520on%2520multiple%250Atext-to-video%2520retrieval%2520benchmarks.%2520Our%2520ablation%2520studies%2520highlight%2520the%2520benefits%250Aof%2520leveraging%2520multiple%2520modalities%2520for%2520enhanced%2520video%2520retrieval%2520accuracy%2520instead%250Aof%2520using%2520a%2520vision-language%2520model%2520alone.%2520Furthermore%252C%2520we%2520show%2520how%2520video%250Aretrieval%2520systems%2520such%2520as%2520ContextIQ%2520can%2520be%2520used%2520for%2520contextual%2520advertising%2520in%250Aan%2520ad%2520ecosystem%2520while%2520also%2520addressing%2520concerns%2520related%2520to%2520brand%2520safety%2520and%250Afiltering%2520inappropriate%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextIQ%3A%20A%20Multimodal%20Expert-Based%20Video%20Retrieval%20System%20for%0A%20%20Contextual%20Advertising&entry.906535625=Ashutosh%20Chaubey%20and%20Anoubhav%20Agarwaal%20and%20Sartaki%20Sinha%20Roy%20and%20Aayush%20Agarwal%20and%20Susmita%20Ghose&entry.1292438233=%20%20Contextual%20advertising%20serves%20ads%20that%20are%20aligned%20to%20the%20content%20that%20the%0Auser%20is%20viewing.%20The%20rapid%20growth%20of%20video%20content%20on%20social%20platforms%20and%0Astreaming%20services%2C%20along%20with%20privacy%20concerns%2C%20has%20increased%20the%20need%20for%0Acontextual%20advertising.%20Placing%20the%20right%20ad%20in%20the%20right%20context%20creates%20a%0Aseamless%20and%20pleasant%20ad%20viewing%20experience%2C%20resulting%20in%20higher%20audience%0Aengagement%20and%2C%20ultimately%2C%20better%20ad%20monetization.%20From%20a%20technology%0Astandpoint%2C%20effective%20contextual%20advertising%20requires%20a%20video%20retrieval%20system%0Acapable%20of%20understanding%20complex%20video%20content%20at%20a%20very%20granular%20level.%0ACurrent%20text-to-video%20retrieval%20models%20based%20on%20joint%20multimodal%20training%0Ademand%20large%20datasets%20and%20computational%20resources%2C%20limiting%20their%20practicality%0Aand%20lacking%20the%20key%20functionalities%20required%20for%20ad%20ecosystem%20integration.%20We%0Aintroduce%20ContextIQ%2C%20a%20multimodal%20expert-based%20video%20retrieval%20system%20designed%0Aspecifically%20for%20contextual%20advertising.%20ContextIQ%20utilizes%20modality-specific%0Aexperts-video%2C%20audio%2C%20transcript%20%28captions%29%2C%20and%20metadata%20such%20as%20objects%2C%0Aactions%2C%20emotion%2C%20etc.-to%20create%20semantically%20rich%20video%20representations.%20We%0Ashow%20that%20our%20system%2C%20without%20joint%20training%2C%20achieves%20better%20or%20comparable%0Aresults%20to%20state-of-the-art%20models%20and%20commercial%20solutions%20on%20multiple%0Atext-to-video%20retrieval%20benchmarks.%20Our%20ablation%20studies%20highlight%20the%20benefits%0Aof%20leveraging%20multiple%20modalities%20for%20enhanced%20video%20retrieval%20accuracy%20instead%0Aof%20using%20a%20vision-language%20model%20alone.%20Furthermore%2C%20we%20show%20how%20video%0Aretrieval%20systems%20such%20as%20ContextIQ%20can%20be%20used%20for%20contextual%20advertising%20in%0Aan%20ad%20ecosystem%20while%20also%20addressing%20concerns%20related%20to%20brand%20safety%20and%0Afiltering%20inappropriate%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22233v1&entry.124074799=Read"},
{"title": "M$^2$IST: Multi-Modal Interactive Side-Tuning for Efficient Referring\n  Expression Comprehension", "author": "Xuyang Liu and Ting Liu and Siteng Huang and Yi Xin and Yue Hu and Quanjun Yin and Donglin Wang and Honggang Chen", "abstract": "  Referring expression comprehension (REC) is a vision-language task to locate\na target object in an image based on a language expression. Fully fine-tuning\ngeneral-purpose pre-trained vision-language foundation models for REC yields\nimpressive performance but becomes increasingly costly. Parameter-efficient\ntransfer learning (PETL) methods have shown strong performance with fewer\ntunable parameters. However, directly applying PETL to REC faces two\nchallenges: (1) insufficient multi-modal interaction between pre-trained\nvision-language foundation models, and (2) high GPU memory usage due to\ngradients passing through the heavy vision-language foundation models. To this\nend, we present M$^2$IST: Multi-Modal Interactive Side-Tuning with M$^3$ISAs:\nMixture of Multi-Modal Interactive Side-Adapters. During fine-tuning, we keep\nthe pre-trained uni-modal encoders fixed, updating M$^3$ISAs on side networks\nto progressively connect them, enabling more comprehensive vision-language\nalignment and efficient tuning for REC. Empirical results reveal that M$^2$IST\nachieves an optimal balance between performance and efficiency compared to most\nfull fine-tuning and other PETL methods. With M$^2$IST, standard\ntransformer-based REC methods present competitive or even superior performance\ncompared to full fine-tuning, while utilizing only 2.11\\% of the tunable\nparameters, 39.61\\% of the GPU memory, and 63.46\\% of the fine-tuning time\nrequired for full fine-tuning.\n", "link": "http://arxiv.org/abs/2407.01131v2", "date": "2024-10-29", "relevancy": 2.7774, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E2%24IST%3A%20Multi-Modal%20Interactive%20Side-Tuning%20for%20Efficient%20Referring%0A%20%20Expression%20Comprehension&body=Title%3A%20M%24%5E2%24IST%3A%20Multi-Modal%20Interactive%20Side-Tuning%20for%20Efficient%20Referring%0A%20%20Expression%20Comprehension%0AAuthor%3A%20Xuyang%20Liu%20and%20Ting%20Liu%20and%20Siteng%20Huang%20and%20Yi%20Xin%20and%20Yue%20Hu%20and%20Quanjun%20Yin%20and%20Donglin%20Wang%20and%20Honggang%20Chen%0AAbstract%3A%20%20%20Referring%20expression%20comprehension%20%28REC%29%20is%20a%20vision-language%20task%20to%20locate%0Aa%20target%20object%20in%20an%20image%20based%20on%20a%20language%20expression.%20Fully%20fine-tuning%0Ageneral-purpose%20pre-trained%20vision-language%20foundation%20models%20for%20REC%20yields%0Aimpressive%20performance%20but%20becomes%20increasingly%20costly.%20Parameter-efficient%0Atransfer%20learning%20%28PETL%29%20methods%20have%20shown%20strong%20performance%20with%20fewer%0Atunable%20parameters.%20However%2C%20directly%20applying%20PETL%20to%20REC%20faces%20two%0Achallenges%3A%20%281%29%20insufficient%20multi-modal%20interaction%20between%20pre-trained%0Avision-language%20foundation%20models%2C%20and%20%282%29%20high%20GPU%20memory%20usage%20due%20to%0Agradients%20passing%20through%20the%20heavy%20vision-language%20foundation%20models.%20To%20this%0Aend%2C%20we%20present%20M%24%5E2%24IST%3A%20Multi-Modal%20Interactive%20Side-Tuning%20with%20M%24%5E3%24ISAs%3A%0AMixture%20of%20Multi-Modal%20Interactive%20Side-Adapters.%20During%20fine-tuning%2C%20we%20keep%0Athe%20pre-trained%20uni-modal%20encoders%20fixed%2C%20updating%20M%24%5E3%24ISAs%20on%20side%20networks%0Ato%20progressively%20connect%20them%2C%20enabling%20more%20comprehensive%20vision-language%0Aalignment%20and%20efficient%20tuning%20for%20REC.%20Empirical%20results%20reveal%20that%20M%24%5E2%24IST%0Aachieves%20an%20optimal%20balance%20between%20performance%20and%20efficiency%20compared%20to%20most%0Afull%20fine-tuning%20and%20other%20PETL%20methods.%20With%20M%24%5E2%24IST%2C%20standard%0Atransformer-based%20REC%20methods%20present%20competitive%20or%20even%20superior%20performance%0Acompared%20to%20full%20fine-tuning%2C%20while%20utilizing%20only%202.11%5C%25%20of%20the%20tunable%0Aparameters%2C%2039.61%5C%25%20of%20the%20GPU%20memory%2C%20and%2063.46%5C%25%20of%20the%20fine-tuning%20time%0Arequired%20for%20full%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E2%2524IST%253A%2520Multi-Modal%2520Interactive%2520Side-Tuning%2520for%2520Efficient%2520Referring%250A%2520%2520Expression%2520Comprehension%26entry.906535625%3DXuyang%2520Liu%2520and%2520Ting%2520Liu%2520and%2520Siteng%2520Huang%2520and%2520Yi%2520Xin%2520and%2520Yue%2520Hu%2520and%2520Quanjun%2520Yin%2520and%2520Donglin%2520Wang%2520and%2520Honggang%2520Chen%26entry.1292438233%3D%2520%2520Referring%2520expression%2520comprehension%2520%2528REC%2529%2520is%2520a%2520vision-language%2520task%2520to%2520locate%250Aa%2520target%2520object%2520in%2520an%2520image%2520based%2520on%2520a%2520language%2520expression.%2520Fully%2520fine-tuning%250Ageneral-purpose%2520pre-trained%2520vision-language%2520foundation%2520models%2520for%2520REC%2520yields%250Aimpressive%2520performance%2520but%2520becomes%2520increasingly%2520costly.%2520Parameter-efficient%250Atransfer%2520learning%2520%2528PETL%2529%2520methods%2520have%2520shown%2520strong%2520performance%2520with%2520fewer%250Atunable%2520parameters.%2520However%252C%2520directly%2520applying%2520PETL%2520to%2520REC%2520faces%2520two%250Achallenges%253A%2520%25281%2529%2520insufficient%2520multi-modal%2520interaction%2520between%2520pre-trained%250Avision-language%2520foundation%2520models%252C%2520and%2520%25282%2529%2520high%2520GPU%2520memory%2520usage%2520due%2520to%250Agradients%2520passing%2520through%2520the%2520heavy%2520vision-language%2520foundation%2520models.%2520To%2520this%250Aend%252C%2520we%2520present%2520M%2524%255E2%2524IST%253A%2520Multi-Modal%2520Interactive%2520Side-Tuning%2520with%2520M%2524%255E3%2524ISAs%253A%250AMixture%2520of%2520Multi-Modal%2520Interactive%2520Side-Adapters.%2520During%2520fine-tuning%252C%2520we%2520keep%250Athe%2520pre-trained%2520uni-modal%2520encoders%2520fixed%252C%2520updating%2520M%2524%255E3%2524ISAs%2520on%2520side%2520networks%250Ato%2520progressively%2520connect%2520them%252C%2520enabling%2520more%2520comprehensive%2520vision-language%250Aalignment%2520and%2520efficient%2520tuning%2520for%2520REC.%2520Empirical%2520results%2520reveal%2520that%2520M%2524%255E2%2524IST%250Aachieves%2520an%2520optimal%2520balance%2520between%2520performance%2520and%2520efficiency%2520compared%2520to%2520most%250Afull%2520fine-tuning%2520and%2520other%2520PETL%2520methods.%2520With%2520M%2524%255E2%2524IST%252C%2520standard%250Atransformer-based%2520REC%2520methods%2520present%2520competitive%2520or%2520even%2520superior%2520performance%250Acompared%2520to%2520full%2520fine-tuning%252C%2520while%2520utilizing%2520only%25202.11%255C%2525%2520of%2520the%2520tunable%250Aparameters%252C%252039.61%255C%2525%2520of%2520the%2520GPU%2520memory%252C%2520and%252063.46%255C%2525%2520of%2520the%2520fine-tuning%2520time%250Arequired%2520for%2520full%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E2%24IST%3A%20Multi-Modal%20Interactive%20Side-Tuning%20for%20Efficient%20Referring%0A%20%20Expression%20Comprehension&entry.906535625=Xuyang%20Liu%20and%20Ting%20Liu%20and%20Siteng%20Huang%20and%20Yi%20Xin%20and%20Yue%20Hu%20and%20Quanjun%20Yin%20and%20Donglin%20Wang%20and%20Honggang%20Chen&entry.1292438233=%20%20Referring%20expression%20comprehension%20%28REC%29%20is%20a%20vision-language%20task%20to%20locate%0Aa%20target%20object%20in%20an%20image%20based%20on%20a%20language%20expression.%20Fully%20fine-tuning%0Ageneral-purpose%20pre-trained%20vision-language%20foundation%20models%20for%20REC%20yields%0Aimpressive%20performance%20but%20becomes%20increasingly%20costly.%20Parameter-efficient%0Atransfer%20learning%20%28PETL%29%20methods%20have%20shown%20strong%20performance%20with%20fewer%0Atunable%20parameters.%20However%2C%20directly%20applying%20PETL%20to%20REC%20faces%20two%0Achallenges%3A%20%281%29%20insufficient%20multi-modal%20interaction%20between%20pre-trained%0Avision-language%20foundation%20models%2C%20and%20%282%29%20high%20GPU%20memory%20usage%20due%20to%0Agradients%20passing%20through%20the%20heavy%20vision-language%20foundation%20models.%20To%20this%0Aend%2C%20we%20present%20M%24%5E2%24IST%3A%20Multi-Modal%20Interactive%20Side-Tuning%20with%20M%24%5E3%24ISAs%3A%0AMixture%20of%20Multi-Modal%20Interactive%20Side-Adapters.%20During%20fine-tuning%2C%20we%20keep%0Athe%20pre-trained%20uni-modal%20encoders%20fixed%2C%20updating%20M%24%5E3%24ISAs%20on%20side%20networks%0Ato%20progressively%20connect%20them%2C%20enabling%20more%20comprehensive%20vision-language%0Aalignment%20and%20efficient%20tuning%20for%20REC.%20Empirical%20results%20reveal%20that%20M%24%5E2%24IST%0Aachieves%20an%20optimal%20balance%20between%20performance%20and%20efficiency%20compared%20to%20most%0Afull%20fine-tuning%20and%20other%20PETL%20methods.%20With%20M%24%5E2%24IST%2C%20standard%0Atransformer-based%20REC%20methods%20present%20competitive%20or%20even%20superior%20performance%0Acompared%20to%20full%20fine-tuning%2C%20while%20utilizing%20only%202.11%5C%25%20of%20the%20tunable%0Aparameters%2C%2039.61%5C%25%20of%20the%20GPU%20memory%2C%20and%2063.46%5C%25%20of%20the%20fine-tuning%20time%0Arequired%20for%20full%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01131v2&entry.124074799=Read"},
{"title": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System", "author": "Wen-Dong Jiang and Chih-Yung Chang and Hsiang-Chuan Chang and Diptendu Sinha Roy", "abstract": "  Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, these black-box systems\nface challenges regarding explainability during training and inference\nprocesses. An important question is how to incorporate explicit knowledge into\nthese implicit models, thereby designing expert-driven and interpretable\nviolence surveillance systems. This paper proposes a new paradigm for weakly\nsupervised violence monitoring (WSVM) called Rule base Violence monitoring\n(RuleVM). The proposed RuleVM uses a dual-branch structure for different\ndesigns for images and text. One of the branches is called the implicit branch,\nwhich uses only visual features for coarse-grained binary classification. In\nthis branch, image feature extraction is divided into two channels: one\nresponsible for extracting scene frames and the other focusing on extracting\nactions. The other branch is called the explicit branch, which utilizes\nlanguage-image alignment to perform fine-grained classification. For the\nlanguage channel design in the explicit branch, the proposed RuleCLIP uses the\nstate-of-the-art YOLO-World model to detect objects and actions in video\nframes, and association rules are identified through data mining methods as\ndescriptions of the video. Leveraging the dual?branch architecture, RuleVM\nachieves interpretable coarse?grained and fine-grained violence surveillance.\nExtensive experiments were conducted on two commonly used benchmarks, and the\nresults show that RuleCLIP achieved the best performance in both coarse-grained\nand fine-grained detection, significantly outperforming existing\nstate-of-the-art methods. Moreover, interpretability experiments uncovered some\ninteresting rules, such as the observation that as the number of people\nincreases, the risk level of violent behavior also rises.\n", "link": "http://arxiv.org/abs/2410.21991v1", "date": "2024-10-29", "relevancy": 2.7337, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Explicit%20Rules%20to%20Implicit%20Reasoning%20in%20an%20Interpretable%20Violence%0A%20%20Monitoring%20System&body=Title%3A%20From%20Explicit%20Rules%20to%20Implicit%20Reasoning%20in%20an%20Interpretable%20Violence%0A%20%20Monitoring%20System%0AAuthor%3A%20Wen-Dong%20Jiang%20and%20Chih-Yung%20Chang%20and%20Hsiang-Chuan%20Chang%20and%20Diptendu%20Sinha%20Roy%0AAbstract%3A%20%20%20Recently%2C%20research%20based%20on%20pre-trained%20models%20has%20demonstrated%20outstanding%0Aperformance%20in%20violence%20surveillance%20tasks.%20However%2C%20these%20black-box%20systems%0Aface%20challenges%20regarding%20explainability%20during%20training%20and%20inference%0Aprocesses.%20An%20important%20question%20is%20how%20to%20incorporate%20explicit%20knowledge%20into%0Athese%20implicit%20models%2C%20thereby%20designing%20expert-driven%20and%20interpretable%0Aviolence%20surveillance%20systems.%20This%20paper%20proposes%20a%20new%20paradigm%20for%20weakly%0Asupervised%20violence%20monitoring%20%28WSVM%29%20called%20Rule%20base%20Violence%20monitoring%0A%28RuleVM%29.%20The%20proposed%20RuleVM%20uses%20a%20dual-branch%20structure%20for%20different%0Adesigns%20for%20images%20and%20text.%20One%20of%20the%20branches%20is%20called%20the%20implicit%20branch%2C%0Awhich%20uses%20only%20visual%20features%20for%20coarse-grained%20binary%20classification.%20In%0Athis%20branch%2C%20image%20feature%20extraction%20is%20divided%20into%20two%20channels%3A%20one%0Aresponsible%20for%20extracting%20scene%20frames%20and%20the%20other%20focusing%20on%20extracting%0Aactions.%20The%20other%20branch%20is%20called%20the%20explicit%20branch%2C%20which%20utilizes%0Alanguage-image%20alignment%20to%20perform%20fine-grained%20classification.%20For%20the%0Alanguage%20channel%20design%20in%20the%20explicit%20branch%2C%20the%20proposed%20RuleCLIP%20uses%20the%0Astate-of-the-art%20YOLO-World%20model%20to%20detect%20objects%20and%20actions%20in%20video%0Aframes%2C%20and%20association%20rules%20are%20identified%20through%20data%20mining%20methods%20as%0Adescriptions%20of%20the%20video.%20Leveraging%20the%20dual%3Fbranch%20architecture%2C%20RuleVM%0Aachieves%20interpretable%20coarse%3Fgrained%20and%20fine-grained%20violence%20surveillance.%0AExtensive%20experiments%20were%20conducted%20on%20two%20commonly%20used%20benchmarks%2C%20and%20the%0Aresults%20show%20that%20RuleCLIP%20achieved%20the%20best%20performance%20in%20both%20coarse-grained%0Aand%20fine-grained%20detection%2C%20significantly%20outperforming%20existing%0Astate-of-the-art%20methods.%20Moreover%2C%20interpretability%20experiments%20uncovered%20some%0Ainteresting%20rules%2C%20such%20as%20the%20observation%20that%20as%20the%20number%20of%20people%0Aincreases%2C%20the%20risk%20level%20of%20violent%20behavior%20also%20rises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Explicit%2520Rules%2520to%2520Implicit%2520Reasoning%2520in%2520an%2520Interpretable%2520Violence%250A%2520%2520Monitoring%2520System%26entry.906535625%3DWen-Dong%2520Jiang%2520and%2520Chih-Yung%2520Chang%2520and%2520Hsiang-Chuan%2520Chang%2520and%2520Diptendu%2520Sinha%2520Roy%26entry.1292438233%3D%2520%2520Recently%252C%2520research%2520based%2520on%2520pre-trained%2520models%2520has%2520demonstrated%2520outstanding%250Aperformance%2520in%2520violence%2520surveillance%2520tasks.%2520However%252C%2520these%2520black-box%2520systems%250Aface%2520challenges%2520regarding%2520explainability%2520during%2520training%2520and%2520inference%250Aprocesses.%2520An%2520important%2520question%2520is%2520how%2520to%2520incorporate%2520explicit%2520knowledge%2520into%250Athese%2520implicit%2520models%252C%2520thereby%2520designing%2520expert-driven%2520and%2520interpretable%250Aviolence%2520surveillance%2520systems.%2520This%2520paper%2520proposes%2520a%2520new%2520paradigm%2520for%2520weakly%250Asupervised%2520violence%2520monitoring%2520%2528WSVM%2529%2520called%2520Rule%2520base%2520Violence%2520monitoring%250A%2528RuleVM%2529.%2520The%2520proposed%2520RuleVM%2520uses%2520a%2520dual-branch%2520structure%2520for%2520different%250Adesigns%2520for%2520images%2520and%2520text.%2520One%2520of%2520the%2520branches%2520is%2520called%2520the%2520implicit%2520branch%252C%250Awhich%2520uses%2520only%2520visual%2520features%2520for%2520coarse-grained%2520binary%2520classification.%2520In%250Athis%2520branch%252C%2520image%2520feature%2520extraction%2520is%2520divided%2520into%2520two%2520channels%253A%2520one%250Aresponsible%2520for%2520extracting%2520scene%2520frames%2520and%2520the%2520other%2520focusing%2520on%2520extracting%250Aactions.%2520The%2520other%2520branch%2520is%2520called%2520the%2520explicit%2520branch%252C%2520which%2520utilizes%250Alanguage-image%2520alignment%2520to%2520perform%2520fine-grained%2520classification.%2520For%2520the%250Alanguage%2520channel%2520design%2520in%2520the%2520explicit%2520branch%252C%2520the%2520proposed%2520RuleCLIP%2520uses%2520the%250Astate-of-the-art%2520YOLO-World%2520model%2520to%2520detect%2520objects%2520and%2520actions%2520in%2520video%250Aframes%252C%2520and%2520association%2520rules%2520are%2520identified%2520through%2520data%2520mining%2520methods%2520as%250Adescriptions%2520of%2520the%2520video.%2520Leveraging%2520the%2520dual%253Fbranch%2520architecture%252C%2520RuleVM%250Aachieves%2520interpretable%2520coarse%253Fgrained%2520and%2520fine-grained%2520violence%2520surveillance.%250AExtensive%2520experiments%2520were%2520conducted%2520on%2520two%2520commonly%2520used%2520benchmarks%252C%2520and%2520the%250Aresults%2520show%2520that%2520RuleCLIP%2520achieved%2520the%2520best%2520performance%2520in%2520both%2520coarse-grained%250Aand%2520fine-grained%2520detection%252C%2520significantly%2520outperforming%2520existing%250Astate-of-the-art%2520methods.%2520Moreover%252C%2520interpretability%2520experiments%2520uncovered%2520some%250Ainteresting%2520rules%252C%2520such%2520as%2520the%2520observation%2520that%2520as%2520the%2520number%2520of%2520people%250Aincreases%252C%2520the%2520risk%2520level%2520of%2520violent%2520behavior%2520also%2520rises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Explicit%20Rules%20to%20Implicit%20Reasoning%20in%20an%20Interpretable%20Violence%0A%20%20Monitoring%20System&entry.906535625=Wen-Dong%20Jiang%20and%20Chih-Yung%20Chang%20and%20Hsiang-Chuan%20Chang%20and%20Diptendu%20Sinha%20Roy&entry.1292438233=%20%20Recently%2C%20research%20based%20on%20pre-trained%20models%20has%20demonstrated%20outstanding%0Aperformance%20in%20violence%20surveillance%20tasks.%20However%2C%20these%20black-box%20systems%0Aface%20challenges%20regarding%20explainability%20during%20training%20and%20inference%0Aprocesses.%20An%20important%20question%20is%20how%20to%20incorporate%20explicit%20knowledge%20into%0Athese%20implicit%20models%2C%20thereby%20designing%20expert-driven%20and%20interpretable%0Aviolence%20surveillance%20systems.%20This%20paper%20proposes%20a%20new%20paradigm%20for%20weakly%0Asupervised%20violence%20monitoring%20%28WSVM%29%20called%20Rule%20base%20Violence%20monitoring%0A%28RuleVM%29.%20The%20proposed%20RuleVM%20uses%20a%20dual-branch%20structure%20for%20different%0Adesigns%20for%20images%20and%20text.%20One%20of%20the%20branches%20is%20called%20the%20implicit%20branch%2C%0Awhich%20uses%20only%20visual%20features%20for%20coarse-grained%20binary%20classification.%20In%0Athis%20branch%2C%20image%20feature%20extraction%20is%20divided%20into%20two%20channels%3A%20one%0Aresponsible%20for%20extracting%20scene%20frames%20and%20the%20other%20focusing%20on%20extracting%0Aactions.%20The%20other%20branch%20is%20called%20the%20explicit%20branch%2C%20which%20utilizes%0Alanguage-image%20alignment%20to%20perform%20fine-grained%20classification.%20For%20the%0Alanguage%20channel%20design%20in%20the%20explicit%20branch%2C%20the%20proposed%20RuleCLIP%20uses%20the%0Astate-of-the-art%20YOLO-World%20model%20to%20detect%20objects%20and%20actions%20in%20video%0Aframes%2C%20and%20association%20rules%20are%20identified%20through%20data%20mining%20methods%20as%0Adescriptions%20of%20the%20video.%20Leveraging%20the%20dual%3Fbranch%20architecture%2C%20RuleVM%0Aachieves%20interpretable%20coarse%3Fgrained%20and%20fine-grained%20violence%20surveillance.%0AExtensive%20experiments%20were%20conducted%20on%20two%20commonly%20used%20benchmarks%2C%20and%20the%0Aresults%20show%20that%20RuleCLIP%20achieved%20the%20best%20performance%20in%20both%20coarse-grained%0Aand%20fine-grained%20detection%2C%20significantly%20outperforming%20existing%0Astate-of-the-art%20methods.%20Moreover%2C%20interpretability%20experiments%20uncovered%20some%0Ainteresting%20rules%2C%20such%20as%20the%20observation%20that%20as%20the%20number%20of%20people%0Aincreases%2C%20the%20risk%20level%20of%20violent%20behavior%20also%20rises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21991v1&entry.124074799=Read"},
{"title": "Meta-Learning Adaptable Foundation Models", "author": "Jacob L. Block and Sundararajan Srinivasan and Liam Collins and Aryan Mokhtari and Sanjay Shakkottai", "abstract": "  The power of foundation models (FMs) lies in their capacity to learn highly\nexpressive representations that can be adapted to a broad spectrum of tasks.\nHowever, these pretrained models require multiple stages of fine-tuning to\nbecome effective for downstream applications. Conventionally, the model is\nfirst retrained on the aggregate of a diverse set of tasks of interest and then\nadapted to specific low-resource downstream tasks by utilizing a\nparameter-efficient fine-tuning (PEFT) scheme. While this two-phase procedure\nseems reasonable, the independence of the retraining and fine-tuning phases\ncauses a major issue, as there is no guarantee the retrained model will achieve\ngood performance post-fine-tuning. To explicitly address this issue, we\nintroduce a meta-learning framework infused with PEFT in this intermediate\nretraining stage to learn a model that can be easily adapted to unseen tasks.\nFor our theoretical results, we focus on linear models using low-rank\nadaptations. In this setting, we demonstrate the suboptimality of standard\nretraining for finding an adaptable set of parameters. Further, we prove that\nour method recovers the optimally adaptable parameters. We then apply these\ntheoretical insights to retraining the RoBERTa model to predict the\ncontinuation of conversations between different personas within the ConvAI2\ndataset. Empirically, we observe significant performance benefits using our\nproposed meta-learning scheme during retraining relative to the conventional\napproach.\n", "link": "http://arxiv.org/abs/2410.22264v1", "date": "2024-10-29", "relevancy": 2.7056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20Adaptable%20Foundation%20Models&body=Title%3A%20Meta-Learning%20Adaptable%20Foundation%20Models%0AAuthor%3A%20Jacob%20L.%20Block%20and%20Sundararajan%20Srinivasan%20and%20Liam%20Collins%20and%20Aryan%20Mokhtari%20and%20Sanjay%20Shakkottai%0AAbstract%3A%20%20%20The%20power%20of%20foundation%20models%20%28FMs%29%20lies%20in%20their%20capacity%20to%20learn%20highly%0Aexpressive%20representations%20that%20can%20be%20adapted%20to%20a%20broad%20spectrum%20of%20tasks.%0AHowever%2C%20these%20pretrained%20models%20require%20multiple%20stages%20of%20fine-tuning%20to%0Abecome%20effective%20for%20downstream%20applications.%20Conventionally%2C%20the%20model%20is%0Afirst%20retrained%20on%20the%20aggregate%20of%20a%20diverse%20set%20of%20tasks%20of%20interest%20and%20then%0Aadapted%20to%20specific%20low-resource%20downstream%20tasks%20by%20utilizing%20a%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20scheme.%20While%20this%20two-phase%20procedure%0Aseems%20reasonable%2C%20the%20independence%20of%20the%20retraining%20and%20fine-tuning%20phases%0Acauses%20a%20major%20issue%2C%20as%20there%20is%20no%20guarantee%20the%20retrained%20model%20will%20achieve%0Agood%20performance%20post-fine-tuning.%20To%20explicitly%20address%20this%20issue%2C%20we%0Aintroduce%20a%20meta-learning%20framework%20infused%20with%20PEFT%20in%20this%20intermediate%0Aretraining%20stage%20to%20learn%20a%20model%20that%20can%20be%20easily%20adapted%20to%20unseen%20tasks.%0AFor%20our%20theoretical%20results%2C%20we%20focus%20on%20linear%20models%20using%20low-rank%0Aadaptations.%20In%20this%20setting%2C%20we%20demonstrate%20the%20suboptimality%20of%20standard%0Aretraining%20for%20finding%20an%20adaptable%20set%20of%20parameters.%20Further%2C%20we%20prove%20that%0Aour%20method%20recovers%20the%20optimally%20adaptable%20parameters.%20We%20then%20apply%20these%0Atheoretical%20insights%20to%20retraining%20the%20RoBERTa%20model%20to%20predict%20the%0Acontinuation%20of%20conversations%20between%20different%20personas%20within%20the%20ConvAI2%0Adataset.%20Empirically%2C%20we%20observe%20significant%20performance%20benefits%20using%20our%0Aproposed%20meta-learning%20scheme%20during%20retraining%20relative%20to%20the%20conventional%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520Adaptable%2520Foundation%2520Models%26entry.906535625%3DJacob%2520L.%2520Block%2520and%2520Sundararajan%2520Srinivasan%2520and%2520Liam%2520Collins%2520and%2520Aryan%2520Mokhtari%2520and%2520Sanjay%2520Shakkottai%26entry.1292438233%3D%2520%2520The%2520power%2520of%2520foundation%2520models%2520%2528FMs%2529%2520lies%2520in%2520their%2520capacity%2520to%2520learn%2520highly%250Aexpressive%2520representations%2520that%2520can%2520be%2520adapted%2520to%2520a%2520broad%2520spectrum%2520of%2520tasks.%250AHowever%252C%2520these%2520pretrained%2520models%2520require%2520multiple%2520stages%2520of%2520fine-tuning%2520to%250Abecome%2520effective%2520for%2520downstream%2520applications.%2520Conventionally%252C%2520the%2520model%2520is%250Afirst%2520retrained%2520on%2520the%2520aggregate%2520of%2520a%2520diverse%2520set%2520of%2520tasks%2520of%2520interest%2520and%2520then%250Aadapted%2520to%2520specific%2520low-resource%2520downstream%2520tasks%2520by%2520utilizing%2520a%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520scheme.%2520While%2520this%2520two-phase%2520procedure%250Aseems%2520reasonable%252C%2520the%2520independence%2520of%2520the%2520retraining%2520and%2520fine-tuning%2520phases%250Acauses%2520a%2520major%2520issue%252C%2520as%2520there%2520is%2520no%2520guarantee%2520the%2520retrained%2520model%2520will%2520achieve%250Agood%2520performance%2520post-fine-tuning.%2520To%2520explicitly%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520meta-learning%2520framework%2520infused%2520with%2520PEFT%2520in%2520this%2520intermediate%250Aretraining%2520stage%2520to%2520learn%2520a%2520model%2520that%2520can%2520be%2520easily%2520adapted%2520to%2520unseen%2520tasks.%250AFor%2520our%2520theoretical%2520results%252C%2520we%2520focus%2520on%2520linear%2520models%2520using%2520low-rank%250Aadaptations.%2520In%2520this%2520setting%252C%2520we%2520demonstrate%2520the%2520suboptimality%2520of%2520standard%250Aretraining%2520for%2520finding%2520an%2520adaptable%2520set%2520of%2520parameters.%2520Further%252C%2520we%2520prove%2520that%250Aour%2520method%2520recovers%2520the%2520optimally%2520adaptable%2520parameters.%2520We%2520then%2520apply%2520these%250Atheoretical%2520insights%2520to%2520retraining%2520the%2520RoBERTa%2520model%2520to%2520predict%2520the%250Acontinuation%2520of%2520conversations%2520between%2520different%2520personas%2520within%2520the%2520ConvAI2%250Adataset.%2520Empirically%252C%2520we%2520observe%2520significant%2520performance%2520benefits%2520using%2520our%250Aproposed%2520meta-learning%2520scheme%2520during%2520retraining%2520relative%2520to%2520the%2520conventional%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20Adaptable%20Foundation%20Models&entry.906535625=Jacob%20L.%20Block%20and%20Sundararajan%20Srinivasan%20and%20Liam%20Collins%20and%20Aryan%20Mokhtari%20and%20Sanjay%20Shakkottai&entry.1292438233=%20%20The%20power%20of%20foundation%20models%20%28FMs%29%20lies%20in%20their%20capacity%20to%20learn%20highly%0Aexpressive%20representations%20that%20can%20be%20adapted%20to%20a%20broad%20spectrum%20of%20tasks.%0AHowever%2C%20these%20pretrained%20models%20require%20multiple%20stages%20of%20fine-tuning%20to%0Abecome%20effective%20for%20downstream%20applications.%20Conventionally%2C%20the%20model%20is%0Afirst%20retrained%20on%20the%20aggregate%20of%20a%20diverse%20set%20of%20tasks%20of%20interest%20and%20then%0Aadapted%20to%20specific%20low-resource%20downstream%20tasks%20by%20utilizing%20a%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20scheme.%20While%20this%20two-phase%20procedure%0Aseems%20reasonable%2C%20the%20independence%20of%20the%20retraining%20and%20fine-tuning%20phases%0Acauses%20a%20major%20issue%2C%20as%20there%20is%20no%20guarantee%20the%20retrained%20model%20will%20achieve%0Agood%20performance%20post-fine-tuning.%20To%20explicitly%20address%20this%20issue%2C%20we%0Aintroduce%20a%20meta-learning%20framework%20infused%20with%20PEFT%20in%20this%20intermediate%0Aretraining%20stage%20to%20learn%20a%20model%20that%20can%20be%20easily%20adapted%20to%20unseen%20tasks.%0AFor%20our%20theoretical%20results%2C%20we%20focus%20on%20linear%20models%20using%20low-rank%0Aadaptations.%20In%20this%20setting%2C%20we%20demonstrate%20the%20suboptimality%20of%20standard%0Aretraining%20for%20finding%20an%20adaptable%20set%20of%20parameters.%20Further%2C%20we%20prove%20that%0Aour%20method%20recovers%20the%20optimally%20adaptable%20parameters.%20We%20then%20apply%20these%0Atheoretical%20insights%20to%20retraining%20the%20RoBERTa%20model%20to%20predict%20the%0Acontinuation%20of%20conversations%20between%20different%20personas%20within%20the%20ConvAI2%0Adataset.%20Empirically%2C%20we%20observe%20significant%20performance%20benefits%20using%20our%0Aproposed%20meta-learning%20scheme%20during%20retraining%20relative%20to%20the%20conventional%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22264v1&entry.124074799=Read"},
{"title": "LLS: Local Learning Rule for Deep Neural Networks Inspired by Neural\n  Activity Synchronization", "author": "Marco Paul E. Apolinario and Arani Roy and Kaushik Roy", "abstract": "  Training deep neural networks (DNNs) using traditional backpropagation (BP)\npresents challenges in terms of computational complexity and energy\nconsumption, particularly for on-device learning where computational resources\nare limited. Various alternatives to BP, including random feedback alignment,\nforward-forward, and local classifiers, have been explored to address these\nchallenges. These methods have their advantages, but they can encounter\ndifficulties when dealing with intricate visual tasks or demand considerable\ncomputational resources. In this paper, we propose a novel Local Learning rule\ninspired by neural activity Synchronization phenomena (LLS) observed in the\nbrain. LLS utilizes fixed periodic basis vectors to synchronize neuron activity\nwithin each layer, enabling efficient training without the need for additional\ntrainable parameters. We demonstrate the effectiveness of LLS and its\nvariations, LLS-M and LLS-MxM, on multiple image classification datasets,\nachieving accuracy comparable to BP with reduced computational complexity and\nminimal additional parameters. Specifically, LLS achieves comparable\nperformance with up to $300 \\times$ fewer multiply-accumulate (MAC) operations\nand half the memory requirements of BP. Furthermore, the performance of LLS on\nthe Visual Wake Word (VWW) dataset highlights its suitability for on-device\nlearning tasks, making it a promising candidate for edge hardware\nimplementations.\n", "link": "http://arxiv.org/abs/2405.15868v2", "date": "2024-10-29", "relevancy": 2.7011, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5454}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.539}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLS%3A%20Local%20Learning%20Rule%20for%20Deep%20Neural%20Networks%20Inspired%20by%20Neural%0A%20%20Activity%20Synchronization&body=Title%3A%20LLS%3A%20Local%20Learning%20Rule%20for%20Deep%20Neural%20Networks%20Inspired%20by%20Neural%0A%20%20Activity%20Synchronization%0AAuthor%3A%20Marco%20Paul%20E.%20Apolinario%20and%20Arani%20Roy%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20using%20traditional%20backpropagation%20%28BP%29%0Apresents%20challenges%20in%20terms%20of%20computational%20complexity%20and%20energy%0Aconsumption%2C%20particularly%20for%20on-device%20learning%20where%20computational%20resources%0Aare%20limited.%20Various%20alternatives%20to%20BP%2C%20including%20random%20feedback%20alignment%2C%0Aforward-forward%2C%20and%20local%20classifiers%2C%20have%20been%20explored%20to%20address%20these%0Achallenges.%20These%20methods%20have%20their%20advantages%2C%20but%20they%20can%20encounter%0Adifficulties%20when%20dealing%20with%20intricate%20visual%20tasks%20or%20demand%20considerable%0Acomputational%20resources.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Local%20Learning%20rule%0Ainspired%20by%20neural%20activity%20Synchronization%20phenomena%20%28LLS%29%20observed%20in%20the%0Abrain.%20LLS%20utilizes%20fixed%20periodic%20basis%20vectors%20to%20synchronize%20neuron%20activity%0Awithin%20each%20layer%2C%20enabling%20efficient%20training%20without%20the%20need%20for%20additional%0Atrainable%20parameters.%20We%20demonstrate%20the%20effectiveness%20of%20LLS%20and%20its%0Avariations%2C%20LLS-M%20and%20LLS-MxM%2C%20on%20multiple%20image%20classification%20datasets%2C%0Aachieving%20accuracy%20comparable%20to%20BP%20with%20reduced%20computational%20complexity%20and%0Aminimal%20additional%20parameters.%20Specifically%2C%20LLS%20achieves%20comparable%0Aperformance%20with%20up%20to%20%24300%20%5Ctimes%24%20fewer%20multiply-accumulate%20%28MAC%29%20operations%0Aand%20half%20the%20memory%20requirements%20of%20BP.%20Furthermore%2C%20the%20performance%20of%20LLS%20on%0Athe%20Visual%20Wake%20Word%20%28VWW%29%20dataset%20highlights%20its%20suitability%20for%20on-device%0Alearning%20tasks%2C%20making%20it%20a%20promising%20candidate%20for%20edge%20hardware%0Aimplementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLS%253A%2520Local%2520Learning%2520Rule%2520for%2520Deep%2520Neural%2520Networks%2520Inspired%2520by%2520Neural%250A%2520%2520Activity%2520Synchronization%26entry.906535625%3DMarco%2520Paul%2520E.%2520Apolinario%2520and%2520Arani%2520Roy%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520using%2520traditional%2520backpropagation%2520%2528BP%2529%250Apresents%2520challenges%2520in%2520terms%2520of%2520computational%2520complexity%2520and%2520energy%250Aconsumption%252C%2520particularly%2520for%2520on-device%2520learning%2520where%2520computational%2520resources%250Aare%2520limited.%2520Various%2520alternatives%2520to%2520BP%252C%2520including%2520random%2520feedback%2520alignment%252C%250Aforward-forward%252C%2520and%2520local%2520classifiers%252C%2520have%2520been%2520explored%2520to%2520address%2520these%250Achallenges.%2520These%2520methods%2520have%2520their%2520advantages%252C%2520but%2520they%2520can%2520encounter%250Adifficulties%2520when%2520dealing%2520with%2520intricate%2520visual%2520tasks%2520or%2520demand%2520considerable%250Acomputational%2520resources.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Local%2520Learning%2520rule%250Ainspired%2520by%2520neural%2520activity%2520Synchronization%2520phenomena%2520%2528LLS%2529%2520observed%2520in%2520the%250Abrain.%2520LLS%2520utilizes%2520fixed%2520periodic%2520basis%2520vectors%2520to%2520synchronize%2520neuron%2520activity%250Awithin%2520each%2520layer%252C%2520enabling%2520efficient%2520training%2520without%2520the%2520need%2520for%2520additional%250Atrainable%2520parameters.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520LLS%2520and%2520its%250Avariations%252C%2520LLS-M%2520and%2520LLS-MxM%252C%2520on%2520multiple%2520image%2520classification%2520datasets%252C%250Aachieving%2520accuracy%2520comparable%2520to%2520BP%2520with%2520reduced%2520computational%2520complexity%2520and%250Aminimal%2520additional%2520parameters.%2520Specifically%252C%2520LLS%2520achieves%2520comparable%250Aperformance%2520with%2520up%2520to%2520%2524300%2520%255Ctimes%2524%2520fewer%2520multiply-accumulate%2520%2528MAC%2529%2520operations%250Aand%2520half%2520the%2520memory%2520requirements%2520of%2520BP.%2520Furthermore%252C%2520the%2520performance%2520of%2520LLS%2520on%250Athe%2520Visual%2520Wake%2520Word%2520%2528VWW%2529%2520dataset%2520highlights%2520its%2520suitability%2520for%2520on-device%250Alearning%2520tasks%252C%2520making%2520it%2520a%2520promising%2520candidate%2520for%2520edge%2520hardware%250Aimplementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLS%3A%20Local%20Learning%20Rule%20for%20Deep%20Neural%20Networks%20Inspired%20by%20Neural%0A%20%20Activity%20Synchronization&entry.906535625=Marco%20Paul%20E.%20Apolinario%20and%20Arani%20Roy%20and%20Kaushik%20Roy&entry.1292438233=%20%20Training%20deep%20neural%20networks%20%28DNNs%29%20using%20traditional%20backpropagation%20%28BP%29%0Apresents%20challenges%20in%20terms%20of%20computational%20complexity%20and%20energy%0Aconsumption%2C%20particularly%20for%20on-device%20learning%20where%20computational%20resources%0Aare%20limited.%20Various%20alternatives%20to%20BP%2C%20including%20random%20feedback%20alignment%2C%0Aforward-forward%2C%20and%20local%20classifiers%2C%20have%20been%20explored%20to%20address%20these%0Achallenges.%20These%20methods%20have%20their%20advantages%2C%20but%20they%20can%20encounter%0Adifficulties%20when%20dealing%20with%20intricate%20visual%20tasks%20or%20demand%20considerable%0Acomputational%20resources.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Local%20Learning%20rule%0Ainspired%20by%20neural%20activity%20Synchronization%20phenomena%20%28LLS%29%20observed%20in%20the%0Abrain.%20LLS%20utilizes%20fixed%20periodic%20basis%20vectors%20to%20synchronize%20neuron%20activity%0Awithin%20each%20layer%2C%20enabling%20efficient%20training%20without%20the%20need%20for%20additional%0Atrainable%20parameters.%20We%20demonstrate%20the%20effectiveness%20of%20LLS%20and%20its%0Avariations%2C%20LLS-M%20and%20LLS-MxM%2C%20on%20multiple%20image%20classification%20datasets%2C%0Aachieving%20accuracy%20comparable%20to%20BP%20with%20reduced%20computational%20complexity%20and%0Aminimal%20additional%20parameters.%20Specifically%2C%20LLS%20achieves%20comparable%0Aperformance%20with%20up%20to%20%24300%20%5Ctimes%24%20fewer%20multiply-accumulate%20%28MAC%29%20operations%0Aand%20half%20the%20memory%20requirements%20of%20BP.%20Furthermore%2C%20the%20performance%20of%20LLS%20on%0Athe%20Visual%20Wake%20Word%20%28VWW%29%20dataset%20highlights%20its%20suitability%20for%20on-device%0Alearning%20tasks%2C%20making%20it%20a%20promising%20candidate%20for%20edge%20hardware%0Aimplementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15868v2&entry.124074799=Read"},
{"title": "Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios:\n  Benchmarking Baseline Semantic Segmentation Models", "author": "Imad Ali Shah and Jiarong Li and Martin Glavin and Edward Jones and Enda Ward and Brian Deegan", "abstract": "  Hyperspectral Imaging (HSI) is known for its advantages over traditional RGB\nimaging in remote sensing, agriculture, and medicine. Recently, it has gained\nattention for enhancing Advanced Driving Assistance Systems (ADAS) perception.\nSeveral HSI datasets such as HyKo, HSI-Drive, HSI-Road, and Hyperspectral City\nhave been made available. However, a comprehensive evaluation of semantic\nsegmentation models (SSM) using these datasets is lacking. To address this gap,\nwe evaluated the available annotated HSI datasets on four deep learning-based\nbaseline SSMs: DeepLab v3+, HRNet, PSPNet, and U-Net, along with its two\nvariants: Coordinate Attention (UNet-CA) and Convolutional Block-Attention\nModule (UNet-CBAM). The original model architectures were adapted to handle the\nvarying spatial and spectral dimensions of the datasets. These baseline SSMs\nwere trained using a class-weighted loss function for individual HSI datasets\nand evaluated using mean-based metrics such as intersection over union (IoU),\nrecall, precision, F1 score, specificity, and accuracy. Our results indicate\nthat UNet-CBAM, which extracts channel-wise features, outperforms other SSMs\nand shows potential to leverage spectral information for enhanced semantic\nsegmentation. This study establishes a baseline SSM benchmark on available\nannotated datasets for future evaluation of HSI-based ADAS perception. However,\nlimitations of current HSI datasets, such as limited dataset size, high class\nimbalance, and lack of fine-grained annotations, remain significant constraints\nfor developing robust SSMs for ADAS applications.\n", "link": "http://arxiv.org/abs/2410.22101v1", "date": "2024-10-29", "relevancy": 2.697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Imaging-Based%20Perception%20in%20Autonomous%20Driving%20Scenarios%3A%0A%20%20Benchmarking%20Baseline%20Semantic%20Segmentation%20Models&body=Title%3A%20Hyperspectral%20Imaging-Based%20Perception%20in%20Autonomous%20Driving%20Scenarios%3A%0A%20%20Benchmarking%20Baseline%20Semantic%20Segmentation%20Models%0AAuthor%3A%20Imad%20Ali%20Shah%20and%20Jiarong%20Li%20and%20Martin%20Glavin%20and%20Edward%20Jones%20and%20Enda%20Ward%20and%20Brian%20Deegan%0AAbstract%3A%20%20%20Hyperspectral%20Imaging%20%28HSI%29%20is%20known%20for%20its%20advantages%20over%20traditional%20RGB%0Aimaging%20in%20remote%20sensing%2C%20agriculture%2C%20and%20medicine.%20Recently%2C%20it%20has%20gained%0Aattention%20for%20enhancing%20Advanced%20Driving%20Assistance%20Systems%20%28ADAS%29%20perception.%0ASeveral%20HSI%20datasets%20such%20as%20HyKo%2C%20HSI-Drive%2C%20HSI-Road%2C%20and%20Hyperspectral%20City%0Ahave%20been%20made%20available.%20However%2C%20a%20comprehensive%20evaluation%20of%20semantic%0Asegmentation%20models%20%28SSM%29%20using%20these%20datasets%20is%20lacking.%20To%20address%20this%20gap%2C%0Awe%20evaluated%20the%20available%20annotated%20HSI%20datasets%20on%20four%20deep%20learning-based%0Abaseline%20SSMs%3A%20DeepLab%20v3%2B%2C%20HRNet%2C%20PSPNet%2C%20and%20U-Net%2C%20along%20with%20its%20two%0Avariants%3A%20Coordinate%20Attention%20%28UNet-CA%29%20and%20Convolutional%20Block-Attention%0AModule%20%28UNet-CBAM%29.%20The%20original%20model%20architectures%20were%20adapted%20to%20handle%20the%0Avarying%20spatial%20and%20spectral%20dimensions%20of%20the%20datasets.%20These%20baseline%20SSMs%0Awere%20trained%20using%20a%20class-weighted%20loss%20function%20for%20individual%20HSI%20datasets%0Aand%20evaluated%20using%20mean-based%20metrics%20such%20as%20intersection%20over%20union%20%28IoU%29%2C%0Arecall%2C%20precision%2C%20F1%20score%2C%20specificity%2C%20and%20accuracy.%20Our%20results%20indicate%0Athat%20UNet-CBAM%2C%20which%20extracts%20channel-wise%20features%2C%20outperforms%20other%20SSMs%0Aand%20shows%20potential%20to%20leverage%20spectral%20information%20for%20enhanced%20semantic%0Asegmentation.%20This%20study%20establishes%20a%20baseline%20SSM%20benchmark%20on%20available%0Aannotated%20datasets%20for%20future%20evaluation%20of%20HSI-based%20ADAS%20perception.%20However%2C%0Alimitations%20of%20current%20HSI%20datasets%2C%20such%20as%20limited%20dataset%20size%2C%20high%20class%0Aimbalance%2C%20and%20lack%20of%20fine-grained%20annotations%2C%20remain%20significant%20constraints%0Afor%20developing%20robust%20SSMs%20for%20ADAS%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Imaging-Based%2520Perception%2520in%2520Autonomous%2520Driving%2520Scenarios%253A%250A%2520%2520Benchmarking%2520Baseline%2520Semantic%2520Segmentation%2520Models%26entry.906535625%3DImad%2520Ali%2520Shah%2520and%2520Jiarong%2520Li%2520and%2520Martin%2520Glavin%2520and%2520Edward%2520Jones%2520and%2520Enda%2520Ward%2520and%2520Brian%2520Deegan%26entry.1292438233%3D%2520%2520Hyperspectral%2520Imaging%2520%2528HSI%2529%2520is%2520known%2520for%2520its%2520advantages%2520over%2520traditional%2520RGB%250Aimaging%2520in%2520remote%2520sensing%252C%2520agriculture%252C%2520and%2520medicine.%2520Recently%252C%2520it%2520has%2520gained%250Aattention%2520for%2520enhancing%2520Advanced%2520Driving%2520Assistance%2520Systems%2520%2528ADAS%2529%2520perception.%250ASeveral%2520HSI%2520datasets%2520such%2520as%2520HyKo%252C%2520HSI-Drive%252C%2520HSI-Road%252C%2520and%2520Hyperspectral%2520City%250Ahave%2520been%2520made%2520available.%2520However%252C%2520a%2520comprehensive%2520evaluation%2520of%2520semantic%250Asegmentation%2520models%2520%2528SSM%2529%2520using%2520these%2520datasets%2520is%2520lacking.%2520To%2520address%2520this%2520gap%252C%250Awe%2520evaluated%2520the%2520available%2520annotated%2520HSI%2520datasets%2520on%2520four%2520deep%2520learning-based%250Abaseline%2520SSMs%253A%2520DeepLab%2520v3%252B%252C%2520HRNet%252C%2520PSPNet%252C%2520and%2520U-Net%252C%2520along%2520with%2520its%2520two%250Avariants%253A%2520Coordinate%2520Attention%2520%2528UNet-CA%2529%2520and%2520Convolutional%2520Block-Attention%250AModule%2520%2528UNet-CBAM%2529.%2520The%2520original%2520model%2520architectures%2520were%2520adapted%2520to%2520handle%2520the%250Avarying%2520spatial%2520and%2520spectral%2520dimensions%2520of%2520the%2520datasets.%2520These%2520baseline%2520SSMs%250Awere%2520trained%2520using%2520a%2520class-weighted%2520loss%2520function%2520for%2520individual%2520HSI%2520datasets%250Aand%2520evaluated%2520using%2520mean-based%2520metrics%2520such%2520as%2520intersection%2520over%2520union%2520%2528IoU%2529%252C%250Arecall%252C%2520precision%252C%2520F1%2520score%252C%2520specificity%252C%2520and%2520accuracy.%2520Our%2520results%2520indicate%250Athat%2520UNet-CBAM%252C%2520which%2520extracts%2520channel-wise%2520features%252C%2520outperforms%2520other%2520SSMs%250Aand%2520shows%2520potential%2520to%2520leverage%2520spectral%2520information%2520for%2520enhanced%2520semantic%250Asegmentation.%2520This%2520study%2520establishes%2520a%2520baseline%2520SSM%2520benchmark%2520on%2520available%250Aannotated%2520datasets%2520for%2520future%2520evaluation%2520of%2520HSI-based%2520ADAS%2520perception.%2520However%252C%250Alimitations%2520of%2520current%2520HSI%2520datasets%252C%2520such%2520as%2520limited%2520dataset%2520size%252C%2520high%2520class%250Aimbalance%252C%2520and%2520lack%2520of%2520fine-grained%2520annotations%252C%2520remain%2520significant%2520constraints%250Afor%2520developing%2520robust%2520SSMs%2520for%2520ADAS%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Imaging-Based%20Perception%20in%20Autonomous%20Driving%20Scenarios%3A%0A%20%20Benchmarking%20Baseline%20Semantic%20Segmentation%20Models&entry.906535625=Imad%20Ali%20Shah%20and%20Jiarong%20Li%20and%20Martin%20Glavin%20and%20Edward%20Jones%20and%20Enda%20Ward%20and%20Brian%20Deegan&entry.1292438233=%20%20Hyperspectral%20Imaging%20%28HSI%29%20is%20known%20for%20its%20advantages%20over%20traditional%20RGB%0Aimaging%20in%20remote%20sensing%2C%20agriculture%2C%20and%20medicine.%20Recently%2C%20it%20has%20gained%0Aattention%20for%20enhancing%20Advanced%20Driving%20Assistance%20Systems%20%28ADAS%29%20perception.%0ASeveral%20HSI%20datasets%20such%20as%20HyKo%2C%20HSI-Drive%2C%20HSI-Road%2C%20and%20Hyperspectral%20City%0Ahave%20been%20made%20available.%20However%2C%20a%20comprehensive%20evaluation%20of%20semantic%0Asegmentation%20models%20%28SSM%29%20using%20these%20datasets%20is%20lacking.%20To%20address%20this%20gap%2C%0Awe%20evaluated%20the%20available%20annotated%20HSI%20datasets%20on%20four%20deep%20learning-based%0Abaseline%20SSMs%3A%20DeepLab%20v3%2B%2C%20HRNet%2C%20PSPNet%2C%20and%20U-Net%2C%20along%20with%20its%20two%0Avariants%3A%20Coordinate%20Attention%20%28UNet-CA%29%20and%20Convolutional%20Block-Attention%0AModule%20%28UNet-CBAM%29.%20The%20original%20model%20architectures%20were%20adapted%20to%20handle%20the%0Avarying%20spatial%20and%20spectral%20dimensions%20of%20the%20datasets.%20These%20baseline%20SSMs%0Awere%20trained%20using%20a%20class-weighted%20loss%20function%20for%20individual%20HSI%20datasets%0Aand%20evaluated%20using%20mean-based%20metrics%20such%20as%20intersection%20over%20union%20%28IoU%29%2C%0Arecall%2C%20precision%2C%20F1%20score%2C%20specificity%2C%20and%20accuracy.%20Our%20results%20indicate%0Athat%20UNet-CBAM%2C%20which%20extracts%20channel-wise%20features%2C%20outperforms%20other%20SSMs%0Aand%20shows%20potential%20to%20leverage%20spectral%20information%20for%20enhanced%20semantic%0Asegmentation.%20This%20study%20establishes%20a%20baseline%20SSM%20benchmark%20on%20available%0Aannotated%20datasets%20for%20future%20evaluation%20of%20HSI-based%20ADAS%20perception.%20However%2C%0Alimitations%20of%20current%20HSI%20datasets%2C%20such%20as%20limited%20dataset%20size%2C%20high%20class%0Aimbalance%2C%20and%20lack%20of%20fine-grained%20annotations%2C%20remain%20significant%20constraints%0Afor%20developing%20robust%20SSMs%20for%20ADAS%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22101v1&entry.124074799=Read"},
{"title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency\n  Determines Multimodal Model Performance", "author": "Vishaal Udandarao and Ameya Prabhu and Adhiraj Ghosh and Yash Sharma and Philip H. S. Torr and Adel Bibi and Samuel Albanie and Matthias Bethge", "abstract": "  Web-crawled pretraining datasets underlie the impressive \"zero-shot\"\nevaluation performance of multimodal models, such as CLIP for\nclassification/retrieval and Stable-Diffusion for image generation. However, it\nis unclear how meaningful the notion of \"zero-shot\" generalization is for such\nmultimodal models, as it is not known to what extent their pretraining datasets\nencompass the downstream concepts targeted for during \"zero-shot\" evaluation.\nIn this work, we ask: How is the performance of multimodal models on downstream\nconcepts influenced by the frequency of these concepts in their pretraining\ndatasets? We comprehensively investigate this question across 34 models and\nfive standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M,\nLAION-Aesthetics), generating over 300GB of data artifacts. We consistently\nfind that, far from exhibiting \"zero-shot\" generalization, multimodal models\nrequire exponentially more data to achieve linear improvements in downstream\n\"zero-shot\" performance, following a sample inefficient log-linear scaling\ntrend. This trend persists even when controlling for sample-level similarity\nbetween pretraining and downstream datasets, and testing on purely synthetic\ndata distributions. Furthermore, upon benchmarking models on long-tailed data\nsampled based on our analysis, we demonstrate that multimodal models across the\nboard perform poorly. We contribute this long-tail test set as the \"Let it\nWag!\" benchmark to further research in this direction. Taken together, our\nstudy reveals an exponential need for training data which implies that the key\nto \"zero-shot\" generalization capabilities under large-scale training paradigms\nremains to be found.\n", "link": "http://arxiv.org/abs/2404.04125v3", "date": "2024-10-29", "relevancy": 2.6836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20%22Zero-Shot%22%20Without%20Exponential%20Data%3A%20Pretraining%20Concept%20Frequency%0A%20%20Determines%20Multimodal%20Model%20Performance&body=Title%3A%20No%20%22Zero-Shot%22%20Without%20Exponential%20Data%3A%20Pretraining%20Concept%20Frequency%0A%20%20Determines%20Multimodal%20Model%20Performance%0AAuthor%3A%20Vishaal%20Udandarao%20and%20Ameya%20Prabhu%20and%20Adhiraj%20Ghosh%20and%20Yash%20Sharma%20and%20Philip%20H.%20S.%20Torr%20and%20Adel%20Bibi%20and%20Samuel%20Albanie%20and%20Matthias%20Bethge%0AAbstract%3A%20%20%20Web-crawled%20pretraining%20datasets%20underlie%20the%20impressive%20%22zero-shot%22%0Aevaluation%20performance%20of%20multimodal%20models%2C%20such%20as%20CLIP%20for%0Aclassification/retrieval%20and%20Stable-Diffusion%20for%20image%20generation.%20However%2C%20it%0Ais%20unclear%20how%20meaningful%20the%20notion%20of%20%22zero-shot%22%20generalization%20is%20for%20such%0Amultimodal%20models%2C%20as%20it%20is%20not%20known%20to%20what%20extent%20their%20pretraining%20datasets%0Aencompass%20the%20downstream%20concepts%20targeted%20for%20during%20%22zero-shot%22%20evaluation.%0AIn%20this%20work%2C%20we%20ask%3A%20How%20is%20the%20performance%20of%20multimodal%20models%20on%20downstream%0Aconcepts%20influenced%20by%20the%20frequency%20of%20these%20concepts%20in%20their%20pretraining%0Adatasets%3F%20We%20comprehensively%20investigate%20this%20question%20across%2034%20models%20and%0Afive%20standard%20pretraining%20datasets%20%28CC-3M%2C%20CC-12M%2C%20YFCC-15M%2C%20LAION-400M%2C%0ALAION-Aesthetics%29%2C%20generating%20over%20300GB%20of%20data%20artifacts.%20We%20consistently%0Afind%20that%2C%20far%20from%20exhibiting%20%22zero-shot%22%20generalization%2C%20multimodal%20models%0Arequire%20exponentially%20more%20data%20to%20achieve%20linear%20improvements%20in%20downstream%0A%22zero-shot%22%20performance%2C%20following%20a%20sample%20inefficient%20log-linear%20scaling%0Atrend.%20This%20trend%20persists%20even%20when%20controlling%20for%20sample-level%20similarity%0Abetween%20pretraining%20and%20downstream%20datasets%2C%20and%20testing%20on%20purely%20synthetic%0Adata%20distributions.%20Furthermore%2C%20upon%20benchmarking%20models%20on%20long-tailed%20data%0Asampled%20based%20on%20our%20analysis%2C%20we%20demonstrate%20that%20multimodal%20models%20across%20the%0Aboard%20perform%20poorly.%20We%20contribute%20this%20long-tail%20test%20set%20as%20the%20%22Let%20it%0AWag%21%22%20benchmark%20to%20further%20research%20in%20this%20direction.%20Taken%20together%2C%20our%0Astudy%20reveals%20an%20exponential%20need%20for%20training%20data%20which%20implies%20that%20the%20key%0Ato%20%22zero-shot%22%20generalization%20capabilities%20under%20large-scale%20training%20paradigms%0Aremains%20to%20be%20found.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04125v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520%2522Zero-Shot%2522%2520Without%2520Exponential%2520Data%253A%2520Pretraining%2520Concept%2520Frequency%250A%2520%2520Determines%2520Multimodal%2520Model%2520Performance%26entry.906535625%3DVishaal%2520Udandarao%2520and%2520Ameya%2520Prabhu%2520and%2520Adhiraj%2520Ghosh%2520and%2520Yash%2520Sharma%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Adel%2520Bibi%2520and%2520Samuel%2520Albanie%2520and%2520Matthias%2520Bethge%26entry.1292438233%3D%2520%2520Web-crawled%2520pretraining%2520datasets%2520underlie%2520the%2520impressive%2520%2522zero-shot%2522%250Aevaluation%2520performance%2520of%2520multimodal%2520models%252C%2520such%2520as%2520CLIP%2520for%250Aclassification/retrieval%2520and%2520Stable-Diffusion%2520for%2520image%2520generation.%2520However%252C%2520it%250Ais%2520unclear%2520how%2520meaningful%2520the%2520notion%2520of%2520%2522zero-shot%2522%2520generalization%2520is%2520for%2520such%250Amultimodal%2520models%252C%2520as%2520it%2520is%2520not%2520known%2520to%2520what%2520extent%2520their%2520pretraining%2520datasets%250Aencompass%2520the%2520downstream%2520concepts%2520targeted%2520for%2520during%2520%2522zero-shot%2522%2520evaluation.%250AIn%2520this%2520work%252C%2520we%2520ask%253A%2520How%2520is%2520the%2520performance%2520of%2520multimodal%2520models%2520on%2520downstream%250Aconcepts%2520influenced%2520by%2520the%2520frequency%2520of%2520these%2520concepts%2520in%2520their%2520pretraining%250Adatasets%253F%2520We%2520comprehensively%2520investigate%2520this%2520question%2520across%252034%2520models%2520and%250Afive%2520standard%2520pretraining%2520datasets%2520%2528CC-3M%252C%2520CC-12M%252C%2520YFCC-15M%252C%2520LAION-400M%252C%250ALAION-Aesthetics%2529%252C%2520generating%2520over%2520300GB%2520of%2520data%2520artifacts.%2520We%2520consistently%250Afind%2520that%252C%2520far%2520from%2520exhibiting%2520%2522zero-shot%2522%2520generalization%252C%2520multimodal%2520models%250Arequire%2520exponentially%2520more%2520data%2520to%2520achieve%2520linear%2520improvements%2520in%2520downstream%250A%2522zero-shot%2522%2520performance%252C%2520following%2520a%2520sample%2520inefficient%2520log-linear%2520scaling%250Atrend.%2520This%2520trend%2520persists%2520even%2520when%2520controlling%2520for%2520sample-level%2520similarity%250Abetween%2520pretraining%2520and%2520downstream%2520datasets%252C%2520and%2520testing%2520on%2520purely%2520synthetic%250Adata%2520distributions.%2520Furthermore%252C%2520upon%2520benchmarking%2520models%2520on%2520long-tailed%2520data%250Asampled%2520based%2520on%2520our%2520analysis%252C%2520we%2520demonstrate%2520that%2520multimodal%2520models%2520across%2520the%250Aboard%2520perform%2520poorly.%2520We%2520contribute%2520this%2520long-tail%2520test%2520set%2520as%2520the%2520%2522Let%2520it%250AWag%2521%2522%2520benchmark%2520to%2520further%2520research%2520in%2520this%2520direction.%2520Taken%2520together%252C%2520our%250Astudy%2520reveals%2520an%2520exponential%2520need%2520for%2520training%2520data%2520which%2520implies%2520that%2520the%2520key%250Ato%2520%2522zero-shot%2522%2520generalization%2520capabilities%2520under%2520large-scale%2520training%2520paradigms%250Aremains%2520to%2520be%2520found.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04125v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20%22Zero-Shot%22%20Without%20Exponential%20Data%3A%20Pretraining%20Concept%20Frequency%0A%20%20Determines%20Multimodal%20Model%20Performance&entry.906535625=Vishaal%20Udandarao%20and%20Ameya%20Prabhu%20and%20Adhiraj%20Ghosh%20and%20Yash%20Sharma%20and%20Philip%20H.%20S.%20Torr%20and%20Adel%20Bibi%20and%20Samuel%20Albanie%20and%20Matthias%20Bethge&entry.1292438233=%20%20Web-crawled%20pretraining%20datasets%20underlie%20the%20impressive%20%22zero-shot%22%0Aevaluation%20performance%20of%20multimodal%20models%2C%20such%20as%20CLIP%20for%0Aclassification/retrieval%20and%20Stable-Diffusion%20for%20image%20generation.%20However%2C%20it%0Ais%20unclear%20how%20meaningful%20the%20notion%20of%20%22zero-shot%22%20generalization%20is%20for%20such%0Amultimodal%20models%2C%20as%20it%20is%20not%20known%20to%20what%20extent%20their%20pretraining%20datasets%0Aencompass%20the%20downstream%20concepts%20targeted%20for%20during%20%22zero-shot%22%20evaluation.%0AIn%20this%20work%2C%20we%20ask%3A%20How%20is%20the%20performance%20of%20multimodal%20models%20on%20downstream%0Aconcepts%20influenced%20by%20the%20frequency%20of%20these%20concepts%20in%20their%20pretraining%0Adatasets%3F%20We%20comprehensively%20investigate%20this%20question%20across%2034%20models%20and%0Afive%20standard%20pretraining%20datasets%20%28CC-3M%2C%20CC-12M%2C%20YFCC-15M%2C%20LAION-400M%2C%0ALAION-Aesthetics%29%2C%20generating%20over%20300GB%20of%20data%20artifacts.%20We%20consistently%0Afind%20that%2C%20far%20from%20exhibiting%20%22zero-shot%22%20generalization%2C%20multimodal%20models%0Arequire%20exponentially%20more%20data%20to%20achieve%20linear%20improvements%20in%20downstream%0A%22zero-shot%22%20performance%2C%20following%20a%20sample%20inefficient%20log-linear%20scaling%0Atrend.%20This%20trend%20persists%20even%20when%20controlling%20for%20sample-level%20similarity%0Abetween%20pretraining%20and%20downstream%20datasets%2C%20and%20testing%20on%20purely%20synthetic%0Adata%20distributions.%20Furthermore%2C%20upon%20benchmarking%20models%20on%20long-tailed%20data%0Asampled%20based%20on%20our%20analysis%2C%20we%20demonstrate%20that%20multimodal%20models%20across%20the%0Aboard%20perform%20poorly.%20We%20contribute%20this%20long-tail%20test%20set%20as%20the%20%22Let%20it%0AWag%21%22%20benchmark%20to%20further%20research%20in%20this%20direction.%20Taken%20together%2C%20our%0Astudy%20reveals%20an%20exponential%20need%20for%20training%20data%20which%20implies%20that%20the%20key%0Ato%20%22zero-shot%22%20generalization%20capabilities%20under%20large-scale%20training%20paradigms%0Aremains%20to%20be%20found.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04125v3&entry.124074799=Read"},
{"title": "Effective Guidance for Model Attention with Simple Yes-no Annotations", "author": "Seongmin Lee and Ali Payani and Duen Horng and  Chau", "abstract": "  Modern deep learning models often make predictions by focusing on irrelevant\nareas, leading to biased performance and limited generalization. Existing\nmethods aimed at rectifying model attention require explicit labels for\nirrelevant areas or complex pixel-wise ground truth attention maps. We present\nCRAYON (Correcting Reasoning with Annotations of Yes Or No), offering\neffective, scalable, and practical solutions to rectify model attention using\nsimple yes-no annotations. CRAYON empowers classical and modern model\ninterpretation techniques to identify and guide model reasoning:\nCRAYON-ATTENTION directs classic interpretations based on saliency maps to\nfocus on relevant image regions, while CRAYON-PRUNING removes irrelevant\nneurons identified by modern concept-based methods to mitigate their influence.\nThrough extensive experiments with both quantitative and human evaluation, we\nshowcase CRAYON's effectiveness, scalability, and practicality in refining\nmodel attention. CRAYON achieves state-of-the-art performance, outperforming 12\nmethods across 3 benchmark datasets, surpassing approaches that require more\ncomplex annotations.\n", "link": "http://arxiv.org/abs/2410.22312v1", "date": "2024-10-29", "relevancy": 2.6763, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Guidance%20for%20Model%20Attention%20with%20Simple%20Yes-no%20Annotations&body=Title%3A%20Effective%20Guidance%20for%20Model%20Attention%20with%20Simple%20Yes-no%20Annotations%0AAuthor%3A%20Seongmin%20Lee%20and%20Ali%20Payani%20and%20Duen%20Horng%20and%20%20Chau%0AAbstract%3A%20%20%20Modern%20deep%20learning%20models%20often%20make%20predictions%20by%20focusing%20on%20irrelevant%0Aareas%2C%20leading%20to%20biased%20performance%20and%20limited%20generalization.%20Existing%0Amethods%20aimed%20at%20rectifying%20model%20attention%20require%20explicit%20labels%20for%0Airrelevant%20areas%20or%20complex%20pixel-wise%20ground%20truth%20attention%20maps.%20We%20present%0ACRAYON%20%28Correcting%20Reasoning%20with%20Annotations%20of%20Yes%20Or%20No%29%2C%20offering%0Aeffective%2C%20scalable%2C%20and%20practical%20solutions%20to%20rectify%20model%20attention%20using%0Asimple%20yes-no%20annotations.%20CRAYON%20empowers%20classical%20and%20modern%20model%0Ainterpretation%20techniques%20to%20identify%20and%20guide%20model%20reasoning%3A%0ACRAYON-ATTENTION%20directs%20classic%20interpretations%20based%20on%20saliency%20maps%20to%0Afocus%20on%20relevant%20image%20regions%2C%20while%20CRAYON-PRUNING%20removes%20irrelevant%0Aneurons%20identified%20by%20modern%20concept-based%20methods%20to%20mitigate%20their%20influence.%0AThrough%20extensive%20experiments%20with%20both%20quantitative%20and%20human%20evaluation%2C%20we%0Ashowcase%20CRAYON%27s%20effectiveness%2C%20scalability%2C%20and%20practicality%20in%20refining%0Amodel%20attention.%20CRAYON%20achieves%20state-of-the-art%20performance%2C%20outperforming%2012%0Amethods%20across%203%20benchmark%20datasets%2C%20surpassing%20approaches%20that%20require%20more%0Acomplex%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Guidance%2520for%2520Model%2520Attention%2520with%2520Simple%2520Yes-no%2520Annotations%26entry.906535625%3DSeongmin%2520Lee%2520and%2520Ali%2520Payani%2520and%2520Duen%2520Horng%2520and%2520%2520Chau%26entry.1292438233%3D%2520%2520Modern%2520deep%2520learning%2520models%2520often%2520make%2520predictions%2520by%2520focusing%2520on%2520irrelevant%250Aareas%252C%2520leading%2520to%2520biased%2520performance%2520and%2520limited%2520generalization.%2520Existing%250Amethods%2520aimed%2520at%2520rectifying%2520model%2520attention%2520require%2520explicit%2520labels%2520for%250Airrelevant%2520areas%2520or%2520complex%2520pixel-wise%2520ground%2520truth%2520attention%2520maps.%2520We%2520present%250ACRAYON%2520%2528Correcting%2520Reasoning%2520with%2520Annotations%2520of%2520Yes%2520Or%2520No%2529%252C%2520offering%250Aeffective%252C%2520scalable%252C%2520and%2520practical%2520solutions%2520to%2520rectify%2520model%2520attention%2520using%250Asimple%2520yes-no%2520annotations.%2520CRAYON%2520empowers%2520classical%2520and%2520modern%2520model%250Ainterpretation%2520techniques%2520to%2520identify%2520and%2520guide%2520model%2520reasoning%253A%250ACRAYON-ATTENTION%2520directs%2520classic%2520interpretations%2520based%2520on%2520saliency%2520maps%2520to%250Afocus%2520on%2520relevant%2520image%2520regions%252C%2520while%2520CRAYON-PRUNING%2520removes%2520irrelevant%250Aneurons%2520identified%2520by%2520modern%2520concept-based%2520methods%2520to%2520mitigate%2520their%2520influence.%250AThrough%2520extensive%2520experiments%2520with%2520both%2520quantitative%2520and%2520human%2520evaluation%252C%2520we%250Ashowcase%2520CRAYON%2527s%2520effectiveness%252C%2520scalability%252C%2520and%2520practicality%2520in%2520refining%250Amodel%2520attention.%2520CRAYON%2520achieves%2520state-of-the-art%2520performance%252C%2520outperforming%252012%250Amethods%2520across%25203%2520benchmark%2520datasets%252C%2520surpassing%2520approaches%2520that%2520require%2520more%250Acomplex%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Guidance%20for%20Model%20Attention%20with%20Simple%20Yes-no%20Annotations&entry.906535625=Seongmin%20Lee%20and%20Ali%20Payani%20and%20Duen%20Horng%20and%20%20Chau&entry.1292438233=%20%20Modern%20deep%20learning%20models%20often%20make%20predictions%20by%20focusing%20on%20irrelevant%0Aareas%2C%20leading%20to%20biased%20performance%20and%20limited%20generalization.%20Existing%0Amethods%20aimed%20at%20rectifying%20model%20attention%20require%20explicit%20labels%20for%0Airrelevant%20areas%20or%20complex%20pixel-wise%20ground%20truth%20attention%20maps.%20We%20present%0ACRAYON%20%28Correcting%20Reasoning%20with%20Annotations%20of%20Yes%20Or%20No%29%2C%20offering%0Aeffective%2C%20scalable%2C%20and%20practical%20solutions%20to%20rectify%20model%20attention%20using%0Asimple%20yes-no%20annotations.%20CRAYON%20empowers%20classical%20and%20modern%20model%0Ainterpretation%20techniques%20to%20identify%20and%20guide%20model%20reasoning%3A%0ACRAYON-ATTENTION%20directs%20classic%20interpretations%20based%20on%20saliency%20maps%20to%0Afocus%20on%20relevant%20image%20regions%2C%20while%20CRAYON-PRUNING%20removes%20irrelevant%0Aneurons%20identified%20by%20modern%20concept-based%20methods%20to%20mitigate%20their%20influence.%0AThrough%20extensive%20experiments%20with%20both%20quantitative%20and%20human%20evaluation%2C%20we%0Ashowcase%20CRAYON%27s%20effectiveness%2C%20scalability%2C%20and%20practicality%20in%20refining%0Amodel%20attention.%20CRAYON%20achieves%20state-of-the-art%20performance%2C%20outperforming%2012%0Amethods%20across%203%20benchmark%20datasets%2C%20surpassing%20approaches%20that%20require%20more%0Acomplex%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22312v1&entry.124074799=Read"},
{"title": "PointCompress3D: A Point Cloud Compression Framework for Roadside LiDARs\n  in Intelligent Transportation Systems", "author": "Walter Zimmer and Ramandika Pranamulia and Xingcheng Zhou and Mingyu Liu and Alois C. Knoll", "abstract": "  In the context of Intelligent Transportation Systems (ITS), efficient data\ncompression is crucial for managing large-scale point cloud data acquired by\nroadside LiDAR sensors. The demand for efficient storage, streaming, and\nreal-time object detection capabilities for point cloud data is substantial.\nThis work introduces PointCompress3D, a novel point cloud compression framework\ntailored specifically for roadside LiDARs. Our framework addresses the\nchallenges of compressing high-resolution point clouds while maintaining\naccuracy and compatibility with roadside LiDAR sensors. We adapt, extend,\nintegrate, and evaluate three cutting-edge compression methods using our\nreal-world-based TUMTraf dataset family. We achieve a frame rate of 10 FPS\nwhile keeping compression sizes below 105 Kb, a reduction of 50 times, and\nmaintaining object detection performance on par with the original data. In\nextensive experiments and ablation studies, we finally achieved a PSNR d2 of\n94.46 and a BPP of 6.54 on our dataset. Future work includes the deployment on\nthe live system. The code is available on our project website:\nhttps://pointcompress3d.github.io.\n", "link": "http://arxiv.org/abs/2405.01750v2", "date": "2024-10-29", "relevancy": 2.6617, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointCompress3D%3A%20A%20Point%20Cloud%20Compression%20Framework%20for%20Roadside%20LiDARs%0A%20%20in%20Intelligent%20Transportation%20Systems&body=Title%3A%20PointCompress3D%3A%20A%20Point%20Cloud%20Compression%20Framework%20for%20Roadside%20LiDARs%0A%20%20in%20Intelligent%20Transportation%20Systems%0AAuthor%3A%20Walter%20Zimmer%20and%20Ramandika%20Pranamulia%20and%20Xingcheng%20Zhou%20and%20Mingyu%20Liu%20and%20Alois%20C.%20Knoll%0AAbstract%3A%20%20%20In%20the%20context%20of%20Intelligent%20Transportation%20Systems%20%28ITS%29%2C%20efficient%20data%0Acompression%20is%20crucial%20for%20managing%20large-scale%20point%20cloud%20data%20acquired%20by%0Aroadside%20LiDAR%20sensors.%20The%20demand%20for%20efficient%20storage%2C%20streaming%2C%20and%0Areal-time%20object%20detection%20capabilities%20for%20point%20cloud%20data%20is%20substantial.%0AThis%20work%20introduces%20PointCompress3D%2C%20a%20novel%20point%20cloud%20compression%20framework%0Atailored%20specifically%20for%20roadside%20LiDARs.%20Our%20framework%20addresses%20the%0Achallenges%20of%20compressing%20high-resolution%20point%20clouds%20while%20maintaining%0Aaccuracy%20and%20compatibility%20with%20roadside%20LiDAR%20sensors.%20We%20adapt%2C%20extend%2C%0Aintegrate%2C%20and%20evaluate%20three%20cutting-edge%20compression%20methods%20using%20our%0Areal-world-based%20TUMTraf%20dataset%20family.%20We%20achieve%20a%20frame%20rate%20of%2010%20FPS%0Awhile%20keeping%20compression%20sizes%20below%20105%20Kb%2C%20a%20reduction%20of%2050%20times%2C%20and%0Amaintaining%20object%20detection%20performance%20on%20par%20with%20the%20original%20data.%20In%0Aextensive%20experiments%20and%20ablation%20studies%2C%20we%20finally%20achieved%20a%20PSNR%20d2%20of%0A94.46%20and%20a%20BPP%20of%206.54%20on%20our%20dataset.%20Future%20work%20includes%20the%20deployment%20on%0Athe%20live%20system.%20The%20code%20is%20available%20on%20our%20project%20website%3A%0Ahttps%3A//pointcompress3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointCompress3D%253A%2520A%2520Point%2520Cloud%2520Compression%2520Framework%2520for%2520Roadside%2520LiDARs%250A%2520%2520in%2520Intelligent%2520Transportation%2520Systems%26entry.906535625%3DWalter%2520Zimmer%2520and%2520Ramandika%2520Pranamulia%2520and%2520Xingcheng%2520Zhou%2520and%2520Mingyu%2520Liu%2520and%2520Alois%2520C.%2520Knoll%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520Intelligent%2520Transportation%2520Systems%2520%2528ITS%2529%252C%2520efficient%2520data%250Acompression%2520is%2520crucial%2520for%2520managing%2520large-scale%2520point%2520cloud%2520data%2520acquired%2520by%250Aroadside%2520LiDAR%2520sensors.%2520The%2520demand%2520for%2520efficient%2520storage%252C%2520streaming%252C%2520and%250Areal-time%2520object%2520detection%2520capabilities%2520for%2520point%2520cloud%2520data%2520is%2520substantial.%250AThis%2520work%2520introduces%2520PointCompress3D%252C%2520a%2520novel%2520point%2520cloud%2520compression%2520framework%250Atailored%2520specifically%2520for%2520roadside%2520LiDARs.%2520Our%2520framework%2520addresses%2520the%250Achallenges%2520of%2520compressing%2520high-resolution%2520point%2520clouds%2520while%2520maintaining%250Aaccuracy%2520and%2520compatibility%2520with%2520roadside%2520LiDAR%2520sensors.%2520We%2520adapt%252C%2520extend%252C%250Aintegrate%252C%2520and%2520evaluate%2520three%2520cutting-edge%2520compression%2520methods%2520using%2520our%250Areal-world-based%2520TUMTraf%2520dataset%2520family.%2520We%2520achieve%2520a%2520frame%2520rate%2520of%252010%2520FPS%250Awhile%2520keeping%2520compression%2520sizes%2520below%2520105%2520Kb%252C%2520a%2520reduction%2520of%252050%2520times%252C%2520and%250Amaintaining%2520object%2520detection%2520performance%2520on%2520par%2520with%2520the%2520original%2520data.%2520In%250Aextensive%2520experiments%2520and%2520ablation%2520studies%252C%2520we%2520finally%2520achieved%2520a%2520PSNR%2520d2%2520of%250A94.46%2520and%2520a%2520BPP%2520of%25206.54%2520on%2520our%2520dataset.%2520Future%2520work%2520includes%2520the%2520deployment%2520on%250Athe%2520live%2520system.%2520The%2520code%2520is%2520available%2520on%2520our%2520project%2520website%253A%250Ahttps%253A//pointcompress3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointCompress3D%3A%20A%20Point%20Cloud%20Compression%20Framework%20for%20Roadside%20LiDARs%0A%20%20in%20Intelligent%20Transportation%20Systems&entry.906535625=Walter%20Zimmer%20and%20Ramandika%20Pranamulia%20and%20Xingcheng%20Zhou%20and%20Mingyu%20Liu%20and%20Alois%20C.%20Knoll&entry.1292438233=%20%20In%20the%20context%20of%20Intelligent%20Transportation%20Systems%20%28ITS%29%2C%20efficient%20data%0Acompression%20is%20crucial%20for%20managing%20large-scale%20point%20cloud%20data%20acquired%20by%0Aroadside%20LiDAR%20sensors.%20The%20demand%20for%20efficient%20storage%2C%20streaming%2C%20and%0Areal-time%20object%20detection%20capabilities%20for%20point%20cloud%20data%20is%20substantial.%0AThis%20work%20introduces%20PointCompress3D%2C%20a%20novel%20point%20cloud%20compression%20framework%0Atailored%20specifically%20for%20roadside%20LiDARs.%20Our%20framework%20addresses%20the%0Achallenges%20of%20compressing%20high-resolution%20point%20clouds%20while%20maintaining%0Aaccuracy%20and%20compatibility%20with%20roadside%20LiDAR%20sensors.%20We%20adapt%2C%20extend%2C%0Aintegrate%2C%20and%20evaluate%20three%20cutting-edge%20compression%20methods%20using%20our%0Areal-world-based%20TUMTraf%20dataset%20family.%20We%20achieve%20a%20frame%20rate%20of%2010%20FPS%0Awhile%20keeping%20compression%20sizes%20below%20105%20Kb%2C%20a%20reduction%20of%2050%20times%2C%20and%0Amaintaining%20object%20detection%20performance%20on%20par%20with%20the%20original%20data.%20In%0Aextensive%20experiments%20and%20ablation%20studies%2C%20we%20finally%20achieved%20a%20PSNR%20d2%20of%0A94.46%20and%20a%20BPP%20of%206.54%20on%20our%20dataset.%20Future%20work%20includes%20the%20deployment%20on%0Athe%20live%20system.%20The%20code%20is%20available%20on%20our%20project%20website%3A%0Ahttps%3A//pointcompress3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01750v2&entry.124074799=Read"},
{"title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation", "author": "Jianzong Wu and Xiangtai Li and Yanhong Zeng and Jiangning Zhang and Qianyu Zhou and Yining Li and Yunhai Tong and Kai Chen", "abstract": "  In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth\n", "link": "http://arxiv.org/abs/2406.17758v3", "date": "2024-10-29", "relevancy": 2.6494, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6669}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation&body=Title%3A%20MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation%0AAuthor%3A%20Jianzong%20Wu%20and%20Xiangtai%20Li%20and%20Yanhong%20Zeng%20and%20Jiangning%20Zhang%20and%20Qianyu%20Zhou%20and%20Yining%20Li%20and%20Yunhai%20Tong%20and%20Kai%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20MotionBooth%2C%20an%20innovative%20framework%20designed%20for%0Aanimating%20customized%20subjects%20with%20precise%20control%20over%20both%20object%20and%20camera%0Amovements.%20By%20leveraging%20a%20few%20images%20of%20a%20specific%20object%2C%20we%20efficiently%0Afine-tune%20a%20text-to-video%20model%20to%20capture%20the%20object%27s%20shape%20and%20attributes%0Aaccurately.%20Our%20approach%20presents%20subject%20region%20loss%20and%20video%20preservation%0Aloss%20to%20enhance%20the%20subject%27s%20learning%20performance%2C%20along%20with%20a%20subject%20token%0Across-attention%20loss%20to%20integrate%20the%20customized%20subject%20with%20motion%20control%0Asignals.%20Additionally%2C%20we%20propose%20training-free%20techniques%20for%20managing%20subject%0Aand%20camera%20motions%20during%20inference.%20In%20particular%2C%20we%20utilize%20cross-attention%0Amap%20manipulation%20to%20govern%20subject%20motion%20and%20introduce%20a%20novel%20latent%20shift%0Amodule%20for%20camera%20movement%20control%20as%20well.%20MotionBooth%20excels%20in%20preserving%0Athe%20appearance%20of%20subjects%20while%20simultaneously%20controlling%20the%20motions%20in%0Agenerated%20videos.%20Extensive%20quantitative%20and%20qualitative%20evaluations%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20our%20method.%20Our%20project%20page%0Ais%20at%20https%3A//jianzongwu.github.io/projects/motionbooth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17758v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionBooth%253A%2520Motion-Aware%2520Customized%2520Text-to-Video%2520Generation%26entry.906535625%3DJianzong%2520Wu%2520and%2520Xiangtai%2520Li%2520and%2520Yanhong%2520Zeng%2520and%2520Jiangning%2520Zhang%2520and%2520Qianyu%2520Zhou%2520and%2520Yining%2520Li%2520and%2520Yunhai%2520Tong%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520MotionBooth%252C%2520an%2520innovative%2520framework%2520designed%2520for%250Aanimating%2520customized%2520subjects%2520with%2520precise%2520control%2520over%2520both%2520object%2520and%2520camera%250Amovements.%2520By%2520leveraging%2520a%2520few%2520images%2520of%2520a%2520specific%2520object%252C%2520we%2520efficiently%250Afine-tune%2520a%2520text-to-video%2520model%2520to%2520capture%2520the%2520object%2527s%2520shape%2520and%2520attributes%250Aaccurately.%2520Our%2520approach%2520presents%2520subject%2520region%2520loss%2520and%2520video%2520preservation%250Aloss%2520to%2520enhance%2520the%2520subject%2527s%2520learning%2520performance%252C%2520along%2520with%2520a%2520subject%2520token%250Across-attention%2520loss%2520to%2520integrate%2520the%2520customized%2520subject%2520with%2520motion%2520control%250Asignals.%2520Additionally%252C%2520we%2520propose%2520training-free%2520techniques%2520for%2520managing%2520subject%250Aand%2520camera%2520motions%2520during%2520inference.%2520In%2520particular%252C%2520we%2520utilize%2520cross-attention%250Amap%2520manipulation%2520to%2520govern%2520subject%2520motion%2520and%2520introduce%2520a%2520novel%2520latent%2520shift%250Amodule%2520for%2520camera%2520movement%2520control%2520as%2520well.%2520MotionBooth%2520excels%2520in%2520preserving%250Athe%2520appearance%2520of%2520subjects%2520while%2520simultaneously%2520controlling%2520the%2520motions%2520in%250Agenerated%2520videos.%2520Extensive%2520quantitative%2520and%2520qualitative%2520evaluations%250Ademonstrate%2520the%2520superiority%2520and%2520effectiveness%2520of%2520our%2520method.%2520Our%2520project%2520page%250Ais%2520at%2520https%253A//jianzongwu.github.io/projects/motionbooth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17758v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation&entry.906535625=Jianzong%20Wu%20and%20Xiangtai%20Li%20and%20Yanhong%20Zeng%20and%20Jiangning%20Zhang%20and%20Qianyu%20Zhou%20and%20Yining%20Li%20and%20Yunhai%20Tong%20and%20Kai%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20MotionBooth%2C%20an%20innovative%20framework%20designed%20for%0Aanimating%20customized%20subjects%20with%20precise%20control%20over%20both%20object%20and%20camera%0Amovements.%20By%20leveraging%20a%20few%20images%20of%20a%20specific%20object%2C%20we%20efficiently%0Afine-tune%20a%20text-to-video%20model%20to%20capture%20the%20object%27s%20shape%20and%20attributes%0Aaccurately.%20Our%20approach%20presents%20subject%20region%20loss%20and%20video%20preservation%0Aloss%20to%20enhance%20the%20subject%27s%20learning%20performance%2C%20along%20with%20a%20subject%20token%0Across-attention%20loss%20to%20integrate%20the%20customized%20subject%20with%20motion%20control%0Asignals.%20Additionally%2C%20we%20propose%20training-free%20techniques%20for%20managing%20subject%0Aand%20camera%20motions%20during%20inference.%20In%20particular%2C%20we%20utilize%20cross-attention%0Amap%20manipulation%20to%20govern%20subject%20motion%20and%20introduce%20a%20novel%20latent%20shift%0Amodule%20for%20camera%20movement%20control%20as%20well.%20MotionBooth%20excels%20in%20preserving%0Athe%20appearance%20of%20subjects%20while%20simultaneously%20controlling%20the%20motions%20in%0Agenerated%20videos.%20Extensive%20quantitative%20and%20qualitative%20evaluations%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20our%20method.%20Our%20project%20page%0Ais%20at%20https%3A//jianzongwu.github.io/projects/motionbooth%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17758v3&entry.124074799=Read"},
{"title": "GARField: Addressing the visual Sim-to-Real gap in garment manipulation\n  with mesh-attached radiance fields", "author": "Donatien Delehelle and Darwin G. Caldwell and Fei Chen", "abstract": "  While humans intuitively manipulate garments and other textile items swiftly\nand accurately, it is a significant challenge for robots. A factor crucial to\nhuman performance is the ability to imagine, a priori, the intended result of\nthe manipulation intents and hence develop predictions on the garment pose.\nThat ability allows us to plan from highly obstructed states, adapt our plans\nas we collect more information and react swiftly to unforeseen circumstances.\nConversely, robots struggle to establish such intuitions and form tight links\nbetween plans and observations. We can partly attribute this to the high cost\nof obtaining densely labelled data for textile manipulation, both in quality\nand quantity. The problem of data collection is a long-standing issue in\ndata-based approaches to garment manipulation. As of today, generating\nhigh-quality and labelled garment manipulation data is mainly attempted through\nadvanced data capture procedures that create simplified state estimations from\nreal-world observations. However, this work proposes a novel approach to the\nproblem by generating real-world observations from object states. To achieve\nthis, we present GARField (Garment Attached Radiance Field), the first\ndifferentiable rendering architecture, to our knowledge, for data generation\nfrom simulated states stored as triangle meshes. Code is available on\nhttps://ddonatien.github.io/garfield-website/\n", "link": "http://arxiv.org/abs/2410.05038v2", "date": "2024-10-29", "relevancy": 2.6422, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6771}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6768}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields&body=Title%3A%20GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields%0AAuthor%3A%20Donatien%20Delehelle%20and%20Darwin%20G.%20Caldwell%20and%20Fei%20Chen%0AAbstract%3A%20%20%20While%20humans%20intuitively%20manipulate%20garments%20and%20other%20textile%20items%20swiftly%0Aand%20accurately%2C%20it%20is%20a%20significant%20challenge%20for%20robots.%20A%20factor%20crucial%20to%0Ahuman%20performance%20is%20the%20ability%20to%20imagine%2C%20a%20priori%2C%20the%20intended%20result%20of%0Athe%20manipulation%20intents%20and%20hence%20develop%20predictions%20on%20the%20garment%20pose.%0AThat%20ability%20allows%20us%20to%20plan%20from%20highly%20obstructed%20states%2C%20adapt%20our%20plans%0Aas%20we%20collect%20more%20information%20and%20react%20swiftly%20to%20unforeseen%20circumstances.%0AConversely%2C%20robots%20struggle%20to%20establish%20such%20intuitions%20and%20form%20tight%20links%0Abetween%20plans%20and%20observations.%20We%20can%20partly%20attribute%20this%20to%20the%20high%20cost%0Aof%20obtaining%20densely%20labelled%20data%20for%20textile%20manipulation%2C%20both%20in%20quality%0Aand%20quantity.%20The%20problem%20of%20data%20collection%20is%20a%20long-standing%20issue%20in%0Adata-based%20approaches%20to%20garment%20manipulation.%20As%20of%20today%2C%20generating%0Ahigh-quality%20and%20labelled%20garment%20manipulation%20data%20is%20mainly%20attempted%20through%0Aadvanced%20data%20capture%20procedures%20that%20create%20simplified%20state%20estimations%20from%0Areal-world%20observations.%20However%2C%20this%20work%20proposes%20a%20novel%20approach%20to%20the%0Aproblem%20by%20generating%20real-world%20observations%20from%20object%20states.%20To%20achieve%0Athis%2C%20we%20present%20GARField%20%28Garment%20Attached%20Radiance%20Field%29%2C%20the%20first%0Adifferentiable%20rendering%20architecture%2C%20to%20our%20knowledge%2C%20for%20data%20generation%0Afrom%20simulated%20states%20stored%20as%20triangle%20meshes.%20Code%20is%20available%20on%0Ahttps%3A//ddonatien.github.io/garfield-website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGARField%253A%2520Addressing%2520the%2520visual%2520Sim-to-Real%2520gap%2520in%2520garment%2520manipulation%250A%2520%2520with%2520mesh-attached%2520radiance%2520fields%26entry.906535625%3DDonatien%2520Delehelle%2520and%2520Darwin%2520G.%2520Caldwell%2520and%2520Fei%2520Chen%26entry.1292438233%3D%2520%2520While%2520humans%2520intuitively%2520manipulate%2520garments%2520and%2520other%2520textile%2520items%2520swiftly%250Aand%2520accurately%252C%2520it%2520is%2520a%2520significant%2520challenge%2520for%2520robots.%2520A%2520factor%2520crucial%2520to%250Ahuman%2520performance%2520is%2520the%2520ability%2520to%2520imagine%252C%2520a%2520priori%252C%2520the%2520intended%2520result%2520of%250Athe%2520manipulation%2520intents%2520and%2520hence%2520develop%2520predictions%2520on%2520the%2520garment%2520pose.%250AThat%2520ability%2520allows%2520us%2520to%2520plan%2520from%2520highly%2520obstructed%2520states%252C%2520adapt%2520our%2520plans%250Aas%2520we%2520collect%2520more%2520information%2520and%2520react%2520swiftly%2520to%2520unforeseen%2520circumstances.%250AConversely%252C%2520robots%2520struggle%2520to%2520establish%2520such%2520intuitions%2520and%2520form%2520tight%2520links%250Abetween%2520plans%2520and%2520observations.%2520We%2520can%2520partly%2520attribute%2520this%2520to%2520the%2520high%2520cost%250Aof%2520obtaining%2520densely%2520labelled%2520data%2520for%2520textile%2520manipulation%252C%2520both%2520in%2520quality%250Aand%2520quantity.%2520The%2520problem%2520of%2520data%2520collection%2520is%2520a%2520long-standing%2520issue%2520in%250Adata-based%2520approaches%2520to%2520garment%2520manipulation.%2520As%2520of%2520today%252C%2520generating%250Ahigh-quality%2520and%2520labelled%2520garment%2520manipulation%2520data%2520is%2520mainly%2520attempted%2520through%250Aadvanced%2520data%2520capture%2520procedures%2520that%2520create%2520simplified%2520state%2520estimations%2520from%250Areal-world%2520observations.%2520However%252C%2520this%2520work%2520proposes%2520a%2520novel%2520approach%2520to%2520the%250Aproblem%2520by%2520generating%2520real-world%2520observations%2520from%2520object%2520states.%2520To%2520achieve%250Athis%252C%2520we%2520present%2520GARField%2520%2528Garment%2520Attached%2520Radiance%2520Field%2529%252C%2520the%2520first%250Adifferentiable%2520rendering%2520architecture%252C%2520to%2520our%2520knowledge%252C%2520for%2520data%2520generation%250Afrom%2520simulated%2520states%2520stored%2520as%2520triangle%2520meshes.%2520Code%2520is%2520available%2520on%250Ahttps%253A//ddonatien.github.io/garfield-website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields&entry.906535625=Donatien%20Delehelle%20and%20Darwin%20G.%20Caldwell%20and%20Fei%20Chen&entry.1292438233=%20%20While%20humans%20intuitively%20manipulate%20garments%20and%20other%20textile%20items%20swiftly%0Aand%20accurately%2C%20it%20is%20a%20significant%20challenge%20for%20robots.%20A%20factor%20crucial%20to%0Ahuman%20performance%20is%20the%20ability%20to%20imagine%2C%20a%20priori%2C%20the%20intended%20result%20of%0Athe%20manipulation%20intents%20and%20hence%20develop%20predictions%20on%20the%20garment%20pose.%0AThat%20ability%20allows%20us%20to%20plan%20from%20highly%20obstructed%20states%2C%20adapt%20our%20plans%0Aas%20we%20collect%20more%20information%20and%20react%20swiftly%20to%20unforeseen%20circumstances.%0AConversely%2C%20robots%20struggle%20to%20establish%20such%20intuitions%20and%20form%20tight%20links%0Abetween%20plans%20and%20observations.%20We%20can%20partly%20attribute%20this%20to%20the%20high%20cost%0Aof%20obtaining%20densely%20labelled%20data%20for%20textile%20manipulation%2C%20both%20in%20quality%0Aand%20quantity.%20The%20problem%20of%20data%20collection%20is%20a%20long-standing%20issue%20in%0Adata-based%20approaches%20to%20garment%20manipulation.%20As%20of%20today%2C%20generating%0Ahigh-quality%20and%20labelled%20garment%20manipulation%20data%20is%20mainly%20attempted%20through%0Aadvanced%20data%20capture%20procedures%20that%20create%20simplified%20state%20estimations%20from%0Areal-world%20observations.%20However%2C%20this%20work%20proposes%20a%20novel%20approach%20to%20the%0Aproblem%20by%20generating%20real-world%20observations%20from%20object%20states.%20To%20achieve%0Athis%2C%20we%20present%20GARField%20%28Garment%20Attached%20Radiance%20Field%29%2C%20the%20first%0Adifferentiable%20rendering%20architecture%2C%20to%20our%20knowledge%2C%20for%20data%20generation%0Afrom%20simulated%20states%20stored%20as%20triangle%20meshes.%20Code%20is%20available%20on%0Ahttps%3A//ddonatien.github.io/garfield-website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05038v2&entry.124074799=Read"},
{"title": "Hard-Negative Sampling for Contrastive Learning: Optimal Representation\n  Geometry and Neural- vs Dimensional-Collapse", "author": "Ruijie Jiang and Thuan Nguyen and Shuchin Aeron and Prakash Ishwar", "abstract": "  For a widely-studied data model and general loss and sample-hardening\nfunctions we prove that the losses of Supervised Contrastive Learning (SCL),\nHard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by\nrepresentations that exhibit Neural-Collapse (NC), i.e., the class means form\nan Equiangular Tight Frame (ETF) and data from the same class are mapped to the\nsame representation. We also prove that for any representation mapping, the\nHSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and\nUCL losses. In contrast to existing literature, our theoretical results for SCL\ndo not require class-conditional independence of augmented views and work for a\ngeneral loss function class that includes the widely used InfoNCE loss\nfunction. Moreover, our proofs are simpler, compact, and transparent. Similar\nto existing literature, our theoretical claims also hold for the practical\nscenario where batching is used for optimization. We empirically demonstrate,\nfor the first time, that Adam optimization (with batching) of HSCL and HUCL\nlosses with random initialization and suitable hardness levels can indeed\nconverge to the NC-geometry if we incorporate unit-ball or unit-sphere feature\nnormalization. Without incorporating hard-negatives or feature normalization,\nhowever, the representations learned via Adam suffer from Dimensional-Collapse\n(DC) and fail to attain the NC-geometry. These results exemplify the role of\nhard-negative sampling in contrastive representation learning and we conclude\nwith several open theoretical problems for future work. The code can be found\nat \\url{https://github.com/rjiang03/HCL/tree/main}\n", "link": "http://arxiv.org/abs/2311.05139v2", "date": "2024-10-29", "relevancy": 2.6404, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hard-Negative%20Sampling%20for%20Contrastive%20Learning%3A%20Optimal%20Representation%0A%20%20Geometry%20and%20Neural-%20vs%20Dimensional-Collapse&body=Title%3A%20Hard-Negative%20Sampling%20for%20Contrastive%20Learning%3A%20Optimal%20Representation%0A%20%20Geometry%20and%20Neural-%20vs%20Dimensional-Collapse%0AAuthor%3A%20Ruijie%20Jiang%20and%20Thuan%20Nguyen%20and%20Shuchin%20Aeron%20and%20Prakash%20Ishwar%0AAbstract%3A%20%20%20For%20a%20widely-studied%20data%20model%20and%20general%20loss%20and%20sample-hardening%0Afunctions%20we%20prove%20that%20the%20losses%20of%20Supervised%20Contrastive%20Learning%20%28SCL%29%2C%0AHard-SCL%20%28HSCL%29%2C%20and%20Unsupervised%20Contrastive%20Learning%20%28UCL%29%20are%20minimized%20by%0Arepresentations%20that%20exhibit%20Neural-Collapse%20%28NC%29%2C%20i.e.%2C%20the%20class%20means%20form%0Aan%20Equiangular%20Tight%20Frame%20%28ETF%29%20and%20data%20from%20the%20same%20class%20are%20mapped%20to%20the%0Asame%20representation.%20We%20also%20prove%20that%20for%20any%20representation%20mapping%2C%20the%0AHSCL%20and%20Hard-UCL%20%28HUCL%29%20losses%20are%20lower%20bounded%20by%20the%20corresponding%20SCL%20and%0AUCL%20losses.%20In%20contrast%20to%20existing%20literature%2C%20our%20theoretical%20results%20for%20SCL%0Ado%20not%20require%20class-conditional%20independence%20of%20augmented%20views%20and%20work%20for%20a%0Ageneral%20loss%20function%20class%20that%20includes%20the%20widely%20used%20InfoNCE%20loss%0Afunction.%20Moreover%2C%20our%20proofs%20are%20simpler%2C%20compact%2C%20and%20transparent.%20Similar%0Ato%20existing%20literature%2C%20our%20theoretical%20claims%20also%20hold%20for%20the%20practical%0Ascenario%20where%20batching%20is%20used%20for%20optimization.%20We%20empirically%20demonstrate%2C%0Afor%20the%20first%20time%2C%20that%20Adam%20optimization%20%28with%20batching%29%20of%20HSCL%20and%20HUCL%0Alosses%20with%20random%20initialization%20and%20suitable%20hardness%20levels%20can%20indeed%0Aconverge%20to%20the%20NC-geometry%20if%20we%20incorporate%20unit-ball%20or%20unit-sphere%20feature%0Anormalization.%20Without%20incorporating%20hard-negatives%20or%20feature%20normalization%2C%0Ahowever%2C%20the%20representations%20learned%20via%20Adam%20suffer%20from%20Dimensional-Collapse%0A%28DC%29%20and%20fail%20to%20attain%20the%20NC-geometry.%20These%20results%20exemplify%20the%20role%20of%0Ahard-negative%20sampling%20in%20contrastive%20representation%20learning%20and%20we%20conclude%0Awith%20several%20open%20theoretical%20problems%20for%20future%20work.%20The%20code%20can%20be%20found%0Aat%20%5Curl%7Bhttps%3A//github.com/rjiang03/HCL/tree/main%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHard-Negative%2520Sampling%2520for%2520Contrastive%2520Learning%253A%2520Optimal%2520Representation%250A%2520%2520Geometry%2520and%2520Neural-%2520vs%2520Dimensional-Collapse%26entry.906535625%3DRuijie%2520Jiang%2520and%2520Thuan%2520Nguyen%2520and%2520Shuchin%2520Aeron%2520and%2520Prakash%2520Ishwar%26entry.1292438233%3D%2520%2520For%2520a%2520widely-studied%2520data%2520model%2520and%2520general%2520loss%2520and%2520sample-hardening%250Afunctions%2520we%2520prove%2520that%2520the%2520losses%2520of%2520Supervised%2520Contrastive%2520Learning%2520%2528SCL%2529%252C%250AHard-SCL%2520%2528HSCL%2529%252C%2520and%2520Unsupervised%2520Contrastive%2520Learning%2520%2528UCL%2529%2520are%2520minimized%2520by%250Arepresentations%2520that%2520exhibit%2520Neural-Collapse%2520%2528NC%2529%252C%2520i.e.%252C%2520the%2520class%2520means%2520form%250Aan%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520and%2520data%2520from%2520the%2520same%2520class%2520are%2520mapped%2520to%2520the%250Asame%2520representation.%2520We%2520also%2520prove%2520that%2520for%2520any%2520representation%2520mapping%252C%2520the%250AHSCL%2520and%2520Hard-UCL%2520%2528HUCL%2529%2520losses%2520are%2520lower%2520bounded%2520by%2520the%2520corresponding%2520SCL%2520and%250AUCL%2520losses.%2520In%2520contrast%2520to%2520existing%2520literature%252C%2520our%2520theoretical%2520results%2520for%2520SCL%250Ado%2520not%2520require%2520class-conditional%2520independence%2520of%2520augmented%2520views%2520and%2520work%2520for%2520a%250Ageneral%2520loss%2520function%2520class%2520that%2520includes%2520the%2520widely%2520used%2520InfoNCE%2520loss%250Afunction.%2520Moreover%252C%2520our%2520proofs%2520are%2520simpler%252C%2520compact%252C%2520and%2520transparent.%2520Similar%250Ato%2520existing%2520literature%252C%2520our%2520theoretical%2520claims%2520also%2520hold%2520for%2520the%2520practical%250Ascenario%2520where%2520batching%2520is%2520used%2520for%2520optimization.%2520We%2520empirically%2520demonstrate%252C%250Afor%2520the%2520first%2520time%252C%2520that%2520Adam%2520optimization%2520%2528with%2520batching%2529%2520of%2520HSCL%2520and%2520HUCL%250Alosses%2520with%2520random%2520initialization%2520and%2520suitable%2520hardness%2520levels%2520can%2520indeed%250Aconverge%2520to%2520the%2520NC-geometry%2520if%2520we%2520incorporate%2520unit-ball%2520or%2520unit-sphere%2520feature%250Anormalization.%2520Without%2520incorporating%2520hard-negatives%2520or%2520feature%2520normalization%252C%250Ahowever%252C%2520the%2520representations%2520learned%2520via%2520Adam%2520suffer%2520from%2520Dimensional-Collapse%250A%2528DC%2529%2520and%2520fail%2520to%2520attain%2520the%2520NC-geometry.%2520These%2520results%2520exemplify%2520the%2520role%2520of%250Ahard-negative%2520sampling%2520in%2520contrastive%2520representation%2520learning%2520and%2520we%2520conclude%250Awith%2520several%2520open%2520theoretical%2520problems%2520for%2520future%2520work.%2520The%2520code%2520can%2520be%2520found%250Aat%2520%255Curl%257Bhttps%253A//github.com/rjiang03/HCL/tree/main%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hard-Negative%20Sampling%20for%20Contrastive%20Learning%3A%20Optimal%20Representation%0A%20%20Geometry%20and%20Neural-%20vs%20Dimensional-Collapse&entry.906535625=Ruijie%20Jiang%20and%20Thuan%20Nguyen%20and%20Shuchin%20Aeron%20and%20Prakash%20Ishwar&entry.1292438233=%20%20For%20a%20widely-studied%20data%20model%20and%20general%20loss%20and%20sample-hardening%0Afunctions%20we%20prove%20that%20the%20losses%20of%20Supervised%20Contrastive%20Learning%20%28SCL%29%2C%0AHard-SCL%20%28HSCL%29%2C%20and%20Unsupervised%20Contrastive%20Learning%20%28UCL%29%20are%20minimized%20by%0Arepresentations%20that%20exhibit%20Neural-Collapse%20%28NC%29%2C%20i.e.%2C%20the%20class%20means%20form%0Aan%20Equiangular%20Tight%20Frame%20%28ETF%29%20and%20data%20from%20the%20same%20class%20are%20mapped%20to%20the%0Asame%20representation.%20We%20also%20prove%20that%20for%20any%20representation%20mapping%2C%20the%0AHSCL%20and%20Hard-UCL%20%28HUCL%29%20losses%20are%20lower%20bounded%20by%20the%20corresponding%20SCL%20and%0AUCL%20losses.%20In%20contrast%20to%20existing%20literature%2C%20our%20theoretical%20results%20for%20SCL%0Ado%20not%20require%20class-conditional%20independence%20of%20augmented%20views%20and%20work%20for%20a%0Ageneral%20loss%20function%20class%20that%20includes%20the%20widely%20used%20InfoNCE%20loss%0Afunction.%20Moreover%2C%20our%20proofs%20are%20simpler%2C%20compact%2C%20and%20transparent.%20Similar%0Ato%20existing%20literature%2C%20our%20theoretical%20claims%20also%20hold%20for%20the%20practical%0Ascenario%20where%20batching%20is%20used%20for%20optimization.%20We%20empirically%20demonstrate%2C%0Afor%20the%20first%20time%2C%20that%20Adam%20optimization%20%28with%20batching%29%20of%20HSCL%20and%20HUCL%0Alosses%20with%20random%20initialization%20and%20suitable%20hardness%20levels%20can%20indeed%0Aconverge%20to%20the%20NC-geometry%20if%20we%20incorporate%20unit-ball%20or%20unit-sphere%20feature%0Anormalization.%20Without%20incorporating%20hard-negatives%20or%20feature%20normalization%2C%0Ahowever%2C%20the%20representations%20learned%20via%20Adam%20suffer%20from%20Dimensional-Collapse%0A%28DC%29%20and%20fail%20to%20attain%20the%20NC-geometry.%20These%20results%20exemplify%20the%20role%20of%0Ahard-negative%20sampling%20in%20contrastive%20representation%20learning%20and%20we%20conclude%0Awith%20several%20open%20theoretical%20problems%20for%20future%20work.%20The%20code%20can%20be%20found%0Aat%20%5Curl%7Bhttps%3A//github.com/rjiang03/HCL/tree/main%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05139v2&entry.124074799=Read"},
{"title": "ReMix: Training Generalized Person Re-identification on a Mixture of\n  Data", "author": "Timur Mamedov and Anton Konushin and Vadim Konushin", "abstract": "  Modern person re-identification (Re-ID) methods have a weak generalization\nability and experience a major accuracy drop when capturing environments\nchange. This is because existing multi-camera Re-ID datasets are limited in\nsize and diversity, since such data is difficult to obtain. At the same time,\nenormous volumes of unlabeled single-camera records are available. Such data\ncan be easily collected, and therefore, it is more diverse. Currently,\nsingle-camera data is used only for self-supervised pre-training of Re-ID\nmethods. However, the diversity of single-camera data is suppressed by\nfine-tuning on limited multi-camera data after pre-training. In this paper, we\npropose ReMix, a generalized Re-ID method jointly trained on a mixture of\nlimited labeled multi-camera and large unlabeled single-camera data. Effective\ntraining of our method is achieved through a novel data sampling strategy and\nnew loss functions that are adapted for joint use with both types of data.\nExperiments show that ReMix has a high generalization ability and outperforms\nstate-of-the-art methods in generalizable person Re-ID. To the best of our\nknowledge, this is the first work that explores joint training on a mixture of\nmulti-camera and single-camera data in person Re-ID.\n", "link": "http://arxiv.org/abs/2410.21938v1", "date": "2024-10-29", "relevancy": 2.6224, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReMix%3A%20Training%20Generalized%20Person%20Re-identification%20on%20a%20Mixture%20of%0A%20%20Data&body=Title%3A%20ReMix%3A%20Training%20Generalized%20Person%20Re-identification%20on%20a%20Mixture%20of%0A%20%20Data%0AAuthor%3A%20Timur%20Mamedov%20and%20Anton%20Konushin%20and%20Vadim%20Konushin%0AAbstract%3A%20%20%20Modern%20person%20re-identification%20%28Re-ID%29%20methods%20have%20a%20weak%20generalization%0Aability%20and%20experience%20a%20major%20accuracy%20drop%20when%20capturing%20environments%0Achange.%20This%20is%20because%20existing%20multi-camera%20Re-ID%20datasets%20are%20limited%20in%0Asize%20and%20diversity%2C%20since%20such%20data%20is%20difficult%20to%20obtain.%20At%20the%20same%20time%2C%0Aenormous%20volumes%20of%20unlabeled%20single-camera%20records%20are%20available.%20Such%20data%0Acan%20be%20easily%20collected%2C%20and%20therefore%2C%20it%20is%20more%20diverse.%20Currently%2C%0Asingle-camera%20data%20is%20used%20only%20for%20self-supervised%20pre-training%20of%20Re-ID%0Amethods.%20However%2C%20the%20diversity%20of%20single-camera%20data%20is%20suppressed%20by%0Afine-tuning%20on%20limited%20multi-camera%20data%20after%20pre-training.%20In%20this%20paper%2C%20we%0Apropose%20ReMix%2C%20a%20generalized%20Re-ID%20method%20jointly%20trained%20on%20a%20mixture%20of%0Alimited%20labeled%20multi-camera%20and%20large%20unlabeled%20single-camera%20data.%20Effective%0Atraining%20of%20our%20method%20is%20achieved%20through%20a%20novel%20data%20sampling%20strategy%20and%0Anew%20loss%20functions%20that%20are%20adapted%20for%20joint%20use%20with%20both%20types%20of%20data.%0AExperiments%20show%20that%20ReMix%20has%20a%20high%20generalization%20ability%20and%20outperforms%0Astate-of-the-art%20methods%20in%20generalizable%20person%20Re-ID.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20that%20explores%20joint%20training%20on%20a%20mixture%20of%0Amulti-camera%20and%20single-camera%20data%20in%20person%20Re-ID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReMix%253A%2520Training%2520Generalized%2520Person%2520Re-identification%2520on%2520a%2520Mixture%2520of%250A%2520%2520Data%26entry.906535625%3DTimur%2520Mamedov%2520and%2520Anton%2520Konushin%2520and%2520Vadim%2520Konushin%26entry.1292438233%3D%2520%2520Modern%2520person%2520re-identification%2520%2528Re-ID%2529%2520methods%2520have%2520a%2520weak%2520generalization%250Aability%2520and%2520experience%2520a%2520major%2520accuracy%2520drop%2520when%2520capturing%2520environments%250Achange.%2520This%2520is%2520because%2520existing%2520multi-camera%2520Re-ID%2520datasets%2520are%2520limited%2520in%250Asize%2520and%2520diversity%252C%2520since%2520such%2520data%2520is%2520difficult%2520to%2520obtain.%2520At%2520the%2520same%2520time%252C%250Aenormous%2520volumes%2520of%2520unlabeled%2520single-camera%2520records%2520are%2520available.%2520Such%2520data%250Acan%2520be%2520easily%2520collected%252C%2520and%2520therefore%252C%2520it%2520is%2520more%2520diverse.%2520Currently%252C%250Asingle-camera%2520data%2520is%2520used%2520only%2520for%2520self-supervised%2520pre-training%2520of%2520Re-ID%250Amethods.%2520However%252C%2520the%2520diversity%2520of%2520single-camera%2520data%2520is%2520suppressed%2520by%250Afine-tuning%2520on%2520limited%2520multi-camera%2520data%2520after%2520pre-training.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520ReMix%252C%2520a%2520generalized%2520Re-ID%2520method%2520jointly%2520trained%2520on%2520a%2520mixture%2520of%250Alimited%2520labeled%2520multi-camera%2520and%2520large%2520unlabeled%2520single-camera%2520data.%2520Effective%250Atraining%2520of%2520our%2520method%2520is%2520achieved%2520through%2520a%2520novel%2520data%2520sampling%2520strategy%2520and%250Anew%2520loss%2520functions%2520that%2520are%2520adapted%2520for%2520joint%2520use%2520with%2520both%2520types%2520of%2520data.%250AExperiments%2520show%2520that%2520ReMix%2520has%2520a%2520high%2520generalization%2520ability%2520and%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520generalizable%2520person%2520Re-ID.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520that%2520explores%2520joint%2520training%2520on%2520a%2520mixture%2520of%250Amulti-camera%2520and%2520single-camera%2520data%2520in%2520person%2520Re-ID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReMix%3A%20Training%20Generalized%20Person%20Re-identification%20on%20a%20Mixture%20of%0A%20%20Data&entry.906535625=Timur%20Mamedov%20and%20Anton%20Konushin%20and%20Vadim%20Konushin&entry.1292438233=%20%20Modern%20person%20re-identification%20%28Re-ID%29%20methods%20have%20a%20weak%20generalization%0Aability%20and%20experience%20a%20major%20accuracy%20drop%20when%20capturing%20environments%0Achange.%20This%20is%20because%20existing%20multi-camera%20Re-ID%20datasets%20are%20limited%20in%0Asize%20and%20diversity%2C%20since%20such%20data%20is%20difficult%20to%20obtain.%20At%20the%20same%20time%2C%0Aenormous%20volumes%20of%20unlabeled%20single-camera%20records%20are%20available.%20Such%20data%0Acan%20be%20easily%20collected%2C%20and%20therefore%2C%20it%20is%20more%20diverse.%20Currently%2C%0Asingle-camera%20data%20is%20used%20only%20for%20self-supervised%20pre-training%20of%20Re-ID%0Amethods.%20However%2C%20the%20diversity%20of%20single-camera%20data%20is%20suppressed%20by%0Afine-tuning%20on%20limited%20multi-camera%20data%20after%20pre-training.%20In%20this%20paper%2C%20we%0Apropose%20ReMix%2C%20a%20generalized%20Re-ID%20method%20jointly%20trained%20on%20a%20mixture%20of%0Alimited%20labeled%20multi-camera%20and%20large%20unlabeled%20single-camera%20data.%20Effective%0Atraining%20of%20our%20method%20is%20achieved%20through%20a%20novel%20data%20sampling%20strategy%20and%0Anew%20loss%20functions%20that%20are%20adapted%20for%20joint%20use%20with%20both%20types%20of%20data.%0AExperiments%20show%20that%20ReMix%20has%20a%20high%20generalization%20ability%20and%20outperforms%0Astate-of-the-art%20methods%20in%20generalizable%20person%20Re-ID.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20that%20explores%20joint%20training%20on%20a%20mixture%20of%0Amulti-camera%20and%20single-camera%20data%20in%20person%20Re-ID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21938v1&entry.124074799=Read"},
{"title": "The Teenager's Problem: Efficient Garment Decluttering as Probabilistic\n  Set Cover", "author": "Aviv Adler and Ayah Ahmad and Yulei Qiu and Shengyin Wang and Wisdom C. Agboh and Edith Llontop and Tianshuang Qiu and Jeffrey Ichnowski and Thomas Kollar and Richard Cheng and Mehmet Dogar and Ken Goldberg", "abstract": "  This paper addresses the \"Teenager's Problem\": efficiently removing scattered\ngarments from a planar surface into a basket. As grasping and transporting\nindividual garments is highly inefficient, we propose policies to select grasp\nlocations for multiple garments using an overhead camera. Our core approach is\nsegment-based, which uses segmentation on the overhead RGB image of the scene.\nWe propose a Probabilistic Set Cover formulation of the problem, aiming to\nminimize the number of grasps that clear all garments off the surface. Grasp\nefficiency is measured by Objects per Transport (OpT), which denotes the\naverage number of objects removed per trip to the laundry basket. Additionally,\nwe explore several depth-based methods, which use overhead depth data to find\nefficient grasps. Experiments suggest that our segment-based method increases\nOpT by $50\\%$ over a random baseline, whereas combined hybrid methods yield\nimprovements of $33\\%$. Finally, a method employing consolidation (with\nsegmentation) is considered, which locally moves the garments on the work\nsurface to increase OpT, when the distance to the basket is much greater than\nthe local motion distances. This yields an improvement of $81\\%$ over the\nbaseline.\n", "link": "http://arxiv.org/abs/2310.16951v3", "date": "2024-10-29", "relevancy": 2.6103, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.541}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5234}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Teenager%27s%20Problem%3A%20Efficient%20Garment%20Decluttering%20as%20Probabilistic%0A%20%20Set%20Cover&body=Title%3A%20The%20Teenager%27s%20Problem%3A%20Efficient%20Garment%20Decluttering%20as%20Probabilistic%0A%20%20Set%20Cover%0AAuthor%3A%20Aviv%20Adler%20and%20Ayah%20Ahmad%20and%20Yulei%20Qiu%20and%20Shengyin%20Wang%20and%20Wisdom%20C.%20Agboh%20and%20Edith%20Llontop%20and%20Tianshuang%20Qiu%20and%20Jeffrey%20Ichnowski%20and%20Thomas%20Kollar%20and%20Richard%20Cheng%20and%20Mehmet%20Dogar%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20%22Teenager%27s%20Problem%22%3A%20efficiently%20removing%20scattered%0Agarments%20from%20a%20planar%20surface%20into%20a%20basket.%20As%20grasping%20and%20transporting%0Aindividual%20garments%20is%20highly%20inefficient%2C%20we%20propose%20policies%20to%20select%20grasp%0Alocations%20for%20multiple%20garments%20using%20an%20overhead%20camera.%20Our%20core%20approach%20is%0Asegment-based%2C%20which%20uses%20segmentation%20on%20the%20overhead%20RGB%20image%20of%20the%20scene.%0AWe%20propose%20a%20Probabilistic%20Set%20Cover%20formulation%20of%20the%20problem%2C%20aiming%20to%0Aminimize%20the%20number%20of%20grasps%20that%20clear%20all%20garments%20off%20the%20surface.%20Grasp%0Aefficiency%20is%20measured%20by%20Objects%20per%20Transport%20%28OpT%29%2C%20which%20denotes%20the%0Aaverage%20number%20of%20objects%20removed%20per%20trip%20to%20the%20laundry%20basket.%20Additionally%2C%0Awe%20explore%20several%20depth-based%20methods%2C%20which%20use%20overhead%20depth%20data%20to%20find%0Aefficient%20grasps.%20Experiments%20suggest%20that%20our%20segment-based%20method%20increases%0AOpT%20by%20%2450%5C%25%24%20over%20a%20random%20baseline%2C%20whereas%20combined%20hybrid%20methods%20yield%0Aimprovements%20of%20%2433%5C%25%24.%20Finally%2C%20a%20method%20employing%20consolidation%20%28with%0Asegmentation%29%20is%20considered%2C%20which%20locally%20moves%20the%20garments%20on%20the%20work%0Asurface%20to%20increase%20OpT%2C%20when%20the%20distance%20to%20the%20basket%20is%20much%20greater%20than%0Athe%20local%20motion%20distances.%20This%20yields%20an%20improvement%20of%20%2481%5C%25%24%20over%20the%0Abaseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16951v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Teenager%2527s%2520Problem%253A%2520Efficient%2520Garment%2520Decluttering%2520as%2520Probabilistic%250A%2520%2520Set%2520Cover%26entry.906535625%3DAviv%2520Adler%2520and%2520Ayah%2520Ahmad%2520and%2520Yulei%2520Qiu%2520and%2520Shengyin%2520Wang%2520and%2520Wisdom%2520C.%2520Agboh%2520and%2520Edith%2520Llontop%2520and%2520Tianshuang%2520Qiu%2520and%2520Jeffrey%2520Ichnowski%2520and%2520Thomas%2520Kollar%2520and%2520Richard%2520Cheng%2520and%2520Mehmet%2520Dogar%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520%2522Teenager%2527s%2520Problem%2522%253A%2520efficiently%2520removing%2520scattered%250Agarments%2520from%2520a%2520planar%2520surface%2520into%2520a%2520basket.%2520As%2520grasping%2520and%2520transporting%250Aindividual%2520garments%2520is%2520highly%2520inefficient%252C%2520we%2520propose%2520policies%2520to%2520select%2520grasp%250Alocations%2520for%2520multiple%2520garments%2520using%2520an%2520overhead%2520camera.%2520Our%2520core%2520approach%2520is%250Asegment-based%252C%2520which%2520uses%2520segmentation%2520on%2520the%2520overhead%2520RGB%2520image%2520of%2520the%2520scene.%250AWe%2520propose%2520a%2520Probabilistic%2520Set%2520Cover%2520formulation%2520of%2520the%2520problem%252C%2520aiming%2520to%250Aminimize%2520the%2520number%2520of%2520grasps%2520that%2520clear%2520all%2520garments%2520off%2520the%2520surface.%2520Grasp%250Aefficiency%2520is%2520measured%2520by%2520Objects%2520per%2520Transport%2520%2528OpT%2529%252C%2520which%2520denotes%2520the%250Aaverage%2520number%2520of%2520objects%2520removed%2520per%2520trip%2520to%2520the%2520laundry%2520basket.%2520Additionally%252C%250Awe%2520explore%2520several%2520depth-based%2520methods%252C%2520which%2520use%2520overhead%2520depth%2520data%2520to%2520find%250Aefficient%2520grasps.%2520Experiments%2520suggest%2520that%2520our%2520segment-based%2520method%2520increases%250AOpT%2520by%2520%252450%255C%2525%2524%2520over%2520a%2520random%2520baseline%252C%2520whereas%2520combined%2520hybrid%2520methods%2520yield%250Aimprovements%2520of%2520%252433%255C%2525%2524.%2520Finally%252C%2520a%2520method%2520employing%2520consolidation%2520%2528with%250Asegmentation%2529%2520is%2520considered%252C%2520which%2520locally%2520moves%2520the%2520garments%2520on%2520the%2520work%250Asurface%2520to%2520increase%2520OpT%252C%2520when%2520the%2520distance%2520to%2520the%2520basket%2520is%2520much%2520greater%2520than%250Athe%2520local%2520motion%2520distances.%2520This%2520yields%2520an%2520improvement%2520of%2520%252481%255C%2525%2524%2520over%2520the%250Abaseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16951v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Teenager%27s%20Problem%3A%20Efficient%20Garment%20Decluttering%20as%20Probabilistic%0A%20%20Set%20Cover&entry.906535625=Aviv%20Adler%20and%20Ayah%20Ahmad%20and%20Yulei%20Qiu%20and%20Shengyin%20Wang%20and%20Wisdom%20C.%20Agboh%20and%20Edith%20Llontop%20and%20Tianshuang%20Qiu%20and%20Jeffrey%20Ichnowski%20and%20Thomas%20Kollar%20and%20Richard%20Cheng%20and%20Mehmet%20Dogar%20and%20Ken%20Goldberg&entry.1292438233=%20%20This%20paper%20addresses%20the%20%22Teenager%27s%20Problem%22%3A%20efficiently%20removing%20scattered%0Agarments%20from%20a%20planar%20surface%20into%20a%20basket.%20As%20grasping%20and%20transporting%0Aindividual%20garments%20is%20highly%20inefficient%2C%20we%20propose%20policies%20to%20select%20grasp%0Alocations%20for%20multiple%20garments%20using%20an%20overhead%20camera.%20Our%20core%20approach%20is%0Asegment-based%2C%20which%20uses%20segmentation%20on%20the%20overhead%20RGB%20image%20of%20the%20scene.%0AWe%20propose%20a%20Probabilistic%20Set%20Cover%20formulation%20of%20the%20problem%2C%20aiming%20to%0Aminimize%20the%20number%20of%20grasps%20that%20clear%20all%20garments%20off%20the%20surface.%20Grasp%0Aefficiency%20is%20measured%20by%20Objects%20per%20Transport%20%28OpT%29%2C%20which%20denotes%20the%0Aaverage%20number%20of%20objects%20removed%20per%20trip%20to%20the%20laundry%20basket.%20Additionally%2C%0Awe%20explore%20several%20depth-based%20methods%2C%20which%20use%20overhead%20depth%20data%20to%20find%0Aefficient%20grasps.%20Experiments%20suggest%20that%20our%20segment-based%20method%20increases%0AOpT%20by%20%2450%5C%25%24%20over%20a%20random%20baseline%2C%20whereas%20combined%20hybrid%20methods%20yield%0Aimprovements%20of%20%2433%5C%25%24.%20Finally%2C%20a%20method%20employing%20consolidation%20%28with%0Asegmentation%29%20is%20considered%2C%20which%20locally%20moves%20the%20garments%20on%20the%20work%0Asurface%20to%20increase%20OpT%2C%20when%20the%20distance%20to%20the%20basket%20is%20much%20greater%20than%0Athe%20local%20motion%20distances.%20This%20yields%20an%20improvement%20of%20%2481%5C%25%24%20over%20the%0Abaseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16951v3&entry.124074799=Read"},
{"title": "InLINE: Inner-Layer Information Exchange for Multi-task Learning on\n  Heterogeneous Graphs", "author": "Xinyue Feng and Jinquan Hang and Yuequn Zhang and Haotian Wang and Desheng Zhang and Guang Wang", "abstract": "  Heterogeneous graph is an important structure for modeling complex relational\ndata in real-world scenarios and usually involves various node prediction tasks\nwithin a single graph. Training these tasks separately may neglect beneficial\ninformation sharing, hence a preferred way is to learn several tasks in a same\nmodel by Multi-Task Learning (MTL). However, MTL introduces the issue of\nnegative transfer, where the training of different tasks interferes with each\nother as they may focus on different information from the data, resulting in\nsuboptimal performance. To solve the issue, existing MTL methods use separate\nbackbones for each task, then selectively exchange beneficial features through\ninteractions among the output embeddings from each layer of different\nbackbones, which we refer to as outer-layer exchange. However, the negative\ntransfer in heterogeneous graphs arises not simply from the varying importance\nof an individual node feature across tasks, but also from the varying\nimportance of inter-relation between two nodes across tasks. These\ninter-relations are entangled in the output embedding, making it difficult for\nexisting methods to discriminate beneficial information from the embedding. To\naddress this challenge, we propose the Inner-Layer Information Exchange\n(InLINE) model that facilitate fine-grained information exchanges within each\ngraph layer rather than through output embeddings. Specifically, InLINE\nconsists of (1) Structure Disentangled Experts for layer-wise structure\ndisentanglement, (2) Structure Disentangled Gates for assigning disentangled\ninformation to different tasks. Evaluations on two public datasets and a large\nindustry dataset show that our model effectively alleviates the significant\nperformance drop on specific tasks caused by negative transfer, improving Macro\nF1 by 6.3% on DBLP dataset and AUC by 3.6% on the industry dataset compared to\nSoA methods.\n", "link": "http://arxiv.org/abs/2410.22089v1", "date": "2024-10-29", "relevancy": 2.5945, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InLINE%3A%20Inner-Layer%20Information%20Exchange%20for%20Multi-task%20Learning%20on%0A%20%20Heterogeneous%20Graphs&body=Title%3A%20InLINE%3A%20Inner-Layer%20Information%20Exchange%20for%20Multi-task%20Learning%20on%0A%20%20Heterogeneous%20Graphs%0AAuthor%3A%20Xinyue%20Feng%20and%20Jinquan%20Hang%20and%20Yuequn%20Zhang%20and%20Haotian%20Wang%20and%20Desheng%20Zhang%20and%20Guang%20Wang%0AAbstract%3A%20%20%20Heterogeneous%20graph%20is%20an%20important%20structure%20for%20modeling%20complex%20relational%0Adata%20in%20real-world%20scenarios%20and%20usually%20involves%20various%20node%20prediction%20tasks%0Awithin%20a%20single%20graph.%20Training%20these%20tasks%20separately%20may%20neglect%20beneficial%0Ainformation%20sharing%2C%20hence%20a%20preferred%20way%20is%20to%20learn%20several%20tasks%20in%20a%20same%0Amodel%20by%20Multi-Task%20Learning%20%28MTL%29.%20However%2C%20MTL%20introduces%20the%20issue%20of%0Anegative%20transfer%2C%20where%20the%20training%20of%20different%20tasks%20interferes%20with%20each%0Aother%20as%20they%20may%20focus%20on%20different%20information%20from%20the%20data%2C%20resulting%20in%0Asuboptimal%20performance.%20To%20solve%20the%20issue%2C%20existing%20MTL%20methods%20use%20separate%0Abackbones%20for%20each%20task%2C%20then%20selectively%20exchange%20beneficial%20features%20through%0Ainteractions%20among%20the%20output%20embeddings%20from%20each%20layer%20of%20different%0Abackbones%2C%20which%20we%20refer%20to%20as%20outer-layer%20exchange.%20However%2C%20the%20negative%0Atransfer%20in%20heterogeneous%20graphs%20arises%20not%20simply%20from%20the%20varying%20importance%0Aof%20an%20individual%20node%20feature%20across%20tasks%2C%20but%20also%20from%20the%20varying%0Aimportance%20of%20inter-relation%20between%20two%20nodes%20across%20tasks.%20These%0Ainter-relations%20are%20entangled%20in%20the%20output%20embedding%2C%20making%20it%20difficult%20for%0Aexisting%20methods%20to%20discriminate%20beneficial%20information%20from%20the%20embedding.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20Inner-Layer%20Information%20Exchange%0A%28InLINE%29%20model%20that%20facilitate%20fine-grained%20information%20exchanges%20within%20each%0Agraph%20layer%20rather%20than%20through%20output%20embeddings.%20Specifically%2C%20InLINE%0Aconsists%20of%20%281%29%20Structure%20Disentangled%20Experts%20for%20layer-wise%20structure%0Adisentanglement%2C%20%282%29%20Structure%20Disentangled%20Gates%20for%20assigning%20disentangled%0Ainformation%20to%20different%20tasks.%20Evaluations%20on%20two%20public%20datasets%20and%20a%20large%0Aindustry%20dataset%20show%20that%20our%20model%20effectively%20alleviates%20the%20significant%0Aperformance%20drop%20on%20specific%20tasks%20caused%20by%20negative%20transfer%2C%20improving%20Macro%0AF1%20by%206.3%25%20on%20DBLP%20dataset%20and%20AUC%20by%203.6%25%20on%20the%20industry%20dataset%20compared%20to%0ASoA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInLINE%253A%2520Inner-Layer%2520Information%2520Exchange%2520for%2520Multi-task%2520Learning%2520on%250A%2520%2520Heterogeneous%2520Graphs%26entry.906535625%3DXinyue%2520Feng%2520and%2520Jinquan%2520Hang%2520and%2520Yuequn%2520Zhang%2520and%2520Haotian%2520Wang%2520and%2520Desheng%2520Zhang%2520and%2520Guang%2520Wang%26entry.1292438233%3D%2520%2520Heterogeneous%2520graph%2520is%2520an%2520important%2520structure%2520for%2520modeling%2520complex%2520relational%250Adata%2520in%2520real-world%2520scenarios%2520and%2520usually%2520involves%2520various%2520node%2520prediction%2520tasks%250Awithin%2520a%2520single%2520graph.%2520Training%2520these%2520tasks%2520separately%2520may%2520neglect%2520beneficial%250Ainformation%2520sharing%252C%2520hence%2520a%2520preferred%2520way%2520is%2520to%2520learn%2520several%2520tasks%2520in%2520a%2520same%250Amodel%2520by%2520Multi-Task%2520Learning%2520%2528MTL%2529.%2520However%252C%2520MTL%2520introduces%2520the%2520issue%2520of%250Anegative%2520transfer%252C%2520where%2520the%2520training%2520of%2520different%2520tasks%2520interferes%2520with%2520each%250Aother%2520as%2520they%2520may%2520focus%2520on%2520different%2520information%2520from%2520the%2520data%252C%2520resulting%2520in%250Asuboptimal%2520performance.%2520To%2520solve%2520the%2520issue%252C%2520existing%2520MTL%2520methods%2520use%2520separate%250Abackbones%2520for%2520each%2520task%252C%2520then%2520selectively%2520exchange%2520beneficial%2520features%2520through%250Ainteractions%2520among%2520the%2520output%2520embeddings%2520from%2520each%2520layer%2520of%2520different%250Abackbones%252C%2520which%2520we%2520refer%2520to%2520as%2520outer-layer%2520exchange.%2520However%252C%2520the%2520negative%250Atransfer%2520in%2520heterogeneous%2520graphs%2520arises%2520not%2520simply%2520from%2520the%2520varying%2520importance%250Aof%2520an%2520individual%2520node%2520feature%2520across%2520tasks%252C%2520but%2520also%2520from%2520the%2520varying%250Aimportance%2520of%2520inter-relation%2520between%2520two%2520nodes%2520across%2520tasks.%2520These%250Ainter-relations%2520are%2520entangled%2520in%2520the%2520output%2520embedding%252C%2520making%2520it%2520difficult%2520for%250Aexisting%2520methods%2520to%2520discriminate%2520beneficial%2520information%2520from%2520the%2520embedding.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Inner-Layer%2520Information%2520Exchange%250A%2528InLINE%2529%2520model%2520that%2520facilitate%2520fine-grained%2520information%2520exchanges%2520within%2520each%250Agraph%2520layer%2520rather%2520than%2520through%2520output%2520embeddings.%2520Specifically%252C%2520InLINE%250Aconsists%2520of%2520%25281%2529%2520Structure%2520Disentangled%2520Experts%2520for%2520layer-wise%2520structure%250Adisentanglement%252C%2520%25282%2529%2520Structure%2520Disentangled%2520Gates%2520for%2520assigning%2520disentangled%250Ainformation%2520to%2520different%2520tasks.%2520Evaluations%2520on%2520two%2520public%2520datasets%2520and%2520a%2520large%250Aindustry%2520dataset%2520show%2520that%2520our%2520model%2520effectively%2520alleviates%2520the%2520significant%250Aperformance%2520drop%2520on%2520specific%2520tasks%2520caused%2520by%2520negative%2520transfer%252C%2520improving%2520Macro%250AF1%2520by%25206.3%2525%2520on%2520DBLP%2520dataset%2520and%2520AUC%2520by%25203.6%2525%2520on%2520the%2520industry%2520dataset%2520compared%2520to%250ASoA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InLINE%3A%20Inner-Layer%20Information%20Exchange%20for%20Multi-task%20Learning%20on%0A%20%20Heterogeneous%20Graphs&entry.906535625=Xinyue%20Feng%20and%20Jinquan%20Hang%20and%20Yuequn%20Zhang%20and%20Haotian%20Wang%20and%20Desheng%20Zhang%20and%20Guang%20Wang&entry.1292438233=%20%20Heterogeneous%20graph%20is%20an%20important%20structure%20for%20modeling%20complex%20relational%0Adata%20in%20real-world%20scenarios%20and%20usually%20involves%20various%20node%20prediction%20tasks%0Awithin%20a%20single%20graph.%20Training%20these%20tasks%20separately%20may%20neglect%20beneficial%0Ainformation%20sharing%2C%20hence%20a%20preferred%20way%20is%20to%20learn%20several%20tasks%20in%20a%20same%0Amodel%20by%20Multi-Task%20Learning%20%28MTL%29.%20However%2C%20MTL%20introduces%20the%20issue%20of%0Anegative%20transfer%2C%20where%20the%20training%20of%20different%20tasks%20interferes%20with%20each%0Aother%20as%20they%20may%20focus%20on%20different%20information%20from%20the%20data%2C%20resulting%20in%0Asuboptimal%20performance.%20To%20solve%20the%20issue%2C%20existing%20MTL%20methods%20use%20separate%0Abackbones%20for%20each%20task%2C%20then%20selectively%20exchange%20beneficial%20features%20through%0Ainteractions%20among%20the%20output%20embeddings%20from%20each%20layer%20of%20different%0Abackbones%2C%20which%20we%20refer%20to%20as%20outer-layer%20exchange.%20However%2C%20the%20negative%0Atransfer%20in%20heterogeneous%20graphs%20arises%20not%20simply%20from%20the%20varying%20importance%0Aof%20an%20individual%20node%20feature%20across%20tasks%2C%20but%20also%20from%20the%20varying%0Aimportance%20of%20inter-relation%20between%20two%20nodes%20across%20tasks.%20These%0Ainter-relations%20are%20entangled%20in%20the%20output%20embedding%2C%20making%20it%20difficult%20for%0Aexisting%20methods%20to%20discriminate%20beneficial%20information%20from%20the%20embedding.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20Inner-Layer%20Information%20Exchange%0A%28InLINE%29%20model%20that%20facilitate%20fine-grained%20information%20exchanges%20within%20each%0Agraph%20layer%20rather%20than%20through%20output%20embeddings.%20Specifically%2C%20InLINE%0Aconsists%20of%20%281%29%20Structure%20Disentangled%20Experts%20for%20layer-wise%20structure%0Adisentanglement%2C%20%282%29%20Structure%20Disentangled%20Gates%20for%20assigning%20disentangled%0Ainformation%20to%20different%20tasks.%20Evaluations%20on%20two%20public%20datasets%20and%20a%20large%0Aindustry%20dataset%20show%20that%20our%20model%20effectively%20alleviates%20the%20significant%0Aperformance%20drop%20on%20specific%20tasks%20caused%20by%20negative%20transfer%2C%20improving%20Macro%0AF1%20by%206.3%25%20on%20DBLP%20dataset%20and%20AUC%20by%203.6%25%20on%20the%20industry%20dataset%20compared%20to%0ASoA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22089v1&entry.124074799=Read"},
{"title": "Stratified Domain Adaptation: A Progressive Self-Training Approach for\n  Scene Text Recognition", "author": "Kha Nhat Le and Hoang-Tuan Nguyen and Hung Tien Tran and Thanh Duc Ngo", "abstract": "  Unsupervised domain adaptation (UDA) has become increasingly prevalent in\nscene text recognition (STR), especially where training and testing data reside\nin different domains. The efficacy of existing UDA approaches tends to degrade\nwhen there is a large gap between the source and target domains. To deal with\nthis problem, gradually shifting or progressively learning to shift from domain\nto domain is the key issue. In this paper, we introduce the Stratified Domain\nAdaptation (StrDA) approach, which examines the gradual escalation of the\ndomain gap for the learning process. The objective is to partition the training\ndata into subsets so that the progressively self-trained model can adapt to\ngradual changes. We stratify the training data by evaluating the proximity of\neach data sample to both the source and target domains. We propose a novel\nmethod for employing domain discriminators to estimate the out-of-distribution\nand domain discriminative levels of data samples. Extensive experiments on\nbenchmark scene-text datasets show that our approach significantly improves the\nperformance of baseline (source-trained) STR models.\n", "link": "http://arxiv.org/abs/2410.09913v3", "date": "2024-10-29", "relevancy": 2.5912, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stratified%20Domain%20Adaptation%3A%20A%20Progressive%20Self-Training%20Approach%20for%0A%20%20Scene%20Text%20Recognition&body=Title%3A%20Stratified%20Domain%20Adaptation%3A%20A%20Progressive%20Self-Training%20Approach%20for%0A%20%20Scene%20Text%20Recognition%0AAuthor%3A%20Kha%20Nhat%20Le%20and%20Hoang-Tuan%20Nguyen%20and%20Hung%20Tien%20Tran%20and%20Thanh%20Duc%20Ngo%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20has%20become%20increasingly%20prevalent%20in%0Ascene%20text%20recognition%20%28STR%29%2C%20especially%20where%20training%20and%20testing%20data%20reside%0Ain%20different%20domains.%20The%20efficacy%20of%20existing%20UDA%20approaches%20tends%20to%20degrade%0Awhen%20there%20is%20a%20large%20gap%20between%20the%20source%20and%20target%20domains.%20To%20deal%20with%0Athis%20problem%2C%20gradually%20shifting%20or%20progressively%20learning%20to%20shift%20from%20domain%0Ato%20domain%20is%20the%20key%20issue.%20In%20this%20paper%2C%20we%20introduce%20the%20Stratified%20Domain%0AAdaptation%20%28StrDA%29%20approach%2C%20which%20examines%20the%20gradual%20escalation%20of%20the%0Adomain%20gap%20for%20the%20learning%20process.%20The%20objective%20is%20to%20partition%20the%20training%0Adata%20into%20subsets%20so%20that%20the%20progressively%20self-trained%20model%20can%20adapt%20to%0Agradual%20changes.%20We%20stratify%20the%20training%20data%20by%20evaluating%20the%20proximity%20of%0Aeach%20data%20sample%20to%20both%20the%20source%20and%20target%20domains.%20We%20propose%20a%20novel%0Amethod%20for%20employing%20domain%20discriminators%20to%20estimate%20the%20out-of-distribution%0Aand%20domain%20discriminative%20levels%20of%20data%20samples.%20Extensive%20experiments%20on%0Abenchmark%20scene-text%20datasets%20show%20that%20our%20approach%20significantly%20improves%20the%0Aperformance%20of%20baseline%20%28source-trained%29%20STR%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09913v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStratified%2520Domain%2520Adaptation%253A%2520A%2520Progressive%2520Self-Training%2520Approach%2520for%250A%2520%2520Scene%2520Text%2520Recognition%26entry.906535625%3DKha%2520Nhat%2520Le%2520and%2520Hoang-Tuan%2520Nguyen%2520and%2520Hung%2520Tien%2520Tran%2520and%2520Thanh%2520Duc%2520Ngo%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520has%2520become%2520increasingly%2520prevalent%2520in%250Ascene%2520text%2520recognition%2520%2528STR%2529%252C%2520especially%2520where%2520training%2520and%2520testing%2520data%2520reside%250Ain%2520different%2520domains.%2520The%2520efficacy%2520of%2520existing%2520UDA%2520approaches%2520tends%2520to%2520degrade%250Awhen%2520there%2520is%2520a%2520large%2520gap%2520between%2520the%2520source%2520and%2520target%2520domains.%2520To%2520deal%2520with%250Athis%2520problem%252C%2520gradually%2520shifting%2520or%2520progressively%2520learning%2520to%2520shift%2520from%2520domain%250Ato%2520domain%2520is%2520the%2520key%2520issue.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Stratified%2520Domain%250AAdaptation%2520%2528StrDA%2529%2520approach%252C%2520which%2520examines%2520the%2520gradual%2520escalation%2520of%2520the%250Adomain%2520gap%2520for%2520the%2520learning%2520process.%2520The%2520objective%2520is%2520to%2520partition%2520the%2520training%250Adata%2520into%2520subsets%2520so%2520that%2520the%2520progressively%2520self-trained%2520model%2520can%2520adapt%2520to%250Agradual%2520changes.%2520We%2520stratify%2520the%2520training%2520data%2520by%2520evaluating%2520the%2520proximity%2520of%250Aeach%2520data%2520sample%2520to%2520both%2520the%2520source%2520and%2520target%2520domains.%2520We%2520propose%2520a%2520novel%250Amethod%2520for%2520employing%2520domain%2520discriminators%2520to%2520estimate%2520the%2520out-of-distribution%250Aand%2520domain%2520discriminative%2520levels%2520of%2520data%2520samples.%2520Extensive%2520experiments%2520on%250Abenchmark%2520scene-text%2520datasets%2520show%2520that%2520our%2520approach%2520significantly%2520improves%2520the%250Aperformance%2520of%2520baseline%2520%2528source-trained%2529%2520STR%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09913v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stratified%20Domain%20Adaptation%3A%20A%20Progressive%20Self-Training%20Approach%20for%0A%20%20Scene%20Text%20Recognition&entry.906535625=Kha%20Nhat%20Le%20and%20Hoang-Tuan%20Nguyen%20and%20Hung%20Tien%20Tran%20and%20Thanh%20Duc%20Ngo&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20has%20become%20increasingly%20prevalent%20in%0Ascene%20text%20recognition%20%28STR%29%2C%20especially%20where%20training%20and%20testing%20data%20reside%0Ain%20different%20domains.%20The%20efficacy%20of%20existing%20UDA%20approaches%20tends%20to%20degrade%0Awhen%20there%20is%20a%20large%20gap%20between%20the%20source%20and%20target%20domains.%20To%20deal%20with%0Athis%20problem%2C%20gradually%20shifting%20or%20progressively%20learning%20to%20shift%20from%20domain%0Ato%20domain%20is%20the%20key%20issue.%20In%20this%20paper%2C%20we%20introduce%20the%20Stratified%20Domain%0AAdaptation%20%28StrDA%29%20approach%2C%20which%20examines%20the%20gradual%20escalation%20of%20the%0Adomain%20gap%20for%20the%20learning%20process.%20The%20objective%20is%20to%20partition%20the%20training%0Adata%20into%20subsets%20so%20that%20the%20progressively%20self-trained%20model%20can%20adapt%20to%0Agradual%20changes.%20We%20stratify%20the%20training%20data%20by%20evaluating%20the%20proximity%20of%0Aeach%20data%20sample%20to%20both%20the%20source%20and%20target%20domains.%20We%20propose%20a%20novel%0Amethod%20for%20employing%20domain%20discriminators%20to%20estimate%20the%20out-of-distribution%0Aand%20domain%20discriminative%20levels%20of%20data%20samples.%20Extensive%20experiments%20on%0Abenchmark%20scene-text%20datasets%20show%20that%20our%20approach%20significantly%20improves%20the%0Aperformance%20of%20baseline%20%28source-trained%29%20STR%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09913v3&entry.124074799=Read"},
{"title": "CAMS: Convolution and Attention-Free Mamba-based Cardiac Image\n  Segmentation", "author": "Abbas Khan and Muhammad Asad and Martin Benning and Caroline Roney and Gregory Slabaugh", "abstract": "  Convolutional Neural Networks (CNNs) and Transformer-based self-attention\nmodels have become the standard for medical image segmentation. This paper\ndemonstrates that convolution and self-attention, while widely used, are not\nthe only effective methods for segmentation. Breaking with convention, we\npresent a Convolution and self-Attention-free Mamba-based semantic Segmentation\nNetwork named CAMS-Net. Specifically, we design Mamba-based Channel Aggregator\nand Spatial Aggregator, which are applied independently in each encoder-decoder\nstage. The Channel Aggregator extracts information across different channels,\nand the Spatial Aggregator learns features across different spatial locations.\nWe also propose a Linearly Interconnected Factorized Mamba (LIFM) block to\nreduce the computational complexity of a Mamba block and to enhance its\ndecision function by introducing a non-linearity between two factorized Mamba\nblocks. Our model outperforms the existing state-of-the-art CNN,\nself-attention, and Mamba-based methods on CMR and M&Ms-2 Cardiac segmentation\ndatasets, showing how this innovative, convolution, and self-attention-free\nmethod can inspire further research beyond CNN and Transformer paradigms,\nachieving linear complexity and reducing the number of parameters. Source code\nand pre-trained models are available at: https://github.com/kabbas570/CAMS-Net.\n", "link": "http://arxiv.org/abs/2406.05786v3", "date": "2024-10-29", "relevancy": 2.5845, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMS%3A%20Convolution%20and%20Attention-Free%20Mamba-based%20Cardiac%20Image%0A%20%20Segmentation&body=Title%3A%20CAMS%3A%20Convolution%20and%20Attention-Free%20Mamba-based%20Cardiac%20Image%0A%20%20Segmentation%0AAuthor%3A%20Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformer-based%20self-attention%0Amodels%20have%20become%20the%20standard%20for%20medical%20image%20segmentation.%20This%20paper%0Ademonstrates%20that%20convolution%20and%20self-attention%2C%20while%20widely%20used%2C%20are%20not%0Athe%20only%20effective%20methods%20for%20segmentation.%20Breaking%20with%20convention%2C%20we%0Apresent%20a%20Convolution%20and%20self-Attention-free%20Mamba-based%20semantic%20Segmentation%0ANetwork%20named%20CAMS-Net.%20Specifically%2C%20we%20design%20Mamba-based%20Channel%20Aggregator%0Aand%20Spatial%20Aggregator%2C%20which%20are%20applied%20independently%20in%20each%20encoder-decoder%0Astage.%20The%20Channel%20Aggregator%20extracts%20information%20across%20different%20channels%2C%0Aand%20the%20Spatial%20Aggregator%20learns%20features%20across%20different%20spatial%20locations.%0AWe%20also%20propose%20a%20Linearly%20Interconnected%20Factorized%20Mamba%20%28LIFM%29%20block%20to%0Areduce%20the%20computational%20complexity%20of%20a%20Mamba%20block%20and%20to%20enhance%20its%0Adecision%20function%20by%20introducing%20a%20non-linearity%20between%20two%20factorized%20Mamba%0Ablocks.%20Our%20model%20outperforms%20the%20existing%20state-of-the-art%20CNN%2C%0Aself-attention%2C%20and%20Mamba-based%20methods%20on%20CMR%20and%20M%26Ms-2%20Cardiac%20segmentation%0Adatasets%2C%20showing%20how%20this%20innovative%2C%20convolution%2C%20and%20self-attention-free%0Amethod%20can%20inspire%20further%20research%20beyond%20CNN%20and%20Transformer%20paradigms%2C%0Aachieving%20linear%20complexity%20and%20reducing%20the%20number%20of%20parameters.%20Source%20code%0Aand%20pre-trained%20models%20are%20available%20at%3A%20https%3A//github.com/kabbas570/CAMS-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05786v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMS%253A%2520Convolution%2520and%2520Attention-Free%2520Mamba-based%2520Cardiac%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DAbbas%2520Khan%2520and%2520Muhammad%2520Asad%2520and%2520Martin%2520Benning%2520and%2520Caroline%2520Roney%2520and%2520Gregory%2520Slabaugh%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformer-based%2520self-attention%250Amodels%2520have%2520become%2520the%2520standard%2520for%2520medical%2520image%2520segmentation.%2520This%2520paper%250Ademonstrates%2520that%2520convolution%2520and%2520self-attention%252C%2520while%2520widely%2520used%252C%2520are%2520not%250Athe%2520only%2520effective%2520methods%2520for%2520segmentation.%2520Breaking%2520with%2520convention%252C%2520we%250Apresent%2520a%2520Convolution%2520and%2520self-Attention-free%2520Mamba-based%2520semantic%2520Segmentation%250ANetwork%2520named%2520CAMS-Net.%2520Specifically%252C%2520we%2520design%2520Mamba-based%2520Channel%2520Aggregator%250Aand%2520Spatial%2520Aggregator%252C%2520which%2520are%2520applied%2520independently%2520in%2520each%2520encoder-decoder%250Astage.%2520The%2520Channel%2520Aggregator%2520extracts%2520information%2520across%2520different%2520channels%252C%250Aand%2520the%2520Spatial%2520Aggregator%2520learns%2520features%2520across%2520different%2520spatial%2520locations.%250AWe%2520also%2520propose%2520a%2520Linearly%2520Interconnected%2520Factorized%2520Mamba%2520%2528LIFM%2529%2520block%2520to%250Areduce%2520the%2520computational%2520complexity%2520of%2520a%2520Mamba%2520block%2520and%2520to%2520enhance%2520its%250Adecision%2520function%2520by%2520introducing%2520a%2520non-linearity%2520between%2520two%2520factorized%2520Mamba%250Ablocks.%2520Our%2520model%2520outperforms%2520the%2520existing%2520state-of-the-art%2520CNN%252C%250Aself-attention%252C%2520and%2520Mamba-based%2520methods%2520on%2520CMR%2520and%2520M%2526Ms-2%2520Cardiac%2520segmentation%250Adatasets%252C%2520showing%2520how%2520this%2520innovative%252C%2520convolution%252C%2520and%2520self-attention-free%250Amethod%2520can%2520inspire%2520further%2520research%2520beyond%2520CNN%2520and%2520Transformer%2520paradigms%252C%250Aachieving%2520linear%2520complexity%2520and%2520reducing%2520the%2520number%2520of%2520parameters.%2520Source%2520code%250Aand%2520pre-trained%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/kabbas570/CAMS-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05786v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMS%3A%20Convolution%20and%20Attention-Free%20Mamba-based%20Cardiac%20Image%0A%20%20Segmentation&entry.906535625=Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformer-based%20self-attention%0Amodels%20have%20become%20the%20standard%20for%20medical%20image%20segmentation.%20This%20paper%0Ademonstrates%20that%20convolution%20and%20self-attention%2C%20while%20widely%20used%2C%20are%20not%0Athe%20only%20effective%20methods%20for%20segmentation.%20Breaking%20with%20convention%2C%20we%0Apresent%20a%20Convolution%20and%20self-Attention-free%20Mamba-based%20semantic%20Segmentation%0ANetwork%20named%20CAMS-Net.%20Specifically%2C%20we%20design%20Mamba-based%20Channel%20Aggregator%0Aand%20Spatial%20Aggregator%2C%20which%20are%20applied%20independently%20in%20each%20encoder-decoder%0Astage.%20The%20Channel%20Aggregator%20extracts%20information%20across%20different%20channels%2C%0Aand%20the%20Spatial%20Aggregator%20learns%20features%20across%20different%20spatial%20locations.%0AWe%20also%20propose%20a%20Linearly%20Interconnected%20Factorized%20Mamba%20%28LIFM%29%20block%20to%0Areduce%20the%20computational%20complexity%20of%20a%20Mamba%20block%20and%20to%20enhance%20its%0Adecision%20function%20by%20introducing%20a%20non-linearity%20between%20two%20factorized%20Mamba%0Ablocks.%20Our%20model%20outperforms%20the%20existing%20state-of-the-art%20CNN%2C%0Aself-attention%2C%20and%20Mamba-based%20methods%20on%20CMR%20and%20M%26Ms-2%20Cardiac%20segmentation%0Adatasets%2C%20showing%20how%20this%20innovative%2C%20convolution%2C%20and%20self-attention-free%0Amethod%20can%20inspire%20further%20research%20beyond%20CNN%20and%20Transformer%20paradigms%2C%0Aachieving%20linear%20complexity%20and%20reducing%20the%20number%20of%20parameters.%20Source%20code%0Aand%20pre-trained%20models%20are%20available%20at%3A%20https%3A//github.com/kabbas570/CAMS-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05786v3&entry.124074799=Read"},
{"title": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate", "author": "Zhiqi Bu and Xiaomeng Jin and Bhanukiran Vinzamuri and Anil Ramakrishna and Kai-Wei Chang and Volkan Cevher and Mingyi Hong", "abstract": "  Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.\n", "link": "http://arxiv.org/abs/2410.22086v1", "date": "2024-10-29", "relevancy": 2.581, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5152}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearning%20as%20multi-task%20optimization%3A%20A%20normalized%20gradient%20difference%0A%20%20approach%20with%20an%20adaptive%20learning%20rate&body=Title%3A%20Unlearning%20as%20multi-task%20optimization%3A%20A%20normalized%20gradient%20difference%0A%20%20approach%20with%20an%20adaptive%20learning%20rate%0AAuthor%3A%20Zhiqi%20Bu%20and%20Xiaomeng%20Jin%20and%20Bhanukiran%20Vinzamuri%20and%20Anil%20Ramakrishna%20and%20Kai-Wei%20Chang%20and%20Volkan%20Cevher%20and%20Mingyi%20Hong%0AAbstract%3A%20%20%20Machine%20unlearning%20has%20been%20used%20to%20remove%20unwanted%20knowledge%20acquired%20by%0Alarge%20language%20models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20examine%20machine%20unlearning%20from%0Aan%20optimization%20perspective%2C%20framing%20it%20as%20a%20regularized%20multi-task%0Aoptimization%20problem%2C%20where%20one%20task%20optimizes%20a%20forgetting%20objective%20and%0Aanother%20optimizes%20the%20model%20performance.%20In%20particular%2C%20we%20introduce%20a%0Anormalized%20gradient%20difference%20%28NGDiff%29%20algorithm%2C%20enabling%20us%20to%20have%20better%0Acontrol%20over%20the%20trade-off%20between%20the%20objectives%2C%20while%20integrating%20a%20new%2C%0Aautomatic%20learning%20rate%20scheduler.%20We%20provide%20a%20theoretical%20analysis%20and%0Aempirically%20demonstrate%20the%20superior%20performance%20of%20NGDiff%20among%0Astate-of-the-art%20unlearning%20methods%20on%20the%20TOFU%20and%20MUSE%20datasets%20while%0Aexhibiting%20stable%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearning%2520as%2520multi-task%2520optimization%253A%2520A%2520normalized%2520gradient%2520difference%250A%2520%2520approach%2520with%2520an%2520adaptive%2520learning%2520rate%26entry.906535625%3DZhiqi%2520Bu%2520and%2520Xiaomeng%2520Jin%2520and%2520Bhanukiran%2520Vinzamuri%2520and%2520Anil%2520Ramakrishna%2520and%2520Kai-Wei%2520Chang%2520and%2520Volkan%2520Cevher%2520and%2520Mingyi%2520Hong%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520has%2520been%2520used%2520to%2520remove%2520unwanted%2520knowledge%2520acquired%2520by%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520In%2520this%2520paper%252C%2520we%2520examine%2520machine%2520unlearning%2520from%250Aan%2520optimization%2520perspective%252C%2520framing%2520it%2520as%2520a%2520regularized%2520multi-task%250Aoptimization%2520problem%252C%2520where%2520one%2520task%2520optimizes%2520a%2520forgetting%2520objective%2520and%250Aanother%2520optimizes%2520the%2520model%2520performance.%2520In%2520particular%252C%2520we%2520introduce%2520a%250Anormalized%2520gradient%2520difference%2520%2528NGDiff%2529%2520algorithm%252C%2520enabling%2520us%2520to%2520have%2520better%250Acontrol%2520over%2520the%2520trade-off%2520between%2520the%2520objectives%252C%2520while%2520integrating%2520a%2520new%252C%250Aautomatic%2520learning%2520rate%2520scheduler.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520and%250Aempirically%2520demonstrate%2520the%2520superior%2520performance%2520of%2520NGDiff%2520among%250Astate-of-the-art%2520unlearning%2520methods%2520on%2520the%2520TOFU%2520and%2520MUSE%2520datasets%2520while%250Aexhibiting%2520stable%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearning%20as%20multi-task%20optimization%3A%20A%20normalized%20gradient%20difference%0A%20%20approach%20with%20an%20adaptive%20learning%20rate&entry.906535625=Zhiqi%20Bu%20and%20Xiaomeng%20Jin%20and%20Bhanukiran%20Vinzamuri%20and%20Anil%20Ramakrishna%20and%20Kai-Wei%20Chang%20and%20Volkan%20Cevher%20and%20Mingyi%20Hong&entry.1292438233=%20%20Machine%20unlearning%20has%20been%20used%20to%20remove%20unwanted%20knowledge%20acquired%20by%0Alarge%20language%20models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20examine%20machine%20unlearning%20from%0Aan%20optimization%20perspective%2C%20framing%20it%20as%20a%20regularized%20multi-task%0Aoptimization%20problem%2C%20where%20one%20task%20optimizes%20a%20forgetting%20objective%20and%0Aanother%20optimizes%20the%20model%20performance.%20In%20particular%2C%20we%20introduce%20a%0Anormalized%20gradient%20difference%20%28NGDiff%29%20algorithm%2C%20enabling%20us%20to%20have%20better%0Acontrol%20over%20the%20trade-off%20between%20the%20objectives%2C%20while%20integrating%20a%20new%2C%0Aautomatic%20learning%20rate%20scheduler.%20We%20provide%20a%20theoretical%20analysis%20and%0Aempirically%20demonstrate%20the%20superior%20performance%20of%20NGDiff%20among%0Astate-of-the-art%20unlearning%20methods%20on%20the%20TOFU%20and%20MUSE%20datasets%20while%0Aexhibiting%20stable%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22086v1&entry.124074799=Read"},
{"title": "Dissecting Query-Key Interaction in Vision Transformers", "author": "Xu Pan and Aaron Philip and Ziqian Xie and Odelia Schwartz", "abstract": "  Self-attention in vision transformers is often thought to perform perceptual\ngrouping where tokens attend to other tokens with similar embeddings, which\ncould correspond to semantically similar features of an object. However,\nattending to dissimilar tokens can be beneficial by providing contextual\ninformation. We propose to analyze the query-key interaction by the singular\nvalue decomposition of the interaction matrix (i.e.\n${\\textbf{W}_q}^\\top\\textbf{W}_k$). We find that in many ViTs, especially those\nwith classification training objectives, early layers attend more to similar\ntokens, while late layers show increased attention to dissimilar tokens,\nproviding evidence corresponding to perceptual grouping and contextualization,\nrespectively. Many of these interactions between features represented by\nsingular vectors are interpretable and semantic, such as attention between\nrelevant objects, between parts of an object, or between the foreground and\nbackground. This offers a novel perspective on interpreting the attention\nmechanism, which contributes to understanding how transformer models utilize\ncontext and salient features when processing images.\n", "link": "http://arxiv.org/abs/2405.14880v3", "date": "2024-10-29", "relevancy": 2.5508, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Query-Key%20Interaction%20in%20Vision%20Transformers&body=Title%3A%20Dissecting%20Query-Key%20Interaction%20in%20Vision%20Transformers%0AAuthor%3A%20Xu%20Pan%20and%20Aaron%20Philip%20and%20Ziqian%20Xie%20and%20Odelia%20Schwartz%0AAbstract%3A%20%20%20Self-attention%20in%20vision%20transformers%20is%20often%20thought%20to%20perform%20perceptual%0Agrouping%20where%20tokens%20attend%20to%20other%20tokens%20with%20similar%20embeddings%2C%20which%0Acould%20correspond%20to%20semantically%20similar%20features%20of%20an%20object.%20However%2C%0Aattending%20to%20dissimilar%20tokens%20can%20be%20beneficial%20by%20providing%20contextual%0Ainformation.%20We%20propose%20to%20analyze%20the%20query-key%20interaction%20by%20the%20singular%0Avalue%20decomposition%20of%20the%20interaction%20matrix%20%28i.e.%0A%24%7B%5Ctextbf%7BW%7D_q%7D%5E%5Ctop%5Ctextbf%7BW%7D_k%24%29.%20We%20find%20that%20in%20many%20ViTs%2C%20especially%20those%0Awith%20classification%20training%20objectives%2C%20early%20layers%20attend%20more%20to%20similar%0Atokens%2C%20while%20late%20layers%20show%20increased%20attention%20to%20dissimilar%20tokens%2C%0Aproviding%20evidence%20corresponding%20to%20perceptual%20grouping%20and%20contextualization%2C%0Arespectively.%20Many%20of%20these%20interactions%20between%20features%20represented%20by%0Asingular%20vectors%20are%20interpretable%20and%20semantic%2C%20such%20as%20attention%20between%0Arelevant%20objects%2C%20between%20parts%20of%20an%20object%2C%20or%20between%20the%20foreground%20and%0Abackground.%20This%20offers%20a%20novel%20perspective%20on%20interpreting%20the%20attention%0Amechanism%2C%20which%20contributes%20to%20understanding%20how%20transformer%20models%20utilize%0Acontext%20and%20salient%20features%20when%20processing%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14880v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Query-Key%2520Interaction%2520in%2520Vision%2520Transformers%26entry.906535625%3DXu%2520Pan%2520and%2520Aaron%2520Philip%2520and%2520Ziqian%2520Xie%2520and%2520Odelia%2520Schwartz%26entry.1292438233%3D%2520%2520Self-attention%2520in%2520vision%2520transformers%2520is%2520often%2520thought%2520to%2520perform%2520perceptual%250Agrouping%2520where%2520tokens%2520attend%2520to%2520other%2520tokens%2520with%2520similar%2520embeddings%252C%2520which%250Acould%2520correspond%2520to%2520semantically%2520similar%2520features%2520of%2520an%2520object.%2520However%252C%250Aattending%2520to%2520dissimilar%2520tokens%2520can%2520be%2520beneficial%2520by%2520providing%2520contextual%250Ainformation.%2520We%2520propose%2520to%2520analyze%2520the%2520query-key%2520interaction%2520by%2520the%2520singular%250Avalue%2520decomposition%2520of%2520the%2520interaction%2520matrix%2520%2528i.e.%250A%2524%257B%255Ctextbf%257BW%257D_q%257D%255E%255Ctop%255Ctextbf%257BW%257D_k%2524%2529.%2520We%2520find%2520that%2520in%2520many%2520ViTs%252C%2520especially%2520those%250Awith%2520classification%2520training%2520objectives%252C%2520early%2520layers%2520attend%2520more%2520to%2520similar%250Atokens%252C%2520while%2520late%2520layers%2520show%2520increased%2520attention%2520to%2520dissimilar%2520tokens%252C%250Aproviding%2520evidence%2520corresponding%2520to%2520perceptual%2520grouping%2520and%2520contextualization%252C%250Arespectively.%2520Many%2520of%2520these%2520interactions%2520between%2520features%2520represented%2520by%250Asingular%2520vectors%2520are%2520interpretable%2520and%2520semantic%252C%2520such%2520as%2520attention%2520between%250Arelevant%2520objects%252C%2520between%2520parts%2520of%2520an%2520object%252C%2520or%2520between%2520the%2520foreground%2520and%250Abackground.%2520This%2520offers%2520a%2520novel%2520perspective%2520on%2520interpreting%2520the%2520attention%250Amechanism%252C%2520which%2520contributes%2520to%2520understanding%2520how%2520transformer%2520models%2520utilize%250Acontext%2520and%2520salient%2520features%2520when%2520processing%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14880v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Query-Key%20Interaction%20in%20Vision%20Transformers&entry.906535625=Xu%20Pan%20and%20Aaron%20Philip%20and%20Ziqian%20Xie%20and%20Odelia%20Schwartz&entry.1292438233=%20%20Self-attention%20in%20vision%20transformers%20is%20often%20thought%20to%20perform%20perceptual%0Agrouping%20where%20tokens%20attend%20to%20other%20tokens%20with%20similar%20embeddings%2C%20which%0Acould%20correspond%20to%20semantically%20similar%20features%20of%20an%20object.%20However%2C%0Aattending%20to%20dissimilar%20tokens%20can%20be%20beneficial%20by%20providing%20contextual%0Ainformation.%20We%20propose%20to%20analyze%20the%20query-key%20interaction%20by%20the%20singular%0Avalue%20decomposition%20of%20the%20interaction%20matrix%20%28i.e.%0A%24%7B%5Ctextbf%7BW%7D_q%7D%5E%5Ctop%5Ctextbf%7BW%7D_k%24%29.%20We%20find%20that%20in%20many%20ViTs%2C%20especially%20those%0Awith%20classification%20training%20objectives%2C%20early%20layers%20attend%20more%20to%20similar%0Atokens%2C%20while%20late%20layers%20show%20increased%20attention%20to%20dissimilar%20tokens%2C%0Aproviding%20evidence%20corresponding%20to%20perceptual%20grouping%20and%20contextualization%2C%0Arespectively.%20Many%20of%20these%20interactions%20between%20features%20represented%20by%0Asingular%20vectors%20are%20interpretable%20and%20semantic%2C%20such%20as%20attention%20between%0Arelevant%20objects%2C%20between%20parts%20of%20an%20object%2C%20or%20between%20the%20foreground%20and%0Abackground.%20This%20offers%20a%20novel%20perspective%20on%20interpreting%20the%20attention%0Amechanism%2C%20which%20contributes%20to%20understanding%20how%20transformer%20models%20utilize%0Acontext%20and%20salient%20features%20when%20processing%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14880v3&entry.124074799=Read"},
{"title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation", "author": "Yixia Li and Boya Xiong and Guanhua Chen and Yun Chen", "abstract": "  Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.\n", "link": "http://arxiv.org/abs/2406.12629v3", "date": "2024-10-29", "relevancy": 2.5461, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5229}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5028}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeTAR%3A%20Out-of-Distribution%20Detection%20with%20Selective%20Low-Rank%0A%20%20Approximation&body=Title%3A%20SeTAR%3A%20Out-of-Distribution%20Detection%20with%20Selective%20Low-Rank%0A%20%20Approximation%0AAuthor%3A%20Yixia%20Li%20and%20Boya%20Xiong%20and%20Guanhua%20Chen%20and%20Yun%20Chen%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20crucial%20for%20the%20safe%20deployment%20of%0Aneural%20networks.%20Existing%20CLIP-based%20approaches%20perform%20OOD%20detection%20by%0Adevising%20novel%20scoring%20functions%20or%20sophisticated%20fine-tuning%20methods.%20In%20this%0Awork%2C%20we%20propose%20SeTAR%2C%20a%20novel%2C%20training-free%20OOD%20detection%20method%20that%0Aleverages%20selective%20low-rank%20approximation%20of%20weight%20matrices%20in%0Avision-language%20and%20vision-only%20models.%20SeTAR%20enhances%20OOD%20detection%20via%0Apost-hoc%20modification%20of%20the%20model%27s%20weight%20matrices%20using%20a%20simple%20greedy%0Asearch%20algorithm.%20Based%20on%20SeTAR%2C%20we%20further%20propose%20SeTAR%2BFT%2C%20a%20fine-tuning%0Aextension%20optimizing%20model%20performance%20for%20OOD%20detection%20tasks.%20Extensive%0Aevaluations%20on%20ImageNet1K%20and%20Pascal-VOC%20benchmarks%20show%20SeTAR%27s%20superior%0Aperformance%2C%20reducing%20the%20relatively%20false%20positive%20rate%20by%20up%20to%2018.95%25%20and%0A36.80%25%20compared%20to%20zero-shot%20and%20fine-tuning%20baselines.%20Ablation%20studies%0Afurther%20validate%20SeTAR%27s%20effectiveness%2C%20robustness%2C%20and%20generalizability%20across%0Adifferent%20model%20backbones.%20Our%20work%20offers%20a%20scalable%2C%20efficient%20solution%20for%0AOOD%20detection%2C%20setting%20a%20new%20state-of-the-art%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12629v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeTAR%253A%2520Out-of-Distribution%2520Detection%2520with%2520Selective%2520Low-Rank%250A%2520%2520Approximation%26entry.906535625%3DYixia%2520Li%2520and%2520Boya%2520Xiong%2520and%2520Guanhua%2520Chen%2520and%2520Yun%2520Chen%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520crucial%2520for%2520the%2520safe%2520deployment%2520of%250Aneural%2520networks.%2520Existing%2520CLIP-based%2520approaches%2520perform%2520OOD%2520detection%2520by%250Adevising%2520novel%2520scoring%2520functions%2520or%2520sophisticated%2520fine-tuning%2520methods.%2520In%2520this%250Awork%252C%2520we%2520propose%2520SeTAR%252C%2520a%2520novel%252C%2520training-free%2520OOD%2520detection%2520method%2520that%250Aleverages%2520selective%2520low-rank%2520approximation%2520of%2520weight%2520matrices%2520in%250Avision-language%2520and%2520vision-only%2520models.%2520SeTAR%2520enhances%2520OOD%2520detection%2520via%250Apost-hoc%2520modification%2520of%2520the%2520model%2527s%2520weight%2520matrices%2520using%2520a%2520simple%2520greedy%250Asearch%2520algorithm.%2520Based%2520on%2520SeTAR%252C%2520we%2520further%2520propose%2520SeTAR%252BFT%252C%2520a%2520fine-tuning%250Aextension%2520optimizing%2520model%2520performance%2520for%2520OOD%2520detection%2520tasks.%2520Extensive%250Aevaluations%2520on%2520ImageNet1K%2520and%2520Pascal-VOC%2520benchmarks%2520show%2520SeTAR%2527s%2520superior%250Aperformance%252C%2520reducing%2520the%2520relatively%2520false%2520positive%2520rate%2520by%2520up%2520to%252018.95%2525%2520and%250A36.80%2525%2520compared%2520to%2520zero-shot%2520and%2520fine-tuning%2520baselines.%2520Ablation%2520studies%250Afurther%2520validate%2520SeTAR%2527s%2520effectiveness%252C%2520robustness%252C%2520and%2520generalizability%2520across%250Adifferent%2520model%2520backbones.%2520Our%2520work%2520offers%2520a%2520scalable%252C%2520efficient%2520solution%2520for%250AOOD%2520detection%252C%2520setting%2520a%2520new%2520state-of-the-art%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12629v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeTAR%3A%20Out-of-Distribution%20Detection%20with%20Selective%20Low-Rank%0A%20%20Approximation&entry.906535625=Yixia%20Li%20and%20Boya%20Xiong%20and%20Guanhua%20Chen%20and%20Yun%20Chen&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20crucial%20for%20the%20safe%20deployment%20of%0Aneural%20networks.%20Existing%20CLIP-based%20approaches%20perform%20OOD%20detection%20by%0Adevising%20novel%20scoring%20functions%20or%20sophisticated%20fine-tuning%20methods.%20In%20this%0Awork%2C%20we%20propose%20SeTAR%2C%20a%20novel%2C%20training-free%20OOD%20detection%20method%20that%0Aleverages%20selective%20low-rank%20approximation%20of%20weight%20matrices%20in%0Avision-language%20and%20vision-only%20models.%20SeTAR%20enhances%20OOD%20detection%20via%0Apost-hoc%20modification%20of%20the%20model%27s%20weight%20matrices%20using%20a%20simple%20greedy%0Asearch%20algorithm.%20Based%20on%20SeTAR%2C%20we%20further%20propose%20SeTAR%2BFT%2C%20a%20fine-tuning%0Aextension%20optimizing%20model%20performance%20for%20OOD%20detection%20tasks.%20Extensive%0Aevaluations%20on%20ImageNet1K%20and%20Pascal-VOC%20benchmarks%20show%20SeTAR%27s%20superior%0Aperformance%2C%20reducing%20the%20relatively%20false%20positive%20rate%20by%20up%20to%2018.95%25%20and%0A36.80%25%20compared%20to%20zero-shot%20and%20fine-tuning%20baselines.%20Ablation%20studies%0Afurther%20validate%20SeTAR%27s%20effectiveness%2C%20robustness%2C%20and%20generalizability%20across%0Adifferent%20model%20backbones.%20Our%20work%20offers%20a%20scalable%2C%20efficient%20solution%20for%0AOOD%20detection%2C%20setting%20a%20new%20state-of-the-art%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12629v3&entry.124074799=Read"},
{"title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression", "author": "Alessio Devoto and Yu Zhao and Simone Scardapane and Pasquale Minervini", "abstract": "  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.\n", "link": "http://arxiv.org/abs/2406.11430v3", "date": "2024-10-29", "relevancy": 2.5135, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20and%20Effective%20%24L_2%24%20Norm-Based%20Strategy%20for%20KV%20Cache%0A%20%20Compression&body=Title%3A%20A%20Simple%20and%20Effective%20%24L_2%24%20Norm-Based%20Strategy%20for%20KV%20Cache%0A%20%20Compression%0AAuthor%3A%20Alessio%20Devoto%20and%20Yu%20Zhao%20and%20Simone%20Scardapane%20and%20Pasquale%20Minervini%0AAbstract%3A%20%20%20The%20deployment%20of%20large%20language%20models%20%28LLMs%29%20is%20often%20hindered%20by%20the%0Aextensive%20memory%20requirements%20of%20the%20Key-Value%20%28KV%29%20cache%2C%20especially%20as%0Acontext%20lengths%20increase.%20Existing%20approaches%20to%20reduce%20the%20KV%20cache%20size%0Ainvolve%20either%20fine-tuning%20the%20model%20to%20learn%20a%20compression%20strategy%20or%0Aleveraging%20attention%20scores%20to%20reduce%20the%20sequence%20length.%20We%20analyse%20the%0Aattention%20distributions%20in%20decoder-only%20Transformers-based%20models%20and%20observe%0Athat%20attention%20allocation%20patterns%20stay%20consistent%20across%20most%20layers.%0ASurprisingly%2C%20we%20find%20a%20clear%20correlation%20between%20the%20%24L_2%24%20and%20the%20attention%0Ascores%20over%20cached%20KV%20pairs%2C%20where%20a%20low%20%24L_2%24%20of%20a%20key%20embedding%20usually%20leads%0Ato%20a%20high%20attention%20score%20during%20decoding.%20This%20finding%20indicates%20that%20the%0Ainfluence%20of%20a%20KV%20pair%20is%20potentially%20determined%20by%20the%20key%20embedding%20itself%0Abefore%20being%20queried.%20Based%20on%20this%20observation%2C%20we%20compress%20the%20KV%20cache%20based%0Aon%20the%20%24L_2%24%20of%20key%20embeddings.%20Our%20experimental%20results%20show%20that%20this%20simple%0Astrategy%20can%20reduce%20the%20KV%20cache%20size%20by%2050%25%20on%20language%20modelling%20and%0Aneedle-in-a-haystack%20tasks%20and%2090%25%20on%20passkey%20retrieval%20tasks%20without%20losing%0Aaccuracy.%20Moreover%2C%20without%20relying%20on%20the%20attention%20scores%2C%20this%20approach%0Aremains%20compatible%20with%20FlashAttention%2C%20enabling%20broader%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11430v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520and%2520Effective%2520%2524L_2%2524%2520Norm-Based%2520Strategy%2520for%2520KV%2520Cache%250A%2520%2520Compression%26entry.906535625%3DAlessio%2520Devoto%2520and%2520Yu%2520Zhao%2520and%2520Simone%2520Scardapane%2520and%2520Pasquale%2520Minervini%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520often%2520hindered%2520by%2520the%250Aextensive%2520memory%2520requirements%2520of%2520the%2520Key-Value%2520%2528KV%2529%2520cache%252C%2520especially%2520as%250Acontext%2520lengths%2520increase.%2520Existing%2520approaches%2520to%2520reduce%2520the%2520KV%2520cache%2520size%250Ainvolve%2520either%2520fine-tuning%2520the%2520model%2520to%2520learn%2520a%2520compression%2520strategy%2520or%250Aleveraging%2520attention%2520scores%2520to%2520reduce%2520the%2520sequence%2520length.%2520We%2520analyse%2520the%250Aattention%2520distributions%2520in%2520decoder-only%2520Transformers-based%2520models%2520and%2520observe%250Athat%2520attention%2520allocation%2520patterns%2520stay%2520consistent%2520across%2520most%2520layers.%250ASurprisingly%252C%2520we%2520find%2520a%2520clear%2520correlation%2520between%2520the%2520%2524L_2%2524%2520and%2520the%2520attention%250Ascores%2520over%2520cached%2520KV%2520pairs%252C%2520where%2520a%2520low%2520%2524L_2%2524%2520of%2520a%2520key%2520embedding%2520usually%2520leads%250Ato%2520a%2520high%2520attention%2520score%2520during%2520decoding.%2520This%2520finding%2520indicates%2520that%2520the%250Ainfluence%2520of%2520a%2520KV%2520pair%2520is%2520potentially%2520determined%2520by%2520the%2520key%2520embedding%2520itself%250Abefore%2520being%2520queried.%2520Based%2520on%2520this%2520observation%252C%2520we%2520compress%2520the%2520KV%2520cache%2520based%250Aon%2520the%2520%2524L_2%2524%2520of%2520key%2520embeddings.%2520Our%2520experimental%2520results%2520show%2520that%2520this%2520simple%250Astrategy%2520can%2520reduce%2520the%2520KV%2520cache%2520size%2520by%252050%2525%2520on%2520language%2520modelling%2520and%250Aneedle-in-a-haystack%2520tasks%2520and%252090%2525%2520on%2520passkey%2520retrieval%2520tasks%2520without%2520losing%250Aaccuracy.%2520Moreover%252C%2520without%2520relying%2520on%2520the%2520attention%2520scores%252C%2520this%2520approach%250Aremains%2520compatible%2520with%2520FlashAttention%252C%2520enabling%2520broader%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11430v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20and%20Effective%20%24L_2%24%20Norm-Based%20Strategy%20for%20KV%20Cache%0A%20%20Compression&entry.906535625=Alessio%20Devoto%20and%20Yu%20Zhao%20and%20Simone%20Scardapane%20and%20Pasquale%20Minervini&entry.1292438233=%20%20The%20deployment%20of%20large%20language%20models%20%28LLMs%29%20is%20often%20hindered%20by%20the%0Aextensive%20memory%20requirements%20of%20the%20Key-Value%20%28KV%29%20cache%2C%20especially%20as%0Acontext%20lengths%20increase.%20Existing%20approaches%20to%20reduce%20the%20KV%20cache%20size%0Ainvolve%20either%20fine-tuning%20the%20model%20to%20learn%20a%20compression%20strategy%20or%0Aleveraging%20attention%20scores%20to%20reduce%20the%20sequence%20length.%20We%20analyse%20the%0Aattention%20distributions%20in%20decoder-only%20Transformers-based%20models%20and%20observe%0Athat%20attention%20allocation%20patterns%20stay%20consistent%20across%20most%20layers.%0ASurprisingly%2C%20we%20find%20a%20clear%20correlation%20between%20the%20%24L_2%24%20and%20the%20attention%0Ascores%20over%20cached%20KV%20pairs%2C%20where%20a%20low%20%24L_2%24%20of%20a%20key%20embedding%20usually%20leads%0Ato%20a%20high%20attention%20score%20during%20decoding.%20This%20finding%20indicates%20that%20the%0Ainfluence%20of%20a%20KV%20pair%20is%20potentially%20determined%20by%20the%20key%20embedding%20itself%0Abefore%20being%20queried.%20Based%20on%20this%20observation%2C%20we%20compress%20the%20KV%20cache%20based%0Aon%20the%20%24L_2%24%20of%20key%20embeddings.%20Our%20experimental%20results%20show%20that%20this%20simple%0Astrategy%20can%20reduce%20the%20KV%20cache%20size%20by%2050%25%20on%20language%20modelling%20and%0Aneedle-in-a-haystack%20tasks%20and%2090%25%20on%20passkey%20retrieval%20tasks%20without%20losing%0Aaccuracy.%20Moreover%2C%20without%20relying%20on%20the%20attention%20scores%2C%20this%20approach%0Aremains%20compatible%20with%20FlashAttention%2C%20enabling%20broader%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11430v3&entry.124074799=Read"},
{"title": "FANCL: Feature-Guided Attention Network with Curriculum Learning for\n  Brain Metastases Segmentation", "author": "Zijiang Liu and Xiaoyu Liu and Linhao Qu and Yonghong Shi", "abstract": "  Accurate segmentation of brain metastases (BMs) in MR image is crucial for\nthe diagnosis and follow-up of patients. Methods based on deep convolutional\nneural networks (CNNs) have achieved high segmentation performance. However,\ndue to the loss of critical feature information caused by convolutional and\npooling operations, CNNs still face great challenges in small BMs segmentation.\nBesides, BMs are irregular and easily confused with healthy tissues, which\nmakes it difficult for the model to effectively learn tumor structure during\ntraining. To address these issues, this paper proposes a novel model called\nfeature-guided attention network with curriculum learning (FANCL). Based on\nCNNs, FANCL utilizes the input image and its feature to establish the intrinsic\nconnections between metastases of different sizes, which can effectively\ncompensate for the loss of high-level feature from small tumors with the\ninformation of large tumors. Furthermore, FANCL applies the voxel-level\ncurriculum learning strategy to help the model gradually learn the structure\nand details of BMs. And baseline models of varying depths are employed as\ncurriculum-mining networks for organizing the curriculum progression. The\nevaluation results on the BraTS-METS 2023 dataset indicate that FANCL\nsignificantly improves the segmentation performance, confirming the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2410.22057v1", "date": "2024-10-29", "relevancy": 2.4873, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FANCL%3A%20Feature-Guided%20Attention%20Network%20with%20Curriculum%20Learning%20for%0A%20%20Brain%20Metastases%20Segmentation&body=Title%3A%20FANCL%3A%20Feature-Guided%20Attention%20Network%20with%20Curriculum%20Learning%20for%0A%20%20Brain%20Metastases%20Segmentation%0AAuthor%3A%20Zijiang%20Liu%20and%20Xiaoyu%20Liu%20and%20Linhao%20Qu%20and%20Yonghong%20Shi%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20brain%20metastases%20%28BMs%29%20in%20MR%20image%20is%20crucial%20for%0Athe%20diagnosis%20and%20follow-up%20of%20patients.%20Methods%20based%20on%20deep%20convolutional%0Aneural%20networks%20%28CNNs%29%20have%20achieved%20high%20segmentation%20performance.%20However%2C%0Adue%20to%20the%20loss%20of%20critical%20feature%20information%20caused%20by%20convolutional%20and%0Apooling%20operations%2C%20CNNs%20still%20face%20great%20challenges%20in%20small%20BMs%20segmentation.%0ABesides%2C%20BMs%20are%20irregular%20and%20easily%20confused%20with%20healthy%20tissues%2C%20which%0Amakes%20it%20difficult%20for%20the%20model%20to%20effectively%20learn%20tumor%20structure%20during%0Atraining.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20a%20novel%20model%20called%0Afeature-guided%20attention%20network%20with%20curriculum%20learning%20%28FANCL%29.%20Based%20on%0ACNNs%2C%20FANCL%20utilizes%20the%20input%20image%20and%20its%20feature%20to%20establish%20the%20intrinsic%0Aconnections%20between%20metastases%20of%20different%20sizes%2C%20which%20can%20effectively%0Acompensate%20for%20the%20loss%20of%20high-level%20feature%20from%20small%20tumors%20with%20the%0Ainformation%20of%20large%20tumors.%20Furthermore%2C%20FANCL%20applies%20the%20voxel-level%0Acurriculum%20learning%20strategy%20to%20help%20the%20model%20gradually%20learn%20the%20structure%0Aand%20details%20of%20BMs.%20And%20baseline%20models%20of%20varying%20depths%20are%20employed%20as%0Acurriculum-mining%20networks%20for%20organizing%20the%20curriculum%20progression.%20The%0Aevaluation%20results%20on%20the%20BraTS-METS%202023%20dataset%20indicate%20that%20FANCL%0Asignificantly%20improves%20the%20segmentation%20performance%2C%20confirming%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFANCL%253A%2520Feature-Guided%2520Attention%2520Network%2520with%2520Curriculum%2520Learning%2520for%250A%2520%2520Brain%2520Metastases%2520Segmentation%26entry.906535625%3DZijiang%2520Liu%2520and%2520Xiaoyu%2520Liu%2520and%2520Linhao%2520Qu%2520and%2520Yonghong%2520Shi%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520brain%2520metastases%2520%2528BMs%2529%2520in%2520MR%2520image%2520is%2520crucial%2520for%250Athe%2520diagnosis%2520and%2520follow-up%2520of%2520patients.%2520Methods%2520based%2520on%2520deep%2520convolutional%250Aneural%2520networks%2520%2528CNNs%2529%2520have%2520achieved%2520high%2520segmentation%2520performance.%2520However%252C%250Adue%2520to%2520the%2520loss%2520of%2520critical%2520feature%2520information%2520caused%2520by%2520convolutional%2520and%250Apooling%2520operations%252C%2520CNNs%2520still%2520face%2520great%2520challenges%2520in%2520small%2520BMs%2520segmentation.%250ABesides%252C%2520BMs%2520are%2520irregular%2520and%2520easily%2520confused%2520with%2520healthy%2520tissues%252C%2520which%250Amakes%2520it%2520difficult%2520for%2520the%2520model%2520to%2520effectively%2520learn%2520tumor%2520structure%2520during%250Atraining.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520model%2520called%250Afeature-guided%2520attention%2520network%2520with%2520curriculum%2520learning%2520%2528FANCL%2529.%2520Based%2520on%250ACNNs%252C%2520FANCL%2520utilizes%2520the%2520input%2520image%2520and%2520its%2520feature%2520to%2520establish%2520the%2520intrinsic%250Aconnections%2520between%2520metastases%2520of%2520different%2520sizes%252C%2520which%2520can%2520effectively%250Acompensate%2520for%2520the%2520loss%2520of%2520high-level%2520feature%2520from%2520small%2520tumors%2520with%2520the%250Ainformation%2520of%2520large%2520tumors.%2520Furthermore%252C%2520FANCL%2520applies%2520the%2520voxel-level%250Acurriculum%2520learning%2520strategy%2520to%2520help%2520the%2520model%2520gradually%2520learn%2520the%2520structure%250Aand%2520details%2520of%2520BMs.%2520And%2520baseline%2520models%2520of%2520varying%2520depths%2520are%2520employed%2520as%250Acurriculum-mining%2520networks%2520for%2520organizing%2520the%2520curriculum%2520progression.%2520The%250Aevaluation%2520results%2520on%2520the%2520BraTS-METS%25202023%2520dataset%2520indicate%2520that%2520FANCL%250Asignificantly%2520improves%2520the%2520segmentation%2520performance%252C%2520confirming%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FANCL%3A%20Feature-Guided%20Attention%20Network%20with%20Curriculum%20Learning%20for%0A%20%20Brain%20Metastases%20Segmentation&entry.906535625=Zijiang%20Liu%20and%20Xiaoyu%20Liu%20and%20Linhao%20Qu%20and%20Yonghong%20Shi&entry.1292438233=%20%20Accurate%20segmentation%20of%20brain%20metastases%20%28BMs%29%20in%20MR%20image%20is%20crucial%20for%0Athe%20diagnosis%20and%20follow-up%20of%20patients.%20Methods%20based%20on%20deep%20convolutional%0Aneural%20networks%20%28CNNs%29%20have%20achieved%20high%20segmentation%20performance.%20However%2C%0Adue%20to%20the%20loss%20of%20critical%20feature%20information%20caused%20by%20convolutional%20and%0Apooling%20operations%2C%20CNNs%20still%20face%20great%20challenges%20in%20small%20BMs%20segmentation.%0ABesides%2C%20BMs%20are%20irregular%20and%20easily%20confused%20with%20healthy%20tissues%2C%20which%0Amakes%20it%20difficult%20for%20the%20model%20to%20effectively%20learn%20tumor%20structure%20during%0Atraining.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20a%20novel%20model%20called%0Afeature-guided%20attention%20network%20with%20curriculum%20learning%20%28FANCL%29.%20Based%20on%0ACNNs%2C%20FANCL%20utilizes%20the%20input%20image%20and%20its%20feature%20to%20establish%20the%20intrinsic%0Aconnections%20between%20metastases%20of%20different%20sizes%2C%20which%20can%20effectively%0Acompensate%20for%20the%20loss%20of%20high-level%20feature%20from%20small%20tumors%20with%20the%0Ainformation%20of%20large%20tumors.%20Furthermore%2C%20FANCL%20applies%20the%20voxel-level%0Acurriculum%20learning%20strategy%20to%20help%20the%20model%20gradually%20learn%20the%20structure%0Aand%20details%20of%20BMs.%20And%20baseline%20models%20of%20varying%20depths%20are%20employed%20as%0Acurriculum-mining%20networks%20for%20organizing%20the%20curriculum%20progression.%20The%0Aevaluation%20results%20on%20the%20BraTS-METS%202023%20dataset%20indicate%20that%20FANCL%0Asignificantly%20improves%20the%20segmentation%20performance%2C%20confirming%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22057v1&entry.124074799=Read"},
{"title": "Hypergraph-based multi-scale spatio-temporal graph convolution network\n  for Time-Series anomaly detection", "author": "Hongyi Xu", "abstract": "  Multivariate time series anomaly detection technology plays an important role\nin many fields including aerospace, water treatment, cloud service providers,\netc. Excellent anomaly detection models can greatly improve work efficiency and\navoid major economic losses. However, with the development of technology, the\nincreasing size and complexity of data, and the lack of labels for relevant\nabnormal data, it is becoming increasingly challenging to perform effective and\naccurate anomaly detection in high-dimensional and complex data sets. In this\npaper, we propose a hypergraph based spatiotemporal graph convolutional neural\nnetwork model STGCN_Hyper, which explicitly captures high-order, multi-hop\ncorrelations between multiple variables through a hypergraph based dynamic\ngraph structure learning module. On this basis, we further use the hypergraph\nbased spatiotemporal graph convolutional network to utilize the learned\nhypergraph structure to effectively propagate and aggregate one-hop and\nmulti-hop related node information in the convolutional network, thereby\nobtaining rich spatial information. Furthermore, through the multi-scale TCN\ndilated convolution module, the STGCN_hyper model can also capture the\ndependencies of features at different scales in the temporal dimension. An\nunsupervised anomaly detector based on PCA and GMM is also integrated into the\nSTGCN_hyper model. Through the anomaly score of the detector, the model can\ndetect the anomalies in an unsupervised way. Experimental results on multiple\ntime series datasets show that our model can flexibly learn the multi-scale\ntime series features in the data and the dependencies between features, and\noutperforms most existing baseline models in terms of precision, recall,\nF1-score on anomaly detection tasks. Our code is available on:\nhttps://git.ecdf.ed.ac.uk/msc-23-24/s2044819\n", "link": "http://arxiv.org/abs/2410.22256v1", "date": "2024-10-29", "relevancy": 2.4729, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5092}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4975}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraph-based%20multi-scale%20spatio-temporal%20graph%20convolution%20network%0A%20%20for%20Time-Series%20anomaly%20detection&body=Title%3A%20Hypergraph-based%20multi-scale%20spatio-temporal%20graph%20convolution%20network%0A%20%20for%20Time-Series%20anomaly%20detection%0AAuthor%3A%20Hongyi%20Xu%0AAbstract%3A%20%20%20Multivariate%20time%20series%20anomaly%20detection%20technology%20plays%20an%20important%20role%0Ain%20many%20fields%20including%20aerospace%2C%20water%20treatment%2C%20cloud%20service%20providers%2C%0Aetc.%20Excellent%20anomaly%20detection%20models%20can%20greatly%20improve%20work%20efficiency%20and%0Aavoid%20major%20economic%20losses.%20However%2C%20with%20the%20development%20of%20technology%2C%20the%0Aincreasing%20size%20and%20complexity%20of%20data%2C%20and%20the%20lack%20of%20labels%20for%20relevant%0Aabnormal%20data%2C%20it%20is%20becoming%20increasingly%20challenging%20to%20perform%20effective%20and%0Aaccurate%20anomaly%20detection%20in%20high-dimensional%20and%20complex%20data%20sets.%20In%20this%0Apaper%2C%20we%20propose%20a%20hypergraph%20based%20spatiotemporal%20graph%20convolutional%20neural%0Anetwork%20model%20STGCN_Hyper%2C%20which%20explicitly%20captures%20high-order%2C%20multi-hop%0Acorrelations%20between%20multiple%20variables%20through%20a%20hypergraph%20based%20dynamic%0Agraph%20structure%20learning%20module.%20On%20this%20basis%2C%20we%20further%20use%20the%20hypergraph%0Abased%20spatiotemporal%20graph%20convolutional%20network%20to%20utilize%20the%20learned%0Ahypergraph%20structure%20to%20effectively%20propagate%20and%20aggregate%20one-hop%20and%0Amulti-hop%20related%20node%20information%20in%20the%20convolutional%20network%2C%20thereby%0Aobtaining%20rich%20spatial%20information.%20Furthermore%2C%20through%20the%20multi-scale%20TCN%0Adilated%20convolution%20module%2C%20the%20STGCN_hyper%20model%20can%20also%20capture%20the%0Adependencies%20of%20features%20at%20different%20scales%20in%20the%20temporal%20dimension.%20An%0Aunsupervised%20anomaly%20detector%20based%20on%20PCA%20and%20GMM%20is%20also%20integrated%20into%20the%0ASTGCN_hyper%20model.%20Through%20the%20anomaly%20score%20of%20the%20detector%2C%20the%20model%20can%0Adetect%20the%20anomalies%20in%20an%20unsupervised%20way.%20Experimental%20results%20on%20multiple%0Atime%20series%20datasets%20show%20that%20our%20model%20can%20flexibly%20learn%20the%20multi-scale%0Atime%20series%20features%20in%20the%20data%20and%20the%20dependencies%20between%20features%2C%20and%0Aoutperforms%20most%20existing%20baseline%20models%20in%20terms%20of%20precision%2C%20recall%2C%0AF1-score%20on%20anomaly%20detection%20tasks.%20Our%20code%20is%20available%20on%3A%0Ahttps%3A//git.ecdf.ed.ac.uk/msc-23-24/s2044819%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraph-based%2520multi-scale%2520spatio-temporal%2520graph%2520convolution%2520network%250A%2520%2520for%2520Time-Series%2520anomaly%2520detection%26entry.906535625%3DHongyi%2520Xu%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520anomaly%2520detection%2520technology%2520plays%2520an%2520important%2520role%250Ain%2520many%2520fields%2520including%2520aerospace%252C%2520water%2520treatment%252C%2520cloud%2520service%2520providers%252C%250Aetc.%2520Excellent%2520anomaly%2520detection%2520models%2520can%2520greatly%2520improve%2520work%2520efficiency%2520and%250Aavoid%2520major%2520economic%2520losses.%2520However%252C%2520with%2520the%2520development%2520of%2520technology%252C%2520the%250Aincreasing%2520size%2520and%2520complexity%2520of%2520data%252C%2520and%2520the%2520lack%2520of%2520labels%2520for%2520relevant%250Aabnormal%2520data%252C%2520it%2520is%2520becoming%2520increasingly%2520challenging%2520to%2520perform%2520effective%2520and%250Aaccurate%2520anomaly%2520detection%2520in%2520high-dimensional%2520and%2520complex%2520data%2520sets.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520hypergraph%2520based%2520spatiotemporal%2520graph%2520convolutional%2520neural%250Anetwork%2520model%2520STGCN_Hyper%252C%2520which%2520explicitly%2520captures%2520high-order%252C%2520multi-hop%250Acorrelations%2520between%2520multiple%2520variables%2520through%2520a%2520hypergraph%2520based%2520dynamic%250Agraph%2520structure%2520learning%2520module.%2520On%2520this%2520basis%252C%2520we%2520further%2520use%2520the%2520hypergraph%250Abased%2520spatiotemporal%2520graph%2520convolutional%2520network%2520to%2520utilize%2520the%2520learned%250Ahypergraph%2520structure%2520to%2520effectively%2520propagate%2520and%2520aggregate%2520one-hop%2520and%250Amulti-hop%2520related%2520node%2520information%2520in%2520the%2520convolutional%2520network%252C%2520thereby%250Aobtaining%2520rich%2520spatial%2520information.%2520Furthermore%252C%2520through%2520the%2520multi-scale%2520TCN%250Adilated%2520convolution%2520module%252C%2520the%2520STGCN_hyper%2520model%2520can%2520also%2520capture%2520the%250Adependencies%2520of%2520features%2520at%2520different%2520scales%2520in%2520the%2520temporal%2520dimension.%2520An%250Aunsupervised%2520anomaly%2520detector%2520based%2520on%2520PCA%2520and%2520GMM%2520is%2520also%2520integrated%2520into%2520the%250ASTGCN_hyper%2520model.%2520Through%2520the%2520anomaly%2520score%2520of%2520the%2520detector%252C%2520the%2520model%2520can%250Adetect%2520the%2520anomalies%2520in%2520an%2520unsupervised%2520way.%2520Experimental%2520results%2520on%2520multiple%250Atime%2520series%2520datasets%2520show%2520that%2520our%2520model%2520can%2520flexibly%2520learn%2520the%2520multi-scale%250Atime%2520series%2520features%2520in%2520the%2520data%2520and%2520the%2520dependencies%2520between%2520features%252C%2520and%250Aoutperforms%2520most%2520existing%2520baseline%2520models%2520in%2520terms%2520of%2520precision%252C%2520recall%252C%250AF1-score%2520on%2520anomaly%2520detection%2520tasks.%2520Our%2520code%2520is%2520available%2520on%253A%250Ahttps%253A//git.ecdf.ed.ac.uk/msc-23-24/s2044819%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph-based%20multi-scale%20spatio-temporal%20graph%20convolution%20network%0A%20%20for%20Time-Series%20anomaly%20detection&entry.906535625=Hongyi%20Xu&entry.1292438233=%20%20Multivariate%20time%20series%20anomaly%20detection%20technology%20plays%20an%20important%20role%0Ain%20many%20fields%20including%20aerospace%2C%20water%20treatment%2C%20cloud%20service%20providers%2C%0Aetc.%20Excellent%20anomaly%20detection%20models%20can%20greatly%20improve%20work%20efficiency%20and%0Aavoid%20major%20economic%20losses.%20However%2C%20with%20the%20development%20of%20technology%2C%20the%0Aincreasing%20size%20and%20complexity%20of%20data%2C%20and%20the%20lack%20of%20labels%20for%20relevant%0Aabnormal%20data%2C%20it%20is%20becoming%20increasingly%20challenging%20to%20perform%20effective%20and%0Aaccurate%20anomaly%20detection%20in%20high-dimensional%20and%20complex%20data%20sets.%20In%20this%0Apaper%2C%20we%20propose%20a%20hypergraph%20based%20spatiotemporal%20graph%20convolutional%20neural%0Anetwork%20model%20STGCN_Hyper%2C%20which%20explicitly%20captures%20high-order%2C%20multi-hop%0Acorrelations%20between%20multiple%20variables%20through%20a%20hypergraph%20based%20dynamic%0Agraph%20structure%20learning%20module.%20On%20this%20basis%2C%20we%20further%20use%20the%20hypergraph%0Abased%20spatiotemporal%20graph%20convolutional%20network%20to%20utilize%20the%20learned%0Ahypergraph%20structure%20to%20effectively%20propagate%20and%20aggregate%20one-hop%20and%0Amulti-hop%20related%20node%20information%20in%20the%20convolutional%20network%2C%20thereby%0Aobtaining%20rich%20spatial%20information.%20Furthermore%2C%20through%20the%20multi-scale%20TCN%0Adilated%20convolution%20module%2C%20the%20STGCN_hyper%20model%20can%20also%20capture%20the%0Adependencies%20of%20features%20at%20different%20scales%20in%20the%20temporal%20dimension.%20An%0Aunsupervised%20anomaly%20detector%20based%20on%20PCA%20and%20GMM%20is%20also%20integrated%20into%20the%0ASTGCN_hyper%20model.%20Through%20the%20anomaly%20score%20of%20the%20detector%2C%20the%20model%20can%0Adetect%20the%20anomalies%20in%20an%20unsupervised%20way.%20Experimental%20results%20on%20multiple%0Atime%20series%20datasets%20show%20that%20our%20model%20can%20flexibly%20learn%20the%20multi-scale%0Atime%20series%20features%20in%20the%20data%20and%20the%20dependencies%20between%20features%2C%20and%0Aoutperforms%20most%20existing%20baseline%20models%20in%20terms%20of%20precision%2C%20recall%2C%0AF1-score%20on%20anomaly%20detection%20tasks.%20Our%20code%20is%20available%20on%3A%0Ahttps%3A//git.ecdf.ed.ac.uk/msc-23-24/s2044819%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22256v1&entry.124074799=Read"},
{"title": "Integration of Large Language Models and Federated Learning", "author": "Chaochao Chen and Xiaohua Feng and Yuyuan Li and Lingjuan Lyu and Jun Zhou and Xiaolin Zheng and Jianwei Yin", "abstract": "  As the parameter size of Large Language Models (LLMs) continues to expand,\nthere is an urgent need to address the scarcity of high-quality data. In\nresponse, existing research has attempted to make a breakthrough by\nincorporating Federated Learning (FL) into LLMs. Conversely, considering the\noutstanding performance of LLMs in task generalization, researchers have also\ntried applying LLMs within FL to tackle challenges in relevant domains. The\ncomplementarity between LLMs and FL has already ignited widespread research\ninterest. In this paper, we aim to deeply explore the integration of LLMs and\nFL. We propose a research framework, dividing the fusion of LLMs and FL into\nthree parts: the combination of LLM sub-technologies with FL, the integration\nof FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We\nfirst provide a comprehensive review of the current state of research in the\ndomain of LLMs combined with FL, including their typical applications,\nintegration advantages, challenges faced, and future directions for resolution.\nSubsequently, we discuss the practical applications of the combination of LLMs\nand FL in critical scenarios such as healthcare, finance, and education, and\nprovide new perspectives and insights into future research directions for LLMs\nand FL.\n", "link": "http://arxiv.org/abs/2307.08925v2", "date": "2024-10-29", "relevancy": 2.4641, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20Large%20Language%20Models%20and%20Federated%20Learning&body=Title%3A%20Integration%20of%20Large%20Language%20Models%20and%20Federated%20Learning%0AAuthor%3A%20Chaochao%20Chen%20and%20Xiaohua%20Feng%20and%20Yuyuan%20Li%20and%20Lingjuan%20Lyu%20and%20Jun%20Zhou%20and%20Xiaolin%20Zheng%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20As%20the%20parameter%20size%20of%20Large%20Language%20Models%20%28LLMs%29%20continues%20to%20expand%2C%0Athere%20is%20an%20urgent%20need%20to%20address%20the%20scarcity%20of%20high-quality%20data.%20In%0Aresponse%2C%20existing%20research%20has%20attempted%20to%20make%20a%20breakthrough%20by%0Aincorporating%20Federated%20Learning%20%28FL%29%20into%20LLMs.%20Conversely%2C%20considering%20the%0Aoutstanding%20performance%20of%20LLMs%20in%20task%20generalization%2C%20researchers%20have%20also%0Atried%20applying%20LLMs%20within%20FL%20to%20tackle%20challenges%20in%20relevant%20domains.%20The%0Acomplementarity%20between%20LLMs%20and%20FL%20has%20already%20ignited%20widespread%20research%0Ainterest.%20In%20this%20paper%2C%20we%20aim%20to%20deeply%20explore%20the%20integration%20of%20LLMs%20and%0AFL.%20We%20propose%20a%20research%20framework%2C%20dividing%20the%20fusion%20of%20LLMs%20and%20FL%20into%0Athree%20parts%3A%20the%20combination%20of%20LLM%20sub-technologies%20with%20FL%2C%20the%20integration%0Aof%20FL%20sub-technologies%20with%20LLMs%2C%20and%20the%20overall%20merger%20of%20LLMs%20and%20FL.%20We%0Afirst%20provide%20a%20comprehensive%20review%20of%20the%20current%20state%20of%20research%20in%20the%0Adomain%20of%20LLMs%20combined%20with%20FL%2C%20including%20their%20typical%20applications%2C%0Aintegration%20advantages%2C%20challenges%20faced%2C%20and%20future%20directions%20for%20resolution.%0ASubsequently%2C%20we%20discuss%20the%20practical%20applications%20of%20the%20combination%20of%20LLMs%0Aand%20FL%20in%20critical%20scenarios%20such%20as%20healthcare%2C%20finance%2C%20and%20education%2C%20and%0Aprovide%20new%20perspectives%20and%20insights%20into%20future%20research%20directions%20for%20LLMs%0Aand%20FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520Large%2520Language%2520Models%2520and%2520Federated%2520Learning%26entry.906535625%3DChaochao%2520Chen%2520and%2520Xiaohua%2520Feng%2520and%2520Yuyuan%2520Li%2520and%2520Lingjuan%2520Lyu%2520and%2520Jun%2520Zhou%2520and%2520Xiaolin%2520Zheng%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520As%2520the%2520parameter%2520size%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520continues%2520to%2520expand%252C%250Athere%2520is%2520an%2520urgent%2520need%2520to%2520address%2520the%2520scarcity%2520of%2520high-quality%2520data.%2520In%250Aresponse%252C%2520existing%2520research%2520has%2520attempted%2520to%2520make%2520a%2520breakthrough%2520by%250Aincorporating%2520Federated%2520Learning%2520%2528FL%2529%2520into%2520LLMs.%2520Conversely%252C%2520considering%2520the%250Aoutstanding%2520performance%2520of%2520LLMs%2520in%2520task%2520generalization%252C%2520researchers%2520have%2520also%250Atried%2520applying%2520LLMs%2520within%2520FL%2520to%2520tackle%2520challenges%2520in%2520relevant%2520domains.%2520The%250Acomplementarity%2520between%2520LLMs%2520and%2520FL%2520has%2520already%2520ignited%2520widespread%2520research%250Ainterest.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520deeply%2520explore%2520the%2520integration%2520of%2520LLMs%2520and%250AFL.%2520We%2520propose%2520a%2520research%2520framework%252C%2520dividing%2520the%2520fusion%2520of%2520LLMs%2520and%2520FL%2520into%250Athree%2520parts%253A%2520the%2520combination%2520of%2520LLM%2520sub-technologies%2520with%2520FL%252C%2520the%2520integration%250Aof%2520FL%2520sub-technologies%2520with%2520LLMs%252C%2520and%2520the%2520overall%2520merger%2520of%2520LLMs%2520and%2520FL.%2520We%250Afirst%2520provide%2520a%2520comprehensive%2520review%2520of%2520the%2520current%2520state%2520of%2520research%2520in%2520the%250Adomain%2520of%2520LLMs%2520combined%2520with%2520FL%252C%2520including%2520their%2520typical%2520applications%252C%250Aintegration%2520advantages%252C%2520challenges%2520faced%252C%2520and%2520future%2520directions%2520for%2520resolution.%250ASubsequently%252C%2520we%2520discuss%2520the%2520practical%2520applications%2520of%2520the%2520combination%2520of%2520LLMs%250Aand%2520FL%2520in%2520critical%2520scenarios%2520such%2520as%2520healthcare%252C%2520finance%252C%2520and%2520education%252C%2520and%250Aprovide%2520new%2520perspectives%2520and%2520insights%2520into%2520future%2520research%2520directions%2520for%2520LLMs%250Aand%2520FL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.08925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20Large%20Language%20Models%20and%20Federated%20Learning&entry.906535625=Chaochao%20Chen%20and%20Xiaohua%20Feng%20and%20Yuyuan%20Li%20and%20Lingjuan%20Lyu%20and%20Jun%20Zhou%20and%20Xiaolin%20Zheng%20and%20Jianwei%20Yin&entry.1292438233=%20%20As%20the%20parameter%20size%20of%20Large%20Language%20Models%20%28LLMs%29%20continues%20to%20expand%2C%0Athere%20is%20an%20urgent%20need%20to%20address%20the%20scarcity%20of%20high-quality%20data.%20In%0Aresponse%2C%20existing%20research%20has%20attempted%20to%20make%20a%20breakthrough%20by%0Aincorporating%20Federated%20Learning%20%28FL%29%20into%20LLMs.%20Conversely%2C%20considering%20the%0Aoutstanding%20performance%20of%20LLMs%20in%20task%20generalization%2C%20researchers%20have%20also%0Atried%20applying%20LLMs%20within%20FL%20to%20tackle%20challenges%20in%20relevant%20domains.%20The%0Acomplementarity%20between%20LLMs%20and%20FL%20has%20already%20ignited%20widespread%20research%0Ainterest.%20In%20this%20paper%2C%20we%20aim%20to%20deeply%20explore%20the%20integration%20of%20LLMs%20and%0AFL.%20We%20propose%20a%20research%20framework%2C%20dividing%20the%20fusion%20of%20LLMs%20and%20FL%20into%0Athree%20parts%3A%20the%20combination%20of%20LLM%20sub-technologies%20with%20FL%2C%20the%20integration%0Aof%20FL%20sub-technologies%20with%20LLMs%2C%20and%20the%20overall%20merger%20of%20LLMs%20and%20FL.%20We%0Afirst%20provide%20a%20comprehensive%20review%20of%20the%20current%20state%20of%20research%20in%20the%0Adomain%20of%20LLMs%20combined%20with%20FL%2C%20including%20their%20typical%20applications%2C%0Aintegration%20advantages%2C%20challenges%20faced%2C%20and%20future%20directions%20for%20resolution.%0ASubsequently%2C%20we%20discuss%20the%20practical%20applications%20of%20the%20combination%20of%20LLMs%0Aand%20FL%20in%20critical%20scenarios%20such%20as%20healthcare%2C%20finance%2C%20and%20education%2C%20and%0Aprovide%20new%20perspectives%20and%20insights%20into%20future%20research%20directions%20for%20LLMs%0Aand%20FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08925v2&entry.124074799=Read"},
{"title": "Feature distribution Adaptation Network for Speech Emotion Recognition", "author": "Shaokai Li and Yixuan Ji and Peng Song and Haoqin Sun and Wenming Zheng", "abstract": "  In this paper, we propose a novel deep inductive transfer learning framework,\nnamed feature distribution adaptation network, to tackle the challenging\nmulti-modal speech emotion recognition problem. Our method aims to use deep\ntransfer learning strategies to align visual and audio feature distributions to\nobtain consistent representation of emotion, thereby improving the performance\nof speech emotion recognition. In our model, the pre-trained ResNet-34 is\nutilized for feature extraction for facial expression images and acoustic Mel\nspectrograms, respectively. Then, the cross-attention mechanism is introduced\nto model the intrinsic similarity relationships of multi-modal features.\nFinally, the multi-modal feature distribution adaptation is performed\nefficiently with feed-forward network, which is extended using the local\nmaximum mean discrepancy loss. Experiments are carried out on two benchmark\ndatasets, and the results demonstrate that our model can achieve excellent\nperformance compared with existing ones.Our code is available at\nhttps://github.com/shaokai1209/FDAN.\n", "link": "http://arxiv.org/abs/2410.22023v1", "date": "2024-10-29", "relevancy": 2.4611, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5075}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4859}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20distribution%20Adaptation%20Network%20for%20Speech%20Emotion%20Recognition&body=Title%3A%20Feature%20distribution%20Adaptation%20Network%20for%20Speech%20Emotion%20Recognition%0AAuthor%3A%20Shaokai%20Li%20and%20Yixuan%20Ji%20and%20Peng%20Song%20and%20Haoqin%20Sun%20and%20Wenming%20Zheng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20deep%20inductive%20transfer%20learning%20framework%2C%0Anamed%20feature%20distribution%20adaptation%20network%2C%20to%20tackle%20the%20challenging%0Amulti-modal%20speech%20emotion%20recognition%20problem.%20Our%20method%20aims%20to%20use%20deep%0Atransfer%20learning%20strategies%20to%20align%20visual%20and%20audio%20feature%20distributions%20to%0Aobtain%20consistent%20representation%20of%20emotion%2C%20thereby%20improving%20the%20performance%0Aof%20speech%20emotion%20recognition.%20In%20our%20model%2C%20the%20pre-trained%20ResNet-34%20is%0Autilized%20for%20feature%20extraction%20for%20facial%20expression%20images%20and%20acoustic%20Mel%0Aspectrograms%2C%20respectively.%20Then%2C%20the%20cross-attention%20mechanism%20is%20introduced%0Ato%20model%20the%20intrinsic%20similarity%20relationships%20of%20multi-modal%20features.%0AFinally%2C%20the%20multi-modal%20feature%20distribution%20adaptation%20is%20performed%0Aefficiently%20with%20feed-forward%20network%2C%20which%20is%20extended%20using%20the%20local%0Amaximum%20mean%20discrepancy%20loss.%20Experiments%20are%20carried%20out%20on%20two%20benchmark%0Adatasets%2C%20and%20the%20results%20demonstrate%20that%20our%20model%20can%20achieve%20excellent%0Aperformance%20compared%20with%20existing%20ones.Our%20code%20is%20available%20at%0Ahttps%3A//github.com/shaokai1209/FDAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520distribution%2520Adaptation%2520Network%2520for%2520Speech%2520Emotion%2520Recognition%26entry.906535625%3DShaokai%2520Li%2520and%2520Yixuan%2520Ji%2520and%2520Peng%2520Song%2520and%2520Haoqin%2520Sun%2520and%2520Wenming%2520Zheng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520deep%2520inductive%2520transfer%2520learning%2520framework%252C%250Anamed%2520feature%2520distribution%2520adaptation%2520network%252C%2520to%2520tackle%2520the%2520challenging%250Amulti-modal%2520speech%2520emotion%2520recognition%2520problem.%2520Our%2520method%2520aims%2520to%2520use%2520deep%250Atransfer%2520learning%2520strategies%2520to%2520align%2520visual%2520and%2520audio%2520feature%2520distributions%2520to%250Aobtain%2520consistent%2520representation%2520of%2520emotion%252C%2520thereby%2520improving%2520the%2520performance%250Aof%2520speech%2520emotion%2520recognition.%2520In%2520our%2520model%252C%2520the%2520pre-trained%2520ResNet-34%2520is%250Autilized%2520for%2520feature%2520extraction%2520for%2520facial%2520expression%2520images%2520and%2520acoustic%2520Mel%250Aspectrograms%252C%2520respectively.%2520Then%252C%2520the%2520cross-attention%2520mechanism%2520is%2520introduced%250Ato%2520model%2520the%2520intrinsic%2520similarity%2520relationships%2520of%2520multi-modal%2520features.%250AFinally%252C%2520the%2520multi-modal%2520feature%2520distribution%2520adaptation%2520is%2520performed%250Aefficiently%2520with%2520feed-forward%2520network%252C%2520which%2520is%2520extended%2520using%2520the%2520local%250Amaximum%2520mean%2520discrepancy%2520loss.%2520Experiments%2520are%2520carried%2520out%2520on%2520two%2520benchmark%250Adatasets%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520our%2520model%2520can%2520achieve%2520excellent%250Aperformance%2520compared%2520with%2520existing%2520ones.Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shaokai1209/FDAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20distribution%20Adaptation%20Network%20for%20Speech%20Emotion%20Recognition&entry.906535625=Shaokai%20Li%20and%20Yixuan%20Ji%20and%20Peng%20Song%20and%20Haoqin%20Sun%20and%20Wenming%20Zheng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20deep%20inductive%20transfer%20learning%20framework%2C%0Anamed%20feature%20distribution%20adaptation%20network%2C%20to%20tackle%20the%20challenging%0Amulti-modal%20speech%20emotion%20recognition%20problem.%20Our%20method%20aims%20to%20use%20deep%0Atransfer%20learning%20strategies%20to%20align%20visual%20and%20audio%20feature%20distributions%20to%0Aobtain%20consistent%20representation%20of%20emotion%2C%20thereby%20improving%20the%20performance%0Aof%20speech%20emotion%20recognition.%20In%20our%20model%2C%20the%20pre-trained%20ResNet-34%20is%0Autilized%20for%20feature%20extraction%20for%20facial%20expression%20images%20and%20acoustic%20Mel%0Aspectrograms%2C%20respectively.%20Then%2C%20the%20cross-attention%20mechanism%20is%20introduced%0Ato%20model%20the%20intrinsic%20similarity%20relationships%20of%20multi-modal%20features.%0AFinally%2C%20the%20multi-modal%20feature%20distribution%20adaptation%20is%20performed%0Aefficiently%20with%20feed-forward%20network%2C%20which%20is%20extended%20using%20the%20local%0Amaximum%20mean%20discrepancy%20loss.%20Experiments%20are%20carried%20out%20on%20two%20benchmark%0Adatasets%2C%20and%20the%20results%20demonstrate%20that%20our%20model%20can%20achieve%20excellent%0Aperformance%20compared%20with%20existing%20ones.Our%20code%20is%20available%20at%0Ahttps%3A//github.com/shaokai1209/FDAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22023v1&entry.124074799=Read"},
{"title": "What Makes ImageNet Look Unlike LAION", "author": "Ali Shirali and Moritz Hardt", "abstract": "  ImageNet was famously created from Flickr image search results. What if we\nrecreated ImageNet instead by searching the massive LAION dataset based on\nimage captions alone? In this work, we carry out this counterfactual\ninvestigation. We find that the resulting ImageNet recreation, which we call\nLAIONet, looks distinctly unlike the original. Specifically, the intra-class\nsimilarity of images in the original ImageNet is dramatically higher than it is\nfor LAIONet. Consequently, models trained on ImageNet perform significantly\nworse on LAIONet. We propose a rigorous explanation for the discrepancy in\nterms of a subtle, yet important, difference in two plausible causal\ndata-generating processes for the respective datasets, that we support with\nsystematic experimentation. In a nutshell, searching based on an image caption\nalone creates an information bottleneck that mitigates the selection bias\notherwise present in image-based filtering. Our explanation formalizes a\nlong-held intuition in the community that ImageNet images are stereotypical,\nunnatural, and overly simple representations of the class category. At the same\ntime, it provides a simple and actionable takeaway for future dataset creation\nefforts.\n", "link": "http://arxiv.org/abs/2306.15769v2", "date": "2024-10-29", "relevancy": 2.4548, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20ImageNet%20Look%20Unlike%20LAION&body=Title%3A%20What%20Makes%20ImageNet%20Look%20Unlike%20LAION%0AAuthor%3A%20Ali%20Shirali%20and%20Moritz%20Hardt%0AAbstract%3A%20%20%20ImageNet%20was%20famously%20created%20from%20Flickr%20image%20search%20results.%20What%20if%20we%0Arecreated%20ImageNet%20instead%20by%20searching%20the%20massive%20LAION%20dataset%20based%20on%0Aimage%20captions%20alone%3F%20In%20this%20work%2C%20we%20carry%20out%20this%20counterfactual%0Ainvestigation.%20We%20find%20that%20the%20resulting%20ImageNet%20recreation%2C%20which%20we%20call%0ALAIONet%2C%20looks%20distinctly%20unlike%20the%20original.%20Specifically%2C%20the%20intra-class%0Asimilarity%20of%20images%20in%20the%20original%20ImageNet%20is%20dramatically%20higher%20than%20it%20is%0Afor%20LAIONet.%20Consequently%2C%20models%20trained%20on%20ImageNet%20perform%20significantly%0Aworse%20on%20LAIONet.%20We%20propose%20a%20rigorous%20explanation%20for%20the%20discrepancy%20in%0Aterms%20of%20a%20subtle%2C%20yet%20important%2C%20difference%20in%20two%20plausible%20causal%0Adata-generating%20processes%20for%20the%20respective%20datasets%2C%20that%20we%20support%20with%0Asystematic%20experimentation.%20In%20a%20nutshell%2C%20searching%20based%20on%20an%20image%20caption%0Aalone%20creates%20an%20information%20bottleneck%20that%20mitigates%20the%20selection%20bias%0Aotherwise%20present%20in%20image-based%20filtering.%20Our%20explanation%20formalizes%20a%0Along-held%20intuition%20in%20the%20community%20that%20ImageNet%20images%20are%20stereotypical%2C%0Aunnatural%2C%20and%20overly%20simple%20representations%20of%20the%20class%20category.%20At%20the%20same%0Atime%2C%20it%20provides%20a%20simple%20and%20actionable%20takeaway%20for%20future%20dataset%20creation%0Aefforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.15769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520ImageNet%2520Look%2520Unlike%2520LAION%26entry.906535625%3DAli%2520Shirali%2520and%2520Moritz%2520Hardt%26entry.1292438233%3D%2520%2520ImageNet%2520was%2520famously%2520created%2520from%2520Flickr%2520image%2520search%2520results.%2520What%2520if%2520we%250Arecreated%2520ImageNet%2520instead%2520by%2520searching%2520the%2520massive%2520LAION%2520dataset%2520based%2520on%250Aimage%2520captions%2520alone%253F%2520In%2520this%2520work%252C%2520we%2520carry%2520out%2520this%2520counterfactual%250Ainvestigation.%2520We%2520find%2520that%2520the%2520resulting%2520ImageNet%2520recreation%252C%2520which%2520we%2520call%250ALAIONet%252C%2520looks%2520distinctly%2520unlike%2520the%2520original.%2520Specifically%252C%2520the%2520intra-class%250Asimilarity%2520of%2520images%2520in%2520the%2520original%2520ImageNet%2520is%2520dramatically%2520higher%2520than%2520it%2520is%250Afor%2520LAIONet.%2520Consequently%252C%2520models%2520trained%2520on%2520ImageNet%2520perform%2520significantly%250Aworse%2520on%2520LAIONet.%2520We%2520propose%2520a%2520rigorous%2520explanation%2520for%2520the%2520discrepancy%2520in%250Aterms%2520of%2520a%2520subtle%252C%2520yet%2520important%252C%2520difference%2520in%2520two%2520plausible%2520causal%250Adata-generating%2520processes%2520for%2520the%2520respective%2520datasets%252C%2520that%2520we%2520support%2520with%250Asystematic%2520experimentation.%2520In%2520a%2520nutshell%252C%2520searching%2520based%2520on%2520an%2520image%2520caption%250Aalone%2520creates%2520an%2520information%2520bottleneck%2520that%2520mitigates%2520the%2520selection%2520bias%250Aotherwise%2520present%2520in%2520image-based%2520filtering.%2520Our%2520explanation%2520formalizes%2520a%250Along-held%2520intuition%2520in%2520the%2520community%2520that%2520ImageNet%2520images%2520are%2520stereotypical%252C%250Aunnatural%252C%2520and%2520overly%2520simple%2520representations%2520of%2520the%2520class%2520category.%2520At%2520the%2520same%250Atime%252C%2520it%2520provides%2520a%2520simple%2520and%2520actionable%2520takeaway%2520for%2520future%2520dataset%2520creation%250Aefforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.15769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20ImageNet%20Look%20Unlike%20LAION&entry.906535625=Ali%20Shirali%20and%20Moritz%20Hardt&entry.1292438233=%20%20ImageNet%20was%20famously%20created%20from%20Flickr%20image%20search%20results.%20What%20if%20we%0Arecreated%20ImageNet%20instead%20by%20searching%20the%20massive%20LAION%20dataset%20based%20on%0Aimage%20captions%20alone%3F%20In%20this%20work%2C%20we%20carry%20out%20this%20counterfactual%0Ainvestigation.%20We%20find%20that%20the%20resulting%20ImageNet%20recreation%2C%20which%20we%20call%0ALAIONet%2C%20looks%20distinctly%20unlike%20the%20original.%20Specifically%2C%20the%20intra-class%0Asimilarity%20of%20images%20in%20the%20original%20ImageNet%20is%20dramatically%20higher%20than%20it%20is%0Afor%20LAIONet.%20Consequently%2C%20models%20trained%20on%20ImageNet%20perform%20significantly%0Aworse%20on%20LAIONet.%20We%20propose%20a%20rigorous%20explanation%20for%20the%20discrepancy%20in%0Aterms%20of%20a%20subtle%2C%20yet%20important%2C%20difference%20in%20two%20plausible%20causal%0Adata-generating%20processes%20for%20the%20respective%20datasets%2C%20that%20we%20support%20with%0Asystematic%20experimentation.%20In%20a%20nutshell%2C%20searching%20based%20on%20an%20image%20caption%0Aalone%20creates%20an%20information%20bottleneck%20that%20mitigates%20the%20selection%20bias%0Aotherwise%20present%20in%20image-based%20filtering.%20Our%20explanation%20formalizes%20a%0Along-held%20intuition%20in%20the%20community%20that%20ImageNet%20images%20are%20stereotypical%2C%0Aunnatural%2C%20and%20overly%20simple%20representations%20of%20the%20class%20category.%20At%20the%20same%0Atime%2C%20it%20provides%20a%20simple%20and%20actionable%20takeaway%20for%20future%20dataset%20creation%0Aefforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.15769v2&entry.124074799=Read"},
{"title": "Natural Language Inference Improves Compositionality in Vision-Language\n  Models", "author": "Paola Cascante-Bonilla and Yu Hou and Yang Trista Cao and Hal Daum\u00e9 III and Rachel Rudinger", "abstract": "  Compositional reasoning in Vision-Language Models (VLMs) remains challenging\nas these models often struggle to relate objects, attributes, and spatial\nrelationships. Recent methods aim to address these limitations by relying on\nthe semantics of the textual description, using Large Language Models (LLMs) to\nbreak them down into subsets of questions and answers. However, these methods\nprimarily operate on the surface level, failing to incorporate deeper lexical\nunderstanding while introducing incorrect assumptions generated by the LLM. In\nresponse to these issues, we present Caption Expansion with Contradictions and\nEntailments (CECE), a principled approach that leverages Natural Language\nInference (NLI) to generate entailments and contradictions from a given\npremise. CECE produces lexically diverse sentences while maintaining their core\nmeaning. Through extensive experiments, we show that CECE enhances\ninterpretability and reduces overreliance on biased or superficial features. By\nbalancing CECE along the original premise, we achieve significant improvements\nover previous methods without requiring additional fine-tuning, producing\nstate-of-the-art results on benchmarks that score agreement with human\njudgments for image-text alignment, and achieving an increase in performance on\nWinoground of +19.2% (group score) and +12.9% on EqBen (group score) over the\nbest prior work (finetuned with targeted data).\n", "link": "http://arxiv.org/abs/2410.22315v1", "date": "2024-10-29", "relevancy": 2.4423, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Inference%20Improves%20Compositionality%20in%20Vision-Language%0A%20%20Models&body=Title%3A%20Natural%20Language%20Inference%20Improves%20Compositionality%20in%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Paola%20Cascante-Bonilla%20and%20Yu%20Hou%20and%20Yang%20Trista%20Cao%20and%20Hal%20Daum%C3%A9%20III%20and%20Rachel%20Rudinger%0AAbstract%3A%20%20%20Compositional%20reasoning%20in%20Vision-Language%20Models%20%28VLMs%29%20remains%20challenging%0Aas%20these%20models%20often%20struggle%20to%20relate%20objects%2C%20attributes%2C%20and%20spatial%0Arelationships.%20Recent%20methods%20aim%20to%20address%20these%20limitations%20by%20relying%20on%0Athe%20semantics%20of%20the%20textual%20description%2C%20using%20Large%20Language%20Models%20%28LLMs%29%20to%0Abreak%20them%20down%20into%20subsets%20of%20questions%20and%20answers.%20However%2C%20these%20methods%0Aprimarily%20operate%20on%20the%20surface%20level%2C%20failing%20to%20incorporate%20deeper%20lexical%0Aunderstanding%20while%20introducing%20incorrect%20assumptions%20generated%20by%20the%20LLM.%20In%0Aresponse%20to%20these%20issues%2C%20we%20present%20Caption%20Expansion%20with%20Contradictions%20and%0AEntailments%20%28CECE%29%2C%20a%20principled%20approach%20that%20leverages%20Natural%20Language%0AInference%20%28NLI%29%20to%20generate%20entailments%20and%20contradictions%20from%20a%20given%0Apremise.%20CECE%20produces%20lexically%20diverse%20sentences%20while%20maintaining%20their%20core%0Ameaning.%20Through%20extensive%20experiments%2C%20we%20show%20that%20CECE%20enhances%0Ainterpretability%20and%20reduces%20overreliance%20on%20biased%20or%20superficial%20features.%20By%0Abalancing%20CECE%20along%20the%20original%20premise%2C%20we%20achieve%20significant%20improvements%0Aover%20previous%20methods%20without%20requiring%20additional%20fine-tuning%2C%20producing%0Astate-of-the-art%20results%20on%20benchmarks%20that%20score%20agreement%20with%20human%0Ajudgments%20for%20image-text%20alignment%2C%20and%20achieving%20an%20increase%20in%20performance%20on%0AWinoground%20of%20%2B19.2%25%20%28group%20score%29%20and%20%2B12.9%25%20on%20EqBen%20%28group%20score%29%20over%20the%0Abest%20prior%20work%20%28finetuned%20with%20targeted%20data%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Inference%2520Improves%2520Compositionality%2520in%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DPaola%2520Cascante-Bonilla%2520and%2520Yu%2520Hou%2520and%2520Yang%2520Trista%2520Cao%2520and%2520Hal%2520Daum%25C3%25A9%2520III%2520and%2520Rachel%2520Rudinger%26entry.1292438233%3D%2520%2520Compositional%2520reasoning%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520remains%2520challenging%250Aas%2520these%2520models%2520often%2520struggle%2520to%2520relate%2520objects%252C%2520attributes%252C%2520and%2520spatial%250Arelationships.%2520Recent%2520methods%2520aim%2520to%2520address%2520these%2520limitations%2520by%2520relying%2520on%250Athe%2520semantics%2520of%2520the%2520textual%2520description%252C%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Abreak%2520them%2520down%2520into%2520subsets%2520of%2520questions%2520and%2520answers.%2520However%252C%2520these%2520methods%250Aprimarily%2520operate%2520on%2520the%2520surface%2520level%252C%2520failing%2520to%2520incorporate%2520deeper%2520lexical%250Aunderstanding%2520while%2520introducing%2520incorrect%2520assumptions%2520generated%2520by%2520the%2520LLM.%2520In%250Aresponse%2520to%2520these%2520issues%252C%2520we%2520present%2520Caption%2520Expansion%2520with%2520Contradictions%2520and%250AEntailments%2520%2528CECE%2529%252C%2520a%2520principled%2520approach%2520that%2520leverages%2520Natural%2520Language%250AInference%2520%2528NLI%2529%2520to%2520generate%2520entailments%2520and%2520contradictions%2520from%2520a%2520given%250Apremise.%2520CECE%2520produces%2520lexically%2520diverse%2520sentences%2520while%2520maintaining%2520their%2520core%250Ameaning.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520CECE%2520enhances%250Ainterpretability%2520and%2520reduces%2520overreliance%2520on%2520biased%2520or%2520superficial%2520features.%2520By%250Abalancing%2520CECE%2520along%2520the%2520original%2520premise%252C%2520we%2520achieve%2520significant%2520improvements%250Aover%2520previous%2520methods%2520without%2520requiring%2520additional%2520fine-tuning%252C%2520producing%250Astate-of-the-art%2520results%2520on%2520benchmarks%2520that%2520score%2520agreement%2520with%2520human%250Ajudgments%2520for%2520image-text%2520alignment%252C%2520and%2520achieving%2520an%2520increase%2520in%2520performance%2520on%250AWinoground%2520of%2520%252B19.2%2525%2520%2528group%2520score%2529%2520and%2520%252B12.9%2525%2520on%2520EqBen%2520%2528group%2520score%2529%2520over%2520the%250Abest%2520prior%2520work%2520%2528finetuned%2520with%2520targeted%2520data%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Inference%20Improves%20Compositionality%20in%20Vision-Language%0A%20%20Models&entry.906535625=Paola%20Cascante-Bonilla%20and%20Yu%20Hou%20and%20Yang%20Trista%20Cao%20and%20Hal%20Daum%C3%A9%20III%20and%20Rachel%20Rudinger&entry.1292438233=%20%20Compositional%20reasoning%20in%20Vision-Language%20Models%20%28VLMs%29%20remains%20challenging%0Aas%20these%20models%20often%20struggle%20to%20relate%20objects%2C%20attributes%2C%20and%20spatial%0Arelationships.%20Recent%20methods%20aim%20to%20address%20these%20limitations%20by%20relying%20on%0Athe%20semantics%20of%20the%20textual%20description%2C%20using%20Large%20Language%20Models%20%28LLMs%29%20to%0Abreak%20them%20down%20into%20subsets%20of%20questions%20and%20answers.%20However%2C%20these%20methods%0Aprimarily%20operate%20on%20the%20surface%20level%2C%20failing%20to%20incorporate%20deeper%20lexical%0Aunderstanding%20while%20introducing%20incorrect%20assumptions%20generated%20by%20the%20LLM.%20In%0Aresponse%20to%20these%20issues%2C%20we%20present%20Caption%20Expansion%20with%20Contradictions%20and%0AEntailments%20%28CECE%29%2C%20a%20principled%20approach%20that%20leverages%20Natural%20Language%0AInference%20%28NLI%29%20to%20generate%20entailments%20and%20contradictions%20from%20a%20given%0Apremise.%20CECE%20produces%20lexically%20diverse%20sentences%20while%20maintaining%20their%20core%0Ameaning.%20Through%20extensive%20experiments%2C%20we%20show%20that%20CECE%20enhances%0Ainterpretability%20and%20reduces%20overreliance%20on%20biased%20or%20superficial%20features.%20By%0Abalancing%20CECE%20along%20the%20original%20premise%2C%20we%20achieve%20significant%20improvements%0Aover%20previous%20methods%20without%20requiring%20additional%20fine-tuning%2C%20producing%0Astate-of-the-art%20results%20on%20benchmarks%20that%20score%20agreement%20with%20human%0Ajudgments%20for%20image-text%20alignment%2C%20and%20achieving%20an%20increase%20in%20performance%20on%0AWinoground%20of%20%2B19.2%25%20%28group%20score%29%20and%20%2B12.9%25%20on%20EqBen%20%28group%20score%29%20over%20the%0Abest%20prior%20work%20%28finetuned%20with%20targeted%20data%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22315v1&entry.124074799=Read"},
{"title": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective", "author": "Shenghao Xie and Wenqiang Zu and Mingyang Zhao and Duo Su and Shilong Liu and Ruohua Shi and Guoqi Li and Shanghang Zhang and Lei Ma", "abstract": "  Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM.\n", "link": "http://arxiv.org/abs/2410.22217v1", "date": "2024-10-29", "relevancy": 2.4077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6176}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unifying%20Understanding%20and%20Generation%20in%20the%20Era%20of%20Vision%0A%20%20Foundation%20Models%3A%20A%20Survey%20from%20the%20Autoregression%20Perspective&body=Title%3A%20Towards%20Unifying%20Understanding%20and%20Generation%20in%20the%20Era%20of%20Vision%0A%20%20Foundation%20Models%3A%20A%20Survey%20from%20the%20Autoregression%20Perspective%0AAuthor%3A%20Shenghao%20Xie%20and%20Wenqiang%20Zu%20and%20Mingyang%20Zhao%20and%20Duo%20Su%20and%20Shilong%20Liu%20and%20Ruohua%20Shi%20and%20Guoqi%20Li%20and%20Shanghang%20Zhang%20and%20Lei%20Ma%0AAbstract%3A%20%20%20Autoregression%20in%20large%20language%20models%20%28LLMs%29%20has%20shown%20impressive%0Ascalability%20by%20unifying%20all%20language%20tasks%20into%20the%20next%20token%20prediction%0Aparadigm.%20Recently%2C%20there%20is%20a%20growing%20interest%20in%20extending%20this%20success%20to%0Avision%20foundation%20models.%20In%20this%20survey%2C%20we%20review%20the%20recent%20advances%20and%0Adiscuss%20future%20directions%20for%20autoregressive%20vision%20foundation%20models.%20First%2C%0Awe%20present%20the%20trend%20for%20next%20generation%20of%20vision%20foundation%20models%2C%20i.e.%2C%0Aunifying%20both%20understanding%20and%20generation%20in%20vision%20tasks.%20We%20then%20analyze%20the%0Alimitations%20of%20existing%20vision%20foundation%20models%2C%20and%20present%20a%20formal%0Adefinition%20of%20autoregression%20with%20its%20advantages.%20Later%2C%20we%20categorize%0Aautoregressive%20vision%20foundation%20models%20from%20their%20vision%20tokenizers%20and%0Aautoregression%20backbones.%20Finally%2C%20we%20discuss%20several%20promising%20research%0Achallenges%20and%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asurvey%20to%20comprehensively%20summarize%20autoregressive%20vision%20foundation%20models%0Aunder%20the%20trend%20of%20unifying%20understanding%20and%20generation.%20A%20collection%20of%0Arelated%20resources%20is%20available%20at%20https%3A//github.com/EmmaSRH/ARVFM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unifying%2520Understanding%2520and%2520Generation%2520in%2520the%2520Era%2520of%2520Vision%250A%2520%2520Foundation%2520Models%253A%2520A%2520Survey%2520from%2520the%2520Autoregression%2520Perspective%26entry.906535625%3DShenghao%2520Xie%2520and%2520Wenqiang%2520Zu%2520and%2520Mingyang%2520Zhao%2520and%2520Duo%2520Su%2520and%2520Shilong%2520Liu%2520and%2520Ruohua%2520Shi%2520and%2520Guoqi%2520Li%2520and%2520Shanghang%2520Zhang%2520and%2520Lei%2520Ma%26entry.1292438233%3D%2520%2520Autoregression%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520shown%2520impressive%250Ascalability%2520by%2520unifying%2520all%2520language%2520tasks%2520into%2520the%2520next%2520token%2520prediction%250Aparadigm.%2520Recently%252C%2520there%2520is%2520a%2520growing%2520interest%2520in%2520extending%2520this%2520success%2520to%250Avision%2520foundation%2520models.%2520In%2520this%2520survey%252C%2520we%2520review%2520the%2520recent%2520advances%2520and%250Adiscuss%2520future%2520directions%2520for%2520autoregressive%2520vision%2520foundation%2520models.%2520First%252C%250Awe%2520present%2520the%2520trend%2520for%2520next%2520generation%2520of%2520vision%2520foundation%2520models%252C%2520i.e.%252C%250Aunifying%2520both%2520understanding%2520and%2520generation%2520in%2520vision%2520tasks.%2520We%2520then%2520analyze%2520the%250Alimitations%2520of%2520existing%2520vision%2520foundation%2520models%252C%2520and%2520present%2520a%2520formal%250Adefinition%2520of%2520autoregression%2520with%2520its%2520advantages.%2520Later%252C%2520we%2520categorize%250Aautoregressive%2520vision%2520foundation%2520models%2520from%2520their%2520vision%2520tokenizers%2520and%250Aautoregression%2520backbones.%2520Finally%252C%2520we%2520discuss%2520several%2520promising%2520research%250Achallenges%2520and%2520directions.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Asurvey%2520to%2520comprehensively%2520summarize%2520autoregressive%2520vision%2520foundation%2520models%250Aunder%2520the%2520trend%2520of%2520unifying%2520understanding%2520and%2520generation.%2520A%2520collection%2520of%250Arelated%2520resources%2520is%2520available%2520at%2520https%253A//github.com/EmmaSRH/ARVFM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unifying%20Understanding%20and%20Generation%20in%20the%20Era%20of%20Vision%0A%20%20Foundation%20Models%3A%20A%20Survey%20from%20the%20Autoregression%20Perspective&entry.906535625=Shenghao%20Xie%20and%20Wenqiang%20Zu%20and%20Mingyang%20Zhao%20and%20Duo%20Su%20and%20Shilong%20Liu%20and%20Ruohua%20Shi%20and%20Guoqi%20Li%20and%20Shanghang%20Zhang%20and%20Lei%20Ma&entry.1292438233=%20%20Autoregression%20in%20large%20language%20models%20%28LLMs%29%20has%20shown%20impressive%0Ascalability%20by%20unifying%20all%20language%20tasks%20into%20the%20next%20token%20prediction%0Aparadigm.%20Recently%2C%20there%20is%20a%20growing%20interest%20in%20extending%20this%20success%20to%0Avision%20foundation%20models.%20In%20this%20survey%2C%20we%20review%20the%20recent%20advances%20and%0Adiscuss%20future%20directions%20for%20autoregressive%20vision%20foundation%20models.%20First%2C%0Awe%20present%20the%20trend%20for%20next%20generation%20of%20vision%20foundation%20models%2C%20i.e.%2C%0Aunifying%20both%20understanding%20and%20generation%20in%20vision%20tasks.%20We%20then%20analyze%20the%0Alimitations%20of%20existing%20vision%20foundation%20models%2C%20and%20present%20a%20formal%0Adefinition%20of%20autoregression%20with%20its%20advantages.%20Later%2C%20we%20categorize%0Aautoregressive%20vision%20foundation%20models%20from%20their%20vision%20tokenizers%20and%0Aautoregression%20backbones.%20Finally%2C%20we%20discuss%20several%20promising%20research%0Achallenges%20and%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Asurvey%20to%20comprehensively%20summarize%20autoregressive%20vision%20foundation%20models%0Aunder%20the%20trend%20of%20unifying%20understanding%20and%20generation.%20A%20collection%20of%0Arelated%20resources%20is%20available%20at%20https%3A//github.com/EmmaSRH/ARVFM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22217v1&entry.124074799=Read"},
{"title": "4D-based Robot Navigation Using Relativistic Image Processing", "author": "Simone M\u00fcller and Dieter Kranzlm\u00fcller", "abstract": "  Machine perception is an important prerequisite for safe interaction and\nlocomotion in dynamic environments. This requires not only the timely\nperception of surrounding geometries and distances but also the ability to\nreact to changing situations through predefined, learned but also reusable\nskill endings of a robot so that physical damage or bodily harm can be avoided.\nIn this context, 4D perception offers the possibility of predicting one's own\nposition and changes in the environment over time. In this paper, we present a\n4D-based approach to robot navigation using relativistic image processing.\nRelativistic image processing handles the temporal-related sensor information\nin a tensor model within a constructive 4D space. 4D-based navigation expands\nthe causal understanding and the resulting interaction radius of a robot\nthrough the use of visual and sensory 4D information.\n", "link": "http://arxiv.org/abs/2410.22087v1", "date": "2024-10-29", "relevancy": 2.3965, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6343}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D-based%20Robot%20Navigation%20Using%20Relativistic%20Image%20Processing&body=Title%3A%204D-based%20Robot%20Navigation%20Using%20Relativistic%20Image%20Processing%0AAuthor%3A%20Simone%20M%C3%BCller%20and%20Dieter%20Kranzlm%C3%BCller%0AAbstract%3A%20%20%20Machine%20perception%20is%20an%20important%20prerequisite%20for%20safe%20interaction%20and%0Alocomotion%20in%20dynamic%20environments.%20This%20requires%20not%20only%20the%20timely%0Aperception%20of%20surrounding%20geometries%20and%20distances%20but%20also%20the%20ability%20to%0Areact%20to%20changing%20situations%20through%20predefined%2C%20learned%20but%20also%20reusable%0Askill%20endings%20of%20a%20robot%20so%20that%20physical%20damage%20or%20bodily%20harm%20can%20be%20avoided.%0AIn%20this%20context%2C%204D%20perception%20offers%20the%20possibility%20of%20predicting%20one%27s%20own%0Aposition%20and%20changes%20in%20the%20environment%20over%20time.%20In%20this%20paper%2C%20we%20present%20a%0A4D-based%20approach%20to%20robot%20navigation%20using%20relativistic%20image%20processing.%0ARelativistic%20image%20processing%20handles%20the%20temporal-related%20sensor%20information%0Ain%20a%20tensor%20model%20within%20a%20constructive%204D%20space.%204D-based%20navigation%20expands%0Athe%20causal%20understanding%20and%20the%20resulting%20interaction%20radius%20of%20a%20robot%0Athrough%20the%20use%20of%20visual%20and%20sensory%204D%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D-based%2520Robot%2520Navigation%2520Using%2520Relativistic%2520Image%2520Processing%26entry.906535625%3DSimone%2520M%25C3%25BCller%2520and%2520Dieter%2520Kranzlm%25C3%25BCller%26entry.1292438233%3D%2520%2520Machine%2520perception%2520is%2520an%2520important%2520prerequisite%2520for%2520safe%2520interaction%2520and%250Alocomotion%2520in%2520dynamic%2520environments.%2520This%2520requires%2520not%2520only%2520the%2520timely%250Aperception%2520of%2520surrounding%2520geometries%2520and%2520distances%2520but%2520also%2520the%2520ability%2520to%250Areact%2520to%2520changing%2520situations%2520through%2520predefined%252C%2520learned%2520but%2520also%2520reusable%250Askill%2520endings%2520of%2520a%2520robot%2520so%2520that%2520physical%2520damage%2520or%2520bodily%2520harm%2520can%2520be%2520avoided.%250AIn%2520this%2520context%252C%25204D%2520perception%2520offers%2520the%2520possibility%2520of%2520predicting%2520one%2527s%2520own%250Aposition%2520and%2520changes%2520in%2520the%2520environment%2520over%2520time.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250A4D-based%2520approach%2520to%2520robot%2520navigation%2520using%2520relativistic%2520image%2520processing.%250ARelativistic%2520image%2520processing%2520handles%2520the%2520temporal-related%2520sensor%2520information%250Ain%2520a%2520tensor%2520model%2520within%2520a%2520constructive%25204D%2520space.%25204D-based%2520navigation%2520expands%250Athe%2520causal%2520understanding%2520and%2520the%2520resulting%2520interaction%2520radius%2520of%2520a%2520robot%250Athrough%2520the%2520use%2520of%2520visual%2520and%2520sensory%25204D%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D-based%20Robot%20Navigation%20Using%20Relativistic%20Image%20Processing&entry.906535625=Simone%20M%C3%BCller%20and%20Dieter%20Kranzlm%C3%BCller&entry.1292438233=%20%20Machine%20perception%20is%20an%20important%20prerequisite%20for%20safe%20interaction%20and%0Alocomotion%20in%20dynamic%20environments.%20This%20requires%20not%20only%20the%20timely%0Aperception%20of%20surrounding%20geometries%20and%20distances%20but%20also%20the%20ability%20to%0Areact%20to%20changing%20situations%20through%20predefined%2C%20learned%20but%20also%20reusable%0Askill%20endings%20of%20a%20robot%20so%20that%20physical%20damage%20or%20bodily%20harm%20can%20be%20avoided.%0AIn%20this%20context%2C%204D%20perception%20offers%20the%20possibility%20of%20predicting%20one%27s%20own%0Aposition%20and%20changes%20in%20the%20environment%20over%20time.%20In%20this%20paper%2C%20we%20present%20a%0A4D-based%20approach%20to%20robot%20navigation%20using%20relativistic%20image%20processing.%0ARelativistic%20image%20processing%20handles%20the%20temporal-related%20sensor%20information%0Ain%20a%20tensor%20model%20within%20a%20constructive%204D%20space.%204D-based%20navigation%20expands%0Athe%20causal%20understanding%20and%20the%20resulting%20interaction%20radius%20of%20a%20robot%0Athrough%20the%20use%20of%20visual%20and%20sensory%204D%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22087v1&entry.124074799=Read"},
{"title": "NaRCan: Natural Refined Canonical Image with Integration of Diffusion\n  Prior for Video Editing", "author": "Ting-Hsuan Chen and Jiewen Chan and Hau-Shiang Shiu and Shih-Han Yen and Chang-Han Yeh and Yu-Lun Liu", "abstract": "  We propose a video editing framework, NaRCan, which integrates a hybrid\ndeformation field and diffusion prior to generate high-quality natural\ncanonical images to represent the input video. Our approach utilizes homography\nto model global motion and employs multi-layer perceptrons (MLPs) to capture\nlocal residual deformations, enhancing the model's ability to handle complex\nvideo dynamics. By introducing a diffusion prior from the early stages of\ntraining, our model ensures that the generated images retain a high-quality\nnatural appearance, making the produced canonical images suitable for various\ndownstream tasks in video editing, a capability not achieved by current\ncanonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA)\nfine-tuning and introduce a noise and diffusion prior update scheduling\ntechnique that accelerates the training process by 14 times. Extensive\nexperimental results show that our method outperforms existing approaches in\nvarious video editing tasks and produces coherent and high-quality edited video\nsequences. See our project page for video results at\nhttps://koi953215.github.io/NaRCan_page/.\n", "link": "http://arxiv.org/abs/2406.06523v2", "date": "2024-10-29", "relevancy": 2.3851, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6289}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6066}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NaRCan%3A%20Natural%20Refined%20Canonical%20Image%20with%20Integration%20of%20Diffusion%0A%20%20Prior%20for%20Video%20Editing&body=Title%3A%20NaRCan%3A%20Natural%20Refined%20Canonical%20Image%20with%20Integration%20of%20Diffusion%0A%20%20Prior%20for%20Video%20Editing%0AAuthor%3A%20Ting-Hsuan%20Chen%20and%20Jiewen%20Chan%20and%20Hau-Shiang%20Shiu%20and%20Shih-Han%20Yen%20and%20Chang-Han%20Yeh%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20We%20propose%20a%20video%20editing%20framework%2C%20NaRCan%2C%20which%20integrates%20a%20hybrid%0Adeformation%20field%20and%20diffusion%20prior%20to%20generate%20high-quality%20natural%0Acanonical%20images%20to%20represent%20the%20input%20video.%20Our%20approach%20utilizes%20homography%0Ato%20model%20global%20motion%20and%20employs%20multi-layer%20perceptrons%20%28MLPs%29%20to%20capture%0Alocal%20residual%20deformations%2C%20enhancing%20the%20model%27s%20ability%20to%20handle%20complex%0Avideo%20dynamics.%20By%20introducing%20a%20diffusion%20prior%20from%20the%20early%20stages%20of%0Atraining%2C%20our%20model%20ensures%20that%20the%20generated%20images%20retain%20a%20high-quality%0Anatural%20appearance%2C%20making%20the%20produced%20canonical%20images%20suitable%20for%20various%0Adownstream%20tasks%20in%20video%20editing%2C%20a%20capability%20not%20achieved%20by%20current%0Acanonical-based%20methods.%20Furthermore%2C%20we%20incorporate%20low-rank%20adaptation%20%28LoRA%29%0Afine-tuning%20and%20introduce%20a%20noise%20and%20diffusion%20prior%20update%20scheduling%0Atechnique%20that%20accelerates%20the%20training%20process%20by%2014%20times.%20Extensive%0Aexperimental%20results%20show%20that%20our%20method%20outperforms%20existing%20approaches%20in%0Avarious%20video%20editing%20tasks%20and%20produces%20coherent%20and%20high-quality%20edited%20video%0Asequences.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//koi953215.github.io/NaRCan_page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaRCan%253A%2520Natural%2520Refined%2520Canonical%2520Image%2520with%2520Integration%2520of%2520Diffusion%250A%2520%2520Prior%2520for%2520Video%2520Editing%26entry.906535625%3DTing-Hsuan%2520Chen%2520and%2520Jiewen%2520Chan%2520and%2520Hau-Shiang%2520Shiu%2520and%2520Shih-Han%2520Yen%2520and%2520Chang-Han%2520Yeh%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520video%2520editing%2520framework%252C%2520NaRCan%252C%2520which%2520integrates%2520a%2520hybrid%250Adeformation%2520field%2520and%2520diffusion%2520prior%2520to%2520generate%2520high-quality%2520natural%250Acanonical%2520images%2520to%2520represent%2520the%2520input%2520video.%2520Our%2520approach%2520utilizes%2520homography%250Ato%2520model%2520global%2520motion%2520and%2520employs%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%2520to%2520capture%250Alocal%2520residual%2520deformations%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520handle%2520complex%250Avideo%2520dynamics.%2520By%2520introducing%2520a%2520diffusion%2520prior%2520from%2520the%2520early%2520stages%2520of%250Atraining%252C%2520our%2520model%2520ensures%2520that%2520the%2520generated%2520images%2520retain%2520a%2520high-quality%250Anatural%2520appearance%252C%2520making%2520the%2520produced%2520canonical%2520images%2520suitable%2520for%2520various%250Adownstream%2520tasks%2520in%2520video%2520editing%252C%2520a%2520capability%2520not%2520achieved%2520by%2520current%250Acanonical-based%2520methods.%2520Furthermore%252C%2520we%2520incorporate%2520low-rank%2520adaptation%2520%2528LoRA%2529%250Afine-tuning%2520and%2520introduce%2520a%2520noise%2520and%2520diffusion%2520prior%2520update%2520scheduling%250Atechnique%2520that%2520accelerates%2520the%2520training%2520process%2520by%252014%2520times.%2520Extensive%250Aexperimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520in%250Avarious%2520video%2520editing%2520tasks%2520and%2520produces%2520coherent%2520and%2520high-quality%2520edited%2520video%250Asequences.%2520See%2520our%2520project%2520page%2520for%2520video%2520results%2520at%250Ahttps%253A//koi953215.github.io/NaRCan_page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaRCan%3A%20Natural%20Refined%20Canonical%20Image%20with%20Integration%20of%20Diffusion%0A%20%20Prior%20for%20Video%20Editing&entry.906535625=Ting-Hsuan%20Chen%20and%20Jiewen%20Chan%20and%20Hau-Shiang%20Shiu%20and%20Shih-Han%20Yen%20and%20Chang-Han%20Yeh%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20We%20propose%20a%20video%20editing%20framework%2C%20NaRCan%2C%20which%20integrates%20a%20hybrid%0Adeformation%20field%20and%20diffusion%20prior%20to%20generate%20high-quality%20natural%0Acanonical%20images%20to%20represent%20the%20input%20video.%20Our%20approach%20utilizes%20homography%0Ato%20model%20global%20motion%20and%20employs%20multi-layer%20perceptrons%20%28MLPs%29%20to%20capture%0Alocal%20residual%20deformations%2C%20enhancing%20the%20model%27s%20ability%20to%20handle%20complex%0Avideo%20dynamics.%20By%20introducing%20a%20diffusion%20prior%20from%20the%20early%20stages%20of%0Atraining%2C%20our%20model%20ensures%20that%20the%20generated%20images%20retain%20a%20high-quality%0Anatural%20appearance%2C%20making%20the%20produced%20canonical%20images%20suitable%20for%20various%0Adownstream%20tasks%20in%20video%20editing%2C%20a%20capability%20not%20achieved%20by%20current%0Acanonical-based%20methods.%20Furthermore%2C%20we%20incorporate%20low-rank%20adaptation%20%28LoRA%29%0Afine-tuning%20and%20introduce%20a%20noise%20and%20diffusion%20prior%20update%20scheduling%0Atechnique%20that%20accelerates%20the%20training%20process%20by%2014%20times.%20Extensive%0Aexperimental%20results%20show%20that%20our%20method%20outperforms%20existing%20approaches%20in%0Avarious%20video%20editing%20tasks%20and%20produces%20coherent%20and%20high-quality%20edited%20video%0Asequences.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//koi953215.github.io/NaRCan_page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06523v2&entry.124074799=Read"},
{"title": "Spatio-temporal Transformers for Action Unit Classification with Event\n  Cameras", "author": "Luca Cultrera and Federico Becattini and Lorenzo Berlincioni and Claudio Ferrari and Alberto Del Bimbo", "abstract": "  Face analysis has been studied from different angles to infer emotion, poses,\nshapes, and landmarks. Traditionally RGB cameras are used, yet for fine-grained\ntasks standard sensors might not be up to the task due to their latency, making\nit impossible to record and detect micro-movements that carry a highly\ninformative signal, which is necessary for inferring the true emotions of a\nsubject. Event cameras have been increasingly gaining interest as a possible\nsolution to this and similar high-frame rate tasks. We propose a novel\nspatiotemporal Vision Transformer model that uses Shifted Patch Tokenization\n(SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit\nclassification from event streams. We also address the lack of labeled event\ndata in the literature, which can be considered one of the main causes of an\nexisting gap between the maturity of RGB and neuromorphic vision models.\nGathering data is harder in the event domain since it cannot be crawled from\nthe web and labeling frames should take into account event aggregation rates\nand the fact that static parts might not be visible in certain frames. To this\nend, we present FACEMORPHIC, a temporally synchronized multimodal face dataset\ncomposed of RGB videos and event streams. The dataset is annotated at a video\nlevel with facial Action Units and contains streams collected with various\npossible applications, ranging from 3D shape estimation to lip-reading. We then\nshow how temporal synchronization can allow effective neuromorphic face\nanalysis without the need to manually annotate videos: we instead leverage\ncross-modal supervision bridging the domain gap by representing face shapes in\na 3D space. Our proposed model outperforms baseline methods by effectively\ncapturing spatial and temporal information, crucial for recognizing subtle\nfacial micro-expressions.\n", "link": "http://arxiv.org/abs/2410.21958v1", "date": "2024-10-29", "relevancy": 2.3628, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5807}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-temporal%20Transformers%20for%20Action%20Unit%20Classification%20with%20Event%0A%20%20Cameras&body=Title%3A%20Spatio-temporal%20Transformers%20for%20Action%20Unit%20Classification%20with%20Event%0A%20%20Cameras%0AAuthor%3A%20Luca%20Cultrera%20and%20Federico%20Becattini%20and%20Lorenzo%20Berlincioni%20and%20Claudio%20Ferrari%20and%20Alberto%20Del%20Bimbo%0AAbstract%3A%20%20%20Face%20analysis%20has%20been%20studied%20from%20different%20angles%20to%20infer%20emotion%2C%20poses%2C%0Ashapes%2C%20and%20landmarks.%20Traditionally%20RGB%20cameras%20are%20used%2C%20yet%20for%20fine-grained%0Atasks%20standard%20sensors%20might%20not%20be%20up%20to%20the%20task%20due%20to%20their%20latency%2C%20making%0Ait%20impossible%20to%20record%20and%20detect%20micro-movements%20that%20carry%20a%20highly%0Ainformative%20signal%2C%20which%20is%20necessary%20for%20inferring%20the%20true%20emotions%20of%20a%0Asubject.%20Event%20cameras%20have%20been%20increasingly%20gaining%20interest%20as%20a%20possible%0Asolution%20to%20this%20and%20similar%20high-frame%20rate%20tasks.%20We%20propose%20a%20novel%0Aspatiotemporal%20Vision%20Transformer%20model%20that%20uses%20Shifted%20Patch%20Tokenization%0A%28SPT%29%20and%20Locality%20Self-Attention%20%28LSA%29%20to%20enhance%20the%20accuracy%20of%20Action%20Unit%0Aclassification%20from%20event%20streams.%20We%20also%20address%20the%20lack%20of%20labeled%20event%0Adata%20in%20the%20literature%2C%20which%20can%20be%20considered%20one%20of%20the%20main%20causes%20of%20an%0Aexisting%20gap%20between%20the%20maturity%20of%20RGB%20and%20neuromorphic%20vision%20models.%0AGathering%20data%20is%20harder%20in%20the%20event%20domain%20since%20it%20cannot%20be%20crawled%20from%0Athe%20web%20and%20labeling%20frames%20should%20take%20into%20account%20event%20aggregation%20rates%0Aand%20the%20fact%20that%20static%20parts%20might%20not%20be%20visible%20in%20certain%20frames.%20To%20this%0Aend%2C%20we%20present%20FACEMORPHIC%2C%20a%20temporally%20synchronized%20multimodal%20face%20dataset%0Acomposed%20of%20RGB%20videos%20and%20event%20streams.%20The%20dataset%20is%20annotated%20at%20a%20video%0Alevel%20with%20facial%20Action%20Units%20and%20contains%20streams%20collected%20with%20various%0Apossible%20applications%2C%20ranging%20from%203D%20shape%20estimation%20to%20lip-reading.%20We%20then%0Ashow%20how%20temporal%20synchronization%20can%20allow%20effective%20neuromorphic%20face%0Aanalysis%20without%20the%20need%20to%20manually%20annotate%20videos%3A%20we%20instead%20leverage%0Across-modal%20supervision%20bridging%20the%20domain%20gap%20by%20representing%20face%20shapes%20in%0Aa%203D%20space.%20Our%20proposed%20model%20outperforms%20baseline%20methods%20by%20effectively%0Acapturing%20spatial%20and%20temporal%20information%2C%20crucial%20for%20recognizing%20subtle%0Afacial%20micro-expressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-temporal%2520Transformers%2520for%2520Action%2520Unit%2520Classification%2520with%2520Event%250A%2520%2520Cameras%26entry.906535625%3DLuca%2520Cultrera%2520and%2520Federico%2520Becattini%2520and%2520Lorenzo%2520Berlincioni%2520and%2520Claudio%2520Ferrari%2520and%2520Alberto%2520Del%2520Bimbo%26entry.1292438233%3D%2520%2520Face%2520analysis%2520has%2520been%2520studied%2520from%2520different%2520angles%2520to%2520infer%2520emotion%252C%2520poses%252C%250Ashapes%252C%2520and%2520landmarks.%2520Traditionally%2520RGB%2520cameras%2520are%2520used%252C%2520yet%2520for%2520fine-grained%250Atasks%2520standard%2520sensors%2520might%2520not%2520be%2520up%2520to%2520the%2520task%2520due%2520to%2520their%2520latency%252C%2520making%250Ait%2520impossible%2520to%2520record%2520and%2520detect%2520micro-movements%2520that%2520carry%2520a%2520highly%250Ainformative%2520signal%252C%2520which%2520is%2520necessary%2520for%2520inferring%2520the%2520true%2520emotions%2520of%2520a%250Asubject.%2520Event%2520cameras%2520have%2520been%2520increasingly%2520gaining%2520interest%2520as%2520a%2520possible%250Asolution%2520to%2520this%2520and%2520similar%2520high-frame%2520rate%2520tasks.%2520We%2520propose%2520a%2520novel%250Aspatiotemporal%2520Vision%2520Transformer%2520model%2520that%2520uses%2520Shifted%2520Patch%2520Tokenization%250A%2528SPT%2529%2520and%2520Locality%2520Self-Attention%2520%2528LSA%2529%2520to%2520enhance%2520the%2520accuracy%2520of%2520Action%2520Unit%250Aclassification%2520from%2520event%2520streams.%2520We%2520also%2520address%2520the%2520lack%2520of%2520labeled%2520event%250Adata%2520in%2520the%2520literature%252C%2520which%2520can%2520be%2520considered%2520one%2520of%2520the%2520main%2520causes%2520of%2520an%250Aexisting%2520gap%2520between%2520the%2520maturity%2520of%2520RGB%2520and%2520neuromorphic%2520vision%2520models.%250AGathering%2520data%2520is%2520harder%2520in%2520the%2520event%2520domain%2520since%2520it%2520cannot%2520be%2520crawled%2520from%250Athe%2520web%2520and%2520labeling%2520frames%2520should%2520take%2520into%2520account%2520event%2520aggregation%2520rates%250Aand%2520the%2520fact%2520that%2520static%2520parts%2520might%2520not%2520be%2520visible%2520in%2520certain%2520frames.%2520To%2520this%250Aend%252C%2520we%2520present%2520FACEMORPHIC%252C%2520a%2520temporally%2520synchronized%2520multimodal%2520face%2520dataset%250Acomposed%2520of%2520RGB%2520videos%2520and%2520event%2520streams.%2520The%2520dataset%2520is%2520annotated%2520at%2520a%2520video%250Alevel%2520with%2520facial%2520Action%2520Units%2520and%2520contains%2520streams%2520collected%2520with%2520various%250Apossible%2520applications%252C%2520ranging%2520from%25203D%2520shape%2520estimation%2520to%2520lip-reading.%2520We%2520then%250Ashow%2520how%2520temporal%2520synchronization%2520can%2520allow%2520effective%2520neuromorphic%2520face%250Aanalysis%2520without%2520the%2520need%2520to%2520manually%2520annotate%2520videos%253A%2520we%2520instead%2520leverage%250Across-modal%2520supervision%2520bridging%2520the%2520domain%2520gap%2520by%2520representing%2520face%2520shapes%2520in%250Aa%25203D%2520space.%2520Our%2520proposed%2520model%2520outperforms%2520baseline%2520methods%2520by%2520effectively%250Acapturing%2520spatial%2520and%2520temporal%2520information%252C%2520crucial%2520for%2520recognizing%2520subtle%250Afacial%2520micro-expressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-temporal%20Transformers%20for%20Action%20Unit%20Classification%20with%20Event%0A%20%20Cameras&entry.906535625=Luca%20Cultrera%20and%20Federico%20Becattini%20and%20Lorenzo%20Berlincioni%20and%20Claudio%20Ferrari%20and%20Alberto%20Del%20Bimbo&entry.1292438233=%20%20Face%20analysis%20has%20been%20studied%20from%20different%20angles%20to%20infer%20emotion%2C%20poses%2C%0Ashapes%2C%20and%20landmarks.%20Traditionally%20RGB%20cameras%20are%20used%2C%20yet%20for%20fine-grained%0Atasks%20standard%20sensors%20might%20not%20be%20up%20to%20the%20task%20due%20to%20their%20latency%2C%20making%0Ait%20impossible%20to%20record%20and%20detect%20micro-movements%20that%20carry%20a%20highly%0Ainformative%20signal%2C%20which%20is%20necessary%20for%20inferring%20the%20true%20emotions%20of%20a%0Asubject.%20Event%20cameras%20have%20been%20increasingly%20gaining%20interest%20as%20a%20possible%0Asolution%20to%20this%20and%20similar%20high-frame%20rate%20tasks.%20We%20propose%20a%20novel%0Aspatiotemporal%20Vision%20Transformer%20model%20that%20uses%20Shifted%20Patch%20Tokenization%0A%28SPT%29%20and%20Locality%20Self-Attention%20%28LSA%29%20to%20enhance%20the%20accuracy%20of%20Action%20Unit%0Aclassification%20from%20event%20streams.%20We%20also%20address%20the%20lack%20of%20labeled%20event%0Adata%20in%20the%20literature%2C%20which%20can%20be%20considered%20one%20of%20the%20main%20causes%20of%20an%0Aexisting%20gap%20between%20the%20maturity%20of%20RGB%20and%20neuromorphic%20vision%20models.%0AGathering%20data%20is%20harder%20in%20the%20event%20domain%20since%20it%20cannot%20be%20crawled%20from%0Athe%20web%20and%20labeling%20frames%20should%20take%20into%20account%20event%20aggregation%20rates%0Aand%20the%20fact%20that%20static%20parts%20might%20not%20be%20visible%20in%20certain%20frames.%20To%20this%0Aend%2C%20we%20present%20FACEMORPHIC%2C%20a%20temporally%20synchronized%20multimodal%20face%20dataset%0Acomposed%20of%20RGB%20videos%20and%20event%20streams.%20The%20dataset%20is%20annotated%20at%20a%20video%0Alevel%20with%20facial%20Action%20Units%20and%20contains%20streams%20collected%20with%20various%0Apossible%20applications%2C%20ranging%20from%203D%20shape%20estimation%20to%20lip-reading.%20We%20then%0Ashow%20how%20temporal%20synchronization%20can%20allow%20effective%20neuromorphic%20face%0Aanalysis%20without%20the%20need%20to%20manually%20annotate%20videos%3A%20we%20instead%20leverage%0Across-modal%20supervision%20bridging%20the%20domain%20gap%20by%20representing%20face%20shapes%20in%0Aa%203D%20space.%20Our%20proposed%20model%20outperforms%20baseline%20methods%20by%20effectively%0Acapturing%20spatial%20and%20temporal%20information%2C%20crucial%20for%20recognizing%20subtle%0Afacial%20micro-expressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21958v1&entry.124074799=Read"},
{"title": "Enhancing Learned Image Compression via Cross Window-based Attention", "author": "Priyanka Mudgal and Feng Liu", "abstract": "  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.21144v2", "date": "2024-10-29", "relevancy": 2.3545, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5961}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5874}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Learned%20Image%20Compression%20via%20Cross%20Window-based%20Attention&body=Title%3A%20Enhancing%20Learned%20Image%20Compression%20via%20Cross%20Window-based%20Attention%0AAuthor%3A%20Priyanka%20Mudgal%20and%20Feng%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20learned%20image%20compression%20methods%20have%20demonstrated%20superior%0Arate-distortion%20performance%20compared%20to%20traditional%20image%20compression%20methods.%0ARecent%20methods%20utilize%20convolutional%20neural%20networks%20%28CNN%29%2C%20variational%0Aautoencoders%20%28VAE%29%2C%20invertible%20neural%20networks%20%28INN%29%2C%20and%20transformers.%20Despite%0Atheir%20significant%20contributions%2C%20a%20main%20drawback%20of%20these%20models%20is%20their%20poor%0Aperformance%20in%20capturing%20local%20redundancy.%20Therefore%2C%20to%20leverage%20global%0Afeatures%20along%20with%20local%20redundancy%2C%20we%20propose%20a%20CNN-based%20solution%0Aintegrated%20with%20a%20feature%20encoding%20module.%20The%20feature%20encoding%20module%20encodes%0Aimportant%20features%20before%20feeding%20them%20to%20the%20CNN%20and%20then%20utilizes%20cross-scale%0Awindow-based%20attention%2C%20which%20further%20captures%20local%20redundancy.%20Cross-scale%0Awindow-based%20attention%20is%20inspired%20by%20the%20attention%20mechanism%20in%20transformers%0Aand%20effectively%20enlarges%20the%20receptive%20field.%20Both%20the%20feature%20encoding%20module%0Aand%20the%20cross-scale%20window-based%20attention%20module%20in%20our%20architecture%20are%0Aflexible%20and%20can%20be%20incorporated%20into%20any%20other%20network%20architecture.%20We%0Aevaluate%20our%20method%20on%20the%20Kodak%20and%20CLIC%20datasets%20and%20demonstrate%20that%20our%0Aapproach%20is%20effective%20and%20on%20par%20with%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21144v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Learned%2520Image%2520Compression%2520via%2520Cross%2520Window-based%2520Attention%26entry.906535625%3DPriyanka%2520Mudgal%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520learned%2520image%2520compression%2520methods%2520have%2520demonstrated%2520superior%250Arate-distortion%2520performance%2520compared%2520to%2520traditional%2520image%2520compression%2520methods.%250ARecent%2520methods%2520utilize%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%252C%2520variational%250Aautoencoders%2520%2528VAE%2529%252C%2520invertible%2520neural%2520networks%2520%2528INN%2529%252C%2520and%2520transformers.%2520Despite%250Atheir%2520significant%2520contributions%252C%2520a%2520main%2520drawback%2520of%2520these%2520models%2520is%2520their%2520poor%250Aperformance%2520in%2520capturing%2520local%2520redundancy.%2520Therefore%252C%2520to%2520leverage%2520global%250Afeatures%2520along%2520with%2520local%2520redundancy%252C%2520we%2520propose%2520a%2520CNN-based%2520solution%250Aintegrated%2520with%2520a%2520feature%2520encoding%2520module.%2520The%2520feature%2520encoding%2520module%2520encodes%250Aimportant%2520features%2520before%2520feeding%2520them%2520to%2520the%2520CNN%2520and%2520then%2520utilizes%2520cross-scale%250Awindow-based%2520attention%252C%2520which%2520further%2520captures%2520local%2520redundancy.%2520Cross-scale%250Awindow-based%2520attention%2520is%2520inspired%2520by%2520the%2520attention%2520mechanism%2520in%2520transformers%250Aand%2520effectively%2520enlarges%2520the%2520receptive%2520field.%2520Both%2520the%2520feature%2520encoding%2520module%250Aand%2520the%2520cross-scale%2520window-based%2520attention%2520module%2520in%2520our%2520architecture%2520are%250Aflexible%2520and%2520can%2520be%2520incorporated%2520into%2520any%2520other%2520network%2520architecture.%2520We%250Aevaluate%2520our%2520method%2520on%2520the%2520Kodak%2520and%2520CLIC%2520datasets%2520and%2520demonstrate%2520that%2520our%250Aapproach%2520is%2520effective%2520and%2520on%2520par%2520with%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21144v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Learned%20Image%20Compression%20via%20Cross%20Window-based%20Attention&entry.906535625=Priyanka%20Mudgal%20and%20Feng%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20learned%20image%20compression%20methods%20have%20demonstrated%20superior%0Arate-distortion%20performance%20compared%20to%20traditional%20image%20compression%20methods.%0ARecent%20methods%20utilize%20convolutional%20neural%20networks%20%28CNN%29%2C%20variational%0Aautoencoders%20%28VAE%29%2C%20invertible%20neural%20networks%20%28INN%29%2C%20and%20transformers.%20Despite%0Atheir%20significant%20contributions%2C%20a%20main%20drawback%20of%20these%20models%20is%20their%20poor%0Aperformance%20in%20capturing%20local%20redundancy.%20Therefore%2C%20to%20leverage%20global%0Afeatures%20along%20with%20local%20redundancy%2C%20we%20propose%20a%20CNN-based%20solution%0Aintegrated%20with%20a%20feature%20encoding%20module.%20The%20feature%20encoding%20module%20encodes%0Aimportant%20features%20before%20feeding%20them%20to%20the%20CNN%20and%20then%20utilizes%20cross-scale%0Awindow-based%20attention%2C%20which%20further%20captures%20local%20redundancy.%20Cross-scale%0Awindow-based%20attention%20is%20inspired%20by%20the%20attention%20mechanism%20in%20transformers%0Aand%20effectively%20enlarges%20the%20receptive%20field.%20Both%20the%20feature%20encoding%20module%0Aand%20the%20cross-scale%20window-based%20attention%20module%20in%20our%20architecture%20are%0Aflexible%20and%20can%20be%20incorporated%20into%20any%20other%20network%20architecture.%20We%0Aevaluate%20our%20method%20on%20the%20Kodak%20and%20CLIC%20datasets%20and%20demonstrate%20that%20our%0Aapproach%20is%20effective%20and%20on%20par%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21144v2&entry.124074799=Read"},
{"title": "An Efficient Approach to Generate Safe Drivable Space by\n  LiDAR-Camera-HDmap Fusion", "author": "Minghao Ning and Ahmad Reza Alghooneh and Chen Sun and Ruihe Zhang and Pouya Panahandeh and Steven Tuer and Ehsan Hashemi and Amir Khajepour", "abstract": "  In this paper, we propose an accurate and robust perception module for\nAutonomous Vehicles (AVs) for drivable space extraction. Perception is crucial\nin autonomous driving, where many deep learning-based methods, while accurate\non benchmark datasets, fail to generalize effectively, especially in diverse\nand unpredictable environments. Our work introduces a robust easy-to-generalize\nperception module that leverages LiDAR, camera, and HD map data fusion to\ndeliver a safe and reliable drivable space in all weather conditions. We\npresent an adaptive ground removal and curb detection method integrated with HD\nmap data for enhanced obstacle detection reliability. Additionally, we propose\nan adaptive DBSCAN clustering algorithm optimized for precipitation noise, and\na cost-effective LiDAR-camera frustum association that is resilient to\ncalibration discrepancies. Our comprehensive drivable space representation\nincorporates all perception data, ensuring compatibility with vehicle\ndimensions and road regulations. This approach not only improves generalization\nand efficiency, but also significantly enhances safety in autonomous vehicle\noperations. Our approach is tested on a real dataset and its reliability is\nverified during the daily (including harsh snowy weather) operation of our\nautonomous shuttle, WATonoBus\n", "link": "http://arxiv.org/abs/2410.22314v1", "date": "2024-10-29", "relevancy": 2.3517, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6404}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Approach%20to%20Generate%20Safe%20Drivable%20Space%20by%0A%20%20LiDAR-Camera-HDmap%20Fusion&body=Title%3A%20An%20Efficient%20Approach%20to%20Generate%20Safe%20Drivable%20Space%20by%0A%20%20LiDAR-Camera-HDmap%20Fusion%0AAuthor%3A%20Minghao%20Ning%20and%20Ahmad%20Reza%20Alghooneh%20and%20Chen%20Sun%20and%20Ruihe%20Zhang%20and%20Pouya%20Panahandeh%20and%20Steven%20Tuer%20and%20Ehsan%20Hashemi%20and%20Amir%20Khajepour%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20accurate%20and%20robust%20perception%20module%20for%0AAutonomous%20Vehicles%20%28AVs%29%20for%20drivable%20space%20extraction.%20Perception%20is%20crucial%0Ain%20autonomous%20driving%2C%20where%20many%20deep%20learning-based%20methods%2C%20while%20accurate%0Aon%20benchmark%20datasets%2C%20fail%20to%20generalize%20effectively%2C%20especially%20in%20diverse%0Aand%20unpredictable%20environments.%20Our%20work%20introduces%20a%20robust%20easy-to-generalize%0Aperception%20module%20that%20leverages%20LiDAR%2C%20camera%2C%20and%20HD%20map%20data%20fusion%20to%0Adeliver%20a%20safe%20and%20reliable%20drivable%20space%20in%20all%20weather%20conditions.%20We%0Apresent%20an%20adaptive%20ground%20removal%20and%20curb%20detection%20method%20integrated%20with%20HD%0Amap%20data%20for%20enhanced%20obstacle%20detection%20reliability.%20Additionally%2C%20we%20propose%0Aan%20adaptive%20DBSCAN%20clustering%20algorithm%20optimized%20for%20precipitation%20noise%2C%20and%0Aa%20cost-effective%20LiDAR-camera%20frustum%20association%20that%20is%20resilient%20to%0Acalibration%20discrepancies.%20Our%20comprehensive%20drivable%20space%20representation%0Aincorporates%20all%20perception%20data%2C%20ensuring%20compatibility%20with%20vehicle%0Adimensions%20and%20road%20regulations.%20This%20approach%20not%20only%20improves%20generalization%0Aand%20efficiency%2C%20but%20also%20significantly%20enhances%20safety%20in%20autonomous%20vehicle%0Aoperations.%20Our%20approach%20is%20tested%20on%20a%20real%20dataset%20and%20its%20reliability%20is%0Averified%20during%20the%20daily%20%28including%20harsh%20snowy%20weather%29%20operation%20of%20our%0Aautonomous%20shuttle%2C%20WATonoBus%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Approach%2520to%2520Generate%2520Safe%2520Drivable%2520Space%2520by%250A%2520%2520LiDAR-Camera-HDmap%2520Fusion%26entry.906535625%3DMinghao%2520Ning%2520and%2520Ahmad%2520Reza%2520Alghooneh%2520and%2520Chen%2520Sun%2520and%2520Ruihe%2520Zhang%2520and%2520Pouya%2520Panahandeh%2520and%2520Steven%2520Tuer%2520and%2520Ehsan%2520Hashemi%2520and%2520Amir%2520Khajepour%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520accurate%2520and%2520robust%2520perception%2520module%2520for%250AAutonomous%2520Vehicles%2520%2528AVs%2529%2520for%2520drivable%2520space%2520extraction.%2520Perception%2520is%2520crucial%250Ain%2520autonomous%2520driving%252C%2520where%2520many%2520deep%2520learning-based%2520methods%252C%2520while%2520accurate%250Aon%2520benchmark%2520datasets%252C%2520fail%2520to%2520generalize%2520effectively%252C%2520especially%2520in%2520diverse%250Aand%2520unpredictable%2520environments.%2520Our%2520work%2520introduces%2520a%2520robust%2520easy-to-generalize%250Aperception%2520module%2520that%2520leverages%2520LiDAR%252C%2520camera%252C%2520and%2520HD%2520map%2520data%2520fusion%2520to%250Adeliver%2520a%2520safe%2520and%2520reliable%2520drivable%2520space%2520in%2520all%2520weather%2520conditions.%2520We%250Apresent%2520an%2520adaptive%2520ground%2520removal%2520and%2520curb%2520detection%2520method%2520integrated%2520with%2520HD%250Amap%2520data%2520for%2520enhanced%2520obstacle%2520detection%2520reliability.%2520Additionally%252C%2520we%2520propose%250Aan%2520adaptive%2520DBSCAN%2520clustering%2520algorithm%2520optimized%2520for%2520precipitation%2520noise%252C%2520and%250Aa%2520cost-effective%2520LiDAR-camera%2520frustum%2520association%2520that%2520is%2520resilient%2520to%250Acalibration%2520discrepancies.%2520Our%2520comprehensive%2520drivable%2520space%2520representation%250Aincorporates%2520all%2520perception%2520data%252C%2520ensuring%2520compatibility%2520with%2520vehicle%250Adimensions%2520and%2520road%2520regulations.%2520This%2520approach%2520not%2520only%2520improves%2520generalization%250Aand%2520efficiency%252C%2520but%2520also%2520significantly%2520enhances%2520safety%2520in%2520autonomous%2520vehicle%250Aoperations.%2520Our%2520approach%2520is%2520tested%2520on%2520a%2520real%2520dataset%2520and%2520its%2520reliability%2520is%250Averified%2520during%2520the%2520daily%2520%2528including%2520harsh%2520snowy%2520weather%2529%2520operation%2520of%2520our%250Aautonomous%2520shuttle%252C%2520WATonoBus%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Approach%20to%20Generate%20Safe%20Drivable%20Space%20by%0A%20%20LiDAR-Camera-HDmap%20Fusion&entry.906535625=Minghao%20Ning%20and%20Ahmad%20Reza%20Alghooneh%20and%20Chen%20Sun%20and%20Ruihe%20Zhang%20and%20Pouya%20Panahandeh%20and%20Steven%20Tuer%20and%20Ehsan%20Hashemi%20and%20Amir%20Khajepour&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20accurate%20and%20robust%20perception%20module%20for%0AAutonomous%20Vehicles%20%28AVs%29%20for%20drivable%20space%20extraction.%20Perception%20is%20crucial%0Ain%20autonomous%20driving%2C%20where%20many%20deep%20learning-based%20methods%2C%20while%20accurate%0Aon%20benchmark%20datasets%2C%20fail%20to%20generalize%20effectively%2C%20especially%20in%20diverse%0Aand%20unpredictable%20environments.%20Our%20work%20introduces%20a%20robust%20easy-to-generalize%0Aperception%20module%20that%20leverages%20LiDAR%2C%20camera%2C%20and%20HD%20map%20data%20fusion%20to%0Adeliver%20a%20safe%20and%20reliable%20drivable%20space%20in%20all%20weather%20conditions.%20We%0Apresent%20an%20adaptive%20ground%20removal%20and%20curb%20detection%20method%20integrated%20with%20HD%0Amap%20data%20for%20enhanced%20obstacle%20detection%20reliability.%20Additionally%2C%20we%20propose%0Aan%20adaptive%20DBSCAN%20clustering%20algorithm%20optimized%20for%20precipitation%20noise%2C%20and%0Aa%20cost-effective%20LiDAR-camera%20frustum%20association%20that%20is%20resilient%20to%0Acalibration%20discrepancies.%20Our%20comprehensive%20drivable%20space%20representation%0Aincorporates%20all%20perception%20data%2C%20ensuring%20compatibility%20with%20vehicle%0Adimensions%20and%20road%20regulations.%20This%20approach%20not%20only%20improves%20generalization%0Aand%20efficiency%2C%20but%20also%20significantly%20enhances%20safety%20in%20autonomous%20vehicle%0Aoperations.%20Our%20approach%20is%20tested%20on%20a%20real%20dataset%20and%20its%20reliability%20is%0Averified%20during%20the%20daily%20%28including%20harsh%20snowy%20weather%29%20operation%20of%20our%0Aautonomous%20shuttle%2C%20WATonoBus%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22314v1&entry.124074799=Read"},
{"title": "CT to PET Translation: A Large-scale Dataset and Domain-Knowledge-Guided\n  Diffusion Approach", "author": "Dac Thai Nguyen and Trung Thanh Nguyen and Huu Tien Nguyen and Thanh Trung Nguyen and Huy Hieu Pham and Thanh Hung Nguyen and Thao Nguyen Truong and Phi Le Nguyen", "abstract": "  Positron Emission Tomography (PET) and Computed Tomography (CT) are essential\nfor diagnosing, staging, and monitoring various diseases, particularly cancer.\nDespite their importance, the use of PET/CT systems is limited by the necessity\nfor radioactive materials, the scarcity of PET scanners, and the high cost\nassociated with PET imaging. In contrast, CT scanners are more widely available\nand significantly less expensive. In response to these challenges, our study\naddresses the issue of generating PET images from CT images, aiming to reduce\nboth the medical examination cost and the associated health risks for patients.\nOur contributions are twofold: First, we introduce a conditional diffusion\nmodel named CPDM, which, to our knowledge, is one of the initial attempts to\nemploy a diffusion model for translating from CT to PET images. Second, we\nprovide the largest CT-PET dataset to date, comprising 2,028,628 paired CT-PET\nimages, which facilitates the training and evaluation of CT-to-PET translation\nmodels. For the CPDM model, we incorporate domain knowledge to develop two\nconditional maps: the Attention map and the Attenuation map. The former helps\nthe diffusion process focus on areas of interest, while the latter improves PET\ndata correction and ensures accurate diagnostic information. Experimental\nevaluations across various benchmarks demonstrate that CPDM surpasses existing\nmethods in generating high-quality PET images in terms of multiple metrics. The\nsource code and data samples are available at https://github.com/thanhhff/CPDM.\n", "link": "http://arxiv.org/abs/2410.21932v1", "date": "2024-10-29", "relevancy": 2.3508, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6012}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT%20to%20PET%20Translation%3A%20A%20Large-scale%20Dataset%20and%20Domain-Knowledge-Guided%0A%20%20Diffusion%20Approach&body=Title%3A%20CT%20to%20PET%20Translation%3A%20A%20Large-scale%20Dataset%20and%20Domain-Knowledge-Guided%0A%20%20Diffusion%20Approach%0AAuthor%3A%20Dac%20Thai%20Nguyen%20and%20Trung%20Thanh%20Nguyen%20and%20Huu%20Tien%20Nguyen%20and%20Thanh%20Trung%20Nguyen%20and%20Huy%20Hieu%20Pham%20and%20Thanh%20Hung%20Nguyen%20and%20Thao%20Nguyen%20Truong%20and%20Phi%20Le%20Nguyen%0AAbstract%3A%20%20%20Positron%20Emission%20Tomography%20%28PET%29%20and%20Computed%20Tomography%20%28CT%29%20are%20essential%0Afor%20diagnosing%2C%20staging%2C%20and%20monitoring%20various%20diseases%2C%20particularly%20cancer.%0ADespite%20their%20importance%2C%20the%20use%20of%20PET/CT%20systems%20is%20limited%20by%20the%20necessity%0Afor%20radioactive%20materials%2C%20the%20scarcity%20of%20PET%20scanners%2C%20and%20the%20high%20cost%0Aassociated%20with%20PET%20imaging.%20In%20contrast%2C%20CT%20scanners%20are%20more%20widely%20available%0Aand%20significantly%20less%20expensive.%20In%20response%20to%20these%20challenges%2C%20our%20study%0Aaddresses%20the%20issue%20of%20generating%20PET%20images%20from%20CT%20images%2C%20aiming%20to%20reduce%0Aboth%20the%20medical%20examination%20cost%20and%20the%20associated%20health%20risks%20for%20patients.%0AOur%20contributions%20are%20twofold%3A%20First%2C%20we%20introduce%20a%20conditional%20diffusion%0Amodel%20named%20CPDM%2C%20which%2C%20to%20our%20knowledge%2C%20is%20one%20of%20the%20initial%20attempts%20to%0Aemploy%20a%20diffusion%20model%20for%20translating%20from%20CT%20to%20PET%20images.%20Second%2C%20we%0Aprovide%20the%20largest%20CT-PET%20dataset%20to%20date%2C%20comprising%202%2C028%2C628%20paired%20CT-PET%0Aimages%2C%20which%20facilitates%20the%20training%20and%20evaluation%20of%20CT-to-PET%20translation%0Amodels.%20For%20the%20CPDM%20model%2C%20we%20incorporate%20domain%20knowledge%20to%20develop%20two%0Aconditional%20maps%3A%20the%20Attention%20map%20and%20the%20Attenuation%20map.%20The%20former%20helps%0Athe%20diffusion%20process%20focus%20on%20areas%20of%20interest%2C%20while%20the%20latter%20improves%20PET%0Adata%20correction%20and%20ensures%20accurate%20diagnostic%20information.%20Experimental%0Aevaluations%20across%20various%20benchmarks%20demonstrate%20that%20CPDM%20surpasses%20existing%0Amethods%20in%20generating%20high-quality%20PET%20images%20in%20terms%20of%20multiple%20metrics.%20The%0Asource%20code%20and%20data%20samples%20are%20available%20at%20https%3A//github.com/thanhhff/CPDM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT%2520to%2520PET%2520Translation%253A%2520A%2520Large-scale%2520Dataset%2520and%2520Domain-Knowledge-Guided%250A%2520%2520Diffusion%2520Approach%26entry.906535625%3DDac%2520Thai%2520Nguyen%2520and%2520Trung%2520Thanh%2520Nguyen%2520and%2520Huu%2520Tien%2520Nguyen%2520and%2520Thanh%2520Trung%2520Nguyen%2520and%2520Huy%2520Hieu%2520Pham%2520and%2520Thanh%2520Hung%2520Nguyen%2520and%2520Thao%2520Nguyen%2520Truong%2520and%2520Phi%2520Le%2520Nguyen%26entry.1292438233%3D%2520%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520and%2520Computed%2520Tomography%2520%2528CT%2529%2520are%2520essential%250Afor%2520diagnosing%252C%2520staging%252C%2520and%2520monitoring%2520various%2520diseases%252C%2520particularly%2520cancer.%250ADespite%2520their%2520importance%252C%2520the%2520use%2520of%2520PET/CT%2520systems%2520is%2520limited%2520by%2520the%2520necessity%250Afor%2520radioactive%2520materials%252C%2520the%2520scarcity%2520of%2520PET%2520scanners%252C%2520and%2520the%2520high%2520cost%250Aassociated%2520with%2520PET%2520imaging.%2520In%2520contrast%252C%2520CT%2520scanners%2520are%2520more%2520widely%2520available%250Aand%2520significantly%2520less%2520expensive.%2520In%2520response%2520to%2520these%2520challenges%252C%2520our%2520study%250Aaddresses%2520the%2520issue%2520of%2520generating%2520PET%2520images%2520from%2520CT%2520images%252C%2520aiming%2520to%2520reduce%250Aboth%2520the%2520medical%2520examination%2520cost%2520and%2520the%2520associated%2520health%2520risks%2520for%2520patients.%250AOur%2520contributions%2520are%2520twofold%253A%2520First%252C%2520we%2520introduce%2520a%2520conditional%2520diffusion%250Amodel%2520named%2520CPDM%252C%2520which%252C%2520to%2520our%2520knowledge%252C%2520is%2520one%2520of%2520the%2520initial%2520attempts%2520to%250Aemploy%2520a%2520diffusion%2520model%2520for%2520translating%2520from%2520CT%2520to%2520PET%2520images.%2520Second%252C%2520we%250Aprovide%2520the%2520largest%2520CT-PET%2520dataset%2520to%2520date%252C%2520comprising%25202%252C028%252C628%2520paired%2520CT-PET%250Aimages%252C%2520which%2520facilitates%2520the%2520training%2520and%2520evaluation%2520of%2520CT-to-PET%2520translation%250Amodels.%2520For%2520the%2520CPDM%2520model%252C%2520we%2520incorporate%2520domain%2520knowledge%2520to%2520develop%2520two%250Aconditional%2520maps%253A%2520the%2520Attention%2520map%2520and%2520the%2520Attenuation%2520map.%2520The%2520former%2520helps%250Athe%2520diffusion%2520process%2520focus%2520on%2520areas%2520of%2520interest%252C%2520while%2520the%2520latter%2520improves%2520PET%250Adata%2520correction%2520and%2520ensures%2520accurate%2520diagnostic%2520information.%2520Experimental%250Aevaluations%2520across%2520various%2520benchmarks%2520demonstrate%2520that%2520CPDM%2520surpasses%2520existing%250Amethods%2520in%2520generating%2520high-quality%2520PET%2520images%2520in%2520terms%2520of%2520multiple%2520metrics.%2520The%250Asource%2520code%2520and%2520data%2520samples%2520are%2520available%2520at%2520https%253A//github.com/thanhhff/CPDM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT%20to%20PET%20Translation%3A%20A%20Large-scale%20Dataset%20and%20Domain-Knowledge-Guided%0A%20%20Diffusion%20Approach&entry.906535625=Dac%20Thai%20Nguyen%20and%20Trung%20Thanh%20Nguyen%20and%20Huu%20Tien%20Nguyen%20and%20Thanh%20Trung%20Nguyen%20and%20Huy%20Hieu%20Pham%20and%20Thanh%20Hung%20Nguyen%20and%20Thao%20Nguyen%20Truong%20and%20Phi%20Le%20Nguyen&entry.1292438233=%20%20Positron%20Emission%20Tomography%20%28PET%29%20and%20Computed%20Tomography%20%28CT%29%20are%20essential%0Afor%20diagnosing%2C%20staging%2C%20and%20monitoring%20various%20diseases%2C%20particularly%20cancer.%0ADespite%20their%20importance%2C%20the%20use%20of%20PET/CT%20systems%20is%20limited%20by%20the%20necessity%0Afor%20radioactive%20materials%2C%20the%20scarcity%20of%20PET%20scanners%2C%20and%20the%20high%20cost%0Aassociated%20with%20PET%20imaging.%20In%20contrast%2C%20CT%20scanners%20are%20more%20widely%20available%0Aand%20significantly%20less%20expensive.%20In%20response%20to%20these%20challenges%2C%20our%20study%0Aaddresses%20the%20issue%20of%20generating%20PET%20images%20from%20CT%20images%2C%20aiming%20to%20reduce%0Aboth%20the%20medical%20examination%20cost%20and%20the%20associated%20health%20risks%20for%20patients.%0AOur%20contributions%20are%20twofold%3A%20First%2C%20we%20introduce%20a%20conditional%20diffusion%0Amodel%20named%20CPDM%2C%20which%2C%20to%20our%20knowledge%2C%20is%20one%20of%20the%20initial%20attempts%20to%0Aemploy%20a%20diffusion%20model%20for%20translating%20from%20CT%20to%20PET%20images.%20Second%2C%20we%0Aprovide%20the%20largest%20CT-PET%20dataset%20to%20date%2C%20comprising%202%2C028%2C628%20paired%20CT-PET%0Aimages%2C%20which%20facilitates%20the%20training%20and%20evaluation%20of%20CT-to-PET%20translation%0Amodels.%20For%20the%20CPDM%20model%2C%20we%20incorporate%20domain%20knowledge%20to%20develop%20two%0Aconditional%20maps%3A%20the%20Attention%20map%20and%20the%20Attenuation%20map.%20The%20former%20helps%0Athe%20diffusion%20process%20focus%20on%20areas%20of%20interest%2C%20while%20the%20latter%20improves%20PET%0Adata%20correction%20and%20ensures%20accurate%20diagnostic%20information.%20Experimental%0Aevaluations%20across%20various%20benchmarks%20demonstrate%20that%20CPDM%20surpasses%20existing%0Amethods%20in%20generating%20high-quality%20PET%20images%20in%20terms%20of%20multiple%20metrics.%20The%0Asource%20code%20and%20data%20samples%20are%20available%20at%20https%3A//github.com/thanhhff/CPDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21932v1&entry.124074799=Read"},
{"title": "Where Do Large Learning Rates Lead Us?", "author": "Ildus Sadrtdinov and Maxim Kodryan and Eduard Pokonechny and Ekaterina Lobacheva and Dmitry Vetrov", "abstract": "  It is generally accepted that starting neural networks training with large\nlearning rates (LRs) improves generalization. Following a line of research\ndevoted to understanding this effect, we conduct an empirical study in a\ncontrolled setting focusing on two questions: 1) how large an initial LR is\nrequired for obtaining optimal quality, and 2) what are the key differences\nbetween models trained with different LRs? We discover that only a narrow range\nof initial LRs slightly above the convergence threshold lead to optimal results\nafter fine-tuning with a small LR or weight averaging. By studying the local\ngeometry of reached minima, we observe that using LRs from this optimal range\nallows for the optimization to locate a basin that only contains high-quality\nminima. Additionally, we show that these initial LRs result in a sparse set of\nlearned features, with a clear focus on those most relevant for the task. In\ncontrast, starting training with too small LRs leads to unstable minima and\nattempts to learn all features simultaneously, resulting in poor\ngeneralization. Conversely, using initial LRs that are too large fails to\ndetect a basin with good solutions and extract meaningful patterns from the\ndata.\n", "link": "http://arxiv.org/abs/2410.22113v1", "date": "2024-10-29", "relevancy": 2.348, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4705}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4693}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20Do%20Large%20Learning%20Rates%20Lead%20Us%3F&body=Title%3A%20Where%20Do%20Large%20Learning%20Rates%20Lead%20Us%3F%0AAuthor%3A%20Ildus%20Sadrtdinov%20and%20Maxim%20Kodryan%20and%20Eduard%20Pokonechny%20and%20Ekaterina%20Lobacheva%20and%20Dmitry%20Vetrov%0AAbstract%3A%20%20%20It%20is%20generally%20accepted%20that%20starting%20neural%20networks%20training%20with%20large%0Alearning%20rates%20%28LRs%29%20improves%20generalization.%20Following%20a%20line%20of%20research%0Adevoted%20to%20understanding%20this%20effect%2C%20we%20conduct%20an%20empirical%20study%20in%20a%0Acontrolled%20setting%20focusing%20on%20two%20questions%3A%201%29%20how%20large%20an%20initial%20LR%20is%0Arequired%20for%20obtaining%20optimal%20quality%2C%20and%202%29%20what%20are%20the%20key%20differences%0Abetween%20models%20trained%20with%20different%20LRs%3F%20We%20discover%20that%20only%20a%20narrow%20range%0Aof%20initial%20LRs%20slightly%20above%20the%20convergence%20threshold%20lead%20to%20optimal%20results%0Aafter%20fine-tuning%20with%20a%20small%20LR%20or%20weight%20averaging.%20By%20studying%20the%20local%0Ageometry%20of%20reached%20minima%2C%20we%20observe%20that%20using%20LRs%20from%20this%20optimal%20range%0Aallows%20for%20the%20optimization%20to%20locate%20a%20basin%20that%20only%20contains%20high-quality%0Aminima.%20Additionally%2C%20we%20show%20that%20these%20initial%20LRs%20result%20in%20a%20sparse%20set%20of%0Alearned%20features%2C%20with%20a%20clear%20focus%20on%20those%20most%20relevant%20for%20the%20task.%20In%0Acontrast%2C%20starting%20training%20with%20too%20small%20LRs%20leads%20to%20unstable%20minima%20and%0Aattempts%20to%20learn%20all%20features%20simultaneously%2C%20resulting%20in%20poor%0Ageneralization.%20Conversely%2C%20using%20initial%20LRs%20that%20are%20too%20large%20fails%20to%0Adetect%20a%20basin%20with%20good%20solutions%20and%20extract%20meaningful%20patterns%20from%20the%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520Do%2520Large%2520Learning%2520Rates%2520Lead%2520Us%253F%26entry.906535625%3DIldus%2520Sadrtdinov%2520and%2520Maxim%2520Kodryan%2520and%2520Eduard%2520Pokonechny%2520and%2520Ekaterina%2520Lobacheva%2520and%2520Dmitry%2520Vetrov%26entry.1292438233%3D%2520%2520It%2520is%2520generally%2520accepted%2520that%2520starting%2520neural%2520networks%2520training%2520with%2520large%250Alearning%2520rates%2520%2528LRs%2529%2520improves%2520generalization.%2520Following%2520a%2520line%2520of%2520research%250Adevoted%2520to%2520understanding%2520this%2520effect%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520in%2520a%250Acontrolled%2520setting%2520focusing%2520on%2520two%2520questions%253A%25201%2529%2520how%2520large%2520an%2520initial%2520LR%2520is%250Arequired%2520for%2520obtaining%2520optimal%2520quality%252C%2520and%25202%2529%2520what%2520are%2520the%2520key%2520differences%250Abetween%2520models%2520trained%2520with%2520different%2520LRs%253F%2520We%2520discover%2520that%2520only%2520a%2520narrow%2520range%250Aof%2520initial%2520LRs%2520slightly%2520above%2520the%2520convergence%2520threshold%2520lead%2520to%2520optimal%2520results%250Aafter%2520fine-tuning%2520with%2520a%2520small%2520LR%2520or%2520weight%2520averaging.%2520By%2520studying%2520the%2520local%250Ageometry%2520of%2520reached%2520minima%252C%2520we%2520observe%2520that%2520using%2520LRs%2520from%2520this%2520optimal%2520range%250Aallows%2520for%2520the%2520optimization%2520to%2520locate%2520a%2520basin%2520that%2520only%2520contains%2520high-quality%250Aminima.%2520Additionally%252C%2520we%2520show%2520that%2520these%2520initial%2520LRs%2520result%2520in%2520a%2520sparse%2520set%2520of%250Alearned%2520features%252C%2520with%2520a%2520clear%2520focus%2520on%2520those%2520most%2520relevant%2520for%2520the%2520task.%2520In%250Acontrast%252C%2520starting%2520training%2520with%2520too%2520small%2520LRs%2520leads%2520to%2520unstable%2520minima%2520and%250Aattempts%2520to%2520learn%2520all%2520features%2520simultaneously%252C%2520resulting%2520in%2520poor%250Ageneralization.%2520Conversely%252C%2520using%2520initial%2520LRs%2520that%2520are%2520too%2520large%2520fails%2520to%250Adetect%2520a%2520basin%2520with%2520good%2520solutions%2520and%2520extract%2520meaningful%2520patterns%2520from%2520the%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20Do%20Large%20Learning%20Rates%20Lead%20Us%3F&entry.906535625=Ildus%20Sadrtdinov%20and%20Maxim%20Kodryan%20and%20Eduard%20Pokonechny%20and%20Ekaterina%20Lobacheva%20and%20Dmitry%20Vetrov&entry.1292438233=%20%20It%20is%20generally%20accepted%20that%20starting%20neural%20networks%20training%20with%20large%0Alearning%20rates%20%28LRs%29%20improves%20generalization.%20Following%20a%20line%20of%20research%0Adevoted%20to%20understanding%20this%20effect%2C%20we%20conduct%20an%20empirical%20study%20in%20a%0Acontrolled%20setting%20focusing%20on%20two%20questions%3A%201%29%20how%20large%20an%20initial%20LR%20is%0Arequired%20for%20obtaining%20optimal%20quality%2C%20and%202%29%20what%20are%20the%20key%20differences%0Abetween%20models%20trained%20with%20different%20LRs%3F%20We%20discover%20that%20only%20a%20narrow%20range%0Aof%20initial%20LRs%20slightly%20above%20the%20convergence%20threshold%20lead%20to%20optimal%20results%0Aafter%20fine-tuning%20with%20a%20small%20LR%20or%20weight%20averaging.%20By%20studying%20the%20local%0Ageometry%20of%20reached%20minima%2C%20we%20observe%20that%20using%20LRs%20from%20this%20optimal%20range%0Aallows%20for%20the%20optimization%20to%20locate%20a%20basin%20that%20only%20contains%20high-quality%0Aminima.%20Additionally%2C%20we%20show%20that%20these%20initial%20LRs%20result%20in%20a%20sparse%20set%20of%0Alearned%20features%2C%20with%20a%20clear%20focus%20on%20those%20most%20relevant%20for%20the%20task.%20In%0Acontrast%2C%20starting%20training%20with%20too%20small%20LRs%20leads%20to%20unstable%20minima%20and%0Aattempts%20to%20learn%20all%20features%20simultaneously%2C%20resulting%20in%20poor%0Ageneralization.%20Conversely%2C%20using%20initial%20LRs%20that%20are%20too%20large%20fails%20to%0Adetect%20a%20basin%20with%20good%20solutions%20and%20extract%20meaningful%20patterns%20from%20the%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22113v1&entry.124074799=Read"},
{"title": "Contrastive Sequential-Diffusion Learning: Non-linear and Multi-Scene\n  Instructional Video Synthesis", "author": "Vasco Ramos and Yonatan Bitton and Michal Yarom and Idan Szpektor and Joao Magalhaes", "abstract": "  Generated video scenes for action-centric sequence descriptions like recipe\ninstructions and do-it-yourself projects include non-linear patterns, in which\nthe next video may require to be visually consistent not on the immediate\nprevious video but on earlier ones. Current multi-scene video synthesis\napproaches fail to meet these consistency requirements. To address this, we\npropose a contrastive sequential video diffusion method that selects the most\nsuitable previously generated scene to guide and condition the denoising\nprocess of the next scene. The result is a multi-scene video that is grounded\nin the scene descriptions and coherent w.r.t the scenes that require visual\nconsistency. Experiments with real-world action-centric data demonstrate the\npracticality and improved consistency of our model compared to prior work.\n", "link": "http://arxiv.org/abs/2407.11814v2", "date": "2024-10-29", "relevancy": 2.3473, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6206}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Sequential-Diffusion%20Learning%3A%20Non-linear%20and%20Multi-Scene%0A%20%20Instructional%20Video%20Synthesis&body=Title%3A%20Contrastive%20Sequential-Diffusion%20Learning%3A%20Non-linear%20and%20Multi-Scene%0A%20%20Instructional%20Video%20Synthesis%0AAuthor%3A%20Vasco%20Ramos%20and%20Yonatan%20Bitton%20and%20Michal%20Yarom%20and%20Idan%20Szpektor%20and%20Joao%20Magalhaes%0AAbstract%3A%20%20%20Generated%20video%20scenes%20for%20action-centric%20sequence%20descriptions%20like%20recipe%0Ainstructions%20and%20do-it-yourself%20projects%20include%20non-linear%20patterns%2C%20in%20which%0Athe%20next%20video%20may%20require%20to%20be%20visually%20consistent%20not%20on%20the%20immediate%0Aprevious%20video%20but%20on%20earlier%20ones.%20Current%20multi-scene%20video%20synthesis%0Aapproaches%20fail%20to%20meet%20these%20consistency%20requirements.%20To%20address%20this%2C%20we%0Apropose%20a%20contrastive%20sequential%20video%20diffusion%20method%20that%20selects%20the%20most%0Asuitable%20previously%20generated%20scene%20to%20guide%20and%20condition%20the%20denoising%0Aprocess%20of%20the%20next%20scene.%20The%20result%20is%20a%20multi-scene%20video%20that%20is%20grounded%0Ain%20the%20scene%20descriptions%20and%20coherent%20w.r.t%20the%20scenes%20that%20require%20visual%0Aconsistency.%20Experiments%20with%20real-world%20action-centric%20data%20demonstrate%20the%0Apracticality%20and%20improved%20consistency%20of%20our%20model%20compared%20to%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Sequential-Diffusion%2520Learning%253A%2520Non-linear%2520and%2520Multi-Scene%250A%2520%2520Instructional%2520Video%2520Synthesis%26entry.906535625%3DVasco%2520Ramos%2520and%2520Yonatan%2520Bitton%2520and%2520Michal%2520Yarom%2520and%2520Idan%2520Szpektor%2520and%2520Joao%2520Magalhaes%26entry.1292438233%3D%2520%2520Generated%2520video%2520scenes%2520for%2520action-centric%2520sequence%2520descriptions%2520like%2520recipe%250Ainstructions%2520and%2520do-it-yourself%2520projects%2520include%2520non-linear%2520patterns%252C%2520in%2520which%250Athe%2520next%2520video%2520may%2520require%2520to%2520be%2520visually%2520consistent%2520not%2520on%2520the%2520immediate%250Aprevious%2520video%2520but%2520on%2520earlier%2520ones.%2520Current%2520multi-scene%2520video%2520synthesis%250Aapproaches%2520fail%2520to%2520meet%2520these%2520consistency%2520requirements.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520contrastive%2520sequential%2520video%2520diffusion%2520method%2520that%2520selects%2520the%2520most%250Asuitable%2520previously%2520generated%2520scene%2520to%2520guide%2520and%2520condition%2520the%2520denoising%250Aprocess%2520of%2520the%2520next%2520scene.%2520The%2520result%2520is%2520a%2520multi-scene%2520video%2520that%2520is%2520grounded%250Ain%2520the%2520scene%2520descriptions%2520and%2520coherent%2520w.r.t%2520the%2520scenes%2520that%2520require%2520visual%250Aconsistency.%2520Experiments%2520with%2520real-world%2520action-centric%2520data%2520demonstrate%2520the%250Apracticality%2520and%2520improved%2520consistency%2520of%2520our%2520model%2520compared%2520to%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Sequential-Diffusion%20Learning%3A%20Non-linear%20and%20Multi-Scene%0A%20%20Instructional%20Video%20Synthesis&entry.906535625=Vasco%20Ramos%20and%20Yonatan%20Bitton%20and%20Michal%20Yarom%20and%20Idan%20Szpektor%20and%20Joao%20Magalhaes&entry.1292438233=%20%20Generated%20video%20scenes%20for%20action-centric%20sequence%20descriptions%20like%20recipe%0Ainstructions%20and%20do-it-yourself%20projects%20include%20non-linear%20patterns%2C%20in%20which%0Athe%20next%20video%20may%20require%20to%20be%20visually%20consistent%20not%20on%20the%20immediate%0Aprevious%20video%20but%20on%20earlier%20ones.%20Current%20multi-scene%20video%20synthesis%0Aapproaches%20fail%20to%20meet%20these%20consistency%20requirements.%20To%20address%20this%2C%20we%0Apropose%20a%20contrastive%20sequential%20video%20diffusion%20method%20that%20selects%20the%20most%0Asuitable%20previously%20generated%20scene%20to%20guide%20and%20condition%20the%20denoising%0Aprocess%20of%20the%20next%20scene.%20The%20result%20is%20a%20multi-scene%20video%20that%20is%20grounded%0Ain%20the%20scene%20descriptions%20and%20coherent%20w.r.t%20the%20scenes%20that%20require%20visual%0Aconsistency.%20Experiments%20with%20real-world%20action-centric%20data%20demonstrate%20the%0Apracticality%20and%20improved%20consistency%20of%20our%20model%20compared%20to%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11814v2&entry.124074799=Read"},
{"title": "Weak-to-Strong Search: Align Large Language Models via Searching over\n  Small Language Models", "author": "Zhanhui Zhou and Zhixuan Liu and Jie Liu and Zhichen Dong and Chao Yang and Yu Qiao", "abstract": "  Large language models are usually fine-tuned to align with human preferences.\nHowever, fine-tuning a large language model can be challenging. In this work,\nwe introduce $\\textit{weak-to-strong search}$, framing the alignment of a large\nlanguage model as a test-time greedy search to maximize the log-probability\ndifference between small tuned and untuned models while sampling from the\nfrozen large model. This method serves both as (1) a compute-efficient model\nup-scaling strategy that avoids directly tuning the large model and as (2) an\ninstance of weak-to-strong generalization that enhances a strong model with\nweak test-time guidance. Empirically, we demonstrate the flexibility of\nweak-to-strong search across different tasks. In controlled-sentiment\ngeneration and summarization, we use tuned and untuned $\\texttt{gpt2}$s to\nimprove the alignment of large models without additional training. Crucially,\nin a more difficult instruction-following benchmark, AlpacaEval 2.0, we show\nthat reusing off-the-shelf small models (e.g., $\\texttt{zephyr-7b-beta}$ and\nits untuned version) can improve the length-controlled win rates of both\nwhite-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g.,\n$34.4\\% \\rightarrow 37.9\\%$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0\\%\n\\rightarrow 20.1\\%$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small\nmodels' low win rates $\\approx 10.0\\%$.\n", "link": "http://arxiv.org/abs/2405.19262v2", "date": "2024-10-29", "relevancy": 2.3364, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4868}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak-to-Strong%20Search%3A%20Align%20Large%20Language%20Models%20via%20Searching%20over%0A%20%20Small%20Language%20Models&body=Title%3A%20Weak-to-Strong%20Search%3A%20Align%20Large%20Language%20Models%20via%20Searching%20over%0A%20%20Small%20Language%20Models%0AAuthor%3A%20Zhanhui%20Zhou%20and%20Zhixuan%20Liu%20and%20Jie%20Liu%20and%20Zhichen%20Dong%20and%20Chao%20Yang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Large%20language%20models%20are%20usually%20fine-tuned%20to%20align%20with%20human%20preferences.%0AHowever%2C%20fine-tuning%20a%20large%20language%20model%20can%20be%20challenging.%20In%20this%20work%2C%0Awe%20introduce%20%24%5Ctextit%7Bweak-to-strong%20search%7D%24%2C%20framing%20the%20alignment%20of%20a%20large%0Alanguage%20model%20as%20a%20test-time%20greedy%20search%20to%20maximize%20the%20log-probability%0Adifference%20between%20small%20tuned%20and%20untuned%20models%20while%20sampling%20from%20the%0Afrozen%20large%20model.%20This%20method%20serves%20both%20as%20%281%29%20a%20compute-efficient%20model%0Aup-scaling%20strategy%20that%20avoids%20directly%20tuning%20the%20large%20model%20and%20as%20%282%29%20an%0Ainstance%20of%20weak-to-strong%20generalization%20that%20enhances%20a%20strong%20model%20with%0Aweak%20test-time%20guidance.%20Empirically%2C%20we%20demonstrate%20the%20flexibility%20of%0Aweak-to-strong%20search%20across%20different%20tasks.%20In%20controlled-sentiment%0Ageneration%20and%20summarization%2C%20we%20use%20tuned%20and%20untuned%20%24%5Ctexttt%7Bgpt2%7D%24s%20to%0Aimprove%20the%20alignment%20of%20large%20models%20without%20additional%20training.%20Crucially%2C%0Ain%20a%20more%20difficult%20instruction-following%20benchmark%2C%20AlpacaEval%202.0%2C%20we%20show%0Athat%20reusing%20off-the-shelf%20small%20models%20%28e.g.%2C%20%24%5Ctexttt%7Bzephyr-7b-beta%7D%24%20and%0Aits%20untuned%20version%29%20can%20improve%20the%20length-controlled%20win%20rates%20of%20both%0Awhite-box%20and%20black-box%20large%20models%20against%20%24%5Ctexttt%7Bgpt-4-turbo%7D%24%20%28e.g.%2C%0A%2434.4%5C%25%20%5Crightarrow%2037.9%5C%25%24%20for%20%24%5Ctexttt%7BLlama-3-70B-Instruct%7D%24%20and%20%2416.0%5C%25%0A%5Crightarrow%2020.1%5C%25%24%20for%20%24%5Ctexttt%7Bgpt-3.5-turbo-instruct%7D%24%29%2C%20despite%20the%20small%0Amodels%27%20low%20win%20rates%20%24%5Capprox%2010.0%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak-to-Strong%2520Search%253A%2520Align%2520Large%2520Language%2520Models%2520via%2520Searching%2520over%250A%2520%2520Small%2520Language%2520Models%26entry.906535625%3DZhanhui%2520Zhou%2520and%2520Zhixuan%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Zhichen%2520Dong%2520and%2520Chao%2520Yang%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520usually%2520fine-tuned%2520to%2520align%2520with%2520human%2520preferences.%250AHowever%252C%2520fine-tuning%2520a%2520large%2520language%2520model%2520can%2520be%2520challenging.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520%2524%255Ctextit%257Bweak-to-strong%2520search%257D%2524%252C%2520framing%2520the%2520alignment%2520of%2520a%2520large%250Alanguage%2520model%2520as%2520a%2520test-time%2520greedy%2520search%2520to%2520maximize%2520the%2520log-probability%250Adifference%2520between%2520small%2520tuned%2520and%2520untuned%2520models%2520while%2520sampling%2520from%2520the%250Afrozen%2520large%2520model.%2520This%2520method%2520serves%2520both%2520as%2520%25281%2529%2520a%2520compute-efficient%2520model%250Aup-scaling%2520strategy%2520that%2520avoids%2520directly%2520tuning%2520the%2520large%2520model%2520and%2520as%2520%25282%2529%2520an%250Ainstance%2520of%2520weak-to-strong%2520generalization%2520that%2520enhances%2520a%2520strong%2520model%2520with%250Aweak%2520test-time%2520guidance.%2520Empirically%252C%2520we%2520demonstrate%2520the%2520flexibility%2520of%250Aweak-to-strong%2520search%2520across%2520different%2520tasks.%2520In%2520controlled-sentiment%250Ageneration%2520and%2520summarization%252C%2520we%2520use%2520tuned%2520and%2520untuned%2520%2524%255Ctexttt%257Bgpt2%257D%2524s%2520to%250Aimprove%2520the%2520alignment%2520of%2520large%2520models%2520without%2520additional%2520training.%2520Crucially%252C%250Ain%2520a%2520more%2520difficult%2520instruction-following%2520benchmark%252C%2520AlpacaEval%25202.0%252C%2520we%2520show%250Athat%2520reusing%2520off-the-shelf%2520small%2520models%2520%2528e.g.%252C%2520%2524%255Ctexttt%257Bzephyr-7b-beta%257D%2524%2520and%250Aits%2520untuned%2520version%2529%2520can%2520improve%2520the%2520length-controlled%2520win%2520rates%2520of%2520both%250Awhite-box%2520and%2520black-box%2520large%2520models%2520against%2520%2524%255Ctexttt%257Bgpt-4-turbo%257D%2524%2520%2528e.g.%252C%250A%252434.4%255C%2525%2520%255Crightarrow%252037.9%255C%2525%2524%2520for%2520%2524%255Ctexttt%257BLlama-3-70B-Instruct%257D%2524%2520and%2520%252416.0%255C%2525%250A%255Crightarrow%252020.1%255C%2525%2524%2520for%2520%2524%255Ctexttt%257Bgpt-3.5-turbo-instruct%257D%2524%2529%252C%2520despite%2520the%2520small%250Amodels%2527%2520low%2520win%2520rates%2520%2524%255Capprox%252010.0%255C%2525%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak-to-Strong%20Search%3A%20Align%20Large%20Language%20Models%20via%20Searching%20over%0A%20%20Small%20Language%20Models&entry.906535625=Zhanhui%20Zhou%20and%20Zhixuan%20Liu%20and%20Jie%20Liu%20and%20Zhichen%20Dong%20and%20Chao%20Yang%20and%20Yu%20Qiao&entry.1292438233=%20%20Large%20language%20models%20are%20usually%20fine-tuned%20to%20align%20with%20human%20preferences.%0AHowever%2C%20fine-tuning%20a%20large%20language%20model%20can%20be%20challenging.%20In%20this%20work%2C%0Awe%20introduce%20%24%5Ctextit%7Bweak-to-strong%20search%7D%24%2C%20framing%20the%20alignment%20of%20a%20large%0Alanguage%20model%20as%20a%20test-time%20greedy%20search%20to%20maximize%20the%20log-probability%0Adifference%20between%20small%20tuned%20and%20untuned%20models%20while%20sampling%20from%20the%0Afrozen%20large%20model.%20This%20method%20serves%20both%20as%20%281%29%20a%20compute-efficient%20model%0Aup-scaling%20strategy%20that%20avoids%20directly%20tuning%20the%20large%20model%20and%20as%20%282%29%20an%0Ainstance%20of%20weak-to-strong%20generalization%20that%20enhances%20a%20strong%20model%20with%0Aweak%20test-time%20guidance.%20Empirically%2C%20we%20demonstrate%20the%20flexibility%20of%0Aweak-to-strong%20search%20across%20different%20tasks.%20In%20controlled-sentiment%0Ageneration%20and%20summarization%2C%20we%20use%20tuned%20and%20untuned%20%24%5Ctexttt%7Bgpt2%7D%24s%20to%0Aimprove%20the%20alignment%20of%20large%20models%20without%20additional%20training.%20Crucially%2C%0Ain%20a%20more%20difficult%20instruction-following%20benchmark%2C%20AlpacaEval%202.0%2C%20we%20show%0Athat%20reusing%20off-the-shelf%20small%20models%20%28e.g.%2C%20%24%5Ctexttt%7Bzephyr-7b-beta%7D%24%20and%0Aits%20untuned%20version%29%20can%20improve%20the%20length-controlled%20win%20rates%20of%20both%0Awhite-box%20and%20black-box%20large%20models%20against%20%24%5Ctexttt%7Bgpt-4-turbo%7D%24%20%28e.g.%2C%0A%2434.4%5C%25%20%5Crightarrow%2037.9%5C%25%24%20for%20%24%5Ctexttt%7BLlama-3-70B-Instruct%7D%24%20and%20%2416.0%5C%25%0A%5Crightarrow%2020.1%5C%25%24%20for%20%24%5Ctexttt%7Bgpt-3.5-turbo-instruct%7D%24%29%2C%20despite%20the%20small%0Amodels%27%20low%20win%20rates%20%24%5Capprox%2010.0%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19262v2&entry.124074799=Read"},
{"title": "Optimizing Posterior Samples for Bayesian Optimization via Rootfinding", "author": "Taiwo A. Adebiyi and Bach Do and Ruda Zhang", "abstract": "  Bayesian optimization devolves the global optimization of a costly objective\nfunction to the global optimization of a sequence of acquisition functions.\nThis inner-loop optimization can be catastrophically difficult if it involves\nposterior samples, especially in higher dimensions. We introduce an efficient\nglobal optimization strategy for posterior samples based on global rootfinding.\nIt provides gradient-based optimizers with judiciously selected starting\npoints, designed to combine exploitation and exploration. The algorithm scales\npractically linearly to high dimensions. For posterior sample-based acquisition\nfunctions such as Gaussian process Thompson sampling (GP-TS) and variants of\nentropy search, we demonstrate remarkable improvement in both inner- and\nouter-loop optimization, surprisingly outperforming alternatives like EI and\nGP-UCB in most cases. We also propose a sample-average formulation of GP-TS,\nwhich has a parameter to explicitly control exploitation and can be computed at\nthe cost of one posterior sample. Our implementation is available at\nhttps://github.com/UQUH/TSRoots .\n", "link": "http://arxiv.org/abs/2410.22322v1", "date": "2024-10-29", "relevancy": 2.3271, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4821}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4587}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Posterior%20Samples%20for%20Bayesian%20Optimization%20via%20Rootfinding&body=Title%3A%20Optimizing%20Posterior%20Samples%20for%20Bayesian%20Optimization%20via%20Rootfinding%0AAuthor%3A%20Taiwo%20A.%20Adebiyi%20and%20Bach%20Do%20and%20Ruda%20Zhang%0AAbstract%3A%20%20%20Bayesian%20optimization%20devolves%20the%20global%20optimization%20of%20a%20costly%20objective%0Afunction%20to%20the%20global%20optimization%20of%20a%20sequence%20of%20acquisition%20functions.%0AThis%20inner-loop%20optimization%20can%20be%20catastrophically%20difficult%20if%20it%20involves%0Aposterior%20samples%2C%20especially%20in%20higher%20dimensions.%20We%20introduce%20an%20efficient%0Aglobal%20optimization%20strategy%20for%20posterior%20samples%20based%20on%20global%20rootfinding.%0AIt%20provides%20gradient-based%20optimizers%20with%20judiciously%20selected%20starting%0Apoints%2C%20designed%20to%20combine%20exploitation%20and%20exploration.%20The%20algorithm%20scales%0Apractically%20linearly%20to%20high%20dimensions.%20For%20posterior%20sample-based%20acquisition%0Afunctions%20such%20as%20Gaussian%20process%20Thompson%20sampling%20%28GP-TS%29%20and%20variants%20of%0Aentropy%20search%2C%20we%20demonstrate%20remarkable%20improvement%20in%20both%20inner-%20and%0Aouter-loop%20optimization%2C%20surprisingly%20outperforming%20alternatives%20like%20EI%20and%0AGP-UCB%20in%20most%20cases.%20We%20also%20propose%20a%20sample-average%20formulation%20of%20GP-TS%2C%0Awhich%20has%20a%20parameter%20to%20explicitly%20control%20exploitation%20and%20can%20be%20computed%20at%0Athe%20cost%20of%20one%20posterior%20sample.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/UQUH/TSRoots%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Posterior%2520Samples%2520for%2520Bayesian%2520Optimization%2520via%2520Rootfinding%26entry.906535625%3DTaiwo%2520A.%2520Adebiyi%2520and%2520Bach%2520Do%2520and%2520Ruda%2520Zhang%26entry.1292438233%3D%2520%2520Bayesian%2520optimization%2520devolves%2520the%2520global%2520optimization%2520of%2520a%2520costly%2520objective%250Afunction%2520to%2520the%2520global%2520optimization%2520of%2520a%2520sequence%2520of%2520acquisition%2520functions.%250AThis%2520inner-loop%2520optimization%2520can%2520be%2520catastrophically%2520difficult%2520if%2520it%2520involves%250Aposterior%2520samples%252C%2520especially%2520in%2520higher%2520dimensions.%2520We%2520introduce%2520an%2520efficient%250Aglobal%2520optimization%2520strategy%2520for%2520posterior%2520samples%2520based%2520on%2520global%2520rootfinding.%250AIt%2520provides%2520gradient-based%2520optimizers%2520with%2520judiciously%2520selected%2520starting%250Apoints%252C%2520designed%2520to%2520combine%2520exploitation%2520and%2520exploration.%2520The%2520algorithm%2520scales%250Apractically%2520linearly%2520to%2520high%2520dimensions.%2520For%2520posterior%2520sample-based%2520acquisition%250Afunctions%2520such%2520as%2520Gaussian%2520process%2520Thompson%2520sampling%2520%2528GP-TS%2529%2520and%2520variants%2520of%250Aentropy%2520search%252C%2520we%2520demonstrate%2520remarkable%2520improvement%2520in%2520both%2520inner-%2520and%250Aouter-loop%2520optimization%252C%2520surprisingly%2520outperforming%2520alternatives%2520like%2520EI%2520and%250AGP-UCB%2520in%2520most%2520cases.%2520We%2520also%2520propose%2520a%2520sample-average%2520formulation%2520of%2520GP-TS%252C%250Awhich%2520has%2520a%2520parameter%2520to%2520explicitly%2520control%2520exploitation%2520and%2520can%2520be%2520computed%2520at%250Athe%2520cost%2520of%2520one%2520posterior%2520sample.%2520Our%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/UQUH/TSRoots%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Posterior%20Samples%20for%20Bayesian%20Optimization%20via%20Rootfinding&entry.906535625=Taiwo%20A.%20Adebiyi%20and%20Bach%20Do%20and%20Ruda%20Zhang&entry.1292438233=%20%20Bayesian%20optimization%20devolves%20the%20global%20optimization%20of%20a%20costly%20objective%0Afunction%20to%20the%20global%20optimization%20of%20a%20sequence%20of%20acquisition%20functions.%0AThis%20inner-loop%20optimization%20can%20be%20catastrophically%20difficult%20if%20it%20involves%0Aposterior%20samples%2C%20especially%20in%20higher%20dimensions.%20We%20introduce%20an%20efficient%0Aglobal%20optimization%20strategy%20for%20posterior%20samples%20based%20on%20global%20rootfinding.%0AIt%20provides%20gradient-based%20optimizers%20with%20judiciously%20selected%20starting%0Apoints%2C%20designed%20to%20combine%20exploitation%20and%20exploration.%20The%20algorithm%20scales%0Apractically%20linearly%20to%20high%20dimensions.%20For%20posterior%20sample-based%20acquisition%0Afunctions%20such%20as%20Gaussian%20process%20Thompson%20sampling%20%28GP-TS%29%20and%20variants%20of%0Aentropy%20search%2C%20we%20demonstrate%20remarkable%20improvement%20in%20both%20inner-%20and%0Aouter-loop%20optimization%2C%20surprisingly%20outperforming%20alternatives%20like%20EI%20and%0AGP-UCB%20in%20most%20cases.%20We%20also%20propose%20a%20sample-average%20formulation%20of%20GP-TS%2C%0Awhich%20has%20a%20parameter%20to%20explicitly%20control%20exploitation%20and%20can%20be%20computed%20at%0Athe%20cost%20of%20one%20posterior%20sample.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/UQUH/TSRoots%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22322v1&entry.124074799=Read"},
{"title": "Analyzing Multimodal Interaction Strategies for LLM-Assisted\n  Manipulation of 3D Scenes", "author": "Junlong Chen and Jens Grubert and Per Ola Kristensson", "abstract": "  As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments.\n", "link": "http://arxiv.org/abs/2410.22177v1", "date": "2024-10-29", "relevancy": 2.3238, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Multimodal%20Interaction%20Strategies%20for%20LLM-Assisted%0A%20%20Manipulation%20of%203D%20Scenes&body=Title%3A%20Analyzing%20Multimodal%20Interaction%20Strategies%20for%20LLM-Assisted%0A%20%20Manipulation%20of%203D%20Scenes%0AAuthor%3A%20Junlong%20Chen%20and%20Jens%20Grubert%20and%20Per%20Ola%20Kristensson%0AAbstract%3A%20%20%20As%20more%20applications%20of%20large%20language%20models%20%28LLMs%29%20for%203D%20content%20for%0Aimmersive%20environments%20emerge%2C%20it%20is%20crucial%20to%20study%20user%20behaviour%20to%0Aidentify%20interaction%20patterns%20and%20potential%20barriers%20to%20guide%20the%20future%20design%0Aof%20immersive%20content%20creation%20and%20editing%20systems%20which%20involve%20LLMs.%20In%20an%0Aempirical%20user%20study%20with%2012%20participants%2C%20we%20combine%20quantitative%20usage%20data%0Awith%20post-experience%20questionnaire%20feedback%20to%20reveal%20common%20interaction%0Apatterns%20and%20key%20barriers%20in%20LLM-assisted%203D%20scene%20editing%20systems.%20We%20identify%0Aopportunities%20for%20improving%20natural%20language%20interfaces%20in%203D%20design%20tools%20and%0Apropose%20design%20recommendations%20for%20future%20LLM-integrated%203D%20content%20creation%0Asystems.%20Through%20an%20empirical%20study%2C%20we%20demonstrate%20that%20LLM-assisted%0Ainteractive%20systems%20can%20be%20used%20productively%20in%20immersive%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Multimodal%2520Interaction%2520Strategies%2520for%2520LLM-Assisted%250A%2520%2520Manipulation%2520of%25203D%2520Scenes%26entry.906535625%3DJunlong%2520Chen%2520and%2520Jens%2520Grubert%2520and%2520Per%2520Ola%2520Kristensson%26entry.1292438233%3D%2520%2520As%2520more%2520applications%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%25203D%2520content%2520for%250Aimmersive%2520environments%2520emerge%252C%2520it%2520is%2520crucial%2520to%2520study%2520user%2520behaviour%2520to%250Aidentify%2520interaction%2520patterns%2520and%2520potential%2520barriers%2520to%2520guide%2520the%2520future%2520design%250Aof%2520immersive%2520content%2520creation%2520and%2520editing%2520systems%2520which%2520involve%2520LLMs.%2520In%2520an%250Aempirical%2520user%2520study%2520with%252012%2520participants%252C%2520we%2520combine%2520quantitative%2520usage%2520data%250Awith%2520post-experience%2520questionnaire%2520feedback%2520to%2520reveal%2520common%2520interaction%250Apatterns%2520and%2520key%2520barriers%2520in%2520LLM-assisted%25203D%2520scene%2520editing%2520systems.%2520We%2520identify%250Aopportunities%2520for%2520improving%2520natural%2520language%2520interfaces%2520in%25203D%2520design%2520tools%2520and%250Apropose%2520design%2520recommendations%2520for%2520future%2520LLM-integrated%25203D%2520content%2520creation%250Asystems.%2520Through%2520an%2520empirical%2520study%252C%2520we%2520demonstrate%2520that%2520LLM-assisted%250Ainteractive%2520systems%2520can%2520be%2520used%2520productively%2520in%2520immersive%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Multimodal%20Interaction%20Strategies%20for%20LLM-Assisted%0A%20%20Manipulation%20of%203D%20Scenes&entry.906535625=Junlong%20Chen%20and%20Jens%20Grubert%20and%20Per%20Ola%20Kristensson&entry.1292438233=%20%20As%20more%20applications%20of%20large%20language%20models%20%28LLMs%29%20for%203D%20content%20for%0Aimmersive%20environments%20emerge%2C%20it%20is%20crucial%20to%20study%20user%20behaviour%20to%0Aidentify%20interaction%20patterns%20and%20potential%20barriers%20to%20guide%20the%20future%20design%0Aof%20immersive%20content%20creation%20and%20editing%20systems%20which%20involve%20LLMs.%20In%20an%0Aempirical%20user%20study%20with%2012%20participants%2C%20we%20combine%20quantitative%20usage%20data%0Awith%20post-experience%20questionnaire%20feedback%20to%20reveal%20common%20interaction%0Apatterns%20and%20key%20barriers%20in%20LLM-assisted%203D%20scene%20editing%20systems.%20We%20identify%0Aopportunities%20for%20improving%20natural%20language%20interfaces%20in%203D%20design%20tools%20and%0Apropose%20design%20recommendations%20for%20future%20LLM-integrated%203D%20content%20creation%0Asystems.%20Through%20an%20empirical%20study%2C%20we%20demonstrate%20that%20LLM-assisted%0Ainteractive%20systems%20can%20be%20used%20productively%20in%20immersive%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22177v1&entry.124074799=Read"},
{"title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent", "author": "Xiao Xia and Dan Zhang and Zibo Liao and Zhenyu Hou and Tianrui Sun and Jing Li and Ling Fu and Yuxiao Dong", "abstract": "  The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .\n", "link": "http://arxiv.org/abs/2410.21909v1", "date": "2024-10-29", "relevancy": 2.3016, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneGenAgent%3A%20Precise%20Industrial%20Scene%20Generation%20with%20Coding%20Agent&body=Title%3A%20SceneGenAgent%3A%20Precise%20Industrial%20Scene%20Generation%20with%20Coding%20Agent%0AAuthor%3A%20Xiao%20Xia%20and%20Dan%20Zhang%20and%20Zibo%20Liao%20and%20Zhenyu%20Hou%20and%20Tianrui%20Sun%20and%20Jing%20Li%20and%20Ling%20Fu%20and%20Yuxiao%20Dong%0AAbstract%3A%20%20%20The%20modeling%20of%20industrial%20scenes%20is%20essential%20for%20simulations%20in%20industrial%0Amanufacturing.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%0Aprogress%20in%20generating%20general%203D%20scenes%20from%20textual%20descriptions%2C%20generating%0Aindustrial%20scenes%20with%20LLMs%20poses%20a%20unique%20challenge%20due%20to%20their%20demand%20for%0Aprecise%20measurements%20and%20positioning%2C%20requiring%20complex%20planning%20over%20spatial%0Aarrangement.%20To%20address%20this%20challenge%2C%20we%20introduce%20SceneGenAgent%2C%20an%0ALLM-based%20agent%20for%20generating%20industrial%20scenes%20through%20C%23%20code.%20SceneGenAgent%0Aensures%20precise%20layout%20planning%20through%20a%20structured%20and%20calculable%20format%2C%0Alayout%20verification%2C%20and%20iterative%20refinement%20to%20meet%20the%20quantitative%0Arequirements%20of%20industrial%20scenarios.%20Experiment%20results%20demonstrate%20that%20LLMs%0Apowered%20by%20SceneGenAgent%20exceed%20their%20original%20performance%2C%20reaching%20up%20to%0A81.0%25%20success%20rate%20in%20real-world%20industrial%20scene%20generation%20tasks%20and%0Aeffectively%20meeting%20most%20scene%20generation%20requirements.%20To%20further%20enhance%0Aaccessibility%2C%20we%20construct%20SceneInstruct%2C%20a%20dataset%20designed%20for%20fine-tuning%0Aopen-source%20LLMs%20to%20integrate%20into%20SceneGenAgent.%20Experiments%20show%20that%0Afine-tuning%20open-source%20LLMs%20on%20SceneInstruct%20yields%20significant%20performance%0Aimprovements%2C%20with%20Llama3.1-70B%20approaching%20the%20capabilities%20of%20GPT-4o.%20Our%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/THUDM/SceneGenAgent%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneGenAgent%253A%2520Precise%2520Industrial%2520Scene%2520Generation%2520with%2520Coding%2520Agent%26entry.906535625%3DXiao%2520Xia%2520and%2520Dan%2520Zhang%2520and%2520Zibo%2520Liao%2520and%2520Zhenyu%2520Hou%2520and%2520Tianrui%2520Sun%2520and%2520Jing%2520Li%2520and%2520Ling%2520Fu%2520and%2520Yuxiao%2520Dong%26entry.1292438233%3D%2520%2520The%2520modeling%2520of%2520industrial%2520scenes%2520is%2520essential%2520for%2520simulations%2520in%2520industrial%250Amanufacturing.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%250Aprogress%2520in%2520generating%2520general%25203D%2520scenes%2520from%2520textual%2520descriptions%252C%2520generating%250Aindustrial%2520scenes%2520with%2520LLMs%2520poses%2520a%2520unique%2520challenge%2520due%2520to%2520their%2520demand%2520for%250Aprecise%2520measurements%2520and%2520positioning%252C%2520requiring%2520complex%2520planning%2520over%2520spatial%250Aarrangement.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SceneGenAgent%252C%2520an%250ALLM-based%2520agent%2520for%2520generating%2520industrial%2520scenes%2520through%2520C%2523%2520code.%2520SceneGenAgent%250Aensures%2520precise%2520layout%2520planning%2520through%2520a%2520structured%2520and%2520calculable%2520format%252C%250Alayout%2520verification%252C%2520and%2520iterative%2520refinement%2520to%2520meet%2520the%2520quantitative%250Arequirements%2520of%2520industrial%2520scenarios.%2520Experiment%2520results%2520demonstrate%2520that%2520LLMs%250Apowered%2520by%2520SceneGenAgent%2520exceed%2520their%2520original%2520performance%252C%2520reaching%2520up%2520to%250A81.0%2525%2520success%2520rate%2520in%2520real-world%2520industrial%2520scene%2520generation%2520tasks%2520and%250Aeffectively%2520meeting%2520most%2520scene%2520generation%2520requirements.%2520To%2520further%2520enhance%250Aaccessibility%252C%2520we%2520construct%2520SceneInstruct%252C%2520a%2520dataset%2520designed%2520for%2520fine-tuning%250Aopen-source%2520LLMs%2520to%2520integrate%2520into%2520SceneGenAgent.%2520Experiments%2520show%2520that%250Afine-tuning%2520open-source%2520LLMs%2520on%2520SceneInstruct%2520yields%2520significant%2520performance%250Aimprovements%252C%2520with%2520Llama3.1-70B%2520approaching%2520the%2520capabilities%2520of%2520GPT-4o.%2520Our%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/THUDM/SceneGenAgent%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneGenAgent%3A%20Precise%20Industrial%20Scene%20Generation%20with%20Coding%20Agent&entry.906535625=Xiao%20Xia%20and%20Dan%20Zhang%20and%20Zibo%20Liao%20and%20Zhenyu%20Hou%20and%20Tianrui%20Sun%20and%20Jing%20Li%20and%20Ling%20Fu%20and%20Yuxiao%20Dong&entry.1292438233=%20%20The%20modeling%20of%20industrial%20scenes%20is%20essential%20for%20simulations%20in%20industrial%0Amanufacturing.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%0Aprogress%20in%20generating%20general%203D%20scenes%20from%20textual%20descriptions%2C%20generating%0Aindustrial%20scenes%20with%20LLMs%20poses%20a%20unique%20challenge%20due%20to%20their%20demand%20for%0Aprecise%20measurements%20and%20positioning%2C%20requiring%20complex%20planning%20over%20spatial%0Aarrangement.%20To%20address%20this%20challenge%2C%20we%20introduce%20SceneGenAgent%2C%20an%0ALLM-based%20agent%20for%20generating%20industrial%20scenes%20through%20C%23%20code.%20SceneGenAgent%0Aensures%20precise%20layout%20planning%20through%20a%20structured%20and%20calculable%20format%2C%0Alayout%20verification%2C%20and%20iterative%20refinement%20to%20meet%20the%20quantitative%0Arequirements%20of%20industrial%20scenarios.%20Experiment%20results%20demonstrate%20that%20LLMs%0Apowered%20by%20SceneGenAgent%20exceed%20their%20original%20performance%2C%20reaching%20up%20to%0A81.0%25%20success%20rate%20in%20real-world%20industrial%20scene%20generation%20tasks%20and%0Aeffectively%20meeting%20most%20scene%20generation%20requirements.%20To%20further%20enhance%0Aaccessibility%2C%20we%20construct%20SceneInstruct%2C%20a%20dataset%20designed%20for%20fine-tuning%0Aopen-source%20LLMs%20to%20integrate%20into%20SceneGenAgent.%20Experiments%20show%20that%0Afine-tuning%20open-source%20LLMs%20on%20SceneInstruct%20yields%20significant%20performance%0Aimprovements%2C%20with%20Llama3.1-70B%20approaching%20the%20capabilities%20of%20GPT-4o.%20Our%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/THUDM/SceneGenAgent%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21909v1&entry.124074799=Read"},
{"title": "Self-supervised pre-training with diffusion model for few-shot landmark\n  detection in x-ray images", "author": "Roberto Di Via and Francesca Odone and Vito Paolo Pastore", "abstract": "  Deep neural networks have been extensively applied in the medical domain for\nvarious tasks, including image classification, segmentation, and landmark\ndetection. However, their application is often hindered by data scarcity, both\nin terms of available annotations and images. This study introduces a novel\napplication of denoising diffusion probabilistic models (DDPMs) to the landmark\ndetection task, specifically addressing the challenge of limited annotated data\nin x-ray imaging. Our key innovation lies in leveraging DDPMs for\nself-supervised pre-training in landmark detection, a previously unexplored\napproach in this domain. This method enables accurate landmark detection with\nminimal annotated training data (as few as 50 images), surpassing both ImageNet\nsupervised pre-training and traditional self-supervised techniques across three\npopular x-ray benchmark datasets. To our knowledge, this work represents the\nfirst application of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot\nregimes, for mitigating data scarcity.\n", "link": "http://arxiv.org/abs/2407.18125v2", "date": "2024-10-29", "relevancy": 2.2769, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.569}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20pre-training%20with%20diffusion%20model%20for%20few-shot%20landmark%0A%20%20detection%20in%20x-ray%20images&body=Title%3A%20Self-supervised%20pre-training%20with%20diffusion%20model%20for%20few-shot%20landmark%0A%20%20detection%20in%20x-ray%20images%0AAuthor%3A%20Roberto%20Di%20Via%20and%20Francesca%20Odone%20and%20Vito%20Paolo%20Pastore%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20been%20extensively%20applied%20in%20the%20medical%20domain%20for%0Avarious%20tasks%2C%20including%20image%20classification%2C%20segmentation%2C%20and%20landmark%0Adetection.%20However%2C%20their%20application%20is%20often%20hindered%20by%20data%20scarcity%2C%20both%0Ain%20terms%20of%20available%20annotations%20and%20images.%20This%20study%20introduces%20a%20novel%0Aapplication%20of%20denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20to%20the%20landmark%0Adetection%20task%2C%20specifically%20addressing%20the%20challenge%20of%20limited%20annotated%20data%0Ain%20x-ray%20imaging.%20Our%20key%20innovation%20lies%20in%20leveraging%20DDPMs%20for%0Aself-supervised%20pre-training%20in%20landmark%20detection%2C%20a%20previously%20unexplored%0Aapproach%20in%20this%20domain.%20This%20method%20enables%20accurate%20landmark%20detection%20with%0Aminimal%20annotated%20training%20data%20%28as%20few%20as%2050%20images%29%2C%20surpassing%20both%20ImageNet%0Asupervised%20pre-training%20and%20traditional%20self-supervised%20techniques%20across%20three%0Apopular%20x-ray%20benchmark%20datasets.%20To%20our%20knowledge%2C%20this%20work%20represents%20the%0Afirst%20application%20of%20diffusion%20models%20for%20self-supervised%20learning%20in%20landmark%0Adetection%2C%20which%20may%20offer%20a%20valuable%20pre-training%20approach%20in%20few-shot%0Aregimes%2C%20for%20mitigating%20data%20scarcity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520pre-training%2520with%2520diffusion%2520model%2520for%2520few-shot%2520landmark%250A%2520%2520detection%2520in%2520x-ray%2520images%26entry.906535625%3DRoberto%2520Di%2520Via%2520and%2520Francesca%2520Odone%2520and%2520Vito%2520Paolo%2520Pastore%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520been%2520extensively%2520applied%2520in%2520the%2520medical%2520domain%2520for%250Avarious%2520tasks%252C%2520including%2520image%2520classification%252C%2520segmentation%252C%2520and%2520landmark%250Adetection.%2520However%252C%2520their%2520application%2520is%2520often%2520hindered%2520by%2520data%2520scarcity%252C%2520both%250Ain%2520terms%2520of%2520available%2520annotations%2520and%2520images.%2520This%2520study%2520introduces%2520a%2520novel%250Aapplication%2520of%2520denoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPMs%2529%2520to%2520the%2520landmark%250Adetection%2520task%252C%2520specifically%2520addressing%2520the%2520challenge%2520of%2520limited%2520annotated%2520data%250Ain%2520x-ray%2520imaging.%2520Our%2520key%2520innovation%2520lies%2520in%2520leveraging%2520DDPMs%2520for%250Aself-supervised%2520pre-training%2520in%2520landmark%2520detection%252C%2520a%2520previously%2520unexplored%250Aapproach%2520in%2520this%2520domain.%2520This%2520method%2520enables%2520accurate%2520landmark%2520detection%2520with%250Aminimal%2520annotated%2520training%2520data%2520%2528as%2520few%2520as%252050%2520images%2529%252C%2520surpassing%2520both%2520ImageNet%250Asupervised%2520pre-training%2520and%2520traditional%2520self-supervised%2520techniques%2520across%2520three%250Apopular%2520x-ray%2520benchmark%2520datasets.%2520To%2520our%2520knowledge%252C%2520this%2520work%2520represents%2520the%250Afirst%2520application%2520of%2520diffusion%2520models%2520for%2520self-supervised%2520learning%2520in%2520landmark%250Adetection%252C%2520which%2520may%2520offer%2520a%2520valuable%2520pre-training%2520approach%2520in%2520few-shot%250Aregimes%252C%2520for%2520mitigating%2520data%2520scarcity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20pre-training%20with%20diffusion%20model%20for%20few-shot%20landmark%0A%20%20detection%20in%20x-ray%20images&entry.906535625=Roberto%20Di%20Via%20and%20Francesca%20Odone%20and%20Vito%20Paolo%20Pastore&entry.1292438233=%20%20Deep%20neural%20networks%20have%20been%20extensively%20applied%20in%20the%20medical%20domain%20for%0Avarious%20tasks%2C%20including%20image%20classification%2C%20segmentation%2C%20and%20landmark%0Adetection.%20However%2C%20their%20application%20is%20often%20hindered%20by%20data%20scarcity%2C%20both%0Ain%20terms%20of%20available%20annotations%20and%20images.%20This%20study%20introduces%20a%20novel%0Aapplication%20of%20denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20to%20the%20landmark%0Adetection%20task%2C%20specifically%20addressing%20the%20challenge%20of%20limited%20annotated%20data%0Ain%20x-ray%20imaging.%20Our%20key%20innovation%20lies%20in%20leveraging%20DDPMs%20for%0Aself-supervised%20pre-training%20in%20landmark%20detection%2C%20a%20previously%20unexplored%0Aapproach%20in%20this%20domain.%20This%20method%20enables%20accurate%20landmark%20detection%20with%0Aminimal%20annotated%20training%20data%20%28as%20few%20as%2050%20images%29%2C%20surpassing%20both%20ImageNet%0Asupervised%20pre-training%20and%20traditional%20self-supervised%20techniques%20across%20three%0Apopular%20x-ray%20benchmark%20datasets.%20To%20our%20knowledge%2C%20this%20work%20represents%20the%0Afirst%20application%20of%20diffusion%20models%20for%20self-supervised%20learning%20in%20landmark%0Adetection%2C%20which%20may%20offer%20a%20valuable%20pre-training%20approach%20in%20few-shot%0Aregimes%2C%20for%20mitigating%20data%20scarcity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18125v2&entry.124074799=Read"},
{"title": "HRPVT: High-Resolution Pyramid Vision Transformer for medium and\n  small-scale human pose estimation", "author": "Zhoujie Xu", "abstract": "  Human pose estimation on medium and small scales has long been a significant\nchallenge in this field. Most existing methods focus on restoring\nhigh-resolution feature maps by stacking multiple costly deconvolutional layers\nor by continuously aggregating semantic information from low-resolution feature\nmaps while maintaining high-resolution ones, which can lead to information\nredundancy. Additionally, due to quantization errors, heatmap-based methods\nhave certain disadvantages in accurately locating keypoints of medium and\nsmall-scale human figures. In this paper, we propose HRPVT, which utilizes PVT\nv2 as the backbone to model long-range dependencies. Building on this, we\nintroduce the High-Resolution Pyramid Module (HRPM), designed to generate\nhigher quality high-resolution representations by incorporating the intrinsic\ninductive biases of Convolutional Neural Networks (CNNs) into the\nhigh-resolution feature maps. The integration of HRPM enhances the performance\nof pure transformer-based models for human pose estimation at medium and small\nscales. Furthermore, we replace the heatmap-based method with SimCC approach,\nwhich eliminates the need for costly upsampling layers, thereby allowing us to\nallocate more computational resources to HRPM. To accommodate models with\nvarying parameter scales, we have developed two insertion strategies of HRPM,\neach designed to enhancing the model's ability to perceive medium and\nsmall-scale human poses from two distinct perspectives.\n", "link": "http://arxiv.org/abs/2410.22079v1", "date": "2024-10-29", "relevancy": 2.2563, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6003}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5403}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HRPVT%3A%20High-Resolution%20Pyramid%20Vision%20Transformer%20for%20medium%20and%0A%20%20small-scale%20human%20pose%20estimation&body=Title%3A%20HRPVT%3A%20High-Resolution%20Pyramid%20Vision%20Transformer%20for%20medium%20and%0A%20%20small-scale%20human%20pose%20estimation%0AAuthor%3A%20Zhoujie%20Xu%0AAbstract%3A%20%20%20Human%20pose%20estimation%20on%20medium%20and%20small%20scales%20has%20long%20been%20a%20significant%0Achallenge%20in%20this%20field.%20Most%20existing%20methods%20focus%20on%20restoring%0Ahigh-resolution%20feature%20maps%20by%20stacking%20multiple%20costly%20deconvolutional%20layers%0Aor%20by%20continuously%20aggregating%20semantic%20information%20from%20low-resolution%20feature%0Amaps%20while%20maintaining%20high-resolution%20ones%2C%20which%20can%20lead%20to%20information%0Aredundancy.%20Additionally%2C%20due%20to%20quantization%20errors%2C%20heatmap-based%20methods%0Ahave%20certain%20disadvantages%20in%20accurately%20locating%20keypoints%20of%20medium%20and%0Asmall-scale%20human%20figures.%20In%20this%20paper%2C%20we%20propose%20HRPVT%2C%20which%20utilizes%20PVT%0Av2%20as%20the%20backbone%20to%20model%20long-range%20dependencies.%20Building%20on%20this%2C%20we%0Aintroduce%20the%20High-Resolution%20Pyramid%20Module%20%28HRPM%29%2C%20designed%20to%20generate%0Ahigher%20quality%20high-resolution%20representations%20by%20incorporating%20the%20intrinsic%0Ainductive%20biases%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%20into%20the%0Ahigh-resolution%20feature%20maps.%20The%20integration%20of%20HRPM%20enhances%20the%20performance%0Aof%20pure%20transformer-based%20models%20for%20human%20pose%20estimation%20at%20medium%20and%20small%0Ascales.%20Furthermore%2C%20we%20replace%20the%20heatmap-based%20method%20with%20SimCC%20approach%2C%0Awhich%20eliminates%20the%20need%20for%20costly%20upsampling%20layers%2C%20thereby%20allowing%20us%20to%0Aallocate%20more%20computational%20resources%20to%20HRPM.%20To%20accommodate%20models%20with%0Avarying%20parameter%20scales%2C%20we%20have%20developed%20two%20insertion%20strategies%20of%20HRPM%2C%0Aeach%20designed%20to%20enhancing%20the%20model%27s%20ability%20to%20perceive%20medium%20and%0Asmall-scale%20human%20poses%20from%20two%20distinct%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHRPVT%253A%2520High-Resolution%2520Pyramid%2520Vision%2520Transformer%2520for%2520medium%2520and%250A%2520%2520small-scale%2520human%2520pose%2520estimation%26entry.906535625%3DZhoujie%2520Xu%26entry.1292438233%3D%2520%2520Human%2520pose%2520estimation%2520on%2520medium%2520and%2520small%2520scales%2520has%2520long%2520been%2520a%2520significant%250Achallenge%2520in%2520this%2520field.%2520Most%2520existing%2520methods%2520focus%2520on%2520restoring%250Ahigh-resolution%2520feature%2520maps%2520by%2520stacking%2520multiple%2520costly%2520deconvolutional%2520layers%250Aor%2520by%2520continuously%2520aggregating%2520semantic%2520information%2520from%2520low-resolution%2520feature%250Amaps%2520while%2520maintaining%2520high-resolution%2520ones%252C%2520which%2520can%2520lead%2520to%2520information%250Aredundancy.%2520Additionally%252C%2520due%2520to%2520quantization%2520errors%252C%2520heatmap-based%2520methods%250Ahave%2520certain%2520disadvantages%2520in%2520accurately%2520locating%2520keypoints%2520of%2520medium%2520and%250Asmall-scale%2520human%2520figures.%2520In%2520this%2520paper%252C%2520we%2520propose%2520HRPVT%252C%2520which%2520utilizes%2520PVT%250Av2%2520as%2520the%2520backbone%2520to%2520model%2520long-range%2520dependencies.%2520Building%2520on%2520this%252C%2520we%250Aintroduce%2520the%2520High-Resolution%2520Pyramid%2520Module%2520%2528HRPM%2529%252C%2520designed%2520to%2520generate%250Ahigher%2520quality%2520high-resolution%2520representations%2520by%2520incorporating%2520the%2520intrinsic%250Ainductive%2520biases%2520of%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520into%2520the%250Ahigh-resolution%2520feature%2520maps.%2520The%2520integration%2520of%2520HRPM%2520enhances%2520the%2520performance%250Aof%2520pure%2520transformer-based%2520models%2520for%2520human%2520pose%2520estimation%2520at%2520medium%2520and%2520small%250Ascales.%2520Furthermore%252C%2520we%2520replace%2520the%2520heatmap-based%2520method%2520with%2520SimCC%2520approach%252C%250Awhich%2520eliminates%2520the%2520need%2520for%2520costly%2520upsampling%2520layers%252C%2520thereby%2520allowing%2520us%2520to%250Aallocate%2520more%2520computational%2520resources%2520to%2520HRPM.%2520To%2520accommodate%2520models%2520with%250Avarying%2520parameter%2520scales%252C%2520we%2520have%2520developed%2520two%2520insertion%2520strategies%2520of%2520HRPM%252C%250Aeach%2520designed%2520to%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520perceive%2520medium%2520and%250Asmall-scale%2520human%2520poses%2520from%2520two%2520distinct%2520perspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRPVT%3A%20High-Resolution%20Pyramid%20Vision%20Transformer%20for%20medium%20and%0A%20%20small-scale%20human%20pose%20estimation&entry.906535625=Zhoujie%20Xu&entry.1292438233=%20%20Human%20pose%20estimation%20on%20medium%20and%20small%20scales%20has%20long%20been%20a%20significant%0Achallenge%20in%20this%20field.%20Most%20existing%20methods%20focus%20on%20restoring%0Ahigh-resolution%20feature%20maps%20by%20stacking%20multiple%20costly%20deconvolutional%20layers%0Aor%20by%20continuously%20aggregating%20semantic%20information%20from%20low-resolution%20feature%0Amaps%20while%20maintaining%20high-resolution%20ones%2C%20which%20can%20lead%20to%20information%0Aredundancy.%20Additionally%2C%20due%20to%20quantization%20errors%2C%20heatmap-based%20methods%0Ahave%20certain%20disadvantages%20in%20accurately%20locating%20keypoints%20of%20medium%20and%0Asmall-scale%20human%20figures.%20In%20this%20paper%2C%20we%20propose%20HRPVT%2C%20which%20utilizes%20PVT%0Av2%20as%20the%20backbone%20to%20model%20long-range%20dependencies.%20Building%20on%20this%2C%20we%0Aintroduce%20the%20High-Resolution%20Pyramid%20Module%20%28HRPM%29%2C%20designed%20to%20generate%0Ahigher%20quality%20high-resolution%20representations%20by%20incorporating%20the%20intrinsic%0Ainductive%20biases%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%20into%20the%0Ahigh-resolution%20feature%20maps.%20The%20integration%20of%20HRPM%20enhances%20the%20performance%0Aof%20pure%20transformer-based%20models%20for%20human%20pose%20estimation%20at%20medium%20and%20small%0Ascales.%20Furthermore%2C%20we%20replace%20the%20heatmap-based%20method%20with%20SimCC%20approach%2C%0Awhich%20eliminates%20the%20need%20for%20costly%20upsampling%20layers%2C%20thereby%20allowing%20us%20to%0Aallocate%20more%20computational%20resources%20to%20HRPM.%20To%20accommodate%20models%20with%0Avarying%20parameter%20scales%2C%20we%20have%20developed%20two%20insertion%20strategies%20of%20HRPM%2C%0Aeach%20designed%20to%20enhancing%20the%20model%27s%20ability%20to%20perceive%20medium%20and%0Asmall-scale%20human%20poses%20from%20two%20distinct%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22079v1&entry.124074799=Read"},
{"title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and\n  Instruction-Tuning Dataset for LVLMs", "author": "Ziyu Liu and Tao Chu and Yuhang Zang and Xilin Wei and Xiaoyi Dong and Pan Zhang and Zijian Liang and Yuanjun Xiong and Yu Qiao and Dahua Lin and Jiaqi Wang", "abstract": "  Generating natural and meaningful responses to communicate with multi-modal\nhuman inputs is a fundamental capability of Large Vision-Language\nModels(LVLMs). While current open-source LVLMs demonstrate promising\nperformance in simplified scenarios such as single-turn single-image input,\nthey fall short in real-world conversation scenarios such as following\ninstructions in a long context history with multi-turn and multi-images.\nExisting LVLM benchmarks primarily focus on single-choice questions or\nshort-form responses, which do not adequately assess the capabilities of LVLMs\nin real-world human-AI interaction applications. Therefore, we introduce MMDU,\na comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning\ndataset, designed to evaluate and improve LVLMs' abilities in multi-turn and\nmulti-image conversations. We employ the clustering algorithm to ffnd the\nrelevant images and textual descriptions from the open-source Wikipedia and\nconstruct the question-answer pairs by human annotators with the assistance of\nthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and\n27 turns, which is at least 5x longer than previous benchmarks and poses\nchallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs\nusing MMDU reveals that open-source LVLMs lag behind closed-source counterparts\ndue to limited conversational instruction tuning data. We demonstrate that\nffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,\ngenerating longer and more accurate conversations, and improving scores on MMDU\nand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Our\ncontributions pave the way for bridging the gap between current LVLM models and\nreal-world application demands. This project is available at\nhttps://github.com/Liuziyu77/MMDU.\n", "link": "http://arxiv.org/abs/2406.11833v2", "date": "2024-10-29", "relevancy": 2.2514, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5761}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMDU%3A%20A%20Multi-Turn%20Multi-Image%20Dialog%20Understanding%20Benchmark%20and%0A%20%20Instruction-Tuning%20Dataset%20for%20LVLMs&body=Title%3A%20MMDU%3A%20A%20Multi-Turn%20Multi-Image%20Dialog%20Understanding%20Benchmark%20and%0A%20%20Instruction-Tuning%20Dataset%20for%20LVLMs%0AAuthor%3A%20Ziyu%20Liu%20and%20Tao%20Chu%20and%20Yuhang%20Zang%20and%20Xilin%20Wei%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Zijian%20Liang%20and%20Yuanjun%20Xiong%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Generating%20natural%20and%20meaningful%20responses%20to%20communicate%20with%20multi-modal%0Ahuman%20inputs%20is%20a%20fundamental%20capability%20of%20Large%20Vision-Language%0AModels%28LVLMs%29.%20While%20current%20open-source%20LVLMs%20demonstrate%20promising%0Aperformance%20in%20simplified%20scenarios%20such%20as%20single-turn%20single-image%20input%2C%0Athey%20fall%20short%20in%20real-world%20conversation%20scenarios%20such%20as%20following%0Ainstructions%20in%20a%20long%20context%20history%20with%20multi-turn%20and%20multi-images.%0AExisting%20LVLM%20benchmarks%20primarily%20focus%20on%20single-choice%20questions%20or%0Ashort-form%20responses%2C%20which%20do%20not%20adequately%20assess%20the%20capabilities%20of%20LVLMs%0Ain%20real-world%20human-AI%20interaction%20applications.%20Therefore%2C%20we%20introduce%20MMDU%2C%0Aa%20comprehensive%20benchmark%2C%20and%20MMDU-45k%2C%20a%20large-scale%20instruction%20tuning%0Adataset%2C%20designed%20to%20evaluate%20and%20improve%20LVLMs%27%20abilities%20in%20multi-turn%20and%0Amulti-image%20conversations.%20We%20employ%20the%20clustering%20algorithm%20to%20ffnd%20the%0Arelevant%20images%20and%20textual%20descriptions%20from%20the%20open-source%20Wikipedia%20and%0Aconstruct%20the%20question-answer%20pairs%20by%20human%20annotators%20with%20the%20assistance%20of%0Athe%20GPT-4o%20model.%20MMDU%20has%20a%20maximum%20of%2018k%20image%2Btext%20tokens%2C%2020%20images%2C%20and%0A27%20turns%2C%20which%20is%20at%20least%205x%20longer%20than%20previous%20benchmarks%20and%20poses%0Achallenges%20to%20current%20LVLMs.%20Our%20in-depth%20analysis%20of%2015%20representative%20LVLMs%0Ausing%20MMDU%20reveals%20that%20open-source%20LVLMs%20lag%20behind%20closed-source%20counterparts%0Adue%20to%20limited%20conversational%20instruction%20tuning%20data.%20We%20demonstrate%20that%0Affne-tuning%20open-source%20LVLMs%20on%20MMDU-45k%20signiffcantly%20address%20this%20gap%2C%0Agenerating%20longer%20and%20more%20accurate%20conversations%2C%20and%20improving%20scores%20on%20MMDU%0Aand%20existing%20benchmarks%20%28MMStar%3A%20%2B1.1%25%2C%20MathVista%3A%20%2B1.5%25%2C%20ChartQA%3A%2B1.2%25%29.%20Our%0Acontributions%20pave%20the%20way%20for%20bridging%20the%20gap%20between%20current%20LVLM%20models%20and%0Areal-world%20application%20demands.%20This%20project%20is%20available%20at%0Ahttps%3A//github.com/Liuziyu77/MMDU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMDU%253A%2520A%2520Multi-Turn%2520Multi-Image%2520Dialog%2520Understanding%2520Benchmark%2520and%250A%2520%2520Instruction-Tuning%2520Dataset%2520for%2520LVLMs%26entry.906535625%3DZiyu%2520Liu%2520and%2520Tao%2520Chu%2520and%2520Yuhang%2520Zang%2520and%2520Xilin%2520Wei%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Zijian%2520Liang%2520and%2520Yuanjun%2520Xiong%2520and%2520Yu%2520Qiao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520natural%2520and%2520meaningful%2520responses%2520to%2520communicate%2520with%2520multi-modal%250Ahuman%2520inputs%2520is%2520a%2520fundamental%2520capability%2520of%2520Large%2520Vision-Language%250AModels%2528LVLMs%2529.%2520While%2520current%2520open-source%2520LVLMs%2520demonstrate%2520promising%250Aperformance%2520in%2520simplified%2520scenarios%2520such%2520as%2520single-turn%2520single-image%2520input%252C%250Athey%2520fall%2520short%2520in%2520real-world%2520conversation%2520scenarios%2520such%2520as%2520following%250Ainstructions%2520in%2520a%2520long%2520context%2520history%2520with%2520multi-turn%2520and%2520multi-images.%250AExisting%2520LVLM%2520benchmarks%2520primarily%2520focus%2520on%2520single-choice%2520questions%2520or%250Ashort-form%2520responses%252C%2520which%2520do%2520not%2520adequately%2520assess%2520the%2520capabilities%2520of%2520LVLMs%250Ain%2520real-world%2520human-AI%2520interaction%2520applications.%2520Therefore%252C%2520we%2520introduce%2520MMDU%252C%250Aa%2520comprehensive%2520benchmark%252C%2520and%2520MMDU-45k%252C%2520a%2520large-scale%2520instruction%2520tuning%250Adataset%252C%2520designed%2520to%2520evaluate%2520and%2520improve%2520LVLMs%2527%2520abilities%2520in%2520multi-turn%2520and%250Amulti-image%2520conversations.%2520We%2520employ%2520the%2520clustering%2520algorithm%2520to%2520ffnd%2520the%250Arelevant%2520images%2520and%2520textual%2520descriptions%2520from%2520the%2520open-source%2520Wikipedia%2520and%250Aconstruct%2520the%2520question-answer%2520pairs%2520by%2520human%2520annotators%2520with%2520the%2520assistance%2520of%250Athe%2520GPT-4o%2520model.%2520MMDU%2520has%2520a%2520maximum%2520of%252018k%2520image%252Btext%2520tokens%252C%252020%2520images%252C%2520and%250A27%2520turns%252C%2520which%2520is%2520at%2520least%25205x%2520longer%2520than%2520previous%2520benchmarks%2520and%2520poses%250Achallenges%2520to%2520current%2520LVLMs.%2520Our%2520in-depth%2520analysis%2520of%252015%2520representative%2520LVLMs%250Ausing%2520MMDU%2520reveals%2520that%2520open-source%2520LVLMs%2520lag%2520behind%2520closed-source%2520counterparts%250Adue%2520to%2520limited%2520conversational%2520instruction%2520tuning%2520data.%2520We%2520demonstrate%2520that%250Affne-tuning%2520open-source%2520LVLMs%2520on%2520MMDU-45k%2520signiffcantly%2520address%2520this%2520gap%252C%250Agenerating%2520longer%2520and%2520more%2520accurate%2520conversations%252C%2520and%2520improving%2520scores%2520on%2520MMDU%250Aand%2520existing%2520benchmarks%2520%2528MMStar%253A%2520%252B1.1%2525%252C%2520MathVista%253A%2520%252B1.5%2525%252C%2520ChartQA%253A%252B1.2%2525%2529.%2520Our%250Acontributions%2520pave%2520the%2520way%2520for%2520bridging%2520the%2520gap%2520between%2520current%2520LVLM%2520models%2520and%250Areal-world%2520application%2520demands.%2520This%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/Liuziyu77/MMDU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMDU%3A%20A%20Multi-Turn%20Multi-Image%20Dialog%20Understanding%20Benchmark%20and%0A%20%20Instruction-Tuning%20Dataset%20for%20LVLMs&entry.906535625=Ziyu%20Liu%20and%20Tao%20Chu%20and%20Yuhang%20Zang%20and%20Xilin%20Wei%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Zijian%20Liang%20and%20Yuanjun%20Xiong%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Generating%20natural%20and%20meaningful%20responses%20to%20communicate%20with%20multi-modal%0Ahuman%20inputs%20is%20a%20fundamental%20capability%20of%20Large%20Vision-Language%0AModels%28LVLMs%29.%20While%20current%20open-source%20LVLMs%20demonstrate%20promising%0Aperformance%20in%20simplified%20scenarios%20such%20as%20single-turn%20single-image%20input%2C%0Athey%20fall%20short%20in%20real-world%20conversation%20scenarios%20such%20as%20following%0Ainstructions%20in%20a%20long%20context%20history%20with%20multi-turn%20and%20multi-images.%0AExisting%20LVLM%20benchmarks%20primarily%20focus%20on%20single-choice%20questions%20or%0Ashort-form%20responses%2C%20which%20do%20not%20adequately%20assess%20the%20capabilities%20of%20LVLMs%0Ain%20real-world%20human-AI%20interaction%20applications.%20Therefore%2C%20we%20introduce%20MMDU%2C%0Aa%20comprehensive%20benchmark%2C%20and%20MMDU-45k%2C%20a%20large-scale%20instruction%20tuning%0Adataset%2C%20designed%20to%20evaluate%20and%20improve%20LVLMs%27%20abilities%20in%20multi-turn%20and%0Amulti-image%20conversations.%20We%20employ%20the%20clustering%20algorithm%20to%20ffnd%20the%0Arelevant%20images%20and%20textual%20descriptions%20from%20the%20open-source%20Wikipedia%20and%0Aconstruct%20the%20question-answer%20pairs%20by%20human%20annotators%20with%20the%20assistance%20of%0Athe%20GPT-4o%20model.%20MMDU%20has%20a%20maximum%20of%2018k%20image%2Btext%20tokens%2C%2020%20images%2C%20and%0A27%20turns%2C%20which%20is%20at%20least%205x%20longer%20than%20previous%20benchmarks%20and%20poses%0Achallenges%20to%20current%20LVLMs.%20Our%20in-depth%20analysis%20of%2015%20representative%20LVLMs%0Ausing%20MMDU%20reveals%20that%20open-source%20LVLMs%20lag%20behind%20closed-source%20counterparts%0Adue%20to%20limited%20conversational%20instruction%20tuning%20data.%20We%20demonstrate%20that%0Affne-tuning%20open-source%20LVLMs%20on%20MMDU-45k%20signiffcantly%20address%20this%20gap%2C%0Agenerating%20longer%20and%20more%20accurate%20conversations%2C%20and%20improving%20scores%20on%20MMDU%0Aand%20existing%20benchmarks%20%28MMStar%3A%20%2B1.1%25%2C%20MathVista%3A%20%2B1.5%25%2C%20ChartQA%3A%2B1.2%25%29.%20Our%0Acontributions%20pave%20the%20way%20for%20bridging%20the%20gap%20between%20current%20LVLM%20models%20and%0Areal-world%20application%20demands.%20This%20project%20is%20available%20at%0Ahttps%3A//github.com/Liuziyu77/MMDU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11833v2&entry.124074799=Read"},
{"title": "Active Learning for Vision-Language Models", "author": "Bardia Safaei and Vishal M. Patel", "abstract": "  Pre-trained vision-language models (VLMs) like CLIP have demonstrated\nimpressive zero-shot performance on a wide range of downstream computer vision\ntasks. However, there still exists a considerable performance gap between these\nmodels and a supervised deep model trained on a downstream dataset. To bridge\nthis gap, we propose a novel active learning (AL) framework that enhances the\nzero-shot classification performance of VLMs by selecting only a few\ninformative samples from the unlabeled data for annotation during training. To\nachieve this, our approach first calibrates the predicted entropy of VLMs and\nthen utilizes a combination of self-uncertainty and neighbor-aware uncertainty\nto calculate a reliable uncertainty measure for active sample selection. Our\nextensive experiments show that the proposed approach outperforms existing AL\napproaches on several image classification datasets, and significantly enhances\nthe zero-shot performance of VLMs.\n", "link": "http://arxiv.org/abs/2410.22187v1", "date": "2024-10-29", "relevancy": 2.2495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5864}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20Active%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20have%20demonstrated%0Aimpressive%20zero-shot%20performance%20on%20a%20wide%20range%20of%20downstream%20computer%20vision%0Atasks.%20However%2C%20there%20still%20exists%20a%20considerable%20performance%20gap%20between%20these%0Amodels%20and%20a%20supervised%20deep%20model%20trained%20on%20a%20downstream%20dataset.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20a%20novel%20active%20learning%20%28AL%29%20framework%20that%20enhances%20the%0Azero-shot%20classification%20performance%20of%20VLMs%20by%20selecting%20only%20a%20few%0Ainformative%20samples%20from%20the%20unlabeled%20data%20for%20annotation%20during%20training.%20To%0Aachieve%20this%2C%20our%20approach%20first%20calibrates%20the%20predicted%20entropy%20of%20VLMs%20and%0Athen%20utilizes%20a%20combination%20of%20self-uncertainty%20and%20neighbor-aware%20uncertainty%0Ato%20calculate%20a%20reliable%20uncertainty%20measure%20for%20active%20sample%20selection.%20Our%0Aextensive%20experiments%20show%20that%20the%20proposed%20approach%20outperforms%20existing%20AL%0Aapproaches%20on%20several%20image%20classification%20datasets%2C%20and%20significantly%20enhances%0Athe%20zero-shot%20performance%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DBardia%2520Safaei%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520have%2520demonstrated%250Aimpressive%2520zero-shot%2520performance%2520on%2520a%2520wide%2520range%2520of%2520downstream%2520computer%2520vision%250Atasks.%2520However%252C%2520there%2520still%2520exists%2520a%2520considerable%2520performance%2520gap%2520between%2520these%250Amodels%2520and%2520a%2520supervised%2520deep%2520model%2520trained%2520on%2520a%2520downstream%2520dataset.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520propose%2520a%2520novel%2520active%2520learning%2520%2528AL%2529%2520framework%2520that%2520enhances%2520the%250Azero-shot%2520classification%2520performance%2520of%2520VLMs%2520by%2520selecting%2520only%2520a%2520few%250Ainformative%2520samples%2520from%2520the%2520unlabeled%2520data%2520for%2520annotation%2520during%2520training.%2520To%250Aachieve%2520this%252C%2520our%2520approach%2520first%2520calibrates%2520the%2520predicted%2520entropy%2520of%2520VLMs%2520and%250Athen%2520utilizes%2520a%2520combination%2520of%2520self-uncertainty%2520and%2520neighbor-aware%2520uncertainty%250Ato%2520calculate%2520a%2520reliable%2520uncertainty%2520measure%2520for%2520active%2520sample%2520selection.%2520Our%250Aextensive%2520experiments%2520show%2520that%2520the%2520proposed%2520approach%2520outperforms%2520existing%2520AL%250Aapproaches%2520on%2520several%2520image%2520classification%2520datasets%252C%2520and%2520significantly%2520enhances%250Athe%2520zero-shot%2520performance%2520of%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20for%20Vision-Language%20Models&entry.906535625=Bardia%20Safaei%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20have%20demonstrated%0Aimpressive%20zero-shot%20performance%20on%20a%20wide%20range%20of%20downstream%20computer%20vision%0Atasks.%20However%2C%20there%20still%20exists%20a%20considerable%20performance%20gap%20between%20these%0Amodels%20and%20a%20supervised%20deep%20model%20trained%20on%20a%20downstream%20dataset.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20a%20novel%20active%20learning%20%28AL%29%20framework%20that%20enhances%20the%0Azero-shot%20classification%20performance%20of%20VLMs%20by%20selecting%20only%20a%20few%0Ainformative%20samples%20from%20the%20unlabeled%20data%20for%20annotation%20during%20training.%20To%0Aachieve%20this%2C%20our%20approach%20first%20calibrates%20the%20predicted%20entropy%20of%20VLMs%20and%0Athen%20utilizes%20a%20combination%20of%20self-uncertainty%20and%20neighbor-aware%20uncertainty%0Ato%20calculate%20a%20reliable%20uncertainty%20measure%20for%20active%20sample%20selection.%20Our%0Aextensive%20experiments%20show%20that%20the%20proposed%20approach%20outperforms%20existing%20AL%0Aapproaches%20on%20several%20image%20classification%20datasets%2C%20and%20significantly%20enhances%0Athe%20zero-shot%20performance%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22187v1&entry.124074799=Read"},
{"title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs", "author": "Yun Zhu and Haizhou Shi and Xiaotang Wang and Yongchao Liu and Yaoke Wang and Boci Peng and Chuntao Hong and Siliang Tang", "abstract": "  Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP\n", "link": "http://arxiv.org/abs/2410.10329v3", "date": "2024-10-29", "relevancy": 2.2495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.598}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.549}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphCLIP%3A%20Enhancing%20Transferability%20in%20Graph%20Foundation%20Models%20for%0A%20%20Text-Attributed%20Graphs&body=Title%3A%20GraphCLIP%3A%20Enhancing%20Transferability%20in%20Graph%20Foundation%20Models%20for%0A%20%20Text-Attributed%20Graphs%0AAuthor%3A%20Yun%20Zhu%20and%20Haizhou%20Shi%20and%20Xiaotang%20Wang%20and%20Yongchao%20Liu%20and%20Yaoke%20Wang%20and%20Boci%20Peng%20and%20Chuntao%20Hong%20and%20Siliang%20Tang%0AAbstract%3A%20%20%20Recently%2C%20research%20on%20Text-Attributed%20Graphs%20%28TAGs%29%20has%20gained%20significant%0Aattention%20due%20to%20the%20prevalence%20of%20free-text%20node%20features%20in%20real-world%0Aapplications%20and%20the%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20that%20bolster%0ATAG%20methodologies.%20However%2C%20current%20TAG%20approaches%20face%20two%20primary%20challenges%3A%0A%28i%29%20Heavy%20reliance%20on%20label%20information%20and%20%28ii%29%20Limited%20cross-domain%0Azero/few-shot%20transferability.%20These%20issues%20constrain%20the%20scaling%20of%20both%20data%0Aand%20model%20size%2C%20owing%20to%20high%20labor%20costs%20and%20scaling%20laws%2C%20complicating%20the%0Adevelopment%20of%20graph%20foundation%20models%20with%20strong%20transferability.%20In%20this%0Awork%2C%20we%20propose%20the%20GraphCLIP%20framework%20to%20address%20these%20challenges%20by%0Alearning%20graph%20foundation%20models%20with%20strong%20cross-domain%20zero/few-shot%0Atransferability%20through%20a%20self-supervised%20contrastive%20graph-summary%20pretraining%0Amethod.%20Specifically%2C%20we%20generate%20and%20curate%20large-scale%20graph-summary%20pair%0Adata%20with%20the%20assistance%20of%20LLMs%2C%20and%20introduce%20a%20novel%20graph-summary%0Apretraining%20method%2C%20combined%20with%20invariant%20learning%2C%20to%20enhance%20graph%0Afoundation%20models%20with%20strong%20cross-domain%20zero-shot%20transferability.%20For%0Afew-shot%20learning%2C%20we%20propose%20a%20novel%20graph%20prompt%20tuning%20technique%20aligned%0Awith%20our%20pretraining%20objective%20to%20mitigate%20catastrophic%20forgetting%20and%20minimize%0Alearning%20costs.%20Extensive%20experiments%20show%20the%20superiority%20of%20GraphCLIP%20in%20both%0Azero-shot%20and%20few-shot%20settings%2C%20while%20evaluations%20across%20various%20downstream%0Atasks%20confirm%20the%20versatility%20of%20GraphCLIP.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZhuYun97/GraphCLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10329v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphCLIP%253A%2520Enhancing%2520Transferability%2520in%2520Graph%2520Foundation%2520Models%2520for%250A%2520%2520Text-Attributed%2520Graphs%26entry.906535625%3DYun%2520Zhu%2520and%2520Haizhou%2520Shi%2520and%2520Xiaotang%2520Wang%2520and%2520Yongchao%2520Liu%2520and%2520Yaoke%2520Wang%2520and%2520Boci%2520Peng%2520and%2520Chuntao%2520Hong%2520and%2520Siliang%2520Tang%26entry.1292438233%3D%2520%2520Recently%252C%2520research%2520on%2520Text-Attributed%2520Graphs%2520%2528TAGs%2529%2520has%2520gained%2520significant%250Aattention%2520due%2520to%2520the%2520prevalence%2520of%2520free-text%2520node%2520features%2520in%2520real-world%250Aapplications%2520and%2520the%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520bolster%250ATAG%2520methodologies.%2520However%252C%2520current%2520TAG%2520approaches%2520face%2520two%2520primary%2520challenges%253A%250A%2528i%2529%2520Heavy%2520reliance%2520on%2520label%2520information%2520and%2520%2528ii%2529%2520Limited%2520cross-domain%250Azero/few-shot%2520transferability.%2520These%2520issues%2520constrain%2520the%2520scaling%2520of%2520both%2520data%250Aand%2520model%2520size%252C%2520owing%2520to%2520high%2520labor%2520costs%2520and%2520scaling%2520laws%252C%2520complicating%2520the%250Adevelopment%2520of%2520graph%2520foundation%2520models%2520with%2520strong%2520transferability.%2520In%2520this%250Awork%252C%2520we%2520propose%2520the%2520GraphCLIP%2520framework%2520to%2520address%2520these%2520challenges%2520by%250Alearning%2520graph%2520foundation%2520models%2520with%2520strong%2520cross-domain%2520zero/few-shot%250Atransferability%2520through%2520a%2520self-supervised%2520contrastive%2520graph-summary%2520pretraining%250Amethod.%2520Specifically%252C%2520we%2520generate%2520and%2520curate%2520large-scale%2520graph-summary%2520pair%250Adata%2520with%2520the%2520assistance%2520of%2520LLMs%252C%2520and%2520introduce%2520a%2520novel%2520graph-summary%250Apretraining%2520method%252C%2520combined%2520with%2520invariant%2520learning%252C%2520to%2520enhance%2520graph%250Afoundation%2520models%2520with%2520strong%2520cross-domain%2520zero-shot%2520transferability.%2520For%250Afew-shot%2520learning%252C%2520we%2520propose%2520a%2520novel%2520graph%2520prompt%2520tuning%2520technique%2520aligned%250Awith%2520our%2520pretraining%2520objective%2520to%2520mitigate%2520catastrophic%2520forgetting%2520and%2520minimize%250Alearning%2520costs.%2520Extensive%2520experiments%2520show%2520the%2520superiority%2520of%2520GraphCLIP%2520in%2520both%250Azero-shot%2520and%2520few-shot%2520settings%252C%2520while%2520evaluations%2520across%2520various%2520downstream%250Atasks%2520confirm%2520the%2520versatility%2520of%2520GraphCLIP.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ZhuYun97/GraphCLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10329v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphCLIP%3A%20Enhancing%20Transferability%20in%20Graph%20Foundation%20Models%20for%0A%20%20Text-Attributed%20Graphs&entry.906535625=Yun%20Zhu%20and%20Haizhou%20Shi%20and%20Xiaotang%20Wang%20and%20Yongchao%20Liu%20and%20Yaoke%20Wang%20and%20Boci%20Peng%20and%20Chuntao%20Hong%20and%20Siliang%20Tang&entry.1292438233=%20%20Recently%2C%20research%20on%20Text-Attributed%20Graphs%20%28TAGs%29%20has%20gained%20significant%0Aattention%20due%20to%20the%20prevalence%20of%20free-text%20node%20features%20in%20real-world%0Aapplications%20and%20the%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20that%20bolster%0ATAG%20methodologies.%20However%2C%20current%20TAG%20approaches%20face%20two%20primary%20challenges%3A%0A%28i%29%20Heavy%20reliance%20on%20label%20information%20and%20%28ii%29%20Limited%20cross-domain%0Azero/few-shot%20transferability.%20These%20issues%20constrain%20the%20scaling%20of%20both%20data%0Aand%20model%20size%2C%20owing%20to%20high%20labor%20costs%20and%20scaling%20laws%2C%20complicating%20the%0Adevelopment%20of%20graph%20foundation%20models%20with%20strong%20transferability.%20In%20this%0Awork%2C%20we%20propose%20the%20GraphCLIP%20framework%20to%20address%20these%20challenges%20by%0Alearning%20graph%20foundation%20models%20with%20strong%20cross-domain%20zero/few-shot%0Atransferability%20through%20a%20self-supervised%20contrastive%20graph-summary%20pretraining%0Amethod.%20Specifically%2C%20we%20generate%20and%20curate%20large-scale%20graph-summary%20pair%0Adata%20with%20the%20assistance%20of%20LLMs%2C%20and%20introduce%20a%20novel%20graph-summary%0Apretraining%20method%2C%20combined%20with%20invariant%20learning%2C%20to%20enhance%20graph%0Afoundation%20models%20with%20strong%20cross-domain%20zero-shot%20transferability.%20For%0Afew-shot%20learning%2C%20we%20propose%20a%20novel%20graph%20prompt%20tuning%20technique%20aligned%0Awith%20our%20pretraining%20objective%20to%20mitigate%20catastrophic%20forgetting%20and%20minimize%0Alearning%20costs.%20Extensive%20experiments%20show%20the%20superiority%20of%20GraphCLIP%20in%20both%0Azero-shot%20and%20few-shot%20settings%2C%20while%20evaluations%20across%20various%20downstream%0Atasks%20confirm%20the%20versatility%20of%20GraphCLIP.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZhuYun97/GraphCLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10329v3&entry.124074799=Read"},
{"title": "BenchX: A Unified Benchmark Framework for Medical Vision-Language\n  Pretraining on Chest X-Rays", "author": "Yang Zhou and Tan Li Hui Faith and Yanyu Xu and Sicong Leng and Xinxing Xu and Yong Liu and Rick Siow Mong Goh", "abstract": "  Medical Vision-Language Pretraining (MedVLP) shows promise in learning\ngeneralizable and transferable visual representations from paired and unpaired\nmedical images and reports. MedVLP can provide useful features to downstream\ntasks and facilitate adapting task-specific models to new setups using fewer\nexamples. However, existing MedVLP methods often differ in terms of datasets,\npreprocessing, and finetuning implementations. This pose great challenges in\nevaluating how well a MedVLP method generalizes to various clinically-relevant\ntasks due to the lack of unified, standardized, and comprehensive benchmark. To\nfill this gap, we propose BenchX, a unified benchmark framework that enables\nhead-to-head comparison and systematical analysis between MedVLP methods using\npublic chest X-ray datasets. Specifically, BenchX is composed of three\ncomponents: 1) Comprehensive datasets covering nine datasets and four medical\ntasks; 2) Benchmark suites to standardize data preprocessing, train-test\nsplits, and parameter selection; 3) Unified finetuning protocols that\naccommodate heterogeneous MedVLP methods for consistent task adaptation in\nclassification, segmentation, and report generation, respectively. Utilizing\nBenchX, we establish baselines for nine state-of-the-art MedVLP methods and\nfound that the performance of some early MedVLP methods can be enhanced to\nsurpass more recent ones, prompting a revisiting of the developments and\nconclusions from prior works in MedVLP. Our code are available at\nhttps://github.com/yangzhou12/BenchX.\n", "link": "http://arxiv.org/abs/2410.21969v1", "date": "2024-10-29", "relevancy": 2.2484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BenchX%3A%20A%20Unified%20Benchmark%20Framework%20for%20Medical%20Vision-Language%0A%20%20Pretraining%20on%20Chest%20X-Rays&body=Title%3A%20BenchX%3A%20A%20Unified%20Benchmark%20Framework%20for%20Medical%20Vision-Language%0A%20%20Pretraining%20on%20Chest%20X-Rays%0AAuthor%3A%20Yang%20Zhou%20and%20Tan%20Li%20Hui%20Faith%20and%20Yanyu%20Xu%20and%20Sicong%20Leng%20and%20Xinxing%20Xu%20and%20Yong%20Liu%20and%20Rick%20Siow%20Mong%20Goh%0AAbstract%3A%20%20%20Medical%20Vision-Language%20Pretraining%20%28MedVLP%29%20shows%20promise%20in%20learning%0Ageneralizable%20and%20transferable%20visual%20representations%20from%20paired%20and%20unpaired%0Amedical%20images%20and%20reports.%20MedVLP%20can%20provide%20useful%20features%20to%20downstream%0Atasks%20and%20facilitate%20adapting%20task-specific%20models%20to%20new%20setups%20using%20fewer%0Aexamples.%20However%2C%20existing%20MedVLP%20methods%20often%20differ%20in%20terms%20of%20datasets%2C%0Apreprocessing%2C%20and%20finetuning%20implementations.%20This%20pose%20great%20challenges%20in%0Aevaluating%20how%20well%20a%20MedVLP%20method%20generalizes%20to%20various%20clinically-relevant%0Atasks%20due%20to%20the%20lack%20of%20unified%2C%20standardized%2C%20and%20comprehensive%20benchmark.%20To%0Afill%20this%20gap%2C%20we%20propose%20BenchX%2C%20a%20unified%20benchmark%20framework%20that%20enables%0Ahead-to-head%20comparison%20and%20systematical%20analysis%20between%20MedVLP%20methods%20using%0Apublic%20chest%20X-ray%20datasets.%20Specifically%2C%20BenchX%20is%20composed%20of%20three%0Acomponents%3A%201%29%20Comprehensive%20datasets%20covering%20nine%20datasets%20and%20four%20medical%0Atasks%3B%202%29%20Benchmark%20suites%20to%20standardize%20data%20preprocessing%2C%20train-test%0Asplits%2C%20and%20parameter%20selection%3B%203%29%20Unified%20finetuning%20protocols%20that%0Aaccommodate%20heterogeneous%20MedVLP%20methods%20for%20consistent%20task%20adaptation%20in%0Aclassification%2C%20segmentation%2C%20and%20report%20generation%2C%20respectively.%20Utilizing%0ABenchX%2C%20we%20establish%20baselines%20for%20nine%20state-of-the-art%20MedVLP%20methods%20and%0Afound%20that%20the%20performance%20of%20some%20early%20MedVLP%20methods%20can%20be%20enhanced%20to%0Asurpass%20more%20recent%20ones%2C%20prompting%20a%20revisiting%20of%20the%20developments%20and%0Aconclusions%20from%20prior%20works%20in%20MedVLP.%20Our%20code%20are%20available%20at%0Ahttps%3A//github.com/yangzhou12/BenchX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchX%253A%2520A%2520Unified%2520Benchmark%2520Framework%2520for%2520Medical%2520Vision-Language%250A%2520%2520Pretraining%2520on%2520Chest%2520X-Rays%26entry.906535625%3DYang%2520Zhou%2520and%2520Tan%2520Li%2520Hui%2520Faith%2520and%2520Yanyu%2520Xu%2520and%2520Sicong%2520Leng%2520and%2520Xinxing%2520Xu%2520and%2520Yong%2520Liu%2520and%2520Rick%2520Siow%2520Mong%2520Goh%26entry.1292438233%3D%2520%2520Medical%2520Vision-Language%2520Pretraining%2520%2528MedVLP%2529%2520shows%2520promise%2520in%2520learning%250Ageneralizable%2520and%2520transferable%2520visual%2520representations%2520from%2520paired%2520and%2520unpaired%250Amedical%2520images%2520and%2520reports.%2520MedVLP%2520can%2520provide%2520useful%2520features%2520to%2520downstream%250Atasks%2520and%2520facilitate%2520adapting%2520task-specific%2520models%2520to%2520new%2520setups%2520using%2520fewer%250Aexamples.%2520However%252C%2520existing%2520MedVLP%2520methods%2520often%2520differ%2520in%2520terms%2520of%2520datasets%252C%250Apreprocessing%252C%2520and%2520finetuning%2520implementations.%2520This%2520pose%2520great%2520challenges%2520in%250Aevaluating%2520how%2520well%2520a%2520MedVLP%2520method%2520generalizes%2520to%2520various%2520clinically-relevant%250Atasks%2520due%2520to%2520the%2520lack%2520of%2520unified%252C%2520standardized%252C%2520and%2520comprehensive%2520benchmark.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520BenchX%252C%2520a%2520unified%2520benchmark%2520framework%2520that%2520enables%250Ahead-to-head%2520comparison%2520and%2520systematical%2520analysis%2520between%2520MedVLP%2520methods%2520using%250Apublic%2520chest%2520X-ray%2520datasets.%2520Specifically%252C%2520BenchX%2520is%2520composed%2520of%2520three%250Acomponents%253A%25201%2529%2520Comprehensive%2520datasets%2520covering%2520nine%2520datasets%2520and%2520four%2520medical%250Atasks%253B%25202%2529%2520Benchmark%2520suites%2520to%2520standardize%2520data%2520preprocessing%252C%2520train-test%250Asplits%252C%2520and%2520parameter%2520selection%253B%25203%2529%2520Unified%2520finetuning%2520protocols%2520that%250Aaccommodate%2520heterogeneous%2520MedVLP%2520methods%2520for%2520consistent%2520task%2520adaptation%2520in%250Aclassification%252C%2520segmentation%252C%2520and%2520report%2520generation%252C%2520respectively.%2520Utilizing%250ABenchX%252C%2520we%2520establish%2520baselines%2520for%2520nine%2520state-of-the-art%2520MedVLP%2520methods%2520and%250Afound%2520that%2520the%2520performance%2520of%2520some%2520early%2520MedVLP%2520methods%2520can%2520be%2520enhanced%2520to%250Asurpass%2520more%2520recent%2520ones%252C%2520prompting%2520a%2520revisiting%2520of%2520the%2520developments%2520and%250Aconclusions%2520from%2520prior%2520works%2520in%2520MedVLP.%2520Our%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/yangzhou12/BenchX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BenchX%3A%20A%20Unified%20Benchmark%20Framework%20for%20Medical%20Vision-Language%0A%20%20Pretraining%20on%20Chest%20X-Rays&entry.906535625=Yang%20Zhou%20and%20Tan%20Li%20Hui%20Faith%20and%20Yanyu%20Xu%20and%20Sicong%20Leng%20and%20Xinxing%20Xu%20and%20Yong%20Liu%20and%20Rick%20Siow%20Mong%20Goh&entry.1292438233=%20%20Medical%20Vision-Language%20Pretraining%20%28MedVLP%29%20shows%20promise%20in%20learning%0Ageneralizable%20and%20transferable%20visual%20representations%20from%20paired%20and%20unpaired%0Amedical%20images%20and%20reports.%20MedVLP%20can%20provide%20useful%20features%20to%20downstream%0Atasks%20and%20facilitate%20adapting%20task-specific%20models%20to%20new%20setups%20using%20fewer%0Aexamples.%20However%2C%20existing%20MedVLP%20methods%20often%20differ%20in%20terms%20of%20datasets%2C%0Apreprocessing%2C%20and%20finetuning%20implementations.%20This%20pose%20great%20challenges%20in%0Aevaluating%20how%20well%20a%20MedVLP%20method%20generalizes%20to%20various%20clinically-relevant%0Atasks%20due%20to%20the%20lack%20of%20unified%2C%20standardized%2C%20and%20comprehensive%20benchmark.%20To%0Afill%20this%20gap%2C%20we%20propose%20BenchX%2C%20a%20unified%20benchmark%20framework%20that%20enables%0Ahead-to-head%20comparison%20and%20systematical%20analysis%20between%20MedVLP%20methods%20using%0Apublic%20chest%20X-ray%20datasets.%20Specifically%2C%20BenchX%20is%20composed%20of%20three%0Acomponents%3A%201%29%20Comprehensive%20datasets%20covering%20nine%20datasets%20and%20four%20medical%0Atasks%3B%202%29%20Benchmark%20suites%20to%20standardize%20data%20preprocessing%2C%20train-test%0Asplits%2C%20and%20parameter%20selection%3B%203%29%20Unified%20finetuning%20protocols%20that%0Aaccommodate%20heterogeneous%20MedVLP%20methods%20for%20consistent%20task%20adaptation%20in%0Aclassification%2C%20segmentation%2C%20and%20report%20generation%2C%20respectively.%20Utilizing%0ABenchX%2C%20we%20establish%20baselines%20for%20nine%20state-of-the-art%20MedVLP%20methods%20and%0Afound%20that%20the%20performance%20of%20some%20early%20MedVLP%20methods%20can%20be%20enhanced%20to%0Asurpass%20more%20recent%20ones%2C%20prompting%20a%20revisiting%20of%20the%20developments%20and%0Aconclusions%20from%20prior%20works%20in%20MedVLP.%20Our%20code%20are%20available%20at%0Ahttps%3A//github.com/yangzhou12/BenchX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21969v1&entry.124074799=Read"},
{"title": "Lighten CARAFE: Dynamic Lightweight Upsampling with Guided Reassemble\n  Kernels", "author": "Ruigang Fu and Qingyong Hu and Xiaohu Dong and Yinghui Gao and Biao Li and Ping Zhong", "abstract": "  As a fundamental operation in modern machine vision models, feature\nupsampling has been widely used and investigated in the literatures. An ideal\nupsampling operation should be lightweight, with low computational complexity.\nThat is, it can not only improve the overall performance but also not affect\nthe model complexity. Content-aware Reassembly of Features (CARAFE) is a\nwell-designed learnable operation to achieve feature upsampling. Albeit\nencouraging performance achieved, this method requires generating large-scale\nkernels, which brings a mass of extra redundant parameters, and inherently has\nlimited scalability. To this end, we propose a lightweight upsampling\noperation, termed Dynamic Lightweight Upsampling (DLU) in this paper. In\nparticular, it first constructs a small-scale source kernel space, and then\nsamples the large-scale kernels from the kernel space by introducing learnable\nguidance offsets, hence avoiding introducing a large collection of trainable\nparameters in upsampling. Experiments on several mainstream vision tasks show\nthat our DLU achieves comparable and even better performance to the original\nCARAFE, but with much lower complexity, e.g., DLU requires 91% fewer parameters\nand at least 63% fewer FLOPs (Floating Point Operations) than CARAFE in the\ncase of 16x upsampling, but outperforms the CARAFE by 0.3% mAP in object\ndetection. Code is available at\nhttps://github.com/Fu0511/Dynamic-Lightweight-Upsampling.\n", "link": "http://arxiv.org/abs/2410.22139v1", "date": "2024-10-29", "relevancy": 2.2391, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5551}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lighten%20CARAFE%3A%20Dynamic%20Lightweight%20Upsampling%20with%20Guided%20Reassemble%0A%20%20Kernels&body=Title%3A%20Lighten%20CARAFE%3A%20Dynamic%20Lightweight%20Upsampling%20with%20Guided%20Reassemble%0A%20%20Kernels%0AAuthor%3A%20Ruigang%20Fu%20and%20Qingyong%20Hu%20and%20Xiaohu%20Dong%20and%20Yinghui%20Gao%20and%20Biao%20Li%20and%20Ping%20Zhong%0AAbstract%3A%20%20%20As%20a%20fundamental%20operation%20in%20modern%20machine%20vision%20models%2C%20feature%0Aupsampling%20has%20been%20widely%20used%20and%20investigated%20in%20the%20literatures.%20An%20ideal%0Aupsampling%20operation%20should%20be%20lightweight%2C%20with%20low%20computational%20complexity.%0AThat%20is%2C%20it%20can%20not%20only%20improve%20the%20overall%20performance%20but%20also%20not%20affect%0Athe%20model%20complexity.%20Content-aware%20Reassembly%20of%20Features%20%28CARAFE%29%20is%20a%0Awell-designed%20learnable%20operation%20to%20achieve%20feature%20upsampling.%20Albeit%0Aencouraging%20performance%20achieved%2C%20this%20method%20requires%20generating%20large-scale%0Akernels%2C%20which%20brings%20a%20mass%20of%20extra%20redundant%20parameters%2C%20and%20inherently%20has%0Alimited%20scalability.%20To%20this%20end%2C%20we%20propose%20a%20lightweight%20upsampling%0Aoperation%2C%20termed%20Dynamic%20Lightweight%20Upsampling%20%28DLU%29%20in%20this%20paper.%20In%0Aparticular%2C%20it%20first%20constructs%20a%20small-scale%20source%20kernel%20space%2C%20and%20then%0Asamples%20the%20large-scale%20kernels%20from%20the%20kernel%20space%20by%20introducing%20learnable%0Aguidance%20offsets%2C%20hence%20avoiding%20introducing%20a%20large%20collection%20of%20trainable%0Aparameters%20in%20upsampling.%20Experiments%20on%20several%20mainstream%20vision%20tasks%20show%0Athat%20our%20DLU%20achieves%20comparable%20and%20even%20better%20performance%20to%20the%20original%0ACARAFE%2C%20but%20with%20much%20lower%20complexity%2C%20e.g.%2C%20DLU%20requires%2091%25%20fewer%20parameters%0Aand%20at%20least%2063%25%20fewer%20FLOPs%20%28Floating%20Point%20Operations%29%20than%20CARAFE%20in%20the%0Acase%20of%2016x%20upsampling%2C%20but%20outperforms%20the%20CARAFE%20by%200.3%25%20mAP%20in%20object%0Adetection.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Fu0511/Dynamic-Lightweight-Upsampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLighten%2520CARAFE%253A%2520Dynamic%2520Lightweight%2520Upsampling%2520with%2520Guided%2520Reassemble%250A%2520%2520Kernels%26entry.906535625%3DRuigang%2520Fu%2520and%2520Qingyong%2520Hu%2520and%2520Xiaohu%2520Dong%2520and%2520Yinghui%2520Gao%2520and%2520Biao%2520Li%2520and%2520Ping%2520Zhong%26entry.1292438233%3D%2520%2520As%2520a%2520fundamental%2520operation%2520in%2520modern%2520machine%2520vision%2520models%252C%2520feature%250Aupsampling%2520has%2520been%2520widely%2520used%2520and%2520investigated%2520in%2520the%2520literatures.%2520An%2520ideal%250Aupsampling%2520operation%2520should%2520be%2520lightweight%252C%2520with%2520low%2520computational%2520complexity.%250AThat%2520is%252C%2520it%2520can%2520not%2520only%2520improve%2520the%2520overall%2520performance%2520but%2520also%2520not%2520affect%250Athe%2520model%2520complexity.%2520Content-aware%2520Reassembly%2520of%2520Features%2520%2528CARAFE%2529%2520is%2520a%250Awell-designed%2520learnable%2520operation%2520to%2520achieve%2520feature%2520upsampling.%2520Albeit%250Aencouraging%2520performance%2520achieved%252C%2520this%2520method%2520requires%2520generating%2520large-scale%250Akernels%252C%2520which%2520brings%2520a%2520mass%2520of%2520extra%2520redundant%2520parameters%252C%2520and%2520inherently%2520has%250Alimited%2520scalability.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520lightweight%2520upsampling%250Aoperation%252C%2520termed%2520Dynamic%2520Lightweight%2520Upsampling%2520%2528DLU%2529%2520in%2520this%2520paper.%2520In%250Aparticular%252C%2520it%2520first%2520constructs%2520a%2520small-scale%2520source%2520kernel%2520space%252C%2520and%2520then%250Asamples%2520the%2520large-scale%2520kernels%2520from%2520the%2520kernel%2520space%2520by%2520introducing%2520learnable%250Aguidance%2520offsets%252C%2520hence%2520avoiding%2520introducing%2520a%2520large%2520collection%2520of%2520trainable%250Aparameters%2520in%2520upsampling.%2520Experiments%2520on%2520several%2520mainstream%2520vision%2520tasks%2520show%250Athat%2520our%2520DLU%2520achieves%2520comparable%2520and%2520even%2520better%2520performance%2520to%2520the%2520original%250ACARAFE%252C%2520but%2520with%2520much%2520lower%2520complexity%252C%2520e.g.%252C%2520DLU%2520requires%252091%2525%2520fewer%2520parameters%250Aand%2520at%2520least%252063%2525%2520fewer%2520FLOPs%2520%2528Floating%2520Point%2520Operations%2529%2520than%2520CARAFE%2520in%2520the%250Acase%2520of%252016x%2520upsampling%252C%2520but%2520outperforms%2520the%2520CARAFE%2520by%25200.3%2525%2520mAP%2520in%2520object%250Adetection.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Fu0511/Dynamic-Lightweight-Upsampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lighten%20CARAFE%3A%20Dynamic%20Lightweight%20Upsampling%20with%20Guided%20Reassemble%0A%20%20Kernels&entry.906535625=Ruigang%20Fu%20and%20Qingyong%20Hu%20and%20Xiaohu%20Dong%20and%20Yinghui%20Gao%20and%20Biao%20Li%20and%20Ping%20Zhong&entry.1292438233=%20%20As%20a%20fundamental%20operation%20in%20modern%20machine%20vision%20models%2C%20feature%0Aupsampling%20has%20been%20widely%20used%20and%20investigated%20in%20the%20literatures.%20An%20ideal%0Aupsampling%20operation%20should%20be%20lightweight%2C%20with%20low%20computational%20complexity.%0AThat%20is%2C%20it%20can%20not%20only%20improve%20the%20overall%20performance%20but%20also%20not%20affect%0Athe%20model%20complexity.%20Content-aware%20Reassembly%20of%20Features%20%28CARAFE%29%20is%20a%0Awell-designed%20learnable%20operation%20to%20achieve%20feature%20upsampling.%20Albeit%0Aencouraging%20performance%20achieved%2C%20this%20method%20requires%20generating%20large-scale%0Akernels%2C%20which%20brings%20a%20mass%20of%20extra%20redundant%20parameters%2C%20and%20inherently%20has%0Alimited%20scalability.%20To%20this%20end%2C%20we%20propose%20a%20lightweight%20upsampling%0Aoperation%2C%20termed%20Dynamic%20Lightweight%20Upsampling%20%28DLU%29%20in%20this%20paper.%20In%0Aparticular%2C%20it%20first%20constructs%20a%20small-scale%20source%20kernel%20space%2C%20and%20then%0Asamples%20the%20large-scale%20kernels%20from%20the%20kernel%20space%20by%20introducing%20learnable%0Aguidance%20offsets%2C%20hence%20avoiding%20introducing%20a%20large%20collection%20of%20trainable%0Aparameters%20in%20upsampling.%20Experiments%20on%20several%20mainstream%20vision%20tasks%20show%0Athat%20our%20DLU%20achieves%20comparable%20and%20even%20better%20performance%20to%20the%20original%0ACARAFE%2C%20but%20with%20much%20lower%20complexity%2C%20e.g.%2C%20DLU%20requires%2091%25%20fewer%20parameters%0Aand%20at%20least%2063%25%20fewer%20FLOPs%20%28Floating%20Point%20Operations%29%20than%20CARAFE%20in%20the%0Acase%20of%2016x%20upsampling%2C%20but%20outperforms%20the%20CARAFE%20by%200.3%25%20mAP%20in%20object%0Adetection.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Fu0511/Dynamic-Lightweight-Upsampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22139v1&entry.124074799=Read"},
{"title": "LiVisSfM: Accurate and Robust Structure-from-Motion with LiDAR and\n  Visual Cues", "author": "Hanqing Jiang and Liyang Zhou and Zhuang Zhang and Yihao Yu and Guofeng Zhang", "abstract": "  This paper presents an accurate and robust Structure-from-Motion (SfM)\npipeline named LiVisSfM, which is an SfM-based reconstruction system that fully\ncombines LiDAR and visual cues. Unlike most existing LiDAR-inertial odometry\n(LIO) and LiDAR-inertial-visual odometry (LIVO) methods relying heavily on\nLiDAR registration coupled with Inertial Measurement Unit (IMU), we propose a\nLiDAR-visual SfM method which innovatively carries out LiDAR frame registration\nto LiDAR voxel map in a Point-to-Gaussian residual metrics, combined with a\nLiDAR-visual BA and explicit loop closure in a bundle optimization way to\nachieve accurate and robust LiDAR pose estimation without dependence on IMU\nincorporation. Besides, we propose an incremental voxel updating strategy for\nefficient voxel map updating during the process of LiDAR frame registration and\nLiDAR-visual BA optimization. Experiments demonstrate the superior\neffectiveness of our LiVisSfM framework over state-of-the-art LIO and LIVO\nworks on more accurate and robust LiDAR pose recovery and dense point cloud\nreconstruction of both public KITTI benchmark and a variety of self-captured\ndataset.\n", "link": "http://arxiv.org/abs/2410.22213v1", "date": "2024-10-29", "relevancy": 2.2304, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5671}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiVisSfM%3A%20Accurate%20and%20Robust%20Structure-from-Motion%20with%20LiDAR%20and%0A%20%20Visual%20Cues&body=Title%3A%20LiVisSfM%3A%20Accurate%20and%20Robust%20Structure-from-Motion%20with%20LiDAR%20and%0A%20%20Visual%20Cues%0AAuthor%3A%20Hanqing%20Jiang%20and%20Liyang%20Zhou%20and%20Zhuang%20Zhang%20and%20Yihao%20Yu%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20accurate%20and%20robust%20Structure-from-Motion%20%28SfM%29%0Apipeline%20named%20LiVisSfM%2C%20which%20is%20an%20SfM-based%20reconstruction%20system%20that%20fully%0Acombines%20LiDAR%20and%20visual%20cues.%20Unlike%20most%20existing%20LiDAR-inertial%20odometry%0A%28LIO%29%20and%20LiDAR-inertial-visual%20odometry%20%28LIVO%29%20methods%20relying%20heavily%20on%0ALiDAR%20registration%20coupled%20with%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20we%20propose%20a%0ALiDAR-visual%20SfM%20method%20which%20innovatively%20carries%20out%20LiDAR%20frame%20registration%0Ato%20LiDAR%20voxel%20map%20in%20a%20Point-to-Gaussian%20residual%20metrics%2C%20combined%20with%20a%0ALiDAR-visual%20BA%20and%20explicit%20loop%20closure%20in%20a%20bundle%20optimization%20way%20to%0Aachieve%20accurate%20and%20robust%20LiDAR%20pose%20estimation%20without%20dependence%20on%20IMU%0Aincorporation.%20Besides%2C%20we%20propose%20an%20incremental%20voxel%20updating%20strategy%20for%0Aefficient%20voxel%20map%20updating%20during%20the%20process%20of%20LiDAR%20frame%20registration%20and%0ALiDAR-visual%20BA%20optimization.%20Experiments%20demonstrate%20the%20superior%0Aeffectiveness%20of%20our%20LiVisSfM%20framework%20over%20state-of-the-art%20LIO%20and%20LIVO%0Aworks%20on%20more%20accurate%20and%20robust%20LiDAR%20pose%20recovery%20and%20dense%20point%20cloud%0Areconstruction%20of%20both%20public%20KITTI%20benchmark%20and%20a%20variety%20of%20self-captured%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiVisSfM%253A%2520Accurate%2520and%2520Robust%2520Structure-from-Motion%2520with%2520LiDAR%2520and%250A%2520%2520Visual%2520Cues%26entry.906535625%3DHanqing%2520Jiang%2520and%2520Liyang%2520Zhou%2520and%2520Zhuang%2520Zhang%2520and%2520Yihao%2520Yu%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520accurate%2520and%2520robust%2520Structure-from-Motion%2520%2528SfM%2529%250Apipeline%2520named%2520LiVisSfM%252C%2520which%2520is%2520an%2520SfM-based%2520reconstruction%2520system%2520that%2520fully%250Acombines%2520LiDAR%2520and%2520visual%2520cues.%2520Unlike%2520most%2520existing%2520LiDAR-inertial%2520odometry%250A%2528LIO%2529%2520and%2520LiDAR-inertial-visual%2520odometry%2520%2528LIVO%2529%2520methods%2520relying%2520heavily%2520on%250ALiDAR%2520registration%2520coupled%2520with%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%252C%2520we%2520propose%2520a%250ALiDAR-visual%2520SfM%2520method%2520which%2520innovatively%2520carries%2520out%2520LiDAR%2520frame%2520registration%250Ato%2520LiDAR%2520voxel%2520map%2520in%2520a%2520Point-to-Gaussian%2520residual%2520metrics%252C%2520combined%2520with%2520a%250ALiDAR-visual%2520BA%2520and%2520explicit%2520loop%2520closure%2520in%2520a%2520bundle%2520optimization%2520way%2520to%250Aachieve%2520accurate%2520and%2520robust%2520LiDAR%2520pose%2520estimation%2520without%2520dependence%2520on%2520IMU%250Aincorporation.%2520Besides%252C%2520we%2520propose%2520an%2520incremental%2520voxel%2520updating%2520strategy%2520for%250Aefficient%2520voxel%2520map%2520updating%2520during%2520the%2520process%2520of%2520LiDAR%2520frame%2520registration%2520and%250ALiDAR-visual%2520BA%2520optimization.%2520Experiments%2520demonstrate%2520the%2520superior%250Aeffectiveness%2520of%2520our%2520LiVisSfM%2520framework%2520over%2520state-of-the-art%2520LIO%2520and%2520LIVO%250Aworks%2520on%2520more%2520accurate%2520and%2520robust%2520LiDAR%2520pose%2520recovery%2520and%2520dense%2520point%2520cloud%250Areconstruction%2520of%2520both%2520public%2520KITTI%2520benchmark%2520and%2520a%2520variety%2520of%2520self-captured%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiVisSfM%3A%20Accurate%20and%20Robust%20Structure-from-Motion%20with%20LiDAR%20and%0A%20%20Visual%20Cues&entry.906535625=Hanqing%20Jiang%20and%20Liyang%20Zhou%20and%20Zhuang%20Zhang%20and%20Yihao%20Yu%20and%20Guofeng%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20an%20accurate%20and%20robust%20Structure-from-Motion%20%28SfM%29%0Apipeline%20named%20LiVisSfM%2C%20which%20is%20an%20SfM-based%20reconstruction%20system%20that%20fully%0Acombines%20LiDAR%20and%20visual%20cues.%20Unlike%20most%20existing%20LiDAR-inertial%20odometry%0A%28LIO%29%20and%20LiDAR-inertial-visual%20odometry%20%28LIVO%29%20methods%20relying%20heavily%20on%0ALiDAR%20registration%20coupled%20with%20Inertial%20Measurement%20Unit%20%28IMU%29%2C%20we%20propose%20a%0ALiDAR-visual%20SfM%20method%20which%20innovatively%20carries%20out%20LiDAR%20frame%20registration%0Ato%20LiDAR%20voxel%20map%20in%20a%20Point-to-Gaussian%20residual%20metrics%2C%20combined%20with%20a%0ALiDAR-visual%20BA%20and%20explicit%20loop%20closure%20in%20a%20bundle%20optimization%20way%20to%0Aachieve%20accurate%20and%20robust%20LiDAR%20pose%20estimation%20without%20dependence%20on%20IMU%0Aincorporation.%20Besides%2C%20we%20propose%20an%20incremental%20voxel%20updating%20strategy%20for%0Aefficient%20voxel%20map%20updating%20during%20the%20process%20of%20LiDAR%20frame%20registration%20and%0ALiDAR-visual%20BA%20optimization.%20Experiments%20demonstrate%20the%20superior%0Aeffectiveness%20of%20our%20LiVisSfM%20framework%20over%20state-of-the-art%20LIO%20and%20LIVO%0Aworks%20on%20more%20accurate%20and%20robust%20LiDAR%20pose%20recovery%20and%20dense%20point%20cloud%0Areconstruction%20of%20both%20public%20KITTI%20benchmark%20and%20a%20variety%20of%20self-captured%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22213v1&entry.124074799=Read"},
{"title": "Senna: Bridging Large Vision-Language Models and End-to-End Autonomous\n  Driving", "author": "Bo Jiang and Shaoyu Chen and Bencheng Liao and Xingyu Zhang and Wei Yin and Qian Zhang and Chang Huang and Wenyu Liu and Xinggang Wang", "abstract": "  End-to-end autonomous driving demonstrates strong planning capabilities with\nlarge-scale data but still struggles in complex, rare scenarios due to limited\ncommonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene\nunderstanding and reasoning. The path forward lies in merging the strengths of\nboth approaches. Previous methods using LVLMs to predict trajectories or\ncontrol signals yield suboptimal results, as LVLMs are not well-suited for\nprecise numerical predictions. This paper presents Senna, an autonomous driving\nsystem combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E).\nSenna decouples high-level planning from low-level trajectory prediction.\nSenna-VLM generates planning decisions in natural language, while Senna-E2E\npredicts precise trajectories. Senna-VLM utilizes a multi-image encoding\napproach and multi-view prompts for efficient scene understanding. Besides, we\nintroduce planning-oriented QAs alongside a three-stage training strategy,\nwhich enhances Senna-VLM's planning performance while preserving commonsense.\nExtensive experiments on two datasets show that Senna achieves state-of-the-art\nplanning performance. Notably, with pre-training on a large-scale dataset\nDriveX and fine-tuning on nuScenes, Senna significantly reduces average\nplanning error by 27.12% and collision rate by 33.33% over model without\npre-training. We believe Senna's cross-scenario generalization and\ntransferability are essential for achieving fully autonomous driving. Code and\nmodels will be released at https://github.com/hustvl/Senna.\n", "link": "http://arxiv.org/abs/2410.22313v1", "date": "2024-10-29", "relevancy": 2.2285, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Senna%3A%20Bridging%20Large%20Vision-Language%20Models%20and%20End-to-End%20Autonomous%0A%20%20Driving&body=Title%3A%20Senna%3A%20Bridging%20Large%20Vision-Language%20Models%20and%20End-to-End%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Bo%20Jiang%20and%20Shaoyu%20Chen%20and%20Bencheng%20Liao%20and%20Xingyu%20Zhang%20and%20Wei%20Yin%20and%20Qian%20Zhang%20and%20Chang%20Huang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20demonstrates%20strong%20planning%20capabilities%20with%0Alarge-scale%20data%20but%20still%20struggles%20in%20complex%2C%20rare%20scenarios%20due%20to%20limited%0Acommonsense.%20In%20contrast%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20in%20scene%0Aunderstanding%20and%20reasoning.%20The%20path%20forward%20lies%20in%20merging%20the%20strengths%20of%0Aboth%20approaches.%20Previous%20methods%20using%20LVLMs%20to%20predict%20trajectories%20or%0Acontrol%20signals%20yield%20suboptimal%20results%2C%20as%20LVLMs%20are%20not%20well-suited%20for%0Aprecise%20numerical%20predictions.%20This%20paper%20presents%20Senna%2C%20an%20autonomous%20driving%0Asystem%20combining%20an%20LVLM%20%28Senna-VLM%29%20with%20an%20end-to-end%20model%20%28Senna-E2E%29.%0ASenna%20decouples%20high-level%20planning%20from%20low-level%20trajectory%20prediction.%0ASenna-VLM%20generates%20planning%20decisions%20in%20natural%20language%2C%20while%20Senna-E2E%0Apredicts%20precise%20trajectories.%20Senna-VLM%20utilizes%20a%20multi-image%20encoding%0Aapproach%20and%20multi-view%20prompts%20for%20efficient%20scene%20understanding.%20Besides%2C%20we%0Aintroduce%20planning-oriented%20QAs%20alongside%20a%20three-stage%20training%20strategy%2C%0Awhich%20enhances%20Senna-VLM%27s%20planning%20performance%20while%20preserving%20commonsense.%0AExtensive%20experiments%20on%20two%20datasets%20show%20that%20Senna%20achieves%20state-of-the-art%0Aplanning%20performance.%20Notably%2C%20with%20pre-training%20on%20a%20large-scale%20dataset%0ADriveX%20and%20fine-tuning%20on%20nuScenes%2C%20Senna%20significantly%20reduces%20average%0Aplanning%20error%20by%2027.12%25%20and%20collision%20rate%20by%2033.33%25%20over%20model%20without%0Apre-training.%20We%20believe%20Senna%27s%20cross-scenario%20generalization%20and%0Atransferability%20are%20essential%20for%20achieving%20fully%20autonomous%20driving.%20Code%20and%0Amodels%20will%20be%20released%20at%20https%3A//github.com/hustvl/Senna.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSenna%253A%2520Bridging%2520Large%2520Vision-Language%2520Models%2520and%2520End-to-End%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DBo%2520Jiang%2520and%2520Shaoyu%2520Chen%2520and%2520Bencheng%2520Liao%2520and%2520Xingyu%2520Zhang%2520and%2520Wei%2520Yin%2520and%2520Qian%2520Zhang%2520and%2520Chang%2520Huang%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520demonstrates%2520strong%2520planning%2520capabilities%2520with%250Alarge-scale%2520data%2520but%2520still%2520struggles%2520in%2520complex%252C%2520rare%2520scenarios%2520due%2520to%2520limited%250Acommonsense.%2520In%2520contrast%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520excel%2520in%2520scene%250Aunderstanding%2520and%2520reasoning.%2520The%2520path%2520forward%2520lies%2520in%2520merging%2520the%2520strengths%2520of%250Aboth%2520approaches.%2520Previous%2520methods%2520using%2520LVLMs%2520to%2520predict%2520trajectories%2520or%250Acontrol%2520signals%2520yield%2520suboptimal%2520results%252C%2520as%2520LVLMs%2520are%2520not%2520well-suited%2520for%250Aprecise%2520numerical%2520predictions.%2520This%2520paper%2520presents%2520Senna%252C%2520an%2520autonomous%2520driving%250Asystem%2520combining%2520an%2520LVLM%2520%2528Senna-VLM%2529%2520with%2520an%2520end-to-end%2520model%2520%2528Senna-E2E%2529.%250ASenna%2520decouples%2520high-level%2520planning%2520from%2520low-level%2520trajectory%2520prediction.%250ASenna-VLM%2520generates%2520planning%2520decisions%2520in%2520natural%2520language%252C%2520while%2520Senna-E2E%250Apredicts%2520precise%2520trajectories.%2520Senna-VLM%2520utilizes%2520a%2520multi-image%2520encoding%250Aapproach%2520and%2520multi-view%2520prompts%2520for%2520efficient%2520scene%2520understanding.%2520Besides%252C%2520we%250Aintroduce%2520planning-oriented%2520QAs%2520alongside%2520a%2520three-stage%2520training%2520strategy%252C%250Awhich%2520enhances%2520Senna-VLM%2527s%2520planning%2520performance%2520while%2520preserving%2520commonsense.%250AExtensive%2520experiments%2520on%2520two%2520datasets%2520show%2520that%2520Senna%2520achieves%2520state-of-the-art%250Aplanning%2520performance.%2520Notably%252C%2520with%2520pre-training%2520on%2520a%2520large-scale%2520dataset%250ADriveX%2520and%2520fine-tuning%2520on%2520nuScenes%252C%2520Senna%2520significantly%2520reduces%2520average%250Aplanning%2520error%2520by%252027.12%2525%2520and%2520collision%2520rate%2520by%252033.33%2525%2520over%2520model%2520without%250Apre-training.%2520We%2520believe%2520Senna%2527s%2520cross-scenario%2520generalization%2520and%250Atransferability%2520are%2520essential%2520for%2520achieving%2520fully%2520autonomous%2520driving.%2520Code%2520and%250Amodels%2520will%2520be%2520released%2520at%2520https%253A//github.com/hustvl/Senna.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Senna%3A%20Bridging%20Large%20Vision-Language%20Models%20and%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Bo%20Jiang%20and%20Shaoyu%20Chen%20and%20Bencheng%20Liao%20and%20Xingyu%20Zhang%20and%20Wei%20Yin%20and%20Qian%20Zhang%20and%20Chang%20Huang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20demonstrates%20strong%20planning%20capabilities%20with%0Alarge-scale%20data%20but%20still%20struggles%20in%20complex%2C%20rare%20scenarios%20due%20to%20limited%0Acommonsense.%20In%20contrast%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20in%20scene%0Aunderstanding%20and%20reasoning.%20The%20path%20forward%20lies%20in%20merging%20the%20strengths%20of%0Aboth%20approaches.%20Previous%20methods%20using%20LVLMs%20to%20predict%20trajectories%20or%0Acontrol%20signals%20yield%20suboptimal%20results%2C%20as%20LVLMs%20are%20not%20well-suited%20for%0Aprecise%20numerical%20predictions.%20This%20paper%20presents%20Senna%2C%20an%20autonomous%20driving%0Asystem%20combining%20an%20LVLM%20%28Senna-VLM%29%20with%20an%20end-to-end%20model%20%28Senna-E2E%29.%0ASenna%20decouples%20high-level%20planning%20from%20low-level%20trajectory%20prediction.%0ASenna-VLM%20generates%20planning%20decisions%20in%20natural%20language%2C%20while%20Senna-E2E%0Apredicts%20precise%20trajectories.%20Senna-VLM%20utilizes%20a%20multi-image%20encoding%0Aapproach%20and%20multi-view%20prompts%20for%20efficient%20scene%20understanding.%20Besides%2C%20we%0Aintroduce%20planning-oriented%20QAs%20alongside%20a%20three-stage%20training%20strategy%2C%0Awhich%20enhances%20Senna-VLM%27s%20planning%20performance%20while%20preserving%20commonsense.%0AExtensive%20experiments%20on%20two%20datasets%20show%20that%20Senna%20achieves%20state-of-the-art%0Aplanning%20performance.%20Notably%2C%20with%20pre-training%20on%20a%20large-scale%20dataset%0ADriveX%20and%20fine-tuning%20on%20nuScenes%2C%20Senna%20significantly%20reduces%20average%0Aplanning%20error%20by%2027.12%25%20and%20collision%20rate%20by%2033.33%25%20over%20model%20without%0Apre-training.%20We%20believe%20Senna%27s%20cross-scenario%20generalization%20and%0Atransferability%20are%20essential%20for%20achieving%20fully%20autonomous%20driving.%20Code%20and%0Amodels%20will%20be%20released%20at%20https%3A//github.com/hustvl/Senna.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22313v1&entry.124074799=Read"},
{"title": "SMART: Scalable Multi-agent Real-time Generation via Next-token\n  Prediction", "author": "Wei Wu and Xiaoxin Feng and Ziyan Gao and Yuheng Kan", "abstract": "  Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART.\n", "link": "http://arxiv.org/abs/2405.15677v2", "date": "2024-10-29", "relevancy": 2.2251, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5449}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Scalable%20Multi-agent%20Real-time%20Generation%20via%20Next-token%0A%20%20Prediction&body=Title%3A%20SMART%3A%20Scalable%20Multi-agent%20Real-time%20Generation%20via%20Next-token%0A%20%20Prediction%0AAuthor%3A%20Wei%20Wu%20and%20Xiaoxin%20Feng%20and%20Ziyan%20Gao%20and%20Yuheng%20Kan%0AAbstract%3A%20%20%20Data-driven%20autonomous%20driving%20motion%20generation%20tasks%20are%20frequently%0Aimpacted%20by%20the%20limitations%20of%20dataset%20size%20and%20the%20domain%20gap%20between%0Adatasets%2C%20which%20precludes%20their%20extensive%20application%20in%20real-world%20scenarios.%0ATo%20address%20this%20issue%2C%20we%20introduce%20SMART%2C%20a%20novel%20autonomous%20driving%20motion%0Ageneration%20paradigm%20that%20models%20vectorized%20map%20and%20agent%20trajectory%20data%20into%0Adiscrete%20sequence%20tokens.%20These%20tokens%20are%20then%20processed%20through%20a%0Adecoder-only%20transformer%20architecture%20to%20train%20for%20the%20next%20token%20prediction%0Atask%20across%20spatial-temporal%20series.%20This%20GPT-style%20method%20allows%20the%20model%20to%0Alearn%20the%20motion%20distribution%20in%20real%20driving%20scenarios.%20SMART%20achieves%0Astate-of-the-art%20performance%20across%20most%20of%20the%20metrics%20on%20the%20generative%20Sim%0AAgents%20challenge%2C%20ranking%201st%20on%20the%20leaderboards%20of%20Waymo%20Open%20Motion%20Dataset%0A%28WOMD%29%2C%20demonstrating%20remarkable%20inference%20speed.%20Moreover%2C%20SMART%20represents%0Athe%20generative%20model%20in%20the%20autonomous%20driving%20motion%20domain%2C%20exhibiting%0Azero-shot%20generalization%20capabilities%3A%20Using%20only%20the%20NuPlan%20dataset%20for%0Atraining%20and%20WOMD%20for%20validation%2C%20SMART%20achieved%20a%20competitive%20score%20of%200.72%20on%0Athe%20Sim%20Agents%20challenge.%20Lastly%2C%20we%20have%20collected%20over%201%20billion%20motion%0Atokens%20from%20multiple%20datasets%2C%20validating%20the%20model%27s%20scalability.%20These%0Aresults%20suggest%20that%20SMART%20has%20initially%20emulated%20two%20important%20properties%3A%0Ascalability%20and%20zero-shot%20generalization%2C%20and%20preliminarily%20meets%20the%20needs%20of%0Alarge-scale%20real-time%20simulation%20applications.%20We%20have%20released%20all%20the%20code%20to%0Apromote%20the%20exploration%20of%20models%20for%20motion%20generation%20in%20the%20autonomous%0Adriving%20field.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/rainmaker22/SMART.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Scalable%2520Multi-agent%2520Real-time%2520Generation%2520via%2520Next-token%250A%2520%2520Prediction%26entry.906535625%3DWei%2520Wu%2520and%2520Xiaoxin%2520Feng%2520and%2520Ziyan%2520Gao%2520and%2520Yuheng%2520Kan%26entry.1292438233%3D%2520%2520Data-driven%2520autonomous%2520driving%2520motion%2520generation%2520tasks%2520are%2520frequently%250Aimpacted%2520by%2520the%2520limitations%2520of%2520dataset%2520size%2520and%2520the%2520domain%2520gap%2520between%250Adatasets%252C%2520which%2520precludes%2520their%2520extensive%2520application%2520in%2520real-world%2520scenarios.%250ATo%2520address%2520this%2520issue%252C%2520we%2520introduce%2520SMART%252C%2520a%2520novel%2520autonomous%2520driving%2520motion%250Ageneration%2520paradigm%2520that%2520models%2520vectorized%2520map%2520and%2520agent%2520trajectory%2520data%2520into%250Adiscrete%2520sequence%2520tokens.%2520These%2520tokens%2520are%2520then%2520processed%2520through%2520a%250Adecoder-only%2520transformer%2520architecture%2520to%2520train%2520for%2520the%2520next%2520token%2520prediction%250Atask%2520across%2520spatial-temporal%2520series.%2520This%2520GPT-style%2520method%2520allows%2520the%2520model%2520to%250Alearn%2520the%2520motion%2520distribution%2520in%2520real%2520driving%2520scenarios.%2520SMART%2520achieves%250Astate-of-the-art%2520performance%2520across%2520most%2520of%2520the%2520metrics%2520on%2520the%2520generative%2520Sim%250AAgents%2520challenge%252C%2520ranking%25201st%2520on%2520the%2520leaderboards%2520of%2520Waymo%2520Open%2520Motion%2520Dataset%250A%2528WOMD%2529%252C%2520demonstrating%2520remarkable%2520inference%2520speed.%2520Moreover%252C%2520SMART%2520represents%250Athe%2520generative%2520model%2520in%2520the%2520autonomous%2520driving%2520motion%2520domain%252C%2520exhibiting%250Azero-shot%2520generalization%2520capabilities%253A%2520Using%2520only%2520the%2520NuPlan%2520dataset%2520for%250Atraining%2520and%2520WOMD%2520for%2520validation%252C%2520SMART%2520achieved%2520a%2520competitive%2520score%2520of%25200.72%2520on%250Athe%2520Sim%2520Agents%2520challenge.%2520Lastly%252C%2520we%2520have%2520collected%2520over%25201%2520billion%2520motion%250Atokens%2520from%2520multiple%2520datasets%252C%2520validating%2520the%2520model%2527s%2520scalability.%2520These%250Aresults%2520suggest%2520that%2520SMART%2520has%2520initially%2520emulated%2520two%2520important%2520properties%253A%250Ascalability%2520and%2520zero-shot%2520generalization%252C%2520and%2520preliminarily%2520meets%2520the%2520needs%2520of%250Alarge-scale%2520real-time%2520simulation%2520applications.%2520We%2520have%2520released%2520all%2520the%2520code%2520to%250Apromote%2520the%2520exploration%2520of%2520models%2520for%2520motion%2520generation%2520in%2520the%2520autonomous%250Adriving%2520field.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/rainmaker22/SMART.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Scalable%20Multi-agent%20Real-time%20Generation%20via%20Next-token%0A%20%20Prediction&entry.906535625=Wei%20Wu%20and%20Xiaoxin%20Feng%20and%20Ziyan%20Gao%20and%20Yuheng%20Kan&entry.1292438233=%20%20Data-driven%20autonomous%20driving%20motion%20generation%20tasks%20are%20frequently%0Aimpacted%20by%20the%20limitations%20of%20dataset%20size%20and%20the%20domain%20gap%20between%0Adatasets%2C%20which%20precludes%20their%20extensive%20application%20in%20real-world%20scenarios.%0ATo%20address%20this%20issue%2C%20we%20introduce%20SMART%2C%20a%20novel%20autonomous%20driving%20motion%0Ageneration%20paradigm%20that%20models%20vectorized%20map%20and%20agent%20trajectory%20data%20into%0Adiscrete%20sequence%20tokens.%20These%20tokens%20are%20then%20processed%20through%20a%0Adecoder-only%20transformer%20architecture%20to%20train%20for%20the%20next%20token%20prediction%0Atask%20across%20spatial-temporal%20series.%20This%20GPT-style%20method%20allows%20the%20model%20to%0Alearn%20the%20motion%20distribution%20in%20real%20driving%20scenarios.%20SMART%20achieves%0Astate-of-the-art%20performance%20across%20most%20of%20the%20metrics%20on%20the%20generative%20Sim%0AAgents%20challenge%2C%20ranking%201st%20on%20the%20leaderboards%20of%20Waymo%20Open%20Motion%20Dataset%0A%28WOMD%29%2C%20demonstrating%20remarkable%20inference%20speed.%20Moreover%2C%20SMART%20represents%0Athe%20generative%20model%20in%20the%20autonomous%20driving%20motion%20domain%2C%20exhibiting%0Azero-shot%20generalization%20capabilities%3A%20Using%20only%20the%20NuPlan%20dataset%20for%0Atraining%20and%20WOMD%20for%20validation%2C%20SMART%20achieved%20a%20competitive%20score%20of%200.72%20on%0Athe%20Sim%20Agents%20challenge.%20Lastly%2C%20we%20have%20collected%20over%201%20billion%20motion%0Atokens%20from%20multiple%20datasets%2C%20validating%20the%20model%27s%20scalability.%20These%0Aresults%20suggest%20that%20SMART%20has%20initially%20emulated%20two%20important%20properties%3A%0Ascalability%20and%20zero-shot%20generalization%2C%20and%20preliminarily%20meets%20the%20needs%20of%0Alarge-scale%20real-time%20simulation%20applications.%20We%20have%20released%20all%20the%20code%20to%0Apromote%20the%20exploration%20of%20models%20for%20motion%20generation%20in%20the%20autonomous%0Adriving%20field.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/rainmaker22/SMART.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15677v2&entry.124074799=Read"},
{"title": "VLMimic: Vision Language Models are Visual Imitation Learner for\n  Fine-grained Actions", "author": "Guanyan Chen and Meiling Wang and Te Cui and Yao Mu and Haoyang Lu and Tianxing Zhou and Zicai Peng and Mengxiao Hu and Haizhou Li and Yuan Li and Yi Yang and Yufeng Yue", "abstract": "  Visual imitation learning (VIL) provides an efficient and intuitive strategy\nfor robotic systems to acquire novel skills. Recent advancements in Vision\nLanguage Models (VLMs) have demonstrated remarkable performance in vision and\nlanguage reasoning capabilities for VIL tasks. Despite the progress, current\nVIL methods naively employ VLMs to learn high-level plans from human videos,\nrelying on pre-defined motion primitives for executing physical interactions,\nwhich remains a major bottleneck. In this work, we present VLMimic, a novel\nparadigm that harnesses VLMs to directly learn even fine-grained action levels,\nonly given a limited number of human videos. Specifically, VLMimic first\ngrounds object-centric movements from human videos, and learns skills using\nhierarchical constraint representations, facilitating the derivation of skills\nwith fine-grained action levels from limited human videos. These skills are\nrefined and updated through an iterative comparison strategy, enabling\nefficient adaptation to unseen environments. Our extensive experiments exhibit\nthat our VLMimic, using only 5 human videos, yields significant improvements of\nover 27% and 21% in RLBench and real-world manipulation tasks, and surpasses\nbaselines by over 37% in long-horizon tasks.\n", "link": "http://arxiv.org/abs/2410.20927v2", "date": "2024-10-29", "relevancy": 2.2241, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLMimic%3A%20Vision%20Language%20Models%20are%20Visual%20Imitation%20Learner%20for%0A%20%20Fine-grained%20Actions&body=Title%3A%20VLMimic%3A%20Vision%20Language%20Models%20are%20Visual%20Imitation%20Learner%20for%0A%20%20Fine-grained%20Actions%0AAuthor%3A%20Guanyan%20Chen%20and%20Meiling%20Wang%20and%20Te%20Cui%20and%20Yao%20Mu%20and%20Haoyang%20Lu%20and%20Tianxing%20Zhou%20and%20Zicai%20Peng%20and%20Mengxiao%20Hu%20and%20Haizhou%20Li%20and%20Yuan%20Li%20and%20Yi%20Yang%20and%20Yufeng%20Yue%0AAbstract%3A%20%20%20Visual%20imitation%20learning%20%28VIL%29%20provides%20an%20efficient%20and%20intuitive%20strategy%0Afor%20robotic%20systems%20to%20acquire%20novel%20skills.%20Recent%20advancements%20in%20Vision%0ALanguage%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20in%20vision%20and%0Alanguage%20reasoning%20capabilities%20for%20VIL%20tasks.%20Despite%20the%20progress%2C%20current%0AVIL%20methods%20naively%20employ%20VLMs%20to%20learn%20high-level%20plans%20from%20human%20videos%2C%0Arelying%20on%20pre-defined%20motion%20primitives%20for%20executing%20physical%20interactions%2C%0Awhich%20remains%20a%20major%20bottleneck.%20In%20this%20work%2C%20we%20present%20VLMimic%2C%20a%20novel%0Aparadigm%20that%20harnesses%20VLMs%20to%20directly%20learn%20even%20fine-grained%20action%20levels%2C%0Aonly%20given%20a%20limited%20number%20of%20human%20videos.%20Specifically%2C%20VLMimic%20first%0Agrounds%20object-centric%20movements%20from%20human%20videos%2C%20and%20learns%20skills%20using%0Ahierarchical%20constraint%20representations%2C%20facilitating%20the%20derivation%20of%20skills%0Awith%20fine-grained%20action%20levels%20from%20limited%20human%20videos.%20These%20skills%20are%0Arefined%20and%20updated%20through%20an%20iterative%20comparison%20strategy%2C%20enabling%0Aefficient%20adaptation%20to%20unseen%20environments.%20Our%20extensive%20experiments%20exhibit%0Athat%20our%20VLMimic%2C%20using%20only%205%20human%20videos%2C%20yields%20significant%20improvements%20of%0Aover%2027%25%20and%2021%25%20in%20RLBench%20and%20real-world%20manipulation%20tasks%2C%20and%20surpasses%0Abaselines%20by%20over%2037%25%20in%20long-horizon%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLMimic%253A%2520Vision%2520Language%2520Models%2520are%2520Visual%2520Imitation%2520Learner%2520for%250A%2520%2520Fine-grained%2520Actions%26entry.906535625%3DGuanyan%2520Chen%2520and%2520Meiling%2520Wang%2520and%2520Te%2520Cui%2520and%2520Yao%2520Mu%2520and%2520Haoyang%2520Lu%2520and%2520Tianxing%2520Zhou%2520and%2520Zicai%2520Peng%2520and%2520Mengxiao%2520Hu%2520and%2520Haizhou%2520Li%2520and%2520Yuan%2520Li%2520and%2520Yi%2520Yang%2520and%2520Yufeng%2520Yue%26entry.1292438233%3D%2520%2520Visual%2520imitation%2520learning%2520%2528VIL%2529%2520provides%2520an%2520efficient%2520and%2520intuitive%2520strategy%250Afor%2520robotic%2520systems%2520to%2520acquire%2520novel%2520skills.%2520Recent%2520advancements%2520in%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520vision%2520and%250Alanguage%2520reasoning%2520capabilities%2520for%2520VIL%2520tasks.%2520Despite%2520the%2520progress%252C%2520current%250AVIL%2520methods%2520naively%2520employ%2520VLMs%2520to%2520learn%2520high-level%2520plans%2520from%2520human%2520videos%252C%250Arelying%2520on%2520pre-defined%2520motion%2520primitives%2520for%2520executing%2520physical%2520interactions%252C%250Awhich%2520remains%2520a%2520major%2520bottleneck.%2520In%2520this%2520work%252C%2520we%2520present%2520VLMimic%252C%2520a%2520novel%250Aparadigm%2520that%2520harnesses%2520VLMs%2520to%2520directly%2520learn%2520even%2520fine-grained%2520action%2520levels%252C%250Aonly%2520given%2520a%2520limited%2520number%2520of%2520human%2520videos.%2520Specifically%252C%2520VLMimic%2520first%250Agrounds%2520object-centric%2520movements%2520from%2520human%2520videos%252C%2520and%2520learns%2520skills%2520using%250Ahierarchical%2520constraint%2520representations%252C%2520facilitating%2520the%2520derivation%2520of%2520skills%250Awith%2520fine-grained%2520action%2520levels%2520from%2520limited%2520human%2520videos.%2520These%2520skills%2520are%250Arefined%2520and%2520updated%2520through%2520an%2520iterative%2520comparison%2520strategy%252C%2520enabling%250Aefficient%2520adaptation%2520to%2520unseen%2520environments.%2520Our%2520extensive%2520experiments%2520exhibit%250Athat%2520our%2520VLMimic%252C%2520using%2520only%25205%2520human%2520videos%252C%2520yields%2520significant%2520improvements%2520of%250Aover%252027%2525%2520and%252021%2525%2520in%2520RLBench%2520and%2520real-world%2520manipulation%2520tasks%252C%2520and%2520surpasses%250Abaselines%2520by%2520over%252037%2525%2520in%2520long-horizon%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLMimic%3A%20Vision%20Language%20Models%20are%20Visual%20Imitation%20Learner%20for%0A%20%20Fine-grained%20Actions&entry.906535625=Guanyan%20Chen%20and%20Meiling%20Wang%20and%20Te%20Cui%20and%20Yao%20Mu%20and%20Haoyang%20Lu%20and%20Tianxing%20Zhou%20and%20Zicai%20Peng%20and%20Mengxiao%20Hu%20and%20Haizhou%20Li%20and%20Yuan%20Li%20and%20Yi%20Yang%20and%20Yufeng%20Yue&entry.1292438233=%20%20Visual%20imitation%20learning%20%28VIL%29%20provides%20an%20efficient%20and%20intuitive%20strategy%0Afor%20robotic%20systems%20to%20acquire%20novel%20skills.%20Recent%20advancements%20in%20Vision%0ALanguage%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20in%20vision%20and%0Alanguage%20reasoning%20capabilities%20for%20VIL%20tasks.%20Despite%20the%20progress%2C%20current%0AVIL%20methods%20naively%20employ%20VLMs%20to%20learn%20high-level%20plans%20from%20human%20videos%2C%0Arelying%20on%20pre-defined%20motion%20primitives%20for%20executing%20physical%20interactions%2C%0Awhich%20remains%20a%20major%20bottleneck.%20In%20this%20work%2C%20we%20present%20VLMimic%2C%20a%20novel%0Aparadigm%20that%20harnesses%20VLMs%20to%20directly%20learn%20even%20fine-grained%20action%20levels%2C%0Aonly%20given%20a%20limited%20number%20of%20human%20videos.%20Specifically%2C%20VLMimic%20first%0Agrounds%20object-centric%20movements%20from%20human%20videos%2C%20and%20learns%20skills%20using%0Ahierarchical%20constraint%20representations%2C%20facilitating%20the%20derivation%20of%20skills%0Awith%20fine-grained%20action%20levels%20from%20limited%20human%20videos.%20These%20skills%20are%0Arefined%20and%20updated%20through%20an%20iterative%20comparison%20strategy%2C%20enabling%0Aefficient%20adaptation%20to%20unseen%20environments.%20Our%20extensive%20experiments%20exhibit%0Athat%20our%20VLMimic%2C%20using%20only%205%20human%20videos%2C%20yields%20significant%20improvements%20of%0Aover%2027%25%20and%2021%25%20in%20RLBench%20and%20real-world%20manipulation%20tasks%2C%20and%20surpasses%0Abaselines%20by%20over%2037%25%20in%20long-horizon%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20927v2&entry.124074799=Read"},
{"title": "Multi-Class Textual-Inversion Secretly Yields a Semantic-Agnostic\n  Classifier", "author": "Kai Wang and Fei Yang and Bogdan Raducanu and Joost van de Weijer", "abstract": "  With the advent of large pre-trained vision-language models such as CLIP,\nprompt learning methods aim to enhance the transferability of the CLIP model.\nThey learn the prompt given few samples from the downstream task given the\nspecific class names as prior knowledge, which we term as semantic-aware\nclassification. However, in many realistic scenarios, we only have access to\nfew samples and knowledge of the class names (e.g., when considering instances\nof classes). This challenging scenario represents the semantic-agnostic\ndiscriminative case. Text-to-Image (T2I) personalization methods aim to adapt\nT2I models to unseen concepts by learning new tokens and endowing these tokens\nwith the capability of generating the learned concepts. These methods do not\nrequire knowledge of class names as a semantic-aware prior. Therefore, in this\npaper, we first explore Textual Inversion and reveal that the new concept\ntokens possess both generation and classification capabilities by regarding\neach category as a single concept. However, learning classifiers from\nsingle-concept textual inversion is limited since the learned tokens are\nsuboptimal for the discriminative tasks. To mitigate this issue, we propose\nMulti-Class textual inversion, which includes a discriminative regularization\nterm for the token updating process. Using this technique, our method MC-TI\nachieves stronger Semantic-Agnostic Classification while preserving the\ngeneration capability of these modifier tokens given only few samples per\ncategory. In the experiments, we extensively evaluate MC-TI on 12 datasets\ncovering various scenarios, which demonstrates that MC-TI achieves superior\nresults in terms of both classification and generation outcomes.\n", "link": "http://arxiv.org/abs/2410.22317v1", "date": "2024-10-29", "relevancy": 2.2203, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Class%20Textual-Inversion%20Secretly%20Yields%20a%20Semantic-Agnostic%0A%20%20Classifier&body=Title%3A%20Multi-Class%20Textual-Inversion%20Secretly%20Yields%20a%20Semantic-Agnostic%0A%20%20Classifier%0AAuthor%3A%20Kai%20Wang%20and%20Fei%20Yang%20and%20Bogdan%20Raducanu%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20%20%20With%20the%20advent%20of%20large%20pre-trained%20vision-language%20models%20such%20as%20CLIP%2C%0Aprompt%20learning%20methods%20aim%20to%20enhance%20the%20transferability%20of%20the%20CLIP%20model.%0AThey%20learn%20the%20prompt%20given%20few%20samples%20from%20the%20downstream%20task%20given%20the%0Aspecific%20class%20names%20as%20prior%20knowledge%2C%20which%20we%20term%20as%20semantic-aware%0Aclassification.%20However%2C%20in%20many%20realistic%20scenarios%2C%20we%20only%20have%20access%20to%0Afew%20samples%20and%20knowledge%20of%20the%20class%20names%20%28e.g.%2C%20when%20considering%20instances%0Aof%20classes%29.%20This%20challenging%20scenario%20represents%20the%20semantic-agnostic%0Adiscriminative%20case.%20Text-to-Image%20%28T2I%29%20personalization%20methods%20aim%20to%20adapt%0AT2I%20models%20to%20unseen%20concepts%20by%20learning%20new%20tokens%20and%20endowing%20these%20tokens%0Awith%20the%20capability%20of%20generating%20the%20learned%20concepts.%20These%20methods%20do%20not%0Arequire%20knowledge%20of%20class%20names%20as%20a%20semantic-aware%20prior.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20first%20explore%20Textual%20Inversion%20and%20reveal%20that%20the%20new%20concept%0Atokens%20possess%20both%20generation%20and%20classification%20capabilities%20by%20regarding%0Aeach%20category%20as%20a%20single%20concept.%20However%2C%20learning%20classifiers%20from%0Asingle-concept%20textual%20inversion%20is%20limited%20since%20the%20learned%20tokens%20are%0Asuboptimal%20for%20the%20discriminative%20tasks.%20To%20mitigate%20this%20issue%2C%20we%20propose%0AMulti-Class%20textual%20inversion%2C%20which%20includes%20a%20discriminative%20regularization%0Aterm%20for%20the%20token%20updating%20process.%20Using%20this%20technique%2C%20our%20method%20MC-TI%0Aachieves%20stronger%20Semantic-Agnostic%20Classification%20while%20preserving%20the%0Ageneration%20capability%20of%20these%20modifier%20tokens%20given%20only%20few%20samples%20per%0Acategory.%20In%20the%20experiments%2C%20we%20extensively%20evaluate%20MC-TI%20on%2012%20datasets%0Acovering%20various%20scenarios%2C%20which%20demonstrates%20that%20MC-TI%20achieves%20superior%0Aresults%20in%20terms%20of%20both%20classification%20and%20generation%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Class%2520Textual-Inversion%2520Secretly%2520Yields%2520a%2520Semantic-Agnostic%250A%2520%2520Classifier%26entry.906535625%3DKai%2520Wang%2520and%2520Fei%2520Yang%2520and%2520Bogdan%2520Raducanu%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520large%2520pre-trained%2520vision-language%2520models%2520such%2520as%2520CLIP%252C%250Aprompt%2520learning%2520methods%2520aim%2520to%2520enhance%2520the%2520transferability%2520of%2520the%2520CLIP%2520model.%250AThey%2520learn%2520the%2520prompt%2520given%2520few%2520samples%2520from%2520the%2520downstream%2520task%2520given%2520the%250Aspecific%2520class%2520names%2520as%2520prior%2520knowledge%252C%2520which%2520we%2520term%2520as%2520semantic-aware%250Aclassification.%2520However%252C%2520in%2520many%2520realistic%2520scenarios%252C%2520we%2520only%2520have%2520access%2520to%250Afew%2520samples%2520and%2520knowledge%2520of%2520the%2520class%2520names%2520%2528e.g.%252C%2520when%2520considering%2520instances%250Aof%2520classes%2529.%2520This%2520challenging%2520scenario%2520represents%2520the%2520semantic-agnostic%250Adiscriminative%2520case.%2520Text-to-Image%2520%2528T2I%2529%2520personalization%2520methods%2520aim%2520to%2520adapt%250AT2I%2520models%2520to%2520unseen%2520concepts%2520by%2520learning%2520new%2520tokens%2520and%2520endowing%2520these%2520tokens%250Awith%2520the%2520capability%2520of%2520generating%2520the%2520learned%2520concepts.%2520These%2520methods%2520do%2520not%250Arequire%2520knowledge%2520of%2520class%2520names%2520as%2520a%2520semantic-aware%2520prior.%2520Therefore%252C%2520in%2520this%250Apaper%252C%2520we%2520first%2520explore%2520Textual%2520Inversion%2520and%2520reveal%2520that%2520the%2520new%2520concept%250Atokens%2520possess%2520both%2520generation%2520and%2520classification%2520capabilities%2520by%2520regarding%250Aeach%2520category%2520as%2520a%2520single%2520concept.%2520However%252C%2520learning%2520classifiers%2520from%250Asingle-concept%2520textual%2520inversion%2520is%2520limited%2520since%2520the%2520learned%2520tokens%2520are%250Asuboptimal%2520for%2520the%2520discriminative%2520tasks.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520propose%250AMulti-Class%2520textual%2520inversion%252C%2520which%2520includes%2520a%2520discriminative%2520regularization%250Aterm%2520for%2520the%2520token%2520updating%2520process.%2520Using%2520this%2520technique%252C%2520our%2520method%2520MC-TI%250Aachieves%2520stronger%2520Semantic-Agnostic%2520Classification%2520while%2520preserving%2520the%250Ageneration%2520capability%2520of%2520these%2520modifier%2520tokens%2520given%2520only%2520few%2520samples%2520per%250Acategory.%2520In%2520the%2520experiments%252C%2520we%2520extensively%2520evaluate%2520MC-TI%2520on%252012%2520datasets%250Acovering%2520various%2520scenarios%252C%2520which%2520demonstrates%2520that%2520MC-TI%2520achieves%2520superior%250Aresults%2520in%2520terms%2520of%2520both%2520classification%2520and%2520generation%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Class%20Textual-Inversion%20Secretly%20Yields%20a%20Semantic-Agnostic%0A%20%20Classifier&entry.906535625=Kai%20Wang%20and%20Fei%20Yang%20and%20Bogdan%20Raducanu%20and%20Joost%20van%20de%20Weijer&entry.1292438233=%20%20With%20the%20advent%20of%20large%20pre-trained%20vision-language%20models%20such%20as%20CLIP%2C%0Aprompt%20learning%20methods%20aim%20to%20enhance%20the%20transferability%20of%20the%20CLIP%20model.%0AThey%20learn%20the%20prompt%20given%20few%20samples%20from%20the%20downstream%20task%20given%20the%0Aspecific%20class%20names%20as%20prior%20knowledge%2C%20which%20we%20term%20as%20semantic-aware%0Aclassification.%20However%2C%20in%20many%20realistic%20scenarios%2C%20we%20only%20have%20access%20to%0Afew%20samples%20and%20knowledge%20of%20the%20class%20names%20%28e.g.%2C%20when%20considering%20instances%0Aof%20classes%29.%20This%20challenging%20scenario%20represents%20the%20semantic-agnostic%0Adiscriminative%20case.%20Text-to-Image%20%28T2I%29%20personalization%20methods%20aim%20to%20adapt%0AT2I%20models%20to%20unseen%20concepts%20by%20learning%20new%20tokens%20and%20endowing%20these%20tokens%0Awith%20the%20capability%20of%20generating%20the%20learned%20concepts.%20These%20methods%20do%20not%0Arequire%20knowledge%20of%20class%20names%20as%20a%20semantic-aware%20prior.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20first%20explore%20Textual%20Inversion%20and%20reveal%20that%20the%20new%20concept%0Atokens%20possess%20both%20generation%20and%20classification%20capabilities%20by%20regarding%0Aeach%20category%20as%20a%20single%20concept.%20However%2C%20learning%20classifiers%20from%0Asingle-concept%20textual%20inversion%20is%20limited%20since%20the%20learned%20tokens%20are%0Asuboptimal%20for%20the%20discriminative%20tasks.%20To%20mitigate%20this%20issue%2C%20we%20propose%0AMulti-Class%20textual%20inversion%2C%20which%20includes%20a%20discriminative%20regularization%0Aterm%20for%20the%20token%20updating%20process.%20Using%20this%20technique%2C%20our%20method%20MC-TI%0Aachieves%20stronger%20Semantic-Agnostic%20Classification%20while%20preserving%20the%0Ageneration%20capability%20of%20these%20modifier%20tokens%20given%20only%20few%20samples%20per%0Acategory.%20In%20the%20experiments%2C%20we%20extensively%20evaluate%20MC-TI%20on%2012%20datasets%0Acovering%20various%20scenarios%2C%20which%20demonstrates%20that%20MC-TI%20achieves%20superior%0Aresults%20in%20terms%20of%20both%20classification%20and%20generation%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22317v1&entry.124074799=Read"},
{"title": "Active Event Alignment for Monocular Distance Estimation", "author": "Nan Cai and Pia Bideau", "abstract": "  Event cameras provide a natural and data efficient representation of visual\ninformation, motivating novel computational strategies towards extracting\nvisual information. Inspired by the biological vision system, we propose a\nbehavior driven approach for object-wise distance estimation from event camera\ndata. This behavior-driven method mimics how biological systems, like the human\neye, stabilize their view based on object distance: distant objects require\nminimal compensatory rotation to stay in focus, while nearby objects demand\ngreater adjustments to maintain alignment. This adaptive strategy leverages\nnatural stabilization behaviors to estimate relative distances effectively.\nUnlike traditional vision algorithms that estimate depth across the entire\nimage, our approach targets local depth estimation within a specific region of\ninterest. By aligning events within a small region, we estimate the angular\nvelocity required to stabilize the image motion. We demonstrate that, under\ncertain assumptions, the compensatory rotational flow is inversely proportional\nto the object's distance. The proposed approach achieves new state-of-the-art\naccuracy in distance estimation - a performance gain of 16% on EVIMO2. EVIMO2\nevent sequences comprise complex camera motion and substantial variance in\ndepth of static real world scenes.\n", "link": "http://arxiv.org/abs/2410.22280v1", "date": "2024-10-29", "relevancy": 2.1996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5526}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Event%20Alignment%20for%20Monocular%20Distance%20Estimation&body=Title%3A%20Active%20Event%20Alignment%20for%20Monocular%20Distance%20Estimation%0AAuthor%3A%20Nan%20Cai%20and%20Pia%20Bideau%0AAbstract%3A%20%20%20Event%20cameras%20provide%20a%20natural%20and%20data%20efficient%20representation%20of%20visual%0Ainformation%2C%20motivating%20novel%20computational%20strategies%20towards%20extracting%0Avisual%20information.%20Inspired%20by%20the%20biological%20vision%20system%2C%20we%20propose%20a%0Abehavior%20driven%20approach%20for%20object-wise%20distance%20estimation%20from%20event%20camera%0Adata.%20This%20behavior-driven%20method%20mimics%20how%20biological%20systems%2C%20like%20the%20human%0Aeye%2C%20stabilize%20their%20view%20based%20on%20object%20distance%3A%20distant%20objects%20require%0Aminimal%20compensatory%20rotation%20to%20stay%20in%20focus%2C%20while%20nearby%20objects%20demand%0Agreater%20adjustments%20to%20maintain%20alignment.%20This%20adaptive%20strategy%20leverages%0Anatural%20stabilization%20behaviors%20to%20estimate%20relative%20distances%20effectively.%0AUnlike%20traditional%20vision%20algorithms%20that%20estimate%20depth%20across%20the%20entire%0Aimage%2C%20our%20approach%20targets%20local%20depth%20estimation%20within%20a%20specific%20region%20of%0Ainterest.%20By%20aligning%20events%20within%20a%20small%20region%2C%20we%20estimate%20the%20angular%0Avelocity%20required%20to%20stabilize%20the%20image%20motion.%20We%20demonstrate%20that%2C%20under%0Acertain%20assumptions%2C%20the%20compensatory%20rotational%20flow%20is%20inversely%20proportional%0Ato%20the%20object%27s%20distance.%20The%20proposed%20approach%20achieves%20new%20state-of-the-art%0Aaccuracy%20in%20distance%20estimation%20-%20a%20performance%20gain%20of%2016%25%20on%20EVIMO2.%20EVIMO2%0Aevent%20sequences%20comprise%20complex%20camera%20motion%20and%20substantial%20variance%20in%0Adepth%20of%20static%20real%20world%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Event%2520Alignment%2520for%2520Monocular%2520Distance%2520Estimation%26entry.906535625%3DNan%2520Cai%2520and%2520Pia%2520Bideau%26entry.1292438233%3D%2520%2520Event%2520cameras%2520provide%2520a%2520natural%2520and%2520data%2520efficient%2520representation%2520of%2520visual%250Ainformation%252C%2520motivating%2520novel%2520computational%2520strategies%2520towards%2520extracting%250Avisual%2520information.%2520Inspired%2520by%2520the%2520biological%2520vision%2520system%252C%2520we%2520propose%2520a%250Abehavior%2520driven%2520approach%2520for%2520object-wise%2520distance%2520estimation%2520from%2520event%2520camera%250Adata.%2520This%2520behavior-driven%2520method%2520mimics%2520how%2520biological%2520systems%252C%2520like%2520the%2520human%250Aeye%252C%2520stabilize%2520their%2520view%2520based%2520on%2520object%2520distance%253A%2520distant%2520objects%2520require%250Aminimal%2520compensatory%2520rotation%2520to%2520stay%2520in%2520focus%252C%2520while%2520nearby%2520objects%2520demand%250Agreater%2520adjustments%2520to%2520maintain%2520alignment.%2520This%2520adaptive%2520strategy%2520leverages%250Anatural%2520stabilization%2520behaviors%2520to%2520estimate%2520relative%2520distances%2520effectively.%250AUnlike%2520traditional%2520vision%2520algorithms%2520that%2520estimate%2520depth%2520across%2520the%2520entire%250Aimage%252C%2520our%2520approach%2520targets%2520local%2520depth%2520estimation%2520within%2520a%2520specific%2520region%2520of%250Ainterest.%2520By%2520aligning%2520events%2520within%2520a%2520small%2520region%252C%2520we%2520estimate%2520the%2520angular%250Avelocity%2520required%2520to%2520stabilize%2520the%2520image%2520motion.%2520We%2520demonstrate%2520that%252C%2520under%250Acertain%2520assumptions%252C%2520the%2520compensatory%2520rotational%2520flow%2520is%2520inversely%2520proportional%250Ato%2520the%2520object%2527s%2520distance.%2520The%2520proposed%2520approach%2520achieves%2520new%2520state-of-the-art%250Aaccuracy%2520in%2520distance%2520estimation%2520-%2520a%2520performance%2520gain%2520of%252016%2525%2520on%2520EVIMO2.%2520EVIMO2%250Aevent%2520sequences%2520comprise%2520complex%2520camera%2520motion%2520and%2520substantial%2520variance%2520in%250Adepth%2520of%2520static%2520real%2520world%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Event%20Alignment%20for%20Monocular%20Distance%20Estimation&entry.906535625=Nan%20Cai%20and%20Pia%20Bideau&entry.1292438233=%20%20Event%20cameras%20provide%20a%20natural%20and%20data%20efficient%20representation%20of%20visual%0Ainformation%2C%20motivating%20novel%20computational%20strategies%20towards%20extracting%0Avisual%20information.%20Inspired%20by%20the%20biological%20vision%20system%2C%20we%20propose%20a%0Abehavior%20driven%20approach%20for%20object-wise%20distance%20estimation%20from%20event%20camera%0Adata.%20This%20behavior-driven%20method%20mimics%20how%20biological%20systems%2C%20like%20the%20human%0Aeye%2C%20stabilize%20their%20view%20based%20on%20object%20distance%3A%20distant%20objects%20require%0Aminimal%20compensatory%20rotation%20to%20stay%20in%20focus%2C%20while%20nearby%20objects%20demand%0Agreater%20adjustments%20to%20maintain%20alignment.%20This%20adaptive%20strategy%20leverages%0Anatural%20stabilization%20behaviors%20to%20estimate%20relative%20distances%20effectively.%0AUnlike%20traditional%20vision%20algorithms%20that%20estimate%20depth%20across%20the%20entire%0Aimage%2C%20our%20approach%20targets%20local%20depth%20estimation%20within%20a%20specific%20region%20of%0Ainterest.%20By%20aligning%20events%20within%20a%20small%20region%2C%20we%20estimate%20the%20angular%0Avelocity%20required%20to%20stabilize%20the%20image%20motion.%20We%20demonstrate%20that%2C%20under%0Acertain%20assumptions%2C%20the%20compensatory%20rotational%20flow%20is%20inversely%20proportional%0Ato%20the%20object%27s%20distance.%20The%20proposed%20approach%20achieves%20new%20state-of-the-art%0Aaccuracy%20in%20distance%20estimation%20-%20a%20performance%20gain%20of%2016%25%20on%20EVIMO2.%20EVIMO2%0Aevent%20sequences%20comprise%20complex%20camera%20motion%20and%20substantial%20variance%20in%0Adepth%20of%20static%20real%20world%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22280v1&entry.124074799=Read"},
{"title": "Learning Successor Features the Simple Way", "author": "Raymond Chua and Arna Ghosh and Christos Kaplanis and Blake A. Richards and Doina Precup", "abstract": "  In Deep Reinforcement Learning (RL), it is a challenge to learn\nrepresentations that do not exhibit catastrophic forgetting or interference in\nnon-stationary environments. Successor Features (SFs) offer a potential\nsolution to this challenge. However, canonical techniques for learning SFs from\npixel-level observations often lead to representation collapse, wherein\nrepresentations degenerate and fail to capture meaningful variations in the\ndata. More recent methods for learning SFs can avoid representation collapse,\nbut they often involve complex losses and multiple learning phases, reducing\ntheir efficiency. We introduce a novel, simple method for learning SFs directly\nfrom pixels. Our approach uses a combination of a Temporal-difference (TD) loss\nand a reward prediction loss, which together capture the basic mathematical\ndefinition of SFs. We show that our approach matches or outperforms existing SF\nlearning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for\nboth single and continual learning scenarios. As well, our technique is\nefficient, and can reach higher levels of performance in less time than other\napproaches. Our work provides a new, streamlined technique for learning SFs\ndirectly from pixel observations, with no pretraining required.\n", "link": "http://arxiv.org/abs/2410.22133v1", "date": "2024-10-29", "relevancy": 2.1545, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.554}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Successor%20Features%20the%20Simple%20Way&body=Title%3A%20Learning%20Successor%20Features%20the%20Simple%20Way%0AAuthor%3A%20Raymond%20Chua%20and%20Arna%20Ghosh%20and%20Christos%20Kaplanis%20and%20Blake%20A.%20Richards%20and%20Doina%20Precup%0AAbstract%3A%20%20%20In%20Deep%20Reinforcement%20Learning%20%28RL%29%2C%20it%20is%20a%20challenge%20to%20learn%0Arepresentations%20that%20do%20not%20exhibit%20catastrophic%20forgetting%20or%20interference%20in%0Anon-stationary%20environments.%20Successor%20Features%20%28SFs%29%20offer%20a%20potential%0Asolution%20to%20this%20challenge.%20However%2C%20canonical%20techniques%20for%20learning%20SFs%20from%0Apixel-level%20observations%20often%20lead%20to%20representation%20collapse%2C%20wherein%0Arepresentations%20degenerate%20and%20fail%20to%20capture%20meaningful%20variations%20in%20the%0Adata.%20More%20recent%20methods%20for%20learning%20SFs%20can%20avoid%20representation%20collapse%2C%0Abut%20they%20often%20involve%20complex%20losses%20and%20multiple%20learning%20phases%2C%20reducing%0Atheir%20efficiency.%20We%20introduce%20a%20novel%2C%20simple%20method%20for%20learning%20SFs%20directly%0Afrom%20pixels.%20Our%20approach%20uses%20a%20combination%20of%20a%20Temporal-difference%20%28TD%29%20loss%0Aand%20a%20reward%20prediction%20loss%2C%20which%20together%20capture%20the%20basic%20mathematical%0Adefinition%20of%20SFs.%20We%20show%20that%20our%20approach%20matches%20or%20outperforms%20existing%20SF%0Alearning%20techniques%20in%20both%202D%20%28Minigrid%29%2C%203D%20%28Miniworld%29%20mazes%20and%20Mujoco%2C%20for%0Aboth%20single%20and%20continual%20learning%20scenarios.%20As%20well%2C%20our%20technique%20is%0Aefficient%2C%20and%20can%20reach%20higher%20levels%20of%20performance%20in%20less%20time%20than%20other%0Aapproaches.%20Our%20work%20provides%20a%20new%2C%20streamlined%20technique%20for%20learning%20SFs%0Adirectly%20from%20pixel%20observations%2C%20with%20no%20pretraining%20required.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Successor%2520Features%2520the%2520Simple%2520Way%26entry.906535625%3DRaymond%2520Chua%2520and%2520Arna%2520Ghosh%2520and%2520Christos%2520Kaplanis%2520and%2520Blake%2520A.%2520Richards%2520and%2520Doina%2520Precup%26entry.1292438233%3D%2520%2520In%2520Deep%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520it%2520is%2520a%2520challenge%2520to%2520learn%250Arepresentations%2520that%2520do%2520not%2520exhibit%2520catastrophic%2520forgetting%2520or%2520interference%2520in%250Anon-stationary%2520environments.%2520Successor%2520Features%2520%2528SFs%2529%2520offer%2520a%2520potential%250Asolution%2520to%2520this%2520challenge.%2520However%252C%2520canonical%2520techniques%2520for%2520learning%2520SFs%2520from%250Apixel-level%2520observations%2520often%2520lead%2520to%2520representation%2520collapse%252C%2520wherein%250Arepresentations%2520degenerate%2520and%2520fail%2520to%2520capture%2520meaningful%2520variations%2520in%2520the%250Adata.%2520More%2520recent%2520methods%2520for%2520learning%2520SFs%2520can%2520avoid%2520representation%2520collapse%252C%250Abut%2520they%2520often%2520involve%2520complex%2520losses%2520and%2520multiple%2520learning%2520phases%252C%2520reducing%250Atheir%2520efficiency.%2520We%2520introduce%2520a%2520novel%252C%2520simple%2520method%2520for%2520learning%2520SFs%2520directly%250Afrom%2520pixels.%2520Our%2520approach%2520uses%2520a%2520combination%2520of%2520a%2520Temporal-difference%2520%2528TD%2529%2520loss%250Aand%2520a%2520reward%2520prediction%2520loss%252C%2520which%2520together%2520capture%2520the%2520basic%2520mathematical%250Adefinition%2520of%2520SFs.%2520We%2520show%2520that%2520our%2520approach%2520matches%2520or%2520outperforms%2520existing%2520SF%250Alearning%2520techniques%2520in%2520both%25202D%2520%2528Minigrid%2529%252C%25203D%2520%2528Miniworld%2529%2520mazes%2520and%2520Mujoco%252C%2520for%250Aboth%2520single%2520and%2520continual%2520learning%2520scenarios.%2520As%2520well%252C%2520our%2520technique%2520is%250Aefficient%252C%2520and%2520can%2520reach%2520higher%2520levels%2520of%2520performance%2520in%2520less%2520time%2520than%2520other%250Aapproaches.%2520Our%2520work%2520provides%2520a%2520new%252C%2520streamlined%2520technique%2520for%2520learning%2520SFs%250Adirectly%2520from%2520pixel%2520observations%252C%2520with%2520no%2520pretraining%2520required.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Successor%20Features%20the%20Simple%20Way&entry.906535625=Raymond%20Chua%20and%20Arna%20Ghosh%20and%20Christos%20Kaplanis%20and%20Blake%20A.%20Richards%20and%20Doina%20Precup&entry.1292438233=%20%20In%20Deep%20Reinforcement%20Learning%20%28RL%29%2C%20it%20is%20a%20challenge%20to%20learn%0Arepresentations%20that%20do%20not%20exhibit%20catastrophic%20forgetting%20or%20interference%20in%0Anon-stationary%20environments.%20Successor%20Features%20%28SFs%29%20offer%20a%20potential%0Asolution%20to%20this%20challenge.%20However%2C%20canonical%20techniques%20for%20learning%20SFs%20from%0Apixel-level%20observations%20often%20lead%20to%20representation%20collapse%2C%20wherein%0Arepresentations%20degenerate%20and%20fail%20to%20capture%20meaningful%20variations%20in%20the%0Adata.%20More%20recent%20methods%20for%20learning%20SFs%20can%20avoid%20representation%20collapse%2C%0Abut%20they%20often%20involve%20complex%20losses%20and%20multiple%20learning%20phases%2C%20reducing%0Atheir%20efficiency.%20We%20introduce%20a%20novel%2C%20simple%20method%20for%20learning%20SFs%20directly%0Afrom%20pixels.%20Our%20approach%20uses%20a%20combination%20of%20a%20Temporal-difference%20%28TD%29%20loss%0Aand%20a%20reward%20prediction%20loss%2C%20which%20together%20capture%20the%20basic%20mathematical%0Adefinition%20of%20SFs.%20We%20show%20that%20our%20approach%20matches%20or%20outperforms%20existing%20SF%0Alearning%20techniques%20in%20both%202D%20%28Minigrid%29%2C%203D%20%28Miniworld%29%20mazes%20and%20Mujoco%2C%20for%0Aboth%20single%20and%20continual%20learning%20scenarios.%20As%20well%2C%20our%20technique%20is%0Aefficient%2C%20and%20can%20reach%20higher%20levels%20of%20performance%20in%20less%20time%20than%20other%0Aapproaches.%20Our%20work%20provides%20a%20new%2C%20streamlined%20technique%20for%20learning%20SFs%0Adirectly%20from%20pixel%20observations%2C%20with%20no%20pretraining%20required.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22133v1&entry.124074799=Read"},
{"title": "FakeFormer: Efficient Vulnerability-Driven Transformers for\n  Generalisable Deepfake Detection", "author": "Dat Nguyen and Marcella Astrid and Enjie Ghorbel and Djamila Aouada", "abstract": "  Recently, Vision Transformers (ViTs) have achieved unprecedented\neffectiveness in the general domain of image classification. Nonetheless, these\nmodels remain underexplored in the field of deepfake detection, given their\nlower performance as compared to Convolution Neural Networks (CNNs) in that\nspecific context. In this paper, we start by investigating why plain ViT\narchitectures exhibit a suboptimal performance when dealing with the detection\nof facial forgeries. Our analysis reveals that, as compared to CNNs, ViT\nstruggles to model localized forgery artifacts that typically characterize\ndeepfakes. Based on this observation, we propose a deepfake detection framework\ncalled FakeFormer, which extends ViTs to enforce the extraction of subtle\ninconsistency-prone information. For that purpose, an explicit attention\nlearning guided by artifact-vulnerable patches and tailored to ViTs is\nintroduced. Extensive experiments are conducted on diverse well-known datasets,\nincluding FF++, Celeb-DF, WildDeepfake, DFD, DFDCP, and DFDC. The results show\nthat FakeFormer outperforms the state-of-the-art in terms of generalization and\ncomputational cost, without the need for large-scale training datasets. The\ncode is available at \\url{https://github.com/10Ring/FakeFormer}.\n", "link": "http://arxiv.org/abs/2410.21964v1", "date": "2024-10-29", "relevancy": 2.1537, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5493}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5319}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakeFormer%3A%20Efficient%20Vulnerability-Driven%20Transformers%20for%0A%20%20Generalisable%20Deepfake%20Detection&body=Title%3A%20FakeFormer%3A%20Efficient%20Vulnerability-Driven%20Transformers%20for%0A%20%20Generalisable%20Deepfake%20Detection%0AAuthor%3A%20Dat%20Nguyen%20and%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20unprecedented%0Aeffectiveness%20in%20the%20general%20domain%20of%20image%20classification.%20Nonetheless%2C%20these%0Amodels%20remain%20underexplored%20in%20the%20field%20of%20deepfake%20detection%2C%20given%20their%0Alower%20performance%20as%20compared%20to%20Convolution%20Neural%20Networks%20%28CNNs%29%20in%20that%0Aspecific%20context.%20In%20this%20paper%2C%20we%20start%20by%20investigating%20why%20plain%20ViT%0Aarchitectures%20exhibit%20a%20suboptimal%20performance%20when%20dealing%20with%20the%20detection%0Aof%20facial%20forgeries.%20Our%20analysis%20reveals%20that%2C%20as%20compared%20to%20CNNs%2C%20ViT%0Astruggles%20to%20model%20localized%20forgery%20artifacts%20that%20typically%20characterize%0Adeepfakes.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20deepfake%20detection%20framework%0Acalled%20FakeFormer%2C%20which%20extends%20ViTs%20to%20enforce%20the%20extraction%20of%20subtle%0Ainconsistency-prone%20information.%20For%20that%20purpose%2C%20an%20explicit%20attention%0Alearning%20guided%20by%20artifact-vulnerable%20patches%20and%20tailored%20to%20ViTs%20is%0Aintroduced.%20Extensive%20experiments%20are%20conducted%20on%20diverse%20well-known%20datasets%2C%0Aincluding%20FF%2B%2B%2C%20Celeb-DF%2C%20WildDeepfake%2C%20DFD%2C%20DFDCP%2C%20and%20DFDC.%20The%20results%20show%0Athat%20FakeFormer%20outperforms%20the%20state-of-the-art%20in%20terms%20of%20generalization%20and%0Acomputational%20cost%2C%20without%20the%20need%20for%20large-scale%20training%20datasets.%20The%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/10Ring/FakeFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakeFormer%253A%2520Efficient%2520Vulnerability-Driven%2520Transformers%2520for%250A%2520%2520Generalisable%2520Deepfake%2520Detection%26entry.906535625%3DDat%2520Nguyen%2520and%2520Marcella%2520Astrid%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Recently%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520unprecedented%250Aeffectiveness%2520in%2520the%2520general%2520domain%2520of%2520image%2520classification.%2520Nonetheless%252C%2520these%250Amodels%2520remain%2520underexplored%2520in%2520the%2520field%2520of%2520deepfake%2520detection%252C%2520given%2520their%250Alower%2520performance%2520as%2520compared%2520to%2520Convolution%2520Neural%2520Networks%2520%2528CNNs%2529%2520in%2520that%250Aspecific%2520context.%2520In%2520this%2520paper%252C%2520we%2520start%2520by%2520investigating%2520why%2520plain%2520ViT%250Aarchitectures%2520exhibit%2520a%2520suboptimal%2520performance%2520when%2520dealing%2520with%2520the%2520detection%250Aof%2520facial%2520forgeries.%2520Our%2520analysis%2520reveals%2520that%252C%2520as%2520compared%2520to%2520CNNs%252C%2520ViT%250Astruggles%2520to%2520model%2520localized%2520forgery%2520artifacts%2520that%2520typically%2520characterize%250Adeepfakes.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520deepfake%2520detection%2520framework%250Acalled%2520FakeFormer%252C%2520which%2520extends%2520ViTs%2520to%2520enforce%2520the%2520extraction%2520of%2520subtle%250Ainconsistency-prone%2520information.%2520For%2520that%2520purpose%252C%2520an%2520explicit%2520attention%250Alearning%2520guided%2520by%2520artifact-vulnerable%2520patches%2520and%2520tailored%2520to%2520ViTs%2520is%250Aintroduced.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520diverse%2520well-known%2520datasets%252C%250Aincluding%2520FF%252B%252B%252C%2520Celeb-DF%252C%2520WildDeepfake%252C%2520DFD%252C%2520DFDCP%252C%2520and%2520DFDC.%2520The%2520results%2520show%250Athat%2520FakeFormer%2520outperforms%2520the%2520state-of-the-art%2520in%2520terms%2520of%2520generalization%2520and%250Acomputational%2520cost%252C%2520without%2520the%2520need%2520for%2520large-scale%2520training%2520datasets.%2520The%250Acode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/10Ring/FakeFormer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakeFormer%3A%20Efficient%20Vulnerability-Driven%20Transformers%20for%0A%20%20Generalisable%20Deepfake%20Detection&entry.906535625=Dat%20Nguyen%20and%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20unprecedented%0Aeffectiveness%20in%20the%20general%20domain%20of%20image%20classification.%20Nonetheless%2C%20these%0Amodels%20remain%20underexplored%20in%20the%20field%20of%20deepfake%20detection%2C%20given%20their%0Alower%20performance%20as%20compared%20to%20Convolution%20Neural%20Networks%20%28CNNs%29%20in%20that%0Aspecific%20context.%20In%20this%20paper%2C%20we%20start%20by%20investigating%20why%20plain%20ViT%0Aarchitectures%20exhibit%20a%20suboptimal%20performance%20when%20dealing%20with%20the%20detection%0Aof%20facial%20forgeries.%20Our%20analysis%20reveals%20that%2C%20as%20compared%20to%20CNNs%2C%20ViT%0Astruggles%20to%20model%20localized%20forgery%20artifacts%20that%20typically%20characterize%0Adeepfakes.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20deepfake%20detection%20framework%0Acalled%20FakeFormer%2C%20which%20extends%20ViTs%20to%20enforce%20the%20extraction%20of%20subtle%0Ainconsistency-prone%20information.%20For%20that%20purpose%2C%20an%20explicit%20attention%0Alearning%20guided%20by%20artifact-vulnerable%20patches%20and%20tailored%20to%20ViTs%20is%0Aintroduced.%20Extensive%20experiments%20are%20conducted%20on%20diverse%20well-known%20datasets%2C%0Aincluding%20FF%2B%2B%2C%20Celeb-DF%2C%20WildDeepfake%2C%20DFD%2C%20DFDCP%2C%20and%20DFDC.%20The%20results%20show%0Athat%20FakeFormer%20outperforms%20the%20state-of-the-art%20in%20terms%20of%20generalization%20and%0Acomputational%20cost%2C%20without%20the%20need%20for%20large-scale%20training%20datasets.%20The%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/10Ring/FakeFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21964v1&entry.124074799=Read"},
{"title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct\n  Image Datasets", "author": "Adrian Iordache and Bogdan Alexe and Radu Tudor Ionescu", "abstract": "  We propose a novel teacher-student framework to distill knowledge from\nmultiple teachers trained on distinct datasets. Each teacher is first trained\nfrom scratch on its own dataset. Then, the teachers are combined into a joint\narchitecture, which fuses the features of all teachers at multiple\nrepresentation levels. The joint teacher architecture is fine-tuned on samples\nfrom all datasets, thus gathering useful generic information from all data\nsamples. Finally, we employ a multi-level feature distillation procedure to\ntransfer the knowledge to a student model for each of the considered datasets.\nWe conduct image classification experiments on seven benchmarks, and action\nrecognition experiments on three benchmarks. To illustrate the power of our\nfeature distillation procedure, the student architectures are chosen to be\nidentical to those of the individual teachers. To demonstrate the flexibility\nof our approach, we combine teachers with distinct architectures. We show that\nour novel Multi-Level Feature Distillation (MLFD) can significantly surpass\nequivalent architectures that are either trained on individual datasets, or\njointly trained on all datasets at once. Furthermore, we confirm that each step\nof the proposed training procedure is well motivated by a comprehensive\nablation study. We publicly release our code at\nhttps://github.com/AdrianIordache/MLFD.\n", "link": "http://arxiv.org/abs/2410.22184v1", "date": "2024-10-29", "relevancy": 2.1497, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5194}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Level%20Feature%20Distillation%20of%20Joint%20Teachers%20Trained%20on%20Distinct%0A%20%20Image%20Datasets&body=Title%3A%20Multi-Level%20Feature%20Distillation%20of%20Joint%20Teachers%20Trained%20on%20Distinct%0A%20%20Image%20Datasets%0AAuthor%3A%20Adrian%20Iordache%20and%20Bogdan%20Alexe%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20teacher-student%20framework%20to%20distill%20knowledge%20from%0Amultiple%20teachers%20trained%20on%20distinct%20datasets.%20Each%20teacher%20is%20first%20trained%0Afrom%20scratch%20on%20its%20own%20dataset.%20Then%2C%20the%20teachers%20are%20combined%20into%20a%20joint%0Aarchitecture%2C%20which%20fuses%20the%20features%20of%20all%20teachers%20at%20multiple%0Arepresentation%20levels.%20The%20joint%20teacher%20architecture%20is%20fine-tuned%20on%20samples%0Afrom%20all%20datasets%2C%20thus%20gathering%20useful%20generic%20information%20from%20all%20data%0Asamples.%20Finally%2C%20we%20employ%20a%20multi-level%20feature%20distillation%20procedure%20to%0Atransfer%20the%20knowledge%20to%20a%20student%20model%20for%20each%20of%20the%20considered%20datasets.%0AWe%20conduct%20image%20classification%20experiments%20on%20seven%20benchmarks%2C%20and%20action%0Arecognition%20experiments%20on%20three%20benchmarks.%20To%20illustrate%20the%20power%20of%20our%0Afeature%20distillation%20procedure%2C%20the%20student%20architectures%20are%20chosen%20to%20be%0Aidentical%20to%20those%20of%20the%20individual%20teachers.%20To%20demonstrate%20the%20flexibility%0Aof%20our%20approach%2C%20we%20combine%20teachers%20with%20distinct%20architectures.%20We%20show%20that%0Aour%20novel%20Multi-Level%20Feature%20Distillation%20%28MLFD%29%20can%20significantly%20surpass%0Aequivalent%20architectures%20that%20are%20either%20trained%20on%20individual%20datasets%2C%20or%0Ajointly%20trained%20on%20all%20datasets%20at%20once.%20Furthermore%2C%20we%20confirm%20that%20each%20step%0Aof%20the%20proposed%20training%20procedure%20is%20well%20motivated%20by%20a%20comprehensive%0Aablation%20study.%20We%20publicly%20release%20our%20code%20at%0Ahttps%3A//github.com/AdrianIordache/MLFD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Level%2520Feature%2520Distillation%2520of%2520Joint%2520Teachers%2520Trained%2520on%2520Distinct%250A%2520%2520Image%2520Datasets%26entry.906535625%3DAdrian%2520Iordache%2520and%2520Bogdan%2520Alexe%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520teacher-student%2520framework%2520to%2520distill%2520knowledge%2520from%250Amultiple%2520teachers%2520trained%2520on%2520distinct%2520datasets.%2520Each%2520teacher%2520is%2520first%2520trained%250Afrom%2520scratch%2520on%2520its%2520own%2520dataset.%2520Then%252C%2520the%2520teachers%2520are%2520combined%2520into%2520a%2520joint%250Aarchitecture%252C%2520which%2520fuses%2520the%2520features%2520of%2520all%2520teachers%2520at%2520multiple%250Arepresentation%2520levels.%2520The%2520joint%2520teacher%2520architecture%2520is%2520fine-tuned%2520on%2520samples%250Afrom%2520all%2520datasets%252C%2520thus%2520gathering%2520useful%2520generic%2520information%2520from%2520all%2520data%250Asamples.%2520Finally%252C%2520we%2520employ%2520a%2520multi-level%2520feature%2520distillation%2520procedure%2520to%250Atransfer%2520the%2520knowledge%2520to%2520a%2520student%2520model%2520for%2520each%2520of%2520the%2520considered%2520datasets.%250AWe%2520conduct%2520image%2520classification%2520experiments%2520on%2520seven%2520benchmarks%252C%2520and%2520action%250Arecognition%2520experiments%2520on%2520three%2520benchmarks.%2520To%2520illustrate%2520the%2520power%2520of%2520our%250Afeature%2520distillation%2520procedure%252C%2520the%2520student%2520architectures%2520are%2520chosen%2520to%2520be%250Aidentical%2520to%2520those%2520of%2520the%2520individual%2520teachers.%2520To%2520demonstrate%2520the%2520flexibility%250Aof%2520our%2520approach%252C%2520we%2520combine%2520teachers%2520with%2520distinct%2520architectures.%2520We%2520show%2520that%250Aour%2520novel%2520Multi-Level%2520Feature%2520Distillation%2520%2528MLFD%2529%2520can%2520significantly%2520surpass%250Aequivalent%2520architectures%2520that%2520are%2520either%2520trained%2520on%2520individual%2520datasets%252C%2520or%250Ajointly%2520trained%2520on%2520all%2520datasets%2520at%2520once.%2520Furthermore%252C%2520we%2520confirm%2520that%2520each%2520step%250Aof%2520the%2520proposed%2520training%2520procedure%2520is%2520well%2520motivated%2520by%2520a%2520comprehensive%250Aablation%2520study.%2520We%2520publicly%2520release%2520our%2520code%2520at%250Ahttps%253A//github.com/AdrianIordache/MLFD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Level%20Feature%20Distillation%20of%20Joint%20Teachers%20Trained%20on%20Distinct%0A%20%20Image%20Datasets&entry.906535625=Adrian%20Iordache%20and%20Bogdan%20Alexe%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20We%20propose%20a%20novel%20teacher-student%20framework%20to%20distill%20knowledge%20from%0Amultiple%20teachers%20trained%20on%20distinct%20datasets.%20Each%20teacher%20is%20first%20trained%0Afrom%20scratch%20on%20its%20own%20dataset.%20Then%2C%20the%20teachers%20are%20combined%20into%20a%20joint%0Aarchitecture%2C%20which%20fuses%20the%20features%20of%20all%20teachers%20at%20multiple%0Arepresentation%20levels.%20The%20joint%20teacher%20architecture%20is%20fine-tuned%20on%20samples%0Afrom%20all%20datasets%2C%20thus%20gathering%20useful%20generic%20information%20from%20all%20data%0Asamples.%20Finally%2C%20we%20employ%20a%20multi-level%20feature%20distillation%20procedure%20to%0Atransfer%20the%20knowledge%20to%20a%20student%20model%20for%20each%20of%20the%20considered%20datasets.%0AWe%20conduct%20image%20classification%20experiments%20on%20seven%20benchmarks%2C%20and%20action%0Arecognition%20experiments%20on%20three%20benchmarks.%20To%20illustrate%20the%20power%20of%20our%0Afeature%20distillation%20procedure%2C%20the%20student%20architectures%20are%20chosen%20to%20be%0Aidentical%20to%20those%20of%20the%20individual%20teachers.%20To%20demonstrate%20the%20flexibility%0Aof%20our%20approach%2C%20we%20combine%20teachers%20with%20distinct%20architectures.%20We%20show%20that%0Aour%20novel%20Multi-Level%20Feature%20Distillation%20%28MLFD%29%20can%20significantly%20surpass%0Aequivalent%20architectures%20that%20are%20either%20trained%20on%20individual%20datasets%2C%20or%0Ajointly%20trained%20on%20all%20datasets%20at%20once.%20Furthermore%2C%20we%20confirm%20that%20each%20step%0Aof%20the%20proposed%20training%20procedure%20is%20well%20motivated%20by%20a%20comprehensive%0Aablation%20study.%20We%20publicly%20release%20our%20code%20at%0Ahttps%3A//github.com/AdrianIordache/MLFD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22184v1&entry.124074799=Read"},
{"title": "Search Wide, Focus Deep: Automated Fetal Brain Extraction with Sparse\n  Training Data", "author": "Javid Dadashkarimi and Valeria Pena Trujillo and Camilo Jaimes and Lilla Z\u00f6llei and Malte Hoffmann", "abstract": "  Automated fetal brain extraction from full-uterus MRI is a challenging task\ndue to variable head sizes, orientations, complex anatomy, and prevalent\nartifacts. While deep-learning (DL) models trained on synthetic images have\nbeen successful in adult brain extraction, adapting these networks for fetal\nMRI is difficult due to the sparsity of labeled data, leading to increased\nfalse-positive predictions. To address this challenge, we propose a test-time\nstrategy that reduces false positives in networks trained on sparse, synthetic\nlabels. The approach uses a breadth-fine search (BFS) to identify a subvolume\nlikely to contain the fetal brain, followed by a deep-focused sliding window\n(DFS) search to refine the extraction, pooling predictions to minimize false\npositives. We train models at different window sizes using synthetic images\nderived from a small number of fetal brain label maps, augmented with random\ngeometric shapes. Each model is trained on diverse head positions and scales,\nincluding cases with partial or no brain tissue. Our framework matches\nstate-of-the-art brain extraction methods on clinical HASTE scans of\nthird-trimester fetuses and exceeds them by up to 5\\% in terms of Dice in the\nsecond trimester as well as EPI scans across both trimesters. Our results\ndemonstrate the utility of a sliding-window approach and combining predictions\nfrom several models trained on synthetic images, for improving brain-extraction\naccuracy by progressively refining regions of interest and minimizing the risk\nof missing brain mask slices or misidentifying other tissues as brain.\n", "link": "http://arxiv.org/abs/2410.20532v2", "date": "2024-10-29", "relevancy": 2.1416, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5537}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search%20Wide%2C%20Focus%20Deep%3A%20Automated%20Fetal%20Brain%20Extraction%20with%20Sparse%0A%20%20Training%20Data&body=Title%3A%20Search%20Wide%2C%20Focus%20Deep%3A%20Automated%20Fetal%20Brain%20Extraction%20with%20Sparse%0A%20%20Training%20Data%0AAuthor%3A%20Javid%20Dadashkarimi%20and%20Valeria%20Pena%20Trujillo%20and%20Camilo%20Jaimes%20and%20Lilla%20Z%C3%B6llei%20and%20Malte%20Hoffmann%0AAbstract%3A%20%20%20Automated%20fetal%20brain%20extraction%20from%20full-uterus%20MRI%20is%20a%20challenging%20task%0Adue%20to%20variable%20head%20sizes%2C%20orientations%2C%20complex%20anatomy%2C%20and%20prevalent%0Aartifacts.%20While%20deep-learning%20%28DL%29%20models%20trained%20on%20synthetic%20images%20have%0Abeen%20successful%20in%20adult%20brain%20extraction%2C%20adapting%20these%20networks%20for%20fetal%0AMRI%20is%20difficult%20due%20to%20the%20sparsity%20of%20labeled%20data%2C%20leading%20to%20increased%0Afalse-positive%20predictions.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20test-time%0Astrategy%20that%20reduces%20false%20positives%20in%20networks%20trained%20on%20sparse%2C%20synthetic%0Alabels.%20The%20approach%20uses%20a%20breadth-fine%20search%20%28BFS%29%20to%20identify%20a%20subvolume%0Alikely%20to%20contain%20the%20fetal%20brain%2C%20followed%20by%20a%20deep-focused%20sliding%20window%0A%28DFS%29%20search%20to%20refine%20the%20extraction%2C%20pooling%20predictions%20to%20minimize%20false%0Apositives.%20We%20train%20models%20at%20different%20window%20sizes%20using%20synthetic%20images%0Aderived%20from%20a%20small%20number%20of%20fetal%20brain%20label%20maps%2C%20augmented%20with%20random%0Ageometric%20shapes.%20Each%20model%20is%20trained%20on%20diverse%20head%20positions%20and%20scales%2C%0Aincluding%20cases%20with%20partial%20or%20no%20brain%20tissue.%20Our%20framework%20matches%0Astate-of-the-art%20brain%20extraction%20methods%20on%20clinical%20HASTE%20scans%20of%0Athird-trimester%20fetuses%20and%20exceeds%20them%20by%20up%20to%205%5C%25%20in%20terms%20of%20Dice%20in%20the%0Asecond%20trimester%20as%20well%20as%20EPI%20scans%20across%20both%20trimesters.%20Our%20results%0Ademonstrate%20the%20utility%20of%20a%20sliding-window%20approach%20and%20combining%20predictions%0Afrom%20several%20models%20trained%20on%20synthetic%20images%2C%20for%20improving%20brain-extraction%0Aaccuracy%20by%20progressively%20refining%20regions%20of%20interest%20and%20minimizing%20the%20risk%0Aof%20missing%20brain%20mask%20slices%20or%20misidentifying%20other%20tissues%20as%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20532v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch%2520Wide%252C%2520Focus%2520Deep%253A%2520Automated%2520Fetal%2520Brain%2520Extraction%2520with%2520Sparse%250A%2520%2520Training%2520Data%26entry.906535625%3DJavid%2520Dadashkarimi%2520and%2520Valeria%2520Pena%2520Trujillo%2520and%2520Camilo%2520Jaimes%2520and%2520Lilla%2520Z%25C3%25B6llei%2520and%2520Malte%2520Hoffmann%26entry.1292438233%3D%2520%2520Automated%2520fetal%2520brain%2520extraction%2520from%2520full-uterus%2520MRI%2520is%2520a%2520challenging%2520task%250Adue%2520to%2520variable%2520head%2520sizes%252C%2520orientations%252C%2520complex%2520anatomy%252C%2520and%2520prevalent%250Aartifacts.%2520While%2520deep-learning%2520%2528DL%2529%2520models%2520trained%2520on%2520synthetic%2520images%2520have%250Abeen%2520successful%2520in%2520adult%2520brain%2520extraction%252C%2520adapting%2520these%2520networks%2520for%2520fetal%250AMRI%2520is%2520difficult%2520due%2520to%2520the%2520sparsity%2520of%2520labeled%2520data%252C%2520leading%2520to%2520increased%250Afalse-positive%2520predictions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520test-time%250Astrategy%2520that%2520reduces%2520false%2520positives%2520in%2520networks%2520trained%2520on%2520sparse%252C%2520synthetic%250Alabels.%2520The%2520approach%2520uses%2520a%2520breadth-fine%2520search%2520%2528BFS%2529%2520to%2520identify%2520a%2520subvolume%250Alikely%2520to%2520contain%2520the%2520fetal%2520brain%252C%2520followed%2520by%2520a%2520deep-focused%2520sliding%2520window%250A%2528DFS%2529%2520search%2520to%2520refine%2520the%2520extraction%252C%2520pooling%2520predictions%2520to%2520minimize%2520false%250Apositives.%2520We%2520train%2520models%2520at%2520different%2520window%2520sizes%2520using%2520synthetic%2520images%250Aderived%2520from%2520a%2520small%2520number%2520of%2520fetal%2520brain%2520label%2520maps%252C%2520augmented%2520with%2520random%250Ageometric%2520shapes.%2520Each%2520model%2520is%2520trained%2520on%2520diverse%2520head%2520positions%2520and%2520scales%252C%250Aincluding%2520cases%2520with%2520partial%2520or%2520no%2520brain%2520tissue.%2520Our%2520framework%2520matches%250Astate-of-the-art%2520brain%2520extraction%2520methods%2520on%2520clinical%2520HASTE%2520scans%2520of%250Athird-trimester%2520fetuses%2520and%2520exceeds%2520them%2520by%2520up%2520to%25205%255C%2525%2520in%2520terms%2520of%2520Dice%2520in%2520the%250Asecond%2520trimester%2520as%2520well%2520as%2520EPI%2520scans%2520across%2520both%2520trimesters.%2520Our%2520results%250Ademonstrate%2520the%2520utility%2520of%2520a%2520sliding-window%2520approach%2520and%2520combining%2520predictions%250Afrom%2520several%2520models%2520trained%2520on%2520synthetic%2520images%252C%2520for%2520improving%2520brain-extraction%250Aaccuracy%2520by%2520progressively%2520refining%2520regions%2520of%2520interest%2520and%2520minimizing%2520the%2520risk%250Aof%2520missing%2520brain%2520mask%2520slices%2520or%2520misidentifying%2520other%2520tissues%2520as%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20532v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search%20Wide%2C%20Focus%20Deep%3A%20Automated%20Fetal%20Brain%20Extraction%20with%20Sparse%0A%20%20Training%20Data&entry.906535625=Javid%20Dadashkarimi%20and%20Valeria%20Pena%20Trujillo%20and%20Camilo%20Jaimes%20and%20Lilla%20Z%C3%B6llei%20and%20Malte%20Hoffmann&entry.1292438233=%20%20Automated%20fetal%20brain%20extraction%20from%20full-uterus%20MRI%20is%20a%20challenging%20task%0Adue%20to%20variable%20head%20sizes%2C%20orientations%2C%20complex%20anatomy%2C%20and%20prevalent%0Aartifacts.%20While%20deep-learning%20%28DL%29%20models%20trained%20on%20synthetic%20images%20have%0Abeen%20successful%20in%20adult%20brain%20extraction%2C%20adapting%20these%20networks%20for%20fetal%0AMRI%20is%20difficult%20due%20to%20the%20sparsity%20of%20labeled%20data%2C%20leading%20to%20increased%0Afalse-positive%20predictions.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20test-time%0Astrategy%20that%20reduces%20false%20positives%20in%20networks%20trained%20on%20sparse%2C%20synthetic%0Alabels.%20The%20approach%20uses%20a%20breadth-fine%20search%20%28BFS%29%20to%20identify%20a%20subvolume%0Alikely%20to%20contain%20the%20fetal%20brain%2C%20followed%20by%20a%20deep-focused%20sliding%20window%0A%28DFS%29%20search%20to%20refine%20the%20extraction%2C%20pooling%20predictions%20to%20minimize%20false%0Apositives.%20We%20train%20models%20at%20different%20window%20sizes%20using%20synthetic%20images%0Aderived%20from%20a%20small%20number%20of%20fetal%20brain%20label%20maps%2C%20augmented%20with%20random%0Ageometric%20shapes.%20Each%20model%20is%20trained%20on%20diverse%20head%20positions%20and%20scales%2C%0Aincluding%20cases%20with%20partial%20or%20no%20brain%20tissue.%20Our%20framework%20matches%0Astate-of-the-art%20brain%20extraction%20methods%20on%20clinical%20HASTE%20scans%20of%0Athird-trimester%20fetuses%20and%20exceeds%20them%20by%20up%20to%205%5C%25%20in%20terms%20of%20Dice%20in%20the%0Asecond%20trimester%20as%20well%20as%20EPI%20scans%20across%20both%20trimesters.%20Our%20results%0Ademonstrate%20the%20utility%20of%20a%20sliding-window%20approach%20and%20combining%20predictions%0Afrom%20several%20models%20trained%20on%20synthetic%20images%2C%20for%20improving%20brain-extraction%0Aaccuracy%20by%20progressively%20refining%20regions%20of%20interest%20and%20minimizing%20the%20risk%0Aof%20missing%20brain%20mask%20slices%20or%20misidentifying%20other%20tissues%20as%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20532v2&entry.124074799=Read"},
{"title": "LogSHIELD: A Graph-based Real-time Anomaly Detection Framework using\n  Frequency Analysis", "author": "Krishna Chandra Roy and Qian Chen", "abstract": "  Anomaly-based cyber threat detection using deep learning is on a constant\ngrowth in popularity for novel cyber-attack detection and forensics. A robust,\nefficient, and real-time threat detector in a large-scale operational\nenterprise network requires high accuracy, high fidelity, and a high throughput\nmodel to detect malicious activities. Traditional anomaly-based detection\nmodels, however, suffer from high computational overhead and low detection\naccuracy, making them unsuitable for real-time threat detection. In this work,\nwe propose LogSHIELD, a highly effective graph-based anomaly detection model in\nhost data. We present a real-time threat detection approach using\nfrequency-domain analysis of provenance graphs. To demonstrate the significance\nof graph-based frequency analysis we proposed two approaches. Approach-I uses a\nGraph Neural Network (GNN) LogGNN and approach-II performs frequency domain\nanalysis on graph node samples for graph embedding. Both approaches use a\nstatistical clustering algorithm for anomaly detection. The proposed models are\nevaluated using a large host log dataset consisting of 774M benign logs and\n375K malware logs. LogSHIELD explores the provenance graph to extract\ncontextual and causal relationships among logs, exposing abnormal activities.\nIt can detect stealthy and sophisticated attacks with over 98% average AUC and\nF1 scores. It significantly improves throughput, achieves an average detection\nlatency of 0.13 seconds, and outperforms state-of-the-art models in detection\ntime.\n", "link": "http://arxiv.org/abs/2410.21936v1", "date": "2024-10-29", "relevancy": 2.124, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4368}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4238}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogSHIELD%3A%20A%20Graph-based%20Real-time%20Anomaly%20Detection%20Framework%20using%0A%20%20Frequency%20Analysis&body=Title%3A%20LogSHIELD%3A%20A%20Graph-based%20Real-time%20Anomaly%20Detection%20Framework%20using%0A%20%20Frequency%20Analysis%0AAuthor%3A%20Krishna%20Chandra%20Roy%20and%20Qian%20Chen%0AAbstract%3A%20%20%20Anomaly-based%20cyber%20threat%20detection%20using%20deep%20learning%20is%20on%20a%20constant%0Agrowth%20in%20popularity%20for%20novel%20cyber-attack%20detection%20and%20forensics.%20A%20robust%2C%0Aefficient%2C%20and%20real-time%20threat%20detector%20in%20a%20large-scale%20operational%0Aenterprise%20network%20requires%20high%20accuracy%2C%20high%20fidelity%2C%20and%20a%20high%20throughput%0Amodel%20to%20detect%20malicious%20activities.%20Traditional%20anomaly-based%20detection%0Amodels%2C%20however%2C%20suffer%20from%20high%20computational%20overhead%20and%20low%20detection%0Aaccuracy%2C%20making%20them%20unsuitable%20for%20real-time%20threat%20detection.%20In%20this%20work%2C%0Awe%20propose%20LogSHIELD%2C%20a%20highly%20effective%20graph-based%20anomaly%20detection%20model%20in%0Ahost%20data.%20We%20present%20a%20real-time%20threat%20detection%20approach%20using%0Afrequency-domain%20analysis%20of%20provenance%20graphs.%20To%20demonstrate%20the%20significance%0Aof%20graph-based%20frequency%20analysis%20we%20proposed%20two%20approaches.%20Approach-I%20uses%20a%0AGraph%20Neural%20Network%20%28GNN%29%20LogGNN%20and%20approach-II%20performs%20frequency%20domain%0Aanalysis%20on%20graph%20node%20samples%20for%20graph%20embedding.%20Both%20approaches%20use%20a%0Astatistical%20clustering%20algorithm%20for%20anomaly%20detection.%20The%20proposed%20models%20are%0Aevaluated%20using%20a%20large%20host%20log%20dataset%20consisting%20of%20774M%20benign%20logs%20and%0A375K%20malware%20logs.%20LogSHIELD%20explores%20the%20provenance%20graph%20to%20extract%0Acontextual%20and%20causal%20relationships%20among%20logs%2C%20exposing%20abnormal%20activities.%0AIt%20can%20detect%20stealthy%20and%20sophisticated%20attacks%20with%20over%2098%25%20average%20AUC%20and%0AF1%20scores.%20It%20significantly%20improves%20throughput%2C%20achieves%20an%20average%20detection%0Alatency%20of%200.13%20seconds%2C%20and%20outperforms%20state-of-the-art%20models%20in%20detection%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogSHIELD%253A%2520A%2520Graph-based%2520Real-time%2520Anomaly%2520Detection%2520Framework%2520using%250A%2520%2520Frequency%2520Analysis%26entry.906535625%3DKrishna%2520Chandra%2520Roy%2520and%2520Qian%2520Chen%26entry.1292438233%3D%2520%2520Anomaly-based%2520cyber%2520threat%2520detection%2520using%2520deep%2520learning%2520is%2520on%2520a%2520constant%250Agrowth%2520in%2520popularity%2520for%2520novel%2520cyber-attack%2520detection%2520and%2520forensics.%2520A%2520robust%252C%250Aefficient%252C%2520and%2520real-time%2520threat%2520detector%2520in%2520a%2520large-scale%2520operational%250Aenterprise%2520network%2520requires%2520high%2520accuracy%252C%2520high%2520fidelity%252C%2520and%2520a%2520high%2520throughput%250Amodel%2520to%2520detect%2520malicious%2520activities.%2520Traditional%2520anomaly-based%2520detection%250Amodels%252C%2520however%252C%2520suffer%2520from%2520high%2520computational%2520overhead%2520and%2520low%2520detection%250Aaccuracy%252C%2520making%2520them%2520unsuitable%2520for%2520real-time%2520threat%2520detection.%2520In%2520this%2520work%252C%250Awe%2520propose%2520LogSHIELD%252C%2520a%2520highly%2520effective%2520graph-based%2520anomaly%2520detection%2520model%2520in%250Ahost%2520data.%2520We%2520present%2520a%2520real-time%2520threat%2520detection%2520approach%2520using%250Afrequency-domain%2520analysis%2520of%2520provenance%2520graphs.%2520To%2520demonstrate%2520the%2520significance%250Aof%2520graph-based%2520frequency%2520analysis%2520we%2520proposed%2520two%2520approaches.%2520Approach-I%2520uses%2520a%250AGraph%2520Neural%2520Network%2520%2528GNN%2529%2520LogGNN%2520and%2520approach-II%2520performs%2520frequency%2520domain%250Aanalysis%2520on%2520graph%2520node%2520samples%2520for%2520graph%2520embedding.%2520Both%2520approaches%2520use%2520a%250Astatistical%2520clustering%2520algorithm%2520for%2520anomaly%2520detection.%2520The%2520proposed%2520models%2520are%250Aevaluated%2520using%2520a%2520large%2520host%2520log%2520dataset%2520consisting%2520of%2520774M%2520benign%2520logs%2520and%250A375K%2520malware%2520logs.%2520LogSHIELD%2520explores%2520the%2520provenance%2520graph%2520to%2520extract%250Acontextual%2520and%2520causal%2520relationships%2520among%2520logs%252C%2520exposing%2520abnormal%2520activities.%250AIt%2520can%2520detect%2520stealthy%2520and%2520sophisticated%2520attacks%2520with%2520over%252098%2525%2520average%2520AUC%2520and%250AF1%2520scores.%2520It%2520significantly%2520improves%2520throughput%252C%2520achieves%2520an%2520average%2520detection%250Alatency%2520of%25200.13%2520seconds%252C%2520and%2520outperforms%2520state-of-the-art%2520models%2520in%2520detection%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogSHIELD%3A%20A%20Graph-based%20Real-time%20Anomaly%20Detection%20Framework%20using%0A%20%20Frequency%20Analysis&entry.906535625=Krishna%20Chandra%20Roy%20and%20Qian%20Chen&entry.1292438233=%20%20Anomaly-based%20cyber%20threat%20detection%20using%20deep%20learning%20is%20on%20a%20constant%0Agrowth%20in%20popularity%20for%20novel%20cyber-attack%20detection%20and%20forensics.%20A%20robust%2C%0Aefficient%2C%20and%20real-time%20threat%20detector%20in%20a%20large-scale%20operational%0Aenterprise%20network%20requires%20high%20accuracy%2C%20high%20fidelity%2C%20and%20a%20high%20throughput%0Amodel%20to%20detect%20malicious%20activities.%20Traditional%20anomaly-based%20detection%0Amodels%2C%20however%2C%20suffer%20from%20high%20computational%20overhead%20and%20low%20detection%0Aaccuracy%2C%20making%20them%20unsuitable%20for%20real-time%20threat%20detection.%20In%20this%20work%2C%0Awe%20propose%20LogSHIELD%2C%20a%20highly%20effective%20graph-based%20anomaly%20detection%20model%20in%0Ahost%20data.%20We%20present%20a%20real-time%20threat%20detection%20approach%20using%0Afrequency-domain%20analysis%20of%20provenance%20graphs.%20To%20demonstrate%20the%20significance%0Aof%20graph-based%20frequency%20analysis%20we%20proposed%20two%20approaches.%20Approach-I%20uses%20a%0AGraph%20Neural%20Network%20%28GNN%29%20LogGNN%20and%20approach-II%20performs%20frequency%20domain%0Aanalysis%20on%20graph%20node%20samples%20for%20graph%20embedding.%20Both%20approaches%20use%20a%0Astatistical%20clustering%20algorithm%20for%20anomaly%20detection.%20The%20proposed%20models%20are%0Aevaluated%20using%20a%20large%20host%20log%20dataset%20consisting%20of%20774M%20benign%20logs%20and%0A375K%20malware%20logs.%20LogSHIELD%20explores%20the%20provenance%20graph%20to%20extract%0Acontextual%20and%20causal%20relationships%20among%20logs%2C%20exposing%20abnormal%20activities.%0AIt%20can%20detect%20stealthy%20and%20sophisticated%20attacks%20with%20over%2098%25%20average%20AUC%20and%0AF1%20scores.%20It%20significantly%20improves%20throughput%2C%20achieves%20an%20average%20detection%0Alatency%20of%200.13%20seconds%2C%20and%20outperforms%20state-of-the-art%20models%20in%20detection%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21936v1&entry.124074799=Read"},
{"title": "Mechanisms and Computational Design of Multi-Modal End-Effector with\n  Force Sensing using Gated Networks", "author": "Yusuke Tanaka and Alvin Zhu and Richard Lin and Ankur Mehta and Dennis Hong", "abstract": "  In limbed robotics, end-effectors must serve dual functions, such as both\nfeet for locomotion and grippers for grasping, which presents design\nchallenges. This paper introduces a multi-modal end-effector capable of\ntransitioning between flat and line foot configurations while providing\ngrasping capabilities. MAGPIE integrates 8-axis force sensing using proposed\nmechanisms with hall effect sensors, enabling both contact and tactile force\nmeasurements. We present a computational design framework for our sensing\nmechanism that accounts for noise and interference, allowing for desired\nsensitivity and force ranges and generating ideal inverse models. The hardware\nimplementation of MAGPIE is validated through experiments, demonstrating its\ncapability as a foot and verifying the performance of the sensing mechanisms,\nideal models, and gated network-based models.\n", "link": "http://arxiv.org/abs/2410.17524v2", "date": "2024-10-29", "relevancy": 2.1168, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5957}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5273}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanisms%20and%20Computational%20Design%20of%20Multi-Modal%20End-Effector%20with%0A%20%20Force%20Sensing%20using%20Gated%20Networks&body=Title%3A%20Mechanisms%20and%20Computational%20Design%20of%20Multi-Modal%20End-Effector%20with%0A%20%20Force%20Sensing%20using%20Gated%20Networks%0AAuthor%3A%20Yusuke%20Tanaka%20and%20Alvin%20Zhu%20and%20Richard%20Lin%20and%20Ankur%20Mehta%20and%20Dennis%20Hong%0AAbstract%3A%20%20%20In%20limbed%20robotics%2C%20end-effectors%20must%20serve%20dual%20functions%2C%20such%20as%20both%0Afeet%20for%20locomotion%20and%20grippers%20for%20grasping%2C%20which%20presents%20design%0Achallenges.%20This%20paper%20introduces%20a%20multi-modal%20end-effector%20capable%20of%0Atransitioning%20between%20flat%20and%20line%20foot%20configurations%20while%20providing%0Agrasping%20capabilities.%20MAGPIE%20integrates%208-axis%20force%20sensing%20using%20proposed%0Amechanisms%20with%20hall%20effect%20sensors%2C%20enabling%20both%20contact%20and%20tactile%20force%0Ameasurements.%20We%20present%20a%20computational%20design%20framework%20for%20our%20sensing%0Amechanism%20that%20accounts%20for%20noise%20and%20interference%2C%20allowing%20for%20desired%0Asensitivity%20and%20force%20ranges%20and%20generating%20ideal%20inverse%20models.%20The%20hardware%0Aimplementation%20of%20MAGPIE%20is%20validated%20through%20experiments%2C%20demonstrating%20its%0Acapability%20as%20a%20foot%20and%20verifying%20the%20performance%20of%20the%20sensing%20mechanisms%2C%0Aideal%20models%2C%20and%20gated%20network-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanisms%2520and%2520Computational%2520Design%2520of%2520Multi-Modal%2520End-Effector%2520with%250A%2520%2520Force%2520Sensing%2520using%2520Gated%2520Networks%26entry.906535625%3DYusuke%2520Tanaka%2520and%2520Alvin%2520Zhu%2520and%2520Richard%2520Lin%2520and%2520Ankur%2520Mehta%2520and%2520Dennis%2520Hong%26entry.1292438233%3D%2520%2520In%2520limbed%2520robotics%252C%2520end-effectors%2520must%2520serve%2520dual%2520functions%252C%2520such%2520as%2520both%250Afeet%2520for%2520locomotion%2520and%2520grippers%2520for%2520grasping%252C%2520which%2520presents%2520design%250Achallenges.%2520This%2520paper%2520introduces%2520a%2520multi-modal%2520end-effector%2520capable%2520of%250Atransitioning%2520between%2520flat%2520and%2520line%2520foot%2520configurations%2520while%2520providing%250Agrasping%2520capabilities.%2520MAGPIE%2520integrates%25208-axis%2520force%2520sensing%2520using%2520proposed%250Amechanisms%2520with%2520hall%2520effect%2520sensors%252C%2520enabling%2520both%2520contact%2520and%2520tactile%2520force%250Ameasurements.%2520We%2520present%2520a%2520computational%2520design%2520framework%2520for%2520our%2520sensing%250Amechanism%2520that%2520accounts%2520for%2520noise%2520and%2520interference%252C%2520allowing%2520for%2520desired%250Asensitivity%2520and%2520force%2520ranges%2520and%2520generating%2520ideal%2520inverse%2520models.%2520The%2520hardware%250Aimplementation%2520of%2520MAGPIE%2520is%2520validated%2520through%2520experiments%252C%2520demonstrating%2520its%250Acapability%2520as%2520a%2520foot%2520and%2520verifying%2520the%2520performance%2520of%2520the%2520sensing%2520mechanisms%252C%250Aideal%2520models%252C%2520and%2520gated%2520network-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanisms%20and%20Computational%20Design%20of%20Multi-Modal%20End-Effector%20with%0A%20%20Force%20Sensing%20using%20Gated%20Networks&entry.906535625=Yusuke%20Tanaka%20and%20Alvin%20Zhu%20and%20Richard%20Lin%20and%20Ankur%20Mehta%20and%20Dennis%20Hong&entry.1292438233=%20%20In%20limbed%20robotics%2C%20end-effectors%20must%20serve%20dual%20functions%2C%20such%20as%20both%0Afeet%20for%20locomotion%20and%20grippers%20for%20grasping%2C%20which%20presents%20design%0Achallenges.%20This%20paper%20introduces%20a%20multi-modal%20end-effector%20capable%20of%0Atransitioning%20between%20flat%20and%20line%20foot%20configurations%20while%20providing%0Agrasping%20capabilities.%20MAGPIE%20integrates%208-axis%20force%20sensing%20using%20proposed%0Amechanisms%20with%20hall%20effect%20sensors%2C%20enabling%20both%20contact%20and%20tactile%20force%0Ameasurements.%20We%20present%20a%20computational%20design%20framework%20for%20our%20sensing%0Amechanism%20that%20accounts%20for%20noise%20and%20interference%2C%20allowing%20for%20desired%0Asensitivity%20and%20force%20ranges%20and%20generating%20ideal%20inverse%20models.%20The%20hardware%0Aimplementation%20of%20MAGPIE%20is%20validated%20through%20experiments%2C%20demonstrating%20its%0Acapability%20as%20a%20foot%20and%20verifying%20the%20performance%20of%20the%20sensing%20mechanisms%2C%0Aideal%20models%2C%20and%20gated%20network-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17524v2&entry.124074799=Read"},
{"title": "On the Robustness of Adversarial Training Against Uncertainty Attacks", "author": "Emanuele Ledda and Giovanni Scodeller and Daniele Angioni and Giorgio Piras and Antonio Emanuele Cin\u00e0 and Giorgio Fumera and Battista Biggio and Fabio Roli", "abstract": "  In learning problems, the noise inherent to the task at hand hinders the\npossibility to infer without a certain degree of uncertainty. Quantifying this\nuncertainty, regardless of its wide use, assumes high relevance for\nsecurity-sensitive applications. Within these scenarios, it becomes fundamental\nto guarantee good (i.e., trustworthy) uncertainty measures, which downstream\nmodules can securely employ to drive the final decision-making process.\nHowever, an attacker may be interested in forcing the system to produce either\n(i) highly uncertain outputs jeopardizing the system's availability or (ii) low\nuncertainty estimates, making the system accept uncertain samples that would\ninstead require a careful inspection (e.g., human intervention). Therefore, it\nbecomes fundamental to understand how to obtain robust uncertainty estimates\nagainst these kinds of attacks. In this work, we reveal both empirically and\ntheoretically that defending against adversarial examples, i.e., carefully\nperturbed samples that cause misclassification, additionally guarantees a more\nsecure, trustworthy uncertainty estimate under common attack scenarios without\nthe need for an ad-hoc defense strategy. To support our claims, we evaluate\nmultiple adversarial-robust models from the publicly available benchmark\nRobustBench on the CIFAR-10 and ImageNet datasets.\n", "link": "http://arxiv.org/abs/2410.21952v1", "date": "2024-10-29", "relevancy": 2.1096, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Adversarial%20Training%20Against%20Uncertainty%20Attacks&body=Title%3A%20On%20the%20Robustness%20of%20Adversarial%20Training%20Against%20Uncertainty%20Attacks%0AAuthor%3A%20Emanuele%20Ledda%20and%20Giovanni%20Scodeller%20and%20Daniele%20Angioni%20and%20Giorgio%20Piras%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Giorgio%20Fumera%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20In%20learning%20problems%2C%20the%20noise%20inherent%20to%20the%20task%20at%20hand%20hinders%20the%0Apossibility%20to%20infer%20without%20a%20certain%20degree%20of%20uncertainty.%20Quantifying%20this%0Auncertainty%2C%20regardless%20of%20its%20wide%20use%2C%20assumes%20high%20relevance%20for%0Asecurity-sensitive%20applications.%20Within%20these%20scenarios%2C%20it%20becomes%20fundamental%0Ato%20guarantee%20good%20%28i.e.%2C%20trustworthy%29%20uncertainty%20measures%2C%20which%20downstream%0Amodules%20can%20securely%20employ%20to%20drive%20the%20final%20decision-making%20process.%0AHowever%2C%20an%20attacker%20may%20be%20interested%20in%20forcing%20the%20system%20to%20produce%20either%0A%28i%29%20highly%20uncertain%20outputs%20jeopardizing%20the%20system%27s%20availability%20or%20%28ii%29%20low%0Auncertainty%20estimates%2C%20making%20the%20system%20accept%20uncertain%20samples%20that%20would%0Ainstead%20require%20a%20careful%20inspection%20%28e.g.%2C%20human%20intervention%29.%20Therefore%2C%20it%0Abecomes%20fundamental%20to%20understand%20how%20to%20obtain%20robust%20uncertainty%20estimates%0Aagainst%20these%20kinds%20of%20attacks.%20In%20this%20work%2C%20we%20reveal%20both%20empirically%20and%0Atheoretically%20that%20defending%20against%20adversarial%20examples%2C%20i.e.%2C%20carefully%0Aperturbed%20samples%20that%20cause%20misclassification%2C%20additionally%20guarantees%20a%20more%0Asecure%2C%20trustworthy%20uncertainty%20estimate%20under%20common%20attack%20scenarios%20without%0Athe%20need%20for%20an%20ad-hoc%20defense%20strategy.%20To%20support%20our%20claims%2C%20we%20evaluate%0Amultiple%20adversarial-robust%20models%20from%20the%20publicly%20available%20benchmark%0ARobustBench%20on%20the%20CIFAR-10%20and%20ImageNet%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520of%2520Adversarial%2520Training%2520Against%2520Uncertainty%2520Attacks%26entry.906535625%3DEmanuele%2520Ledda%2520and%2520Giovanni%2520Scodeller%2520and%2520Daniele%2520Angioni%2520and%2520Giorgio%2520Piras%2520and%2520Antonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Giorgio%2520Fumera%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520In%2520learning%2520problems%252C%2520the%2520noise%2520inherent%2520to%2520the%2520task%2520at%2520hand%2520hinders%2520the%250Apossibility%2520to%2520infer%2520without%2520a%2520certain%2520degree%2520of%2520uncertainty.%2520Quantifying%2520this%250Auncertainty%252C%2520regardless%2520of%2520its%2520wide%2520use%252C%2520assumes%2520high%2520relevance%2520for%250Asecurity-sensitive%2520applications.%2520Within%2520these%2520scenarios%252C%2520it%2520becomes%2520fundamental%250Ato%2520guarantee%2520good%2520%2528i.e.%252C%2520trustworthy%2529%2520uncertainty%2520measures%252C%2520which%2520downstream%250Amodules%2520can%2520securely%2520employ%2520to%2520drive%2520the%2520final%2520decision-making%2520process.%250AHowever%252C%2520an%2520attacker%2520may%2520be%2520interested%2520in%2520forcing%2520the%2520system%2520to%2520produce%2520either%250A%2528i%2529%2520highly%2520uncertain%2520outputs%2520jeopardizing%2520the%2520system%2527s%2520availability%2520or%2520%2528ii%2529%2520low%250Auncertainty%2520estimates%252C%2520making%2520the%2520system%2520accept%2520uncertain%2520samples%2520that%2520would%250Ainstead%2520require%2520a%2520careful%2520inspection%2520%2528e.g.%252C%2520human%2520intervention%2529.%2520Therefore%252C%2520it%250Abecomes%2520fundamental%2520to%2520understand%2520how%2520to%2520obtain%2520robust%2520uncertainty%2520estimates%250Aagainst%2520these%2520kinds%2520of%2520attacks.%2520In%2520this%2520work%252C%2520we%2520reveal%2520both%2520empirically%2520and%250Atheoretically%2520that%2520defending%2520against%2520adversarial%2520examples%252C%2520i.e.%252C%2520carefully%250Aperturbed%2520samples%2520that%2520cause%2520misclassification%252C%2520additionally%2520guarantees%2520a%2520more%250Asecure%252C%2520trustworthy%2520uncertainty%2520estimate%2520under%2520common%2520attack%2520scenarios%2520without%250Athe%2520need%2520for%2520an%2520ad-hoc%2520defense%2520strategy.%2520To%2520support%2520our%2520claims%252C%2520we%2520evaluate%250Amultiple%2520adversarial-robust%2520models%2520from%2520the%2520publicly%2520available%2520benchmark%250ARobustBench%2520on%2520the%2520CIFAR-10%2520and%2520ImageNet%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Adversarial%20Training%20Against%20Uncertainty%20Attacks&entry.906535625=Emanuele%20Ledda%20and%20Giovanni%20Scodeller%20and%20Daniele%20Angioni%20and%20Giorgio%20Piras%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Giorgio%20Fumera%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=%20%20In%20learning%20problems%2C%20the%20noise%20inherent%20to%20the%20task%20at%20hand%20hinders%20the%0Apossibility%20to%20infer%20without%20a%20certain%20degree%20of%20uncertainty.%20Quantifying%20this%0Auncertainty%2C%20regardless%20of%20its%20wide%20use%2C%20assumes%20high%20relevance%20for%0Asecurity-sensitive%20applications.%20Within%20these%20scenarios%2C%20it%20becomes%20fundamental%0Ato%20guarantee%20good%20%28i.e.%2C%20trustworthy%29%20uncertainty%20measures%2C%20which%20downstream%0Amodules%20can%20securely%20employ%20to%20drive%20the%20final%20decision-making%20process.%0AHowever%2C%20an%20attacker%20may%20be%20interested%20in%20forcing%20the%20system%20to%20produce%20either%0A%28i%29%20highly%20uncertain%20outputs%20jeopardizing%20the%20system%27s%20availability%20or%20%28ii%29%20low%0Auncertainty%20estimates%2C%20making%20the%20system%20accept%20uncertain%20samples%20that%20would%0Ainstead%20require%20a%20careful%20inspection%20%28e.g.%2C%20human%20intervention%29.%20Therefore%2C%20it%0Abecomes%20fundamental%20to%20understand%20how%20to%20obtain%20robust%20uncertainty%20estimates%0Aagainst%20these%20kinds%20of%20attacks.%20In%20this%20work%2C%20we%20reveal%20both%20empirically%20and%0Atheoretically%20that%20defending%20against%20adversarial%20examples%2C%20i.e.%2C%20carefully%0Aperturbed%20samples%20that%20cause%20misclassification%2C%20additionally%20guarantees%20a%20more%0Asecure%2C%20trustworthy%20uncertainty%20estimate%20under%20common%20attack%20scenarios%20without%0Athe%20need%20for%20an%20ad-hoc%20defense%20strategy.%20To%20support%20our%20claims%2C%20we%20evaluate%0Amultiple%20adversarial-robust%20models%20from%20the%20publicly%20available%20benchmark%0ARobustBench%20on%20the%20CIFAR-10%20and%20ImageNet%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21952v1&entry.124074799=Read"},
{"title": "Spatial-Aware Conformal Prediction for Trustworthy Hyperspectral Image\n  Classification", "author": "Kangdao Liu and Tianhao Sun and Hao Zeng and Yongshan Zhang and Chi-Man Pun and Chi-Man Vong", "abstract": "  Hyperspectral image (HSI) classification involves assigning unique labels to\neach pixel to identify various land cover categories. While deep classifiers\nhave achieved high predictive accuracy in this field, they lack the ability to\nrigorously quantify confidence in their predictions. Quantifying the certainty\nof model predictions is crucial for the safe usage of predictive models, and\nthis limitation restricts their application in critical contexts where the cost\nof prediction errors is significant. To support the safe deployment of HSI\nclassifiers, we first provide a theoretical proof establishing the validity of\nthe emerging uncertainty quantification technique, conformal prediction, in the\ncontext of HSI classification. We then propose a conformal procedure that\nequips any trained HSI classifier with trustworthy prediction sets, ensuring\nthat these sets include the true labels with a user-specified probability\n(e.g., 95\\%). Building on this foundation, we introduce Spatial-Aware Conformal\nPrediction (\\texttt{SACP}), a conformal prediction framework specifically\ndesigned for HSI data. This method integrates essential spatial information\ninherent in HSIs by aggregating the non-conformity scores of pixels with high\nspatial correlation, which effectively enhances the efficiency of prediction\nsets. Both theoretical and empirical results validate the effectiveness of our\nproposed approach. The source code is available at\n\\url{https://github.com/J4ckLiu/SACP}.\n", "link": "http://arxiv.org/abs/2409.01236v2", "date": "2024-10-29", "relevancy": 2.106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5343}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Aware%20Conformal%20Prediction%20for%20Trustworthy%20Hyperspectral%20Image%0A%20%20Classification&body=Title%3A%20Spatial-Aware%20Conformal%20Prediction%20for%20Trustworthy%20Hyperspectral%20Image%0A%20%20Classification%0AAuthor%3A%20Kangdao%20Liu%20and%20Tianhao%20Sun%20and%20Hao%20Zeng%20and%20Yongshan%20Zhang%20and%20Chi-Man%20Pun%20and%20Chi-Man%20Vong%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20classification%20involves%20assigning%20unique%20labels%20to%0Aeach%20pixel%20to%20identify%20various%20land%20cover%20categories.%20While%20deep%20classifiers%0Ahave%20achieved%20high%20predictive%20accuracy%20in%20this%20field%2C%20they%20lack%20the%20ability%20to%0Arigorously%20quantify%20confidence%20in%20their%20predictions.%20Quantifying%20the%20certainty%0Aof%20model%20predictions%20is%20crucial%20for%20the%20safe%20usage%20of%20predictive%20models%2C%20and%0Athis%20limitation%20restricts%20their%20application%20in%20critical%20contexts%20where%20the%20cost%0Aof%20prediction%20errors%20is%20significant.%20To%20support%20the%20safe%20deployment%20of%20HSI%0Aclassifiers%2C%20we%20first%20provide%20a%20theoretical%20proof%20establishing%20the%20validity%20of%0Athe%20emerging%20uncertainty%20quantification%20technique%2C%20conformal%20prediction%2C%20in%20the%0Acontext%20of%20HSI%20classification.%20We%20then%20propose%20a%20conformal%20procedure%20that%0Aequips%20any%20trained%20HSI%20classifier%20with%20trustworthy%20prediction%20sets%2C%20ensuring%0Athat%20these%20sets%20include%20the%20true%20labels%20with%20a%20user-specified%20probability%0A%28e.g.%2C%2095%5C%25%29.%20Building%20on%20this%20foundation%2C%20we%20introduce%20Spatial-Aware%20Conformal%0APrediction%20%28%5Ctexttt%7BSACP%7D%29%2C%20a%20conformal%20prediction%20framework%20specifically%0Adesigned%20for%20HSI%20data.%20This%20method%20integrates%20essential%20spatial%20information%0Ainherent%20in%20HSIs%20by%20aggregating%20the%20non-conformity%20scores%20of%20pixels%20with%20high%0Aspatial%20correlation%2C%20which%20effectively%20enhances%20the%20efficiency%20of%20prediction%0Asets.%20Both%20theoretical%20and%20empirical%20results%20validate%20the%20effectiveness%20of%20our%0Aproposed%20approach.%20The%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/J4ckLiu/SACP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01236v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Aware%2520Conformal%2520Prediction%2520for%2520Trustworthy%2520Hyperspectral%2520Image%250A%2520%2520Classification%26entry.906535625%3DKangdao%2520Liu%2520and%2520Tianhao%2520Sun%2520and%2520Hao%2520Zeng%2520and%2520Yongshan%2520Zhang%2520and%2520Chi-Man%2520Pun%2520and%2520Chi-Man%2520Vong%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520involves%2520assigning%2520unique%2520labels%2520to%250Aeach%2520pixel%2520to%2520identify%2520various%2520land%2520cover%2520categories.%2520While%2520deep%2520classifiers%250Ahave%2520achieved%2520high%2520predictive%2520accuracy%2520in%2520this%2520field%252C%2520they%2520lack%2520the%2520ability%2520to%250Arigorously%2520quantify%2520confidence%2520in%2520their%2520predictions.%2520Quantifying%2520the%2520certainty%250Aof%2520model%2520predictions%2520is%2520crucial%2520for%2520the%2520safe%2520usage%2520of%2520predictive%2520models%252C%2520and%250Athis%2520limitation%2520restricts%2520their%2520application%2520in%2520critical%2520contexts%2520where%2520the%2520cost%250Aof%2520prediction%2520errors%2520is%2520significant.%2520To%2520support%2520the%2520safe%2520deployment%2520of%2520HSI%250Aclassifiers%252C%2520we%2520first%2520provide%2520a%2520theoretical%2520proof%2520establishing%2520the%2520validity%2520of%250Athe%2520emerging%2520uncertainty%2520quantification%2520technique%252C%2520conformal%2520prediction%252C%2520in%2520the%250Acontext%2520of%2520HSI%2520classification.%2520We%2520then%2520propose%2520a%2520conformal%2520procedure%2520that%250Aequips%2520any%2520trained%2520HSI%2520classifier%2520with%2520trustworthy%2520prediction%2520sets%252C%2520ensuring%250Athat%2520these%2520sets%2520include%2520the%2520true%2520labels%2520with%2520a%2520user-specified%2520probability%250A%2528e.g.%252C%252095%255C%2525%2529.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520introduce%2520Spatial-Aware%2520Conformal%250APrediction%2520%2528%255Ctexttt%257BSACP%257D%2529%252C%2520a%2520conformal%2520prediction%2520framework%2520specifically%250Adesigned%2520for%2520HSI%2520data.%2520This%2520method%2520integrates%2520essential%2520spatial%2520information%250Ainherent%2520in%2520HSIs%2520by%2520aggregating%2520the%2520non-conformity%2520scores%2520of%2520pixels%2520with%2520high%250Aspatial%2520correlation%252C%2520which%2520effectively%2520enhances%2520the%2520efficiency%2520of%2520prediction%250Asets.%2520Both%2520theoretical%2520and%2520empirical%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520approach.%2520The%2520source%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/J4ckLiu/SACP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01236v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Aware%20Conformal%20Prediction%20for%20Trustworthy%20Hyperspectral%20Image%0A%20%20Classification&entry.906535625=Kangdao%20Liu%20and%20Tianhao%20Sun%20and%20Hao%20Zeng%20and%20Yongshan%20Zhang%20and%20Chi-Man%20Pun%20and%20Chi-Man%20Vong&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20classification%20involves%20assigning%20unique%20labels%20to%0Aeach%20pixel%20to%20identify%20various%20land%20cover%20categories.%20While%20deep%20classifiers%0Ahave%20achieved%20high%20predictive%20accuracy%20in%20this%20field%2C%20they%20lack%20the%20ability%20to%0Arigorously%20quantify%20confidence%20in%20their%20predictions.%20Quantifying%20the%20certainty%0Aof%20model%20predictions%20is%20crucial%20for%20the%20safe%20usage%20of%20predictive%20models%2C%20and%0Athis%20limitation%20restricts%20their%20application%20in%20critical%20contexts%20where%20the%20cost%0Aof%20prediction%20errors%20is%20significant.%20To%20support%20the%20safe%20deployment%20of%20HSI%0Aclassifiers%2C%20we%20first%20provide%20a%20theoretical%20proof%20establishing%20the%20validity%20of%0Athe%20emerging%20uncertainty%20quantification%20technique%2C%20conformal%20prediction%2C%20in%20the%0Acontext%20of%20HSI%20classification.%20We%20then%20propose%20a%20conformal%20procedure%20that%0Aequips%20any%20trained%20HSI%20classifier%20with%20trustworthy%20prediction%20sets%2C%20ensuring%0Athat%20these%20sets%20include%20the%20true%20labels%20with%20a%20user-specified%20probability%0A%28e.g.%2C%2095%5C%25%29.%20Building%20on%20this%20foundation%2C%20we%20introduce%20Spatial-Aware%20Conformal%0APrediction%20%28%5Ctexttt%7BSACP%7D%29%2C%20a%20conformal%20prediction%20framework%20specifically%0Adesigned%20for%20HSI%20data.%20This%20method%20integrates%20essential%20spatial%20information%0Ainherent%20in%20HSIs%20by%20aggregating%20the%20non-conformity%20scores%20of%20pixels%20with%20high%0Aspatial%20correlation%2C%20which%20effectively%20enhances%20the%20efficiency%20of%20prediction%0Asets.%20Both%20theoretical%20and%20empirical%20results%20validate%20the%20effectiveness%20of%20our%0Aproposed%20approach.%20The%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/J4ckLiu/SACP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01236v2&entry.124074799=Read"},
{"title": "Structured Analysis and Comparison of Alphabets in Historical\n  Handwritten Ciphers", "author": "Mart\u00edn M\u00e9ndez and Pau Torras and Adri\u00e0 Molina and Jialuo Chen and Oriol Ramos-Terrades and Alicia Forn\u00e9s", "abstract": "  Historical ciphered manuscripts are documents that were typically used in\nsensitive communications within military and diplomatic contexts or among\nmembers of secret societies. These secret messages were concealed by inventing\na method of writing employing symbols from diverse sources such as digits,\nalchemy signs and Latin or Greek characters. When studying a new, unseen\ncipher, the automatic search and grouping of ciphers with a similar alphabet\ncan aid the scholar in its transcription and cryptanalysis because it indicates\na probability that the underlying cipher is similar. In this study, we address\nthis need by proposing the CSI metric, a novel way of comparing pairs of\nciphered documents. We assess their effectiveness in an unsupervised clustering\nscenario utilising visual features, including SIFT, pre-trained learnt\nembeddings, and OCR descriptors.\n", "link": "http://arxiv.org/abs/2410.21913v1", "date": "2024-10-29", "relevancy": 2.0923, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4303}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Analysis%20and%20Comparison%20of%20Alphabets%20in%20Historical%0A%20%20Handwritten%20Ciphers&body=Title%3A%20Structured%20Analysis%20and%20Comparison%20of%20Alphabets%20in%20Historical%0A%20%20Handwritten%20Ciphers%0AAuthor%3A%20Mart%C3%ADn%20M%C3%A9ndez%20and%20Pau%20Torras%20and%20Adri%C3%A0%20Molina%20and%20Jialuo%20Chen%20and%20Oriol%20Ramos-Terrades%20and%20Alicia%20Forn%C3%A9s%0AAbstract%3A%20%20%20Historical%20ciphered%20manuscripts%20are%20documents%20that%20were%20typically%20used%20in%0Asensitive%20communications%20within%20military%20and%20diplomatic%20contexts%20or%20among%0Amembers%20of%20secret%20societies.%20These%20secret%20messages%20were%20concealed%20by%20inventing%0Aa%20method%20of%20writing%20employing%20symbols%20from%20diverse%20sources%20such%20as%20digits%2C%0Aalchemy%20signs%20and%20Latin%20or%20Greek%20characters.%20When%20studying%20a%20new%2C%20unseen%0Acipher%2C%20the%20automatic%20search%20and%20grouping%20of%20ciphers%20with%20a%20similar%20alphabet%0Acan%20aid%20the%20scholar%20in%20its%20transcription%20and%20cryptanalysis%20because%20it%20indicates%0Aa%20probability%20that%20the%20underlying%20cipher%20is%20similar.%20In%20this%20study%2C%20we%20address%0Athis%20need%20by%20proposing%20the%20CSI%20metric%2C%20a%20novel%20way%20of%20comparing%20pairs%20of%0Aciphered%20documents.%20We%20assess%20their%20effectiveness%20in%20an%20unsupervised%20clustering%0Ascenario%20utilising%20visual%20features%2C%20including%20SIFT%2C%20pre-trained%20learnt%0Aembeddings%2C%20and%20OCR%20descriptors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Analysis%2520and%2520Comparison%2520of%2520Alphabets%2520in%2520Historical%250A%2520%2520Handwritten%2520Ciphers%26entry.906535625%3DMart%25C3%25ADn%2520M%25C3%25A9ndez%2520and%2520Pau%2520Torras%2520and%2520Adri%25C3%25A0%2520Molina%2520and%2520Jialuo%2520Chen%2520and%2520Oriol%2520Ramos-Terrades%2520and%2520Alicia%2520Forn%25C3%25A9s%26entry.1292438233%3D%2520%2520Historical%2520ciphered%2520manuscripts%2520are%2520documents%2520that%2520were%2520typically%2520used%2520in%250Asensitive%2520communications%2520within%2520military%2520and%2520diplomatic%2520contexts%2520or%2520among%250Amembers%2520of%2520secret%2520societies.%2520These%2520secret%2520messages%2520were%2520concealed%2520by%2520inventing%250Aa%2520method%2520of%2520writing%2520employing%2520symbols%2520from%2520diverse%2520sources%2520such%2520as%2520digits%252C%250Aalchemy%2520signs%2520and%2520Latin%2520or%2520Greek%2520characters.%2520When%2520studying%2520a%2520new%252C%2520unseen%250Acipher%252C%2520the%2520automatic%2520search%2520and%2520grouping%2520of%2520ciphers%2520with%2520a%2520similar%2520alphabet%250Acan%2520aid%2520the%2520scholar%2520in%2520its%2520transcription%2520and%2520cryptanalysis%2520because%2520it%2520indicates%250Aa%2520probability%2520that%2520the%2520underlying%2520cipher%2520is%2520similar.%2520In%2520this%2520study%252C%2520we%2520address%250Athis%2520need%2520by%2520proposing%2520the%2520CSI%2520metric%252C%2520a%2520novel%2520way%2520of%2520comparing%2520pairs%2520of%250Aciphered%2520documents.%2520We%2520assess%2520their%2520effectiveness%2520in%2520an%2520unsupervised%2520clustering%250Ascenario%2520utilising%2520visual%2520features%252C%2520including%2520SIFT%252C%2520pre-trained%2520learnt%250Aembeddings%252C%2520and%2520OCR%2520descriptors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Analysis%20and%20Comparison%20of%20Alphabets%20in%20Historical%0A%20%20Handwritten%20Ciphers&entry.906535625=Mart%C3%ADn%20M%C3%A9ndez%20and%20Pau%20Torras%20and%20Adri%C3%A0%20Molina%20and%20Jialuo%20Chen%20and%20Oriol%20Ramos-Terrades%20and%20Alicia%20Forn%C3%A9s&entry.1292438233=%20%20Historical%20ciphered%20manuscripts%20are%20documents%20that%20were%20typically%20used%20in%0Asensitive%20communications%20within%20military%20and%20diplomatic%20contexts%20or%20among%0Amembers%20of%20secret%20societies.%20These%20secret%20messages%20were%20concealed%20by%20inventing%0Aa%20method%20of%20writing%20employing%20symbols%20from%20diverse%20sources%20such%20as%20digits%2C%0Aalchemy%20signs%20and%20Latin%20or%20Greek%20characters.%20When%20studying%20a%20new%2C%20unseen%0Acipher%2C%20the%20automatic%20search%20and%20grouping%20of%20ciphers%20with%20a%20similar%20alphabet%0Acan%20aid%20the%20scholar%20in%20its%20transcription%20and%20cryptanalysis%20because%20it%20indicates%0Aa%20probability%20that%20the%20underlying%20cipher%20is%20similar.%20In%20this%20study%2C%20we%20address%0Athis%20need%20by%20proposing%20the%20CSI%20metric%2C%20a%20novel%20way%20of%20comparing%20pairs%20of%0Aciphered%20documents.%20We%20assess%20their%20effectiveness%20in%20an%20unsupervised%20clustering%0Ascenario%20utilising%20visual%20features%2C%20including%20SIFT%2C%20pre-trained%20learnt%0Aembeddings%2C%20and%20OCR%20descriptors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21913v1&entry.124074799=Read"},
{"title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural\n  Networks", "author": "Marco Paul E. Apolinario and Kaushik Roy", "abstract": "  Spiking Neural Networks (SNNs) are biologically plausible models that have\nbeen identified as potentially apt for deploying energy-efficient intelligence\nat the edge, particularly for sequential learning tasks. However, training of\nSNNs poses significant challenges due to the necessity for precise temporal and\nspatial credit assignment. Back-propagation through time (BPTT) algorithm,\nwhilst the most widely used method for addressing these issues, incurs high\ncomputational cost due to its temporal dependency. In this work, we propose\nS-TLLR, a novel three-factor temporal local learning rule inspired by the\nSpike-Timing Dependent Plasticity (STDP) mechanism, aimed at training deep SNNs\non event-based learning tasks. Furthermore, S-TLLR is designed to have low\nmemory and time complexities, which are independent of the number of time\nsteps, rendering it suitable for online learning on low-power edge devices. To\ndemonstrate the scalability of our proposed method, we have conducted extensive\nevaluations on event-based datasets spanning a wide range of applications, such\nas image and gesture recognition, audio classification, and optical flow\nestimation. In all the experiments, S-TLLR achieved high accuracy, comparable\nto BPTT, with a reduction in memory between $5-50\\times$ and\nmultiply-accumulate (MAC) operations between $1.3-6.6\\times$.\n", "link": "http://arxiv.org/abs/2306.15220v4", "date": "2024-10-29", "relevancy": 2.0876, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5245}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5243}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S-TLLR%3A%20STDP-inspired%20Temporal%20Local%20Learning%20Rule%20for%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20S-TLLR%3A%20STDP-inspired%20Temporal%20Local%20Learning%20Rule%20for%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Marco%20Paul%20E.%20Apolinario%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20biologically%20plausible%20models%20that%20have%0Abeen%20identified%20as%20potentially%20apt%20for%20deploying%20energy-efficient%20intelligence%0Aat%20the%20edge%2C%20particularly%20for%20sequential%20learning%20tasks.%20However%2C%20training%20of%0ASNNs%20poses%20significant%20challenges%20due%20to%20the%20necessity%20for%20precise%20temporal%20and%0Aspatial%20credit%20assignment.%20Back-propagation%20through%20time%20%28BPTT%29%20algorithm%2C%0Awhilst%20the%20most%20widely%20used%20method%20for%20addressing%20these%20issues%2C%20incurs%20high%0Acomputational%20cost%20due%20to%20its%20temporal%20dependency.%20In%20this%20work%2C%20we%20propose%0AS-TLLR%2C%20a%20novel%20three-factor%20temporal%20local%20learning%20rule%20inspired%20by%20the%0ASpike-Timing%20Dependent%20Plasticity%20%28STDP%29%20mechanism%2C%20aimed%20at%20training%20deep%20SNNs%0Aon%20event-based%20learning%20tasks.%20Furthermore%2C%20S-TLLR%20is%20designed%20to%20have%20low%0Amemory%20and%20time%20complexities%2C%20which%20are%20independent%20of%20the%20number%20of%20time%0Asteps%2C%20rendering%20it%20suitable%20for%20online%20learning%20on%20low-power%20edge%20devices.%20To%0Ademonstrate%20the%20scalability%20of%20our%20proposed%20method%2C%20we%20have%20conducted%20extensive%0Aevaluations%20on%20event-based%20datasets%20spanning%20a%20wide%20range%20of%20applications%2C%20such%0Aas%20image%20and%20gesture%20recognition%2C%20audio%20classification%2C%20and%20optical%20flow%0Aestimation.%20In%20all%20the%20experiments%2C%20S-TLLR%20achieved%20high%20accuracy%2C%20comparable%0Ato%20BPTT%2C%20with%20a%20reduction%20in%20memory%20between%20%245-50%5Ctimes%24%20and%0Amultiply-accumulate%20%28MAC%29%20operations%20between%20%241.3-6.6%5Ctimes%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.15220v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS-TLLR%253A%2520STDP-inspired%2520Temporal%2520Local%2520Learning%2520Rule%2520for%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMarco%2520Paul%2520E.%2520Apolinario%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520biologically%2520plausible%2520models%2520that%2520have%250Abeen%2520identified%2520as%2520potentially%2520apt%2520for%2520deploying%2520energy-efficient%2520intelligence%250Aat%2520the%2520edge%252C%2520particularly%2520for%2520sequential%2520learning%2520tasks.%2520However%252C%2520training%2520of%250ASNNs%2520poses%2520significant%2520challenges%2520due%2520to%2520the%2520necessity%2520for%2520precise%2520temporal%2520and%250Aspatial%2520credit%2520assignment.%2520Back-propagation%2520through%2520time%2520%2528BPTT%2529%2520algorithm%252C%250Awhilst%2520the%2520most%2520widely%2520used%2520method%2520for%2520addressing%2520these%2520issues%252C%2520incurs%2520high%250Acomputational%2520cost%2520due%2520to%2520its%2520temporal%2520dependency.%2520In%2520this%2520work%252C%2520we%2520propose%250AS-TLLR%252C%2520a%2520novel%2520three-factor%2520temporal%2520local%2520learning%2520rule%2520inspired%2520by%2520the%250ASpike-Timing%2520Dependent%2520Plasticity%2520%2528STDP%2529%2520mechanism%252C%2520aimed%2520at%2520training%2520deep%2520SNNs%250Aon%2520event-based%2520learning%2520tasks.%2520Furthermore%252C%2520S-TLLR%2520is%2520designed%2520to%2520have%2520low%250Amemory%2520and%2520time%2520complexities%252C%2520which%2520are%2520independent%2520of%2520the%2520number%2520of%2520time%250Asteps%252C%2520rendering%2520it%2520suitable%2520for%2520online%2520learning%2520on%2520low-power%2520edge%2520devices.%2520To%250Ademonstrate%2520the%2520scalability%2520of%2520our%2520proposed%2520method%252C%2520we%2520have%2520conducted%2520extensive%250Aevaluations%2520on%2520event-based%2520datasets%2520spanning%2520a%2520wide%2520range%2520of%2520applications%252C%2520such%250Aas%2520image%2520and%2520gesture%2520recognition%252C%2520audio%2520classification%252C%2520and%2520optical%2520flow%250Aestimation.%2520In%2520all%2520the%2520experiments%252C%2520S-TLLR%2520achieved%2520high%2520accuracy%252C%2520comparable%250Ato%2520BPTT%252C%2520with%2520a%2520reduction%2520in%2520memory%2520between%2520%25245-50%255Ctimes%2524%2520and%250Amultiply-accumulate%2520%2528MAC%2529%2520operations%2520between%2520%25241.3-6.6%255Ctimes%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.15220v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S-TLLR%3A%20STDP-inspired%20Temporal%20Local%20Learning%20Rule%20for%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Marco%20Paul%20E.%20Apolinario%20and%20Kaushik%20Roy&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20biologically%20plausible%20models%20that%20have%0Abeen%20identified%20as%20potentially%20apt%20for%20deploying%20energy-efficient%20intelligence%0Aat%20the%20edge%2C%20particularly%20for%20sequential%20learning%20tasks.%20However%2C%20training%20of%0ASNNs%20poses%20significant%20challenges%20due%20to%20the%20necessity%20for%20precise%20temporal%20and%0Aspatial%20credit%20assignment.%20Back-propagation%20through%20time%20%28BPTT%29%20algorithm%2C%0Awhilst%20the%20most%20widely%20used%20method%20for%20addressing%20these%20issues%2C%20incurs%20high%0Acomputational%20cost%20due%20to%20its%20temporal%20dependency.%20In%20this%20work%2C%20we%20propose%0AS-TLLR%2C%20a%20novel%20three-factor%20temporal%20local%20learning%20rule%20inspired%20by%20the%0ASpike-Timing%20Dependent%20Plasticity%20%28STDP%29%20mechanism%2C%20aimed%20at%20training%20deep%20SNNs%0Aon%20event-based%20learning%20tasks.%20Furthermore%2C%20S-TLLR%20is%20designed%20to%20have%20low%0Amemory%20and%20time%20complexities%2C%20which%20are%20independent%20of%20the%20number%20of%20time%0Asteps%2C%20rendering%20it%20suitable%20for%20online%20learning%20on%20low-power%20edge%20devices.%20To%0Ademonstrate%20the%20scalability%20of%20our%20proposed%20method%2C%20we%20have%20conducted%20extensive%0Aevaluations%20on%20event-based%20datasets%20spanning%20a%20wide%20range%20of%20applications%2C%20such%0Aas%20image%20and%20gesture%20recognition%2C%20audio%20classification%2C%20and%20optical%20flow%0Aestimation.%20In%20all%20the%20experiments%2C%20S-TLLR%20achieved%20high%20accuracy%2C%20comparable%0Ato%20BPTT%2C%20with%20a%20reduction%20in%20memory%20between%20%245-50%5Ctimes%24%20and%0Amultiply-accumulate%20%28MAC%29%20operations%20between%20%241.3-6.6%5Ctimes%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.15220v4&entry.124074799=Read"},
{"title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism", "author": "Bingyang Wu and Shengyu Liu and Yinmin Zhong and Peng Sun and Xuanzhe Liu and Xin Jin", "abstract": "  The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.\n", "link": "http://arxiv.org/abs/2404.09526v2", "date": "2024-10-29", "relevancy": 2.0798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoongServe%3A%20Efficiently%20Serving%20Long-Context%20Large%20Language%20Models%20with%0A%20%20Elastic%20Sequence%20Parallelism&body=Title%3A%20LoongServe%3A%20Efficiently%20Serving%20Long-Context%20Large%20Language%20Models%20with%0A%20%20Elastic%20Sequence%20Parallelism%0AAuthor%3A%20Bingyang%20Wu%20and%20Shengyu%20Liu%20and%20Yinmin%20Zhong%20and%20Peng%20Sun%20and%20Xuanzhe%20Liu%20and%20Xin%20Jin%0AAbstract%3A%20%20%20The%20context%20window%20of%20large%20language%20models%20%28LLMs%29%20is%20rapidly%20increasing%2C%0Aleading%20to%20a%20huge%20variance%20in%20resource%20usage%20between%20different%20requests%20as%20well%0Aas%20between%20different%20phases%20of%20the%20same%20request.%20Restricted%20by%20static%0Aparallelism%20strategies%2C%20existing%20LLM%20serving%20systems%20cannot%20efficiently%20utilize%0Athe%20underlying%20resources%20to%20serve%20variable-length%20requests%20in%20different%20phases.%0ATo%20address%20this%20problem%2C%20we%20propose%20a%20new%20parallelism%20paradigm%2C%20elastic%0Asequence%20parallelism%20%28ESP%29%2C%20to%20elastically%20adapt%20to%20the%20variance%20between%0Adifferent%20requests%20and%20phases.%20Based%20on%20ESP%2C%20we%20design%20and%20build%20LoongServe%2C%20an%0ALLM%20serving%20system%20that%20%281%29%20improves%20computation%20efficiency%20by%20elastically%0Aadjusting%20the%20degree%20of%20parallelism%20in%20real-time%2C%20%282%29%20improves%20communication%0Aefficiency%20by%20reducing%20key-value%20cache%20migration%20overhead%20and%20overlapping%0Apartial%20decoding%20communication%20with%20computation%2C%20and%20%283%29%20improves%20GPU%20memory%0Aefficiency%20by%20reducing%20key-value%20cache%20fragmentation%20across%20instances.%20Our%0Aevaluation%20under%20diverse%20real-world%20datasets%20shows%20that%20LoongServe%20improves%20the%0Amaximum%20throughput%20by%20up%20to%203.85%24%5Ctimes%24%20compared%20to%20the%20chunked%20prefill%20and%0A5.81%24%5Ctimes%24%20compared%20to%20the%20prefill-decoding%20disaggregation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoongServe%253A%2520Efficiently%2520Serving%2520Long-Context%2520Large%2520Language%2520Models%2520with%250A%2520%2520Elastic%2520Sequence%2520Parallelism%26entry.906535625%3DBingyang%2520Wu%2520and%2520Shengyu%2520Liu%2520and%2520Yinmin%2520Zhong%2520and%2520Peng%2520Sun%2520and%2520Xuanzhe%2520Liu%2520and%2520Xin%2520Jin%26entry.1292438233%3D%2520%2520The%2520context%2520window%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520rapidly%2520increasing%252C%250Aleading%2520to%2520a%2520huge%2520variance%2520in%2520resource%2520usage%2520between%2520different%2520requests%2520as%2520well%250Aas%2520between%2520different%2520phases%2520of%2520the%2520same%2520request.%2520Restricted%2520by%2520static%250Aparallelism%2520strategies%252C%2520existing%2520LLM%2520serving%2520systems%2520cannot%2520efficiently%2520utilize%250Athe%2520underlying%2520resources%2520to%2520serve%2520variable-length%2520requests%2520in%2520different%2520phases.%250ATo%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%2520parallelism%2520paradigm%252C%2520elastic%250Asequence%2520parallelism%2520%2528ESP%2529%252C%2520to%2520elastically%2520adapt%2520to%2520the%2520variance%2520between%250Adifferent%2520requests%2520and%2520phases.%2520Based%2520on%2520ESP%252C%2520we%2520design%2520and%2520build%2520LoongServe%252C%2520an%250ALLM%2520serving%2520system%2520that%2520%25281%2529%2520improves%2520computation%2520efficiency%2520by%2520elastically%250Aadjusting%2520the%2520degree%2520of%2520parallelism%2520in%2520real-time%252C%2520%25282%2529%2520improves%2520communication%250Aefficiency%2520by%2520reducing%2520key-value%2520cache%2520migration%2520overhead%2520and%2520overlapping%250Apartial%2520decoding%2520communication%2520with%2520computation%252C%2520and%2520%25283%2529%2520improves%2520GPU%2520memory%250Aefficiency%2520by%2520reducing%2520key-value%2520cache%2520fragmentation%2520across%2520instances.%2520Our%250Aevaluation%2520under%2520diverse%2520real-world%2520datasets%2520shows%2520that%2520LoongServe%2520improves%2520the%250Amaximum%2520throughput%2520by%2520up%2520to%25203.85%2524%255Ctimes%2524%2520compared%2520to%2520the%2520chunked%2520prefill%2520and%250A5.81%2524%255Ctimes%2524%2520compared%2520to%2520the%2520prefill-decoding%2520disaggregation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoongServe%3A%20Efficiently%20Serving%20Long-Context%20Large%20Language%20Models%20with%0A%20%20Elastic%20Sequence%20Parallelism&entry.906535625=Bingyang%20Wu%20and%20Shengyu%20Liu%20and%20Yinmin%20Zhong%20and%20Peng%20Sun%20and%20Xuanzhe%20Liu%20and%20Xin%20Jin&entry.1292438233=%20%20The%20context%20window%20of%20large%20language%20models%20%28LLMs%29%20is%20rapidly%20increasing%2C%0Aleading%20to%20a%20huge%20variance%20in%20resource%20usage%20between%20different%20requests%20as%20well%0Aas%20between%20different%20phases%20of%20the%20same%20request.%20Restricted%20by%20static%0Aparallelism%20strategies%2C%20existing%20LLM%20serving%20systems%20cannot%20efficiently%20utilize%0Athe%20underlying%20resources%20to%20serve%20variable-length%20requests%20in%20different%20phases.%0ATo%20address%20this%20problem%2C%20we%20propose%20a%20new%20parallelism%20paradigm%2C%20elastic%0Asequence%20parallelism%20%28ESP%29%2C%20to%20elastically%20adapt%20to%20the%20variance%20between%0Adifferent%20requests%20and%20phases.%20Based%20on%20ESP%2C%20we%20design%20and%20build%20LoongServe%2C%20an%0ALLM%20serving%20system%20that%20%281%29%20improves%20computation%20efficiency%20by%20elastically%0Aadjusting%20the%20degree%20of%20parallelism%20in%20real-time%2C%20%282%29%20improves%20communication%0Aefficiency%20by%20reducing%20key-value%20cache%20migration%20overhead%20and%20overlapping%0Apartial%20decoding%20communication%20with%20computation%2C%20and%20%283%29%20improves%20GPU%20memory%0Aefficiency%20by%20reducing%20key-value%20cache%20fragmentation%20across%20instances.%20Our%0Aevaluation%20under%20diverse%20real-world%20datasets%20shows%20that%20LoongServe%20improves%20the%0Amaximum%20throughput%20by%20up%20to%203.85%24%5Ctimes%24%20compared%20to%20the%20chunked%20prefill%20and%0A5.81%24%5Ctimes%24%20compared%20to%20the%20prefill-decoding%20disaggregation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09526v2&entry.124074799=Read"},
{"title": "Human-Readable Programs as Actors of Reinforcement Learning Agents Using\n  Critic-Moderated Evolution", "author": "Senne Deproost and Denis Steckelmacher and Ann Now\u00e9", "abstract": "  With Deep Reinforcement Learning (DRL) being increasingly considered for the\ncontrol of real-world systems, the lack of transparency of the neural network\nat the core of RL becomes a concern. Programmatic Reinforcement Learning (PRL)\nis able to to create representations of this black-box in the form of source\ncode, not only increasing the explainability of the controller but also\nallowing for user adaptations. However, these methods focus on distilling a\nblack-box policy into a program and do so after learning using the Mean Squared\nError between produced and wanted behaviour, discarding other elements of the\nRL algorithm. The distilled policy may therefore perform significantly worse\nthan the black-box learned policy.\n  In this paper, we propose to directly learn a program as the policy of an RL\nagent. We build on TD3 and use its critics as the basis of the objective\nfunction of a genetic algorithm that syntheses the program. Our approach builds\nthe program during training, as opposed to after the fact. This steers the\nprogram to actual high rewards, instead of a simple Mean Squared Error. Also,\nour approach leverages the TD3 critics to achieve high sample-efficiency, as\nopposed to pure genetic methods that rely on Monte-Carlo evaluations. Our\nexperiments demonstrate the validity, explainability and sample-efficiency of\nour approach in a simple gridworld environment.\n", "link": "http://arxiv.org/abs/2410.21940v1", "date": "2024-10-29", "relevancy": 2.075, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5343}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5202}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Readable%20Programs%20as%20Actors%20of%20Reinforcement%20Learning%20Agents%20Using%0A%20%20Critic-Moderated%20Evolution&body=Title%3A%20Human-Readable%20Programs%20as%20Actors%20of%20Reinforcement%20Learning%20Agents%20Using%0A%20%20Critic-Moderated%20Evolution%0AAuthor%3A%20Senne%20Deproost%20and%20Denis%20Steckelmacher%20and%20Ann%20Now%C3%A9%0AAbstract%3A%20%20%20With%20Deep%20Reinforcement%20Learning%20%28DRL%29%20being%20increasingly%20considered%20for%20the%0Acontrol%20of%20real-world%20systems%2C%20the%20lack%20of%20transparency%20of%20the%20neural%20network%0Aat%20the%20core%20of%20RL%20becomes%20a%20concern.%20Programmatic%20Reinforcement%20Learning%20%28PRL%29%0Ais%20able%20to%20to%20create%20representations%20of%20this%20black-box%20in%20the%20form%20of%20source%0Acode%2C%20not%20only%20increasing%20the%20explainability%20of%20the%20controller%20but%20also%0Aallowing%20for%20user%20adaptations.%20However%2C%20these%20methods%20focus%20on%20distilling%20a%0Ablack-box%20policy%20into%20a%20program%20and%20do%20so%20after%20learning%20using%20the%20Mean%20Squared%0AError%20between%20produced%20and%20wanted%20behaviour%2C%20discarding%20other%20elements%20of%20the%0ARL%20algorithm.%20The%20distilled%20policy%20may%20therefore%20perform%20significantly%20worse%0Athan%20the%20black-box%20learned%20policy.%0A%20%20In%20this%20paper%2C%20we%20propose%20to%20directly%20learn%20a%20program%20as%20the%20policy%20of%20an%20RL%0Aagent.%20We%20build%20on%20TD3%20and%20use%20its%20critics%20as%20the%20basis%20of%20the%20objective%0Afunction%20of%20a%20genetic%20algorithm%20that%20syntheses%20the%20program.%20Our%20approach%20builds%0Athe%20program%20during%20training%2C%20as%20opposed%20to%20after%20the%20fact.%20This%20steers%20the%0Aprogram%20to%20actual%20high%20rewards%2C%20instead%20of%20a%20simple%20Mean%20Squared%20Error.%20Also%2C%0Aour%20approach%20leverages%20the%20TD3%20critics%20to%20achieve%20high%20sample-efficiency%2C%20as%0Aopposed%20to%20pure%20genetic%20methods%20that%20rely%20on%20Monte-Carlo%20evaluations.%20Our%0Aexperiments%20demonstrate%20the%20validity%2C%20explainability%20and%20sample-efficiency%20of%0Aour%20approach%20in%20a%20simple%20gridworld%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Readable%2520Programs%2520as%2520Actors%2520of%2520Reinforcement%2520Learning%2520Agents%2520Using%250A%2520%2520Critic-Moderated%2520Evolution%26entry.906535625%3DSenne%2520Deproost%2520and%2520Denis%2520Steckelmacher%2520and%2520Ann%2520Now%25C3%25A9%26entry.1292438233%3D%2520%2520With%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520being%2520increasingly%2520considered%2520for%2520the%250Acontrol%2520of%2520real-world%2520systems%252C%2520the%2520lack%2520of%2520transparency%2520of%2520the%2520neural%2520network%250Aat%2520the%2520core%2520of%2520RL%2520becomes%2520a%2520concern.%2520Programmatic%2520Reinforcement%2520Learning%2520%2528PRL%2529%250Ais%2520able%2520to%2520to%2520create%2520representations%2520of%2520this%2520black-box%2520in%2520the%2520form%2520of%2520source%250Acode%252C%2520not%2520only%2520increasing%2520the%2520explainability%2520of%2520the%2520controller%2520but%2520also%250Aallowing%2520for%2520user%2520adaptations.%2520However%252C%2520these%2520methods%2520focus%2520on%2520distilling%2520a%250Ablack-box%2520policy%2520into%2520a%2520program%2520and%2520do%2520so%2520after%2520learning%2520using%2520the%2520Mean%2520Squared%250AError%2520between%2520produced%2520and%2520wanted%2520behaviour%252C%2520discarding%2520other%2520elements%2520of%2520the%250ARL%2520algorithm.%2520The%2520distilled%2520policy%2520may%2520therefore%2520perform%2520significantly%2520worse%250Athan%2520the%2520black-box%2520learned%2520policy.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520directly%2520learn%2520a%2520program%2520as%2520the%2520policy%2520of%2520an%2520RL%250Aagent.%2520We%2520build%2520on%2520TD3%2520and%2520use%2520its%2520critics%2520as%2520the%2520basis%2520of%2520the%2520objective%250Afunction%2520of%2520a%2520genetic%2520algorithm%2520that%2520syntheses%2520the%2520program.%2520Our%2520approach%2520builds%250Athe%2520program%2520during%2520training%252C%2520as%2520opposed%2520to%2520after%2520the%2520fact.%2520This%2520steers%2520the%250Aprogram%2520to%2520actual%2520high%2520rewards%252C%2520instead%2520of%2520a%2520simple%2520Mean%2520Squared%2520Error.%2520Also%252C%250Aour%2520approach%2520leverages%2520the%2520TD3%2520critics%2520to%2520achieve%2520high%2520sample-efficiency%252C%2520as%250Aopposed%2520to%2520pure%2520genetic%2520methods%2520that%2520rely%2520on%2520Monte-Carlo%2520evaluations.%2520Our%250Aexperiments%2520demonstrate%2520the%2520validity%252C%2520explainability%2520and%2520sample-efficiency%2520of%250Aour%2520approach%2520in%2520a%2520simple%2520gridworld%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Readable%20Programs%20as%20Actors%20of%20Reinforcement%20Learning%20Agents%20Using%0A%20%20Critic-Moderated%20Evolution&entry.906535625=Senne%20Deproost%20and%20Denis%20Steckelmacher%20and%20Ann%20Now%C3%A9&entry.1292438233=%20%20With%20Deep%20Reinforcement%20Learning%20%28DRL%29%20being%20increasingly%20considered%20for%20the%0Acontrol%20of%20real-world%20systems%2C%20the%20lack%20of%20transparency%20of%20the%20neural%20network%0Aat%20the%20core%20of%20RL%20becomes%20a%20concern.%20Programmatic%20Reinforcement%20Learning%20%28PRL%29%0Ais%20able%20to%20to%20create%20representations%20of%20this%20black-box%20in%20the%20form%20of%20source%0Acode%2C%20not%20only%20increasing%20the%20explainability%20of%20the%20controller%20but%20also%0Aallowing%20for%20user%20adaptations.%20However%2C%20these%20methods%20focus%20on%20distilling%20a%0Ablack-box%20policy%20into%20a%20program%20and%20do%20so%20after%20learning%20using%20the%20Mean%20Squared%0AError%20between%20produced%20and%20wanted%20behaviour%2C%20discarding%20other%20elements%20of%20the%0ARL%20algorithm.%20The%20distilled%20policy%20may%20therefore%20perform%20significantly%20worse%0Athan%20the%20black-box%20learned%20policy.%0A%20%20In%20this%20paper%2C%20we%20propose%20to%20directly%20learn%20a%20program%20as%20the%20policy%20of%20an%20RL%0Aagent.%20We%20build%20on%20TD3%20and%20use%20its%20critics%20as%20the%20basis%20of%20the%20objective%0Afunction%20of%20a%20genetic%20algorithm%20that%20syntheses%20the%20program.%20Our%20approach%20builds%0Athe%20program%20during%20training%2C%20as%20opposed%20to%20after%20the%20fact.%20This%20steers%20the%0Aprogram%20to%20actual%20high%20rewards%2C%20instead%20of%20a%20simple%20Mean%20Squared%20Error.%20Also%2C%0Aour%20approach%20leverages%20the%20TD3%20critics%20to%20achieve%20high%20sample-efficiency%2C%20as%0Aopposed%20to%20pure%20genetic%20methods%20that%20rely%20on%20Monte-Carlo%20evaluations.%20Our%0Aexperiments%20demonstrate%20the%20validity%2C%20explainability%20and%20sample-efficiency%20of%0Aour%20approach%20in%20a%20simple%20gridworld%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21940v1&entry.124074799=Read"},
{"title": "CHORDONOMICON: A Dataset of 666,000 Songs and their Chord Progressions", "author": "Spyridon Kantarelis and Konstantinos Thomas and Vassilis Lyberatos and Edmund Dervakos and Giorgos Stamou", "abstract": "  Chord progressions encapsulate important information about music, pertaining\nto its structure and conveyed emotions. They serve as the backbone of musical\ncomposition, and in many cases, they are the sole information required for a\nmusician to play along and follow the music. Despite their importance, chord\nprogressions as a data domain remain underexplored. There is a lack of\nlarge-scale datasets suitable for deep learning applications, and limited\nresearch exploring chord progressions as an input modality. In this work, we\npresent Chordonomicon, a dataset of over 666,000 songs and their chord\nprogressions, annotated with structural parts, genre, and release date -\ncreated by scraping various sources of user-generated progressions and\nassociated metadata. We demonstrate the practical utility of the Chordonomicon\ndataset for classification and generation tasks, and discuss its potential to\nprovide valuable insights to the research community. Chord progressions are\nunique in their ability to be represented in multiple formats (e.g. text,\ngraph) and the wealth of information chords convey in given contexts, such as\ntheir harmonic function . These characteristics make the Chordonomicon an ideal\ntestbed for exploring advanced machine learning techniques, including\ntransformers, graph machine learning, and hybrid systems that combine knowledge\nrepresentation and machine learning.\n", "link": "http://arxiv.org/abs/2410.22046v1", "date": "2024-10-29", "relevancy": 2.0665, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHORDONOMICON%3A%20A%20Dataset%20of%20666%2C000%20Songs%20and%20their%20Chord%20Progressions&body=Title%3A%20CHORDONOMICON%3A%20A%20Dataset%20of%20666%2C000%20Songs%20and%20their%20Chord%20Progressions%0AAuthor%3A%20Spyridon%20Kantarelis%20and%20Konstantinos%20Thomas%20and%20Vassilis%20Lyberatos%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Chord%20progressions%20encapsulate%20important%20information%20about%20music%2C%20pertaining%0Ato%20its%20structure%20and%20conveyed%20emotions.%20They%20serve%20as%20the%20backbone%20of%20musical%0Acomposition%2C%20and%20in%20many%20cases%2C%20they%20are%20the%20sole%20information%20required%20for%20a%0Amusician%20to%20play%20along%20and%20follow%20the%20music.%20Despite%20their%20importance%2C%20chord%0Aprogressions%20as%20a%20data%20domain%20remain%20underexplored.%20There%20is%20a%20lack%20of%0Alarge-scale%20datasets%20suitable%20for%20deep%20learning%20applications%2C%20and%20limited%0Aresearch%20exploring%20chord%20progressions%20as%20an%20input%20modality.%20In%20this%20work%2C%20we%0Apresent%20Chordonomicon%2C%20a%20dataset%20of%20over%20666%2C000%20songs%20and%20their%20chord%0Aprogressions%2C%20annotated%20with%20structural%20parts%2C%20genre%2C%20and%20release%20date%20-%0Acreated%20by%20scraping%20various%20sources%20of%20user-generated%20progressions%20and%0Aassociated%20metadata.%20We%20demonstrate%20the%20practical%20utility%20of%20the%20Chordonomicon%0Adataset%20for%20classification%20and%20generation%20tasks%2C%20and%20discuss%20its%20potential%20to%0Aprovide%20valuable%20insights%20to%20the%20research%20community.%20Chord%20progressions%20are%0Aunique%20in%20their%20ability%20to%20be%20represented%20in%20multiple%20formats%20%28e.g.%20text%2C%0Agraph%29%20and%20the%20wealth%20of%20information%20chords%20convey%20in%20given%20contexts%2C%20such%20as%0Atheir%20harmonic%20function%20.%20These%20characteristics%20make%20the%20Chordonomicon%20an%20ideal%0Atestbed%20for%20exploring%20advanced%20machine%20learning%20techniques%2C%20including%0Atransformers%2C%20graph%20machine%20learning%2C%20and%20hybrid%20systems%20that%20combine%20knowledge%0Arepresentation%20and%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHORDONOMICON%253A%2520A%2520Dataset%2520of%2520666%252C000%2520Songs%2520and%2520their%2520Chord%2520Progressions%26entry.906535625%3DSpyridon%2520Kantarelis%2520and%2520Konstantinos%2520Thomas%2520and%2520Vassilis%2520Lyberatos%2520and%2520Edmund%2520Dervakos%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Chord%2520progressions%2520encapsulate%2520important%2520information%2520about%2520music%252C%2520pertaining%250Ato%2520its%2520structure%2520and%2520conveyed%2520emotions.%2520They%2520serve%2520as%2520the%2520backbone%2520of%2520musical%250Acomposition%252C%2520and%2520in%2520many%2520cases%252C%2520they%2520are%2520the%2520sole%2520information%2520required%2520for%2520a%250Amusician%2520to%2520play%2520along%2520and%2520follow%2520the%2520music.%2520Despite%2520their%2520importance%252C%2520chord%250Aprogressions%2520as%2520a%2520data%2520domain%2520remain%2520underexplored.%2520There%2520is%2520a%2520lack%2520of%250Alarge-scale%2520datasets%2520suitable%2520for%2520deep%2520learning%2520applications%252C%2520and%2520limited%250Aresearch%2520exploring%2520chord%2520progressions%2520as%2520an%2520input%2520modality.%2520In%2520this%2520work%252C%2520we%250Apresent%2520Chordonomicon%252C%2520a%2520dataset%2520of%2520over%2520666%252C000%2520songs%2520and%2520their%2520chord%250Aprogressions%252C%2520annotated%2520with%2520structural%2520parts%252C%2520genre%252C%2520and%2520release%2520date%2520-%250Acreated%2520by%2520scraping%2520various%2520sources%2520of%2520user-generated%2520progressions%2520and%250Aassociated%2520metadata.%2520We%2520demonstrate%2520the%2520practical%2520utility%2520of%2520the%2520Chordonomicon%250Adataset%2520for%2520classification%2520and%2520generation%2520tasks%252C%2520and%2520discuss%2520its%2520potential%2520to%250Aprovide%2520valuable%2520insights%2520to%2520the%2520research%2520community.%2520Chord%2520progressions%2520are%250Aunique%2520in%2520their%2520ability%2520to%2520be%2520represented%2520in%2520multiple%2520formats%2520%2528e.g.%2520text%252C%250Agraph%2529%2520and%2520the%2520wealth%2520of%2520information%2520chords%2520convey%2520in%2520given%2520contexts%252C%2520such%2520as%250Atheir%2520harmonic%2520function%2520.%2520These%2520characteristics%2520make%2520the%2520Chordonomicon%2520an%2520ideal%250Atestbed%2520for%2520exploring%2520advanced%2520machine%2520learning%2520techniques%252C%2520including%250Atransformers%252C%2520graph%2520machine%2520learning%252C%2520and%2520hybrid%2520systems%2520that%2520combine%2520knowledge%250Arepresentation%2520and%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHORDONOMICON%3A%20A%20Dataset%20of%20666%2C000%20Songs%20and%20their%20Chord%20Progressions&entry.906535625=Spyridon%20Kantarelis%20and%20Konstantinos%20Thomas%20and%20Vassilis%20Lyberatos%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Chord%20progressions%20encapsulate%20important%20information%20about%20music%2C%20pertaining%0Ato%20its%20structure%20and%20conveyed%20emotions.%20They%20serve%20as%20the%20backbone%20of%20musical%0Acomposition%2C%20and%20in%20many%20cases%2C%20they%20are%20the%20sole%20information%20required%20for%20a%0Amusician%20to%20play%20along%20and%20follow%20the%20music.%20Despite%20their%20importance%2C%20chord%0Aprogressions%20as%20a%20data%20domain%20remain%20underexplored.%20There%20is%20a%20lack%20of%0Alarge-scale%20datasets%20suitable%20for%20deep%20learning%20applications%2C%20and%20limited%0Aresearch%20exploring%20chord%20progressions%20as%20an%20input%20modality.%20In%20this%20work%2C%20we%0Apresent%20Chordonomicon%2C%20a%20dataset%20of%20over%20666%2C000%20songs%20and%20their%20chord%0Aprogressions%2C%20annotated%20with%20structural%20parts%2C%20genre%2C%20and%20release%20date%20-%0Acreated%20by%20scraping%20various%20sources%20of%20user-generated%20progressions%20and%0Aassociated%20metadata.%20We%20demonstrate%20the%20practical%20utility%20of%20the%20Chordonomicon%0Adataset%20for%20classification%20and%20generation%20tasks%2C%20and%20discuss%20its%20potential%20to%0Aprovide%20valuable%20insights%20to%20the%20research%20community.%20Chord%20progressions%20are%0Aunique%20in%20their%20ability%20to%20be%20represented%20in%20multiple%20formats%20%28e.g.%20text%2C%0Agraph%29%20and%20the%20wealth%20of%20information%20chords%20convey%20in%20given%20contexts%2C%20such%20as%0Atheir%20harmonic%20function%20.%20These%20characteristics%20make%20the%20Chordonomicon%20an%20ideal%0Atestbed%20for%20exploring%20advanced%20machine%20learning%20techniques%2C%20including%0Atransformers%2C%20graph%20machine%20learning%2C%20and%20hybrid%20systems%20that%20combine%20knowledge%0Arepresentation%20and%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22046v1&entry.124074799=Read"},
{"title": "Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic\n  Segmentation", "author": "Jintao Tong and Yixiong Zou and Yuhua Li and Ruixuan Li", "abstract": "  Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train\nthe model on a large-scale source-domain dataset, and then transfer the model\nto data-scarce target-domain datasets for pixel-level segmentation. The\nsignificant domain gap between the source and target datasets leads to a sharp\ndecline in the performance of existing few-shot segmentation (FSS) methods in\ncross-domain scenarios. In this work, we discover an intriguing phenomenon:\nsimply filtering different frequency components for target domains can lead to\na significant performance improvement, sometimes even as high as 14% mIoU.\nThen, we delve into this phenomenon for an interpretation, and find such\nimprovements stem from the reduced inter-channel correlation in feature maps,\nwhich benefits CD-FSS with enhanced robustness against domain gaps and larger\nactivated regions for segmentation. Based on this, we propose a lightweight\nfrequency masker, which further reduces channel correlations by an\namplitude-phase-masker (APM) module and an Adaptive Channel Phase Attention\n(ACPA) module. Notably, APM introduces only 0.01% additional parameters but\nimproves the average performance by over 10%, and ACPA imports only 2.5%\nparameters but further improves the performance by over 1.5%, which\nsignificantly surpasses the state-of-the-art CD-FSS methods.\n", "link": "http://arxiv.org/abs/2410.22135v1", "date": "2024-10-29", "relevancy": 2.0658, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5241}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Frequency%20Masker%20for%20Cross-Domain%20Few-Shot%20Semantic%0A%20%20Segmentation&body=Title%3A%20Lightweight%20Frequency%20Masker%20for%20Cross-Domain%20Few-Shot%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jintao%20Tong%20and%20Yixiong%20Zou%20and%20Yuhua%20Li%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Cross-domain%20few-shot%20segmentation%20%28CD-FSS%29%20is%20proposed%20to%20first%20pre-train%0Athe%20model%20on%20a%20large-scale%20source-domain%20dataset%2C%20and%20then%20transfer%20the%20model%0Ato%20data-scarce%20target-domain%20datasets%20for%20pixel-level%20segmentation.%20The%0Asignificant%20domain%20gap%20between%20the%20source%20and%20target%20datasets%20leads%20to%20a%20sharp%0Adecline%20in%20the%20performance%20of%20existing%20few-shot%20segmentation%20%28FSS%29%20methods%20in%0Across-domain%20scenarios.%20In%20this%20work%2C%20we%20discover%20an%20intriguing%20phenomenon%3A%0Asimply%20filtering%20different%20frequency%20components%20for%20target%20domains%20can%20lead%20to%0Aa%20significant%20performance%20improvement%2C%20sometimes%20even%20as%20high%20as%2014%25%20mIoU.%0AThen%2C%20we%20delve%20into%20this%20phenomenon%20for%20an%20interpretation%2C%20and%20find%20such%0Aimprovements%20stem%20from%20the%20reduced%20inter-channel%20correlation%20in%20feature%20maps%2C%0Awhich%20benefits%20CD-FSS%20with%20enhanced%20robustness%20against%20domain%20gaps%20and%20larger%0Aactivated%20regions%20for%20segmentation.%20Based%20on%20this%2C%20we%20propose%20a%20lightweight%0Afrequency%20masker%2C%20which%20further%20reduces%20channel%20correlations%20by%20an%0Aamplitude-phase-masker%20%28APM%29%20module%20and%20an%20Adaptive%20Channel%20Phase%20Attention%0A%28ACPA%29%20module.%20Notably%2C%20APM%20introduces%20only%200.01%25%20additional%20parameters%20but%0Aimproves%20the%20average%20performance%20by%20over%2010%25%2C%20and%20ACPA%20imports%20only%202.5%25%0Aparameters%20but%20further%20improves%20the%20performance%20by%20over%201.5%25%2C%20which%0Asignificantly%20surpasses%20the%20state-of-the-art%20CD-FSS%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Frequency%2520Masker%2520for%2520Cross-Domain%2520Few-Shot%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DJintao%2520Tong%2520and%2520Yixiong%2520Zou%2520and%2520Yuhua%2520Li%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Cross-domain%2520few-shot%2520segmentation%2520%2528CD-FSS%2529%2520is%2520proposed%2520to%2520first%2520pre-train%250Athe%2520model%2520on%2520a%2520large-scale%2520source-domain%2520dataset%252C%2520and%2520then%2520transfer%2520the%2520model%250Ato%2520data-scarce%2520target-domain%2520datasets%2520for%2520pixel-level%2520segmentation.%2520The%250Asignificant%2520domain%2520gap%2520between%2520the%2520source%2520and%2520target%2520datasets%2520leads%2520to%2520a%2520sharp%250Adecline%2520in%2520the%2520performance%2520of%2520existing%2520few-shot%2520segmentation%2520%2528FSS%2529%2520methods%2520in%250Across-domain%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520discover%2520an%2520intriguing%2520phenomenon%253A%250Asimply%2520filtering%2520different%2520frequency%2520components%2520for%2520target%2520domains%2520can%2520lead%2520to%250Aa%2520significant%2520performance%2520improvement%252C%2520sometimes%2520even%2520as%2520high%2520as%252014%2525%2520mIoU.%250AThen%252C%2520we%2520delve%2520into%2520this%2520phenomenon%2520for%2520an%2520interpretation%252C%2520and%2520find%2520such%250Aimprovements%2520stem%2520from%2520the%2520reduced%2520inter-channel%2520correlation%2520in%2520feature%2520maps%252C%250Awhich%2520benefits%2520CD-FSS%2520with%2520enhanced%2520robustness%2520against%2520domain%2520gaps%2520and%2520larger%250Aactivated%2520regions%2520for%2520segmentation.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%2520lightweight%250Afrequency%2520masker%252C%2520which%2520further%2520reduces%2520channel%2520correlations%2520by%2520an%250Aamplitude-phase-masker%2520%2528APM%2529%2520module%2520and%2520an%2520Adaptive%2520Channel%2520Phase%2520Attention%250A%2528ACPA%2529%2520module.%2520Notably%252C%2520APM%2520introduces%2520only%25200.01%2525%2520additional%2520parameters%2520but%250Aimproves%2520the%2520average%2520performance%2520by%2520over%252010%2525%252C%2520and%2520ACPA%2520imports%2520only%25202.5%2525%250Aparameters%2520but%2520further%2520improves%2520the%2520performance%2520by%2520over%25201.5%2525%252C%2520which%250Asignificantly%2520surpasses%2520the%2520state-of-the-art%2520CD-FSS%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Frequency%20Masker%20for%20Cross-Domain%20Few-Shot%20Semantic%0A%20%20Segmentation&entry.906535625=Jintao%20Tong%20and%20Yixiong%20Zou%20and%20Yuhua%20Li%20and%20Ruixuan%20Li&entry.1292438233=%20%20Cross-domain%20few-shot%20segmentation%20%28CD-FSS%29%20is%20proposed%20to%20first%20pre-train%0Athe%20model%20on%20a%20large-scale%20source-domain%20dataset%2C%20and%20then%20transfer%20the%20model%0Ato%20data-scarce%20target-domain%20datasets%20for%20pixel-level%20segmentation.%20The%0Asignificant%20domain%20gap%20between%20the%20source%20and%20target%20datasets%20leads%20to%20a%20sharp%0Adecline%20in%20the%20performance%20of%20existing%20few-shot%20segmentation%20%28FSS%29%20methods%20in%0Across-domain%20scenarios.%20In%20this%20work%2C%20we%20discover%20an%20intriguing%20phenomenon%3A%0Asimply%20filtering%20different%20frequency%20components%20for%20target%20domains%20can%20lead%20to%0Aa%20significant%20performance%20improvement%2C%20sometimes%20even%20as%20high%20as%2014%25%20mIoU.%0AThen%2C%20we%20delve%20into%20this%20phenomenon%20for%20an%20interpretation%2C%20and%20find%20such%0Aimprovements%20stem%20from%20the%20reduced%20inter-channel%20correlation%20in%20feature%20maps%2C%0Awhich%20benefits%20CD-FSS%20with%20enhanced%20robustness%20against%20domain%20gaps%20and%20larger%0Aactivated%20regions%20for%20segmentation.%20Based%20on%20this%2C%20we%20propose%20a%20lightweight%0Afrequency%20masker%2C%20which%20further%20reduces%20channel%20correlations%20by%20an%0Aamplitude-phase-masker%20%28APM%29%20module%20and%20an%20Adaptive%20Channel%20Phase%20Attention%0A%28ACPA%29%20module.%20Notably%2C%20APM%20introduces%20only%200.01%25%20additional%20parameters%20but%0Aimproves%20the%20average%20performance%20by%20over%2010%25%2C%20and%20ACPA%20imports%20only%202.5%25%0Aparameters%20but%20further%20improves%20the%20performance%20by%20over%201.5%25%2C%20which%0Asignificantly%20surpasses%20the%20state-of-the-art%20CD-FSS%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22135v1&entry.124074799=Read"},
{"title": "Towards a theory of how the structure of language is acquired by deep\n  neural networks", "author": "Francesco Cagnetta and Matthieu Wyart", "abstract": "  How much data is required to learn the structure of a language via next-token\nprediction? We study this question for synthetic datasets generated via a\nProbabilistic Context-Free Grammar (PCFG) -- a tree-like generative model that\ncaptures many of the hierarchical structures found in natural languages. We\ndetermine token-token correlations analytically in our model and show that they\ncan be used to build a representation of the grammar's hidden variables, the\nlonger the range the deeper the variable. In addition, a finite training set\nlimits the resolution of correlations to an effective range, whose size grows\nwith that of the training set. As a result, a Language Model trained with\nincreasingly many examples can build a deeper representation of the grammar's\nstructure, thus reaching good performance despite the high dimensionality of\nthe problem. We conjecture that the relationship between training set size and\neffective range of correlations holds beyond our synthetic datasets. In\nparticular, our conjecture predicts how the scaling law for the test loss\nbehaviour with training set size depends on the length of the context window,\nwhich we confirm empirically in Shakespeare's plays and Wikipedia articles.\n", "link": "http://arxiv.org/abs/2406.00048v3", "date": "2024-10-29", "relevancy": 2.0598, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20theory%20of%20how%20the%20structure%20of%20language%20is%20acquired%20by%20deep%0A%20%20neural%20networks&body=Title%3A%20Towards%20a%20theory%20of%20how%20the%20structure%20of%20language%20is%20acquired%20by%20deep%0A%20%20neural%20networks%0AAuthor%3A%20Francesco%20Cagnetta%20and%20Matthieu%20Wyart%0AAbstract%3A%20%20%20How%20much%20data%20is%20required%20to%20learn%20the%20structure%20of%20a%20language%20via%20next-token%0Aprediction%3F%20We%20study%20this%20question%20for%20synthetic%20datasets%20generated%20via%20a%0AProbabilistic%20Context-Free%20Grammar%20%28PCFG%29%20--%20a%20tree-like%20generative%20model%20that%0Acaptures%20many%20of%20the%20hierarchical%20structures%20found%20in%20natural%20languages.%20We%0Adetermine%20token-token%20correlations%20analytically%20in%20our%20model%20and%20show%20that%20they%0Acan%20be%20used%20to%20build%20a%20representation%20of%20the%20grammar%27s%20hidden%20variables%2C%20the%0Alonger%20the%20range%20the%20deeper%20the%20variable.%20In%20addition%2C%20a%20finite%20training%20set%0Alimits%20the%20resolution%20of%20correlations%20to%20an%20effective%20range%2C%20whose%20size%20grows%0Awith%20that%20of%20the%20training%20set.%20As%20a%20result%2C%20a%20Language%20Model%20trained%20with%0Aincreasingly%20many%20examples%20can%20build%20a%20deeper%20representation%20of%20the%20grammar%27s%0Astructure%2C%20thus%20reaching%20good%20performance%20despite%20the%20high%20dimensionality%20of%0Athe%20problem.%20We%20conjecture%20that%20the%20relationship%20between%20training%20set%20size%20and%0Aeffective%20range%20of%20correlations%20holds%20beyond%20our%20synthetic%20datasets.%20In%0Aparticular%2C%20our%20conjecture%20predicts%20how%20the%20scaling%20law%20for%20the%20test%20loss%0Abehaviour%20with%20training%20set%20size%20depends%20on%20the%20length%20of%20the%20context%20window%2C%0Awhich%20we%20confirm%20empirically%20in%20Shakespeare%27s%20plays%20and%20Wikipedia%20articles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00048v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520theory%2520of%2520how%2520the%2520structure%2520of%2520language%2520is%2520acquired%2520by%2520deep%250A%2520%2520neural%2520networks%26entry.906535625%3DFrancesco%2520Cagnetta%2520and%2520Matthieu%2520Wyart%26entry.1292438233%3D%2520%2520How%2520much%2520data%2520is%2520required%2520to%2520learn%2520the%2520structure%2520of%2520a%2520language%2520via%2520next-token%250Aprediction%253F%2520We%2520study%2520this%2520question%2520for%2520synthetic%2520datasets%2520generated%2520via%2520a%250AProbabilistic%2520Context-Free%2520Grammar%2520%2528PCFG%2529%2520--%2520a%2520tree-like%2520generative%2520model%2520that%250Acaptures%2520many%2520of%2520the%2520hierarchical%2520structures%2520found%2520in%2520natural%2520languages.%2520We%250Adetermine%2520token-token%2520correlations%2520analytically%2520in%2520our%2520model%2520and%2520show%2520that%2520they%250Acan%2520be%2520used%2520to%2520build%2520a%2520representation%2520of%2520the%2520grammar%2527s%2520hidden%2520variables%252C%2520the%250Alonger%2520the%2520range%2520the%2520deeper%2520the%2520variable.%2520In%2520addition%252C%2520a%2520finite%2520training%2520set%250Alimits%2520the%2520resolution%2520of%2520correlations%2520to%2520an%2520effective%2520range%252C%2520whose%2520size%2520grows%250Awith%2520that%2520of%2520the%2520training%2520set.%2520As%2520a%2520result%252C%2520a%2520Language%2520Model%2520trained%2520with%250Aincreasingly%2520many%2520examples%2520can%2520build%2520a%2520deeper%2520representation%2520of%2520the%2520grammar%2527s%250Astructure%252C%2520thus%2520reaching%2520good%2520performance%2520despite%2520the%2520high%2520dimensionality%2520of%250Athe%2520problem.%2520We%2520conjecture%2520that%2520the%2520relationship%2520between%2520training%2520set%2520size%2520and%250Aeffective%2520range%2520of%2520correlations%2520holds%2520beyond%2520our%2520synthetic%2520datasets.%2520In%250Aparticular%252C%2520our%2520conjecture%2520predicts%2520how%2520the%2520scaling%2520law%2520for%2520the%2520test%2520loss%250Abehaviour%2520with%2520training%2520set%2520size%2520depends%2520on%2520the%2520length%2520of%2520the%2520context%2520window%252C%250Awhich%2520we%2520confirm%2520empirically%2520in%2520Shakespeare%2527s%2520plays%2520and%2520Wikipedia%2520articles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00048v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20theory%20of%20how%20the%20structure%20of%20language%20is%20acquired%20by%20deep%0A%20%20neural%20networks&entry.906535625=Francesco%20Cagnetta%20and%20Matthieu%20Wyart&entry.1292438233=%20%20How%20much%20data%20is%20required%20to%20learn%20the%20structure%20of%20a%20language%20via%20next-token%0Aprediction%3F%20We%20study%20this%20question%20for%20synthetic%20datasets%20generated%20via%20a%0AProbabilistic%20Context-Free%20Grammar%20%28PCFG%29%20--%20a%20tree-like%20generative%20model%20that%0Acaptures%20many%20of%20the%20hierarchical%20structures%20found%20in%20natural%20languages.%20We%0Adetermine%20token-token%20correlations%20analytically%20in%20our%20model%20and%20show%20that%20they%0Acan%20be%20used%20to%20build%20a%20representation%20of%20the%20grammar%27s%20hidden%20variables%2C%20the%0Alonger%20the%20range%20the%20deeper%20the%20variable.%20In%20addition%2C%20a%20finite%20training%20set%0Alimits%20the%20resolution%20of%20correlations%20to%20an%20effective%20range%2C%20whose%20size%20grows%0Awith%20that%20of%20the%20training%20set.%20As%20a%20result%2C%20a%20Language%20Model%20trained%20with%0Aincreasingly%20many%20examples%20can%20build%20a%20deeper%20representation%20of%20the%20grammar%27s%0Astructure%2C%20thus%20reaching%20good%20performance%20despite%20the%20high%20dimensionality%20of%0Athe%20problem.%20We%20conjecture%20that%20the%20relationship%20between%20training%20set%20size%20and%0Aeffective%20range%20of%20correlations%20holds%20beyond%20our%20synthetic%20datasets.%20In%0Aparticular%2C%20our%20conjecture%20predicts%20how%20the%20scaling%20law%20for%20the%20test%20loss%0Abehaviour%20with%20training%20set%20size%20depends%20on%20the%20length%20of%20the%20context%20window%2C%0Awhich%20we%20confirm%20empirically%20in%20Shakespeare%27s%20plays%20and%20Wikipedia%20articles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00048v3&entry.124074799=Read"},
{"title": "Reducing the Transformer Architecture to a Minimum", "author": "Bernhard Bermeitinger and Tomas Hrycej and Massimo Pavone and Julianus Kath and Siegfried Handschuh", "abstract": "  Transformers are a widespread and successful model architecture, particularly\nin Natural Language Processing (NLP) and Computer Vision (CV). The essential\ninnovation of this architecture is the Attention Mechanism, which solves the\nproblem of extracting relevant context information from long sequences in NLP\nand realistic scenes in CV. A classical neural network component, a Multi-Layer\nPerceptron (MLP), complements the attention mechanism. Its necessity is\nfrequently justified by its capability of modeling nonlinear relationships.\nHowever, the attention mechanism itself is nonlinear through its internal use\nof similarity measures. A possible hypothesis is that this nonlinearity is\nsufficient for modeling typical application problems. As the MLPs usually\ncontain the most trainable parameters of the whole model, their omission would\nsubstantially reduce the parameter set size. Further components can also be\nreorganized to reduce the number of parameters. Under some conditions, query\nand key matrices can be collapsed into a single matrix of the same size. The\nsame is true about value and projection matrices, which can also be omitted\nwithout eliminating the substance of the attention mechanism. Initially, the\nsimilarity measure was defined asymmetrically, with peculiar properties such as\nthat a token is possibly dissimilar to itself. A possible symmetric definition\nrequires only half of the parameters. We have laid the groundwork by testing\nwidespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that\nsimplified transformer architectures (a) without MLP, (b) with collapsed\nmatrices, and (c) symmetric similarity matrices exhibit similar performance as\nthe original architecture, saving up to 90% of parameters without hurting the\nclassification performance.\n", "link": "http://arxiv.org/abs/2410.13732v2", "date": "2024-10-29", "relevancy": 2.0559, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5495}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20the%20Transformer%20Architecture%20to%20a%20Minimum&body=Title%3A%20Reducing%20the%20Transformer%20Architecture%20to%20a%20Minimum%0AAuthor%3A%20Bernhard%20Bermeitinger%20and%20Tomas%20Hrycej%20and%20Massimo%20Pavone%20and%20Julianus%20Kath%20and%20Siegfried%20Handschuh%0AAbstract%3A%20%20%20Transformers%20are%20a%20widespread%20and%20successful%20model%20architecture%2C%20particularly%0Ain%20Natural%20Language%20Processing%20%28NLP%29%20and%20Computer%20Vision%20%28CV%29.%20The%20essential%0Ainnovation%20of%20this%20architecture%20is%20the%20Attention%20Mechanism%2C%20which%20solves%20the%0Aproblem%20of%20extracting%20relevant%20context%20information%20from%20long%20sequences%20in%20NLP%0Aand%20realistic%20scenes%20in%20CV.%20A%20classical%20neural%20network%20component%2C%20a%20Multi-Layer%0APerceptron%20%28MLP%29%2C%20complements%20the%20attention%20mechanism.%20Its%20necessity%20is%0Afrequently%20justified%20by%20its%20capability%20of%20modeling%20nonlinear%20relationships.%0AHowever%2C%20the%20attention%20mechanism%20itself%20is%20nonlinear%20through%20its%20internal%20use%0Aof%20similarity%20measures.%20A%20possible%20hypothesis%20is%20that%20this%20nonlinearity%20is%0Asufficient%20for%20modeling%20typical%20application%20problems.%20As%20the%20MLPs%20usually%0Acontain%20the%20most%20trainable%20parameters%20of%20the%20whole%20model%2C%20their%20omission%20would%0Asubstantially%20reduce%20the%20parameter%20set%20size.%20Further%20components%20can%20also%20be%0Areorganized%20to%20reduce%20the%20number%20of%20parameters.%20Under%20some%20conditions%2C%20query%0Aand%20key%20matrices%20can%20be%20collapsed%20into%20a%20single%20matrix%20of%20the%20same%20size.%20The%0Asame%20is%20true%20about%20value%20and%20projection%20matrices%2C%20which%20can%20also%20be%20omitted%0Awithout%20eliminating%20the%20substance%20of%20the%20attention%20mechanism.%20Initially%2C%20the%0Asimilarity%20measure%20was%20defined%20asymmetrically%2C%20with%20peculiar%20properties%20such%20as%0Athat%20a%20token%20is%20possibly%20dissimilar%20to%20itself.%20A%20possible%20symmetric%20definition%0Arequires%20only%20half%20of%20the%20parameters.%20We%20have%20laid%20the%20groundwork%20by%20testing%0Awidespread%20CV%20benchmarks%3A%20MNIST%20and%20CIFAR-10.%20The%20tests%20have%20shown%20that%0Asimplified%20transformer%20architectures%20%28a%29%20without%20MLP%2C%20%28b%29%20with%20collapsed%0Amatrices%2C%20and%20%28c%29%20symmetric%20similarity%20matrices%20exhibit%20similar%20performance%20as%0Athe%20original%20architecture%2C%20saving%20up%20to%2090%25%20of%20parameters%20without%20hurting%20the%0Aclassification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520the%2520Transformer%2520Architecture%2520to%2520a%2520Minimum%26entry.906535625%3DBernhard%2520Bermeitinger%2520and%2520Tomas%2520Hrycej%2520and%2520Massimo%2520Pavone%2520and%2520Julianus%2520Kath%2520and%2520Siegfried%2520Handschuh%26entry.1292438233%3D%2520%2520Transformers%2520are%2520a%2520widespread%2520and%2520successful%2520model%2520architecture%252C%2520particularly%250Ain%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520and%2520Computer%2520Vision%2520%2528CV%2529.%2520The%2520essential%250Ainnovation%2520of%2520this%2520architecture%2520is%2520the%2520Attention%2520Mechanism%252C%2520which%2520solves%2520the%250Aproblem%2520of%2520extracting%2520relevant%2520context%2520information%2520from%2520long%2520sequences%2520in%2520NLP%250Aand%2520realistic%2520scenes%2520in%2520CV.%2520A%2520classical%2520neural%2520network%2520component%252C%2520a%2520Multi-Layer%250APerceptron%2520%2528MLP%2529%252C%2520complements%2520the%2520attention%2520mechanism.%2520Its%2520necessity%2520is%250Afrequently%2520justified%2520by%2520its%2520capability%2520of%2520modeling%2520nonlinear%2520relationships.%250AHowever%252C%2520the%2520attention%2520mechanism%2520itself%2520is%2520nonlinear%2520through%2520its%2520internal%2520use%250Aof%2520similarity%2520measures.%2520A%2520possible%2520hypothesis%2520is%2520that%2520this%2520nonlinearity%2520is%250Asufficient%2520for%2520modeling%2520typical%2520application%2520problems.%2520As%2520the%2520MLPs%2520usually%250Acontain%2520the%2520most%2520trainable%2520parameters%2520of%2520the%2520whole%2520model%252C%2520their%2520omission%2520would%250Asubstantially%2520reduce%2520the%2520parameter%2520set%2520size.%2520Further%2520components%2520can%2520also%2520be%250Areorganized%2520to%2520reduce%2520the%2520number%2520of%2520parameters.%2520Under%2520some%2520conditions%252C%2520query%250Aand%2520key%2520matrices%2520can%2520be%2520collapsed%2520into%2520a%2520single%2520matrix%2520of%2520the%2520same%2520size.%2520The%250Asame%2520is%2520true%2520about%2520value%2520and%2520projection%2520matrices%252C%2520which%2520can%2520also%2520be%2520omitted%250Awithout%2520eliminating%2520the%2520substance%2520of%2520the%2520attention%2520mechanism.%2520Initially%252C%2520the%250Asimilarity%2520measure%2520was%2520defined%2520asymmetrically%252C%2520with%2520peculiar%2520properties%2520such%2520as%250Athat%2520a%2520token%2520is%2520possibly%2520dissimilar%2520to%2520itself.%2520A%2520possible%2520symmetric%2520definition%250Arequires%2520only%2520half%2520of%2520the%2520parameters.%2520We%2520have%2520laid%2520the%2520groundwork%2520by%2520testing%250Awidespread%2520CV%2520benchmarks%253A%2520MNIST%2520and%2520CIFAR-10.%2520The%2520tests%2520have%2520shown%2520that%250Asimplified%2520transformer%2520architectures%2520%2528a%2529%2520without%2520MLP%252C%2520%2528b%2529%2520with%2520collapsed%250Amatrices%252C%2520and%2520%2528c%2529%2520symmetric%2520similarity%2520matrices%2520exhibit%2520similar%2520performance%2520as%250Athe%2520original%2520architecture%252C%2520saving%2520up%2520to%252090%2525%2520of%2520parameters%2520without%2520hurting%2520the%250Aclassification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20the%20Transformer%20Architecture%20to%20a%20Minimum&entry.906535625=Bernhard%20Bermeitinger%20and%20Tomas%20Hrycej%20and%20Massimo%20Pavone%20and%20Julianus%20Kath%20and%20Siegfried%20Handschuh&entry.1292438233=%20%20Transformers%20are%20a%20widespread%20and%20successful%20model%20architecture%2C%20particularly%0Ain%20Natural%20Language%20Processing%20%28NLP%29%20and%20Computer%20Vision%20%28CV%29.%20The%20essential%0Ainnovation%20of%20this%20architecture%20is%20the%20Attention%20Mechanism%2C%20which%20solves%20the%0Aproblem%20of%20extracting%20relevant%20context%20information%20from%20long%20sequences%20in%20NLP%0Aand%20realistic%20scenes%20in%20CV.%20A%20classical%20neural%20network%20component%2C%20a%20Multi-Layer%0APerceptron%20%28MLP%29%2C%20complements%20the%20attention%20mechanism.%20Its%20necessity%20is%0Afrequently%20justified%20by%20its%20capability%20of%20modeling%20nonlinear%20relationships.%0AHowever%2C%20the%20attention%20mechanism%20itself%20is%20nonlinear%20through%20its%20internal%20use%0Aof%20similarity%20measures.%20A%20possible%20hypothesis%20is%20that%20this%20nonlinearity%20is%0Asufficient%20for%20modeling%20typical%20application%20problems.%20As%20the%20MLPs%20usually%0Acontain%20the%20most%20trainable%20parameters%20of%20the%20whole%20model%2C%20their%20omission%20would%0Asubstantially%20reduce%20the%20parameter%20set%20size.%20Further%20components%20can%20also%20be%0Areorganized%20to%20reduce%20the%20number%20of%20parameters.%20Under%20some%20conditions%2C%20query%0Aand%20key%20matrices%20can%20be%20collapsed%20into%20a%20single%20matrix%20of%20the%20same%20size.%20The%0Asame%20is%20true%20about%20value%20and%20projection%20matrices%2C%20which%20can%20also%20be%20omitted%0Awithout%20eliminating%20the%20substance%20of%20the%20attention%20mechanism.%20Initially%2C%20the%0Asimilarity%20measure%20was%20defined%20asymmetrically%2C%20with%20peculiar%20properties%20such%20as%0Athat%20a%20token%20is%20possibly%20dissimilar%20to%20itself.%20A%20possible%20symmetric%20definition%0Arequires%20only%20half%20of%20the%20parameters.%20We%20have%20laid%20the%20groundwork%20by%20testing%0Awidespread%20CV%20benchmarks%3A%20MNIST%20and%20CIFAR-10.%20The%20tests%20have%20shown%20that%0Asimplified%20transformer%20architectures%20%28a%29%20without%20MLP%2C%20%28b%29%20with%20collapsed%0Amatrices%2C%20and%20%28c%29%20symmetric%20similarity%20matrices%20exhibit%20similar%20performance%20as%0Athe%20original%20architecture%2C%20saving%20up%20to%2090%25%20of%20parameters%20without%20hurting%20the%0Aclassification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13732v2&entry.124074799=Read"},
{"title": "A Machine Learning-Based Secure Face Verification Scheme and Its\n  Applications to Digital Surveillance", "author": "Huan-Chih Wang and Ja-Ling Wu", "abstract": "  Face verification is a well-known image analysis application and is widely\nused to recognize individuals in contemporary society. However, most real-world\nrecognition systems ignore the importance of protecting the identity-sensitive\nfacial images that are used for verification. To address this problem, we\ninvestigate how to implement a secure face verification system that protects\nthe facial images from being imitated. In our work, we use the DeepID2\nconvolutional neural network to extract the features of a facial image and an\nEM algorithm to solve the facial verification problem. To maintain the privacy\nof facial images, we apply homomorphic encryption schemes to encrypt the facial\ndata and compute the EM algorithm in the ciphertext domain. We develop three\nface verification systems for surveillance (or entrance) control of a local\ncommunity based on three levels of privacy concerns. The associated timing\nperformances are presented to demonstrate their feasibility for practical\nimplementation.\n", "link": "http://arxiv.org/abs/2410.21993v1", "date": "2024-10-29", "relevancy": 2.0479, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5463}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4885}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20Learning-Based%20Secure%20Face%20Verification%20Scheme%20and%20Its%0A%20%20Applications%20to%20Digital%20Surveillance&body=Title%3A%20A%20Machine%20Learning-Based%20Secure%20Face%20Verification%20Scheme%20and%20Its%0A%20%20Applications%20to%20Digital%20Surveillance%0AAuthor%3A%20Huan-Chih%20Wang%20and%20Ja-Ling%20Wu%0AAbstract%3A%20%20%20Face%20verification%20is%20a%20well-known%20image%20analysis%20application%20and%20is%20widely%0Aused%20to%20recognize%20individuals%20in%20contemporary%20society.%20However%2C%20most%20real-world%0Arecognition%20systems%20ignore%20the%20importance%20of%20protecting%20the%20identity-sensitive%0Afacial%20images%20that%20are%20used%20for%20verification.%20To%20address%20this%20problem%2C%20we%0Ainvestigate%20how%20to%20implement%20a%20secure%20face%20verification%20system%20that%20protects%0Athe%20facial%20images%20from%20being%20imitated.%20In%20our%20work%2C%20we%20use%20the%20DeepID2%0Aconvolutional%20neural%20network%20to%20extract%20the%20features%20of%20a%20facial%20image%20and%20an%0AEM%20algorithm%20to%20solve%20the%20facial%20verification%20problem.%20To%20maintain%20the%20privacy%0Aof%20facial%20images%2C%20we%20apply%20homomorphic%20encryption%20schemes%20to%20encrypt%20the%20facial%0Adata%20and%20compute%20the%20EM%20algorithm%20in%20the%20ciphertext%20domain.%20We%20develop%20three%0Aface%20verification%20systems%20for%20surveillance%20%28or%20entrance%29%20control%20of%20a%20local%0Acommunity%20based%20on%20three%20levels%20of%20privacy%20concerns.%20The%20associated%20timing%0Aperformances%20are%20presented%20to%20demonstrate%20their%20feasibility%20for%20practical%0Aimplementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Machine%2520Learning-Based%2520Secure%2520Face%2520Verification%2520Scheme%2520and%2520Its%250A%2520%2520Applications%2520to%2520Digital%2520Surveillance%26entry.906535625%3DHuan-Chih%2520Wang%2520and%2520Ja-Ling%2520Wu%26entry.1292438233%3D%2520%2520Face%2520verification%2520is%2520a%2520well-known%2520image%2520analysis%2520application%2520and%2520is%2520widely%250Aused%2520to%2520recognize%2520individuals%2520in%2520contemporary%2520society.%2520However%252C%2520most%2520real-world%250Arecognition%2520systems%2520ignore%2520the%2520importance%2520of%2520protecting%2520the%2520identity-sensitive%250Afacial%2520images%2520that%2520are%2520used%2520for%2520verification.%2520To%2520address%2520this%2520problem%252C%2520we%250Ainvestigate%2520how%2520to%2520implement%2520a%2520secure%2520face%2520verification%2520system%2520that%2520protects%250Athe%2520facial%2520images%2520from%2520being%2520imitated.%2520In%2520our%2520work%252C%2520we%2520use%2520the%2520DeepID2%250Aconvolutional%2520neural%2520network%2520to%2520extract%2520the%2520features%2520of%2520a%2520facial%2520image%2520and%2520an%250AEM%2520algorithm%2520to%2520solve%2520the%2520facial%2520verification%2520problem.%2520To%2520maintain%2520the%2520privacy%250Aof%2520facial%2520images%252C%2520we%2520apply%2520homomorphic%2520encryption%2520schemes%2520to%2520encrypt%2520the%2520facial%250Adata%2520and%2520compute%2520the%2520EM%2520algorithm%2520in%2520the%2520ciphertext%2520domain.%2520We%2520develop%2520three%250Aface%2520verification%2520systems%2520for%2520surveillance%2520%2528or%2520entrance%2529%2520control%2520of%2520a%2520local%250Acommunity%2520based%2520on%2520three%2520levels%2520of%2520privacy%2520concerns.%2520The%2520associated%2520timing%250Aperformances%2520are%2520presented%2520to%2520demonstrate%2520their%2520feasibility%2520for%2520practical%250Aimplementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20Learning-Based%20Secure%20Face%20Verification%20Scheme%20and%20Its%0A%20%20Applications%20to%20Digital%20Surveillance&entry.906535625=Huan-Chih%20Wang%20and%20Ja-Ling%20Wu&entry.1292438233=%20%20Face%20verification%20is%20a%20well-known%20image%20analysis%20application%20and%20is%20widely%0Aused%20to%20recognize%20individuals%20in%20contemporary%20society.%20However%2C%20most%20real-world%0Arecognition%20systems%20ignore%20the%20importance%20of%20protecting%20the%20identity-sensitive%0Afacial%20images%20that%20are%20used%20for%20verification.%20To%20address%20this%20problem%2C%20we%0Ainvestigate%20how%20to%20implement%20a%20secure%20face%20verification%20system%20that%20protects%0Athe%20facial%20images%20from%20being%20imitated.%20In%20our%20work%2C%20we%20use%20the%20DeepID2%0Aconvolutional%20neural%20network%20to%20extract%20the%20features%20of%20a%20facial%20image%20and%20an%0AEM%20algorithm%20to%20solve%20the%20facial%20verification%20problem.%20To%20maintain%20the%20privacy%0Aof%20facial%20images%2C%20we%20apply%20homomorphic%20encryption%20schemes%20to%20encrypt%20the%20facial%0Adata%20and%20compute%20the%20EM%20algorithm%20in%20the%20ciphertext%20domain.%20We%20develop%20three%0Aface%20verification%20systems%20for%20surveillance%20%28or%20entrance%29%20control%20of%20a%20local%0Acommunity%20based%20on%20three%20levels%20of%20privacy%20concerns.%20The%20associated%20timing%0Aperformances%20are%20presented%20to%20demonstrate%20their%20feasibility%20for%20practical%0Aimplementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21993v1&entry.124074799=Read"},
{"title": "Emotion-Guided Image to Music Generation", "author": "Souraja Kundu and Saket Singh and Yuji Iwahori", "abstract": "  Generating music from images can enhance various applications, including\nbackground music for photo slideshows, social media experiences, and video\ncreation. This paper presents an emotion-guided image-to-music generation\nframework that leverages the Valence-Arousal (VA) emotional space to produce\nmusic that aligns with the emotional tone of a given image. Unlike previous\nmodels that rely on contrastive learning for emotional consistency, the\nproposed approach directly integrates a VA loss function to enable accurate\nemotional alignment. The model employs a CNN-Transformer architecture,\nfeaturing pre-trained CNN image feature extractors and three Transformer\nencoders to capture complex, high-level emotional features from MIDI music.\nThree Transformer decoders refine these features to generate musically and\nemotionally consistent MIDI sequences. Experimental results on a newly curated\nemotionally paired image-MIDI dataset demonstrate the proposed model's superior\nperformance across metrics such as Polyphony Rate, Pitch Entropy, Groove\nConsistency, and loss convergence.\n", "link": "http://arxiv.org/abs/2410.22299v1", "date": "2024-10-29", "relevancy": 2.041, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5206}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5109}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotion-Guided%20Image%20to%20Music%20Generation&body=Title%3A%20Emotion-Guided%20Image%20to%20Music%20Generation%0AAuthor%3A%20Souraja%20Kundu%20and%20Saket%20Singh%20and%20Yuji%20Iwahori%0AAbstract%3A%20%20%20Generating%20music%20from%20images%20can%20enhance%20various%20applications%2C%20including%0Abackground%20music%20for%20photo%20slideshows%2C%20social%20media%20experiences%2C%20and%20video%0Acreation.%20This%20paper%20presents%20an%20emotion-guided%20image-to-music%20generation%0Aframework%20that%20leverages%20the%20Valence-Arousal%20%28VA%29%20emotional%20space%20to%20produce%0Amusic%20that%20aligns%20with%20the%20emotional%20tone%20of%20a%20given%20image.%20Unlike%20previous%0Amodels%20that%20rely%20on%20contrastive%20learning%20for%20emotional%20consistency%2C%20the%0Aproposed%20approach%20directly%20integrates%20a%20VA%20loss%20function%20to%20enable%20accurate%0Aemotional%20alignment.%20The%20model%20employs%20a%20CNN-Transformer%20architecture%2C%0Afeaturing%20pre-trained%20CNN%20image%20feature%20extractors%20and%20three%20Transformer%0Aencoders%20to%20capture%20complex%2C%20high-level%20emotional%20features%20from%20MIDI%20music.%0AThree%20Transformer%20decoders%20refine%20these%20features%20to%20generate%20musically%20and%0Aemotionally%20consistent%20MIDI%20sequences.%20Experimental%20results%20on%20a%20newly%20curated%0Aemotionally%20paired%20image-MIDI%20dataset%20demonstrate%20the%20proposed%20model%27s%20superior%0Aperformance%20across%20metrics%20such%20as%20Polyphony%20Rate%2C%20Pitch%20Entropy%2C%20Groove%0AConsistency%2C%20and%20loss%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotion-Guided%2520Image%2520to%2520Music%2520Generation%26entry.906535625%3DSouraja%2520Kundu%2520and%2520Saket%2520Singh%2520and%2520Yuji%2520Iwahori%26entry.1292438233%3D%2520%2520Generating%2520music%2520from%2520images%2520can%2520enhance%2520various%2520applications%252C%2520including%250Abackground%2520music%2520for%2520photo%2520slideshows%252C%2520social%2520media%2520experiences%252C%2520and%2520video%250Acreation.%2520This%2520paper%2520presents%2520an%2520emotion-guided%2520image-to-music%2520generation%250Aframework%2520that%2520leverages%2520the%2520Valence-Arousal%2520%2528VA%2529%2520emotional%2520space%2520to%2520produce%250Amusic%2520that%2520aligns%2520with%2520the%2520emotional%2520tone%2520of%2520a%2520given%2520image.%2520Unlike%2520previous%250Amodels%2520that%2520rely%2520on%2520contrastive%2520learning%2520for%2520emotional%2520consistency%252C%2520the%250Aproposed%2520approach%2520directly%2520integrates%2520a%2520VA%2520loss%2520function%2520to%2520enable%2520accurate%250Aemotional%2520alignment.%2520The%2520model%2520employs%2520a%2520CNN-Transformer%2520architecture%252C%250Afeaturing%2520pre-trained%2520CNN%2520image%2520feature%2520extractors%2520and%2520three%2520Transformer%250Aencoders%2520to%2520capture%2520complex%252C%2520high-level%2520emotional%2520features%2520from%2520MIDI%2520music.%250AThree%2520Transformer%2520decoders%2520refine%2520these%2520features%2520to%2520generate%2520musically%2520and%250Aemotionally%2520consistent%2520MIDI%2520sequences.%2520Experimental%2520results%2520on%2520a%2520newly%2520curated%250Aemotionally%2520paired%2520image-MIDI%2520dataset%2520demonstrate%2520the%2520proposed%2520model%2527s%2520superior%250Aperformance%2520across%2520metrics%2520such%2520as%2520Polyphony%2520Rate%252C%2520Pitch%2520Entropy%252C%2520Groove%250AConsistency%252C%2520and%2520loss%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion-Guided%20Image%20to%20Music%20Generation&entry.906535625=Souraja%20Kundu%20and%20Saket%20Singh%20and%20Yuji%20Iwahori&entry.1292438233=%20%20Generating%20music%20from%20images%20can%20enhance%20various%20applications%2C%20including%0Abackground%20music%20for%20photo%20slideshows%2C%20social%20media%20experiences%2C%20and%20video%0Acreation.%20This%20paper%20presents%20an%20emotion-guided%20image-to-music%20generation%0Aframework%20that%20leverages%20the%20Valence-Arousal%20%28VA%29%20emotional%20space%20to%20produce%0Amusic%20that%20aligns%20with%20the%20emotional%20tone%20of%20a%20given%20image.%20Unlike%20previous%0Amodels%20that%20rely%20on%20contrastive%20learning%20for%20emotional%20consistency%2C%20the%0Aproposed%20approach%20directly%20integrates%20a%20VA%20loss%20function%20to%20enable%20accurate%0Aemotional%20alignment.%20The%20model%20employs%20a%20CNN-Transformer%20architecture%2C%0Afeaturing%20pre-trained%20CNN%20image%20feature%20extractors%20and%20three%20Transformer%0Aencoders%20to%20capture%20complex%2C%20high-level%20emotional%20features%20from%20MIDI%20music.%0AThree%20Transformer%20decoders%20refine%20these%20features%20to%20generate%20musically%20and%0Aemotionally%20consistent%20MIDI%20sequences.%20Experimental%20results%20on%20a%20newly%20curated%0Aemotionally%20paired%20image-MIDI%20dataset%20demonstrate%20the%20proposed%20model%27s%20superior%0Aperformance%20across%20metrics%20such%20as%20Polyphony%20Rate%2C%20Pitch%20Entropy%2C%20Groove%0AConsistency%2C%20and%20loss%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22299v1&entry.124074799=Read"},
{"title": "A Probabilistic Hadamard U-Net for MRI Bias Field Correction", "author": "Xin Zhu and Hongyi Pan and Yury Velichko and Adam B. Murphy and Ashley Ross and Baris Turkbey and Ahmet Enis Cetin and Ulas Bagci", "abstract": "  Magnetic field inhomogeneity correction remains a challenging task in MRI\nanalysis. Most established techniques are designed for brain MRI by supposing\nthat image intensities in the identical tissue follow a uniform distribution.\nSuch an assumption cannot be easily applied to other organs, especially those\nthat are small in size and heterogeneous in texture (large variations in\nintensity), such as the prostate. To address this problem, this paper proposes\na probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field\ncorrection. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the\nlow-frequency scalar field, multiplied by the original input to obtain the\nprototypical corrected image. HU-Net converts the input image from the time\ndomain into the frequency domain via Hadamard transform. In the frequency\ndomain, high-frequency components are eliminated using the trainable filter\n(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a\nconditional variational autoencoder is used to encode possible bias\nfield-corrected variants into a low-dimensional latent space. Random samples\ndrawn from latent space are then incorporated with a prototypical corrected\nimage to generate multiple plausible images. Experimental results demonstrate\nthe effectiveness of PHU-Net in correcting bias-field in prostate MRI with a\nfast inference speed. It has also been shown that prostate MRI segmentation\naccuracy improves with the high-quality corrected images from PHU-Net. The code\nwill be available in the final version of this manuscript.\n", "link": "http://arxiv.org/abs/2403.05024v2", "date": "2024-10-29", "relevancy": 2.0391, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.52}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Probabilistic%20Hadamard%20U-Net%20for%20MRI%20Bias%20Field%20Correction&body=Title%3A%20A%20Probabilistic%20Hadamard%20U-Net%20for%20MRI%20Bias%20Field%20Correction%0AAuthor%3A%20Xin%20Zhu%20and%20Hongyi%20Pan%20and%20Yury%20Velichko%20and%20Adam%20B.%20Murphy%20and%20Ashley%20Ross%20and%20Baris%20Turkbey%20and%20Ahmet%20Enis%20Cetin%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Magnetic%20field%20inhomogeneity%20correction%20remains%20a%20challenging%20task%20in%20MRI%0Aanalysis.%20Most%20established%20techniques%20are%20designed%20for%20brain%20MRI%20by%20supposing%0Athat%20image%20intensities%20in%20the%20identical%20tissue%20follow%20a%20uniform%20distribution.%0ASuch%20an%20assumption%20cannot%20be%20easily%20applied%20to%20other%20organs%2C%20especially%20those%0Athat%20are%20small%20in%20size%20and%20heterogeneous%20in%20texture%20%28large%20variations%20in%0Aintensity%29%2C%20such%20as%20the%20prostate.%20To%20address%20this%20problem%2C%20this%20paper%20proposes%0Aa%20probabilistic%20Hadamard%20U-Net%20%28PHU-Net%29%20for%20prostate%20MRI%20bias%20field%0Acorrection.%20First%2C%20a%20novel%20Hadamard%20U-Net%20%28HU-Net%29%20is%20introduced%20to%20extract%20the%0Alow-frequency%20scalar%20field%2C%20multiplied%20by%20the%20original%20input%20to%20obtain%20the%0Aprototypical%20corrected%20image.%20HU-Net%20converts%20the%20input%20image%20from%20the%20time%0Adomain%20into%20the%20frequency%20domain%20via%20Hadamard%20transform.%20In%20the%20frequency%0Adomain%2C%20high-frequency%20components%20are%20eliminated%20using%20the%20trainable%20filter%0A%28scaling%20layer%29%2C%20hard-thresholding%20layer%2C%20and%20sparsity%20penalty.%20Next%2C%20a%0Aconditional%20variational%20autoencoder%20is%20used%20to%20encode%20possible%20bias%0Afield-corrected%20variants%20into%20a%20low-dimensional%20latent%20space.%20Random%20samples%0Adrawn%20from%20latent%20space%20are%20then%20incorporated%20with%20a%20prototypical%20corrected%0Aimage%20to%20generate%20multiple%20plausible%20images.%20Experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20PHU-Net%20in%20correcting%20bias-field%20in%20prostate%20MRI%20with%20a%0Afast%20inference%20speed.%20It%20has%20also%20been%20shown%20that%20prostate%20MRI%20segmentation%0Aaccuracy%20improves%20with%20the%20high-quality%20corrected%20images%20from%20PHU-Net.%20The%20code%0Awill%20be%20available%20in%20the%20final%20version%20of%20this%20manuscript.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Probabilistic%2520Hadamard%2520U-Net%2520for%2520MRI%2520Bias%2520Field%2520Correction%26entry.906535625%3DXin%2520Zhu%2520and%2520Hongyi%2520Pan%2520and%2520Yury%2520Velichko%2520and%2520Adam%2520B.%2520Murphy%2520and%2520Ashley%2520Ross%2520and%2520Baris%2520Turkbey%2520and%2520Ahmet%2520Enis%2520Cetin%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Magnetic%2520field%2520inhomogeneity%2520correction%2520remains%2520a%2520challenging%2520task%2520in%2520MRI%250Aanalysis.%2520Most%2520established%2520techniques%2520are%2520designed%2520for%2520brain%2520MRI%2520by%2520supposing%250Athat%2520image%2520intensities%2520in%2520the%2520identical%2520tissue%2520follow%2520a%2520uniform%2520distribution.%250ASuch%2520an%2520assumption%2520cannot%2520be%2520easily%2520applied%2520to%2520other%2520organs%252C%2520especially%2520those%250Athat%2520are%2520small%2520in%2520size%2520and%2520heterogeneous%2520in%2520texture%2520%2528large%2520variations%2520in%250Aintensity%2529%252C%2520such%2520as%2520the%2520prostate.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%2520proposes%250Aa%2520probabilistic%2520Hadamard%2520U-Net%2520%2528PHU-Net%2529%2520for%2520prostate%2520MRI%2520bias%2520field%250Acorrection.%2520First%252C%2520a%2520novel%2520Hadamard%2520U-Net%2520%2528HU-Net%2529%2520is%2520introduced%2520to%2520extract%2520the%250Alow-frequency%2520scalar%2520field%252C%2520multiplied%2520by%2520the%2520original%2520input%2520to%2520obtain%2520the%250Aprototypical%2520corrected%2520image.%2520HU-Net%2520converts%2520the%2520input%2520image%2520from%2520the%2520time%250Adomain%2520into%2520the%2520frequency%2520domain%2520via%2520Hadamard%2520transform.%2520In%2520the%2520frequency%250Adomain%252C%2520high-frequency%2520components%2520are%2520eliminated%2520using%2520the%2520trainable%2520filter%250A%2528scaling%2520layer%2529%252C%2520hard-thresholding%2520layer%252C%2520and%2520sparsity%2520penalty.%2520Next%252C%2520a%250Aconditional%2520variational%2520autoencoder%2520is%2520used%2520to%2520encode%2520possible%2520bias%250Afield-corrected%2520variants%2520into%2520a%2520low-dimensional%2520latent%2520space.%2520Random%2520samples%250Adrawn%2520from%2520latent%2520space%2520are%2520then%2520incorporated%2520with%2520a%2520prototypical%2520corrected%250Aimage%2520to%2520generate%2520multiple%2520plausible%2520images.%2520Experimental%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520PHU-Net%2520in%2520correcting%2520bias-field%2520in%2520prostate%2520MRI%2520with%2520a%250Afast%2520inference%2520speed.%2520It%2520has%2520also%2520been%2520shown%2520that%2520prostate%2520MRI%2520segmentation%250Aaccuracy%2520improves%2520with%2520the%2520high-quality%2520corrected%2520images%2520from%2520PHU-Net.%2520The%2520code%250Awill%2520be%2520available%2520in%2520the%2520final%2520version%2520of%2520this%2520manuscript.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Probabilistic%20Hadamard%20U-Net%20for%20MRI%20Bias%20Field%20Correction&entry.906535625=Xin%20Zhu%20and%20Hongyi%20Pan%20and%20Yury%20Velichko%20and%20Adam%20B.%20Murphy%20and%20Ashley%20Ross%20and%20Baris%20Turkbey%20and%20Ahmet%20Enis%20Cetin%20and%20Ulas%20Bagci&entry.1292438233=%20%20Magnetic%20field%20inhomogeneity%20correction%20remains%20a%20challenging%20task%20in%20MRI%0Aanalysis.%20Most%20established%20techniques%20are%20designed%20for%20brain%20MRI%20by%20supposing%0Athat%20image%20intensities%20in%20the%20identical%20tissue%20follow%20a%20uniform%20distribution.%0ASuch%20an%20assumption%20cannot%20be%20easily%20applied%20to%20other%20organs%2C%20especially%20those%0Athat%20are%20small%20in%20size%20and%20heterogeneous%20in%20texture%20%28large%20variations%20in%0Aintensity%29%2C%20such%20as%20the%20prostate.%20To%20address%20this%20problem%2C%20this%20paper%20proposes%0Aa%20probabilistic%20Hadamard%20U-Net%20%28PHU-Net%29%20for%20prostate%20MRI%20bias%20field%0Acorrection.%20First%2C%20a%20novel%20Hadamard%20U-Net%20%28HU-Net%29%20is%20introduced%20to%20extract%20the%0Alow-frequency%20scalar%20field%2C%20multiplied%20by%20the%20original%20input%20to%20obtain%20the%0Aprototypical%20corrected%20image.%20HU-Net%20converts%20the%20input%20image%20from%20the%20time%0Adomain%20into%20the%20frequency%20domain%20via%20Hadamard%20transform.%20In%20the%20frequency%0Adomain%2C%20high-frequency%20components%20are%20eliminated%20using%20the%20trainable%20filter%0A%28scaling%20layer%29%2C%20hard-thresholding%20layer%2C%20and%20sparsity%20penalty.%20Next%2C%20a%0Aconditional%20variational%20autoencoder%20is%20used%20to%20encode%20possible%20bias%0Afield-corrected%20variants%20into%20a%20low-dimensional%20latent%20space.%20Random%20samples%0Adrawn%20from%20latent%20space%20are%20then%20incorporated%20with%20a%20prototypical%20corrected%0Aimage%20to%20generate%20multiple%20plausible%20images.%20Experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20PHU-Net%20in%20correcting%20bias-field%20in%20prostate%20MRI%20with%20a%0Afast%20inference%20speed.%20It%20has%20also%20been%20shown%20that%20prostate%20MRI%20segmentation%0Aaccuracy%20improves%20with%20the%20high-quality%20corrected%20images%20from%20PHU-Net.%20The%20code%0Awill%20be%20available%20in%20the%20final%20version%20of%20this%20manuscript.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05024v2&entry.124074799=Read"},
{"title": "Forging the Forger: An Attempt to Improve Authorship Verification via\n  Data Augmentation", "author": "Silvia Corbara and Alejandro Moreo", "abstract": "  Authorship Verification (AV) is a text classification task concerned with\ninferring whether a candidate text has been written by one specific author or\nby someone else. It has been shown that many AV systems are vulnerable to\nadversarial attacks, where a malicious author actively tries to fool the\nclassifier by either concealing their writing style, or by imitating the style\nof another author. In this paper, we investigate the potential benefits of\naugmenting the classifier training set with (negative) synthetic examples.\nThese synthetic examples are generated to imitate the style of the author of\ninterest. We analyze the improvements in classifier prediction that this\naugmentation brings to bear in the task of AV in an adversarial setting. In\nparticular, we experiment with three different generator architectures (one\nbased on Recurrent Neural Networks, another based on small-scale transformers,\nand another based on the popular GPT model) and with two training strategies\n(one inspired by standard Language Models, and another inspired by Wasserstein\nGenerative Adversarial Networks). We evaluate our hypothesis on five datasets\n(three of which have been specifically collected to represent an adversarial\nsetting) and using two learning algorithms for the AV classifier (Support\nVector Machines and Convolutional Neural Networks). This experimentation has\nyielded negative results, revealing that, although our methodology proves\neffective in many adversarial settings, its benefits are too sporadic for a\npragmatical application.\n", "link": "http://arxiv.org/abs/2403.11265v2", "date": "2024-10-29", "relevancy": 2.0389, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5205}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5142}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forging%20the%20Forger%3A%20An%20Attempt%20to%20Improve%20Authorship%20Verification%20via%0A%20%20Data%20Augmentation&body=Title%3A%20Forging%20the%20Forger%3A%20An%20Attempt%20to%20Improve%20Authorship%20Verification%20via%0A%20%20Data%20Augmentation%0AAuthor%3A%20Silvia%20Corbara%20and%20Alejandro%20Moreo%0AAbstract%3A%20%20%20Authorship%20Verification%20%28AV%29%20is%20a%20text%20classification%20task%20concerned%20with%0Ainferring%20whether%20a%20candidate%20text%20has%20been%20written%20by%20one%20specific%20author%20or%0Aby%20someone%20else.%20It%20has%20been%20shown%20that%20many%20AV%20systems%20are%20vulnerable%20to%0Aadversarial%20attacks%2C%20where%20a%20malicious%20author%20actively%20tries%20to%20fool%20the%0Aclassifier%20by%20either%20concealing%20their%20writing%20style%2C%20or%20by%20imitating%20the%20style%0Aof%20another%20author.%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20benefits%20of%0Aaugmenting%20the%20classifier%20training%20set%20with%20%28negative%29%20synthetic%20examples.%0AThese%20synthetic%20examples%20are%20generated%20to%20imitate%20the%20style%20of%20the%20author%20of%0Ainterest.%20We%20analyze%20the%20improvements%20in%20classifier%20prediction%20that%20this%0Aaugmentation%20brings%20to%20bear%20in%20the%20task%20of%20AV%20in%20an%20adversarial%20setting.%20In%0Aparticular%2C%20we%20experiment%20with%20three%20different%20generator%20architectures%20%28one%0Abased%20on%20Recurrent%20Neural%20Networks%2C%20another%20based%20on%20small-scale%20transformers%2C%0Aand%20another%20based%20on%20the%20popular%20GPT%20model%29%20and%20with%20two%20training%20strategies%0A%28one%20inspired%20by%20standard%20Language%20Models%2C%20and%20another%20inspired%20by%20Wasserstein%0AGenerative%20Adversarial%20Networks%29.%20We%20evaluate%20our%20hypothesis%20on%20five%20datasets%0A%28three%20of%20which%20have%20been%20specifically%20collected%20to%20represent%20an%20adversarial%0Asetting%29%20and%20using%20two%20learning%20algorithms%20for%20the%20AV%20classifier%20%28Support%0AVector%20Machines%20and%20Convolutional%20Neural%20Networks%29.%20This%20experimentation%20has%0Ayielded%20negative%20results%2C%20revealing%20that%2C%20although%20our%20methodology%20proves%0Aeffective%20in%20many%20adversarial%20settings%2C%20its%20benefits%20are%20too%20sporadic%20for%20a%0Apragmatical%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForging%2520the%2520Forger%253A%2520An%2520Attempt%2520to%2520Improve%2520Authorship%2520Verification%2520via%250A%2520%2520Data%2520Augmentation%26entry.906535625%3DSilvia%2520Corbara%2520and%2520Alejandro%2520Moreo%26entry.1292438233%3D%2520%2520Authorship%2520Verification%2520%2528AV%2529%2520is%2520a%2520text%2520classification%2520task%2520concerned%2520with%250Ainferring%2520whether%2520a%2520candidate%2520text%2520has%2520been%2520written%2520by%2520one%2520specific%2520author%2520or%250Aby%2520someone%2520else.%2520It%2520has%2520been%2520shown%2520that%2520many%2520AV%2520systems%2520are%2520vulnerable%2520to%250Aadversarial%2520attacks%252C%2520where%2520a%2520malicious%2520author%2520actively%2520tries%2520to%2520fool%2520the%250Aclassifier%2520by%2520either%2520concealing%2520their%2520writing%2520style%252C%2520or%2520by%2520imitating%2520the%2520style%250Aof%2520another%2520author.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520potential%2520benefits%2520of%250Aaugmenting%2520the%2520classifier%2520training%2520set%2520with%2520%2528negative%2529%2520synthetic%2520examples.%250AThese%2520synthetic%2520examples%2520are%2520generated%2520to%2520imitate%2520the%2520style%2520of%2520the%2520author%2520of%250Ainterest.%2520We%2520analyze%2520the%2520improvements%2520in%2520classifier%2520prediction%2520that%2520this%250Aaugmentation%2520brings%2520to%2520bear%2520in%2520the%2520task%2520of%2520AV%2520in%2520an%2520adversarial%2520setting.%2520In%250Aparticular%252C%2520we%2520experiment%2520with%2520three%2520different%2520generator%2520architectures%2520%2528one%250Abased%2520on%2520Recurrent%2520Neural%2520Networks%252C%2520another%2520based%2520on%2520small-scale%2520transformers%252C%250Aand%2520another%2520based%2520on%2520the%2520popular%2520GPT%2520model%2529%2520and%2520with%2520two%2520training%2520strategies%250A%2528one%2520inspired%2520by%2520standard%2520Language%2520Models%252C%2520and%2520another%2520inspired%2520by%2520Wasserstein%250AGenerative%2520Adversarial%2520Networks%2529.%2520We%2520evaluate%2520our%2520hypothesis%2520on%2520five%2520datasets%250A%2528three%2520of%2520which%2520have%2520been%2520specifically%2520collected%2520to%2520represent%2520an%2520adversarial%250Asetting%2529%2520and%2520using%2520two%2520learning%2520algorithms%2520for%2520the%2520AV%2520classifier%2520%2528Support%250AVector%2520Machines%2520and%2520Convolutional%2520Neural%2520Networks%2529.%2520This%2520experimentation%2520has%250Ayielded%2520negative%2520results%252C%2520revealing%2520that%252C%2520although%2520our%2520methodology%2520proves%250Aeffective%2520in%2520many%2520adversarial%2520settings%252C%2520its%2520benefits%2520are%2520too%2520sporadic%2520for%2520a%250Apragmatical%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forging%20the%20Forger%3A%20An%20Attempt%20to%20Improve%20Authorship%20Verification%20via%0A%20%20Data%20Augmentation&entry.906535625=Silvia%20Corbara%20and%20Alejandro%20Moreo&entry.1292438233=%20%20Authorship%20Verification%20%28AV%29%20is%20a%20text%20classification%20task%20concerned%20with%0Ainferring%20whether%20a%20candidate%20text%20has%20been%20written%20by%20one%20specific%20author%20or%0Aby%20someone%20else.%20It%20has%20been%20shown%20that%20many%20AV%20systems%20are%20vulnerable%20to%0Aadversarial%20attacks%2C%20where%20a%20malicious%20author%20actively%20tries%20to%20fool%20the%0Aclassifier%20by%20either%20concealing%20their%20writing%20style%2C%20or%20by%20imitating%20the%20style%0Aof%20another%20author.%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20benefits%20of%0Aaugmenting%20the%20classifier%20training%20set%20with%20%28negative%29%20synthetic%20examples.%0AThese%20synthetic%20examples%20are%20generated%20to%20imitate%20the%20style%20of%20the%20author%20of%0Ainterest.%20We%20analyze%20the%20improvements%20in%20classifier%20prediction%20that%20this%0Aaugmentation%20brings%20to%20bear%20in%20the%20task%20of%20AV%20in%20an%20adversarial%20setting.%20In%0Aparticular%2C%20we%20experiment%20with%20three%20different%20generator%20architectures%20%28one%0Abased%20on%20Recurrent%20Neural%20Networks%2C%20another%20based%20on%20small-scale%20transformers%2C%0Aand%20another%20based%20on%20the%20popular%20GPT%20model%29%20and%20with%20two%20training%20strategies%0A%28one%20inspired%20by%20standard%20Language%20Models%2C%20and%20another%20inspired%20by%20Wasserstein%0AGenerative%20Adversarial%20Networks%29.%20We%20evaluate%20our%20hypothesis%20on%20five%20datasets%0A%28three%20of%20which%20have%20been%20specifically%20collected%20to%20represent%20an%20adversarial%0Asetting%29%20and%20using%20two%20learning%20algorithms%20for%20the%20AV%20classifier%20%28Support%0AVector%20Machines%20and%20Convolutional%20Neural%20Networks%29.%20This%20experimentation%20has%0Ayielded%20negative%20results%2C%20revealing%20that%2C%20although%20our%20methodology%20proves%0Aeffective%20in%20many%20adversarial%20settings%2C%20its%20benefits%20are%20too%20sporadic%20for%20a%0Apragmatical%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11265v2&entry.124074799=Read"},
{"title": "DISCERN: Decoding Systematic Errors in Natural Language for Text\n  Classifiers", "author": "Rakesh R. Menon and Shashank Srivastava", "abstract": "  Despite their high predictive accuracies, current machine learning systems\noften exhibit systematic biases stemming from annotation artifacts or\ninsufficient support for certain classes in the dataset. Recent work proposes\nautomatic methods for identifying and explaining systematic biases using\nkeywords. We introduce DISCERN, a framework for interpreting systematic biases\nin text classifiers using language explanations. DISCERN iteratively generates\nprecise natural language descriptions of systematic errors by employing an\ninteractive loop between two large language models. Finally, we use the\ndescriptions to improve classifiers by augmenting classifier training sets with\nsynthetically generated instances or annotated examples via active learning. On\nthree text-classification datasets, we demonstrate that language explanations\nfrom our framework induce consistent performance improvements that go beyond\nwhat is achievable with exemplars of systematic bias. Finally, in human\nevaluations, we show that users can interpret systematic biases more\neffectively (by over 25% relative) and efficiently when described through\nlanguage explanations as opposed to cluster exemplars.\n", "link": "http://arxiv.org/abs/2410.22239v1", "date": "2024-10-29", "relevancy": 2.0382, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISCERN%3A%20Decoding%20Systematic%20Errors%20in%20Natural%20Language%20for%20Text%0A%20%20Classifiers&body=Title%3A%20DISCERN%3A%20Decoding%20Systematic%20Errors%20in%20Natural%20Language%20for%20Text%0A%20%20Classifiers%0AAuthor%3A%20Rakesh%20R.%20Menon%20and%20Shashank%20Srivastava%0AAbstract%3A%20%20%20Despite%20their%20high%20predictive%20accuracies%2C%20current%20machine%20learning%20systems%0Aoften%20exhibit%20systematic%20biases%20stemming%20from%20annotation%20artifacts%20or%0Ainsufficient%20support%20for%20certain%20classes%20in%20the%20dataset.%20Recent%20work%20proposes%0Aautomatic%20methods%20for%20identifying%20and%20explaining%20systematic%20biases%20using%0Akeywords.%20We%20introduce%20DISCERN%2C%20a%20framework%20for%20interpreting%20systematic%20biases%0Ain%20text%20classifiers%20using%20language%20explanations.%20DISCERN%20iteratively%20generates%0Aprecise%20natural%20language%20descriptions%20of%20systematic%20errors%20by%20employing%20an%0Ainteractive%20loop%20between%20two%20large%20language%20models.%20Finally%2C%20we%20use%20the%0Adescriptions%20to%20improve%20classifiers%20by%20augmenting%20classifier%20training%20sets%20with%0Asynthetically%20generated%20instances%20or%20annotated%20examples%20via%20active%20learning.%20On%0Athree%20text-classification%20datasets%2C%20we%20demonstrate%20that%20language%20explanations%0Afrom%20our%20framework%20induce%20consistent%20performance%20improvements%20that%20go%20beyond%0Awhat%20is%20achievable%20with%20exemplars%20of%20systematic%20bias.%20Finally%2C%20in%20human%0Aevaluations%2C%20we%20show%20that%20users%20can%20interpret%20systematic%20biases%20more%0Aeffectively%20%28by%20over%2025%25%20relative%29%20and%20efficiently%20when%20described%20through%0Alanguage%20explanations%20as%20opposed%20to%20cluster%20exemplars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISCERN%253A%2520Decoding%2520Systematic%2520Errors%2520in%2520Natural%2520Language%2520for%2520Text%250A%2520%2520Classifiers%26entry.906535625%3DRakesh%2520R.%2520Menon%2520and%2520Shashank%2520Srivastava%26entry.1292438233%3D%2520%2520Despite%2520their%2520high%2520predictive%2520accuracies%252C%2520current%2520machine%2520learning%2520systems%250Aoften%2520exhibit%2520systematic%2520biases%2520stemming%2520from%2520annotation%2520artifacts%2520or%250Ainsufficient%2520support%2520for%2520certain%2520classes%2520in%2520the%2520dataset.%2520Recent%2520work%2520proposes%250Aautomatic%2520methods%2520for%2520identifying%2520and%2520explaining%2520systematic%2520biases%2520using%250Akeywords.%2520We%2520introduce%2520DISCERN%252C%2520a%2520framework%2520for%2520interpreting%2520systematic%2520biases%250Ain%2520text%2520classifiers%2520using%2520language%2520explanations.%2520DISCERN%2520iteratively%2520generates%250Aprecise%2520natural%2520language%2520descriptions%2520of%2520systematic%2520errors%2520by%2520employing%2520an%250Ainteractive%2520loop%2520between%2520two%2520large%2520language%2520models.%2520Finally%252C%2520we%2520use%2520the%250Adescriptions%2520to%2520improve%2520classifiers%2520by%2520augmenting%2520classifier%2520training%2520sets%2520with%250Asynthetically%2520generated%2520instances%2520or%2520annotated%2520examples%2520via%2520active%2520learning.%2520On%250Athree%2520text-classification%2520datasets%252C%2520we%2520demonstrate%2520that%2520language%2520explanations%250Afrom%2520our%2520framework%2520induce%2520consistent%2520performance%2520improvements%2520that%2520go%2520beyond%250Awhat%2520is%2520achievable%2520with%2520exemplars%2520of%2520systematic%2520bias.%2520Finally%252C%2520in%2520human%250Aevaluations%252C%2520we%2520show%2520that%2520users%2520can%2520interpret%2520systematic%2520biases%2520more%250Aeffectively%2520%2528by%2520over%252025%2525%2520relative%2529%2520and%2520efficiently%2520when%2520described%2520through%250Alanguage%2520explanations%2520as%2520opposed%2520to%2520cluster%2520exemplars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISCERN%3A%20Decoding%20Systematic%20Errors%20in%20Natural%20Language%20for%20Text%0A%20%20Classifiers&entry.906535625=Rakesh%20R.%20Menon%20and%20Shashank%20Srivastava&entry.1292438233=%20%20Despite%20their%20high%20predictive%20accuracies%2C%20current%20machine%20learning%20systems%0Aoften%20exhibit%20systematic%20biases%20stemming%20from%20annotation%20artifacts%20or%0Ainsufficient%20support%20for%20certain%20classes%20in%20the%20dataset.%20Recent%20work%20proposes%0Aautomatic%20methods%20for%20identifying%20and%20explaining%20systematic%20biases%20using%0Akeywords.%20We%20introduce%20DISCERN%2C%20a%20framework%20for%20interpreting%20systematic%20biases%0Ain%20text%20classifiers%20using%20language%20explanations.%20DISCERN%20iteratively%20generates%0Aprecise%20natural%20language%20descriptions%20of%20systematic%20errors%20by%20employing%20an%0Ainteractive%20loop%20between%20two%20large%20language%20models.%20Finally%2C%20we%20use%20the%0Adescriptions%20to%20improve%20classifiers%20by%20augmenting%20classifier%20training%20sets%20with%0Asynthetically%20generated%20instances%20or%20annotated%20examples%20via%20active%20learning.%20On%0Athree%20text-classification%20datasets%2C%20we%20demonstrate%20that%20language%20explanations%0Afrom%20our%20framework%20induce%20consistent%20performance%20improvements%20that%20go%20beyond%0Awhat%20is%20achievable%20with%20exemplars%20of%20systematic%20bias.%20Finally%2C%20in%20human%0Aevaluations%2C%20we%20show%20that%20users%20can%20interpret%20systematic%20biases%20more%0Aeffectively%20%28by%20over%2025%25%20relative%29%20and%20efficiently%20when%20described%20through%0Alanguage%20explanations%20as%20opposed%20to%20cluster%20exemplars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22239v1&entry.124074799=Read"},
{"title": "Very Attentive Tacotron: Robust and Unbounded Length Generalization in\n  Autoregressive Transformer-Based Text-to-Speech", "author": "Eric Battenberg and RJ Skerry-Ryan and Daisy Stanton and Soroosh Mariooryad and Matt Shannon and Julian Salazar and David Kao", "abstract": "  Autoregressive (AR) Transformer-based sequence models are known to have\ndifficulty generalizing to sequences longer than those seen during training.\nWhen applied to text-to-speech (TTS), these models tend to drop or repeat words\nor produce erratic output, especially for longer utterances. In this paper, we\nintroduce enhancements aimed at AR Transformer-based encoder-decoder TTS\nsystems that address these robustness and length generalization issues. Our\napproach uses an alignment mechanism to provide cross-attention operations with\nrelative location information. The associated alignment position is learned as\na latent property of the model via backprop and requires no external alignment\ninformation during training. While the approach is tailored to the monotonic\nnature of TTS input-output alignment, it is still able to benefit from the\nflexible modeling power of interleaved multi-head self- and cross-attention\noperations. A system incorporating these improvements, which we call Very\nAttentive Tacotron, matches the naturalness and expressiveness of a baseline\nT5-based TTS system, while eliminating problems with repeated or dropped words\nand enabling generalization to any practical utterance length.\n", "link": "http://arxiv.org/abs/2410.22179v1", "date": "2024-10-29", "relevancy": 1.487, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5132}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Very%20Attentive%20Tacotron%3A%20Robust%20and%20Unbounded%20Length%20Generalization%20in%0A%20%20Autoregressive%20Transformer-Based%20Text-to-Speech&body=Title%3A%20Very%20Attentive%20Tacotron%3A%20Robust%20and%20Unbounded%20Length%20Generalization%20in%0A%20%20Autoregressive%20Transformer-Based%20Text-to-Speech%0AAuthor%3A%20Eric%20Battenberg%20and%20RJ%20Skerry-Ryan%20and%20Daisy%20Stanton%20and%20Soroosh%20Mariooryad%20and%20Matt%20Shannon%20and%20Julian%20Salazar%20and%20David%20Kao%0AAbstract%3A%20%20%20Autoregressive%20%28AR%29%20Transformer-based%20sequence%20models%20are%20known%20to%20have%0Adifficulty%20generalizing%20to%20sequences%20longer%20than%20those%20seen%20during%20training.%0AWhen%20applied%20to%20text-to-speech%20%28TTS%29%2C%20these%20models%20tend%20to%20drop%20or%20repeat%20words%0Aor%20produce%20erratic%20output%2C%20especially%20for%20longer%20utterances.%20In%20this%20paper%2C%20we%0Aintroduce%20enhancements%20aimed%20at%20AR%20Transformer-based%20encoder-decoder%20TTS%0Asystems%20that%20address%20these%20robustness%20and%20length%20generalization%20issues.%20Our%0Aapproach%20uses%20an%20alignment%20mechanism%20to%20provide%20cross-attention%20operations%20with%0Arelative%20location%20information.%20The%20associated%20alignment%20position%20is%20learned%20as%0Aa%20latent%20property%20of%20the%20model%20via%20backprop%20and%20requires%20no%20external%20alignment%0Ainformation%20during%20training.%20While%20the%20approach%20is%20tailored%20to%20the%20monotonic%0Anature%20of%20TTS%20input-output%20alignment%2C%20it%20is%20still%20able%20to%20benefit%20from%20the%0Aflexible%20modeling%20power%20of%20interleaved%20multi-head%20self-%20and%20cross-attention%0Aoperations.%20A%20system%20incorporating%20these%20improvements%2C%20which%20we%20call%20Very%0AAttentive%20Tacotron%2C%20matches%20the%20naturalness%20and%20expressiveness%20of%20a%20baseline%0AT5-based%20TTS%20system%2C%20while%20eliminating%20problems%20with%20repeated%20or%20dropped%20words%0Aand%20enabling%20generalization%20to%20any%20practical%20utterance%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVery%2520Attentive%2520Tacotron%253A%2520Robust%2520and%2520Unbounded%2520Length%2520Generalization%2520in%250A%2520%2520Autoregressive%2520Transformer-Based%2520Text-to-Speech%26entry.906535625%3DEric%2520Battenberg%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Daisy%2520Stanton%2520and%2520Soroosh%2520Mariooryad%2520and%2520Matt%2520Shannon%2520and%2520Julian%2520Salazar%2520and%2520David%2520Kao%26entry.1292438233%3D%2520%2520Autoregressive%2520%2528AR%2529%2520Transformer-based%2520sequence%2520models%2520are%2520known%2520to%2520have%250Adifficulty%2520generalizing%2520to%2520sequences%2520longer%2520than%2520those%2520seen%2520during%2520training.%250AWhen%2520applied%2520to%2520text-to-speech%2520%2528TTS%2529%252C%2520these%2520models%2520tend%2520to%2520drop%2520or%2520repeat%2520words%250Aor%2520produce%2520erratic%2520output%252C%2520especially%2520for%2520longer%2520utterances.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520enhancements%2520aimed%2520at%2520AR%2520Transformer-based%2520encoder-decoder%2520TTS%250Asystems%2520that%2520address%2520these%2520robustness%2520and%2520length%2520generalization%2520issues.%2520Our%250Aapproach%2520uses%2520an%2520alignment%2520mechanism%2520to%2520provide%2520cross-attention%2520operations%2520with%250Arelative%2520location%2520information.%2520The%2520associated%2520alignment%2520position%2520is%2520learned%2520as%250Aa%2520latent%2520property%2520of%2520the%2520model%2520via%2520backprop%2520and%2520requires%2520no%2520external%2520alignment%250Ainformation%2520during%2520training.%2520While%2520the%2520approach%2520is%2520tailored%2520to%2520the%2520monotonic%250Anature%2520of%2520TTS%2520input-output%2520alignment%252C%2520it%2520is%2520still%2520able%2520to%2520benefit%2520from%2520the%250Aflexible%2520modeling%2520power%2520of%2520interleaved%2520multi-head%2520self-%2520and%2520cross-attention%250Aoperations.%2520A%2520system%2520incorporating%2520these%2520improvements%252C%2520which%2520we%2520call%2520Very%250AAttentive%2520Tacotron%252C%2520matches%2520the%2520naturalness%2520and%2520expressiveness%2520of%2520a%2520baseline%250AT5-based%2520TTS%2520system%252C%2520while%2520eliminating%2520problems%2520with%2520repeated%2520or%2520dropped%2520words%250Aand%2520enabling%2520generalization%2520to%2520any%2520practical%2520utterance%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Very%20Attentive%20Tacotron%3A%20Robust%20and%20Unbounded%20Length%20Generalization%20in%0A%20%20Autoregressive%20Transformer-Based%20Text-to-Speech&entry.906535625=Eric%20Battenberg%20and%20RJ%20Skerry-Ryan%20and%20Daisy%20Stanton%20and%20Soroosh%20Mariooryad%20and%20Matt%20Shannon%20and%20Julian%20Salazar%20and%20David%20Kao&entry.1292438233=%20%20Autoregressive%20%28AR%29%20Transformer-based%20sequence%20models%20are%20known%20to%20have%0Adifficulty%20generalizing%20to%20sequences%20longer%20than%20those%20seen%20during%20training.%0AWhen%20applied%20to%20text-to-speech%20%28TTS%29%2C%20these%20models%20tend%20to%20drop%20or%20repeat%20words%0Aor%20produce%20erratic%20output%2C%20especially%20for%20longer%20utterances.%20In%20this%20paper%2C%20we%0Aintroduce%20enhancements%20aimed%20at%20AR%20Transformer-based%20encoder-decoder%20TTS%0Asystems%20that%20address%20these%20robustness%20and%20length%20generalization%20issues.%20Our%0Aapproach%20uses%20an%20alignment%20mechanism%20to%20provide%20cross-attention%20operations%20with%0Arelative%20location%20information.%20The%20associated%20alignment%20position%20is%20learned%20as%0Aa%20latent%20property%20of%20the%20model%20via%20backprop%20and%20requires%20no%20external%20alignment%0Ainformation%20during%20training.%20While%20the%20approach%20is%20tailored%20to%20the%20monotonic%0Anature%20of%20TTS%20input-output%20alignment%2C%20it%20is%20still%20able%20to%20benefit%20from%20the%0Aflexible%20modeling%20power%20of%20interleaved%20multi-head%20self-%20and%20cross-attention%0Aoperations.%20A%20system%20incorporating%20these%20improvements%2C%20which%20we%20call%20Very%0AAttentive%20Tacotron%2C%20matches%20the%20naturalness%20and%20expressiveness%20of%20a%20baseline%0AT5-based%20TTS%20system%2C%20while%20eliminating%20problems%20with%20repeated%20or%20dropped%20words%0Aand%20enabling%20generalization%20to%20any%20practical%20utterance%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22179v1&entry.124074799=Read"},
{"title": "Fourier Head: Helping Large Language Models Learn Complex Probability\n  Distributions", "author": "Nate Gillman and Daksh Aggarwal and Michael Freeman and Saurabh Singh and Chen Sun", "abstract": "  As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns by 46% on the Atari Seaquest game, and increases a\nstate-of-the-art times series foundation model's forecasting performance by\n3.5% across 20 benchmarks unseen during training.\n", "link": "http://arxiv.org/abs/2410.22269v1", "date": "2024-10-29", "relevancy": 1.5694, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5784}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5149}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Head%3A%20Helping%20Large%20Language%20Models%20Learn%20Complex%20Probability%0A%20%20Distributions&body=Title%3A%20Fourier%20Head%3A%20Helping%20Large%20Language%20Models%20Learn%20Complex%20Probability%0A%20%20Distributions%0AAuthor%3A%20Nate%20Gillman%20and%20Daksh%20Aggarwal%20and%20Michael%20Freeman%20and%20Saurabh%20Singh%20and%20Chen%20Sun%0AAbstract%3A%20%20%20As%20the%20quality%20of%20large%20language%20models%20has%20improved%2C%20there%20has%20been%0Aincreased%20interest%20in%20using%20them%20to%20model%20non-linguistic%20tokens.%20For%20example%2C%0Athe%20Decision%20Transformer%20recasts%20agentic%20decision%20making%20as%20a%20sequence%20modeling%0Aproblem%2C%20using%20a%20decoder-only%20LLM%20to%20model%20the%20distribution%20over%20the%20discrete%0Aaction%20space%20for%20an%20Atari%20agent.%20However%2C%20when%20adapting%20LLMs%20to%20non-linguistic%0Adomains%2C%20it%20remains%20unclear%20if%20softmax%20over%20discrete%20bins%20captures%20the%0Acontinuous%20structure%20of%20the%20tokens%20and%20the%20potentially%20complex%20distributions%0Aneeded%20for%20high%20quality%20token%20generation.%20We%20introduce%20a%20neural%20network%20layer%2C%0Aconstructed%20using%20Fourier%20series%2C%20which%20we%20can%20easily%20substitute%20for%20any%20linear%0Alayer%20if%20we%20want%20the%20outputs%20to%20have%20a%20more%20continuous%20structure.%20We%20perform%0Aextensive%20analysis%20on%20synthetic%20datasets%2C%20as%20well%20as%20on%20large-scale%20decision%0Amaking%20and%20time%20series%20forecasting%20tasks.%20We%20also%20provide%20theoretical%20evidence%0Athat%20this%20layer%20can%20better%20learn%20signal%20from%20data%20while%20ignoring%20high-frequency%0Anoise.%20All%20of%20our%20results%20support%20the%20effectiveness%20of%20our%20proposed%20Fourier%0Ahead%20in%20scenarios%20where%20the%20underlying%20data%20distribution%20has%20a%20natural%0Acontinuous%20structure.%20For%20example%2C%20the%20Fourier%20head%20improves%20a%20Decision%0ATransformer%20agent%27s%20returns%20by%2046%25%20on%20the%20Atari%20Seaquest%20game%2C%20and%20increases%20a%0Astate-of-the-art%20times%20series%20foundation%20model%27s%20forecasting%20performance%20by%0A3.5%25%20across%2020%20benchmarks%20unseen%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Head%253A%2520Helping%2520Large%2520Language%2520Models%2520Learn%2520Complex%2520Probability%250A%2520%2520Distributions%26entry.906535625%3DNate%2520Gillman%2520and%2520Daksh%2520Aggarwal%2520and%2520Michael%2520Freeman%2520and%2520Saurabh%2520Singh%2520and%2520Chen%2520Sun%26entry.1292438233%3D%2520%2520As%2520the%2520quality%2520of%2520large%2520language%2520models%2520has%2520improved%252C%2520there%2520has%2520been%250Aincreased%2520interest%2520in%2520using%2520them%2520to%2520model%2520non-linguistic%2520tokens.%2520For%2520example%252C%250Athe%2520Decision%2520Transformer%2520recasts%2520agentic%2520decision%2520making%2520as%2520a%2520sequence%2520modeling%250Aproblem%252C%2520using%2520a%2520decoder-only%2520LLM%2520to%2520model%2520the%2520distribution%2520over%2520the%2520discrete%250Aaction%2520space%2520for%2520an%2520Atari%2520agent.%2520However%252C%2520when%2520adapting%2520LLMs%2520to%2520non-linguistic%250Adomains%252C%2520it%2520remains%2520unclear%2520if%2520softmax%2520over%2520discrete%2520bins%2520captures%2520the%250Acontinuous%2520structure%2520of%2520the%2520tokens%2520and%2520the%2520potentially%2520complex%2520distributions%250Aneeded%2520for%2520high%2520quality%2520token%2520generation.%2520We%2520introduce%2520a%2520neural%2520network%2520layer%252C%250Aconstructed%2520using%2520Fourier%2520series%252C%2520which%2520we%2520can%2520easily%2520substitute%2520for%2520any%2520linear%250Alayer%2520if%2520we%2520want%2520the%2520outputs%2520to%2520have%2520a%2520more%2520continuous%2520structure.%2520We%2520perform%250Aextensive%2520analysis%2520on%2520synthetic%2520datasets%252C%2520as%2520well%2520as%2520on%2520large-scale%2520decision%250Amaking%2520and%2520time%2520series%2520forecasting%2520tasks.%2520We%2520also%2520provide%2520theoretical%2520evidence%250Athat%2520this%2520layer%2520can%2520better%2520learn%2520signal%2520from%2520data%2520while%2520ignoring%2520high-frequency%250Anoise.%2520All%2520of%2520our%2520results%2520support%2520the%2520effectiveness%2520of%2520our%2520proposed%2520Fourier%250Ahead%2520in%2520scenarios%2520where%2520the%2520underlying%2520data%2520distribution%2520has%2520a%2520natural%250Acontinuous%2520structure.%2520For%2520example%252C%2520the%2520Fourier%2520head%2520improves%2520a%2520Decision%250ATransformer%2520agent%2527s%2520returns%2520by%252046%2525%2520on%2520the%2520Atari%2520Seaquest%2520game%252C%2520and%2520increases%2520a%250Astate-of-the-art%2520times%2520series%2520foundation%2520model%2527s%2520forecasting%2520performance%2520by%250A3.5%2525%2520across%252020%2520benchmarks%2520unseen%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Head%3A%20Helping%20Large%20Language%20Models%20Learn%20Complex%20Probability%0A%20%20Distributions&entry.906535625=Nate%20Gillman%20and%20Daksh%20Aggarwal%20and%20Michael%20Freeman%20and%20Saurabh%20Singh%20and%20Chen%20Sun&entry.1292438233=%20%20As%20the%20quality%20of%20large%20language%20models%20has%20improved%2C%20there%20has%20been%0Aincreased%20interest%20in%20using%20them%20to%20model%20non-linguistic%20tokens.%20For%20example%2C%0Athe%20Decision%20Transformer%20recasts%20agentic%20decision%20making%20as%20a%20sequence%20modeling%0Aproblem%2C%20using%20a%20decoder-only%20LLM%20to%20model%20the%20distribution%20over%20the%20discrete%0Aaction%20space%20for%20an%20Atari%20agent.%20However%2C%20when%20adapting%20LLMs%20to%20non-linguistic%0Adomains%2C%20it%20remains%20unclear%20if%20softmax%20over%20discrete%20bins%20captures%20the%0Acontinuous%20structure%20of%20the%20tokens%20and%20the%20potentially%20complex%20distributions%0Aneeded%20for%20high%20quality%20token%20generation.%20We%20introduce%20a%20neural%20network%20layer%2C%0Aconstructed%20using%20Fourier%20series%2C%20which%20we%20can%20easily%20substitute%20for%20any%20linear%0Alayer%20if%20we%20want%20the%20outputs%20to%20have%20a%20more%20continuous%20structure.%20We%20perform%0Aextensive%20analysis%20on%20synthetic%20datasets%2C%20as%20well%20as%20on%20large-scale%20decision%0Amaking%20and%20time%20series%20forecasting%20tasks.%20We%20also%20provide%20theoretical%20evidence%0Athat%20this%20layer%20can%20better%20learn%20signal%20from%20data%20while%20ignoring%20high-frequency%0Anoise.%20All%20of%20our%20results%20support%20the%20effectiveness%20of%20our%20proposed%20Fourier%0Ahead%20in%20scenarios%20where%20the%20underlying%20data%20distribution%20has%20a%20natural%0Acontinuous%20structure.%20For%20example%2C%20the%20Fourier%20head%20improves%20a%20Decision%0ATransformer%20agent%27s%20returns%20by%2046%25%20on%20the%20Atari%20Seaquest%20game%2C%20and%20increases%20a%0Astate-of-the-art%20times%20series%20foundation%20model%27s%20forecasting%20performance%20by%0A3.5%25%20across%2020%20benchmarks%20unseen%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22269v1&entry.124074799=Read"},
{"title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management", "author": "Tuowei Wang and Ruwen Fan and Minxing Huang and Zixu Hao and Kun Li and Ting Cao and Youyou Lu and Yaoxue Zhang and Ju Ren", "abstract": "  Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.\n", "link": "http://arxiv.org/abs/2410.19274v2", "date": "2024-10-29", "relevancy": 1.4732, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5168}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ripple%3A%20Accelerating%20LLM%20Inference%20on%20Smartphones%20with%20Correlation-Aware%0A%20%20Neuron%20Management&body=Title%3A%20Ripple%3A%20Accelerating%20LLM%20Inference%20on%20Smartphones%20with%20Correlation-Aware%0A%20%20Neuron%20Management%0AAuthor%3A%20Tuowei%20Wang%20and%20Ruwen%20Fan%20and%20Minxing%20Huang%20and%20Zixu%20Hao%20and%20Kun%20Li%20and%20Ting%20Cao%20and%20Youyou%20Lu%20and%20Yaoxue%20Zhang%20and%20Ju%20Ren%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20various%0Adomains%2C%20yet%20deploying%20them%20on%20mobile%20devices%20remains%20an%20arduous%20challenge%20due%0Ato%20their%20extensive%20computational%20and%20memory%20demands.%20While%20lightweight%20LLMs%0Ahave%20been%20developed%20to%20fit%20mobile%20environments%2C%20they%20suffer%20from%20degraded%20model%0Aaccuracy.%20In%20contrast%2C%20sparsity-based%20techniques%20minimize%20DRAM%20usage%20by%0Aselectively%20transferring%20only%20relevant%20neurons%20to%20DRAM%20while%20retaining%20the%20full%0Amodel%20in%20external%20storage%2C%20such%20as%20flash.%20However%2C%20such%20approaches%20are%0Acritically%20limited%20by%20numerous%20I/O%20operations%2C%20particularly%20on%20smartphones%20with%0Asevere%20IOPS%20constraints.%0A%20%20In%20this%20paper%2C%20we%20propose%20Ripple%2C%20a%20novel%20approach%20that%20accelerates%20LLM%0Ainference%20on%20smartphones%20by%20optimizing%20neuron%20placement%20in%20flash%20memory.%20Ripple%0Aleverages%20the%20concept%20of%20Neuron%20Co-Activation%2C%20where%20neurons%20frequently%0Aactivated%20together%20are%20linked%20to%20facilitate%20continuous%20read%20access%20and%20optimize%0Adata%20transfer%20efficiency.%20Our%20approach%20incorporates%20a%20two-stage%20solution%3A%20an%0Aoffline%20stage%20that%20reorganizes%20neuron%20placement%20based%20on%20co-activation%0Apatterns%2C%20and%20an%20online%20stage%20that%20employs%20tailored%20data%20access%20and%20caching%0Astrategies%20to%20align%20well%20with%20hardware%20characteristics.%20Evaluations%20conducted%0Aon%20a%20variety%20of%20smartphones%20and%20LLMs%20demonstrate%20that%20Ripple%20achieves%20up%20to%0A5.93x%20improvements%20in%20I/O%20latency%20compared%20to%20the%20state-of-the-art.%20As%20the%0Afirst%20solution%20to%20optimize%20storage%20placement%20under%20sparsity%2C%20Ripple%20explores%20a%0Anew%20optimization%20space%20at%20the%20intersection%20of%20sparsity-driven%20algorithm%20and%0Astorage-level%20system%20co-design%20in%20LLM%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRipple%253A%2520Accelerating%2520LLM%2520Inference%2520on%2520Smartphones%2520with%2520Correlation-Aware%250A%2520%2520Neuron%2520Management%26entry.906535625%3DTuowei%2520Wang%2520and%2520Ruwen%2520Fan%2520and%2520Minxing%2520Huang%2520and%2520Zixu%2520Hao%2520and%2520Kun%2520Li%2520and%2520Ting%2520Cao%2520and%2520Youyou%2520Lu%2520and%2520Yaoxue%2520Zhang%2520and%2520Ju%2520Ren%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520various%250Adomains%252C%2520yet%2520deploying%2520them%2520on%2520mobile%2520devices%2520remains%2520an%2520arduous%2520challenge%2520due%250Ato%2520their%2520extensive%2520computational%2520and%2520memory%2520demands.%2520While%2520lightweight%2520LLMs%250Ahave%2520been%2520developed%2520to%2520fit%2520mobile%2520environments%252C%2520they%2520suffer%2520from%2520degraded%2520model%250Aaccuracy.%2520In%2520contrast%252C%2520sparsity-based%2520techniques%2520minimize%2520DRAM%2520usage%2520by%250Aselectively%2520transferring%2520only%2520relevant%2520neurons%2520to%2520DRAM%2520while%2520retaining%2520the%2520full%250Amodel%2520in%2520external%2520storage%252C%2520such%2520as%2520flash.%2520However%252C%2520such%2520approaches%2520are%250Acritically%2520limited%2520by%2520numerous%2520I/O%2520operations%252C%2520particularly%2520on%2520smartphones%2520with%250Asevere%2520IOPS%2520constraints.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Ripple%252C%2520a%2520novel%2520approach%2520that%2520accelerates%2520LLM%250Ainference%2520on%2520smartphones%2520by%2520optimizing%2520neuron%2520placement%2520in%2520flash%2520memory.%2520Ripple%250Aleverages%2520the%2520concept%2520of%2520Neuron%2520Co-Activation%252C%2520where%2520neurons%2520frequently%250Aactivated%2520together%2520are%2520linked%2520to%2520facilitate%2520continuous%2520read%2520access%2520and%2520optimize%250Adata%2520transfer%2520efficiency.%2520Our%2520approach%2520incorporates%2520a%2520two-stage%2520solution%253A%2520an%250Aoffline%2520stage%2520that%2520reorganizes%2520neuron%2520placement%2520based%2520on%2520co-activation%250Apatterns%252C%2520and%2520an%2520online%2520stage%2520that%2520employs%2520tailored%2520data%2520access%2520and%2520caching%250Astrategies%2520to%2520align%2520well%2520with%2520hardware%2520characteristics.%2520Evaluations%2520conducted%250Aon%2520a%2520variety%2520of%2520smartphones%2520and%2520LLMs%2520demonstrate%2520that%2520Ripple%2520achieves%2520up%2520to%250A5.93x%2520improvements%2520in%2520I/O%2520latency%2520compared%2520to%2520the%2520state-of-the-art.%2520As%2520the%250Afirst%2520solution%2520to%2520optimize%2520storage%2520placement%2520under%2520sparsity%252C%2520Ripple%2520explores%2520a%250Anew%2520optimization%2520space%2520at%2520the%2520intersection%2520of%2520sparsity-driven%2520algorithm%2520and%250Astorage-level%2520system%2520co-design%2520in%2520LLM%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ripple%3A%20Accelerating%20LLM%20Inference%20on%20Smartphones%20with%20Correlation-Aware%0A%20%20Neuron%20Management&entry.906535625=Tuowei%20Wang%20and%20Ruwen%20Fan%20and%20Minxing%20Huang%20and%20Zixu%20Hao%20and%20Kun%20Li%20and%20Ting%20Cao%20and%20Youyou%20Lu%20and%20Yaoxue%20Zhang%20and%20Ju%20Ren&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20various%0Adomains%2C%20yet%20deploying%20them%20on%20mobile%20devices%20remains%20an%20arduous%20challenge%20due%0Ato%20their%20extensive%20computational%20and%20memory%20demands.%20While%20lightweight%20LLMs%0Ahave%20been%20developed%20to%20fit%20mobile%20environments%2C%20they%20suffer%20from%20degraded%20model%0Aaccuracy.%20In%20contrast%2C%20sparsity-based%20techniques%20minimize%20DRAM%20usage%20by%0Aselectively%20transferring%20only%20relevant%20neurons%20to%20DRAM%20while%20retaining%20the%20full%0Amodel%20in%20external%20storage%2C%20such%20as%20flash.%20However%2C%20such%20approaches%20are%0Acritically%20limited%20by%20numerous%20I/O%20operations%2C%20particularly%20on%20smartphones%20with%0Asevere%20IOPS%20constraints.%0A%20%20In%20this%20paper%2C%20we%20propose%20Ripple%2C%20a%20novel%20approach%20that%20accelerates%20LLM%0Ainference%20on%20smartphones%20by%20optimizing%20neuron%20placement%20in%20flash%20memory.%20Ripple%0Aleverages%20the%20concept%20of%20Neuron%20Co-Activation%2C%20where%20neurons%20frequently%0Aactivated%20together%20are%20linked%20to%20facilitate%20continuous%20read%20access%20and%20optimize%0Adata%20transfer%20efficiency.%20Our%20approach%20incorporates%20a%20two-stage%20solution%3A%20an%0Aoffline%20stage%20that%20reorganizes%20neuron%20placement%20based%20on%20co-activation%0Apatterns%2C%20and%20an%20online%20stage%20that%20employs%20tailored%20data%20access%20and%20caching%0Astrategies%20to%20align%20well%20with%20hardware%20characteristics.%20Evaluations%20conducted%0Aon%20a%20variety%20of%20smartphones%20and%20LLMs%20demonstrate%20that%20Ripple%20achieves%20up%20to%0A5.93x%20improvements%20in%20I/O%20latency%20compared%20to%20the%20state-of-the-art.%20As%20the%0Afirst%20solution%20to%20optimize%20storage%20placement%20under%20sparsity%2C%20Ripple%20explores%20a%0Anew%20optimization%20space%20at%20the%20intersection%20of%20sparsity-driven%20algorithm%20and%0Astorage-level%20system%20co-design%20in%20LLM%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19274v2&entry.124074799=Read"},
{"title": "Analyzing Noise Models and Advanced Filtering Algorithms for Image\n  Enhancement", "author": "Sahil Ali Akbar and Ananya Verma", "abstract": "  Noise, an unwanted component in an image, can be the reason for the\ndegradation of Image at the time of transmission or capturing. Noise reduction\nfrom images is still a challenging task. Digital Image Processing is a\ncomponent of Digital signal processing. A wide variety of algorithms can be\nused in image processing to apply to an image or an input dataset and obtain\nimportant outcomes. In image processing research, removing noise from images\nbefore further analysis is essential. Post-noise removal of images improves\nclarity, enabling better interpretation and analysis across medical imaging,\nsatellite imagery, and radar applications. While numerous algorithms exist,\neach comes with its own assumptions, strengths, and limitations. The paper aims\nto evaluate the effectiveness of different filtering techniques on images with\neight types of noise. It evaluates methodologies like Wiener, Median, Gaussian,\nMean, Low pass, High pass, Laplacian and bilateral filtering, using the\nperformance metric Peak signal to noise ratio. It shows us the impact of\ndifferent filters on noise models by applying a variety of filters to various\nkinds of noise. Additionally, it also assists us in determining which filtering\nstrategy is most appropriate for a certain noise model based on the\ncircumstances.\n", "link": "http://arxiv.org/abs/2410.21946v1", "date": "2024-10-29", "relevancy": 1.5893, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5601}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4964}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Noise%20Models%20and%20Advanced%20Filtering%20Algorithms%20for%20Image%0A%20%20Enhancement&body=Title%3A%20Analyzing%20Noise%20Models%20and%20Advanced%20Filtering%20Algorithms%20for%20Image%0A%20%20Enhancement%0AAuthor%3A%20Sahil%20Ali%20Akbar%20and%20Ananya%20Verma%0AAbstract%3A%20%20%20Noise%2C%20an%20unwanted%20component%20in%20an%20image%2C%20can%20be%20the%20reason%20for%20the%0Adegradation%20of%20Image%20at%20the%20time%20of%20transmission%20or%20capturing.%20Noise%20reduction%0Afrom%20images%20is%20still%20a%20challenging%20task.%20Digital%20Image%20Processing%20is%20a%0Acomponent%20of%20Digital%20signal%20processing.%20A%20wide%20variety%20of%20algorithms%20can%20be%0Aused%20in%20image%20processing%20to%20apply%20to%20an%20image%20or%20an%20input%20dataset%20and%20obtain%0Aimportant%20outcomes.%20In%20image%20processing%20research%2C%20removing%20noise%20from%20images%0Abefore%20further%20analysis%20is%20essential.%20Post-noise%20removal%20of%20images%20improves%0Aclarity%2C%20enabling%20better%20interpretation%20and%20analysis%20across%20medical%20imaging%2C%0Asatellite%20imagery%2C%20and%20radar%20applications.%20While%20numerous%20algorithms%20exist%2C%0Aeach%20comes%20with%20its%20own%20assumptions%2C%20strengths%2C%20and%20limitations.%20The%20paper%20aims%0Ato%20evaluate%20the%20effectiveness%20of%20different%20filtering%20techniques%20on%20images%20with%0Aeight%20types%20of%20noise.%20It%20evaluates%20methodologies%20like%20Wiener%2C%20Median%2C%20Gaussian%2C%0AMean%2C%20Low%20pass%2C%20High%20pass%2C%20Laplacian%20and%20bilateral%20filtering%2C%20using%20the%0Aperformance%20metric%20Peak%20signal%20to%20noise%20ratio.%20It%20shows%20us%20the%20impact%20of%0Adifferent%20filters%20on%20noise%20models%20by%20applying%20a%20variety%20of%20filters%20to%20various%0Akinds%20of%20noise.%20Additionally%2C%20it%20also%20assists%20us%20in%20determining%20which%20filtering%0Astrategy%20is%20most%20appropriate%20for%20a%20certain%20noise%20model%20based%20on%20the%0Acircumstances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Noise%2520Models%2520and%2520Advanced%2520Filtering%2520Algorithms%2520for%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DSahil%2520Ali%2520Akbar%2520and%2520Ananya%2520Verma%26entry.1292438233%3D%2520%2520Noise%252C%2520an%2520unwanted%2520component%2520in%2520an%2520image%252C%2520can%2520be%2520the%2520reason%2520for%2520the%250Adegradation%2520of%2520Image%2520at%2520the%2520time%2520of%2520transmission%2520or%2520capturing.%2520Noise%2520reduction%250Afrom%2520images%2520is%2520still%2520a%2520challenging%2520task.%2520Digital%2520Image%2520Processing%2520is%2520a%250Acomponent%2520of%2520Digital%2520signal%2520processing.%2520A%2520wide%2520variety%2520of%2520algorithms%2520can%2520be%250Aused%2520in%2520image%2520processing%2520to%2520apply%2520to%2520an%2520image%2520or%2520an%2520input%2520dataset%2520and%2520obtain%250Aimportant%2520outcomes.%2520In%2520image%2520processing%2520research%252C%2520removing%2520noise%2520from%2520images%250Abefore%2520further%2520analysis%2520is%2520essential.%2520Post-noise%2520removal%2520of%2520images%2520improves%250Aclarity%252C%2520enabling%2520better%2520interpretation%2520and%2520analysis%2520across%2520medical%2520imaging%252C%250Asatellite%2520imagery%252C%2520and%2520radar%2520applications.%2520While%2520numerous%2520algorithms%2520exist%252C%250Aeach%2520comes%2520with%2520its%2520own%2520assumptions%252C%2520strengths%252C%2520and%2520limitations.%2520The%2520paper%2520aims%250Ato%2520evaluate%2520the%2520effectiveness%2520of%2520different%2520filtering%2520techniques%2520on%2520images%2520with%250Aeight%2520types%2520of%2520noise.%2520It%2520evaluates%2520methodologies%2520like%2520Wiener%252C%2520Median%252C%2520Gaussian%252C%250AMean%252C%2520Low%2520pass%252C%2520High%2520pass%252C%2520Laplacian%2520and%2520bilateral%2520filtering%252C%2520using%2520the%250Aperformance%2520metric%2520Peak%2520signal%2520to%2520noise%2520ratio.%2520It%2520shows%2520us%2520the%2520impact%2520of%250Adifferent%2520filters%2520on%2520noise%2520models%2520by%2520applying%2520a%2520variety%2520of%2520filters%2520to%2520various%250Akinds%2520of%2520noise.%2520Additionally%252C%2520it%2520also%2520assists%2520us%2520in%2520determining%2520which%2520filtering%250Astrategy%2520is%2520most%2520appropriate%2520for%2520a%2520certain%2520noise%2520model%2520based%2520on%2520the%250Acircumstances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Noise%20Models%20and%20Advanced%20Filtering%20Algorithms%20for%20Image%0A%20%20Enhancement&entry.906535625=Sahil%20Ali%20Akbar%20and%20Ananya%20Verma&entry.1292438233=%20%20Noise%2C%20an%20unwanted%20component%20in%20an%20image%2C%20can%20be%20the%20reason%20for%20the%0Adegradation%20of%20Image%20at%20the%20time%20of%20transmission%20or%20capturing.%20Noise%20reduction%0Afrom%20images%20is%20still%20a%20challenging%20task.%20Digital%20Image%20Processing%20is%20a%0Acomponent%20of%20Digital%20signal%20processing.%20A%20wide%20variety%20of%20algorithms%20can%20be%0Aused%20in%20image%20processing%20to%20apply%20to%20an%20image%20or%20an%20input%20dataset%20and%20obtain%0Aimportant%20outcomes.%20In%20image%20processing%20research%2C%20removing%20noise%20from%20images%0Abefore%20further%20analysis%20is%20essential.%20Post-noise%20removal%20of%20images%20improves%0Aclarity%2C%20enabling%20better%20interpretation%20and%20analysis%20across%20medical%20imaging%2C%0Asatellite%20imagery%2C%20and%20radar%20applications.%20While%20numerous%20algorithms%20exist%2C%0Aeach%20comes%20with%20its%20own%20assumptions%2C%20strengths%2C%20and%20limitations.%20The%20paper%20aims%0Ato%20evaluate%20the%20effectiveness%20of%20different%20filtering%20techniques%20on%20images%20with%0Aeight%20types%20of%20noise.%20It%20evaluates%20methodologies%20like%20Wiener%2C%20Median%2C%20Gaussian%2C%0AMean%2C%20Low%20pass%2C%20High%20pass%2C%20Laplacian%20and%20bilateral%20filtering%2C%20using%20the%0Aperformance%20metric%20Peak%20signal%20to%20noise%20ratio.%20It%20shows%20us%20the%20impact%20of%0Adifferent%20filters%20on%20noise%20models%20by%20applying%20a%20variety%20of%20filters%20to%20various%0Akinds%20of%20noise.%20Additionally%2C%20it%20also%20assists%20us%20in%20determining%20which%20filtering%0Astrategy%20is%20most%20appropriate%20for%20a%20certain%20noise%20model%20based%20on%20the%0Acircumstances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21946v1&entry.124074799=Read"},
{"title": "Benchmarking Counterfactual Image Generation", "author": "Thomas Melistas and Nikos Spyrou and Nefeli Gkouti and Pedro Sanchez and Athanasios Vlontzos and Yannis Panagakis and Giorgos Papanastasiou and Sotirios A. Tsaftaris", "abstract": "  Generative AI has revolutionised visual content editing, empowering users to\neffortlessly modify images and videos. However, not all edits are equal. To\nperform realistic edits in domains such as natural image or medical imaging,\nmodifications must respect causal relationships inherent to the data generation\nprocess. Such image editing falls into the counterfactual image generation\nregime. Evaluating counterfactual image generation is substantially complex:\nnot only it lacks observable ground truths, but also requires adherence to\ncausal constraints. Although several counterfactual image generation methods\nand evaluation metrics exist, a comprehensive comparison within a unified\nsetting is lacking. We present a comparison framework to thoroughly benchmark\ncounterfactual image generation methods. We integrate all models that have been\nused for the task at hand and expand them to novel datasets and causal graphs,\ndemonstrating the superiority of Hierarchical VAEs across most datasets and\nmetrics. Our framework is implemented in a user-friendly Python package that\ncan be extended to incorporate additional SCMs, causal methods, generative\nmodels, and datasets for the community to build on. Code:\nhttps://github.com/gulnazaki/counterfactual-benchmark.\n", "link": "http://arxiv.org/abs/2403.20287v3", "date": "2024-10-29", "relevancy": 1.6358, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6007}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5322}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Counterfactual%20Image%20Generation&body=Title%3A%20Benchmarking%20Counterfactual%20Image%20Generation%0AAuthor%3A%20Thomas%20Melistas%20and%20Nikos%20Spyrou%20and%20Nefeli%20Gkouti%20and%20Pedro%20Sanchez%20and%20Athanasios%20Vlontzos%20and%20Yannis%20Panagakis%20and%20Giorgos%20Papanastasiou%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Generative%20AI%20has%20revolutionised%20visual%20content%20editing%2C%20empowering%20users%20to%0Aeffortlessly%20modify%20images%20and%20videos.%20However%2C%20not%20all%20edits%20are%20equal.%20To%0Aperform%20realistic%20edits%20in%20domains%20such%20as%20natural%20image%20or%20medical%20imaging%2C%0Amodifications%20must%20respect%20causal%20relationships%20inherent%20to%20the%20data%20generation%0Aprocess.%20Such%20image%20editing%20falls%20into%20the%20counterfactual%20image%20generation%0Aregime.%20Evaluating%20counterfactual%20image%20generation%20is%20substantially%20complex%3A%0Anot%20only%20it%20lacks%20observable%20ground%20truths%2C%20but%20also%20requires%20adherence%20to%0Acausal%20constraints.%20Although%20several%20counterfactual%20image%20generation%20methods%0Aand%20evaluation%20metrics%20exist%2C%20a%20comprehensive%20comparison%20within%20a%20unified%0Asetting%20is%20lacking.%20We%20present%20a%20comparison%20framework%20to%20thoroughly%20benchmark%0Acounterfactual%20image%20generation%20methods.%20We%20integrate%20all%20models%20that%20have%20been%0Aused%20for%20the%20task%20at%20hand%20and%20expand%20them%20to%20novel%20datasets%20and%20causal%20graphs%2C%0Ademonstrating%20the%20superiority%20of%20Hierarchical%20VAEs%20across%20most%20datasets%20and%0Ametrics.%20Our%20framework%20is%20implemented%20in%20a%20user-friendly%20Python%20package%20that%0Acan%20be%20extended%20to%20incorporate%20additional%20SCMs%2C%20causal%20methods%2C%20generative%0Amodels%2C%20and%20datasets%20for%20the%20community%20to%20build%20on.%20Code%3A%0Ahttps%3A//github.com/gulnazaki/counterfactual-benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20287v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Counterfactual%2520Image%2520Generation%26entry.906535625%3DThomas%2520Melistas%2520and%2520Nikos%2520Spyrou%2520and%2520Nefeli%2520Gkouti%2520and%2520Pedro%2520Sanchez%2520and%2520Athanasios%2520Vlontzos%2520and%2520Yannis%2520Panagakis%2520and%2520Giorgos%2520Papanastasiou%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520revolutionised%2520visual%2520content%2520editing%252C%2520empowering%2520users%2520to%250Aeffortlessly%2520modify%2520images%2520and%2520videos.%2520However%252C%2520not%2520all%2520edits%2520are%2520equal.%2520To%250Aperform%2520realistic%2520edits%2520in%2520domains%2520such%2520as%2520natural%2520image%2520or%2520medical%2520imaging%252C%250Amodifications%2520must%2520respect%2520causal%2520relationships%2520inherent%2520to%2520the%2520data%2520generation%250Aprocess.%2520Such%2520image%2520editing%2520falls%2520into%2520the%2520counterfactual%2520image%2520generation%250Aregime.%2520Evaluating%2520counterfactual%2520image%2520generation%2520is%2520substantially%2520complex%253A%250Anot%2520only%2520it%2520lacks%2520observable%2520ground%2520truths%252C%2520but%2520also%2520requires%2520adherence%2520to%250Acausal%2520constraints.%2520Although%2520several%2520counterfactual%2520image%2520generation%2520methods%250Aand%2520evaluation%2520metrics%2520exist%252C%2520a%2520comprehensive%2520comparison%2520within%2520a%2520unified%250Asetting%2520is%2520lacking.%2520We%2520present%2520a%2520comparison%2520framework%2520to%2520thoroughly%2520benchmark%250Acounterfactual%2520image%2520generation%2520methods.%2520We%2520integrate%2520all%2520models%2520that%2520have%2520been%250Aused%2520for%2520the%2520task%2520at%2520hand%2520and%2520expand%2520them%2520to%2520novel%2520datasets%2520and%2520causal%2520graphs%252C%250Ademonstrating%2520the%2520superiority%2520of%2520Hierarchical%2520VAEs%2520across%2520most%2520datasets%2520and%250Ametrics.%2520Our%2520framework%2520is%2520implemented%2520in%2520a%2520user-friendly%2520Python%2520package%2520that%250Acan%2520be%2520extended%2520to%2520incorporate%2520additional%2520SCMs%252C%2520causal%2520methods%252C%2520generative%250Amodels%252C%2520and%2520datasets%2520for%2520the%2520community%2520to%2520build%2520on.%2520Code%253A%250Ahttps%253A//github.com/gulnazaki/counterfactual-benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20287v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Counterfactual%20Image%20Generation&entry.906535625=Thomas%20Melistas%20and%20Nikos%20Spyrou%20and%20Nefeli%20Gkouti%20and%20Pedro%20Sanchez%20and%20Athanasios%20Vlontzos%20and%20Yannis%20Panagakis%20and%20Giorgos%20Papanastasiou%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Generative%20AI%20has%20revolutionised%20visual%20content%20editing%2C%20empowering%20users%20to%0Aeffortlessly%20modify%20images%20and%20videos.%20However%2C%20not%20all%20edits%20are%20equal.%20To%0Aperform%20realistic%20edits%20in%20domains%20such%20as%20natural%20image%20or%20medical%20imaging%2C%0Amodifications%20must%20respect%20causal%20relationships%20inherent%20to%20the%20data%20generation%0Aprocess.%20Such%20image%20editing%20falls%20into%20the%20counterfactual%20image%20generation%0Aregime.%20Evaluating%20counterfactual%20image%20generation%20is%20substantially%20complex%3A%0Anot%20only%20it%20lacks%20observable%20ground%20truths%2C%20but%20also%20requires%20adherence%20to%0Acausal%20constraints.%20Although%20several%20counterfactual%20image%20generation%20methods%0Aand%20evaluation%20metrics%20exist%2C%20a%20comprehensive%20comparison%20within%20a%20unified%0Asetting%20is%20lacking.%20We%20present%20a%20comparison%20framework%20to%20thoroughly%20benchmark%0Acounterfactual%20image%20generation%20methods.%20We%20integrate%20all%20models%20that%20have%20been%0Aused%20for%20the%20task%20at%20hand%20and%20expand%20them%20to%20novel%20datasets%20and%20causal%20graphs%2C%0Ademonstrating%20the%20superiority%20of%20Hierarchical%20VAEs%20across%20most%20datasets%20and%0Ametrics.%20Our%20framework%20is%20implemented%20in%20a%20user-friendly%20Python%20package%20that%0Acan%20be%20extended%20to%20incorporate%20additional%20SCMs%2C%20causal%20methods%2C%20generative%0Amodels%2C%20and%20datasets%20for%20the%20community%20to%20build%20on.%20Code%3A%0Ahttps%3A//github.com/gulnazaki/counterfactual-benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20287v3&entry.124074799=Read"},
{"title": "Beyond Throughput and Compression Ratios: Towards High End-to-end\n  Utility of Gradient Compression", "author": "Wenchen Han and Shay Vargaftik and Michael Mitzenmacher and Brad Karp and Ran Ben Basat", "abstract": "  Gradient aggregation has long been identified as a major bottleneck in\ntoday's large-scale distributed machine learning training systems. One\npromising solution to mitigate such bottlenecks is gradient compression,\ndirectly reducing communicated gradient data volume. However, in practice, many\ngradient compression schemes do not achieve acceleration of the training\nprocess while also preserving accuracy.\n  In this work, we identify common issues in previous gradient compression\nsystems and evaluation methodologies. These include excessive computational\noverheads; incompatibility with all-reduce; and insufficient evaluation\nmethods, such as not using an end-to-end metric or using a 32-bit baseline\ninstead of the stronger 16-bit baseline. We revisit common compression\napproaches (sparsification, quantization, and low-rank decomposition) and\ndemonstrate how considering the above issues can lead to minor but strategic\ndesign changes, resulting in notably better performance. Our goal is to raise\nawareness of the need for design and evaluation standards that naturally\ntranslate to the end-to-end utility of gradient compression.\n", "link": "http://arxiv.org/abs/2407.01378v2", "date": "2024-10-29", "relevancy": 1.564, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5356}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5189}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Throughput%20and%20Compression%20Ratios%3A%20Towards%20High%20End-to-end%0A%20%20Utility%20of%20Gradient%20Compression&body=Title%3A%20Beyond%20Throughput%20and%20Compression%20Ratios%3A%20Towards%20High%20End-to-end%0A%20%20Utility%20of%20Gradient%20Compression%0AAuthor%3A%20Wenchen%20Han%20and%20Shay%20Vargaftik%20and%20Michael%20Mitzenmacher%20and%20Brad%20Karp%20and%20Ran%20Ben%20Basat%0AAbstract%3A%20%20%20Gradient%20aggregation%20has%20long%20been%20identified%20as%20a%20major%20bottleneck%20in%0Atoday%27s%20large-scale%20distributed%20machine%20learning%20training%20systems.%20One%0Apromising%20solution%20to%20mitigate%20such%20bottlenecks%20is%20gradient%20compression%2C%0Adirectly%20reducing%20communicated%20gradient%20data%20volume.%20However%2C%20in%20practice%2C%20many%0Agradient%20compression%20schemes%20do%20not%20achieve%20acceleration%20of%20the%20training%0Aprocess%20while%20also%20preserving%20accuracy.%0A%20%20In%20this%20work%2C%20we%20identify%20common%20issues%20in%20previous%20gradient%20compression%0Asystems%20and%20evaluation%20methodologies.%20These%20include%20excessive%20computational%0Aoverheads%3B%20incompatibility%20with%20all-reduce%3B%20and%20insufficient%20evaluation%0Amethods%2C%20such%20as%20not%20using%20an%20end-to-end%20metric%20or%20using%20a%2032-bit%20baseline%0Ainstead%20of%20the%20stronger%2016-bit%20baseline.%20We%20revisit%20common%20compression%0Aapproaches%20%28sparsification%2C%20quantization%2C%20and%20low-rank%20decomposition%29%20and%0Ademonstrate%20how%20considering%20the%20above%20issues%20can%20lead%20to%20minor%20but%20strategic%0Adesign%20changes%2C%20resulting%20in%20notably%20better%20performance.%20Our%20goal%20is%20to%20raise%0Aawareness%20of%20the%20need%20for%20design%20and%20evaluation%20standards%20that%20naturally%0Atranslate%20to%20the%20end-to-end%20utility%20of%20gradient%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Throughput%2520and%2520Compression%2520Ratios%253A%2520Towards%2520High%2520End-to-end%250A%2520%2520Utility%2520of%2520Gradient%2520Compression%26entry.906535625%3DWenchen%2520Han%2520and%2520Shay%2520Vargaftik%2520and%2520Michael%2520Mitzenmacher%2520and%2520Brad%2520Karp%2520and%2520Ran%2520Ben%2520Basat%26entry.1292438233%3D%2520%2520Gradient%2520aggregation%2520has%2520long%2520been%2520identified%2520as%2520a%2520major%2520bottleneck%2520in%250Atoday%2527s%2520large-scale%2520distributed%2520machine%2520learning%2520training%2520systems.%2520One%250Apromising%2520solution%2520to%2520mitigate%2520such%2520bottlenecks%2520is%2520gradient%2520compression%252C%250Adirectly%2520reducing%2520communicated%2520gradient%2520data%2520volume.%2520However%252C%2520in%2520practice%252C%2520many%250Agradient%2520compression%2520schemes%2520do%2520not%2520achieve%2520acceleration%2520of%2520the%2520training%250Aprocess%2520while%2520also%2520preserving%2520accuracy.%250A%2520%2520In%2520this%2520work%252C%2520we%2520identify%2520common%2520issues%2520in%2520previous%2520gradient%2520compression%250Asystems%2520and%2520evaluation%2520methodologies.%2520These%2520include%2520excessive%2520computational%250Aoverheads%253B%2520incompatibility%2520with%2520all-reduce%253B%2520and%2520insufficient%2520evaluation%250Amethods%252C%2520such%2520as%2520not%2520using%2520an%2520end-to-end%2520metric%2520or%2520using%2520a%252032-bit%2520baseline%250Ainstead%2520of%2520the%2520stronger%252016-bit%2520baseline.%2520We%2520revisit%2520common%2520compression%250Aapproaches%2520%2528sparsification%252C%2520quantization%252C%2520and%2520low-rank%2520decomposition%2529%2520and%250Ademonstrate%2520how%2520considering%2520the%2520above%2520issues%2520can%2520lead%2520to%2520minor%2520but%2520strategic%250Adesign%2520changes%252C%2520resulting%2520in%2520notably%2520better%2520performance.%2520Our%2520goal%2520is%2520to%2520raise%250Aawareness%2520of%2520the%2520need%2520for%2520design%2520and%2520evaluation%2520standards%2520that%2520naturally%250Atranslate%2520to%2520the%2520end-to-end%2520utility%2520of%2520gradient%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Throughput%20and%20Compression%20Ratios%3A%20Towards%20High%20End-to-end%0A%20%20Utility%20of%20Gradient%20Compression&entry.906535625=Wenchen%20Han%20and%20Shay%20Vargaftik%20and%20Michael%20Mitzenmacher%20and%20Brad%20Karp%20and%20Ran%20Ben%20Basat&entry.1292438233=%20%20Gradient%20aggregation%20has%20long%20been%20identified%20as%20a%20major%20bottleneck%20in%0Atoday%27s%20large-scale%20distributed%20machine%20learning%20training%20systems.%20One%0Apromising%20solution%20to%20mitigate%20such%20bottlenecks%20is%20gradient%20compression%2C%0Adirectly%20reducing%20communicated%20gradient%20data%20volume.%20However%2C%20in%20practice%2C%20many%0Agradient%20compression%20schemes%20do%20not%20achieve%20acceleration%20of%20the%20training%0Aprocess%20while%20also%20preserving%20accuracy.%0A%20%20In%20this%20work%2C%20we%20identify%20common%20issues%20in%20previous%20gradient%20compression%0Asystems%20and%20evaluation%20methodologies.%20These%20include%20excessive%20computational%0Aoverheads%3B%20incompatibility%20with%20all-reduce%3B%20and%20insufficient%20evaluation%0Amethods%2C%20such%20as%20not%20using%20an%20end-to-end%20metric%20or%20using%20a%2032-bit%20baseline%0Ainstead%20of%20the%20stronger%2016-bit%20baseline.%20We%20revisit%20common%20compression%0Aapproaches%20%28sparsification%2C%20quantization%2C%20and%20low-rank%20decomposition%29%20and%0Ademonstrate%20how%20considering%20the%20above%20issues%20can%20lead%20to%20minor%20but%20strategic%0Adesign%20changes%2C%20resulting%20in%20notably%20better%20performance.%20Our%20goal%20is%20to%20raise%0Aawareness%20of%20the%20need%20for%20design%20and%20evaluation%20standards%20that%20naturally%0Atranslate%20to%20the%20end-to-end%20utility%20of%20gradient%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01378v2&entry.124074799=Read"},
{"title": "Building a Scalable, Effective, and Steerable Search and Ranking\n  Platform", "author": "Marjan Celikik and Jacek Wasilewski and Ana Peleteiro Ramallo and Alexey Kurennoy and Evgeny Labzin and Danilo Ascione and Tural Gurbanov and G\u00e9raud Le Falher and Andrii Dzhoha and Ian Harris", "abstract": "  Modern e-commerce platforms offer vast product selections, making it\ndifficult for customers to find items that they like and that are relevant to\ntheir current session intent. This is why it is key for e-commerce platforms to\nhave near real-time scalable and adaptable personalized ranking and search\nsystems. While numerous methods exist in the scientific literature for building\nsuch systems, many are unsuitable for large-scale industrial use due to\ncomplexity and performance limitations. Consequently, industrial ranking\nsystems often resort to computationally efficient yet simplistic retrieval or\ncandidate generation approaches, which overlook near real-time and\nheterogeneous customer signals, which results in a less personalized and\nrelevant experience. Moreover, related customer experiences are served by\ncompletely different systems, which increases complexity, maintenance, and\ninconsistent experiences.\n  In this paper, we present a personalized, adaptable near real-time ranking\nplatform that is reusable across various use cases, such as browsing and\nsearch, and that is able to cater to millions of items and customers under\nheavy load (thousands of requests per second). We employ transformer-based\nmodels through different ranking layers which can learn complex behavior\npatterns directly from customer action sequences while being able to\nincorporate temporal (e.g. in-session) and contextual information. We validate\nour system through a series of comprehensive offline and online real-world\nexperiments at a large online e-commerce platform, and we demonstrate its\nsuperiority when compared to existing systems, both in terms of customer\nexperience as well as in net revenue. Finally, we share the lessons learned\nfrom building a comprehensive, modern ranking platform for use in a large-scale\ne-commerce environment.\n", "link": "http://arxiv.org/abs/2409.02856v2", "date": "2024-10-29", "relevancy": 1.4103, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.456}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20a%20Scalable%2C%20Effective%2C%20and%20Steerable%20Search%20and%20Ranking%0A%20%20Platform&body=Title%3A%20Building%20a%20Scalable%2C%20Effective%2C%20and%20Steerable%20Search%20and%20Ranking%0A%20%20Platform%0AAuthor%3A%20Marjan%20Celikik%20and%20Jacek%20Wasilewski%20and%20Ana%20Peleteiro%20Ramallo%20and%20Alexey%20Kurennoy%20and%20Evgeny%20Labzin%20and%20Danilo%20Ascione%20and%20Tural%20Gurbanov%20and%20G%C3%A9raud%20Le%20Falher%20and%20Andrii%20Dzhoha%20and%20Ian%20Harris%0AAbstract%3A%20%20%20Modern%20e-commerce%20platforms%20offer%20vast%20product%20selections%2C%20making%20it%0Adifficult%20for%20customers%20to%20find%20items%20that%20they%20like%20and%20that%20are%20relevant%20to%0Atheir%20current%20session%20intent.%20This%20is%20why%20it%20is%20key%20for%20e-commerce%20platforms%20to%0Ahave%20near%20real-time%20scalable%20and%20adaptable%20personalized%20ranking%20and%20search%0Asystems.%20While%20numerous%20methods%20exist%20in%20the%20scientific%20literature%20for%20building%0Asuch%20systems%2C%20many%20are%20unsuitable%20for%20large-scale%20industrial%20use%20due%20to%0Acomplexity%20and%20performance%20limitations.%20Consequently%2C%20industrial%20ranking%0Asystems%20often%20resort%20to%20computationally%20efficient%20yet%20simplistic%20retrieval%20or%0Acandidate%20generation%20approaches%2C%20which%20overlook%20near%20real-time%20and%0Aheterogeneous%20customer%20signals%2C%20which%20results%20in%20a%20less%20personalized%20and%0Arelevant%20experience.%20Moreover%2C%20related%20customer%20experiences%20are%20served%20by%0Acompletely%20different%20systems%2C%20which%20increases%20complexity%2C%20maintenance%2C%20and%0Ainconsistent%20experiences.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20personalized%2C%20adaptable%20near%20real-time%20ranking%0Aplatform%20that%20is%20reusable%20across%20various%20use%20cases%2C%20such%20as%20browsing%20and%0Asearch%2C%20and%20that%20is%20able%20to%20cater%20to%20millions%20of%20items%20and%20customers%20under%0Aheavy%20load%20%28thousands%20of%20requests%20per%20second%29.%20We%20employ%20transformer-based%0Amodels%20through%20different%20ranking%20layers%20which%20can%20learn%20complex%20behavior%0Apatterns%20directly%20from%20customer%20action%20sequences%20while%20being%20able%20to%0Aincorporate%20temporal%20%28e.g.%20in-session%29%20and%20contextual%20information.%20We%20validate%0Aour%20system%20through%20a%20series%20of%20comprehensive%20offline%20and%20online%20real-world%0Aexperiments%20at%20a%20large%20online%20e-commerce%20platform%2C%20and%20we%20demonstrate%20its%0Asuperiority%20when%20compared%20to%20existing%20systems%2C%20both%20in%20terms%20of%20customer%0Aexperience%20as%20well%20as%20in%20net%20revenue.%20Finally%2C%20we%20share%20the%20lessons%20learned%0Afrom%20building%20a%20comprehensive%2C%20modern%20ranking%20platform%20for%20use%20in%20a%20large-scale%0Ae-commerce%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520a%2520Scalable%252C%2520Effective%252C%2520and%2520Steerable%2520Search%2520and%2520Ranking%250A%2520%2520Platform%26entry.906535625%3DMarjan%2520Celikik%2520and%2520Jacek%2520Wasilewski%2520and%2520Ana%2520Peleteiro%2520Ramallo%2520and%2520Alexey%2520Kurennoy%2520and%2520Evgeny%2520Labzin%2520and%2520Danilo%2520Ascione%2520and%2520Tural%2520Gurbanov%2520and%2520G%25C3%25A9raud%2520Le%2520Falher%2520and%2520Andrii%2520Dzhoha%2520and%2520Ian%2520Harris%26entry.1292438233%3D%2520%2520Modern%2520e-commerce%2520platforms%2520offer%2520vast%2520product%2520selections%252C%2520making%2520it%250Adifficult%2520for%2520customers%2520to%2520find%2520items%2520that%2520they%2520like%2520and%2520that%2520are%2520relevant%2520to%250Atheir%2520current%2520session%2520intent.%2520This%2520is%2520why%2520it%2520is%2520key%2520for%2520e-commerce%2520platforms%2520to%250Ahave%2520near%2520real-time%2520scalable%2520and%2520adaptable%2520personalized%2520ranking%2520and%2520search%250Asystems.%2520While%2520numerous%2520methods%2520exist%2520in%2520the%2520scientific%2520literature%2520for%2520building%250Asuch%2520systems%252C%2520many%2520are%2520unsuitable%2520for%2520large-scale%2520industrial%2520use%2520due%2520to%250Acomplexity%2520and%2520performance%2520limitations.%2520Consequently%252C%2520industrial%2520ranking%250Asystems%2520often%2520resort%2520to%2520computationally%2520efficient%2520yet%2520simplistic%2520retrieval%2520or%250Acandidate%2520generation%2520approaches%252C%2520which%2520overlook%2520near%2520real-time%2520and%250Aheterogeneous%2520customer%2520signals%252C%2520which%2520results%2520in%2520a%2520less%2520personalized%2520and%250Arelevant%2520experience.%2520Moreover%252C%2520related%2520customer%2520experiences%2520are%2520served%2520by%250Acompletely%2520different%2520systems%252C%2520which%2520increases%2520complexity%252C%2520maintenance%252C%2520and%250Ainconsistent%2520experiences.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520personalized%252C%2520adaptable%2520near%2520real-time%2520ranking%250Aplatform%2520that%2520is%2520reusable%2520across%2520various%2520use%2520cases%252C%2520such%2520as%2520browsing%2520and%250Asearch%252C%2520and%2520that%2520is%2520able%2520to%2520cater%2520to%2520millions%2520of%2520items%2520and%2520customers%2520under%250Aheavy%2520load%2520%2528thousands%2520of%2520requests%2520per%2520second%2529.%2520We%2520employ%2520transformer-based%250Amodels%2520through%2520different%2520ranking%2520layers%2520which%2520can%2520learn%2520complex%2520behavior%250Apatterns%2520directly%2520from%2520customer%2520action%2520sequences%2520while%2520being%2520able%2520to%250Aincorporate%2520temporal%2520%2528e.g.%2520in-session%2529%2520and%2520contextual%2520information.%2520We%2520validate%250Aour%2520system%2520through%2520a%2520series%2520of%2520comprehensive%2520offline%2520and%2520online%2520real-world%250Aexperiments%2520at%2520a%2520large%2520online%2520e-commerce%2520platform%252C%2520and%2520we%2520demonstrate%2520its%250Asuperiority%2520when%2520compared%2520to%2520existing%2520systems%252C%2520both%2520in%2520terms%2520of%2520customer%250Aexperience%2520as%2520well%2520as%2520in%2520net%2520revenue.%2520Finally%252C%2520we%2520share%2520the%2520lessons%2520learned%250Afrom%2520building%2520a%2520comprehensive%252C%2520modern%2520ranking%2520platform%2520for%2520use%2520in%2520a%2520large-scale%250Ae-commerce%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20a%20Scalable%2C%20Effective%2C%20and%20Steerable%20Search%20and%20Ranking%0A%20%20Platform&entry.906535625=Marjan%20Celikik%20and%20Jacek%20Wasilewski%20and%20Ana%20Peleteiro%20Ramallo%20and%20Alexey%20Kurennoy%20and%20Evgeny%20Labzin%20and%20Danilo%20Ascione%20and%20Tural%20Gurbanov%20and%20G%C3%A9raud%20Le%20Falher%20and%20Andrii%20Dzhoha%20and%20Ian%20Harris&entry.1292438233=%20%20Modern%20e-commerce%20platforms%20offer%20vast%20product%20selections%2C%20making%20it%0Adifficult%20for%20customers%20to%20find%20items%20that%20they%20like%20and%20that%20are%20relevant%20to%0Atheir%20current%20session%20intent.%20This%20is%20why%20it%20is%20key%20for%20e-commerce%20platforms%20to%0Ahave%20near%20real-time%20scalable%20and%20adaptable%20personalized%20ranking%20and%20search%0Asystems.%20While%20numerous%20methods%20exist%20in%20the%20scientific%20literature%20for%20building%0Asuch%20systems%2C%20many%20are%20unsuitable%20for%20large-scale%20industrial%20use%20due%20to%0Acomplexity%20and%20performance%20limitations.%20Consequently%2C%20industrial%20ranking%0Asystems%20often%20resort%20to%20computationally%20efficient%20yet%20simplistic%20retrieval%20or%0Acandidate%20generation%20approaches%2C%20which%20overlook%20near%20real-time%20and%0Aheterogeneous%20customer%20signals%2C%20which%20results%20in%20a%20less%20personalized%20and%0Arelevant%20experience.%20Moreover%2C%20related%20customer%20experiences%20are%20served%20by%0Acompletely%20different%20systems%2C%20which%20increases%20complexity%2C%20maintenance%2C%20and%0Ainconsistent%20experiences.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20personalized%2C%20adaptable%20near%20real-time%20ranking%0Aplatform%20that%20is%20reusable%20across%20various%20use%20cases%2C%20such%20as%20browsing%20and%0Asearch%2C%20and%20that%20is%20able%20to%20cater%20to%20millions%20of%20items%20and%20customers%20under%0Aheavy%20load%20%28thousands%20of%20requests%20per%20second%29.%20We%20employ%20transformer-based%0Amodels%20through%20different%20ranking%20layers%20which%20can%20learn%20complex%20behavior%0Apatterns%20directly%20from%20customer%20action%20sequences%20while%20being%20able%20to%0Aincorporate%20temporal%20%28e.g.%20in-session%29%20and%20contextual%20information.%20We%20validate%0Aour%20system%20through%20a%20series%20of%20comprehensive%20offline%20and%20online%20real-world%0Aexperiments%20at%20a%20large%20online%20e-commerce%20platform%2C%20and%20we%20demonstrate%20its%0Asuperiority%20when%20compared%20to%20existing%20systems%2C%20both%20in%20terms%20of%20customer%0Aexperience%20as%20well%20as%20in%20net%20revenue.%20Finally%2C%20we%20share%20the%20lessons%20learned%0Afrom%20building%20a%20comprehensive%2C%20modern%20ranking%20platform%20for%20use%20in%20a%20large-scale%0Ae-commerce%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02856v2&entry.124074799=Read"},
{"title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative\n  Decoding", "author": "Bohan Li and Hankun Wang and Situo Zhang and Yiwei Guo and Kai Yu", "abstract": "  The auto-regressive architecture, like GPTs, is widely used in modern\nText-to-Speech (TTS) systems. However, it incurs substantial inference time,\nparticularly due to the challenges in the next-token prediction posed by\nlengthy sequences of speech tokens. In this work, we introduce VADUSA, one of\nthe first approaches to accelerate auto-regressive TTS through speculative\ndecoding. Our results show that VADUSA not only significantly improves\ninference speed but also enhances performance by incorporating draft heads to\npredict future speech content auto-regressively. Furthermore, the inclusion of\na tolerance mechanism during sampling accelerates inference without\ncompromising quality. Our approach demonstrates strong generalization across\nlarge datasets and various types of speech tokens.\n", "link": "http://arxiv.org/abs/2410.21951v1", "date": "2024-10-29", "relevancy": 1.4585, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.49}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4859}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20High-Quality%20Auto-Regressive%20Speech%20Synthesis%20via%20Speculative%0A%20%20Decoding&body=Title%3A%20Fast%20and%20High-Quality%20Auto-Regressive%20Speech%20Synthesis%20via%20Speculative%0A%20%20Decoding%0AAuthor%3A%20Bohan%20Li%20and%20Hankun%20Wang%20and%20Situo%20Zhang%20and%20Yiwei%20Guo%20and%20Kai%20Yu%0AAbstract%3A%20%20%20The%20auto-regressive%20architecture%2C%20like%20GPTs%2C%20is%20widely%20used%20in%20modern%0AText-to-Speech%20%28TTS%29%20systems.%20However%2C%20it%20incurs%20substantial%20inference%20time%2C%0Aparticularly%20due%20to%20the%20challenges%20in%20the%20next-token%20prediction%20posed%20by%0Alengthy%20sequences%20of%20speech%20tokens.%20In%20this%20work%2C%20we%20introduce%20VADUSA%2C%20one%20of%0Athe%20first%20approaches%20to%20accelerate%20auto-regressive%20TTS%20through%20speculative%0Adecoding.%20Our%20results%20show%20that%20VADUSA%20not%20only%20significantly%20improves%0Ainference%20speed%20but%20also%20enhances%20performance%20by%20incorporating%20draft%20heads%20to%0Apredict%20future%20speech%20content%20auto-regressively.%20Furthermore%2C%20the%20inclusion%20of%0Aa%20tolerance%20mechanism%20during%20sampling%20accelerates%20inference%20without%0Acompromising%20quality.%20Our%20approach%20demonstrates%20strong%20generalization%20across%0Alarge%20datasets%20and%20various%20types%20of%20speech%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520High-Quality%2520Auto-Regressive%2520Speech%2520Synthesis%2520via%2520Speculative%250A%2520%2520Decoding%26entry.906535625%3DBohan%2520Li%2520and%2520Hankun%2520Wang%2520and%2520Situo%2520Zhang%2520and%2520Yiwei%2520Guo%2520and%2520Kai%2520Yu%26entry.1292438233%3D%2520%2520The%2520auto-regressive%2520architecture%252C%2520like%2520GPTs%252C%2520is%2520widely%2520used%2520in%2520modern%250AText-to-Speech%2520%2528TTS%2529%2520systems.%2520However%252C%2520it%2520incurs%2520substantial%2520inference%2520time%252C%250Aparticularly%2520due%2520to%2520the%2520challenges%2520in%2520the%2520next-token%2520prediction%2520posed%2520by%250Alengthy%2520sequences%2520of%2520speech%2520tokens.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VADUSA%252C%2520one%2520of%250Athe%2520first%2520approaches%2520to%2520accelerate%2520auto-regressive%2520TTS%2520through%2520speculative%250Adecoding.%2520Our%2520results%2520show%2520that%2520VADUSA%2520not%2520only%2520significantly%2520improves%250Ainference%2520speed%2520but%2520also%2520enhances%2520performance%2520by%2520incorporating%2520draft%2520heads%2520to%250Apredict%2520future%2520speech%2520content%2520auto-regressively.%2520Furthermore%252C%2520the%2520inclusion%2520of%250Aa%2520tolerance%2520mechanism%2520during%2520sampling%2520accelerates%2520inference%2520without%250Acompromising%2520quality.%2520Our%2520approach%2520demonstrates%2520strong%2520generalization%2520across%250Alarge%2520datasets%2520and%2520various%2520types%2520of%2520speech%2520tokens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20High-Quality%20Auto-Regressive%20Speech%20Synthesis%20via%20Speculative%0A%20%20Decoding&entry.906535625=Bohan%20Li%20and%20Hankun%20Wang%20and%20Situo%20Zhang%20and%20Yiwei%20Guo%20and%20Kai%20Yu&entry.1292438233=%20%20The%20auto-regressive%20architecture%2C%20like%20GPTs%2C%20is%20widely%20used%20in%20modern%0AText-to-Speech%20%28TTS%29%20systems.%20However%2C%20it%20incurs%20substantial%20inference%20time%2C%0Aparticularly%20due%20to%20the%20challenges%20in%20the%20next-token%20prediction%20posed%20by%0Alengthy%20sequences%20of%20speech%20tokens.%20In%20this%20work%2C%20we%20introduce%20VADUSA%2C%20one%20of%0Athe%20first%20approaches%20to%20accelerate%20auto-regressive%20TTS%20through%20speculative%0Adecoding.%20Our%20results%20show%20that%20VADUSA%20not%20only%20significantly%20improves%0Ainference%20speed%20but%20also%20enhances%20performance%20by%20incorporating%20draft%20heads%20to%0Apredict%20future%20speech%20content%20auto-regressively.%20Furthermore%2C%20the%20inclusion%20of%0Aa%20tolerance%20mechanism%20during%20sampling%20accelerates%20inference%20without%0Acompromising%20quality.%20Our%20approach%20demonstrates%20strong%20generalization%20across%0Alarge%20datasets%20and%20various%20types%20of%20speech%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21951v1&entry.124074799=Read"},
{"title": "MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for\n  Efficient and Interpretable Medical Image Segmentation", "author": "Ovais Iqbal Shah and Danish Raza Rizvi and Aqib Nazir Mir", "abstract": "  Medical image segmentation is pivotal in healthcare, enhancing diagnostic\naccuracy, informing treatment strategies, and tracking disease progression.\nThis process allows clinicians to extract critical information from visual\ndata, enabling personalized patient care. However, developing neural networks\nfor segmentation remains challenging, especially when preserving image\nresolution, which is essential in detecting subtle details that influence\ndiagnoses. Moreover, the lack of transparency in these deep learning models has\nslowed their adoption in clinical practice. Efforts in model interpretability\nare increasingly focused on making these models' decision-making processes more\ntransparent. In this paper, we introduce MAPUNetR, a novel architecture that\nsynergizes the strengths of transformer models with the proven U-Net framework\nfor medical image segmentation. Our model addresses the resolution preservation\nchallenge and incorporates attention maps highlighting segmented regions,\nincreasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset,\nMAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the\nISIC 2018 dataset. Our experiments show that the model maintains stable\nperformance and potential as a powerful tool for medical image segmentation in\nclinical practice.\n", "link": "http://arxiv.org/abs/2410.22223v1", "date": "2024-10-29", "relevancy": 1.6726, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5667}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPUNetR%3A%20A%20Hybrid%20Vision%20Transformer%20and%20U-Net%20Architecture%20for%0A%20%20Efficient%20and%20Interpretable%20Medical%20Image%20Segmentation&body=Title%3A%20MAPUNetR%3A%20A%20Hybrid%20Vision%20Transformer%20and%20U-Net%20Architecture%20for%0A%20%20Efficient%20and%20Interpretable%20Medical%20Image%20Segmentation%0AAuthor%3A%20Ovais%20Iqbal%20Shah%20and%20Danish%20Raza%20Rizvi%20and%20Aqib%20Nazir%20Mir%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20pivotal%20in%20healthcare%2C%20enhancing%20diagnostic%0Aaccuracy%2C%20informing%20treatment%20strategies%2C%20and%20tracking%20disease%20progression.%0AThis%20process%20allows%20clinicians%20to%20extract%20critical%20information%20from%20visual%0Adata%2C%20enabling%20personalized%20patient%20care.%20However%2C%20developing%20neural%20networks%0Afor%20segmentation%20remains%20challenging%2C%20especially%20when%20preserving%20image%0Aresolution%2C%20which%20is%20essential%20in%20detecting%20subtle%20details%20that%20influence%0Adiagnoses.%20Moreover%2C%20the%20lack%20of%20transparency%20in%20these%20deep%20learning%20models%20has%0Aslowed%20their%20adoption%20in%20clinical%20practice.%20Efforts%20in%20model%20interpretability%0Aare%20increasingly%20focused%20on%20making%20these%20models%27%20decision-making%20processes%20more%0Atransparent.%20In%20this%20paper%2C%20we%20introduce%20MAPUNetR%2C%20a%20novel%20architecture%20that%0Asynergizes%20the%20strengths%20of%20transformer%20models%20with%20the%20proven%20U-Net%20framework%0Afor%20medical%20image%20segmentation.%20Our%20model%20addresses%20the%20resolution%20preservation%0Achallenge%20and%20incorporates%20attention%20maps%20highlighting%20segmented%20regions%2C%0Aincreasing%20accuracy%20and%20interpretability.%20Evaluated%20on%20the%20BraTS%202020%20dataset%2C%0AMAPUNetR%20achieved%20a%20dice%20score%20of%200.88%20and%20a%20dice%20coefficient%20of%200.92%20on%20the%0AISIC%202018%20dataset.%20Our%20experiments%20show%20that%20the%20model%20maintains%20stable%0Aperformance%20and%20potential%20as%20a%20powerful%20tool%20for%20medical%20image%20segmentation%20in%0Aclinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPUNetR%253A%2520A%2520Hybrid%2520Vision%2520Transformer%2520and%2520U-Net%2520Architecture%2520for%250A%2520%2520Efficient%2520and%2520Interpretable%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DOvais%2520Iqbal%2520Shah%2520and%2520Danish%2520Raza%2520Rizvi%2520and%2520Aqib%2520Nazir%2520Mir%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520pivotal%2520in%2520healthcare%252C%2520enhancing%2520diagnostic%250Aaccuracy%252C%2520informing%2520treatment%2520strategies%252C%2520and%2520tracking%2520disease%2520progression.%250AThis%2520process%2520allows%2520clinicians%2520to%2520extract%2520critical%2520information%2520from%2520visual%250Adata%252C%2520enabling%2520personalized%2520patient%2520care.%2520However%252C%2520developing%2520neural%2520networks%250Afor%2520segmentation%2520remains%2520challenging%252C%2520especially%2520when%2520preserving%2520image%250Aresolution%252C%2520which%2520is%2520essential%2520in%2520detecting%2520subtle%2520details%2520that%2520influence%250Adiagnoses.%2520Moreover%252C%2520the%2520lack%2520of%2520transparency%2520in%2520these%2520deep%2520learning%2520models%2520has%250Aslowed%2520their%2520adoption%2520in%2520clinical%2520practice.%2520Efforts%2520in%2520model%2520interpretability%250Aare%2520increasingly%2520focused%2520on%2520making%2520these%2520models%2527%2520decision-making%2520processes%2520more%250Atransparent.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MAPUNetR%252C%2520a%2520novel%2520architecture%2520that%250Asynergizes%2520the%2520strengths%2520of%2520transformer%2520models%2520with%2520the%2520proven%2520U-Net%2520framework%250Afor%2520medical%2520image%2520segmentation.%2520Our%2520model%2520addresses%2520the%2520resolution%2520preservation%250Achallenge%2520and%2520incorporates%2520attention%2520maps%2520highlighting%2520segmented%2520regions%252C%250Aincreasing%2520accuracy%2520and%2520interpretability.%2520Evaluated%2520on%2520the%2520BraTS%25202020%2520dataset%252C%250AMAPUNetR%2520achieved%2520a%2520dice%2520score%2520of%25200.88%2520and%2520a%2520dice%2520coefficient%2520of%25200.92%2520on%2520the%250AISIC%25202018%2520dataset.%2520Our%2520experiments%2520show%2520that%2520the%2520model%2520maintains%2520stable%250Aperformance%2520and%2520potential%2520as%2520a%2520powerful%2520tool%2520for%2520medical%2520image%2520segmentation%2520in%250Aclinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPUNetR%3A%20A%20Hybrid%20Vision%20Transformer%20and%20U-Net%20Architecture%20for%0A%20%20Efficient%20and%20Interpretable%20Medical%20Image%20Segmentation&entry.906535625=Ovais%20Iqbal%20Shah%20and%20Danish%20Raza%20Rizvi%20and%20Aqib%20Nazir%20Mir&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20pivotal%20in%20healthcare%2C%20enhancing%20diagnostic%0Aaccuracy%2C%20informing%20treatment%20strategies%2C%20and%20tracking%20disease%20progression.%0AThis%20process%20allows%20clinicians%20to%20extract%20critical%20information%20from%20visual%0Adata%2C%20enabling%20personalized%20patient%20care.%20However%2C%20developing%20neural%20networks%0Afor%20segmentation%20remains%20challenging%2C%20especially%20when%20preserving%20image%0Aresolution%2C%20which%20is%20essential%20in%20detecting%20subtle%20details%20that%20influence%0Adiagnoses.%20Moreover%2C%20the%20lack%20of%20transparency%20in%20these%20deep%20learning%20models%20has%0Aslowed%20their%20adoption%20in%20clinical%20practice.%20Efforts%20in%20model%20interpretability%0Aare%20increasingly%20focused%20on%20making%20these%20models%27%20decision-making%20processes%20more%0Atransparent.%20In%20this%20paper%2C%20we%20introduce%20MAPUNetR%2C%20a%20novel%20architecture%20that%0Asynergizes%20the%20strengths%20of%20transformer%20models%20with%20the%20proven%20U-Net%20framework%0Afor%20medical%20image%20segmentation.%20Our%20model%20addresses%20the%20resolution%20preservation%0Achallenge%20and%20incorporates%20attention%20maps%20highlighting%20segmented%20regions%2C%0Aincreasing%20accuracy%20and%20interpretability.%20Evaluated%20on%20the%20BraTS%202020%20dataset%2C%0AMAPUNetR%20achieved%20a%20dice%20score%20of%200.88%20and%20a%20dice%20coefficient%20of%200.92%20on%20the%0AISIC%202018%20dataset.%20Our%20experiments%20show%20that%20the%20model%20maintains%20stable%0Aperformance%20and%20potential%20as%20a%20powerful%20tool%20for%20medical%20image%20segmentation%20in%0Aclinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22223v1&entry.124074799=Read"},
{"title": "Convex Formulations for Training Two-Layer ReLU Neural Networks", "author": "Karthik Prakhya and Tolga Birdal and Alp Yurtsever", "abstract": "  Solving non-convex, NP-hard optimization problems is crucial for training\nmachine learning models, including neural networks. However, non-convexity\noften leads to black-box machine learning models with unclear inner workings.\nWhile convex formulations have been used for verifying neural network\nrobustness, their application to training neural networks remains less\nexplored. In response to this challenge, we reformulate the problem of training\ninfinite-width two-layer ReLU networks as a convex completely positive program\nin a finite-dimensional (lifted) space. Despite the convexity, solving this\nproblem remains NP-hard due to the complete positivity constraint. To overcome\nthis challenge, we introduce a semidefinite relaxation that can be solved in\npolynomial time. We then experimentally evaluate the tightness of this\nrelaxation, demonstrating its competitive performance in test accuracy across a\nrange of classification tasks.\n", "link": "http://arxiv.org/abs/2410.22311v1", "date": "2024-10-29", "relevancy": 1.4829, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5142}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4782}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convex%20Formulations%20for%20Training%20Two-Layer%20ReLU%20Neural%20Networks&body=Title%3A%20Convex%20Formulations%20for%20Training%20Two-Layer%20ReLU%20Neural%20Networks%0AAuthor%3A%20Karthik%20Prakhya%20and%20Tolga%20Birdal%20and%20Alp%20Yurtsever%0AAbstract%3A%20%20%20Solving%20non-convex%2C%20NP-hard%20optimization%20problems%20is%20crucial%20for%20training%0Amachine%20learning%20models%2C%20including%20neural%20networks.%20However%2C%20non-convexity%0Aoften%20leads%20to%20black-box%20machine%20learning%20models%20with%20unclear%20inner%20workings.%0AWhile%20convex%20formulations%20have%20been%20used%20for%20verifying%20neural%20network%0Arobustness%2C%20their%20application%20to%20training%20neural%20networks%20remains%20less%0Aexplored.%20In%20response%20to%20this%20challenge%2C%20we%20reformulate%20the%20problem%20of%20training%0Ainfinite-width%20two-layer%20ReLU%20networks%20as%20a%20convex%20completely%20positive%20program%0Ain%20a%20finite-dimensional%20%28lifted%29%20space.%20Despite%20the%20convexity%2C%20solving%20this%0Aproblem%20remains%20NP-hard%20due%20to%20the%20complete%20positivity%20constraint.%20To%20overcome%0Athis%20challenge%2C%20we%20introduce%20a%20semidefinite%20relaxation%20that%20can%20be%20solved%20in%0Apolynomial%20time.%20We%20then%20experimentally%20evaluate%20the%20tightness%20of%20this%0Arelaxation%2C%20demonstrating%20its%20competitive%20performance%20in%20test%20accuracy%20across%20a%0Arange%20of%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvex%2520Formulations%2520for%2520Training%2520Two-Layer%2520ReLU%2520Neural%2520Networks%26entry.906535625%3DKarthik%2520Prakhya%2520and%2520Tolga%2520Birdal%2520and%2520Alp%2520Yurtsever%26entry.1292438233%3D%2520%2520Solving%2520non-convex%252C%2520NP-hard%2520optimization%2520problems%2520is%2520crucial%2520for%2520training%250Amachine%2520learning%2520models%252C%2520including%2520neural%2520networks.%2520However%252C%2520non-convexity%250Aoften%2520leads%2520to%2520black-box%2520machine%2520learning%2520models%2520with%2520unclear%2520inner%2520workings.%250AWhile%2520convex%2520formulations%2520have%2520been%2520used%2520for%2520verifying%2520neural%2520network%250Arobustness%252C%2520their%2520application%2520to%2520training%2520neural%2520networks%2520remains%2520less%250Aexplored.%2520In%2520response%2520to%2520this%2520challenge%252C%2520we%2520reformulate%2520the%2520problem%2520of%2520training%250Ainfinite-width%2520two-layer%2520ReLU%2520networks%2520as%2520a%2520convex%2520completely%2520positive%2520program%250Ain%2520a%2520finite-dimensional%2520%2528lifted%2529%2520space.%2520Despite%2520the%2520convexity%252C%2520solving%2520this%250Aproblem%2520remains%2520NP-hard%2520due%2520to%2520the%2520complete%2520positivity%2520constraint.%2520To%2520overcome%250Athis%2520challenge%252C%2520we%2520introduce%2520a%2520semidefinite%2520relaxation%2520that%2520can%2520be%2520solved%2520in%250Apolynomial%2520time.%2520We%2520then%2520experimentally%2520evaluate%2520the%2520tightness%2520of%2520this%250Arelaxation%252C%2520demonstrating%2520its%2520competitive%2520performance%2520in%2520test%2520accuracy%2520across%2520a%250Arange%2520of%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20Formulations%20for%20Training%20Two-Layer%20ReLU%20Neural%20Networks&entry.906535625=Karthik%20Prakhya%20and%20Tolga%20Birdal%20and%20Alp%20Yurtsever&entry.1292438233=%20%20Solving%20non-convex%2C%20NP-hard%20optimization%20problems%20is%20crucial%20for%20training%0Amachine%20learning%20models%2C%20including%20neural%20networks.%20However%2C%20non-convexity%0Aoften%20leads%20to%20black-box%20machine%20learning%20models%20with%20unclear%20inner%20workings.%0AWhile%20convex%20formulations%20have%20been%20used%20for%20verifying%20neural%20network%0Arobustness%2C%20their%20application%20to%20training%20neural%20networks%20remains%20less%0Aexplored.%20In%20response%20to%20this%20challenge%2C%20we%20reformulate%20the%20problem%20of%20training%0Ainfinite-width%20two-layer%20ReLU%20networks%20as%20a%20convex%20completely%20positive%20program%0Ain%20a%20finite-dimensional%20%28lifted%29%20space.%20Despite%20the%20convexity%2C%20solving%20this%0Aproblem%20remains%20NP-hard%20due%20to%20the%20complete%20positivity%20constraint.%20To%20overcome%0Athis%20challenge%2C%20we%20introduce%20a%20semidefinite%20relaxation%20that%20can%20be%20solved%20in%0Apolynomial%20time.%20We%20then%20experimentally%20evaluate%20the%20tightness%20of%20this%0Arelaxation%2C%20demonstrating%20its%20competitive%20performance%20in%20test%20accuracy%20across%20a%0Arange%20of%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22311v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


