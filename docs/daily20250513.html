<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250512.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting", "author": "Zhihao Guo and Jingxuan Su and Shenglin Wang and Jinlong Fan and Jing Zhang and Wei Zhou and Hadi Amirpour and Yunlong Zhao and Liangxiu Han and Peng Wang", "abstract": "  3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds often limits scene reconstruction quality. To address the\nlimitation, this paper proposes a novel 3D reconstruction framework, Gaussian\nProcesses enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian\nProcess model is developed to enable adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. These densified point clouds\nprovide high-quality initial 3D Gaussians, enhancing reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.\n", "link": "http://arxiv.org/abs/2502.02283v4", "date": "2025-05-12", "relevancy": 3.5893, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7507}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7229}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.68}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&body=Title%3A%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting%0AAuthor%3A%20Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Wei%20Zhou%20and%20Hadi%20Amirpour%20and%20Yunlong%20Zhao%20and%20Liangxiu%20Han%20and%20Peng%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20often%20limits%20scene%20reconstruction%20quality.%20To%20address%20the%0Alimitation%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%20framework%2C%20Gaussian%0AProcesses%20enhanced%20Gaussian%20Splatting%20%28GP-GS%29%2C%20in%20which%20a%20multi-output%20Gaussian%0AProcess%20model%20is%20developed%20to%20enable%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20These%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%2C%20enhancing%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02283v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGP-GS%253A%2520Gaussian%2520Processes%2520for%2520Enhanced%2520Gaussian%2520Splatting%26entry.906535625%3DZhihao%2520Guo%2520and%2520Jingxuan%2520Su%2520and%2520Shenglin%2520Wang%2520and%2520Jinlong%2520Fan%2520and%2520Jing%2520Zhang%2520and%2520Wei%2520Zhou%2520and%2520Hadi%2520Amirpour%2520and%2520Yunlong%2520Zhao%2520and%2520Liangxiu%2520Han%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520an%2520efficient%2520photorealistic%2520novel%2520view%250Asynthesis%2520method.%2520However%252C%2520its%2520reliance%2520on%2520sparse%2520Structure-from-Motion%2520%2528SfM%2529%250Apoint%2520clouds%2520often%2520limits%2520scene%2520reconstruction%2520quality.%2520To%2520address%2520the%250Alimitation%252C%2520this%2520paper%2520proposes%2520a%2520novel%25203D%2520reconstruction%2520framework%252C%2520Gaussian%250AProcesses%2520enhanced%2520Gaussian%2520Splatting%2520%2528GP-GS%2529%252C%2520in%2520which%2520a%2520multi-output%2520Gaussian%250AProcess%2520model%2520is%2520developed%2520to%2520enable%2520adaptive%2520and%2520uncertainty-guided%250Adensification%2520of%2520sparse%2520SfM%2520point%2520clouds.%2520Specifically%252C%2520we%2520propose%2520a%2520dynamic%250Asampling%2520and%2520filtering%2520pipeline%2520that%2520adaptively%2520expands%2520the%2520SfM%2520point%2520clouds%2520by%250Aleveraging%2520GP-based%2520predictions%2520to%2520infer%2520new%2520candidate%2520points%2520from%2520the%2520input%25202D%250Apixels%2520and%2520depth%2520maps.%2520The%2520pipeline%2520utilizes%2520uncertainty%2520estimates%2520to%2520guide%2520the%250Apruning%2520of%2520high-variance%2520predictions%252C%2520ensuring%2520geometric%2520consistency%2520and%250Aenabling%2520the%2520generation%2520of%2520dense%2520point%2520clouds.%2520These%2520densified%2520point%2520clouds%250Aprovide%2520high-quality%2520initial%25203D%2520Gaussians%252C%2520enhancing%2520reconstruction%250Aperformance.%2520Extensive%2520experiments%2520conducted%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520across%2520various%2520scales%2520validate%2520the%2520effectiveness%2520and%2520practicality%2520of%250Athe%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02283v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&entry.906535625=Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Wei%20Zhou%20and%20Hadi%20Amirpour%20and%20Yunlong%20Zhao%20and%20Liangxiu%20Han%20and%20Peng%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20often%20limits%20scene%20reconstruction%20quality.%20To%20address%20the%0Alimitation%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%20framework%2C%20Gaussian%0AProcesses%20enhanced%20Gaussian%20Splatting%20%28GP-GS%29%2C%20in%20which%20a%20multi-output%20Gaussian%0AProcess%20model%20is%20developed%20to%20enable%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20These%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%2C%20enhancing%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02283v4&entry.124074799=Read"},
{"title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets", "author": "Weiyu Li and Xuanyang Zhang and Zheng Sun and Di Qi and Hao Li and Wei Cheng and Weiwei Cai and Shihao Wu and Jiarui Liu and Zihao Wang and Xiao Chen and Feipeng Tian and Jianxiong Pan and Zeming Li and Gang Yu and Xiangyu Zhang and Daxin Jiang and Ping Tan", "abstract": "  While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.\n", "link": "http://arxiv.org/abs/2505.07747v1", "date": "2025-05-12", "relevancy": 3.2531, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6632}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6632}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step1X-3D%3A%20Towards%20High-Fidelity%20and%20Controllable%20Generation%20of%20Textured%0A%20%203D%20Assets&body=Title%3A%20Step1X-3D%3A%20Towards%20High-Fidelity%20and%20Controllable%20Generation%20of%20Textured%0A%20%203D%20Assets%0AAuthor%3A%20Weiyu%20Li%20and%20Xuanyang%20Zhang%20and%20Zheng%20Sun%20and%20Di%20Qi%20and%20Hao%20Li%20and%20Wei%20Cheng%20and%20Weiwei%20Cai%20and%20Shihao%20Wu%20and%20Jiarui%20Liu%20and%20Zihao%20Wang%20and%20Xiao%20Chen%20and%20Feipeng%20Tian%20and%20Jianxiong%20Pan%20and%20Zeming%20Li%20and%20Gang%20Yu%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%20and%20Ping%20Tan%0AAbstract%3A%20%20%20While%20generative%20artificial%20intelligence%20has%20advanced%20significantly%20across%0Atext%2C%20image%2C%20audio%2C%20and%20video%20domains%2C%203D%20generation%20remains%20comparatively%0Aunderdeveloped%20due%20to%20fundamental%20challenges%20such%20as%20data%20scarcity%2C%20algorithmic%0Alimitations%2C%20and%20ecosystem%20fragmentation.%20To%20this%20end%2C%20we%20present%20Step1X-3D%2C%20an%0Aopen%20framework%20addressing%20these%20challenges%20through%3A%20%281%29%20a%20rigorous%20data%0Acuration%20pipeline%20processing%20%3E5M%20assets%20to%20create%20a%202M%20high-quality%20dataset%0Awith%20standardized%20geometric%20and%20textural%20properties%3B%20%282%29%20a%20two-stage%203D-native%0Aarchitecture%20combining%20a%20hybrid%20VAE-DiT%20geometry%20generator%20with%20an%0Adiffusion-based%20texture%20synthesis%20module%3B%20and%20%283%29%20the%20full%20open-source%20release%0Aof%20models%2C%20training%20code%2C%20and%20adaptation%20modules.%20For%20geometry%20generation%2C%20the%0Ahybrid%20VAE-DiT%20component%20produces%20TSDF%20representations%20by%20employing%0Aperceiver-based%20latent%20encoding%20with%20sharp%20edge%20sampling%20for%20detail%0Apreservation.%20The%20diffusion-based%20texture%20synthesis%20module%20then%20ensures%0Across-view%20consistency%20through%20geometric%20conditioning%20and%20latent-space%0Asynchronization.%20Benchmark%20results%20demonstrate%20state-of-the-art%20performance%0Athat%20exceeds%20existing%20open-source%20methods%2C%20while%20also%20achieving%20competitive%0Aquality%20with%20proprietary%20solutions.%20Notably%2C%20the%20framework%20uniquely%20bridges%20the%0A2D%20and%203D%20generation%20paradigms%20by%20supporting%20direct%20transfer%20of%202D%20control%0Atechniques~%28e.g.%2C%20LoRA%29%20to%203D%20synthesis.%20By%20simultaneously%20advancing%20data%0Aquality%2C%20algorithmic%20fidelity%2C%20and%20reproducibility%2C%20Step1X-3D%20aims%20to%20establish%0Anew%20standards%20for%20open%20research%20in%20controllable%203D%20asset%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep1X-3D%253A%2520Towards%2520High-Fidelity%2520and%2520Controllable%2520Generation%2520of%2520Textured%250A%2520%25203D%2520Assets%26entry.906535625%3DWeiyu%2520Li%2520and%2520Xuanyang%2520Zhang%2520and%2520Zheng%2520Sun%2520and%2520Di%2520Qi%2520and%2520Hao%2520Li%2520and%2520Wei%2520Cheng%2520and%2520Weiwei%2520Cai%2520and%2520Shihao%2520Wu%2520and%2520Jiarui%2520Liu%2520and%2520Zihao%2520Wang%2520and%2520Xiao%2520Chen%2520and%2520Feipeng%2520Tian%2520and%2520Jianxiong%2520Pan%2520and%2520Zeming%2520Li%2520and%2520Gang%2520Yu%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520While%2520generative%2520artificial%2520intelligence%2520has%2520advanced%2520significantly%2520across%250Atext%252C%2520image%252C%2520audio%252C%2520and%2520video%2520domains%252C%25203D%2520generation%2520remains%2520comparatively%250Aunderdeveloped%2520due%2520to%2520fundamental%2520challenges%2520such%2520as%2520data%2520scarcity%252C%2520algorithmic%250Alimitations%252C%2520and%2520ecosystem%2520fragmentation.%2520To%2520this%2520end%252C%2520we%2520present%2520Step1X-3D%252C%2520an%250Aopen%2520framework%2520addressing%2520these%2520challenges%2520through%253A%2520%25281%2529%2520a%2520rigorous%2520data%250Acuration%2520pipeline%2520processing%2520%253E5M%2520assets%2520to%2520create%2520a%25202M%2520high-quality%2520dataset%250Awith%2520standardized%2520geometric%2520and%2520textural%2520properties%253B%2520%25282%2529%2520a%2520two-stage%25203D-native%250Aarchitecture%2520combining%2520a%2520hybrid%2520VAE-DiT%2520geometry%2520generator%2520with%2520an%250Adiffusion-based%2520texture%2520synthesis%2520module%253B%2520and%2520%25283%2529%2520the%2520full%2520open-source%2520release%250Aof%2520models%252C%2520training%2520code%252C%2520and%2520adaptation%2520modules.%2520For%2520geometry%2520generation%252C%2520the%250Ahybrid%2520VAE-DiT%2520component%2520produces%2520TSDF%2520representations%2520by%2520employing%250Aperceiver-based%2520latent%2520encoding%2520with%2520sharp%2520edge%2520sampling%2520for%2520detail%250Apreservation.%2520The%2520diffusion-based%2520texture%2520synthesis%2520module%2520then%2520ensures%250Across-view%2520consistency%2520through%2520geometric%2520conditioning%2520and%2520latent-space%250Asynchronization.%2520Benchmark%2520results%2520demonstrate%2520state-of-the-art%2520performance%250Athat%2520exceeds%2520existing%2520open-source%2520methods%252C%2520while%2520also%2520achieving%2520competitive%250Aquality%2520with%2520proprietary%2520solutions.%2520Notably%252C%2520the%2520framework%2520uniquely%2520bridges%2520the%250A2D%2520and%25203D%2520generation%2520paradigms%2520by%2520supporting%2520direct%2520transfer%2520of%25202D%2520control%250Atechniques~%2528e.g.%252C%2520LoRA%2529%2520to%25203D%2520synthesis.%2520By%2520simultaneously%2520advancing%2520data%250Aquality%252C%2520algorithmic%2520fidelity%252C%2520and%2520reproducibility%252C%2520Step1X-3D%2520aims%2520to%2520establish%250Anew%2520standards%2520for%2520open%2520research%2520in%2520controllable%25203D%2520asset%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step1X-3D%3A%20Towards%20High-Fidelity%20and%20Controllable%20Generation%20of%20Textured%0A%20%203D%20Assets&entry.906535625=Weiyu%20Li%20and%20Xuanyang%20Zhang%20and%20Zheng%20Sun%20and%20Di%20Qi%20and%20Hao%20Li%20and%20Wei%20Cheng%20and%20Weiwei%20Cai%20and%20Shihao%20Wu%20and%20Jiarui%20Liu%20and%20Zihao%20Wang%20and%20Xiao%20Chen%20and%20Feipeng%20Tian%20and%20Jianxiong%20Pan%20and%20Zeming%20Li%20and%20Gang%20Yu%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%20and%20Ping%20Tan&entry.1292438233=%20%20While%20generative%20artificial%20intelligence%20has%20advanced%20significantly%20across%0Atext%2C%20image%2C%20audio%2C%20and%20video%20domains%2C%203D%20generation%20remains%20comparatively%0Aunderdeveloped%20due%20to%20fundamental%20challenges%20such%20as%20data%20scarcity%2C%20algorithmic%0Alimitations%2C%20and%20ecosystem%20fragmentation.%20To%20this%20end%2C%20we%20present%20Step1X-3D%2C%20an%0Aopen%20framework%20addressing%20these%20challenges%20through%3A%20%281%29%20a%20rigorous%20data%0Acuration%20pipeline%20processing%20%3E5M%20assets%20to%20create%20a%202M%20high-quality%20dataset%0Awith%20standardized%20geometric%20and%20textural%20properties%3B%20%282%29%20a%20two-stage%203D-native%0Aarchitecture%20combining%20a%20hybrid%20VAE-DiT%20geometry%20generator%20with%20an%0Adiffusion-based%20texture%20synthesis%20module%3B%20and%20%283%29%20the%20full%20open-source%20release%0Aof%20models%2C%20training%20code%2C%20and%20adaptation%20modules.%20For%20geometry%20generation%2C%20the%0Ahybrid%20VAE-DiT%20component%20produces%20TSDF%20representations%20by%20employing%0Aperceiver-based%20latent%20encoding%20with%20sharp%20edge%20sampling%20for%20detail%0Apreservation.%20The%20diffusion-based%20texture%20synthesis%20module%20then%20ensures%0Across-view%20consistency%20through%20geometric%20conditioning%20and%20latent-space%0Asynchronization.%20Benchmark%20results%20demonstrate%20state-of-the-art%20performance%0Athat%20exceeds%20existing%20open-source%20methods%2C%20while%20also%20achieving%20competitive%0Aquality%20with%20proprietary%20solutions.%20Notably%2C%20the%20framework%20uniquely%20bridges%20the%0A2D%20and%203D%20generation%20paradigms%20by%20supporting%20direct%20transfer%20of%202D%20control%0Atechniques~%28e.g.%2C%20LoRA%29%20to%203D%20synthesis.%20By%20simultaneously%20advancing%20data%0Aquality%2C%20algorithmic%20fidelity%2C%20and%20reproducibility%2C%20Step1X-3D%20aims%20to%20establish%0Anew%20standards%20for%20open%20research%20in%20controllable%203D%20asset%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07747v1&entry.124074799=Read"},
{"title": "Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy\n  Surface Reconstruction", "author": "Xiaoming Peng and Yixin Yang and Yang Zhou and Hui Huang", "abstract": "  Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry\nreconstruction quality than the popular 3DGS by using 2D surfels to approximate\nthin surfaces. However, it falls short when dealing with glossy surfaces,\nresulting in visible holes in these areas. We found the reflection\ndiscontinuity causes the issue. To fit the jump from diffuse to specular\nreflection at different viewing angles, depth bias is introduced in the\noptimized Gaussian primitives. To address that, we first replace the depth\ndistortion loss in 2DGS with a novel depth convergence loss, which imposes a\nstrong constraint on depth continuity. Then, we rectified the depth criterion\nin determining the actual surface, which fully accounts for all the\nintersecting Gaussians along the ray. Qualitative and quantitative evaluations\nacross various datasets reveal that our method significantly improves\nreconstruction quality, with more complete and accurate surfaces than 2DGS.\n", "link": "http://arxiv.org/abs/2503.06587v2", "date": "2025-05-12", "relevancy": 3.2365, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6772}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6393}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Unbiased%20Depth%20into%202D%20Gaussian%20Splatting%20for%20High-accuracy%0A%20%20Surface%20Reconstruction&body=Title%3A%20Introducing%20Unbiased%20Depth%20into%202D%20Gaussian%20Splatting%20for%20High-accuracy%0A%20%20Surface%20Reconstruction%0AAuthor%3A%20Xiaoming%20Peng%20and%20Yixin%20Yang%20and%20Yang%20Zhou%20and%20Hui%20Huang%0AAbstract%3A%20%20%20Recently%2C%202D%20Gaussian%20Splatting%20%282DGS%29%20has%20demonstrated%20superior%20geometry%0Areconstruction%20quality%20than%20the%20popular%203DGS%20by%20using%202D%20surfels%20to%20approximate%0Athin%20surfaces.%20However%2C%20it%20falls%20short%20when%20dealing%20with%20glossy%20surfaces%2C%0Aresulting%20in%20visible%20holes%20in%20these%20areas.%20We%20found%20the%20reflection%0Adiscontinuity%20causes%20the%20issue.%20To%20fit%20the%20jump%20from%20diffuse%20to%20specular%0Areflection%20at%20different%20viewing%20angles%2C%20depth%20bias%20is%20introduced%20in%20the%0Aoptimized%20Gaussian%20primitives.%20To%20address%20that%2C%20we%20first%20replace%20the%20depth%0Adistortion%20loss%20in%202DGS%20with%20a%20novel%20depth%20convergence%20loss%2C%20which%20imposes%20a%0Astrong%20constraint%20on%20depth%20continuity.%20Then%2C%20we%20rectified%20the%20depth%20criterion%0Ain%20determining%20the%20actual%20surface%2C%20which%20fully%20accounts%20for%20all%20the%0Aintersecting%20Gaussians%20along%20the%20ray.%20Qualitative%20and%20quantitative%20evaluations%0Aacross%20various%20datasets%20reveal%20that%20our%20method%20significantly%20improves%0Areconstruction%20quality%2C%20with%20more%20complete%20and%20accurate%20surfaces%20than%202DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Unbiased%2520Depth%2520into%25202D%2520Gaussian%2520Splatting%2520for%2520High-accuracy%250A%2520%2520Surface%2520Reconstruction%26entry.906535625%3DXiaoming%2520Peng%2520and%2520Yixin%2520Yang%2520and%2520Yang%2520Zhou%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520Recently%252C%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520has%2520demonstrated%2520superior%2520geometry%250Areconstruction%2520quality%2520than%2520the%2520popular%25203DGS%2520by%2520using%25202D%2520surfels%2520to%2520approximate%250Athin%2520surfaces.%2520However%252C%2520it%2520falls%2520short%2520when%2520dealing%2520with%2520glossy%2520surfaces%252C%250Aresulting%2520in%2520visible%2520holes%2520in%2520these%2520areas.%2520We%2520found%2520the%2520reflection%250Adiscontinuity%2520causes%2520the%2520issue.%2520To%2520fit%2520the%2520jump%2520from%2520diffuse%2520to%2520specular%250Areflection%2520at%2520different%2520viewing%2520angles%252C%2520depth%2520bias%2520is%2520introduced%2520in%2520the%250Aoptimized%2520Gaussian%2520primitives.%2520To%2520address%2520that%252C%2520we%2520first%2520replace%2520the%2520depth%250Adistortion%2520loss%2520in%25202DGS%2520with%2520a%2520novel%2520depth%2520convergence%2520loss%252C%2520which%2520imposes%2520a%250Astrong%2520constraint%2520on%2520depth%2520continuity.%2520Then%252C%2520we%2520rectified%2520the%2520depth%2520criterion%250Ain%2520determining%2520the%2520actual%2520surface%252C%2520which%2520fully%2520accounts%2520for%2520all%2520the%250Aintersecting%2520Gaussians%2520along%2520the%2520ray.%2520Qualitative%2520and%2520quantitative%2520evaluations%250Aacross%2520various%2520datasets%2520reveal%2520that%2520our%2520method%2520significantly%2520improves%250Areconstruction%2520quality%252C%2520with%2520more%2520complete%2520and%2520accurate%2520surfaces%2520than%25202DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Unbiased%20Depth%20into%202D%20Gaussian%20Splatting%20for%20High-accuracy%0A%20%20Surface%20Reconstruction&entry.906535625=Xiaoming%20Peng%20and%20Yixin%20Yang%20and%20Yang%20Zhou%20and%20Hui%20Huang&entry.1292438233=%20%20Recently%2C%202D%20Gaussian%20Splatting%20%282DGS%29%20has%20demonstrated%20superior%20geometry%0Areconstruction%20quality%20than%20the%20popular%203DGS%20by%20using%202D%20surfels%20to%20approximate%0Athin%20surfaces.%20However%2C%20it%20falls%20short%20when%20dealing%20with%20glossy%20surfaces%2C%0Aresulting%20in%20visible%20holes%20in%20these%20areas.%20We%20found%20the%20reflection%0Adiscontinuity%20causes%20the%20issue.%20To%20fit%20the%20jump%20from%20diffuse%20to%20specular%0Areflection%20at%20different%20viewing%20angles%2C%20depth%20bias%20is%20introduced%20in%20the%0Aoptimized%20Gaussian%20primitives.%20To%20address%20that%2C%20we%20first%20replace%20the%20depth%0Adistortion%20loss%20in%202DGS%20with%20a%20novel%20depth%20convergence%20loss%2C%20which%20imposes%20a%0Astrong%20constraint%20on%20depth%20continuity.%20Then%2C%20we%20rectified%20the%20depth%20criterion%0Ain%20determining%20the%20actual%20surface%2C%20which%20fully%20accounts%20for%20all%20the%0Aintersecting%20Gaussians%20along%20the%20ray.%20Qualitative%20and%20quantitative%20evaluations%0Aacross%20various%20datasets%20reveal%20that%20our%20method%20significantly%20improves%0Areconstruction%20quality%2C%20with%20more%20complete%20and%20accurate%20surfaces%20than%202DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06587v2&entry.124074799=Read"},
{"title": "DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D\n  Object Detection", "author": "Mingqian Ji and Jian Yang and Shanshan Zhang", "abstract": "  State-of-the-art LiDAR-camera 3D object detectors usually focus on feature\nfusion. However, they neglect the factor of depth while designing the fusion\nstrategy. In this work, we are the first to observe that different modalities\nplay different roles as depth varies via statistical analysis and\nvisualization. Based on this finding, we propose a Depth-Aware Hybrid Feature\nFusion (DepthFusion) strategy that guides the weights of point cloud and RGB\nimage modalities by introducing depth encoding at both global and local levels.\nSpecifically, the Depth-GFusion module adaptively adjusts the weights of image\nBird's-Eye-View (BEV) features in multi-modal global features via depth\nencoding. Furthermore, to compensate for the information lost when transferring\nraw features to the BEV space, we propose a Depth-LFusion module, which\nadaptively adjusts the weights of original voxel features and multi-view image\nfeatures in multi-modal local features via depth encoding. Extensive\nexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion\nmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusion\nis more robust to various kinds of corruptions, outperforming previous methods\non the nuScenes-C dataset.\n", "link": "http://arxiv.org/abs/2505.07398v1", "date": "2025-05-12", "relevancy": 3.0369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthFusion%3A%20Depth-Aware%20Hybrid%20Feature%20Fusion%20for%20LiDAR-Camera%203D%0A%20%20Object%20Detection&body=Title%3A%20DepthFusion%3A%20Depth-Aware%20Hybrid%20Feature%20Fusion%20for%20LiDAR-Camera%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Mingqian%20Ji%20and%20Jian%20Yang%20and%20Shanshan%20Zhang%0AAbstract%3A%20%20%20State-of-the-art%20LiDAR-camera%203D%20object%20detectors%20usually%20focus%20on%20feature%0Afusion.%20However%2C%20they%20neglect%20the%20factor%20of%20depth%20while%20designing%20the%20fusion%0Astrategy.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20observe%20that%20different%20modalities%0Aplay%20different%20roles%20as%20depth%20varies%20via%20statistical%20analysis%20and%0Avisualization.%20Based%20on%20this%20finding%2C%20we%20propose%20a%20Depth-Aware%20Hybrid%20Feature%0AFusion%20%28DepthFusion%29%20strategy%20that%20guides%20the%20weights%20of%20point%20cloud%20and%20RGB%0Aimage%20modalities%20by%20introducing%20depth%20encoding%20at%20both%20global%20and%20local%20levels.%0ASpecifically%2C%20the%20Depth-GFusion%20module%20adaptively%20adjusts%20the%20weights%20of%20image%0ABird%27s-Eye-View%20%28BEV%29%20features%20in%20multi-modal%20global%20features%20via%20depth%0Aencoding.%20Furthermore%2C%20to%20compensate%20for%20the%20information%20lost%20when%20transferring%0Araw%20features%20to%20the%20BEV%20space%2C%20we%20propose%20a%20Depth-LFusion%20module%2C%20which%0Aadaptively%20adjusts%20the%20weights%20of%20original%20voxel%20features%20and%20multi-view%20image%0Afeatures%20in%20multi-modal%20local%20features%20via%20depth%20encoding.%20Extensive%0Aexperiments%20on%20the%20nuScenes%20and%20KITTI%20datasets%20demonstrate%20that%20our%20DepthFusion%0Amethod%20surpasses%20previous%20state-of-the-art%20methods.%20Moreover%2C%20our%20DepthFusion%0Ais%20more%20robust%20to%20various%20kinds%20of%20corruptions%2C%20outperforming%20previous%20methods%0Aon%20the%20nuScenes-C%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthFusion%253A%2520Depth-Aware%2520Hybrid%2520Feature%2520Fusion%2520for%2520LiDAR-Camera%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DMingqian%2520Ji%2520and%2520Jian%2520Yang%2520and%2520Shanshan%2520Zhang%26entry.1292438233%3D%2520%2520State-of-the-art%2520LiDAR-camera%25203D%2520object%2520detectors%2520usually%2520focus%2520on%2520feature%250Afusion.%2520However%252C%2520they%2520neglect%2520the%2520factor%2520of%2520depth%2520while%2520designing%2520the%2520fusion%250Astrategy.%2520In%2520this%2520work%252C%2520we%2520are%2520the%2520first%2520to%2520observe%2520that%2520different%2520modalities%250Aplay%2520different%2520roles%2520as%2520depth%2520varies%2520via%2520statistical%2520analysis%2520and%250Avisualization.%2520Based%2520on%2520this%2520finding%252C%2520we%2520propose%2520a%2520Depth-Aware%2520Hybrid%2520Feature%250AFusion%2520%2528DepthFusion%2529%2520strategy%2520that%2520guides%2520the%2520weights%2520of%2520point%2520cloud%2520and%2520RGB%250Aimage%2520modalities%2520by%2520introducing%2520depth%2520encoding%2520at%2520both%2520global%2520and%2520local%2520levels.%250ASpecifically%252C%2520the%2520Depth-GFusion%2520module%2520adaptively%2520adjusts%2520the%2520weights%2520of%2520image%250ABird%2527s-Eye-View%2520%2528BEV%2529%2520features%2520in%2520multi-modal%2520global%2520features%2520via%2520depth%250Aencoding.%2520Furthermore%252C%2520to%2520compensate%2520for%2520the%2520information%2520lost%2520when%2520transferring%250Araw%2520features%2520to%2520the%2520BEV%2520space%252C%2520we%2520propose%2520a%2520Depth-LFusion%2520module%252C%2520which%250Aadaptively%2520adjusts%2520the%2520weights%2520of%2520original%2520voxel%2520features%2520and%2520multi-view%2520image%250Afeatures%2520in%2520multi-modal%2520local%2520features%2520via%2520depth%2520encoding.%2520Extensive%250Aexperiments%2520on%2520the%2520nuScenes%2520and%2520KITTI%2520datasets%2520demonstrate%2520that%2520our%2520DepthFusion%250Amethod%2520surpasses%2520previous%2520state-of-the-art%2520methods.%2520Moreover%252C%2520our%2520DepthFusion%250Ais%2520more%2520robust%2520to%2520various%2520kinds%2520of%2520corruptions%252C%2520outperforming%2520previous%2520methods%250Aon%2520the%2520nuScenes-C%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthFusion%3A%20Depth-Aware%20Hybrid%20Feature%20Fusion%20for%20LiDAR-Camera%203D%0A%20%20Object%20Detection&entry.906535625=Mingqian%20Ji%20and%20Jian%20Yang%20and%20Shanshan%20Zhang&entry.1292438233=%20%20State-of-the-art%20LiDAR-camera%203D%20object%20detectors%20usually%20focus%20on%20feature%0Afusion.%20However%2C%20they%20neglect%20the%20factor%20of%20depth%20while%20designing%20the%20fusion%0Astrategy.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20observe%20that%20different%20modalities%0Aplay%20different%20roles%20as%20depth%20varies%20via%20statistical%20analysis%20and%0Avisualization.%20Based%20on%20this%20finding%2C%20we%20propose%20a%20Depth-Aware%20Hybrid%20Feature%0AFusion%20%28DepthFusion%29%20strategy%20that%20guides%20the%20weights%20of%20point%20cloud%20and%20RGB%0Aimage%20modalities%20by%20introducing%20depth%20encoding%20at%20both%20global%20and%20local%20levels.%0ASpecifically%2C%20the%20Depth-GFusion%20module%20adaptively%20adjusts%20the%20weights%20of%20image%0ABird%27s-Eye-View%20%28BEV%29%20features%20in%20multi-modal%20global%20features%20via%20depth%0Aencoding.%20Furthermore%2C%20to%20compensate%20for%20the%20information%20lost%20when%20transferring%0Araw%20features%20to%20the%20BEV%20space%2C%20we%20propose%20a%20Depth-LFusion%20module%2C%20which%0Aadaptively%20adjusts%20the%20weights%20of%20original%20voxel%20features%20and%20multi-view%20image%0Afeatures%20in%20multi-modal%20local%20features%20via%20depth%20encoding.%20Extensive%0Aexperiments%20on%20the%20nuScenes%20and%20KITTI%20datasets%20demonstrate%20that%20our%20DepthFusion%0Amethod%20surpasses%20previous%20state-of-the-art%20methods.%20Moreover%2C%20our%20DepthFusion%0Ais%20more%20robust%20to%20various%20kinds%20of%20corruptions%2C%20outperforming%20previous%20methods%0Aon%20the%20nuScenes-C%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07398v1&entry.124074799=Read"},
{"title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream", "author": "Hao Li and Sicheng Li and Xiang Gao and Abudouaihati Batuer and Lu Yu and Yiyi Liao", "abstract": "  Immersive video offers a 6-Dof-free viewing experience, potentially playing a\nkey role in future video technology. Recently, 4D Gaussian Splatting has gained\nattention as an effective approach for immersive video due to its high\nrendering efficiency and quality, though maintaining quality with manageable\nstorage remains challenging. To address this, we introduce GIFStream, a novel\n4D Gaussian representation using a canonical space and a deformation field\nenhanced with time-dependent feature streams. These feature streams enable\ncomplex motion modeling and allow efficient compression by leveraging temporal\ncorrespondence and motion-aware pruning. Additionally, we incorporate both\ntemporal and spatial compression networks for end-to-end compression.\nExperimental results show that GIFStream delivers high-quality immersive video\nat 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project\npage: https://xdimlab.github.io/GIFStream\n", "link": "http://arxiv.org/abs/2505.07539v1", "date": "2025-05-12", "relevancy": 2.9808, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.618}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5939}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIFStream%3A%204D%20Gaussian-based%20Immersive%20Video%20with%20Feature%20Stream&body=Title%3A%20GIFStream%3A%204D%20Gaussian-based%20Immersive%20Video%20with%20Feature%20Stream%0AAuthor%3A%20Hao%20Li%20and%20Sicheng%20Li%20and%20Xiang%20Gao%20and%20Abudouaihati%20Batuer%20and%20Lu%20Yu%20and%20Yiyi%20Liao%0AAbstract%3A%20%20%20Immersive%20video%20offers%20a%206-Dof-free%20viewing%20experience%2C%20potentially%20playing%20a%0Akey%20role%20in%20future%20video%20technology.%20Recently%2C%204D%20Gaussian%20Splatting%20has%20gained%0Aattention%20as%20an%20effective%20approach%20for%20immersive%20video%20due%20to%20its%20high%0Arendering%20efficiency%20and%20quality%2C%20though%20maintaining%20quality%20with%20manageable%0Astorage%20remains%20challenging.%20To%20address%20this%2C%20we%20introduce%20GIFStream%2C%20a%20novel%0A4D%20Gaussian%20representation%20using%20a%20canonical%20space%20and%20a%20deformation%20field%0Aenhanced%20with%20time-dependent%20feature%20streams.%20These%20feature%20streams%20enable%0Acomplex%20motion%20modeling%20and%20allow%20efficient%20compression%20by%20leveraging%20temporal%0Acorrespondence%20and%20motion-aware%20pruning.%20Additionally%2C%20we%20incorporate%20both%0Atemporal%20and%20spatial%20compression%20networks%20for%20end-to-end%20compression.%0AExperimental%20results%20show%20that%20GIFStream%20delivers%20high-quality%20immersive%20video%0Aat%2030%20Mbps%2C%20with%20real-time%20rendering%20and%20fast%20decoding%20on%20an%20RTX%204090.%20Project%0Apage%3A%20https%3A//xdimlab.github.io/GIFStream%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIFStream%253A%25204D%2520Gaussian-based%2520Immersive%2520Video%2520with%2520Feature%2520Stream%26entry.906535625%3DHao%2520Li%2520and%2520Sicheng%2520Li%2520and%2520Xiang%2520Gao%2520and%2520Abudouaihati%2520Batuer%2520and%2520Lu%2520Yu%2520and%2520Yiyi%2520Liao%26entry.1292438233%3D%2520%2520Immersive%2520video%2520offers%2520a%25206-Dof-free%2520viewing%2520experience%252C%2520potentially%2520playing%2520a%250Akey%2520role%2520in%2520future%2520video%2520technology.%2520Recently%252C%25204D%2520Gaussian%2520Splatting%2520has%2520gained%250Aattention%2520as%2520an%2520effective%2520approach%2520for%2520immersive%2520video%2520due%2520to%2520its%2520high%250Arendering%2520efficiency%2520and%2520quality%252C%2520though%2520maintaining%2520quality%2520with%2520manageable%250Astorage%2520remains%2520challenging.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GIFStream%252C%2520a%2520novel%250A4D%2520Gaussian%2520representation%2520using%2520a%2520canonical%2520space%2520and%2520a%2520deformation%2520field%250Aenhanced%2520with%2520time-dependent%2520feature%2520streams.%2520These%2520feature%2520streams%2520enable%250Acomplex%2520motion%2520modeling%2520and%2520allow%2520efficient%2520compression%2520by%2520leveraging%2520temporal%250Acorrespondence%2520and%2520motion-aware%2520pruning.%2520Additionally%252C%2520we%2520incorporate%2520both%250Atemporal%2520and%2520spatial%2520compression%2520networks%2520for%2520end-to-end%2520compression.%250AExperimental%2520results%2520show%2520that%2520GIFStream%2520delivers%2520high-quality%2520immersive%2520video%250Aat%252030%2520Mbps%252C%2520with%2520real-time%2520rendering%2520and%2520fast%2520decoding%2520on%2520an%2520RTX%25204090.%2520Project%250Apage%253A%2520https%253A//xdimlab.github.io/GIFStream%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIFStream%3A%204D%20Gaussian-based%20Immersive%20Video%20with%20Feature%20Stream&entry.906535625=Hao%20Li%20and%20Sicheng%20Li%20and%20Xiang%20Gao%20and%20Abudouaihati%20Batuer%20and%20Lu%20Yu%20and%20Yiyi%20Liao&entry.1292438233=%20%20Immersive%20video%20offers%20a%206-Dof-free%20viewing%20experience%2C%20potentially%20playing%20a%0Akey%20role%20in%20future%20video%20technology.%20Recently%2C%204D%20Gaussian%20Splatting%20has%20gained%0Aattention%20as%20an%20effective%20approach%20for%20immersive%20video%20due%20to%20its%20high%0Arendering%20efficiency%20and%20quality%2C%20though%20maintaining%20quality%20with%20manageable%0Astorage%20remains%20challenging.%20To%20address%20this%2C%20we%20introduce%20GIFStream%2C%20a%20novel%0A4D%20Gaussian%20representation%20using%20a%20canonical%20space%20and%20a%20deformation%20field%0Aenhanced%20with%20time-dependent%20feature%20streams.%20These%20feature%20streams%20enable%0Acomplex%20motion%20modeling%20and%20allow%20efficient%20compression%20by%20leveraging%20temporal%0Acorrespondence%20and%20motion-aware%20pruning.%20Additionally%2C%20we%20incorporate%20both%0Atemporal%20and%20spatial%20compression%20networks%20for%20end-to-end%20compression.%0AExperimental%20results%20show%20that%20GIFStream%20delivers%20high-quality%20immersive%20video%0Aat%2030%20Mbps%2C%20with%20real-time%20rendering%20and%20fast%20decoding%20on%20an%20RTX%204090.%20Project%0Apage%3A%20https%3A//xdimlab.github.io/GIFStream%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07539v1&entry.124074799=Read"},
{"title": "LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided\n  Attention", "author": "Jiangling Zhang and Weijie Zhu and Jirui Huang and Yaxiong Chen", "abstract": "  Detecting AI-synthetic faces presents a critical challenge: it is hard to\ncapture consistent structural relationships between facial regions across\ndiverse generation techniques. Current methods, which focus on specific\nartifacts rather than fundamental inconsistencies, often fail when confronted\nwith novel generative models. To address this limitation, we introduce\nLayer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer\ndesigned for robust facial forgery detection. This model integrates distinct\nRegion-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation\n(LAMM) components within each layer. RG-MHA utilizes facial landmarks to create\nregional attention masks, guiding the model to scrutinize architectural\ninconsistencies across different facial areas. Crucially, the separate LAMM\nmodule dynamically generates layer-specific parameters, including mask weights\nand gating values, based on network context. These parameters then modulate the\nbehavior of RG-MHA, enabling adaptive adjustment of regional focus across\nnetwork depths. This architecture facilitates the capture of subtle,\nhierarchical forgery cues ubiquitous among diverse generation techniques, such\nas GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT\ndemonstrates superior performance, achieving 94.09% mean ACC (a +5.45%\nimprovement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results\ndemonstrate LAMM-ViT's exceptional ability to generalize and its potential for\nreliable deployment against evolving synthetic media threats.\n", "link": "http://arxiv.org/abs/2505.07734v1", "date": "2025-05-12", "relevancy": 2.903, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5842}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAMM-ViT%3A%20AI%20Face%20Detection%20via%20Layer-Aware%20Modulation%20of%20Region-Guided%0A%20%20Attention&body=Title%3A%20LAMM-ViT%3A%20AI%20Face%20Detection%20via%20Layer-Aware%20Modulation%20of%20Region-Guided%0A%20%20Attention%0AAuthor%3A%20Jiangling%20Zhang%20and%20Weijie%20Zhu%20and%20Jirui%20Huang%20and%20Yaxiong%20Chen%0AAbstract%3A%20%20%20Detecting%20AI-synthetic%20faces%20presents%20a%20critical%20challenge%3A%20it%20is%20hard%20to%0Acapture%20consistent%20structural%20relationships%20between%20facial%20regions%20across%0Adiverse%20generation%20techniques.%20Current%20methods%2C%20which%20focus%20on%20specific%0Aartifacts%20rather%20than%20fundamental%20inconsistencies%2C%20often%20fail%20when%20confronted%0Awith%20novel%20generative%20models.%20To%20address%20this%20limitation%2C%20we%20introduce%0ALayer-aware%20Mask%20Modulation%20Vision%20Transformer%20%28LAMM-ViT%29%2C%20a%20Vision%20Transformer%0Adesigned%20for%20robust%20facial%20forgery%20detection.%20This%20model%20integrates%20distinct%0ARegion-Guided%20Multi-Head%20Attention%20%28RG-MHA%29%20and%20Layer-aware%20Mask%20Modulation%0A%28LAMM%29%20components%20within%20each%20layer.%20RG-MHA%20utilizes%20facial%20landmarks%20to%20create%0Aregional%20attention%20masks%2C%20guiding%20the%20model%20to%20scrutinize%20architectural%0Ainconsistencies%20across%20different%20facial%20areas.%20Crucially%2C%20the%20separate%20LAMM%0Amodule%20dynamically%20generates%20layer-specific%20parameters%2C%20including%20mask%20weights%0Aand%20gating%20values%2C%20based%20on%20network%20context.%20These%20parameters%20then%20modulate%20the%0Abehavior%20of%20RG-MHA%2C%20enabling%20adaptive%20adjustment%20of%20regional%20focus%20across%0Anetwork%20depths.%20This%20architecture%20facilitates%20the%20capture%20of%20subtle%2C%0Ahierarchical%20forgery%20cues%20ubiquitous%20among%20diverse%20generation%20techniques%2C%20such%0Aas%20GANs%20and%20Diffusion%20Models.%20In%20cross-model%20generalization%20tests%2C%20LAMM-ViT%0Ademonstrates%20superior%20performance%2C%20achieving%2094.09%25%20mean%20ACC%20%28a%20%2B5.45%25%0Aimprovement%20over%20SoTA%29%20and%2098.62%25%20mean%20AP%20%28a%20%2B3.09%25%20improvement%29.%20These%20results%0Ademonstrate%20LAMM-ViT%27s%20exceptional%20ability%20to%20generalize%20and%20its%20potential%20for%0Areliable%20deployment%20against%20evolving%20synthetic%20media%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAMM-ViT%253A%2520AI%2520Face%2520Detection%2520via%2520Layer-Aware%2520Modulation%2520of%2520Region-Guided%250A%2520%2520Attention%26entry.906535625%3DJiangling%2520Zhang%2520and%2520Weijie%2520Zhu%2520and%2520Jirui%2520Huang%2520and%2520Yaxiong%2520Chen%26entry.1292438233%3D%2520%2520Detecting%2520AI-synthetic%2520faces%2520presents%2520a%2520critical%2520challenge%253A%2520it%2520is%2520hard%2520to%250Acapture%2520consistent%2520structural%2520relationships%2520between%2520facial%2520regions%2520across%250Adiverse%2520generation%2520techniques.%2520Current%2520methods%252C%2520which%2520focus%2520on%2520specific%250Aartifacts%2520rather%2520than%2520fundamental%2520inconsistencies%252C%2520often%2520fail%2520when%2520confronted%250Awith%2520novel%2520generative%2520models.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250ALayer-aware%2520Mask%2520Modulation%2520Vision%2520Transformer%2520%2528LAMM-ViT%2529%252C%2520a%2520Vision%2520Transformer%250Adesigned%2520for%2520robust%2520facial%2520forgery%2520detection.%2520This%2520model%2520integrates%2520distinct%250ARegion-Guided%2520Multi-Head%2520Attention%2520%2528RG-MHA%2529%2520and%2520Layer-aware%2520Mask%2520Modulation%250A%2528LAMM%2529%2520components%2520within%2520each%2520layer.%2520RG-MHA%2520utilizes%2520facial%2520landmarks%2520to%2520create%250Aregional%2520attention%2520masks%252C%2520guiding%2520the%2520model%2520to%2520scrutinize%2520architectural%250Ainconsistencies%2520across%2520different%2520facial%2520areas.%2520Crucially%252C%2520the%2520separate%2520LAMM%250Amodule%2520dynamically%2520generates%2520layer-specific%2520parameters%252C%2520including%2520mask%2520weights%250Aand%2520gating%2520values%252C%2520based%2520on%2520network%2520context.%2520These%2520parameters%2520then%2520modulate%2520the%250Abehavior%2520of%2520RG-MHA%252C%2520enabling%2520adaptive%2520adjustment%2520of%2520regional%2520focus%2520across%250Anetwork%2520depths.%2520This%2520architecture%2520facilitates%2520the%2520capture%2520of%2520subtle%252C%250Ahierarchical%2520forgery%2520cues%2520ubiquitous%2520among%2520diverse%2520generation%2520techniques%252C%2520such%250Aas%2520GANs%2520and%2520Diffusion%2520Models.%2520In%2520cross-model%2520generalization%2520tests%252C%2520LAMM-ViT%250Ademonstrates%2520superior%2520performance%252C%2520achieving%252094.09%2525%2520mean%2520ACC%2520%2528a%2520%252B5.45%2525%250Aimprovement%2520over%2520SoTA%2529%2520and%252098.62%2525%2520mean%2520AP%2520%2528a%2520%252B3.09%2525%2520improvement%2529.%2520These%2520results%250Ademonstrate%2520LAMM-ViT%2527s%2520exceptional%2520ability%2520to%2520generalize%2520and%2520its%2520potential%2520for%250Areliable%2520deployment%2520against%2520evolving%2520synthetic%2520media%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAMM-ViT%3A%20AI%20Face%20Detection%20via%20Layer-Aware%20Modulation%20of%20Region-Guided%0A%20%20Attention&entry.906535625=Jiangling%20Zhang%20and%20Weijie%20Zhu%20and%20Jirui%20Huang%20and%20Yaxiong%20Chen&entry.1292438233=%20%20Detecting%20AI-synthetic%20faces%20presents%20a%20critical%20challenge%3A%20it%20is%20hard%20to%0Acapture%20consistent%20structural%20relationships%20between%20facial%20regions%20across%0Adiverse%20generation%20techniques.%20Current%20methods%2C%20which%20focus%20on%20specific%0Aartifacts%20rather%20than%20fundamental%20inconsistencies%2C%20often%20fail%20when%20confronted%0Awith%20novel%20generative%20models.%20To%20address%20this%20limitation%2C%20we%20introduce%0ALayer-aware%20Mask%20Modulation%20Vision%20Transformer%20%28LAMM-ViT%29%2C%20a%20Vision%20Transformer%0Adesigned%20for%20robust%20facial%20forgery%20detection.%20This%20model%20integrates%20distinct%0ARegion-Guided%20Multi-Head%20Attention%20%28RG-MHA%29%20and%20Layer-aware%20Mask%20Modulation%0A%28LAMM%29%20components%20within%20each%20layer.%20RG-MHA%20utilizes%20facial%20landmarks%20to%20create%0Aregional%20attention%20masks%2C%20guiding%20the%20model%20to%20scrutinize%20architectural%0Ainconsistencies%20across%20different%20facial%20areas.%20Crucially%2C%20the%20separate%20LAMM%0Amodule%20dynamically%20generates%20layer-specific%20parameters%2C%20including%20mask%20weights%0Aand%20gating%20values%2C%20based%20on%20network%20context.%20These%20parameters%20then%20modulate%20the%0Abehavior%20of%20RG-MHA%2C%20enabling%20adaptive%20adjustment%20of%20regional%20focus%20across%0Anetwork%20depths.%20This%20architecture%20facilitates%20the%20capture%20of%20subtle%2C%0Ahierarchical%20forgery%20cues%20ubiquitous%20among%20diverse%20generation%20techniques%2C%20such%0Aas%20GANs%20and%20Diffusion%20Models.%20In%20cross-model%20generalization%20tests%2C%20LAMM-ViT%0Ademonstrates%20superior%20performance%2C%20achieving%2094.09%25%20mean%20ACC%20%28a%20%2B5.45%25%0Aimprovement%20over%20SoTA%29%20and%2098.62%25%20mean%20AP%20%28a%20%2B3.09%25%20improvement%29.%20These%20results%0Ademonstrate%20LAMM-ViT%27s%20exceptional%20ability%20to%20generalize%20and%20its%20potential%20for%0Areliable%20deployment%20against%20evolving%20synthetic%20media%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07734v1&entry.124074799=Read"},
{"title": "Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter\n  for Continual Learning of Vision-Language Models", "author": "Songlin Dong and Chenhao Ding and Jiangyang Li and Jizhou Han and Qiang Wang and Yuhang He and Yihong Gong", "abstract": "  This study aims to address the problem of multi-domain task incremental\nlearning~(MTIL), which requires that vision-language models~(VLMs) continuously\nacquire new knowledge while maintaining their inherent zero-shot recognition\ncapability. Existing paradigms delegate the testing of unseen-domain samples to\nthe original CLIP, which only prevents the degradation of the model's zero-shot\ncapability but fails to enhance the generalization of the VLM further. To this\nend, we propose a novel MTIL framework, named AFA, which comprises two core\nmodules: (1) an against forward-forgetting adapter that learns task-invariant\ninformation for each dataset in the incremental tasks to enhance the zero-shot\nrecognition ability of VLMs; (2) an against backward-forgetting adapter that\nstrengthens the few-shot learning capability of VLMs while supporting\nincremental learning. Extensive experiments demonstrate that the AFA method\nsignificantly outperforms existing state-of-the-art approaches, especially in\nfew-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP\nin terms of transferability. The code is provided in the Supplementary\nMaterial.\n", "link": "http://arxiv.org/abs/2505.07690v1", "date": "2025-05-12", "relevancy": 2.8561, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20CLIP%20Generalization%3A%20Against%20Forward%26Backward%20Forgetting%20Adapter%0A%20%20for%20Continual%20Learning%20of%20Vision-Language%20Models&body=Title%3A%20Beyond%20CLIP%20Generalization%3A%20Against%20Forward%26Backward%20Forgetting%20Adapter%0A%20%20for%20Continual%20Learning%20of%20Vision-Language%20Models%0AAuthor%3A%20Songlin%20Dong%20and%20Chenhao%20Ding%20and%20Jiangyang%20Li%20and%20Jizhou%20Han%20and%20Qiang%20Wang%20and%20Yuhang%20He%20and%20Yihong%20Gong%0AAbstract%3A%20%20%20This%20study%20aims%20to%20address%20the%20problem%20of%20multi-domain%20task%20incremental%0Alearning~%28MTIL%29%2C%20which%20requires%20that%20vision-language%20models~%28VLMs%29%20continuously%0Aacquire%20new%20knowledge%20while%20maintaining%20their%20inherent%20zero-shot%20recognition%0Acapability.%20Existing%20paradigms%20delegate%20the%20testing%20of%20unseen-domain%20samples%20to%0Athe%20original%20CLIP%2C%20which%20only%20prevents%20the%20degradation%20of%20the%20model%27s%20zero-shot%0Acapability%20but%20fails%20to%20enhance%20the%20generalization%20of%20the%20VLM%20further.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20MTIL%20framework%2C%20named%20AFA%2C%20which%20comprises%20two%20core%0Amodules%3A%20%281%29%20an%20against%20forward-forgetting%20adapter%20that%20learns%20task-invariant%0Ainformation%20for%20each%20dataset%20in%20the%20incremental%20tasks%20to%20enhance%20the%20zero-shot%0Arecognition%20ability%20of%20VLMs%3B%20%282%29%20an%20against%20backward-forgetting%20adapter%20that%0Astrengthens%20the%20few-shot%20learning%20capability%20of%20VLMs%20while%20supporting%0Aincremental%20learning.%20Extensive%20experiments%20demonstrate%20that%20the%20AFA%20method%0Asignificantly%20outperforms%20existing%20state-of-the-art%20approaches%2C%20especially%20in%0Afew-shot%20MTIL%20tasks%2C%20and%20surpasses%20the%20inherent%20zero-shot%20performance%20of%20CLIP%0Ain%20terms%20of%20transferability.%20The%20code%20is%20provided%20in%20the%20Supplementary%0AMaterial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520CLIP%2520Generalization%253A%2520Against%2520Forward%2526Backward%2520Forgetting%2520Adapter%250A%2520%2520for%2520Continual%2520Learning%2520of%2520Vision-Language%2520Models%26entry.906535625%3DSonglin%2520Dong%2520and%2520Chenhao%2520Ding%2520and%2520Jiangyang%2520Li%2520and%2520Jizhou%2520Han%2520and%2520Qiang%2520Wang%2520and%2520Yuhang%2520He%2520and%2520Yihong%2520Gong%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520address%2520the%2520problem%2520of%2520multi-domain%2520task%2520incremental%250Alearning~%2528MTIL%2529%252C%2520which%2520requires%2520that%2520vision-language%2520models~%2528VLMs%2529%2520continuously%250Aacquire%2520new%2520knowledge%2520while%2520maintaining%2520their%2520inherent%2520zero-shot%2520recognition%250Acapability.%2520Existing%2520paradigms%2520delegate%2520the%2520testing%2520of%2520unseen-domain%2520samples%2520to%250Athe%2520original%2520CLIP%252C%2520which%2520only%2520prevents%2520the%2520degradation%2520of%2520the%2520model%2527s%2520zero-shot%250Acapability%2520but%2520fails%2520to%2520enhance%2520the%2520generalization%2520of%2520the%2520VLM%2520further.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520MTIL%2520framework%252C%2520named%2520AFA%252C%2520which%2520comprises%2520two%2520core%250Amodules%253A%2520%25281%2529%2520an%2520against%2520forward-forgetting%2520adapter%2520that%2520learns%2520task-invariant%250Ainformation%2520for%2520each%2520dataset%2520in%2520the%2520incremental%2520tasks%2520to%2520enhance%2520the%2520zero-shot%250Arecognition%2520ability%2520of%2520VLMs%253B%2520%25282%2529%2520an%2520against%2520backward-forgetting%2520adapter%2520that%250Astrengthens%2520the%2520few-shot%2520learning%2520capability%2520of%2520VLMs%2520while%2520supporting%250Aincremental%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520AFA%2520method%250Asignificantly%2520outperforms%2520existing%2520state-of-the-art%2520approaches%252C%2520especially%2520in%250Afew-shot%2520MTIL%2520tasks%252C%2520and%2520surpasses%2520the%2520inherent%2520zero-shot%2520performance%2520of%2520CLIP%250Ain%2520terms%2520of%2520transferability.%2520The%2520code%2520is%2520provided%2520in%2520the%2520Supplementary%250AMaterial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20CLIP%20Generalization%3A%20Against%20Forward%26Backward%20Forgetting%20Adapter%0A%20%20for%20Continual%20Learning%20of%20Vision-Language%20Models&entry.906535625=Songlin%20Dong%20and%20Chenhao%20Ding%20and%20Jiangyang%20Li%20and%20Jizhou%20Han%20and%20Qiang%20Wang%20and%20Yuhang%20He%20and%20Yihong%20Gong&entry.1292438233=%20%20This%20study%20aims%20to%20address%20the%20problem%20of%20multi-domain%20task%20incremental%0Alearning~%28MTIL%29%2C%20which%20requires%20that%20vision-language%20models~%28VLMs%29%20continuously%0Aacquire%20new%20knowledge%20while%20maintaining%20their%20inherent%20zero-shot%20recognition%0Acapability.%20Existing%20paradigms%20delegate%20the%20testing%20of%20unseen-domain%20samples%20to%0Athe%20original%20CLIP%2C%20which%20only%20prevents%20the%20degradation%20of%20the%20model%27s%20zero-shot%0Acapability%20but%20fails%20to%20enhance%20the%20generalization%20of%20the%20VLM%20further.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20MTIL%20framework%2C%20named%20AFA%2C%20which%20comprises%20two%20core%0Amodules%3A%20%281%29%20an%20against%20forward-forgetting%20adapter%20that%20learns%20task-invariant%0Ainformation%20for%20each%20dataset%20in%20the%20incremental%20tasks%20to%20enhance%20the%20zero-shot%0Arecognition%20ability%20of%20VLMs%3B%20%282%29%20an%20against%20backward-forgetting%20adapter%20that%0Astrengthens%20the%20few-shot%20learning%20capability%20of%20VLMs%20while%20supporting%0Aincremental%20learning.%20Extensive%20experiments%20demonstrate%20that%20the%20AFA%20method%0Asignificantly%20outperforms%20existing%20state-of-the-art%20approaches%2C%20especially%20in%0Afew-shot%20MTIL%20tasks%2C%20and%20surpasses%20the%20inherent%20zero-shot%20performance%20of%20CLIP%0Ain%20terms%20of%20transferability.%20The%20code%20is%20provided%20in%20the%20Supplementary%0AMaterial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07690v1&entry.124074799=Read"},
{"title": "ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical\n  Image Translation", "author": "Feng Yuan and Yifan Gao and Wenbin Wu and Keqing Wu and Xiaotong Guo and Jie Jiang and Xin Gao", "abstract": "  Accurate multi-modal medical image translation requires ha-rmonizing global\nanatomical semantics and local structural fidelity, a challenge complicated by\nintermodality information loss and structural distortion. We propose ABS-Mamba,\na novel architecture integrating the Segment Anything Model 2 (SAM2) for\norgan-aware semantic representation, specialized convolutional neural networks\n(CNNs) for preserving modality-specific edge and texture details, and Mamba's\nselective state-space modeling for efficient long- and short-range feature\ndependencies. Structurally, our dual-resolution framework leverages SAM2's\nimage encoder to capture organ-scale semantics from high-resolution inputs,\nwhile a parallel CNNs branch extracts fine-grained local features. The Robust\nFeature Fusion Network (RFFN) integrates these epresentations, and the\nBidirectional Mamba Residual Network (BMRN) models spatial dependencies using\nspiral scanning and bidirectional state-space dynamics. A three-stage skip\nfusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank\nAdaptation (LoRA+) fine-tuning to enable precise domain specialization while\nmaintaining the foundational capabilities of the pre-trained components.\nExtensive experimental validation on the SynthRAD2023 and BraTS2019 datasets\ndemonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering\nhigh-fidelity cross-modal synthesis that preserves anatomical semantics and\nstructural details to enhance diagnostic accuracy in clinical applications. The\ncode is available at https://github.com/gatina-yone/ABS-Mamba\n", "link": "http://arxiv.org/abs/2505.07687v1", "date": "2025-05-12", "relevancy": 2.831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5691}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABS-Mamba%3A%20SAM2-Driven%20Bidirectional%20Spiral%20Mamba%20Network%20for%20Medical%0A%20%20Image%20Translation&body=Title%3A%20ABS-Mamba%3A%20SAM2-Driven%20Bidirectional%20Spiral%20Mamba%20Network%20for%20Medical%0A%20%20Image%20Translation%0AAuthor%3A%20Feng%20Yuan%20and%20Yifan%20Gao%20and%20Wenbin%20Wu%20and%20Keqing%20Wu%20and%20Xiaotong%20Guo%20and%20Jie%20Jiang%20and%20Xin%20Gao%0AAbstract%3A%20%20%20Accurate%20multi-modal%20medical%20image%20translation%20requires%20ha-rmonizing%20global%0Aanatomical%20semantics%20and%20local%20structural%20fidelity%2C%20a%20challenge%20complicated%20by%0Aintermodality%20information%20loss%20and%20structural%20distortion.%20We%20propose%20ABS-Mamba%2C%0Aa%20novel%20architecture%20integrating%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20for%0Aorgan-aware%20semantic%20representation%2C%20specialized%20convolutional%20neural%20networks%0A%28CNNs%29%20for%20preserving%20modality-specific%20edge%20and%20texture%20details%2C%20and%20Mamba%27s%0Aselective%20state-space%20modeling%20for%20efficient%20long-%20and%20short-range%20feature%0Adependencies.%20Structurally%2C%20our%20dual-resolution%20framework%20leverages%20SAM2%27s%0Aimage%20encoder%20to%20capture%20organ-scale%20semantics%20from%20high-resolution%20inputs%2C%0Awhile%20a%20parallel%20CNNs%20branch%20extracts%20fine-grained%20local%20features.%20The%20Robust%0AFeature%20Fusion%20Network%20%28RFFN%29%20integrates%20these%20epresentations%2C%20and%20the%0ABidirectional%20Mamba%20Residual%20Network%20%28BMRN%29%20models%20spatial%20dependencies%20using%0Aspiral%20scanning%20and%20bidirectional%20state-space%20dynamics.%20A%20three-stage%20skip%0Afusion%20decoder%20enhances%20edge%20and%20texture%20fidelity.%20We%20employ%20Efficient%20Low-Rank%0AAdaptation%20%28LoRA%2B%29%20fine-tuning%20to%20enable%20precise%20domain%20specialization%20while%0Amaintaining%20the%20foundational%20capabilities%20of%20the%20pre-trained%20components.%0AExtensive%20experimental%20validation%20on%20the%20SynthRAD2023%20and%20BraTS2019%20datasets%0Ademonstrates%20that%20ABS-Mamba%20outperforms%20state-of-the-art%20methods%2C%20delivering%0Ahigh-fidelity%20cross-modal%20synthesis%20that%20preserves%20anatomical%20semantics%20and%0Astructural%20details%20to%20enhance%20diagnostic%20accuracy%20in%20clinical%20applications.%20The%0Acode%20is%20available%20at%20https%3A//github.com/gatina-yone/ABS-Mamba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABS-Mamba%253A%2520SAM2-Driven%2520Bidirectional%2520Spiral%2520Mamba%2520Network%2520for%2520Medical%250A%2520%2520Image%2520Translation%26entry.906535625%3DFeng%2520Yuan%2520and%2520Yifan%2520Gao%2520and%2520Wenbin%2520Wu%2520and%2520Keqing%2520Wu%2520and%2520Xiaotong%2520Guo%2520and%2520Jie%2520Jiang%2520and%2520Xin%2520Gao%26entry.1292438233%3D%2520%2520Accurate%2520multi-modal%2520medical%2520image%2520translation%2520requires%2520ha-rmonizing%2520global%250Aanatomical%2520semantics%2520and%2520local%2520structural%2520fidelity%252C%2520a%2520challenge%2520complicated%2520by%250Aintermodality%2520information%2520loss%2520and%2520structural%2520distortion.%2520We%2520propose%2520ABS-Mamba%252C%250Aa%2520novel%2520architecture%2520integrating%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520for%250Aorgan-aware%2520semantic%2520representation%252C%2520specialized%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520for%2520preserving%2520modality-specific%2520edge%2520and%2520texture%2520details%252C%2520and%2520Mamba%2527s%250Aselective%2520state-space%2520modeling%2520for%2520efficient%2520long-%2520and%2520short-range%2520feature%250Adependencies.%2520Structurally%252C%2520our%2520dual-resolution%2520framework%2520leverages%2520SAM2%2527s%250Aimage%2520encoder%2520to%2520capture%2520organ-scale%2520semantics%2520from%2520high-resolution%2520inputs%252C%250Awhile%2520a%2520parallel%2520CNNs%2520branch%2520extracts%2520fine-grained%2520local%2520features.%2520The%2520Robust%250AFeature%2520Fusion%2520Network%2520%2528RFFN%2529%2520integrates%2520these%2520epresentations%252C%2520and%2520the%250ABidirectional%2520Mamba%2520Residual%2520Network%2520%2528BMRN%2529%2520models%2520spatial%2520dependencies%2520using%250Aspiral%2520scanning%2520and%2520bidirectional%2520state-space%2520dynamics.%2520A%2520three-stage%2520skip%250Afusion%2520decoder%2520enhances%2520edge%2520and%2520texture%2520fidelity.%2520We%2520employ%2520Efficient%2520Low-Rank%250AAdaptation%2520%2528LoRA%252B%2529%2520fine-tuning%2520to%2520enable%2520precise%2520domain%2520specialization%2520while%250Amaintaining%2520the%2520foundational%2520capabilities%2520of%2520the%2520pre-trained%2520components.%250AExtensive%2520experimental%2520validation%2520on%2520the%2520SynthRAD2023%2520and%2520BraTS2019%2520datasets%250Ademonstrates%2520that%2520ABS-Mamba%2520outperforms%2520state-of-the-art%2520methods%252C%2520delivering%250Ahigh-fidelity%2520cross-modal%2520synthesis%2520that%2520preserves%2520anatomical%2520semantics%2520and%250Astructural%2520details%2520to%2520enhance%2520diagnostic%2520accuracy%2520in%2520clinical%2520applications.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/gatina-yone/ABS-Mamba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABS-Mamba%3A%20SAM2-Driven%20Bidirectional%20Spiral%20Mamba%20Network%20for%20Medical%0A%20%20Image%20Translation&entry.906535625=Feng%20Yuan%20and%20Yifan%20Gao%20and%20Wenbin%20Wu%20and%20Keqing%20Wu%20and%20Xiaotong%20Guo%20and%20Jie%20Jiang%20and%20Xin%20Gao&entry.1292438233=%20%20Accurate%20multi-modal%20medical%20image%20translation%20requires%20ha-rmonizing%20global%0Aanatomical%20semantics%20and%20local%20structural%20fidelity%2C%20a%20challenge%20complicated%20by%0Aintermodality%20information%20loss%20and%20structural%20distortion.%20We%20propose%20ABS-Mamba%2C%0Aa%20novel%20architecture%20integrating%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20for%0Aorgan-aware%20semantic%20representation%2C%20specialized%20convolutional%20neural%20networks%0A%28CNNs%29%20for%20preserving%20modality-specific%20edge%20and%20texture%20details%2C%20and%20Mamba%27s%0Aselective%20state-space%20modeling%20for%20efficient%20long-%20and%20short-range%20feature%0Adependencies.%20Structurally%2C%20our%20dual-resolution%20framework%20leverages%20SAM2%27s%0Aimage%20encoder%20to%20capture%20organ-scale%20semantics%20from%20high-resolution%20inputs%2C%0Awhile%20a%20parallel%20CNNs%20branch%20extracts%20fine-grained%20local%20features.%20The%20Robust%0AFeature%20Fusion%20Network%20%28RFFN%29%20integrates%20these%20epresentations%2C%20and%20the%0ABidirectional%20Mamba%20Residual%20Network%20%28BMRN%29%20models%20spatial%20dependencies%20using%0Aspiral%20scanning%20and%20bidirectional%20state-space%20dynamics.%20A%20three-stage%20skip%0Afusion%20decoder%20enhances%20edge%20and%20texture%20fidelity.%20We%20employ%20Efficient%20Low-Rank%0AAdaptation%20%28LoRA%2B%29%20fine-tuning%20to%20enable%20precise%20domain%20specialization%20while%0Amaintaining%20the%20foundational%20capabilities%20of%20the%20pre-trained%20components.%0AExtensive%20experimental%20validation%20on%20the%20SynthRAD2023%20and%20BraTS2019%20datasets%0Ademonstrates%20that%20ABS-Mamba%20outperforms%20state-of-the-art%20methods%2C%20delivering%0Ahigh-fidelity%20cross-modal%20synthesis%20that%20preserves%20anatomical%20semantics%20and%0Astructural%20details%20to%20enhance%20diagnostic%20accuracy%20in%20clinical%20applications.%20The%0Acode%20is%20available%20at%20https%3A//github.com/gatina-yone/ABS-Mamba%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07687v1&entry.124074799=Read"},
{"title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird\n  Images", "author": "Elisei Rykov and Kseniia Petrushina and Kseniia Titova and Anton Razzhigaev and Alexander Panchenko and Vasily Konovalov", "abstract": "  Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent.\n", "link": "http://arxiv.org/abs/2505.07704v1", "date": "2025-05-12", "relevancy": 2.7961, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Through%20the%20Looking%20Glass%3A%20Common%20Sense%20Consistency%20Evaluation%20of%20Weird%0A%20%20Images&body=Title%3A%20Through%20the%20Looking%20Glass%3A%20Common%20Sense%20Consistency%20Evaluation%20of%20Weird%0A%20%20Images%0AAuthor%3A%20Elisei%20Rykov%20and%20Kseniia%20Petrushina%20and%20Kseniia%20Titova%20and%20Anton%20Razzhigaev%20and%20Alexander%20Panchenko%20and%20Vasily%20Konovalov%0AAbstract%3A%20%20%20Measuring%20how%20real%20images%20look%20is%20a%20complex%20task%20in%20artificial%20intelligence%0Aresearch.%20For%20example%2C%20an%20image%20of%20a%20boy%20with%20a%20vacuum%20cleaner%20in%20a%20desert%0Aviolates%20common%20sense.%20We%20introduce%20a%20novel%20method%2C%20which%20we%20call%20Through%20the%0ALooking%20Glass%20%28TLG%29%2C%20to%20assess%20image%20common%20sense%20consistency%20using%20Large%0AVision-Language%20Models%20%28LVLMs%29%20and%20Transformer-based%20encoder.%20By%20leveraging%0ALVLMs%20to%20extract%20atomic%20facts%20from%20these%20images%2C%20we%20obtain%20a%20mix%20of%20accurate%0Afacts.%20We%20proceed%20by%20fine-tuning%20a%20compact%20attention-pooling%20classifier%20over%0Aencoded%20atomic%20facts.%20Our%20TLG%20has%20achieved%20a%20new%20state-of-the-art%20performance%0Aon%20the%20WHOOPS%21%20and%20WEIRD%20datasets%20while%20leveraging%20a%20compact%20fine-tuning%0Acomponent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThrough%2520the%2520Looking%2520Glass%253A%2520Common%2520Sense%2520Consistency%2520Evaluation%2520of%2520Weird%250A%2520%2520Images%26entry.906535625%3DElisei%2520Rykov%2520and%2520Kseniia%2520Petrushina%2520and%2520Kseniia%2520Titova%2520and%2520Anton%2520Razzhigaev%2520and%2520Alexander%2520Panchenko%2520and%2520Vasily%2520Konovalov%26entry.1292438233%3D%2520%2520Measuring%2520how%2520real%2520images%2520look%2520is%2520a%2520complex%2520task%2520in%2520artificial%2520intelligence%250Aresearch.%2520For%2520example%252C%2520an%2520image%2520of%2520a%2520boy%2520with%2520a%2520vacuum%2520cleaner%2520in%2520a%2520desert%250Aviolates%2520common%2520sense.%2520We%2520introduce%2520a%2520novel%2520method%252C%2520which%2520we%2520call%2520Through%2520the%250ALooking%2520Glass%2520%2528TLG%2529%252C%2520to%2520assess%2520image%2520common%2520sense%2520consistency%2520using%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520and%2520Transformer-based%2520encoder.%2520By%2520leveraging%250ALVLMs%2520to%2520extract%2520atomic%2520facts%2520from%2520these%2520images%252C%2520we%2520obtain%2520a%2520mix%2520of%2520accurate%250Afacts.%2520We%2520proceed%2520by%2520fine-tuning%2520a%2520compact%2520attention-pooling%2520classifier%2520over%250Aencoded%2520atomic%2520facts.%2520Our%2520TLG%2520has%2520achieved%2520a%2520new%2520state-of-the-art%2520performance%250Aon%2520the%2520WHOOPS%2521%2520and%2520WEIRD%2520datasets%2520while%2520leveraging%2520a%2520compact%2520fine-tuning%250Acomponent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Through%20the%20Looking%20Glass%3A%20Common%20Sense%20Consistency%20Evaluation%20of%20Weird%0A%20%20Images&entry.906535625=Elisei%20Rykov%20and%20Kseniia%20Petrushina%20and%20Kseniia%20Titova%20and%20Anton%20Razzhigaev%20and%20Alexander%20Panchenko%20and%20Vasily%20Konovalov&entry.1292438233=%20%20Measuring%20how%20real%20images%20look%20is%20a%20complex%20task%20in%20artificial%20intelligence%0Aresearch.%20For%20example%2C%20an%20image%20of%20a%20boy%20with%20a%20vacuum%20cleaner%20in%20a%20desert%0Aviolates%20common%20sense.%20We%20introduce%20a%20novel%20method%2C%20which%20we%20call%20Through%20the%0ALooking%20Glass%20%28TLG%29%2C%20to%20assess%20image%20common%20sense%20consistency%20using%20Large%0AVision-Language%20Models%20%28LVLMs%29%20and%20Transformer-based%20encoder.%20By%20leveraging%0ALVLMs%20to%20extract%20atomic%20facts%20from%20these%20images%2C%20we%20obtain%20a%20mix%20of%20accurate%0Afacts.%20We%20proceed%20by%20fine-tuning%20a%20compact%20attention-pooling%20classifier%20over%0Aencoded%20atomic%20facts.%20Our%20TLG%20has%20achieved%20a%20new%20state-of-the-art%20performance%0Aon%20the%20WHOOPS%21%20and%20WEIRD%20datasets%20while%20leveraging%20a%20compact%20fine-tuning%0Acomponent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07704v1&entry.124074799=Read"},
{"title": "BodyGPS: Anatomical Positioning System", "author": "Halid Ziya Yerebakan and Kritika Iyer and Xueqi Guo and Yoshihisa Shinagawa and Gerardo Hermosillo Valadez", "abstract": "  We introduce a new type of foundational model for parsing human anatomy in\nmedical images that works for different modalities. It supports supervised or\nunsupervised training and can perform matching, registration, classification,\nor segmentation with or without user interaction. We achieve this by training a\nneural network estimator that maps query locations to atlas coordinates via\nregression. Efficiency is improved by sparsely sampling the input, enabling\nresponse times of less than 1 ms without additional accelerator hardware. We\ndemonstrate the utility of the algorithm in both CT and MRI modalities.\n", "link": "http://arxiv.org/abs/2505.07744v1", "date": "2025-05-12", "relevancy": 2.7739, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6184}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5232}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BodyGPS%3A%20Anatomical%20Positioning%20System&body=Title%3A%20BodyGPS%3A%20Anatomical%20Positioning%20System%0AAuthor%3A%20Halid%20Ziya%20Yerebakan%20and%20Kritika%20Iyer%20and%20Xueqi%20Guo%20and%20Yoshihisa%20Shinagawa%20and%20Gerardo%20Hermosillo%20Valadez%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20type%20of%20foundational%20model%20for%20parsing%20human%20anatomy%20in%0Amedical%20images%20that%20works%20for%20different%20modalities.%20It%20supports%20supervised%20or%0Aunsupervised%20training%20and%20can%20perform%20matching%2C%20registration%2C%20classification%2C%0Aor%20segmentation%20with%20or%20without%20user%20interaction.%20We%20achieve%20this%20by%20training%20a%0Aneural%20network%20estimator%20that%20maps%20query%20locations%20to%20atlas%20coordinates%20via%0Aregression.%20Efficiency%20is%20improved%20by%20sparsely%20sampling%20the%20input%2C%20enabling%0Aresponse%20times%20of%20less%20than%201%20ms%20without%20additional%20accelerator%20hardware.%20We%0Ademonstrate%20the%20utility%20of%20the%20algorithm%20in%20both%20CT%20and%20MRI%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBodyGPS%253A%2520Anatomical%2520Positioning%2520System%26entry.906535625%3DHalid%2520Ziya%2520Yerebakan%2520and%2520Kritika%2520Iyer%2520and%2520Xueqi%2520Guo%2520and%2520Yoshihisa%2520Shinagawa%2520and%2520Gerardo%2520Hermosillo%2520Valadez%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520type%2520of%2520foundational%2520model%2520for%2520parsing%2520human%2520anatomy%2520in%250Amedical%2520images%2520that%2520works%2520for%2520different%2520modalities.%2520It%2520supports%2520supervised%2520or%250Aunsupervised%2520training%2520and%2520can%2520perform%2520matching%252C%2520registration%252C%2520classification%252C%250Aor%2520segmentation%2520with%2520or%2520without%2520user%2520interaction.%2520We%2520achieve%2520this%2520by%2520training%2520a%250Aneural%2520network%2520estimator%2520that%2520maps%2520query%2520locations%2520to%2520atlas%2520coordinates%2520via%250Aregression.%2520Efficiency%2520is%2520improved%2520by%2520sparsely%2520sampling%2520the%2520input%252C%2520enabling%250Aresponse%2520times%2520of%2520less%2520than%25201%2520ms%2520without%2520additional%2520accelerator%2520hardware.%2520We%250Ademonstrate%2520the%2520utility%2520of%2520the%2520algorithm%2520in%2520both%2520CT%2520and%2520MRI%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BodyGPS%3A%20Anatomical%20Positioning%20System&entry.906535625=Halid%20Ziya%20Yerebakan%20and%20Kritika%20Iyer%20and%20Xueqi%20Guo%20and%20Yoshihisa%20Shinagawa%20and%20Gerardo%20Hermosillo%20Valadez&entry.1292438233=%20%20We%20introduce%20a%20new%20type%20of%20foundational%20model%20for%20parsing%20human%20anatomy%20in%0Amedical%20images%20that%20works%20for%20different%20modalities.%20It%20supports%20supervised%20or%0Aunsupervised%20training%20and%20can%20perform%20matching%2C%20registration%2C%20classification%2C%0Aor%20segmentation%20with%20or%20without%20user%20interaction.%20We%20achieve%20this%20by%20training%20a%0Aneural%20network%20estimator%20that%20maps%20query%20locations%20to%20atlas%20coordinates%20via%0Aregression.%20Efficiency%20is%20improved%20by%20sparsely%20sampling%20the%20input%2C%20enabling%0Aresponse%20times%20of%20less%20than%201%20ms%20without%20additional%20accelerator%20hardware.%20We%0Ademonstrate%20the%20utility%20of%20the%20algorithm%20in%20both%20CT%20and%20MRI%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07744v1&entry.124074799=Read"},
{"title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models", "author": "Justus Westerhoff and Erblina Purelku and Jakob Hackstein and Jonas Loos and Leo Pinetzki and Lorenz Hufe", "abstract": "  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper along with the code for evaluations at\nwww.bliss.berlin/research/scam.\n", "link": "http://arxiv.org/abs/2504.04893v3", "date": "2025-05-12", "relevancy": 2.7253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&body=Title%3A%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models%0AAuthor%3A%20Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Lorenz%20Hufe%0AAbstract%3A%20%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20However%2C%20existing%20datasets%20are%20limited%20in%20size%20and%0Adiversity%2C%20making%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%0Aintroduce%20SCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%0Aattack%20images%20to%20date%2C%20containing%201%2C162%20images%20across%20hundreds%20of%20object%0Acategories%20and%20attack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%0AModels%20%28VLMs%29%20on%20SCAM%2C%20we%20demonstrate%20that%20typographic%20attacks%20significantly%0Adegrade%20performance%2C%20and%20identify%20that%20training%20data%20and%20model%20architecture%0Ainfluence%20the%20susceptibility%20to%20these%20attacks.%20Our%20findings%20reveal%20that%0Atypographic%20attacks%20persist%20in%20state-of-the-art%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20due%20to%20the%20choice%20of%20their%20vision%20encoder%2C%20though%20larger%20Large%20Language%0AModels%20%28LLMs%29%20backbones%20help%20mitigate%20their%20vulnerability.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20We%20publicly%20release%20the%20datasets%20introduced%0Ain%20this%20paper%20along%20with%20the%20code%20for%20evaluations%20at%0Awww.bliss.berlin/research/scam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04893v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAM%253A%2520A%2520Real-World%2520Typographic%2520Robustness%2520Evaluation%2520for%2520Multimodal%250A%2520%2520Foundation%2520Models%26entry.906535625%3DJustus%2520Westerhoff%2520and%2520Erblina%2520Purelku%2520and%2520Jakob%2520Hackstein%2520and%2520Jonas%2520Loos%2520and%2520Leo%2520Pinetzki%2520and%2520Lorenz%2520Hufe%26entry.1292438233%3D%2520%2520Typographic%2520attacks%2520exploit%2520the%2520interplay%2520between%2520text%2520and%2520visual%2520content%2520in%250Amultimodal%2520foundation%2520models%252C%2520causing%2520misclassifications%2520when%2520misleading%2520text%250Ais%2520embedded%2520within%2520images.%2520However%252C%2520existing%2520datasets%2520are%2520limited%2520in%2520size%2520and%250Adiversity%252C%2520making%2520it%2520difficult%2520to%2520study%2520such%2520vulnerabilities.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520SCAM%252C%2520the%2520largest%2520and%2520most%2520diverse%2520dataset%2520of%2520real-world%2520typographic%250Aattack%2520images%2520to%2520date%252C%2520containing%25201%252C162%2520images%2520across%2520hundreds%2520of%2520object%250Acategories%2520and%2520attack%2520words.%2520Through%2520extensive%2520benchmarking%2520of%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520on%2520SCAM%252C%2520we%2520demonstrate%2520that%2520typographic%2520attacks%2520significantly%250Adegrade%2520performance%252C%2520and%2520identify%2520that%2520training%2520data%2520and%2520model%2520architecture%250Ainfluence%2520the%2520susceptibility%2520to%2520these%2520attacks.%2520Our%2520findings%2520reveal%2520that%250Atypographic%2520attacks%2520persist%2520in%2520state-of-the-art%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520due%2520to%2520the%2520choice%2520of%2520their%2520vision%2520encoder%252C%2520though%2520larger%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520backbones%2520help%2520mitigate%2520their%2520vulnerability.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520synthetic%2520attacks%2520closely%2520resemble%2520real-world%2520%2528handwritten%2529%250Aattacks%252C%2520validating%2520their%2520use%2520in%2520research.%2520Our%2520work%2520provides%2520a%2520comprehensive%250Aresource%2520and%2520empirical%2520insights%2520to%2520facilitate%2520future%2520research%2520toward%2520robust%2520and%250Atrustworthy%2520multimodal%2520AI%2520systems.%2520We%2520publicly%2520release%2520the%2520datasets%2520introduced%250Ain%2520this%2520paper%2520along%2520with%2520the%2520code%2520for%2520evaluations%2520at%250Awww.bliss.berlin/research/scam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04893v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&entry.906535625=Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Lorenz%20Hufe&entry.1292438233=%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20However%2C%20existing%20datasets%20are%20limited%20in%20size%20and%0Adiversity%2C%20making%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%0Aintroduce%20SCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%0Aattack%20images%20to%20date%2C%20containing%201%2C162%20images%20across%20hundreds%20of%20object%0Acategories%20and%20attack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%0AModels%20%28VLMs%29%20on%20SCAM%2C%20we%20demonstrate%20that%20typographic%20attacks%20significantly%0Adegrade%20performance%2C%20and%20identify%20that%20training%20data%20and%20model%20architecture%0Ainfluence%20the%20susceptibility%20to%20these%20attacks.%20Our%20findings%20reveal%20that%0Atypographic%20attacks%20persist%20in%20state-of-the-art%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20due%20to%20the%20choice%20of%20their%20vision%20encoder%2C%20though%20larger%20Large%20Language%0AModels%20%28LLMs%29%20backbones%20help%20mitigate%20their%20vulnerability.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20We%20publicly%20release%20the%20datasets%20introduced%0Ain%20this%20paper%20along%20with%20the%20code%20for%20evaluations%20at%0Awww.bliss.berlin/research/scam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04893v3&entry.124074799=Read"},
{"title": "Veri-Car: Towards Open-world Vehicle Information Retrieval", "author": "Andr\u00e9s Mu\u00f1oz and Nancy Thomas and Annita Vapsi and Daniel Borrajo", "abstract": "  Many industrial and service sectors require tools to extract vehicle\ncharacteristics from images. This is a complex task not only by the variety of\nnoise, and large number of classes, but also by the constant introduction of\nnew vehicle models to the market. In this paper, we present Veri-Car, an\ninformation retrieval integrated approach designed to help on this task. It\nleverages supervised learning techniques to accurately identify the make, type,\nmodel, year, color, and license plate of cars. The approach also addresses the\nchallenge of handling open-world problems, where new car models and variations\nfrequently emerge, by employing a sophisticated combination of pre-trained\nmodels, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust\nperformance, achieving high precision and accuracy in classifying both seen and\nunseen data. Additionally, it integrates an ensemble license plate detection,\nand an OCR model to extract license plate numbers with impressive accuracy.\n", "link": "http://arxiv.org/abs/2411.06864v4", "date": "2025-05-12", "relevancy": 2.7119, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Veri-Car%3A%20Towards%20Open-world%20Vehicle%20Information%20Retrieval&body=Title%3A%20Veri-Car%3A%20Towards%20Open-world%20Vehicle%20Information%20Retrieval%0AAuthor%3A%20Andr%C3%A9s%20Mu%C3%B1oz%20and%20Nancy%20Thomas%20and%20Annita%20Vapsi%20and%20Daniel%20Borrajo%0AAbstract%3A%20%20%20Many%20industrial%20and%20service%20sectors%20require%20tools%20to%20extract%20vehicle%0Acharacteristics%20from%20images.%20This%20is%20a%20complex%20task%20not%20only%20by%20the%20variety%20of%0Anoise%2C%20and%20large%20number%20of%20classes%2C%20but%20also%20by%20the%20constant%20introduction%20of%0Anew%20vehicle%20models%20to%20the%20market.%20In%20this%20paper%2C%20we%20present%20Veri-Car%2C%20an%0Ainformation%20retrieval%20integrated%20approach%20designed%20to%20help%20on%20this%20task.%20It%0Aleverages%20supervised%20learning%20techniques%20to%20accurately%20identify%20the%20make%2C%20type%2C%0Amodel%2C%20year%2C%20color%2C%20and%20license%20plate%20of%20cars.%20The%20approach%20also%20addresses%20the%0Achallenge%20of%20handling%20open-world%20problems%2C%20where%20new%20car%20models%20and%20variations%0Afrequently%20emerge%2C%20by%20employing%20a%20sophisticated%20combination%20of%20pre-trained%0Amodels%2C%20and%20a%20hierarchical%20multi-similarity%20loss.%20Veri-Car%20demonstrates%20robust%0Aperformance%2C%20achieving%20high%20precision%20and%20accuracy%20in%20classifying%20both%20seen%20and%0Aunseen%20data.%20Additionally%2C%20it%20integrates%20an%20ensemble%20license%20plate%20detection%2C%0Aand%20an%20OCR%20model%20to%20extract%20license%20plate%20numbers%20with%20impressive%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06864v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeri-Car%253A%2520Towards%2520Open-world%2520Vehicle%2520Information%2520Retrieval%26entry.906535625%3DAndr%25C3%25A9s%2520Mu%25C3%25B1oz%2520and%2520Nancy%2520Thomas%2520and%2520Annita%2520Vapsi%2520and%2520Daniel%2520Borrajo%26entry.1292438233%3D%2520%2520Many%2520industrial%2520and%2520service%2520sectors%2520require%2520tools%2520to%2520extract%2520vehicle%250Acharacteristics%2520from%2520images.%2520This%2520is%2520a%2520complex%2520task%2520not%2520only%2520by%2520the%2520variety%2520of%250Anoise%252C%2520and%2520large%2520number%2520of%2520classes%252C%2520but%2520also%2520by%2520the%2520constant%2520introduction%2520of%250Anew%2520vehicle%2520models%2520to%2520the%2520market.%2520In%2520this%2520paper%252C%2520we%2520present%2520Veri-Car%252C%2520an%250Ainformation%2520retrieval%2520integrated%2520approach%2520designed%2520to%2520help%2520on%2520this%2520task.%2520It%250Aleverages%2520supervised%2520learning%2520techniques%2520to%2520accurately%2520identify%2520the%2520make%252C%2520type%252C%250Amodel%252C%2520year%252C%2520color%252C%2520and%2520license%2520plate%2520of%2520cars.%2520The%2520approach%2520also%2520addresses%2520the%250Achallenge%2520of%2520handling%2520open-world%2520problems%252C%2520where%2520new%2520car%2520models%2520and%2520variations%250Afrequently%2520emerge%252C%2520by%2520employing%2520a%2520sophisticated%2520combination%2520of%2520pre-trained%250Amodels%252C%2520and%2520a%2520hierarchical%2520multi-similarity%2520loss.%2520Veri-Car%2520demonstrates%2520robust%250Aperformance%252C%2520achieving%2520high%2520precision%2520and%2520accuracy%2520in%2520classifying%2520both%2520seen%2520and%250Aunseen%2520data.%2520Additionally%252C%2520it%2520integrates%2520an%2520ensemble%2520license%2520plate%2520detection%252C%250Aand%2520an%2520OCR%2520model%2520to%2520extract%2520license%2520plate%2520numbers%2520with%2520impressive%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06864v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Veri-Car%3A%20Towards%20Open-world%20Vehicle%20Information%20Retrieval&entry.906535625=Andr%C3%A9s%20Mu%C3%B1oz%20and%20Nancy%20Thomas%20and%20Annita%20Vapsi%20and%20Daniel%20Borrajo&entry.1292438233=%20%20Many%20industrial%20and%20service%20sectors%20require%20tools%20to%20extract%20vehicle%0Acharacteristics%20from%20images.%20This%20is%20a%20complex%20task%20not%20only%20by%20the%20variety%20of%0Anoise%2C%20and%20large%20number%20of%20classes%2C%20but%20also%20by%20the%20constant%20introduction%20of%0Anew%20vehicle%20models%20to%20the%20market.%20In%20this%20paper%2C%20we%20present%20Veri-Car%2C%20an%0Ainformation%20retrieval%20integrated%20approach%20designed%20to%20help%20on%20this%20task.%20It%0Aleverages%20supervised%20learning%20techniques%20to%20accurately%20identify%20the%20make%2C%20type%2C%0Amodel%2C%20year%2C%20color%2C%20and%20license%20plate%20of%20cars.%20The%20approach%20also%20addresses%20the%0Achallenge%20of%20handling%20open-world%20problems%2C%20where%20new%20car%20models%20and%20variations%0Afrequently%20emerge%2C%20by%20employing%20a%20sophisticated%20combination%20of%20pre-trained%0Amodels%2C%20and%20a%20hierarchical%20multi-similarity%20loss.%20Veri-Car%20demonstrates%20robust%0Aperformance%2C%20achieving%20high%20precision%20and%20accuracy%20in%20classifying%20both%20seen%20and%0Aunseen%20data.%20Additionally%2C%20it%20integrates%20an%20ensemble%20license%20plate%20detection%2C%0Aand%20an%20OCR%20model%20to%20extract%20license%20plate%20numbers%20with%20impressive%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06864v4&entry.124074799=Read"},
{"title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?", "author": "Yuhang Liu and Dong Gong and Yichao Cai and Erdun Gao and Zhen Zhang and Biwei Huang and Mingming Gong and Anton van den Hengel and Javen Qinfeng Shi", "abstract": "  The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human-interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result, i.e., the representations learned by LLMs through\nnext-token prediction can be approximately modeled as the logarithm of the\nposterior probabilities of these latent discrete concepts given input context,\nup to an invertible linear transformation. This theoretical finding not only\nprovides evidence that LLMs capture underlying generative factors, but also\nprovide a unified prospective for understanding of the linear representation\nhypothesis. Taking this a step further, our finding motivates a reliable\nevaluation of sparse autoencoders by treating the performance of supervised\nconcept extractors as an upper bound. Pushing this idea even further, it\ninspires a structural variant that enforces dependence among latent concepts in\naddition to promoting sparsity. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families, and demonstrate the effectiveness of our structured\nsparse autoencoder.\n", "link": "http://arxiv.org/abs/2503.08980v3", "date": "2025-05-12", "relevancy": 2.7107, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Predict%20Therefore%20I%20Am%3A%20Is%20Next%20Token%20Prediction%20Enough%20to%20Learn%0A%20%20Human-Interpretable%20Concepts%20from%20Data%3F&body=Title%3A%20I%20Predict%20Therefore%20I%20Am%3A%20Is%20Next%20Token%20Prediction%20Enough%20to%20Learn%0A%20%20Human-Interpretable%20Concepts%20from%20Data%3F%0AAuthor%3A%20Yuhang%20Liu%20and%20Dong%20Gong%20and%20Yichao%20Cai%20and%20Erdun%20Gao%20and%20Zhen%20Zhang%20and%20Biwei%20Huang%20and%20Mingming%20Gong%20and%20Anton%20van%20den%20Hengel%20and%20Javen%20Qinfeng%20Shi%0AAbstract%3A%20%20%20The%20remarkable%20achievements%20of%20large%20language%20models%20%28LLMs%29%20have%20led%20many%20to%0Aconclude%20that%20they%20exhibit%20a%20form%20of%20intelligence.%20This%20is%20as%20opposed%20to%0Aexplanations%20of%20their%20capabilities%20based%20on%20their%20ability%20to%20perform%20relatively%0Asimple%20manipulations%20of%20vast%20volumes%20of%20data.%20To%20illuminate%20the%20distinction%0Abetween%20these%20explanations%2C%20we%20introduce%20a%20novel%20generative%20model%20that%0Agenerates%20tokens%20on%20the%20basis%20of%20human-interpretable%20concepts%20represented%20as%0Alatent%20discrete%20variables.%20Under%20mild%20conditions%2C%20even%20when%20the%20mapping%20from%0Athe%20latent%20space%20to%20the%20observed%20space%20is%20non-invertible%2C%20we%20establish%20an%0Aidentifiability%20result%2C%20i.e.%2C%20the%20representations%20learned%20by%20LLMs%20through%0Anext-token%20prediction%20can%20be%20approximately%20modeled%20as%20the%20logarithm%20of%20the%0Aposterior%20probabilities%20of%20these%20latent%20discrete%20concepts%20given%20input%20context%2C%0Aup%20to%20an%20invertible%20linear%20transformation.%20This%20theoretical%20finding%20not%20only%0Aprovides%20evidence%20that%20LLMs%20capture%20underlying%20generative%20factors%2C%20but%20also%0Aprovide%20a%20unified%20prospective%20for%20understanding%20of%20the%20linear%20representation%0Ahypothesis.%20Taking%20this%20a%20step%20further%2C%20our%20finding%20motivates%20a%20reliable%0Aevaluation%20of%20sparse%20autoencoders%20by%20treating%20the%20performance%20of%20supervised%0Aconcept%20extractors%20as%20an%20upper%20bound.%20Pushing%20this%20idea%20even%20further%2C%20it%0Ainspires%20a%20structural%20variant%20that%20enforces%20dependence%20among%20latent%20concepts%20in%0Aaddition%20to%20promoting%20sparsity.%20Empirically%2C%20we%20validate%20our%20theoretical%0Aresults%20through%20evaluations%20on%20both%20simulation%20data%20and%20the%20Pythia%2C%20Llama%2C%20and%0ADeepSeek%20model%20families%2C%20and%20demonstrate%20the%20effectiveness%20of%20our%20structured%0Asparse%20autoencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08980v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Predict%2520Therefore%2520I%2520Am%253A%2520Is%2520Next%2520Token%2520Prediction%2520Enough%2520to%2520Learn%250A%2520%2520Human-Interpretable%2520Concepts%2520from%2520Data%253F%26entry.906535625%3DYuhang%2520Liu%2520and%2520Dong%2520Gong%2520and%2520Yichao%2520Cai%2520and%2520Erdun%2520Gao%2520and%2520Zhen%2520Zhang%2520and%2520Biwei%2520Huang%2520and%2520Mingming%2520Gong%2520and%2520Anton%2520van%2520den%2520Hengel%2520and%2520Javen%2520Qinfeng%2520Shi%26entry.1292438233%3D%2520%2520The%2520remarkable%2520achievements%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520led%2520many%2520to%250Aconclude%2520that%2520they%2520exhibit%2520a%2520form%2520of%2520intelligence.%2520This%2520is%2520as%2520opposed%2520to%250Aexplanations%2520of%2520their%2520capabilities%2520based%2520on%2520their%2520ability%2520to%2520perform%2520relatively%250Asimple%2520manipulations%2520of%2520vast%2520volumes%2520of%2520data.%2520To%2520illuminate%2520the%2520distinction%250Abetween%2520these%2520explanations%252C%2520we%2520introduce%2520a%2520novel%2520generative%2520model%2520that%250Agenerates%2520tokens%2520on%2520the%2520basis%2520of%2520human-interpretable%2520concepts%2520represented%2520as%250Alatent%2520discrete%2520variables.%2520Under%2520mild%2520conditions%252C%2520even%2520when%2520the%2520mapping%2520from%250Athe%2520latent%2520space%2520to%2520the%2520observed%2520space%2520is%2520non-invertible%252C%2520we%2520establish%2520an%250Aidentifiability%2520result%252C%2520i.e.%252C%2520the%2520representations%2520learned%2520by%2520LLMs%2520through%250Anext-token%2520prediction%2520can%2520be%2520approximately%2520modeled%2520as%2520the%2520logarithm%2520of%2520the%250Aposterior%2520probabilities%2520of%2520these%2520latent%2520discrete%2520concepts%2520given%2520input%2520context%252C%250Aup%2520to%2520an%2520invertible%2520linear%2520transformation.%2520This%2520theoretical%2520finding%2520not%2520only%250Aprovides%2520evidence%2520that%2520LLMs%2520capture%2520underlying%2520generative%2520factors%252C%2520but%2520also%250Aprovide%2520a%2520unified%2520prospective%2520for%2520understanding%2520of%2520the%2520linear%2520representation%250Ahypothesis.%2520Taking%2520this%2520a%2520step%2520further%252C%2520our%2520finding%2520motivates%2520a%2520reliable%250Aevaluation%2520of%2520sparse%2520autoencoders%2520by%2520treating%2520the%2520performance%2520of%2520supervised%250Aconcept%2520extractors%2520as%2520an%2520upper%2520bound.%2520Pushing%2520this%2520idea%2520even%2520further%252C%2520it%250Ainspires%2520a%2520structural%2520variant%2520that%2520enforces%2520dependence%2520among%2520latent%2520concepts%2520in%250Aaddition%2520to%2520promoting%2520sparsity.%2520Empirically%252C%2520we%2520validate%2520our%2520theoretical%250Aresults%2520through%2520evaluations%2520on%2520both%2520simulation%2520data%2520and%2520the%2520Pythia%252C%2520Llama%252C%2520and%250ADeepSeek%2520model%2520families%252C%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520structured%250Asparse%2520autoencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08980v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Predict%20Therefore%20I%20Am%3A%20Is%20Next%20Token%20Prediction%20Enough%20to%20Learn%0A%20%20Human-Interpretable%20Concepts%20from%20Data%3F&entry.906535625=Yuhang%20Liu%20and%20Dong%20Gong%20and%20Yichao%20Cai%20and%20Erdun%20Gao%20and%20Zhen%20Zhang%20and%20Biwei%20Huang%20and%20Mingming%20Gong%20and%20Anton%20van%20den%20Hengel%20and%20Javen%20Qinfeng%20Shi&entry.1292438233=%20%20The%20remarkable%20achievements%20of%20large%20language%20models%20%28LLMs%29%20have%20led%20many%20to%0Aconclude%20that%20they%20exhibit%20a%20form%20of%20intelligence.%20This%20is%20as%20opposed%20to%0Aexplanations%20of%20their%20capabilities%20based%20on%20their%20ability%20to%20perform%20relatively%0Asimple%20manipulations%20of%20vast%20volumes%20of%20data.%20To%20illuminate%20the%20distinction%0Abetween%20these%20explanations%2C%20we%20introduce%20a%20novel%20generative%20model%20that%0Agenerates%20tokens%20on%20the%20basis%20of%20human-interpretable%20concepts%20represented%20as%0Alatent%20discrete%20variables.%20Under%20mild%20conditions%2C%20even%20when%20the%20mapping%20from%0Athe%20latent%20space%20to%20the%20observed%20space%20is%20non-invertible%2C%20we%20establish%20an%0Aidentifiability%20result%2C%20i.e.%2C%20the%20representations%20learned%20by%20LLMs%20through%0Anext-token%20prediction%20can%20be%20approximately%20modeled%20as%20the%20logarithm%20of%20the%0Aposterior%20probabilities%20of%20these%20latent%20discrete%20concepts%20given%20input%20context%2C%0Aup%20to%20an%20invertible%20linear%20transformation.%20This%20theoretical%20finding%20not%20only%0Aprovides%20evidence%20that%20LLMs%20capture%20underlying%20generative%20factors%2C%20but%20also%0Aprovide%20a%20unified%20prospective%20for%20understanding%20of%20the%20linear%20representation%0Ahypothesis.%20Taking%20this%20a%20step%20further%2C%20our%20finding%20motivates%20a%20reliable%0Aevaluation%20of%20sparse%20autoencoders%20by%20treating%20the%20performance%20of%20supervised%0Aconcept%20extractors%20as%20an%20upper%20bound.%20Pushing%20this%20idea%20even%20further%2C%20it%0Ainspires%20a%20structural%20variant%20that%20enforces%20dependence%20among%20latent%20concepts%20in%0Aaddition%20to%20promoting%20sparsity.%20Empirically%2C%20we%20validate%20our%20theoretical%0Aresults%20through%20evaluations%20on%20both%20simulation%20data%20and%20the%20Pythia%2C%20Llama%2C%20and%0ADeepSeek%20model%20families%2C%20and%20demonstrate%20the%20effectiveness%20of%20our%20structured%0Asparse%20autoencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08980v3&entry.124074799=Read"},
{"title": "Hybrid Spiking Vision Transformer for Object Detection with Event\n  Cameras", "author": "Qi Xu and Jie Deng and Jiangrong Shen and Biwu Chen and Huajin Tang and Gang Pan", "abstract": "  Event-based object detection has gained increasing attention due to its\nadvantages such as high temporal resolution, wide dynamic range, and\nasynchronous address-event representation. Leveraging these advantages, Spiking\nNeural Networks (SNNs) have emerged as a promising approach, offering low\nenergy consumption and rich spatiotemporal dynamics. To further enhance the\nperformance of event-based object detection, this study proposes a novel hybrid\nspike vision Transformer (HsVT) model. The HsVT model integrates a spatial\nfeature extraction module to capture local and global features, and a temporal\nfeature extraction module to model time dependencies and long-term patterns in\nevent sequences. This combination enables HsVT to capture spatiotemporal\nfeatures, improving its capability to handle complex event-based object\ndetection tasks. To support research in this area, we developed and publicly\nreleased The Fall Detection Dataset as a benchmark for event-based object\ndetection tasks. This dataset, captured using an event-based camera, ensures\nfacial privacy protection and reduces memory usage due to the event\nrepresentation format. We evaluated the HsVT model on GEN1 and Fall Detection\ndatasets across various model sizes. Experimental results demonstrate that HsVT\nachieves significant performance improvements in event detection with fewer\nparameters.\n", "link": "http://arxiv.org/abs/2505.07715v1", "date": "2025-05-12", "relevancy": 2.6933, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.549}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5406}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Spiking%20Vision%20Transformer%20for%20Object%20Detection%20with%20Event%0A%20%20Cameras&body=Title%3A%20Hybrid%20Spiking%20Vision%20Transformer%20for%20Object%20Detection%20with%20Event%0A%20%20Cameras%0AAuthor%3A%20Qi%20Xu%20and%20Jie%20Deng%20and%20Jiangrong%20Shen%20and%20Biwu%20Chen%20and%20Huajin%20Tang%20and%20Gang%20Pan%0AAbstract%3A%20%20%20Event-based%20object%20detection%20has%20gained%20increasing%20attention%20due%20to%20its%0Aadvantages%20such%20as%20high%20temporal%20resolution%2C%20wide%20dynamic%20range%2C%20and%0Aasynchronous%20address-event%20representation.%20Leveraging%20these%20advantages%2C%20Spiking%0ANeural%20Networks%20%28SNNs%29%20have%20emerged%20as%20a%20promising%20approach%2C%20offering%20low%0Aenergy%20consumption%20and%20rich%20spatiotemporal%20dynamics.%20To%20further%20enhance%20the%0Aperformance%20of%20event-based%20object%20detection%2C%20this%20study%20proposes%20a%20novel%20hybrid%0Aspike%20vision%20Transformer%20%28HsVT%29%20model.%20The%20HsVT%20model%20integrates%20a%20spatial%0Afeature%20extraction%20module%20to%20capture%20local%20and%20global%20features%2C%20and%20a%20temporal%0Afeature%20extraction%20module%20to%20model%20time%20dependencies%20and%20long-term%20patterns%20in%0Aevent%20sequences.%20This%20combination%20enables%20HsVT%20to%20capture%20spatiotemporal%0Afeatures%2C%20improving%20its%20capability%20to%20handle%20complex%20event-based%20object%0Adetection%20tasks.%20To%20support%20research%20in%20this%20area%2C%20we%20developed%20and%20publicly%0Areleased%20The%20Fall%20Detection%20Dataset%20as%20a%20benchmark%20for%20event-based%20object%0Adetection%20tasks.%20This%20dataset%2C%20captured%20using%20an%20event-based%20camera%2C%20ensures%0Afacial%20privacy%20protection%20and%20reduces%20memory%20usage%20due%20to%20the%20event%0Arepresentation%20format.%20We%20evaluated%20the%20HsVT%20model%20on%20GEN1%20and%20Fall%20Detection%0Adatasets%20across%20various%20model%20sizes.%20Experimental%20results%20demonstrate%20that%20HsVT%0Aachieves%20significant%20performance%20improvements%20in%20event%20detection%20with%20fewer%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Spiking%2520Vision%2520Transformer%2520for%2520Object%2520Detection%2520with%2520Event%250A%2520%2520Cameras%26entry.906535625%3DQi%2520Xu%2520and%2520Jie%2520Deng%2520and%2520Jiangrong%2520Shen%2520and%2520Biwu%2520Chen%2520and%2520Huajin%2520Tang%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%2520Event-based%2520object%2520detection%2520has%2520gained%2520increasing%2520attention%2520due%2520to%2520its%250Aadvantages%2520such%2520as%2520high%2520temporal%2520resolution%252C%2520wide%2520dynamic%2520range%252C%2520and%250Aasynchronous%2520address-event%2520representation.%2520Leveraging%2520these%2520advantages%252C%2520Spiking%250ANeural%2520Networks%2520%2528SNNs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%252C%2520offering%2520low%250Aenergy%2520consumption%2520and%2520rich%2520spatiotemporal%2520dynamics.%2520To%2520further%2520enhance%2520the%250Aperformance%2520of%2520event-based%2520object%2520detection%252C%2520this%2520study%2520proposes%2520a%2520novel%2520hybrid%250Aspike%2520vision%2520Transformer%2520%2528HsVT%2529%2520model.%2520The%2520HsVT%2520model%2520integrates%2520a%2520spatial%250Afeature%2520extraction%2520module%2520to%2520capture%2520local%2520and%2520global%2520features%252C%2520and%2520a%2520temporal%250Afeature%2520extraction%2520module%2520to%2520model%2520time%2520dependencies%2520and%2520long-term%2520patterns%2520in%250Aevent%2520sequences.%2520This%2520combination%2520enables%2520HsVT%2520to%2520capture%2520spatiotemporal%250Afeatures%252C%2520improving%2520its%2520capability%2520to%2520handle%2520complex%2520event-based%2520object%250Adetection%2520tasks.%2520To%2520support%2520research%2520in%2520this%2520area%252C%2520we%2520developed%2520and%2520publicly%250Areleased%2520The%2520Fall%2520Detection%2520Dataset%2520as%2520a%2520benchmark%2520for%2520event-based%2520object%250Adetection%2520tasks.%2520This%2520dataset%252C%2520captured%2520using%2520an%2520event-based%2520camera%252C%2520ensures%250Afacial%2520privacy%2520protection%2520and%2520reduces%2520memory%2520usage%2520due%2520to%2520the%2520event%250Arepresentation%2520format.%2520We%2520evaluated%2520the%2520HsVT%2520model%2520on%2520GEN1%2520and%2520Fall%2520Detection%250Adatasets%2520across%2520various%2520model%2520sizes.%2520Experimental%2520results%2520demonstrate%2520that%2520HsVT%250Aachieves%2520significant%2520performance%2520improvements%2520in%2520event%2520detection%2520with%2520fewer%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Spiking%20Vision%20Transformer%20for%20Object%20Detection%20with%20Event%0A%20%20Cameras&entry.906535625=Qi%20Xu%20and%20Jie%20Deng%20and%20Jiangrong%20Shen%20and%20Biwu%20Chen%20and%20Huajin%20Tang%20and%20Gang%20Pan&entry.1292438233=%20%20Event-based%20object%20detection%20has%20gained%20increasing%20attention%20due%20to%20its%0Aadvantages%20such%20as%20high%20temporal%20resolution%2C%20wide%20dynamic%20range%2C%20and%0Aasynchronous%20address-event%20representation.%20Leveraging%20these%20advantages%2C%20Spiking%0ANeural%20Networks%20%28SNNs%29%20have%20emerged%20as%20a%20promising%20approach%2C%20offering%20low%0Aenergy%20consumption%20and%20rich%20spatiotemporal%20dynamics.%20To%20further%20enhance%20the%0Aperformance%20of%20event-based%20object%20detection%2C%20this%20study%20proposes%20a%20novel%20hybrid%0Aspike%20vision%20Transformer%20%28HsVT%29%20model.%20The%20HsVT%20model%20integrates%20a%20spatial%0Afeature%20extraction%20module%20to%20capture%20local%20and%20global%20features%2C%20and%20a%20temporal%0Afeature%20extraction%20module%20to%20model%20time%20dependencies%20and%20long-term%20patterns%20in%0Aevent%20sequences.%20This%20combination%20enables%20HsVT%20to%20capture%20spatiotemporal%0Afeatures%2C%20improving%20its%20capability%20to%20handle%20complex%20event-based%20object%0Adetection%20tasks.%20To%20support%20research%20in%20this%20area%2C%20we%20developed%20and%20publicly%0Areleased%20The%20Fall%20Detection%20Dataset%20as%20a%20benchmark%20for%20event-based%20object%0Adetection%20tasks.%20This%20dataset%2C%20captured%20using%20an%20event-based%20camera%2C%20ensures%0Afacial%20privacy%20protection%20and%20reduces%20memory%20usage%20due%20to%20the%20event%0Arepresentation%20format.%20We%20evaluated%20the%20HsVT%20model%20on%20GEN1%20and%20Fall%20Detection%0Adatasets%20across%20various%20model%20sizes.%20Experimental%20results%20demonstrate%20that%20HsVT%0Aachieves%20significant%20performance%20improvements%20in%20event%20detection%20with%20fewer%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07715v1&entry.124074799=Read"},
{"title": "Prototype Augmented Hypernetworks for Continual Learning", "author": "Neil De La Fuente and Maria Pilligua and Daniel Vidal and Albin Soutiff and Cecilia Curreli and Daniel Cremers and Andrey Barsky", "abstract": "  Continual learning (CL) aims to learn a sequence of tasks without forgetting\nprior knowledge, but gradient updates for a new task often overwrite the\nweights learned earlier, causing catastrophic forgetting (CF). We propose\nPrototype-Augmented Hypernetworks (PAH), a framework where a single\nhypernetwork, conditioned on learnable task prototypes, dynamically generates\ntask-specific classifier heads on demand. To mitigate forgetting, PAH combines\ncross-entropy with dual distillation losses, one to align logits and another to\nalign prototypes, ensuring stable feature representations across tasks.\nEvaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves\nstate-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7\n% and 4.4 % forgetting, respectively, surpassing prior methods without storing\nsamples or heads.\n", "link": "http://arxiv.org/abs/2505.07450v1", "date": "2025-05-12", "relevancy": 2.5934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5176}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Augmented%20Hypernetworks%20for%20Continual%20Learning&body=Title%3A%20Prototype%20Augmented%20Hypernetworks%20for%20Continual%20Learning%0AAuthor%3A%20Neil%20De%20La%20Fuente%20and%20Maria%20Pilligua%20and%20Daniel%20Vidal%20and%20Albin%20Soutiff%20and%20Cecilia%20Curreli%20and%20Daniel%20Cremers%20and%20Andrey%20Barsky%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20aims%20to%20learn%20a%20sequence%20of%20tasks%20without%20forgetting%0Aprior%20knowledge%2C%20but%20gradient%20updates%20for%20a%20new%20task%20often%20overwrite%20the%0Aweights%20learned%20earlier%2C%20causing%20catastrophic%20forgetting%20%28CF%29.%20We%20propose%0APrototype-Augmented%20Hypernetworks%20%28PAH%29%2C%20a%20framework%20where%20a%20single%0Ahypernetwork%2C%20conditioned%20on%20learnable%20task%20prototypes%2C%20dynamically%20generates%0Atask-specific%20classifier%20heads%20on%20demand.%20To%20mitigate%20forgetting%2C%20PAH%20combines%0Across-entropy%20with%20dual%20distillation%20losses%2C%20one%20to%20align%20logits%20and%20another%20to%0Aalign%20prototypes%2C%20ensuring%20stable%20feature%20representations%20across%20tasks.%0AEvaluations%20on%20Split-CIFAR100%20and%20TinyImageNet%20demonstrate%20that%20PAH%20achieves%0Astate-of-the-art%20performance%2C%20reaching%2074.5%20%25%20and%2063.7%20%25%20accuracy%20with%20only%201.7%0A%25%20and%204.4%20%25%20forgetting%2C%20respectively%2C%20surpassing%20prior%20methods%20without%20storing%0Asamples%20or%20heads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Augmented%2520Hypernetworks%2520for%2520Continual%2520Learning%26entry.906535625%3DNeil%2520De%2520La%2520Fuente%2520and%2520Maria%2520Pilligua%2520and%2520Daniel%2520Vidal%2520and%2520Albin%2520Soutiff%2520and%2520Cecilia%2520Curreli%2520and%2520Daniel%2520Cremers%2520and%2520Andrey%2520Barsky%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520aims%2520to%2520learn%2520a%2520sequence%2520of%2520tasks%2520without%2520forgetting%250Aprior%2520knowledge%252C%2520but%2520gradient%2520updates%2520for%2520a%2520new%2520task%2520often%2520overwrite%2520the%250Aweights%2520learned%2520earlier%252C%2520causing%2520catastrophic%2520forgetting%2520%2528CF%2529.%2520We%2520propose%250APrototype-Augmented%2520Hypernetworks%2520%2528PAH%2529%252C%2520a%2520framework%2520where%2520a%2520single%250Ahypernetwork%252C%2520conditioned%2520on%2520learnable%2520task%2520prototypes%252C%2520dynamically%2520generates%250Atask-specific%2520classifier%2520heads%2520on%2520demand.%2520To%2520mitigate%2520forgetting%252C%2520PAH%2520combines%250Across-entropy%2520with%2520dual%2520distillation%2520losses%252C%2520one%2520to%2520align%2520logits%2520and%2520another%2520to%250Aalign%2520prototypes%252C%2520ensuring%2520stable%2520feature%2520representations%2520across%2520tasks.%250AEvaluations%2520on%2520Split-CIFAR100%2520and%2520TinyImageNet%2520demonstrate%2520that%2520PAH%2520achieves%250Astate-of-the-art%2520performance%252C%2520reaching%252074.5%2520%2525%2520and%252063.7%2520%2525%2520accuracy%2520with%2520only%25201.7%250A%2525%2520and%25204.4%2520%2525%2520forgetting%252C%2520respectively%252C%2520surpassing%2520prior%2520methods%2520without%2520storing%250Asamples%2520or%2520heads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Augmented%20Hypernetworks%20for%20Continual%20Learning&entry.906535625=Neil%20De%20La%20Fuente%20and%20Maria%20Pilligua%20and%20Daniel%20Vidal%20and%20Albin%20Soutiff%20and%20Cecilia%20Curreli%20and%20Daniel%20Cremers%20and%20Andrey%20Barsky&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20aims%20to%20learn%20a%20sequence%20of%20tasks%20without%20forgetting%0Aprior%20knowledge%2C%20but%20gradient%20updates%20for%20a%20new%20task%20often%20overwrite%20the%0Aweights%20learned%20earlier%2C%20causing%20catastrophic%20forgetting%20%28CF%29.%20We%20propose%0APrototype-Augmented%20Hypernetworks%20%28PAH%29%2C%20a%20framework%20where%20a%20single%0Ahypernetwork%2C%20conditioned%20on%20learnable%20task%20prototypes%2C%20dynamically%20generates%0Atask-specific%20classifier%20heads%20on%20demand.%20To%20mitigate%20forgetting%2C%20PAH%20combines%0Across-entropy%20with%20dual%20distillation%20losses%2C%20one%20to%20align%20logits%20and%20another%20to%0Aalign%20prototypes%2C%20ensuring%20stable%20feature%20representations%20across%20tasks.%0AEvaluations%20on%20Split-CIFAR100%20and%20TinyImageNet%20demonstrate%20that%20PAH%20achieves%0Astate-of-the-art%20performance%2C%20reaching%2074.5%20%25%20and%2063.7%20%25%20accuracy%20with%20only%201.7%0A%25%20and%204.4%20%25%20forgetting%2C%20respectively%2C%20surpassing%20prior%20methods%20without%20storing%0Asamples%20or%20heads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07450v1&entry.124074799=Read"},
{"title": "Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene\n  Understanding", "author": "Yuchen Rao and Stefan Ainetter and Sinisa Stekovic and Vincent Lepetit and Friedrich Fraundorfer", "abstract": "  High-level 3D scene understanding is essential in many applications. However,\nthe challenges of generating accurate 3D annotations make development of deep\nlearning models difficult. We turn to recent advancements in automatic\nretrieval of synthetic CAD models, and show that data generated by such methods\ncan be used as high-quality ground truth for training supervised deep learning\nmodels. More exactly, we employ a pipeline akin to the one previously used to\nautomatically annotate objects in ScanNet scenes with their 9D poses and CAD\nmodels. This time, we apply it to the recent ScanNet++ v1 dataset, which\npreviously lacked such annotations. Our findings demonstrate that it is not\nonly possible to train deep learning models on these automatically-obtained\nannotations but that the resulting models outperform those trained on manually\nannotated data. We validate this on two distinct tasks: point cloud completion\nand single-view CAD model retrieval and alignment. Our results underscore the\npotential of automatic 3D annotations to enhance model performance while\nsignificantly reducing annotation costs. To support future research in 3D scene\nunderstanding, we will release our annotations, which we call SCANnotate++,\nalong with our trained models.\n", "link": "http://arxiv.org/abs/2504.13580v3", "date": "2025-05-12", "relevancy": 2.564, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding&body=Title%3A%20Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Yuchen%20Rao%20and%20Stefan%20Ainetter%20and%20Sinisa%20Stekovic%20and%20Vincent%20Lepetit%20and%20Friedrich%20Fraundorfer%0AAbstract%3A%20%20%20High-level%203D%20scene%20understanding%20is%20essential%20in%20many%20applications.%20However%2C%0Athe%20challenges%20of%20generating%20accurate%203D%20annotations%20make%20development%20of%20deep%0Alearning%20models%20difficult.%20We%20turn%20to%20recent%20advancements%20in%20automatic%0Aretrieval%20of%20synthetic%20CAD%20models%2C%20and%20show%20that%20data%20generated%20by%20such%20methods%0Acan%20be%20used%20as%20high-quality%20ground%20truth%20for%20training%20supervised%20deep%20learning%0Amodels.%20More%20exactly%2C%20we%20employ%20a%20pipeline%20akin%20to%20the%20one%20previously%20used%20to%0Aautomatically%20annotate%20objects%20in%20ScanNet%20scenes%20with%20their%209D%20poses%20and%20CAD%0Amodels.%20This%20time%2C%20we%20apply%20it%20to%20the%20recent%20ScanNet%2B%2B%20v1%20dataset%2C%20which%0Apreviously%20lacked%20such%20annotations.%20Our%20findings%20demonstrate%20that%20it%20is%20not%0Aonly%20possible%20to%20train%20deep%20learning%20models%20on%20these%20automatically-obtained%0Aannotations%20but%20that%20the%20resulting%20models%20outperform%20those%20trained%20on%20manually%0Aannotated%20data.%20We%20validate%20this%20on%20two%20distinct%20tasks%3A%20point%20cloud%20completion%0Aand%20single-view%20CAD%20model%20retrieval%20and%20alignment.%20Our%20results%20underscore%20the%0Apotential%20of%20automatic%203D%20annotations%20to%20enhance%20model%20performance%20while%0Asignificantly%20reducing%20annotation%20costs.%20To%20support%20future%20research%20in%203D%20scene%0Aunderstanding%2C%20we%20will%20release%20our%20annotations%2C%20which%20we%20call%20SCANnotate%2B%2B%2C%0Aalong%20with%20our%20trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13580v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Automatic%2520CAD%2520Annotations%2520for%2520Supervised%2520Learning%2520in%25203D%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYuchen%2520Rao%2520and%2520Stefan%2520Ainetter%2520and%2520Sinisa%2520Stekovic%2520and%2520Vincent%2520Lepetit%2520and%2520Friedrich%2520Fraundorfer%26entry.1292438233%3D%2520%2520High-level%25203D%2520scene%2520understanding%2520is%2520essential%2520in%2520many%2520applications.%2520However%252C%250Athe%2520challenges%2520of%2520generating%2520accurate%25203D%2520annotations%2520make%2520development%2520of%2520deep%250Alearning%2520models%2520difficult.%2520We%2520turn%2520to%2520recent%2520advancements%2520in%2520automatic%250Aretrieval%2520of%2520synthetic%2520CAD%2520models%252C%2520and%2520show%2520that%2520data%2520generated%2520by%2520such%2520methods%250Acan%2520be%2520used%2520as%2520high-quality%2520ground%2520truth%2520for%2520training%2520supervised%2520deep%2520learning%250Amodels.%2520More%2520exactly%252C%2520we%2520employ%2520a%2520pipeline%2520akin%2520to%2520the%2520one%2520previously%2520used%2520to%250Aautomatically%2520annotate%2520objects%2520in%2520ScanNet%2520scenes%2520with%2520their%25209D%2520poses%2520and%2520CAD%250Amodels.%2520This%2520time%252C%2520we%2520apply%2520it%2520to%2520the%2520recent%2520ScanNet%252B%252B%2520v1%2520dataset%252C%2520which%250Apreviously%2520lacked%2520such%2520annotations.%2520Our%2520findings%2520demonstrate%2520that%2520it%2520is%2520not%250Aonly%2520possible%2520to%2520train%2520deep%2520learning%2520models%2520on%2520these%2520automatically-obtained%250Aannotations%2520but%2520that%2520the%2520resulting%2520models%2520outperform%2520those%2520trained%2520on%2520manually%250Aannotated%2520data.%2520We%2520validate%2520this%2520on%2520two%2520distinct%2520tasks%253A%2520point%2520cloud%2520completion%250Aand%2520single-view%2520CAD%2520model%2520retrieval%2520and%2520alignment.%2520Our%2520results%2520underscore%2520the%250Apotential%2520of%2520automatic%25203D%2520annotations%2520to%2520enhance%2520model%2520performance%2520while%250Asignificantly%2520reducing%2520annotation%2520costs.%2520To%2520support%2520future%2520research%2520in%25203D%2520scene%250Aunderstanding%252C%2520we%2520will%2520release%2520our%2520annotations%252C%2520which%2520we%2520call%2520SCANnotate%252B%252B%252C%250Aalong%2520with%2520our%2520trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13580v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding&entry.906535625=Yuchen%20Rao%20and%20Stefan%20Ainetter%20and%20Sinisa%20Stekovic%20and%20Vincent%20Lepetit%20and%20Friedrich%20Fraundorfer&entry.1292438233=%20%20High-level%203D%20scene%20understanding%20is%20essential%20in%20many%20applications.%20However%2C%0Athe%20challenges%20of%20generating%20accurate%203D%20annotations%20make%20development%20of%20deep%0Alearning%20models%20difficult.%20We%20turn%20to%20recent%20advancements%20in%20automatic%0Aretrieval%20of%20synthetic%20CAD%20models%2C%20and%20show%20that%20data%20generated%20by%20such%20methods%0Acan%20be%20used%20as%20high-quality%20ground%20truth%20for%20training%20supervised%20deep%20learning%0Amodels.%20More%20exactly%2C%20we%20employ%20a%20pipeline%20akin%20to%20the%20one%20previously%20used%20to%0Aautomatically%20annotate%20objects%20in%20ScanNet%20scenes%20with%20their%209D%20poses%20and%20CAD%0Amodels.%20This%20time%2C%20we%20apply%20it%20to%20the%20recent%20ScanNet%2B%2B%20v1%20dataset%2C%20which%0Apreviously%20lacked%20such%20annotations.%20Our%20findings%20demonstrate%20that%20it%20is%20not%0Aonly%20possible%20to%20train%20deep%20learning%20models%20on%20these%20automatically-obtained%0Aannotations%20but%20that%20the%20resulting%20models%20outperform%20those%20trained%20on%20manually%0Aannotated%20data.%20We%20validate%20this%20on%20two%20distinct%20tasks%3A%20point%20cloud%20completion%0Aand%20single-view%20CAD%20model%20retrieval%20and%20alignment.%20Our%20results%20underscore%20the%0Apotential%20of%20automatic%203D%20annotations%20to%20enhance%20model%20performance%20while%0Asignificantly%20reducing%20annotation%20costs.%20To%20support%20future%20research%20in%203D%20scene%0Aunderstanding%2C%20we%20will%20release%20our%20annotations%2C%20which%20we%20call%20SCANnotate%2B%2B%2C%0Aalong%20with%20our%20trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13580v3&entry.124074799=Read"},
{"title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with\n  Vision-Language Models", "author": "Seungjae Lee and Daniel Ekpo and Haowen Liu and Furong Huang and Abhinav Shrivastava and Jia-Bin Huang", "abstract": "  Exploration is essential for general-purpose robotic learning, especially in\nopen-ended environments where dense rewards, explicit goals, or task-specific\nsupervision are scarce. Vision-language models (VLMs), with their semantic\nreasoning over objects, spatial relations, and potential outcomes, present a\ncompelling foundation for generating high-level exploratory behaviors. However,\ntheir outputs are often ungrounded, making it difficult to determine whether\nimagined transitions are physically feasible or informative. To bridge the gap\nbetween imagination and execution, we present IVE (Imagine, Verify, Execute),\nan agentic exploration framework inspired by human curiosity. Human exploration\nis often driven by the desire to discover novel scene configurations and to\ndeepen understanding of the environment. Similarly, IVE leverages VLMs to\nabstract RGB-D observations into semantic scene graphs, imagine novel scenes,\npredict their physical plausibility, and generate executable skill sequences\nthrough action tools. We evaluate IVE in both simulated and real-world tabletop\nenvironments. The results show that IVE enables more diverse and meaningful\nexploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the\nentropy of visited states. Moreover, the collected experience supports\ndownstream learning, producing policies that closely match or exceed the\nperformance of those trained on human-collected demonstrations.\n", "link": "http://arxiv.org/abs/2505.07815v1", "date": "2025-05-12", "relevancy": 2.5576, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagine%2C%20Verify%2C%20Execute%3A%20Memory-Guided%20Agentic%20Exploration%20with%0A%20%20Vision-Language%20Models&body=Title%3A%20Imagine%2C%20Verify%2C%20Execute%3A%20Memory-Guided%20Agentic%20Exploration%20with%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Seungjae%20Lee%20and%20Daniel%20Ekpo%20and%20Haowen%20Liu%20and%20Furong%20Huang%20and%20Abhinav%20Shrivastava%20and%20Jia-Bin%20Huang%0AAbstract%3A%20%20%20Exploration%20is%20essential%20for%20general-purpose%20robotic%20learning%2C%20especially%20in%0Aopen-ended%20environments%20where%20dense%20rewards%2C%20explicit%20goals%2C%20or%20task-specific%0Asupervision%20are%20scarce.%20Vision-language%20models%20%28VLMs%29%2C%20with%20their%20semantic%0Areasoning%20over%20objects%2C%20spatial%20relations%2C%20and%20potential%20outcomes%2C%20present%20a%0Acompelling%20foundation%20for%20generating%20high-level%20exploratory%20behaviors.%20However%2C%0Atheir%20outputs%20are%20often%20ungrounded%2C%20making%20it%20difficult%20to%20determine%20whether%0Aimagined%20transitions%20are%20physically%20feasible%20or%20informative.%20To%20bridge%20the%20gap%0Abetween%20imagination%20and%20execution%2C%20we%20present%20IVE%20%28Imagine%2C%20Verify%2C%20Execute%29%2C%0Aan%20agentic%20exploration%20framework%20inspired%20by%20human%20curiosity.%20Human%20exploration%0Ais%20often%20driven%20by%20the%20desire%20to%20discover%20novel%20scene%20configurations%20and%20to%0Adeepen%20understanding%20of%20the%20environment.%20Similarly%2C%20IVE%20leverages%20VLMs%20to%0Aabstract%20RGB-D%20observations%20into%20semantic%20scene%20graphs%2C%20imagine%20novel%20scenes%2C%0Apredict%20their%20physical%20plausibility%2C%20and%20generate%20executable%20skill%20sequences%0Athrough%20action%20tools.%20We%20evaluate%20IVE%20in%20both%20simulated%20and%20real-world%20tabletop%0Aenvironments.%20The%20results%20show%20that%20IVE%20enables%20more%20diverse%20and%20meaningful%0Aexploration%20than%20RL%20baselines%2C%20as%20evidenced%20by%20a%204.1%20to%207.8x%20increase%20in%20the%0Aentropy%20of%20visited%20states.%20Moreover%2C%20the%20collected%20experience%20supports%0Adownstream%20learning%2C%20producing%20policies%20that%20closely%20match%20or%20exceed%20the%0Aperformance%20of%20those%20trained%20on%20human-collected%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagine%252C%2520Verify%252C%2520Execute%253A%2520Memory-Guided%2520Agentic%2520Exploration%2520with%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DSeungjae%2520Lee%2520and%2520Daniel%2520Ekpo%2520and%2520Haowen%2520Liu%2520and%2520Furong%2520Huang%2520and%2520Abhinav%2520Shrivastava%2520and%2520Jia-Bin%2520Huang%26entry.1292438233%3D%2520%2520Exploration%2520is%2520essential%2520for%2520general-purpose%2520robotic%2520learning%252C%2520especially%2520in%250Aopen-ended%2520environments%2520where%2520dense%2520rewards%252C%2520explicit%2520goals%252C%2520or%2520task-specific%250Asupervision%2520are%2520scarce.%2520Vision-language%2520models%2520%2528VLMs%2529%252C%2520with%2520their%2520semantic%250Areasoning%2520over%2520objects%252C%2520spatial%2520relations%252C%2520and%2520potential%2520outcomes%252C%2520present%2520a%250Acompelling%2520foundation%2520for%2520generating%2520high-level%2520exploratory%2520behaviors.%2520However%252C%250Atheir%2520outputs%2520are%2520often%2520ungrounded%252C%2520making%2520it%2520difficult%2520to%2520determine%2520whether%250Aimagined%2520transitions%2520are%2520physically%2520feasible%2520or%2520informative.%2520To%2520bridge%2520the%2520gap%250Abetween%2520imagination%2520and%2520execution%252C%2520we%2520present%2520IVE%2520%2528Imagine%252C%2520Verify%252C%2520Execute%2529%252C%250Aan%2520agentic%2520exploration%2520framework%2520inspired%2520by%2520human%2520curiosity.%2520Human%2520exploration%250Ais%2520often%2520driven%2520by%2520the%2520desire%2520to%2520discover%2520novel%2520scene%2520configurations%2520and%2520to%250Adeepen%2520understanding%2520of%2520the%2520environment.%2520Similarly%252C%2520IVE%2520leverages%2520VLMs%2520to%250Aabstract%2520RGB-D%2520observations%2520into%2520semantic%2520scene%2520graphs%252C%2520imagine%2520novel%2520scenes%252C%250Apredict%2520their%2520physical%2520plausibility%252C%2520and%2520generate%2520executable%2520skill%2520sequences%250Athrough%2520action%2520tools.%2520We%2520evaluate%2520IVE%2520in%2520both%2520simulated%2520and%2520real-world%2520tabletop%250Aenvironments.%2520The%2520results%2520show%2520that%2520IVE%2520enables%2520more%2520diverse%2520and%2520meaningful%250Aexploration%2520than%2520RL%2520baselines%252C%2520as%2520evidenced%2520by%2520a%25204.1%2520to%25207.8x%2520increase%2520in%2520the%250Aentropy%2520of%2520visited%2520states.%2520Moreover%252C%2520the%2520collected%2520experience%2520supports%250Adownstream%2520learning%252C%2520producing%2520policies%2520that%2520closely%2520match%2520or%2520exceed%2520the%250Aperformance%2520of%2520those%2520trained%2520on%2520human-collected%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine%2C%20Verify%2C%20Execute%3A%20Memory-Guided%20Agentic%20Exploration%20with%0A%20%20Vision-Language%20Models&entry.906535625=Seungjae%20Lee%20and%20Daniel%20Ekpo%20and%20Haowen%20Liu%20and%20Furong%20Huang%20and%20Abhinav%20Shrivastava%20and%20Jia-Bin%20Huang&entry.1292438233=%20%20Exploration%20is%20essential%20for%20general-purpose%20robotic%20learning%2C%20especially%20in%0Aopen-ended%20environments%20where%20dense%20rewards%2C%20explicit%20goals%2C%20or%20task-specific%0Asupervision%20are%20scarce.%20Vision-language%20models%20%28VLMs%29%2C%20with%20their%20semantic%0Areasoning%20over%20objects%2C%20spatial%20relations%2C%20and%20potential%20outcomes%2C%20present%20a%0Acompelling%20foundation%20for%20generating%20high-level%20exploratory%20behaviors.%20However%2C%0Atheir%20outputs%20are%20often%20ungrounded%2C%20making%20it%20difficult%20to%20determine%20whether%0Aimagined%20transitions%20are%20physically%20feasible%20or%20informative.%20To%20bridge%20the%20gap%0Abetween%20imagination%20and%20execution%2C%20we%20present%20IVE%20%28Imagine%2C%20Verify%2C%20Execute%29%2C%0Aan%20agentic%20exploration%20framework%20inspired%20by%20human%20curiosity.%20Human%20exploration%0Ais%20often%20driven%20by%20the%20desire%20to%20discover%20novel%20scene%20configurations%20and%20to%0Adeepen%20understanding%20of%20the%20environment.%20Similarly%2C%20IVE%20leverages%20VLMs%20to%0Aabstract%20RGB-D%20observations%20into%20semantic%20scene%20graphs%2C%20imagine%20novel%20scenes%2C%0Apredict%20their%20physical%20plausibility%2C%20and%20generate%20executable%20skill%20sequences%0Athrough%20action%20tools.%20We%20evaluate%20IVE%20in%20both%20simulated%20and%20real-world%20tabletop%0Aenvironments.%20The%20results%20show%20that%20IVE%20enables%20more%20diverse%20and%20meaningful%0Aexploration%20than%20RL%20baselines%2C%20as%20evidenced%20by%20a%204.1%20to%207.8x%20increase%20in%20the%0Aentropy%20of%20visited%20states.%20Moreover%2C%20the%20collected%20experience%20supports%0Adownstream%20learning%2C%20producing%20policies%20that%20closely%20match%20or%20exceed%20the%0Aperformance%20of%20those%20trained%20on%20human-collected%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07815v1&entry.124074799=Read"},
{"title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining", "author": "Xiaomi LLM-Core Team and  : and Bingquan Xia and Bowen Shen and  Cici and Dawei Zhu and Di Zhang and Gang Wang and Hailin Zhang and Huaqiu Liu and Jiebao Xiao and Jinhao Dong and Liang Zhao and Peidian Li and Peng Wang and Shihua Yu and Shimao Chen and Weikun Wang and Wenhan Ma and Xiangwei Deng and Yi Huang and Yifan Song and Zihan Jiang and Bowen Ye and Can Cai and Chenhong He and Dong Zhang and Duo Zhang and Guoan Wang and Hao Tian and Haochen Zhao and Heng Qu and Hongshen Xu and Jun Shi and Kainan Bao and QingKai Fang and Kang Zhou and Kangyang Zhou and Lei Li and Menghang Zhu and Nuo Chen and Qiantong Wang and Shaohui Liu and Shicheng Li and Shuhao Gu and Shuhuai Ren and Shuo Liu and Sirui Deng and Weiji Zhuang and Weiwei Lv and Wenyu Yang and Xin Zhang and Xing Yong and Xing Zhang and Xingchen Song and Xinzhe Xu and Xu Wang and Yihan Yan and Yu Tu and Yuanyuan Tian and Yudong Wang and Yue Yu and Zhenru Lin and Zhichao Song and Zihao Yue", "abstract": "  We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.\n", "link": "http://arxiv.org/abs/2505.07608v1", "date": "2025-05-12", "relevancy": 2.5322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiMo%3A%20Unlocking%20the%20Reasoning%20Potential%20of%20Language%20Model%20--%20From%0A%20%20Pretraining%20to%20Posttraining&body=Title%3A%20MiMo%3A%20Unlocking%20the%20Reasoning%20Potential%20of%20Language%20Model%20--%20From%0A%20%20Pretraining%20to%20Posttraining%0AAuthor%3A%20Xiaomi%20LLM-Core%20Team%20and%20%20%3A%20and%20Bingquan%20Xia%20and%20Bowen%20Shen%20and%20%20Cici%20and%20Dawei%20Zhu%20and%20Di%20Zhang%20and%20Gang%20Wang%20and%20Hailin%20Zhang%20and%20Huaqiu%20Liu%20and%20Jiebao%20Xiao%20and%20Jinhao%20Dong%20and%20Liang%20Zhao%20and%20Peidian%20Li%20and%20Peng%20Wang%20and%20Shihua%20Yu%20and%20Shimao%20Chen%20and%20Weikun%20Wang%20and%20Wenhan%20Ma%20and%20Xiangwei%20Deng%20and%20Yi%20Huang%20and%20Yifan%20Song%20and%20Zihan%20Jiang%20and%20Bowen%20Ye%20and%20Can%20Cai%20and%20Chenhong%20He%20and%20Dong%20Zhang%20and%20Duo%20Zhang%20and%20Guoan%20Wang%20and%20Hao%20Tian%20and%20Haochen%20Zhao%20and%20Heng%20Qu%20and%20Hongshen%20Xu%20and%20Jun%20Shi%20and%20Kainan%20Bao%20and%20QingKai%20Fang%20and%20Kang%20Zhou%20and%20Kangyang%20Zhou%20and%20Lei%20Li%20and%20Menghang%20Zhu%20and%20Nuo%20Chen%20and%20Qiantong%20Wang%20and%20Shaohui%20Liu%20and%20Shicheng%20Li%20and%20Shuhao%20Gu%20and%20Shuhuai%20Ren%20and%20Shuo%20Liu%20and%20Sirui%20Deng%20and%20Weiji%20Zhuang%20and%20Weiwei%20Lv%20and%20Wenyu%20Yang%20and%20Xin%20Zhang%20and%20Xing%20Yong%20and%20Xing%20Zhang%20and%20Xingchen%20Song%20and%20Xinzhe%20Xu%20and%20Xu%20Wang%20and%20Yihan%20Yan%20and%20Yu%20Tu%20and%20Yuanyuan%20Tian%20and%20Yudong%20Wang%20and%20Yue%20Yu%20and%20Zhenru%20Lin%20and%20Zhichao%20Song%20and%20Zihao%20Yue%0AAbstract%3A%20%20%20We%20present%20MiMo-7B%2C%20a%20large%20language%20model%20born%20for%20reasoning%20tasks%2C%20with%0Aoptimization%20across%20both%20pre-training%20and%20post-training%20stages.%20During%0Apre-training%2C%20we%20enhance%20the%20data%20preprocessing%20pipeline%20and%20employ%20a%0Athree-stage%20data%20mixing%20strategy%20to%20strengthen%20the%20base%20model%27s%20reasoning%0Apotential.%20MiMo-7B-Base%20is%20pre-trained%20on%2025%20trillion%20tokens%2C%20with%20additional%0AMulti-Token%20Prediction%20objective%20for%20enhanced%20performance%20and%20accelerated%0Ainference%20speed.%20During%20post-training%2C%20we%20curate%20a%20dataset%20of%20130K%20verifiable%0Amathematics%20and%20programming%20problems%20for%20reinforcement%20learning%2C%20integrating%20a%0Atest-difficulty-driven%20code-reward%20scheme%20to%20alleviate%20sparse-reward%20issues%20and%0Aemploying%20strategic%20data%20resampling%20to%20stabilize%20training.%20Extensive%0Aevaluations%20show%20that%20MiMo-7B-Base%20possesses%20exceptional%20reasoning%20potential%2C%0Aoutperforming%20even%20much%20larger%2032B%20models.%20The%20final%20RL-tuned%20model%2C%0AMiMo-7B-RL%2C%20achieves%20superior%20performance%20on%20mathematics%2C%20code%20and%20general%0Areasoning%20tasks%2C%20surpassing%20the%20performance%20of%20OpenAI%20o1-mini.%20The%20model%0Acheckpoints%20are%20available%20at%20https%3A//github.com/xiaomimimo/MiMo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiMo%253A%2520Unlocking%2520the%2520Reasoning%2520Potential%2520of%2520Language%2520Model%2520--%2520From%250A%2520%2520Pretraining%2520to%2520Posttraining%26entry.906535625%3DXiaomi%2520LLM-Core%2520Team%2520and%2520%2520%253A%2520and%2520Bingquan%2520Xia%2520and%2520Bowen%2520Shen%2520and%2520%2520Cici%2520and%2520Dawei%2520Zhu%2520and%2520Di%2520Zhang%2520and%2520Gang%2520Wang%2520and%2520Hailin%2520Zhang%2520and%2520Huaqiu%2520Liu%2520and%2520Jiebao%2520Xiao%2520and%2520Jinhao%2520Dong%2520and%2520Liang%2520Zhao%2520and%2520Peidian%2520Li%2520and%2520Peng%2520Wang%2520and%2520Shihua%2520Yu%2520and%2520Shimao%2520Chen%2520and%2520Weikun%2520Wang%2520and%2520Wenhan%2520Ma%2520and%2520Xiangwei%2520Deng%2520and%2520Yi%2520Huang%2520and%2520Yifan%2520Song%2520and%2520Zihan%2520Jiang%2520and%2520Bowen%2520Ye%2520and%2520Can%2520Cai%2520and%2520Chenhong%2520He%2520and%2520Dong%2520Zhang%2520and%2520Duo%2520Zhang%2520and%2520Guoan%2520Wang%2520and%2520Hao%2520Tian%2520and%2520Haochen%2520Zhao%2520and%2520Heng%2520Qu%2520and%2520Hongshen%2520Xu%2520and%2520Jun%2520Shi%2520and%2520Kainan%2520Bao%2520and%2520QingKai%2520Fang%2520and%2520Kang%2520Zhou%2520and%2520Kangyang%2520Zhou%2520and%2520Lei%2520Li%2520and%2520Menghang%2520Zhu%2520and%2520Nuo%2520Chen%2520and%2520Qiantong%2520Wang%2520and%2520Shaohui%2520Liu%2520and%2520Shicheng%2520Li%2520and%2520Shuhao%2520Gu%2520and%2520Shuhuai%2520Ren%2520and%2520Shuo%2520Liu%2520and%2520Sirui%2520Deng%2520and%2520Weiji%2520Zhuang%2520and%2520Weiwei%2520Lv%2520and%2520Wenyu%2520Yang%2520and%2520Xin%2520Zhang%2520and%2520Xing%2520Yong%2520and%2520Xing%2520Zhang%2520and%2520Xingchen%2520Song%2520and%2520Xinzhe%2520Xu%2520and%2520Xu%2520Wang%2520and%2520Yihan%2520Yan%2520and%2520Yu%2520Tu%2520and%2520Yuanyuan%2520Tian%2520and%2520Yudong%2520Wang%2520and%2520Yue%2520Yu%2520and%2520Zhenru%2520Lin%2520and%2520Zhichao%2520Song%2520and%2520Zihao%2520Yue%26entry.1292438233%3D%2520%2520We%2520present%2520MiMo-7B%252C%2520a%2520large%2520language%2520model%2520born%2520for%2520reasoning%2520tasks%252C%2520with%250Aoptimization%2520across%2520both%2520pre-training%2520and%2520post-training%2520stages.%2520During%250Apre-training%252C%2520we%2520enhance%2520the%2520data%2520preprocessing%2520pipeline%2520and%2520employ%2520a%250Athree-stage%2520data%2520mixing%2520strategy%2520to%2520strengthen%2520the%2520base%2520model%2527s%2520reasoning%250Apotential.%2520MiMo-7B-Base%2520is%2520pre-trained%2520on%252025%2520trillion%2520tokens%252C%2520with%2520additional%250AMulti-Token%2520Prediction%2520objective%2520for%2520enhanced%2520performance%2520and%2520accelerated%250Ainference%2520speed.%2520During%2520post-training%252C%2520we%2520curate%2520a%2520dataset%2520of%2520130K%2520verifiable%250Amathematics%2520and%2520programming%2520problems%2520for%2520reinforcement%2520learning%252C%2520integrating%2520a%250Atest-difficulty-driven%2520code-reward%2520scheme%2520to%2520alleviate%2520sparse-reward%2520issues%2520and%250Aemploying%2520strategic%2520data%2520resampling%2520to%2520stabilize%2520training.%2520Extensive%250Aevaluations%2520show%2520that%2520MiMo-7B-Base%2520possesses%2520exceptional%2520reasoning%2520potential%252C%250Aoutperforming%2520even%2520much%2520larger%252032B%2520models.%2520The%2520final%2520RL-tuned%2520model%252C%250AMiMo-7B-RL%252C%2520achieves%2520superior%2520performance%2520on%2520mathematics%252C%2520code%2520and%2520general%250Areasoning%2520tasks%252C%2520surpassing%2520the%2520performance%2520of%2520OpenAI%2520o1-mini.%2520The%2520model%250Acheckpoints%2520are%2520available%2520at%2520https%253A//github.com/xiaomimimo/MiMo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiMo%3A%20Unlocking%20the%20Reasoning%20Potential%20of%20Language%20Model%20--%20From%0A%20%20Pretraining%20to%20Posttraining&entry.906535625=Xiaomi%20LLM-Core%20Team%20and%20%20%3A%20and%20Bingquan%20Xia%20and%20Bowen%20Shen%20and%20%20Cici%20and%20Dawei%20Zhu%20and%20Di%20Zhang%20and%20Gang%20Wang%20and%20Hailin%20Zhang%20and%20Huaqiu%20Liu%20and%20Jiebao%20Xiao%20and%20Jinhao%20Dong%20and%20Liang%20Zhao%20and%20Peidian%20Li%20and%20Peng%20Wang%20and%20Shihua%20Yu%20and%20Shimao%20Chen%20and%20Weikun%20Wang%20and%20Wenhan%20Ma%20and%20Xiangwei%20Deng%20and%20Yi%20Huang%20and%20Yifan%20Song%20and%20Zihan%20Jiang%20and%20Bowen%20Ye%20and%20Can%20Cai%20and%20Chenhong%20He%20and%20Dong%20Zhang%20and%20Duo%20Zhang%20and%20Guoan%20Wang%20and%20Hao%20Tian%20and%20Haochen%20Zhao%20and%20Heng%20Qu%20and%20Hongshen%20Xu%20and%20Jun%20Shi%20and%20Kainan%20Bao%20and%20QingKai%20Fang%20and%20Kang%20Zhou%20and%20Kangyang%20Zhou%20and%20Lei%20Li%20and%20Menghang%20Zhu%20and%20Nuo%20Chen%20and%20Qiantong%20Wang%20and%20Shaohui%20Liu%20and%20Shicheng%20Li%20and%20Shuhao%20Gu%20and%20Shuhuai%20Ren%20and%20Shuo%20Liu%20and%20Sirui%20Deng%20and%20Weiji%20Zhuang%20and%20Weiwei%20Lv%20and%20Wenyu%20Yang%20and%20Xin%20Zhang%20and%20Xing%20Yong%20and%20Xing%20Zhang%20and%20Xingchen%20Song%20and%20Xinzhe%20Xu%20and%20Xu%20Wang%20and%20Yihan%20Yan%20and%20Yu%20Tu%20and%20Yuanyuan%20Tian%20and%20Yudong%20Wang%20and%20Yue%20Yu%20and%20Zhenru%20Lin%20and%20Zhichao%20Song%20and%20Zihao%20Yue&entry.1292438233=%20%20We%20present%20MiMo-7B%2C%20a%20large%20language%20model%20born%20for%20reasoning%20tasks%2C%20with%0Aoptimization%20across%20both%20pre-training%20and%20post-training%20stages.%20During%0Apre-training%2C%20we%20enhance%20the%20data%20preprocessing%20pipeline%20and%20employ%20a%0Athree-stage%20data%20mixing%20strategy%20to%20strengthen%20the%20base%20model%27s%20reasoning%0Apotential.%20MiMo-7B-Base%20is%20pre-trained%20on%2025%20trillion%20tokens%2C%20with%20additional%0AMulti-Token%20Prediction%20objective%20for%20enhanced%20performance%20and%20accelerated%0Ainference%20speed.%20During%20post-training%2C%20we%20curate%20a%20dataset%20of%20130K%20verifiable%0Amathematics%20and%20programming%20problems%20for%20reinforcement%20learning%2C%20integrating%20a%0Atest-difficulty-driven%20code-reward%20scheme%20to%20alleviate%20sparse-reward%20issues%20and%0Aemploying%20strategic%20data%20resampling%20to%20stabilize%20training.%20Extensive%0Aevaluations%20show%20that%20MiMo-7B-Base%20possesses%20exceptional%20reasoning%20potential%2C%0Aoutperforming%20even%20much%20larger%2032B%20models.%20The%20final%20RL-tuned%20model%2C%0AMiMo-7B-RL%2C%20achieves%20superior%20performance%20on%20mathematics%2C%20code%20and%20general%0Areasoning%20tasks%2C%20surpassing%20the%20performance%20of%20OpenAI%20o1-mini.%20The%20model%0Acheckpoints%20are%20available%20at%20https%3A//github.com/xiaomimimo/MiMo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07608v1&entry.124074799=Read"},
{"title": "The Human-Data-Model Interaction Canvas for Visual Analytics", "author": "J\u00fcrgen Bernard", "abstract": "  Visual Analytics (VA) integrates humans, data, and models as key actors in\ninsight generation and data-driven decision-making. This position paper values\nand reflects on 16 VA process models and frameworks and makes nine high-level\nobservations that motivate a fresh perspective on VA. The contribution is the\nHDMI Canvas, a perspective to VA that complements the strengths of existing VA\nprocess models and frameworks. It systematically characterizes diverse roles of\nhumans, data, and models, and how these actors benefit from and contribute to\nVA processes. The descriptive power of the HDMI Canvas eases the\ndifferentiation between a series of VA building blocks, rather than describing\ngeneral VA principles only. The canvas includes modern human-centered\nmethodologies, including human knowledge externalization and forms of feedback\nloops, while interpretable and explainable AI highlight model contributions\nbeyond their conventional outputs. The HDMI Canvas has generative power,\nguiding the design of new VA processes and is optimized for external\nstakeholders, improving VA outreach, interdisciplinary collaboration, and\nuser-centered design. The utility of the HDMI Canvas is demonstrated through\ntwo preliminary case studies.\n", "link": "http://arxiv.org/abs/2505.07534v1", "date": "2025-05-12", "relevancy": 2.5283, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5393}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4888}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Human-Data-Model%20Interaction%20Canvas%20for%20Visual%20Analytics&body=Title%3A%20The%20Human-Data-Model%20Interaction%20Canvas%20for%20Visual%20Analytics%0AAuthor%3A%20J%C3%BCrgen%20Bernard%0AAbstract%3A%20%20%20Visual%20Analytics%20%28VA%29%20integrates%20humans%2C%20data%2C%20and%20models%20as%20key%20actors%20in%0Ainsight%20generation%20and%20data-driven%20decision-making.%20This%20position%20paper%20values%0Aand%20reflects%20on%2016%20VA%20process%20models%20and%20frameworks%20and%20makes%20nine%20high-level%0Aobservations%20that%20motivate%20a%20fresh%20perspective%20on%20VA.%20The%20contribution%20is%20the%0AHDMI%20Canvas%2C%20a%20perspective%20to%20VA%20that%20complements%20the%20strengths%20of%20existing%20VA%0Aprocess%20models%20and%20frameworks.%20It%20systematically%20characterizes%20diverse%20roles%20of%0Ahumans%2C%20data%2C%20and%20models%2C%20and%20how%20these%20actors%20benefit%20from%20and%20contribute%20to%0AVA%20processes.%20The%20descriptive%20power%20of%20the%20HDMI%20Canvas%20eases%20the%0Adifferentiation%20between%20a%20series%20of%20VA%20building%20blocks%2C%20rather%20than%20describing%0Ageneral%20VA%20principles%20only.%20The%20canvas%20includes%20modern%20human-centered%0Amethodologies%2C%20including%20human%20knowledge%20externalization%20and%20forms%20of%20feedback%0Aloops%2C%20while%20interpretable%20and%20explainable%20AI%20highlight%20model%20contributions%0Abeyond%20their%20conventional%20outputs.%20The%20HDMI%20Canvas%20has%20generative%20power%2C%0Aguiding%20the%20design%20of%20new%20VA%20processes%20and%20is%20optimized%20for%20external%0Astakeholders%2C%20improving%20VA%20outreach%2C%20interdisciplinary%20collaboration%2C%20and%0Auser-centered%20design.%20The%20utility%20of%20the%20HDMI%20Canvas%20is%20demonstrated%20through%0Atwo%20preliminary%20case%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Human-Data-Model%2520Interaction%2520Canvas%2520for%2520Visual%2520Analytics%26entry.906535625%3DJ%25C3%25BCrgen%2520Bernard%26entry.1292438233%3D%2520%2520Visual%2520Analytics%2520%2528VA%2529%2520integrates%2520humans%252C%2520data%252C%2520and%2520models%2520as%2520key%2520actors%2520in%250Ainsight%2520generation%2520and%2520data-driven%2520decision-making.%2520This%2520position%2520paper%2520values%250Aand%2520reflects%2520on%252016%2520VA%2520process%2520models%2520and%2520frameworks%2520and%2520makes%2520nine%2520high-level%250Aobservations%2520that%2520motivate%2520a%2520fresh%2520perspective%2520on%2520VA.%2520The%2520contribution%2520is%2520the%250AHDMI%2520Canvas%252C%2520a%2520perspective%2520to%2520VA%2520that%2520complements%2520the%2520strengths%2520of%2520existing%2520VA%250Aprocess%2520models%2520and%2520frameworks.%2520It%2520systematically%2520characterizes%2520diverse%2520roles%2520of%250Ahumans%252C%2520data%252C%2520and%2520models%252C%2520and%2520how%2520these%2520actors%2520benefit%2520from%2520and%2520contribute%2520to%250AVA%2520processes.%2520The%2520descriptive%2520power%2520of%2520the%2520HDMI%2520Canvas%2520eases%2520the%250Adifferentiation%2520between%2520a%2520series%2520of%2520VA%2520building%2520blocks%252C%2520rather%2520than%2520describing%250Ageneral%2520VA%2520principles%2520only.%2520The%2520canvas%2520includes%2520modern%2520human-centered%250Amethodologies%252C%2520including%2520human%2520knowledge%2520externalization%2520and%2520forms%2520of%2520feedback%250Aloops%252C%2520while%2520interpretable%2520and%2520explainable%2520AI%2520highlight%2520model%2520contributions%250Abeyond%2520their%2520conventional%2520outputs.%2520The%2520HDMI%2520Canvas%2520has%2520generative%2520power%252C%250Aguiding%2520the%2520design%2520of%2520new%2520VA%2520processes%2520and%2520is%2520optimized%2520for%2520external%250Astakeholders%252C%2520improving%2520VA%2520outreach%252C%2520interdisciplinary%2520collaboration%252C%2520and%250Auser-centered%2520design.%2520The%2520utility%2520of%2520the%2520HDMI%2520Canvas%2520is%2520demonstrated%2520through%250Atwo%2520preliminary%2520case%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Human-Data-Model%20Interaction%20Canvas%20for%20Visual%20Analytics&entry.906535625=J%C3%BCrgen%20Bernard&entry.1292438233=%20%20Visual%20Analytics%20%28VA%29%20integrates%20humans%2C%20data%2C%20and%20models%20as%20key%20actors%20in%0Ainsight%20generation%20and%20data-driven%20decision-making.%20This%20position%20paper%20values%0Aand%20reflects%20on%2016%20VA%20process%20models%20and%20frameworks%20and%20makes%20nine%20high-level%0Aobservations%20that%20motivate%20a%20fresh%20perspective%20on%20VA.%20The%20contribution%20is%20the%0AHDMI%20Canvas%2C%20a%20perspective%20to%20VA%20that%20complements%20the%20strengths%20of%20existing%20VA%0Aprocess%20models%20and%20frameworks.%20It%20systematically%20characterizes%20diverse%20roles%20of%0Ahumans%2C%20data%2C%20and%20models%2C%20and%20how%20these%20actors%20benefit%20from%20and%20contribute%20to%0AVA%20processes.%20The%20descriptive%20power%20of%20the%20HDMI%20Canvas%20eases%20the%0Adifferentiation%20between%20a%20series%20of%20VA%20building%20blocks%2C%20rather%20than%20describing%0Ageneral%20VA%20principles%20only.%20The%20canvas%20includes%20modern%20human-centered%0Amethodologies%2C%20including%20human%20knowledge%20externalization%20and%20forms%20of%20feedback%0Aloops%2C%20while%20interpretable%20and%20explainable%20AI%20highlight%20model%20contributions%0Abeyond%20their%20conventional%20outputs.%20The%20HDMI%20Canvas%20has%20generative%20power%2C%0Aguiding%20the%20design%20of%20new%20VA%20processes%20and%20is%20optimized%20for%20external%0Astakeholders%2C%20improving%20VA%20outreach%2C%20interdisciplinary%20collaboration%2C%20and%0Auser-centered%20design.%20The%20utility%20of%20the%20HDMI%20Canvas%20is%20demonstrated%20through%0Atwo%20preliminary%20case%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07534v1&entry.124074799=Read"},
{"title": "Anatomical Attention Alignment representation for Radiology Report\n  Generation", "author": "Quang Vinh Nguyen and Minh Duc Nguyen and Thanh Hoang Son Vo and Hyung-Jeong Yang and Soo-Hyung Kim", "abstract": "  Automated Radiology report generation (RRG) aims at producing detailed\ndescriptions of medical images, reducing radiologists' workload and improving\naccess to high-quality diagnostic services. Existing encoder-decoder models\nonly rely on visual features extracted from raw input images, which can limit\nthe understanding of spatial structures and semantic relationships, often\nresulting in suboptimal text generation. To address this, we propose Anatomical\nAttention Alignment Network (A3Net), a framework that enhance visual-textual\nunderstanding by constructing hyper-visual representations. Our approach\nintegrates a knowledge dictionary of anatomical structures with patch-level\nvisual features, enabling the model to effectively associate image regions with\ntheir corresponding anatomical entities. This structured representation\nimproves semantic reasoning, interpretability, and cross-modal alignment,\nultimately enhancing the accuracy and clinical relevance of generated reports.\nExperimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net\nsignificantly improves both visual perception and text generation quality. Our\ncode is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}.\n", "link": "http://arxiv.org/abs/2505.07689v1", "date": "2025-05-12", "relevancy": 2.5211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomical%20Attention%20Alignment%20representation%20for%20Radiology%20Report%0A%20%20Generation&body=Title%3A%20Anatomical%20Attention%20Alignment%20representation%20for%20Radiology%20Report%0A%20%20Generation%0AAuthor%3A%20Quang%20Vinh%20Nguyen%20and%20Minh%20Duc%20Nguyen%20and%20Thanh%20Hoang%20Son%20Vo%20and%20Hyung-Jeong%20Yang%20and%20Soo-Hyung%20Kim%0AAbstract%3A%20%20%20Automated%20Radiology%20report%20generation%20%28RRG%29%20aims%20at%20producing%20detailed%0Adescriptions%20of%20medical%20images%2C%20reducing%20radiologists%27%20workload%20and%20improving%0Aaccess%20to%20high-quality%20diagnostic%20services.%20Existing%20encoder-decoder%20models%0Aonly%20rely%20on%20visual%20features%20extracted%20from%20raw%20input%20images%2C%20which%20can%20limit%0Athe%20understanding%20of%20spatial%20structures%20and%20semantic%20relationships%2C%20often%0Aresulting%20in%20suboptimal%20text%20generation.%20To%20address%20this%2C%20we%20propose%20Anatomical%0AAttention%20Alignment%20Network%20%28A3Net%29%2C%20a%20framework%20that%20enhance%20visual-textual%0Aunderstanding%20by%20constructing%20hyper-visual%20representations.%20Our%20approach%0Aintegrates%20a%20knowledge%20dictionary%20of%20anatomical%20structures%20with%20patch-level%0Avisual%20features%2C%20enabling%20the%20model%20to%20effectively%20associate%20image%20regions%20with%0Atheir%20corresponding%20anatomical%20entities.%20This%20structured%20representation%0Aimproves%20semantic%20reasoning%2C%20interpretability%2C%20and%20cross-modal%20alignment%2C%0Aultimately%20enhancing%20the%20accuracy%20and%20clinical%20relevance%20of%20generated%20reports.%0AExperimental%20results%20on%20IU%20X-Ray%20and%20MIMIC-CXR%20datasets%20demonstrate%20that%20A3Net%0Asignificantly%20improves%20both%20visual%20perception%20and%20text%20generation%20quality.%20Our%0Acode%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Vinh-AI/A3Net%7D%7BGitHub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomical%2520Attention%2520Alignment%2520representation%2520for%2520Radiology%2520Report%250A%2520%2520Generation%26entry.906535625%3DQuang%2520Vinh%2520Nguyen%2520and%2520Minh%2520Duc%2520Nguyen%2520and%2520Thanh%2520Hoang%2520Son%2520Vo%2520and%2520Hyung-Jeong%2520Yang%2520and%2520Soo-Hyung%2520Kim%26entry.1292438233%3D%2520%2520Automated%2520Radiology%2520report%2520generation%2520%2528RRG%2529%2520aims%2520at%2520producing%2520detailed%250Adescriptions%2520of%2520medical%2520images%252C%2520reducing%2520radiologists%2527%2520workload%2520and%2520improving%250Aaccess%2520to%2520high-quality%2520diagnostic%2520services.%2520Existing%2520encoder-decoder%2520models%250Aonly%2520rely%2520on%2520visual%2520features%2520extracted%2520from%2520raw%2520input%2520images%252C%2520which%2520can%2520limit%250Athe%2520understanding%2520of%2520spatial%2520structures%2520and%2520semantic%2520relationships%252C%2520often%250Aresulting%2520in%2520suboptimal%2520text%2520generation.%2520To%2520address%2520this%252C%2520we%2520propose%2520Anatomical%250AAttention%2520Alignment%2520Network%2520%2528A3Net%2529%252C%2520a%2520framework%2520that%2520enhance%2520visual-textual%250Aunderstanding%2520by%2520constructing%2520hyper-visual%2520representations.%2520Our%2520approach%250Aintegrates%2520a%2520knowledge%2520dictionary%2520of%2520anatomical%2520structures%2520with%2520patch-level%250Avisual%2520features%252C%2520enabling%2520the%2520model%2520to%2520effectively%2520associate%2520image%2520regions%2520with%250Atheir%2520corresponding%2520anatomical%2520entities.%2520This%2520structured%2520representation%250Aimproves%2520semantic%2520reasoning%252C%2520interpretability%252C%2520and%2520cross-modal%2520alignment%252C%250Aultimately%2520enhancing%2520the%2520accuracy%2520and%2520clinical%2520relevance%2520of%2520generated%2520reports.%250AExperimental%2520results%2520on%2520IU%2520X-Ray%2520and%2520MIMIC-CXR%2520datasets%2520demonstrate%2520that%2520A3Net%250Asignificantly%2520improves%2520both%2520visual%2520perception%2520and%2520text%2520generation%2520quality.%2520Our%250Acode%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/Vinh-AI/A3Net%257D%257BGitHub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomical%20Attention%20Alignment%20representation%20for%20Radiology%20Report%0A%20%20Generation&entry.906535625=Quang%20Vinh%20Nguyen%20and%20Minh%20Duc%20Nguyen%20and%20Thanh%20Hoang%20Son%20Vo%20and%20Hyung-Jeong%20Yang%20and%20Soo-Hyung%20Kim&entry.1292438233=%20%20Automated%20Radiology%20report%20generation%20%28RRG%29%20aims%20at%20producing%20detailed%0Adescriptions%20of%20medical%20images%2C%20reducing%20radiologists%27%20workload%20and%20improving%0Aaccess%20to%20high-quality%20diagnostic%20services.%20Existing%20encoder-decoder%20models%0Aonly%20rely%20on%20visual%20features%20extracted%20from%20raw%20input%20images%2C%20which%20can%20limit%0Athe%20understanding%20of%20spatial%20structures%20and%20semantic%20relationships%2C%20often%0Aresulting%20in%20suboptimal%20text%20generation.%20To%20address%20this%2C%20we%20propose%20Anatomical%0AAttention%20Alignment%20Network%20%28A3Net%29%2C%20a%20framework%20that%20enhance%20visual-textual%0Aunderstanding%20by%20constructing%20hyper-visual%20representations.%20Our%20approach%0Aintegrates%20a%20knowledge%20dictionary%20of%20anatomical%20structures%20with%20patch-level%0Avisual%20features%2C%20enabling%20the%20model%20to%20effectively%20associate%20image%20regions%20with%0Atheir%20corresponding%20anatomical%20entities.%20This%20structured%20representation%0Aimproves%20semantic%20reasoning%2C%20interpretability%2C%20and%20cross-modal%20alignment%2C%0Aultimately%20enhancing%20the%20accuracy%20and%20clinical%20relevance%20of%20generated%20reports.%0AExperimental%20results%20on%20IU%20X-Ray%20and%20MIMIC-CXR%20datasets%20demonstrate%20that%20A3Net%0Asignificantly%20improves%20both%20visual%20perception%20and%20text%20generation%20quality.%20Our%0Acode%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Vinh-AI/A3Net%7D%7BGitHub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07689v1&entry.124074799=Read"},
{"title": "TableCenterNet: A one-stage network for table structure recognition", "author": "Anyi Xiao and Cihui Yang", "abstract": "  Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.\n", "link": "http://arxiv.org/abs/2504.17522v2", "date": "2025-05-12", "relevancy": 2.4941, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5047}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TableCenterNet%3A%20A%20one-stage%20network%20for%20table%20structure%20recognition&body=Title%3A%20TableCenterNet%3A%20A%20one-stage%20network%20for%20table%20structure%20recognition%0AAuthor%3A%20Anyi%20Xiao%20and%20Cihui%20Yang%0AAbstract%3A%20%20%20Table%20structure%20recognition%20aims%20to%20parse%20tables%20in%20unstructured%20data%20into%0Amachine-understandable%20formats.%20Recent%20methods%20address%20this%20problem%20through%20a%0Atwo-stage%20process%20or%20optimized%20one-stage%20approaches.%20However%2C%20these%20methods%0Aeither%20require%20multiple%20networks%20to%20be%20serially%20trained%20and%20perform%20more%0Atime-consuming%20sequential%20decoding%2C%20or%20rely%20on%20complex%20post-processing%0Aalgorithms%20to%20parse%20the%20logical%20structure%20of%20tables.%20They%20struggle%20to%20balance%0Across-scenario%20adaptability%2C%20robustness%2C%20and%20computational%20efficiency.%20In%20this%0Apaper%2C%20we%20propose%20a%20one-stage%20end-to-end%20table%20structure%20parsing%20network%20called%0ATableCenterNet.%20This%20network%20unifies%20the%20prediction%20of%20table%20spatial%20and%0Alogical%20structure%20into%20a%20parallel%20regression%20task%20for%20the%20first%20time%2C%20and%0Aimplicitly%20learns%20the%20spatial-logical%20location%20mapping%20laws%20of%20cells%20through%20a%0Asynergistic%20architecture%20of%20shared%20feature%20extraction%20layers%20and%20task-specific%0Adecoding.%20Compared%20with%20two-stage%20methods%2C%20our%20method%20is%20easier%20to%20train%20and%0Afaster%20to%20infer.%20Experiments%20on%20benchmark%20datasets%20show%20that%20TableCenterNet%20can%0Aeffectively%20parse%20table%20structures%20in%20diverse%20scenarios%20and%20achieve%0Astate-of-the-art%20performance%20on%20the%20TableGraph-24k%20dataset.%20Code%20is%20available%0Aat%20https%3A//github.com/dreamy-xay/TableCenterNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTableCenterNet%253A%2520A%2520one-stage%2520network%2520for%2520table%2520structure%2520recognition%26entry.906535625%3DAnyi%2520Xiao%2520and%2520Cihui%2520Yang%26entry.1292438233%3D%2520%2520Table%2520structure%2520recognition%2520aims%2520to%2520parse%2520tables%2520in%2520unstructured%2520data%2520into%250Amachine-understandable%2520formats.%2520Recent%2520methods%2520address%2520this%2520problem%2520through%2520a%250Atwo-stage%2520process%2520or%2520optimized%2520one-stage%2520approaches.%2520However%252C%2520these%2520methods%250Aeither%2520require%2520multiple%2520networks%2520to%2520be%2520serially%2520trained%2520and%2520perform%2520more%250Atime-consuming%2520sequential%2520decoding%252C%2520or%2520rely%2520on%2520complex%2520post-processing%250Aalgorithms%2520to%2520parse%2520the%2520logical%2520structure%2520of%2520tables.%2520They%2520struggle%2520to%2520balance%250Across-scenario%2520adaptability%252C%2520robustness%252C%2520and%2520computational%2520efficiency.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520one-stage%2520end-to-end%2520table%2520structure%2520parsing%2520network%2520called%250ATableCenterNet.%2520This%2520network%2520unifies%2520the%2520prediction%2520of%2520table%2520spatial%2520and%250Alogical%2520structure%2520into%2520a%2520parallel%2520regression%2520task%2520for%2520the%2520first%2520time%252C%2520and%250Aimplicitly%2520learns%2520the%2520spatial-logical%2520location%2520mapping%2520laws%2520of%2520cells%2520through%2520a%250Asynergistic%2520architecture%2520of%2520shared%2520feature%2520extraction%2520layers%2520and%2520task-specific%250Adecoding.%2520Compared%2520with%2520two-stage%2520methods%252C%2520our%2520method%2520is%2520easier%2520to%2520train%2520and%250Afaster%2520to%2520infer.%2520Experiments%2520on%2520benchmark%2520datasets%2520show%2520that%2520TableCenterNet%2520can%250Aeffectively%2520parse%2520table%2520structures%2520in%2520diverse%2520scenarios%2520and%2520achieve%250Astate-of-the-art%2520performance%2520on%2520the%2520TableGraph-24k%2520dataset.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/dreamy-xay/TableCenterNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TableCenterNet%3A%20A%20one-stage%20network%20for%20table%20structure%20recognition&entry.906535625=Anyi%20Xiao%20and%20Cihui%20Yang&entry.1292438233=%20%20Table%20structure%20recognition%20aims%20to%20parse%20tables%20in%20unstructured%20data%20into%0Amachine-understandable%20formats.%20Recent%20methods%20address%20this%20problem%20through%20a%0Atwo-stage%20process%20or%20optimized%20one-stage%20approaches.%20However%2C%20these%20methods%0Aeither%20require%20multiple%20networks%20to%20be%20serially%20trained%20and%20perform%20more%0Atime-consuming%20sequential%20decoding%2C%20or%20rely%20on%20complex%20post-processing%0Aalgorithms%20to%20parse%20the%20logical%20structure%20of%20tables.%20They%20struggle%20to%20balance%0Across-scenario%20adaptability%2C%20robustness%2C%20and%20computational%20efficiency.%20In%20this%0Apaper%2C%20we%20propose%20a%20one-stage%20end-to-end%20table%20structure%20parsing%20network%20called%0ATableCenterNet.%20This%20network%20unifies%20the%20prediction%20of%20table%20spatial%20and%0Alogical%20structure%20into%20a%20parallel%20regression%20task%20for%20the%20first%20time%2C%20and%0Aimplicitly%20learns%20the%20spatial-logical%20location%20mapping%20laws%20of%20cells%20through%20a%0Asynergistic%20architecture%20of%20shared%20feature%20extraction%20layers%20and%20task-specific%0Adecoding.%20Compared%20with%20two-stage%20methods%2C%20our%20method%20is%20easier%20to%20train%20and%0Afaster%20to%20infer.%20Experiments%20on%20benchmark%20datasets%20show%20that%20TableCenterNet%20can%0Aeffectively%20parse%20table%20structures%20in%20diverse%20scenarios%20and%20achieve%0Astate-of-the-art%20performance%20on%20the%20TableGraph-24k%20dataset.%20Code%20is%20available%0Aat%20https%3A//github.com/dreamy-xay/TableCenterNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17522v2&entry.124074799=Read"},
{"title": "A Survey on Collaborative Mechanisms Between Large and Small Language\n  Models", "author": "Yi Chen and JiaHao Zhao and HaoHao Han", "abstract": "  Large Language Models (LLMs) deliver powerful AI capabilities but face\ndeployment challenges due to high resource costs and latency, whereas Small\nLanguage Models (SLMs) offer efficiency and deployability at the cost of\nreduced performance. Collaboration between LLMs and SLMs emerges as a crucial\nparadigm to synergistically balance these trade-offs, enabling advanced AI\napplications, especially on resource-constrained edge devices. This survey\nprovides a comprehensive overview of LLM-SLM collaboration, detailing various\ninteraction mechanisms (pipeline, routing, auxiliary, distillation, fusion),\nkey enabling technologies, and diverse application scenarios driven by\non-device needs like low latency, privacy, personalization, and offline\noperation. While highlighting the significant potential for creating more\nefficient, adaptable, and accessible AI, we also discuss persistent challenges\nincluding system overhead, inter-model consistency, robust task allocation,\nevaluation complexity, and security/privacy concerns. Future directions point\ntowards more intelligent adaptive frameworks, deeper model fusion, and\nexpansion into multimodal and embodied AI, positioning LLM-SLM collaboration as\na key driver for the next generation of practical and ubiquitous artificial\nintelligence.\n", "link": "http://arxiv.org/abs/2505.07460v1", "date": "2025-05-12", "relevancy": 2.4665, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Collaborative%20Mechanisms%20Between%20Large%20and%20Small%20Language%0A%20%20Models&body=Title%3A%20A%20Survey%20on%20Collaborative%20Mechanisms%20Between%20Large%20and%20Small%20Language%0A%20%20Models%0AAuthor%3A%20Yi%20Chen%20and%20JiaHao%20Zhao%20and%20HaoHao%20Han%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20deliver%20powerful%20AI%20capabilities%20but%20face%0Adeployment%20challenges%20due%20to%20high%20resource%20costs%20and%20latency%2C%20whereas%20Small%0ALanguage%20Models%20%28SLMs%29%20offer%20efficiency%20and%20deployability%20at%20the%20cost%20of%0Areduced%20performance.%20Collaboration%20between%20LLMs%20and%20SLMs%20emerges%20as%20a%20crucial%0Aparadigm%20to%20synergistically%20balance%20these%20trade-offs%2C%20enabling%20advanced%20AI%0Aapplications%2C%20especially%20on%20resource-constrained%20edge%20devices.%20This%20survey%0Aprovides%20a%20comprehensive%20overview%20of%20LLM-SLM%20collaboration%2C%20detailing%20various%0Ainteraction%20mechanisms%20%28pipeline%2C%20routing%2C%20auxiliary%2C%20distillation%2C%20fusion%29%2C%0Akey%20enabling%20technologies%2C%20and%20diverse%20application%20scenarios%20driven%20by%0Aon-device%20needs%20like%20low%20latency%2C%20privacy%2C%20personalization%2C%20and%20offline%0Aoperation.%20While%20highlighting%20the%20significant%20potential%20for%20creating%20more%0Aefficient%2C%20adaptable%2C%20and%20accessible%20AI%2C%20we%20also%20discuss%20persistent%20challenges%0Aincluding%20system%20overhead%2C%20inter-model%20consistency%2C%20robust%20task%20allocation%2C%0Aevaluation%20complexity%2C%20and%20security/privacy%20concerns.%20Future%20directions%20point%0Atowards%20more%20intelligent%20adaptive%20frameworks%2C%20deeper%20model%20fusion%2C%20and%0Aexpansion%20into%20multimodal%20and%20embodied%20AI%2C%20positioning%20LLM-SLM%20collaboration%20as%0Aa%20key%20driver%20for%20the%20next%20generation%20of%20practical%20and%20ubiquitous%20artificial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Collaborative%2520Mechanisms%2520Between%2520Large%2520and%2520Small%2520Language%250A%2520%2520Models%26entry.906535625%3DYi%2520Chen%2520and%2520JiaHao%2520Zhao%2520and%2520HaoHao%2520Han%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520deliver%2520powerful%2520AI%2520capabilities%2520but%2520face%250Adeployment%2520challenges%2520due%2520to%2520high%2520resource%2520costs%2520and%2520latency%252C%2520whereas%2520Small%250ALanguage%2520Models%2520%2528SLMs%2529%2520offer%2520efficiency%2520and%2520deployability%2520at%2520the%2520cost%2520of%250Areduced%2520performance.%2520Collaboration%2520between%2520LLMs%2520and%2520SLMs%2520emerges%2520as%2520a%2520crucial%250Aparadigm%2520to%2520synergistically%2520balance%2520these%2520trade-offs%252C%2520enabling%2520advanced%2520AI%250Aapplications%252C%2520especially%2520on%2520resource-constrained%2520edge%2520devices.%2520This%2520survey%250Aprovides%2520a%2520comprehensive%2520overview%2520of%2520LLM-SLM%2520collaboration%252C%2520detailing%2520various%250Ainteraction%2520mechanisms%2520%2528pipeline%252C%2520routing%252C%2520auxiliary%252C%2520distillation%252C%2520fusion%2529%252C%250Akey%2520enabling%2520technologies%252C%2520and%2520diverse%2520application%2520scenarios%2520driven%2520by%250Aon-device%2520needs%2520like%2520low%2520latency%252C%2520privacy%252C%2520personalization%252C%2520and%2520offline%250Aoperation.%2520While%2520highlighting%2520the%2520significant%2520potential%2520for%2520creating%2520more%250Aefficient%252C%2520adaptable%252C%2520and%2520accessible%2520AI%252C%2520we%2520also%2520discuss%2520persistent%2520challenges%250Aincluding%2520system%2520overhead%252C%2520inter-model%2520consistency%252C%2520robust%2520task%2520allocation%252C%250Aevaluation%2520complexity%252C%2520and%2520security/privacy%2520concerns.%2520Future%2520directions%2520point%250Atowards%2520more%2520intelligent%2520adaptive%2520frameworks%252C%2520deeper%2520model%2520fusion%252C%2520and%250Aexpansion%2520into%2520multimodal%2520and%2520embodied%2520AI%252C%2520positioning%2520LLM-SLM%2520collaboration%2520as%250Aa%2520key%2520driver%2520for%2520the%2520next%2520generation%2520of%2520practical%2520and%2520ubiquitous%2520artificial%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Collaborative%20Mechanisms%20Between%20Large%20and%20Small%20Language%0A%20%20Models&entry.906535625=Yi%20Chen%20and%20JiaHao%20Zhao%20and%20HaoHao%20Han&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20deliver%20powerful%20AI%20capabilities%20but%20face%0Adeployment%20challenges%20due%20to%20high%20resource%20costs%20and%20latency%2C%20whereas%20Small%0ALanguage%20Models%20%28SLMs%29%20offer%20efficiency%20and%20deployability%20at%20the%20cost%20of%0Areduced%20performance.%20Collaboration%20between%20LLMs%20and%20SLMs%20emerges%20as%20a%20crucial%0Aparadigm%20to%20synergistically%20balance%20these%20trade-offs%2C%20enabling%20advanced%20AI%0Aapplications%2C%20especially%20on%20resource-constrained%20edge%20devices.%20This%20survey%0Aprovides%20a%20comprehensive%20overview%20of%20LLM-SLM%20collaboration%2C%20detailing%20various%0Ainteraction%20mechanisms%20%28pipeline%2C%20routing%2C%20auxiliary%2C%20distillation%2C%20fusion%29%2C%0Akey%20enabling%20technologies%2C%20and%20diverse%20application%20scenarios%20driven%20by%0Aon-device%20needs%20like%20low%20latency%2C%20privacy%2C%20personalization%2C%20and%20offline%0Aoperation.%20While%20highlighting%20the%20significant%20potential%20for%20creating%20more%0Aefficient%2C%20adaptable%2C%20and%20accessible%20AI%2C%20we%20also%20discuss%20persistent%20challenges%0Aincluding%20system%20overhead%2C%20inter-model%20consistency%2C%20robust%20task%20allocation%2C%0Aevaluation%20complexity%2C%20and%20security/privacy%20concerns.%20Future%20directions%20point%0Atowards%20more%20intelligent%20adaptive%20frameworks%2C%20deeper%20model%20fusion%2C%20and%0Aexpansion%20into%20multimodal%20and%20embodied%20AI%2C%20positioning%20LLM-SLM%20collaboration%20as%0Aa%20key%20driver%20for%20the%20next%20generation%20of%20practical%20and%20ubiquitous%20artificial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07460v1&entry.124074799=Read"},
{"title": "Gaussian entropic optimal transport: Schr\u00f6dinger bridges and the\n  Sinkhorn algorithm", "author": "O. Deniz Akyildiz and Pierre Del Moral and Joaqu\u00edn Miguez", "abstract": "  Entropic optimal transport problems are regularized versions of optimal\ntransport problems. These models play an increasingly important role in machine\nlearning and generative modelling. For finite spaces, these problems are\ncommonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting\nprocedure). However, in more general settings the Sinkhorn iterations are based\non nonlinear conditional/conjugate transformations and exact finite-dimensional\nsolutions cannot be computed.\n  This article presents a finite-dimensional recursive formulation of the\niterative proportional fitting procedure for general Gaussian multivariate\nmodels. As expected, this recursive formulation is closely related to the\ncelebrated Kalman filter and related Riccati matrix difference equations, and\nit yields algorithms that can be implemented in practical settings without\nfurther approximations. We extend this filtering methodology to develop a\nrefined and self-contained convergence analysis of Gaussian Sinkhorn\nalgorithms, including closed form expressions of entropic transport maps and\nSchr\\\"odinger bridges.\n", "link": "http://arxiv.org/abs/2412.18432v4", "date": "2025-05-12", "relevancy": 2.4556, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5127}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4857}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm&body=Title%3A%20Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm%0AAuthor%3A%20O.%20Deniz%20Akyildiz%20and%20Pierre%20Del%20Moral%20and%20Joaqu%C3%ADn%20Miguez%0AAbstract%3A%20%20%20Entropic%20optimal%20transport%20problems%20are%20regularized%20versions%20of%20optimal%0Atransport%20problems.%20These%20models%20play%20an%20increasingly%20important%20role%20in%20machine%0Alearning%20and%20generative%20modelling.%20For%20finite%20spaces%2C%20these%20problems%20are%0Acommonly%20solved%20using%20Sinkhorn%20algorithm%20%28a.k.a.%20iterative%20proportional%20fitting%0Aprocedure%29.%20However%2C%20in%20more%20general%20settings%20the%20Sinkhorn%20iterations%20are%20based%0Aon%20nonlinear%20conditional/conjugate%20transformations%20and%20exact%20finite-dimensional%0Asolutions%20cannot%20be%20computed.%0A%20%20This%20article%20presents%20a%20finite-dimensional%20recursive%20formulation%20of%20the%0Aiterative%20proportional%20fitting%20procedure%20for%20general%20Gaussian%20multivariate%0Amodels.%20As%20expected%2C%20this%20recursive%20formulation%20is%20closely%20related%20to%20the%0Acelebrated%20Kalman%20filter%20and%20related%20Riccati%20matrix%20difference%20equations%2C%20and%0Ait%20yields%20algorithms%20that%20can%20be%20implemented%20in%20practical%20settings%20without%0Afurther%20approximations.%20We%20extend%20this%20filtering%20methodology%20to%20develop%20a%0Arefined%20and%20self-contained%20convergence%20analysis%20of%20Gaussian%20Sinkhorn%0Aalgorithms%2C%20including%20closed%20form%20expressions%20of%20entropic%20transport%20maps%20and%0ASchr%5C%22odinger%20bridges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18432v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520entropic%2520optimal%2520transport%253A%2520Schr%25C3%25B6dinger%2520bridges%2520and%2520the%250A%2520%2520Sinkhorn%2520algorithm%26entry.906535625%3DO.%2520Deniz%2520Akyildiz%2520and%2520Pierre%2520Del%2520Moral%2520and%2520Joaqu%25C3%25ADn%2520Miguez%26entry.1292438233%3D%2520%2520Entropic%2520optimal%2520transport%2520problems%2520are%2520regularized%2520versions%2520of%2520optimal%250Atransport%2520problems.%2520These%2520models%2520play%2520an%2520increasingly%2520important%2520role%2520in%2520machine%250Alearning%2520and%2520generative%2520modelling.%2520For%2520finite%2520spaces%252C%2520these%2520problems%2520are%250Acommonly%2520solved%2520using%2520Sinkhorn%2520algorithm%2520%2528a.k.a.%2520iterative%2520proportional%2520fitting%250Aprocedure%2529.%2520However%252C%2520in%2520more%2520general%2520settings%2520the%2520Sinkhorn%2520iterations%2520are%2520based%250Aon%2520nonlinear%2520conditional/conjugate%2520transformations%2520and%2520exact%2520finite-dimensional%250Asolutions%2520cannot%2520be%2520computed.%250A%2520%2520This%2520article%2520presents%2520a%2520finite-dimensional%2520recursive%2520formulation%2520of%2520the%250Aiterative%2520proportional%2520fitting%2520procedure%2520for%2520general%2520Gaussian%2520multivariate%250Amodels.%2520As%2520expected%252C%2520this%2520recursive%2520formulation%2520is%2520closely%2520related%2520to%2520the%250Acelebrated%2520Kalman%2520filter%2520and%2520related%2520Riccati%2520matrix%2520difference%2520equations%252C%2520and%250Ait%2520yields%2520algorithms%2520that%2520can%2520be%2520implemented%2520in%2520practical%2520settings%2520without%250Afurther%2520approximations.%2520We%2520extend%2520this%2520filtering%2520methodology%2520to%2520develop%2520a%250Arefined%2520and%2520self-contained%2520convergence%2520analysis%2520of%2520Gaussian%2520Sinkhorn%250Aalgorithms%252C%2520including%2520closed%2520form%2520expressions%2520of%2520entropic%2520transport%2520maps%2520and%250ASchr%255C%2522odinger%2520bridges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18432v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20entropic%20optimal%20transport%3A%20Schr%C3%B6dinger%20bridges%20and%20the%0A%20%20Sinkhorn%20algorithm&entry.906535625=O.%20Deniz%20Akyildiz%20and%20Pierre%20Del%20Moral%20and%20Joaqu%C3%ADn%20Miguez&entry.1292438233=%20%20Entropic%20optimal%20transport%20problems%20are%20regularized%20versions%20of%20optimal%0Atransport%20problems.%20These%20models%20play%20an%20increasingly%20important%20role%20in%20machine%0Alearning%20and%20generative%20modelling.%20For%20finite%20spaces%2C%20these%20problems%20are%0Acommonly%20solved%20using%20Sinkhorn%20algorithm%20%28a.k.a.%20iterative%20proportional%20fitting%0Aprocedure%29.%20However%2C%20in%20more%20general%20settings%20the%20Sinkhorn%20iterations%20are%20based%0Aon%20nonlinear%20conditional/conjugate%20transformations%20and%20exact%20finite-dimensional%0Asolutions%20cannot%20be%20computed.%0A%20%20This%20article%20presents%20a%20finite-dimensional%20recursive%20formulation%20of%20the%0Aiterative%20proportional%20fitting%20procedure%20for%20general%20Gaussian%20multivariate%0Amodels.%20As%20expected%2C%20this%20recursive%20formulation%20is%20closely%20related%20to%20the%0Acelebrated%20Kalman%20filter%20and%20related%20Riccati%20matrix%20difference%20equations%2C%20and%0Ait%20yields%20algorithms%20that%20can%20be%20implemented%20in%20practical%20settings%20without%0Afurther%20approximations.%20We%20extend%20this%20filtering%20methodology%20to%20develop%20a%0Arefined%20and%20self-contained%20convergence%20analysis%20of%20Gaussian%20Sinkhorn%0Aalgorithms%2C%20including%20closed%20form%20expressions%20of%20entropic%20transport%20maps%20and%0ASchr%5C%22odinger%20bridges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18432v4&entry.124074799=Read"},
{"title": "Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual\n  Reinforcement Fine-Tuning", "author": "Ming Li and Jike Zhong and Shitian Zhao and Yuxiang Lai and Haoquan Zhang and Wang Bill Zhu and Kaipeng Zhang", "abstract": "  This paper investigates the role of explicit thinking process in rule-based\nreinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM\nimage classification, using verifiable rewards for fine-tuning. Experiments\nshow CLS-RL significantly outperforms SFT and yields a cross-dataset\ngeneralization effect. We then rethink and question whether explicit thinking\nin RFT is always necessary. Challenging the convention that explicit thinking\nis crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT\nwithout thinking by introducing a simple equality accuracy reward. We evaluate\nNo-Thinking-RL on 6 diverse tasks across different model sizes and types.\nExperimental results reveal three key findings: 1). Visual perception tasks do\nnot require thinking during RFT, as No-Thinking-RL consistently outperforms or\nmatches Thinking-based RFT across model sizes. 2).} Models with limited\ncapabilities struggle to generate high-quality CoT for RFT, making\nThinking-based RFT less effective than No-Thinking-RL. 3). There are\ninconsistencies between the answers in the thinking and answer tags for some\nresponses of thinking-based RFT, which show lower accuracy than the overall\naccuracy. We hypothesize that explicit thinking before verifiable answers may\nhinder reward convergence and reduce performance. To test this hypothesis, we\npropose Think-After-Answer, which places thinking after the answer to mitigate\nthis effect for experimental verification. Lastly, we conduct a pilot study to\nexplore whether MLLMs can learn when to think during RFT, introducing an\nAdaptive-Thinking method. Experiments show that it converges to a specific\nprompt depending on model capability and task complexity, achieving comparable\nor better performance than both Thinking and No-Thinking-RL. This suggests\nMLLMs can adaptively decide to think or not based on their capabilities and\ntask complexity.\n", "link": "http://arxiv.org/abs/2503.16188v4", "date": "2025-05-12", "relevancy": 2.4533, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20or%20Not%20Think%3A%20A%20Study%20of%20Explicit%20Thinking%20in%20Rule-Based%20Visual%0A%20%20Reinforcement%20Fine-Tuning&body=Title%3A%20Think%20or%20Not%20Think%3A%20A%20Study%20of%20Explicit%20Thinking%20in%20Rule-Based%20Visual%0A%20%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Ming%20Li%20and%20Jike%20Zhong%20and%20Shitian%20Zhao%20and%20Yuxiang%20Lai%20and%20Haoquan%20Zhang%20and%20Wang%20Bill%20Zhu%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20role%20of%20explicit%20thinking%20process%20in%20rule-based%0Areinforcement%20fine-tuning%20%28RFT%29%20for%20MLLMs.%20We%20first%20propose%20CLS-RL%20for%20MLLM%0Aimage%20classification%2C%20using%20verifiable%20rewards%20for%20fine-tuning.%20Experiments%0Ashow%20CLS-RL%20significantly%20outperforms%20SFT%20and%20yields%20a%20cross-dataset%0Ageneralization%20effect.%20We%20then%20rethink%20and%20question%20whether%20explicit%20thinking%0Ain%20RFT%20is%20always%20necessary.%20Challenging%20the%20convention%20that%20explicit%20thinking%0Ais%20crucial%20for%20the%20success%20of%20RFT%2C%20we%20introduce%20No-Thinking-RL%2C%20exploring%20RFT%0Awithout%20thinking%20by%20introducing%20a%20simple%20equality%20accuracy%20reward.%20We%20evaluate%0ANo-Thinking-RL%20on%206%20diverse%20tasks%20across%20different%20model%20sizes%20and%20types.%0AExperimental%20results%20reveal%20three%20key%20findings%3A%201%29.%20Visual%20perception%20tasks%20do%0Anot%20require%20thinking%20during%20RFT%2C%20as%20No-Thinking-RL%20consistently%20outperforms%20or%0Amatches%20Thinking-based%20RFT%20across%20model%20sizes.%202%29.%7D%20Models%20with%20limited%0Acapabilities%20struggle%20to%20generate%20high-quality%20CoT%20for%20RFT%2C%20making%0AThinking-based%20RFT%20less%20effective%20than%20No-Thinking-RL.%203%29.%20There%20are%0Ainconsistencies%20between%20the%20answers%20in%20the%20thinking%20and%20answer%20tags%20for%20some%0Aresponses%20of%20thinking-based%20RFT%2C%20which%20show%20lower%20accuracy%20than%20the%20overall%0Aaccuracy.%20We%20hypothesize%20that%20explicit%20thinking%20before%20verifiable%20answers%20may%0Ahinder%20reward%20convergence%20and%20reduce%20performance.%20To%20test%20this%20hypothesis%2C%20we%0Apropose%20Think-After-Answer%2C%20which%20places%20thinking%20after%20the%20answer%20to%20mitigate%0Athis%20effect%20for%20experimental%20verification.%20Lastly%2C%20we%20conduct%20a%20pilot%20study%20to%0Aexplore%20whether%20MLLMs%20can%20learn%20when%20to%20think%20during%20RFT%2C%20introducing%20an%0AAdaptive-Thinking%20method.%20Experiments%20show%20that%20it%20converges%20to%20a%20specific%0Aprompt%20depending%20on%20model%20capability%20and%20task%20complexity%2C%20achieving%20comparable%0Aor%20better%20performance%20than%20both%20Thinking%20and%20No-Thinking-RL.%20This%20suggests%0AMLLMs%20can%20adaptively%20decide%20to%20think%20or%20not%20based%20on%20their%20capabilities%20and%0Atask%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16188v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520or%2520Not%2520Think%253A%2520A%2520Study%2520of%2520Explicit%2520Thinking%2520in%2520Rule-Based%2520Visual%250A%2520%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DMing%2520Li%2520and%2520Jike%2520Zhong%2520and%2520Shitian%2520Zhao%2520and%2520Yuxiang%2520Lai%2520and%2520Haoquan%2520Zhang%2520and%2520Wang%2520Bill%2520Zhu%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520role%2520of%2520explicit%2520thinking%2520process%2520in%2520rule-based%250Areinforcement%2520fine-tuning%2520%2528RFT%2529%2520for%2520MLLMs.%2520We%2520first%2520propose%2520CLS-RL%2520for%2520MLLM%250Aimage%2520classification%252C%2520using%2520verifiable%2520rewards%2520for%2520fine-tuning.%2520Experiments%250Ashow%2520CLS-RL%2520significantly%2520outperforms%2520SFT%2520and%2520yields%2520a%2520cross-dataset%250Ageneralization%2520effect.%2520We%2520then%2520rethink%2520and%2520question%2520whether%2520explicit%2520thinking%250Ain%2520RFT%2520is%2520always%2520necessary.%2520Challenging%2520the%2520convention%2520that%2520explicit%2520thinking%250Ais%2520crucial%2520for%2520the%2520success%2520of%2520RFT%252C%2520we%2520introduce%2520No-Thinking-RL%252C%2520exploring%2520RFT%250Awithout%2520thinking%2520by%2520introducing%2520a%2520simple%2520equality%2520accuracy%2520reward.%2520We%2520evaluate%250ANo-Thinking-RL%2520on%25206%2520diverse%2520tasks%2520across%2520different%2520model%2520sizes%2520and%2520types.%250AExperimental%2520results%2520reveal%2520three%2520key%2520findings%253A%25201%2529.%2520Visual%2520perception%2520tasks%2520do%250Anot%2520require%2520thinking%2520during%2520RFT%252C%2520as%2520No-Thinking-RL%2520consistently%2520outperforms%2520or%250Amatches%2520Thinking-based%2520RFT%2520across%2520model%2520sizes.%25202%2529.%257D%2520Models%2520with%2520limited%250Acapabilities%2520struggle%2520to%2520generate%2520high-quality%2520CoT%2520for%2520RFT%252C%2520making%250AThinking-based%2520RFT%2520less%2520effective%2520than%2520No-Thinking-RL.%25203%2529.%2520There%2520are%250Ainconsistencies%2520between%2520the%2520answers%2520in%2520the%2520thinking%2520and%2520answer%2520tags%2520for%2520some%250Aresponses%2520of%2520thinking-based%2520RFT%252C%2520which%2520show%2520lower%2520accuracy%2520than%2520the%2520overall%250Aaccuracy.%2520We%2520hypothesize%2520that%2520explicit%2520thinking%2520before%2520verifiable%2520answers%2520may%250Ahinder%2520reward%2520convergence%2520and%2520reduce%2520performance.%2520To%2520test%2520this%2520hypothesis%252C%2520we%250Apropose%2520Think-After-Answer%252C%2520which%2520places%2520thinking%2520after%2520the%2520answer%2520to%2520mitigate%250Athis%2520effect%2520for%2520experimental%2520verification.%2520Lastly%252C%2520we%2520conduct%2520a%2520pilot%2520study%2520to%250Aexplore%2520whether%2520MLLMs%2520can%2520learn%2520when%2520to%2520think%2520during%2520RFT%252C%2520introducing%2520an%250AAdaptive-Thinking%2520method.%2520Experiments%2520show%2520that%2520it%2520converges%2520to%2520a%2520specific%250Aprompt%2520depending%2520on%2520model%2520capability%2520and%2520task%2520complexity%252C%2520achieving%2520comparable%250Aor%2520better%2520performance%2520than%2520both%2520Thinking%2520and%2520No-Thinking-RL.%2520This%2520suggests%250AMLLMs%2520can%2520adaptively%2520decide%2520to%2520think%2520or%2520not%2520based%2520on%2520their%2520capabilities%2520and%250Atask%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16188v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20or%20Not%20Think%3A%20A%20Study%20of%20Explicit%20Thinking%20in%20Rule-Based%20Visual%0A%20%20Reinforcement%20Fine-Tuning&entry.906535625=Ming%20Li%20and%20Jike%20Zhong%20and%20Shitian%20Zhao%20and%20Yuxiang%20Lai%20and%20Haoquan%20Zhang%20and%20Wang%20Bill%20Zhu%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20This%20paper%20investigates%20the%20role%20of%20explicit%20thinking%20process%20in%20rule-based%0Areinforcement%20fine-tuning%20%28RFT%29%20for%20MLLMs.%20We%20first%20propose%20CLS-RL%20for%20MLLM%0Aimage%20classification%2C%20using%20verifiable%20rewards%20for%20fine-tuning.%20Experiments%0Ashow%20CLS-RL%20significantly%20outperforms%20SFT%20and%20yields%20a%20cross-dataset%0Ageneralization%20effect.%20We%20then%20rethink%20and%20question%20whether%20explicit%20thinking%0Ain%20RFT%20is%20always%20necessary.%20Challenging%20the%20convention%20that%20explicit%20thinking%0Ais%20crucial%20for%20the%20success%20of%20RFT%2C%20we%20introduce%20No-Thinking-RL%2C%20exploring%20RFT%0Awithout%20thinking%20by%20introducing%20a%20simple%20equality%20accuracy%20reward.%20We%20evaluate%0ANo-Thinking-RL%20on%206%20diverse%20tasks%20across%20different%20model%20sizes%20and%20types.%0AExperimental%20results%20reveal%20three%20key%20findings%3A%201%29.%20Visual%20perception%20tasks%20do%0Anot%20require%20thinking%20during%20RFT%2C%20as%20No-Thinking-RL%20consistently%20outperforms%20or%0Amatches%20Thinking-based%20RFT%20across%20model%20sizes.%202%29.%7D%20Models%20with%20limited%0Acapabilities%20struggle%20to%20generate%20high-quality%20CoT%20for%20RFT%2C%20making%0AThinking-based%20RFT%20less%20effective%20than%20No-Thinking-RL.%203%29.%20There%20are%0Ainconsistencies%20between%20the%20answers%20in%20the%20thinking%20and%20answer%20tags%20for%20some%0Aresponses%20of%20thinking-based%20RFT%2C%20which%20show%20lower%20accuracy%20than%20the%20overall%0Aaccuracy.%20We%20hypothesize%20that%20explicit%20thinking%20before%20verifiable%20answers%20may%0Ahinder%20reward%20convergence%20and%20reduce%20performance.%20To%20test%20this%20hypothesis%2C%20we%0Apropose%20Think-After-Answer%2C%20which%20places%20thinking%20after%20the%20answer%20to%20mitigate%0Athis%20effect%20for%20experimental%20verification.%20Lastly%2C%20we%20conduct%20a%20pilot%20study%20to%0Aexplore%20whether%20MLLMs%20can%20learn%20when%20to%20think%20during%20RFT%2C%20introducing%20an%0AAdaptive-Thinking%20method.%20Experiments%20show%20that%20it%20converges%20to%20a%20specific%0Aprompt%20depending%20on%20model%20capability%20and%20task%20complexity%2C%20achieving%20comparable%0Aor%20better%20performance%20than%20both%20Thinking%20and%20No-Thinking-RL.%20This%20suggests%0AMLLMs%20can%20adaptively%20decide%20to%20think%20or%20not%20based%20on%20their%20capabilities%20and%0Atask%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16188v4&entry.124074799=Read"},
{"title": "EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection", "author": "Jing Ren and Mingliang Hou and Zhixuan Liu and Xiaomei Bai", "abstract": "  Graph anomaly detection is a popular and vital task in various real-world\nscenarios, which has been studied for several decades. Recently, many studies\nextending deep learning-based methods have shown preferable performance on\ngraph anomaly detection. However, existing methods are lack of efficiency that\nis definitely necessary for embedded devices. Towards this end, we propose an\nEfficient Anomaly detection model on heterogeneous Graphs via contrastive\nLEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of\ntheir distances to the local context. The proposed method first samples\ninstance pairs on meta path-level for contrastive learning. Then, a graph\nautoencoder-based model is applied to learn informative node embeddings in an\nunsupervised way, which will be further combined with the discriminator to\npredict the anomaly scores of nodes. Experimental results show that EAGLE\noutperforms the state-of-the-art methods on three heterogeneous network\ndatasets.\n", "link": "http://arxiv.org/abs/2505.07508v1", "date": "2025-05-12", "relevancy": 2.4396, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.508}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EAGLE%3A%20Contrastive%20Learning%20for%20Efficient%20Graph%20Anomaly%20Detection&body=Title%3A%20EAGLE%3A%20Contrastive%20Learning%20for%20Efficient%20Graph%20Anomaly%20Detection%0AAuthor%3A%20Jing%20Ren%20and%20Mingliang%20Hou%20and%20Zhixuan%20Liu%20and%20Xiaomei%20Bai%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20is%20a%20popular%20and%20vital%20task%20in%20various%20real-world%0Ascenarios%2C%20which%20has%20been%20studied%20for%20several%20decades.%20Recently%2C%20many%20studies%0Aextending%20deep%20learning-based%20methods%20have%20shown%20preferable%20performance%20on%0Agraph%20anomaly%20detection.%20However%2C%20existing%20methods%20are%20lack%20of%20efficiency%20that%0Ais%20definitely%20necessary%20for%20embedded%20devices.%20Towards%20this%20end%2C%20we%20propose%20an%0AEfficient%20Anomaly%20detection%20model%20on%20heterogeneous%20Graphs%20via%20contrastive%0ALEarning%20%28EAGLE%29%20by%20contrasting%20abnormal%20nodes%20with%20normal%20ones%20in%20terms%20of%0Atheir%20distances%20to%20the%20local%20context.%20The%20proposed%20method%20first%20samples%0Ainstance%20pairs%20on%20meta%20path-level%20for%20contrastive%20learning.%20Then%2C%20a%20graph%0Aautoencoder-based%20model%20is%20applied%20to%20learn%20informative%20node%20embeddings%20in%20an%0Aunsupervised%20way%2C%20which%20will%20be%20further%20combined%20with%20the%20discriminator%20to%0Apredict%20the%20anomaly%20scores%20of%20nodes.%20Experimental%20results%20show%20that%20EAGLE%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20three%20heterogeneous%20network%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEAGLE%253A%2520Contrastive%2520Learning%2520for%2520Efficient%2520Graph%2520Anomaly%2520Detection%26entry.906535625%3DJing%2520Ren%2520and%2520Mingliang%2520Hou%2520and%2520Zhixuan%2520Liu%2520and%2520Xiaomei%2520Bai%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520is%2520a%2520popular%2520and%2520vital%2520task%2520in%2520various%2520real-world%250Ascenarios%252C%2520which%2520has%2520been%2520studied%2520for%2520several%2520decades.%2520Recently%252C%2520many%2520studies%250Aextending%2520deep%2520learning-based%2520methods%2520have%2520shown%2520preferable%2520performance%2520on%250Agraph%2520anomaly%2520detection.%2520However%252C%2520existing%2520methods%2520are%2520lack%2520of%2520efficiency%2520that%250Ais%2520definitely%2520necessary%2520for%2520embedded%2520devices.%2520Towards%2520this%2520end%252C%2520we%2520propose%2520an%250AEfficient%2520Anomaly%2520detection%2520model%2520on%2520heterogeneous%2520Graphs%2520via%2520contrastive%250ALEarning%2520%2528EAGLE%2529%2520by%2520contrasting%2520abnormal%2520nodes%2520with%2520normal%2520ones%2520in%2520terms%2520of%250Atheir%2520distances%2520to%2520the%2520local%2520context.%2520The%2520proposed%2520method%2520first%2520samples%250Ainstance%2520pairs%2520on%2520meta%2520path-level%2520for%2520contrastive%2520learning.%2520Then%252C%2520a%2520graph%250Aautoencoder-based%2520model%2520is%2520applied%2520to%2520learn%2520informative%2520node%2520embeddings%2520in%2520an%250Aunsupervised%2520way%252C%2520which%2520will%2520be%2520further%2520combined%2520with%2520the%2520discriminator%2520to%250Apredict%2520the%2520anomaly%2520scores%2520of%2520nodes.%2520Experimental%2520results%2520show%2520that%2520EAGLE%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520on%2520three%2520heterogeneous%2520network%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAGLE%3A%20Contrastive%20Learning%20for%20Efficient%20Graph%20Anomaly%20Detection&entry.906535625=Jing%20Ren%20and%20Mingliang%20Hou%20and%20Zhixuan%20Liu%20and%20Xiaomei%20Bai&entry.1292438233=%20%20Graph%20anomaly%20detection%20is%20a%20popular%20and%20vital%20task%20in%20various%20real-world%0Ascenarios%2C%20which%20has%20been%20studied%20for%20several%20decades.%20Recently%2C%20many%20studies%0Aextending%20deep%20learning-based%20methods%20have%20shown%20preferable%20performance%20on%0Agraph%20anomaly%20detection.%20However%2C%20existing%20methods%20are%20lack%20of%20efficiency%20that%0Ais%20definitely%20necessary%20for%20embedded%20devices.%20Towards%20this%20end%2C%20we%20propose%20an%0AEfficient%20Anomaly%20detection%20model%20on%20heterogeneous%20Graphs%20via%20contrastive%0ALEarning%20%28EAGLE%29%20by%20contrasting%20abnormal%20nodes%20with%20normal%20ones%20in%20terms%20of%0Atheir%20distances%20to%20the%20local%20context.%20The%20proposed%20method%20first%20samples%0Ainstance%20pairs%20on%20meta%20path-level%20for%20contrastive%20learning.%20Then%2C%20a%20graph%0Aautoencoder-based%20model%20is%20applied%20to%20learn%20informative%20node%20embeddings%20in%20an%0Aunsupervised%20way%2C%20which%20will%20be%20further%20combined%20with%20the%20discriminator%20to%0Apredict%20the%20anomaly%20scores%20of%20nodes.%20Experimental%20results%20show%20that%20EAGLE%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20three%20heterogeneous%20network%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07508v1&entry.124074799=Read"},
{"title": "Breast Cancer Classification in Deep Ultraviolet Fluorescence Images\n  Using a Patch-Level Vision Transformer Framework", "author": "Pouya Afshin and David Helminiak and Tongtong Lu and Tina Yen and Julie M. Jorns and Mollie Patton and Bing Yu and Dong Hye Ye", "abstract": "  Breast-conserving surgery (BCS) aims to completely remove malignant lesions\nwhile maximizing healthy tissue preservation. Intraoperative margin assessment\nis essential to achieve a balance between thorough cancer resection and tissue\nconservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM)\nenables rapid acquisition of whole surface images (WSIs) for excised tissue,\nproviding contrast between malignant and normal tissues. However, breast cancer\nclassification with DUV WSIs is challenged by high resolutions and complex\nhistopathological features. This study introduces a DUV WSI classification\nframework using a patch-level vision transformer (ViT) model, capturing local\nand global features. Grad-CAM++ saliency weighting highlights relevant spatial\nregions, enhances result interpretability, and improves diagnostic accuracy for\nbenign and malignant tissue classification. A comprehensive 5-fold\ncross-validation demonstrates the proposed approach significantly outperforms\nconventional deep learning methods, achieving a classification accuracy of\n98.33%.\n", "link": "http://arxiv.org/abs/2505.07654v1", "date": "2025-05-12", "relevancy": 2.4382, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breast%20Cancer%20Classification%20in%20Deep%20Ultraviolet%20Fluorescence%20Images%0A%20%20Using%20a%20Patch-Level%20Vision%20Transformer%20Framework&body=Title%3A%20Breast%20Cancer%20Classification%20in%20Deep%20Ultraviolet%20Fluorescence%20Images%0A%20%20Using%20a%20Patch-Level%20Vision%20Transformer%20Framework%0AAuthor%3A%20Pouya%20Afshin%20and%20David%20Helminiak%20and%20Tongtong%20Lu%20and%20Tina%20Yen%20and%20Julie%20M.%20Jorns%20and%20Mollie%20Patton%20and%20Bing%20Yu%20and%20Dong%20Hye%20Ye%0AAbstract%3A%20%20%20Breast-conserving%20surgery%20%28BCS%29%20aims%20to%20completely%20remove%20malignant%20lesions%0Awhile%20maximizing%20healthy%20tissue%20preservation.%20Intraoperative%20margin%20assessment%0Ais%20essential%20to%20achieve%20a%20balance%20between%20thorough%20cancer%20resection%20and%20tissue%0Aconservation.%20A%20deep%20ultraviolet%20fluorescence%20scanning%20microscope%20%28DUV-FSM%29%0Aenables%20rapid%20acquisition%20of%20whole%20surface%20images%20%28WSIs%29%20for%20excised%20tissue%2C%0Aproviding%20contrast%20between%20malignant%20and%20normal%20tissues.%20However%2C%20breast%20cancer%0Aclassification%20with%20DUV%20WSIs%20is%20challenged%20by%20high%20resolutions%20and%20complex%0Ahistopathological%20features.%20This%20study%20introduces%20a%20DUV%20WSI%20classification%0Aframework%20using%20a%20patch-level%20vision%20transformer%20%28ViT%29%20model%2C%20capturing%20local%0Aand%20global%20features.%20Grad-CAM%2B%2B%20saliency%20weighting%20highlights%20relevant%20spatial%0Aregions%2C%20enhances%20result%20interpretability%2C%20and%20improves%20diagnostic%20accuracy%20for%0Abenign%20and%20malignant%20tissue%20classification.%20A%20comprehensive%205-fold%0Across-validation%20demonstrates%20the%20proposed%20approach%20significantly%20outperforms%0Aconventional%20deep%20learning%20methods%2C%20achieving%20a%20classification%20accuracy%20of%0A98.33%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreast%2520Cancer%2520Classification%2520in%2520Deep%2520Ultraviolet%2520Fluorescence%2520Images%250A%2520%2520Using%2520a%2520Patch-Level%2520Vision%2520Transformer%2520Framework%26entry.906535625%3DPouya%2520Afshin%2520and%2520David%2520Helminiak%2520and%2520Tongtong%2520Lu%2520and%2520Tina%2520Yen%2520and%2520Julie%2520M.%2520Jorns%2520and%2520Mollie%2520Patton%2520and%2520Bing%2520Yu%2520and%2520Dong%2520Hye%2520Ye%26entry.1292438233%3D%2520%2520Breast-conserving%2520surgery%2520%2528BCS%2529%2520aims%2520to%2520completely%2520remove%2520malignant%2520lesions%250Awhile%2520maximizing%2520healthy%2520tissue%2520preservation.%2520Intraoperative%2520margin%2520assessment%250Ais%2520essential%2520to%2520achieve%2520a%2520balance%2520between%2520thorough%2520cancer%2520resection%2520and%2520tissue%250Aconservation.%2520A%2520deep%2520ultraviolet%2520fluorescence%2520scanning%2520microscope%2520%2528DUV-FSM%2529%250Aenables%2520rapid%2520acquisition%2520of%2520whole%2520surface%2520images%2520%2528WSIs%2529%2520for%2520excised%2520tissue%252C%250Aproviding%2520contrast%2520between%2520malignant%2520and%2520normal%2520tissues.%2520However%252C%2520breast%2520cancer%250Aclassification%2520with%2520DUV%2520WSIs%2520is%2520challenged%2520by%2520high%2520resolutions%2520and%2520complex%250Ahistopathological%2520features.%2520This%2520study%2520introduces%2520a%2520DUV%2520WSI%2520classification%250Aframework%2520using%2520a%2520patch-level%2520vision%2520transformer%2520%2528ViT%2529%2520model%252C%2520capturing%2520local%250Aand%2520global%2520features.%2520Grad-CAM%252B%252B%2520saliency%2520weighting%2520highlights%2520relevant%2520spatial%250Aregions%252C%2520enhances%2520result%2520interpretability%252C%2520and%2520improves%2520diagnostic%2520accuracy%2520for%250Abenign%2520and%2520malignant%2520tissue%2520classification.%2520A%2520comprehensive%25205-fold%250Across-validation%2520demonstrates%2520the%2520proposed%2520approach%2520significantly%2520outperforms%250Aconventional%2520deep%2520learning%2520methods%252C%2520achieving%2520a%2520classification%2520accuracy%2520of%250A98.33%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Cancer%20Classification%20in%20Deep%20Ultraviolet%20Fluorescence%20Images%0A%20%20Using%20a%20Patch-Level%20Vision%20Transformer%20Framework&entry.906535625=Pouya%20Afshin%20and%20David%20Helminiak%20and%20Tongtong%20Lu%20and%20Tina%20Yen%20and%20Julie%20M.%20Jorns%20and%20Mollie%20Patton%20and%20Bing%20Yu%20and%20Dong%20Hye%20Ye&entry.1292438233=%20%20Breast-conserving%20surgery%20%28BCS%29%20aims%20to%20completely%20remove%20malignant%20lesions%0Awhile%20maximizing%20healthy%20tissue%20preservation.%20Intraoperative%20margin%20assessment%0Ais%20essential%20to%20achieve%20a%20balance%20between%20thorough%20cancer%20resection%20and%20tissue%0Aconservation.%20A%20deep%20ultraviolet%20fluorescence%20scanning%20microscope%20%28DUV-FSM%29%0Aenables%20rapid%20acquisition%20of%20whole%20surface%20images%20%28WSIs%29%20for%20excised%20tissue%2C%0Aproviding%20contrast%20between%20malignant%20and%20normal%20tissues.%20However%2C%20breast%20cancer%0Aclassification%20with%20DUV%20WSIs%20is%20challenged%20by%20high%20resolutions%20and%20complex%0Ahistopathological%20features.%20This%20study%20introduces%20a%20DUV%20WSI%20classification%0Aframework%20using%20a%20patch-level%20vision%20transformer%20%28ViT%29%20model%2C%20capturing%20local%0Aand%20global%20features.%20Grad-CAM%2B%2B%20saliency%20weighting%20highlights%20relevant%20spatial%0Aregions%2C%20enhances%20result%20interpretability%2C%20and%20improves%20diagnostic%20accuracy%20for%0Abenign%20and%20malignant%20tissue%20classification.%20A%20comprehensive%205-fold%0Across-validation%20demonstrates%20the%20proposed%20approach%20significantly%20outperforms%0Aconventional%20deep%20learning%20methods%2C%20achieving%20a%20classification%20accuracy%20of%0A98.33%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07654v1&entry.124074799=Read"},
{"title": "From Distributional to Overton Pluralism: Investigating Large Language\n  Model Alignment", "author": "Thom Lake and Eunsol Choi and Greg Durrett", "abstract": "  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n", "link": "http://arxiv.org/abs/2406.17692v2", "date": "2025-05-12", "relevancy": 2.4238, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4873}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Distributional%20to%20Overton%20Pluralism%3A%20Investigating%20Large%20Language%0A%20%20Model%20Alignment&body=Title%3A%20From%20Distributional%20to%20Overton%20Pluralism%3A%20Investigating%20Large%20Language%0A%20%20Model%20Alignment%0AAuthor%3A%20Thom%20Lake%20and%20Eunsol%20Choi%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20The%20alignment%20process%20changes%20several%20properties%20of%20a%20large%20language%20model%27s%0A%28LLM%27s%29%20output%20distribution.%20We%20analyze%20two%20aspects%20of%20post-alignment%0Adistributional%20shift%20of%20LLM%20responses.%20First%2C%20we%20re-examine%20previously%20reported%0Areductions%20in%20response%20diversity%20post-alignment.%20Our%20analysis%20suggests%20that%20an%0Aapparent%20drop%20in%20the%20diversity%20of%20responses%20is%20largely%20explained%20by%20quality%0Acontrol%20and%20information%20aggregation.%20Alignment%20suppresses%20irrelevant%20and%0Aunhelpful%20content%20while%20shifting%20the%20output%20distribution%20toward%20longer%0Aresponses%20that%20cover%20information%20spanning%20several%20responses%20from%20the%20base%20LLM%2C%0Aessentially%20presenting%20diverse%20information%20in%20a%20single%20response.%20Finding%20little%0Aevidence%20that%20alignment%20suppresses%20useful%20information%2C%20it%20is%20natural%20to%20ask%20the%0Aopposite%20question%3A%20do%20aligned%20models%20surface%20information%20that%20cannot%20be%0Arecovered%20from%20base%20models%3F%20Our%20second%20investigation%20shows%20this%20is%20not%20the%20case%0Aand%20the%20behavior%20of%20aligned%20models%20is%20recoverable%20from%20base%20models%20without%0Afine-tuning.%20A%20combination%20of%20in-context%20examples%20and%20lower-resolution%20semantic%0Ahints%20about%20response%20content%20can%20elicit%20responses%20from%20base%20LLMs%20that%20are%20as%0Asimilar%20to%20alignment-tuned%20LLM%20responses%20as%20alignment-tuned%20LLM%20responses%20are%0Ato%20each%20other.%20Taken%20together%2C%20these%20results%20indicate%20that%20current%20alignment%0Atechniques%20capture%20but%20do%20not%20extend%20the%20useful%20subset%20of%20assistant-like%20base%0ALLM%20behavior%2C%20providing%20further%20evidence%20for%20the%20Superficial%20Alignment%0AHypothesis.%20They%20also%20show%20that%20in-context%20alignment%20can%20go%20surprisingly%20far%20as%0Aa%20strategy%20for%20imitating%20aligned%20LLMs%20without%20fine-tuning.%20Our%20code%20and%20data%20is%0Aavailable%20at%20https%3A//github.com/thomlake/investigating-alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Distributional%2520to%2520Overton%2520Pluralism%253A%2520Investigating%2520Large%2520Language%250A%2520%2520Model%2520Alignment%26entry.906535625%3DThom%2520Lake%2520and%2520Eunsol%2520Choi%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520The%2520alignment%2520process%2520changes%2520several%2520properties%2520of%2520a%2520large%2520language%2520model%2527s%250A%2528LLM%2527s%2529%2520output%2520distribution.%2520We%2520analyze%2520two%2520aspects%2520of%2520post-alignment%250Adistributional%2520shift%2520of%2520LLM%2520responses.%2520First%252C%2520we%2520re-examine%2520previously%2520reported%250Areductions%2520in%2520response%2520diversity%2520post-alignment.%2520Our%2520analysis%2520suggests%2520that%2520an%250Aapparent%2520drop%2520in%2520the%2520diversity%2520of%2520responses%2520is%2520largely%2520explained%2520by%2520quality%250Acontrol%2520and%2520information%2520aggregation.%2520Alignment%2520suppresses%2520irrelevant%2520and%250Aunhelpful%2520content%2520while%2520shifting%2520the%2520output%2520distribution%2520toward%2520longer%250Aresponses%2520that%2520cover%2520information%2520spanning%2520several%2520responses%2520from%2520the%2520base%2520LLM%252C%250Aessentially%2520presenting%2520diverse%2520information%2520in%2520a%2520single%2520response.%2520Finding%2520little%250Aevidence%2520that%2520alignment%2520suppresses%2520useful%2520information%252C%2520it%2520is%2520natural%2520to%2520ask%2520the%250Aopposite%2520question%253A%2520do%2520aligned%2520models%2520surface%2520information%2520that%2520cannot%2520be%250Arecovered%2520from%2520base%2520models%253F%2520Our%2520second%2520investigation%2520shows%2520this%2520is%2520not%2520the%2520case%250Aand%2520the%2520behavior%2520of%2520aligned%2520models%2520is%2520recoverable%2520from%2520base%2520models%2520without%250Afine-tuning.%2520A%2520combination%2520of%2520in-context%2520examples%2520and%2520lower-resolution%2520semantic%250Ahints%2520about%2520response%2520content%2520can%2520elicit%2520responses%2520from%2520base%2520LLMs%2520that%2520are%2520as%250Asimilar%2520to%2520alignment-tuned%2520LLM%2520responses%2520as%2520alignment-tuned%2520LLM%2520responses%2520are%250Ato%2520each%2520other.%2520Taken%2520together%252C%2520these%2520results%2520indicate%2520that%2520current%2520alignment%250Atechniques%2520capture%2520but%2520do%2520not%2520extend%2520the%2520useful%2520subset%2520of%2520assistant-like%2520base%250ALLM%2520behavior%252C%2520providing%2520further%2520evidence%2520for%2520the%2520Superficial%2520Alignment%250AHypothesis.%2520They%2520also%2520show%2520that%2520in-context%2520alignment%2520can%2520go%2520surprisingly%2520far%2520as%250Aa%2520strategy%2520for%2520imitating%2520aligned%2520LLMs%2520without%2520fine-tuning.%2520Our%2520code%2520and%2520data%2520is%250Aavailable%2520at%2520https%253A//github.com/thomlake/investigating-alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Distributional%20to%20Overton%20Pluralism%3A%20Investigating%20Large%20Language%0A%20%20Model%20Alignment&entry.906535625=Thom%20Lake%20and%20Eunsol%20Choi%20and%20Greg%20Durrett&entry.1292438233=%20%20The%20alignment%20process%20changes%20several%20properties%20of%20a%20large%20language%20model%27s%0A%28LLM%27s%29%20output%20distribution.%20We%20analyze%20two%20aspects%20of%20post-alignment%0Adistributional%20shift%20of%20LLM%20responses.%20First%2C%20we%20re-examine%20previously%20reported%0Areductions%20in%20response%20diversity%20post-alignment.%20Our%20analysis%20suggests%20that%20an%0Aapparent%20drop%20in%20the%20diversity%20of%20responses%20is%20largely%20explained%20by%20quality%0Acontrol%20and%20information%20aggregation.%20Alignment%20suppresses%20irrelevant%20and%0Aunhelpful%20content%20while%20shifting%20the%20output%20distribution%20toward%20longer%0Aresponses%20that%20cover%20information%20spanning%20several%20responses%20from%20the%20base%20LLM%2C%0Aessentially%20presenting%20diverse%20information%20in%20a%20single%20response.%20Finding%20little%0Aevidence%20that%20alignment%20suppresses%20useful%20information%2C%20it%20is%20natural%20to%20ask%20the%0Aopposite%20question%3A%20do%20aligned%20models%20surface%20information%20that%20cannot%20be%0Arecovered%20from%20base%20models%3F%20Our%20second%20investigation%20shows%20this%20is%20not%20the%20case%0Aand%20the%20behavior%20of%20aligned%20models%20is%20recoverable%20from%20base%20models%20without%0Afine-tuning.%20A%20combination%20of%20in-context%20examples%20and%20lower-resolution%20semantic%0Ahints%20about%20response%20content%20can%20elicit%20responses%20from%20base%20LLMs%20that%20are%20as%0Asimilar%20to%20alignment-tuned%20LLM%20responses%20as%20alignment-tuned%20LLM%20responses%20are%0Ato%20each%20other.%20Taken%20together%2C%20these%20results%20indicate%20that%20current%20alignment%0Atechniques%20capture%20but%20do%20not%20extend%20the%20useful%20subset%20of%20assistant-like%20base%0ALLM%20behavior%2C%20providing%20further%20evidence%20for%20the%20Superficial%20Alignment%0AHypothesis.%20They%20also%20show%20that%20in-context%20alignment%20can%20go%20surprisingly%20far%20as%0Aa%20strategy%20for%20imitating%20aligned%20LLMs%20without%20fine-tuning.%20Our%20code%20and%20data%20is%0Aavailable%20at%20https%3A//github.com/thomlake/investigating-alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17692v2&entry.124074799=Read"},
{"title": "DanceGRPO: Unleashing GRPO on Visual Generation", "author": "Zeyue Xue and Jie Wu and Yu Gao and Fangyuan Kong and Lingting Zhu and Mengzhao Chen and Zhiheng Liu and Wei Liu and Qiushan Guo and Weilin Huang and Ping Luo", "abstract": "  Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.\n", "link": "http://arxiv.org/abs/2505.07818v1", "date": "2025-05-12", "relevancy": 2.4139, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6158}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5961}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanceGRPO%3A%20Unleashing%20GRPO%20on%20Visual%20Generation&body=Title%3A%20DanceGRPO%3A%20Unleashing%20GRPO%20on%20Visual%20Generation%0AAuthor%3A%20Zeyue%20Xue%20and%20Jie%20Wu%20and%20Yu%20Gao%20and%20Fangyuan%20Kong%20and%20Lingting%20Zhu%20and%20Mengzhao%20Chen%20and%20Zhiheng%20Liu%20and%20Wei%20Liu%20and%20Qiushan%20Guo%20and%20Weilin%20Huang%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20generative%20models-particularly%20diffusion%20models%20and%0Arectified%20flows-have%20revolutionized%20visual%20content%20creation%2C%20yet%20aligning%20model%0Aoutputs%20with%20human%20preferences%20remains%20a%20critical%20challenge.%20Existing%0Areinforcement%20learning%20%28RL%29-based%20methods%20for%20visual%20generation%20face%20critical%0Alimitations%3A%20incompatibility%20with%20modern%20Ordinary%20Differential%20Equations%0A%28ODEs%29-based%20sampling%20paradigms%2C%20instability%20in%20large-scale%20training%2C%20and%20lack%0Aof%20validation%20for%20video%20generation.%20This%20paper%20introduces%20DanceGRPO%2C%20the%20first%0Aunified%20framework%20to%20adapt%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20visual%0Ageneration%20paradigms%2C%20unleashing%20one%20unified%20RL%20algorithm%20across%20two%20generative%0Aparadigms%20%28diffusion%20models%20and%20rectified%20flows%29%2C%20three%20tasks%20%28text-to-image%2C%0Atext-to-video%2C%20image-to-video%29%2C%20four%20foundation%20models%20%28Stable%20Diffusion%2C%0AHunyuanVideo%2C%20FLUX%2C%20SkyReel-I2V%29%2C%20and%20five%20reward%20models%20%28image/video%0Aaesthetics%2C%20text-image%20alignment%2C%20video%20motion%20quality%2C%20and%20binary%20reward%29.%20To%0Aour%20knowledge%2C%20DanceGRPO%20is%20the%20first%20RL-based%20unified%20framework%20capable%20of%0Aseamless%20adaptation%20across%20diverse%20generative%20paradigms%2C%20tasks%2C%20foundational%0Amodels%2C%20and%20reward%20models.%20DanceGRPO%20demonstrates%20consistent%20and%20substantial%0Aimprovements%2C%20which%20outperform%20baselines%20by%20up%20to%20181%25%20on%20benchmarks%20such%20as%0AHPS-v2.1%2C%20CLIP%20Score%2C%20VideoAlign%2C%20and%20GenEval.%20Notably%2C%20DanceGRPO%20not%20only%20can%0Astabilize%20policy%20optimization%20for%20complex%20video%20generation%2C%20but%20also%20enables%0Agenerative%20policy%20to%20better%20capture%20denoising%20trajectories%20for%20Best-of-N%0Ainference%20scaling%20and%20learn%20from%20sparse%20binary%20feedback.%20Our%20results%20establish%0ADanceGRPO%20as%20a%20robust%20and%20versatile%20solution%20for%20scaling%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20tasks%20in%20visual%20generation%2C%20offering%20new%20insights%0Ainto%20harmonizing%20reinforcement%20learning%20and%20visual%20synthesis.%20The%20code%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanceGRPO%253A%2520Unleashing%2520GRPO%2520on%2520Visual%2520Generation%26entry.906535625%3DZeyue%2520Xue%2520and%2520Jie%2520Wu%2520and%2520Yu%2520Gao%2520and%2520Fangyuan%2520Kong%2520and%2520Lingting%2520Zhu%2520and%2520Mengzhao%2520Chen%2520and%2520Zhiheng%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Qiushan%2520Guo%2520and%2520Weilin%2520Huang%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520generative%2520models-particularly%2520diffusion%2520models%2520and%250Arectified%2520flows-have%2520revolutionized%2520visual%2520content%2520creation%252C%2520yet%2520aligning%2520model%250Aoutputs%2520with%2520human%2520preferences%2520remains%2520a%2520critical%2520challenge.%2520Existing%250Areinforcement%2520learning%2520%2528RL%2529-based%2520methods%2520for%2520visual%2520generation%2520face%2520critical%250Alimitations%253A%2520incompatibility%2520with%2520modern%2520Ordinary%2520Differential%2520Equations%250A%2528ODEs%2529-based%2520sampling%2520paradigms%252C%2520instability%2520in%2520large-scale%2520training%252C%2520and%2520lack%250Aof%2520validation%2520for%2520video%2520generation.%2520This%2520paper%2520introduces%2520DanceGRPO%252C%2520the%2520first%250Aunified%2520framework%2520to%2520adapt%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520visual%250Ageneration%2520paradigms%252C%2520unleashing%2520one%2520unified%2520RL%2520algorithm%2520across%2520two%2520generative%250Aparadigms%2520%2528diffusion%2520models%2520and%2520rectified%2520flows%2529%252C%2520three%2520tasks%2520%2528text-to-image%252C%250Atext-to-video%252C%2520image-to-video%2529%252C%2520four%2520foundation%2520models%2520%2528Stable%2520Diffusion%252C%250AHunyuanVideo%252C%2520FLUX%252C%2520SkyReel-I2V%2529%252C%2520and%2520five%2520reward%2520models%2520%2528image/video%250Aaesthetics%252C%2520text-image%2520alignment%252C%2520video%2520motion%2520quality%252C%2520and%2520binary%2520reward%2529.%2520To%250Aour%2520knowledge%252C%2520DanceGRPO%2520is%2520the%2520first%2520RL-based%2520unified%2520framework%2520capable%2520of%250Aseamless%2520adaptation%2520across%2520diverse%2520generative%2520paradigms%252C%2520tasks%252C%2520foundational%250Amodels%252C%2520and%2520reward%2520models.%2520DanceGRPO%2520demonstrates%2520consistent%2520and%2520substantial%250Aimprovements%252C%2520which%2520outperform%2520baselines%2520by%2520up%2520to%2520181%2525%2520on%2520benchmarks%2520such%2520as%250AHPS-v2.1%252C%2520CLIP%2520Score%252C%2520VideoAlign%252C%2520and%2520GenEval.%2520Notably%252C%2520DanceGRPO%2520not%2520only%2520can%250Astabilize%2520policy%2520optimization%2520for%2520complex%2520video%2520generation%252C%2520but%2520also%2520enables%250Agenerative%2520policy%2520to%2520better%2520capture%2520denoising%2520trajectories%2520for%2520Best-of-N%250Ainference%2520scaling%2520and%2520learn%2520from%2520sparse%2520binary%2520feedback.%2520Our%2520results%2520establish%250ADanceGRPO%2520as%2520a%2520robust%2520and%2520versatile%2520solution%2520for%2520scaling%2520Reinforcement%2520Learning%250Afrom%2520Human%2520Feedback%2520%2528RLHF%2529%2520tasks%2520in%2520visual%2520generation%252C%2520offering%2520new%2520insights%250Ainto%2520harmonizing%2520reinforcement%2520learning%2520and%2520visual%2520synthesis.%2520The%2520code%2520will%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceGRPO%3A%20Unleashing%20GRPO%20on%20Visual%20Generation&entry.906535625=Zeyue%20Xue%20and%20Jie%20Wu%20and%20Yu%20Gao%20and%20Fangyuan%20Kong%20and%20Lingting%20Zhu%20and%20Mengzhao%20Chen%20and%20Zhiheng%20Liu%20and%20Wei%20Liu%20and%20Qiushan%20Guo%20and%20Weilin%20Huang%20and%20Ping%20Luo&entry.1292438233=%20%20Recent%20breakthroughs%20in%20generative%20models-particularly%20diffusion%20models%20and%0Arectified%20flows-have%20revolutionized%20visual%20content%20creation%2C%20yet%20aligning%20model%0Aoutputs%20with%20human%20preferences%20remains%20a%20critical%20challenge.%20Existing%0Areinforcement%20learning%20%28RL%29-based%20methods%20for%20visual%20generation%20face%20critical%0Alimitations%3A%20incompatibility%20with%20modern%20Ordinary%20Differential%20Equations%0A%28ODEs%29-based%20sampling%20paradigms%2C%20instability%20in%20large-scale%20training%2C%20and%20lack%0Aof%20validation%20for%20video%20generation.%20This%20paper%20introduces%20DanceGRPO%2C%20the%20first%0Aunified%20framework%20to%20adapt%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20visual%0Ageneration%20paradigms%2C%20unleashing%20one%20unified%20RL%20algorithm%20across%20two%20generative%0Aparadigms%20%28diffusion%20models%20and%20rectified%20flows%29%2C%20three%20tasks%20%28text-to-image%2C%0Atext-to-video%2C%20image-to-video%29%2C%20four%20foundation%20models%20%28Stable%20Diffusion%2C%0AHunyuanVideo%2C%20FLUX%2C%20SkyReel-I2V%29%2C%20and%20five%20reward%20models%20%28image/video%0Aaesthetics%2C%20text-image%20alignment%2C%20video%20motion%20quality%2C%20and%20binary%20reward%29.%20To%0Aour%20knowledge%2C%20DanceGRPO%20is%20the%20first%20RL-based%20unified%20framework%20capable%20of%0Aseamless%20adaptation%20across%20diverse%20generative%20paradigms%2C%20tasks%2C%20foundational%0Amodels%2C%20and%20reward%20models.%20DanceGRPO%20demonstrates%20consistent%20and%20substantial%0Aimprovements%2C%20which%20outperform%20baselines%20by%20up%20to%20181%25%20on%20benchmarks%20such%20as%0AHPS-v2.1%2C%20CLIP%20Score%2C%20VideoAlign%2C%20and%20GenEval.%20Notably%2C%20DanceGRPO%20not%20only%20can%0Astabilize%20policy%20optimization%20for%20complex%20video%20generation%2C%20but%20also%20enables%0Agenerative%20policy%20to%20better%20capture%20denoising%20trajectories%20for%20Best-of-N%0Ainference%20scaling%20and%20learn%20from%20sparse%20binary%20feedback.%20Our%20results%20establish%0ADanceGRPO%20as%20a%20robust%20and%20versatile%20solution%20for%20scaling%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20tasks%20in%20visual%20generation%2C%20offering%20new%20insights%0Ainto%20harmonizing%20reinforcement%20learning%20and%20visual%20synthesis.%20The%20code%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07818v1&entry.124074799=Read"},
{"title": "Injecting Knowledge Graphs into Large Language Models", "author": "Erica Coppolillo", "abstract": "  Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) remains a key challenge for symbolic reasoning. Existing\nmethods mainly rely on prompt engineering or fine-tuning, which lose structural\nfidelity or incur high computational costs. Building on recent encoding\ntechniques which integrate graph embeddings within the LLM input as tokens, we\nextend this paradigm to the KG domain by leveraging Knowledge Graph Embedding\n(KGE) models, thus enabling graph-aware reasoning. Our approach is\nmodel-agnostic, resource-efficient, and compatible with any LLMs. Extensive\nexperimentation on synthetic and real-world datasets shows that our method\nimproves reasoning performance over established baselines, further achieving\nthe best trade-off in terms of accuracy and efficiency against state-of-the-art\nLLMs.\n", "link": "http://arxiv.org/abs/2505.07554v1", "date": "2025-05-12", "relevancy": 2.4101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Injecting%20Knowledge%20Graphs%20into%20Large%20Language%20Models&body=Title%3A%20Injecting%20Knowledge%20Graphs%20into%20Large%20Language%20Models%0AAuthor%3A%20Erica%20Coppolillo%0AAbstract%3A%20%20%20Integrating%20structured%20knowledge%20from%20Knowledge%20Graphs%20%28KGs%29%20into%20Large%0ALanguage%20Models%20%28LLMs%29%20remains%20a%20key%20challenge%20for%20symbolic%20reasoning.%20Existing%0Amethods%20mainly%20rely%20on%20prompt%20engineering%20or%20fine-tuning%2C%20which%20lose%20structural%0Afidelity%20or%20incur%20high%20computational%20costs.%20Building%20on%20recent%20encoding%0Atechniques%20which%20integrate%20graph%20embeddings%20within%20the%20LLM%20input%20as%20tokens%2C%20we%0Aextend%20this%20paradigm%20to%20the%20KG%20domain%20by%20leveraging%20Knowledge%20Graph%20Embedding%0A%28KGE%29%20models%2C%20thus%20enabling%20graph-aware%20reasoning.%20Our%20approach%20is%0Amodel-agnostic%2C%20resource-efficient%2C%20and%20compatible%20with%20any%20LLMs.%20Extensive%0Aexperimentation%20on%20synthetic%20and%20real-world%20datasets%20shows%20that%20our%20method%0Aimproves%20reasoning%20performance%20over%20established%20baselines%2C%20further%20achieving%0Athe%20best%20trade-off%20in%20terms%20of%20accuracy%20and%20efficiency%20against%20state-of-the-art%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInjecting%2520Knowledge%2520Graphs%2520into%2520Large%2520Language%2520Models%26entry.906535625%3DErica%2520Coppolillo%26entry.1292438233%3D%2520%2520Integrating%2520structured%2520knowledge%2520from%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520into%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520remains%2520a%2520key%2520challenge%2520for%2520symbolic%2520reasoning.%2520Existing%250Amethods%2520mainly%2520rely%2520on%2520prompt%2520engineering%2520or%2520fine-tuning%252C%2520which%2520lose%2520structural%250Afidelity%2520or%2520incur%2520high%2520computational%2520costs.%2520Building%2520on%2520recent%2520encoding%250Atechniques%2520which%2520integrate%2520graph%2520embeddings%2520within%2520the%2520LLM%2520input%2520as%2520tokens%252C%2520we%250Aextend%2520this%2520paradigm%2520to%2520the%2520KG%2520domain%2520by%2520leveraging%2520Knowledge%2520Graph%2520Embedding%250A%2528KGE%2529%2520models%252C%2520thus%2520enabling%2520graph-aware%2520reasoning.%2520Our%2520approach%2520is%250Amodel-agnostic%252C%2520resource-efficient%252C%2520and%2520compatible%2520with%2520any%2520LLMs.%2520Extensive%250Aexperimentation%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520shows%2520that%2520our%2520method%250Aimproves%2520reasoning%2520performance%2520over%2520established%2520baselines%252C%2520further%2520achieving%250Athe%2520best%2520trade-off%2520in%2520terms%2520of%2520accuracy%2520and%2520efficiency%2520against%2520state-of-the-art%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Injecting%20Knowledge%20Graphs%20into%20Large%20Language%20Models&entry.906535625=Erica%20Coppolillo&entry.1292438233=%20%20Integrating%20structured%20knowledge%20from%20Knowledge%20Graphs%20%28KGs%29%20into%20Large%0ALanguage%20Models%20%28LLMs%29%20remains%20a%20key%20challenge%20for%20symbolic%20reasoning.%20Existing%0Amethods%20mainly%20rely%20on%20prompt%20engineering%20or%20fine-tuning%2C%20which%20lose%20structural%0Afidelity%20or%20incur%20high%20computational%20costs.%20Building%20on%20recent%20encoding%0Atechniques%20which%20integrate%20graph%20embeddings%20within%20the%20LLM%20input%20as%20tokens%2C%20we%0Aextend%20this%20paradigm%20to%20the%20KG%20domain%20by%20leveraging%20Knowledge%20Graph%20Embedding%0A%28KGE%29%20models%2C%20thus%20enabling%20graph-aware%20reasoning.%20Our%20approach%20is%0Amodel-agnostic%2C%20resource-efficient%2C%20and%20compatible%20with%20any%20LLMs.%20Extensive%0Aexperimentation%20on%20synthetic%20and%20real-world%20datasets%20shows%20that%20our%20method%0Aimproves%20reasoning%20performance%20over%20established%20baselines%2C%20further%20achieving%0Athe%20best%20trade-off%20in%20terms%20of%20accuracy%20and%20efficiency%20against%20state-of-the-art%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07554v1&entry.124074799=Read"},
{"title": "Transfer Learning with Foundational Models for Time Series Forecasting\n  using Low-Rank Adaptations", "author": "M. Germ\u00e1n-Morales and A. J. Rivera-Rivas and M. J. del Jesus D\u00edaz and C. J. Carmona", "abstract": "  Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI.\n", "link": "http://arxiv.org/abs/2410.11539v3", "date": "2025-05-12", "relevancy": 2.4083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20with%20Foundational%20Models%20for%20Time%20Series%20Forecasting%0A%20%20using%20Low-Rank%20Adaptations&body=Title%3A%20Transfer%20Learning%20with%20Foundational%20Models%20for%20Time%20Series%20Forecasting%0A%20%20using%20Low-Rank%20Adaptations%0AAuthor%3A%20M.%20Germ%C3%A1n-Morales%20and%20A.%20J.%20Rivera-Rivas%20and%20M.%20J.%20del%20Jesus%20D%C3%ADaz%20and%20C.%20J.%20Carmona%0AAbstract%3A%20%20%20Foundational%20Models%20are%20an%20emerging%20widely%20used%20technique%20of%20GenAI.%20These%0Amodels%20are%20distinguished%20by%20their%20scalability%20and%20the%20ease%20with%20which%20they%20can%0Abe%20adapted%20through%20the%20exploitation%20of%20Transfer%20Learning.%20The%20availability%20of%0Ahigh%20computational%20power%20and%20large%20datasets%20have%20supported%20their%20development%2C%0Aachieving%20a%20high%20generalization%20capacity%20due%20to%20the%20enormous%20and%20heterogeneous%0Aamounts%20of%20data%20used%20in%20their%20initial%20training.%20These%20characteristics%0Acontribute%20to%20a%20solid%20base%20that%20can%20be%20adapted%20or%20adjusted%20to%20a%20wide%20range%20of%0Atasks%2C%20increasing%20their%20applicability.%20This%20study%20proposes%20the%20methodology%0ALLIAM%2C%20a%20straightforward%20adaptation%20of%20a%20kind%20of%20FM%2C%20Large%20Language%20Models%2C%20for%0Athe%20Time%20Series%20Forecasting%20task.%20An%20adequate%20time-series%20prompting%20schema%20and%0ALow-Rank%20Adaptations%20are%20used%20to%20enhance%20the%20knowledge%20of%20the%20model%20with%0Adiverse%20time%20series%20datasets%2C%20known%20as%20the%20fine-tuning%20phase.%20A%20study%20divided%0Ain%20two%20stages%20has%20been%20performed%20for%20evaluating%20the%20effectiveness%20of%20the%0Aproposed%20methodology.%20Initially%2C%20a%20comparison%20was%20made%20between%20the%20performance%0Aof%20LLIAM%20and%20different%20state-of-the-art%20DL%20algorithms%2C%20including%20Recurrent%0ANeural%20Networks%20and%20Temporal%20Convolutional%20Networks%2C%20as%20well%20as%20a%20LLM-based%0Amethod%2C%20TimeLLM.%20Following%20this%2C%20a%20zero-shot%20study%20is%20presented%20in%20order%20to%0Aevaluate%20the%20generalization%20capacity%20of%20the%20proposed%20methodology%20with%20time%0Aseries%20datasets%20from%20unknown%20domains%20not%20considered%20in%20the%20model%20training.%20The%0Aoutcomes%20of%20this%20investigation%20demonstrate%20the%20efficacy%20of%20LLIAM%2C%20highlighting%0Athat%20this%20straightforward%20and%20general%20approach%20can%20attain%20competent%20results%0Awithout%20the%20necessity%20for%20applying%20complex%20modifications.%20This%20work%20also%0Aencourages%20the%20use%20of%20available%20resources%20%28such%20as%20these%20pre-trained%20models%29%0Aand%20efficient%20fine-tuning%20techniques%20to%20avoid%20unnecessary%20and%20costly%20training%2C%0Anarrowing%20the%20gap%20between%20the%20goals%20of%20traditional%20AI%20and%20Green%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520with%2520Foundational%2520Models%2520for%2520Time%2520Series%2520Forecasting%250A%2520%2520using%2520Low-Rank%2520Adaptations%26entry.906535625%3DM.%2520Germ%25C3%25A1n-Morales%2520and%2520A.%2520J.%2520Rivera-Rivas%2520and%2520M.%2520J.%2520del%2520Jesus%2520D%25C3%25ADaz%2520and%2520C.%2520J.%2520Carmona%26entry.1292438233%3D%2520%2520Foundational%2520Models%2520are%2520an%2520emerging%2520widely%2520used%2520technique%2520of%2520GenAI.%2520These%250Amodels%2520are%2520distinguished%2520by%2520their%2520scalability%2520and%2520the%2520ease%2520with%2520which%2520they%2520can%250Abe%2520adapted%2520through%2520the%2520exploitation%2520of%2520Transfer%2520Learning.%2520The%2520availability%2520of%250Ahigh%2520computational%2520power%2520and%2520large%2520datasets%2520have%2520supported%2520their%2520development%252C%250Aachieving%2520a%2520high%2520generalization%2520capacity%2520due%2520to%2520the%2520enormous%2520and%2520heterogeneous%250Aamounts%2520of%2520data%2520used%2520in%2520their%2520initial%2520training.%2520These%2520characteristics%250Acontribute%2520to%2520a%2520solid%2520base%2520that%2520can%2520be%2520adapted%2520or%2520adjusted%2520to%2520a%2520wide%2520range%2520of%250Atasks%252C%2520increasing%2520their%2520applicability.%2520This%2520study%2520proposes%2520the%2520methodology%250ALLIAM%252C%2520a%2520straightforward%2520adaptation%2520of%2520a%2520kind%2520of%2520FM%252C%2520Large%2520Language%2520Models%252C%2520for%250Athe%2520Time%2520Series%2520Forecasting%2520task.%2520An%2520adequate%2520time-series%2520prompting%2520schema%2520and%250ALow-Rank%2520Adaptations%2520are%2520used%2520to%2520enhance%2520the%2520knowledge%2520of%2520the%2520model%2520with%250Adiverse%2520time%2520series%2520datasets%252C%2520known%2520as%2520the%2520fine-tuning%2520phase.%2520A%2520study%2520divided%250Ain%2520two%2520stages%2520has%2520been%2520performed%2520for%2520evaluating%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520methodology.%2520Initially%252C%2520a%2520comparison%2520was%2520made%2520between%2520the%2520performance%250Aof%2520LLIAM%2520and%2520different%2520state-of-the-art%2520DL%2520algorithms%252C%2520including%2520Recurrent%250ANeural%2520Networks%2520and%2520Temporal%2520Convolutional%2520Networks%252C%2520as%2520well%2520as%2520a%2520LLM-based%250Amethod%252C%2520TimeLLM.%2520Following%2520this%252C%2520a%2520zero-shot%2520study%2520is%2520presented%2520in%2520order%2520to%250Aevaluate%2520the%2520generalization%2520capacity%2520of%2520the%2520proposed%2520methodology%2520with%2520time%250Aseries%2520datasets%2520from%2520unknown%2520domains%2520not%2520considered%2520in%2520the%2520model%2520training.%2520The%250Aoutcomes%2520of%2520this%2520investigation%2520demonstrate%2520the%2520efficacy%2520of%2520LLIAM%252C%2520highlighting%250Athat%2520this%2520straightforward%2520and%2520general%2520approach%2520can%2520attain%2520competent%2520results%250Awithout%2520the%2520necessity%2520for%2520applying%2520complex%2520modifications.%2520This%2520work%2520also%250Aencourages%2520the%2520use%2520of%2520available%2520resources%2520%2528such%2520as%2520these%2520pre-trained%2520models%2529%250Aand%2520efficient%2520fine-tuning%2520techniques%2520to%2520avoid%2520unnecessary%2520and%2520costly%2520training%252C%250Anarrowing%2520the%2520gap%2520between%2520the%2520goals%2520of%2520traditional%2520AI%2520and%2520Green%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20with%20Foundational%20Models%20for%20Time%20Series%20Forecasting%0A%20%20using%20Low-Rank%20Adaptations&entry.906535625=M.%20Germ%C3%A1n-Morales%20and%20A.%20J.%20Rivera-Rivas%20and%20M.%20J.%20del%20Jesus%20D%C3%ADaz%20and%20C.%20J.%20Carmona&entry.1292438233=%20%20Foundational%20Models%20are%20an%20emerging%20widely%20used%20technique%20of%20GenAI.%20These%0Amodels%20are%20distinguished%20by%20their%20scalability%20and%20the%20ease%20with%20which%20they%20can%0Abe%20adapted%20through%20the%20exploitation%20of%20Transfer%20Learning.%20The%20availability%20of%0Ahigh%20computational%20power%20and%20large%20datasets%20have%20supported%20their%20development%2C%0Aachieving%20a%20high%20generalization%20capacity%20due%20to%20the%20enormous%20and%20heterogeneous%0Aamounts%20of%20data%20used%20in%20their%20initial%20training.%20These%20characteristics%0Acontribute%20to%20a%20solid%20base%20that%20can%20be%20adapted%20or%20adjusted%20to%20a%20wide%20range%20of%0Atasks%2C%20increasing%20their%20applicability.%20This%20study%20proposes%20the%20methodology%0ALLIAM%2C%20a%20straightforward%20adaptation%20of%20a%20kind%20of%20FM%2C%20Large%20Language%20Models%2C%20for%0Athe%20Time%20Series%20Forecasting%20task.%20An%20adequate%20time-series%20prompting%20schema%20and%0ALow-Rank%20Adaptations%20are%20used%20to%20enhance%20the%20knowledge%20of%20the%20model%20with%0Adiverse%20time%20series%20datasets%2C%20known%20as%20the%20fine-tuning%20phase.%20A%20study%20divided%0Ain%20two%20stages%20has%20been%20performed%20for%20evaluating%20the%20effectiveness%20of%20the%0Aproposed%20methodology.%20Initially%2C%20a%20comparison%20was%20made%20between%20the%20performance%0Aof%20LLIAM%20and%20different%20state-of-the-art%20DL%20algorithms%2C%20including%20Recurrent%0ANeural%20Networks%20and%20Temporal%20Convolutional%20Networks%2C%20as%20well%20as%20a%20LLM-based%0Amethod%2C%20TimeLLM.%20Following%20this%2C%20a%20zero-shot%20study%20is%20presented%20in%20order%20to%0Aevaluate%20the%20generalization%20capacity%20of%20the%20proposed%20methodology%20with%20time%0Aseries%20datasets%20from%20unknown%20domains%20not%20considered%20in%20the%20model%20training.%20The%0Aoutcomes%20of%20this%20investigation%20demonstrate%20the%20efficacy%20of%20LLIAM%2C%20highlighting%0Athat%20this%20straightforward%20and%20general%20approach%20can%20attain%20competent%20results%0Awithout%20the%20necessity%20for%20applying%20complex%20modifications.%20This%20work%20also%0Aencourages%20the%20use%20of%20available%20resources%20%28such%20as%20these%20pre-trained%20models%29%0Aand%20efficient%20fine-tuning%20techniques%20to%20avoid%20unnecessary%20and%20costly%20training%2C%0Anarrowing%20the%20gap%20between%20the%20goals%20of%20traditional%20AI%20and%20Green%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11539v3&entry.124074799=Read"},
{"title": "Beyond Static Perception: Integrating Temporal Context into VLMs for\n  Cloth Folding", "author": "Oriol Barbany and Adri\u00e0 Colom\u00e9 and Carme Torras", "abstract": "  Manipulating clothes is challenging due to their complex dynamics, high\ndeformability, and frequent self-occlusions. Garments exhibit a nearly infinite\nnumber of configurations, making explicit state representations difficult to\ndefine. In this paper, we analyze BiFold, a model that predicts\nlanguage-conditioned pick-and-place actions from visual observations, while\nimplicitly encoding garment state through end-to-end learning. To address\nscenarios such as crumpled garments or recovery from failed manipulations,\nBiFold leverages temporal context to improve state estimation. We examine the\ninternal representations of the model and present evidence that its fine-tuning\nand temporal context enable effective alignment between text and image regions,\nas well as temporal consistency.\n", "link": "http://arxiv.org/abs/2505.07600v1", "date": "2025-05-12", "relevancy": 2.3955, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6329}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6052}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Static%20Perception%3A%20Integrating%20Temporal%20Context%20into%20VLMs%20for%0A%20%20Cloth%20Folding&body=Title%3A%20Beyond%20Static%20Perception%3A%20Integrating%20Temporal%20Context%20into%20VLMs%20for%0A%20%20Cloth%20Folding%0AAuthor%3A%20Oriol%20Barbany%20and%20Adri%C3%A0%20Colom%C3%A9%20and%20Carme%20Torras%0AAbstract%3A%20%20%20Manipulating%20clothes%20is%20challenging%20due%20to%20their%20complex%20dynamics%2C%20high%0Adeformability%2C%20and%20frequent%20self-occlusions.%20Garments%20exhibit%20a%20nearly%20infinite%0Anumber%20of%20configurations%2C%20making%20explicit%20state%20representations%20difficult%20to%0Adefine.%20In%20this%20paper%2C%20we%20analyze%20BiFold%2C%20a%20model%20that%20predicts%0Alanguage-conditioned%20pick-and-place%20actions%20from%20visual%20observations%2C%20while%0Aimplicitly%20encoding%20garment%20state%20through%20end-to-end%20learning.%20To%20address%0Ascenarios%20such%20as%20crumpled%20garments%20or%20recovery%20from%20failed%20manipulations%2C%0ABiFold%20leverages%20temporal%20context%20to%20improve%20state%20estimation.%20We%20examine%20the%0Ainternal%20representations%20of%20the%20model%20and%20present%20evidence%20that%20its%20fine-tuning%0Aand%20temporal%20context%20enable%20effective%20alignment%20between%20text%20and%20image%20regions%2C%0Aas%20well%20as%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Static%2520Perception%253A%2520Integrating%2520Temporal%2520Context%2520into%2520VLMs%2520for%250A%2520%2520Cloth%2520Folding%26entry.906535625%3DOriol%2520Barbany%2520and%2520Adri%25C3%25A0%2520Colom%25C3%25A9%2520and%2520Carme%2520Torras%26entry.1292438233%3D%2520%2520Manipulating%2520clothes%2520is%2520challenging%2520due%2520to%2520their%2520complex%2520dynamics%252C%2520high%250Adeformability%252C%2520and%2520frequent%2520self-occlusions.%2520Garments%2520exhibit%2520a%2520nearly%2520infinite%250Anumber%2520of%2520configurations%252C%2520making%2520explicit%2520state%2520representations%2520difficult%2520to%250Adefine.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520BiFold%252C%2520a%2520model%2520that%2520predicts%250Alanguage-conditioned%2520pick-and-place%2520actions%2520from%2520visual%2520observations%252C%2520while%250Aimplicitly%2520encoding%2520garment%2520state%2520through%2520end-to-end%2520learning.%2520To%2520address%250Ascenarios%2520such%2520as%2520crumpled%2520garments%2520or%2520recovery%2520from%2520failed%2520manipulations%252C%250ABiFold%2520leverages%2520temporal%2520context%2520to%2520improve%2520state%2520estimation.%2520We%2520examine%2520the%250Ainternal%2520representations%2520of%2520the%2520model%2520and%2520present%2520evidence%2520that%2520its%2520fine-tuning%250Aand%2520temporal%2520context%2520enable%2520effective%2520alignment%2520between%2520text%2520and%2520image%2520regions%252C%250Aas%2520well%2520as%2520temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Static%20Perception%3A%20Integrating%20Temporal%20Context%20into%20VLMs%20for%0A%20%20Cloth%20Folding&entry.906535625=Oriol%20Barbany%20and%20Adri%C3%A0%20Colom%C3%A9%20and%20Carme%20Torras&entry.1292438233=%20%20Manipulating%20clothes%20is%20challenging%20due%20to%20their%20complex%20dynamics%2C%20high%0Adeformability%2C%20and%20frequent%20self-occlusions.%20Garments%20exhibit%20a%20nearly%20infinite%0Anumber%20of%20configurations%2C%20making%20explicit%20state%20representations%20difficult%20to%0Adefine.%20In%20this%20paper%2C%20we%20analyze%20BiFold%2C%20a%20model%20that%20predicts%0Alanguage-conditioned%20pick-and-place%20actions%20from%20visual%20observations%2C%20while%0Aimplicitly%20encoding%20garment%20state%20through%20end-to-end%20learning.%20To%20address%0Ascenarios%20such%20as%20crumpled%20garments%20or%20recovery%20from%20failed%20manipulations%2C%0ABiFold%20leverages%20temporal%20context%20to%20improve%20state%20estimation.%20We%20examine%20the%0Ainternal%20representations%20of%20the%20model%20and%20present%20evidence%20that%20its%20fine-tuning%0Aand%20temporal%20context%20enable%20effective%20alignment%20between%20text%20and%20image%20regions%2C%0Aas%20well%20as%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07600v1&entry.124074799=Read"},
{"title": "BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching", "author": "RuiKang OuYang and Bo Qiang and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, Noised Energy Matching, which\ntheoretically has lower variance and more complexity compared to related works.\nFurthermore, a novel bootstrapping technique is applied to NEM to balance\nbetween bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40\nGaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The\nexperimental results demonstrate that BNEM can achieve state-of-the-art\nperformance while being more robust.\n", "link": "http://arxiv.org/abs/2409.09787v4", "date": "2025-05-12", "relevancy": 2.3755, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5369}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4444}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching&body=Title%3A%20BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching%0AAuthor%3A%20RuiKang%20OuYang%20and%20Bo%20Qiang%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Developing%20an%20efficient%20sampler%20capable%20of%20generating%20independent%20and%0Aidentically%20distributed%20%28IID%29%20samples%20from%20a%20Boltzmann%20distribution%20is%20a%0Acrucial%20challenge%20in%20scientific%20research%2C%20e.g.%20molecular%20dynamics.%20In%20this%0Awork%2C%20we%20intend%20to%20learn%20neural%20samplers%20given%20energy%20functions%20instead%20of%20data%0Asampled%20from%20the%20Boltzmann%20distribution.%20By%20learning%20the%20energies%20of%20the%20noised%0Adata%2C%20we%20propose%20a%20diffusion-based%20sampler%2C%20Noised%20Energy%20Matching%2C%20which%0Atheoretically%20has%20lower%20variance%20and%20more%20complexity%20compared%20to%20related%20works.%0AFurthermore%2C%20a%20novel%20bootstrapping%20technique%20is%20applied%20to%20NEM%20to%20balance%0Abetween%20bias%20and%20variance.%20We%20evaluate%20NEM%20and%20BNEM%20on%20a%202-dimensional%2040%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20a%204-particle%20double-well%20potential%20%28DW-4%29.%20The%0Aexperimental%20results%20demonstrate%20that%20BNEM%20can%20achieve%20state-of-the-art%0Aperformance%20while%20being%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09787v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBNEM%253A%2520A%2520Boltzmann%2520Sampler%2520Based%2520on%2520Bootstrapped%2520Noised%2520Energy%2520Matching%26entry.906535625%3DRuiKang%2520OuYang%2520and%2520Bo%2520Qiang%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Developing%2520an%2520efficient%2520sampler%2520capable%2520of%2520generating%2520independent%2520and%250Aidentically%2520distributed%2520%2528IID%2529%2520samples%2520from%2520a%2520Boltzmann%2520distribution%2520is%2520a%250Acrucial%2520challenge%2520in%2520scientific%2520research%252C%2520e.g.%2520molecular%2520dynamics.%2520In%2520this%250Awork%252C%2520we%2520intend%2520to%2520learn%2520neural%2520samplers%2520given%2520energy%2520functions%2520instead%2520of%2520data%250Asampled%2520from%2520the%2520Boltzmann%2520distribution.%2520By%2520learning%2520the%2520energies%2520of%2520the%2520noised%250Adata%252C%2520we%2520propose%2520a%2520diffusion-based%2520sampler%252C%2520Noised%2520Energy%2520Matching%252C%2520which%250Atheoretically%2520has%2520lower%2520variance%2520and%2520more%2520complexity%2520compared%2520to%2520related%2520works.%250AFurthermore%252C%2520a%2520novel%2520bootstrapping%2520technique%2520is%2520applied%2520to%2520NEM%2520to%2520balance%250Abetween%2520bias%2520and%2520variance.%2520We%2520evaluate%2520NEM%2520and%2520BNEM%2520on%2520a%25202-dimensional%252040%250AGaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520and%2520a%25204-particle%2520double-well%2520potential%2520%2528DW-4%2529.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520BNEM%2520can%2520achieve%2520state-of-the-art%250Aperformance%2520while%2520being%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09787v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching&entry.906535625=RuiKang%20OuYang%20and%20Bo%20Qiang%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Developing%20an%20efficient%20sampler%20capable%20of%20generating%20independent%20and%0Aidentically%20distributed%20%28IID%29%20samples%20from%20a%20Boltzmann%20distribution%20is%20a%0Acrucial%20challenge%20in%20scientific%20research%2C%20e.g.%20molecular%20dynamics.%20In%20this%0Awork%2C%20we%20intend%20to%20learn%20neural%20samplers%20given%20energy%20functions%20instead%20of%20data%0Asampled%20from%20the%20Boltzmann%20distribution.%20By%20learning%20the%20energies%20of%20the%20noised%0Adata%2C%20we%20propose%20a%20diffusion-based%20sampler%2C%20Noised%20Energy%20Matching%2C%20which%0Atheoretically%20has%20lower%20variance%20and%20more%20complexity%20compared%20to%20related%20works.%0AFurthermore%2C%20a%20novel%20bootstrapping%20technique%20is%20applied%20to%20NEM%20to%20balance%0Abetween%20bias%20and%20variance.%20We%20evaluate%20NEM%20and%20BNEM%20on%20a%202-dimensional%2040%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20a%204-particle%20double-well%20potential%20%28DW-4%29.%20The%0Aexperimental%20results%20demonstrate%20that%20BNEM%20can%20achieve%20state-of-the-art%0Aperformance%20while%20being%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09787v4&entry.124074799=Read"},
{"title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for\n  Reasoning", "author": "Bohan Wang and Zhongqi Yue and Fengda Zhang and Shuo Chen and Li'an Bi and Junzhe Zhang and Xue Song and Kennard Yanting Chan and Jiachun Pan and Weijia Wu and Mingze Zhou and Wang Lin and Kaihang Pan and Saining Zhang and Liyu Jia and Wentao Hu and Wei Zhao and Hanwang Zhang", "abstract": "  We completely discard the conventional spatial prior in image representation\nand introduce a novel discrete visual tokenizer: Self-consistency Tokenizer\n(Selftok). At its design core, we compose an autoregressive (AR) prior --\nmirroring the causal structure of language -- into visual tokens by using the\nreverse diffusion process of image generation. The AR property makes Selftok\nfundamentally distinct from traditional spatial tokens in the following two key\nways: - Selftok offers an elegant and minimalist approach to unify diffusion\nand AR for vision-language models (VLMs): By representing images with Selftok\ntokens, we can train a VLM using a purely discrete autoregressive architecture\n-- like that in LLMs -- without requiring additional modules or training\nobjectives. - We theoretically show that the AR prior satisfies the Bellman\nequation, whereas the spatial prior does not. Therefore, Selftok supports\nreinforcement learning (RL) for visual generation with effectiveness comparable\nto that achieved in LLMs. Besides the AR property, Selftok is also a SoTA\ntokenizer that achieves a favorable trade-off between high-quality\nreconstruction and compression rate. We use Selftok to build a pure AR VLM for\nboth visual comprehension and generation tasks. Impressively, without using any\ntext-image training pairs, a simple policy gradient RL working in the visual\ntokens can significantly boost the visual generation benchmark, surpassing all\nthe existing models by a large margin. Therefore, we believe that Selftok\neffectively addresses the long-standing challenge that visual tokens cannot\nsupport effective RL. When combined with the well-established strengths of RL\nin LLMs, this brings us one step closer to realizing a truly multimodal LLM.\nProject Page: https://selftok-team.github.io/report/.\n", "link": "http://arxiv.org/abs/2505.07538v1", "date": "2025-05-12", "relevancy": 2.3714, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6351}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5649}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Visual%20Tokens%20of%20Autoregression%2C%20by%20Diffusion%2C%20and%20for%0A%20%20Reasoning&body=Title%3A%20Discrete%20Visual%20Tokens%20of%20Autoregression%2C%20by%20Diffusion%2C%20and%20for%0A%20%20Reasoning%0AAuthor%3A%20Bohan%20Wang%20and%20Zhongqi%20Yue%20and%20Fengda%20Zhang%20and%20Shuo%20Chen%20and%20Li%27an%20Bi%20and%20Junzhe%20Zhang%20and%20Xue%20Song%20and%20Kennard%20Yanting%20Chan%20and%20Jiachun%20Pan%20and%20Weijia%20Wu%20and%20Mingze%20Zhou%20and%20Wang%20Lin%20and%20Kaihang%20Pan%20and%20Saining%20Zhang%20and%20Liyu%20Jia%20and%20Wentao%20Hu%20and%20Wei%20Zhao%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20We%20completely%20discard%20the%20conventional%20spatial%20prior%20in%20image%20representation%0Aand%20introduce%20a%20novel%20discrete%20visual%20tokenizer%3A%20Self-consistency%20Tokenizer%0A%28Selftok%29.%20At%20its%20design%20core%2C%20we%20compose%20an%20autoregressive%20%28AR%29%20prior%20--%0Amirroring%20the%20causal%20structure%20of%20language%20--%20into%20visual%20tokens%20by%20using%20the%0Areverse%20diffusion%20process%20of%20image%20generation.%20The%20AR%20property%20makes%20Selftok%0Afundamentally%20distinct%20from%20traditional%20spatial%20tokens%20in%20the%20following%20two%20key%0Aways%3A%20-%20Selftok%20offers%20an%20elegant%20and%20minimalist%20approach%20to%20unify%20diffusion%0Aand%20AR%20for%20vision-language%20models%20%28VLMs%29%3A%20By%20representing%20images%20with%20Selftok%0Atokens%2C%20we%20can%20train%20a%20VLM%20using%20a%20purely%20discrete%20autoregressive%20architecture%0A--%20like%20that%20in%20LLMs%20--%20without%20requiring%20additional%20modules%20or%20training%0Aobjectives.%20-%20We%20theoretically%20show%20that%20the%20AR%20prior%20satisfies%20the%20Bellman%0Aequation%2C%20whereas%20the%20spatial%20prior%20does%20not.%20Therefore%2C%20Selftok%20supports%0Areinforcement%20learning%20%28RL%29%20for%20visual%20generation%20with%20effectiveness%20comparable%0Ato%20that%20achieved%20in%20LLMs.%20Besides%20the%20AR%20property%2C%20Selftok%20is%20also%20a%20SoTA%0Atokenizer%20that%20achieves%20a%20favorable%20trade-off%20between%20high-quality%0Areconstruction%20and%20compression%20rate.%20We%20use%20Selftok%20to%20build%20a%20pure%20AR%20VLM%20for%0Aboth%20visual%20comprehension%20and%20generation%20tasks.%20Impressively%2C%20without%20using%20any%0Atext-image%20training%20pairs%2C%20a%20simple%20policy%20gradient%20RL%20working%20in%20the%20visual%0Atokens%20can%20significantly%20boost%20the%20visual%20generation%20benchmark%2C%20surpassing%20all%0Athe%20existing%20models%20by%20a%20large%20margin.%20Therefore%2C%20we%20believe%20that%20Selftok%0Aeffectively%20addresses%20the%20long-standing%20challenge%20that%20visual%20tokens%20cannot%0Asupport%20effective%20RL.%20When%20combined%20with%20the%20well-established%20strengths%20of%20RL%0Ain%20LLMs%2C%20this%20brings%20us%20one%20step%20closer%20to%20realizing%20a%20truly%20multimodal%20LLM.%0AProject%20Page%3A%20https%3A//selftok-team.github.io/report/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Visual%2520Tokens%2520of%2520Autoregression%252C%2520by%2520Diffusion%252C%2520and%2520for%250A%2520%2520Reasoning%26entry.906535625%3DBohan%2520Wang%2520and%2520Zhongqi%2520Yue%2520and%2520Fengda%2520Zhang%2520and%2520Shuo%2520Chen%2520and%2520Li%2527an%2520Bi%2520and%2520Junzhe%2520Zhang%2520and%2520Xue%2520Song%2520and%2520Kennard%2520Yanting%2520Chan%2520and%2520Jiachun%2520Pan%2520and%2520Weijia%2520Wu%2520and%2520Mingze%2520Zhou%2520and%2520Wang%2520Lin%2520and%2520Kaihang%2520Pan%2520and%2520Saining%2520Zhang%2520and%2520Liyu%2520Jia%2520and%2520Wentao%2520Hu%2520and%2520Wei%2520Zhao%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520We%2520completely%2520discard%2520the%2520conventional%2520spatial%2520prior%2520in%2520image%2520representation%250Aand%2520introduce%2520a%2520novel%2520discrete%2520visual%2520tokenizer%253A%2520Self-consistency%2520Tokenizer%250A%2528Selftok%2529.%2520At%2520its%2520design%2520core%252C%2520we%2520compose%2520an%2520autoregressive%2520%2528AR%2529%2520prior%2520--%250Amirroring%2520the%2520causal%2520structure%2520of%2520language%2520--%2520into%2520visual%2520tokens%2520by%2520using%2520the%250Areverse%2520diffusion%2520process%2520of%2520image%2520generation.%2520The%2520AR%2520property%2520makes%2520Selftok%250Afundamentally%2520distinct%2520from%2520traditional%2520spatial%2520tokens%2520in%2520the%2520following%2520two%2520key%250Aways%253A%2520-%2520Selftok%2520offers%2520an%2520elegant%2520and%2520minimalist%2520approach%2520to%2520unify%2520diffusion%250Aand%2520AR%2520for%2520vision-language%2520models%2520%2528VLMs%2529%253A%2520By%2520representing%2520images%2520with%2520Selftok%250Atokens%252C%2520we%2520can%2520train%2520a%2520VLM%2520using%2520a%2520purely%2520discrete%2520autoregressive%2520architecture%250A--%2520like%2520that%2520in%2520LLMs%2520--%2520without%2520requiring%2520additional%2520modules%2520or%2520training%250Aobjectives.%2520-%2520We%2520theoretically%2520show%2520that%2520the%2520AR%2520prior%2520satisfies%2520the%2520Bellman%250Aequation%252C%2520whereas%2520the%2520spatial%2520prior%2520does%2520not.%2520Therefore%252C%2520Selftok%2520supports%250Areinforcement%2520learning%2520%2528RL%2529%2520for%2520visual%2520generation%2520with%2520effectiveness%2520comparable%250Ato%2520that%2520achieved%2520in%2520LLMs.%2520Besides%2520the%2520AR%2520property%252C%2520Selftok%2520is%2520also%2520a%2520SoTA%250Atokenizer%2520that%2520achieves%2520a%2520favorable%2520trade-off%2520between%2520high-quality%250Areconstruction%2520and%2520compression%2520rate.%2520We%2520use%2520Selftok%2520to%2520build%2520a%2520pure%2520AR%2520VLM%2520for%250Aboth%2520visual%2520comprehension%2520and%2520generation%2520tasks.%2520Impressively%252C%2520without%2520using%2520any%250Atext-image%2520training%2520pairs%252C%2520a%2520simple%2520policy%2520gradient%2520RL%2520working%2520in%2520the%2520visual%250Atokens%2520can%2520significantly%2520boost%2520the%2520visual%2520generation%2520benchmark%252C%2520surpassing%2520all%250Athe%2520existing%2520models%2520by%2520a%2520large%2520margin.%2520Therefore%252C%2520we%2520believe%2520that%2520Selftok%250Aeffectively%2520addresses%2520the%2520long-standing%2520challenge%2520that%2520visual%2520tokens%2520cannot%250Asupport%2520effective%2520RL.%2520When%2520combined%2520with%2520the%2520well-established%2520strengths%2520of%2520RL%250Ain%2520LLMs%252C%2520this%2520brings%2520us%2520one%2520step%2520closer%2520to%2520realizing%2520a%2520truly%2520multimodal%2520LLM.%250AProject%2520Page%253A%2520https%253A//selftok-team.github.io/report/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Visual%20Tokens%20of%20Autoregression%2C%20by%20Diffusion%2C%20and%20for%0A%20%20Reasoning&entry.906535625=Bohan%20Wang%20and%20Zhongqi%20Yue%20and%20Fengda%20Zhang%20and%20Shuo%20Chen%20and%20Li%27an%20Bi%20and%20Junzhe%20Zhang%20and%20Xue%20Song%20and%20Kennard%20Yanting%20Chan%20and%20Jiachun%20Pan%20and%20Weijia%20Wu%20and%20Mingze%20Zhou%20and%20Wang%20Lin%20and%20Kaihang%20Pan%20and%20Saining%20Zhang%20and%20Liyu%20Jia%20and%20Wentao%20Hu%20and%20Wei%20Zhao%20and%20Hanwang%20Zhang&entry.1292438233=%20%20We%20completely%20discard%20the%20conventional%20spatial%20prior%20in%20image%20representation%0Aand%20introduce%20a%20novel%20discrete%20visual%20tokenizer%3A%20Self-consistency%20Tokenizer%0A%28Selftok%29.%20At%20its%20design%20core%2C%20we%20compose%20an%20autoregressive%20%28AR%29%20prior%20--%0Amirroring%20the%20causal%20structure%20of%20language%20--%20into%20visual%20tokens%20by%20using%20the%0Areverse%20diffusion%20process%20of%20image%20generation.%20The%20AR%20property%20makes%20Selftok%0Afundamentally%20distinct%20from%20traditional%20spatial%20tokens%20in%20the%20following%20two%20key%0Aways%3A%20-%20Selftok%20offers%20an%20elegant%20and%20minimalist%20approach%20to%20unify%20diffusion%0Aand%20AR%20for%20vision-language%20models%20%28VLMs%29%3A%20By%20representing%20images%20with%20Selftok%0Atokens%2C%20we%20can%20train%20a%20VLM%20using%20a%20purely%20discrete%20autoregressive%20architecture%0A--%20like%20that%20in%20LLMs%20--%20without%20requiring%20additional%20modules%20or%20training%0Aobjectives.%20-%20We%20theoretically%20show%20that%20the%20AR%20prior%20satisfies%20the%20Bellman%0Aequation%2C%20whereas%20the%20spatial%20prior%20does%20not.%20Therefore%2C%20Selftok%20supports%0Areinforcement%20learning%20%28RL%29%20for%20visual%20generation%20with%20effectiveness%20comparable%0Ato%20that%20achieved%20in%20LLMs.%20Besides%20the%20AR%20property%2C%20Selftok%20is%20also%20a%20SoTA%0Atokenizer%20that%20achieves%20a%20favorable%20trade-off%20between%20high-quality%0Areconstruction%20and%20compression%20rate.%20We%20use%20Selftok%20to%20build%20a%20pure%20AR%20VLM%20for%0Aboth%20visual%20comprehension%20and%20generation%20tasks.%20Impressively%2C%20without%20using%20any%0Atext-image%20training%20pairs%2C%20a%20simple%20policy%20gradient%20RL%20working%20in%20the%20visual%0Atokens%20can%20significantly%20boost%20the%20visual%20generation%20benchmark%2C%20surpassing%20all%0Athe%20existing%20models%20by%20a%20large%20margin.%20Therefore%2C%20we%20believe%20that%20Selftok%0Aeffectively%20addresses%20the%20long-standing%20challenge%20that%20visual%20tokens%20cannot%0Asupport%20effective%20RL.%20When%20combined%20with%20the%20well-established%20strengths%20of%20RL%0Ain%20LLMs%2C%20this%20brings%20us%20one%20step%20closer%20to%20realizing%20a%20truly%20multimodal%20LLM.%0AProject%20Page%3A%20https%3A//selftok-team.github.io/report/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07538v1&entry.124074799=Read"},
{"title": "Learning Penalty for Optimal Partitioning via Automatic Feature\n  Extraction", "author": "Tung L Nguyen and Toby Hocking", "abstract": "  Changepoint detection identifies significant shifts in data sequences, making\nit important in areas like finance, genetics, and healthcare. The Optimal\nPartitioning algorithms efficiently detect these changes, using a penalty\nparameter to limit the changepoints number. Determining the appropriate value\nfor this penalty can be challenging. Traditionally, this process involved\nmanually extracting statistical features, such as sequence length or variance\nto make the prediction. This study proposes a novel approach that uses\nrecurrent neural networks to learn this penalty directly from raw sequences by\nautomatically extracting features. Experiments conducted on 20 benchmark\ngenomic datasets show that this novel method surpasses traditional methods in\npartitioning accuracy in most cases.\n", "link": "http://arxiv.org/abs/2505.07413v1", "date": "2025-05-12", "relevancy": 2.3595, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.475}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4711}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Penalty%20for%20Optimal%20Partitioning%20via%20Automatic%20Feature%0A%20%20Extraction&body=Title%3A%20Learning%20Penalty%20for%20Optimal%20Partitioning%20via%20Automatic%20Feature%0A%20%20Extraction%0AAuthor%3A%20Tung%20L%20Nguyen%20and%20Toby%20Hocking%0AAbstract%3A%20%20%20Changepoint%20detection%20identifies%20significant%20shifts%20in%20data%20sequences%2C%20making%0Ait%20important%20in%20areas%20like%20finance%2C%20genetics%2C%20and%20healthcare.%20The%20Optimal%0APartitioning%20algorithms%20efficiently%20detect%20these%20changes%2C%20using%20a%20penalty%0Aparameter%20to%20limit%20the%20changepoints%20number.%20Determining%20the%20appropriate%20value%0Afor%20this%20penalty%20can%20be%20challenging.%20Traditionally%2C%20this%20process%20involved%0Amanually%20extracting%20statistical%20features%2C%20such%20as%20sequence%20length%20or%20variance%0Ato%20make%20the%20prediction.%20This%20study%20proposes%20a%20novel%20approach%20that%20uses%0Arecurrent%20neural%20networks%20to%20learn%20this%20penalty%20directly%20from%20raw%20sequences%20by%0Aautomatically%20extracting%20features.%20Experiments%20conducted%20on%2020%20benchmark%0Agenomic%20datasets%20show%20that%20this%20novel%20method%20surpasses%20traditional%20methods%20in%0Apartitioning%20accuracy%20in%20most%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Penalty%2520for%2520Optimal%2520Partitioning%2520via%2520Automatic%2520Feature%250A%2520%2520Extraction%26entry.906535625%3DTung%2520L%2520Nguyen%2520and%2520Toby%2520Hocking%26entry.1292438233%3D%2520%2520Changepoint%2520detection%2520identifies%2520significant%2520shifts%2520in%2520data%2520sequences%252C%2520making%250Ait%2520important%2520in%2520areas%2520like%2520finance%252C%2520genetics%252C%2520and%2520healthcare.%2520The%2520Optimal%250APartitioning%2520algorithms%2520efficiently%2520detect%2520these%2520changes%252C%2520using%2520a%2520penalty%250Aparameter%2520to%2520limit%2520the%2520changepoints%2520number.%2520Determining%2520the%2520appropriate%2520value%250Afor%2520this%2520penalty%2520can%2520be%2520challenging.%2520Traditionally%252C%2520this%2520process%2520involved%250Amanually%2520extracting%2520statistical%2520features%252C%2520such%2520as%2520sequence%2520length%2520or%2520variance%250Ato%2520make%2520the%2520prediction.%2520This%2520study%2520proposes%2520a%2520novel%2520approach%2520that%2520uses%250Arecurrent%2520neural%2520networks%2520to%2520learn%2520this%2520penalty%2520directly%2520from%2520raw%2520sequences%2520by%250Aautomatically%2520extracting%2520features.%2520Experiments%2520conducted%2520on%252020%2520benchmark%250Agenomic%2520datasets%2520show%2520that%2520this%2520novel%2520method%2520surpasses%2520traditional%2520methods%2520in%250Apartitioning%2520accuracy%2520in%2520most%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Penalty%20for%20Optimal%20Partitioning%20via%20Automatic%20Feature%0A%20%20Extraction&entry.906535625=Tung%20L%20Nguyen%20and%20Toby%20Hocking&entry.1292438233=%20%20Changepoint%20detection%20identifies%20significant%20shifts%20in%20data%20sequences%2C%20making%0Ait%20important%20in%20areas%20like%20finance%2C%20genetics%2C%20and%20healthcare.%20The%20Optimal%0APartitioning%20algorithms%20efficiently%20detect%20these%20changes%2C%20using%20a%20penalty%0Aparameter%20to%20limit%20the%20changepoints%20number.%20Determining%20the%20appropriate%20value%0Afor%20this%20penalty%20can%20be%20challenging.%20Traditionally%2C%20this%20process%20involved%0Amanually%20extracting%20statistical%20features%2C%20such%20as%20sequence%20length%20or%20variance%0Ato%20make%20the%20prediction.%20This%20study%20proposes%20a%20novel%20approach%20that%20uses%0Arecurrent%20neural%20networks%20to%20learn%20this%20penalty%20directly%20from%20raw%20sequences%20by%0Aautomatically%20extracting%20features.%20Experiments%20conducted%20on%2020%20benchmark%0Agenomic%20datasets%20show%20that%20this%20novel%20method%20surpasses%20traditional%20methods%20in%0Apartitioning%20accuracy%20in%20most%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07413v1&entry.124074799=Read"},
{"title": "Skeletonization of neuronal processes using Discrete Morse techniques\n  from computational topology", "author": "Samik Banerjee and Caleb Stam and Daniel J. Tward and Steven Savoia and Yusu Wang and Partha P. Mitra", "abstract": "  To understand biological intelligence we need to map neuronal networks in\nvertebrate brains. Mapping mesoscale neural circuitry is done using injections\nof tracers that label groups of neurons whose axons project to different brain\nregions. Since many neurons are labeled, it is difficult to follow individual\naxons. Previous approaches have instead quantified the regional projections\nusing the total label intensity within a region. However, such a quantification\nis not biologically meaningful. We propose a new approach better connected to\nthe underlying neurons by skeletonizing labeled axon fragments and then\nestimating a volumetric length density. Our approach uses a combination of deep\nnets and the Discrete Morse (DM) technique from computational topology. This\ntechnique takes into account nonlocal connectivity information and therefore\nprovides noise-robustness. We demonstrate the utility and scalability of the\napproach on whole-brain tracer injected data. We also define and illustrate an\ninformation theoretic measure that quantifies the additional information\nobtained, compared to the skeletonized tracer injection fragments, when\nindividual axon morphologies are available. Our approach is the first\napplication of the DM technique to computational neuroanatomy. It can help\nbridge between single-axon skeletons and tracer injections, two important data\ntypes in mapping neural networks in vertebrates.\n", "link": "http://arxiv.org/abs/2505.07754v1", "date": "2025-05-12", "relevancy": 2.3545, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4881}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4642}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeletonization%20of%20neuronal%20processes%20using%20Discrete%20Morse%20techniques%0A%20%20from%20computational%20topology&body=Title%3A%20Skeletonization%20of%20neuronal%20processes%20using%20Discrete%20Morse%20techniques%0A%20%20from%20computational%20topology%0AAuthor%3A%20Samik%20Banerjee%20and%20Caleb%20Stam%20and%20Daniel%20J.%20Tward%20and%20Steven%20Savoia%20and%20Yusu%20Wang%20and%20Partha%20P.%20Mitra%0AAbstract%3A%20%20%20To%20understand%20biological%20intelligence%20we%20need%20to%20map%20neuronal%20networks%20in%0Avertebrate%20brains.%20Mapping%20mesoscale%20neural%20circuitry%20is%20done%20using%20injections%0Aof%20tracers%20that%20label%20groups%20of%20neurons%20whose%20axons%20project%20to%20different%20brain%0Aregions.%20Since%20many%20neurons%20are%20labeled%2C%20it%20is%20difficult%20to%20follow%20individual%0Aaxons.%20Previous%20approaches%20have%20instead%20quantified%20the%20regional%20projections%0Ausing%20the%20total%20label%20intensity%20within%20a%20region.%20However%2C%20such%20a%20quantification%0Ais%20not%20biologically%20meaningful.%20We%20propose%20a%20new%20approach%20better%20connected%20to%0Athe%20underlying%20neurons%20by%20skeletonizing%20labeled%20axon%20fragments%20and%20then%0Aestimating%20a%20volumetric%20length%20density.%20Our%20approach%20uses%20a%20combination%20of%20deep%0Anets%20and%20the%20Discrete%20Morse%20%28DM%29%20technique%20from%20computational%20topology.%20This%0Atechnique%20takes%20into%20account%20nonlocal%20connectivity%20information%20and%20therefore%0Aprovides%20noise-robustness.%20We%20demonstrate%20the%20utility%20and%20scalability%20of%20the%0Aapproach%20on%20whole-brain%20tracer%20injected%20data.%20We%20also%20define%20and%20illustrate%20an%0Ainformation%20theoretic%20measure%20that%20quantifies%20the%20additional%20information%0Aobtained%2C%20compared%20to%20the%20skeletonized%20tracer%20injection%20fragments%2C%20when%0Aindividual%20axon%20morphologies%20are%20available.%20Our%20approach%20is%20the%20first%0Aapplication%20of%20the%20DM%20technique%20to%20computational%20neuroanatomy.%20It%20can%20help%0Abridge%20between%20single-axon%20skeletons%20and%20tracer%20injections%2C%20two%20important%20data%0Atypes%20in%20mapping%20neural%20networks%20in%20vertebrates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeletonization%2520of%2520neuronal%2520processes%2520using%2520Discrete%2520Morse%2520techniques%250A%2520%2520from%2520computational%2520topology%26entry.906535625%3DSamik%2520Banerjee%2520and%2520Caleb%2520Stam%2520and%2520Daniel%2520J.%2520Tward%2520and%2520Steven%2520Savoia%2520and%2520Yusu%2520Wang%2520and%2520Partha%2520P.%2520Mitra%26entry.1292438233%3D%2520%2520To%2520understand%2520biological%2520intelligence%2520we%2520need%2520to%2520map%2520neuronal%2520networks%2520in%250Avertebrate%2520brains.%2520Mapping%2520mesoscale%2520neural%2520circuitry%2520is%2520done%2520using%2520injections%250Aof%2520tracers%2520that%2520label%2520groups%2520of%2520neurons%2520whose%2520axons%2520project%2520to%2520different%2520brain%250Aregions.%2520Since%2520many%2520neurons%2520are%2520labeled%252C%2520it%2520is%2520difficult%2520to%2520follow%2520individual%250Aaxons.%2520Previous%2520approaches%2520have%2520instead%2520quantified%2520the%2520regional%2520projections%250Ausing%2520the%2520total%2520label%2520intensity%2520within%2520a%2520region.%2520However%252C%2520such%2520a%2520quantification%250Ais%2520not%2520biologically%2520meaningful.%2520We%2520propose%2520a%2520new%2520approach%2520better%2520connected%2520to%250Athe%2520underlying%2520neurons%2520by%2520skeletonizing%2520labeled%2520axon%2520fragments%2520and%2520then%250Aestimating%2520a%2520volumetric%2520length%2520density.%2520Our%2520approach%2520uses%2520a%2520combination%2520of%2520deep%250Anets%2520and%2520the%2520Discrete%2520Morse%2520%2528DM%2529%2520technique%2520from%2520computational%2520topology.%2520This%250Atechnique%2520takes%2520into%2520account%2520nonlocal%2520connectivity%2520information%2520and%2520therefore%250Aprovides%2520noise-robustness.%2520We%2520demonstrate%2520the%2520utility%2520and%2520scalability%2520of%2520the%250Aapproach%2520on%2520whole-brain%2520tracer%2520injected%2520data.%2520We%2520also%2520define%2520and%2520illustrate%2520an%250Ainformation%2520theoretic%2520measure%2520that%2520quantifies%2520the%2520additional%2520information%250Aobtained%252C%2520compared%2520to%2520the%2520skeletonized%2520tracer%2520injection%2520fragments%252C%2520when%250Aindividual%2520axon%2520morphologies%2520are%2520available.%2520Our%2520approach%2520is%2520the%2520first%250Aapplication%2520of%2520the%2520DM%2520technique%2520to%2520computational%2520neuroanatomy.%2520It%2520can%2520help%250Abridge%2520between%2520single-axon%2520skeletons%2520and%2520tracer%2520injections%252C%2520two%2520important%2520data%250Atypes%2520in%2520mapping%2520neural%2520networks%2520in%2520vertebrates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeletonization%20of%20neuronal%20processes%20using%20Discrete%20Morse%20techniques%0A%20%20from%20computational%20topology&entry.906535625=Samik%20Banerjee%20and%20Caleb%20Stam%20and%20Daniel%20J.%20Tward%20and%20Steven%20Savoia%20and%20Yusu%20Wang%20and%20Partha%20P.%20Mitra&entry.1292438233=%20%20To%20understand%20biological%20intelligence%20we%20need%20to%20map%20neuronal%20networks%20in%0Avertebrate%20brains.%20Mapping%20mesoscale%20neural%20circuitry%20is%20done%20using%20injections%0Aof%20tracers%20that%20label%20groups%20of%20neurons%20whose%20axons%20project%20to%20different%20brain%0Aregions.%20Since%20many%20neurons%20are%20labeled%2C%20it%20is%20difficult%20to%20follow%20individual%0Aaxons.%20Previous%20approaches%20have%20instead%20quantified%20the%20regional%20projections%0Ausing%20the%20total%20label%20intensity%20within%20a%20region.%20However%2C%20such%20a%20quantification%0Ais%20not%20biologically%20meaningful.%20We%20propose%20a%20new%20approach%20better%20connected%20to%0Athe%20underlying%20neurons%20by%20skeletonizing%20labeled%20axon%20fragments%20and%20then%0Aestimating%20a%20volumetric%20length%20density.%20Our%20approach%20uses%20a%20combination%20of%20deep%0Anets%20and%20the%20Discrete%20Morse%20%28DM%29%20technique%20from%20computational%20topology.%20This%0Atechnique%20takes%20into%20account%20nonlocal%20connectivity%20information%20and%20therefore%0Aprovides%20noise-robustness.%20We%20demonstrate%20the%20utility%20and%20scalability%20of%20the%0Aapproach%20on%20whole-brain%20tracer%20injected%20data.%20We%20also%20define%20and%20illustrate%20an%0Ainformation%20theoretic%20measure%20that%20quantifies%20the%20additional%20information%0Aobtained%2C%20compared%20to%20the%20skeletonized%20tracer%20injection%20fragments%2C%20when%0Aindividual%20axon%20morphologies%20are%20available.%20Our%20approach%20is%20the%20first%0Aapplication%20of%20the%20DM%20technique%20to%20computational%20neuroanatomy.%20It%20can%20help%0Abridge%20between%20single-axon%20skeletons%20and%20tracer%20injections%2C%20two%20important%20data%0Atypes%20in%20mapping%20neural%20networks%20in%20vertebrates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07754v1&entry.124074799=Read"},
{"title": "A Unified Hierarchical Framework for Fine-grained Cross-view\n  Geo-localization over Large-scale Scenarios", "author": "Zhuo Song and Ye Zhang and Kunhong Li and Longguang Wang and Yulan Guo", "abstract": "  Cross-view geo-localization is a promising solution for large-scale\nlocalization problems, requiring the sequential execution of retrieval and\nmetric localization tasks to achieve fine-grained predictions. However,\nexisting methods typically focus on designing standalone models for these two\ntasks, resulting in inefficient collaboration and increased training overhead.\nIn this paper, we propose UnifyGeo, a novel unified hierarchical\ngeo-localization framework that integrates retrieval and metric localization\ntasks into a single network. Specifically, we first employ a unified learning\nstrategy with shared parameters to jointly learn multi-granularity\nrepresentation, facilitating mutual reinforcement between these two tasks.\nSubsequently, we design a re-ranking mechanism guided by a dedicated loss\nfunction, which enhances geo-localization performance by improving both\nretrieval accuracy and metric localization references. Extensive experiments\ndemonstrate that UnifyGeo significantly outperforms the state-of-the-arts in\nboth task-isolated and task-associated settings. Remarkably, on the challenging\nVIGOR benchmark, which supports fine-grained localization evaluation, the\n1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from\n0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively.\nCode will be made publicly available.\n", "link": "http://arxiv.org/abs/2505.07622v1", "date": "2025-05-12", "relevancy": 2.3495, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6218}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5809}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Hierarchical%20Framework%20for%20Fine-grained%20Cross-view%0A%20%20Geo-localization%20over%20Large-scale%20Scenarios&body=Title%3A%20A%20Unified%20Hierarchical%20Framework%20for%20Fine-grained%20Cross-view%0A%20%20Geo-localization%20over%20Large-scale%20Scenarios%0AAuthor%3A%20Zhuo%20Song%20and%20Ye%20Zhang%20and%20Kunhong%20Li%20and%20Longguang%20Wang%20and%20Yulan%20Guo%0AAbstract%3A%20%20%20Cross-view%20geo-localization%20is%20a%20promising%20solution%20for%20large-scale%0Alocalization%20problems%2C%20requiring%20the%20sequential%20execution%20of%20retrieval%20and%0Ametric%20localization%20tasks%20to%20achieve%20fine-grained%20predictions.%20However%2C%0Aexisting%20methods%20typically%20focus%20on%20designing%20standalone%20models%20for%20these%20two%0Atasks%2C%20resulting%20in%20inefficient%20collaboration%20and%20increased%20training%20overhead.%0AIn%20this%20paper%2C%20we%20propose%20UnifyGeo%2C%20a%20novel%20unified%20hierarchical%0Ageo-localization%20framework%20that%20integrates%20retrieval%20and%20metric%20localization%0Atasks%20into%20a%20single%20network.%20Specifically%2C%20we%20first%20employ%20a%20unified%20learning%0Astrategy%20with%20shared%20parameters%20to%20jointly%20learn%20multi-granularity%0Arepresentation%2C%20facilitating%20mutual%20reinforcement%20between%20these%20two%20tasks.%0ASubsequently%2C%20we%20design%20a%20re-ranking%20mechanism%20guided%20by%20a%20dedicated%20loss%0Afunction%2C%20which%20enhances%20geo-localization%20performance%20by%20improving%20both%0Aretrieval%20accuracy%20and%20metric%20localization%20references.%20Extensive%20experiments%0Ademonstrate%20that%20UnifyGeo%20significantly%20outperforms%20the%20state-of-the-arts%20in%0Aboth%20task-isolated%20and%20task-associated%20settings.%20Remarkably%2C%20on%20the%20challenging%0AVIGOR%20benchmark%2C%20which%20supports%20fine-grained%20localization%20evaluation%2C%20the%0A1-meter-level%20localization%20recall%20rate%20improves%20from%201.53%5C%25%20to%2039.64%5C%25%20and%20from%0A0.43%5C%25%20to%2025.58%5C%25%20under%20same-area%20and%20cross-area%20evaluations%2C%20respectively.%0ACode%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Hierarchical%2520Framework%2520for%2520Fine-grained%2520Cross-view%250A%2520%2520Geo-localization%2520over%2520Large-scale%2520Scenarios%26entry.906535625%3DZhuo%2520Song%2520and%2520Ye%2520Zhang%2520and%2520Kunhong%2520Li%2520and%2520Longguang%2520Wang%2520and%2520Yulan%2520Guo%26entry.1292438233%3D%2520%2520Cross-view%2520geo-localization%2520is%2520a%2520promising%2520solution%2520for%2520large-scale%250Alocalization%2520problems%252C%2520requiring%2520the%2520sequential%2520execution%2520of%2520retrieval%2520and%250Ametric%2520localization%2520tasks%2520to%2520achieve%2520fine-grained%2520predictions.%2520However%252C%250Aexisting%2520methods%2520typically%2520focus%2520on%2520designing%2520standalone%2520models%2520for%2520these%2520two%250Atasks%252C%2520resulting%2520in%2520inefficient%2520collaboration%2520and%2520increased%2520training%2520overhead.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520UnifyGeo%252C%2520a%2520novel%2520unified%2520hierarchical%250Ageo-localization%2520framework%2520that%2520integrates%2520retrieval%2520and%2520metric%2520localization%250Atasks%2520into%2520a%2520single%2520network.%2520Specifically%252C%2520we%2520first%2520employ%2520a%2520unified%2520learning%250Astrategy%2520with%2520shared%2520parameters%2520to%2520jointly%2520learn%2520multi-granularity%250Arepresentation%252C%2520facilitating%2520mutual%2520reinforcement%2520between%2520these%2520two%2520tasks.%250ASubsequently%252C%2520we%2520design%2520a%2520re-ranking%2520mechanism%2520guided%2520by%2520a%2520dedicated%2520loss%250Afunction%252C%2520which%2520enhances%2520geo-localization%2520performance%2520by%2520improving%2520both%250Aretrieval%2520accuracy%2520and%2520metric%2520localization%2520references.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520UnifyGeo%2520significantly%2520outperforms%2520the%2520state-of-the-arts%2520in%250Aboth%2520task-isolated%2520and%2520task-associated%2520settings.%2520Remarkably%252C%2520on%2520the%2520challenging%250AVIGOR%2520benchmark%252C%2520which%2520supports%2520fine-grained%2520localization%2520evaluation%252C%2520the%250A1-meter-level%2520localization%2520recall%2520rate%2520improves%2520from%25201.53%255C%2525%2520to%252039.64%255C%2525%2520and%2520from%250A0.43%255C%2525%2520to%252025.58%255C%2525%2520under%2520same-area%2520and%2520cross-area%2520evaluations%252C%2520respectively.%250ACode%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Hierarchical%20Framework%20for%20Fine-grained%20Cross-view%0A%20%20Geo-localization%20over%20Large-scale%20Scenarios&entry.906535625=Zhuo%20Song%20and%20Ye%20Zhang%20and%20Kunhong%20Li%20and%20Longguang%20Wang%20and%20Yulan%20Guo&entry.1292438233=%20%20Cross-view%20geo-localization%20is%20a%20promising%20solution%20for%20large-scale%0Alocalization%20problems%2C%20requiring%20the%20sequential%20execution%20of%20retrieval%20and%0Ametric%20localization%20tasks%20to%20achieve%20fine-grained%20predictions.%20However%2C%0Aexisting%20methods%20typically%20focus%20on%20designing%20standalone%20models%20for%20these%20two%0Atasks%2C%20resulting%20in%20inefficient%20collaboration%20and%20increased%20training%20overhead.%0AIn%20this%20paper%2C%20we%20propose%20UnifyGeo%2C%20a%20novel%20unified%20hierarchical%0Ageo-localization%20framework%20that%20integrates%20retrieval%20and%20metric%20localization%0Atasks%20into%20a%20single%20network.%20Specifically%2C%20we%20first%20employ%20a%20unified%20learning%0Astrategy%20with%20shared%20parameters%20to%20jointly%20learn%20multi-granularity%0Arepresentation%2C%20facilitating%20mutual%20reinforcement%20between%20these%20two%20tasks.%0ASubsequently%2C%20we%20design%20a%20re-ranking%20mechanism%20guided%20by%20a%20dedicated%20loss%0Afunction%2C%20which%20enhances%20geo-localization%20performance%20by%20improving%20both%0Aretrieval%20accuracy%20and%20metric%20localization%20references.%20Extensive%20experiments%0Ademonstrate%20that%20UnifyGeo%20significantly%20outperforms%20the%20state-of-the-arts%20in%0Aboth%20task-isolated%20and%20task-associated%20settings.%20Remarkably%2C%20on%20the%20challenging%0AVIGOR%20benchmark%2C%20which%20supports%20fine-grained%20localization%20evaluation%2C%20the%0A1-meter-level%20localization%20recall%20rate%20improves%20from%201.53%5C%25%20to%2039.64%5C%25%20and%20from%0A0.43%5C%25%20to%2025.58%5C%25%20under%20same-area%20and%20cross-area%20evaluations%2C%20respectively.%0ACode%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07622v1&entry.124074799=Read"},
{"title": "ReFeree: Radar-Based Lightweight and Robust Localization using Feature\n  and Free space", "author": "Hogyun Kim and Byunghee Choi and Euncheol Choi and Younggun Cho", "abstract": "  Place recognition plays an important role in achieving robust long-term\nautonomy. Real-world robots face a wide range of weather conditions (e.g.\novercast, heavy rain, and snowing) and most sensors (i.e. camera, LiDAR)\nessentially functioning within or near-visible electromagnetic waves are\nsensitive to adverse weather conditions, making reliable localization\ndifficult. In contrast, radar is gaining traction due to long electromagnetic\nwaves, which are less affected by environmental changes and weather\nindependence. In this work, we propose a radar-based lightweight and robust\nplace recognition. We achieve rotational invariance and lightweight by\nselecting a one-dimensional ring-shaped description and robustness by\nmitigating the impact of false detection utilizing opposite noise\ncharacteristics between free space and feature. In addition, the initial\nheading can be estimated, which can assist in building a SLAM pipeline that\ncombines odometry and registration, which takes into account onboard computing.\nThe proposed method was tested for rigorous validation across various scenarios\n(i.e. single session, multi-session, and different weather conditions). In\nparticular, we validate our descriptor achieving reliable place recognition\nperformance through the results of extreme environments that lacked structural\ninformation such as an OORD dataset.\n", "link": "http://arxiv.org/abs/2410.01325v2", "date": "2025-05-12", "relevancy": 2.3418, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6188}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReFeree%3A%20Radar-Based%20Lightweight%20and%20Robust%20Localization%20using%20Feature%0A%20%20and%20Free%20space&body=Title%3A%20ReFeree%3A%20Radar-Based%20Lightweight%20and%20Robust%20Localization%20using%20Feature%0A%20%20and%20Free%20space%0AAuthor%3A%20Hogyun%20Kim%20and%20Byunghee%20Choi%20and%20Euncheol%20Choi%20and%20Younggun%20Cho%0AAbstract%3A%20%20%20Place%20recognition%20plays%20an%20important%20role%20in%20achieving%20robust%20long-term%0Aautonomy.%20Real-world%20robots%20face%20a%20wide%20range%20of%20weather%20conditions%20%28e.g.%0Aovercast%2C%20heavy%20rain%2C%20and%20snowing%29%20and%20most%20sensors%20%28i.e.%20camera%2C%20LiDAR%29%0Aessentially%20functioning%20within%20or%20near-visible%20electromagnetic%20waves%20are%0Asensitive%20to%20adverse%20weather%20conditions%2C%20making%20reliable%20localization%0Adifficult.%20In%20contrast%2C%20radar%20is%20gaining%20traction%20due%20to%20long%20electromagnetic%0Awaves%2C%20which%20are%20less%20affected%20by%20environmental%20changes%20and%20weather%0Aindependence.%20In%20this%20work%2C%20we%20propose%20a%20radar-based%20lightweight%20and%20robust%0Aplace%20recognition.%20We%20achieve%20rotational%20invariance%20and%20lightweight%20by%0Aselecting%20a%20one-dimensional%20ring-shaped%20description%20and%20robustness%20by%0Amitigating%20the%20impact%20of%20false%20detection%20utilizing%20opposite%20noise%0Acharacteristics%20between%20free%20space%20and%20feature.%20In%20addition%2C%20the%20initial%0Aheading%20can%20be%20estimated%2C%20which%20can%20assist%20in%20building%20a%20SLAM%20pipeline%20that%0Acombines%20odometry%20and%20registration%2C%20which%20takes%20into%20account%20onboard%20computing.%0AThe%20proposed%20method%20was%20tested%20for%20rigorous%20validation%20across%20various%20scenarios%0A%28i.e.%20single%20session%2C%20multi-session%2C%20and%20different%20weather%20conditions%29.%20In%0Aparticular%2C%20we%20validate%20our%20descriptor%20achieving%20reliable%20place%20recognition%0Aperformance%20through%20the%20results%20of%20extreme%20environments%20that%20lacked%20structural%0Ainformation%20such%20as%20an%20OORD%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReFeree%253A%2520Radar-Based%2520Lightweight%2520and%2520Robust%2520Localization%2520using%2520Feature%250A%2520%2520and%2520Free%2520space%26entry.906535625%3DHogyun%2520Kim%2520and%2520Byunghee%2520Choi%2520and%2520Euncheol%2520Choi%2520and%2520Younggun%2520Cho%26entry.1292438233%3D%2520%2520Place%2520recognition%2520plays%2520an%2520important%2520role%2520in%2520achieving%2520robust%2520long-term%250Aautonomy.%2520Real-world%2520robots%2520face%2520a%2520wide%2520range%2520of%2520weather%2520conditions%2520%2528e.g.%250Aovercast%252C%2520heavy%2520rain%252C%2520and%2520snowing%2529%2520and%2520most%2520sensors%2520%2528i.e.%2520camera%252C%2520LiDAR%2529%250Aessentially%2520functioning%2520within%2520or%2520near-visible%2520electromagnetic%2520waves%2520are%250Asensitive%2520to%2520adverse%2520weather%2520conditions%252C%2520making%2520reliable%2520localization%250Adifficult.%2520In%2520contrast%252C%2520radar%2520is%2520gaining%2520traction%2520due%2520to%2520long%2520electromagnetic%250Awaves%252C%2520which%2520are%2520less%2520affected%2520by%2520environmental%2520changes%2520and%2520weather%250Aindependence.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520radar-based%2520lightweight%2520and%2520robust%250Aplace%2520recognition.%2520We%2520achieve%2520rotational%2520invariance%2520and%2520lightweight%2520by%250Aselecting%2520a%2520one-dimensional%2520ring-shaped%2520description%2520and%2520robustness%2520by%250Amitigating%2520the%2520impact%2520of%2520false%2520detection%2520utilizing%2520opposite%2520noise%250Acharacteristics%2520between%2520free%2520space%2520and%2520feature.%2520In%2520addition%252C%2520the%2520initial%250Aheading%2520can%2520be%2520estimated%252C%2520which%2520can%2520assist%2520in%2520building%2520a%2520SLAM%2520pipeline%2520that%250Acombines%2520odometry%2520and%2520registration%252C%2520which%2520takes%2520into%2520account%2520onboard%2520computing.%250AThe%2520proposed%2520method%2520was%2520tested%2520for%2520rigorous%2520validation%2520across%2520various%2520scenarios%250A%2528i.e.%2520single%2520session%252C%2520multi-session%252C%2520and%2520different%2520weather%2520conditions%2529.%2520In%250Aparticular%252C%2520we%2520validate%2520our%2520descriptor%2520achieving%2520reliable%2520place%2520recognition%250Aperformance%2520through%2520the%2520results%2520of%2520extreme%2520environments%2520that%2520lacked%2520structural%250Ainformation%2520such%2520as%2520an%2520OORD%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReFeree%3A%20Radar-Based%20Lightweight%20and%20Robust%20Localization%20using%20Feature%0A%20%20and%20Free%20space&entry.906535625=Hogyun%20Kim%20and%20Byunghee%20Choi%20and%20Euncheol%20Choi%20and%20Younggun%20Cho&entry.1292438233=%20%20Place%20recognition%20plays%20an%20important%20role%20in%20achieving%20robust%20long-term%0Aautonomy.%20Real-world%20robots%20face%20a%20wide%20range%20of%20weather%20conditions%20%28e.g.%0Aovercast%2C%20heavy%20rain%2C%20and%20snowing%29%20and%20most%20sensors%20%28i.e.%20camera%2C%20LiDAR%29%0Aessentially%20functioning%20within%20or%20near-visible%20electromagnetic%20waves%20are%0Asensitive%20to%20adverse%20weather%20conditions%2C%20making%20reliable%20localization%0Adifficult.%20In%20contrast%2C%20radar%20is%20gaining%20traction%20due%20to%20long%20electromagnetic%0Awaves%2C%20which%20are%20less%20affected%20by%20environmental%20changes%20and%20weather%0Aindependence.%20In%20this%20work%2C%20we%20propose%20a%20radar-based%20lightweight%20and%20robust%0Aplace%20recognition.%20We%20achieve%20rotational%20invariance%20and%20lightweight%20by%0Aselecting%20a%20one-dimensional%20ring-shaped%20description%20and%20robustness%20by%0Amitigating%20the%20impact%20of%20false%20detection%20utilizing%20opposite%20noise%0Acharacteristics%20between%20free%20space%20and%20feature.%20In%20addition%2C%20the%20initial%0Aheading%20can%20be%20estimated%2C%20which%20can%20assist%20in%20building%20a%20SLAM%20pipeline%20that%0Acombines%20odometry%20and%20registration%2C%20which%20takes%20into%20account%20onboard%20computing.%0AThe%20proposed%20method%20was%20tested%20for%20rigorous%20validation%20across%20various%20scenarios%0A%28i.e.%20single%20session%2C%20multi-session%2C%20and%20different%20weather%20conditions%29.%20In%0Aparticular%2C%20we%20validate%20our%20descriptor%20achieving%20reliable%20place%20recognition%0Aperformance%20through%20the%20results%20of%20extreme%20environments%20that%20lacked%20structural%0Ainformation%20such%20as%20an%20OORD%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01325v2&entry.124074799=Read"},
{"title": "H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning", "author": "Yiyang Lu and Yufeng Tian and Zhecheng Yuan and Xianbang Wang and Pu Hua and Zhengrong Xue and Huazhe Xu", "abstract": "  Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce $\\textbf{Triply-Hierarchical Diffusion\nPolicy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H$^{3}$DP contains\n$\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$\naverage relative improvement over baselines across $\\mathbf{44}$ simulation\ntasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.\n", "link": "http://arxiv.org/abs/2505.07819v1", "date": "2025-05-12", "relevancy": 2.3403, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6205}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5857}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H%24%5E%7B%5Cmathbf%7B3%7D%7D%24DP%3A%20Triply-Hierarchical%20Diffusion%20Policy%20for%20Visuomotor%0A%20%20Learning&body=Title%3A%20H%24%5E%7B%5Cmathbf%7B3%7D%7D%24DP%3A%20Triply-Hierarchical%20Diffusion%20Policy%20for%20Visuomotor%0A%20%20Learning%0AAuthor%3A%20Yiyang%20Lu%20and%20Yufeng%20Tian%20and%20Zhecheng%20Yuan%20and%20Xianbang%20Wang%20and%20Pu%20Hua%20and%20Zhengrong%20Xue%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Visuomotor%20policy%20learning%20has%20witnessed%20substantial%20progress%20in%20robotic%0Amanipulation%2C%20with%20recent%20approaches%20predominantly%20relying%20on%20generative%20models%0Ato%20model%20the%20action%20distribution.%20However%2C%20these%20methods%20often%20overlook%20the%0Acritical%20coupling%20between%20visual%20perception%20and%20action%20prediction.%20In%20this%0Awork%2C%20we%20introduce%20%24%5Ctextbf%7BTriply-Hierarchical%20Diffusion%0APolicy%7D~%28%5Ctextbf%7BH%24%5E%7B%5Cmathbf%7B3%7D%7D%24DP%7D%29%24%2C%20a%20novel%20visuomotor%20learning%20framework%0Athat%20explicitly%20incorporates%20hierarchical%20structures%20to%20strengthen%20the%0Aintegration%20between%20visual%20features%20and%20action%20generation.%20H%24%5E%7B3%7D%24DP%20contains%0A%24%5Cmathbf%7B3%7D%24%20levels%20of%20hierarchy%3A%20%281%29%20depth-aware%20input%20layering%20that%20organizes%0ARGB-D%20observations%20based%20on%20depth%20information%3B%20%282%29%20multi-scale%20visual%0Arepresentations%20that%20encode%20semantic%20features%20at%20varying%20levels%20of%20granularity%3B%0Aand%20%283%29%20a%20hierarchically%20conditioned%20diffusion%20process%20that%20aligns%20the%0Ageneration%20of%20coarse-to-fine%20actions%20with%20corresponding%20visual%20features.%0AExtensive%20experiments%20demonstrate%20that%20H%24%5E%7B3%7D%24DP%20yields%20a%20%24%5Cmathbf%7B%2B27.5%5C%25%7D%24%0Aaverage%20relative%20improvement%20over%20baselines%20across%20%24%5Cmathbf%7B44%7D%24%20simulation%0Atasks%20and%20achieves%20superior%20performance%20in%20%24%5Cmathbf%7B4%7D%24%20challenging%20bimanual%0Areal-world%20manipulation%20tasks.%20Project%20Page%3A%20https%3A//lyy-iiis.github.io/h3dp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH%2524%255E%257B%255Cmathbf%257B3%257D%257D%2524DP%253A%2520Triply-Hierarchical%2520Diffusion%2520Policy%2520for%2520Visuomotor%250A%2520%2520Learning%26entry.906535625%3DYiyang%2520Lu%2520and%2520Yufeng%2520Tian%2520and%2520Zhecheng%2520Yuan%2520and%2520Xianbang%2520Wang%2520and%2520Pu%2520Hua%2520and%2520Zhengrong%2520Xue%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Visuomotor%2520policy%2520learning%2520has%2520witnessed%2520substantial%2520progress%2520in%2520robotic%250Amanipulation%252C%2520with%2520recent%2520approaches%2520predominantly%2520relying%2520on%2520generative%2520models%250Ato%2520model%2520the%2520action%2520distribution.%2520However%252C%2520these%2520methods%2520often%2520overlook%2520the%250Acritical%2520coupling%2520between%2520visual%2520perception%2520and%2520action%2520prediction.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BTriply-Hierarchical%2520Diffusion%250APolicy%257D~%2528%255Ctextbf%257BH%2524%255E%257B%255Cmathbf%257B3%257D%257D%2524DP%257D%2529%2524%252C%2520a%2520novel%2520visuomotor%2520learning%2520framework%250Athat%2520explicitly%2520incorporates%2520hierarchical%2520structures%2520to%2520strengthen%2520the%250Aintegration%2520between%2520visual%2520features%2520and%2520action%2520generation.%2520H%2524%255E%257B3%257D%2524DP%2520contains%250A%2524%255Cmathbf%257B3%257D%2524%2520levels%2520of%2520hierarchy%253A%2520%25281%2529%2520depth-aware%2520input%2520layering%2520that%2520organizes%250ARGB-D%2520observations%2520based%2520on%2520depth%2520information%253B%2520%25282%2529%2520multi-scale%2520visual%250Arepresentations%2520that%2520encode%2520semantic%2520features%2520at%2520varying%2520levels%2520of%2520granularity%253B%250Aand%2520%25283%2529%2520a%2520hierarchically%2520conditioned%2520diffusion%2520process%2520that%2520aligns%2520the%250Ageneration%2520of%2520coarse-to-fine%2520actions%2520with%2520corresponding%2520visual%2520features.%250AExtensive%2520experiments%2520demonstrate%2520that%2520H%2524%255E%257B3%257D%2524DP%2520yields%2520a%2520%2524%255Cmathbf%257B%252B27.5%255C%2525%257D%2524%250Aaverage%2520relative%2520improvement%2520over%2520baselines%2520across%2520%2524%255Cmathbf%257B44%257D%2524%2520simulation%250Atasks%2520and%2520achieves%2520superior%2520performance%2520in%2520%2524%255Cmathbf%257B4%257D%2524%2520challenging%2520bimanual%250Areal-world%2520manipulation%2520tasks.%2520Project%2520Page%253A%2520https%253A//lyy-iiis.github.io/h3dp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H%24%5E%7B%5Cmathbf%7B3%7D%7D%24DP%3A%20Triply-Hierarchical%20Diffusion%20Policy%20for%20Visuomotor%0A%20%20Learning&entry.906535625=Yiyang%20Lu%20and%20Yufeng%20Tian%20and%20Zhecheng%20Yuan%20and%20Xianbang%20Wang%20and%20Pu%20Hua%20and%20Zhengrong%20Xue%20and%20Huazhe%20Xu&entry.1292438233=%20%20Visuomotor%20policy%20learning%20has%20witnessed%20substantial%20progress%20in%20robotic%0Amanipulation%2C%20with%20recent%20approaches%20predominantly%20relying%20on%20generative%20models%0Ato%20model%20the%20action%20distribution.%20However%2C%20these%20methods%20often%20overlook%20the%0Acritical%20coupling%20between%20visual%20perception%20and%20action%20prediction.%20In%20this%0Awork%2C%20we%20introduce%20%24%5Ctextbf%7BTriply-Hierarchical%20Diffusion%0APolicy%7D~%28%5Ctextbf%7BH%24%5E%7B%5Cmathbf%7B3%7D%7D%24DP%7D%29%24%2C%20a%20novel%20visuomotor%20learning%20framework%0Athat%20explicitly%20incorporates%20hierarchical%20structures%20to%20strengthen%20the%0Aintegration%20between%20visual%20features%20and%20action%20generation.%20H%24%5E%7B3%7D%24DP%20contains%0A%24%5Cmathbf%7B3%7D%24%20levels%20of%20hierarchy%3A%20%281%29%20depth-aware%20input%20layering%20that%20organizes%0ARGB-D%20observations%20based%20on%20depth%20information%3B%20%282%29%20multi-scale%20visual%0Arepresentations%20that%20encode%20semantic%20features%20at%20varying%20levels%20of%20granularity%3B%0Aand%20%283%29%20a%20hierarchically%20conditioned%20diffusion%20process%20that%20aligns%20the%0Ageneration%20of%20coarse-to-fine%20actions%20with%20corresponding%20visual%20features.%0AExtensive%20experiments%20demonstrate%20that%20H%24%5E%7B3%7D%24DP%20yields%20a%20%24%5Cmathbf%7B%2B27.5%5C%25%7D%24%0Aaverage%20relative%20improvement%20over%20baselines%20across%20%24%5Cmathbf%7B44%7D%24%20simulation%0Atasks%20and%20achieves%20superior%20performance%20in%20%24%5Cmathbf%7B4%7D%24%20challenging%20bimanual%0Areal-world%20manipulation%20tasks.%20Project%20Page%3A%20https%3A//lyy-iiis.github.io/h3dp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07819v1&entry.124074799=Read"},
{"title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection", "author": "Juan E. Tapia and Fabian Stockhardt and L\u00e1zaro Janier Gonz\u00e1lez-Soler and Christoph Busch", "abstract": "  The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images.\n", "link": "http://arxiv.org/abs/2505.07540v1", "date": "2025-05-12", "relevancy": 2.3323, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4923}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4649}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynID%3A%20Passport%20Synthetic%20Dataset%20for%20Presentation%20Attack%20Detection&body=Title%3A%20SynID%3A%20Passport%20Synthetic%20Dataset%20for%20Presentation%20Attack%20Detection%0AAuthor%3A%20Juan%20E.%20Tapia%20and%20Fabian%20Stockhardt%20and%20L%C3%A1zaro%20Janier%20Gonz%C3%A1lez-Soler%20and%20Christoph%20Busch%0AAbstract%3A%20%20%20The%20demand%20for%20Presentation%20Attack%20Detection%20%28PAD%29%20to%20identify%20fraudulent%20ID%0Adocuments%20in%20remote%20verification%20systems%20has%20significantly%20risen%20in%20recent%0Ayears.%20This%20increase%20is%20driven%20by%20several%20factors%2C%20including%20the%20rise%20of%20remote%0Awork%2C%20online%20purchasing%2C%20migration%2C%20and%20advancements%20in%20synthetic%20images.%0AAdditionally%2C%20we%20have%20noticed%20a%20surge%20in%20the%20number%20of%20attacks%20aimed%20at%20the%0Aenrolment%20process.%20Training%20a%20PAD%20to%20detect%20fake%20ID%20documents%20is%20very%0Achallenging%20because%20of%20the%20limited%20number%20of%20ID%20documents%20available%20due%20to%0Aprivacy%20concerns.%20This%20work%20proposes%20a%20new%20passport%20dataset%20generated%20from%20a%0Ahybrid%20method%20that%20combines%20synthetic%20data%20and%20open-access%20information%20using%0Athe%20ICAO%20requirement%20to%20obtain%20realistic%20training%20and%20testing%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynID%253A%2520Passport%2520Synthetic%2520Dataset%2520for%2520Presentation%2520Attack%2520Detection%26entry.906535625%3DJuan%2520E.%2520Tapia%2520and%2520Fabian%2520Stockhardt%2520and%2520L%25C3%25A1zaro%2520Janier%2520Gonz%25C3%25A1lez-Soler%2520and%2520Christoph%2520Busch%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520Presentation%2520Attack%2520Detection%2520%2528PAD%2529%2520to%2520identify%2520fraudulent%2520ID%250Adocuments%2520in%2520remote%2520verification%2520systems%2520has%2520significantly%2520risen%2520in%2520recent%250Ayears.%2520This%2520increase%2520is%2520driven%2520by%2520several%2520factors%252C%2520including%2520the%2520rise%2520of%2520remote%250Awork%252C%2520online%2520purchasing%252C%2520migration%252C%2520and%2520advancements%2520in%2520synthetic%2520images.%250AAdditionally%252C%2520we%2520have%2520noticed%2520a%2520surge%2520in%2520the%2520number%2520of%2520attacks%2520aimed%2520at%2520the%250Aenrolment%2520process.%2520Training%2520a%2520PAD%2520to%2520detect%2520fake%2520ID%2520documents%2520is%2520very%250Achallenging%2520because%2520of%2520the%2520limited%2520number%2520of%2520ID%2520documents%2520available%2520due%2520to%250Aprivacy%2520concerns.%2520This%2520work%2520proposes%2520a%2520new%2520passport%2520dataset%2520generated%2520from%2520a%250Ahybrid%2520method%2520that%2520combines%2520synthetic%2520data%2520and%2520open-access%2520information%2520using%250Athe%2520ICAO%2520requirement%2520to%2520obtain%2520realistic%2520training%2520and%2520testing%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynID%3A%20Passport%20Synthetic%20Dataset%20for%20Presentation%20Attack%20Detection&entry.906535625=Juan%20E.%20Tapia%20and%20Fabian%20Stockhardt%20and%20L%C3%A1zaro%20Janier%20Gonz%C3%A1lez-Soler%20and%20Christoph%20Busch&entry.1292438233=%20%20The%20demand%20for%20Presentation%20Attack%20Detection%20%28PAD%29%20to%20identify%20fraudulent%20ID%0Adocuments%20in%20remote%20verification%20systems%20has%20significantly%20risen%20in%20recent%0Ayears.%20This%20increase%20is%20driven%20by%20several%20factors%2C%20including%20the%20rise%20of%20remote%0Awork%2C%20online%20purchasing%2C%20migration%2C%20and%20advancements%20in%20synthetic%20images.%0AAdditionally%2C%20we%20have%20noticed%20a%20surge%20in%20the%20number%20of%20attacks%20aimed%20at%20the%0Aenrolment%20process.%20Training%20a%20PAD%20to%20detect%20fake%20ID%20documents%20is%20very%0Achallenging%20because%20of%20the%20limited%20number%20of%20ID%20documents%20available%20due%20to%0Aprivacy%20concerns.%20This%20work%20proposes%20a%20new%20passport%20dataset%20generated%20from%20a%0Ahybrid%20method%20that%20combines%20synthetic%20data%20and%20open-access%20information%20using%0Athe%20ICAO%20requirement%20to%20obtain%20realistic%20training%20and%20testing%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07540v1&entry.124074799=Read"},
{"title": "SICNav-Diffusion: Safe and Interactive Crowd Navigation with Diffusion\n  Trajectory Predictions", "author": "Sepehr Samavi and Anthony Lem and Fumiaki Sato and Sirui Chen and Qiao Gu and Keijiro Yano and Angela P. Schoellig and Florian Shkurti", "abstract": "  To navigate crowds without collisions, robots must interact with humans by\nforecasting their future motion and reacting accordingly. While learning-based\nprediction models have shown success in generating likely human trajectory\npredictions, integrating these stochastic models into a robot controller\npresents several challenges. The controller needs to account for interactive\ncoupling between planned robot motion and human predictions while ensuring both\npredictions and robot actions are safe (i.e. collision-free). To address these\nchallenges, we present a receding horizon crowd navigation method for\nsingle-robot multi-human environments. We first propose a diffusion model to\ngenerate joint trajectory predictions for all humans in the scene. We then\nincorporate these multi-modal predictions into a SICNav Bilevel MPC problem\nthat simultaneously solves for a robot plan (upper-level) and acts as a safety\nfilter to refine the predictions for non-collision (lower-level). Combining\nplanning and prediction refinement into one bilevel problem ensures that the\nrobot plan and human predictions are coupled. We validate the open-loop\ntrajectory prediction performance of our diffusion model on the commonly used\nETH/UCY benchmark and evaluate the closed-loop performance of our robot\nnavigation method in simulation and extensive real-robot experiments\ndemonstrating safe, efficient, and reactive robot motion.\n", "link": "http://arxiv.org/abs/2503.08858v2", "date": "2025-05-12", "relevancy": 2.3181, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6032}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5822}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SICNav-Diffusion%3A%20Safe%20and%20Interactive%20Crowd%20Navigation%20with%20Diffusion%0A%20%20Trajectory%20Predictions&body=Title%3A%20SICNav-Diffusion%3A%20Safe%20and%20Interactive%20Crowd%20Navigation%20with%20Diffusion%0A%20%20Trajectory%20Predictions%0AAuthor%3A%20Sepehr%20Samavi%20and%20Anthony%20Lem%20and%20Fumiaki%20Sato%20and%20Sirui%20Chen%20and%20Qiao%20Gu%20and%20Keijiro%20Yano%20and%20Angela%20P.%20Schoellig%20and%20Florian%20Shkurti%0AAbstract%3A%20%20%20To%20navigate%20crowds%20without%20collisions%2C%20robots%20must%20interact%20with%20humans%20by%0Aforecasting%20their%20future%20motion%20and%20reacting%20accordingly.%20While%20learning-based%0Aprediction%20models%20have%20shown%20success%20in%20generating%20likely%20human%20trajectory%0Apredictions%2C%20integrating%20these%20stochastic%20models%20into%20a%20robot%20controller%0Apresents%20several%20challenges.%20The%20controller%20needs%20to%20account%20for%20interactive%0Acoupling%20between%20planned%20robot%20motion%20and%20human%20predictions%20while%20ensuring%20both%0Apredictions%20and%20robot%20actions%20are%20safe%20%28i.e.%20collision-free%29.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20receding%20horizon%20crowd%20navigation%20method%20for%0Asingle-robot%20multi-human%20environments.%20We%20first%20propose%20a%20diffusion%20model%20to%0Agenerate%20joint%20trajectory%20predictions%20for%20all%20humans%20in%20the%20scene.%20We%20then%0Aincorporate%20these%20multi-modal%20predictions%20into%20a%20SICNav%20Bilevel%20MPC%20problem%0Athat%20simultaneously%20solves%20for%20a%20robot%20plan%20%28upper-level%29%20and%20acts%20as%20a%20safety%0Afilter%20to%20refine%20the%20predictions%20for%20non-collision%20%28lower-level%29.%20Combining%0Aplanning%20and%20prediction%20refinement%20into%20one%20bilevel%20problem%20ensures%20that%20the%0Arobot%20plan%20and%20human%20predictions%20are%20coupled.%20We%20validate%20the%20open-loop%0Atrajectory%20prediction%20performance%20of%20our%20diffusion%20model%20on%20the%20commonly%20used%0AETH/UCY%20benchmark%20and%20evaluate%20the%20closed-loop%20performance%20of%20our%20robot%0Anavigation%20method%20in%20simulation%20and%20extensive%20real-robot%20experiments%0Ademonstrating%20safe%2C%20efficient%2C%20and%20reactive%20robot%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSICNav-Diffusion%253A%2520Safe%2520and%2520Interactive%2520Crowd%2520Navigation%2520with%2520Diffusion%250A%2520%2520Trajectory%2520Predictions%26entry.906535625%3DSepehr%2520Samavi%2520and%2520Anthony%2520Lem%2520and%2520Fumiaki%2520Sato%2520and%2520Sirui%2520Chen%2520and%2520Qiao%2520Gu%2520and%2520Keijiro%2520Yano%2520and%2520Angela%2520P.%2520Schoellig%2520and%2520Florian%2520Shkurti%26entry.1292438233%3D%2520%2520To%2520navigate%2520crowds%2520without%2520collisions%252C%2520robots%2520must%2520interact%2520with%2520humans%2520by%250Aforecasting%2520their%2520future%2520motion%2520and%2520reacting%2520accordingly.%2520While%2520learning-based%250Aprediction%2520models%2520have%2520shown%2520success%2520in%2520generating%2520likely%2520human%2520trajectory%250Apredictions%252C%2520integrating%2520these%2520stochastic%2520models%2520into%2520a%2520robot%2520controller%250Apresents%2520several%2520challenges.%2520The%2520controller%2520needs%2520to%2520account%2520for%2520interactive%250Acoupling%2520between%2520planned%2520robot%2520motion%2520and%2520human%2520predictions%2520while%2520ensuring%2520both%250Apredictions%2520and%2520robot%2520actions%2520are%2520safe%2520%2528i.e.%2520collision-free%2529.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520present%2520a%2520receding%2520horizon%2520crowd%2520navigation%2520method%2520for%250Asingle-robot%2520multi-human%2520environments.%2520We%2520first%2520propose%2520a%2520diffusion%2520model%2520to%250Agenerate%2520joint%2520trajectory%2520predictions%2520for%2520all%2520humans%2520in%2520the%2520scene.%2520We%2520then%250Aincorporate%2520these%2520multi-modal%2520predictions%2520into%2520a%2520SICNav%2520Bilevel%2520MPC%2520problem%250Athat%2520simultaneously%2520solves%2520for%2520a%2520robot%2520plan%2520%2528upper-level%2529%2520and%2520acts%2520as%2520a%2520safety%250Afilter%2520to%2520refine%2520the%2520predictions%2520for%2520non-collision%2520%2528lower-level%2529.%2520Combining%250Aplanning%2520and%2520prediction%2520refinement%2520into%2520one%2520bilevel%2520problem%2520ensures%2520that%2520the%250Arobot%2520plan%2520and%2520human%2520predictions%2520are%2520coupled.%2520We%2520validate%2520the%2520open-loop%250Atrajectory%2520prediction%2520performance%2520of%2520our%2520diffusion%2520model%2520on%2520the%2520commonly%2520used%250AETH/UCY%2520benchmark%2520and%2520evaluate%2520the%2520closed-loop%2520performance%2520of%2520our%2520robot%250Anavigation%2520method%2520in%2520simulation%2520and%2520extensive%2520real-robot%2520experiments%250Ademonstrating%2520safe%252C%2520efficient%252C%2520and%2520reactive%2520robot%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SICNav-Diffusion%3A%20Safe%20and%20Interactive%20Crowd%20Navigation%20with%20Diffusion%0A%20%20Trajectory%20Predictions&entry.906535625=Sepehr%20Samavi%20and%20Anthony%20Lem%20and%20Fumiaki%20Sato%20and%20Sirui%20Chen%20and%20Qiao%20Gu%20and%20Keijiro%20Yano%20and%20Angela%20P.%20Schoellig%20and%20Florian%20Shkurti&entry.1292438233=%20%20To%20navigate%20crowds%20without%20collisions%2C%20robots%20must%20interact%20with%20humans%20by%0Aforecasting%20their%20future%20motion%20and%20reacting%20accordingly.%20While%20learning-based%0Aprediction%20models%20have%20shown%20success%20in%20generating%20likely%20human%20trajectory%0Apredictions%2C%20integrating%20these%20stochastic%20models%20into%20a%20robot%20controller%0Apresents%20several%20challenges.%20The%20controller%20needs%20to%20account%20for%20interactive%0Acoupling%20between%20planned%20robot%20motion%20and%20human%20predictions%20while%20ensuring%20both%0Apredictions%20and%20robot%20actions%20are%20safe%20%28i.e.%20collision-free%29.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20receding%20horizon%20crowd%20navigation%20method%20for%0Asingle-robot%20multi-human%20environments.%20We%20first%20propose%20a%20diffusion%20model%20to%0Agenerate%20joint%20trajectory%20predictions%20for%20all%20humans%20in%20the%20scene.%20We%20then%0Aincorporate%20these%20multi-modal%20predictions%20into%20a%20SICNav%20Bilevel%20MPC%20problem%0Athat%20simultaneously%20solves%20for%20a%20robot%20plan%20%28upper-level%29%20and%20acts%20as%20a%20safety%0Afilter%20to%20refine%20the%20predictions%20for%20non-collision%20%28lower-level%29.%20Combining%0Aplanning%20and%20prediction%20refinement%20into%20one%20bilevel%20problem%20ensures%20that%20the%0Arobot%20plan%20and%20human%20predictions%20are%20coupled.%20We%20validate%20the%20open-loop%0Atrajectory%20prediction%20performance%20of%20our%20diffusion%20model%20on%20the%20commonly%20used%0AETH/UCY%20benchmark%20and%20evaluate%20the%20closed-loop%20performance%20of%20our%20robot%0Anavigation%20method%20in%20simulation%20and%20extensive%20real-robot%20experiments%0Ademonstrating%20safe%2C%20efficient%2C%20and%20reactive%20robot%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08858v2&entry.124074799=Read"},
{"title": "Privacy of SGD under Gaussian or Heavy-Tailed Noise: Guarantees without\n  Gradient Clipping", "author": "Umut \u015eim\u015fekli and Mert G\u00fcrb\u00fczbalaban and Sinan Y\u0131ld\u0131r\u0131m and Lingjiong Zhu", "abstract": "  The injection of heavy-tailed noise into the iterates of stochastic gradient\ndescent (SGD) has garnered growing interest in recent years due to its\ntheoretical and empirical benefits for optimization and generalization.\nHowever, its implications for privacy preservation remain largely unexplored.\nAiming to bridge this gap, we provide differential privacy (DP) guarantees for\nnoisy SGD, when the injected noise follows an $\\alpha$-stable distribution,\nwhich includes a spectrum of heavy-tailed distributions (with infinite\nvariance) as well as the light-tailed Gaussian distribution. Considering the\n$(\\epsilon, \\delta)$-DP framework, we show that SGD with heavy-tailed\nperturbations achieves $(0, O(1/n))$-DP for a broad class of loss functions\nwhich can be non-convex, where $n$ is the number of data points. As a\nremarkable byproduct, contrary to prior work that necessitates bounded\nsensitivity for the gradients or clipping the iterates, our theory can handle\nunbounded gradients without clipping, and reveals that under mild assumptions,\nsuch a projection step is not actually necessary. Our results suggest that,\ngiven other benefits of heavy-tails in optimization, heavy-tailed noising\nschemes can be a viable alternative to their light-tailed counterparts.\n", "link": "http://arxiv.org/abs/2403.02051v2", "date": "2025-05-12", "relevancy": 2.3172, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4668}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4659}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20of%20SGD%20under%20Gaussian%20or%20Heavy-Tailed%20Noise%3A%20Guarantees%20without%0A%20%20Gradient%20Clipping&body=Title%3A%20Privacy%20of%20SGD%20under%20Gaussian%20or%20Heavy-Tailed%20Noise%3A%20Guarantees%20without%0A%20%20Gradient%20Clipping%0AAuthor%3A%20Umut%20%C5%9Eim%C5%9Fekli%20and%20Mert%20G%C3%BCrb%C3%BCzbalaban%20and%20Sinan%20Y%C4%B1ld%C4%B1r%C4%B1m%20and%20Lingjiong%20Zhu%0AAbstract%3A%20%20%20The%20injection%20of%20heavy-tailed%20noise%20into%20the%20iterates%20of%20stochastic%20gradient%0Adescent%20%28SGD%29%20has%20garnered%20growing%20interest%20in%20recent%20years%20due%20to%20its%0Atheoretical%20and%20empirical%20benefits%20for%20optimization%20and%20generalization.%0AHowever%2C%20its%20implications%20for%20privacy%20preservation%20remain%20largely%20unexplored.%0AAiming%20to%20bridge%20this%20gap%2C%20we%20provide%20differential%20privacy%20%28DP%29%20guarantees%20for%0Anoisy%20SGD%2C%20when%20the%20injected%20noise%20follows%20an%20%24%5Calpha%24-stable%20distribution%2C%0Awhich%20includes%20a%20spectrum%20of%20heavy-tailed%20distributions%20%28with%20infinite%0Avariance%29%20as%20well%20as%20the%20light-tailed%20Gaussian%20distribution.%20Considering%20the%0A%24%28%5Cepsilon%2C%20%5Cdelta%29%24-DP%20framework%2C%20we%20show%20that%20SGD%20with%20heavy-tailed%0Aperturbations%20achieves%20%24%280%2C%20O%281/n%29%29%24-DP%20for%20a%20broad%20class%20of%20loss%20functions%0Awhich%20can%20be%20non-convex%2C%20where%20%24n%24%20is%20the%20number%20of%20data%20points.%20As%20a%0Aremarkable%20byproduct%2C%20contrary%20to%20prior%20work%20that%20necessitates%20bounded%0Asensitivity%20for%20the%20gradients%20or%20clipping%20the%20iterates%2C%20our%20theory%20can%20handle%0Aunbounded%20gradients%20without%20clipping%2C%20and%20reveals%20that%20under%20mild%20assumptions%2C%0Asuch%20a%20projection%20step%20is%20not%20actually%20necessary.%20Our%20results%20suggest%20that%2C%0Agiven%20other%20benefits%20of%20heavy-tails%20in%20optimization%2C%20heavy-tailed%20noising%0Aschemes%20can%20be%20a%20viable%20alternative%20to%20their%20light-tailed%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02051v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520of%2520SGD%2520under%2520Gaussian%2520or%2520Heavy-Tailed%2520Noise%253A%2520Guarantees%2520without%250A%2520%2520Gradient%2520Clipping%26entry.906535625%3DUmut%2520%25C5%259Eim%25C5%259Fekli%2520and%2520Mert%2520G%25C3%25BCrb%25C3%25BCzbalaban%2520and%2520Sinan%2520Y%25C4%25B1ld%25C4%25B1r%25C4%25B1m%2520and%2520Lingjiong%2520Zhu%26entry.1292438233%3D%2520%2520The%2520injection%2520of%2520heavy-tailed%2520noise%2520into%2520the%2520iterates%2520of%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520has%2520garnered%2520growing%2520interest%2520in%2520recent%2520years%2520due%2520to%2520its%250Atheoretical%2520and%2520empirical%2520benefits%2520for%2520optimization%2520and%2520generalization.%250AHowever%252C%2520its%2520implications%2520for%2520privacy%2520preservation%2520remain%2520largely%2520unexplored.%250AAiming%2520to%2520bridge%2520this%2520gap%252C%2520we%2520provide%2520differential%2520privacy%2520%2528DP%2529%2520guarantees%2520for%250Anoisy%2520SGD%252C%2520when%2520the%2520injected%2520noise%2520follows%2520an%2520%2524%255Calpha%2524-stable%2520distribution%252C%250Awhich%2520includes%2520a%2520spectrum%2520of%2520heavy-tailed%2520distributions%2520%2528with%2520infinite%250Avariance%2529%2520as%2520well%2520as%2520the%2520light-tailed%2520Gaussian%2520distribution.%2520Considering%2520the%250A%2524%2528%255Cepsilon%252C%2520%255Cdelta%2529%2524-DP%2520framework%252C%2520we%2520show%2520that%2520SGD%2520with%2520heavy-tailed%250Aperturbations%2520achieves%2520%2524%25280%252C%2520O%25281/n%2529%2529%2524-DP%2520for%2520a%2520broad%2520class%2520of%2520loss%2520functions%250Awhich%2520can%2520be%2520non-convex%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520data%2520points.%2520As%2520a%250Aremarkable%2520byproduct%252C%2520contrary%2520to%2520prior%2520work%2520that%2520necessitates%2520bounded%250Asensitivity%2520for%2520the%2520gradients%2520or%2520clipping%2520the%2520iterates%252C%2520our%2520theory%2520can%2520handle%250Aunbounded%2520gradients%2520without%2520clipping%252C%2520and%2520reveals%2520that%2520under%2520mild%2520assumptions%252C%250Asuch%2520a%2520projection%2520step%2520is%2520not%2520actually%2520necessary.%2520Our%2520results%2520suggest%2520that%252C%250Agiven%2520other%2520benefits%2520of%2520heavy-tails%2520in%2520optimization%252C%2520heavy-tailed%2520noising%250Aschemes%2520can%2520be%2520a%2520viable%2520alternative%2520to%2520their%2520light-tailed%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02051v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20of%20SGD%20under%20Gaussian%20or%20Heavy-Tailed%20Noise%3A%20Guarantees%20without%0A%20%20Gradient%20Clipping&entry.906535625=Umut%20%C5%9Eim%C5%9Fekli%20and%20Mert%20G%C3%BCrb%C3%BCzbalaban%20and%20Sinan%20Y%C4%B1ld%C4%B1r%C4%B1m%20and%20Lingjiong%20Zhu&entry.1292438233=%20%20The%20injection%20of%20heavy-tailed%20noise%20into%20the%20iterates%20of%20stochastic%20gradient%0Adescent%20%28SGD%29%20has%20garnered%20growing%20interest%20in%20recent%20years%20due%20to%20its%0Atheoretical%20and%20empirical%20benefits%20for%20optimization%20and%20generalization.%0AHowever%2C%20its%20implications%20for%20privacy%20preservation%20remain%20largely%20unexplored.%0AAiming%20to%20bridge%20this%20gap%2C%20we%20provide%20differential%20privacy%20%28DP%29%20guarantees%20for%0Anoisy%20SGD%2C%20when%20the%20injected%20noise%20follows%20an%20%24%5Calpha%24-stable%20distribution%2C%0Awhich%20includes%20a%20spectrum%20of%20heavy-tailed%20distributions%20%28with%20infinite%0Avariance%29%20as%20well%20as%20the%20light-tailed%20Gaussian%20distribution.%20Considering%20the%0A%24%28%5Cepsilon%2C%20%5Cdelta%29%24-DP%20framework%2C%20we%20show%20that%20SGD%20with%20heavy-tailed%0Aperturbations%20achieves%20%24%280%2C%20O%281/n%29%29%24-DP%20for%20a%20broad%20class%20of%20loss%20functions%0Awhich%20can%20be%20non-convex%2C%20where%20%24n%24%20is%20the%20number%20of%20data%20points.%20As%20a%0Aremarkable%20byproduct%2C%20contrary%20to%20prior%20work%20that%20necessitates%20bounded%0Asensitivity%20for%20the%20gradients%20or%20clipping%20the%20iterates%2C%20our%20theory%20can%20handle%0Aunbounded%20gradients%20without%20clipping%2C%20and%20reveals%20that%20under%20mild%20assumptions%2C%0Asuch%20a%20projection%20step%20is%20not%20actually%20necessary.%20Our%20results%20suggest%20that%2C%0Agiven%20other%20benefits%20of%20heavy-tails%20in%20optimization%2C%20heavy-tailed%20noising%0Aschemes%20can%20be%20a%20viable%20alternative%20to%20their%20light-tailed%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02051v2&entry.124074799=Read"},
{"title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation\n  with Document and Live Images", "author": "Raul Ismayilov and Luuk Spreeuwers and Dzemila Sero", "abstract": "  Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nwith user-defined identity attribute distributions and paired document-style\nand trusted live capture images. The dataset generated using the FLUXSynID\nframework shows improved alignment with real-world identity distributions and\ngreater inter-set diversity compared to prior work. The FLUXSynID framework for\ngenerating custom datasets, along with a dataset of 14,889 synthetic\nidentities, is publicly released to support biometric research, including face\nrecognition and morphing attack detection.\n", "link": "http://arxiv.org/abs/2505.07530v1", "date": "2025-05-12", "relevancy": 2.2936, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5985}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5574}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLUXSynID%3A%20A%20Framework%20for%20Identity-Controlled%20Synthetic%20Face%20Generation%0A%20%20with%20Document%20and%20Live%20Images&body=Title%3A%20FLUXSynID%3A%20A%20Framework%20for%20Identity-Controlled%20Synthetic%20Face%20Generation%0A%20%20with%20Document%20and%20Live%20Images%0AAuthor%3A%20Raul%20Ismayilov%20and%20Luuk%20Spreeuwers%20and%20Dzemila%20Sero%0AAbstract%3A%20%20%20Synthetic%20face%20datasets%20are%20increasingly%20used%20to%20overcome%20the%20limitations%20of%0Areal-world%20biometric%20data%2C%20including%20privacy%20concerns%2C%20demographic%20imbalance%2C%0Aand%20high%20collection%20costs.%20However%2C%20many%20existing%20methods%20lack%20fine-grained%0Acontrol%20over%20identity%20attributes%20and%20fail%20to%20produce%20paired%2C%0Aidentity-consistent%20images%20under%20structured%20capture%20conditions.%20We%20introduce%0AFLUXSynID%2C%20a%20framework%20for%20generating%20high-resolution%20synthetic%20face%20datasets%0Awith%20user-defined%20identity%20attribute%20distributions%20and%20paired%20document-style%0Aand%20trusted%20live%20capture%20images.%20The%20dataset%20generated%20using%20the%20FLUXSynID%0Aframework%20shows%20improved%20alignment%20with%20real-world%20identity%20distributions%20and%0Agreater%20inter-set%20diversity%20compared%20to%20prior%20work.%20The%20FLUXSynID%20framework%20for%0Agenerating%20custom%20datasets%2C%20along%20with%20a%20dataset%20of%2014%2C889%20synthetic%0Aidentities%2C%20is%20publicly%20released%20to%20support%20biometric%20research%2C%20including%20face%0Arecognition%20and%20morphing%20attack%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLUXSynID%253A%2520A%2520Framework%2520for%2520Identity-Controlled%2520Synthetic%2520Face%2520Generation%250A%2520%2520with%2520Document%2520and%2520Live%2520Images%26entry.906535625%3DRaul%2520Ismayilov%2520and%2520Luuk%2520Spreeuwers%2520and%2520Dzemila%2520Sero%26entry.1292438233%3D%2520%2520Synthetic%2520face%2520datasets%2520are%2520increasingly%2520used%2520to%2520overcome%2520the%2520limitations%2520of%250Areal-world%2520biometric%2520data%252C%2520including%2520privacy%2520concerns%252C%2520demographic%2520imbalance%252C%250Aand%2520high%2520collection%2520costs.%2520However%252C%2520many%2520existing%2520methods%2520lack%2520fine-grained%250Acontrol%2520over%2520identity%2520attributes%2520and%2520fail%2520to%2520produce%2520paired%252C%250Aidentity-consistent%2520images%2520under%2520structured%2520capture%2520conditions.%2520We%2520introduce%250AFLUXSynID%252C%2520a%2520framework%2520for%2520generating%2520high-resolution%2520synthetic%2520face%2520datasets%250Awith%2520user-defined%2520identity%2520attribute%2520distributions%2520and%2520paired%2520document-style%250Aand%2520trusted%2520live%2520capture%2520images.%2520The%2520dataset%2520generated%2520using%2520the%2520FLUXSynID%250Aframework%2520shows%2520improved%2520alignment%2520with%2520real-world%2520identity%2520distributions%2520and%250Agreater%2520inter-set%2520diversity%2520compared%2520to%2520prior%2520work.%2520The%2520FLUXSynID%2520framework%2520for%250Agenerating%2520custom%2520datasets%252C%2520along%2520with%2520a%2520dataset%2520of%252014%252C889%2520synthetic%250Aidentities%252C%2520is%2520publicly%2520released%2520to%2520support%2520biometric%2520research%252C%2520including%2520face%250Arecognition%2520and%2520morphing%2520attack%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLUXSynID%3A%20A%20Framework%20for%20Identity-Controlled%20Synthetic%20Face%20Generation%0A%20%20with%20Document%20and%20Live%20Images&entry.906535625=Raul%20Ismayilov%20and%20Luuk%20Spreeuwers%20and%20Dzemila%20Sero&entry.1292438233=%20%20Synthetic%20face%20datasets%20are%20increasingly%20used%20to%20overcome%20the%20limitations%20of%0Areal-world%20biometric%20data%2C%20including%20privacy%20concerns%2C%20demographic%20imbalance%2C%0Aand%20high%20collection%20costs.%20However%2C%20many%20existing%20methods%20lack%20fine-grained%0Acontrol%20over%20identity%20attributes%20and%20fail%20to%20produce%20paired%2C%0Aidentity-consistent%20images%20under%20structured%20capture%20conditions.%20We%20introduce%0AFLUXSynID%2C%20a%20framework%20for%20generating%20high-resolution%20synthetic%20face%20datasets%0Awith%20user-defined%20identity%20attribute%20distributions%20and%20paired%20document-style%0Aand%20trusted%20live%20capture%20images.%20The%20dataset%20generated%20using%20the%20FLUXSynID%0Aframework%20shows%20improved%20alignment%20with%20real-world%20identity%20distributions%20and%0Agreater%20inter-set%20diversity%20compared%20to%20prior%20work.%20The%20FLUXSynID%20framework%20for%0Agenerating%20custom%20datasets%2C%20along%20with%20a%20dataset%20of%2014%2C889%20synthetic%0Aidentities%2C%20is%20publicly%20released%20to%20support%20biometric%20research%2C%20including%20face%0Arecognition%20and%20morphing%20attack%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07530v1&entry.124074799=Read"},
{"title": "Geospatial Mechanistic Interpretability of Large Language Models", "author": "Stef De Sabbata and Stefano Mizzaro and Kevin Roitero", "abstract": "  Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.\n", "link": "http://arxiv.org/abs/2505.03368v2", "date": "2025-05-12", "relevancy": 2.292, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geospatial%20Mechanistic%20Interpretability%20of%20Large%20Language%20Models&body=Title%3A%20Geospatial%20Mechanistic%20Interpretability%20of%20Large%20Language%20Models%0AAuthor%3A%20Stef%20De%20Sabbata%20and%20Stefano%20Mizzaro%20and%20Kevin%20Roitero%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20unprecedented%20capabilities%0Aacross%20various%20natural%20language%20processing%20tasks.%20Their%20ability%20to%20process%20and%0Agenerate%20viable%20text%20and%20code%20has%20made%20them%20ubiquitous%20in%20many%20fields%2C%20while%0Atheir%20deployment%20as%20knowledge%20bases%20and%20%22reasoning%22%20tools%20remains%20an%20area%20of%0Aongoing%20research.%20In%20geography%2C%20a%20growing%20body%20of%20literature%20has%20been%20focusing%0Aon%20evaluating%20LLMs%27%20geographical%20knowledge%20and%20their%20ability%20to%20perform%20spatial%0Areasoning.%20However%2C%20very%20little%20is%20still%20known%20about%20the%20internal%20functioning%0Aof%20these%20models%2C%20especially%20about%20how%20they%20process%20geographical%20information.%0A%20%20In%20this%20chapter%2C%20we%20establish%20a%20novel%20framework%20for%20the%20study%20of%20geospatial%0Amechanistic%20interpretability%20-%20using%20spatial%20analysis%20to%20reverse%20engineer%20how%0ALLMs%20handle%20geographical%20information.%20Our%20aim%20is%20to%20advance%20our%20understanding%0Aof%20the%20internal%20representations%20that%20these%20complex%20models%20generate%20while%0Aprocessing%20geographical%20information%20-%20what%20one%20might%20call%20%22how%20LLMs%20think%20about%0Ageographic%20information%22%20if%20such%20phrasing%20was%20not%20an%20undue%20anthropomorphism.%0A%20%20We%20first%20outline%20the%20use%20of%20probing%20in%20revealing%20internal%20structures%20within%0ALLMs.%20We%20then%20introduce%20the%20field%20of%20mechanistic%20interpretability%2C%20discussing%0Athe%20superposition%20hypothesis%20and%20the%20role%20of%20sparse%20autoencoders%20in%0Adisentangling%20polysemantic%20internal%20representations%20of%20LLMs%20into%20more%0Ainterpretable%2C%20monosemantic%20features.%20In%20our%20experiments%2C%20we%20use%20spatial%0Aautocorrelation%20to%20show%20how%20features%20obtained%20for%20placenames%20display%20spatial%0Apatterns%20related%20to%20their%20geographic%20location%20and%20can%20thus%20be%20interpreted%0Ageospatially%2C%20providing%20insights%20into%20how%20these%20models%20process%20geographical%0Ainformation.%20We%20conclude%20by%20discussing%20how%20our%20framework%20can%20help%20shape%20the%0Astudy%20and%20use%20of%20foundation%20models%20in%20geography.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeospatial%2520Mechanistic%2520Interpretability%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DStef%2520De%2520Sabbata%2520and%2520Stefano%2520Mizzaro%2520and%2520Kevin%2520Roitero%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520unprecedented%2520capabilities%250Aacross%2520various%2520natural%2520language%2520processing%2520tasks.%2520Their%2520ability%2520to%2520process%2520and%250Agenerate%2520viable%2520text%2520and%2520code%2520has%2520made%2520them%2520ubiquitous%2520in%2520many%2520fields%252C%2520while%250Atheir%2520deployment%2520as%2520knowledge%2520bases%2520and%2520%2522reasoning%2522%2520tools%2520remains%2520an%2520area%2520of%250Aongoing%2520research.%2520In%2520geography%252C%2520a%2520growing%2520body%2520of%2520literature%2520has%2520been%2520focusing%250Aon%2520evaluating%2520LLMs%2527%2520geographical%2520knowledge%2520and%2520their%2520ability%2520to%2520perform%2520spatial%250Areasoning.%2520However%252C%2520very%2520little%2520is%2520still%2520known%2520about%2520the%2520internal%2520functioning%250Aof%2520these%2520models%252C%2520especially%2520about%2520how%2520they%2520process%2520geographical%2520information.%250A%2520%2520In%2520this%2520chapter%252C%2520we%2520establish%2520a%2520novel%2520framework%2520for%2520the%2520study%2520of%2520geospatial%250Amechanistic%2520interpretability%2520-%2520using%2520spatial%2520analysis%2520to%2520reverse%2520engineer%2520how%250ALLMs%2520handle%2520geographical%2520information.%2520Our%2520aim%2520is%2520to%2520advance%2520our%2520understanding%250Aof%2520the%2520internal%2520representations%2520that%2520these%2520complex%2520models%2520generate%2520while%250Aprocessing%2520geographical%2520information%2520-%2520what%2520one%2520might%2520call%2520%2522how%2520LLMs%2520think%2520about%250Ageographic%2520information%2522%2520if%2520such%2520phrasing%2520was%2520not%2520an%2520undue%2520anthropomorphism.%250A%2520%2520We%2520first%2520outline%2520the%2520use%2520of%2520probing%2520in%2520revealing%2520internal%2520structures%2520within%250ALLMs.%2520We%2520then%2520introduce%2520the%2520field%2520of%2520mechanistic%2520interpretability%252C%2520discussing%250Athe%2520superposition%2520hypothesis%2520and%2520the%2520role%2520of%2520sparse%2520autoencoders%2520in%250Adisentangling%2520polysemantic%2520internal%2520representations%2520of%2520LLMs%2520into%2520more%250Ainterpretable%252C%2520monosemantic%2520features.%2520In%2520our%2520experiments%252C%2520we%2520use%2520spatial%250Aautocorrelation%2520to%2520show%2520how%2520features%2520obtained%2520for%2520placenames%2520display%2520spatial%250Apatterns%2520related%2520to%2520their%2520geographic%2520location%2520and%2520can%2520thus%2520be%2520interpreted%250Ageospatially%252C%2520providing%2520insights%2520into%2520how%2520these%2520models%2520process%2520geographical%250Ainformation.%2520We%2520conclude%2520by%2520discussing%2520how%2520our%2520framework%2520can%2520help%2520shape%2520the%250Astudy%2520and%2520use%2520of%2520foundation%2520models%2520in%2520geography.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geospatial%20Mechanistic%20Interpretability%20of%20Large%20Language%20Models&entry.906535625=Stef%20De%20Sabbata%20and%20Stefano%20Mizzaro%20and%20Kevin%20Roitero&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20unprecedented%20capabilities%0Aacross%20various%20natural%20language%20processing%20tasks.%20Their%20ability%20to%20process%20and%0Agenerate%20viable%20text%20and%20code%20has%20made%20them%20ubiquitous%20in%20many%20fields%2C%20while%0Atheir%20deployment%20as%20knowledge%20bases%20and%20%22reasoning%22%20tools%20remains%20an%20area%20of%0Aongoing%20research.%20In%20geography%2C%20a%20growing%20body%20of%20literature%20has%20been%20focusing%0Aon%20evaluating%20LLMs%27%20geographical%20knowledge%20and%20their%20ability%20to%20perform%20spatial%0Areasoning.%20However%2C%20very%20little%20is%20still%20known%20about%20the%20internal%20functioning%0Aof%20these%20models%2C%20especially%20about%20how%20they%20process%20geographical%20information.%0A%20%20In%20this%20chapter%2C%20we%20establish%20a%20novel%20framework%20for%20the%20study%20of%20geospatial%0Amechanistic%20interpretability%20-%20using%20spatial%20analysis%20to%20reverse%20engineer%20how%0ALLMs%20handle%20geographical%20information.%20Our%20aim%20is%20to%20advance%20our%20understanding%0Aof%20the%20internal%20representations%20that%20these%20complex%20models%20generate%20while%0Aprocessing%20geographical%20information%20-%20what%20one%20might%20call%20%22how%20LLMs%20think%20about%0Ageographic%20information%22%20if%20such%20phrasing%20was%20not%20an%20undue%20anthropomorphism.%0A%20%20We%20first%20outline%20the%20use%20of%20probing%20in%20revealing%20internal%20structures%20within%0ALLMs.%20We%20then%20introduce%20the%20field%20of%20mechanistic%20interpretability%2C%20discussing%0Athe%20superposition%20hypothesis%20and%20the%20role%20of%20sparse%20autoencoders%20in%0Adisentangling%20polysemantic%20internal%20representations%20of%20LLMs%20into%20more%0Ainterpretable%2C%20monosemantic%20features.%20In%20our%20experiments%2C%20we%20use%20spatial%0Aautocorrelation%20to%20show%20how%20features%20obtained%20for%20placenames%20display%20spatial%0Apatterns%20related%20to%20their%20geographic%20location%20and%20can%20thus%20be%20interpreted%0Ageospatially%2C%20providing%20insights%20into%20how%20these%20models%20process%20geographical%0Ainformation.%20We%20conclude%20by%20discussing%20how%20our%20framework%20can%20help%20shape%20the%0Astudy%20and%20use%20of%20foundation%20models%20in%20geography.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03368v2&entry.124074799=Read"},
{"title": "Large Language Models Think Too Fast To Explore Effectively", "author": "Lan Pan and Hanbo Xie and Robert C. Wilson", "abstract": "  Large Language Models (LLMs) have emerged with many intellectual capacities.\nWhile numerous benchmarks assess their intelligence, limited attention has been\ngiven to their ability to explore--an essential capacity for discovering new\ninformation and adapting to novel environments in both natural and artificial\nsystems. The extent to which LLMs can effectively explore, particularly in\nopen-ended tasks, remains unclear. This study investigates whether LLMs can\nsurpass humans in exploration during an open-ended task, using Little Alchemy 2\nas a paradigm, where agents combine elements to discover new ones. Results show\nmost LLMs underperform compared to humans, except for the o1 model, with\ntraditional LLMs relying primarily on uncertainty-driven strategies, unlike\nhumans who balance uncertainty and empowerment. Results indicate that\ntraditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly\nfaster and less detailed reasoning process, limiting their exploratory\nperformance. In contrast, the DeepSeek reasoning model demonstrates prolonged,\niterative thought processes marked by repetitive analysis of combinations and\npast trials, reflecting a more thorough and human-like exploration strategy.\nRepresentational analysis of the models with Sparse Autoencoders (SAE) revealed\nthat uncertainty and choices are represented at earlier transformer blocks,\nwhile empowerment values are processed later, causing LLMs to think too fast\nand make premature decisions, hindering effective exploration. These findings\nshed light on the limitations of LLM exploration and suggest directions for\nimproving their adaptability.\n", "link": "http://arxiv.org/abs/2501.18009v2", "date": "2025-05-12", "relevancy": 2.2769, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Think%20Too%20Fast%20To%20Explore%20Effectively&body=Title%3A%20Large%20Language%20Models%20Think%20Too%20Fast%20To%20Explore%20Effectively%0AAuthor%3A%20Lan%20Pan%20and%20Hanbo%20Xie%20and%20Robert%20C.%20Wilson%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20with%20many%20intellectual%20capacities.%0AWhile%20numerous%20benchmarks%20assess%20their%20intelligence%2C%20limited%20attention%20has%20been%0Agiven%20to%20their%20ability%20to%20explore--an%20essential%20capacity%20for%20discovering%20new%0Ainformation%20and%20adapting%20to%20novel%20environments%20in%20both%20natural%20and%20artificial%0Asystems.%20The%20extent%20to%20which%20LLMs%20can%20effectively%20explore%2C%20particularly%20in%0Aopen-ended%20tasks%2C%20remains%20unclear.%20This%20study%20investigates%20whether%20LLMs%20can%0Asurpass%20humans%20in%20exploration%20during%20an%20open-ended%20task%2C%20using%20Little%20Alchemy%202%0Aas%20a%20paradigm%2C%20where%20agents%20combine%20elements%20to%20discover%20new%20ones.%20Results%20show%0Amost%20LLMs%20underperform%20compared%20to%20humans%2C%20except%20for%20the%20o1%20model%2C%20with%0Atraditional%20LLMs%20relying%20primarily%20on%20uncertainty-driven%20strategies%2C%20unlike%0Ahumans%20who%20balance%20uncertainty%20and%20empowerment.%20Results%20indicate%20that%0Atraditional%20reasoning-focused%20LLMs%2C%20such%20as%20GPT-4o%2C%20exhibit%20a%20significantly%0Afaster%20and%20less%20detailed%20reasoning%20process%2C%20limiting%20their%20exploratory%0Aperformance.%20In%20contrast%2C%20the%20DeepSeek%20reasoning%20model%20demonstrates%20prolonged%2C%0Aiterative%20thought%20processes%20marked%20by%20repetitive%20analysis%20of%20combinations%20and%0Apast%20trials%2C%20reflecting%20a%20more%20thorough%20and%20human-like%20exploration%20strategy.%0ARepresentational%20analysis%20of%20the%20models%20with%20Sparse%20Autoencoders%20%28SAE%29%20revealed%0Athat%20uncertainty%20and%20choices%20are%20represented%20at%20earlier%20transformer%20blocks%2C%0Awhile%20empowerment%20values%20are%20processed%20later%2C%20causing%20LLMs%20to%20think%20too%20fast%0Aand%20make%20premature%20decisions%2C%20hindering%20effective%20exploration.%20These%20findings%0Ashed%20light%20on%20the%20limitations%20of%20LLM%20exploration%20and%20suggest%20directions%20for%0Aimproving%20their%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Think%2520Too%2520Fast%2520To%2520Explore%2520Effectively%26entry.906535625%3DLan%2520Pan%2520and%2520Hanbo%2520Xie%2520and%2520Robert%2520C.%2520Wilson%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520with%2520many%2520intellectual%2520capacities.%250AWhile%2520numerous%2520benchmarks%2520assess%2520their%2520intelligence%252C%2520limited%2520attention%2520has%2520been%250Agiven%2520to%2520their%2520ability%2520to%2520explore--an%2520essential%2520capacity%2520for%2520discovering%2520new%250Ainformation%2520and%2520adapting%2520to%2520novel%2520environments%2520in%2520both%2520natural%2520and%2520artificial%250Asystems.%2520The%2520extent%2520to%2520which%2520LLMs%2520can%2520effectively%2520explore%252C%2520particularly%2520in%250Aopen-ended%2520tasks%252C%2520remains%2520unclear.%2520This%2520study%2520investigates%2520whether%2520LLMs%2520can%250Asurpass%2520humans%2520in%2520exploration%2520during%2520an%2520open-ended%2520task%252C%2520using%2520Little%2520Alchemy%25202%250Aas%2520a%2520paradigm%252C%2520where%2520agents%2520combine%2520elements%2520to%2520discover%2520new%2520ones.%2520Results%2520show%250Amost%2520LLMs%2520underperform%2520compared%2520to%2520humans%252C%2520except%2520for%2520the%2520o1%2520model%252C%2520with%250Atraditional%2520LLMs%2520relying%2520primarily%2520on%2520uncertainty-driven%2520strategies%252C%2520unlike%250Ahumans%2520who%2520balance%2520uncertainty%2520and%2520empowerment.%2520Results%2520indicate%2520that%250Atraditional%2520reasoning-focused%2520LLMs%252C%2520such%2520as%2520GPT-4o%252C%2520exhibit%2520a%2520significantly%250Afaster%2520and%2520less%2520detailed%2520reasoning%2520process%252C%2520limiting%2520their%2520exploratory%250Aperformance.%2520In%2520contrast%252C%2520the%2520DeepSeek%2520reasoning%2520model%2520demonstrates%2520prolonged%252C%250Aiterative%2520thought%2520processes%2520marked%2520by%2520repetitive%2520analysis%2520of%2520combinations%2520and%250Apast%2520trials%252C%2520reflecting%2520a%2520more%2520thorough%2520and%2520human-like%2520exploration%2520strategy.%250ARepresentational%2520analysis%2520of%2520the%2520models%2520with%2520Sparse%2520Autoencoders%2520%2528SAE%2529%2520revealed%250Athat%2520uncertainty%2520and%2520choices%2520are%2520represented%2520at%2520earlier%2520transformer%2520blocks%252C%250Awhile%2520empowerment%2520values%2520are%2520processed%2520later%252C%2520causing%2520LLMs%2520to%2520think%2520too%2520fast%250Aand%2520make%2520premature%2520decisions%252C%2520hindering%2520effective%2520exploration.%2520These%2520findings%250Ashed%2520light%2520on%2520the%2520limitations%2520of%2520LLM%2520exploration%2520and%2520suggest%2520directions%2520for%250Aimproving%2520their%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Think%20Too%20Fast%20To%20Explore%20Effectively&entry.906535625=Lan%20Pan%20and%20Hanbo%20Xie%20and%20Robert%20C.%20Wilson&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20with%20many%20intellectual%20capacities.%0AWhile%20numerous%20benchmarks%20assess%20their%20intelligence%2C%20limited%20attention%20has%20been%0Agiven%20to%20their%20ability%20to%20explore--an%20essential%20capacity%20for%20discovering%20new%0Ainformation%20and%20adapting%20to%20novel%20environments%20in%20both%20natural%20and%20artificial%0Asystems.%20The%20extent%20to%20which%20LLMs%20can%20effectively%20explore%2C%20particularly%20in%0Aopen-ended%20tasks%2C%20remains%20unclear.%20This%20study%20investigates%20whether%20LLMs%20can%0Asurpass%20humans%20in%20exploration%20during%20an%20open-ended%20task%2C%20using%20Little%20Alchemy%202%0Aas%20a%20paradigm%2C%20where%20agents%20combine%20elements%20to%20discover%20new%20ones.%20Results%20show%0Amost%20LLMs%20underperform%20compared%20to%20humans%2C%20except%20for%20the%20o1%20model%2C%20with%0Atraditional%20LLMs%20relying%20primarily%20on%20uncertainty-driven%20strategies%2C%20unlike%0Ahumans%20who%20balance%20uncertainty%20and%20empowerment.%20Results%20indicate%20that%0Atraditional%20reasoning-focused%20LLMs%2C%20such%20as%20GPT-4o%2C%20exhibit%20a%20significantly%0Afaster%20and%20less%20detailed%20reasoning%20process%2C%20limiting%20their%20exploratory%0Aperformance.%20In%20contrast%2C%20the%20DeepSeek%20reasoning%20model%20demonstrates%20prolonged%2C%0Aiterative%20thought%20processes%20marked%20by%20repetitive%20analysis%20of%20combinations%20and%0Apast%20trials%2C%20reflecting%20a%20more%20thorough%20and%20human-like%20exploration%20strategy.%0ARepresentational%20analysis%20of%20the%20models%20with%20Sparse%20Autoencoders%20%28SAE%29%20revealed%0Athat%20uncertainty%20and%20choices%20are%20represented%20at%20earlier%20transformer%20blocks%2C%0Awhile%20empowerment%20values%20are%20processed%20later%2C%20causing%20LLMs%20to%20think%20too%20fast%0Aand%20make%20premature%20decisions%2C%20hindering%20effective%20exploration.%20These%20findings%0Ashed%20light%20on%20the%20limitations%20of%20LLM%20exploration%20and%20suggest%20directions%20for%0Aimproving%20their%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18009v2&entry.124074799=Read"},
{"title": "Gameplay Highlights Generation", "author": "Vignesh Edithal and Le Zhang and Ilia Blank and Imran Junejo", "abstract": "  In this work, we enable gamers to share their gaming experience on social\nmedia by automatically generating eye-catching highlight reels from their\ngameplay session Our automation will save time for gamers while increasing\naudience engagement. We approach the highlight generation problem by first\nidentifying intervals in the video where interesting events occur and then\nconcatenate them. We developed an in-house gameplay event detection dataset\ncontaining interesting events annotated by humans using VIA video annotator.\nTraditional techniques for highlight detection such as game engine integration\nrequires expensive collaboration with game developers. OCR techniques which\ndetect patches of specific images or texts require expensive per game\nengineering and may not generalize across game UI and different language. We\nfinetuned a multimodal general purpose video understanding model such as X-CLIP\nusing our dataset which generalizes across multiple games in a genre without\nper game engineering. Prompt engineering was performed to improve the\nclassification performance of this multimodal model. Our evaluation showed that\nsuch a finetuned model can detect interesting events in first person shooting\ngames from unseen gameplay footage with more than 90% accuracy. Moreover, our\nmodel performed significantly better on low resource games (small dataset) when\ntrained along with high resource games, showing signs of transfer learning. To\nmake the model production ready, we used ONNX libraries to enable cross\nplatform inference. These libraries also provide post training quantization\ntools to reduce model size and inference time for deployment. ONNX runtime\nlibraries with DirectML backend were used to perform efficient inference on\nWindows OS. We show that natural language supervision in the X-CLIP model leads\nto data efficient and highly performant video recognition models.\n", "link": "http://arxiv.org/abs/2505.07721v1", "date": "2025-05-12", "relevancy": 2.2596, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5707}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5641}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gameplay%20Highlights%20Generation&body=Title%3A%20Gameplay%20Highlights%20Generation%0AAuthor%3A%20Vignesh%20Edithal%20and%20Le%20Zhang%20and%20Ilia%20Blank%20and%20Imran%20Junejo%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20enable%20gamers%20to%20share%20their%20gaming%20experience%20on%20social%0Amedia%20by%20automatically%20generating%20eye-catching%20highlight%20reels%20from%20their%0Agameplay%20session%20Our%20automation%20will%20save%20time%20for%20gamers%20while%20increasing%0Aaudience%20engagement.%20We%20approach%20the%20highlight%20generation%20problem%20by%20first%0Aidentifying%20intervals%20in%20the%20video%20where%20interesting%20events%20occur%20and%20then%0Aconcatenate%20them.%20We%20developed%20an%20in-house%20gameplay%20event%20detection%20dataset%0Acontaining%20interesting%20events%20annotated%20by%20humans%20using%20VIA%20video%20annotator.%0ATraditional%20techniques%20for%20highlight%20detection%20such%20as%20game%20engine%20integration%0Arequires%20expensive%20collaboration%20with%20game%20developers.%20OCR%20techniques%20which%0Adetect%20patches%20of%20specific%20images%20or%20texts%20require%20expensive%20per%20game%0Aengineering%20and%20may%20not%20generalize%20across%20game%20UI%20and%20different%20language.%20We%0Afinetuned%20a%20multimodal%20general%20purpose%20video%20understanding%20model%20such%20as%20X-CLIP%0Ausing%20our%20dataset%20which%20generalizes%20across%20multiple%20games%20in%20a%20genre%20without%0Aper%20game%20engineering.%20Prompt%20engineering%20was%20performed%20to%20improve%20the%0Aclassification%20performance%20of%20this%20multimodal%20model.%20Our%20evaluation%20showed%20that%0Asuch%20a%20finetuned%20model%20can%20detect%20interesting%20events%20in%20first%20person%20shooting%0Agames%20from%20unseen%20gameplay%20footage%20with%20more%20than%2090%25%20accuracy.%20Moreover%2C%20our%0Amodel%20performed%20significantly%20better%20on%20low%20resource%20games%20%28small%20dataset%29%20when%0Atrained%20along%20with%20high%20resource%20games%2C%20showing%20signs%20of%20transfer%20learning.%20To%0Amake%20the%20model%20production%20ready%2C%20we%20used%20ONNX%20libraries%20to%20enable%20cross%0Aplatform%20inference.%20These%20libraries%20also%20provide%20post%20training%20quantization%0Atools%20to%20reduce%20model%20size%20and%20inference%20time%20for%20deployment.%20ONNX%20runtime%0Alibraries%20with%20DirectML%20backend%20were%20used%20to%20perform%20efficient%20inference%20on%0AWindows%20OS.%20We%20show%20that%20natural%20language%20supervision%20in%20the%20X-CLIP%20model%20leads%0Ato%20data%20efficient%20and%20highly%20performant%20video%20recognition%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGameplay%2520Highlights%2520Generation%26entry.906535625%3DVignesh%2520Edithal%2520and%2520Le%2520Zhang%2520and%2520Ilia%2520Blank%2520and%2520Imran%2520Junejo%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520enable%2520gamers%2520to%2520share%2520their%2520gaming%2520experience%2520on%2520social%250Amedia%2520by%2520automatically%2520generating%2520eye-catching%2520highlight%2520reels%2520from%2520their%250Agameplay%2520session%2520Our%2520automation%2520will%2520save%2520time%2520for%2520gamers%2520while%2520increasing%250Aaudience%2520engagement.%2520We%2520approach%2520the%2520highlight%2520generation%2520problem%2520by%2520first%250Aidentifying%2520intervals%2520in%2520the%2520video%2520where%2520interesting%2520events%2520occur%2520and%2520then%250Aconcatenate%2520them.%2520We%2520developed%2520an%2520in-house%2520gameplay%2520event%2520detection%2520dataset%250Acontaining%2520interesting%2520events%2520annotated%2520by%2520humans%2520using%2520VIA%2520video%2520annotator.%250ATraditional%2520techniques%2520for%2520highlight%2520detection%2520such%2520as%2520game%2520engine%2520integration%250Arequires%2520expensive%2520collaboration%2520with%2520game%2520developers.%2520OCR%2520techniques%2520which%250Adetect%2520patches%2520of%2520specific%2520images%2520or%2520texts%2520require%2520expensive%2520per%2520game%250Aengineering%2520and%2520may%2520not%2520generalize%2520across%2520game%2520UI%2520and%2520different%2520language.%2520We%250Afinetuned%2520a%2520multimodal%2520general%2520purpose%2520video%2520understanding%2520model%2520such%2520as%2520X-CLIP%250Ausing%2520our%2520dataset%2520which%2520generalizes%2520across%2520multiple%2520games%2520in%2520a%2520genre%2520without%250Aper%2520game%2520engineering.%2520Prompt%2520engineering%2520was%2520performed%2520to%2520improve%2520the%250Aclassification%2520performance%2520of%2520this%2520multimodal%2520model.%2520Our%2520evaluation%2520showed%2520that%250Asuch%2520a%2520finetuned%2520model%2520can%2520detect%2520interesting%2520events%2520in%2520first%2520person%2520shooting%250Agames%2520from%2520unseen%2520gameplay%2520footage%2520with%2520more%2520than%252090%2525%2520accuracy.%2520Moreover%252C%2520our%250Amodel%2520performed%2520significantly%2520better%2520on%2520low%2520resource%2520games%2520%2528small%2520dataset%2529%2520when%250Atrained%2520along%2520with%2520high%2520resource%2520games%252C%2520showing%2520signs%2520of%2520transfer%2520learning.%2520To%250Amake%2520the%2520model%2520production%2520ready%252C%2520we%2520used%2520ONNX%2520libraries%2520to%2520enable%2520cross%250Aplatform%2520inference.%2520These%2520libraries%2520also%2520provide%2520post%2520training%2520quantization%250Atools%2520to%2520reduce%2520model%2520size%2520and%2520inference%2520time%2520for%2520deployment.%2520ONNX%2520runtime%250Alibraries%2520with%2520DirectML%2520backend%2520were%2520used%2520to%2520perform%2520efficient%2520inference%2520on%250AWindows%2520OS.%2520We%2520show%2520that%2520natural%2520language%2520supervision%2520in%2520the%2520X-CLIP%2520model%2520leads%250Ato%2520data%2520efficient%2520and%2520highly%2520performant%2520video%2520recognition%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gameplay%20Highlights%20Generation&entry.906535625=Vignesh%20Edithal%20and%20Le%20Zhang%20and%20Ilia%20Blank%20and%20Imran%20Junejo&entry.1292438233=%20%20In%20this%20work%2C%20we%20enable%20gamers%20to%20share%20their%20gaming%20experience%20on%20social%0Amedia%20by%20automatically%20generating%20eye-catching%20highlight%20reels%20from%20their%0Agameplay%20session%20Our%20automation%20will%20save%20time%20for%20gamers%20while%20increasing%0Aaudience%20engagement.%20We%20approach%20the%20highlight%20generation%20problem%20by%20first%0Aidentifying%20intervals%20in%20the%20video%20where%20interesting%20events%20occur%20and%20then%0Aconcatenate%20them.%20We%20developed%20an%20in-house%20gameplay%20event%20detection%20dataset%0Acontaining%20interesting%20events%20annotated%20by%20humans%20using%20VIA%20video%20annotator.%0ATraditional%20techniques%20for%20highlight%20detection%20such%20as%20game%20engine%20integration%0Arequires%20expensive%20collaboration%20with%20game%20developers.%20OCR%20techniques%20which%0Adetect%20patches%20of%20specific%20images%20or%20texts%20require%20expensive%20per%20game%0Aengineering%20and%20may%20not%20generalize%20across%20game%20UI%20and%20different%20language.%20We%0Afinetuned%20a%20multimodal%20general%20purpose%20video%20understanding%20model%20such%20as%20X-CLIP%0Ausing%20our%20dataset%20which%20generalizes%20across%20multiple%20games%20in%20a%20genre%20without%0Aper%20game%20engineering.%20Prompt%20engineering%20was%20performed%20to%20improve%20the%0Aclassification%20performance%20of%20this%20multimodal%20model.%20Our%20evaluation%20showed%20that%0Asuch%20a%20finetuned%20model%20can%20detect%20interesting%20events%20in%20first%20person%20shooting%0Agames%20from%20unseen%20gameplay%20footage%20with%20more%20than%2090%25%20accuracy.%20Moreover%2C%20our%0Amodel%20performed%20significantly%20better%20on%20low%20resource%20games%20%28small%20dataset%29%20when%0Atrained%20along%20with%20high%20resource%20games%2C%20showing%20signs%20of%20transfer%20learning.%20To%0Amake%20the%20model%20production%20ready%2C%20we%20used%20ONNX%20libraries%20to%20enable%20cross%0Aplatform%20inference.%20These%20libraries%20also%20provide%20post%20training%20quantization%0Atools%20to%20reduce%20model%20size%20and%20inference%20time%20for%20deployment.%20ONNX%20runtime%0Alibraries%20with%20DirectML%20backend%20were%20used%20to%20perform%20efficient%20inference%20on%0AWindows%20OS.%20We%20show%20that%20natural%20language%20supervision%20in%20the%20X-CLIP%20model%20leads%0Ato%20data%20efficient%20and%20highly%20performant%20video%20recognition%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07721v1&entry.124074799=Read"},
{"title": "TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining", "author": "Paul Primus and Florian Schmid and Gerhard Widmer", "abstract": "  Learning to associate audio with textual descriptions is valuable for a range\nof tasks, including pretraining, zero-shot classification, audio retrieval,\naudio captioning, and text-conditioned audio generation. Existing contrastive\nlanguage-audio pretrained models are typically trained using global, clip-level\ndescriptions, which provide only weak temporal supervision. We hypothesize that\nCLAP-like language-audio models - particularly, if they are expected to produce\nframe-level embeddings - can benefit from a stronger temporal supervision. To\nconfirm our hypothesis, we curate a novel dataset of approximately 12,000 audio\nrecordings from Freesound, each annotated with single-sentence free-text\ndescriptions linked to a specific temporal segment in an audio recording. We\nuse large language models to clean these annotations by removing references to\nnon-audible events, transcribed speech, typos, and annotator language bias. We\nfurther propose a frame-wise contrastive training strategy that learns to align\ntext descriptions with temporal regions in an audio recording and demonstrate\nthat our model has better temporal text-audio alignment abilities compared to\nmodels trained only on global captions when evaluated on the AudioSet Strong\nbenchmark. The dataset and our source code are available on Zenodo and GitHub,\nrespectively.\n", "link": "http://arxiv.org/abs/2505.07609v1", "date": "2025-05-12", "relevancy": 2.2287, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5638}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACOS%3A%20Temporally-aligned%20Audio%20CaptiOnS%20for%20Language-Audio%20Pretraining&body=Title%3A%20TACOS%3A%20Temporally-aligned%20Audio%20CaptiOnS%20for%20Language-Audio%20Pretraining%0AAuthor%3A%20Paul%20Primus%20and%20Florian%20Schmid%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20Learning%20to%20associate%20audio%20with%20textual%20descriptions%20is%20valuable%20for%20a%20range%0Aof%20tasks%2C%20including%20pretraining%2C%20zero-shot%20classification%2C%20audio%20retrieval%2C%0Aaudio%20captioning%2C%20and%20text-conditioned%20audio%20generation.%20Existing%20contrastive%0Alanguage-audio%20pretrained%20models%20are%20typically%20trained%20using%20global%2C%20clip-level%0Adescriptions%2C%20which%20provide%20only%20weak%20temporal%20supervision.%20We%20hypothesize%20that%0ACLAP-like%20language-audio%20models%20-%20particularly%2C%20if%20they%20are%20expected%20to%20produce%0Aframe-level%20embeddings%20-%20can%20benefit%20from%20a%20stronger%20temporal%20supervision.%20To%0Aconfirm%20our%20hypothesis%2C%20we%20curate%20a%20novel%20dataset%20of%20approximately%2012%2C000%20audio%0Arecordings%20from%20Freesound%2C%20each%20annotated%20with%20single-sentence%20free-text%0Adescriptions%20linked%20to%20a%20specific%20temporal%20segment%20in%20an%20audio%20recording.%20We%0Ause%20large%20language%20models%20to%20clean%20these%20annotations%20by%20removing%20references%20to%0Anon-audible%20events%2C%20transcribed%20speech%2C%20typos%2C%20and%20annotator%20language%20bias.%20We%0Afurther%20propose%20a%20frame-wise%20contrastive%20training%20strategy%20that%20learns%20to%20align%0Atext%20descriptions%20with%20temporal%20regions%20in%20an%20audio%20recording%20and%20demonstrate%0Athat%20our%20model%20has%20better%20temporal%20text-audio%20alignment%20abilities%20compared%20to%0Amodels%20trained%20only%20on%20global%20captions%20when%20evaluated%20on%20the%20AudioSet%20Strong%0Abenchmark.%20The%20dataset%20and%20our%20source%20code%20are%20available%20on%20Zenodo%20and%20GitHub%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACOS%253A%2520Temporally-aligned%2520Audio%2520CaptiOnS%2520for%2520Language-Audio%2520Pretraining%26entry.906535625%3DPaul%2520Primus%2520and%2520Florian%2520Schmid%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520Learning%2520to%2520associate%2520audio%2520with%2520textual%2520descriptions%2520is%2520valuable%2520for%2520a%2520range%250Aof%2520tasks%252C%2520including%2520pretraining%252C%2520zero-shot%2520classification%252C%2520audio%2520retrieval%252C%250Aaudio%2520captioning%252C%2520and%2520text-conditioned%2520audio%2520generation.%2520Existing%2520contrastive%250Alanguage-audio%2520pretrained%2520models%2520are%2520typically%2520trained%2520using%2520global%252C%2520clip-level%250Adescriptions%252C%2520which%2520provide%2520only%2520weak%2520temporal%2520supervision.%2520We%2520hypothesize%2520that%250ACLAP-like%2520language-audio%2520models%2520-%2520particularly%252C%2520if%2520they%2520are%2520expected%2520to%2520produce%250Aframe-level%2520embeddings%2520-%2520can%2520benefit%2520from%2520a%2520stronger%2520temporal%2520supervision.%2520To%250Aconfirm%2520our%2520hypothesis%252C%2520we%2520curate%2520a%2520novel%2520dataset%2520of%2520approximately%252012%252C000%2520audio%250Arecordings%2520from%2520Freesound%252C%2520each%2520annotated%2520with%2520single-sentence%2520free-text%250Adescriptions%2520linked%2520to%2520a%2520specific%2520temporal%2520segment%2520in%2520an%2520audio%2520recording.%2520We%250Ause%2520large%2520language%2520models%2520to%2520clean%2520these%2520annotations%2520by%2520removing%2520references%2520to%250Anon-audible%2520events%252C%2520transcribed%2520speech%252C%2520typos%252C%2520and%2520annotator%2520language%2520bias.%2520We%250Afurther%2520propose%2520a%2520frame-wise%2520contrastive%2520training%2520strategy%2520that%2520learns%2520to%2520align%250Atext%2520descriptions%2520with%2520temporal%2520regions%2520in%2520an%2520audio%2520recording%2520and%2520demonstrate%250Athat%2520our%2520model%2520has%2520better%2520temporal%2520text-audio%2520alignment%2520abilities%2520compared%2520to%250Amodels%2520trained%2520only%2520on%2520global%2520captions%2520when%2520evaluated%2520on%2520the%2520AudioSet%2520Strong%250Abenchmark.%2520The%2520dataset%2520and%2520our%2520source%2520code%2520are%2520available%2520on%2520Zenodo%2520and%2520GitHub%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACOS%3A%20Temporally-aligned%20Audio%20CaptiOnS%20for%20Language-Audio%20Pretraining&entry.906535625=Paul%20Primus%20and%20Florian%20Schmid%20and%20Gerhard%20Widmer&entry.1292438233=%20%20Learning%20to%20associate%20audio%20with%20textual%20descriptions%20is%20valuable%20for%20a%20range%0Aof%20tasks%2C%20including%20pretraining%2C%20zero-shot%20classification%2C%20audio%20retrieval%2C%0Aaudio%20captioning%2C%20and%20text-conditioned%20audio%20generation.%20Existing%20contrastive%0Alanguage-audio%20pretrained%20models%20are%20typically%20trained%20using%20global%2C%20clip-level%0Adescriptions%2C%20which%20provide%20only%20weak%20temporal%20supervision.%20We%20hypothesize%20that%0ACLAP-like%20language-audio%20models%20-%20particularly%2C%20if%20they%20are%20expected%20to%20produce%0Aframe-level%20embeddings%20-%20can%20benefit%20from%20a%20stronger%20temporal%20supervision.%20To%0Aconfirm%20our%20hypothesis%2C%20we%20curate%20a%20novel%20dataset%20of%20approximately%2012%2C000%20audio%0Arecordings%20from%20Freesound%2C%20each%20annotated%20with%20single-sentence%20free-text%0Adescriptions%20linked%20to%20a%20specific%20temporal%20segment%20in%20an%20audio%20recording.%20We%0Ause%20large%20language%20models%20to%20clean%20these%20annotations%20by%20removing%20references%20to%0Anon-audible%20events%2C%20transcribed%20speech%2C%20typos%2C%20and%20annotator%20language%20bias.%20We%0Afurther%20propose%20a%20frame-wise%20contrastive%20training%20strategy%20that%20learns%20to%20align%0Atext%20descriptions%20with%20temporal%20regions%20in%20an%20audio%20recording%20and%20demonstrate%0Athat%20our%20model%20has%20better%20temporal%20text-audio%20alignment%20abilities%20compared%20to%0Amodels%20trained%20only%20on%20global%20captions%20when%20evaluated%20on%20the%20AudioSet%20Strong%0Abenchmark.%20The%20dataset%20and%20our%20source%20code%20are%20available%20on%20Zenodo%20and%20GitHub%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07609v1&entry.124074799=Read"},
{"title": "Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks", "author": "Kai Xu and YiWei Mao and XinYi Guan and ZiLong Feng", "abstract": "  The application of large language models (LLMs) in the field of coding is\nevolving rapidly: from code assistants, to autonomous coding agents, and then\nto generating complete projects through natural language. Early LLM code\nbenchmarks primarily focused on code generation accuracy, but these benchmarks\nhave gradually become saturated. Benchmark saturation weakens their guiding\nrole for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.\nAmong various attempts to address benchmark saturation, approaches based on\nsoftware engineering have stood out, but the saturation of existing software\nengineering benchmarks is rapidly increasing. To address this, we propose a new\nbenchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks\nwith sequential dependencies. The tasks implement project features in sequence,\nsimulating real-world human development workflows. When designing Web-Bench, we\naim to cover the foundational elements of Web development: Web Standards and\nWeb Frameworks. Given the scale and complexity of these projects, which were\ndesigned by engineers with 5 to 10 years of experience, each presents a\nsignificant challenge. On average, a single project takes 4 to 8 hours for a\nsenior engineer to complete. On our given benchmark agent (Web-Agent), SOTA\n(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)\nthan SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss\nthat in any development field, Standards and Frameworks represent foundational\nknowledge and efficiency tools, respectively, and LLMs require optimization\ntailored to them.\n", "link": "http://arxiv.org/abs/2505.07473v1", "date": "2025-05-12", "relevancy": 2.2239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4594}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Web-Bench%3A%20A%20LLM%20Code%20Benchmark%20Based%20on%20Web%20Standards%20and%20Frameworks&body=Title%3A%20Web-Bench%3A%20A%20LLM%20Code%20Benchmark%20Based%20on%20Web%20Standards%20and%20Frameworks%0AAuthor%3A%20Kai%20Xu%20and%20YiWei%20Mao%20and%20XinYi%20Guan%20and%20ZiLong%20Feng%0AAbstract%3A%20%20%20The%20application%20of%20large%20language%20models%20%28LLMs%29%20in%20the%20field%20of%20coding%20is%0Aevolving%20rapidly%3A%20from%20code%20assistants%2C%20to%20autonomous%20coding%20agents%2C%20and%20then%0Ato%20generating%20complete%20projects%20through%20natural%20language.%20Early%20LLM%20code%0Abenchmarks%20primarily%20focused%20on%20code%20generation%20accuracy%2C%20but%20these%20benchmarks%0Ahave%20gradually%20become%20saturated.%20Benchmark%20saturation%20weakens%20their%20guiding%0Arole%20for%20LLMs.%20For%20example%2C%20HumanEval%20Pass%401%20has%20reached%2099.4%25%20and%20MBPP%2094.2%25.%0AAmong%20various%20attempts%20to%20address%20benchmark%20saturation%2C%20approaches%20based%20on%0Asoftware%20engineering%20have%20stood%20out%2C%20but%20the%20saturation%20of%20existing%20software%0Aengineering%20benchmarks%20is%20rapidly%20increasing.%20To%20address%20this%2C%20we%20propose%20a%20new%0Abenchmark%2C%20Web-Bench%2C%20which%20contains%2050%20projects%2C%20each%20consisting%20of%2020%20tasks%0Awith%20sequential%20dependencies.%20The%20tasks%20implement%20project%20features%20in%20sequence%2C%0Asimulating%20real-world%20human%20development%20workflows.%20When%20designing%20Web-Bench%2C%20we%0Aaim%20to%20cover%20the%20foundational%20elements%20of%20Web%20development%3A%20Web%20Standards%20and%0AWeb%20Frameworks.%20Given%20the%20scale%20and%20complexity%20of%20these%20projects%2C%20which%20were%0Adesigned%20by%20engineers%20with%205%20to%2010%20years%20of%20experience%2C%20each%20presents%20a%0Asignificant%20challenge.%20On%20average%2C%20a%20single%20project%20takes%204%20to%208%20hours%20for%20a%0Asenior%20engineer%20to%20complete.%20On%20our%20given%20benchmark%20agent%20%28Web-Agent%29%2C%20SOTA%0A%28Claude%203.7%20Sonnet%29%20achieves%20only%2025.1%25%20Pass%401%2C%20significantly%20lower%20%28better%29%0Athan%20SWE-Bench%27s%20Verified%20%2865.4%25%29%20and%20Full%20%2833.8%25%29%20scores.%20Finally%2C%20we%20discuss%0Athat%20in%20any%20development%20field%2C%20Standards%20and%20Frameworks%20represent%20foundational%0Aknowledge%20and%20efficiency%20tools%2C%20respectively%2C%20and%20LLMs%20require%20optimization%0Atailored%20to%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeb-Bench%253A%2520A%2520LLM%2520Code%2520Benchmark%2520Based%2520on%2520Web%2520Standards%2520and%2520Frameworks%26entry.906535625%3DKai%2520Xu%2520and%2520YiWei%2520Mao%2520and%2520XinYi%2520Guan%2520and%2520ZiLong%2520Feng%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520the%2520field%2520of%2520coding%2520is%250Aevolving%2520rapidly%253A%2520from%2520code%2520assistants%252C%2520to%2520autonomous%2520coding%2520agents%252C%2520and%2520then%250Ato%2520generating%2520complete%2520projects%2520through%2520natural%2520language.%2520Early%2520LLM%2520code%250Abenchmarks%2520primarily%2520focused%2520on%2520code%2520generation%2520accuracy%252C%2520but%2520these%2520benchmarks%250Ahave%2520gradually%2520become%2520saturated.%2520Benchmark%2520saturation%2520weakens%2520their%2520guiding%250Arole%2520for%2520LLMs.%2520For%2520example%252C%2520HumanEval%2520Pass%25401%2520has%2520reached%252099.4%2525%2520and%2520MBPP%252094.2%2525.%250AAmong%2520various%2520attempts%2520to%2520address%2520benchmark%2520saturation%252C%2520approaches%2520based%2520on%250Asoftware%2520engineering%2520have%2520stood%2520out%252C%2520but%2520the%2520saturation%2520of%2520existing%2520software%250Aengineering%2520benchmarks%2520is%2520rapidly%2520increasing.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%250Abenchmark%252C%2520Web-Bench%252C%2520which%2520contains%252050%2520projects%252C%2520each%2520consisting%2520of%252020%2520tasks%250Awith%2520sequential%2520dependencies.%2520The%2520tasks%2520implement%2520project%2520features%2520in%2520sequence%252C%250Asimulating%2520real-world%2520human%2520development%2520workflows.%2520When%2520designing%2520Web-Bench%252C%2520we%250Aaim%2520to%2520cover%2520the%2520foundational%2520elements%2520of%2520Web%2520development%253A%2520Web%2520Standards%2520and%250AWeb%2520Frameworks.%2520Given%2520the%2520scale%2520and%2520complexity%2520of%2520these%2520projects%252C%2520which%2520were%250Adesigned%2520by%2520engineers%2520with%25205%2520to%252010%2520years%2520of%2520experience%252C%2520each%2520presents%2520a%250Asignificant%2520challenge.%2520On%2520average%252C%2520a%2520single%2520project%2520takes%25204%2520to%25208%2520hours%2520for%2520a%250Asenior%2520engineer%2520to%2520complete.%2520On%2520our%2520given%2520benchmark%2520agent%2520%2528Web-Agent%2529%252C%2520SOTA%250A%2528Claude%25203.7%2520Sonnet%2529%2520achieves%2520only%252025.1%2525%2520Pass%25401%252C%2520significantly%2520lower%2520%2528better%2529%250Athan%2520SWE-Bench%2527s%2520Verified%2520%252865.4%2525%2529%2520and%2520Full%2520%252833.8%2525%2529%2520scores.%2520Finally%252C%2520we%2520discuss%250Athat%2520in%2520any%2520development%2520field%252C%2520Standards%2520and%2520Frameworks%2520represent%2520foundational%250Aknowledge%2520and%2520efficiency%2520tools%252C%2520respectively%252C%2520and%2520LLMs%2520require%2520optimization%250Atailored%2520to%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Web-Bench%3A%20A%20LLM%20Code%20Benchmark%20Based%20on%20Web%20Standards%20and%20Frameworks&entry.906535625=Kai%20Xu%20and%20YiWei%20Mao%20and%20XinYi%20Guan%20and%20ZiLong%20Feng&entry.1292438233=%20%20The%20application%20of%20large%20language%20models%20%28LLMs%29%20in%20the%20field%20of%20coding%20is%0Aevolving%20rapidly%3A%20from%20code%20assistants%2C%20to%20autonomous%20coding%20agents%2C%20and%20then%0Ato%20generating%20complete%20projects%20through%20natural%20language.%20Early%20LLM%20code%0Abenchmarks%20primarily%20focused%20on%20code%20generation%20accuracy%2C%20but%20these%20benchmarks%0Ahave%20gradually%20become%20saturated.%20Benchmark%20saturation%20weakens%20their%20guiding%0Arole%20for%20LLMs.%20For%20example%2C%20HumanEval%20Pass%401%20has%20reached%2099.4%25%20and%20MBPP%2094.2%25.%0AAmong%20various%20attempts%20to%20address%20benchmark%20saturation%2C%20approaches%20based%20on%0Asoftware%20engineering%20have%20stood%20out%2C%20but%20the%20saturation%20of%20existing%20software%0Aengineering%20benchmarks%20is%20rapidly%20increasing.%20To%20address%20this%2C%20we%20propose%20a%20new%0Abenchmark%2C%20Web-Bench%2C%20which%20contains%2050%20projects%2C%20each%20consisting%20of%2020%20tasks%0Awith%20sequential%20dependencies.%20The%20tasks%20implement%20project%20features%20in%20sequence%2C%0Asimulating%20real-world%20human%20development%20workflows.%20When%20designing%20Web-Bench%2C%20we%0Aaim%20to%20cover%20the%20foundational%20elements%20of%20Web%20development%3A%20Web%20Standards%20and%0AWeb%20Frameworks.%20Given%20the%20scale%20and%20complexity%20of%20these%20projects%2C%20which%20were%0Adesigned%20by%20engineers%20with%205%20to%2010%20years%20of%20experience%2C%20each%20presents%20a%0Asignificant%20challenge.%20On%20average%2C%20a%20single%20project%20takes%204%20to%208%20hours%20for%20a%0Asenior%20engineer%20to%20complete.%20On%20our%20given%20benchmark%20agent%20%28Web-Agent%29%2C%20SOTA%0A%28Claude%203.7%20Sonnet%29%20achieves%20only%2025.1%25%20Pass%401%2C%20significantly%20lower%20%28better%29%0Athan%20SWE-Bench%27s%20Verified%20%2865.4%25%29%20and%20Full%20%2833.8%25%29%20scores.%20Finally%2C%20we%20discuss%0Athat%20in%20any%20development%20field%2C%20Standards%20and%20Frameworks%20represent%20foundational%0Aknowledge%20and%20efficiency%20tools%2C%20respectively%2C%20and%20LLMs%20require%20optimization%0Atailored%20to%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07473v1&entry.124074799=Read"},
{"title": "Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video\n  Generation Model", "author": "Wei Li and Ming Hu and Guoan Wang and Lihao Liu and Kaijin Zhou and Junzhi Ning and Xin Guo and Zongyuan Ge and Lixu Gu and Junjun He", "abstract": "  In ophthalmic surgery, developing an AI system capable of interpreting\nsurgical videos and predicting subsequent operations requires numerous\nophthalmic surgical videos with high-quality annotations, which are difficult\nto collect due to privacy concerns and labor consumption. Text-guided video\ngeneration (T2V) emerges as a promising solution to overcome this issue by\ngenerating ophthalmic surgical videos based on surgeon instructions. In this\npaper, we present Ophora, a pioneering model that can generate ophthalmic\nsurgical videos following natural language instructions. To construct Ophora,\nwe first propose a Comprehensive Data Curation pipeline to convert narrative\nophthalmic surgical videos into a large-scale, high-quality dataset comprising\nover 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive\nVideo-Instruction Tuning scheme to transfer rich spatial-temporal knowledge\nfrom a T2V model pre-trained on natural video-text datasets for\nprivacy-preserved ophthalmic surgical video generation based on Ophora-160K.\nExperiments on video quality evaluation via quantitative analysis and\nophthalmologist feedback demonstrate that Ophora can generate realistic and\nreliable ophthalmic surgical videos based on surgeon instructions. We also\nvalidate the capability of Ophora for empowering downstream tasks of ophthalmic\nsurgical workflow understanding. Code is available at\nhttps://github.com/mar-cry/Ophora.\n", "link": "http://arxiv.org/abs/2505.07449v1", "date": "2025-05-12", "relevancy": 2.2187, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.556}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5547}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ophora%3A%20A%20Large-Scale%20Data-Driven%20Text-Guided%20Ophthalmic%20Surgical%20Video%0A%20%20Generation%20Model&body=Title%3A%20Ophora%3A%20A%20Large-Scale%20Data-Driven%20Text-Guided%20Ophthalmic%20Surgical%20Video%0A%20%20Generation%20Model%0AAuthor%3A%20Wei%20Li%20and%20Ming%20Hu%20and%20Guoan%20Wang%20and%20Lihao%20Liu%20and%20Kaijin%20Zhou%20and%20Junzhi%20Ning%20and%20Xin%20Guo%20and%20Zongyuan%20Ge%20and%20Lixu%20Gu%20and%20Junjun%20He%0AAbstract%3A%20%20%20In%20ophthalmic%20surgery%2C%20developing%20an%20AI%20system%20capable%20of%20interpreting%0Asurgical%20videos%20and%20predicting%20subsequent%20operations%20requires%20numerous%0Aophthalmic%20surgical%20videos%20with%20high-quality%20annotations%2C%20which%20are%20difficult%0Ato%20collect%20due%20to%20privacy%20concerns%20and%20labor%20consumption.%20Text-guided%20video%0Ageneration%20%28T2V%29%20emerges%20as%20a%20promising%20solution%20to%20overcome%20this%20issue%20by%0Agenerating%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20In%20this%0Apaper%2C%20we%20present%20Ophora%2C%20a%20pioneering%20model%20that%20can%20generate%20ophthalmic%0Asurgical%20videos%20following%20natural%20language%20instructions.%20To%20construct%20Ophora%2C%0Awe%20first%20propose%20a%20Comprehensive%20Data%20Curation%20pipeline%20to%20convert%20narrative%0Aophthalmic%20surgical%20videos%20into%20a%20large-scale%2C%20high-quality%20dataset%20comprising%0Aover%20160K%20video-instruction%20pairs%2C%20Ophora-160K.%20Then%2C%20we%20propose%20a%20Progressive%0AVideo-Instruction%20Tuning%20scheme%20to%20transfer%20rich%20spatial-temporal%20knowledge%0Afrom%20a%20T2V%20model%20pre-trained%20on%20natural%20video-text%20datasets%20for%0Aprivacy-preserved%20ophthalmic%20surgical%20video%20generation%20based%20on%20Ophora-160K.%0AExperiments%20on%20video%20quality%20evaluation%20via%20quantitative%20analysis%20and%0Aophthalmologist%20feedback%20demonstrate%20that%20Ophora%20can%20generate%20realistic%20and%0Areliable%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20We%20also%0Avalidate%20the%20capability%20of%20Ophora%20for%20empowering%20downstream%20tasks%20of%20ophthalmic%0Asurgical%20workflow%20understanding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mar-cry/Ophora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOphora%253A%2520A%2520Large-Scale%2520Data-Driven%2520Text-Guided%2520Ophthalmic%2520Surgical%2520Video%250A%2520%2520Generation%2520Model%26entry.906535625%3DWei%2520Li%2520and%2520Ming%2520Hu%2520and%2520Guoan%2520Wang%2520and%2520Lihao%2520Liu%2520and%2520Kaijin%2520Zhou%2520and%2520Junzhi%2520Ning%2520and%2520Xin%2520Guo%2520and%2520Zongyuan%2520Ge%2520and%2520Lixu%2520Gu%2520and%2520Junjun%2520He%26entry.1292438233%3D%2520%2520In%2520ophthalmic%2520surgery%252C%2520developing%2520an%2520AI%2520system%2520capable%2520of%2520interpreting%250Asurgical%2520videos%2520and%2520predicting%2520subsequent%2520operations%2520requires%2520numerous%250Aophthalmic%2520surgical%2520videos%2520with%2520high-quality%2520annotations%252C%2520which%2520are%2520difficult%250Ato%2520collect%2520due%2520to%2520privacy%2520concerns%2520and%2520labor%2520consumption.%2520Text-guided%2520video%250Ageneration%2520%2528T2V%2529%2520emerges%2520as%2520a%2520promising%2520solution%2520to%2520overcome%2520this%2520issue%2520by%250Agenerating%2520ophthalmic%2520surgical%2520videos%2520based%2520on%2520surgeon%2520instructions.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Ophora%252C%2520a%2520pioneering%2520model%2520that%2520can%2520generate%2520ophthalmic%250Asurgical%2520videos%2520following%2520natural%2520language%2520instructions.%2520To%2520construct%2520Ophora%252C%250Awe%2520first%2520propose%2520a%2520Comprehensive%2520Data%2520Curation%2520pipeline%2520to%2520convert%2520narrative%250Aophthalmic%2520surgical%2520videos%2520into%2520a%2520large-scale%252C%2520high-quality%2520dataset%2520comprising%250Aover%2520160K%2520video-instruction%2520pairs%252C%2520Ophora-160K.%2520Then%252C%2520we%2520propose%2520a%2520Progressive%250AVideo-Instruction%2520Tuning%2520scheme%2520to%2520transfer%2520rich%2520spatial-temporal%2520knowledge%250Afrom%2520a%2520T2V%2520model%2520pre-trained%2520on%2520natural%2520video-text%2520datasets%2520for%250Aprivacy-preserved%2520ophthalmic%2520surgical%2520video%2520generation%2520based%2520on%2520Ophora-160K.%250AExperiments%2520on%2520video%2520quality%2520evaluation%2520via%2520quantitative%2520analysis%2520and%250Aophthalmologist%2520feedback%2520demonstrate%2520that%2520Ophora%2520can%2520generate%2520realistic%2520and%250Areliable%2520ophthalmic%2520surgical%2520videos%2520based%2520on%2520surgeon%2520instructions.%2520We%2520also%250Avalidate%2520the%2520capability%2520of%2520Ophora%2520for%2520empowering%2520downstream%2520tasks%2520of%2520ophthalmic%250Asurgical%2520workflow%2520understanding.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mar-cry/Ophora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ophora%3A%20A%20Large-Scale%20Data-Driven%20Text-Guided%20Ophthalmic%20Surgical%20Video%0A%20%20Generation%20Model&entry.906535625=Wei%20Li%20and%20Ming%20Hu%20and%20Guoan%20Wang%20and%20Lihao%20Liu%20and%20Kaijin%20Zhou%20and%20Junzhi%20Ning%20and%20Xin%20Guo%20and%20Zongyuan%20Ge%20and%20Lixu%20Gu%20and%20Junjun%20He&entry.1292438233=%20%20In%20ophthalmic%20surgery%2C%20developing%20an%20AI%20system%20capable%20of%20interpreting%0Asurgical%20videos%20and%20predicting%20subsequent%20operations%20requires%20numerous%0Aophthalmic%20surgical%20videos%20with%20high-quality%20annotations%2C%20which%20are%20difficult%0Ato%20collect%20due%20to%20privacy%20concerns%20and%20labor%20consumption.%20Text-guided%20video%0Ageneration%20%28T2V%29%20emerges%20as%20a%20promising%20solution%20to%20overcome%20this%20issue%20by%0Agenerating%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20In%20this%0Apaper%2C%20we%20present%20Ophora%2C%20a%20pioneering%20model%20that%20can%20generate%20ophthalmic%0Asurgical%20videos%20following%20natural%20language%20instructions.%20To%20construct%20Ophora%2C%0Awe%20first%20propose%20a%20Comprehensive%20Data%20Curation%20pipeline%20to%20convert%20narrative%0Aophthalmic%20surgical%20videos%20into%20a%20large-scale%2C%20high-quality%20dataset%20comprising%0Aover%20160K%20video-instruction%20pairs%2C%20Ophora-160K.%20Then%2C%20we%20propose%20a%20Progressive%0AVideo-Instruction%20Tuning%20scheme%20to%20transfer%20rich%20spatial-temporal%20knowledge%0Afrom%20a%20T2V%20model%20pre-trained%20on%20natural%20video-text%20datasets%20for%0Aprivacy-preserved%20ophthalmic%20surgical%20video%20generation%20based%20on%20Ophora-160K.%0AExperiments%20on%20video%20quality%20evaluation%20via%20quantitative%20analysis%20and%0Aophthalmologist%20feedback%20demonstrate%20that%20Ophora%20can%20generate%20realistic%20and%0Areliable%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20We%20also%0Avalidate%20the%20capability%20of%20Ophora%20for%20empowering%20downstream%20tasks%20of%20ophthalmic%0Asurgical%20workflow%20understanding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mar-cry/Ophora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07449v1&entry.124074799=Read"},
{"title": "Hybrid Control Strategies for Safe and Adaptive Robot-Assisted Dressing", "author": "Yasmin Rafiq and Baslin A. James and Ke Xu and Robert M. Hierons and Sanja Dogramadzi", "abstract": "  Safety, reliability, and user trust are crucial in human-robot interaction\n(HRI) where the robots must address hazards in real-time. This study presents\nhazard driven low-level control strategies implemented in robot-assisted\ndressing (RAD) scenarios where hazards like garment snags and user discomfort\nin real-time can affect task performance and user safety. The proposed control\nmechanisms include: (1) Garment Snagging Control Strategy, which detects\nexcessive forces and either seeks user intervention via a chatbot or\nautonomously adjusts its trajectory, and (2) User Discomfort/Pain Mitigation\nStrategy, which dynamically reduces velocity based on user feedback and aborts\nthe task if necessary. We used physical dressing trials in order to evaluate\nthese control strategies. Results confirm that integrating force monitoring\nwith user feedback improves safety and task continuity. The findings emphasise\nthe need for hybrid approaches that balance autonomous intervention, user\ninvolvement, and controlled task termination, supported by bi-directional\ninteraction and real-time user-driven adaptability, paving the way for more\nresponsive and personalised HRI systems.\n", "link": "http://arxiv.org/abs/2505.07710v1", "date": "2025-05-12", "relevancy": 2.2121, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5845}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5311}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Control%20Strategies%20for%20Safe%20and%20Adaptive%20Robot-Assisted%20Dressing&body=Title%3A%20Hybrid%20Control%20Strategies%20for%20Safe%20and%20Adaptive%20Robot-Assisted%20Dressing%0AAuthor%3A%20Yasmin%20Rafiq%20and%20Baslin%20A.%20James%20and%20Ke%20Xu%20and%20Robert%20M.%20Hierons%20and%20Sanja%20Dogramadzi%0AAbstract%3A%20%20%20Safety%2C%20reliability%2C%20and%20user%20trust%20are%20crucial%20in%20human-robot%20interaction%0A%28HRI%29%20where%20the%20robots%20must%20address%20hazards%20in%20real-time.%20This%20study%20presents%0Ahazard%20driven%20low-level%20control%20strategies%20implemented%20in%20robot-assisted%0Adressing%20%28RAD%29%20scenarios%20where%20hazards%20like%20garment%20snags%20and%20user%20discomfort%0Ain%20real-time%20can%20affect%20task%20performance%20and%20user%20safety.%20The%20proposed%20control%0Amechanisms%20include%3A%20%281%29%20Garment%20Snagging%20Control%20Strategy%2C%20which%20detects%0Aexcessive%20forces%20and%20either%20seeks%20user%20intervention%20via%20a%20chatbot%20or%0Aautonomously%20adjusts%20its%20trajectory%2C%20and%20%282%29%20User%20Discomfort/Pain%20Mitigation%0AStrategy%2C%20which%20dynamically%20reduces%20velocity%20based%20on%20user%20feedback%20and%20aborts%0Athe%20task%20if%20necessary.%20We%20used%20physical%20dressing%20trials%20in%20order%20to%20evaluate%0Athese%20control%20strategies.%20Results%20confirm%20that%20integrating%20force%20monitoring%0Awith%20user%20feedback%20improves%20safety%20and%20task%20continuity.%20The%20findings%20emphasise%0Athe%20need%20for%20hybrid%20approaches%20that%20balance%20autonomous%20intervention%2C%20user%0Ainvolvement%2C%20and%20controlled%20task%20termination%2C%20supported%20by%20bi-directional%0Ainteraction%20and%20real-time%20user-driven%20adaptability%2C%20paving%20the%20way%20for%20more%0Aresponsive%20and%20personalised%20HRI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Control%2520Strategies%2520for%2520Safe%2520and%2520Adaptive%2520Robot-Assisted%2520Dressing%26entry.906535625%3DYasmin%2520Rafiq%2520and%2520Baslin%2520A.%2520James%2520and%2520Ke%2520Xu%2520and%2520Robert%2520M.%2520Hierons%2520and%2520Sanja%2520Dogramadzi%26entry.1292438233%3D%2520%2520Safety%252C%2520reliability%252C%2520and%2520user%2520trust%2520are%2520crucial%2520in%2520human-robot%2520interaction%250A%2528HRI%2529%2520where%2520the%2520robots%2520must%2520address%2520hazards%2520in%2520real-time.%2520This%2520study%2520presents%250Ahazard%2520driven%2520low-level%2520control%2520strategies%2520implemented%2520in%2520robot-assisted%250Adressing%2520%2528RAD%2529%2520scenarios%2520where%2520hazards%2520like%2520garment%2520snags%2520and%2520user%2520discomfort%250Ain%2520real-time%2520can%2520affect%2520task%2520performance%2520and%2520user%2520safety.%2520The%2520proposed%2520control%250Amechanisms%2520include%253A%2520%25281%2529%2520Garment%2520Snagging%2520Control%2520Strategy%252C%2520which%2520detects%250Aexcessive%2520forces%2520and%2520either%2520seeks%2520user%2520intervention%2520via%2520a%2520chatbot%2520or%250Aautonomously%2520adjusts%2520its%2520trajectory%252C%2520and%2520%25282%2529%2520User%2520Discomfort/Pain%2520Mitigation%250AStrategy%252C%2520which%2520dynamically%2520reduces%2520velocity%2520based%2520on%2520user%2520feedback%2520and%2520aborts%250Athe%2520task%2520if%2520necessary.%2520We%2520used%2520physical%2520dressing%2520trials%2520in%2520order%2520to%2520evaluate%250Athese%2520control%2520strategies.%2520Results%2520confirm%2520that%2520integrating%2520force%2520monitoring%250Awith%2520user%2520feedback%2520improves%2520safety%2520and%2520task%2520continuity.%2520The%2520findings%2520emphasise%250Athe%2520need%2520for%2520hybrid%2520approaches%2520that%2520balance%2520autonomous%2520intervention%252C%2520user%250Ainvolvement%252C%2520and%2520controlled%2520task%2520termination%252C%2520supported%2520by%2520bi-directional%250Ainteraction%2520and%2520real-time%2520user-driven%2520adaptability%252C%2520paving%2520the%2520way%2520for%2520more%250Aresponsive%2520and%2520personalised%2520HRI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Control%20Strategies%20for%20Safe%20and%20Adaptive%20Robot-Assisted%20Dressing&entry.906535625=Yasmin%20Rafiq%20and%20Baslin%20A.%20James%20and%20Ke%20Xu%20and%20Robert%20M.%20Hierons%20and%20Sanja%20Dogramadzi&entry.1292438233=%20%20Safety%2C%20reliability%2C%20and%20user%20trust%20are%20crucial%20in%20human-robot%20interaction%0A%28HRI%29%20where%20the%20robots%20must%20address%20hazards%20in%20real-time.%20This%20study%20presents%0Ahazard%20driven%20low-level%20control%20strategies%20implemented%20in%20robot-assisted%0Adressing%20%28RAD%29%20scenarios%20where%20hazards%20like%20garment%20snags%20and%20user%20discomfort%0Ain%20real-time%20can%20affect%20task%20performance%20and%20user%20safety.%20The%20proposed%20control%0Amechanisms%20include%3A%20%281%29%20Garment%20Snagging%20Control%20Strategy%2C%20which%20detects%0Aexcessive%20forces%20and%20either%20seeks%20user%20intervention%20via%20a%20chatbot%20or%0Aautonomously%20adjusts%20its%20trajectory%2C%20and%20%282%29%20User%20Discomfort/Pain%20Mitigation%0AStrategy%2C%20which%20dynamically%20reduces%20velocity%20based%20on%20user%20feedback%20and%20aborts%0Athe%20task%20if%20necessary.%20We%20used%20physical%20dressing%20trials%20in%20order%20to%20evaluate%0Athese%20control%20strategies.%20Results%20confirm%20that%20integrating%20force%20monitoring%0Awith%20user%20feedback%20improves%20safety%20and%20task%20continuity.%20The%20findings%20emphasise%0Athe%20need%20for%20hybrid%20approaches%20that%20balance%20autonomous%20intervention%2C%20user%0Ainvolvement%2C%20and%20controlled%20task%20termination%2C%20supported%20by%20bi-directional%0Ainteraction%20and%20real-time%20user-driven%20adaptability%2C%20paving%20the%20way%20for%20more%0Aresponsive%20and%20personalised%20HRI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07710v1&entry.124074799=Read"},
{"title": "DocVXQA: Context-Aware Visual Explanations for Document Question\n  Answering", "author": "Mohamed Ali Souibgui and Changkyu Choi and Andrey Barsky and Kangsoo Jung and Ernest Valveny and Dimosthenis Karatzas", "abstract": "  We propose DocVXQA, a novel framework for visually self-explainable document\nquestion answering. The framework is designed not only to produce accurate\nanswers to questions but also to learn visual heatmaps that highlight\ncontextually critical regions, thereby offering interpretable justifications\nfor the model's decisions. To integrate explanations into the learning process,\nwe quantitatively formulate explainability principles as explicit learning\nobjectives. Unlike conventional methods that emphasize only the regions\npertinent to the answer, our framework delivers explanations that are\n\\textit{contextually sufficient} while remaining\n\\textit{representation-efficient}. This fosters user trust while achieving a\nbalance between predictive performance and interpretability in DocVQA\napplications. Extensive experiments, including human evaluation, provide strong\nevidence supporting the effectiveness of our method. The code is available at\nhttps://github.com/dali92002/DocVXQA.\n", "link": "http://arxiv.org/abs/2505.07496v1", "date": "2025-05-12", "relevancy": 2.1838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocVXQA%3A%20Context-Aware%20Visual%20Explanations%20for%20Document%20Question%0A%20%20Answering&body=Title%3A%20DocVXQA%3A%20Context-Aware%20Visual%20Explanations%20for%20Document%20Question%0A%20%20Answering%0AAuthor%3A%20Mohamed%20Ali%20Souibgui%20and%20Changkyu%20Choi%20and%20Andrey%20Barsky%20and%20Kangsoo%20Jung%20and%20Ernest%20Valveny%20and%20Dimosthenis%20Karatzas%0AAbstract%3A%20%20%20We%20propose%20DocVXQA%2C%20a%20novel%20framework%20for%20visually%20self-explainable%20document%0Aquestion%20answering.%20The%20framework%20is%20designed%20not%20only%20to%20produce%20accurate%0Aanswers%20to%20questions%20but%20also%20to%20learn%20visual%20heatmaps%20that%20highlight%0Acontextually%20critical%20regions%2C%20thereby%20offering%20interpretable%20justifications%0Afor%20the%20model%27s%20decisions.%20To%20integrate%20explanations%20into%20the%20learning%20process%2C%0Awe%20quantitatively%20formulate%20explainability%20principles%20as%20explicit%20learning%0Aobjectives.%20Unlike%20conventional%20methods%20that%20emphasize%20only%20the%20regions%0Apertinent%20to%20the%20answer%2C%20our%20framework%20delivers%20explanations%20that%20are%0A%5Ctextit%7Bcontextually%20sufficient%7D%20while%20remaining%0A%5Ctextit%7Brepresentation-efficient%7D.%20This%20fosters%20user%20trust%20while%20achieving%20a%0Abalance%20between%20predictive%20performance%20and%20interpretability%20in%20DocVQA%0Aapplications.%20Extensive%20experiments%2C%20including%20human%20evaluation%2C%20provide%20strong%0Aevidence%20supporting%20the%20effectiveness%20of%20our%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/dali92002/DocVXQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocVXQA%253A%2520Context-Aware%2520Visual%2520Explanations%2520for%2520Document%2520Question%250A%2520%2520Answering%26entry.906535625%3DMohamed%2520Ali%2520Souibgui%2520and%2520Changkyu%2520Choi%2520and%2520Andrey%2520Barsky%2520and%2520Kangsoo%2520Jung%2520and%2520Ernest%2520Valveny%2520and%2520Dimosthenis%2520Karatzas%26entry.1292438233%3D%2520%2520We%2520propose%2520DocVXQA%252C%2520a%2520novel%2520framework%2520for%2520visually%2520self-explainable%2520document%250Aquestion%2520answering.%2520The%2520framework%2520is%2520designed%2520not%2520only%2520to%2520produce%2520accurate%250Aanswers%2520to%2520questions%2520but%2520also%2520to%2520learn%2520visual%2520heatmaps%2520that%2520highlight%250Acontextually%2520critical%2520regions%252C%2520thereby%2520offering%2520interpretable%2520justifications%250Afor%2520the%2520model%2527s%2520decisions.%2520To%2520integrate%2520explanations%2520into%2520the%2520learning%2520process%252C%250Awe%2520quantitatively%2520formulate%2520explainability%2520principles%2520as%2520explicit%2520learning%250Aobjectives.%2520Unlike%2520conventional%2520methods%2520that%2520emphasize%2520only%2520the%2520regions%250Apertinent%2520to%2520the%2520answer%252C%2520our%2520framework%2520delivers%2520explanations%2520that%2520are%250A%255Ctextit%257Bcontextually%2520sufficient%257D%2520while%2520remaining%250A%255Ctextit%257Brepresentation-efficient%257D.%2520This%2520fosters%2520user%2520trust%2520while%2520achieving%2520a%250Abalance%2520between%2520predictive%2520performance%2520and%2520interpretability%2520in%2520DocVQA%250Aapplications.%2520Extensive%2520experiments%252C%2520including%2520human%2520evaluation%252C%2520provide%2520strong%250Aevidence%2520supporting%2520the%2520effectiveness%2520of%2520our%2520method.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/dali92002/DocVXQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocVXQA%3A%20Context-Aware%20Visual%20Explanations%20for%20Document%20Question%0A%20%20Answering&entry.906535625=Mohamed%20Ali%20Souibgui%20and%20Changkyu%20Choi%20and%20Andrey%20Barsky%20and%20Kangsoo%20Jung%20and%20Ernest%20Valveny%20and%20Dimosthenis%20Karatzas&entry.1292438233=%20%20We%20propose%20DocVXQA%2C%20a%20novel%20framework%20for%20visually%20self-explainable%20document%0Aquestion%20answering.%20The%20framework%20is%20designed%20not%20only%20to%20produce%20accurate%0Aanswers%20to%20questions%20but%20also%20to%20learn%20visual%20heatmaps%20that%20highlight%0Acontextually%20critical%20regions%2C%20thereby%20offering%20interpretable%20justifications%0Afor%20the%20model%27s%20decisions.%20To%20integrate%20explanations%20into%20the%20learning%20process%2C%0Awe%20quantitatively%20formulate%20explainability%20principles%20as%20explicit%20learning%0Aobjectives.%20Unlike%20conventional%20methods%20that%20emphasize%20only%20the%20regions%0Apertinent%20to%20the%20answer%2C%20our%20framework%20delivers%20explanations%20that%20are%0A%5Ctextit%7Bcontextually%20sufficient%7D%20while%20remaining%0A%5Ctextit%7Brepresentation-efficient%7D.%20This%20fosters%20user%20trust%20while%20achieving%20a%0Abalance%20between%20predictive%20performance%20and%20interpretability%20in%20DocVQA%0Aapplications.%20Extensive%20experiments%2C%20including%20human%20evaluation%2C%20provide%20strong%0Aevidence%20supporting%20the%20effectiveness%20of%20our%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/dali92002/DocVXQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07496v1&entry.124074799=Read"},
{"title": "Privacy Risks of Robot Vision: A User Study on Image Modalities and\n  Resolution", "author": "Xuying Huang and Sicong Pan and Maren Bennewitz", "abstract": "  User privacy is a crucial concern in robotic applications, especially when\nmobile service robots are deployed in personal or sensitive environments.\nHowever, many robotic downstream tasks require the use of cameras, which may\nraise privacy risks. To better understand user perceptions of privacy in\nrelation to visual data, we conducted a user study investigating how different\nimage modalities and image resolutions affect users' privacy concerns. The\nresults show that depth images are broadly viewed as privacy-safe, and a\nsimilarly high proportion of respondents feel the same about semantic\nsegmentation images. Additionally, the majority of participants consider 32*32\nresolution RGB images to be almost sufficiently privacy-preserving, while most\nbelieve that 16*16 resolution can fully guarantee privacy protection.\n", "link": "http://arxiv.org/abs/2505.07766v1", "date": "2025-05-12", "relevancy": 2.1607, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20Risks%20of%20Robot%20Vision%3A%20A%20User%20Study%20on%20Image%20Modalities%20and%0A%20%20Resolution&body=Title%3A%20Privacy%20Risks%20of%20Robot%20Vision%3A%20A%20User%20Study%20on%20Image%20Modalities%20and%0A%20%20Resolution%0AAuthor%3A%20Xuying%20Huang%20and%20Sicong%20Pan%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20User%20privacy%20is%20a%20crucial%20concern%20in%20robotic%20applications%2C%20especially%20when%0Amobile%20service%20robots%20are%20deployed%20in%20personal%20or%20sensitive%20environments.%0AHowever%2C%20many%20robotic%20downstream%20tasks%20require%20the%20use%20of%20cameras%2C%20which%20may%0Araise%20privacy%20risks.%20To%20better%20understand%20user%20perceptions%20of%20privacy%20in%0Arelation%20to%20visual%20data%2C%20we%20conducted%20a%20user%20study%20investigating%20how%20different%0Aimage%20modalities%20and%20image%20resolutions%20affect%20users%27%20privacy%20concerns.%20The%0Aresults%20show%20that%20depth%20images%20are%20broadly%20viewed%20as%20privacy-safe%2C%20and%20a%0Asimilarly%20high%20proportion%20of%20respondents%20feel%20the%20same%20about%20semantic%0Asegmentation%20images.%20Additionally%2C%20the%20majority%20of%20participants%20consider%2032%2A32%0Aresolution%20RGB%20images%20to%20be%20almost%20sufficiently%20privacy-preserving%2C%20while%20most%0Abelieve%20that%2016%2A16%20resolution%20can%20fully%20guarantee%20privacy%20protection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520Risks%2520of%2520Robot%2520Vision%253A%2520A%2520User%2520Study%2520on%2520Image%2520Modalities%2520and%250A%2520%2520Resolution%26entry.906535625%3DXuying%2520Huang%2520and%2520Sicong%2520Pan%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520User%2520privacy%2520is%2520a%2520crucial%2520concern%2520in%2520robotic%2520applications%252C%2520especially%2520when%250Amobile%2520service%2520robots%2520are%2520deployed%2520in%2520personal%2520or%2520sensitive%2520environments.%250AHowever%252C%2520many%2520robotic%2520downstream%2520tasks%2520require%2520the%2520use%2520of%2520cameras%252C%2520which%2520may%250Araise%2520privacy%2520risks.%2520To%2520better%2520understand%2520user%2520perceptions%2520of%2520privacy%2520in%250Arelation%2520to%2520visual%2520data%252C%2520we%2520conducted%2520a%2520user%2520study%2520investigating%2520how%2520different%250Aimage%2520modalities%2520and%2520image%2520resolutions%2520affect%2520users%2527%2520privacy%2520concerns.%2520The%250Aresults%2520show%2520that%2520depth%2520images%2520are%2520broadly%2520viewed%2520as%2520privacy-safe%252C%2520and%2520a%250Asimilarly%2520high%2520proportion%2520of%2520respondents%2520feel%2520the%2520same%2520about%2520semantic%250Asegmentation%2520images.%2520Additionally%252C%2520the%2520majority%2520of%2520participants%2520consider%252032%252A32%250Aresolution%2520RGB%2520images%2520to%2520be%2520almost%2520sufficiently%2520privacy-preserving%252C%2520while%2520most%250Abelieve%2520that%252016%252A16%2520resolution%2520can%2520fully%2520guarantee%2520privacy%2520protection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20Risks%20of%20Robot%20Vision%3A%20A%20User%20Study%20on%20Image%20Modalities%20and%0A%20%20Resolution&entry.906535625=Xuying%20Huang%20and%20Sicong%20Pan%20and%20Maren%20Bennewitz&entry.1292438233=%20%20User%20privacy%20is%20a%20crucial%20concern%20in%20robotic%20applications%2C%20especially%20when%0Amobile%20service%20robots%20are%20deployed%20in%20personal%20or%20sensitive%20environments.%0AHowever%2C%20many%20robotic%20downstream%20tasks%20require%20the%20use%20of%20cameras%2C%20which%20may%0Araise%20privacy%20risks.%20To%20better%20understand%20user%20perceptions%20of%20privacy%20in%0Arelation%20to%20visual%20data%2C%20we%20conducted%20a%20user%20study%20investigating%20how%20different%0Aimage%20modalities%20and%20image%20resolutions%20affect%20users%27%20privacy%20concerns.%20The%0Aresults%20show%20that%20depth%20images%20are%20broadly%20viewed%20as%20privacy-safe%2C%20and%20a%0Asimilarly%20high%20proportion%20of%20respondents%20feel%20the%20same%20about%20semantic%0Asegmentation%20images.%20Additionally%2C%20the%20majority%20of%20participants%20consider%2032%2A32%0Aresolution%20RGB%20images%20to%20be%20almost%20sufficiently%20privacy-preserving%2C%20while%20most%0Abelieve%20that%2016%2A16%20resolution%20can%20fully%20guarantee%20privacy%20protection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07766v1&entry.124074799=Read"},
{"title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents", "author": "Jian Liu and Xiongtao Shi and Thai Duy Nguyen and Haitian Zhang and Tianxiang Zhang and Wei Sun and Yanjie Li and Athanasios V. Vasilakos and Giovanni Iacca and Arshad Ali Khan and Arvind Kumar and Jae Won Cho and Ajmal Mian and Lihua Xie and Erik Cambria and Lin Wang", "abstract": "  The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.\n", "link": "http://arxiv.org/abs/2505.07634v1", "date": "2025-05-12", "relevancy": 2.1606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.536}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Brain%3A%20A%20Neuroscience-inspired%20Framework%20for%20Embodied%20Agents&body=Title%3A%20Neural%20Brain%3A%20A%20Neuroscience-inspired%20Framework%20for%20Embodied%20Agents%0AAuthor%3A%20Jian%20Liu%20and%20Xiongtao%20Shi%20and%20Thai%20Duy%20Nguyen%20and%20Haitian%20Zhang%20and%20Tianxiang%20Zhang%20and%20Wei%20Sun%20and%20Yanjie%20Li%20and%20Athanasios%20V.%20Vasilakos%20and%20Giovanni%20Iacca%20and%20Arshad%20Ali%20Khan%20and%20Arvind%20Kumar%20and%20Jae%20Won%20Cho%20and%20Ajmal%20Mian%20and%20Lihua%20Xie%20and%20Erik%20Cambria%20and%20Lin%20Wang%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20artificial%20intelligence%20%28AI%29%20has%20shifted%20from%20static%2C%0Adata-driven%20models%20to%20dynamic%20systems%20capable%20of%20perceiving%20and%20interacting%0Awith%20real-world%20environments.%20Despite%20advancements%20in%20pattern%20recognition%20and%0Asymbolic%20reasoning%2C%20current%20AI%20systems%2C%20such%20as%20large%20language%20models%2C%20remain%0Adisembodied%2C%20unable%20to%20physically%20engage%20with%20the%20world.%20This%20limitation%20has%0Adriven%20the%20rise%20of%20embodied%20AI%2C%20where%20autonomous%20agents%2C%20such%20as%20humanoid%0Arobots%2C%20must%20navigate%20and%20manipulate%20unstructured%20environments%20with%20human-like%0Aadaptability.%20At%20the%20core%20of%20this%20challenge%20lies%20the%20concept%20of%20Neural%20Brain%2C%20a%0Acentral%20intelligence%20system%20designed%20to%20drive%20embodied%20agents%20with%20human-like%0Aadaptability.%20A%20Neural%20Brain%20must%20seamlessly%20integrate%20multimodal%20sensing%20and%0Aperception%20with%20cognitive%20capabilities.%20Achieving%20this%20also%20requires%20an%0Aadaptive%20memory%20system%20and%20energy-efficient%20hardware-software%20co-design%2C%0Aenabling%20real-time%20action%20in%20dynamic%20environments.%20This%20paper%20introduces%20a%0Aunified%20framework%20for%20the%20Neural%20Brain%20of%20embodied%20agents%2C%20addressing%20two%0Afundamental%20challenges%3A%20%281%29%20defining%20the%20core%20components%20of%20Neural%20Brain%20and%0A%282%29%20bridging%20the%20gap%20between%20static%20AI%20models%20and%20the%20dynamic%20adaptability%0Arequired%20for%20real-world%20deployment.%20To%20this%20end%2C%20we%20propose%20a%20biologically%0Ainspired%20architecture%20that%20integrates%20multimodal%20active%20sensing%2C%0Aperception-cognition-action%20function%2C%20neuroplasticity-based%20memory%20storage%20and%0Aupdating%2C%20and%20neuromorphic%20hardware/software%20optimization.%20Furthermore%2C%20we%20also%0Areview%20the%20latest%20research%20on%20embodied%20agents%20across%20these%20four%20aspects%20and%0Aanalyze%20the%20gap%20between%20current%20AI%20systems%20and%20human%20intelligence.%20By%0Asynthesizing%20insights%20from%20neuroscience%2C%20we%20outline%20a%20roadmap%20towards%20the%0Adevelopment%20of%20generalizable%2C%20autonomous%20agents%20capable%20of%20human-level%0Aintelligence%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Brain%253A%2520A%2520Neuroscience-inspired%2520Framework%2520for%2520Embodied%2520Agents%26entry.906535625%3DJian%2520Liu%2520and%2520Xiongtao%2520Shi%2520and%2520Thai%2520Duy%2520Nguyen%2520and%2520Haitian%2520Zhang%2520and%2520Tianxiang%2520Zhang%2520and%2520Wei%2520Sun%2520and%2520Yanjie%2520Li%2520and%2520Athanasios%2520V.%2520Vasilakos%2520and%2520Giovanni%2520Iacca%2520and%2520Arshad%2520Ali%2520Khan%2520and%2520Arvind%2520Kumar%2520and%2520Jae%2520Won%2520Cho%2520and%2520Ajmal%2520Mian%2520and%2520Lihua%2520Xie%2520and%2520Erik%2520Cambria%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520shifted%2520from%2520static%252C%250Adata-driven%2520models%2520to%2520dynamic%2520systems%2520capable%2520of%2520perceiving%2520and%2520interacting%250Awith%2520real-world%2520environments.%2520Despite%2520advancements%2520in%2520pattern%2520recognition%2520and%250Asymbolic%2520reasoning%252C%2520current%2520AI%2520systems%252C%2520such%2520as%2520large%2520language%2520models%252C%2520remain%250Adisembodied%252C%2520unable%2520to%2520physically%2520engage%2520with%2520the%2520world.%2520This%2520limitation%2520has%250Adriven%2520the%2520rise%2520of%2520embodied%2520AI%252C%2520where%2520autonomous%2520agents%252C%2520such%2520as%2520humanoid%250Arobots%252C%2520must%2520navigate%2520and%2520manipulate%2520unstructured%2520environments%2520with%2520human-like%250Aadaptability.%2520At%2520the%2520core%2520of%2520this%2520challenge%2520lies%2520the%2520concept%2520of%2520Neural%2520Brain%252C%2520a%250Acentral%2520intelligence%2520system%2520designed%2520to%2520drive%2520embodied%2520agents%2520with%2520human-like%250Aadaptability.%2520A%2520Neural%2520Brain%2520must%2520seamlessly%2520integrate%2520multimodal%2520sensing%2520and%250Aperception%2520with%2520cognitive%2520capabilities.%2520Achieving%2520this%2520also%2520requires%2520an%250Aadaptive%2520memory%2520system%2520and%2520energy-efficient%2520hardware-software%2520co-design%252C%250Aenabling%2520real-time%2520action%2520in%2520dynamic%2520environments.%2520This%2520paper%2520introduces%2520a%250Aunified%2520framework%2520for%2520the%2520Neural%2520Brain%2520of%2520embodied%2520agents%252C%2520addressing%2520two%250Afundamental%2520challenges%253A%2520%25281%2529%2520defining%2520the%2520core%2520components%2520of%2520Neural%2520Brain%2520and%250A%25282%2529%2520bridging%2520the%2520gap%2520between%2520static%2520AI%2520models%2520and%2520the%2520dynamic%2520adaptability%250Arequired%2520for%2520real-world%2520deployment.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520biologically%250Ainspired%2520architecture%2520that%2520integrates%2520multimodal%2520active%2520sensing%252C%250Aperception-cognition-action%2520function%252C%2520neuroplasticity-based%2520memory%2520storage%2520and%250Aupdating%252C%2520and%2520neuromorphic%2520hardware/software%2520optimization.%2520Furthermore%252C%2520we%2520also%250Areview%2520the%2520latest%2520research%2520on%2520embodied%2520agents%2520across%2520these%2520four%2520aspects%2520and%250Aanalyze%2520the%2520gap%2520between%2520current%2520AI%2520systems%2520and%2520human%2520intelligence.%2520By%250Asynthesizing%2520insights%2520from%2520neuroscience%252C%2520we%2520outline%2520a%2520roadmap%2520towards%2520the%250Adevelopment%2520of%2520generalizable%252C%2520autonomous%2520agents%2520capable%2520of%2520human-level%250Aintelligence%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Brain%3A%20A%20Neuroscience-inspired%20Framework%20for%20Embodied%20Agents&entry.906535625=Jian%20Liu%20and%20Xiongtao%20Shi%20and%20Thai%20Duy%20Nguyen%20and%20Haitian%20Zhang%20and%20Tianxiang%20Zhang%20and%20Wei%20Sun%20and%20Yanjie%20Li%20and%20Athanasios%20V.%20Vasilakos%20and%20Giovanni%20Iacca%20and%20Arshad%20Ali%20Khan%20and%20Arvind%20Kumar%20and%20Jae%20Won%20Cho%20and%20Ajmal%20Mian%20and%20Lihua%20Xie%20and%20Erik%20Cambria%20and%20Lin%20Wang&entry.1292438233=%20%20The%20rapid%20evolution%20of%20artificial%20intelligence%20%28AI%29%20has%20shifted%20from%20static%2C%0Adata-driven%20models%20to%20dynamic%20systems%20capable%20of%20perceiving%20and%20interacting%0Awith%20real-world%20environments.%20Despite%20advancements%20in%20pattern%20recognition%20and%0Asymbolic%20reasoning%2C%20current%20AI%20systems%2C%20such%20as%20large%20language%20models%2C%20remain%0Adisembodied%2C%20unable%20to%20physically%20engage%20with%20the%20world.%20This%20limitation%20has%0Adriven%20the%20rise%20of%20embodied%20AI%2C%20where%20autonomous%20agents%2C%20such%20as%20humanoid%0Arobots%2C%20must%20navigate%20and%20manipulate%20unstructured%20environments%20with%20human-like%0Aadaptability.%20At%20the%20core%20of%20this%20challenge%20lies%20the%20concept%20of%20Neural%20Brain%2C%20a%0Acentral%20intelligence%20system%20designed%20to%20drive%20embodied%20agents%20with%20human-like%0Aadaptability.%20A%20Neural%20Brain%20must%20seamlessly%20integrate%20multimodal%20sensing%20and%0Aperception%20with%20cognitive%20capabilities.%20Achieving%20this%20also%20requires%20an%0Aadaptive%20memory%20system%20and%20energy-efficient%20hardware-software%20co-design%2C%0Aenabling%20real-time%20action%20in%20dynamic%20environments.%20This%20paper%20introduces%20a%0Aunified%20framework%20for%20the%20Neural%20Brain%20of%20embodied%20agents%2C%20addressing%20two%0Afundamental%20challenges%3A%20%281%29%20defining%20the%20core%20components%20of%20Neural%20Brain%20and%0A%282%29%20bridging%20the%20gap%20between%20static%20AI%20models%20and%20the%20dynamic%20adaptability%0Arequired%20for%20real-world%20deployment.%20To%20this%20end%2C%20we%20propose%20a%20biologically%0Ainspired%20architecture%20that%20integrates%20multimodal%20active%20sensing%2C%0Aperception-cognition-action%20function%2C%20neuroplasticity-based%20memory%20storage%20and%0Aupdating%2C%20and%20neuromorphic%20hardware/software%20optimization.%20Furthermore%2C%20we%20also%0Areview%20the%20latest%20research%20on%20embodied%20agents%20across%20these%20four%20aspects%20and%0Aanalyze%20the%20gap%20between%20current%20AI%20systems%20and%20human%20intelligence.%20By%0Asynthesizing%20insights%20from%20neuroscience%2C%20we%20outline%20a%20roadmap%20towards%20the%0Adevelopment%20of%20generalizable%2C%20autonomous%20agents%20capable%20of%20human-level%0Aintelligence%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07634v1&entry.124074799=Read"},
{"title": "A Comparative Study on Dynamic Graph Embedding based on Mamba and\n  Transformers", "author": "Ashish Parmanand Pandey and Alan John Varghese and Sarang Patil and Mengjia Xu", "abstract": "  Dynamic graph embedding has emerged as an important technique for modeling\ncomplex time-evolving networks across diverse domains. While transformer-based\nmodels have shown promise in capturing long-range dependencies in temporal\ngraph data, they face scalability challenges due to quadratic computational\ncomplexity. This study presents a comparative analysis of dynamic graph\nembedding approaches using transformers and the recently proposed Mamba\narchitecture, a state-space model with linear complexity. We introduce three\nnovel models: TransformerG2G augment with graph convolutional networks,\n\\mathcal{DG}-Mamba, and \\mathcal{GDG}-Mamba with graph isomorphism network edge\nconvolutions. Our experiments on multiple benchmark datasets demonstrate that\nMamba-based models achieve comparable or superior performance to\ntransformer-based approaches in link prediction tasks while offering\nsignificant computational efficiency gains on longer sequences. Notably,\n\\mathcal{DG}-Mamba variants consistently outperform transformer-based models on\ndatasets with high temporal variability, such as UCI, Bitcoin, and Reality\nMining, while maintaining competitive performance on more stable graphs like\nSBM. We provide insights into the learned temporal dependencies through\nanalysis of attention weights and state matrices, revealing the models' ability\nto capture complex temporal patterns. By effectively combining state-space\nmodels with graph neural networks, our work addresses key limitations of\nprevious approaches and contributes to the growing body of research on\nefficient temporal graph representation learning. These findings offer\npromising directions for scaling dynamic graph embedding to larger, more\ncomplex real-world networks, potentially enabling new applications in areas\nsuch as social network analysis, financial modeling, and biological system\ndynamics.\n", "link": "http://arxiv.org/abs/2412.11293v2", "date": "2025-05-12", "relevancy": 2.1581, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5512}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5388}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20on%20Dynamic%20Graph%20Embedding%20based%20on%20Mamba%20and%0A%20%20Transformers&body=Title%3A%20A%20Comparative%20Study%20on%20Dynamic%20Graph%20Embedding%20based%20on%20Mamba%20and%0A%20%20Transformers%0AAuthor%3A%20Ashish%20Parmanand%20Pandey%20and%20Alan%20John%20Varghese%20and%20Sarang%20Patil%20and%20Mengjia%20Xu%0AAbstract%3A%20%20%20Dynamic%20graph%20embedding%20has%20emerged%20as%20an%20important%20technique%20for%20modeling%0Acomplex%20time-evolving%20networks%20across%20diverse%20domains.%20While%20transformer-based%0Amodels%20have%20shown%20promise%20in%20capturing%20long-range%20dependencies%20in%20temporal%0Agraph%20data%2C%20they%20face%20scalability%20challenges%20due%20to%20quadratic%20computational%0Acomplexity.%20This%20study%20presents%20a%20comparative%20analysis%20of%20dynamic%20graph%0Aembedding%20approaches%20using%20transformers%20and%20the%20recently%20proposed%20Mamba%0Aarchitecture%2C%20a%20state-space%20model%20with%20linear%20complexity.%20We%20introduce%20three%0Anovel%20models%3A%20TransformerG2G%20augment%20with%20graph%20convolutional%20networks%2C%0A%5Cmathcal%7BDG%7D-Mamba%2C%20and%20%5Cmathcal%7BGDG%7D-Mamba%20with%20graph%20isomorphism%20network%20edge%0Aconvolutions.%20Our%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20that%0AMamba-based%20models%20achieve%20comparable%20or%20superior%20performance%20to%0Atransformer-based%20approaches%20in%20link%20prediction%20tasks%20while%20offering%0Asignificant%20computational%20efficiency%20gains%20on%20longer%20sequences.%20Notably%2C%0A%5Cmathcal%7BDG%7D-Mamba%20variants%20consistently%20outperform%20transformer-based%20models%20on%0Adatasets%20with%20high%20temporal%20variability%2C%20such%20as%20UCI%2C%20Bitcoin%2C%20and%20Reality%0AMining%2C%20while%20maintaining%20competitive%20performance%20on%20more%20stable%20graphs%20like%0ASBM.%20We%20provide%20insights%20into%20the%20learned%20temporal%20dependencies%20through%0Aanalysis%20of%20attention%20weights%20and%20state%20matrices%2C%20revealing%20the%20models%27%20ability%0Ato%20capture%20complex%20temporal%20patterns.%20By%20effectively%20combining%20state-space%0Amodels%20with%20graph%20neural%20networks%2C%20our%20work%20addresses%20key%20limitations%20of%0Aprevious%20approaches%20and%20contributes%20to%20the%20growing%20body%20of%20research%20on%0Aefficient%20temporal%20graph%20representation%20learning.%20These%20findings%20offer%0Apromising%20directions%20for%20scaling%20dynamic%20graph%20embedding%20to%20larger%2C%20more%0Acomplex%20real-world%20networks%2C%20potentially%20enabling%20new%20applications%20in%20areas%0Asuch%20as%20social%20network%20analysis%2C%20financial%20modeling%2C%20and%20biological%20system%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520on%2520Dynamic%2520Graph%2520Embedding%2520based%2520on%2520Mamba%2520and%250A%2520%2520Transformers%26entry.906535625%3DAshish%2520Parmanand%2520Pandey%2520and%2520Alan%2520John%2520Varghese%2520and%2520Sarang%2520Patil%2520and%2520Mengjia%2520Xu%26entry.1292438233%3D%2520%2520Dynamic%2520graph%2520embedding%2520has%2520emerged%2520as%2520an%2520important%2520technique%2520for%2520modeling%250Acomplex%2520time-evolving%2520networks%2520across%2520diverse%2520domains.%2520While%2520transformer-based%250Amodels%2520have%2520shown%2520promise%2520in%2520capturing%2520long-range%2520dependencies%2520in%2520temporal%250Agraph%2520data%252C%2520they%2520face%2520scalability%2520challenges%2520due%2520to%2520quadratic%2520computational%250Acomplexity.%2520This%2520study%2520presents%2520a%2520comparative%2520analysis%2520of%2520dynamic%2520graph%250Aembedding%2520approaches%2520using%2520transformers%2520and%2520the%2520recently%2520proposed%2520Mamba%250Aarchitecture%252C%2520a%2520state-space%2520model%2520with%2520linear%2520complexity.%2520We%2520introduce%2520three%250Anovel%2520models%253A%2520TransformerG2G%2520augment%2520with%2520graph%2520convolutional%2520networks%252C%250A%255Cmathcal%257BDG%257D-Mamba%252C%2520and%2520%255Cmathcal%257BGDG%257D-Mamba%2520with%2520graph%2520isomorphism%2520network%2520edge%250Aconvolutions.%2520Our%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520that%250AMamba-based%2520models%2520achieve%2520comparable%2520or%2520superior%2520performance%2520to%250Atransformer-based%2520approaches%2520in%2520link%2520prediction%2520tasks%2520while%2520offering%250Asignificant%2520computational%2520efficiency%2520gains%2520on%2520longer%2520sequences.%2520Notably%252C%250A%255Cmathcal%257BDG%257D-Mamba%2520variants%2520consistently%2520outperform%2520transformer-based%2520models%2520on%250Adatasets%2520with%2520high%2520temporal%2520variability%252C%2520such%2520as%2520UCI%252C%2520Bitcoin%252C%2520and%2520Reality%250AMining%252C%2520while%2520maintaining%2520competitive%2520performance%2520on%2520more%2520stable%2520graphs%2520like%250ASBM.%2520We%2520provide%2520insights%2520into%2520the%2520learned%2520temporal%2520dependencies%2520through%250Aanalysis%2520of%2520attention%2520weights%2520and%2520state%2520matrices%252C%2520revealing%2520the%2520models%2527%2520ability%250Ato%2520capture%2520complex%2520temporal%2520patterns.%2520By%2520effectively%2520combining%2520state-space%250Amodels%2520with%2520graph%2520neural%2520networks%252C%2520our%2520work%2520addresses%2520key%2520limitations%2520of%250Aprevious%2520approaches%2520and%2520contributes%2520to%2520the%2520growing%2520body%2520of%2520research%2520on%250Aefficient%2520temporal%2520graph%2520representation%2520learning.%2520These%2520findings%2520offer%250Apromising%2520directions%2520for%2520scaling%2520dynamic%2520graph%2520embedding%2520to%2520larger%252C%2520more%250Acomplex%2520real-world%2520networks%252C%2520potentially%2520enabling%2520new%2520applications%2520in%2520areas%250Asuch%2520as%2520social%2520network%2520analysis%252C%2520financial%2520modeling%252C%2520and%2520biological%2520system%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20on%20Dynamic%20Graph%20Embedding%20based%20on%20Mamba%20and%0A%20%20Transformers&entry.906535625=Ashish%20Parmanand%20Pandey%20and%20Alan%20John%20Varghese%20and%20Sarang%20Patil%20and%20Mengjia%20Xu&entry.1292438233=%20%20Dynamic%20graph%20embedding%20has%20emerged%20as%20an%20important%20technique%20for%20modeling%0Acomplex%20time-evolving%20networks%20across%20diverse%20domains.%20While%20transformer-based%0Amodels%20have%20shown%20promise%20in%20capturing%20long-range%20dependencies%20in%20temporal%0Agraph%20data%2C%20they%20face%20scalability%20challenges%20due%20to%20quadratic%20computational%0Acomplexity.%20This%20study%20presents%20a%20comparative%20analysis%20of%20dynamic%20graph%0Aembedding%20approaches%20using%20transformers%20and%20the%20recently%20proposed%20Mamba%0Aarchitecture%2C%20a%20state-space%20model%20with%20linear%20complexity.%20We%20introduce%20three%0Anovel%20models%3A%20TransformerG2G%20augment%20with%20graph%20convolutional%20networks%2C%0A%5Cmathcal%7BDG%7D-Mamba%2C%20and%20%5Cmathcal%7BGDG%7D-Mamba%20with%20graph%20isomorphism%20network%20edge%0Aconvolutions.%20Our%20experiments%20on%20multiple%20benchmark%20datasets%20demonstrate%20that%0AMamba-based%20models%20achieve%20comparable%20or%20superior%20performance%20to%0Atransformer-based%20approaches%20in%20link%20prediction%20tasks%20while%20offering%0Asignificant%20computational%20efficiency%20gains%20on%20longer%20sequences.%20Notably%2C%0A%5Cmathcal%7BDG%7D-Mamba%20variants%20consistently%20outperform%20transformer-based%20models%20on%0Adatasets%20with%20high%20temporal%20variability%2C%20such%20as%20UCI%2C%20Bitcoin%2C%20and%20Reality%0AMining%2C%20while%20maintaining%20competitive%20performance%20on%20more%20stable%20graphs%20like%0ASBM.%20We%20provide%20insights%20into%20the%20learned%20temporal%20dependencies%20through%0Aanalysis%20of%20attention%20weights%20and%20state%20matrices%2C%20revealing%20the%20models%27%20ability%0Ato%20capture%20complex%20temporal%20patterns.%20By%20effectively%20combining%20state-space%0Amodels%20with%20graph%20neural%20networks%2C%20our%20work%20addresses%20key%20limitations%20of%0Aprevious%20approaches%20and%20contributes%20to%20the%20growing%20body%20of%20research%20on%0Aefficient%20temporal%20graph%20representation%20learning.%20These%20findings%20offer%0Apromising%20directions%20for%20scaling%20dynamic%20graph%20embedding%20to%20larger%2C%20more%0Acomplex%20real-world%20networks%2C%20potentially%20enabling%20new%20applications%20in%20areas%0Asuch%20as%20social%20network%20analysis%2C%20financial%20modeling%2C%20and%20biological%20system%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11293v2&entry.124074799=Read"},
{"title": "Self-Supervised Event Representations: Towards Accurate, Real-Time\n  Perception on SoC FPGAs", "author": "Kamil Jeziorek and Tomasz Kryjak", "abstract": "  Event cameras offer significant advantages over traditional frame-based\nsensors. These include microsecond temporal resolution, robustness under\nvarying lighting conditions and low power consumption. Nevertheless, the\neffective processing of their sparse, asynchronous event streams remains\nchallenging. Existing approaches to this problem can be categorised into two\ndistinct groups. The first group involves the direct processing of event data\nwith neural models, such as Spiking Neural Networks or Graph Convolutional\nNeural Networks. However, this approach is often accompanied by a compromise in\nterms of qualitative performance. The second group involves the conversion of\nevents into dense representations with handcrafted aggregation functions, which\ncan boost accuracy at the cost of temporal fidelity. This paper introduces a\nnovel Self-Supervised Event Representation (SSER) method leveraging Gated\nRecurrent Unit (GRU) networks to achieve precise per-pixel encoding of event\ntimestamps and polarities without temporal discretisation. The recurrent layers\nare trained in a self-supervised manner to maximise the fidelity of event-time\nencoding. The inference is performed with event representations generated\nasynchronously, thus ensuring compatibility with high-throughput sensors. The\nexperimental validation demonstrates that SSER outperforms aggregation-based\nbaselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx\nobject detection datasets. Furthermore, the paper presents the first hardware\nimplementation of recurrent representation for event data on a System-on-Chip\nFPGA, achieving sub-microsecond latency and power consumption between 1-2 W,\nsuitable for real-time, power-efficient applications. Code is available at\nhttps://github.com/vision-agh/RecRepEvent.\n", "link": "http://arxiv.org/abs/2505.07556v1", "date": "2025-05-12", "relevancy": 2.1348, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5707}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Event%20Representations%3A%20Towards%20Accurate%2C%20Real-Time%0A%20%20Perception%20on%20SoC%20FPGAs&body=Title%3A%20Self-Supervised%20Event%20Representations%3A%20Towards%20Accurate%2C%20Real-Time%0A%20%20Perception%20on%20SoC%20FPGAs%0AAuthor%3A%20Kamil%20Jeziorek%20and%20Tomasz%20Kryjak%0AAbstract%3A%20%20%20Event%20cameras%20offer%20significant%20advantages%20over%20traditional%20frame-based%0Asensors.%20These%20include%20microsecond%20temporal%20resolution%2C%20robustness%20under%0Avarying%20lighting%20conditions%20and%20low%20power%20consumption.%20Nevertheless%2C%20the%0Aeffective%20processing%20of%20their%20sparse%2C%20asynchronous%20event%20streams%20remains%0Achallenging.%20Existing%20approaches%20to%20this%20problem%20can%20be%20categorised%20into%20two%0Adistinct%20groups.%20The%20first%20group%20involves%20the%20direct%20processing%20of%20event%20data%0Awith%20neural%20models%2C%20such%20as%20Spiking%20Neural%20Networks%20or%20Graph%20Convolutional%0ANeural%20Networks.%20However%2C%20this%20approach%20is%20often%20accompanied%20by%20a%20compromise%20in%0Aterms%20of%20qualitative%20performance.%20The%20second%20group%20involves%20the%20conversion%20of%0Aevents%20into%20dense%20representations%20with%20handcrafted%20aggregation%20functions%2C%20which%0Acan%20boost%20accuracy%20at%20the%20cost%20of%20temporal%20fidelity.%20This%20paper%20introduces%20a%0Anovel%20Self-Supervised%20Event%20Representation%20%28SSER%29%20method%20leveraging%20Gated%0ARecurrent%20Unit%20%28GRU%29%20networks%20to%20achieve%20precise%20per-pixel%20encoding%20of%20event%0Atimestamps%20and%20polarities%20without%20temporal%20discretisation.%20The%20recurrent%20layers%0Aare%20trained%20in%20a%20self-supervised%20manner%20to%20maximise%20the%20fidelity%20of%20event-time%0Aencoding.%20The%20inference%20is%20performed%20with%20event%20representations%20generated%0Aasynchronously%2C%20thus%20ensuring%20compatibility%20with%20high-throughput%20sensors.%20The%0Aexperimental%20validation%20demonstrates%20that%20SSER%20outperforms%20aggregation-based%0Abaselines%2C%20achieving%20improvements%20of%202.4%25%20mAP%20and%200.6%25%20on%20the%20Gen1%20and%201%20Mpx%0Aobject%20detection%20datasets.%20Furthermore%2C%20the%20paper%20presents%20the%20first%20hardware%0Aimplementation%20of%20recurrent%20representation%20for%20event%20data%20on%20a%20System-on-Chip%0AFPGA%2C%20achieving%20sub-microsecond%20latency%20and%20power%20consumption%20between%201-2%20W%2C%0Asuitable%20for%20real-time%2C%20power-efficient%20applications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/vision-agh/RecRepEvent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Event%2520Representations%253A%2520Towards%2520Accurate%252C%2520Real-Time%250A%2520%2520Perception%2520on%2520SoC%2520FPGAs%26entry.906535625%3DKamil%2520Jeziorek%2520and%2520Tomasz%2520Kryjak%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520significant%2520advantages%2520over%2520traditional%2520frame-based%250Asensors.%2520These%2520include%2520microsecond%2520temporal%2520resolution%252C%2520robustness%2520under%250Avarying%2520lighting%2520conditions%2520and%2520low%2520power%2520consumption.%2520Nevertheless%252C%2520the%250Aeffective%2520processing%2520of%2520their%2520sparse%252C%2520asynchronous%2520event%2520streams%2520remains%250Achallenging.%2520Existing%2520approaches%2520to%2520this%2520problem%2520can%2520be%2520categorised%2520into%2520two%250Adistinct%2520groups.%2520The%2520first%2520group%2520involves%2520the%2520direct%2520processing%2520of%2520event%2520data%250Awith%2520neural%2520models%252C%2520such%2520as%2520Spiking%2520Neural%2520Networks%2520or%2520Graph%2520Convolutional%250ANeural%2520Networks.%2520However%252C%2520this%2520approach%2520is%2520often%2520accompanied%2520by%2520a%2520compromise%2520in%250Aterms%2520of%2520qualitative%2520performance.%2520The%2520second%2520group%2520involves%2520the%2520conversion%2520of%250Aevents%2520into%2520dense%2520representations%2520with%2520handcrafted%2520aggregation%2520functions%252C%2520which%250Acan%2520boost%2520accuracy%2520at%2520the%2520cost%2520of%2520temporal%2520fidelity.%2520This%2520paper%2520introduces%2520a%250Anovel%2520Self-Supervised%2520Event%2520Representation%2520%2528SSER%2529%2520method%2520leveraging%2520Gated%250ARecurrent%2520Unit%2520%2528GRU%2529%2520networks%2520to%2520achieve%2520precise%2520per-pixel%2520encoding%2520of%2520event%250Atimestamps%2520and%2520polarities%2520without%2520temporal%2520discretisation.%2520The%2520recurrent%2520layers%250Aare%2520trained%2520in%2520a%2520self-supervised%2520manner%2520to%2520maximise%2520the%2520fidelity%2520of%2520event-time%250Aencoding.%2520The%2520inference%2520is%2520performed%2520with%2520event%2520representations%2520generated%250Aasynchronously%252C%2520thus%2520ensuring%2520compatibility%2520with%2520high-throughput%2520sensors.%2520The%250Aexperimental%2520validation%2520demonstrates%2520that%2520SSER%2520outperforms%2520aggregation-based%250Abaselines%252C%2520achieving%2520improvements%2520of%25202.4%2525%2520mAP%2520and%25200.6%2525%2520on%2520the%2520Gen1%2520and%25201%2520Mpx%250Aobject%2520detection%2520datasets.%2520Furthermore%252C%2520the%2520paper%2520presents%2520the%2520first%2520hardware%250Aimplementation%2520of%2520recurrent%2520representation%2520for%2520event%2520data%2520on%2520a%2520System-on-Chip%250AFPGA%252C%2520achieving%2520sub-microsecond%2520latency%2520and%2520power%2520consumption%2520between%25201-2%2520W%252C%250Asuitable%2520for%2520real-time%252C%2520power-efficient%2520applications.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/vision-agh/RecRepEvent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Event%20Representations%3A%20Towards%20Accurate%2C%20Real-Time%0A%20%20Perception%20on%20SoC%20FPGAs&entry.906535625=Kamil%20Jeziorek%20and%20Tomasz%20Kryjak&entry.1292438233=%20%20Event%20cameras%20offer%20significant%20advantages%20over%20traditional%20frame-based%0Asensors.%20These%20include%20microsecond%20temporal%20resolution%2C%20robustness%20under%0Avarying%20lighting%20conditions%20and%20low%20power%20consumption.%20Nevertheless%2C%20the%0Aeffective%20processing%20of%20their%20sparse%2C%20asynchronous%20event%20streams%20remains%0Achallenging.%20Existing%20approaches%20to%20this%20problem%20can%20be%20categorised%20into%20two%0Adistinct%20groups.%20The%20first%20group%20involves%20the%20direct%20processing%20of%20event%20data%0Awith%20neural%20models%2C%20such%20as%20Spiking%20Neural%20Networks%20or%20Graph%20Convolutional%0ANeural%20Networks.%20However%2C%20this%20approach%20is%20often%20accompanied%20by%20a%20compromise%20in%0Aterms%20of%20qualitative%20performance.%20The%20second%20group%20involves%20the%20conversion%20of%0Aevents%20into%20dense%20representations%20with%20handcrafted%20aggregation%20functions%2C%20which%0Acan%20boost%20accuracy%20at%20the%20cost%20of%20temporal%20fidelity.%20This%20paper%20introduces%20a%0Anovel%20Self-Supervised%20Event%20Representation%20%28SSER%29%20method%20leveraging%20Gated%0ARecurrent%20Unit%20%28GRU%29%20networks%20to%20achieve%20precise%20per-pixel%20encoding%20of%20event%0Atimestamps%20and%20polarities%20without%20temporal%20discretisation.%20The%20recurrent%20layers%0Aare%20trained%20in%20a%20self-supervised%20manner%20to%20maximise%20the%20fidelity%20of%20event-time%0Aencoding.%20The%20inference%20is%20performed%20with%20event%20representations%20generated%0Aasynchronously%2C%20thus%20ensuring%20compatibility%20with%20high-throughput%20sensors.%20The%0Aexperimental%20validation%20demonstrates%20that%20SSER%20outperforms%20aggregation-based%0Abaselines%2C%20achieving%20improvements%20of%202.4%25%20mAP%20and%200.6%25%20on%20the%20Gen1%20and%201%20Mpx%0Aobject%20detection%20datasets.%20Furthermore%2C%20the%20paper%20presents%20the%20first%20hardware%0Aimplementation%20of%20recurrent%20representation%20for%20event%20data%20on%20a%20System-on-Chip%0AFPGA%2C%20achieving%20sub-microsecond%20latency%20and%20power%20consumption%20between%201-2%20W%2C%0Asuitable%20for%20real-time%2C%20power-efficient%20applications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/vision-agh/RecRepEvent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07556v1&entry.124074799=Read"},
{"title": "Identifying Drivers of Predictive Aleatoric Uncertainty", "author": "Pascal Iversen and Simon Witzke and Katharina Baum and Bernhard Y. Renard", "abstract": "  Explainability and uncertainty quantification are key to trustable artificial\nintelligence. However, the reasoning behind uncertainty estimates is generally\nleft unexplained. Identifying the drivers of uncertainty complements\nexplanations of point predictions in recognizing model limitations and\nenhancing transparent decision-making. So far, explanations of uncertainties\nhave been rarely studied. The few exceptions rely on Bayesian neural networks\nor technically intricate approaches, such as auxiliary generative models,\nthereby hindering their broad adoption. We propose a straightforward approach\nto explain predictive aleatoric uncertainties. We estimate uncertainty in\nregression as predictive variance by adapting a neural network with a Gaussian\noutput distribution. Subsequently, we apply out-of-the-box explainers to the\nmodel's variance output. This approach can explain uncertainty influences more\nreliably than complex published approaches, which we demonstrate in a synthetic\nsetting with a known data-generating process. We substantiate our findings with\na nuanced, quantitative benchmark including synthetic and real, tabular and\nimage datasets. For this, we adapt metrics from conventional XAI research to\nuncertainty explanations. Overall, the proposed method explains uncertainty\nestimates with little modifications to the model architecture and outperforms\nmore intricate methods in most settings.\n", "link": "http://arxiv.org/abs/2312.07252v3", "date": "2025-05-12", "relevancy": 2.13, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5618}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5327}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Drivers%20of%20Predictive%20Aleatoric%20Uncertainty&body=Title%3A%20Identifying%20Drivers%20of%20Predictive%20Aleatoric%20Uncertainty%0AAuthor%3A%20Pascal%20Iversen%20and%20Simon%20Witzke%20and%20Katharina%20Baum%20and%20Bernhard%20Y.%20Renard%0AAbstract%3A%20%20%20Explainability%20and%20uncertainty%20quantification%20are%20key%20to%20trustable%20artificial%0Aintelligence.%20However%2C%20the%20reasoning%20behind%20uncertainty%20estimates%20is%20generally%0Aleft%20unexplained.%20Identifying%20the%20drivers%20of%20uncertainty%20complements%0Aexplanations%20of%20point%20predictions%20in%20recognizing%20model%20limitations%20and%0Aenhancing%20transparent%20decision-making.%20So%20far%2C%20explanations%20of%20uncertainties%0Ahave%20been%20rarely%20studied.%20The%20few%20exceptions%20rely%20on%20Bayesian%20neural%20networks%0Aor%20technically%20intricate%20approaches%2C%20such%20as%20auxiliary%20generative%20models%2C%0Athereby%20hindering%20their%20broad%20adoption.%20We%20propose%20a%20straightforward%20approach%0Ato%20explain%20predictive%20aleatoric%20uncertainties.%20We%20estimate%20uncertainty%20in%0Aregression%20as%20predictive%20variance%20by%20adapting%20a%20neural%20network%20with%20a%20Gaussian%0Aoutput%20distribution.%20Subsequently%2C%20we%20apply%20out-of-the-box%20explainers%20to%20the%0Amodel%27s%20variance%20output.%20This%20approach%20can%20explain%20uncertainty%20influences%20more%0Areliably%20than%20complex%20published%20approaches%2C%20which%20we%20demonstrate%20in%20a%20synthetic%0Asetting%20with%20a%20known%20data-generating%20process.%20We%20substantiate%20our%20findings%20with%0Aa%20nuanced%2C%20quantitative%20benchmark%20including%20synthetic%20and%20real%2C%20tabular%20and%0Aimage%20datasets.%20For%20this%2C%20we%20adapt%20metrics%20from%20conventional%20XAI%20research%20to%0Auncertainty%20explanations.%20Overall%2C%20the%20proposed%20method%20explains%20uncertainty%0Aestimates%20with%20little%20modifications%20to%20the%20model%20architecture%20and%20outperforms%0Amore%20intricate%20methods%20in%20most%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07252v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Drivers%2520of%2520Predictive%2520Aleatoric%2520Uncertainty%26entry.906535625%3DPascal%2520Iversen%2520and%2520Simon%2520Witzke%2520and%2520Katharina%2520Baum%2520and%2520Bernhard%2520Y.%2520Renard%26entry.1292438233%3D%2520%2520Explainability%2520and%2520uncertainty%2520quantification%2520are%2520key%2520to%2520trustable%2520artificial%250Aintelligence.%2520However%252C%2520the%2520reasoning%2520behind%2520uncertainty%2520estimates%2520is%2520generally%250Aleft%2520unexplained.%2520Identifying%2520the%2520drivers%2520of%2520uncertainty%2520complements%250Aexplanations%2520of%2520point%2520predictions%2520in%2520recognizing%2520model%2520limitations%2520and%250Aenhancing%2520transparent%2520decision-making.%2520So%2520far%252C%2520explanations%2520of%2520uncertainties%250Ahave%2520been%2520rarely%2520studied.%2520The%2520few%2520exceptions%2520rely%2520on%2520Bayesian%2520neural%2520networks%250Aor%2520technically%2520intricate%2520approaches%252C%2520such%2520as%2520auxiliary%2520generative%2520models%252C%250Athereby%2520hindering%2520their%2520broad%2520adoption.%2520We%2520propose%2520a%2520straightforward%2520approach%250Ato%2520explain%2520predictive%2520aleatoric%2520uncertainties.%2520We%2520estimate%2520uncertainty%2520in%250Aregression%2520as%2520predictive%2520variance%2520by%2520adapting%2520a%2520neural%2520network%2520with%2520a%2520Gaussian%250Aoutput%2520distribution.%2520Subsequently%252C%2520we%2520apply%2520out-of-the-box%2520explainers%2520to%2520the%250Amodel%2527s%2520variance%2520output.%2520This%2520approach%2520can%2520explain%2520uncertainty%2520influences%2520more%250Areliably%2520than%2520complex%2520published%2520approaches%252C%2520which%2520we%2520demonstrate%2520in%2520a%2520synthetic%250Asetting%2520with%2520a%2520known%2520data-generating%2520process.%2520We%2520substantiate%2520our%2520findings%2520with%250Aa%2520nuanced%252C%2520quantitative%2520benchmark%2520including%2520synthetic%2520and%2520real%252C%2520tabular%2520and%250Aimage%2520datasets.%2520For%2520this%252C%2520we%2520adapt%2520metrics%2520from%2520conventional%2520XAI%2520research%2520to%250Auncertainty%2520explanations.%2520Overall%252C%2520the%2520proposed%2520method%2520explains%2520uncertainty%250Aestimates%2520with%2520little%2520modifications%2520to%2520the%2520model%2520architecture%2520and%2520outperforms%250Amore%2520intricate%2520methods%2520in%2520most%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07252v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Drivers%20of%20Predictive%20Aleatoric%20Uncertainty&entry.906535625=Pascal%20Iversen%20and%20Simon%20Witzke%20and%20Katharina%20Baum%20and%20Bernhard%20Y.%20Renard&entry.1292438233=%20%20Explainability%20and%20uncertainty%20quantification%20are%20key%20to%20trustable%20artificial%0Aintelligence.%20However%2C%20the%20reasoning%20behind%20uncertainty%20estimates%20is%20generally%0Aleft%20unexplained.%20Identifying%20the%20drivers%20of%20uncertainty%20complements%0Aexplanations%20of%20point%20predictions%20in%20recognizing%20model%20limitations%20and%0Aenhancing%20transparent%20decision-making.%20So%20far%2C%20explanations%20of%20uncertainties%0Ahave%20been%20rarely%20studied.%20The%20few%20exceptions%20rely%20on%20Bayesian%20neural%20networks%0Aor%20technically%20intricate%20approaches%2C%20such%20as%20auxiliary%20generative%20models%2C%0Athereby%20hindering%20their%20broad%20adoption.%20We%20propose%20a%20straightforward%20approach%0Ato%20explain%20predictive%20aleatoric%20uncertainties.%20We%20estimate%20uncertainty%20in%0Aregression%20as%20predictive%20variance%20by%20adapting%20a%20neural%20network%20with%20a%20Gaussian%0Aoutput%20distribution.%20Subsequently%2C%20we%20apply%20out-of-the-box%20explainers%20to%20the%0Amodel%27s%20variance%20output.%20This%20approach%20can%20explain%20uncertainty%20influences%20more%0Areliably%20than%20complex%20published%20approaches%2C%20which%20we%20demonstrate%20in%20a%20synthetic%0Asetting%20with%20a%20known%20data-generating%20process.%20We%20substantiate%20our%20findings%20with%0Aa%20nuanced%2C%20quantitative%20benchmark%20including%20synthetic%20and%20real%2C%20tabular%20and%0Aimage%20datasets.%20For%20this%2C%20we%20adapt%20metrics%20from%20conventional%20XAI%20research%20to%0Auncertainty%20explanations.%20Overall%2C%20the%20proposed%20method%20explains%20uncertainty%0Aestimates%20with%20little%20modifications%20to%20the%20model%20architecture%20and%20outperforms%0Amore%20intricate%20methods%20in%20most%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07252v3&entry.124074799=Read"},
{"title": "TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for\n  Benchmarking Target Person Tracking", "author": "Hanjing Ye and Yu Zhan and Weixi Situ and Guangcheng Chen and Jingwen Yu and Kuanqi Cai and Hong Zhang", "abstract": "  Tracking a target person from robot-egocentric views is crucial for\ndeveloping autonomous robots that provide continuous personalized assistance or\ncollaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most\nexisting target person tracking (TPT) benchmarks are limited to controlled\nlaboratory environments with few distractions, clean backgrounds, and\nshort-term occlusions. In this paper, we introduce a large-scale dataset\ndesigned for TPT in crowded and unstructured environments, demonstrated through\na robot-person following task. The dataset is collected by a human pushing a\nsensor-equipped cart while following a target person, capturing human-like\nfollowing behavior and emphasizing long-term tracking challenges, including\nfrequent occlusions and the need for re-identification from numerous\npedestrians. It includes multi-modal data streams, including odometry, 3D\nLiDAR, IMU, panoptic, and RGB-D images, along with exhaustively annotated 2D\nbounding boxes of the target person across 35 sequences, both indoors and\noutdoors. Using this dataset and visual annotations, we perform extensive\nexperiments with existing TPT methods, offering a thorough analysis of their\nlimitations and suggesting future research directions.\n", "link": "http://arxiv.org/abs/2505.07446v1", "date": "2025-05-12", "relevancy": 2.1198, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5374}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPT-Bench%3A%20A%20Large-Scale%2C%20Long-Term%20and%20Robot-Egocentric%20Dataset%20for%0A%20%20Benchmarking%20Target%20Person%20Tracking&body=Title%3A%20TPT-Bench%3A%20A%20Large-Scale%2C%20Long-Term%20and%20Robot-Egocentric%20Dataset%20for%0A%20%20Benchmarking%20Target%20Person%20Tracking%0AAuthor%3A%20Hanjing%20Ye%20and%20Yu%20Zhan%20and%20Weixi%20Situ%20and%20Guangcheng%20Chen%20and%20Jingwen%20Yu%20and%20Kuanqi%20Cai%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Tracking%20a%20target%20person%20from%20robot-egocentric%20views%20is%20crucial%20for%0Adeveloping%20autonomous%20robots%20that%20provide%20continuous%20personalized%20assistance%20or%0Acollaboration%20in%20Human-Robot%20Interaction%20%28HRI%29%20and%20Embodied%20AI.%20However%2C%20most%0Aexisting%20target%20person%20tracking%20%28TPT%29%20benchmarks%20are%20limited%20to%20controlled%0Alaboratory%20environments%20with%20few%20distractions%2C%20clean%20backgrounds%2C%20and%0Ashort-term%20occlusions.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20dataset%0Adesigned%20for%20TPT%20in%20crowded%20and%20unstructured%20environments%2C%20demonstrated%20through%0Aa%20robot-person%20following%20task.%20The%20dataset%20is%20collected%20by%20a%20human%20pushing%20a%0Asensor-equipped%20cart%20while%20following%20a%20target%20person%2C%20capturing%20human-like%0Afollowing%20behavior%20and%20emphasizing%20long-term%20tracking%20challenges%2C%20including%0Afrequent%20occlusions%20and%20the%20need%20for%20re-identification%20from%20numerous%0Apedestrians.%20It%20includes%20multi-modal%20data%20streams%2C%20including%20odometry%2C%203D%0ALiDAR%2C%20IMU%2C%20panoptic%2C%20and%20RGB-D%20images%2C%20along%20with%20exhaustively%20annotated%202D%0Abounding%20boxes%20of%20the%20target%20person%20across%2035%20sequences%2C%20both%20indoors%20and%0Aoutdoors.%20Using%20this%20dataset%20and%20visual%20annotations%2C%20we%20perform%20extensive%0Aexperiments%20with%20existing%20TPT%20methods%2C%20offering%20a%20thorough%20analysis%20of%20their%0Alimitations%20and%20suggesting%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPT-Bench%253A%2520A%2520Large-Scale%252C%2520Long-Term%2520and%2520Robot-Egocentric%2520Dataset%2520for%250A%2520%2520Benchmarking%2520Target%2520Person%2520Tracking%26entry.906535625%3DHanjing%2520Ye%2520and%2520Yu%2520Zhan%2520and%2520Weixi%2520Situ%2520and%2520Guangcheng%2520Chen%2520and%2520Jingwen%2520Yu%2520and%2520Kuanqi%2520Cai%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Tracking%2520a%2520target%2520person%2520from%2520robot-egocentric%2520views%2520is%2520crucial%2520for%250Adeveloping%2520autonomous%2520robots%2520that%2520provide%2520continuous%2520personalized%2520assistance%2520or%250Acollaboration%2520in%2520Human-Robot%2520Interaction%2520%2528HRI%2529%2520and%2520Embodied%2520AI.%2520However%252C%2520most%250Aexisting%2520target%2520person%2520tracking%2520%2528TPT%2529%2520benchmarks%2520are%2520limited%2520to%2520controlled%250Alaboratory%2520environments%2520with%2520few%2520distractions%252C%2520clean%2520backgrounds%252C%2520and%250Ashort-term%2520occlusions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520large-scale%2520dataset%250Adesigned%2520for%2520TPT%2520in%2520crowded%2520and%2520unstructured%2520environments%252C%2520demonstrated%2520through%250Aa%2520robot-person%2520following%2520task.%2520The%2520dataset%2520is%2520collected%2520by%2520a%2520human%2520pushing%2520a%250Asensor-equipped%2520cart%2520while%2520following%2520a%2520target%2520person%252C%2520capturing%2520human-like%250Afollowing%2520behavior%2520and%2520emphasizing%2520long-term%2520tracking%2520challenges%252C%2520including%250Afrequent%2520occlusions%2520and%2520the%2520need%2520for%2520re-identification%2520from%2520numerous%250Apedestrians.%2520It%2520includes%2520multi-modal%2520data%2520streams%252C%2520including%2520odometry%252C%25203D%250ALiDAR%252C%2520IMU%252C%2520panoptic%252C%2520and%2520RGB-D%2520images%252C%2520along%2520with%2520exhaustively%2520annotated%25202D%250Abounding%2520boxes%2520of%2520the%2520target%2520person%2520across%252035%2520sequences%252C%2520both%2520indoors%2520and%250Aoutdoors.%2520Using%2520this%2520dataset%2520and%2520visual%2520annotations%252C%2520we%2520perform%2520extensive%250Aexperiments%2520with%2520existing%2520TPT%2520methods%252C%2520offering%2520a%2520thorough%2520analysis%2520of%2520their%250Alimitations%2520and%2520suggesting%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPT-Bench%3A%20A%20Large-Scale%2C%20Long-Term%20and%20Robot-Egocentric%20Dataset%20for%0A%20%20Benchmarking%20Target%20Person%20Tracking&entry.906535625=Hanjing%20Ye%20and%20Yu%20Zhan%20and%20Weixi%20Situ%20and%20Guangcheng%20Chen%20and%20Jingwen%20Yu%20and%20Kuanqi%20Cai%20and%20Hong%20Zhang&entry.1292438233=%20%20Tracking%20a%20target%20person%20from%20robot-egocentric%20views%20is%20crucial%20for%0Adeveloping%20autonomous%20robots%20that%20provide%20continuous%20personalized%20assistance%20or%0Acollaboration%20in%20Human-Robot%20Interaction%20%28HRI%29%20and%20Embodied%20AI.%20However%2C%20most%0Aexisting%20target%20person%20tracking%20%28TPT%29%20benchmarks%20are%20limited%20to%20controlled%0Alaboratory%20environments%20with%20few%20distractions%2C%20clean%20backgrounds%2C%20and%0Ashort-term%20occlusions.%20In%20this%20paper%2C%20we%20introduce%20a%20large-scale%20dataset%0Adesigned%20for%20TPT%20in%20crowded%20and%20unstructured%20environments%2C%20demonstrated%20through%0Aa%20robot-person%20following%20task.%20The%20dataset%20is%20collected%20by%20a%20human%20pushing%20a%0Asensor-equipped%20cart%20while%20following%20a%20target%20person%2C%20capturing%20human-like%0Afollowing%20behavior%20and%20emphasizing%20long-term%20tracking%20challenges%2C%20including%0Afrequent%20occlusions%20and%20the%20need%20for%20re-identification%20from%20numerous%0Apedestrians.%20It%20includes%20multi-modal%20data%20streams%2C%20including%20odometry%2C%203D%0ALiDAR%2C%20IMU%2C%20panoptic%2C%20and%20RGB-D%20images%2C%20along%20with%20exhaustively%20annotated%202D%0Abounding%20boxes%20of%20the%20target%20person%20across%2035%20sequences%2C%20both%20indoors%20and%0Aoutdoors.%20Using%20this%20dataset%20and%20visual%20annotations%2C%20we%20perform%20extensive%0Aexperiments%20with%20existing%20TPT%20methods%2C%20offering%20a%20thorough%20analysis%20of%20their%0Alimitations%20and%20suggesting%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07446v1&entry.124074799=Read"},
{"title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular\n  Design", "author": "Zimo Yan and Jie Zhang and Zheng Xie and Chang Liu and Yizhen Liu and Yiping Song", "abstract": "  Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign.\n", "link": "http://arxiv.org/abs/2504.15587v2", "date": "2025-05-12", "relevancy": 2.1117, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5479}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5407}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaMolGen%3A%20A%20Neural%20Graph%20Motif%20Generation%20Model%20for%20De%20Novo%20Molecular%0A%20%20Design&body=Title%3A%20MetaMolGen%3A%20A%20Neural%20Graph%20Motif%20Generation%20Model%20for%20De%20Novo%20Molecular%0A%20%20Design%0AAuthor%3A%20Zimo%20Yan%20and%20Jie%20Zhang%20and%20Zheng%20Xie%20and%20Chang%20Liu%20and%20Yizhen%20Liu%20and%20Yiping%20Song%0AAbstract%3A%20%20%20Molecular%20generation%20plays%20an%20important%20role%20in%20drug%20discovery%20and%20materials%0Ascience%2C%20especially%20in%20data-scarce%20scenarios%20where%20traditional%20generative%0Amodels%20often%20struggle%20to%20achieve%20satisfactory%20conditional%20generalization.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20MetaMolGen%2C%20a%20first-order%0Ameta-learning-based%20molecular%20generator%20designed%20for%20few-shot%20and%0Aproperty-conditioned%20molecular%20generation.%20MetaMolGen%20standardizes%20the%0Adistribution%20of%20graph%20motifs%20by%20mapping%20them%20to%20a%20normalized%20latent%20space%2C%20and%0Aemploys%20a%20lightweight%20autoregressive%20sequence%20model%20to%20generate%20SMILES%0Asequences%20that%20faithfully%20reflect%20the%20underlying%20molecular%20structure.%20In%0Aaddition%2C%20it%20supports%20conditional%20generation%20of%20molecules%20with%20target%0Aproperties%20through%20a%20learnable%20property%20projector%20integrated%20into%20the%0Agenerative%20process.Experimental%20results%20demonstrate%20that%20MetaMolGen%0Aconsistently%20generates%20valid%20and%20diverse%20SMILES%20sequences%20under%20low-data%0Aregimes%2C%20outperforming%20conventional%20baselines.%20This%20highlights%20its%20advantage%20in%0Afast%20adaptation%20and%20efficient%20conditional%20generation%20for%20practical%20molecular%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaMolGen%253A%2520A%2520Neural%2520Graph%2520Motif%2520Generation%2520Model%2520for%2520De%2520Novo%2520Molecular%250A%2520%2520Design%26entry.906535625%3DZimo%2520Yan%2520and%2520Jie%2520Zhang%2520and%2520Zheng%2520Xie%2520and%2520Chang%2520Liu%2520and%2520Yizhen%2520Liu%2520and%2520Yiping%2520Song%26entry.1292438233%3D%2520%2520Molecular%2520generation%2520plays%2520an%2520important%2520role%2520in%2520drug%2520discovery%2520and%2520materials%250Ascience%252C%2520especially%2520in%2520data-scarce%2520scenarios%2520where%2520traditional%2520generative%250Amodels%2520often%2520struggle%2520to%2520achieve%2520satisfactory%2520conditional%2520generalization.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520MetaMolGen%252C%2520a%2520first-order%250Ameta-learning-based%2520molecular%2520generator%2520designed%2520for%2520few-shot%2520and%250Aproperty-conditioned%2520molecular%2520generation.%2520MetaMolGen%2520standardizes%2520the%250Adistribution%2520of%2520graph%2520motifs%2520by%2520mapping%2520them%2520to%2520a%2520normalized%2520latent%2520space%252C%2520and%250Aemploys%2520a%2520lightweight%2520autoregressive%2520sequence%2520model%2520to%2520generate%2520SMILES%250Asequences%2520that%2520faithfully%2520reflect%2520the%2520underlying%2520molecular%2520structure.%2520In%250Aaddition%252C%2520it%2520supports%2520conditional%2520generation%2520of%2520molecules%2520with%2520target%250Aproperties%2520through%2520a%2520learnable%2520property%2520projector%2520integrated%2520into%2520the%250Agenerative%2520process.Experimental%2520results%2520demonstrate%2520that%2520MetaMolGen%250Aconsistently%2520generates%2520valid%2520and%2520diverse%2520SMILES%2520sequences%2520under%2520low-data%250Aregimes%252C%2520outperforming%2520conventional%2520baselines.%2520This%2520highlights%2520its%2520advantage%2520in%250Afast%2520adaptation%2520and%2520efficient%2520conditional%2520generation%2520for%2520practical%2520molecular%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaMolGen%3A%20A%20Neural%20Graph%20Motif%20Generation%20Model%20for%20De%20Novo%20Molecular%0A%20%20Design&entry.906535625=Zimo%20Yan%20and%20Jie%20Zhang%20and%20Zheng%20Xie%20and%20Chang%20Liu%20and%20Yizhen%20Liu%20and%20Yiping%20Song&entry.1292438233=%20%20Molecular%20generation%20plays%20an%20important%20role%20in%20drug%20discovery%20and%20materials%0Ascience%2C%20especially%20in%20data-scarce%20scenarios%20where%20traditional%20generative%0Amodels%20often%20struggle%20to%20achieve%20satisfactory%20conditional%20generalization.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20MetaMolGen%2C%20a%20first-order%0Ameta-learning-based%20molecular%20generator%20designed%20for%20few-shot%20and%0Aproperty-conditioned%20molecular%20generation.%20MetaMolGen%20standardizes%20the%0Adistribution%20of%20graph%20motifs%20by%20mapping%20them%20to%20a%20normalized%20latent%20space%2C%20and%0Aemploys%20a%20lightweight%20autoregressive%20sequence%20model%20to%20generate%20SMILES%0Asequences%20that%20faithfully%20reflect%20the%20underlying%20molecular%20structure.%20In%0Aaddition%2C%20it%20supports%20conditional%20generation%20of%20molecules%20with%20target%0Aproperties%20through%20a%20learnable%20property%20projector%20integrated%20into%20the%0Agenerative%20process.Experimental%20results%20demonstrate%20that%20MetaMolGen%0Aconsistently%20generates%20valid%20and%20diverse%20SMILES%20sequences%20under%20low-data%0Aregimes%2C%20outperforming%20conventional%20baselines.%20This%20highlights%20its%20advantage%20in%0Afast%20adaptation%20and%20efficient%20conditional%20generation%20for%20practical%20molecular%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15587v2&entry.124074799=Read"},
{"title": "SmartUT: Receive Beamforming for Spectral Coexistence of NGSO Satellite\n  Systems", "author": "Almoatssimbillah Saifaldawla and Eva Lagunas and Flor Ortiz and Abuzar B. M. Adam and Symeon Chatzinotas", "abstract": "  In this paper, we investigate downlink co-frequency interference (CFI)\nmitigation in non-geostationary satellites orbits (NGSOs) co-existing systems.\nTraditional mitigation techniques, such as Zero-forcing (ZF), produce a null\ntowards the direction of arrivals (DOAs) of the interfering signals, but they\nsuffer from high computational complexity due to matrix inversions and required\nknowledge of the channel state information (CSI). Furthermore, adaptive\nbeamformers, such as sample matrix inversion (SMI)-based minimum variance,\nprovide poor performance when the available snapshots are limited. We propose a\nMamba-based beamformer (MambaBF) that leverages an unsupervised deep learning\n(DL) approach and can be deployed on the user terminal (UT) antenna array, for\nassisting downlink beamforming and CFI mitigation using only a limited number\nof available array snapshots as input, and without CSI knowledge. Simulation\nresults demonstrate that MambaBF consistently outperforms conventional\nbeamforming techniques in mitigating interference and maximizing the\nsignal-to-interference-plus-noise ratio (SINR), particularly under challenging\nconditions characterized by low SINR, limited snapshots, and imperfect CSI.\n", "link": "http://arxiv.org/abs/2505.07714v1", "date": "2025-05-12", "relevancy": 2.1099, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4316}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.419}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmartUT%3A%20Receive%20Beamforming%20for%20Spectral%20Coexistence%20of%20NGSO%20Satellite%0A%20%20Systems&body=Title%3A%20SmartUT%3A%20Receive%20Beamforming%20for%20Spectral%20Coexistence%20of%20NGSO%20Satellite%0A%20%20Systems%0AAuthor%3A%20Almoatssimbillah%20Saifaldawla%20and%20Eva%20Lagunas%20and%20Flor%20Ortiz%20and%20Abuzar%20B.%20M.%20Adam%20and%20Symeon%20Chatzinotas%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20downlink%20co-frequency%20interference%20%28CFI%29%0Amitigation%20in%20non-geostationary%20satellites%20orbits%20%28NGSOs%29%20co-existing%20systems.%0ATraditional%20mitigation%20techniques%2C%20such%20as%20Zero-forcing%20%28ZF%29%2C%20produce%20a%20null%0Atowards%20the%20direction%20of%20arrivals%20%28DOAs%29%20of%20the%20interfering%20signals%2C%20but%20they%0Asuffer%20from%20high%20computational%20complexity%20due%20to%20matrix%20inversions%20and%20required%0Aknowledge%20of%20the%20channel%20state%20information%20%28CSI%29.%20Furthermore%2C%20adaptive%0Abeamformers%2C%20such%20as%20sample%20matrix%20inversion%20%28SMI%29-based%20minimum%20variance%2C%0Aprovide%20poor%20performance%20when%20the%20available%20snapshots%20are%20limited.%20We%20propose%20a%0AMamba-based%20beamformer%20%28MambaBF%29%20that%20leverages%20an%20unsupervised%20deep%20learning%0A%28DL%29%20approach%20and%20can%20be%20deployed%20on%20the%20user%20terminal%20%28UT%29%20antenna%20array%2C%20for%0Aassisting%20downlink%20beamforming%20and%20CFI%20mitigation%20using%20only%20a%20limited%20number%0Aof%20available%20array%20snapshots%20as%20input%2C%20and%20without%20CSI%20knowledge.%20Simulation%0Aresults%20demonstrate%20that%20MambaBF%20consistently%20outperforms%20conventional%0Abeamforming%20techniques%20in%20mitigating%20interference%20and%20maximizing%20the%0Asignal-to-interference-plus-noise%20ratio%20%28SINR%29%2C%20particularly%20under%20challenging%0Aconditions%20characterized%20by%20low%20SINR%2C%20limited%20snapshots%2C%20and%20imperfect%20CSI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartUT%253A%2520Receive%2520Beamforming%2520for%2520Spectral%2520Coexistence%2520of%2520NGSO%2520Satellite%250A%2520%2520Systems%26entry.906535625%3DAlmoatssimbillah%2520Saifaldawla%2520and%2520Eva%2520Lagunas%2520and%2520Flor%2520Ortiz%2520and%2520Abuzar%2520B.%2520M.%2520Adam%2520and%2520Symeon%2520Chatzinotas%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520downlink%2520co-frequency%2520interference%2520%2528CFI%2529%250Amitigation%2520in%2520non-geostationary%2520satellites%2520orbits%2520%2528NGSOs%2529%2520co-existing%2520systems.%250ATraditional%2520mitigation%2520techniques%252C%2520such%2520as%2520Zero-forcing%2520%2528ZF%2529%252C%2520produce%2520a%2520null%250Atowards%2520the%2520direction%2520of%2520arrivals%2520%2528DOAs%2529%2520of%2520the%2520interfering%2520signals%252C%2520but%2520they%250Asuffer%2520from%2520high%2520computational%2520complexity%2520due%2520to%2520matrix%2520inversions%2520and%2520required%250Aknowledge%2520of%2520the%2520channel%2520state%2520information%2520%2528CSI%2529.%2520Furthermore%252C%2520adaptive%250Abeamformers%252C%2520such%2520as%2520sample%2520matrix%2520inversion%2520%2528SMI%2529-based%2520minimum%2520variance%252C%250Aprovide%2520poor%2520performance%2520when%2520the%2520available%2520snapshots%2520are%2520limited.%2520We%2520propose%2520a%250AMamba-based%2520beamformer%2520%2528MambaBF%2529%2520that%2520leverages%2520an%2520unsupervised%2520deep%2520learning%250A%2528DL%2529%2520approach%2520and%2520can%2520be%2520deployed%2520on%2520the%2520user%2520terminal%2520%2528UT%2529%2520antenna%2520array%252C%2520for%250Aassisting%2520downlink%2520beamforming%2520and%2520CFI%2520mitigation%2520using%2520only%2520a%2520limited%2520number%250Aof%2520available%2520array%2520snapshots%2520as%2520input%252C%2520and%2520without%2520CSI%2520knowledge.%2520Simulation%250Aresults%2520demonstrate%2520that%2520MambaBF%2520consistently%2520outperforms%2520conventional%250Abeamforming%2520techniques%2520in%2520mitigating%2520interference%2520and%2520maximizing%2520the%250Asignal-to-interference-plus-noise%2520ratio%2520%2528SINR%2529%252C%2520particularly%2520under%2520challenging%250Aconditions%2520characterized%2520by%2520low%2520SINR%252C%2520limited%2520snapshots%252C%2520and%2520imperfect%2520CSI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmartUT%3A%20Receive%20Beamforming%20for%20Spectral%20Coexistence%20of%20NGSO%20Satellite%0A%20%20Systems&entry.906535625=Almoatssimbillah%20Saifaldawla%20and%20Eva%20Lagunas%20and%20Flor%20Ortiz%20and%20Abuzar%20B.%20M.%20Adam%20and%20Symeon%20Chatzinotas&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20downlink%20co-frequency%20interference%20%28CFI%29%0Amitigation%20in%20non-geostationary%20satellites%20orbits%20%28NGSOs%29%20co-existing%20systems.%0ATraditional%20mitigation%20techniques%2C%20such%20as%20Zero-forcing%20%28ZF%29%2C%20produce%20a%20null%0Atowards%20the%20direction%20of%20arrivals%20%28DOAs%29%20of%20the%20interfering%20signals%2C%20but%20they%0Asuffer%20from%20high%20computational%20complexity%20due%20to%20matrix%20inversions%20and%20required%0Aknowledge%20of%20the%20channel%20state%20information%20%28CSI%29.%20Furthermore%2C%20adaptive%0Abeamformers%2C%20such%20as%20sample%20matrix%20inversion%20%28SMI%29-based%20minimum%20variance%2C%0Aprovide%20poor%20performance%20when%20the%20available%20snapshots%20are%20limited.%20We%20propose%20a%0AMamba-based%20beamformer%20%28MambaBF%29%20that%20leverages%20an%20unsupervised%20deep%20learning%0A%28DL%29%20approach%20and%20can%20be%20deployed%20on%20the%20user%20terminal%20%28UT%29%20antenna%20array%2C%20for%0Aassisting%20downlink%20beamforming%20and%20CFI%20mitigation%20using%20only%20a%20limited%20number%0Aof%20available%20array%20snapshots%20as%20input%2C%20and%20without%20CSI%20knowledge.%20Simulation%0Aresults%20demonstrate%20that%20MambaBF%20consistently%20outperforms%20conventional%0Abeamforming%20techniques%20in%20mitigating%20interference%20and%20maximizing%20the%0Asignal-to-interference-plus-noise%20ratio%20%28SINR%29%2C%20particularly%20under%20challenging%0Aconditions%20characterized%20by%20low%20SINR%2C%20limited%20snapshots%2C%20and%20imperfect%20CSI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07714v1&entry.124074799=Read"},
{"title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n  $\\mathbf{\\texttt{O}}$ptimization", "author": "Seongjae Kang and Dong Bok Lee and Hyungjoon Jang and Sung Ju Hwang", "abstract": "  Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.\n", "link": "http://arxiv.org/abs/2505.07675v1", "date": "2025-05-12", "relevancy": 2.1056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Semi-supervised%20Knowledge%20Distillation%20from%20Vision-Language%0A%20%20Models%20via%20%24%5Cmathbf%7B%5Ctexttt%7BD%7D%7D%24ual-%24%5Cmathbf%7B%5Ctexttt%7BH%7D%7D%24ead%0A%20%20%24%5Cmathbf%7B%5Ctexttt%7BO%7D%7D%24ptimization&body=Title%3A%20Simple%20Semi-supervised%20Knowledge%20Distillation%20from%20Vision-Language%0A%20%20Models%20via%20%24%5Cmathbf%7B%5Ctexttt%7BD%7D%7D%24ual-%24%5Cmathbf%7B%5Ctexttt%7BH%7D%7D%24ead%0A%20%20%24%5Cmathbf%7B%5Ctexttt%7BO%7D%7D%24ptimization%0AAuthor%3A%20Seongjae%20Kang%20and%20Dong%20Bok%20Lee%20and%20Hyungjoon%20Jang%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Atasks%20by%20leveraging%20rich%20textual%20information%20with%20minimal%20labeled%20data.%0AHowever%2C%20deploying%20such%20large%20models%20remains%20challenging%2C%20particularly%20in%0Aresource-constrained%20environments.%20Knowledge%20distillation%20%28KD%29%20offers%20a%0Awell-established%20solution%20to%20this%20problem%3B%20however%2C%20recent%20KD%20approaches%20from%0AVLMs%20often%20involve%20multi-stage%20training%20or%20additional%20tuning%2C%20increasing%0Acomputational%20overhead%20and%20optimization%20complexity.%20In%20this%20paper%2C%20we%20propose%0A%24%5Cmathbf%7B%5Ctexttt%7BD%7D%7D%24ual-%24%5Cmathbf%7B%5Ctexttt%7BH%7D%7D%24ead%0A%24%5Cmathbf%7B%5Ctexttt%7BO%7D%7D%24ptimization%20%28%24%5Cmathbf%7B%5Ctexttt%7BDHO%7D%7D%24%29%20--%20a%20simple%20yet%0Aeffective%20KD%20framework%20that%20transfers%20knowledge%20from%20VLMs%20to%20compact%2C%0Atask-specific%20models%20in%20semi-supervised%20settings.%20Specifically%2C%20we%20introduce%0Adual%20prediction%20heads%20that%20independently%20learn%20from%20labeled%20data%20and%20teacher%0Apredictions%2C%20and%20propose%20to%20linearly%20combine%20their%20outputs%20during%20inference.%20We%0Aobserve%20that%20%24%5Ctexttt%7BDHO%7D%24%20mitigates%20gradient%20conflicts%20between%20supervised%20and%0Adistillation%20signals%2C%20enabling%20more%20effective%20feature%20learning%20than%20single-head%0AKD%20baselines.%20As%20a%20result%2C%20extensive%20experiments%20show%20that%20%24%5Ctexttt%7BDHO%7D%24%0Aconsistently%20outperforms%20baselines%20across%20multiple%20domains%20and%20fine-grained%0Adatasets.%20Notably%2C%20on%20ImageNet%2C%20it%20achieves%20state-of-the-art%20performance%2C%0Aimproving%20accuracy%20by%203%25%20and%200.1%25%20with%201%25%20and%2010%25%20labeled%20data%2C%20respectively%2C%0Awhile%20using%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Semi-supervised%2520Knowledge%2520Distillation%2520from%2520Vision-Language%250A%2520%2520Models%2520via%2520%2524%255Cmathbf%257B%255Ctexttt%257BD%257D%257D%2524ual-%2524%255Cmathbf%257B%255Ctexttt%257BH%257D%257D%2524ead%250A%2520%2520%2524%255Cmathbf%257B%255Ctexttt%257BO%257D%257D%2524ptimization%26entry.906535625%3DSeongjae%2520Kang%2520and%2520Dong%2520Bok%2520Lee%2520and%2520Hyungjoon%2520Jang%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520diverse%250Atasks%2520by%2520leveraging%2520rich%2520textual%2520information%2520with%2520minimal%2520labeled%2520data.%250AHowever%252C%2520deploying%2520such%2520large%2520models%2520remains%2520challenging%252C%2520particularly%2520in%250Aresource-constrained%2520environments.%2520Knowledge%2520distillation%2520%2528KD%2529%2520offers%2520a%250Awell-established%2520solution%2520to%2520this%2520problem%253B%2520however%252C%2520recent%2520KD%2520approaches%2520from%250AVLMs%2520often%2520involve%2520multi-stage%2520training%2520or%2520additional%2520tuning%252C%2520increasing%250Acomputational%2520overhead%2520and%2520optimization%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%2524%255Cmathbf%257B%255Ctexttt%257BD%257D%257D%2524ual-%2524%255Cmathbf%257B%255Ctexttt%257BH%257D%257D%2524ead%250A%2524%255Cmathbf%257B%255Ctexttt%257BO%257D%257D%2524ptimization%2520%2528%2524%255Cmathbf%257B%255Ctexttt%257BDHO%257D%257D%2524%2529%2520--%2520a%2520simple%2520yet%250Aeffective%2520KD%2520framework%2520that%2520transfers%2520knowledge%2520from%2520VLMs%2520to%2520compact%252C%250Atask-specific%2520models%2520in%2520semi-supervised%2520settings.%2520Specifically%252C%2520we%2520introduce%250Adual%2520prediction%2520heads%2520that%2520independently%2520learn%2520from%2520labeled%2520data%2520and%2520teacher%250Apredictions%252C%2520and%2520propose%2520to%2520linearly%2520combine%2520their%2520outputs%2520during%2520inference.%2520We%250Aobserve%2520that%2520%2524%255Ctexttt%257BDHO%257D%2524%2520mitigates%2520gradient%2520conflicts%2520between%2520supervised%2520and%250Adistillation%2520signals%252C%2520enabling%2520more%2520effective%2520feature%2520learning%2520than%2520single-head%250AKD%2520baselines.%2520As%2520a%2520result%252C%2520extensive%2520experiments%2520show%2520that%2520%2524%255Ctexttt%257BDHO%257D%2524%250Aconsistently%2520outperforms%2520baselines%2520across%2520multiple%2520domains%2520and%2520fine-grained%250Adatasets.%2520Notably%252C%2520on%2520ImageNet%252C%2520it%2520achieves%2520state-of-the-art%2520performance%252C%250Aimproving%2520accuracy%2520by%25203%2525%2520and%25200.1%2525%2520with%25201%2525%2520and%252010%2525%2520labeled%2520data%252C%2520respectively%252C%250Awhile%2520using%2520fewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Semi-supervised%20Knowledge%20Distillation%20from%20Vision-Language%0A%20%20Models%20via%20%24%5Cmathbf%7B%5Ctexttt%7BD%7D%7D%24ual-%24%5Cmathbf%7B%5Ctexttt%7BH%7D%7D%24ead%0A%20%20%24%5Cmathbf%7B%5Ctexttt%7BO%7D%7D%24ptimization&entry.906535625=Seongjae%20Kang%20and%20Dong%20Bok%20Lee%20and%20Hyungjoon%20Jang%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Atasks%20by%20leveraging%20rich%20textual%20information%20with%20minimal%20labeled%20data.%0AHowever%2C%20deploying%20such%20large%20models%20remains%20challenging%2C%20particularly%20in%0Aresource-constrained%20environments.%20Knowledge%20distillation%20%28KD%29%20offers%20a%0Awell-established%20solution%20to%20this%20problem%3B%20however%2C%20recent%20KD%20approaches%20from%0AVLMs%20often%20involve%20multi-stage%20training%20or%20additional%20tuning%2C%20increasing%0Acomputational%20overhead%20and%20optimization%20complexity.%20In%20this%20paper%2C%20we%20propose%0A%24%5Cmathbf%7B%5Ctexttt%7BD%7D%7D%24ual-%24%5Cmathbf%7B%5Ctexttt%7BH%7D%7D%24ead%0A%24%5Cmathbf%7B%5Ctexttt%7BO%7D%7D%24ptimization%20%28%24%5Cmathbf%7B%5Ctexttt%7BDHO%7D%7D%24%29%20--%20a%20simple%20yet%0Aeffective%20KD%20framework%20that%20transfers%20knowledge%20from%20VLMs%20to%20compact%2C%0Atask-specific%20models%20in%20semi-supervised%20settings.%20Specifically%2C%20we%20introduce%0Adual%20prediction%20heads%20that%20independently%20learn%20from%20labeled%20data%20and%20teacher%0Apredictions%2C%20and%20propose%20to%20linearly%20combine%20their%20outputs%20during%20inference.%20We%0Aobserve%20that%20%24%5Ctexttt%7BDHO%7D%24%20mitigates%20gradient%20conflicts%20between%20supervised%20and%0Adistillation%20signals%2C%20enabling%20more%20effective%20feature%20learning%20than%20single-head%0AKD%20baselines.%20As%20a%20result%2C%20extensive%20experiments%20show%20that%20%24%5Ctexttt%7BDHO%7D%24%0Aconsistently%20outperforms%20baselines%20across%20multiple%20domains%20and%20fine-grained%0Adatasets.%20Notably%2C%20on%20ImageNet%2C%20it%20achieves%20state-of-the-art%20performance%2C%0Aimproving%20accuracy%20by%203%25%20and%200.1%25%20with%201%25%20and%2010%25%20labeled%20data%2C%20respectively%2C%0Awhile%20using%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07675v1&entry.124074799=Read"},
{"title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models", "author": "Junjie Ye and Caishuang Huang and Zhuohan Chen and Wenjie Fu and Chenyuan Yang and Leyi Yang and Yilong Wu and Peng Wang and Meng Zhou and Xiaolong Yang and Tao Gui and Qi Zhang and Zhongchao Shi and Jianping Fan and Xuanjing Huang", "abstract": "  Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.\n", "link": "http://arxiv.org/abs/2505.07591v1", "date": "2025-05-12", "relevancy": 2.1025, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Dimensional%20Constraint%20Framework%20for%20Evaluating%20and%20Improving%0A%20%20Instruction%20Following%20in%20Large%20Language%20Models&body=Title%3A%20A%20Multi-Dimensional%20Constraint%20Framework%20for%20Evaluating%20and%20Improving%0A%20%20Instruction%20Following%20in%20Large%20Language%20Models%0AAuthor%3A%20Junjie%20Ye%20and%20Caishuang%20Huang%20and%20Zhuohan%20Chen%20and%20Wenjie%20Fu%20and%20Chenyuan%20Yang%20and%20Leyi%20Yang%20and%20Yilong%20Wu%20and%20Peng%20Wang%20and%20Meng%20Zhou%20and%20Xiaolong%20Yang%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Zhongchao%20Shi%20and%20Jianping%20Fan%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20Instruction%20following%20evaluates%20large%20language%20models%20%28LLMs%29%20on%20their%20ability%0Ato%20generate%20outputs%20that%20adhere%20to%20user-defined%20constraints.%20However%2C%20existing%0Abenchmarks%20often%20rely%20on%20templated%20constraint%20prompts%2C%20which%20lack%20the%20diversity%0Aof%20real-world%20usage%20and%20limit%20fine-grained%20performance%20assessment.%20To%20fill%20this%0Agap%2C%20we%20propose%20a%20multi-dimensional%20constraint%20framework%20encompassing%20three%0Aconstraint%20patterns%2C%20four%20constraint%20categories%2C%20and%20four%20difficulty%20levels.%0ABuilding%20on%20this%20framework%2C%20we%20develop%20an%20automated%20instruction%20generation%0Apipeline%20that%20performs%20constraint%20expansion%2C%20conflict%20detection%2C%20and%0Ainstruction%20rewriting%2C%20yielding%201%2C200%20code-verifiable%20instruction-following%0Atest%20samples.%20We%20evaluate%2019%20LLMs%20across%20seven%20model%20families%20and%20uncover%0Asubstantial%20variation%20in%20performance%20across%20constraint%20forms.%20For%20instance%2C%0Aaverage%20performance%20drops%20from%2077.67%25%20at%20Level%20I%20to%2032.96%25%20at%20Level%20IV.%0AFurthermore%2C%20we%20demonstrate%20the%20utility%20of%20our%20approach%20by%20using%20it%20to%20generate%0Adata%20for%20reinforcement%20learning%2C%20achieving%20substantial%20gains%20in%20instruction%0Afollowing%20without%20degrading%20general%20performance.%20In-depth%20analysis%20indicates%0Athat%20these%20gains%20stem%20primarily%20from%20modifications%20in%20the%20model%27s%20attention%0Amodules%20parameters%2C%20which%20enhance%20constraint%20recognition%20and%20adherence.%20Code%0Aand%20data%20are%20available%20in%20https%3A//github.com/Junjie-Ye/MulDimIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Dimensional%2520Constraint%2520Framework%2520for%2520Evaluating%2520and%2520Improving%250A%2520%2520Instruction%2520Following%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DJunjie%2520Ye%2520and%2520Caishuang%2520Huang%2520and%2520Zhuohan%2520Chen%2520and%2520Wenjie%2520Fu%2520and%2520Chenyuan%2520Yang%2520and%2520Leyi%2520Yang%2520and%2520Yilong%2520Wu%2520and%2520Peng%2520Wang%2520and%2520Meng%2520Zhou%2520and%2520Xiaolong%2520Yang%2520and%2520Tao%2520Gui%2520and%2520Qi%2520Zhang%2520and%2520Zhongchao%2520Shi%2520and%2520Jianping%2520Fan%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520Instruction%2520following%2520evaluates%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520their%2520ability%250Ato%2520generate%2520outputs%2520that%2520adhere%2520to%2520user-defined%2520constraints.%2520However%252C%2520existing%250Abenchmarks%2520often%2520rely%2520on%2520templated%2520constraint%2520prompts%252C%2520which%2520lack%2520the%2520diversity%250Aof%2520real-world%2520usage%2520and%2520limit%2520fine-grained%2520performance%2520assessment.%2520To%2520fill%2520this%250Agap%252C%2520we%2520propose%2520a%2520multi-dimensional%2520constraint%2520framework%2520encompassing%2520three%250Aconstraint%2520patterns%252C%2520four%2520constraint%2520categories%252C%2520and%2520four%2520difficulty%2520levels.%250ABuilding%2520on%2520this%2520framework%252C%2520we%2520develop%2520an%2520automated%2520instruction%2520generation%250Apipeline%2520that%2520performs%2520constraint%2520expansion%252C%2520conflict%2520detection%252C%2520and%250Ainstruction%2520rewriting%252C%2520yielding%25201%252C200%2520code-verifiable%2520instruction-following%250Atest%2520samples.%2520We%2520evaluate%252019%2520LLMs%2520across%2520seven%2520model%2520families%2520and%2520uncover%250Asubstantial%2520variation%2520in%2520performance%2520across%2520constraint%2520forms.%2520For%2520instance%252C%250Aaverage%2520performance%2520drops%2520from%252077.67%2525%2520at%2520Level%2520I%2520to%252032.96%2525%2520at%2520Level%2520IV.%250AFurthermore%252C%2520we%2520demonstrate%2520the%2520utility%2520of%2520our%2520approach%2520by%2520using%2520it%2520to%2520generate%250Adata%2520for%2520reinforcement%2520learning%252C%2520achieving%2520substantial%2520gains%2520in%2520instruction%250Afollowing%2520without%2520degrading%2520general%2520performance.%2520In-depth%2520analysis%2520indicates%250Athat%2520these%2520gains%2520stem%2520primarily%2520from%2520modifications%2520in%2520the%2520model%2527s%2520attention%250Amodules%2520parameters%252C%2520which%2520enhance%2520constraint%2520recognition%2520and%2520adherence.%2520Code%250Aand%2520data%2520are%2520available%2520in%2520https%253A//github.com/Junjie-Ye/MulDimIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Dimensional%20Constraint%20Framework%20for%20Evaluating%20and%20Improving%0A%20%20Instruction%20Following%20in%20Large%20Language%20Models&entry.906535625=Junjie%20Ye%20and%20Caishuang%20Huang%20and%20Zhuohan%20Chen%20and%20Wenjie%20Fu%20and%20Chenyuan%20Yang%20and%20Leyi%20Yang%20and%20Yilong%20Wu%20and%20Peng%20Wang%20and%20Meng%20Zhou%20and%20Xiaolong%20Yang%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Zhongchao%20Shi%20and%20Jianping%20Fan%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Instruction%20following%20evaluates%20large%20language%20models%20%28LLMs%29%20on%20their%20ability%0Ato%20generate%20outputs%20that%20adhere%20to%20user-defined%20constraints.%20However%2C%20existing%0Abenchmarks%20often%20rely%20on%20templated%20constraint%20prompts%2C%20which%20lack%20the%20diversity%0Aof%20real-world%20usage%20and%20limit%20fine-grained%20performance%20assessment.%20To%20fill%20this%0Agap%2C%20we%20propose%20a%20multi-dimensional%20constraint%20framework%20encompassing%20three%0Aconstraint%20patterns%2C%20four%20constraint%20categories%2C%20and%20four%20difficulty%20levels.%0ABuilding%20on%20this%20framework%2C%20we%20develop%20an%20automated%20instruction%20generation%0Apipeline%20that%20performs%20constraint%20expansion%2C%20conflict%20detection%2C%20and%0Ainstruction%20rewriting%2C%20yielding%201%2C200%20code-verifiable%20instruction-following%0Atest%20samples.%20We%20evaluate%2019%20LLMs%20across%20seven%20model%20families%20and%20uncover%0Asubstantial%20variation%20in%20performance%20across%20constraint%20forms.%20For%20instance%2C%0Aaverage%20performance%20drops%20from%2077.67%25%20at%20Level%20I%20to%2032.96%25%20at%20Level%20IV.%0AFurthermore%2C%20we%20demonstrate%20the%20utility%20of%20our%20approach%20by%20using%20it%20to%20generate%0Adata%20for%20reinforcement%20learning%2C%20achieving%20substantial%20gains%20in%20instruction%0Afollowing%20without%20degrading%20general%20performance.%20In-depth%20analysis%20indicates%0Athat%20these%20gains%20stem%20primarily%20from%20modifications%20in%20the%20model%27s%20attention%0Amodules%20parameters%2C%20which%20enhance%20constraint%20recognition%20and%20adherence.%20Code%0Aand%20data%20are%20available%20in%20https%3A//github.com/Junjie-Ye/MulDimIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07591v1&entry.124074799=Read"},
{"title": "LLMs Outperform Experts on Challenging Biology Benchmarks", "author": "Lennart Justen", "abstract": "  This study systematically evaluates 27 frontier Large Language Models on\neight biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with OpenAI's o3 now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including the biology subsets of GPQA and WMDP and\nLAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance.\n", "link": "http://arxiv.org/abs/2505.06108v2", "date": "2025-05-12", "relevancy": 2.1003, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Outperform%20Experts%20on%20Challenging%20Biology%20Benchmarks&body=Title%3A%20LLMs%20Outperform%20Experts%20on%20Challenging%20Biology%20Benchmarks%0AAuthor%3A%20Lennart%20Justen%0AAbstract%3A%20%20%20This%20study%20systematically%20evaluates%2027%20frontier%20Large%20Language%20Models%20on%0Aeight%20biology%20benchmarks%20spanning%20molecular%20biology%2C%20genetics%2C%20cloning%2C%0Avirology%2C%20and%20biosecurity.%20Models%20from%20major%20AI%20developers%20released%20between%0ANovember%202022%20and%20April%202025%20were%20assessed%20through%20ten%20independent%20runs%20per%0Abenchmark.%20The%20findings%20reveal%20dramatic%20improvements%20in%20biological%0Acapabilities.%20Top%20model%20performance%20increased%20more%20than%204-fold%20on%20the%0Achallenging%20text-only%20subset%20of%20the%20Virology%20Capabilities%20Test%20over%20the%20study%0Aperiod%2C%20with%20OpenAI%27s%20o3%20now%20performing%20twice%20as%20well%20as%20expert%20virologists.%0ASeveral%20models%20now%20match%20or%20exceed%20expert-level%20performance%20on%20other%0Achallenging%20benchmarks%2C%20including%20the%20biology%20subsets%20of%20GPQA%20and%20WMDP%20and%0ALAB-Bench%20CloningScenarios.%20Contrary%20to%20expectations%2C%20chain-of-thought%20did%20not%0Asubstantially%20improve%20performance%20over%20zero-shot%20evaluation%2C%20while%20extended%0Areasoning%20features%20in%20o3-mini%20and%20Claude%203.7%20Sonnet%20typically%20improved%0Aperformance%20as%20predicted%20by%20inference%20scaling.%20Benchmarks%20such%20as%20PubMedQA%20and%0Athe%20MMLU%20and%20WMDP%20biology%20subsets%20exhibited%20performance%20plateaus%20well%20below%0A100%25%2C%20suggesting%20benchmark%20saturation%20and%20errors%20in%20the%20underlying%20benchmark%0Adata.%20The%20analysis%20highlights%20the%20need%20for%20more%20sophisticated%20evaluation%0Amethodologies%20as%20AI%20systems%20continue%20to%20advance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Outperform%2520Experts%2520on%2520Challenging%2520Biology%2520Benchmarks%26entry.906535625%3DLennart%2520Justen%26entry.1292438233%3D%2520%2520This%2520study%2520systematically%2520evaluates%252027%2520frontier%2520Large%2520Language%2520Models%2520on%250Aeight%2520biology%2520benchmarks%2520spanning%2520molecular%2520biology%252C%2520genetics%252C%2520cloning%252C%250Avirology%252C%2520and%2520biosecurity.%2520Models%2520from%2520major%2520AI%2520developers%2520released%2520between%250ANovember%25202022%2520and%2520April%25202025%2520were%2520assessed%2520through%2520ten%2520independent%2520runs%2520per%250Abenchmark.%2520The%2520findings%2520reveal%2520dramatic%2520improvements%2520in%2520biological%250Acapabilities.%2520Top%2520model%2520performance%2520increased%2520more%2520than%25204-fold%2520on%2520the%250Achallenging%2520text-only%2520subset%2520of%2520the%2520Virology%2520Capabilities%2520Test%2520over%2520the%2520study%250Aperiod%252C%2520with%2520OpenAI%2527s%2520o3%2520now%2520performing%2520twice%2520as%2520well%2520as%2520expert%2520virologists.%250ASeveral%2520models%2520now%2520match%2520or%2520exceed%2520expert-level%2520performance%2520on%2520other%250Achallenging%2520benchmarks%252C%2520including%2520the%2520biology%2520subsets%2520of%2520GPQA%2520and%2520WMDP%2520and%250ALAB-Bench%2520CloningScenarios.%2520Contrary%2520to%2520expectations%252C%2520chain-of-thought%2520did%2520not%250Asubstantially%2520improve%2520performance%2520over%2520zero-shot%2520evaluation%252C%2520while%2520extended%250Areasoning%2520features%2520in%2520o3-mini%2520and%2520Claude%25203.7%2520Sonnet%2520typically%2520improved%250Aperformance%2520as%2520predicted%2520by%2520inference%2520scaling.%2520Benchmarks%2520such%2520as%2520PubMedQA%2520and%250Athe%2520MMLU%2520and%2520WMDP%2520biology%2520subsets%2520exhibited%2520performance%2520plateaus%2520well%2520below%250A100%2525%252C%2520suggesting%2520benchmark%2520saturation%2520and%2520errors%2520in%2520the%2520underlying%2520benchmark%250Adata.%2520The%2520analysis%2520highlights%2520the%2520need%2520for%2520more%2520sophisticated%2520evaluation%250Amethodologies%2520as%2520AI%2520systems%2520continue%2520to%2520advance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Outperform%20Experts%20on%20Challenging%20Biology%20Benchmarks&entry.906535625=Lennart%20Justen&entry.1292438233=%20%20This%20study%20systematically%20evaluates%2027%20frontier%20Large%20Language%20Models%20on%0Aeight%20biology%20benchmarks%20spanning%20molecular%20biology%2C%20genetics%2C%20cloning%2C%0Avirology%2C%20and%20biosecurity.%20Models%20from%20major%20AI%20developers%20released%20between%0ANovember%202022%20and%20April%202025%20were%20assessed%20through%20ten%20independent%20runs%20per%0Abenchmark.%20The%20findings%20reveal%20dramatic%20improvements%20in%20biological%0Acapabilities.%20Top%20model%20performance%20increased%20more%20than%204-fold%20on%20the%0Achallenging%20text-only%20subset%20of%20the%20Virology%20Capabilities%20Test%20over%20the%20study%0Aperiod%2C%20with%20OpenAI%27s%20o3%20now%20performing%20twice%20as%20well%20as%20expert%20virologists.%0ASeveral%20models%20now%20match%20or%20exceed%20expert-level%20performance%20on%20other%0Achallenging%20benchmarks%2C%20including%20the%20biology%20subsets%20of%20GPQA%20and%20WMDP%20and%0ALAB-Bench%20CloningScenarios.%20Contrary%20to%20expectations%2C%20chain-of-thought%20did%20not%0Asubstantially%20improve%20performance%20over%20zero-shot%20evaluation%2C%20while%20extended%0Areasoning%20features%20in%20o3-mini%20and%20Claude%203.7%20Sonnet%20typically%20improved%0Aperformance%20as%20predicted%20by%20inference%20scaling.%20Benchmarks%20such%20as%20PubMedQA%20and%0Athe%20MMLU%20and%20WMDP%20biology%20subsets%20exhibited%20performance%20plateaus%20well%20below%0A100%25%2C%20suggesting%20benchmark%20saturation%20and%20errors%20in%20the%20underlying%20benchmark%0Adata.%20The%20analysis%20highlights%20the%20need%20for%20more%20sophisticated%20evaluation%0Amethodologies%20as%20AI%20systems%20continue%20to%20advance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06108v2&entry.124074799=Read"},
{"title": "Spoken Language Understanding on Unseen Tasks With In-Context Learning", "author": "Neeraj Agrawal and Sriram Ganapathy", "abstract": "  Spoken language understanding (SLU) tasks involve diverse skills that probe\nthe information extraction, classification and/or generation capabilities of\nmodels. In this setting, task-specific training data may not always be\navailable. While traditional task-specific SLU models are unable to cater to\nsuch requirements, the speech-text large language models (LLMs) offer a\npromising alternative with emergent abilities. However, out of-the-box, our\nevaluations indicate that the zero/few-shot performance of prominent\nopen-source speech-text LLMs on SLU tasks are not up to the mark. In this\npaper, we introduce a novel approach to robust task-agnostic fine-tuning using\nrandomized class labels. With this proposed fine-tuning, we illustrate that the\nperformance of the speech-text LLMs on an unseen task is significantly improved\nover standard approaches. Critically, the proposed approach avoids the\nrequirement of task-specific data annotations for enabling new tasks in\nspeech-text LLMs.\n", "link": "http://arxiv.org/abs/2505.07731v1", "date": "2025-05-12", "relevancy": 2.093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spoken%20Language%20Understanding%20on%20Unseen%20Tasks%20With%20In-Context%20Learning&body=Title%3A%20Spoken%20Language%20Understanding%20on%20Unseen%20Tasks%20With%20In-Context%20Learning%0AAuthor%3A%20Neeraj%20Agrawal%20and%20Sriram%20Ganapathy%0AAbstract%3A%20%20%20Spoken%20language%20understanding%20%28SLU%29%20tasks%20involve%20diverse%20skills%20that%20probe%0Athe%20information%20extraction%2C%20classification%20and/or%20generation%20capabilities%20of%0Amodels.%20In%20this%20setting%2C%20task-specific%20training%20data%20may%20not%20always%20be%0Aavailable.%20While%20traditional%20task-specific%20SLU%20models%20are%20unable%20to%20cater%20to%0Asuch%20requirements%2C%20the%20speech-text%20large%20language%20models%20%28LLMs%29%20offer%20a%0Apromising%20alternative%20with%20emergent%20abilities.%20However%2C%20out%20of-the-box%2C%20our%0Aevaluations%20indicate%20that%20the%20zero/few-shot%20performance%20of%20prominent%0Aopen-source%20speech-text%20LLMs%20on%20SLU%20tasks%20are%20not%20up%20to%20the%20mark.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20approach%20to%20robust%20task-agnostic%20fine-tuning%20using%0Arandomized%20class%20labels.%20With%20this%20proposed%20fine-tuning%2C%20we%20illustrate%20that%20the%0Aperformance%20of%20the%20speech-text%20LLMs%20on%20an%20unseen%20task%20is%20significantly%20improved%0Aover%20standard%20approaches.%20Critically%2C%20the%20proposed%20approach%20avoids%20the%0Arequirement%20of%20task-specific%20data%20annotations%20for%20enabling%20new%20tasks%20in%0Aspeech-text%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpoken%2520Language%2520Understanding%2520on%2520Unseen%2520Tasks%2520With%2520In-Context%2520Learning%26entry.906535625%3DNeeraj%2520Agrawal%2520and%2520Sriram%2520Ganapathy%26entry.1292438233%3D%2520%2520Spoken%2520language%2520understanding%2520%2528SLU%2529%2520tasks%2520involve%2520diverse%2520skills%2520that%2520probe%250Athe%2520information%2520extraction%252C%2520classification%2520and/or%2520generation%2520capabilities%2520of%250Amodels.%2520In%2520this%2520setting%252C%2520task-specific%2520training%2520data%2520may%2520not%2520always%2520be%250Aavailable.%2520While%2520traditional%2520task-specific%2520SLU%2520models%2520are%2520unable%2520to%2520cater%2520to%250Asuch%2520requirements%252C%2520the%2520speech-text%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520a%250Apromising%2520alternative%2520with%2520emergent%2520abilities.%2520However%252C%2520out%2520of-the-box%252C%2520our%250Aevaluations%2520indicate%2520that%2520the%2520zero/few-shot%2520performance%2520of%2520prominent%250Aopen-source%2520speech-text%2520LLMs%2520on%2520SLU%2520tasks%2520are%2520not%2520up%2520to%2520the%2520mark.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%2520robust%2520task-agnostic%2520fine-tuning%2520using%250Arandomized%2520class%2520labels.%2520With%2520this%2520proposed%2520fine-tuning%252C%2520we%2520illustrate%2520that%2520the%250Aperformance%2520of%2520the%2520speech-text%2520LLMs%2520on%2520an%2520unseen%2520task%2520is%2520significantly%2520improved%250Aover%2520standard%2520approaches.%2520Critically%252C%2520the%2520proposed%2520approach%2520avoids%2520the%250Arequirement%2520of%2520task-specific%2520data%2520annotations%2520for%2520enabling%2520new%2520tasks%2520in%250Aspeech-text%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spoken%20Language%20Understanding%20on%20Unseen%20Tasks%20With%20In-Context%20Learning&entry.906535625=Neeraj%20Agrawal%20and%20Sriram%20Ganapathy&entry.1292438233=%20%20Spoken%20language%20understanding%20%28SLU%29%20tasks%20involve%20diverse%20skills%20that%20probe%0Athe%20information%20extraction%2C%20classification%20and/or%20generation%20capabilities%20of%0Amodels.%20In%20this%20setting%2C%20task-specific%20training%20data%20may%20not%20always%20be%0Aavailable.%20While%20traditional%20task-specific%20SLU%20models%20are%20unable%20to%20cater%20to%0Asuch%20requirements%2C%20the%20speech-text%20large%20language%20models%20%28LLMs%29%20offer%20a%0Apromising%20alternative%20with%20emergent%20abilities.%20However%2C%20out%20of-the-box%2C%20our%0Aevaluations%20indicate%20that%20the%20zero/few-shot%20performance%20of%20prominent%0Aopen-source%20speech-text%20LLMs%20on%20SLU%20tasks%20are%20not%20up%20to%20the%20mark.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20approach%20to%20robust%20task-agnostic%20fine-tuning%20using%0Arandomized%20class%20labels.%20With%20this%20proposed%20fine-tuning%2C%20we%20illustrate%20that%20the%0Aperformance%20of%20the%20speech-text%20LLMs%20on%20an%20unseen%20task%20is%20significantly%20improved%0Aover%20standard%20approaches.%20Critically%2C%20the%20proposed%20approach%20avoids%20the%0Arequirement%20of%20task-specific%20data%20annotations%20for%20enabling%20new%20tasks%20in%0Aspeech-text%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07731v1&entry.124074799=Read"},
{"title": "Multimodal Survival Modeling in the Age of Foundation Models", "author": "Steven Song and Morgan Borjigin-Wang and Irene Madejski and Robert L. Grossman", "abstract": "  The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a\nlarge-scale reference through its harmonized genomics, clinical, and image\ndata. Prior studies have trained bespoke cancer survival prediction models from\nunimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning\nis the development of foundation models (FMs) to derive meaningful feature\nembeddings, agnostic to a specific modeling task. Biomedical text especially\nhas seen growing development of FMs. While TCGA contains free-text data as\npathology reports, these have been historically underutilized. Here, we\ninvestigate the feasibility of training classical, multimodal survival models\nover zero-shot embeddings extracted by FMs. We show the ease and additive\neffect of multimodal fusion, outperforming unimodal models. We demonstrate the\nbenefit of including pathology report text and rigorously evaluate the effect\nof model-based text summarization and hallucination. Overall, we modernize\nsurvival modeling by leveraging FMs and information extraction from pathology\nreports.\n", "link": "http://arxiv.org/abs/2505.07683v1", "date": "2025-05-12", "relevancy": 2.0925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Survival%20Modeling%20in%20the%20Age%20of%20Foundation%20Models&body=Title%3A%20Multimodal%20Survival%20Modeling%20in%20the%20Age%20of%20Foundation%20Models%0AAuthor%3A%20Steven%20Song%20and%20Morgan%20Borjigin-Wang%20and%20Irene%20Madejski%20and%20Robert%20L.%20Grossman%0AAbstract%3A%20%20%20The%20Cancer%20Genome%20Atlas%20%28TCGA%29%20has%20enabled%20novel%20discoveries%20and%20served%20as%20a%0Alarge-scale%20reference%20through%20its%20harmonized%20genomics%2C%20clinical%2C%20and%20image%0Adata.%20Prior%20studies%20have%20trained%20bespoke%20cancer%20survival%20prediction%20models%20from%0Aunimodal%20or%20multimodal%20TCGA%20data.%20A%20modern%20paradigm%20in%20biomedical%20deep%20learning%0Ais%20the%20development%20of%20foundation%20models%20%28FMs%29%20to%20derive%20meaningful%20feature%0Aembeddings%2C%20agnostic%20to%20a%20specific%20modeling%20task.%20Biomedical%20text%20especially%0Ahas%20seen%20growing%20development%20of%20FMs.%20While%20TCGA%20contains%20free-text%20data%20as%0Apathology%20reports%2C%20these%20have%20been%20historically%20underutilized.%20Here%2C%20we%0Ainvestigate%20the%20feasibility%20of%20training%20classical%2C%20multimodal%20survival%20models%0Aover%20zero-shot%20embeddings%20extracted%20by%20FMs.%20We%20show%20the%20ease%20and%20additive%0Aeffect%20of%20multimodal%20fusion%2C%20outperforming%20unimodal%20models.%20We%20demonstrate%20the%0Abenefit%20of%20including%20pathology%20report%20text%20and%20rigorously%20evaluate%20the%20effect%0Aof%20model-based%20text%20summarization%20and%20hallucination.%20Overall%2C%20we%20modernize%0Asurvival%20modeling%20by%20leveraging%20FMs%20and%20information%20extraction%20from%20pathology%0Areports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Survival%2520Modeling%2520in%2520the%2520Age%2520of%2520Foundation%2520Models%26entry.906535625%3DSteven%2520Song%2520and%2520Morgan%2520Borjigin-Wang%2520and%2520Irene%2520Madejski%2520and%2520Robert%2520L.%2520Grossman%26entry.1292438233%3D%2520%2520The%2520Cancer%2520Genome%2520Atlas%2520%2528TCGA%2529%2520has%2520enabled%2520novel%2520discoveries%2520and%2520served%2520as%2520a%250Alarge-scale%2520reference%2520through%2520its%2520harmonized%2520genomics%252C%2520clinical%252C%2520and%2520image%250Adata.%2520Prior%2520studies%2520have%2520trained%2520bespoke%2520cancer%2520survival%2520prediction%2520models%2520from%250Aunimodal%2520or%2520multimodal%2520TCGA%2520data.%2520A%2520modern%2520paradigm%2520in%2520biomedical%2520deep%2520learning%250Ais%2520the%2520development%2520of%2520foundation%2520models%2520%2528FMs%2529%2520to%2520derive%2520meaningful%2520feature%250Aembeddings%252C%2520agnostic%2520to%2520a%2520specific%2520modeling%2520task.%2520Biomedical%2520text%2520especially%250Ahas%2520seen%2520growing%2520development%2520of%2520FMs.%2520While%2520TCGA%2520contains%2520free-text%2520data%2520as%250Apathology%2520reports%252C%2520these%2520have%2520been%2520historically%2520underutilized.%2520Here%252C%2520we%250Ainvestigate%2520the%2520feasibility%2520of%2520training%2520classical%252C%2520multimodal%2520survival%2520models%250Aover%2520zero-shot%2520embeddings%2520extracted%2520by%2520FMs.%2520We%2520show%2520the%2520ease%2520and%2520additive%250Aeffect%2520of%2520multimodal%2520fusion%252C%2520outperforming%2520unimodal%2520models.%2520We%2520demonstrate%2520the%250Abenefit%2520of%2520including%2520pathology%2520report%2520text%2520and%2520rigorously%2520evaluate%2520the%2520effect%250Aof%2520model-based%2520text%2520summarization%2520and%2520hallucination.%2520Overall%252C%2520we%2520modernize%250Asurvival%2520modeling%2520by%2520leveraging%2520FMs%2520and%2520information%2520extraction%2520from%2520pathology%250Areports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Survival%20Modeling%20in%20the%20Age%20of%20Foundation%20Models&entry.906535625=Steven%20Song%20and%20Morgan%20Borjigin-Wang%20and%20Irene%20Madejski%20and%20Robert%20L.%20Grossman&entry.1292438233=%20%20The%20Cancer%20Genome%20Atlas%20%28TCGA%29%20has%20enabled%20novel%20discoveries%20and%20served%20as%20a%0Alarge-scale%20reference%20through%20its%20harmonized%20genomics%2C%20clinical%2C%20and%20image%0Adata.%20Prior%20studies%20have%20trained%20bespoke%20cancer%20survival%20prediction%20models%20from%0Aunimodal%20or%20multimodal%20TCGA%20data.%20A%20modern%20paradigm%20in%20biomedical%20deep%20learning%0Ais%20the%20development%20of%20foundation%20models%20%28FMs%29%20to%20derive%20meaningful%20feature%0Aembeddings%2C%20agnostic%20to%20a%20specific%20modeling%20task.%20Biomedical%20text%20especially%0Ahas%20seen%20growing%20development%20of%20FMs.%20While%20TCGA%20contains%20free-text%20data%20as%0Apathology%20reports%2C%20these%20have%20been%20historically%20underutilized.%20Here%2C%20we%0Ainvestigate%20the%20feasibility%20of%20training%20classical%2C%20multimodal%20survival%20models%0Aover%20zero-shot%20embeddings%20extracted%20by%20FMs.%20We%20show%20the%20ease%20and%20additive%0Aeffect%20of%20multimodal%20fusion%2C%20outperforming%20unimodal%20models.%20We%20demonstrate%20the%0Abenefit%20of%20including%20pathology%20report%20text%20and%20rigorously%20evaluate%20the%20effect%0Aof%20model-based%20text%20summarization%20and%20hallucination.%20Overall%2C%20we%20modernize%0Asurvival%20modeling%20by%20leveraging%20FMs%20and%20information%20extraction%20from%20pathology%0Areports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07683v1&entry.124074799=Read"},
{"title": "Semi-supervised Node Importance Estimation with Informative Distribution\n  Modeling for Uncertainty Regularization", "author": "Yankai Chen and Taotao Wang and Yixiang Fang and Yunyu Xiao", "abstract": "  Node importance estimation, a classical problem in network analysis,\nunderpins various web applications. Previous methods either exploit intrinsic\ntopological characteristics, e.g., graph centrality, or leverage additional\ninformation, e.g., data heterogeneity, for node feature enhancement. However,\nthese methods follow the supervised learning setting, overlooking the fact that\nground-truth node-importance data are usually partially labeled in practice. In\nthis work, we propose the first semi-supervised node importance estimation\nframework, i.e., EASING, to improve learning quality for unlabeled data in\nheterogeneous graphs. Different from previous approaches, EASING explicitly\ncaptures uncertainty to reflect the confidence of model predictions. To jointly\nestimate the importance values and uncertainties, EASING incorporates DJE, a\ndeep encoder-decoder neural architecture. DJE introduces distribution modeling\nfor graph nodes, where the distribution representations derive both importance\nand uncertainty estimates. Additionally, DJE facilitates effective pseudo-label\ngeneration for the unlabeled data to enrich the training samples. Based on\nlabeled and pseudo-labeled data, EASING develops effective semi-supervised\nheteroscedastic learning with varying node uncertainty regularization.\nExtensive experiments on three real-world datasets highlight the superior\nperformance of EASING compared to competing methods. Codes are available via\nhttps://github.com/yankai-chen/EASING.\n", "link": "http://arxiv.org/abs/2503.20697v2", "date": "2025-05-12", "relevancy": 2.0917, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5856}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-supervised%20Node%20Importance%20Estimation%20with%20Informative%20Distribution%0A%20%20Modeling%20for%20Uncertainty%20Regularization&body=Title%3A%20Semi-supervised%20Node%20Importance%20Estimation%20with%20Informative%20Distribution%0A%20%20Modeling%20for%20Uncertainty%20Regularization%0AAuthor%3A%20Yankai%20Chen%20and%20Taotao%20Wang%20and%20Yixiang%20Fang%20and%20Yunyu%20Xiao%0AAbstract%3A%20%20%20Node%20importance%20estimation%2C%20a%20classical%20problem%20in%20network%20analysis%2C%0Aunderpins%20various%20web%20applications.%20Previous%20methods%20either%20exploit%20intrinsic%0Atopological%20characteristics%2C%20e.g.%2C%20graph%20centrality%2C%20or%20leverage%20additional%0Ainformation%2C%20e.g.%2C%20data%20heterogeneity%2C%20for%20node%20feature%20enhancement.%20However%2C%0Athese%20methods%20follow%20the%20supervised%20learning%20setting%2C%20overlooking%20the%20fact%20that%0Aground-truth%20node-importance%20data%20are%20usually%20partially%20labeled%20in%20practice.%20In%0Athis%20work%2C%20we%20propose%20the%20first%20semi-supervised%20node%20importance%20estimation%0Aframework%2C%20i.e.%2C%20EASING%2C%20to%20improve%20learning%20quality%20for%20unlabeled%20data%20in%0Aheterogeneous%20graphs.%20Different%20from%20previous%20approaches%2C%20EASING%20explicitly%0Acaptures%20uncertainty%20to%20reflect%20the%20confidence%20of%20model%20predictions.%20To%20jointly%0Aestimate%20the%20importance%20values%20and%20uncertainties%2C%20EASING%20incorporates%20DJE%2C%20a%0Adeep%20encoder-decoder%20neural%20architecture.%20DJE%20introduces%20distribution%20modeling%0Afor%20graph%20nodes%2C%20where%20the%20distribution%20representations%20derive%20both%20importance%0Aand%20uncertainty%20estimates.%20Additionally%2C%20DJE%20facilitates%20effective%20pseudo-label%0Ageneration%20for%20the%20unlabeled%20data%20to%20enrich%20the%20training%20samples.%20Based%20on%0Alabeled%20and%20pseudo-labeled%20data%2C%20EASING%20develops%20effective%20semi-supervised%0Aheteroscedastic%20learning%20with%20varying%20node%20uncertainty%20regularization.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20highlight%20the%20superior%0Aperformance%20of%20EASING%20compared%20to%20competing%20methods.%20Codes%20are%20available%20via%0Ahttps%3A//github.com/yankai-chen/EASING.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-supervised%2520Node%2520Importance%2520Estimation%2520with%2520Informative%2520Distribution%250A%2520%2520Modeling%2520for%2520Uncertainty%2520Regularization%26entry.906535625%3DYankai%2520Chen%2520and%2520Taotao%2520Wang%2520and%2520Yixiang%2520Fang%2520and%2520Yunyu%2520Xiao%26entry.1292438233%3D%2520%2520Node%2520importance%2520estimation%252C%2520a%2520classical%2520problem%2520in%2520network%2520analysis%252C%250Aunderpins%2520various%2520web%2520applications.%2520Previous%2520methods%2520either%2520exploit%2520intrinsic%250Atopological%2520characteristics%252C%2520e.g.%252C%2520graph%2520centrality%252C%2520or%2520leverage%2520additional%250Ainformation%252C%2520e.g.%252C%2520data%2520heterogeneity%252C%2520for%2520node%2520feature%2520enhancement.%2520However%252C%250Athese%2520methods%2520follow%2520the%2520supervised%2520learning%2520setting%252C%2520overlooking%2520the%2520fact%2520that%250Aground-truth%2520node-importance%2520data%2520are%2520usually%2520partially%2520labeled%2520in%2520practice.%2520In%250Athis%2520work%252C%2520we%2520propose%2520the%2520first%2520semi-supervised%2520node%2520importance%2520estimation%250Aframework%252C%2520i.e.%252C%2520EASING%252C%2520to%2520improve%2520learning%2520quality%2520for%2520unlabeled%2520data%2520in%250Aheterogeneous%2520graphs.%2520Different%2520from%2520previous%2520approaches%252C%2520EASING%2520explicitly%250Acaptures%2520uncertainty%2520to%2520reflect%2520the%2520confidence%2520of%2520model%2520predictions.%2520To%2520jointly%250Aestimate%2520the%2520importance%2520values%2520and%2520uncertainties%252C%2520EASING%2520incorporates%2520DJE%252C%2520a%250Adeep%2520encoder-decoder%2520neural%2520architecture.%2520DJE%2520introduces%2520distribution%2520modeling%250Afor%2520graph%2520nodes%252C%2520where%2520the%2520distribution%2520representations%2520derive%2520both%2520importance%250Aand%2520uncertainty%2520estimates.%2520Additionally%252C%2520DJE%2520facilitates%2520effective%2520pseudo-label%250Ageneration%2520for%2520the%2520unlabeled%2520data%2520to%2520enrich%2520the%2520training%2520samples.%2520Based%2520on%250Alabeled%2520and%2520pseudo-labeled%2520data%252C%2520EASING%2520develops%2520effective%2520semi-supervised%250Aheteroscedastic%2520learning%2520with%2520varying%2520node%2520uncertainty%2520regularization.%250AExtensive%2520experiments%2520on%2520three%2520real-world%2520datasets%2520highlight%2520the%2520superior%250Aperformance%2520of%2520EASING%2520compared%2520to%2520competing%2520methods.%2520Codes%2520are%2520available%2520via%250Ahttps%253A//github.com/yankai-chen/EASING.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%20Node%20Importance%20Estimation%20with%20Informative%20Distribution%0A%20%20Modeling%20for%20Uncertainty%20Regularization&entry.906535625=Yankai%20Chen%20and%20Taotao%20Wang%20and%20Yixiang%20Fang%20and%20Yunyu%20Xiao&entry.1292438233=%20%20Node%20importance%20estimation%2C%20a%20classical%20problem%20in%20network%20analysis%2C%0Aunderpins%20various%20web%20applications.%20Previous%20methods%20either%20exploit%20intrinsic%0Atopological%20characteristics%2C%20e.g.%2C%20graph%20centrality%2C%20or%20leverage%20additional%0Ainformation%2C%20e.g.%2C%20data%20heterogeneity%2C%20for%20node%20feature%20enhancement.%20However%2C%0Athese%20methods%20follow%20the%20supervised%20learning%20setting%2C%20overlooking%20the%20fact%20that%0Aground-truth%20node-importance%20data%20are%20usually%20partially%20labeled%20in%20practice.%20In%0Athis%20work%2C%20we%20propose%20the%20first%20semi-supervised%20node%20importance%20estimation%0Aframework%2C%20i.e.%2C%20EASING%2C%20to%20improve%20learning%20quality%20for%20unlabeled%20data%20in%0Aheterogeneous%20graphs.%20Different%20from%20previous%20approaches%2C%20EASING%20explicitly%0Acaptures%20uncertainty%20to%20reflect%20the%20confidence%20of%20model%20predictions.%20To%20jointly%0Aestimate%20the%20importance%20values%20and%20uncertainties%2C%20EASING%20incorporates%20DJE%2C%20a%0Adeep%20encoder-decoder%20neural%20architecture.%20DJE%20introduces%20distribution%20modeling%0Afor%20graph%20nodes%2C%20where%20the%20distribution%20representations%20derive%20both%20importance%0Aand%20uncertainty%20estimates.%20Additionally%2C%20DJE%20facilitates%20effective%20pseudo-label%0Ageneration%20for%20the%20unlabeled%20data%20to%20enrich%20the%20training%20samples.%20Based%20on%0Alabeled%20and%20pseudo-labeled%20data%2C%20EASING%20develops%20effective%20semi-supervised%0Aheteroscedastic%20learning%20with%20varying%20node%20uncertainty%20regularization.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20highlight%20the%20superior%0Aperformance%20of%20EASING%20compared%20to%20competing%20methods.%20Codes%20are%20available%20via%0Ahttps%3A//github.com/yankai-chen/EASING.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20697v2&entry.124074799=Read"},
{"title": "Relative Overfitting and Accept-Reject Framework", "author": "Yanxin Liu and Yunqi Zhang", "abstract": "  Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR). In Natural Language\nProcessing (NLP), we use LLMs and Small Language Models (SLMs) as the medium\nfor discussion. This framework enables SLMs to exert a universal positive\ninfluence on LLM decision outputs, rather than the intuitively expected\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our structure, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks.\n", "link": "http://arxiv.org/abs/2505.07783v1", "date": "2025-05-12", "relevancy": 2.0905, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Overfitting%20and%20Accept-Reject%20Framework&body=Title%3A%20Relative%20Overfitting%20and%20Accept-Reject%20Framework%0AAuthor%3A%20Yanxin%20Liu%20and%20Yunqi%20Zhang%0AAbstract%3A%20%20%20Currently%2C%20the%20scaling%20law%20of%20Large%20Language%20Models%20%28LLMs%29%20faces%20challenges%0Aand%20bottlenecks.%20This%20paper%20posits%20that%20noise%20effects%2C%20stemming%20from%20changes%20in%0Athe%20signal-to-noise%20ratio%20under%20diminishing%20marginal%20returns%2C%20are%20the%20root%0Acause%20of%20these%20issues.%20To%20control%20this%20noise%2C%20we%20investigated%20the%20differences%0Abetween%20models%20with%20performance%20advantages%20and%20disadvantages%2C%20introducing%20the%0Aconcept%20of%20%22relative%20overfitting.%22%20Based%20on%20their%20complementary%20strengths%2C%20we%0Ahave%20proposed%20an%20application%20framework%2C%20Accept-Reject%20%28AR%29.%20In%20Natural%20Language%0AProcessing%20%28NLP%29%2C%20we%20use%20LLMs%20and%20Small%20Language%20Models%20%28SLMs%29%20as%20the%20medium%0Afor%20discussion.%20This%20framework%20enables%20SLMs%20to%20exert%20a%20universal%20positive%0Ainfluence%20on%20LLM%20decision%20outputs%2C%20rather%20than%20the%20intuitively%20expected%0Anegative%20influence.%20We%20validated%20our%20approach%20using%20self-built%20models%20based%20on%0Amainstream%20architectures%20and%20pre-trained%20mainstream%20models%20across%20multiple%0Adatasets%2C%20including%20basic%20language%20modeling%2C%20long-context%20tasks%2C%20subject%0Aexamination%2C%20and%20question-answering%20%28QA%29%20benchmarks.%20The%20results%20demonstrate%0Athat%20through%20our%20structure%2C%20compared%20to%20increasing%20the%20LLM%27s%20parameters%2C%20we%20can%0Aachieve%20better%20performance%20improvements%20with%20significantly%20lower%20parameter%20and%0Acomputational%20costs%20in%20many%20scenarios.%20These%20improvements%20are%20universal%2C%0Astable%2C%20and%20effective.%20Furthermore%2C%20we%20explore%20the%20potential%20of%20%22relative%0Aoverfitting%22%20and%20the%20AR%20framework%20in%20other%20machine%20learning%20domains%2C%20such%20as%0Acomputer%20vision%20%28CV%29%20and%20AI%20for%20science.%20We%20hope%20the%20proposed%20approach%20can%20help%0Ascale%20laws%20overcome%20existing%20bottlenecks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Overfitting%2520and%2520Accept-Reject%2520Framework%26entry.906535625%3DYanxin%2520Liu%2520and%2520Yunqi%2520Zhang%26entry.1292438233%3D%2520%2520Currently%252C%2520the%2520scaling%2520law%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520faces%2520challenges%250Aand%2520bottlenecks.%2520This%2520paper%2520posits%2520that%2520noise%2520effects%252C%2520stemming%2520from%2520changes%2520in%250Athe%2520signal-to-noise%2520ratio%2520under%2520diminishing%2520marginal%2520returns%252C%2520are%2520the%2520root%250Acause%2520of%2520these%2520issues.%2520To%2520control%2520this%2520noise%252C%2520we%2520investigated%2520the%2520differences%250Abetween%2520models%2520with%2520performance%2520advantages%2520and%2520disadvantages%252C%2520introducing%2520the%250Aconcept%2520of%2520%2522relative%2520overfitting.%2522%2520Based%2520on%2520their%2520complementary%2520strengths%252C%2520we%250Ahave%2520proposed%2520an%2520application%2520framework%252C%2520Accept-Reject%2520%2528AR%2529.%2520In%2520Natural%2520Language%250AProcessing%2520%2528NLP%2529%252C%2520we%2520use%2520LLMs%2520and%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520as%2520the%2520medium%250Afor%2520discussion.%2520This%2520framework%2520enables%2520SLMs%2520to%2520exert%2520a%2520universal%2520positive%250Ainfluence%2520on%2520LLM%2520decision%2520outputs%252C%2520rather%2520than%2520the%2520intuitively%2520expected%250Anegative%2520influence.%2520We%2520validated%2520our%2520approach%2520using%2520self-built%2520models%2520based%2520on%250Amainstream%2520architectures%2520and%2520pre-trained%2520mainstream%2520models%2520across%2520multiple%250Adatasets%252C%2520including%2520basic%2520language%2520modeling%252C%2520long-context%2520tasks%252C%2520subject%250Aexamination%252C%2520and%2520question-answering%2520%2528QA%2529%2520benchmarks.%2520The%2520results%2520demonstrate%250Athat%2520through%2520our%2520structure%252C%2520compared%2520to%2520increasing%2520the%2520LLM%2527s%2520parameters%252C%2520we%2520can%250Aachieve%2520better%2520performance%2520improvements%2520with%2520significantly%2520lower%2520parameter%2520and%250Acomputational%2520costs%2520in%2520many%2520scenarios.%2520These%2520improvements%2520are%2520universal%252C%250Astable%252C%2520and%2520effective.%2520Furthermore%252C%2520we%2520explore%2520the%2520potential%2520of%2520%2522relative%250Aoverfitting%2522%2520and%2520the%2520AR%2520framework%2520in%2520other%2520machine%2520learning%2520domains%252C%2520such%2520as%250Acomputer%2520vision%2520%2528CV%2529%2520and%2520AI%2520for%2520science.%2520We%2520hope%2520the%2520proposed%2520approach%2520can%2520help%250Ascale%2520laws%2520overcome%2520existing%2520bottlenecks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Overfitting%20and%20Accept-Reject%20Framework&entry.906535625=Yanxin%20Liu%20and%20Yunqi%20Zhang&entry.1292438233=%20%20Currently%2C%20the%20scaling%20law%20of%20Large%20Language%20Models%20%28LLMs%29%20faces%20challenges%0Aand%20bottlenecks.%20This%20paper%20posits%20that%20noise%20effects%2C%20stemming%20from%20changes%20in%0Athe%20signal-to-noise%20ratio%20under%20diminishing%20marginal%20returns%2C%20are%20the%20root%0Acause%20of%20these%20issues.%20To%20control%20this%20noise%2C%20we%20investigated%20the%20differences%0Abetween%20models%20with%20performance%20advantages%20and%20disadvantages%2C%20introducing%20the%0Aconcept%20of%20%22relative%20overfitting.%22%20Based%20on%20their%20complementary%20strengths%2C%20we%0Ahave%20proposed%20an%20application%20framework%2C%20Accept-Reject%20%28AR%29.%20In%20Natural%20Language%0AProcessing%20%28NLP%29%2C%20we%20use%20LLMs%20and%20Small%20Language%20Models%20%28SLMs%29%20as%20the%20medium%0Afor%20discussion.%20This%20framework%20enables%20SLMs%20to%20exert%20a%20universal%20positive%0Ainfluence%20on%20LLM%20decision%20outputs%2C%20rather%20than%20the%20intuitively%20expected%0Anegative%20influence.%20We%20validated%20our%20approach%20using%20self-built%20models%20based%20on%0Amainstream%20architectures%20and%20pre-trained%20mainstream%20models%20across%20multiple%0Adatasets%2C%20including%20basic%20language%20modeling%2C%20long-context%20tasks%2C%20subject%0Aexamination%2C%20and%20question-answering%20%28QA%29%20benchmarks.%20The%20results%20demonstrate%0Athat%20through%20our%20structure%2C%20compared%20to%20increasing%20the%20LLM%27s%20parameters%2C%20we%20can%0Aachieve%20better%20performance%20improvements%20with%20significantly%20lower%20parameter%20and%0Acomputational%20costs%20in%20many%20scenarios.%20These%20improvements%20are%20universal%2C%0Astable%2C%20and%20effective.%20Furthermore%2C%20we%20explore%20the%20potential%20of%20%22relative%0Aoverfitting%22%20and%20the%20AR%20framework%20in%20other%20machine%20learning%20domains%2C%20such%20as%0Acomputer%20vision%20%28CV%29%20and%20AI%20for%20science.%20We%20hope%20the%20proposed%20approach%20can%20help%0Ascale%20laws%20overcome%20existing%20bottlenecks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07783v1&entry.124074799=Read"},
{"title": "FD-RIO: Fast Dense Radar Inertial Odometry", "author": "Nader J. Abu-Alrub and Nathir A. Rawashdeh", "abstract": "  Radar-based odometry is a popular solution for ego-motion estimation in\nconditions where other exteroceptive sensors may degrade, whether due to poor\nlighting or challenging weather conditions; however, scanning radars have the\ndownside of relatively lower sampling rate and spatial resolution. In this\nwork, we present FD-RIO, a method to alleviate this problem by fusing noisy,\ndrift-prone, but high-frequency IMU data with dense radar scans. To the best of\nour knowledge, this is the first attempt to fuse dense scanning radar odometry\nwith IMU using a Kalman filter. We evaluate our methods using two publicly\navailable datasets and report accuracies using standard KITTI evaluation\nmetrics, in addition to ablation tests and runtime analysis. Our phase\ncorrelation -based approach is compact, intuitive, and is designed to be a\npractical solution deployable on a realistic hardware setup of a mobile\nplatform. Despite its simplicity, FD-RIO is on par with other state-of-the-art\nmethods and outperforms in some test sequences.\n", "link": "http://arxiv.org/abs/2505.07694v1", "date": "2025-05-12", "relevancy": 2.0796, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FD-RIO%3A%20Fast%20Dense%20Radar%20Inertial%20Odometry&body=Title%3A%20FD-RIO%3A%20Fast%20Dense%20Radar%20Inertial%20Odometry%0AAuthor%3A%20Nader%20J.%20Abu-Alrub%20and%20Nathir%20A.%20Rawashdeh%0AAbstract%3A%20%20%20Radar-based%20odometry%20is%20a%20popular%20solution%20for%20ego-motion%20estimation%20in%0Aconditions%20where%20other%20exteroceptive%20sensors%20may%20degrade%2C%20whether%20due%20to%20poor%0Alighting%20or%20challenging%20weather%20conditions%3B%20however%2C%20scanning%20radars%20have%20the%0Adownside%20of%20relatively%20lower%20sampling%20rate%20and%20spatial%20resolution.%20In%20this%0Awork%2C%20we%20present%20FD-RIO%2C%20a%20method%20to%20alleviate%20this%20problem%20by%20fusing%20noisy%2C%0Adrift-prone%2C%20but%20high-frequency%20IMU%20data%20with%20dense%20radar%20scans.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20attempt%20to%20fuse%20dense%20scanning%20radar%20odometry%0Awith%20IMU%20using%20a%20Kalman%20filter.%20We%20evaluate%20our%20methods%20using%20two%20publicly%0Aavailable%20datasets%20and%20report%20accuracies%20using%20standard%20KITTI%20evaluation%0Ametrics%2C%20in%20addition%20to%20ablation%20tests%20and%20runtime%20analysis.%20Our%20phase%0Acorrelation%20-based%20approach%20is%20compact%2C%20intuitive%2C%20and%20is%20designed%20to%20be%20a%0Apractical%20solution%20deployable%20on%20a%20realistic%20hardware%20setup%20of%20a%20mobile%0Aplatform.%20Despite%20its%20simplicity%2C%20FD-RIO%20is%20on%20par%20with%20other%20state-of-the-art%0Amethods%20and%20outperforms%20in%20some%20test%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFD-RIO%253A%2520Fast%2520Dense%2520Radar%2520Inertial%2520Odometry%26entry.906535625%3DNader%2520J.%2520Abu-Alrub%2520and%2520Nathir%2520A.%2520Rawashdeh%26entry.1292438233%3D%2520%2520Radar-based%2520odometry%2520is%2520a%2520popular%2520solution%2520for%2520ego-motion%2520estimation%2520in%250Aconditions%2520where%2520other%2520exteroceptive%2520sensors%2520may%2520degrade%252C%2520whether%2520due%2520to%2520poor%250Alighting%2520or%2520challenging%2520weather%2520conditions%253B%2520however%252C%2520scanning%2520radars%2520have%2520the%250Adownside%2520of%2520relatively%2520lower%2520sampling%2520rate%2520and%2520spatial%2520resolution.%2520In%2520this%250Awork%252C%2520we%2520present%2520FD-RIO%252C%2520a%2520method%2520to%2520alleviate%2520this%2520problem%2520by%2520fusing%2520noisy%252C%250Adrift-prone%252C%2520but%2520high-frequency%2520IMU%2520data%2520with%2520dense%2520radar%2520scans.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520to%2520fuse%2520dense%2520scanning%2520radar%2520odometry%250Awith%2520IMU%2520using%2520a%2520Kalman%2520filter.%2520We%2520evaluate%2520our%2520methods%2520using%2520two%2520publicly%250Aavailable%2520datasets%2520and%2520report%2520accuracies%2520using%2520standard%2520KITTI%2520evaluation%250Ametrics%252C%2520in%2520addition%2520to%2520ablation%2520tests%2520and%2520runtime%2520analysis.%2520Our%2520phase%250Acorrelation%2520-based%2520approach%2520is%2520compact%252C%2520intuitive%252C%2520and%2520is%2520designed%2520to%2520be%2520a%250Apractical%2520solution%2520deployable%2520on%2520a%2520realistic%2520hardware%2520setup%2520of%2520a%2520mobile%250Aplatform.%2520Despite%2520its%2520simplicity%252C%2520FD-RIO%2520is%2520on%2520par%2520with%2520other%2520state-of-the-art%250Amethods%2520and%2520outperforms%2520in%2520some%2520test%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FD-RIO%3A%20Fast%20Dense%20Radar%20Inertial%20Odometry&entry.906535625=Nader%20J.%20Abu-Alrub%20and%20Nathir%20A.%20Rawashdeh&entry.1292438233=%20%20Radar-based%20odometry%20is%20a%20popular%20solution%20for%20ego-motion%20estimation%20in%0Aconditions%20where%20other%20exteroceptive%20sensors%20may%20degrade%2C%20whether%20due%20to%20poor%0Alighting%20or%20challenging%20weather%20conditions%3B%20however%2C%20scanning%20radars%20have%20the%0Adownside%20of%20relatively%20lower%20sampling%20rate%20and%20spatial%20resolution.%20In%20this%0Awork%2C%20we%20present%20FD-RIO%2C%20a%20method%20to%20alleviate%20this%20problem%20by%20fusing%20noisy%2C%0Adrift-prone%2C%20but%20high-frequency%20IMU%20data%20with%20dense%20radar%20scans.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20attempt%20to%20fuse%20dense%20scanning%20radar%20odometry%0Awith%20IMU%20using%20a%20Kalman%20filter.%20We%20evaluate%20our%20methods%20using%20two%20publicly%0Aavailable%20datasets%20and%20report%20accuracies%20using%20standard%20KITTI%20evaluation%0Ametrics%2C%20in%20addition%20to%20ablation%20tests%20and%20runtime%20analysis.%20Our%20phase%0Acorrelation%20-based%20approach%20is%20compact%2C%20intuitive%2C%20and%20is%20designed%20to%20be%20a%0Apractical%20solution%20deployable%20on%20a%20realistic%20hardware%20setup%20of%20a%20mobile%0Aplatform.%20Despite%20its%20simplicity%2C%20FD-RIO%20is%20on%20par%20with%20other%20state-of-the-art%0Amethods%20and%20outperforms%20in%20some%20test%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07694v1&entry.124074799=Read"},
{"title": "Continuous Visual Autoregressive Generation via Score Maximization", "author": "Chenze Shao and Fandong Meng and Jie Zhou", "abstract": "  Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR.\n", "link": "http://arxiv.org/abs/2505.07812v1", "date": "2025-05-12", "relevancy": 2.0793, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5255}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5167}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Visual%20Autoregressive%20Generation%20via%20Score%20Maximization&body=Title%3A%20Continuous%20Visual%20Autoregressive%20Generation%20via%20Score%20Maximization%0AAuthor%3A%20Chenze%20Shao%20and%20Fandong%20Meng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Conventional%20wisdom%20suggests%20that%20autoregressive%20models%20are%20used%20to%20process%0Adiscrete%20data.%20When%20applied%20to%20continuous%20modalities%20such%20as%20visual%20data%2C%0AVisual%20AutoRegressive%20modeling%20%28VAR%29%20typically%20resorts%20to%20quantization-based%0Aapproaches%20to%20cast%20the%20data%20into%20a%20discrete%20space%2C%20which%20can%20introduce%0Asignificant%20information%20loss.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20Continuous%0AVAR%20framework%20that%20enables%20direct%20visual%20autoregressive%20generation%20without%0Avector%20quantization.%20The%20underlying%20theoretical%20foundation%20is%20strictly%20proper%0Ascoring%20rules%2C%20which%20provide%20powerful%20statistical%20tools%20capable%20of%20evaluating%0Ahow%20well%20a%20generative%20model%20approximates%20the%20true%20distribution.%20Within%20this%0Aframework%2C%20all%20we%20need%20is%20to%20select%20a%20strictly%20proper%20score%20and%20set%20it%20as%20the%0Atraining%20objective%20to%20optimize.%20We%20primarily%20explore%20a%20class%20of%20training%0Aobjectives%20based%20on%20the%20energy%20score%2C%20which%20is%20likelihood-free%20and%20thus%0Aovercomes%20the%20difficulty%20of%20making%20probabilistic%20predictions%20in%20the%20continuous%0Aspace.%20Previous%20efforts%20on%20continuous%20autoregressive%20generation%2C%20such%20as%20GIVT%0Aand%20diffusion%20loss%2C%20can%20also%20be%20derived%20from%20our%20framework%20using%20other%20strictly%0Aproper%20scores.%20Source%20code%3A%20https%3A//github.com/shaochenze/EAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Visual%2520Autoregressive%2520Generation%2520via%2520Score%2520Maximization%26entry.906535625%3DChenze%2520Shao%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Conventional%2520wisdom%2520suggests%2520that%2520autoregressive%2520models%2520are%2520used%2520to%2520process%250Adiscrete%2520data.%2520When%2520applied%2520to%2520continuous%2520modalities%2520such%2520as%2520visual%2520data%252C%250AVisual%2520AutoRegressive%2520modeling%2520%2528VAR%2529%2520typically%2520resorts%2520to%2520quantization-based%250Aapproaches%2520to%2520cast%2520the%2520data%2520into%2520a%2520discrete%2520space%252C%2520which%2520can%2520introduce%250Asignificant%2520information%2520loss.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520a%2520Continuous%250AVAR%2520framework%2520that%2520enables%2520direct%2520visual%2520autoregressive%2520generation%2520without%250Avector%2520quantization.%2520The%2520underlying%2520theoretical%2520foundation%2520is%2520strictly%2520proper%250Ascoring%2520rules%252C%2520which%2520provide%2520powerful%2520statistical%2520tools%2520capable%2520of%2520evaluating%250Ahow%2520well%2520a%2520generative%2520model%2520approximates%2520the%2520true%2520distribution.%2520Within%2520this%250Aframework%252C%2520all%2520we%2520need%2520is%2520to%2520select%2520a%2520strictly%2520proper%2520score%2520and%2520set%2520it%2520as%2520the%250Atraining%2520objective%2520to%2520optimize.%2520We%2520primarily%2520explore%2520a%2520class%2520of%2520training%250Aobjectives%2520based%2520on%2520the%2520energy%2520score%252C%2520which%2520is%2520likelihood-free%2520and%2520thus%250Aovercomes%2520the%2520difficulty%2520of%2520making%2520probabilistic%2520predictions%2520in%2520the%2520continuous%250Aspace.%2520Previous%2520efforts%2520on%2520continuous%2520autoregressive%2520generation%252C%2520such%2520as%2520GIVT%250Aand%2520diffusion%2520loss%252C%2520can%2520also%2520be%2520derived%2520from%2520our%2520framework%2520using%2520other%2520strictly%250Aproper%2520scores.%2520Source%2520code%253A%2520https%253A//github.com/shaochenze/EAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Visual%20Autoregressive%20Generation%20via%20Score%20Maximization&entry.906535625=Chenze%20Shao%20and%20Fandong%20Meng%20and%20Jie%20Zhou&entry.1292438233=%20%20Conventional%20wisdom%20suggests%20that%20autoregressive%20models%20are%20used%20to%20process%0Adiscrete%20data.%20When%20applied%20to%20continuous%20modalities%20such%20as%20visual%20data%2C%0AVisual%20AutoRegressive%20modeling%20%28VAR%29%20typically%20resorts%20to%20quantization-based%0Aapproaches%20to%20cast%20the%20data%20into%20a%20discrete%20space%2C%20which%20can%20introduce%0Asignificant%20information%20loss.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20Continuous%0AVAR%20framework%20that%20enables%20direct%20visual%20autoregressive%20generation%20without%0Avector%20quantization.%20The%20underlying%20theoretical%20foundation%20is%20strictly%20proper%0Ascoring%20rules%2C%20which%20provide%20powerful%20statistical%20tools%20capable%20of%20evaluating%0Ahow%20well%20a%20generative%20model%20approximates%20the%20true%20distribution.%20Within%20this%0Aframework%2C%20all%20we%20need%20is%20to%20select%20a%20strictly%20proper%20score%20and%20set%20it%20as%20the%0Atraining%20objective%20to%20optimize.%20We%20primarily%20explore%20a%20class%20of%20training%0Aobjectives%20based%20on%20the%20energy%20score%2C%20which%20is%20likelihood-free%20and%20thus%0Aovercomes%20the%20difficulty%20of%20making%20probabilistic%20predictions%20in%20the%20continuous%0Aspace.%20Previous%20efforts%20on%20continuous%20autoregressive%20generation%2C%20such%20as%20GIVT%0Aand%20diffusion%20loss%2C%20can%20also%20be%20derived%20from%20our%20framework%20using%20other%20strictly%0Aproper%20scores.%20Source%20code%3A%20https%3A//github.com/shaochenze/EAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07812v1&entry.124074799=Read"},
{"title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models", "author": "Hang Wu and Jianian Zhu and Yinghui Li and Haojie Wang and Biao Hou and Jidong Zhai", "abstract": "  Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.\n", "link": "http://arxiv.org/abs/2505.07680v1", "date": "2025-05-12", "relevancy": 2.0745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecRouter%3A%20Adaptive%20Routing%20for%20Multi-Level%20Speculative%20Decoding%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20SpecRouter%3A%20Adaptive%20Routing%20for%20Multi-Level%20Speculative%20Decoding%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Hang%20Wu%20and%20Jianian%20Zhu%20and%20Yinghui%20Li%20and%20Haojie%20Wang%20and%20Biao%20Hou%20and%20Jidong%20Zhai%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20present%20a%20critical%20trade-off%20between%20inference%0Aquality%20and%20computational%20cost%3A%20larger%20models%20offer%20superior%20capabilities%20but%0Aincur%20significant%20latency%2C%20while%20smaller%20models%20are%20faster%20but%20less%20powerful.%0AExisting%20serving%20strategies%20often%20employ%20fixed%20model%20scales%20or%20static%20two-stage%0Aspeculative%20decoding%2C%20failing%20to%20dynamically%20adapt%20to%20the%20varying%20complexities%0Aof%20user%20requests%20or%20fluctuations%20in%20system%20performance.%20This%20paper%20introduces%0A%5Csystemname%7B%7D%2C%20a%20novel%20framework%20that%20reimagines%20LLM%20inference%20as%20an%20adaptive%0Arouting%20problem%20solved%20through%20multi-level%20speculative%20decoding.%20%5Csystemname%7B%7D%0Adynamically%20constructs%20and%20optimizes%20inference%20%22paths%22%20%28chains%20of%20models%29%20based%0Aon%20real-time%20feedback%2C%20addressing%20the%20limitations%20of%20static%20approaches.%20Our%0Acontributions%20are%20threefold%3A%20%281%29%20An%20%5Ctextbf%7Badaptive%20model%20chain%20scheduling%7D%0Amechanism%20that%20leverages%20performance%20profiling%20%28execution%20times%29%20and%20predictive%0Asimilarity%20metrics%20%28derived%20from%20token%20distribution%20divergence%29%20to%20continuously%0Aselect%20the%20optimal%20sequence%20of%20draft%20and%20verifier%20models%2C%20minimizing%20predicted%0Alatency%20per%20generated%20token.%20%282%29%20A%20%5Ctextbf%7Bmulti-level%20collaborative%0Averification%7D%20framework%20where%20intermediate%20models%20within%20the%20selected%20chain%20can%0Avalidate%20speculative%20tokens%2C%20reducing%20the%20verification%20burden%20on%20the%20final%2C%0Amost%20powerful%20target%20model.%20%283%29%20A%20%5Ctextbf%7Bsynchronized%20state%20management%7D%20system%0Aproviding%20efficient%2C%20consistent%20KV%20cache%20handling%20across%20heterogeneous%20models%0Ain%20the%20chain%2C%20including%20precise%2C%20low-overhead%20rollbacks%20tailored%20for%0Aasynchronous%20batch%20processing%20inherent%20in%20multi-level%20speculation.%20Preliminary%0Aexperiments%20demonstrate%20the%20validity%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecRouter%253A%2520Adaptive%2520Routing%2520for%2520Multi-Level%2520Speculative%2520Decoding%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DHang%2520Wu%2520and%2520Jianian%2520Zhu%2520and%2520Yinghui%2520Li%2520and%2520Haojie%2520Wang%2520and%2520Biao%2520Hou%2520and%2520Jidong%2520Zhai%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520present%2520a%2520critical%2520trade-off%2520between%2520inference%250Aquality%2520and%2520computational%2520cost%253A%2520larger%2520models%2520offer%2520superior%2520capabilities%2520but%250Aincur%2520significant%2520latency%252C%2520while%2520smaller%2520models%2520are%2520faster%2520but%2520less%2520powerful.%250AExisting%2520serving%2520strategies%2520often%2520employ%2520fixed%2520model%2520scales%2520or%2520static%2520two-stage%250Aspeculative%2520decoding%252C%2520failing%2520to%2520dynamically%2520adapt%2520to%2520the%2520varying%2520complexities%250Aof%2520user%2520requests%2520or%2520fluctuations%2520in%2520system%2520performance.%2520This%2520paper%2520introduces%250A%255Csystemname%257B%257D%252C%2520a%2520novel%2520framework%2520that%2520reimagines%2520LLM%2520inference%2520as%2520an%2520adaptive%250Arouting%2520problem%2520solved%2520through%2520multi-level%2520speculative%2520decoding.%2520%255Csystemname%257B%257D%250Adynamically%2520constructs%2520and%2520optimizes%2520inference%2520%2522paths%2522%2520%2528chains%2520of%2520models%2529%2520based%250Aon%2520real-time%2520feedback%252C%2520addressing%2520the%2520limitations%2520of%2520static%2520approaches.%2520Our%250Acontributions%2520are%2520threefold%253A%2520%25281%2529%2520An%2520%255Ctextbf%257Badaptive%2520model%2520chain%2520scheduling%257D%250Amechanism%2520that%2520leverages%2520performance%2520profiling%2520%2528execution%2520times%2529%2520and%2520predictive%250Asimilarity%2520metrics%2520%2528derived%2520from%2520token%2520distribution%2520divergence%2529%2520to%2520continuously%250Aselect%2520the%2520optimal%2520sequence%2520of%2520draft%2520and%2520verifier%2520models%252C%2520minimizing%2520predicted%250Alatency%2520per%2520generated%2520token.%2520%25282%2529%2520A%2520%255Ctextbf%257Bmulti-level%2520collaborative%250Averification%257D%2520framework%2520where%2520intermediate%2520models%2520within%2520the%2520selected%2520chain%2520can%250Avalidate%2520speculative%2520tokens%252C%2520reducing%2520the%2520verification%2520burden%2520on%2520the%2520final%252C%250Amost%2520powerful%2520target%2520model.%2520%25283%2529%2520A%2520%255Ctextbf%257Bsynchronized%2520state%2520management%257D%2520system%250Aproviding%2520efficient%252C%2520consistent%2520KV%2520cache%2520handling%2520across%2520heterogeneous%2520models%250Ain%2520the%2520chain%252C%2520including%2520precise%252C%2520low-overhead%2520rollbacks%2520tailored%2520for%250Aasynchronous%2520batch%2520processing%2520inherent%2520in%2520multi-level%2520speculation.%2520Preliminary%250Aexperiments%2520demonstrate%2520the%2520validity%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecRouter%3A%20Adaptive%20Routing%20for%20Multi-Level%20Speculative%20Decoding%20in%0A%20%20Large%20Language%20Models&entry.906535625=Hang%20Wu%20and%20Jianian%20Zhu%20and%20Yinghui%20Li%20and%20Haojie%20Wang%20and%20Biao%20Hou%20and%20Jidong%20Zhai&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20present%20a%20critical%20trade-off%20between%20inference%0Aquality%20and%20computational%20cost%3A%20larger%20models%20offer%20superior%20capabilities%20but%0Aincur%20significant%20latency%2C%20while%20smaller%20models%20are%20faster%20but%20less%20powerful.%0AExisting%20serving%20strategies%20often%20employ%20fixed%20model%20scales%20or%20static%20two-stage%0Aspeculative%20decoding%2C%20failing%20to%20dynamically%20adapt%20to%20the%20varying%20complexities%0Aof%20user%20requests%20or%20fluctuations%20in%20system%20performance.%20This%20paper%20introduces%0A%5Csystemname%7B%7D%2C%20a%20novel%20framework%20that%20reimagines%20LLM%20inference%20as%20an%20adaptive%0Arouting%20problem%20solved%20through%20multi-level%20speculative%20decoding.%20%5Csystemname%7B%7D%0Adynamically%20constructs%20and%20optimizes%20inference%20%22paths%22%20%28chains%20of%20models%29%20based%0Aon%20real-time%20feedback%2C%20addressing%20the%20limitations%20of%20static%20approaches.%20Our%0Acontributions%20are%20threefold%3A%20%281%29%20An%20%5Ctextbf%7Badaptive%20model%20chain%20scheduling%7D%0Amechanism%20that%20leverages%20performance%20profiling%20%28execution%20times%29%20and%20predictive%0Asimilarity%20metrics%20%28derived%20from%20token%20distribution%20divergence%29%20to%20continuously%0Aselect%20the%20optimal%20sequence%20of%20draft%20and%20verifier%20models%2C%20minimizing%20predicted%0Alatency%20per%20generated%20token.%20%282%29%20A%20%5Ctextbf%7Bmulti-level%20collaborative%0Averification%7D%20framework%20where%20intermediate%20models%20within%20the%20selected%20chain%20can%0Avalidate%20speculative%20tokens%2C%20reducing%20the%20verification%20burden%20on%20the%20final%2C%0Amost%20powerful%20target%20model.%20%283%29%20A%20%5Ctextbf%7Bsynchronized%20state%20management%7D%20system%0Aproviding%20efficient%2C%20consistent%20KV%20cache%20handling%20across%20heterogeneous%20models%0Ain%20the%20chain%2C%20including%20precise%2C%20low-overhead%20rollbacks%20tailored%20for%0Aasynchronous%20batch%20processing%20inherent%20in%20multi-level%20speculation.%20Preliminary%0Aexperiments%20demonstrate%20the%20validity%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07680v1&entry.124074799=Read"},
{"title": "Hierarchical Sparse Attention Framework for Computationally Efficient\n  Classification of Biological Cells", "author": "Elad Yoshai and Dana Yagoda-Aharoni and Eden Dotan and Natan T. Shaked", "abstract": "  We present SparseAttnNet, a new hierarchical attention-driven framework for\nefficient image classification that adaptively selects and processes only the\nmost informative pixels from images. Traditional convolutional neural networks\ntypically process the entire images regardless of information density, leading\nto computational inefficiency and potential focus on irrelevant features. Our\napproach leverages a dynamic selection mechanism that uses coarse attention\ndistilled by fine multi-head attention from the downstream layers of the model,\nallowing the model to identify and extract the most salient k pixels, where k\nis adaptively learned during training based on loss convergence trends. Once\nthe top-k pixels are selected, the model processes only these pixels, embedding\nthem as words in a language model to capture their semantics, followed by\nmulti-head attention to incorporate global context. For biological cell images,\nwe demonstrate that SparseAttnNet can process approximately 15% of the pixels\ninstead of the full image. Applied to cell classification tasks using white\nblood cells images from the following modalities: optical path difference (OPD)\nimages from digital holography for stain-free cells, images from\nmotion-sensitive (event) camera from stain-free cells, and brightfield\nmicroscopy images of stained cells, For all three imaging modalities,\nSparseAttnNet achieves competitive accuracy while drastically reducing\ncomputational requirements in terms of both parameters and floating-point\noperations per second, compared to traditional CNNs and Vision Transformers.\nSince the model focuses on biologically relevant regions, it also offers\nimproved explainability. The adaptive and lightweight nature of SparseAttnNet\nmakes it ideal for deployment in resource-constrained and high-throughput\nsettings, including imaging flow cytometry.\n", "link": "http://arxiv.org/abs/2505.07661v1", "date": "2025-05-12", "relevancy": 2.0691, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5592}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4932}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Sparse%20Attention%20Framework%20for%20Computationally%20Efficient%0A%20%20Classification%20of%20Biological%20Cells&body=Title%3A%20Hierarchical%20Sparse%20Attention%20Framework%20for%20Computationally%20Efficient%0A%20%20Classification%20of%20Biological%20Cells%0AAuthor%3A%20Elad%20Yoshai%20and%20Dana%20Yagoda-Aharoni%20and%20Eden%20Dotan%20and%20Natan%20T.%20Shaked%0AAbstract%3A%20%20%20We%20present%20SparseAttnNet%2C%20a%20new%20hierarchical%20attention-driven%20framework%20for%0Aefficient%20image%20classification%20that%20adaptively%20selects%20and%20processes%20only%20the%0Amost%20informative%20pixels%20from%20images.%20Traditional%20convolutional%20neural%20networks%0Atypically%20process%20the%20entire%20images%20regardless%20of%20information%20density%2C%20leading%0Ato%20computational%20inefficiency%20and%20potential%20focus%20on%20irrelevant%20features.%20Our%0Aapproach%20leverages%20a%20dynamic%20selection%20mechanism%20that%20uses%20coarse%20attention%0Adistilled%20by%20fine%20multi-head%20attention%20from%20the%20downstream%20layers%20of%20the%20model%2C%0Aallowing%20the%20model%20to%20identify%20and%20extract%20the%20most%20salient%20k%20pixels%2C%20where%20k%0Ais%20adaptively%20learned%20during%20training%20based%20on%20loss%20convergence%20trends.%20Once%0Athe%20top-k%20pixels%20are%20selected%2C%20the%20model%20processes%20only%20these%20pixels%2C%20embedding%0Athem%20as%20words%20in%20a%20language%20model%20to%20capture%20their%20semantics%2C%20followed%20by%0Amulti-head%20attention%20to%20incorporate%20global%20context.%20For%20biological%20cell%20images%2C%0Awe%20demonstrate%20that%20SparseAttnNet%20can%20process%20approximately%2015%25%20of%20the%20pixels%0Ainstead%20of%20the%20full%20image.%20Applied%20to%20cell%20classification%20tasks%20using%20white%0Ablood%20cells%20images%20from%20the%20following%20modalities%3A%20optical%20path%20difference%20%28OPD%29%0Aimages%20from%20digital%20holography%20for%20stain-free%20cells%2C%20images%20from%0Amotion-sensitive%20%28event%29%20camera%20from%20stain-free%20cells%2C%20and%20brightfield%0Amicroscopy%20images%20of%20stained%20cells%2C%20For%20all%20three%20imaging%20modalities%2C%0ASparseAttnNet%20achieves%20competitive%20accuracy%20while%20drastically%20reducing%0Acomputational%20requirements%20in%20terms%20of%20both%20parameters%20and%20floating-point%0Aoperations%20per%20second%2C%20compared%20to%20traditional%20CNNs%20and%20Vision%20Transformers.%0ASince%20the%20model%20focuses%20on%20biologically%20relevant%20regions%2C%20it%20also%20offers%0Aimproved%20explainability.%20The%20adaptive%20and%20lightweight%20nature%20of%20SparseAttnNet%0Amakes%20it%20ideal%20for%20deployment%20in%20resource-constrained%20and%20high-throughput%0Asettings%2C%20including%20imaging%20flow%20cytometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Sparse%2520Attention%2520Framework%2520for%2520Computationally%2520Efficient%250A%2520%2520Classification%2520of%2520Biological%2520Cells%26entry.906535625%3DElad%2520Yoshai%2520and%2520Dana%2520Yagoda-Aharoni%2520and%2520Eden%2520Dotan%2520and%2520Natan%2520T.%2520Shaked%26entry.1292438233%3D%2520%2520We%2520present%2520SparseAttnNet%252C%2520a%2520new%2520hierarchical%2520attention-driven%2520framework%2520for%250Aefficient%2520image%2520classification%2520that%2520adaptively%2520selects%2520and%2520processes%2520only%2520the%250Amost%2520informative%2520pixels%2520from%2520images.%2520Traditional%2520convolutional%2520neural%2520networks%250Atypically%2520process%2520the%2520entire%2520images%2520regardless%2520of%2520information%2520density%252C%2520leading%250Ato%2520computational%2520inefficiency%2520and%2520potential%2520focus%2520on%2520irrelevant%2520features.%2520Our%250Aapproach%2520leverages%2520a%2520dynamic%2520selection%2520mechanism%2520that%2520uses%2520coarse%2520attention%250Adistilled%2520by%2520fine%2520multi-head%2520attention%2520from%2520the%2520downstream%2520layers%2520of%2520the%2520model%252C%250Aallowing%2520the%2520model%2520to%2520identify%2520and%2520extract%2520the%2520most%2520salient%2520k%2520pixels%252C%2520where%2520k%250Ais%2520adaptively%2520learned%2520during%2520training%2520based%2520on%2520loss%2520convergence%2520trends.%2520Once%250Athe%2520top-k%2520pixels%2520are%2520selected%252C%2520the%2520model%2520processes%2520only%2520these%2520pixels%252C%2520embedding%250Athem%2520as%2520words%2520in%2520a%2520language%2520model%2520to%2520capture%2520their%2520semantics%252C%2520followed%2520by%250Amulti-head%2520attention%2520to%2520incorporate%2520global%2520context.%2520For%2520biological%2520cell%2520images%252C%250Awe%2520demonstrate%2520that%2520SparseAttnNet%2520can%2520process%2520approximately%252015%2525%2520of%2520the%2520pixels%250Ainstead%2520of%2520the%2520full%2520image.%2520Applied%2520to%2520cell%2520classification%2520tasks%2520using%2520white%250Ablood%2520cells%2520images%2520from%2520the%2520following%2520modalities%253A%2520optical%2520path%2520difference%2520%2528OPD%2529%250Aimages%2520from%2520digital%2520holography%2520for%2520stain-free%2520cells%252C%2520images%2520from%250Amotion-sensitive%2520%2528event%2529%2520camera%2520from%2520stain-free%2520cells%252C%2520and%2520brightfield%250Amicroscopy%2520images%2520of%2520stained%2520cells%252C%2520For%2520all%2520three%2520imaging%2520modalities%252C%250ASparseAttnNet%2520achieves%2520competitive%2520accuracy%2520while%2520drastically%2520reducing%250Acomputational%2520requirements%2520in%2520terms%2520of%2520both%2520parameters%2520and%2520floating-point%250Aoperations%2520per%2520second%252C%2520compared%2520to%2520traditional%2520CNNs%2520and%2520Vision%2520Transformers.%250ASince%2520the%2520model%2520focuses%2520on%2520biologically%2520relevant%2520regions%252C%2520it%2520also%2520offers%250Aimproved%2520explainability.%2520The%2520adaptive%2520and%2520lightweight%2520nature%2520of%2520SparseAttnNet%250Amakes%2520it%2520ideal%2520for%2520deployment%2520in%2520resource-constrained%2520and%2520high-throughput%250Asettings%252C%2520including%2520imaging%2520flow%2520cytometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Sparse%20Attention%20Framework%20for%20Computationally%20Efficient%0A%20%20Classification%20of%20Biological%20Cells&entry.906535625=Elad%20Yoshai%20and%20Dana%20Yagoda-Aharoni%20and%20Eden%20Dotan%20and%20Natan%20T.%20Shaked&entry.1292438233=%20%20We%20present%20SparseAttnNet%2C%20a%20new%20hierarchical%20attention-driven%20framework%20for%0Aefficient%20image%20classification%20that%20adaptively%20selects%20and%20processes%20only%20the%0Amost%20informative%20pixels%20from%20images.%20Traditional%20convolutional%20neural%20networks%0Atypically%20process%20the%20entire%20images%20regardless%20of%20information%20density%2C%20leading%0Ato%20computational%20inefficiency%20and%20potential%20focus%20on%20irrelevant%20features.%20Our%0Aapproach%20leverages%20a%20dynamic%20selection%20mechanism%20that%20uses%20coarse%20attention%0Adistilled%20by%20fine%20multi-head%20attention%20from%20the%20downstream%20layers%20of%20the%20model%2C%0Aallowing%20the%20model%20to%20identify%20and%20extract%20the%20most%20salient%20k%20pixels%2C%20where%20k%0Ais%20adaptively%20learned%20during%20training%20based%20on%20loss%20convergence%20trends.%20Once%0Athe%20top-k%20pixels%20are%20selected%2C%20the%20model%20processes%20only%20these%20pixels%2C%20embedding%0Athem%20as%20words%20in%20a%20language%20model%20to%20capture%20their%20semantics%2C%20followed%20by%0Amulti-head%20attention%20to%20incorporate%20global%20context.%20For%20biological%20cell%20images%2C%0Awe%20demonstrate%20that%20SparseAttnNet%20can%20process%20approximately%2015%25%20of%20the%20pixels%0Ainstead%20of%20the%20full%20image.%20Applied%20to%20cell%20classification%20tasks%20using%20white%0Ablood%20cells%20images%20from%20the%20following%20modalities%3A%20optical%20path%20difference%20%28OPD%29%0Aimages%20from%20digital%20holography%20for%20stain-free%20cells%2C%20images%20from%0Amotion-sensitive%20%28event%29%20camera%20from%20stain-free%20cells%2C%20and%20brightfield%0Amicroscopy%20images%20of%20stained%20cells%2C%20For%20all%20three%20imaging%20modalities%2C%0ASparseAttnNet%20achieves%20competitive%20accuracy%20while%20drastically%20reducing%0Acomputational%20requirements%20in%20terms%20of%20both%20parameters%20and%20floating-point%0Aoperations%20per%20second%2C%20compared%20to%20traditional%20CNNs%20and%20Vision%20Transformers.%0ASince%20the%20model%20focuses%20on%20biologically%20relevant%20regions%2C%20it%20also%20offers%0Aimproved%20explainability.%20The%20adaptive%20and%20lightweight%20nature%20of%20SparseAttnNet%0Amakes%20it%20ideal%20for%20deployment%20in%20resource-constrained%20and%20high-throughput%0Asettings%2C%20including%20imaging%20flow%20cytometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07661v1&entry.124074799=Read"},
{"title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and\n  EVolution", "author": "Xu Huang and Weiwen Liu and Xingshan Zeng and Yuefeng Huang and Xinlong Hao and Yuxian Wang and Yirong Zeng and Chuhan Wu and Yasheng Wang and Ruiming Tang and Defu Lian", "abstract": "  The tool-using capability of large language models (LLMs) enables them to\naccess up-to-date external information and handle complex tasks. Current\napproaches to enhancing this capability primarily rely on distilling advanced\nmodels by data synthesis. However, this method incurs significant costs\nassociated with advanced model usage and often results in data compatibility\nissues, led by the high discrepancy in the knowledge scope between the advanced\nmodel and the target model. To address these challenges, we propose\nToolACE-DEV, a self-improving framework for tool learning. First, we decompose\nthe tool-learning objective into sub-tasks that enhance basic tool-making and\ntool-using abilities. Then, we introduce a self-evolving paradigm that allows\nlightweight models to self-improve, reducing reliance on advanced LLMs.\nExtensive experiments validate the effectiveness of our approach across models\nof varying scales and architectures.\n", "link": "http://arxiv.org/abs/2505.07512v1", "date": "2025-05-12", "relevancy": 2.0651, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5259}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToolACE-DEV%3A%20Self-Improving%20Tool%20Learning%20via%20Decomposition%20and%0A%20%20EVolution&body=Title%3A%20ToolACE-DEV%3A%20Self-Improving%20Tool%20Learning%20via%20Decomposition%20and%0A%20%20EVolution%0AAuthor%3A%20Xu%20Huang%20and%20Weiwen%20Liu%20and%20Xingshan%20Zeng%20and%20Yuefeng%20Huang%20and%20Xinlong%20Hao%20and%20Yuxian%20Wang%20and%20Yirong%20Zeng%20and%20Chuhan%20Wu%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%20and%20Defu%20Lian%0AAbstract%3A%20%20%20The%20tool-using%20capability%20of%20large%20language%20models%20%28LLMs%29%20enables%20them%20to%0Aaccess%20up-to-date%20external%20information%20and%20handle%20complex%20tasks.%20Current%0Aapproaches%20to%20enhancing%20this%20capability%20primarily%20rely%20on%20distilling%20advanced%0Amodels%20by%20data%20synthesis.%20However%2C%20this%20method%20incurs%20significant%20costs%0Aassociated%20with%20advanced%20model%20usage%20and%20often%20results%20in%20data%20compatibility%0Aissues%2C%20led%20by%20the%20high%20discrepancy%20in%20the%20knowledge%20scope%20between%20the%20advanced%0Amodel%20and%20the%20target%20model.%20To%20address%20these%20challenges%2C%20we%20propose%0AToolACE-DEV%2C%20a%20self-improving%20framework%20for%20tool%20learning.%20First%2C%20we%20decompose%0Athe%20tool-learning%20objective%20into%20sub-tasks%20that%20enhance%20basic%20tool-making%20and%0Atool-using%20abilities.%20Then%2C%20we%20introduce%20a%20self-evolving%20paradigm%20that%20allows%0Alightweight%20models%20to%20self-improve%2C%20reducing%20reliance%20on%20advanced%20LLMs.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%20across%20models%0Aof%20varying%20scales%20and%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToolACE-DEV%253A%2520Self-Improving%2520Tool%2520Learning%2520via%2520Decomposition%2520and%250A%2520%2520EVolution%26entry.906535625%3DXu%2520Huang%2520and%2520Weiwen%2520Liu%2520and%2520Xingshan%2520Zeng%2520and%2520Yuefeng%2520Huang%2520and%2520Xinlong%2520Hao%2520and%2520Yuxian%2520Wang%2520and%2520Yirong%2520Zeng%2520and%2520Chuhan%2520Wu%2520and%2520Yasheng%2520Wang%2520and%2520Ruiming%2520Tang%2520and%2520Defu%2520Lian%26entry.1292438233%3D%2520%2520The%2520tool-using%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520enables%2520them%2520to%250Aaccess%2520up-to-date%2520external%2520information%2520and%2520handle%2520complex%2520tasks.%2520Current%250Aapproaches%2520to%2520enhancing%2520this%2520capability%2520primarily%2520rely%2520on%2520distilling%2520advanced%250Amodels%2520by%2520data%2520synthesis.%2520However%252C%2520this%2520method%2520incurs%2520significant%2520costs%250Aassociated%2520with%2520advanced%2520model%2520usage%2520and%2520often%2520results%2520in%2520data%2520compatibility%250Aissues%252C%2520led%2520by%2520the%2520high%2520discrepancy%2520in%2520the%2520knowledge%2520scope%2520between%2520the%2520advanced%250Amodel%2520and%2520the%2520target%2520model.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AToolACE-DEV%252C%2520a%2520self-improving%2520framework%2520for%2520tool%2520learning.%2520First%252C%2520we%2520decompose%250Athe%2520tool-learning%2520objective%2520into%2520sub-tasks%2520that%2520enhance%2520basic%2520tool-making%2520and%250Atool-using%2520abilities.%2520Then%252C%2520we%2520introduce%2520a%2520self-evolving%2520paradigm%2520that%2520allows%250Alightweight%2520models%2520to%2520self-improve%252C%2520reducing%2520reliance%2520on%2520advanced%2520LLMs.%250AExtensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520across%2520models%250Aof%2520varying%2520scales%2520and%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolACE-DEV%3A%20Self-Improving%20Tool%20Learning%20via%20Decomposition%20and%0A%20%20EVolution&entry.906535625=Xu%20Huang%20and%20Weiwen%20Liu%20and%20Xingshan%20Zeng%20and%20Yuefeng%20Huang%20and%20Xinlong%20Hao%20and%20Yuxian%20Wang%20and%20Yirong%20Zeng%20and%20Chuhan%20Wu%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%20and%20Defu%20Lian&entry.1292438233=%20%20The%20tool-using%20capability%20of%20large%20language%20models%20%28LLMs%29%20enables%20them%20to%0Aaccess%20up-to-date%20external%20information%20and%20handle%20complex%20tasks.%20Current%0Aapproaches%20to%20enhancing%20this%20capability%20primarily%20rely%20on%20distilling%20advanced%0Amodels%20by%20data%20synthesis.%20However%2C%20this%20method%20incurs%20significant%20costs%0Aassociated%20with%20advanced%20model%20usage%20and%20often%20results%20in%20data%20compatibility%0Aissues%2C%20led%20by%20the%20high%20discrepancy%20in%20the%20knowledge%20scope%20between%20the%20advanced%0Amodel%20and%20the%20target%20model.%20To%20address%20these%20challenges%2C%20we%20propose%0AToolACE-DEV%2C%20a%20self-improving%20framework%20for%20tool%20learning.%20First%2C%20we%20decompose%0Athe%20tool-learning%20objective%20into%20sub-tasks%20that%20enhance%20basic%20tool-making%20and%0Atool-using%20abilities.%20Then%2C%20we%20introduce%20a%20self-evolving%20paradigm%20that%20allows%0Alightweight%20models%20to%20self-improve%2C%20reducing%20reliance%20on%20advanced%20LLMs.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%20across%20models%0Aof%20varying%20scales%20and%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07512v1&entry.124074799=Read"},
{"title": "A constraints-based approach to fully interpretable neural networks for\n  detecting learner behaviors", "author": "Juan D. Pinto and Luc Paquette", "abstract": "  The increasing use of complex machine learning models in education has led to\nconcerns about their interpretability, which in turn has spurred interest in\ndeveloping explainability techniques that are both faithful to the model's\ninner workings and intelligible to human end-users. In this paper, we describe\na novel approach to creating a neural-network-based behavior detection model\nthat is interpretable by design. Our model is fully interpretable, meaning that\nthe parameters we extract for our explanations have a clear interpretation,\nfully capture the model's learned knowledge about the learner behavior of\ninterest, and can be used to create explanations that are both faithful and\nintelligible. We achieve this by implementing a series of constraints to the\nmodel that both simplify its inference process and bring it closer to a human\nconception of the task at hand. We train the model to detect gaming-the-system\nbehavior, evaluate its performance on this task, and compare its learned\npatterns to those identified by human experts. Our results show that the model\nis successfully able to learn patterns indicative of gaming-the-system behavior\nwhile providing evidence for fully interpretable explanations. We discuss the\nimplications of our approach and suggest ways to evaluate explainability using\na human-grounded approach.\n", "link": "http://arxiv.org/abs/2504.20055v2", "date": "2025-05-12", "relevancy": 2.0589, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20constraints-based%20approach%20to%20fully%20interpretable%20neural%20networks%20for%0A%20%20detecting%20learner%20behaviors&body=Title%3A%20A%20constraints-based%20approach%20to%20fully%20interpretable%20neural%20networks%20for%0A%20%20detecting%20learner%20behaviors%0AAuthor%3A%20Juan%20D.%20Pinto%20and%20Luc%20Paquette%0AAbstract%3A%20%20%20The%20increasing%20use%20of%20complex%20machine%20learning%20models%20in%20education%20has%20led%20to%0Aconcerns%20about%20their%20interpretability%2C%20which%20in%20turn%20has%20spurred%20interest%20in%0Adeveloping%20explainability%20techniques%20that%20are%20both%20faithful%20to%20the%20model%27s%0Ainner%20workings%20and%20intelligible%20to%20human%20end-users.%20In%20this%20paper%2C%20we%20describe%0Aa%20novel%20approach%20to%20creating%20a%20neural-network-based%20behavior%20detection%20model%0Athat%20is%20interpretable%20by%20design.%20Our%20model%20is%20fully%20interpretable%2C%20meaning%20that%0Athe%20parameters%20we%20extract%20for%20our%20explanations%20have%20a%20clear%20interpretation%2C%0Afully%20capture%20the%20model%27s%20learned%20knowledge%20about%20the%20learner%20behavior%20of%0Ainterest%2C%20and%20can%20be%20used%20to%20create%20explanations%20that%20are%20both%20faithful%20and%0Aintelligible.%20We%20achieve%20this%20by%20implementing%20a%20series%20of%20constraints%20to%20the%0Amodel%20that%20both%20simplify%20its%20inference%20process%20and%20bring%20it%20closer%20to%20a%20human%0Aconception%20of%20the%20task%20at%20hand.%20We%20train%20the%20model%20to%20detect%20gaming-the-system%0Abehavior%2C%20evaluate%20its%20performance%20on%20this%20task%2C%20and%20compare%20its%20learned%0Apatterns%20to%20those%20identified%20by%20human%20experts.%20Our%20results%20show%20that%20the%20model%0Ais%20successfully%20able%20to%20learn%20patterns%20indicative%20of%20gaming-the-system%20behavior%0Awhile%20providing%20evidence%20for%20fully%20interpretable%20explanations.%20We%20discuss%20the%0Aimplications%20of%20our%20approach%20and%20suggest%20ways%20to%20evaluate%20explainability%20using%0Aa%20human-grounded%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520constraints-based%2520approach%2520to%2520fully%2520interpretable%2520neural%2520networks%2520for%250A%2520%2520detecting%2520learner%2520behaviors%26entry.906535625%3DJuan%2520D.%2520Pinto%2520and%2520Luc%2520Paquette%26entry.1292438233%3D%2520%2520The%2520increasing%2520use%2520of%2520complex%2520machine%2520learning%2520models%2520in%2520education%2520has%2520led%2520to%250Aconcerns%2520about%2520their%2520interpretability%252C%2520which%2520in%2520turn%2520has%2520spurred%2520interest%2520in%250Adeveloping%2520explainability%2520techniques%2520that%2520are%2520both%2520faithful%2520to%2520the%2520model%2527s%250Ainner%2520workings%2520and%2520intelligible%2520to%2520human%2520end-users.%2520In%2520this%2520paper%252C%2520we%2520describe%250Aa%2520novel%2520approach%2520to%2520creating%2520a%2520neural-network-based%2520behavior%2520detection%2520model%250Athat%2520is%2520interpretable%2520by%2520design.%2520Our%2520model%2520is%2520fully%2520interpretable%252C%2520meaning%2520that%250Athe%2520parameters%2520we%2520extract%2520for%2520our%2520explanations%2520have%2520a%2520clear%2520interpretation%252C%250Afully%2520capture%2520the%2520model%2527s%2520learned%2520knowledge%2520about%2520the%2520learner%2520behavior%2520of%250Ainterest%252C%2520and%2520can%2520be%2520used%2520to%2520create%2520explanations%2520that%2520are%2520both%2520faithful%2520and%250Aintelligible.%2520We%2520achieve%2520this%2520by%2520implementing%2520a%2520series%2520of%2520constraints%2520to%2520the%250Amodel%2520that%2520both%2520simplify%2520its%2520inference%2520process%2520and%2520bring%2520it%2520closer%2520to%2520a%2520human%250Aconception%2520of%2520the%2520task%2520at%2520hand.%2520We%2520train%2520the%2520model%2520to%2520detect%2520gaming-the-system%250Abehavior%252C%2520evaluate%2520its%2520performance%2520on%2520this%2520task%252C%2520and%2520compare%2520its%2520learned%250Apatterns%2520to%2520those%2520identified%2520by%2520human%2520experts.%2520Our%2520results%2520show%2520that%2520the%2520model%250Ais%2520successfully%2520able%2520to%2520learn%2520patterns%2520indicative%2520of%2520gaming-the-system%2520behavior%250Awhile%2520providing%2520evidence%2520for%2520fully%2520interpretable%2520explanations.%2520We%2520discuss%2520the%250Aimplications%2520of%2520our%2520approach%2520and%2520suggest%2520ways%2520to%2520evaluate%2520explainability%2520using%250Aa%2520human-grounded%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20constraints-based%20approach%20to%20fully%20interpretable%20neural%20networks%20for%0A%20%20detecting%20learner%20behaviors&entry.906535625=Juan%20D.%20Pinto%20and%20Luc%20Paquette&entry.1292438233=%20%20The%20increasing%20use%20of%20complex%20machine%20learning%20models%20in%20education%20has%20led%20to%0Aconcerns%20about%20their%20interpretability%2C%20which%20in%20turn%20has%20spurred%20interest%20in%0Adeveloping%20explainability%20techniques%20that%20are%20both%20faithful%20to%20the%20model%27s%0Ainner%20workings%20and%20intelligible%20to%20human%20end-users.%20In%20this%20paper%2C%20we%20describe%0Aa%20novel%20approach%20to%20creating%20a%20neural-network-based%20behavior%20detection%20model%0Athat%20is%20interpretable%20by%20design.%20Our%20model%20is%20fully%20interpretable%2C%20meaning%20that%0Athe%20parameters%20we%20extract%20for%20our%20explanations%20have%20a%20clear%20interpretation%2C%0Afully%20capture%20the%20model%27s%20learned%20knowledge%20about%20the%20learner%20behavior%20of%0Ainterest%2C%20and%20can%20be%20used%20to%20create%20explanations%20that%20are%20both%20faithful%20and%0Aintelligible.%20We%20achieve%20this%20by%20implementing%20a%20series%20of%20constraints%20to%20the%0Amodel%20that%20both%20simplify%20its%20inference%20process%20and%20bring%20it%20closer%20to%20a%20human%0Aconception%20of%20the%20task%20at%20hand.%20We%20train%20the%20model%20to%20detect%20gaming-the-system%0Abehavior%2C%20evaluate%20its%20performance%20on%20this%20task%2C%20and%20compare%20its%20learned%0Apatterns%20to%20those%20identified%20by%20human%20experts.%20Our%20results%20show%20that%20the%20model%0Ais%20successfully%20able%20to%20learn%20patterns%20indicative%20of%20gaming-the-system%20behavior%0Awhile%20providing%20evidence%20for%20fully%20interpretable%20explanations.%20We%20discuss%20the%0Aimplications%20of%20our%20approach%20and%20suggest%20ways%20to%20evaluate%20explainability%20using%0Aa%20human-grounded%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20055v2&entry.124074799=Read"},
{"title": "Feedback-Driven Pseudo-Label Reliability Assessment: Redefining\n  Thresholding for Semi-Supervised Semantic Segmentation", "author": "Negin Ghamsarian and Sahar Nasirihaghighi and Klaus Schoeffmann and Raphael Sznitman", "abstract": "  Semi-supervised learning leverages unlabeled data to enhance model\nperformance, addressing the limitations of fully supervised approaches. Among\nits strategies, pseudo-supervision has proven highly effective, typically\nrelying on one or multiple teacher networks to refine pseudo-labels before\ntraining a student network. A common practice in pseudo-supervision is\nfiltering pseudo-labels based on pre-defined confidence thresholds or entropy.\nHowever, selecting optimal thresholds requires large labeled datasets, which\nare often scarce in real-world semi-supervised scenarios. To overcome this\nchallenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic\nfeedback-driven thresholding strategy for pseudo-label selection. Instead of\nrelying on static confidence thresholds, ENCORE estimates class-wise\ntrue-positive confidence within the unlabeled dataset and continuously adjusts\nthresholds based on the model's response to different levels of pseudo-label\nfiltering. This feedback-driven mechanism ensures the retention of informative\npseudo-labels while filtering unreliable ones, enhancing model training without\nmanual threshold tuning. Our method seamlessly integrates into existing\npseudo-supervision frameworks and significantly improves segmentation\nperformance, particularly in data-scarce conditions. Extensive experiments\ndemonstrate that integrating ENCORE with existing pseudo-supervision frameworks\nenhances performance across multiple datasets and network architectures,\nvalidating its effectiveness in semi-supervised learning.\n", "link": "http://arxiv.org/abs/2505.07691v1", "date": "2025-05-12", "relevancy": 2.0576, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5767}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedback-Driven%20Pseudo-Label%20Reliability%20Assessment%3A%20Redefining%0A%20%20Thresholding%20for%20Semi-Supervised%20Semantic%20Segmentation&body=Title%3A%20Feedback-Driven%20Pseudo-Label%20Reliability%20Assessment%3A%20Redefining%0A%20%20Thresholding%20for%20Semi-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Negin%20Ghamsarian%20and%20Sahar%20Nasirihaghighi%20and%20Klaus%20Schoeffmann%20and%20Raphael%20Sznitman%0AAbstract%3A%20%20%20Semi-supervised%20learning%20leverages%20unlabeled%20data%20to%20enhance%20model%0Aperformance%2C%20addressing%20the%20limitations%20of%20fully%20supervised%20approaches.%20Among%0Aits%20strategies%2C%20pseudo-supervision%20has%20proven%20highly%20effective%2C%20typically%0Arelying%20on%20one%20or%20multiple%20teacher%20networks%20to%20refine%20pseudo-labels%20before%0Atraining%20a%20student%20network.%20A%20common%20practice%20in%20pseudo-supervision%20is%0Afiltering%20pseudo-labels%20based%20on%20pre-defined%20confidence%20thresholds%20or%20entropy.%0AHowever%2C%20selecting%20optimal%20thresholds%20requires%20large%20labeled%20datasets%2C%20which%0Aare%20often%20scarce%20in%20real-world%20semi-supervised%20scenarios.%20To%20overcome%20this%0Achallenge%2C%20we%20propose%20Ensemble-of-Confidence%20Reinforcement%20%28ENCORE%29%2C%20a%20dynamic%0Afeedback-driven%20thresholding%20strategy%20for%20pseudo-label%20selection.%20Instead%20of%0Arelying%20on%20static%20confidence%20thresholds%2C%20ENCORE%20estimates%20class-wise%0Atrue-positive%20confidence%20within%20the%20unlabeled%20dataset%20and%20continuously%20adjusts%0Athresholds%20based%20on%20the%20model%27s%20response%20to%20different%20levels%20of%20pseudo-label%0Afiltering.%20This%20feedback-driven%20mechanism%20ensures%20the%20retention%20of%20informative%0Apseudo-labels%20while%20filtering%20unreliable%20ones%2C%20enhancing%20model%20training%20without%0Amanual%20threshold%20tuning.%20Our%20method%20seamlessly%20integrates%20into%20existing%0Apseudo-supervision%20frameworks%20and%20significantly%20improves%20segmentation%0Aperformance%2C%20particularly%20in%20data-scarce%20conditions.%20Extensive%20experiments%0Ademonstrate%20that%20integrating%20ENCORE%20with%20existing%20pseudo-supervision%20frameworks%0Aenhances%20performance%20across%20multiple%20datasets%20and%20network%20architectures%2C%0Avalidating%20its%20effectiveness%20in%20semi-supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedback-Driven%2520Pseudo-Label%2520Reliability%2520Assessment%253A%2520Redefining%250A%2520%2520Thresholding%2520for%2520Semi-Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DNegin%2520Ghamsarian%2520and%2520Sahar%2520Nasirihaghighi%2520and%2520Klaus%2520Schoeffmann%2520and%2520Raphael%2520Sznitman%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520leverages%2520unlabeled%2520data%2520to%2520enhance%2520model%250Aperformance%252C%2520addressing%2520the%2520limitations%2520of%2520fully%2520supervised%2520approaches.%2520Among%250Aits%2520strategies%252C%2520pseudo-supervision%2520has%2520proven%2520highly%2520effective%252C%2520typically%250Arelying%2520on%2520one%2520or%2520multiple%2520teacher%2520networks%2520to%2520refine%2520pseudo-labels%2520before%250Atraining%2520a%2520student%2520network.%2520A%2520common%2520practice%2520in%2520pseudo-supervision%2520is%250Afiltering%2520pseudo-labels%2520based%2520on%2520pre-defined%2520confidence%2520thresholds%2520or%2520entropy.%250AHowever%252C%2520selecting%2520optimal%2520thresholds%2520requires%2520large%2520labeled%2520datasets%252C%2520which%250Aare%2520often%2520scarce%2520in%2520real-world%2520semi-supervised%2520scenarios.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520propose%2520Ensemble-of-Confidence%2520Reinforcement%2520%2528ENCORE%2529%252C%2520a%2520dynamic%250Afeedback-driven%2520thresholding%2520strategy%2520for%2520pseudo-label%2520selection.%2520Instead%2520of%250Arelying%2520on%2520static%2520confidence%2520thresholds%252C%2520ENCORE%2520estimates%2520class-wise%250Atrue-positive%2520confidence%2520within%2520the%2520unlabeled%2520dataset%2520and%2520continuously%2520adjusts%250Athresholds%2520based%2520on%2520the%2520model%2527s%2520response%2520to%2520different%2520levels%2520of%2520pseudo-label%250Afiltering.%2520This%2520feedback-driven%2520mechanism%2520ensures%2520the%2520retention%2520of%2520informative%250Apseudo-labels%2520while%2520filtering%2520unreliable%2520ones%252C%2520enhancing%2520model%2520training%2520without%250Amanual%2520threshold%2520tuning.%2520Our%2520method%2520seamlessly%2520integrates%2520into%2520existing%250Apseudo-supervision%2520frameworks%2520and%2520significantly%2520improves%2520segmentation%250Aperformance%252C%2520particularly%2520in%2520data-scarce%2520conditions.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520integrating%2520ENCORE%2520with%2520existing%2520pseudo-supervision%2520frameworks%250Aenhances%2520performance%2520across%2520multiple%2520datasets%2520and%2520network%2520architectures%252C%250Avalidating%2520its%2520effectiveness%2520in%2520semi-supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedback-Driven%20Pseudo-Label%20Reliability%20Assessment%3A%20Redefining%0A%20%20Thresholding%20for%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Negin%20Ghamsarian%20and%20Sahar%20Nasirihaghighi%20and%20Klaus%20Schoeffmann%20and%20Raphael%20Sznitman&entry.1292438233=%20%20Semi-supervised%20learning%20leverages%20unlabeled%20data%20to%20enhance%20model%0Aperformance%2C%20addressing%20the%20limitations%20of%20fully%20supervised%20approaches.%20Among%0Aits%20strategies%2C%20pseudo-supervision%20has%20proven%20highly%20effective%2C%20typically%0Arelying%20on%20one%20or%20multiple%20teacher%20networks%20to%20refine%20pseudo-labels%20before%0Atraining%20a%20student%20network.%20A%20common%20practice%20in%20pseudo-supervision%20is%0Afiltering%20pseudo-labels%20based%20on%20pre-defined%20confidence%20thresholds%20or%20entropy.%0AHowever%2C%20selecting%20optimal%20thresholds%20requires%20large%20labeled%20datasets%2C%20which%0Aare%20often%20scarce%20in%20real-world%20semi-supervised%20scenarios.%20To%20overcome%20this%0Achallenge%2C%20we%20propose%20Ensemble-of-Confidence%20Reinforcement%20%28ENCORE%29%2C%20a%20dynamic%0Afeedback-driven%20thresholding%20strategy%20for%20pseudo-label%20selection.%20Instead%20of%0Arelying%20on%20static%20confidence%20thresholds%2C%20ENCORE%20estimates%20class-wise%0Atrue-positive%20confidence%20within%20the%20unlabeled%20dataset%20and%20continuously%20adjusts%0Athresholds%20based%20on%20the%20model%27s%20response%20to%20different%20levels%20of%20pseudo-label%0Afiltering.%20This%20feedback-driven%20mechanism%20ensures%20the%20retention%20of%20informative%0Apseudo-labels%20while%20filtering%20unreliable%20ones%2C%20enhancing%20model%20training%20without%0Amanual%20threshold%20tuning.%20Our%20method%20seamlessly%20integrates%20into%20existing%0Apseudo-supervision%20frameworks%20and%20significantly%20improves%20segmentation%0Aperformance%2C%20particularly%20in%20data-scarce%20conditions.%20Extensive%20experiments%0Ademonstrate%20that%20integrating%20ENCORE%20with%20existing%20pseudo-supervision%20frameworks%0Aenhances%20performance%20across%20multiple%20datasets%20and%20network%20architectures%2C%0Avalidating%20its%20effectiveness%20in%20semi-supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07691v1&entry.124074799=Read"},
{"title": "Leveraging Gradients for Unsupervised Accuracy Estimation under\n  Distribution Shift", "author": "Renchunzi Xie and Ambroise Odonnat and Vasilii Feofanov and Ievgen Redko and Jianfeng Zhang and Bo An", "abstract": "  Estimating the test performance of a model, possibly under distribution\nshift, without having access to the ground-truth labels is a challenging, yet\nvery important problem for the safe deployment of machine learning algorithms\nin the wild. Existing works mostly rely on information from either the outputs\nor the extracted features of neural networks to estimate a score that\ncorrelates with the ground-truth test accuracy. In this paper, we investigate\n-- both empirically and theoretically -- how the information provided by the\ngradients can be predictive of the ground-truth test accuracy even under\ndistribution shifts. More specifically, we use the norm of classification-layer\ngradients, backpropagated from the cross-entropy loss after only one gradient\nstep over test data. Our intuition is that these gradients should be of higher\nmagnitude when the model generalizes poorly. We provide the theoretical\ninsights behind our approach and the key ingredients that ensure its empirical\nsuccess. Extensive experiments conducted with various architectures on diverse\ndistribution shifts demonstrate that our method significantly outperforms\ncurrent state-of-the-art approaches. The code is available at\nhttps://github.com/Renchunzi-Xie/GdScore\n", "link": "http://arxiv.org/abs/2401.08909v3", "date": "2025-05-12", "relevancy": 2.0541, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5375}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4995}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Gradients%20for%20Unsupervised%20Accuracy%20Estimation%20under%0A%20%20Distribution%20Shift&body=Title%3A%20Leveraging%20Gradients%20for%20Unsupervised%20Accuracy%20Estimation%20under%0A%20%20Distribution%20Shift%0AAuthor%3A%20Renchunzi%20Xie%20and%20Ambroise%20Odonnat%20and%20Vasilii%20Feofanov%20and%20Ievgen%20Redko%20and%20Jianfeng%20Zhang%20and%20Bo%20An%0AAbstract%3A%20%20%20Estimating%20the%20test%20performance%20of%20a%20model%2C%20possibly%20under%20distribution%0Ashift%2C%20without%20having%20access%20to%20the%20ground-truth%20labels%20is%20a%20challenging%2C%20yet%0Avery%20important%20problem%20for%20the%20safe%20deployment%20of%20machine%20learning%20algorithms%0Ain%20the%20wild.%20Existing%20works%20mostly%20rely%20on%20information%20from%20either%20the%20outputs%0Aor%20the%20extracted%20features%20of%20neural%20networks%20to%20estimate%20a%20score%20that%0Acorrelates%20with%20the%20ground-truth%20test%20accuracy.%20In%20this%20paper%2C%20we%20investigate%0A--%20both%20empirically%20and%20theoretically%20--%20how%20the%20information%20provided%20by%20the%0Agradients%20can%20be%20predictive%20of%20the%20ground-truth%20test%20accuracy%20even%20under%0Adistribution%20shifts.%20More%20specifically%2C%20we%20use%20the%20norm%20of%20classification-layer%0Agradients%2C%20backpropagated%20from%20the%20cross-entropy%20loss%20after%20only%20one%20gradient%0Astep%20over%20test%20data.%20Our%20intuition%20is%20that%20these%20gradients%20should%20be%20of%20higher%0Amagnitude%20when%20the%20model%20generalizes%20poorly.%20We%20provide%20the%20theoretical%0Ainsights%20behind%20our%20approach%20and%20the%20key%20ingredients%20that%20ensure%20its%20empirical%0Asuccess.%20Extensive%20experiments%20conducted%20with%20various%20architectures%20on%20diverse%0Adistribution%20shifts%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Acurrent%20state-of-the-art%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Renchunzi-Xie/GdScore%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08909v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Gradients%2520for%2520Unsupervised%2520Accuracy%2520Estimation%2520under%250A%2520%2520Distribution%2520Shift%26entry.906535625%3DRenchunzi%2520Xie%2520and%2520Ambroise%2520Odonnat%2520and%2520Vasilii%2520Feofanov%2520and%2520Ievgen%2520Redko%2520and%2520Jianfeng%2520Zhang%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Estimating%2520the%2520test%2520performance%2520of%2520a%2520model%252C%2520possibly%2520under%2520distribution%250Ashift%252C%2520without%2520having%2520access%2520to%2520the%2520ground-truth%2520labels%2520is%2520a%2520challenging%252C%2520yet%250Avery%2520important%2520problem%2520for%2520the%2520safe%2520deployment%2520of%2520machine%2520learning%2520algorithms%250Ain%2520the%2520wild.%2520Existing%2520works%2520mostly%2520rely%2520on%2520information%2520from%2520either%2520the%2520outputs%250Aor%2520the%2520extracted%2520features%2520of%2520neural%2520networks%2520to%2520estimate%2520a%2520score%2520that%250Acorrelates%2520with%2520the%2520ground-truth%2520test%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520investigate%250A--%2520both%2520empirically%2520and%2520theoretically%2520--%2520how%2520the%2520information%2520provided%2520by%2520the%250Agradients%2520can%2520be%2520predictive%2520of%2520the%2520ground-truth%2520test%2520accuracy%2520even%2520under%250Adistribution%2520shifts.%2520More%2520specifically%252C%2520we%2520use%2520the%2520norm%2520of%2520classification-layer%250Agradients%252C%2520backpropagated%2520from%2520the%2520cross-entropy%2520loss%2520after%2520only%2520one%2520gradient%250Astep%2520over%2520test%2520data.%2520Our%2520intuition%2520is%2520that%2520these%2520gradients%2520should%2520be%2520of%2520higher%250Amagnitude%2520when%2520the%2520model%2520generalizes%2520poorly.%2520We%2520provide%2520the%2520theoretical%250Ainsights%2520behind%2520our%2520approach%2520and%2520the%2520key%2520ingredients%2520that%2520ensure%2520its%2520empirical%250Asuccess.%2520Extensive%2520experiments%2520conducted%2520with%2520various%2520architectures%2520on%2520diverse%250Adistribution%2520shifts%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Acurrent%2520state-of-the-art%2520approaches.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Renchunzi-Xie/GdScore%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08909v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Gradients%20for%20Unsupervised%20Accuracy%20Estimation%20under%0A%20%20Distribution%20Shift&entry.906535625=Renchunzi%20Xie%20and%20Ambroise%20Odonnat%20and%20Vasilii%20Feofanov%20and%20Ievgen%20Redko%20and%20Jianfeng%20Zhang%20and%20Bo%20An&entry.1292438233=%20%20Estimating%20the%20test%20performance%20of%20a%20model%2C%20possibly%20under%20distribution%0Ashift%2C%20without%20having%20access%20to%20the%20ground-truth%20labels%20is%20a%20challenging%2C%20yet%0Avery%20important%20problem%20for%20the%20safe%20deployment%20of%20machine%20learning%20algorithms%0Ain%20the%20wild.%20Existing%20works%20mostly%20rely%20on%20information%20from%20either%20the%20outputs%0Aor%20the%20extracted%20features%20of%20neural%20networks%20to%20estimate%20a%20score%20that%0Acorrelates%20with%20the%20ground-truth%20test%20accuracy.%20In%20this%20paper%2C%20we%20investigate%0A--%20both%20empirically%20and%20theoretically%20--%20how%20the%20information%20provided%20by%20the%0Agradients%20can%20be%20predictive%20of%20the%20ground-truth%20test%20accuracy%20even%20under%0Adistribution%20shifts.%20More%20specifically%2C%20we%20use%20the%20norm%20of%20classification-layer%0Agradients%2C%20backpropagated%20from%20the%20cross-entropy%20loss%20after%20only%20one%20gradient%0Astep%20over%20test%20data.%20Our%20intuition%20is%20that%20these%20gradients%20should%20be%20of%20higher%0Amagnitude%20when%20the%20model%20generalizes%20poorly.%20We%20provide%20the%20theoretical%0Ainsights%20behind%20our%20approach%20and%20the%20key%20ingredients%20that%20ensure%20its%20empirical%0Asuccess.%20Extensive%20experiments%20conducted%20with%20various%20architectures%20on%20diverse%0Adistribution%20shifts%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Acurrent%20state-of-the-art%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Renchunzi-Xie/GdScore%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08909v3&entry.124074799=Read"},
{"title": "LightSNN: Lightweight Architecture Search for Sparse and Accurate\n  Spiking Neural Networks", "author": "Yesmine Abdennadher and Giovanni Perin and Riccardo Mazzieri and Jacopo Pegoraro and Michele Rossi", "abstract": "  Spiking Neural Networks (SNNs) are highly regarded for their energy\nefficiency, inherent activation sparsity, and suitability for real-time\nprocessing in edge devices. However, most current SNN methods adopt\narchitectures resembling traditional artificial neural networks (ANNs), leading\nto suboptimal performance when applied to SNNs. While SNNs excel in energy\nefficiency, they have been associated with lower accuracy levels than\ntraditional ANNs when utilizing conventional architectures. In response, in\nthis work we present LightSNN, a rapid and efficient Neural Network\nArchitecture Search (NAS) technique specifically tailored for SNNs that\nautonomously leverages the most suitable architecture, striking a good balance\nbetween accuracy and efficiency by enforcing sparsity. Based on the spiking NAS\nnetwork (SNASNet) framework, a cell-based search space including backward\nconnections is utilized to build our training-free pruning-based NAS mechanism.\nOur technique assesses diverse spike activation patterns across different data\nsamples using a sparsity-aware Hamming distance fitness evaluation. Thorough\nexperiments are conducted on both static (CIFAR10 and CIFAR100) and\nneuromorphic datasets (DVS128-Gesture). Our LightSNN model achieves\nstate-of-the-art results on CIFAR10 and CIFAR100, improves performance on\nDVS128Gesture by 4.49\\%, and significantly reduces search time most notably\noffering a $98\\times$ speedup over SNASNet and running 30\\% faster than the\nbest existing method on DVS128Gesture. Code is available on Github at:\nhttps://github.com/YesmineAbdennadher/LightSNN.\n", "link": "http://arxiv.org/abs/2503.21846v2", "date": "2025-05-12", "relevancy": 2.0436, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5197}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5187}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightSNN%3A%20Lightweight%20Architecture%20Search%20for%20Sparse%20and%20Accurate%0A%20%20Spiking%20Neural%20Networks&body=Title%3A%20LightSNN%3A%20Lightweight%20Architecture%20Search%20for%20Sparse%20and%20Accurate%0A%20%20Spiking%20Neural%20Networks%0AAuthor%3A%20Yesmine%20Abdennadher%20and%20Giovanni%20Perin%20and%20Riccardo%20Mazzieri%20and%20Jacopo%20Pegoraro%20and%20Michele%20Rossi%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20highly%20regarded%20for%20their%20energy%0Aefficiency%2C%20inherent%20activation%20sparsity%2C%20and%20suitability%20for%20real-time%0Aprocessing%20in%20edge%20devices.%20However%2C%20most%20current%20SNN%20methods%20adopt%0Aarchitectures%20resembling%20traditional%20artificial%20neural%20networks%20%28ANNs%29%2C%20leading%0Ato%20suboptimal%20performance%20when%20applied%20to%20SNNs.%20While%20SNNs%20excel%20in%20energy%0Aefficiency%2C%20they%20have%20been%20associated%20with%20lower%20accuracy%20levels%20than%0Atraditional%20ANNs%20when%20utilizing%20conventional%20architectures.%20In%20response%2C%20in%0Athis%20work%20we%20present%20LightSNN%2C%20a%20rapid%20and%20efficient%20Neural%20Network%0AArchitecture%20Search%20%28NAS%29%20technique%20specifically%20tailored%20for%20SNNs%20that%0Aautonomously%20leverages%20the%20most%20suitable%20architecture%2C%20striking%20a%20good%20balance%0Abetween%20accuracy%20and%20efficiency%20by%20enforcing%20sparsity.%20Based%20on%20the%20spiking%20NAS%0Anetwork%20%28SNASNet%29%20framework%2C%20a%20cell-based%20search%20space%20including%20backward%0Aconnections%20is%20utilized%20to%20build%20our%20training-free%20pruning-based%20NAS%20mechanism.%0AOur%20technique%20assesses%20diverse%20spike%20activation%20patterns%20across%20different%20data%0Asamples%20using%20a%20sparsity-aware%20Hamming%20distance%20fitness%20evaluation.%20Thorough%0Aexperiments%20are%20conducted%20on%20both%20static%20%28CIFAR10%20and%20CIFAR100%29%20and%0Aneuromorphic%20datasets%20%28DVS128-Gesture%29.%20Our%20LightSNN%20model%20achieves%0Astate-of-the-art%20results%20on%20CIFAR10%20and%20CIFAR100%2C%20improves%20performance%20on%0ADVS128Gesture%20by%204.49%5C%25%2C%20and%20significantly%20reduces%20search%20time%20most%20notably%0Aoffering%20a%20%2498%5Ctimes%24%20speedup%20over%20SNASNet%20and%20running%2030%5C%25%20faster%20than%20the%0Abest%20existing%20method%20on%20DVS128Gesture.%20Code%20is%20available%20on%20Github%20at%3A%0Ahttps%3A//github.com/YesmineAbdennadher/LightSNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightSNN%253A%2520Lightweight%2520Architecture%2520Search%2520for%2520Sparse%2520and%2520Accurate%250A%2520%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DYesmine%2520Abdennadher%2520and%2520Giovanni%2520Perin%2520and%2520Riccardo%2520Mazzieri%2520and%2520Jacopo%2520Pegoraro%2520and%2520Michele%2520Rossi%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520highly%2520regarded%2520for%2520their%2520energy%250Aefficiency%252C%2520inherent%2520activation%2520sparsity%252C%2520and%2520suitability%2520for%2520real-time%250Aprocessing%2520in%2520edge%2520devices.%2520However%252C%2520most%2520current%2520SNN%2520methods%2520adopt%250Aarchitectures%2520resembling%2520traditional%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%252C%2520leading%250Ato%2520suboptimal%2520performance%2520when%2520applied%2520to%2520SNNs.%2520While%2520SNNs%2520excel%2520in%2520energy%250Aefficiency%252C%2520they%2520have%2520been%2520associated%2520with%2520lower%2520accuracy%2520levels%2520than%250Atraditional%2520ANNs%2520when%2520utilizing%2520conventional%2520architectures.%2520In%2520response%252C%2520in%250Athis%2520work%2520we%2520present%2520LightSNN%252C%2520a%2520rapid%2520and%2520efficient%2520Neural%2520Network%250AArchitecture%2520Search%2520%2528NAS%2529%2520technique%2520specifically%2520tailored%2520for%2520SNNs%2520that%250Aautonomously%2520leverages%2520the%2520most%2520suitable%2520architecture%252C%2520striking%2520a%2520good%2520balance%250Abetween%2520accuracy%2520and%2520efficiency%2520by%2520enforcing%2520sparsity.%2520Based%2520on%2520the%2520spiking%2520NAS%250Anetwork%2520%2528SNASNet%2529%2520framework%252C%2520a%2520cell-based%2520search%2520space%2520including%2520backward%250Aconnections%2520is%2520utilized%2520to%2520build%2520our%2520training-free%2520pruning-based%2520NAS%2520mechanism.%250AOur%2520technique%2520assesses%2520diverse%2520spike%2520activation%2520patterns%2520across%2520different%2520data%250Asamples%2520using%2520a%2520sparsity-aware%2520Hamming%2520distance%2520fitness%2520evaluation.%2520Thorough%250Aexperiments%2520are%2520conducted%2520on%2520both%2520static%2520%2528CIFAR10%2520and%2520CIFAR100%2529%2520and%250Aneuromorphic%2520datasets%2520%2528DVS128-Gesture%2529.%2520Our%2520LightSNN%2520model%2520achieves%250Astate-of-the-art%2520results%2520on%2520CIFAR10%2520and%2520CIFAR100%252C%2520improves%2520performance%2520on%250ADVS128Gesture%2520by%25204.49%255C%2525%252C%2520and%2520significantly%2520reduces%2520search%2520time%2520most%2520notably%250Aoffering%2520a%2520%252498%255Ctimes%2524%2520speedup%2520over%2520SNASNet%2520and%2520running%252030%255C%2525%2520faster%2520than%2520the%250Abest%2520existing%2520method%2520on%2520DVS128Gesture.%2520Code%2520is%2520available%2520on%2520Github%2520at%253A%250Ahttps%253A//github.com/YesmineAbdennadher/LightSNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightSNN%3A%20Lightweight%20Architecture%20Search%20for%20Sparse%20and%20Accurate%0A%20%20Spiking%20Neural%20Networks&entry.906535625=Yesmine%20Abdennadher%20and%20Giovanni%20Perin%20and%20Riccardo%20Mazzieri%20and%20Jacopo%20Pegoraro%20and%20Michele%20Rossi&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20highly%20regarded%20for%20their%20energy%0Aefficiency%2C%20inherent%20activation%20sparsity%2C%20and%20suitability%20for%20real-time%0Aprocessing%20in%20edge%20devices.%20However%2C%20most%20current%20SNN%20methods%20adopt%0Aarchitectures%20resembling%20traditional%20artificial%20neural%20networks%20%28ANNs%29%2C%20leading%0Ato%20suboptimal%20performance%20when%20applied%20to%20SNNs.%20While%20SNNs%20excel%20in%20energy%0Aefficiency%2C%20they%20have%20been%20associated%20with%20lower%20accuracy%20levels%20than%0Atraditional%20ANNs%20when%20utilizing%20conventional%20architectures.%20In%20response%2C%20in%0Athis%20work%20we%20present%20LightSNN%2C%20a%20rapid%20and%20efficient%20Neural%20Network%0AArchitecture%20Search%20%28NAS%29%20technique%20specifically%20tailored%20for%20SNNs%20that%0Aautonomously%20leverages%20the%20most%20suitable%20architecture%2C%20striking%20a%20good%20balance%0Abetween%20accuracy%20and%20efficiency%20by%20enforcing%20sparsity.%20Based%20on%20the%20spiking%20NAS%0Anetwork%20%28SNASNet%29%20framework%2C%20a%20cell-based%20search%20space%20including%20backward%0Aconnections%20is%20utilized%20to%20build%20our%20training-free%20pruning-based%20NAS%20mechanism.%0AOur%20technique%20assesses%20diverse%20spike%20activation%20patterns%20across%20different%20data%0Asamples%20using%20a%20sparsity-aware%20Hamming%20distance%20fitness%20evaluation.%20Thorough%0Aexperiments%20are%20conducted%20on%20both%20static%20%28CIFAR10%20and%20CIFAR100%29%20and%0Aneuromorphic%20datasets%20%28DVS128-Gesture%29.%20Our%20LightSNN%20model%20achieves%0Astate-of-the-art%20results%20on%20CIFAR10%20and%20CIFAR100%2C%20improves%20performance%20on%0ADVS128Gesture%20by%204.49%5C%25%2C%20and%20significantly%20reduces%20search%20time%20most%20notably%0Aoffering%20a%20%2498%5Ctimes%24%20speedup%20over%20SNASNet%20and%20running%2030%5C%25%20faster%20than%20the%0Abest%20existing%20method%20on%20DVS128Gesture.%20Code%20is%20available%20on%20Github%20at%3A%0Ahttps%3A//github.com/YesmineAbdennadher/LightSNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21846v2&entry.124074799=Read"},
{"title": "MAIS: Memory-Attention for Interactive Segmentation", "author": "Mauricio Orbes-Arteaga and Oeslle Lucena and Sabastien Ourselin and M. Jorge Cardoso", "abstract": "  Interactive medical segmentation reduces annotation effort by refining\npredictions through user feedback. Vision Transformer (ViT)-based models, such\nas the Segment Anything Model (SAM), achieve state-of-the-art performance using\nuser clicks and prior masks as prompts. However, existing methods treat\ninteractions as independent events, leading to redundant corrections and\nlimited refinement gains. We address this by introducing MAIS, a\nMemory-Attention mechanism for Interactive Segmentation that stores past user\ninputs and segmentation states, enabling temporal context integration. Our\napproach enhances ViT-based segmentation across diverse imaging modalities,\nachieving more efficient and accurate refinements.\n", "link": "http://arxiv.org/abs/2505.07511v1", "date": "2025-05-12", "relevancy": 2.0283, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAIS%3A%20Memory-Attention%20for%20Interactive%20Segmentation&body=Title%3A%20MAIS%3A%20Memory-Attention%20for%20Interactive%20Segmentation%0AAuthor%3A%20Mauricio%20Orbes-Arteaga%20and%20Oeslle%20Lucena%20and%20Sabastien%20Ourselin%20and%20M.%20Jorge%20Cardoso%0AAbstract%3A%20%20%20Interactive%20medical%20segmentation%20reduces%20annotation%20effort%20by%20refining%0Apredictions%20through%20user%20feedback.%20Vision%20Transformer%20%28ViT%29-based%20models%2C%20such%0Aas%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20achieve%20state-of-the-art%20performance%20using%0Auser%20clicks%20and%20prior%20masks%20as%20prompts.%20However%2C%20existing%20methods%20treat%0Ainteractions%20as%20independent%20events%2C%20leading%20to%20redundant%20corrections%20and%0Alimited%20refinement%20gains.%20We%20address%20this%20by%20introducing%20MAIS%2C%20a%0AMemory-Attention%20mechanism%20for%20Interactive%20Segmentation%20that%20stores%20past%20user%0Ainputs%20and%20segmentation%20states%2C%20enabling%20temporal%20context%20integration.%20Our%0Aapproach%20enhances%20ViT-based%20segmentation%20across%20diverse%20imaging%20modalities%2C%0Aachieving%20more%20efficient%20and%20accurate%20refinements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAIS%253A%2520Memory-Attention%2520for%2520Interactive%2520Segmentation%26entry.906535625%3DMauricio%2520Orbes-Arteaga%2520and%2520Oeslle%2520Lucena%2520and%2520Sabastien%2520Ourselin%2520and%2520M.%2520Jorge%2520Cardoso%26entry.1292438233%3D%2520%2520Interactive%2520medical%2520segmentation%2520reduces%2520annotation%2520effort%2520by%2520refining%250Apredictions%2520through%2520user%2520feedback.%2520Vision%2520Transformer%2520%2528ViT%2529-based%2520models%252C%2520such%250Aas%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520achieve%2520state-of-the-art%2520performance%2520using%250Auser%2520clicks%2520and%2520prior%2520masks%2520as%2520prompts.%2520However%252C%2520existing%2520methods%2520treat%250Ainteractions%2520as%2520independent%2520events%252C%2520leading%2520to%2520redundant%2520corrections%2520and%250Alimited%2520refinement%2520gains.%2520We%2520address%2520this%2520by%2520introducing%2520MAIS%252C%2520a%250AMemory-Attention%2520mechanism%2520for%2520Interactive%2520Segmentation%2520that%2520stores%2520past%2520user%250Ainputs%2520and%2520segmentation%2520states%252C%2520enabling%2520temporal%2520context%2520integration.%2520Our%250Aapproach%2520enhances%2520ViT-based%2520segmentation%2520across%2520diverse%2520imaging%2520modalities%252C%250Aachieving%2520more%2520efficient%2520and%2520accurate%2520refinements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAIS%3A%20Memory-Attention%20for%20Interactive%20Segmentation&entry.906535625=Mauricio%20Orbes-Arteaga%20and%20Oeslle%20Lucena%20and%20Sabastien%20Ourselin%20and%20M.%20Jorge%20Cardoso&entry.1292438233=%20%20Interactive%20medical%20segmentation%20reduces%20annotation%20effort%20by%20refining%0Apredictions%20through%20user%20feedback.%20Vision%20Transformer%20%28ViT%29-based%20models%2C%20such%0Aas%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20achieve%20state-of-the-art%20performance%20using%0Auser%20clicks%20and%20prior%20masks%20as%20prompts.%20However%2C%20existing%20methods%20treat%0Ainteractions%20as%20independent%20events%2C%20leading%20to%20redundant%20corrections%20and%0Alimited%20refinement%20gains.%20We%20address%20this%20by%20introducing%20MAIS%2C%20a%0AMemory-Attention%20mechanism%20for%20Interactive%20Segmentation%20that%20stores%20past%20user%0Ainputs%20and%20segmentation%20states%2C%20enabling%20temporal%20context%20integration.%20Our%0Aapproach%20enhances%20ViT-based%20segmentation%20across%20diverse%20imaging%20modalities%2C%0Aachieving%20more%20efficient%20and%20accurate%20refinements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07511v1&entry.124074799=Read"},
{"title": "GRADA: Graph-based Reranker against Adversarial Documents Attack", "author": "Jingjie Zheng and Aryo Pradipta Gema and Giwon Hong and Xuanli He and Pasquale Minervini and Youcheng Sun and Qiongkai Xu", "abstract": "  Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy.\n", "link": "http://arxiv.org/abs/2505.07546v1", "date": "2025-05-12", "relevancy": 2.0266, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5047}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRADA%3A%20Graph-based%20Reranker%20against%20Adversarial%20Documents%20Attack&body=Title%3A%20GRADA%3A%20Graph-based%20Reranker%20against%20Adversarial%20Documents%20Attack%0AAuthor%3A%20Jingjie%20Zheng%20and%20Aryo%20Pradipta%20Gema%20and%20Giwon%20Hong%20and%20Xuanli%20He%20and%20Pasquale%20Minervini%20and%20Youcheng%20Sun%20and%20Qiongkai%20Xu%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20frameworks%20improve%20the%20accuracy%20of%20large%0Alanguage%20models%20%28LLMs%29%20by%20integrating%20external%20knowledge%20from%20retrieved%0Adocuments%2C%20thereby%20overcoming%20the%20limitations%20of%20models%27%20static%20intrinsic%0Aknowledge.%20However%2C%20these%20systems%20are%20susceptible%20to%20adversarial%20attacks%20that%0Amanipulate%20the%20retrieval%20process%20by%20introducing%20documents%20that%20are%20adversarial%0Ayet%20semantically%20similar%20to%20the%20query.%20Notably%2C%20while%20these%20adversarial%0Adocuments%20resemble%20the%20query%2C%20they%20exhibit%20weak%20similarity%20to%20benign%20documents%0Ain%20the%20retrieval%20set.%20Thus%2C%20we%20propose%20a%20simple%20yet%20effective%20Graph-based%0AReranking%20against%20Adversarial%20Document%20Attacks%20%28GRADA%29%20framework%20aiming%20at%0Apreserving%20retrieval%20quality%20while%20significantly%20reducing%20the%20success%20of%0Aadversaries.%20Our%20study%20evaluates%20the%20effectiveness%20of%20our%20approach%20through%0Aexperiments%20conducted%20on%20five%20LLMs%3A%20GPT-3.5-Turbo%2C%20GPT-4o%2C%20Llama3.1-8b%2C%0ALlama3.1-70b%2C%20and%20Qwen2.5-7b.%20We%20use%20three%20datasets%20to%20assess%20performance%2C%20with%0Aresults%20from%20the%20Natural%20Questions%20dataset%20demonstrating%20up%20to%20an%2080%25%20reduction%0Ain%20attack%20success%20rates%20while%20maintaining%20minimal%20loss%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRADA%253A%2520Graph-based%2520Reranker%2520against%2520Adversarial%2520Documents%2520Attack%26entry.906535625%3DJingjie%2520Zheng%2520and%2520Aryo%2520Pradipta%2520Gema%2520and%2520Giwon%2520Hong%2520and%2520Xuanli%2520He%2520and%2520Pasquale%2520Minervini%2520and%2520Youcheng%2520Sun%2520and%2520Qiongkai%2520Xu%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520frameworks%2520improve%2520the%2520accuracy%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520by%2520integrating%2520external%2520knowledge%2520from%2520retrieved%250Adocuments%252C%2520thereby%2520overcoming%2520the%2520limitations%2520of%2520models%2527%2520static%2520intrinsic%250Aknowledge.%2520However%252C%2520these%2520systems%2520are%2520susceptible%2520to%2520adversarial%2520attacks%2520that%250Amanipulate%2520the%2520retrieval%2520process%2520by%2520introducing%2520documents%2520that%2520are%2520adversarial%250Ayet%2520semantically%2520similar%2520to%2520the%2520query.%2520Notably%252C%2520while%2520these%2520adversarial%250Adocuments%2520resemble%2520the%2520query%252C%2520they%2520exhibit%2520weak%2520similarity%2520to%2520benign%2520documents%250Ain%2520the%2520retrieval%2520set.%2520Thus%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520Graph-based%250AReranking%2520against%2520Adversarial%2520Document%2520Attacks%2520%2528GRADA%2529%2520framework%2520aiming%2520at%250Apreserving%2520retrieval%2520quality%2520while%2520significantly%2520reducing%2520the%2520success%2520of%250Aadversaries.%2520Our%2520study%2520evaluates%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%250Aexperiments%2520conducted%2520on%2520five%2520LLMs%253A%2520GPT-3.5-Turbo%252C%2520GPT-4o%252C%2520Llama3.1-8b%252C%250ALlama3.1-70b%252C%2520and%2520Qwen2.5-7b.%2520We%2520use%2520three%2520datasets%2520to%2520assess%2520performance%252C%2520with%250Aresults%2520from%2520the%2520Natural%2520Questions%2520dataset%2520demonstrating%2520up%2520to%2520an%252080%2525%2520reduction%250Ain%2520attack%2520success%2520rates%2520while%2520maintaining%2520minimal%2520loss%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRADA%3A%20Graph-based%20Reranker%20against%20Adversarial%20Documents%20Attack&entry.906535625=Jingjie%20Zheng%20and%20Aryo%20Pradipta%20Gema%20and%20Giwon%20Hong%20and%20Xuanli%20He%20and%20Pasquale%20Minervini%20and%20Youcheng%20Sun%20and%20Qiongkai%20Xu&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20frameworks%20improve%20the%20accuracy%20of%20large%0Alanguage%20models%20%28LLMs%29%20by%20integrating%20external%20knowledge%20from%20retrieved%0Adocuments%2C%20thereby%20overcoming%20the%20limitations%20of%20models%27%20static%20intrinsic%0Aknowledge.%20However%2C%20these%20systems%20are%20susceptible%20to%20adversarial%20attacks%20that%0Amanipulate%20the%20retrieval%20process%20by%20introducing%20documents%20that%20are%20adversarial%0Ayet%20semantically%20similar%20to%20the%20query.%20Notably%2C%20while%20these%20adversarial%0Adocuments%20resemble%20the%20query%2C%20they%20exhibit%20weak%20similarity%20to%20benign%20documents%0Ain%20the%20retrieval%20set.%20Thus%2C%20we%20propose%20a%20simple%20yet%20effective%20Graph-based%0AReranking%20against%20Adversarial%20Document%20Attacks%20%28GRADA%29%20framework%20aiming%20at%0Apreserving%20retrieval%20quality%20while%20significantly%20reducing%20the%20success%20of%0Aadversaries.%20Our%20study%20evaluates%20the%20effectiveness%20of%20our%20approach%20through%0Aexperiments%20conducted%20on%20five%20LLMs%3A%20GPT-3.5-Turbo%2C%20GPT-4o%2C%20Llama3.1-8b%2C%0ALlama3.1-70b%2C%20and%20Qwen2.5-7b.%20We%20use%20three%20datasets%20to%20assess%20performance%2C%20with%0Aresults%20from%20the%20Natural%20Questions%20dataset%20demonstrating%20up%20to%20an%2080%25%20reduction%0Ain%20attack%20success%20rates%20while%20maintaining%20minimal%20loss%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07546v1&entry.124074799=Read"},
{"title": "Automatically Differentiable Model Updating (ADiMU): conventional,\n  hybrid, and neural network material model discovery including\n  history-dependency", "author": "Bernardo P. Ferreira and Miguel A. Bessa", "abstract": "  We introduce the first Automatically Differentiable Model Updating (ADiMU)\nframework that finds any history-dependent material model from full-field\ndisplacement and global force data (global, indirect discovery) or from\nstrain-stress data (local, direct discovery). We show that ADiMU can update\nconventional (physics-based), neural network (data-driven), and hybrid material\nmodels. Moreover, this framework requires no fine-tuning of hyperparameters or\nadditional quantities beyond those inherent to the user-selected material model\narchitecture and optimizer. The robustness and versatility of ADiMU is\nextensively exemplified by updating different models spanning tens to millions\nof parameters, in both local and global discovery settings. Relying on fully\ndifferentiable code, the algorithmic implementation leverages vectorizing maps\nthat enable history-dependent automatic differentiation via efficient batched\nexecution of shared computation graphs. This contribution also aims to\nfacilitate the integration, evaluation and application of future material model\narchitectures by openly supporting the research community. Therefore, ADiMU is\nreleased as an open-source computational tool, integrated into a carefully\ndesigned and documented software named HookeAI.\n", "link": "http://arxiv.org/abs/2505.07801v1", "date": "2025-05-12", "relevancy": 2.0131, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5022}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatically%20Differentiable%20Model%20Updating%20%28ADiMU%29%3A%20conventional%2C%0A%20%20hybrid%2C%20and%20neural%20network%20material%20model%20discovery%20including%0A%20%20history-dependency&body=Title%3A%20Automatically%20Differentiable%20Model%20Updating%20%28ADiMU%29%3A%20conventional%2C%0A%20%20hybrid%2C%20and%20neural%20network%20material%20model%20discovery%20including%0A%20%20history-dependency%0AAuthor%3A%20Bernardo%20P.%20Ferreira%20and%20Miguel%20A.%20Bessa%0AAbstract%3A%20%20%20We%20introduce%20the%20first%20Automatically%20Differentiable%20Model%20Updating%20%28ADiMU%29%0Aframework%20that%20finds%20any%20history-dependent%20material%20model%20from%20full-field%0Adisplacement%20and%20global%20force%20data%20%28global%2C%20indirect%20discovery%29%20or%20from%0Astrain-stress%20data%20%28local%2C%20direct%20discovery%29.%20We%20show%20that%20ADiMU%20can%20update%0Aconventional%20%28physics-based%29%2C%20neural%20network%20%28data-driven%29%2C%20and%20hybrid%20material%0Amodels.%20Moreover%2C%20this%20framework%20requires%20no%20fine-tuning%20of%20hyperparameters%20or%0Aadditional%20quantities%20beyond%20those%20inherent%20to%20the%20user-selected%20material%20model%0Aarchitecture%20and%20optimizer.%20The%20robustness%20and%20versatility%20of%20ADiMU%20is%0Aextensively%20exemplified%20by%20updating%20different%20models%20spanning%20tens%20to%20millions%0Aof%20parameters%2C%20in%20both%20local%20and%20global%20discovery%20settings.%20Relying%20on%20fully%0Adifferentiable%20code%2C%20the%20algorithmic%20implementation%20leverages%20vectorizing%20maps%0Athat%20enable%20history-dependent%20automatic%20differentiation%20via%20efficient%20batched%0Aexecution%20of%20shared%20computation%20graphs.%20This%20contribution%20also%20aims%20to%0Afacilitate%20the%20integration%2C%20evaluation%20and%20application%20of%20future%20material%20model%0Aarchitectures%20by%20openly%20supporting%20the%20research%20community.%20Therefore%2C%20ADiMU%20is%0Areleased%20as%20an%20open-source%20computational%20tool%2C%20integrated%20into%20a%20carefully%0Adesigned%20and%20documented%20software%20named%20HookeAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatically%2520Differentiable%2520Model%2520Updating%2520%2528ADiMU%2529%253A%2520conventional%252C%250A%2520%2520hybrid%252C%2520and%2520neural%2520network%2520material%2520model%2520discovery%2520including%250A%2520%2520history-dependency%26entry.906535625%3DBernardo%2520P.%2520Ferreira%2520and%2520Miguel%2520A.%2520Bessa%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520first%2520Automatically%2520Differentiable%2520Model%2520Updating%2520%2528ADiMU%2529%250Aframework%2520that%2520finds%2520any%2520history-dependent%2520material%2520model%2520from%2520full-field%250Adisplacement%2520and%2520global%2520force%2520data%2520%2528global%252C%2520indirect%2520discovery%2529%2520or%2520from%250Astrain-stress%2520data%2520%2528local%252C%2520direct%2520discovery%2529.%2520We%2520show%2520that%2520ADiMU%2520can%2520update%250Aconventional%2520%2528physics-based%2529%252C%2520neural%2520network%2520%2528data-driven%2529%252C%2520and%2520hybrid%2520material%250Amodels.%2520Moreover%252C%2520this%2520framework%2520requires%2520no%2520fine-tuning%2520of%2520hyperparameters%2520or%250Aadditional%2520quantities%2520beyond%2520those%2520inherent%2520to%2520the%2520user-selected%2520material%2520model%250Aarchitecture%2520and%2520optimizer.%2520The%2520robustness%2520and%2520versatility%2520of%2520ADiMU%2520is%250Aextensively%2520exemplified%2520by%2520updating%2520different%2520models%2520spanning%2520tens%2520to%2520millions%250Aof%2520parameters%252C%2520in%2520both%2520local%2520and%2520global%2520discovery%2520settings.%2520Relying%2520on%2520fully%250Adifferentiable%2520code%252C%2520the%2520algorithmic%2520implementation%2520leverages%2520vectorizing%2520maps%250Athat%2520enable%2520history-dependent%2520automatic%2520differentiation%2520via%2520efficient%2520batched%250Aexecution%2520of%2520shared%2520computation%2520graphs.%2520This%2520contribution%2520also%2520aims%2520to%250Afacilitate%2520the%2520integration%252C%2520evaluation%2520and%2520application%2520of%2520future%2520material%2520model%250Aarchitectures%2520by%2520openly%2520supporting%2520the%2520research%2520community.%2520Therefore%252C%2520ADiMU%2520is%250Areleased%2520as%2520an%2520open-source%2520computational%2520tool%252C%2520integrated%2520into%2520a%2520carefully%250Adesigned%2520and%2520documented%2520software%2520named%2520HookeAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatically%20Differentiable%20Model%20Updating%20%28ADiMU%29%3A%20conventional%2C%0A%20%20hybrid%2C%20and%20neural%20network%20material%20model%20discovery%20including%0A%20%20history-dependency&entry.906535625=Bernardo%20P.%20Ferreira%20and%20Miguel%20A.%20Bessa&entry.1292438233=%20%20We%20introduce%20the%20first%20Automatically%20Differentiable%20Model%20Updating%20%28ADiMU%29%0Aframework%20that%20finds%20any%20history-dependent%20material%20model%20from%20full-field%0Adisplacement%20and%20global%20force%20data%20%28global%2C%20indirect%20discovery%29%20or%20from%0Astrain-stress%20data%20%28local%2C%20direct%20discovery%29.%20We%20show%20that%20ADiMU%20can%20update%0Aconventional%20%28physics-based%29%2C%20neural%20network%20%28data-driven%29%2C%20and%20hybrid%20material%0Amodels.%20Moreover%2C%20this%20framework%20requires%20no%20fine-tuning%20of%20hyperparameters%20or%0Aadditional%20quantities%20beyond%20those%20inherent%20to%20the%20user-selected%20material%20model%0Aarchitecture%20and%20optimizer.%20The%20robustness%20and%20versatility%20of%20ADiMU%20is%0Aextensively%20exemplified%20by%20updating%20different%20models%20spanning%20tens%20to%20millions%0Aof%20parameters%2C%20in%20both%20local%20and%20global%20discovery%20settings.%20Relying%20on%20fully%0Adifferentiable%20code%2C%20the%20algorithmic%20implementation%20leverages%20vectorizing%20maps%0Athat%20enable%20history-dependent%20automatic%20differentiation%20via%20efficient%20batched%0Aexecution%20of%20shared%20computation%20graphs.%20This%20contribution%20also%20aims%20to%0Afacilitate%20the%20integration%2C%20evaluation%20and%20application%20of%20future%20material%20model%0Aarchitectures%20by%20openly%20supporting%20the%20research%20community.%20Therefore%2C%20ADiMU%20is%0Areleased%20as%20an%20open-source%20computational%20tool%2C%20integrated%20into%20a%20carefully%0Adesigned%20and%20documented%20software%20named%20HookeAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07801v1&entry.124074799=Read"},
{"title": "Overflow Prevention Enhances Long-Context Recurrent LLMs", "author": "Assaf Ben-Kish and Itamar Zimerman and M. Jehanzeb Mirza and James Glass and Leonid Karlinsky and Raja Giryes", "abstract": "  A recent trend in LLMs is developing recurrent sub-quadratic models that\nimprove long-context processing efficiency. We investigate leading large\nlong-context models, focusing on how their fixed-size recurrent memory affects\ntheir performance. Our experiments reveal that, even when these models are\ntrained for extended contexts, their use of long contexts remains\nunderutilized. Specifically, we demonstrate that a chunk-based inference\nprocedure, which identifies and processes only the most relevant portion of the\ninput can mitigate recurrent memory failures and be effective for many\nlong-context tasks: On LongBench, our method improves the overall performance\nof Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,\nRecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this\nsimple approach also leads to state-of-the-art results in the challenging\nLongBench v2 benchmark, showing competitive performance with equivalent size\nTransformers. Furthermore, our findings raise questions about whether recurrent\nmodels genuinely exploit long-range dependencies, as our single-chunk strategy\ndelivers stronger performance - even in tasks that presumably require\ncross-context relations.\n", "link": "http://arxiv.org/abs/2505.07793v1", "date": "2025-05-12", "relevancy": 2.006, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overflow%20Prevention%20Enhances%20Long-Context%20Recurrent%20LLMs&body=Title%3A%20Overflow%20Prevention%20Enhances%20Long-Context%20Recurrent%20LLMs%0AAuthor%3A%20Assaf%20Ben-Kish%20and%20Itamar%20Zimerman%20and%20M.%20Jehanzeb%20Mirza%20and%20James%20Glass%20and%20Leonid%20Karlinsky%20and%20Raja%20Giryes%0AAbstract%3A%20%20%20A%20recent%20trend%20in%20LLMs%20is%20developing%20recurrent%20sub-quadratic%20models%20that%0Aimprove%20long-context%20processing%20efficiency.%20We%20investigate%20leading%20large%0Along-context%20models%2C%20focusing%20on%20how%20their%20fixed-size%20recurrent%20memory%20affects%0Atheir%20performance.%20Our%20experiments%20reveal%20that%2C%20even%20when%20these%20models%20are%0Atrained%20for%20extended%20contexts%2C%20their%20use%20of%20long%20contexts%20remains%0Aunderutilized.%20Specifically%2C%20we%20demonstrate%20that%20a%20chunk-based%20inference%0Aprocedure%2C%20which%20identifies%20and%20processes%20only%20the%20most%20relevant%20portion%20of%20the%0Ainput%20can%20mitigate%20recurrent%20memory%20failures%20and%20be%20effective%20for%20many%0Along-context%20tasks%3A%20On%20LongBench%2C%20our%20method%20improves%20the%20overall%20performance%0Aof%20Falcon3-Mamba-Inst-7B%20by%2014%25%2C%20Falcon-Mamba-Inst-7B%20by%2028%25%2C%0ARecurrentGemma-IT-9B%20by%2050%25%2C%20and%20RWKV6-Finch-7B%20by%2051%25.%20Surprisingly%2C%20this%0Asimple%20approach%20also%20leads%20to%20state-of-the-art%20results%20in%20the%20challenging%0ALongBench%20v2%20benchmark%2C%20showing%20competitive%20performance%20with%20equivalent%20size%0ATransformers.%20Furthermore%2C%20our%20findings%20raise%20questions%20about%20whether%20recurrent%0Amodels%20genuinely%20exploit%20long-range%20dependencies%2C%20as%20our%20single-chunk%20strategy%0Adelivers%20stronger%20performance%20-%20even%20in%20tasks%20that%20presumably%20require%0Across-context%20relations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverflow%2520Prevention%2520Enhances%2520Long-Context%2520Recurrent%2520LLMs%26entry.906535625%3DAssaf%2520Ben-Kish%2520and%2520Itamar%2520Zimerman%2520and%2520M.%2520Jehanzeb%2520Mirza%2520and%2520James%2520Glass%2520and%2520Leonid%2520Karlinsky%2520and%2520Raja%2520Giryes%26entry.1292438233%3D%2520%2520A%2520recent%2520trend%2520in%2520LLMs%2520is%2520developing%2520recurrent%2520sub-quadratic%2520models%2520that%250Aimprove%2520long-context%2520processing%2520efficiency.%2520We%2520investigate%2520leading%2520large%250Along-context%2520models%252C%2520focusing%2520on%2520how%2520their%2520fixed-size%2520recurrent%2520memory%2520affects%250Atheir%2520performance.%2520Our%2520experiments%2520reveal%2520that%252C%2520even%2520when%2520these%2520models%2520are%250Atrained%2520for%2520extended%2520contexts%252C%2520their%2520use%2520of%2520long%2520contexts%2520remains%250Aunderutilized.%2520Specifically%252C%2520we%2520demonstrate%2520that%2520a%2520chunk-based%2520inference%250Aprocedure%252C%2520which%2520identifies%2520and%2520processes%2520only%2520the%2520most%2520relevant%2520portion%2520of%2520the%250Ainput%2520can%2520mitigate%2520recurrent%2520memory%2520failures%2520and%2520be%2520effective%2520for%2520many%250Along-context%2520tasks%253A%2520On%2520LongBench%252C%2520our%2520method%2520improves%2520the%2520overall%2520performance%250Aof%2520Falcon3-Mamba-Inst-7B%2520by%252014%2525%252C%2520Falcon-Mamba-Inst-7B%2520by%252028%2525%252C%250ARecurrentGemma-IT-9B%2520by%252050%2525%252C%2520and%2520RWKV6-Finch-7B%2520by%252051%2525.%2520Surprisingly%252C%2520this%250Asimple%2520approach%2520also%2520leads%2520to%2520state-of-the-art%2520results%2520in%2520the%2520challenging%250ALongBench%2520v2%2520benchmark%252C%2520showing%2520competitive%2520performance%2520with%2520equivalent%2520size%250ATransformers.%2520Furthermore%252C%2520our%2520findings%2520raise%2520questions%2520about%2520whether%2520recurrent%250Amodels%2520genuinely%2520exploit%2520long-range%2520dependencies%252C%2520as%2520our%2520single-chunk%2520strategy%250Adelivers%2520stronger%2520performance%2520-%2520even%2520in%2520tasks%2520that%2520presumably%2520require%250Across-context%2520relations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overflow%20Prevention%20Enhances%20Long-Context%20Recurrent%20LLMs&entry.906535625=Assaf%20Ben-Kish%20and%20Itamar%20Zimerman%20and%20M.%20Jehanzeb%20Mirza%20and%20James%20Glass%20and%20Leonid%20Karlinsky%20and%20Raja%20Giryes&entry.1292438233=%20%20A%20recent%20trend%20in%20LLMs%20is%20developing%20recurrent%20sub-quadratic%20models%20that%0Aimprove%20long-context%20processing%20efficiency.%20We%20investigate%20leading%20large%0Along-context%20models%2C%20focusing%20on%20how%20their%20fixed-size%20recurrent%20memory%20affects%0Atheir%20performance.%20Our%20experiments%20reveal%20that%2C%20even%20when%20these%20models%20are%0Atrained%20for%20extended%20contexts%2C%20their%20use%20of%20long%20contexts%20remains%0Aunderutilized.%20Specifically%2C%20we%20demonstrate%20that%20a%20chunk-based%20inference%0Aprocedure%2C%20which%20identifies%20and%20processes%20only%20the%20most%20relevant%20portion%20of%20the%0Ainput%20can%20mitigate%20recurrent%20memory%20failures%20and%20be%20effective%20for%20many%0Along-context%20tasks%3A%20On%20LongBench%2C%20our%20method%20improves%20the%20overall%20performance%0Aof%20Falcon3-Mamba-Inst-7B%20by%2014%25%2C%20Falcon-Mamba-Inst-7B%20by%2028%25%2C%0ARecurrentGemma-IT-9B%20by%2050%25%2C%20and%20RWKV6-Finch-7B%20by%2051%25.%20Surprisingly%2C%20this%0Asimple%20approach%20also%20leads%20to%20state-of-the-art%20results%20in%20the%20challenging%0ALongBench%20v2%20benchmark%2C%20showing%20competitive%20performance%20with%20equivalent%20size%0ATransformers.%20Furthermore%2C%20our%20findings%20raise%20questions%20about%20whether%20recurrent%0Amodels%20genuinely%20exploit%20long-range%20dependencies%2C%20as%20our%20single-chunk%20strategy%0Adelivers%20stronger%20performance%20-%20even%20in%20tasks%20that%20presumably%20require%0Across-context%20relations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07793v1&entry.124074799=Read"},
{"title": "Multilingual Performance of a Multimodal Artificial Intelligence System\n  on Multisubject Physics Concept Inventories", "author": "Gerd Kortemeyer and Marina Babayeva and Giulia Polverini and Ralf Widenhorn and Bor Gregorcic", "abstract": "  We investigate the multilingual and multimodal performance of a large\nlanguage model-based artificial intelligence (AI) system, GPT-4o, using a\ndiverse set of physics concept inventories spanning multiple languages and\nsubject categories. The inventories, sourced from the PhysPort website, cover\nclassical physics topics such as mechanics, electromagnetism, optics, and\nthermodynamics, as well as relativity, quantum mechanics, astronomy,\nmathematics, and laboratory skills. Unlike previous text-only studies, we\nuploaded the inventories as images to reflect what a student would see on\npaper, thereby assessing the system's multimodal functionality. Our results\nindicate variation in performance across subjects, with laboratory skills\nstanding out as the weakest. We also observe differences across languages, with\nEnglish and European languages showing the strongest performance. Notably, the\nrelative difficulty of an inventory item is largely independent of the language\nof the survey. When comparing AI results to existing literature on student\nperformance, we find that the AI system outperforms average post-instruction\nundergraduate students in all subject categories except laboratory skills.\nFurthermore, the AI performs worse on items requiring visual interpretation of\nimages than on those that are purely text-based. While our exploratory findings\nshow GPT-4o's potential usefulness in physics education, they highlight the\ncritical need for instructors to foster students' ability to critically\nevaluate AI outputs, adapt curricula thoughtfully in response to AI\nadvancements, and address equity concerns associated with AI integration.\n", "link": "http://arxiv.org/abs/2501.06143v3", "date": "2025-05-12", "relevancy": 1.9988, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5496}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Performance%20of%20a%20Multimodal%20Artificial%20Intelligence%20System%0A%20%20on%20Multisubject%20Physics%20Concept%20Inventories&body=Title%3A%20Multilingual%20Performance%20of%20a%20Multimodal%20Artificial%20Intelligence%20System%0A%20%20on%20Multisubject%20Physics%20Concept%20Inventories%0AAuthor%3A%20Gerd%20Kortemeyer%20and%20Marina%20Babayeva%20and%20Giulia%20Polverini%20and%20Ralf%20Widenhorn%20and%20Bor%20Gregorcic%0AAbstract%3A%20%20%20We%20investigate%20the%20multilingual%20and%20multimodal%20performance%20of%20a%20large%0Alanguage%20model-based%20artificial%20intelligence%20%28AI%29%20system%2C%20GPT-4o%2C%20using%20a%0Adiverse%20set%20of%20physics%20concept%20inventories%20spanning%20multiple%20languages%20and%0Asubject%20categories.%20The%20inventories%2C%20sourced%20from%20the%20PhysPort%20website%2C%20cover%0Aclassical%20physics%20topics%20such%20as%20mechanics%2C%20electromagnetism%2C%20optics%2C%20and%0Athermodynamics%2C%20as%20well%20as%20relativity%2C%20quantum%20mechanics%2C%20astronomy%2C%0Amathematics%2C%20and%20laboratory%20skills.%20Unlike%20previous%20text-only%20studies%2C%20we%0Auploaded%20the%20inventories%20as%20images%20to%20reflect%20what%20a%20student%20would%20see%20on%0Apaper%2C%20thereby%20assessing%20the%20system%27s%20multimodal%20functionality.%20Our%20results%0Aindicate%20variation%20in%20performance%20across%20subjects%2C%20with%20laboratory%20skills%0Astanding%20out%20as%20the%20weakest.%20We%20also%20observe%20differences%20across%20languages%2C%20with%0AEnglish%20and%20European%20languages%20showing%20the%20strongest%20performance.%20Notably%2C%20the%0Arelative%20difficulty%20of%20an%20inventory%20item%20is%20largely%20independent%20of%20the%20language%0Aof%20the%20survey.%20When%20comparing%20AI%20results%20to%20existing%20literature%20on%20student%0Aperformance%2C%20we%20find%20that%20the%20AI%20system%20outperforms%20average%20post-instruction%0Aundergraduate%20students%20in%20all%20subject%20categories%20except%20laboratory%20skills.%0AFurthermore%2C%20the%20AI%20performs%20worse%20on%20items%20requiring%20visual%20interpretation%20of%0Aimages%20than%20on%20those%20that%20are%20purely%20text-based.%20While%20our%20exploratory%20findings%0Ashow%20GPT-4o%27s%20potential%20usefulness%20in%20physics%20education%2C%20they%20highlight%20the%0Acritical%20need%20for%20instructors%20to%20foster%20students%27%20ability%20to%20critically%0Aevaluate%20AI%20outputs%2C%20adapt%20curricula%20thoughtfully%20in%20response%20to%20AI%0Aadvancements%2C%20and%20address%20equity%20concerns%20associated%20with%20AI%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06143v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Performance%2520of%2520a%2520Multimodal%2520Artificial%2520Intelligence%2520System%250A%2520%2520on%2520Multisubject%2520Physics%2520Concept%2520Inventories%26entry.906535625%3DGerd%2520Kortemeyer%2520and%2520Marina%2520Babayeva%2520and%2520Giulia%2520Polverini%2520and%2520Ralf%2520Widenhorn%2520and%2520Bor%2520Gregorcic%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520multilingual%2520and%2520multimodal%2520performance%2520of%2520a%2520large%250Alanguage%2520model-based%2520artificial%2520intelligence%2520%2528AI%2529%2520system%252C%2520GPT-4o%252C%2520using%2520a%250Adiverse%2520set%2520of%2520physics%2520concept%2520inventories%2520spanning%2520multiple%2520languages%2520and%250Asubject%2520categories.%2520The%2520inventories%252C%2520sourced%2520from%2520the%2520PhysPort%2520website%252C%2520cover%250Aclassical%2520physics%2520topics%2520such%2520as%2520mechanics%252C%2520electromagnetism%252C%2520optics%252C%2520and%250Athermodynamics%252C%2520as%2520well%2520as%2520relativity%252C%2520quantum%2520mechanics%252C%2520astronomy%252C%250Amathematics%252C%2520and%2520laboratory%2520skills.%2520Unlike%2520previous%2520text-only%2520studies%252C%2520we%250Auploaded%2520the%2520inventories%2520as%2520images%2520to%2520reflect%2520what%2520a%2520student%2520would%2520see%2520on%250Apaper%252C%2520thereby%2520assessing%2520the%2520system%2527s%2520multimodal%2520functionality.%2520Our%2520results%250Aindicate%2520variation%2520in%2520performance%2520across%2520subjects%252C%2520with%2520laboratory%2520skills%250Astanding%2520out%2520as%2520the%2520weakest.%2520We%2520also%2520observe%2520differences%2520across%2520languages%252C%2520with%250AEnglish%2520and%2520European%2520languages%2520showing%2520the%2520strongest%2520performance.%2520Notably%252C%2520the%250Arelative%2520difficulty%2520of%2520an%2520inventory%2520item%2520is%2520largely%2520independent%2520of%2520the%2520language%250Aof%2520the%2520survey.%2520When%2520comparing%2520AI%2520results%2520to%2520existing%2520literature%2520on%2520student%250Aperformance%252C%2520we%2520find%2520that%2520the%2520AI%2520system%2520outperforms%2520average%2520post-instruction%250Aundergraduate%2520students%2520in%2520all%2520subject%2520categories%2520except%2520laboratory%2520skills.%250AFurthermore%252C%2520the%2520AI%2520performs%2520worse%2520on%2520items%2520requiring%2520visual%2520interpretation%2520of%250Aimages%2520than%2520on%2520those%2520that%2520are%2520purely%2520text-based.%2520While%2520our%2520exploratory%2520findings%250Ashow%2520GPT-4o%2527s%2520potential%2520usefulness%2520in%2520physics%2520education%252C%2520they%2520highlight%2520the%250Acritical%2520need%2520for%2520instructors%2520to%2520foster%2520students%2527%2520ability%2520to%2520critically%250Aevaluate%2520AI%2520outputs%252C%2520adapt%2520curricula%2520thoughtfully%2520in%2520response%2520to%2520AI%250Aadvancements%252C%2520and%2520address%2520equity%2520concerns%2520associated%2520with%2520AI%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06143v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Performance%20of%20a%20Multimodal%20Artificial%20Intelligence%20System%0A%20%20on%20Multisubject%20Physics%20Concept%20Inventories&entry.906535625=Gerd%20Kortemeyer%20and%20Marina%20Babayeva%20and%20Giulia%20Polverini%20and%20Ralf%20Widenhorn%20and%20Bor%20Gregorcic&entry.1292438233=%20%20We%20investigate%20the%20multilingual%20and%20multimodal%20performance%20of%20a%20large%0Alanguage%20model-based%20artificial%20intelligence%20%28AI%29%20system%2C%20GPT-4o%2C%20using%20a%0Adiverse%20set%20of%20physics%20concept%20inventories%20spanning%20multiple%20languages%20and%0Asubject%20categories.%20The%20inventories%2C%20sourced%20from%20the%20PhysPort%20website%2C%20cover%0Aclassical%20physics%20topics%20such%20as%20mechanics%2C%20electromagnetism%2C%20optics%2C%20and%0Athermodynamics%2C%20as%20well%20as%20relativity%2C%20quantum%20mechanics%2C%20astronomy%2C%0Amathematics%2C%20and%20laboratory%20skills.%20Unlike%20previous%20text-only%20studies%2C%20we%0Auploaded%20the%20inventories%20as%20images%20to%20reflect%20what%20a%20student%20would%20see%20on%0Apaper%2C%20thereby%20assessing%20the%20system%27s%20multimodal%20functionality.%20Our%20results%0Aindicate%20variation%20in%20performance%20across%20subjects%2C%20with%20laboratory%20skills%0Astanding%20out%20as%20the%20weakest.%20We%20also%20observe%20differences%20across%20languages%2C%20with%0AEnglish%20and%20European%20languages%20showing%20the%20strongest%20performance.%20Notably%2C%20the%0Arelative%20difficulty%20of%20an%20inventory%20item%20is%20largely%20independent%20of%20the%20language%0Aof%20the%20survey.%20When%20comparing%20AI%20results%20to%20existing%20literature%20on%20student%0Aperformance%2C%20we%20find%20that%20the%20AI%20system%20outperforms%20average%20post-instruction%0Aundergraduate%20students%20in%20all%20subject%20categories%20except%20laboratory%20skills.%0AFurthermore%2C%20the%20AI%20performs%20worse%20on%20items%20requiring%20visual%20interpretation%20of%0Aimages%20than%20on%20those%20that%20are%20purely%20text-based.%20While%20our%20exploratory%20findings%0Ashow%20GPT-4o%27s%20potential%20usefulness%20in%20physics%20education%2C%20they%20highlight%20the%0Acritical%20need%20for%20instructors%20to%20foster%20students%27%20ability%20to%20critically%0Aevaluate%20AI%20outputs%2C%20adapt%20curricula%20thoughtfully%20in%20response%20to%20AI%0Aadvancements%2C%20and%20address%20equity%20concerns%20associated%20with%20AI%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06143v3&entry.124074799=Read"},
{"title": "Lightweight Multispectral Crop-Weed Segmentation for Precision\n  Agriculture", "author": "Zeynep Galymzhankyzy and Eric Martinson", "abstract": "  Efficient crop-weed segmentation is critical for site-specific weed control\nin precision agriculture. Conventional CNN-based methods struggle to generalize\nand rely on RGB imagery, limiting performance under complex field conditions.\nTo address these challenges, we propose a lightweight transformer-CNN hybrid.\nIt processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using\nspecialized encoders and dynamic modality integration. Evaluated on the\nWeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of\n78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7\nmillion parameters, the model offers high accuracy, computational efficiency,\nand potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and\nedge devices, advancing precision weed management.\n", "link": "http://arxiv.org/abs/2505.07444v1", "date": "2025-05-12", "relevancy": 1.9918, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5056}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5029}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Multispectral%20Crop-Weed%20Segmentation%20for%20Precision%0A%20%20Agriculture&body=Title%3A%20Lightweight%20Multispectral%20Crop-Weed%20Segmentation%20for%20Precision%0A%20%20Agriculture%0AAuthor%3A%20Zeynep%20Galymzhankyzy%20and%20Eric%20Martinson%0AAbstract%3A%20%20%20Efficient%20crop-weed%20segmentation%20is%20critical%20for%20site-specific%20weed%20control%0Ain%20precision%20agriculture.%20Conventional%20CNN-based%20methods%20struggle%20to%20generalize%0Aand%20rely%20on%20RGB%20imagery%2C%20limiting%20performance%20under%20complex%20field%20conditions.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20lightweight%20transformer-CNN%20hybrid.%0AIt%20processes%20RGB%2C%20Near-Infrared%20%28NIR%29%2C%20and%20Red-Edge%20%28RE%29%20bands%20using%0Aspecialized%20encoders%20and%20dynamic%20modality%20integration.%20Evaluated%20on%20the%0AWeedsGalore%20dataset%2C%20the%20model%20achieves%20a%20segmentation%20accuracy%20%28mean%20IoU%29%20of%0A78.88%25%2C%20outperforming%20RGB-only%20models%20by%2015.8%20percentage%20points.%20With%20only%208.7%0Amillion%20parameters%2C%20the%20model%20offers%20high%20accuracy%2C%20computational%20efficiency%2C%0Aand%20potential%20for%20real-time%20deployment%20on%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%0Aedge%20devices%2C%20advancing%20precision%20weed%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Multispectral%2520Crop-Weed%2520Segmentation%2520for%2520Precision%250A%2520%2520Agriculture%26entry.906535625%3DZeynep%2520Galymzhankyzy%2520and%2520Eric%2520Martinson%26entry.1292438233%3D%2520%2520Efficient%2520crop-weed%2520segmentation%2520is%2520critical%2520for%2520site-specific%2520weed%2520control%250Ain%2520precision%2520agriculture.%2520Conventional%2520CNN-based%2520methods%2520struggle%2520to%2520generalize%250Aand%2520rely%2520on%2520RGB%2520imagery%252C%2520limiting%2520performance%2520under%2520complex%2520field%2520conditions.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520lightweight%2520transformer-CNN%2520hybrid.%250AIt%2520processes%2520RGB%252C%2520Near-Infrared%2520%2528NIR%2529%252C%2520and%2520Red-Edge%2520%2528RE%2529%2520bands%2520using%250Aspecialized%2520encoders%2520and%2520dynamic%2520modality%2520integration.%2520Evaluated%2520on%2520the%250AWeedsGalore%2520dataset%252C%2520the%2520model%2520achieves%2520a%2520segmentation%2520accuracy%2520%2528mean%2520IoU%2529%2520of%250A78.88%2525%252C%2520outperforming%2520RGB-only%2520models%2520by%252015.8%2520percentage%2520points.%2520With%2520only%25208.7%250Amillion%2520parameters%252C%2520the%2520model%2520offers%2520high%2520accuracy%252C%2520computational%2520efficiency%252C%250Aand%2520potential%2520for%2520real-time%2520deployment%2520on%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520and%250Aedge%2520devices%252C%2520advancing%2520precision%2520weed%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Multispectral%20Crop-Weed%20Segmentation%20for%20Precision%0A%20%20Agriculture&entry.906535625=Zeynep%20Galymzhankyzy%20and%20Eric%20Martinson&entry.1292438233=%20%20Efficient%20crop-weed%20segmentation%20is%20critical%20for%20site-specific%20weed%20control%0Ain%20precision%20agriculture.%20Conventional%20CNN-based%20methods%20struggle%20to%20generalize%0Aand%20rely%20on%20RGB%20imagery%2C%20limiting%20performance%20under%20complex%20field%20conditions.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20lightweight%20transformer-CNN%20hybrid.%0AIt%20processes%20RGB%2C%20Near-Infrared%20%28NIR%29%2C%20and%20Red-Edge%20%28RE%29%20bands%20using%0Aspecialized%20encoders%20and%20dynamic%20modality%20integration.%20Evaluated%20on%20the%0AWeedsGalore%20dataset%2C%20the%20model%20achieves%20a%20segmentation%20accuracy%20%28mean%20IoU%29%20of%0A78.88%25%2C%20outperforming%20RGB-only%20models%20by%2015.8%20percentage%20points.%20With%20only%208.7%0Amillion%20parameters%2C%20the%20model%20offers%20high%20accuracy%2C%20computational%20efficiency%2C%0Aand%20potential%20for%20real-time%20deployment%20on%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%0Aedge%20devices%2C%20advancing%20precision%20weed%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07444v1&entry.124074799=Read"},
{"title": "Bang for the Buck: Vector Search on Cloud CPUs", "author": "Leonardo Kuffo and Peter Boncz", "abstract": "  Vector databases have emerged as a new type of systems that support efficient\nquerying of high-dimensional vectors. Many of these offer their database as a\nservice in the cloud. However, the variety of available CPUs and the lack of\nvector search benchmarks across CPUs make it difficult for users to choose one.\nIn this study, we show that CPU microarchitectures available in the cloud\nperform significantly differently across vector search scenarios. For instance,\nin an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per\nsecond (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the\ntables turn. However, when looking at the number of queries per dollar (QP$),\nGraviton3 is the best option for most indexes and quantization settings, even\nover Graviton4 (Table 1). With this work, we hope to guide users in getting the\nbest \"bang for the buck\" when deploying vector search systems.\n", "link": "http://arxiv.org/abs/2505.07621v1", "date": "2025-05-12", "relevancy": 1.9819, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bang%20for%20the%20Buck%3A%20Vector%20Search%20on%20Cloud%20CPUs&body=Title%3A%20Bang%20for%20the%20Buck%3A%20Vector%20Search%20on%20Cloud%20CPUs%0AAuthor%3A%20Leonardo%20Kuffo%20and%20Peter%20Boncz%0AAbstract%3A%20%20%20Vector%20databases%20have%20emerged%20as%20a%20new%20type%20of%20systems%20that%20support%20efficient%0Aquerying%20of%20high-dimensional%20vectors.%20Many%20of%20these%20offer%20their%20database%20as%20a%0Aservice%20in%20the%20cloud.%20However%2C%20the%20variety%20of%20available%20CPUs%20and%20the%20lack%20of%0Avector%20search%20benchmarks%20across%20CPUs%20make%20it%20difficult%20for%20users%20to%20choose%20one.%0AIn%20this%20study%2C%20we%20show%20that%20CPU%20microarchitectures%20available%20in%20the%20cloud%0Aperform%20significantly%20differently%20across%20vector%20search%20scenarios.%20For%20instance%2C%0Ain%20an%20IVF%20index%20on%20float32%20vectors%2C%20AMD%27s%20Zen4%20gives%20almost%203x%20more%20queries%20per%0Asecond%20%28QPS%29%20compared%20to%20Intel%27s%20Sapphire%20Rapids%2C%20but%20for%20HNSW%20indexes%2C%20the%0Atables%20turn.%20However%2C%20when%20looking%20at%20the%20number%20of%20queries%20per%20dollar%20%28QP%24%29%2C%0AGraviton3%20is%20the%20best%20option%20for%20most%20indexes%20and%20quantization%20settings%2C%20even%0Aover%20Graviton4%20%28Table%201%29.%20With%20this%20work%2C%20we%20hope%20to%20guide%20users%20in%20getting%20the%0Abest%20%22bang%20for%20the%20buck%22%20when%20deploying%20vector%20search%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBang%2520for%2520the%2520Buck%253A%2520Vector%2520Search%2520on%2520Cloud%2520CPUs%26entry.906535625%3DLeonardo%2520Kuffo%2520and%2520Peter%2520Boncz%26entry.1292438233%3D%2520%2520Vector%2520databases%2520have%2520emerged%2520as%2520a%2520new%2520type%2520of%2520systems%2520that%2520support%2520efficient%250Aquerying%2520of%2520high-dimensional%2520vectors.%2520Many%2520of%2520these%2520offer%2520their%2520database%2520as%2520a%250Aservice%2520in%2520the%2520cloud.%2520However%252C%2520the%2520variety%2520of%2520available%2520CPUs%2520and%2520the%2520lack%2520of%250Avector%2520search%2520benchmarks%2520across%2520CPUs%2520make%2520it%2520difficult%2520for%2520users%2520to%2520choose%2520one.%250AIn%2520this%2520study%252C%2520we%2520show%2520that%2520CPU%2520microarchitectures%2520available%2520in%2520the%2520cloud%250Aperform%2520significantly%2520differently%2520across%2520vector%2520search%2520scenarios.%2520For%2520instance%252C%250Ain%2520an%2520IVF%2520index%2520on%2520float32%2520vectors%252C%2520AMD%2527s%2520Zen4%2520gives%2520almost%25203x%2520more%2520queries%2520per%250Asecond%2520%2528QPS%2529%2520compared%2520to%2520Intel%2527s%2520Sapphire%2520Rapids%252C%2520but%2520for%2520HNSW%2520indexes%252C%2520the%250Atables%2520turn.%2520However%252C%2520when%2520looking%2520at%2520the%2520number%2520of%2520queries%2520per%2520dollar%2520%2528QP%2524%2529%252C%250AGraviton3%2520is%2520the%2520best%2520option%2520for%2520most%2520indexes%2520and%2520quantization%2520settings%252C%2520even%250Aover%2520Graviton4%2520%2528Table%25201%2529.%2520With%2520this%2520work%252C%2520we%2520hope%2520to%2520guide%2520users%2520in%2520getting%2520the%250Abest%2520%2522bang%2520for%2520the%2520buck%2522%2520when%2520deploying%2520vector%2520search%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bang%20for%20the%20Buck%3A%20Vector%20Search%20on%20Cloud%20CPUs&entry.906535625=Leonardo%20Kuffo%20and%20Peter%20Boncz&entry.1292438233=%20%20Vector%20databases%20have%20emerged%20as%20a%20new%20type%20of%20systems%20that%20support%20efficient%0Aquerying%20of%20high-dimensional%20vectors.%20Many%20of%20these%20offer%20their%20database%20as%20a%0Aservice%20in%20the%20cloud.%20However%2C%20the%20variety%20of%20available%20CPUs%20and%20the%20lack%20of%0Avector%20search%20benchmarks%20across%20CPUs%20make%20it%20difficult%20for%20users%20to%20choose%20one.%0AIn%20this%20study%2C%20we%20show%20that%20CPU%20microarchitectures%20available%20in%20the%20cloud%0Aperform%20significantly%20differently%20across%20vector%20search%20scenarios.%20For%20instance%2C%0Ain%20an%20IVF%20index%20on%20float32%20vectors%2C%20AMD%27s%20Zen4%20gives%20almost%203x%20more%20queries%20per%0Asecond%20%28QPS%29%20compared%20to%20Intel%27s%20Sapphire%20Rapids%2C%20but%20for%20HNSW%20indexes%2C%20the%0Atables%20turn.%20However%2C%20when%20looking%20at%20the%20number%20of%20queries%20per%20dollar%20%28QP%24%29%2C%0AGraviton3%20is%20the%20best%20option%20for%20most%20indexes%20and%20quantization%20settings%2C%20even%0Aover%20Graviton4%20%28Table%201%29.%20With%20this%20work%2C%20we%20hope%20to%20guide%20users%20in%20getting%20the%0Abest%20%22bang%20for%20the%20buck%22%20when%20deploying%20vector%20search%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07621v1&entry.124074799=Read"},
{"title": "Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A\n  Comprehensive Review of Methods,Datasets,and Future Directions", "author": "Yi Zhang and Wenye Zhou and Ruonan Lin and Xin Yang and Hao Zheng", "abstract": "  Traffic accident prediction and detection are critical for enhancing road\nsafety,and vision-based traffic accident anticipation (Vision-TAA) has emerged\nas a promising approach in the era of deep learning.This paper reviews 147\nrecent studies,focusing on the application of supervised,unsupervised,and\nhybrid deep learning models for accident prediction,alongside the use of\nreal-world and synthetic datasets.Current methodologies are categorized into\nfour key approaches: image and video feature-based prediction, spatiotemporal\nfeature-based prediction, scene understanding,and multimodal data fusion.While\nthese methods demonstrate significant potential,challenges such as data\nscarcity,limited generalization to complex scenarios,and real-time performance\nconstraints remain prevalent. This review highlights opportunities for future\nresearch,including the integration of multimodal data fusion, self-supervised\nlearning,and Transformer-based architectures to enhance prediction accuracy and\nscalability.By synthesizing existing advancements and identifying critical\ngaps, this paper provides a foundational reference for developing robust and\nadaptive Vision-TAA systems,contributing to road safety and traffic management.\n", "link": "http://arxiv.org/abs/2505.07611v1", "date": "2025-05-12", "relevancy": 1.9753, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Advances%20in%20Vision-Based%20Traffic%20Accident%20Anticipation%3A%20A%0A%20%20Comprehensive%20Review%20of%20Methods%2CDatasets%2Cand%20Future%20Directions&body=Title%3A%20Deep%20Learning%20Advances%20in%20Vision-Based%20Traffic%20Accident%20Anticipation%3A%20A%0A%20%20Comprehensive%20Review%20of%20Methods%2CDatasets%2Cand%20Future%20Directions%0AAuthor%3A%20Yi%20Zhang%20and%20Wenye%20Zhou%20and%20Ruonan%20Lin%20and%20Xin%20Yang%20and%20Hao%20Zheng%0AAbstract%3A%20%20%20Traffic%20accident%20prediction%20and%20detection%20are%20critical%20for%20enhancing%20road%0Asafety%2Cand%20vision-based%20traffic%20accident%20anticipation%20%28Vision-TAA%29%20has%20emerged%0Aas%20a%20promising%20approach%20in%20the%20era%20of%20deep%20learning.This%20paper%20reviews%20147%0Arecent%20studies%2Cfocusing%20on%20the%20application%20of%20supervised%2Cunsupervised%2Cand%0Ahybrid%20deep%20learning%20models%20for%20accident%20prediction%2Calongside%20the%20use%20of%0Areal-world%20and%20synthetic%20datasets.Current%20methodologies%20are%20categorized%20into%0Afour%20key%20approaches%3A%20image%20and%20video%20feature-based%20prediction%2C%20spatiotemporal%0Afeature-based%20prediction%2C%20scene%20understanding%2Cand%20multimodal%20data%20fusion.While%0Athese%20methods%20demonstrate%20significant%20potential%2Cchallenges%20such%20as%20data%0Ascarcity%2Climited%20generalization%20to%20complex%20scenarios%2Cand%20real-time%20performance%0Aconstraints%20remain%20prevalent.%20This%20review%20highlights%20opportunities%20for%20future%0Aresearch%2Cincluding%20the%20integration%20of%20multimodal%20data%20fusion%2C%20self-supervised%0Alearning%2Cand%20Transformer-based%20architectures%20to%20enhance%20prediction%20accuracy%20and%0Ascalability.By%20synthesizing%20existing%20advancements%20and%20identifying%20critical%0Agaps%2C%20this%20paper%20provides%20a%20foundational%20reference%20for%20developing%20robust%20and%0Aadaptive%20Vision-TAA%20systems%2Ccontributing%20to%20road%20safety%20and%20traffic%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Advances%2520in%2520Vision-Based%2520Traffic%2520Accident%2520Anticipation%253A%2520A%250A%2520%2520Comprehensive%2520Review%2520of%2520Methods%252CDatasets%252Cand%2520Future%2520Directions%26entry.906535625%3DYi%2520Zhang%2520and%2520Wenye%2520Zhou%2520and%2520Ruonan%2520Lin%2520and%2520Xin%2520Yang%2520and%2520Hao%2520Zheng%26entry.1292438233%3D%2520%2520Traffic%2520accident%2520prediction%2520and%2520detection%2520are%2520critical%2520for%2520enhancing%2520road%250Asafety%252Cand%2520vision-based%2520traffic%2520accident%2520anticipation%2520%2528Vision-TAA%2529%2520has%2520emerged%250Aas%2520a%2520promising%2520approach%2520in%2520the%2520era%2520of%2520deep%2520learning.This%2520paper%2520reviews%2520147%250Arecent%2520studies%252Cfocusing%2520on%2520the%2520application%2520of%2520supervised%252Cunsupervised%252Cand%250Ahybrid%2520deep%2520learning%2520models%2520for%2520accident%2520prediction%252Calongside%2520the%2520use%2520of%250Areal-world%2520and%2520synthetic%2520datasets.Current%2520methodologies%2520are%2520categorized%2520into%250Afour%2520key%2520approaches%253A%2520image%2520and%2520video%2520feature-based%2520prediction%252C%2520spatiotemporal%250Afeature-based%2520prediction%252C%2520scene%2520understanding%252Cand%2520multimodal%2520data%2520fusion.While%250Athese%2520methods%2520demonstrate%2520significant%2520potential%252Cchallenges%2520such%2520as%2520data%250Ascarcity%252Climited%2520generalization%2520to%2520complex%2520scenarios%252Cand%2520real-time%2520performance%250Aconstraints%2520remain%2520prevalent.%2520This%2520review%2520highlights%2520opportunities%2520for%2520future%250Aresearch%252Cincluding%2520the%2520integration%2520of%2520multimodal%2520data%2520fusion%252C%2520self-supervised%250Alearning%252Cand%2520Transformer-based%2520architectures%2520to%2520enhance%2520prediction%2520accuracy%2520and%250Ascalability.By%2520synthesizing%2520existing%2520advancements%2520and%2520identifying%2520critical%250Agaps%252C%2520this%2520paper%2520provides%2520a%2520foundational%2520reference%2520for%2520developing%2520robust%2520and%250Aadaptive%2520Vision-TAA%2520systems%252Ccontributing%2520to%2520road%2520safety%2520and%2520traffic%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Advances%20in%20Vision-Based%20Traffic%20Accident%20Anticipation%3A%20A%0A%20%20Comprehensive%20Review%20of%20Methods%2CDatasets%2Cand%20Future%20Directions&entry.906535625=Yi%20Zhang%20and%20Wenye%20Zhou%20and%20Ruonan%20Lin%20and%20Xin%20Yang%20and%20Hao%20Zheng&entry.1292438233=%20%20Traffic%20accident%20prediction%20and%20detection%20are%20critical%20for%20enhancing%20road%0Asafety%2Cand%20vision-based%20traffic%20accident%20anticipation%20%28Vision-TAA%29%20has%20emerged%0Aas%20a%20promising%20approach%20in%20the%20era%20of%20deep%20learning.This%20paper%20reviews%20147%0Arecent%20studies%2Cfocusing%20on%20the%20application%20of%20supervised%2Cunsupervised%2Cand%0Ahybrid%20deep%20learning%20models%20for%20accident%20prediction%2Calongside%20the%20use%20of%0Areal-world%20and%20synthetic%20datasets.Current%20methodologies%20are%20categorized%20into%0Afour%20key%20approaches%3A%20image%20and%20video%20feature-based%20prediction%2C%20spatiotemporal%0Afeature-based%20prediction%2C%20scene%20understanding%2Cand%20multimodal%20data%20fusion.While%0Athese%20methods%20demonstrate%20significant%20potential%2Cchallenges%20such%20as%20data%0Ascarcity%2Climited%20generalization%20to%20complex%20scenarios%2Cand%20real-time%20performance%0Aconstraints%20remain%20prevalent.%20This%20review%20highlights%20opportunities%20for%20future%0Aresearch%2Cincluding%20the%20integration%20of%20multimodal%20data%20fusion%2C%20self-supervised%0Alearning%2Cand%20Transformer-based%20architectures%20to%20enhance%20prediction%20accuracy%20and%0Ascalability.By%20synthesizing%20existing%20advancements%20and%20identifying%20critical%0Agaps%2C%20this%20paper%20provides%20a%20foundational%20reference%20for%20developing%20robust%20and%0Aadaptive%20Vision-TAA%20systems%2Ccontributing%20to%20road%20safety%20and%20traffic%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07611v1&entry.124074799=Read"},
{"title": "Keep your distance: learning dispersed embeddings on $\\mathbb{S}_m$", "author": "Evgeniia Tokarchuk and Hua Chang Bakker and Vlad Niculae", "abstract": "  Learning well-separated features in high-dimensional spaces, such as text or\nimage embeddings, is crucial for many machine learning applications. Achieving\nsuch separation can be effectively accomplished through the dispersion of\nembeddings, where unrelated vectors are pushed apart as much as possible. By\nconstraining features to be on a hypersphere, we can connect dispersion to\nwell-studied problems in mathematics and physics, where optimal solutions are\nknown for limited low-dimensional cases. However, in representation learning we\ntypically deal with a large number of features in high-dimensional space, and\nmoreover, dispersion is usually traded off with some other task-oriented\ntraining objective, making existing theoretical and numerical solutions\ninapplicable. Therefore, it is common to rely on gradient-based methods to\nencourage dispersion, usually by minimizing some function of the pairwise\ndistances. In this work, we first give an overview of existing methods from\ndisconnected literature, making new connections and highlighting similarities.\nNext, we introduce some new angles. We propose to reinterpret pairwise\ndispersion using a maximum mean discrepancy (MMD) motivation. We then propose\nan online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an\neffective alternative regularizer for dispersion on generic domains. Finally,\nwe derive a novel dispersion method that directly exploits properties of the\nhypersphere. Our experiments show the importance of dispersion in image\nclassification and natural language processing tasks, and how algorithms\nexhibit different trade-offs in different regimes.\n", "link": "http://arxiv.org/abs/2502.08231v3", "date": "2025-05-12", "relevancy": 1.9737, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keep%20your%20distance%3A%20learning%20dispersed%20embeddings%20on%20%24%5Cmathbb%7BS%7D_m%24&body=Title%3A%20Keep%20your%20distance%3A%20learning%20dispersed%20embeddings%20on%20%24%5Cmathbb%7BS%7D_m%24%0AAuthor%3A%20Evgeniia%20Tokarchuk%20and%20Hua%20Chang%20Bakker%20and%20Vlad%20Niculae%0AAbstract%3A%20%20%20Learning%20well-separated%20features%20in%20high-dimensional%20spaces%2C%20such%20as%20text%20or%0Aimage%20embeddings%2C%20is%20crucial%20for%20many%20machine%20learning%20applications.%20Achieving%0Asuch%20separation%20can%20be%20effectively%20accomplished%20through%20the%20dispersion%20of%0Aembeddings%2C%20where%20unrelated%20vectors%20are%20pushed%20apart%20as%20much%20as%20possible.%20By%0Aconstraining%20features%20to%20be%20on%20a%20hypersphere%2C%20we%20can%20connect%20dispersion%20to%0Awell-studied%20problems%20in%20mathematics%20and%20physics%2C%20where%20optimal%20solutions%20are%0Aknown%20for%20limited%20low-dimensional%20cases.%20However%2C%20in%20representation%20learning%20we%0Atypically%20deal%20with%20a%20large%20number%20of%20features%20in%20high-dimensional%20space%2C%20and%0Amoreover%2C%20dispersion%20is%20usually%20traded%20off%20with%20some%20other%20task-oriented%0Atraining%20objective%2C%20making%20existing%20theoretical%20and%20numerical%20solutions%0Ainapplicable.%20Therefore%2C%20it%20is%20common%20to%20rely%20on%20gradient-based%20methods%20to%0Aencourage%20dispersion%2C%20usually%20by%20minimizing%20some%20function%20of%20the%20pairwise%0Adistances.%20In%20this%20work%2C%20we%20first%20give%20an%20overview%20of%20existing%20methods%20from%0Adisconnected%20literature%2C%20making%20new%20connections%20and%20highlighting%20similarities.%0ANext%2C%20we%20introduce%20some%20new%20angles.%20We%20propose%20to%20reinterpret%20pairwise%0Adispersion%20using%20a%20maximum%20mean%20discrepancy%20%28MMD%29%20motivation.%20We%20then%20propose%0Aan%20online%20variant%20of%20the%20celebrated%20Lloyd%27s%20algorithm%2C%20of%20K-Means%20fame%2C%20as%20an%0Aeffective%20alternative%20regularizer%20for%20dispersion%20on%20generic%20domains.%20Finally%2C%0Awe%20derive%20a%20novel%20dispersion%20method%20that%20directly%20exploits%20properties%20of%20the%0Ahypersphere.%20Our%20experiments%20show%20the%20importance%20of%20dispersion%20in%20image%0Aclassification%20and%20natural%20language%20processing%20tasks%2C%20and%20how%20algorithms%0Aexhibit%20different%20trade-offs%20in%20different%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08231v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeep%2520your%2520distance%253A%2520learning%2520dispersed%2520embeddings%2520on%2520%2524%255Cmathbb%257BS%257D_m%2524%26entry.906535625%3DEvgeniia%2520Tokarchuk%2520and%2520Hua%2520Chang%2520Bakker%2520and%2520Vlad%2520Niculae%26entry.1292438233%3D%2520%2520Learning%2520well-separated%2520features%2520in%2520high-dimensional%2520spaces%252C%2520such%2520as%2520text%2520or%250Aimage%2520embeddings%252C%2520is%2520crucial%2520for%2520many%2520machine%2520learning%2520applications.%2520Achieving%250Asuch%2520separation%2520can%2520be%2520effectively%2520accomplished%2520through%2520the%2520dispersion%2520of%250Aembeddings%252C%2520where%2520unrelated%2520vectors%2520are%2520pushed%2520apart%2520as%2520much%2520as%2520possible.%2520By%250Aconstraining%2520features%2520to%2520be%2520on%2520a%2520hypersphere%252C%2520we%2520can%2520connect%2520dispersion%2520to%250Awell-studied%2520problems%2520in%2520mathematics%2520and%2520physics%252C%2520where%2520optimal%2520solutions%2520are%250Aknown%2520for%2520limited%2520low-dimensional%2520cases.%2520However%252C%2520in%2520representation%2520learning%2520we%250Atypically%2520deal%2520with%2520a%2520large%2520number%2520of%2520features%2520in%2520high-dimensional%2520space%252C%2520and%250Amoreover%252C%2520dispersion%2520is%2520usually%2520traded%2520off%2520with%2520some%2520other%2520task-oriented%250Atraining%2520objective%252C%2520making%2520existing%2520theoretical%2520and%2520numerical%2520solutions%250Ainapplicable.%2520Therefore%252C%2520it%2520is%2520common%2520to%2520rely%2520on%2520gradient-based%2520methods%2520to%250Aencourage%2520dispersion%252C%2520usually%2520by%2520minimizing%2520some%2520function%2520of%2520the%2520pairwise%250Adistances.%2520In%2520this%2520work%252C%2520we%2520first%2520give%2520an%2520overview%2520of%2520existing%2520methods%2520from%250Adisconnected%2520literature%252C%2520making%2520new%2520connections%2520and%2520highlighting%2520similarities.%250ANext%252C%2520we%2520introduce%2520some%2520new%2520angles.%2520We%2520propose%2520to%2520reinterpret%2520pairwise%250Adispersion%2520using%2520a%2520maximum%2520mean%2520discrepancy%2520%2528MMD%2529%2520motivation.%2520We%2520then%2520propose%250Aan%2520online%2520variant%2520of%2520the%2520celebrated%2520Lloyd%2527s%2520algorithm%252C%2520of%2520K-Means%2520fame%252C%2520as%2520an%250Aeffective%2520alternative%2520regularizer%2520for%2520dispersion%2520on%2520generic%2520domains.%2520Finally%252C%250Awe%2520derive%2520a%2520novel%2520dispersion%2520method%2520that%2520directly%2520exploits%2520properties%2520of%2520the%250Ahypersphere.%2520Our%2520experiments%2520show%2520the%2520importance%2520of%2520dispersion%2520in%2520image%250Aclassification%2520and%2520natural%2520language%2520processing%2520tasks%252C%2520and%2520how%2520algorithms%250Aexhibit%2520different%2520trade-offs%2520in%2520different%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08231v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keep%20your%20distance%3A%20learning%20dispersed%20embeddings%20on%20%24%5Cmathbb%7BS%7D_m%24&entry.906535625=Evgeniia%20Tokarchuk%20and%20Hua%20Chang%20Bakker%20and%20Vlad%20Niculae&entry.1292438233=%20%20Learning%20well-separated%20features%20in%20high-dimensional%20spaces%2C%20such%20as%20text%20or%0Aimage%20embeddings%2C%20is%20crucial%20for%20many%20machine%20learning%20applications.%20Achieving%0Asuch%20separation%20can%20be%20effectively%20accomplished%20through%20the%20dispersion%20of%0Aembeddings%2C%20where%20unrelated%20vectors%20are%20pushed%20apart%20as%20much%20as%20possible.%20By%0Aconstraining%20features%20to%20be%20on%20a%20hypersphere%2C%20we%20can%20connect%20dispersion%20to%0Awell-studied%20problems%20in%20mathematics%20and%20physics%2C%20where%20optimal%20solutions%20are%0Aknown%20for%20limited%20low-dimensional%20cases.%20However%2C%20in%20representation%20learning%20we%0Atypically%20deal%20with%20a%20large%20number%20of%20features%20in%20high-dimensional%20space%2C%20and%0Amoreover%2C%20dispersion%20is%20usually%20traded%20off%20with%20some%20other%20task-oriented%0Atraining%20objective%2C%20making%20existing%20theoretical%20and%20numerical%20solutions%0Ainapplicable.%20Therefore%2C%20it%20is%20common%20to%20rely%20on%20gradient-based%20methods%20to%0Aencourage%20dispersion%2C%20usually%20by%20minimizing%20some%20function%20of%20the%20pairwise%0Adistances.%20In%20this%20work%2C%20we%20first%20give%20an%20overview%20of%20existing%20methods%20from%0Adisconnected%20literature%2C%20making%20new%20connections%20and%20highlighting%20similarities.%0ANext%2C%20we%20introduce%20some%20new%20angles.%20We%20propose%20to%20reinterpret%20pairwise%0Adispersion%20using%20a%20maximum%20mean%20discrepancy%20%28MMD%29%20motivation.%20We%20then%20propose%0Aan%20online%20variant%20of%20the%20celebrated%20Lloyd%27s%20algorithm%2C%20of%20K-Means%20fame%2C%20as%20an%0Aeffective%20alternative%20regularizer%20for%20dispersion%20on%20generic%20domains.%20Finally%2C%0Awe%20derive%20a%20novel%20dispersion%20method%20that%20directly%20exploits%20properties%20of%20the%0Ahypersphere.%20Our%20experiments%20show%20the%20importance%20of%20dispersion%20in%20image%0Aclassification%20and%20natural%20language%20processing%20tasks%2C%20and%20how%20algorithms%0Aexhibit%20different%20trade-offs%20in%20different%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08231v3&entry.124074799=Read"},
{"title": "Assessing the Chemical Intelligence of Large Language Models", "author": "Nicholas T. Runcie and Charlotte M. Deane and Fergus Imrie", "abstract": "  Large Language Models are versatile, general-purpose tools with a wide range\nof applications. Recently, the advent of \"reasoning models\" has led to\nsubstantial improvements in their abilities in advanced problem-solving domains\nsuch as mathematics and software engineering. In this work, we assessed the\nability of reasoning models to directly perform chemistry tasks, without any\nassistance from external tools. We created a novel benchmark, called ChemIQ,\nwhich consists of 796 questions assessing core concepts in organic chemistry,\nfocused on molecular comprehension and chemical reasoning. Unlike previous\nbenchmarks, which primarily use multiple choice formats, our approach requires\nmodels to construct short-answer responses, more closely reflecting real-world\napplications. The reasoning models, exemplified by OpenAI's o3-mini, correctly\nanswered 28%-59% of questions depending on the reasoning level used, with\nhigher reasoning levels significantly increasing performance on all tasks.\nThese models substantially outperformed the non-reasoning model, GPT-4o, which\nachieved only 7% accuracy. We found that Large Language Models can now convert\nSMILES strings to IUPAC names, a task earlier models were unable to perform.\nAdditionally, we show that the latest reasoning models can elucidate structures\nfrom 1H and 13C NMR data, correctly generating SMILES strings for 74% of\nmolecules containing up to 10 heavy atoms, and in one case solving a structure\ncomprising 21 heavy atoms. For each task, we found evidence that the reasoning\nprocess mirrors that of a human chemist. Our results demonstrate that the\nlatest reasoning models have the ability to perform advanced chemical\nreasoning.\n", "link": "http://arxiv.org/abs/2505.07735v1", "date": "2025-05-12", "relevancy": 1.9717, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Chemical%20Intelligence%20of%20Large%20Language%20Models&body=Title%3A%20Assessing%20the%20Chemical%20Intelligence%20of%20Large%20Language%20Models%0AAuthor%3A%20Nicholas%20T.%20Runcie%20and%20Charlotte%20M.%20Deane%20and%20Fergus%20Imrie%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20versatile%2C%20general-purpose%20tools%20with%20a%20wide%20range%0Aof%20applications.%20Recently%2C%20the%20advent%20of%20%22reasoning%20models%22%20has%20led%20to%0Asubstantial%20improvements%20in%20their%20abilities%20in%20advanced%20problem-solving%20domains%0Asuch%20as%20mathematics%20and%20software%20engineering.%20In%20this%20work%2C%20we%20assessed%20the%0Aability%20of%20reasoning%20models%20to%20directly%20perform%20chemistry%20tasks%2C%20without%20any%0Aassistance%20from%20external%20tools.%20We%20created%20a%20novel%20benchmark%2C%20called%20ChemIQ%2C%0Awhich%20consists%20of%20796%20questions%20assessing%20core%20concepts%20in%20organic%20chemistry%2C%0Afocused%20on%20molecular%20comprehension%20and%20chemical%20reasoning.%20Unlike%20previous%0Abenchmarks%2C%20which%20primarily%20use%20multiple%20choice%20formats%2C%20our%20approach%20requires%0Amodels%20to%20construct%20short-answer%20responses%2C%20more%20closely%20reflecting%20real-world%0Aapplications.%20The%20reasoning%20models%2C%20exemplified%20by%20OpenAI%27s%20o3-mini%2C%20correctly%0Aanswered%2028%25-59%25%20of%20questions%20depending%20on%20the%20reasoning%20level%20used%2C%20with%0Ahigher%20reasoning%20levels%20significantly%20increasing%20performance%20on%20all%20tasks.%0AThese%20models%20substantially%20outperformed%20the%20non-reasoning%20model%2C%20GPT-4o%2C%20which%0Aachieved%20only%207%25%20accuracy.%20We%20found%20that%20Large%20Language%20Models%20can%20now%20convert%0ASMILES%20strings%20to%20IUPAC%20names%2C%20a%20task%20earlier%20models%20were%20unable%20to%20perform.%0AAdditionally%2C%20we%20show%20that%20the%20latest%20reasoning%20models%20can%20elucidate%20structures%0Afrom%201H%20and%2013C%20NMR%20data%2C%20correctly%20generating%20SMILES%20strings%20for%2074%25%20of%0Amolecules%20containing%20up%20to%2010%20heavy%20atoms%2C%20and%20in%20one%20case%20solving%20a%20structure%0Acomprising%2021%20heavy%20atoms.%20For%20each%20task%2C%20we%20found%20evidence%20that%20the%20reasoning%0Aprocess%20mirrors%20that%20of%20a%20human%20chemist.%20Our%20results%20demonstrate%20that%20the%0Alatest%20reasoning%20models%20have%20the%20ability%20to%20perform%20advanced%20chemical%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Chemical%2520Intelligence%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DNicholas%2520T.%2520Runcie%2520and%2520Charlotte%2520M.%2520Deane%2520and%2520Fergus%2520Imrie%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520versatile%252C%2520general-purpose%2520tools%2520with%2520a%2520wide%2520range%250Aof%2520applications.%2520Recently%252C%2520the%2520advent%2520of%2520%2522reasoning%2520models%2522%2520has%2520led%2520to%250Asubstantial%2520improvements%2520in%2520their%2520abilities%2520in%2520advanced%2520problem-solving%2520domains%250Asuch%2520as%2520mathematics%2520and%2520software%2520engineering.%2520In%2520this%2520work%252C%2520we%2520assessed%2520the%250Aability%2520of%2520reasoning%2520models%2520to%2520directly%2520perform%2520chemistry%2520tasks%252C%2520without%2520any%250Aassistance%2520from%2520external%2520tools.%2520We%2520created%2520a%2520novel%2520benchmark%252C%2520called%2520ChemIQ%252C%250Awhich%2520consists%2520of%2520796%2520questions%2520assessing%2520core%2520concepts%2520in%2520organic%2520chemistry%252C%250Afocused%2520on%2520molecular%2520comprehension%2520and%2520chemical%2520reasoning.%2520Unlike%2520previous%250Abenchmarks%252C%2520which%2520primarily%2520use%2520multiple%2520choice%2520formats%252C%2520our%2520approach%2520requires%250Amodels%2520to%2520construct%2520short-answer%2520responses%252C%2520more%2520closely%2520reflecting%2520real-world%250Aapplications.%2520The%2520reasoning%2520models%252C%2520exemplified%2520by%2520OpenAI%2527s%2520o3-mini%252C%2520correctly%250Aanswered%252028%2525-59%2525%2520of%2520questions%2520depending%2520on%2520the%2520reasoning%2520level%2520used%252C%2520with%250Ahigher%2520reasoning%2520levels%2520significantly%2520increasing%2520performance%2520on%2520all%2520tasks.%250AThese%2520models%2520substantially%2520outperformed%2520the%2520non-reasoning%2520model%252C%2520GPT-4o%252C%2520which%250Aachieved%2520only%25207%2525%2520accuracy.%2520We%2520found%2520that%2520Large%2520Language%2520Models%2520can%2520now%2520convert%250ASMILES%2520strings%2520to%2520IUPAC%2520names%252C%2520a%2520task%2520earlier%2520models%2520were%2520unable%2520to%2520perform.%250AAdditionally%252C%2520we%2520show%2520that%2520the%2520latest%2520reasoning%2520models%2520can%2520elucidate%2520structures%250Afrom%25201H%2520and%252013C%2520NMR%2520data%252C%2520correctly%2520generating%2520SMILES%2520strings%2520for%252074%2525%2520of%250Amolecules%2520containing%2520up%2520to%252010%2520heavy%2520atoms%252C%2520and%2520in%2520one%2520case%2520solving%2520a%2520structure%250Acomprising%252021%2520heavy%2520atoms.%2520For%2520each%2520task%252C%2520we%2520found%2520evidence%2520that%2520the%2520reasoning%250Aprocess%2520mirrors%2520that%2520of%2520a%2520human%2520chemist.%2520Our%2520results%2520demonstrate%2520that%2520the%250Alatest%2520reasoning%2520models%2520have%2520the%2520ability%2520to%2520perform%2520advanced%2520chemical%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Chemical%20Intelligence%20of%20Large%20Language%20Models&entry.906535625=Nicholas%20T.%20Runcie%20and%20Charlotte%20M.%20Deane%20and%20Fergus%20Imrie&entry.1292438233=%20%20Large%20Language%20Models%20are%20versatile%2C%20general-purpose%20tools%20with%20a%20wide%20range%0Aof%20applications.%20Recently%2C%20the%20advent%20of%20%22reasoning%20models%22%20has%20led%20to%0Asubstantial%20improvements%20in%20their%20abilities%20in%20advanced%20problem-solving%20domains%0Asuch%20as%20mathematics%20and%20software%20engineering.%20In%20this%20work%2C%20we%20assessed%20the%0Aability%20of%20reasoning%20models%20to%20directly%20perform%20chemistry%20tasks%2C%20without%20any%0Aassistance%20from%20external%20tools.%20We%20created%20a%20novel%20benchmark%2C%20called%20ChemIQ%2C%0Awhich%20consists%20of%20796%20questions%20assessing%20core%20concepts%20in%20organic%20chemistry%2C%0Afocused%20on%20molecular%20comprehension%20and%20chemical%20reasoning.%20Unlike%20previous%0Abenchmarks%2C%20which%20primarily%20use%20multiple%20choice%20formats%2C%20our%20approach%20requires%0Amodels%20to%20construct%20short-answer%20responses%2C%20more%20closely%20reflecting%20real-world%0Aapplications.%20The%20reasoning%20models%2C%20exemplified%20by%20OpenAI%27s%20o3-mini%2C%20correctly%0Aanswered%2028%25-59%25%20of%20questions%20depending%20on%20the%20reasoning%20level%20used%2C%20with%0Ahigher%20reasoning%20levels%20significantly%20increasing%20performance%20on%20all%20tasks.%0AThese%20models%20substantially%20outperformed%20the%20non-reasoning%20model%2C%20GPT-4o%2C%20which%0Aachieved%20only%207%25%20accuracy.%20We%20found%20that%20Large%20Language%20Models%20can%20now%20convert%0ASMILES%20strings%20to%20IUPAC%20names%2C%20a%20task%20earlier%20models%20were%20unable%20to%20perform.%0AAdditionally%2C%20we%20show%20that%20the%20latest%20reasoning%20models%20can%20elucidate%20structures%0Afrom%201H%20and%2013C%20NMR%20data%2C%20correctly%20generating%20SMILES%20strings%20for%2074%25%20of%0Amolecules%20containing%20up%20to%2010%20heavy%20atoms%2C%20and%20in%20one%20case%20solving%20a%20structure%0Acomprising%2021%20heavy%20atoms.%20For%20each%20task%2C%20we%20found%20evidence%20that%20the%20reasoning%0Aprocess%20mirrors%20that%20of%20a%20human%20chemist.%20Our%20results%20demonstrate%20that%20the%0Alatest%20reasoning%20models%20have%20the%20ability%20to%20perform%20advanced%20chemical%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07735v1&entry.124074799=Read"},
{"title": "Finite-Sample-Based Reachability for Safe Control with Gaussian Process\n  Dynamics", "author": "Manish Prajapat and Johannes K\u00f6hler and Amon Lahr and Andreas Krause and Melanie N. Zeilinger", "abstract": "  Gaussian Process (GP) regression is shown to be effective for learning\nunknown dynamics, enabling efficient and safety-aware control strategies across\ndiverse applications. However, existing GP-based model predictive control\n(GP-MPC) methods either rely on approximations, thus lacking guarantees, or are\noverly conservative, which limits their practical utility. To close this gap,\nwe present a sampling-based framework that efficiently propagates the model's\nepistemic uncertainty while avoiding conservatism. We establish a novel sample\ncomplexity result that enables the construction of a reachable set using a\nfinite number of dynamics functions sampled from the GP posterior. Building on\nthis, we design a sampling-based GP-MPC scheme that is recursively feasible and\nguarantees closed-loop safety and stability with high probability. Finally, we\nshowcase the effectiveness of our method on two numerical examples,\nhighlighting accurate reachable set over-approximation and safe closed-loop\nperformance.\n", "link": "http://arxiv.org/abs/2505.07594v1", "date": "2025-05-12", "relevancy": 1.9686, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5056}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5036}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite-Sample-Based%20Reachability%20for%20Safe%20Control%20with%20Gaussian%20Process%0A%20%20Dynamics&body=Title%3A%20Finite-Sample-Based%20Reachability%20for%20Safe%20Control%20with%20Gaussian%20Process%0A%20%20Dynamics%0AAuthor%3A%20Manish%20Prajapat%20and%20Johannes%20K%C3%B6hler%20and%20Amon%20Lahr%20and%20Andreas%20Krause%20and%20Melanie%20N.%20Zeilinger%0AAbstract%3A%20%20%20Gaussian%20Process%20%28GP%29%20regression%20is%20shown%20to%20be%20effective%20for%20learning%0Aunknown%20dynamics%2C%20enabling%20efficient%20and%20safety-aware%20control%20strategies%20across%0Adiverse%20applications.%20However%2C%20existing%20GP-based%20model%20predictive%20control%0A%28GP-MPC%29%20methods%20either%20rely%20on%20approximations%2C%20thus%20lacking%20guarantees%2C%20or%20are%0Aoverly%20conservative%2C%20which%20limits%20their%20practical%20utility.%20To%20close%20this%20gap%2C%0Awe%20present%20a%20sampling-based%20framework%20that%20efficiently%20propagates%20the%20model%27s%0Aepistemic%20uncertainty%20while%20avoiding%20conservatism.%20We%20establish%20a%20novel%20sample%0Acomplexity%20result%20that%20enables%20the%20construction%20of%20a%20reachable%20set%20using%20a%0Afinite%20number%20of%20dynamics%20functions%20sampled%20from%20the%20GP%20posterior.%20Building%20on%0Athis%2C%20we%20design%20a%20sampling-based%20GP-MPC%20scheme%20that%20is%20recursively%20feasible%20and%0Aguarantees%20closed-loop%20safety%20and%20stability%20with%20high%20probability.%20Finally%2C%20we%0Ashowcase%20the%20effectiveness%20of%20our%20method%20on%20two%20numerical%20examples%2C%0Ahighlighting%20accurate%20reachable%20set%20over-approximation%20and%20safe%20closed-loop%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite-Sample-Based%2520Reachability%2520for%2520Safe%2520Control%2520with%2520Gaussian%2520Process%250A%2520%2520Dynamics%26entry.906535625%3DManish%2520Prajapat%2520and%2520Johannes%2520K%25C3%25B6hler%2520and%2520Amon%2520Lahr%2520and%2520Andreas%2520Krause%2520and%2520Melanie%2520N.%2520Zeilinger%26entry.1292438233%3D%2520%2520Gaussian%2520Process%2520%2528GP%2529%2520regression%2520is%2520shown%2520to%2520be%2520effective%2520for%2520learning%250Aunknown%2520dynamics%252C%2520enabling%2520efficient%2520and%2520safety-aware%2520control%2520strategies%2520across%250Adiverse%2520applications.%2520However%252C%2520existing%2520GP-based%2520model%2520predictive%2520control%250A%2528GP-MPC%2529%2520methods%2520either%2520rely%2520on%2520approximations%252C%2520thus%2520lacking%2520guarantees%252C%2520or%2520are%250Aoverly%2520conservative%252C%2520which%2520limits%2520their%2520practical%2520utility.%2520To%2520close%2520this%2520gap%252C%250Awe%2520present%2520a%2520sampling-based%2520framework%2520that%2520efficiently%2520propagates%2520the%2520model%2527s%250Aepistemic%2520uncertainty%2520while%2520avoiding%2520conservatism.%2520We%2520establish%2520a%2520novel%2520sample%250Acomplexity%2520result%2520that%2520enables%2520the%2520construction%2520of%2520a%2520reachable%2520set%2520using%2520a%250Afinite%2520number%2520of%2520dynamics%2520functions%2520sampled%2520from%2520the%2520GP%2520posterior.%2520Building%2520on%250Athis%252C%2520we%2520design%2520a%2520sampling-based%2520GP-MPC%2520scheme%2520that%2520is%2520recursively%2520feasible%2520and%250Aguarantees%2520closed-loop%2520safety%2520and%2520stability%2520with%2520high%2520probability.%2520Finally%252C%2520we%250Ashowcase%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520two%2520numerical%2520examples%252C%250Ahighlighting%2520accurate%2520reachable%2520set%2520over-approximation%2520and%2520safe%2520closed-loop%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite-Sample-Based%20Reachability%20for%20Safe%20Control%20with%20Gaussian%20Process%0A%20%20Dynamics&entry.906535625=Manish%20Prajapat%20and%20Johannes%20K%C3%B6hler%20and%20Amon%20Lahr%20and%20Andreas%20Krause%20and%20Melanie%20N.%20Zeilinger&entry.1292438233=%20%20Gaussian%20Process%20%28GP%29%20regression%20is%20shown%20to%20be%20effective%20for%20learning%0Aunknown%20dynamics%2C%20enabling%20efficient%20and%20safety-aware%20control%20strategies%20across%0Adiverse%20applications.%20However%2C%20existing%20GP-based%20model%20predictive%20control%0A%28GP-MPC%29%20methods%20either%20rely%20on%20approximations%2C%20thus%20lacking%20guarantees%2C%20or%20are%0Aoverly%20conservative%2C%20which%20limits%20their%20practical%20utility.%20To%20close%20this%20gap%2C%0Awe%20present%20a%20sampling-based%20framework%20that%20efficiently%20propagates%20the%20model%27s%0Aepistemic%20uncertainty%20while%20avoiding%20conservatism.%20We%20establish%20a%20novel%20sample%0Acomplexity%20result%20that%20enables%20the%20construction%20of%20a%20reachable%20set%20using%20a%0Afinite%20number%20of%20dynamics%20functions%20sampled%20from%20the%20GP%20posterior.%20Building%20on%0Athis%2C%20we%20design%20a%20sampling-based%20GP-MPC%20scheme%20that%20is%20recursively%20feasible%20and%0Aguarantees%20closed-loop%20safety%20and%20stability%20with%20high%20probability.%20Finally%2C%20we%0Ashowcase%20the%20effectiveness%20of%20our%20method%20on%20two%20numerical%20examples%2C%0Ahighlighting%20accurate%20reachable%20set%20over-approximation%20and%20safe%20closed-loop%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07594v1&entry.124074799=Read"},
{"title": "Characterizing the Investigative Methods of Fictional Detectives with\n  Large Language Models", "author": "Edirlei Soares de Lima and Marco A. Casanova and Bruno Feij\u00f3 and Antonio L. Furtado", "abstract": "  Detective fiction, a genre defined by its complex narrative structures and\ncharacter-driven storytelling, presents unique challenges for computational\nnarratology, a research field focused on integrating literary theory into\nautomated narrative generation. While traditional literary studies have offered\ndeep insights into the methods and archetypes of fictional detectives, these\nanalyses often focus on a limited number of characters and lack the scalability\nneeded for the extraction of unique traits that can be used to guide narrative\ngeneration methods. In this paper, we present an AI-driven approach for\nsystematically characterizing the investigative methods of fictional\ndetectives. Our multi-phase workflow explores the capabilities of 15 Large\nLanguage Models (LLMs) to extract, synthesize, and validate distinctive\ninvestigative traits of fictional detectives. This approach was tested on a\ndiverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,\nWilliam Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -\ncapturing the distinctive investigative styles that define each character. The\nidentified traits were validated against existing literary analyses and further\ntested in a reverse identification phase, achieving an overall accuracy of\n91.43%, demonstrating the method's effectiveness in capturing the distinctive\ninvestigative approaches of each detective. This work contributes to the\nbroader field of computational narratology by providing a scalable framework\nfor character analysis, with potential applications in AI-driven interactive\nstorytelling and automated narrative generation.\n", "link": "http://arxiv.org/abs/2505.07601v1", "date": "2025-05-12", "relevancy": 1.9656, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20the%20Investigative%20Methods%20of%20Fictional%20Detectives%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20Characterizing%20the%20Investigative%20Methods%20of%20Fictional%20Detectives%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Edirlei%20Soares%20de%20Lima%20and%20Marco%20A.%20Casanova%20and%20Bruno%20Feij%C3%B3%20and%20Antonio%20L.%20Furtado%0AAbstract%3A%20%20%20Detective%20fiction%2C%20a%20genre%20defined%20by%20its%20complex%20narrative%20structures%20and%0Acharacter-driven%20storytelling%2C%20presents%20unique%20challenges%20for%20computational%0Anarratology%2C%20a%20research%20field%20focused%20on%20integrating%20literary%20theory%20into%0Aautomated%20narrative%20generation.%20While%20traditional%20literary%20studies%20have%20offered%0Adeep%20insights%20into%20the%20methods%20and%20archetypes%20of%20fictional%20detectives%2C%20these%0Aanalyses%20often%20focus%20on%20a%20limited%20number%20of%20characters%20and%20lack%20the%20scalability%0Aneeded%20for%20the%20extraction%20of%20unique%20traits%20that%20can%20be%20used%20to%20guide%20narrative%0Ageneration%20methods.%20In%20this%20paper%2C%20we%20present%20an%20AI-driven%20approach%20for%0Asystematically%20characterizing%20the%20investigative%20methods%20of%20fictional%0Adetectives.%20Our%20multi-phase%20workflow%20explores%20the%20capabilities%20of%2015%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20extract%2C%20synthesize%2C%20and%20validate%20distinctive%0Ainvestigative%20traits%20of%20fictional%20detectives.%20This%20approach%20was%20tested%20on%20a%0Adiverse%20set%20of%20seven%20iconic%20detectives%20-%20Hercule%20Poirot%2C%20Sherlock%20Holmes%2C%0AWilliam%20Murdoch%2C%20Columbo%2C%20Father%20Brown%2C%20Miss%20Marple%2C%20and%20Auguste%20Dupin%20-%0Acapturing%20the%20distinctive%20investigative%20styles%20that%20define%20each%20character.%20The%0Aidentified%20traits%20were%20validated%20against%20existing%20literary%20analyses%20and%20further%0Atested%20in%20a%20reverse%20identification%20phase%2C%20achieving%20an%20overall%20accuracy%20of%0A91.43%25%2C%20demonstrating%20the%20method%27s%20effectiveness%20in%20capturing%20the%20distinctive%0Ainvestigative%20approaches%20of%20each%20detective.%20This%20work%20contributes%20to%20the%0Abroader%20field%20of%20computational%20narratology%20by%20providing%20a%20scalable%20framework%0Afor%20character%20analysis%2C%20with%20potential%20applications%20in%20AI-driven%20interactive%0Astorytelling%20and%20automated%20narrative%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520the%2520Investigative%2520Methods%2520of%2520Fictional%2520Detectives%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DEdirlei%2520Soares%2520de%2520Lima%2520and%2520Marco%2520A.%2520Casanova%2520and%2520Bruno%2520Feij%25C3%25B3%2520and%2520Antonio%2520L.%2520Furtado%26entry.1292438233%3D%2520%2520Detective%2520fiction%252C%2520a%2520genre%2520defined%2520by%2520its%2520complex%2520narrative%2520structures%2520and%250Acharacter-driven%2520storytelling%252C%2520presents%2520unique%2520challenges%2520for%2520computational%250Anarratology%252C%2520a%2520research%2520field%2520focused%2520on%2520integrating%2520literary%2520theory%2520into%250Aautomated%2520narrative%2520generation.%2520While%2520traditional%2520literary%2520studies%2520have%2520offered%250Adeep%2520insights%2520into%2520the%2520methods%2520and%2520archetypes%2520of%2520fictional%2520detectives%252C%2520these%250Aanalyses%2520often%2520focus%2520on%2520a%2520limited%2520number%2520of%2520characters%2520and%2520lack%2520the%2520scalability%250Aneeded%2520for%2520the%2520extraction%2520of%2520unique%2520traits%2520that%2520can%2520be%2520used%2520to%2520guide%2520narrative%250Ageneration%2520methods.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520AI-driven%2520approach%2520for%250Asystematically%2520characterizing%2520the%2520investigative%2520methods%2520of%2520fictional%250Adetectives.%2520Our%2520multi-phase%2520workflow%2520explores%2520the%2520capabilities%2520of%252015%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520extract%252C%2520synthesize%252C%2520and%2520validate%2520distinctive%250Ainvestigative%2520traits%2520of%2520fictional%2520detectives.%2520This%2520approach%2520was%2520tested%2520on%2520a%250Adiverse%2520set%2520of%2520seven%2520iconic%2520detectives%2520-%2520Hercule%2520Poirot%252C%2520Sherlock%2520Holmes%252C%250AWilliam%2520Murdoch%252C%2520Columbo%252C%2520Father%2520Brown%252C%2520Miss%2520Marple%252C%2520and%2520Auguste%2520Dupin%2520-%250Acapturing%2520the%2520distinctive%2520investigative%2520styles%2520that%2520define%2520each%2520character.%2520The%250Aidentified%2520traits%2520were%2520validated%2520against%2520existing%2520literary%2520analyses%2520and%2520further%250Atested%2520in%2520a%2520reverse%2520identification%2520phase%252C%2520achieving%2520an%2520overall%2520accuracy%2520of%250A91.43%2525%252C%2520demonstrating%2520the%2520method%2527s%2520effectiveness%2520in%2520capturing%2520the%2520distinctive%250Ainvestigative%2520approaches%2520of%2520each%2520detective.%2520This%2520work%2520contributes%2520to%2520the%250Abroader%2520field%2520of%2520computational%2520narratology%2520by%2520providing%2520a%2520scalable%2520framework%250Afor%2520character%2520analysis%252C%2520with%2520potential%2520applications%2520in%2520AI-driven%2520interactive%250Astorytelling%2520and%2520automated%2520narrative%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20the%20Investigative%20Methods%20of%20Fictional%20Detectives%20with%0A%20%20Large%20Language%20Models&entry.906535625=Edirlei%20Soares%20de%20Lima%20and%20Marco%20A.%20Casanova%20and%20Bruno%20Feij%C3%B3%20and%20Antonio%20L.%20Furtado&entry.1292438233=%20%20Detective%20fiction%2C%20a%20genre%20defined%20by%20its%20complex%20narrative%20structures%20and%0Acharacter-driven%20storytelling%2C%20presents%20unique%20challenges%20for%20computational%0Anarratology%2C%20a%20research%20field%20focused%20on%20integrating%20literary%20theory%20into%0Aautomated%20narrative%20generation.%20While%20traditional%20literary%20studies%20have%20offered%0Adeep%20insights%20into%20the%20methods%20and%20archetypes%20of%20fictional%20detectives%2C%20these%0Aanalyses%20often%20focus%20on%20a%20limited%20number%20of%20characters%20and%20lack%20the%20scalability%0Aneeded%20for%20the%20extraction%20of%20unique%20traits%20that%20can%20be%20used%20to%20guide%20narrative%0Ageneration%20methods.%20In%20this%20paper%2C%20we%20present%20an%20AI-driven%20approach%20for%0Asystematically%20characterizing%20the%20investigative%20methods%20of%20fictional%0Adetectives.%20Our%20multi-phase%20workflow%20explores%20the%20capabilities%20of%2015%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20extract%2C%20synthesize%2C%20and%20validate%20distinctive%0Ainvestigative%20traits%20of%20fictional%20detectives.%20This%20approach%20was%20tested%20on%20a%0Adiverse%20set%20of%20seven%20iconic%20detectives%20-%20Hercule%20Poirot%2C%20Sherlock%20Holmes%2C%0AWilliam%20Murdoch%2C%20Columbo%2C%20Father%20Brown%2C%20Miss%20Marple%2C%20and%20Auguste%20Dupin%20-%0Acapturing%20the%20distinctive%20investigative%20styles%20that%20define%20each%20character.%20The%0Aidentified%20traits%20were%20validated%20against%20existing%20literary%20analyses%20and%20further%0Atested%20in%20a%20reverse%20identification%20phase%2C%20achieving%20an%20overall%20accuracy%20of%0A91.43%25%2C%20demonstrating%20the%20method%27s%20effectiveness%20in%20capturing%20the%20distinctive%0Ainvestigative%20approaches%20of%20each%20detective.%20This%20work%20contributes%20to%20the%0Abroader%20field%20of%20computational%20narratology%20by%20providing%20a%20scalable%20framework%0Afor%20character%20analysis%2C%20with%20potential%20applications%20in%20AI-driven%20interactive%0Astorytelling%20and%20automated%20narrative%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07601v1&entry.124074799=Read"},
{"title": "Can Generative AI agents behave like humans? Evidence from laboratory\n  market experiments", "author": "R. Maria del Rio-Chanona and Marco Pangallo and Cars Hommes", "abstract": "  We explore the potential of Large Language Models (LLMs) to replicate human\nbehavior in economic market experiments. Compared to previous studies, we focus\non dynamic feedback between LLM agents: the decisions of each LLM impact the\nmarket price at the current step, and so affect the decisions of the other LLMs\nat the next step. We compare LLM behavior to market dynamics observed in\nlaboratory settings and assess their alignment with human participants'\nbehavior. Our findings indicate that LLMs do not adhere strictly to rational\nexpectations, displaying instead bounded rationality, similarly to human\nparticipants. Providing a minimal context window i.e. memory of three previous\ntime steps, combined with a high variability setting capturing response\nheterogeneity, allows LLMs to replicate broad trends seen in human experiments,\nsuch as the distinction between positive and negative feedback markets.\nHowever, differences remain at a granular level--LLMs exhibit less\nheterogeneity in behavior than humans. These results suggest that LLMs hold\npromise as tools for simulating realistic human behavior in economic contexts,\nthough further research is needed to refine their accuracy and increase\nbehavioral diversity.\n", "link": "http://arxiv.org/abs/2505.07457v1", "date": "2025-05-12", "relevancy": 1.9609, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5148}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4842}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Generative%20AI%20agents%20behave%20like%20humans%3F%20Evidence%20from%20laboratory%0A%20%20market%20experiments&body=Title%3A%20Can%20Generative%20AI%20agents%20behave%20like%20humans%3F%20Evidence%20from%20laboratory%0A%20%20market%20experiments%0AAuthor%3A%20R.%20Maria%20del%20Rio-Chanona%20and%20Marco%20Pangallo%20and%20Cars%20Hommes%0AAbstract%3A%20%20%20We%20explore%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20replicate%20human%0Abehavior%20in%20economic%20market%20experiments.%20Compared%20to%20previous%20studies%2C%20we%20focus%0Aon%20dynamic%20feedback%20between%20LLM%20agents%3A%20the%20decisions%20of%20each%20LLM%20impact%20the%0Amarket%20price%20at%20the%20current%20step%2C%20and%20so%20affect%20the%20decisions%20of%20the%20other%20LLMs%0Aat%20the%20next%20step.%20We%20compare%20LLM%20behavior%20to%20market%20dynamics%20observed%20in%0Alaboratory%20settings%20and%20assess%20their%20alignment%20with%20human%20participants%27%0Abehavior.%20Our%20findings%20indicate%20that%20LLMs%20do%20not%20adhere%20strictly%20to%20rational%0Aexpectations%2C%20displaying%20instead%20bounded%20rationality%2C%20similarly%20to%20human%0Aparticipants.%20Providing%20a%20minimal%20context%20window%20i.e.%20memory%20of%20three%20previous%0Atime%20steps%2C%20combined%20with%20a%20high%20variability%20setting%20capturing%20response%0Aheterogeneity%2C%20allows%20LLMs%20to%20replicate%20broad%20trends%20seen%20in%20human%20experiments%2C%0Asuch%20as%20the%20distinction%20between%20positive%20and%20negative%20feedback%20markets.%0AHowever%2C%20differences%20remain%20at%20a%20granular%20level--LLMs%20exhibit%20less%0Aheterogeneity%20in%20behavior%20than%20humans.%20These%20results%20suggest%20that%20LLMs%20hold%0Apromise%20as%20tools%20for%20simulating%20realistic%20human%20behavior%20in%20economic%20contexts%2C%0Athough%20further%20research%20is%20needed%20to%20refine%20their%20accuracy%20and%20increase%0Abehavioral%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Generative%2520AI%2520agents%2520behave%2520like%2520humans%253F%2520Evidence%2520from%2520laboratory%250A%2520%2520market%2520experiments%26entry.906535625%3DR.%2520Maria%2520del%2520Rio-Chanona%2520and%2520Marco%2520Pangallo%2520and%2520Cars%2520Hommes%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520replicate%2520human%250Abehavior%2520in%2520economic%2520market%2520experiments.%2520Compared%2520to%2520previous%2520studies%252C%2520we%2520focus%250Aon%2520dynamic%2520feedback%2520between%2520LLM%2520agents%253A%2520the%2520decisions%2520of%2520each%2520LLM%2520impact%2520the%250Amarket%2520price%2520at%2520the%2520current%2520step%252C%2520and%2520so%2520affect%2520the%2520decisions%2520of%2520the%2520other%2520LLMs%250Aat%2520the%2520next%2520step.%2520We%2520compare%2520LLM%2520behavior%2520to%2520market%2520dynamics%2520observed%2520in%250Alaboratory%2520settings%2520and%2520assess%2520their%2520alignment%2520with%2520human%2520participants%2527%250Abehavior.%2520Our%2520findings%2520indicate%2520that%2520LLMs%2520do%2520not%2520adhere%2520strictly%2520to%2520rational%250Aexpectations%252C%2520displaying%2520instead%2520bounded%2520rationality%252C%2520similarly%2520to%2520human%250Aparticipants.%2520Providing%2520a%2520minimal%2520context%2520window%2520i.e.%2520memory%2520of%2520three%2520previous%250Atime%2520steps%252C%2520combined%2520with%2520a%2520high%2520variability%2520setting%2520capturing%2520response%250Aheterogeneity%252C%2520allows%2520LLMs%2520to%2520replicate%2520broad%2520trends%2520seen%2520in%2520human%2520experiments%252C%250Asuch%2520as%2520the%2520distinction%2520between%2520positive%2520and%2520negative%2520feedback%2520markets.%250AHowever%252C%2520differences%2520remain%2520at%2520a%2520granular%2520level--LLMs%2520exhibit%2520less%250Aheterogeneity%2520in%2520behavior%2520than%2520humans.%2520These%2520results%2520suggest%2520that%2520LLMs%2520hold%250Apromise%2520as%2520tools%2520for%2520simulating%2520realistic%2520human%2520behavior%2520in%2520economic%2520contexts%252C%250Athough%2520further%2520research%2520is%2520needed%2520to%2520refine%2520their%2520accuracy%2520and%2520increase%250Abehavioral%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Generative%20AI%20agents%20behave%20like%20humans%3F%20Evidence%20from%20laboratory%0A%20%20market%20experiments&entry.906535625=R.%20Maria%20del%20Rio-Chanona%20and%20Marco%20Pangallo%20and%20Cars%20Hommes&entry.1292438233=%20%20We%20explore%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20replicate%20human%0Abehavior%20in%20economic%20market%20experiments.%20Compared%20to%20previous%20studies%2C%20we%20focus%0Aon%20dynamic%20feedback%20between%20LLM%20agents%3A%20the%20decisions%20of%20each%20LLM%20impact%20the%0Amarket%20price%20at%20the%20current%20step%2C%20and%20so%20affect%20the%20decisions%20of%20the%20other%20LLMs%0Aat%20the%20next%20step.%20We%20compare%20LLM%20behavior%20to%20market%20dynamics%20observed%20in%0Alaboratory%20settings%20and%20assess%20their%20alignment%20with%20human%20participants%27%0Abehavior.%20Our%20findings%20indicate%20that%20LLMs%20do%20not%20adhere%20strictly%20to%20rational%0Aexpectations%2C%20displaying%20instead%20bounded%20rationality%2C%20similarly%20to%20human%0Aparticipants.%20Providing%20a%20minimal%20context%20window%20i.e.%20memory%20of%20three%20previous%0Atime%20steps%2C%20combined%20with%20a%20high%20variability%20setting%20capturing%20response%0Aheterogeneity%2C%20allows%20LLMs%20to%20replicate%20broad%20trends%20seen%20in%20human%20experiments%2C%0Asuch%20as%20the%20distinction%20between%20positive%20and%20negative%20feedback%20markets.%0AHowever%2C%20differences%20remain%20at%20a%20granular%20level--LLMs%20exhibit%20less%0Aheterogeneity%20in%20behavior%20than%20humans.%20These%20results%20suggest%20that%20LLMs%20hold%0Apromise%20as%20tools%20for%20simulating%20realistic%20human%20behavior%20in%20economic%20contexts%2C%0Athough%20further%20research%20is%20needed%20to%20refine%20their%20accuracy%20and%20increase%0Abehavioral%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07457v1&entry.124074799=Read"},
{"title": "LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning", "author": "Xiaotian Lin and Yanlin Qi and Yizhang Zhu and Themis Palpanas and Chengliang Chai and Nan Tang and Yuyu Luo", "abstract": "  Instruction tuning has emerged as a critical paradigm for improving the\ncapabilities and alignment of large language models (LLMs). However, existing\niterative model-aware data selection methods incur significant computational\noverhead, as they rely on repeatedly performing full-dataset model inference to\nestimate sample utility for subsequent training iterations, creating a\nfundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient\niterative data selection framework that accurately estimates sample utility\nentirely within the standard training loop, eliminating the need for costly\nadditional model inference. At its core, LEAD introduces Instance-Level Dynamic\nUncertainty (IDU), a theoretically grounded utility function combining\ninstantaneous training loss, gradient-based approximation of loss changes, and\nexponential smoothing of historical loss signals. To further scale efficiently\nto large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,\nadaptively prioritizing informative clusters through a multi-armed bandit\nmechanism, followed by precise fine-grained selection of high-utility samples\nusing IDU. Extensive experiments across four diverse benchmarks show that LEAD\nsignificantly outperforms state-of-the-art methods, improving average model\nperformance by 6.1%-10.8% while using only 2.5% of the training data and\nreducing overall training time by 5-10x.\n", "link": "http://arxiv.org/abs/2505.07437v1", "date": "2025-05-12", "relevancy": 1.9595, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5343}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAD%3A%20Iterative%20Data%20Selection%20for%20Efficient%20LLM%20Instruction%20Tuning&body=Title%3A%20LEAD%3A%20Iterative%20Data%20Selection%20for%20Efficient%20LLM%20Instruction%20Tuning%0AAuthor%3A%20Xiaotian%20Lin%20and%20Yanlin%20Qi%20and%20Yizhang%20Zhu%20and%20Themis%20Palpanas%20and%20Chengliang%20Chai%20and%20Nan%20Tang%20and%20Yuyu%20Luo%0AAbstract%3A%20%20%20Instruction%20tuning%20has%20emerged%20as%20a%20critical%20paradigm%20for%20improving%20the%0Acapabilities%20and%20alignment%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20existing%0Aiterative%20model-aware%20data%20selection%20methods%20incur%20significant%20computational%0Aoverhead%2C%20as%20they%20rely%20on%20repeatedly%20performing%20full-dataset%20model%20inference%20to%0Aestimate%20sample%20utility%20for%20subsequent%20training%20iterations%2C%20creating%20a%0Afundamental%20efficiency%20bottleneck.%20In%20this%20paper%2C%20we%20propose%20LEAD%2C%20an%20efficient%0Aiterative%20data%20selection%20framework%20that%20accurately%20estimates%20sample%20utility%0Aentirely%20within%20the%20standard%20training%20loop%2C%20eliminating%20the%20need%20for%20costly%0Aadditional%20model%20inference.%20At%20its%20core%2C%20LEAD%20introduces%20Instance-Level%20Dynamic%0AUncertainty%20%28IDU%29%2C%20a%20theoretically%20grounded%20utility%20function%20combining%0Ainstantaneous%20training%20loss%2C%20gradient-based%20approximation%20of%20loss%20changes%2C%20and%0Aexponential%20smoothing%20of%20historical%20loss%20signals.%20To%20further%20scale%20efficiently%0Ato%20large%20datasets%2C%20LEAD%20employs%20a%20two-stage%2C%20coarse-to-fine%20selection%20strategy%2C%0Aadaptively%20prioritizing%20informative%20clusters%20through%20a%20multi-armed%20bandit%0Amechanism%2C%20followed%20by%20precise%20fine-grained%20selection%20of%20high-utility%20samples%0Ausing%20IDU.%20Extensive%20experiments%20across%20four%20diverse%20benchmarks%20show%20that%20LEAD%0Asignificantly%20outperforms%20state-of-the-art%20methods%2C%20improving%20average%20model%0Aperformance%20by%206.1%25-10.8%25%20while%20using%20only%202.5%25%20of%20the%20training%20data%20and%0Areducing%20overall%20training%20time%20by%205-10x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAD%253A%2520Iterative%2520Data%2520Selection%2520for%2520Efficient%2520LLM%2520Instruction%2520Tuning%26entry.906535625%3DXiaotian%2520Lin%2520and%2520Yanlin%2520Qi%2520and%2520Yizhang%2520Zhu%2520and%2520Themis%2520Palpanas%2520and%2520Chengliang%2520Chai%2520and%2520Nan%2520Tang%2520and%2520Yuyu%2520Luo%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520has%2520emerged%2520as%2520a%2520critical%2520paradigm%2520for%2520improving%2520the%250Acapabilities%2520and%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520existing%250Aiterative%2520model-aware%2520data%2520selection%2520methods%2520incur%2520significant%2520computational%250Aoverhead%252C%2520as%2520they%2520rely%2520on%2520repeatedly%2520performing%2520full-dataset%2520model%2520inference%2520to%250Aestimate%2520sample%2520utility%2520for%2520subsequent%2520training%2520iterations%252C%2520creating%2520a%250Afundamental%2520efficiency%2520bottleneck.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LEAD%252C%2520an%2520efficient%250Aiterative%2520data%2520selection%2520framework%2520that%2520accurately%2520estimates%2520sample%2520utility%250Aentirely%2520within%2520the%2520standard%2520training%2520loop%252C%2520eliminating%2520the%2520need%2520for%2520costly%250Aadditional%2520model%2520inference.%2520At%2520its%2520core%252C%2520LEAD%2520introduces%2520Instance-Level%2520Dynamic%250AUncertainty%2520%2528IDU%2529%252C%2520a%2520theoretically%2520grounded%2520utility%2520function%2520combining%250Ainstantaneous%2520training%2520loss%252C%2520gradient-based%2520approximation%2520of%2520loss%2520changes%252C%2520and%250Aexponential%2520smoothing%2520of%2520historical%2520loss%2520signals.%2520To%2520further%2520scale%2520efficiently%250Ato%2520large%2520datasets%252C%2520LEAD%2520employs%2520a%2520two-stage%252C%2520coarse-to-fine%2520selection%2520strategy%252C%250Aadaptively%2520prioritizing%2520informative%2520clusters%2520through%2520a%2520multi-armed%2520bandit%250Amechanism%252C%2520followed%2520by%2520precise%2520fine-grained%2520selection%2520of%2520high-utility%2520samples%250Ausing%2520IDU.%2520Extensive%2520experiments%2520across%2520four%2520diverse%2520benchmarks%2520show%2520that%2520LEAD%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520improving%2520average%2520model%250Aperformance%2520by%25206.1%2525-10.8%2525%2520while%2520using%2520only%25202.5%2525%2520of%2520the%2520training%2520data%2520and%250Areducing%2520overall%2520training%2520time%2520by%25205-10x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAD%3A%20Iterative%20Data%20Selection%20for%20Efficient%20LLM%20Instruction%20Tuning&entry.906535625=Xiaotian%20Lin%20and%20Yanlin%20Qi%20and%20Yizhang%20Zhu%20and%20Themis%20Palpanas%20and%20Chengliang%20Chai%20and%20Nan%20Tang%20and%20Yuyu%20Luo&entry.1292438233=%20%20Instruction%20tuning%20has%20emerged%20as%20a%20critical%20paradigm%20for%20improving%20the%0Acapabilities%20and%20alignment%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20existing%0Aiterative%20model-aware%20data%20selection%20methods%20incur%20significant%20computational%0Aoverhead%2C%20as%20they%20rely%20on%20repeatedly%20performing%20full-dataset%20model%20inference%20to%0Aestimate%20sample%20utility%20for%20subsequent%20training%20iterations%2C%20creating%20a%0Afundamental%20efficiency%20bottleneck.%20In%20this%20paper%2C%20we%20propose%20LEAD%2C%20an%20efficient%0Aiterative%20data%20selection%20framework%20that%20accurately%20estimates%20sample%20utility%0Aentirely%20within%20the%20standard%20training%20loop%2C%20eliminating%20the%20need%20for%20costly%0Aadditional%20model%20inference.%20At%20its%20core%2C%20LEAD%20introduces%20Instance-Level%20Dynamic%0AUncertainty%20%28IDU%29%2C%20a%20theoretically%20grounded%20utility%20function%20combining%0Ainstantaneous%20training%20loss%2C%20gradient-based%20approximation%20of%20loss%20changes%2C%20and%0Aexponential%20smoothing%20of%20historical%20loss%20signals.%20To%20further%20scale%20efficiently%0Ato%20large%20datasets%2C%20LEAD%20employs%20a%20two-stage%2C%20coarse-to-fine%20selection%20strategy%2C%0Aadaptively%20prioritizing%20informative%20clusters%20through%20a%20multi-armed%20bandit%0Amechanism%2C%20followed%20by%20precise%20fine-grained%20selection%20of%20high-utility%20samples%0Ausing%20IDU.%20Extensive%20experiments%20across%20four%20diverse%20benchmarks%20show%20that%20LEAD%0Asignificantly%20outperforms%20state-of-the-art%20methods%2C%20improving%20average%20model%0Aperformance%20by%206.1%25-10.8%25%20while%20using%20only%202.5%25%20of%20the%20training%20data%20and%0Areducing%20overall%20training%20time%20by%205-10x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07437v1&entry.124074799=Read"},
{"title": "Nonlinear functional regression by functional deep neural network with\n  kernel embedding", "author": "Zhongjie Shi and Jun Fan and Linhao Song and Ding-Xuan Zhou and Johan A. K. Suykens", "abstract": "  Recently, deep learning has been widely applied in functional data analysis\n(FDA) with notable empirical success. However, the infinite dimensionality of\nfunctional data necessitates an effective dimension reduction approach for\nfunctional learning tasks, particularly in nonlinear functional regression. In\nthis paper, we introduce a functional deep neural network with an adaptive and\ndiscretization-invariant dimension reduction method. Our functional network\narchitecture consists of three parts: first, a kernel embedding step that\nfeatures an integral transformation with an adaptive smooth kernel; next, a\nprojection step that utilizes eigenfunction bases based on a projection Mercer\nkernel for the dimension reduction; and finally, a deep ReLU neural network is\nemployed for the prediction. Explicit rates of approximating nonlinear smooth\nfunctionals across various input function spaces by our proposed functional\nnetwork are derived. Additionally, we conduct a generalization analysis for the\nempirical risk minimization (ERM) algorithm applied to our functional net, by\nemploying a novel two-stage oracle inequality and the established functional\napproximation results. Ultimately, we conduct numerical experiments on both\nsimulated and real datasets to demonstrate the effectiveness and benefits of\nour functional net.\n", "link": "http://arxiv.org/abs/2401.02890v2", "date": "2025-05-12", "relevancy": 1.9557, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5105}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4747}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20functional%20regression%20by%20functional%20deep%20neural%20network%20with%0A%20%20kernel%20embedding&body=Title%3A%20Nonlinear%20functional%20regression%20by%20functional%20deep%20neural%20network%20with%0A%20%20kernel%20embedding%0AAuthor%3A%20Zhongjie%20Shi%20and%20Jun%20Fan%20and%20Linhao%20Song%20and%20Ding-Xuan%20Zhou%20and%20Johan%20A.%20K.%20Suykens%0AAbstract%3A%20%20%20Recently%2C%20deep%20learning%20has%20been%20widely%20applied%20in%20functional%20data%20analysis%0A%28FDA%29%20with%20notable%20empirical%20success.%20However%2C%20the%20infinite%20dimensionality%20of%0Afunctional%20data%20necessitates%20an%20effective%20dimension%20reduction%20approach%20for%0Afunctional%20learning%20tasks%2C%20particularly%20in%20nonlinear%20functional%20regression.%20In%0Athis%20paper%2C%20we%20introduce%20a%20functional%20deep%20neural%20network%20with%20an%20adaptive%20and%0Adiscretization-invariant%20dimension%20reduction%20method.%20Our%20functional%20network%0Aarchitecture%20consists%20of%20three%20parts%3A%20first%2C%20a%20kernel%20embedding%20step%20that%0Afeatures%20an%20integral%20transformation%20with%20an%20adaptive%20smooth%20kernel%3B%20next%2C%20a%0Aprojection%20step%20that%20utilizes%20eigenfunction%20bases%20based%20on%20a%20projection%20Mercer%0Akernel%20for%20the%20dimension%20reduction%3B%20and%20finally%2C%20a%20deep%20ReLU%20neural%20network%20is%0Aemployed%20for%20the%20prediction.%20Explicit%20rates%20of%20approximating%20nonlinear%20smooth%0Afunctionals%20across%20various%20input%20function%20spaces%20by%20our%20proposed%20functional%0Anetwork%20are%20derived.%20Additionally%2C%20we%20conduct%20a%20generalization%20analysis%20for%20the%0Aempirical%20risk%20minimization%20%28ERM%29%20algorithm%20applied%20to%20our%20functional%20net%2C%20by%0Aemploying%20a%20novel%20two-stage%20oracle%20inequality%20and%20the%20established%20functional%0Aapproximation%20results.%20Ultimately%2C%20we%20conduct%20numerical%20experiments%20on%20both%0Asimulated%20and%20real%20datasets%20to%20demonstrate%20the%20effectiveness%20and%20benefits%20of%0Aour%20functional%20net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520functional%2520regression%2520by%2520functional%2520deep%2520neural%2520network%2520with%250A%2520%2520kernel%2520embedding%26entry.906535625%3DZhongjie%2520Shi%2520and%2520Jun%2520Fan%2520and%2520Linhao%2520Song%2520and%2520Ding-Xuan%2520Zhou%2520and%2520Johan%2520A.%2520K.%2520Suykens%26entry.1292438233%3D%2520%2520Recently%252C%2520deep%2520learning%2520has%2520been%2520widely%2520applied%2520in%2520functional%2520data%2520analysis%250A%2528FDA%2529%2520with%2520notable%2520empirical%2520success.%2520However%252C%2520the%2520infinite%2520dimensionality%2520of%250Afunctional%2520data%2520necessitates%2520an%2520effective%2520dimension%2520reduction%2520approach%2520for%250Afunctional%2520learning%2520tasks%252C%2520particularly%2520in%2520nonlinear%2520functional%2520regression.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520functional%2520deep%2520neural%2520network%2520with%2520an%2520adaptive%2520and%250Adiscretization-invariant%2520dimension%2520reduction%2520method.%2520Our%2520functional%2520network%250Aarchitecture%2520consists%2520of%2520three%2520parts%253A%2520first%252C%2520a%2520kernel%2520embedding%2520step%2520that%250Afeatures%2520an%2520integral%2520transformation%2520with%2520an%2520adaptive%2520smooth%2520kernel%253B%2520next%252C%2520a%250Aprojection%2520step%2520that%2520utilizes%2520eigenfunction%2520bases%2520based%2520on%2520a%2520projection%2520Mercer%250Akernel%2520for%2520the%2520dimension%2520reduction%253B%2520and%2520finally%252C%2520a%2520deep%2520ReLU%2520neural%2520network%2520is%250Aemployed%2520for%2520the%2520prediction.%2520Explicit%2520rates%2520of%2520approximating%2520nonlinear%2520smooth%250Afunctionals%2520across%2520various%2520input%2520function%2520spaces%2520by%2520our%2520proposed%2520functional%250Anetwork%2520are%2520derived.%2520Additionally%252C%2520we%2520conduct%2520a%2520generalization%2520analysis%2520for%2520the%250Aempirical%2520risk%2520minimization%2520%2528ERM%2529%2520algorithm%2520applied%2520to%2520our%2520functional%2520net%252C%2520by%250Aemploying%2520a%2520novel%2520two-stage%2520oracle%2520inequality%2520and%2520the%2520established%2520functional%250Aapproximation%2520results.%2520Ultimately%252C%2520we%2520conduct%2520numerical%2520experiments%2520on%2520both%250Asimulated%2520and%2520real%2520datasets%2520to%2520demonstrate%2520the%2520effectiveness%2520and%2520benefits%2520of%250Aour%2520functional%2520net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20functional%20regression%20by%20functional%20deep%20neural%20network%20with%0A%20%20kernel%20embedding&entry.906535625=Zhongjie%20Shi%20and%20Jun%20Fan%20and%20Linhao%20Song%20and%20Ding-Xuan%20Zhou%20and%20Johan%20A.%20K.%20Suykens&entry.1292438233=%20%20Recently%2C%20deep%20learning%20has%20been%20widely%20applied%20in%20functional%20data%20analysis%0A%28FDA%29%20with%20notable%20empirical%20success.%20However%2C%20the%20infinite%20dimensionality%20of%0Afunctional%20data%20necessitates%20an%20effective%20dimension%20reduction%20approach%20for%0Afunctional%20learning%20tasks%2C%20particularly%20in%20nonlinear%20functional%20regression.%20In%0Athis%20paper%2C%20we%20introduce%20a%20functional%20deep%20neural%20network%20with%20an%20adaptive%20and%0Adiscretization-invariant%20dimension%20reduction%20method.%20Our%20functional%20network%0Aarchitecture%20consists%20of%20three%20parts%3A%20first%2C%20a%20kernel%20embedding%20step%20that%0Afeatures%20an%20integral%20transformation%20with%20an%20adaptive%20smooth%20kernel%3B%20next%2C%20a%0Aprojection%20step%20that%20utilizes%20eigenfunction%20bases%20based%20on%20a%20projection%20Mercer%0Akernel%20for%20the%20dimension%20reduction%3B%20and%20finally%2C%20a%20deep%20ReLU%20neural%20network%20is%0Aemployed%20for%20the%20prediction.%20Explicit%20rates%20of%20approximating%20nonlinear%20smooth%0Afunctionals%20across%20various%20input%20function%20spaces%20by%20our%20proposed%20functional%0Anetwork%20are%20derived.%20Additionally%2C%20we%20conduct%20a%20generalization%20analysis%20for%20the%0Aempirical%20risk%20minimization%20%28ERM%29%20algorithm%20applied%20to%20our%20functional%20net%2C%20by%0Aemploying%20a%20novel%20two-stage%20oracle%20inequality%20and%20the%20established%20functional%0Aapproximation%20results.%20Ultimately%2C%20we%20conduct%20numerical%20experiments%20on%20both%0Asimulated%20and%20real%20datasets%20to%20demonstrate%20the%20effectiveness%20and%20benefits%20of%0Aour%20functional%20net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02890v2&entry.124074799=Read"},
{"title": "Robust Kidney Abnormality Segmentation: A Validation Study of an\n  AI-Based Framework", "author": "Sarah de Boer and Hartmut H\u00e4ntze and Kiran Vaidhya Venkadesh and Myrthe A. D. Buser and Gabriel E. Humpire Mamani and Lina Xu and Lisa C. Adams and Jawed Nawabi and Keno K. Bressem and Bram van Ginneken and Mathias Prokop and Alessa Hering", "abstract": "  Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.\n", "link": "http://arxiv.org/abs/2505.07573v1", "date": "2025-05-12", "relevancy": 1.9515, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5105}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5023}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Kidney%20Abnormality%20Segmentation%3A%20A%20Validation%20Study%20of%20an%0A%20%20AI-Based%20Framework&body=Title%3A%20Robust%20Kidney%20Abnormality%20Segmentation%3A%20A%20Validation%20Study%20of%20an%0A%20%20AI-Based%20Framework%0AAuthor%3A%20Sarah%20de%20Boer%20and%20Hartmut%20H%C3%A4ntze%20and%20Kiran%20Vaidhya%20Venkadesh%20and%20Myrthe%20A.%20D.%20Buser%20and%20Gabriel%20E.%20Humpire%20Mamani%20and%20Lina%20Xu%20and%20Lisa%20C.%20Adams%20and%20Jawed%20Nawabi%20and%20Keno%20K.%20Bressem%20and%20Bram%20van%20Ginneken%20and%20Mathias%20Prokop%20and%20Alessa%20Hering%0AAbstract%3A%20%20%20Kidney%20abnormality%20segmentation%20has%20important%20potential%20to%20enhance%20the%0Aclinical%20workflow%2C%20especially%20in%20settings%20requiring%20quantitative%20assessments.%0AKidney%20volume%20could%20serve%20as%20an%20important%20biomarker%20for%20renal%20diseases%2C%20with%0Achanges%20in%20volume%20correlating%20directly%20with%20kidney%20function.%20Currently%2C%0Aclinical%20practice%20often%20relies%20on%20subjective%20visual%20assessment%20for%20evaluating%0Akidney%20size%20and%20abnormalities%2C%20including%20tumors%20and%20cysts%2C%20which%20are%20typically%0Astaged%20based%20on%20diameter%2C%20volume%2C%20and%20anatomical%20location.%20To%20support%20a%20more%0Aobjective%20and%20reproducible%20approach%2C%20this%20research%20aims%20to%20develop%20a%20robust%2C%0Athoroughly%20validated%20kidney%20abnormality%20segmentation%20algorithm%2C%20made%20publicly%0Aavailable%20for%20clinical%20and%20research%20use.%20We%20employ%20publicly%20available%20training%0Adatasets%20and%20leverage%20the%20state-of-the-art%20medical%20image%20segmentation%20framework%0AnnU-Net.%20Validation%20is%20conducted%20using%20both%20proprietary%20and%20public%20test%0Adatasets%2C%20with%20segmentation%20performance%20quantified%20by%20Dice%20coefficient%20and%20the%0A95th%20percentile%20Hausdorff%20distance.%20Furthermore%2C%20we%20analyze%20robustness%20across%0Asubgroups%20based%20on%20patient%20sex%2C%20age%2C%20CT%20contrast%20phases%2C%20and%20tumor%20histologic%0Asubtypes.%20Our%20findings%20demonstrate%20that%20our%20segmentation%20algorithm%2C%20trained%0Aexclusively%20on%20publicly%20available%20data%2C%20generalizes%20effectively%20to%20external%0Atest%20sets%20and%20outperforms%20existing%20state-of-the-art%20models%20across%20all%20tested%0Adatasets.%20Subgroup%20analyses%20reveal%20consistent%20high%20performance%2C%20indicating%0Astrong%20robustness%20and%20reliability.%20The%20developed%20algorithm%20and%20associated%20code%0Aare%20publicly%20accessible%20at%0Ahttps%3A//github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Kidney%2520Abnormality%2520Segmentation%253A%2520A%2520Validation%2520Study%2520of%2520an%250A%2520%2520AI-Based%2520Framework%26entry.906535625%3DSarah%2520de%2520Boer%2520and%2520Hartmut%2520H%25C3%25A4ntze%2520and%2520Kiran%2520Vaidhya%2520Venkadesh%2520and%2520Myrthe%2520A.%2520D.%2520Buser%2520and%2520Gabriel%2520E.%2520Humpire%2520Mamani%2520and%2520Lina%2520Xu%2520and%2520Lisa%2520C.%2520Adams%2520and%2520Jawed%2520Nawabi%2520and%2520Keno%2520K.%2520Bressem%2520and%2520Bram%2520van%2520Ginneken%2520and%2520Mathias%2520Prokop%2520and%2520Alessa%2520Hering%26entry.1292438233%3D%2520%2520Kidney%2520abnormality%2520segmentation%2520has%2520important%2520potential%2520to%2520enhance%2520the%250Aclinical%2520workflow%252C%2520especially%2520in%2520settings%2520requiring%2520quantitative%2520assessments.%250AKidney%2520volume%2520could%2520serve%2520as%2520an%2520important%2520biomarker%2520for%2520renal%2520diseases%252C%2520with%250Achanges%2520in%2520volume%2520correlating%2520directly%2520with%2520kidney%2520function.%2520Currently%252C%250Aclinical%2520practice%2520often%2520relies%2520on%2520subjective%2520visual%2520assessment%2520for%2520evaluating%250Akidney%2520size%2520and%2520abnormalities%252C%2520including%2520tumors%2520and%2520cysts%252C%2520which%2520are%2520typically%250Astaged%2520based%2520on%2520diameter%252C%2520volume%252C%2520and%2520anatomical%2520location.%2520To%2520support%2520a%2520more%250Aobjective%2520and%2520reproducible%2520approach%252C%2520this%2520research%2520aims%2520to%2520develop%2520a%2520robust%252C%250Athoroughly%2520validated%2520kidney%2520abnormality%2520segmentation%2520algorithm%252C%2520made%2520publicly%250Aavailable%2520for%2520clinical%2520and%2520research%2520use.%2520We%2520employ%2520publicly%2520available%2520training%250Adatasets%2520and%2520leverage%2520the%2520state-of-the-art%2520medical%2520image%2520segmentation%2520framework%250AnnU-Net.%2520Validation%2520is%2520conducted%2520using%2520both%2520proprietary%2520and%2520public%2520test%250Adatasets%252C%2520with%2520segmentation%2520performance%2520quantified%2520by%2520Dice%2520coefficient%2520and%2520the%250A95th%2520percentile%2520Hausdorff%2520distance.%2520Furthermore%252C%2520we%2520analyze%2520robustness%2520across%250Asubgroups%2520based%2520on%2520patient%2520sex%252C%2520age%252C%2520CT%2520contrast%2520phases%252C%2520and%2520tumor%2520histologic%250Asubtypes.%2520Our%2520findings%2520demonstrate%2520that%2520our%2520segmentation%2520algorithm%252C%2520trained%250Aexclusively%2520on%2520publicly%2520available%2520data%252C%2520generalizes%2520effectively%2520to%2520external%250Atest%2520sets%2520and%2520outperforms%2520existing%2520state-of-the-art%2520models%2520across%2520all%2520tested%250Adatasets.%2520Subgroup%2520analyses%2520reveal%2520consistent%2520high%2520performance%252C%2520indicating%250Astrong%2520robustness%2520and%2520reliability.%2520The%2520developed%2520algorithm%2520and%2520associated%2520code%250Aare%2520publicly%2520accessible%2520at%250Ahttps%253A//github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Kidney%20Abnormality%20Segmentation%3A%20A%20Validation%20Study%20of%20an%0A%20%20AI-Based%20Framework&entry.906535625=Sarah%20de%20Boer%20and%20Hartmut%20H%C3%A4ntze%20and%20Kiran%20Vaidhya%20Venkadesh%20and%20Myrthe%20A.%20D.%20Buser%20and%20Gabriel%20E.%20Humpire%20Mamani%20and%20Lina%20Xu%20and%20Lisa%20C.%20Adams%20and%20Jawed%20Nawabi%20and%20Keno%20K.%20Bressem%20and%20Bram%20van%20Ginneken%20and%20Mathias%20Prokop%20and%20Alessa%20Hering&entry.1292438233=%20%20Kidney%20abnormality%20segmentation%20has%20important%20potential%20to%20enhance%20the%0Aclinical%20workflow%2C%20especially%20in%20settings%20requiring%20quantitative%20assessments.%0AKidney%20volume%20could%20serve%20as%20an%20important%20biomarker%20for%20renal%20diseases%2C%20with%0Achanges%20in%20volume%20correlating%20directly%20with%20kidney%20function.%20Currently%2C%0Aclinical%20practice%20often%20relies%20on%20subjective%20visual%20assessment%20for%20evaluating%0Akidney%20size%20and%20abnormalities%2C%20including%20tumors%20and%20cysts%2C%20which%20are%20typically%0Astaged%20based%20on%20diameter%2C%20volume%2C%20and%20anatomical%20location.%20To%20support%20a%20more%0Aobjective%20and%20reproducible%20approach%2C%20this%20research%20aims%20to%20develop%20a%20robust%2C%0Athoroughly%20validated%20kidney%20abnormality%20segmentation%20algorithm%2C%20made%20publicly%0Aavailable%20for%20clinical%20and%20research%20use.%20We%20employ%20publicly%20available%20training%0Adatasets%20and%20leverage%20the%20state-of-the-art%20medical%20image%20segmentation%20framework%0AnnU-Net.%20Validation%20is%20conducted%20using%20both%20proprietary%20and%20public%20test%0Adatasets%2C%20with%20segmentation%20performance%20quantified%20by%20Dice%20coefficient%20and%20the%0A95th%20percentile%20Hausdorff%20distance.%20Furthermore%2C%20we%20analyze%20robustness%20across%0Asubgroups%20based%20on%20patient%20sex%2C%20age%2C%20CT%20contrast%20phases%2C%20and%20tumor%20histologic%0Asubtypes.%20Our%20findings%20demonstrate%20that%20our%20segmentation%20algorithm%2C%20trained%0Aexclusively%20on%20publicly%20available%20data%2C%20generalizes%20effectively%20to%20external%0Atest%20sets%20and%20outperforms%20existing%20state-of-the-art%20models%20across%20all%20tested%0Adatasets.%20Subgroup%20analyses%20reveal%20consistent%20high%20performance%2C%20indicating%0Astrong%20robustness%20and%20reliability.%20The%20developed%20algorithm%20and%20associated%20code%0Aare%20publicly%20accessible%20at%0Ahttps%3A//github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07573v1&entry.124074799=Read"},
{"title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using\n  Approximate Computing", "author": "Ayesha Siddique and Khurram Khalil and Khaza Anuarul Hoque", "abstract": "  Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications.\n", "link": "http://arxiv.org/abs/2504.17929v2", "date": "2025-05-12", "relevancy": 1.357, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4903}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4429}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ApproXAI%3A%20Energy-Efficient%20Hardware%20Acceleration%20of%20Explainable%20AI%20using%0A%20%20Approximate%20Computing&body=Title%3A%20ApproXAI%3A%20Energy-Efficient%20Hardware%20Acceleration%20of%20Explainable%20AI%20using%0A%20%20Approximate%20Computing%0AAuthor%3A%20Ayesha%20Siddique%20and%20Khurram%20Khalil%20and%20Khaza%20Anuarul%20Hoque%0AAbstract%3A%20%20%20Explainable%20artificial%20intelligence%20%28XAI%29%20enhances%20AI%20system%20transparency%20by%0Aframing%20interpretability%20as%20an%20optimization%20problem.%20However%2C%20this%20approach%0Aoften%20necessitates%20numerous%20iterations%20of%20computationally%20intensive%20operations%2C%0Alimiting%20its%20applicability%20in%20real-time%20scenarios.%20While%20recent%20research%20has%0Afocused%20on%20XAI%20hardware%20acceleration%20on%20FPGAs%20and%20TPU%2C%20these%20methods%20do%20not%0Afully%20address%20energy%20efficiency%20in%20real-time%20settings.%20To%20address%20this%0Alimitation%2C%20we%20propose%20XAIedge%2C%20a%20novel%20framework%20that%20leverages%20approximate%0Acomputing%20techniques%20into%20XAI%20algorithms%2C%20including%20integrated%20gradients%2C%20model%0Adistillation%2C%20and%20Shapley%20analysis.%20XAIedge%20translates%20these%20algorithms%20into%0Aapproximate%20matrix%20computations%20and%20exploits%20the%20synergy%20between%20convolution%2C%0AFourier%20transform%2C%20and%20approximate%20computing%20paradigms.%20This%20approach%20enables%0Aefficient%20hardware%20acceleration%20on%20TPU-based%20edge%20devices%2C%20facilitating%20faster%0Areal-time%20outcome%20interpretations.%20Our%20comprehensive%20evaluation%20demonstrates%0Athat%20XAIedge%20achieves%20a%20%242%5Ctimes%24%20improvement%20in%20energy%20efficiency%20compared%20to%0Aexisting%20accurate%20XAI%20hardware%20acceleration%20techniques%20while%20maintaining%0Acomparable%20accuracy.%20These%20results%20highlight%20the%20potential%20of%20XAIedge%20to%0Asignificantly%20advance%20the%20deployment%20of%20explainable%20AI%20in%20energy-constrained%0Areal-time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproXAI%253A%2520Energy-Efficient%2520Hardware%2520Acceleration%2520of%2520Explainable%2520AI%2520using%250A%2520%2520Approximate%2520Computing%26entry.906535625%3DAyesha%2520Siddique%2520and%2520Khurram%2520Khalil%2520and%2520Khaza%2520Anuarul%2520Hoque%26entry.1292438233%3D%2520%2520Explainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520enhances%2520AI%2520system%2520transparency%2520by%250Aframing%2520interpretability%2520as%2520an%2520optimization%2520problem.%2520However%252C%2520this%2520approach%250Aoften%2520necessitates%2520numerous%2520iterations%2520of%2520computationally%2520intensive%2520operations%252C%250Alimiting%2520its%2520applicability%2520in%2520real-time%2520scenarios.%2520While%2520recent%2520research%2520has%250Afocused%2520on%2520XAI%2520hardware%2520acceleration%2520on%2520FPGAs%2520and%2520TPU%252C%2520these%2520methods%2520do%2520not%250Afully%2520address%2520energy%2520efficiency%2520in%2520real-time%2520settings.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520XAIedge%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520approximate%250Acomputing%2520techniques%2520into%2520XAI%2520algorithms%252C%2520including%2520integrated%2520gradients%252C%2520model%250Adistillation%252C%2520and%2520Shapley%2520analysis.%2520XAIedge%2520translates%2520these%2520algorithms%2520into%250Aapproximate%2520matrix%2520computations%2520and%2520exploits%2520the%2520synergy%2520between%2520convolution%252C%250AFourier%2520transform%252C%2520and%2520approximate%2520computing%2520paradigms.%2520This%2520approach%2520enables%250Aefficient%2520hardware%2520acceleration%2520on%2520TPU-based%2520edge%2520devices%252C%2520facilitating%2520faster%250Areal-time%2520outcome%2520interpretations.%2520Our%2520comprehensive%2520evaluation%2520demonstrates%250Athat%2520XAIedge%2520achieves%2520a%2520%25242%255Ctimes%2524%2520improvement%2520in%2520energy%2520efficiency%2520compared%2520to%250Aexisting%2520accurate%2520XAI%2520hardware%2520acceleration%2520techniques%2520while%2520maintaining%250Acomparable%2520accuracy.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520XAIedge%2520to%250Asignificantly%2520advance%2520the%2520deployment%2520of%2520explainable%2520AI%2520in%2520energy-constrained%250Areal-time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ApproXAI%3A%20Energy-Efficient%20Hardware%20Acceleration%20of%20Explainable%20AI%20using%0A%20%20Approximate%20Computing&entry.906535625=Ayesha%20Siddique%20and%20Khurram%20Khalil%20and%20Khaza%20Anuarul%20Hoque&entry.1292438233=%20%20Explainable%20artificial%20intelligence%20%28XAI%29%20enhances%20AI%20system%20transparency%20by%0Aframing%20interpretability%20as%20an%20optimization%20problem.%20However%2C%20this%20approach%0Aoften%20necessitates%20numerous%20iterations%20of%20computationally%20intensive%20operations%2C%0Alimiting%20its%20applicability%20in%20real-time%20scenarios.%20While%20recent%20research%20has%0Afocused%20on%20XAI%20hardware%20acceleration%20on%20FPGAs%20and%20TPU%2C%20these%20methods%20do%20not%0Afully%20address%20energy%20efficiency%20in%20real-time%20settings.%20To%20address%20this%0Alimitation%2C%20we%20propose%20XAIedge%2C%20a%20novel%20framework%20that%20leverages%20approximate%0Acomputing%20techniques%20into%20XAI%20algorithms%2C%20including%20integrated%20gradients%2C%20model%0Adistillation%2C%20and%20Shapley%20analysis.%20XAIedge%20translates%20these%20algorithms%20into%0Aapproximate%20matrix%20computations%20and%20exploits%20the%20synergy%20between%20convolution%2C%0AFourier%20transform%2C%20and%20approximate%20computing%20paradigms.%20This%20approach%20enables%0Aefficient%20hardware%20acceleration%20on%20TPU-based%20edge%20devices%2C%20facilitating%20faster%0Areal-time%20outcome%20interpretations.%20Our%20comprehensive%20evaluation%20demonstrates%0Athat%20XAIedge%20achieves%20a%20%242%5Ctimes%24%20improvement%20in%20energy%20efficiency%20compared%20to%0Aexisting%20accurate%20XAI%20hardware%20acceleration%20techniques%20while%20maintaining%0Acomparable%20accuracy.%20These%20results%20highlight%20the%20potential%20of%20XAIedge%20to%0Asignificantly%20advance%20the%20deployment%20of%20explainable%20AI%20in%20energy-constrained%0Areal-time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17929v2&entry.124074799=Read"},
{"title": "Towards Requirements Engineering for RAG Systems", "author": "Tor Sporsem and Rasmus Ulfsnes", "abstract": "  This short paper explores how a maritime company develops and integrates\nlarge-language models (LLM). Specifically by looking at the requirements\nengineering for Retrieval Augmented Generation (RAG) systems in expert\nsettings. Through a case study at a maritime service provider, we demonstrate\nhow data scientists face a fundamental tension between user expectations of AI\nperfection and the correctness of the generated outputs. Our findings reveal\nthat data scientists must identify context-specific \"retrieval requirements\"\nthrough iterative experimentation together with users because they are the ones\nwho can determine correctness. We present an empirical process model describing\nhow data scientists practically elicited these \"retrieval requirements\" and\nmanaged system limitations. This work advances software engineering knowledge\nby providing insights into the specialized requirements engineering processes\nfor implementing RAG systems in complex domain-specific applications.\n", "link": "http://arxiv.org/abs/2505.07553v1", "date": "2025-05-12", "relevancy": 0.9131, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4947}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4472}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Requirements%20Engineering%20for%20RAG%20Systems&body=Title%3A%20Towards%20Requirements%20Engineering%20for%20RAG%20Systems%0AAuthor%3A%20Tor%20Sporsem%20and%20Rasmus%20Ulfsnes%0AAbstract%3A%20%20%20This%20short%20paper%20explores%20how%20a%20maritime%20company%20develops%20and%20integrates%0Alarge-language%20models%20%28LLM%29.%20Specifically%20by%20looking%20at%20the%20requirements%0Aengineering%20for%20Retrieval%20Augmented%20Generation%20%28RAG%29%20systems%20in%20expert%0Asettings.%20Through%20a%20case%20study%20at%20a%20maritime%20service%20provider%2C%20we%20demonstrate%0Ahow%20data%20scientists%20face%20a%20fundamental%20tension%20between%20user%20expectations%20of%20AI%0Aperfection%20and%20the%20correctness%20of%20the%20generated%20outputs.%20Our%20findings%20reveal%0Athat%20data%20scientists%20must%20identify%20context-specific%20%22retrieval%20requirements%22%0Athrough%20iterative%20experimentation%20together%20with%20users%20because%20they%20are%20the%20ones%0Awho%20can%20determine%20correctness.%20We%20present%20an%20empirical%20process%20model%20describing%0Ahow%20data%20scientists%20practically%20elicited%20these%20%22retrieval%20requirements%22%20and%0Amanaged%20system%20limitations.%20This%20work%20advances%20software%20engineering%20knowledge%0Aby%20providing%20insights%20into%20the%20specialized%20requirements%20engineering%20processes%0Afor%20implementing%20RAG%20systems%20in%20complex%20domain-specific%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Requirements%2520Engineering%2520for%2520RAG%2520Systems%26entry.906535625%3DTor%2520Sporsem%2520and%2520Rasmus%2520Ulfsnes%26entry.1292438233%3D%2520%2520This%2520short%2520paper%2520explores%2520how%2520a%2520maritime%2520company%2520develops%2520and%2520integrates%250Alarge-language%2520models%2520%2528LLM%2529.%2520Specifically%2520by%2520looking%2520at%2520the%2520requirements%250Aengineering%2520for%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520in%2520expert%250Asettings.%2520Through%2520a%2520case%2520study%2520at%2520a%2520maritime%2520service%2520provider%252C%2520we%2520demonstrate%250Ahow%2520data%2520scientists%2520face%2520a%2520fundamental%2520tension%2520between%2520user%2520expectations%2520of%2520AI%250Aperfection%2520and%2520the%2520correctness%2520of%2520the%2520generated%2520outputs.%2520Our%2520findings%2520reveal%250Athat%2520data%2520scientists%2520must%2520identify%2520context-specific%2520%2522retrieval%2520requirements%2522%250Athrough%2520iterative%2520experimentation%2520together%2520with%2520users%2520because%2520they%2520are%2520the%2520ones%250Awho%2520can%2520determine%2520correctness.%2520We%2520present%2520an%2520empirical%2520process%2520model%2520describing%250Ahow%2520data%2520scientists%2520practically%2520elicited%2520these%2520%2522retrieval%2520requirements%2522%2520and%250Amanaged%2520system%2520limitations.%2520This%2520work%2520advances%2520software%2520engineering%2520knowledge%250Aby%2520providing%2520insights%2520into%2520the%2520specialized%2520requirements%2520engineering%2520processes%250Afor%2520implementing%2520RAG%2520systems%2520in%2520complex%2520domain-specific%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Requirements%20Engineering%20for%20RAG%20Systems&entry.906535625=Tor%20Sporsem%20and%20Rasmus%20Ulfsnes&entry.1292438233=%20%20This%20short%20paper%20explores%20how%20a%20maritime%20company%20develops%20and%20integrates%0Alarge-language%20models%20%28LLM%29.%20Specifically%20by%20looking%20at%20the%20requirements%0Aengineering%20for%20Retrieval%20Augmented%20Generation%20%28RAG%29%20systems%20in%20expert%0Asettings.%20Through%20a%20case%20study%20at%20a%20maritime%20service%20provider%2C%20we%20demonstrate%0Ahow%20data%20scientists%20face%20a%20fundamental%20tension%20between%20user%20expectations%20of%20AI%0Aperfection%20and%20the%20correctness%20of%20the%20generated%20outputs.%20Our%20findings%20reveal%0Athat%20data%20scientists%20must%20identify%20context-specific%20%22retrieval%20requirements%22%0Athrough%20iterative%20experimentation%20together%20with%20users%20because%20they%20are%20the%20ones%0Awho%20can%20determine%20correctness.%20We%20present%20an%20empirical%20process%20model%20describing%0Ahow%20data%20scientists%20practically%20elicited%20these%20%22retrieval%20requirements%22%20and%0Amanaged%20system%20limitations.%20This%20work%20advances%20software%20engineering%20knowledge%0Aby%20providing%20insights%20into%20the%20specialized%20requirements%20engineering%20processes%0Afor%20implementing%20RAG%20systems%20in%20complex%20domain-specific%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07553v1&entry.124074799=Read"},
{"title": "ISAC: An Invertible and Stable Auditory Filter Bank with Customizable\n  Kernels for ML Integration", "author": "Daniel Haider and Felix Perfler and Peter Balazs and Clara Hollomey and Nicki Holighaus", "abstract": "  This paper introduces ISAC, an invertible and stable, perceptually-motivated\nfilter bank that is specifically designed to be integrated into machine\nlearning paradigms. More precisely, the center frequencies and bandwidths of\nthe filters are chosen to follow a non-linear, auditory frequency scale, the\nfilter kernels have user-defined maximum temporal support and may serve as\nlearnable convolutional kernels, and there exists a corresponding filter bank\nsuch that both form a perfect reconstruction pair. ISAC provides a powerful and\nuser-friendly audio front-end suitable for any application, including\nanalysis-synthesis schemes.\n", "link": "http://arxiv.org/abs/2505.07709v1", "date": "2025-05-12", "relevancy": 1.7046, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4195}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ISAC%3A%20An%20Invertible%20and%20Stable%20Auditory%20Filter%20Bank%20with%20Customizable%0A%20%20Kernels%20for%20ML%20Integration&body=Title%3A%20ISAC%3A%20An%20Invertible%20and%20Stable%20Auditory%20Filter%20Bank%20with%20Customizable%0A%20%20Kernels%20for%20ML%20Integration%0AAuthor%3A%20Daniel%20Haider%20and%20Felix%20Perfler%20and%20Peter%20Balazs%20and%20Clara%20Hollomey%20and%20Nicki%20Holighaus%0AAbstract%3A%20%20%20This%20paper%20introduces%20ISAC%2C%20an%20invertible%20and%20stable%2C%20perceptually-motivated%0Afilter%20bank%20that%20is%20specifically%20designed%20to%20be%20integrated%20into%20machine%0Alearning%20paradigms.%20More%20precisely%2C%20the%20center%20frequencies%20and%20bandwidths%20of%0Athe%20filters%20are%20chosen%20to%20follow%20a%20non-linear%2C%20auditory%20frequency%20scale%2C%20the%0Afilter%20kernels%20have%20user-defined%20maximum%20temporal%20support%20and%20may%20serve%20as%0Alearnable%20convolutional%20kernels%2C%20and%20there%20exists%20a%20corresponding%20filter%20bank%0Asuch%20that%20both%20form%20a%20perfect%20reconstruction%20pair.%20ISAC%20provides%20a%20powerful%20and%0Auser-friendly%20audio%20front-end%20suitable%20for%20any%20application%2C%20including%0Aanalysis-synthesis%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DISAC%253A%2520An%2520Invertible%2520and%2520Stable%2520Auditory%2520Filter%2520Bank%2520with%2520Customizable%250A%2520%2520Kernels%2520for%2520ML%2520Integration%26entry.906535625%3DDaniel%2520Haider%2520and%2520Felix%2520Perfler%2520and%2520Peter%2520Balazs%2520and%2520Clara%2520Hollomey%2520and%2520Nicki%2520Holighaus%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520ISAC%252C%2520an%2520invertible%2520and%2520stable%252C%2520perceptually-motivated%250Afilter%2520bank%2520that%2520is%2520specifically%2520designed%2520to%2520be%2520integrated%2520into%2520machine%250Alearning%2520paradigms.%2520More%2520precisely%252C%2520the%2520center%2520frequencies%2520and%2520bandwidths%2520of%250Athe%2520filters%2520are%2520chosen%2520to%2520follow%2520a%2520non-linear%252C%2520auditory%2520frequency%2520scale%252C%2520the%250Afilter%2520kernels%2520have%2520user-defined%2520maximum%2520temporal%2520support%2520and%2520may%2520serve%2520as%250Alearnable%2520convolutional%2520kernels%252C%2520and%2520there%2520exists%2520a%2520corresponding%2520filter%2520bank%250Asuch%2520that%2520both%2520form%2520a%2520perfect%2520reconstruction%2520pair.%2520ISAC%2520provides%2520a%2520powerful%2520and%250Auser-friendly%2520audio%2520front-end%2520suitable%2520for%2520any%2520application%252C%2520including%250Aanalysis-synthesis%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ISAC%3A%20An%20Invertible%20and%20Stable%20Auditory%20Filter%20Bank%20with%20Customizable%0A%20%20Kernels%20for%20ML%20Integration&entry.906535625=Daniel%20Haider%20and%20Felix%20Perfler%20and%20Peter%20Balazs%20and%20Clara%20Hollomey%20and%20Nicki%20Holighaus&entry.1292438233=%20%20This%20paper%20introduces%20ISAC%2C%20an%20invertible%20and%20stable%2C%20perceptually-motivated%0Afilter%20bank%20that%20is%20specifically%20designed%20to%20be%20integrated%20into%20machine%0Alearning%20paradigms.%20More%20precisely%2C%20the%20center%20frequencies%20and%20bandwidths%20of%0Athe%20filters%20are%20chosen%20to%20follow%20a%20non-linear%2C%20auditory%20frequency%20scale%2C%20the%0Afilter%20kernels%20have%20user-defined%20maximum%20temporal%20support%20and%20may%20serve%20as%0Alearnable%20convolutional%20kernels%2C%20and%20there%20exists%20a%20corresponding%20filter%20bank%0Asuch%20that%20both%20form%20a%20perfect%20reconstruction%20pair.%20ISAC%20provides%20a%20powerful%20and%0Auser-friendly%20audio%20front-end%20suitable%20for%20any%20application%2C%20including%0Aanalysis-synthesis%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07709v1&entry.124074799=Read"},
{"title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies", "author": "Tony Tao and Mohan Kumar Srirama and Jason Jingzhou Liu and Kenneth Shaw and Deepak Pathak", "abstract": "  Large-scale, diverse robot datasets have emerged as a promising path toward\nenabling dexterous manipulation policies to generalize to novel environments,\nbut acquiring such datasets presents many challenges. While teleoperation\nprovides high-fidelity datasets, its high cost limits its scalability. Instead,\nwhat if people could use their own hands, just as they do in everyday life, to\ncollect data? In DexWild, a diverse team of data collectors uses their hands to\ncollect hours of interactions across a multitude of environments and objects.\nTo record this data, we create DexWild-System, a low-cost, mobile, and\neasy-to-use device. The DexWild learning framework co-trains on both human and\nrobot demonstrations, leading to improved performance compared to training on\neach dataset individually. This combination results in robust robot policies\ncapable of generalizing to novel environments, tasks, and embodiments with\nminimal additional robot-specific data. Experimental results demonstrate that\nDexWild significantly improves performance, achieving a 68.5% success rate in\nunseen environments-nearly four times higher than policies trained with robot\ndata only-and offering 5.8x better cross-embodiment generalization. Video\nresults, codebases, and instructions at https://dexwild.github.io\n", "link": "http://arxiv.org/abs/2505.07813v1", "date": "2025-05-12", "relevancy": 1.6813, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5912}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5638}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexWild%3A%20Dexterous%20Human%20Interactions%20for%20In-the-Wild%20Robot%20Policies&body=Title%3A%20DexWild%3A%20Dexterous%20Human%20Interactions%20for%20In-the-Wild%20Robot%20Policies%0AAuthor%3A%20Tony%20Tao%20and%20Mohan%20Kumar%20Srirama%20and%20Jason%20Jingzhou%20Liu%20and%20Kenneth%20Shaw%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20Large-scale%2C%20diverse%20robot%20datasets%20have%20emerged%20as%20a%20promising%20path%20toward%0Aenabling%20dexterous%20manipulation%20policies%20to%20generalize%20to%20novel%20environments%2C%0Abut%20acquiring%20such%20datasets%20presents%20many%20challenges.%20While%20teleoperation%0Aprovides%20high-fidelity%20datasets%2C%20its%20high%20cost%20limits%20its%20scalability.%20Instead%2C%0Awhat%20if%20people%20could%20use%20their%20own%20hands%2C%20just%20as%20they%20do%20in%20everyday%20life%2C%20to%0Acollect%20data%3F%20In%20DexWild%2C%20a%20diverse%20team%20of%20data%20collectors%20uses%20their%20hands%20to%0Acollect%20hours%20of%20interactions%20across%20a%20multitude%20of%20environments%20and%20objects.%0ATo%20record%20this%20data%2C%20we%20create%20DexWild-System%2C%20a%20low-cost%2C%20mobile%2C%20and%0Aeasy-to-use%20device.%20The%20DexWild%20learning%20framework%20co-trains%20on%20both%20human%20and%0Arobot%20demonstrations%2C%20leading%20to%20improved%20performance%20compared%20to%20training%20on%0Aeach%20dataset%20individually.%20This%20combination%20results%20in%20robust%20robot%20policies%0Acapable%20of%20generalizing%20to%20novel%20environments%2C%20tasks%2C%20and%20embodiments%20with%0Aminimal%20additional%20robot-specific%20data.%20Experimental%20results%20demonstrate%20that%0ADexWild%20significantly%20improves%20performance%2C%20achieving%20a%2068.5%25%20success%20rate%20in%0Aunseen%20environments-nearly%20four%20times%20higher%20than%20policies%20trained%20with%20robot%0Adata%20only-and%20offering%205.8x%20better%20cross-embodiment%20generalization.%20Video%0Aresults%2C%20codebases%2C%20and%20instructions%20at%20https%3A//dexwild.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexWild%253A%2520Dexterous%2520Human%2520Interactions%2520for%2520In-the-Wild%2520Robot%2520Policies%26entry.906535625%3DTony%2520Tao%2520and%2520Mohan%2520Kumar%2520Srirama%2520and%2520Jason%2520Jingzhou%2520Liu%2520and%2520Kenneth%2520Shaw%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520Large-scale%252C%2520diverse%2520robot%2520datasets%2520have%2520emerged%2520as%2520a%2520promising%2520path%2520toward%250Aenabling%2520dexterous%2520manipulation%2520policies%2520to%2520generalize%2520to%2520novel%2520environments%252C%250Abut%2520acquiring%2520such%2520datasets%2520presents%2520many%2520challenges.%2520While%2520teleoperation%250Aprovides%2520high-fidelity%2520datasets%252C%2520its%2520high%2520cost%2520limits%2520its%2520scalability.%2520Instead%252C%250Awhat%2520if%2520people%2520could%2520use%2520their%2520own%2520hands%252C%2520just%2520as%2520they%2520do%2520in%2520everyday%2520life%252C%2520to%250Acollect%2520data%253F%2520In%2520DexWild%252C%2520a%2520diverse%2520team%2520of%2520data%2520collectors%2520uses%2520their%2520hands%2520to%250Acollect%2520hours%2520of%2520interactions%2520across%2520a%2520multitude%2520of%2520environments%2520and%2520objects.%250ATo%2520record%2520this%2520data%252C%2520we%2520create%2520DexWild-System%252C%2520a%2520low-cost%252C%2520mobile%252C%2520and%250Aeasy-to-use%2520device.%2520The%2520DexWild%2520learning%2520framework%2520co-trains%2520on%2520both%2520human%2520and%250Arobot%2520demonstrations%252C%2520leading%2520to%2520improved%2520performance%2520compared%2520to%2520training%2520on%250Aeach%2520dataset%2520individually.%2520This%2520combination%2520results%2520in%2520robust%2520robot%2520policies%250Acapable%2520of%2520generalizing%2520to%2520novel%2520environments%252C%2520tasks%252C%2520and%2520embodiments%2520with%250Aminimal%2520additional%2520robot-specific%2520data.%2520Experimental%2520results%2520demonstrate%2520that%250ADexWild%2520significantly%2520improves%2520performance%252C%2520achieving%2520a%252068.5%2525%2520success%2520rate%2520in%250Aunseen%2520environments-nearly%2520four%2520times%2520higher%2520than%2520policies%2520trained%2520with%2520robot%250Adata%2520only-and%2520offering%25205.8x%2520better%2520cross-embodiment%2520generalization.%2520Video%250Aresults%252C%2520codebases%252C%2520and%2520instructions%2520at%2520https%253A//dexwild.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexWild%3A%20Dexterous%20Human%20Interactions%20for%20In-the-Wild%20Robot%20Policies&entry.906535625=Tony%20Tao%20and%20Mohan%20Kumar%20Srirama%20and%20Jason%20Jingzhou%20Liu%20and%20Kenneth%20Shaw%20and%20Deepak%20Pathak&entry.1292438233=%20%20Large-scale%2C%20diverse%20robot%20datasets%20have%20emerged%20as%20a%20promising%20path%20toward%0Aenabling%20dexterous%20manipulation%20policies%20to%20generalize%20to%20novel%20environments%2C%0Abut%20acquiring%20such%20datasets%20presents%20many%20challenges.%20While%20teleoperation%0Aprovides%20high-fidelity%20datasets%2C%20its%20high%20cost%20limits%20its%20scalability.%20Instead%2C%0Awhat%20if%20people%20could%20use%20their%20own%20hands%2C%20just%20as%20they%20do%20in%20everyday%20life%2C%20to%0Acollect%20data%3F%20In%20DexWild%2C%20a%20diverse%20team%20of%20data%20collectors%20uses%20their%20hands%20to%0Acollect%20hours%20of%20interactions%20across%20a%20multitude%20of%20environments%20and%20objects.%0ATo%20record%20this%20data%2C%20we%20create%20DexWild-System%2C%20a%20low-cost%2C%20mobile%2C%20and%0Aeasy-to-use%20device.%20The%20DexWild%20learning%20framework%20co-trains%20on%20both%20human%20and%0Arobot%20demonstrations%2C%20leading%20to%20improved%20performance%20compared%20to%20training%20on%0Aeach%20dataset%20individually.%20This%20combination%20results%20in%20robust%20robot%20policies%0Acapable%20of%20generalizing%20to%20novel%20environments%2C%20tasks%2C%20and%20embodiments%20with%0Aminimal%20additional%20robot-specific%20data.%20Experimental%20results%20demonstrate%20that%0ADexWild%20significantly%20improves%20performance%2C%20achieving%20a%2068.5%25%20success%20rate%20in%0Aunseen%20environments-nearly%20four%20times%20higher%20than%20policies%20trained%20with%20robot%0Adata%20only-and%20offering%205.8x%20better%20cross-embodiment%20generalization.%20Video%0Aresults%2C%20codebases%2C%20and%20instructions%20at%20https%3A//dexwild.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07813v1&entry.124074799=Read"},
{"title": "Understanding Stragglers in Large Model Training Using What-if Analysis", "author": "Jinkun Lin and Ziheng Jiang and Zuquan Song and Sida Zhao and Menghan Yu and Zhanghan Wang and Chenyuan Wang and Zuocheng Shi and Xiang Shi and Wei Jia and Zherui Liu and Shuguang Wang and Haibin Lin and Xin Liu and Aurojit Panda and Jinyang Li", "abstract": "  Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?\n", "link": "http://arxiv.org/abs/2505.05713v2", "date": "2025-05-12", "relevancy": 1.7641, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4423}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Stragglers%20in%20Large%20Model%20Training%20Using%20What-if%20Analysis&body=Title%3A%20Understanding%20Stragglers%20in%20Large%20Model%20Training%20Using%20What-if%20Analysis%0AAuthor%3A%20Jinkun%20Lin%20and%20Ziheng%20Jiang%20and%20Zuquan%20Song%20and%20Sida%20Zhao%20and%20Menghan%20Yu%20and%20Zhanghan%20Wang%20and%20Chenyuan%20Wang%20and%20Zuocheng%20Shi%20and%20Xiang%20Shi%20and%20Wei%20Jia%20and%20Zherui%20Liu%20and%20Shuguang%20Wang%20and%20Haibin%20Lin%20and%20Xin%20Liu%20and%20Aurojit%20Panda%20and%20Jinyang%20Li%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20training%20is%20one%20of%20the%20most%20demanding%20distributed%0Acomputations%20today%2C%20often%20requiring%20thousands%20of%20GPUs%20with%20frequent%0Asynchronization%20across%20machines.%20Such%20a%20workload%20pattern%20makes%20it%20susceptible%0Ato%20stragglers%2C%20where%20the%20training%20can%20be%20stalled%20by%20few%20slow%20workers.%20At%0AByteDance%20we%20find%20stragglers%20are%20not%20trivially%20always%20caused%20by%20hardware%0Afailures%2C%20but%20can%20arise%20from%20multiple%20complex%20factors.%20This%20work%20aims%20to%0Apresent%20a%20comprehensive%20study%20on%20the%20straggler%20issues%20in%20LLM%20training%2C%20using%20a%0Afive-month%20trace%20collected%20from%20our%20ByteDance%20LLM%20training%20cluster.%20The%20core%0Amethodology%20is%20what-if%20analysis%20that%20simulates%20the%20scenario%20without%20any%0Astragglers%20and%20contrasts%20with%20the%20actual%20case.%20We%20use%20this%20method%20to%20study%20the%0Afollowing%20questions%3A%20%281%29%20how%20often%20do%20stragglers%20affect%20training%20jobs%2C%20and%20what%0Aeffect%20do%20they%20have%20on%20job%20performance%3B%20%282%29%20do%20stragglers%20exhibit%20temporal%20or%0Aspatial%20patterns%3B%20and%20%283%29%20what%20are%20the%20potential%20root%20causes%20for%20stragglers%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Stragglers%2520in%2520Large%2520Model%2520Training%2520Using%2520What-if%2520Analysis%26entry.906535625%3DJinkun%2520Lin%2520and%2520Ziheng%2520Jiang%2520and%2520Zuquan%2520Song%2520and%2520Sida%2520Zhao%2520and%2520Menghan%2520Yu%2520and%2520Zhanghan%2520Wang%2520and%2520Chenyuan%2520Wang%2520and%2520Zuocheng%2520Shi%2520and%2520Xiang%2520Shi%2520and%2520Wei%2520Jia%2520and%2520Zherui%2520Liu%2520and%2520Shuguang%2520Wang%2520and%2520Haibin%2520Lin%2520and%2520Xin%2520Liu%2520and%2520Aurojit%2520Panda%2520and%2520Jinyang%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520training%2520is%2520one%2520of%2520the%2520most%2520demanding%2520distributed%250Acomputations%2520today%252C%2520often%2520requiring%2520thousands%2520of%2520GPUs%2520with%2520frequent%250Asynchronization%2520across%2520machines.%2520Such%2520a%2520workload%2520pattern%2520makes%2520it%2520susceptible%250Ato%2520stragglers%252C%2520where%2520the%2520training%2520can%2520be%2520stalled%2520by%2520few%2520slow%2520workers.%2520At%250AByteDance%2520we%2520find%2520stragglers%2520are%2520not%2520trivially%2520always%2520caused%2520by%2520hardware%250Afailures%252C%2520but%2520can%2520arise%2520from%2520multiple%2520complex%2520factors.%2520This%2520work%2520aims%2520to%250Apresent%2520a%2520comprehensive%2520study%2520on%2520the%2520straggler%2520issues%2520in%2520LLM%2520training%252C%2520using%2520a%250Afive-month%2520trace%2520collected%2520from%2520our%2520ByteDance%2520LLM%2520training%2520cluster.%2520The%2520core%250Amethodology%2520is%2520what-if%2520analysis%2520that%2520simulates%2520the%2520scenario%2520without%2520any%250Astragglers%2520and%2520contrasts%2520with%2520the%2520actual%2520case.%2520We%2520use%2520this%2520method%2520to%2520study%2520the%250Afollowing%2520questions%253A%2520%25281%2529%2520how%2520often%2520do%2520stragglers%2520affect%2520training%2520jobs%252C%2520and%2520what%250Aeffect%2520do%2520they%2520have%2520on%2520job%2520performance%253B%2520%25282%2529%2520do%2520stragglers%2520exhibit%2520temporal%2520or%250Aspatial%2520patterns%253B%2520and%2520%25283%2529%2520what%2520are%2520the%2520potential%2520root%2520causes%2520for%2520stragglers%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Stragglers%20in%20Large%20Model%20Training%20Using%20What-if%20Analysis&entry.906535625=Jinkun%20Lin%20and%20Ziheng%20Jiang%20and%20Zuquan%20Song%20and%20Sida%20Zhao%20and%20Menghan%20Yu%20and%20Zhanghan%20Wang%20and%20Chenyuan%20Wang%20and%20Zuocheng%20Shi%20and%20Xiang%20Shi%20and%20Wei%20Jia%20and%20Zherui%20Liu%20and%20Shuguang%20Wang%20and%20Haibin%20Lin%20and%20Xin%20Liu%20and%20Aurojit%20Panda%20and%20Jinyang%20Li&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20training%20is%20one%20of%20the%20most%20demanding%20distributed%0Acomputations%20today%2C%20often%20requiring%20thousands%20of%20GPUs%20with%20frequent%0Asynchronization%20across%20machines.%20Such%20a%20workload%20pattern%20makes%20it%20susceptible%0Ato%20stragglers%2C%20where%20the%20training%20can%20be%20stalled%20by%20few%20slow%20workers.%20At%0AByteDance%20we%20find%20stragglers%20are%20not%20trivially%20always%20caused%20by%20hardware%0Afailures%2C%20but%20can%20arise%20from%20multiple%20complex%20factors.%20This%20work%20aims%20to%0Apresent%20a%20comprehensive%20study%20on%20the%20straggler%20issues%20in%20LLM%20training%2C%20using%20a%0Afive-month%20trace%20collected%20from%20our%20ByteDance%20LLM%20training%20cluster.%20The%20core%0Amethodology%20is%20what-if%20analysis%20that%20simulates%20the%20scenario%20without%20any%0Astragglers%20and%20contrasts%20with%20the%20actual%20case.%20We%20use%20this%20method%20to%20study%20the%0Afollowing%20questions%3A%20%281%29%20how%20often%20do%20stragglers%20affect%20training%20jobs%2C%20and%20what%0Aeffect%20do%20they%20have%20on%20job%20performance%3B%20%282%29%20do%20stragglers%20exhibit%20temporal%20or%0Aspatial%20patterns%3B%20and%20%283%29%20what%20are%20the%20potential%20root%20causes%20for%20stragglers%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05713v2&entry.124074799=Read"},
{"title": "dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph", "author": "Nitish Shukla and Arun Ross", "abstract": "  A facial morph is an image created by combining two face images pertaining to\ntwo distinct identities. Face demorphing inverts the process and tries to\nrecover the original images constituting a facial morph. While morph attack\ndetection (MAD) techniques can be used to flag morph images, they do not\ndivulge any visual information about the faces used to create them. Demorphing\nhelps address this problem. Existing demorphing techniques are either very\nrestrictive (assume identities during testing) or produce feeble outputs (both\noutputs look very similar). In this paper, we overcome these issues by\nproposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph\nimages. Our method overcomes morph-replication and produces high quality\nreconstructions of the bonafide images used to create the morphs. Moreover, our\nmethod is highly generalizable across demorphing paradigms\n(differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and\nMorDiff datasets to showcase the efficacy of our method.\n", "link": "http://arxiv.org/abs/2411.14494v3", "date": "2025-05-12", "relevancy": 1.0402, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5367}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5146}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20dc-GAN%3A%20Dual-Conditioned%20GAN%20for%20Face%20Demorphing%20From%20a%20Single%20Morph&body=Title%3A%20dc-GAN%3A%20Dual-Conditioned%20GAN%20for%20Face%20Demorphing%20From%20a%20Single%20Morph%0AAuthor%3A%20Nitish%20Shukla%20and%20Arun%20Ross%0AAbstract%3A%20%20%20A%20facial%20morph%20is%20an%20image%20created%20by%20combining%20two%20face%20images%20pertaining%20to%0Atwo%20distinct%20identities.%20Face%20demorphing%20inverts%20the%20process%20and%20tries%20to%0Arecover%20the%20original%20images%20constituting%20a%20facial%20morph.%20While%20morph%20attack%0Adetection%20%28MAD%29%20techniques%20can%20be%20used%20to%20flag%20morph%20images%2C%20they%20do%20not%0Adivulge%20any%20visual%20information%20about%20the%20faces%20used%20to%20create%20them.%20Demorphing%0Ahelps%20address%20this%20problem.%20Existing%20demorphing%20techniques%20are%20either%20very%0Arestrictive%20%28assume%20identities%20during%20testing%29%20or%20produce%20feeble%20outputs%20%28both%0Aoutputs%20look%20very%20similar%29.%20In%20this%20paper%2C%20we%20overcome%20these%20issues%20by%0Aproposing%20dc-GAN%2C%20a%20novel%20GAN-based%20demorphing%20method%20conditioned%20on%20the%20morph%0Aimages.%20Our%20method%20overcomes%20morph-replication%20and%20produces%20high%20quality%0Areconstructions%20of%20the%20bonafide%20images%20used%20to%20create%20the%20morphs.%20Moreover%2C%20our%0Amethod%20is%20highly%20generalizable%20across%20demorphing%20paradigms%0A%28differential/reference-free%29.%20We%20conduct%20experiments%20on%20AMSL%2C%20FRLL-Morphs%20and%0AMorDiff%20datasets%20to%20showcase%20the%20efficacy%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14494v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Ddc-GAN%253A%2520Dual-Conditioned%2520GAN%2520for%2520Face%2520Demorphing%2520From%2520a%2520Single%2520Morph%26entry.906535625%3DNitish%2520Shukla%2520and%2520Arun%2520Ross%26entry.1292438233%3D%2520%2520A%2520facial%2520morph%2520is%2520an%2520image%2520created%2520by%2520combining%2520two%2520face%2520images%2520pertaining%2520to%250Atwo%2520distinct%2520identities.%2520Face%2520demorphing%2520inverts%2520the%2520process%2520and%2520tries%2520to%250Arecover%2520the%2520original%2520images%2520constituting%2520a%2520facial%2520morph.%2520While%2520morph%2520attack%250Adetection%2520%2528MAD%2529%2520techniques%2520can%2520be%2520used%2520to%2520flag%2520morph%2520images%252C%2520they%2520do%2520not%250Adivulge%2520any%2520visual%2520information%2520about%2520the%2520faces%2520used%2520to%2520create%2520them.%2520Demorphing%250Ahelps%2520address%2520this%2520problem.%2520Existing%2520demorphing%2520techniques%2520are%2520either%2520very%250Arestrictive%2520%2528assume%2520identities%2520during%2520testing%2529%2520or%2520produce%2520feeble%2520outputs%2520%2528both%250Aoutputs%2520look%2520very%2520similar%2529.%2520In%2520this%2520paper%252C%2520we%2520overcome%2520these%2520issues%2520by%250Aproposing%2520dc-GAN%252C%2520a%2520novel%2520GAN-based%2520demorphing%2520method%2520conditioned%2520on%2520the%2520morph%250Aimages.%2520Our%2520method%2520overcomes%2520morph-replication%2520and%2520produces%2520high%2520quality%250Areconstructions%2520of%2520the%2520bonafide%2520images%2520used%2520to%2520create%2520the%2520morphs.%2520Moreover%252C%2520our%250Amethod%2520is%2520highly%2520generalizable%2520across%2520demorphing%2520paradigms%250A%2528differential/reference-free%2529.%2520We%2520conduct%2520experiments%2520on%2520AMSL%252C%2520FRLL-Morphs%2520and%250AMorDiff%2520datasets%2520to%2520showcase%2520the%2520efficacy%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14494v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=dc-GAN%3A%20Dual-Conditioned%20GAN%20for%20Face%20Demorphing%20From%20a%20Single%20Morph&entry.906535625=Nitish%20Shukla%20and%20Arun%20Ross&entry.1292438233=%20%20A%20facial%20morph%20is%20an%20image%20created%20by%20combining%20two%20face%20images%20pertaining%20to%0Atwo%20distinct%20identities.%20Face%20demorphing%20inverts%20the%20process%20and%20tries%20to%0Arecover%20the%20original%20images%20constituting%20a%20facial%20morph.%20While%20morph%20attack%0Adetection%20%28MAD%29%20techniques%20can%20be%20used%20to%20flag%20morph%20images%2C%20they%20do%20not%0Adivulge%20any%20visual%20information%20about%20the%20faces%20used%20to%20create%20them.%20Demorphing%0Ahelps%20address%20this%20problem.%20Existing%20demorphing%20techniques%20are%20either%20very%0Arestrictive%20%28assume%20identities%20during%20testing%29%20or%20produce%20feeble%20outputs%20%28both%0Aoutputs%20look%20very%20similar%29.%20In%20this%20paper%2C%20we%20overcome%20these%20issues%20by%0Aproposing%20dc-GAN%2C%20a%20novel%20GAN-based%20demorphing%20method%20conditioned%20on%20the%20morph%0Aimages.%20Our%20method%20overcomes%20morph-replication%20and%20produces%20high%20quality%0Areconstructions%20of%20the%20bonafide%20images%20used%20to%20create%20the%20morphs.%20Moreover%2C%20our%0Amethod%20is%20highly%20generalizable%20across%20demorphing%20paradigms%0A%28differential/reference-free%29.%20We%20conduct%20experiments%20on%20AMSL%2C%20FRLL-Morphs%20and%0AMorDiff%20datasets%20to%20showcase%20the%20efficacy%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14494v3&entry.124074799=Read"},
{"title": "A Frugal Model for Accurate Early Student Failure Prediction", "author": "Ikram Gagaoua and Armelle Brun and Anne Boyer", "abstract": "  Predicting student success or failure is vital for timely interventions and\npersonalized support. Early failure prediction is particularly crucial, yet\nlimited data availability in the early stages poses challenges, one of the\npossible solutions is to make use of additional data from other contexts,\nhowever, this might lead to overconsumption with no guarantee of better\nresults. To address this, we propose the Frugal Early Prediction (FEP) model, a\nnew hybrid model that selectively incorporates additional data, promoting data\nfrugality and efficient resource utilization. Experiments conducted on a public\ndataset from a VLE demonstrate FEP's effectiveness in reducing data usage, a\nprimary goal of this research.Experiments showcase a remarkable 27% reduction\nin data consumption, compared to a systematic use of additional data, aligning\nwith our commitment to data frugality and offering substantial benefits to\neducational institutions seeking efficient data consumption. Additionally, FEP\nalso excels in enhancing prediction accuracy. Compared to traditional\napproaches, FEP achieves an average accuracy gain of 7.3%. This not only\nhighlights the practicality and efficiency of FEP but also its superiority in\nperformance, while respecting resource constraints, providing beneficial\nfindings for educational institutions seeking data frugality.\n", "link": "http://arxiv.org/abs/2502.00017v2", "date": "2025-05-12", "relevancy": 1.7663, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4455}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.441}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Frugal%20Model%20for%20Accurate%20Early%20Student%20Failure%20Prediction&body=Title%3A%20A%20Frugal%20Model%20for%20Accurate%20Early%20Student%20Failure%20Prediction%0AAuthor%3A%20Ikram%20Gagaoua%20and%20Armelle%20Brun%20and%20Anne%20Boyer%0AAbstract%3A%20%20%20Predicting%20student%20success%20or%20failure%20is%20vital%20for%20timely%20interventions%20and%0Apersonalized%20support.%20Early%20failure%20prediction%20is%20particularly%20crucial%2C%20yet%0Alimited%20data%20availability%20in%20the%20early%20stages%20poses%20challenges%2C%20one%20of%20the%0Apossible%20solutions%20is%20to%20make%20use%20of%20additional%20data%20from%20other%20contexts%2C%0Ahowever%2C%20this%20might%20lead%20to%20overconsumption%20with%20no%20guarantee%20of%20better%0Aresults.%20To%20address%20this%2C%20we%20propose%20the%20Frugal%20Early%20Prediction%20%28FEP%29%20model%2C%20a%0Anew%20hybrid%20model%20that%20selectively%20incorporates%20additional%20data%2C%20promoting%20data%0Afrugality%20and%20efficient%20resource%20utilization.%20Experiments%20conducted%20on%20a%20public%0Adataset%20from%20a%20VLE%20demonstrate%20FEP%27s%20effectiveness%20in%20reducing%20data%20usage%2C%20a%0Aprimary%20goal%20of%20this%20research.Experiments%20showcase%20a%20remarkable%2027%25%20reduction%0Ain%20data%20consumption%2C%20compared%20to%20a%20systematic%20use%20of%20additional%20data%2C%20aligning%0Awith%20our%20commitment%20to%20data%20frugality%20and%20offering%20substantial%20benefits%20to%0Aeducational%20institutions%20seeking%20efficient%20data%20consumption.%20Additionally%2C%20FEP%0Aalso%20excels%20in%20enhancing%20prediction%20accuracy.%20Compared%20to%20traditional%0Aapproaches%2C%20FEP%20achieves%20an%20average%20accuracy%20gain%20of%207.3%25.%20This%20not%20only%0Ahighlights%20the%20practicality%20and%20efficiency%20of%20FEP%20but%20also%20its%20superiority%20in%0Aperformance%2C%20while%20respecting%20resource%20constraints%2C%20providing%20beneficial%0Afindings%20for%20educational%20institutions%20seeking%20data%20frugality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Frugal%2520Model%2520for%2520Accurate%2520Early%2520Student%2520Failure%2520Prediction%26entry.906535625%3DIkram%2520Gagaoua%2520and%2520Armelle%2520Brun%2520and%2520Anne%2520Boyer%26entry.1292438233%3D%2520%2520Predicting%2520student%2520success%2520or%2520failure%2520is%2520vital%2520for%2520timely%2520interventions%2520and%250Apersonalized%2520support.%2520Early%2520failure%2520prediction%2520is%2520particularly%2520crucial%252C%2520yet%250Alimited%2520data%2520availability%2520in%2520the%2520early%2520stages%2520poses%2520challenges%252C%2520one%2520of%2520the%250Apossible%2520solutions%2520is%2520to%2520make%2520use%2520of%2520additional%2520data%2520from%2520other%2520contexts%252C%250Ahowever%252C%2520this%2520might%2520lead%2520to%2520overconsumption%2520with%2520no%2520guarantee%2520of%2520better%250Aresults.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Frugal%2520Early%2520Prediction%2520%2528FEP%2529%2520model%252C%2520a%250Anew%2520hybrid%2520model%2520that%2520selectively%2520incorporates%2520additional%2520data%252C%2520promoting%2520data%250Afrugality%2520and%2520efficient%2520resource%2520utilization.%2520Experiments%2520conducted%2520on%2520a%2520public%250Adataset%2520from%2520a%2520VLE%2520demonstrate%2520FEP%2527s%2520effectiveness%2520in%2520reducing%2520data%2520usage%252C%2520a%250Aprimary%2520goal%2520of%2520this%2520research.Experiments%2520showcase%2520a%2520remarkable%252027%2525%2520reduction%250Ain%2520data%2520consumption%252C%2520compared%2520to%2520a%2520systematic%2520use%2520of%2520additional%2520data%252C%2520aligning%250Awith%2520our%2520commitment%2520to%2520data%2520frugality%2520and%2520offering%2520substantial%2520benefits%2520to%250Aeducational%2520institutions%2520seeking%2520efficient%2520data%2520consumption.%2520Additionally%252C%2520FEP%250Aalso%2520excels%2520in%2520enhancing%2520prediction%2520accuracy.%2520Compared%2520to%2520traditional%250Aapproaches%252C%2520FEP%2520achieves%2520an%2520average%2520accuracy%2520gain%2520of%25207.3%2525.%2520This%2520not%2520only%250Ahighlights%2520the%2520practicality%2520and%2520efficiency%2520of%2520FEP%2520but%2520also%2520its%2520superiority%2520in%250Aperformance%252C%2520while%2520respecting%2520resource%2520constraints%252C%2520providing%2520beneficial%250Afindings%2520for%2520educational%2520institutions%2520seeking%2520data%2520frugality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Frugal%20Model%20for%20Accurate%20Early%20Student%20Failure%20Prediction&entry.906535625=Ikram%20Gagaoua%20and%20Armelle%20Brun%20and%20Anne%20Boyer&entry.1292438233=%20%20Predicting%20student%20success%20or%20failure%20is%20vital%20for%20timely%20interventions%20and%0Apersonalized%20support.%20Early%20failure%20prediction%20is%20particularly%20crucial%2C%20yet%0Alimited%20data%20availability%20in%20the%20early%20stages%20poses%20challenges%2C%20one%20of%20the%0Apossible%20solutions%20is%20to%20make%20use%20of%20additional%20data%20from%20other%20contexts%2C%0Ahowever%2C%20this%20might%20lead%20to%20overconsumption%20with%20no%20guarantee%20of%20better%0Aresults.%20To%20address%20this%2C%20we%20propose%20the%20Frugal%20Early%20Prediction%20%28FEP%29%20model%2C%20a%0Anew%20hybrid%20model%20that%20selectively%20incorporates%20additional%20data%2C%20promoting%20data%0Afrugality%20and%20efficient%20resource%20utilization.%20Experiments%20conducted%20on%20a%20public%0Adataset%20from%20a%20VLE%20demonstrate%20FEP%27s%20effectiveness%20in%20reducing%20data%20usage%2C%20a%0Aprimary%20goal%20of%20this%20research.Experiments%20showcase%20a%20remarkable%2027%25%20reduction%0Ain%20data%20consumption%2C%20compared%20to%20a%20systematic%20use%20of%20additional%20data%2C%20aligning%0Awith%20our%20commitment%20to%20data%20frugality%20and%20offering%20substantial%20benefits%20to%0Aeducational%20institutions%20seeking%20efficient%20data%20consumption.%20Additionally%2C%20FEP%0Aalso%20excels%20in%20enhancing%20prediction%20accuracy.%20Compared%20to%20traditional%0Aapproaches%2C%20FEP%20achieves%20an%20average%20accuracy%20gain%20of%207.3%25.%20This%20not%20only%0Ahighlights%20the%20practicality%20and%20efficiency%20of%20FEP%20but%20also%20its%20superiority%20in%0Aperformance%2C%20while%20respecting%20resource%20constraints%2C%20providing%20beneficial%0Afindings%20for%20educational%20institutions%20seeking%20data%20frugality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00017v2&entry.124074799=Read"},
{"title": "The Leaderboard Illusion", "author": "Shivalika Singh and Yiyang Nan and Alex Wang and Daniel D'Souza and Sayash Kapoor and Ahmet \u00dcst\u00fcn and Sanmi Koyejo and Yuntian Deng and Shayne Longpre and Noah A. Smith and Beyza Ermis and Marzieh Fadaee and Sara Hooker", "abstract": "  Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field\n", "link": "http://arxiv.org/abs/2504.20879v2", "date": "2025-05-12", "relevancy": 1.3123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4378}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Leaderboard%20Illusion&body=Title%3A%20The%20Leaderboard%20Illusion%0AAuthor%3A%20Shivalika%20Singh%20and%20Yiyang%20Nan%20and%20Alex%20Wang%20and%20Daniel%20D%27Souza%20and%20Sayash%20Kapoor%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Sanmi%20Koyejo%20and%20Yuntian%20Deng%20and%20Shayne%20Longpre%20and%20Noah%20A.%20Smith%20and%20Beyza%20Ermis%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker%0AAbstract%3A%20%20%20Measuring%20progress%20is%20fundamental%20to%20the%20advancement%20of%20any%20scientific%20field.%0AAs%20benchmarks%20play%20an%20increasingly%20central%20role%2C%20they%20also%20grow%20more%0Asusceptible%20to%20distortion.%20Chatbot%20Arena%20has%20emerged%20as%20the%20go-to%20leaderboard%0Afor%20ranking%20the%20most%20capable%20AI%20systems.%20Yet%2C%20in%20this%20work%20we%20identify%0Asystematic%20issues%20that%20have%20resulted%20in%20a%20distorted%20playing%20field.%20We%20find%20that%0Aundisclosed%20private%20testing%20practices%20benefit%20a%20handful%20of%20providers%20who%20are%0Aable%20to%20test%20multiple%20variants%20before%20public%20release%20and%20retract%20scores%20if%0Adesired.%20We%20establish%20that%20the%20ability%20of%20these%20providers%20to%20choose%20the%20best%0Ascore%20leads%20to%20biased%20Arena%20scores%20due%20to%20selective%20disclosure%20of%20performance%0Aresults.%20At%20an%20extreme%2C%20we%20identify%2027%20private%20LLM%20variants%20tested%20by%20Meta%20in%0Athe%20lead-up%20to%20the%20Llama-4%20release.%20We%20also%20establish%20that%20proprietary%20closed%0Amodels%20are%20sampled%20at%20higher%20rates%20%28number%20of%20battles%29%20and%20have%20fewer%20models%0Aremoved%20from%20the%20arena%20than%20open-weight%20and%20open-source%20alternatives.%20Both%0Athese%20policies%20lead%20to%20large%20data%20access%20asymmetries%20over%20time.%20Providers%20like%0AGoogle%20and%20OpenAI%20have%20received%20an%20estimated%2019.2%25%20and%2020.4%25%20of%20all%20data%20on%20the%0Aarena%2C%20respectively.%20In%20contrast%2C%20a%20combined%2083%20open-weight%20models%20have%20only%0Areceived%20an%20estimated%2029.7%25%20of%20the%20total%20data.%20We%20show%20that%20access%20to%20Chatbot%0AArena%20data%20yields%20substantial%20benefits%3B%20even%20limited%20additional%20data%20can%20result%0Ain%20relative%20performance%20gains%20of%20up%20to%20112%25%20on%20the%20arena%20distribution%2C%20based%20on%0Aour%20conservative%20estimates.%20Together%2C%20these%20dynamics%20result%20in%20overfitting%20to%0AArena-specific%20dynamics%20rather%20than%20general%20model%20quality.%20The%20Arena%20builds%20on%0Athe%20substantial%20efforts%20of%20both%20the%20organizers%20and%20an%20open%20community%20that%0Amaintains%20this%20valuable%20evaluation%20platform.%20We%20offer%20actionable%0Arecommendations%20to%20reform%20the%20Chatbot%20Arena%27s%20evaluation%20framework%20and%20promote%0Afairer%2C%20more%20transparent%20benchmarking%20for%20the%20field%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Leaderboard%2520Illusion%26entry.906535625%3DShivalika%2520Singh%2520and%2520Yiyang%2520Nan%2520and%2520Alex%2520Wang%2520and%2520Daniel%2520D%2527Souza%2520and%2520Sayash%2520Kapoor%2520and%2520Ahmet%2520%25C3%259Cst%25C3%25BCn%2520and%2520Sanmi%2520Koyejo%2520and%2520Yuntian%2520Deng%2520and%2520Shayne%2520Longpre%2520and%2520Noah%2520A.%2520Smith%2520and%2520Beyza%2520Ermis%2520and%2520Marzieh%2520Fadaee%2520and%2520Sara%2520Hooker%26entry.1292438233%3D%2520%2520Measuring%2520progress%2520is%2520fundamental%2520to%2520the%2520advancement%2520of%2520any%2520scientific%2520field.%250AAs%2520benchmarks%2520play%2520an%2520increasingly%2520central%2520role%252C%2520they%2520also%2520grow%2520more%250Asusceptible%2520to%2520distortion.%2520Chatbot%2520Arena%2520has%2520emerged%2520as%2520the%2520go-to%2520leaderboard%250Afor%2520ranking%2520the%2520most%2520capable%2520AI%2520systems.%2520Yet%252C%2520in%2520this%2520work%2520we%2520identify%250Asystematic%2520issues%2520that%2520have%2520resulted%2520in%2520a%2520distorted%2520playing%2520field.%2520We%2520find%2520that%250Aundisclosed%2520private%2520testing%2520practices%2520benefit%2520a%2520handful%2520of%2520providers%2520who%2520are%250Aable%2520to%2520test%2520multiple%2520variants%2520before%2520public%2520release%2520and%2520retract%2520scores%2520if%250Adesired.%2520We%2520establish%2520that%2520the%2520ability%2520of%2520these%2520providers%2520to%2520choose%2520the%2520best%250Ascore%2520leads%2520to%2520biased%2520Arena%2520scores%2520due%2520to%2520selective%2520disclosure%2520of%2520performance%250Aresults.%2520At%2520an%2520extreme%252C%2520we%2520identify%252027%2520private%2520LLM%2520variants%2520tested%2520by%2520Meta%2520in%250Athe%2520lead-up%2520to%2520the%2520Llama-4%2520release.%2520We%2520also%2520establish%2520that%2520proprietary%2520closed%250Amodels%2520are%2520sampled%2520at%2520higher%2520rates%2520%2528number%2520of%2520battles%2529%2520and%2520have%2520fewer%2520models%250Aremoved%2520from%2520the%2520arena%2520than%2520open-weight%2520and%2520open-source%2520alternatives.%2520Both%250Athese%2520policies%2520lead%2520to%2520large%2520data%2520access%2520asymmetries%2520over%2520time.%2520Providers%2520like%250AGoogle%2520and%2520OpenAI%2520have%2520received%2520an%2520estimated%252019.2%2525%2520and%252020.4%2525%2520of%2520all%2520data%2520on%2520the%250Aarena%252C%2520respectively.%2520In%2520contrast%252C%2520a%2520combined%252083%2520open-weight%2520models%2520have%2520only%250Areceived%2520an%2520estimated%252029.7%2525%2520of%2520the%2520total%2520data.%2520We%2520show%2520that%2520access%2520to%2520Chatbot%250AArena%2520data%2520yields%2520substantial%2520benefits%253B%2520even%2520limited%2520additional%2520data%2520can%2520result%250Ain%2520relative%2520performance%2520gains%2520of%2520up%2520to%2520112%2525%2520on%2520the%2520arena%2520distribution%252C%2520based%2520on%250Aour%2520conservative%2520estimates.%2520Together%252C%2520these%2520dynamics%2520result%2520in%2520overfitting%2520to%250AArena-specific%2520dynamics%2520rather%2520than%2520general%2520model%2520quality.%2520The%2520Arena%2520builds%2520on%250Athe%2520substantial%2520efforts%2520of%2520both%2520the%2520organizers%2520and%2520an%2520open%2520community%2520that%250Amaintains%2520this%2520valuable%2520evaluation%2520platform.%2520We%2520offer%2520actionable%250Arecommendations%2520to%2520reform%2520the%2520Chatbot%2520Arena%2527s%2520evaluation%2520framework%2520and%2520promote%250Afairer%252C%2520more%2520transparent%2520benchmarking%2520for%2520the%2520field%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Leaderboard%20Illusion&entry.906535625=Shivalika%20Singh%20and%20Yiyang%20Nan%20and%20Alex%20Wang%20and%20Daniel%20D%27Souza%20and%20Sayash%20Kapoor%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Sanmi%20Koyejo%20and%20Yuntian%20Deng%20and%20Shayne%20Longpre%20and%20Noah%20A.%20Smith%20and%20Beyza%20Ermis%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker&entry.1292438233=%20%20Measuring%20progress%20is%20fundamental%20to%20the%20advancement%20of%20any%20scientific%20field.%0AAs%20benchmarks%20play%20an%20increasingly%20central%20role%2C%20they%20also%20grow%20more%0Asusceptible%20to%20distortion.%20Chatbot%20Arena%20has%20emerged%20as%20the%20go-to%20leaderboard%0Afor%20ranking%20the%20most%20capable%20AI%20systems.%20Yet%2C%20in%20this%20work%20we%20identify%0Asystematic%20issues%20that%20have%20resulted%20in%20a%20distorted%20playing%20field.%20We%20find%20that%0Aundisclosed%20private%20testing%20practices%20benefit%20a%20handful%20of%20providers%20who%20are%0Aable%20to%20test%20multiple%20variants%20before%20public%20release%20and%20retract%20scores%20if%0Adesired.%20We%20establish%20that%20the%20ability%20of%20these%20providers%20to%20choose%20the%20best%0Ascore%20leads%20to%20biased%20Arena%20scores%20due%20to%20selective%20disclosure%20of%20performance%0Aresults.%20At%20an%20extreme%2C%20we%20identify%2027%20private%20LLM%20variants%20tested%20by%20Meta%20in%0Athe%20lead-up%20to%20the%20Llama-4%20release.%20We%20also%20establish%20that%20proprietary%20closed%0Amodels%20are%20sampled%20at%20higher%20rates%20%28number%20of%20battles%29%20and%20have%20fewer%20models%0Aremoved%20from%20the%20arena%20than%20open-weight%20and%20open-source%20alternatives.%20Both%0Athese%20policies%20lead%20to%20large%20data%20access%20asymmetries%20over%20time.%20Providers%20like%0AGoogle%20and%20OpenAI%20have%20received%20an%20estimated%2019.2%25%20and%2020.4%25%20of%20all%20data%20on%20the%0Aarena%2C%20respectively.%20In%20contrast%2C%20a%20combined%2083%20open-weight%20models%20have%20only%0Areceived%20an%20estimated%2029.7%25%20of%20the%20total%20data.%20We%20show%20that%20access%20to%20Chatbot%0AArena%20data%20yields%20substantial%20benefits%3B%20even%20limited%20additional%20data%20can%20result%0Ain%20relative%20performance%20gains%20of%20up%20to%20112%25%20on%20the%20arena%20distribution%2C%20based%20on%0Aour%20conservative%20estimates.%20Together%2C%20these%20dynamics%20result%20in%20overfitting%20to%0AArena-specific%20dynamics%20rather%20than%20general%20model%20quality.%20The%20Arena%20builds%20on%0Athe%20substantial%20efforts%20of%20both%20the%20organizers%20and%20an%20open%20community%20that%0Amaintains%20this%20valuable%20evaluation%20platform.%20We%20offer%20actionable%0Arecommendations%20to%20reform%20the%20Chatbot%20Arena%27s%20evaluation%20framework%20and%20promote%0Afairer%2C%20more%20transparent%20benchmarking%20for%20the%20field%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20879v2&entry.124074799=Read"},
{"title": "Hybrid Local Causal Discovery", "author": "Zhaolong Ling and Honghui Peng and Yiwen Zhang and Debo Cheng and Xingyu Wu and Peng Zhou and Kui Yu", "abstract": "  Local causal discovery aims to learn and distinguish the direct causes and\neffects of a target variable from observed data. Existing constraint-based\nlocal causal discovery methods use AND or OR rules in constructing the local\ncausal skeleton, but using either rule alone is prone to produce cascading\nerrors in the learned local causal skeleton, and thus impacting the inference\nof local causal relationships. On the other hand, directly applying score-based\nglobal causal discovery methods to local causal discovery may randomly return\nincorrect results due to the existence of local equivalence classes. To address\nthe above issues, we propose a Hybrid Local Causal Discovery algorithm, called\nHLCD. Specifically, HLCD initially utilizes a constraint-based approach\ncombined with the OR rule to obtain a candidate skeleton and then employs a\nscore-based method to eliminate redundant portions in the candidate skeleton.\nFurthermore, during the local causal orientation phase, HLCD distinguishes\nbetween V-structures and equivalence classes by comparing the local structure\nscores between the two, thereby avoiding orientation interference caused by\nlocal equivalence classes. We conducted extensive experiments with seven\nstate-of-the-art competitors on 14 benchmark Bayesian network datasets, and the\nexperimental results demonstrate that HLCD significantly outperforms existing\nlocal causal discovery algorithms.\n", "link": "http://arxiv.org/abs/2412.19507v2", "date": "2025-05-12", "relevancy": 1.8764, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4831}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4617}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Local%20Causal%20Discovery&body=Title%3A%20Hybrid%20Local%20Causal%20Discovery%0AAuthor%3A%20Zhaolong%20Ling%20and%20Honghui%20Peng%20and%20Yiwen%20Zhang%20and%20Debo%20Cheng%20and%20Xingyu%20Wu%20and%20Peng%20Zhou%20and%20Kui%20Yu%0AAbstract%3A%20%20%20Local%20causal%20discovery%20aims%20to%20learn%20and%20distinguish%20the%20direct%20causes%20and%0Aeffects%20of%20a%20target%20variable%20from%20observed%20data.%20Existing%20constraint-based%0Alocal%20causal%20discovery%20methods%20use%20AND%20or%20OR%20rules%20in%20constructing%20the%20local%0Acausal%20skeleton%2C%20but%20using%20either%20rule%20alone%20is%20prone%20to%20produce%20cascading%0Aerrors%20in%20the%20learned%20local%20causal%20skeleton%2C%20and%20thus%20impacting%20the%20inference%0Aof%20local%20causal%20relationships.%20On%20the%20other%20hand%2C%20directly%20applying%20score-based%0Aglobal%20causal%20discovery%20methods%20to%20local%20causal%20discovery%20may%20randomly%20return%0Aincorrect%20results%20due%20to%20the%20existence%20of%20local%20equivalence%20classes.%20To%20address%0Athe%20above%20issues%2C%20we%20propose%20a%20Hybrid%20Local%20Causal%20Discovery%20algorithm%2C%20called%0AHLCD.%20Specifically%2C%20HLCD%20initially%20utilizes%20a%20constraint-based%20approach%0Acombined%20with%20the%20OR%20rule%20to%20obtain%20a%20candidate%20skeleton%20and%20then%20employs%20a%0Ascore-based%20method%20to%20eliminate%20redundant%20portions%20in%20the%20candidate%20skeleton.%0AFurthermore%2C%20during%20the%20local%20causal%20orientation%20phase%2C%20HLCD%20distinguishes%0Abetween%20V-structures%20and%20equivalence%20classes%20by%20comparing%20the%20local%20structure%0Ascores%20between%20the%20two%2C%20thereby%20avoiding%20orientation%20interference%20caused%20by%0Alocal%20equivalence%20classes.%20We%20conducted%20extensive%20experiments%20with%20seven%0Astate-of-the-art%20competitors%20on%2014%20benchmark%20Bayesian%20network%20datasets%2C%20and%20the%0Aexperimental%20results%20demonstrate%20that%20HLCD%20significantly%20outperforms%20existing%0Alocal%20causal%20discovery%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Local%2520Causal%2520Discovery%26entry.906535625%3DZhaolong%2520Ling%2520and%2520Honghui%2520Peng%2520and%2520Yiwen%2520Zhang%2520and%2520Debo%2520Cheng%2520and%2520Xingyu%2520Wu%2520and%2520Peng%2520Zhou%2520and%2520Kui%2520Yu%26entry.1292438233%3D%2520%2520Local%2520causal%2520discovery%2520aims%2520to%2520learn%2520and%2520distinguish%2520the%2520direct%2520causes%2520and%250Aeffects%2520of%2520a%2520target%2520variable%2520from%2520observed%2520data.%2520Existing%2520constraint-based%250Alocal%2520causal%2520discovery%2520methods%2520use%2520AND%2520or%2520OR%2520rules%2520in%2520constructing%2520the%2520local%250Acausal%2520skeleton%252C%2520but%2520using%2520either%2520rule%2520alone%2520is%2520prone%2520to%2520produce%2520cascading%250Aerrors%2520in%2520the%2520learned%2520local%2520causal%2520skeleton%252C%2520and%2520thus%2520impacting%2520the%2520inference%250Aof%2520local%2520causal%2520relationships.%2520On%2520the%2520other%2520hand%252C%2520directly%2520applying%2520score-based%250Aglobal%2520causal%2520discovery%2520methods%2520to%2520local%2520causal%2520discovery%2520may%2520randomly%2520return%250Aincorrect%2520results%2520due%2520to%2520the%2520existence%2520of%2520local%2520equivalence%2520classes.%2520To%2520address%250Athe%2520above%2520issues%252C%2520we%2520propose%2520a%2520Hybrid%2520Local%2520Causal%2520Discovery%2520algorithm%252C%2520called%250AHLCD.%2520Specifically%252C%2520HLCD%2520initially%2520utilizes%2520a%2520constraint-based%2520approach%250Acombined%2520with%2520the%2520OR%2520rule%2520to%2520obtain%2520a%2520candidate%2520skeleton%2520and%2520then%2520employs%2520a%250Ascore-based%2520method%2520to%2520eliminate%2520redundant%2520portions%2520in%2520the%2520candidate%2520skeleton.%250AFurthermore%252C%2520during%2520the%2520local%2520causal%2520orientation%2520phase%252C%2520HLCD%2520distinguishes%250Abetween%2520V-structures%2520and%2520equivalence%2520classes%2520by%2520comparing%2520the%2520local%2520structure%250Ascores%2520between%2520the%2520two%252C%2520thereby%2520avoiding%2520orientation%2520interference%2520caused%2520by%250Alocal%2520equivalence%2520classes.%2520We%2520conducted%2520extensive%2520experiments%2520with%2520seven%250Astate-of-the-art%2520competitors%2520on%252014%2520benchmark%2520Bayesian%2520network%2520datasets%252C%2520and%2520the%250Aexperimental%2520results%2520demonstrate%2520that%2520HLCD%2520significantly%2520outperforms%2520existing%250Alocal%2520causal%2520discovery%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Local%20Causal%20Discovery&entry.906535625=Zhaolong%20Ling%20and%20Honghui%20Peng%20and%20Yiwen%20Zhang%20and%20Debo%20Cheng%20and%20Xingyu%20Wu%20and%20Peng%20Zhou%20and%20Kui%20Yu&entry.1292438233=%20%20Local%20causal%20discovery%20aims%20to%20learn%20and%20distinguish%20the%20direct%20causes%20and%0Aeffects%20of%20a%20target%20variable%20from%20observed%20data.%20Existing%20constraint-based%0Alocal%20causal%20discovery%20methods%20use%20AND%20or%20OR%20rules%20in%20constructing%20the%20local%0Acausal%20skeleton%2C%20but%20using%20either%20rule%20alone%20is%20prone%20to%20produce%20cascading%0Aerrors%20in%20the%20learned%20local%20causal%20skeleton%2C%20and%20thus%20impacting%20the%20inference%0Aof%20local%20causal%20relationships.%20On%20the%20other%20hand%2C%20directly%20applying%20score-based%0Aglobal%20causal%20discovery%20methods%20to%20local%20causal%20discovery%20may%20randomly%20return%0Aincorrect%20results%20due%20to%20the%20existence%20of%20local%20equivalence%20classes.%20To%20address%0Athe%20above%20issues%2C%20we%20propose%20a%20Hybrid%20Local%20Causal%20Discovery%20algorithm%2C%20called%0AHLCD.%20Specifically%2C%20HLCD%20initially%20utilizes%20a%20constraint-based%20approach%0Acombined%20with%20the%20OR%20rule%20to%20obtain%20a%20candidate%20skeleton%20and%20then%20employs%20a%0Ascore-based%20method%20to%20eliminate%20redundant%20portions%20in%20the%20candidate%20skeleton.%0AFurthermore%2C%20during%20the%20local%20causal%20orientation%20phase%2C%20HLCD%20distinguishes%0Abetween%20V-structures%20and%20equivalence%20classes%20by%20comparing%20the%20local%20structure%0Ascores%20between%20the%20two%2C%20thereby%20avoiding%20orientation%20interference%20caused%20by%0Alocal%20equivalence%20classes.%20We%20conducted%20extensive%20experiments%20with%20seven%0Astate-of-the-art%20competitors%20on%2014%20benchmark%20Bayesian%20network%20datasets%2C%20and%20the%0Aexperimental%20results%20demonstrate%20that%20HLCD%20significantly%20outperforms%20existing%0Alocal%20causal%20discovery%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19507v2&entry.124074799=Read"},
{"title": "Training neural control variates using correlated configurations", "author": "Hyunwoo Oh", "abstract": "  Neural control variates (NCVs) have emerged as a powerful tool for variance\nreduction in Monte Carlo (MC) simulations, particularly in high-dimensional\nproblems where traditional control variates are difficult to construct\nanalytically. By training neural networks to learn auxiliary functions\ncorrelated with the target observable, NCVs can significantly reduce estimator\nvariance while preserving unbiasedness. However, a critical but often\noverlooked aspect of NCV training is the role of autocorrelated samples\ngenerated by Markov Chain Monte Carlo (MCMC). While such samples are typically\ndiscarded for error estimation due to their statistical redundancy, they may\ncontain useful information about the structure of the underlying probability\ndistribution that can benefit the training process. In this work, we\nsystematically examine the effect of using correlated configurations in\ntraining neural control variates. We demonstrate, both conceptually and\nnumerically, that training on correlated data can improve control variate\nperformance, especially in settings with limited computational resources. Our\nanalysis includes empirical results from $U(1)$ gauge theory and scalar field\ntheory, illustrating when and how autocorrelated samples enhance NCV\nconstruction. These findings provide practical guidance for the efficient use\nof MCMC data in training neural networks.\n", "link": "http://arxiv.org/abs/2505.07719v1", "date": "2025-05-12", "relevancy": 1.3609, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4581}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20neural%20control%20variates%20using%20correlated%20configurations&body=Title%3A%20Training%20neural%20control%20variates%20using%20correlated%20configurations%0AAuthor%3A%20Hyunwoo%20Oh%0AAbstract%3A%20%20%20Neural%20control%20variates%20%28NCVs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20variance%0Areduction%20in%20Monte%20Carlo%20%28MC%29%20simulations%2C%20particularly%20in%20high-dimensional%0Aproblems%20where%20traditional%20control%20variates%20are%20difficult%20to%20construct%0Aanalytically.%20By%20training%20neural%20networks%20to%20learn%20auxiliary%20functions%0Acorrelated%20with%20the%20target%20observable%2C%20NCVs%20can%20significantly%20reduce%20estimator%0Avariance%20while%20preserving%20unbiasedness.%20However%2C%20a%20critical%20but%20often%0Aoverlooked%20aspect%20of%20NCV%20training%20is%20the%20role%20of%20autocorrelated%20samples%0Agenerated%20by%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29.%20While%20such%20samples%20are%20typically%0Adiscarded%20for%20error%20estimation%20due%20to%20their%20statistical%20redundancy%2C%20they%20may%0Acontain%20useful%20information%20about%20the%20structure%20of%20the%20underlying%20probability%0Adistribution%20that%20can%20benefit%20the%20training%20process.%20In%20this%20work%2C%20we%0Asystematically%20examine%20the%20effect%20of%20using%20correlated%20configurations%20in%0Atraining%20neural%20control%20variates.%20We%20demonstrate%2C%20both%20conceptually%20and%0Anumerically%2C%20that%20training%20on%20correlated%20data%20can%20improve%20control%20variate%0Aperformance%2C%20especially%20in%20settings%20with%20limited%20computational%20resources.%20Our%0Aanalysis%20includes%20empirical%20results%20from%20%24U%281%29%24%20gauge%20theory%20and%20scalar%20field%0Atheory%2C%20illustrating%20when%20and%20how%20autocorrelated%20samples%20enhance%20NCV%0Aconstruction.%20These%20findings%20provide%20practical%20guidance%20for%20the%20efficient%20use%0Aof%20MCMC%20data%20in%20training%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520neural%2520control%2520variates%2520using%2520correlated%2520configurations%26entry.906535625%3DHyunwoo%2520Oh%26entry.1292438233%3D%2520%2520Neural%2520control%2520variates%2520%2528NCVs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520variance%250Areduction%2520in%2520Monte%2520Carlo%2520%2528MC%2529%2520simulations%252C%2520particularly%2520in%2520high-dimensional%250Aproblems%2520where%2520traditional%2520control%2520variates%2520are%2520difficult%2520to%2520construct%250Aanalytically.%2520By%2520training%2520neural%2520networks%2520to%2520learn%2520auxiliary%2520functions%250Acorrelated%2520with%2520the%2520target%2520observable%252C%2520NCVs%2520can%2520significantly%2520reduce%2520estimator%250Avariance%2520while%2520preserving%2520unbiasedness.%2520However%252C%2520a%2520critical%2520but%2520often%250Aoverlooked%2520aspect%2520of%2520NCV%2520training%2520is%2520the%2520role%2520of%2520autocorrelated%2520samples%250Agenerated%2520by%2520Markov%2520Chain%2520Monte%2520Carlo%2520%2528MCMC%2529.%2520While%2520such%2520samples%2520are%2520typically%250Adiscarded%2520for%2520error%2520estimation%2520due%2520to%2520their%2520statistical%2520redundancy%252C%2520they%2520may%250Acontain%2520useful%2520information%2520about%2520the%2520structure%2520of%2520the%2520underlying%2520probability%250Adistribution%2520that%2520can%2520benefit%2520the%2520training%2520process.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520examine%2520the%2520effect%2520of%2520using%2520correlated%2520configurations%2520in%250Atraining%2520neural%2520control%2520variates.%2520We%2520demonstrate%252C%2520both%2520conceptually%2520and%250Anumerically%252C%2520that%2520training%2520on%2520correlated%2520data%2520can%2520improve%2520control%2520variate%250Aperformance%252C%2520especially%2520in%2520settings%2520with%2520limited%2520computational%2520resources.%2520Our%250Aanalysis%2520includes%2520empirical%2520results%2520from%2520%2524U%25281%2529%2524%2520gauge%2520theory%2520and%2520scalar%2520field%250Atheory%252C%2520illustrating%2520when%2520and%2520how%2520autocorrelated%2520samples%2520enhance%2520NCV%250Aconstruction.%2520These%2520findings%2520provide%2520practical%2520guidance%2520for%2520the%2520efficient%2520use%250Aof%2520MCMC%2520data%2520in%2520training%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20neural%20control%20variates%20using%20correlated%20configurations&entry.906535625=Hyunwoo%20Oh&entry.1292438233=%20%20Neural%20control%20variates%20%28NCVs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20variance%0Areduction%20in%20Monte%20Carlo%20%28MC%29%20simulations%2C%20particularly%20in%20high-dimensional%0Aproblems%20where%20traditional%20control%20variates%20are%20difficult%20to%20construct%0Aanalytically.%20By%20training%20neural%20networks%20to%20learn%20auxiliary%20functions%0Acorrelated%20with%20the%20target%20observable%2C%20NCVs%20can%20significantly%20reduce%20estimator%0Avariance%20while%20preserving%20unbiasedness.%20However%2C%20a%20critical%20but%20often%0Aoverlooked%20aspect%20of%20NCV%20training%20is%20the%20role%20of%20autocorrelated%20samples%0Agenerated%20by%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29.%20While%20such%20samples%20are%20typically%0Adiscarded%20for%20error%20estimation%20due%20to%20their%20statistical%20redundancy%2C%20they%20may%0Acontain%20useful%20information%20about%20the%20structure%20of%20the%20underlying%20probability%0Adistribution%20that%20can%20benefit%20the%20training%20process.%20In%20this%20work%2C%20we%0Asystematically%20examine%20the%20effect%20of%20using%20correlated%20configurations%20in%0Atraining%20neural%20control%20variates.%20We%20demonstrate%2C%20both%20conceptually%20and%0Anumerically%2C%20that%20training%20on%20correlated%20data%20can%20improve%20control%20variate%0Aperformance%2C%20especially%20in%20settings%20with%20limited%20computational%20resources.%20Our%0Aanalysis%20includes%20empirical%20results%20from%20%24U%281%29%24%20gauge%20theory%20and%20scalar%20field%0Atheory%2C%20illustrating%20when%20and%20how%20autocorrelated%20samples%20enhance%20NCV%0Aconstruction.%20These%20findings%20provide%20practical%20guidance%20for%20the%20efficient%20use%0Aof%20MCMC%20data%20in%20training%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07719v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


