<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240818.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for\n  Realistic Endoscopic Reconstruction", "author": "Sierra Bonilla and Shuai Zhang and Dimitrios Psychogyios and Danail Stoyanov and Francisco Vasconcelos and Sophia Bano", "abstract": "  Within colorectal cancer diagnostics, conventional colonoscopy techniques\nface critical limitations, including a limited field of view and a lack of\ndepth information, which can impede the detection of precancerous lesions.\nCurrent methods struggle to provide comprehensive and accurate 3D\nreconstructions of the colonic surface which can help minimize the missing\nregions and reinspection for pre-cancerous polyps. Addressing this, we\nintroduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting\n(3D GS) combined with a Recurrent Neural Network-based Simultaneous\nLocalization and Mapping (RNNSLAM) system. By introducing geometric and depth\nregularization into the 3D GS framework, our approach ensures more accurate\nalignment of Gaussians with the colon surface, resulting in smoother 3D\nreconstructions with novel viewing of detailed textures and structures.\nEvaluations across three diverse datasets show that Gaussian Pancakes enhances\nnovel view synthesis quality, surpassing current leading methods with a 18%\nboost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster\nrendering and more than 10X shorter training times, making it a practical tool\nfor real-time applications. Hence, this holds promise for achieving clinical\ntranslation for better detection and diagnosis of colorectal cancer.\n", "link": "http://arxiv.org/abs/2404.06128v2", "date": "2024-08-16", "relevancy": 3.0946, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6993}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6206}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Pancakes%3A%20Geometrically-Regularized%203D%20Gaussian%20Splatting%20for%0A%20%20Realistic%20Endoscopic%20Reconstruction&body=Title%3A%20Gaussian%20Pancakes%3A%20Geometrically-Regularized%203D%20Gaussian%20Splatting%20for%0A%20%20Realistic%20Endoscopic%20Reconstruction%0AAuthor%3A%20Sierra%20Bonilla%20and%20Shuai%20Zhang%20and%20Dimitrios%20Psychogyios%20and%20Danail%20Stoyanov%20and%20Francisco%20Vasconcelos%20and%20Sophia%20Bano%0AAbstract%3A%20%20%20Within%20colorectal%20cancer%20diagnostics%2C%20conventional%20colonoscopy%20techniques%0Aface%20critical%20limitations%2C%20including%20a%20limited%20field%20of%20view%20and%20a%20lack%20of%0Adepth%20information%2C%20which%20can%20impede%20the%20detection%20of%20precancerous%20lesions.%0ACurrent%20methods%20struggle%20to%20provide%20comprehensive%20and%20accurate%203D%0Areconstructions%20of%20the%20colonic%20surface%20which%20can%20help%20minimize%20the%20missing%0Aregions%20and%20reinspection%20for%20pre-cancerous%20polyps.%20Addressing%20this%2C%20we%0Aintroduce%20%27Gaussian%20Pancakes%27%2C%20a%20method%20that%20leverages%203D%20Gaussian%20Splatting%0A%283D%20GS%29%20combined%20with%20a%20Recurrent%20Neural%20Network-based%20Simultaneous%0ALocalization%20and%20Mapping%20%28RNNSLAM%29%20system.%20By%20introducing%20geometric%20and%20depth%0Aregularization%20into%20the%203D%20GS%20framework%2C%20our%20approach%20ensures%20more%20accurate%0Aalignment%20of%20Gaussians%20with%20the%20colon%20surface%2C%20resulting%20in%20smoother%203D%0Areconstructions%20with%20novel%20viewing%20of%20detailed%20textures%20and%20structures.%0AEvaluations%20across%20three%20diverse%20datasets%20show%20that%20Gaussian%20Pancakes%20enhances%0Anovel%20view%20synthesis%20quality%2C%20surpassing%20current%20leading%20methods%20with%20a%2018%25%0Aboost%20in%20PSNR%20and%20a%2016%25%20improvement%20in%20SSIM.%20It%20also%20delivers%20over%20100X%20faster%0Arendering%20and%20more%20than%2010X%20shorter%20training%20times%2C%20making%20it%20a%20practical%20tool%0Afor%20real-time%20applications.%20Hence%2C%20this%20holds%20promise%20for%20achieving%20clinical%0Atranslation%20for%20better%20detection%20and%20diagnosis%20of%20colorectal%20cancer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Pancakes%253A%2520Geometrically-Regularized%25203D%2520Gaussian%2520Splatting%2520for%250A%2520%2520Realistic%2520Endoscopic%2520Reconstruction%26entry.906535625%3DSierra%2520Bonilla%2520and%2520Shuai%2520Zhang%2520and%2520Dimitrios%2520Psychogyios%2520and%2520Danail%2520Stoyanov%2520and%2520Francisco%2520Vasconcelos%2520and%2520Sophia%2520Bano%26entry.1292438233%3D%2520%2520Within%2520colorectal%2520cancer%2520diagnostics%252C%2520conventional%2520colonoscopy%2520techniques%250Aface%2520critical%2520limitations%252C%2520including%2520a%2520limited%2520field%2520of%2520view%2520and%2520a%2520lack%2520of%250Adepth%2520information%252C%2520which%2520can%2520impede%2520the%2520detection%2520of%2520precancerous%2520lesions.%250ACurrent%2520methods%2520struggle%2520to%2520provide%2520comprehensive%2520and%2520accurate%25203D%250Areconstructions%2520of%2520the%2520colonic%2520surface%2520which%2520can%2520help%2520minimize%2520the%2520missing%250Aregions%2520and%2520reinspection%2520for%2520pre-cancerous%2520polyps.%2520Addressing%2520this%252C%2520we%250Aintroduce%2520%2527Gaussian%2520Pancakes%2527%252C%2520a%2520method%2520that%2520leverages%25203D%2520Gaussian%2520Splatting%250A%25283D%2520GS%2529%2520combined%2520with%2520a%2520Recurrent%2520Neural%2520Network-based%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528RNNSLAM%2529%2520system.%2520By%2520introducing%2520geometric%2520and%2520depth%250Aregularization%2520into%2520the%25203D%2520GS%2520framework%252C%2520our%2520approach%2520ensures%2520more%2520accurate%250Aalignment%2520of%2520Gaussians%2520with%2520the%2520colon%2520surface%252C%2520resulting%2520in%2520smoother%25203D%250Areconstructions%2520with%2520novel%2520viewing%2520of%2520detailed%2520textures%2520and%2520structures.%250AEvaluations%2520across%2520three%2520diverse%2520datasets%2520show%2520that%2520Gaussian%2520Pancakes%2520enhances%250Anovel%2520view%2520synthesis%2520quality%252C%2520surpassing%2520current%2520leading%2520methods%2520with%2520a%252018%2525%250Aboost%2520in%2520PSNR%2520and%2520a%252016%2525%2520improvement%2520in%2520SSIM.%2520It%2520also%2520delivers%2520over%2520100X%2520faster%250Arendering%2520and%2520more%2520than%252010X%2520shorter%2520training%2520times%252C%2520making%2520it%2520a%2520practical%2520tool%250Afor%2520real-time%2520applications.%2520Hence%252C%2520this%2520holds%2520promise%2520for%2520achieving%2520clinical%250Atranslation%2520for%2520better%2520detection%2520and%2520diagnosis%2520of%2520colorectal%2520cancer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Pancakes%3A%20Geometrically-Regularized%203D%20Gaussian%20Splatting%20for%0A%20%20Realistic%20Endoscopic%20Reconstruction&entry.906535625=Sierra%20Bonilla%20and%20Shuai%20Zhang%20and%20Dimitrios%20Psychogyios%20and%20Danail%20Stoyanov%20and%20Francisco%20Vasconcelos%20and%20Sophia%20Bano&entry.1292438233=%20%20Within%20colorectal%20cancer%20diagnostics%2C%20conventional%20colonoscopy%20techniques%0Aface%20critical%20limitations%2C%20including%20a%20limited%20field%20of%20view%20and%20a%20lack%20of%0Adepth%20information%2C%20which%20can%20impede%20the%20detection%20of%20precancerous%20lesions.%0ACurrent%20methods%20struggle%20to%20provide%20comprehensive%20and%20accurate%203D%0Areconstructions%20of%20the%20colonic%20surface%20which%20can%20help%20minimize%20the%20missing%0Aregions%20and%20reinspection%20for%20pre-cancerous%20polyps.%20Addressing%20this%2C%20we%0Aintroduce%20%27Gaussian%20Pancakes%27%2C%20a%20method%20that%20leverages%203D%20Gaussian%20Splatting%0A%283D%20GS%29%20combined%20with%20a%20Recurrent%20Neural%20Network-based%20Simultaneous%0ALocalization%20and%20Mapping%20%28RNNSLAM%29%20system.%20By%20introducing%20geometric%20and%20depth%0Aregularization%20into%20the%203D%20GS%20framework%2C%20our%20approach%20ensures%20more%20accurate%0Aalignment%20of%20Gaussians%20with%20the%20colon%20surface%2C%20resulting%20in%20smoother%203D%0Areconstructions%20with%20novel%20viewing%20of%20detailed%20textures%20and%20structures.%0AEvaluations%20across%20three%20diverse%20datasets%20show%20that%20Gaussian%20Pancakes%20enhances%0Anovel%20view%20synthesis%20quality%2C%20surpassing%20current%20leading%20methods%20with%20a%2018%25%0Aboost%20in%20PSNR%20and%20a%2016%25%20improvement%20in%20SSIM.%20It%20also%20delivers%20over%20100X%20faster%0Arendering%20and%20more%20than%2010X%20shorter%20training%20times%2C%20making%20it%20a%20practical%20tool%0Afor%20real-time%20applications.%20Hence%2C%20this%20holds%20promise%20for%20achieving%20clinical%0Atranslation%20for%20better%20detection%20and%20diagnosis%20of%20colorectal%20cancer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06128v2&entry.124074799=Read"},
{"title": "Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS", "author": "Wei Sun and Xiaosong Zhang and Fang Wan and Yanzhao Zhou and Yuan Li and Qixiang Ye and Jianbin Jiao", "abstract": "  Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed\ncamera poses--referred to as SfM-free methods--is crucial for promoting rapid\nresponse capabilities and enhancing robustness against variable operating\nconditions. Recent SfM-free methods have integrated pose optimization,\ndesigning end-to-end frameworks for joint camera pose estimation and NVS.\nHowever, most existing works rely on per-pixel image loss functions, such as L2\nloss. In SfM-free methods, inaccurate initial poses lead to misalignment issue,\nwhich, under the constraints of per-pixel image loss functions, results in\nexcessive gradients, causing unstable optimization and poor convergence for\nNVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian\nsplatting for NVS. We use correspondences between the target and the rendered\nresult to achieve better pixel alignment, facilitating the optimization of\nrelative poses between frames. We then apply the learned poses to optimize the\nentire scene. Each 2D screen-space pixel is associated with its corresponding\n3D Gaussians through approximated surface rendering to facilitate gradient back\npropagation. Experimental results underline the superior performance and time\nefficiency of the proposed approach compared to the state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2408.08723v1", "date": "2024-08-16", "relevancy": 2.9789, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6652}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5774}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correspondence-Guided%20SfM-Free%203D%20Gaussian%20Splatting%20for%20NVS&body=Title%3A%20Correspondence-Guided%20SfM-Free%203D%20Gaussian%20Splatting%20for%20NVS%0AAuthor%3A%20Wei%20Sun%20and%20Xiaosong%20Zhang%20and%20Fang%20Wan%20and%20Yanzhao%20Zhou%20and%20Yuan%20Li%20and%20Qixiang%20Ye%20and%20Jianbin%20Jiao%0AAbstract%3A%20%20%20Novel%20View%20Synthesis%20%28NVS%29%20without%20Structure-from-Motion%20%28SfM%29%20pre-processed%0Acamera%20poses--referred%20to%20as%20SfM-free%20methods--is%20crucial%20for%20promoting%20rapid%0Aresponse%20capabilities%20and%20enhancing%20robustness%20against%20variable%20operating%0Aconditions.%20Recent%20SfM-free%20methods%20have%20integrated%20pose%20optimization%2C%0Adesigning%20end-to-end%20frameworks%20for%20joint%20camera%20pose%20estimation%20and%20NVS.%0AHowever%2C%20most%20existing%20works%20rely%20on%20per-pixel%20image%20loss%20functions%2C%20such%20as%20L2%0Aloss.%20In%20SfM-free%20methods%2C%20inaccurate%20initial%20poses%20lead%20to%20misalignment%20issue%2C%0Awhich%2C%20under%20the%20constraints%20of%20per-pixel%20image%20loss%20functions%2C%20results%20in%0Aexcessive%20gradients%2C%20causing%20unstable%20optimization%20and%20poor%20convergence%20for%0ANVS.%20In%20this%20study%2C%20we%20propose%20a%20correspondence-guided%20SfM-free%203D%20Gaussian%0Asplatting%20for%20NVS.%20We%20use%20correspondences%20between%20the%20target%20and%20the%20rendered%0Aresult%20to%20achieve%20better%20pixel%20alignment%2C%20facilitating%20the%20optimization%20of%0Arelative%20poses%20between%20frames.%20We%20then%20apply%20the%20learned%20poses%20to%20optimize%20the%0Aentire%20scene.%20Each%202D%20screen-space%20pixel%20is%20associated%20with%20its%20corresponding%0A3D%20Gaussians%20through%20approximated%20surface%20rendering%20to%20facilitate%20gradient%20back%0Apropagation.%20Experimental%20results%20underline%20the%20superior%20performance%20and%20time%0Aefficiency%20of%20the%20proposed%20approach%20compared%20to%20the%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrespondence-Guided%2520SfM-Free%25203D%2520Gaussian%2520Splatting%2520for%2520NVS%26entry.906535625%3DWei%2520Sun%2520and%2520Xiaosong%2520Zhang%2520and%2520Fang%2520Wan%2520and%2520Yanzhao%2520Zhou%2520and%2520Yuan%2520Li%2520and%2520Qixiang%2520Ye%2520and%2520Jianbin%2520Jiao%26entry.1292438233%3D%2520%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%2520without%2520Structure-from-Motion%2520%2528SfM%2529%2520pre-processed%250Acamera%2520poses--referred%2520to%2520as%2520SfM-free%2520methods--is%2520crucial%2520for%2520promoting%2520rapid%250Aresponse%2520capabilities%2520and%2520enhancing%2520robustness%2520against%2520variable%2520operating%250Aconditions.%2520Recent%2520SfM-free%2520methods%2520have%2520integrated%2520pose%2520optimization%252C%250Adesigning%2520end-to-end%2520frameworks%2520for%2520joint%2520camera%2520pose%2520estimation%2520and%2520NVS.%250AHowever%252C%2520most%2520existing%2520works%2520rely%2520on%2520per-pixel%2520image%2520loss%2520functions%252C%2520such%2520as%2520L2%250Aloss.%2520In%2520SfM-free%2520methods%252C%2520inaccurate%2520initial%2520poses%2520lead%2520to%2520misalignment%2520issue%252C%250Awhich%252C%2520under%2520the%2520constraints%2520of%2520per-pixel%2520image%2520loss%2520functions%252C%2520results%2520in%250Aexcessive%2520gradients%252C%2520causing%2520unstable%2520optimization%2520and%2520poor%2520convergence%2520for%250ANVS.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520correspondence-guided%2520SfM-free%25203D%2520Gaussian%250Asplatting%2520for%2520NVS.%2520We%2520use%2520correspondences%2520between%2520the%2520target%2520and%2520the%2520rendered%250Aresult%2520to%2520achieve%2520better%2520pixel%2520alignment%252C%2520facilitating%2520the%2520optimization%2520of%250Arelative%2520poses%2520between%2520frames.%2520We%2520then%2520apply%2520the%2520learned%2520poses%2520to%2520optimize%2520the%250Aentire%2520scene.%2520Each%25202D%2520screen-space%2520pixel%2520is%2520associated%2520with%2520its%2520corresponding%250A3D%2520Gaussians%2520through%2520approximated%2520surface%2520rendering%2520to%2520facilitate%2520gradient%2520back%250Apropagation.%2520Experimental%2520results%2520underline%2520the%2520superior%2520performance%2520and%2520time%250Aefficiency%2520of%2520the%2520proposed%2520approach%2520compared%2520to%2520the%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correspondence-Guided%20SfM-Free%203D%20Gaussian%20Splatting%20for%20NVS&entry.906535625=Wei%20Sun%20and%20Xiaosong%20Zhang%20and%20Fang%20Wan%20and%20Yanzhao%20Zhou%20and%20Yuan%20Li%20and%20Qixiang%20Ye%20and%20Jianbin%20Jiao&entry.1292438233=%20%20Novel%20View%20Synthesis%20%28NVS%29%20without%20Structure-from-Motion%20%28SfM%29%20pre-processed%0Acamera%20poses--referred%20to%20as%20SfM-free%20methods--is%20crucial%20for%20promoting%20rapid%0Aresponse%20capabilities%20and%20enhancing%20robustness%20against%20variable%20operating%0Aconditions.%20Recent%20SfM-free%20methods%20have%20integrated%20pose%20optimization%2C%0Adesigning%20end-to-end%20frameworks%20for%20joint%20camera%20pose%20estimation%20and%20NVS.%0AHowever%2C%20most%20existing%20works%20rely%20on%20per-pixel%20image%20loss%20functions%2C%20such%20as%20L2%0Aloss.%20In%20SfM-free%20methods%2C%20inaccurate%20initial%20poses%20lead%20to%20misalignment%20issue%2C%0Awhich%2C%20under%20the%20constraints%20of%20per-pixel%20image%20loss%20functions%2C%20results%20in%0Aexcessive%20gradients%2C%20causing%20unstable%20optimization%20and%20poor%20convergence%20for%0ANVS.%20In%20this%20study%2C%20we%20propose%20a%20correspondence-guided%20SfM-free%203D%20Gaussian%0Asplatting%20for%20NVS.%20We%20use%20correspondences%20between%20the%20target%20and%20the%20rendered%0Aresult%20to%20achieve%20better%20pixel%20alignment%2C%20facilitating%20the%20optimization%20of%0Arelative%20poses%20between%20frames.%20We%20then%20apply%20the%20learned%20poses%20to%20optimize%20the%0Aentire%20scene.%20Each%202D%20screen-space%20pixel%20is%20associated%20with%20its%20corresponding%0A3D%20Gaussians%20through%20approximated%20surface%20rendering%20to%20facilitate%20gradient%20back%0Apropagation.%20Experimental%20results%20underline%20the%20superior%20performance%20and%20time%0Aefficiency%20of%20the%20proposed%20approach%20compared%20to%20the%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08723v1&entry.124074799=Read"},
{"title": "DPA: Dual Prototypes Alignment for Unsupervised Adaptation of\n  Vision-Language Models", "author": "Eman Ali and Sathira Silva and Muhammad Haris Khan", "abstract": "  Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential in\nzero-shot image classification. However, adapting these models to new domains\nremains challenging, especially in unsupervised settings where labelled data is\nunavailable. Recent research has proposed pseudo-labelling approaches to adapt\nCLIP in an unsupervised manner using unlabelled target data. Nonetheless, these\nmethods struggle due to noisy pseudo-labels resulting from the misalignment\nbetween CLIP's visual and textual representations. This study introduces DPA,\nan unsupervised domain adaptation method for VLMs. DPA introduces the concept\nof dual prototypes, acting as distinct classifiers, along with the convex\ncombination of their outputs, thereby leading to accurate pseudo-label\nconstruction. Next, it ranks pseudo-labels to facilitate robust self-training,\nparticularly during early training. Finally, it addresses visual-textual\nmisalignment by aligning textual prototypes with image prototypes to further\nimprove the adaptation performance. Experiments on 13 downstream vision tasks\ndemonstrate that DPA significantly outperforms zero-shot CLIP and the\nstate-of-the-art unsupervised adaptation baselines.\n", "link": "http://arxiv.org/abs/2408.08855v1", "date": "2024-08-16", "relevancy": 2.8992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6435}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPA%3A%20Dual%20Prototypes%20Alignment%20for%20Unsupervised%20Adaptation%20of%0A%20%20Vision-Language%20Models&body=Title%3A%20DPA%3A%20Dual%20Prototypes%20Alignment%20for%20Unsupervised%20Adaptation%20of%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Eman%20Ali%20and%20Sathira%20Silva%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%2C%20e.g.%2C%20CLIP%2C%20have%20shown%20remarkable%20potential%20in%0Azero-shot%20image%20classification.%20However%2C%20adapting%20these%20models%20to%20new%20domains%0Aremains%20challenging%2C%20especially%20in%20unsupervised%20settings%20where%20labelled%20data%20is%0Aunavailable.%20Recent%20research%20has%20proposed%20pseudo-labelling%20approaches%20to%20adapt%0ACLIP%20in%20an%20unsupervised%20manner%20using%20unlabelled%20target%20data.%20Nonetheless%2C%20these%0Amethods%20struggle%20due%20to%20noisy%20pseudo-labels%20resulting%20from%20the%20misalignment%0Abetween%20CLIP%27s%20visual%20and%20textual%20representations.%20This%20study%20introduces%20DPA%2C%0Aan%20unsupervised%20domain%20adaptation%20method%20for%20VLMs.%20DPA%20introduces%20the%20concept%0Aof%20dual%20prototypes%2C%20acting%20as%20distinct%20classifiers%2C%20along%20with%20the%20convex%0Acombination%20of%20their%20outputs%2C%20thereby%20leading%20to%20accurate%20pseudo-label%0Aconstruction.%20Next%2C%20it%20ranks%20pseudo-labels%20to%20facilitate%20robust%20self-training%2C%0Aparticularly%20during%20early%20training.%20Finally%2C%20it%20addresses%20visual-textual%0Amisalignment%20by%20aligning%20textual%20prototypes%20with%20image%20prototypes%20to%20further%0Aimprove%20the%20adaptation%20performance.%20Experiments%20on%2013%20downstream%20vision%20tasks%0Ademonstrate%20that%20DPA%20significantly%20outperforms%20zero-shot%20CLIP%20and%20the%0Astate-of-the-art%20unsupervised%20adaptation%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPA%253A%2520Dual%2520Prototypes%2520Alignment%2520for%2520Unsupervised%2520Adaptation%2520of%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DEman%2520Ali%2520and%2520Sathira%2520Silva%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%252C%2520e.g.%252C%2520CLIP%252C%2520have%2520shown%2520remarkable%2520potential%2520in%250Azero-shot%2520image%2520classification.%2520However%252C%2520adapting%2520these%2520models%2520to%2520new%2520domains%250Aremains%2520challenging%252C%2520especially%2520in%2520unsupervised%2520settings%2520where%2520labelled%2520data%2520is%250Aunavailable.%2520Recent%2520research%2520has%2520proposed%2520pseudo-labelling%2520approaches%2520to%2520adapt%250ACLIP%2520in%2520an%2520unsupervised%2520manner%2520using%2520unlabelled%2520target%2520data.%2520Nonetheless%252C%2520these%250Amethods%2520struggle%2520due%2520to%2520noisy%2520pseudo-labels%2520resulting%2520from%2520the%2520misalignment%250Abetween%2520CLIP%2527s%2520visual%2520and%2520textual%2520representations.%2520This%2520study%2520introduces%2520DPA%252C%250Aan%2520unsupervised%2520domain%2520adaptation%2520method%2520for%2520VLMs.%2520DPA%2520introduces%2520the%2520concept%250Aof%2520dual%2520prototypes%252C%2520acting%2520as%2520distinct%2520classifiers%252C%2520along%2520with%2520the%2520convex%250Acombination%2520of%2520their%2520outputs%252C%2520thereby%2520leading%2520to%2520accurate%2520pseudo-label%250Aconstruction.%2520Next%252C%2520it%2520ranks%2520pseudo-labels%2520to%2520facilitate%2520robust%2520self-training%252C%250Aparticularly%2520during%2520early%2520training.%2520Finally%252C%2520it%2520addresses%2520visual-textual%250Amisalignment%2520by%2520aligning%2520textual%2520prototypes%2520with%2520image%2520prototypes%2520to%2520further%250Aimprove%2520the%2520adaptation%2520performance.%2520Experiments%2520on%252013%2520downstream%2520vision%2520tasks%250Ademonstrate%2520that%2520DPA%2520significantly%2520outperforms%2520zero-shot%2520CLIP%2520and%2520the%250Astate-of-the-art%2520unsupervised%2520adaptation%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPA%3A%20Dual%20Prototypes%20Alignment%20for%20Unsupervised%20Adaptation%20of%0A%20%20Vision-Language%20Models&entry.906535625=Eman%20Ali%20and%20Sathira%20Silva%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%2C%20e.g.%2C%20CLIP%2C%20have%20shown%20remarkable%20potential%20in%0Azero-shot%20image%20classification.%20However%2C%20adapting%20these%20models%20to%20new%20domains%0Aremains%20challenging%2C%20especially%20in%20unsupervised%20settings%20where%20labelled%20data%20is%0Aunavailable.%20Recent%20research%20has%20proposed%20pseudo-labelling%20approaches%20to%20adapt%0ACLIP%20in%20an%20unsupervised%20manner%20using%20unlabelled%20target%20data.%20Nonetheless%2C%20these%0Amethods%20struggle%20due%20to%20noisy%20pseudo-labels%20resulting%20from%20the%20misalignment%0Abetween%20CLIP%27s%20visual%20and%20textual%20representations.%20This%20study%20introduces%20DPA%2C%0Aan%20unsupervised%20domain%20adaptation%20method%20for%20VLMs.%20DPA%20introduces%20the%20concept%0Aof%20dual%20prototypes%2C%20acting%20as%20distinct%20classifiers%2C%20along%20with%20the%20convex%0Acombination%20of%20their%20outputs%2C%20thereby%20leading%20to%20accurate%20pseudo-label%0Aconstruction.%20Next%2C%20it%20ranks%20pseudo-labels%20to%20facilitate%20robust%20self-training%2C%0Aparticularly%20during%20early%20training.%20Finally%2C%20it%20addresses%20visual-textual%0Amisalignment%20by%20aligning%20textual%20prototypes%20with%20image%20prototypes%20to%20further%0Aimprove%20the%20adaptation%20performance.%20Experiments%20on%2013%20downstream%20vision%20tasks%0Ademonstrate%20that%20DPA%20significantly%20outperforms%20zero-shot%20CLIP%20and%20the%0Astate-of-the-art%20unsupervised%20adaptation%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08855v1&entry.124074799=Read"},
{"title": "PCP-MAE: Learning to Predict Centers for Point Masked Autoencoders", "author": "Xiangdong Zhang and Shaofeng Zhang and Junchi Yan", "abstract": "  Masked autoencoder has been widely explored in point cloud self-supervised\nlearning, whereby the point cloud is generally divided into visible and masked\nparts. These methods typically include an encoder accepting visible patches\n(normalized) and corresponding patch centers (position) as input, with the\ndecoder accepting the output of the encoder and the centers (position) of the\nmasked parts to reconstruct each point in the masked patches. Then, the\npre-trained encoders are used for downstream tasks. In this paper, we show a\nmotivating empirical result that when directly feeding the centers of masked\npatches to the decoder without information from the encoder, it still\nreconstructs well. In other words, the centers of patches are important and the\nreconstruction objective does not necessarily rely on representations of the\nencoder, thus preventing the encoder from learning semantic representations.\nBased on this key observation, we propose a simple yet effective method, i.e.,\nlearning to Predict Centers for Point Masked AutoEncoders (PCP-MAE) which\nguides the model to learn to predict the significant centers and use the\npredicted centers to replace the directly provided centers. Specifically, we\npropose a Predicting Center Module (PCM) that shares parameters with the\noriginal encoder with extra cross-attention to predict centers. Our method is\nof high pre-training efficiency compared to other alternatives and achieves\ngreat improvement over Point-MAE, particularly outperforming it by 5.50%,\n6.03%, and 5.17% on three variants of ScanObjectNN. The code will be made\npublicly available.\n", "link": "http://arxiv.org/abs/2408.08753v1", "date": "2024-08-16", "relevancy": 2.8536, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6493}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5343}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCP-MAE%3A%20Learning%20to%20Predict%20Centers%20for%20Point%20Masked%20Autoencoders&body=Title%3A%20PCP-MAE%3A%20Learning%20to%20Predict%20Centers%20for%20Point%20Masked%20Autoencoders%0AAuthor%3A%20Xiangdong%20Zhang%20and%20Shaofeng%20Zhang%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Masked%20autoencoder%20has%20been%20widely%20explored%20in%20point%20cloud%20self-supervised%0Alearning%2C%20whereby%20the%20point%20cloud%20is%20generally%20divided%20into%20visible%20and%20masked%0Aparts.%20These%20methods%20typically%20include%20an%20encoder%20accepting%20visible%20patches%0A%28normalized%29%20and%20corresponding%20patch%20centers%20%28position%29%20as%20input%2C%20with%20the%0Adecoder%20accepting%20the%20output%20of%20the%20encoder%20and%20the%20centers%20%28position%29%20of%20the%0Amasked%20parts%20to%20reconstruct%20each%20point%20in%20the%20masked%20patches.%20Then%2C%20the%0Apre-trained%20encoders%20are%20used%20for%20downstream%20tasks.%20In%20this%20paper%2C%20we%20show%20a%0Amotivating%20empirical%20result%20that%20when%20directly%20feeding%20the%20centers%20of%20masked%0Apatches%20to%20the%20decoder%20without%20information%20from%20the%20encoder%2C%20it%20still%0Areconstructs%20well.%20In%20other%20words%2C%20the%20centers%20of%20patches%20are%20important%20and%20the%0Areconstruction%20objective%20does%20not%20necessarily%20rely%20on%20representations%20of%20the%0Aencoder%2C%20thus%20preventing%20the%20encoder%20from%20learning%20semantic%20representations.%0ABased%20on%20this%20key%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20i.e.%2C%0Alearning%20to%20Predict%20Centers%20for%20Point%20Masked%20AutoEncoders%20%28PCP-MAE%29%20which%0Aguides%20the%20model%20to%20learn%20to%20predict%20the%20significant%20centers%20and%20use%20the%0Apredicted%20centers%20to%20replace%20the%20directly%20provided%20centers.%20Specifically%2C%20we%0Apropose%20a%20Predicting%20Center%20Module%20%28PCM%29%20that%20shares%20parameters%20with%20the%0Aoriginal%20encoder%20with%20extra%20cross-attention%20to%20predict%20centers.%20Our%20method%20is%0Aof%20high%20pre-training%20efficiency%20compared%20to%20other%20alternatives%20and%20achieves%0Agreat%20improvement%20over%20Point-MAE%2C%20particularly%20outperforming%20it%20by%205.50%25%2C%0A6.03%25%2C%20and%205.17%25%20on%20three%20variants%20of%20ScanObjectNN.%20The%20code%20will%20be%20made%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCP-MAE%253A%2520Learning%2520to%2520Predict%2520Centers%2520for%2520Point%2520Masked%2520Autoencoders%26entry.906535625%3DXiangdong%2520Zhang%2520and%2520Shaofeng%2520Zhang%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Masked%2520autoencoder%2520has%2520been%2520widely%2520explored%2520in%2520point%2520cloud%2520self-supervised%250Alearning%252C%2520whereby%2520the%2520point%2520cloud%2520is%2520generally%2520divided%2520into%2520visible%2520and%2520masked%250Aparts.%2520These%2520methods%2520typically%2520include%2520an%2520encoder%2520accepting%2520visible%2520patches%250A%2528normalized%2529%2520and%2520corresponding%2520patch%2520centers%2520%2528position%2529%2520as%2520input%252C%2520with%2520the%250Adecoder%2520accepting%2520the%2520output%2520of%2520the%2520encoder%2520and%2520the%2520centers%2520%2528position%2529%2520of%2520the%250Amasked%2520parts%2520to%2520reconstruct%2520each%2520point%2520in%2520the%2520masked%2520patches.%2520Then%252C%2520the%250Apre-trained%2520encoders%2520are%2520used%2520for%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520show%2520a%250Amotivating%2520empirical%2520result%2520that%2520when%2520directly%2520feeding%2520the%2520centers%2520of%2520masked%250Apatches%2520to%2520the%2520decoder%2520without%2520information%2520from%2520the%2520encoder%252C%2520it%2520still%250Areconstructs%2520well.%2520In%2520other%2520words%252C%2520the%2520centers%2520of%2520patches%2520are%2520important%2520and%2520the%250Areconstruction%2520objective%2520does%2520not%2520necessarily%2520rely%2520on%2520representations%2520of%2520the%250Aencoder%252C%2520thus%2520preventing%2520the%2520encoder%2520from%2520learning%2520semantic%2520representations.%250ABased%2520on%2520this%2520key%2520observation%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520i.e.%252C%250Alearning%2520to%2520Predict%2520Centers%2520for%2520Point%2520Masked%2520AutoEncoders%2520%2528PCP-MAE%2529%2520which%250Aguides%2520the%2520model%2520to%2520learn%2520to%2520predict%2520the%2520significant%2520centers%2520and%2520use%2520the%250Apredicted%2520centers%2520to%2520replace%2520the%2520directly%2520provided%2520centers.%2520Specifically%252C%2520we%250Apropose%2520a%2520Predicting%2520Center%2520Module%2520%2528PCM%2529%2520that%2520shares%2520parameters%2520with%2520the%250Aoriginal%2520encoder%2520with%2520extra%2520cross-attention%2520to%2520predict%2520centers.%2520Our%2520method%2520is%250Aof%2520high%2520pre-training%2520efficiency%2520compared%2520to%2520other%2520alternatives%2520and%2520achieves%250Agreat%2520improvement%2520over%2520Point-MAE%252C%2520particularly%2520outperforming%2520it%2520by%25205.50%2525%252C%250A6.03%2525%252C%2520and%25205.17%2525%2520on%2520three%2520variants%2520of%2520ScanObjectNN.%2520The%2520code%2520will%2520be%2520made%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCP-MAE%3A%20Learning%20to%20Predict%20Centers%20for%20Point%20Masked%20Autoencoders&entry.906535625=Xiangdong%20Zhang%20and%20Shaofeng%20Zhang%20and%20Junchi%20Yan&entry.1292438233=%20%20Masked%20autoencoder%20has%20been%20widely%20explored%20in%20point%20cloud%20self-supervised%0Alearning%2C%20whereby%20the%20point%20cloud%20is%20generally%20divided%20into%20visible%20and%20masked%0Aparts.%20These%20methods%20typically%20include%20an%20encoder%20accepting%20visible%20patches%0A%28normalized%29%20and%20corresponding%20patch%20centers%20%28position%29%20as%20input%2C%20with%20the%0Adecoder%20accepting%20the%20output%20of%20the%20encoder%20and%20the%20centers%20%28position%29%20of%20the%0Amasked%20parts%20to%20reconstruct%20each%20point%20in%20the%20masked%20patches.%20Then%2C%20the%0Apre-trained%20encoders%20are%20used%20for%20downstream%20tasks.%20In%20this%20paper%2C%20we%20show%20a%0Amotivating%20empirical%20result%20that%20when%20directly%20feeding%20the%20centers%20of%20masked%0Apatches%20to%20the%20decoder%20without%20information%20from%20the%20encoder%2C%20it%20still%0Areconstructs%20well.%20In%20other%20words%2C%20the%20centers%20of%20patches%20are%20important%20and%20the%0Areconstruction%20objective%20does%20not%20necessarily%20rely%20on%20representations%20of%20the%0Aencoder%2C%20thus%20preventing%20the%20encoder%20from%20learning%20semantic%20representations.%0ABased%20on%20this%20key%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20i.e.%2C%0Alearning%20to%20Predict%20Centers%20for%20Point%20Masked%20AutoEncoders%20%28PCP-MAE%29%20which%0Aguides%20the%20model%20to%20learn%20to%20predict%20the%20significant%20centers%20and%20use%20the%0Apredicted%20centers%20to%20replace%20the%20directly%20provided%20centers.%20Specifically%2C%20we%0Apropose%20a%20Predicting%20Center%20Module%20%28PCM%29%20that%20shares%20parameters%20with%20the%0Aoriginal%20encoder%20with%20extra%20cross-attention%20to%20predict%20centers.%20Our%20method%20is%0Aof%20high%20pre-training%20efficiency%20compared%20to%20other%20alternatives%20and%20achieves%0Agreat%20improvement%20over%20Point-MAE%2C%20particularly%20outperforming%20it%20by%205.50%25%2C%0A6.03%25%2C%20and%205.17%25%20on%20three%20variants%20of%20ScanObjectNN.%20The%20code%20will%20be%20made%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08753v1&entry.124074799=Read"},
{"title": "Representation Learning of Geometric Trees", "author": "Zheng Zhang and Allen Zhang and Ruth Nelson and Giorgio Ascoli and Liang Zhao", "abstract": "  Geometric trees are characterized by their tree-structured layout and\nspatially constrained nodes and edges, which significantly impacts their\ntopological attributes. This inherent hierarchical structure plays a crucial\nrole in domains such as neuron morphology and river geomorphology, but\ntraditional graph representation methods often overlook these specific\ncharacteristics of tree structures. To address this, we introduce a new\nrepresentation learning framework tailored for geometric trees. It first\nfeatures a unique message passing neural network, which is both provably\ngeometrical structure-recoverable and rotation-translation invariant. To\naddress the data label scarcity issue, our approach also includes two\ninnovative training targets that reflect the hierarchical ordering and\ngeometric structure of these geometric trees. This enables fully\nself-supervised learning without explicit labels. We validate our method's\neffectiveness on eight real-world datasets, demonstrating its capability to\nrepresent geometric trees.\n", "link": "http://arxiv.org/abs/2408.08799v1", "date": "2024-08-16", "relevancy": 2.7935, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20of%20Geometric%20Trees&body=Title%3A%20Representation%20Learning%20of%20Geometric%20Trees%0AAuthor%3A%20Zheng%20Zhang%20and%20Allen%20Zhang%20and%20Ruth%20Nelson%20and%20Giorgio%20Ascoli%20and%20Liang%20Zhao%0AAbstract%3A%20%20%20Geometric%20trees%20are%20characterized%20by%20their%20tree-structured%20layout%20and%0Aspatially%20constrained%20nodes%20and%20edges%2C%20which%20significantly%20impacts%20their%0Atopological%20attributes.%20This%20inherent%20hierarchical%20structure%20plays%20a%20crucial%0Arole%20in%20domains%20such%20as%20neuron%20morphology%20and%20river%20geomorphology%2C%20but%0Atraditional%20graph%20representation%20methods%20often%20overlook%20these%20specific%0Acharacteristics%20of%20tree%20structures.%20To%20address%20this%2C%20we%20introduce%20a%20new%0Arepresentation%20learning%20framework%20tailored%20for%20geometric%20trees.%20It%20first%0Afeatures%20a%20unique%20message%20passing%20neural%20network%2C%20which%20is%20both%20provably%0Ageometrical%20structure-recoverable%20and%20rotation-translation%20invariant.%20To%0Aaddress%20the%20data%20label%20scarcity%20issue%2C%20our%20approach%20also%20includes%20two%0Ainnovative%20training%20targets%20that%20reflect%20the%20hierarchical%20ordering%20and%0Ageometric%20structure%20of%20these%20geometric%20trees.%20This%20enables%20fully%0Aself-supervised%20learning%20without%20explicit%20labels.%20We%20validate%20our%20method%27s%0Aeffectiveness%20on%20eight%20real-world%20datasets%2C%20demonstrating%20its%20capability%20to%0Arepresent%20geometric%20trees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520of%2520Geometric%2520Trees%26entry.906535625%3DZheng%2520Zhang%2520and%2520Allen%2520Zhang%2520and%2520Ruth%2520Nelson%2520and%2520Giorgio%2520Ascoli%2520and%2520Liang%2520Zhao%26entry.1292438233%3D%2520%2520Geometric%2520trees%2520are%2520characterized%2520by%2520their%2520tree-structured%2520layout%2520and%250Aspatially%2520constrained%2520nodes%2520and%2520edges%252C%2520which%2520significantly%2520impacts%2520their%250Atopological%2520attributes.%2520This%2520inherent%2520hierarchical%2520structure%2520plays%2520a%2520crucial%250Arole%2520in%2520domains%2520such%2520as%2520neuron%2520morphology%2520and%2520river%2520geomorphology%252C%2520but%250Atraditional%2520graph%2520representation%2520methods%2520often%2520overlook%2520these%2520specific%250Acharacteristics%2520of%2520tree%2520structures.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520new%250Arepresentation%2520learning%2520framework%2520tailored%2520for%2520geometric%2520trees.%2520It%2520first%250Afeatures%2520a%2520unique%2520message%2520passing%2520neural%2520network%252C%2520which%2520is%2520both%2520provably%250Ageometrical%2520structure-recoverable%2520and%2520rotation-translation%2520invariant.%2520To%250Aaddress%2520the%2520data%2520label%2520scarcity%2520issue%252C%2520our%2520approach%2520also%2520includes%2520two%250Ainnovative%2520training%2520targets%2520that%2520reflect%2520the%2520hierarchical%2520ordering%2520and%250Ageometric%2520structure%2520of%2520these%2520geometric%2520trees.%2520This%2520enables%2520fully%250Aself-supervised%2520learning%2520without%2520explicit%2520labels.%2520We%2520validate%2520our%2520method%2527s%250Aeffectiveness%2520on%2520eight%2520real-world%2520datasets%252C%2520demonstrating%2520its%2520capability%2520to%250Arepresent%2520geometric%2520trees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20of%20Geometric%20Trees&entry.906535625=Zheng%20Zhang%20and%20Allen%20Zhang%20and%20Ruth%20Nelson%20and%20Giorgio%20Ascoli%20and%20Liang%20Zhao&entry.1292438233=%20%20Geometric%20trees%20are%20characterized%20by%20their%20tree-structured%20layout%20and%0Aspatially%20constrained%20nodes%20and%20edges%2C%20which%20significantly%20impacts%20their%0Atopological%20attributes.%20This%20inherent%20hierarchical%20structure%20plays%20a%20crucial%0Arole%20in%20domains%20such%20as%20neuron%20morphology%20and%20river%20geomorphology%2C%20but%0Atraditional%20graph%20representation%20methods%20often%20overlook%20these%20specific%0Acharacteristics%20of%20tree%20structures.%20To%20address%20this%2C%20we%20introduce%20a%20new%0Arepresentation%20learning%20framework%20tailored%20for%20geometric%20trees.%20It%20first%0Afeatures%20a%20unique%20message%20passing%20neural%20network%2C%20which%20is%20both%20provably%0Ageometrical%20structure-recoverable%20and%20rotation-translation%20invariant.%20To%0Aaddress%20the%20data%20label%20scarcity%20issue%2C%20our%20approach%20also%20includes%20two%0Ainnovative%20training%20targets%20that%20reflect%20the%20hierarchical%20ordering%20and%0Ageometric%20structure%20of%20these%20geometric%20trees.%20This%20enables%20fully%0Aself-supervised%20learning%20without%20explicit%20labels.%20We%20validate%20our%20method%27s%0Aeffectiveness%20on%20eight%20real-world%20datasets%2C%20demonstrating%20its%20capability%20to%0Arepresent%20geometric%20trees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08799v1&entry.124074799=Read"},
{"title": "DualFocus: Integrating Plausible Descriptions in Text-based Person\n  Re-identification", "author": "Yuchuan Deng and Zhanpeng Hu and Jiakun Han and Chuang Deng and Qijun Zhao", "abstract": "  Text-based Person Re-identification (TPR) aims to retrieve specific\nindividual images from datasets based on textual descriptions. Existing TPR\nmethods primarily focus on recognizing explicit and positive characteristics,\noften overlooking the role of negative descriptions. This oversight can lead to\nfalse positives-images that meet positive criteria but should be excluded based\non negative descriptions. To address these limitations, we introduce DualFocus,\na unified framework that integrates plausible descriptions to enhance the\ninterpretative accuracy of vision-language models in TPR tasks. DualFocus\nleverages Dual (Positive/Negative) Attribute Prompt Learning (DAPL), which\nincorporates Dual Image-Attribute Contrastive (DIAC) Learning and Sensitive\nImage-Attributes Matching (SIAM) Learning, enabling the detection of\nnon-existent attributes and reducing false positives. To achieve a balance\nbetween coarse and fine-grained alignment of visual and textual embeddings, we\npropose the Dynamic Tokenwise Similarity (DTS) loss, which refines the\nrepresentation of both matching and non-matching descriptions, thereby\nimproving the matching process through detailed and adaptable similarity\nassessments. The comprehensive experiments on CUHK-PEDES, ICFG-PEDES, and\nRSTPReid, DualFocus demonstrates superior performance over state-of-the-art\nmethods, significantly enhancing both precision and robustness in TPR.\n", "link": "http://arxiv.org/abs/2405.07459v2", "date": "2024-08-16", "relevancy": 2.7172, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5826}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5245}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualFocus%3A%20Integrating%20Plausible%20Descriptions%20in%20Text-based%20Person%0A%20%20Re-identification&body=Title%3A%20DualFocus%3A%20Integrating%20Plausible%20Descriptions%20in%20Text-based%20Person%0A%20%20Re-identification%0AAuthor%3A%20Yuchuan%20Deng%20and%20Zhanpeng%20Hu%20and%20Jiakun%20Han%20and%20Chuang%20Deng%20and%20Qijun%20Zhao%0AAbstract%3A%20%20%20Text-based%20Person%20Re-identification%20%28TPR%29%20aims%20to%20retrieve%20specific%0Aindividual%20images%20from%20datasets%20based%20on%20textual%20descriptions.%20Existing%20TPR%0Amethods%20primarily%20focus%20on%20recognizing%20explicit%20and%20positive%20characteristics%2C%0Aoften%20overlooking%20the%20role%20of%20negative%20descriptions.%20This%20oversight%20can%20lead%20to%0Afalse%20positives-images%20that%20meet%20positive%20criteria%20but%20should%20be%20excluded%20based%0Aon%20negative%20descriptions.%20To%20address%20these%20limitations%2C%20we%20introduce%20DualFocus%2C%0Aa%20unified%20framework%20that%20integrates%20plausible%20descriptions%20to%20enhance%20the%0Ainterpretative%20accuracy%20of%20vision-language%20models%20in%20TPR%20tasks.%20DualFocus%0Aleverages%20Dual%20%28Positive/Negative%29%20Attribute%20Prompt%20Learning%20%28DAPL%29%2C%20which%0Aincorporates%20Dual%20Image-Attribute%20Contrastive%20%28DIAC%29%20Learning%20and%20Sensitive%0AImage-Attributes%20Matching%20%28SIAM%29%20Learning%2C%20enabling%20the%20detection%20of%0Anon-existent%20attributes%20and%20reducing%20false%20positives.%20To%20achieve%20a%20balance%0Abetween%20coarse%20and%20fine-grained%20alignment%20of%20visual%20and%20textual%20embeddings%2C%20we%0Apropose%20the%20Dynamic%20Tokenwise%20Similarity%20%28DTS%29%20loss%2C%20which%20refines%20the%0Arepresentation%20of%20both%20matching%20and%20non-matching%20descriptions%2C%20thereby%0Aimproving%20the%20matching%20process%20through%20detailed%20and%20adaptable%20similarity%0Aassessments.%20The%20comprehensive%20experiments%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%0ARSTPReid%2C%20DualFocus%20demonstrates%20superior%20performance%20over%20state-of-the-art%0Amethods%2C%20significantly%20enhancing%20both%20precision%20and%20robustness%20in%20TPR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualFocus%253A%2520Integrating%2520Plausible%2520Descriptions%2520in%2520Text-based%2520Person%250A%2520%2520Re-identification%26entry.906535625%3DYuchuan%2520Deng%2520and%2520Zhanpeng%2520Hu%2520and%2520Jiakun%2520Han%2520and%2520Chuang%2520Deng%2520and%2520Qijun%2520Zhao%26entry.1292438233%3D%2520%2520Text-based%2520Person%2520Re-identification%2520%2528TPR%2529%2520aims%2520to%2520retrieve%2520specific%250Aindividual%2520images%2520from%2520datasets%2520based%2520on%2520textual%2520descriptions.%2520Existing%2520TPR%250Amethods%2520primarily%2520focus%2520on%2520recognizing%2520explicit%2520and%2520positive%2520characteristics%252C%250Aoften%2520overlooking%2520the%2520role%2520of%2520negative%2520descriptions.%2520This%2520oversight%2520can%2520lead%2520to%250Afalse%2520positives-images%2520that%2520meet%2520positive%2520criteria%2520but%2520should%2520be%2520excluded%2520based%250Aon%2520negative%2520descriptions.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520DualFocus%252C%250Aa%2520unified%2520framework%2520that%2520integrates%2520plausible%2520descriptions%2520to%2520enhance%2520the%250Ainterpretative%2520accuracy%2520of%2520vision-language%2520models%2520in%2520TPR%2520tasks.%2520DualFocus%250Aleverages%2520Dual%2520%2528Positive/Negative%2529%2520Attribute%2520Prompt%2520Learning%2520%2528DAPL%2529%252C%2520which%250Aincorporates%2520Dual%2520Image-Attribute%2520Contrastive%2520%2528DIAC%2529%2520Learning%2520and%2520Sensitive%250AImage-Attributes%2520Matching%2520%2528SIAM%2529%2520Learning%252C%2520enabling%2520the%2520detection%2520of%250Anon-existent%2520attributes%2520and%2520reducing%2520false%2520positives.%2520To%2520achieve%2520a%2520balance%250Abetween%2520coarse%2520and%2520fine-grained%2520alignment%2520of%2520visual%2520and%2520textual%2520embeddings%252C%2520we%250Apropose%2520the%2520Dynamic%2520Tokenwise%2520Similarity%2520%2528DTS%2529%2520loss%252C%2520which%2520refines%2520the%250Arepresentation%2520of%2520both%2520matching%2520and%2520non-matching%2520descriptions%252C%2520thereby%250Aimproving%2520the%2520matching%2520process%2520through%2520detailed%2520and%2520adaptable%2520similarity%250Aassessments.%2520The%2520comprehensive%2520experiments%2520on%2520CUHK-PEDES%252C%2520ICFG-PEDES%252C%2520and%250ARSTPReid%252C%2520DualFocus%2520demonstrates%2520superior%2520performance%2520over%2520state-of-the-art%250Amethods%252C%2520significantly%2520enhancing%2520both%2520precision%2520and%2520robustness%2520in%2520TPR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualFocus%3A%20Integrating%20Plausible%20Descriptions%20in%20Text-based%20Person%0A%20%20Re-identification&entry.906535625=Yuchuan%20Deng%20and%20Zhanpeng%20Hu%20and%20Jiakun%20Han%20and%20Chuang%20Deng%20and%20Qijun%20Zhao&entry.1292438233=%20%20Text-based%20Person%20Re-identification%20%28TPR%29%20aims%20to%20retrieve%20specific%0Aindividual%20images%20from%20datasets%20based%20on%20textual%20descriptions.%20Existing%20TPR%0Amethods%20primarily%20focus%20on%20recognizing%20explicit%20and%20positive%20characteristics%2C%0Aoften%20overlooking%20the%20role%20of%20negative%20descriptions.%20This%20oversight%20can%20lead%20to%0Afalse%20positives-images%20that%20meet%20positive%20criteria%20but%20should%20be%20excluded%20based%0Aon%20negative%20descriptions.%20To%20address%20these%20limitations%2C%20we%20introduce%20DualFocus%2C%0Aa%20unified%20framework%20that%20integrates%20plausible%20descriptions%20to%20enhance%20the%0Ainterpretative%20accuracy%20of%20vision-language%20models%20in%20TPR%20tasks.%20DualFocus%0Aleverages%20Dual%20%28Positive/Negative%29%20Attribute%20Prompt%20Learning%20%28DAPL%29%2C%20which%0Aincorporates%20Dual%20Image-Attribute%20Contrastive%20%28DIAC%29%20Learning%20and%20Sensitive%0AImage-Attributes%20Matching%20%28SIAM%29%20Learning%2C%20enabling%20the%20detection%20of%0Anon-existent%20attributes%20and%20reducing%20false%20positives.%20To%20achieve%20a%20balance%0Abetween%20coarse%20and%20fine-grained%20alignment%20of%20visual%20and%20textual%20embeddings%2C%20we%0Apropose%20the%20Dynamic%20Tokenwise%20Similarity%20%28DTS%29%20loss%2C%20which%20refines%20the%0Arepresentation%20of%20both%20matching%20and%20non-matching%20descriptions%2C%20thereby%0Aimproving%20the%20matching%20process%20through%20detailed%20and%20adaptable%20similarity%0Aassessments.%20The%20comprehensive%20experiments%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%0ARSTPReid%2C%20DualFocus%20demonstrates%20superior%20performance%20over%20state-of-the-art%0Amethods%2C%20significantly%20enhancing%20both%20precision%20and%20robustness%20in%20TPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07459v2&entry.124074799=Read"},
{"title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models", "author": "Le Xue and Manli Shu and Anas Awadalla and Jun Wang and An Yan and Senthil Purushwalkam and Honglu Zhou and Viraj Prabhu and Yutong Dai and Michael S Ryoo and Shrikant Kendre and Jieyu Zhang and Can Qin and Shu Zhang and Chia-Chih Chen and Ning Yu and Juntao Tan and Tulika Manoj Awalgaonkar and Shelby Heinecke and Huan Wang and Yejin Choi and Ludwig Schmidt and Zeyuan Chen and Silvio Savarese and Juan Carlos Niebles and Caiming Xiong and Ran Xu", "abstract": "  This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.\n", "link": "http://arxiv.org/abs/2408.08872v1", "date": "2024-08-16", "relevancy": 2.7126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5372}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xGen-MM%20%28BLIP-3%29%3A%20A%20Family%20of%20Open%20Large%20Multimodal%20Models&body=Title%3A%20xGen-MM%20%28BLIP-3%29%3A%20A%20Family%20of%20Open%20Large%20Multimodal%20Models%0AAuthor%3A%20Le%20Xue%20and%20Manli%20Shu%20and%20Anas%20Awadalla%20and%20Jun%20Wang%20and%20An%20Yan%20and%20Senthil%20Purushwalkam%20and%20Honglu%20Zhou%20and%20Viraj%20Prabhu%20and%20Yutong%20Dai%20and%20Michael%20S%20Ryoo%20and%20Shrikant%20Kendre%20and%20Jieyu%20Zhang%20and%20Can%20Qin%20and%20Shu%20Zhang%20and%20Chia-Chih%20Chen%20and%20Ning%20Yu%20and%20Juntao%20Tan%20and%20Tulika%20Manoj%20Awalgaonkar%20and%20Shelby%20Heinecke%20and%20Huan%20Wang%20and%20Yejin%20Choi%20and%20Ludwig%20Schmidt%20and%20Zeyuan%20Chen%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Caiming%20Xiong%20and%20Ran%20Xu%0AAbstract%3A%20%20%20This%20report%20introduces%20xGen-MM%20%28also%20known%20as%20BLIP-3%29%2C%20a%20framework%20for%0Adeveloping%20Large%20Multimodal%20Models%20%28LMMs%29.%20The%20framework%20comprises%20meticulously%0Acurated%20datasets%2C%20a%20training%20recipe%2C%20model%20architectures%2C%20and%20a%20resulting%20suite%0Aof%20LMMs.%20xGen-MM%2C%20short%20for%20xGen-MultiModal%2C%20expands%20the%20Salesforce%20xGen%0Ainitiative%20on%20foundation%20AI%20models.%20Our%20models%20undergo%20rigorous%20evaluation%0Aacross%20a%20range%20of%20tasks%2C%20including%20both%20single%20and%20multi-image%20benchmarks.%20Our%0Apre-trained%20base%20model%20exhibits%20strong%20in-context%20learning%20capabilities%20and%20the%0Ainstruction-tuned%20model%20demonstrates%20competitive%20performance%20among%20open-source%0ALMMs%20with%20similar%20model%20sizes.%20In%20addition%2C%20we%20introduce%20a%20safety-tuned%20model%0Awith%20DPO%2C%20aiming%20to%20mitigate%20harmful%20behaviors%20such%20as%20hallucinations%20and%0Aimprove%20safety.%20We%20open-source%20our%20models%2C%20curated%20large-scale%20datasets%2C%20and%0Aour%20fine-tuning%20codebase%20to%20facilitate%20further%20advancements%20in%20LMM%20research.%0AAssociated%20resources%20will%20be%20available%20on%20our%20project%20page%20above.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxGen-MM%2520%2528BLIP-3%2529%253A%2520A%2520Family%2520of%2520Open%2520Large%2520Multimodal%2520Models%26entry.906535625%3DLe%2520Xue%2520and%2520Manli%2520Shu%2520and%2520Anas%2520Awadalla%2520and%2520Jun%2520Wang%2520and%2520An%2520Yan%2520and%2520Senthil%2520Purushwalkam%2520and%2520Honglu%2520Zhou%2520and%2520Viraj%2520Prabhu%2520and%2520Yutong%2520Dai%2520and%2520Michael%2520S%2520Ryoo%2520and%2520Shrikant%2520Kendre%2520and%2520Jieyu%2520Zhang%2520and%2520Can%2520Qin%2520and%2520Shu%2520Zhang%2520and%2520Chia-Chih%2520Chen%2520and%2520Ning%2520Yu%2520and%2520Juntao%2520Tan%2520and%2520Tulika%2520Manoj%2520Awalgaonkar%2520and%2520Shelby%2520Heinecke%2520and%2520Huan%2520Wang%2520and%2520Yejin%2520Choi%2520and%2520Ludwig%2520Schmidt%2520and%2520Zeyuan%2520Chen%2520and%2520Silvio%2520Savarese%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Caiming%2520Xiong%2520and%2520Ran%2520Xu%26entry.1292438233%3D%2520%2520This%2520report%2520introduces%2520xGen-MM%2520%2528also%2520known%2520as%2520BLIP-3%2529%252C%2520a%2520framework%2520for%250Adeveloping%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529.%2520The%2520framework%2520comprises%2520meticulously%250Acurated%2520datasets%252C%2520a%2520training%2520recipe%252C%2520model%2520architectures%252C%2520and%2520a%2520resulting%2520suite%250Aof%2520LMMs.%2520xGen-MM%252C%2520short%2520for%2520xGen-MultiModal%252C%2520expands%2520the%2520Salesforce%2520xGen%250Ainitiative%2520on%2520foundation%2520AI%2520models.%2520Our%2520models%2520undergo%2520rigorous%2520evaluation%250Aacross%2520a%2520range%2520of%2520tasks%252C%2520including%2520both%2520single%2520and%2520multi-image%2520benchmarks.%2520Our%250Apre-trained%2520base%2520model%2520exhibits%2520strong%2520in-context%2520learning%2520capabilities%2520and%2520the%250Ainstruction-tuned%2520model%2520demonstrates%2520competitive%2520performance%2520among%2520open-source%250ALMMs%2520with%2520similar%2520model%2520sizes.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520safety-tuned%2520model%250Awith%2520DPO%252C%2520aiming%2520to%2520mitigate%2520harmful%2520behaviors%2520such%2520as%2520hallucinations%2520and%250Aimprove%2520safety.%2520We%2520open-source%2520our%2520models%252C%2520curated%2520large-scale%2520datasets%252C%2520and%250Aour%2520fine-tuning%2520codebase%2520to%2520facilitate%2520further%2520advancements%2520in%2520LMM%2520research.%250AAssociated%2520resources%2520will%2520be%2520available%2520on%2520our%2520project%2520page%2520above.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xGen-MM%20%28BLIP-3%29%3A%20A%20Family%20of%20Open%20Large%20Multimodal%20Models&entry.906535625=Le%20Xue%20and%20Manli%20Shu%20and%20Anas%20Awadalla%20and%20Jun%20Wang%20and%20An%20Yan%20and%20Senthil%20Purushwalkam%20and%20Honglu%20Zhou%20and%20Viraj%20Prabhu%20and%20Yutong%20Dai%20and%20Michael%20S%20Ryoo%20and%20Shrikant%20Kendre%20and%20Jieyu%20Zhang%20and%20Can%20Qin%20and%20Shu%20Zhang%20and%20Chia-Chih%20Chen%20and%20Ning%20Yu%20and%20Juntao%20Tan%20and%20Tulika%20Manoj%20Awalgaonkar%20and%20Shelby%20Heinecke%20and%20Huan%20Wang%20and%20Yejin%20Choi%20and%20Ludwig%20Schmidt%20and%20Zeyuan%20Chen%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Caiming%20Xiong%20and%20Ran%20Xu&entry.1292438233=%20%20This%20report%20introduces%20xGen-MM%20%28also%20known%20as%20BLIP-3%29%2C%20a%20framework%20for%0Adeveloping%20Large%20Multimodal%20Models%20%28LMMs%29.%20The%20framework%20comprises%20meticulously%0Acurated%20datasets%2C%20a%20training%20recipe%2C%20model%20architectures%2C%20and%20a%20resulting%20suite%0Aof%20LMMs.%20xGen-MM%2C%20short%20for%20xGen-MultiModal%2C%20expands%20the%20Salesforce%20xGen%0Ainitiative%20on%20foundation%20AI%20models.%20Our%20models%20undergo%20rigorous%20evaluation%0Aacross%20a%20range%20of%20tasks%2C%20including%20both%20single%20and%20multi-image%20benchmarks.%20Our%0Apre-trained%20base%20model%20exhibits%20strong%20in-context%20learning%20capabilities%20and%20the%0Ainstruction-tuned%20model%20demonstrates%20competitive%20performance%20among%20open-source%0ALMMs%20with%20similar%20model%20sizes.%20In%20addition%2C%20we%20introduce%20a%20safety-tuned%20model%0Awith%20DPO%2C%20aiming%20to%20mitigate%20harmful%20behaviors%20such%20as%20hallucinations%20and%0Aimprove%20safety.%20We%20open-source%20our%20models%2C%20curated%20large-scale%20datasets%2C%20and%0Aour%20fine-tuning%20codebase%20to%20facilitate%20further%20advancements%20in%20LMM%20research.%0AAssociated%20resources%20will%20be%20available%20on%20our%20project%20page%20above.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08872v1&entry.124074799=Read"},
{"title": "Backward-Compatible Aligned Representations via an Orthogonal\n  Transformation Layer", "author": "Simone Ricci and Niccol\u00f2 Biondi and Federico Pernici and Alberto Del Bimbo", "abstract": "  Visual retrieval systems face significant challenges when updating models\nwith improved representations due to misalignment between the old and new\nrepresentations. The costly and resource-intensive backfilling process involves\nrecalculating feature vectors for images in the gallery set whenever a new\nmodel is introduced. To address this, prior research has explored\nbackward-compatible training methods that enable direct comparisons between new\nand old representations without backfilling. Despite these advancements,\nachieving a balance between backward compatibility and the performance of\nindependently trained models remains an open problem. In this paper, we address\nit by expanding the representation space with additional dimensions and\nlearning an orthogonal transformation to achieve compatibility with old models\nand, at the same time, integrate new information. This transformation preserves\nthe original feature space's geometry, ensuring that our model aligns with\nprevious versions while also learning new data. Our Orthogonal Compatible\nAligned (OCA) approach eliminates the need for re-indexing during model updates\nand ensures that features can be compared directly across different model\nupdates without additional mapping functions. Experimental results on CIFAR-100\nand ImageNet-1k demonstrate that our method not only maintains compatibility\nwith previous models but also achieves state-of-the-art accuracy, outperforming\nseveral existing methods.\n", "link": "http://arxiv.org/abs/2408.08793v1", "date": "2024-08-16", "relevancy": 2.6699, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backward-Compatible%20Aligned%20Representations%20via%20an%20Orthogonal%0A%20%20Transformation%20Layer&body=Title%3A%20Backward-Compatible%20Aligned%20Representations%20via%20an%20Orthogonal%0A%20%20Transformation%20Layer%0AAuthor%3A%20Simone%20Ricci%20and%20Niccol%C3%B2%20Biondi%20and%20Federico%20Pernici%20and%20Alberto%20Del%20Bimbo%0AAbstract%3A%20%20%20Visual%20retrieval%20systems%20face%20significant%20challenges%20when%20updating%20models%0Awith%20improved%20representations%20due%20to%20misalignment%20between%20the%20old%20and%20new%0Arepresentations.%20The%20costly%20and%20resource-intensive%20backfilling%20process%20involves%0Arecalculating%20feature%20vectors%20for%20images%20in%20the%20gallery%20set%20whenever%20a%20new%0Amodel%20is%20introduced.%20To%20address%20this%2C%20prior%20research%20has%20explored%0Abackward-compatible%20training%20methods%20that%20enable%20direct%20comparisons%20between%20new%0Aand%20old%20representations%20without%20backfilling.%20Despite%20these%20advancements%2C%0Aachieving%20a%20balance%20between%20backward%20compatibility%20and%20the%20performance%20of%0Aindependently%20trained%20models%20remains%20an%20open%20problem.%20In%20this%20paper%2C%20we%20address%0Ait%20by%20expanding%20the%20representation%20space%20with%20additional%20dimensions%20and%0Alearning%20an%20orthogonal%20transformation%20to%20achieve%20compatibility%20with%20old%20models%0Aand%2C%20at%20the%20same%20time%2C%20integrate%20new%20information.%20This%20transformation%20preserves%0Athe%20original%20feature%20space%27s%20geometry%2C%20ensuring%20that%20our%20model%20aligns%20with%0Aprevious%20versions%20while%20also%20learning%20new%20data.%20Our%20Orthogonal%20Compatible%0AAligned%20%28OCA%29%20approach%20eliminates%20the%20need%20for%20re-indexing%20during%20model%20updates%0Aand%20ensures%20that%20features%20can%20be%20compared%20directly%20across%20different%20model%0Aupdates%20without%20additional%20mapping%20functions.%20Experimental%20results%20on%20CIFAR-100%0Aand%20ImageNet-1k%20demonstrate%20that%20our%20method%20not%20only%20maintains%20compatibility%0Awith%20previous%20models%20but%20also%20achieves%20state-of-the-art%20accuracy%2C%20outperforming%0Aseveral%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackward-Compatible%2520Aligned%2520Representations%2520via%2520an%2520Orthogonal%250A%2520%2520Transformation%2520Layer%26entry.906535625%3DSimone%2520Ricci%2520and%2520Niccol%25C3%25B2%2520Biondi%2520and%2520Federico%2520Pernici%2520and%2520Alberto%2520Del%2520Bimbo%26entry.1292438233%3D%2520%2520Visual%2520retrieval%2520systems%2520face%2520significant%2520challenges%2520when%2520updating%2520models%250Awith%2520improved%2520representations%2520due%2520to%2520misalignment%2520between%2520the%2520old%2520and%2520new%250Arepresentations.%2520The%2520costly%2520and%2520resource-intensive%2520backfilling%2520process%2520involves%250Arecalculating%2520feature%2520vectors%2520for%2520images%2520in%2520the%2520gallery%2520set%2520whenever%2520a%2520new%250Amodel%2520is%2520introduced.%2520To%2520address%2520this%252C%2520prior%2520research%2520has%2520explored%250Abackward-compatible%2520training%2520methods%2520that%2520enable%2520direct%2520comparisons%2520between%2520new%250Aand%2520old%2520representations%2520without%2520backfilling.%2520Despite%2520these%2520advancements%252C%250Aachieving%2520a%2520balance%2520between%2520backward%2520compatibility%2520and%2520the%2520performance%2520of%250Aindependently%2520trained%2520models%2520remains%2520an%2520open%2520problem.%2520In%2520this%2520paper%252C%2520we%2520address%250Ait%2520by%2520expanding%2520the%2520representation%2520space%2520with%2520additional%2520dimensions%2520and%250Alearning%2520an%2520orthogonal%2520transformation%2520to%2520achieve%2520compatibility%2520with%2520old%2520models%250Aand%252C%2520at%2520the%2520same%2520time%252C%2520integrate%2520new%2520information.%2520This%2520transformation%2520preserves%250Athe%2520original%2520feature%2520space%2527s%2520geometry%252C%2520ensuring%2520that%2520our%2520model%2520aligns%2520with%250Aprevious%2520versions%2520while%2520also%2520learning%2520new%2520data.%2520Our%2520Orthogonal%2520Compatible%250AAligned%2520%2528OCA%2529%2520approach%2520eliminates%2520the%2520need%2520for%2520re-indexing%2520during%2520model%2520updates%250Aand%2520ensures%2520that%2520features%2520can%2520be%2520compared%2520directly%2520across%2520different%2520model%250Aupdates%2520without%2520additional%2520mapping%2520functions.%2520Experimental%2520results%2520on%2520CIFAR-100%250Aand%2520ImageNet-1k%2520demonstrate%2520that%2520our%2520method%2520not%2520only%2520maintains%2520compatibility%250Awith%2520previous%2520models%2520but%2520also%2520achieves%2520state-of-the-art%2520accuracy%252C%2520outperforming%250Aseveral%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backward-Compatible%20Aligned%20Representations%20via%20an%20Orthogonal%0A%20%20Transformation%20Layer&entry.906535625=Simone%20Ricci%20and%20Niccol%C3%B2%20Biondi%20and%20Federico%20Pernici%20and%20Alberto%20Del%20Bimbo&entry.1292438233=%20%20Visual%20retrieval%20systems%20face%20significant%20challenges%20when%20updating%20models%0Awith%20improved%20representations%20due%20to%20misalignment%20between%20the%20old%20and%20new%0Arepresentations.%20The%20costly%20and%20resource-intensive%20backfilling%20process%20involves%0Arecalculating%20feature%20vectors%20for%20images%20in%20the%20gallery%20set%20whenever%20a%20new%0Amodel%20is%20introduced.%20To%20address%20this%2C%20prior%20research%20has%20explored%0Abackward-compatible%20training%20methods%20that%20enable%20direct%20comparisons%20between%20new%0Aand%20old%20representations%20without%20backfilling.%20Despite%20these%20advancements%2C%0Aachieving%20a%20balance%20between%20backward%20compatibility%20and%20the%20performance%20of%0Aindependently%20trained%20models%20remains%20an%20open%20problem.%20In%20this%20paper%2C%20we%20address%0Ait%20by%20expanding%20the%20representation%20space%20with%20additional%20dimensions%20and%0Alearning%20an%20orthogonal%20transformation%20to%20achieve%20compatibility%20with%20old%20models%0Aand%2C%20at%20the%20same%20time%2C%20integrate%20new%20information.%20This%20transformation%20preserves%0Athe%20original%20feature%20space%27s%20geometry%2C%20ensuring%20that%20our%20model%20aligns%20with%0Aprevious%20versions%20while%20also%20learning%20new%20data.%20Our%20Orthogonal%20Compatible%0AAligned%20%28OCA%29%20approach%20eliminates%20the%20need%20for%20re-indexing%20during%20model%20updates%0Aand%20ensures%20that%20features%20can%20be%20compared%20directly%20across%20different%20model%0Aupdates%20without%20additional%20mapping%20functions.%20Experimental%20results%20on%20CIFAR-100%0Aand%20ImageNet-1k%20demonstrate%20that%20our%20method%20not%20only%20maintains%20compatibility%0Awith%20previous%20models%20but%20also%20achieves%20state-of-the-art%20accuracy%2C%20outperforming%0Aseveral%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08793v1&entry.124074799=Read"},
{"title": "VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction", "author": "Albert Gassol Puigjaner and Edoardo Mello Rella and Erik Sandstr\u00f6m and Ajad Chhatkuli and Luc Van Gool", "abstract": "  Implicit surfaces via neural radiance fields (NeRF) have shown surprising\naccuracy in surface reconstruction. Despite their success in reconstructing\nrichly textured surfaces, existing methods struggle with planar regions with\nweak textures, which account for the majority of indoor scenes. In this paper,\nwe address indoor dense surface reconstruction by revisiting key aspects of\nNeRF in order to use the recently proposed Vector Field (VF) as the implicit\nrepresentation. VF is defined by the unit vector directed to the nearest\nsurface point. It therefore flips direction at the surface and equals to the\nexplicit surface normals. Except for this flip, VF remains constant along\nplanar surfaces and provides a strong inductive bias in representing planar\nsurfaces. Concretely, we develop a novel density-VF relationship and a training\nscheme that allows us to learn VF via volume rendering By doing this, VF-NeRF\ncan model large planar surfaces and sharp corners accurately. We show that,\nwhen depth cues are available, our method further improves and achieves\nstate-of-the-art results in reconstructing indoor scenes and rendering novel\nviews. We extensively evaluate VF-NeRF on indoor datasets and run ablations of\nits components.\n", "link": "http://arxiv.org/abs/2408.08766v1", "date": "2024-08-16", "relevancy": 2.6453, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5321}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5292}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VF-NeRF%3A%20Learning%20Neural%20Vector%20Fields%20for%20Indoor%20Scene%20Reconstruction&body=Title%3A%20VF-NeRF%3A%20Learning%20Neural%20Vector%20Fields%20for%20Indoor%20Scene%20Reconstruction%0AAuthor%3A%20Albert%20Gassol%20Puigjaner%20and%20Edoardo%20Mello%20Rella%20and%20Erik%20Sandstr%C3%B6m%20and%20Ajad%20Chhatkuli%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Implicit%20surfaces%20via%20neural%20radiance%20fields%20%28NeRF%29%20have%20shown%20surprising%0Aaccuracy%20in%20surface%20reconstruction.%20Despite%20their%20success%20in%20reconstructing%0Arichly%20textured%20surfaces%2C%20existing%20methods%20struggle%20with%20planar%20regions%20with%0Aweak%20textures%2C%20which%20account%20for%20the%20majority%20of%20indoor%20scenes.%20In%20this%20paper%2C%0Awe%20address%20indoor%20dense%20surface%20reconstruction%20by%20revisiting%20key%20aspects%20of%0ANeRF%20in%20order%20to%20use%20the%20recently%20proposed%20Vector%20Field%20%28VF%29%20as%20the%20implicit%0Arepresentation.%20VF%20is%20defined%20by%20the%20unit%20vector%20directed%20to%20the%20nearest%0Asurface%20point.%20It%20therefore%20flips%20direction%20at%20the%20surface%20and%20equals%20to%20the%0Aexplicit%20surface%20normals.%20Except%20for%20this%20flip%2C%20VF%20remains%20constant%20along%0Aplanar%20surfaces%20and%20provides%20a%20strong%20inductive%20bias%20in%20representing%20planar%0Asurfaces.%20Concretely%2C%20we%20develop%20a%20novel%20density-VF%20relationship%20and%20a%20training%0Ascheme%20that%20allows%20us%20to%20learn%20VF%20via%20volume%20rendering%20By%20doing%20this%2C%20VF-NeRF%0Acan%20model%20large%20planar%20surfaces%20and%20sharp%20corners%20accurately.%20We%20show%20that%2C%0Awhen%20depth%20cues%20are%20available%2C%20our%20method%20further%20improves%20and%20achieves%0Astate-of-the-art%20results%20in%20reconstructing%20indoor%20scenes%20and%20rendering%20novel%0Aviews.%20We%20extensively%20evaluate%20VF-NeRF%20on%20indoor%20datasets%20and%20run%20ablations%20of%0Aits%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVF-NeRF%253A%2520Learning%2520Neural%2520Vector%2520Fields%2520for%2520Indoor%2520Scene%2520Reconstruction%26entry.906535625%3DAlbert%2520Gassol%2520Puigjaner%2520and%2520Edoardo%2520Mello%2520Rella%2520and%2520Erik%2520Sandstr%25C3%25B6m%2520and%2520Ajad%2520Chhatkuli%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Implicit%2520surfaces%2520via%2520neural%2520radiance%2520fields%2520%2528NeRF%2529%2520have%2520shown%2520surprising%250Aaccuracy%2520in%2520surface%2520reconstruction.%2520Despite%2520their%2520success%2520in%2520reconstructing%250Arichly%2520textured%2520surfaces%252C%2520existing%2520methods%2520struggle%2520with%2520planar%2520regions%2520with%250Aweak%2520textures%252C%2520which%2520account%2520for%2520the%2520majority%2520of%2520indoor%2520scenes.%2520In%2520this%2520paper%252C%250Awe%2520address%2520indoor%2520dense%2520surface%2520reconstruction%2520by%2520revisiting%2520key%2520aspects%2520of%250ANeRF%2520in%2520order%2520to%2520use%2520the%2520recently%2520proposed%2520Vector%2520Field%2520%2528VF%2529%2520as%2520the%2520implicit%250Arepresentation.%2520VF%2520is%2520defined%2520by%2520the%2520unit%2520vector%2520directed%2520to%2520the%2520nearest%250Asurface%2520point.%2520It%2520therefore%2520flips%2520direction%2520at%2520the%2520surface%2520and%2520equals%2520to%2520the%250Aexplicit%2520surface%2520normals.%2520Except%2520for%2520this%2520flip%252C%2520VF%2520remains%2520constant%2520along%250Aplanar%2520surfaces%2520and%2520provides%2520a%2520strong%2520inductive%2520bias%2520in%2520representing%2520planar%250Asurfaces.%2520Concretely%252C%2520we%2520develop%2520a%2520novel%2520density-VF%2520relationship%2520and%2520a%2520training%250Ascheme%2520that%2520allows%2520us%2520to%2520learn%2520VF%2520via%2520volume%2520rendering%2520By%2520doing%2520this%252C%2520VF-NeRF%250Acan%2520model%2520large%2520planar%2520surfaces%2520and%2520sharp%2520corners%2520accurately.%2520We%2520show%2520that%252C%250Awhen%2520depth%2520cues%2520are%2520available%252C%2520our%2520method%2520further%2520improves%2520and%2520achieves%250Astate-of-the-art%2520results%2520in%2520reconstructing%2520indoor%2520scenes%2520and%2520rendering%2520novel%250Aviews.%2520We%2520extensively%2520evaluate%2520VF-NeRF%2520on%2520indoor%2520datasets%2520and%2520run%2520ablations%2520of%250Aits%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VF-NeRF%3A%20Learning%20Neural%20Vector%20Fields%20for%20Indoor%20Scene%20Reconstruction&entry.906535625=Albert%20Gassol%20Puigjaner%20and%20Edoardo%20Mello%20Rella%20and%20Erik%20Sandstr%C3%B6m%20and%20Ajad%20Chhatkuli%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Implicit%20surfaces%20via%20neural%20radiance%20fields%20%28NeRF%29%20have%20shown%20surprising%0Aaccuracy%20in%20surface%20reconstruction.%20Despite%20their%20success%20in%20reconstructing%0Arichly%20textured%20surfaces%2C%20existing%20methods%20struggle%20with%20planar%20regions%20with%0Aweak%20textures%2C%20which%20account%20for%20the%20majority%20of%20indoor%20scenes.%20In%20this%20paper%2C%0Awe%20address%20indoor%20dense%20surface%20reconstruction%20by%20revisiting%20key%20aspects%20of%0ANeRF%20in%20order%20to%20use%20the%20recently%20proposed%20Vector%20Field%20%28VF%29%20as%20the%20implicit%0Arepresentation.%20VF%20is%20defined%20by%20the%20unit%20vector%20directed%20to%20the%20nearest%0Asurface%20point.%20It%20therefore%20flips%20direction%20at%20the%20surface%20and%20equals%20to%20the%0Aexplicit%20surface%20normals.%20Except%20for%20this%20flip%2C%20VF%20remains%20constant%20along%0Aplanar%20surfaces%20and%20provides%20a%20strong%20inductive%20bias%20in%20representing%20planar%0Asurfaces.%20Concretely%2C%20we%20develop%20a%20novel%20density-VF%20relationship%20and%20a%20training%0Ascheme%20that%20allows%20us%20to%20learn%20VF%20via%20volume%20rendering%20By%20doing%20this%2C%20VF-NeRF%0Acan%20model%20large%20planar%20surfaces%20and%20sharp%20corners%20accurately.%20We%20show%20that%2C%0Awhen%20depth%20cues%20are%20available%2C%20our%20method%20further%20improves%20and%20achieves%0Astate-of-the-art%20results%20in%20reconstructing%20indoor%20scenes%20and%20rendering%20novel%0Aviews.%20We%20extensively%20evaluate%20VF-NeRF%20on%20indoor%20datasets%20and%20run%20ablations%20of%0Aits%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08766v1&entry.124074799=Read"},
{"title": "Motion-compensated MR CINE reconstruction with reconstruction-driven\n  motion estimation", "author": "Jiazhen Pan and Wenqi Huang and Daniel Rueckert and Thomas K\u00fcstner and Kerstin Hammernik", "abstract": "  In cardiac CINE, motion-compensated MR reconstruction (MCMR) is an effective\napproach to address highly undersampled acquisitions by incorporating motion\ninformation between frames. In this work, we propose a novel perspective for\naddressing the MCMR problem and a more integrated and efficient solution to the\nMCMR field. Contrary to state-of-the-art (SOTA) MCMR methods which break the\noriginal problem into two sub-optimization problems, i.e. motion estimation and\nreconstruction, we formulate this problem as a single entity with one single\noptimization. Our approach is unique in that the motion estimation is directly\ndriven by the ultimate goal, reconstruction, but not by the canonical\nmotion-warping loss (similarity measurement between motion-warped images and\ntarget images). We align the objectives of motion estimation and\nreconstruction, eliminating the drawbacks of artifacts-affected motion\nestimation and therefore error-propagated reconstruction. Further, we can\ndeliver high-quality reconstruction and realistic motion without applying any\nregularization/smoothness loss terms, circumventing the non-trivial weighting\nfactor tuning. We evaluate our method on two datasets: 1) an in-house acquired\n2D CINE dataset for the retrospective study and 2) the public OCMR cardiac\ndataset for the prospective study. The conducted experiments indicate that the\nproposed MCMR framework can deliver artifact-free motion estimation and\nhigh-quality MR images even for imaging accelerations up to 20x, outperforming\nSOTA non-MCMR and MCMR methods in both qualitative and quantitative evaluation\nacross all experiments. The code is available at\nhttps://github.com/JZPeterPan/MCMR-Recon-Driven-Motion.\n", "link": "http://arxiv.org/abs/2302.02504v2", "date": "2024-08-16", "relevancy": 2.6193, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5295}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5295}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-compensated%20MR%20CINE%20reconstruction%20with%20reconstruction-driven%0A%20%20motion%20estimation&body=Title%3A%20Motion-compensated%20MR%20CINE%20reconstruction%20with%20reconstruction-driven%0A%20%20motion%20estimation%0AAuthor%3A%20Jiazhen%20Pan%20and%20Wenqi%20Huang%20and%20Daniel%20Rueckert%20and%20Thomas%20K%C3%BCstner%20and%20Kerstin%20Hammernik%0AAbstract%3A%20%20%20In%20cardiac%20CINE%2C%20motion-compensated%20MR%20reconstruction%20%28MCMR%29%20is%20an%20effective%0Aapproach%20to%20address%20highly%20undersampled%20acquisitions%20by%20incorporating%20motion%0Ainformation%20between%20frames.%20In%20this%20work%2C%20we%20propose%20a%20novel%20perspective%20for%0Aaddressing%20the%20MCMR%20problem%20and%20a%20more%20integrated%20and%20efficient%20solution%20to%20the%0AMCMR%20field.%20Contrary%20to%20state-of-the-art%20%28SOTA%29%20MCMR%20methods%20which%20break%20the%0Aoriginal%20problem%20into%20two%20sub-optimization%20problems%2C%20i.e.%20motion%20estimation%20and%0Areconstruction%2C%20we%20formulate%20this%20problem%20as%20a%20single%20entity%20with%20one%20single%0Aoptimization.%20Our%20approach%20is%20unique%20in%20that%20the%20motion%20estimation%20is%20directly%0Adriven%20by%20the%20ultimate%20goal%2C%20reconstruction%2C%20but%20not%20by%20the%20canonical%0Amotion-warping%20loss%20%28similarity%20measurement%20between%20motion-warped%20images%20and%0Atarget%20images%29.%20We%20align%20the%20objectives%20of%20motion%20estimation%20and%0Areconstruction%2C%20eliminating%20the%20drawbacks%20of%20artifacts-affected%20motion%0Aestimation%20and%20therefore%20error-propagated%20reconstruction.%20Further%2C%20we%20can%0Adeliver%20high-quality%20reconstruction%20and%20realistic%20motion%20without%20applying%20any%0Aregularization/smoothness%20loss%20terms%2C%20circumventing%20the%20non-trivial%20weighting%0Afactor%20tuning.%20We%20evaluate%20our%20method%20on%20two%20datasets%3A%201%29%20an%20in-house%20acquired%0A2D%20CINE%20dataset%20for%20the%20retrospective%20study%20and%202%29%20the%20public%20OCMR%20cardiac%0Adataset%20for%20the%20prospective%20study.%20The%20conducted%20experiments%20indicate%20that%20the%0Aproposed%20MCMR%20framework%20can%20deliver%20artifact-free%20motion%20estimation%20and%0Ahigh-quality%20MR%20images%20even%20for%20imaging%20accelerations%20up%20to%2020x%2C%20outperforming%0ASOTA%20non-MCMR%20and%20MCMR%20methods%20in%20both%20qualitative%20and%20quantitative%20evaluation%0Aacross%20all%20experiments.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/JZPeterPan/MCMR-Recon-Driven-Motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.02504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-compensated%2520MR%2520CINE%2520reconstruction%2520with%2520reconstruction-driven%250A%2520%2520motion%2520estimation%26entry.906535625%3DJiazhen%2520Pan%2520and%2520Wenqi%2520Huang%2520and%2520Daniel%2520Rueckert%2520and%2520Thomas%2520K%25C3%25BCstner%2520and%2520Kerstin%2520Hammernik%26entry.1292438233%3D%2520%2520In%2520cardiac%2520CINE%252C%2520motion-compensated%2520MR%2520reconstruction%2520%2528MCMR%2529%2520is%2520an%2520effective%250Aapproach%2520to%2520address%2520highly%2520undersampled%2520acquisitions%2520by%2520incorporating%2520motion%250Ainformation%2520between%2520frames.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520perspective%2520for%250Aaddressing%2520the%2520MCMR%2520problem%2520and%2520a%2520more%2520integrated%2520and%2520efficient%2520solution%2520to%2520the%250AMCMR%2520field.%2520Contrary%2520to%2520state-of-the-art%2520%2528SOTA%2529%2520MCMR%2520methods%2520which%2520break%2520the%250Aoriginal%2520problem%2520into%2520two%2520sub-optimization%2520problems%252C%2520i.e.%2520motion%2520estimation%2520and%250Areconstruction%252C%2520we%2520formulate%2520this%2520problem%2520as%2520a%2520single%2520entity%2520with%2520one%2520single%250Aoptimization.%2520Our%2520approach%2520is%2520unique%2520in%2520that%2520the%2520motion%2520estimation%2520is%2520directly%250Adriven%2520by%2520the%2520ultimate%2520goal%252C%2520reconstruction%252C%2520but%2520not%2520by%2520the%2520canonical%250Amotion-warping%2520loss%2520%2528similarity%2520measurement%2520between%2520motion-warped%2520images%2520and%250Atarget%2520images%2529.%2520We%2520align%2520the%2520objectives%2520of%2520motion%2520estimation%2520and%250Areconstruction%252C%2520eliminating%2520the%2520drawbacks%2520of%2520artifacts-affected%2520motion%250Aestimation%2520and%2520therefore%2520error-propagated%2520reconstruction.%2520Further%252C%2520we%2520can%250Adeliver%2520high-quality%2520reconstruction%2520and%2520realistic%2520motion%2520without%2520applying%2520any%250Aregularization/smoothness%2520loss%2520terms%252C%2520circumventing%2520the%2520non-trivial%2520weighting%250Afactor%2520tuning.%2520We%2520evaluate%2520our%2520method%2520on%2520two%2520datasets%253A%25201%2529%2520an%2520in-house%2520acquired%250A2D%2520CINE%2520dataset%2520for%2520the%2520retrospective%2520study%2520and%25202%2529%2520the%2520public%2520OCMR%2520cardiac%250Adataset%2520for%2520the%2520prospective%2520study.%2520The%2520conducted%2520experiments%2520indicate%2520that%2520the%250Aproposed%2520MCMR%2520framework%2520can%2520deliver%2520artifact-free%2520motion%2520estimation%2520and%250Ahigh-quality%2520MR%2520images%2520even%2520for%2520imaging%2520accelerations%2520up%2520to%252020x%252C%2520outperforming%250ASOTA%2520non-MCMR%2520and%2520MCMR%2520methods%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluation%250Aacross%2520all%2520experiments.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JZPeterPan/MCMR-Recon-Driven-Motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.02504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-compensated%20MR%20CINE%20reconstruction%20with%20reconstruction-driven%0A%20%20motion%20estimation&entry.906535625=Jiazhen%20Pan%20and%20Wenqi%20Huang%20and%20Daniel%20Rueckert%20and%20Thomas%20K%C3%BCstner%20and%20Kerstin%20Hammernik&entry.1292438233=%20%20In%20cardiac%20CINE%2C%20motion-compensated%20MR%20reconstruction%20%28MCMR%29%20is%20an%20effective%0Aapproach%20to%20address%20highly%20undersampled%20acquisitions%20by%20incorporating%20motion%0Ainformation%20between%20frames.%20In%20this%20work%2C%20we%20propose%20a%20novel%20perspective%20for%0Aaddressing%20the%20MCMR%20problem%20and%20a%20more%20integrated%20and%20efficient%20solution%20to%20the%0AMCMR%20field.%20Contrary%20to%20state-of-the-art%20%28SOTA%29%20MCMR%20methods%20which%20break%20the%0Aoriginal%20problem%20into%20two%20sub-optimization%20problems%2C%20i.e.%20motion%20estimation%20and%0Areconstruction%2C%20we%20formulate%20this%20problem%20as%20a%20single%20entity%20with%20one%20single%0Aoptimization.%20Our%20approach%20is%20unique%20in%20that%20the%20motion%20estimation%20is%20directly%0Adriven%20by%20the%20ultimate%20goal%2C%20reconstruction%2C%20but%20not%20by%20the%20canonical%0Amotion-warping%20loss%20%28similarity%20measurement%20between%20motion-warped%20images%20and%0Atarget%20images%29.%20We%20align%20the%20objectives%20of%20motion%20estimation%20and%0Areconstruction%2C%20eliminating%20the%20drawbacks%20of%20artifacts-affected%20motion%0Aestimation%20and%20therefore%20error-propagated%20reconstruction.%20Further%2C%20we%20can%0Adeliver%20high-quality%20reconstruction%20and%20realistic%20motion%20without%20applying%20any%0Aregularization/smoothness%20loss%20terms%2C%20circumventing%20the%20non-trivial%20weighting%0Afactor%20tuning.%20We%20evaluate%20our%20method%20on%20two%20datasets%3A%201%29%20an%20in-house%20acquired%0A2D%20CINE%20dataset%20for%20the%20retrospective%20study%20and%202%29%20the%20public%20OCMR%20cardiac%0Adataset%20for%20the%20prospective%20study.%20The%20conducted%20experiments%20indicate%20that%20the%0Aproposed%20MCMR%20framework%20can%20deliver%20artifact-free%20motion%20estimation%20and%0Ahigh-quality%20MR%20images%20even%20for%20imaging%20accelerations%20up%20to%2020x%2C%20outperforming%0ASOTA%20non-MCMR%20and%20MCMR%20methods%20in%20both%20qualitative%20and%20quantitative%20evaluation%0Aacross%20all%20experiments.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/JZPeterPan/MCMR-Recon-Driven-Motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.02504v2&entry.124074799=Read"},
{"title": "A lifted Bregman strategy for training unfolded proximal neural network\n  Gaussian denoisers", "author": "Xiaoyu Wang and Martin Benning and Audrey Repetti", "abstract": "  Unfolded proximal neural networks (PNNs) form a family of methods that\ncombines deep learning and proximal optimization approaches. They consist in\ndesigning a neural network for a specific task by unrolling a proximal\nalgorithm for a fixed number of iterations, where linearities can be learned\nfrom prior training procedure. PNNs have shown to be more robust than\ntraditional deep learning approaches while reaching at least as good\nperformances, in particular in computational imaging. However, training PNNs\nstill depends on the efficiency of available training algorithms. In this work,\nwe propose a lifted training formulation based on Bregman distances for\nunfolded PNNs. Leveraging the deterministic mini-batch block-coordinate\nforward-backward method, we design a bespoke computational strategy beyond\ntraditional back-propagation methods for solving the resulting learning problem\nefficiently. We assess the behaviour of the proposed training approach for PNNs\nthrough numerical simulations on image denoising, considering a denoising PNN\nwhose structure is based on dual proximal-gradient iterations.\n", "link": "http://arxiv.org/abs/2408.08742v1", "date": "2024-08-16", "relevancy": 2.5955, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5044}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20lifted%20Bregman%20strategy%20for%20training%20unfolded%20proximal%20neural%20network%0A%20%20Gaussian%20denoisers&body=Title%3A%20A%20lifted%20Bregman%20strategy%20for%20training%20unfolded%20proximal%20neural%20network%0A%20%20Gaussian%20denoisers%0AAuthor%3A%20Xiaoyu%20Wang%20and%20Martin%20Benning%20and%20Audrey%20Repetti%0AAbstract%3A%20%20%20Unfolded%20proximal%20neural%20networks%20%28PNNs%29%20form%20a%20family%20of%20methods%20that%0Acombines%20deep%20learning%20and%20proximal%20optimization%20approaches.%20They%20consist%20in%0Adesigning%20a%20neural%20network%20for%20a%20specific%20task%20by%20unrolling%20a%20proximal%0Aalgorithm%20for%20a%20fixed%20number%20of%20iterations%2C%20where%20linearities%20can%20be%20learned%0Afrom%20prior%20training%20procedure.%20PNNs%20have%20shown%20to%20be%20more%20robust%20than%0Atraditional%20deep%20learning%20approaches%20while%20reaching%20at%20least%20as%20good%0Aperformances%2C%20in%20particular%20in%20computational%20imaging.%20However%2C%20training%20PNNs%0Astill%20depends%20on%20the%20efficiency%20of%20available%20training%20algorithms.%20In%20this%20work%2C%0Awe%20propose%20a%20lifted%20training%20formulation%20based%20on%20Bregman%20distances%20for%0Aunfolded%20PNNs.%20Leveraging%20the%20deterministic%20mini-batch%20block-coordinate%0Aforward-backward%20method%2C%20we%20design%20a%20bespoke%20computational%20strategy%20beyond%0Atraditional%20back-propagation%20methods%20for%20solving%20the%20resulting%20learning%20problem%0Aefficiently.%20We%20assess%20the%20behaviour%20of%20the%20proposed%20training%20approach%20for%20PNNs%0Athrough%20numerical%20simulations%20on%20image%20denoising%2C%20considering%20a%20denoising%20PNN%0Awhose%20structure%20is%20based%20on%20dual%20proximal-gradient%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520lifted%2520Bregman%2520strategy%2520for%2520training%2520unfolded%2520proximal%2520neural%2520network%250A%2520%2520Gaussian%2520denoisers%26entry.906535625%3DXiaoyu%2520Wang%2520and%2520Martin%2520Benning%2520and%2520Audrey%2520Repetti%26entry.1292438233%3D%2520%2520Unfolded%2520proximal%2520neural%2520networks%2520%2528PNNs%2529%2520form%2520a%2520family%2520of%2520methods%2520that%250Acombines%2520deep%2520learning%2520and%2520proximal%2520optimization%2520approaches.%2520They%2520consist%2520in%250Adesigning%2520a%2520neural%2520network%2520for%2520a%2520specific%2520task%2520by%2520unrolling%2520a%2520proximal%250Aalgorithm%2520for%2520a%2520fixed%2520number%2520of%2520iterations%252C%2520where%2520linearities%2520can%2520be%2520learned%250Afrom%2520prior%2520training%2520procedure.%2520PNNs%2520have%2520shown%2520to%2520be%2520more%2520robust%2520than%250Atraditional%2520deep%2520learning%2520approaches%2520while%2520reaching%2520at%2520least%2520as%2520good%250Aperformances%252C%2520in%2520particular%2520in%2520computational%2520imaging.%2520However%252C%2520training%2520PNNs%250Astill%2520depends%2520on%2520the%2520efficiency%2520of%2520available%2520training%2520algorithms.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520lifted%2520training%2520formulation%2520based%2520on%2520Bregman%2520distances%2520for%250Aunfolded%2520PNNs.%2520Leveraging%2520the%2520deterministic%2520mini-batch%2520block-coordinate%250Aforward-backward%2520method%252C%2520we%2520design%2520a%2520bespoke%2520computational%2520strategy%2520beyond%250Atraditional%2520back-propagation%2520methods%2520for%2520solving%2520the%2520resulting%2520learning%2520problem%250Aefficiently.%2520We%2520assess%2520the%2520behaviour%2520of%2520the%2520proposed%2520training%2520approach%2520for%2520PNNs%250Athrough%2520numerical%2520simulations%2520on%2520image%2520denoising%252C%2520considering%2520a%2520denoising%2520PNN%250Awhose%2520structure%2520is%2520based%2520on%2520dual%2520proximal-gradient%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20lifted%20Bregman%20strategy%20for%20training%20unfolded%20proximal%20neural%20network%0A%20%20Gaussian%20denoisers&entry.906535625=Xiaoyu%20Wang%20and%20Martin%20Benning%20and%20Audrey%20Repetti&entry.1292438233=%20%20Unfolded%20proximal%20neural%20networks%20%28PNNs%29%20form%20a%20family%20of%20methods%20that%0Acombines%20deep%20learning%20and%20proximal%20optimization%20approaches.%20They%20consist%20in%0Adesigning%20a%20neural%20network%20for%20a%20specific%20task%20by%20unrolling%20a%20proximal%0Aalgorithm%20for%20a%20fixed%20number%20of%20iterations%2C%20where%20linearities%20can%20be%20learned%0Afrom%20prior%20training%20procedure.%20PNNs%20have%20shown%20to%20be%20more%20robust%20than%0Atraditional%20deep%20learning%20approaches%20while%20reaching%20at%20least%20as%20good%0Aperformances%2C%20in%20particular%20in%20computational%20imaging.%20However%2C%20training%20PNNs%0Astill%20depends%20on%20the%20efficiency%20of%20available%20training%20algorithms.%20In%20this%20work%2C%0Awe%20propose%20a%20lifted%20training%20formulation%20based%20on%20Bregman%20distances%20for%0Aunfolded%20PNNs.%20Leveraging%20the%20deterministic%20mini-batch%20block-coordinate%0Aforward-backward%20method%2C%20we%20design%20a%20bespoke%20computational%20strategy%20beyond%0Atraditional%20back-propagation%20methods%20for%20solving%20the%20resulting%20learning%20problem%0Aefficiently.%20We%20assess%20the%20behaviour%20of%20the%20proposed%20training%20approach%20for%20PNNs%0Athrough%20numerical%20simulations%20on%20image%20denoising%2C%20considering%20a%20denoising%20PNN%0Awhose%20structure%20is%20based%20on%20dual%20proximal-gradient%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08742v1&entry.124074799=Read"},
{"title": "TextCAVs: Debugging vision models using text", "author": "Angus Nicolson and Yarin Gal and J. Alison Noble", "abstract": "  Concept-based interpretability methods are a popular form of explanation for\ndeep learning models which provide explanations in the form of high-level human\ninterpretable concepts. These methods typically find concept activation vectors\n(CAVs) using a probe dataset of concept examples. This requires labelled data\nfor these concepts -- an expensive task in the medical domain. We introduce\nTextCAVs: a novel method which creates CAVs using vision-language models such\nas CLIP, allowing for explanations to be created solely using text descriptions\nof the concept, as opposed to image exemplars. This reduced cost in testing\nconcepts allows for many concepts to be tested and for users to interact with\nthe model, testing new ideas as they are thought of, rather than a delay caused\nby image collection and annotation. In early experimental results, we\ndemonstrate that TextCAVs produces reasonable explanations for a chest x-ray\ndataset (MIMIC-CXR) and natural images (ImageNet), and that these explanations\ncan be used to debug deep learning-based models.\n", "link": "http://arxiv.org/abs/2408.08652v1", "date": "2024-08-16", "relevancy": 2.5746, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5105}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextCAVs%3A%20Debugging%20vision%20models%20using%20text&body=Title%3A%20TextCAVs%3A%20Debugging%20vision%20models%20using%20text%0AAuthor%3A%20Angus%20Nicolson%20and%20Yarin%20Gal%20and%20J.%20Alison%20Noble%0AAbstract%3A%20%20%20Concept-based%20interpretability%20methods%20are%20a%20popular%20form%20of%20explanation%20for%0Adeep%20learning%20models%20which%20provide%20explanations%20in%20the%20form%20of%20high-level%20human%0Ainterpretable%20concepts.%20These%20methods%20typically%20find%20concept%20activation%20vectors%0A%28CAVs%29%20using%20a%20probe%20dataset%20of%20concept%20examples.%20This%20requires%20labelled%20data%0Afor%20these%20concepts%20--%20an%20expensive%20task%20in%20the%20medical%20domain.%20We%20introduce%0ATextCAVs%3A%20a%20novel%20method%20which%20creates%20CAVs%20using%20vision-language%20models%20such%0Aas%20CLIP%2C%20allowing%20for%20explanations%20to%20be%20created%20solely%20using%20text%20descriptions%0Aof%20the%20concept%2C%20as%20opposed%20to%20image%20exemplars.%20This%20reduced%20cost%20in%20testing%0Aconcepts%20allows%20for%20many%20concepts%20to%20be%20tested%20and%20for%20users%20to%20interact%20with%0Athe%20model%2C%20testing%20new%20ideas%20as%20they%20are%20thought%20of%2C%20rather%20than%20a%20delay%20caused%0Aby%20image%20collection%20and%20annotation.%20In%20early%20experimental%20results%2C%20we%0Ademonstrate%20that%20TextCAVs%20produces%20reasonable%20explanations%20for%20a%20chest%20x-ray%0Adataset%20%28MIMIC-CXR%29%20and%20natural%20images%20%28ImageNet%29%2C%20and%20that%20these%20explanations%0Acan%20be%20used%20to%20debug%20deep%20learning-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextCAVs%253A%2520Debugging%2520vision%2520models%2520using%2520text%26entry.906535625%3DAngus%2520Nicolson%2520and%2520Yarin%2520Gal%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3D%2520%2520Concept-based%2520interpretability%2520methods%2520are%2520a%2520popular%2520form%2520of%2520explanation%2520for%250Adeep%2520learning%2520models%2520which%2520provide%2520explanations%2520in%2520the%2520form%2520of%2520high-level%2520human%250Ainterpretable%2520concepts.%2520These%2520methods%2520typically%2520find%2520concept%2520activation%2520vectors%250A%2528CAVs%2529%2520using%2520a%2520probe%2520dataset%2520of%2520concept%2520examples.%2520This%2520requires%2520labelled%2520data%250Afor%2520these%2520concepts%2520--%2520an%2520expensive%2520task%2520in%2520the%2520medical%2520domain.%2520We%2520introduce%250ATextCAVs%253A%2520a%2520novel%2520method%2520which%2520creates%2520CAVs%2520using%2520vision-language%2520models%2520such%250Aas%2520CLIP%252C%2520allowing%2520for%2520explanations%2520to%2520be%2520created%2520solely%2520using%2520text%2520descriptions%250Aof%2520the%2520concept%252C%2520as%2520opposed%2520to%2520image%2520exemplars.%2520This%2520reduced%2520cost%2520in%2520testing%250Aconcepts%2520allows%2520for%2520many%2520concepts%2520to%2520be%2520tested%2520and%2520for%2520users%2520to%2520interact%2520with%250Athe%2520model%252C%2520testing%2520new%2520ideas%2520as%2520they%2520are%2520thought%2520of%252C%2520rather%2520than%2520a%2520delay%2520caused%250Aby%2520image%2520collection%2520and%2520annotation.%2520In%2520early%2520experimental%2520results%252C%2520we%250Ademonstrate%2520that%2520TextCAVs%2520produces%2520reasonable%2520explanations%2520for%2520a%2520chest%2520x-ray%250Adataset%2520%2528MIMIC-CXR%2529%2520and%2520natural%2520images%2520%2528ImageNet%2529%252C%2520and%2520that%2520these%2520explanations%250Acan%2520be%2520used%2520to%2520debug%2520deep%2520learning-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextCAVs%3A%20Debugging%20vision%20models%20using%20text&entry.906535625=Angus%20Nicolson%20and%20Yarin%20Gal%20and%20J.%20Alison%20Noble&entry.1292438233=%20%20Concept-based%20interpretability%20methods%20are%20a%20popular%20form%20of%20explanation%20for%0Adeep%20learning%20models%20which%20provide%20explanations%20in%20the%20form%20of%20high-level%20human%0Ainterpretable%20concepts.%20These%20methods%20typically%20find%20concept%20activation%20vectors%0A%28CAVs%29%20using%20a%20probe%20dataset%20of%20concept%20examples.%20This%20requires%20labelled%20data%0Afor%20these%20concepts%20--%20an%20expensive%20task%20in%20the%20medical%20domain.%20We%20introduce%0ATextCAVs%3A%20a%20novel%20method%20which%20creates%20CAVs%20using%20vision-language%20models%20such%0Aas%20CLIP%2C%20allowing%20for%20explanations%20to%20be%20created%20solely%20using%20text%20descriptions%0Aof%20the%20concept%2C%20as%20opposed%20to%20image%20exemplars.%20This%20reduced%20cost%20in%20testing%0Aconcepts%20allows%20for%20many%20concepts%20to%20be%20tested%20and%20for%20users%20to%20interact%20with%0Athe%20model%2C%20testing%20new%20ideas%20as%20they%20are%20thought%20of%2C%20rather%20than%20a%20delay%20caused%0Aby%20image%20collection%20and%20annotation.%20In%20early%20experimental%20results%2C%20we%0Ademonstrate%20that%20TextCAVs%20produces%20reasonable%20explanations%20for%20a%20chest%20x-ray%0Adataset%20%28MIMIC-CXR%29%20and%20natural%20images%20%28ImageNet%29%2C%20and%20that%20these%20explanations%0Acan%20be%20used%20to%20debug%20deep%20learning-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08652v1&entry.124074799=Read"},
{"title": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression", "author": "Yuqi Ye and Wei Gao", "abstract": "  The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method.\n", "link": "http://arxiv.org/abs/2408.08682v1", "date": "2024-08-16", "relevancy": 2.5598, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.522}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5184}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-PCGC%3A%20Large%20Language%20Model-based%20Point%20Cloud%20Geometry%20Compression&body=Title%3A%20LLM-PCGC%3A%20Large%20Language%20Model-based%20Point%20Cloud%20Geometry%20Compression%0AAuthor%3A%20Yuqi%20Ye%20and%20Wei%20Gao%0AAbstract%3A%20%20%20The%20key%20to%20effective%20point%20cloud%20compression%20is%20to%20obtain%20a%20robust%20context%0Amodel%20consistent%20with%20complex%203D%20data%20structures.%20Recently%2C%20the%20advancement%20of%0Alarge%20language%20models%20%28LLMs%29%20has%20highlighted%20their%20capabilities%20not%20only%20as%0Apowerful%20generators%20for%20in-context%20learning%20and%20generation%20but%20also%20as%0Aeffective%20compressors.%20These%20dual%20attributes%20of%20LLMs%20make%20them%20particularly%0Awell-suited%20to%20meet%20the%20demands%20of%20data%20compression.%20Therefore%2C%20this%20paper%0Aexplores%20the%20potential%20of%20using%20LLM%20for%20compression%20tasks%2C%20focusing%20on%20lossless%0Apoint%20cloud%20geometry%20compression%20%28PCGC%29%20experiments.%20However%2C%20applying%20LLM%0Adirectly%20to%20PCGC%20tasks%20presents%20some%20significant%20challenges%2C%20i.e.%2C%20LLM%20does%20not%0Aunderstand%20the%20structure%20of%20the%20point%20cloud%20well%2C%20and%20it%20is%20a%20difficult%20task%20to%0Afill%20the%20gap%20between%20text%20and%20point%20cloud%20through%20text%20description%2C%20especially%0Afor%20large%20complicated%20and%20small%20shapeless%20point%20clouds.%20To%20address%20these%0Aproblems%2C%20we%20introduce%20a%20novel%20architecture%2C%20namely%20the%20Large%20Language%0AModel-based%20Point%20Cloud%20Geometry%20Compression%20%28LLM-PCGC%29%20method%2C%20using%20LLM%20to%0Acompress%20point%20cloud%20geometry%20information%20without%20any%20text%20description%20or%0Aaligning%20operation.%20By%20utilizing%20different%20adaptation%20techniques%20for%0Across-modality%20representation%20alignment%20and%20semantic%20consistency%2C%20including%0Aclustering%2C%20K-tree%2C%20token%20mapping%20invariance%2C%20and%20Low%20Rank%20Adaptation%20%28LoRA%29%2C%0Athe%20proposed%20method%20can%20translate%20LLM%20to%20a%20compressor/generator%20for%20point%0Acloud.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20structure%20to%20employ%20LLM%0Aas%20a%20compressor%20for%20point%20cloud%20data.%20Experiments%20demonstrate%20that%20the%20LLM-PCGC%0Aoutperforms%20the%20other%20existing%20methods%20significantly%2C%20by%20achieving%20-40.213%25%20bit%0Arate%20reduction%20compared%20to%20the%20reference%20software%20of%20MPEG%20Geometry-based%20Point%0ACloud%20Compression%20%28G-PCC%29%20standard%2C%20and%20by%20achieving%20-2.267%25%20bit%20rate%20reduction%0Acompared%20to%20the%20state-of-the-art%20learning-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-PCGC%253A%2520Large%2520Language%2520Model-based%2520Point%2520Cloud%2520Geometry%2520Compression%26entry.906535625%3DYuqi%2520Ye%2520and%2520Wei%2520Gao%26entry.1292438233%3D%2520%2520The%2520key%2520to%2520effective%2520point%2520cloud%2520compression%2520is%2520to%2520obtain%2520a%2520robust%2520context%250Amodel%2520consistent%2520with%2520complex%25203D%2520data%2520structures.%2520Recently%252C%2520the%2520advancement%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520has%2520highlighted%2520their%2520capabilities%2520not%2520only%2520as%250Apowerful%2520generators%2520for%2520in-context%2520learning%2520and%2520generation%2520but%2520also%2520as%250Aeffective%2520compressors.%2520These%2520dual%2520attributes%2520of%2520LLMs%2520make%2520them%2520particularly%250Awell-suited%2520to%2520meet%2520the%2520demands%2520of%2520data%2520compression.%2520Therefore%252C%2520this%2520paper%250Aexplores%2520the%2520potential%2520of%2520using%2520LLM%2520for%2520compression%2520tasks%252C%2520focusing%2520on%2520lossless%250Apoint%2520cloud%2520geometry%2520compression%2520%2528PCGC%2529%2520experiments.%2520However%252C%2520applying%2520LLM%250Adirectly%2520to%2520PCGC%2520tasks%2520presents%2520some%2520significant%2520challenges%252C%2520i.e.%252C%2520LLM%2520does%2520not%250Aunderstand%2520the%2520structure%2520of%2520the%2520point%2520cloud%2520well%252C%2520and%2520it%2520is%2520a%2520difficult%2520task%2520to%250Afill%2520the%2520gap%2520between%2520text%2520and%2520point%2520cloud%2520through%2520text%2520description%252C%2520especially%250Afor%2520large%2520complicated%2520and%2520small%2520shapeless%2520point%2520clouds.%2520To%2520address%2520these%250Aproblems%252C%2520we%2520introduce%2520a%2520novel%2520architecture%252C%2520namely%2520the%2520Large%2520Language%250AModel-based%2520Point%2520Cloud%2520Geometry%2520Compression%2520%2528LLM-PCGC%2529%2520method%252C%2520using%2520LLM%2520to%250Acompress%2520point%2520cloud%2520geometry%2520information%2520without%2520any%2520text%2520description%2520or%250Aaligning%2520operation.%2520By%2520utilizing%2520different%2520adaptation%2520techniques%2520for%250Across-modality%2520representation%2520alignment%2520and%2520semantic%2520consistency%252C%2520including%250Aclustering%252C%2520K-tree%252C%2520token%2520mapping%2520invariance%252C%2520and%2520Low%2520Rank%2520Adaptation%2520%2528LoRA%2529%252C%250Athe%2520proposed%2520method%2520can%2520translate%2520LLM%2520to%2520a%2520compressor/generator%2520for%2520point%250Acloud.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520structure%2520to%2520employ%2520LLM%250Aas%2520a%2520compressor%2520for%2520point%2520cloud%2520data.%2520Experiments%2520demonstrate%2520that%2520the%2520LLM-PCGC%250Aoutperforms%2520the%2520other%2520existing%2520methods%2520significantly%252C%2520by%2520achieving%2520-40.213%2525%2520bit%250Arate%2520reduction%2520compared%2520to%2520the%2520reference%2520software%2520of%2520MPEG%2520Geometry-based%2520Point%250ACloud%2520Compression%2520%2528G-PCC%2529%2520standard%252C%2520and%2520by%2520achieving%2520-2.267%2525%2520bit%2520rate%2520reduction%250Acompared%2520to%2520the%2520state-of-the-art%2520learning-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-PCGC%3A%20Large%20Language%20Model-based%20Point%20Cloud%20Geometry%20Compression&entry.906535625=Yuqi%20Ye%20and%20Wei%20Gao&entry.1292438233=%20%20The%20key%20to%20effective%20point%20cloud%20compression%20is%20to%20obtain%20a%20robust%20context%0Amodel%20consistent%20with%20complex%203D%20data%20structures.%20Recently%2C%20the%20advancement%20of%0Alarge%20language%20models%20%28LLMs%29%20has%20highlighted%20their%20capabilities%20not%20only%20as%0Apowerful%20generators%20for%20in-context%20learning%20and%20generation%20but%20also%20as%0Aeffective%20compressors.%20These%20dual%20attributes%20of%20LLMs%20make%20them%20particularly%0Awell-suited%20to%20meet%20the%20demands%20of%20data%20compression.%20Therefore%2C%20this%20paper%0Aexplores%20the%20potential%20of%20using%20LLM%20for%20compression%20tasks%2C%20focusing%20on%20lossless%0Apoint%20cloud%20geometry%20compression%20%28PCGC%29%20experiments.%20However%2C%20applying%20LLM%0Adirectly%20to%20PCGC%20tasks%20presents%20some%20significant%20challenges%2C%20i.e.%2C%20LLM%20does%20not%0Aunderstand%20the%20structure%20of%20the%20point%20cloud%20well%2C%20and%20it%20is%20a%20difficult%20task%20to%0Afill%20the%20gap%20between%20text%20and%20point%20cloud%20through%20text%20description%2C%20especially%0Afor%20large%20complicated%20and%20small%20shapeless%20point%20clouds.%20To%20address%20these%0Aproblems%2C%20we%20introduce%20a%20novel%20architecture%2C%20namely%20the%20Large%20Language%0AModel-based%20Point%20Cloud%20Geometry%20Compression%20%28LLM-PCGC%29%20method%2C%20using%20LLM%20to%0Acompress%20point%20cloud%20geometry%20information%20without%20any%20text%20description%20or%0Aaligning%20operation.%20By%20utilizing%20different%20adaptation%20techniques%20for%0Across-modality%20representation%20alignment%20and%20semantic%20consistency%2C%20including%0Aclustering%2C%20K-tree%2C%20token%20mapping%20invariance%2C%20and%20Low%20Rank%20Adaptation%20%28LoRA%29%2C%0Athe%20proposed%20method%20can%20translate%20LLM%20to%20a%20compressor/generator%20for%20point%0Acloud.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20structure%20to%20employ%20LLM%0Aas%20a%20compressor%20for%20point%20cloud%20data.%20Experiments%20demonstrate%20that%20the%20LLM-PCGC%0Aoutperforms%20the%20other%20existing%20methods%20significantly%2C%20by%20achieving%20-40.213%25%20bit%0Arate%20reduction%20compared%20to%20the%20reference%20software%20of%20MPEG%20Geometry-based%20Point%0ACloud%20Compression%20%28G-PCC%29%20standard%2C%20and%20by%20achieving%20-2.267%25%20bit%20rate%20reduction%0Acompared%20to%20the%20state-of-the-art%20learning-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08682v1&entry.124074799=Read"},
{"title": "Active Learning with Weak Supervision for Gaussian Processes", "author": "Amanda Olmin and Jakob Lindqvist and Lennart Svensson and Fredrik Lindsten", "abstract": "  Annotating data for supervised learning can be costly. When the annotation\nbudget is limited, active learning can be used to select and annotate those\nobservations that are likely to give the most gain in model performance. We\npropose an active learning algorithm that, in addition to selecting which\nobservation to annotate, selects the precision of the annotation that is\nacquired. Assuming that annotations with low precision are cheaper to obtain,\nthis allows the model to explore a larger part of the input space, with the\nsame annotation budget. We build our acquisition function on the previously\nproposed BALD objective for Gaussian Processes, and empirically demonstrate the\ngains of being able to adjust the annotation precision in the active learning\nloop.\n", "link": "http://arxiv.org/abs/2204.08335v3", "date": "2024-08-16", "relevancy": 2.4408, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4937}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.486}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20with%20Weak%20Supervision%20for%20Gaussian%20Processes&body=Title%3A%20Active%20Learning%20with%20Weak%20Supervision%20for%20Gaussian%20Processes%0AAuthor%3A%20Amanda%20Olmin%20and%20Jakob%20Lindqvist%20and%20Lennart%20Svensson%20and%20Fredrik%20Lindsten%0AAbstract%3A%20%20%20Annotating%20data%20for%20supervised%20learning%20can%20be%20costly.%20When%20the%20annotation%0Abudget%20is%20limited%2C%20active%20learning%20can%20be%20used%20to%20select%20and%20annotate%20those%0Aobservations%20that%20are%20likely%20to%20give%20the%20most%20gain%20in%20model%20performance.%20We%0Apropose%20an%20active%20learning%20algorithm%20that%2C%20in%20addition%20to%20selecting%20which%0Aobservation%20to%20annotate%2C%20selects%20the%20precision%20of%20the%20annotation%20that%20is%0Aacquired.%20Assuming%20that%20annotations%20with%20low%20precision%20are%20cheaper%20to%20obtain%2C%0Athis%20allows%20the%20model%20to%20explore%20a%20larger%20part%20of%20the%20input%20space%2C%20with%20the%0Asame%20annotation%20budget.%20We%20build%20our%20acquisition%20function%20on%20the%20previously%0Aproposed%20BALD%20objective%20for%20Gaussian%20Processes%2C%20and%20empirically%20demonstrate%20the%0Agains%20of%20being%20able%20to%20adjust%20the%20annotation%20precision%20in%20the%20active%20learning%0Aloop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.08335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520with%2520Weak%2520Supervision%2520for%2520Gaussian%2520Processes%26entry.906535625%3DAmanda%2520Olmin%2520and%2520Jakob%2520Lindqvist%2520and%2520Lennart%2520Svensson%2520and%2520Fredrik%2520Lindsten%26entry.1292438233%3D%2520%2520Annotating%2520data%2520for%2520supervised%2520learning%2520can%2520be%2520costly.%2520When%2520the%2520annotation%250Abudget%2520is%2520limited%252C%2520active%2520learning%2520can%2520be%2520used%2520to%2520select%2520and%2520annotate%2520those%250Aobservations%2520that%2520are%2520likely%2520to%2520give%2520the%2520most%2520gain%2520in%2520model%2520performance.%2520We%250Apropose%2520an%2520active%2520learning%2520algorithm%2520that%252C%2520in%2520addition%2520to%2520selecting%2520which%250Aobservation%2520to%2520annotate%252C%2520selects%2520the%2520precision%2520of%2520the%2520annotation%2520that%2520is%250Aacquired.%2520Assuming%2520that%2520annotations%2520with%2520low%2520precision%2520are%2520cheaper%2520to%2520obtain%252C%250Athis%2520allows%2520the%2520model%2520to%2520explore%2520a%2520larger%2520part%2520of%2520the%2520input%2520space%252C%2520with%2520the%250Asame%2520annotation%2520budget.%2520We%2520build%2520our%2520acquisition%2520function%2520on%2520the%2520previously%250Aproposed%2520BALD%2520objective%2520for%2520Gaussian%2520Processes%252C%2520and%2520empirically%2520demonstrate%2520the%250Agains%2520of%2520being%2520able%2520to%2520adjust%2520the%2520annotation%2520precision%2520in%2520the%2520active%2520learning%250Aloop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.08335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20with%20Weak%20Supervision%20for%20Gaussian%20Processes&entry.906535625=Amanda%20Olmin%20and%20Jakob%20Lindqvist%20and%20Lennart%20Svensson%20and%20Fredrik%20Lindsten&entry.1292438233=%20%20Annotating%20data%20for%20supervised%20learning%20can%20be%20costly.%20When%20the%20annotation%0Abudget%20is%20limited%2C%20active%20learning%20can%20be%20used%20to%20select%20and%20annotate%20those%0Aobservations%20that%20are%20likely%20to%20give%20the%20most%20gain%20in%20model%20performance.%20We%0Apropose%20an%20active%20learning%20algorithm%20that%2C%20in%20addition%20to%20selecting%20which%0Aobservation%20to%20annotate%2C%20selects%20the%20precision%20of%20the%20annotation%20that%20is%0Aacquired.%20Assuming%20that%20annotations%20with%20low%20precision%20are%20cheaper%20to%20obtain%2C%0Athis%20allows%20the%20model%20to%20explore%20a%20larger%20part%20of%20the%20input%20space%2C%20with%20the%0Asame%20annotation%20budget.%20We%20build%20our%20acquisition%20function%20on%20the%20previously%0Aproposed%20BALD%20objective%20for%20Gaussian%20Processes%2C%20and%20empirically%20demonstrate%20the%0Agains%20of%20being%20able%20to%20adjust%20the%20annotation%20precision%20in%20the%20active%20learning%0Aloop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.08335v3&entry.124074799=Read"},
{"title": "A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree\n  Aggregation, Use BLTs", "author": "H. Brendan McMahan and Zheng Xu and Yanxiang Zhang", "abstract": "  The state-of-the-art for training on-device language models for mobile\nkeyboard applications combines federated learning (FL) with differential\nprivacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two\nvariants of DP-FTRL are used in practice, tree aggregation and matrix\nfactorization. However, tree aggregation suffers from significantly suboptimal\nprivacy/utility tradeoffs, while matrix mechanisms require expensive\noptimization parameterized by hard-to-estimate-in-advance constants, and high\nruntime memory costs.This paper extends the recently introduced Buffered Linear\nToeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRL\nmaintains the ease-of-use advantages of tree aggregation, while essentially\nmatching matrix factorization in terms of utility and privacy. We evaluate\nBLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulation\nbenchmark, and across four on-device language model tasks in a production FL\nsystem. Our empirical results highlight the advantages of the BLT mechanism and\nelevate the practicality and effectiveness of DP in real-world scenarios.\n", "link": "http://arxiv.org/abs/2408.08868v1", "date": "2024-08-16", "relevancy": 2.4081, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4847}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hassle-free%20Algorithm%20for%20Private%20Learning%20in%20Practice%3A%20Don%27t%20Use%20Tree%0A%20%20Aggregation%2C%20Use%20BLTs&body=Title%3A%20A%20Hassle-free%20Algorithm%20for%20Private%20Learning%20in%20Practice%3A%20Don%27t%20Use%20Tree%0A%20%20Aggregation%2C%20Use%20BLTs%0AAuthor%3A%20H.%20Brendan%20McMahan%20and%20Zheng%20Xu%20and%20Yanxiang%20Zhang%0AAbstract%3A%20%20%20The%20state-of-the-art%20for%20training%20on-device%20language%20models%20for%20mobile%0Akeyboard%20applications%20combines%20federated%20learning%20%28FL%29%20with%20differential%0Aprivacy%20%28DP%29%20via%20the%20DP-Follow-the-Regularized-Leader%20%28DP-FTRL%29%20algorithm.%20Two%0Avariants%20of%20DP-FTRL%20are%20used%20in%20practice%2C%20tree%20aggregation%20and%20matrix%0Afactorization.%20However%2C%20tree%20aggregation%20suffers%20from%20significantly%20suboptimal%0Aprivacy/utility%20tradeoffs%2C%20while%20matrix%20mechanisms%20require%20expensive%0Aoptimization%20parameterized%20by%20hard-to-estimate-in-advance%20constants%2C%20and%20high%0Aruntime%20memory%20costs.This%20paper%20extends%20the%20recently%20introduced%20Buffered%20Linear%0AToeplitz%20%28BLT%29%20mechanism%20to%20multi-participation%20scenarios.%20Our%20BLT-DP-FTRL%0Amaintains%20the%20ease-of-use%20advantages%20of%20tree%20aggregation%2C%20while%20essentially%0Amatching%20matrix%20factorization%20in%20terms%20of%20utility%20and%20privacy.%20We%20evaluate%0ABLT-DP-FTRL%20on%20the%20StackOverflow%20dataset%2C%20serving%20as%20a%20re-producible%20simulation%0Abenchmark%2C%20and%20across%20four%20on-device%20language%20model%20tasks%20in%20a%20production%20FL%0Asystem.%20Our%20empirical%20results%20highlight%20the%20advantages%20of%20the%20BLT%20mechanism%20and%0Aelevate%20the%20practicality%20and%20effectiveness%20of%20DP%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hassle-free%2520Algorithm%2520for%2520Private%2520Learning%2520in%2520Practice%253A%2520Don%2527t%2520Use%2520Tree%250A%2520%2520Aggregation%252C%2520Use%2520BLTs%26entry.906535625%3DH.%2520Brendan%2520McMahan%2520and%2520Zheng%2520Xu%2520and%2520Yanxiang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520state-of-the-art%2520for%2520training%2520on-device%2520language%2520models%2520for%2520mobile%250Akeyboard%2520applications%2520combines%2520federated%2520learning%2520%2528FL%2529%2520with%2520differential%250Aprivacy%2520%2528DP%2529%2520via%2520the%2520DP-Follow-the-Regularized-Leader%2520%2528DP-FTRL%2529%2520algorithm.%2520Two%250Avariants%2520of%2520DP-FTRL%2520are%2520used%2520in%2520practice%252C%2520tree%2520aggregation%2520and%2520matrix%250Afactorization.%2520However%252C%2520tree%2520aggregation%2520suffers%2520from%2520significantly%2520suboptimal%250Aprivacy/utility%2520tradeoffs%252C%2520while%2520matrix%2520mechanisms%2520require%2520expensive%250Aoptimization%2520parameterized%2520by%2520hard-to-estimate-in-advance%2520constants%252C%2520and%2520high%250Aruntime%2520memory%2520costs.This%2520paper%2520extends%2520the%2520recently%2520introduced%2520Buffered%2520Linear%250AToeplitz%2520%2528BLT%2529%2520mechanism%2520to%2520multi-participation%2520scenarios.%2520Our%2520BLT-DP-FTRL%250Amaintains%2520the%2520ease-of-use%2520advantages%2520of%2520tree%2520aggregation%252C%2520while%2520essentially%250Amatching%2520matrix%2520factorization%2520in%2520terms%2520of%2520utility%2520and%2520privacy.%2520We%2520evaluate%250ABLT-DP-FTRL%2520on%2520the%2520StackOverflow%2520dataset%252C%2520serving%2520as%2520a%2520re-producible%2520simulation%250Abenchmark%252C%2520and%2520across%2520four%2520on-device%2520language%2520model%2520tasks%2520in%2520a%2520production%2520FL%250Asystem.%2520Our%2520empirical%2520results%2520highlight%2520the%2520advantages%2520of%2520the%2520BLT%2520mechanism%2520and%250Aelevate%2520the%2520practicality%2520and%2520effectiveness%2520of%2520DP%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hassle-free%20Algorithm%20for%20Private%20Learning%20in%20Practice%3A%20Don%27t%20Use%20Tree%0A%20%20Aggregation%2C%20Use%20BLTs&entry.906535625=H.%20Brendan%20McMahan%20and%20Zheng%20Xu%20and%20Yanxiang%20Zhang&entry.1292438233=%20%20The%20state-of-the-art%20for%20training%20on-device%20language%20models%20for%20mobile%0Akeyboard%20applications%20combines%20federated%20learning%20%28FL%29%20with%20differential%0Aprivacy%20%28DP%29%20via%20the%20DP-Follow-the-Regularized-Leader%20%28DP-FTRL%29%20algorithm.%20Two%0Avariants%20of%20DP-FTRL%20are%20used%20in%20practice%2C%20tree%20aggregation%20and%20matrix%0Afactorization.%20However%2C%20tree%20aggregation%20suffers%20from%20significantly%20suboptimal%0Aprivacy/utility%20tradeoffs%2C%20while%20matrix%20mechanisms%20require%20expensive%0Aoptimization%20parameterized%20by%20hard-to-estimate-in-advance%20constants%2C%20and%20high%0Aruntime%20memory%20costs.This%20paper%20extends%20the%20recently%20introduced%20Buffered%20Linear%0AToeplitz%20%28BLT%29%20mechanism%20to%20multi-participation%20scenarios.%20Our%20BLT-DP-FTRL%0Amaintains%20the%20ease-of-use%20advantages%20of%20tree%20aggregation%2C%20while%20essentially%0Amatching%20matrix%20factorization%20in%20terms%20of%20utility%20and%20privacy.%20We%20evaluate%0ABLT-DP-FTRL%20on%20the%20StackOverflow%20dataset%2C%20serving%20as%20a%20re-producible%20simulation%0Abenchmark%2C%20and%20across%20four%20on-device%20language%20model%20tasks%20in%20a%20production%20FL%0Asystem.%20Our%20empirical%20results%20highlight%20the%20advantages%20of%20the%20BLT%20mechanism%20and%0Aelevate%20the%20practicality%20and%20effectiveness%20of%20DP%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08868v1&entry.124074799=Read"},
{"title": "Neighbor Overlay-Induced Graph Attention Network", "author": "Tiqiao Wei and Ye Yuan", "abstract": "  Graph neural networks (GNNs) have garnered significant attention due to their\nability to represent graph data. Among various GNN variants, graph attention\nnetwork (GAT) stands out since it is able to dynamically learn the importance\nof different nodes. However, present GATs heavily rely on the smoothed node\nfeatures to obtain the attention coefficients rather than graph structural\ninformation, which fails to provide crucial contextual cues for node\nrepresentations. To address this issue, this study proposes a neighbor\noverlay-induced graph attention network (NO-GAT) with the following two-fold\nideas: a) learning favorable structural information, i.e., overlaid neighbors,\noutside the node feature propagation process from an adjacency matrix; b)\ninjecting the information of overlaid neighbors into the node feature\npropagation process to compute the attention coefficient jointly. Empirical\nstudies on graph benchmark datasets indicate that the proposed NO-GAT\nconsistently outperforms state-of-the-art models.\n", "link": "http://arxiv.org/abs/2408.08788v1", "date": "2024-08-16", "relevancy": 2.3988, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4965}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neighbor%20Overlay-Induced%20Graph%20Attention%20Network&body=Title%3A%20Neighbor%20Overlay-Induced%20Graph%20Attention%20Network%0AAuthor%3A%20Tiqiao%20Wei%20and%20Ye%20Yuan%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20garnered%20significant%20attention%20due%20to%20their%0Aability%20to%20represent%20graph%20data.%20Among%20various%20GNN%20variants%2C%20graph%20attention%0Anetwork%20%28GAT%29%20stands%20out%20since%20it%20is%20able%20to%20dynamically%20learn%20the%20importance%0Aof%20different%20nodes.%20However%2C%20present%20GATs%20heavily%20rely%20on%20the%20smoothed%20node%0Afeatures%20to%20obtain%20the%20attention%20coefficients%20rather%20than%20graph%20structural%0Ainformation%2C%20which%20fails%20to%20provide%20crucial%20contextual%20cues%20for%20node%0Arepresentations.%20To%20address%20this%20issue%2C%20this%20study%20proposes%20a%20neighbor%0Aoverlay-induced%20graph%20attention%20network%20%28NO-GAT%29%20with%20the%20following%20two-fold%0Aideas%3A%20a%29%20learning%20favorable%20structural%20information%2C%20i.e.%2C%20overlaid%20neighbors%2C%0Aoutside%20the%20node%20feature%20propagation%20process%20from%20an%20adjacency%20matrix%3B%20b%29%0Ainjecting%20the%20information%20of%20overlaid%20neighbors%20into%20the%20node%20feature%0Apropagation%20process%20to%20compute%20the%20attention%20coefficient%20jointly.%20Empirical%0Astudies%20on%20graph%20benchmark%20datasets%20indicate%20that%20the%20proposed%20NO-GAT%0Aconsistently%20outperforms%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeighbor%2520Overlay-Induced%2520Graph%2520Attention%2520Network%26entry.906535625%3DTiqiao%2520Wei%2520and%2520Ye%2520Yuan%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520garnered%2520significant%2520attention%2520due%2520to%2520their%250Aability%2520to%2520represent%2520graph%2520data.%2520Among%2520various%2520GNN%2520variants%252C%2520graph%2520attention%250Anetwork%2520%2528GAT%2529%2520stands%2520out%2520since%2520it%2520is%2520able%2520to%2520dynamically%2520learn%2520the%2520importance%250Aof%2520different%2520nodes.%2520However%252C%2520present%2520GATs%2520heavily%2520rely%2520on%2520the%2520smoothed%2520node%250Afeatures%2520to%2520obtain%2520the%2520attention%2520coefficients%2520rather%2520than%2520graph%2520structural%250Ainformation%252C%2520which%2520fails%2520to%2520provide%2520crucial%2520contextual%2520cues%2520for%2520node%250Arepresentations.%2520To%2520address%2520this%2520issue%252C%2520this%2520study%2520proposes%2520a%2520neighbor%250Aoverlay-induced%2520graph%2520attention%2520network%2520%2528NO-GAT%2529%2520with%2520the%2520following%2520two-fold%250Aideas%253A%2520a%2529%2520learning%2520favorable%2520structural%2520information%252C%2520i.e.%252C%2520overlaid%2520neighbors%252C%250Aoutside%2520the%2520node%2520feature%2520propagation%2520process%2520from%2520an%2520adjacency%2520matrix%253B%2520b%2529%250Ainjecting%2520the%2520information%2520of%2520overlaid%2520neighbors%2520into%2520the%2520node%2520feature%250Apropagation%2520process%2520to%2520compute%2520the%2520attention%2520coefficient%2520jointly.%2520Empirical%250Astudies%2520on%2520graph%2520benchmark%2520datasets%2520indicate%2520that%2520the%2520proposed%2520NO-GAT%250Aconsistently%2520outperforms%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neighbor%20Overlay-Induced%20Graph%20Attention%20Network&entry.906535625=Tiqiao%20Wei%20and%20Ye%20Yuan&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20garnered%20significant%20attention%20due%20to%20their%0Aability%20to%20represent%20graph%20data.%20Among%20various%20GNN%20variants%2C%20graph%20attention%0Anetwork%20%28GAT%29%20stands%20out%20since%20it%20is%20able%20to%20dynamically%20learn%20the%20importance%0Aof%20different%20nodes.%20However%2C%20present%20GATs%20heavily%20rely%20on%20the%20smoothed%20node%0Afeatures%20to%20obtain%20the%20attention%20coefficients%20rather%20than%20graph%20structural%0Ainformation%2C%20which%20fails%20to%20provide%20crucial%20contextual%20cues%20for%20node%0Arepresentations.%20To%20address%20this%20issue%2C%20this%20study%20proposes%20a%20neighbor%0Aoverlay-induced%20graph%20attention%20network%20%28NO-GAT%29%20with%20the%20following%20two-fold%0Aideas%3A%20a%29%20learning%20favorable%20structural%20information%2C%20i.e.%2C%20overlaid%20neighbors%2C%0Aoutside%20the%20node%20feature%20propagation%20process%20from%20an%20adjacency%20matrix%3B%20b%29%0Ainjecting%20the%20information%20of%20overlaid%20neighbors%20into%20the%20node%20feature%0Apropagation%20process%20to%20compute%20the%20attention%20coefficient%20jointly.%20Empirical%0Astudies%20on%20graph%20benchmark%20datasets%20indicate%20that%20the%20proposed%20NO-GAT%0Aconsistently%20outperforms%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08788v1&entry.124074799=Read"},
{"title": "S-BDT: Distributed Differentially Private Boosted Decision Trees", "author": "Thorsten Peinemann and Moritz Kirschte and Joshua Stock and Carlos Cotrini and Esfandiar Mohammadi", "abstract": "  We introduce S-BDT: a novel $(\\varepsilon,\\delta)$-differentially private\ndistributed gradient boosted decision tree (GBDT) learner that improves the\nprotection of single training data points (privacy) while achieving meaningful\nlearning goals, such as accuracy or regression error (utility). S-BDT uses less\nnoise by relying on non-spherical multivariate Gaussian noise, for which we\nshow tight subsampling bounds for privacy amplification and incorporate that\ninto a R\\'enyi filter for individual privacy accounting. We experimentally\nreach the same utility while saving $50\\%$ in terms of epsilon for $\\varepsilon\n\\le 0.5$ on the Abalone regression dataset (dataset size $\\approx 4K$), saving\n$30\\%$ in terms of epsilon for $\\varepsilon \\le 0.08$ for the Adult\nclassification dataset (dataset size $\\approx 50K$), and saving $30\\%$ in terms\nof epsilon for $\\varepsilon\\leq0.03$ for the Spambase classification dataset\n(dataset size $\\approx 5K$). Moreover, we show that for situations where a GBDT\nis learning a stream of data that originates from different subpopulations\n(non-IID), S-BDT improves the saving of epsilon even further.\n", "link": "http://arxiv.org/abs/2309.12041v3", "date": "2024-08-16", "relevancy": 2.3639, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5084}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4671}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S-BDT%3A%20Distributed%20Differentially%20Private%20Boosted%20Decision%20Trees&body=Title%3A%20S-BDT%3A%20Distributed%20Differentially%20Private%20Boosted%20Decision%20Trees%0AAuthor%3A%20Thorsten%20Peinemann%20and%20Moritz%20Kirschte%20and%20Joshua%20Stock%20and%20Carlos%20Cotrini%20and%20Esfandiar%20Mohammadi%0AAbstract%3A%20%20%20We%20introduce%20S-BDT%3A%20a%20novel%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-differentially%20private%0Adistributed%20gradient%20boosted%20decision%20tree%20%28GBDT%29%20learner%20that%20improves%20the%0Aprotection%20of%20single%20training%20data%20points%20%28privacy%29%20while%20achieving%20meaningful%0Alearning%20goals%2C%20such%20as%20accuracy%20or%20regression%20error%20%28utility%29.%20S-BDT%20uses%20less%0Anoise%20by%20relying%20on%20non-spherical%20multivariate%20Gaussian%20noise%2C%20for%20which%20we%0Ashow%20tight%20subsampling%20bounds%20for%20privacy%20amplification%20and%20incorporate%20that%0Ainto%20a%20R%5C%27enyi%20filter%20for%20individual%20privacy%20accounting.%20We%20experimentally%0Areach%20the%20same%20utility%20while%20saving%20%2450%5C%25%24%20in%20terms%20of%20epsilon%20for%20%24%5Cvarepsilon%0A%5Cle%200.5%24%20on%20the%20Abalone%20regression%20dataset%20%28dataset%20size%20%24%5Capprox%204K%24%29%2C%20saving%0A%2430%5C%25%24%20in%20terms%20of%20epsilon%20for%20%24%5Cvarepsilon%20%5Cle%200.08%24%20for%20the%20Adult%0Aclassification%20dataset%20%28dataset%20size%20%24%5Capprox%2050K%24%29%2C%20and%20saving%20%2430%5C%25%24%20in%20terms%0Aof%20epsilon%20for%20%24%5Cvarepsilon%5Cleq0.03%24%20for%20the%20Spambase%20classification%20dataset%0A%28dataset%20size%20%24%5Capprox%205K%24%29.%20Moreover%2C%20we%20show%20that%20for%20situations%20where%20a%20GBDT%0Ais%20learning%20a%20stream%20of%20data%20that%20originates%20from%20different%20subpopulations%0A%28non-IID%29%2C%20S-BDT%20improves%20the%20saving%20of%20epsilon%20even%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12041v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS-BDT%253A%2520Distributed%2520Differentially%2520Private%2520Boosted%2520Decision%2520Trees%26entry.906535625%3DThorsten%2520Peinemann%2520and%2520Moritz%2520Kirschte%2520and%2520Joshua%2520Stock%2520and%2520Carlos%2520Cotrini%2520and%2520Esfandiar%2520Mohammadi%26entry.1292438233%3D%2520%2520We%2520introduce%2520S-BDT%253A%2520a%2520novel%2520%2524%2528%255Cvarepsilon%252C%255Cdelta%2529%2524-differentially%2520private%250Adistributed%2520gradient%2520boosted%2520decision%2520tree%2520%2528GBDT%2529%2520learner%2520that%2520improves%2520the%250Aprotection%2520of%2520single%2520training%2520data%2520points%2520%2528privacy%2529%2520while%2520achieving%2520meaningful%250Alearning%2520goals%252C%2520such%2520as%2520accuracy%2520or%2520regression%2520error%2520%2528utility%2529.%2520S-BDT%2520uses%2520less%250Anoise%2520by%2520relying%2520on%2520non-spherical%2520multivariate%2520Gaussian%2520noise%252C%2520for%2520which%2520we%250Ashow%2520tight%2520subsampling%2520bounds%2520for%2520privacy%2520amplification%2520and%2520incorporate%2520that%250Ainto%2520a%2520R%255C%2527enyi%2520filter%2520for%2520individual%2520privacy%2520accounting.%2520We%2520experimentally%250Areach%2520the%2520same%2520utility%2520while%2520saving%2520%252450%255C%2525%2524%2520in%2520terms%2520of%2520epsilon%2520for%2520%2524%255Cvarepsilon%250A%255Cle%25200.5%2524%2520on%2520the%2520Abalone%2520regression%2520dataset%2520%2528dataset%2520size%2520%2524%255Capprox%25204K%2524%2529%252C%2520saving%250A%252430%255C%2525%2524%2520in%2520terms%2520of%2520epsilon%2520for%2520%2524%255Cvarepsilon%2520%255Cle%25200.08%2524%2520for%2520the%2520Adult%250Aclassification%2520dataset%2520%2528dataset%2520size%2520%2524%255Capprox%252050K%2524%2529%252C%2520and%2520saving%2520%252430%255C%2525%2524%2520in%2520terms%250Aof%2520epsilon%2520for%2520%2524%255Cvarepsilon%255Cleq0.03%2524%2520for%2520the%2520Spambase%2520classification%2520dataset%250A%2528dataset%2520size%2520%2524%255Capprox%25205K%2524%2529.%2520Moreover%252C%2520we%2520show%2520that%2520for%2520situations%2520where%2520a%2520GBDT%250Ais%2520learning%2520a%2520stream%2520of%2520data%2520that%2520originates%2520from%2520different%2520subpopulations%250A%2528non-IID%2529%252C%2520S-BDT%2520improves%2520the%2520saving%2520of%2520epsilon%2520even%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12041v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S-BDT%3A%20Distributed%20Differentially%20Private%20Boosted%20Decision%20Trees&entry.906535625=Thorsten%20Peinemann%20and%20Moritz%20Kirschte%20and%20Joshua%20Stock%20and%20Carlos%20Cotrini%20and%20Esfandiar%20Mohammadi&entry.1292438233=%20%20We%20introduce%20S-BDT%3A%20a%20novel%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-differentially%20private%0Adistributed%20gradient%20boosted%20decision%20tree%20%28GBDT%29%20learner%20that%20improves%20the%0Aprotection%20of%20single%20training%20data%20points%20%28privacy%29%20while%20achieving%20meaningful%0Alearning%20goals%2C%20such%20as%20accuracy%20or%20regression%20error%20%28utility%29.%20S-BDT%20uses%20less%0Anoise%20by%20relying%20on%20non-spherical%20multivariate%20Gaussian%20noise%2C%20for%20which%20we%0Ashow%20tight%20subsampling%20bounds%20for%20privacy%20amplification%20and%20incorporate%20that%0Ainto%20a%20R%5C%27enyi%20filter%20for%20individual%20privacy%20accounting.%20We%20experimentally%0Areach%20the%20same%20utility%20while%20saving%20%2450%5C%25%24%20in%20terms%20of%20epsilon%20for%20%24%5Cvarepsilon%0A%5Cle%200.5%24%20on%20the%20Abalone%20regression%20dataset%20%28dataset%20size%20%24%5Capprox%204K%24%29%2C%20saving%0A%2430%5C%25%24%20in%20terms%20of%20epsilon%20for%20%24%5Cvarepsilon%20%5Cle%200.08%24%20for%20the%20Adult%0Aclassification%20dataset%20%28dataset%20size%20%24%5Capprox%2050K%24%29%2C%20and%20saving%20%2430%5C%25%24%20in%20terms%0Aof%20epsilon%20for%20%24%5Cvarepsilon%5Cleq0.03%24%20for%20the%20Spambase%20classification%20dataset%0A%28dataset%20size%20%24%5Capprox%205K%24%29.%20Moreover%2C%20we%20show%20that%20for%20situations%20where%20a%20GBDT%0Ais%20learning%20a%20stream%20of%20data%20that%20originates%20from%20different%20subpopulations%0A%28non-IID%29%2C%20S-BDT%20improves%20the%20saving%20of%20epsilon%20even%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12041v3&entry.124074799=Read"},
{"title": "Mitigating Backdoor Attacks in Federated Learning via Flipping Weight\n  Updates of Low-Activation Input Neurons", "author": "Binbin Ding and Penghui Yang and Zeqing Ge and Shengjun Huang", "abstract": "  Federated learning enables multiple clients to collaboratively train machine\nlearning models under the overall planning of the server while adhering to\nprivacy requirements. However, the server cannot directly oversee the local\ntraining process, creating an opportunity for malicious clients to introduce\nbackdoors. Existing research shows that backdoor attacks activate specific\nneurons in the compromised model, which remain dormant when processing clean\ndata. Leveraging this insight, we propose a method called Flipping Weight\nUpdates of Low-Activation Input Neurons (FLAIN) to defend against backdoor\nattacks in federated learning. Specifically, after completing global training,\nwe employ an auxiliary dataset to identify low-activation input neurons and\nflip the associated weight updates. We incrementally raise the threshold for\nlow-activation inputs and flip the weight updates iteratively, until the\nperformance degradation on the auxiliary data becomes unacceptable. Extensive\nexperiments validate that our method can effectively reduce the success rate of\nbackdoor attacks to a low level in various attack scenarios including those\nwith non-IID data distribution or high MCRs, causing only minimal performance\ndegradation on clean data.\n", "link": "http://arxiv.org/abs/2408.08655v1", "date": "2024-08-16", "relevancy": 2.3562, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4823}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4671}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Flipping%20Weight%0A%20%20Updates%20of%20Low-Activation%20Input%20Neurons&body=Title%3A%20Mitigating%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Flipping%20Weight%0A%20%20Updates%20of%20Low-Activation%20Input%20Neurons%0AAuthor%3A%20Binbin%20Ding%20and%20Penghui%20Yang%20and%20Zeqing%20Ge%20and%20Shengjun%20Huang%0AAbstract%3A%20%20%20Federated%20learning%20enables%20multiple%20clients%20to%20collaboratively%20train%20machine%0Alearning%20models%20under%20the%20overall%20planning%20of%20the%20server%20while%20adhering%20to%0Aprivacy%20requirements.%20However%2C%20the%20server%20cannot%20directly%20oversee%20the%20local%0Atraining%20process%2C%20creating%20an%20opportunity%20for%20malicious%20clients%20to%20introduce%0Abackdoors.%20Existing%20research%20shows%20that%20backdoor%20attacks%20activate%20specific%0Aneurons%20in%20the%20compromised%20model%2C%20which%20remain%20dormant%20when%20processing%20clean%0Adata.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20method%20called%20Flipping%20Weight%0AUpdates%20of%20Low-Activation%20Input%20Neurons%20%28FLAIN%29%20to%20defend%20against%20backdoor%0Aattacks%20in%20federated%20learning.%20Specifically%2C%20after%20completing%20global%20training%2C%0Awe%20employ%20an%20auxiliary%20dataset%20to%20identify%20low-activation%20input%20neurons%20and%0Aflip%20the%20associated%20weight%20updates.%20We%20incrementally%20raise%20the%20threshold%20for%0Alow-activation%20inputs%20and%20flip%20the%20weight%20updates%20iteratively%2C%20until%20the%0Aperformance%20degradation%20on%20the%20auxiliary%20data%20becomes%20unacceptable.%20Extensive%0Aexperiments%20validate%20that%20our%20method%20can%20effectively%20reduce%20the%20success%20rate%20of%0Abackdoor%20attacks%20to%20a%20low%20level%20in%20various%20attack%20scenarios%20including%20those%0Awith%20non-IID%20data%20distribution%20or%20high%20MCRs%2C%20causing%20only%20minimal%20performance%0Adegradation%20on%20clean%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Backdoor%2520Attacks%2520in%2520Federated%2520Learning%2520via%2520Flipping%2520Weight%250A%2520%2520Updates%2520of%2520Low-Activation%2520Input%2520Neurons%26entry.906535625%3DBinbin%2520Ding%2520and%2520Penghui%2520Yang%2520and%2520Zeqing%2520Ge%2520and%2520Shengjun%2520Huang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520multiple%2520clients%2520to%2520collaboratively%2520train%2520machine%250Alearning%2520models%2520under%2520the%2520overall%2520planning%2520of%2520the%2520server%2520while%2520adhering%2520to%250Aprivacy%2520requirements.%2520However%252C%2520the%2520server%2520cannot%2520directly%2520oversee%2520the%2520local%250Atraining%2520process%252C%2520creating%2520an%2520opportunity%2520for%2520malicious%2520clients%2520to%2520introduce%250Abackdoors.%2520Existing%2520research%2520shows%2520that%2520backdoor%2520attacks%2520activate%2520specific%250Aneurons%2520in%2520the%2520compromised%2520model%252C%2520which%2520remain%2520dormant%2520when%2520processing%2520clean%250Adata.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520a%2520method%2520called%2520Flipping%2520Weight%250AUpdates%2520of%2520Low-Activation%2520Input%2520Neurons%2520%2528FLAIN%2529%2520to%2520defend%2520against%2520backdoor%250Aattacks%2520in%2520federated%2520learning.%2520Specifically%252C%2520after%2520completing%2520global%2520training%252C%250Awe%2520employ%2520an%2520auxiliary%2520dataset%2520to%2520identify%2520low-activation%2520input%2520neurons%2520and%250Aflip%2520the%2520associated%2520weight%2520updates.%2520We%2520incrementally%2520raise%2520the%2520threshold%2520for%250Alow-activation%2520inputs%2520and%2520flip%2520the%2520weight%2520updates%2520iteratively%252C%2520until%2520the%250Aperformance%2520degradation%2520on%2520the%2520auxiliary%2520data%2520becomes%2520unacceptable.%2520Extensive%250Aexperiments%2520validate%2520that%2520our%2520method%2520can%2520effectively%2520reduce%2520the%2520success%2520rate%2520of%250Abackdoor%2520attacks%2520to%2520a%2520low%2520level%2520in%2520various%2520attack%2520scenarios%2520including%2520those%250Awith%2520non-IID%2520data%2520distribution%2520or%2520high%2520MCRs%252C%2520causing%2520only%2520minimal%2520performance%250Adegradation%2520on%2520clean%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Flipping%20Weight%0A%20%20Updates%20of%20Low-Activation%20Input%20Neurons&entry.906535625=Binbin%20Ding%20and%20Penghui%20Yang%20and%20Zeqing%20Ge%20and%20Shengjun%20Huang&entry.1292438233=%20%20Federated%20learning%20enables%20multiple%20clients%20to%20collaboratively%20train%20machine%0Alearning%20models%20under%20the%20overall%20planning%20of%20the%20server%20while%20adhering%20to%0Aprivacy%20requirements.%20However%2C%20the%20server%20cannot%20directly%20oversee%20the%20local%0Atraining%20process%2C%20creating%20an%20opportunity%20for%20malicious%20clients%20to%20introduce%0Abackdoors.%20Existing%20research%20shows%20that%20backdoor%20attacks%20activate%20specific%0Aneurons%20in%20the%20compromised%20model%2C%20which%20remain%20dormant%20when%20processing%20clean%0Adata.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20method%20called%20Flipping%20Weight%0AUpdates%20of%20Low-Activation%20Input%20Neurons%20%28FLAIN%29%20to%20defend%20against%20backdoor%0Aattacks%20in%20federated%20learning.%20Specifically%2C%20after%20completing%20global%20training%2C%0Awe%20employ%20an%20auxiliary%20dataset%20to%20identify%20low-activation%20input%20neurons%20and%0Aflip%20the%20associated%20weight%20updates.%20We%20incrementally%20raise%20the%20threshold%20for%0Alow-activation%20inputs%20and%20flip%20the%20weight%20updates%20iteratively%2C%20until%20the%0Aperformance%20degradation%20on%20the%20auxiliary%20data%20becomes%20unacceptable.%20Extensive%0Aexperiments%20validate%20that%20our%20method%20can%20effectively%20reduce%20the%20success%20rate%20of%0Abackdoor%20attacks%20to%20a%20low%20level%20in%20various%20attack%20scenarios%20including%20those%0Awith%20non-IID%20data%20distribution%20or%20high%20MCRs%2C%20causing%20only%20minimal%20performance%0Adegradation%20on%20clean%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08655v1&entry.124074799=Read"},
{"title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation", "author": "Eric Zelikman and Eliana Lorch and Lester Mackey and Adam Tauman Kalai", "abstract": "  Several recent advances in AI systems solve problems by providing a\n\"scaffolding\" program that structures multiple calls to language models (LMs)\nto generate better outputs. A scaffolding program is written in a programming\nlanguage such as Python. In this work, we use a language-model-infused\nscaffolding program to improve itself. We start with a seed \"improver\" that\nimproves an input program according to a given utility function by querying an\nLM several times and returning the best solution. We then run this seed\nimprover to improve itself. Across a small set of downstream tasks, the\nresulting improved improver generates programs with significantly better\nperformance than its seed improver. A variety of self-improvement strategies\nare proposed by the language model, including beam search, genetic algorithms,\nand simulated annealing. Since the language models themselves are not altered,\nthis is not full recursive self-improvement. Nonetheless, it demonstrates that\na modern language model, GPT-4 in our experiments, is capable of writing code\nthat can call itself to improve itself. We consider concerns around the\ndevelopment of self-improving technologies and evaluate the frequency with\nwhich the generated code bypasses a sandbox.\n", "link": "http://arxiv.org/abs/2310.02304v3", "date": "2024-08-16", "relevancy": 2.3548, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4748}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4745}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Taught%20Optimizer%20%28STOP%29%3A%20Recursively%20Self-Improving%20Code%20Generation&body=Title%3A%20Self-Taught%20Optimizer%20%28STOP%29%3A%20Recursively%20Self-Improving%20Code%20Generation%0AAuthor%3A%20Eric%20Zelikman%20and%20Eliana%20Lorch%20and%20Lester%20Mackey%20and%20Adam%20Tauman%20Kalai%0AAbstract%3A%20%20%20Several%20recent%20advances%20in%20AI%20systems%20solve%20problems%20by%20providing%20a%0A%22scaffolding%22%20program%20that%20structures%20multiple%20calls%20to%20language%20models%20%28LMs%29%0Ato%20generate%20better%20outputs.%20A%20scaffolding%20program%20is%20written%20in%20a%20programming%0Alanguage%20such%20as%20Python.%20In%20this%20work%2C%20we%20use%20a%20language-model-infused%0Ascaffolding%20program%20to%20improve%20itself.%20We%20start%20with%20a%20seed%20%22improver%22%20that%0Aimproves%20an%20input%20program%20according%20to%20a%20given%20utility%20function%20by%20querying%20an%0ALM%20several%20times%20and%20returning%20the%20best%20solution.%20We%20then%20run%20this%20seed%0Aimprover%20to%20improve%20itself.%20Across%20a%20small%20set%20of%20downstream%20tasks%2C%20the%0Aresulting%20improved%20improver%20generates%20programs%20with%20significantly%20better%0Aperformance%20than%20its%20seed%20improver.%20A%20variety%20of%20self-improvement%20strategies%0Aare%20proposed%20by%20the%20language%20model%2C%20including%20beam%20search%2C%20genetic%20algorithms%2C%0Aand%20simulated%20annealing.%20Since%20the%20language%20models%20themselves%20are%20not%20altered%2C%0Athis%20is%20not%20full%20recursive%20self-improvement.%20Nonetheless%2C%20it%20demonstrates%20that%0Aa%20modern%20language%20model%2C%20GPT-4%20in%20our%20experiments%2C%20is%20capable%20of%20writing%20code%0Athat%20can%20call%20itself%20to%20improve%20itself.%20We%20consider%20concerns%20around%20the%0Adevelopment%20of%20self-improving%20technologies%20and%20evaluate%20the%20frequency%20with%0Awhich%20the%20generated%20code%20bypasses%20a%20sandbox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02304v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Taught%2520Optimizer%2520%2528STOP%2529%253A%2520Recursively%2520Self-Improving%2520Code%2520Generation%26entry.906535625%3DEric%2520Zelikman%2520and%2520Eliana%2520Lorch%2520and%2520Lester%2520Mackey%2520and%2520Adam%2520Tauman%2520Kalai%26entry.1292438233%3D%2520%2520Several%2520recent%2520advances%2520in%2520AI%2520systems%2520solve%2520problems%2520by%2520providing%2520a%250A%2522scaffolding%2522%2520program%2520that%2520structures%2520multiple%2520calls%2520to%2520language%2520models%2520%2528LMs%2529%250Ato%2520generate%2520better%2520outputs.%2520A%2520scaffolding%2520program%2520is%2520written%2520in%2520a%2520programming%250Alanguage%2520such%2520as%2520Python.%2520In%2520this%2520work%252C%2520we%2520use%2520a%2520language-model-infused%250Ascaffolding%2520program%2520to%2520improve%2520itself.%2520We%2520start%2520with%2520a%2520seed%2520%2522improver%2522%2520that%250Aimproves%2520an%2520input%2520program%2520according%2520to%2520a%2520given%2520utility%2520function%2520by%2520querying%2520an%250ALM%2520several%2520times%2520and%2520returning%2520the%2520best%2520solution.%2520We%2520then%2520run%2520this%2520seed%250Aimprover%2520to%2520improve%2520itself.%2520Across%2520a%2520small%2520set%2520of%2520downstream%2520tasks%252C%2520the%250Aresulting%2520improved%2520improver%2520generates%2520programs%2520with%2520significantly%2520better%250Aperformance%2520than%2520its%2520seed%2520improver.%2520A%2520variety%2520of%2520self-improvement%2520strategies%250Aare%2520proposed%2520by%2520the%2520language%2520model%252C%2520including%2520beam%2520search%252C%2520genetic%2520algorithms%252C%250Aand%2520simulated%2520annealing.%2520Since%2520the%2520language%2520models%2520themselves%2520are%2520not%2520altered%252C%250Athis%2520is%2520not%2520full%2520recursive%2520self-improvement.%2520Nonetheless%252C%2520it%2520demonstrates%2520that%250Aa%2520modern%2520language%2520model%252C%2520GPT-4%2520in%2520our%2520experiments%252C%2520is%2520capable%2520of%2520writing%2520code%250Athat%2520can%2520call%2520itself%2520to%2520improve%2520itself.%2520We%2520consider%2520concerns%2520around%2520the%250Adevelopment%2520of%2520self-improving%2520technologies%2520and%2520evaluate%2520the%2520frequency%2520with%250Awhich%2520the%2520generated%2520code%2520bypasses%2520a%2520sandbox.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02304v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Taught%20Optimizer%20%28STOP%29%3A%20Recursively%20Self-Improving%20Code%20Generation&entry.906535625=Eric%20Zelikman%20and%20Eliana%20Lorch%20and%20Lester%20Mackey%20and%20Adam%20Tauman%20Kalai&entry.1292438233=%20%20Several%20recent%20advances%20in%20AI%20systems%20solve%20problems%20by%20providing%20a%0A%22scaffolding%22%20program%20that%20structures%20multiple%20calls%20to%20language%20models%20%28LMs%29%0Ato%20generate%20better%20outputs.%20A%20scaffolding%20program%20is%20written%20in%20a%20programming%0Alanguage%20such%20as%20Python.%20In%20this%20work%2C%20we%20use%20a%20language-model-infused%0Ascaffolding%20program%20to%20improve%20itself.%20We%20start%20with%20a%20seed%20%22improver%22%20that%0Aimproves%20an%20input%20program%20according%20to%20a%20given%20utility%20function%20by%20querying%20an%0ALM%20several%20times%20and%20returning%20the%20best%20solution.%20We%20then%20run%20this%20seed%0Aimprover%20to%20improve%20itself.%20Across%20a%20small%20set%20of%20downstream%20tasks%2C%20the%0Aresulting%20improved%20improver%20generates%20programs%20with%20significantly%20better%0Aperformance%20than%20its%20seed%20improver.%20A%20variety%20of%20self-improvement%20strategies%0Aare%20proposed%20by%20the%20language%20model%2C%20including%20beam%20search%2C%20genetic%20algorithms%2C%0Aand%20simulated%20annealing.%20Since%20the%20language%20models%20themselves%20are%20not%20altered%2C%0Athis%20is%20not%20full%20recursive%20self-improvement.%20Nonetheless%2C%20it%20demonstrates%20that%0Aa%20modern%20language%20model%2C%20GPT-4%20in%20our%20experiments%2C%20is%20capable%20of%20writing%20code%0Athat%20can%20call%20itself%20to%20improve%20itself.%20We%20consider%20concerns%20around%20the%0Adevelopment%20of%20self-improving%20technologies%20and%20evaluate%20the%20frequency%20with%0Awhich%20the%20generated%20code%20bypasses%20a%20sandbox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02304v3&entry.124074799=Read"},
{"title": "RBLA: Rank-Based-LoRA-Aggregation for Fine-tuning Heterogeneous Models\n  in FLaaS", "author": "Shuaijun Chen and Omid Tavallaie and Niousha Nazemi and Albert Y. Zomaya", "abstract": "  Federated Learning (FL) is a promising privacy-aware distributed learning\nframework that can be deployed on various devices, such as mobile phones,\ndesktops, and devices equipped with CPUs or GPUs. In the context of\nserver-based Federated Learning as a Service (FLaas), FL enables the central\nserver to coordinate the training process across multiple devices without\ndirect access to the local data, thereby enhancing privacy and data security.\nLow-Rank Adaptation (LoRA) is a method that fine-tunes models efficiently by\nfocusing on a low-dimensional subspace of the model's parameters. This approach\nsignificantly reduces computational and memory costs compared to fine-tuning\nall parameters from scratch. When integrated with FL, especially in a FLaas\nenvironment, LoRA allows for flexible and efficient deployment across diverse\nhardware with varying computational capabilities by adjusting the local model's\nrank. However, in LoRA-enabled FL, different clients may train models with\nvarying ranks, which poses a challenge for model aggregation on the server.\nCurrent methods of aggregating models of different ranks require padding\nweights to a uniform shape, which can degrade the global model's performance.\nTo address this issue, we propose Rank-Based LoRA Aggregation (RBLA), a novel\nmodel aggregation method designed for heterogeneous LoRA structures. RBLA\npreserves key features across models with different ranks. This paper analyzes\nthe issues with current padding methods that reshape models for aggregation in\na FLaas environment. Then, we introduce RBLA, a rank-based aggregation method\nthat maintains both low-rank and high-rank features. Finally, we demonstrate\nthe effectiveness of RBLA through comparative experiments with state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2408.08699v1", "date": "2024-08-16", "relevancy": 2.322, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4718}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4682}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RBLA%3A%20Rank-Based-LoRA-Aggregation%20for%20Fine-tuning%20Heterogeneous%20Models%0A%20%20in%20FLaaS&body=Title%3A%20RBLA%3A%20Rank-Based-LoRA-Aggregation%20for%20Fine-tuning%20Heterogeneous%20Models%0A%20%20in%20FLaaS%0AAuthor%3A%20Shuaijun%20Chen%20and%20Omid%20Tavallaie%20and%20Niousha%20Nazemi%20and%20Albert%20Y.%20Zomaya%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20promising%20privacy-aware%20distributed%20learning%0Aframework%20that%20can%20be%20deployed%20on%20various%20devices%2C%20such%20as%20mobile%20phones%2C%0Adesktops%2C%20and%20devices%20equipped%20with%20CPUs%20or%20GPUs.%20In%20the%20context%20of%0Aserver-based%20Federated%20Learning%20as%20a%20Service%20%28FLaas%29%2C%20FL%20enables%20the%20central%0Aserver%20to%20coordinate%20the%20training%20process%20across%20multiple%20devices%20without%0Adirect%20access%20to%20the%20local%20data%2C%20thereby%20enhancing%20privacy%20and%20data%20security.%0ALow-Rank%20Adaptation%20%28LoRA%29%20is%20a%20method%20that%20fine-tunes%20models%20efficiently%20by%0Afocusing%20on%20a%20low-dimensional%20subspace%20of%20the%20model%27s%20parameters.%20This%20approach%0Asignificantly%20reduces%20computational%20and%20memory%20costs%20compared%20to%20fine-tuning%0Aall%20parameters%20from%20scratch.%20When%20integrated%20with%20FL%2C%20especially%20in%20a%20FLaas%0Aenvironment%2C%20LoRA%20allows%20for%20flexible%20and%20efficient%20deployment%20across%20diverse%0Ahardware%20with%20varying%20computational%20capabilities%20by%20adjusting%20the%20local%20model%27s%0Arank.%20However%2C%20in%20LoRA-enabled%20FL%2C%20different%20clients%20may%20train%20models%20with%0Avarying%20ranks%2C%20which%20poses%20a%20challenge%20for%20model%20aggregation%20on%20the%20server.%0ACurrent%20methods%20of%20aggregating%20models%20of%20different%20ranks%20require%20padding%0Aweights%20to%20a%20uniform%20shape%2C%20which%20can%20degrade%20the%20global%20model%27s%20performance.%0ATo%20address%20this%20issue%2C%20we%20propose%20Rank-Based%20LoRA%20Aggregation%20%28RBLA%29%2C%20a%20novel%0Amodel%20aggregation%20method%20designed%20for%20heterogeneous%20LoRA%20structures.%20RBLA%0Apreserves%20key%20features%20across%20models%20with%20different%20ranks.%20This%20paper%20analyzes%0Athe%20issues%20with%20current%20padding%20methods%20that%20reshape%20models%20for%20aggregation%20in%0Aa%20FLaas%20environment.%20Then%2C%20we%20introduce%20RBLA%2C%20a%20rank-based%20aggregation%20method%0Athat%20maintains%20both%20low-rank%20and%20high-rank%20features.%20Finally%2C%20we%20demonstrate%0Athe%20effectiveness%20of%20RBLA%20through%20comparative%20experiments%20with%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRBLA%253A%2520Rank-Based-LoRA-Aggregation%2520for%2520Fine-tuning%2520Heterogeneous%2520Models%250A%2520%2520in%2520FLaaS%26entry.906535625%3DShuaijun%2520Chen%2520and%2520Omid%2520Tavallaie%2520and%2520Niousha%2520Nazemi%2520and%2520Albert%2520Y.%2520Zomaya%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520promising%2520privacy-aware%2520distributed%2520learning%250Aframework%2520that%2520can%2520be%2520deployed%2520on%2520various%2520devices%252C%2520such%2520as%2520mobile%2520phones%252C%250Adesktops%252C%2520and%2520devices%2520equipped%2520with%2520CPUs%2520or%2520GPUs.%2520In%2520the%2520context%2520of%250Aserver-based%2520Federated%2520Learning%2520as%2520a%2520Service%2520%2528FLaas%2529%252C%2520FL%2520enables%2520the%2520central%250Aserver%2520to%2520coordinate%2520the%2520training%2520process%2520across%2520multiple%2520devices%2520without%250Adirect%2520access%2520to%2520the%2520local%2520data%252C%2520thereby%2520enhancing%2520privacy%2520and%2520data%2520security.%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520is%2520a%2520method%2520that%2520fine-tunes%2520models%2520efficiently%2520by%250Afocusing%2520on%2520a%2520low-dimensional%2520subspace%2520of%2520the%2520model%2527s%2520parameters.%2520This%2520approach%250Asignificantly%2520reduces%2520computational%2520and%2520memory%2520costs%2520compared%2520to%2520fine-tuning%250Aall%2520parameters%2520from%2520scratch.%2520When%2520integrated%2520with%2520FL%252C%2520especially%2520in%2520a%2520FLaas%250Aenvironment%252C%2520LoRA%2520allows%2520for%2520flexible%2520and%2520efficient%2520deployment%2520across%2520diverse%250Ahardware%2520with%2520varying%2520computational%2520capabilities%2520by%2520adjusting%2520the%2520local%2520model%2527s%250Arank.%2520However%252C%2520in%2520LoRA-enabled%2520FL%252C%2520different%2520clients%2520may%2520train%2520models%2520with%250Avarying%2520ranks%252C%2520which%2520poses%2520a%2520challenge%2520for%2520model%2520aggregation%2520on%2520the%2520server.%250ACurrent%2520methods%2520of%2520aggregating%2520models%2520of%2520different%2520ranks%2520require%2520padding%250Aweights%2520to%2520a%2520uniform%2520shape%252C%2520which%2520can%2520degrade%2520the%2520global%2520model%2527s%2520performance.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520Rank-Based%2520LoRA%2520Aggregation%2520%2528RBLA%2529%252C%2520a%2520novel%250Amodel%2520aggregation%2520method%2520designed%2520for%2520heterogeneous%2520LoRA%2520structures.%2520RBLA%250Apreserves%2520key%2520features%2520across%2520models%2520with%2520different%2520ranks.%2520This%2520paper%2520analyzes%250Athe%2520issues%2520with%2520current%2520padding%2520methods%2520that%2520reshape%2520models%2520for%2520aggregation%2520in%250Aa%2520FLaas%2520environment.%2520Then%252C%2520we%2520introduce%2520RBLA%252C%2520a%2520rank-based%2520aggregation%2520method%250Athat%2520maintains%2520both%2520low-rank%2520and%2520high-rank%2520features.%2520Finally%252C%2520we%2520demonstrate%250Athe%2520effectiveness%2520of%2520RBLA%2520through%2520comparative%2520experiments%2520with%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RBLA%3A%20Rank-Based-LoRA-Aggregation%20for%20Fine-tuning%20Heterogeneous%20Models%0A%20%20in%20FLaaS&entry.906535625=Shuaijun%20Chen%20and%20Omid%20Tavallaie%20and%20Niousha%20Nazemi%20and%20Albert%20Y.%20Zomaya&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20promising%20privacy-aware%20distributed%20learning%0Aframework%20that%20can%20be%20deployed%20on%20various%20devices%2C%20such%20as%20mobile%20phones%2C%0Adesktops%2C%20and%20devices%20equipped%20with%20CPUs%20or%20GPUs.%20In%20the%20context%20of%0Aserver-based%20Federated%20Learning%20as%20a%20Service%20%28FLaas%29%2C%20FL%20enables%20the%20central%0Aserver%20to%20coordinate%20the%20training%20process%20across%20multiple%20devices%20without%0Adirect%20access%20to%20the%20local%20data%2C%20thereby%20enhancing%20privacy%20and%20data%20security.%0ALow-Rank%20Adaptation%20%28LoRA%29%20is%20a%20method%20that%20fine-tunes%20models%20efficiently%20by%0Afocusing%20on%20a%20low-dimensional%20subspace%20of%20the%20model%27s%20parameters.%20This%20approach%0Asignificantly%20reduces%20computational%20and%20memory%20costs%20compared%20to%20fine-tuning%0Aall%20parameters%20from%20scratch.%20When%20integrated%20with%20FL%2C%20especially%20in%20a%20FLaas%0Aenvironment%2C%20LoRA%20allows%20for%20flexible%20and%20efficient%20deployment%20across%20diverse%0Ahardware%20with%20varying%20computational%20capabilities%20by%20adjusting%20the%20local%20model%27s%0Arank.%20However%2C%20in%20LoRA-enabled%20FL%2C%20different%20clients%20may%20train%20models%20with%0Avarying%20ranks%2C%20which%20poses%20a%20challenge%20for%20model%20aggregation%20on%20the%20server.%0ACurrent%20methods%20of%20aggregating%20models%20of%20different%20ranks%20require%20padding%0Aweights%20to%20a%20uniform%20shape%2C%20which%20can%20degrade%20the%20global%20model%27s%20performance.%0ATo%20address%20this%20issue%2C%20we%20propose%20Rank-Based%20LoRA%20Aggregation%20%28RBLA%29%2C%20a%20novel%0Amodel%20aggregation%20method%20designed%20for%20heterogeneous%20LoRA%20structures.%20RBLA%0Apreserves%20key%20features%20across%20models%20with%20different%20ranks.%20This%20paper%20analyzes%0Athe%20issues%20with%20current%20padding%20methods%20that%20reshape%20models%20for%20aggregation%20in%0Aa%20FLaas%20environment.%20Then%2C%20we%20introduce%20RBLA%2C%20a%20rank-based%20aggregation%20method%0Athat%20maintains%20both%20low-rank%20and%20high-rank%20features.%20Finally%2C%20we%20demonstrate%0Athe%20effectiveness%20of%20RBLA%20through%20comparative%20experiments%20with%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08699v1&entry.124074799=Read"},
{"title": "PriorMapNet: Enhancing Online Vectorized HD Map Construction with Priors", "author": "Rongxuan Wang and Xin Lu and Xiaoyang Liu and Xiaoyi Zou and Tongyi Cao and Ying Li", "abstract": "  Online vectorized High-Definition (HD) map construction is crucial for\nsubsequent prediction and planning tasks in autonomous driving. Following MapTR\nparadigm, recent works have made noteworthy achievements. However, reference\npoints are randomly initialized in mainstream methods, leading to unstable\nmatching between predictions and ground truth. To address this issue, we\nintroduce PriorMapNet to enhance online vectorized HD map construction with\npriors. We propose the PPS-Decoder, which provides reference points with\nposition and structure priors. Fitted from the map elements in the dataset,\nprior reference points lower the learning difficulty and achieve stable\nmatching. Furthermore, we propose the PF-Encoder to enhance the image-to-BEV\ntransformation with BEV feature priors. Besides, we propose the DMD\ncross-attention, which decouples cross-attention along multi-scale and\nmulti-sample respectively to achieve efficiency. Our proposed PriorMapNet\nachieves state-of-the-art performance in the online vectorized HD map\nconstruction task on nuScenes and Argoverse2 datasets. The code will be\nreleased publicly soon.\n", "link": "http://arxiv.org/abs/2408.08802v1", "date": "2024-08-16", "relevancy": 2.2927, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6075}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PriorMapNet%3A%20Enhancing%20Online%20Vectorized%20HD%20Map%20Construction%20with%20Priors&body=Title%3A%20PriorMapNet%3A%20Enhancing%20Online%20Vectorized%20HD%20Map%20Construction%20with%20Priors%0AAuthor%3A%20Rongxuan%20Wang%20and%20Xin%20Lu%20and%20Xiaoyang%20Liu%20and%20Xiaoyi%20Zou%20and%20Tongyi%20Cao%20and%20Ying%20Li%0AAbstract%3A%20%20%20Online%20vectorized%20High-Definition%20%28HD%29%20map%20construction%20is%20crucial%20for%0Asubsequent%20prediction%20and%20planning%20tasks%20in%20autonomous%20driving.%20Following%20MapTR%0Aparadigm%2C%20recent%20works%20have%20made%20noteworthy%20achievements.%20However%2C%20reference%0Apoints%20are%20randomly%20initialized%20in%20mainstream%20methods%2C%20leading%20to%20unstable%0Amatching%20between%20predictions%20and%20ground%20truth.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20PriorMapNet%20to%20enhance%20online%20vectorized%20HD%20map%20construction%20with%0Apriors.%20We%20propose%20the%20PPS-Decoder%2C%20which%20provides%20reference%20points%20with%0Aposition%20and%20structure%20priors.%20Fitted%20from%20the%20map%20elements%20in%20the%20dataset%2C%0Aprior%20reference%20points%20lower%20the%20learning%20difficulty%20and%20achieve%20stable%0Amatching.%20Furthermore%2C%20we%20propose%20the%20PF-Encoder%20to%20enhance%20the%20image-to-BEV%0Atransformation%20with%20BEV%20feature%20priors.%20Besides%2C%20we%20propose%20the%20DMD%0Across-attention%2C%20which%20decouples%20cross-attention%20along%20multi-scale%20and%0Amulti-sample%20respectively%20to%20achieve%20efficiency.%20Our%20proposed%20PriorMapNet%0Aachieves%20state-of-the-art%20performance%20in%20the%20online%20vectorized%20HD%20map%0Aconstruction%20task%20on%20nuScenes%20and%20Argoverse2%20datasets.%20The%20code%20will%20be%0Areleased%20publicly%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPriorMapNet%253A%2520Enhancing%2520Online%2520Vectorized%2520HD%2520Map%2520Construction%2520with%2520Priors%26entry.906535625%3DRongxuan%2520Wang%2520and%2520Xin%2520Lu%2520and%2520Xiaoyang%2520Liu%2520and%2520Xiaoyi%2520Zou%2520and%2520Tongyi%2520Cao%2520and%2520Ying%2520Li%26entry.1292438233%3D%2520%2520Online%2520vectorized%2520High-Definition%2520%2528HD%2529%2520map%2520construction%2520is%2520crucial%2520for%250Asubsequent%2520prediction%2520and%2520planning%2520tasks%2520in%2520autonomous%2520driving.%2520Following%2520MapTR%250Aparadigm%252C%2520recent%2520works%2520have%2520made%2520noteworthy%2520achievements.%2520However%252C%2520reference%250Apoints%2520are%2520randomly%2520initialized%2520in%2520mainstream%2520methods%252C%2520leading%2520to%2520unstable%250Amatching%2520between%2520predictions%2520and%2520ground%2520truth.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520PriorMapNet%2520to%2520enhance%2520online%2520vectorized%2520HD%2520map%2520construction%2520with%250Apriors.%2520We%2520propose%2520the%2520PPS-Decoder%252C%2520which%2520provides%2520reference%2520points%2520with%250Aposition%2520and%2520structure%2520priors.%2520Fitted%2520from%2520the%2520map%2520elements%2520in%2520the%2520dataset%252C%250Aprior%2520reference%2520points%2520lower%2520the%2520learning%2520difficulty%2520and%2520achieve%2520stable%250Amatching.%2520Furthermore%252C%2520we%2520propose%2520the%2520PF-Encoder%2520to%2520enhance%2520the%2520image-to-BEV%250Atransformation%2520with%2520BEV%2520feature%2520priors.%2520Besides%252C%2520we%2520propose%2520the%2520DMD%250Across-attention%252C%2520which%2520decouples%2520cross-attention%2520along%2520multi-scale%2520and%250Amulti-sample%2520respectively%2520to%2520achieve%2520efficiency.%2520Our%2520proposed%2520PriorMapNet%250Aachieves%2520state-of-the-art%2520performance%2520in%2520the%2520online%2520vectorized%2520HD%2520map%250Aconstruction%2520task%2520on%2520nuScenes%2520and%2520Argoverse2%2520datasets.%2520The%2520code%2520will%2520be%250Areleased%2520publicly%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PriorMapNet%3A%20Enhancing%20Online%20Vectorized%20HD%20Map%20Construction%20with%20Priors&entry.906535625=Rongxuan%20Wang%20and%20Xin%20Lu%20and%20Xiaoyang%20Liu%20and%20Xiaoyi%20Zou%20and%20Tongyi%20Cao%20and%20Ying%20Li&entry.1292438233=%20%20Online%20vectorized%20High-Definition%20%28HD%29%20map%20construction%20is%20crucial%20for%0Asubsequent%20prediction%20and%20planning%20tasks%20in%20autonomous%20driving.%20Following%20MapTR%0Aparadigm%2C%20recent%20works%20have%20made%20noteworthy%20achievements.%20However%2C%20reference%0Apoints%20are%20randomly%20initialized%20in%20mainstream%20methods%2C%20leading%20to%20unstable%0Amatching%20between%20predictions%20and%20ground%20truth.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20PriorMapNet%20to%20enhance%20online%20vectorized%20HD%20map%20construction%20with%0Apriors.%20We%20propose%20the%20PPS-Decoder%2C%20which%20provides%20reference%20points%20with%0Aposition%20and%20structure%20priors.%20Fitted%20from%20the%20map%20elements%20in%20the%20dataset%2C%0Aprior%20reference%20points%20lower%20the%20learning%20difficulty%20and%20achieve%20stable%0Amatching.%20Furthermore%2C%20we%20propose%20the%20PF-Encoder%20to%20enhance%20the%20image-to-BEV%0Atransformation%20with%20BEV%20feature%20priors.%20Besides%2C%20we%20propose%20the%20DMD%0Across-attention%2C%20which%20decouples%20cross-attention%20along%20multi-scale%20and%0Amulti-sample%20respectively%20to%20achieve%20efficiency.%20Our%20proposed%20PriorMapNet%0Aachieves%20state-of-the-art%20performance%20in%20the%20online%20vectorized%20HD%20map%0Aconstruction%20task%20on%20nuScenes%20and%20Argoverse2%20datasets.%20The%20code%20will%20be%0Areleased%20publicly%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08802v1&entry.124074799=Read"},
{"title": "Speckle Noise Analysis for Synthetic Aperture Radar (SAR) Space Data", "author": "Sanjjushri Varshini R and Rohith Mahadevan and Bagiya Lakshmi S and Mathivanan Periasamy and Raja CSP Raman and Lokesh M", "abstract": "  This research tackles the challenge of speckle noise in Synthetic Aperture\nRadar (SAR) space data, a prevalent issue that hampers the clarity and utility\nof SAR images. The study presents a comparative analysis of six distinct\nspeckle noise reduction techniques: Lee Filtering, Frost Filtering, Kuan\nFiltering, Gaussian Filtering, Median Filtering, and Bilateral Filtering. These\nmethods, selected for their unique approaches to noise reduction and image\npreservation, were applied to SAR datasets sourced from the Alaska Satellite\nFacility (ASF). The performance of each technique was evaluated using a\ncomprehensive set of metrics, including Peak Signal-to-Noise Ratio (PSNR), Mean\nSquared Error (MSE), Structural Similarity Index (SSIM), Equivalent Number of\nLooks (ENL), and Speckle Suppression Index (SSI). The study concludes that both\nthe Lee and Kuan Filters are effective, with the choice of filter depending on\nthe specific application requirements for image quality and noise suppression.\nThis work provides valuable insights into optimizing SAR image processing, with\nsignificant implications for remote sensing, environmental monitoring, and\ngeological surveying.\n", "link": "http://arxiv.org/abs/2408.08774v1", "date": "2024-08-16", "relevancy": 2.2788, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4836}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speckle%20Noise%20Analysis%20for%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Space%20Data&body=Title%3A%20Speckle%20Noise%20Analysis%20for%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Space%20Data%0AAuthor%3A%20Sanjjushri%20Varshini%20R%20and%20Rohith%20Mahadevan%20and%20Bagiya%20Lakshmi%20S%20and%20Mathivanan%20Periasamy%20and%20Raja%20CSP%20Raman%20and%20Lokesh%20M%0AAbstract%3A%20%20%20This%20research%20tackles%20the%20challenge%20of%20speckle%20noise%20in%20Synthetic%20Aperture%0ARadar%20%28SAR%29%20space%20data%2C%20a%20prevalent%20issue%20that%20hampers%20the%20clarity%20and%20utility%0Aof%20SAR%20images.%20The%20study%20presents%20a%20comparative%20analysis%20of%20six%20distinct%0Aspeckle%20noise%20reduction%20techniques%3A%20Lee%20Filtering%2C%20Frost%20Filtering%2C%20Kuan%0AFiltering%2C%20Gaussian%20Filtering%2C%20Median%20Filtering%2C%20and%20Bilateral%20Filtering.%20These%0Amethods%2C%20selected%20for%20their%20unique%20approaches%20to%20noise%20reduction%20and%20image%0Apreservation%2C%20were%20applied%20to%20SAR%20datasets%20sourced%20from%20the%20Alaska%20Satellite%0AFacility%20%28ASF%29.%20The%20performance%20of%20each%20technique%20was%20evaluated%20using%20a%0Acomprehensive%20set%20of%20metrics%2C%20including%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%2C%20Mean%0ASquared%20Error%20%28MSE%29%2C%20Structural%20Similarity%20Index%20%28SSIM%29%2C%20Equivalent%20Number%20of%0ALooks%20%28ENL%29%2C%20and%20Speckle%20Suppression%20Index%20%28SSI%29.%20The%20study%20concludes%20that%20both%0Athe%20Lee%20and%20Kuan%20Filters%20are%20effective%2C%20with%20the%20choice%20of%20filter%20depending%20on%0Athe%20specific%20application%20requirements%20for%20image%20quality%20and%20noise%20suppression.%0AThis%20work%20provides%20valuable%20insights%20into%20optimizing%20SAR%20image%20processing%2C%20with%0Asignificant%20implications%20for%20remote%20sensing%2C%20environmental%20monitoring%2C%20and%0Ageological%20surveying.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeckle%2520Noise%2520Analysis%2520for%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520Space%2520Data%26entry.906535625%3DSanjjushri%2520Varshini%2520R%2520and%2520Rohith%2520Mahadevan%2520and%2520Bagiya%2520Lakshmi%2520S%2520and%2520Mathivanan%2520Periasamy%2520and%2520Raja%2520CSP%2520Raman%2520and%2520Lokesh%2520M%26entry.1292438233%3D%2520%2520This%2520research%2520tackles%2520the%2520challenge%2520of%2520speckle%2520noise%2520in%2520Synthetic%2520Aperture%250ARadar%2520%2528SAR%2529%2520space%2520data%252C%2520a%2520prevalent%2520issue%2520that%2520hampers%2520the%2520clarity%2520and%2520utility%250Aof%2520SAR%2520images.%2520The%2520study%2520presents%2520a%2520comparative%2520analysis%2520of%2520six%2520distinct%250Aspeckle%2520noise%2520reduction%2520techniques%253A%2520Lee%2520Filtering%252C%2520Frost%2520Filtering%252C%2520Kuan%250AFiltering%252C%2520Gaussian%2520Filtering%252C%2520Median%2520Filtering%252C%2520and%2520Bilateral%2520Filtering.%2520These%250Amethods%252C%2520selected%2520for%2520their%2520unique%2520approaches%2520to%2520noise%2520reduction%2520and%2520image%250Apreservation%252C%2520were%2520applied%2520to%2520SAR%2520datasets%2520sourced%2520from%2520the%2520Alaska%2520Satellite%250AFacility%2520%2528ASF%2529.%2520The%2520performance%2520of%2520each%2520technique%2520was%2520evaluated%2520using%2520a%250Acomprehensive%2520set%2520of%2520metrics%252C%2520including%2520Peak%2520Signal-to-Noise%2520Ratio%2520%2528PSNR%2529%252C%2520Mean%250ASquared%2520Error%2520%2528MSE%2529%252C%2520Structural%2520Similarity%2520Index%2520%2528SSIM%2529%252C%2520Equivalent%2520Number%2520of%250ALooks%2520%2528ENL%2529%252C%2520and%2520Speckle%2520Suppression%2520Index%2520%2528SSI%2529.%2520The%2520study%2520concludes%2520that%2520both%250Athe%2520Lee%2520and%2520Kuan%2520Filters%2520are%2520effective%252C%2520with%2520the%2520choice%2520of%2520filter%2520depending%2520on%250Athe%2520specific%2520application%2520requirements%2520for%2520image%2520quality%2520and%2520noise%2520suppression.%250AThis%2520work%2520provides%2520valuable%2520insights%2520into%2520optimizing%2520SAR%2520image%2520processing%252C%2520with%250Asignificant%2520implications%2520for%2520remote%2520sensing%252C%2520environmental%2520monitoring%252C%2520and%250Ageological%2520surveying.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speckle%20Noise%20Analysis%20for%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Space%20Data&entry.906535625=Sanjjushri%20Varshini%20R%20and%20Rohith%20Mahadevan%20and%20Bagiya%20Lakshmi%20S%20and%20Mathivanan%20Periasamy%20and%20Raja%20CSP%20Raman%20and%20Lokesh%20M&entry.1292438233=%20%20This%20research%20tackles%20the%20challenge%20of%20speckle%20noise%20in%20Synthetic%20Aperture%0ARadar%20%28SAR%29%20space%20data%2C%20a%20prevalent%20issue%20that%20hampers%20the%20clarity%20and%20utility%0Aof%20SAR%20images.%20The%20study%20presents%20a%20comparative%20analysis%20of%20six%20distinct%0Aspeckle%20noise%20reduction%20techniques%3A%20Lee%20Filtering%2C%20Frost%20Filtering%2C%20Kuan%0AFiltering%2C%20Gaussian%20Filtering%2C%20Median%20Filtering%2C%20and%20Bilateral%20Filtering.%20These%0Amethods%2C%20selected%20for%20their%20unique%20approaches%20to%20noise%20reduction%20and%20image%0Apreservation%2C%20were%20applied%20to%20SAR%20datasets%20sourced%20from%20the%20Alaska%20Satellite%0AFacility%20%28ASF%29.%20The%20performance%20of%20each%20technique%20was%20evaluated%20using%20a%0Acomprehensive%20set%20of%20metrics%2C%20including%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%2C%20Mean%0ASquared%20Error%20%28MSE%29%2C%20Structural%20Similarity%20Index%20%28SSIM%29%2C%20Equivalent%20Number%20of%0ALooks%20%28ENL%29%2C%20and%20Speckle%20Suppression%20Index%20%28SSI%29.%20The%20study%20concludes%20that%20both%0Athe%20Lee%20and%20Kuan%20Filters%20are%20effective%2C%20with%20the%20choice%20of%20filter%20depending%20on%0Athe%20specific%20application%20requirements%20for%20image%20quality%20and%20noise%20suppression.%0AThis%20work%20provides%20valuable%20insights%20into%20optimizing%20SAR%20image%20processing%2C%20with%0Asignificant%20implications%20for%20remote%20sensing%2C%20environmental%20monitoring%2C%20and%0Ageological%20surveying.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08774v1&entry.124074799=Read"},
{"title": "Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image\n  Super-Resolution", "author": "Tianyi Xu and Yiji Zhou and Xiaotao Hu and Kai Zhang and Anran Zhang and Xingye Qiu and Jun Xu", "abstract": "  Arbitrary-scale super-resolution (ASSR) aims to learn a single model for\nimage super-resolution at arbitrary magnifying scales. Existing ASSR networks\ntypically comprise an off-the-shelf scale-agnostic feature extractor and an\narbitrary scale upsampler. These feature extractors often use fixed network\narchitectures to address different ASSR inference tasks, each of which is\ncharacterized by an input image and an upsampling scale. However, this\noverlooks the difficulty variance of super-resolution on different inference\nscenarios, where simple images or small SR scales could be resolved with less\ncomputational effort than difficult images or large SR scales. To tackle this\ndifficulty variability, in this paper, we propose a Task-Aware Dynamic\nTransformer (TADT) as an input-adaptive feature extractor for efficient image\nASSR. Our TADT consists of a multi-scale feature extraction backbone built upon\ngroups of Multi-Scale Transformer Blocks (MSTBs) and a Task-Aware Routing\nController (TARC). The TARC predicts the inference paths within feature\nextraction backbone, specifically selecting MSTBs based on the input images and\nSR scales. The prediction of inference path is guided by a new loss function to\ntrade-off the SR accuracy and efficiency. Experiments demonstrate that, when\nworking with three popular arbitrary-scale upsamplers, our TADT achieves\nstate-of-the-art ASSR performance when compared with mainstream feature\nextractors, but with relatively fewer computational costs. The code will be\npublicly released.\n", "link": "http://arxiv.org/abs/2408.08736v1", "date": "2024-08-16", "relevancy": 2.2721, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5946}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Aware%20Dynamic%20Transformer%20for%20Efficient%20Arbitrary-Scale%20Image%0A%20%20Super-Resolution&body=Title%3A%20Task-Aware%20Dynamic%20Transformer%20for%20Efficient%20Arbitrary-Scale%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Tianyi%20Xu%20and%20Yiji%20Zhou%20and%20Xiaotao%20Hu%20and%20Kai%20Zhang%20and%20Anran%20Zhang%20and%20Xingye%20Qiu%20and%20Jun%20Xu%0AAbstract%3A%20%20%20Arbitrary-scale%20super-resolution%20%28ASSR%29%20aims%20to%20learn%20a%20single%20model%20for%0Aimage%20super-resolution%20at%20arbitrary%20magnifying%20scales.%20Existing%20ASSR%20networks%0Atypically%20comprise%20an%20off-the-shelf%20scale-agnostic%20feature%20extractor%20and%20an%0Aarbitrary%20scale%20upsampler.%20These%20feature%20extractors%20often%20use%20fixed%20network%0Aarchitectures%20to%20address%20different%20ASSR%20inference%20tasks%2C%20each%20of%20which%20is%0Acharacterized%20by%20an%20input%20image%20and%20an%20upsampling%20scale.%20However%2C%20this%0Aoverlooks%20the%20difficulty%20variance%20of%20super-resolution%20on%20different%20inference%0Ascenarios%2C%20where%20simple%20images%20or%20small%20SR%20scales%20could%20be%20resolved%20with%20less%0Acomputational%20effort%20than%20difficult%20images%20or%20large%20SR%20scales.%20To%20tackle%20this%0Adifficulty%20variability%2C%20in%20this%20paper%2C%20we%20propose%20a%20Task-Aware%20Dynamic%0ATransformer%20%28TADT%29%20as%20an%20input-adaptive%20feature%20extractor%20for%20efficient%20image%0AASSR.%20Our%20TADT%20consists%20of%20a%20multi-scale%20feature%20extraction%20backbone%20built%20upon%0Agroups%20of%20Multi-Scale%20Transformer%20Blocks%20%28MSTBs%29%20and%20a%20Task-Aware%20Routing%0AController%20%28TARC%29.%20The%20TARC%20predicts%20the%20inference%20paths%20within%20feature%0Aextraction%20backbone%2C%20specifically%20selecting%20MSTBs%20based%20on%20the%20input%20images%20and%0ASR%20scales.%20The%20prediction%20of%20inference%20path%20is%20guided%20by%20a%20new%20loss%20function%20to%0Atrade-off%20the%20SR%20accuracy%20and%20efficiency.%20Experiments%20demonstrate%20that%2C%20when%0Aworking%20with%20three%20popular%20arbitrary-scale%20upsamplers%2C%20our%20TADT%20achieves%0Astate-of-the-art%20ASSR%20performance%20when%20compared%20with%20mainstream%20feature%0Aextractors%2C%20but%20with%20relatively%20fewer%20computational%20costs.%20The%20code%20will%20be%0Apublicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Aware%2520Dynamic%2520Transformer%2520for%2520Efficient%2520Arbitrary-Scale%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DTianyi%2520Xu%2520and%2520Yiji%2520Zhou%2520and%2520Xiaotao%2520Hu%2520and%2520Kai%2520Zhang%2520and%2520Anran%2520Zhang%2520and%2520Xingye%2520Qiu%2520and%2520Jun%2520Xu%26entry.1292438233%3D%2520%2520Arbitrary-scale%2520super-resolution%2520%2528ASSR%2529%2520aims%2520to%2520learn%2520a%2520single%2520model%2520for%250Aimage%2520super-resolution%2520at%2520arbitrary%2520magnifying%2520scales.%2520Existing%2520ASSR%2520networks%250Atypically%2520comprise%2520an%2520off-the-shelf%2520scale-agnostic%2520feature%2520extractor%2520and%2520an%250Aarbitrary%2520scale%2520upsampler.%2520These%2520feature%2520extractors%2520often%2520use%2520fixed%2520network%250Aarchitectures%2520to%2520address%2520different%2520ASSR%2520inference%2520tasks%252C%2520each%2520of%2520which%2520is%250Acharacterized%2520by%2520an%2520input%2520image%2520and%2520an%2520upsampling%2520scale.%2520However%252C%2520this%250Aoverlooks%2520the%2520difficulty%2520variance%2520of%2520super-resolution%2520on%2520different%2520inference%250Ascenarios%252C%2520where%2520simple%2520images%2520or%2520small%2520SR%2520scales%2520could%2520be%2520resolved%2520with%2520less%250Acomputational%2520effort%2520than%2520difficult%2520images%2520or%2520large%2520SR%2520scales.%2520To%2520tackle%2520this%250Adifficulty%2520variability%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520Task-Aware%2520Dynamic%250ATransformer%2520%2528TADT%2529%2520as%2520an%2520input-adaptive%2520feature%2520extractor%2520for%2520efficient%2520image%250AASSR.%2520Our%2520TADT%2520consists%2520of%2520a%2520multi-scale%2520feature%2520extraction%2520backbone%2520built%2520upon%250Agroups%2520of%2520Multi-Scale%2520Transformer%2520Blocks%2520%2528MSTBs%2529%2520and%2520a%2520Task-Aware%2520Routing%250AController%2520%2528TARC%2529.%2520The%2520TARC%2520predicts%2520the%2520inference%2520paths%2520within%2520feature%250Aextraction%2520backbone%252C%2520specifically%2520selecting%2520MSTBs%2520based%2520on%2520the%2520input%2520images%2520and%250ASR%2520scales.%2520The%2520prediction%2520of%2520inference%2520path%2520is%2520guided%2520by%2520a%2520new%2520loss%2520function%2520to%250Atrade-off%2520the%2520SR%2520accuracy%2520and%2520efficiency.%2520Experiments%2520demonstrate%2520that%252C%2520when%250Aworking%2520with%2520three%2520popular%2520arbitrary-scale%2520upsamplers%252C%2520our%2520TADT%2520achieves%250Astate-of-the-art%2520ASSR%2520performance%2520when%2520compared%2520with%2520mainstream%2520feature%250Aextractors%252C%2520but%2520with%2520relatively%2520fewer%2520computational%2520costs.%2520The%2520code%2520will%2520be%250Apublicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Aware%20Dynamic%20Transformer%20for%20Efficient%20Arbitrary-Scale%20Image%0A%20%20Super-Resolution&entry.906535625=Tianyi%20Xu%20and%20Yiji%20Zhou%20and%20Xiaotao%20Hu%20and%20Kai%20Zhang%20and%20Anran%20Zhang%20and%20Xingye%20Qiu%20and%20Jun%20Xu&entry.1292438233=%20%20Arbitrary-scale%20super-resolution%20%28ASSR%29%20aims%20to%20learn%20a%20single%20model%20for%0Aimage%20super-resolution%20at%20arbitrary%20magnifying%20scales.%20Existing%20ASSR%20networks%0Atypically%20comprise%20an%20off-the-shelf%20scale-agnostic%20feature%20extractor%20and%20an%0Aarbitrary%20scale%20upsampler.%20These%20feature%20extractors%20often%20use%20fixed%20network%0Aarchitectures%20to%20address%20different%20ASSR%20inference%20tasks%2C%20each%20of%20which%20is%0Acharacterized%20by%20an%20input%20image%20and%20an%20upsampling%20scale.%20However%2C%20this%0Aoverlooks%20the%20difficulty%20variance%20of%20super-resolution%20on%20different%20inference%0Ascenarios%2C%20where%20simple%20images%20or%20small%20SR%20scales%20could%20be%20resolved%20with%20less%0Acomputational%20effort%20than%20difficult%20images%20or%20large%20SR%20scales.%20To%20tackle%20this%0Adifficulty%20variability%2C%20in%20this%20paper%2C%20we%20propose%20a%20Task-Aware%20Dynamic%0ATransformer%20%28TADT%29%20as%20an%20input-adaptive%20feature%20extractor%20for%20efficient%20image%0AASSR.%20Our%20TADT%20consists%20of%20a%20multi-scale%20feature%20extraction%20backbone%20built%20upon%0Agroups%20of%20Multi-Scale%20Transformer%20Blocks%20%28MSTBs%29%20and%20a%20Task-Aware%20Routing%0AController%20%28TARC%29.%20The%20TARC%20predicts%20the%20inference%20paths%20within%20feature%0Aextraction%20backbone%2C%20specifically%20selecting%20MSTBs%20based%20on%20the%20input%20images%20and%0ASR%20scales.%20The%20prediction%20of%20inference%20path%20is%20guided%20by%20a%20new%20loss%20function%20to%0Atrade-off%20the%20SR%20accuracy%20and%20efficiency.%20Experiments%20demonstrate%20that%2C%20when%0Aworking%20with%20three%20popular%20arbitrary-scale%20upsamplers%2C%20our%20TADT%20achieves%0Astate-of-the-art%20ASSR%20performance%20when%20compared%20with%20mainstream%20feature%0Aextractors%2C%20but%20with%20relatively%20fewer%20computational%20costs.%20The%20code%20will%20be%0Apublicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08736v1&entry.124074799=Read"},
{"title": "SLAM for Visually Impaired People: a Survey", "author": "Marziyeh Bamdad and Davide Scaramuzza and Alireza Darvishy", "abstract": "  In recent decades, several assistive technologies have been developed to\nimprove the ability of blind and visually impaired (BVI) individuals to\nnavigate independently and safely. At the same time, simultaneous localization\nand mapping (SLAM) techniques have become sufficiently robust and efficient to\nbe adopted in developing these assistive technologies. We present the first\nsystematic literature review of 54 recent studies on SLAM-based solutions for\nblind and visually impaired people, focusing on literature published from 2017\nonward. This review explores various localization and mapping techniques\nemployed in this context. We systematically identified and categorized diverse\nSLAM approaches and analyzed their localization and mapping techniques, sensor\ntypes, computing resources, and machine-learning methods. We discuss the\nadvantages and limitations of these techniques for blind and visually impaired\nnavigation. Moreover, we examine the major challenges described across studies,\nincluding practical challenges and considerations that affect usability and\nadoption. Our analysis also evaluates the effectiveness of these SLAM-based\nsolutions in real-world scenarios and user satisfaction, providing insights\ninto their practical impact on BVI mobility. The insights derived from this\nreview identify critical gaps and opportunities for future research activities,\nparticularly in addressing the challenges presented by dynamic and complex\nenvironments. We explain how SLAM technology offers the potential to improve\nthe ability of visually impaired individuals to navigate effectively. Finally,\nwe present future opportunities and challenges in this domain.\n", "link": "http://arxiv.org/abs/2212.04745v6", "date": "2024-08-16", "relevancy": 2.2122, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5796}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5405}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM%20for%20Visually%20Impaired%20People%3A%20a%20Survey&body=Title%3A%20SLAM%20for%20Visually%20Impaired%20People%3A%20a%20Survey%0AAuthor%3A%20Marziyeh%20Bamdad%20and%20Davide%20Scaramuzza%20and%20Alireza%20Darvishy%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20several%20assistive%20technologies%20have%20been%20developed%20to%0Aimprove%20the%20ability%20of%20blind%20and%20visually%20impaired%20%28BVI%29%20individuals%20to%0Anavigate%20independently%20and%20safely.%20At%20the%20same%20time%2C%20simultaneous%20localization%0Aand%20mapping%20%28SLAM%29%20techniques%20have%20become%20sufficiently%20robust%20and%20efficient%20to%0Abe%20adopted%20in%20developing%20these%20assistive%20technologies.%20We%20present%20the%20first%0Asystematic%20literature%20review%20of%2054%20recent%20studies%20on%20SLAM-based%20solutions%20for%0Ablind%20and%20visually%20impaired%20people%2C%20focusing%20on%20literature%20published%20from%202017%0Aonward.%20This%20review%20explores%20various%20localization%20and%20mapping%20techniques%0Aemployed%20in%20this%20context.%20We%20systematically%20identified%20and%20categorized%20diverse%0ASLAM%20approaches%20and%20analyzed%20their%20localization%20and%20mapping%20techniques%2C%20sensor%0Atypes%2C%20computing%20resources%2C%20and%20machine-learning%20methods.%20We%20discuss%20the%0Aadvantages%20and%20limitations%20of%20these%20techniques%20for%20blind%20and%20visually%20impaired%0Anavigation.%20Moreover%2C%20we%20examine%20the%20major%20challenges%20described%20across%20studies%2C%0Aincluding%20practical%20challenges%20and%20considerations%20that%20affect%20usability%20and%0Aadoption.%20Our%20analysis%20also%20evaluates%20the%20effectiveness%20of%20these%20SLAM-based%0Asolutions%20in%20real-world%20scenarios%20and%20user%20satisfaction%2C%20providing%20insights%0Ainto%20their%20practical%20impact%20on%20BVI%20mobility.%20The%20insights%20derived%20from%20this%0Areview%20identify%20critical%20gaps%20and%20opportunities%20for%20future%20research%20activities%2C%0Aparticularly%20in%20addressing%20the%20challenges%20presented%20by%20dynamic%20and%20complex%0Aenvironments.%20We%20explain%20how%20SLAM%20technology%20offers%20the%20potential%20to%20improve%0Athe%20ability%20of%20visually%20impaired%20individuals%20to%20navigate%20effectively.%20Finally%2C%0Awe%20present%20future%20opportunities%20and%20challenges%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.04745v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM%2520for%2520Visually%2520Impaired%2520People%253A%2520a%2520Survey%26entry.906535625%3DMarziyeh%2520Bamdad%2520and%2520Davide%2520Scaramuzza%2520and%2520Alireza%2520Darvishy%26entry.1292438233%3D%2520%2520In%2520recent%2520decades%252C%2520several%2520assistive%2520technologies%2520have%2520been%2520developed%2520to%250Aimprove%2520the%2520ability%2520of%2520blind%2520and%2520visually%2520impaired%2520%2528BVI%2529%2520individuals%2520to%250Anavigate%2520independently%2520and%2520safely.%2520At%2520the%2520same%2520time%252C%2520simultaneous%2520localization%250Aand%2520mapping%2520%2528SLAM%2529%2520techniques%2520have%2520become%2520sufficiently%2520robust%2520and%2520efficient%2520to%250Abe%2520adopted%2520in%2520developing%2520these%2520assistive%2520technologies.%2520We%2520present%2520the%2520first%250Asystematic%2520literature%2520review%2520of%252054%2520recent%2520studies%2520on%2520SLAM-based%2520solutions%2520for%250Ablind%2520and%2520visually%2520impaired%2520people%252C%2520focusing%2520on%2520literature%2520published%2520from%25202017%250Aonward.%2520This%2520review%2520explores%2520various%2520localization%2520and%2520mapping%2520techniques%250Aemployed%2520in%2520this%2520context.%2520We%2520systematically%2520identified%2520and%2520categorized%2520diverse%250ASLAM%2520approaches%2520and%2520analyzed%2520their%2520localization%2520and%2520mapping%2520techniques%252C%2520sensor%250Atypes%252C%2520computing%2520resources%252C%2520and%2520machine-learning%2520methods.%2520We%2520discuss%2520the%250Aadvantages%2520and%2520limitations%2520of%2520these%2520techniques%2520for%2520blind%2520and%2520visually%2520impaired%250Anavigation.%2520Moreover%252C%2520we%2520examine%2520the%2520major%2520challenges%2520described%2520across%2520studies%252C%250Aincluding%2520practical%2520challenges%2520and%2520considerations%2520that%2520affect%2520usability%2520and%250Aadoption.%2520Our%2520analysis%2520also%2520evaluates%2520the%2520effectiveness%2520of%2520these%2520SLAM-based%250Asolutions%2520in%2520real-world%2520scenarios%2520and%2520user%2520satisfaction%252C%2520providing%2520insights%250Ainto%2520their%2520practical%2520impact%2520on%2520BVI%2520mobility.%2520The%2520insights%2520derived%2520from%2520this%250Areview%2520identify%2520critical%2520gaps%2520and%2520opportunities%2520for%2520future%2520research%2520activities%252C%250Aparticularly%2520in%2520addressing%2520the%2520challenges%2520presented%2520by%2520dynamic%2520and%2520complex%250Aenvironments.%2520We%2520explain%2520how%2520SLAM%2520technology%2520offers%2520the%2520potential%2520to%2520improve%250Athe%2520ability%2520of%2520visually%2520impaired%2520individuals%2520to%2520navigate%2520effectively.%2520Finally%252C%250Awe%2520present%2520future%2520opportunities%2520and%2520challenges%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.04745v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM%20for%20Visually%20Impaired%20People%3A%20a%20Survey&entry.906535625=Marziyeh%20Bamdad%20and%20Davide%20Scaramuzza%20and%20Alireza%20Darvishy&entry.1292438233=%20%20In%20recent%20decades%2C%20several%20assistive%20technologies%20have%20been%20developed%20to%0Aimprove%20the%20ability%20of%20blind%20and%20visually%20impaired%20%28BVI%29%20individuals%20to%0Anavigate%20independently%20and%20safely.%20At%20the%20same%20time%2C%20simultaneous%20localization%0Aand%20mapping%20%28SLAM%29%20techniques%20have%20become%20sufficiently%20robust%20and%20efficient%20to%0Abe%20adopted%20in%20developing%20these%20assistive%20technologies.%20We%20present%20the%20first%0Asystematic%20literature%20review%20of%2054%20recent%20studies%20on%20SLAM-based%20solutions%20for%0Ablind%20and%20visually%20impaired%20people%2C%20focusing%20on%20literature%20published%20from%202017%0Aonward.%20This%20review%20explores%20various%20localization%20and%20mapping%20techniques%0Aemployed%20in%20this%20context.%20We%20systematically%20identified%20and%20categorized%20diverse%0ASLAM%20approaches%20and%20analyzed%20their%20localization%20and%20mapping%20techniques%2C%20sensor%0Atypes%2C%20computing%20resources%2C%20and%20machine-learning%20methods.%20We%20discuss%20the%0Aadvantages%20and%20limitations%20of%20these%20techniques%20for%20blind%20and%20visually%20impaired%0Anavigation.%20Moreover%2C%20we%20examine%20the%20major%20challenges%20described%20across%20studies%2C%0Aincluding%20practical%20challenges%20and%20considerations%20that%20affect%20usability%20and%0Aadoption.%20Our%20analysis%20also%20evaluates%20the%20effectiveness%20of%20these%20SLAM-based%0Asolutions%20in%20real-world%20scenarios%20and%20user%20satisfaction%2C%20providing%20insights%0Ainto%20their%20practical%20impact%20on%20BVI%20mobility.%20The%20insights%20derived%20from%20this%0Areview%20identify%20critical%20gaps%20and%20opportunities%20for%20future%20research%20activities%2C%0Aparticularly%20in%20addressing%20the%20challenges%20presented%20by%20dynamic%20and%20complex%0Aenvironments.%20We%20explain%20how%20SLAM%20technology%20offers%20the%20potential%20to%20improve%0Athe%20ability%20of%20visually%20impaired%20individuals%20to%20navigate%20effectively.%20Finally%2C%0Awe%20present%20future%20opportunities%20and%20challenges%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.04745v6&entry.124074799=Read"},
{"title": "DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training\n  Quantization for Vision Transformers", "author": "Lianwei Yang and Haisong Gong and Qingyi Gu", "abstract": "  Vision transformers (ViTs) have garnered significant attention for their\nperformance in vision tasks, but the high computational cost and significant\nlatency issues have hindered widespread adoption. Post-training quantization\n(PTQ), a promising method for model compression, still faces accuracy\ndegradation challenges with ViTs. There are two reasons for this: the existing\nquantization paradigm does not fit the power-law distribution of post-Softmax\nactivations well, and accuracy inevitably decreases after reparameterizing\npost-LayerNorm activations. We propose a Distribution-Friendly and\nOutlier-Aware Post-training Quantization method for Vision Transformers, named\nDopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and\nintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more\non values near 1, more accurately preserving the power-law distribution of\npost-Softmax activations, and achieves favorable results. Besides, during the\nreparameterization of post-LayerNorm activations from channel-wise to\nlayer-wise quantization, the accuracy degradation is mainly due to the\nsignificant impact of outliers in the scaling factors. Therefore, DopQ-ViT\nproposes a method to select Median as the Optimal Scaling Factor, denoted as\nMOSF, which compensates for the influence of outliers and preserves the\nperformance of the quantization model. DopQ-ViT has been extensively validated\nand significantly improves the performance of quantization models, especially\nin low-bit settings.\n", "link": "http://arxiv.org/abs/2408.03291v2", "date": "2024-08-16", "relevancy": 2.2109, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5637}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.552}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DopQ-ViT%3A%20Towards%20Distribution-Friendly%20and%20Outlier-Aware%20Post-Training%0A%20%20Quantization%20for%20Vision%20Transformers&body=Title%3A%20DopQ-ViT%3A%20Towards%20Distribution-Friendly%20and%20Outlier-Aware%20Post-Training%0A%20%20Quantization%20for%20Vision%20Transformers%0AAuthor%3A%20Lianwei%20Yang%20and%20Haisong%20Gong%20and%20Qingyi%20Gu%0AAbstract%3A%20%20%20Vision%20transformers%20%28ViTs%29%20have%20garnered%20significant%20attention%20for%20their%0Aperformance%20in%20vision%20tasks%2C%20but%20the%20high%20computational%20cost%20and%20significant%0Alatency%20issues%20have%20hindered%20widespread%20adoption.%20Post-training%20quantization%0A%28PTQ%29%2C%20a%20promising%20method%20for%20model%20compression%2C%20still%20faces%20accuracy%0Adegradation%20challenges%20with%20ViTs.%20There%20are%20two%20reasons%20for%20this%3A%20the%20existing%0Aquantization%20paradigm%20does%20not%20fit%20the%20power-law%20distribution%20of%20post-Softmax%0Aactivations%20well%2C%20and%20accuracy%20inevitably%20decreases%20after%20reparameterizing%0Apost-LayerNorm%20activations.%20We%20propose%20a%20Distribution-Friendly%20and%0AOutlier-Aware%20Post-training%20Quantization%20method%20for%20Vision%20Transformers%2C%20named%0ADopQ-ViT.%20DopQ-ViT%20analyzes%20the%20inefficiencies%20of%20current%20quantizers%20and%0Aintroduces%20a%20distribution-friendly%20Tan%20Quantizer%20called%20TanQ.%20TanQ%20focuses%20more%0Aon%20values%20near%201%2C%20more%20accurately%20preserving%20the%20power-law%20distribution%20of%0Apost-Softmax%20activations%2C%20and%20achieves%20favorable%20results.%20Besides%2C%20during%20the%0Areparameterization%20of%20post-LayerNorm%20activations%20from%20channel-wise%20to%0Alayer-wise%20quantization%2C%20the%20accuracy%20degradation%20is%20mainly%20due%20to%20the%0Asignificant%20impact%20of%20outliers%20in%20the%20scaling%20factors.%20Therefore%2C%20DopQ-ViT%0Aproposes%20a%20method%20to%20select%20Median%20as%20the%20Optimal%20Scaling%20Factor%2C%20denoted%20as%0AMOSF%2C%20which%20compensates%20for%20the%20influence%20of%20outliers%20and%20preserves%20the%0Aperformance%20of%20the%20quantization%20model.%20DopQ-ViT%20has%20been%20extensively%20validated%0Aand%20significantly%20improves%20the%20performance%20of%20quantization%20models%2C%20especially%0Ain%20low-bit%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDopQ-ViT%253A%2520Towards%2520Distribution-Friendly%2520and%2520Outlier-Aware%2520Post-Training%250A%2520%2520Quantization%2520for%2520Vision%2520Transformers%26entry.906535625%3DLianwei%2520Yang%2520and%2520Haisong%2520Gong%2520and%2520Qingyi%2520Gu%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520%2528ViTs%2529%2520have%2520garnered%2520significant%2520attention%2520for%2520their%250Aperformance%2520in%2520vision%2520tasks%252C%2520but%2520the%2520high%2520computational%2520cost%2520and%2520significant%250Alatency%2520issues%2520have%2520hindered%2520widespread%2520adoption.%2520Post-training%2520quantization%250A%2528PTQ%2529%252C%2520a%2520promising%2520method%2520for%2520model%2520compression%252C%2520still%2520faces%2520accuracy%250Adegradation%2520challenges%2520with%2520ViTs.%2520There%2520are%2520two%2520reasons%2520for%2520this%253A%2520the%2520existing%250Aquantization%2520paradigm%2520does%2520not%2520fit%2520the%2520power-law%2520distribution%2520of%2520post-Softmax%250Aactivations%2520well%252C%2520and%2520accuracy%2520inevitably%2520decreases%2520after%2520reparameterizing%250Apost-LayerNorm%2520activations.%2520We%2520propose%2520a%2520Distribution-Friendly%2520and%250AOutlier-Aware%2520Post-training%2520Quantization%2520method%2520for%2520Vision%2520Transformers%252C%2520named%250ADopQ-ViT.%2520DopQ-ViT%2520analyzes%2520the%2520inefficiencies%2520of%2520current%2520quantizers%2520and%250Aintroduces%2520a%2520distribution-friendly%2520Tan%2520Quantizer%2520called%2520TanQ.%2520TanQ%2520focuses%2520more%250Aon%2520values%2520near%25201%252C%2520more%2520accurately%2520preserving%2520the%2520power-law%2520distribution%2520of%250Apost-Softmax%2520activations%252C%2520and%2520achieves%2520favorable%2520results.%2520Besides%252C%2520during%2520the%250Areparameterization%2520of%2520post-LayerNorm%2520activations%2520from%2520channel-wise%2520to%250Alayer-wise%2520quantization%252C%2520the%2520accuracy%2520degradation%2520is%2520mainly%2520due%2520to%2520the%250Asignificant%2520impact%2520of%2520outliers%2520in%2520the%2520scaling%2520factors.%2520Therefore%252C%2520DopQ-ViT%250Aproposes%2520a%2520method%2520to%2520select%2520Median%2520as%2520the%2520Optimal%2520Scaling%2520Factor%252C%2520denoted%2520as%250AMOSF%252C%2520which%2520compensates%2520for%2520the%2520influence%2520of%2520outliers%2520and%2520preserves%2520the%250Aperformance%2520of%2520the%2520quantization%2520model.%2520DopQ-ViT%2520has%2520been%2520extensively%2520validated%250Aand%2520significantly%2520improves%2520the%2520performance%2520of%2520quantization%2520models%252C%2520especially%250Ain%2520low-bit%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DopQ-ViT%3A%20Towards%20Distribution-Friendly%20and%20Outlier-Aware%20Post-Training%0A%20%20Quantization%20for%20Vision%20Transformers&entry.906535625=Lianwei%20Yang%20and%20Haisong%20Gong%20and%20Qingyi%20Gu&entry.1292438233=%20%20Vision%20transformers%20%28ViTs%29%20have%20garnered%20significant%20attention%20for%20their%0Aperformance%20in%20vision%20tasks%2C%20but%20the%20high%20computational%20cost%20and%20significant%0Alatency%20issues%20have%20hindered%20widespread%20adoption.%20Post-training%20quantization%0A%28PTQ%29%2C%20a%20promising%20method%20for%20model%20compression%2C%20still%20faces%20accuracy%0Adegradation%20challenges%20with%20ViTs.%20There%20are%20two%20reasons%20for%20this%3A%20the%20existing%0Aquantization%20paradigm%20does%20not%20fit%20the%20power-law%20distribution%20of%20post-Softmax%0Aactivations%20well%2C%20and%20accuracy%20inevitably%20decreases%20after%20reparameterizing%0Apost-LayerNorm%20activations.%20We%20propose%20a%20Distribution-Friendly%20and%0AOutlier-Aware%20Post-training%20Quantization%20method%20for%20Vision%20Transformers%2C%20named%0ADopQ-ViT.%20DopQ-ViT%20analyzes%20the%20inefficiencies%20of%20current%20quantizers%20and%0Aintroduces%20a%20distribution-friendly%20Tan%20Quantizer%20called%20TanQ.%20TanQ%20focuses%20more%0Aon%20values%20near%201%2C%20more%20accurately%20preserving%20the%20power-law%20distribution%20of%0Apost-Softmax%20activations%2C%20and%20achieves%20favorable%20results.%20Besides%2C%20during%20the%0Areparameterization%20of%20post-LayerNorm%20activations%20from%20channel-wise%20to%0Alayer-wise%20quantization%2C%20the%20accuracy%20degradation%20is%20mainly%20due%20to%20the%0Asignificant%20impact%20of%20outliers%20in%20the%20scaling%20factors.%20Therefore%2C%20DopQ-ViT%0Aproposes%20a%20method%20to%20select%20Median%20as%20the%20Optimal%20Scaling%20Factor%2C%20denoted%20as%0AMOSF%2C%20which%20compensates%20for%20the%20influence%20of%20outliers%20and%20preserves%20the%0Aperformance%20of%20the%20quantization%20model.%20DopQ-ViT%20has%20been%20extensively%20validated%0Aand%20significantly%20improves%20the%20performance%20of%20quantization%20models%2C%20especially%0Ain%20low-bit%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03291v2&entry.124074799=Read"},
{"title": "Self-Supervised Multimodal Learning: A Survey", "author": "Yongshuo Zong and Oisin Mac Aodha and Timothy Hospedales", "abstract": "  Multimodal learning, which aims to understand and analyze information from\nmultiple modalities, has achieved substantial progress in the supervised regime\nin recent years. However, the heavy dependence on data paired with expensive\nhuman annotations impedes scaling up models. Meanwhile, given the availability\nof large-scale unannotated data in the wild, self-supervised learning has\nbecome an attractive strategy to alleviate the annotation bottleneck. Building\non these two directions, self-supervised multimodal learning (SSML) provides\nways to learn from raw multimodal data. In this survey, we provide a\ncomprehensive review of the state-of-the-art in SSML, in which we elucidate\nthree major challenges intrinsic to self-supervised learning with multimodal\ndata: (1) learning representations from multimodal data without labels, (2)\nfusion of different modalities, and (3) learning with unaligned data. We then\ndetail existing solutions to these challenges. Specifically, we consider (1)\nobjectives for learning from multimodal unlabeled data via self-supervision,\n(2) model architectures from the perspective of different multimodal fusion\nstrategies, and (3) pair-free learning strategies for coarse-grained and\nfine-grained alignment. We also review real-world applications of SSML\nalgorithms in diverse fields such as healthcare, remote sensing, and machine\ntranslation. Finally, we discuss challenges and future directions for SSML. A\ncollection of related resources can be found at:\nhttps://github.com/ys-zong/awesome-self-supervised-multimodal-learning.\n", "link": "http://arxiv.org/abs/2304.01008v3", "date": "2024-08-16", "relevancy": 2.2105, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5609}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Multimodal%20Learning%3A%20A%20Survey&body=Title%3A%20Self-Supervised%20Multimodal%20Learning%3A%20A%20Survey%0AAuthor%3A%20Yongshuo%20Zong%20and%20Oisin%20Mac%20Aodha%20and%20Timothy%20Hospedales%0AAbstract%3A%20%20%20Multimodal%20learning%2C%20which%20aims%20to%20understand%20and%20analyze%20information%20from%0Amultiple%20modalities%2C%20has%20achieved%20substantial%20progress%20in%20the%20supervised%20regime%0Ain%20recent%20years.%20However%2C%20the%20heavy%20dependence%20on%20data%20paired%20with%20expensive%0Ahuman%20annotations%20impedes%20scaling%20up%20models.%20Meanwhile%2C%20given%20the%20availability%0Aof%20large-scale%20unannotated%20data%20in%20the%20wild%2C%20self-supervised%20learning%20has%0Abecome%20an%20attractive%20strategy%20to%20alleviate%20the%20annotation%20bottleneck.%20Building%0Aon%20these%20two%20directions%2C%20self-supervised%20multimodal%20learning%20%28SSML%29%20provides%0Aways%20to%20learn%20from%20raw%20multimodal%20data.%20In%20this%20survey%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20the%20state-of-the-art%20in%20SSML%2C%20in%20which%20we%20elucidate%0Athree%20major%20challenges%20intrinsic%20to%20self-supervised%20learning%20with%20multimodal%0Adata%3A%20%281%29%20learning%20representations%20from%20multimodal%20data%20without%20labels%2C%20%282%29%0Afusion%20of%20different%20modalities%2C%20and%20%283%29%20learning%20with%20unaligned%20data.%20We%20then%0Adetail%20existing%20solutions%20to%20these%20challenges.%20Specifically%2C%20we%20consider%20%281%29%0Aobjectives%20for%20learning%20from%20multimodal%20unlabeled%20data%20via%20self-supervision%2C%0A%282%29%20model%20architectures%20from%20the%20perspective%20of%20different%20multimodal%20fusion%0Astrategies%2C%20and%20%283%29%20pair-free%20learning%20strategies%20for%20coarse-grained%20and%0Afine-grained%20alignment.%20We%20also%20review%20real-world%20applications%20of%20SSML%0Aalgorithms%20in%20diverse%20fields%20such%20as%20healthcare%2C%20remote%20sensing%2C%20and%20machine%0Atranslation.%20Finally%2C%20we%20discuss%20challenges%20and%20future%20directions%20for%20SSML.%20A%0Acollection%20of%20related%20resources%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/ys-zong/awesome-self-supervised-multimodal-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.01008v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Multimodal%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DYongshuo%2520Zong%2520and%2520Oisin%2520Mac%2520Aodha%2520and%2520Timothy%2520Hospedales%26entry.1292438233%3D%2520%2520Multimodal%2520learning%252C%2520which%2520aims%2520to%2520understand%2520and%2520analyze%2520information%2520from%250Amultiple%2520modalities%252C%2520has%2520achieved%2520substantial%2520progress%2520in%2520the%2520supervised%2520regime%250Ain%2520recent%2520years.%2520However%252C%2520the%2520heavy%2520dependence%2520on%2520data%2520paired%2520with%2520expensive%250Ahuman%2520annotations%2520impedes%2520scaling%2520up%2520models.%2520Meanwhile%252C%2520given%2520the%2520availability%250Aof%2520large-scale%2520unannotated%2520data%2520in%2520the%2520wild%252C%2520self-supervised%2520learning%2520has%250Abecome%2520an%2520attractive%2520strategy%2520to%2520alleviate%2520the%2520annotation%2520bottleneck.%2520Building%250Aon%2520these%2520two%2520directions%252C%2520self-supervised%2520multimodal%2520learning%2520%2528SSML%2529%2520provides%250Aways%2520to%2520learn%2520from%2520raw%2520multimodal%2520data.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%250Acomprehensive%2520review%2520of%2520the%2520state-of-the-art%2520in%2520SSML%252C%2520in%2520which%2520we%2520elucidate%250Athree%2520major%2520challenges%2520intrinsic%2520to%2520self-supervised%2520learning%2520with%2520multimodal%250Adata%253A%2520%25281%2529%2520learning%2520representations%2520from%2520multimodal%2520data%2520without%2520labels%252C%2520%25282%2529%250Afusion%2520of%2520different%2520modalities%252C%2520and%2520%25283%2529%2520learning%2520with%2520unaligned%2520data.%2520We%2520then%250Adetail%2520existing%2520solutions%2520to%2520these%2520challenges.%2520Specifically%252C%2520we%2520consider%2520%25281%2529%250Aobjectives%2520for%2520learning%2520from%2520multimodal%2520unlabeled%2520data%2520via%2520self-supervision%252C%250A%25282%2529%2520model%2520architectures%2520from%2520the%2520perspective%2520of%2520different%2520multimodal%2520fusion%250Astrategies%252C%2520and%2520%25283%2529%2520pair-free%2520learning%2520strategies%2520for%2520coarse-grained%2520and%250Afine-grained%2520alignment.%2520We%2520also%2520review%2520real-world%2520applications%2520of%2520SSML%250Aalgorithms%2520in%2520diverse%2520fields%2520such%2520as%2520healthcare%252C%2520remote%2520sensing%252C%2520and%2520machine%250Atranslation.%2520Finally%252C%2520we%2520discuss%2520challenges%2520and%2520future%2520directions%2520for%2520SSML.%2520A%250Acollection%2520of%2520related%2520resources%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/ys-zong/awesome-self-supervised-multimodal-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.01008v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Multimodal%20Learning%3A%20A%20Survey&entry.906535625=Yongshuo%20Zong%20and%20Oisin%20Mac%20Aodha%20and%20Timothy%20Hospedales&entry.1292438233=%20%20Multimodal%20learning%2C%20which%20aims%20to%20understand%20and%20analyze%20information%20from%0Amultiple%20modalities%2C%20has%20achieved%20substantial%20progress%20in%20the%20supervised%20regime%0Ain%20recent%20years.%20However%2C%20the%20heavy%20dependence%20on%20data%20paired%20with%20expensive%0Ahuman%20annotations%20impedes%20scaling%20up%20models.%20Meanwhile%2C%20given%20the%20availability%0Aof%20large-scale%20unannotated%20data%20in%20the%20wild%2C%20self-supervised%20learning%20has%0Abecome%20an%20attractive%20strategy%20to%20alleviate%20the%20annotation%20bottleneck.%20Building%0Aon%20these%20two%20directions%2C%20self-supervised%20multimodal%20learning%20%28SSML%29%20provides%0Aways%20to%20learn%20from%20raw%20multimodal%20data.%20In%20this%20survey%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20the%20state-of-the-art%20in%20SSML%2C%20in%20which%20we%20elucidate%0Athree%20major%20challenges%20intrinsic%20to%20self-supervised%20learning%20with%20multimodal%0Adata%3A%20%281%29%20learning%20representations%20from%20multimodal%20data%20without%20labels%2C%20%282%29%0Afusion%20of%20different%20modalities%2C%20and%20%283%29%20learning%20with%20unaligned%20data.%20We%20then%0Adetail%20existing%20solutions%20to%20these%20challenges.%20Specifically%2C%20we%20consider%20%281%29%0Aobjectives%20for%20learning%20from%20multimodal%20unlabeled%20data%20via%20self-supervision%2C%0A%282%29%20model%20architectures%20from%20the%20perspective%20of%20different%20multimodal%20fusion%0Astrategies%2C%20and%20%283%29%20pair-free%20learning%20strategies%20for%20coarse-grained%20and%0Afine-grained%20alignment.%20We%20also%20review%20real-world%20applications%20of%20SSML%0Aalgorithms%20in%20diverse%20fields%20such%20as%20healthcare%2C%20remote%20sensing%2C%20and%20machine%0Atranslation.%20Finally%2C%20we%20discuss%20challenges%20and%20future%20directions%20for%20SSML.%20A%0Acollection%20of%20related%20resources%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/ys-zong/awesome-self-supervised-multimodal-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.01008v3&entry.124074799=Read"},
{"title": "MonoForce: Self-supervised Learning of Physics-informed Model for\n  Predicting Robot-terrain Interaction", "author": "Ruslan Agishev and Karel Zimmermann and Vladim\u00edr Kubelka and Martin Pecka and Tom\u00e1\u0161 Svoboda", "abstract": "  While autonomous navigation of mobile robots on rigid terrain is a\nwell-explored problem, navigating on deformable terrain such as tall grass or\nbushes remains a challenge. To address it, we introduce an explainable,\nphysics-aware and end-to-end differentiable model which predicts the outcome of\nrobot-terrain interaction from camera images, both on rigid and non-rigid\nterrain. The proposed MonoForce model consists of a black-box module which\npredicts robot-terrain interaction forces from onboard cameras, followed by a\nwhite-box module, which transforms these forces and a control signals into\npredicted trajectories, using only the laws of classical mechanics. The\ndifferentiable white-box module allows backpropagating the predicted trajectory\nerrors into the black-box module, serving as a self-supervised loss that\nmeasures consistency between the predicted forces and ground-truth trajectories\nof the robot. Experimental evaluation on a public dataset and our data has\nshown that while the prediction capabilities are comparable to state-of-the-art\nalgorithms on rigid terrain, MonoForce shows superior accuracy on non-rigid\nterrain such as tall grass or bushes. To facilitate the reproducibility of our\nresults, we release both the code and datasets.\n", "link": "http://arxiv.org/abs/2309.09007v4", "date": "2024-08-16", "relevancy": 2.208, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6469}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5334}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoForce%3A%20Self-supervised%20Learning%20of%20Physics-informed%20Model%20for%0A%20%20Predicting%20Robot-terrain%20Interaction&body=Title%3A%20MonoForce%3A%20Self-supervised%20Learning%20of%20Physics-informed%20Model%20for%0A%20%20Predicting%20Robot-terrain%20Interaction%0AAuthor%3A%20Ruslan%20Agishev%20and%20Karel%20Zimmermann%20and%20Vladim%C3%ADr%20Kubelka%20and%20Martin%20Pecka%20and%20Tom%C3%A1%C5%A1%20Svoboda%0AAbstract%3A%20%20%20While%20autonomous%20navigation%20of%20mobile%20robots%20on%20rigid%20terrain%20is%20a%0Awell-explored%20problem%2C%20navigating%20on%20deformable%20terrain%20such%20as%20tall%20grass%20or%0Abushes%20remains%20a%20challenge.%20To%20address%20it%2C%20we%20introduce%20an%20explainable%2C%0Aphysics-aware%20and%20end-to-end%20differentiable%20model%20which%20predicts%20the%20outcome%20of%0Arobot-terrain%20interaction%20from%20camera%20images%2C%20both%20on%20rigid%20and%20non-rigid%0Aterrain.%20The%20proposed%20MonoForce%20model%20consists%20of%20a%20black-box%20module%20which%0Apredicts%20robot-terrain%20interaction%20forces%20from%20onboard%20cameras%2C%20followed%20by%20a%0Awhite-box%20module%2C%20which%20transforms%20these%20forces%20and%20a%20control%20signals%20into%0Apredicted%20trajectories%2C%20using%20only%20the%20laws%20of%20classical%20mechanics.%20The%0Adifferentiable%20white-box%20module%20allows%20backpropagating%20the%20predicted%20trajectory%0Aerrors%20into%20the%20black-box%20module%2C%20serving%20as%20a%20self-supervised%20loss%20that%0Ameasures%20consistency%20between%20the%20predicted%20forces%20and%20ground-truth%20trajectories%0Aof%20the%20robot.%20Experimental%20evaluation%20on%20a%20public%20dataset%20and%20our%20data%20has%0Ashown%20that%20while%20the%20prediction%20capabilities%20are%20comparable%20to%20state-of-the-art%0Aalgorithms%20on%20rigid%20terrain%2C%20MonoForce%20shows%20superior%20accuracy%20on%20non-rigid%0Aterrain%20such%20as%20tall%20grass%20or%20bushes.%20To%20facilitate%20the%20reproducibility%20of%20our%0Aresults%2C%20we%20release%20both%20the%20code%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09007v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoForce%253A%2520Self-supervised%2520Learning%2520of%2520Physics-informed%2520Model%2520for%250A%2520%2520Predicting%2520Robot-terrain%2520Interaction%26entry.906535625%3DRuslan%2520Agishev%2520and%2520Karel%2520Zimmermann%2520and%2520Vladim%25C3%25ADr%2520Kubelka%2520and%2520Martin%2520Pecka%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Svoboda%26entry.1292438233%3D%2520%2520While%2520autonomous%2520navigation%2520of%2520mobile%2520robots%2520on%2520rigid%2520terrain%2520is%2520a%250Awell-explored%2520problem%252C%2520navigating%2520on%2520deformable%2520terrain%2520such%2520as%2520tall%2520grass%2520or%250Abushes%2520remains%2520a%2520challenge.%2520To%2520address%2520it%252C%2520we%2520introduce%2520an%2520explainable%252C%250Aphysics-aware%2520and%2520end-to-end%2520differentiable%2520model%2520which%2520predicts%2520the%2520outcome%2520of%250Arobot-terrain%2520interaction%2520from%2520camera%2520images%252C%2520both%2520on%2520rigid%2520and%2520non-rigid%250Aterrain.%2520The%2520proposed%2520MonoForce%2520model%2520consists%2520of%2520a%2520black-box%2520module%2520which%250Apredicts%2520robot-terrain%2520interaction%2520forces%2520from%2520onboard%2520cameras%252C%2520followed%2520by%2520a%250Awhite-box%2520module%252C%2520which%2520transforms%2520these%2520forces%2520and%2520a%2520control%2520signals%2520into%250Apredicted%2520trajectories%252C%2520using%2520only%2520the%2520laws%2520of%2520classical%2520mechanics.%2520The%250Adifferentiable%2520white-box%2520module%2520allows%2520backpropagating%2520the%2520predicted%2520trajectory%250Aerrors%2520into%2520the%2520black-box%2520module%252C%2520serving%2520as%2520a%2520self-supervised%2520loss%2520that%250Ameasures%2520consistency%2520between%2520the%2520predicted%2520forces%2520and%2520ground-truth%2520trajectories%250Aof%2520the%2520robot.%2520Experimental%2520evaluation%2520on%2520a%2520public%2520dataset%2520and%2520our%2520data%2520has%250Ashown%2520that%2520while%2520the%2520prediction%2520capabilities%2520are%2520comparable%2520to%2520state-of-the-art%250Aalgorithms%2520on%2520rigid%2520terrain%252C%2520MonoForce%2520shows%2520superior%2520accuracy%2520on%2520non-rigid%250Aterrain%2520such%2520as%2520tall%2520grass%2520or%2520bushes.%2520To%2520facilitate%2520the%2520reproducibility%2520of%2520our%250Aresults%252C%2520we%2520release%2520both%2520the%2520code%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09007v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoForce%3A%20Self-supervised%20Learning%20of%20Physics-informed%20Model%20for%0A%20%20Predicting%20Robot-terrain%20Interaction&entry.906535625=Ruslan%20Agishev%20and%20Karel%20Zimmermann%20and%20Vladim%C3%ADr%20Kubelka%20and%20Martin%20Pecka%20and%20Tom%C3%A1%C5%A1%20Svoboda&entry.1292438233=%20%20While%20autonomous%20navigation%20of%20mobile%20robots%20on%20rigid%20terrain%20is%20a%0Awell-explored%20problem%2C%20navigating%20on%20deformable%20terrain%20such%20as%20tall%20grass%20or%0Abushes%20remains%20a%20challenge.%20To%20address%20it%2C%20we%20introduce%20an%20explainable%2C%0Aphysics-aware%20and%20end-to-end%20differentiable%20model%20which%20predicts%20the%20outcome%20of%0Arobot-terrain%20interaction%20from%20camera%20images%2C%20both%20on%20rigid%20and%20non-rigid%0Aterrain.%20The%20proposed%20MonoForce%20model%20consists%20of%20a%20black-box%20module%20which%0Apredicts%20robot-terrain%20interaction%20forces%20from%20onboard%20cameras%2C%20followed%20by%20a%0Awhite-box%20module%2C%20which%20transforms%20these%20forces%20and%20a%20control%20signals%20into%0Apredicted%20trajectories%2C%20using%20only%20the%20laws%20of%20classical%20mechanics.%20The%0Adifferentiable%20white-box%20module%20allows%20backpropagating%20the%20predicted%20trajectory%0Aerrors%20into%20the%20black-box%20module%2C%20serving%20as%20a%20self-supervised%20loss%20that%0Ameasures%20consistency%20between%20the%20predicted%20forces%20and%20ground-truth%20trajectories%0Aof%20the%20robot.%20Experimental%20evaluation%20on%20a%20public%20dataset%20and%20our%20data%20has%0Ashown%20that%20while%20the%20prediction%20capabilities%20are%20comparable%20to%20state-of-the-art%0Aalgorithms%20on%20rigid%20terrain%2C%20MonoForce%20shows%20superior%20accuracy%20on%20non-rigid%0Aterrain%20such%20as%20tall%20grass%20or%20bushes.%20To%20facilitate%20the%20reproducibility%20of%20our%0Aresults%2C%20we%20release%20both%20the%20code%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09007v4&entry.124074799=Read"},
{"title": "SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and\n  Medical Image Segmentation", "author": "Xinyu Xiong and Zihuang Wu and Shuangyi Tan and Wenxue Li and Feilong Tang and Ying Chen and Siying Li and Jie Ma and Guanbin Li", "abstract": "  Image segmentation plays an important role in vision understanding. Recently,\nthe emerging vision foundation models continuously achieved superior\nperformance on various tasks. Following such success, in this paper, we prove\nthat the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped\nsegmentation models. We propose a simple but effective framework, termed\nSAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the\nHiera backbone of SAM2 as the encoder, while the decoder uses the classic\nU-shaped design. Additionally, adapters are inserted into the encoder to allow\nparameter-efficient fine-tuning. Preliminary experiments on various downstream\ntasks, such as camouflaged object detection, salient object detection, marine\nanimal segmentation, mirror detection, and polyp segmentation, demonstrate that\nour SAM2-UNet can simply beat existing specialized state-of-the-art methods\nwithout bells and whistles. Project page:\n\\url{https://github.com/WZH0120/SAM2-UNet}.\n", "link": "http://arxiv.org/abs/2408.08870v1", "date": "2024-08-16", "relevancy": 2.1869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.543}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2-UNet%3A%20Segment%20Anything%202%20Makes%20Strong%20Encoder%20for%20Natural%20and%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20SAM2-UNet%3A%20Segment%20Anything%202%20Makes%20Strong%20Encoder%20for%20Natural%20and%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Xinyu%20Xiong%20and%20Zihuang%20Wu%20and%20Shuangyi%20Tan%20and%20Wenxue%20Li%20and%20Feilong%20Tang%20and%20Ying%20Chen%20and%20Siying%20Li%20and%20Jie%20Ma%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Image%20segmentation%20plays%20an%20important%20role%20in%20vision%20understanding.%20Recently%2C%0Athe%20emerging%20vision%20foundation%20models%20continuously%20achieved%20superior%0Aperformance%20on%20various%20tasks.%20Following%20such%20success%2C%20in%20this%20paper%2C%20we%20prove%0Athat%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20can%20be%20a%20strong%20encoder%20for%20U-shaped%0Asegmentation%20models.%20We%20propose%20a%20simple%20but%20effective%20framework%2C%20termed%0ASAM2-UNet%2C%20for%20versatile%20image%20segmentation.%20Specifically%2C%20SAM2-UNet%20adopts%20the%0AHiera%20backbone%20of%20SAM2%20as%20the%20encoder%2C%20while%20the%20decoder%20uses%20the%20classic%0AU-shaped%20design.%20Additionally%2C%20adapters%20are%20inserted%20into%20the%20encoder%20to%20allow%0Aparameter-efficient%20fine-tuning.%20Preliminary%20experiments%20on%20various%20downstream%0Atasks%2C%20such%20as%20camouflaged%20object%20detection%2C%20salient%20object%20detection%2C%20marine%0Aanimal%20segmentation%2C%20mirror%20detection%2C%20and%20polyp%20segmentation%2C%20demonstrate%20that%0Aour%20SAM2-UNet%20can%20simply%20beat%20existing%20specialized%20state-of-the-art%20methods%0Awithout%20bells%20and%20whistles.%20Project%20page%3A%0A%5Curl%7Bhttps%3A//github.com/WZH0120/SAM2-UNet%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2-UNet%253A%2520Segment%2520Anything%25202%2520Makes%2520Strong%2520Encoder%2520for%2520Natural%2520and%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DXinyu%2520Xiong%2520and%2520Zihuang%2520Wu%2520and%2520Shuangyi%2520Tan%2520and%2520Wenxue%2520Li%2520and%2520Feilong%2520Tang%2520and%2520Ying%2520Chen%2520and%2520Siying%2520Li%2520and%2520Jie%2520Ma%2520and%2520Guanbin%2520Li%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520plays%2520an%2520important%2520role%2520in%2520vision%2520understanding.%2520Recently%252C%250Athe%2520emerging%2520vision%2520foundation%2520models%2520continuously%2520achieved%2520superior%250Aperformance%2520on%2520various%2520tasks.%2520Following%2520such%2520success%252C%2520in%2520this%2520paper%252C%2520we%2520prove%250Athat%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520can%2520be%2520a%2520strong%2520encoder%2520for%2520U-shaped%250Asegmentation%2520models.%2520We%2520propose%2520a%2520simple%2520but%2520effective%2520framework%252C%2520termed%250ASAM2-UNet%252C%2520for%2520versatile%2520image%2520segmentation.%2520Specifically%252C%2520SAM2-UNet%2520adopts%2520the%250AHiera%2520backbone%2520of%2520SAM2%2520as%2520the%2520encoder%252C%2520while%2520the%2520decoder%2520uses%2520the%2520classic%250AU-shaped%2520design.%2520Additionally%252C%2520adapters%2520are%2520inserted%2520into%2520the%2520encoder%2520to%2520allow%250Aparameter-efficient%2520fine-tuning.%2520Preliminary%2520experiments%2520on%2520various%2520downstream%250Atasks%252C%2520such%2520as%2520camouflaged%2520object%2520detection%252C%2520salient%2520object%2520detection%252C%2520marine%250Aanimal%2520segmentation%252C%2520mirror%2520detection%252C%2520and%2520polyp%2520segmentation%252C%2520demonstrate%2520that%250Aour%2520SAM2-UNet%2520can%2520simply%2520beat%2520existing%2520specialized%2520state-of-the-art%2520methods%250Awithout%2520bells%2520and%2520whistles.%2520Project%2520page%253A%250A%255Curl%257Bhttps%253A//github.com/WZH0120/SAM2-UNet%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2-UNet%3A%20Segment%20Anything%202%20Makes%20Strong%20Encoder%20for%20Natural%20and%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Xinyu%20Xiong%20and%20Zihuang%20Wu%20and%20Shuangyi%20Tan%20and%20Wenxue%20Li%20and%20Feilong%20Tang%20and%20Ying%20Chen%20and%20Siying%20Li%20and%20Jie%20Ma%20and%20Guanbin%20Li&entry.1292438233=%20%20Image%20segmentation%20plays%20an%20important%20role%20in%20vision%20understanding.%20Recently%2C%0Athe%20emerging%20vision%20foundation%20models%20continuously%20achieved%20superior%0Aperformance%20on%20various%20tasks.%20Following%20such%20success%2C%20in%20this%20paper%2C%20we%20prove%0Athat%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20can%20be%20a%20strong%20encoder%20for%20U-shaped%0Asegmentation%20models.%20We%20propose%20a%20simple%20but%20effective%20framework%2C%20termed%0ASAM2-UNet%2C%20for%20versatile%20image%20segmentation.%20Specifically%2C%20SAM2-UNet%20adopts%20the%0AHiera%20backbone%20of%20SAM2%20as%20the%20encoder%2C%20while%20the%20decoder%20uses%20the%20classic%0AU-shaped%20design.%20Additionally%2C%20adapters%20are%20inserted%20into%20the%20encoder%20to%20allow%0Aparameter-efficient%20fine-tuning.%20Preliminary%20experiments%20on%20various%20downstream%0Atasks%2C%20such%20as%20camouflaged%20object%20detection%2C%20salient%20object%20detection%2C%20marine%0Aanimal%20segmentation%2C%20mirror%20detection%2C%20and%20polyp%20segmentation%2C%20demonstrate%20that%0Aour%20SAM2-UNet%20can%20simply%20beat%20existing%20specialized%20state-of-the-art%20methods%0Awithout%20bells%20and%20whistles.%20Project%20page%3A%0A%5Curl%7Bhttps%3A//github.com/WZH0120/SAM2-UNet%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08870v1&entry.124074799=Read"},
{"title": "Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning", "author": "Alessio Devoto and Federico Alvetreti and Jary Pomponi and Paolo Di Lorenzo and Pasquale Minervini and Simone Scardapane", "abstract": "  Recently, foundation models based on Vision Transformers (ViTs) have become\nwidely available. However, their fine-tuning process is highly\nresource-intensive, and it hinders their adoption in several edge or low-energy\napplications. To this end, in this paper we introduce an efficient fine-tuning\nmethod for ViTs called $\\textbf{ALaST}$ ($\\textit{Adaptive Layer Selection\nFine-Tuning for Vision Transformers}$) to speed up the fine-tuning process\nwhile reducing computational cost, memory load, and training time. Our approach\nis based on the observation that not all layers are equally critical during\nfine-tuning, and their importance varies depending on the current mini-batch.\nTherefore, at each fine-tuning step, we adaptively estimate the importance of\nall layers and we assign what we call ``compute budgets'' accordingly. Layers\nthat were allocated lower budgets are either trained with a reduced number of\ninput tokens or kept frozen. Freezing a layer reduces the computational cost\nand memory usage by preventing updates to its weights, while discarding tokens\nremoves redundant data, speeding up processing and reducing memory\nrequirements. We show that this adaptive compute allocation enables a\nnearly-optimal schedule for distributing computational resources across layers,\nresulting in substantial reductions in training time (up to 1.5x), FLOPs (up to\n2x), and memory load (up to 2x) compared to traditional full fine-tuning\napproaches. Additionally, it can be successfully combined with other\nparameter-efficient fine-tuning methods, such as LoRA.\n", "link": "http://arxiv.org/abs/2408.08670v1", "date": "2024-08-16", "relevancy": 2.1462, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5548}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5483}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Layer%20Selection%20for%20Efficient%20Vision%20Transformer%20Fine-Tuning&body=Title%3A%20Adaptive%20Layer%20Selection%20for%20Efficient%20Vision%20Transformer%20Fine-Tuning%0AAuthor%3A%20Alessio%20Devoto%20and%20Federico%20Alvetreti%20and%20Jary%20Pomponi%20and%20Paolo%20Di%20Lorenzo%20and%20Pasquale%20Minervini%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20Recently%2C%20foundation%20models%20based%20on%20Vision%20Transformers%20%28ViTs%29%20have%20become%0Awidely%20available.%20However%2C%20their%20fine-tuning%20process%20is%20highly%0Aresource-intensive%2C%20and%20it%20hinders%20their%20adoption%20in%20several%20edge%20or%20low-energy%0Aapplications.%20To%20this%20end%2C%20in%20this%20paper%20we%20introduce%20an%20efficient%20fine-tuning%0Amethod%20for%20ViTs%20called%20%24%5Ctextbf%7BALaST%7D%24%20%28%24%5Ctextit%7BAdaptive%20Layer%20Selection%0AFine-Tuning%20for%20Vision%20Transformers%7D%24%29%20to%20speed%20up%20the%20fine-tuning%20process%0Awhile%20reducing%20computational%20cost%2C%20memory%20load%2C%20and%20training%20time.%20Our%20approach%0Ais%20based%20on%20the%20observation%20that%20not%20all%20layers%20are%20equally%20critical%20during%0Afine-tuning%2C%20and%20their%20importance%20varies%20depending%20on%20the%20current%20mini-batch.%0ATherefore%2C%20at%20each%20fine-tuning%20step%2C%20we%20adaptively%20estimate%20the%20importance%20of%0Aall%20layers%20and%20we%20assign%20what%20we%20call%20%60%60compute%20budgets%27%27%20accordingly.%20Layers%0Athat%20were%20allocated%20lower%20budgets%20are%20either%20trained%20with%20a%20reduced%20number%20of%0Ainput%20tokens%20or%20kept%20frozen.%20Freezing%20a%20layer%20reduces%20the%20computational%20cost%0Aand%20memory%20usage%20by%20preventing%20updates%20to%20its%20weights%2C%20while%20discarding%20tokens%0Aremoves%20redundant%20data%2C%20speeding%20up%20processing%20and%20reducing%20memory%0Arequirements.%20We%20show%20that%20this%20adaptive%20compute%20allocation%20enables%20a%0Anearly-optimal%20schedule%20for%20distributing%20computational%20resources%20across%20layers%2C%0Aresulting%20in%20substantial%20reductions%20in%20training%20time%20%28up%20to%201.5x%29%2C%20FLOPs%20%28up%20to%0A2x%29%2C%20and%20memory%20load%20%28up%20to%202x%29%20compared%20to%20traditional%20full%20fine-tuning%0Aapproaches.%20Additionally%2C%20it%20can%20be%20successfully%20combined%20with%20other%0Aparameter-efficient%20fine-tuning%20methods%2C%20such%20as%20LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Layer%2520Selection%2520for%2520Efficient%2520Vision%2520Transformer%2520Fine-Tuning%26entry.906535625%3DAlessio%2520Devoto%2520and%2520Federico%2520Alvetreti%2520and%2520Jary%2520Pomponi%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Pasquale%2520Minervini%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520Recently%252C%2520foundation%2520models%2520based%2520on%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520become%250Awidely%2520available.%2520However%252C%2520their%2520fine-tuning%2520process%2520is%2520highly%250Aresource-intensive%252C%2520and%2520it%2520hinders%2520their%2520adoption%2520in%2520several%2520edge%2520or%2520low-energy%250Aapplications.%2520To%2520this%2520end%252C%2520in%2520this%2520paper%2520we%2520introduce%2520an%2520efficient%2520fine-tuning%250Amethod%2520for%2520ViTs%2520called%2520%2524%255Ctextbf%257BALaST%257D%2524%2520%2528%2524%255Ctextit%257BAdaptive%2520Layer%2520Selection%250AFine-Tuning%2520for%2520Vision%2520Transformers%257D%2524%2529%2520to%2520speed%2520up%2520the%2520fine-tuning%2520process%250Awhile%2520reducing%2520computational%2520cost%252C%2520memory%2520load%252C%2520and%2520training%2520time.%2520Our%2520approach%250Ais%2520based%2520on%2520the%2520observation%2520that%2520not%2520all%2520layers%2520are%2520equally%2520critical%2520during%250Afine-tuning%252C%2520and%2520their%2520importance%2520varies%2520depending%2520on%2520the%2520current%2520mini-batch.%250ATherefore%252C%2520at%2520each%2520fine-tuning%2520step%252C%2520we%2520adaptively%2520estimate%2520the%2520importance%2520of%250Aall%2520layers%2520and%2520we%2520assign%2520what%2520we%2520call%2520%2560%2560compute%2520budgets%2527%2527%2520accordingly.%2520Layers%250Athat%2520were%2520allocated%2520lower%2520budgets%2520are%2520either%2520trained%2520with%2520a%2520reduced%2520number%2520of%250Ainput%2520tokens%2520or%2520kept%2520frozen.%2520Freezing%2520a%2520layer%2520reduces%2520the%2520computational%2520cost%250Aand%2520memory%2520usage%2520by%2520preventing%2520updates%2520to%2520its%2520weights%252C%2520while%2520discarding%2520tokens%250Aremoves%2520redundant%2520data%252C%2520speeding%2520up%2520processing%2520and%2520reducing%2520memory%250Arequirements.%2520We%2520show%2520that%2520this%2520adaptive%2520compute%2520allocation%2520enables%2520a%250Anearly-optimal%2520schedule%2520for%2520distributing%2520computational%2520resources%2520across%2520layers%252C%250Aresulting%2520in%2520substantial%2520reductions%2520in%2520training%2520time%2520%2528up%2520to%25201.5x%2529%252C%2520FLOPs%2520%2528up%2520to%250A2x%2529%252C%2520and%2520memory%2520load%2520%2528up%2520to%25202x%2529%2520compared%2520to%2520traditional%2520full%2520fine-tuning%250Aapproaches.%2520Additionally%252C%2520it%2520can%2520be%2520successfully%2520combined%2520with%2520other%250Aparameter-efficient%2520fine-tuning%2520methods%252C%2520such%2520as%2520LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Layer%20Selection%20for%20Efficient%20Vision%20Transformer%20Fine-Tuning&entry.906535625=Alessio%20Devoto%20and%20Federico%20Alvetreti%20and%20Jary%20Pomponi%20and%20Paolo%20Di%20Lorenzo%20and%20Pasquale%20Minervini%20and%20Simone%20Scardapane&entry.1292438233=%20%20Recently%2C%20foundation%20models%20based%20on%20Vision%20Transformers%20%28ViTs%29%20have%20become%0Awidely%20available.%20However%2C%20their%20fine-tuning%20process%20is%20highly%0Aresource-intensive%2C%20and%20it%20hinders%20their%20adoption%20in%20several%20edge%20or%20low-energy%0Aapplications.%20To%20this%20end%2C%20in%20this%20paper%20we%20introduce%20an%20efficient%20fine-tuning%0Amethod%20for%20ViTs%20called%20%24%5Ctextbf%7BALaST%7D%24%20%28%24%5Ctextit%7BAdaptive%20Layer%20Selection%0AFine-Tuning%20for%20Vision%20Transformers%7D%24%29%20to%20speed%20up%20the%20fine-tuning%20process%0Awhile%20reducing%20computational%20cost%2C%20memory%20load%2C%20and%20training%20time.%20Our%20approach%0Ais%20based%20on%20the%20observation%20that%20not%20all%20layers%20are%20equally%20critical%20during%0Afine-tuning%2C%20and%20their%20importance%20varies%20depending%20on%20the%20current%20mini-batch.%0ATherefore%2C%20at%20each%20fine-tuning%20step%2C%20we%20adaptively%20estimate%20the%20importance%20of%0Aall%20layers%20and%20we%20assign%20what%20we%20call%20%60%60compute%20budgets%27%27%20accordingly.%20Layers%0Athat%20were%20allocated%20lower%20budgets%20are%20either%20trained%20with%20a%20reduced%20number%20of%0Ainput%20tokens%20or%20kept%20frozen.%20Freezing%20a%20layer%20reduces%20the%20computational%20cost%0Aand%20memory%20usage%20by%20preventing%20updates%20to%20its%20weights%2C%20while%20discarding%20tokens%0Aremoves%20redundant%20data%2C%20speeding%20up%20processing%20and%20reducing%20memory%0Arequirements.%20We%20show%20that%20this%20adaptive%20compute%20allocation%20enables%20a%0Anearly-optimal%20schedule%20for%20distributing%20computational%20resources%20across%20layers%2C%0Aresulting%20in%20substantial%20reductions%20in%20training%20time%20%28up%20to%201.5x%29%2C%20FLOPs%20%28up%20to%0A2x%29%2C%20and%20memory%20load%20%28up%20to%202x%29%20compared%20to%20traditional%20full%20fine-tuning%0Aapproaches.%20Additionally%2C%20it%20can%20be%20successfully%20combined%20with%20other%0Aparameter-efficient%20fine-tuning%20methods%2C%20such%20as%20LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08670v1&entry.124074799=Read"},
{"title": "RGBT Tracking via All-layer Multimodal Interactions with Progressive\n  Fusion Mamba", "author": "Andong Lu and Wanyu Wang and Chenglong Li and Jin Tang and Bin Luo", "abstract": "  Existing RGBT tracking methods often design various interaction models to\nperform cross-modal fusion of each layer, but can not execute the feature\ninteractions among all layers, which plays a critical role in robust multimodal\nrepresentation, due to large computational burden. To address this issue, this\npaper presents a novel All-layer multimodal Interaction Network, named AINet,\nwhich performs efficient and effective feature interactions of all modalities\nand layers in a progressive fusion Mamba, for robust RGBT tracking. Even though\nmodality features in different layers are known to contain different cues, it\nis always challenging to build multimodal interactions in each layer due to\nstruggling in balancing interaction capabilities and efficiency. Meanwhile,\nconsidering that the feature discrepancy between RGB and thermal modalities\nreflects their complementary information to some extent, we design a\nDifference-based Fusion Mamba (DFM) to achieve enhanced fusion of different\nmodalities with linear complexity. When interacting with features from all\nlayers, a huge number of token sequences (3840 tokens in this work) are\ninvolved and the computational burden is thus large. To handle this problem, we\ndesign an Order-dynamic Fusion Mamba (OFM) to execute efficient and effective\nfeature interactions of all layers by dynamically adjusting the scan order of\ndifferent layers in Mamba. Extensive experiments on four public RGBT tracking\ndatasets show that AINet achieves leading performance against existing\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.08827v1", "date": "2024-08-16", "relevancy": 2.1362, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGBT%20Tracking%20via%20All-layer%20Multimodal%20Interactions%20with%20Progressive%0A%20%20Fusion%20Mamba&body=Title%3A%20RGBT%20Tracking%20via%20All-layer%20Multimodal%20Interactions%20with%20Progressive%0A%20%20Fusion%20Mamba%0AAuthor%3A%20Andong%20Lu%20and%20Wanyu%20Wang%20and%20Chenglong%20Li%20and%20Jin%20Tang%20and%20Bin%20Luo%0AAbstract%3A%20%20%20Existing%20RGBT%20tracking%20methods%20often%20design%20various%20interaction%20models%20to%0Aperform%20cross-modal%20fusion%20of%20each%20layer%2C%20but%20can%20not%20execute%20the%20feature%0Ainteractions%20among%20all%20layers%2C%20which%20plays%20a%20critical%20role%20in%20robust%20multimodal%0Arepresentation%2C%20due%20to%20large%20computational%20burden.%20To%20address%20this%20issue%2C%20this%0Apaper%20presents%20a%20novel%20All-layer%20multimodal%20Interaction%20Network%2C%20named%20AINet%2C%0Awhich%20performs%20efficient%20and%20effective%20feature%20interactions%20of%20all%20modalities%0Aand%20layers%20in%20a%20progressive%20fusion%20Mamba%2C%20for%20robust%20RGBT%20tracking.%20Even%20though%0Amodality%20features%20in%20different%20layers%20are%20known%20to%20contain%20different%20cues%2C%20it%0Ais%20always%20challenging%20to%20build%20multimodal%20interactions%20in%20each%20layer%20due%20to%0Astruggling%20in%20balancing%20interaction%20capabilities%20and%20efficiency.%20Meanwhile%2C%0Aconsidering%20that%20the%20feature%20discrepancy%20between%20RGB%20and%20thermal%20modalities%0Areflects%20their%20complementary%20information%20to%20some%20extent%2C%20we%20design%20a%0ADifference-based%20Fusion%20Mamba%20%28DFM%29%20to%20achieve%20enhanced%20fusion%20of%20different%0Amodalities%20with%20linear%20complexity.%20When%20interacting%20with%20features%20from%20all%0Alayers%2C%20a%20huge%20number%20of%20token%20sequences%20%283840%20tokens%20in%20this%20work%29%20are%0Ainvolved%20and%20the%20computational%20burden%20is%20thus%20large.%20To%20handle%20this%20problem%2C%20we%0Adesign%20an%20Order-dynamic%20Fusion%20Mamba%20%28OFM%29%20to%20execute%20efficient%20and%20effective%0Afeature%20interactions%20of%20all%20layers%20by%20dynamically%20adjusting%20the%20scan%20order%20of%0Adifferent%20layers%20in%20Mamba.%20Extensive%20experiments%20on%20four%20public%20RGBT%20tracking%0Adatasets%20show%20that%20AINet%20achieves%20leading%20performance%20against%20existing%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGBT%2520Tracking%2520via%2520All-layer%2520Multimodal%2520Interactions%2520with%2520Progressive%250A%2520%2520Fusion%2520Mamba%26entry.906535625%3DAndong%2520Lu%2520and%2520Wanyu%2520Wang%2520and%2520Chenglong%2520Li%2520and%2520Jin%2520Tang%2520and%2520Bin%2520Luo%26entry.1292438233%3D%2520%2520Existing%2520RGBT%2520tracking%2520methods%2520often%2520design%2520various%2520interaction%2520models%2520to%250Aperform%2520cross-modal%2520fusion%2520of%2520each%2520layer%252C%2520but%2520can%2520not%2520execute%2520the%2520feature%250Ainteractions%2520among%2520all%2520layers%252C%2520which%2520plays%2520a%2520critical%2520role%2520in%2520robust%2520multimodal%250Arepresentation%252C%2520due%2520to%2520large%2520computational%2520burden.%2520To%2520address%2520this%2520issue%252C%2520this%250Apaper%2520presents%2520a%2520novel%2520All-layer%2520multimodal%2520Interaction%2520Network%252C%2520named%2520AINet%252C%250Awhich%2520performs%2520efficient%2520and%2520effective%2520feature%2520interactions%2520of%2520all%2520modalities%250Aand%2520layers%2520in%2520a%2520progressive%2520fusion%2520Mamba%252C%2520for%2520robust%2520RGBT%2520tracking.%2520Even%2520though%250Amodality%2520features%2520in%2520different%2520layers%2520are%2520known%2520to%2520contain%2520different%2520cues%252C%2520it%250Ais%2520always%2520challenging%2520to%2520build%2520multimodal%2520interactions%2520in%2520each%2520layer%2520due%2520to%250Astruggling%2520in%2520balancing%2520interaction%2520capabilities%2520and%2520efficiency.%2520Meanwhile%252C%250Aconsidering%2520that%2520the%2520feature%2520discrepancy%2520between%2520RGB%2520and%2520thermal%2520modalities%250Areflects%2520their%2520complementary%2520information%2520to%2520some%2520extent%252C%2520we%2520design%2520a%250ADifference-based%2520Fusion%2520Mamba%2520%2528DFM%2529%2520to%2520achieve%2520enhanced%2520fusion%2520of%2520different%250Amodalities%2520with%2520linear%2520complexity.%2520When%2520interacting%2520with%2520features%2520from%2520all%250Alayers%252C%2520a%2520huge%2520number%2520of%2520token%2520sequences%2520%25283840%2520tokens%2520in%2520this%2520work%2529%2520are%250Ainvolved%2520and%2520the%2520computational%2520burden%2520is%2520thus%2520large.%2520To%2520handle%2520this%2520problem%252C%2520we%250Adesign%2520an%2520Order-dynamic%2520Fusion%2520Mamba%2520%2528OFM%2529%2520to%2520execute%2520efficient%2520and%2520effective%250Afeature%2520interactions%2520of%2520all%2520layers%2520by%2520dynamically%2520adjusting%2520the%2520scan%2520order%2520of%250Adifferent%2520layers%2520in%2520Mamba.%2520Extensive%2520experiments%2520on%2520four%2520public%2520RGBT%2520tracking%250Adatasets%2520show%2520that%2520AINet%2520achieves%2520leading%2520performance%2520against%2520existing%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGBT%20Tracking%20via%20All-layer%20Multimodal%20Interactions%20with%20Progressive%0A%20%20Fusion%20Mamba&entry.906535625=Andong%20Lu%20and%20Wanyu%20Wang%20and%20Chenglong%20Li%20and%20Jin%20Tang%20and%20Bin%20Luo&entry.1292438233=%20%20Existing%20RGBT%20tracking%20methods%20often%20design%20various%20interaction%20models%20to%0Aperform%20cross-modal%20fusion%20of%20each%20layer%2C%20but%20can%20not%20execute%20the%20feature%0Ainteractions%20among%20all%20layers%2C%20which%20plays%20a%20critical%20role%20in%20robust%20multimodal%0Arepresentation%2C%20due%20to%20large%20computational%20burden.%20To%20address%20this%20issue%2C%20this%0Apaper%20presents%20a%20novel%20All-layer%20multimodal%20Interaction%20Network%2C%20named%20AINet%2C%0Awhich%20performs%20efficient%20and%20effective%20feature%20interactions%20of%20all%20modalities%0Aand%20layers%20in%20a%20progressive%20fusion%20Mamba%2C%20for%20robust%20RGBT%20tracking.%20Even%20though%0Amodality%20features%20in%20different%20layers%20are%20known%20to%20contain%20different%20cues%2C%20it%0Ais%20always%20challenging%20to%20build%20multimodal%20interactions%20in%20each%20layer%20due%20to%0Astruggling%20in%20balancing%20interaction%20capabilities%20and%20efficiency.%20Meanwhile%2C%0Aconsidering%20that%20the%20feature%20discrepancy%20between%20RGB%20and%20thermal%20modalities%0Areflects%20their%20complementary%20information%20to%20some%20extent%2C%20we%20design%20a%0ADifference-based%20Fusion%20Mamba%20%28DFM%29%20to%20achieve%20enhanced%20fusion%20of%20different%0Amodalities%20with%20linear%20complexity.%20When%20interacting%20with%20features%20from%20all%0Alayers%2C%20a%20huge%20number%20of%20token%20sequences%20%283840%20tokens%20in%20this%20work%29%20are%0Ainvolved%20and%20the%20computational%20burden%20is%20thus%20large.%20To%20handle%20this%20problem%2C%20we%0Adesign%20an%20Order-dynamic%20Fusion%20Mamba%20%28OFM%29%20to%20execute%20efficient%20and%20effective%0Afeature%20interactions%20of%20all%20layers%20by%20dynamically%20adjusting%20the%20scan%20order%20of%0Adifferent%20layers%20in%20Mamba.%20Extensive%20experiments%20on%20four%20public%20RGBT%20tracking%0Adatasets%20show%20that%20AINet%20achieves%20leading%20performance%20against%20existing%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08827v1&entry.124074799=Read"},
{"title": "CeCNN: Copula-enhanced convolutional neural networks in joint prediction\n  of refraction error and axial length based on ultra-widefield fundus images", "author": "Chong Zhong and Yang Li and Danjuan Yang and Meiyan Li and Xingyao Zhou and Bo Fu and Catherine C. Liu and A. H. Welsh", "abstract": "  The ultra-widefield (UWF) fundus image is an attractive 3D biomarker in\nAI-aided myopia screening because it provides much richer myopia-related\ninformation. Though axial length (AL) has been acknowledged to be highly\nrelated to the two key targets of myopia screening, Spherical Equivalence (SE)\nmeasurement and high myopia diagnosis, its prediction based on the UWF fundus\nimage is rarely considered. To save the high expense and time costs of\nmeasuring SE and AL, we propose the Copula-enhanced Convolutional Neural\nNetwork (CeCNN), a one-stop UWF-based ophthalmic AI framework to jointly\npredict SE, AL, and myopia status. The CeCNN formulates a multiresponse\nregression that relates multiple dependent discrete-continuous responses and\nthe image covariate, where the nonlinearity of the association is modeled by a\nbackbone CNN. To thoroughly describe the dependence structure among the\nresponses, we model and incorporate the conditional dependence among responses\nin a CNN through a new copula-likelihood loss. We provide statistical\ninterpretations of the conditional dependence among responses, and reveal that\nsuch dependence is beyond the dependence explained by the image covariate. We\nheuristically justify that the proposed loss can enhance the estimation\nefficiency of the CNN weights. We apply the CeCNN to the UWF dataset collected\nby us and demonstrate that the CeCNN sharply enhances the predictive capability\nof various backbone CNNs. Our study evidences the ophthalmology view that\nbesides SE, AL is also an important measure to myopia.\n", "link": "http://arxiv.org/abs/2311.03967v4", "date": "2024-08-16", "relevancy": 2.1294, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CeCNN%3A%20Copula-enhanced%20convolutional%20neural%20networks%20in%20joint%20prediction%0A%20%20of%20refraction%20error%20and%20axial%20length%20based%20on%20ultra-widefield%20fundus%20images&body=Title%3A%20CeCNN%3A%20Copula-enhanced%20convolutional%20neural%20networks%20in%20joint%20prediction%0A%20%20of%20refraction%20error%20and%20axial%20length%20based%20on%20ultra-widefield%20fundus%20images%0AAuthor%3A%20Chong%20Zhong%20and%20Yang%20Li%20and%20Danjuan%20Yang%20and%20Meiyan%20Li%20and%20Xingyao%20Zhou%20and%20Bo%20Fu%20and%20Catherine%20C.%20Liu%20and%20A.%20H.%20Welsh%0AAbstract%3A%20%20%20The%20ultra-widefield%20%28UWF%29%20fundus%20image%20is%20an%20attractive%203D%20biomarker%20in%0AAI-aided%20myopia%20screening%20because%20it%20provides%20much%20richer%20myopia-related%0Ainformation.%20Though%20axial%20length%20%28AL%29%20has%20been%20acknowledged%20to%20be%20highly%0Arelated%20to%20the%20two%20key%20targets%20of%20myopia%20screening%2C%20Spherical%20Equivalence%20%28SE%29%0Ameasurement%20and%20high%20myopia%20diagnosis%2C%20its%20prediction%20based%20on%20the%20UWF%20fundus%0Aimage%20is%20rarely%20considered.%20To%20save%20the%20high%20expense%20and%20time%20costs%20of%0Ameasuring%20SE%20and%20AL%2C%20we%20propose%20the%20Copula-enhanced%20Convolutional%20Neural%0ANetwork%20%28CeCNN%29%2C%20a%20one-stop%20UWF-based%20ophthalmic%20AI%20framework%20to%20jointly%0Apredict%20SE%2C%20AL%2C%20and%20myopia%20status.%20The%20CeCNN%20formulates%20a%20multiresponse%0Aregression%20that%20relates%20multiple%20dependent%20discrete-continuous%20responses%20and%0Athe%20image%20covariate%2C%20where%20the%20nonlinearity%20of%20the%20association%20is%20modeled%20by%20a%0Abackbone%20CNN.%20To%20thoroughly%20describe%20the%20dependence%20structure%20among%20the%0Aresponses%2C%20we%20model%20and%20incorporate%20the%20conditional%20dependence%20among%20responses%0Ain%20a%20CNN%20through%20a%20new%20copula-likelihood%20loss.%20We%20provide%20statistical%0Ainterpretations%20of%20the%20conditional%20dependence%20among%20responses%2C%20and%20reveal%20that%0Asuch%20dependence%20is%20beyond%20the%20dependence%20explained%20by%20the%20image%20covariate.%20We%0Aheuristically%20justify%20that%20the%20proposed%20loss%20can%20enhance%20the%20estimation%0Aefficiency%20of%20the%20CNN%20weights.%20We%20apply%20the%20CeCNN%20to%20the%20UWF%20dataset%20collected%0Aby%20us%20and%20demonstrate%20that%20the%20CeCNN%20sharply%20enhances%20the%20predictive%20capability%0Aof%20various%20backbone%20CNNs.%20Our%20study%20evidences%20the%20ophthalmology%20view%20that%0Abesides%20SE%2C%20AL%20is%20also%20an%20important%20measure%20to%20myopia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03967v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCeCNN%253A%2520Copula-enhanced%2520convolutional%2520neural%2520networks%2520in%2520joint%2520prediction%250A%2520%2520of%2520refraction%2520error%2520and%2520axial%2520length%2520based%2520on%2520ultra-widefield%2520fundus%2520images%26entry.906535625%3DChong%2520Zhong%2520and%2520Yang%2520Li%2520and%2520Danjuan%2520Yang%2520and%2520Meiyan%2520Li%2520and%2520Xingyao%2520Zhou%2520and%2520Bo%2520Fu%2520and%2520Catherine%2520C.%2520Liu%2520and%2520A.%2520H.%2520Welsh%26entry.1292438233%3D%2520%2520The%2520ultra-widefield%2520%2528UWF%2529%2520fundus%2520image%2520is%2520an%2520attractive%25203D%2520biomarker%2520in%250AAI-aided%2520myopia%2520screening%2520because%2520it%2520provides%2520much%2520richer%2520myopia-related%250Ainformation.%2520Though%2520axial%2520length%2520%2528AL%2529%2520has%2520been%2520acknowledged%2520to%2520be%2520highly%250Arelated%2520to%2520the%2520two%2520key%2520targets%2520of%2520myopia%2520screening%252C%2520Spherical%2520Equivalence%2520%2528SE%2529%250Ameasurement%2520and%2520high%2520myopia%2520diagnosis%252C%2520its%2520prediction%2520based%2520on%2520the%2520UWF%2520fundus%250Aimage%2520is%2520rarely%2520considered.%2520To%2520save%2520the%2520high%2520expense%2520and%2520time%2520costs%2520of%250Ameasuring%2520SE%2520and%2520AL%252C%2520we%2520propose%2520the%2520Copula-enhanced%2520Convolutional%2520Neural%250ANetwork%2520%2528CeCNN%2529%252C%2520a%2520one-stop%2520UWF-based%2520ophthalmic%2520AI%2520framework%2520to%2520jointly%250Apredict%2520SE%252C%2520AL%252C%2520and%2520myopia%2520status.%2520The%2520CeCNN%2520formulates%2520a%2520multiresponse%250Aregression%2520that%2520relates%2520multiple%2520dependent%2520discrete-continuous%2520responses%2520and%250Athe%2520image%2520covariate%252C%2520where%2520the%2520nonlinearity%2520of%2520the%2520association%2520is%2520modeled%2520by%2520a%250Abackbone%2520CNN.%2520To%2520thoroughly%2520describe%2520the%2520dependence%2520structure%2520among%2520the%250Aresponses%252C%2520we%2520model%2520and%2520incorporate%2520the%2520conditional%2520dependence%2520among%2520responses%250Ain%2520a%2520CNN%2520through%2520a%2520new%2520copula-likelihood%2520loss.%2520We%2520provide%2520statistical%250Ainterpretations%2520of%2520the%2520conditional%2520dependence%2520among%2520responses%252C%2520and%2520reveal%2520that%250Asuch%2520dependence%2520is%2520beyond%2520the%2520dependence%2520explained%2520by%2520the%2520image%2520covariate.%2520We%250Aheuristically%2520justify%2520that%2520the%2520proposed%2520loss%2520can%2520enhance%2520the%2520estimation%250Aefficiency%2520of%2520the%2520CNN%2520weights.%2520We%2520apply%2520the%2520CeCNN%2520to%2520the%2520UWF%2520dataset%2520collected%250Aby%2520us%2520and%2520demonstrate%2520that%2520the%2520CeCNN%2520sharply%2520enhances%2520the%2520predictive%2520capability%250Aof%2520various%2520backbone%2520CNNs.%2520Our%2520study%2520evidences%2520the%2520ophthalmology%2520view%2520that%250Abesides%2520SE%252C%2520AL%2520is%2520also%2520an%2520important%2520measure%2520to%2520myopia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03967v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CeCNN%3A%20Copula-enhanced%20convolutional%20neural%20networks%20in%20joint%20prediction%0A%20%20of%20refraction%20error%20and%20axial%20length%20based%20on%20ultra-widefield%20fundus%20images&entry.906535625=Chong%20Zhong%20and%20Yang%20Li%20and%20Danjuan%20Yang%20and%20Meiyan%20Li%20and%20Xingyao%20Zhou%20and%20Bo%20Fu%20and%20Catherine%20C.%20Liu%20and%20A.%20H.%20Welsh&entry.1292438233=%20%20The%20ultra-widefield%20%28UWF%29%20fundus%20image%20is%20an%20attractive%203D%20biomarker%20in%0AAI-aided%20myopia%20screening%20because%20it%20provides%20much%20richer%20myopia-related%0Ainformation.%20Though%20axial%20length%20%28AL%29%20has%20been%20acknowledged%20to%20be%20highly%0Arelated%20to%20the%20two%20key%20targets%20of%20myopia%20screening%2C%20Spherical%20Equivalence%20%28SE%29%0Ameasurement%20and%20high%20myopia%20diagnosis%2C%20its%20prediction%20based%20on%20the%20UWF%20fundus%0Aimage%20is%20rarely%20considered.%20To%20save%20the%20high%20expense%20and%20time%20costs%20of%0Ameasuring%20SE%20and%20AL%2C%20we%20propose%20the%20Copula-enhanced%20Convolutional%20Neural%0ANetwork%20%28CeCNN%29%2C%20a%20one-stop%20UWF-based%20ophthalmic%20AI%20framework%20to%20jointly%0Apredict%20SE%2C%20AL%2C%20and%20myopia%20status.%20The%20CeCNN%20formulates%20a%20multiresponse%0Aregression%20that%20relates%20multiple%20dependent%20discrete-continuous%20responses%20and%0Athe%20image%20covariate%2C%20where%20the%20nonlinearity%20of%20the%20association%20is%20modeled%20by%20a%0Abackbone%20CNN.%20To%20thoroughly%20describe%20the%20dependence%20structure%20among%20the%0Aresponses%2C%20we%20model%20and%20incorporate%20the%20conditional%20dependence%20among%20responses%0Ain%20a%20CNN%20through%20a%20new%20copula-likelihood%20loss.%20We%20provide%20statistical%0Ainterpretations%20of%20the%20conditional%20dependence%20among%20responses%2C%20and%20reveal%20that%0Asuch%20dependence%20is%20beyond%20the%20dependence%20explained%20by%20the%20image%20covariate.%20We%0Aheuristically%20justify%20that%20the%20proposed%20loss%20can%20enhance%20the%20estimation%0Aefficiency%20of%20the%20CNN%20weights.%20We%20apply%20the%20CeCNN%20to%20the%20UWF%20dataset%20collected%0Aby%20us%20and%20demonstrate%20that%20the%20CeCNN%20sharply%20enhances%20the%20predictive%20capability%0Aof%20various%20backbone%20CNNs.%20Our%20study%20evidences%20the%20ophthalmology%20view%20that%0Abesides%20SE%2C%20AL%20is%20also%20an%20important%20measure%20to%20myopia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03967v4&entry.124074799=Read"},
{"title": "TsCA: On the Semantic Consistency Alignment via Conditional Transport\n  for Compositional Zero-Shot Learning", "author": "Miaoge Li and Jingcai Guo and Richard Yi Da Xu and Dongsheng Wang and Xiaofeng Cao and Song Guo", "abstract": "  Compositional Zero-Shot Learning (CZSL) aims to recognize novel\n\\textit{state-object} compositions by leveraging the shared knowledge of their\nprimitive components. Despite considerable progress, effectively calibrating\nthe bias between semantically similar multimodal representations, as well as\ngeneralizing pre-trained knowledge to novel compositional contexts, remains an\nenduring challenge. In this paper, our interest is to revisit the conditional\ntransport (CT) theory and its homology to the visual-semantics interaction in\nCZSL and further, propose a novel Trisets Consistency Alignment framework\n(dubbed TsCA) that well-addresses these issues. Concretely, we utilize three\ndistinct yet semantically homologous sets, i.e., patches, primitives, and\ncompositions, to construct pairwise CT costs to minimize their semantic\ndiscrepancies. To further ensure the consistency transfer within these sets, we\nimplement a cycle-consistency constraint that refines the learning by\nguaranteeing the feature consistency of the self-mapping during transport flow,\nregardless of modality. Moreover, we extend the CT plans to an open-world\nsetting, which enables the model to effectively filter out unfeasible pairs,\nthereby speeding up the inference as well as increasing the accuracy. Extensive\nexperiments are conducted to verify the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2408.08703v1", "date": "2024-08-16", "relevancy": 2.1276, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5084}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TsCA%3A%20On%20the%20Semantic%20Consistency%20Alignment%20via%20Conditional%20Transport%0A%20%20for%20Compositional%20Zero-Shot%20Learning&body=Title%3A%20TsCA%3A%20On%20the%20Semantic%20Consistency%20Alignment%20via%20Conditional%20Transport%0A%20%20for%20Compositional%20Zero-Shot%20Learning%0AAuthor%3A%20Miaoge%20Li%20and%20Jingcai%20Guo%20and%20Richard%20Yi%20Da%20Xu%20and%20Dongsheng%20Wang%20and%20Xiaofeng%20Cao%20and%20Song%20Guo%0AAbstract%3A%20%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20recognize%20novel%0A%5Ctextit%7Bstate-object%7D%20compositions%20by%20leveraging%20the%20shared%20knowledge%20of%20their%0Aprimitive%20components.%20Despite%20considerable%20progress%2C%20effectively%20calibrating%0Athe%20bias%20between%20semantically%20similar%20multimodal%20representations%2C%20as%20well%20as%0Ageneralizing%20pre-trained%20knowledge%20to%20novel%20compositional%20contexts%2C%20remains%20an%0Aenduring%20challenge.%20In%20this%20paper%2C%20our%20interest%20is%20to%20revisit%20the%20conditional%0Atransport%20%28CT%29%20theory%20and%20its%20homology%20to%20the%20visual-semantics%20interaction%20in%0ACZSL%20and%20further%2C%20propose%20a%20novel%20Trisets%20Consistency%20Alignment%20framework%0A%28dubbed%20TsCA%29%20that%20well-addresses%20these%20issues.%20Concretely%2C%20we%20utilize%20three%0Adistinct%20yet%20semantically%20homologous%20sets%2C%20i.e.%2C%20patches%2C%20primitives%2C%20and%0Acompositions%2C%20to%20construct%20pairwise%20CT%20costs%20to%20minimize%20their%20semantic%0Adiscrepancies.%20To%20further%20ensure%20the%20consistency%20transfer%20within%20these%20sets%2C%20we%0Aimplement%20a%20cycle-consistency%20constraint%20that%20refines%20the%20learning%20by%0Aguaranteeing%20the%20feature%20consistency%20of%20the%20self-mapping%20during%20transport%20flow%2C%0Aregardless%20of%20modality.%20Moreover%2C%20we%20extend%20the%20CT%20plans%20to%20an%20open-world%0Asetting%2C%20which%20enables%20the%20model%20to%20effectively%20filter%20out%20unfeasible%20pairs%2C%0Athereby%20speeding%20up%20the%20inference%20as%20well%20as%20increasing%20the%20accuracy.%20Extensive%0Aexperiments%20are%20conducted%20to%20verify%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTsCA%253A%2520On%2520the%2520Semantic%2520Consistency%2520Alignment%2520via%2520Conditional%2520Transport%250A%2520%2520for%2520Compositional%2520Zero-Shot%2520Learning%26entry.906535625%3DMiaoge%2520Li%2520and%2520Jingcai%2520Guo%2520and%2520Richard%2520Yi%2520Da%2520Xu%2520and%2520Dongsheng%2520Wang%2520and%2520Xiaofeng%2520Cao%2520and%2520Song%2520Guo%26entry.1292438233%3D%2520%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%2520aims%2520to%2520recognize%2520novel%250A%255Ctextit%257Bstate-object%257D%2520compositions%2520by%2520leveraging%2520the%2520shared%2520knowledge%2520of%2520their%250Aprimitive%2520components.%2520Despite%2520considerable%2520progress%252C%2520effectively%2520calibrating%250Athe%2520bias%2520between%2520semantically%2520similar%2520multimodal%2520representations%252C%2520as%2520well%2520as%250Ageneralizing%2520pre-trained%2520knowledge%2520to%2520novel%2520compositional%2520contexts%252C%2520remains%2520an%250Aenduring%2520challenge.%2520In%2520this%2520paper%252C%2520our%2520interest%2520is%2520to%2520revisit%2520the%2520conditional%250Atransport%2520%2528CT%2529%2520theory%2520and%2520its%2520homology%2520to%2520the%2520visual-semantics%2520interaction%2520in%250ACZSL%2520and%2520further%252C%2520propose%2520a%2520novel%2520Trisets%2520Consistency%2520Alignment%2520framework%250A%2528dubbed%2520TsCA%2529%2520that%2520well-addresses%2520these%2520issues.%2520Concretely%252C%2520we%2520utilize%2520three%250Adistinct%2520yet%2520semantically%2520homologous%2520sets%252C%2520i.e.%252C%2520patches%252C%2520primitives%252C%2520and%250Acompositions%252C%2520to%2520construct%2520pairwise%2520CT%2520costs%2520to%2520minimize%2520their%2520semantic%250Adiscrepancies.%2520To%2520further%2520ensure%2520the%2520consistency%2520transfer%2520within%2520these%2520sets%252C%2520we%250Aimplement%2520a%2520cycle-consistency%2520constraint%2520that%2520refines%2520the%2520learning%2520by%250Aguaranteeing%2520the%2520feature%2520consistency%2520of%2520the%2520self-mapping%2520during%2520transport%2520flow%252C%250Aregardless%2520of%2520modality.%2520Moreover%252C%2520we%2520extend%2520the%2520CT%2520plans%2520to%2520an%2520open-world%250Asetting%252C%2520which%2520enables%2520the%2520model%2520to%2520effectively%2520filter%2520out%2520unfeasible%2520pairs%252C%250Athereby%2520speeding%2520up%2520the%2520inference%2520as%2520well%2520as%2520increasing%2520the%2520accuracy.%2520Extensive%250Aexperiments%2520are%2520conducted%2520to%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TsCA%3A%20On%20the%20Semantic%20Consistency%20Alignment%20via%20Conditional%20Transport%0A%20%20for%20Compositional%20Zero-Shot%20Learning&entry.906535625=Miaoge%20Li%20and%20Jingcai%20Guo%20and%20Richard%20Yi%20Da%20Xu%20and%20Dongsheng%20Wang%20and%20Xiaofeng%20Cao%20and%20Song%20Guo&entry.1292438233=%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20recognize%20novel%0A%5Ctextit%7Bstate-object%7D%20compositions%20by%20leveraging%20the%20shared%20knowledge%20of%20their%0Aprimitive%20components.%20Despite%20considerable%20progress%2C%20effectively%20calibrating%0Athe%20bias%20between%20semantically%20similar%20multimodal%20representations%2C%20as%20well%20as%0Ageneralizing%20pre-trained%20knowledge%20to%20novel%20compositional%20contexts%2C%20remains%20an%0Aenduring%20challenge.%20In%20this%20paper%2C%20our%20interest%20is%20to%20revisit%20the%20conditional%0Atransport%20%28CT%29%20theory%20and%20its%20homology%20to%20the%20visual-semantics%20interaction%20in%0ACZSL%20and%20further%2C%20propose%20a%20novel%20Trisets%20Consistency%20Alignment%20framework%0A%28dubbed%20TsCA%29%20that%20well-addresses%20these%20issues.%20Concretely%2C%20we%20utilize%20three%0Adistinct%20yet%20semantically%20homologous%20sets%2C%20i.e.%2C%20patches%2C%20primitives%2C%20and%0Acompositions%2C%20to%20construct%20pairwise%20CT%20costs%20to%20minimize%20their%20semantic%0Adiscrepancies.%20To%20further%20ensure%20the%20consistency%20transfer%20within%20these%20sets%2C%20we%0Aimplement%20a%20cycle-consistency%20constraint%20that%20refines%20the%20learning%20by%0Aguaranteeing%20the%20feature%20consistency%20of%20the%20self-mapping%20during%20transport%20flow%2C%0Aregardless%20of%20modality.%20Moreover%2C%20we%20extend%20the%20CT%20plans%20to%20an%20open-world%0Asetting%2C%20which%20enables%20the%20model%20to%20effectively%20filter%20out%20unfeasible%20pairs%2C%0Athereby%20speeding%20up%20the%20inference%20as%20well%20as%20increasing%20the%20accuracy.%20Extensive%0Aexperiments%20are%20conducted%20to%20verify%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08703v1&entry.124074799=Read"},
{"title": "QMambaBSR: Burst Image Super-Resolution with Query State Space Model", "author": "Xin Di and Long Peng and Peizhe Xia and Wenbo Li and Renjing Pei and Yang Cao and Yang Wang and Zheng-Jun Zha", "abstract": "  Burst super-resolution aims to reconstruct high-resolution images with higher\nquality and richer details by fusing the sub-pixel information from multiple\nburst low-resolution frames. In BusrtSR, the key challenge lies in extracting\nthe base frame's content complementary sub-pixel details while simultaneously\nsuppressing high-frequency noise disturbance. Existing methods attempt to\nextract sub-pixels by modeling inter-frame relationships frame by frame while\noverlooking the mutual correlations among multi-current frames and neglecting\nthe intra-frame interactions, leading to inaccurate and noisy sub-pixels for\nbase frame super-resolution. Further, existing methods mainly employ static\nupsampling with fixed parameters to improve spatial resolution for all scenes,\nfailing to perceive the sub-pixel distribution difference across multiple\nframes and cannot balance the fusion weights of different frames, resulting in\nover-smoothed details and artifacts. To address these limitations, we introduce\na novel Query Mamba Burst Super-Resolution (QMambaBSR) network, which\nincorporates a Query State Space Model (QSSM) and Adaptive Up-sampling module\n(AdaUp). Specifically, based on the observation that sub-pixels have consistent\nspatial distribution while random noise is inconsistently distributed, a novel\nQSSM is proposed to efficiently extract sub-pixels through inter-frame querying\nand intra-frame scanning while mitigating noise interference in a single step.\nMoreover, AdaUp is designed to dynamically adjust the upsampling kernel based\non the spatial distribution of multi-frame sub-pixel information in the\ndifferent burst scenes, thereby facilitating the reconstruction of the spatial\narrangement of high-resolution details. Extensive experiments on four popular\nsynthetic and real-world benchmarks demonstrate that our method achieves a new\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2408.08665v1", "date": "2024-08-16", "relevancy": 2.1165, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5554}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5302}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QMambaBSR%3A%20Burst%20Image%20Super-Resolution%20with%20Query%20State%20Space%20Model&body=Title%3A%20QMambaBSR%3A%20Burst%20Image%20Super-Resolution%20with%20Query%20State%20Space%20Model%0AAuthor%3A%20Xin%20Di%20and%20Long%20Peng%20and%20Peizhe%20Xia%20and%20Wenbo%20Li%20and%20Renjing%20Pei%20and%20Yang%20Cao%20and%20Yang%20Wang%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Burst%20super-resolution%20aims%20to%20reconstruct%20high-resolution%20images%20with%20higher%0Aquality%20and%20richer%20details%20by%20fusing%20the%20sub-pixel%20information%20from%20multiple%0Aburst%20low-resolution%20frames.%20In%20BusrtSR%2C%20the%20key%20challenge%20lies%20in%20extracting%0Athe%20base%20frame%27s%20content%20complementary%20sub-pixel%20details%20while%20simultaneously%0Asuppressing%20high-frequency%20noise%20disturbance.%20Existing%20methods%20attempt%20to%0Aextract%20sub-pixels%20by%20modeling%20inter-frame%20relationships%20frame%20by%20frame%20while%0Aoverlooking%20the%20mutual%20correlations%20among%20multi-current%20frames%20and%20neglecting%0Athe%20intra-frame%20interactions%2C%20leading%20to%20inaccurate%20and%20noisy%20sub-pixels%20for%0Abase%20frame%20super-resolution.%20Further%2C%20existing%20methods%20mainly%20employ%20static%0Aupsampling%20with%20fixed%20parameters%20to%20improve%20spatial%20resolution%20for%20all%20scenes%2C%0Afailing%20to%20perceive%20the%20sub-pixel%20distribution%20difference%20across%20multiple%0Aframes%20and%20cannot%20balance%20the%20fusion%20weights%20of%20different%20frames%2C%20resulting%20in%0Aover-smoothed%20details%20and%20artifacts.%20To%20address%20these%20limitations%2C%20we%20introduce%0Aa%20novel%20Query%20Mamba%20Burst%20Super-Resolution%20%28QMambaBSR%29%20network%2C%20which%0Aincorporates%20a%20Query%20State%20Space%20Model%20%28QSSM%29%20and%20Adaptive%20Up-sampling%20module%0A%28AdaUp%29.%20Specifically%2C%20based%20on%20the%20observation%20that%20sub-pixels%20have%20consistent%0Aspatial%20distribution%20while%20random%20noise%20is%20inconsistently%20distributed%2C%20a%20novel%0AQSSM%20is%20proposed%20to%20efficiently%20extract%20sub-pixels%20through%20inter-frame%20querying%0Aand%20intra-frame%20scanning%20while%20mitigating%20noise%20interference%20in%20a%20single%20step.%0AMoreover%2C%20AdaUp%20is%20designed%20to%20dynamically%20adjust%20the%20upsampling%20kernel%20based%0Aon%20the%20spatial%20distribution%20of%20multi-frame%20sub-pixel%20information%20in%20the%0Adifferent%20burst%20scenes%2C%20thereby%20facilitating%20the%20reconstruction%20of%20the%20spatial%0Aarrangement%20of%20high-resolution%20details.%20Extensive%20experiments%20on%20four%20popular%0Asynthetic%20and%20real-world%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQMambaBSR%253A%2520Burst%2520Image%2520Super-Resolution%2520with%2520Query%2520State%2520Space%2520Model%26entry.906535625%3DXin%2520Di%2520and%2520Long%2520Peng%2520and%2520Peizhe%2520Xia%2520and%2520Wenbo%2520Li%2520and%2520Renjing%2520Pei%2520and%2520Yang%2520Cao%2520and%2520Yang%2520Wang%2520and%2520Zheng-Jun%2520Zha%26entry.1292438233%3D%2520%2520Burst%2520super-resolution%2520aims%2520to%2520reconstruct%2520high-resolution%2520images%2520with%2520higher%250Aquality%2520and%2520richer%2520details%2520by%2520fusing%2520the%2520sub-pixel%2520information%2520from%2520multiple%250Aburst%2520low-resolution%2520frames.%2520In%2520BusrtSR%252C%2520the%2520key%2520challenge%2520lies%2520in%2520extracting%250Athe%2520base%2520frame%2527s%2520content%2520complementary%2520sub-pixel%2520details%2520while%2520simultaneously%250Asuppressing%2520high-frequency%2520noise%2520disturbance.%2520Existing%2520methods%2520attempt%2520to%250Aextract%2520sub-pixels%2520by%2520modeling%2520inter-frame%2520relationships%2520frame%2520by%2520frame%2520while%250Aoverlooking%2520the%2520mutual%2520correlations%2520among%2520multi-current%2520frames%2520and%2520neglecting%250Athe%2520intra-frame%2520interactions%252C%2520leading%2520to%2520inaccurate%2520and%2520noisy%2520sub-pixels%2520for%250Abase%2520frame%2520super-resolution.%2520Further%252C%2520existing%2520methods%2520mainly%2520employ%2520static%250Aupsampling%2520with%2520fixed%2520parameters%2520to%2520improve%2520spatial%2520resolution%2520for%2520all%2520scenes%252C%250Afailing%2520to%2520perceive%2520the%2520sub-pixel%2520distribution%2520difference%2520across%2520multiple%250Aframes%2520and%2520cannot%2520balance%2520the%2520fusion%2520weights%2520of%2520different%2520frames%252C%2520resulting%2520in%250Aover-smoothed%2520details%2520and%2520artifacts.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250Aa%2520novel%2520Query%2520Mamba%2520Burst%2520Super-Resolution%2520%2528QMambaBSR%2529%2520network%252C%2520which%250Aincorporates%2520a%2520Query%2520State%2520Space%2520Model%2520%2528QSSM%2529%2520and%2520Adaptive%2520Up-sampling%2520module%250A%2528AdaUp%2529.%2520Specifically%252C%2520based%2520on%2520the%2520observation%2520that%2520sub-pixels%2520have%2520consistent%250Aspatial%2520distribution%2520while%2520random%2520noise%2520is%2520inconsistently%2520distributed%252C%2520a%2520novel%250AQSSM%2520is%2520proposed%2520to%2520efficiently%2520extract%2520sub-pixels%2520through%2520inter-frame%2520querying%250Aand%2520intra-frame%2520scanning%2520while%2520mitigating%2520noise%2520interference%2520in%2520a%2520single%2520step.%250AMoreover%252C%2520AdaUp%2520is%2520designed%2520to%2520dynamically%2520adjust%2520the%2520upsampling%2520kernel%2520based%250Aon%2520the%2520spatial%2520distribution%2520of%2520multi-frame%2520sub-pixel%2520information%2520in%2520the%250Adifferent%2520burst%2520scenes%252C%2520thereby%2520facilitating%2520the%2520reconstruction%2520of%2520the%2520spatial%250Aarrangement%2520of%2520high-resolution%2520details.%2520Extensive%2520experiments%2520on%2520four%2520popular%250Asynthetic%2520and%2520real-world%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520a%2520new%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QMambaBSR%3A%20Burst%20Image%20Super-Resolution%20with%20Query%20State%20Space%20Model&entry.906535625=Xin%20Di%20and%20Long%20Peng%20and%20Peizhe%20Xia%20and%20Wenbo%20Li%20and%20Renjing%20Pei%20and%20Yang%20Cao%20and%20Yang%20Wang%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Burst%20super-resolution%20aims%20to%20reconstruct%20high-resolution%20images%20with%20higher%0Aquality%20and%20richer%20details%20by%20fusing%20the%20sub-pixel%20information%20from%20multiple%0Aburst%20low-resolution%20frames.%20In%20BusrtSR%2C%20the%20key%20challenge%20lies%20in%20extracting%0Athe%20base%20frame%27s%20content%20complementary%20sub-pixel%20details%20while%20simultaneously%0Asuppressing%20high-frequency%20noise%20disturbance.%20Existing%20methods%20attempt%20to%0Aextract%20sub-pixels%20by%20modeling%20inter-frame%20relationships%20frame%20by%20frame%20while%0Aoverlooking%20the%20mutual%20correlations%20among%20multi-current%20frames%20and%20neglecting%0Athe%20intra-frame%20interactions%2C%20leading%20to%20inaccurate%20and%20noisy%20sub-pixels%20for%0Abase%20frame%20super-resolution.%20Further%2C%20existing%20methods%20mainly%20employ%20static%0Aupsampling%20with%20fixed%20parameters%20to%20improve%20spatial%20resolution%20for%20all%20scenes%2C%0Afailing%20to%20perceive%20the%20sub-pixel%20distribution%20difference%20across%20multiple%0Aframes%20and%20cannot%20balance%20the%20fusion%20weights%20of%20different%20frames%2C%20resulting%20in%0Aover-smoothed%20details%20and%20artifacts.%20To%20address%20these%20limitations%2C%20we%20introduce%0Aa%20novel%20Query%20Mamba%20Burst%20Super-Resolution%20%28QMambaBSR%29%20network%2C%20which%0Aincorporates%20a%20Query%20State%20Space%20Model%20%28QSSM%29%20and%20Adaptive%20Up-sampling%20module%0A%28AdaUp%29.%20Specifically%2C%20based%20on%20the%20observation%20that%20sub-pixels%20have%20consistent%0Aspatial%20distribution%20while%20random%20noise%20is%20inconsistently%20distributed%2C%20a%20novel%0AQSSM%20is%20proposed%20to%20efficiently%20extract%20sub-pixels%20through%20inter-frame%20querying%0Aand%20intra-frame%20scanning%20while%20mitigating%20noise%20interference%20in%20a%20single%20step.%0AMoreover%2C%20AdaUp%20is%20designed%20to%20dynamically%20adjust%20the%20upsampling%20kernel%20based%0Aon%20the%20spatial%20distribution%20of%20multi-frame%20sub-pixel%20information%20in%20the%0Adifferent%20burst%20scenes%2C%20thereby%20facilitating%20the%20reconstruction%20of%20the%20spatial%0Aarrangement%20of%20high-resolution%20details.%20Extensive%20experiments%20on%20four%20popular%0Asynthetic%20and%20real-world%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08665v1&entry.124074799=Read"},
{"title": "Extracting polygonal footprints in off-nadir images with Segment\n  Anything Model", "author": "Kai Li and Jingbo Chen and Yupeng Deng and Yu Meng and Diyou Liu and Junxian Ma and Chenhao Wang", "abstract": "  Building Footprint Extraction (BFE) in off-nadir aerial images often relies\non roof segmentation and roof-to-footprint offset prediction, then drugging\nroof-to-footprint via the offset. However, the results from this multi-stage\ninference are not applicable in data production, because of the low quality of\nmasks given by prediction. To solve this problem, we proposed OBMv2 in this\npaper, which supports both end-to-end and promptable polygonal footprint\nprediction. Different from OBM, OBMv2 using a newly proposed Self Offset\nAttention (SOFA) to bridge the performance gap on bungalow and skyscraper,\nwhich realized a real end-to-end footprint polygon prediction without\npostprocessing. %, such as Non-Maximum Suppression (NMS) and Distance NMS\n(DNMS). % To fully use information contained in roof masks, building masks and\noffsets, we proposed a Multi-level Information SyStem (MISS) for footprint\nprediction, with which OBMv2 can predict footprints even with insufficient\npredictions. Additionally, to squeeze information from the same model, we were\ninspired by Retrieval-Augmented Generation (RAG) in Nature Language Processing\nand proposed \"RAG in BFE\" problem. To verify the effectiveness of the proposed\nmethod, experiments were conducted on open datasets BONAI and OmniCity-view3. A\ngeneralization test was also conducted on Huizhou test set. The code will be\navailable at \\url{https://github.com/likaiucas/OBM}.\n", "link": "http://arxiv.org/abs/2408.08645v1", "date": "2024-08-16", "relevancy": 2.1065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5269}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20polygonal%20footprints%20in%20off-nadir%20images%20with%20Segment%0A%20%20Anything%20Model&body=Title%3A%20Extracting%20polygonal%20footprints%20in%20off-nadir%20images%20with%20Segment%0A%20%20Anything%20Model%0AAuthor%3A%20Kai%20Li%20and%20Jingbo%20Chen%20and%20Yupeng%20Deng%20and%20Yu%20Meng%20and%20Diyou%20Liu%20and%20Junxian%20Ma%20and%20Chenhao%20Wang%0AAbstract%3A%20%20%20Building%20Footprint%20Extraction%20%28BFE%29%20in%20off-nadir%20aerial%20images%20often%20relies%0Aon%20roof%20segmentation%20and%20roof-to-footprint%20offset%20prediction%2C%20then%20drugging%0Aroof-to-footprint%20via%20the%20offset.%20However%2C%20the%20results%20from%20this%20multi-stage%0Ainference%20are%20not%20applicable%20in%20data%20production%2C%20because%20of%20the%20low%20quality%20of%0Amasks%20given%20by%20prediction.%20To%20solve%20this%20problem%2C%20we%20proposed%20OBMv2%20in%20this%0Apaper%2C%20which%20supports%20both%20end-to-end%20and%20promptable%20polygonal%20footprint%0Aprediction.%20Different%20from%20OBM%2C%20OBMv2%20using%20a%20newly%20proposed%20Self%20Offset%0AAttention%20%28SOFA%29%20to%20bridge%20the%20performance%20gap%20on%20bungalow%20and%20skyscraper%2C%0Awhich%20realized%20a%20real%20end-to-end%20footprint%20polygon%20prediction%20without%0Apostprocessing.%20%25%2C%20such%20as%20Non-Maximum%20Suppression%20%28NMS%29%20and%20Distance%20NMS%0A%28DNMS%29.%20%25%20To%20fully%20use%20information%20contained%20in%20roof%20masks%2C%20building%20masks%20and%0Aoffsets%2C%20we%20proposed%20a%20Multi-level%20Information%20SyStem%20%28MISS%29%20for%20footprint%0Aprediction%2C%20with%20which%20OBMv2%20can%20predict%20footprints%20even%20with%20insufficient%0Apredictions.%20Additionally%2C%20to%20squeeze%20information%20from%20the%20same%20model%2C%20we%20were%0Ainspired%20by%20Retrieval-Augmented%20Generation%20%28RAG%29%20in%20Nature%20Language%20Processing%0Aand%20proposed%20%22RAG%20in%20BFE%22%20problem.%20To%20verify%20the%20effectiveness%20of%20the%20proposed%0Amethod%2C%20experiments%20were%20conducted%20on%20open%20datasets%20BONAI%20and%20OmniCity-view3.%20A%0Ageneralization%20test%20was%20also%20conducted%20on%20Huizhou%20test%20set.%20The%20code%20will%20be%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/likaiucas/OBM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520polygonal%2520footprints%2520in%2520off-nadir%2520images%2520with%2520Segment%250A%2520%2520Anything%2520Model%26entry.906535625%3DKai%2520Li%2520and%2520Jingbo%2520Chen%2520and%2520Yupeng%2520Deng%2520and%2520Yu%2520Meng%2520and%2520Diyou%2520Liu%2520and%2520Junxian%2520Ma%2520and%2520Chenhao%2520Wang%26entry.1292438233%3D%2520%2520Building%2520Footprint%2520Extraction%2520%2528BFE%2529%2520in%2520off-nadir%2520aerial%2520images%2520often%2520relies%250Aon%2520roof%2520segmentation%2520and%2520roof-to-footprint%2520offset%2520prediction%252C%2520then%2520drugging%250Aroof-to-footprint%2520via%2520the%2520offset.%2520However%252C%2520the%2520results%2520from%2520this%2520multi-stage%250Ainference%2520are%2520not%2520applicable%2520in%2520data%2520production%252C%2520because%2520of%2520the%2520low%2520quality%2520of%250Amasks%2520given%2520by%2520prediction.%2520To%2520solve%2520this%2520problem%252C%2520we%2520proposed%2520OBMv2%2520in%2520this%250Apaper%252C%2520which%2520supports%2520both%2520end-to-end%2520and%2520promptable%2520polygonal%2520footprint%250Aprediction.%2520Different%2520from%2520OBM%252C%2520OBMv2%2520using%2520a%2520newly%2520proposed%2520Self%2520Offset%250AAttention%2520%2528SOFA%2529%2520to%2520bridge%2520the%2520performance%2520gap%2520on%2520bungalow%2520and%2520skyscraper%252C%250Awhich%2520realized%2520a%2520real%2520end-to-end%2520footprint%2520polygon%2520prediction%2520without%250Apostprocessing.%2520%2525%252C%2520such%2520as%2520Non-Maximum%2520Suppression%2520%2528NMS%2529%2520and%2520Distance%2520NMS%250A%2528DNMS%2529.%2520%2525%2520To%2520fully%2520use%2520information%2520contained%2520in%2520roof%2520masks%252C%2520building%2520masks%2520and%250Aoffsets%252C%2520we%2520proposed%2520a%2520Multi-level%2520Information%2520SyStem%2520%2528MISS%2529%2520for%2520footprint%250Aprediction%252C%2520with%2520which%2520OBMv2%2520can%2520predict%2520footprints%2520even%2520with%2520insufficient%250Apredictions.%2520Additionally%252C%2520to%2520squeeze%2520information%2520from%2520the%2520same%2520model%252C%2520we%2520were%250Ainspired%2520by%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520in%2520Nature%2520Language%2520Processing%250Aand%2520proposed%2520%2522RAG%2520in%2520BFE%2522%2520problem.%2520To%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod%252C%2520experiments%2520were%2520conducted%2520on%2520open%2520datasets%2520BONAI%2520and%2520OmniCity-view3.%2520A%250Ageneralization%2520test%2520was%2520also%2520conducted%2520on%2520Huizhou%2520test%2520set.%2520The%2520code%2520will%2520be%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/likaiucas/OBM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20polygonal%20footprints%20in%20off-nadir%20images%20with%20Segment%0A%20%20Anything%20Model&entry.906535625=Kai%20Li%20and%20Jingbo%20Chen%20and%20Yupeng%20Deng%20and%20Yu%20Meng%20and%20Diyou%20Liu%20and%20Junxian%20Ma%20and%20Chenhao%20Wang&entry.1292438233=%20%20Building%20Footprint%20Extraction%20%28BFE%29%20in%20off-nadir%20aerial%20images%20often%20relies%0Aon%20roof%20segmentation%20and%20roof-to-footprint%20offset%20prediction%2C%20then%20drugging%0Aroof-to-footprint%20via%20the%20offset.%20However%2C%20the%20results%20from%20this%20multi-stage%0Ainference%20are%20not%20applicable%20in%20data%20production%2C%20because%20of%20the%20low%20quality%20of%0Amasks%20given%20by%20prediction.%20To%20solve%20this%20problem%2C%20we%20proposed%20OBMv2%20in%20this%0Apaper%2C%20which%20supports%20both%20end-to-end%20and%20promptable%20polygonal%20footprint%0Aprediction.%20Different%20from%20OBM%2C%20OBMv2%20using%20a%20newly%20proposed%20Self%20Offset%0AAttention%20%28SOFA%29%20to%20bridge%20the%20performance%20gap%20on%20bungalow%20and%20skyscraper%2C%0Awhich%20realized%20a%20real%20end-to-end%20footprint%20polygon%20prediction%20without%0Apostprocessing.%20%25%2C%20such%20as%20Non-Maximum%20Suppression%20%28NMS%29%20and%20Distance%20NMS%0A%28DNMS%29.%20%25%20To%20fully%20use%20information%20contained%20in%20roof%20masks%2C%20building%20masks%20and%0Aoffsets%2C%20we%20proposed%20a%20Multi-level%20Information%20SyStem%20%28MISS%29%20for%20footprint%0Aprediction%2C%20with%20which%20OBMv2%20can%20predict%20footprints%20even%20with%20insufficient%0Apredictions.%20Additionally%2C%20to%20squeeze%20information%20from%20the%20same%20model%2C%20we%20were%0Ainspired%20by%20Retrieval-Augmented%20Generation%20%28RAG%29%20in%20Nature%20Language%20Processing%0Aand%20proposed%20%22RAG%20in%20BFE%22%20problem.%20To%20verify%20the%20effectiveness%20of%20the%20proposed%0Amethod%2C%20experiments%20were%20conducted%20on%20open%20datasets%20BONAI%20and%20OmniCity-view3.%20A%0Ageneralization%20test%20was%20also%20conducted%20on%20Huizhou%20test%20set.%20The%20code%20will%20be%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/likaiucas/OBM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08645v1&entry.124074799=Read"},
{"title": "Optimal Symmetries in Binary Classification", "author": "Vishal S. Ngairangbam and Michael Spannowsky", "abstract": "  We explore the role of group symmetries in binary classification tasks,\npresenting a novel framework that leverages the principles of Neyman-Pearson\noptimality. Contrary to the common intuition that larger symmetry groups lead\nto improved classification performance, our findings show that selecting the\nappropriate group symmetries is crucial for optimising generalisation and\nsample efficiency. We develop a theoretical foundation for designing group\nequivariant neural networks that align the choice of symmetries with the\nunderlying probability distributions of the data. Our approach provides a\nunified methodology for improving classification accuracy across a broad range\nof applications by carefully tailoring the symmetry group to the specific\ncharacteristics of the problem. Theoretical analysis and experimental results\ndemonstrate that optimal classification performance is not always associated\nwith the largest equivariant groups possible in the domain, even when the\nlikelihood ratio is invariant under one of its proper subgroups, but rather\nwith those subgroups themselves. This work offers insights and practical\nguidelines for constructing more effective group equivariant architectures in\ndiverse machine-learning contexts.\n", "link": "http://arxiv.org/abs/2408.08823v1", "date": "2024-08-16", "relevancy": 2.0883, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.424}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Symmetries%20in%20Binary%20Classification&body=Title%3A%20Optimal%20Symmetries%20in%20Binary%20Classification%0AAuthor%3A%20Vishal%20S.%20Ngairangbam%20and%20Michael%20Spannowsky%0AAbstract%3A%20%20%20We%20explore%20the%20role%20of%20group%20symmetries%20in%20binary%20classification%20tasks%2C%0Apresenting%20a%20novel%20framework%20that%20leverages%20the%20principles%20of%20Neyman-Pearson%0Aoptimality.%20Contrary%20to%20the%20common%20intuition%20that%20larger%20symmetry%20groups%20lead%0Ato%20improved%20classification%20performance%2C%20our%20findings%20show%20that%20selecting%20the%0Aappropriate%20group%20symmetries%20is%20crucial%20for%20optimising%20generalisation%20and%0Asample%20efficiency.%20We%20develop%20a%20theoretical%20foundation%20for%20designing%20group%0Aequivariant%20neural%20networks%20that%20align%20the%20choice%20of%20symmetries%20with%20the%0Aunderlying%20probability%20distributions%20of%20the%20data.%20Our%20approach%20provides%20a%0Aunified%20methodology%20for%20improving%20classification%20accuracy%20across%20a%20broad%20range%0Aof%20applications%20by%20carefully%20tailoring%20the%20symmetry%20group%20to%20the%20specific%0Acharacteristics%20of%20the%20problem.%20Theoretical%20analysis%20and%20experimental%20results%0Ademonstrate%20that%20optimal%20classification%20performance%20is%20not%20always%20associated%0Awith%20the%20largest%20equivariant%20groups%20possible%20in%20the%20domain%2C%20even%20when%20the%0Alikelihood%20ratio%20is%20invariant%20under%20one%20of%20its%20proper%20subgroups%2C%20but%20rather%0Awith%20those%20subgroups%20themselves.%20This%20work%20offers%20insights%20and%20practical%0Aguidelines%20for%20constructing%20more%20effective%20group%20equivariant%20architectures%20in%0Adiverse%20machine-learning%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Symmetries%2520in%2520Binary%2520Classification%26entry.906535625%3DVishal%2520S.%2520Ngairangbam%2520and%2520Michael%2520Spannowsky%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520role%2520of%2520group%2520symmetries%2520in%2520binary%2520classification%2520tasks%252C%250Apresenting%2520a%2520novel%2520framework%2520that%2520leverages%2520the%2520principles%2520of%2520Neyman-Pearson%250Aoptimality.%2520Contrary%2520to%2520the%2520common%2520intuition%2520that%2520larger%2520symmetry%2520groups%2520lead%250Ato%2520improved%2520classification%2520performance%252C%2520our%2520findings%2520show%2520that%2520selecting%2520the%250Aappropriate%2520group%2520symmetries%2520is%2520crucial%2520for%2520optimising%2520generalisation%2520and%250Asample%2520efficiency.%2520We%2520develop%2520a%2520theoretical%2520foundation%2520for%2520designing%2520group%250Aequivariant%2520neural%2520networks%2520that%2520align%2520the%2520choice%2520of%2520symmetries%2520with%2520the%250Aunderlying%2520probability%2520distributions%2520of%2520the%2520data.%2520Our%2520approach%2520provides%2520a%250Aunified%2520methodology%2520for%2520improving%2520classification%2520accuracy%2520across%2520a%2520broad%2520range%250Aof%2520applications%2520by%2520carefully%2520tailoring%2520the%2520symmetry%2520group%2520to%2520the%2520specific%250Acharacteristics%2520of%2520the%2520problem.%2520Theoretical%2520analysis%2520and%2520experimental%2520results%250Ademonstrate%2520that%2520optimal%2520classification%2520performance%2520is%2520not%2520always%2520associated%250Awith%2520the%2520largest%2520equivariant%2520groups%2520possible%2520in%2520the%2520domain%252C%2520even%2520when%2520the%250Alikelihood%2520ratio%2520is%2520invariant%2520under%2520one%2520of%2520its%2520proper%2520subgroups%252C%2520but%2520rather%250Awith%2520those%2520subgroups%2520themselves.%2520This%2520work%2520offers%2520insights%2520and%2520practical%250Aguidelines%2520for%2520constructing%2520more%2520effective%2520group%2520equivariant%2520architectures%2520in%250Adiverse%2520machine-learning%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Symmetries%20in%20Binary%20Classification&entry.906535625=Vishal%20S.%20Ngairangbam%20and%20Michael%20Spannowsky&entry.1292438233=%20%20We%20explore%20the%20role%20of%20group%20symmetries%20in%20binary%20classification%20tasks%2C%0Apresenting%20a%20novel%20framework%20that%20leverages%20the%20principles%20of%20Neyman-Pearson%0Aoptimality.%20Contrary%20to%20the%20common%20intuition%20that%20larger%20symmetry%20groups%20lead%0Ato%20improved%20classification%20performance%2C%20our%20findings%20show%20that%20selecting%20the%0Aappropriate%20group%20symmetries%20is%20crucial%20for%20optimising%20generalisation%20and%0Asample%20efficiency.%20We%20develop%20a%20theoretical%20foundation%20for%20designing%20group%0Aequivariant%20neural%20networks%20that%20align%20the%20choice%20of%20symmetries%20with%20the%0Aunderlying%20probability%20distributions%20of%20the%20data.%20Our%20approach%20provides%20a%0Aunified%20methodology%20for%20improving%20classification%20accuracy%20across%20a%20broad%20range%0Aof%20applications%20by%20carefully%20tailoring%20the%20symmetry%20group%20to%20the%20specific%0Acharacteristics%20of%20the%20problem.%20Theoretical%20analysis%20and%20experimental%20results%0Ademonstrate%20that%20optimal%20classification%20performance%20is%20not%20always%20associated%0Awith%20the%20largest%20equivariant%20groups%20possible%20in%20the%20domain%2C%20even%20when%20the%0Alikelihood%20ratio%20is%20invariant%20under%20one%20of%20its%20proper%20subgroups%2C%20but%20rather%0Awith%20those%20subgroups%20themselves.%20This%20work%20offers%20insights%20and%20practical%0Aguidelines%20for%20constructing%20more%20effective%20group%20equivariant%20architectures%20in%0Adiverse%20machine-learning%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08823v1&entry.124074799=Read"},
{"title": "MIMIR: Masked Image Modeling for Mutual Information-based Adversarial\n  Robustness", "author": "Xiaoyun Xu and Shujian Yu and Zhuoran Liu and Stjepan Picek", "abstract": "  Vision Transformers (ViTs) achieve excellent performance in various tasks,\nbut they are also vulnerable to adversarial attacks. Building robust ViTs is\nhighly dependent on dedicated Adversarial Training (AT) strategies. However,\ncurrent ViTs' adversarial training only employs well-established training\napproaches from convolutional neural network (CNN) training, where pre-training\nprovides the basis for AT fine-tuning with the additional help of tailored data\naugmentations. In this paper, we take a closer look at the adversarial\nrobustness of ViTs by providing a novel theoretical Mutual Information (MI)\nanalysis in its autoencoder-based self-supervised pre-training. Specifically,\nwe show that MI between the adversarial example and its latent representation\nin ViT-based autoencoders should be constrained by utilizing the MI bounds.\nBased on this finding, we propose a masked autoencoder-based pre-training\nmethod, MIMIR, that employs an MI penalty to facilitate the adversarial\ntraining of ViTs. Extensive experiments show that MIMIR outperforms\nstate-of-the-art adversarially trained ViTs on benchmark datasets with higher\nnatural and robust accuracy, indicating that ViTs can substantially benefit\nfrom exploiting MI. In addition, we consider two adaptive attacks by assuming\nthat the adversary is aware of the MIMIR design, which further verifies the\nprovided robustness.\n", "link": "http://arxiv.org/abs/2312.04960v3", "date": "2024-08-16", "relevancy": 2.0738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5129}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIMIR%3A%20Masked%20Image%20Modeling%20for%20Mutual%20Information-based%20Adversarial%0A%20%20Robustness&body=Title%3A%20MIMIR%3A%20Masked%20Image%20Modeling%20for%20Mutual%20Information-based%20Adversarial%0A%20%20Robustness%0AAuthor%3A%20Xiaoyun%20Xu%20and%20Shujian%20Yu%20and%20Zhuoran%20Liu%20and%20Stjepan%20Picek%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20achieve%20excellent%20performance%20in%20various%20tasks%2C%0Abut%20they%20are%20also%20vulnerable%20to%20adversarial%20attacks.%20Building%20robust%20ViTs%20is%0Ahighly%20dependent%20on%20dedicated%20Adversarial%20Training%20%28AT%29%20strategies.%20However%2C%0Acurrent%20ViTs%27%20adversarial%20training%20only%20employs%20well-established%20training%0Aapproaches%20from%20convolutional%20neural%20network%20%28CNN%29%20training%2C%20where%20pre-training%0Aprovides%20the%20basis%20for%20AT%20fine-tuning%20with%20the%20additional%20help%20of%20tailored%20data%0Aaugmentations.%20In%20this%20paper%2C%20we%20take%20a%20closer%20look%20at%20the%20adversarial%0Arobustness%20of%20ViTs%20by%20providing%20a%20novel%20theoretical%20Mutual%20Information%20%28MI%29%0Aanalysis%20in%20its%20autoencoder-based%20self-supervised%20pre-training.%20Specifically%2C%0Awe%20show%20that%20MI%20between%20the%20adversarial%20example%20and%20its%20latent%20representation%0Ain%20ViT-based%20autoencoders%20should%20be%20constrained%20by%20utilizing%20the%20MI%20bounds.%0ABased%20on%20this%20finding%2C%20we%20propose%20a%20masked%20autoencoder-based%20pre-training%0Amethod%2C%20MIMIR%2C%20that%20employs%20an%20MI%20penalty%20to%20facilitate%20the%20adversarial%0Atraining%20of%20ViTs.%20Extensive%20experiments%20show%20that%20MIMIR%20outperforms%0Astate-of-the-art%20adversarially%20trained%20ViTs%20on%20benchmark%20datasets%20with%20higher%0Anatural%20and%20robust%20accuracy%2C%20indicating%20that%20ViTs%20can%20substantially%20benefit%0Afrom%20exploiting%20MI.%20In%20addition%2C%20we%20consider%20two%20adaptive%20attacks%20by%20assuming%0Athat%20the%20adversary%20is%20aware%20of%20the%20MIMIR%20design%2C%20which%20further%20verifies%20the%0Aprovided%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04960v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIMIR%253A%2520Masked%2520Image%2520Modeling%2520for%2520Mutual%2520Information-based%2520Adversarial%250A%2520%2520Robustness%26entry.906535625%3DXiaoyun%2520Xu%2520and%2520Shujian%2520Yu%2520and%2520Zhuoran%2520Liu%2520and%2520Stjepan%2520Picek%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520achieve%2520excellent%2520performance%2520in%2520various%2520tasks%252C%250Abut%2520they%2520are%2520also%2520vulnerable%2520to%2520adversarial%2520attacks.%2520Building%2520robust%2520ViTs%2520is%250Ahighly%2520dependent%2520on%2520dedicated%2520Adversarial%2520Training%2520%2528AT%2529%2520strategies.%2520However%252C%250Acurrent%2520ViTs%2527%2520adversarial%2520training%2520only%2520employs%2520well-established%2520training%250Aapproaches%2520from%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520training%252C%2520where%2520pre-training%250Aprovides%2520the%2520basis%2520for%2520AT%2520fine-tuning%2520with%2520the%2520additional%2520help%2520of%2520tailored%2520data%250Aaugmentations.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520closer%2520look%2520at%2520the%2520adversarial%250Arobustness%2520of%2520ViTs%2520by%2520providing%2520a%2520novel%2520theoretical%2520Mutual%2520Information%2520%2528MI%2529%250Aanalysis%2520in%2520its%2520autoencoder-based%2520self-supervised%2520pre-training.%2520Specifically%252C%250Awe%2520show%2520that%2520MI%2520between%2520the%2520adversarial%2520example%2520and%2520its%2520latent%2520representation%250Ain%2520ViT-based%2520autoencoders%2520should%2520be%2520constrained%2520by%2520utilizing%2520the%2520MI%2520bounds.%250ABased%2520on%2520this%2520finding%252C%2520we%2520propose%2520a%2520masked%2520autoencoder-based%2520pre-training%250Amethod%252C%2520MIMIR%252C%2520that%2520employs%2520an%2520MI%2520penalty%2520to%2520facilitate%2520the%2520adversarial%250Atraining%2520of%2520ViTs.%2520Extensive%2520experiments%2520show%2520that%2520MIMIR%2520outperforms%250Astate-of-the-art%2520adversarially%2520trained%2520ViTs%2520on%2520benchmark%2520datasets%2520with%2520higher%250Anatural%2520and%2520robust%2520accuracy%252C%2520indicating%2520that%2520ViTs%2520can%2520substantially%2520benefit%250Afrom%2520exploiting%2520MI.%2520In%2520addition%252C%2520we%2520consider%2520two%2520adaptive%2520attacks%2520by%2520assuming%250Athat%2520the%2520adversary%2520is%2520aware%2520of%2520the%2520MIMIR%2520design%252C%2520which%2520further%2520verifies%2520the%250Aprovided%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04960v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIMIR%3A%20Masked%20Image%20Modeling%20for%20Mutual%20Information-based%20Adversarial%0A%20%20Robustness&entry.906535625=Xiaoyun%20Xu%20and%20Shujian%20Yu%20and%20Zhuoran%20Liu%20and%20Stjepan%20Picek&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20achieve%20excellent%20performance%20in%20various%20tasks%2C%0Abut%20they%20are%20also%20vulnerable%20to%20adversarial%20attacks.%20Building%20robust%20ViTs%20is%0Ahighly%20dependent%20on%20dedicated%20Adversarial%20Training%20%28AT%29%20strategies.%20However%2C%0Acurrent%20ViTs%27%20adversarial%20training%20only%20employs%20well-established%20training%0Aapproaches%20from%20convolutional%20neural%20network%20%28CNN%29%20training%2C%20where%20pre-training%0Aprovides%20the%20basis%20for%20AT%20fine-tuning%20with%20the%20additional%20help%20of%20tailored%20data%0Aaugmentations.%20In%20this%20paper%2C%20we%20take%20a%20closer%20look%20at%20the%20adversarial%0Arobustness%20of%20ViTs%20by%20providing%20a%20novel%20theoretical%20Mutual%20Information%20%28MI%29%0Aanalysis%20in%20its%20autoencoder-based%20self-supervised%20pre-training.%20Specifically%2C%0Awe%20show%20that%20MI%20between%20the%20adversarial%20example%20and%20its%20latent%20representation%0Ain%20ViT-based%20autoencoders%20should%20be%20constrained%20by%20utilizing%20the%20MI%20bounds.%0ABased%20on%20this%20finding%2C%20we%20propose%20a%20masked%20autoencoder-based%20pre-training%0Amethod%2C%20MIMIR%2C%20that%20employs%20an%20MI%20penalty%20to%20facilitate%20the%20adversarial%0Atraining%20of%20ViTs.%20Extensive%20experiments%20show%20that%20MIMIR%20outperforms%0Astate-of-the-art%20adversarially%20trained%20ViTs%20on%20benchmark%20datasets%20with%20higher%0Anatural%20and%20robust%20accuracy%2C%20indicating%20that%20ViTs%20can%20substantially%20benefit%0Afrom%20exploiting%20MI.%20In%20addition%2C%20we%20consider%20two%20adaptive%20attacks%20by%20assuming%0Athat%20the%20adversary%20is%20aware%20of%20the%20MIMIR%20design%2C%20which%20further%20verifies%20the%0Aprovided%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04960v3&entry.124074799=Read"},
{"title": "Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams", "author": "Cristiano Mesquita Garcia and Alessandro Lameiras Koerich and Alceu de Souza Britto Jr and Jean Paul Barddal", "abstract": "  The proliferation of textual data on the Internet presents a unique\nopportunity for institutions and companies to monitor public opinion about\ntheir services and products. Given the rapid generation of such data, the text\nstream mining setting, which handles sequentially arriving, potentially\ninfinite text streams, is often more suitable than traditional batch learning.\nWhile pre-trained language models are commonly employed for their high-quality\ntext vectorization capabilities in streaming contexts, they face challenges\nadapting to concept drift - the phenomenon where the data distribution changes\nover time, adversely affecting model performance. Addressing the issue of\nconcept drift, this study explores the efficacy of seven text sampling methods\ndesigned to selectively fine-tune language models, thereby mitigating\nperformance degradation. We precisely assess the impact of these methods on\nfine-tuning the SBERT model using four different loss functions. Our\nevaluation, focused on Macro F1-score and elapsed time, employs two text stream\ndatasets and an incremental SVM classifier to benchmark performance. Our\nfindings indicate that Softmax loss and Batch All Triplets loss are\nparticularly effective for text stream classification, demonstrating that\nlarger sample sizes generally correlate with improved macro F1-scores. Notably,\nour proposed WordPieceToken ratio sampling method significantly enhances\nperformance with the identified loss functions, surpassing baseline results.\n", "link": "http://arxiv.org/abs/2403.15455v2", "date": "2024-08-16", "relevancy": 2.0733, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5374}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Sampling%20Methods%20for%20Fine-tuning%20SentenceBERT%20in%20Text%20Streams&body=Title%3A%20Improving%20Sampling%20Methods%20for%20Fine-tuning%20SentenceBERT%20in%20Text%20Streams%0AAuthor%3A%20Cristiano%20Mesquita%20Garcia%20and%20Alessandro%20Lameiras%20Koerich%20and%20Alceu%20de%20Souza%20Britto%20Jr%20and%20Jean%20Paul%20Barddal%0AAbstract%3A%20%20%20The%20proliferation%20of%20textual%20data%20on%20the%20Internet%20presents%20a%20unique%0Aopportunity%20for%20institutions%20and%20companies%20to%20monitor%20public%20opinion%20about%0Atheir%20services%20and%20products.%20Given%20the%20rapid%20generation%20of%20such%20data%2C%20the%20text%0Astream%20mining%20setting%2C%20which%20handles%20sequentially%20arriving%2C%20potentially%0Ainfinite%20text%20streams%2C%20is%20often%20more%20suitable%20than%20traditional%20batch%20learning.%0AWhile%20pre-trained%20language%20models%20are%20commonly%20employed%20for%20their%20high-quality%0Atext%20vectorization%20capabilities%20in%20streaming%20contexts%2C%20they%20face%20challenges%0Aadapting%20to%20concept%20drift%20-%20the%20phenomenon%20where%20the%20data%20distribution%20changes%0Aover%20time%2C%20adversely%20affecting%20model%20performance.%20Addressing%20the%20issue%20of%0Aconcept%20drift%2C%20this%20study%20explores%20the%20efficacy%20of%20seven%20text%20sampling%20methods%0Adesigned%20to%20selectively%20fine-tune%20language%20models%2C%20thereby%20mitigating%0Aperformance%20degradation.%20We%20precisely%20assess%20the%20impact%20of%20these%20methods%20on%0Afine-tuning%20the%20SBERT%20model%20using%20four%20different%20loss%20functions.%20Our%0Aevaluation%2C%20focused%20on%20Macro%20F1-score%20and%20elapsed%20time%2C%20employs%20two%20text%20stream%0Adatasets%20and%20an%20incremental%20SVM%20classifier%20to%20benchmark%20performance.%20Our%0Afindings%20indicate%20that%20Softmax%20loss%20and%20Batch%20All%20Triplets%20loss%20are%0Aparticularly%20effective%20for%20text%20stream%20classification%2C%20demonstrating%20that%0Alarger%20sample%20sizes%20generally%20correlate%20with%20improved%20macro%20F1-scores.%20Notably%2C%0Aour%20proposed%20WordPieceToken%20ratio%20sampling%20method%20significantly%20enhances%0Aperformance%20with%20the%20identified%20loss%20functions%2C%20surpassing%20baseline%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15455v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Sampling%2520Methods%2520for%2520Fine-tuning%2520SentenceBERT%2520in%2520Text%2520Streams%26entry.906535625%3DCristiano%2520Mesquita%2520Garcia%2520and%2520Alessandro%2520Lameiras%2520Koerich%2520and%2520Alceu%2520de%2520Souza%2520Britto%2520Jr%2520and%2520Jean%2520Paul%2520Barddal%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520textual%2520data%2520on%2520the%2520Internet%2520presents%2520a%2520unique%250Aopportunity%2520for%2520institutions%2520and%2520companies%2520to%2520monitor%2520public%2520opinion%2520about%250Atheir%2520services%2520and%2520products.%2520Given%2520the%2520rapid%2520generation%2520of%2520such%2520data%252C%2520the%2520text%250Astream%2520mining%2520setting%252C%2520which%2520handles%2520sequentially%2520arriving%252C%2520potentially%250Ainfinite%2520text%2520streams%252C%2520is%2520often%2520more%2520suitable%2520than%2520traditional%2520batch%2520learning.%250AWhile%2520pre-trained%2520language%2520models%2520are%2520commonly%2520employed%2520for%2520their%2520high-quality%250Atext%2520vectorization%2520capabilities%2520in%2520streaming%2520contexts%252C%2520they%2520face%2520challenges%250Aadapting%2520to%2520concept%2520drift%2520-%2520the%2520phenomenon%2520where%2520the%2520data%2520distribution%2520changes%250Aover%2520time%252C%2520adversely%2520affecting%2520model%2520performance.%2520Addressing%2520the%2520issue%2520of%250Aconcept%2520drift%252C%2520this%2520study%2520explores%2520the%2520efficacy%2520of%2520seven%2520text%2520sampling%2520methods%250Adesigned%2520to%2520selectively%2520fine-tune%2520language%2520models%252C%2520thereby%2520mitigating%250Aperformance%2520degradation.%2520We%2520precisely%2520assess%2520the%2520impact%2520of%2520these%2520methods%2520on%250Afine-tuning%2520the%2520SBERT%2520model%2520using%2520four%2520different%2520loss%2520functions.%2520Our%250Aevaluation%252C%2520focused%2520on%2520Macro%2520F1-score%2520and%2520elapsed%2520time%252C%2520employs%2520two%2520text%2520stream%250Adatasets%2520and%2520an%2520incremental%2520SVM%2520classifier%2520to%2520benchmark%2520performance.%2520Our%250Afindings%2520indicate%2520that%2520Softmax%2520loss%2520and%2520Batch%2520All%2520Triplets%2520loss%2520are%250Aparticularly%2520effective%2520for%2520text%2520stream%2520classification%252C%2520demonstrating%2520that%250Alarger%2520sample%2520sizes%2520generally%2520correlate%2520with%2520improved%2520macro%2520F1-scores.%2520Notably%252C%250Aour%2520proposed%2520WordPieceToken%2520ratio%2520sampling%2520method%2520significantly%2520enhances%250Aperformance%2520with%2520the%2520identified%2520loss%2520functions%252C%2520surpassing%2520baseline%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15455v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Sampling%20Methods%20for%20Fine-tuning%20SentenceBERT%20in%20Text%20Streams&entry.906535625=Cristiano%20Mesquita%20Garcia%20and%20Alessandro%20Lameiras%20Koerich%20and%20Alceu%20de%20Souza%20Britto%20Jr%20and%20Jean%20Paul%20Barddal&entry.1292438233=%20%20The%20proliferation%20of%20textual%20data%20on%20the%20Internet%20presents%20a%20unique%0Aopportunity%20for%20institutions%20and%20companies%20to%20monitor%20public%20opinion%20about%0Atheir%20services%20and%20products.%20Given%20the%20rapid%20generation%20of%20such%20data%2C%20the%20text%0Astream%20mining%20setting%2C%20which%20handles%20sequentially%20arriving%2C%20potentially%0Ainfinite%20text%20streams%2C%20is%20often%20more%20suitable%20than%20traditional%20batch%20learning.%0AWhile%20pre-trained%20language%20models%20are%20commonly%20employed%20for%20their%20high-quality%0Atext%20vectorization%20capabilities%20in%20streaming%20contexts%2C%20they%20face%20challenges%0Aadapting%20to%20concept%20drift%20-%20the%20phenomenon%20where%20the%20data%20distribution%20changes%0Aover%20time%2C%20adversely%20affecting%20model%20performance.%20Addressing%20the%20issue%20of%0Aconcept%20drift%2C%20this%20study%20explores%20the%20efficacy%20of%20seven%20text%20sampling%20methods%0Adesigned%20to%20selectively%20fine-tune%20language%20models%2C%20thereby%20mitigating%0Aperformance%20degradation.%20We%20precisely%20assess%20the%20impact%20of%20these%20methods%20on%0Afine-tuning%20the%20SBERT%20model%20using%20four%20different%20loss%20functions.%20Our%0Aevaluation%2C%20focused%20on%20Macro%20F1-score%20and%20elapsed%20time%2C%20employs%20two%20text%20stream%0Adatasets%20and%20an%20incremental%20SVM%20classifier%20to%20benchmark%20performance.%20Our%0Afindings%20indicate%20that%20Softmax%20loss%20and%20Batch%20All%20Triplets%20loss%20are%0Aparticularly%20effective%20for%20text%20stream%20classification%2C%20demonstrating%20that%0Alarger%20sample%20sizes%20generally%20correlate%20with%20improved%20macro%20F1-scores.%20Notably%2C%0Aour%20proposed%20WordPieceToken%20ratio%20sampling%20method%20significantly%20enhances%0Aperformance%20with%20the%20identified%20loss%20functions%2C%20surpassing%20baseline%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15455v2&entry.124074799=Read"},
{"title": "Beyond the Hype: A dispassionate look at vision-language models in\n  medical scenario", "author": "Yang Nan and Huichi Zhou and Xiaodan Xing and Guang Yang", "abstract": "  Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nremarkable capabilities across diverse tasks, garnering significant attention\nin AI communities. However, their performance and reliability in specialized\ndomains such as medicine remain insufficiently assessed. In particular, most\nassessments over-concentrate in evaluating VLMs based on simple Visual Question\nAnswering (VQA) on multi-modality data, while ignoring the in-depth\ncharacteristic of LVLMs. In this study, we introduce RadVUQA, a novel\nRadiological Visual Understanding and Question Answering benchmark, to\ncomprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across\nfive dimensions: 1) Anatomical understanding, assessing the models' ability to\nvisually identify biological structures; 2) Multimodal comprehension, which\ninvolves the capability of interpreting linguistic and visual instructions to\nproduce desired outcomes; 3) Quantitative and spatial reasoning, evaluating the\nmodels' spatial awareness and proficiency in combining quantitative analysis\nwith visual and linguistic information; 4) Physiological knowledge, measuring\nthe models' capability to comprehend functions and mechanisms of organs and\nsystems; and 5) Robustness, which assesses the models' capabilities against\nunharmonised and synthetic data. The results indicate that both generalized\nLVLMs and medical-specific LVLMs have critical deficiencies with weak\nmultimodal comprehension and quantitative reasoning capabilities. Our findings\nreveal the large gap between existing LVLMs and clinicians, highlighting the\nurgent need for more robust and intelligent LVLMs. The code and dataset will be\navailable after the acceptance of this paper.\n", "link": "http://arxiv.org/abs/2408.08704v1", "date": "2024-08-16", "relevancy": 2.0709, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5126}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Hype%3A%20A%20dispassionate%20look%20at%20vision-language%20models%20in%0A%20%20medical%20scenario&body=Title%3A%20Beyond%20the%20Hype%3A%20A%20dispassionate%20look%20at%20vision-language%20models%20in%0A%20%20medical%20scenario%0AAuthor%3A%20Yang%20Nan%20and%20Huichi%20Zhou%20and%20Xiaodan%20Xing%20and%20Guang%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%0Aremarkable%20capabilities%20across%20diverse%20tasks%2C%20garnering%20significant%20attention%0Ain%20AI%20communities.%20However%2C%20their%20performance%20and%20reliability%20in%20specialized%0Adomains%20such%20as%20medicine%20remain%20insufficiently%20assessed.%20In%20particular%2C%20most%0Aassessments%20over-concentrate%20in%20evaluating%20VLMs%20based%20on%20simple%20Visual%20Question%0AAnswering%20%28VQA%29%20on%20multi-modality%20data%2C%20while%20ignoring%20the%20in-depth%0Acharacteristic%20of%20LVLMs.%20In%20this%20study%2C%20we%20introduce%20RadVUQA%2C%20a%20novel%0ARadiological%20Visual%20Understanding%20and%20Question%20Answering%20benchmark%2C%20to%0Acomprehensively%20evaluate%20existing%20LVLMs.%20RadVUQA%20mainly%20validates%20LVLMs%20across%0Afive%20dimensions%3A%201%29%20Anatomical%20understanding%2C%20assessing%20the%20models%27%20ability%20to%0Avisually%20identify%20biological%20structures%3B%202%29%20Multimodal%20comprehension%2C%20which%0Ainvolves%20the%20capability%20of%20interpreting%20linguistic%20and%20visual%20instructions%20to%0Aproduce%20desired%20outcomes%3B%203%29%20Quantitative%20and%20spatial%20reasoning%2C%20evaluating%20the%0Amodels%27%20spatial%20awareness%20and%20proficiency%20in%20combining%20quantitative%20analysis%0Awith%20visual%20and%20linguistic%20information%3B%204%29%20Physiological%20knowledge%2C%20measuring%0Athe%20models%27%20capability%20to%20comprehend%20functions%20and%20mechanisms%20of%20organs%20and%0Asystems%3B%20and%205%29%20Robustness%2C%20which%20assesses%20the%20models%27%20capabilities%20against%0Aunharmonised%20and%20synthetic%20data.%20The%20results%20indicate%20that%20both%20generalized%0ALVLMs%20and%20medical-specific%20LVLMs%20have%20critical%20deficiencies%20with%20weak%0Amultimodal%20comprehension%20and%20quantitative%20reasoning%20capabilities.%20Our%20findings%0Areveal%20the%20large%20gap%20between%20existing%20LVLMs%20and%20clinicians%2C%20highlighting%20the%0Aurgent%20need%20for%20more%20robust%20and%20intelligent%20LVLMs.%20The%20code%20and%20dataset%20will%20be%0Aavailable%20after%20the%20acceptance%20of%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Hype%253A%2520A%2520dispassionate%2520look%2520at%2520vision-language%2520models%2520in%250A%2520%2520medical%2520scenario%26entry.906535625%3DYang%2520Nan%2520and%2520Huichi%2520Zhou%2520and%2520Xiaodan%2520Xing%2520and%2520Guang%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%250Aremarkable%2520capabilities%2520across%2520diverse%2520tasks%252C%2520garnering%2520significant%2520attention%250Ain%2520AI%2520communities.%2520However%252C%2520their%2520performance%2520and%2520reliability%2520in%2520specialized%250Adomains%2520such%2520as%2520medicine%2520remain%2520insufficiently%2520assessed.%2520In%2520particular%252C%2520most%250Aassessments%2520over-concentrate%2520in%2520evaluating%2520VLMs%2520based%2520on%2520simple%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520on%2520multi-modality%2520data%252C%2520while%2520ignoring%2520the%2520in-depth%250Acharacteristic%2520of%2520LVLMs.%2520In%2520this%2520study%252C%2520we%2520introduce%2520RadVUQA%252C%2520a%2520novel%250ARadiological%2520Visual%2520Understanding%2520and%2520Question%2520Answering%2520benchmark%252C%2520to%250Acomprehensively%2520evaluate%2520existing%2520LVLMs.%2520RadVUQA%2520mainly%2520validates%2520LVLMs%2520across%250Afive%2520dimensions%253A%25201%2529%2520Anatomical%2520understanding%252C%2520assessing%2520the%2520models%2527%2520ability%2520to%250Avisually%2520identify%2520biological%2520structures%253B%25202%2529%2520Multimodal%2520comprehension%252C%2520which%250Ainvolves%2520the%2520capability%2520of%2520interpreting%2520linguistic%2520and%2520visual%2520instructions%2520to%250Aproduce%2520desired%2520outcomes%253B%25203%2529%2520Quantitative%2520and%2520spatial%2520reasoning%252C%2520evaluating%2520the%250Amodels%2527%2520spatial%2520awareness%2520and%2520proficiency%2520in%2520combining%2520quantitative%2520analysis%250Awith%2520visual%2520and%2520linguistic%2520information%253B%25204%2529%2520Physiological%2520knowledge%252C%2520measuring%250Athe%2520models%2527%2520capability%2520to%2520comprehend%2520functions%2520and%2520mechanisms%2520of%2520organs%2520and%250Asystems%253B%2520and%25205%2529%2520Robustness%252C%2520which%2520assesses%2520the%2520models%2527%2520capabilities%2520against%250Aunharmonised%2520and%2520synthetic%2520data.%2520The%2520results%2520indicate%2520that%2520both%2520generalized%250ALVLMs%2520and%2520medical-specific%2520LVLMs%2520have%2520critical%2520deficiencies%2520with%2520weak%250Amultimodal%2520comprehension%2520and%2520quantitative%2520reasoning%2520capabilities.%2520Our%2520findings%250Areveal%2520the%2520large%2520gap%2520between%2520existing%2520LVLMs%2520and%2520clinicians%252C%2520highlighting%2520the%250Aurgent%2520need%2520for%2520more%2520robust%2520and%2520intelligent%2520LVLMs.%2520The%2520code%2520and%2520dataset%2520will%2520be%250Aavailable%2520after%2520the%2520acceptance%2520of%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Hype%3A%20A%20dispassionate%20look%20at%20vision-language%20models%20in%0A%20%20medical%20scenario&entry.906535625=Yang%20Nan%20and%20Huichi%20Zhou%20and%20Xiaodan%20Xing%20and%20Guang%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%0Aremarkable%20capabilities%20across%20diverse%20tasks%2C%20garnering%20significant%20attention%0Ain%20AI%20communities.%20However%2C%20their%20performance%20and%20reliability%20in%20specialized%0Adomains%20such%20as%20medicine%20remain%20insufficiently%20assessed.%20In%20particular%2C%20most%0Aassessments%20over-concentrate%20in%20evaluating%20VLMs%20based%20on%20simple%20Visual%20Question%0AAnswering%20%28VQA%29%20on%20multi-modality%20data%2C%20while%20ignoring%20the%20in-depth%0Acharacteristic%20of%20LVLMs.%20In%20this%20study%2C%20we%20introduce%20RadVUQA%2C%20a%20novel%0ARadiological%20Visual%20Understanding%20and%20Question%20Answering%20benchmark%2C%20to%0Acomprehensively%20evaluate%20existing%20LVLMs.%20RadVUQA%20mainly%20validates%20LVLMs%20across%0Afive%20dimensions%3A%201%29%20Anatomical%20understanding%2C%20assessing%20the%20models%27%20ability%20to%0Avisually%20identify%20biological%20structures%3B%202%29%20Multimodal%20comprehension%2C%20which%0Ainvolves%20the%20capability%20of%20interpreting%20linguistic%20and%20visual%20instructions%20to%0Aproduce%20desired%20outcomes%3B%203%29%20Quantitative%20and%20spatial%20reasoning%2C%20evaluating%20the%0Amodels%27%20spatial%20awareness%20and%20proficiency%20in%20combining%20quantitative%20analysis%0Awith%20visual%20and%20linguistic%20information%3B%204%29%20Physiological%20knowledge%2C%20measuring%0Athe%20models%27%20capability%20to%20comprehend%20functions%20and%20mechanisms%20of%20organs%20and%0Asystems%3B%20and%205%29%20Robustness%2C%20which%20assesses%20the%20models%27%20capabilities%20against%0Aunharmonised%20and%20synthetic%20data.%20The%20results%20indicate%20that%20both%20generalized%0ALVLMs%20and%20medical-specific%20LVLMs%20have%20critical%20deficiencies%20with%20weak%0Amultimodal%20comprehension%20and%20quantitative%20reasoning%20capabilities.%20Our%20findings%0Areveal%20the%20large%20gap%20between%20existing%20LVLMs%20and%20clinicians%2C%20highlighting%20the%0Aurgent%20need%20for%20more%20robust%20and%20intelligent%20LVLMs.%20The%20code%20and%20dataset%20will%20be%0Aavailable%20after%20the%20acceptance%20of%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08704v1&entry.124074799=Read"},
{"title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification", "author": "Martin Juan Jos\u00e9 Bucher and Marco Martini", "abstract": "  Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort.\n", "link": "http://arxiv.org/abs/2406.08660v2", "date": "2024-08-16", "relevancy": 2.0691, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5209}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuned%20%27Small%27%20LLMs%20%28Still%29%20Significantly%20Outperform%20Zero-Shot%0A%20%20Generative%20AI%20Models%20in%20Text%20Classification&body=Title%3A%20Fine-Tuned%20%27Small%27%20LLMs%20%28Still%29%20Significantly%20Outperform%20Zero-Shot%0A%20%20Generative%20AI%20Models%20in%20Text%20Classification%0AAuthor%3A%20Martin%20Juan%20Jos%C3%A9%20Bucher%20and%20Marco%20Martini%0AAbstract%3A%20%20%20Generative%20AI%20offers%20a%20simple%2C%20prompt-based%20alternative%20to%20fine-tuning%0Asmaller%20BERT-style%20LLMs%20for%20text%20classification%20tasks.%20This%20promises%20to%0Aeliminate%20the%20need%20for%20manually%20labeled%20training%20data%20and%20task-specific%20model%0Atraining.%20However%2C%20it%20remains%20an%20open%20question%20whether%20tools%20like%20ChatGPT%20can%0Adeliver%20on%20this%20promise.%20In%20this%20paper%2C%20we%20show%20that%20smaller%2C%20fine-tuned%20LLMs%0A%28still%29%20consistently%20and%20significantly%20outperform%20larger%2C%20zero-shot%20prompted%0Amodels%20in%20text%20classification.%20We%20compare%20three%20major%20generative%20AI%20models%0A%28ChatGPT%20with%20GPT-3.5/GPT-4%20and%20Claude%20Opus%29%20with%20several%20fine-tuned%20LLMs%0Aacross%20a%20diverse%20set%20of%20classification%20tasks%20%28sentiment%2C%20approval/disapproval%2C%0Aemotions%2C%20party%20positions%29%20and%20text%20categories%20%28news%2C%20tweets%2C%20speeches%29.%20We%0Afind%20that%20fine-tuning%20with%20application-specific%20training%20data%20achieves%20superior%0Aperformance%20in%20all%20cases.%20To%20make%20this%20approach%20more%20accessible%20to%20a%20broader%0Aaudience%2C%20we%20provide%20an%20easy-to-use%20toolkit%20alongside%20this%20paper.%20Our%20toolkit%2C%0Aaccompanied%20by%20non-technical%20step-by-step%20guidance%2C%20enables%20users%20to%20select%20and%0Afine-tune%20BERT-like%20LLMs%20for%20any%20classification%20task%20with%20minimal%20technical%20and%0Acomputational%20effort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuned%2520%2527Small%2527%2520LLMs%2520%2528Still%2529%2520Significantly%2520Outperform%2520Zero-Shot%250A%2520%2520Generative%2520AI%2520Models%2520in%2520Text%2520Classification%26entry.906535625%3DMartin%2520Juan%2520Jos%25C3%25A9%2520Bucher%2520and%2520Marco%2520Martini%26entry.1292438233%3D%2520%2520Generative%2520AI%2520offers%2520a%2520simple%252C%2520prompt-based%2520alternative%2520to%2520fine-tuning%250Asmaller%2520BERT-style%2520LLMs%2520for%2520text%2520classification%2520tasks.%2520This%2520promises%2520to%250Aeliminate%2520the%2520need%2520for%2520manually%2520labeled%2520training%2520data%2520and%2520task-specific%2520model%250Atraining.%2520However%252C%2520it%2520remains%2520an%2520open%2520question%2520whether%2520tools%2520like%2520ChatGPT%2520can%250Adeliver%2520on%2520this%2520promise.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520smaller%252C%2520fine-tuned%2520LLMs%250A%2528still%2529%2520consistently%2520and%2520significantly%2520outperform%2520larger%252C%2520zero-shot%2520prompted%250Amodels%2520in%2520text%2520classification.%2520We%2520compare%2520three%2520major%2520generative%2520AI%2520models%250A%2528ChatGPT%2520with%2520GPT-3.5/GPT-4%2520and%2520Claude%2520Opus%2529%2520with%2520several%2520fine-tuned%2520LLMs%250Aacross%2520a%2520diverse%2520set%2520of%2520classification%2520tasks%2520%2528sentiment%252C%2520approval/disapproval%252C%250Aemotions%252C%2520party%2520positions%2529%2520and%2520text%2520categories%2520%2528news%252C%2520tweets%252C%2520speeches%2529.%2520We%250Afind%2520that%2520fine-tuning%2520with%2520application-specific%2520training%2520data%2520achieves%2520superior%250Aperformance%2520in%2520all%2520cases.%2520To%2520make%2520this%2520approach%2520more%2520accessible%2520to%2520a%2520broader%250Aaudience%252C%2520we%2520provide%2520an%2520easy-to-use%2520toolkit%2520alongside%2520this%2520paper.%2520Our%2520toolkit%252C%250Aaccompanied%2520by%2520non-technical%2520step-by-step%2520guidance%252C%2520enables%2520users%2520to%2520select%2520and%250Afine-tune%2520BERT-like%2520LLMs%2520for%2520any%2520classification%2520task%2520with%2520minimal%2520technical%2520and%250Acomputational%2520effort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuned%20%27Small%27%20LLMs%20%28Still%29%20Significantly%20Outperform%20Zero-Shot%0A%20%20Generative%20AI%20Models%20in%20Text%20Classification&entry.906535625=Martin%20Juan%20Jos%C3%A9%20Bucher%20and%20Marco%20Martini&entry.1292438233=%20%20Generative%20AI%20offers%20a%20simple%2C%20prompt-based%20alternative%20to%20fine-tuning%0Asmaller%20BERT-style%20LLMs%20for%20text%20classification%20tasks.%20This%20promises%20to%0Aeliminate%20the%20need%20for%20manually%20labeled%20training%20data%20and%20task-specific%20model%0Atraining.%20However%2C%20it%20remains%20an%20open%20question%20whether%20tools%20like%20ChatGPT%20can%0Adeliver%20on%20this%20promise.%20In%20this%20paper%2C%20we%20show%20that%20smaller%2C%20fine-tuned%20LLMs%0A%28still%29%20consistently%20and%20significantly%20outperform%20larger%2C%20zero-shot%20prompted%0Amodels%20in%20text%20classification.%20We%20compare%20three%20major%20generative%20AI%20models%0A%28ChatGPT%20with%20GPT-3.5/GPT-4%20and%20Claude%20Opus%29%20with%20several%20fine-tuned%20LLMs%0Aacross%20a%20diverse%20set%20of%20classification%20tasks%20%28sentiment%2C%20approval/disapproval%2C%0Aemotions%2C%20party%20positions%29%20and%20text%20categories%20%28news%2C%20tweets%2C%20speeches%29.%20We%0Afind%20that%20fine-tuning%20with%20application-specific%20training%20data%20achieves%20superior%0Aperformance%20in%20all%20cases.%20To%20make%20this%20approach%20more%20accessible%20to%20a%20broader%0Aaudience%2C%20we%20provide%20an%20easy-to-use%20toolkit%20alongside%20this%20paper.%20Our%20toolkit%2C%0Aaccompanied%20by%20non-technical%20step-by-step%20guidance%2C%20enables%20users%20to%20select%20and%0Afine-tune%20BERT-like%20LLMs%20for%20any%20classification%20task%20with%20minimal%20technical%20and%0Acomputational%20effort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08660v2&entry.124074799=Read"},
{"title": "Rethinking of Encoder-based Warm-start Methods in Hyperparameter\n  Optimization", "author": "Dawid P\u0142udowski and Antoni Zajko and Anna Kozak and Katarzyna Wo\u017anica", "abstract": "  Effectively representing heterogeneous tabular datasets for meta-learning\npurposes remains an open problem. Previous approaches rely on predefined\nmeta-features, for example, statistical measures or landmarkers. The emergence\nof dataset encoders opens new possibilities for the extraction of meta-features\nbecause they do not involve any handmade design. Moreover, they are proven to\ngenerate dataset representations with desired spatial properties. In this\nresearch, we evaluate an encoder-based approach to one of the most established\nmeta-tasks - warm-starting of the Bayesian Hyperparameter Optimization. To\nbroaden our analysis we introduce a new approach for representation learning on\ntabular data based on [Tomoharu Iwata and Atsutoshi Kumagai. Meta-learning from\nTasks with Heterogeneous Attribute Spaces. In Advances in Neural Information\nProcessing Systems, 2020]. The validation on over 100 datasets from UCI and an\nindependent metaMIMIC set of datasets highlights the nuanced challenges in\nrepresentation learning. We show that general representations may not suffice\nfor some meta-tasks where requirements are not explicitly considered during\nextraction.\n", "link": "http://arxiv.org/abs/2403.04720v4", "date": "2024-08-16", "relevancy": 2.0683, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5364}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization&body=Title%3A%20Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization%0AAuthor%3A%20Dawid%20P%C5%82udowski%20and%20Antoni%20Zajko%20and%20Anna%20Kozak%20and%20Katarzyna%20Wo%C5%BAnica%0AAbstract%3A%20%20%20Effectively%20representing%20heterogeneous%20tabular%20datasets%20for%20meta-learning%0Apurposes%20remains%20an%20open%20problem.%20Previous%20approaches%20rely%20on%20predefined%0Ameta-features%2C%20for%20example%2C%20statistical%20measures%20or%20landmarkers.%20The%20emergence%0Aof%20dataset%20encoders%20opens%20new%20possibilities%20for%20the%20extraction%20of%20meta-features%0Abecause%20they%20do%20not%20involve%20any%20handmade%20design.%20Moreover%2C%20they%20are%20proven%20to%0Agenerate%20dataset%20representations%20with%20desired%20spatial%20properties.%20In%20this%0Aresearch%2C%20we%20evaluate%20an%20encoder-based%20approach%20to%20one%20of%20the%20most%20established%0Ameta-tasks%20-%20warm-starting%20of%20the%20Bayesian%20Hyperparameter%20Optimization.%20To%0Abroaden%20our%20analysis%20we%20introduce%20a%20new%20approach%20for%20representation%20learning%20on%0Atabular%20data%20based%20on%20%5BTomoharu%20Iwata%20and%20Atsutoshi%20Kumagai.%20Meta-learning%20from%0ATasks%20with%20Heterogeneous%20Attribute%20Spaces.%20In%20Advances%20in%20Neural%20Information%0AProcessing%20Systems%2C%202020%5D.%20The%20validation%20on%20over%20100%20datasets%20from%20UCI%20and%20an%0Aindependent%20metaMIMIC%20set%20of%20datasets%20highlights%20the%20nuanced%20challenges%20in%0Arepresentation%20learning.%20We%20show%20that%20general%20representations%20may%20not%20suffice%0Afor%20some%20meta-tasks%20where%20requirements%20are%20not%20explicitly%20considered%20during%0Aextraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04720v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520of%2520Encoder-based%2520Warm-start%2520Methods%2520in%2520Hyperparameter%250A%2520%2520Optimization%26entry.906535625%3DDawid%2520P%25C5%2582udowski%2520and%2520Antoni%2520Zajko%2520and%2520Anna%2520Kozak%2520and%2520Katarzyna%2520Wo%25C5%25BAnica%26entry.1292438233%3D%2520%2520Effectively%2520representing%2520heterogeneous%2520tabular%2520datasets%2520for%2520meta-learning%250Apurposes%2520remains%2520an%2520open%2520problem.%2520Previous%2520approaches%2520rely%2520on%2520predefined%250Ameta-features%252C%2520for%2520example%252C%2520statistical%2520measures%2520or%2520landmarkers.%2520The%2520emergence%250Aof%2520dataset%2520encoders%2520opens%2520new%2520possibilities%2520for%2520the%2520extraction%2520of%2520meta-features%250Abecause%2520they%2520do%2520not%2520involve%2520any%2520handmade%2520design.%2520Moreover%252C%2520they%2520are%2520proven%2520to%250Agenerate%2520dataset%2520representations%2520with%2520desired%2520spatial%2520properties.%2520In%2520this%250Aresearch%252C%2520we%2520evaluate%2520an%2520encoder-based%2520approach%2520to%2520one%2520of%2520the%2520most%2520established%250Ameta-tasks%2520-%2520warm-starting%2520of%2520the%2520Bayesian%2520Hyperparameter%2520Optimization.%2520To%250Abroaden%2520our%2520analysis%2520we%2520introduce%2520a%2520new%2520approach%2520for%2520representation%2520learning%2520on%250Atabular%2520data%2520based%2520on%2520%255BTomoharu%2520Iwata%2520and%2520Atsutoshi%2520Kumagai.%2520Meta-learning%2520from%250ATasks%2520with%2520Heterogeneous%2520Attribute%2520Spaces.%2520In%2520Advances%2520in%2520Neural%2520Information%250AProcessing%2520Systems%252C%25202020%255D.%2520The%2520validation%2520on%2520over%2520100%2520datasets%2520from%2520UCI%2520and%2520an%250Aindependent%2520metaMIMIC%2520set%2520of%2520datasets%2520highlights%2520the%2520nuanced%2520challenges%2520in%250Arepresentation%2520learning.%2520We%2520show%2520that%2520general%2520representations%2520may%2520not%2520suffice%250Afor%2520some%2520meta-tasks%2520where%2520requirements%2520are%2520not%2520explicitly%2520considered%2520during%250Aextraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04720v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization&entry.906535625=Dawid%20P%C5%82udowski%20and%20Antoni%20Zajko%20and%20Anna%20Kozak%20and%20Katarzyna%20Wo%C5%BAnica&entry.1292438233=%20%20Effectively%20representing%20heterogeneous%20tabular%20datasets%20for%20meta-learning%0Apurposes%20remains%20an%20open%20problem.%20Previous%20approaches%20rely%20on%20predefined%0Ameta-features%2C%20for%20example%2C%20statistical%20measures%20or%20landmarkers.%20The%20emergence%0Aof%20dataset%20encoders%20opens%20new%20possibilities%20for%20the%20extraction%20of%20meta-features%0Abecause%20they%20do%20not%20involve%20any%20handmade%20design.%20Moreover%2C%20they%20are%20proven%20to%0Agenerate%20dataset%20representations%20with%20desired%20spatial%20properties.%20In%20this%0Aresearch%2C%20we%20evaluate%20an%20encoder-based%20approach%20to%20one%20of%20the%20most%20established%0Ameta-tasks%20-%20warm-starting%20of%20the%20Bayesian%20Hyperparameter%20Optimization.%20To%0Abroaden%20our%20analysis%20we%20introduce%20a%20new%20approach%20for%20representation%20learning%20on%0Atabular%20data%20based%20on%20%5BTomoharu%20Iwata%20and%20Atsutoshi%20Kumagai.%20Meta-learning%20from%0ATasks%20with%20Heterogeneous%20Attribute%20Spaces.%20In%20Advances%20in%20Neural%20Information%0AProcessing%20Systems%2C%202020%5D.%20The%20validation%20on%20over%20100%20datasets%20from%20UCI%20and%20an%0Aindependent%20metaMIMIC%20set%20of%20datasets%20highlights%20the%20nuanced%20challenges%20in%0Arepresentation%20learning.%20We%20show%20that%20general%20representations%20may%20not%20suffice%0Afor%20some%20meta-tasks%20where%20requirements%20are%20not%20explicitly%20considered%20during%0Aextraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04720v4&entry.124074799=Read"},
{"title": "LEVIS: Large Exact Verifiable Input Spaces for Neural Networks", "author": "Mohamad Fares El Hajj Chehade and Brian Wesley Bell and Russell Bent and Hao Zhu and Wenting Li", "abstract": "  The robustness of neural networks is paramount in safety-critical\napplications. While most current robustness verification methods assess the\nworst-case output under the assumption that the input space is known,\nidentifying a verifiable input space $\\mathcal{C}$, where no adversarial\nexamples exist, is crucial for effective model selection, robustness\nevaluation, and the development of reliable control strategies. To address this\nchallenge, we introduce a novel framework, $\\texttt{LEVIS}$, comprising\n$\\texttt{LEVIS}$-$\\alpha$ and $\\texttt{LEVIS}$-$\\beta$.\n$\\texttt{LEVIS}$-$\\alpha$ locates the largest possible verifiable ball within\nthe central region of $\\mathcal{C}$ that intersects at least two boundaries. In\ncontrast, $\\texttt{LEVIS}$-$\\beta$ integrates multiple verifiable balls to\nencapsulate the entirety of the verifiable space comprehensively. Our\ncontributions are threefold: (1) We propose $\\texttt{LEVIS}$ equipped with\nthree pioneering techniques that identify the maximum verifiable ball and the\nnearest adversarial point along collinear or orthogonal directions. (2) We\noffer a theoretical analysis elucidating the properties of the verifiable balls\nacquired through $\\texttt{LEVIS}$-$\\alpha$ and $\\texttt{LEVIS}$-$\\beta$. (3) We\nvalidate our methodology across diverse applications, including electrical\npower flow regression and image classification, showcasing performance\nenhancements and visualizations of the searching characteristics.\n", "link": "http://arxiv.org/abs/2408.08824v1", "date": "2024-08-16", "relevancy": 2.0659, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEVIS%3A%20Large%20Exact%20Verifiable%20Input%20Spaces%20for%20Neural%20Networks&body=Title%3A%20LEVIS%3A%20Large%20Exact%20Verifiable%20Input%20Spaces%20for%20Neural%20Networks%0AAuthor%3A%20Mohamad%20Fares%20El%20Hajj%20Chehade%20and%20Brian%20Wesley%20Bell%20and%20Russell%20Bent%20and%20Hao%20Zhu%20and%20Wenting%20Li%0AAbstract%3A%20%20%20The%20robustness%20of%20neural%20networks%20is%20paramount%20in%20safety-critical%0Aapplications.%20While%20most%20current%20robustness%20verification%20methods%20assess%20the%0Aworst-case%20output%20under%20the%20assumption%20that%20the%20input%20space%20is%20known%2C%0Aidentifying%20a%20verifiable%20input%20space%20%24%5Cmathcal%7BC%7D%24%2C%20where%20no%20adversarial%0Aexamples%20exist%2C%20is%20crucial%20for%20effective%20model%20selection%2C%20robustness%0Aevaluation%2C%20and%20the%20development%20of%20reliable%20control%20strategies.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20framework%2C%20%24%5Ctexttt%7BLEVIS%7D%24%2C%20comprising%0A%24%5Ctexttt%7BLEVIS%7D%24-%24%5Calpha%24%20and%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Cbeta%24.%0A%24%5Ctexttt%7BLEVIS%7D%24-%24%5Calpha%24%20locates%20the%20largest%20possible%20verifiable%20ball%20within%0Athe%20central%20region%20of%20%24%5Cmathcal%7BC%7D%24%20that%20intersects%20at%20least%20two%20boundaries.%20In%0Acontrast%2C%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Cbeta%24%20integrates%20multiple%20verifiable%20balls%20to%0Aencapsulate%20the%20entirety%20of%20the%20verifiable%20space%20comprehensively.%20Our%0Acontributions%20are%20threefold%3A%20%281%29%20We%20propose%20%24%5Ctexttt%7BLEVIS%7D%24%20equipped%20with%0Athree%20pioneering%20techniques%20that%20identify%20the%20maximum%20verifiable%20ball%20and%20the%0Anearest%20adversarial%20point%20along%20collinear%20or%20orthogonal%20directions.%20%282%29%20We%0Aoffer%20a%20theoretical%20analysis%20elucidating%20the%20properties%20of%20the%20verifiable%20balls%0Aacquired%20through%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Calpha%24%20and%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Cbeta%24.%20%283%29%20We%0Avalidate%20our%20methodology%20across%20diverse%20applications%2C%20including%20electrical%0Apower%20flow%20regression%20and%20image%20classification%2C%20showcasing%20performance%0Aenhancements%20and%20visualizations%20of%20the%20searching%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEVIS%253A%2520Large%2520Exact%2520Verifiable%2520Input%2520Spaces%2520for%2520Neural%2520Networks%26entry.906535625%3DMohamad%2520Fares%2520El%2520Hajj%2520Chehade%2520and%2520Brian%2520Wesley%2520Bell%2520and%2520Russell%2520Bent%2520and%2520Hao%2520Zhu%2520and%2520Wenting%2520Li%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520neural%2520networks%2520is%2520paramount%2520in%2520safety-critical%250Aapplications.%2520While%2520most%2520current%2520robustness%2520verification%2520methods%2520assess%2520the%250Aworst-case%2520output%2520under%2520the%2520assumption%2520that%2520the%2520input%2520space%2520is%2520known%252C%250Aidentifying%2520a%2520verifiable%2520input%2520space%2520%2524%255Cmathcal%257BC%257D%2524%252C%2520where%2520no%2520adversarial%250Aexamples%2520exist%252C%2520is%2520crucial%2520for%2520effective%2520model%2520selection%252C%2520robustness%250Aevaluation%252C%2520and%2520the%2520development%2520of%2520reliable%2520control%2520strategies.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%2520%2524%255Ctexttt%257BLEVIS%257D%2524%252C%2520comprising%250A%2524%255Ctexttt%257BLEVIS%257D%2524-%2524%255Calpha%2524%2520and%2520%2524%255Ctexttt%257BLEVIS%257D%2524-%2524%255Cbeta%2524.%250A%2524%255Ctexttt%257BLEVIS%257D%2524-%2524%255Calpha%2524%2520locates%2520the%2520largest%2520possible%2520verifiable%2520ball%2520within%250Athe%2520central%2520region%2520of%2520%2524%255Cmathcal%257BC%257D%2524%2520that%2520intersects%2520at%2520least%2520two%2520boundaries.%2520In%250Acontrast%252C%2520%2524%255Ctexttt%257BLEVIS%257D%2524-%2524%255Cbeta%2524%2520integrates%2520multiple%2520verifiable%2520balls%2520to%250Aencapsulate%2520the%2520entirety%2520of%2520the%2520verifiable%2520space%2520comprehensively.%2520Our%250Acontributions%2520are%2520threefold%253A%2520%25281%2529%2520We%2520propose%2520%2524%255Ctexttt%257BLEVIS%257D%2524%2520equipped%2520with%250Athree%2520pioneering%2520techniques%2520that%2520identify%2520the%2520maximum%2520verifiable%2520ball%2520and%2520the%250Anearest%2520adversarial%2520point%2520along%2520collinear%2520or%2520orthogonal%2520directions.%2520%25282%2529%2520We%250Aoffer%2520a%2520theoretical%2520analysis%2520elucidating%2520the%2520properties%2520of%2520the%2520verifiable%2520balls%250Aacquired%2520through%2520%2524%255Ctexttt%257BLEVIS%257D%2524-%2524%255Calpha%2524%2520and%2520%2524%255Ctexttt%257BLEVIS%257D%2524-%2524%255Cbeta%2524.%2520%25283%2529%2520We%250Avalidate%2520our%2520methodology%2520across%2520diverse%2520applications%252C%2520including%2520electrical%250Apower%2520flow%2520regression%2520and%2520image%2520classification%252C%2520showcasing%2520performance%250Aenhancements%2520and%2520visualizations%2520of%2520the%2520searching%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEVIS%3A%20Large%20Exact%20Verifiable%20Input%20Spaces%20for%20Neural%20Networks&entry.906535625=Mohamad%20Fares%20El%20Hajj%20Chehade%20and%20Brian%20Wesley%20Bell%20and%20Russell%20Bent%20and%20Hao%20Zhu%20and%20Wenting%20Li&entry.1292438233=%20%20The%20robustness%20of%20neural%20networks%20is%20paramount%20in%20safety-critical%0Aapplications.%20While%20most%20current%20robustness%20verification%20methods%20assess%20the%0Aworst-case%20output%20under%20the%20assumption%20that%20the%20input%20space%20is%20known%2C%0Aidentifying%20a%20verifiable%20input%20space%20%24%5Cmathcal%7BC%7D%24%2C%20where%20no%20adversarial%0Aexamples%20exist%2C%20is%20crucial%20for%20effective%20model%20selection%2C%20robustness%0Aevaluation%2C%20and%20the%20development%20of%20reliable%20control%20strategies.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20framework%2C%20%24%5Ctexttt%7BLEVIS%7D%24%2C%20comprising%0A%24%5Ctexttt%7BLEVIS%7D%24-%24%5Calpha%24%20and%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Cbeta%24.%0A%24%5Ctexttt%7BLEVIS%7D%24-%24%5Calpha%24%20locates%20the%20largest%20possible%20verifiable%20ball%20within%0Athe%20central%20region%20of%20%24%5Cmathcal%7BC%7D%24%20that%20intersects%20at%20least%20two%20boundaries.%20In%0Acontrast%2C%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Cbeta%24%20integrates%20multiple%20verifiable%20balls%20to%0Aencapsulate%20the%20entirety%20of%20the%20verifiable%20space%20comprehensively.%20Our%0Acontributions%20are%20threefold%3A%20%281%29%20We%20propose%20%24%5Ctexttt%7BLEVIS%7D%24%20equipped%20with%0Athree%20pioneering%20techniques%20that%20identify%20the%20maximum%20verifiable%20ball%20and%20the%0Anearest%20adversarial%20point%20along%20collinear%20or%20orthogonal%20directions.%20%282%29%20We%0Aoffer%20a%20theoretical%20analysis%20elucidating%20the%20properties%20of%20the%20verifiable%20balls%0Aacquired%20through%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Calpha%24%20and%20%24%5Ctexttt%7BLEVIS%7D%24-%24%5Cbeta%24.%20%283%29%20We%0Avalidate%20our%20methodology%20across%20diverse%20applications%2C%20including%20electrical%0Apower%20flow%20regression%20and%20image%20classification%2C%20showcasing%20performance%0Aenhancements%20and%20visualizations%20of%20the%20searching%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08824v1&entry.124074799=Read"},
{"title": "Indirectly Parameterized Concrete Autoencoders", "author": "Alfred Nilsson and Klas Wijk and Sai bharath chandra Gutha and Erik Englesson and Alexandra Hotti and Carlo Saccardi and Oskar Kviman and Jens Lagergren and Ricardo Vinuesa and Hossein Azizpour", "abstract": "  Feature selection is a crucial task in settings where data is\nhigh-dimensional or acquiring the full set of features is costly. Recent\ndevelopments in neural network-based embedded feature selection show promising\nresults across a wide range of applications. Concrete Autoencoders (CAEs),\nconsidered state-of-the-art in embedded feature selection, may struggle to\nachieve stable joint optimization, hurting their training time and\ngeneralization. In this work, we identify that this instability is correlated\nwith the CAE learning duplicate selections. To remedy this, we propose a simple\nand effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs\nlearn an embedding and a mapping from it to the Gumbel-Softmax distributions'\nparameters. Despite being simple to implement, IP-CAE exhibits significant and\nconsistent improvements over CAE in both generalization and training time\nacross several datasets for reconstruction and classification. Unlike CAE,\nIP-CAE effectively leverages non-linear relationships and does not require\nretraining the jointly optimized decoder. Furthermore, our approach is, in\nprinciple, generalizable to Gumbel-Softmax distributions beyond feature\nselection.\n", "link": "http://arxiv.org/abs/2403.00563v2", "date": "2024-08-16", "relevancy": 2.0499, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5963}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4605}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Indirectly%20Parameterized%20Concrete%20Autoencoders&body=Title%3A%20Indirectly%20Parameterized%20Concrete%20Autoencoders%0AAuthor%3A%20Alfred%20Nilsson%20and%20Klas%20Wijk%20and%20Sai%20bharath%20chandra%20Gutha%20and%20Erik%20Englesson%20and%20Alexandra%20Hotti%20and%20Carlo%20Saccardi%20and%20Oskar%20Kviman%20and%20Jens%20Lagergren%20and%20Ricardo%20Vinuesa%20and%20Hossein%20Azizpour%0AAbstract%3A%20%20%20Feature%20selection%20is%20a%20crucial%20task%20in%20settings%20where%20data%20is%0Ahigh-dimensional%20or%20acquiring%20the%20full%20set%20of%20features%20is%20costly.%20Recent%0Adevelopments%20in%20neural%20network-based%20embedded%20feature%20selection%20show%20promising%0Aresults%20across%20a%20wide%20range%20of%20applications.%20Concrete%20Autoencoders%20%28CAEs%29%2C%0Aconsidered%20state-of-the-art%20in%20embedded%20feature%20selection%2C%20may%20struggle%20to%0Aachieve%20stable%20joint%20optimization%2C%20hurting%20their%20training%20time%20and%0Ageneralization.%20In%20this%20work%2C%20we%20identify%20that%20this%20instability%20is%20correlated%0Awith%20the%20CAE%20learning%20duplicate%20selections.%20To%20remedy%20this%2C%20we%20propose%20a%20simple%0Aand%20effective%20improvement%3A%20Indirectly%20Parameterized%20CAEs%20%28IP-CAEs%29.%20IP-CAEs%0Alearn%20an%20embedding%20and%20a%20mapping%20from%20it%20to%20the%20Gumbel-Softmax%20distributions%27%0Aparameters.%20Despite%20being%20simple%20to%20implement%2C%20IP-CAE%20exhibits%20significant%20and%0Aconsistent%20improvements%20over%20CAE%20in%20both%20generalization%20and%20training%20time%0Aacross%20several%20datasets%20for%20reconstruction%20and%20classification.%20Unlike%20CAE%2C%0AIP-CAE%20effectively%20leverages%20non-linear%20relationships%20and%20does%20not%20require%0Aretraining%20the%20jointly%20optimized%20decoder.%20Furthermore%2C%20our%20approach%20is%2C%20in%0Aprinciple%2C%20generalizable%20to%20Gumbel-Softmax%20distributions%20beyond%20feature%0Aselection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndirectly%2520Parameterized%2520Concrete%2520Autoencoders%26entry.906535625%3DAlfred%2520Nilsson%2520and%2520Klas%2520Wijk%2520and%2520Sai%2520bharath%2520chandra%2520Gutha%2520and%2520Erik%2520Englesson%2520and%2520Alexandra%2520Hotti%2520and%2520Carlo%2520Saccardi%2520and%2520Oskar%2520Kviman%2520and%2520Jens%2520Lagergren%2520and%2520Ricardo%2520Vinuesa%2520and%2520Hossein%2520Azizpour%26entry.1292438233%3D%2520%2520Feature%2520selection%2520is%2520a%2520crucial%2520task%2520in%2520settings%2520where%2520data%2520is%250Ahigh-dimensional%2520or%2520acquiring%2520the%2520full%2520set%2520of%2520features%2520is%2520costly.%2520Recent%250Adevelopments%2520in%2520neural%2520network-based%2520embedded%2520feature%2520selection%2520show%2520promising%250Aresults%2520across%2520a%2520wide%2520range%2520of%2520applications.%2520Concrete%2520Autoencoders%2520%2528CAEs%2529%252C%250Aconsidered%2520state-of-the-art%2520in%2520embedded%2520feature%2520selection%252C%2520may%2520struggle%2520to%250Aachieve%2520stable%2520joint%2520optimization%252C%2520hurting%2520their%2520training%2520time%2520and%250Ageneralization.%2520In%2520this%2520work%252C%2520we%2520identify%2520that%2520this%2520instability%2520is%2520correlated%250Awith%2520the%2520CAE%2520learning%2520duplicate%2520selections.%2520To%2520remedy%2520this%252C%2520we%2520propose%2520a%2520simple%250Aand%2520effective%2520improvement%253A%2520Indirectly%2520Parameterized%2520CAEs%2520%2528IP-CAEs%2529.%2520IP-CAEs%250Alearn%2520an%2520embedding%2520and%2520a%2520mapping%2520from%2520it%2520to%2520the%2520Gumbel-Softmax%2520distributions%2527%250Aparameters.%2520Despite%2520being%2520simple%2520to%2520implement%252C%2520IP-CAE%2520exhibits%2520significant%2520and%250Aconsistent%2520improvements%2520over%2520CAE%2520in%2520both%2520generalization%2520and%2520training%2520time%250Aacross%2520several%2520datasets%2520for%2520reconstruction%2520and%2520classification.%2520Unlike%2520CAE%252C%250AIP-CAE%2520effectively%2520leverages%2520non-linear%2520relationships%2520and%2520does%2520not%2520require%250Aretraining%2520the%2520jointly%2520optimized%2520decoder.%2520Furthermore%252C%2520our%2520approach%2520is%252C%2520in%250Aprinciple%252C%2520generalizable%2520to%2520Gumbel-Softmax%2520distributions%2520beyond%2520feature%250Aselection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Indirectly%20Parameterized%20Concrete%20Autoencoders&entry.906535625=Alfred%20Nilsson%20and%20Klas%20Wijk%20and%20Sai%20bharath%20chandra%20Gutha%20and%20Erik%20Englesson%20and%20Alexandra%20Hotti%20and%20Carlo%20Saccardi%20and%20Oskar%20Kviman%20and%20Jens%20Lagergren%20and%20Ricardo%20Vinuesa%20and%20Hossein%20Azizpour&entry.1292438233=%20%20Feature%20selection%20is%20a%20crucial%20task%20in%20settings%20where%20data%20is%0Ahigh-dimensional%20or%20acquiring%20the%20full%20set%20of%20features%20is%20costly.%20Recent%0Adevelopments%20in%20neural%20network-based%20embedded%20feature%20selection%20show%20promising%0Aresults%20across%20a%20wide%20range%20of%20applications.%20Concrete%20Autoencoders%20%28CAEs%29%2C%0Aconsidered%20state-of-the-art%20in%20embedded%20feature%20selection%2C%20may%20struggle%20to%0Aachieve%20stable%20joint%20optimization%2C%20hurting%20their%20training%20time%20and%0Ageneralization.%20In%20this%20work%2C%20we%20identify%20that%20this%20instability%20is%20correlated%0Awith%20the%20CAE%20learning%20duplicate%20selections.%20To%20remedy%20this%2C%20we%20propose%20a%20simple%0Aand%20effective%20improvement%3A%20Indirectly%20Parameterized%20CAEs%20%28IP-CAEs%29.%20IP-CAEs%0Alearn%20an%20embedding%20and%20a%20mapping%20from%20it%20to%20the%20Gumbel-Softmax%20distributions%27%0Aparameters.%20Despite%20being%20simple%20to%20implement%2C%20IP-CAE%20exhibits%20significant%20and%0Aconsistent%20improvements%20over%20CAE%20in%20both%20generalization%20and%20training%20time%0Aacross%20several%20datasets%20for%20reconstruction%20and%20classification.%20Unlike%20CAE%2C%0AIP-CAE%20effectively%20leverages%20non-linear%20relationships%20and%20does%20not%20require%0Aretraining%20the%20jointly%20optimized%20decoder.%20Furthermore%2C%20our%20approach%20is%2C%20in%0Aprinciple%2C%20generalizable%20to%20Gumbel-Softmax%20distributions%20beyond%20feature%0Aselection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00563v2&entry.124074799=Read"},
{"title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars", "author": "Sumanth Prabhu", "abstract": "  Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable gains in accuracy for Large\nLanguage Models (LLMs). However, such techniques depend on the availability of\nan accurate answer extraction process to aggregate across multiple outputs.\nMoreover, they acquire higher inference cost, in comparison to Greedy Decoding,\ndue to generation of relatively higher number of output tokens. Research has\nshown that the free form text outputs from Self-Consistency can be aggregated\nreliably using LLMs to produce the final output. Additionally, recent\nadvancements in LLM inference have demonstrated that usage of diverse exemplars\nin prompts have the ability to induce diversity in the LLM outputs. Such proven\ntechniques can be easily extended to self-ensembling based approaches to\nachieve enhanced results in text generation. In this paper, we introduce PEDAL\n(Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid\nself-ensembling approach, that combines the strengths of diverse exemplar based\nprompts and LLM based aggregation to achieve improvement in overall\nperformance. On the publicly available SVAMP and ARC datasets, our experiments\nreveal that PEDAL can achieve better accuracy than Greedy Decoding based\nstrategies with lower inference cost compared to Self Consistency based\napproaches.\n", "link": "http://arxiv.org/abs/2408.08869v1", "date": "2024-08-16", "relevancy": 2.0479, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5237}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEDAL%3A%20Enhancing%20Greedy%20Decoding%20with%20Large%20Language%20Models%20using%0A%20%20Diverse%20Exemplars&body=Title%3A%20PEDAL%3A%20Enhancing%20Greedy%20Decoding%20with%20Large%20Language%20Models%20using%0A%20%20Diverse%20Exemplars%0AAuthor%3A%20Sumanth%20Prabhu%0AAbstract%3A%20%20%20Self-ensembling%20techniques%20with%20diverse%20reasoning%20paths%20such%20as%0ASelf-Consistency%20have%20demonstrated%20remarkable%20gains%20in%20accuracy%20for%20Large%0ALanguage%20Models%20%28LLMs%29.%20However%2C%20such%20techniques%20depend%20on%20the%20availability%20of%0Aan%20accurate%20answer%20extraction%20process%20to%20aggregate%20across%20multiple%20outputs.%0AMoreover%2C%20they%20acquire%20higher%20inference%20cost%2C%20in%20comparison%20to%20Greedy%20Decoding%2C%0Adue%20to%20generation%20of%20relatively%20higher%20number%20of%20output%20tokens.%20Research%20has%0Ashown%20that%20the%20free%20form%20text%20outputs%20from%20Self-Consistency%20can%20be%20aggregated%0Areliably%20using%20LLMs%20to%20produce%20the%20final%20output.%20Additionally%2C%20recent%0Aadvancements%20in%20LLM%20inference%20have%20demonstrated%20that%20usage%20of%20diverse%20exemplars%0Ain%20prompts%20have%20the%20ability%20to%20induce%20diversity%20in%20the%20LLM%20outputs.%20Such%20proven%0Atechniques%20can%20be%20easily%20extended%20to%20self-ensembling%20based%20approaches%20to%0Aachieve%20enhanced%20results%20in%20text%20generation.%20In%20this%20paper%2C%20we%20introduce%20PEDAL%0A%28Prompts%20based%20on%20Exemplar%20Diversity%20Aggregated%20using%20LLMs%29%2C%20a%20hybrid%0Aself-ensembling%20approach%2C%20that%20combines%20the%20strengths%20of%20diverse%20exemplar%20based%0Aprompts%20and%20LLM%20based%20aggregation%20to%20achieve%20improvement%20in%20overall%0Aperformance.%20On%20the%20publicly%20available%20SVAMP%20and%20ARC%20datasets%2C%20our%20experiments%0Areveal%20that%20PEDAL%20can%20achieve%20better%20accuracy%20than%20Greedy%20Decoding%20based%0Astrategies%20with%20lower%20inference%20cost%20compared%20to%20Self%20Consistency%20based%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEDAL%253A%2520Enhancing%2520Greedy%2520Decoding%2520with%2520Large%2520Language%2520Models%2520using%250A%2520%2520Diverse%2520Exemplars%26entry.906535625%3DSumanth%2520Prabhu%26entry.1292438233%3D%2520%2520Self-ensembling%2520techniques%2520with%2520diverse%2520reasoning%2520paths%2520such%2520as%250ASelf-Consistency%2520have%2520demonstrated%2520remarkable%2520gains%2520in%2520accuracy%2520for%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520However%252C%2520such%2520techniques%2520depend%2520on%2520the%2520availability%2520of%250Aan%2520accurate%2520answer%2520extraction%2520process%2520to%2520aggregate%2520across%2520multiple%2520outputs.%250AMoreover%252C%2520they%2520acquire%2520higher%2520inference%2520cost%252C%2520in%2520comparison%2520to%2520Greedy%2520Decoding%252C%250Adue%2520to%2520generation%2520of%2520relatively%2520higher%2520number%2520of%2520output%2520tokens.%2520Research%2520has%250Ashown%2520that%2520the%2520free%2520form%2520text%2520outputs%2520from%2520Self-Consistency%2520can%2520be%2520aggregated%250Areliably%2520using%2520LLMs%2520to%2520produce%2520the%2520final%2520output.%2520Additionally%252C%2520recent%250Aadvancements%2520in%2520LLM%2520inference%2520have%2520demonstrated%2520that%2520usage%2520of%2520diverse%2520exemplars%250Ain%2520prompts%2520have%2520the%2520ability%2520to%2520induce%2520diversity%2520in%2520the%2520LLM%2520outputs.%2520Such%2520proven%250Atechniques%2520can%2520be%2520easily%2520extended%2520to%2520self-ensembling%2520based%2520approaches%2520to%250Aachieve%2520enhanced%2520results%2520in%2520text%2520generation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PEDAL%250A%2528Prompts%2520based%2520on%2520Exemplar%2520Diversity%2520Aggregated%2520using%2520LLMs%2529%252C%2520a%2520hybrid%250Aself-ensembling%2520approach%252C%2520that%2520combines%2520the%2520strengths%2520of%2520diverse%2520exemplar%2520based%250Aprompts%2520and%2520LLM%2520based%2520aggregation%2520to%2520achieve%2520improvement%2520in%2520overall%250Aperformance.%2520On%2520the%2520publicly%2520available%2520SVAMP%2520and%2520ARC%2520datasets%252C%2520our%2520experiments%250Areveal%2520that%2520PEDAL%2520can%2520achieve%2520better%2520accuracy%2520than%2520Greedy%2520Decoding%2520based%250Astrategies%2520with%2520lower%2520inference%2520cost%2520compared%2520to%2520Self%2520Consistency%2520based%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEDAL%3A%20Enhancing%20Greedy%20Decoding%20with%20Large%20Language%20Models%20using%0A%20%20Diverse%20Exemplars&entry.906535625=Sumanth%20Prabhu&entry.1292438233=%20%20Self-ensembling%20techniques%20with%20diverse%20reasoning%20paths%20such%20as%0ASelf-Consistency%20have%20demonstrated%20remarkable%20gains%20in%20accuracy%20for%20Large%0ALanguage%20Models%20%28LLMs%29.%20However%2C%20such%20techniques%20depend%20on%20the%20availability%20of%0Aan%20accurate%20answer%20extraction%20process%20to%20aggregate%20across%20multiple%20outputs.%0AMoreover%2C%20they%20acquire%20higher%20inference%20cost%2C%20in%20comparison%20to%20Greedy%20Decoding%2C%0Adue%20to%20generation%20of%20relatively%20higher%20number%20of%20output%20tokens.%20Research%20has%0Ashown%20that%20the%20free%20form%20text%20outputs%20from%20Self-Consistency%20can%20be%20aggregated%0Areliably%20using%20LLMs%20to%20produce%20the%20final%20output.%20Additionally%2C%20recent%0Aadvancements%20in%20LLM%20inference%20have%20demonstrated%20that%20usage%20of%20diverse%20exemplars%0Ain%20prompts%20have%20the%20ability%20to%20induce%20diversity%20in%20the%20LLM%20outputs.%20Such%20proven%0Atechniques%20can%20be%20easily%20extended%20to%20self-ensembling%20based%20approaches%20to%0Aachieve%20enhanced%20results%20in%20text%20generation.%20In%20this%20paper%2C%20we%20introduce%20PEDAL%0A%28Prompts%20based%20on%20Exemplar%20Diversity%20Aggregated%20using%20LLMs%29%2C%20a%20hybrid%0Aself-ensembling%20approach%2C%20that%20combines%20the%20strengths%20of%20diverse%20exemplar%20based%0Aprompts%20and%20LLM%20based%20aggregation%20to%20achieve%20improvement%20in%20overall%0Aperformance.%20On%20the%20publicly%20available%20SVAMP%20and%20ARC%20datasets%2C%20our%20experiments%0Areveal%20that%20PEDAL%20can%20achieve%20better%20accuracy%20than%20Greedy%20Decoding%20based%0Astrategies%20with%20lower%20inference%20cost%20compared%20to%20Self%20Consistency%20based%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08869v1&entry.124074799=Read"},
{"title": "GeoTransformer: Enhancing Urban Forecasting with Geospatial Attention\n  Mechanisms", "author": "Yuhao Jia and Zile Wu and Shengao Yi and Yifei Sun", "abstract": "  Recent advancements have focused on encoding urban spatial information into\nhigh-dimensional spaces, with notable efforts dedicated to integrating\nsociodemographic data and satellite imagery. These efforts have established\nfoundational models in this field. However, the effective utilization of these\nspatial representations for urban forecasting applications remains\nunder-explored. To address this gap, we introduce GeoTransformer, a novel\nstructure that synergizes the Transformer architecture with geospatial\nstatistics prior. GeoTransformer employs an innovative geospatial attention\nmechanism to incorporate extensive urban information and spatial dependencies\ninto a unified predictive model. Specifically, we compute geospatial weighted\nattention scores between the target region and surrounding regions and leverage\nthe integrated urban information for predictions. Extensive experiments on GDP\nand ride-share demand prediction tasks demonstrate that GeoTransformer\nsignificantly outperforms existing baseline models, showcasing its potential to\nenhance urban forecasting tasks.\n", "link": "http://arxiv.org/abs/2408.08852v1", "date": "2024-08-16", "relevancy": 2.0405, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5396}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5161}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoTransformer%3A%20Enhancing%20Urban%20Forecasting%20with%20Geospatial%20Attention%0A%20%20Mechanisms&body=Title%3A%20GeoTransformer%3A%20Enhancing%20Urban%20Forecasting%20with%20Geospatial%20Attention%0A%20%20Mechanisms%0AAuthor%3A%20Yuhao%20Jia%20and%20Zile%20Wu%20and%20Shengao%20Yi%20and%20Yifei%20Sun%0AAbstract%3A%20%20%20Recent%20advancements%20have%20focused%20on%20encoding%20urban%20spatial%20information%20into%0Ahigh-dimensional%20spaces%2C%20with%20notable%20efforts%20dedicated%20to%20integrating%0Asociodemographic%20data%20and%20satellite%20imagery.%20These%20efforts%20have%20established%0Afoundational%20models%20in%20this%20field.%20However%2C%20the%20effective%20utilization%20of%20these%0Aspatial%20representations%20for%20urban%20forecasting%20applications%20remains%0Aunder-explored.%20To%20address%20this%20gap%2C%20we%20introduce%20GeoTransformer%2C%20a%20novel%0Astructure%20that%20synergizes%20the%20Transformer%20architecture%20with%20geospatial%0Astatistics%20prior.%20GeoTransformer%20employs%20an%20innovative%20geospatial%20attention%0Amechanism%20to%20incorporate%20extensive%20urban%20information%20and%20spatial%20dependencies%0Ainto%20a%20unified%20predictive%20model.%20Specifically%2C%20we%20compute%20geospatial%20weighted%0Aattention%20scores%20between%20the%20target%20region%20and%20surrounding%20regions%20and%20leverage%0Athe%20integrated%20urban%20information%20for%20predictions.%20Extensive%20experiments%20on%20GDP%0Aand%20ride-share%20demand%20prediction%20tasks%20demonstrate%20that%20GeoTransformer%0Asignificantly%20outperforms%20existing%20baseline%20models%2C%20showcasing%20its%20potential%20to%0Aenhance%20urban%20forecasting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoTransformer%253A%2520Enhancing%2520Urban%2520Forecasting%2520with%2520Geospatial%2520Attention%250A%2520%2520Mechanisms%26entry.906535625%3DYuhao%2520Jia%2520and%2520Zile%2520Wu%2520and%2520Shengao%2520Yi%2520and%2520Yifei%2520Sun%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520focused%2520on%2520encoding%2520urban%2520spatial%2520information%2520into%250Ahigh-dimensional%2520spaces%252C%2520with%2520notable%2520efforts%2520dedicated%2520to%2520integrating%250Asociodemographic%2520data%2520and%2520satellite%2520imagery.%2520These%2520efforts%2520have%2520established%250Afoundational%2520models%2520in%2520this%2520field.%2520However%252C%2520the%2520effective%2520utilization%2520of%2520these%250Aspatial%2520representations%2520for%2520urban%2520forecasting%2520applications%2520remains%250Aunder-explored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520GeoTransformer%252C%2520a%2520novel%250Astructure%2520that%2520synergizes%2520the%2520Transformer%2520architecture%2520with%2520geospatial%250Astatistics%2520prior.%2520GeoTransformer%2520employs%2520an%2520innovative%2520geospatial%2520attention%250Amechanism%2520to%2520incorporate%2520extensive%2520urban%2520information%2520and%2520spatial%2520dependencies%250Ainto%2520a%2520unified%2520predictive%2520model.%2520Specifically%252C%2520we%2520compute%2520geospatial%2520weighted%250Aattention%2520scores%2520between%2520the%2520target%2520region%2520and%2520surrounding%2520regions%2520and%2520leverage%250Athe%2520integrated%2520urban%2520information%2520for%2520predictions.%2520Extensive%2520experiments%2520on%2520GDP%250Aand%2520ride-share%2520demand%2520prediction%2520tasks%2520demonstrate%2520that%2520GeoTransformer%250Asignificantly%2520outperforms%2520existing%2520baseline%2520models%252C%2520showcasing%2520its%2520potential%2520to%250Aenhance%2520urban%2520forecasting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoTransformer%3A%20Enhancing%20Urban%20Forecasting%20with%20Geospatial%20Attention%0A%20%20Mechanisms&entry.906535625=Yuhao%20Jia%20and%20Zile%20Wu%20and%20Shengao%20Yi%20and%20Yifei%20Sun&entry.1292438233=%20%20Recent%20advancements%20have%20focused%20on%20encoding%20urban%20spatial%20information%20into%0Ahigh-dimensional%20spaces%2C%20with%20notable%20efforts%20dedicated%20to%20integrating%0Asociodemographic%20data%20and%20satellite%20imagery.%20These%20efforts%20have%20established%0Afoundational%20models%20in%20this%20field.%20However%2C%20the%20effective%20utilization%20of%20these%0Aspatial%20representations%20for%20urban%20forecasting%20applications%20remains%0Aunder-explored.%20To%20address%20this%20gap%2C%20we%20introduce%20GeoTransformer%2C%20a%20novel%0Astructure%20that%20synergizes%20the%20Transformer%20architecture%20with%20geospatial%0Astatistics%20prior.%20GeoTransformer%20employs%20an%20innovative%20geospatial%20attention%0Amechanism%20to%20incorporate%20extensive%20urban%20information%20and%20spatial%20dependencies%0Ainto%20a%20unified%20predictive%20model.%20Specifically%2C%20we%20compute%20geospatial%20weighted%0Aattention%20scores%20between%20the%20target%20region%20and%20surrounding%20regions%20and%20leverage%0Athe%20integrated%20urban%20information%20for%20predictions.%20Extensive%20experiments%20on%20GDP%0Aand%20ride-share%20demand%20prediction%20tasks%20demonstrate%20that%20GeoTransformer%0Asignificantly%20outperforms%20existing%20baseline%20models%2C%20showcasing%20its%20potential%20to%0Aenhance%20urban%20forecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08852v1&entry.124074799=Read"},
{"title": "Kernel Density Estimators in Large Dimensions", "author": "Giulio Biroli and Marc M\u00e9zard", "abstract": "  This paper studies Kernel density estimation for a high-dimensional\ndistribution $\\rho(x)$. Traditional approaches have focused on the limit of\nlarge number of data points $n$ and fixed dimension $d$. We analyze instead the\nregime where both the number $n$ of data points $y_i$ and their dimensionality\n$d$ grow with a fixed ratio $\\alpha=(\\log n)/d$. Our study reveals three\ndistinct statistical regimes for the kernel-based estimate of the density $\\hat\n\\rho_h^{\\mathcal {D}}(x)=\\frac{1}{n h^d}\\sum_{i=1}^n\nK\\left(\\frac{x-y_i}{h}\\right)$, depending on the bandwidth $h$: a classical\nregime for large bandwidth where the Central Limit Theorem (CLT) holds, which\nis akin to the one found in traditional approaches. Below a certain value of\nthe bandwidth, $h_{CLT}(\\alpha)$, we find that the CLT breaks down. The\nstatistics of $\\hat \\rho_h^{\\mathcal {D}}(x)$ for a fixed $x$ drawn from\n$\\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable\ndistribution). In particular below a value $h_G(\\alpha)$, we find that $\\hat\n\\rho_h^{\\mathcal {D}}(x)$ is governed by extreme value statistics: only a few\npoints in the database matter and give the dominant contribution to the density\nestimator. We provide a detailed analysis for high-dimensional multivariate\nGaussian data. We show that the optimal bandwidth threshold based on\nKullback-Leibler divergence lies in the new statistical regime identified in\nthis paper. Our findings reveal limitations of classical approaches, show the\nrelevance of these new statistical regimes, and offer new insights for Kernel\ndensity estimation in high-dimensional settings.\n", "link": "http://arxiv.org/abs/2408.05807v2", "date": "2024-08-16", "relevancy": 2.0265, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4144}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4034}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Density%20Estimators%20in%20Large%20Dimensions&body=Title%3A%20Kernel%20Density%20Estimators%20in%20Large%20Dimensions%0AAuthor%3A%20Giulio%20Biroli%20and%20Marc%20M%C3%A9zard%0AAbstract%3A%20%20%20This%20paper%20studies%20Kernel%20density%20estimation%20for%20a%20high-dimensional%0Adistribution%20%24%5Crho%28x%29%24.%20Traditional%20approaches%20have%20focused%20on%20the%20limit%20of%0Alarge%20number%20of%20data%20points%20%24n%24%20and%20fixed%20dimension%20%24d%24.%20We%20analyze%20instead%20the%0Aregime%20where%20both%20the%20number%20%24n%24%20of%20data%20points%20%24y_i%24%20and%20their%20dimensionality%0A%24d%24%20grow%20with%20a%20fixed%20ratio%20%24%5Calpha%3D%28%5Clog%20n%29/d%24.%20Our%20study%20reveals%20three%0Adistinct%20statistical%20regimes%20for%20the%20kernel-based%20estimate%20of%20the%20density%20%24%5Chat%0A%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%3D%5Cfrac%7B1%7D%7Bn%20h%5Ed%7D%5Csum_%7Bi%3D1%7D%5En%0AK%5Cleft%28%5Cfrac%7Bx-y_i%7D%7Bh%7D%5Cright%29%24%2C%20depending%20on%20the%20bandwidth%20%24h%24%3A%20a%20classical%0Aregime%20for%20large%20bandwidth%20where%20the%20Central%20Limit%20Theorem%20%28CLT%29%20holds%2C%20which%0Ais%20akin%20to%20the%20one%20found%20in%20traditional%20approaches.%20Below%20a%20certain%20value%20of%0Athe%20bandwidth%2C%20%24h_%7BCLT%7D%28%5Calpha%29%24%2C%20we%20find%20that%20the%20CLT%20breaks%20down.%20The%0Astatistics%20of%20%24%5Chat%20%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20for%20a%20fixed%20%24x%24%20drawn%20from%0A%24%5Crho%28x%29%24%20is%20given%20by%20a%20heavy-tailed%20distribution%20%28an%20alpha-stable%0Adistribution%29.%20In%20particular%20below%20a%20value%20%24h_G%28%5Calpha%29%24%2C%20we%20find%20that%20%24%5Chat%0A%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20is%20governed%20by%20extreme%20value%20statistics%3A%20only%20a%20few%0Apoints%20in%20the%20database%20matter%20and%20give%20the%20dominant%20contribution%20to%20the%20density%0Aestimator.%20We%20provide%20a%20detailed%20analysis%20for%20high-dimensional%20multivariate%0AGaussian%20data.%20We%20show%20that%20the%20optimal%20bandwidth%20threshold%20based%20on%0AKullback-Leibler%20divergence%20lies%20in%20the%20new%20statistical%20regime%20identified%20in%0Athis%20paper.%20Our%20findings%20reveal%20limitations%20of%20classical%20approaches%2C%20show%20the%0Arelevance%20of%20these%20new%20statistical%20regimes%2C%20and%20offer%20new%20insights%20for%20Kernel%0Adensity%20estimation%20in%20high-dimensional%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Density%2520Estimators%2520in%2520Large%2520Dimensions%26entry.906535625%3DGiulio%2520Biroli%2520and%2520Marc%2520M%25C3%25A9zard%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520Kernel%2520density%2520estimation%2520for%2520a%2520high-dimensional%250Adistribution%2520%2524%255Crho%2528x%2529%2524.%2520Traditional%2520approaches%2520have%2520focused%2520on%2520the%2520limit%2520of%250Alarge%2520number%2520of%2520data%2520points%2520%2524n%2524%2520and%2520fixed%2520dimension%2520%2524d%2524.%2520We%2520analyze%2520instead%2520the%250Aregime%2520where%2520both%2520the%2520number%2520%2524n%2524%2520of%2520data%2520points%2520%2524y_i%2524%2520and%2520their%2520dimensionality%250A%2524d%2524%2520grow%2520with%2520a%2520fixed%2520ratio%2520%2524%255Calpha%253D%2528%255Clog%2520n%2529/d%2524.%2520Our%2520study%2520reveals%2520three%250Adistinct%2520statistical%2520regimes%2520for%2520the%2520kernel-based%2520estimate%2520of%2520the%2520density%2520%2524%255Chat%250A%255Crho_h%255E%257B%255Cmathcal%2520%257BD%257D%257D%2528x%2529%253D%255Cfrac%257B1%257D%257Bn%2520h%255Ed%257D%255Csum_%257Bi%253D1%257D%255En%250AK%255Cleft%2528%255Cfrac%257Bx-y_i%257D%257Bh%257D%255Cright%2529%2524%252C%2520depending%2520on%2520the%2520bandwidth%2520%2524h%2524%253A%2520a%2520classical%250Aregime%2520for%2520large%2520bandwidth%2520where%2520the%2520Central%2520Limit%2520Theorem%2520%2528CLT%2529%2520holds%252C%2520which%250Ais%2520akin%2520to%2520the%2520one%2520found%2520in%2520traditional%2520approaches.%2520Below%2520a%2520certain%2520value%2520of%250Athe%2520bandwidth%252C%2520%2524h_%257BCLT%257D%2528%255Calpha%2529%2524%252C%2520we%2520find%2520that%2520the%2520CLT%2520breaks%2520down.%2520The%250Astatistics%2520of%2520%2524%255Chat%2520%255Crho_h%255E%257B%255Cmathcal%2520%257BD%257D%257D%2528x%2529%2524%2520for%2520a%2520fixed%2520%2524x%2524%2520drawn%2520from%250A%2524%255Crho%2528x%2529%2524%2520is%2520given%2520by%2520a%2520heavy-tailed%2520distribution%2520%2528an%2520alpha-stable%250Adistribution%2529.%2520In%2520particular%2520below%2520a%2520value%2520%2524h_G%2528%255Calpha%2529%2524%252C%2520we%2520find%2520that%2520%2524%255Chat%250A%255Crho_h%255E%257B%255Cmathcal%2520%257BD%257D%257D%2528x%2529%2524%2520is%2520governed%2520by%2520extreme%2520value%2520statistics%253A%2520only%2520a%2520few%250Apoints%2520in%2520the%2520database%2520matter%2520and%2520give%2520the%2520dominant%2520contribution%2520to%2520the%2520density%250Aestimator.%2520We%2520provide%2520a%2520detailed%2520analysis%2520for%2520high-dimensional%2520multivariate%250AGaussian%2520data.%2520We%2520show%2520that%2520the%2520optimal%2520bandwidth%2520threshold%2520based%2520on%250AKullback-Leibler%2520divergence%2520lies%2520in%2520the%2520new%2520statistical%2520regime%2520identified%2520in%250Athis%2520paper.%2520Our%2520findings%2520reveal%2520limitations%2520of%2520classical%2520approaches%252C%2520show%2520the%250Arelevance%2520of%2520these%2520new%2520statistical%2520regimes%252C%2520and%2520offer%2520new%2520insights%2520for%2520Kernel%250Adensity%2520estimation%2520in%2520high-dimensional%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Density%20Estimators%20in%20Large%20Dimensions&entry.906535625=Giulio%20Biroli%20and%20Marc%20M%C3%A9zard&entry.1292438233=%20%20This%20paper%20studies%20Kernel%20density%20estimation%20for%20a%20high-dimensional%0Adistribution%20%24%5Crho%28x%29%24.%20Traditional%20approaches%20have%20focused%20on%20the%20limit%20of%0Alarge%20number%20of%20data%20points%20%24n%24%20and%20fixed%20dimension%20%24d%24.%20We%20analyze%20instead%20the%0Aregime%20where%20both%20the%20number%20%24n%24%20of%20data%20points%20%24y_i%24%20and%20their%20dimensionality%0A%24d%24%20grow%20with%20a%20fixed%20ratio%20%24%5Calpha%3D%28%5Clog%20n%29/d%24.%20Our%20study%20reveals%20three%0Adistinct%20statistical%20regimes%20for%20the%20kernel-based%20estimate%20of%20the%20density%20%24%5Chat%0A%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%3D%5Cfrac%7B1%7D%7Bn%20h%5Ed%7D%5Csum_%7Bi%3D1%7D%5En%0AK%5Cleft%28%5Cfrac%7Bx-y_i%7D%7Bh%7D%5Cright%29%24%2C%20depending%20on%20the%20bandwidth%20%24h%24%3A%20a%20classical%0Aregime%20for%20large%20bandwidth%20where%20the%20Central%20Limit%20Theorem%20%28CLT%29%20holds%2C%20which%0Ais%20akin%20to%20the%20one%20found%20in%20traditional%20approaches.%20Below%20a%20certain%20value%20of%0Athe%20bandwidth%2C%20%24h_%7BCLT%7D%28%5Calpha%29%24%2C%20we%20find%20that%20the%20CLT%20breaks%20down.%20The%0Astatistics%20of%20%24%5Chat%20%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20for%20a%20fixed%20%24x%24%20drawn%20from%0A%24%5Crho%28x%29%24%20is%20given%20by%20a%20heavy-tailed%20distribution%20%28an%20alpha-stable%0Adistribution%29.%20In%20particular%20below%20a%20value%20%24h_G%28%5Calpha%29%24%2C%20we%20find%20that%20%24%5Chat%0A%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20is%20governed%20by%20extreme%20value%20statistics%3A%20only%20a%20few%0Apoints%20in%20the%20database%20matter%20and%20give%20the%20dominant%20contribution%20to%20the%20density%0Aestimator.%20We%20provide%20a%20detailed%20analysis%20for%20high-dimensional%20multivariate%0AGaussian%20data.%20We%20show%20that%20the%20optimal%20bandwidth%20threshold%20based%20on%0AKullback-Leibler%20divergence%20lies%20in%20the%20new%20statistical%20regime%20identified%20in%0Athis%20paper.%20Our%20findings%20reveal%20limitations%20of%20classical%20approaches%2C%20show%20the%0Arelevance%20of%20these%20new%20statistical%20regimes%2C%20and%20offer%20new%20insights%20for%20Kernel%0Adensity%20estimation%20in%20high-dimensional%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05807v2&entry.124074799=Read"},
{"title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector", "author": "Wenjie Fu and Huandong Wang and Chen Gao and Guanghua Liu and Yong Li and Tao Jiang", "abstract": "  The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9.\n", "link": "http://arxiv.org/abs/2408.08661v1", "date": "2024-08-16", "relevancy": 2.0205, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5338}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5219}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIA-Tuner%3A%20Adapting%20Large%20Language%20Models%20as%20Pre-training%20Text%20Detector&body=Title%3A%20MIA-Tuner%3A%20Adapting%20Large%20Language%20Models%20as%20Pre-training%20Text%20Detector%0AAuthor%3A%20Wenjie%20Fu%20and%20Huandong%20Wang%20and%20Chen%20Gao%20and%20Guanghua%20Liu%20and%20Yong%20Li%20and%20Tao%20Jiang%0AAbstract%3A%20%20%20The%20increasing%20parameters%20and%20expansive%20dataset%20of%20large%20language%20models%0A%28LLMs%29%20highlight%20the%20urgent%20demand%20for%20a%20technical%20solution%20to%20audit%20the%0Aunderlying%20privacy%20risks%20and%20copyright%20issues%20associated%20with%20LLMs.%20Existing%0Astudies%20have%20partially%20addressed%20this%20need%20through%20an%20exploration%20of%20the%0Apre-training%20data%20detection%20problem%2C%20which%20is%20an%20instance%20of%20a%20membership%0Ainference%20attack%20%28MIA%29.%20This%20problem%20involves%20determining%20whether%20a%20given%20piece%0Aof%20text%20has%20been%20used%20during%20the%20pre-training%20phase%20of%20the%20target%20LLM.%20Although%0Aexisting%20methods%20have%20designed%20various%20sophisticated%20MIA%20score%20functions%20to%0Aachieve%20considerable%20detection%20performance%20in%20pre-trained%20LLMs%2C%20how%20to%20achieve%0Ahigh-confidence%20detection%20and%20how%20to%20perform%20MIA%20on%20aligned%20LLMs%20remain%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20MIA-Tuner%2C%20a%20novel%20instruction-based%20MIA%0Amethod%2C%20which%20instructs%20LLMs%20themselves%20to%20serve%20as%20a%20more%20precise%20pre-training%0Adata%20detector%20internally%2C%20rather%20than%20design%20an%20external%20MIA%20score%20function.%0AFurthermore%2C%20we%20design%20two%20instruction-based%20safeguards%20to%20respectively%0Amitigate%20the%20privacy%20risks%20brought%20by%20the%20existing%20methods%20and%20MIA-Tuner.%20To%0Acomprehensively%20evaluate%20the%20most%20recent%20state-of-the-art%20LLMs%2C%20we%20collect%20a%0Amore%20up-to-date%20MIA%20benchmark%20dataset%2C%20named%20WIKIMIA-24%2C%20to%20replace%20the%20widely%0Aadopted%20benchmark%20WIKIMIA.%20We%20conduct%20extensive%20experiments%20across%20various%0Aaligned%20and%20unaligned%20LLMs%20over%20the%20two%20benchmark%20datasets.%20The%20results%0Ademonstrate%20that%20MIA-Tuner%20increases%20the%20AUC%20of%20MIAs%20from%200.7%20to%20a%0Asignificantly%20high%20level%20of%200.9.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIA-Tuner%253A%2520Adapting%2520Large%2520Language%2520Models%2520as%2520Pre-training%2520Text%2520Detector%26entry.906535625%3DWenjie%2520Fu%2520and%2520Huandong%2520Wang%2520and%2520Chen%2520Gao%2520and%2520Guanghua%2520Liu%2520and%2520Yong%2520Li%2520and%2520Tao%2520Jiang%26entry.1292438233%3D%2520%2520The%2520increasing%2520parameters%2520and%2520expansive%2520dataset%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520highlight%2520the%2520urgent%2520demand%2520for%2520a%2520technical%2520solution%2520to%2520audit%2520the%250Aunderlying%2520privacy%2520risks%2520and%2520copyright%2520issues%2520associated%2520with%2520LLMs.%2520Existing%250Astudies%2520have%2520partially%2520addressed%2520this%2520need%2520through%2520an%2520exploration%2520of%2520the%250Apre-training%2520data%2520detection%2520problem%252C%2520which%2520is%2520an%2520instance%2520of%2520a%2520membership%250Ainference%2520attack%2520%2528MIA%2529.%2520This%2520problem%2520involves%2520determining%2520whether%2520a%2520given%2520piece%250Aof%2520text%2520has%2520been%2520used%2520during%2520the%2520pre-training%2520phase%2520of%2520the%2520target%2520LLM.%2520Although%250Aexisting%2520methods%2520have%2520designed%2520various%2520sophisticated%2520MIA%2520score%2520functions%2520to%250Aachieve%2520considerable%2520detection%2520performance%2520in%2520pre-trained%2520LLMs%252C%2520how%2520to%2520achieve%250Ahigh-confidence%2520detection%2520and%2520how%2520to%2520perform%2520MIA%2520on%2520aligned%2520LLMs%2520remain%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MIA-Tuner%252C%2520a%2520novel%2520instruction-based%2520MIA%250Amethod%252C%2520which%2520instructs%2520LLMs%2520themselves%2520to%2520serve%2520as%2520a%2520more%2520precise%2520pre-training%250Adata%2520detector%2520internally%252C%2520rather%2520than%2520design%2520an%2520external%2520MIA%2520score%2520function.%250AFurthermore%252C%2520we%2520design%2520two%2520instruction-based%2520safeguards%2520to%2520respectively%250Amitigate%2520the%2520privacy%2520risks%2520brought%2520by%2520the%2520existing%2520methods%2520and%2520MIA-Tuner.%2520To%250Acomprehensively%2520evaluate%2520the%2520most%2520recent%2520state-of-the-art%2520LLMs%252C%2520we%2520collect%2520a%250Amore%2520up-to-date%2520MIA%2520benchmark%2520dataset%252C%2520named%2520WIKIMIA-24%252C%2520to%2520replace%2520the%2520widely%250Aadopted%2520benchmark%2520WIKIMIA.%2520We%2520conduct%2520extensive%2520experiments%2520across%2520various%250Aaligned%2520and%2520unaligned%2520LLMs%2520over%2520the%2520two%2520benchmark%2520datasets.%2520The%2520results%250Ademonstrate%2520that%2520MIA-Tuner%2520increases%2520the%2520AUC%2520of%2520MIAs%2520from%25200.7%2520to%2520a%250Asignificantly%2520high%2520level%2520of%25200.9.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIA-Tuner%3A%20Adapting%20Large%20Language%20Models%20as%20Pre-training%20Text%20Detector&entry.906535625=Wenjie%20Fu%20and%20Huandong%20Wang%20and%20Chen%20Gao%20and%20Guanghua%20Liu%20and%20Yong%20Li%20and%20Tao%20Jiang&entry.1292438233=%20%20The%20increasing%20parameters%20and%20expansive%20dataset%20of%20large%20language%20models%0A%28LLMs%29%20highlight%20the%20urgent%20demand%20for%20a%20technical%20solution%20to%20audit%20the%0Aunderlying%20privacy%20risks%20and%20copyright%20issues%20associated%20with%20LLMs.%20Existing%0Astudies%20have%20partially%20addressed%20this%20need%20through%20an%20exploration%20of%20the%0Apre-training%20data%20detection%20problem%2C%20which%20is%20an%20instance%20of%20a%20membership%0Ainference%20attack%20%28MIA%29.%20This%20problem%20involves%20determining%20whether%20a%20given%20piece%0Aof%20text%20has%20been%20used%20during%20the%20pre-training%20phase%20of%20the%20target%20LLM.%20Although%0Aexisting%20methods%20have%20designed%20various%20sophisticated%20MIA%20score%20functions%20to%0Aachieve%20considerable%20detection%20performance%20in%20pre-trained%20LLMs%2C%20how%20to%20achieve%0Ahigh-confidence%20detection%20and%20how%20to%20perform%20MIA%20on%20aligned%20LLMs%20remain%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20MIA-Tuner%2C%20a%20novel%20instruction-based%20MIA%0Amethod%2C%20which%20instructs%20LLMs%20themselves%20to%20serve%20as%20a%20more%20precise%20pre-training%0Adata%20detector%20internally%2C%20rather%20than%20design%20an%20external%20MIA%20score%20function.%0AFurthermore%2C%20we%20design%20two%20instruction-based%20safeguards%20to%20respectively%0Amitigate%20the%20privacy%20risks%20brought%20by%20the%20existing%20methods%20and%20MIA-Tuner.%20To%0Acomprehensively%20evaluate%20the%20most%20recent%20state-of-the-art%20LLMs%2C%20we%20collect%20a%0Amore%20up-to-date%20MIA%20benchmark%20dataset%2C%20named%20WIKIMIA-24%2C%20to%20replace%20the%20widely%0Aadopted%20benchmark%20WIKIMIA.%20We%20conduct%20extensive%20experiments%20across%20various%0Aaligned%20and%20unaligned%20LLMs%20over%20the%20two%20benchmark%20datasets.%20The%20results%0Ademonstrate%20that%20MIA-Tuner%20increases%20the%20AUC%20of%20MIAs%20from%200.7%20to%20a%0Asignificantly%20high%20level%20of%200.9.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08661v1&entry.124074799=Read"},
{"title": "Beyond Full Label: Single-Point Prompt for Infrared Small Target Label\n  Generation", "author": "Shuai Yuan and Hanlin Qin and Renke Kou and Xiang Yan and Zechuan Li and Chenxu Peng and Abd-Krim Seghouane", "abstract": "  In this work, we make the first attempt to construct a learning-based\nsingle-point annotation paradigm for infrared small target label generation\n(IRSTLG). Our intuition is that label generation requires just one more point\nprompt than target detection: IRSTLG can be regarded as an infrared small\ntarget detection (IRSTD) task with the target location hint. Based on this\ninsight, we introduce an energy double guided single-point prompt (EDGSP)\nframework, which adeptly transforms the target detection network into a refined\nlabel generation method. Specifically, the proposed EDGSP includes: 1) target\nenergy initialization (TEI) to create a foundational outline for sufficient\nshape evolution of pseudo label, 2) double prompt embedding (DPE) for rapid\nlocalization of interested regions and reinforcement of individual differences\nto avoid label adhesion, and 3) bounding box-based matching (BBM) to eliminate\nfalse alarms. Experimental results show that pseudo labels generated by three\nbaselines equipped with EDGSP achieve 100% object-level probability of\ndetection (Pd) and 0% false-alarm rate (Fa) on SIRST, NUDT-SIRST, and IRSTD-1k\ndatasets, with a pixel-level intersection over union (IoU) improvement of\n13.28% over state-of-the-art (SOTA) label generation methods. In the practical\napplication of downstream IRSTD, EDGSP realizes, for the first time, a\nsingle-point generated pseudo mask beyond the full label. Even with coarse\nsingle-point annotations, it still achieves 99.5% performance of full labeling.\n", "link": "http://arxiv.org/abs/2408.08191v2", "date": "2024-08-16", "relevancy": 2.0066, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5438}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4974}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Full%20Label%3A%20Single-Point%20Prompt%20for%20Infrared%20Small%20Target%20Label%0A%20%20Generation&body=Title%3A%20Beyond%20Full%20Label%3A%20Single-Point%20Prompt%20for%20Infrared%20Small%20Target%20Label%0A%20%20Generation%0AAuthor%3A%20Shuai%20Yuan%20and%20Hanlin%20Qin%20and%20Renke%20Kou%20and%20Xiang%20Yan%20and%20Zechuan%20Li%20and%20Chenxu%20Peng%20and%20Abd-Krim%20Seghouane%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20make%20the%20first%20attempt%20to%20construct%20a%20learning-based%0Asingle-point%20annotation%20paradigm%20for%20infrared%20small%20target%20label%20generation%0A%28IRSTLG%29.%20Our%20intuition%20is%20that%20label%20generation%20requires%20just%20one%20more%20point%0Aprompt%20than%20target%20detection%3A%20IRSTLG%20can%20be%20regarded%20as%20an%20infrared%20small%0Atarget%20detection%20%28IRSTD%29%20task%20with%20the%20target%20location%20hint.%20Based%20on%20this%0Ainsight%2C%20we%20introduce%20an%20energy%20double%20guided%20single-point%20prompt%20%28EDGSP%29%0Aframework%2C%20which%20adeptly%20transforms%20the%20target%20detection%20network%20into%20a%20refined%0Alabel%20generation%20method.%20Specifically%2C%20the%20proposed%20EDGSP%20includes%3A%201%29%20target%0Aenergy%20initialization%20%28TEI%29%20to%20create%20a%20foundational%20outline%20for%20sufficient%0Ashape%20evolution%20of%20pseudo%20label%2C%202%29%20double%20prompt%20embedding%20%28DPE%29%20for%20rapid%0Alocalization%20of%20interested%20regions%20and%20reinforcement%20of%20individual%20differences%0Ato%20avoid%20label%20adhesion%2C%20and%203%29%20bounding%20box-based%20matching%20%28BBM%29%20to%20eliminate%0Afalse%20alarms.%20Experimental%20results%20show%20that%20pseudo%20labels%20generated%20by%20three%0Abaselines%20equipped%20with%20EDGSP%20achieve%20100%25%20object-level%20probability%20of%0Adetection%20%28Pd%29%20and%200%25%20false-alarm%20rate%20%28Fa%29%20on%20SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1k%0Adatasets%2C%20with%20a%20pixel-level%20intersection%20over%20union%20%28IoU%29%20improvement%20of%0A13.28%25%20over%20state-of-the-art%20%28SOTA%29%20label%20generation%20methods.%20In%20the%20practical%0Aapplication%20of%20downstream%20IRSTD%2C%20EDGSP%20realizes%2C%20for%20the%20first%20time%2C%20a%0Asingle-point%20generated%20pseudo%20mask%20beyond%20the%20full%20label.%20Even%20with%20coarse%0Asingle-point%20annotations%2C%20it%20still%20achieves%2099.5%25%20performance%20of%20full%20labeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Full%2520Label%253A%2520Single-Point%2520Prompt%2520for%2520Infrared%2520Small%2520Target%2520Label%250A%2520%2520Generation%26entry.906535625%3DShuai%2520Yuan%2520and%2520Hanlin%2520Qin%2520and%2520Renke%2520Kou%2520and%2520Xiang%2520Yan%2520and%2520Zechuan%2520Li%2520and%2520Chenxu%2520Peng%2520and%2520Abd-Krim%2520Seghouane%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520make%2520the%2520first%2520attempt%2520to%2520construct%2520a%2520learning-based%250Asingle-point%2520annotation%2520paradigm%2520for%2520infrared%2520small%2520target%2520label%2520generation%250A%2528IRSTLG%2529.%2520Our%2520intuition%2520is%2520that%2520label%2520generation%2520requires%2520just%2520one%2520more%2520point%250Aprompt%2520than%2520target%2520detection%253A%2520IRSTLG%2520can%2520be%2520regarded%2520as%2520an%2520infrared%2520small%250Atarget%2520detection%2520%2528IRSTD%2529%2520task%2520with%2520the%2520target%2520location%2520hint.%2520Based%2520on%2520this%250Ainsight%252C%2520we%2520introduce%2520an%2520energy%2520double%2520guided%2520single-point%2520prompt%2520%2528EDGSP%2529%250Aframework%252C%2520which%2520adeptly%2520transforms%2520the%2520target%2520detection%2520network%2520into%2520a%2520refined%250Alabel%2520generation%2520method.%2520Specifically%252C%2520the%2520proposed%2520EDGSP%2520includes%253A%25201%2529%2520target%250Aenergy%2520initialization%2520%2528TEI%2529%2520to%2520create%2520a%2520foundational%2520outline%2520for%2520sufficient%250Ashape%2520evolution%2520of%2520pseudo%2520label%252C%25202%2529%2520double%2520prompt%2520embedding%2520%2528DPE%2529%2520for%2520rapid%250Alocalization%2520of%2520interested%2520regions%2520and%2520reinforcement%2520of%2520individual%2520differences%250Ato%2520avoid%2520label%2520adhesion%252C%2520and%25203%2529%2520bounding%2520box-based%2520matching%2520%2528BBM%2529%2520to%2520eliminate%250Afalse%2520alarms.%2520Experimental%2520results%2520show%2520that%2520pseudo%2520labels%2520generated%2520by%2520three%250Abaselines%2520equipped%2520with%2520EDGSP%2520achieve%2520100%2525%2520object-level%2520probability%2520of%250Adetection%2520%2528Pd%2529%2520and%25200%2525%2520false-alarm%2520rate%2520%2528Fa%2529%2520on%2520SIRST%252C%2520NUDT-SIRST%252C%2520and%2520IRSTD-1k%250Adatasets%252C%2520with%2520a%2520pixel-level%2520intersection%2520over%2520union%2520%2528IoU%2529%2520improvement%2520of%250A13.28%2525%2520over%2520state-of-the-art%2520%2528SOTA%2529%2520label%2520generation%2520methods.%2520In%2520the%2520practical%250Aapplication%2520of%2520downstream%2520IRSTD%252C%2520EDGSP%2520realizes%252C%2520for%2520the%2520first%2520time%252C%2520a%250Asingle-point%2520generated%2520pseudo%2520mask%2520beyond%2520the%2520full%2520label.%2520Even%2520with%2520coarse%250Asingle-point%2520annotations%252C%2520it%2520still%2520achieves%252099.5%2525%2520performance%2520of%2520full%2520labeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Full%20Label%3A%20Single-Point%20Prompt%20for%20Infrared%20Small%20Target%20Label%0A%20%20Generation&entry.906535625=Shuai%20Yuan%20and%20Hanlin%20Qin%20and%20Renke%20Kou%20and%20Xiang%20Yan%20and%20Zechuan%20Li%20and%20Chenxu%20Peng%20and%20Abd-Krim%20Seghouane&entry.1292438233=%20%20In%20this%20work%2C%20we%20make%20the%20first%20attempt%20to%20construct%20a%20learning-based%0Asingle-point%20annotation%20paradigm%20for%20infrared%20small%20target%20label%20generation%0A%28IRSTLG%29.%20Our%20intuition%20is%20that%20label%20generation%20requires%20just%20one%20more%20point%0Aprompt%20than%20target%20detection%3A%20IRSTLG%20can%20be%20regarded%20as%20an%20infrared%20small%0Atarget%20detection%20%28IRSTD%29%20task%20with%20the%20target%20location%20hint.%20Based%20on%20this%0Ainsight%2C%20we%20introduce%20an%20energy%20double%20guided%20single-point%20prompt%20%28EDGSP%29%0Aframework%2C%20which%20adeptly%20transforms%20the%20target%20detection%20network%20into%20a%20refined%0Alabel%20generation%20method.%20Specifically%2C%20the%20proposed%20EDGSP%20includes%3A%201%29%20target%0Aenergy%20initialization%20%28TEI%29%20to%20create%20a%20foundational%20outline%20for%20sufficient%0Ashape%20evolution%20of%20pseudo%20label%2C%202%29%20double%20prompt%20embedding%20%28DPE%29%20for%20rapid%0Alocalization%20of%20interested%20regions%20and%20reinforcement%20of%20individual%20differences%0Ato%20avoid%20label%20adhesion%2C%20and%203%29%20bounding%20box-based%20matching%20%28BBM%29%20to%20eliminate%0Afalse%20alarms.%20Experimental%20results%20show%20that%20pseudo%20labels%20generated%20by%20three%0Abaselines%20equipped%20with%20EDGSP%20achieve%20100%25%20object-level%20probability%20of%0Adetection%20%28Pd%29%20and%200%25%20false-alarm%20rate%20%28Fa%29%20on%20SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1k%0Adatasets%2C%20with%20a%20pixel-level%20intersection%20over%20union%20%28IoU%29%20improvement%20of%0A13.28%25%20over%20state-of-the-art%20%28SOTA%29%20label%20generation%20methods.%20In%20the%20practical%0Aapplication%20of%20downstream%20IRSTD%2C%20EDGSP%20realizes%2C%20for%20the%20first%20time%2C%20a%0Asingle-point%20generated%20pseudo%20mask%20beyond%20the%20full%20label.%20Even%20with%20coarse%0Asingle-point%20annotations%2C%20it%20still%20achieves%2099.5%25%20performance%20of%20full%20labeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08191v2&entry.124074799=Read"},
{"title": "On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A\n  Gradient-Norm Perspective", "author": "Zeke Xie and Zhiqiang Xu and Jingzhao Zhang and Issei Sato and Masashi Sugiyama", "abstract": "  Weight decay is a simple yet powerful regularization technique that has been\nvery widely used in training of deep neural networks (DNNs). While weight decay\nhas attracted much attention, previous studies fail to discover some overlooked\npitfalls on large gradient norms resulted by weight decay. In this paper, we\ndiscover that, weight decay can unfortunately lead to large gradient norms at\nthe final phase (or the terminated solution) of training, which often indicates\nbad convergence and poor generalization. To mitigate the gradient-norm-centered\npitfalls, we present the first practical scheduler for weight decay, called the\nScheduled Weight Decay (SWD) method that can dynamically adjust the weight\ndecay strength according to the gradient norm and significantly penalize large\ngradient norms during training. Our experiments also support that SWD indeed\nmitigates large gradient norms and often significantly outperforms the\nconventional constant weight decay strategy for Adaptive Moment Estimation\n(Adam).\n", "link": "http://arxiv.org/abs/2011.11152v6", "date": "2024-08-16", "relevancy": 1.997, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5058}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5018}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Overlooked%20Pitfalls%20of%20Weight%20Decay%20and%20How%20to%20Mitigate%20Them%3A%20A%0A%20%20Gradient-Norm%20Perspective&body=Title%3A%20On%20the%20Overlooked%20Pitfalls%20of%20Weight%20Decay%20and%20How%20to%20Mitigate%20Them%3A%20A%0A%20%20Gradient-Norm%20Perspective%0AAuthor%3A%20Zeke%20Xie%20and%20Zhiqiang%20Xu%20and%20Jingzhao%20Zhang%20and%20Issei%20Sato%20and%20Masashi%20Sugiyama%0AAbstract%3A%20%20%20Weight%20decay%20is%20a%20simple%20yet%20powerful%20regularization%20technique%20that%20has%20been%0Avery%20widely%20used%20in%20training%20of%20deep%20neural%20networks%20%28DNNs%29.%20While%20weight%20decay%0Ahas%20attracted%20much%20attention%2C%20previous%20studies%20fail%20to%20discover%20some%20overlooked%0Apitfalls%20on%20large%20gradient%20norms%20resulted%20by%20weight%20decay.%20In%20this%20paper%2C%20we%0Adiscover%20that%2C%20weight%20decay%20can%20unfortunately%20lead%20to%20large%20gradient%20norms%20at%0Athe%20final%20phase%20%28or%20the%20terminated%20solution%29%20of%20training%2C%20which%20often%20indicates%0Abad%20convergence%20and%20poor%20generalization.%20To%20mitigate%20the%20gradient-norm-centered%0Apitfalls%2C%20we%20present%20the%20first%20practical%20scheduler%20for%20weight%20decay%2C%20called%20the%0AScheduled%20Weight%20Decay%20%28SWD%29%20method%20that%20can%20dynamically%20adjust%20the%20weight%0Adecay%20strength%20according%20to%20the%20gradient%20norm%20and%20significantly%20penalize%20large%0Agradient%20norms%20during%20training.%20Our%20experiments%20also%20support%20that%20SWD%20indeed%0Amitigates%20large%20gradient%20norms%20and%20often%20significantly%20outperforms%20the%0Aconventional%20constant%20weight%20decay%20strategy%20for%20Adaptive%20Moment%20Estimation%0A%28Adam%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2011.11152v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Overlooked%2520Pitfalls%2520of%2520Weight%2520Decay%2520and%2520How%2520to%2520Mitigate%2520Them%253A%2520A%250A%2520%2520Gradient-Norm%2520Perspective%26entry.906535625%3DZeke%2520Xie%2520and%2520Zhiqiang%2520Xu%2520and%2520Jingzhao%2520Zhang%2520and%2520Issei%2520Sato%2520and%2520Masashi%2520Sugiyama%26entry.1292438233%3D%2520%2520Weight%2520decay%2520is%2520a%2520simple%2520yet%2520powerful%2520regularization%2520technique%2520that%2520has%2520been%250Avery%2520widely%2520used%2520in%2520training%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520While%2520weight%2520decay%250Ahas%2520attracted%2520much%2520attention%252C%2520previous%2520studies%2520fail%2520to%2520discover%2520some%2520overlooked%250Apitfalls%2520on%2520large%2520gradient%2520norms%2520resulted%2520by%2520weight%2520decay.%2520In%2520this%2520paper%252C%2520we%250Adiscover%2520that%252C%2520weight%2520decay%2520can%2520unfortunately%2520lead%2520to%2520large%2520gradient%2520norms%2520at%250Athe%2520final%2520phase%2520%2528or%2520the%2520terminated%2520solution%2529%2520of%2520training%252C%2520which%2520often%2520indicates%250Abad%2520convergence%2520and%2520poor%2520generalization.%2520To%2520mitigate%2520the%2520gradient-norm-centered%250Apitfalls%252C%2520we%2520present%2520the%2520first%2520practical%2520scheduler%2520for%2520weight%2520decay%252C%2520called%2520the%250AScheduled%2520Weight%2520Decay%2520%2528SWD%2529%2520method%2520that%2520can%2520dynamically%2520adjust%2520the%2520weight%250Adecay%2520strength%2520according%2520to%2520the%2520gradient%2520norm%2520and%2520significantly%2520penalize%2520large%250Agradient%2520norms%2520during%2520training.%2520Our%2520experiments%2520also%2520support%2520that%2520SWD%2520indeed%250Amitigates%2520large%2520gradient%2520norms%2520and%2520often%2520significantly%2520outperforms%2520the%250Aconventional%2520constant%2520weight%2520decay%2520strategy%2520for%2520Adaptive%2520Moment%2520Estimation%250A%2528Adam%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2011.11152v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Overlooked%20Pitfalls%20of%20Weight%20Decay%20and%20How%20to%20Mitigate%20Them%3A%20A%0A%20%20Gradient-Norm%20Perspective&entry.906535625=Zeke%20Xie%20and%20Zhiqiang%20Xu%20and%20Jingzhao%20Zhang%20and%20Issei%20Sato%20and%20Masashi%20Sugiyama&entry.1292438233=%20%20Weight%20decay%20is%20a%20simple%20yet%20powerful%20regularization%20technique%20that%20has%20been%0Avery%20widely%20used%20in%20training%20of%20deep%20neural%20networks%20%28DNNs%29.%20While%20weight%20decay%0Ahas%20attracted%20much%20attention%2C%20previous%20studies%20fail%20to%20discover%20some%20overlooked%0Apitfalls%20on%20large%20gradient%20norms%20resulted%20by%20weight%20decay.%20In%20this%20paper%2C%20we%0Adiscover%20that%2C%20weight%20decay%20can%20unfortunately%20lead%20to%20large%20gradient%20norms%20at%0Athe%20final%20phase%20%28or%20the%20terminated%20solution%29%20of%20training%2C%20which%20often%20indicates%0Abad%20convergence%20and%20poor%20generalization.%20To%20mitigate%20the%20gradient-norm-centered%0Apitfalls%2C%20we%20present%20the%20first%20practical%20scheduler%20for%20weight%20decay%2C%20called%20the%0AScheduled%20Weight%20Decay%20%28SWD%29%20method%20that%20can%20dynamically%20adjust%20the%20weight%0Adecay%20strength%20according%20to%20the%20gradient%20norm%20and%20significantly%20penalize%20large%0Agradient%20norms%20during%20training.%20Our%20experiments%20also%20support%20that%20SWD%20indeed%0Amitigates%20large%20gradient%20norms%20and%20often%20significantly%20outperforms%20the%0Aconventional%20constant%20weight%20decay%20strategy%20for%20Adaptive%20Moment%20Estimation%0A%28Adam%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2011.11152v6&entry.124074799=Read"},
{"title": "Automated Contrastive Learning Strategy Search for Time Series", "author": "Baoyu Jing and Yansen Wang and Guoxin Sui and Jing Hong and Jingrui He and Yuqing Yang and Dongsheng Li and Kan Ren", "abstract": "  In recent years, Contrastive Learning (CL) has become a predominant\nrepresentation learning paradigm for time series. Most existing methods\nmanually build specific CL Strategies (CLS) by human heuristics for certain\ndatasets and tasks. However, manually developing CLS usually requires excessive\nprior knowledge about the data, and massive experiments to determine the\ndetailed CL configurations. In this paper, we present an Automated Machine\nLearning (AutoML) practice at Microsoft, which automatically learns CLS for\ntime series datasets and tasks, namely Automated Contrastive Learning (AutoCL).\nWe first construct a principled search space of size over $3\\times10^{12}$,\ncovering data augmentation, embedding transformation, contrastive pair\nconstruction, and contrastive losses. Further, we introduce an efficient\nreinforcement learning algorithm, which optimizes CLS from the performance on\nthe validation tasks, to obtain effective CLS within the space. Experimental\nresults on various real-world datasets demonstrate that AutoCL could\nautomatically find the suitable CLS for the given dataset and task. From the\ncandidate CLS found by AutoCL on several public datasets/tasks, we compose a\ntransferable Generally Good Strategy (GGS), which has a strong performance for\nother datasets. We also provide empirical analysis as a guide for the future\ndesign of CLS.\n", "link": "http://arxiv.org/abs/2403.12641v2", "date": "2024-08-16", "relevancy": 1.9786, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4974}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Contrastive%20Learning%20Strategy%20Search%20for%20Time%20Series&body=Title%3A%20Automated%20Contrastive%20Learning%20Strategy%20Search%20for%20Time%20Series%0AAuthor%3A%20Baoyu%20Jing%20and%20Yansen%20Wang%20and%20Guoxin%20Sui%20and%20Jing%20Hong%20and%20Jingrui%20He%20and%20Yuqing%20Yang%20and%20Dongsheng%20Li%20and%20Kan%20Ren%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Contrastive%20Learning%20%28CL%29%20has%20become%20a%20predominant%0Arepresentation%20learning%20paradigm%20for%20time%20series.%20Most%20existing%20methods%0Amanually%20build%20specific%20CL%20Strategies%20%28CLS%29%20by%20human%20heuristics%20for%20certain%0Adatasets%20and%20tasks.%20However%2C%20manually%20developing%20CLS%20usually%20requires%20excessive%0Aprior%20knowledge%20about%20the%20data%2C%20and%20massive%20experiments%20to%20determine%20the%0Adetailed%20CL%20configurations.%20In%20this%20paper%2C%20we%20present%20an%20Automated%20Machine%0ALearning%20%28AutoML%29%20practice%20at%20Microsoft%2C%20which%20automatically%20learns%20CLS%20for%0Atime%20series%20datasets%20and%20tasks%2C%20namely%20Automated%20Contrastive%20Learning%20%28AutoCL%29.%0AWe%20first%20construct%20a%20principled%20search%20space%20of%20size%20over%20%243%5Ctimes10%5E%7B12%7D%24%2C%0Acovering%20data%20augmentation%2C%20embedding%20transformation%2C%20contrastive%20pair%0Aconstruction%2C%20and%20contrastive%20losses.%20Further%2C%20we%20introduce%20an%20efficient%0Areinforcement%20learning%20algorithm%2C%20which%20optimizes%20CLS%20from%20the%20performance%20on%0Athe%20validation%20tasks%2C%20to%20obtain%20effective%20CLS%20within%20the%20space.%20Experimental%0Aresults%20on%20various%20real-world%20datasets%20demonstrate%20that%20AutoCL%20could%0Aautomatically%20find%20the%20suitable%20CLS%20for%20the%20given%20dataset%20and%20task.%20From%20the%0Acandidate%20CLS%20found%20by%20AutoCL%20on%20several%20public%20datasets/tasks%2C%20we%20compose%20a%0Atransferable%20Generally%20Good%20Strategy%20%28GGS%29%2C%20which%20has%20a%20strong%20performance%20for%0Aother%20datasets.%20We%20also%20provide%20empirical%20analysis%20as%20a%20guide%20for%20the%20future%0Adesign%20of%20CLS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Contrastive%2520Learning%2520Strategy%2520Search%2520for%2520Time%2520Series%26entry.906535625%3DBaoyu%2520Jing%2520and%2520Yansen%2520Wang%2520and%2520Guoxin%2520Sui%2520and%2520Jing%2520Hong%2520and%2520Jingrui%2520He%2520and%2520Yuqing%2520Yang%2520and%2520Dongsheng%2520Li%2520and%2520Kan%2520Ren%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Contrastive%2520Learning%2520%2528CL%2529%2520has%2520become%2520a%2520predominant%250Arepresentation%2520learning%2520paradigm%2520for%2520time%2520series.%2520Most%2520existing%2520methods%250Amanually%2520build%2520specific%2520CL%2520Strategies%2520%2528CLS%2529%2520by%2520human%2520heuristics%2520for%2520certain%250Adatasets%2520and%2520tasks.%2520However%252C%2520manually%2520developing%2520CLS%2520usually%2520requires%2520excessive%250Aprior%2520knowledge%2520about%2520the%2520data%252C%2520and%2520massive%2520experiments%2520to%2520determine%2520the%250Adetailed%2520CL%2520configurations.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520Automated%2520Machine%250ALearning%2520%2528AutoML%2529%2520practice%2520at%2520Microsoft%252C%2520which%2520automatically%2520learns%2520CLS%2520for%250Atime%2520series%2520datasets%2520and%2520tasks%252C%2520namely%2520Automated%2520Contrastive%2520Learning%2520%2528AutoCL%2529.%250AWe%2520first%2520construct%2520a%2520principled%2520search%2520space%2520of%2520size%2520over%2520%25243%255Ctimes10%255E%257B12%257D%2524%252C%250Acovering%2520data%2520augmentation%252C%2520embedding%2520transformation%252C%2520contrastive%2520pair%250Aconstruction%252C%2520and%2520contrastive%2520losses.%2520Further%252C%2520we%2520introduce%2520an%2520efficient%250Areinforcement%2520learning%2520algorithm%252C%2520which%2520optimizes%2520CLS%2520from%2520the%2520performance%2520on%250Athe%2520validation%2520tasks%252C%2520to%2520obtain%2520effective%2520CLS%2520within%2520the%2520space.%2520Experimental%250Aresults%2520on%2520various%2520real-world%2520datasets%2520demonstrate%2520that%2520AutoCL%2520could%250Aautomatically%2520find%2520the%2520suitable%2520CLS%2520for%2520the%2520given%2520dataset%2520and%2520task.%2520From%2520the%250Acandidate%2520CLS%2520found%2520by%2520AutoCL%2520on%2520several%2520public%2520datasets/tasks%252C%2520we%2520compose%2520a%250Atransferable%2520Generally%2520Good%2520Strategy%2520%2528GGS%2529%252C%2520which%2520has%2520a%2520strong%2520performance%2520for%250Aother%2520datasets.%2520We%2520also%2520provide%2520empirical%2520analysis%2520as%2520a%2520guide%2520for%2520the%2520future%250Adesign%2520of%2520CLS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Contrastive%20Learning%20Strategy%20Search%20for%20Time%20Series&entry.906535625=Baoyu%20Jing%20and%20Yansen%20Wang%20and%20Guoxin%20Sui%20and%20Jing%20Hong%20and%20Jingrui%20He%20and%20Yuqing%20Yang%20and%20Dongsheng%20Li%20and%20Kan%20Ren&entry.1292438233=%20%20In%20recent%20years%2C%20Contrastive%20Learning%20%28CL%29%20has%20become%20a%20predominant%0Arepresentation%20learning%20paradigm%20for%20time%20series.%20Most%20existing%20methods%0Amanually%20build%20specific%20CL%20Strategies%20%28CLS%29%20by%20human%20heuristics%20for%20certain%0Adatasets%20and%20tasks.%20However%2C%20manually%20developing%20CLS%20usually%20requires%20excessive%0Aprior%20knowledge%20about%20the%20data%2C%20and%20massive%20experiments%20to%20determine%20the%0Adetailed%20CL%20configurations.%20In%20this%20paper%2C%20we%20present%20an%20Automated%20Machine%0ALearning%20%28AutoML%29%20practice%20at%20Microsoft%2C%20which%20automatically%20learns%20CLS%20for%0Atime%20series%20datasets%20and%20tasks%2C%20namely%20Automated%20Contrastive%20Learning%20%28AutoCL%29.%0AWe%20first%20construct%20a%20principled%20search%20space%20of%20size%20over%20%243%5Ctimes10%5E%7B12%7D%24%2C%0Acovering%20data%20augmentation%2C%20embedding%20transformation%2C%20contrastive%20pair%0Aconstruction%2C%20and%20contrastive%20losses.%20Further%2C%20we%20introduce%20an%20efficient%0Areinforcement%20learning%20algorithm%2C%20which%20optimizes%20CLS%20from%20the%20performance%20on%0Athe%20validation%20tasks%2C%20to%20obtain%20effective%20CLS%20within%20the%20space.%20Experimental%0Aresults%20on%20various%20real-world%20datasets%20demonstrate%20that%20AutoCL%20could%0Aautomatically%20find%20the%20suitable%20CLS%20for%20the%20given%20dataset%20and%20task.%20From%20the%0Acandidate%20CLS%20found%20by%20AutoCL%20on%20several%20public%20datasets/tasks%2C%20we%20compose%20a%0Atransferable%20Generally%20Good%20Strategy%20%28GGS%29%2C%20which%20has%20a%20strong%20performance%20for%0Aother%20datasets.%20We%20also%20provide%20empirical%20analysis%20as%20a%20guide%20for%20the%20future%0Adesign%20of%20CLS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12641v2&entry.124074799=Read"},
{"title": "Federated Natural Policy Gradient and Actor Critic Methods for\n  Multi-task Reinforcement Learning", "author": "Tong Yang and Shicong Cen and Yuting Wei and Yuxin Chen and Yuejie Chi", "abstract": "  Federated reinforcement learning (RL) enables collaborative decision making\nof multiple distributed agents without sharing local data trajectories. In this\nwork, we consider a multi-task setting, in which each agent has its own private\nreward function corresponding to different tasks, while sharing the same\ntransition kernel of the environment. Focusing on infinite-horizon Markov\ndecision processes, the goal is to learn a globally optimal policy that\nmaximizes the sum of the discounted total rewards of all the agents in a\ndecentralized manner, where each agent only communicates with its neighbors\nover some prescribed graph topology.\n  We develop federated vanilla and entropy-regularized natural policy gradient\n(NPG) methods in the tabular setting under softmax parameterization, where\ngradient tracking is applied to estimate the global Q-function to mitigate the\nimpact of imperfect information sharing. We establish non-asymptotic global\nconvergence guarantees under exact policy evaluation, where the rates are\nnearly independent of the size of the state-action space and illuminate the\nimpacts of network size and connectivity. To the best of our knowledge, this is\nthe first time that near dimension-free global convergence is established for\nfederated multi-task RL using policy optimization. We further go beyond the\ntabular setting by proposing a federated natural actor critic (NAC) method for\nmulti-task RL with function approximation, and establish its finite-time sample\ncomplexity taking the errors of function approximation into account.\n", "link": "http://arxiv.org/abs/2311.00201v2", "date": "2024-08-16", "relevancy": 1.9357, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Natural%20Policy%20Gradient%20and%20Actor%20Critic%20Methods%20for%0A%20%20Multi-task%20Reinforcement%20Learning&body=Title%3A%20Federated%20Natural%20Policy%20Gradient%20and%20Actor%20Critic%20Methods%20for%0A%20%20Multi-task%20Reinforcement%20Learning%0AAuthor%3A%20Tong%20Yang%20and%20Shicong%20Cen%20and%20Yuting%20Wei%20and%20Yuxin%20Chen%20and%20Yuejie%20Chi%0AAbstract%3A%20%20%20Federated%20reinforcement%20learning%20%28RL%29%20enables%20collaborative%20decision%20making%0Aof%20multiple%20distributed%20agents%20without%20sharing%20local%20data%20trajectories.%20In%20this%0Awork%2C%20we%20consider%20a%20multi-task%20setting%2C%20in%20which%20each%20agent%20has%20its%20own%20private%0Areward%20function%20corresponding%20to%20different%20tasks%2C%20while%20sharing%20the%20same%0Atransition%20kernel%20of%20the%20environment.%20Focusing%20on%20infinite-horizon%20Markov%0Adecision%20processes%2C%20the%20goal%20is%20to%20learn%20a%20globally%20optimal%20policy%20that%0Amaximizes%20the%20sum%20of%20the%20discounted%20total%20rewards%20of%20all%20the%20agents%20in%20a%0Adecentralized%20manner%2C%20where%20each%20agent%20only%20communicates%20with%20its%20neighbors%0Aover%20some%20prescribed%20graph%20topology.%0A%20%20We%20develop%20federated%20vanilla%20and%20entropy-regularized%20natural%20policy%20gradient%0A%28NPG%29%20methods%20in%20the%20tabular%20setting%20under%20softmax%20parameterization%2C%20where%0Agradient%20tracking%20is%20applied%20to%20estimate%20the%20global%20Q-function%20to%20mitigate%20the%0Aimpact%20of%20imperfect%20information%20sharing.%20We%20establish%20non-asymptotic%20global%0Aconvergence%20guarantees%20under%20exact%20policy%20evaluation%2C%20where%20the%20rates%20are%0Anearly%20independent%20of%20the%20size%20of%20the%20state-action%20space%20and%20illuminate%20the%0Aimpacts%20of%20network%20size%20and%20connectivity.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20time%20that%20near%20dimension-free%20global%20convergence%20is%20established%20for%0Afederated%20multi-task%20RL%20using%20policy%20optimization.%20We%20further%20go%20beyond%20the%0Atabular%20setting%20by%20proposing%20a%20federated%20natural%20actor%20critic%20%28NAC%29%20method%20for%0Amulti-task%20RL%20with%20function%20approximation%2C%20and%20establish%20its%20finite-time%20sample%0Acomplexity%20taking%20the%20errors%20of%20function%20approximation%20into%20account.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Natural%2520Policy%2520Gradient%2520and%2520Actor%2520Critic%2520Methods%2520for%250A%2520%2520Multi-task%2520Reinforcement%2520Learning%26entry.906535625%3DTong%2520Yang%2520and%2520Shicong%2520Cen%2520and%2520Yuting%2520Wei%2520and%2520Yuxin%2520Chen%2520and%2520Yuejie%2520Chi%26entry.1292438233%3D%2520%2520Federated%2520reinforcement%2520learning%2520%2528RL%2529%2520enables%2520collaborative%2520decision%2520making%250Aof%2520multiple%2520distributed%2520agents%2520without%2520sharing%2520local%2520data%2520trajectories.%2520In%2520this%250Awork%252C%2520we%2520consider%2520a%2520multi-task%2520setting%252C%2520in%2520which%2520each%2520agent%2520has%2520its%2520own%2520private%250Areward%2520function%2520corresponding%2520to%2520different%2520tasks%252C%2520while%2520sharing%2520the%2520same%250Atransition%2520kernel%2520of%2520the%2520environment.%2520Focusing%2520on%2520infinite-horizon%2520Markov%250Adecision%2520processes%252C%2520the%2520goal%2520is%2520to%2520learn%2520a%2520globally%2520optimal%2520policy%2520that%250Amaximizes%2520the%2520sum%2520of%2520the%2520discounted%2520total%2520rewards%2520of%2520all%2520the%2520agents%2520in%2520a%250Adecentralized%2520manner%252C%2520where%2520each%2520agent%2520only%2520communicates%2520with%2520its%2520neighbors%250Aover%2520some%2520prescribed%2520graph%2520topology.%250A%2520%2520We%2520develop%2520federated%2520vanilla%2520and%2520entropy-regularized%2520natural%2520policy%2520gradient%250A%2528NPG%2529%2520methods%2520in%2520the%2520tabular%2520setting%2520under%2520softmax%2520parameterization%252C%2520where%250Agradient%2520tracking%2520is%2520applied%2520to%2520estimate%2520the%2520global%2520Q-function%2520to%2520mitigate%2520the%250Aimpact%2520of%2520imperfect%2520information%2520sharing.%2520We%2520establish%2520non-asymptotic%2520global%250Aconvergence%2520guarantees%2520under%2520exact%2520policy%2520evaluation%252C%2520where%2520the%2520rates%2520are%250Anearly%2520independent%2520of%2520the%2520size%2520of%2520the%2520state-action%2520space%2520and%2520illuminate%2520the%250Aimpacts%2520of%2520network%2520size%2520and%2520connectivity.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520time%2520that%2520near%2520dimension-free%2520global%2520convergence%2520is%2520established%2520for%250Afederated%2520multi-task%2520RL%2520using%2520policy%2520optimization.%2520We%2520further%2520go%2520beyond%2520the%250Atabular%2520setting%2520by%2520proposing%2520a%2520federated%2520natural%2520actor%2520critic%2520%2528NAC%2529%2520method%2520for%250Amulti-task%2520RL%2520with%2520function%2520approximation%252C%2520and%2520establish%2520its%2520finite-time%2520sample%250Acomplexity%2520taking%2520the%2520errors%2520of%2520function%2520approximation%2520into%2520account.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Natural%20Policy%20Gradient%20and%20Actor%20Critic%20Methods%20for%0A%20%20Multi-task%20Reinforcement%20Learning&entry.906535625=Tong%20Yang%20and%20Shicong%20Cen%20and%20Yuting%20Wei%20and%20Yuxin%20Chen%20and%20Yuejie%20Chi&entry.1292438233=%20%20Federated%20reinforcement%20learning%20%28RL%29%20enables%20collaborative%20decision%20making%0Aof%20multiple%20distributed%20agents%20without%20sharing%20local%20data%20trajectories.%20In%20this%0Awork%2C%20we%20consider%20a%20multi-task%20setting%2C%20in%20which%20each%20agent%20has%20its%20own%20private%0Areward%20function%20corresponding%20to%20different%20tasks%2C%20while%20sharing%20the%20same%0Atransition%20kernel%20of%20the%20environment.%20Focusing%20on%20infinite-horizon%20Markov%0Adecision%20processes%2C%20the%20goal%20is%20to%20learn%20a%20globally%20optimal%20policy%20that%0Amaximizes%20the%20sum%20of%20the%20discounted%20total%20rewards%20of%20all%20the%20agents%20in%20a%0Adecentralized%20manner%2C%20where%20each%20agent%20only%20communicates%20with%20its%20neighbors%0Aover%20some%20prescribed%20graph%20topology.%0A%20%20We%20develop%20federated%20vanilla%20and%20entropy-regularized%20natural%20policy%20gradient%0A%28NPG%29%20methods%20in%20the%20tabular%20setting%20under%20softmax%20parameterization%2C%20where%0Agradient%20tracking%20is%20applied%20to%20estimate%20the%20global%20Q-function%20to%20mitigate%20the%0Aimpact%20of%20imperfect%20information%20sharing.%20We%20establish%20non-asymptotic%20global%0Aconvergence%20guarantees%20under%20exact%20policy%20evaluation%2C%20where%20the%20rates%20are%0Anearly%20independent%20of%20the%20size%20of%20the%20state-action%20space%20and%20illuminate%20the%0Aimpacts%20of%20network%20size%20and%20connectivity.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20time%20that%20near%20dimension-free%20global%20convergence%20is%20established%20for%0Afederated%20multi-task%20RL%20using%20policy%20optimization.%20We%20further%20go%20beyond%20the%0Atabular%20setting%20by%20proposing%20a%20federated%20natural%20actor%20critic%20%28NAC%29%20method%20for%0Amulti-task%20RL%20with%20function%20approximation%2C%20and%20establish%20its%20finite-time%20sample%0Acomplexity%20taking%20the%20errors%20of%20function%20approximation%20into%20account.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00201v2&entry.124074799=Read"},
{"title": "CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise\n  Classification", "author": "Yiming Ma and Victor Sanchez and Tanaya Guha", "abstract": "  We propose CLIP-EBC, the first fully CLIP-based model for accurate crowd\ndensity estimation. While the CLIP model has demonstrated remarkable success in\naddressing recognition tasks such as zero-shot image classification, its\npotential for counting has been largely unexplored due to the inherent\nchallenges in transforming a regression problem, such as counting, into a\nrecognition task. In this work, we investigate and enhance CLIP's ability to\ncount, focusing specifically on the task of estimating crowd sizes from images.\nExisting classification-based crowd-counting frameworks have significant\nlimitations, including the quantization of count values into bordering\nreal-valued bins and the sole focus on classification errors. These practices\nresult in label ambiguity near the shared borders and inaccurate prediction of\ncount values. Hence, directly applying CLIP within these frameworks may yield\nsuboptimal performance.\n  To address these challenges, we first propose the Enhanced Blockwise\nClassification (EBC) framework. Unlike previous methods, EBC utilizes\ninteger-valued bins, effectively reducing ambiguity near bin boundaries.\nAdditionally, it incorporates a regression loss based on density maps to\nimprove the prediction of count values. Within our backbone-agnostic EBC\nframework, we then introduce CLIP-EBC to fully leverage CLIP's recognition\ncapabilities for this task. Extensive experiments demonstrate the effectiveness\nof EBC and the competitive performance of CLIP-EBC. Specifically, our EBC\nframework can improve existing classification-based methods by up to 44.5% on\nthe UCF-QNRF dataset, and CLIP-EBC achieves state-of-the-art performance on the\nNWPU-Crowd test set, with an MAE of 58.2 and an RMSE of 268.5, representing\nimprovements of 8.6% and 13.3% over the previous best method, STEERER. The code\nand weights are available at https://github.com/Yiming-M/CLIP-EBC.\n", "link": "http://arxiv.org/abs/2403.09281v2", "date": "2024-08-16", "relevancy": 1.9346, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4924}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4832}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification&body=Title%3A%20CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification%0AAuthor%3A%20Yiming%20Ma%20and%20Victor%20Sanchez%20and%20Tanaya%20Guha%0AAbstract%3A%20%20%20We%20propose%20CLIP-EBC%2C%20the%20first%20fully%20CLIP-based%20model%20for%20accurate%20crowd%0Adensity%20estimation.%20While%20the%20CLIP%20model%20has%20demonstrated%20remarkable%20success%20in%0Aaddressing%20recognition%20tasks%20such%20as%20zero-shot%20image%20classification%2C%20its%0Apotential%20for%20counting%20has%20been%20largely%20unexplored%20due%20to%20the%20inherent%0Achallenges%20in%20transforming%20a%20regression%20problem%2C%20such%20as%20counting%2C%20into%20a%0Arecognition%20task.%20In%20this%20work%2C%20we%20investigate%20and%20enhance%20CLIP%27s%20ability%20to%0Acount%2C%20focusing%20specifically%20on%20the%20task%20of%20estimating%20crowd%20sizes%20from%20images.%0AExisting%20classification-based%20crowd-counting%20frameworks%20have%20significant%0Alimitations%2C%20including%20the%20quantization%20of%20count%20values%20into%20bordering%0Areal-valued%20bins%20and%20the%20sole%20focus%20on%20classification%20errors.%20These%20practices%0Aresult%20in%20label%20ambiguity%20near%20the%20shared%20borders%20and%20inaccurate%20prediction%20of%0Acount%20values.%20Hence%2C%20directly%20applying%20CLIP%20within%20these%20frameworks%20may%20yield%0Asuboptimal%20performance.%0A%20%20To%20address%20these%20challenges%2C%20we%20first%20propose%20the%20Enhanced%20Blockwise%0AClassification%20%28EBC%29%20framework.%20Unlike%20previous%20methods%2C%20EBC%20utilizes%0Ainteger-valued%20bins%2C%20effectively%20reducing%20ambiguity%20near%20bin%20boundaries.%0AAdditionally%2C%20it%20incorporates%20a%20regression%20loss%20based%20on%20density%20maps%20to%0Aimprove%20the%20prediction%20of%20count%20values.%20Within%20our%20backbone-agnostic%20EBC%0Aframework%2C%20we%20then%20introduce%20CLIP-EBC%20to%20fully%20leverage%20CLIP%27s%20recognition%0Acapabilities%20for%20this%20task.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20EBC%20and%20the%20competitive%20performance%20of%20CLIP-EBC.%20Specifically%2C%20our%20EBC%0Aframework%20can%20improve%20existing%20classification-based%20methods%20by%20up%20to%2044.5%25%20on%0Athe%20UCF-QNRF%20dataset%2C%20and%20CLIP-EBC%20achieves%20state-of-the-art%20performance%20on%20the%0ANWPU-Crowd%20test%20set%2C%20with%20an%20MAE%20of%2058.2%20and%20an%20RMSE%20of%20268.5%2C%20representing%0Aimprovements%20of%208.6%25%20and%2013.3%25%20over%20the%20previous%20best%20method%2C%20STEERER.%20The%20code%0Aand%20weights%20are%20available%20at%20https%3A//github.com/Yiming-M/CLIP-EBC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09281v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-EBC%253A%2520CLIP%2520Can%2520Count%2520Accurately%2520through%2520Enhanced%2520Blockwise%250A%2520%2520Classification%26entry.906535625%3DYiming%2520Ma%2520and%2520Victor%2520Sanchez%2520and%2520Tanaya%2520Guha%26entry.1292438233%3D%2520%2520We%2520propose%2520CLIP-EBC%252C%2520the%2520first%2520fully%2520CLIP-based%2520model%2520for%2520accurate%2520crowd%250Adensity%2520estimation.%2520While%2520the%2520CLIP%2520model%2520has%2520demonstrated%2520remarkable%2520success%2520in%250Aaddressing%2520recognition%2520tasks%2520such%2520as%2520zero-shot%2520image%2520classification%252C%2520its%250Apotential%2520for%2520counting%2520has%2520been%2520largely%2520unexplored%2520due%2520to%2520the%2520inherent%250Achallenges%2520in%2520transforming%2520a%2520regression%2520problem%252C%2520such%2520as%2520counting%252C%2520into%2520a%250Arecognition%2520task.%2520In%2520this%2520work%252C%2520we%2520investigate%2520and%2520enhance%2520CLIP%2527s%2520ability%2520to%250Acount%252C%2520focusing%2520specifically%2520on%2520the%2520task%2520of%2520estimating%2520crowd%2520sizes%2520from%2520images.%250AExisting%2520classification-based%2520crowd-counting%2520frameworks%2520have%2520significant%250Alimitations%252C%2520including%2520the%2520quantization%2520of%2520count%2520values%2520into%2520bordering%250Areal-valued%2520bins%2520and%2520the%2520sole%2520focus%2520on%2520classification%2520errors.%2520These%2520practices%250Aresult%2520in%2520label%2520ambiguity%2520near%2520the%2520shared%2520borders%2520and%2520inaccurate%2520prediction%2520of%250Acount%2520values.%2520Hence%252C%2520directly%2520applying%2520CLIP%2520within%2520these%2520frameworks%2520may%2520yield%250Asuboptimal%2520performance.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520first%2520propose%2520the%2520Enhanced%2520Blockwise%250AClassification%2520%2528EBC%2529%2520framework.%2520Unlike%2520previous%2520methods%252C%2520EBC%2520utilizes%250Ainteger-valued%2520bins%252C%2520effectively%2520reducing%2520ambiguity%2520near%2520bin%2520boundaries.%250AAdditionally%252C%2520it%2520incorporates%2520a%2520regression%2520loss%2520based%2520on%2520density%2520maps%2520to%250Aimprove%2520the%2520prediction%2520of%2520count%2520values.%2520Within%2520our%2520backbone-agnostic%2520EBC%250Aframework%252C%2520we%2520then%2520introduce%2520CLIP-EBC%2520to%2520fully%2520leverage%2520CLIP%2527s%2520recognition%250Acapabilities%2520for%2520this%2520task.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aof%2520EBC%2520and%2520the%2520competitive%2520performance%2520of%2520CLIP-EBC.%2520Specifically%252C%2520our%2520EBC%250Aframework%2520can%2520improve%2520existing%2520classification-based%2520methods%2520by%2520up%2520to%252044.5%2525%2520on%250Athe%2520UCF-QNRF%2520dataset%252C%2520and%2520CLIP-EBC%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250ANWPU-Crowd%2520test%2520set%252C%2520with%2520an%2520MAE%2520of%252058.2%2520and%2520an%2520RMSE%2520of%2520268.5%252C%2520representing%250Aimprovements%2520of%25208.6%2525%2520and%252013.3%2525%2520over%2520the%2520previous%2520best%2520method%252C%2520STEERER.%2520The%2520code%250Aand%2520weights%2520are%2520available%2520at%2520https%253A//github.com/Yiming-M/CLIP-EBC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09281v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification&entry.906535625=Yiming%20Ma%20and%20Victor%20Sanchez%20and%20Tanaya%20Guha&entry.1292438233=%20%20We%20propose%20CLIP-EBC%2C%20the%20first%20fully%20CLIP-based%20model%20for%20accurate%20crowd%0Adensity%20estimation.%20While%20the%20CLIP%20model%20has%20demonstrated%20remarkable%20success%20in%0Aaddressing%20recognition%20tasks%20such%20as%20zero-shot%20image%20classification%2C%20its%0Apotential%20for%20counting%20has%20been%20largely%20unexplored%20due%20to%20the%20inherent%0Achallenges%20in%20transforming%20a%20regression%20problem%2C%20such%20as%20counting%2C%20into%20a%0Arecognition%20task.%20In%20this%20work%2C%20we%20investigate%20and%20enhance%20CLIP%27s%20ability%20to%0Acount%2C%20focusing%20specifically%20on%20the%20task%20of%20estimating%20crowd%20sizes%20from%20images.%0AExisting%20classification-based%20crowd-counting%20frameworks%20have%20significant%0Alimitations%2C%20including%20the%20quantization%20of%20count%20values%20into%20bordering%0Areal-valued%20bins%20and%20the%20sole%20focus%20on%20classification%20errors.%20These%20practices%0Aresult%20in%20label%20ambiguity%20near%20the%20shared%20borders%20and%20inaccurate%20prediction%20of%0Acount%20values.%20Hence%2C%20directly%20applying%20CLIP%20within%20these%20frameworks%20may%20yield%0Asuboptimal%20performance.%0A%20%20To%20address%20these%20challenges%2C%20we%20first%20propose%20the%20Enhanced%20Blockwise%0AClassification%20%28EBC%29%20framework.%20Unlike%20previous%20methods%2C%20EBC%20utilizes%0Ainteger-valued%20bins%2C%20effectively%20reducing%20ambiguity%20near%20bin%20boundaries.%0AAdditionally%2C%20it%20incorporates%20a%20regression%20loss%20based%20on%20density%20maps%20to%0Aimprove%20the%20prediction%20of%20count%20values.%20Within%20our%20backbone-agnostic%20EBC%0Aframework%2C%20we%20then%20introduce%20CLIP-EBC%20to%20fully%20leverage%20CLIP%27s%20recognition%0Acapabilities%20for%20this%20task.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20EBC%20and%20the%20competitive%20performance%20of%20CLIP-EBC.%20Specifically%2C%20our%20EBC%0Aframework%20can%20improve%20existing%20classification-based%20methods%20by%20up%20to%2044.5%25%20on%0Athe%20UCF-QNRF%20dataset%2C%20and%20CLIP-EBC%20achieves%20state-of-the-art%20performance%20on%20the%0ANWPU-Crowd%20test%20set%2C%20with%20an%20MAE%20of%2058.2%20and%20an%20RMSE%20of%20268.5%2C%20representing%0Aimprovements%20of%208.6%25%20and%2013.3%25%20over%20the%20previous%20best%20method%2C%20STEERER.%20The%20code%0Aand%20weights%20are%20available%20at%20https%3A//github.com/Yiming-M/CLIP-EBC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09281v2&entry.124074799=Read"},
{"title": "A Mean Field Ansatz for Zero-Shot Weight Transfer", "author": "Xingyuan Chen and Wenwei Kuang and Lei Deng and Wei Han and Bo Bai and Goncalo dos Reis", "abstract": "  The pre-training cost of large language models (LLMs) is prohibitive. One\ncutting-edge approach to reduce the cost is zero-shot weight transfer, also\nknown as model growth for some cases, which magically transfers the weights\ntrained in a small model to a large model. However, there are still some\ntheoretical mysteries behind the weight transfer. In this paper, inspired by\nprior applications of mean field theory to neural network dynamics, we\nintroduce a mean field ansatz to provide a theoretical explanation for weight\ntransfer. Specifically, we propose the row-column (RC) ansatz under the mean\nfield point of view, which describes the measure structure of the weights in\nthe neural network (NN) and admits a close measure dynamic. Thus, the weights\nof different sizes NN admit a common distribution under proper assumptions, and\nweight transfer methods can be viewed as sampling methods. We empirically\nvalidate the RC ansatz by exploring simple MLP examples and LLMs such as GPT-3\nand Llama-3.1. We show the mean-field point of view is adequate under suitable\nassumptions which can provide theoretical support for zero-shot weight\ntransfer.\n", "link": "http://arxiv.org/abs/2408.08681v1", "date": "2024-08-16", "relevancy": 1.9293, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4743}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mean%20Field%20Ansatz%20for%20Zero-Shot%20Weight%20Transfer&body=Title%3A%20A%20Mean%20Field%20Ansatz%20for%20Zero-Shot%20Weight%20Transfer%0AAuthor%3A%20Xingyuan%20Chen%20and%20Wenwei%20Kuang%20and%20Lei%20Deng%20and%20Wei%20Han%20and%20Bo%20Bai%20and%20Goncalo%20dos%20Reis%0AAbstract%3A%20%20%20The%20pre-training%20cost%20of%20large%20language%20models%20%28LLMs%29%20is%20prohibitive.%20One%0Acutting-edge%20approach%20to%20reduce%20the%20cost%20is%20zero-shot%20weight%20transfer%2C%20also%0Aknown%20as%20model%20growth%20for%20some%20cases%2C%20which%20magically%20transfers%20the%20weights%0Atrained%20in%20a%20small%20model%20to%20a%20large%20model.%20However%2C%20there%20are%20still%20some%0Atheoretical%20mysteries%20behind%20the%20weight%20transfer.%20In%20this%20paper%2C%20inspired%20by%0Aprior%20applications%20of%20mean%20field%20theory%20to%20neural%20network%20dynamics%2C%20we%0Aintroduce%20a%20mean%20field%20ansatz%20to%20provide%20a%20theoretical%20explanation%20for%20weight%0Atransfer.%20Specifically%2C%20we%20propose%20the%20row-column%20%28RC%29%20ansatz%20under%20the%20mean%0Afield%20point%20of%20view%2C%20which%20describes%20the%20measure%20structure%20of%20the%20weights%20in%0Athe%20neural%20network%20%28NN%29%20and%20admits%20a%20close%20measure%20dynamic.%20Thus%2C%20the%20weights%0Aof%20different%20sizes%20NN%20admit%20a%20common%20distribution%20under%20proper%20assumptions%2C%20and%0Aweight%20transfer%20methods%20can%20be%20viewed%20as%20sampling%20methods.%20We%20empirically%0Avalidate%20the%20RC%20ansatz%20by%20exploring%20simple%20MLP%20examples%20and%20LLMs%20such%20as%20GPT-3%0Aand%20Llama-3.1.%20We%20show%20the%20mean-field%20point%20of%20view%20is%20adequate%20under%20suitable%0Aassumptions%20which%20can%20provide%20theoretical%20support%20for%20zero-shot%20weight%0Atransfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mean%2520Field%2520Ansatz%2520for%2520Zero-Shot%2520Weight%2520Transfer%26entry.906535625%3DXingyuan%2520Chen%2520and%2520Wenwei%2520Kuang%2520and%2520Lei%2520Deng%2520and%2520Wei%2520Han%2520and%2520Bo%2520Bai%2520and%2520Goncalo%2520dos%2520Reis%26entry.1292438233%3D%2520%2520The%2520pre-training%2520cost%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520prohibitive.%2520One%250Acutting-edge%2520approach%2520to%2520reduce%2520the%2520cost%2520is%2520zero-shot%2520weight%2520transfer%252C%2520also%250Aknown%2520as%2520model%2520growth%2520for%2520some%2520cases%252C%2520which%2520magically%2520transfers%2520the%2520weights%250Atrained%2520in%2520a%2520small%2520model%2520to%2520a%2520large%2520model.%2520However%252C%2520there%2520are%2520still%2520some%250Atheoretical%2520mysteries%2520behind%2520the%2520weight%2520transfer.%2520In%2520this%2520paper%252C%2520inspired%2520by%250Aprior%2520applications%2520of%2520mean%2520field%2520theory%2520to%2520neural%2520network%2520dynamics%252C%2520we%250Aintroduce%2520a%2520mean%2520field%2520ansatz%2520to%2520provide%2520a%2520theoretical%2520explanation%2520for%2520weight%250Atransfer.%2520Specifically%252C%2520we%2520propose%2520the%2520row-column%2520%2528RC%2529%2520ansatz%2520under%2520the%2520mean%250Afield%2520point%2520of%2520view%252C%2520which%2520describes%2520the%2520measure%2520structure%2520of%2520the%2520weights%2520in%250Athe%2520neural%2520network%2520%2528NN%2529%2520and%2520admits%2520a%2520close%2520measure%2520dynamic.%2520Thus%252C%2520the%2520weights%250Aof%2520different%2520sizes%2520NN%2520admit%2520a%2520common%2520distribution%2520under%2520proper%2520assumptions%252C%2520and%250Aweight%2520transfer%2520methods%2520can%2520be%2520viewed%2520as%2520sampling%2520methods.%2520We%2520empirically%250Avalidate%2520the%2520RC%2520ansatz%2520by%2520exploring%2520simple%2520MLP%2520examples%2520and%2520LLMs%2520such%2520as%2520GPT-3%250Aand%2520Llama-3.1.%2520We%2520show%2520the%2520mean-field%2520point%2520of%2520view%2520is%2520adequate%2520under%2520suitable%250Aassumptions%2520which%2520can%2520provide%2520theoretical%2520support%2520for%2520zero-shot%2520weight%250Atransfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mean%20Field%20Ansatz%20for%20Zero-Shot%20Weight%20Transfer&entry.906535625=Xingyuan%20Chen%20and%20Wenwei%20Kuang%20and%20Lei%20Deng%20and%20Wei%20Han%20and%20Bo%20Bai%20and%20Goncalo%20dos%20Reis&entry.1292438233=%20%20The%20pre-training%20cost%20of%20large%20language%20models%20%28LLMs%29%20is%20prohibitive.%20One%0Acutting-edge%20approach%20to%20reduce%20the%20cost%20is%20zero-shot%20weight%20transfer%2C%20also%0Aknown%20as%20model%20growth%20for%20some%20cases%2C%20which%20magically%20transfers%20the%20weights%0Atrained%20in%20a%20small%20model%20to%20a%20large%20model.%20However%2C%20there%20are%20still%20some%0Atheoretical%20mysteries%20behind%20the%20weight%20transfer.%20In%20this%20paper%2C%20inspired%20by%0Aprior%20applications%20of%20mean%20field%20theory%20to%20neural%20network%20dynamics%2C%20we%0Aintroduce%20a%20mean%20field%20ansatz%20to%20provide%20a%20theoretical%20explanation%20for%20weight%0Atransfer.%20Specifically%2C%20we%20propose%20the%20row-column%20%28RC%29%20ansatz%20under%20the%20mean%0Afield%20point%20of%20view%2C%20which%20describes%20the%20measure%20structure%20of%20the%20weights%20in%0Athe%20neural%20network%20%28NN%29%20and%20admits%20a%20close%20measure%20dynamic.%20Thus%2C%20the%20weights%0Aof%20different%20sizes%20NN%20admit%20a%20common%20distribution%20under%20proper%20assumptions%2C%20and%0Aweight%20transfer%20methods%20can%20be%20viewed%20as%20sampling%20methods.%20We%20empirically%0Avalidate%20the%20RC%20ansatz%20by%20exploring%20simple%20MLP%20examples%20and%20LLMs%20such%20as%20GPT-3%0Aand%20Llama-3.1.%20We%20show%20the%20mean-field%20point%20of%20view%20is%20adequate%20under%20suitable%0Aassumptions%20which%20can%20provide%20theoretical%20support%20for%20zero-shot%20weight%0Atransfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08681v1&entry.124074799=Read"},
{"title": "Dataset-learning duality and emergent criticality", "author": "Ekaterina Kukleva and Vitaly Vanchurin", "abstract": "  In artificial neural networks, the activation dynamics of non-trainable\nvariables is strongly coupled to the learning dynamics of trainable variables.\nDuring the activation pass, the boundary neurons (e.g., input neurons) are\nmapped to the bulk neurons (e.g., hidden neurons), and during the learning\npass, both bulk and boundary neurons are mapped to changes in trainable\nvariables (e.g., weights and biases). For example, in feed-forward neural\nnetworks, forward propagation is the activation pass and backward propagation\nis the learning pass. We show that a composition of the two maps establishes a\nduality map between a subspace of non-trainable boundary variables (e.g.,\ndataset) and a tangent subspace of trainable variables (i.e., learning). In\ngeneral, the dataset-learning duality is a complex non-linear map between\nhigh-dimensional spaces, but in a learning equilibrium, the problem can be\nlinearized and reduced to many weakly coupled one-dimensional problems. We use\nthe duality to study the emergence of criticality, or the power-law\ndistributions of fluctuations of the trainable variables. In particular, we\nshow that criticality can emerge in the learning system even from the dataset\nin a non-critical state, and that the power-law distribution can be modified by\nchanging either the activation function or the loss function.\n", "link": "http://arxiv.org/abs/2405.17391v2", "date": "2024-08-16", "relevancy": 1.9216, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5021}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4756}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset-learning%20duality%20and%20emergent%20criticality&body=Title%3A%20Dataset-learning%20duality%20and%20emergent%20criticality%0AAuthor%3A%20Ekaterina%20Kukleva%20and%20Vitaly%20Vanchurin%0AAbstract%3A%20%20%20In%20artificial%20neural%20networks%2C%20the%20activation%20dynamics%20of%20non-trainable%0Avariables%20is%20strongly%20coupled%20to%20the%20learning%20dynamics%20of%20trainable%20variables.%0ADuring%20the%20activation%20pass%2C%20the%20boundary%20neurons%20%28e.g.%2C%20input%20neurons%29%20are%0Amapped%20to%20the%20bulk%20neurons%20%28e.g.%2C%20hidden%20neurons%29%2C%20and%20during%20the%20learning%0Apass%2C%20both%20bulk%20and%20boundary%20neurons%20are%20mapped%20to%20changes%20in%20trainable%0Avariables%20%28e.g.%2C%20weights%20and%20biases%29.%20For%20example%2C%20in%20feed-forward%20neural%0Anetworks%2C%20forward%20propagation%20is%20the%20activation%20pass%20and%20backward%20propagation%0Ais%20the%20learning%20pass.%20We%20show%20that%20a%20composition%20of%20the%20two%20maps%20establishes%20a%0Aduality%20map%20between%20a%20subspace%20of%20non-trainable%20boundary%20variables%20%28e.g.%2C%0Adataset%29%20and%20a%20tangent%20subspace%20of%20trainable%20variables%20%28i.e.%2C%20learning%29.%20In%0Ageneral%2C%20the%20dataset-learning%20duality%20is%20a%20complex%20non-linear%20map%20between%0Ahigh-dimensional%20spaces%2C%20but%20in%20a%20learning%20equilibrium%2C%20the%20problem%20can%20be%0Alinearized%20and%20reduced%20to%20many%20weakly%20coupled%20one-dimensional%20problems.%20We%20use%0Athe%20duality%20to%20study%20the%20emergence%20of%20criticality%2C%20or%20the%20power-law%0Adistributions%20of%20fluctuations%20of%20the%20trainable%20variables.%20In%20particular%2C%20we%0Ashow%20that%20criticality%20can%20emerge%20in%20the%20learning%20system%20even%20from%20the%20dataset%0Ain%20a%20non-critical%20state%2C%20and%20that%20the%20power-law%20distribution%20can%20be%20modified%20by%0Achanging%20either%20the%20activation%20function%20or%20the%20loss%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset-learning%2520duality%2520and%2520emergent%2520criticality%26entry.906535625%3DEkaterina%2520Kukleva%2520and%2520Vitaly%2520Vanchurin%26entry.1292438233%3D%2520%2520In%2520artificial%2520neural%2520networks%252C%2520the%2520activation%2520dynamics%2520of%2520non-trainable%250Avariables%2520is%2520strongly%2520coupled%2520to%2520the%2520learning%2520dynamics%2520of%2520trainable%2520variables.%250ADuring%2520the%2520activation%2520pass%252C%2520the%2520boundary%2520neurons%2520%2528e.g.%252C%2520input%2520neurons%2529%2520are%250Amapped%2520to%2520the%2520bulk%2520neurons%2520%2528e.g.%252C%2520hidden%2520neurons%2529%252C%2520and%2520during%2520the%2520learning%250Apass%252C%2520both%2520bulk%2520and%2520boundary%2520neurons%2520are%2520mapped%2520to%2520changes%2520in%2520trainable%250Avariables%2520%2528e.g.%252C%2520weights%2520and%2520biases%2529.%2520For%2520example%252C%2520in%2520feed-forward%2520neural%250Anetworks%252C%2520forward%2520propagation%2520is%2520the%2520activation%2520pass%2520and%2520backward%2520propagation%250Ais%2520the%2520learning%2520pass.%2520We%2520show%2520that%2520a%2520composition%2520of%2520the%2520two%2520maps%2520establishes%2520a%250Aduality%2520map%2520between%2520a%2520subspace%2520of%2520non-trainable%2520boundary%2520variables%2520%2528e.g.%252C%250Adataset%2529%2520and%2520a%2520tangent%2520subspace%2520of%2520trainable%2520variables%2520%2528i.e.%252C%2520learning%2529.%2520In%250Ageneral%252C%2520the%2520dataset-learning%2520duality%2520is%2520a%2520complex%2520non-linear%2520map%2520between%250Ahigh-dimensional%2520spaces%252C%2520but%2520in%2520a%2520learning%2520equilibrium%252C%2520the%2520problem%2520can%2520be%250Alinearized%2520and%2520reduced%2520to%2520many%2520weakly%2520coupled%2520one-dimensional%2520problems.%2520We%2520use%250Athe%2520duality%2520to%2520study%2520the%2520emergence%2520of%2520criticality%252C%2520or%2520the%2520power-law%250Adistributions%2520of%2520fluctuations%2520of%2520the%2520trainable%2520variables.%2520In%2520particular%252C%2520we%250Ashow%2520that%2520criticality%2520can%2520emerge%2520in%2520the%2520learning%2520system%2520even%2520from%2520the%2520dataset%250Ain%2520a%2520non-critical%2520state%252C%2520and%2520that%2520the%2520power-law%2520distribution%2520can%2520be%2520modified%2520by%250Achanging%2520either%2520the%2520activation%2520function%2520or%2520the%2520loss%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset-learning%20duality%20and%20emergent%20criticality&entry.906535625=Ekaterina%20Kukleva%20and%20Vitaly%20Vanchurin&entry.1292438233=%20%20In%20artificial%20neural%20networks%2C%20the%20activation%20dynamics%20of%20non-trainable%0Avariables%20is%20strongly%20coupled%20to%20the%20learning%20dynamics%20of%20trainable%20variables.%0ADuring%20the%20activation%20pass%2C%20the%20boundary%20neurons%20%28e.g.%2C%20input%20neurons%29%20are%0Amapped%20to%20the%20bulk%20neurons%20%28e.g.%2C%20hidden%20neurons%29%2C%20and%20during%20the%20learning%0Apass%2C%20both%20bulk%20and%20boundary%20neurons%20are%20mapped%20to%20changes%20in%20trainable%0Avariables%20%28e.g.%2C%20weights%20and%20biases%29.%20For%20example%2C%20in%20feed-forward%20neural%0Anetworks%2C%20forward%20propagation%20is%20the%20activation%20pass%20and%20backward%20propagation%0Ais%20the%20learning%20pass.%20We%20show%20that%20a%20composition%20of%20the%20two%20maps%20establishes%20a%0Aduality%20map%20between%20a%20subspace%20of%20non-trainable%20boundary%20variables%20%28e.g.%2C%0Adataset%29%20and%20a%20tangent%20subspace%20of%20trainable%20variables%20%28i.e.%2C%20learning%29.%20In%0Ageneral%2C%20the%20dataset-learning%20duality%20is%20a%20complex%20non-linear%20map%20between%0Ahigh-dimensional%20spaces%2C%20but%20in%20a%20learning%20equilibrium%2C%20the%20problem%20can%20be%0Alinearized%20and%20reduced%20to%20many%20weakly%20coupled%20one-dimensional%20problems.%20We%20use%0Athe%20duality%20to%20study%20the%20emergence%20of%20criticality%2C%20or%20the%20power-law%0Adistributions%20of%20fluctuations%20of%20the%20trainable%20variables.%20In%20particular%2C%20we%0Ashow%20that%20criticality%20can%20emerge%20in%20the%20learning%20system%20even%20from%20the%20dataset%0Ain%20a%20non-critical%20state%2C%20and%20that%20the%20power-law%20distribution%20can%20be%20modified%20by%0Achanging%20either%20the%20activation%20function%20or%20the%20loss%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17391v2&entry.124074799=Read"},
{"title": "A new perspective on Bayesian Operational Modal Analysis", "author": "Brandon J. O'Connell and Max D. Champneys and Timothy J. Rogers", "abstract": "  In the field of operational modal analysis (OMA), obtained modal information\nis frequently used to assess the current state of aerospace, mechanical,\noffshore and civil structures. However, the stochasticity of operational\nsystems and the lack of forcing information can lead to inconsistent results.\nQuantifying the uncertainty of the recovered modal parameters through OMA is\ntherefore of significant value. In this article, a new perspective on Bayesian\nOMA is proposed: a Bayesian stochastic subspace identification (SSI) algorithm.\nDistinct from existing approaches to Bayesian OMA, a hierarchical probabilistic\nmodel is embedded at the core of covariance-driven SSI. Through substitution of\ncanonical correlation analysis with a Bayesian equivalent, posterior\ndistributions over the modal properties are obtained. Two inference schemes are\npresented for the proposed Bayesian formulation: Markov Chain Monte Carlo and\nvariational Bayes. Two case studies are then explored. The first is benchmark\nstudy using data from a simulated, multi degree-of-freedom, linear system.\nFollowing application of Bayesian SSI, it is shown that the same posterior is\ntargeted and recovered by both inference schemes, with good agreement between\nthe posterior mean and the conventional SSI result. The second study applies\nthe variational form to data obtained from an in-service structure: The Z24\nbridge. The results of this study are presented at single model orders, and\nthen using a stabilisation diagram. The recovered posterior uncertainty is\npresented and compared to the classic SSI result. It is observed that the\nposterior distributions with mean values coinciding with the natural\nfrequencies exhibit lower variance than values situated away from the natural\nfrequencies.\n", "link": "http://arxiv.org/abs/2408.08664v1", "date": "2024-08-16", "relevancy": 1.9171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4693}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20perspective%20on%20Bayesian%20Operational%20Modal%20Analysis&body=Title%3A%20A%20new%20perspective%20on%20Bayesian%20Operational%20Modal%20Analysis%0AAuthor%3A%20Brandon%20J.%20O%27Connell%20and%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers%0AAbstract%3A%20%20%20In%20the%20field%20of%20operational%20modal%20analysis%20%28OMA%29%2C%20obtained%20modal%20information%0Ais%20frequently%20used%20to%20assess%20the%20current%20state%20of%20aerospace%2C%20mechanical%2C%0Aoffshore%20and%20civil%20structures.%20However%2C%20the%20stochasticity%20of%20operational%0Asystems%20and%20the%20lack%20of%20forcing%20information%20can%20lead%20to%20inconsistent%20results.%0AQuantifying%20the%20uncertainty%20of%20the%20recovered%20modal%20parameters%20through%20OMA%20is%0Atherefore%20of%20significant%20value.%20In%20this%20article%2C%20a%20new%20perspective%20on%20Bayesian%0AOMA%20is%20proposed%3A%20a%20Bayesian%20stochastic%20subspace%20identification%20%28SSI%29%20algorithm.%0ADistinct%20from%20existing%20approaches%20to%20Bayesian%20OMA%2C%20a%20hierarchical%20probabilistic%0Amodel%20is%20embedded%20at%20the%20core%20of%20covariance-driven%20SSI.%20Through%20substitution%20of%0Acanonical%20correlation%20analysis%20with%20a%20Bayesian%20equivalent%2C%20posterior%0Adistributions%20over%20the%20modal%20properties%20are%20obtained.%20Two%20inference%20schemes%20are%0Apresented%20for%20the%20proposed%20Bayesian%20formulation%3A%20Markov%20Chain%20Monte%20Carlo%20and%0Avariational%20Bayes.%20Two%20case%20studies%20are%20then%20explored.%20The%20first%20is%20benchmark%0Astudy%20using%20data%20from%20a%20simulated%2C%20multi%20degree-of-freedom%2C%20linear%20system.%0AFollowing%20application%20of%20Bayesian%20SSI%2C%20it%20is%20shown%20that%20the%20same%20posterior%20is%0Atargeted%20and%20recovered%20by%20both%20inference%20schemes%2C%20with%20good%20agreement%20between%0Athe%20posterior%20mean%20and%20the%20conventional%20SSI%20result.%20The%20second%20study%20applies%0Athe%20variational%20form%20to%20data%20obtained%20from%20an%20in-service%20structure%3A%20The%20Z24%0Abridge.%20The%20results%20of%20this%20study%20are%20presented%20at%20single%20model%20orders%2C%20and%0Athen%20using%20a%20stabilisation%20diagram.%20The%20recovered%20posterior%20uncertainty%20is%0Apresented%20and%20compared%20to%20the%20classic%20SSI%20result.%20It%20is%20observed%20that%20the%0Aposterior%20distributions%20with%20mean%20values%20coinciding%20with%20the%20natural%0Afrequencies%20exhibit%20lower%20variance%20than%20values%20situated%20away%20from%20the%20natural%0Afrequencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520perspective%2520on%2520Bayesian%2520Operational%2520Modal%2520Analysis%26entry.906535625%3DBrandon%2520J.%2520O%2527Connell%2520and%2520Max%2520D.%2520Champneys%2520and%2520Timothy%2520J.%2520Rogers%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520operational%2520modal%2520analysis%2520%2528OMA%2529%252C%2520obtained%2520modal%2520information%250Ais%2520frequently%2520used%2520to%2520assess%2520the%2520current%2520state%2520of%2520aerospace%252C%2520mechanical%252C%250Aoffshore%2520and%2520civil%2520structures.%2520However%252C%2520the%2520stochasticity%2520of%2520operational%250Asystems%2520and%2520the%2520lack%2520of%2520forcing%2520information%2520can%2520lead%2520to%2520inconsistent%2520results.%250AQuantifying%2520the%2520uncertainty%2520of%2520the%2520recovered%2520modal%2520parameters%2520through%2520OMA%2520is%250Atherefore%2520of%2520significant%2520value.%2520In%2520this%2520article%252C%2520a%2520new%2520perspective%2520on%2520Bayesian%250AOMA%2520is%2520proposed%253A%2520a%2520Bayesian%2520stochastic%2520subspace%2520identification%2520%2528SSI%2529%2520algorithm.%250ADistinct%2520from%2520existing%2520approaches%2520to%2520Bayesian%2520OMA%252C%2520a%2520hierarchical%2520probabilistic%250Amodel%2520is%2520embedded%2520at%2520the%2520core%2520of%2520covariance-driven%2520SSI.%2520Through%2520substitution%2520of%250Acanonical%2520correlation%2520analysis%2520with%2520a%2520Bayesian%2520equivalent%252C%2520posterior%250Adistributions%2520over%2520the%2520modal%2520properties%2520are%2520obtained.%2520Two%2520inference%2520schemes%2520are%250Apresented%2520for%2520the%2520proposed%2520Bayesian%2520formulation%253A%2520Markov%2520Chain%2520Monte%2520Carlo%2520and%250Avariational%2520Bayes.%2520Two%2520case%2520studies%2520are%2520then%2520explored.%2520The%2520first%2520is%2520benchmark%250Astudy%2520using%2520data%2520from%2520a%2520simulated%252C%2520multi%2520degree-of-freedom%252C%2520linear%2520system.%250AFollowing%2520application%2520of%2520Bayesian%2520SSI%252C%2520it%2520is%2520shown%2520that%2520the%2520same%2520posterior%2520is%250Atargeted%2520and%2520recovered%2520by%2520both%2520inference%2520schemes%252C%2520with%2520good%2520agreement%2520between%250Athe%2520posterior%2520mean%2520and%2520the%2520conventional%2520SSI%2520result.%2520The%2520second%2520study%2520applies%250Athe%2520variational%2520form%2520to%2520data%2520obtained%2520from%2520an%2520in-service%2520structure%253A%2520The%2520Z24%250Abridge.%2520The%2520results%2520of%2520this%2520study%2520are%2520presented%2520at%2520single%2520model%2520orders%252C%2520and%250Athen%2520using%2520a%2520stabilisation%2520diagram.%2520The%2520recovered%2520posterior%2520uncertainty%2520is%250Apresented%2520and%2520compared%2520to%2520the%2520classic%2520SSI%2520result.%2520It%2520is%2520observed%2520that%2520the%250Aposterior%2520distributions%2520with%2520mean%2520values%2520coinciding%2520with%2520the%2520natural%250Afrequencies%2520exhibit%2520lower%2520variance%2520than%2520values%2520situated%2520away%2520from%2520the%2520natural%250Afrequencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20perspective%20on%20Bayesian%20Operational%20Modal%20Analysis&entry.906535625=Brandon%20J.%20O%27Connell%20and%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers&entry.1292438233=%20%20In%20the%20field%20of%20operational%20modal%20analysis%20%28OMA%29%2C%20obtained%20modal%20information%0Ais%20frequently%20used%20to%20assess%20the%20current%20state%20of%20aerospace%2C%20mechanical%2C%0Aoffshore%20and%20civil%20structures.%20However%2C%20the%20stochasticity%20of%20operational%0Asystems%20and%20the%20lack%20of%20forcing%20information%20can%20lead%20to%20inconsistent%20results.%0AQuantifying%20the%20uncertainty%20of%20the%20recovered%20modal%20parameters%20through%20OMA%20is%0Atherefore%20of%20significant%20value.%20In%20this%20article%2C%20a%20new%20perspective%20on%20Bayesian%0AOMA%20is%20proposed%3A%20a%20Bayesian%20stochastic%20subspace%20identification%20%28SSI%29%20algorithm.%0ADistinct%20from%20existing%20approaches%20to%20Bayesian%20OMA%2C%20a%20hierarchical%20probabilistic%0Amodel%20is%20embedded%20at%20the%20core%20of%20covariance-driven%20SSI.%20Through%20substitution%20of%0Acanonical%20correlation%20analysis%20with%20a%20Bayesian%20equivalent%2C%20posterior%0Adistributions%20over%20the%20modal%20properties%20are%20obtained.%20Two%20inference%20schemes%20are%0Apresented%20for%20the%20proposed%20Bayesian%20formulation%3A%20Markov%20Chain%20Monte%20Carlo%20and%0Avariational%20Bayes.%20Two%20case%20studies%20are%20then%20explored.%20The%20first%20is%20benchmark%0Astudy%20using%20data%20from%20a%20simulated%2C%20multi%20degree-of-freedom%2C%20linear%20system.%0AFollowing%20application%20of%20Bayesian%20SSI%2C%20it%20is%20shown%20that%20the%20same%20posterior%20is%0Atargeted%20and%20recovered%20by%20both%20inference%20schemes%2C%20with%20good%20agreement%20between%0Athe%20posterior%20mean%20and%20the%20conventional%20SSI%20result.%20The%20second%20study%20applies%0Athe%20variational%20form%20to%20data%20obtained%20from%20an%20in-service%20structure%3A%20The%20Z24%0Abridge.%20The%20results%20of%20this%20study%20are%20presented%20at%20single%20model%20orders%2C%20and%0Athen%20using%20a%20stabilisation%20diagram.%20The%20recovered%20posterior%20uncertainty%20is%0Apresented%20and%20compared%20to%20the%20classic%20SSI%20result.%20It%20is%20observed%20that%20the%0Aposterior%20distributions%20with%20mean%20values%20coinciding%20with%20the%20natural%0Afrequencies%20exhibit%20lower%20variance%20than%20values%20situated%20away%20from%20the%20natural%0Afrequencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08664v1&entry.124074799=Read"},
{"title": "MicroSSIM: Improved Structural Similarity for Comparing Microscopy Data", "author": "Ashesh Ashesh and Joran Deschamps and Florian Jug", "abstract": "  Microscopy is routinely used to image biological structures of interest. Due\nto imaging constraints, acquired images are typically low-SNR and contain\nnoise. Over the last few years, regression-based tasks like unsupervised\ndenoising and splitting have found utility in working with such noisy\nmicrographs. For evaluation, Structural Similarity (SSIM) is one of the most\npopular measures used in the field. For such tasks, the best evaluation would\nbe when both low-SNR noisy images and corresponding high-SNR clean images are\nobtained directly from a microscope. However, due to the following three\npeculiar properties of the microscopy data, we observe that SSIM is not well\nsuited to this data regime: (a) high-SNR micrographs have higher intensity\npixels as compared to low SNR micrographs, (b) high-SNR micrographs have higher\nintensity pixels than found in natural images, images for which SSIM was\ndeveloped, and (c) a digitally configurable offset is added by the detector\npresent inside the microscope. We show that SSIM components behave unexpectedly\nwhen the prediction generated from low-SNR input is compared with the\ncorresponding high-SNR data. We explain this behavior by introducing the\nphenomenon of saturation, where the value of SSIM components becomes less\nsensitive to (dis)similarity between the images. We introduce microSSIM, a\nvariant of SSIM, which overcomes the above-discussed issues. We justify the\nsoundness and utility of microSSIM using theoretical and empirical arguments\nand show the utility of microSSIM on two tasks: unsupervised denoising and\njoint image splitting with unsupervised denoising. Since our formulation can be\napplied to a broad family of SSIM-based measures, we also introduce MicroMS3IM,\na microscopy-specific variation of MS-SSIM. The source code and python package\nis available at https://github.com/juglab/MicroSSIM.\n", "link": "http://arxiv.org/abs/2408.08747v1", "date": "2024-08-16", "relevancy": 1.9133, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.475}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroSSIM%3A%20Improved%20Structural%20Similarity%20for%20Comparing%20Microscopy%20Data&body=Title%3A%20MicroSSIM%3A%20Improved%20Structural%20Similarity%20for%20Comparing%20Microscopy%20Data%0AAuthor%3A%20Ashesh%20Ashesh%20and%20Joran%20Deschamps%20and%20Florian%20Jug%0AAbstract%3A%20%20%20Microscopy%20is%20routinely%20used%20to%20image%20biological%20structures%20of%20interest.%20Due%0Ato%20imaging%20constraints%2C%20acquired%20images%20are%20typically%20low-SNR%20and%20contain%0Anoise.%20Over%20the%20last%20few%20years%2C%20regression-based%20tasks%20like%20unsupervised%0Adenoising%20and%20splitting%20have%20found%20utility%20in%20working%20with%20such%20noisy%0Amicrographs.%20For%20evaluation%2C%20Structural%20Similarity%20%28SSIM%29%20is%20one%20of%20the%20most%0Apopular%20measures%20used%20in%20the%20field.%20For%20such%20tasks%2C%20the%20best%20evaluation%20would%0Abe%20when%20both%20low-SNR%20noisy%20images%20and%20corresponding%20high-SNR%20clean%20images%20are%0Aobtained%20directly%20from%20a%20microscope.%20However%2C%20due%20to%20the%20following%20three%0Apeculiar%20properties%20of%20the%20microscopy%20data%2C%20we%20observe%20that%20SSIM%20is%20not%20well%0Asuited%20to%20this%20data%20regime%3A%20%28a%29%20high-SNR%20micrographs%20have%20higher%20intensity%0Apixels%20as%20compared%20to%20low%20SNR%20micrographs%2C%20%28b%29%20high-SNR%20micrographs%20have%20higher%0Aintensity%20pixels%20than%20found%20in%20natural%20images%2C%20images%20for%20which%20SSIM%20was%0Adeveloped%2C%20and%20%28c%29%20a%20digitally%20configurable%20offset%20is%20added%20by%20the%20detector%0Apresent%20inside%20the%20microscope.%20We%20show%20that%20SSIM%20components%20behave%20unexpectedly%0Awhen%20the%20prediction%20generated%20from%20low-SNR%20input%20is%20compared%20with%20the%0Acorresponding%20high-SNR%20data.%20We%20explain%20this%20behavior%20by%20introducing%20the%0Aphenomenon%20of%20saturation%2C%20where%20the%20value%20of%20SSIM%20components%20becomes%20less%0Asensitive%20to%20%28dis%29similarity%20between%20the%20images.%20We%20introduce%20microSSIM%2C%20a%0Avariant%20of%20SSIM%2C%20which%20overcomes%20the%20above-discussed%20issues.%20We%20justify%20the%0Asoundness%20and%20utility%20of%20microSSIM%20using%20theoretical%20and%20empirical%20arguments%0Aand%20show%20the%20utility%20of%20microSSIM%20on%20two%20tasks%3A%20unsupervised%20denoising%20and%0Ajoint%20image%20splitting%20with%20unsupervised%20denoising.%20Since%20our%20formulation%20can%20be%0Aapplied%20to%20a%20broad%20family%20of%20SSIM-based%20measures%2C%20we%20also%20introduce%20MicroMS3IM%2C%0Aa%20microscopy-specific%20variation%20of%20MS-SSIM.%20The%20source%20code%20and%20python%20package%0Ais%20available%20at%20https%3A//github.com/juglab/MicroSSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroSSIM%253A%2520Improved%2520Structural%2520Similarity%2520for%2520Comparing%2520Microscopy%2520Data%26entry.906535625%3DAshesh%2520Ashesh%2520and%2520Joran%2520Deschamps%2520and%2520Florian%2520Jug%26entry.1292438233%3D%2520%2520Microscopy%2520is%2520routinely%2520used%2520to%2520image%2520biological%2520structures%2520of%2520interest.%2520Due%250Ato%2520imaging%2520constraints%252C%2520acquired%2520images%2520are%2520typically%2520low-SNR%2520and%2520contain%250Anoise.%2520Over%2520the%2520last%2520few%2520years%252C%2520regression-based%2520tasks%2520like%2520unsupervised%250Adenoising%2520and%2520splitting%2520have%2520found%2520utility%2520in%2520working%2520with%2520such%2520noisy%250Amicrographs.%2520For%2520evaluation%252C%2520Structural%2520Similarity%2520%2528SSIM%2529%2520is%2520one%2520of%2520the%2520most%250Apopular%2520measures%2520used%2520in%2520the%2520field.%2520For%2520such%2520tasks%252C%2520the%2520best%2520evaluation%2520would%250Abe%2520when%2520both%2520low-SNR%2520noisy%2520images%2520and%2520corresponding%2520high-SNR%2520clean%2520images%2520are%250Aobtained%2520directly%2520from%2520a%2520microscope.%2520However%252C%2520due%2520to%2520the%2520following%2520three%250Apeculiar%2520properties%2520of%2520the%2520microscopy%2520data%252C%2520we%2520observe%2520that%2520SSIM%2520is%2520not%2520well%250Asuited%2520to%2520this%2520data%2520regime%253A%2520%2528a%2529%2520high-SNR%2520micrographs%2520have%2520higher%2520intensity%250Apixels%2520as%2520compared%2520to%2520low%2520SNR%2520micrographs%252C%2520%2528b%2529%2520high-SNR%2520micrographs%2520have%2520higher%250Aintensity%2520pixels%2520than%2520found%2520in%2520natural%2520images%252C%2520images%2520for%2520which%2520SSIM%2520was%250Adeveloped%252C%2520and%2520%2528c%2529%2520a%2520digitally%2520configurable%2520offset%2520is%2520added%2520by%2520the%2520detector%250Apresent%2520inside%2520the%2520microscope.%2520We%2520show%2520that%2520SSIM%2520components%2520behave%2520unexpectedly%250Awhen%2520the%2520prediction%2520generated%2520from%2520low-SNR%2520input%2520is%2520compared%2520with%2520the%250Acorresponding%2520high-SNR%2520data.%2520We%2520explain%2520this%2520behavior%2520by%2520introducing%2520the%250Aphenomenon%2520of%2520saturation%252C%2520where%2520the%2520value%2520of%2520SSIM%2520components%2520becomes%2520less%250Asensitive%2520to%2520%2528dis%2529similarity%2520between%2520the%2520images.%2520We%2520introduce%2520microSSIM%252C%2520a%250Avariant%2520of%2520SSIM%252C%2520which%2520overcomes%2520the%2520above-discussed%2520issues.%2520We%2520justify%2520the%250Asoundness%2520and%2520utility%2520of%2520microSSIM%2520using%2520theoretical%2520and%2520empirical%2520arguments%250Aand%2520show%2520the%2520utility%2520of%2520microSSIM%2520on%2520two%2520tasks%253A%2520unsupervised%2520denoising%2520and%250Ajoint%2520image%2520splitting%2520with%2520unsupervised%2520denoising.%2520Since%2520our%2520formulation%2520can%2520be%250Aapplied%2520to%2520a%2520broad%2520family%2520of%2520SSIM-based%2520measures%252C%2520we%2520also%2520introduce%2520MicroMS3IM%252C%250Aa%2520microscopy-specific%2520variation%2520of%2520MS-SSIM.%2520The%2520source%2520code%2520and%2520python%2520package%250Ais%2520available%2520at%2520https%253A//github.com/juglab/MicroSSIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroSSIM%3A%20Improved%20Structural%20Similarity%20for%20Comparing%20Microscopy%20Data&entry.906535625=Ashesh%20Ashesh%20and%20Joran%20Deschamps%20and%20Florian%20Jug&entry.1292438233=%20%20Microscopy%20is%20routinely%20used%20to%20image%20biological%20structures%20of%20interest.%20Due%0Ato%20imaging%20constraints%2C%20acquired%20images%20are%20typically%20low-SNR%20and%20contain%0Anoise.%20Over%20the%20last%20few%20years%2C%20regression-based%20tasks%20like%20unsupervised%0Adenoising%20and%20splitting%20have%20found%20utility%20in%20working%20with%20such%20noisy%0Amicrographs.%20For%20evaluation%2C%20Structural%20Similarity%20%28SSIM%29%20is%20one%20of%20the%20most%0Apopular%20measures%20used%20in%20the%20field.%20For%20such%20tasks%2C%20the%20best%20evaluation%20would%0Abe%20when%20both%20low-SNR%20noisy%20images%20and%20corresponding%20high-SNR%20clean%20images%20are%0Aobtained%20directly%20from%20a%20microscope.%20However%2C%20due%20to%20the%20following%20three%0Apeculiar%20properties%20of%20the%20microscopy%20data%2C%20we%20observe%20that%20SSIM%20is%20not%20well%0Asuited%20to%20this%20data%20regime%3A%20%28a%29%20high-SNR%20micrographs%20have%20higher%20intensity%0Apixels%20as%20compared%20to%20low%20SNR%20micrographs%2C%20%28b%29%20high-SNR%20micrographs%20have%20higher%0Aintensity%20pixels%20than%20found%20in%20natural%20images%2C%20images%20for%20which%20SSIM%20was%0Adeveloped%2C%20and%20%28c%29%20a%20digitally%20configurable%20offset%20is%20added%20by%20the%20detector%0Apresent%20inside%20the%20microscope.%20We%20show%20that%20SSIM%20components%20behave%20unexpectedly%0Awhen%20the%20prediction%20generated%20from%20low-SNR%20input%20is%20compared%20with%20the%0Acorresponding%20high-SNR%20data.%20We%20explain%20this%20behavior%20by%20introducing%20the%0Aphenomenon%20of%20saturation%2C%20where%20the%20value%20of%20SSIM%20components%20becomes%20less%0Asensitive%20to%20%28dis%29similarity%20between%20the%20images.%20We%20introduce%20microSSIM%2C%20a%0Avariant%20of%20SSIM%2C%20which%20overcomes%20the%20above-discussed%20issues.%20We%20justify%20the%0Asoundness%20and%20utility%20of%20microSSIM%20using%20theoretical%20and%20empirical%20arguments%0Aand%20show%20the%20utility%20of%20microSSIM%20on%20two%20tasks%3A%20unsupervised%20denoising%20and%0Ajoint%20image%20splitting%20with%20unsupervised%20denoising.%20Since%20our%20formulation%20can%20be%0Aapplied%20to%20a%20broad%20family%20of%20SSIM-based%20measures%2C%20we%20also%20introduce%20MicroMS3IM%2C%0Aa%20microscopy-specific%20variation%20of%20MS-SSIM.%20The%20source%20code%20and%20python%20package%0Ais%20available%20at%20https%3A//github.com/juglab/MicroSSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08747v1&entry.124074799=Read"},
{"title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge", "author": "Ravi Raju and Swayambhoo Jain and Bo Li and Jonathan Li and Urmish Thakkar", "abstract": "  Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.\n", "link": "http://arxiv.org/abs/2408.08808v1", "date": "2024-08-16", "relevancy": 1.9081, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4676}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20Domain-Specific%20Evaluation%20Sets%20for%20LLM-as-a-judge&body=Title%3A%20Constructing%20Domain-Specific%20Evaluation%20Sets%20for%20LLM-as-a-judge%0AAuthor%3A%20Ravi%20Raju%20and%20Swayambhoo%20Jain%20and%20Bo%20Li%20and%20Jonathan%20Li%20and%20Urmish%20Thakkar%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20landscape%20of%20machine%0Alearning%2C%20yet%20current%20benchmarks%20often%20fall%20short%20in%20capturing%20the%20diverse%0Abehavior%20of%20these%20models%20in%20real-world%20applications.%20A%20benchmark%27s%20usefulness%0Ais%20determined%20by%20its%20ability%20to%20clearly%20differentiate%20between%20models%20of%20varying%0Acapabilities%20%28separability%29%20and%20closely%20align%20with%20human%20preferences.%20Existing%0Aframeworks%20like%20Alpaca-Eval%202.0%20LC%0A%5Ccite%7Bdubois2024lengthcontrolledalpacaevalsimpleway%7D%20and%20Arena-Hard%20v0.1%0A%5Ccite%7Bli2024crowdsourced%7D%20are%20limited%20by%20their%20focus%20on%20general-purpose%20queries%0Aand%20lack%20of%20diversity%20across%20domains%20such%20as%20law%2C%20medicine%2C%20and%20multilingual%0Acontexts.%20In%20this%20paper%2C%20we%20address%20these%20limitations%20by%20introducing%20a%20novel%0Adata%20pipeline%20that%20curates%20diverse%2C%20domain-specific%20evaluation%20sets%20tailored%0Afor%20LLM-as-a-Judge%20frameworks.%20Our%20approach%20leverages%20a%20combination%20of%20manual%0Acuration%2C%20semi-supervised%20learning%20to%20generate%20clusters%2C%20and%20stratified%0Asampling%20to%20ensure%20balanced%20representation%20across%20a%20wide%20range%20of%20domains%20and%0Alanguages.%20The%20resulting%20evaluation%20set%2C%20which%20includes%201573%20samples%20across%2014%0Acategories%2C%20demonstrates%20high%20separability%20%2884%5C%25%29%20across%20ten%20top-ranked%20models%2C%0Aand%20agreement%20%2884%5C%25%29%20with%20Chatbot%20Arena%20and%20%280.915%29%20Spearman%20correlation.%20The%0Aagreement%20values%20are%209%5C%25%20better%20than%20Arena%20Hard%20and%2020%5C%25%20better%20than%20AlpacaEval%0A2.0%20LC%2C%20while%20the%20Spearman%20coefficient%20is%200.7%20more%20than%20the%20next%20best%0Abenchmark%2C%20showcasing%20a%20significant%20improvement%20in%20the%20usefulness%20of%20the%0Abenchmark.%20We%20further%20provide%20an%20open-source%20evaluation%20tool%20that%20enables%0Afine-grained%20analysis%20of%20model%20performance%20across%20user-defined%20categories%2C%0Aoffering%20valuable%20insights%20for%20practitioners.%20This%20work%20contributes%20to%20the%0Aongoing%20effort%20to%20enhance%20the%20transparency%2C%20diversity%2C%20and%20effectiveness%20of%20LLM%0Aevaluation%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520Domain-Specific%2520Evaluation%2520Sets%2520for%2520LLM-as-a-judge%26entry.906535625%3DRavi%2520Raju%2520and%2520Swayambhoo%2520Jain%2520and%2520Bo%2520Li%2520and%2520Jonathan%2520Li%2520and%2520Urmish%2520Thakkar%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520the%2520landscape%2520of%2520machine%250Alearning%252C%2520yet%2520current%2520benchmarks%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520diverse%250Abehavior%2520of%2520these%2520models%2520in%2520real-world%2520applications.%2520A%2520benchmark%2527s%2520usefulness%250Ais%2520determined%2520by%2520its%2520ability%2520to%2520clearly%2520differentiate%2520between%2520models%2520of%2520varying%250Acapabilities%2520%2528separability%2529%2520and%2520closely%2520align%2520with%2520human%2520preferences.%2520Existing%250Aframeworks%2520like%2520Alpaca-Eval%25202.0%2520LC%250A%255Ccite%257Bdubois2024lengthcontrolledalpacaevalsimpleway%257D%2520and%2520Arena-Hard%2520v0.1%250A%255Ccite%257Bli2024crowdsourced%257D%2520are%2520limited%2520by%2520their%2520focus%2520on%2520general-purpose%2520queries%250Aand%2520lack%2520of%2520diversity%2520across%2520domains%2520such%2520as%2520law%252C%2520medicine%252C%2520and%2520multilingual%250Acontexts.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520limitations%2520by%2520introducing%2520a%2520novel%250Adata%2520pipeline%2520that%2520curates%2520diverse%252C%2520domain-specific%2520evaluation%2520sets%2520tailored%250Afor%2520LLM-as-a-Judge%2520frameworks.%2520Our%2520approach%2520leverages%2520a%2520combination%2520of%2520manual%250Acuration%252C%2520semi-supervised%2520learning%2520to%2520generate%2520clusters%252C%2520and%2520stratified%250Asampling%2520to%2520ensure%2520balanced%2520representation%2520across%2520a%2520wide%2520range%2520of%2520domains%2520and%250Alanguages.%2520The%2520resulting%2520evaluation%2520set%252C%2520which%2520includes%25201573%2520samples%2520across%252014%250Acategories%252C%2520demonstrates%2520high%2520separability%2520%252884%255C%2525%2529%2520across%2520ten%2520top-ranked%2520models%252C%250Aand%2520agreement%2520%252884%255C%2525%2529%2520with%2520Chatbot%2520Arena%2520and%2520%25280.915%2529%2520Spearman%2520correlation.%2520The%250Aagreement%2520values%2520are%25209%255C%2525%2520better%2520than%2520Arena%2520Hard%2520and%252020%255C%2525%2520better%2520than%2520AlpacaEval%250A2.0%2520LC%252C%2520while%2520the%2520Spearman%2520coefficient%2520is%25200.7%2520more%2520than%2520the%2520next%2520best%250Abenchmark%252C%2520showcasing%2520a%2520significant%2520improvement%2520in%2520the%2520usefulness%2520of%2520the%250Abenchmark.%2520We%2520further%2520provide%2520an%2520open-source%2520evaluation%2520tool%2520that%2520enables%250Afine-grained%2520analysis%2520of%2520model%2520performance%2520across%2520user-defined%2520categories%252C%250Aoffering%2520valuable%2520insights%2520for%2520practitioners.%2520This%2520work%2520contributes%2520to%2520the%250Aongoing%2520effort%2520to%2520enhance%2520the%2520transparency%252C%2520diversity%252C%2520and%2520effectiveness%2520of%2520LLM%250Aevaluation%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20Domain-Specific%20Evaluation%20Sets%20for%20LLM-as-a-judge&entry.906535625=Ravi%20Raju%20and%20Swayambhoo%20Jain%20and%20Bo%20Li%20and%20Jonathan%20Li%20and%20Urmish%20Thakkar&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20landscape%20of%20machine%0Alearning%2C%20yet%20current%20benchmarks%20often%20fall%20short%20in%20capturing%20the%20diverse%0Abehavior%20of%20these%20models%20in%20real-world%20applications.%20A%20benchmark%27s%20usefulness%0Ais%20determined%20by%20its%20ability%20to%20clearly%20differentiate%20between%20models%20of%20varying%0Acapabilities%20%28separability%29%20and%20closely%20align%20with%20human%20preferences.%20Existing%0Aframeworks%20like%20Alpaca-Eval%202.0%20LC%0A%5Ccite%7Bdubois2024lengthcontrolledalpacaevalsimpleway%7D%20and%20Arena-Hard%20v0.1%0A%5Ccite%7Bli2024crowdsourced%7D%20are%20limited%20by%20their%20focus%20on%20general-purpose%20queries%0Aand%20lack%20of%20diversity%20across%20domains%20such%20as%20law%2C%20medicine%2C%20and%20multilingual%0Acontexts.%20In%20this%20paper%2C%20we%20address%20these%20limitations%20by%20introducing%20a%20novel%0Adata%20pipeline%20that%20curates%20diverse%2C%20domain-specific%20evaluation%20sets%20tailored%0Afor%20LLM-as-a-Judge%20frameworks.%20Our%20approach%20leverages%20a%20combination%20of%20manual%0Acuration%2C%20semi-supervised%20learning%20to%20generate%20clusters%2C%20and%20stratified%0Asampling%20to%20ensure%20balanced%20representation%20across%20a%20wide%20range%20of%20domains%20and%0Alanguages.%20The%20resulting%20evaluation%20set%2C%20which%20includes%201573%20samples%20across%2014%0Acategories%2C%20demonstrates%20high%20separability%20%2884%5C%25%29%20across%20ten%20top-ranked%20models%2C%0Aand%20agreement%20%2884%5C%25%29%20with%20Chatbot%20Arena%20and%20%280.915%29%20Spearman%20correlation.%20The%0Aagreement%20values%20are%209%5C%25%20better%20than%20Arena%20Hard%20and%2020%5C%25%20better%20than%20AlpacaEval%0A2.0%20LC%2C%20while%20the%20Spearman%20coefficient%20is%200.7%20more%20than%20the%20next%20best%0Abenchmark%2C%20showcasing%20a%20significant%20improvement%20in%20the%20usefulness%20of%20the%0Abenchmark.%20We%20further%20provide%20an%20open-source%20evaluation%20tool%20that%20enables%0Afine-grained%20analysis%20of%20model%20performance%20across%20user-defined%20categories%2C%0Aoffering%20valuable%20insights%20for%20practitioners.%20This%20work%20contributes%20to%20the%0Aongoing%20effort%20to%20enhance%20the%20transparency%2C%20diversity%2C%20and%20effectiveness%20of%20LLM%0Aevaluation%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08808v1&entry.124074799=Read"},
{"title": "A Transparency Paradox? Investigating the Impact of Explanation\n  Specificity and Autonomous Vehicle Perceptual Inaccuracies on Passengers", "author": "Daniel Omeiza and Raunak Bhattacharyya and Marina Jirotka and Nick Hawes and Lars Kunze", "abstract": "  Transparency in automated systems could be afforded through the provision of\nintelligible explanations. While transparency is desirable, might it lead to\ncatastrophic outcomes (such as anxiety), that could outweigh its benefits? It's\nquite unclear how the specificity of explanations (level of transparency)\ninfluences recipients, especially in autonomous driving (AD). In this work, we\nexamined the effects of transparency mediated through varying levels of\nexplanation specificity in AD. We first extended a data-driven explainer model\nby adding a rule-based option for explanation generation in AD, and then\nconducted a within-subject lab study with 39 participants in an immersive\ndriving simulator to study the effect of the resulting explanations.\nSpecifically, our investigation focused on: (1) how different types of\nexplanations (specific vs. abstract) affect passengers' perceived safety,\nanxiety, and willingness to take control of the vehicle when the vehicle\nperception system makes erroneous predictions; and (2) the relationship between\npassengers' behavioural cues and their feelings during the autonomous drives.\nOur findings showed that passengers felt safer with specific explanations when\nthe vehicle's perception system had minimal errors, while abstract explanations\nthat hid perception errors led to lower feelings of safety. Anxiety levels\nincreased when specific explanations revealed perception system errors (high\ntransparency). We found no significant link between passengers' visual patterns\nand their anxiety levels. Our study suggests that passengers prefer clear and\nspecific explanations (high transparency) when they originate from autonomous\nvehicles (AVs) with optimal perceptual accuracy.\n", "link": "http://arxiv.org/abs/2408.08785v1", "date": "2024-08-16", "relevancy": 1.9058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4696}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Transparency%20Paradox%3F%20Investigating%20the%20Impact%20of%20Explanation%0A%20%20Specificity%20and%20Autonomous%20Vehicle%20Perceptual%20Inaccuracies%20on%20Passengers&body=Title%3A%20A%20Transparency%20Paradox%3F%20Investigating%20the%20Impact%20of%20Explanation%0A%20%20Specificity%20and%20Autonomous%20Vehicle%20Perceptual%20Inaccuracies%20on%20Passengers%0AAuthor%3A%20Daniel%20Omeiza%20and%20Raunak%20Bhattacharyya%20and%20Marina%20Jirotka%20and%20Nick%20Hawes%20and%20Lars%20Kunze%0AAbstract%3A%20%20%20Transparency%20in%20automated%20systems%20could%20be%20afforded%20through%20the%20provision%20of%0Aintelligible%20explanations.%20While%20transparency%20is%20desirable%2C%20might%20it%20lead%20to%0Acatastrophic%20outcomes%20%28such%20as%20anxiety%29%2C%20that%20could%20outweigh%20its%20benefits%3F%20It%27s%0Aquite%20unclear%20how%20the%20specificity%20of%20explanations%20%28level%20of%20transparency%29%0Ainfluences%20recipients%2C%20especially%20in%20autonomous%20driving%20%28AD%29.%20In%20this%20work%2C%20we%0Aexamined%20the%20effects%20of%20transparency%20mediated%20through%20varying%20levels%20of%0Aexplanation%20specificity%20in%20AD.%20We%20first%20extended%20a%20data-driven%20explainer%20model%0Aby%20adding%20a%20rule-based%20option%20for%20explanation%20generation%20in%20AD%2C%20and%20then%0Aconducted%20a%20within-subject%20lab%20study%20with%2039%20participants%20in%20an%20immersive%0Adriving%20simulator%20to%20study%20the%20effect%20of%20the%20resulting%20explanations.%0ASpecifically%2C%20our%20investigation%20focused%20on%3A%20%281%29%20how%20different%20types%20of%0Aexplanations%20%28specific%20vs.%20abstract%29%20affect%20passengers%27%20perceived%20safety%2C%0Aanxiety%2C%20and%20willingness%20to%20take%20control%20of%20the%20vehicle%20when%20the%20vehicle%0Aperception%20system%20makes%20erroneous%20predictions%3B%20and%20%282%29%20the%20relationship%20between%0Apassengers%27%20behavioural%20cues%20and%20their%20feelings%20during%20the%20autonomous%20drives.%0AOur%20findings%20showed%20that%20passengers%20felt%20safer%20with%20specific%20explanations%20when%0Athe%20vehicle%27s%20perception%20system%20had%20minimal%20errors%2C%20while%20abstract%20explanations%0Athat%20hid%20perception%20errors%20led%20to%20lower%20feelings%20of%20safety.%20Anxiety%20levels%0Aincreased%20when%20specific%20explanations%20revealed%20perception%20system%20errors%20%28high%0Atransparency%29.%20We%20found%20no%20significant%20link%20between%20passengers%27%20visual%20patterns%0Aand%20their%20anxiety%20levels.%20Our%20study%20suggests%20that%20passengers%20prefer%20clear%20and%0Aspecific%20explanations%20%28high%20transparency%29%20when%20they%20originate%20from%20autonomous%0Avehicles%20%28AVs%29%20with%20optimal%20perceptual%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Transparency%2520Paradox%253F%2520Investigating%2520the%2520Impact%2520of%2520Explanation%250A%2520%2520Specificity%2520and%2520Autonomous%2520Vehicle%2520Perceptual%2520Inaccuracies%2520on%2520Passengers%26entry.906535625%3DDaniel%2520Omeiza%2520and%2520Raunak%2520Bhattacharyya%2520and%2520Marina%2520Jirotka%2520and%2520Nick%2520Hawes%2520and%2520Lars%2520Kunze%26entry.1292438233%3D%2520%2520Transparency%2520in%2520automated%2520systems%2520could%2520be%2520afforded%2520through%2520the%2520provision%2520of%250Aintelligible%2520explanations.%2520While%2520transparency%2520is%2520desirable%252C%2520might%2520it%2520lead%2520to%250Acatastrophic%2520outcomes%2520%2528such%2520as%2520anxiety%2529%252C%2520that%2520could%2520outweigh%2520its%2520benefits%253F%2520It%2527s%250Aquite%2520unclear%2520how%2520the%2520specificity%2520of%2520explanations%2520%2528level%2520of%2520transparency%2529%250Ainfluences%2520recipients%252C%2520especially%2520in%2520autonomous%2520driving%2520%2528AD%2529.%2520In%2520this%2520work%252C%2520we%250Aexamined%2520the%2520effects%2520of%2520transparency%2520mediated%2520through%2520varying%2520levels%2520of%250Aexplanation%2520specificity%2520in%2520AD.%2520We%2520first%2520extended%2520a%2520data-driven%2520explainer%2520model%250Aby%2520adding%2520a%2520rule-based%2520option%2520for%2520explanation%2520generation%2520in%2520AD%252C%2520and%2520then%250Aconducted%2520a%2520within-subject%2520lab%2520study%2520with%252039%2520participants%2520in%2520an%2520immersive%250Adriving%2520simulator%2520to%2520study%2520the%2520effect%2520of%2520the%2520resulting%2520explanations.%250ASpecifically%252C%2520our%2520investigation%2520focused%2520on%253A%2520%25281%2529%2520how%2520different%2520types%2520of%250Aexplanations%2520%2528specific%2520vs.%2520abstract%2529%2520affect%2520passengers%2527%2520perceived%2520safety%252C%250Aanxiety%252C%2520and%2520willingness%2520to%2520take%2520control%2520of%2520the%2520vehicle%2520when%2520the%2520vehicle%250Aperception%2520system%2520makes%2520erroneous%2520predictions%253B%2520and%2520%25282%2529%2520the%2520relationship%2520between%250Apassengers%2527%2520behavioural%2520cues%2520and%2520their%2520feelings%2520during%2520the%2520autonomous%2520drives.%250AOur%2520findings%2520showed%2520that%2520passengers%2520felt%2520safer%2520with%2520specific%2520explanations%2520when%250Athe%2520vehicle%2527s%2520perception%2520system%2520had%2520minimal%2520errors%252C%2520while%2520abstract%2520explanations%250Athat%2520hid%2520perception%2520errors%2520led%2520to%2520lower%2520feelings%2520of%2520safety.%2520Anxiety%2520levels%250Aincreased%2520when%2520specific%2520explanations%2520revealed%2520perception%2520system%2520errors%2520%2528high%250Atransparency%2529.%2520We%2520found%2520no%2520significant%2520link%2520between%2520passengers%2527%2520visual%2520patterns%250Aand%2520their%2520anxiety%2520levels.%2520Our%2520study%2520suggests%2520that%2520passengers%2520prefer%2520clear%2520and%250Aspecific%2520explanations%2520%2528high%2520transparency%2529%2520when%2520they%2520originate%2520from%2520autonomous%250Avehicles%2520%2528AVs%2529%2520with%2520optimal%2520perceptual%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Transparency%20Paradox%3F%20Investigating%20the%20Impact%20of%20Explanation%0A%20%20Specificity%20and%20Autonomous%20Vehicle%20Perceptual%20Inaccuracies%20on%20Passengers&entry.906535625=Daniel%20Omeiza%20and%20Raunak%20Bhattacharyya%20and%20Marina%20Jirotka%20and%20Nick%20Hawes%20and%20Lars%20Kunze&entry.1292438233=%20%20Transparency%20in%20automated%20systems%20could%20be%20afforded%20through%20the%20provision%20of%0Aintelligible%20explanations.%20While%20transparency%20is%20desirable%2C%20might%20it%20lead%20to%0Acatastrophic%20outcomes%20%28such%20as%20anxiety%29%2C%20that%20could%20outweigh%20its%20benefits%3F%20It%27s%0Aquite%20unclear%20how%20the%20specificity%20of%20explanations%20%28level%20of%20transparency%29%0Ainfluences%20recipients%2C%20especially%20in%20autonomous%20driving%20%28AD%29.%20In%20this%20work%2C%20we%0Aexamined%20the%20effects%20of%20transparency%20mediated%20through%20varying%20levels%20of%0Aexplanation%20specificity%20in%20AD.%20We%20first%20extended%20a%20data-driven%20explainer%20model%0Aby%20adding%20a%20rule-based%20option%20for%20explanation%20generation%20in%20AD%2C%20and%20then%0Aconducted%20a%20within-subject%20lab%20study%20with%2039%20participants%20in%20an%20immersive%0Adriving%20simulator%20to%20study%20the%20effect%20of%20the%20resulting%20explanations.%0ASpecifically%2C%20our%20investigation%20focused%20on%3A%20%281%29%20how%20different%20types%20of%0Aexplanations%20%28specific%20vs.%20abstract%29%20affect%20passengers%27%20perceived%20safety%2C%0Aanxiety%2C%20and%20willingness%20to%20take%20control%20of%20the%20vehicle%20when%20the%20vehicle%0Aperception%20system%20makes%20erroneous%20predictions%3B%20and%20%282%29%20the%20relationship%20between%0Apassengers%27%20behavioural%20cues%20and%20their%20feelings%20during%20the%20autonomous%20drives.%0AOur%20findings%20showed%20that%20passengers%20felt%20safer%20with%20specific%20explanations%20when%0Athe%20vehicle%27s%20perception%20system%20had%20minimal%20errors%2C%20while%20abstract%20explanations%0Athat%20hid%20perception%20errors%20led%20to%20lower%20feelings%20of%20safety.%20Anxiety%20levels%0Aincreased%20when%20specific%20explanations%20revealed%20perception%20system%20errors%20%28high%0Atransparency%29.%20We%20found%20no%20significant%20link%20between%20passengers%27%20visual%20patterns%0Aand%20their%20anxiety%20levels.%20Our%20study%20suggests%20that%20passengers%20prefer%20clear%20and%0Aspecific%20explanations%20%28high%20transparency%29%20when%20they%20originate%20from%20autonomous%0Avehicles%20%28AVs%29%20with%20optimal%20perceptual%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08785v1&entry.124074799=Read"},
{"title": "Maximizing V-information for Pre-training Superior Foundation Models", "author": "Wenxuan Yang and Weimin Tan and Hanyu Zhang and Bo Yan", "abstract": "  Pre-training foundation models on large-scale datasets demonstrates\nexceptional performance. However, recent research questions this traditional\nnotion, exploring whether an increase in pre-training data always leads to\nenhanced model performance. To address this issue, data-effective learning\napproaches have been introduced. However, current methods in this area lack a\nclear standard for sample selection. Our experiments reveal that by maximizing\nV-information, sample selection can be framed as an optimization problem,\nenabling effective improvement in model performance even with fewer samples.\nUnder this guidance, we develop an optimal data-effective learning method\n(OptiDEL) to maximize V-information. The OptiDEL method generates hard samples\nto achieve or even exceed the performance of models trained on the full dataset\nwhile using substantially less data. We compare the OptiDEL method with\nstate-of-the-art approaches finding that OptiDEL consistently outperforms\nexisting approaches across different datasets, with foundation models trained\non only 5% of the pre-training data surpassing the performance of those trained\non the full dataset.\n", "link": "http://arxiv.org/abs/2408.07107v2", "date": "2024-08-16", "relevancy": 1.9042, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4771}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maximizing%20V-information%20for%20Pre-training%20Superior%20Foundation%20Models&body=Title%3A%20Maximizing%20V-information%20for%20Pre-training%20Superior%20Foundation%20Models%0AAuthor%3A%20Wenxuan%20Yang%20and%20Weimin%20Tan%20and%20Hanyu%20Zhang%20and%20Bo%20Yan%0AAbstract%3A%20%20%20Pre-training%20foundation%20models%20on%20large-scale%20datasets%20demonstrates%0Aexceptional%20performance.%20However%2C%20recent%20research%20questions%20this%20traditional%0Anotion%2C%20exploring%20whether%20an%20increase%20in%20pre-training%20data%20always%20leads%20to%0Aenhanced%20model%20performance.%20To%20address%20this%20issue%2C%20data-effective%20learning%0Aapproaches%20have%20been%20introduced.%20However%2C%20current%20methods%20in%20this%20area%20lack%20a%0Aclear%20standard%20for%20sample%20selection.%20Our%20experiments%20reveal%20that%20by%20maximizing%0AV-information%2C%20sample%20selection%20can%20be%20framed%20as%20an%20optimization%20problem%2C%0Aenabling%20effective%20improvement%20in%20model%20performance%20even%20with%20fewer%20samples.%0AUnder%20this%20guidance%2C%20we%20develop%20an%20optimal%20data-effective%20learning%20method%0A%28OptiDEL%29%20to%20maximize%20V-information.%20The%20OptiDEL%20method%20generates%20hard%20samples%0Ato%20achieve%20or%20even%20exceed%20the%20performance%20of%20models%20trained%20on%20the%20full%20dataset%0Awhile%20using%20substantially%20less%20data.%20We%20compare%20the%20OptiDEL%20method%20with%0Astate-of-the-art%20approaches%20finding%20that%20OptiDEL%20consistently%20outperforms%0Aexisting%20approaches%20across%20different%20datasets%2C%20with%20foundation%20models%20trained%0Aon%20only%205%25%20of%20the%20pre-training%20data%20surpassing%20the%20performance%20of%20those%20trained%0Aon%20the%20full%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaximizing%2520V-information%2520for%2520Pre-training%2520Superior%2520Foundation%2520Models%26entry.906535625%3DWenxuan%2520Yang%2520and%2520Weimin%2520Tan%2520and%2520Hanyu%2520Zhang%2520and%2520Bo%2520Yan%26entry.1292438233%3D%2520%2520Pre-training%2520foundation%2520models%2520on%2520large-scale%2520datasets%2520demonstrates%250Aexceptional%2520performance.%2520However%252C%2520recent%2520research%2520questions%2520this%2520traditional%250Anotion%252C%2520exploring%2520whether%2520an%2520increase%2520in%2520pre-training%2520data%2520always%2520leads%2520to%250Aenhanced%2520model%2520performance.%2520To%2520address%2520this%2520issue%252C%2520data-effective%2520learning%250Aapproaches%2520have%2520been%2520introduced.%2520However%252C%2520current%2520methods%2520in%2520this%2520area%2520lack%2520a%250Aclear%2520standard%2520for%2520sample%2520selection.%2520Our%2520experiments%2520reveal%2520that%2520by%2520maximizing%250AV-information%252C%2520sample%2520selection%2520can%2520be%2520framed%2520as%2520an%2520optimization%2520problem%252C%250Aenabling%2520effective%2520improvement%2520in%2520model%2520performance%2520even%2520with%2520fewer%2520samples.%250AUnder%2520this%2520guidance%252C%2520we%2520develop%2520an%2520optimal%2520data-effective%2520learning%2520method%250A%2528OptiDEL%2529%2520to%2520maximize%2520V-information.%2520The%2520OptiDEL%2520method%2520generates%2520hard%2520samples%250Ato%2520achieve%2520or%2520even%2520exceed%2520the%2520performance%2520of%2520models%2520trained%2520on%2520the%2520full%2520dataset%250Awhile%2520using%2520substantially%2520less%2520data.%2520We%2520compare%2520the%2520OptiDEL%2520method%2520with%250Astate-of-the-art%2520approaches%2520finding%2520that%2520OptiDEL%2520consistently%2520outperforms%250Aexisting%2520approaches%2520across%2520different%2520datasets%252C%2520with%2520foundation%2520models%2520trained%250Aon%2520only%25205%2525%2520of%2520the%2520pre-training%2520data%2520surpassing%2520the%2520performance%2520of%2520those%2520trained%250Aon%2520the%2520full%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maximizing%20V-information%20for%20Pre-training%20Superior%20Foundation%20Models&entry.906535625=Wenxuan%20Yang%20and%20Weimin%20Tan%20and%20Hanyu%20Zhang%20and%20Bo%20Yan&entry.1292438233=%20%20Pre-training%20foundation%20models%20on%20large-scale%20datasets%20demonstrates%0Aexceptional%20performance.%20However%2C%20recent%20research%20questions%20this%20traditional%0Anotion%2C%20exploring%20whether%20an%20increase%20in%20pre-training%20data%20always%20leads%20to%0Aenhanced%20model%20performance.%20To%20address%20this%20issue%2C%20data-effective%20learning%0Aapproaches%20have%20been%20introduced.%20However%2C%20current%20methods%20in%20this%20area%20lack%20a%0Aclear%20standard%20for%20sample%20selection.%20Our%20experiments%20reveal%20that%20by%20maximizing%0AV-information%2C%20sample%20selection%20can%20be%20framed%20as%20an%20optimization%20problem%2C%0Aenabling%20effective%20improvement%20in%20model%20performance%20even%20with%20fewer%20samples.%0AUnder%20this%20guidance%2C%20we%20develop%20an%20optimal%20data-effective%20learning%20method%0A%28OptiDEL%29%20to%20maximize%20V-information.%20The%20OptiDEL%20method%20generates%20hard%20samples%0Ato%20achieve%20or%20even%20exceed%20the%20performance%20of%20models%20trained%20on%20the%20full%20dataset%0Awhile%20using%20substantially%20less%20data.%20We%20compare%20the%20OptiDEL%20method%20with%0Astate-of-the-art%20approaches%20finding%20that%20OptiDEL%20consistently%20outperforms%0Aexisting%20approaches%20across%20different%20datasets%2C%20with%20foundation%20models%20trained%0Aon%20only%205%25%20of%20the%20pre-training%20data%20surpassing%20the%20performance%20of%20those%20trained%0Aon%20the%20full%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07107v2&entry.124074799=Read"},
{"title": "SE-SGformer: A Self-Explainable Signed Graph Transformer for Link Sign\n  Prediction", "author": "Lu Li and Jiale Liu and Xingyu Ji and Maojun Wang and Zeyu Zhang", "abstract": "  Signed Graph Neural Networks (SGNNs) have been shown to be effective in\nanalyzing complex patterns in real-world situations where positive and negative\nlinks coexist. However, SGNN models suffer from poor explainability, which\nlimit their adoptions in critical scenarios that require understanding the\nrationale behind predictions. To the best of our knowledge, there is currently\nno research work on the explainability of the SGNN models. Our goal is to\naddress the explainability of decision-making for the downstream task of link\nsign prediction specific to signed graph neural networks. Since post-hoc\nexplanations are not derived directly from the models, they may be biased and\nmisrepresent the true explanations. Therefore, in this paper we introduce a\nSelf-Explainable Signed Graph transformer (SE-SGformer) framework, which can\nnot only outputs explainable information while ensuring high prediction\naccuracy. Specifically, We propose a new Transformer architecture for signed\ngraphs and theoretically demonstrate that using positional encoding based on\nsigned random walks has greater expressive power than current SGNN methods and\nother positional encoding graph Transformer-based approaches. We constructs a\nnovel explainable decision process by discovering the $K$-nearest (farthest)\npositive (negative) neighbors of a node to replace the neural network-based\ndecoder for predicting edge signs. These $K$ positive (negative) neighbors\nrepresent crucial information about the formation of positive (negative) edges\nbetween nodes and thus can serve as important explanatory information in the\ndecision-making process. We conducted experiments on several real-world\ndatasets to validate the effectiveness of SE-SGformer, which outperforms the\nstate-of-the-art methods by improving 2.2\\% prediction accuracy and 73.1\\%\nexplainablity accuracy in the best-case scenario.\n", "link": "http://arxiv.org/abs/2408.08754v1", "date": "2024-08-16", "relevancy": 1.9023, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5258}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4656}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SE-SGformer%3A%20A%20Self-Explainable%20Signed%20Graph%20Transformer%20for%20Link%20Sign%0A%20%20Prediction&body=Title%3A%20SE-SGformer%3A%20A%20Self-Explainable%20Signed%20Graph%20Transformer%20for%20Link%20Sign%0A%20%20Prediction%0AAuthor%3A%20Lu%20Li%20and%20Jiale%20Liu%20and%20Xingyu%20Ji%20and%20Maojun%20Wang%20and%20Zeyu%20Zhang%0AAbstract%3A%20%20%20Signed%20Graph%20Neural%20Networks%20%28SGNNs%29%20have%20been%20shown%20to%20be%20effective%20in%0Aanalyzing%20complex%20patterns%20in%20real-world%20situations%20where%20positive%20and%20negative%0Alinks%20coexist.%20However%2C%20SGNN%20models%20suffer%20from%20poor%20explainability%2C%20which%0Alimit%20their%20adoptions%20in%20critical%20scenarios%20that%20require%20understanding%20the%0Arationale%20behind%20predictions.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20currently%0Ano%20research%20work%20on%20the%20explainability%20of%20the%20SGNN%20models.%20Our%20goal%20is%20to%0Aaddress%20the%20explainability%20of%20decision-making%20for%20the%20downstream%20task%20of%20link%0Asign%20prediction%20specific%20to%20signed%20graph%20neural%20networks.%20Since%20post-hoc%0Aexplanations%20are%20not%20derived%20directly%20from%20the%20models%2C%20they%20may%20be%20biased%20and%0Amisrepresent%20the%20true%20explanations.%20Therefore%2C%20in%20this%20paper%20we%20introduce%20a%0ASelf-Explainable%20Signed%20Graph%20transformer%20%28SE-SGformer%29%20framework%2C%20which%20can%0Anot%20only%20outputs%20explainable%20information%20while%20ensuring%20high%20prediction%0Aaccuracy.%20Specifically%2C%20We%20propose%20a%20new%20Transformer%20architecture%20for%20signed%0Agraphs%20and%20theoretically%20demonstrate%20that%20using%20positional%20encoding%20based%20on%0Asigned%20random%20walks%20has%20greater%20expressive%20power%20than%20current%20SGNN%20methods%20and%0Aother%20positional%20encoding%20graph%20Transformer-based%20approaches.%20We%20constructs%20a%0Anovel%20explainable%20decision%20process%20by%20discovering%20the%20%24K%24-nearest%20%28farthest%29%0Apositive%20%28negative%29%20neighbors%20of%20a%20node%20to%20replace%20the%20neural%20network-based%0Adecoder%20for%20predicting%20edge%20signs.%20These%20%24K%24%20positive%20%28negative%29%20neighbors%0Arepresent%20crucial%20information%20about%20the%20formation%20of%20positive%20%28negative%29%20edges%0Abetween%20nodes%20and%20thus%20can%20serve%20as%20important%20explanatory%20information%20in%20the%0Adecision-making%20process.%20We%20conducted%20experiments%20on%20several%20real-world%0Adatasets%20to%20validate%20the%20effectiveness%20of%20SE-SGformer%2C%20which%20outperforms%20the%0Astate-of-the-art%20methods%20by%20improving%202.2%5C%25%20prediction%20accuracy%20and%2073.1%5C%25%0Aexplainablity%20accuracy%20in%20the%20best-case%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSE-SGformer%253A%2520A%2520Self-Explainable%2520Signed%2520Graph%2520Transformer%2520for%2520Link%2520Sign%250A%2520%2520Prediction%26entry.906535625%3DLu%2520Li%2520and%2520Jiale%2520Liu%2520and%2520Xingyu%2520Ji%2520and%2520Maojun%2520Wang%2520and%2520Zeyu%2520Zhang%26entry.1292438233%3D%2520%2520Signed%2520Graph%2520Neural%2520Networks%2520%2528SGNNs%2529%2520have%2520been%2520shown%2520to%2520be%2520effective%2520in%250Aanalyzing%2520complex%2520patterns%2520in%2520real-world%2520situations%2520where%2520positive%2520and%2520negative%250Alinks%2520coexist.%2520However%252C%2520SGNN%2520models%2520suffer%2520from%2520poor%2520explainability%252C%2520which%250Alimit%2520their%2520adoptions%2520in%2520critical%2520scenarios%2520that%2520require%2520understanding%2520the%250Arationale%2520behind%2520predictions.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520there%2520is%2520currently%250Ano%2520research%2520work%2520on%2520the%2520explainability%2520of%2520the%2520SGNN%2520models.%2520Our%2520goal%2520is%2520to%250Aaddress%2520the%2520explainability%2520of%2520decision-making%2520for%2520the%2520downstream%2520task%2520of%2520link%250Asign%2520prediction%2520specific%2520to%2520signed%2520graph%2520neural%2520networks.%2520Since%2520post-hoc%250Aexplanations%2520are%2520not%2520derived%2520directly%2520from%2520the%2520models%252C%2520they%2520may%2520be%2520biased%2520and%250Amisrepresent%2520the%2520true%2520explanations.%2520Therefore%252C%2520in%2520this%2520paper%2520we%2520introduce%2520a%250ASelf-Explainable%2520Signed%2520Graph%2520transformer%2520%2528SE-SGformer%2529%2520framework%252C%2520which%2520can%250Anot%2520only%2520outputs%2520explainable%2520information%2520while%2520ensuring%2520high%2520prediction%250Aaccuracy.%2520Specifically%252C%2520We%2520propose%2520a%2520new%2520Transformer%2520architecture%2520for%2520signed%250Agraphs%2520and%2520theoretically%2520demonstrate%2520that%2520using%2520positional%2520encoding%2520based%2520on%250Asigned%2520random%2520walks%2520has%2520greater%2520expressive%2520power%2520than%2520current%2520SGNN%2520methods%2520and%250Aother%2520positional%2520encoding%2520graph%2520Transformer-based%2520approaches.%2520We%2520constructs%2520a%250Anovel%2520explainable%2520decision%2520process%2520by%2520discovering%2520the%2520%2524K%2524-nearest%2520%2528farthest%2529%250Apositive%2520%2528negative%2529%2520neighbors%2520of%2520a%2520node%2520to%2520replace%2520the%2520neural%2520network-based%250Adecoder%2520for%2520predicting%2520edge%2520signs.%2520These%2520%2524K%2524%2520positive%2520%2528negative%2529%2520neighbors%250Arepresent%2520crucial%2520information%2520about%2520the%2520formation%2520of%2520positive%2520%2528negative%2529%2520edges%250Abetween%2520nodes%2520and%2520thus%2520can%2520serve%2520as%2520important%2520explanatory%2520information%2520in%2520the%250Adecision-making%2520process.%2520We%2520conducted%2520experiments%2520on%2520several%2520real-world%250Adatasets%2520to%2520validate%2520the%2520effectiveness%2520of%2520SE-SGformer%252C%2520which%2520outperforms%2520the%250Astate-of-the-art%2520methods%2520by%2520improving%25202.2%255C%2525%2520prediction%2520accuracy%2520and%252073.1%255C%2525%250Aexplainablity%2520accuracy%2520in%2520the%2520best-case%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE-SGformer%3A%20A%20Self-Explainable%20Signed%20Graph%20Transformer%20for%20Link%20Sign%0A%20%20Prediction&entry.906535625=Lu%20Li%20and%20Jiale%20Liu%20and%20Xingyu%20Ji%20and%20Maojun%20Wang%20and%20Zeyu%20Zhang&entry.1292438233=%20%20Signed%20Graph%20Neural%20Networks%20%28SGNNs%29%20have%20been%20shown%20to%20be%20effective%20in%0Aanalyzing%20complex%20patterns%20in%20real-world%20situations%20where%20positive%20and%20negative%0Alinks%20coexist.%20However%2C%20SGNN%20models%20suffer%20from%20poor%20explainability%2C%20which%0Alimit%20their%20adoptions%20in%20critical%20scenarios%20that%20require%20understanding%20the%0Arationale%20behind%20predictions.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20currently%0Ano%20research%20work%20on%20the%20explainability%20of%20the%20SGNN%20models.%20Our%20goal%20is%20to%0Aaddress%20the%20explainability%20of%20decision-making%20for%20the%20downstream%20task%20of%20link%0Asign%20prediction%20specific%20to%20signed%20graph%20neural%20networks.%20Since%20post-hoc%0Aexplanations%20are%20not%20derived%20directly%20from%20the%20models%2C%20they%20may%20be%20biased%20and%0Amisrepresent%20the%20true%20explanations.%20Therefore%2C%20in%20this%20paper%20we%20introduce%20a%0ASelf-Explainable%20Signed%20Graph%20transformer%20%28SE-SGformer%29%20framework%2C%20which%20can%0Anot%20only%20outputs%20explainable%20information%20while%20ensuring%20high%20prediction%0Aaccuracy.%20Specifically%2C%20We%20propose%20a%20new%20Transformer%20architecture%20for%20signed%0Agraphs%20and%20theoretically%20demonstrate%20that%20using%20positional%20encoding%20based%20on%0Asigned%20random%20walks%20has%20greater%20expressive%20power%20than%20current%20SGNN%20methods%20and%0Aother%20positional%20encoding%20graph%20Transformer-based%20approaches.%20We%20constructs%20a%0Anovel%20explainable%20decision%20process%20by%20discovering%20the%20%24K%24-nearest%20%28farthest%29%0Apositive%20%28negative%29%20neighbors%20of%20a%20node%20to%20replace%20the%20neural%20network-based%0Adecoder%20for%20predicting%20edge%20signs.%20These%20%24K%24%20positive%20%28negative%29%20neighbors%0Arepresent%20crucial%20information%20about%20the%20formation%20of%20positive%20%28negative%29%20edges%0Abetween%20nodes%20and%20thus%20can%20serve%20as%20important%20explanatory%20information%20in%20the%0Adecision-making%20process.%20We%20conducted%20experiments%20on%20several%20real-world%0Adatasets%20to%20validate%20the%20effectiveness%20of%20SE-SGformer%2C%20which%20outperforms%20the%0Astate-of-the-art%20methods%20by%20improving%202.2%5C%25%20prediction%20accuracy%20and%2073.1%5C%25%0Aexplainablity%20accuracy%20in%20the%20best-case%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08754v1&entry.124074799=Read"},
{"title": "Historical Printed Ornaments: Dataset and Tasks", "author": "Sayan Kumar Chaki and Zeynep Sonat Baltaci and Elliot Vincent and Remi Emonet and Fabienne Vial-Bonacci and Christelle Bahier-Porte and Mathieu Aubry and Thierry Fournel", "abstract": "  This paper aims to develop the study of historical printed ornaments with\nmodern unsupervised computer vision. We highlight three complex tasks that are\nof critical interest to book historians: clustering, element discovery, and\nunsupervised change localization. For each of these tasks, we introduce an\nevaluation benchmark, and we adapt and evaluate state-of-the-art models. Our\nRey's Ornaments dataset is designed to be a representative example of a set of\nornaments historians would be interested in. It focuses on an XVIIIth century\nbookseller, Marc-Michel Rey, providing a consistent set of ornaments with a\nwide diversity and representative challenges. Our results highlight the\nlimitations of state-of-the-art models when faced with real data and show\nsimple baselines such as k-means or congealing can outperform more\nsophisticated approaches on such data. Our dataset and code can be found at\nhttps://printed-ornaments.github.io/.\n", "link": "http://arxiv.org/abs/2408.08633v1", "date": "2024-08-16", "relevancy": 1.897, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4817}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4754}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Historical%20Printed%20Ornaments%3A%20Dataset%20and%20Tasks&body=Title%3A%20Historical%20Printed%20Ornaments%3A%20Dataset%20and%20Tasks%0AAuthor%3A%20Sayan%20Kumar%20Chaki%20and%20Zeynep%20Sonat%20Baltaci%20and%20Elliot%20Vincent%20and%20Remi%20Emonet%20and%20Fabienne%20Vial-Bonacci%20and%20Christelle%20Bahier-Porte%20and%20Mathieu%20Aubry%20and%20Thierry%20Fournel%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20develop%20the%20study%20of%20historical%20printed%20ornaments%20with%0Amodern%20unsupervised%20computer%20vision.%20We%20highlight%20three%20complex%20tasks%20that%20are%0Aof%20critical%20interest%20to%20book%20historians%3A%20clustering%2C%20element%20discovery%2C%20and%0Aunsupervised%20change%20localization.%20For%20each%20of%20these%20tasks%2C%20we%20introduce%20an%0Aevaluation%20benchmark%2C%20and%20we%20adapt%20and%20evaluate%20state-of-the-art%20models.%20Our%0ARey%27s%20Ornaments%20dataset%20is%20designed%20to%20be%20a%20representative%20example%20of%20a%20set%20of%0Aornaments%20historians%20would%20be%20interested%20in.%20It%20focuses%20on%20an%20XVIIIth%20century%0Abookseller%2C%20Marc-Michel%20Rey%2C%20providing%20a%20consistent%20set%20of%20ornaments%20with%20a%0Awide%20diversity%20and%20representative%20challenges.%20Our%20results%20highlight%20the%0Alimitations%20of%20state-of-the-art%20models%20when%20faced%20with%20real%20data%20and%20show%0Asimple%20baselines%20such%20as%20k-means%20or%20congealing%20can%20outperform%20more%0Asophisticated%20approaches%20on%20such%20data.%20Our%20dataset%20and%20code%20can%20be%20found%20at%0Ahttps%3A//printed-ornaments.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistorical%2520Printed%2520Ornaments%253A%2520Dataset%2520and%2520Tasks%26entry.906535625%3DSayan%2520Kumar%2520Chaki%2520and%2520Zeynep%2520Sonat%2520Baltaci%2520and%2520Elliot%2520Vincent%2520and%2520Remi%2520Emonet%2520and%2520Fabienne%2520Vial-Bonacci%2520and%2520Christelle%2520Bahier-Porte%2520and%2520Mathieu%2520Aubry%2520and%2520Thierry%2520Fournel%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520develop%2520the%2520study%2520of%2520historical%2520printed%2520ornaments%2520with%250Amodern%2520unsupervised%2520computer%2520vision.%2520We%2520highlight%2520three%2520complex%2520tasks%2520that%2520are%250Aof%2520critical%2520interest%2520to%2520book%2520historians%253A%2520clustering%252C%2520element%2520discovery%252C%2520and%250Aunsupervised%2520change%2520localization.%2520For%2520each%2520of%2520these%2520tasks%252C%2520we%2520introduce%2520an%250Aevaluation%2520benchmark%252C%2520and%2520we%2520adapt%2520and%2520evaluate%2520state-of-the-art%2520models.%2520Our%250ARey%2527s%2520Ornaments%2520dataset%2520is%2520designed%2520to%2520be%2520a%2520representative%2520example%2520of%2520a%2520set%2520of%250Aornaments%2520historians%2520would%2520be%2520interested%2520in.%2520It%2520focuses%2520on%2520an%2520XVIIIth%2520century%250Abookseller%252C%2520Marc-Michel%2520Rey%252C%2520providing%2520a%2520consistent%2520set%2520of%2520ornaments%2520with%2520a%250Awide%2520diversity%2520and%2520representative%2520challenges.%2520Our%2520results%2520highlight%2520the%250Alimitations%2520of%2520state-of-the-art%2520models%2520when%2520faced%2520with%2520real%2520data%2520and%2520show%250Asimple%2520baselines%2520such%2520as%2520k-means%2520or%2520congealing%2520can%2520outperform%2520more%250Asophisticated%2520approaches%2520on%2520such%2520data.%2520Our%2520dataset%2520and%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//printed-ornaments.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Historical%20Printed%20Ornaments%3A%20Dataset%20and%20Tasks&entry.906535625=Sayan%20Kumar%20Chaki%20and%20Zeynep%20Sonat%20Baltaci%20and%20Elliot%20Vincent%20and%20Remi%20Emonet%20and%20Fabienne%20Vial-Bonacci%20and%20Christelle%20Bahier-Porte%20and%20Mathieu%20Aubry%20and%20Thierry%20Fournel&entry.1292438233=%20%20This%20paper%20aims%20to%20develop%20the%20study%20of%20historical%20printed%20ornaments%20with%0Amodern%20unsupervised%20computer%20vision.%20We%20highlight%20three%20complex%20tasks%20that%20are%0Aof%20critical%20interest%20to%20book%20historians%3A%20clustering%2C%20element%20discovery%2C%20and%0Aunsupervised%20change%20localization.%20For%20each%20of%20these%20tasks%2C%20we%20introduce%20an%0Aevaluation%20benchmark%2C%20and%20we%20adapt%20and%20evaluate%20state-of-the-art%20models.%20Our%0ARey%27s%20Ornaments%20dataset%20is%20designed%20to%20be%20a%20representative%20example%20of%20a%20set%20of%0Aornaments%20historians%20would%20be%20interested%20in.%20It%20focuses%20on%20an%20XVIIIth%20century%0Abookseller%2C%20Marc-Michel%20Rey%2C%20providing%20a%20consistent%20set%20of%20ornaments%20with%20a%0Awide%20diversity%20and%20representative%20challenges.%20Our%20results%20highlight%20the%0Alimitations%20of%20state-of-the-art%20models%20when%20faced%20with%20real%20data%20and%20show%0Asimple%20baselines%20such%20as%20k-means%20or%20congealing%20can%20outperform%20more%0Asophisticated%20approaches%20on%20such%20data.%20Our%20dataset%20and%20code%20can%20be%20found%20at%0Ahttps%3A//printed-ornaments.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08633v1&entry.124074799=Read"},
{"title": "The Power of Bias: Optimizing Client Selection in Federated Learning\n  with Heterogeneous Differential Privacy", "author": "Jiating Ma and Yipeng Zhou and Qi Li and Quan Z. Sheng and Laizhong Cui and Jiangchuan Liu", "abstract": "  To preserve the data privacy, the federated learning (FL) paradigm emerges in\nwhich clients only expose model gradients rather than original data for\nconducting model training. To enhance the protection of model gradients in FL,\ndifferentially private federated learning (DPFL) is proposed which incorporates\ndifferentially private (DP) noises to obfuscate gradients before they are\nexposed. Yet, an essential but largely overlooked problem in DPFL is the\nheterogeneity of clients' privacy requirement, which can vary significantly\nbetween clients and extremely complicates the client selection problem in DPFL.\nIn other words, both the data quality and the influence of DP noises should be\ntaken into account when selecting clients. To address this problem, we conduct\nconvergence analysis of DPFL under heterogeneous privacy, a generic client\nselection strategy, popular DP mechanisms and convex loss. Based on convergence\nanalysis, we formulate the client selection problem to minimize the value of\nloss function in DPFL with heterogeneous privacy, which is a convex\noptimization problem and can be solved efficiently. Accordingly, we propose the\nDPFL-BCS (biased client selection) algorithm. The extensive experiment results\nwith real datasets under both convex and non-convex loss functions indicate\nthat DPFL-BCS can remarkably improve model utility compared with the SOTA\nbaselines.\n", "link": "http://arxiv.org/abs/2408.08642v1", "date": "2024-08-16", "relevancy": 1.8814, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4695}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Power%20of%20Bias%3A%20Optimizing%20Client%20Selection%20in%20Federated%20Learning%0A%20%20with%20Heterogeneous%20Differential%20Privacy&body=Title%3A%20The%20Power%20of%20Bias%3A%20Optimizing%20Client%20Selection%20in%20Federated%20Learning%0A%20%20with%20Heterogeneous%20Differential%20Privacy%0AAuthor%3A%20Jiating%20Ma%20and%20Yipeng%20Zhou%20and%20Qi%20Li%20and%20Quan%20Z.%20Sheng%20and%20Laizhong%20Cui%20and%20Jiangchuan%20Liu%0AAbstract%3A%20%20%20To%20preserve%20the%20data%20privacy%2C%20the%20federated%20learning%20%28FL%29%20paradigm%20emerges%20in%0Awhich%20clients%20only%20expose%20model%20gradients%20rather%20than%20original%20data%20for%0Aconducting%20model%20training.%20To%20enhance%20the%20protection%20of%20model%20gradients%20in%20FL%2C%0Adifferentially%20private%20federated%20learning%20%28DPFL%29%20is%20proposed%20which%20incorporates%0Adifferentially%20private%20%28DP%29%20noises%20to%20obfuscate%20gradients%20before%20they%20are%0Aexposed.%20Yet%2C%20an%20essential%20but%20largely%20overlooked%20problem%20in%20DPFL%20is%20the%0Aheterogeneity%20of%20clients%27%20privacy%20requirement%2C%20which%20can%20vary%20significantly%0Abetween%20clients%20and%20extremely%20complicates%20the%20client%20selection%20problem%20in%20DPFL.%0AIn%20other%20words%2C%20both%20the%20data%20quality%20and%20the%20influence%20of%20DP%20noises%20should%20be%0Ataken%20into%20account%20when%20selecting%20clients.%20To%20address%20this%20problem%2C%20we%20conduct%0Aconvergence%20analysis%20of%20DPFL%20under%20heterogeneous%20privacy%2C%20a%20generic%20client%0Aselection%20strategy%2C%20popular%20DP%20mechanisms%20and%20convex%20loss.%20Based%20on%20convergence%0Aanalysis%2C%20we%20formulate%20the%20client%20selection%20problem%20to%20minimize%20the%20value%20of%0Aloss%20function%20in%20DPFL%20with%20heterogeneous%20privacy%2C%20which%20is%20a%20convex%0Aoptimization%20problem%20and%20can%20be%20solved%20efficiently.%20Accordingly%2C%20we%20propose%20the%0ADPFL-BCS%20%28biased%20client%20selection%29%20algorithm.%20The%20extensive%20experiment%20results%0Awith%20real%20datasets%20under%20both%20convex%20and%20non-convex%20loss%20functions%20indicate%0Athat%20DPFL-BCS%20can%20remarkably%20improve%20model%20utility%20compared%20with%20the%20SOTA%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Power%2520of%2520Bias%253A%2520Optimizing%2520Client%2520Selection%2520in%2520Federated%2520Learning%250A%2520%2520with%2520Heterogeneous%2520Differential%2520Privacy%26entry.906535625%3DJiating%2520Ma%2520and%2520Yipeng%2520Zhou%2520and%2520Qi%2520Li%2520and%2520Quan%2520Z.%2520Sheng%2520and%2520Laizhong%2520Cui%2520and%2520Jiangchuan%2520Liu%26entry.1292438233%3D%2520%2520To%2520preserve%2520the%2520data%2520privacy%252C%2520the%2520federated%2520learning%2520%2528FL%2529%2520paradigm%2520emerges%2520in%250Awhich%2520clients%2520only%2520expose%2520model%2520gradients%2520rather%2520than%2520original%2520data%2520for%250Aconducting%2520model%2520training.%2520To%2520enhance%2520the%2520protection%2520of%2520model%2520gradients%2520in%2520FL%252C%250Adifferentially%2520private%2520federated%2520learning%2520%2528DPFL%2529%2520is%2520proposed%2520which%2520incorporates%250Adifferentially%2520private%2520%2528DP%2529%2520noises%2520to%2520obfuscate%2520gradients%2520before%2520they%2520are%250Aexposed.%2520Yet%252C%2520an%2520essential%2520but%2520largely%2520overlooked%2520problem%2520in%2520DPFL%2520is%2520the%250Aheterogeneity%2520of%2520clients%2527%2520privacy%2520requirement%252C%2520which%2520can%2520vary%2520significantly%250Abetween%2520clients%2520and%2520extremely%2520complicates%2520the%2520client%2520selection%2520problem%2520in%2520DPFL.%250AIn%2520other%2520words%252C%2520both%2520the%2520data%2520quality%2520and%2520the%2520influence%2520of%2520DP%2520noises%2520should%2520be%250Ataken%2520into%2520account%2520when%2520selecting%2520clients.%2520To%2520address%2520this%2520problem%252C%2520we%2520conduct%250Aconvergence%2520analysis%2520of%2520DPFL%2520under%2520heterogeneous%2520privacy%252C%2520a%2520generic%2520client%250Aselection%2520strategy%252C%2520popular%2520DP%2520mechanisms%2520and%2520convex%2520loss.%2520Based%2520on%2520convergence%250Aanalysis%252C%2520we%2520formulate%2520the%2520client%2520selection%2520problem%2520to%2520minimize%2520the%2520value%2520of%250Aloss%2520function%2520in%2520DPFL%2520with%2520heterogeneous%2520privacy%252C%2520which%2520is%2520a%2520convex%250Aoptimization%2520problem%2520and%2520can%2520be%2520solved%2520efficiently.%2520Accordingly%252C%2520we%2520propose%2520the%250ADPFL-BCS%2520%2528biased%2520client%2520selection%2529%2520algorithm.%2520The%2520extensive%2520experiment%2520results%250Awith%2520real%2520datasets%2520under%2520both%2520convex%2520and%2520non-convex%2520loss%2520functions%2520indicate%250Athat%2520DPFL-BCS%2520can%2520remarkably%2520improve%2520model%2520utility%2520compared%2520with%2520the%2520SOTA%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Power%20of%20Bias%3A%20Optimizing%20Client%20Selection%20in%20Federated%20Learning%0A%20%20with%20Heterogeneous%20Differential%20Privacy&entry.906535625=Jiating%20Ma%20and%20Yipeng%20Zhou%20and%20Qi%20Li%20and%20Quan%20Z.%20Sheng%20and%20Laizhong%20Cui%20and%20Jiangchuan%20Liu&entry.1292438233=%20%20To%20preserve%20the%20data%20privacy%2C%20the%20federated%20learning%20%28FL%29%20paradigm%20emerges%20in%0Awhich%20clients%20only%20expose%20model%20gradients%20rather%20than%20original%20data%20for%0Aconducting%20model%20training.%20To%20enhance%20the%20protection%20of%20model%20gradients%20in%20FL%2C%0Adifferentially%20private%20federated%20learning%20%28DPFL%29%20is%20proposed%20which%20incorporates%0Adifferentially%20private%20%28DP%29%20noises%20to%20obfuscate%20gradients%20before%20they%20are%0Aexposed.%20Yet%2C%20an%20essential%20but%20largely%20overlooked%20problem%20in%20DPFL%20is%20the%0Aheterogeneity%20of%20clients%27%20privacy%20requirement%2C%20which%20can%20vary%20significantly%0Abetween%20clients%20and%20extremely%20complicates%20the%20client%20selection%20problem%20in%20DPFL.%0AIn%20other%20words%2C%20both%20the%20data%20quality%20and%20the%20influence%20of%20DP%20noises%20should%20be%0Ataken%20into%20account%20when%20selecting%20clients.%20To%20address%20this%20problem%2C%20we%20conduct%0Aconvergence%20analysis%20of%20DPFL%20under%20heterogeneous%20privacy%2C%20a%20generic%20client%0Aselection%20strategy%2C%20popular%20DP%20mechanisms%20and%20convex%20loss.%20Based%20on%20convergence%0Aanalysis%2C%20we%20formulate%20the%20client%20selection%20problem%20to%20minimize%20the%20value%20of%0Aloss%20function%20in%20DPFL%20with%20heterogeneous%20privacy%2C%20which%20is%20a%20convex%0Aoptimization%20problem%20and%20can%20be%20solved%20efficiently.%20Accordingly%2C%20we%20propose%20the%0ADPFL-BCS%20%28biased%20client%20selection%29%20algorithm.%20The%20extensive%20experiment%20results%0Awith%20real%20datasets%20under%20both%20convex%20and%20non-convex%20loss%20functions%20indicate%0Athat%20DPFL-BCS%20can%20remarkably%20improve%20model%20utility%20compared%20with%20the%20SOTA%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08642v1&entry.124074799=Read"},
{"title": "Accelerating Giant Impact Simulations with Machine Learning", "author": "Caleb Lammers and Miles Cranmer and Sam Hadden and Shirley Ho and Norman Murray and Daniel Tamayo", "abstract": "  Constraining planet formation models based on the observed exoplanet\npopulation requires generating large samples of synthetic planetary systems,\nwhich can be computationally prohibitive. A significant bottleneck is\nsimulating the giant impact phase, during which planetary embryos evolve\ngravitationally and combine to form planets, which may themselves experience\nlater collisions. To accelerate giant impact simulations, we present a machine\nlearning (ML) approach to predicting collisional outcomes in multiplanet\nsystems. Trained on more than 500,000 $N$-body simulations of three-planet\nsystems, we develop an ML model that can accurately predict which two planets\nwill experience a collision, along with the state of the post-collision\nplanets, from a short integration of the system's initial conditions. Our model\ngreatly improves on non-ML baselines that rely on metrics from dynamics theory,\nwhich struggle to accurately predict which pair of planets will experience a\ncollision. By combining with a model for predicting long-term stability, we\ncreate an efficient ML-based giant impact emulator, which can predict the\noutcomes of giant impact simulations with a speedup of up to four orders of\nmagnitude. We expect our model to enable analyses that would not otherwise be\ncomputationally feasible. As such, we release our full training code, along\nwith an easy-to-use API for our collision outcome model and giant impact\nemulator.\n", "link": "http://arxiv.org/abs/2408.08873v1", "date": "2024-08-16", "relevancy": 1.8797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4652}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Giant%20Impact%20Simulations%20with%20Machine%20Learning&body=Title%3A%20Accelerating%20Giant%20Impact%20Simulations%20with%20Machine%20Learning%0AAuthor%3A%20Caleb%20Lammers%20and%20Miles%20Cranmer%20and%20Sam%20Hadden%20and%20Shirley%20Ho%20and%20Norman%20Murray%20and%20Daniel%20Tamayo%0AAbstract%3A%20%20%20Constraining%20planet%20formation%20models%20based%20on%20the%20observed%20exoplanet%0Apopulation%20requires%20generating%20large%20samples%20of%20synthetic%20planetary%20systems%2C%0Awhich%20can%20be%20computationally%20prohibitive.%20A%20significant%20bottleneck%20is%0Asimulating%20the%20giant%20impact%20phase%2C%20during%20which%20planetary%20embryos%20evolve%0Agravitationally%20and%20combine%20to%20form%20planets%2C%20which%20may%20themselves%20experience%0Alater%20collisions.%20To%20accelerate%20giant%20impact%20simulations%2C%20we%20present%20a%20machine%0Alearning%20%28ML%29%20approach%20to%20predicting%20collisional%20outcomes%20in%20multiplanet%0Asystems.%20Trained%20on%20more%20than%20500%2C000%20%24N%24-body%20simulations%20of%20three-planet%0Asystems%2C%20we%20develop%20an%20ML%20model%20that%20can%20accurately%20predict%20which%20two%20planets%0Awill%20experience%20a%20collision%2C%20along%20with%20the%20state%20of%20the%20post-collision%0Aplanets%2C%20from%20a%20short%20integration%20of%20the%20system%27s%20initial%20conditions.%20Our%20model%0Agreatly%20improves%20on%20non-ML%20baselines%20that%20rely%20on%20metrics%20from%20dynamics%20theory%2C%0Awhich%20struggle%20to%20accurately%20predict%20which%20pair%20of%20planets%20will%20experience%20a%0Acollision.%20By%20combining%20with%20a%20model%20for%20predicting%20long-term%20stability%2C%20we%0Acreate%20an%20efficient%20ML-based%20giant%20impact%20emulator%2C%20which%20can%20predict%20the%0Aoutcomes%20of%20giant%20impact%20simulations%20with%20a%20speedup%20of%20up%20to%20four%20orders%20of%0Amagnitude.%20We%20expect%20our%20model%20to%20enable%20analyses%20that%20would%20not%20otherwise%20be%0Acomputationally%20feasible.%20As%20such%2C%20we%20release%20our%20full%20training%20code%2C%20along%0Awith%20an%20easy-to-use%20API%20for%20our%20collision%20outcome%20model%20and%20giant%20impact%0Aemulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Giant%2520Impact%2520Simulations%2520with%2520Machine%2520Learning%26entry.906535625%3DCaleb%2520Lammers%2520and%2520Miles%2520Cranmer%2520and%2520Sam%2520Hadden%2520and%2520Shirley%2520Ho%2520and%2520Norman%2520Murray%2520and%2520Daniel%2520Tamayo%26entry.1292438233%3D%2520%2520Constraining%2520planet%2520formation%2520models%2520based%2520on%2520the%2520observed%2520exoplanet%250Apopulation%2520requires%2520generating%2520large%2520samples%2520of%2520synthetic%2520planetary%2520systems%252C%250Awhich%2520can%2520be%2520computationally%2520prohibitive.%2520A%2520significant%2520bottleneck%2520is%250Asimulating%2520the%2520giant%2520impact%2520phase%252C%2520during%2520which%2520planetary%2520embryos%2520evolve%250Agravitationally%2520and%2520combine%2520to%2520form%2520planets%252C%2520which%2520may%2520themselves%2520experience%250Alater%2520collisions.%2520To%2520accelerate%2520giant%2520impact%2520simulations%252C%2520we%2520present%2520a%2520machine%250Alearning%2520%2528ML%2529%2520approach%2520to%2520predicting%2520collisional%2520outcomes%2520in%2520multiplanet%250Asystems.%2520Trained%2520on%2520more%2520than%2520500%252C000%2520%2524N%2524-body%2520simulations%2520of%2520three-planet%250Asystems%252C%2520we%2520develop%2520an%2520ML%2520model%2520that%2520can%2520accurately%2520predict%2520which%2520two%2520planets%250Awill%2520experience%2520a%2520collision%252C%2520along%2520with%2520the%2520state%2520of%2520the%2520post-collision%250Aplanets%252C%2520from%2520a%2520short%2520integration%2520of%2520the%2520system%2527s%2520initial%2520conditions.%2520Our%2520model%250Agreatly%2520improves%2520on%2520non-ML%2520baselines%2520that%2520rely%2520on%2520metrics%2520from%2520dynamics%2520theory%252C%250Awhich%2520struggle%2520to%2520accurately%2520predict%2520which%2520pair%2520of%2520planets%2520will%2520experience%2520a%250Acollision.%2520By%2520combining%2520with%2520a%2520model%2520for%2520predicting%2520long-term%2520stability%252C%2520we%250Acreate%2520an%2520efficient%2520ML-based%2520giant%2520impact%2520emulator%252C%2520which%2520can%2520predict%2520the%250Aoutcomes%2520of%2520giant%2520impact%2520simulations%2520with%2520a%2520speedup%2520of%2520up%2520to%2520four%2520orders%2520of%250Amagnitude.%2520We%2520expect%2520our%2520model%2520to%2520enable%2520analyses%2520that%2520would%2520not%2520otherwise%2520be%250Acomputationally%2520feasible.%2520As%2520such%252C%2520we%2520release%2520our%2520full%2520training%2520code%252C%2520along%250Awith%2520an%2520easy-to-use%2520API%2520for%2520our%2520collision%2520outcome%2520model%2520and%2520giant%2520impact%250Aemulator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Giant%20Impact%20Simulations%20with%20Machine%20Learning&entry.906535625=Caleb%20Lammers%20and%20Miles%20Cranmer%20and%20Sam%20Hadden%20and%20Shirley%20Ho%20and%20Norman%20Murray%20and%20Daniel%20Tamayo&entry.1292438233=%20%20Constraining%20planet%20formation%20models%20based%20on%20the%20observed%20exoplanet%0Apopulation%20requires%20generating%20large%20samples%20of%20synthetic%20planetary%20systems%2C%0Awhich%20can%20be%20computationally%20prohibitive.%20A%20significant%20bottleneck%20is%0Asimulating%20the%20giant%20impact%20phase%2C%20during%20which%20planetary%20embryos%20evolve%0Agravitationally%20and%20combine%20to%20form%20planets%2C%20which%20may%20themselves%20experience%0Alater%20collisions.%20To%20accelerate%20giant%20impact%20simulations%2C%20we%20present%20a%20machine%0Alearning%20%28ML%29%20approach%20to%20predicting%20collisional%20outcomes%20in%20multiplanet%0Asystems.%20Trained%20on%20more%20than%20500%2C000%20%24N%24-body%20simulations%20of%20three-planet%0Asystems%2C%20we%20develop%20an%20ML%20model%20that%20can%20accurately%20predict%20which%20two%20planets%0Awill%20experience%20a%20collision%2C%20along%20with%20the%20state%20of%20the%20post-collision%0Aplanets%2C%20from%20a%20short%20integration%20of%20the%20system%27s%20initial%20conditions.%20Our%20model%0Agreatly%20improves%20on%20non-ML%20baselines%20that%20rely%20on%20metrics%20from%20dynamics%20theory%2C%0Awhich%20struggle%20to%20accurately%20predict%20which%20pair%20of%20planets%20will%20experience%20a%0Acollision.%20By%20combining%20with%20a%20model%20for%20predicting%20long-term%20stability%2C%20we%0Acreate%20an%20efficient%20ML-based%20giant%20impact%20emulator%2C%20which%20can%20predict%20the%0Aoutcomes%20of%20giant%20impact%20simulations%20with%20a%20speedup%20of%20up%20to%20four%20orders%20of%0Amagnitude.%20We%20expect%20our%20model%20to%20enable%20analyses%20that%20would%20not%20otherwise%20be%0Acomputationally%20feasible.%20As%20such%2C%20we%20release%20our%20full%20training%20code%2C%20along%0Awith%20an%20easy-to-use%20API%20for%20our%20collision%20outcome%20model%20and%20giant%20impact%0Aemulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08873v1&entry.124074799=Read"},
{"title": "Potion: Towards Poison Unlearning", "author": "Stefan Schoepf and Jack Foster and Alexandra Brintrup", "abstract": "  Adversarial attacks by malicious actors on machine learning systems, such as\nintroducing poison triggers into training datasets, pose significant risks. The\nchallenge in resolving such an attack arises in practice when only a subset of\nthe poisoned data can be identified. This necessitates the development of\nmethods to remove, i.e. unlearn, poison triggers from already trained models\nwith only a subset of the poison data available. The requirements for this task\nsignificantly deviate from privacy-focused unlearning where all of the data to\nbe forgotten by the model is known. Previous work has shown that the\nundiscovered poisoned samples lead to a failure of established unlearning\nmethods, with only one method, Selective Synaptic Dampening (SSD), showing\nlimited success. Even full retraining, after the removal of the identified\npoison, cannot address this challenge as the undiscovered poison samples lead\nto a reintroduction of the poison trigger in the model. Our work addresses two\nkey challenges to advance the state of the art in poison unlearning. First, we\nintroduce a novel outlier-resistant method, based on SSD, that significantly\nimproves model protection and unlearning performance. Second, we introduce\nPoison Trigger Neutralisation (PTN) search, a fast, parallelisable,\nhyperparameter search that utilises the characteristic \"unlearning versus model\nprotection\" trade-off to find suitable hyperparameters in settings where the\nforget set size is unknown and the retain set is contaminated. We benchmark our\ncontributions using ResNet-9 on CIFAR10 and WideResNet-28x10 on CIFAR100.\nExperimental results show that our method heals 93.72% of poison compared to\nSSD with 83.41% and full retraining with 40.68%. We achieve this while also\nlowering the average model accuracy drop caused by unlearning from 5.68% (SSD)\nto 1.41% (ours).\n", "link": "http://arxiv.org/abs/2406.09173v2", "date": "2024-08-16", "relevancy": 1.8788, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4741}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4696}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Potion%3A%20Towards%20Poison%20Unlearning&body=Title%3A%20Potion%3A%20Towards%20Poison%20Unlearning%0AAuthor%3A%20Stefan%20Schoepf%20and%20Jack%20Foster%20and%20Alexandra%20Brintrup%0AAbstract%3A%20%20%20Adversarial%20attacks%20by%20malicious%20actors%20on%20machine%20learning%20systems%2C%20such%20as%0Aintroducing%20poison%20triggers%20into%20training%20datasets%2C%20pose%20significant%20risks.%20The%0Achallenge%20in%20resolving%20such%20an%20attack%20arises%20in%20practice%20when%20only%20a%20subset%20of%0Athe%20poisoned%20data%20can%20be%20identified.%20This%20necessitates%20the%20development%20of%0Amethods%20to%20remove%2C%20i.e.%20unlearn%2C%20poison%20triggers%20from%20already%20trained%20models%0Awith%20only%20a%20subset%20of%20the%20poison%20data%20available.%20The%20requirements%20for%20this%20task%0Asignificantly%20deviate%20from%20privacy-focused%20unlearning%20where%20all%20of%20the%20data%20to%0Abe%20forgotten%20by%20the%20model%20is%20known.%20Previous%20work%20has%20shown%20that%20the%0Aundiscovered%20poisoned%20samples%20lead%20to%20a%20failure%20of%20established%20unlearning%0Amethods%2C%20with%20only%20one%20method%2C%20Selective%20Synaptic%20Dampening%20%28SSD%29%2C%20showing%0Alimited%20success.%20Even%20full%20retraining%2C%20after%20the%20removal%20of%20the%20identified%0Apoison%2C%20cannot%20address%20this%20challenge%20as%20the%20undiscovered%20poison%20samples%20lead%0Ato%20a%20reintroduction%20of%20the%20poison%20trigger%20in%20the%20model.%20Our%20work%20addresses%20two%0Akey%20challenges%20to%20advance%20the%20state%20of%20the%20art%20in%20poison%20unlearning.%20First%2C%20we%0Aintroduce%20a%20novel%20outlier-resistant%20method%2C%20based%20on%20SSD%2C%20that%20significantly%0Aimproves%20model%20protection%20and%20unlearning%20performance.%20Second%2C%20we%20introduce%0APoison%20Trigger%20Neutralisation%20%28PTN%29%20search%2C%20a%20fast%2C%20parallelisable%2C%0Ahyperparameter%20search%20that%20utilises%20the%20characteristic%20%22unlearning%20versus%20model%0Aprotection%22%20trade-off%20to%20find%20suitable%20hyperparameters%20in%20settings%20where%20the%0Aforget%20set%20size%20is%20unknown%20and%20the%20retain%20set%20is%20contaminated.%20We%20benchmark%20our%0Acontributions%20using%20ResNet-9%20on%20CIFAR10%20and%20WideResNet-28x10%20on%20CIFAR100.%0AExperimental%20results%20show%20that%20our%20method%20heals%2093.72%25%20of%20poison%20compared%20to%0ASSD%20with%2083.41%25%20and%20full%20retraining%20with%2040.68%25.%20We%20achieve%20this%20while%20also%0Alowering%20the%20average%20model%20accuracy%20drop%20caused%20by%20unlearning%20from%205.68%25%20%28SSD%29%0Ato%201.41%25%20%28ours%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPotion%253A%2520Towards%2520Poison%2520Unlearning%26entry.906535625%3DStefan%2520Schoepf%2520and%2520Jack%2520Foster%2520and%2520Alexandra%2520Brintrup%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520by%2520malicious%2520actors%2520on%2520machine%2520learning%2520systems%252C%2520such%2520as%250Aintroducing%2520poison%2520triggers%2520into%2520training%2520datasets%252C%2520pose%2520significant%2520risks.%2520The%250Achallenge%2520in%2520resolving%2520such%2520an%2520attack%2520arises%2520in%2520practice%2520when%2520only%2520a%2520subset%2520of%250Athe%2520poisoned%2520data%2520can%2520be%2520identified.%2520This%2520necessitates%2520the%2520development%2520of%250Amethods%2520to%2520remove%252C%2520i.e.%2520unlearn%252C%2520poison%2520triggers%2520from%2520already%2520trained%2520models%250Awith%2520only%2520a%2520subset%2520of%2520the%2520poison%2520data%2520available.%2520The%2520requirements%2520for%2520this%2520task%250Asignificantly%2520deviate%2520from%2520privacy-focused%2520unlearning%2520where%2520all%2520of%2520the%2520data%2520to%250Abe%2520forgotten%2520by%2520the%2520model%2520is%2520known.%2520Previous%2520work%2520has%2520shown%2520that%2520the%250Aundiscovered%2520poisoned%2520samples%2520lead%2520to%2520a%2520failure%2520of%2520established%2520unlearning%250Amethods%252C%2520with%2520only%2520one%2520method%252C%2520Selective%2520Synaptic%2520Dampening%2520%2528SSD%2529%252C%2520showing%250Alimited%2520success.%2520Even%2520full%2520retraining%252C%2520after%2520the%2520removal%2520of%2520the%2520identified%250Apoison%252C%2520cannot%2520address%2520this%2520challenge%2520as%2520the%2520undiscovered%2520poison%2520samples%2520lead%250Ato%2520a%2520reintroduction%2520of%2520the%2520poison%2520trigger%2520in%2520the%2520model.%2520Our%2520work%2520addresses%2520two%250Akey%2520challenges%2520to%2520advance%2520the%2520state%2520of%2520the%2520art%2520in%2520poison%2520unlearning.%2520First%252C%2520we%250Aintroduce%2520a%2520novel%2520outlier-resistant%2520method%252C%2520based%2520on%2520SSD%252C%2520that%2520significantly%250Aimproves%2520model%2520protection%2520and%2520unlearning%2520performance.%2520Second%252C%2520we%2520introduce%250APoison%2520Trigger%2520Neutralisation%2520%2528PTN%2529%2520search%252C%2520a%2520fast%252C%2520parallelisable%252C%250Ahyperparameter%2520search%2520that%2520utilises%2520the%2520characteristic%2520%2522unlearning%2520versus%2520model%250Aprotection%2522%2520trade-off%2520to%2520find%2520suitable%2520hyperparameters%2520in%2520settings%2520where%2520the%250Aforget%2520set%2520size%2520is%2520unknown%2520and%2520the%2520retain%2520set%2520is%2520contaminated.%2520We%2520benchmark%2520our%250Acontributions%2520using%2520ResNet-9%2520on%2520CIFAR10%2520and%2520WideResNet-28x10%2520on%2520CIFAR100.%250AExperimental%2520results%2520show%2520that%2520our%2520method%2520heals%252093.72%2525%2520of%2520poison%2520compared%2520to%250ASSD%2520with%252083.41%2525%2520and%2520full%2520retraining%2520with%252040.68%2525.%2520We%2520achieve%2520this%2520while%2520also%250Alowering%2520the%2520average%2520model%2520accuracy%2520drop%2520caused%2520by%2520unlearning%2520from%25205.68%2525%2520%2528SSD%2529%250Ato%25201.41%2525%2520%2528ours%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Potion%3A%20Towards%20Poison%20Unlearning&entry.906535625=Stefan%20Schoepf%20and%20Jack%20Foster%20and%20Alexandra%20Brintrup&entry.1292438233=%20%20Adversarial%20attacks%20by%20malicious%20actors%20on%20machine%20learning%20systems%2C%20such%20as%0Aintroducing%20poison%20triggers%20into%20training%20datasets%2C%20pose%20significant%20risks.%20The%0Achallenge%20in%20resolving%20such%20an%20attack%20arises%20in%20practice%20when%20only%20a%20subset%20of%0Athe%20poisoned%20data%20can%20be%20identified.%20This%20necessitates%20the%20development%20of%0Amethods%20to%20remove%2C%20i.e.%20unlearn%2C%20poison%20triggers%20from%20already%20trained%20models%0Awith%20only%20a%20subset%20of%20the%20poison%20data%20available.%20The%20requirements%20for%20this%20task%0Asignificantly%20deviate%20from%20privacy-focused%20unlearning%20where%20all%20of%20the%20data%20to%0Abe%20forgotten%20by%20the%20model%20is%20known.%20Previous%20work%20has%20shown%20that%20the%0Aundiscovered%20poisoned%20samples%20lead%20to%20a%20failure%20of%20established%20unlearning%0Amethods%2C%20with%20only%20one%20method%2C%20Selective%20Synaptic%20Dampening%20%28SSD%29%2C%20showing%0Alimited%20success.%20Even%20full%20retraining%2C%20after%20the%20removal%20of%20the%20identified%0Apoison%2C%20cannot%20address%20this%20challenge%20as%20the%20undiscovered%20poison%20samples%20lead%0Ato%20a%20reintroduction%20of%20the%20poison%20trigger%20in%20the%20model.%20Our%20work%20addresses%20two%0Akey%20challenges%20to%20advance%20the%20state%20of%20the%20art%20in%20poison%20unlearning.%20First%2C%20we%0Aintroduce%20a%20novel%20outlier-resistant%20method%2C%20based%20on%20SSD%2C%20that%20significantly%0Aimproves%20model%20protection%20and%20unlearning%20performance.%20Second%2C%20we%20introduce%0APoison%20Trigger%20Neutralisation%20%28PTN%29%20search%2C%20a%20fast%2C%20parallelisable%2C%0Ahyperparameter%20search%20that%20utilises%20the%20characteristic%20%22unlearning%20versus%20model%0Aprotection%22%20trade-off%20to%20find%20suitable%20hyperparameters%20in%20settings%20where%20the%0Aforget%20set%20size%20is%20unknown%20and%20the%20retain%20set%20is%20contaminated.%20We%20benchmark%20our%0Acontributions%20using%20ResNet-9%20on%20CIFAR10%20and%20WideResNet-28x10%20on%20CIFAR100.%0AExperimental%20results%20show%20that%20our%20method%20heals%2093.72%25%20of%20poison%20compared%20to%0ASSD%20with%2083.41%25%20and%20full%20retraining%20with%2040.68%25.%20We%20achieve%20this%20while%20also%0Alowering%20the%20average%20model%20accuracy%20drop%20caused%20by%20unlearning%20from%205.68%25%20%28SSD%29%0Ato%201.41%25%20%28ours%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09173v2&entry.124074799=Read"},
{"title": "Misclassification excess risk bounds for PAC-Bayesian classification via\n  convexified loss", "author": "The Tien Mai", "abstract": "  PAC-Bayesian bounds have proven to be a valuable tool for deriving\ngeneralization bounds and for designing new learning algorithms in machine\nlearning. However, it typically focus on providing generalization bounds with\nrespect to a chosen loss function. In classification tasks, due to the\nnon-convex nature of the 0-1 loss, a convex surrogate loss is often used, and\nthus current PAC-Bayesian bounds are primarily specified for this convex\nsurrogate. This work shifts its focus to providing misclassification excess\nrisk bounds for PAC-Bayesian classification when using a convex surrogate loss.\nOur key ingredient here is to leverage PAC-Bayesian relative bounds in\nexpectation rather than relying on PAC-Bayesian bounds in probability. We\ndemonstrate our approach in several important applications.\n", "link": "http://arxiv.org/abs/2408.08675v1", "date": "2024-08-16", "relevancy": 1.8688, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Misclassification%20excess%20risk%20bounds%20for%20PAC-Bayesian%20classification%20via%0A%20%20convexified%20loss&body=Title%3A%20Misclassification%20excess%20risk%20bounds%20for%20PAC-Bayesian%20classification%20via%0A%20%20convexified%20loss%0AAuthor%3A%20The%20Tien%20Mai%0AAbstract%3A%20%20%20PAC-Bayesian%20bounds%20have%20proven%20to%20be%20a%20valuable%20tool%20for%20deriving%0Ageneralization%20bounds%20and%20for%20designing%20new%20learning%20algorithms%20in%20machine%0Alearning.%20However%2C%20it%20typically%20focus%20on%20providing%20generalization%20bounds%20with%0Arespect%20to%20a%20chosen%20loss%20function.%20In%20classification%20tasks%2C%20due%20to%20the%0Anon-convex%20nature%20of%20the%200-1%20loss%2C%20a%20convex%20surrogate%20loss%20is%20often%20used%2C%20and%0Athus%20current%20PAC-Bayesian%20bounds%20are%20primarily%20specified%20for%20this%20convex%0Asurrogate.%20This%20work%20shifts%20its%20focus%20to%20providing%20misclassification%20excess%0Arisk%20bounds%20for%20PAC-Bayesian%20classification%20when%20using%20a%20convex%20surrogate%20loss.%0AOur%20key%20ingredient%20here%20is%20to%20leverage%20PAC-Bayesian%20relative%20bounds%20in%0Aexpectation%20rather%20than%20relying%20on%20PAC-Bayesian%20bounds%20in%20probability.%20We%0Ademonstrate%20our%20approach%20in%20several%20important%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMisclassification%2520excess%2520risk%2520bounds%2520for%2520PAC-Bayesian%2520classification%2520via%250A%2520%2520convexified%2520loss%26entry.906535625%3DThe%2520Tien%2520Mai%26entry.1292438233%3D%2520%2520PAC-Bayesian%2520bounds%2520have%2520proven%2520to%2520be%2520a%2520valuable%2520tool%2520for%2520deriving%250Ageneralization%2520bounds%2520and%2520for%2520designing%2520new%2520learning%2520algorithms%2520in%2520machine%250Alearning.%2520However%252C%2520it%2520typically%2520focus%2520on%2520providing%2520generalization%2520bounds%2520with%250Arespect%2520to%2520a%2520chosen%2520loss%2520function.%2520In%2520classification%2520tasks%252C%2520due%2520to%2520the%250Anon-convex%2520nature%2520of%2520the%25200-1%2520loss%252C%2520a%2520convex%2520surrogate%2520loss%2520is%2520often%2520used%252C%2520and%250Athus%2520current%2520PAC-Bayesian%2520bounds%2520are%2520primarily%2520specified%2520for%2520this%2520convex%250Asurrogate.%2520This%2520work%2520shifts%2520its%2520focus%2520to%2520providing%2520misclassification%2520excess%250Arisk%2520bounds%2520for%2520PAC-Bayesian%2520classification%2520when%2520using%2520a%2520convex%2520surrogate%2520loss.%250AOur%2520key%2520ingredient%2520here%2520is%2520to%2520leverage%2520PAC-Bayesian%2520relative%2520bounds%2520in%250Aexpectation%2520rather%2520than%2520relying%2520on%2520PAC-Bayesian%2520bounds%2520in%2520probability.%2520We%250Ademonstrate%2520our%2520approach%2520in%2520several%2520important%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Misclassification%20excess%20risk%20bounds%20for%20PAC-Bayesian%20classification%20via%0A%20%20convexified%20loss&entry.906535625=The%20Tien%20Mai&entry.1292438233=%20%20PAC-Bayesian%20bounds%20have%20proven%20to%20be%20a%20valuable%20tool%20for%20deriving%0Ageneralization%20bounds%20and%20for%20designing%20new%20learning%20algorithms%20in%20machine%0Alearning.%20However%2C%20it%20typically%20focus%20on%20providing%20generalization%20bounds%20with%0Arespect%20to%20a%20chosen%20loss%20function.%20In%20classification%20tasks%2C%20due%20to%20the%0Anon-convex%20nature%20of%20the%200-1%20loss%2C%20a%20convex%20surrogate%20loss%20is%20often%20used%2C%20and%0Athus%20current%20PAC-Bayesian%20bounds%20are%20primarily%20specified%20for%20this%20convex%0Asurrogate.%20This%20work%20shifts%20its%20focus%20to%20providing%20misclassification%20excess%0Arisk%20bounds%20for%20PAC-Bayesian%20classification%20when%20using%20a%20convex%20surrogate%20loss.%0AOur%20key%20ingredient%20here%20is%20to%20leverage%20PAC-Bayesian%20relative%20bounds%20in%0Aexpectation%20rather%20than%20relying%20on%20PAC-Bayesian%20bounds%20in%20probability.%20We%0Ademonstrate%20our%20approach%20in%20several%20important%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08675v1&entry.124074799=Read"},
{"title": "CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational\n  Dialogue Systems", "author": "Joanito Agili Lopo and Marina Indah Prasasti and Alma Permatasari", "abstract": "  In this study, we introduce CIKMar, an efficient approach to educational\ndialogue systems powered by the Gemma Language model. By leveraging a\nDual-Encoder ranking system that incorporates both BERT and SBERT model, we\nhave designed CIKMar to deliver highly relevant and accurate responses, even\nwith the constraints of a smaller language model size. Our evaluation reveals\nthat CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore\nmetrics. However, we have identified a significant challenge: the Dual-Encoder\ntends to prioritize theoretical responses over practical ones. These findings\nunderscore the potential of compact and efficient models like Gemma in\ndemocratizing access to advanced educational AI systems, ensuring effective and\ncontextually appropriate responses.\n", "link": "http://arxiv.org/abs/2408.08805v1", "date": "2024-08-16", "relevancy": 1.8683, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4764}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4749}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CIKMar%3A%20A%20Dual-Encoder%20Approach%20to%20Prompt-Based%20Reranking%20in%20Educational%0A%20%20Dialogue%20Systems&body=Title%3A%20CIKMar%3A%20A%20Dual-Encoder%20Approach%20to%20Prompt-Based%20Reranking%20in%20Educational%0A%20%20Dialogue%20Systems%0AAuthor%3A%20Joanito%20Agili%20Lopo%20and%20Marina%20Indah%20Prasasti%20and%20Alma%20Permatasari%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20CIKMar%2C%20an%20efficient%20approach%20to%20educational%0Adialogue%20systems%20powered%20by%20the%20Gemma%20Language%20model.%20By%20leveraging%20a%0ADual-Encoder%20ranking%20system%20that%20incorporates%20both%20BERT%20and%20SBERT%20model%2C%20we%0Ahave%20designed%20CIKMar%20to%20deliver%20highly%20relevant%20and%20accurate%20responses%2C%20even%0Awith%20the%20constraints%20of%20a%20smaller%20language%20model%20size.%20Our%20evaluation%20reveals%0Athat%20CIKMar%20achieves%20a%20robust%20recall%20and%20F1-score%20of%200.70%20using%20BERTScore%0Ametrics.%20However%2C%20we%20have%20identified%20a%20significant%20challenge%3A%20the%20Dual-Encoder%0Atends%20to%20prioritize%20theoretical%20responses%20over%20practical%20ones.%20These%20findings%0Aunderscore%20the%20potential%20of%20compact%20and%20efficient%20models%20like%20Gemma%20in%0Ademocratizing%20access%20to%20advanced%20educational%20AI%20systems%2C%20ensuring%20effective%20and%0Acontextually%20appropriate%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCIKMar%253A%2520A%2520Dual-Encoder%2520Approach%2520to%2520Prompt-Based%2520Reranking%2520in%2520Educational%250A%2520%2520Dialogue%2520Systems%26entry.906535625%3DJoanito%2520Agili%2520Lopo%2520and%2520Marina%2520Indah%2520Prasasti%2520and%2520Alma%2520Permatasari%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520CIKMar%252C%2520an%2520efficient%2520approach%2520to%2520educational%250Adialogue%2520systems%2520powered%2520by%2520the%2520Gemma%2520Language%2520model.%2520By%2520leveraging%2520a%250ADual-Encoder%2520ranking%2520system%2520that%2520incorporates%2520both%2520BERT%2520and%2520SBERT%2520model%252C%2520we%250Ahave%2520designed%2520CIKMar%2520to%2520deliver%2520highly%2520relevant%2520and%2520accurate%2520responses%252C%2520even%250Awith%2520the%2520constraints%2520of%2520a%2520smaller%2520language%2520model%2520size.%2520Our%2520evaluation%2520reveals%250Athat%2520CIKMar%2520achieves%2520a%2520robust%2520recall%2520and%2520F1-score%2520of%25200.70%2520using%2520BERTScore%250Ametrics.%2520However%252C%2520we%2520have%2520identified%2520a%2520significant%2520challenge%253A%2520the%2520Dual-Encoder%250Atends%2520to%2520prioritize%2520theoretical%2520responses%2520over%2520practical%2520ones.%2520These%2520findings%250Aunderscore%2520the%2520potential%2520of%2520compact%2520and%2520efficient%2520models%2520like%2520Gemma%2520in%250Ademocratizing%2520access%2520to%2520advanced%2520educational%2520AI%2520systems%252C%2520ensuring%2520effective%2520and%250Acontextually%2520appropriate%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CIKMar%3A%20A%20Dual-Encoder%20Approach%20to%20Prompt-Based%20Reranking%20in%20Educational%0A%20%20Dialogue%20Systems&entry.906535625=Joanito%20Agili%20Lopo%20and%20Marina%20Indah%20Prasasti%20and%20Alma%20Permatasari&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20CIKMar%2C%20an%20efficient%20approach%20to%20educational%0Adialogue%20systems%20powered%20by%20the%20Gemma%20Language%20model.%20By%20leveraging%20a%0ADual-Encoder%20ranking%20system%20that%20incorporates%20both%20BERT%20and%20SBERT%20model%2C%20we%0Ahave%20designed%20CIKMar%20to%20deliver%20highly%20relevant%20and%20accurate%20responses%2C%20even%0Awith%20the%20constraints%20of%20a%20smaller%20language%20model%20size.%20Our%20evaluation%20reveals%0Athat%20CIKMar%20achieves%20a%20robust%20recall%20and%20F1-score%20of%200.70%20using%20BERTScore%0Ametrics.%20However%2C%20we%20have%20identified%20a%20significant%20challenge%3A%20the%20Dual-Encoder%0Atends%20to%20prioritize%20theoretical%20responses%20over%20practical%20ones.%20These%20findings%0Aunderscore%20the%20potential%20of%20compact%20and%20efficient%20models%20like%20Gemma%20in%0Ademocratizing%20access%20to%20advanced%20educational%20AI%20systems%2C%20ensuring%20effective%20and%0Acontextually%20appropriate%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08805v1&entry.124074799=Read"},
{"title": "Centralized and Federated Heart Disease Classification Models Using UCI\n  Dataset and their Shapley-value Based Interpretability", "author": "Mario Padilla Rodriguez and Mohamed Nafea", "abstract": "  Cardiovascular diseases are a leading cause of mortality worldwide,\nhighlighting the need for accurate diagnostic methods. This study benchmarks\ncentralized and federated machine learning algorithms for heart disease\nclassification using the UCI dataset which includes 920 patient records from\nfour hospitals in the USA, Hungary and Switzerland. Our benchmark is supported\nby Shapley-value interpretability analysis to quantify features' importance for\nclassification. In the centralized setup, various binary classification\nalgorithms are trained on pooled data, with a support vector machine (SVM)\nachieving the highest testing accuracy of 83.3\\%, surpassing the established\nbenchmark of 78.7\\% with logistic regression. Additionally, federated learning\nalgorithms with four clients (hospitals) are explored, leveraging the dataset's\nnatural partition to enhance privacy without sacrificing accuracy. Federated\nSVM, an uncommon approach in the literature, achieves a top testing accuracy of\n73.8\\%. Our interpretability analysis aligns with existing medical knowledge of\nheart disease indicators. Overall, this study establishes a benchmark for\nefficient and interpretable pre-screening tools for heart disease while\nmaintaining patients' privacy. This work is available at\nhttps://github.com/padillma1/Heart-Disease-Classification-on-UCI-dataset-and-Shapley-Interpretability-Analysis.\n", "link": "http://arxiv.org/abs/2408.06183v2", "date": "2024-08-16", "relevancy": 1.8445, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.463}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centralized%20and%20Federated%20Heart%20Disease%20Classification%20Models%20Using%20UCI%0A%20%20Dataset%20and%20their%20Shapley-value%20Based%20Interpretability&body=Title%3A%20Centralized%20and%20Federated%20Heart%20Disease%20Classification%20Models%20Using%20UCI%0A%20%20Dataset%20and%20their%20Shapley-value%20Based%20Interpretability%0AAuthor%3A%20Mario%20Padilla%20Rodriguez%20and%20Mohamed%20Nafea%0AAbstract%3A%20%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20mortality%20worldwide%2C%0Ahighlighting%20the%20need%20for%20accurate%20diagnostic%20methods.%20This%20study%20benchmarks%0Acentralized%20and%20federated%20machine%20learning%20algorithms%20for%20heart%20disease%0Aclassification%20using%20the%20UCI%20dataset%20which%20includes%20920%20patient%20records%20from%0Afour%20hospitals%20in%20the%20USA%2C%20Hungary%20and%20Switzerland.%20Our%20benchmark%20is%20supported%0Aby%20Shapley-value%20interpretability%20analysis%20to%20quantify%20features%27%20importance%20for%0Aclassification.%20In%20the%20centralized%20setup%2C%20various%20binary%20classification%0Aalgorithms%20are%20trained%20on%20pooled%20data%2C%20with%20a%20support%20vector%20machine%20%28SVM%29%0Aachieving%20the%20highest%20testing%20accuracy%20of%2083.3%5C%25%2C%20surpassing%20the%20established%0Abenchmark%20of%2078.7%5C%25%20with%20logistic%20regression.%20Additionally%2C%20federated%20learning%0Aalgorithms%20with%20four%20clients%20%28hospitals%29%20are%20explored%2C%20leveraging%20the%20dataset%27s%0Anatural%20partition%20to%20enhance%20privacy%20without%20sacrificing%20accuracy.%20Federated%0ASVM%2C%20an%20uncommon%20approach%20in%20the%20literature%2C%20achieves%20a%20top%20testing%20accuracy%20of%0A73.8%5C%25.%20Our%20interpretability%20analysis%20aligns%20with%20existing%20medical%20knowledge%20of%0Aheart%20disease%20indicators.%20Overall%2C%20this%20study%20establishes%20a%20benchmark%20for%0Aefficient%20and%20interpretable%20pre-screening%20tools%20for%20heart%20disease%20while%0Amaintaining%20patients%27%20privacy.%20This%20work%20is%20available%20at%0Ahttps%3A//github.com/padillma1/Heart-Disease-Classification-on-UCI-dataset-and-Shapley-Interpretability-Analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentralized%2520and%2520Federated%2520Heart%2520Disease%2520Classification%2520Models%2520Using%2520UCI%250A%2520%2520Dataset%2520and%2520their%2520Shapley-value%2520Based%2520Interpretability%26entry.906535625%3DMario%2520Padilla%2520Rodriguez%2520and%2520Mohamed%2520Nafea%26entry.1292438233%3D%2520%2520Cardiovascular%2520diseases%2520are%2520a%2520leading%2520cause%2520of%2520mortality%2520worldwide%252C%250Ahighlighting%2520the%2520need%2520for%2520accurate%2520diagnostic%2520methods.%2520This%2520study%2520benchmarks%250Acentralized%2520and%2520federated%2520machine%2520learning%2520algorithms%2520for%2520heart%2520disease%250Aclassification%2520using%2520the%2520UCI%2520dataset%2520which%2520includes%2520920%2520patient%2520records%2520from%250Afour%2520hospitals%2520in%2520the%2520USA%252C%2520Hungary%2520and%2520Switzerland.%2520Our%2520benchmark%2520is%2520supported%250Aby%2520Shapley-value%2520interpretability%2520analysis%2520to%2520quantify%2520features%2527%2520importance%2520for%250Aclassification.%2520In%2520the%2520centralized%2520setup%252C%2520various%2520binary%2520classification%250Aalgorithms%2520are%2520trained%2520on%2520pooled%2520data%252C%2520with%2520a%2520support%2520vector%2520machine%2520%2528SVM%2529%250Aachieving%2520the%2520highest%2520testing%2520accuracy%2520of%252083.3%255C%2525%252C%2520surpassing%2520the%2520established%250Abenchmark%2520of%252078.7%255C%2525%2520with%2520logistic%2520regression.%2520Additionally%252C%2520federated%2520learning%250Aalgorithms%2520with%2520four%2520clients%2520%2528hospitals%2529%2520are%2520explored%252C%2520leveraging%2520the%2520dataset%2527s%250Anatural%2520partition%2520to%2520enhance%2520privacy%2520without%2520sacrificing%2520accuracy.%2520Federated%250ASVM%252C%2520an%2520uncommon%2520approach%2520in%2520the%2520literature%252C%2520achieves%2520a%2520top%2520testing%2520accuracy%2520of%250A73.8%255C%2525.%2520Our%2520interpretability%2520analysis%2520aligns%2520with%2520existing%2520medical%2520knowledge%2520of%250Aheart%2520disease%2520indicators.%2520Overall%252C%2520this%2520study%2520establishes%2520a%2520benchmark%2520for%250Aefficient%2520and%2520interpretable%2520pre-screening%2520tools%2520for%2520heart%2520disease%2520while%250Amaintaining%2520patients%2527%2520privacy.%2520This%2520work%2520is%2520available%2520at%250Ahttps%253A//github.com/padillma1/Heart-Disease-Classification-on-UCI-dataset-and-Shapley-Interpretability-Analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centralized%20and%20Federated%20Heart%20Disease%20Classification%20Models%20Using%20UCI%0A%20%20Dataset%20and%20their%20Shapley-value%20Based%20Interpretability&entry.906535625=Mario%20Padilla%20Rodriguez%20and%20Mohamed%20Nafea&entry.1292438233=%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20mortality%20worldwide%2C%0Ahighlighting%20the%20need%20for%20accurate%20diagnostic%20methods.%20This%20study%20benchmarks%0Acentralized%20and%20federated%20machine%20learning%20algorithms%20for%20heart%20disease%0Aclassification%20using%20the%20UCI%20dataset%20which%20includes%20920%20patient%20records%20from%0Afour%20hospitals%20in%20the%20USA%2C%20Hungary%20and%20Switzerland.%20Our%20benchmark%20is%20supported%0Aby%20Shapley-value%20interpretability%20analysis%20to%20quantify%20features%27%20importance%20for%0Aclassification.%20In%20the%20centralized%20setup%2C%20various%20binary%20classification%0Aalgorithms%20are%20trained%20on%20pooled%20data%2C%20with%20a%20support%20vector%20machine%20%28SVM%29%0Aachieving%20the%20highest%20testing%20accuracy%20of%2083.3%5C%25%2C%20surpassing%20the%20established%0Abenchmark%20of%2078.7%5C%25%20with%20logistic%20regression.%20Additionally%2C%20federated%20learning%0Aalgorithms%20with%20four%20clients%20%28hospitals%29%20are%20explored%2C%20leveraging%20the%20dataset%27s%0Anatural%20partition%20to%20enhance%20privacy%20without%20sacrificing%20accuracy.%20Federated%0ASVM%2C%20an%20uncommon%20approach%20in%20the%20literature%2C%20achieves%20a%20top%20testing%20accuracy%20of%0A73.8%5C%25.%20Our%20interpretability%20analysis%20aligns%20with%20existing%20medical%20knowledge%20of%0Aheart%20disease%20indicators.%20Overall%2C%20this%20study%20establishes%20a%20benchmark%20for%0Aefficient%20and%20interpretable%20pre-screening%20tools%20for%20heart%20disease%20while%0Amaintaining%20patients%27%20privacy.%20This%20work%20is%20available%20at%0Ahttps%3A//github.com/padillma1/Heart-Disease-Classification-on-UCI-dataset-and-Shapley-Interpretability-Analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06183v2&entry.124074799=Read"},
{"title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation\n  Instructions", "author": "Bhuvanashree Murugadoss and Christian Poelitz and Ian Drosos and Vu Le and Nick McKenna and Carina Suzana Negreanu and Chris Parnin and Advait Sarkar", "abstract": "  LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality.\n", "link": "http://arxiv.org/abs/2408.08781v1", "date": "2024-08-16", "relevancy": 1.8444, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5179}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4555}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Evaluator%3A%20Measuring%20LLMs%27%20Adherence%20to%20Task%20Evaluation%0A%20%20Instructions&body=Title%3A%20Evaluating%20the%20Evaluator%3A%20Measuring%20LLMs%27%20Adherence%20to%20Task%20Evaluation%0A%20%20Instructions%0AAuthor%3A%20Bhuvanashree%20Murugadoss%20and%20Christian%20Poelitz%20and%20Ian%20Drosos%20and%20Vu%20Le%20and%20Nick%20McKenna%20and%20Carina%20Suzana%20Negreanu%20and%20Chris%20Parnin%20and%20Advait%20Sarkar%0AAbstract%3A%20%20%20LLMs-as-a-judge%20is%20a%20recently%20popularized%20method%20which%20replaces%20human%0Ajudgements%20in%20task%20evaluation%20%28Zheng%20et%20al.%202024%29%20with%20automatic%20evaluation%0Ausing%20LLMs.%20Due%20to%20widespread%20use%20of%20RLHF%20%28Reinforcement%20Learning%20from%20Human%0AFeedback%29%2C%20state-of-the-art%20LLMs%20like%20GPT4%20and%20Llama3%20are%20expected%20to%20have%0Astrong%20alignment%20with%20human%20preferences%20when%20prompted%20for%20a%20quality%20judgement%2C%0Asuch%20as%20the%20coherence%20of%20a%20text.%20While%20this%20seems%20beneficial%2C%20it%20is%20not%20clear%0Awhether%20the%20assessments%20by%20an%20LLM-as-a-judge%20constitute%20only%20an%20evaluation%0Abased%20on%20the%20instructions%20in%20the%20prompts%2C%20or%20reflect%20its%20preference%20for%0Ahigh-quality%20data%20similar%20to%20its%20fine-tune%20data.%20To%20investigate%20how%20much%0Ainfluence%20prompting%20the%20LLMs-as-a-judge%20has%20on%20the%20alignment%20of%20AI%20judgements%0Ato%20human%20judgements%2C%20we%20analyze%20prompts%20with%20increasing%20levels%20of%20instructions%0Aabout%20the%20target%20quality%20of%20an%20evaluation%2C%20for%20several%20LLMs-as-a-judge.%0AFurther%2C%20we%20compare%20to%20a%20prompt-free%20method%20using%20model%20perplexity%20as%20a%20quality%0Ameasure%20instead.%20We%20aggregate%20a%20taxonomy%20of%20quality%20criteria%20commonly%20used%0Aacross%20state-of-the-art%20evaluations%20with%20LLMs%20and%20provide%20this%20as%20a%20rigorous%0Abenchmark%20of%20models%20as%20judges.%20Overall%2C%20we%20show%20that%20the%20LLMs-as-a-judge%0Abenefit%20only%20little%20from%20highly%20detailed%20instructions%20in%20prompts%20and%20that%0Aperplexity%20can%20sometimes%20align%20better%20with%20human%20judgements%20than%20prompting%2C%0Aespecially%20on%20textual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Evaluator%253A%2520Measuring%2520LLMs%2527%2520Adherence%2520to%2520Task%2520Evaluation%250A%2520%2520Instructions%26entry.906535625%3DBhuvanashree%2520Murugadoss%2520and%2520Christian%2520Poelitz%2520and%2520Ian%2520Drosos%2520and%2520Vu%2520Le%2520and%2520Nick%2520McKenna%2520and%2520Carina%2520Suzana%2520Negreanu%2520and%2520Chris%2520Parnin%2520and%2520Advait%2520Sarkar%26entry.1292438233%3D%2520%2520LLMs-as-a-judge%2520is%2520a%2520recently%2520popularized%2520method%2520which%2520replaces%2520human%250Ajudgements%2520in%2520task%2520evaluation%2520%2528Zheng%2520et%2520al.%25202024%2529%2520with%2520automatic%2520evaluation%250Ausing%2520LLMs.%2520Due%2520to%2520widespread%2520use%2520of%2520RLHF%2520%2528Reinforcement%2520Learning%2520from%2520Human%250AFeedback%2529%252C%2520state-of-the-art%2520LLMs%2520like%2520GPT4%2520and%2520Llama3%2520are%2520expected%2520to%2520have%250Astrong%2520alignment%2520with%2520human%2520preferences%2520when%2520prompted%2520for%2520a%2520quality%2520judgement%252C%250Asuch%2520as%2520the%2520coherence%2520of%2520a%2520text.%2520While%2520this%2520seems%2520beneficial%252C%2520it%2520is%2520not%2520clear%250Awhether%2520the%2520assessments%2520by%2520an%2520LLM-as-a-judge%2520constitute%2520only%2520an%2520evaluation%250Abased%2520on%2520the%2520instructions%2520in%2520the%2520prompts%252C%2520or%2520reflect%2520its%2520preference%2520for%250Ahigh-quality%2520data%2520similar%2520to%2520its%2520fine-tune%2520data.%2520To%2520investigate%2520how%2520much%250Ainfluence%2520prompting%2520the%2520LLMs-as-a-judge%2520has%2520on%2520the%2520alignment%2520of%2520AI%2520judgements%250Ato%2520human%2520judgements%252C%2520we%2520analyze%2520prompts%2520with%2520increasing%2520levels%2520of%2520instructions%250Aabout%2520the%2520target%2520quality%2520of%2520an%2520evaluation%252C%2520for%2520several%2520LLMs-as-a-judge.%250AFurther%252C%2520we%2520compare%2520to%2520a%2520prompt-free%2520method%2520using%2520model%2520perplexity%2520as%2520a%2520quality%250Ameasure%2520instead.%2520We%2520aggregate%2520a%2520taxonomy%2520of%2520quality%2520criteria%2520commonly%2520used%250Aacross%2520state-of-the-art%2520evaluations%2520with%2520LLMs%2520and%2520provide%2520this%2520as%2520a%2520rigorous%250Abenchmark%2520of%2520models%2520as%2520judges.%2520Overall%252C%2520we%2520show%2520that%2520the%2520LLMs-as-a-judge%250Abenefit%2520only%2520little%2520from%2520highly%2520detailed%2520instructions%2520in%2520prompts%2520and%2520that%250Aperplexity%2520can%2520sometimes%2520align%2520better%2520with%2520human%2520judgements%2520than%2520prompting%252C%250Aespecially%2520on%2520textual%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Evaluator%3A%20Measuring%20LLMs%27%20Adherence%20to%20Task%20Evaluation%0A%20%20Instructions&entry.906535625=Bhuvanashree%20Murugadoss%20and%20Christian%20Poelitz%20and%20Ian%20Drosos%20and%20Vu%20Le%20and%20Nick%20McKenna%20and%20Carina%20Suzana%20Negreanu%20and%20Chris%20Parnin%20and%20Advait%20Sarkar&entry.1292438233=%20%20LLMs-as-a-judge%20is%20a%20recently%20popularized%20method%20which%20replaces%20human%0Ajudgements%20in%20task%20evaluation%20%28Zheng%20et%20al.%202024%29%20with%20automatic%20evaluation%0Ausing%20LLMs.%20Due%20to%20widespread%20use%20of%20RLHF%20%28Reinforcement%20Learning%20from%20Human%0AFeedback%29%2C%20state-of-the-art%20LLMs%20like%20GPT4%20and%20Llama3%20are%20expected%20to%20have%0Astrong%20alignment%20with%20human%20preferences%20when%20prompted%20for%20a%20quality%20judgement%2C%0Asuch%20as%20the%20coherence%20of%20a%20text.%20While%20this%20seems%20beneficial%2C%20it%20is%20not%20clear%0Awhether%20the%20assessments%20by%20an%20LLM-as-a-judge%20constitute%20only%20an%20evaluation%0Abased%20on%20the%20instructions%20in%20the%20prompts%2C%20or%20reflect%20its%20preference%20for%0Ahigh-quality%20data%20similar%20to%20its%20fine-tune%20data.%20To%20investigate%20how%20much%0Ainfluence%20prompting%20the%20LLMs-as-a-judge%20has%20on%20the%20alignment%20of%20AI%20judgements%0Ato%20human%20judgements%2C%20we%20analyze%20prompts%20with%20increasing%20levels%20of%20instructions%0Aabout%20the%20target%20quality%20of%20an%20evaluation%2C%20for%20several%20LLMs-as-a-judge.%0AFurther%2C%20we%20compare%20to%20a%20prompt-free%20method%20using%20model%20perplexity%20as%20a%20quality%0Ameasure%20instead.%20We%20aggregate%20a%20taxonomy%20of%20quality%20criteria%20commonly%20used%0Aacross%20state-of-the-art%20evaluations%20with%20LLMs%20and%20provide%20this%20as%20a%20rigorous%0Abenchmark%20of%20models%20as%20judges.%20Overall%2C%20we%20show%20that%20the%20LLMs-as-a-judge%0Abenefit%20only%20little%20from%20highly%20detailed%20instructions%20in%20prompts%20and%20that%0Aperplexity%20can%20sometimes%20align%20better%20with%20human%20judgements%20than%20prompting%2C%0Aespecially%20on%20textual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08781v1&entry.124074799=Read"},
{"title": "Unlocking the Non-Native Language Context Limitation: Native Language\n  Prompting Facilitates Knowledge Elicitation", "author": "Baixuan Li and Yunlong Fan and Zhiqiang Gao", "abstract": "  Multilingual large language models (MLLMs) struggle to answer questions posed\nin non-dominant languages, even though they have acquired the relevant\nknowledge from their dominant language corpus. In contrast, human multilinguals\ncan overcome such non-native language context limitations through Positive\nNative Language Transfer (PNLT). Inspired by the process of PNLT, we analogize\nthe dominant language of MLLMs to the native language of human multilinguals,\nand propose Native Language Prompting (NatLan) to simulate the PNLT observed in\nhuman multilinguals. It explicitly creates native language contexts for MLLMs\nto facilitate the elicitation of the rich native language knowledge during\nquestion-answering, unlocking the limitations imposed by non-native language\ncontexts. By employing multi-MLLM collaboration, NatLan reduces the workload on\neach MLLM in simulating PNLT and refines semantic transfer. On the C-Eval\nbenchmark, NatLan provides up to a 10.1% average accuracy improvement and up to\na 5.0% increase in the hard-level subset across five MLLMs, surpassing all\ntop-notch related methods. Our code is available at\nhttps://github.com/AnonyNLP/NatLan.\n", "link": "http://arxiv.org/abs/2408.03544v2", "date": "2024-08-16", "relevancy": 1.8352, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20the%20Non-Native%20Language%20Context%20Limitation%3A%20Native%20Language%0A%20%20Prompting%20Facilitates%20Knowledge%20Elicitation&body=Title%3A%20Unlocking%20the%20Non-Native%20Language%20Context%20Limitation%3A%20Native%20Language%0A%20%20Prompting%20Facilitates%20Knowledge%20Elicitation%0AAuthor%3A%20Baixuan%20Li%20and%20Yunlong%20Fan%20and%20Zhiqiang%20Gao%0AAbstract%3A%20%20%20Multilingual%20large%20language%20models%20%28MLLMs%29%20struggle%20to%20answer%20questions%20posed%0Ain%20non-dominant%20languages%2C%20even%20though%20they%20have%20acquired%20the%20relevant%0Aknowledge%20from%20their%20dominant%20language%20corpus.%20In%20contrast%2C%20human%20multilinguals%0Acan%20overcome%20such%20non-native%20language%20context%20limitations%20through%20Positive%0ANative%20Language%20Transfer%20%28PNLT%29.%20Inspired%20by%20the%20process%20of%20PNLT%2C%20we%20analogize%0Athe%20dominant%20language%20of%20MLLMs%20to%20the%20native%20language%20of%20human%20multilinguals%2C%0Aand%20propose%20Native%20Language%20Prompting%20%28NatLan%29%20to%20simulate%20the%20PNLT%20observed%20in%0Ahuman%20multilinguals.%20It%20explicitly%20creates%20native%20language%20contexts%20for%20MLLMs%0Ato%20facilitate%20the%20elicitation%20of%20the%20rich%20native%20language%20knowledge%20during%0Aquestion-answering%2C%20unlocking%20the%20limitations%20imposed%20by%20non-native%20language%0Acontexts.%20By%20employing%20multi-MLLM%20collaboration%2C%20NatLan%20reduces%20the%20workload%20on%0Aeach%20MLLM%20in%20simulating%20PNLT%20and%20refines%20semantic%20transfer.%20On%20the%20C-Eval%0Abenchmark%2C%20NatLan%20provides%20up%20to%20a%2010.1%25%20average%20accuracy%20improvement%20and%20up%20to%0Aa%205.0%25%20increase%20in%20the%20hard-level%20subset%20across%20five%20MLLMs%2C%20surpassing%20all%0Atop-notch%20related%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AnonyNLP/NatLan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520the%2520Non-Native%2520Language%2520Context%2520Limitation%253A%2520Native%2520Language%250A%2520%2520Prompting%2520Facilitates%2520Knowledge%2520Elicitation%26entry.906535625%3DBaixuan%2520Li%2520and%2520Yunlong%2520Fan%2520and%2520Zhiqiang%2520Gao%26entry.1292438233%3D%2520%2520Multilingual%2520large%2520language%2520models%2520%2528MLLMs%2529%2520struggle%2520to%2520answer%2520questions%2520posed%250Ain%2520non-dominant%2520languages%252C%2520even%2520though%2520they%2520have%2520acquired%2520the%2520relevant%250Aknowledge%2520from%2520their%2520dominant%2520language%2520corpus.%2520In%2520contrast%252C%2520human%2520multilinguals%250Acan%2520overcome%2520such%2520non-native%2520language%2520context%2520limitations%2520through%2520Positive%250ANative%2520Language%2520Transfer%2520%2528PNLT%2529.%2520Inspired%2520by%2520the%2520process%2520of%2520PNLT%252C%2520we%2520analogize%250Athe%2520dominant%2520language%2520of%2520MLLMs%2520to%2520the%2520native%2520language%2520of%2520human%2520multilinguals%252C%250Aand%2520propose%2520Native%2520Language%2520Prompting%2520%2528NatLan%2529%2520to%2520simulate%2520the%2520PNLT%2520observed%2520in%250Ahuman%2520multilinguals.%2520It%2520explicitly%2520creates%2520native%2520language%2520contexts%2520for%2520MLLMs%250Ato%2520facilitate%2520the%2520elicitation%2520of%2520the%2520rich%2520native%2520language%2520knowledge%2520during%250Aquestion-answering%252C%2520unlocking%2520the%2520limitations%2520imposed%2520by%2520non-native%2520language%250Acontexts.%2520By%2520employing%2520multi-MLLM%2520collaboration%252C%2520NatLan%2520reduces%2520the%2520workload%2520on%250Aeach%2520MLLM%2520in%2520simulating%2520PNLT%2520and%2520refines%2520semantic%2520transfer.%2520On%2520the%2520C-Eval%250Abenchmark%252C%2520NatLan%2520provides%2520up%2520to%2520a%252010.1%2525%2520average%2520accuracy%2520improvement%2520and%2520up%2520to%250Aa%25205.0%2525%2520increase%2520in%2520the%2520hard-level%2520subset%2520across%2520five%2520MLLMs%252C%2520surpassing%2520all%250Atop-notch%2520related%2520methods.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AnonyNLP/NatLan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20the%20Non-Native%20Language%20Context%20Limitation%3A%20Native%20Language%0A%20%20Prompting%20Facilitates%20Knowledge%20Elicitation&entry.906535625=Baixuan%20Li%20and%20Yunlong%20Fan%20and%20Zhiqiang%20Gao&entry.1292438233=%20%20Multilingual%20large%20language%20models%20%28MLLMs%29%20struggle%20to%20answer%20questions%20posed%0Ain%20non-dominant%20languages%2C%20even%20though%20they%20have%20acquired%20the%20relevant%0Aknowledge%20from%20their%20dominant%20language%20corpus.%20In%20contrast%2C%20human%20multilinguals%0Acan%20overcome%20such%20non-native%20language%20context%20limitations%20through%20Positive%0ANative%20Language%20Transfer%20%28PNLT%29.%20Inspired%20by%20the%20process%20of%20PNLT%2C%20we%20analogize%0Athe%20dominant%20language%20of%20MLLMs%20to%20the%20native%20language%20of%20human%20multilinguals%2C%0Aand%20propose%20Native%20Language%20Prompting%20%28NatLan%29%20to%20simulate%20the%20PNLT%20observed%20in%0Ahuman%20multilinguals.%20It%20explicitly%20creates%20native%20language%20contexts%20for%20MLLMs%0Ato%20facilitate%20the%20elicitation%20of%20the%20rich%20native%20language%20knowledge%20during%0Aquestion-answering%2C%20unlocking%20the%20limitations%20imposed%20by%20non-native%20language%0Acontexts.%20By%20employing%20multi-MLLM%20collaboration%2C%20NatLan%20reduces%20the%20workload%20on%0Aeach%20MLLM%20in%20simulating%20PNLT%20and%20refines%20semantic%20transfer.%20On%20the%20C-Eval%0Abenchmark%2C%20NatLan%20provides%20up%20to%20a%2010.1%25%20average%20accuracy%20improvement%20and%20up%20to%0Aa%205.0%25%20increase%20in%20the%20hard-level%20subset%20across%20five%20MLLMs%2C%20surpassing%20all%0Atop-notch%20related%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AnonyNLP/NatLan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03544v2&entry.124074799=Read"},
{"title": "Revisiting Score Function Estimators for $k$-Subset Sampling", "author": "Klas Wijk and Ricardo Vinuesa and Hossein Azizpour", "abstract": "  Are score function estimators an underestimated approach to learning with\n$k$-subset sampling? Sampling $k$-subsets is a fundamental operation in many\nmachine learning tasks that is not amenable to differentiable parametrization,\nimpeding gradient-based optimization. Prior work has focused on relaxed\nsampling or pathwise gradient estimators. Inspired by the success of score\nfunction estimators in variational inference and reinforcement learning, we\nrevisit them within the context of $k$-subset sampling. Specifically, we\ndemonstrate how to efficiently compute the $k$-subset distribution's score\nfunction using a discrete Fourier transform, and reduce the estimator's\nvariance with control variates. The resulting estimator provides both exact\nsamples and unbiased gradient estimates while also applying to\nnon-differentiable downstream models, unlike existing methods. Experiments in\nfeature selection show results competitive with current methods, despite weaker\nassumptions.\n", "link": "http://arxiv.org/abs/2407.16058v2", "date": "2024-08-16", "relevancy": 1.8284, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4801}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Score%20Function%20Estimators%20for%20%24k%24-Subset%20Sampling&body=Title%3A%20Revisiting%20Score%20Function%20Estimators%20for%20%24k%24-Subset%20Sampling%0AAuthor%3A%20Klas%20Wijk%20and%20Ricardo%20Vinuesa%20and%20Hossein%20Azizpour%0AAbstract%3A%20%20%20Are%20score%20function%20estimators%20an%20underestimated%20approach%20to%20learning%20with%0A%24k%24-subset%20sampling%3F%20Sampling%20%24k%24-subsets%20is%20a%20fundamental%20operation%20in%20many%0Amachine%20learning%20tasks%20that%20is%20not%20amenable%20to%20differentiable%20parametrization%2C%0Aimpeding%20gradient-based%20optimization.%20Prior%20work%20has%20focused%20on%20relaxed%0Asampling%20or%20pathwise%20gradient%20estimators.%20Inspired%20by%20the%20success%20of%20score%0Afunction%20estimators%20in%20variational%20inference%20and%20reinforcement%20learning%2C%20we%0Arevisit%20them%20within%20the%20context%20of%20%24k%24-subset%20sampling.%20Specifically%2C%20we%0Ademonstrate%20how%20to%20efficiently%20compute%20the%20%24k%24-subset%20distribution%27s%20score%0Afunction%20using%20a%20discrete%20Fourier%20transform%2C%20and%20reduce%20the%20estimator%27s%0Avariance%20with%20control%20variates.%20The%20resulting%20estimator%20provides%20both%20exact%0Asamples%20and%20unbiased%20gradient%20estimates%20while%20also%20applying%20to%0Anon-differentiable%20downstream%20models%2C%20unlike%20existing%20methods.%20Experiments%20in%0Afeature%20selection%20show%20results%20competitive%20with%20current%20methods%2C%20despite%20weaker%0Aassumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Score%2520Function%2520Estimators%2520for%2520%2524k%2524-Subset%2520Sampling%26entry.906535625%3DKlas%2520Wijk%2520and%2520Ricardo%2520Vinuesa%2520and%2520Hossein%2520Azizpour%26entry.1292438233%3D%2520%2520Are%2520score%2520function%2520estimators%2520an%2520underestimated%2520approach%2520to%2520learning%2520with%250A%2524k%2524-subset%2520sampling%253F%2520Sampling%2520%2524k%2524-subsets%2520is%2520a%2520fundamental%2520operation%2520in%2520many%250Amachine%2520learning%2520tasks%2520that%2520is%2520not%2520amenable%2520to%2520differentiable%2520parametrization%252C%250Aimpeding%2520gradient-based%2520optimization.%2520Prior%2520work%2520has%2520focused%2520on%2520relaxed%250Asampling%2520or%2520pathwise%2520gradient%2520estimators.%2520Inspired%2520by%2520the%2520success%2520of%2520score%250Afunction%2520estimators%2520in%2520variational%2520inference%2520and%2520reinforcement%2520learning%252C%2520we%250Arevisit%2520them%2520within%2520the%2520context%2520of%2520%2524k%2524-subset%2520sampling.%2520Specifically%252C%2520we%250Ademonstrate%2520how%2520to%2520efficiently%2520compute%2520the%2520%2524k%2524-subset%2520distribution%2527s%2520score%250Afunction%2520using%2520a%2520discrete%2520Fourier%2520transform%252C%2520and%2520reduce%2520the%2520estimator%2527s%250Avariance%2520with%2520control%2520variates.%2520The%2520resulting%2520estimator%2520provides%2520both%2520exact%250Asamples%2520and%2520unbiased%2520gradient%2520estimates%2520while%2520also%2520applying%2520to%250Anon-differentiable%2520downstream%2520models%252C%2520unlike%2520existing%2520methods.%2520Experiments%2520in%250Afeature%2520selection%2520show%2520results%2520competitive%2520with%2520current%2520methods%252C%2520despite%2520weaker%250Aassumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Score%20Function%20Estimators%20for%20%24k%24-Subset%20Sampling&entry.906535625=Klas%20Wijk%20and%20Ricardo%20Vinuesa%20and%20Hossein%20Azizpour&entry.1292438233=%20%20Are%20score%20function%20estimators%20an%20underestimated%20approach%20to%20learning%20with%0A%24k%24-subset%20sampling%3F%20Sampling%20%24k%24-subsets%20is%20a%20fundamental%20operation%20in%20many%0Amachine%20learning%20tasks%20that%20is%20not%20amenable%20to%20differentiable%20parametrization%2C%0Aimpeding%20gradient-based%20optimization.%20Prior%20work%20has%20focused%20on%20relaxed%0Asampling%20or%20pathwise%20gradient%20estimators.%20Inspired%20by%20the%20success%20of%20score%0Afunction%20estimators%20in%20variational%20inference%20and%20reinforcement%20learning%2C%20we%0Arevisit%20them%20within%20the%20context%20of%20%24k%24-subset%20sampling.%20Specifically%2C%20we%0Ademonstrate%20how%20to%20efficiently%20compute%20the%20%24k%24-subset%20distribution%27s%20score%0Afunction%20using%20a%20discrete%20Fourier%20transform%2C%20and%20reduce%20the%20estimator%27s%0Avariance%20with%20control%20variates.%20The%20resulting%20estimator%20provides%20both%20exact%0Asamples%20and%20unbiased%20gradient%20estimates%20while%20also%20applying%20to%0Anon-differentiable%20downstream%20models%2C%20unlike%20existing%20methods.%20Experiments%20in%0Afeature%20selection%20show%20results%20competitive%20with%20current%20methods%2C%20despite%20weaker%0Aassumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16058v2&entry.124074799=Read"},
{"title": "PFDiff: Training-free Acceleration of Diffusion Models through the\n  Gradient Guidance of Past and Future", "author": "Guangyi Wang and Yuren Cai and Lijiang Li and Wei Peng and Songzhi Su", "abstract": "  Diffusion Probabilistic Models (DPMs) have shown remarkable potential in\nimage generation, but their sampling efficiency is hindered by the need for\nnumerous denoising steps. Most existing solutions accelerate the sampling\nprocess by proposing fast ODE solvers. However, the inevitable discretization\nerrors of the ODE solvers are significantly magnified when the number of\nfunction evaluations (NFE) is fewer. In this work, we propose PFDiff, a novel\ntraining-free and orthogonal timestep-skipping strategy, which enables existing\nfast ODE solvers to operate with fewer NFE. Based on two key observations: a\nsignificant similarity in the model's outputs at time step size that is not\nexcessively large during the denoising process of existing ODE solvers, and a\nhigh resemblance between the denoising process and SGD. PFDiff, by employing\ngradient replacement from past time steps and foresight updates inspired by\nNesterov momentum, rapidly updates intermediate states, thereby reducing\nunnecessary NFE while correcting for discretization errors inherent in\nfirst-order ODE solvers. Experimental results demonstrate that PFDiff exhibits\nflexible applicability across various pre-trained DPMs, particularly excelling\nin conditional DPMs and surpassing previous state-of-the-art training-free\nmethods. For instance, using DDIM as a baseline, we achieved 16.46 FID (4 NFE)\ncompared to 138.81 FID with DDIM on ImageNet 64x64 with classifier guidance,\nand 13.06 FID (10 NFE) on Stable Diffusion with 7.5 guidance scale.\n", "link": "http://arxiv.org/abs/2408.08822v1", "date": "2024-08-16", "relevancy": 1.8266, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6816}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.598}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PFDiff%3A%20Training-free%20Acceleration%20of%20Diffusion%20Models%20through%20the%0A%20%20Gradient%20Guidance%20of%20Past%20and%20Future&body=Title%3A%20PFDiff%3A%20Training-free%20Acceleration%20of%20Diffusion%20Models%20through%20the%0A%20%20Gradient%20Guidance%20of%20Past%20and%20Future%0AAuthor%3A%20Guangyi%20Wang%20and%20Yuren%20Cai%20and%20Lijiang%20Li%20and%20Wei%20Peng%20and%20Songzhi%20Su%0AAbstract%3A%20%20%20Diffusion%20Probabilistic%20Models%20%28DPMs%29%20have%20shown%20remarkable%20potential%20in%0Aimage%20generation%2C%20but%20their%20sampling%20efficiency%20is%20hindered%20by%20the%20need%20for%0Anumerous%20denoising%20steps.%20Most%20existing%20solutions%20accelerate%20the%20sampling%0Aprocess%20by%20proposing%20fast%20ODE%20solvers.%20However%2C%20the%20inevitable%20discretization%0Aerrors%20of%20the%20ODE%20solvers%20are%20significantly%20magnified%20when%20the%20number%20of%0Afunction%20evaluations%20%28NFE%29%20is%20fewer.%20In%20this%20work%2C%20we%20propose%20PFDiff%2C%20a%20novel%0Atraining-free%20and%20orthogonal%20timestep-skipping%20strategy%2C%20which%20enables%20existing%0Afast%20ODE%20solvers%20to%20operate%20with%20fewer%20NFE.%20Based%20on%20two%20key%20observations%3A%20a%0Asignificant%20similarity%20in%20the%20model%27s%20outputs%20at%20time%20step%20size%20that%20is%20not%0Aexcessively%20large%20during%20the%20denoising%20process%20of%20existing%20ODE%20solvers%2C%20and%20a%0Ahigh%20resemblance%20between%20the%20denoising%20process%20and%20SGD.%20PFDiff%2C%20by%20employing%0Agradient%20replacement%20from%20past%20time%20steps%20and%20foresight%20updates%20inspired%20by%0ANesterov%20momentum%2C%20rapidly%20updates%20intermediate%20states%2C%20thereby%20reducing%0Aunnecessary%20NFE%20while%20correcting%20for%20discretization%20errors%20inherent%20in%0Afirst-order%20ODE%20solvers.%20Experimental%20results%20demonstrate%20that%20PFDiff%20exhibits%0Aflexible%20applicability%20across%20various%20pre-trained%20DPMs%2C%20particularly%20excelling%0Ain%20conditional%20DPMs%20and%20surpassing%20previous%20state-of-the-art%20training-free%0Amethods.%20For%20instance%2C%20using%20DDIM%20as%20a%20baseline%2C%20we%20achieved%2016.46%20FID%20%284%20NFE%29%0Acompared%20to%20138.81%20FID%20with%20DDIM%20on%20ImageNet%2064x64%20with%20classifier%20guidance%2C%0Aand%2013.06%20FID%20%2810%20NFE%29%20on%20Stable%20Diffusion%20with%207.5%20guidance%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPFDiff%253A%2520Training-free%2520Acceleration%2520of%2520Diffusion%2520Models%2520through%2520the%250A%2520%2520Gradient%2520Guidance%2520of%2520Past%2520and%2520Future%26entry.906535625%3DGuangyi%2520Wang%2520and%2520Yuren%2520Cai%2520and%2520Lijiang%2520Li%2520and%2520Wei%2520Peng%2520and%2520Songzhi%2520Su%26entry.1292438233%3D%2520%2520Diffusion%2520Probabilistic%2520Models%2520%2528DPMs%2529%2520have%2520shown%2520remarkable%2520potential%2520in%250Aimage%2520generation%252C%2520but%2520their%2520sampling%2520efficiency%2520is%2520hindered%2520by%2520the%2520need%2520for%250Anumerous%2520denoising%2520steps.%2520Most%2520existing%2520solutions%2520accelerate%2520the%2520sampling%250Aprocess%2520by%2520proposing%2520fast%2520ODE%2520solvers.%2520However%252C%2520the%2520inevitable%2520discretization%250Aerrors%2520of%2520the%2520ODE%2520solvers%2520are%2520significantly%2520magnified%2520when%2520the%2520number%2520of%250Afunction%2520evaluations%2520%2528NFE%2529%2520is%2520fewer.%2520In%2520this%2520work%252C%2520we%2520propose%2520PFDiff%252C%2520a%2520novel%250Atraining-free%2520and%2520orthogonal%2520timestep-skipping%2520strategy%252C%2520which%2520enables%2520existing%250Afast%2520ODE%2520solvers%2520to%2520operate%2520with%2520fewer%2520NFE.%2520Based%2520on%2520two%2520key%2520observations%253A%2520a%250Asignificant%2520similarity%2520in%2520the%2520model%2527s%2520outputs%2520at%2520time%2520step%2520size%2520that%2520is%2520not%250Aexcessively%2520large%2520during%2520the%2520denoising%2520process%2520of%2520existing%2520ODE%2520solvers%252C%2520and%2520a%250Ahigh%2520resemblance%2520between%2520the%2520denoising%2520process%2520and%2520SGD.%2520PFDiff%252C%2520by%2520employing%250Agradient%2520replacement%2520from%2520past%2520time%2520steps%2520and%2520foresight%2520updates%2520inspired%2520by%250ANesterov%2520momentum%252C%2520rapidly%2520updates%2520intermediate%2520states%252C%2520thereby%2520reducing%250Aunnecessary%2520NFE%2520while%2520correcting%2520for%2520discretization%2520errors%2520inherent%2520in%250Afirst-order%2520ODE%2520solvers.%2520Experimental%2520results%2520demonstrate%2520that%2520PFDiff%2520exhibits%250Aflexible%2520applicability%2520across%2520various%2520pre-trained%2520DPMs%252C%2520particularly%2520excelling%250Ain%2520conditional%2520DPMs%2520and%2520surpassing%2520previous%2520state-of-the-art%2520training-free%250Amethods.%2520For%2520instance%252C%2520using%2520DDIM%2520as%2520a%2520baseline%252C%2520we%2520achieved%252016.46%2520FID%2520%25284%2520NFE%2529%250Acompared%2520to%2520138.81%2520FID%2520with%2520DDIM%2520on%2520ImageNet%252064x64%2520with%2520classifier%2520guidance%252C%250Aand%252013.06%2520FID%2520%252810%2520NFE%2529%2520on%2520Stable%2520Diffusion%2520with%25207.5%2520guidance%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFDiff%3A%20Training-free%20Acceleration%20of%20Diffusion%20Models%20through%20the%0A%20%20Gradient%20Guidance%20of%20Past%20and%20Future&entry.906535625=Guangyi%20Wang%20and%20Yuren%20Cai%20and%20Lijiang%20Li%20and%20Wei%20Peng%20and%20Songzhi%20Su&entry.1292438233=%20%20Diffusion%20Probabilistic%20Models%20%28DPMs%29%20have%20shown%20remarkable%20potential%20in%0Aimage%20generation%2C%20but%20their%20sampling%20efficiency%20is%20hindered%20by%20the%20need%20for%0Anumerous%20denoising%20steps.%20Most%20existing%20solutions%20accelerate%20the%20sampling%0Aprocess%20by%20proposing%20fast%20ODE%20solvers.%20However%2C%20the%20inevitable%20discretization%0Aerrors%20of%20the%20ODE%20solvers%20are%20significantly%20magnified%20when%20the%20number%20of%0Afunction%20evaluations%20%28NFE%29%20is%20fewer.%20In%20this%20work%2C%20we%20propose%20PFDiff%2C%20a%20novel%0Atraining-free%20and%20orthogonal%20timestep-skipping%20strategy%2C%20which%20enables%20existing%0Afast%20ODE%20solvers%20to%20operate%20with%20fewer%20NFE.%20Based%20on%20two%20key%20observations%3A%20a%0Asignificant%20similarity%20in%20the%20model%27s%20outputs%20at%20time%20step%20size%20that%20is%20not%0Aexcessively%20large%20during%20the%20denoising%20process%20of%20existing%20ODE%20solvers%2C%20and%20a%0Ahigh%20resemblance%20between%20the%20denoising%20process%20and%20SGD.%20PFDiff%2C%20by%20employing%0Agradient%20replacement%20from%20past%20time%20steps%20and%20foresight%20updates%20inspired%20by%0ANesterov%20momentum%2C%20rapidly%20updates%20intermediate%20states%2C%20thereby%20reducing%0Aunnecessary%20NFE%20while%20correcting%20for%20discretization%20errors%20inherent%20in%0Afirst-order%20ODE%20solvers.%20Experimental%20results%20demonstrate%20that%20PFDiff%20exhibits%0Aflexible%20applicability%20across%20various%20pre-trained%20DPMs%2C%20particularly%20excelling%0Ain%20conditional%20DPMs%20and%20surpassing%20previous%20state-of-the-art%20training-free%0Amethods.%20For%20instance%2C%20using%20DDIM%20as%20a%20baseline%2C%20we%20achieved%2016.46%20FID%20%284%20NFE%29%0Acompared%20to%20138.81%20FID%20with%20DDIM%20on%20ImageNet%2064x64%20with%20classifier%20guidance%2C%0Aand%2013.06%20FID%20%2810%20NFE%29%20on%20Stable%20Diffusion%20with%207.5%20guidance%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08822v1&entry.124074799=Read"},
{"title": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People", "author": "Xidong Wang and Nuo Chen and Junyin Chen and Yidong Wang and Guorui Zhen and Yan Hu and Xiangbo Wu and Anningzhe Gao and Xiang Wan and Haizhou Li and Benyou Wang", "abstract": "  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n", "link": "http://arxiv.org/abs/2403.03640v4", "date": "2024-08-16", "relevancy": 1.81, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4745}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4722}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Apollo%3A%20A%20Lightweight%20Multilingual%20Medical%20LLM%20towards%20Democratizing%0A%20%20Medical%20AI%20to%206B%20People&body=Title%3A%20Apollo%3A%20A%20Lightweight%20Multilingual%20Medical%20LLM%20towards%20Democratizing%0A%20%20Medical%20AI%20to%206B%20People%0AAuthor%3A%20Xidong%20Wang%20and%20Nuo%20Chen%20and%20Junyin%20Chen%20and%20Yidong%20Wang%20and%20Guorui%20Zhen%20and%20Yan%20Hu%20and%20Xiangbo%20Wu%20and%20Anningzhe%20Gao%20and%20Xiang%20Wan%20and%20Haizhou%20Li%20and%20Benyou%20Wang%0AAbstract%3A%20%20%20Despite%20the%20vast%20repository%20of%20global%20medical%20knowledge%20predominantly%20being%0Ain%20English%2C%20local%20languages%20are%20crucial%20for%20delivering%20tailored%20healthcare%0Aservices%2C%20particularly%20in%20areas%20with%20limited%20medical%20resources.%20To%20extend%20the%0Areach%20of%20medical%20AI%20advancements%20to%20a%20broader%20population%2C%20we%20aim%20to%20develop%0Amedical%20LLMs%20across%20the%20six%20most%20widely%20spoken%20languages%2C%20encompassing%20a%20global%0Apopulation%20of%206.1%20billion.%20This%20effort%20culminates%20in%20the%20creation%20of%20the%0AApolloCorpora%20multilingual%20medical%20dataset%20and%20the%20XMedBench%20benchmark.%20In%20the%0Amultilingual%20medical%20benchmark%2C%20the%20released%20Apollo%20models%2C%20at%20various%0Arelatively-small%20sizes%20%28i.e.%2C%200.5B%2C%201.8B%2C%202B%2C%206B%2C%20and%207B%29%2C%20achieve%20the%20best%0Aperformance%20among%20models%20of%20equivalent%20size.%20Especially%2C%20Apollo-7B%20is%20the%0Astate-of-the-art%20multilingual%20medical%20LLMs%20up%20to%2070B.%20Additionally%2C%20these%20lite%0Amodels%20could%20be%20used%20to%20improve%20the%20multi-lingual%20medical%20capabilities%20of%0Alarger%20models%20without%20fine-tuning%20in%20a%20proxy-tuning%20fashion.%20We%20will%0Aopen-source%20training%20corpora%2C%20code%2C%20model%20weights%20and%20evaluation%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03640v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApollo%253A%2520A%2520Lightweight%2520Multilingual%2520Medical%2520LLM%2520towards%2520Democratizing%250A%2520%2520Medical%2520AI%2520to%25206B%2520People%26entry.906535625%3DXidong%2520Wang%2520and%2520Nuo%2520Chen%2520and%2520Junyin%2520Chen%2520and%2520Yidong%2520Wang%2520and%2520Guorui%2520Zhen%2520and%2520Yan%2520Hu%2520and%2520Xiangbo%2520Wu%2520and%2520Anningzhe%2520Gao%2520and%2520Xiang%2520Wan%2520and%2520Haizhou%2520Li%2520and%2520Benyou%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520vast%2520repository%2520of%2520global%2520medical%2520knowledge%2520predominantly%2520being%250Ain%2520English%252C%2520local%2520languages%2520are%2520crucial%2520for%2520delivering%2520tailored%2520healthcare%250Aservices%252C%2520particularly%2520in%2520areas%2520with%2520limited%2520medical%2520resources.%2520To%2520extend%2520the%250Areach%2520of%2520medical%2520AI%2520advancements%2520to%2520a%2520broader%2520population%252C%2520we%2520aim%2520to%2520develop%250Amedical%2520LLMs%2520across%2520the%2520six%2520most%2520widely%2520spoken%2520languages%252C%2520encompassing%2520a%2520global%250Apopulation%2520of%25206.1%2520billion.%2520This%2520effort%2520culminates%2520in%2520the%2520creation%2520of%2520the%250AApolloCorpora%2520multilingual%2520medical%2520dataset%2520and%2520the%2520XMedBench%2520benchmark.%2520In%2520the%250Amultilingual%2520medical%2520benchmark%252C%2520the%2520released%2520Apollo%2520models%252C%2520at%2520various%250Arelatively-small%2520sizes%2520%2528i.e.%252C%25200.5B%252C%25201.8B%252C%25202B%252C%25206B%252C%2520and%25207B%2529%252C%2520achieve%2520the%2520best%250Aperformance%2520among%2520models%2520of%2520equivalent%2520size.%2520Especially%252C%2520Apollo-7B%2520is%2520the%250Astate-of-the-art%2520multilingual%2520medical%2520LLMs%2520up%2520to%252070B.%2520Additionally%252C%2520these%2520lite%250Amodels%2520could%2520be%2520used%2520to%2520improve%2520the%2520multi-lingual%2520medical%2520capabilities%2520of%250Alarger%2520models%2520without%2520fine-tuning%2520in%2520a%2520proxy-tuning%2520fashion.%2520We%2520will%250Aopen-source%2520training%2520corpora%252C%2520code%252C%2520model%2520weights%2520and%2520evaluation%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03640v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apollo%3A%20A%20Lightweight%20Multilingual%20Medical%20LLM%20towards%20Democratizing%0A%20%20Medical%20AI%20to%206B%20People&entry.906535625=Xidong%20Wang%20and%20Nuo%20Chen%20and%20Junyin%20Chen%20and%20Yidong%20Wang%20and%20Guorui%20Zhen%20and%20Yan%20Hu%20and%20Xiangbo%20Wu%20and%20Anningzhe%20Gao%20and%20Xiang%20Wan%20and%20Haizhou%20Li%20and%20Benyou%20Wang&entry.1292438233=%20%20Despite%20the%20vast%20repository%20of%20global%20medical%20knowledge%20predominantly%20being%0Ain%20English%2C%20local%20languages%20are%20crucial%20for%20delivering%20tailored%20healthcare%0Aservices%2C%20particularly%20in%20areas%20with%20limited%20medical%20resources.%20To%20extend%20the%0Areach%20of%20medical%20AI%20advancements%20to%20a%20broader%20population%2C%20we%20aim%20to%20develop%0Amedical%20LLMs%20across%20the%20six%20most%20widely%20spoken%20languages%2C%20encompassing%20a%20global%0Apopulation%20of%206.1%20billion.%20This%20effort%20culminates%20in%20the%20creation%20of%20the%0AApolloCorpora%20multilingual%20medical%20dataset%20and%20the%20XMedBench%20benchmark.%20In%20the%0Amultilingual%20medical%20benchmark%2C%20the%20released%20Apollo%20models%2C%20at%20various%0Arelatively-small%20sizes%20%28i.e.%2C%200.5B%2C%201.8B%2C%202B%2C%206B%2C%20and%207B%29%2C%20achieve%20the%20best%0Aperformance%20among%20models%20of%20equivalent%20size.%20Especially%2C%20Apollo-7B%20is%20the%0Astate-of-the-art%20multilingual%20medical%20LLMs%20up%20to%2070B.%20Additionally%2C%20these%20lite%0Amodels%20could%20be%20used%20to%20improve%20the%20multi-lingual%20medical%20capabilities%20of%0Alarger%20models%20without%20fine-tuning%20in%20a%20proxy-tuning%20fashion.%20We%20will%0Aopen-source%20training%20corpora%2C%20code%2C%20model%20weights%20and%20evaluation%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03640v4&entry.124074799=Read"},
{"title": "DivCon: Divide and Conquer for Progressive Text-to-Image Generation", "author": "Yuhao Jia and Wenhan Tan", "abstract": "  Diffusion-driven text-to-image (T2I) generation has achieved remarkable\nadvancements. To further improve T2I models' capability in numerical and\nspatial reasoning, the layout is employed as an intermedium to bridge large\nlanguage models and layout-based diffusion models. However, these methods still\nstruggle with generating images from textural prompts with multiple objects and\ncomplicated spatial relationships. To tackle this challenge, we introduce a\ndivide-and-conquer approach which decouples the T2I generation task into simple\nsubtasks. Our approach divides the layout prediction stage into numerical &\nspatial reasoning and bounding box prediction. Then, the layout-to-image\ngeneration stage is conducted in an iterative manner to reconstruct objects\nfrom easy ones to difficult ones. We conduct experiments on the HRS and NSR-1K\nbenchmarks and our approach outperforms previous state-of-the-art models with\nnotable margins. In addition, visual results demonstrate that our approach\nsignificantly improves the controllability and consistency in generating\nmultiple objects from complex textural prompts.\n", "link": "http://arxiv.org/abs/2403.06400v2", "date": "2024-08-16", "relevancy": 1.8088, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6215}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6176}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DivCon%3A%20Divide%20and%20Conquer%20for%20Progressive%20Text-to-Image%20Generation&body=Title%3A%20DivCon%3A%20Divide%20and%20Conquer%20for%20Progressive%20Text-to-Image%20Generation%0AAuthor%3A%20Yuhao%20Jia%20and%20Wenhan%20Tan%0AAbstract%3A%20%20%20Diffusion-driven%20text-to-image%20%28T2I%29%20generation%20has%20achieved%20remarkable%0Aadvancements.%20To%20further%20improve%20T2I%20models%27%20capability%20in%20numerical%20and%0Aspatial%20reasoning%2C%20the%20layout%20is%20employed%20as%20an%20intermedium%20to%20bridge%20large%0Alanguage%20models%20and%20layout-based%20diffusion%20models.%20However%2C%20these%20methods%20still%0Astruggle%20with%20generating%20images%20from%20textural%20prompts%20with%20multiple%20objects%20and%0Acomplicated%20spatial%20relationships.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%0Adivide-and-conquer%20approach%20which%20decouples%20the%20T2I%20generation%20task%20into%20simple%0Asubtasks.%20Our%20approach%20divides%20the%20layout%20prediction%20stage%20into%20numerical%20%26%0Aspatial%20reasoning%20and%20bounding%20box%20prediction.%20Then%2C%20the%20layout-to-image%0Ageneration%20stage%20is%20conducted%20in%20an%20iterative%20manner%20to%20reconstruct%20objects%0Afrom%20easy%20ones%20to%20difficult%20ones.%20We%20conduct%20experiments%20on%20the%20HRS%20and%20NSR-1K%0Abenchmarks%20and%20our%20approach%20outperforms%20previous%20state-of-the-art%20models%20with%0Anotable%20margins.%20In%20addition%2C%20visual%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20the%20controllability%20and%20consistency%20in%20generating%0Amultiple%20objects%20from%20complex%20textural%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivCon%253A%2520Divide%2520and%2520Conquer%2520for%2520Progressive%2520Text-to-Image%2520Generation%26entry.906535625%3DYuhao%2520Jia%2520and%2520Wenhan%2520Tan%26entry.1292438233%3D%2520%2520Diffusion-driven%2520text-to-image%2520%2528T2I%2529%2520generation%2520has%2520achieved%2520remarkable%250Aadvancements.%2520To%2520further%2520improve%2520T2I%2520models%2527%2520capability%2520in%2520numerical%2520and%250Aspatial%2520reasoning%252C%2520the%2520layout%2520is%2520employed%2520as%2520an%2520intermedium%2520to%2520bridge%2520large%250Alanguage%2520models%2520and%2520layout-based%2520diffusion%2520models.%2520However%252C%2520these%2520methods%2520still%250Astruggle%2520with%2520generating%2520images%2520from%2520textural%2520prompts%2520with%2520multiple%2520objects%2520and%250Acomplicated%2520spatial%2520relationships.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520introduce%2520a%250Adivide-and-conquer%2520approach%2520which%2520decouples%2520the%2520T2I%2520generation%2520task%2520into%2520simple%250Asubtasks.%2520Our%2520approach%2520divides%2520the%2520layout%2520prediction%2520stage%2520into%2520numerical%2520%2526%250Aspatial%2520reasoning%2520and%2520bounding%2520box%2520prediction.%2520Then%252C%2520the%2520layout-to-image%250Ageneration%2520stage%2520is%2520conducted%2520in%2520an%2520iterative%2520manner%2520to%2520reconstruct%2520objects%250Afrom%2520easy%2520ones%2520to%2520difficult%2520ones.%2520We%2520conduct%2520experiments%2520on%2520the%2520HRS%2520and%2520NSR-1K%250Abenchmarks%2520and%2520our%2520approach%2520outperforms%2520previous%2520state-of-the-art%2520models%2520with%250Anotable%2520margins.%2520In%2520addition%252C%2520visual%2520results%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520improves%2520the%2520controllability%2520and%2520consistency%2520in%2520generating%250Amultiple%2520objects%2520from%2520complex%2520textural%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DivCon%3A%20Divide%20and%20Conquer%20for%20Progressive%20Text-to-Image%20Generation&entry.906535625=Yuhao%20Jia%20and%20Wenhan%20Tan&entry.1292438233=%20%20Diffusion-driven%20text-to-image%20%28T2I%29%20generation%20has%20achieved%20remarkable%0Aadvancements.%20To%20further%20improve%20T2I%20models%27%20capability%20in%20numerical%20and%0Aspatial%20reasoning%2C%20the%20layout%20is%20employed%20as%20an%20intermedium%20to%20bridge%20large%0Alanguage%20models%20and%20layout-based%20diffusion%20models.%20However%2C%20these%20methods%20still%0Astruggle%20with%20generating%20images%20from%20textural%20prompts%20with%20multiple%20objects%20and%0Acomplicated%20spatial%20relationships.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%0Adivide-and-conquer%20approach%20which%20decouples%20the%20T2I%20generation%20task%20into%20simple%0Asubtasks.%20Our%20approach%20divides%20the%20layout%20prediction%20stage%20into%20numerical%20%26%0Aspatial%20reasoning%20and%20bounding%20box%20prediction.%20Then%2C%20the%20layout-to-image%0Ageneration%20stage%20is%20conducted%20in%20an%20iterative%20manner%20to%20reconstruct%20objects%0Afrom%20easy%20ones%20to%20difficult%20ones.%20We%20conduct%20experiments%20on%20the%20HRS%20and%20NSR-1K%0Abenchmarks%20and%20our%20approach%20outperforms%20previous%20state-of-the-art%20models%20with%0Anotable%20margins.%20In%20addition%2C%20visual%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20the%20controllability%20and%20consistency%20in%20generating%0Amultiple%20objects%20from%20complex%20textural%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06400v2&entry.124074799=Read"},
{"title": "Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal\n  Time Series Imputation", "author": "Baoyu Jing and Dawei Zhou and Kan Ren and Carl Yang", "abstract": "  Spatiotemporal time series are usually collected via monitoring sensors\nplaced at different locations, which usually contain missing values due to\nvarious mechanical failures. Imputing the missing values is crucial for\nanalyzing time series. When recovering a specific data point, most existing\nmethods consider all the information relevant to that point regardless of the\ncause-and-effect relationship. During data collection, it is inevitable that\nsome unknown confounders are included, e.g., background noise in time series\nand non-causal shortcut edges in the constructed sensor network. These\nconfounders could open backdoor paths and establish non-causal correlations\nbetween the input and output. Over-exploiting these non-causal correlations\ncould cause overfitting. In this paper, we first revisit spatiotemporal time\nseries imputation from a causal perspective and show how to block the\nconfounders via the frontdoor adjustment. Based on the results of frontdoor\nadjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural\nNetwork (Casper), which contains a novel Prompt Based Decoder (PBD) and a\nSpatiotemporal Causal Attention (SCA). PBD could reduce the impact of\nconfounders and SCA could discover the sparse causal relationships among\nembeddings. Theoretical analysis reveals that SCA discovers causal\nrelationships based on the values of gradients. We evaluate Casper on three\nreal-world datasets, and the experimental results show that Casper could\noutperform the baselines and could effectively discover causal relationships.\n", "link": "http://arxiv.org/abs/2403.11960v2", "date": "2024-08-16", "relevancy": 1.8029, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causality-Aware%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Spatiotemporal%0A%20%20Time%20Series%20Imputation&body=Title%3A%20Causality-Aware%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Spatiotemporal%0A%20%20Time%20Series%20Imputation%0AAuthor%3A%20Baoyu%20Jing%20and%20Dawei%20Zhou%20and%20Kan%20Ren%20and%20Carl%20Yang%0AAbstract%3A%20%20%20Spatiotemporal%20time%20series%20are%20usually%20collected%20via%20monitoring%20sensors%0Aplaced%20at%20different%20locations%2C%20which%20usually%20contain%20missing%20values%20due%20to%0Avarious%20mechanical%20failures.%20Imputing%20the%20missing%20values%20is%20crucial%20for%0Aanalyzing%20time%20series.%20When%20recovering%20a%20specific%20data%20point%2C%20most%20existing%0Amethods%20consider%20all%20the%20information%20relevant%20to%20that%20point%20regardless%20of%20the%0Acause-and-effect%20relationship.%20During%20data%20collection%2C%20it%20is%20inevitable%20that%0Asome%20unknown%20confounders%20are%20included%2C%20e.g.%2C%20background%20noise%20in%20time%20series%0Aand%20non-causal%20shortcut%20edges%20in%20the%20constructed%20sensor%20network.%20These%0Aconfounders%20could%20open%20backdoor%20paths%20and%20establish%20non-causal%20correlations%0Abetween%20the%20input%20and%20output.%20Over-exploiting%20these%20non-causal%20correlations%0Acould%20cause%20overfitting.%20In%20this%20paper%2C%20we%20first%20revisit%20spatiotemporal%20time%0Aseries%20imputation%20from%20a%20causal%20perspective%20and%20show%20how%20to%20block%20the%0Aconfounders%20via%20the%20frontdoor%20adjustment.%20Based%20on%20the%20results%20of%20frontdoor%0Aadjustment%2C%20we%20introduce%20a%20novel%20Causality-Aware%20Spatiotemporal%20Graph%20Neural%0ANetwork%20%28Casper%29%2C%20which%20contains%20a%20novel%20Prompt%20Based%20Decoder%20%28PBD%29%20and%20a%0ASpatiotemporal%20Causal%20Attention%20%28SCA%29.%20PBD%20could%20reduce%20the%20impact%20of%0Aconfounders%20and%20SCA%20could%20discover%20the%20sparse%20causal%20relationships%20among%0Aembeddings.%20Theoretical%20analysis%20reveals%20that%20SCA%20discovers%20causal%0Arelationships%20based%20on%20the%20values%20of%20gradients.%20We%20evaluate%20Casper%20on%20three%0Areal-world%20datasets%2C%20and%20the%20experimental%20results%20show%20that%20Casper%20could%0Aoutperform%20the%20baselines%20and%20could%20effectively%20discover%20causal%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausality-Aware%2520Spatiotemporal%2520Graph%2520Neural%2520Networks%2520for%2520Spatiotemporal%250A%2520%2520Time%2520Series%2520Imputation%26entry.906535625%3DBaoyu%2520Jing%2520and%2520Dawei%2520Zhou%2520and%2520Kan%2520Ren%2520and%2520Carl%2520Yang%26entry.1292438233%3D%2520%2520Spatiotemporal%2520time%2520series%2520are%2520usually%2520collected%2520via%2520monitoring%2520sensors%250Aplaced%2520at%2520different%2520locations%252C%2520which%2520usually%2520contain%2520missing%2520values%2520due%2520to%250Avarious%2520mechanical%2520failures.%2520Imputing%2520the%2520missing%2520values%2520is%2520crucial%2520for%250Aanalyzing%2520time%2520series.%2520When%2520recovering%2520a%2520specific%2520data%2520point%252C%2520most%2520existing%250Amethods%2520consider%2520all%2520the%2520information%2520relevant%2520to%2520that%2520point%2520regardless%2520of%2520the%250Acause-and-effect%2520relationship.%2520During%2520data%2520collection%252C%2520it%2520is%2520inevitable%2520that%250Asome%2520unknown%2520confounders%2520are%2520included%252C%2520e.g.%252C%2520background%2520noise%2520in%2520time%2520series%250Aand%2520non-causal%2520shortcut%2520edges%2520in%2520the%2520constructed%2520sensor%2520network.%2520These%250Aconfounders%2520could%2520open%2520backdoor%2520paths%2520and%2520establish%2520non-causal%2520correlations%250Abetween%2520the%2520input%2520and%2520output.%2520Over-exploiting%2520these%2520non-causal%2520correlations%250Acould%2520cause%2520overfitting.%2520In%2520this%2520paper%252C%2520we%2520first%2520revisit%2520spatiotemporal%2520time%250Aseries%2520imputation%2520from%2520a%2520causal%2520perspective%2520and%2520show%2520how%2520to%2520block%2520the%250Aconfounders%2520via%2520the%2520frontdoor%2520adjustment.%2520Based%2520on%2520the%2520results%2520of%2520frontdoor%250Aadjustment%252C%2520we%2520introduce%2520a%2520novel%2520Causality-Aware%2520Spatiotemporal%2520Graph%2520Neural%250ANetwork%2520%2528Casper%2529%252C%2520which%2520contains%2520a%2520novel%2520Prompt%2520Based%2520Decoder%2520%2528PBD%2529%2520and%2520a%250ASpatiotemporal%2520Causal%2520Attention%2520%2528SCA%2529.%2520PBD%2520could%2520reduce%2520the%2520impact%2520of%250Aconfounders%2520and%2520SCA%2520could%2520discover%2520the%2520sparse%2520causal%2520relationships%2520among%250Aembeddings.%2520Theoretical%2520analysis%2520reveals%2520that%2520SCA%2520discovers%2520causal%250Arelationships%2520based%2520on%2520the%2520values%2520of%2520gradients.%2520We%2520evaluate%2520Casper%2520on%2520three%250Areal-world%2520datasets%252C%2520and%2520the%2520experimental%2520results%2520show%2520that%2520Casper%2520could%250Aoutperform%2520the%2520baselines%2520and%2520could%2520effectively%2520discover%2520causal%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causality-Aware%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Spatiotemporal%0A%20%20Time%20Series%20Imputation&entry.906535625=Baoyu%20Jing%20and%20Dawei%20Zhou%20and%20Kan%20Ren%20and%20Carl%20Yang&entry.1292438233=%20%20Spatiotemporal%20time%20series%20are%20usually%20collected%20via%20monitoring%20sensors%0Aplaced%20at%20different%20locations%2C%20which%20usually%20contain%20missing%20values%20due%20to%0Avarious%20mechanical%20failures.%20Imputing%20the%20missing%20values%20is%20crucial%20for%0Aanalyzing%20time%20series.%20When%20recovering%20a%20specific%20data%20point%2C%20most%20existing%0Amethods%20consider%20all%20the%20information%20relevant%20to%20that%20point%20regardless%20of%20the%0Acause-and-effect%20relationship.%20During%20data%20collection%2C%20it%20is%20inevitable%20that%0Asome%20unknown%20confounders%20are%20included%2C%20e.g.%2C%20background%20noise%20in%20time%20series%0Aand%20non-causal%20shortcut%20edges%20in%20the%20constructed%20sensor%20network.%20These%0Aconfounders%20could%20open%20backdoor%20paths%20and%20establish%20non-causal%20correlations%0Abetween%20the%20input%20and%20output.%20Over-exploiting%20these%20non-causal%20correlations%0Acould%20cause%20overfitting.%20In%20this%20paper%2C%20we%20first%20revisit%20spatiotemporal%20time%0Aseries%20imputation%20from%20a%20causal%20perspective%20and%20show%20how%20to%20block%20the%0Aconfounders%20via%20the%20frontdoor%20adjustment.%20Based%20on%20the%20results%20of%20frontdoor%0Aadjustment%2C%20we%20introduce%20a%20novel%20Causality-Aware%20Spatiotemporal%20Graph%20Neural%0ANetwork%20%28Casper%29%2C%20which%20contains%20a%20novel%20Prompt%20Based%20Decoder%20%28PBD%29%20and%20a%0ASpatiotemporal%20Causal%20Attention%20%28SCA%29.%20PBD%20could%20reduce%20the%20impact%20of%0Aconfounders%20and%20SCA%20could%20discover%20the%20sparse%20causal%20relationships%20among%0Aembeddings.%20Theoretical%20analysis%20reveals%20that%20SCA%20discovers%20causal%0Arelationships%20based%20on%20the%20values%20of%20gradients.%20We%20evaluate%20Casper%20on%20three%0Areal-world%20datasets%2C%20and%20the%20experimental%20results%20show%20that%20Casper%20could%0Aoutperform%20the%20baselines%20and%20could%20effectively%20discover%20causal%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11960v2&entry.124074799=Read"},
{"title": "Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation", "author": "Yanjie Dong and Haijun Zhang and Gang Wang and Shisheng Cui and Xiping Hu", "abstract": "  By using an parametric value function to replace the Monte-Carlo rollouts for\nvalue estimation, the actor-critic (AC) algorithms can reduce the variance of\nstochastic policy gradient so that to improve the convergence rate. While\nexisting works mainly focus on analyzing convergence rate of AC algorithms\nunder Markovian noise, the impacts of momentum on AC algorithms remain largely\nunexplored. In this work, we first propose a heavy-ball momentum based\nadvantage actor-critic (\\mbox{HB-A2C}) algorithm by integrating the heavy-ball\nmomentum into the critic recursion that is parameterized by a linear function.\nWhen the sample trajectory follows a Markov decision process, we quantitatively\ncertify the acceleration capability of the proposed HB-A2C algorithm. Our\ntheoretical results demonstrate that the proposed HB-A2C finds an\n$\\epsilon$-approximate stationary point with $\\oo{\\epsilon^{-2}}$ iterations\nfor reinforcement learning tasks with Markovian noise. Moreover, we also reveal\nthe dependence of learning rates on the length of the sample trajectory. By\ncarefully selecting the momentum factor of the critic recursion, the proposed\nHB-A2C can balance the errors introduced by the initialization and the\nstoschastic approximation.\n", "link": "http://arxiv.org/abs/2408.06945v2", "date": "2024-08-16", "relevancy": 1.8021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5075}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heavy-Ball%20Momentum%20Accelerated%20Actor-Critic%20With%20Function%20Approximation&body=Title%3A%20Heavy-Ball%20Momentum%20Accelerated%20Actor-Critic%20With%20Function%20Approximation%0AAuthor%3A%20Yanjie%20Dong%20and%20Haijun%20Zhang%20and%20Gang%20Wang%20and%20Shisheng%20Cui%20and%20Xiping%20Hu%0AAbstract%3A%20%20%20By%20using%20an%20parametric%20value%20function%20to%20replace%20the%20Monte-Carlo%20rollouts%20for%0Avalue%20estimation%2C%20the%20actor-critic%20%28AC%29%20algorithms%20can%20reduce%20the%20variance%20of%0Astochastic%20policy%20gradient%20so%20that%20to%20improve%20the%20convergence%20rate.%20While%0Aexisting%20works%20mainly%20focus%20on%20analyzing%20convergence%20rate%20of%20AC%20algorithms%0Aunder%20Markovian%20noise%2C%20the%20impacts%20of%20momentum%20on%20AC%20algorithms%20remain%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20first%20propose%20a%20heavy-ball%20momentum%20based%0Aadvantage%20actor-critic%20%28%5Cmbox%7BHB-A2C%7D%29%20algorithm%20by%20integrating%20the%20heavy-ball%0Amomentum%20into%20the%20critic%20recursion%20that%20is%20parameterized%20by%20a%20linear%20function.%0AWhen%20the%20sample%20trajectory%20follows%20a%20Markov%20decision%20process%2C%20we%20quantitatively%0Acertify%20the%20acceleration%20capability%20of%20the%20proposed%20HB-A2C%20algorithm.%20Our%0Atheoretical%20results%20demonstrate%20that%20the%20proposed%20HB-A2C%20finds%20an%0A%24%5Cepsilon%24-approximate%20stationary%20point%20with%20%24%5Coo%7B%5Cepsilon%5E%7B-2%7D%7D%24%20iterations%0Afor%20reinforcement%20learning%20tasks%20with%20Markovian%20noise.%20Moreover%2C%20we%20also%20reveal%0Athe%20dependence%20of%20learning%20rates%20on%20the%20length%20of%20the%20sample%20trajectory.%20By%0Acarefully%20selecting%20the%20momentum%20factor%20of%20the%20critic%20recursion%2C%20the%20proposed%0AHB-A2C%20can%20balance%20the%20errors%20introduced%20by%20the%20initialization%20and%20the%0Astoschastic%20approximation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeavy-Ball%2520Momentum%2520Accelerated%2520Actor-Critic%2520With%2520Function%2520Approximation%26entry.906535625%3DYanjie%2520Dong%2520and%2520Haijun%2520Zhang%2520and%2520Gang%2520Wang%2520and%2520Shisheng%2520Cui%2520and%2520Xiping%2520Hu%26entry.1292438233%3D%2520%2520By%2520using%2520an%2520parametric%2520value%2520function%2520to%2520replace%2520the%2520Monte-Carlo%2520rollouts%2520for%250Avalue%2520estimation%252C%2520the%2520actor-critic%2520%2528AC%2529%2520algorithms%2520can%2520reduce%2520the%2520variance%2520of%250Astochastic%2520policy%2520gradient%2520so%2520that%2520to%2520improve%2520the%2520convergence%2520rate.%2520While%250Aexisting%2520works%2520mainly%2520focus%2520on%2520analyzing%2520convergence%2520rate%2520of%2520AC%2520algorithms%250Aunder%2520Markovian%2520noise%252C%2520the%2520impacts%2520of%2520momentum%2520on%2520AC%2520algorithms%2520remain%2520largely%250Aunexplored.%2520In%2520this%2520work%252C%2520we%2520first%2520propose%2520a%2520heavy-ball%2520momentum%2520based%250Aadvantage%2520actor-critic%2520%2528%255Cmbox%257BHB-A2C%257D%2529%2520algorithm%2520by%2520integrating%2520the%2520heavy-ball%250Amomentum%2520into%2520the%2520critic%2520recursion%2520that%2520is%2520parameterized%2520by%2520a%2520linear%2520function.%250AWhen%2520the%2520sample%2520trajectory%2520follows%2520a%2520Markov%2520decision%2520process%252C%2520we%2520quantitatively%250Acertify%2520the%2520acceleration%2520capability%2520of%2520the%2520proposed%2520HB-A2C%2520algorithm.%2520Our%250Atheoretical%2520results%2520demonstrate%2520that%2520the%2520proposed%2520HB-A2C%2520finds%2520an%250A%2524%255Cepsilon%2524-approximate%2520stationary%2520point%2520with%2520%2524%255Coo%257B%255Cepsilon%255E%257B-2%257D%257D%2524%2520iterations%250Afor%2520reinforcement%2520learning%2520tasks%2520with%2520Markovian%2520noise.%2520Moreover%252C%2520we%2520also%2520reveal%250Athe%2520dependence%2520of%2520learning%2520rates%2520on%2520the%2520length%2520of%2520the%2520sample%2520trajectory.%2520By%250Acarefully%2520selecting%2520the%2520momentum%2520factor%2520of%2520the%2520critic%2520recursion%252C%2520the%2520proposed%250AHB-A2C%2520can%2520balance%2520the%2520errors%2520introduced%2520by%2520the%2520initialization%2520and%2520the%250Astoschastic%2520approximation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heavy-Ball%20Momentum%20Accelerated%20Actor-Critic%20With%20Function%20Approximation&entry.906535625=Yanjie%20Dong%20and%20Haijun%20Zhang%20and%20Gang%20Wang%20and%20Shisheng%20Cui%20and%20Xiping%20Hu&entry.1292438233=%20%20By%20using%20an%20parametric%20value%20function%20to%20replace%20the%20Monte-Carlo%20rollouts%20for%0Avalue%20estimation%2C%20the%20actor-critic%20%28AC%29%20algorithms%20can%20reduce%20the%20variance%20of%0Astochastic%20policy%20gradient%20so%20that%20to%20improve%20the%20convergence%20rate.%20While%0Aexisting%20works%20mainly%20focus%20on%20analyzing%20convergence%20rate%20of%20AC%20algorithms%0Aunder%20Markovian%20noise%2C%20the%20impacts%20of%20momentum%20on%20AC%20algorithms%20remain%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20first%20propose%20a%20heavy-ball%20momentum%20based%0Aadvantage%20actor-critic%20%28%5Cmbox%7BHB-A2C%7D%29%20algorithm%20by%20integrating%20the%20heavy-ball%0Amomentum%20into%20the%20critic%20recursion%20that%20is%20parameterized%20by%20a%20linear%20function.%0AWhen%20the%20sample%20trajectory%20follows%20a%20Markov%20decision%20process%2C%20we%20quantitatively%0Acertify%20the%20acceleration%20capability%20of%20the%20proposed%20HB-A2C%20algorithm.%20Our%0Atheoretical%20results%20demonstrate%20that%20the%20proposed%20HB-A2C%20finds%20an%0A%24%5Cepsilon%24-approximate%20stationary%20point%20with%20%24%5Coo%7B%5Cepsilon%5E%7B-2%7D%7D%24%20iterations%0Afor%20reinforcement%20learning%20tasks%20with%20Markovian%20noise.%20Moreover%2C%20we%20also%20reveal%0Athe%20dependence%20of%20learning%20rates%20on%20the%20length%20of%20the%20sample%20trajectory.%20By%0Acarefully%20selecting%20the%20momentum%20factor%20of%20the%20critic%20recursion%2C%20the%20proposed%0AHB-A2C%20can%20balance%20the%20errors%20introduced%20by%20the%20initialization%20and%20the%0Astoschastic%20approximation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06945v2&entry.124074799=Read"},
{"title": "GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent\n  Diffusion Transformer", "author": "Yihong Lin and Zhaoxin Fan and Lingyu Xiong and Liang Peng and Xiandong Li and Wenxiong Kang and Xianjia Wu and Songju Lei and Huang Xu", "abstract": "  Speech-driven talking head generation is an important but challenging task\nfor many downstream applications such as augmented reality. Existing methods\nhave achieved remarkable performance by utilizing autoregressive models or\ndiffusion models. However, most still suffer from modality inconsistencies,\nspecifically the misalignment between audio and mesh modalities, which causes\ninconsistencies in motion diversity and lip-sync accuracy. To address this\nissue, this paper introduces GLDiTalker, a novel speech-driven 3D facial\nanimation model that employs a Graph Latent Diffusion Transformer. The core\nidea behind GLDiTalker is that the audio-mesh modality misalignment can be\nresolved by diffusing the signal in a latent quantilized spatial-temporal\nspace. To achieve this, GLDiTalker builds upon a quantilized space-time\ndiffusion training pipeline, which consists of a Graph Enhanced Quantilized\nSpace Learning Stage and a Space-Time Powered Latent Diffusion Stage. The first\nstage ensures lip-sync accuracy, while the second stage enhances motion\ndiversity. Together, these stages enable GLDiTalker to generate temporally and\nspatially stable, realistic models. Extensive evaluations on several widely\nused benchmarks demonstrate that our method achieves superior performance\ncompared to existing methods.\n", "link": "http://arxiv.org/abs/2408.01826v2", "date": "2024-08-16", "relevancy": 1.8017, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6165}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5995}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLDiTalker%3A%20Speech-Driven%203D%20Facial%20Animation%20with%20Graph%20Latent%0A%20%20Diffusion%20Transformer&body=Title%3A%20GLDiTalker%3A%20Speech-Driven%203D%20Facial%20Animation%20with%20Graph%20Latent%0A%20%20Diffusion%20Transformer%0AAuthor%3A%20Yihong%20Lin%20and%20Zhaoxin%20Fan%20and%20Lingyu%20Xiong%20and%20Liang%20Peng%20and%20Xiandong%20Li%20and%20Wenxiong%20Kang%20and%20Xianjia%20Wu%20and%20Songju%20Lei%20and%20Huang%20Xu%0AAbstract%3A%20%20%20Speech-driven%20talking%20head%20generation%20is%20an%20important%20but%20challenging%20task%0Afor%20many%20downstream%20applications%20such%20as%20augmented%20reality.%20Existing%20methods%0Ahave%20achieved%20remarkable%20performance%20by%20utilizing%20autoregressive%20models%20or%0Adiffusion%20models.%20However%2C%20most%20still%20suffer%20from%20modality%20inconsistencies%2C%0Aspecifically%20the%20misalignment%20between%20audio%20and%20mesh%20modalities%2C%20which%20causes%0Ainconsistencies%20in%20motion%20diversity%20and%20lip-sync%20accuracy.%20To%20address%20this%0Aissue%2C%20this%20paper%20introduces%20GLDiTalker%2C%20a%20novel%20speech-driven%203D%20facial%0Aanimation%20model%20that%20employs%20a%20Graph%20Latent%20Diffusion%20Transformer.%20The%20core%0Aidea%20behind%20GLDiTalker%20is%20that%20the%20audio-mesh%20modality%20misalignment%20can%20be%0Aresolved%20by%20diffusing%20the%20signal%20in%20a%20latent%20quantilized%20spatial-temporal%0Aspace.%20To%20achieve%20this%2C%20GLDiTalker%20builds%20upon%20a%20quantilized%20space-time%0Adiffusion%20training%20pipeline%2C%20which%20consists%20of%20a%20Graph%20Enhanced%20Quantilized%0ASpace%20Learning%20Stage%20and%20a%20Space-Time%20Powered%20Latent%20Diffusion%20Stage.%20The%20first%0Astage%20ensures%20lip-sync%20accuracy%2C%20while%20the%20second%20stage%20enhances%20motion%0Adiversity.%20Together%2C%20these%20stages%20enable%20GLDiTalker%20to%20generate%20temporally%20and%0Aspatially%20stable%2C%20realistic%20models.%20Extensive%20evaluations%20on%20several%20widely%0Aused%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%0Acompared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLDiTalker%253A%2520Speech-Driven%25203D%2520Facial%2520Animation%2520with%2520Graph%2520Latent%250A%2520%2520Diffusion%2520Transformer%26entry.906535625%3DYihong%2520Lin%2520and%2520Zhaoxin%2520Fan%2520and%2520Lingyu%2520Xiong%2520and%2520Liang%2520Peng%2520and%2520Xiandong%2520Li%2520and%2520Wenxiong%2520Kang%2520and%2520Xianjia%2520Wu%2520and%2520Songju%2520Lei%2520and%2520Huang%2520Xu%26entry.1292438233%3D%2520%2520Speech-driven%2520talking%2520head%2520generation%2520is%2520an%2520important%2520but%2520challenging%2520task%250Afor%2520many%2520downstream%2520applications%2520such%2520as%2520augmented%2520reality.%2520Existing%2520methods%250Ahave%2520achieved%2520remarkable%2520performance%2520by%2520utilizing%2520autoregressive%2520models%2520or%250Adiffusion%2520models.%2520However%252C%2520most%2520still%2520suffer%2520from%2520modality%2520inconsistencies%252C%250Aspecifically%2520the%2520misalignment%2520between%2520audio%2520and%2520mesh%2520modalities%252C%2520which%2520causes%250Ainconsistencies%2520in%2520motion%2520diversity%2520and%2520lip-sync%2520accuracy.%2520To%2520address%2520this%250Aissue%252C%2520this%2520paper%2520introduces%2520GLDiTalker%252C%2520a%2520novel%2520speech-driven%25203D%2520facial%250Aanimation%2520model%2520that%2520employs%2520a%2520Graph%2520Latent%2520Diffusion%2520Transformer.%2520The%2520core%250Aidea%2520behind%2520GLDiTalker%2520is%2520that%2520the%2520audio-mesh%2520modality%2520misalignment%2520can%2520be%250Aresolved%2520by%2520diffusing%2520the%2520signal%2520in%2520a%2520latent%2520quantilized%2520spatial-temporal%250Aspace.%2520To%2520achieve%2520this%252C%2520GLDiTalker%2520builds%2520upon%2520a%2520quantilized%2520space-time%250Adiffusion%2520training%2520pipeline%252C%2520which%2520consists%2520of%2520a%2520Graph%2520Enhanced%2520Quantilized%250ASpace%2520Learning%2520Stage%2520and%2520a%2520Space-Time%2520Powered%2520Latent%2520Diffusion%2520Stage.%2520The%2520first%250Astage%2520ensures%2520lip-sync%2520accuracy%252C%2520while%2520the%2520second%2520stage%2520enhances%2520motion%250Adiversity.%2520Together%252C%2520these%2520stages%2520enable%2520GLDiTalker%2520to%2520generate%2520temporally%2520and%250Aspatially%2520stable%252C%2520realistic%2520models.%2520Extensive%2520evaluations%2520on%2520several%2520widely%250Aused%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance%250Acompared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLDiTalker%3A%20Speech-Driven%203D%20Facial%20Animation%20with%20Graph%20Latent%0A%20%20Diffusion%20Transformer&entry.906535625=Yihong%20Lin%20and%20Zhaoxin%20Fan%20and%20Lingyu%20Xiong%20and%20Liang%20Peng%20and%20Xiandong%20Li%20and%20Wenxiong%20Kang%20and%20Xianjia%20Wu%20and%20Songju%20Lei%20and%20Huang%20Xu&entry.1292438233=%20%20Speech-driven%20talking%20head%20generation%20is%20an%20important%20but%20challenging%20task%0Afor%20many%20downstream%20applications%20such%20as%20augmented%20reality.%20Existing%20methods%0Ahave%20achieved%20remarkable%20performance%20by%20utilizing%20autoregressive%20models%20or%0Adiffusion%20models.%20However%2C%20most%20still%20suffer%20from%20modality%20inconsistencies%2C%0Aspecifically%20the%20misalignment%20between%20audio%20and%20mesh%20modalities%2C%20which%20causes%0Ainconsistencies%20in%20motion%20diversity%20and%20lip-sync%20accuracy.%20To%20address%20this%0Aissue%2C%20this%20paper%20introduces%20GLDiTalker%2C%20a%20novel%20speech-driven%203D%20facial%0Aanimation%20model%20that%20employs%20a%20Graph%20Latent%20Diffusion%20Transformer.%20The%20core%0Aidea%20behind%20GLDiTalker%20is%20that%20the%20audio-mesh%20modality%20misalignment%20can%20be%0Aresolved%20by%20diffusing%20the%20signal%20in%20a%20latent%20quantilized%20spatial-temporal%0Aspace.%20To%20achieve%20this%2C%20GLDiTalker%20builds%20upon%20a%20quantilized%20space-time%0Adiffusion%20training%20pipeline%2C%20which%20consists%20of%20a%20Graph%20Enhanced%20Quantilized%0ASpace%20Learning%20Stage%20and%20a%20Space-Time%20Powered%20Latent%20Diffusion%20Stage.%20The%20first%0Astage%20ensures%20lip-sync%20accuracy%2C%20while%20the%20second%20stage%20enhances%20motion%0Adiversity.%20Together%2C%20these%20stages%20enable%20GLDiTalker%20to%20generate%20temporally%20and%0Aspatially%20stable%2C%20realistic%20models.%20Extensive%20evaluations%20on%20several%20widely%0Aused%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%0Acompared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01826v2&entry.124074799=Read"},
{"title": "Can Large Language Models Improve the Adversarial Robustness of Graph\n  Neural Networks?", "author": "Zhongjian Zhang and Xiao Wang and Huichi Zhou and Yue Yu and Mengmei Zhang and Cheng Yang and Chuan Shi", "abstract": "  Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nespecially for topology attacks, and many methods that improve the robustness\nof GNNs have received considerable attention. Recently, we have witnessed the\nsignificant success of large language models (LLMs), leading many to explore\nthe great potential of LLMs on GNNs. However, they mainly focus on improving\nthe performance of GNNs by utilizing LLMs to enhance the node features.\nTherefore, we ask: Will the robustness of GNNs also be enhanced with the\npowerful understanding and inference capabilities of LLMs? By presenting the\nempirical results, we find that despite that LLMs can improve the robustness of\nGNNs, there is still an average decrease of 23.1% in accuracy, implying that\nthe GNNs remain extremely vulnerable against topology attack. Therefore,\nanother question is how to extend the capabilities of LLMs on graph adversarial\nrobustness. In this paper, we propose an LLM-based robust graph structure\ninference framework, LLM4RGNN, which distills the inference capabilities of\nGPT-4 into a local LLM for identifying malicious edges and an LM-based edge\npredictor for finding missing important edges, so as to recover a robust graph\nstructure. Extensive experiments demonstrate that LLM4RGNN consistently\nimproves the robustness across various GNNs. Even in some cases where the\nperturbation ratio increases to 40%, the accuracy of GNNs is still better than\nthat on the clean graph.\n", "link": "http://arxiv.org/abs/2408.08685v1", "date": "2024-08-16", "relevancy": 1.801, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4612}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4437}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Improve%20the%20Adversarial%20Robustness%20of%20Graph%0A%20%20Neural%20Networks%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Improve%20the%20Adversarial%20Robustness%20of%20Graph%0A%20%20Neural%20Networks%3F%0AAuthor%3A%20Zhongjian%20Zhang%20and%20Xiao%20Wang%20and%20Huichi%20Zhou%20and%20Yue%20Yu%20and%20Mengmei%20Zhang%20and%20Cheng%20Yang%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20vulnerable%20to%20adversarial%20perturbations%2C%0Aespecially%20for%20topology%20attacks%2C%20and%20many%20methods%20that%20improve%20the%20robustness%0Aof%20GNNs%20have%20received%20considerable%20attention.%20Recently%2C%20we%20have%20witnessed%20the%0Asignificant%20success%20of%20large%20language%20models%20%28LLMs%29%2C%20leading%20many%20to%20explore%0Athe%20great%20potential%20of%20LLMs%20on%20GNNs.%20However%2C%20they%20mainly%20focus%20on%20improving%0Athe%20performance%20of%20GNNs%20by%20utilizing%20LLMs%20to%20enhance%20the%20node%20features.%0ATherefore%2C%20we%20ask%3A%20Will%20the%20robustness%20of%20GNNs%20also%20be%20enhanced%20with%20the%0Apowerful%20understanding%20and%20inference%20capabilities%20of%20LLMs%3F%20By%20presenting%20the%0Aempirical%20results%2C%20we%20find%20that%20despite%20that%20LLMs%20can%20improve%20the%20robustness%20of%0AGNNs%2C%20there%20is%20still%20an%20average%20decrease%20of%2023.1%25%20in%20accuracy%2C%20implying%20that%0Athe%20GNNs%20remain%20extremely%20vulnerable%20against%20topology%20attack.%20Therefore%2C%0Aanother%20question%20is%20how%20to%20extend%20the%20capabilities%20of%20LLMs%20on%20graph%20adversarial%0Arobustness.%20In%20this%20paper%2C%20we%20propose%20an%20LLM-based%20robust%20graph%20structure%0Ainference%20framework%2C%20LLM4RGNN%2C%20which%20distills%20the%20inference%20capabilities%20of%0AGPT-4%20into%20a%20local%20LLM%20for%20identifying%20malicious%20edges%20and%20an%20LM-based%20edge%0Apredictor%20for%20finding%20missing%20important%20edges%2C%20so%20as%20to%20recover%20a%20robust%20graph%0Astructure.%20Extensive%20experiments%20demonstrate%20that%20LLM4RGNN%20consistently%0Aimproves%20the%20robustness%20across%20various%20GNNs.%20Even%20in%20some%20cases%20where%20the%0Aperturbation%20ratio%20increases%20to%2040%25%2C%20the%20accuracy%20of%20GNNs%20is%20still%20better%20than%0Athat%20on%20the%20clean%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Improve%2520the%2520Adversarial%2520Robustness%2520of%2520Graph%250A%2520%2520Neural%2520Networks%253F%26entry.906535625%3DZhongjian%2520Zhang%2520and%2520Xiao%2520Wang%2520and%2520Huichi%2520Zhou%2520and%2520Yue%2520Yu%2520and%2520Mengmei%2520Zhang%2520and%2520Cheng%2520Yang%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520vulnerable%2520to%2520adversarial%2520perturbations%252C%250Aespecially%2520for%2520topology%2520attacks%252C%2520and%2520many%2520methods%2520that%2520improve%2520the%2520robustness%250Aof%2520GNNs%2520have%2520received%2520considerable%2520attention.%2520Recently%252C%2520we%2520have%2520witnessed%2520the%250Asignificant%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520leading%2520many%2520to%2520explore%250Athe%2520great%2520potential%2520of%2520LLMs%2520on%2520GNNs.%2520However%252C%2520they%2520mainly%2520focus%2520on%2520improving%250Athe%2520performance%2520of%2520GNNs%2520by%2520utilizing%2520LLMs%2520to%2520enhance%2520the%2520node%2520features.%250ATherefore%252C%2520we%2520ask%253A%2520Will%2520the%2520robustness%2520of%2520GNNs%2520also%2520be%2520enhanced%2520with%2520the%250Apowerful%2520understanding%2520and%2520inference%2520capabilities%2520of%2520LLMs%253F%2520By%2520presenting%2520the%250Aempirical%2520results%252C%2520we%2520find%2520that%2520despite%2520that%2520LLMs%2520can%2520improve%2520the%2520robustness%2520of%250AGNNs%252C%2520there%2520is%2520still%2520an%2520average%2520decrease%2520of%252023.1%2525%2520in%2520accuracy%252C%2520implying%2520that%250Athe%2520GNNs%2520remain%2520extremely%2520vulnerable%2520against%2520topology%2520attack.%2520Therefore%252C%250Aanother%2520question%2520is%2520how%2520to%2520extend%2520the%2520capabilities%2520of%2520LLMs%2520on%2520graph%2520adversarial%250Arobustness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520LLM-based%2520robust%2520graph%2520structure%250Ainference%2520framework%252C%2520LLM4RGNN%252C%2520which%2520distills%2520the%2520inference%2520capabilities%2520of%250AGPT-4%2520into%2520a%2520local%2520LLM%2520for%2520identifying%2520malicious%2520edges%2520and%2520an%2520LM-based%2520edge%250Apredictor%2520for%2520finding%2520missing%2520important%2520edges%252C%2520so%2520as%2520to%2520recover%2520a%2520robust%2520graph%250Astructure.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LLM4RGNN%2520consistently%250Aimproves%2520the%2520robustness%2520across%2520various%2520GNNs.%2520Even%2520in%2520some%2520cases%2520where%2520the%250Aperturbation%2520ratio%2520increases%2520to%252040%2525%252C%2520the%2520accuracy%2520of%2520GNNs%2520is%2520still%2520better%2520than%250Athat%2520on%2520the%2520clean%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Improve%20the%20Adversarial%20Robustness%20of%20Graph%0A%20%20Neural%20Networks%3F&entry.906535625=Zhongjian%20Zhang%20and%20Xiao%20Wang%20and%20Huichi%20Zhou%20and%20Yue%20Yu%20and%20Mengmei%20Zhang%20and%20Cheng%20Yang%20and%20Chuan%20Shi&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20vulnerable%20to%20adversarial%20perturbations%2C%0Aespecially%20for%20topology%20attacks%2C%20and%20many%20methods%20that%20improve%20the%20robustness%0Aof%20GNNs%20have%20received%20considerable%20attention.%20Recently%2C%20we%20have%20witnessed%20the%0Asignificant%20success%20of%20large%20language%20models%20%28LLMs%29%2C%20leading%20many%20to%20explore%0Athe%20great%20potential%20of%20LLMs%20on%20GNNs.%20However%2C%20they%20mainly%20focus%20on%20improving%0Athe%20performance%20of%20GNNs%20by%20utilizing%20LLMs%20to%20enhance%20the%20node%20features.%0ATherefore%2C%20we%20ask%3A%20Will%20the%20robustness%20of%20GNNs%20also%20be%20enhanced%20with%20the%0Apowerful%20understanding%20and%20inference%20capabilities%20of%20LLMs%3F%20By%20presenting%20the%0Aempirical%20results%2C%20we%20find%20that%20despite%20that%20LLMs%20can%20improve%20the%20robustness%20of%0AGNNs%2C%20there%20is%20still%20an%20average%20decrease%20of%2023.1%25%20in%20accuracy%2C%20implying%20that%0Athe%20GNNs%20remain%20extremely%20vulnerable%20against%20topology%20attack.%20Therefore%2C%0Aanother%20question%20is%20how%20to%20extend%20the%20capabilities%20of%20LLMs%20on%20graph%20adversarial%0Arobustness.%20In%20this%20paper%2C%20we%20propose%20an%20LLM-based%20robust%20graph%20structure%0Ainference%20framework%2C%20LLM4RGNN%2C%20which%20distills%20the%20inference%20capabilities%20of%0AGPT-4%20into%20a%20local%20LLM%20for%20identifying%20malicious%20edges%20and%20an%20LM-based%20edge%0Apredictor%20for%20finding%20missing%20important%20edges%2C%20so%20as%20to%20recover%20a%20robust%20graph%0Astructure.%20Extensive%20experiments%20demonstrate%20that%20LLM4RGNN%20consistently%0Aimproves%20the%20robustness%20across%20various%20GNNs.%20Even%20in%20some%20cases%20where%20the%0Aperturbation%20ratio%20increases%20to%2040%25%2C%20the%20accuracy%20of%20GNNs%20is%20still%20better%20than%0Athat%20on%20the%20clean%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08685v1&entry.124074799=Read"},
{"title": "A Multivocal Literature Review on Privacy and Fairness in Federated\n  Learning", "author": "Beatrice Balbierer and Lukas Heinlein and Domenique Zipperling and Niklas K\u00fchl", "abstract": "  Federated Learning presents a way to revolutionize AI applications by\neliminating the necessity for data sharing. Yet, research has shown that\ninformation can still be extracted during training, making additional\nprivacy-preserving measures such as differential privacy imperative. To\nimplement real-world federated learning applications, fairness, ranging from a\nfair distribution of performance to non-discriminative behaviour, must be\nconsidered. Particularly in high-risk applications (e.g. healthcare), avoiding\nthe repetition of past discriminatory errors is paramount. As recent research\nhas demonstrated an inherent tension between privacy and fairness, we conduct a\nmultivocal literature review to examine the current methods to integrate\nprivacy and fairness in federated learning. Our analyses illustrate that the\nrelationship between privacy and fairness has been neglected, posing a critical\nrisk for real-world applications. We highlight the need to explore the\nrelationship between privacy, fairness, and performance, advocating for the\ncreation of integrated federated learning frameworks.\n", "link": "http://arxiv.org/abs/2408.08666v1", "date": "2024-08-16", "relevancy": 1.782, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4468}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multivocal%20Literature%20Review%20on%20Privacy%20and%20Fairness%20in%20Federated%0A%20%20Learning&body=Title%3A%20A%20Multivocal%20Literature%20Review%20on%20Privacy%20and%20Fairness%20in%20Federated%0A%20%20Learning%0AAuthor%3A%20Beatrice%20Balbierer%20and%20Lukas%20Heinlein%20and%20Domenique%20Zipperling%20and%20Niklas%20K%C3%BChl%0AAbstract%3A%20%20%20Federated%20Learning%20presents%20a%20way%20to%20revolutionize%20AI%20applications%20by%0Aeliminating%20the%20necessity%20for%20data%20sharing.%20Yet%2C%20research%20has%20shown%20that%0Ainformation%20can%20still%20be%20extracted%20during%20training%2C%20making%20additional%0Aprivacy-preserving%20measures%20such%20as%20differential%20privacy%20imperative.%20To%0Aimplement%20real-world%20federated%20learning%20applications%2C%20fairness%2C%20ranging%20from%20a%0Afair%20distribution%20of%20performance%20to%20non-discriminative%20behaviour%2C%20must%20be%0Aconsidered.%20Particularly%20in%20high-risk%20applications%20%28e.g.%20healthcare%29%2C%20avoiding%0Athe%20repetition%20of%20past%20discriminatory%20errors%20is%20paramount.%20As%20recent%20research%0Ahas%20demonstrated%20an%20inherent%20tension%20between%20privacy%20and%20fairness%2C%20we%20conduct%20a%0Amultivocal%20literature%20review%20to%20examine%20the%20current%20methods%20to%20integrate%0Aprivacy%20and%20fairness%20in%20federated%20learning.%20Our%20analyses%20illustrate%20that%20the%0Arelationship%20between%20privacy%20and%20fairness%20has%20been%20neglected%2C%20posing%20a%20critical%0Arisk%20for%20real-world%20applications.%20We%20highlight%20the%20need%20to%20explore%20the%0Arelationship%20between%20privacy%2C%20fairness%2C%20and%20performance%2C%20advocating%20for%20the%0Acreation%20of%20integrated%20federated%20learning%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multivocal%2520Literature%2520Review%2520on%2520Privacy%2520and%2520Fairness%2520in%2520Federated%250A%2520%2520Learning%26entry.906535625%3DBeatrice%2520Balbierer%2520and%2520Lukas%2520Heinlein%2520and%2520Domenique%2520Zipperling%2520and%2520Niklas%2520K%25C3%25BChl%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520presents%2520a%2520way%2520to%2520revolutionize%2520AI%2520applications%2520by%250Aeliminating%2520the%2520necessity%2520for%2520data%2520sharing.%2520Yet%252C%2520research%2520has%2520shown%2520that%250Ainformation%2520can%2520still%2520be%2520extracted%2520during%2520training%252C%2520making%2520additional%250Aprivacy-preserving%2520measures%2520such%2520as%2520differential%2520privacy%2520imperative.%2520To%250Aimplement%2520real-world%2520federated%2520learning%2520applications%252C%2520fairness%252C%2520ranging%2520from%2520a%250Afair%2520distribution%2520of%2520performance%2520to%2520non-discriminative%2520behaviour%252C%2520must%2520be%250Aconsidered.%2520Particularly%2520in%2520high-risk%2520applications%2520%2528e.g.%2520healthcare%2529%252C%2520avoiding%250Athe%2520repetition%2520of%2520past%2520discriminatory%2520errors%2520is%2520paramount.%2520As%2520recent%2520research%250Ahas%2520demonstrated%2520an%2520inherent%2520tension%2520between%2520privacy%2520and%2520fairness%252C%2520we%2520conduct%2520a%250Amultivocal%2520literature%2520review%2520to%2520examine%2520the%2520current%2520methods%2520to%2520integrate%250Aprivacy%2520and%2520fairness%2520in%2520federated%2520learning.%2520Our%2520analyses%2520illustrate%2520that%2520the%250Arelationship%2520between%2520privacy%2520and%2520fairness%2520has%2520been%2520neglected%252C%2520posing%2520a%2520critical%250Arisk%2520for%2520real-world%2520applications.%2520We%2520highlight%2520the%2520need%2520to%2520explore%2520the%250Arelationship%2520between%2520privacy%252C%2520fairness%252C%2520and%2520performance%252C%2520advocating%2520for%2520the%250Acreation%2520of%2520integrated%2520federated%2520learning%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multivocal%20Literature%20Review%20on%20Privacy%20and%20Fairness%20in%20Federated%0A%20%20Learning&entry.906535625=Beatrice%20Balbierer%20and%20Lukas%20Heinlein%20and%20Domenique%20Zipperling%20and%20Niklas%20K%C3%BChl&entry.1292438233=%20%20Federated%20Learning%20presents%20a%20way%20to%20revolutionize%20AI%20applications%20by%0Aeliminating%20the%20necessity%20for%20data%20sharing.%20Yet%2C%20research%20has%20shown%20that%0Ainformation%20can%20still%20be%20extracted%20during%20training%2C%20making%20additional%0Aprivacy-preserving%20measures%20such%20as%20differential%20privacy%20imperative.%20To%0Aimplement%20real-world%20federated%20learning%20applications%2C%20fairness%2C%20ranging%20from%20a%0Afair%20distribution%20of%20performance%20to%20non-discriminative%20behaviour%2C%20must%20be%0Aconsidered.%20Particularly%20in%20high-risk%20applications%20%28e.g.%20healthcare%29%2C%20avoiding%0Athe%20repetition%20of%20past%20discriminatory%20errors%20is%20paramount.%20As%20recent%20research%0Ahas%20demonstrated%20an%20inherent%20tension%20between%20privacy%20and%20fairness%2C%20we%20conduct%20a%0Amultivocal%20literature%20review%20to%20examine%20the%20current%20methods%20to%20integrate%0Aprivacy%20and%20fairness%20in%20federated%20learning.%20Our%20analyses%20illustrate%20that%20the%0Arelationship%20between%20privacy%20and%20fairness%20has%20been%20neglected%2C%20posing%20a%20critical%0Arisk%20for%20real-world%20applications.%20We%20highlight%20the%20need%20to%20explore%20the%0Arelationship%20between%20privacy%2C%20fairness%2C%20and%20performance%2C%20advocating%20for%20the%0Acreation%20of%20integrated%20federated%20learning%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08666v1&entry.124074799=Read"},
{"title": "Comparative Analysis of Generative Models: Enhancing Image Synthesis\n  with VAEs, GANs, and Stable Diffusion", "author": "Sanchayan Vivekananthan", "abstract": "  This paper examines three major generative modelling frameworks: Variational\nAutoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable\nDiffusion models. VAEs are effective at learning latent representations but\nfrequently yield blurry results. GANs can generate realistic images but face\nissues such as mode collapse. Stable Diffusion models, while producing\nhigh-quality images with strong semantic coherence, are demanding in terms of\ncomputational resources. Additionally, the paper explores how incorporating\nGrounding DINO and Grounded SAM with Stable Diffusion improves image accuracy\nby utilising sophisticated segmentation and inpainting techniques. The analysis\nguides on selecting suitable models for various applications and highlights\nareas for further research.\n", "link": "http://arxiv.org/abs/2408.08751v1", "date": "2024-08-16", "relevancy": 1.7689, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6075}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5901}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20Generative%20Models%3A%20Enhancing%20Image%20Synthesis%0A%20%20with%20VAEs%2C%20GANs%2C%20and%20Stable%20Diffusion&body=Title%3A%20Comparative%20Analysis%20of%20Generative%20Models%3A%20Enhancing%20Image%20Synthesis%0A%20%20with%20VAEs%2C%20GANs%2C%20and%20Stable%20Diffusion%0AAuthor%3A%20Sanchayan%20Vivekananthan%0AAbstract%3A%20%20%20This%20paper%20examines%20three%20major%20generative%20modelling%20frameworks%3A%20Variational%0AAutoencoders%20%28VAEs%29%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20and%20Stable%0ADiffusion%20models.%20VAEs%20are%20effective%20at%20learning%20latent%20representations%20but%0Afrequently%20yield%20blurry%20results.%20GANs%20can%20generate%20realistic%20images%20but%20face%0Aissues%20such%20as%20mode%20collapse.%20Stable%20Diffusion%20models%2C%20while%20producing%0Ahigh-quality%20images%20with%20strong%20semantic%20coherence%2C%20are%20demanding%20in%20terms%20of%0Acomputational%20resources.%20Additionally%2C%20the%20paper%20explores%20how%20incorporating%0AGrounding%20DINO%20and%20Grounded%20SAM%20with%20Stable%20Diffusion%20improves%20image%20accuracy%0Aby%20utilising%20sophisticated%20segmentation%20and%20inpainting%20techniques.%20The%20analysis%0Aguides%20on%20selecting%20suitable%20models%20for%20various%20applications%20and%20highlights%0Aareas%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520Generative%2520Models%253A%2520Enhancing%2520Image%2520Synthesis%250A%2520%2520with%2520VAEs%252C%2520GANs%252C%2520and%2520Stable%2520Diffusion%26entry.906535625%3DSanchayan%2520Vivekananthan%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520three%2520major%2520generative%2520modelling%2520frameworks%253A%2520Variational%250AAutoencoders%2520%2528VAEs%2529%252C%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%252C%2520and%2520Stable%250ADiffusion%2520models.%2520VAEs%2520are%2520effective%2520at%2520learning%2520latent%2520representations%2520but%250Afrequently%2520yield%2520blurry%2520results.%2520GANs%2520can%2520generate%2520realistic%2520images%2520but%2520face%250Aissues%2520such%2520as%2520mode%2520collapse.%2520Stable%2520Diffusion%2520models%252C%2520while%2520producing%250Ahigh-quality%2520images%2520with%2520strong%2520semantic%2520coherence%252C%2520are%2520demanding%2520in%2520terms%2520of%250Acomputational%2520resources.%2520Additionally%252C%2520the%2520paper%2520explores%2520how%2520incorporating%250AGrounding%2520DINO%2520and%2520Grounded%2520SAM%2520with%2520Stable%2520Diffusion%2520improves%2520image%2520accuracy%250Aby%2520utilising%2520sophisticated%2520segmentation%2520and%2520inpainting%2520techniques.%2520The%2520analysis%250Aguides%2520on%2520selecting%2520suitable%2520models%2520for%2520various%2520applications%2520and%2520highlights%250Aareas%2520for%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20Generative%20Models%3A%20Enhancing%20Image%20Synthesis%0A%20%20with%20VAEs%2C%20GANs%2C%20and%20Stable%20Diffusion&entry.906535625=Sanchayan%20Vivekananthan&entry.1292438233=%20%20This%20paper%20examines%20three%20major%20generative%20modelling%20frameworks%3A%20Variational%0AAutoencoders%20%28VAEs%29%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20and%20Stable%0ADiffusion%20models.%20VAEs%20are%20effective%20at%20learning%20latent%20representations%20but%0Afrequently%20yield%20blurry%20results.%20GANs%20can%20generate%20realistic%20images%20but%20face%0Aissues%20such%20as%20mode%20collapse.%20Stable%20Diffusion%20models%2C%20while%20producing%0Ahigh-quality%20images%20with%20strong%20semantic%20coherence%2C%20are%20demanding%20in%20terms%20of%0Acomputational%20resources.%20Additionally%2C%20the%20paper%20explores%20how%20incorporating%0AGrounding%20DINO%20and%20Grounded%20SAM%20with%20Stable%20Diffusion%20improves%20image%20accuracy%0Aby%20utilising%20sophisticated%20segmentation%20and%20inpainting%20techniques.%20The%20analysis%0Aguides%20on%20selecting%20suitable%20models%20for%20various%20applications%20and%20highlights%0Aareas%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08751v1&entry.124074799=Read"},
{"title": "Decoupling Feature Representations of Ego and Other Modalities for\n  Incomplete Multi-modal Brain Tumor Segmentation", "author": "Kaixiang Yang and Wenqi Shan and Xudong Li and Xuan Wang and Xikai Yang and Xi Wang and Pheng-Ann Heng and Qiang Li and Zhiwei Wang", "abstract": "  Multi-modal brain tumor segmentation typically involves four magnetic\nresonance imaging (MRI) modalities, while incomplete modalities significantly\ndegrade performance. Existing solutions employ explicit or implicit modality\nadaptation, aligning features across modalities or learning a fused feature\nrobust to modality incompleteness. They share a common goal of encouraging each\nmodality to express both itself and the others. However, the two expression\nabilities are entangled as a whole in a seamless feature space, resulting in\nprohibitive learning burdens. In this paper, we propose DeMoSeg to enhance the\nmodality adaptation by Decoupling the task of representing the ego and other\nModalities for robust incomplete multi-modal Segmentation. The decoupling is\nsuper lightweight by simply using two convolutions to map each modality onto\nfour feature sub-spaces. The first sub-space expresses itself (Self-feature),\nwhile the remaining sub-spaces substitute for other modalities\n(Mutual-features). The Self- and Mutual-features interactively guide each other\nthrough a carefully-designed Channel-wised Sparse Self-Attention (CSSA). After\nthat, a Radiologist-mimic Cross-modality expression Relationships (RCR) is\nintroduced to have available modalities provide Self-feature and also `lend'\ntheir Mutual-features to compensate for the absent ones by exploiting the\nclinical prior knowledge. The benchmark results on BraTS2020, BraTS2018 and\nBraTS2015 verify the DeMoSeg's superiority thanks to the alleviated modality\nadaptation difficulty. Concretely, for BraTS2020, DeMoSeg increases Dice by at\nleast 0.92%, 2.95% and 4.95% on whole tumor, tumor core and enhanced tumor\nregions, respectively, compared to other state-of-the-arts. Codes are at\nhttps://github.com/kk42yy/DeMoSeg\n", "link": "http://arxiv.org/abs/2408.08708v1", "date": "2024-08-16", "relevancy": 1.6956, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Feature%20Representations%20of%20Ego%20and%20Other%20Modalities%20for%0A%20%20Incomplete%20Multi-modal%20Brain%20Tumor%20Segmentation&body=Title%3A%20Decoupling%20Feature%20Representations%20of%20Ego%20and%20Other%20Modalities%20for%0A%20%20Incomplete%20Multi-modal%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Kaixiang%20Yang%20and%20Wenqi%20Shan%20and%20Xudong%20Li%20and%20Xuan%20Wang%20and%20Xikai%20Yang%20and%20Xi%20Wang%20and%20Pheng-Ann%20Heng%20and%20Qiang%20Li%20and%20Zhiwei%20Wang%0AAbstract%3A%20%20%20Multi-modal%20brain%20tumor%20segmentation%20typically%20involves%20four%20magnetic%0Aresonance%20imaging%20%28MRI%29%20modalities%2C%20while%20incomplete%20modalities%20significantly%0Adegrade%20performance.%20Existing%20solutions%20employ%20explicit%20or%20implicit%20modality%0Aadaptation%2C%20aligning%20features%20across%20modalities%20or%20learning%20a%20fused%20feature%0Arobust%20to%20modality%20incompleteness.%20They%20share%20a%20common%20goal%20of%20encouraging%20each%0Amodality%20to%20express%20both%20itself%20and%20the%20others.%20However%2C%20the%20two%20expression%0Aabilities%20are%20entangled%20as%20a%20whole%20in%20a%20seamless%20feature%20space%2C%20resulting%20in%0Aprohibitive%20learning%20burdens.%20In%20this%20paper%2C%20we%20propose%20DeMoSeg%20to%20enhance%20the%0Amodality%20adaptation%20by%20Decoupling%20the%20task%20of%20representing%20the%20ego%20and%20other%0AModalities%20for%20robust%20incomplete%20multi-modal%20Segmentation.%20The%20decoupling%20is%0Asuper%20lightweight%20by%20simply%20using%20two%20convolutions%20to%20map%20each%20modality%20onto%0Afour%20feature%20sub-spaces.%20The%20first%20sub-space%20expresses%20itself%20%28Self-feature%29%2C%0Awhile%20the%20remaining%20sub-spaces%20substitute%20for%20other%20modalities%0A%28Mutual-features%29.%20The%20Self-%20and%20Mutual-features%20interactively%20guide%20each%20other%0Athrough%20a%20carefully-designed%20Channel-wised%20Sparse%20Self-Attention%20%28CSSA%29.%20After%0Athat%2C%20a%20Radiologist-mimic%20Cross-modality%20expression%20Relationships%20%28RCR%29%20is%0Aintroduced%20to%20have%20available%20modalities%20provide%20Self-feature%20and%20also%20%60lend%27%0Atheir%20Mutual-features%20to%20compensate%20for%20the%20absent%20ones%20by%20exploiting%20the%0Aclinical%20prior%20knowledge.%20The%20benchmark%20results%20on%20BraTS2020%2C%20BraTS2018%20and%0ABraTS2015%20verify%20the%20DeMoSeg%27s%20superiority%20thanks%20to%20the%20alleviated%20modality%0Aadaptation%20difficulty.%20Concretely%2C%20for%20BraTS2020%2C%20DeMoSeg%20increases%20Dice%20by%20at%0Aleast%200.92%25%2C%202.95%25%20and%204.95%25%20on%20whole%20tumor%2C%20tumor%20core%20and%20enhanced%20tumor%0Aregions%2C%20respectively%2C%20compared%20to%20other%20state-of-the-arts.%20Codes%20are%20at%0Ahttps%3A//github.com/kk42yy/DeMoSeg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Feature%2520Representations%2520of%2520Ego%2520and%2520Other%2520Modalities%2520for%250A%2520%2520Incomplete%2520Multi-modal%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DKaixiang%2520Yang%2520and%2520Wenqi%2520Shan%2520and%2520Xudong%2520Li%2520and%2520Xuan%2520Wang%2520and%2520Xikai%2520Yang%2520and%2520Xi%2520Wang%2520and%2520Pheng-Ann%2520Heng%2520and%2520Qiang%2520Li%2520and%2520Zhiwei%2520Wang%26entry.1292438233%3D%2520%2520Multi-modal%2520brain%2520tumor%2520segmentation%2520typically%2520involves%2520four%2520magnetic%250Aresonance%2520imaging%2520%2528MRI%2529%2520modalities%252C%2520while%2520incomplete%2520modalities%2520significantly%250Adegrade%2520performance.%2520Existing%2520solutions%2520employ%2520explicit%2520or%2520implicit%2520modality%250Aadaptation%252C%2520aligning%2520features%2520across%2520modalities%2520or%2520learning%2520a%2520fused%2520feature%250Arobust%2520to%2520modality%2520incompleteness.%2520They%2520share%2520a%2520common%2520goal%2520of%2520encouraging%2520each%250Amodality%2520to%2520express%2520both%2520itself%2520and%2520the%2520others.%2520However%252C%2520the%2520two%2520expression%250Aabilities%2520are%2520entangled%2520as%2520a%2520whole%2520in%2520a%2520seamless%2520feature%2520space%252C%2520resulting%2520in%250Aprohibitive%2520learning%2520burdens.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DeMoSeg%2520to%2520enhance%2520the%250Amodality%2520adaptation%2520by%2520Decoupling%2520the%2520task%2520of%2520representing%2520the%2520ego%2520and%2520other%250AModalities%2520for%2520robust%2520incomplete%2520multi-modal%2520Segmentation.%2520The%2520decoupling%2520is%250Asuper%2520lightweight%2520by%2520simply%2520using%2520two%2520convolutions%2520to%2520map%2520each%2520modality%2520onto%250Afour%2520feature%2520sub-spaces.%2520The%2520first%2520sub-space%2520expresses%2520itself%2520%2528Self-feature%2529%252C%250Awhile%2520the%2520remaining%2520sub-spaces%2520substitute%2520for%2520other%2520modalities%250A%2528Mutual-features%2529.%2520The%2520Self-%2520and%2520Mutual-features%2520interactively%2520guide%2520each%2520other%250Athrough%2520a%2520carefully-designed%2520Channel-wised%2520Sparse%2520Self-Attention%2520%2528CSSA%2529.%2520After%250Athat%252C%2520a%2520Radiologist-mimic%2520Cross-modality%2520expression%2520Relationships%2520%2528RCR%2529%2520is%250Aintroduced%2520to%2520have%2520available%2520modalities%2520provide%2520Self-feature%2520and%2520also%2520%2560lend%2527%250Atheir%2520Mutual-features%2520to%2520compensate%2520for%2520the%2520absent%2520ones%2520by%2520exploiting%2520the%250Aclinical%2520prior%2520knowledge.%2520The%2520benchmark%2520results%2520on%2520BraTS2020%252C%2520BraTS2018%2520and%250ABraTS2015%2520verify%2520the%2520DeMoSeg%2527s%2520superiority%2520thanks%2520to%2520the%2520alleviated%2520modality%250Aadaptation%2520difficulty.%2520Concretely%252C%2520for%2520BraTS2020%252C%2520DeMoSeg%2520increases%2520Dice%2520by%2520at%250Aleast%25200.92%2525%252C%25202.95%2525%2520and%25204.95%2525%2520on%2520whole%2520tumor%252C%2520tumor%2520core%2520and%2520enhanced%2520tumor%250Aregions%252C%2520respectively%252C%2520compared%2520to%2520other%2520state-of-the-arts.%2520Codes%2520are%2520at%250Ahttps%253A//github.com/kk42yy/DeMoSeg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Feature%20Representations%20of%20Ego%20and%20Other%20Modalities%20for%0A%20%20Incomplete%20Multi-modal%20Brain%20Tumor%20Segmentation&entry.906535625=Kaixiang%20Yang%20and%20Wenqi%20Shan%20and%20Xudong%20Li%20and%20Xuan%20Wang%20and%20Xikai%20Yang%20and%20Xi%20Wang%20and%20Pheng-Ann%20Heng%20and%20Qiang%20Li%20and%20Zhiwei%20Wang&entry.1292438233=%20%20Multi-modal%20brain%20tumor%20segmentation%20typically%20involves%20four%20magnetic%0Aresonance%20imaging%20%28MRI%29%20modalities%2C%20while%20incomplete%20modalities%20significantly%0Adegrade%20performance.%20Existing%20solutions%20employ%20explicit%20or%20implicit%20modality%0Aadaptation%2C%20aligning%20features%20across%20modalities%20or%20learning%20a%20fused%20feature%0Arobust%20to%20modality%20incompleteness.%20They%20share%20a%20common%20goal%20of%20encouraging%20each%0Amodality%20to%20express%20both%20itself%20and%20the%20others.%20However%2C%20the%20two%20expression%0Aabilities%20are%20entangled%20as%20a%20whole%20in%20a%20seamless%20feature%20space%2C%20resulting%20in%0Aprohibitive%20learning%20burdens.%20In%20this%20paper%2C%20we%20propose%20DeMoSeg%20to%20enhance%20the%0Amodality%20adaptation%20by%20Decoupling%20the%20task%20of%20representing%20the%20ego%20and%20other%0AModalities%20for%20robust%20incomplete%20multi-modal%20Segmentation.%20The%20decoupling%20is%0Asuper%20lightweight%20by%20simply%20using%20two%20convolutions%20to%20map%20each%20modality%20onto%0Afour%20feature%20sub-spaces.%20The%20first%20sub-space%20expresses%20itself%20%28Self-feature%29%2C%0Awhile%20the%20remaining%20sub-spaces%20substitute%20for%20other%20modalities%0A%28Mutual-features%29.%20The%20Self-%20and%20Mutual-features%20interactively%20guide%20each%20other%0Athrough%20a%20carefully-designed%20Channel-wised%20Sparse%20Self-Attention%20%28CSSA%29.%20After%0Athat%2C%20a%20Radiologist-mimic%20Cross-modality%20expression%20Relationships%20%28RCR%29%20is%0Aintroduced%20to%20have%20available%20modalities%20provide%20Self-feature%20and%20also%20%60lend%27%0Atheir%20Mutual-features%20to%20compensate%20for%20the%20absent%20ones%20by%20exploiting%20the%0Aclinical%20prior%20knowledge.%20The%20benchmark%20results%20on%20BraTS2020%2C%20BraTS2018%20and%0ABraTS2015%20verify%20the%20DeMoSeg%27s%20superiority%20thanks%20to%20the%20alleviated%20modality%0Aadaptation%20difficulty.%20Concretely%2C%20for%20BraTS2020%2C%20DeMoSeg%20increases%20Dice%20by%20at%0Aleast%200.92%25%2C%202.95%25%20and%204.95%25%20on%20whole%20tumor%2C%20tumor%20core%20and%20enhanced%20tumor%0Aregions%2C%20respectively%2C%20compared%20to%20other%20state-of-the-arts.%20Codes%20are%20at%0Ahttps%3A//github.com/kk42yy/DeMoSeg%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08708v1&entry.124074799=Read"},
{"title": "Resilience in Online Federated Learning: Mitigating Model-Poisoning\n  Attacks via Partial Sharing", "author": "Ehsan Lari and Reza Arablouei and Vinay Chakravarthi Gogineni and Stefan Werner", "abstract": "  Federated learning (FL) allows training machine learning models on\ndistributed data without compromising privacy. However, FL is vulnerable to\nmodel-poisoning attacks where malicious clients tamper with their local models\nto manipulate the global model. In this work, we investigate the resilience of\nthe partial-sharing online FL (PSO-Fed) algorithm against such attacks. PSO-Fed\nreduces communication overhead by allowing clients to share only a fraction of\ntheir model updates with the server. We demonstrate that this partial sharing\nmechanism has the added advantage of enhancing PSO-Fed's robustness to\nmodel-poisoning attacks. Through theoretical analysis, we show that PSO-Fed\nmaintains convergence even under Byzantine attacks, where malicious clients\ninject noise into their updates. Furthermore, we derive a formula for PSO-Fed's\nmean square error, considering factors like stepsize, attack probability, and\nthe number of malicious clients. Interestingly, we find a non-trivial optimal\nstepsize that maximizes PSO-Fed's resistance to these attacks. Extensive\nnumerical experiments confirm our theoretical findings and showcase PSO-Fed's\nsuperior performance against model-poisoning attacks compared to other leading\nFL algorithms.\n", "link": "http://arxiv.org/abs/2403.13108v2", "date": "2024-08-16", "relevancy": 1.691, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4369}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4147}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resilience%20in%20Online%20Federated%20Learning%3A%20Mitigating%20Model-Poisoning%0A%20%20Attacks%20via%20Partial%20Sharing&body=Title%3A%20Resilience%20in%20Online%20Federated%20Learning%3A%20Mitigating%20Model-Poisoning%0A%20%20Attacks%20via%20Partial%20Sharing%0AAuthor%3A%20Ehsan%20Lari%20and%20Reza%20Arablouei%20and%20Vinay%20Chakravarthi%20Gogineni%20and%20Stefan%20Werner%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20allows%20training%20machine%20learning%20models%20on%0Adistributed%20data%20without%20compromising%20privacy.%20However%2C%20FL%20is%20vulnerable%20to%0Amodel-poisoning%20attacks%20where%20malicious%20clients%20tamper%20with%20their%20local%20models%0Ato%20manipulate%20the%20global%20model.%20In%20this%20work%2C%20we%20investigate%20the%20resilience%20of%0Athe%20partial-sharing%20online%20FL%20%28PSO-Fed%29%20algorithm%20against%20such%20attacks.%20PSO-Fed%0Areduces%20communication%20overhead%20by%20allowing%20clients%20to%20share%20only%20a%20fraction%20of%0Atheir%20model%20updates%20with%20the%20server.%20We%20demonstrate%20that%20this%20partial%20sharing%0Amechanism%20has%20the%20added%20advantage%20of%20enhancing%20PSO-Fed%27s%20robustness%20to%0Amodel-poisoning%20attacks.%20Through%20theoretical%20analysis%2C%20we%20show%20that%20PSO-Fed%0Amaintains%20convergence%20even%20under%20Byzantine%20attacks%2C%20where%20malicious%20clients%0Ainject%20noise%20into%20their%20updates.%20Furthermore%2C%20we%20derive%20a%20formula%20for%20PSO-Fed%27s%0Amean%20square%20error%2C%20considering%20factors%20like%20stepsize%2C%20attack%20probability%2C%20and%0Athe%20number%20of%20malicious%20clients.%20Interestingly%2C%20we%20find%20a%20non-trivial%20optimal%0Astepsize%20that%20maximizes%20PSO-Fed%27s%20resistance%20to%20these%20attacks.%20Extensive%0Anumerical%20experiments%20confirm%20our%20theoretical%20findings%20and%20showcase%20PSO-Fed%27s%0Asuperior%20performance%20against%20model-poisoning%20attacks%20compared%20to%20other%20leading%0AFL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResilience%2520in%2520Online%2520Federated%2520Learning%253A%2520Mitigating%2520Model-Poisoning%250A%2520%2520Attacks%2520via%2520Partial%2520Sharing%26entry.906535625%3DEhsan%2520Lari%2520and%2520Reza%2520Arablouei%2520and%2520Vinay%2520Chakravarthi%2520Gogineni%2520and%2520Stefan%2520Werner%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520allows%2520training%2520machine%2520learning%2520models%2520on%250Adistributed%2520data%2520without%2520compromising%2520privacy.%2520However%252C%2520FL%2520is%2520vulnerable%2520to%250Amodel-poisoning%2520attacks%2520where%2520malicious%2520clients%2520tamper%2520with%2520their%2520local%2520models%250Ato%2520manipulate%2520the%2520global%2520model.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520resilience%2520of%250Athe%2520partial-sharing%2520online%2520FL%2520%2528PSO-Fed%2529%2520algorithm%2520against%2520such%2520attacks.%2520PSO-Fed%250Areduces%2520communication%2520overhead%2520by%2520allowing%2520clients%2520to%2520share%2520only%2520a%2520fraction%2520of%250Atheir%2520model%2520updates%2520with%2520the%2520server.%2520We%2520demonstrate%2520that%2520this%2520partial%2520sharing%250Amechanism%2520has%2520the%2520added%2520advantage%2520of%2520enhancing%2520PSO-Fed%2527s%2520robustness%2520to%250Amodel-poisoning%2520attacks.%2520Through%2520theoretical%2520analysis%252C%2520we%2520show%2520that%2520PSO-Fed%250Amaintains%2520convergence%2520even%2520under%2520Byzantine%2520attacks%252C%2520where%2520malicious%2520clients%250Ainject%2520noise%2520into%2520their%2520updates.%2520Furthermore%252C%2520we%2520derive%2520a%2520formula%2520for%2520PSO-Fed%2527s%250Amean%2520square%2520error%252C%2520considering%2520factors%2520like%2520stepsize%252C%2520attack%2520probability%252C%2520and%250Athe%2520number%2520of%2520malicious%2520clients.%2520Interestingly%252C%2520we%2520find%2520a%2520non-trivial%2520optimal%250Astepsize%2520that%2520maximizes%2520PSO-Fed%2527s%2520resistance%2520to%2520these%2520attacks.%2520Extensive%250Anumerical%2520experiments%2520confirm%2520our%2520theoretical%2520findings%2520and%2520showcase%2520PSO-Fed%2527s%250Asuperior%2520performance%2520against%2520model-poisoning%2520attacks%2520compared%2520to%2520other%2520leading%250AFL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resilience%20in%20Online%20Federated%20Learning%3A%20Mitigating%20Model-Poisoning%0A%20%20Attacks%20via%20Partial%20Sharing&entry.906535625=Ehsan%20Lari%20and%20Reza%20Arablouei%20and%20Vinay%20Chakravarthi%20Gogineni%20and%20Stefan%20Werner&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20allows%20training%20machine%20learning%20models%20on%0Adistributed%20data%20without%20compromising%20privacy.%20However%2C%20FL%20is%20vulnerable%20to%0Amodel-poisoning%20attacks%20where%20malicious%20clients%20tamper%20with%20their%20local%20models%0Ato%20manipulate%20the%20global%20model.%20In%20this%20work%2C%20we%20investigate%20the%20resilience%20of%0Athe%20partial-sharing%20online%20FL%20%28PSO-Fed%29%20algorithm%20against%20such%20attacks.%20PSO-Fed%0Areduces%20communication%20overhead%20by%20allowing%20clients%20to%20share%20only%20a%20fraction%20of%0Atheir%20model%20updates%20with%20the%20server.%20We%20demonstrate%20that%20this%20partial%20sharing%0Amechanism%20has%20the%20added%20advantage%20of%20enhancing%20PSO-Fed%27s%20robustness%20to%0Amodel-poisoning%20attacks.%20Through%20theoretical%20analysis%2C%20we%20show%20that%20PSO-Fed%0Amaintains%20convergence%20even%20under%20Byzantine%20attacks%2C%20where%20malicious%20clients%0Ainject%20noise%20into%20their%20updates.%20Furthermore%2C%20we%20derive%20a%20formula%20for%20PSO-Fed%27s%0Amean%20square%20error%2C%20considering%20factors%20like%20stepsize%2C%20attack%20probability%2C%20and%0Athe%20number%20of%20malicious%20clients.%20Interestingly%2C%20we%20find%20a%20non-trivial%20optimal%0Astepsize%20that%20maximizes%20PSO-Fed%27s%20resistance%20to%20these%20attacks.%20Extensive%0Anumerical%20experiments%20confirm%20our%20theoretical%20findings%20and%20showcase%20PSO-Fed%27s%0Asuperior%20performance%20against%20model-poisoning%20attacks%20compared%20to%20other%20leading%0AFL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13108v2&entry.124074799=Read"},
{"title": "Explore-then-Commit Algorithms for Decentralized Two-Sided Matching\n  Markets", "author": "Tejas Pagare and Avishek Ghosh", "abstract": "  Online learning in a decentralized two-sided matching markets, where the\ndemand-side (players) compete to match with the supply-side (arms), has\nreceived substantial interest because it abstracts out the complex interactions\nin matching platforms (e.g. UpWork, TaskRabbit). However, past works assume\nthat each arm knows their preference ranking over the players (one-sided\nlearning), and each player aim to learn the preference over arms through\nsuccessive interactions. Moreover, several (impractical) assumptions on the\nproblem are usually made for theoretical tractability such as broadcast\nplayer-arm match Liu et al. (2020; 2021); Kong & Li (2023) or serial\ndictatorship Sankararaman et al. (2021); Basu et al. (2021); Ghosh et al.\n(2022). In this paper, we study a decentralized two-sided matching market,\nwhere we do not assume that the preference ranking over players are known to\nthe arms apriori. Furthermore, we do not have any structural assumptions on the\nproblem. We propose a multi-phase explore-then-commit type algorithm namely\nepoch-based CA-ETC (collision avoidance explore then commit) (\\texttt{CA-ETC}\nin short) for this problem that does not require any communication across\nagents (players and arms) and hence decentralized. We show that for the initial\nepoch length of $T_{\\circ}$ and subsequent epoch-lengths of $2^{l/\\gamma}\nT_{\\circ}$ (for the $l-$th epoch with $\\gamma \\in (0,1)$ as an input parameter\nto the algorithm), \\texttt{CA-ETC} yields a player optimal expected regret of\n$\\mathcal{O}\\left(T_{\\circ} (\\frac{K \\log T}{T_{\\circ} \\Delta^2})^{1/\\gamma} +\nT_{\\circ} (\\frac{T}{T_{\\circ}})^\\gamma\\right)$ for the $i$-th player, where $T$\nis the learning horizon, $K$ is the number of arms and $\\Delta$ is an\nappropriately defined problem gap. Furthermore, we propose a blackboard\ncommunication based baseline achieving logarithmic regret in $T$.\n", "link": "http://arxiv.org/abs/2408.08690v1", "date": "2024-08-16", "relevancy": 1.6802, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4376}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4244}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explore-then-Commit%20Algorithms%20for%20Decentralized%20Two-Sided%20Matching%0A%20%20Markets&body=Title%3A%20Explore-then-Commit%20Algorithms%20for%20Decentralized%20Two-Sided%20Matching%0A%20%20Markets%0AAuthor%3A%20Tejas%20Pagare%20and%20Avishek%20Ghosh%0AAbstract%3A%20%20%20Online%20learning%20in%20a%20decentralized%20two-sided%20matching%20markets%2C%20where%20the%0Ademand-side%20%28players%29%20compete%20to%20match%20with%20the%20supply-side%20%28arms%29%2C%20has%0Areceived%20substantial%20interest%20because%20it%20abstracts%20out%20the%20complex%20interactions%0Ain%20matching%20platforms%20%28e.g.%20UpWork%2C%20TaskRabbit%29.%20However%2C%20past%20works%20assume%0Athat%20each%20arm%20knows%20their%20preference%20ranking%20over%20the%20players%20%28one-sided%0Alearning%29%2C%20and%20each%20player%20aim%20to%20learn%20the%20preference%20over%20arms%20through%0Asuccessive%20interactions.%20Moreover%2C%20several%20%28impractical%29%20assumptions%20on%20the%0Aproblem%20are%20usually%20made%20for%20theoretical%20tractability%20such%20as%20broadcast%0Aplayer-arm%20match%20Liu%20et%20al.%20%282020%3B%202021%29%3B%20Kong%20%26%20Li%20%282023%29%20or%20serial%0Adictatorship%20Sankararaman%20et%20al.%20%282021%29%3B%20Basu%20et%20al.%20%282021%29%3B%20Ghosh%20et%20al.%0A%282022%29.%20In%20this%20paper%2C%20we%20study%20a%20decentralized%20two-sided%20matching%20market%2C%0Awhere%20we%20do%20not%20assume%20that%20the%20preference%20ranking%20over%20players%20are%20known%20to%0Athe%20arms%20apriori.%20Furthermore%2C%20we%20do%20not%20have%20any%20structural%20assumptions%20on%20the%0Aproblem.%20We%20propose%20a%20multi-phase%20explore-then-commit%20type%20algorithm%20namely%0Aepoch-based%20CA-ETC%20%28collision%20avoidance%20explore%20then%20commit%29%20%28%5Ctexttt%7BCA-ETC%7D%0Ain%20short%29%20for%20this%20problem%20that%20does%20not%20require%20any%20communication%20across%0Aagents%20%28players%20and%20arms%29%20and%20hence%20decentralized.%20We%20show%20that%20for%20the%20initial%0Aepoch%20length%20of%20%24T_%7B%5Ccirc%7D%24%20and%20subsequent%20epoch-lengths%20of%20%242%5E%7Bl/%5Cgamma%7D%0AT_%7B%5Ccirc%7D%24%20%28for%20the%20%24l-%24th%20epoch%20with%20%24%5Cgamma%20%5Cin%20%280%2C1%29%24%20as%20an%20input%20parameter%0Ato%20the%20algorithm%29%2C%20%5Ctexttt%7BCA-ETC%7D%20yields%20a%20player%20optimal%20expected%20regret%20of%0A%24%5Cmathcal%7BO%7D%5Cleft%28T_%7B%5Ccirc%7D%20%28%5Cfrac%7BK%20%5Clog%20T%7D%7BT_%7B%5Ccirc%7D%20%5CDelta%5E2%7D%29%5E%7B1/%5Cgamma%7D%20%2B%0AT_%7B%5Ccirc%7D%20%28%5Cfrac%7BT%7D%7BT_%7B%5Ccirc%7D%7D%29%5E%5Cgamma%5Cright%29%24%20for%20the%20%24i%24-th%20player%2C%20where%20%24T%24%0Ais%20the%20learning%20horizon%2C%20%24K%24%20is%20the%20number%20of%20arms%20and%20%24%5CDelta%24%20is%20an%0Aappropriately%20defined%20problem%20gap.%20Furthermore%2C%20we%20propose%20a%20blackboard%0Acommunication%20based%20baseline%20achieving%20logarithmic%20regret%20in%20%24T%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplore-then-Commit%2520Algorithms%2520for%2520Decentralized%2520Two-Sided%2520Matching%250A%2520%2520Markets%26entry.906535625%3DTejas%2520Pagare%2520and%2520Avishek%2520Ghosh%26entry.1292438233%3D%2520%2520Online%2520learning%2520in%2520a%2520decentralized%2520two-sided%2520matching%2520markets%252C%2520where%2520the%250Ademand-side%2520%2528players%2529%2520compete%2520to%2520match%2520with%2520the%2520supply-side%2520%2528arms%2529%252C%2520has%250Areceived%2520substantial%2520interest%2520because%2520it%2520abstracts%2520out%2520the%2520complex%2520interactions%250Ain%2520matching%2520platforms%2520%2528e.g.%2520UpWork%252C%2520TaskRabbit%2529.%2520However%252C%2520past%2520works%2520assume%250Athat%2520each%2520arm%2520knows%2520their%2520preference%2520ranking%2520over%2520the%2520players%2520%2528one-sided%250Alearning%2529%252C%2520and%2520each%2520player%2520aim%2520to%2520learn%2520the%2520preference%2520over%2520arms%2520through%250Asuccessive%2520interactions.%2520Moreover%252C%2520several%2520%2528impractical%2529%2520assumptions%2520on%2520the%250Aproblem%2520are%2520usually%2520made%2520for%2520theoretical%2520tractability%2520such%2520as%2520broadcast%250Aplayer-arm%2520match%2520Liu%2520et%2520al.%2520%25282020%253B%25202021%2529%253B%2520Kong%2520%2526%2520Li%2520%25282023%2529%2520or%2520serial%250Adictatorship%2520Sankararaman%2520et%2520al.%2520%25282021%2529%253B%2520Basu%2520et%2520al.%2520%25282021%2529%253B%2520Ghosh%2520et%2520al.%250A%25282022%2529.%2520In%2520this%2520paper%252C%2520we%2520study%2520a%2520decentralized%2520two-sided%2520matching%2520market%252C%250Awhere%2520we%2520do%2520not%2520assume%2520that%2520the%2520preference%2520ranking%2520over%2520players%2520are%2520known%2520to%250Athe%2520arms%2520apriori.%2520Furthermore%252C%2520we%2520do%2520not%2520have%2520any%2520structural%2520assumptions%2520on%2520the%250Aproblem.%2520We%2520propose%2520a%2520multi-phase%2520explore-then-commit%2520type%2520algorithm%2520namely%250Aepoch-based%2520CA-ETC%2520%2528collision%2520avoidance%2520explore%2520then%2520commit%2529%2520%2528%255Ctexttt%257BCA-ETC%257D%250Ain%2520short%2529%2520for%2520this%2520problem%2520that%2520does%2520not%2520require%2520any%2520communication%2520across%250Aagents%2520%2528players%2520and%2520arms%2529%2520and%2520hence%2520decentralized.%2520We%2520show%2520that%2520for%2520the%2520initial%250Aepoch%2520length%2520of%2520%2524T_%257B%255Ccirc%257D%2524%2520and%2520subsequent%2520epoch-lengths%2520of%2520%25242%255E%257Bl/%255Cgamma%257D%250AT_%257B%255Ccirc%257D%2524%2520%2528for%2520the%2520%2524l-%2524th%2520epoch%2520with%2520%2524%255Cgamma%2520%255Cin%2520%25280%252C1%2529%2524%2520as%2520an%2520input%2520parameter%250Ato%2520the%2520algorithm%2529%252C%2520%255Ctexttt%257BCA-ETC%257D%2520yields%2520a%2520player%2520optimal%2520expected%2520regret%2520of%250A%2524%255Cmathcal%257BO%257D%255Cleft%2528T_%257B%255Ccirc%257D%2520%2528%255Cfrac%257BK%2520%255Clog%2520T%257D%257BT_%257B%255Ccirc%257D%2520%255CDelta%255E2%257D%2529%255E%257B1/%255Cgamma%257D%2520%252B%250AT_%257B%255Ccirc%257D%2520%2528%255Cfrac%257BT%257D%257BT_%257B%255Ccirc%257D%257D%2529%255E%255Cgamma%255Cright%2529%2524%2520for%2520the%2520%2524i%2524-th%2520player%252C%2520where%2520%2524T%2524%250Ais%2520the%2520learning%2520horizon%252C%2520%2524K%2524%2520is%2520the%2520number%2520of%2520arms%2520and%2520%2524%255CDelta%2524%2520is%2520an%250Aappropriately%2520defined%2520problem%2520gap.%2520Furthermore%252C%2520we%2520propose%2520a%2520blackboard%250Acommunication%2520based%2520baseline%2520achieving%2520logarithmic%2520regret%2520in%2520%2524T%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explore-then-Commit%20Algorithms%20for%20Decentralized%20Two-Sided%20Matching%0A%20%20Markets&entry.906535625=Tejas%20Pagare%20and%20Avishek%20Ghosh&entry.1292438233=%20%20Online%20learning%20in%20a%20decentralized%20two-sided%20matching%20markets%2C%20where%20the%0Ademand-side%20%28players%29%20compete%20to%20match%20with%20the%20supply-side%20%28arms%29%2C%20has%0Areceived%20substantial%20interest%20because%20it%20abstracts%20out%20the%20complex%20interactions%0Ain%20matching%20platforms%20%28e.g.%20UpWork%2C%20TaskRabbit%29.%20However%2C%20past%20works%20assume%0Athat%20each%20arm%20knows%20their%20preference%20ranking%20over%20the%20players%20%28one-sided%0Alearning%29%2C%20and%20each%20player%20aim%20to%20learn%20the%20preference%20over%20arms%20through%0Asuccessive%20interactions.%20Moreover%2C%20several%20%28impractical%29%20assumptions%20on%20the%0Aproblem%20are%20usually%20made%20for%20theoretical%20tractability%20such%20as%20broadcast%0Aplayer-arm%20match%20Liu%20et%20al.%20%282020%3B%202021%29%3B%20Kong%20%26%20Li%20%282023%29%20or%20serial%0Adictatorship%20Sankararaman%20et%20al.%20%282021%29%3B%20Basu%20et%20al.%20%282021%29%3B%20Ghosh%20et%20al.%0A%282022%29.%20In%20this%20paper%2C%20we%20study%20a%20decentralized%20two-sided%20matching%20market%2C%0Awhere%20we%20do%20not%20assume%20that%20the%20preference%20ranking%20over%20players%20are%20known%20to%0Athe%20arms%20apriori.%20Furthermore%2C%20we%20do%20not%20have%20any%20structural%20assumptions%20on%20the%0Aproblem.%20We%20propose%20a%20multi-phase%20explore-then-commit%20type%20algorithm%20namely%0Aepoch-based%20CA-ETC%20%28collision%20avoidance%20explore%20then%20commit%29%20%28%5Ctexttt%7BCA-ETC%7D%0Ain%20short%29%20for%20this%20problem%20that%20does%20not%20require%20any%20communication%20across%0Aagents%20%28players%20and%20arms%29%20and%20hence%20decentralized.%20We%20show%20that%20for%20the%20initial%0Aepoch%20length%20of%20%24T_%7B%5Ccirc%7D%24%20and%20subsequent%20epoch-lengths%20of%20%242%5E%7Bl/%5Cgamma%7D%0AT_%7B%5Ccirc%7D%24%20%28for%20the%20%24l-%24th%20epoch%20with%20%24%5Cgamma%20%5Cin%20%280%2C1%29%24%20as%20an%20input%20parameter%0Ato%20the%20algorithm%29%2C%20%5Ctexttt%7BCA-ETC%7D%20yields%20a%20player%20optimal%20expected%20regret%20of%0A%24%5Cmathcal%7BO%7D%5Cleft%28T_%7B%5Ccirc%7D%20%28%5Cfrac%7BK%20%5Clog%20T%7D%7BT_%7B%5Ccirc%7D%20%5CDelta%5E2%7D%29%5E%7B1/%5Cgamma%7D%20%2B%0AT_%7B%5Ccirc%7D%20%28%5Cfrac%7BT%7D%7BT_%7B%5Ccirc%7D%7D%29%5E%5Cgamma%5Cright%29%24%20for%20the%20%24i%24-th%20player%2C%20where%20%24T%24%0Ais%20the%20learning%20horizon%2C%20%24K%24%20is%20the%20number%20of%20arms%20and%20%24%5CDelta%24%20is%20an%0Aappropriately%20defined%20problem%20gap.%20Furthermore%2C%20we%20propose%20a%20blackboard%0Acommunication%20based%20baseline%20achieving%20logarithmic%20regret%20in%20%24T%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08690v1&entry.124074799=Read"},
{"title": "Agentic Skill Discovery", "author": "Xufeng Zhao and Cornelius Weber and Stefan Wermter", "abstract": "  Language-conditioned robotic skills make it possible to apply the high-level\nreasoning of Large Language Models (LLMs) to low-level robotic control. A\nremaining challenge is to acquire a diverse set of fundamental skills. Existing\napproaches either manually decompose a complex task into atomic robotic actions\nin a top-down fashion, or bootstrap as many combinations as possible in a\nbottom-up fashion to cover a wider range of task possibilities. These\ndecompositions or combinations, however, require an initial skill library. For\nexample, a ``grasping'' capability can never emerge from a skill library\ncontaining only diverse ``pushing'' skills. Existing skill discovery techniques\nwith reinforcement learning acquire skills by an exhaustive exploration but\noften yield non-meaningful behaviors. In this study, we introduce a novel\nframework for skill discovery that is entirely driven by LLMs. The framework\nbegins with an LLM generating task proposals based on the provided scene\ndescription and the robot's configurations, aiming to incrementally acquire new\nskills upon task completion. For each proposed task, a series of reinforcement\nlearning processes are initiated, utilizing reward and success determination\nfunctions sampled by the LLM to develop the corresponding policy. The\nreliability and trustworthiness of learned behaviors are further ensured by an\nindependent vision-language model. We show that starting with zero skill, the\nskill library emerges and expands to more and more meaningful and reliable\nskills, enabling the robot to efficiently further propose and complete advanced\ntasks. Project page: \\url{https://agentic-skill-discovery.github.io}.\n", "link": "http://arxiv.org/abs/2405.15019v2", "date": "2024-08-16", "relevancy": 1.6799, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6495}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Skill%20Discovery&body=Title%3A%20Agentic%20Skill%20Discovery%0AAuthor%3A%20Xufeng%20Zhao%20and%20Cornelius%20Weber%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Language-conditioned%20robotic%20skills%20make%20it%20possible%20to%20apply%20the%20high-level%0Areasoning%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20low-level%20robotic%20control.%20A%0Aremaining%20challenge%20is%20to%20acquire%20a%20diverse%20set%20of%20fundamental%20skills.%20Existing%0Aapproaches%20either%20manually%20decompose%20a%20complex%20task%20into%20atomic%20robotic%20actions%0Ain%20a%20top-down%20fashion%2C%20or%20bootstrap%20as%20many%20combinations%20as%20possible%20in%20a%0Abottom-up%20fashion%20to%20cover%20a%20wider%20range%20of%20task%20possibilities.%20These%0Adecompositions%20or%20combinations%2C%20however%2C%20require%20an%20initial%20skill%20library.%20For%0Aexample%2C%20a%20%60%60grasping%27%27%20capability%20can%20never%20emerge%20from%20a%20skill%20library%0Acontaining%20only%20diverse%20%60%60pushing%27%27%20skills.%20Existing%20skill%20discovery%20techniques%0Awith%20reinforcement%20learning%20acquire%20skills%20by%20an%20exhaustive%20exploration%20but%0Aoften%20yield%20non-meaningful%20behaviors.%20In%20this%20study%2C%20we%20introduce%20a%20novel%0Aframework%20for%20skill%20discovery%20that%20is%20entirely%20driven%20by%20LLMs.%20The%20framework%0Abegins%20with%20an%20LLM%20generating%20task%20proposals%20based%20on%20the%20provided%20scene%0Adescription%20and%20the%20robot%27s%20configurations%2C%20aiming%20to%20incrementally%20acquire%20new%0Askills%20upon%20task%20completion.%20For%20each%20proposed%20task%2C%20a%20series%20of%20reinforcement%0Alearning%20processes%20are%20initiated%2C%20utilizing%20reward%20and%20success%20determination%0Afunctions%20sampled%20by%20the%20LLM%20to%20develop%20the%20corresponding%20policy.%20The%0Areliability%20and%20trustworthiness%20of%20learned%20behaviors%20are%20further%20ensured%20by%20an%0Aindependent%20vision-language%20model.%20We%20show%20that%20starting%20with%20zero%20skill%2C%20the%0Askill%20library%20emerges%20and%20expands%20to%20more%20and%20more%20meaningful%20and%20reliable%0Askills%2C%20enabling%20the%20robot%20to%20efficiently%20further%20propose%20and%20complete%20advanced%0Atasks.%20Project%20page%3A%20%5Curl%7Bhttps%3A//agentic-skill-discovery.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Skill%2520Discovery%26entry.906535625%3DXufeng%2520Zhao%2520and%2520Cornelius%2520Weber%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520Language-conditioned%2520robotic%2520skills%2520make%2520it%2520possible%2520to%2520apply%2520the%2520high-level%250Areasoning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520low-level%2520robotic%2520control.%2520A%250Aremaining%2520challenge%2520is%2520to%2520acquire%2520a%2520diverse%2520set%2520of%2520fundamental%2520skills.%2520Existing%250Aapproaches%2520either%2520manually%2520decompose%2520a%2520complex%2520task%2520into%2520atomic%2520robotic%2520actions%250Ain%2520a%2520top-down%2520fashion%252C%2520or%2520bootstrap%2520as%2520many%2520combinations%2520as%2520possible%2520in%2520a%250Abottom-up%2520fashion%2520to%2520cover%2520a%2520wider%2520range%2520of%2520task%2520possibilities.%2520These%250Adecompositions%2520or%2520combinations%252C%2520however%252C%2520require%2520an%2520initial%2520skill%2520library.%2520For%250Aexample%252C%2520a%2520%2560%2560grasping%2527%2527%2520capability%2520can%2520never%2520emerge%2520from%2520a%2520skill%2520library%250Acontaining%2520only%2520diverse%2520%2560%2560pushing%2527%2527%2520skills.%2520Existing%2520skill%2520discovery%2520techniques%250Awith%2520reinforcement%2520learning%2520acquire%2520skills%2520by%2520an%2520exhaustive%2520exploration%2520but%250Aoften%2520yield%2520non-meaningful%2520behaviors.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%250Aframework%2520for%2520skill%2520discovery%2520that%2520is%2520entirely%2520driven%2520by%2520LLMs.%2520The%2520framework%250Abegins%2520with%2520an%2520LLM%2520generating%2520task%2520proposals%2520based%2520on%2520the%2520provided%2520scene%250Adescription%2520and%2520the%2520robot%2527s%2520configurations%252C%2520aiming%2520to%2520incrementally%2520acquire%2520new%250Askills%2520upon%2520task%2520completion.%2520For%2520each%2520proposed%2520task%252C%2520a%2520series%2520of%2520reinforcement%250Alearning%2520processes%2520are%2520initiated%252C%2520utilizing%2520reward%2520and%2520success%2520determination%250Afunctions%2520sampled%2520by%2520the%2520LLM%2520to%2520develop%2520the%2520corresponding%2520policy.%2520The%250Areliability%2520and%2520trustworthiness%2520of%2520learned%2520behaviors%2520are%2520further%2520ensured%2520by%2520an%250Aindependent%2520vision-language%2520model.%2520We%2520show%2520that%2520starting%2520with%2520zero%2520skill%252C%2520the%250Askill%2520library%2520emerges%2520and%2520expands%2520to%2520more%2520and%2520more%2520meaningful%2520and%2520reliable%250Askills%252C%2520enabling%2520the%2520robot%2520to%2520efficiently%2520further%2520propose%2520and%2520complete%2520advanced%250Atasks.%2520Project%2520page%253A%2520%255Curl%257Bhttps%253A//agentic-skill-discovery.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Skill%20Discovery&entry.906535625=Xufeng%20Zhao%20and%20Cornelius%20Weber%20and%20Stefan%20Wermter&entry.1292438233=%20%20Language-conditioned%20robotic%20skills%20make%20it%20possible%20to%20apply%20the%20high-level%0Areasoning%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20low-level%20robotic%20control.%20A%0Aremaining%20challenge%20is%20to%20acquire%20a%20diverse%20set%20of%20fundamental%20skills.%20Existing%0Aapproaches%20either%20manually%20decompose%20a%20complex%20task%20into%20atomic%20robotic%20actions%0Ain%20a%20top-down%20fashion%2C%20or%20bootstrap%20as%20many%20combinations%20as%20possible%20in%20a%0Abottom-up%20fashion%20to%20cover%20a%20wider%20range%20of%20task%20possibilities.%20These%0Adecompositions%20or%20combinations%2C%20however%2C%20require%20an%20initial%20skill%20library.%20For%0Aexample%2C%20a%20%60%60grasping%27%27%20capability%20can%20never%20emerge%20from%20a%20skill%20library%0Acontaining%20only%20diverse%20%60%60pushing%27%27%20skills.%20Existing%20skill%20discovery%20techniques%0Awith%20reinforcement%20learning%20acquire%20skills%20by%20an%20exhaustive%20exploration%20but%0Aoften%20yield%20non-meaningful%20behaviors.%20In%20this%20study%2C%20we%20introduce%20a%20novel%0Aframework%20for%20skill%20discovery%20that%20is%20entirely%20driven%20by%20LLMs.%20The%20framework%0Abegins%20with%20an%20LLM%20generating%20task%20proposals%20based%20on%20the%20provided%20scene%0Adescription%20and%20the%20robot%27s%20configurations%2C%20aiming%20to%20incrementally%20acquire%20new%0Askills%20upon%20task%20completion.%20For%20each%20proposed%20task%2C%20a%20series%20of%20reinforcement%0Alearning%20processes%20are%20initiated%2C%20utilizing%20reward%20and%20success%20determination%0Afunctions%20sampled%20by%20the%20LLM%20to%20develop%20the%20corresponding%20policy.%20The%0Areliability%20and%20trustworthiness%20of%20learned%20behaviors%20are%20further%20ensured%20by%20an%0Aindependent%20vision-language%20model.%20We%20show%20that%20starting%20with%20zero%20skill%2C%20the%0Askill%20library%20emerges%20and%20expands%20to%20more%20and%20more%20meaningful%20and%20reliable%0Askills%2C%20enabling%20the%20robot%20to%20efficiently%20further%20propose%20and%20complete%20advanced%0Atasks.%20Project%20page%3A%20%5Curl%7Bhttps%3A//agentic-skill-discovery.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15019v2&entry.124074799=Read"},
{"title": "Stochastic Bandits Robust to Adversarial Attacks", "author": "Xuchuang Wang and Jinhang Zuo and Xutong Liu and John C. S. Lui and Mohammad Hajiesmaili", "abstract": "  This paper investigates stochastic multi-armed bandit algorithms that are\nrobust to adversarial attacks, where an attacker can first observe the\nlearner's action and {then} alter their reward observation. We study two cases\nof this model, with or without the knowledge of an attack budget $C$, defined\nas an upper bound of the summation of the difference between the actual and\naltered rewards. For both cases, we devise two types of algorithms with regret\nbounds having additive or multiplicative $C$ dependence terms. For the known\nattack budget case, we prove our algorithms achieve the regret bound of\n${O}((K/\\Delta)\\log T + KC)$ and $\\tilde{O}(\\sqrt{KTC})$ for the additive and\nmultiplicative $C$ terms, respectively, where $K$ is the number of arms, $T$ is\nthe time horizon, $\\Delta$ is the gap between the expected rewards of the\noptimal arm and the second-best arm, and $\\tilde{O}$ hides the logarithmic\nfactors. For the unknown case, we prove our algorithms achieve the regret bound\nof $\\tilde{O}(\\sqrt{KT} + KC^2)$ and $\\tilde{O}(KC\\sqrt{T})$ for the additive\nand multiplicative $C$ terms, respectively. In addition to these upper bound\nresults, we provide several lower bounds showing the tightness of our bounds\nand the optimality of our algorithms. These results delineate an intrinsic\nseparation between the bandits with attacks and corruption models [Lykouris et\nal., 2018].\n", "link": "http://arxiv.org/abs/2408.08859v1", "date": "2024-08-16", "relevancy": 1.6648, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4428}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4287}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Bandits%20Robust%20to%20Adversarial%20Attacks&body=Title%3A%20Stochastic%20Bandits%20Robust%20to%20Adversarial%20Attacks%0AAuthor%3A%20Xuchuang%20Wang%20and%20Jinhang%20Zuo%20and%20Xutong%20Liu%20and%20John%20C.%20S.%20Lui%20and%20Mohammad%20Hajiesmaili%0AAbstract%3A%20%20%20This%20paper%20investigates%20stochastic%20multi-armed%20bandit%20algorithms%20that%20are%0Arobust%20to%20adversarial%20attacks%2C%20where%20an%20attacker%20can%20first%20observe%20the%0Alearner%27s%20action%20and%20%7Bthen%7D%20alter%20their%20reward%20observation.%20We%20study%20two%20cases%0Aof%20this%20model%2C%20with%20or%20without%20the%20knowledge%20of%20an%20attack%20budget%20%24C%24%2C%20defined%0Aas%20an%20upper%20bound%20of%20the%20summation%20of%20the%20difference%20between%20the%20actual%20and%0Aaltered%20rewards.%20For%20both%20cases%2C%20we%20devise%20two%20types%20of%20algorithms%20with%20regret%0Abounds%20having%20additive%20or%20multiplicative%20%24C%24%20dependence%20terms.%20For%20the%20known%0Aattack%20budget%20case%2C%20we%20prove%20our%20algorithms%20achieve%20the%20regret%20bound%20of%0A%24%7BO%7D%28%28K/%5CDelta%29%5Clog%20T%20%2B%20KC%29%24%20and%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BKTC%7D%29%24%20for%20the%20additive%20and%0Amultiplicative%20%24C%24%20terms%2C%20respectively%2C%20where%20%24K%24%20is%20the%20number%20of%20arms%2C%20%24T%24%20is%0Athe%20time%20horizon%2C%20%24%5CDelta%24%20is%20the%20gap%20between%20the%20expected%20rewards%20of%20the%0Aoptimal%20arm%20and%20the%20second-best%20arm%2C%20and%20%24%5Ctilde%7BO%7D%24%20hides%20the%20logarithmic%0Afactors.%20For%20the%20unknown%20case%2C%20we%20prove%20our%20algorithms%20achieve%20the%20regret%20bound%0Aof%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BKT%7D%20%2B%20KC%5E2%29%24%20and%20%24%5Ctilde%7BO%7D%28KC%5Csqrt%7BT%7D%29%24%20for%20the%20additive%0Aand%20multiplicative%20%24C%24%20terms%2C%20respectively.%20In%20addition%20to%20these%20upper%20bound%0Aresults%2C%20we%20provide%20several%20lower%20bounds%20showing%20the%20tightness%20of%20our%20bounds%0Aand%20the%20optimality%20of%20our%20algorithms.%20These%20results%20delineate%20an%20intrinsic%0Aseparation%20between%20the%20bandits%20with%20attacks%20and%20corruption%20models%20%5BLykouris%20et%0Aal.%2C%202018%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Bandits%2520Robust%2520to%2520Adversarial%2520Attacks%26entry.906535625%3DXuchuang%2520Wang%2520and%2520Jinhang%2520Zuo%2520and%2520Xutong%2520Liu%2520and%2520John%2520C.%2520S.%2520Lui%2520and%2520Mohammad%2520Hajiesmaili%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520stochastic%2520multi-armed%2520bandit%2520algorithms%2520that%2520are%250Arobust%2520to%2520adversarial%2520attacks%252C%2520where%2520an%2520attacker%2520can%2520first%2520observe%2520the%250Alearner%2527s%2520action%2520and%2520%257Bthen%257D%2520alter%2520their%2520reward%2520observation.%2520We%2520study%2520two%2520cases%250Aof%2520this%2520model%252C%2520with%2520or%2520without%2520the%2520knowledge%2520of%2520an%2520attack%2520budget%2520%2524C%2524%252C%2520defined%250Aas%2520an%2520upper%2520bound%2520of%2520the%2520summation%2520of%2520the%2520difference%2520between%2520the%2520actual%2520and%250Aaltered%2520rewards.%2520For%2520both%2520cases%252C%2520we%2520devise%2520two%2520types%2520of%2520algorithms%2520with%2520regret%250Abounds%2520having%2520additive%2520or%2520multiplicative%2520%2524C%2524%2520dependence%2520terms.%2520For%2520the%2520known%250Aattack%2520budget%2520case%252C%2520we%2520prove%2520our%2520algorithms%2520achieve%2520the%2520regret%2520bound%2520of%250A%2524%257BO%257D%2528%2528K/%255CDelta%2529%255Clog%2520T%2520%252B%2520KC%2529%2524%2520and%2520%2524%255Ctilde%257BO%257D%2528%255Csqrt%257BKTC%257D%2529%2524%2520for%2520the%2520additive%2520and%250Amultiplicative%2520%2524C%2524%2520terms%252C%2520respectively%252C%2520where%2520%2524K%2524%2520is%2520the%2520number%2520of%2520arms%252C%2520%2524T%2524%2520is%250Athe%2520time%2520horizon%252C%2520%2524%255CDelta%2524%2520is%2520the%2520gap%2520between%2520the%2520expected%2520rewards%2520of%2520the%250Aoptimal%2520arm%2520and%2520the%2520second-best%2520arm%252C%2520and%2520%2524%255Ctilde%257BO%257D%2524%2520hides%2520the%2520logarithmic%250Afactors.%2520For%2520the%2520unknown%2520case%252C%2520we%2520prove%2520our%2520algorithms%2520achieve%2520the%2520regret%2520bound%250Aof%2520%2524%255Ctilde%257BO%257D%2528%255Csqrt%257BKT%257D%2520%252B%2520KC%255E2%2529%2524%2520and%2520%2524%255Ctilde%257BO%257D%2528KC%255Csqrt%257BT%257D%2529%2524%2520for%2520the%2520additive%250Aand%2520multiplicative%2520%2524C%2524%2520terms%252C%2520respectively.%2520In%2520addition%2520to%2520these%2520upper%2520bound%250Aresults%252C%2520we%2520provide%2520several%2520lower%2520bounds%2520showing%2520the%2520tightness%2520of%2520our%2520bounds%250Aand%2520the%2520optimality%2520of%2520our%2520algorithms.%2520These%2520results%2520delineate%2520an%2520intrinsic%250Aseparation%2520between%2520the%2520bandits%2520with%2520attacks%2520and%2520corruption%2520models%2520%255BLykouris%2520et%250Aal.%252C%25202018%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Bandits%20Robust%20to%20Adversarial%20Attacks&entry.906535625=Xuchuang%20Wang%20and%20Jinhang%20Zuo%20and%20Xutong%20Liu%20and%20John%20C.%20S.%20Lui%20and%20Mohammad%20Hajiesmaili&entry.1292438233=%20%20This%20paper%20investigates%20stochastic%20multi-armed%20bandit%20algorithms%20that%20are%0Arobust%20to%20adversarial%20attacks%2C%20where%20an%20attacker%20can%20first%20observe%20the%0Alearner%27s%20action%20and%20%7Bthen%7D%20alter%20their%20reward%20observation.%20We%20study%20two%20cases%0Aof%20this%20model%2C%20with%20or%20without%20the%20knowledge%20of%20an%20attack%20budget%20%24C%24%2C%20defined%0Aas%20an%20upper%20bound%20of%20the%20summation%20of%20the%20difference%20between%20the%20actual%20and%0Aaltered%20rewards.%20For%20both%20cases%2C%20we%20devise%20two%20types%20of%20algorithms%20with%20regret%0Abounds%20having%20additive%20or%20multiplicative%20%24C%24%20dependence%20terms.%20For%20the%20known%0Aattack%20budget%20case%2C%20we%20prove%20our%20algorithms%20achieve%20the%20regret%20bound%20of%0A%24%7BO%7D%28%28K/%5CDelta%29%5Clog%20T%20%2B%20KC%29%24%20and%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BKTC%7D%29%24%20for%20the%20additive%20and%0Amultiplicative%20%24C%24%20terms%2C%20respectively%2C%20where%20%24K%24%20is%20the%20number%20of%20arms%2C%20%24T%24%20is%0Athe%20time%20horizon%2C%20%24%5CDelta%24%20is%20the%20gap%20between%20the%20expected%20rewards%20of%20the%0Aoptimal%20arm%20and%20the%20second-best%20arm%2C%20and%20%24%5Ctilde%7BO%7D%24%20hides%20the%20logarithmic%0Afactors.%20For%20the%20unknown%20case%2C%20we%20prove%20our%20algorithms%20achieve%20the%20regret%20bound%0Aof%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BKT%7D%20%2B%20KC%5E2%29%24%20and%20%24%5Ctilde%7BO%7D%28KC%5Csqrt%7BT%7D%29%24%20for%20the%20additive%0Aand%20multiplicative%20%24C%24%20terms%2C%20respectively.%20In%20addition%20to%20these%20upper%20bound%0Aresults%2C%20we%20provide%20several%20lower%20bounds%20showing%20the%20tightness%20of%20our%20bounds%0Aand%20the%20optimality%20of%20our%20algorithms.%20These%20results%20delineate%20an%20intrinsic%0Aseparation%20between%20the%20bandits%20with%20attacks%20and%20corruption%20models%20%5BLykouris%20et%0Aal.%2C%202018%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08859v1&entry.124074799=Read"},
{"title": "Learning Diffusion Priors from Observations by Expectation Maximization", "author": "Fran\u00e7ois Rozet and G\u00e9r\u00f4me Andry and Fran\u00e7ois Lanusse and Gilles Louppe", "abstract": "  Diffusion models recently proved to be remarkable priors for Bayesian inverse\nproblems. However, training these models typically requires access to large\namounts of clean data, which could prove difficult in some settings. In this\nwork, we present a novel method based on the expectation-maximization algorithm\nfor training diffusion models from incomplete and noisy observations only.\nUnlike previous works, our method leads to proper diffusion models, which is\ncrucial for downstream tasks. As part of our method, we propose and motivate an\nimproved posterior sampling scheme for unconditional diffusion models. We\npresent empirical evidence supporting the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2405.13712v3", "date": "2024-08-16", "relevancy": 1.6564, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5898}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5427}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Diffusion%20Priors%20from%20Observations%20by%20Expectation%20Maximization&body=Title%3A%20Learning%20Diffusion%20Priors%20from%20Observations%20by%20Expectation%20Maximization%0AAuthor%3A%20Fran%C3%A7ois%20Rozet%20and%20G%C3%A9r%C3%B4me%20Andry%20and%20Fran%C3%A7ois%20Lanusse%20and%20Gilles%20Louppe%0AAbstract%3A%20%20%20Diffusion%20models%20recently%20proved%20to%20be%20remarkable%20priors%20for%20Bayesian%20inverse%0Aproblems.%20However%2C%20training%20these%20models%20typically%20requires%20access%20to%20large%0Aamounts%20of%20clean%20data%2C%20which%20could%20prove%20difficult%20in%20some%20settings.%20In%20this%0Awork%2C%20we%20present%20a%20novel%20method%20based%20on%20the%20expectation-maximization%20algorithm%0Afor%20training%20diffusion%20models%20from%20incomplete%20and%20noisy%20observations%20only.%0AUnlike%20previous%20works%2C%20our%20method%20leads%20to%20proper%20diffusion%20models%2C%20which%20is%0Acrucial%20for%20downstream%20tasks.%20As%20part%20of%20our%20method%2C%20we%20propose%20and%20motivate%20an%0Aimproved%20posterior%20sampling%20scheme%20for%20unconditional%20diffusion%20models.%20We%0Apresent%20empirical%20evidence%20supporting%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13712v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Diffusion%2520Priors%2520from%2520Observations%2520by%2520Expectation%2520Maximization%26entry.906535625%3DFran%25C3%25A7ois%2520Rozet%2520and%2520G%25C3%25A9r%25C3%25B4me%2520Andry%2520and%2520Fran%25C3%25A7ois%2520Lanusse%2520and%2520Gilles%2520Louppe%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520recently%2520proved%2520to%2520be%2520remarkable%2520priors%2520for%2520Bayesian%2520inverse%250Aproblems.%2520However%252C%2520training%2520these%2520models%2520typically%2520requires%2520access%2520to%2520large%250Aamounts%2520of%2520clean%2520data%252C%2520which%2520could%2520prove%2520difficult%2520in%2520some%2520settings.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520novel%2520method%2520based%2520on%2520the%2520expectation-maximization%2520algorithm%250Afor%2520training%2520diffusion%2520models%2520from%2520incomplete%2520and%2520noisy%2520observations%2520only.%250AUnlike%2520previous%2520works%252C%2520our%2520method%2520leads%2520to%2520proper%2520diffusion%2520models%252C%2520which%2520is%250Acrucial%2520for%2520downstream%2520tasks.%2520As%2520part%2520of%2520our%2520method%252C%2520we%2520propose%2520and%2520motivate%2520an%250Aimproved%2520posterior%2520sampling%2520scheme%2520for%2520unconditional%2520diffusion%2520models.%2520We%250Apresent%2520empirical%2520evidence%2520supporting%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13712v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Diffusion%20Priors%20from%20Observations%20by%20Expectation%20Maximization&entry.906535625=Fran%C3%A7ois%20Rozet%20and%20G%C3%A9r%C3%B4me%20Andry%20and%20Fran%C3%A7ois%20Lanusse%20and%20Gilles%20Louppe&entry.1292438233=%20%20Diffusion%20models%20recently%20proved%20to%20be%20remarkable%20priors%20for%20Bayesian%20inverse%0Aproblems.%20However%2C%20training%20these%20models%20typically%20requires%20access%20to%20large%0Aamounts%20of%20clean%20data%2C%20which%20could%20prove%20difficult%20in%20some%20settings.%20In%20this%0Awork%2C%20we%20present%20a%20novel%20method%20based%20on%20the%20expectation-maximization%20algorithm%0Afor%20training%20diffusion%20models%20from%20incomplete%20and%20noisy%20observations%20only.%0AUnlike%20previous%20works%2C%20our%20method%20leads%20to%20proper%20diffusion%20models%2C%20which%20is%0Acrucial%20for%20downstream%20tasks.%20As%20part%20of%20our%20method%2C%20we%20propose%20and%20motivate%20an%0Aimproved%20posterior%20sampling%20scheme%20for%20unconditional%20diffusion%20models.%20We%0Apresent%20empirical%20evidence%20supporting%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13712v3&entry.124074799=Read"},
{"title": "MAT-SED: AMasked Audio Transformer with Masked-Reconstruction Based\n  Pre-training for Sound Event Detection", "author": "Pengfei Cai and Yan Song and Kang Li and Haoyu Song and Ian McLoughlin", "abstract": "  Sound event detection (SED) methods that leverage a large pre-trained\nTransformer encoder network have shown promising performance in recent DCASE\nchallenges. However, they still rely on an RNN-based context network to model\ntemporal dependencies, largely due to the scarcity of labeled data. In this\nwork, we propose a pure Transformer-based SED model with masked-reconstruction\nbased pre-training, termed MAT-SED. Specifically, a Transformer with relative\npositional encoding is first designed as the context network, pre-trained by\nthe masked-reconstruction task on all available target data in a\nself-supervised way. Both the encoder and the context network are jointly\nfine-tuned in a semi-supervised manner. Furthermore, a global-local feature\nfusion strategy is proposed to enhance the localization capability. Evaluation\nof MAT-SED on DCASE2023 task4 surpasses state-of-the-art performance, achieving\n0.587/0.896 PSDS1/PSDS2 respectively.\n", "link": "http://arxiv.org/abs/2408.08673v1", "date": "2024-08-16", "relevancy": 1.6494, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.57}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAT-SED%3A%20AMasked%20Audio%20Transformer%20with%20Masked-Reconstruction%20Based%0A%20%20Pre-training%20for%20Sound%20Event%20Detection&body=Title%3A%20MAT-SED%3A%20AMasked%20Audio%20Transformer%20with%20Masked-Reconstruction%20Based%0A%20%20Pre-training%20for%20Sound%20Event%20Detection%0AAuthor%3A%20Pengfei%20Cai%20and%20Yan%20Song%20and%20Kang%20Li%20and%20Haoyu%20Song%20and%20Ian%20McLoughlin%0AAbstract%3A%20%20%20Sound%20event%20detection%20%28SED%29%20methods%20that%20leverage%20a%20large%20pre-trained%0ATransformer%20encoder%20network%20have%20shown%20promising%20performance%20in%20recent%20DCASE%0Achallenges.%20However%2C%20they%20still%20rely%20on%20an%20RNN-based%20context%20network%20to%20model%0Atemporal%20dependencies%2C%20largely%20due%20to%20the%20scarcity%20of%20labeled%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20pure%20Transformer-based%20SED%20model%20with%20masked-reconstruction%0Abased%20pre-training%2C%20termed%20MAT-SED.%20Specifically%2C%20a%20Transformer%20with%20relative%0Apositional%20encoding%20is%20first%20designed%20as%20the%20context%20network%2C%20pre-trained%20by%0Athe%20masked-reconstruction%20task%20on%20all%20available%20target%20data%20in%20a%0Aself-supervised%20way.%20Both%20the%20encoder%20and%20the%20context%20network%20are%20jointly%0Afine-tuned%20in%20a%20semi-supervised%20manner.%20Furthermore%2C%20a%20global-local%20feature%0Afusion%20strategy%20is%20proposed%20to%20enhance%20the%20localization%20capability.%20Evaluation%0Aof%20MAT-SED%20on%20DCASE2023%20task4%20surpasses%20state-of-the-art%20performance%2C%20achieving%0A0.587/0.896%20PSDS1/PSDS2%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAT-SED%253A%2520AMasked%2520Audio%2520Transformer%2520with%2520Masked-Reconstruction%2520Based%250A%2520%2520Pre-training%2520for%2520Sound%2520Event%2520Detection%26entry.906535625%3DPengfei%2520Cai%2520and%2520Yan%2520Song%2520and%2520Kang%2520Li%2520and%2520Haoyu%2520Song%2520and%2520Ian%2520McLoughlin%26entry.1292438233%3D%2520%2520Sound%2520event%2520detection%2520%2528SED%2529%2520methods%2520that%2520leverage%2520a%2520large%2520pre-trained%250ATransformer%2520encoder%2520network%2520have%2520shown%2520promising%2520performance%2520in%2520recent%2520DCASE%250Achallenges.%2520However%252C%2520they%2520still%2520rely%2520on%2520an%2520RNN-based%2520context%2520network%2520to%2520model%250Atemporal%2520dependencies%252C%2520largely%2520due%2520to%2520the%2520scarcity%2520of%2520labeled%2520data.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520pure%2520Transformer-based%2520SED%2520model%2520with%2520masked-reconstruction%250Abased%2520pre-training%252C%2520termed%2520MAT-SED.%2520Specifically%252C%2520a%2520Transformer%2520with%2520relative%250Apositional%2520encoding%2520is%2520first%2520designed%2520as%2520the%2520context%2520network%252C%2520pre-trained%2520by%250Athe%2520masked-reconstruction%2520task%2520on%2520all%2520available%2520target%2520data%2520in%2520a%250Aself-supervised%2520way.%2520Both%2520the%2520encoder%2520and%2520the%2520context%2520network%2520are%2520jointly%250Afine-tuned%2520in%2520a%2520semi-supervised%2520manner.%2520Furthermore%252C%2520a%2520global-local%2520feature%250Afusion%2520strategy%2520is%2520proposed%2520to%2520enhance%2520the%2520localization%2520capability.%2520Evaluation%250Aof%2520MAT-SED%2520on%2520DCASE2023%2520task4%2520surpasses%2520state-of-the-art%2520performance%252C%2520achieving%250A0.587/0.896%2520PSDS1/PSDS2%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAT-SED%3A%20AMasked%20Audio%20Transformer%20with%20Masked-Reconstruction%20Based%0A%20%20Pre-training%20for%20Sound%20Event%20Detection&entry.906535625=Pengfei%20Cai%20and%20Yan%20Song%20and%20Kang%20Li%20and%20Haoyu%20Song%20and%20Ian%20McLoughlin&entry.1292438233=%20%20Sound%20event%20detection%20%28SED%29%20methods%20that%20leverage%20a%20large%20pre-trained%0ATransformer%20encoder%20network%20have%20shown%20promising%20performance%20in%20recent%20DCASE%0Achallenges.%20However%2C%20they%20still%20rely%20on%20an%20RNN-based%20context%20network%20to%20model%0Atemporal%20dependencies%2C%20largely%20due%20to%20the%20scarcity%20of%20labeled%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20pure%20Transformer-based%20SED%20model%20with%20masked-reconstruction%0Abased%20pre-training%2C%20termed%20MAT-SED.%20Specifically%2C%20a%20Transformer%20with%20relative%0Apositional%20encoding%20is%20first%20designed%20as%20the%20context%20network%2C%20pre-trained%20by%0Athe%20masked-reconstruction%20task%20on%20all%20available%20target%20data%20in%20a%0Aself-supervised%20way.%20Both%20the%20encoder%20and%20the%20context%20network%20are%20jointly%0Afine-tuned%20in%20a%20semi-supervised%20manner.%20Furthermore%2C%20a%20global-local%20feature%0Afusion%20strategy%20is%20proposed%20to%20enhance%20the%20localization%20capability.%20Evaluation%0Aof%20MAT-SED%20on%20DCASE2023%20task4%20surpasses%20state-of-the-art%20performance%2C%20achieving%0A0.587/0.896%20PSDS1/PSDS2%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08673v1&entry.124074799=Read"},
{"title": "Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling", "author": "Xianzhen Luo and Yixuan Wang and Qingfu Zhu and Zhiming Zhang and Xuanyu Zhang and Qing Yang and Dongliang Xu and Wanxiang Che", "abstract": "  The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation.\n", "link": "http://arxiv.org/abs/2408.08696v1", "date": "2024-08-16", "relevancy": 1.6355, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5687}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5177}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turning%20Trash%20into%20Treasure%3A%20Accelerating%20Inference%20of%20Large%20Language%0A%20%20Models%20with%20Token%20Recycling&body=Title%3A%20Turning%20Trash%20into%20Treasure%3A%20Accelerating%20Inference%20of%20Large%20Language%0A%20%20Models%20with%20Token%20Recycling%0AAuthor%3A%20Xianzhen%20Luo%20and%20Yixuan%20Wang%20and%20Qingfu%20Zhu%20and%20Zhiming%20Zhang%20and%20Xuanyu%20Zhang%20and%20Qing%20Yang%20and%20Dongliang%20Xu%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20The%20rapid%20growth%20in%20the%20parameters%20of%20large%20language%20models%20%28LLMs%29%20has%20made%0Ainference%20latency%20a%20fundamental%20bottleneck%2C%20limiting%20broader%20application%20of%0ALLMs.%20Speculative%20decoding%20represents%20a%20lossless%20approach%20to%20accelerate%0Ainference%20through%20a%20guess-and-verify%20paradigm%2C%20leveraging%20the%20parallel%0Acapabilities%20of%20modern%20hardware.%20Some%20speculative%20decoding%20methods%20rely%20on%0Aadditional%20structures%20to%20guess%20draft%20tokens%2C%20such%20as%20small%20models%20or%0Aparameter-efficient%20architectures%2C%20which%20need%20extra%20training%20before%20use.%0AAlternatively%2C%20retrieval-based%20train-free%20techniques%20build%20libraries%20from%0Apre-existing%20corpora%20or%20by%20n-gram%20generation.%20However%2C%20they%20face%20challenges%0Alike%20large%20storage%20requirements%2C%20time-consuming%20retrieval%2C%20and%20limited%0Aadaptability.%20Observing%20that%20candidate%20tokens%20generated%20during%20the%20decoding%0Aprocess%20are%20likely%20to%20reoccur%20in%20future%20sequences%2C%20we%20propose%20Token%20Recycling.%0AThis%20approach%20stores%20candidate%20tokens%20in%20an%20adjacency%20matrix%20and%20employs%20a%0Abreadth-first%20search%20%28BFS%29-like%20algorithm%20on%20the%20matrix%20to%20construct%20a%20draft%0Atree.%20The%20tree%20is%20then%20validated%20through%20tree%20attention.%20New%20candidate%20tokens%0Afrom%20the%20decoding%20process%20are%20then%20used%20to%20update%20the%20matrix.%20Token%20Recycling%0Arequires%20%5Ctextless2MB%20of%20additional%20storage%20and%20achieves%20approximately%202x%0Aspeedup%20across%20all%20sizes%20of%20LLMs.%20It%20significantly%20outperforms%20existing%0Atrain-free%20methods%20by%2030%5C%25%20and%20even%20a%20training%20method%20by%2025%5C%25.%20It%20can%20be%0Adirectly%20applied%20to%20any%20existing%20LLMs%20and%20tasks%20without%20the%20need%20for%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurning%2520Trash%2520into%2520Treasure%253A%2520Accelerating%2520Inference%2520of%2520Large%2520Language%250A%2520%2520Models%2520with%2520Token%2520Recycling%26entry.906535625%3DXianzhen%2520Luo%2520and%2520Yixuan%2520Wang%2520and%2520Qingfu%2520Zhu%2520and%2520Zhiming%2520Zhang%2520and%2520Xuanyu%2520Zhang%2520and%2520Qing%2520Yang%2520and%2520Dongliang%2520Xu%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520in%2520the%2520parameters%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520made%250Ainference%2520latency%2520a%2520fundamental%2520bottleneck%252C%2520limiting%2520broader%2520application%2520of%250ALLMs.%2520Speculative%2520decoding%2520represents%2520a%2520lossless%2520approach%2520to%2520accelerate%250Ainference%2520through%2520a%2520guess-and-verify%2520paradigm%252C%2520leveraging%2520the%2520parallel%250Acapabilities%2520of%2520modern%2520hardware.%2520Some%2520speculative%2520decoding%2520methods%2520rely%2520on%250Aadditional%2520structures%2520to%2520guess%2520draft%2520tokens%252C%2520such%2520as%2520small%2520models%2520or%250Aparameter-efficient%2520architectures%252C%2520which%2520need%2520extra%2520training%2520before%2520use.%250AAlternatively%252C%2520retrieval-based%2520train-free%2520techniques%2520build%2520libraries%2520from%250Apre-existing%2520corpora%2520or%2520by%2520n-gram%2520generation.%2520However%252C%2520they%2520face%2520challenges%250Alike%2520large%2520storage%2520requirements%252C%2520time-consuming%2520retrieval%252C%2520and%2520limited%250Aadaptability.%2520Observing%2520that%2520candidate%2520tokens%2520generated%2520during%2520the%2520decoding%250Aprocess%2520are%2520likely%2520to%2520reoccur%2520in%2520future%2520sequences%252C%2520we%2520propose%2520Token%2520Recycling.%250AThis%2520approach%2520stores%2520candidate%2520tokens%2520in%2520an%2520adjacency%2520matrix%2520and%2520employs%2520a%250Abreadth-first%2520search%2520%2528BFS%2529-like%2520algorithm%2520on%2520the%2520matrix%2520to%2520construct%2520a%2520draft%250Atree.%2520The%2520tree%2520is%2520then%2520validated%2520through%2520tree%2520attention.%2520New%2520candidate%2520tokens%250Afrom%2520the%2520decoding%2520process%2520are%2520then%2520used%2520to%2520update%2520the%2520matrix.%2520Token%2520Recycling%250Arequires%2520%255Ctextless2MB%2520of%2520additional%2520storage%2520and%2520achieves%2520approximately%25202x%250Aspeedup%2520across%2520all%2520sizes%2520of%2520LLMs.%2520It%2520significantly%2520outperforms%2520existing%250Atrain-free%2520methods%2520by%252030%255C%2525%2520and%2520even%2520a%2520training%2520method%2520by%252025%255C%2525.%2520It%2520can%2520be%250Adirectly%2520applied%2520to%2520any%2520existing%2520LLMs%2520and%2520tasks%2520without%2520the%2520need%2520for%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turning%20Trash%20into%20Treasure%3A%20Accelerating%20Inference%20of%20Large%20Language%0A%20%20Models%20with%20Token%20Recycling&entry.906535625=Xianzhen%20Luo%20and%20Yixuan%20Wang%20and%20Qingfu%20Zhu%20and%20Zhiming%20Zhang%20and%20Xuanyu%20Zhang%20and%20Qing%20Yang%20and%20Dongliang%20Xu%20and%20Wanxiang%20Che&entry.1292438233=%20%20The%20rapid%20growth%20in%20the%20parameters%20of%20large%20language%20models%20%28LLMs%29%20has%20made%0Ainference%20latency%20a%20fundamental%20bottleneck%2C%20limiting%20broader%20application%20of%0ALLMs.%20Speculative%20decoding%20represents%20a%20lossless%20approach%20to%20accelerate%0Ainference%20through%20a%20guess-and-verify%20paradigm%2C%20leveraging%20the%20parallel%0Acapabilities%20of%20modern%20hardware.%20Some%20speculative%20decoding%20methods%20rely%20on%0Aadditional%20structures%20to%20guess%20draft%20tokens%2C%20such%20as%20small%20models%20or%0Aparameter-efficient%20architectures%2C%20which%20need%20extra%20training%20before%20use.%0AAlternatively%2C%20retrieval-based%20train-free%20techniques%20build%20libraries%20from%0Apre-existing%20corpora%20or%20by%20n-gram%20generation.%20However%2C%20they%20face%20challenges%0Alike%20large%20storage%20requirements%2C%20time-consuming%20retrieval%2C%20and%20limited%0Aadaptability.%20Observing%20that%20candidate%20tokens%20generated%20during%20the%20decoding%0Aprocess%20are%20likely%20to%20reoccur%20in%20future%20sequences%2C%20we%20propose%20Token%20Recycling.%0AThis%20approach%20stores%20candidate%20tokens%20in%20an%20adjacency%20matrix%20and%20employs%20a%0Abreadth-first%20search%20%28BFS%29-like%20algorithm%20on%20the%20matrix%20to%20construct%20a%20draft%0Atree.%20The%20tree%20is%20then%20validated%20through%20tree%20attention.%20New%20candidate%20tokens%0Afrom%20the%20decoding%20process%20are%20then%20used%20to%20update%20the%20matrix.%20Token%20Recycling%0Arequires%20%5Ctextless2MB%20of%20additional%20storage%20and%20achieves%20approximately%202x%0Aspeedup%20across%20all%20sizes%20of%20LLMs.%20It%20significantly%20outperforms%20existing%0Atrain-free%20methods%20by%2030%5C%25%20and%20even%20a%20training%20method%20by%2025%5C%25.%20It%20can%20be%0Adirectly%20applied%20to%20any%20existing%20LLMs%20and%20tasks%20without%20the%20need%20for%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08696v1&entry.124074799=Read"},
{"title": "Retrieval-augmented Few-shot Medical Image Segmentation with Foundation\n  Models", "author": "Lin Zhao and Xiao Chen and Eric Z. Chen and Yikang Liu and Terrence Chen and Shanhui Sun", "abstract": "  Medical image segmentation is crucial for clinical decision-making, but the\nscarcity of annotated data presents significant challenges. Few-shot\nsegmentation (FSS) methods show promise but often require retraining on the\ntarget domain and struggle to generalize across different modalities.\nSimilarly, adapting foundation models like the Segment Anything Model (SAM) for\nmedical imaging has limitations, including the need for finetuning and\ndomain-specific adaptation. To address these issues, we propose a novel method\nthat adapts DINOv2 and Segment Anything Model 2 (SAM 2) for retrieval-augmented\nfew-shot medical image segmentation. Our approach uses DINOv2's feature as\nquery to retrieve similar samples from limited annotated data, which are then\nencoded as memories and stored in memory bank. With the memory attention\nmechanism of SAM 2, the model leverages these memories as conditions to\ngenerate accurate segmentation of the target image. We evaluated our framework\non three medical image segmentation tasks, demonstrating superior performance\nand generalizability across various modalities without the need for any\nretraining or finetuning. Overall, this method offers a practical and effective\nsolution for few-shot medical image segmentation and holds significant\npotential as a valuable annotation tool in clinical applications.\n", "link": "http://arxiv.org/abs/2408.08813v1", "date": "2024-08-16", "relevancy": 1.6345, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-augmented%20Few-shot%20Medical%20Image%20Segmentation%20with%20Foundation%0A%20%20Models&body=Title%3A%20Retrieval-augmented%20Few-shot%20Medical%20Image%20Segmentation%20with%20Foundation%0A%20%20Models%0AAuthor%3A%20Lin%20Zhao%20and%20Xiao%20Chen%20and%20Eric%20Z.%20Chen%20and%20Yikang%20Liu%20and%20Terrence%20Chen%20and%20Shanhui%20Sun%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20crucial%20for%20clinical%20decision-making%2C%20but%20the%0Ascarcity%20of%20annotated%20data%20presents%20significant%20challenges.%20Few-shot%0Asegmentation%20%28FSS%29%20methods%20show%20promise%20but%20often%20require%20retraining%20on%20the%0Atarget%20domain%20and%20struggle%20to%20generalize%20across%20different%20modalities.%0ASimilarly%2C%20adapting%20foundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%0Amedical%20imaging%20has%20limitations%2C%20including%20the%20need%20for%20finetuning%20and%0Adomain-specific%20adaptation.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%0Athat%20adapts%20DINOv2%20and%20Segment%20Anything%20Model%202%20%28SAM%202%29%20for%20retrieval-augmented%0Afew-shot%20medical%20image%20segmentation.%20Our%20approach%20uses%20DINOv2%27s%20feature%20as%0Aquery%20to%20retrieve%20similar%20samples%20from%20limited%20annotated%20data%2C%20which%20are%20then%0Aencoded%20as%20memories%20and%20stored%20in%20memory%20bank.%20With%20the%20memory%20attention%0Amechanism%20of%20SAM%202%2C%20the%20model%20leverages%20these%20memories%20as%20conditions%20to%0Agenerate%20accurate%20segmentation%20of%20the%20target%20image.%20We%20evaluated%20our%20framework%0Aon%20three%20medical%20image%20segmentation%20tasks%2C%20demonstrating%20superior%20performance%0Aand%20generalizability%20across%20various%20modalities%20without%20the%20need%20for%20any%0Aretraining%20or%20finetuning.%20Overall%2C%20this%20method%20offers%20a%20practical%20and%20effective%0Asolution%20for%20few-shot%20medical%20image%20segmentation%20and%20holds%20significant%0Apotential%20as%20a%20valuable%20annotation%20tool%20in%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-augmented%2520Few-shot%2520Medical%2520Image%2520Segmentation%2520with%2520Foundation%250A%2520%2520Models%26entry.906535625%3DLin%2520Zhao%2520and%2520Xiao%2520Chen%2520and%2520Eric%2520Z.%2520Chen%2520and%2520Yikang%2520Liu%2520and%2520Terrence%2520Chen%2520and%2520Shanhui%2520Sun%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520crucial%2520for%2520clinical%2520decision-making%252C%2520but%2520the%250Ascarcity%2520of%2520annotated%2520data%2520presents%2520significant%2520challenges.%2520Few-shot%250Asegmentation%2520%2528FSS%2529%2520methods%2520show%2520promise%2520but%2520often%2520require%2520retraining%2520on%2520the%250Atarget%2520domain%2520and%2520struggle%2520to%2520generalize%2520across%2520different%2520modalities.%250ASimilarly%252C%2520adapting%2520foundation%2520models%2520like%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520for%250Amedical%2520imaging%2520has%2520limitations%252C%2520including%2520the%2520need%2520for%2520finetuning%2520and%250Adomain-specific%2520adaptation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520method%250Athat%2520adapts%2520DINOv2%2520and%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%2520for%2520retrieval-augmented%250Afew-shot%2520medical%2520image%2520segmentation.%2520Our%2520approach%2520uses%2520DINOv2%2527s%2520feature%2520as%250Aquery%2520to%2520retrieve%2520similar%2520samples%2520from%2520limited%2520annotated%2520data%252C%2520which%2520are%2520then%250Aencoded%2520as%2520memories%2520and%2520stored%2520in%2520memory%2520bank.%2520With%2520the%2520memory%2520attention%250Amechanism%2520of%2520SAM%25202%252C%2520the%2520model%2520leverages%2520these%2520memories%2520as%2520conditions%2520to%250Agenerate%2520accurate%2520segmentation%2520of%2520the%2520target%2520image.%2520We%2520evaluated%2520our%2520framework%250Aon%2520three%2520medical%2520image%2520segmentation%2520tasks%252C%2520demonstrating%2520superior%2520performance%250Aand%2520generalizability%2520across%2520various%2520modalities%2520without%2520the%2520need%2520for%2520any%250Aretraining%2520or%2520finetuning.%2520Overall%252C%2520this%2520method%2520offers%2520a%2520practical%2520and%2520effective%250Asolution%2520for%2520few-shot%2520medical%2520image%2520segmentation%2520and%2520holds%2520significant%250Apotential%2520as%2520a%2520valuable%2520annotation%2520tool%2520in%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-augmented%20Few-shot%20Medical%20Image%20Segmentation%20with%20Foundation%0A%20%20Models&entry.906535625=Lin%20Zhao%20and%20Xiao%20Chen%20and%20Eric%20Z.%20Chen%20and%20Yikang%20Liu%20and%20Terrence%20Chen%20and%20Shanhui%20Sun&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20crucial%20for%20clinical%20decision-making%2C%20but%20the%0Ascarcity%20of%20annotated%20data%20presents%20significant%20challenges.%20Few-shot%0Asegmentation%20%28FSS%29%20methods%20show%20promise%20but%20often%20require%20retraining%20on%20the%0Atarget%20domain%20and%20struggle%20to%20generalize%20across%20different%20modalities.%0ASimilarly%2C%20adapting%20foundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20for%0Amedical%20imaging%20has%20limitations%2C%20including%20the%20need%20for%20finetuning%20and%0Adomain-specific%20adaptation.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%0Athat%20adapts%20DINOv2%20and%20Segment%20Anything%20Model%202%20%28SAM%202%29%20for%20retrieval-augmented%0Afew-shot%20medical%20image%20segmentation.%20Our%20approach%20uses%20DINOv2%27s%20feature%20as%0Aquery%20to%20retrieve%20similar%20samples%20from%20limited%20annotated%20data%2C%20which%20are%20then%0Aencoded%20as%20memories%20and%20stored%20in%20memory%20bank.%20With%20the%20memory%20attention%0Amechanism%20of%20SAM%202%2C%20the%20model%20leverages%20these%20memories%20as%20conditions%20to%0Agenerate%20accurate%20segmentation%20of%20the%20target%20image.%20We%20evaluated%20our%20framework%0Aon%20three%20medical%20image%20segmentation%20tasks%2C%20demonstrating%20superior%20performance%0Aand%20generalizability%20across%20various%20modalities%20without%20the%20need%20for%20any%0Aretraining%20or%20finetuning.%20Overall%2C%20this%20method%20offers%20a%20practical%20and%20effective%0Asolution%20for%20few-shot%20medical%20image%20segmentation%20and%20holds%20significant%0Apotential%20as%20a%20valuable%20annotation%20tool%20in%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08813v1&entry.124074799=Read"},
{"title": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement\n  Learning", "author": "Adriana Hugessen and Roger Creus Castanyer and Faisal Mohamed and Glen Berseth", "abstract": "  Both entropy-minimizing and entropy-maximizing (curiosity) objectives for\nunsupervised reinforcement learning (RL) have been shown to be effective in\ndifferent environments, depending on the environment's level of natural\nentropy. However, neither method alone results in an agent that will\nconsistently learn intelligent behavior across environments. In an effort to\nfind a single entropy-based method that will encourage emergent behaviors in\nany environment, we propose an agent that can adapt its objective online,\ndepending on the entropy conditions by framing the choice as a multi-armed\nbandit problem. We devise a novel intrinsic feedback signal for the bandit,\nwhich captures the agent's ability to control the entropy in its environment.\nWe demonstrate that such agents can learn to control entropy and exhibit\nemergent behaviors in both high- and low-entropy regimes and can learn skillful\nbehaviors in benchmark tasks. Videos of the trained agents and summarized\nfindings can be found on our project page\nhttps://sites.google.com/view/surprise-adaptive-agents\n", "link": "http://arxiv.org/abs/2405.17243v2", "date": "2024-08-16", "relevancy": 1.6269, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5526}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5298}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surprise-Adaptive%20Intrinsic%20Motivation%20for%20Unsupervised%20Reinforcement%0A%20%20Learning&body=Title%3A%20Surprise-Adaptive%20Intrinsic%20Motivation%20for%20Unsupervised%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Adriana%20Hugessen%20and%20Roger%20Creus%20Castanyer%20and%20Faisal%20Mohamed%20and%20Glen%20Berseth%0AAbstract%3A%20%20%20Both%20entropy-minimizing%20and%20entropy-maximizing%20%28curiosity%29%20objectives%20for%0Aunsupervised%20reinforcement%20learning%20%28RL%29%20have%20been%20shown%20to%20be%20effective%20in%0Adifferent%20environments%2C%20depending%20on%20the%20environment%27s%20level%20of%20natural%0Aentropy.%20However%2C%20neither%20method%20alone%20results%20in%20an%20agent%20that%20will%0Aconsistently%20learn%20intelligent%20behavior%20across%20environments.%20In%20an%20effort%20to%0Afind%20a%20single%20entropy-based%20method%20that%20will%20encourage%20emergent%20behaviors%20in%0Aany%20environment%2C%20we%20propose%20an%20agent%20that%20can%20adapt%20its%20objective%20online%2C%0Adepending%20on%20the%20entropy%20conditions%20by%20framing%20the%20choice%20as%20a%20multi-armed%0Abandit%20problem.%20We%20devise%20a%20novel%20intrinsic%20feedback%20signal%20for%20the%20bandit%2C%0Awhich%20captures%20the%20agent%27s%20ability%20to%20control%20the%20entropy%20in%20its%20environment.%0AWe%20demonstrate%20that%20such%20agents%20can%20learn%20to%20control%20entropy%20and%20exhibit%0Aemergent%20behaviors%20in%20both%20high-%20and%20low-entropy%20regimes%20and%20can%20learn%20skillful%0Abehaviors%20in%20benchmark%20tasks.%20Videos%20of%20the%20trained%20agents%20and%20summarized%0Afindings%20can%20be%20found%20on%20our%20project%20page%0Ahttps%3A//sites.google.com/view/surprise-adaptive-agents%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurprise-Adaptive%2520Intrinsic%2520Motivation%2520for%2520Unsupervised%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DAdriana%2520Hugessen%2520and%2520Roger%2520Creus%2520Castanyer%2520and%2520Faisal%2520Mohamed%2520and%2520Glen%2520Berseth%26entry.1292438233%3D%2520%2520Both%2520entropy-minimizing%2520and%2520entropy-maximizing%2520%2528curiosity%2529%2520objectives%2520for%250Aunsupervised%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520been%2520shown%2520to%2520be%2520effective%2520in%250Adifferent%2520environments%252C%2520depending%2520on%2520the%2520environment%2527s%2520level%2520of%2520natural%250Aentropy.%2520However%252C%2520neither%2520method%2520alone%2520results%2520in%2520an%2520agent%2520that%2520will%250Aconsistently%2520learn%2520intelligent%2520behavior%2520across%2520environments.%2520In%2520an%2520effort%2520to%250Afind%2520a%2520single%2520entropy-based%2520method%2520that%2520will%2520encourage%2520emergent%2520behaviors%2520in%250Aany%2520environment%252C%2520we%2520propose%2520an%2520agent%2520that%2520can%2520adapt%2520its%2520objective%2520online%252C%250Adepending%2520on%2520the%2520entropy%2520conditions%2520by%2520framing%2520the%2520choice%2520as%2520a%2520multi-armed%250Abandit%2520problem.%2520We%2520devise%2520a%2520novel%2520intrinsic%2520feedback%2520signal%2520for%2520the%2520bandit%252C%250Awhich%2520captures%2520the%2520agent%2527s%2520ability%2520to%2520control%2520the%2520entropy%2520in%2520its%2520environment.%250AWe%2520demonstrate%2520that%2520such%2520agents%2520can%2520learn%2520to%2520control%2520entropy%2520and%2520exhibit%250Aemergent%2520behaviors%2520in%2520both%2520high-%2520and%2520low-entropy%2520regimes%2520and%2520can%2520learn%2520skillful%250Abehaviors%2520in%2520benchmark%2520tasks.%2520Videos%2520of%2520the%2520trained%2520agents%2520and%2520summarized%250Afindings%2520can%2520be%2520found%2520on%2520our%2520project%2520page%250Ahttps%253A//sites.google.com/view/surprise-adaptive-agents%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surprise-Adaptive%20Intrinsic%20Motivation%20for%20Unsupervised%20Reinforcement%0A%20%20Learning&entry.906535625=Adriana%20Hugessen%20and%20Roger%20Creus%20Castanyer%20and%20Faisal%20Mohamed%20and%20Glen%20Berseth&entry.1292438233=%20%20Both%20entropy-minimizing%20and%20entropy-maximizing%20%28curiosity%29%20objectives%20for%0Aunsupervised%20reinforcement%20learning%20%28RL%29%20have%20been%20shown%20to%20be%20effective%20in%0Adifferent%20environments%2C%20depending%20on%20the%20environment%27s%20level%20of%20natural%0Aentropy.%20However%2C%20neither%20method%20alone%20results%20in%20an%20agent%20that%20will%0Aconsistently%20learn%20intelligent%20behavior%20across%20environments.%20In%20an%20effort%20to%0Afind%20a%20single%20entropy-based%20method%20that%20will%20encourage%20emergent%20behaviors%20in%0Aany%20environment%2C%20we%20propose%20an%20agent%20that%20can%20adapt%20its%20objective%20online%2C%0Adepending%20on%20the%20entropy%20conditions%20by%20framing%20the%20choice%20as%20a%20multi-armed%0Abandit%20problem.%20We%20devise%20a%20novel%20intrinsic%20feedback%20signal%20for%20the%20bandit%2C%0Awhich%20captures%20the%20agent%27s%20ability%20to%20control%20the%20entropy%20in%20its%20environment.%0AWe%20demonstrate%20that%20such%20agents%20can%20learn%20to%20control%20entropy%20and%20exhibit%0Aemergent%20behaviors%20in%20both%20high-%20and%20low-entropy%20regimes%20and%20can%20learn%20skillful%0Abehaviors%20in%20benchmark%20tasks.%20Videos%20of%20the%20trained%20agents%20and%20summarized%0Afindings%20can%20be%20found%20on%20our%20project%20page%0Ahttps%3A//sites.google.com/view/surprise-adaptive-agents%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17243v2&entry.124074799=Read"},
{"title": "Multi-task Image Restoration Guided By Robust DINO Features", "author": "Xin Lin and Jingtong Yue and Kelvin C. K. Chan and Lu Qi and Chao Ren and Jinshan Pan and Ming-Hsuan Yang", "abstract": "  Multi-task image restoration has gained significant interest due to its\ninherent versatility and efficiency compared to its single-task counterpart.\nHowever, performance decline is observed with an increase in the number of\ntasks, primarily attributed to the restoration model's challenge in handling\ndifferent tasks with distinct natures at the same time. Thus, a perspective\nemerged aiming to explore the degradation-insensitive semantic commonalities\namong different degradation tasks. In this paper, we observe that the features\nof DINOv2 can effectively model semantic information and are independent of\ndegradation factors. Motivated by this observation, we propose\n\\mbox{\\textbf{DINO-IR}}, a multi-task image restoration approach leveraging\nrobust features extracted from DINOv2 to solve multi-task image restoration\nsimultaneously. We first propose a pixel-semantic fusion (PSF) module to\ndynamically fuse DINOV2's shallow features containing pixel-level information\nand deep features containing degradation-independent semantic information. To\nguide the restoration model with the features of DINOv2, we develop a\nDINO-Restore adaption and fusion module to adjust the channel of fused features\nfrom PSF and then integrate them with the features from the restoration model.\nBy formulating these modules into a unified deep model, we propose a DINO\nperception contrastive loss to constrain the model training. Extensive\nexperimental results demonstrate that our DINO-IR performs favorably against\nexisting multi-task image restoration approaches in various tasks by a large\nmargin. The source codes and trained models will be made available.\n", "link": "http://arxiv.org/abs/2312.01677v3", "date": "2024-08-16", "relevancy": 1.6181, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5492}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5291}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20Image%20Restoration%20Guided%20By%20Robust%20DINO%20Features&body=Title%3A%20Multi-task%20Image%20Restoration%20Guided%20By%20Robust%20DINO%20Features%0AAuthor%3A%20Xin%20Lin%20and%20Jingtong%20Yue%20and%20Kelvin%20C.%20K.%20Chan%20and%20Lu%20Qi%20and%20Chao%20Ren%20and%20Jinshan%20Pan%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Multi-task%20image%20restoration%20has%20gained%20significant%20interest%20due%20to%20its%0Ainherent%20versatility%20and%20efficiency%20compared%20to%20its%20single-task%20counterpart.%0AHowever%2C%20performance%20decline%20is%20observed%20with%20an%20increase%20in%20the%20number%20of%0Atasks%2C%20primarily%20attributed%20to%20the%20restoration%20model%27s%20challenge%20in%20handling%0Adifferent%20tasks%20with%20distinct%20natures%20at%20the%20same%20time.%20Thus%2C%20a%20perspective%0Aemerged%20aiming%20to%20explore%20the%20degradation-insensitive%20semantic%20commonalities%0Aamong%20different%20degradation%20tasks.%20In%20this%20paper%2C%20we%20observe%20that%20the%20features%0Aof%20DINOv2%20can%20effectively%20model%20semantic%20information%20and%20are%20independent%20of%0Adegradation%20factors.%20Motivated%20by%20this%20observation%2C%20we%20propose%0A%5Cmbox%7B%5Ctextbf%7BDINO-IR%7D%7D%2C%20a%20multi-task%20image%20restoration%20approach%20leveraging%0Arobust%20features%20extracted%20from%20DINOv2%20to%20solve%20multi-task%20image%20restoration%0Asimultaneously.%20We%20first%20propose%20a%20pixel-semantic%20fusion%20%28PSF%29%20module%20to%0Adynamically%20fuse%20DINOV2%27s%20shallow%20features%20containing%20pixel-level%20information%0Aand%20deep%20features%20containing%20degradation-independent%20semantic%20information.%20To%0Aguide%20the%20restoration%20model%20with%20the%20features%20of%20DINOv2%2C%20we%20develop%20a%0ADINO-Restore%20adaption%20and%20fusion%20module%20to%20adjust%20the%20channel%20of%20fused%20features%0Afrom%20PSF%20and%20then%20integrate%20them%20with%20the%20features%20from%20the%20restoration%20model.%0ABy%20formulating%20these%20modules%20into%20a%20unified%20deep%20model%2C%20we%20propose%20a%20DINO%0Aperception%20contrastive%20loss%20to%20constrain%20the%20model%20training.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20DINO-IR%20performs%20favorably%20against%0Aexisting%20multi-task%20image%20restoration%20approaches%20in%20various%20tasks%20by%20a%20large%0Amargin.%20The%20source%20codes%20and%20trained%20models%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520Image%2520Restoration%2520Guided%2520By%2520Robust%2520DINO%2520Features%26entry.906535625%3DXin%2520Lin%2520and%2520Jingtong%2520Yue%2520and%2520Kelvin%2520C.%2520K.%2520Chan%2520and%2520Lu%2520Qi%2520and%2520Chao%2520Ren%2520and%2520Jinshan%2520Pan%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Multi-task%2520image%2520restoration%2520has%2520gained%2520significant%2520interest%2520due%2520to%2520its%250Ainherent%2520versatility%2520and%2520efficiency%2520compared%2520to%2520its%2520single-task%2520counterpart.%250AHowever%252C%2520performance%2520decline%2520is%2520observed%2520with%2520an%2520increase%2520in%2520the%2520number%2520of%250Atasks%252C%2520primarily%2520attributed%2520to%2520the%2520restoration%2520model%2527s%2520challenge%2520in%2520handling%250Adifferent%2520tasks%2520with%2520distinct%2520natures%2520at%2520the%2520same%2520time.%2520Thus%252C%2520a%2520perspective%250Aemerged%2520aiming%2520to%2520explore%2520the%2520degradation-insensitive%2520semantic%2520commonalities%250Aamong%2520different%2520degradation%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520observe%2520that%2520the%2520features%250Aof%2520DINOv2%2520can%2520effectively%2520model%2520semantic%2520information%2520and%2520are%2520independent%2520of%250Adegradation%2520factors.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%250A%255Cmbox%257B%255Ctextbf%257BDINO-IR%257D%257D%252C%2520a%2520multi-task%2520image%2520restoration%2520approach%2520leveraging%250Arobust%2520features%2520extracted%2520from%2520DINOv2%2520to%2520solve%2520multi-task%2520image%2520restoration%250Asimultaneously.%2520We%2520first%2520propose%2520a%2520pixel-semantic%2520fusion%2520%2528PSF%2529%2520module%2520to%250Adynamically%2520fuse%2520DINOV2%2527s%2520shallow%2520features%2520containing%2520pixel-level%2520information%250Aand%2520deep%2520features%2520containing%2520degradation-independent%2520semantic%2520information.%2520To%250Aguide%2520the%2520restoration%2520model%2520with%2520the%2520features%2520of%2520DINOv2%252C%2520we%2520develop%2520a%250ADINO-Restore%2520adaption%2520and%2520fusion%2520module%2520to%2520adjust%2520the%2520channel%2520of%2520fused%2520features%250Afrom%2520PSF%2520and%2520then%2520integrate%2520them%2520with%2520the%2520features%2520from%2520the%2520restoration%2520model.%250ABy%2520formulating%2520these%2520modules%2520into%2520a%2520unified%2520deep%2520model%252C%2520we%2520propose%2520a%2520DINO%250Aperception%2520contrastive%2520loss%2520to%2520constrain%2520the%2520model%2520training.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520DINO-IR%2520performs%2520favorably%2520against%250Aexisting%2520multi-task%2520image%2520restoration%2520approaches%2520in%2520various%2520tasks%2520by%2520a%2520large%250Amargin.%2520The%2520source%2520codes%2520and%2520trained%2520models%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20Image%20Restoration%20Guided%20By%20Robust%20DINO%20Features&entry.906535625=Xin%20Lin%20and%20Jingtong%20Yue%20and%20Kelvin%20C.%20K.%20Chan%20and%20Lu%20Qi%20and%20Chao%20Ren%20and%20Jinshan%20Pan%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Multi-task%20image%20restoration%20has%20gained%20significant%20interest%20due%20to%20its%0Ainherent%20versatility%20and%20efficiency%20compared%20to%20its%20single-task%20counterpart.%0AHowever%2C%20performance%20decline%20is%20observed%20with%20an%20increase%20in%20the%20number%20of%0Atasks%2C%20primarily%20attributed%20to%20the%20restoration%20model%27s%20challenge%20in%20handling%0Adifferent%20tasks%20with%20distinct%20natures%20at%20the%20same%20time.%20Thus%2C%20a%20perspective%0Aemerged%20aiming%20to%20explore%20the%20degradation-insensitive%20semantic%20commonalities%0Aamong%20different%20degradation%20tasks.%20In%20this%20paper%2C%20we%20observe%20that%20the%20features%0Aof%20DINOv2%20can%20effectively%20model%20semantic%20information%20and%20are%20independent%20of%0Adegradation%20factors.%20Motivated%20by%20this%20observation%2C%20we%20propose%0A%5Cmbox%7B%5Ctextbf%7BDINO-IR%7D%7D%2C%20a%20multi-task%20image%20restoration%20approach%20leveraging%0Arobust%20features%20extracted%20from%20DINOv2%20to%20solve%20multi-task%20image%20restoration%0Asimultaneously.%20We%20first%20propose%20a%20pixel-semantic%20fusion%20%28PSF%29%20module%20to%0Adynamically%20fuse%20DINOV2%27s%20shallow%20features%20containing%20pixel-level%20information%0Aand%20deep%20features%20containing%20degradation-independent%20semantic%20information.%20To%0Aguide%20the%20restoration%20model%20with%20the%20features%20of%20DINOv2%2C%20we%20develop%20a%0ADINO-Restore%20adaption%20and%20fusion%20module%20to%20adjust%20the%20channel%20of%20fused%20features%0Afrom%20PSF%20and%20then%20integrate%20them%20with%20the%20features%20from%20the%20restoration%20model.%0ABy%20formulating%20these%20modules%20into%20a%20unified%20deep%20model%2C%20we%20propose%20a%20DINO%0Aperception%20contrastive%20loss%20to%20constrain%20the%20model%20training.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20DINO-IR%20performs%20favorably%20against%0Aexisting%20multi-task%20image%20restoration%20approaches%20in%20various%20tasks%20by%20a%20large%0Amargin.%20The%20source%20codes%20and%20trained%20models%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01677v3&entry.124074799=Read"},
{"title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area", "author": "Junxian Li and Di Zhang and Xunzhi Wang and Zeying Hao and Jingdi Lei and Qian Tan and Cai Zhou and Wei Liu and Yaotian Yang and Xinrui Xiong and Weiyun Wang and Zhe Chen and Wenhai Wang and Wei Li and Shufei Zhang and Mao Su and Wanli Ouyang and Yuqiang Li and Dongzhan Zhou", "abstract": "  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n", "link": "http://arxiv.org/abs/2408.07246v2", "date": "2024-08-16", "relevancy": 1.6148, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChemVLM%3A%20Exploring%20the%20Power%20of%20Multimodal%20Large%20Language%20Models%20in%0A%20%20Chemistry%20Area&body=Title%3A%20ChemVLM%3A%20Exploring%20the%20Power%20of%20Multimodal%20Large%20Language%20Models%20in%0A%20%20Chemistry%20Area%0AAuthor%3A%20Junxian%20Li%20and%20Di%20Zhang%20and%20Xunzhi%20Wang%20and%20Zeying%20Hao%20and%20Jingdi%20Lei%20and%20Qian%20Tan%20and%20Cai%20Zhou%20and%20Wei%20Liu%20and%20Yaotian%20Yang%20and%20Xinrui%20Xiong%20and%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Wei%20Li%20and%20Shufei%20Zhang%20and%20Mao%20Su%20and%20Wanli%20Ouyang%20and%20Yuqiang%20Li%20and%20Dongzhan%20Zhou%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20and%20have%20been%0Aapplied%20across%20various%20scientific%20fields%2C%20including%20chemistry.%20However%2C%20many%0Achemical%20tasks%20require%20the%20processing%20of%20visual%20information%2C%20which%20cannot%20be%0Asuccessfully%20handled%20by%20existing%20chemical%20LLMs.%20This%20brings%20a%20growing%20need%20for%0Amodels%20capable%20of%20integrating%20multimodal%20information%20in%20the%20chemical%20domain.%20In%0Athis%20paper%2C%20we%20introduce%20%5Ctextbf%7BChemVLM%7D%2C%20an%20open-source%20chemical%20multimodal%0Alarge%20language%20model%20specifically%20designed%20for%20chemical%20applications.%20ChemVLM%0Ais%20trained%20on%20a%20carefully%20curated%20bilingual%20multimodal%20dataset%20that%20enhances%0Aits%20ability%20to%20understand%20both%20textual%20and%20visual%20chemical%20information%2C%0Aincluding%20molecular%20structures%2C%20reactions%2C%20and%20chemistry%20examination%20questions.%0AWe%20develop%20three%20datasets%20for%20comprehensive%20evaluation%2C%20tailored%20to%20Chemical%0AOptical%20Character%20Recognition%20%28OCR%29%2C%20Multimodal%20Chemical%20Reasoning%20%28MMCR%29%2C%20and%0AMultimodal%20Molecule%20Understanding%20tasks.%20We%20benchmark%20ChemVLM%20against%20a%20range%0Aof%20open-source%20and%20proprietary%20multimodal%20large%20language%20models%20on%20various%0Atasks.%20Experimental%20results%20demonstrate%20that%20ChemVLM%20achieves%20competitive%0Aperformance%20across%20all%20evaluated%20tasks.%20Our%20model%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/AI4Chem/ChemVLM-26B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07246v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemVLM%253A%2520Exploring%2520the%2520Power%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520in%250A%2520%2520Chemistry%2520Area%26entry.906535625%3DJunxian%2520Li%2520and%2520Di%2520Zhang%2520and%2520Xunzhi%2520Wang%2520and%2520Zeying%2520Hao%2520and%2520Jingdi%2520Lei%2520and%2520Qian%2520Tan%2520and%2520Cai%2520Zhou%2520and%2520Wei%2520Liu%2520and%2520Yaotian%2520Yang%2520and%2520Xinrui%2520Xiong%2520and%2520Weiyun%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Wenhai%2520Wang%2520and%2520Wei%2520Li%2520and%2520Shufei%2520Zhang%2520and%2520Mao%2520Su%2520and%2520Wanli%2520Ouyang%2520and%2520Yuqiang%2520Li%2520and%2520Dongzhan%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520and%2520have%2520been%250Aapplied%2520across%2520various%2520scientific%2520fields%252C%2520including%2520chemistry.%2520However%252C%2520many%250Achemical%2520tasks%2520require%2520the%2520processing%2520of%2520visual%2520information%252C%2520which%2520cannot%2520be%250Asuccessfully%2520handled%2520by%2520existing%2520chemical%2520LLMs.%2520This%2520brings%2520a%2520growing%2520need%2520for%250Amodels%2520capable%2520of%2520integrating%2520multimodal%2520information%2520in%2520the%2520chemical%2520domain.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BChemVLM%257D%252C%2520an%2520open-source%2520chemical%2520multimodal%250Alarge%2520language%2520model%2520specifically%2520designed%2520for%2520chemical%2520applications.%2520ChemVLM%250Ais%2520trained%2520on%2520a%2520carefully%2520curated%2520bilingual%2520multimodal%2520dataset%2520that%2520enhances%250Aits%2520ability%2520to%2520understand%2520both%2520textual%2520and%2520visual%2520chemical%2520information%252C%250Aincluding%2520molecular%2520structures%252C%2520reactions%252C%2520and%2520chemistry%2520examination%2520questions.%250AWe%2520develop%2520three%2520datasets%2520for%2520comprehensive%2520evaluation%252C%2520tailored%2520to%2520Chemical%250AOptical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520Multimodal%2520Chemical%2520Reasoning%2520%2528MMCR%2529%252C%2520and%250AMultimodal%2520Molecule%2520Understanding%2520tasks.%2520We%2520benchmark%2520ChemVLM%2520against%2520a%2520range%250Aof%2520open-source%2520and%2520proprietary%2520multimodal%2520large%2520language%2520models%2520on%2520various%250Atasks.%2520Experimental%2520results%2520demonstrate%2520that%2520ChemVLM%2520achieves%2520competitive%250Aperformance%2520across%2520all%2520evaluated%2520tasks.%2520Our%2520model%2520can%2520be%2520found%2520at%250Ahttps%253A//huggingface.co/AI4Chem/ChemVLM-26B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07246v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChemVLM%3A%20Exploring%20the%20Power%20of%20Multimodal%20Large%20Language%20Models%20in%0A%20%20Chemistry%20Area&entry.906535625=Junxian%20Li%20and%20Di%20Zhang%20and%20Xunzhi%20Wang%20and%20Zeying%20Hao%20and%20Jingdi%20Lei%20and%20Qian%20Tan%20and%20Cai%20Zhou%20and%20Wei%20Liu%20and%20Yaotian%20Yang%20and%20Xinrui%20Xiong%20and%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wenhai%20Wang%20and%20Wei%20Li%20and%20Shufei%20Zhang%20and%20Mao%20Su%20and%20Wanli%20Ouyang%20and%20Yuqiang%20Li%20and%20Dongzhan%20Zhou&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20and%20have%20been%0Aapplied%20across%20various%20scientific%20fields%2C%20including%20chemistry.%20However%2C%20many%0Achemical%20tasks%20require%20the%20processing%20of%20visual%20information%2C%20which%20cannot%20be%0Asuccessfully%20handled%20by%20existing%20chemical%20LLMs.%20This%20brings%20a%20growing%20need%20for%0Amodels%20capable%20of%20integrating%20multimodal%20information%20in%20the%20chemical%20domain.%20In%0Athis%20paper%2C%20we%20introduce%20%5Ctextbf%7BChemVLM%7D%2C%20an%20open-source%20chemical%20multimodal%0Alarge%20language%20model%20specifically%20designed%20for%20chemical%20applications.%20ChemVLM%0Ais%20trained%20on%20a%20carefully%20curated%20bilingual%20multimodal%20dataset%20that%20enhances%0Aits%20ability%20to%20understand%20both%20textual%20and%20visual%20chemical%20information%2C%0Aincluding%20molecular%20structures%2C%20reactions%2C%20and%20chemistry%20examination%20questions.%0AWe%20develop%20three%20datasets%20for%20comprehensive%20evaluation%2C%20tailored%20to%20Chemical%0AOptical%20Character%20Recognition%20%28OCR%29%2C%20Multimodal%20Chemical%20Reasoning%20%28MMCR%29%2C%20and%0AMultimodal%20Molecule%20Understanding%20tasks.%20We%20benchmark%20ChemVLM%20against%20a%20range%0Aof%20open-source%20and%20proprietary%20multimodal%20large%20language%20models%20on%20various%0Atasks.%20Experimental%20results%20demonstrate%20that%20ChemVLM%20achieves%20competitive%0Aperformance%20across%20all%20evaluated%20tasks.%20Our%20model%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/AI4Chem/ChemVLM-26B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07246v2&entry.124074799=Read"},
{"title": "AI-as-exploration: Navigating intelligence space", "author": "Dimitri Coelho Mollo", "abstract": "  Artificial Intelligence is a field that lives many lives, and the term has\ncome to encompass a motley collection of scientific and commercial endeavours.\nIn this paper, I articulate the contours of a rather neglected but central\nscientific role that AI has to play, which I dub `AI-as-exploration'.The basic\nthrust of AI-as-exploration is that of creating and studying systems that can\nreveal candidate building blocks of intelligence that may differ from the forms\nof human and animal intelligence we are familiar with. In other words, I\nsuggest that AI is one of the best tools we have for exploring intelligence\nspace, namely the space of possible intelligent systems. I illustrate the value\nof AI-as-exploration by focusing on a specific case study, i.e., recent work on\nthe capacity to combine novel and invented concepts in humans and Large\nLanguage Models. I show that the latter, despite showing human-level accuracy\nin such a task, probably solve it in ways radically different, but no less\nrelevant to intelligence research, to those hypothesised for humans.\n", "link": "http://arxiv.org/abs/2401.07964v3", "date": "2024-08-16", "relevancy": 1.5554, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6035}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-as-exploration%3A%20Navigating%20intelligence%20space&body=Title%3A%20AI-as-exploration%3A%20Navigating%20intelligence%20space%0AAuthor%3A%20Dimitri%20Coelho%20Mollo%0AAbstract%3A%20%20%20Artificial%20Intelligence%20is%20a%20field%20that%20lives%20many%20lives%2C%20and%20the%20term%20has%0Acome%20to%20encompass%20a%20motley%20collection%20of%20scientific%20and%20commercial%20endeavours.%0AIn%20this%20paper%2C%20I%20articulate%20the%20contours%20of%20a%20rather%20neglected%20but%20central%0Ascientific%20role%20that%20AI%20has%20to%20play%2C%20which%20I%20dub%20%60AI-as-exploration%27.The%20basic%0Athrust%20of%20AI-as-exploration%20is%20that%20of%20creating%20and%20studying%20systems%20that%20can%0Areveal%20candidate%20building%20blocks%20of%20intelligence%20that%20may%20differ%20from%20the%20forms%0Aof%20human%20and%20animal%20intelligence%20we%20are%20familiar%20with.%20In%20other%20words%2C%20I%0Asuggest%20that%20AI%20is%20one%20of%20the%20best%20tools%20we%20have%20for%20exploring%20intelligence%0Aspace%2C%20namely%20the%20space%20of%20possible%20intelligent%20systems.%20I%20illustrate%20the%20value%0Aof%20AI-as-exploration%20by%20focusing%20on%20a%20specific%20case%20study%2C%20i.e.%2C%20recent%20work%20on%0Athe%20capacity%20to%20combine%20novel%20and%20invented%20concepts%20in%20humans%20and%20Large%0ALanguage%20Models.%20I%20show%20that%20the%20latter%2C%20despite%20showing%20human-level%20accuracy%0Ain%20such%20a%20task%2C%20probably%20solve%20it%20in%20ways%20radically%20different%2C%20but%20no%20less%0Arelevant%20to%20intelligence%20research%2C%20to%20those%20hypothesised%20for%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07964v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-as-exploration%253A%2520Navigating%2520intelligence%2520space%26entry.906535625%3DDimitri%2520Coelho%2520Mollo%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520is%2520a%2520field%2520that%2520lives%2520many%2520lives%252C%2520and%2520the%2520term%2520has%250Acome%2520to%2520encompass%2520a%2520motley%2520collection%2520of%2520scientific%2520and%2520commercial%2520endeavours.%250AIn%2520this%2520paper%252C%2520I%2520articulate%2520the%2520contours%2520of%2520a%2520rather%2520neglected%2520but%2520central%250Ascientific%2520role%2520that%2520AI%2520has%2520to%2520play%252C%2520which%2520I%2520dub%2520%2560AI-as-exploration%2527.The%2520basic%250Athrust%2520of%2520AI-as-exploration%2520is%2520that%2520of%2520creating%2520and%2520studying%2520systems%2520that%2520can%250Areveal%2520candidate%2520building%2520blocks%2520of%2520intelligence%2520that%2520may%2520differ%2520from%2520the%2520forms%250Aof%2520human%2520and%2520animal%2520intelligence%2520we%2520are%2520familiar%2520with.%2520In%2520other%2520words%252C%2520I%250Asuggest%2520that%2520AI%2520is%2520one%2520of%2520the%2520best%2520tools%2520we%2520have%2520for%2520exploring%2520intelligence%250Aspace%252C%2520namely%2520the%2520space%2520of%2520possible%2520intelligent%2520systems.%2520I%2520illustrate%2520the%2520value%250Aof%2520AI-as-exploration%2520by%2520focusing%2520on%2520a%2520specific%2520case%2520study%252C%2520i.e.%252C%2520recent%2520work%2520on%250Athe%2520capacity%2520to%2520combine%2520novel%2520and%2520invented%2520concepts%2520in%2520humans%2520and%2520Large%250ALanguage%2520Models.%2520I%2520show%2520that%2520the%2520latter%252C%2520despite%2520showing%2520human-level%2520accuracy%250Ain%2520such%2520a%2520task%252C%2520probably%2520solve%2520it%2520in%2520ways%2520radically%2520different%252C%2520but%2520no%2520less%250Arelevant%2520to%2520intelligence%2520research%252C%2520to%2520those%2520hypothesised%2520for%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07964v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-as-exploration%3A%20Navigating%20intelligence%20space&entry.906535625=Dimitri%20Coelho%20Mollo&entry.1292438233=%20%20Artificial%20Intelligence%20is%20a%20field%20that%20lives%20many%20lives%2C%20and%20the%20term%20has%0Acome%20to%20encompass%20a%20motley%20collection%20of%20scientific%20and%20commercial%20endeavours.%0AIn%20this%20paper%2C%20I%20articulate%20the%20contours%20of%20a%20rather%20neglected%20but%20central%0Ascientific%20role%20that%20AI%20has%20to%20play%2C%20which%20I%20dub%20%60AI-as-exploration%27.The%20basic%0Athrust%20of%20AI-as-exploration%20is%20that%20of%20creating%20and%20studying%20systems%20that%20can%0Areveal%20candidate%20building%20blocks%20of%20intelligence%20that%20may%20differ%20from%20the%20forms%0Aof%20human%20and%20animal%20intelligence%20we%20are%20familiar%20with.%20In%20other%20words%2C%20I%0Asuggest%20that%20AI%20is%20one%20of%20the%20best%20tools%20we%20have%20for%20exploring%20intelligence%0Aspace%2C%20namely%20the%20space%20of%20possible%20intelligent%20systems.%20I%20illustrate%20the%20value%0Aof%20AI-as-exploration%20by%20focusing%20on%20a%20specific%20case%20study%2C%20i.e.%2C%20recent%20work%20on%0Athe%20capacity%20to%20combine%20novel%20and%20invented%20concepts%20in%20humans%20and%20Large%0ALanguage%20Models.%20I%20show%20that%20the%20latter%2C%20despite%20showing%20human-level%20accuracy%0Ain%20such%20a%20task%2C%20probably%20solve%20it%20in%20ways%20radically%20different%2C%20but%20no%20less%0Arelevant%20to%20intelligence%20research%2C%20to%20those%20hypothesised%20for%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07964v3&entry.124074799=Read"},
{"title": "Mind the Privacy Unit! User-Level Differential Privacy for Language\n  Model Fine-Tuning", "author": "Lynn Chua and Badih Ghazi and Yangsibo Huang and Pritish Kamath and Ravi Kumar and Daogao Liu and Pasin Manurangsi and Amer Sinha and Chiyuan Zhang", "abstract": "  Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.\n", "link": "http://arxiv.org/abs/2406.14322v3", "date": "2024-08-16", "relevancy": 1.4109, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5179}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4585}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Privacy%20Unit%21%20User-Level%20Differential%20Privacy%20for%20Language%0A%20%20Model%20Fine-Tuning&body=Title%3A%20Mind%20the%20Privacy%20Unit%21%20User-Level%20Differential%20Privacy%20for%20Language%0A%20%20Model%20Fine-Tuning%0AAuthor%3A%20Lynn%20Chua%20and%20Badih%20Ghazi%20and%20Yangsibo%20Huang%20and%20Pritish%20Kamath%20and%20Ravi%20Kumar%20and%20Daogao%20Liu%20and%20Pasin%20Manurangsi%20and%20Amer%20Sinha%20and%20Chiyuan%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20tackling%0Acomplex%20tasks%20across%20diverse%20domains%2C%20but%20they%20also%20raise%20privacy%20concerns%20when%0Afine-tuned%20on%20sensitive%20data%20due%20to%20potential%20memorization.%20While%20differential%0Aprivacy%20%28DP%29%20offers%20a%20promising%20solution%20by%20ensuring%20models%20are%20%27almost%0Aindistinguishable%27%20with%20or%20without%20any%20particular%20privacy%20unit%2C%20current%0Aevaluations%20on%20LLMs%20mostly%20treat%20each%20example%20%28text%20record%29%20as%20the%20privacy%0Aunit.%20This%20leads%20to%20uneven%20user%20privacy%20guarantees%20when%20contributions%20per%20user%0Avary.%20We%20therefore%20study%20user-level%20DP%20motivated%20by%20applications%20where%20it%0Anecessary%20to%20ensure%20uniform%20privacy%20protection%20across%20users.%20We%20present%20a%0Asystematic%20evaluation%20of%20user-level%20DP%20for%20LLM%20fine-tuning%20on%20natural%20language%0Ageneration%20tasks.%20Focusing%20on%20two%20mechanisms%20for%20achieving%20user-level%20DP%0Aguarantees%2C%20Group%20Privacy%20and%20User-wise%20DP-SGD%2C%20we%20investigate%20design%20choices%0Alike%20data%20selection%20strategies%20and%20parameter%20tuning%20for%20the%20best%0Aprivacy-utility%20tradeoff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Privacy%2520Unit%2521%2520User-Level%2520Differential%2520Privacy%2520for%2520Language%250A%2520%2520Model%2520Fine-Tuning%26entry.906535625%3DLynn%2520Chua%2520and%2520Badih%2520Ghazi%2520and%2520Yangsibo%2520Huang%2520and%2520Pritish%2520Kamath%2520and%2520Ravi%2520Kumar%2520and%2520Daogao%2520Liu%2520and%2520Pasin%2520Manurangsi%2520and%2520Amer%2520Sinha%2520and%2520Chiyuan%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520tackling%250Acomplex%2520tasks%2520across%2520diverse%2520domains%252C%2520but%2520they%2520also%2520raise%2520privacy%2520concerns%2520when%250Afine-tuned%2520on%2520sensitive%2520data%2520due%2520to%2520potential%2520memorization.%2520While%2520differential%250Aprivacy%2520%2528DP%2529%2520offers%2520a%2520promising%2520solution%2520by%2520ensuring%2520models%2520are%2520%2527almost%250Aindistinguishable%2527%2520with%2520or%2520without%2520any%2520particular%2520privacy%2520unit%252C%2520current%250Aevaluations%2520on%2520LLMs%2520mostly%2520treat%2520each%2520example%2520%2528text%2520record%2529%2520as%2520the%2520privacy%250Aunit.%2520This%2520leads%2520to%2520uneven%2520user%2520privacy%2520guarantees%2520when%2520contributions%2520per%2520user%250Avary.%2520We%2520therefore%2520study%2520user-level%2520DP%2520motivated%2520by%2520applications%2520where%2520it%250Anecessary%2520to%2520ensure%2520uniform%2520privacy%2520protection%2520across%2520users.%2520We%2520present%2520a%250Asystematic%2520evaluation%2520of%2520user-level%2520DP%2520for%2520LLM%2520fine-tuning%2520on%2520natural%2520language%250Ageneration%2520tasks.%2520Focusing%2520on%2520two%2520mechanisms%2520for%2520achieving%2520user-level%2520DP%250Aguarantees%252C%2520Group%2520Privacy%2520and%2520User-wise%2520DP-SGD%252C%2520we%2520investigate%2520design%2520choices%250Alike%2520data%2520selection%2520strategies%2520and%2520parameter%2520tuning%2520for%2520the%2520best%250Aprivacy-utility%2520tradeoff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Privacy%20Unit%21%20User-Level%20Differential%20Privacy%20for%20Language%0A%20%20Model%20Fine-Tuning&entry.906535625=Lynn%20Chua%20and%20Badih%20Ghazi%20and%20Yangsibo%20Huang%20and%20Pritish%20Kamath%20and%20Ravi%20Kumar%20and%20Daogao%20Liu%20and%20Pasin%20Manurangsi%20and%20Amer%20Sinha%20and%20Chiyuan%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20tackling%0Acomplex%20tasks%20across%20diverse%20domains%2C%20but%20they%20also%20raise%20privacy%20concerns%20when%0Afine-tuned%20on%20sensitive%20data%20due%20to%20potential%20memorization.%20While%20differential%0Aprivacy%20%28DP%29%20offers%20a%20promising%20solution%20by%20ensuring%20models%20are%20%27almost%0Aindistinguishable%27%20with%20or%20without%20any%20particular%20privacy%20unit%2C%20current%0Aevaluations%20on%20LLMs%20mostly%20treat%20each%20example%20%28text%20record%29%20as%20the%20privacy%0Aunit.%20This%20leads%20to%20uneven%20user%20privacy%20guarantees%20when%20contributions%20per%20user%0Avary.%20We%20therefore%20study%20user-level%20DP%20motivated%20by%20applications%20where%20it%0Anecessary%20to%20ensure%20uniform%20privacy%20protection%20across%20users.%20We%20present%20a%0Asystematic%20evaluation%20of%20user-level%20DP%20for%20LLM%20fine-tuning%20on%20natural%20language%0Ageneration%20tasks.%20Focusing%20on%20two%20mechanisms%20for%20achieving%20user-level%20DP%0Aguarantees%2C%20Group%20Privacy%20and%20User-wise%20DP-SGD%2C%20we%20investigate%20design%20choices%0Alike%20data%20selection%20strategies%20and%20parameter%20tuning%20for%20the%20best%0Aprivacy-utility%20tradeoff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14322v3&entry.124074799=Read"},
{"title": "SYMPOL: Symbolic Tree-Based On-Policy Reinforcement Learning", "author": "Sascha Marton and Tim Grams and Florian Vogt and Stefan L\u00fcdtke and Christian Bartelt and Heiner Stuckenschmidt", "abstract": "  Reinforcement learning (RL) has seen significant success across various\ndomains, but its adoption is often limited by the black-box nature of neural\nnetwork policies, making them difficult to interpret. In contrast, symbolic\npolicies allow representing decision-making strategies in a compact and\ninterpretable way. However, learning symbolic policies directly within\non-policy methods remains challenging. In this paper, we introduce SYMPOL, a\nnovel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based\nmodel integrated with a policy gradient method, enabling the agent to learn and\nadapt its actions while maintaining a high level of interpretability. We\nevaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority\nover alternative tree-based RL approaches in terms of performance and\ninterpretability. To the best of our knowledge, this is the first method, that\nallows a gradient-based end-to-end learning of interpretable, axis-aligned\ndecision trees on-policy. Therefore, SYMPOL can become the foundation for a new\nclass of interpretable RL based on decision trees. Our implementation is\navailable under: https://github.com/s-marton/SYMPOL\n", "link": "http://arxiv.org/abs/2408.08761v1", "date": "2024-08-16", "relevancy": 1.3186, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4646}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4516}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SYMPOL%3A%20Symbolic%20Tree-Based%20On-Policy%20Reinforcement%20Learning&body=Title%3A%20SYMPOL%3A%20Symbolic%20Tree-Based%20On-Policy%20Reinforcement%20Learning%0AAuthor%3A%20Sascha%20Marton%20and%20Tim%20Grams%20and%20Florian%20Vogt%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20seen%20significant%20success%20across%20various%0Adomains%2C%20but%20its%20adoption%20is%20often%20limited%20by%20the%20black-box%20nature%20of%20neural%0Anetwork%20policies%2C%20making%20them%20difficult%20to%20interpret.%20In%20contrast%2C%20symbolic%0Apolicies%20allow%20representing%20decision-making%20strategies%20in%20a%20compact%20and%0Ainterpretable%20way.%20However%2C%20learning%20symbolic%20policies%20directly%20within%0Aon-policy%20methods%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20SYMPOL%2C%20a%0Anovel%20method%20for%20SYMbolic%20tree-based%20on-POLicy%20RL.%20SYMPOL%20employs%20a%20tree-based%0Amodel%20integrated%20with%20a%20policy%20gradient%20method%2C%20enabling%20the%20agent%20to%20learn%20and%0Aadapt%20its%20actions%20while%20maintaining%20a%20high%20level%20of%20interpretability.%20We%0Aevaluate%20SYMPOL%20on%20a%20set%20of%20benchmark%20RL%20tasks%2C%20demonstrating%20its%20superiority%0Aover%20alternative%20tree-based%20RL%20approaches%20in%20terms%20of%20performance%20and%0Ainterpretability.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20method%2C%20that%0Aallows%20a%20gradient-based%20end-to-end%20learning%20of%20interpretable%2C%20axis-aligned%0Adecision%20trees%20on-policy.%20Therefore%2C%20SYMPOL%20can%20become%20the%20foundation%20for%20a%20new%0Aclass%20of%20interpretable%20RL%20based%20on%20decision%20trees.%20Our%20implementation%20is%0Aavailable%20under%3A%20https%3A//github.com/s-marton/SYMPOL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSYMPOL%253A%2520Symbolic%2520Tree-Based%2520On-Policy%2520Reinforcement%2520Learning%26entry.906535625%3DSascha%2520Marton%2520and%2520Tim%2520Grams%2520and%2520Florian%2520Vogt%2520and%2520Stefan%2520L%25C3%25BCdtke%2520and%2520Christian%2520Bartelt%2520and%2520Heiner%2520Stuckenschmidt%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520seen%2520significant%2520success%2520across%2520various%250Adomains%252C%2520but%2520its%2520adoption%2520is%2520often%2520limited%2520by%2520the%2520black-box%2520nature%2520of%2520neural%250Anetwork%2520policies%252C%2520making%2520them%2520difficult%2520to%2520interpret.%2520In%2520contrast%252C%2520symbolic%250Apolicies%2520allow%2520representing%2520decision-making%2520strategies%2520in%2520a%2520compact%2520and%250Ainterpretable%2520way.%2520However%252C%2520learning%2520symbolic%2520policies%2520directly%2520within%250Aon-policy%2520methods%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SYMPOL%252C%2520a%250Anovel%2520method%2520for%2520SYMbolic%2520tree-based%2520on-POLicy%2520RL.%2520SYMPOL%2520employs%2520a%2520tree-based%250Amodel%2520integrated%2520with%2520a%2520policy%2520gradient%2520method%252C%2520enabling%2520the%2520agent%2520to%2520learn%2520and%250Aadapt%2520its%2520actions%2520while%2520maintaining%2520a%2520high%2520level%2520of%2520interpretability.%2520We%250Aevaluate%2520SYMPOL%2520on%2520a%2520set%2520of%2520benchmark%2520RL%2520tasks%252C%2520demonstrating%2520its%2520superiority%250Aover%2520alternative%2520tree-based%2520RL%2520approaches%2520in%2520terms%2520of%2520performance%2520and%250Ainterpretability.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520method%252C%2520that%250Aallows%2520a%2520gradient-based%2520end-to-end%2520learning%2520of%2520interpretable%252C%2520axis-aligned%250Adecision%2520trees%2520on-policy.%2520Therefore%252C%2520SYMPOL%2520can%2520become%2520the%2520foundation%2520for%2520a%2520new%250Aclass%2520of%2520interpretable%2520RL%2520based%2520on%2520decision%2520trees.%2520Our%2520implementation%2520is%250Aavailable%2520under%253A%2520https%253A//github.com/s-marton/SYMPOL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SYMPOL%3A%20Symbolic%20Tree-Based%20On-Policy%20Reinforcement%20Learning&entry.906535625=Sascha%20Marton%20and%20Tim%20Grams%20and%20Florian%20Vogt%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20seen%20significant%20success%20across%20various%0Adomains%2C%20but%20its%20adoption%20is%20often%20limited%20by%20the%20black-box%20nature%20of%20neural%0Anetwork%20policies%2C%20making%20them%20difficult%20to%20interpret.%20In%20contrast%2C%20symbolic%0Apolicies%20allow%20representing%20decision-making%20strategies%20in%20a%20compact%20and%0Ainterpretable%20way.%20However%2C%20learning%20symbolic%20policies%20directly%20within%0Aon-policy%20methods%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20SYMPOL%2C%20a%0Anovel%20method%20for%20SYMbolic%20tree-based%20on-POLicy%20RL.%20SYMPOL%20employs%20a%20tree-based%0Amodel%20integrated%20with%20a%20policy%20gradient%20method%2C%20enabling%20the%20agent%20to%20learn%20and%0Aadapt%20its%20actions%20while%20maintaining%20a%20high%20level%20of%20interpretability.%20We%0Aevaluate%20SYMPOL%20on%20a%20set%20of%20benchmark%20RL%20tasks%2C%20demonstrating%20its%20superiority%0Aover%20alternative%20tree-based%20RL%20approaches%20in%20terms%20of%20performance%20and%0Ainterpretability.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20method%2C%20that%0Aallows%20a%20gradient-based%20end-to-end%20learning%20of%20interpretable%2C%20axis-aligned%0Adecision%20trees%20on-policy.%20Therefore%2C%20SYMPOL%20can%20become%20the%20foundation%20for%20a%20new%0Aclass%20of%20interpretable%20RL%20based%20on%20decision%20trees.%20Our%20implementation%20is%0Aavailable%20under%3A%20https%3A//github.com/s-marton/SYMPOL%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08761v1&entry.124074799=Read"},
{"title": "CollaFuse: Navigating Limited Resources and Privacy in Collaborative\n  Generative AI", "author": "Domenique Zipperling and Simeon Allmendinger and Lukas Struppek and Niklas K\u00fchl", "abstract": "  In the landscape of generative artificial intelligence, diffusion-based\nmodels present challenges for socio-technical systems in data requirements and\nprivacy. Traditional approaches like federated learning distribute the learning\nprocess but strain individual clients, especially with constrained resources\n(e.g., edge devices). In response to these challenges, we introduce CollaFuse,\na novel framework inspired by split learning. Tailored for efficient and\ncollaborative use of denoising diffusion probabilistic models, CollaFuse\nenables shared server training and inference, alleviating client computational\nburdens. This is achieved by retaining data and computationally inexpensive GPU\nprocesses locally at each client while outsourcing the computationally\nexpensive processes to the shared server. Demonstrated in a healthcare context,\nCollaFuse enhances privacy by highly reducing the need for sensitive\ninformation sharing. These capabilities hold the potential to impact various\napplication areas, such as the design of edge computing solutions, healthcare\nresearch, or autonomous driving. In essence, our work advances distributed\nmachine learning, shaping the future of collaborative GenAI networks.\n", "link": "http://arxiv.org/abs/2402.19105v2", "date": "2024-08-16", "relevancy": 1.5192, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5171}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5062}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CollaFuse%3A%20Navigating%20Limited%20Resources%20and%20Privacy%20in%20Collaborative%0A%20%20Generative%20AI&body=Title%3A%20CollaFuse%3A%20Navigating%20Limited%20Resources%20and%20Privacy%20in%20Collaborative%0A%20%20Generative%20AI%0AAuthor%3A%20Domenique%20Zipperling%20and%20Simeon%20Allmendinger%20and%20Lukas%20Struppek%20and%20Niklas%20K%C3%BChl%0AAbstract%3A%20%20%20In%20the%20landscape%20of%20generative%20artificial%20intelligence%2C%20diffusion-based%0Amodels%20present%20challenges%20for%20socio-technical%20systems%20in%20data%20requirements%20and%0Aprivacy.%20Traditional%20approaches%20like%20federated%20learning%20distribute%20the%20learning%0Aprocess%20but%20strain%20individual%20clients%2C%20especially%20with%20constrained%20resources%0A%28e.g.%2C%20edge%20devices%29.%20In%20response%20to%20these%20challenges%2C%20we%20introduce%20CollaFuse%2C%0Aa%20novel%20framework%20inspired%20by%20split%20learning.%20Tailored%20for%20efficient%20and%0Acollaborative%20use%20of%20denoising%20diffusion%20probabilistic%20models%2C%20CollaFuse%0Aenables%20shared%20server%20training%20and%20inference%2C%20alleviating%20client%20computational%0Aburdens.%20This%20is%20achieved%20by%20retaining%20data%20and%20computationally%20inexpensive%20GPU%0Aprocesses%20locally%20at%20each%20client%20while%20outsourcing%20the%20computationally%0Aexpensive%20processes%20to%20the%20shared%20server.%20Demonstrated%20in%20a%20healthcare%20context%2C%0ACollaFuse%20enhances%20privacy%20by%20highly%20reducing%20the%20need%20for%20sensitive%0Ainformation%20sharing.%20These%20capabilities%20hold%20the%20potential%20to%20impact%20various%0Aapplication%20areas%2C%20such%20as%20the%20design%20of%20edge%20computing%20solutions%2C%20healthcare%0Aresearch%2C%20or%20autonomous%20driving.%20In%20essence%2C%20our%20work%20advances%20distributed%0Amachine%20learning%2C%20shaping%20the%20future%20of%20collaborative%20GenAI%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaFuse%253A%2520Navigating%2520Limited%2520Resources%2520and%2520Privacy%2520in%2520Collaborative%250A%2520%2520Generative%2520AI%26entry.906535625%3DDomenique%2520Zipperling%2520and%2520Simeon%2520Allmendinger%2520and%2520Lukas%2520Struppek%2520and%2520Niklas%2520K%25C3%25BChl%26entry.1292438233%3D%2520%2520In%2520the%2520landscape%2520of%2520generative%2520artificial%2520intelligence%252C%2520diffusion-based%250Amodels%2520present%2520challenges%2520for%2520socio-technical%2520systems%2520in%2520data%2520requirements%2520and%250Aprivacy.%2520Traditional%2520approaches%2520like%2520federated%2520learning%2520distribute%2520the%2520learning%250Aprocess%2520but%2520strain%2520individual%2520clients%252C%2520especially%2520with%2520constrained%2520resources%250A%2528e.g.%252C%2520edge%2520devices%2529.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520introduce%2520CollaFuse%252C%250Aa%2520novel%2520framework%2520inspired%2520by%2520split%2520learning.%2520Tailored%2520for%2520efficient%2520and%250Acollaborative%2520use%2520of%2520denoising%2520diffusion%2520probabilistic%2520models%252C%2520CollaFuse%250Aenables%2520shared%2520server%2520training%2520and%2520inference%252C%2520alleviating%2520client%2520computational%250Aburdens.%2520This%2520is%2520achieved%2520by%2520retaining%2520data%2520and%2520computationally%2520inexpensive%2520GPU%250Aprocesses%2520locally%2520at%2520each%2520client%2520while%2520outsourcing%2520the%2520computationally%250Aexpensive%2520processes%2520to%2520the%2520shared%2520server.%2520Demonstrated%2520in%2520a%2520healthcare%2520context%252C%250ACollaFuse%2520enhances%2520privacy%2520by%2520highly%2520reducing%2520the%2520need%2520for%2520sensitive%250Ainformation%2520sharing.%2520These%2520capabilities%2520hold%2520the%2520potential%2520to%2520impact%2520various%250Aapplication%2520areas%252C%2520such%2520as%2520the%2520design%2520of%2520edge%2520computing%2520solutions%252C%2520healthcare%250Aresearch%252C%2520or%2520autonomous%2520driving.%2520In%2520essence%252C%2520our%2520work%2520advances%2520distributed%250Amachine%2520learning%252C%2520shaping%2520the%2520future%2520of%2520collaborative%2520GenAI%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CollaFuse%3A%20Navigating%20Limited%20Resources%20and%20Privacy%20in%20Collaborative%0A%20%20Generative%20AI&entry.906535625=Domenique%20Zipperling%20and%20Simeon%20Allmendinger%20and%20Lukas%20Struppek%20and%20Niklas%20K%C3%BChl&entry.1292438233=%20%20In%20the%20landscape%20of%20generative%20artificial%20intelligence%2C%20diffusion-based%0Amodels%20present%20challenges%20for%20socio-technical%20systems%20in%20data%20requirements%20and%0Aprivacy.%20Traditional%20approaches%20like%20federated%20learning%20distribute%20the%20learning%0Aprocess%20but%20strain%20individual%20clients%2C%20especially%20with%20constrained%20resources%0A%28e.g.%2C%20edge%20devices%29.%20In%20response%20to%20these%20challenges%2C%20we%20introduce%20CollaFuse%2C%0Aa%20novel%20framework%20inspired%20by%20split%20learning.%20Tailored%20for%20efficient%20and%0Acollaborative%20use%20of%20denoising%20diffusion%20probabilistic%20models%2C%20CollaFuse%0Aenables%20shared%20server%20training%20and%20inference%2C%20alleviating%20client%20computational%0Aburdens.%20This%20is%20achieved%20by%20retaining%20data%20and%20computationally%20inexpensive%20GPU%0Aprocesses%20locally%20at%20each%20client%20while%20outsourcing%20the%20computationally%0Aexpensive%20processes%20to%20the%20shared%20server.%20Demonstrated%20in%20a%20healthcare%20context%2C%0ACollaFuse%20enhances%20privacy%20by%20highly%20reducing%20the%20need%20for%20sensitive%0Ainformation%20sharing.%20These%20capabilities%20hold%20the%20potential%20to%20impact%20various%0Aapplication%20areas%2C%20such%20as%20the%20design%20of%20edge%20computing%20solutions%2C%20healthcare%0Aresearch%2C%20or%20autonomous%20driving.%20In%20essence%2C%20our%20work%20advances%20distributed%0Amachine%20learning%2C%20shaping%20the%20future%20of%20collaborative%20GenAI%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19105v2&entry.124074799=Read"},
{"title": "Multi-task Learning Approach for Intracranial Hemorrhage Prognosis", "author": "Miriam Cobo and Amaia P\u00e9rez del Barrio and Pablo Men\u00e9ndez Fern\u00e1ndez-Miranda and Pablo Sanz Bell\u00f3n and Lara Lloret Iglesias and Wilson Silva", "abstract": "  Prognosis after intracranial hemorrhage (ICH) is influenced by a complex\ninterplay between imaging and tabular data. Rapid and reliable prognosis are\ncrucial for effective patient stratification and informed treatment\ndecision-making. In this study, we aim to enhance image-based prognosis by\nlearning a robust feature representation shared between prognosis and the\nclinical and demographic variables most highly correlated with it. Our approach\nmimics clinical decision-making by reinforcing the model to learn valuable\nprognostic data embedded in the image. We propose a 3D multi-task image model\nto predict prognosis, Glasgow Coma Scale and age, improving accuracy and\ninterpretability. Our method outperforms current state-of-the-art baseline\nimage models, and demonstrates superior performance in ICH prognosis compared\nto four board-certified neuroradiologists using only CT scans as input. We\nfurther validate our model with interpretability saliency maps. Code is\navailable at https://github.com/MiriamCobo/MultitaskLearning_ICH_Prognosis.git.\n", "link": "http://arxiv.org/abs/2408.08784v1", "date": "2024-08-16", "relevancy": 1.5174, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20Learning%20Approach%20for%20Intracranial%20Hemorrhage%20Prognosis&body=Title%3A%20Multi-task%20Learning%20Approach%20for%20Intracranial%20Hemorrhage%20Prognosis%0AAuthor%3A%20Miriam%20Cobo%20and%20Amaia%20P%C3%A9rez%20del%20Barrio%20and%20Pablo%20Men%C3%A9ndez%20Fern%C3%A1ndez-Miranda%20and%20Pablo%20Sanz%20Bell%C3%B3n%20and%20Lara%20Lloret%20Iglesias%20and%20Wilson%20Silva%0AAbstract%3A%20%20%20Prognosis%20after%20intracranial%20hemorrhage%20%28ICH%29%20is%20influenced%20by%20a%20complex%0Ainterplay%20between%20imaging%20and%20tabular%20data.%20Rapid%20and%20reliable%20prognosis%20are%0Acrucial%20for%20effective%20patient%20stratification%20and%20informed%20treatment%0Adecision-making.%20In%20this%20study%2C%20we%20aim%20to%20enhance%20image-based%20prognosis%20by%0Alearning%20a%20robust%20feature%20representation%20shared%20between%20prognosis%20and%20the%0Aclinical%20and%20demographic%20variables%20most%20highly%20correlated%20with%20it.%20Our%20approach%0Amimics%20clinical%20decision-making%20by%20reinforcing%20the%20model%20to%20learn%20valuable%0Aprognostic%20data%20embedded%20in%20the%20image.%20We%20propose%20a%203D%20multi-task%20image%20model%0Ato%20predict%20prognosis%2C%20Glasgow%20Coma%20Scale%20and%20age%2C%20improving%20accuracy%20and%0Ainterpretability.%20Our%20method%20outperforms%20current%20state-of-the-art%20baseline%0Aimage%20models%2C%20and%20demonstrates%20superior%20performance%20in%20ICH%20prognosis%20compared%0Ato%20four%20board-certified%20neuroradiologists%20using%20only%20CT%20scans%20as%20input.%20We%0Afurther%20validate%20our%20model%20with%20interpretability%20saliency%20maps.%20Code%20is%0Aavailable%20at%20https%3A//github.com/MiriamCobo/MultitaskLearning_ICH_Prognosis.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520Learning%2520Approach%2520for%2520Intracranial%2520Hemorrhage%2520Prognosis%26entry.906535625%3DMiriam%2520Cobo%2520and%2520Amaia%2520P%25C3%25A9rez%2520del%2520Barrio%2520and%2520Pablo%2520Men%25C3%25A9ndez%2520Fern%25C3%25A1ndez-Miranda%2520and%2520Pablo%2520Sanz%2520Bell%25C3%25B3n%2520and%2520Lara%2520Lloret%2520Iglesias%2520and%2520Wilson%2520Silva%26entry.1292438233%3D%2520%2520Prognosis%2520after%2520intracranial%2520hemorrhage%2520%2528ICH%2529%2520is%2520influenced%2520by%2520a%2520complex%250Ainterplay%2520between%2520imaging%2520and%2520tabular%2520data.%2520Rapid%2520and%2520reliable%2520prognosis%2520are%250Acrucial%2520for%2520effective%2520patient%2520stratification%2520and%2520informed%2520treatment%250Adecision-making.%2520In%2520this%2520study%252C%2520we%2520aim%2520to%2520enhance%2520image-based%2520prognosis%2520by%250Alearning%2520a%2520robust%2520feature%2520representation%2520shared%2520between%2520prognosis%2520and%2520the%250Aclinical%2520and%2520demographic%2520variables%2520most%2520highly%2520correlated%2520with%2520it.%2520Our%2520approach%250Amimics%2520clinical%2520decision-making%2520by%2520reinforcing%2520the%2520model%2520to%2520learn%2520valuable%250Aprognostic%2520data%2520embedded%2520in%2520the%2520image.%2520We%2520propose%2520a%25203D%2520multi-task%2520image%2520model%250Ato%2520predict%2520prognosis%252C%2520Glasgow%2520Coma%2520Scale%2520and%2520age%252C%2520improving%2520accuracy%2520and%250Ainterpretability.%2520Our%2520method%2520outperforms%2520current%2520state-of-the-art%2520baseline%250Aimage%2520models%252C%2520and%2520demonstrates%2520superior%2520performance%2520in%2520ICH%2520prognosis%2520compared%250Ato%2520four%2520board-certified%2520neuroradiologists%2520using%2520only%2520CT%2520scans%2520as%2520input.%2520We%250Afurther%2520validate%2520our%2520model%2520with%2520interpretability%2520saliency%2520maps.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/MiriamCobo/MultitaskLearning_ICH_Prognosis.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20Learning%20Approach%20for%20Intracranial%20Hemorrhage%20Prognosis&entry.906535625=Miriam%20Cobo%20and%20Amaia%20P%C3%A9rez%20del%20Barrio%20and%20Pablo%20Men%C3%A9ndez%20Fern%C3%A1ndez-Miranda%20and%20Pablo%20Sanz%20Bell%C3%B3n%20and%20Lara%20Lloret%20Iglesias%20and%20Wilson%20Silva&entry.1292438233=%20%20Prognosis%20after%20intracranial%20hemorrhage%20%28ICH%29%20is%20influenced%20by%20a%20complex%0Ainterplay%20between%20imaging%20and%20tabular%20data.%20Rapid%20and%20reliable%20prognosis%20are%0Acrucial%20for%20effective%20patient%20stratification%20and%20informed%20treatment%0Adecision-making.%20In%20this%20study%2C%20we%20aim%20to%20enhance%20image-based%20prognosis%20by%0Alearning%20a%20robust%20feature%20representation%20shared%20between%20prognosis%20and%20the%0Aclinical%20and%20demographic%20variables%20most%20highly%20correlated%20with%20it.%20Our%20approach%0Amimics%20clinical%20decision-making%20by%20reinforcing%20the%20model%20to%20learn%20valuable%0Aprognostic%20data%20embedded%20in%20the%20image.%20We%20propose%20a%203D%20multi-task%20image%20model%0Ato%20predict%20prognosis%2C%20Glasgow%20Coma%20Scale%20and%20age%2C%20improving%20accuracy%20and%0Ainterpretability.%20Our%20method%20outperforms%20current%20state-of-the-art%20baseline%0Aimage%20models%2C%20and%20demonstrates%20superior%20performance%20in%20ICH%20prognosis%20compared%0Ato%20four%20board-certified%20neuroradiologists%20using%20only%20CT%20scans%20as%20input.%20We%0Afurther%20validate%20our%20model%20with%20interpretability%20saliency%20maps.%20Code%20is%0Aavailable%20at%20https%3A//github.com/MiriamCobo/MultitaskLearning_ICH_Prognosis.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08784v1&entry.124074799=Read"},
{"title": "Magazine Supply Optimization: a Case-study", "author": "Duong Nguyen and Ana Ulianovici and Sami Achour and Soline Aubry and Nicolas Chesneau", "abstract": "  Supply optimization is a complex and challenging task in the magazine retail\nindustry because of the fixed inventory assumption, irregular sales patterns,\nand varying product and point-of-sale characteristics. We introduce AthenIA, an\nindustrialized magazine supply optimization solution that plans the supply for\nover 20,000 points of sale in France. We modularize the supply planning process\ninto a four-step pipeline: demand sensing, optimization, business rules, and\noperating. The core of the solution is a novel group conformalized quantile\nregression method that integrates domain expert insights, coupled with a supply\noptimization technique that balances the costs of out-of-stock against the\ncosts of over-supply. AthenIA has proven to be a valuable tool for magazine\npublishers, particularly in the context of evolving economic and ecological\nchallenges.\n", "link": "http://arxiv.org/abs/2408.08637v1", "date": "2024-08-16", "relevancy": 1.1849, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.416}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.39}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Magazine%20Supply%20Optimization%3A%20a%20Case-study&body=Title%3A%20Magazine%20Supply%20Optimization%3A%20a%20Case-study%0AAuthor%3A%20Duong%20Nguyen%20and%20Ana%20Ulianovici%20and%20Sami%20Achour%20and%20Soline%20Aubry%20and%20Nicolas%20Chesneau%0AAbstract%3A%20%20%20Supply%20optimization%20is%20a%20complex%20and%20challenging%20task%20in%20the%20magazine%20retail%0Aindustry%20because%20of%20the%20fixed%20inventory%20assumption%2C%20irregular%20sales%20patterns%2C%0Aand%20varying%20product%20and%20point-of-sale%20characteristics.%20We%20introduce%20AthenIA%2C%20an%0Aindustrialized%20magazine%20supply%20optimization%20solution%20that%20plans%20the%20supply%20for%0Aover%2020%2C000%20points%20of%20sale%20in%20France.%20We%20modularize%20the%20supply%20planning%20process%0Ainto%20a%20four-step%20pipeline%3A%20demand%20sensing%2C%20optimization%2C%20business%20rules%2C%20and%0Aoperating.%20The%20core%20of%20the%20solution%20is%20a%20novel%20group%20conformalized%20quantile%0Aregression%20method%20that%20integrates%20domain%20expert%20insights%2C%20coupled%20with%20a%20supply%0Aoptimization%20technique%20that%20balances%20the%20costs%20of%20out-of-stock%20against%20the%0Acosts%20of%20over-supply.%20AthenIA%20has%20proven%20to%20be%20a%20valuable%20tool%20for%20magazine%0Apublishers%2C%20particularly%20in%20the%20context%20of%20evolving%20economic%20and%20ecological%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagazine%2520Supply%2520Optimization%253A%2520a%2520Case-study%26entry.906535625%3DDuong%2520Nguyen%2520and%2520Ana%2520Ulianovici%2520and%2520Sami%2520Achour%2520and%2520Soline%2520Aubry%2520and%2520Nicolas%2520Chesneau%26entry.1292438233%3D%2520%2520Supply%2520optimization%2520is%2520a%2520complex%2520and%2520challenging%2520task%2520in%2520the%2520magazine%2520retail%250Aindustry%2520because%2520of%2520the%2520fixed%2520inventory%2520assumption%252C%2520irregular%2520sales%2520patterns%252C%250Aand%2520varying%2520product%2520and%2520point-of-sale%2520characteristics.%2520We%2520introduce%2520AthenIA%252C%2520an%250Aindustrialized%2520magazine%2520supply%2520optimization%2520solution%2520that%2520plans%2520the%2520supply%2520for%250Aover%252020%252C000%2520points%2520of%2520sale%2520in%2520France.%2520We%2520modularize%2520the%2520supply%2520planning%2520process%250Ainto%2520a%2520four-step%2520pipeline%253A%2520demand%2520sensing%252C%2520optimization%252C%2520business%2520rules%252C%2520and%250Aoperating.%2520The%2520core%2520of%2520the%2520solution%2520is%2520a%2520novel%2520group%2520conformalized%2520quantile%250Aregression%2520method%2520that%2520integrates%2520domain%2520expert%2520insights%252C%2520coupled%2520with%2520a%2520supply%250Aoptimization%2520technique%2520that%2520balances%2520the%2520costs%2520of%2520out-of-stock%2520against%2520the%250Acosts%2520of%2520over-supply.%2520AthenIA%2520has%2520proven%2520to%2520be%2520a%2520valuable%2520tool%2520for%2520magazine%250Apublishers%252C%2520particularly%2520in%2520the%2520context%2520of%2520evolving%2520economic%2520and%2520ecological%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magazine%20Supply%20Optimization%3A%20a%20Case-study&entry.906535625=Duong%20Nguyen%20and%20Ana%20Ulianovici%20and%20Sami%20Achour%20and%20Soline%20Aubry%20and%20Nicolas%20Chesneau&entry.1292438233=%20%20Supply%20optimization%20is%20a%20complex%20and%20challenging%20task%20in%20the%20magazine%20retail%0Aindustry%20because%20of%20the%20fixed%20inventory%20assumption%2C%20irregular%20sales%20patterns%2C%0Aand%20varying%20product%20and%20point-of-sale%20characteristics.%20We%20introduce%20AthenIA%2C%20an%0Aindustrialized%20magazine%20supply%20optimization%20solution%20that%20plans%20the%20supply%20for%0Aover%2020%2C000%20points%20of%20sale%20in%20France.%20We%20modularize%20the%20supply%20planning%20process%0Ainto%20a%20four-step%20pipeline%3A%20demand%20sensing%2C%20optimization%2C%20business%20rules%2C%20and%0Aoperating.%20The%20core%20of%20the%20solution%20is%20a%20novel%20group%20conformalized%20quantile%0Aregression%20method%20that%20integrates%20domain%20expert%20insights%2C%20coupled%20with%20a%20supply%0Aoptimization%20technique%20that%20balances%20the%20costs%20of%20out-of-stock%20against%20the%0Acosts%20of%20over-supply.%20AthenIA%20has%20proven%20to%20be%20a%20valuable%20tool%20for%20magazine%0Apublishers%2C%20particularly%20in%20the%20context%20of%20evolving%20economic%20and%20ecological%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08637v1&entry.124074799=Read"},
{"title": "ML Study of MaliciousTransactions in Ethereum", "author": "Natan Katz", "abstract": "  Smart contracts are a major tool in Ethereum transactions. Therefore hackers\ncan exploit them by adding code vulnerabilities to their sources and using\nthese vulnerabilities for performing malicious transactions. This paper\npresents two successful approaches for detecting malicious contracts: one uses\nopcode and relies on GPT2 and the other uses the Solidity source and a LORA\nfine-tuned CodeLlama. Finally, we present an XGBOOST model that combines gas\nproperties and Hexa-decimal signatures for detecting malicious transactions.\nThis approach relies on early assumptions that maliciousness is manifested by\nthe uncommon usage of the contracts' functions and the effort to pursue the\ntransaction.\n", "link": "http://arxiv.org/abs/2408.08749v1", "date": "2024-08-16", "relevancy": 1.0972, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3914}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3598}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ML%20Study%20of%20MaliciousTransactions%20in%20Ethereum&body=Title%3A%20ML%20Study%20of%20MaliciousTransactions%20in%20Ethereum%0AAuthor%3A%20Natan%20Katz%0AAbstract%3A%20%20%20Smart%20contracts%20are%20a%20major%20tool%20in%20Ethereum%20transactions.%20Therefore%20hackers%0Acan%20exploit%20them%20by%20adding%20code%20vulnerabilities%20to%20their%20sources%20and%20using%0Athese%20vulnerabilities%20for%20performing%20malicious%20transactions.%20This%20paper%0Apresents%20two%20successful%20approaches%20for%20detecting%20malicious%20contracts%3A%20one%20uses%0Aopcode%20and%20relies%20on%20GPT2%20and%20the%20other%20uses%20the%20Solidity%20source%20and%20a%20LORA%0Afine-tuned%20CodeLlama.%20Finally%2C%20we%20present%20an%20XGBOOST%20model%20that%20combines%20gas%0Aproperties%20and%20Hexa-decimal%20signatures%20for%20detecting%20malicious%20transactions.%0AThis%20approach%20relies%20on%20early%20assumptions%20that%20maliciousness%20is%20manifested%20by%0Athe%20uncommon%20usage%20of%20the%20contracts%27%20functions%20and%20the%20effort%20to%20pursue%20the%0Atransaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DML%2520Study%2520of%2520MaliciousTransactions%2520in%2520Ethereum%26entry.906535625%3DNatan%2520Katz%26entry.1292438233%3D%2520%2520Smart%2520contracts%2520are%2520a%2520major%2520tool%2520in%2520Ethereum%2520transactions.%2520Therefore%2520hackers%250Acan%2520exploit%2520them%2520by%2520adding%2520code%2520vulnerabilities%2520to%2520their%2520sources%2520and%2520using%250Athese%2520vulnerabilities%2520for%2520performing%2520malicious%2520transactions.%2520This%2520paper%250Apresents%2520two%2520successful%2520approaches%2520for%2520detecting%2520malicious%2520contracts%253A%2520one%2520uses%250Aopcode%2520and%2520relies%2520on%2520GPT2%2520and%2520the%2520other%2520uses%2520the%2520Solidity%2520source%2520and%2520a%2520LORA%250Afine-tuned%2520CodeLlama.%2520Finally%252C%2520we%2520present%2520an%2520XGBOOST%2520model%2520that%2520combines%2520gas%250Aproperties%2520and%2520Hexa-decimal%2520signatures%2520for%2520detecting%2520malicious%2520transactions.%250AThis%2520approach%2520relies%2520on%2520early%2520assumptions%2520that%2520maliciousness%2520is%2520manifested%2520by%250Athe%2520uncommon%2520usage%2520of%2520the%2520contracts%2527%2520functions%2520and%2520the%2520effort%2520to%2520pursue%2520the%250Atransaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ML%20Study%20of%20MaliciousTransactions%20in%20Ethereum&entry.906535625=Natan%20Katz&entry.1292438233=%20%20Smart%20contracts%20are%20a%20major%20tool%20in%20Ethereum%20transactions.%20Therefore%20hackers%0Acan%20exploit%20them%20by%20adding%20code%20vulnerabilities%20to%20their%20sources%20and%20using%0Athese%20vulnerabilities%20for%20performing%20malicious%20transactions.%20This%20paper%0Apresents%20two%20successful%20approaches%20for%20detecting%20malicious%20contracts%3A%20one%20uses%0Aopcode%20and%20relies%20on%20GPT2%20and%20the%20other%20uses%20the%20Solidity%20source%20and%20a%20LORA%0Afine-tuned%20CodeLlama.%20Finally%2C%20we%20present%20an%20XGBOOST%20model%20that%20combines%20gas%0Aproperties%20and%20Hexa-decimal%20signatures%20for%20detecting%20malicious%20transactions.%0AThis%20approach%20relies%20on%20early%20assumptions%20that%20maliciousness%20is%20manifested%20by%0Athe%20uncommon%20usage%20of%20the%20contracts%27%20functions%20and%20the%20effort%20to%20pursue%20the%0Atransaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08749v1&entry.124074799=Read"},
{"title": "A Survey on Benchmarks of Multimodal Large Language Models", "author": "Jian Li and Weiheng Lu", "abstract": "  Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of \\textbf{180 benchmarks} and evaluation for MLLMs,\nfocusing on (1)perception and understanding, (2)cognition and reasoning,\n(3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we\ndiscuss the limitations of the current evaluation methods for MLLMs and explore\npromising future directions. Our key argument is that evaluation should be\nregarded as a crucial discipline to better support the development of MLLMs.\nFor more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.\n", "link": "http://arxiv.org/abs/2408.08632v1", "date": "2024-08-16", "relevancy": 1.5405, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Benchmarks%20of%20Multimodal%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Benchmarks%20of%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Jian%20Li%20and%20Weiheng%20Lu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20gaining%20increasing%20popularity%20in%0Aboth%20academia%20and%20industry%20due%20to%20their%20remarkable%20performance%20in%20various%0Aapplications%20such%20as%20visual%20question%20answering%2C%20visual%20perception%2C%0Aunderstanding%2C%20and%20reasoning.%20Over%20the%20past%20few%20years%2C%20significant%20efforts%20have%0Abeen%20made%20to%20examine%20MLLMs%20from%20multiple%20perspectives.%20This%20paper%20presents%20a%0Acomprehensive%20review%20of%20%5Ctextbf%7B180%20benchmarks%7D%20and%20evaluation%20for%20MLLMs%2C%0Afocusing%20on%20%281%29perception%20and%20understanding%2C%20%282%29cognition%20and%20reasoning%2C%0A%283%29specific%20domains%2C%20%284%29key%20capabilities%2C%20and%20%285%29other%20modalities.%20Finally%2C%20we%0Adiscuss%20the%20limitations%20of%20the%20current%20evaluation%20methods%20for%20MLLMs%20and%20explore%0Apromising%20future%20directions.%20Our%20key%20argument%20is%20that%20evaluation%20should%20be%0Aregarded%20as%20a%20crucial%20discipline%20to%20better%20support%20the%20development%20of%20MLLMs.%0AFor%20more%20details%2C%20please%20visit%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Benchmarks%2520of%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJian%2520Li%2520and%2520Weiheng%2520Lu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520gaining%2520increasing%2520popularity%2520in%250Aboth%2520academia%2520and%2520industry%2520due%2520to%2520their%2520remarkable%2520performance%2520in%2520various%250Aapplications%2520such%2520as%2520visual%2520question%2520answering%252C%2520visual%2520perception%252C%250Aunderstanding%252C%2520and%2520reasoning.%2520Over%2520the%2520past%2520few%2520years%252C%2520significant%2520efforts%2520have%250Abeen%2520made%2520to%2520examine%2520MLLMs%2520from%2520multiple%2520perspectives.%2520This%2520paper%2520presents%2520a%250Acomprehensive%2520review%2520of%2520%255Ctextbf%257B180%2520benchmarks%257D%2520and%2520evaluation%2520for%2520MLLMs%252C%250Afocusing%2520on%2520%25281%2529perception%2520and%2520understanding%252C%2520%25282%2529cognition%2520and%2520reasoning%252C%250A%25283%2529specific%2520domains%252C%2520%25284%2529key%2520capabilities%252C%2520and%2520%25285%2529other%2520modalities.%2520Finally%252C%2520we%250Adiscuss%2520the%2520limitations%2520of%2520the%2520current%2520evaluation%2520methods%2520for%2520MLLMs%2520and%2520explore%250Apromising%2520future%2520directions.%2520Our%2520key%2520argument%2520is%2520that%2520evaluation%2520should%2520be%250Aregarded%2520as%2520a%2520crucial%2520discipline%2520to%2520better%2520support%2520the%2520development%2520of%2520MLLMs.%250AFor%2520more%2520details%252C%2520please%2520visit%2520our%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Benchmarks%20of%20Multimodal%20Large%20Language%20Models&entry.906535625=Jian%20Li%20and%20Weiheng%20Lu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20gaining%20increasing%20popularity%20in%0Aboth%20academia%20and%20industry%20due%20to%20their%20remarkable%20performance%20in%20various%0Aapplications%20such%20as%20visual%20question%20answering%2C%20visual%20perception%2C%0Aunderstanding%2C%20and%20reasoning.%20Over%20the%20past%20few%20years%2C%20significant%20efforts%20have%0Abeen%20made%20to%20examine%20MLLMs%20from%20multiple%20perspectives.%20This%20paper%20presents%20a%0Acomprehensive%20review%20of%20%5Ctextbf%7B180%20benchmarks%7D%20and%20evaluation%20for%20MLLMs%2C%0Afocusing%20on%20%281%29perception%20and%20understanding%2C%20%282%29cognition%20and%20reasoning%2C%0A%283%29specific%20domains%2C%20%284%29key%20capabilities%2C%20and%20%285%29other%20modalities.%20Finally%2C%20we%0Adiscuss%20the%20limitations%20of%20the%20current%20evaluation%20methods%20for%20MLLMs%20and%20explore%0Apromising%20future%20directions.%20Our%20key%20argument%20is%20that%20evaluation%20should%20be%0Aregarded%20as%20a%20crucial%20discipline%20to%20better%20support%20the%20development%20of%20MLLMs.%0AFor%20more%20details%2C%20please%20visit%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08632v1&entry.124074799=Read"},
{"title": "CAT: Caution Aware Transfer in Reinforcement Learning via Distributional\n  Risk", "author": "Mohamad Fares El Hajj Chehade and Amrit Singh Bedi and Amy Zhang and Hao Zhu", "abstract": "  Transfer learning in reinforcement learning (RL) has become a pivotal\nstrategy for improving data efficiency in new, unseen tasks by utilizing\nknowledge from previously learned tasks. This approach is especially beneficial\nin real-world deployment scenarios where computational resources are\nconstrained and agents must adapt rapidly to novel environments. However,\ncurrent state-of-the-art methods often fall short in ensuring safety during the\ntransfer process, particularly when unforeseen risks emerge in the deployment\nphase. In this work, we address these limitations by introducing a novel\nCaution-Aware Transfer Learning (CAT) framework. Unlike traditional approaches\nthat limit risk considerations to mean-variance, we define \"caution\" as a more\ngeneralized and comprehensive notion of risk. Our core innovation lies in\noptimizing a weighted sum of reward return and caution-based on state-action\noccupancy measures-during the transfer process, allowing for a rich\nrepresentation of diverse risk factors. To the best of our knowledge, this is\nthe first work to explore the optimization of such a generalized risk notion\nwithin the context of transfer RL. Our contributions are threefold: (1) We\npropose a Caution-Aware Transfer (CAT) framework that evaluates source policies\nwithin the test environment and constructs a new policy that balances reward\nmaximization and caution. (2) We derive theoretical sub-optimality bounds for\nour method, providing rigorous guarantees of its efficacy. (3) We empirically\nvalidate CAT, demonstrating that it consistently outperforms existing methods\nby delivering safer policies under varying risk conditions in the test tasks.\n", "link": "http://arxiv.org/abs/2408.08812v1", "date": "2024-08-16", "relevancy": 1.5758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20Caution%20Aware%20Transfer%20in%20Reinforcement%20Learning%20via%20Distributional%0A%20%20Risk&body=Title%3A%20CAT%3A%20Caution%20Aware%20Transfer%20in%20Reinforcement%20Learning%20via%20Distributional%0A%20%20Risk%0AAuthor%3A%20Mohamad%20Fares%20El%20Hajj%20Chehade%20and%20Amrit%20Singh%20Bedi%20and%20Amy%20Zhang%20and%20Hao%20Zhu%0AAbstract%3A%20%20%20Transfer%20learning%20in%20reinforcement%20learning%20%28RL%29%20has%20become%20a%20pivotal%0Astrategy%20for%20improving%20data%20efficiency%20in%20new%2C%20unseen%20tasks%20by%20utilizing%0Aknowledge%20from%20previously%20learned%20tasks.%20This%20approach%20is%20especially%20beneficial%0Ain%20real-world%20deployment%20scenarios%20where%20computational%20resources%20are%0Aconstrained%20and%20agents%20must%20adapt%20rapidly%20to%20novel%20environments.%20However%2C%0Acurrent%20state-of-the-art%20methods%20often%20fall%20short%20in%20ensuring%20safety%20during%20the%0Atransfer%20process%2C%20particularly%20when%20unforeseen%20risks%20emerge%20in%20the%20deployment%0Aphase.%20In%20this%20work%2C%20we%20address%20these%20limitations%20by%20introducing%20a%20novel%0ACaution-Aware%20Transfer%20Learning%20%28CAT%29%20framework.%20Unlike%20traditional%20approaches%0Athat%20limit%20risk%20considerations%20to%20mean-variance%2C%20we%20define%20%22caution%22%20as%20a%20more%0Ageneralized%20and%20comprehensive%20notion%20of%20risk.%20Our%20core%20innovation%20lies%20in%0Aoptimizing%20a%20weighted%20sum%20of%20reward%20return%20and%20caution-based%20on%20state-action%0Aoccupancy%20measures-during%20the%20transfer%20process%2C%20allowing%20for%20a%20rich%0Arepresentation%20of%20diverse%20risk%20factors.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20work%20to%20explore%20the%20optimization%20of%20such%20a%20generalized%20risk%20notion%0Awithin%20the%20context%20of%20transfer%20RL.%20Our%20contributions%20are%20threefold%3A%20%281%29%20We%0Apropose%20a%20Caution-Aware%20Transfer%20%28CAT%29%20framework%20that%20evaluates%20source%20policies%0Awithin%20the%20test%20environment%20and%20constructs%20a%20new%20policy%20that%20balances%20reward%0Amaximization%20and%20caution.%20%282%29%20We%20derive%20theoretical%20sub-optimality%20bounds%20for%0Aour%20method%2C%20providing%20rigorous%20guarantees%20of%20its%20efficacy.%20%283%29%20We%20empirically%0Avalidate%20CAT%2C%20demonstrating%20that%20it%20consistently%20outperforms%20existing%20methods%0Aby%20delivering%20safer%20policies%20under%20varying%20risk%20conditions%20in%20the%20test%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%253A%2520Caution%2520Aware%2520Transfer%2520in%2520Reinforcement%2520Learning%2520via%2520Distributional%250A%2520%2520Risk%26entry.906535625%3DMohamad%2520Fares%2520El%2520Hajj%2520Chehade%2520and%2520Amrit%2520Singh%2520Bedi%2520and%2520Amy%2520Zhang%2520and%2520Hao%2520Zhu%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520become%2520a%2520pivotal%250Astrategy%2520for%2520improving%2520data%2520efficiency%2520in%2520new%252C%2520unseen%2520tasks%2520by%2520utilizing%250Aknowledge%2520from%2520previously%2520learned%2520tasks.%2520This%2520approach%2520is%2520especially%2520beneficial%250Ain%2520real-world%2520deployment%2520scenarios%2520where%2520computational%2520resources%2520are%250Aconstrained%2520and%2520agents%2520must%2520adapt%2520rapidly%2520to%2520novel%2520environments.%2520However%252C%250Acurrent%2520state-of-the-art%2520methods%2520often%2520fall%2520short%2520in%2520ensuring%2520safety%2520during%2520the%250Atransfer%2520process%252C%2520particularly%2520when%2520unforeseen%2520risks%2520emerge%2520in%2520the%2520deployment%250Aphase.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520limitations%2520by%2520introducing%2520a%2520novel%250ACaution-Aware%2520Transfer%2520Learning%2520%2528CAT%2529%2520framework.%2520Unlike%2520traditional%2520approaches%250Athat%2520limit%2520risk%2520considerations%2520to%2520mean-variance%252C%2520we%2520define%2520%2522caution%2522%2520as%2520a%2520more%250Ageneralized%2520and%2520comprehensive%2520notion%2520of%2520risk.%2520Our%2520core%2520innovation%2520lies%2520in%250Aoptimizing%2520a%2520weighted%2520sum%2520of%2520reward%2520return%2520and%2520caution-based%2520on%2520state-action%250Aoccupancy%2520measures-during%2520the%2520transfer%2520process%252C%2520allowing%2520for%2520a%2520rich%250Arepresentation%2520of%2520diverse%2520risk%2520factors.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520work%2520to%2520explore%2520the%2520optimization%2520of%2520such%2520a%2520generalized%2520risk%2520notion%250Awithin%2520the%2520context%2520of%2520transfer%2520RL.%2520Our%2520contributions%2520are%2520threefold%253A%2520%25281%2529%2520We%250Apropose%2520a%2520Caution-Aware%2520Transfer%2520%2528CAT%2529%2520framework%2520that%2520evaluates%2520source%2520policies%250Awithin%2520the%2520test%2520environment%2520and%2520constructs%2520a%2520new%2520policy%2520that%2520balances%2520reward%250Amaximization%2520and%2520caution.%2520%25282%2529%2520We%2520derive%2520theoretical%2520sub-optimality%2520bounds%2520for%250Aour%2520method%252C%2520providing%2520rigorous%2520guarantees%2520of%2520its%2520efficacy.%2520%25283%2529%2520We%2520empirically%250Avalidate%2520CAT%252C%2520demonstrating%2520that%2520it%2520consistently%2520outperforms%2520existing%2520methods%250Aby%2520delivering%2520safer%2520policies%2520under%2520varying%2520risk%2520conditions%2520in%2520the%2520test%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20Caution%20Aware%20Transfer%20in%20Reinforcement%20Learning%20via%20Distributional%0A%20%20Risk&entry.906535625=Mohamad%20Fares%20El%20Hajj%20Chehade%20and%20Amrit%20Singh%20Bedi%20and%20Amy%20Zhang%20and%20Hao%20Zhu&entry.1292438233=%20%20Transfer%20learning%20in%20reinforcement%20learning%20%28RL%29%20has%20become%20a%20pivotal%0Astrategy%20for%20improving%20data%20efficiency%20in%20new%2C%20unseen%20tasks%20by%20utilizing%0Aknowledge%20from%20previously%20learned%20tasks.%20This%20approach%20is%20especially%20beneficial%0Ain%20real-world%20deployment%20scenarios%20where%20computational%20resources%20are%0Aconstrained%20and%20agents%20must%20adapt%20rapidly%20to%20novel%20environments.%20However%2C%0Acurrent%20state-of-the-art%20methods%20often%20fall%20short%20in%20ensuring%20safety%20during%20the%0Atransfer%20process%2C%20particularly%20when%20unforeseen%20risks%20emerge%20in%20the%20deployment%0Aphase.%20In%20this%20work%2C%20we%20address%20these%20limitations%20by%20introducing%20a%20novel%0ACaution-Aware%20Transfer%20Learning%20%28CAT%29%20framework.%20Unlike%20traditional%20approaches%0Athat%20limit%20risk%20considerations%20to%20mean-variance%2C%20we%20define%20%22caution%22%20as%20a%20more%0Ageneralized%20and%20comprehensive%20notion%20of%20risk.%20Our%20core%20innovation%20lies%20in%0Aoptimizing%20a%20weighted%20sum%20of%20reward%20return%20and%20caution-based%20on%20state-action%0Aoccupancy%20measures-during%20the%20transfer%20process%2C%20allowing%20for%20a%20rich%0Arepresentation%20of%20diverse%20risk%20factors.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20work%20to%20explore%20the%20optimization%20of%20such%20a%20generalized%20risk%20notion%0Awithin%20the%20context%20of%20transfer%20RL.%20Our%20contributions%20are%20threefold%3A%20%281%29%20We%0Apropose%20a%20Caution-Aware%20Transfer%20%28CAT%29%20framework%20that%20evaluates%20source%20policies%0Awithin%20the%20test%20environment%20and%20constructs%20a%20new%20policy%20that%20balances%20reward%0Amaximization%20and%20caution.%20%282%29%20We%20derive%20theoretical%20sub-optimality%20bounds%20for%0Aour%20method%2C%20providing%20rigorous%20guarantees%20of%20its%20efficacy.%20%283%29%20We%20empirically%0Avalidate%20CAT%2C%20demonstrating%20that%20it%20consistently%20outperforms%20existing%20methods%0Aby%20delivering%20safer%20policies%20under%20varying%20risk%20conditions%20in%20the%20test%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08812v1&entry.124074799=Read"},
{"title": "Active Inference Tree Search in Large POMDPs", "author": "Domenico Maisto and Francesco Gregoretti and Karl Friston and Giovanni Pezzulo", "abstract": "  The ability to plan ahead efficiently is key for both living organisms and\nartificial systems. Model-based planning and prospection are widely studied in\ncognitive neuroscience and artificial intelligence (AI), but from different\nperspectives--and with different desiderata in mind (biological realism versus\nscalability) that are difficult to reconcile. Here, we introduce a novel method\nto plan in POMDPs--Active Inference Tree Search (AcT)--that combines the\nnormative character and biological realism of a leading planning theory in\nneuroscience (Active Inference) and the scalability of tree search methods in\nAI. This unification enhances both approaches. On the one hand, tree searches\nenable the biologically grounded, first principle method of active inference to\nbe applied to large-scale problems. On the other hand, active inference\nprovides a principled solution to the exploration-exploitation dilemma, which\nis often addressed heuristically in tree search methods. Our simulations show\nthat AcT successfully navigates binary trees that are challenging for\nsampling-based methods, problems that require adaptive exploration, and the\nlarge POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art\nPOMDP solutions. Furthermore, we illustrate how AcT can be used to simulate\nneurophysiological responses (e.g., in the hippocampus and prefrontal cortex)\nof humans and other animals that solve large planning problems. These numerical\nanalyses show that Active Tree Search is a principled realisation of\nneuroscientific and AI planning theories, which offer both biological realism\nand scalability.\n", "link": "http://arxiv.org/abs/2103.13860v4", "date": "2024-08-16", "relevancy": 1.5586, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5921}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Inference%20Tree%20Search%20in%20Large%20POMDPs&body=Title%3A%20Active%20Inference%20Tree%20Search%20in%20Large%20POMDPs%0AAuthor%3A%20Domenico%20Maisto%20and%20Francesco%20Gregoretti%20and%20Karl%20Friston%20and%20Giovanni%20Pezzulo%0AAbstract%3A%20%20%20The%20ability%20to%20plan%20ahead%20efficiently%20is%20key%20for%20both%20living%20organisms%20and%0Aartificial%20systems.%20Model-based%20planning%20and%20prospection%20are%20widely%20studied%20in%0Acognitive%20neuroscience%20and%20artificial%20intelligence%20%28AI%29%2C%20but%20from%20different%0Aperspectives--and%20with%20different%20desiderata%20in%20mind%20%28biological%20realism%20versus%0Ascalability%29%20that%20are%20difficult%20to%20reconcile.%20Here%2C%20we%20introduce%20a%20novel%20method%0Ato%20plan%20in%20POMDPs--Active%20Inference%20Tree%20Search%20%28AcT%29--that%20combines%20the%0Anormative%20character%20and%20biological%20realism%20of%20a%20leading%20planning%20theory%20in%0Aneuroscience%20%28Active%20Inference%29%20and%20the%20scalability%20of%20tree%20search%20methods%20in%0AAI.%20This%20unification%20enhances%20both%20approaches.%20On%20the%20one%20hand%2C%20tree%20searches%0Aenable%20the%20biologically%20grounded%2C%20first%20principle%20method%20of%20active%20inference%20to%0Abe%20applied%20to%20large-scale%20problems.%20On%20the%20other%20hand%2C%20active%20inference%0Aprovides%20a%20principled%20solution%20to%20the%20exploration-exploitation%20dilemma%2C%20which%0Ais%20often%20addressed%20heuristically%20in%20tree%20search%20methods.%20Our%20simulations%20show%0Athat%20AcT%20successfully%20navigates%20binary%20trees%20that%20are%20challenging%20for%0Asampling-based%20methods%2C%20problems%20that%20require%20adaptive%20exploration%2C%20and%20the%0Alarge%20POMDP%20problem%20%27RockSample%27--in%20which%20AcT%20reproduces%20state-of-the-art%0APOMDP%20solutions.%20Furthermore%2C%20we%20illustrate%20how%20AcT%20can%20be%20used%20to%20simulate%0Aneurophysiological%20responses%20%28e.g.%2C%20in%20the%20hippocampus%20and%20prefrontal%20cortex%29%0Aof%20humans%20and%20other%20animals%20that%20solve%20large%20planning%20problems.%20These%20numerical%0Aanalyses%20show%20that%20Active%20Tree%20Search%20is%20a%20principled%20realisation%20of%0Aneuroscientific%20and%20AI%20planning%20theories%2C%20which%20offer%20both%20biological%20realism%0Aand%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2103.13860v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Inference%2520Tree%2520Search%2520in%2520Large%2520POMDPs%26entry.906535625%3DDomenico%2520Maisto%2520and%2520Francesco%2520Gregoretti%2520and%2520Karl%2520Friston%2520and%2520Giovanni%2520Pezzulo%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520plan%2520ahead%2520efficiently%2520is%2520key%2520for%2520both%2520living%2520organisms%2520and%250Aartificial%2520systems.%2520Model-based%2520planning%2520and%2520prospection%2520are%2520widely%2520studied%2520in%250Acognitive%2520neuroscience%2520and%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520but%2520from%2520different%250Aperspectives--and%2520with%2520different%2520desiderata%2520in%2520mind%2520%2528biological%2520realism%2520versus%250Ascalability%2529%2520that%2520are%2520difficult%2520to%2520reconcile.%2520Here%252C%2520we%2520introduce%2520a%2520novel%2520method%250Ato%2520plan%2520in%2520POMDPs--Active%2520Inference%2520Tree%2520Search%2520%2528AcT%2529--that%2520combines%2520the%250Anormative%2520character%2520and%2520biological%2520realism%2520of%2520a%2520leading%2520planning%2520theory%2520in%250Aneuroscience%2520%2528Active%2520Inference%2529%2520and%2520the%2520scalability%2520of%2520tree%2520search%2520methods%2520in%250AAI.%2520This%2520unification%2520enhances%2520both%2520approaches.%2520On%2520the%2520one%2520hand%252C%2520tree%2520searches%250Aenable%2520the%2520biologically%2520grounded%252C%2520first%2520principle%2520method%2520of%2520active%2520inference%2520to%250Abe%2520applied%2520to%2520large-scale%2520problems.%2520On%2520the%2520other%2520hand%252C%2520active%2520inference%250Aprovides%2520a%2520principled%2520solution%2520to%2520the%2520exploration-exploitation%2520dilemma%252C%2520which%250Ais%2520often%2520addressed%2520heuristically%2520in%2520tree%2520search%2520methods.%2520Our%2520simulations%2520show%250Athat%2520AcT%2520successfully%2520navigates%2520binary%2520trees%2520that%2520are%2520challenging%2520for%250Asampling-based%2520methods%252C%2520problems%2520that%2520require%2520adaptive%2520exploration%252C%2520and%2520the%250Alarge%2520POMDP%2520problem%2520%2527RockSample%2527--in%2520which%2520AcT%2520reproduces%2520state-of-the-art%250APOMDP%2520solutions.%2520Furthermore%252C%2520we%2520illustrate%2520how%2520AcT%2520can%2520be%2520used%2520to%2520simulate%250Aneurophysiological%2520responses%2520%2528e.g.%252C%2520in%2520the%2520hippocampus%2520and%2520prefrontal%2520cortex%2529%250Aof%2520humans%2520and%2520other%2520animals%2520that%2520solve%2520large%2520planning%2520problems.%2520These%2520numerical%250Aanalyses%2520show%2520that%2520Active%2520Tree%2520Search%2520is%2520a%2520principled%2520realisation%2520of%250Aneuroscientific%2520and%2520AI%2520planning%2520theories%252C%2520which%2520offer%2520both%2520biological%2520realism%250Aand%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2103.13860v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Inference%20Tree%20Search%20in%20Large%20POMDPs&entry.906535625=Domenico%20Maisto%20and%20Francesco%20Gregoretti%20and%20Karl%20Friston%20and%20Giovanni%20Pezzulo&entry.1292438233=%20%20The%20ability%20to%20plan%20ahead%20efficiently%20is%20key%20for%20both%20living%20organisms%20and%0Aartificial%20systems.%20Model-based%20planning%20and%20prospection%20are%20widely%20studied%20in%0Acognitive%20neuroscience%20and%20artificial%20intelligence%20%28AI%29%2C%20but%20from%20different%0Aperspectives--and%20with%20different%20desiderata%20in%20mind%20%28biological%20realism%20versus%0Ascalability%29%20that%20are%20difficult%20to%20reconcile.%20Here%2C%20we%20introduce%20a%20novel%20method%0Ato%20plan%20in%20POMDPs--Active%20Inference%20Tree%20Search%20%28AcT%29--that%20combines%20the%0Anormative%20character%20and%20biological%20realism%20of%20a%20leading%20planning%20theory%20in%0Aneuroscience%20%28Active%20Inference%29%20and%20the%20scalability%20of%20tree%20search%20methods%20in%0AAI.%20This%20unification%20enhances%20both%20approaches.%20On%20the%20one%20hand%2C%20tree%20searches%0Aenable%20the%20biologically%20grounded%2C%20first%20principle%20method%20of%20active%20inference%20to%0Abe%20applied%20to%20large-scale%20problems.%20On%20the%20other%20hand%2C%20active%20inference%0Aprovides%20a%20principled%20solution%20to%20the%20exploration-exploitation%20dilemma%2C%20which%0Ais%20often%20addressed%20heuristically%20in%20tree%20search%20methods.%20Our%20simulations%20show%0Athat%20AcT%20successfully%20navigates%20binary%20trees%20that%20are%20challenging%20for%0Asampling-based%20methods%2C%20problems%20that%20require%20adaptive%20exploration%2C%20and%20the%0Alarge%20POMDP%20problem%20%27RockSample%27--in%20which%20AcT%20reproduces%20state-of-the-art%0APOMDP%20solutions.%20Furthermore%2C%20we%20illustrate%20how%20AcT%20can%20be%20used%20to%20simulate%0Aneurophysiological%20responses%20%28e.g.%2C%20in%20the%20hippocampus%20and%20prefrontal%20cortex%29%0Aof%20humans%20and%20other%20animals%20that%20solve%20large%20planning%20problems.%20These%20numerical%0Aanalyses%20show%20that%20Active%20Tree%20Search%20is%20a%20principled%20realisation%20of%0Aneuroscientific%20and%20AI%20planning%20theories%2C%20which%20offer%20both%20biological%20realism%0Aand%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2103.13860v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


