<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260111.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time", "author": "Christopher Thirgood and Oscar Mendez and Erin Ling and Jon Storey and Simon Hadfield", "abstract": "We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\\% lower pose error and 8\\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.", "link": "http://arxiv.org/abs/2601.05738v1", "date": "2026-01-09", "relevancy": 3.4479, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8162}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6422}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FeatureSLAM%3A%20Feature-enriched%203D%20gaussian%20splatting%20SLAM%20in%20real%20time&body=Title%3A%20FeatureSLAM%3A%20Feature-enriched%203D%20gaussian%20splatting%20SLAM%20in%20real%20time%0AAuthor%3A%20Christopher%20Thirgood%20and%20Oscar%20Mendez%20and%20Erin%20Ling%20and%20Jon%20Storey%20and%20Simon%20Hadfield%0AAbstract%3A%20We%20present%20a%20real-time%20tracking%20SLAM%20system%20that%20unifies%20efficient%20camera%20tracking%20with%20photorealistic%20feature-enriched%20mapping%20using%203D%20Gaussian%20Splatting%20%283DGS%29.%20Our%20main%20contribution%20is%20integrating%20dense%20feature%20rasterization%20into%20the%20novel-view%20synthesis%2C%20aligned%20with%20a%20visual%20foundation%20model.%20This%20yields%20strong%20semantics%2C%20going%20beyond%20basic%20RGB-D%20input%2C%20aiding%20both%20tracking%20and%20mapping%20accuracy.%20Unlike%20previous%20semantic%20SLAM%20approaches%20%28which%20embed%20pre-defined%20class%20labels%29%20FeatureSLAM%20enables%20entirely%20new%20downstream%20tasks%20via%20free-viewpoint%2C%20open-set%20segmentation.%20Across%20standard%20benchmarks%2C%20our%20method%20achieves%20real-time%20tracking%2C%20on%20par%20with%20state-of-the-art%20systems%20while%20improving%20tracking%20stability%20and%20map%20fidelity%20without%20prohibitive%20compute.%20Quantitatively%2C%20we%20obtain%209%5C%25%20lower%20pose%20error%20and%208%5C%25%20higher%20mapping%20accuracy%20compared%20to%20recent%20fixed-set%20SLAM%20baselines.%20Our%20results%20confirm%20that%20real-time%20feature-embedded%20SLAM%2C%20is%20not%20only%20valuable%20for%20enabling%20new%20downstream%20applications.%20It%20also%20improves%20the%20performance%20of%20the%20underlying%20tracking%20and%20mapping%20subsystems%2C%20providing%20semantic%20and%20language%20masking%20results%20that%20are%20on-par%20with%20offline%203DGS%20models%2C%20alongside%20state-of-the-art%20tracking%2C%20depth%20and%20RGB%20rendering.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatureSLAM%253A%2520Feature-enriched%25203D%2520gaussian%2520splatting%2520SLAM%2520in%2520real%2520time%26entry.906535625%3DChristopher%2520Thirgood%2520and%2520Oscar%2520Mendez%2520and%2520Erin%2520Ling%2520and%2520Jon%2520Storey%2520and%2520Simon%2520Hadfield%26entry.1292438233%3DWe%2520present%2520a%2520real-time%2520tracking%2520SLAM%2520system%2520that%2520unifies%2520efficient%2520camera%2520tracking%2520with%2520photorealistic%2520feature-enriched%2520mapping%2520using%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520Our%2520main%2520contribution%2520is%2520integrating%2520dense%2520feature%2520rasterization%2520into%2520the%2520novel-view%2520synthesis%252C%2520aligned%2520with%2520a%2520visual%2520foundation%2520model.%2520This%2520yields%2520strong%2520semantics%252C%2520going%2520beyond%2520basic%2520RGB-D%2520input%252C%2520aiding%2520both%2520tracking%2520and%2520mapping%2520accuracy.%2520Unlike%2520previous%2520semantic%2520SLAM%2520approaches%2520%2528which%2520embed%2520pre-defined%2520class%2520labels%2529%2520FeatureSLAM%2520enables%2520entirely%2520new%2520downstream%2520tasks%2520via%2520free-viewpoint%252C%2520open-set%2520segmentation.%2520Across%2520standard%2520benchmarks%252C%2520our%2520method%2520achieves%2520real-time%2520tracking%252C%2520on%2520par%2520with%2520state-of-the-art%2520systems%2520while%2520improving%2520tracking%2520stability%2520and%2520map%2520fidelity%2520without%2520prohibitive%2520compute.%2520Quantitatively%252C%2520we%2520obtain%25209%255C%2525%2520lower%2520pose%2520error%2520and%25208%255C%2525%2520higher%2520mapping%2520accuracy%2520compared%2520to%2520recent%2520fixed-set%2520SLAM%2520baselines.%2520Our%2520results%2520confirm%2520that%2520real-time%2520feature-embedded%2520SLAM%252C%2520is%2520not%2520only%2520valuable%2520for%2520enabling%2520new%2520downstream%2520applications.%2520It%2520also%2520improves%2520the%2520performance%2520of%2520the%2520underlying%2520tracking%2520and%2520mapping%2520subsystems%252C%2520providing%2520semantic%2520and%2520language%2520masking%2520results%2520that%2520are%2520on-par%2520with%2520offline%25203DGS%2520models%252C%2520alongside%2520state-of-the-art%2520tracking%252C%2520depth%2520and%2520RGB%2520rendering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeatureSLAM%3A%20Feature-enriched%203D%20gaussian%20splatting%20SLAM%20in%20real%20time&entry.906535625=Christopher%20Thirgood%20and%20Oscar%20Mendez%20and%20Erin%20Ling%20and%20Jon%20Storey%20and%20Simon%20Hadfield&entry.1292438233=We%20present%20a%20real-time%20tracking%20SLAM%20system%20that%20unifies%20efficient%20camera%20tracking%20with%20photorealistic%20feature-enriched%20mapping%20using%203D%20Gaussian%20Splatting%20%283DGS%29.%20Our%20main%20contribution%20is%20integrating%20dense%20feature%20rasterization%20into%20the%20novel-view%20synthesis%2C%20aligned%20with%20a%20visual%20foundation%20model.%20This%20yields%20strong%20semantics%2C%20going%20beyond%20basic%20RGB-D%20input%2C%20aiding%20both%20tracking%20and%20mapping%20accuracy.%20Unlike%20previous%20semantic%20SLAM%20approaches%20%28which%20embed%20pre-defined%20class%20labels%29%20FeatureSLAM%20enables%20entirely%20new%20downstream%20tasks%20via%20free-viewpoint%2C%20open-set%20segmentation.%20Across%20standard%20benchmarks%2C%20our%20method%20achieves%20real-time%20tracking%2C%20on%20par%20with%20state-of-the-art%20systems%20while%20improving%20tracking%20stability%20and%20map%20fidelity%20without%20prohibitive%20compute.%20Quantitatively%2C%20we%20obtain%209%5C%25%20lower%20pose%20error%20and%208%5C%25%20higher%20mapping%20accuracy%20compared%20to%20recent%20fixed-set%20SLAM%20baselines.%20Our%20results%20confirm%20that%20real-time%20feature-embedded%20SLAM%2C%20is%20not%20only%20valuable%20for%20enabling%20new%20downstream%20applications.%20It%20also%20improves%20the%20performance%20of%20the%20underlying%20tracking%20and%20mapping%20subsystems%2C%20providing%20semantic%20and%20language%20masking%20results%20that%20are%20on-par%20with%20offline%203DGS%20models%2C%20alongside%20state-of-the-art%20tracking%2C%20depth%20and%20RGB%20rendering.&entry.1838667208=http%3A//arxiv.org/abs/2601.05738v1&entry.124074799=Read"},
{"title": "Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation", "author": "Jin Wang and Jianxiang Lu and Comi Chen and Guangzheng Xu and Haoyu Yang and Peng Chen and Na Zhang and Yifan Xu and Longhuang Wu and Shuai Shao and Qinglin Lu and Ping Luo", "abstract": "Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.", "link": "http://arxiv.org/abs/2601.05722v1", "date": "2026-01-09", "relevancy": 3.3837, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6775}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6775}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rotate%20Your%20Character%3A%20Revisiting%20Video%20Diffusion%20Models%20for%20High-Quality%203D%20Character%20Generation&body=Title%3A%20Rotate%20Your%20Character%3A%20Revisiting%20Video%20Diffusion%20Models%20for%20High-Quality%203D%20Character%20Generation%0AAuthor%3A%20Jin%20Wang%20and%20Jianxiang%20Lu%20and%20Comi%20Chen%20and%20Guangzheng%20Xu%20and%20Haoyu%20Yang%20and%20Peng%20Chen%20and%20Na%20Zhang%20and%20Yifan%20Xu%20and%20Longhuang%20Wu%20and%20Shuai%20Shao%20and%20Qinglin%20Lu%20and%20Ping%20Luo%0AAbstract%3A%20Generating%20high-quality%203D%20characters%20from%20single%20images%20remains%20a%20significant%20challenge%20in%20digital%20content%20creation%2C%20particularly%20due%20to%20complex%20body%20poses%20and%20self-occlusion.%20In%20this%20paper%2C%20we%20present%20RCM%20%28Rotate%20your%20Character%20Model%29%2C%20an%20advanced%20image-to-video%20diffusion%20framework%20tailored%20for%20high-quality%20novel%20view%20synthesis%20%28NVS%29%20and%203D%20character%20generation.%20Compared%20to%20existing%20diffusion-based%20approaches%2C%20RCM%20offers%20several%20key%20advantages%3A%20%281%29%20transferring%20characters%20with%20any%20complex%20poses%20into%20a%20canonical%20pose%2C%20enabling%20consistent%20novel%20view%20synthesis%20across%20the%20entire%20viewing%20orbit%2C%20%282%29%20high-resolution%20orbital%20video%20generation%20at%201024x1024%20resolution%2C%20%283%29%20controllable%20observation%20positions%20given%20different%20initial%20camera%20poses%2C%20and%20%284%29%20multi-view%20conditioning%20supporting%20up%20to%204%20input%20images%2C%20accommodating%20diverse%20user%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20RCM%20outperforms%20state-of-the-art%20methods%20in%20both%20novel%20view%20synthesis%20and%203D%20generation%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotate%2520Your%2520Character%253A%2520Revisiting%2520Video%2520Diffusion%2520Models%2520for%2520High-Quality%25203D%2520Character%2520Generation%26entry.906535625%3DJin%2520Wang%2520and%2520Jianxiang%2520Lu%2520and%2520Comi%2520Chen%2520and%2520Guangzheng%2520Xu%2520and%2520Haoyu%2520Yang%2520and%2520Peng%2520Chen%2520and%2520Na%2520Zhang%2520and%2520Yifan%2520Xu%2520and%2520Longhuang%2520Wu%2520and%2520Shuai%2520Shao%2520and%2520Qinglin%2520Lu%2520and%2520Ping%2520Luo%26entry.1292438233%3DGenerating%2520high-quality%25203D%2520characters%2520from%2520single%2520images%2520remains%2520a%2520significant%2520challenge%2520in%2520digital%2520content%2520creation%252C%2520particularly%2520due%2520to%2520complex%2520body%2520poses%2520and%2520self-occlusion.%2520In%2520this%2520paper%252C%2520we%2520present%2520RCM%2520%2528Rotate%2520your%2520Character%2520Model%2529%252C%2520an%2520advanced%2520image-to-video%2520diffusion%2520framework%2520tailored%2520for%2520high-quality%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520and%25203D%2520character%2520generation.%2520Compared%2520to%2520existing%2520diffusion-based%2520approaches%252C%2520RCM%2520offers%2520several%2520key%2520advantages%253A%2520%25281%2529%2520transferring%2520characters%2520with%2520any%2520complex%2520poses%2520into%2520a%2520canonical%2520pose%252C%2520enabling%2520consistent%2520novel%2520view%2520synthesis%2520across%2520the%2520entire%2520viewing%2520orbit%252C%2520%25282%2529%2520high-resolution%2520orbital%2520video%2520generation%2520at%25201024x1024%2520resolution%252C%2520%25283%2529%2520controllable%2520observation%2520positions%2520given%2520different%2520initial%2520camera%2520poses%252C%2520and%2520%25284%2529%2520multi-view%2520conditioning%2520supporting%2520up%2520to%25204%2520input%2520images%252C%2520accommodating%2520diverse%2520user%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RCM%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520novel%2520view%2520synthesis%2520and%25203D%2520generation%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotate%20Your%20Character%3A%20Revisiting%20Video%20Diffusion%20Models%20for%20High-Quality%203D%20Character%20Generation&entry.906535625=Jin%20Wang%20and%20Jianxiang%20Lu%20and%20Comi%20Chen%20and%20Guangzheng%20Xu%20and%20Haoyu%20Yang%20and%20Peng%20Chen%20and%20Na%20Zhang%20and%20Yifan%20Xu%20and%20Longhuang%20Wu%20and%20Shuai%20Shao%20and%20Qinglin%20Lu%20and%20Ping%20Luo&entry.1292438233=Generating%20high-quality%203D%20characters%20from%20single%20images%20remains%20a%20significant%20challenge%20in%20digital%20content%20creation%2C%20particularly%20due%20to%20complex%20body%20poses%20and%20self-occlusion.%20In%20this%20paper%2C%20we%20present%20RCM%20%28Rotate%20your%20Character%20Model%29%2C%20an%20advanced%20image-to-video%20diffusion%20framework%20tailored%20for%20high-quality%20novel%20view%20synthesis%20%28NVS%29%20and%203D%20character%20generation.%20Compared%20to%20existing%20diffusion-based%20approaches%2C%20RCM%20offers%20several%20key%20advantages%3A%20%281%29%20transferring%20characters%20with%20any%20complex%20poses%20into%20a%20canonical%20pose%2C%20enabling%20consistent%20novel%20view%20synthesis%20across%20the%20entire%20viewing%20orbit%2C%20%282%29%20high-resolution%20orbital%20video%20generation%20at%201024x1024%20resolution%2C%20%283%29%20controllable%20observation%20positions%20given%20different%20initial%20camera%20poses%2C%20and%20%284%29%20multi-view%20conditioning%20supporting%20up%20to%204%20input%20images%2C%20accommodating%20diverse%20user%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20RCM%20outperforms%20state-of-the-art%20methods%20in%20both%20novel%20view%20synthesis%20and%203D%20generation%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.05722v1&entry.124074799=Read"},
{"title": "LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting", "author": "Yinghan Xu and John Dingliana", "abstract": "We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS", "link": "http://arxiv.org/abs/2601.05853v1", "date": "2026-01-09", "relevancy": 3.3423, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7188}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6643}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerGS%3A%20Decomposition%20and%20Inpainting%20of%20Layered%203D%20Human%20Avatars%20via%202D%20Gaussian%20Splatting&body=Title%3A%20LayerGS%3A%20Decomposition%20and%20Inpainting%20of%20Layered%203D%20Human%20Avatars%20via%202D%20Gaussian%20Splatting%0AAuthor%3A%20Yinghan%20Xu%20and%20John%20Dingliana%0AAbstract%3A%20We%20propose%20a%20novel%20framework%20for%20decomposing%20arbitrarily%20posed%20humans%20into%20animatable%20multi-layered%203D%20human%20avatars%2C%20separating%20the%20body%20and%20garments.%20Conventional%20single-layer%20reconstruction%20methods%20lock%20clothing%20to%20one%20identity%2C%20while%20prior%20multi-layer%20approaches%20struggle%20with%20occluded%20regions.%20We%20overcome%20both%20limitations%20by%20encoding%20each%20layer%20as%20a%20set%20of%202D%20Gaussians%20for%20accurate%20geometry%20and%20photorealistic%20rendering%2C%20and%20inpainting%20hidden%20regions%20with%20a%20pretrained%202D%20diffusion%20model%20via%20score-distillation%20sampling%20%28SDS%29.%20Our%20three-stage%20training%20strategy%20first%20reconstructs%20the%20coarse%20canonical%20garment%20via%20single-layer%20reconstruction%2C%20followed%20by%20multi-layer%20training%20to%20jointly%20recover%20the%20inner-layer%20body%20and%20outer-layer%20garment%20details.%20Experiments%20on%20two%203D%20human%20benchmark%20datasets%20%284D-Dress%2C%20Thuman2.0%29%20show%20that%20our%20approach%20achieves%20better%20rendering%20quality%20and%20layer%20decomposition%20and%20recomposition%20than%20the%20previous%20state-of-the-art%2C%20enabling%20realistic%20virtual%20try-on%20under%20novel%20viewpoints%20and%20poses%2C%20and%20advancing%20practical%20creation%20of%20high-fidelity%203D%20human%20assets%20for%20immersive%20applications.%20Our%20code%20is%20available%20at%20https%3A//github.com/RockyXu66/LayerGS%0ALink%3A%20http%3A//arxiv.org/abs/2601.05853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerGS%253A%2520Decomposition%2520and%2520Inpainting%2520of%2520Layered%25203D%2520Human%2520Avatars%2520via%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DYinghan%2520Xu%2520and%2520John%2520Dingliana%26entry.1292438233%3DWe%2520propose%2520a%2520novel%2520framework%2520for%2520decomposing%2520arbitrarily%2520posed%2520humans%2520into%2520animatable%2520multi-layered%25203D%2520human%2520avatars%252C%2520separating%2520the%2520body%2520and%2520garments.%2520Conventional%2520single-layer%2520reconstruction%2520methods%2520lock%2520clothing%2520to%2520one%2520identity%252C%2520while%2520prior%2520multi-layer%2520approaches%2520struggle%2520with%2520occluded%2520regions.%2520We%2520overcome%2520both%2520limitations%2520by%2520encoding%2520each%2520layer%2520as%2520a%2520set%2520of%25202D%2520Gaussians%2520for%2520accurate%2520geometry%2520and%2520photorealistic%2520rendering%252C%2520and%2520inpainting%2520hidden%2520regions%2520with%2520a%2520pretrained%25202D%2520diffusion%2520model%2520via%2520score-distillation%2520sampling%2520%2528SDS%2529.%2520Our%2520three-stage%2520training%2520strategy%2520first%2520reconstructs%2520the%2520coarse%2520canonical%2520garment%2520via%2520single-layer%2520reconstruction%252C%2520followed%2520by%2520multi-layer%2520training%2520to%2520jointly%2520recover%2520the%2520inner-layer%2520body%2520and%2520outer-layer%2520garment%2520details.%2520Experiments%2520on%2520two%25203D%2520human%2520benchmark%2520datasets%2520%25284D-Dress%252C%2520Thuman2.0%2529%2520show%2520that%2520our%2520approach%2520achieves%2520better%2520rendering%2520quality%2520and%2520layer%2520decomposition%2520and%2520recomposition%2520than%2520the%2520previous%2520state-of-the-art%252C%2520enabling%2520realistic%2520virtual%2520try-on%2520under%2520novel%2520viewpoints%2520and%2520poses%252C%2520and%2520advancing%2520practical%2520creation%2520of%2520high-fidelity%25203D%2520human%2520assets%2520for%2520immersive%2520applications.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/RockyXu66/LayerGS%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerGS%3A%20Decomposition%20and%20Inpainting%20of%20Layered%203D%20Human%20Avatars%20via%202D%20Gaussian%20Splatting&entry.906535625=Yinghan%20Xu%20and%20John%20Dingliana&entry.1292438233=We%20propose%20a%20novel%20framework%20for%20decomposing%20arbitrarily%20posed%20humans%20into%20animatable%20multi-layered%203D%20human%20avatars%2C%20separating%20the%20body%20and%20garments.%20Conventional%20single-layer%20reconstruction%20methods%20lock%20clothing%20to%20one%20identity%2C%20while%20prior%20multi-layer%20approaches%20struggle%20with%20occluded%20regions.%20We%20overcome%20both%20limitations%20by%20encoding%20each%20layer%20as%20a%20set%20of%202D%20Gaussians%20for%20accurate%20geometry%20and%20photorealistic%20rendering%2C%20and%20inpainting%20hidden%20regions%20with%20a%20pretrained%202D%20diffusion%20model%20via%20score-distillation%20sampling%20%28SDS%29.%20Our%20three-stage%20training%20strategy%20first%20reconstructs%20the%20coarse%20canonical%20garment%20via%20single-layer%20reconstruction%2C%20followed%20by%20multi-layer%20training%20to%20jointly%20recover%20the%20inner-layer%20body%20and%20outer-layer%20garment%20details.%20Experiments%20on%20two%203D%20human%20benchmark%20datasets%20%284D-Dress%2C%20Thuman2.0%29%20show%20that%20our%20approach%20achieves%20better%20rendering%20quality%20and%20layer%20decomposition%20and%20recomposition%20than%20the%20previous%20state-of-the-art%2C%20enabling%20realistic%20virtual%20try-on%20under%20novel%20viewpoints%20and%20poses%2C%20and%20advancing%20practical%20creation%20of%20high-fidelity%203D%20human%20assets%20for%20immersive%20applications.%20Our%20code%20is%20available%20at%20https%3A//github.com/RockyXu66/LayerGS&entry.1838667208=http%3A//arxiv.org/abs/2601.05853v1&entry.124074799=Read"},
{"title": "Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism", "author": "Siyu Zhang and Lianlei Shan and Runhe Qiu", "abstract": "Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.", "link": "http://arxiv.org/abs/2512.23243v2", "date": "2026-01-09", "relevancy": 3.0189, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Interpretation%20of%20Remote%20Sensing%20Images%3A%20Dynamic%20Resolution%20Input%20Strategy%20and%20Multi-scale%20Vision-Language%20Alignment%20Mechanism&body=Title%3A%20Multimodal%20Interpretation%20of%20Remote%20Sensing%20Images%3A%20Dynamic%20Resolution%20Input%20Strategy%20and%20Multi-scale%20Vision-Language%20Alignment%20Mechanism%0AAuthor%3A%20Siyu%20Zhang%20and%20Lianlei%20Shan%20and%20Runhe%20Qiu%0AAbstract%3A%20Multimodal%20fusion%20of%20remote%20sensing%20images%20serves%20as%20a%20core%20technology%20for%20overcoming%20the%20limitations%20of%20single-source%20data%20and%20improving%20the%20accuracy%20of%20surface%20information%20extraction%2C%20which%20exhibits%20significant%20application%20value%20in%20fields%20such%20as%20environmental%20monitoring%20and%20urban%20planning.%20To%20address%20the%20deficiencies%20of%20existing%20methods%2C%20including%20the%20failure%20of%20fixed%20resolutions%20to%20balance%20efficiency%20and%20detail%2C%20as%20well%20as%20the%20lack%20of%20semantic%20hierarchy%20in%20single-scale%20alignment%2C%20this%20study%20proposes%20a%20Vision-language%20Model%20%28VLM%29%20framework%20integrated%20with%20two%20key%20innovations%3A%20the%20Dynamic%20Resolution%20Input%20Strategy%20%28DRIS%29%20and%20the%20Multi-scale%20Vision-language%20Alignment%20Mechanism%20%28MS-VLAM%29.Specifically%2C%20the%20DRIS%20adopts%20a%20coarse-to-fine%20approach%20to%20adaptively%20allocate%20computational%20resources%20according%20to%20the%20complexity%20of%20image%20content%2C%20thereby%20preserving%20key%20fine-grained%20features%20while%20reducing%20redundant%20computational%20overhead.%20The%20MS-VLAM%20constructs%20a%20three-tier%20alignment%20mechanism%20covering%20object%2C%20local-region%20and%20global%20levels%2C%20which%20systematically%20captures%20cross-modal%20semantic%20consistency%20and%20alleviates%20issues%20of%20semantic%20misalignment%20and%20granularity%20imbalance.Experimental%20results%20on%20the%20RS-GPT4V%20dataset%20demonstrate%20that%20the%20proposed%20framework%20significantly%20improves%20the%20accuracy%20of%20semantic%20understanding%20and%20computational%20efficiency%20in%20tasks%20including%20image%20captioning%20and%20cross-modal%20retrieval.%20Compared%20with%20conventional%20methods%2C%20it%20achieves%20superior%20performance%20in%20evaluation%20metrics%20such%20as%20BLEU-4%20and%20CIDEr%20for%20image%20captioning%2C%20as%20well%20as%20R%4010%20for%20cross-modal%20retrieval.%20This%20technical%20framework%20provides%20a%20novel%20approach%20for%20constructing%20efficient%20and%20robust%20multimodal%20remote%20sensing%20systems%2C%20laying%20a%20theoretical%20foundation%20and%20offering%20technical%20guidance%20for%20the%20engineering%20application%20of%20intelligent%20remote%20sensing%20interpretation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Interpretation%2520of%2520Remote%2520Sensing%2520Images%253A%2520Dynamic%2520Resolution%2520Input%2520Strategy%2520and%2520Multi-scale%2520Vision-Language%2520Alignment%2520Mechanism%26entry.906535625%3DSiyu%2520Zhang%2520and%2520Lianlei%2520Shan%2520and%2520Runhe%2520Qiu%26entry.1292438233%3DMultimodal%2520fusion%2520of%2520remote%2520sensing%2520images%2520serves%2520as%2520a%2520core%2520technology%2520for%2520overcoming%2520the%2520limitations%2520of%2520single-source%2520data%2520and%2520improving%2520the%2520accuracy%2520of%2520surface%2520information%2520extraction%252C%2520which%2520exhibits%2520significant%2520application%2520value%2520in%2520fields%2520such%2520as%2520environmental%2520monitoring%2520and%2520urban%2520planning.%2520To%2520address%2520the%2520deficiencies%2520of%2520existing%2520methods%252C%2520including%2520the%2520failure%2520of%2520fixed%2520resolutions%2520to%2520balance%2520efficiency%2520and%2520detail%252C%2520as%2520well%2520as%2520the%2520lack%2520of%2520semantic%2520hierarchy%2520in%2520single-scale%2520alignment%252C%2520this%2520study%2520proposes%2520a%2520Vision-language%2520Model%2520%2528VLM%2529%2520framework%2520integrated%2520with%2520two%2520key%2520innovations%253A%2520the%2520Dynamic%2520Resolution%2520Input%2520Strategy%2520%2528DRIS%2529%2520and%2520the%2520Multi-scale%2520Vision-language%2520Alignment%2520Mechanism%2520%2528MS-VLAM%2529.Specifically%252C%2520the%2520DRIS%2520adopts%2520a%2520coarse-to-fine%2520approach%2520to%2520adaptively%2520allocate%2520computational%2520resources%2520according%2520to%2520the%2520complexity%2520of%2520image%2520content%252C%2520thereby%2520preserving%2520key%2520fine-grained%2520features%2520while%2520reducing%2520redundant%2520computational%2520overhead.%2520The%2520MS-VLAM%2520constructs%2520a%2520three-tier%2520alignment%2520mechanism%2520covering%2520object%252C%2520local-region%2520and%2520global%2520levels%252C%2520which%2520systematically%2520captures%2520cross-modal%2520semantic%2520consistency%2520and%2520alleviates%2520issues%2520of%2520semantic%2520misalignment%2520and%2520granularity%2520imbalance.Experimental%2520results%2520on%2520the%2520RS-GPT4V%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520framework%2520significantly%2520improves%2520the%2520accuracy%2520of%2520semantic%2520understanding%2520and%2520computational%2520efficiency%2520in%2520tasks%2520including%2520image%2520captioning%2520and%2520cross-modal%2520retrieval.%2520Compared%2520with%2520conventional%2520methods%252C%2520it%2520achieves%2520superior%2520performance%2520in%2520evaluation%2520metrics%2520such%2520as%2520BLEU-4%2520and%2520CIDEr%2520for%2520image%2520captioning%252C%2520as%2520well%2520as%2520R%254010%2520for%2520cross-modal%2520retrieval.%2520This%2520technical%2520framework%2520provides%2520a%2520novel%2520approach%2520for%2520constructing%2520efficient%2520and%2520robust%2520multimodal%2520remote%2520sensing%2520systems%252C%2520laying%2520a%2520theoretical%2520foundation%2520and%2520offering%2520technical%2520guidance%2520for%2520the%2520engineering%2520application%2520of%2520intelligent%2520remote%2520sensing%2520interpretation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Interpretation%20of%20Remote%20Sensing%20Images%3A%20Dynamic%20Resolution%20Input%20Strategy%20and%20Multi-scale%20Vision-Language%20Alignment%20Mechanism&entry.906535625=Siyu%20Zhang%20and%20Lianlei%20Shan%20and%20Runhe%20Qiu&entry.1292438233=Multimodal%20fusion%20of%20remote%20sensing%20images%20serves%20as%20a%20core%20technology%20for%20overcoming%20the%20limitations%20of%20single-source%20data%20and%20improving%20the%20accuracy%20of%20surface%20information%20extraction%2C%20which%20exhibits%20significant%20application%20value%20in%20fields%20such%20as%20environmental%20monitoring%20and%20urban%20planning.%20To%20address%20the%20deficiencies%20of%20existing%20methods%2C%20including%20the%20failure%20of%20fixed%20resolutions%20to%20balance%20efficiency%20and%20detail%2C%20as%20well%20as%20the%20lack%20of%20semantic%20hierarchy%20in%20single-scale%20alignment%2C%20this%20study%20proposes%20a%20Vision-language%20Model%20%28VLM%29%20framework%20integrated%20with%20two%20key%20innovations%3A%20the%20Dynamic%20Resolution%20Input%20Strategy%20%28DRIS%29%20and%20the%20Multi-scale%20Vision-language%20Alignment%20Mechanism%20%28MS-VLAM%29.Specifically%2C%20the%20DRIS%20adopts%20a%20coarse-to-fine%20approach%20to%20adaptively%20allocate%20computational%20resources%20according%20to%20the%20complexity%20of%20image%20content%2C%20thereby%20preserving%20key%20fine-grained%20features%20while%20reducing%20redundant%20computational%20overhead.%20The%20MS-VLAM%20constructs%20a%20three-tier%20alignment%20mechanism%20covering%20object%2C%20local-region%20and%20global%20levels%2C%20which%20systematically%20captures%20cross-modal%20semantic%20consistency%20and%20alleviates%20issues%20of%20semantic%20misalignment%20and%20granularity%20imbalance.Experimental%20results%20on%20the%20RS-GPT4V%20dataset%20demonstrate%20that%20the%20proposed%20framework%20significantly%20improves%20the%20accuracy%20of%20semantic%20understanding%20and%20computational%20efficiency%20in%20tasks%20including%20image%20captioning%20and%20cross-modal%20retrieval.%20Compared%20with%20conventional%20methods%2C%20it%20achieves%20superior%20performance%20in%20evaluation%20metrics%20such%20as%20BLEU-4%20and%20CIDEr%20for%20image%20captioning%2C%20as%20well%20as%20R%4010%20for%20cross-modal%20retrieval.%20This%20technical%20framework%20provides%20a%20novel%20approach%20for%20constructing%20efficient%20and%20robust%20multimodal%20remote%20sensing%20systems%2C%20laying%20a%20theoretical%20foundation%20and%20offering%20technical%20guidance%20for%20the%20engineering%20application%20of%20intelligent%20remote%20sensing%20interpretation.&entry.1838667208=http%3A//arxiv.org/abs/2512.23243v2&entry.124074799=Read"},
{"title": "Co-Training Vision Language Models for Remote Sensing Multi-task Learning", "author": "Qingyun Li and Shuran Ma and Junwei Luo and Yi Yu and Yue Zhou and Fengxiang Wang and Xudong Lu and Xiaoxing Wang and Xin He and Yushi Chen and Xue Yang", "abstract": "With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.", "link": "http://arxiv.org/abs/2511.21272v2", "date": "2026-01-09", "relevancy": 2.9106, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Training%20Vision%20Language%20Models%20for%20Remote%20Sensing%20Multi-task%20Learning&body=Title%3A%20Co-Training%20Vision%20Language%20Models%20for%20Remote%20Sensing%20Multi-task%20Learning%0AAuthor%3A%20Qingyun%20Li%20and%20Shuran%20Ma%20and%20Junwei%20Luo%20and%20Yi%20Yu%20and%20Yue%20Zhou%20and%20Fengxiang%20Wang%20and%20Xudong%20Lu%20and%20Xiaoxing%20Wang%20and%20Xin%20He%20and%20Yushi%20Chen%20and%20Xue%20Yang%0AAbstract%3A%20With%20Transformers%20achieving%20outstanding%20performance%20on%20individual%20remote%20sensing%20%28RS%29%20tasks%2C%20we%20are%20now%20approaching%20the%20realization%20of%20a%20unified%20model%20that%20excels%20across%20multiple%20tasks%20through%20multi-task%20learning%20%28MTL%29.%20Compared%20to%20single-task%20approaches%2C%20MTL%20methods%20offer%20improved%20generalization%2C%20enhanced%20scalability%2C%20and%20greater%20practical%20applicability.%20Recently%2C%20vision%20language%20models%20%28VLMs%29%20have%20achieved%20promising%20results%20in%20RS%20image%20understanding%2C%20grounding%2C%20and%20ultra-high-resolution%20%28UHR%29%20image%20reasoning%2C%20respectively.%20Moreover%2C%20the%20unified%20text-based%20interface%20demonstrates%20significant%20potential%20for%20MTL.%20Hence%2C%20in%20this%20work%2C%20we%20present%20RSCoVLM%2C%20a%20simple%20yet%20flexible%20VLM%20baseline%20for%20RS%20MTL.%20Firstly%2C%20we%20create%20the%20data%20curation%20engine%2C%20including%20data%20acquisition%2C%20offline%20processing%20and%20integrating%2C%20as%20well%20as%20online%20loading%20and%20weighting.%20This%20data%20engine%20effectively%20addresses%20complex%20RS%20data%20enviroment%20and%20generates%20flexible%20vision-language%20conversations.%20Furthermore%2C%20we%20propose%20a%20unified%20dynamic-resolution%20strategy%20to%20address%20the%20diverse%20image%20scales%20inherent%20in%20RS%20imagery.%20For%20UHR%20images%2C%20we%20introduce%20the%20Zoom-in%20Chain%20mechanism%20together%20with%20its%20corresponding%20dataset%2C%20LRS-VQA-Zoom.%20The%20strategies%20are%20flexible%20and%20effectively%20mitigate%20the%20computational%20burdens.%20Additionally%2C%20we%20significantly%20enhance%20the%20model%27s%20object%20detection%20capability%20and%20propose%20a%20novel%20evaluation%20protocol%20that%20ensures%20fair%20comparison%20between%20VLMs%20and%20conventional%20detection%20models.%20Extensive%20experiments%20demonstrate%20that%20RSCoVLM%20achieves%20state-of-the-art%20performance%20across%20diverse%20tasks%2C%20outperforming%20existing%20RS%20VLMs%20and%20even%20rivaling%20specialized%20expert%20models.%20All%20the%20training%20and%20evaluating%20tools%2C%20model%20weights%2C%20and%20datasets%20have%20been%20fully%20open-sourced%20to%20support%20reproducibility.%20We%20expect%20that%20this%20baseline%20will%20promote%20further%20progress%20toward%20general-purpose%20RS%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Training%2520Vision%2520Language%2520Models%2520for%2520Remote%2520Sensing%2520Multi-task%2520Learning%26entry.906535625%3DQingyun%2520Li%2520and%2520Shuran%2520Ma%2520and%2520Junwei%2520Luo%2520and%2520Yi%2520Yu%2520and%2520Yue%2520Zhou%2520and%2520Fengxiang%2520Wang%2520and%2520Xudong%2520Lu%2520and%2520Xiaoxing%2520Wang%2520and%2520Xin%2520He%2520and%2520Yushi%2520Chen%2520and%2520Xue%2520Yang%26entry.1292438233%3DWith%2520Transformers%2520achieving%2520outstanding%2520performance%2520on%2520individual%2520remote%2520sensing%2520%2528RS%2529%2520tasks%252C%2520we%2520are%2520now%2520approaching%2520the%2520realization%2520of%2520a%2520unified%2520model%2520that%2520excels%2520across%2520multiple%2520tasks%2520through%2520multi-task%2520learning%2520%2528MTL%2529.%2520Compared%2520to%2520single-task%2520approaches%252C%2520MTL%2520methods%2520offer%2520improved%2520generalization%252C%2520enhanced%2520scalability%252C%2520and%2520greater%2520practical%2520applicability.%2520Recently%252C%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520promising%2520results%2520in%2520RS%2520image%2520understanding%252C%2520grounding%252C%2520and%2520ultra-high-resolution%2520%2528UHR%2529%2520image%2520reasoning%252C%2520respectively.%2520Moreover%252C%2520the%2520unified%2520text-based%2520interface%2520demonstrates%2520significant%2520potential%2520for%2520MTL.%2520Hence%252C%2520in%2520this%2520work%252C%2520we%2520present%2520RSCoVLM%252C%2520a%2520simple%2520yet%2520flexible%2520VLM%2520baseline%2520for%2520RS%2520MTL.%2520Firstly%252C%2520we%2520create%2520the%2520data%2520curation%2520engine%252C%2520including%2520data%2520acquisition%252C%2520offline%2520processing%2520and%2520integrating%252C%2520as%2520well%2520as%2520online%2520loading%2520and%2520weighting.%2520This%2520data%2520engine%2520effectively%2520addresses%2520complex%2520RS%2520data%2520enviroment%2520and%2520generates%2520flexible%2520vision-language%2520conversations.%2520Furthermore%252C%2520we%2520propose%2520a%2520unified%2520dynamic-resolution%2520strategy%2520to%2520address%2520the%2520diverse%2520image%2520scales%2520inherent%2520in%2520RS%2520imagery.%2520For%2520UHR%2520images%252C%2520we%2520introduce%2520the%2520Zoom-in%2520Chain%2520mechanism%2520together%2520with%2520its%2520corresponding%2520dataset%252C%2520LRS-VQA-Zoom.%2520The%2520strategies%2520are%2520flexible%2520and%2520effectively%2520mitigate%2520the%2520computational%2520burdens.%2520Additionally%252C%2520we%2520significantly%2520enhance%2520the%2520model%2527s%2520object%2520detection%2520capability%2520and%2520propose%2520a%2520novel%2520evaluation%2520protocol%2520that%2520ensures%2520fair%2520comparison%2520between%2520VLMs%2520and%2520conventional%2520detection%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RSCoVLM%2520achieves%2520state-of-the-art%2520performance%2520across%2520diverse%2520tasks%252C%2520outperforming%2520existing%2520RS%2520VLMs%2520and%2520even%2520rivaling%2520specialized%2520expert%2520models.%2520All%2520the%2520training%2520and%2520evaluating%2520tools%252C%2520model%2520weights%252C%2520and%2520datasets%2520have%2520been%2520fully%2520open-sourced%2520to%2520support%2520reproducibility.%2520We%2520expect%2520that%2520this%2520baseline%2520will%2520promote%2520further%2520progress%2520toward%2520general-purpose%2520RS%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Training%20Vision%20Language%20Models%20for%20Remote%20Sensing%20Multi-task%20Learning&entry.906535625=Qingyun%20Li%20and%20Shuran%20Ma%20and%20Junwei%20Luo%20and%20Yi%20Yu%20and%20Yue%20Zhou%20and%20Fengxiang%20Wang%20and%20Xudong%20Lu%20and%20Xiaoxing%20Wang%20and%20Xin%20He%20and%20Yushi%20Chen%20and%20Xue%20Yang&entry.1292438233=With%20Transformers%20achieving%20outstanding%20performance%20on%20individual%20remote%20sensing%20%28RS%29%20tasks%2C%20we%20are%20now%20approaching%20the%20realization%20of%20a%20unified%20model%20that%20excels%20across%20multiple%20tasks%20through%20multi-task%20learning%20%28MTL%29.%20Compared%20to%20single-task%20approaches%2C%20MTL%20methods%20offer%20improved%20generalization%2C%20enhanced%20scalability%2C%20and%20greater%20practical%20applicability.%20Recently%2C%20vision%20language%20models%20%28VLMs%29%20have%20achieved%20promising%20results%20in%20RS%20image%20understanding%2C%20grounding%2C%20and%20ultra-high-resolution%20%28UHR%29%20image%20reasoning%2C%20respectively.%20Moreover%2C%20the%20unified%20text-based%20interface%20demonstrates%20significant%20potential%20for%20MTL.%20Hence%2C%20in%20this%20work%2C%20we%20present%20RSCoVLM%2C%20a%20simple%20yet%20flexible%20VLM%20baseline%20for%20RS%20MTL.%20Firstly%2C%20we%20create%20the%20data%20curation%20engine%2C%20including%20data%20acquisition%2C%20offline%20processing%20and%20integrating%2C%20as%20well%20as%20online%20loading%20and%20weighting.%20This%20data%20engine%20effectively%20addresses%20complex%20RS%20data%20enviroment%20and%20generates%20flexible%20vision-language%20conversations.%20Furthermore%2C%20we%20propose%20a%20unified%20dynamic-resolution%20strategy%20to%20address%20the%20diverse%20image%20scales%20inherent%20in%20RS%20imagery.%20For%20UHR%20images%2C%20we%20introduce%20the%20Zoom-in%20Chain%20mechanism%20together%20with%20its%20corresponding%20dataset%2C%20LRS-VQA-Zoom.%20The%20strategies%20are%20flexible%20and%20effectively%20mitigate%20the%20computational%20burdens.%20Additionally%2C%20we%20significantly%20enhance%20the%20model%27s%20object%20detection%20capability%20and%20propose%20a%20novel%20evaluation%20protocol%20that%20ensures%20fair%20comparison%20between%20VLMs%20and%20conventional%20detection%20models.%20Extensive%20experiments%20demonstrate%20that%20RSCoVLM%20achieves%20state-of-the-art%20performance%20across%20diverse%20tasks%2C%20outperforming%20existing%20RS%20VLMs%20and%20even%20rivaling%20specialized%20expert%20models.%20All%20the%20training%20and%20evaluating%20tools%2C%20model%20weights%2C%20and%20datasets%20have%20been%20fully%20open-sourced%20to%20support%20reproducibility.%20We%20expect%20that%20this%20baseline%20will%20promote%20further%20progress%20toward%20general-purpose%20RS%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.21272v2&entry.124074799=Read"},
{"title": "Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images", "author": "Zhaoyi Wang and Jemil Avers Butt and Shengyu Huang and Tomislav Medic and Andreas Wieser", "abstract": "Landslide monitoring is essential for understanding geohazards and mitigating associated risks. Existing point cloud-based methods, however, typically rely on either geometric or radiometric information and often yield sparse or non-3D displacement estimates. In this paper, we propose a hierarchical partitioning-based coarse-to-fine approach that integrates 3D point clouds and co-registered RGB images to estimate dense 3D displacement vector fields. Patch-level matches are constructed using both 3D geometry and 2D image features, refined via geometric consistency checks, and followed by rigid transformation estimation per match. Experimental results on two real-world landslide datasets demonstrate that the proposed method produces 3D displacement estimates with high spatial coverage (79% and 97%) and accuracy. Deviations in displacement magnitude with respect to external measurements (total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets, respectively, and only 0.07 m and 0.20 m compared to manually derived references, all below the mean scan resolutions (0.08 m and 0.30 m). Compared with the state-of-the-art method F2S3, the proposed approach improves spatial coverage while maintaining comparable accuracy. The proposed approach offers a practical and adaptable solution for TLS-based landslide monitoring and is extensible to other types of point clouds and monitoring tasks. The example data and source code are publicly available at https://github.com/gseg-ethz/fusion4landslide.", "link": "http://arxiv.org/abs/2506.16265v2", "date": "2026-01-09", "relevancy": 2.8633, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6037}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5623}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%203D%20Displacement%20Estimation%20for%20Landslide%20Monitoring%20via%20Fusion%20of%20TLS%20Point%20Clouds%20and%20Embedded%20RGB%20Images&body=Title%3A%20Dense%203D%20Displacement%20Estimation%20for%20Landslide%20Monitoring%20via%20Fusion%20of%20TLS%20Point%20Clouds%20and%20Embedded%20RGB%20Images%0AAuthor%3A%20Zhaoyi%20Wang%20and%20Jemil%20Avers%20Butt%20and%20Shengyu%20Huang%20and%20Tomislav%20Medic%20and%20Andreas%20Wieser%0AAbstract%3A%20Landslide%20monitoring%20is%20essential%20for%20understanding%20geohazards%20and%20mitigating%20associated%20risks.%20Existing%20point%20cloud-based%20methods%2C%20however%2C%20typically%20rely%20on%20either%20geometric%20or%20radiometric%20information%20and%20often%20yield%20sparse%20or%20non-3D%20displacement%20estimates.%20In%20this%20paper%2C%20we%20propose%20a%20hierarchical%20partitioning-based%20coarse-to-fine%20approach%20that%20integrates%203D%20point%20clouds%20and%20co-registered%20RGB%20images%20to%20estimate%20dense%203D%20displacement%20vector%20fields.%20Patch-level%20matches%20are%20constructed%20using%20both%203D%20geometry%20and%202D%20image%20features%2C%20refined%20via%20geometric%20consistency%20checks%2C%20and%20followed%20by%20rigid%20transformation%20estimation%20per%20match.%20Experimental%20results%20on%20two%20real-world%20landslide%20datasets%20demonstrate%20that%20the%20proposed%20method%20produces%203D%20displacement%20estimates%20with%20high%20spatial%20coverage%20%2879%25%20and%2097%25%29%20and%20accuracy.%20Deviations%20in%20displacement%20magnitude%20with%20respect%20to%20external%20measurements%20%28total%20station%20or%20GNSS%20observations%29%20are%200.15%20m%20and%200.25%20m%20on%20the%20two%20datasets%2C%20respectively%2C%20and%20only%200.07%20m%20and%200.20%20m%20compared%20to%20manually%20derived%20references%2C%20all%20below%20the%20mean%20scan%20resolutions%20%280.08%20m%20and%200.30%20m%29.%20Compared%20with%20the%20state-of-the-art%20method%20F2S3%2C%20the%20proposed%20approach%20improves%20spatial%20coverage%20while%20maintaining%20comparable%20accuracy.%20The%20proposed%20approach%20offers%20a%20practical%20and%20adaptable%20solution%20for%20TLS-based%20landslide%20monitoring%20and%20is%20extensible%20to%20other%20types%20of%20point%20clouds%20and%20monitoring%20tasks.%20The%20example%20data%20and%20source%20code%20are%20publicly%20available%20at%20https%3A//github.com/gseg-ethz/fusion4landslide.%0ALink%3A%20http%3A//arxiv.org/abs/2506.16265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%25203D%2520Displacement%2520Estimation%2520for%2520Landslide%2520Monitoring%2520via%2520Fusion%2520of%2520TLS%2520Point%2520Clouds%2520and%2520Embedded%2520RGB%2520Images%26entry.906535625%3DZhaoyi%2520Wang%2520and%2520Jemil%2520Avers%2520Butt%2520and%2520Shengyu%2520Huang%2520and%2520Tomislav%2520Medic%2520and%2520Andreas%2520Wieser%26entry.1292438233%3DLandslide%2520monitoring%2520is%2520essential%2520for%2520understanding%2520geohazards%2520and%2520mitigating%2520associated%2520risks.%2520Existing%2520point%2520cloud-based%2520methods%252C%2520however%252C%2520typically%2520rely%2520on%2520either%2520geometric%2520or%2520radiometric%2520information%2520and%2520often%2520yield%2520sparse%2520or%2520non-3D%2520displacement%2520estimates.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hierarchical%2520partitioning-based%2520coarse-to-fine%2520approach%2520that%2520integrates%25203D%2520point%2520clouds%2520and%2520co-registered%2520RGB%2520images%2520to%2520estimate%2520dense%25203D%2520displacement%2520vector%2520fields.%2520Patch-level%2520matches%2520are%2520constructed%2520using%2520both%25203D%2520geometry%2520and%25202D%2520image%2520features%252C%2520refined%2520via%2520geometric%2520consistency%2520checks%252C%2520and%2520followed%2520by%2520rigid%2520transformation%2520estimation%2520per%2520match.%2520Experimental%2520results%2520on%2520two%2520real-world%2520landslide%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520produces%25203D%2520displacement%2520estimates%2520with%2520high%2520spatial%2520coverage%2520%252879%2525%2520and%252097%2525%2529%2520and%2520accuracy.%2520Deviations%2520in%2520displacement%2520magnitude%2520with%2520respect%2520to%2520external%2520measurements%2520%2528total%2520station%2520or%2520GNSS%2520observations%2529%2520are%25200.15%2520m%2520and%25200.25%2520m%2520on%2520the%2520two%2520datasets%252C%2520respectively%252C%2520and%2520only%25200.07%2520m%2520and%25200.20%2520m%2520compared%2520to%2520manually%2520derived%2520references%252C%2520all%2520below%2520the%2520mean%2520scan%2520resolutions%2520%25280.08%2520m%2520and%25200.30%2520m%2529.%2520Compared%2520with%2520the%2520state-of-the-art%2520method%2520F2S3%252C%2520the%2520proposed%2520approach%2520improves%2520spatial%2520coverage%2520while%2520maintaining%2520comparable%2520accuracy.%2520The%2520proposed%2520approach%2520offers%2520a%2520practical%2520and%2520adaptable%2520solution%2520for%2520TLS-based%2520landslide%2520monitoring%2520and%2520is%2520extensible%2520to%2520other%2520types%2520of%2520point%2520clouds%2520and%2520monitoring%2520tasks.%2520The%2520example%2520data%2520and%2520source%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/gseg-ethz/fusion4landslide.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%203D%20Displacement%20Estimation%20for%20Landslide%20Monitoring%20via%20Fusion%20of%20TLS%20Point%20Clouds%20and%20Embedded%20RGB%20Images&entry.906535625=Zhaoyi%20Wang%20and%20Jemil%20Avers%20Butt%20and%20Shengyu%20Huang%20and%20Tomislav%20Medic%20and%20Andreas%20Wieser&entry.1292438233=Landslide%20monitoring%20is%20essential%20for%20understanding%20geohazards%20and%20mitigating%20associated%20risks.%20Existing%20point%20cloud-based%20methods%2C%20however%2C%20typically%20rely%20on%20either%20geometric%20or%20radiometric%20information%20and%20often%20yield%20sparse%20or%20non-3D%20displacement%20estimates.%20In%20this%20paper%2C%20we%20propose%20a%20hierarchical%20partitioning-based%20coarse-to-fine%20approach%20that%20integrates%203D%20point%20clouds%20and%20co-registered%20RGB%20images%20to%20estimate%20dense%203D%20displacement%20vector%20fields.%20Patch-level%20matches%20are%20constructed%20using%20both%203D%20geometry%20and%202D%20image%20features%2C%20refined%20via%20geometric%20consistency%20checks%2C%20and%20followed%20by%20rigid%20transformation%20estimation%20per%20match.%20Experimental%20results%20on%20two%20real-world%20landslide%20datasets%20demonstrate%20that%20the%20proposed%20method%20produces%203D%20displacement%20estimates%20with%20high%20spatial%20coverage%20%2879%25%20and%2097%25%29%20and%20accuracy.%20Deviations%20in%20displacement%20magnitude%20with%20respect%20to%20external%20measurements%20%28total%20station%20or%20GNSS%20observations%29%20are%200.15%20m%20and%200.25%20m%20on%20the%20two%20datasets%2C%20respectively%2C%20and%20only%200.07%20m%20and%200.20%20m%20compared%20to%20manually%20derived%20references%2C%20all%20below%20the%20mean%20scan%20resolutions%20%280.08%20m%20and%200.30%20m%29.%20Compared%20with%20the%20state-of-the-art%20method%20F2S3%2C%20the%20proposed%20approach%20improves%20spatial%20coverage%20while%20maintaining%20comparable%20accuracy.%20The%20proposed%20approach%20offers%20a%20practical%20and%20adaptable%20solution%20for%20TLS-based%20landslide%20monitoring%20and%20is%20extensible%20to%20other%20types%20of%20point%20clouds%20and%20monitoring%20tasks.%20The%20example%20data%20and%20source%20code%20are%20publicly%20available%20at%20https%3A//github.com/gseg-ethz/fusion4landslide.&entry.1838667208=http%3A//arxiv.org/abs/2506.16265v2&entry.124074799=Read"},
{"title": "A Novel Patch-Based TDA Approach for Computed Tomography", "author": "Dashti A. Ali and Aras T. Asaad and Jacob J. Peoples and Mohammad Hamghalam and Alex Robins and Mane Piliposyan and Richard K. G. Do and Natalie Gangai and Yun S. Chun and Ahmad Bashir Barekzai and Jayasree Chakraborty and Hala Khasawneh and Camila Vilela and Natally Horvat and Jo\u00e3o Miranda and Alice C. Wei and Amber L. Simpson", "abstract": "The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.", "link": "http://arxiv.org/abs/2512.12108v2", "date": "2026-01-09", "relevancy": 2.8247, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5688}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Patch-Based%20TDA%20Approach%20for%20Computed%20Tomography&body=Title%3A%20A%20Novel%20Patch-Based%20TDA%20Approach%20for%20Computed%20Tomography%0AAuthor%3A%20Dashti%20A.%20Ali%20and%20Aras%20T.%20Asaad%20and%20Jacob%20J.%20Peoples%20and%20Mohammad%20Hamghalam%20and%20Alex%20Robins%20and%20Mane%20Piliposyan%20and%20Richard%20K.%20G.%20Do%20and%20Natalie%20Gangai%20and%20Yun%20S.%20Chun%20and%20Ahmad%20Bashir%20Barekzai%20and%20Jayasree%20Chakraborty%20and%20Hala%20Khasawneh%20and%20Camila%20Vilela%20and%20Natally%20Horvat%20and%20Jo%C3%A3o%20Miranda%20and%20Alice%20C.%20Wei%20and%20Amber%20L.%20Simpson%0AAbstract%3A%20The%20development%20of%20machine%20learning%20%28ML%29%20models%20based%20on%20computed%20tomography%20%28CT%29%20imaging%20modality%20has%20been%20a%20major%20focus%20of%20recent%20research%20in%20the%20medical%20imaging%20domain.%20Incorporating%20robust%20feature%20engineering%20approach%20can%20highly%20improve%20the%20performance%20of%20these%20models.%20Topological%20data%20analysis%20%28TDA%29%2C%20a%20recent%20development%20based%20on%20the%20mathematical%20field%20of%20algebraic%20topology%2C%20mainly%20focuses%20on%20the%20data%20from%20a%20topological%20perspective%2C%20extracting%20deeper%20insight%20and%20higher%20dimensional%20structures%20from%20the%20data.%20Persistent%20homology%20%28PH%29%2C%20a%20fundamental%20tool%20in%20the%20area%20of%20TDA%2C%20can%20extract%20topological%20features%20such%20as%20connected%20components%2C%20cycles%20and%20voids%20from%20the%20data.%20A%20popular%20approach%20to%20construct%20PH%20from%203D%20CT%20images%20is%20to%20utilize%20the%203D%20cubical%20complex%20filtration%2C%20a%20method%20adapted%20for%20grid-structured%20data.%20However%2C%20this%20approach%20may%20not%20always%20yield%20the%20best%20performance%20and%20can%20suffer%20from%20computational%20complexity%20with%20higher%20resolution%20CT%20images.%20This%20study%20introduces%20a%20novel%20patch-based%20PH%20construction%20approach%20tailored%20for%20volumetric%20medical%20imaging%20data%2C%20in%20particular%20CT%20modality.%20A%20wide%20range%20of%20experiments%20has%20been%20conducted%20on%20several%20datasets%20of%203D%20CT%20images%20to%20comprehensively%20analyze%20the%20performance%20of%20the%20proposed%20method%20with%20various%20parameters%20and%20benchmark%20it%20against%20the%203D%20cubical%20complex%20algorithm.%20Our%20results%20highlight%20the%20dominance%20of%20the%20patch-based%20TDA%20approach%20in%20terms%20of%20both%20classification%20performance%20and%20time-efficiency.%20The%20proposed%20approach%20outperformed%20the%20cubical%20complex%20method%2C%20achieving%20average%20improvement%20of%2010.38%25%2C%206.94%25%2C%202.06%25%2C%2011.58%25%2C%20and%208.51%25%20in%20accuracy%2C%20AUC%2C%20sensitivity%2C%20specificity%2C%20and%20F1%20score%2C%20respectively%2C%20across%20all%20datasets.%20Finally%2C%20we%20provide%20a%20convenient%20python%20package%2C%20Patch-TDA%2C%20to%20facilitate%20the%20utilization%20of%20the%20proposed%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Patch-Based%2520TDA%2520Approach%2520for%2520Computed%2520Tomography%26entry.906535625%3DDashti%2520A.%2520Ali%2520and%2520Aras%2520T.%2520Asaad%2520and%2520Jacob%2520J.%2520Peoples%2520and%2520Mohammad%2520Hamghalam%2520and%2520Alex%2520Robins%2520and%2520Mane%2520Piliposyan%2520and%2520Richard%2520K.%2520G.%2520Do%2520and%2520Natalie%2520Gangai%2520and%2520Yun%2520S.%2520Chun%2520and%2520Ahmad%2520Bashir%2520Barekzai%2520and%2520Jayasree%2520Chakraborty%2520and%2520Hala%2520Khasawneh%2520and%2520Camila%2520Vilela%2520and%2520Natally%2520Horvat%2520and%2520Jo%25C3%25A3o%2520Miranda%2520and%2520Alice%2520C.%2520Wei%2520and%2520Amber%2520L.%2520Simpson%26entry.1292438233%3DThe%2520development%2520of%2520machine%2520learning%2520%2528ML%2529%2520models%2520based%2520on%2520computed%2520tomography%2520%2528CT%2529%2520imaging%2520modality%2520has%2520been%2520a%2520major%2520focus%2520of%2520recent%2520research%2520in%2520the%2520medical%2520imaging%2520domain.%2520Incorporating%2520robust%2520feature%2520engineering%2520approach%2520can%2520highly%2520improve%2520the%2520performance%2520of%2520these%2520models.%2520Topological%2520data%2520analysis%2520%2528TDA%2529%252C%2520a%2520recent%2520development%2520based%2520on%2520the%2520mathematical%2520field%2520of%2520algebraic%2520topology%252C%2520mainly%2520focuses%2520on%2520the%2520data%2520from%2520a%2520topological%2520perspective%252C%2520extracting%2520deeper%2520insight%2520and%2520higher%2520dimensional%2520structures%2520from%2520the%2520data.%2520Persistent%2520homology%2520%2528PH%2529%252C%2520a%2520fundamental%2520tool%2520in%2520the%2520area%2520of%2520TDA%252C%2520can%2520extract%2520topological%2520features%2520such%2520as%2520connected%2520components%252C%2520cycles%2520and%2520voids%2520from%2520the%2520data.%2520A%2520popular%2520approach%2520to%2520construct%2520PH%2520from%25203D%2520CT%2520images%2520is%2520to%2520utilize%2520the%25203D%2520cubical%2520complex%2520filtration%252C%2520a%2520method%2520adapted%2520for%2520grid-structured%2520data.%2520However%252C%2520this%2520approach%2520may%2520not%2520always%2520yield%2520the%2520best%2520performance%2520and%2520can%2520suffer%2520from%2520computational%2520complexity%2520with%2520higher%2520resolution%2520CT%2520images.%2520This%2520study%2520introduces%2520a%2520novel%2520patch-based%2520PH%2520construction%2520approach%2520tailored%2520for%2520volumetric%2520medical%2520imaging%2520data%252C%2520in%2520particular%2520CT%2520modality.%2520A%2520wide%2520range%2520of%2520experiments%2520has%2520been%2520conducted%2520on%2520several%2520datasets%2520of%25203D%2520CT%2520images%2520to%2520comprehensively%2520analyze%2520the%2520performance%2520of%2520the%2520proposed%2520method%2520with%2520various%2520parameters%2520and%2520benchmark%2520it%2520against%2520the%25203D%2520cubical%2520complex%2520algorithm.%2520Our%2520results%2520highlight%2520the%2520dominance%2520of%2520the%2520patch-based%2520TDA%2520approach%2520in%2520terms%2520of%2520both%2520classification%2520performance%2520and%2520time-efficiency.%2520The%2520proposed%2520approach%2520outperformed%2520the%2520cubical%2520complex%2520method%252C%2520achieving%2520average%2520improvement%2520of%252010.38%2525%252C%25206.94%2525%252C%25202.06%2525%252C%252011.58%2525%252C%2520and%25208.51%2525%2520in%2520accuracy%252C%2520AUC%252C%2520sensitivity%252C%2520specificity%252C%2520and%2520F1%2520score%252C%2520respectively%252C%2520across%2520all%2520datasets.%2520Finally%252C%2520we%2520provide%2520a%2520convenient%2520python%2520package%252C%2520Patch-TDA%252C%2520to%2520facilitate%2520the%2520utilization%2520of%2520the%2520proposed%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Patch-Based%20TDA%20Approach%20for%20Computed%20Tomography&entry.906535625=Dashti%20A.%20Ali%20and%20Aras%20T.%20Asaad%20and%20Jacob%20J.%20Peoples%20and%20Mohammad%20Hamghalam%20and%20Alex%20Robins%20and%20Mane%20Piliposyan%20and%20Richard%20K.%20G.%20Do%20and%20Natalie%20Gangai%20and%20Yun%20S.%20Chun%20and%20Ahmad%20Bashir%20Barekzai%20and%20Jayasree%20Chakraborty%20and%20Hala%20Khasawneh%20and%20Camila%20Vilela%20and%20Natally%20Horvat%20and%20Jo%C3%A3o%20Miranda%20and%20Alice%20C.%20Wei%20and%20Amber%20L.%20Simpson&entry.1292438233=The%20development%20of%20machine%20learning%20%28ML%29%20models%20based%20on%20computed%20tomography%20%28CT%29%20imaging%20modality%20has%20been%20a%20major%20focus%20of%20recent%20research%20in%20the%20medical%20imaging%20domain.%20Incorporating%20robust%20feature%20engineering%20approach%20can%20highly%20improve%20the%20performance%20of%20these%20models.%20Topological%20data%20analysis%20%28TDA%29%2C%20a%20recent%20development%20based%20on%20the%20mathematical%20field%20of%20algebraic%20topology%2C%20mainly%20focuses%20on%20the%20data%20from%20a%20topological%20perspective%2C%20extracting%20deeper%20insight%20and%20higher%20dimensional%20structures%20from%20the%20data.%20Persistent%20homology%20%28PH%29%2C%20a%20fundamental%20tool%20in%20the%20area%20of%20TDA%2C%20can%20extract%20topological%20features%20such%20as%20connected%20components%2C%20cycles%20and%20voids%20from%20the%20data.%20A%20popular%20approach%20to%20construct%20PH%20from%203D%20CT%20images%20is%20to%20utilize%20the%203D%20cubical%20complex%20filtration%2C%20a%20method%20adapted%20for%20grid-structured%20data.%20However%2C%20this%20approach%20may%20not%20always%20yield%20the%20best%20performance%20and%20can%20suffer%20from%20computational%20complexity%20with%20higher%20resolution%20CT%20images.%20This%20study%20introduces%20a%20novel%20patch-based%20PH%20construction%20approach%20tailored%20for%20volumetric%20medical%20imaging%20data%2C%20in%20particular%20CT%20modality.%20A%20wide%20range%20of%20experiments%20has%20been%20conducted%20on%20several%20datasets%20of%203D%20CT%20images%20to%20comprehensively%20analyze%20the%20performance%20of%20the%20proposed%20method%20with%20various%20parameters%20and%20benchmark%20it%20against%20the%203D%20cubical%20complex%20algorithm.%20Our%20results%20highlight%20the%20dominance%20of%20the%20patch-based%20TDA%20approach%20in%20terms%20of%20both%20classification%20performance%20and%20time-efficiency.%20The%20proposed%20approach%20outperformed%20the%20cubical%20complex%20method%2C%20achieving%20average%20improvement%20of%2010.38%25%2C%206.94%25%2C%202.06%25%2C%2011.58%25%2C%20and%208.51%25%20in%20accuracy%2C%20AUC%2C%20sensitivity%2C%20specificity%2C%20and%20F1%20score%2C%20respectively%2C%20across%20all%20datasets.%20Finally%2C%20we%20provide%20a%20convenient%20python%20package%2C%20Patch-TDA%2C%20to%20facilitate%20the%20utilization%20of%20the%20proposed%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2512.12108v2&entry.124074799=Read"},
{"title": "DYRECT Computed Tomography: DYnamic Reconstruction of Events on a Continuous Timescale", "author": "Wannes Goethals and Tom Bultreys and Steffen Berg and Matthieu N. Boone and Jan Aelterman", "abstract": "Time-resolved high-resolution X-ray Computed Tomography (4D $\u03bc$CT) is an imaging technique that offers insight into the evolution of dynamic processes inside materials that are opaque to visible light. Conventional tomographic reconstruction techniques are based on recording a sequence of 3D images that represent the sample state at different moments in time. This frame-based approach limits the temporal resolution compared to dynamic radiography experiments due to the time needed to make CT scans. Moreover, it leads to an inflation of the amount of data and thus to costly post-processing computations to quantify the dynamic behaviour from the sequence of time frames, hereby often ignoring the temporal correlations of the sample structure. Our proposed 4D $\u03bc$CT reconstruction technique, named DYRECT, estimates individual attenuation evolution profiles for each position in the sample. This leads to a novel memory-efficient event-based representation of the sample, using as little as three image volumes: its initial attenuation, its final attenuation and the transition times. This third volume represents local events on a continuous timescale instead of the discrete global time frames. We propose a method to iteratively reconstruct the transition times and the attenuation volumes. The dynamic reconstruction technique was validated on synthetic ground truth data and experimental data, and was found to effectively pinpoint the transition times in the synthetic dataset with a time resolution corresponding to less than a tenth of the amount of projections required to reconstruct traditional $\u03bc$CT time frames.", "link": "http://arxiv.org/abs/2412.00065v2", "date": "2026-01-09", "relevancy": 2.8181, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5917}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5917}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DYRECT%20Computed%20Tomography%3A%20DYnamic%20Reconstruction%20of%20Events%20on%20a%20Continuous%20Timescale&body=Title%3A%20DYRECT%20Computed%20Tomography%3A%20DYnamic%20Reconstruction%20of%20Events%20on%20a%20Continuous%20Timescale%0AAuthor%3A%20Wannes%20Goethals%20and%20Tom%20Bultreys%20and%20Steffen%20Berg%20and%20Matthieu%20N.%20Boone%20and%20Jan%20Aelterman%0AAbstract%3A%20Time-resolved%20high-resolution%20X-ray%20Computed%20Tomography%20%284D%20%24%CE%BC%24CT%29%20is%20an%20imaging%20technique%20that%20offers%20insight%20into%20the%20evolution%20of%20dynamic%20processes%20inside%20materials%20that%20are%20opaque%20to%20visible%20light.%20Conventional%20tomographic%20reconstruction%20techniques%20are%20based%20on%20recording%20a%20sequence%20of%203D%20images%20that%20represent%20the%20sample%20state%20at%20different%20moments%20in%20time.%20This%20frame-based%20approach%20limits%20the%20temporal%20resolution%20compared%20to%20dynamic%20radiography%20experiments%20due%20to%20the%20time%20needed%20to%20make%20CT%20scans.%20Moreover%2C%20it%20leads%20to%20an%20inflation%20of%20the%20amount%20of%20data%20and%20thus%20to%20costly%20post-processing%20computations%20to%20quantify%20the%20dynamic%20behaviour%20from%20the%20sequence%20of%20time%20frames%2C%20hereby%20often%20ignoring%20the%20temporal%20correlations%20of%20the%20sample%20structure.%20Our%20proposed%204D%20%24%CE%BC%24CT%20reconstruction%20technique%2C%20named%20DYRECT%2C%20estimates%20individual%20attenuation%20evolution%20profiles%20for%20each%20position%20in%20the%20sample.%20This%20leads%20to%20a%20novel%20memory-efficient%20event-based%20representation%20of%20the%20sample%2C%20using%20as%20little%20as%20three%20image%20volumes%3A%20its%20initial%20attenuation%2C%20its%20final%20attenuation%20and%20the%20transition%20times.%20This%20third%20volume%20represents%20local%20events%20on%20a%20continuous%20timescale%20instead%20of%20the%20discrete%20global%20time%20frames.%20We%20propose%20a%20method%20to%20iteratively%20reconstruct%20the%20transition%20times%20and%20the%20attenuation%20volumes.%20The%20dynamic%20reconstruction%20technique%20was%20validated%20on%20synthetic%20ground%20truth%20data%20and%20experimental%20data%2C%20and%20was%20found%20to%20effectively%20pinpoint%20the%20transition%20times%20in%20the%20synthetic%20dataset%20with%20a%20time%20resolution%20corresponding%20to%20less%20than%20a%20tenth%20of%20the%20amount%20of%20projections%20required%20to%20reconstruct%20traditional%20%24%CE%BC%24CT%20time%20frames.%0ALink%3A%20http%3A//arxiv.org/abs/2412.00065v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDYRECT%2520Computed%2520Tomography%253A%2520DYnamic%2520Reconstruction%2520of%2520Events%2520on%2520a%2520Continuous%2520Timescale%26entry.906535625%3DWannes%2520Goethals%2520and%2520Tom%2520Bultreys%2520and%2520Steffen%2520Berg%2520and%2520Matthieu%2520N.%2520Boone%2520and%2520Jan%2520Aelterman%26entry.1292438233%3DTime-resolved%2520high-resolution%2520X-ray%2520Computed%2520Tomography%2520%25284D%2520%2524%25CE%25BC%2524CT%2529%2520is%2520an%2520imaging%2520technique%2520that%2520offers%2520insight%2520into%2520the%2520evolution%2520of%2520dynamic%2520processes%2520inside%2520materials%2520that%2520are%2520opaque%2520to%2520visible%2520light.%2520Conventional%2520tomographic%2520reconstruction%2520techniques%2520are%2520based%2520on%2520recording%2520a%2520sequence%2520of%25203D%2520images%2520that%2520represent%2520the%2520sample%2520state%2520at%2520different%2520moments%2520in%2520time.%2520This%2520frame-based%2520approach%2520limits%2520the%2520temporal%2520resolution%2520compared%2520to%2520dynamic%2520radiography%2520experiments%2520due%2520to%2520the%2520time%2520needed%2520to%2520make%2520CT%2520scans.%2520Moreover%252C%2520it%2520leads%2520to%2520an%2520inflation%2520of%2520the%2520amount%2520of%2520data%2520and%2520thus%2520to%2520costly%2520post-processing%2520computations%2520to%2520quantify%2520the%2520dynamic%2520behaviour%2520from%2520the%2520sequence%2520of%2520time%2520frames%252C%2520hereby%2520often%2520ignoring%2520the%2520temporal%2520correlations%2520of%2520the%2520sample%2520structure.%2520Our%2520proposed%25204D%2520%2524%25CE%25BC%2524CT%2520reconstruction%2520technique%252C%2520named%2520DYRECT%252C%2520estimates%2520individual%2520attenuation%2520evolution%2520profiles%2520for%2520each%2520position%2520in%2520the%2520sample.%2520This%2520leads%2520to%2520a%2520novel%2520memory-efficient%2520event-based%2520representation%2520of%2520the%2520sample%252C%2520using%2520as%2520little%2520as%2520three%2520image%2520volumes%253A%2520its%2520initial%2520attenuation%252C%2520its%2520final%2520attenuation%2520and%2520the%2520transition%2520times.%2520This%2520third%2520volume%2520represents%2520local%2520events%2520on%2520a%2520continuous%2520timescale%2520instead%2520of%2520the%2520discrete%2520global%2520time%2520frames.%2520We%2520propose%2520a%2520method%2520to%2520iteratively%2520reconstruct%2520the%2520transition%2520times%2520and%2520the%2520attenuation%2520volumes.%2520The%2520dynamic%2520reconstruction%2520technique%2520was%2520validated%2520on%2520synthetic%2520ground%2520truth%2520data%2520and%2520experimental%2520data%252C%2520and%2520was%2520found%2520to%2520effectively%2520pinpoint%2520the%2520transition%2520times%2520in%2520the%2520synthetic%2520dataset%2520with%2520a%2520time%2520resolution%2520corresponding%2520to%2520less%2520than%2520a%2520tenth%2520of%2520the%2520amount%2520of%2520projections%2520required%2520to%2520reconstruct%2520traditional%2520%2524%25CE%25BC%2524CT%2520time%2520frames.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00065v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DYRECT%20Computed%20Tomography%3A%20DYnamic%20Reconstruction%20of%20Events%20on%20a%20Continuous%20Timescale&entry.906535625=Wannes%20Goethals%20and%20Tom%20Bultreys%20and%20Steffen%20Berg%20and%20Matthieu%20N.%20Boone%20and%20Jan%20Aelterman&entry.1292438233=Time-resolved%20high-resolution%20X-ray%20Computed%20Tomography%20%284D%20%24%CE%BC%24CT%29%20is%20an%20imaging%20technique%20that%20offers%20insight%20into%20the%20evolution%20of%20dynamic%20processes%20inside%20materials%20that%20are%20opaque%20to%20visible%20light.%20Conventional%20tomographic%20reconstruction%20techniques%20are%20based%20on%20recording%20a%20sequence%20of%203D%20images%20that%20represent%20the%20sample%20state%20at%20different%20moments%20in%20time.%20This%20frame-based%20approach%20limits%20the%20temporal%20resolution%20compared%20to%20dynamic%20radiography%20experiments%20due%20to%20the%20time%20needed%20to%20make%20CT%20scans.%20Moreover%2C%20it%20leads%20to%20an%20inflation%20of%20the%20amount%20of%20data%20and%20thus%20to%20costly%20post-processing%20computations%20to%20quantify%20the%20dynamic%20behaviour%20from%20the%20sequence%20of%20time%20frames%2C%20hereby%20often%20ignoring%20the%20temporal%20correlations%20of%20the%20sample%20structure.%20Our%20proposed%204D%20%24%CE%BC%24CT%20reconstruction%20technique%2C%20named%20DYRECT%2C%20estimates%20individual%20attenuation%20evolution%20profiles%20for%20each%20position%20in%20the%20sample.%20This%20leads%20to%20a%20novel%20memory-efficient%20event-based%20representation%20of%20the%20sample%2C%20using%20as%20little%20as%20three%20image%20volumes%3A%20its%20initial%20attenuation%2C%20its%20final%20attenuation%20and%20the%20transition%20times.%20This%20third%20volume%20represents%20local%20events%20on%20a%20continuous%20timescale%20instead%20of%20the%20discrete%20global%20time%20frames.%20We%20propose%20a%20method%20to%20iteratively%20reconstruct%20the%20transition%20times%20and%20the%20attenuation%20volumes.%20The%20dynamic%20reconstruction%20technique%20was%20validated%20on%20synthetic%20ground%20truth%20data%20and%20experimental%20data%2C%20and%20was%20found%20to%20effectively%20pinpoint%20the%20transition%20times%20in%20the%20synthetic%20dataset%20with%20a%20time%20resolution%20corresponding%20to%20less%20than%20a%20tenth%20of%20the%20amount%20of%20projections%20required%20to%20reconstruct%20traditional%20%24%CE%BC%24CT%20time%20frames.&entry.1838667208=http%3A//arxiv.org/abs/2412.00065v2&entry.124074799=Read"},
{"title": "Context-Aware Decoding for Faithful Vision-Language Generation", "author": "Mehrdad Fazli and Bowen Wei and Ziwei Zhu", "abstract": "Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.", "link": "http://arxiv.org/abs/2601.05939v1", "date": "2026-01-09", "relevancy": 2.8032, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Decoding%20for%20Faithful%20Vision-Language%20Generation&body=Title%3A%20Context-Aware%20Decoding%20for%20Faithful%20Vision-Language%20Generation%0AAuthor%3A%20Mehrdad%20Fazli%20and%20Bowen%20Wei%20and%20Ziwei%20Zhu%0AAbstract%3A%20Hallucinations%2C%20generating%20responses%20inconsistent%20with%20the%20visual%20input%2C%20remain%20a%20critical%20limitation%20of%20large%20vision-language%20models%20%28LVLMs%29%2C%20especially%20in%20open-ended%20tasks%20such%20as%20image%20captioning%20and%20visual%20reasoning.%20In%20this%20work%2C%20we%20probe%20the%20layer-wise%20generation%20dynamics%20that%20drive%20hallucinations%20and%20propose%20a%20training-free%20mitigation%20strategy.%20Employing%20the%20Logit%20Lens%2C%20we%20examine%20how%20LVLMs%20construct%20next-token%20distributions%20across%20decoder%20layers%2C%20uncovering%20a%20pronounced%20commitment-depth%20gap%3A%20truthful%20tokens%20accumulate%20probability%20mass%20on%20their%20final%20candidates%20earlier%20than%20hallucinatory%20ones.%20Drawing%20on%20this%20discovery%2C%20we%20introduce%20Context%20Embedding%20Injection%20%28CEI%29%2C%20a%20lightweight%20method%20that%20harnesses%20the%20hidden%20state%20of%20the%20last%20input%20token-the%20context%20embedding-as%20a%20grounding%20signal%20to%20maintain%20visual%20fidelity%20throughout%20decoding%20and%20curb%20hallucinations.%20Evaluated%20on%20the%20CHAIR%2C%20AMBER%2C%20and%20MMHal-Bench%20benchmarks%20%28with%20a%20maximum%20token%20length%20of%20512%29%2C%20CEI%20outperforms%20state-of-the-art%20baselines%20across%20three%20LVLMs%2C%20with%20its%20dynamic%20variant%20yielding%20the%20lowest%20overall%20hallucination%20rates.%20By%20integrating%20novel%20mechanistic%20insights%20with%20a%20scalable%20intervention%2C%20this%20work%20advances%20the%20mitigation%20of%20hallucinations%20in%20LVLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Decoding%2520for%2520Faithful%2520Vision-Language%2520Generation%26entry.906535625%3DMehrdad%2520Fazli%2520and%2520Bowen%2520Wei%2520and%2520Ziwei%2520Zhu%26entry.1292438233%3DHallucinations%252C%2520generating%2520responses%2520inconsistent%2520with%2520the%2520visual%2520input%252C%2520remain%2520a%2520critical%2520limitation%2520of%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%252C%2520especially%2520in%2520open-ended%2520tasks%2520such%2520as%2520image%2520captioning%2520and%2520visual%2520reasoning.%2520In%2520this%2520work%252C%2520we%2520probe%2520the%2520layer-wise%2520generation%2520dynamics%2520that%2520drive%2520hallucinations%2520and%2520propose%2520a%2520training-free%2520mitigation%2520strategy.%2520Employing%2520the%2520Logit%2520Lens%252C%2520we%2520examine%2520how%2520LVLMs%2520construct%2520next-token%2520distributions%2520across%2520decoder%2520layers%252C%2520uncovering%2520a%2520pronounced%2520commitment-depth%2520gap%253A%2520truthful%2520tokens%2520accumulate%2520probability%2520mass%2520on%2520their%2520final%2520candidates%2520earlier%2520than%2520hallucinatory%2520ones.%2520Drawing%2520on%2520this%2520discovery%252C%2520we%2520introduce%2520Context%2520Embedding%2520Injection%2520%2528CEI%2529%252C%2520a%2520lightweight%2520method%2520that%2520harnesses%2520the%2520hidden%2520state%2520of%2520the%2520last%2520input%2520token-the%2520context%2520embedding-as%2520a%2520grounding%2520signal%2520to%2520maintain%2520visual%2520fidelity%2520throughout%2520decoding%2520and%2520curb%2520hallucinations.%2520Evaluated%2520on%2520the%2520CHAIR%252C%2520AMBER%252C%2520and%2520MMHal-Bench%2520benchmarks%2520%2528with%2520a%2520maximum%2520token%2520length%2520of%2520512%2529%252C%2520CEI%2520outperforms%2520state-of-the-art%2520baselines%2520across%2520three%2520LVLMs%252C%2520with%2520its%2520dynamic%2520variant%2520yielding%2520the%2520lowest%2520overall%2520hallucination%2520rates.%2520By%2520integrating%2520novel%2520mechanistic%2520insights%2520with%2520a%2520scalable%2520intervention%252C%2520this%2520work%2520advances%2520the%2520mitigation%2520of%2520hallucinations%2520in%2520LVLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Decoding%20for%20Faithful%20Vision-Language%20Generation&entry.906535625=Mehrdad%20Fazli%20and%20Bowen%20Wei%20and%20Ziwei%20Zhu&entry.1292438233=Hallucinations%2C%20generating%20responses%20inconsistent%20with%20the%20visual%20input%2C%20remain%20a%20critical%20limitation%20of%20large%20vision-language%20models%20%28LVLMs%29%2C%20especially%20in%20open-ended%20tasks%20such%20as%20image%20captioning%20and%20visual%20reasoning.%20In%20this%20work%2C%20we%20probe%20the%20layer-wise%20generation%20dynamics%20that%20drive%20hallucinations%20and%20propose%20a%20training-free%20mitigation%20strategy.%20Employing%20the%20Logit%20Lens%2C%20we%20examine%20how%20LVLMs%20construct%20next-token%20distributions%20across%20decoder%20layers%2C%20uncovering%20a%20pronounced%20commitment-depth%20gap%3A%20truthful%20tokens%20accumulate%20probability%20mass%20on%20their%20final%20candidates%20earlier%20than%20hallucinatory%20ones.%20Drawing%20on%20this%20discovery%2C%20we%20introduce%20Context%20Embedding%20Injection%20%28CEI%29%2C%20a%20lightweight%20method%20that%20harnesses%20the%20hidden%20state%20of%20the%20last%20input%20token-the%20context%20embedding-as%20a%20grounding%20signal%20to%20maintain%20visual%20fidelity%20throughout%20decoding%20and%20curb%20hallucinations.%20Evaluated%20on%20the%20CHAIR%2C%20AMBER%2C%20and%20MMHal-Bench%20benchmarks%20%28with%20a%20maximum%20token%20length%20of%20512%29%2C%20CEI%20outperforms%20state-of-the-art%20baselines%20across%20three%20LVLMs%2C%20with%20its%20dynamic%20variant%20yielding%20the%20lowest%20overall%20hallucination%20rates.%20By%20integrating%20novel%20mechanistic%20insights%20with%20a%20scalable%20intervention%2C%20this%20work%20advances%20the%20mitigation%20of%20hallucinations%20in%20LVLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.05939v1&entry.124074799=Read"},
{"title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views", "author": "Hassaan Farooq and Marvin Brenner and Peter St\\\u00fctz", "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.", "link": "http://arxiv.org/abs/2601.05747v1", "date": "2026-01-09", "relevancy": 2.7861, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5623}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlyPose%3A%20Towards%20Robust%20Human%20Pose%20Estimation%20From%20Aerial%20Views&body=Title%3A%20FlyPose%3A%20Towards%20Robust%20Human%20Pose%20Estimation%20From%20Aerial%20Views%0AAuthor%3A%20Hassaan%20Farooq%20and%20Marvin%20Brenner%20and%20Peter%20St%5C%C3%BCtz%0AAbstract%3A%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20deployed%20in%20close%20proximity%20to%20humans%20for%20applications%20such%20as%20parcel%20delivery%2C%20traffic%20monitoring%2C%20disaster%20response%20and%20infrastructure%20inspections.%20Ensuring%20safe%20and%20reliable%20operation%20in%20these%20human-populated%20environments%20demands%20accurate%20perception%20of%20human%20poses%20and%20actions%20from%20an%20aerial%20viewpoint.%20This%20perspective%20challenges%20existing%20methods%20with%20low%20resolution%2C%20steep%20viewing%20angles%20and%20%28self-%29occlusion%2C%20especially%20if%20the%20application%20demands%20realtime%20feasibile%20models.%20We%20train%20and%20deploy%20FlyPose%2C%20a%20lightweight%20top-down%20human%20pose%20estimation%20pipeline%20for%20aerial%20imagery.%20Through%20multi-dataset%20training%2C%20we%20achieve%20an%20average%20improvement%20of%206.8%20mAP%20in%20person%20detection%20across%20the%20test-sets%20of%20Manipal-UAV%2C%20VisDrone%2C%20HIT-UAV%20as%20well%20as%20our%20custom%20dataset.%20For%202D%20human%20pose%20estimation%20we%20report%20an%20improvement%20of%2016.3%20mAP%20on%20the%20challenging%20UAV-Human%20dataset.%20FlyPose%20runs%20with%20an%20inference%20latency%20of%20~20%20milliseconds%20including%20preprocessing%20on%20a%20Jetson%20Orin%20AGX%20Developer%20Kit%20and%20is%20deployed%20onboard%20a%20quadrotor%20UAV%20during%20flight%20experiments.%20We%20also%20publish%20FlyPose-104%2C%20a%20small%20but%20challenging%20aerial%20human%20pose%20estimation%20dataset%2C%20that%20includes%20manual%20annotations%20from%20difficult%20aerial%20perspectives%3A%20https%3A//github.com/farooqhassaan/FlyPose.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlyPose%253A%2520Towards%2520Robust%2520Human%2520Pose%2520Estimation%2520From%2520Aerial%2520Views%26entry.906535625%3DHassaan%2520Farooq%2520and%2520Marvin%2520Brenner%2520and%2520Peter%2520St%255C%25C3%25BCtz%26entry.1292438233%3DUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520increasingly%2520deployed%2520in%2520close%2520proximity%2520to%2520humans%2520for%2520applications%2520such%2520as%2520parcel%2520delivery%252C%2520traffic%2520monitoring%252C%2520disaster%2520response%2520and%2520infrastructure%2520inspections.%2520Ensuring%2520safe%2520and%2520reliable%2520operation%2520in%2520these%2520human-populated%2520environments%2520demands%2520accurate%2520perception%2520of%2520human%2520poses%2520and%2520actions%2520from%2520an%2520aerial%2520viewpoint.%2520This%2520perspective%2520challenges%2520existing%2520methods%2520with%2520low%2520resolution%252C%2520steep%2520viewing%2520angles%2520and%2520%2528self-%2529occlusion%252C%2520especially%2520if%2520the%2520application%2520demands%2520realtime%2520feasibile%2520models.%2520We%2520train%2520and%2520deploy%2520FlyPose%252C%2520a%2520lightweight%2520top-down%2520human%2520pose%2520estimation%2520pipeline%2520for%2520aerial%2520imagery.%2520Through%2520multi-dataset%2520training%252C%2520we%2520achieve%2520an%2520average%2520improvement%2520of%25206.8%2520mAP%2520in%2520person%2520detection%2520across%2520the%2520test-sets%2520of%2520Manipal-UAV%252C%2520VisDrone%252C%2520HIT-UAV%2520as%2520well%2520as%2520our%2520custom%2520dataset.%2520For%25202D%2520human%2520pose%2520estimation%2520we%2520report%2520an%2520improvement%2520of%252016.3%2520mAP%2520on%2520the%2520challenging%2520UAV-Human%2520dataset.%2520FlyPose%2520runs%2520with%2520an%2520inference%2520latency%2520of%2520~20%2520milliseconds%2520including%2520preprocessing%2520on%2520a%2520Jetson%2520Orin%2520AGX%2520Developer%2520Kit%2520and%2520is%2520deployed%2520onboard%2520a%2520quadrotor%2520UAV%2520during%2520flight%2520experiments.%2520We%2520also%2520publish%2520FlyPose-104%252C%2520a%2520small%2520but%2520challenging%2520aerial%2520human%2520pose%2520estimation%2520dataset%252C%2520that%2520includes%2520manual%2520annotations%2520from%2520difficult%2520aerial%2520perspectives%253A%2520https%253A//github.com/farooqhassaan/FlyPose.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlyPose%3A%20Towards%20Robust%20Human%20Pose%20Estimation%20From%20Aerial%20Views&entry.906535625=Hassaan%20Farooq%20and%20Marvin%20Brenner%20and%20Peter%20St%5C%C3%BCtz&entry.1292438233=Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20deployed%20in%20close%20proximity%20to%20humans%20for%20applications%20such%20as%20parcel%20delivery%2C%20traffic%20monitoring%2C%20disaster%20response%20and%20infrastructure%20inspections.%20Ensuring%20safe%20and%20reliable%20operation%20in%20these%20human-populated%20environments%20demands%20accurate%20perception%20of%20human%20poses%20and%20actions%20from%20an%20aerial%20viewpoint.%20This%20perspective%20challenges%20existing%20methods%20with%20low%20resolution%2C%20steep%20viewing%20angles%20and%20%28self-%29occlusion%2C%20especially%20if%20the%20application%20demands%20realtime%20feasibile%20models.%20We%20train%20and%20deploy%20FlyPose%2C%20a%20lightweight%20top-down%20human%20pose%20estimation%20pipeline%20for%20aerial%20imagery.%20Through%20multi-dataset%20training%2C%20we%20achieve%20an%20average%20improvement%20of%206.8%20mAP%20in%20person%20detection%20across%20the%20test-sets%20of%20Manipal-UAV%2C%20VisDrone%2C%20HIT-UAV%20as%20well%20as%20our%20custom%20dataset.%20For%202D%20human%20pose%20estimation%20we%20report%20an%20improvement%20of%2016.3%20mAP%20on%20the%20challenging%20UAV-Human%20dataset.%20FlyPose%20runs%20with%20an%20inference%20latency%20of%20~20%20milliseconds%20including%20preprocessing%20on%20a%20Jetson%20Orin%20AGX%20Developer%20Kit%20and%20is%20deployed%20onboard%20a%20quadrotor%20UAV%20during%20flight%20experiments.%20We%20also%20publish%20FlyPose-104%2C%20a%20small%20but%20challenging%20aerial%20human%20pose%20estimation%20dataset%2C%20that%20includes%20manual%20annotations%20from%20difficult%20aerial%20perspectives%3A%20https%3A//github.com/farooqhassaan/FlyPose.&entry.1838667208=http%3A//arxiv.org/abs/2601.05747v1&entry.124074799=Read"},
{"title": "Visual Attention Reasoning via Hierarchical Search and Self-Verification", "author": "Wei Cai and Jian Zhao and Yuchen Yuan and Tianle Zhang and Ming Zhu and Haichuan Tang and Xuelong Li", "abstract": "Multimodal Large Language Models (MLLMs) frequently hallucinate due to their reliance on fragile, linear reasoning and weak visual grounding. We propose Visual Attention Reasoning (VAR), a reinforcement learning framework that reformulates reasoning as a hierarchical search with self-verification. VAR enforces traceable evidence grounding by generating explicit bounding boxes, guided by a novel reward function combining geometric precision and semantic sufficiency. Furthermore, it replaces linear Chain-of-Thought with a tree-search policy capable of backtracking to correct logical errors. Theoretical analysis validates the framework's reliability, and extensive experiments demonstrate that VAR significantly outperforms state-of-the-art methods on complex hallucination and safety benchmarks.", "link": "http://arxiv.org/abs/2510.18619v2", "date": "2026-01-09", "relevancy": 2.7729, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Attention%20Reasoning%20via%20Hierarchical%20Search%20and%20Self-Verification&body=Title%3A%20Visual%20Attention%20Reasoning%20via%20Hierarchical%20Search%20and%20Self-Verification%0AAuthor%3A%20Wei%20Cai%20and%20Jian%20Zhao%20and%20Yuchen%20Yuan%20and%20Tianle%20Zhang%20and%20Ming%20Zhu%20and%20Haichuan%20Tang%20and%20Xuelong%20Li%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20frequently%20hallucinate%20due%20to%20their%20reliance%20on%20fragile%2C%20linear%20reasoning%20and%20weak%20visual%20grounding.%20We%20propose%20Visual%20Attention%20Reasoning%20%28VAR%29%2C%20a%20reinforcement%20learning%20framework%20that%20reformulates%20reasoning%20as%20a%20hierarchical%20search%20with%20self-verification.%20VAR%20enforces%20traceable%20evidence%20grounding%20by%20generating%20explicit%20bounding%20boxes%2C%20guided%20by%20a%20novel%20reward%20function%20combining%20geometric%20precision%20and%20semantic%20sufficiency.%20Furthermore%2C%20it%20replaces%20linear%20Chain-of-Thought%20with%20a%20tree-search%20policy%20capable%20of%20backtracking%20to%20correct%20logical%20errors.%20Theoretical%20analysis%20validates%20the%20framework%27s%20reliability%2C%20and%20extensive%20experiments%20demonstrate%20that%20VAR%20significantly%20outperforms%20state-of-the-art%20methods%20on%20complex%20hallucination%20and%20safety%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Attention%2520Reasoning%2520via%2520Hierarchical%2520Search%2520and%2520Self-Verification%26entry.906535625%3DWei%2520Cai%2520and%2520Jian%2520Zhao%2520and%2520Yuchen%2520Yuan%2520and%2520Tianle%2520Zhang%2520and%2520Ming%2520Zhu%2520and%2520Haichuan%2520Tang%2520and%2520Xuelong%2520Li%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520frequently%2520hallucinate%2520due%2520to%2520their%2520reliance%2520on%2520fragile%252C%2520linear%2520reasoning%2520and%2520weak%2520visual%2520grounding.%2520We%2520propose%2520Visual%2520Attention%2520Reasoning%2520%2528VAR%2529%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%2520reformulates%2520reasoning%2520as%2520a%2520hierarchical%2520search%2520with%2520self-verification.%2520VAR%2520enforces%2520traceable%2520evidence%2520grounding%2520by%2520generating%2520explicit%2520bounding%2520boxes%252C%2520guided%2520by%2520a%2520novel%2520reward%2520function%2520combining%2520geometric%2520precision%2520and%2520semantic%2520sufficiency.%2520Furthermore%252C%2520it%2520replaces%2520linear%2520Chain-of-Thought%2520with%2520a%2520tree-search%2520policy%2520capable%2520of%2520backtracking%2520to%2520correct%2520logical%2520errors.%2520Theoretical%2520analysis%2520validates%2520the%2520framework%2527s%2520reliability%252C%2520and%2520extensive%2520experiments%2520demonstrate%2520that%2520VAR%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520on%2520complex%2520hallucination%2520and%2520safety%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Attention%20Reasoning%20via%20Hierarchical%20Search%20and%20Self-Verification&entry.906535625=Wei%20Cai%20and%20Jian%20Zhao%20and%20Yuchen%20Yuan%20and%20Tianle%20Zhang%20and%20Ming%20Zhu%20and%20Haichuan%20Tang%20and%20Xuelong%20Li&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20frequently%20hallucinate%20due%20to%20their%20reliance%20on%20fragile%2C%20linear%20reasoning%20and%20weak%20visual%20grounding.%20We%20propose%20Visual%20Attention%20Reasoning%20%28VAR%29%2C%20a%20reinforcement%20learning%20framework%20that%20reformulates%20reasoning%20as%20a%20hierarchical%20search%20with%20self-verification.%20VAR%20enforces%20traceable%20evidence%20grounding%20by%20generating%20explicit%20bounding%20boxes%2C%20guided%20by%20a%20novel%20reward%20function%20combining%20geometric%20precision%20and%20semantic%20sufficiency.%20Furthermore%2C%20it%20replaces%20linear%20Chain-of-Thought%20with%20a%20tree-search%20policy%20capable%20of%20backtracking%20to%20correct%20logical%20errors.%20Theoretical%20analysis%20validates%20the%20framework%27s%20reliability%2C%20and%20extensive%20experiments%20demonstrate%20that%20VAR%20significantly%20outperforms%20state-of-the-art%20methods%20on%20complex%20hallucination%20and%20safety%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2510.18619v2&entry.124074799=Read"},
{"title": "InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection", "author": "Simon Archieri and Ahmet Cinar and Shu Pan and Jonatan Scharff Willners and Michele Grimald and Ignacio Carlucho and Yvan Petillot", "abstract": "This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS). Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity. We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation. We evaluated InsSo3D performance inside a test tank with access to ground truth data and in an outdoor flooded quarry. Comparisons to reference trajectories and maps obtained from an underwater motion tracking system and visual Structure From Motion (SFM) demonstrate that InsSo3D efficiently corrects odometry drift. The average trajectory error is below 21cm during a 50-minute-long mission, producing a map of 10m by 20m with a 9cm average reconstruction error, enabling safe inspection of natural or artificial underwater structures even in murky water conditions.", "link": "http://arxiv.org/abs/2601.05805v1", "date": "2026-01-09", "relevancy": 2.7226, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.57}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsSo3D%3A%20Inertial%20Navigation%20System%20and%203D%20Sonar%20SLAM%20for%20turbid%20environment%20inspection&body=Title%3A%20InsSo3D%3A%20Inertial%20Navigation%20System%20and%203D%20Sonar%20SLAM%20for%20turbid%20environment%20inspection%0AAuthor%3A%20Simon%20Archieri%20and%20Ahmet%20Cinar%20and%20Shu%20Pan%20and%20Jonatan%20Scharff%20Willners%20and%20Michele%20Grimald%20and%20Ignacio%20Carlucho%20and%20Yvan%20Petillot%0AAbstract%3A%20This%20paper%20presents%20InsSo3D%2C%20an%20accurate%20and%20efficient%20method%20for%20large-scale%203D%20Simultaneous%20Localisation%20and%20Mapping%20%28SLAM%29%20using%20a%203D%20Sonar%20and%20an%20Inertial%20Navigation%20System%20%28INS%29.%20Unlike%20traditional%20sonar%2C%20which%20produces%202D%20images%20containing%20range%20and%20azimuth%20information%20but%20lacks%20elevation%20information%2C%203D%20Sonar%20produces%20a%203D%20point%20cloud%2C%20which%20therefore%20does%20not%20suffer%20from%20elevation%20ambiguity.%20We%20introduce%20a%20robust%20and%20modern%20SLAM%20framework%20adapted%20to%20the%203D%20Sonar%20data%20using%20INS%20as%20prior%2C%20detecting%20loop%20closure%20and%20performing%20pose%20graph%20optimisation.%20We%20evaluated%20InsSo3D%20performance%20inside%20a%20test%20tank%20with%20access%20to%20ground%20truth%20data%20and%20in%20an%20outdoor%20flooded%20quarry.%20Comparisons%20to%20reference%20trajectories%20and%20maps%20obtained%20from%20an%20underwater%20motion%20tracking%20system%20and%20visual%20Structure%20From%20Motion%20%28SFM%29%20demonstrate%20that%20InsSo3D%20efficiently%20corrects%20odometry%20drift.%20The%20average%20trajectory%20error%20is%20below%2021cm%20during%20a%2050-minute-long%20mission%2C%20producing%20a%20map%20of%2010m%20by%2020m%20with%20a%209cm%20average%20reconstruction%20error%2C%20enabling%20safe%20inspection%20of%20natural%20or%20artificial%20underwater%20structures%20even%20in%20murky%20water%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsSo3D%253A%2520Inertial%2520Navigation%2520System%2520and%25203D%2520Sonar%2520SLAM%2520for%2520turbid%2520environment%2520inspection%26entry.906535625%3DSimon%2520Archieri%2520and%2520Ahmet%2520Cinar%2520and%2520Shu%2520Pan%2520and%2520Jonatan%2520Scharff%2520Willners%2520and%2520Michele%2520Grimald%2520and%2520Ignacio%2520Carlucho%2520and%2520Yvan%2520Petillot%26entry.1292438233%3DThis%2520paper%2520presents%2520InsSo3D%252C%2520an%2520accurate%2520and%2520efficient%2520method%2520for%2520large-scale%25203D%2520Simultaneous%2520Localisation%2520and%2520Mapping%2520%2528SLAM%2529%2520using%2520a%25203D%2520Sonar%2520and%2520an%2520Inertial%2520Navigation%2520System%2520%2528INS%2529.%2520Unlike%2520traditional%2520sonar%252C%2520which%2520produces%25202D%2520images%2520containing%2520range%2520and%2520azimuth%2520information%2520but%2520lacks%2520elevation%2520information%252C%25203D%2520Sonar%2520produces%2520a%25203D%2520point%2520cloud%252C%2520which%2520therefore%2520does%2520not%2520suffer%2520from%2520elevation%2520ambiguity.%2520We%2520introduce%2520a%2520robust%2520and%2520modern%2520SLAM%2520framework%2520adapted%2520to%2520the%25203D%2520Sonar%2520data%2520using%2520INS%2520as%2520prior%252C%2520detecting%2520loop%2520closure%2520and%2520performing%2520pose%2520graph%2520optimisation.%2520We%2520evaluated%2520InsSo3D%2520performance%2520inside%2520a%2520test%2520tank%2520with%2520access%2520to%2520ground%2520truth%2520data%2520and%2520in%2520an%2520outdoor%2520flooded%2520quarry.%2520Comparisons%2520to%2520reference%2520trajectories%2520and%2520maps%2520obtained%2520from%2520an%2520underwater%2520motion%2520tracking%2520system%2520and%2520visual%2520Structure%2520From%2520Motion%2520%2528SFM%2529%2520demonstrate%2520that%2520InsSo3D%2520efficiently%2520corrects%2520odometry%2520drift.%2520The%2520average%2520trajectory%2520error%2520is%2520below%252021cm%2520during%2520a%252050-minute-long%2520mission%252C%2520producing%2520a%2520map%2520of%252010m%2520by%252020m%2520with%2520a%25209cm%2520average%2520reconstruction%2520error%252C%2520enabling%2520safe%2520inspection%2520of%2520natural%2520or%2520artificial%2520underwater%2520structures%2520even%2520in%2520murky%2520water%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsSo3D%3A%20Inertial%20Navigation%20System%20and%203D%20Sonar%20SLAM%20for%20turbid%20environment%20inspection&entry.906535625=Simon%20Archieri%20and%20Ahmet%20Cinar%20and%20Shu%20Pan%20and%20Jonatan%20Scharff%20Willners%20and%20Michele%20Grimald%20and%20Ignacio%20Carlucho%20and%20Yvan%20Petillot&entry.1292438233=This%20paper%20presents%20InsSo3D%2C%20an%20accurate%20and%20efficient%20method%20for%20large-scale%203D%20Simultaneous%20Localisation%20and%20Mapping%20%28SLAM%29%20using%20a%203D%20Sonar%20and%20an%20Inertial%20Navigation%20System%20%28INS%29.%20Unlike%20traditional%20sonar%2C%20which%20produces%202D%20images%20containing%20range%20and%20azimuth%20information%20but%20lacks%20elevation%20information%2C%203D%20Sonar%20produces%20a%203D%20point%20cloud%2C%20which%20therefore%20does%20not%20suffer%20from%20elevation%20ambiguity.%20We%20introduce%20a%20robust%20and%20modern%20SLAM%20framework%20adapted%20to%20the%203D%20Sonar%20data%20using%20INS%20as%20prior%2C%20detecting%20loop%20closure%20and%20performing%20pose%20graph%20optimisation.%20We%20evaluated%20InsSo3D%20performance%20inside%20a%20test%20tank%20with%20access%20to%20ground%20truth%20data%20and%20in%20an%20outdoor%20flooded%20quarry.%20Comparisons%20to%20reference%20trajectories%20and%20maps%20obtained%20from%20an%20underwater%20motion%20tracking%20system%20and%20visual%20Structure%20From%20Motion%20%28SFM%29%20demonstrate%20that%20InsSo3D%20efficiently%20corrects%20odometry%20drift.%20The%20average%20trajectory%20error%20is%20below%2021cm%20during%20a%2050-minute-long%20mission%2C%20producing%20a%20map%20of%2010m%20by%2020m%20with%20a%209cm%20average%20reconstruction%20error%2C%20enabling%20safe%20inspection%20of%20natural%20or%20artificial%20underwater%20structures%20even%20in%20murky%20water%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2601.05805v1&entry.124074799=Read"},
{"title": "Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease", "author": "Zobia Batool and Diala Lteif and Vijaya B. Kolachalama and Huseyin Ozkan and Erchan Aptoula", "abstract": "Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.", "link": "http://arxiv.org/abs/2601.01485v2", "date": "2026-01-09", "relevancy": 2.6766, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5329}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Domain%20Generalization%20in%20Magnetic%20Resonance-Based%20Assessment%20of%20Alzheimer%27s%20Disease&body=Title%3A%20Higher-Order%20Domain%20Generalization%20in%20Magnetic%20Resonance-Based%20Assessment%20of%20Alzheimer%27s%20Disease%0AAuthor%3A%20Zobia%20Batool%20and%20Diala%20Lteif%20and%20Vijaya%20B.%20Kolachalama%20and%20Huseyin%20Ozkan%20and%20Erchan%20Aptoula%0AAbstract%3A%20Despite%20progress%20in%20deep%20learning%20for%20Alzheimer%27s%20disease%20%28AD%29%20diagnostics%2C%20models%20trained%20on%20structural%20magnetic%20resonance%20imaging%20%28sMRI%29%20often%20do%20not%20perform%20well%20when%20applied%20to%20new%20cohorts%20due%20to%20domain%20shifts%20from%20varying%20scanners%2C%20protocols%20and%20patient%20demographics.%20AD%2C%20the%20primary%20driver%20of%20dementia%2C%20manifests%20through%20progressive%20cognitive%20and%20neuroanatomical%20changes%20like%20atrophy%20and%20ventricular%20expansion%2C%20making%20robust%2C%20generalizable%20classification%20essential%20for%20real-world%20use.%20While%20convolutional%20neural%20networks%20and%20transformers%20have%20advanced%20feature%20extraction%20via%20attention%20and%20fusion%20techniques%2C%20single-domain%20generalization%20%28SDG%29%20remains%20underexplored%20yet%20critical%2C%20given%20the%20fragmented%20nature%20of%20AD%20datasets.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Extended%20MixStyle%20%28EM%29%2C%20a%20framework%20for%20blending%20higher-order%20feature%20moments%20%28skewness%20and%20kurtosis%29%20to%20mimic%20diverse%20distributional%20variations.%20Trained%20on%20sMRI%20data%20from%20the%20National%20Alzheimer%27s%20Coordinating%20Center%20%28NACC%3B%20n%3D4%2C647%29%20to%20differentiate%20persons%20with%20normal%20cognition%20%28NC%29%20from%20those%20with%20mild%20cognitive%20impairment%20%28MCI%29%20or%20AD%20and%20tested%20on%20three%20unseen%20cohorts%20%28total%20n%3D3%2C126%29%2C%20EM%20yields%20enhanced%20cross-domain%20performance%2C%20improving%20macro-F1%20on%20average%20by%202.4%20percentage%20points%20over%20state-of-the-art%20SDG%20benchmarks%2C%20underscoring%20its%20promise%20for%20invariant%2C%20reliable%20AD%20detection%20in%20heterogeneous%20real-world%20settings.%20The%20source%20code%20will%20be%20made%20available%20upon%20acceptance%20at%20https%3A//github.com/zobia111/Extended-Mixstyle.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Domain%2520Generalization%2520in%2520Magnetic%2520Resonance-Based%2520Assessment%2520of%2520Alzheimer%2527s%2520Disease%26entry.906535625%3DZobia%2520Batool%2520and%2520Diala%2520Lteif%2520and%2520Vijaya%2520B.%2520Kolachalama%2520and%2520Huseyin%2520Ozkan%2520and%2520Erchan%2520Aptoula%26entry.1292438233%3DDespite%2520progress%2520in%2520deep%2520learning%2520for%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520diagnostics%252C%2520models%2520trained%2520on%2520structural%2520magnetic%2520resonance%2520imaging%2520%2528sMRI%2529%2520often%2520do%2520not%2520perform%2520well%2520when%2520applied%2520to%2520new%2520cohorts%2520due%2520to%2520domain%2520shifts%2520from%2520varying%2520scanners%252C%2520protocols%2520and%2520patient%2520demographics.%2520AD%252C%2520the%2520primary%2520driver%2520of%2520dementia%252C%2520manifests%2520through%2520progressive%2520cognitive%2520and%2520neuroanatomical%2520changes%2520like%2520atrophy%2520and%2520ventricular%2520expansion%252C%2520making%2520robust%252C%2520generalizable%2520classification%2520essential%2520for%2520real-world%2520use.%2520While%2520convolutional%2520neural%2520networks%2520and%2520transformers%2520have%2520advanced%2520feature%2520extraction%2520via%2520attention%2520and%2520fusion%2520techniques%252C%2520single-domain%2520generalization%2520%2528SDG%2529%2520remains%2520underexplored%2520yet%2520critical%252C%2520given%2520the%2520fragmented%2520nature%2520of%2520AD%2520datasets.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Extended%2520MixStyle%2520%2528EM%2529%252C%2520a%2520framework%2520for%2520blending%2520higher-order%2520feature%2520moments%2520%2528skewness%2520and%2520kurtosis%2529%2520to%2520mimic%2520diverse%2520distributional%2520variations.%2520Trained%2520on%2520sMRI%2520data%2520from%2520the%2520National%2520Alzheimer%2527s%2520Coordinating%2520Center%2520%2528NACC%253B%2520n%253D4%252C647%2529%2520to%2520differentiate%2520persons%2520with%2520normal%2520cognition%2520%2528NC%2529%2520from%2520those%2520with%2520mild%2520cognitive%2520impairment%2520%2528MCI%2529%2520or%2520AD%2520and%2520tested%2520on%2520three%2520unseen%2520cohorts%2520%2528total%2520n%253D3%252C126%2529%252C%2520EM%2520yields%2520enhanced%2520cross-domain%2520performance%252C%2520improving%2520macro-F1%2520on%2520average%2520by%25202.4%2520percentage%2520points%2520over%2520state-of-the-art%2520SDG%2520benchmarks%252C%2520underscoring%2520its%2520promise%2520for%2520invariant%252C%2520reliable%2520AD%2520detection%2520in%2520heterogeneous%2520real-world%2520settings.%2520The%2520source%2520code%2520will%2520be%2520made%2520available%2520upon%2520acceptance%2520at%2520https%253A//github.com/zobia111/Extended-Mixstyle.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Domain%20Generalization%20in%20Magnetic%20Resonance-Based%20Assessment%20of%20Alzheimer%27s%20Disease&entry.906535625=Zobia%20Batool%20and%20Diala%20Lteif%20and%20Vijaya%20B.%20Kolachalama%20and%20Huseyin%20Ozkan%20and%20Erchan%20Aptoula&entry.1292438233=Despite%20progress%20in%20deep%20learning%20for%20Alzheimer%27s%20disease%20%28AD%29%20diagnostics%2C%20models%20trained%20on%20structural%20magnetic%20resonance%20imaging%20%28sMRI%29%20often%20do%20not%20perform%20well%20when%20applied%20to%20new%20cohorts%20due%20to%20domain%20shifts%20from%20varying%20scanners%2C%20protocols%20and%20patient%20demographics.%20AD%2C%20the%20primary%20driver%20of%20dementia%2C%20manifests%20through%20progressive%20cognitive%20and%20neuroanatomical%20changes%20like%20atrophy%20and%20ventricular%20expansion%2C%20making%20robust%2C%20generalizable%20classification%20essential%20for%20real-world%20use.%20While%20convolutional%20neural%20networks%20and%20transformers%20have%20advanced%20feature%20extraction%20via%20attention%20and%20fusion%20techniques%2C%20single-domain%20generalization%20%28SDG%29%20remains%20underexplored%20yet%20critical%2C%20given%20the%20fragmented%20nature%20of%20AD%20datasets.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Extended%20MixStyle%20%28EM%29%2C%20a%20framework%20for%20blending%20higher-order%20feature%20moments%20%28skewness%20and%20kurtosis%29%20to%20mimic%20diverse%20distributional%20variations.%20Trained%20on%20sMRI%20data%20from%20the%20National%20Alzheimer%27s%20Coordinating%20Center%20%28NACC%3B%20n%3D4%2C647%29%20to%20differentiate%20persons%20with%20normal%20cognition%20%28NC%29%20from%20those%20with%20mild%20cognitive%20impairment%20%28MCI%29%20or%20AD%20and%20tested%20on%20three%20unseen%20cohorts%20%28total%20n%3D3%2C126%29%2C%20EM%20yields%20enhanced%20cross-domain%20performance%2C%20improving%20macro-F1%20on%20average%20by%202.4%20percentage%20points%20over%20state-of-the-art%20SDG%20benchmarks%2C%20underscoring%20its%20promise%20for%20invariant%2C%20reliable%20AD%20detection%20in%20heterogeneous%20real-world%20settings.%20The%20source%20code%20will%20be%20made%20available%20upon%20acceptance%20at%20https%3A//github.com/zobia111/Extended-Mixstyle.&entry.1838667208=http%3A//arxiv.org/abs/2601.01485v2&entry.124074799=Read"},
{"title": "Pyramidal Adaptive Cross-Gating for Multimodal Detection", "author": "Zidong Gu and Shoufu Tian", "abstract": "Object detection in aerial imagery is a critical task in applications such as UAV reconnaissance. Although existing methods have extensively explored feature interaction between different modalities, they commonly rely on simple fusion strategies for feature aggregation. This introduces two critical flaws: it is prone to cross-modal noise and disrupts the hierarchical structure of the feature pyramid, thereby impairing the fine-grained detection of small objects. To address this challenge, we propose the Pyramidal Adaptive Cross-Gating Network (PACGNet), an architecture designed to perform deep fusion within the backbone. To this end, we design two core components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module employs a bidirectional, symmetrical \"horizontal\" gating mechanism to selectively absorb complementary information, suppress noise, and preserve the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy via a progressive hierarchical gating mechanism. This leverages the detailed features from a preceding, higher-resolution level to guide the fusion at the current, lower-resolution level, effectively preserving fine-grained details as features propagate. Through evaluations conducted on the DroneVehicle and VEDAI datasets, our PACGNet sets a new state-of-the-art benchmark, with mAP50 scores reaching 82.2% and 82.1% respectively.", "link": "http://arxiv.org/abs/2512.18291v2", "date": "2026-01-09", "relevancy": 2.6442, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.533}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5271}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pyramidal%20Adaptive%20Cross-Gating%20for%20Multimodal%20Detection&body=Title%3A%20Pyramidal%20Adaptive%20Cross-Gating%20for%20Multimodal%20Detection%0AAuthor%3A%20Zidong%20Gu%20and%20Shoufu%20Tian%0AAbstract%3A%20Object%20detection%20in%20aerial%20imagery%20is%20a%20critical%20task%20in%20applications%20such%20as%20UAV%20reconnaissance.%20Although%20existing%20methods%20have%20extensively%20explored%20feature%20interaction%20between%20different%20modalities%2C%20they%20commonly%20rely%20on%20simple%20fusion%20strategies%20for%20feature%20aggregation.%20This%20introduces%20two%20critical%20flaws%3A%20it%20is%20prone%20to%20cross-modal%20noise%20and%20disrupts%20the%20hierarchical%20structure%20of%20the%20feature%20pyramid%2C%20thereby%20impairing%20the%20fine-grained%20detection%20of%20small%20objects.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Pyramidal%20Adaptive%20Cross-Gating%20Network%20%28PACGNet%29%2C%20an%20architecture%20designed%20to%20perform%20deep%20fusion%20within%20the%20backbone.%20To%20this%20end%2C%20we%20design%20two%20core%20components%3A%20the%20Symmetrical%20Cross-Gating%20%28SCG%29%20module%20and%20the%20Pyramidal%20Feature-aware%20Multimodal%20Gating%20%28PFMG%29%20module.%20The%20SCG%20module%20employs%20a%20bidirectional%2C%20symmetrical%20%22horizontal%22%20gating%20mechanism%20to%20selectively%20absorb%20complementary%20information%2C%20suppress%20noise%2C%20and%20preserve%20the%20semantic%20integrity%20of%20each%20modality.%20The%20PFMG%20module%20reconstructs%20the%20feature%20hierarchy%20via%20a%20progressive%20hierarchical%20gating%20mechanism.%20This%20leverages%20the%20detailed%20features%20from%20a%20preceding%2C%20higher-resolution%20level%20to%20guide%20the%20fusion%20at%20the%20current%2C%20lower-resolution%20level%2C%20effectively%20preserving%20fine-grained%20details%20as%20features%20propagate.%20Through%20evaluations%20conducted%20on%20the%20DroneVehicle%20and%20VEDAI%20datasets%2C%20our%20PACGNet%20sets%20a%20new%20state-of-the-art%20benchmark%2C%20with%20mAP50%20scores%20reaching%2082.2%25%20and%2082.1%25%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyramidal%2520Adaptive%2520Cross-Gating%2520for%2520Multimodal%2520Detection%26entry.906535625%3DZidong%2520Gu%2520and%2520Shoufu%2520Tian%26entry.1292438233%3DObject%2520detection%2520in%2520aerial%2520imagery%2520is%2520a%2520critical%2520task%2520in%2520applications%2520such%2520as%2520UAV%2520reconnaissance.%2520Although%2520existing%2520methods%2520have%2520extensively%2520explored%2520feature%2520interaction%2520between%2520different%2520modalities%252C%2520they%2520commonly%2520rely%2520on%2520simple%2520fusion%2520strategies%2520for%2520feature%2520aggregation.%2520This%2520introduces%2520two%2520critical%2520flaws%253A%2520it%2520is%2520prone%2520to%2520cross-modal%2520noise%2520and%2520disrupts%2520the%2520hierarchical%2520structure%2520of%2520the%2520feature%2520pyramid%252C%2520thereby%2520impairing%2520the%2520fine-grained%2520detection%2520of%2520small%2520objects.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Pyramidal%2520Adaptive%2520Cross-Gating%2520Network%2520%2528PACGNet%2529%252C%2520an%2520architecture%2520designed%2520to%2520perform%2520deep%2520fusion%2520within%2520the%2520backbone.%2520To%2520this%2520end%252C%2520we%2520design%2520two%2520core%2520components%253A%2520the%2520Symmetrical%2520Cross-Gating%2520%2528SCG%2529%2520module%2520and%2520the%2520Pyramidal%2520Feature-aware%2520Multimodal%2520Gating%2520%2528PFMG%2529%2520module.%2520The%2520SCG%2520module%2520employs%2520a%2520bidirectional%252C%2520symmetrical%2520%2522horizontal%2522%2520gating%2520mechanism%2520to%2520selectively%2520absorb%2520complementary%2520information%252C%2520suppress%2520noise%252C%2520and%2520preserve%2520the%2520semantic%2520integrity%2520of%2520each%2520modality.%2520The%2520PFMG%2520module%2520reconstructs%2520the%2520feature%2520hierarchy%2520via%2520a%2520progressive%2520hierarchical%2520gating%2520mechanism.%2520This%2520leverages%2520the%2520detailed%2520features%2520from%2520a%2520preceding%252C%2520higher-resolution%2520level%2520to%2520guide%2520the%2520fusion%2520at%2520the%2520current%252C%2520lower-resolution%2520level%252C%2520effectively%2520preserving%2520fine-grained%2520details%2520as%2520features%2520propagate.%2520Through%2520evaluations%2520conducted%2520on%2520the%2520DroneVehicle%2520and%2520VEDAI%2520datasets%252C%2520our%2520PACGNet%2520sets%2520a%2520new%2520state-of-the-art%2520benchmark%252C%2520with%2520mAP50%2520scores%2520reaching%252082.2%2525%2520and%252082.1%2525%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pyramidal%20Adaptive%20Cross-Gating%20for%20Multimodal%20Detection&entry.906535625=Zidong%20Gu%20and%20Shoufu%20Tian&entry.1292438233=Object%20detection%20in%20aerial%20imagery%20is%20a%20critical%20task%20in%20applications%20such%20as%20UAV%20reconnaissance.%20Although%20existing%20methods%20have%20extensively%20explored%20feature%20interaction%20between%20different%20modalities%2C%20they%20commonly%20rely%20on%20simple%20fusion%20strategies%20for%20feature%20aggregation.%20This%20introduces%20two%20critical%20flaws%3A%20it%20is%20prone%20to%20cross-modal%20noise%20and%20disrupts%20the%20hierarchical%20structure%20of%20the%20feature%20pyramid%2C%20thereby%20impairing%20the%20fine-grained%20detection%20of%20small%20objects.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Pyramidal%20Adaptive%20Cross-Gating%20Network%20%28PACGNet%29%2C%20an%20architecture%20designed%20to%20perform%20deep%20fusion%20within%20the%20backbone.%20To%20this%20end%2C%20we%20design%20two%20core%20components%3A%20the%20Symmetrical%20Cross-Gating%20%28SCG%29%20module%20and%20the%20Pyramidal%20Feature-aware%20Multimodal%20Gating%20%28PFMG%29%20module.%20The%20SCG%20module%20employs%20a%20bidirectional%2C%20symmetrical%20%22horizontal%22%20gating%20mechanism%20to%20selectively%20absorb%20complementary%20information%2C%20suppress%20noise%2C%20and%20preserve%20the%20semantic%20integrity%20of%20each%20modality.%20The%20PFMG%20module%20reconstructs%20the%20feature%20hierarchy%20via%20a%20progressive%20hierarchical%20gating%20mechanism.%20This%20leverages%20the%20detailed%20features%20from%20a%20preceding%2C%20higher-resolution%20level%20to%20guide%20the%20fusion%20at%20the%20current%2C%20lower-resolution%20level%2C%20effectively%20preserving%20fine-grained%20details%20as%20features%20propagate.%20Through%20evaluations%20conducted%20on%20the%20DroneVehicle%20and%20VEDAI%20datasets%2C%20our%20PACGNet%20sets%20a%20new%20state-of-the-art%20benchmark%2C%20with%20mAP50%20scores%20reaching%2082.2%25%20and%2082.1%25%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2512.18291v2&entry.124074799=Read"},
{"title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models", "author": "Santosh Srinath K and Mudit Somani and Varun Reddy Padala and Prajna Devi Upadhyay and Abhijit Das", "abstract": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.", "link": "http://arxiv.org/abs/2601.05874v1", "date": "2026-01-09", "relevancy": 2.5885, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual-learning%20for%20Modelling%20Low-Resource%20Languages%20from%20Large%20Language%20Models&body=Title%3A%20Continual-learning%20for%20Modelling%20Low-Resource%20Languages%20from%20Large%20Language%20Models%0AAuthor%3A%20Santosh%20Srinath%20K%20and%20Mudit%20Somani%20and%20Varun%20Reddy%20Padala%20and%20Prajna%20Devi%20Upadhyay%20and%20Abhijit%20Das%0AAbstract%3A%20Modelling%20a%20language%20model%20for%20a%20multi-lingual%20scenario%20includes%20several%20potential%20challenges%2C%20among%20which%20catastrophic%20forgetting%20is%20the%20major%20challenge.%20For%20example%2C%20small%20language%20models%20%28SLM%29%20built%20for%20low-resource%20languages%20by%20adapting%20large%20language%20models%20%28LLMs%29%20pose%20the%20challenge%20of%20catastrophic%20forgetting.%20This%20work%20proposes%20to%20employ%20a%20continual%20learning%20strategy%20using%20parts-of-speech%20%28POS%29-based%20code-switching%20along%20with%20a%20replay%20adapter%20strategy%20to%20mitigate%20the%20identified%20gap%20of%20catastrophic%20forgetting%20while%20training%20SLM%20from%20LLM.%20Experiments%20conducted%20on%20vision%20language%20tasks%20such%20as%20visual%20question%20answering%20and%20language%20modelling%20task%20exhibits%20the%20success%20of%20the%20proposed%20architecture.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual-learning%2520for%2520Modelling%2520Low-Resource%2520Languages%2520from%2520Large%2520Language%2520Models%26entry.906535625%3DSantosh%2520Srinath%2520K%2520and%2520Mudit%2520Somani%2520and%2520Varun%2520Reddy%2520Padala%2520and%2520Prajna%2520Devi%2520Upadhyay%2520and%2520Abhijit%2520Das%26entry.1292438233%3DModelling%2520a%2520language%2520model%2520for%2520a%2520multi-lingual%2520scenario%2520includes%2520several%2520potential%2520challenges%252C%2520among%2520which%2520catastrophic%2520forgetting%2520is%2520the%2520major%2520challenge.%2520For%2520example%252C%2520small%2520language%2520models%2520%2528SLM%2529%2520built%2520for%2520low-resource%2520languages%2520by%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520pose%2520the%2520challenge%2520of%2520catastrophic%2520forgetting.%2520This%2520work%2520proposes%2520to%2520employ%2520a%2520continual%2520learning%2520strategy%2520using%2520parts-of-speech%2520%2528POS%2529-based%2520code-switching%2520along%2520with%2520a%2520replay%2520adapter%2520strategy%2520to%2520mitigate%2520the%2520identified%2520gap%2520of%2520catastrophic%2520forgetting%2520while%2520training%2520SLM%2520from%2520LLM.%2520Experiments%2520conducted%2520on%2520vision%2520language%2520tasks%2520such%2520as%2520visual%2520question%2520answering%2520and%2520language%2520modelling%2520task%2520exhibits%2520the%2520success%2520of%2520the%2520proposed%2520architecture.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual-learning%20for%20Modelling%20Low-Resource%20Languages%20from%20Large%20Language%20Models&entry.906535625=Santosh%20Srinath%20K%20and%20Mudit%20Somani%20and%20Varun%20Reddy%20Padala%20and%20Prajna%20Devi%20Upadhyay%20and%20Abhijit%20Das&entry.1292438233=Modelling%20a%20language%20model%20for%20a%20multi-lingual%20scenario%20includes%20several%20potential%20challenges%2C%20among%20which%20catastrophic%20forgetting%20is%20the%20major%20challenge.%20For%20example%2C%20small%20language%20models%20%28SLM%29%20built%20for%20low-resource%20languages%20by%20adapting%20large%20language%20models%20%28LLMs%29%20pose%20the%20challenge%20of%20catastrophic%20forgetting.%20This%20work%20proposes%20to%20employ%20a%20continual%20learning%20strategy%20using%20parts-of-speech%20%28POS%29-based%20code-switching%20along%20with%20a%20replay%20adapter%20strategy%20to%20mitigate%20the%20identified%20gap%20of%20catastrophic%20forgetting%20while%20training%20SLM%20from%20LLM.%20Experiments%20conducted%20on%20vision%20language%20tasks%20such%20as%20visual%20question%20answering%20and%20language%20modelling%20task%20exhibits%20the%20success%20of%20the%20proposed%20architecture.&entry.1838667208=http%3A//arxiv.org/abs/2601.05874v1&entry.124074799=Read"},
{"title": "Phase4DFD: Multi-Domain Phase-Aware Attention for Deepfake Detection", "author": "Zhen-Xin Lin and Shang-Kuan Chen", "abstract": "Recent deepfake detection methods have increasingly explored frequency domain representations to reveal manipulation artifacts that are difficult to detect in the spatial domain. However, most existing approaches rely primarily on spectral magnitude, implicitly under exploring the role of phase information. In this work, we propose Phase4DFD, a phase aware frequency domain deepfake detection framework that explicitly models phase magnitude interactions via a learnable attention mechanism. Our approach augments standard RGB input with Fast Fourier Transform (FFT) magnitude and local binary pattern (LBP) representations to expose subtle synthesis artifacts that remain indistinguishable under spatial analysis alone. Crucially, we introduce an input level phase aware attention module that uses phase discontinuities commonly introduced by synthetic generation to guide the model toward frequency patterns that are most indicative of manipulation before backbone feature extraction. The attended multi domain representation is processed by an efficient BNext M backbone, with optional channel spatial attention applied for semantic feature refinement. Extensive experiments on the CIFAKE and DFFD datasets demonstrate that our proposed model Phase4DFD outperforms state of the art spatial and frequency-based detectors while maintaining low computational overhead. Comprehensive ablation studies further confirm that explicit phase modeling provides complementary and non-redundant information beyond magnitude-only frequency representations.", "link": "http://arxiv.org/abs/2601.05861v1", "date": "2026-01-09", "relevancy": 2.5688, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5188}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5117}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phase4DFD%3A%20Multi-Domain%20Phase-Aware%20Attention%20for%20Deepfake%20Detection&body=Title%3A%20Phase4DFD%3A%20Multi-Domain%20Phase-Aware%20Attention%20for%20Deepfake%20Detection%0AAuthor%3A%20Zhen-Xin%20Lin%20and%20Shang-Kuan%20Chen%0AAbstract%3A%20Recent%20deepfake%20detection%20methods%20have%20increasingly%20explored%20frequency%20domain%20representations%20to%20reveal%20manipulation%20artifacts%20that%20are%20difficult%20to%20detect%20in%20the%20spatial%20domain.%20However%2C%20most%20existing%20approaches%20rely%20primarily%20on%20spectral%20magnitude%2C%20implicitly%20under%20exploring%20the%20role%20of%20phase%20information.%20In%20this%20work%2C%20we%20propose%20Phase4DFD%2C%20a%20phase%20aware%20frequency%20domain%20deepfake%20detection%20framework%20that%20explicitly%20models%20phase%20magnitude%20interactions%20via%20a%20learnable%20attention%20mechanism.%20Our%20approach%20augments%20standard%20RGB%20input%20with%20Fast%20Fourier%20Transform%20%28FFT%29%20magnitude%20and%20local%20binary%20pattern%20%28LBP%29%20representations%20to%20expose%20subtle%20synthesis%20artifacts%20that%20remain%20indistinguishable%20under%20spatial%20analysis%20alone.%20Crucially%2C%20we%20introduce%20an%20input%20level%20phase%20aware%20attention%20module%20that%20uses%20phase%20discontinuities%20commonly%20introduced%20by%20synthetic%20generation%20to%20guide%20the%20model%20toward%20frequency%20patterns%20that%20are%20most%20indicative%20of%20manipulation%20before%20backbone%20feature%20extraction.%20The%20attended%20multi%20domain%20representation%20is%20processed%20by%20an%20efficient%20BNext%20M%20backbone%2C%20with%20optional%20channel%20spatial%20attention%20applied%20for%20semantic%20feature%20refinement.%20Extensive%20experiments%20on%20the%20CIFAKE%20and%20DFFD%20datasets%20demonstrate%20that%20our%20proposed%20model%20Phase4DFD%20outperforms%20state%20of%20the%20art%20spatial%20and%20frequency-based%20detectors%20while%20maintaining%20low%20computational%20overhead.%20Comprehensive%20ablation%20studies%20further%20confirm%20that%20explicit%20phase%20modeling%20provides%20complementary%20and%20non-redundant%20information%20beyond%20magnitude-only%20frequency%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhase4DFD%253A%2520Multi-Domain%2520Phase-Aware%2520Attention%2520for%2520Deepfake%2520Detection%26entry.906535625%3DZhen-Xin%2520Lin%2520and%2520Shang-Kuan%2520Chen%26entry.1292438233%3DRecent%2520deepfake%2520detection%2520methods%2520have%2520increasingly%2520explored%2520frequency%2520domain%2520representations%2520to%2520reveal%2520manipulation%2520artifacts%2520that%2520are%2520difficult%2520to%2520detect%2520in%2520the%2520spatial%2520domain.%2520However%252C%2520most%2520existing%2520approaches%2520rely%2520primarily%2520on%2520spectral%2520magnitude%252C%2520implicitly%2520under%2520exploring%2520the%2520role%2520of%2520phase%2520information.%2520In%2520this%2520work%252C%2520we%2520propose%2520Phase4DFD%252C%2520a%2520phase%2520aware%2520frequency%2520domain%2520deepfake%2520detection%2520framework%2520that%2520explicitly%2520models%2520phase%2520magnitude%2520interactions%2520via%2520a%2520learnable%2520attention%2520mechanism.%2520Our%2520approach%2520augments%2520standard%2520RGB%2520input%2520with%2520Fast%2520Fourier%2520Transform%2520%2528FFT%2529%2520magnitude%2520and%2520local%2520binary%2520pattern%2520%2528LBP%2529%2520representations%2520to%2520expose%2520subtle%2520synthesis%2520artifacts%2520that%2520remain%2520indistinguishable%2520under%2520spatial%2520analysis%2520alone.%2520Crucially%252C%2520we%2520introduce%2520an%2520input%2520level%2520phase%2520aware%2520attention%2520module%2520that%2520uses%2520phase%2520discontinuities%2520commonly%2520introduced%2520by%2520synthetic%2520generation%2520to%2520guide%2520the%2520model%2520toward%2520frequency%2520patterns%2520that%2520are%2520most%2520indicative%2520of%2520manipulation%2520before%2520backbone%2520feature%2520extraction.%2520The%2520attended%2520multi%2520domain%2520representation%2520is%2520processed%2520by%2520an%2520efficient%2520BNext%2520M%2520backbone%252C%2520with%2520optional%2520channel%2520spatial%2520attention%2520applied%2520for%2520semantic%2520feature%2520refinement.%2520Extensive%2520experiments%2520on%2520the%2520CIFAKE%2520and%2520DFFD%2520datasets%2520demonstrate%2520that%2520our%2520proposed%2520model%2520Phase4DFD%2520outperforms%2520state%2520of%2520the%2520art%2520spatial%2520and%2520frequency-based%2520detectors%2520while%2520maintaining%2520low%2520computational%2520overhead.%2520Comprehensive%2520ablation%2520studies%2520further%2520confirm%2520that%2520explicit%2520phase%2520modeling%2520provides%2520complementary%2520and%2520non-redundant%2520information%2520beyond%2520magnitude-only%2520frequency%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phase4DFD%3A%20Multi-Domain%20Phase-Aware%20Attention%20for%20Deepfake%20Detection&entry.906535625=Zhen-Xin%20Lin%20and%20Shang-Kuan%20Chen&entry.1292438233=Recent%20deepfake%20detection%20methods%20have%20increasingly%20explored%20frequency%20domain%20representations%20to%20reveal%20manipulation%20artifacts%20that%20are%20difficult%20to%20detect%20in%20the%20spatial%20domain.%20However%2C%20most%20existing%20approaches%20rely%20primarily%20on%20spectral%20magnitude%2C%20implicitly%20under%20exploring%20the%20role%20of%20phase%20information.%20In%20this%20work%2C%20we%20propose%20Phase4DFD%2C%20a%20phase%20aware%20frequency%20domain%20deepfake%20detection%20framework%20that%20explicitly%20models%20phase%20magnitude%20interactions%20via%20a%20learnable%20attention%20mechanism.%20Our%20approach%20augments%20standard%20RGB%20input%20with%20Fast%20Fourier%20Transform%20%28FFT%29%20magnitude%20and%20local%20binary%20pattern%20%28LBP%29%20representations%20to%20expose%20subtle%20synthesis%20artifacts%20that%20remain%20indistinguishable%20under%20spatial%20analysis%20alone.%20Crucially%2C%20we%20introduce%20an%20input%20level%20phase%20aware%20attention%20module%20that%20uses%20phase%20discontinuities%20commonly%20introduced%20by%20synthetic%20generation%20to%20guide%20the%20model%20toward%20frequency%20patterns%20that%20are%20most%20indicative%20of%20manipulation%20before%20backbone%20feature%20extraction.%20The%20attended%20multi%20domain%20representation%20is%20processed%20by%20an%20efficient%20BNext%20M%20backbone%2C%20with%20optional%20channel%20spatial%20attention%20applied%20for%20semantic%20feature%20refinement.%20Extensive%20experiments%20on%20the%20CIFAKE%20and%20DFFD%20datasets%20demonstrate%20that%20our%20proposed%20model%20Phase4DFD%20outperforms%20state%20of%20the%20art%20spatial%20and%20frequency-based%20detectors%20while%20maintaining%20low%20computational%20overhead.%20Comprehensive%20ablation%20studies%20further%20confirm%20that%20explicit%20phase%20modeling%20provides%20complementary%20and%20non-redundant%20information%20beyond%20magnitude-only%20frequency%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2601.05861v1&entry.124074799=Read"},
{"title": "GlueNN: gluing patchwise analytic solutions with neural networks", "author": "Doyoung Kim and Donghee Lee and Hye-Sung Lee and Jiheon Lee and Jaeok Yi", "abstract": "In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.", "link": "http://arxiv.org/abs/2601.05889v1", "date": "2026-01-09", "relevancy": 2.5484, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5129}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5129}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlueNN%3A%20gluing%20patchwise%20analytic%20solutions%20with%20neural%20networks&body=Title%3A%20GlueNN%3A%20gluing%20patchwise%20analytic%20solutions%20with%20neural%20networks%0AAuthor%3A%20Doyoung%20Kim%20and%20Donghee%20Lee%20and%20Hye-Sung%20Lee%20and%20Jiheon%20Lee%20and%20Jaeok%20Yi%0AAbstract%3A%20In%20many%20problems%20in%20physics%20and%20engineering%2C%20one%20encounters%20complicated%20differential%20equations%20with%20strongly%20scale-dependent%20terms%20for%20which%20exact%20analytical%20or%20numerical%20solutions%20are%20not%20available.%20A%20common%20strategy%20is%20to%20divide%20the%20domain%20into%20several%20regions%20%28patches%29%20and%20simplify%20the%20equation%20in%20each%20region.%20When%20approximate%20analytic%20solutions%20can%20be%20obtained%20in%20each%20patch%2C%20they%20are%20then%20matched%20at%20the%20interfaces%20to%20construct%20a%20global%20solution.%20However%2C%20this%20patching%20procedure%20can%20fail%20to%20reproduce%20the%20correct%20solution%2C%20since%20the%20approximate%20forms%20may%20break%20down%20near%20the%20matching%20boundaries.%20In%20this%20work%2C%20we%20propose%20a%20learning%20framework%20in%20which%20the%20integration%20constants%20of%20asymptotic%20analytic%20solutions%20are%20promoted%20to%20scale-dependent%20functions.%20By%20constraining%20these%20coefficient%20functions%20with%20the%20original%20differential%20equation%20over%20the%20domain%2C%20the%20network%20learns%20a%20globally%20valid%20solution%20that%20smoothly%20interpolates%20between%20asymptotic%20regimes%2C%20eliminating%20the%20need%20for%20arbitrary%20boundary%20matching.%20We%20demonstrate%20the%20effectiveness%20of%20this%20framework%20in%20representative%20problems%20from%20chemical%20kinetics%20and%20cosmology%2C%20where%20it%20accurately%20reproduces%20global%20solutions%20and%20outperforms%20conventional%20matching%20procedures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlueNN%253A%2520gluing%2520patchwise%2520analytic%2520solutions%2520with%2520neural%2520networks%26entry.906535625%3DDoyoung%2520Kim%2520and%2520Donghee%2520Lee%2520and%2520Hye-Sung%2520Lee%2520and%2520Jiheon%2520Lee%2520and%2520Jaeok%2520Yi%26entry.1292438233%3DIn%2520many%2520problems%2520in%2520physics%2520and%2520engineering%252C%2520one%2520encounters%2520complicated%2520differential%2520equations%2520with%2520strongly%2520scale-dependent%2520terms%2520for%2520which%2520exact%2520analytical%2520or%2520numerical%2520solutions%2520are%2520not%2520available.%2520A%2520common%2520strategy%2520is%2520to%2520divide%2520the%2520domain%2520into%2520several%2520regions%2520%2528patches%2529%2520and%2520simplify%2520the%2520equation%2520in%2520each%2520region.%2520When%2520approximate%2520analytic%2520solutions%2520can%2520be%2520obtained%2520in%2520each%2520patch%252C%2520they%2520are%2520then%2520matched%2520at%2520the%2520interfaces%2520to%2520construct%2520a%2520global%2520solution.%2520However%252C%2520this%2520patching%2520procedure%2520can%2520fail%2520to%2520reproduce%2520the%2520correct%2520solution%252C%2520since%2520the%2520approximate%2520forms%2520may%2520break%2520down%2520near%2520the%2520matching%2520boundaries.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520learning%2520framework%2520in%2520which%2520the%2520integration%2520constants%2520of%2520asymptotic%2520analytic%2520solutions%2520are%2520promoted%2520to%2520scale-dependent%2520functions.%2520By%2520constraining%2520these%2520coefficient%2520functions%2520with%2520the%2520original%2520differential%2520equation%2520over%2520the%2520domain%252C%2520the%2520network%2520learns%2520a%2520globally%2520valid%2520solution%2520that%2520smoothly%2520interpolates%2520between%2520asymptotic%2520regimes%252C%2520eliminating%2520the%2520need%2520for%2520arbitrary%2520boundary%2520matching.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520framework%2520in%2520representative%2520problems%2520from%2520chemical%2520kinetics%2520and%2520cosmology%252C%2520where%2520it%2520accurately%2520reproduces%2520global%2520solutions%2520and%2520outperforms%2520conventional%2520matching%2520procedures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlueNN%3A%20gluing%20patchwise%20analytic%20solutions%20with%20neural%20networks&entry.906535625=Doyoung%20Kim%20and%20Donghee%20Lee%20and%20Hye-Sung%20Lee%20and%20Jiheon%20Lee%20and%20Jaeok%20Yi&entry.1292438233=In%20many%20problems%20in%20physics%20and%20engineering%2C%20one%20encounters%20complicated%20differential%20equations%20with%20strongly%20scale-dependent%20terms%20for%20which%20exact%20analytical%20or%20numerical%20solutions%20are%20not%20available.%20A%20common%20strategy%20is%20to%20divide%20the%20domain%20into%20several%20regions%20%28patches%29%20and%20simplify%20the%20equation%20in%20each%20region.%20When%20approximate%20analytic%20solutions%20can%20be%20obtained%20in%20each%20patch%2C%20they%20are%20then%20matched%20at%20the%20interfaces%20to%20construct%20a%20global%20solution.%20However%2C%20this%20patching%20procedure%20can%20fail%20to%20reproduce%20the%20correct%20solution%2C%20since%20the%20approximate%20forms%20may%20break%20down%20near%20the%20matching%20boundaries.%20In%20this%20work%2C%20we%20propose%20a%20learning%20framework%20in%20which%20the%20integration%20constants%20of%20asymptotic%20analytic%20solutions%20are%20promoted%20to%20scale-dependent%20functions.%20By%20constraining%20these%20coefficient%20functions%20with%20the%20original%20differential%20equation%20over%20the%20domain%2C%20the%20network%20learns%20a%20globally%20valid%20solution%20that%20smoothly%20interpolates%20between%20asymptotic%20regimes%2C%20eliminating%20the%20need%20for%20arbitrary%20boundary%20matching.%20We%20demonstrate%20the%20effectiveness%20of%20this%20framework%20in%20representative%20problems%20from%20chemical%20kinetics%20and%20cosmology%2C%20where%20it%20accurately%20reproduces%20global%20solutions%20and%20outperforms%20conventional%20matching%20procedures.&entry.1838667208=http%3A//arxiv.org/abs/2601.05889v1&entry.124074799=Read"},
{"title": "Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection", "author": "Yong Xie and Karan Aggarwal and Aitzaz Ahmad and Stephen Lau", "abstract": "We present a novel approach to automatically generate non-trivial task-specific synthetic datasets for hallucination detection. Our approach features a two-step generation-selection pipeline, using hallucination pattern guidance and a language style alignment during generation. Hallucination pattern guidance leverages the most important task-specific hallucination patterns while language style alignment aligns the style of the synthetic dataset with benchmark text. To obtain robust supervised detectors from synthetic datasets, we also adopt a data mixture strategy to improve performance robustness and generalization. Our results on three datasets show that our generated hallucination text is more closely aligned with non-hallucinated text versus baselines, to train hallucination detectors with better generalization. Our hallucination detectors trained on synthetic datasets outperform in-context-learning (ICL)-based detectors by a large margin of 32%. Our extensive experiments confirm the benefits of our approach with cross-task and cross-generator generalization. Our data-mixture-based training further improves the generalization and robustness of hallucination detection.", "link": "http://arxiv.org/abs/2410.12278v2", "date": "2026-01-09", "relevancy": 2.5435, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.523}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5064}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlled%20Automatic%20Task-Specific%20Synthetic%20Data%20Generation%20for%20Hallucination%20Detection&body=Title%3A%20Controlled%20Automatic%20Task-Specific%20Synthetic%20Data%20Generation%20for%20Hallucination%20Detection%0AAuthor%3A%20Yong%20Xie%20and%20Karan%20Aggarwal%20and%20Aitzaz%20Ahmad%20and%20Stephen%20Lau%0AAbstract%3A%20We%20present%20a%20novel%20approach%20to%20automatically%20generate%20non-trivial%20task-specific%20synthetic%20datasets%20for%20hallucination%20detection.%20Our%20approach%20features%20a%20two-step%20generation-selection%20pipeline%2C%20using%20hallucination%20pattern%20guidance%20and%20a%20language%20style%20alignment%20during%20generation.%20Hallucination%20pattern%20guidance%20leverages%20the%20most%20important%20task-specific%20hallucination%20patterns%20while%20language%20style%20alignment%20aligns%20the%20style%20of%20the%20synthetic%20dataset%20with%20benchmark%20text.%20To%20obtain%20robust%20supervised%20detectors%20from%20synthetic%20datasets%2C%20we%20also%20adopt%20a%20data%20mixture%20strategy%20to%20improve%20performance%20robustness%20and%20generalization.%20Our%20results%20on%20three%20datasets%20show%20that%20our%20generated%20hallucination%20text%20is%20more%20closely%20aligned%20with%20non-hallucinated%20text%20versus%20baselines%2C%20to%20train%20hallucination%20detectors%20with%20better%20generalization.%20Our%20hallucination%20detectors%20trained%20on%20synthetic%20datasets%20outperform%20in-context-learning%20%28ICL%29-based%20detectors%20by%20a%20large%20margin%20of%2032%25.%20Our%20extensive%20experiments%20confirm%20the%20benefits%20of%20our%20approach%20with%20cross-task%20and%20cross-generator%20generalization.%20Our%20data-mixture-based%20training%20further%20improves%20the%20generalization%20and%20robustness%20of%20hallucination%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2410.12278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlled%2520Automatic%2520Task-Specific%2520Synthetic%2520Data%2520Generation%2520for%2520Hallucination%2520Detection%26entry.906535625%3DYong%2520Xie%2520and%2520Karan%2520Aggarwal%2520and%2520Aitzaz%2520Ahmad%2520and%2520Stephen%2520Lau%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520approach%2520to%2520automatically%2520generate%2520non-trivial%2520task-specific%2520synthetic%2520datasets%2520for%2520hallucination%2520detection.%2520Our%2520approach%2520features%2520a%2520two-step%2520generation-selection%2520pipeline%252C%2520using%2520hallucination%2520pattern%2520guidance%2520and%2520a%2520language%2520style%2520alignment%2520during%2520generation.%2520Hallucination%2520pattern%2520guidance%2520leverages%2520the%2520most%2520important%2520task-specific%2520hallucination%2520patterns%2520while%2520language%2520style%2520alignment%2520aligns%2520the%2520style%2520of%2520the%2520synthetic%2520dataset%2520with%2520benchmark%2520text.%2520To%2520obtain%2520robust%2520supervised%2520detectors%2520from%2520synthetic%2520datasets%252C%2520we%2520also%2520adopt%2520a%2520data%2520mixture%2520strategy%2520to%2520improve%2520performance%2520robustness%2520and%2520generalization.%2520Our%2520results%2520on%2520three%2520datasets%2520show%2520that%2520our%2520generated%2520hallucination%2520text%2520is%2520more%2520closely%2520aligned%2520with%2520non-hallucinated%2520text%2520versus%2520baselines%252C%2520to%2520train%2520hallucination%2520detectors%2520with%2520better%2520generalization.%2520Our%2520hallucination%2520detectors%2520trained%2520on%2520synthetic%2520datasets%2520outperform%2520in-context-learning%2520%2528ICL%2529-based%2520detectors%2520by%2520a%2520large%2520margin%2520of%252032%2525.%2520Our%2520extensive%2520experiments%2520confirm%2520the%2520benefits%2520of%2520our%2520approach%2520with%2520cross-task%2520and%2520cross-generator%2520generalization.%2520Our%2520data-mixture-based%2520training%2520further%2520improves%2520the%2520generalization%2520and%2520robustness%2520of%2520hallucination%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlled%20Automatic%20Task-Specific%20Synthetic%20Data%20Generation%20for%20Hallucination%20Detection&entry.906535625=Yong%20Xie%20and%20Karan%20Aggarwal%20and%20Aitzaz%20Ahmad%20and%20Stephen%20Lau&entry.1292438233=We%20present%20a%20novel%20approach%20to%20automatically%20generate%20non-trivial%20task-specific%20synthetic%20datasets%20for%20hallucination%20detection.%20Our%20approach%20features%20a%20two-step%20generation-selection%20pipeline%2C%20using%20hallucination%20pattern%20guidance%20and%20a%20language%20style%20alignment%20during%20generation.%20Hallucination%20pattern%20guidance%20leverages%20the%20most%20important%20task-specific%20hallucination%20patterns%20while%20language%20style%20alignment%20aligns%20the%20style%20of%20the%20synthetic%20dataset%20with%20benchmark%20text.%20To%20obtain%20robust%20supervised%20detectors%20from%20synthetic%20datasets%2C%20we%20also%20adopt%20a%20data%20mixture%20strategy%20to%20improve%20performance%20robustness%20and%20generalization.%20Our%20results%20on%20three%20datasets%20show%20that%20our%20generated%20hallucination%20text%20is%20more%20closely%20aligned%20with%20non-hallucinated%20text%20versus%20baselines%2C%20to%20train%20hallucination%20detectors%20with%20better%20generalization.%20Our%20hallucination%20detectors%20trained%20on%20synthetic%20datasets%20outperform%20in-context-learning%20%28ICL%29-based%20detectors%20by%20a%20large%20margin%20of%2032%25.%20Our%20extensive%20experiments%20confirm%20the%20benefits%20of%20our%20approach%20with%20cross-task%20and%20cross-generator%20generalization.%20Our%20data-mixture-based%20training%20further%20improves%20the%20generalization%20and%20robustness%20of%20hallucination%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2410.12278v2&entry.124074799=Read"},
{"title": "PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility", "author": "G M Shahariar and Zabir Al Nazi and Md Olid Hasan Bhuiyan and Zhouxing Shi", "abstract": "Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.", "link": "http://arxiv.org/abs/2601.05739v1", "date": "2026-01-09", "relevancy": 2.5392, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PII-VisBench%3A%20Evaluating%20Personally%20Identifiable%20Information%20Safety%20in%20Vision%20Language%20Models%20Along%20a%20Continuum%20of%20Visibility&body=Title%3A%20PII-VisBench%3A%20Evaluating%20Personally%20Identifiable%20Information%20Safety%20in%20Vision%20Language%20Models%20Along%20a%20Continuum%20of%20Visibility%0AAuthor%3A%20G%20M%20Shahariar%20and%20Zabir%20Al%20Nazi%20and%20Md%20Olid%20Hasan%20Bhuiyan%20and%20Zhouxing%20Shi%0AAbstract%3A%20Vision%20Language%20Models%20%28VLMs%29%20are%20increasingly%20integrated%20into%20privacy-critical%20domains%2C%20yet%20existing%20evaluations%20of%20personally%20identifiable%20information%20%28PII%29%20leakage%20largely%20treat%20privacy%20as%20a%20static%20extraction%20task%20and%20ignore%20how%20a%20subject%27s%20online%20presence--the%20volume%20of%20their%20data%20available%20online--influences%20privacy%20alignment.%20We%20introduce%20PII-VisBench%2C%20a%20novel%20benchmark%20containing%204000%20unique%20probes%20designed%20to%20evaluate%20VLM%20safety%20through%20the%20continuum%20of%20online%20presence.%20The%20benchmark%20stratifies%20200%20subjects%20into%20four%20visibility%20categories%3A%20high%2C%20medium%2C%20low%2C%20and%20zero--based%20on%20the%20extent%20and%20nature%20of%20their%20information%20available%20online.%20We%20evaluate%2018%20open-source%20VLMs%20%280.3B-32B%29%20based%20on%20two%20key%20metrics%3A%20percentage%20of%20PII%20probing%20queries%20refused%20%28Refusal%20Rate%29%20and%20the%20fraction%20of%20non-refusal%20responses%20flagged%20for%20containing%20PII%20%28Conditional%20PII%20Disclosure%20Rate%29.%20Across%20models%2C%20we%20observe%20a%20consistent%20pattern%3A%20refusals%20increase%20and%20PII%20disclosures%20decrease%20%289.10%25%20high%20to%205.34%25%20low%29%20as%20subject%20visibility%20drops.%20We%20identify%20that%20models%20are%20more%20likely%20to%20disclose%20PII%20for%20high-visibility%20subjects%2C%20alongside%20substantial%20model-family%20heterogeneity%20and%20PII-type%20disparities.%20Finally%2C%20paraphrasing%20and%20jailbreak-style%20prompts%20expose%20attack%20and%20model-dependent%20failures%2C%20motivating%20visibility-aware%20safety%20evaluation%20and%20training%20interventions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPII-VisBench%253A%2520Evaluating%2520Personally%2520Identifiable%2520Information%2520Safety%2520in%2520Vision%2520Language%2520Models%2520Along%2520a%2520Continuum%2520of%2520Visibility%26entry.906535625%3DG%2520M%2520Shahariar%2520and%2520Zabir%2520Al%2520Nazi%2520and%2520Md%2520Olid%2520Hasan%2520Bhuiyan%2520and%2520Zhouxing%2520Shi%26entry.1292438233%3DVision%2520Language%2520Models%2520%2528VLMs%2529%2520are%2520increasingly%2520integrated%2520into%2520privacy-critical%2520domains%252C%2520yet%2520existing%2520evaluations%2520of%2520personally%2520identifiable%2520information%2520%2528PII%2529%2520leakage%2520largely%2520treat%2520privacy%2520as%2520a%2520static%2520extraction%2520task%2520and%2520ignore%2520how%2520a%2520subject%2527s%2520online%2520presence--the%2520volume%2520of%2520their%2520data%2520available%2520online--influences%2520privacy%2520alignment.%2520We%2520introduce%2520PII-VisBench%252C%2520a%2520novel%2520benchmark%2520containing%25204000%2520unique%2520probes%2520designed%2520to%2520evaluate%2520VLM%2520safety%2520through%2520the%2520continuum%2520of%2520online%2520presence.%2520The%2520benchmark%2520stratifies%2520200%2520subjects%2520into%2520four%2520visibility%2520categories%253A%2520high%252C%2520medium%252C%2520low%252C%2520and%2520zero--based%2520on%2520the%2520extent%2520and%2520nature%2520of%2520their%2520information%2520available%2520online.%2520We%2520evaluate%252018%2520open-source%2520VLMs%2520%25280.3B-32B%2529%2520based%2520on%2520two%2520key%2520metrics%253A%2520percentage%2520of%2520PII%2520probing%2520queries%2520refused%2520%2528Refusal%2520Rate%2529%2520and%2520the%2520fraction%2520of%2520non-refusal%2520responses%2520flagged%2520for%2520containing%2520PII%2520%2528Conditional%2520PII%2520Disclosure%2520Rate%2529.%2520Across%2520models%252C%2520we%2520observe%2520a%2520consistent%2520pattern%253A%2520refusals%2520increase%2520and%2520PII%2520disclosures%2520decrease%2520%25289.10%2525%2520high%2520to%25205.34%2525%2520low%2529%2520as%2520subject%2520visibility%2520drops.%2520We%2520identify%2520that%2520models%2520are%2520more%2520likely%2520to%2520disclose%2520PII%2520for%2520high-visibility%2520subjects%252C%2520alongside%2520substantial%2520model-family%2520heterogeneity%2520and%2520PII-type%2520disparities.%2520Finally%252C%2520paraphrasing%2520and%2520jailbreak-style%2520prompts%2520expose%2520attack%2520and%2520model-dependent%2520failures%252C%2520motivating%2520visibility-aware%2520safety%2520evaluation%2520and%2520training%2520interventions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PII-VisBench%3A%20Evaluating%20Personally%20Identifiable%20Information%20Safety%20in%20Vision%20Language%20Models%20Along%20a%20Continuum%20of%20Visibility&entry.906535625=G%20M%20Shahariar%20and%20Zabir%20Al%20Nazi%20and%20Md%20Olid%20Hasan%20Bhuiyan%20and%20Zhouxing%20Shi&entry.1292438233=Vision%20Language%20Models%20%28VLMs%29%20are%20increasingly%20integrated%20into%20privacy-critical%20domains%2C%20yet%20existing%20evaluations%20of%20personally%20identifiable%20information%20%28PII%29%20leakage%20largely%20treat%20privacy%20as%20a%20static%20extraction%20task%20and%20ignore%20how%20a%20subject%27s%20online%20presence--the%20volume%20of%20their%20data%20available%20online--influences%20privacy%20alignment.%20We%20introduce%20PII-VisBench%2C%20a%20novel%20benchmark%20containing%204000%20unique%20probes%20designed%20to%20evaluate%20VLM%20safety%20through%20the%20continuum%20of%20online%20presence.%20The%20benchmark%20stratifies%20200%20subjects%20into%20four%20visibility%20categories%3A%20high%2C%20medium%2C%20low%2C%20and%20zero--based%20on%20the%20extent%20and%20nature%20of%20their%20information%20available%20online.%20We%20evaluate%2018%20open-source%20VLMs%20%280.3B-32B%29%20based%20on%20two%20key%20metrics%3A%20percentage%20of%20PII%20probing%20queries%20refused%20%28Refusal%20Rate%29%20and%20the%20fraction%20of%20non-refusal%20responses%20flagged%20for%20containing%20PII%20%28Conditional%20PII%20Disclosure%20Rate%29.%20Across%20models%2C%20we%20observe%20a%20consistent%20pattern%3A%20refusals%20increase%20and%20PII%20disclosures%20decrease%20%289.10%25%20high%20to%205.34%25%20low%29%20as%20subject%20visibility%20drops.%20We%20identify%20that%20models%20are%20more%20likely%20to%20disclose%20PII%20for%20high-visibility%20subjects%2C%20alongside%20substantial%20model-family%20heterogeneity%20and%20PII-type%20disparities.%20Finally%2C%20paraphrasing%20and%20jailbreak-style%20prompts%20expose%20attack%20and%20model-dependent%20failures%2C%20motivating%20visibility-aware%20safety%20evaluation%20and%20training%20interventions.&entry.1838667208=http%3A//arxiv.org/abs/2601.05739v1&entry.124074799=Read"},
{"title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals", "author": "Nate Gillman and Yinghua Zhou and Zitian Tang and Evan Luo and Arjan Chakravarthy and Daksh Aggarwal and Michael Freeman and Charles Herrmann and Chen Sun", "abstract": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.", "link": "http://arxiv.org/abs/2601.05848v1", "date": "2026-01-09", "relevancy": 2.5347, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7086}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goal%20Force%3A%20Teaching%20Video%20Models%20To%20Accomplish%20Physics-Conditioned%20Goals&body=Title%3A%20Goal%20Force%3A%20Teaching%20Video%20Models%20To%20Accomplish%20Physics-Conditioned%20Goals%0AAuthor%3A%20Nate%20Gillman%20and%20Yinghua%20Zhou%20and%20Zitian%20Tang%20and%20Evan%20Luo%20and%20Arjan%20Chakravarthy%20and%20Daksh%20Aggarwal%20and%20Michael%20Freeman%20and%20Charles%20Herrmann%20and%20Chen%20Sun%0AAbstract%3A%20Recent%20advancements%20in%20video%20generation%20have%20enabled%20the%20development%20of%20%60%60world%20models%27%27%20capable%20of%20simulating%20potential%20futures%20for%20robotics%20and%20planning.%20However%2C%20specifying%20precise%20goals%20for%20these%20models%20remains%20a%20challenge%3B%20text%20instructions%20are%20often%20too%20abstract%20to%20capture%20physical%20nuances%2C%20while%20target%20images%20are%20frequently%20infeasible%20to%20specify%20for%20dynamic%20tasks.%20To%20address%20this%2C%20we%20introduce%20Goal%20Force%2C%20a%20novel%20framework%20that%20allows%20users%20to%20define%20goals%20via%20explicit%20force%20vectors%20and%20intermediate%20dynamics%2C%20mirroring%20how%20humans%20conceptualize%20physical%20tasks.%20We%20train%20a%20video%20generation%20model%20on%20a%20curated%20dataset%20of%20synthetic%20causal%20primitives-such%20as%20elastic%20collisions%20and%20falling%20dominos-teaching%20it%20to%20propagate%20forces%20through%20time%20and%20space.%20Despite%20being%20trained%20on%20simple%20physics%20data%2C%20our%20model%20exhibits%20remarkable%20zero-shot%20generalization%20to%20complex%2C%20real-world%20scenarios%2C%20including%20tool%20manipulation%20and%20multi-object%20causal%20chains.%20Our%20results%20suggest%20that%20by%20grounding%20video%20generation%20in%20fundamental%20physical%20interactions%2C%20models%20can%20emerge%20as%20implicit%20neural%20physics%20simulators%2C%20enabling%20precise%2C%20physics-aware%20planning%20without%20reliance%20on%20external%20engines.%20We%20release%20all%20datasets%2C%20code%2C%20model%20weights%2C%20and%20interactive%20video%20demos%20at%20our%20project%20page.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoal%2520Force%253A%2520Teaching%2520Video%2520Models%2520To%2520Accomplish%2520Physics-Conditioned%2520Goals%26entry.906535625%3DNate%2520Gillman%2520and%2520Yinghua%2520Zhou%2520and%2520Zitian%2520Tang%2520and%2520Evan%2520Luo%2520and%2520Arjan%2520Chakravarthy%2520and%2520Daksh%2520Aggarwal%2520and%2520Michael%2520Freeman%2520and%2520Charles%2520Herrmann%2520and%2520Chen%2520Sun%26entry.1292438233%3DRecent%2520advancements%2520in%2520video%2520generation%2520have%2520enabled%2520the%2520development%2520of%2520%2560%2560world%2520models%2527%2527%2520capable%2520of%2520simulating%2520potential%2520futures%2520for%2520robotics%2520and%2520planning.%2520However%252C%2520specifying%2520precise%2520goals%2520for%2520these%2520models%2520remains%2520a%2520challenge%253B%2520text%2520instructions%2520are%2520often%2520too%2520abstract%2520to%2520capture%2520physical%2520nuances%252C%2520while%2520target%2520images%2520are%2520frequently%2520infeasible%2520to%2520specify%2520for%2520dynamic%2520tasks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Goal%2520Force%252C%2520a%2520novel%2520framework%2520that%2520allows%2520users%2520to%2520define%2520goals%2520via%2520explicit%2520force%2520vectors%2520and%2520intermediate%2520dynamics%252C%2520mirroring%2520how%2520humans%2520conceptualize%2520physical%2520tasks.%2520We%2520train%2520a%2520video%2520generation%2520model%2520on%2520a%2520curated%2520dataset%2520of%2520synthetic%2520causal%2520primitives-such%2520as%2520elastic%2520collisions%2520and%2520falling%2520dominos-teaching%2520it%2520to%2520propagate%2520forces%2520through%2520time%2520and%2520space.%2520Despite%2520being%2520trained%2520on%2520simple%2520physics%2520data%252C%2520our%2520model%2520exhibits%2520remarkable%2520zero-shot%2520generalization%2520to%2520complex%252C%2520real-world%2520scenarios%252C%2520including%2520tool%2520manipulation%2520and%2520multi-object%2520causal%2520chains.%2520Our%2520results%2520suggest%2520that%2520by%2520grounding%2520video%2520generation%2520in%2520fundamental%2520physical%2520interactions%252C%2520models%2520can%2520emerge%2520as%2520implicit%2520neural%2520physics%2520simulators%252C%2520enabling%2520precise%252C%2520physics-aware%2520planning%2520without%2520reliance%2520on%2520external%2520engines.%2520We%2520release%2520all%2520datasets%252C%2520code%252C%2520model%2520weights%252C%2520and%2520interactive%2520video%2520demos%2520at%2520our%2520project%2520page.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal%20Force%3A%20Teaching%20Video%20Models%20To%20Accomplish%20Physics-Conditioned%20Goals&entry.906535625=Nate%20Gillman%20and%20Yinghua%20Zhou%20and%20Zitian%20Tang%20and%20Evan%20Luo%20and%20Arjan%20Chakravarthy%20and%20Daksh%20Aggarwal%20and%20Michael%20Freeman%20and%20Charles%20Herrmann%20and%20Chen%20Sun&entry.1292438233=Recent%20advancements%20in%20video%20generation%20have%20enabled%20the%20development%20of%20%60%60world%20models%27%27%20capable%20of%20simulating%20potential%20futures%20for%20robotics%20and%20planning.%20However%2C%20specifying%20precise%20goals%20for%20these%20models%20remains%20a%20challenge%3B%20text%20instructions%20are%20often%20too%20abstract%20to%20capture%20physical%20nuances%2C%20while%20target%20images%20are%20frequently%20infeasible%20to%20specify%20for%20dynamic%20tasks.%20To%20address%20this%2C%20we%20introduce%20Goal%20Force%2C%20a%20novel%20framework%20that%20allows%20users%20to%20define%20goals%20via%20explicit%20force%20vectors%20and%20intermediate%20dynamics%2C%20mirroring%20how%20humans%20conceptualize%20physical%20tasks.%20We%20train%20a%20video%20generation%20model%20on%20a%20curated%20dataset%20of%20synthetic%20causal%20primitives-such%20as%20elastic%20collisions%20and%20falling%20dominos-teaching%20it%20to%20propagate%20forces%20through%20time%20and%20space.%20Despite%20being%20trained%20on%20simple%20physics%20data%2C%20our%20model%20exhibits%20remarkable%20zero-shot%20generalization%20to%20complex%2C%20real-world%20scenarios%2C%20including%20tool%20manipulation%20and%20multi-object%20causal%20chains.%20Our%20results%20suggest%20that%20by%20grounding%20video%20generation%20in%20fundamental%20physical%20interactions%2C%20models%20can%20emerge%20as%20implicit%20neural%20physics%20simulators%2C%20enabling%20precise%2C%20physics-aware%20planning%20without%20reliance%20on%20external%20engines.%20We%20release%20all%20datasets%2C%20code%2C%20model%20weights%2C%20and%20interactive%20video%20demos%20at%20our%20project%20page.&entry.1838667208=http%3A//arxiv.org/abs/2601.05848v1&entry.124074799=Read"},
{"title": "Simple Mechanisms for Representing, Indexing and Manipulating Concepts", "author": "Yuanzhi Li and Raghu Meka and Rina Panigrahy and Kulin Shah", "abstract": "Supervised and unsupervised learning using deep neural networks typically aims to exploit the underlying structure in the training data; this structure is often explained using a latent generative process that produces the data, and the generative process is often hierarchical, involving latent concepts. Despite the significant work on understanding the learning of the latent structure and underlying concepts using theory and experiments, a framework that mathematically captures the definition of a concept and provides ways to operate on concepts is missing. In this work, we propose to characterize a simple primitive concept by the zero set of a collection of polynomials and use moment statistics of the data to uniquely represent the concepts; we show how this view can be used to obtain a signature of the concept. These signatures can be used to discover a common structure across the set of concepts and could recursively produce the signature of higher-level concepts from the signatures of lower-level concepts. To utilize such desired properties, we propose a method by keeping a dictionary of concepts and show that the proposed method can learn different types of hierarchical structures of the data.", "link": "http://arxiv.org/abs/2310.12143v2", "date": "2026-01-09", "relevancy": 2.5244, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5354}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Mechanisms%20for%20Representing%2C%20Indexing%20and%20Manipulating%20Concepts&body=Title%3A%20Simple%20Mechanisms%20for%20Representing%2C%20Indexing%20and%20Manipulating%20Concepts%0AAuthor%3A%20Yuanzhi%20Li%20and%20Raghu%20Meka%20and%20Rina%20Panigrahy%20and%20Kulin%20Shah%0AAbstract%3A%20Supervised%20and%20unsupervised%20learning%20using%20deep%20neural%20networks%20typically%20aims%20to%20exploit%20the%20underlying%20structure%20in%20the%20training%20data%3B%20this%20structure%20is%20often%20explained%20using%20a%20latent%20generative%20process%20that%20produces%20the%20data%2C%20and%20the%20generative%20process%20is%20often%20hierarchical%2C%20involving%20latent%20concepts.%20Despite%20the%20significant%20work%20on%20understanding%20the%20learning%20of%20the%20latent%20structure%20and%20underlying%20concepts%20using%20theory%20and%20experiments%2C%20a%20framework%20that%20mathematically%20captures%20the%20definition%20of%20a%20concept%20and%20provides%20ways%20to%20operate%20on%20concepts%20is%20missing.%20In%20this%20work%2C%20we%20propose%20to%20characterize%20a%20simple%20primitive%20concept%20by%20the%20zero%20set%20of%20a%20collection%20of%20polynomials%20and%20use%20moment%20statistics%20of%20the%20data%20to%20uniquely%20represent%20the%20concepts%3B%20we%20show%20how%20this%20view%20can%20be%20used%20to%20obtain%20a%20signature%20of%20the%20concept.%20These%20signatures%20can%20be%20used%20to%20discover%20a%20common%20structure%20across%20the%20set%20of%20concepts%20and%20could%20recursively%20produce%20the%20signature%20of%20higher-level%20concepts%20from%20the%20signatures%20of%20lower-level%20concepts.%20To%20utilize%20such%20desired%20properties%2C%20we%20propose%20a%20method%20by%20keeping%20a%20dictionary%20of%20concepts%20and%20show%20that%20the%20proposed%20method%20can%20learn%20different%20types%20of%20hierarchical%20structures%20of%20the%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2310.12143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Mechanisms%2520for%2520Representing%252C%2520Indexing%2520and%2520Manipulating%2520Concepts%26entry.906535625%3DYuanzhi%2520Li%2520and%2520Raghu%2520Meka%2520and%2520Rina%2520Panigrahy%2520and%2520Kulin%2520Shah%26entry.1292438233%3DSupervised%2520and%2520unsupervised%2520learning%2520using%2520deep%2520neural%2520networks%2520typically%2520aims%2520to%2520exploit%2520the%2520underlying%2520structure%2520in%2520the%2520training%2520data%253B%2520this%2520structure%2520is%2520often%2520explained%2520using%2520a%2520latent%2520generative%2520process%2520that%2520produces%2520the%2520data%252C%2520and%2520the%2520generative%2520process%2520is%2520often%2520hierarchical%252C%2520involving%2520latent%2520concepts.%2520Despite%2520the%2520significant%2520work%2520on%2520understanding%2520the%2520learning%2520of%2520the%2520latent%2520structure%2520and%2520underlying%2520concepts%2520using%2520theory%2520and%2520experiments%252C%2520a%2520framework%2520that%2520mathematically%2520captures%2520the%2520definition%2520of%2520a%2520concept%2520and%2520provides%2520ways%2520to%2520operate%2520on%2520concepts%2520is%2520missing.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520characterize%2520a%2520simple%2520primitive%2520concept%2520by%2520the%2520zero%2520set%2520of%2520a%2520collection%2520of%2520polynomials%2520and%2520use%2520moment%2520statistics%2520of%2520the%2520data%2520to%2520uniquely%2520represent%2520the%2520concepts%253B%2520we%2520show%2520how%2520this%2520view%2520can%2520be%2520used%2520to%2520obtain%2520a%2520signature%2520of%2520the%2520concept.%2520These%2520signatures%2520can%2520be%2520used%2520to%2520discover%2520a%2520common%2520structure%2520across%2520the%2520set%2520of%2520concepts%2520and%2520could%2520recursively%2520produce%2520the%2520signature%2520of%2520higher-level%2520concepts%2520from%2520the%2520signatures%2520of%2520lower-level%2520concepts.%2520To%2520utilize%2520such%2520desired%2520properties%252C%2520we%2520propose%2520a%2520method%2520by%2520keeping%2520a%2520dictionary%2520of%2520concepts%2520and%2520show%2520that%2520the%2520proposed%2520method%2520can%2520learn%2520different%2520types%2520of%2520hierarchical%2520structures%2520of%2520the%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Mechanisms%20for%20Representing%2C%20Indexing%20and%20Manipulating%20Concepts&entry.906535625=Yuanzhi%20Li%20and%20Raghu%20Meka%20and%20Rina%20Panigrahy%20and%20Kulin%20Shah&entry.1292438233=Supervised%20and%20unsupervised%20learning%20using%20deep%20neural%20networks%20typically%20aims%20to%20exploit%20the%20underlying%20structure%20in%20the%20training%20data%3B%20this%20structure%20is%20often%20explained%20using%20a%20latent%20generative%20process%20that%20produces%20the%20data%2C%20and%20the%20generative%20process%20is%20often%20hierarchical%2C%20involving%20latent%20concepts.%20Despite%20the%20significant%20work%20on%20understanding%20the%20learning%20of%20the%20latent%20structure%20and%20underlying%20concepts%20using%20theory%20and%20experiments%2C%20a%20framework%20that%20mathematically%20captures%20the%20definition%20of%20a%20concept%20and%20provides%20ways%20to%20operate%20on%20concepts%20is%20missing.%20In%20this%20work%2C%20we%20propose%20to%20characterize%20a%20simple%20primitive%20concept%20by%20the%20zero%20set%20of%20a%20collection%20of%20polynomials%20and%20use%20moment%20statistics%20of%20the%20data%20to%20uniquely%20represent%20the%20concepts%3B%20we%20show%20how%20this%20view%20can%20be%20used%20to%20obtain%20a%20signature%20of%20the%20concept.%20These%20signatures%20can%20be%20used%20to%20discover%20a%20common%20structure%20across%20the%20set%20of%20concepts%20and%20could%20recursively%20produce%20the%20signature%20of%20higher-level%20concepts%20from%20the%20signatures%20of%20lower-level%20concepts.%20To%20utilize%20such%20desired%20properties%2C%20we%20propose%20a%20method%20by%20keeping%20a%20dictionary%20of%20concepts%20and%20show%20that%20the%20proposed%20method%20can%20learn%20different%20types%20of%20hierarchical%20structures%20of%20the%20data.&entry.1838667208=http%3A//arxiv.org/abs/2310.12143v2&entry.124074799=Read"},
{"title": "Learning Reconstructive Embeddings in Reproducing Kernel Hilbert Spaces via the Representer Theorem", "author": "Enrique Feito-Casares and Francisco M. Melgarejo-Meseguer and Jos\u00e9-Luis Rojo-\u00c1lvarez", "abstract": "Motivated by the growing interest in representation learning approaches that uncover the latent structure of high-dimensional data, this work proposes new algorithms for reconstruction-based manifold learning within Reproducing-Kernel Hilbert Spaces (RKHS). Each observation is first reconstructed as a linear combination of the other samples in the RKHS, by optimizing a vector form of the Representer Theorem for their autorepresentation property. A separable operator-valued kernel extends the formulation to vector-valued data while retaining the simplicity of a single scalar similarity function. A subsequent kernel-alignment task projects the data into a lower-dimensional latent space whose Gram matrix aims to match the high-dimensional reconstruction kernel, thus transferring the auto-reconstruction geometry of the RKHS to the embedding. Therefore, the proposed algorithms represent an extended approach to the autorepresentation property, exhibited by many natural data, by using and adapting well-known results of Kernel Learning Theory. Numerical experiments on both simulated (concentric circles and swiss-roll) and real (cancer molecular activity and IoT network intrusions) datasets provide empirical evidence of the practical effectiveness of the proposed approach.", "link": "http://arxiv.org/abs/2601.05811v1", "date": "2026-01-09", "relevancy": 2.4987, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5182}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5164}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Reconstructive%20Embeddings%20in%20Reproducing%20Kernel%20Hilbert%20Spaces%20via%20the%20Representer%20Theorem&body=Title%3A%20Learning%20Reconstructive%20Embeddings%20in%20Reproducing%20Kernel%20Hilbert%20Spaces%20via%20the%20Representer%20Theorem%0AAuthor%3A%20Enrique%20Feito-Casares%20and%20Francisco%20M.%20Melgarejo-Meseguer%20and%20Jos%C3%A9-Luis%20Rojo-%C3%81lvarez%0AAbstract%3A%20Motivated%20by%20the%20growing%20interest%20in%20representation%20learning%20approaches%20that%20uncover%20the%20latent%20structure%20of%20high-dimensional%20data%2C%20this%20work%20proposes%20new%20algorithms%20for%20reconstruction-based%20manifold%20learning%20within%20Reproducing-Kernel%20Hilbert%20Spaces%20%28RKHS%29.%20Each%20observation%20is%20first%20reconstructed%20as%20a%20linear%20combination%20of%20the%20other%20samples%20in%20the%20RKHS%2C%20by%20optimizing%20a%20vector%20form%20of%20the%20Representer%20Theorem%20for%20their%20autorepresentation%20property.%20A%20separable%20operator-valued%20kernel%20extends%20the%20formulation%20to%20vector-valued%20data%20while%20retaining%20the%20simplicity%20of%20a%20single%20scalar%20similarity%20function.%20A%20subsequent%20kernel-alignment%20task%20projects%20the%20data%20into%20a%20lower-dimensional%20latent%20space%20whose%20Gram%20matrix%20aims%20to%20match%20the%20high-dimensional%20reconstruction%20kernel%2C%20thus%20transferring%20the%20auto-reconstruction%20geometry%20of%20the%20RKHS%20to%20the%20embedding.%20Therefore%2C%20the%20proposed%20algorithms%20represent%20an%20extended%20approach%20to%20the%20autorepresentation%20property%2C%20exhibited%20by%20many%20natural%20data%2C%20by%20using%20and%20adapting%20well-known%20results%20of%20Kernel%20Learning%20Theory.%20Numerical%20experiments%20on%20both%20simulated%20%28concentric%20circles%20and%20swiss-roll%29%20and%20real%20%28cancer%20molecular%20activity%20and%20IoT%20network%20intrusions%29%20datasets%20provide%20empirical%20evidence%20of%20the%20practical%20effectiveness%20of%20the%20proposed%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Reconstructive%2520Embeddings%2520in%2520Reproducing%2520Kernel%2520Hilbert%2520Spaces%2520via%2520the%2520Representer%2520Theorem%26entry.906535625%3DEnrique%2520Feito-Casares%2520and%2520Francisco%2520M.%2520Melgarejo-Meseguer%2520and%2520Jos%25C3%25A9-Luis%2520Rojo-%25C3%2581lvarez%26entry.1292438233%3DMotivated%2520by%2520the%2520growing%2520interest%2520in%2520representation%2520learning%2520approaches%2520that%2520uncover%2520the%2520latent%2520structure%2520of%2520high-dimensional%2520data%252C%2520this%2520work%2520proposes%2520new%2520algorithms%2520for%2520reconstruction-based%2520manifold%2520learning%2520within%2520Reproducing-Kernel%2520Hilbert%2520Spaces%2520%2528RKHS%2529.%2520Each%2520observation%2520is%2520first%2520reconstructed%2520as%2520a%2520linear%2520combination%2520of%2520the%2520other%2520samples%2520in%2520the%2520RKHS%252C%2520by%2520optimizing%2520a%2520vector%2520form%2520of%2520the%2520Representer%2520Theorem%2520for%2520their%2520autorepresentation%2520property.%2520A%2520separable%2520operator-valued%2520kernel%2520extends%2520the%2520formulation%2520to%2520vector-valued%2520data%2520while%2520retaining%2520the%2520simplicity%2520of%2520a%2520single%2520scalar%2520similarity%2520function.%2520A%2520subsequent%2520kernel-alignment%2520task%2520projects%2520the%2520data%2520into%2520a%2520lower-dimensional%2520latent%2520space%2520whose%2520Gram%2520matrix%2520aims%2520to%2520match%2520the%2520high-dimensional%2520reconstruction%2520kernel%252C%2520thus%2520transferring%2520the%2520auto-reconstruction%2520geometry%2520of%2520the%2520RKHS%2520to%2520the%2520embedding.%2520Therefore%252C%2520the%2520proposed%2520algorithms%2520represent%2520an%2520extended%2520approach%2520to%2520the%2520autorepresentation%2520property%252C%2520exhibited%2520by%2520many%2520natural%2520data%252C%2520by%2520using%2520and%2520adapting%2520well-known%2520results%2520of%2520Kernel%2520Learning%2520Theory.%2520Numerical%2520experiments%2520on%2520both%2520simulated%2520%2528concentric%2520circles%2520and%2520swiss-roll%2529%2520and%2520real%2520%2528cancer%2520molecular%2520activity%2520and%2520IoT%2520network%2520intrusions%2529%2520datasets%2520provide%2520empirical%2520evidence%2520of%2520the%2520practical%2520effectiveness%2520of%2520the%2520proposed%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Reconstructive%20Embeddings%20in%20Reproducing%20Kernel%20Hilbert%20Spaces%20via%20the%20Representer%20Theorem&entry.906535625=Enrique%20Feito-Casares%20and%20Francisco%20M.%20Melgarejo-Meseguer%20and%20Jos%C3%A9-Luis%20Rojo-%C3%81lvarez&entry.1292438233=Motivated%20by%20the%20growing%20interest%20in%20representation%20learning%20approaches%20that%20uncover%20the%20latent%20structure%20of%20high-dimensional%20data%2C%20this%20work%20proposes%20new%20algorithms%20for%20reconstruction-based%20manifold%20learning%20within%20Reproducing-Kernel%20Hilbert%20Spaces%20%28RKHS%29.%20Each%20observation%20is%20first%20reconstructed%20as%20a%20linear%20combination%20of%20the%20other%20samples%20in%20the%20RKHS%2C%20by%20optimizing%20a%20vector%20form%20of%20the%20Representer%20Theorem%20for%20their%20autorepresentation%20property.%20A%20separable%20operator-valued%20kernel%20extends%20the%20formulation%20to%20vector-valued%20data%20while%20retaining%20the%20simplicity%20of%20a%20single%20scalar%20similarity%20function.%20A%20subsequent%20kernel-alignment%20task%20projects%20the%20data%20into%20a%20lower-dimensional%20latent%20space%20whose%20Gram%20matrix%20aims%20to%20match%20the%20high-dimensional%20reconstruction%20kernel%2C%20thus%20transferring%20the%20auto-reconstruction%20geometry%20of%20the%20RKHS%20to%20the%20embedding.%20Therefore%2C%20the%20proposed%20algorithms%20represent%20an%20extended%20approach%20to%20the%20autorepresentation%20property%2C%20exhibited%20by%20many%20natural%20data%2C%20by%20using%20and%20adapting%20well-known%20results%20of%20Kernel%20Learning%20Theory.%20Numerical%20experiments%20on%20both%20simulated%20%28concentric%20circles%20and%20swiss-roll%29%20and%20real%20%28cancer%20molecular%20activity%20and%20IoT%20network%20intrusions%29%20datasets%20provide%20empirical%20evidence%20of%20the%20practical%20effectiveness%20of%20the%20proposed%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2601.05811v1&entry.124074799=Read"},
{"title": "Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning", "author": "Manel Gil-Sorribes and J\u00falia Vilalta-Mor and Isaac Filella-Merc\u00e8 and Robert Soliva and \u00c1lvaro Ciudad and V\u00edctor Guallar and Alexis Molina", "abstract": "Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.", "link": "http://arxiv.org/abs/2601.05792v1", "date": "2026-01-09", "relevancy": 2.4924, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4944}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor-DTI%3A%20Enhancing%20Biomolecular%20Interaction%20Prediction%20with%20Contrastive%20Embedding%20Learning&body=Title%3A%20Tensor-DTI%3A%20Enhancing%20Biomolecular%20Interaction%20Prediction%20with%20Contrastive%20Embedding%20Learning%0AAuthor%3A%20Manel%20Gil-Sorribes%20and%20J%C3%BAlia%20Vilalta-Mor%20and%20Isaac%20Filella-Merc%C3%A8%20and%20Robert%20Soliva%20and%20%C3%81lvaro%20Ciudad%20and%20V%C3%ADctor%20Guallar%20and%20Alexis%20Molina%0AAbstract%3A%20Accurate%20drug-target%20interaction%20%28DTI%29%20prediction%20is%20essential%20for%20computational%20drug%20discovery%2C%20yet%20existing%20models%20often%20rely%20on%20single-modality%20predefined%20molecular%20descriptors%20or%20sequence-based%20embeddings%20with%20limited%20representativeness.%20We%20propose%20Tensor-DTI%2C%20a%20contrastive%20learning%20framework%20that%20integrates%20multimodal%20embeddings%20from%20molecular%20graphs%2C%20protein%20language%20models%2C%20and%20binding-site%20predictions%20to%20improve%20interaction%20modeling.%20Tensor-DTI%20employs%20a%20siamese%20dual-encoder%20architecture%2C%20enabling%20it%20to%20capture%20both%20chemical%20and%20structural%20interaction%20features%20while%20distinguishing%20interacting%20from%20non-interacting%20pairs.%20Evaluations%20on%20multiple%20DTI%20benchmarks%20demonstrate%20that%20Tensor-DTI%20outperforms%20existing%20sequence-based%20and%20graph-based%20models.%20We%20also%20conduct%20large-scale%20inference%20experiments%20on%20CDK2%20across%20billion-scale%20chemical%20libraries%2C%20where%20Tensor-DTI%20produces%20chemically%20plausible%20hit%20distributions%20even%20when%20CDK2%20is%20withheld%20from%20training.%20In%20enrichment%20studies%20against%20Glide%20docking%20and%20Boltz-2%20co-folder%2C%20Tensor-DTI%20remains%20competitive%20on%20CDK2%20and%20improves%20the%20screening%20budget%20required%20to%20recover%20moderate%20fractions%20of%20high-affinity%20ligands%20on%20out-of-family%20targets%20under%20strict%20family-holdout%20splits.%20Additionally%2C%20we%20explore%20its%20applicability%20to%20protein-RNA%20and%20peptide-protein%20interactions.%20Our%20findings%20highlight%20the%20benefits%20of%20integrating%20multimodal%20information%20with%20contrastive%20objectives%20to%20enhance%20interaction-prediction%20accuracy%20and%20to%20provide%20more%20interpretable%20and%20reliability-aware%20models%20for%20virtual%20screening.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor-DTI%253A%2520Enhancing%2520Biomolecular%2520Interaction%2520Prediction%2520with%2520Contrastive%2520Embedding%2520Learning%26entry.906535625%3DManel%2520Gil-Sorribes%2520and%2520J%25C3%25BAlia%2520Vilalta-Mor%2520and%2520Isaac%2520Filella-Merc%25C3%25A8%2520and%2520Robert%2520Soliva%2520and%2520%25C3%2581lvaro%2520Ciudad%2520and%2520V%25C3%25ADctor%2520Guallar%2520and%2520Alexis%2520Molina%26entry.1292438233%3DAccurate%2520drug-target%2520interaction%2520%2528DTI%2529%2520prediction%2520is%2520essential%2520for%2520computational%2520drug%2520discovery%252C%2520yet%2520existing%2520models%2520often%2520rely%2520on%2520single-modality%2520predefined%2520molecular%2520descriptors%2520or%2520sequence-based%2520embeddings%2520with%2520limited%2520representativeness.%2520We%2520propose%2520Tensor-DTI%252C%2520a%2520contrastive%2520learning%2520framework%2520that%2520integrates%2520multimodal%2520embeddings%2520from%2520molecular%2520graphs%252C%2520protein%2520language%2520models%252C%2520and%2520binding-site%2520predictions%2520to%2520improve%2520interaction%2520modeling.%2520Tensor-DTI%2520employs%2520a%2520siamese%2520dual-encoder%2520architecture%252C%2520enabling%2520it%2520to%2520capture%2520both%2520chemical%2520and%2520structural%2520interaction%2520features%2520while%2520distinguishing%2520interacting%2520from%2520non-interacting%2520pairs.%2520Evaluations%2520on%2520multiple%2520DTI%2520benchmarks%2520demonstrate%2520that%2520Tensor-DTI%2520outperforms%2520existing%2520sequence-based%2520and%2520graph-based%2520models.%2520We%2520also%2520conduct%2520large-scale%2520inference%2520experiments%2520on%2520CDK2%2520across%2520billion-scale%2520chemical%2520libraries%252C%2520where%2520Tensor-DTI%2520produces%2520chemically%2520plausible%2520hit%2520distributions%2520even%2520when%2520CDK2%2520is%2520withheld%2520from%2520training.%2520In%2520enrichment%2520studies%2520against%2520Glide%2520docking%2520and%2520Boltz-2%2520co-folder%252C%2520Tensor-DTI%2520remains%2520competitive%2520on%2520CDK2%2520and%2520improves%2520the%2520screening%2520budget%2520required%2520to%2520recover%2520moderate%2520fractions%2520of%2520high-affinity%2520ligands%2520on%2520out-of-family%2520targets%2520under%2520strict%2520family-holdout%2520splits.%2520Additionally%252C%2520we%2520explore%2520its%2520applicability%2520to%2520protein-RNA%2520and%2520peptide-protein%2520interactions.%2520Our%2520findings%2520highlight%2520the%2520benefits%2520of%2520integrating%2520multimodal%2520information%2520with%2520contrastive%2520objectives%2520to%2520enhance%2520interaction-prediction%2520accuracy%2520and%2520to%2520provide%2520more%2520interpretable%2520and%2520reliability-aware%2520models%2520for%2520virtual%2520screening.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor-DTI%3A%20Enhancing%20Biomolecular%20Interaction%20Prediction%20with%20Contrastive%20Embedding%20Learning&entry.906535625=Manel%20Gil-Sorribes%20and%20J%C3%BAlia%20Vilalta-Mor%20and%20Isaac%20Filella-Merc%C3%A8%20and%20Robert%20Soliva%20and%20%C3%81lvaro%20Ciudad%20and%20V%C3%ADctor%20Guallar%20and%20Alexis%20Molina&entry.1292438233=Accurate%20drug-target%20interaction%20%28DTI%29%20prediction%20is%20essential%20for%20computational%20drug%20discovery%2C%20yet%20existing%20models%20often%20rely%20on%20single-modality%20predefined%20molecular%20descriptors%20or%20sequence-based%20embeddings%20with%20limited%20representativeness.%20We%20propose%20Tensor-DTI%2C%20a%20contrastive%20learning%20framework%20that%20integrates%20multimodal%20embeddings%20from%20molecular%20graphs%2C%20protein%20language%20models%2C%20and%20binding-site%20predictions%20to%20improve%20interaction%20modeling.%20Tensor-DTI%20employs%20a%20siamese%20dual-encoder%20architecture%2C%20enabling%20it%20to%20capture%20both%20chemical%20and%20structural%20interaction%20features%20while%20distinguishing%20interacting%20from%20non-interacting%20pairs.%20Evaluations%20on%20multiple%20DTI%20benchmarks%20demonstrate%20that%20Tensor-DTI%20outperforms%20existing%20sequence-based%20and%20graph-based%20models.%20We%20also%20conduct%20large-scale%20inference%20experiments%20on%20CDK2%20across%20billion-scale%20chemical%20libraries%2C%20where%20Tensor-DTI%20produces%20chemically%20plausible%20hit%20distributions%20even%20when%20CDK2%20is%20withheld%20from%20training.%20In%20enrichment%20studies%20against%20Glide%20docking%20and%20Boltz-2%20co-folder%2C%20Tensor-DTI%20remains%20competitive%20on%20CDK2%20and%20improves%20the%20screening%20budget%20required%20to%20recover%20moderate%20fractions%20of%20high-affinity%20ligands%20on%20out-of-family%20targets%20under%20strict%20family-holdout%20splits.%20Additionally%2C%20we%20explore%20its%20applicability%20to%20protein-RNA%20and%20peptide-protein%20interactions.%20Our%20findings%20highlight%20the%20benefits%20of%20integrating%20multimodal%20information%20with%20contrastive%20objectives%20to%20enhance%20interaction-prediction%20accuracy%20and%20to%20provide%20more%20interpretable%20and%20reliability-aware%20models%20for%20virtual%20screening.&entry.1838667208=http%3A//arxiv.org/abs/2601.05792v1&entry.124074799=Read"},
{"title": "Open-Vocabulary 3D Instruction Ambiguity Detection", "author": "Jiayu Ding and Haoran Tang and Ge Li", "abstract": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.", "link": "http://arxiv.org/abs/2601.05991v1", "date": "2026-01-09", "relevancy": 2.4842, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%203D%20Instruction%20Ambiguity%20Detection&body=Title%3A%20Open-Vocabulary%203D%20Instruction%20Ambiguity%20Detection%0AAuthor%3A%20Jiayu%20Ding%20and%20Haoran%20Tang%20and%20Ge%20Li%0AAbstract%3A%20In%20safety-critical%20domains%2C%20linguistic%20ambiguity%20can%20have%20severe%20consequences%3B%20a%20vague%20command%20like%20%22Pass%20me%20the%20vial%22%20in%20a%20surgical%20setting%20could%20lead%20to%20catastrophic%20errors.%20Yet%2C%20most%20embodied%20AI%20research%20overlooks%20this%2C%20assuming%20instructions%20are%20clear%20and%20focusing%20on%20execution%20rather%20than%20confirmation.%20To%20address%20this%20critical%20safety%20gap%2C%20we%20are%20the%20first%20to%20define%20Open-Vocabulary%203D%20Instruction%20Ambiguity%20Detection%2C%20a%20fundamental%20new%20task%20where%20a%20model%20must%20determine%20if%20a%20command%20has%20a%20single%2C%20unambiguous%20meaning%20within%20a%20given%203D%20scene.%20To%20support%20this%20research%2C%20we%20build%20Ambi3D%2C%20the%20large-scale%20benchmark%20for%20this%20task%2C%20featuring%20over%20700%20diverse%203D%20scenes%20and%20around%2022k%20instructions.%20Our%20analysis%20reveals%20a%20surprising%20limitation%3A%20state-of-the-art%203D%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%20reliably%20determine%20if%20an%20instruction%20is%20ambiguous.%20To%20address%20this%20challenge%2C%20we%20propose%20AmbiVer%2C%20a%20two-stage%20framework%20that%20collects%20explicit%20visual%20evidence%20from%20multiple%20views%20and%20uses%20it%20to%20guide%20an%20vision-language%20model%20%28VLM%29%20in%20judging%20instruction%20ambiguity.%20Extensive%20experiments%20demonstrate%20the%20challenge%20of%20our%20task%20and%20the%20effectiveness%20of%20AmbiVer%2C%20paving%20the%20way%20for%20safer%20and%20more%20trustworthy%20embodied%20AI.%20Code%20and%20dataset%20available%20at%20https%3A//jiayuding031020.github.io/ambi3d/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%25203D%2520Instruction%2520Ambiguity%2520Detection%26entry.906535625%3DJiayu%2520Ding%2520and%2520Haoran%2520Tang%2520and%2520Ge%2520Li%26entry.1292438233%3DIn%2520safety-critical%2520domains%252C%2520linguistic%2520ambiguity%2520can%2520have%2520severe%2520consequences%253B%2520a%2520vague%2520command%2520like%2520%2522Pass%2520me%2520the%2520vial%2522%2520in%2520a%2520surgical%2520setting%2520could%2520lead%2520to%2520catastrophic%2520errors.%2520Yet%252C%2520most%2520embodied%2520AI%2520research%2520overlooks%2520this%252C%2520assuming%2520instructions%2520are%2520clear%2520and%2520focusing%2520on%2520execution%2520rather%2520than%2520confirmation.%2520To%2520address%2520this%2520critical%2520safety%2520gap%252C%2520we%2520are%2520the%2520first%2520to%2520define%2520Open-Vocabulary%25203D%2520Instruction%2520Ambiguity%2520Detection%252C%2520a%2520fundamental%2520new%2520task%2520where%2520a%2520model%2520must%2520determine%2520if%2520a%2520command%2520has%2520a%2520single%252C%2520unambiguous%2520meaning%2520within%2520a%2520given%25203D%2520scene.%2520To%2520support%2520this%2520research%252C%2520we%2520build%2520Ambi3D%252C%2520the%2520large-scale%2520benchmark%2520for%2520this%2520task%252C%2520featuring%2520over%2520700%2520diverse%25203D%2520scenes%2520and%2520around%252022k%2520instructions.%2520Our%2520analysis%2520reveals%2520a%2520surprising%2520limitation%253A%2520state-of-the-art%25203D%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%2520to%2520reliably%2520determine%2520if%2520an%2520instruction%2520is%2520ambiguous.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520AmbiVer%252C%2520a%2520two-stage%2520framework%2520that%2520collects%2520explicit%2520visual%2520evidence%2520from%2520multiple%2520views%2520and%2520uses%2520it%2520to%2520guide%2520an%2520vision-language%2520model%2520%2528VLM%2529%2520in%2520judging%2520instruction%2520ambiguity.%2520Extensive%2520experiments%2520demonstrate%2520the%2520challenge%2520of%2520our%2520task%2520and%2520the%2520effectiveness%2520of%2520AmbiVer%252C%2520paving%2520the%2520way%2520for%2520safer%2520and%2520more%2520trustworthy%2520embodied%2520AI.%2520Code%2520and%2520dataset%2520available%2520at%2520https%253A//jiayuding031020.github.io/ambi3d/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%203D%20Instruction%20Ambiguity%20Detection&entry.906535625=Jiayu%20Ding%20and%20Haoran%20Tang%20and%20Ge%20Li&entry.1292438233=In%20safety-critical%20domains%2C%20linguistic%20ambiguity%20can%20have%20severe%20consequences%3B%20a%20vague%20command%20like%20%22Pass%20me%20the%20vial%22%20in%20a%20surgical%20setting%20could%20lead%20to%20catastrophic%20errors.%20Yet%2C%20most%20embodied%20AI%20research%20overlooks%20this%2C%20assuming%20instructions%20are%20clear%20and%20focusing%20on%20execution%20rather%20than%20confirmation.%20To%20address%20this%20critical%20safety%20gap%2C%20we%20are%20the%20first%20to%20define%20Open-Vocabulary%203D%20Instruction%20Ambiguity%20Detection%2C%20a%20fundamental%20new%20task%20where%20a%20model%20must%20determine%20if%20a%20command%20has%20a%20single%2C%20unambiguous%20meaning%20within%20a%20given%203D%20scene.%20To%20support%20this%20research%2C%20we%20build%20Ambi3D%2C%20the%20large-scale%20benchmark%20for%20this%20task%2C%20featuring%20over%20700%20diverse%203D%20scenes%20and%20around%2022k%20instructions.%20Our%20analysis%20reveals%20a%20surprising%20limitation%3A%20state-of-the-art%203D%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%20reliably%20determine%20if%20an%20instruction%20is%20ambiguous.%20To%20address%20this%20challenge%2C%20we%20propose%20AmbiVer%2C%20a%20two-stage%20framework%20that%20collects%20explicit%20visual%20evidence%20from%20multiple%20views%20and%20uses%20it%20to%20guide%20an%20vision-language%20model%20%28VLM%29%20in%20judging%20instruction%20ambiguity.%20Extensive%20experiments%20demonstrate%20the%20challenge%20of%20our%20task%20and%20the%20effectiveness%20of%20AmbiVer%2C%20paving%20the%20way%20for%20safer%20and%20more%20trustworthy%20embodied%20AI.%20Code%20and%20dataset%20available%20at%20https%3A//jiayuding031020.github.io/ambi3d/.&entry.1838667208=http%3A//arxiv.org/abs/2601.05991v1&entry.124074799=Read"},
{"title": "CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks", "author": "Isaiah J. King and Bernardo Trindade and Benjamin Bowman and H. Howie Huang", "abstract": "Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.", "link": "http://arxiv.org/abs/2601.05988v1", "date": "2026-01-09", "relevancy": 2.4814, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4988}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4956}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CyberGFM%3A%20Graph%20Foundation%20Models%20for%20Lateral%20Movement%20Detection%20in%20Enterprise%20Networks&body=Title%3A%20CyberGFM%3A%20Graph%20Foundation%20Models%20for%20Lateral%20Movement%20Detection%20in%20Enterprise%20Networks%0AAuthor%3A%20Isaiah%20J.%20King%20and%20Bernardo%20Trindade%20and%20Benjamin%20Bowman%20and%20H.%20Howie%20Huang%0AAbstract%3A%20Representing%20networks%20as%20a%20graph%20and%20training%20a%20link%20prediction%20model%20using%20benign%20connections%20is%20an%20effective%20method%20of%20anomaly-based%20intrusion%20detection.%20Existing%20works%20using%20this%20technique%20have%20shown%20great%20success%20using%20temporal%20graph%20neural%20networks%20and%20skip-gram-based%20approaches%20on%20random%20walks.%20However%2C%20random%20walk-based%20approaches%20are%20unable%20to%20incorporate%20rich%20edge%20data%2C%20while%20the%20GNN-based%20approaches%20require%20large%20amounts%20of%20memory%20to%20train.%20In%20this%20work%2C%20we%20propose%20extending%20the%20original%20insight%20from%20random%20walk-based%20skip-grams--that%20random%20walks%20through%20a%20graph%20are%20analogous%20to%20sentences%20in%20a%20corpus--to%20the%20more%20modern%20transformer-based%20foundation%20models.%20Using%20language%20models%20that%20take%20advantage%20of%20GPU%20optimizations%2C%20we%20can%20quickly%20train%20a%20graph%20foundation%20model%20to%20predict%20missing%20tokens%20in%20random%20walks%20through%20a%20network%20of%20computers.%20The%20graph%20foundation%20model%20is%20then%20finetuned%20for%20link%20prediction%20and%20used%20as%20a%20network%20anomaly%20detector.%20This%20new%20approach%20allows%20us%20to%20combine%20the%20efficiency%20of%20random%20walk-based%20methods%20and%20the%20rich%20semantic%20representation%20of%20deep%20learning%20methods.%20This%20system%2C%20which%20we%20call%20CyberGFM%2C%20achieved%20state-of-the-art%20results%20on%20three%20widely%20used%20network%20anomaly%20detection%20datasets%2C%20delivering%20a%20up%20to%202%24%5Ctimes%24%20improvement%20in%20average%20precision.%20We%20found%20that%20CyberGFM%20outperforms%20all%20prior%20works%20in%20unsupervised%20link%20prediction%20for%20network%20anomaly%20detection%2C%20using%20the%20same%20number%20of%20parameters%2C%20and%20with%20equal%20or%20better%20efficiency%20than%20the%20previous%20best%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyberGFM%253A%2520Graph%2520Foundation%2520Models%2520for%2520Lateral%2520Movement%2520Detection%2520in%2520Enterprise%2520Networks%26entry.906535625%3DIsaiah%2520J.%2520King%2520and%2520Bernardo%2520Trindade%2520and%2520Benjamin%2520Bowman%2520and%2520H.%2520Howie%2520Huang%26entry.1292438233%3DRepresenting%2520networks%2520as%2520a%2520graph%2520and%2520training%2520a%2520link%2520prediction%2520model%2520using%2520benign%2520connections%2520is%2520an%2520effective%2520method%2520of%2520anomaly-based%2520intrusion%2520detection.%2520Existing%2520works%2520using%2520this%2520technique%2520have%2520shown%2520great%2520success%2520using%2520temporal%2520graph%2520neural%2520networks%2520and%2520skip-gram-based%2520approaches%2520on%2520random%2520walks.%2520However%252C%2520random%2520walk-based%2520approaches%2520are%2520unable%2520to%2520incorporate%2520rich%2520edge%2520data%252C%2520while%2520the%2520GNN-based%2520approaches%2520require%2520large%2520amounts%2520of%2520memory%2520to%2520train.%2520In%2520this%2520work%252C%2520we%2520propose%2520extending%2520the%2520original%2520insight%2520from%2520random%2520walk-based%2520skip-grams--that%2520random%2520walks%2520through%2520a%2520graph%2520are%2520analogous%2520to%2520sentences%2520in%2520a%2520corpus--to%2520the%2520more%2520modern%2520transformer-based%2520foundation%2520models.%2520Using%2520language%2520models%2520that%2520take%2520advantage%2520of%2520GPU%2520optimizations%252C%2520we%2520can%2520quickly%2520train%2520a%2520graph%2520foundation%2520model%2520to%2520predict%2520missing%2520tokens%2520in%2520random%2520walks%2520through%2520a%2520network%2520of%2520computers.%2520The%2520graph%2520foundation%2520model%2520is%2520then%2520finetuned%2520for%2520link%2520prediction%2520and%2520used%2520as%2520a%2520network%2520anomaly%2520detector.%2520This%2520new%2520approach%2520allows%2520us%2520to%2520combine%2520the%2520efficiency%2520of%2520random%2520walk-based%2520methods%2520and%2520the%2520rich%2520semantic%2520representation%2520of%2520deep%2520learning%2520methods.%2520This%2520system%252C%2520which%2520we%2520call%2520CyberGFM%252C%2520achieved%2520state-of-the-art%2520results%2520on%2520three%2520widely%2520used%2520network%2520anomaly%2520detection%2520datasets%252C%2520delivering%2520a%2520up%2520to%25202%2524%255Ctimes%2524%2520improvement%2520in%2520average%2520precision.%2520We%2520found%2520that%2520CyberGFM%2520outperforms%2520all%2520prior%2520works%2520in%2520unsupervised%2520link%2520prediction%2520for%2520network%2520anomaly%2520detection%252C%2520using%2520the%2520same%2520number%2520of%2520parameters%252C%2520and%2520with%2520equal%2520or%2520better%2520efficiency%2520than%2520the%2520previous%2520best%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CyberGFM%3A%20Graph%20Foundation%20Models%20for%20Lateral%20Movement%20Detection%20in%20Enterprise%20Networks&entry.906535625=Isaiah%20J.%20King%20and%20Bernardo%20Trindade%20and%20Benjamin%20Bowman%20and%20H.%20Howie%20Huang&entry.1292438233=Representing%20networks%20as%20a%20graph%20and%20training%20a%20link%20prediction%20model%20using%20benign%20connections%20is%20an%20effective%20method%20of%20anomaly-based%20intrusion%20detection.%20Existing%20works%20using%20this%20technique%20have%20shown%20great%20success%20using%20temporal%20graph%20neural%20networks%20and%20skip-gram-based%20approaches%20on%20random%20walks.%20However%2C%20random%20walk-based%20approaches%20are%20unable%20to%20incorporate%20rich%20edge%20data%2C%20while%20the%20GNN-based%20approaches%20require%20large%20amounts%20of%20memory%20to%20train.%20In%20this%20work%2C%20we%20propose%20extending%20the%20original%20insight%20from%20random%20walk-based%20skip-grams--that%20random%20walks%20through%20a%20graph%20are%20analogous%20to%20sentences%20in%20a%20corpus--to%20the%20more%20modern%20transformer-based%20foundation%20models.%20Using%20language%20models%20that%20take%20advantage%20of%20GPU%20optimizations%2C%20we%20can%20quickly%20train%20a%20graph%20foundation%20model%20to%20predict%20missing%20tokens%20in%20random%20walks%20through%20a%20network%20of%20computers.%20The%20graph%20foundation%20model%20is%20then%20finetuned%20for%20link%20prediction%20and%20used%20as%20a%20network%20anomaly%20detector.%20This%20new%20approach%20allows%20us%20to%20combine%20the%20efficiency%20of%20random%20walk-based%20methods%20and%20the%20rich%20semantic%20representation%20of%20deep%20learning%20methods.%20This%20system%2C%20which%20we%20call%20CyberGFM%2C%20achieved%20state-of-the-art%20results%20on%20three%20widely%20used%20network%20anomaly%20detection%20datasets%2C%20delivering%20a%20up%20to%202%24%5Ctimes%24%20improvement%20in%20average%20precision.%20We%20found%20that%20CyberGFM%20outperforms%20all%20prior%20works%20in%20unsupervised%20link%20prediction%20for%20network%20anomaly%20detection%2C%20using%20the%20same%20number%20of%20parameters%2C%20and%20with%20equal%20or%20better%20efficiency%20than%20the%20previous%20best%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.05988v1&entry.124074799=Read"},
{"title": "Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections", "author": "Jing Wu and Zirui Wang and Iro Laina and Victor Adrian Prisacariu", "abstract": "Mirror reflections are common in everyday environments and can provide stereo information within a single capture, as the real and reflected virtual views are visible simultaneously. We exploit this property by treating the reflection as an auxiliary view and designing a transformation that constructs a physically valid virtual camera, allowing direct pixel-domain generation of the virtual view while adhering to the real-world imaging process. This enables a multi-view stereo setup from a single image, simplifying the imaging process, making it compatible with powerful feed-forward reconstruction models for generalizable and robust 3D reconstruction. To further exploit the geometric symmetry introduced by mirrors, we propose a symmetric-aware loss to refine pose estimation. Our framework also naturally extends to dynamic scenes, where each frame contains a mirror reflection, enabling efficient per-frame geometry recovery. For quantitative evaluation, we provide a fully customizable synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and camera poses. Extensive experiments on real-world data and synthetic data are conducted to illustrate the effectiveness of our method.", "link": "http://arxiv.org/abs/2509.20607v2", "date": "2026-01-09", "relevancy": 2.4797, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6255}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6255}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflect3r%3A%20Single-View%203D%20Stereo%20Reconstruction%20Aided%20by%20Mirror%20Reflections&body=Title%3A%20Reflect3r%3A%20Single-View%203D%20Stereo%20Reconstruction%20Aided%20by%20Mirror%20Reflections%0AAuthor%3A%20Jing%20Wu%20and%20Zirui%20Wang%20and%20Iro%20Laina%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20Mirror%20reflections%20are%20common%20in%20everyday%20environments%20and%20can%20provide%20stereo%20information%20within%20a%20single%20capture%2C%20as%20the%20real%20and%20reflected%20virtual%20views%20are%20visible%20simultaneously.%20We%20exploit%20this%20property%20by%20treating%20the%20reflection%20as%20an%20auxiliary%20view%20and%20designing%20a%20transformation%20that%20constructs%20a%20physically%20valid%20virtual%20camera%2C%20allowing%20direct%20pixel-domain%20generation%20of%20the%20virtual%20view%20while%20adhering%20to%20the%20real-world%20imaging%20process.%20This%20enables%20a%20multi-view%20stereo%20setup%20from%20a%20single%20image%2C%20simplifying%20the%20imaging%20process%2C%20making%20it%20compatible%20with%20powerful%20feed-forward%20reconstruction%20models%20for%20generalizable%20and%20robust%203D%20reconstruction.%20To%20further%20exploit%20the%20geometric%20symmetry%20introduced%20by%20mirrors%2C%20we%20propose%20a%20symmetric-aware%20loss%20to%20refine%20pose%20estimation.%20Our%20framework%20also%20naturally%20extends%20to%20dynamic%20scenes%2C%20where%20each%20frame%20contains%20a%20mirror%20reflection%2C%20enabling%20efficient%20per-frame%20geometry%20recovery.%20For%20quantitative%20evaluation%2C%20we%20provide%20a%20fully%20customizable%20synthetic%20dataset%20of%2016%20Blender%20scenes%2C%20each%20with%20ground-truth%20point%20clouds%20and%20camera%20poses.%20Extensive%20experiments%20on%20real-world%20data%20and%20synthetic%20data%20are%20conducted%20to%20illustrate%20the%20effectiveness%20of%20our%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflect3r%253A%2520Single-View%25203D%2520Stereo%2520Reconstruction%2520Aided%2520by%2520Mirror%2520Reflections%26entry.906535625%3DJing%2520Wu%2520and%2520Zirui%2520Wang%2520and%2520Iro%2520Laina%2520and%2520Victor%2520Adrian%2520Prisacariu%26entry.1292438233%3DMirror%2520reflections%2520are%2520common%2520in%2520everyday%2520environments%2520and%2520can%2520provide%2520stereo%2520information%2520within%2520a%2520single%2520capture%252C%2520as%2520the%2520real%2520and%2520reflected%2520virtual%2520views%2520are%2520visible%2520simultaneously.%2520We%2520exploit%2520this%2520property%2520by%2520treating%2520the%2520reflection%2520as%2520an%2520auxiliary%2520view%2520and%2520designing%2520a%2520transformation%2520that%2520constructs%2520a%2520physically%2520valid%2520virtual%2520camera%252C%2520allowing%2520direct%2520pixel-domain%2520generation%2520of%2520the%2520virtual%2520view%2520while%2520adhering%2520to%2520the%2520real-world%2520imaging%2520process.%2520This%2520enables%2520a%2520multi-view%2520stereo%2520setup%2520from%2520a%2520single%2520image%252C%2520simplifying%2520the%2520imaging%2520process%252C%2520making%2520it%2520compatible%2520with%2520powerful%2520feed-forward%2520reconstruction%2520models%2520for%2520generalizable%2520and%2520robust%25203D%2520reconstruction.%2520To%2520further%2520exploit%2520the%2520geometric%2520symmetry%2520introduced%2520by%2520mirrors%252C%2520we%2520propose%2520a%2520symmetric-aware%2520loss%2520to%2520refine%2520pose%2520estimation.%2520Our%2520framework%2520also%2520naturally%2520extends%2520to%2520dynamic%2520scenes%252C%2520where%2520each%2520frame%2520contains%2520a%2520mirror%2520reflection%252C%2520enabling%2520efficient%2520per-frame%2520geometry%2520recovery.%2520For%2520quantitative%2520evaluation%252C%2520we%2520provide%2520a%2520fully%2520customizable%2520synthetic%2520dataset%2520of%252016%2520Blender%2520scenes%252C%2520each%2520with%2520ground-truth%2520point%2520clouds%2520and%2520camera%2520poses.%2520Extensive%2520experiments%2520on%2520real-world%2520data%2520and%2520synthetic%2520data%2520are%2520conducted%2520to%2520illustrate%2520the%2520effectiveness%2520of%2520our%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflect3r%3A%20Single-View%203D%20Stereo%20Reconstruction%20Aided%20by%20Mirror%20Reflections&entry.906535625=Jing%20Wu%20and%20Zirui%20Wang%20and%20Iro%20Laina%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=Mirror%20reflections%20are%20common%20in%20everyday%20environments%20and%20can%20provide%20stereo%20information%20within%20a%20single%20capture%2C%20as%20the%20real%20and%20reflected%20virtual%20views%20are%20visible%20simultaneously.%20We%20exploit%20this%20property%20by%20treating%20the%20reflection%20as%20an%20auxiliary%20view%20and%20designing%20a%20transformation%20that%20constructs%20a%20physically%20valid%20virtual%20camera%2C%20allowing%20direct%20pixel-domain%20generation%20of%20the%20virtual%20view%20while%20adhering%20to%20the%20real-world%20imaging%20process.%20This%20enables%20a%20multi-view%20stereo%20setup%20from%20a%20single%20image%2C%20simplifying%20the%20imaging%20process%2C%20making%20it%20compatible%20with%20powerful%20feed-forward%20reconstruction%20models%20for%20generalizable%20and%20robust%203D%20reconstruction.%20To%20further%20exploit%20the%20geometric%20symmetry%20introduced%20by%20mirrors%2C%20we%20propose%20a%20symmetric-aware%20loss%20to%20refine%20pose%20estimation.%20Our%20framework%20also%20naturally%20extends%20to%20dynamic%20scenes%2C%20where%20each%20frame%20contains%20a%20mirror%20reflection%2C%20enabling%20efficient%20per-frame%20geometry%20recovery.%20For%20quantitative%20evaluation%2C%20we%20provide%20a%20fully%20customizable%20synthetic%20dataset%20of%2016%20Blender%20scenes%2C%20each%20with%20ground-truth%20point%20clouds%20and%20camera%20poses.%20Extensive%20experiments%20on%20real-world%20data%20and%20synthetic%20data%20are%20conducted%20to%20illustrate%20the%20effectiveness%20of%20our%20method.&entry.1838667208=http%3A//arxiv.org/abs/2509.20607v2&entry.124074799=Read"},
{"title": "Multimodal In-context Learning for ASR of Low-resource Languages", "author": "Zhaolin Li and Jan Niehues", "abstract": "Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.", "link": "http://arxiv.org/abs/2601.05707v1", "date": "2026-01-09", "relevancy": 2.4725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20In-context%20Learning%20for%20ASR%20of%20Low-resource%20Languages&body=Title%3A%20Multimodal%20In-context%20Learning%20for%20ASR%20of%20Low-resource%20Languages%0AAuthor%3A%20Zhaolin%20Li%20and%20Jan%20Niehues%0AAbstract%3A%20Automatic%20speech%20recognition%20%28ASR%29%20still%20covers%20only%20a%20small%20fraction%20of%20the%20world%27s%20languages%2C%20mainly%20due%20to%20supervised%20data%20scarcity.%20In-context%20learning%20%28ICL%29%20with%20large%20language%20models%20%28LLMs%29%20addresses%20this%20problem%2C%20but%20prior%20work%20largely%20focuses%20on%20high-resource%20languages%20covered%20during%20training%20and%20text-only%20settings.%20This%20paper%20investigates%20whether%20speech%20LLMs%20can%20learn%20unseen%20languages%20with%20multimodal%20ICL%20%28MICL%29%2C%20and%20how%20this%20learning%20can%20be%20used%20to%20improve%20ASR.%20We%20conduct%20experiments%20with%20two%20speech%20LLMs%2C%20Phi-4%20and%20Qwen3-Omni%2C%20on%20three%20diverse%20endangered%20languages.%20Firstly%2C%20we%20find%20that%20MICL%20is%20effective%20for%20unseen%20languages%2C%20leveraging%20both%20speech%20and%20text%20modalities.%20We%20further%20show%20that%20cross-lingual%20transfer%20learning%20improves%20MICL%20efficiency%20on%20target%20languages%20without%20training%20on%20them.%20Moreover%2C%20we%20analyze%20attention%20patterns%20to%20interpret%20MICL%20mechanisms%2C%20and%20we%20observe%20layer-dependent%20preferences%20between%20audio%20and%20text%20context%2C%20with%20an%20overall%20bias%20towards%20text.%20Finally%2C%20we%20show%20that%20prompt-based%20ASR%20with%20speech%20LLMs%20performs%20poorly%20on%20unseen%20languages%2C%20motivating%20a%20simple%20ASR%20system%20that%20combines%20a%20stronger%20acoustic%20model%20with%20a%20speech%20LLM%20via%20MICL-based%20selection%20of%20acoustic%20hypotheses.%20Results%20show%20that%20MICL%20consistently%20improves%20ASR%20performance%2C%20and%20that%20cross-lingual%20transfer%20learning%20matches%20or%20outperforms%20corpus-trained%20language%20models%20without%20using%20target-language%20data.%20Our%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520In-context%2520Learning%2520for%2520ASR%2520of%2520Low-resource%2520Languages%26entry.906535625%3DZhaolin%2520Li%2520and%2520Jan%2520Niehues%26entry.1292438233%3DAutomatic%2520speech%2520recognition%2520%2528ASR%2529%2520still%2520covers%2520only%2520a%2520small%2520fraction%2520of%2520the%2520world%2527s%2520languages%252C%2520mainly%2520due%2520to%2520supervised%2520data%2520scarcity.%2520In-context%2520learning%2520%2528ICL%2529%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520addresses%2520this%2520problem%252C%2520but%2520prior%2520work%2520largely%2520focuses%2520on%2520high-resource%2520languages%2520covered%2520during%2520training%2520and%2520text-only%2520settings.%2520This%2520paper%2520investigates%2520whether%2520speech%2520LLMs%2520can%2520learn%2520unseen%2520languages%2520with%2520multimodal%2520ICL%2520%2528MICL%2529%252C%2520and%2520how%2520this%2520learning%2520can%2520be%2520used%2520to%2520improve%2520ASR.%2520We%2520conduct%2520experiments%2520with%2520two%2520speech%2520LLMs%252C%2520Phi-4%2520and%2520Qwen3-Omni%252C%2520on%2520three%2520diverse%2520endangered%2520languages.%2520Firstly%252C%2520we%2520find%2520that%2520MICL%2520is%2520effective%2520for%2520unseen%2520languages%252C%2520leveraging%2520both%2520speech%2520and%2520text%2520modalities.%2520We%2520further%2520show%2520that%2520cross-lingual%2520transfer%2520learning%2520improves%2520MICL%2520efficiency%2520on%2520target%2520languages%2520without%2520training%2520on%2520them.%2520Moreover%252C%2520we%2520analyze%2520attention%2520patterns%2520to%2520interpret%2520MICL%2520mechanisms%252C%2520and%2520we%2520observe%2520layer-dependent%2520preferences%2520between%2520audio%2520and%2520text%2520context%252C%2520with%2520an%2520overall%2520bias%2520towards%2520text.%2520Finally%252C%2520we%2520show%2520that%2520prompt-based%2520ASR%2520with%2520speech%2520LLMs%2520performs%2520poorly%2520on%2520unseen%2520languages%252C%2520motivating%2520a%2520simple%2520ASR%2520system%2520that%2520combines%2520a%2520stronger%2520acoustic%2520model%2520with%2520a%2520speech%2520LLM%2520via%2520MICL-based%2520selection%2520of%2520acoustic%2520hypotheses.%2520Results%2520show%2520that%2520MICL%2520consistently%2520improves%2520ASR%2520performance%252C%2520and%2520that%2520cross-lingual%2520transfer%2520learning%2520matches%2520or%2520outperforms%2520corpus-trained%2520language%2520models%2520without%2520using%2520target-language%2520data.%2520Our%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20In-context%20Learning%20for%20ASR%20of%20Low-resource%20Languages&entry.906535625=Zhaolin%20Li%20and%20Jan%20Niehues&entry.1292438233=Automatic%20speech%20recognition%20%28ASR%29%20still%20covers%20only%20a%20small%20fraction%20of%20the%20world%27s%20languages%2C%20mainly%20due%20to%20supervised%20data%20scarcity.%20In-context%20learning%20%28ICL%29%20with%20large%20language%20models%20%28LLMs%29%20addresses%20this%20problem%2C%20but%20prior%20work%20largely%20focuses%20on%20high-resource%20languages%20covered%20during%20training%20and%20text-only%20settings.%20This%20paper%20investigates%20whether%20speech%20LLMs%20can%20learn%20unseen%20languages%20with%20multimodal%20ICL%20%28MICL%29%2C%20and%20how%20this%20learning%20can%20be%20used%20to%20improve%20ASR.%20We%20conduct%20experiments%20with%20two%20speech%20LLMs%2C%20Phi-4%20and%20Qwen3-Omni%2C%20on%20three%20diverse%20endangered%20languages.%20Firstly%2C%20we%20find%20that%20MICL%20is%20effective%20for%20unseen%20languages%2C%20leveraging%20both%20speech%20and%20text%20modalities.%20We%20further%20show%20that%20cross-lingual%20transfer%20learning%20improves%20MICL%20efficiency%20on%20target%20languages%20without%20training%20on%20them.%20Moreover%2C%20we%20analyze%20attention%20patterns%20to%20interpret%20MICL%20mechanisms%2C%20and%20we%20observe%20layer-dependent%20preferences%20between%20audio%20and%20text%20context%2C%20with%20an%20overall%20bias%20towards%20text.%20Finally%2C%20we%20show%20that%20prompt-based%20ASR%20with%20speech%20LLMs%20performs%20poorly%20on%20unseen%20languages%2C%20motivating%20a%20simple%20ASR%20system%20that%20combines%20a%20stronger%20acoustic%20model%20with%20a%20speech%20LLM%20via%20MICL-based%20selection%20of%20acoustic%20hypotheses.%20Results%20show%20that%20MICL%20consistently%20improves%20ASR%20performance%2C%20and%20that%20cross-lingual%20transfer%20learning%20matches%20or%20outperforms%20corpus-trained%20language%20models%20without%20using%20target-language%20data.%20Our%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.05707v1&entry.124074799=Read"},
{"title": "Communication-Efficient Stochastic Distributed Learning", "author": "Xiaoxing Ren and Nicola Bastianello and Karl H. Johansson and Thomas Parisini", "abstract": "We address distributed learning problems, both nonconvex and convex, over undirected networks. In particular, we design a novel algorithm based on the distributed Alternating Direction Method of Multipliers (ADMM) to address the challenges of high communication costs, and large datasets. Our design tackles these challenges i) by enabling the agents to perform multiple local training steps between each round of communications; and ii) by allowing the agents to employ stochastic gradients while carrying out local computations. We show that the proposed algorithm converges to a neighborhood of a stationary point, for nonconvex problems, and of an optimal point, for convex problems. We also propose a variant of the algorithm to incorporate variance reduction thus achieving exact convergence. We show that the resulting algorithm indeed converges to a stationary (or optimal) point, and moreover that local training accelerates convergence. We thoroughly compare the proposed algorithms with the state of the art, both theoretically and through numerical results.", "link": "http://arxiv.org/abs/2501.13516v2", "date": "2026-01-09", "relevancy": 2.466, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.51}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4954}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Stochastic%20Distributed%20Learning&body=Title%3A%20Communication-Efficient%20Stochastic%20Distributed%20Learning%0AAuthor%3A%20Xiaoxing%20Ren%20and%20Nicola%20Bastianello%20and%20Karl%20H.%20Johansson%20and%20Thomas%20Parisini%0AAbstract%3A%20We%20address%20distributed%20learning%20problems%2C%20both%20nonconvex%20and%20convex%2C%20over%20undirected%20networks.%20In%20particular%2C%20we%20design%20a%20novel%20algorithm%20based%20on%20the%20distributed%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20to%20address%20the%20challenges%20of%20high%20communication%20costs%2C%20and%20large%20datasets.%20Our%20design%20tackles%20these%20challenges%20i%29%20by%20enabling%20the%20agents%20to%20perform%20multiple%20local%20training%20steps%20between%20each%20round%20of%20communications%3B%20and%20ii%29%20by%20allowing%20the%20agents%20to%20employ%20stochastic%20gradients%20while%20carrying%20out%20local%20computations.%20We%20show%20that%20the%20proposed%20algorithm%20converges%20to%20a%20neighborhood%20of%20a%20stationary%20point%2C%20for%20nonconvex%20problems%2C%20and%20of%20an%20optimal%20point%2C%20for%20convex%20problems.%20We%20also%20propose%20a%20variant%20of%20the%20algorithm%20to%20incorporate%20variance%20reduction%20thus%20achieving%20exact%20convergence.%20We%20show%20that%20the%20resulting%20algorithm%20indeed%20converges%20to%20a%20stationary%20%28or%20optimal%29%20point%2C%20and%20moreover%20that%20local%20training%20accelerates%20convergence.%20We%20thoroughly%20compare%20the%20proposed%20algorithms%20with%20the%20state%20of%20the%20art%2C%20both%20theoretically%20and%20through%20numerical%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2501.13516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Stochastic%2520Distributed%2520Learning%26entry.906535625%3DXiaoxing%2520Ren%2520and%2520Nicola%2520Bastianello%2520and%2520Karl%2520H.%2520Johansson%2520and%2520Thomas%2520Parisini%26entry.1292438233%3DWe%2520address%2520distributed%2520learning%2520problems%252C%2520both%2520nonconvex%2520and%2520convex%252C%2520over%2520undirected%2520networks.%2520In%2520particular%252C%2520we%2520design%2520a%2520novel%2520algorithm%2520based%2520on%2520the%2520distributed%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528ADMM%2529%2520to%2520address%2520the%2520challenges%2520of%2520high%2520communication%2520costs%252C%2520and%2520large%2520datasets.%2520Our%2520design%2520tackles%2520these%2520challenges%2520i%2529%2520by%2520enabling%2520the%2520agents%2520to%2520perform%2520multiple%2520local%2520training%2520steps%2520between%2520each%2520round%2520of%2520communications%253B%2520and%2520ii%2529%2520by%2520allowing%2520the%2520agents%2520to%2520employ%2520stochastic%2520gradients%2520while%2520carrying%2520out%2520local%2520computations.%2520We%2520show%2520that%2520the%2520proposed%2520algorithm%2520converges%2520to%2520a%2520neighborhood%2520of%2520a%2520stationary%2520point%252C%2520for%2520nonconvex%2520problems%252C%2520and%2520of%2520an%2520optimal%2520point%252C%2520for%2520convex%2520problems.%2520We%2520also%2520propose%2520a%2520variant%2520of%2520the%2520algorithm%2520to%2520incorporate%2520variance%2520reduction%2520thus%2520achieving%2520exact%2520convergence.%2520We%2520show%2520that%2520the%2520resulting%2520algorithm%2520indeed%2520converges%2520to%2520a%2520stationary%2520%2528or%2520optimal%2529%2520point%252C%2520and%2520moreover%2520that%2520local%2520training%2520accelerates%2520convergence.%2520We%2520thoroughly%2520compare%2520the%2520proposed%2520algorithms%2520with%2520the%2520state%2520of%2520the%2520art%252C%2520both%2520theoretically%2520and%2520through%2520numerical%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Stochastic%20Distributed%20Learning&entry.906535625=Xiaoxing%20Ren%20and%20Nicola%20Bastianello%20and%20Karl%20H.%20Johansson%20and%20Thomas%20Parisini&entry.1292438233=We%20address%20distributed%20learning%20problems%2C%20both%20nonconvex%20and%20convex%2C%20over%20undirected%20networks.%20In%20particular%2C%20we%20design%20a%20novel%20algorithm%20based%20on%20the%20distributed%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20to%20address%20the%20challenges%20of%20high%20communication%20costs%2C%20and%20large%20datasets.%20Our%20design%20tackles%20these%20challenges%20i%29%20by%20enabling%20the%20agents%20to%20perform%20multiple%20local%20training%20steps%20between%20each%20round%20of%20communications%3B%20and%20ii%29%20by%20allowing%20the%20agents%20to%20employ%20stochastic%20gradients%20while%20carrying%20out%20local%20computations.%20We%20show%20that%20the%20proposed%20algorithm%20converges%20to%20a%20neighborhood%20of%20a%20stationary%20point%2C%20for%20nonconvex%20problems%2C%20and%20of%20an%20optimal%20point%2C%20for%20convex%20problems.%20We%20also%20propose%20a%20variant%20of%20the%20algorithm%20to%20incorporate%20variance%20reduction%20thus%20achieving%20exact%20convergence.%20We%20show%20that%20the%20resulting%20algorithm%20indeed%20converges%20to%20a%20stationary%20%28or%20optimal%29%20point%2C%20and%20moreover%20that%20local%20training%20accelerates%20convergence.%20We%20thoroughly%20compare%20the%20proposed%20algorithms%20with%20the%20state%20of%20the%20art%2C%20both%20theoretically%20and%20through%20numerical%20results.&entry.1838667208=http%3A//arxiv.org/abs/2501.13516v2&entry.124074799=Read"},
{"title": "Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding", "author": "Yuxuan Zhou and Fei Huang and Heng Li and Fengyi Wu and Tianyu Wang and Jianwei Zhang and Junyang Lin and Zhi-Qi Cheng", "abstract": "Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.", "link": "http://arxiv.org/abs/2601.05724v1", "date": "2026-01-09", "relevancy": 2.431, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Joint%20Intractability%20with%20Lossless%20Hierarchical%20Speculative%20Decoding&body=Title%3A%20Overcoming%20Joint%20Intractability%20with%20Lossless%20Hierarchical%20Speculative%20Decoding%0AAuthor%3A%20Yuxuan%20Zhou%20and%20Fei%20Huang%20and%20Heng%20Li%20and%20Fengyi%20Wu%20and%20Tianyu%20Wang%20and%20Jianwei%20Zhang%20and%20Junyang%20Lin%20and%20Zhi-Qi%20Cheng%0AAbstract%3A%20Verification%20is%20a%20key%20bottleneck%20in%20improving%20inference%20speed%20while%20maintaining%20distribution%20fidelity%20in%20Speculative%20Decoding.%20Recent%20work%20has%20shown%20that%20sequence-level%20verification%20leads%20to%20a%20higher%20number%20of%20accepted%20tokens%20compared%20to%20token-wise%20verification.%20However%2C%20existing%20solutions%20often%20rely%20on%20surrogate%20approximations%20or%20are%20constrained%20by%20partial%20information%2C%20struggling%20with%20joint%20intractability.%20In%20this%20work%2C%20we%20propose%20Hierarchical%20Speculative%20Decoding%20%28HSD%29%2C%20a%20provably%20lossless%20verification%20method%20that%20significantly%20boosts%20the%20expected%20number%20of%20accepted%20tokens%20and%20overcomes%20joint%20intractability%20by%20balancing%20excess%20and%20deficient%20probability%20mass%20across%20accessible%20branches.%20Our%20extensive%20large-scale%20experiments%20demonstrate%20that%20HSD%20yields%20consistent%20improvements%20in%20acceptance%20rates%20across%20diverse%20model%20families%20and%20benchmarks.%20Moreover%2C%20its%20strong%20explainability%20and%20generality%20make%20it%20readily%20integrable%20into%20a%20wide%20range%20of%20speculative%20decoding%20frameworks.%20Notably%2C%20integrating%20HSD%20into%20EAGLE-3%20yields%20over%20a%2012%25%20performance%20gain%2C%20establishing%20state-of-the-art%20decoding%20efficiency%20without%20compromising%20distribution%20fidelity.%20Code%20is%20available%20at%20https%3A//github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Joint%2520Intractability%2520with%2520Lossless%2520Hierarchical%2520Speculative%2520Decoding%26entry.906535625%3DYuxuan%2520Zhou%2520and%2520Fei%2520Huang%2520and%2520Heng%2520Li%2520and%2520Fengyi%2520Wu%2520and%2520Tianyu%2520Wang%2520and%2520Jianwei%2520Zhang%2520and%2520Junyang%2520Lin%2520and%2520Zhi-Qi%2520Cheng%26entry.1292438233%3DVerification%2520is%2520a%2520key%2520bottleneck%2520in%2520improving%2520inference%2520speed%2520while%2520maintaining%2520distribution%2520fidelity%2520in%2520Speculative%2520Decoding.%2520Recent%2520work%2520has%2520shown%2520that%2520sequence-level%2520verification%2520leads%2520to%2520a%2520higher%2520number%2520of%2520accepted%2520tokens%2520compared%2520to%2520token-wise%2520verification.%2520However%252C%2520existing%2520solutions%2520often%2520rely%2520on%2520surrogate%2520approximations%2520or%2520are%2520constrained%2520by%2520partial%2520information%252C%2520struggling%2520with%2520joint%2520intractability.%2520In%2520this%2520work%252C%2520we%2520propose%2520Hierarchical%2520Speculative%2520Decoding%2520%2528HSD%2529%252C%2520a%2520provably%2520lossless%2520verification%2520method%2520that%2520significantly%2520boosts%2520the%2520expected%2520number%2520of%2520accepted%2520tokens%2520and%2520overcomes%2520joint%2520intractability%2520by%2520balancing%2520excess%2520and%2520deficient%2520probability%2520mass%2520across%2520accessible%2520branches.%2520Our%2520extensive%2520large-scale%2520experiments%2520demonstrate%2520that%2520HSD%2520yields%2520consistent%2520improvements%2520in%2520acceptance%2520rates%2520across%2520diverse%2520model%2520families%2520and%2520benchmarks.%2520Moreover%252C%2520its%2520strong%2520explainability%2520and%2520generality%2520make%2520it%2520readily%2520integrable%2520into%2520a%2520wide%2520range%2520of%2520speculative%2520decoding%2520frameworks.%2520Notably%252C%2520integrating%2520HSD%2520into%2520EAGLE-3%2520yields%2520over%2520a%252012%2525%2520performance%2520gain%252C%2520establishing%2520state-of-the-art%2520decoding%2520efficiency%2520without%2520compromising%2520distribution%2520fidelity.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Joint%20Intractability%20with%20Lossless%20Hierarchical%20Speculative%20Decoding&entry.906535625=Yuxuan%20Zhou%20and%20Fei%20Huang%20and%20Heng%20Li%20and%20Fengyi%20Wu%20and%20Tianyu%20Wang%20and%20Jianwei%20Zhang%20and%20Junyang%20Lin%20and%20Zhi-Qi%20Cheng&entry.1292438233=Verification%20is%20a%20key%20bottleneck%20in%20improving%20inference%20speed%20while%20maintaining%20distribution%20fidelity%20in%20Speculative%20Decoding.%20Recent%20work%20has%20shown%20that%20sequence-level%20verification%20leads%20to%20a%20higher%20number%20of%20accepted%20tokens%20compared%20to%20token-wise%20verification.%20However%2C%20existing%20solutions%20often%20rely%20on%20surrogate%20approximations%20or%20are%20constrained%20by%20partial%20information%2C%20struggling%20with%20joint%20intractability.%20In%20this%20work%2C%20we%20propose%20Hierarchical%20Speculative%20Decoding%20%28HSD%29%2C%20a%20provably%20lossless%20verification%20method%20that%20significantly%20boosts%20the%20expected%20number%20of%20accepted%20tokens%20and%20overcomes%20joint%20intractability%20by%20balancing%20excess%20and%20deficient%20probability%20mass%20across%20accessible%20branches.%20Our%20extensive%20large-scale%20experiments%20demonstrate%20that%20HSD%20yields%20consistent%20improvements%20in%20acceptance%20rates%20across%20diverse%20model%20families%20and%20benchmarks.%20Moreover%2C%20its%20strong%20explainability%20and%20generality%20make%20it%20readily%20integrable%20into%20a%20wide%20range%20of%20speculative%20decoding%20frameworks.%20Notably%2C%20integrating%20HSD%20into%20EAGLE-3%20yields%20over%20a%2012%25%20performance%20gain%2C%20establishing%20state-of-the-art%20decoding%20efficiency%20without%20compromising%20distribution%20fidelity.%20Code%20is%20available%20at%20https%3A//github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.&entry.1838667208=http%3A//arxiv.org/abs/2601.05724v1&entry.124074799=Read"},
{"title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models", "author": "Oriol Rabasseda and Zenjie Li and Kamal Nasrollahi and Sergio Escalera", "abstract": "Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.\n  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.", "link": "http://arxiv.org/abs/2601.04824v2", "date": "2026-01-09", "relevancy": 2.4139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOVABench%3A%20A%20Vehicle%20Surveillance%20Action%20Retrieval%20Benchmark%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20SOVABench%3A%20A%20Vehicle%20Surveillance%20Action%20Retrieval%20Benchmark%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Oriol%20Rabasseda%20and%20Zenjie%20Li%20and%20Kamal%20Nasrollahi%20and%20Sergio%20Escalera%0AAbstract%3A%20Automatic%20identification%20of%20events%20and%20recurrent%20behavior%20analysis%20are%20critical%20for%20video%20surveillance.%20However%2C%20most%20existing%20content-based%20video%20retrieval%20benchmarks%20focus%20on%20scene-level%20similarity%20and%20do%20not%20evaluate%20the%20action%20discrimination%20required%20in%20surveillance.%20To%20address%20this%20gap%2C%20we%20introduce%20SOVABench%20%28Surveillance%20Opposite%20Vehicle%20Actions%20Benchmark%29%2C%20a%20real-world%20retrieval%20benchmark%20built%20from%20surveillance%20footage%20and%20centered%20on%20vehicle-related%20actions.%20SOVABench%20defines%20two%20evaluation%20protocols%20%28inter-pair%20and%20intra-pair%29%20to%20assess%20cross-action%20discrimination%20and%20temporal%20direction%20understanding.%20Although%20action%20distinctions%20are%20generally%20intuitive%20for%20human%20observers%2C%20our%20experiments%20show%20that%20they%20remain%20challenging%20for%20state-of-the-art%20vision%20and%20multimodal%20models.%0A%20%20Leveraging%20the%20visual%20reasoning%20and%20instruction-following%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20we%20present%20a%20training-free%20framework%20for%20producing%20interpretable%20embeddings%20from%20MLLM-generated%20descriptions%20for%20both%20images%20and%20videos.%20The%20framework%20achieves%20strong%20performance%20on%20SOVABench%20as%20well%20as%20on%20several%20spatial%20and%20counting%20benchmarks%20where%20contrastive%20Vision-Language%20Models%20often%20fail.%20The%20code%2C%20annotations%2C%20and%20instructions%20to%20construct%20the%20benchmark%20are%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOVABench%253A%2520A%2520Vehicle%2520Surveillance%2520Action%2520Retrieval%2520Benchmark%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DOriol%2520Rabasseda%2520and%2520Zenjie%2520Li%2520and%2520Kamal%2520Nasrollahi%2520and%2520Sergio%2520Escalera%26entry.1292438233%3DAutomatic%2520identification%2520of%2520events%2520and%2520recurrent%2520behavior%2520analysis%2520are%2520critical%2520for%2520video%2520surveillance.%2520However%252C%2520most%2520existing%2520content-based%2520video%2520retrieval%2520benchmarks%2520focus%2520on%2520scene-level%2520similarity%2520and%2520do%2520not%2520evaluate%2520the%2520action%2520discrimination%2520required%2520in%2520surveillance.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520SOVABench%2520%2528Surveillance%2520Opposite%2520Vehicle%2520Actions%2520Benchmark%2529%252C%2520a%2520real-world%2520retrieval%2520benchmark%2520built%2520from%2520surveillance%2520footage%2520and%2520centered%2520on%2520vehicle-related%2520actions.%2520SOVABench%2520defines%2520two%2520evaluation%2520protocols%2520%2528inter-pair%2520and%2520intra-pair%2529%2520to%2520assess%2520cross-action%2520discrimination%2520and%2520temporal%2520direction%2520understanding.%2520Although%2520action%2520distinctions%2520are%2520generally%2520intuitive%2520for%2520human%2520observers%252C%2520our%2520experiments%2520show%2520that%2520they%2520remain%2520challenging%2520for%2520state-of-the-art%2520vision%2520and%2520multimodal%2520models.%250A%2520%2520Leveraging%2520the%2520visual%2520reasoning%2520and%2520instruction-following%2520capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520we%2520present%2520a%2520training-free%2520framework%2520for%2520producing%2520interpretable%2520embeddings%2520from%2520MLLM-generated%2520descriptions%2520for%2520both%2520images%2520and%2520videos.%2520The%2520framework%2520achieves%2520strong%2520performance%2520on%2520SOVABench%2520as%2520well%2520as%2520on%2520several%2520spatial%2520and%2520counting%2520benchmarks%2520where%2520contrastive%2520Vision-Language%2520Models%2520often%2520fail.%2520The%2520code%252C%2520annotations%252C%2520and%2520instructions%2520to%2520construct%2520the%2520benchmark%2520are%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOVABench%3A%20A%20Vehicle%20Surveillance%20Action%20Retrieval%20Benchmark%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Oriol%20Rabasseda%20and%20Zenjie%20Li%20and%20Kamal%20Nasrollahi%20and%20Sergio%20Escalera&entry.1292438233=Automatic%20identification%20of%20events%20and%20recurrent%20behavior%20analysis%20are%20critical%20for%20video%20surveillance.%20However%2C%20most%20existing%20content-based%20video%20retrieval%20benchmarks%20focus%20on%20scene-level%20similarity%20and%20do%20not%20evaluate%20the%20action%20discrimination%20required%20in%20surveillance.%20To%20address%20this%20gap%2C%20we%20introduce%20SOVABench%20%28Surveillance%20Opposite%20Vehicle%20Actions%20Benchmark%29%2C%20a%20real-world%20retrieval%20benchmark%20built%20from%20surveillance%20footage%20and%20centered%20on%20vehicle-related%20actions.%20SOVABench%20defines%20two%20evaluation%20protocols%20%28inter-pair%20and%20intra-pair%29%20to%20assess%20cross-action%20discrimination%20and%20temporal%20direction%20understanding.%20Although%20action%20distinctions%20are%20generally%20intuitive%20for%20human%20observers%2C%20our%20experiments%20show%20that%20they%20remain%20challenging%20for%20state-of-the-art%20vision%20and%20multimodal%20models.%0A%20%20Leveraging%20the%20visual%20reasoning%20and%20instruction-following%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20we%20present%20a%20training-free%20framework%20for%20producing%20interpretable%20embeddings%20from%20MLLM-generated%20descriptions%20for%20both%20images%20and%20videos.%20The%20framework%20achieves%20strong%20performance%20on%20SOVABench%20as%20well%20as%20on%20several%20spatial%20and%20counting%20benchmarks%20where%20contrastive%20Vision-Language%20Models%20often%20fail.%20The%20code%2C%20annotations%2C%20and%20instructions%20to%20construct%20the%20benchmark%20are%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.04824v2&entry.124074799=Read"},
{"title": "A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning", "author": "Md Sultanul Islam Ovi and Muhsina Tarannum Munfa and Miftahul Alam Adib and Syed Sabbir Hasan", "abstract": "Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper presents a Dual Pipeline Machine Learning Framework for multi class sleep disorder screening using the Sleep Health and Lifestyle dataset. The framework consists of two parallel processing streams: a statistical pipeline that targets linear separability using Mutual Information and Linear Discriminant Analysis, and a wrapper based pipeline that applies Boruta feature selection with an autoencoder for non linear representation learning. To address class imbalance, we use the hybrid SMOTETomek resampling strategy. In experiments, Extra Trees and K Nearest Neighbors achieved an accuracy of 98.67%, outperforming recent baselines on the same dataset. Statistical testing using the Wilcoxon Signed Rank Test indicates that the improvement over baseline configurations is significant, and inference latency remains below 400 milliseconds. These results suggest that the proposed dual pipeline design supports accurate and efficient automated screening for non invasive sleep disorder risk stratification.", "link": "http://arxiv.org/abs/2601.05814v1", "date": "2026-01-09", "relevancy": 2.4074, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dual%20Pipeline%20Machine%20Learning%20Framework%20for%20Automated%20Multi%20Class%20Sleep%20Disorder%20Screening%20Using%20Hybrid%20Resampling%20and%20Ensemble%20Learning&body=Title%3A%20A%20Dual%20Pipeline%20Machine%20Learning%20Framework%20for%20Automated%20Multi%20Class%20Sleep%20Disorder%20Screening%20Using%20Hybrid%20Resampling%20and%20Ensemble%20Learning%0AAuthor%3A%20Md%20Sultanul%20Islam%20Ovi%20and%20Muhsina%20Tarannum%20Munfa%20and%20Miftahul%20Alam%20Adib%20and%20Syed%20Sabbir%20Hasan%0AAbstract%3A%20Accurate%20classification%20of%20sleep%20disorders%2C%20particularly%20insomnia%20and%20sleep%20apnea%2C%20is%20important%20for%20reducing%20long%20term%20health%20risks%20and%20improving%20patient%20quality%20of%20life.%20However%2C%20clinical%20sleep%20studies%20are%20resource%20intensive%20and%20are%20difficult%20to%20scale%20for%20population%20level%20screening.%20This%20paper%20presents%20a%20Dual%20Pipeline%20Machine%20Learning%20Framework%20for%20multi%20class%20sleep%20disorder%20screening%20using%20the%20Sleep%20Health%20and%20Lifestyle%20dataset.%20The%20framework%20consists%20of%20two%20parallel%20processing%20streams%3A%20a%20statistical%20pipeline%20that%20targets%20linear%20separability%20using%20Mutual%20Information%20and%20Linear%20Discriminant%20Analysis%2C%20and%20a%20wrapper%20based%20pipeline%20that%20applies%20Boruta%20feature%20selection%20with%20an%20autoencoder%20for%20non%20linear%20representation%20learning.%20To%20address%20class%20imbalance%2C%20we%20use%20the%20hybrid%20SMOTETomek%20resampling%20strategy.%20In%20experiments%2C%20Extra%20Trees%20and%20K%20Nearest%20Neighbors%20achieved%20an%20accuracy%20of%2098.67%25%2C%20outperforming%20recent%20baselines%20on%20the%20same%20dataset.%20Statistical%20testing%20using%20the%20Wilcoxon%20Signed%20Rank%20Test%20indicates%20that%20the%20improvement%20over%20baseline%20configurations%20is%20significant%2C%20and%20inference%20latency%20remains%20below%20400%20milliseconds.%20These%20results%20suggest%20that%20the%20proposed%20dual%20pipeline%20design%20supports%20accurate%20and%20efficient%20automated%20screening%20for%20non%20invasive%20sleep%20disorder%20risk%20stratification.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dual%2520Pipeline%2520Machine%2520Learning%2520Framework%2520for%2520Automated%2520Multi%2520Class%2520Sleep%2520Disorder%2520Screening%2520Using%2520Hybrid%2520Resampling%2520and%2520Ensemble%2520Learning%26entry.906535625%3DMd%2520Sultanul%2520Islam%2520Ovi%2520and%2520Muhsina%2520Tarannum%2520Munfa%2520and%2520Miftahul%2520Alam%2520Adib%2520and%2520Syed%2520Sabbir%2520Hasan%26entry.1292438233%3DAccurate%2520classification%2520of%2520sleep%2520disorders%252C%2520particularly%2520insomnia%2520and%2520sleep%2520apnea%252C%2520is%2520important%2520for%2520reducing%2520long%2520term%2520health%2520risks%2520and%2520improving%2520patient%2520quality%2520of%2520life.%2520However%252C%2520clinical%2520sleep%2520studies%2520are%2520resource%2520intensive%2520and%2520are%2520difficult%2520to%2520scale%2520for%2520population%2520level%2520screening.%2520This%2520paper%2520presents%2520a%2520Dual%2520Pipeline%2520Machine%2520Learning%2520Framework%2520for%2520multi%2520class%2520sleep%2520disorder%2520screening%2520using%2520the%2520Sleep%2520Health%2520and%2520Lifestyle%2520dataset.%2520The%2520framework%2520consists%2520of%2520two%2520parallel%2520processing%2520streams%253A%2520a%2520statistical%2520pipeline%2520that%2520targets%2520linear%2520separability%2520using%2520Mutual%2520Information%2520and%2520Linear%2520Discriminant%2520Analysis%252C%2520and%2520a%2520wrapper%2520based%2520pipeline%2520that%2520applies%2520Boruta%2520feature%2520selection%2520with%2520an%2520autoencoder%2520for%2520non%2520linear%2520representation%2520learning.%2520To%2520address%2520class%2520imbalance%252C%2520we%2520use%2520the%2520hybrid%2520SMOTETomek%2520resampling%2520strategy.%2520In%2520experiments%252C%2520Extra%2520Trees%2520and%2520K%2520Nearest%2520Neighbors%2520achieved%2520an%2520accuracy%2520of%252098.67%2525%252C%2520outperforming%2520recent%2520baselines%2520on%2520the%2520same%2520dataset.%2520Statistical%2520testing%2520using%2520the%2520Wilcoxon%2520Signed%2520Rank%2520Test%2520indicates%2520that%2520the%2520improvement%2520over%2520baseline%2520configurations%2520is%2520significant%252C%2520and%2520inference%2520latency%2520remains%2520below%2520400%2520milliseconds.%2520These%2520results%2520suggest%2520that%2520the%2520proposed%2520dual%2520pipeline%2520design%2520supports%2520accurate%2520and%2520efficient%2520automated%2520screening%2520for%2520non%2520invasive%2520sleep%2520disorder%2520risk%2520stratification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dual%20Pipeline%20Machine%20Learning%20Framework%20for%20Automated%20Multi%20Class%20Sleep%20Disorder%20Screening%20Using%20Hybrid%20Resampling%20and%20Ensemble%20Learning&entry.906535625=Md%20Sultanul%20Islam%20Ovi%20and%20Muhsina%20Tarannum%20Munfa%20and%20Miftahul%20Alam%20Adib%20and%20Syed%20Sabbir%20Hasan&entry.1292438233=Accurate%20classification%20of%20sleep%20disorders%2C%20particularly%20insomnia%20and%20sleep%20apnea%2C%20is%20important%20for%20reducing%20long%20term%20health%20risks%20and%20improving%20patient%20quality%20of%20life.%20However%2C%20clinical%20sleep%20studies%20are%20resource%20intensive%20and%20are%20difficult%20to%20scale%20for%20population%20level%20screening.%20This%20paper%20presents%20a%20Dual%20Pipeline%20Machine%20Learning%20Framework%20for%20multi%20class%20sleep%20disorder%20screening%20using%20the%20Sleep%20Health%20and%20Lifestyle%20dataset.%20The%20framework%20consists%20of%20two%20parallel%20processing%20streams%3A%20a%20statistical%20pipeline%20that%20targets%20linear%20separability%20using%20Mutual%20Information%20and%20Linear%20Discriminant%20Analysis%2C%20and%20a%20wrapper%20based%20pipeline%20that%20applies%20Boruta%20feature%20selection%20with%20an%20autoencoder%20for%20non%20linear%20representation%20learning.%20To%20address%20class%20imbalance%2C%20we%20use%20the%20hybrid%20SMOTETomek%20resampling%20strategy.%20In%20experiments%2C%20Extra%20Trees%20and%20K%20Nearest%20Neighbors%20achieved%20an%20accuracy%20of%2098.67%25%2C%20outperforming%20recent%20baselines%20on%20the%20same%20dataset.%20Statistical%20testing%20using%20the%20Wilcoxon%20Signed%20Rank%20Test%20indicates%20that%20the%20improvement%20over%20baseline%20configurations%20is%20significant%2C%20and%20inference%20latency%20remains%20below%20400%20milliseconds.%20These%20results%20suggest%20that%20the%20proposed%20dual%20pipeline%20design%20supports%20accurate%20and%20efficient%20automated%20screening%20for%20non%20invasive%20sleep%20disorder%20risk%20stratification.&entry.1838667208=http%3A//arxiv.org/abs/2601.05814v1&entry.124074799=Read"},
{"title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction", "author": "Longbin Ji and Xiaoxiong Liu and Junyuan Shang and Shuohuan Wang and Yu Sun and Hua Wu and Haifeng Wang", "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.", "link": "http://arxiv.org/abs/2601.05966v1", "date": "2026-01-09", "relevancy": 2.3699, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6072}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5907}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAR%3A%20Autoregressive%20Video%20Generation%20via%20Next-Frame%20%26%20Scale%20Prediction&body=Title%3A%20VideoAR%3A%20Autoregressive%20Video%20Generation%20via%20Next-Frame%20%26%20Scale%20Prediction%0AAuthor%3A%20Longbin%20Ji%20and%20Xiaoxiong%20Liu%20and%20Junyuan%20Shang%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%20and%20Haifeng%20Wang%0AAbstract%3A%20Recent%20advances%20in%20video%20generation%20have%20been%20dominated%20by%20diffusion%20and%20flow-matching%20models%2C%20which%20produce%20high-quality%20results%20but%20remain%20computationally%20intensive%20and%20difficult%20to%20scale.%20In%20this%20work%2C%20we%20introduce%20VideoAR%2C%20the%20first%20large-scale%20Visual%20Autoregressive%20%28VAR%29%20framework%20for%20video%20generation%20that%20combines%20multi-scale%20next-frame%20prediction%20with%20autoregressive%20modeling.%20VideoAR%20disentangles%20spatial%20and%20temporal%20dependencies%20by%20integrating%20intra-frame%20VAR%20modeling%20with%20causal%20next-frame%20prediction%2C%20supported%20by%20a%203D%20multi-scale%20tokenizer%20that%20efficiently%20encodes%20spatio-temporal%20dynamics.%20To%20improve%20long-term%20consistency%2C%20we%20propose%20Multi-scale%20Temporal%20RoPE%2C%20Cross-Frame%20Error%20Correction%2C%20and%20Random%20Frame%20Mask%2C%20which%20collectively%20mitigate%20error%20propagation%20and%20stabilize%20temporal%20coherence.%20Our%20multi-stage%20pretraining%20pipeline%20progressively%20aligns%20spatial%20and%20temporal%20learning%20across%20increasing%20resolutions%20and%20durations.%20Empirically%2C%20VideoAR%20achieves%20new%20state-of-the-art%20results%20among%20autoregressive%20models%2C%20improving%20FVD%20on%20UCF-101%20from%2099.5%20to%2088.6%20while%20reducing%20inference%20steps%20by%20over%2010x%2C%20and%20reaching%20a%20VBench%20score%20of%2081.74-competitive%20with%20diffusion-based%20models%20an%20order%20of%20magnitude%20larger.%20These%20results%20demonstrate%20that%20VideoAR%20narrows%20the%20performance%20gap%20between%20autoregressive%20and%20diffusion%20paradigms%2C%20offering%20a%20scalable%2C%20efficient%2C%20and%20temporally%20consistent%20foundation%20for%20future%20video%20generation%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAR%253A%2520Autoregressive%2520Video%2520Generation%2520via%2520Next-Frame%2520%2526%2520Scale%2520Prediction%26entry.906535625%3DLongbin%2520Ji%2520and%2520Xiaoxiong%2520Liu%2520and%2520Junyuan%2520Shang%2520and%2520Shuohuan%2520Wang%2520and%2520Yu%2520Sun%2520and%2520Hua%2520Wu%2520and%2520Haifeng%2520Wang%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520generation%2520have%2520been%2520dominated%2520by%2520diffusion%2520and%2520flow-matching%2520models%252C%2520which%2520produce%2520high-quality%2520results%2520but%2520remain%2520computationally%2520intensive%2520and%2520difficult%2520to%2520scale.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VideoAR%252C%2520the%2520first%2520large-scale%2520Visual%2520Autoregressive%2520%2528VAR%2529%2520framework%2520for%2520video%2520generation%2520that%2520combines%2520multi-scale%2520next-frame%2520prediction%2520with%2520autoregressive%2520modeling.%2520VideoAR%2520disentangles%2520spatial%2520and%2520temporal%2520dependencies%2520by%2520integrating%2520intra-frame%2520VAR%2520modeling%2520with%2520causal%2520next-frame%2520prediction%252C%2520supported%2520by%2520a%25203D%2520multi-scale%2520tokenizer%2520that%2520efficiently%2520encodes%2520spatio-temporal%2520dynamics.%2520To%2520improve%2520long-term%2520consistency%252C%2520we%2520propose%2520Multi-scale%2520Temporal%2520RoPE%252C%2520Cross-Frame%2520Error%2520Correction%252C%2520and%2520Random%2520Frame%2520Mask%252C%2520which%2520collectively%2520mitigate%2520error%2520propagation%2520and%2520stabilize%2520temporal%2520coherence.%2520Our%2520multi-stage%2520pretraining%2520pipeline%2520progressively%2520aligns%2520spatial%2520and%2520temporal%2520learning%2520across%2520increasing%2520resolutions%2520and%2520durations.%2520Empirically%252C%2520VideoAR%2520achieves%2520new%2520state-of-the-art%2520results%2520among%2520autoregressive%2520models%252C%2520improving%2520FVD%2520on%2520UCF-101%2520from%252099.5%2520to%252088.6%2520while%2520reducing%2520inference%2520steps%2520by%2520over%252010x%252C%2520and%2520reaching%2520a%2520VBench%2520score%2520of%252081.74-competitive%2520with%2520diffusion-based%2520models%2520an%2520order%2520of%2520magnitude%2520larger.%2520These%2520results%2520demonstrate%2520that%2520VideoAR%2520narrows%2520the%2520performance%2520gap%2520between%2520autoregressive%2520and%2520diffusion%2520paradigms%252C%2520offering%2520a%2520scalable%252C%2520efficient%252C%2520and%2520temporally%2520consistent%2520foundation%2520for%2520future%2520video%2520generation%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAR%3A%20Autoregressive%20Video%20Generation%20via%20Next-Frame%20%26%20Scale%20Prediction&entry.906535625=Longbin%20Ji%20and%20Xiaoxiong%20Liu%20and%20Junyuan%20Shang%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%20and%20Haifeng%20Wang&entry.1292438233=Recent%20advances%20in%20video%20generation%20have%20been%20dominated%20by%20diffusion%20and%20flow-matching%20models%2C%20which%20produce%20high-quality%20results%20but%20remain%20computationally%20intensive%20and%20difficult%20to%20scale.%20In%20this%20work%2C%20we%20introduce%20VideoAR%2C%20the%20first%20large-scale%20Visual%20Autoregressive%20%28VAR%29%20framework%20for%20video%20generation%20that%20combines%20multi-scale%20next-frame%20prediction%20with%20autoregressive%20modeling.%20VideoAR%20disentangles%20spatial%20and%20temporal%20dependencies%20by%20integrating%20intra-frame%20VAR%20modeling%20with%20causal%20next-frame%20prediction%2C%20supported%20by%20a%203D%20multi-scale%20tokenizer%20that%20efficiently%20encodes%20spatio-temporal%20dynamics.%20To%20improve%20long-term%20consistency%2C%20we%20propose%20Multi-scale%20Temporal%20RoPE%2C%20Cross-Frame%20Error%20Correction%2C%20and%20Random%20Frame%20Mask%2C%20which%20collectively%20mitigate%20error%20propagation%20and%20stabilize%20temporal%20coherence.%20Our%20multi-stage%20pretraining%20pipeline%20progressively%20aligns%20spatial%20and%20temporal%20learning%20across%20increasing%20resolutions%20and%20durations.%20Empirically%2C%20VideoAR%20achieves%20new%20state-of-the-art%20results%20among%20autoregressive%20models%2C%20improving%20FVD%20on%20UCF-101%20from%2099.5%20to%2088.6%20while%20reducing%20inference%20steps%20by%20over%2010x%2C%20and%20reaching%20a%20VBench%20score%20of%2081.74-competitive%20with%20diffusion-based%20models%20an%20order%20of%20magnitude%20larger.%20These%20results%20demonstrate%20that%20VideoAR%20narrows%20the%20performance%20gap%20between%20autoregressive%20and%20diffusion%20paradigms%2C%20offering%20a%20scalable%2C%20efficient%2C%20and%20temporally%20consistent%20foundation%20for%20future%20video%20generation%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.05966v1&entry.124074799=Read"},
{"title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?", "author": "George Ma and Zhongyuan Liang and Irene Y. Chen and Somayeh Sojoudi", "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.", "link": "http://arxiv.org/abs/2601.05679v1", "date": "2026-01-09", "relevancy": 2.3675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F&body=Title%3A%20Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F%0AAuthor%3A%20George%20Ma%20and%20Zhongyuan%20Liang%20and%20Irene%20Y.%20Chen%20and%20Somayeh%20Sojoudi%0AAbstract%3A%20We%20investigate%20whether%20sparse%20autoencoders%20%28SAEs%29%20identify%20genuine%20reasoning%20features%20in%20large%20language%20models%20%28LLMs%29.%20Starting%20from%20features%20selected%20using%20standard%20contrastive%20activation%20methods%2C%20we%20introduce%20a%20falsification-oriented%20framework%20that%20combines%20causal%20token%20injection%20experiments%20and%20LLM-guided%20falsification%20to%20test%20whether%20feature%20activation%20reflects%20reasoning%20processes%20or%20superficial%20linguistic%20correlates.%20Across%2020%20configurations%20spanning%20multiple%20model%20families%2C%20layers%2C%20and%20reasoning%20datasets%2C%20we%20find%20that%20identified%20reasoning%20features%20are%20highly%20sensitive%20to%20token-level%20interventions.%20Injecting%20a%20small%20number%20of%20feature-associated%20tokens%20into%20non-reasoning%20text%20is%20sufficient%20to%20elicit%20strong%20activation%20for%2059%25%20to%2094%25%20of%20features%2C%20indicating%20reliance%20on%20lexical%20artifacts.%20For%20the%20remaining%20features%20that%20are%20not%20explained%20by%20simple%20token%20triggers%2C%20LLM-guided%20falsification%20consistently%20produces%20non-reasoning%20inputs%20that%20activate%20the%20feature%20and%20reasoning%20inputs%20that%20do%20not%2C%20with%20no%20analyzed%20feature%20satisfying%20our%20criteria%20for%20genuine%20reasoning%20behavior.%20Steering%20these%20features%20yields%20minimal%20changes%20or%20slight%20degradations%20in%20benchmark%20performance.%20Together%2C%20these%20results%20suggest%20that%20SAE%20features%20identified%20by%20contrastive%20approaches%20primarily%20capture%20linguistic%20correlates%20of%20reasoning%20rather%20than%20the%20underlying%20reasoning%20computations%20themselves.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Sparse%2520Autoencoders%2520Identify%2520Reasoning%2520Features%2520in%2520Language%2520Models%253F%26entry.906535625%3DGeorge%2520Ma%2520and%2520Zhongyuan%2520Liang%2520and%2520Irene%2520Y.%2520Chen%2520and%2520Somayeh%2520Sojoudi%26entry.1292438233%3DWe%2520investigate%2520whether%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520identify%2520genuine%2520reasoning%2520features%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Starting%2520from%2520features%2520selected%2520using%2520standard%2520contrastive%2520activation%2520methods%252C%2520we%2520introduce%2520a%2520falsification-oriented%2520framework%2520that%2520combines%2520causal%2520token%2520injection%2520experiments%2520and%2520LLM-guided%2520falsification%2520to%2520test%2520whether%2520feature%2520activation%2520reflects%2520reasoning%2520processes%2520or%2520superficial%2520linguistic%2520correlates.%2520Across%252020%2520configurations%2520spanning%2520multiple%2520model%2520families%252C%2520layers%252C%2520and%2520reasoning%2520datasets%252C%2520we%2520find%2520that%2520identified%2520reasoning%2520features%2520are%2520highly%2520sensitive%2520to%2520token-level%2520interventions.%2520Injecting%2520a%2520small%2520number%2520of%2520feature-associated%2520tokens%2520into%2520non-reasoning%2520text%2520is%2520sufficient%2520to%2520elicit%2520strong%2520activation%2520for%252059%2525%2520to%252094%2525%2520of%2520features%252C%2520indicating%2520reliance%2520on%2520lexical%2520artifacts.%2520For%2520the%2520remaining%2520features%2520that%2520are%2520not%2520explained%2520by%2520simple%2520token%2520triggers%252C%2520LLM-guided%2520falsification%2520consistently%2520produces%2520non-reasoning%2520inputs%2520that%2520activate%2520the%2520feature%2520and%2520reasoning%2520inputs%2520that%2520do%2520not%252C%2520with%2520no%2520analyzed%2520feature%2520satisfying%2520our%2520criteria%2520for%2520genuine%2520reasoning%2520behavior.%2520Steering%2520these%2520features%2520yields%2520minimal%2520changes%2520or%2520slight%2520degradations%2520in%2520benchmark%2520performance.%2520Together%252C%2520these%2520results%2520suggest%2520that%2520SAE%2520features%2520identified%2520by%2520contrastive%2520approaches%2520primarily%2520capture%2520linguistic%2520correlates%2520of%2520reasoning%2520rather%2520than%2520the%2520underlying%2520reasoning%2520computations%2520themselves.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F&entry.906535625=George%20Ma%20and%20Zhongyuan%20Liang%20and%20Irene%20Y.%20Chen%20and%20Somayeh%20Sojoudi&entry.1292438233=We%20investigate%20whether%20sparse%20autoencoders%20%28SAEs%29%20identify%20genuine%20reasoning%20features%20in%20large%20language%20models%20%28LLMs%29.%20Starting%20from%20features%20selected%20using%20standard%20contrastive%20activation%20methods%2C%20we%20introduce%20a%20falsification-oriented%20framework%20that%20combines%20causal%20token%20injection%20experiments%20and%20LLM-guided%20falsification%20to%20test%20whether%20feature%20activation%20reflects%20reasoning%20processes%20or%20superficial%20linguistic%20correlates.%20Across%2020%20configurations%20spanning%20multiple%20model%20families%2C%20layers%2C%20and%20reasoning%20datasets%2C%20we%20find%20that%20identified%20reasoning%20features%20are%20highly%20sensitive%20to%20token-level%20interventions.%20Injecting%20a%20small%20number%20of%20feature-associated%20tokens%20into%20non-reasoning%20text%20is%20sufficient%20to%20elicit%20strong%20activation%20for%2059%25%20to%2094%25%20of%20features%2C%20indicating%20reliance%20on%20lexical%20artifacts.%20For%20the%20remaining%20features%20that%20are%20not%20explained%20by%20simple%20token%20triggers%2C%20LLM-guided%20falsification%20consistently%20produces%20non-reasoning%20inputs%20that%20activate%20the%20feature%20and%20reasoning%20inputs%20that%20do%20not%2C%20with%20no%20analyzed%20feature%20satisfying%20our%20criteria%20for%20genuine%20reasoning%20behavior.%20Steering%20these%20features%20yields%20minimal%20changes%20or%20slight%20degradations%20in%20benchmark%20performance.%20Together%2C%20these%20results%20suggest%20that%20SAE%20features%20identified%20by%20contrastive%20approaches%20primarily%20capture%20linguistic%20correlates%20of%20reasoning%20rather%20than%20the%20underlying%20reasoning%20computations%20themselves.&entry.1838667208=http%3A//arxiv.org/abs/2601.05679v1&entry.124074799=Read"},
{"title": "Unsupervised Domain Adaptation for Binary Classification with an Unobservable Source Subpopulation", "author": "Chao Ying and Jun Jin and Haotian Zhang and Qinglong Tian and Yanyuan Ma and Yixuan Li and Jiwei Zhao", "abstract": "We study an unsupervised domain adaptation problem where the source domain consists of subpopulations defined by the binary label $Y$ and a binary background (or environment) $A$. We focus on a challenging setting in which one such subpopulation in the source domain is unobservable. Naively ignoring this unobserved group can result in biased estimates and degraded predictive performance. Despite this structured missingness, we show that the prediction in the target domain can still be recovered. Specifically, we rigorously derive both background-specific and overall prediction models for the target domain. For practical implementation, we propose the distribution matching method to estimate the subpopulation proportions. We provide theoretical guarantees for the asymptotic behavior of our estimator, and establish an upper bound on the prediction error. Experiments on both synthetic and real-world datasets show that our method outperforms the naive benchmark that does not account for this unobservable source subpopulation.", "link": "http://arxiv.org/abs/2509.20587v2", "date": "2026-01-09", "relevancy": 2.3587, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5057}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4602}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Domain%20Adaptation%20for%20Binary%20Classification%20with%20an%20Unobservable%20Source%20Subpopulation&body=Title%3A%20Unsupervised%20Domain%20Adaptation%20for%20Binary%20Classification%20with%20an%20Unobservable%20Source%20Subpopulation%0AAuthor%3A%20Chao%20Ying%20and%20Jun%20Jin%20and%20Haotian%20Zhang%20and%20Qinglong%20Tian%20and%20Yanyuan%20Ma%20and%20Yixuan%20Li%20and%20Jiwei%20Zhao%0AAbstract%3A%20We%20study%20an%20unsupervised%20domain%20adaptation%20problem%20where%20the%20source%20domain%20consists%20of%20subpopulations%20defined%20by%20the%20binary%20label%20%24Y%24%20and%20a%20binary%20background%20%28or%20environment%29%20%24A%24.%20We%20focus%20on%20a%20challenging%20setting%20in%20which%20one%20such%20subpopulation%20in%20the%20source%20domain%20is%20unobservable.%20Naively%20ignoring%20this%20unobserved%20group%20can%20result%20in%20biased%20estimates%20and%20degraded%20predictive%20performance.%20Despite%20this%20structured%20missingness%2C%20we%20show%20that%20the%20prediction%20in%20the%20target%20domain%20can%20still%20be%20recovered.%20Specifically%2C%20we%20rigorously%20derive%20both%20background-specific%20and%20overall%20prediction%20models%20for%20the%20target%20domain.%20For%20practical%20implementation%2C%20we%20propose%20the%20distribution%20matching%20method%20to%20estimate%20the%20subpopulation%20proportions.%20We%20provide%20theoretical%20guarantees%20for%20the%20asymptotic%20behavior%20of%20our%20estimator%2C%20and%20establish%20an%20upper%20bound%20on%20the%20prediction%20error.%20Experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20show%20that%20our%20method%20outperforms%20the%20naive%20benchmark%20that%20does%20not%20account%20for%20this%20unobservable%20source%20subpopulation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Domain%2520Adaptation%2520for%2520Binary%2520Classification%2520with%2520an%2520Unobservable%2520Source%2520Subpopulation%26entry.906535625%3DChao%2520Ying%2520and%2520Jun%2520Jin%2520and%2520Haotian%2520Zhang%2520and%2520Qinglong%2520Tian%2520and%2520Yanyuan%2520Ma%2520and%2520Yixuan%2520Li%2520and%2520Jiwei%2520Zhao%26entry.1292438233%3DWe%2520study%2520an%2520unsupervised%2520domain%2520adaptation%2520problem%2520where%2520the%2520source%2520domain%2520consists%2520of%2520subpopulations%2520defined%2520by%2520the%2520binary%2520label%2520%2524Y%2524%2520and%2520a%2520binary%2520background%2520%2528or%2520environment%2529%2520%2524A%2524.%2520We%2520focus%2520on%2520a%2520challenging%2520setting%2520in%2520which%2520one%2520such%2520subpopulation%2520in%2520the%2520source%2520domain%2520is%2520unobservable.%2520Naively%2520ignoring%2520this%2520unobserved%2520group%2520can%2520result%2520in%2520biased%2520estimates%2520and%2520degraded%2520predictive%2520performance.%2520Despite%2520this%2520structured%2520missingness%252C%2520we%2520show%2520that%2520the%2520prediction%2520in%2520the%2520target%2520domain%2520can%2520still%2520be%2520recovered.%2520Specifically%252C%2520we%2520rigorously%2520derive%2520both%2520background-specific%2520and%2520overall%2520prediction%2520models%2520for%2520the%2520target%2520domain.%2520For%2520practical%2520implementation%252C%2520we%2520propose%2520the%2520distribution%2520matching%2520method%2520to%2520estimate%2520the%2520subpopulation%2520proportions.%2520We%2520provide%2520theoretical%2520guarantees%2520for%2520the%2520asymptotic%2520behavior%2520of%2520our%2520estimator%252C%2520and%2520establish%2520an%2520upper%2520bound%2520on%2520the%2520prediction%2520error.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520show%2520that%2520our%2520method%2520outperforms%2520the%2520naive%2520benchmark%2520that%2520does%2520not%2520account%2520for%2520this%2520unobservable%2520source%2520subpopulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Domain%20Adaptation%20for%20Binary%20Classification%20with%20an%20Unobservable%20Source%20Subpopulation&entry.906535625=Chao%20Ying%20and%20Jun%20Jin%20and%20Haotian%20Zhang%20and%20Qinglong%20Tian%20and%20Yanyuan%20Ma%20and%20Yixuan%20Li%20and%20Jiwei%20Zhao&entry.1292438233=We%20study%20an%20unsupervised%20domain%20adaptation%20problem%20where%20the%20source%20domain%20consists%20of%20subpopulations%20defined%20by%20the%20binary%20label%20%24Y%24%20and%20a%20binary%20background%20%28or%20environment%29%20%24A%24.%20We%20focus%20on%20a%20challenging%20setting%20in%20which%20one%20such%20subpopulation%20in%20the%20source%20domain%20is%20unobservable.%20Naively%20ignoring%20this%20unobserved%20group%20can%20result%20in%20biased%20estimates%20and%20degraded%20predictive%20performance.%20Despite%20this%20structured%20missingness%2C%20we%20show%20that%20the%20prediction%20in%20the%20target%20domain%20can%20still%20be%20recovered.%20Specifically%2C%20we%20rigorously%20derive%20both%20background-specific%20and%20overall%20prediction%20models%20for%20the%20target%20domain.%20For%20practical%20implementation%2C%20we%20propose%20the%20distribution%20matching%20method%20to%20estimate%20the%20subpopulation%20proportions.%20We%20provide%20theoretical%20guarantees%20for%20the%20asymptotic%20behavior%20of%20our%20estimator%2C%20and%20establish%20an%20upper%20bound%20on%20the%20prediction%20error.%20Experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20show%20that%20our%20method%20outperforms%20the%20naive%20benchmark%20that%20does%20not%20account%20for%20this%20unobservable%20source%20subpopulation.&entry.1838667208=http%3A//arxiv.org/abs/2509.20587v2&entry.124074799=Read"},
{"title": "Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens", "author": "Yohann Perron and Vladyslav Sydorov and Christophe Pottier and Loic Landrieu", "abstract": "Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .", "link": "http://arxiv.org/abs/2601.05927v1", "date": "2026-01-09", "relevancy": 2.3481, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6026}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Vision%20Transformers%20to%20Ultra-High%20Resolution%20Semantic%20Segmentation%20with%20Relay%20Tokens&body=Title%3A%20Adapting%20Vision%20Transformers%20to%20Ultra-High%20Resolution%20Semantic%20Segmentation%20with%20Relay%20Tokens%0AAuthor%3A%20Yohann%20Perron%20and%20Vladyslav%20Sydorov%20and%20Christophe%20Pottier%20and%20Loic%20Landrieu%0AAbstract%3A%20Current%20approaches%20for%20segmenting%20ultra%20high%20resolution%20images%20either%20slide%20a%20window%2C%20thereby%20discarding%20global%20context%2C%20or%20downsample%20and%20lose%20fine%20detail.%20We%20propose%20a%20simple%20yet%20effective%20method%20that%20brings%20explicit%20multi%20scale%20reasoning%20to%20vision%20transformers%2C%20simultaneously%20preserving%20local%20details%20and%20global%20awareness.%20Concretely%2C%20we%20process%20each%20image%20in%20parallel%20at%20a%20local%20scale%20%28high%20resolution%2C%20small%20crops%29%20and%20a%20global%20scale%20%28low%20resolution%2C%20large%20crops%29%2C%20and%20aggregate%20and%20propagate%20features%20between%20the%20two%20branches%20with%20a%20small%20set%20of%20learnable%20relay%20tokens.%20The%20design%20plugs%20directly%20into%20standard%20transformer%20backbones%20%28eg%20ViT%20and%20Swin%29%20and%20adds%20fewer%20than%202%20%25%20parameters.%20Extensive%20experiments%20on%20three%20ultra%20high%20resolution%20segmentation%20benchmarks%2C%20Archaeoscape%2C%20URUR%2C%20and%20Gleason%2C%20and%20on%20the%20conventional%20Cityscapes%20dataset%20show%20consistent%20gains%2C%20with%20up%20to%2015%20%25%20relative%20mIoU%20improvement.%20Code%20and%20pretrained%20models%20are%20available%20at%20https%3A//archaeoscape.ai/work/relay-tokens/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Vision%2520Transformers%2520to%2520Ultra-High%2520Resolution%2520Semantic%2520Segmentation%2520with%2520Relay%2520Tokens%26entry.906535625%3DYohann%2520Perron%2520and%2520Vladyslav%2520Sydorov%2520and%2520Christophe%2520Pottier%2520and%2520Loic%2520Landrieu%26entry.1292438233%3DCurrent%2520approaches%2520for%2520segmenting%2520ultra%2520high%2520resolution%2520images%2520either%2520slide%2520a%2520window%252C%2520thereby%2520discarding%2520global%2520context%252C%2520or%2520downsample%2520and%2520lose%2520fine%2520detail.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520method%2520that%2520brings%2520explicit%2520multi%2520scale%2520reasoning%2520to%2520vision%2520transformers%252C%2520simultaneously%2520preserving%2520local%2520details%2520and%2520global%2520awareness.%2520Concretely%252C%2520we%2520process%2520each%2520image%2520in%2520parallel%2520at%2520a%2520local%2520scale%2520%2528high%2520resolution%252C%2520small%2520crops%2529%2520and%2520a%2520global%2520scale%2520%2528low%2520resolution%252C%2520large%2520crops%2529%252C%2520and%2520aggregate%2520and%2520propagate%2520features%2520between%2520the%2520two%2520branches%2520with%2520a%2520small%2520set%2520of%2520learnable%2520relay%2520tokens.%2520The%2520design%2520plugs%2520directly%2520into%2520standard%2520transformer%2520backbones%2520%2528eg%2520ViT%2520and%2520Swin%2529%2520and%2520adds%2520fewer%2520than%25202%2520%2525%2520parameters.%2520Extensive%2520experiments%2520on%2520three%2520ultra%2520high%2520resolution%2520segmentation%2520benchmarks%252C%2520Archaeoscape%252C%2520URUR%252C%2520and%2520Gleason%252C%2520and%2520on%2520the%2520conventional%2520Cityscapes%2520dataset%2520show%2520consistent%2520gains%252C%2520with%2520up%2520to%252015%2520%2525%2520relative%2520mIoU%2520improvement.%2520Code%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//archaeoscape.ai/work/relay-tokens/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Vision%20Transformers%20to%20Ultra-High%20Resolution%20Semantic%20Segmentation%20with%20Relay%20Tokens&entry.906535625=Yohann%20Perron%20and%20Vladyslav%20Sydorov%20and%20Christophe%20Pottier%20and%20Loic%20Landrieu&entry.1292438233=Current%20approaches%20for%20segmenting%20ultra%20high%20resolution%20images%20either%20slide%20a%20window%2C%20thereby%20discarding%20global%20context%2C%20or%20downsample%20and%20lose%20fine%20detail.%20We%20propose%20a%20simple%20yet%20effective%20method%20that%20brings%20explicit%20multi%20scale%20reasoning%20to%20vision%20transformers%2C%20simultaneously%20preserving%20local%20details%20and%20global%20awareness.%20Concretely%2C%20we%20process%20each%20image%20in%20parallel%20at%20a%20local%20scale%20%28high%20resolution%2C%20small%20crops%29%20and%20a%20global%20scale%20%28low%20resolution%2C%20large%20crops%29%2C%20and%20aggregate%20and%20propagate%20features%20between%20the%20two%20branches%20with%20a%20small%20set%20of%20learnable%20relay%20tokens.%20The%20design%20plugs%20directly%20into%20standard%20transformer%20backbones%20%28eg%20ViT%20and%20Swin%29%20and%20adds%20fewer%20than%202%20%25%20parameters.%20Extensive%20experiments%20on%20three%20ultra%20high%20resolution%20segmentation%20benchmarks%2C%20Archaeoscape%2C%20URUR%2C%20and%20Gleason%2C%20and%20on%20the%20conventional%20Cityscapes%20dataset%20show%20consistent%20gains%2C%20with%20up%20to%2015%20%25%20relative%20mIoU%20improvement.%20Code%20and%20pretrained%20models%20are%20available%20at%20https%3A//archaeoscape.ai/work/relay-tokens/%20.&entry.1838667208=http%3A//arxiv.org/abs/2601.05927v1&entry.124074799=Read"},
{"title": "Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs", "author": "Eilam Cohen and Itamar Bul and Danielle Inbar and Omri Loewenbach", "abstract": "Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.", "link": "http://arxiv.org/abs/2601.05794v1", "date": "2026-01-09", "relevancy": 2.3367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4711}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplify-This%3A%20A%20Comparative%20Analysis%20of%20Prompt-Based%20and%20Fine-Tuned%20LLMs&body=Title%3A%20Simplify-This%3A%20A%20Comparative%20Analysis%20of%20Prompt-Based%20and%20Fine-Tuned%20LLMs%0AAuthor%3A%20Eilam%20Cohen%20and%20Itamar%20Bul%20and%20Danielle%20Inbar%20and%20Omri%20Loewenbach%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20enable%20strong%20text%20generation%2C%20and%20in%20general%20there%20is%20a%20practical%20tradeoff%20between%20fine-tuning%20and%20prompt%20engineering.%20We%20introduce%20Simplify-This%2C%20a%20comparative%20study%20evaluating%20both%20paradigms%20for%20text%20simplification%20with%20encoder-decoder%20LLMs%20across%20multiple%20benchmarks%2C%20using%20a%20range%20of%20evaluation%20metrics.%20Fine-tuned%20models%20consistently%20deliver%20stronger%20structural%20simplification%2C%20whereas%20prompting%20often%20attains%20higher%20semantic%20similarity%20scores%20yet%20tends%20to%20copy%20inputs.%20A%20human%20evaluation%20favors%20fine-tuned%20outputs%20overall.%20We%20release%20code%2C%20a%20cleaned%20derivative%20dataset%20used%20in%20our%20study%2C%20checkpoints%20of%20fine-tuned%20models%2C%20and%20prompt%20templates%20to%20facilitate%20reproducibility%20and%20future%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplify-This%253A%2520A%2520Comparative%2520Analysis%2520of%2520Prompt-Based%2520and%2520Fine-Tuned%2520LLMs%26entry.906535625%3DEilam%2520Cohen%2520and%2520Itamar%2520Bul%2520and%2520Danielle%2520Inbar%2520and%2520Omri%2520Loewenbach%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520enable%2520strong%2520text%2520generation%252C%2520and%2520in%2520general%2520there%2520is%2520a%2520practical%2520tradeoff%2520between%2520fine-tuning%2520and%2520prompt%2520engineering.%2520We%2520introduce%2520Simplify-This%252C%2520a%2520comparative%2520study%2520evaluating%2520both%2520paradigms%2520for%2520text%2520simplification%2520with%2520encoder-decoder%2520LLMs%2520across%2520multiple%2520benchmarks%252C%2520using%2520a%2520range%2520of%2520evaluation%2520metrics.%2520Fine-tuned%2520models%2520consistently%2520deliver%2520stronger%2520structural%2520simplification%252C%2520whereas%2520prompting%2520often%2520attains%2520higher%2520semantic%2520similarity%2520scores%2520yet%2520tends%2520to%2520copy%2520inputs.%2520A%2520human%2520evaluation%2520favors%2520fine-tuned%2520outputs%2520overall.%2520We%2520release%2520code%252C%2520a%2520cleaned%2520derivative%2520dataset%2520used%2520in%2520our%2520study%252C%2520checkpoints%2520of%2520fine-tuned%2520models%252C%2520and%2520prompt%2520templates%2520to%2520facilitate%2520reproducibility%2520and%2520future%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplify-This%3A%20A%20Comparative%20Analysis%20of%20Prompt-Based%20and%20Fine-Tuned%20LLMs&entry.906535625=Eilam%20Cohen%20and%20Itamar%20Bul%20and%20Danielle%20Inbar%20and%20Omri%20Loewenbach&entry.1292438233=Large%20language%20models%20%28LLMs%29%20enable%20strong%20text%20generation%2C%20and%20in%20general%20there%20is%20a%20practical%20tradeoff%20between%20fine-tuning%20and%20prompt%20engineering.%20We%20introduce%20Simplify-This%2C%20a%20comparative%20study%20evaluating%20both%20paradigms%20for%20text%20simplification%20with%20encoder-decoder%20LLMs%20across%20multiple%20benchmarks%2C%20using%20a%20range%20of%20evaluation%20metrics.%20Fine-tuned%20models%20consistently%20deliver%20stronger%20structural%20simplification%2C%20whereas%20prompting%20often%20attains%20higher%20semantic%20similarity%20scores%20yet%20tends%20to%20copy%20inputs.%20A%20human%20evaluation%20favors%20fine-tuned%20outputs%20overall.%20We%20release%20code%2C%20a%20cleaned%20derivative%20dataset%20used%20in%20our%20study%2C%20checkpoints%20of%20fine-tuned%20models%2C%20and%20prompt%20templates%20to%20facilitate%20reproducibility%20and%20future%20work.&entry.1838667208=http%3A//arxiv.org/abs/2601.05794v1&entry.124074799=Read"},
{"title": "DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation", "author": "Yutong Liang and Shiyi Xu and Yulong Zhang and Bowen Zhan and He Zhang and Libin Liu", "abstract": "Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-cost vision-based methods often suffer from reduced accuracy and reliability under occlusion. To address these challenges, we present DexterCap, a low-cost optical capture system for dexterous in-hand manipulation. DexterCap uses dense, character-coded marker patches to achieve robust tracking under severe self-occlusion, together with an automated reconstruction pipeline that requires minimal manual effort. With DexterCap, we introduce DexterHand, a dataset of fine-grained hand-object interactions covering diverse manipulation behaviors and objects, from simple primitives to complex articulated objects such as a Rubik's Cube. We release the dataset and code to support future research on dexterous hand-object interaction.", "link": "http://arxiv.org/abs/2601.05844v1", "date": "2026-01-09", "relevancy": 2.3316, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.611}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5716}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexterCap%3A%20An%20Affordable%20and%20Automated%20System%20for%20Capturing%20Dexterous%20Hand-Object%20Manipulation&body=Title%3A%20DexterCap%3A%20An%20Affordable%20and%20Automated%20System%20for%20Capturing%20Dexterous%20Hand-Object%20Manipulation%0AAuthor%3A%20Yutong%20Liang%20and%20Shiyi%20Xu%20and%20Yulong%20Zhang%20and%20Bowen%20Zhan%20and%20He%20Zhang%20and%20Libin%20Liu%0AAbstract%3A%20Capturing%20fine-grained%20hand-object%20interactions%20is%20challenging%20due%20to%20severe%20self-occlusion%20from%20closely%20spaced%20fingers%20and%20the%20subtlety%20of%20in-hand%20manipulation%20motions.%20Existing%20optical%20motion%20capture%20systems%20rely%20on%20expensive%20camera%20setups%20and%20extensive%20manual%20post-processing%2C%20while%20low-cost%20vision-based%20methods%20often%20suffer%20from%20reduced%20accuracy%20and%20reliability%20under%20occlusion.%20To%20address%20these%20challenges%2C%20we%20present%20DexterCap%2C%20a%20low-cost%20optical%20capture%20system%20for%20dexterous%20in-hand%20manipulation.%20DexterCap%20uses%20dense%2C%20character-coded%20marker%20patches%20to%20achieve%20robust%20tracking%20under%20severe%20self-occlusion%2C%20together%20with%20an%20automated%20reconstruction%20pipeline%20that%20requires%20minimal%20manual%20effort.%20With%20DexterCap%2C%20we%20introduce%20DexterHand%2C%20a%20dataset%20of%20fine-grained%20hand-object%20interactions%20covering%20diverse%20manipulation%20behaviors%20and%20objects%2C%20from%20simple%20primitives%20to%20complex%20articulated%20objects%20such%20as%20a%20Rubik%27s%20Cube.%20We%20release%20the%20dataset%20and%20code%20to%20support%20future%20research%20on%20dexterous%20hand-object%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexterCap%253A%2520An%2520Affordable%2520and%2520Automated%2520System%2520for%2520Capturing%2520Dexterous%2520Hand-Object%2520Manipulation%26entry.906535625%3DYutong%2520Liang%2520and%2520Shiyi%2520Xu%2520and%2520Yulong%2520Zhang%2520and%2520Bowen%2520Zhan%2520and%2520He%2520Zhang%2520and%2520Libin%2520Liu%26entry.1292438233%3DCapturing%2520fine-grained%2520hand-object%2520interactions%2520is%2520challenging%2520due%2520to%2520severe%2520self-occlusion%2520from%2520closely%2520spaced%2520fingers%2520and%2520the%2520subtlety%2520of%2520in-hand%2520manipulation%2520motions.%2520Existing%2520optical%2520motion%2520capture%2520systems%2520rely%2520on%2520expensive%2520camera%2520setups%2520and%2520extensive%2520manual%2520post-processing%252C%2520while%2520low-cost%2520vision-based%2520methods%2520often%2520suffer%2520from%2520reduced%2520accuracy%2520and%2520reliability%2520under%2520occlusion.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520DexterCap%252C%2520a%2520low-cost%2520optical%2520capture%2520system%2520for%2520dexterous%2520in-hand%2520manipulation.%2520DexterCap%2520uses%2520dense%252C%2520character-coded%2520marker%2520patches%2520to%2520achieve%2520robust%2520tracking%2520under%2520severe%2520self-occlusion%252C%2520together%2520with%2520an%2520automated%2520reconstruction%2520pipeline%2520that%2520requires%2520minimal%2520manual%2520effort.%2520With%2520DexterCap%252C%2520we%2520introduce%2520DexterHand%252C%2520a%2520dataset%2520of%2520fine-grained%2520hand-object%2520interactions%2520covering%2520diverse%2520manipulation%2520behaviors%2520and%2520objects%252C%2520from%2520simple%2520primitives%2520to%2520complex%2520articulated%2520objects%2520such%2520as%2520a%2520Rubik%2527s%2520Cube.%2520We%2520release%2520the%2520dataset%2520and%2520code%2520to%2520support%2520future%2520research%2520on%2520dexterous%2520hand-object%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexterCap%3A%20An%20Affordable%20and%20Automated%20System%20for%20Capturing%20Dexterous%20Hand-Object%20Manipulation&entry.906535625=Yutong%20Liang%20and%20Shiyi%20Xu%20and%20Yulong%20Zhang%20and%20Bowen%20Zhan%20and%20He%20Zhang%20and%20Libin%20Liu&entry.1292438233=Capturing%20fine-grained%20hand-object%20interactions%20is%20challenging%20due%20to%20severe%20self-occlusion%20from%20closely%20spaced%20fingers%20and%20the%20subtlety%20of%20in-hand%20manipulation%20motions.%20Existing%20optical%20motion%20capture%20systems%20rely%20on%20expensive%20camera%20setups%20and%20extensive%20manual%20post-processing%2C%20while%20low-cost%20vision-based%20methods%20often%20suffer%20from%20reduced%20accuracy%20and%20reliability%20under%20occlusion.%20To%20address%20these%20challenges%2C%20we%20present%20DexterCap%2C%20a%20low-cost%20optical%20capture%20system%20for%20dexterous%20in-hand%20manipulation.%20DexterCap%20uses%20dense%2C%20character-coded%20marker%20patches%20to%20achieve%20robust%20tracking%20under%20severe%20self-occlusion%2C%20together%20with%20an%20automated%20reconstruction%20pipeline%20that%20requires%20minimal%20manual%20effort.%20With%20DexterCap%2C%20we%20introduce%20DexterHand%2C%20a%20dataset%20of%20fine-grained%20hand-object%20interactions%20covering%20diverse%20manipulation%20behaviors%20and%20objects%2C%20from%20simple%20primitives%20to%20complex%20articulated%20objects%20such%20as%20a%20Rubik%27s%20Cube.%20We%20release%20the%20dataset%20and%20code%20to%20support%20future%20research%20on%20dexterous%20hand-object%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2601.05844v1&entry.124074799=Read"},
{"title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI", "author": "Lucija Mihi\u0107 Zidar and Philipp Wicke and Praneel Bhatia and Rosa Lutz and Marius Klug and Thorsten O. Zander", "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.", "link": "http://arxiv.org/abs/2601.05825v1", "date": "2026-01-09", "relevancy": 2.3173, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Workload%20and%20Agreement%20From%20EEG%20During%20Spoken%20Dialogue%20With%20Conversational%20AI&body=Title%3A%20Decoding%20Workload%20and%20Agreement%20From%20EEG%20During%20Spoken%20Dialogue%20With%20Conversational%20AI%0AAuthor%3A%20Lucija%20Mihi%C4%87%20Zidar%20and%20Philipp%20Wicke%20and%20Praneel%20Bhatia%20and%20Rosa%20Lutz%20and%20Marius%20Klug%20and%20Thorsten%20O.%20Zander%0AAbstract%3A%20Passive%20brain-computer%20interfaces%20offer%20a%20potential%20source%20of%20implicit%20feedback%20for%20alignment%20of%20large%20language%20models%2C%20but%20most%20mental%20state%20decoding%20has%20been%20done%20in%20controlled%20tasks.%20This%20paper%20investigates%20whether%20established%20EEG%20classifiers%20for%20mental%20workload%20and%20implicit%20agreement%20can%20be%20transferred%20to%20spoken%20human-AI%20dialogue.%20We%20introduce%20two%20conversational%20paradigms%20-%20a%20Spelling%20Bee%20task%20and%20a%20sentence%20completion%20task-%20and%20an%20end-to-end%20pipeline%20for%20transcribing%2C%20annotating%2C%20and%20aligning%20word-level%20conversational%20events%20with%20continuous%20EEG%20classifier%20output.%20In%20a%20pilot%20study%2C%20workload%20decoding%20showed%20interpretable%20trends%20during%20spoken%20interaction%2C%20supporting%20cross-paradigm%20transfer.%20For%20implicit%20agreement%2C%20we%20demonstrate%20continuous%20application%20and%20precise%20temporal%20alignment%20to%20conversational%20events%2C%20while%20identifying%20limitations%20related%20to%20construct%20transfer%20and%20asynchronous%20application%20of%20event-based%20classifiers.%20Overall%2C%20the%20results%20establish%20feasibility%20and%20constraints%20for%20integrating%20passive%20BCI%20signals%20into%20conversational%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Workload%2520and%2520Agreement%2520From%2520EEG%2520During%2520Spoken%2520Dialogue%2520With%2520Conversational%2520AI%26entry.906535625%3DLucija%2520Mihi%25C4%2587%2520Zidar%2520and%2520Philipp%2520Wicke%2520and%2520Praneel%2520Bhatia%2520and%2520Rosa%2520Lutz%2520and%2520Marius%2520Klug%2520and%2520Thorsten%2520O.%2520Zander%26entry.1292438233%3DPassive%2520brain-computer%2520interfaces%2520offer%2520a%2520potential%2520source%2520of%2520implicit%2520feedback%2520for%2520alignment%2520of%2520large%2520language%2520models%252C%2520but%2520most%2520mental%2520state%2520decoding%2520has%2520been%2520done%2520in%2520controlled%2520tasks.%2520This%2520paper%2520investigates%2520whether%2520established%2520EEG%2520classifiers%2520for%2520mental%2520workload%2520and%2520implicit%2520agreement%2520can%2520be%2520transferred%2520to%2520spoken%2520human-AI%2520dialogue.%2520We%2520introduce%2520two%2520conversational%2520paradigms%2520-%2520a%2520Spelling%2520Bee%2520task%2520and%2520a%2520sentence%2520completion%2520task-%2520and%2520an%2520end-to-end%2520pipeline%2520for%2520transcribing%252C%2520annotating%252C%2520and%2520aligning%2520word-level%2520conversational%2520events%2520with%2520continuous%2520EEG%2520classifier%2520output.%2520In%2520a%2520pilot%2520study%252C%2520workload%2520decoding%2520showed%2520interpretable%2520trends%2520during%2520spoken%2520interaction%252C%2520supporting%2520cross-paradigm%2520transfer.%2520For%2520implicit%2520agreement%252C%2520we%2520demonstrate%2520continuous%2520application%2520and%2520precise%2520temporal%2520alignment%2520to%2520conversational%2520events%252C%2520while%2520identifying%2520limitations%2520related%2520to%2520construct%2520transfer%2520and%2520asynchronous%2520application%2520of%2520event-based%2520classifiers.%2520Overall%252C%2520the%2520results%2520establish%2520feasibility%2520and%2520constraints%2520for%2520integrating%2520passive%2520BCI%2520signals%2520into%2520conversational%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Workload%20and%20Agreement%20From%20EEG%20During%20Spoken%20Dialogue%20With%20Conversational%20AI&entry.906535625=Lucija%20Mihi%C4%87%20Zidar%20and%20Philipp%20Wicke%20and%20Praneel%20Bhatia%20and%20Rosa%20Lutz%20and%20Marius%20Klug%20and%20Thorsten%20O.%20Zander&entry.1292438233=Passive%20brain-computer%20interfaces%20offer%20a%20potential%20source%20of%20implicit%20feedback%20for%20alignment%20of%20large%20language%20models%2C%20but%20most%20mental%20state%20decoding%20has%20been%20done%20in%20controlled%20tasks.%20This%20paper%20investigates%20whether%20established%20EEG%20classifiers%20for%20mental%20workload%20and%20implicit%20agreement%20can%20be%20transferred%20to%20spoken%20human-AI%20dialogue.%20We%20introduce%20two%20conversational%20paradigms%20-%20a%20Spelling%20Bee%20task%20and%20a%20sentence%20completion%20task-%20and%20an%20end-to-end%20pipeline%20for%20transcribing%2C%20annotating%2C%20and%20aligning%20word-level%20conversational%20events%20with%20continuous%20EEG%20classifier%20output.%20In%20a%20pilot%20study%2C%20workload%20decoding%20showed%20interpretable%20trends%20during%20spoken%20interaction%2C%20supporting%20cross-paradigm%20transfer.%20For%20implicit%20agreement%2C%20we%20demonstrate%20continuous%20application%20and%20precise%20temporal%20alignment%20to%20conversational%20events%2C%20while%20identifying%20limitations%20related%20to%20construct%20transfer%20and%20asynchronous%20application%20of%20event-based%20classifiers.%20Overall%2C%20the%20results%20establish%20feasibility%20and%20constraints%20for%20integrating%20passive%20BCI%20signals%20into%20conversational%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.05825v1&entry.124074799=Read"},
{"title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence", "author": "Feng Liang and Sizhe Cheng and Chenqi Yi and Yong Wang", "abstract": "Omni-modal models that have multimodal input and output are emerging. However, benchmarking their multimodal generation, especially in image generation, is challenging due to the subtleties of human preferences and model biases. Many image generation benchmarks focus on aesthetics instead of the fine-grained generation capabilities of these models, failing to evaluate their visual intelligence with objective metrics. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. With our benchmark and experiments, we find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to dataset development, omni-modal model development, and the design of metrics.", "link": "http://arxiv.org/abs/2512.16303v2", "date": "2026-01-09", "relevancy": 2.3128, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixelArena%3A%20A%20benchmark%20for%20Pixel-Precision%20Visual%20Intelligence&body=Title%3A%20PixelArena%3A%20A%20benchmark%20for%20Pixel-Precision%20Visual%20Intelligence%0AAuthor%3A%20Feng%20Liang%20and%20Sizhe%20Cheng%20and%20Chenqi%20Yi%20and%20Yong%20Wang%0AAbstract%3A%20Omni-modal%20models%20that%20have%20multimodal%20input%20and%20output%20are%20emerging.%20However%2C%20benchmarking%20their%20multimodal%20generation%2C%20especially%20in%20image%20generation%2C%20is%20challenging%20due%20to%20the%20subtleties%20of%20human%20preferences%20and%20model%20biases.%20Many%20image%20generation%20benchmarks%20focus%20on%20aesthetics%20instead%20of%20the%20fine-grained%20generation%20capabilities%20of%20these%20models%2C%20failing%20to%20evaluate%20their%20visual%20intelligence%20with%20objective%20metrics.%20In%20PixelArena%2C%20we%20propose%20using%20semantic%20segmentation%20tasks%20to%20objectively%20examine%20their%20fine-grained%20generative%20intelligence%20with%20pixel%20precision.%20With%20our%20benchmark%20and%20experiments%2C%20we%20find%20the%20latest%20Gemini%203%20Pro%20Image%20has%20emergent%20image%20generation%20capabilities%20that%20generate%20semantic%20masks%20with%20high%20fidelity%20under%20zero-shot%20settings%2C%20showcasing%20visual%20intelligence%20unseen%20before%20and%20true%20generalization%20in%20new%20image%20generation%20tasks.%20We%20further%20investigate%20its%20results%2C%20compare%20them%20qualitatively%20and%20quantitatively%20with%20those%20of%20other%20models%2C%20and%20present%20failure%20cases.%20The%20findings%20not%20only%20signal%20exciting%20progress%20in%20the%20field%20but%20also%20provide%20insights%20into%20future%20research%20related%20to%20dataset%20development%2C%20omni-modal%20model%20development%2C%20and%20the%20design%20of%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixelArena%253A%2520A%2520benchmark%2520for%2520Pixel-Precision%2520Visual%2520Intelligence%26entry.906535625%3DFeng%2520Liang%2520and%2520Sizhe%2520Cheng%2520and%2520Chenqi%2520Yi%2520and%2520Yong%2520Wang%26entry.1292438233%3DOmni-modal%2520models%2520that%2520have%2520multimodal%2520input%2520and%2520output%2520are%2520emerging.%2520However%252C%2520benchmarking%2520their%2520multimodal%2520generation%252C%2520especially%2520in%2520image%2520generation%252C%2520is%2520challenging%2520due%2520to%2520the%2520subtleties%2520of%2520human%2520preferences%2520and%2520model%2520biases.%2520Many%2520image%2520generation%2520benchmarks%2520focus%2520on%2520aesthetics%2520instead%2520of%2520the%2520fine-grained%2520generation%2520capabilities%2520of%2520these%2520models%252C%2520failing%2520to%2520evaluate%2520their%2520visual%2520intelligence%2520with%2520objective%2520metrics.%2520In%2520PixelArena%252C%2520we%2520propose%2520using%2520semantic%2520segmentation%2520tasks%2520to%2520objectively%2520examine%2520their%2520fine-grained%2520generative%2520intelligence%2520with%2520pixel%2520precision.%2520With%2520our%2520benchmark%2520and%2520experiments%252C%2520we%2520find%2520the%2520latest%2520Gemini%25203%2520Pro%2520Image%2520has%2520emergent%2520image%2520generation%2520capabilities%2520that%2520generate%2520semantic%2520masks%2520with%2520high%2520fidelity%2520under%2520zero-shot%2520settings%252C%2520showcasing%2520visual%2520intelligence%2520unseen%2520before%2520and%2520true%2520generalization%2520in%2520new%2520image%2520generation%2520tasks.%2520We%2520further%2520investigate%2520its%2520results%252C%2520compare%2520them%2520qualitatively%2520and%2520quantitatively%2520with%2520those%2520of%2520other%2520models%252C%2520and%2520present%2520failure%2520cases.%2520The%2520findings%2520not%2520only%2520signal%2520exciting%2520progress%2520in%2520the%2520field%2520but%2520also%2520provide%2520insights%2520into%2520future%2520research%2520related%2520to%2520dataset%2520development%252C%2520omni-modal%2520model%2520development%252C%2520and%2520the%2520design%2520of%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixelArena%3A%20A%20benchmark%20for%20Pixel-Precision%20Visual%20Intelligence&entry.906535625=Feng%20Liang%20and%20Sizhe%20Cheng%20and%20Chenqi%20Yi%20and%20Yong%20Wang&entry.1292438233=Omni-modal%20models%20that%20have%20multimodal%20input%20and%20output%20are%20emerging.%20However%2C%20benchmarking%20their%20multimodal%20generation%2C%20especially%20in%20image%20generation%2C%20is%20challenging%20due%20to%20the%20subtleties%20of%20human%20preferences%20and%20model%20biases.%20Many%20image%20generation%20benchmarks%20focus%20on%20aesthetics%20instead%20of%20the%20fine-grained%20generation%20capabilities%20of%20these%20models%2C%20failing%20to%20evaluate%20their%20visual%20intelligence%20with%20objective%20metrics.%20In%20PixelArena%2C%20we%20propose%20using%20semantic%20segmentation%20tasks%20to%20objectively%20examine%20their%20fine-grained%20generative%20intelligence%20with%20pixel%20precision.%20With%20our%20benchmark%20and%20experiments%2C%20we%20find%20the%20latest%20Gemini%203%20Pro%20Image%20has%20emergent%20image%20generation%20capabilities%20that%20generate%20semantic%20masks%20with%20high%20fidelity%20under%20zero-shot%20settings%2C%20showcasing%20visual%20intelligence%20unseen%20before%20and%20true%20generalization%20in%20new%20image%20generation%20tasks.%20We%20further%20investigate%20its%20results%2C%20compare%20them%20qualitatively%20and%20quantitatively%20with%20those%20of%20other%20models%2C%20and%20present%20failure%20cases.%20The%20findings%20not%20only%20signal%20exciting%20progress%20in%20the%20field%20but%20also%20provide%20insights%20into%20future%20research%20related%20to%20dataset%20development%2C%20omni-modal%20model%20development%2C%20and%20the%20design%20of%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2512.16303v2&entry.124074799=Read"},
{"title": "AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces", "author": "Yeonsang Shin and Insoo Kim and Bongkeun Kim and Keonwoo Bae and Bohyung Han", "abstract": "Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.", "link": "http://arxiv.org/abs/2601.05680v1", "date": "2026-01-09", "relevancy": 2.3063, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5972}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5818}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGDC%3A%20Autoregressive%20Generation%20of%20Variable-Length%20Sequences%20with%20Joint%20Discrete%20and%20Continuous%20Spaces&body=Title%3A%20AGDC%3A%20Autoregressive%20Generation%20of%20Variable-Length%20Sequences%20with%20Joint%20Discrete%20and%20Continuous%20Spaces%0AAuthor%3A%20Yeonsang%20Shin%20and%20Insoo%20Kim%20and%20Bongkeun%20Kim%20and%20Keonwoo%20Bae%20and%20Bohyung%20Han%0AAbstract%3A%20Transformer-based%20autoregressive%20models%20excel%20in%20data%20generation%20but%20are%20inherently%20constrained%20by%20their%20reliance%20on%20discretized%20tokens%2C%20which%20limits%20their%20ability%20to%20represent%20continuous%20values%20with%20high%20precision.%20We%20analyze%20the%20scalability%20limitations%20of%20existing%20discretization-based%20approaches%20for%20generating%20hybrid%20discrete-continuous%20sequences%2C%20particularly%20in%20high-precision%20domains%20such%20as%20semiconductor%20circuit%20designs%2C%20where%20precision%20loss%20can%20lead%20to%20functional%20failure.%20To%20address%20the%20challenge%2C%20we%20propose%20AGDC%2C%20a%20novel%20unified%20framework%20that%20jointly%20models%20discrete%20and%20continuous%20values%20for%20variable-length%20sequences.%20AGDC%20employs%20a%20hybrid%20approach%20that%20combines%20categorical%20prediction%20for%20discrete%20values%20with%20diffusion-based%20modeling%20for%20continuous%20values%2C%20incorporating%20two%20key%20technical%20components%3A%20an%20end-of-sequence%20%28EOS%29%20logit%20adjustment%20mechanism%20that%20uses%20an%20MLP%20to%20dynamically%20adjust%20EOS%20token%20logits%20based%20on%20sequence%20context%2C%20and%20a%20length%20regularization%20term%20integrated%20into%20the%20loss%20function.%20Additionally%2C%20we%20present%20ContLayNet%2C%20a%20large-scale%20benchmark%20comprising%20334K%20high-precision%20semiconductor%20layout%20samples%20with%20specialized%20evaluation%20metrics%20that%20capture%20functional%20correctness%20where%20precision%20errors%20significantly%20impact%20performance.%20Experiments%20on%20semiconductor%20layouts%20%28ContLayNet%29%2C%20graphic%20layouts%2C%20and%20SVGs%20demonstrate%20AGDC%27s%20superior%20performance%20in%20generating%20high-fidelity%20hybrid%20vector%20representations%20compared%20to%20discretization-based%20and%20fixed-schema%20baselines%2C%20achieving%20scalable%20high-precision%20generation%20across%20diverse%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGDC%253A%2520Autoregressive%2520Generation%2520of%2520Variable-Length%2520Sequences%2520with%2520Joint%2520Discrete%2520and%2520Continuous%2520Spaces%26entry.906535625%3DYeonsang%2520Shin%2520and%2520Insoo%2520Kim%2520and%2520Bongkeun%2520Kim%2520and%2520Keonwoo%2520Bae%2520and%2520Bohyung%2520Han%26entry.1292438233%3DTransformer-based%2520autoregressive%2520models%2520excel%2520in%2520data%2520generation%2520but%2520are%2520inherently%2520constrained%2520by%2520their%2520reliance%2520on%2520discretized%2520tokens%252C%2520which%2520limits%2520their%2520ability%2520to%2520represent%2520continuous%2520values%2520with%2520high%2520precision.%2520We%2520analyze%2520the%2520scalability%2520limitations%2520of%2520existing%2520discretization-based%2520approaches%2520for%2520generating%2520hybrid%2520discrete-continuous%2520sequences%252C%2520particularly%2520in%2520high-precision%2520domains%2520such%2520as%2520semiconductor%2520circuit%2520designs%252C%2520where%2520precision%2520loss%2520can%2520lead%2520to%2520functional%2520failure.%2520To%2520address%2520the%2520challenge%252C%2520we%2520propose%2520AGDC%252C%2520a%2520novel%2520unified%2520framework%2520that%2520jointly%2520models%2520discrete%2520and%2520continuous%2520values%2520for%2520variable-length%2520sequences.%2520AGDC%2520employs%2520a%2520hybrid%2520approach%2520that%2520combines%2520categorical%2520prediction%2520for%2520discrete%2520values%2520with%2520diffusion-based%2520modeling%2520for%2520continuous%2520values%252C%2520incorporating%2520two%2520key%2520technical%2520components%253A%2520an%2520end-of-sequence%2520%2528EOS%2529%2520logit%2520adjustment%2520mechanism%2520that%2520uses%2520an%2520MLP%2520to%2520dynamically%2520adjust%2520EOS%2520token%2520logits%2520based%2520on%2520sequence%2520context%252C%2520and%2520a%2520length%2520regularization%2520term%2520integrated%2520into%2520the%2520loss%2520function.%2520Additionally%252C%2520we%2520present%2520ContLayNet%252C%2520a%2520large-scale%2520benchmark%2520comprising%2520334K%2520high-precision%2520semiconductor%2520layout%2520samples%2520with%2520specialized%2520evaluation%2520metrics%2520that%2520capture%2520functional%2520correctness%2520where%2520precision%2520errors%2520significantly%2520impact%2520performance.%2520Experiments%2520on%2520semiconductor%2520layouts%2520%2528ContLayNet%2529%252C%2520graphic%2520layouts%252C%2520and%2520SVGs%2520demonstrate%2520AGDC%2527s%2520superior%2520performance%2520in%2520generating%2520high-fidelity%2520hybrid%2520vector%2520representations%2520compared%2520to%2520discretization-based%2520and%2520fixed-schema%2520baselines%252C%2520achieving%2520scalable%2520high-precision%2520generation%2520across%2520diverse%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGDC%3A%20Autoregressive%20Generation%20of%20Variable-Length%20Sequences%20with%20Joint%20Discrete%20and%20Continuous%20Spaces&entry.906535625=Yeonsang%20Shin%20and%20Insoo%20Kim%20and%20Bongkeun%20Kim%20and%20Keonwoo%20Bae%20and%20Bohyung%20Han&entry.1292438233=Transformer-based%20autoregressive%20models%20excel%20in%20data%20generation%20but%20are%20inherently%20constrained%20by%20their%20reliance%20on%20discretized%20tokens%2C%20which%20limits%20their%20ability%20to%20represent%20continuous%20values%20with%20high%20precision.%20We%20analyze%20the%20scalability%20limitations%20of%20existing%20discretization-based%20approaches%20for%20generating%20hybrid%20discrete-continuous%20sequences%2C%20particularly%20in%20high-precision%20domains%20such%20as%20semiconductor%20circuit%20designs%2C%20where%20precision%20loss%20can%20lead%20to%20functional%20failure.%20To%20address%20the%20challenge%2C%20we%20propose%20AGDC%2C%20a%20novel%20unified%20framework%20that%20jointly%20models%20discrete%20and%20continuous%20values%20for%20variable-length%20sequences.%20AGDC%20employs%20a%20hybrid%20approach%20that%20combines%20categorical%20prediction%20for%20discrete%20values%20with%20diffusion-based%20modeling%20for%20continuous%20values%2C%20incorporating%20two%20key%20technical%20components%3A%20an%20end-of-sequence%20%28EOS%29%20logit%20adjustment%20mechanism%20that%20uses%20an%20MLP%20to%20dynamically%20adjust%20EOS%20token%20logits%20based%20on%20sequence%20context%2C%20and%20a%20length%20regularization%20term%20integrated%20into%20the%20loss%20function.%20Additionally%2C%20we%20present%20ContLayNet%2C%20a%20large-scale%20benchmark%20comprising%20334K%20high-precision%20semiconductor%20layout%20samples%20with%20specialized%20evaluation%20metrics%20that%20capture%20functional%20correctness%20where%20precision%20errors%20significantly%20impact%20performance.%20Experiments%20on%20semiconductor%20layouts%20%28ContLayNet%29%2C%20graphic%20layouts%2C%20and%20SVGs%20demonstrate%20AGDC%27s%20superior%20performance%20in%20generating%20high-fidelity%20hybrid%20vector%20representations%20compared%20to%20discretization-based%20and%20fixed-schema%20baselines%2C%20achieving%20scalable%20high-precision%20generation%20across%20diverse%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.05680v1&entry.124074799=Read"},
{"title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning", "author": "Alexandra Dragomir and Florin Brad and Radu Tudor Ionescu", "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.", "link": "http://arxiv.org/abs/2601.05858v1", "date": "2026-01-09", "relevancy": 2.3046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5011}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLewR%3A%20Curriculum%20Learning%20with%20Restarts%20for%20Machine%20Translation%20Preference%20Learning&body=Title%3A%20CLewR%3A%20Curriculum%20Learning%20with%20Restarts%20for%20Machine%20Translation%20Preference%20Learning%0AAuthor%3A%20Alexandra%20Dragomir%20and%20Florin%20Brad%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20competitive%20performance%20in%20zero-shot%20multilingual%20machine%20translation%20%28MT%29.%20Some%20follow-up%20works%20further%20improved%20MT%20performance%20via%20preference%20optimization%2C%20but%20they%20leave%20a%20key%20aspect%20largely%20underexplored%3A%20the%20order%20in%20which%20data%20samples%20are%20given%20during%20training.%20We%20address%20this%20topic%20by%20integrating%20curriculum%20learning%20into%20various%20state-of-the-art%20preference%20optimization%20algorithms%20to%20boost%20MT%20performance.%20We%20introduce%20a%20novel%20curriculum%20learning%20strategy%20with%20restarts%20%28CLewR%29%2C%20which%20reiterates%20easy-to-hard%20curriculum%20multiple%20times%20during%20training%20to%20effectively%20mitigate%20the%20catastrophic%20forgetting%20of%20easy%20examples.%20We%20demonstrate%20consistent%20gains%20across%20several%20model%20families%20%28Gemma2%2C%20Qwen2.5%2C%20Llama3.1%29%20and%20preference%20optimization%20techniques.%20We%20publicly%20release%20our%20code%20at%20https%3A//github.com/alexandra-dragomir/CLewR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLewR%253A%2520Curriculum%2520Learning%2520with%2520Restarts%2520for%2520Machine%2520Translation%2520Preference%2520Learning%26entry.906535625%3DAlexandra%2520Dragomir%2520and%2520Florin%2520Brad%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520competitive%2520performance%2520in%2520zero-shot%2520multilingual%2520machine%2520translation%2520%2528MT%2529.%2520Some%2520follow-up%2520works%2520further%2520improved%2520MT%2520performance%2520via%2520preference%2520optimization%252C%2520but%2520they%2520leave%2520a%2520key%2520aspect%2520largely%2520underexplored%253A%2520the%2520order%2520in%2520which%2520data%2520samples%2520are%2520given%2520during%2520training.%2520We%2520address%2520this%2520topic%2520by%2520integrating%2520curriculum%2520learning%2520into%2520various%2520state-of-the-art%2520preference%2520optimization%2520algorithms%2520to%2520boost%2520MT%2520performance.%2520We%2520introduce%2520a%2520novel%2520curriculum%2520learning%2520strategy%2520with%2520restarts%2520%2528CLewR%2529%252C%2520which%2520reiterates%2520easy-to-hard%2520curriculum%2520multiple%2520times%2520during%2520training%2520to%2520effectively%2520mitigate%2520the%2520catastrophic%2520forgetting%2520of%2520easy%2520examples.%2520We%2520demonstrate%2520consistent%2520gains%2520across%2520several%2520model%2520families%2520%2528Gemma2%252C%2520Qwen2.5%252C%2520Llama3.1%2529%2520and%2520preference%2520optimization%2520techniques.%2520We%2520publicly%2520release%2520our%2520code%2520at%2520https%253A//github.com/alexandra-dragomir/CLewR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLewR%3A%20Curriculum%20Learning%20with%20Restarts%20for%20Machine%20Translation%20Preference%20Learning&entry.906535625=Alexandra%20Dragomir%20and%20Florin%20Brad%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20competitive%20performance%20in%20zero-shot%20multilingual%20machine%20translation%20%28MT%29.%20Some%20follow-up%20works%20further%20improved%20MT%20performance%20via%20preference%20optimization%2C%20but%20they%20leave%20a%20key%20aspect%20largely%20underexplored%3A%20the%20order%20in%20which%20data%20samples%20are%20given%20during%20training.%20We%20address%20this%20topic%20by%20integrating%20curriculum%20learning%20into%20various%20state-of-the-art%20preference%20optimization%20algorithms%20to%20boost%20MT%20performance.%20We%20introduce%20a%20novel%20curriculum%20learning%20strategy%20with%20restarts%20%28CLewR%29%2C%20which%20reiterates%20easy-to-hard%20curriculum%20multiple%20times%20during%20training%20to%20effectively%20mitigate%20the%20catastrophic%20forgetting%20of%20easy%20examples.%20We%20demonstrate%20consistent%20gains%20across%20several%20model%20families%20%28Gemma2%2C%20Qwen2.5%2C%20Llama3.1%29%20and%20preference%20optimization%20techniques.%20We%20publicly%20release%20our%20code%20at%20https%3A//github.com/alexandra-dragomir/CLewR.&entry.1838667208=http%3A//arxiv.org/abs/2601.05858v1&entry.124074799=Read"},
{"title": "Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms", "author": "Turkan Simge Ispak and Salih Tileylioglu and Erdem Akagunduz", "abstract": "Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce \"overgeneralization\", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.", "link": "http://arxiv.org/abs/2601.05759v1", "date": "2026-01-09", "relevancy": 2.3023, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5123}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4429}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Autoencoders%20for%20P-wave%20Detection%20on%20Strong%20Motion%20Earthquake%20Spectrograms&body=Title%3A%20Variational%20Autoencoders%20for%20P-wave%20Detection%20on%20Strong%20Motion%20Earthquake%20Spectrograms%0AAuthor%3A%20Turkan%20Simge%20Ispak%20and%20Salih%20Tileylioglu%20and%20Erdem%20Akagunduz%0AAbstract%3A%20Accurate%20P-wave%20detection%20is%20critical%20for%20earthquake%20early%20warning%2C%20yet%20strong-motion%20records%20pose%20challenges%20due%20to%20high%20noise%20levels%2C%20limited%20labeled%20data%2C%20and%20complex%20waveform%20characteristics.%20This%20study%20reframes%20P-wave%20arrival%20detection%20as%20a%20self-supervised%20anomaly%20detection%20task%20to%20evaluate%20how%20architectural%20variations%20regulate%20the%20trade-off%20between%20reconstruction%20fidelity%20and%20anomaly%20discrimination.%20Through%20a%20comprehensive%20grid%20search%20of%20492%20Variational%20Autoencoder%20configurations%2C%20we%20show%20that%20while%20skip%20connections%20minimize%20reconstruction%20error%20%28Mean%20Absolute%20Error%20approximately%200.0012%29%2C%20they%20induce%20%22overgeneralization%22%2C%20allowing%20the%20model%20to%20reconstruct%20noise%20and%20masking%20the%20detection%20signal.%20In%20contrast%2C%20attention%20mechanisms%20prioritize%20global%20context%20over%20local%20detail%20and%20yield%20the%20highest%20detection%20performance%20with%20an%20area-under-the-curve%20of%200.875.%20The%20attention-based%20Variational%20Autoencoder%20achieves%20an%20area-under-the-curve%20of%200.91%20in%20the%200%20to%2040-kilometer%20near-source%20range%2C%20demonstrating%20high%20suitability%20for%20immediate%20early%20warning%20applications.%20These%20findings%20establish%20that%20architectural%20constraints%20favoring%20global%20context%20over%20pixel-perfect%20reconstruction%20are%20essential%20for%20robust%2C%20self-supervised%20P-wave%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Autoencoders%2520for%2520P-wave%2520Detection%2520on%2520Strong%2520Motion%2520Earthquake%2520Spectrograms%26entry.906535625%3DTurkan%2520Simge%2520Ispak%2520and%2520Salih%2520Tileylioglu%2520and%2520Erdem%2520Akagunduz%26entry.1292438233%3DAccurate%2520P-wave%2520detection%2520is%2520critical%2520for%2520earthquake%2520early%2520warning%252C%2520yet%2520strong-motion%2520records%2520pose%2520challenges%2520due%2520to%2520high%2520noise%2520levels%252C%2520limited%2520labeled%2520data%252C%2520and%2520complex%2520waveform%2520characteristics.%2520This%2520study%2520reframes%2520P-wave%2520arrival%2520detection%2520as%2520a%2520self-supervised%2520anomaly%2520detection%2520task%2520to%2520evaluate%2520how%2520architectural%2520variations%2520regulate%2520the%2520trade-off%2520between%2520reconstruction%2520fidelity%2520and%2520anomaly%2520discrimination.%2520Through%2520a%2520comprehensive%2520grid%2520search%2520of%2520492%2520Variational%2520Autoencoder%2520configurations%252C%2520we%2520show%2520that%2520while%2520skip%2520connections%2520minimize%2520reconstruction%2520error%2520%2528Mean%2520Absolute%2520Error%2520approximately%25200.0012%2529%252C%2520they%2520induce%2520%2522overgeneralization%2522%252C%2520allowing%2520the%2520model%2520to%2520reconstruct%2520noise%2520and%2520masking%2520the%2520detection%2520signal.%2520In%2520contrast%252C%2520attention%2520mechanisms%2520prioritize%2520global%2520context%2520over%2520local%2520detail%2520and%2520yield%2520the%2520highest%2520detection%2520performance%2520with%2520an%2520area-under-the-curve%2520of%25200.875.%2520The%2520attention-based%2520Variational%2520Autoencoder%2520achieves%2520an%2520area-under-the-curve%2520of%25200.91%2520in%2520the%25200%2520to%252040-kilometer%2520near-source%2520range%252C%2520demonstrating%2520high%2520suitability%2520for%2520immediate%2520early%2520warning%2520applications.%2520These%2520findings%2520establish%2520that%2520architectural%2520constraints%2520favoring%2520global%2520context%2520over%2520pixel-perfect%2520reconstruction%2520are%2520essential%2520for%2520robust%252C%2520self-supervised%2520P-wave%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Autoencoders%20for%20P-wave%20Detection%20on%20Strong%20Motion%20Earthquake%20Spectrograms&entry.906535625=Turkan%20Simge%20Ispak%20and%20Salih%20Tileylioglu%20and%20Erdem%20Akagunduz&entry.1292438233=Accurate%20P-wave%20detection%20is%20critical%20for%20earthquake%20early%20warning%2C%20yet%20strong-motion%20records%20pose%20challenges%20due%20to%20high%20noise%20levels%2C%20limited%20labeled%20data%2C%20and%20complex%20waveform%20characteristics.%20This%20study%20reframes%20P-wave%20arrival%20detection%20as%20a%20self-supervised%20anomaly%20detection%20task%20to%20evaluate%20how%20architectural%20variations%20regulate%20the%20trade-off%20between%20reconstruction%20fidelity%20and%20anomaly%20discrimination.%20Through%20a%20comprehensive%20grid%20search%20of%20492%20Variational%20Autoencoder%20configurations%2C%20we%20show%20that%20while%20skip%20connections%20minimize%20reconstruction%20error%20%28Mean%20Absolute%20Error%20approximately%200.0012%29%2C%20they%20induce%20%22overgeneralization%22%2C%20allowing%20the%20model%20to%20reconstruct%20noise%20and%20masking%20the%20detection%20signal.%20In%20contrast%2C%20attention%20mechanisms%20prioritize%20global%20context%20over%20local%20detail%20and%20yield%20the%20highest%20detection%20performance%20with%20an%20area-under-the-curve%20of%200.875.%20The%20attention-based%20Variational%20Autoencoder%20achieves%20an%20area-under-the-curve%20of%200.91%20in%20the%200%20to%2040-kilometer%20near-source%20range%2C%20demonstrating%20high%20suitability%20for%20immediate%20early%20warning%20applications.%20These%20findings%20establish%20that%20architectural%20constraints%20favoring%20global%20context%20over%20pixel-perfect%20reconstruction%20are%20essential%20for%20robust%2C%20self-supervised%20P-wave%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2601.05759v1&entry.124074799=Read"},
{"title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera", "author": "Xiaopeng Guo and Yinzhe Xu and Huajian Huang and Sai-Kit Yeung", "abstract": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage", "link": "http://arxiv.org/abs/2601.02309v2", "date": "2026-01-09", "relevancy": 2.2929, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6096}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360DVO%3A%20Deep%20Visual%20Odometry%20for%20Monocular%20360-Degree%20Camera&body=Title%3A%20360DVO%3A%20Deep%20Visual%20Odometry%20for%20Monocular%20360-Degree%20Camera%0AAuthor%3A%20Xiaopeng%20Guo%20and%20Yinzhe%20Xu%20and%20Huajian%20Huang%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20Monocular%20omnidirectional%20visual%20odometry%20%28OVO%29%20systems%20leverage%20360-degree%20cameras%20to%20overcome%20field-of-view%20limitations%20of%20perspective%20VO%20systems.%20However%2C%20existing%20methods%2C%20reliant%20on%20handcrafted%20features%20or%20photometric%20objectives%2C%20often%20lack%20robustness%20in%20challenging%20scenarios%2C%20such%20as%20aggressive%20motion%20and%20varying%20illumination.%20To%20address%20this%2C%20we%20present%20360DVO%2C%20the%20first%20deep%20learning-based%20OVO%20framework.%20Our%20approach%20introduces%20a%20distortion-aware%20spherical%20feature%20extractor%20%28DAS-Feat%29%20that%20adaptively%20learns%20distortion-resistant%20features%20from%20360-degree%20images.%20These%20sparse%20feature%20patches%20are%20then%20used%20to%20establish%20constraints%20for%20effective%20pose%20estimation%20within%20a%20novel%20omnidirectional%20differentiable%20bundle%20adjustment%20%28ODBA%29%20module.%20To%20facilitate%20evaluation%20in%20realistic%20settings%2C%20we%20also%20contribute%20a%20new%20real-world%20OVO%20benchmark.%20Extensive%20experiments%20on%20this%20benchmark%20and%20public%20synthetic%20datasets%20%28TartanAir%20V2%20and%20360VO%29%20demonstrate%20that%20360DVO%20surpasses%20state-of-the-art%20baselines%20%28including%20360VO%20and%20OpenVSLAM%29%2C%20improving%20robustness%20by%2050%25%20and%20accuracy%20by%2037.5%25.%20Homepage%3A%20https%3A//chris1004336379.github.io/360DVO-homepage%0ALink%3A%20http%3A//arxiv.org/abs/2601.02309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360DVO%253A%2520Deep%2520Visual%2520Odometry%2520for%2520Monocular%2520360-Degree%2520Camera%26entry.906535625%3DXiaopeng%2520Guo%2520and%2520Yinzhe%2520Xu%2520and%2520Huajian%2520Huang%2520and%2520Sai-Kit%2520Yeung%26entry.1292438233%3DMonocular%2520omnidirectional%2520visual%2520odometry%2520%2528OVO%2529%2520systems%2520leverage%2520360-degree%2520cameras%2520to%2520overcome%2520field-of-view%2520limitations%2520of%2520perspective%2520VO%2520systems.%2520However%252C%2520existing%2520methods%252C%2520reliant%2520on%2520handcrafted%2520features%2520or%2520photometric%2520objectives%252C%2520often%2520lack%2520robustness%2520in%2520challenging%2520scenarios%252C%2520such%2520as%2520aggressive%2520motion%2520and%2520varying%2520illumination.%2520To%2520address%2520this%252C%2520we%2520present%2520360DVO%252C%2520the%2520first%2520deep%2520learning-based%2520OVO%2520framework.%2520Our%2520approach%2520introduces%2520a%2520distortion-aware%2520spherical%2520feature%2520extractor%2520%2528DAS-Feat%2529%2520that%2520adaptively%2520learns%2520distortion-resistant%2520features%2520from%2520360-degree%2520images.%2520These%2520sparse%2520feature%2520patches%2520are%2520then%2520used%2520to%2520establish%2520constraints%2520for%2520effective%2520pose%2520estimation%2520within%2520a%2520novel%2520omnidirectional%2520differentiable%2520bundle%2520adjustment%2520%2528ODBA%2529%2520module.%2520To%2520facilitate%2520evaluation%2520in%2520realistic%2520settings%252C%2520we%2520also%2520contribute%2520a%2520new%2520real-world%2520OVO%2520benchmark.%2520Extensive%2520experiments%2520on%2520this%2520benchmark%2520and%2520public%2520synthetic%2520datasets%2520%2528TartanAir%2520V2%2520and%2520360VO%2529%2520demonstrate%2520that%2520360DVO%2520surpasses%2520state-of-the-art%2520baselines%2520%2528including%2520360VO%2520and%2520OpenVSLAM%2529%252C%2520improving%2520robustness%2520by%252050%2525%2520and%2520accuracy%2520by%252037.5%2525.%2520Homepage%253A%2520https%253A//chris1004336379.github.io/360DVO-homepage%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360DVO%3A%20Deep%20Visual%20Odometry%20for%20Monocular%20360-Degree%20Camera&entry.906535625=Xiaopeng%20Guo%20and%20Yinzhe%20Xu%20and%20Huajian%20Huang%20and%20Sai-Kit%20Yeung&entry.1292438233=Monocular%20omnidirectional%20visual%20odometry%20%28OVO%29%20systems%20leverage%20360-degree%20cameras%20to%20overcome%20field-of-view%20limitations%20of%20perspective%20VO%20systems.%20However%2C%20existing%20methods%2C%20reliant%20on%20handcrafted%20features%20or%20photometric%20objectives%2C%20often%20lack%20robustness%20in%20challenging%20scenarios%2C%20such%20as%20aggressive%20motion%20and%20varying%20illumination.%20To%20address%20this%2C%20we%20present%20360DVO%2C%20the%20first%20deep%20learning-based%20OVO%20framework.%20Our%20approach%20introduces%20a%20distortion-aware%20spherical%20feature%20extractor%20%28DAS-Feat%29%20that%20adaptively%20learns%20distortion-resistant%20features%20from%20360-degree%20images.%20These%20sparse%20feature%20patches%20are%20then%20used%20to%20establish%20constraints%20for%20effective%20pose%20estimation%20within%20a%20novel%20omnidirectional%20differentiable%20bundle%20adjustment%20%28ODBA%29%20module.%20To%20facilitate%20evaluation%20in%20realistic%20settings%2C%20we%20also%20contribute%20a%20new%20real-world%20OVO%20benchmark.%20Extensive%20experiments%20on%20this%20benchmark%20and%20public%20synthetic%20datasets%20%28TartanAir%20V2%20and%20360VO%29%20demonstrate%20that%20360DVO%20surpasses%20state-of-the-art%20baselines%20%28including%20360VO%20and%20OpenVSLAM%29%2C%20improving%20robustness%20by%2050%25%20and%20accuracy%20by%2037.5%25.%20Homepage%3A%20https%3A//chris1004336379.github.io/360DVO-homepage&entry.1838667208=http%3A//arxiv.org/abs/2601.02309v2&entry.124074799=Read"},
{"title": "An Evaluation on Large Language Model Outputs: Discourse and Memorization", "author": "Adrian de Wynter and Xun Wang and Alex Sokolov and Qilong Gu and Si-Qing Chen", "abstract": "We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.", "link": "http://arxiv.org/abs/2304.08637v2", "date": "2026-01-09", "relevancy": 2.2911, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Evaluation%20on%20Large%20Language%20Model%20Outputs%3A%20Discourse%20and%20Memorization&body=Title%3A%20An%20Evaluation%20on%20Large%20Language%20Model%20Outputs%3A%20Discourse%20and%20Memorization%0AAuthor%3A%20Adrian%20de%20Wynter%20and%20Xun%20Wang%20and%20Alex%20Sokolov%20and%20Qilong%20Gu%20and%20Si-Qing%20Chen%0AAbstract%3A%20We%20present%20an%20empirical%20evaluation%20of%20various%20outputs%20generated%20by%20nine%20of%20the%20most%20widely-available%20large%20language%20models%20%28LLMs%29.%20Our%20analysis%20is%20done%20with%20off-the-shelf%2C%20readily-available%20tools.%20We%20find%20a%20correlation%20between%20percentage%20of%20memorized%20text%2C%20percentage%20of%20unique%20text%2C%20and%20overall%20output%20quality%2C%20when%20measured%20with%20respect%20to%20output%20pathologies%20such%20as%20counterfactual%20and%20logically-flawed%20statements%2C%20and%20general%20failures%20like%20not%20staying%20on%20topic.%20Overall%2C%2080.0%25%20of%20the%20outputs%20evaluated%20contained%20memorized%20data%2C%20but%20outputs%20containing%20the%20most%20memorized%20content%20were%20also%20more%20likely%20to%20be%20considered%20of%20high%20quality.%20We%20discuss%20and%20evaluate%20mitigation%20strategies%2C%20showing%20that%2C%20in%20the%20models%20evaluated%2C%20the%20rate%20of%20memorized%20text%20being%20output%20is%20reduced.%20We%20conclude%20with%20a%20discussion%20on%20potential%20implications%20around%20what%20it%20means%20to%20learn%2C%20to%20memorize%2C%20and%20to%20evaluate%20quality%20text.%0ALink%3A%20http%3A//arxiv.org/abs/2304.08637v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Evaluation%2520on%2520Large%2520Language%2520Model%2520Outputs%253A%2520Discourse%2520and%2520Memorization%26entry.906535625%3DAdrian%2520de%2520Wynter%2520and%2520Xun%2520Wang%2520and%2520Alex%2520Sokolov%2520and%2520Qilong%2520Gu%2520and%2520Si-Qing%2520Chen%26entry.1292438233%3DWe%2520present%2520an%2520empirical%2520evaluation%2520of%2520various%2520outputs%2520generated%2520by%2520nine%2520of%2520the%2520most%2520widely-available%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520analysis%2520is%2520done%2520with%2520off-the-shelf%252C%2520readily-available%2520tools.%2520We%2520find%2520a%2520correlation%2520between%2520percentage%2520of%2520memorized%2520text%252C%2520percentage%2520of%2520unique%2520text%252C%2520and%2520overall%2520output%2520quality%252C%2520when%2520measured%2520with%2520respect%2520to%2520output%2520pathologies%2520such%2520as%2520counterfactual%2520and%2520logically-flawed%2520statements%252C%2520and%2520general%2520failures%2520like%2520not%2520staying%2520on%2520topic.%2520Overall%252C%252080.0%2525%2520of%2520the%2520outputs%2520evaluated%2520contained%2520memorized%2520data%252C%2520but%2520outputs%2520containing%2520the%2520most%2520memorized%2520content%2520were%2520also%2520more%2520likely%2520to%2520be%2520considered%2520of%2520high%2520quality.%2520We%2520discuss%2520and%2520evaluate%2520mitigation%2520strategies%252C%2520showing%2520that%252C%2520in%2520the%2520models%2520evaluated%252C%2520the%2520rate%2520of%2520memorized%2520text%2520being%2520output%2520is%2520reduced.%2520We%2520conclude%2520with%2520a%2520discussion%2520on%2520potential%2520implications%2520around%2520what%2520it%2520means%2520to%2520learn%252C%2520to%2520memorize%252C%2520and%2520to%2520evaluate%2520quality%2520text.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.08637v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Evaluation%20on%20Large%20Language%20Model%20Outputs%3A%20Discourse%20and%20Memorization&entry.906535625=Adrian%20de%20Wynter%20and%20Xun%20Wang%20and%20Alex%20Sokolov%20and%20Qilong%20Gu%20and%20Si-Qing%20Chen&entry.1292438233=We%20present%20an%20empirical%20evaluation%20of%20various%20outputs%20generated%20by%20nine%20of%20the%20most%20widely-available%20large%20language%20models%20%28LLMs%29.%20Our%20analysis%20is%20done%20with%20off-the-shelf%2C%20readily-available%20tools.%20We%20find%20a%20correlation%20between%20percentage%20of%20memorized%20text%2C%20percentage%20of%20unique%20text%2C%20and%20overall%20output%20quality%2C%20when%20measured%20with%20respect%20to%20output%20pathologies%20such%20as%20counterfactual%20and%20logically-flawed%20statements%2C%20and%20general%20failures%20like%20not%20staying%20on%20topic.%20Overall%2C%2080.0%25%20of%20the%20outputs%20evaluated%20contained%20memorized%20data%2C%20but%20outputs%20containing%20the%20most%20memorized%20content%20were%20also%20more%20likely%20to%20be%20considered%20of%20high%20quality.%20We%20discuss%20and%20evaluate%20mitigation%20strategies%2C%20showing%20that%2C%20in%20the%20models%20evaluated%2C%20the%20rate%20of%20memorized%20text%20being%20output%20is%20reduced.%20We%20conclude%20with%20a%20discussion%20on%20potential%20implications%20around%20what%20it%20means%20to%20learn%2C%20to%20memorize%2C%20and%20to%20evaluate%20quality%20text.&entry.1838667208=http%3A//arxiv.org/abs/2304.08637v2&entry.124074799=Read"},
{"title": "There are no Champions in Supervised Long-Term Time Series Forecasting", "author": "Lorenzo Brigato and Rafael Morand and Knut Str\u00f8mmen and Maria Panagiotou and Markus Schmidt and Stavroula Mougiakakou", "abstract": "Recent advances in long-term time series forecasting have introduced numerous complex supervised prediction models that consistently outperform previously published architectures. However, this rapid progression raises concerns regarding inconsistent benchmarking and reporting practices, which may undermine the reliability of these comparisons. In this study, we first perform a broad, thorough, and reproducible evaluation of the top-performing supervised models on the most popular benchmark and additional baselines representing the most active architecture families. This extensive evaluation assesses eight models on 14 datasets, encompassing $\\sim$5,000 trained networks for the hyperparameter (HP) searches. Then, through a comprehensive analysis, we find that slight changes to experimental setups or current evaluation metrics drastically shift the common belief that newly published results are advancing the state of the art. Our findings emphasize the need to shift focus away from pursuing ever-more complex models, towards enhancing benchmarking practices through rigorous and standardized evaluations that enable more substantiated claims, including reproducible HP setups and statistical testing. We offer recommendations for future research.", "link": "http://arxiv.org/abs/2502.14045v2", "date": "2026-01-09", "relevancy": 2.2866, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4752}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4494}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20There%20are%20no%20Champions%20in%20Supervised%20Long-Term%20Time%20Series%20Forecasting&body=Title%3A%20There%20are%20no%20Champions%20in%20Supervised%20Long-Term%20Time%20Series%20Forecasting%0AAuthor%3A%20Lorenzo%20Brigato%20and%20Rafael%20Morand%20and%20Knut%20Str%C3%B8mmen%20and%20Maria%20Panagiotou%20and%20Markus%20Schmidt%20and%20Stavroula%20Mougiakakou%0AAbstract%3A%20Recent%20advances%20in%20long-term%20time%20series%20forecasting%20have%20introduced%20numerous%20complex%20supervised%20prediction%20models%20that%20consistently%20outperform%20previously%20published%20architectures.%20However%2C%20this%20rapid%20progression%20raises%20concerns%20regarding%20inconsistent%20benchmarking%20and%20reporting%20practices%2C%20which%20may%20undermine%20the%20reliability%20of%20these%20comparisons.%20In%20this%20study%2C%20we%20first%20perform%20a%20broad%2C%20thorough%2C%20and%20reproducible%20evaluation%20of%20the%20top-performing%20supervised%20models%20on%20the%20most%20popular%20benchmark%20and%20additional%20baselines%20representing%20the%20most%20active%20architecture%20families.%20This%20extensive%20evaluation%20assesses%20eight%20models%20on%2014%20datasets%2C%20encompassing%20%24%5Csim%245%2C000%20trained%20networks%20for%20the%20hyperparameter%20%28HP%29%20searches.%20Then%2C%20through%20a%20comprehensive%20analysis%2C%20we%20find%20that%20slight%20changes%20to%20experimental%20setups%20or%20current%20evaluation%20metrics%20drastically%20shift%20the%20common%20belief%20that%20newly%20published%20results%20are%20advancing%20the%20state%20of%20the%20art.%20Our%20findings%20emphasize%20the%20need%20to%20shift%20focus%20away%20from%20pursuing%20ever-more%20complex%20models%2C%20towards%20enhancing%20benchmarking%20practices%20through%20rigorous%20and%20standardized%20evaluations%20that%20enable%20more%20substantiated%20claims%2C%20including%20reproducible%20HP%20setups%20and%20statistical%20testing.%20We%20offer%20recommendations%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2502.14045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThere%2520are%2520no%2520Champions%2520in%2520Supervised%2520Long-Term%2520Time%2520Series%2520Forecasting%26entry.906535625%3DLorenzo%2520Brigato%2520and%2520Rafael%2520Morand%2520and%2520Knut%2520Str%25C3%25B8mmen%2520and%2520Maria%2520Panagiotou%2520and%2520Markus%2520Schmidt%2520and%2520Stavroula%2520Mougiakakou%26entry.1292438233%3DRecent%2520advances%2520in%2520long-term%2520time%2520series%2520forecasting%2520have%2520introduced%2520numerous%2520complex%2520supervised%2520prediction%2520models%2520that%2520consistently%2520outperform%2520previously%2520published%2520architectures.%2520However%252C%2520this%2520rapid%2520progression%2520raises%2520concerns%2520regarding%2520inconsistent%2520benchmarking%2520and%2520reporting%2520practices%252C%2520which%2520may%2520undermine%2520the%2520reliability%2520of%2520these%2520comparisons.%2520In%2520this%2520study%252C%2520we%2520first%2520perform%2520a%2520broad%252C%2520thorough%252C%2520and%2520reproducible%2520evaluation%2520of%2520the%2520top-performing%2520supervised%2520models%2520on%2520the%2520most%2520popular%2520benchmark%2520and%2520additional%2520baselines%2520representing%2520the%2520most%2520active%2520architecture%2520families.%2520This%2520extensive%2520evaluation%2520assesses%2520eight%2520models%2520on%252014%2520datasets%252C%2520encompassing%2520%2524%255Csim%25245%252C000%2520trained%2520networks%2520for%2520the%2520hyperparameter%2520%2528HP%2529%2520searches.%2520Then%252C%2520through%2520a%2520comprehensive%2520analysis%252C%2520we%2520find%2520that%2520slight%2520changes%2520to%2520experimental%2520setups%2520or%2520current%2520evaluation%2520metrics%2520drastically%2520shift%2520the%2520common%2520belief%2520that%2520newly%2520published%2520results%2520are%2520advancing%2520the%2520state%2520of%2520the%2520art.%2520Our%2520findings%2520emphasize%2520the%2520need%2520to%2520shift%2520focus%2520away%2520from%2520pursuing%2520ever-more%2520complex%2520models%252C%2520towards%2520enhancing%2520benchmarking%2520practices%2520through%2520rigorous%2520and%2520standardized%2520evaluations%2520that%2520enable%2520more%2520substantiated%2520claims%252C%2520including%2520reproducible%2520HP%2520setups%2520and%2520statistical%2520testing.%2520We%2520offer%2520recommendations%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=There%20are%20no%20Champions%20in%20Supervised%20Long-Term%20Time%20Series%20Forecasting&entry.906535625=Lorenzo%20Brigato%20and%20Rafael%20Morand%20and%20Knut%20Str%C3%B8mmen%20and%20Maria%20Panagiotou%20and%20Markus%20Schmidt%20and%20Stavroula%20Mougiakakou&entry.1292438233=Recent%20advances%20in%20long-term%20time%20series%20forecasting%20have%20introduced%20numerous%20complex%20supervised%20prediction%20models%20that%20consistently%20outperform%20previously%20published%20architectures.%20However%2C%20this%20rapid%20progression%20raises%20concerns%20regarding%20inconsistent%20benchmarking%20and%20reporting%20practices%2C%20which%20may%20undermine%20the%20reliability%20of%20these%20comparisons.%20In%20this%20study%2C%20we%20first%20perform%20a%20broad%2C%20thorough%2C%20and%20reproducible%20evaluation%20of%20the%20top-performing%20supervised%20models%20on%20the%20most%20popular%20benchmark%20and%20additional%20baselines%20representing%20the%20most%20active%20architecture%20families.%20This%20extensive%20evaluation%20assesses%20eight%20models%20on%2014%20datasets%2C%20encompassing%20%24%5Csim%245%2C000%20trained%20networks%20for%20the%20hyperparameter%20%28HP%29%20searches.%20Then%2C%20through%20a%20comprehensive%20analysis%2C%20we%20find%20that%20slight%20changes%20to%20experimental%20setups%20or%20current%20evaluation%20metrics%20drastically%20shift%20the%20common%20belief%20that%20newly%20published%20results%20are%20advancing%20the%20state%20of%20the%20art.%20Our%20findings%20emphasize%20the%20need%20to%20shift%20focus%20away%20from%20pursuing%20ever-more%20complex%20models%2C%20towards%20enhancing%20benchmarking%20practices%20through%20rigorous%20and%20standardized%20evaluations%20that%20enable%20more%20substantiated%20claims%2C%20including%20reproducible%20HP%20setups%20and%20statistical%20testing.%20We%20offer%20recommendations%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2502.14045v2&entry.124074799=Read"},
{"title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications", "author": "Anran Li and Lingfei Qian and Mengmeng Du and Yu Yin and Yan Hu and Zihao Sun and Yihang Fu and Hyunjae Kim and Erica Stutz and Xuguang Ai and Qianqian Xie and Rui Zhu and Jimin Huang and Yifan Yang and Siru Liu and Yih-Chung Tham and Lucila Ohno-Machado and Hyunghoon Cho and Zhiyong Lu and Hua Xu and Qingyu Chen", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in medicine, with many studies adapting them through continued pre-training or fine-tuning on medical data to enhance domain-specific accuracy and safety. However, a key open question remains: to what extent do LLMs memorize medical training data. Memorization can be beneficial when it enables LLMs to retain valuable medical knowledge during domain adaptation. Yet, it also raises concerns. LLMs may inadvertently reproduce sensitive clinical content (e.g., patient-specific details), and excessive memorization may reduce model generalizability, increasing risks of misdiagnosis and making unwarranted recommendations. These risks are further amplified by the generative nature of LLMs, which can not only surface memorized content but also produce overconfident, misleading outputs that may hinder clinical adoption. In this work, we present a study on memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than that reported in the general domain. Moreover, memorization has distinct characteristics during continued pre-training and fine-tuning, and it is persistent: up to 87% of content memorized during continued pre-training remains after fine-tuning on new medical tasks.", "link": "http://arxiv.org/abs/2509.08604v3", "date": "2026-01-09", "relevancy": 2.2843, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization%20in%20Large%20Language%20Models%20in%20Medicine%3A%20Prevalence%2C%20Characteristics%2C%20and%20Implications&body=Title%3A%20Memorization%20in%20Large%20Language%20Models%20in%20Medicine%3A%20Prevalence%2C%20Characteristics%2C%20and%20Implications%0AAuthor%3A%20Anran%20Li%20and%20Lingfei%20Qian%20and%20Mengmeng%20Du%20and%20Yu%20Yin%20and%20Yan%20Hu%20and%20Zihao%20Sun%20and%20Yihang%20Fu%20and%20Hyunjae%20Kim%20and%20Erica%20Stutz%20and%20Xuguang%20Ai%20and%20Qianqian%20Xie%20and%20Rui%20Zhu%20and%20Jimin%20Huang%20and%20Yifan%20Yang%20and%20Siru%20Liu%20and%20Yih-Chung%20Tham%20and%20Lucila%20Ohno-Machado%20and%20Hyunghoon%20Cho%20and%20Zhiyong%20Lu%20and%20Hua%20Xu%20and%20Qingyu%20Chen%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%20medicine%2C%20with%20many%20studies%20adapting%20them%20through%20continued%20pre-training%20or%20fine-tuning%20on%20medical%20data%20to%20enhance%20domain-specific%20accuracy%20and%20safety.%20However%2C%20a%20key%20open%20question%20remains%3A%20to%20what%20extent%20do%20LLMs%20memorize%20medical%20training%20data.%20Memorization%20can%20be%20beneficial%20when%20it%20enables%20LLMs%20to%20retain%20valuable%20medical%20knowledge%20during%20domain%20adaptation.%20Yet%2C%20it%20also%20raises%20concerns.%20LLMs%20may%20inadvertently%20reproduce%20sensitive%20clinical%20content%20%28e.g.%2C%20patient-specific%20details%29%2C%20and%20excessive%20memorization%20may%20reduce%20model%20generalizability%2C%20increasing%20risks%20of%20misdiagnosis%20and%20making%20unwarranted%20recommendations.%20These%20risks%20are%20further%20amplified%20by%20the%20generative%20nature%20of%20LLMs%2C%20which%20can%20not%20only%20surface%20memorized%20content%20but%20also%20produce%20overconfident%2C%20misleading%20outputs%20that%20may%20hinder%20clinical%20adoption.%20In%20this%20work%2C%20we%20present%20a%20study%20on%20memorization%20of%20LLMs%20in%20medicine%2C%20assessing%20its%20prevalence%20%28how%20frequently%20it%20occurs%29%2C%20characteristics%20%28what%20is%20memorized%29%2C%20volume%20%28how%20much%20content%20is%20memorized%29%2C%20and%20potential%20downstream%20impacts%20%28how%20memorization%20may%20affect%20medical%20applications%29.%20We%20systematically%20analyze%20common%20adaptation%20scenarios%3A%20%281%29%20continued%20pretraining%20on%20medical%20corpora%2C%20%282%29%20fine-tuning%20on%20standard%20medical%20benchmarks%2C%20and%20%283%29%20fine-tuning%20on%20real-world%20clinical%20data%2C%20including%20over%2013%2C000%20unique%20inpatient%20records%20from%20Yale%20New%20Haven%20Health%20System.%20The%20results%20demonstrate%20that%20memorization%20is%20prevalent%20across%20all%20adaptation%20scenarios%20and%20significantly%20higher%20than%20that%20reported%20in%20the%20general%20domain.%20Moreover%2C%20memorization%20has%20distinct%20characteristics%20during%20continued%20pre-training%20and%20fine-tuning%2C%20and%20it%20is%20persistent%3A%20up%20to%2087%25%20of%20content%20memorized%20during%20continued%20pre-training%20remains%20after%20fine-tuning%20on%20new%20medical%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2509.08604v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization%2520in%2520Large%2520Language%2520Models%2520in%2520Medicine%253A%2520Prevalence%252C%2520Characteristics%252C%2520and%2520Implications%26entry.906535625%3DAnran%2520Li%2520and%2520Lingfei%2520Qian%2520and%2520Mengmeng%2520Du%2520and%2520Yu%2520Yin%2520and%2520Yan%2520Hu%2520and%2520Zihao%2520Sun%2520and%2520Yihang%2520Fu%2520and%2520Hyunjae%2520Kim%2520and%2520Erica%2520Stutz%2520and%2520Xuguang%2520Ai%2520and%2520Qianqian%2520Xie%2520and%2520Rui%2520Zhu%2520and%2520Jimin%2520Huang%2520and%2520Yifan%2520Yang%2520and%2520Siru%2520Liu%2520and%2520Yih-Chung%2520Tham%2520and%2520Lucila%2520Ohno-Machado%2520and%2520Hyunghoon%2520Cho%2520and%2520Zhiyong%2520Lu%2520and%2520Hua%2520Xu%2520and%2520Qingyu%2520Chen%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520potential%2520in%2520medicine%252C%2520with%2520many%2520studies%2520adapting%2520them%2520through%2520continued%2520pre-training%2520or%2520fine-tuning%2520on%2520medical%2520data%2520to%2520enhance%2520domain-specific%2520accuracy%2520and%2520safety.%2520However%252C%2520a%2520key%2520open%2520question%2520remains%253A%2520to%2520what%2520extent%2520do%2520LLMs%2520memorize%2520medical%2520training%2520data.%2520Memorization%2520can%2520be%2520beneficial%2520when%2520it%2520enables%2520LLMs%2520to%2520retain%2520valuable%2520medical%2520knowledge%2520during%2520domain%2520adaptation.%2520Yet%252C%2520it%2520also%2520raises%2520concerns.%2520LLMs%2520may%2520inadvertently%2520reproduce%2520sensitive%2520clinical%2520content%2520%2528e.g.%252C%2520patient-specific%2520details%2529%252C%2520and%2520excessive%2520memorization%2520may%2520reduce%2520model%2520generalizability%252C%2520increasing%2520risks%2520of%2520misdiagnosis%2520and%2520making%2520unwarranted%2520recommendations.%2520These%2520risks%2520are%2520further%2520amplified%2520by%2520the%2520generative%2520nature%2520of%2520LLMs%252C%2520which%2520can%2520not%2520only%2520surface%2520memorized%2520content%2520but%2520also%2520produce%2520overconfident%252C%2520misleading%2520outputs%2520that%2520may%2520hinder%2520clinical%2520adoption.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520study%2520on%2520memorization%2520of%2520LLMs%2520in%2520medicine%252C%2520assessing%2520its%2520prevalence%2520%2528how%2520frequently%2520it%2520occurs%2529%252C%2520characteristics%2520%2528what%2520is%2520memorized%2529%252C%2520volume%2520%2528how%2520much%2520content%2520is%2520memorized%2529%252C%2520and%2520potential%2520downstream%2520impacts%2520%2528how%2520memorization%2520may%2520affect%2520medical%2520applications%2529.%2520We%2520systematically%2520analyze%2520common%2520adaptation%2520scenarios%253A%2520%25281%2529%2520continued%2520pretraining%2520on%2520medical%2520corpora%252C%2520%25282%2529%2520fine-tuning%2520on%2520standard%2520medical%2520benchmarks%252C%2520and%2520%25283%2529%2520fine-tuning%2520on%2520real-world%2520clinical%2520data%252C%2520including%2520over%252013%252C000%2520unique%2520inpatient%2520records%2520from%2520Yale%2520New%2520Haven%2520Health%2520System.%2520The%2520results%2520demonstrate%2520that%2520memorization%2520is%2520prevalent%2520across%2520all%2520adaptation%2520scenarios%2520and%2520significantly%2520higher%2520than%2520that%2520reported%2520in%2520the%2520general%2520domain.%2520Moreover%252C%2520memorization%2520has%2520distinct%2520characteristics%2520during%2520continued%2520pre-training%2520and%2520fine-tuning%252C%2520and%2520it%2520is%2520persistent%253A%2520up%2520to%252087%2525%2520of%2520content%2520memorized%2520during%2520continued%2520pre-training%2520remains%2520after%2520fine-tuning%2520on%2520new%2520medical%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08604v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%20in%20Large%20Language%20Models%20in%20Medicine%3A%20Prevalence%2C%20Characteristics%2C%20and%20Implications&entry.906535625=Anran%20Li%20and%20Lingfei%20Qian%20and%20Mengmeng%20Du%20and%20Yu%20Yin%20and%20Yan%20Hu%20and%20Zihao%20Sun%20and%20Yihang%20Fu%20and%20Hyunjae%20Kim%20and%20Erica%20Stutz%20and%20Xuguang%20Ai%20and%20Qianqian%20Xie%20and%20Rui%20Zhu%20and%20Jimin%20Huang%20and%20Yifan%20Yang%20and%20Siru%20Liu%20and%20Yih-Chung%20Tham%20and%20Lucila%20Ohno-Machado%20and%20Hyunghoon%20Cho%20and%20Zhiyong%20Lu%20and%20Hua%20Xu%20and%20Qingyu%20Chen&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20potential%20in%20medicine%2C%20with%20many%20studies%20adapting%20them%20through%20continued%20pre-training%20or%20fine-tuning%20on%20medical%20data%20to%20enhance%20domain-specific%20accuracy%20and%20safety.%20However%2C%20a%20key%20open%20question%20remains%3A%20to%20what%20extent%20do%20LLMs%20memorize%20medical%20training%20data.%20Memorization%20can%20be%20beneficial%20when%20it%20enables%20LLMs%20to%20retain%20valuable%20medical%20knowledge%20during%20domain%20adaptation.%20Yet%2C%20it%20also%20raises%20concerns.%20LLMs%20may%20inadvertently%20reproduce%20sensitive%20clinical%20content%20%28e.g.%2C%20patient-specific%20details%29%2C%20and%20excessive%20memorization%20may%20reduce%20model%20generalizability%2C%20increasing%20risks%20of%20misdiagnosis%20and%20making%20unwarranted%20recommendations.%20These%20risks%20are%20further%20amplified%20by%20the%20generative%20nature%20of%20LLMs%2C%20which%20can%20not%20only%20surface%20memorized%20content%20but%20also%20produce%20overconfident%2C%20misleading%20outputs%20that%20may%20hinder%20clinical%20adoption.%20In%20this%20work%2C%20we%20present%20a%20study%20on%20memorization%20of%20LLMs%20in%20medicine%2C%20assessing%20its%20prevalence%20%28how%20frequently%20it%20occurs%29%2C%20characteristics%20%28what%20is%20memorized%29%2C%20volume%20%28how%20much%20content%20is%20memorized%29%2C%20and%20potential%20downstream%20impacts%20%28how%20memorization%20may%20affect%20medical%20applications%29.%20We%20systematically%20analyze%20common%20adaptation%20scenarios%3A%20%281%29%20continued%20pretraining%20on%20medical%20corpora%2C%20%282%29%20fine-tuning%20on%20standard%20medical%20benchmarks%2C%20and%20%283%29%20fine-tuning%20on%20real-world%20clinical%20data%2C%20including%20over%2013%2C000%20unique%20inpatient%20records%20from%20Yale%20New%20Haven%20Health%20System.%20The%20results%20demonstrate%20that%20memorization%20is%20prevalent%20across%20all%20adaptation%20scenarios%20and%20significantly%20higher%20than%20that%20reported%20in%20the%20general%20domain.%20Moreover%2C%20memorization%20has%20distinct%20characteristics%20during%20continued%20pre-training%20and%20fine-tuning%2C%20and%20it%20is%20persistent%3A%20up%20to%2087%25%20of%20content%20memorized%20during%20continued%20pre-training%20remains%20after%20fine-tuning%20on%20new%20medical%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2509.08604v3&entry.124074799=Read"},
{"title": "Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification", "author": "Quanjiang Li and Zhiming Liu and Tianxiang Xu and Tingjin Luo and Chenping Hou", "abstract": "Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.", "link": "http://arxiv.org/abs/2601.05785v1", "date": "2026-01-09", "relevancy": 2.2762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6026}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5682}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Disentangled%20Representation%20Learning%20for%20Incomplete%20Multi-View%20Multi-Label%20Classification&body=Title%3A%20Adaptive%20Disentangled%20Representation%20Learning%20for%20Incomplete%20Multi-View%20Multi-Label%20Classification%0AAuthor%3A%20Quanjiang%20Li%20and%20Zhiming%20Liu%20and%20Tianxiang%20Xu%20and%20Tingjin%20Luo%20and%20Chenping%20Hou%0AAbstract%3A%20Multi-view%20multi-label%20learning%20frequently%20suffers%20from%20simultaneous%20feature%20absence%20and%20incomplete%20annotations%2C%20due%20to%20challenges%20in%20data%20acquisition%20and%20cost-intensive%20supervision.%20To%20tackle%20the%20complex%20yet%20highly%20practical%20problem%20while%20overcoming%20the%20existing%20limitations%20of%20feature%20recovery%2C%20representation%20disentanglement%2C%20and%20label%20semantics%20modeling%2C%20we%20propose%20an%20Adaptive%20Disentangled%20Representation%20Learning%20method%20%28ADRL%29.%20ADRL%20achieves%20robust%20view%20completion%20by%20propagating%20feature-level%20affinity%20across%20modalities%20with%20neighborhood%20awareness%2C%20and%20reinforces%20reconstruction%20effectiveness%20by%20leveraging%20a%20stochastic%20masking%20strategy.%20Through%20disseminating%20category-level%20association%20across%20label%20distributions%2C%20ADRL%20refines%20distribution%20parameters%20for%20capturing%20interdependent%20label%20prototypes.%20Besides%2C%20we%20formulate%20a%20mutual-information-based%20objective%20to%20promote%20consistency%20among%20shared%20representations%20and%20suppress%20information%20overlap%20between%20view-specific%20representation%20and%20other%20modalities.%20Theoretically%2C%20we%20derive%20the%20tractable%20bounds%20to%20train%20the%20dual-channel%20network.%20Moreover%2C%20ADRL%20performs%20prototype-specific%20feature%20selection%20by%20enabling%20independent%20interactions%20between%20label%20embeddings%20and%20view%20representations%2C%20accompanied%20by%20the%20generation%20of%20pseudo-labels%20for%20each%20category.%20The%20structural%20characteristics%20of%20the%20pseudo-label%20space%20are%20then%20exploited%20to%20guide%20a%20discriminative%20trade-off%20during%20view%20fusion.%20Finally%2C%20extensive%20experiments%20on%20public%20datasets%20and%20real-world%20applications%20demonstrate%20the%20superior%20performance%20of%20ADRL.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Disentangled%2520Representation%2520Learning%2520for%2520Incomplete%2520Multi-View%2520Multi-Label%2520Classification%26entry.906535625%3DQuanjiang%2520Li%2520and%2520Zhiming%2520Liu%2520and%2520Tianxiang%2520Xu%2520and%2520Tingjin%2520Luo%2520and%2520Chenping%2520Hou%26entry.1292438233%3DMulti-view%2520multi-label%2520learning%2520frequently%2520suffers%2520from%2520simultaneous%2520feature%2520absence%2520and%2520incomplete%2520annotations%252C%2520due%2520to%2520challenges%2520in%2520data%2520acquisition%2520and%2520cost-intensive%2520supervision.%2520To%2520tackle%2520the%2520complex%2520yet%2520highly%2520practical%2520problem%2520while%2520overcoming%2520the%2520existing%2520limitations%2520of%2520feature%2520recovery%252C%2520representation%2520disentanglement%252C%2520and%2520label%2520semantics%2520modeling%252C%2520we%2520propose%2520an%2520Adaptive%2520Disentangled%2520Representation%2520Learning%2520method%2520%2528ADRL%2529.%2520ADRL%2520achieves%2520robust%2520view%2520completion%2520by%2520propagating%2520feature-level%2520affinity%2520across%2520modalities%2520with%2520neighborhood%2520awareness%252C%2520and%2520reinforces%2520reconstruction%2520effectiveness%2520by%2520leveraging%2520a%2520stochastic%2520masking%2520strategy.%2520Through%2520disseminating%2520category-level%2520association%2520across%2520label%2520distributions%252C%2520ADRL%2520refines%2520distribution%2520parameters%2520for%2520capturing%2520interdependent%2520label%2520prototypes.%2520Besides%252C%2520we%2520formulate%2520a%2520mutual-information-based%2520objective%2520to%2520promote%2520consistency%2520among%2520shared%2520representations%2520and%2520suppress%2520information%2520overlap%2520between%2520view-specific%2520representation%2520and%2520other%2520modalities.%2520Theoretically%252C%2520we%2520derive%2520the%2520tractable%2520bounds%2520to%2520train%2520the%2520dual-channel%2520network.%2520Moreover%252C%2520ADRL%2520performs%2520prototype-specific%2520feature%2520selection%2520by%2520enabling%2520independent%2520interactions%2520between%2520label%2520embeddings%2520and%2520view%2520representations%252C%2520accompanied%2520by%2520the%2520generation%2520of%2520pseudo-labels%2520for%2520each%2520category.%2520The%2520structural%2520characteristics%2520of%2520the%2520pseudo-label%2520space%2520are%2520then%2520exploited%2520to%2520guide%2520a%2520discriminative%2520trade-off%2520during%2520view%2520fusion.%2520Finally%252C%2520extensive%2520experiments%2520on%2520public%2520datasets%2520and%2520real-world%2520applications%2520demonstrate%2520the%2520superior%2520performance%2520of%2520ADRL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Disentangled%20Representation%20Learning%20for%20Incomplete%20Multi-View%20Multi-Label%20Classification&entry.906535625=Quanjiang%20Li%20and%20Zhiming%20Liu%20and%20Tianxiang%20Xu%20and%20Tingjin%20Luo%20and%20Chenping%20Hou&entry.1292438233=Multi-view%20multi-label%20learning%20frequently%20suffers%20from%20simultaneous%20feature%20absence%20and%20incomplete%20annotations%2C%20due%20to%20challenges%20in%20data%20acquisition%20and%20cost-intensive%20supervision.%20To%20tackle%20the%20complex%20yet%20highly%20practical%20problem%20while%20overcoming%20the%20existing%20limitations%20of%20feature%20recovery%2C%20representation%20disentanglement%2C%20and%20label%20semantics%20modeling%2C%20we%20propose%20an%20Adaptive%20Disentangled%20Representation%20Learning%20method%20%28ADRL%29.%20ADRL%20achieves%20robust%20view%20completion%20by%20propagating%20feature-level%20affinity%20across%20modalities%20with%20neighborhood%20awareness%2C%20and%20reinforces%20reconstruction%20effectiveness%20by%20leveraging%20a%20stochastic%20masking%20strategy.%20Through%20disseminating%20category-level%20association%20across%20label%20distributions%2C%20ADRL%20refines%20distribution%20parameters%20for%20capturing%20interdependent%20label%20prototypes.%20Besides%2C%20we%20formulate%20a%20mutual-information-based%20objective%20to%20promote%20consistency%20among%20shared%20representations%20and%20suppress%20information%20overlap%20between%20view-specific%20representation%20and%20other%20modalities.%20Theoretically%2C%20we%20derive%20the%20tractable%20bounds%20to%20train%20the%20dual-channel%20network.%20Moreover%2C%20ADRL%20performs%20prototype-specific%20feature%20selection%20by%20enabling%20independent%20interactions%20between%20label%20embeddings%20and%20view%20representations%2C%20accompanied%20by%20the%20generation%20of%20pseudo-labels%20for%20each%20category.%20The%20structural%20characteristics%20of%20the%20pseudo-label%20space%20are%20then%20exploited%20to%20guide%20a%20discriminative%20trade-off%20during%20view%20fusion.%20Finally%2C%20extensive%20experiments%20on%20public%20datasets%20and%20real-world%20applications%20demonstrate%20the%20superior%20performance%20of%20ADRL.&entry.1838667208=http%3A//arxiv.org/abs/2601.05785v1&entry.124074799=Read"},
{"title": "Kidney Cancer Detection Using 3D-Based Latent Diffusion Models", "author": "Jen Dusseljee and Sarah de Boer and Alessa Hering", "abstract": "In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.", "link": "http://arxiv.org/abs/2601.05852v1", "date": "2026-01-09", "relevancy": 2.2751, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5693}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5692}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kidney%20Cancer%20Detection%20Using%203D-Based%20Latent%20Diffusion%20Models&body=Title%3A%20Kidney%20Cancer%20Detection%20Using%203D-Based%20Latent%20Diffusion%20Models%0AAuthor%3A%20Jen%20Dusseljee%20and%20Sarah%20de%20Boer%20and%20Alessa%20Hering%0AAbstract%3A%20In%20this%20work%2C%20we%20present%20a%20novel%20latent%20diffusion-based%20pipeline%20for%203D%20kidney%20anomaly%20detection%20on%20contrast-enhanced%20abdominal%20CT.%20The%20method%20combines%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%2C%20Denoising%20Diffusion%20Implicit%20Models%20%28DDIMs%29%2C%20and%20Vector-Quantized%20Generative%20Adversarial%20Networks%20%28VQ-GANs%29.%20Unlike%20prior%20slice-wise%20approaches%2C%20our%20method%20operates%20directly%20on%20an%20image%20volume%20and%20leverages%20weak%20supervision%20with%20only%20case-level%20pseudo-labels.%20We%20benchmark%20our%20approach%20against%20state-of-the-art%20supervised%20segmentation%20and%20detection%20models.%20This%20study%20demonstrates%20the%20feasibility%20and%20promise%20of%203D%20latent%20diffusion%20for%20weakly%20supervised%20anomaly%20detection.%20While%20the%20current%20results%20do%20not%20yet%20match%20supervised%20baselines%2C%20they%20reveal%20key%20directions%20for%20improving%20reconstruction%20fidelity%20and%20lesion%20localization.%20Our%20findings%20provide%20an%20important%20step%20toward%20annotation-efficient%2C%20generative%20modeling%20of%20complex%20abdominal%20anatomy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKidney%2520Cancer%2520Detection%2520Using%25203D-Based%2520Latent%2520Diffusion%2520Models%26entry.906535625%3DJen%2520Dusseljee%2520and%2520Sarah%2520de%2520Boer%2520and%2520Alessa%2520Hering%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520latent%2520diffusion-based%2520pipeline%2520for%25203D%2520kidney%2520anomaly%2520detection%2520on%2520contrast-enhanced%2520abdominal%2520CT.%2520The%2520method%2520combines%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%252C%2520Denoising%2520Diffusion%2520Implicit%2520Models%2520%2528DDIMs%2529%252C%2520and%2520Vector-Quantized%2520Generative%2520Adversarial%2520Networks%2520%2528VQ-GANs%2529.%2520Unlike%2520prior%2520slice-wise%2520approaches%252C%2520our%2520method%2520operates%2520directly%2520on%2520an%2520image%2520volume%2520and%2520leverages%2520weak%2520supervision%2520with%2520only%2520case-level%2520pseudo-labels.%2520We%2520benchmark%2520our%2520approach%2520against%2520state-of-the-art%2520supervised%2520segmentation%2520and%2520detection%2520models.%2520This%2520study%2520demonstrates%2520the%2520feasibility%2520and%2520promise%2520of%25203D%2520latent%2520diffusion%2520for%2520weakly%2520supervised%2520anomaly%2520detection.%2520While%2520the%2520current%2520results%2520do%2520not%2520yet%2520match%2520supervised%2520baselines%252C%2520they%2520reveal%2520key%2520directions%2520for%2520improving%2520reconstruction%2520fidelity%2520and%2520lesion%2520localization.%2520Our%2520findings%2520provide%2520an%2520important%2520step%2520toward%2520annotation-efficient%252C%2520generative%2520modeling%2520of%2520complex%2520abdominal%2520anatomy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kidney%20Cancer%20Detection%20Using%203D-Based%20Latent%20Diffusion%20Models&entry.906535625=Jen%20Dusseljee%20and%20Sarah%20de%20Boer%20and%20Alessa%20Hering&entry.1292438233=In%20this%20work%2C%20we%20present%20a%20novel%20latent%20diffusion-based%20pipeline%20for%203D%20kidney%20anomaly%20detection%20on%20contrast-enhanced%20abdominal%20CT.%20The%20method%20combines%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%2C%20Denoising%20Diffusion%20Implicit%20Models%20%28DDIMs%29%2C%20and%20Vector-Quantized%20Generative%20Adversarial%20Networks%20%28VQ-GANs%29.%20Unlike%20prior%20slice-wise%20approaches%2C%20our%20method%20operates%20directly%20on%20an%20image%20volume%20and%20leverages%20weak%20supervision%20with%20only%20case-level%20pseudo-labels.%20We%20benchmark%20our%20approach%20against%20state-of-the-art%20supervised%20segmentation%20and%20detection%20models.%20This%20study%20demonstrates%20the%20feasibility%20and%20promise%20of%203D%20latent%20diffusion%20for%20weakly%20supervised%20anomaly%20detection.%20While%20the%20current%20results%20do%20not%20yet%20match%20supervised%20baselines%2C%20they%20reveal%20key%20directions%20for%20improving%20reconstruction%20fidelity%20and%20lesion%20localization.%20Our%20findings%20provide%20an%20important%20step%20toward%20annotation-efficient%2C%20generative%20modeling%20of%20complex%20abdominal%20anatomy.&entry.1838667208=http%3A//arxiv.org/abs/2601.05852v1&entry.124074799=Read"},
{"title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "author": "Xiaoshuai Song and Haofei Chang and Guanting Dong and Yutao Zhu and Zhicheng Dou and Ji-Rong Wen", "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "link": "http://arxiv.org/abs/2601.05808v1", "date": "2026-01-09", "relevancy": 2.2613, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6052}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnvScaler%3A%20Scaling%20Tool-Interactive%20Environments%20for%20LLM%20Agent%20via%20Programmatic%20Synthesis&body=Title%3A%20EnvScaler%3A%20Scaling%20Tool-Interactive%20Environments%20for%20LLM%20Agent%20via%20Programmatic%20Synthesis%0AAuthor%3A%20Xiaoshuai%20Song%20and%20Haofei%20Chang%20and%20Guanting%20Dong%20and%20Yutao%20Zhu%20and%20Zhicheng%20Dou%20and%20Ji-Rong%20Wen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20expected%20to%20be%20trained%20to%20act%20as%20agents%20in%20various%20real-world%20environments%2C%20but%20this%20process%20relies%20on%20rich%20and%20varied%20tool-interaction%20sandboxes.%20However%2C%20access%20to%20real%20systems%20is%20often%20restricted%3B%20LLM-simulated%20environments%20are%20prone%20to%20hallucinations%20and%20inconsistencies%3B%20and%20manually%20built%20sandboxes%20are%20hard%20to%20scale.%20In%20this%20paper%2C%20we%20propose%20EnvScaler%2C%20an%20automated%20framework%20for%20scalable%20tool-interaction%20environments%20via%20programmatic%20synthesis.%20EnvScaler%20comprises%20two%20components.%20First%2C%20SkelBuilder%20constructs%20diverse%20environment%20skeletons%20through%20topic%20mining%2C%20logic%20modeling%2C%20and%20quality%20evaluation.%20Then%2C%20ScenGenerator%20generates%20multiple%20task%20scenarios%20and%20rule-based%20trajectory%20validation%20functions%20for%20each%20environment.%20With%20EnvScaler%2C%20we%20synthesize%20191%20environments%20and%20about%207K%20scenarios%2C%20and%20apply%20them%20to%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%20for%20Qwen3%20series%20models.%20Results%20on%20three%20benchmarks%20show%20that%20EnvScaler%20significantly%20improves%20LLMs%27%20ability%20to%20solve%20tasks%20in%20complex%20environments%20involving%20multi-turn%2C%20multi-tool%20interactions.%20We%20release%20our%20code%20and%20data%20at%20https%3A//github.com/RUC-NLPIR/EnvScaler.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvScaler%253A%2520Scaling%2520Tool-Interactive%2520Environments%2520for%2520LLM%2520Agent%2520via%2520Programmatic%2520Synthesis%26entry.906535625%3DXiaoshuai%2520Song%2520and%2520Haofei%2520Chang%2520and%2520Guanting%2520Dong%2520and%2520Yutao%2520Zhu%2520and%2520Zhicheng%2520Dou%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520expected%2520to%2520be%2520trained%2520to%2520act%2520as%2520agents%2520in%2520various%2520real-world%2520environments%252C%2520but%2520this%2520process%2520relies%2520on%2520rich%2520and%2520varied%2520tool-interaction%2520sandboxes.%2520However%252C%2520access%2520to%2520real%2520systems%2520is%2520often%2520restricted%253B%2520LLM-simulated%2520environments%2520are%2520prone%2520to%2520hallucinations%2520and%2520inconsistencies%253B%2520and%2520manually%2520built%2520sandboxes%2520are%2520hard%2520to%2520scale.%2520In%2520this%2520paper%252C%2520we%2520propose%2520EnvScaler%252C%2520an%2520automated%2520framework%2520for%2520scalable%2520tool-interaction%2520environments%2520via%2520programmatic%2520synthesis.%2520EnvScaler%2520comprises%2520two%2520components.%2520First%252C%2520SkelBuilder%2520constructs%2520diverse%2520environment%2520skeletons%2520through%2520topic%2520mining%252C%2520logic%2520modeling%252C%2520and%2520quality%2520evaluation.%2520Then%252C%2520ScenGenerator%2520generates%2520multiple%2520task%2520scenarios%2520and%2520rule-based%2520trajectory%2520validation%2520functions%2520for%2520each%2520environment.%2520With%2520EnvScaler%252C%2520we%2520synthesize%2520191%2520environments%2520and%2520about%25207K%2520scenarios%252C%2520and%2520apply%2520them%2520to%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Reinforcement%2520Learning%2520%2528RL%2529%2520for%2520Qwen3%2520series%2520models.%2520Results%2520on%2520three%2520benchmarks%2520show%2520that%2520EnvScaler%2520significantly%2520improves%2520LLMs%2527%2520ability%2520to%2520solve%2520tasks%2520in%2520complex%2520environments%2520involving%2520multi-turn%252C%2520multi-tool%2520interactions.%2520We%2520release%2520our%2520code%2520and%2520data%2520at%2520https%253A//github.com/RUC-NLPIR/EnvScaler.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnvScaler%3A%20Scaling%20Tool-Interactive%20Environments%20for%20LLM%20Agent%20via%20Programmatic%20Synthesis&entry.906535625=Xiaoshuai%20Song%20and%20Haofei%20Chang%20and%20Guanting%20Dong%20and%20Yutao%20Zhu%20and%20Zhicheng%20Dou%20and%20Ji-Rong%20Wen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20expected%20to%20be%20trained%20to%20act%20as%20agents%20in%20various%20real-world%20environments%2C%20but%20this%20process%20relies%20on%20rich%20and%20varied%20tool-interaction%20sandboxes.%20However%2C%20access%20to%20real%20systems%20is%20often%20restricted%3B%20LLM-simulated%20environments%20are%20prone%20to%20hallucinations%20and%20inconsistencies%3B%20and%20manually%20built%20sandboxes%20are%20hard%20to%20scale.%20In%20this%20paper%2C%20we%20propose%20EnvScaler%2C%20an%20automated%20framework%20for%20scalable%20tool-interaction%20environments%20via%20programmatic%20synthesis.%20EnvScaler%20comprises%20two%20components.%20First%2C%20SkelBuilder%20constructs%20diverse%20environment%20skeletons%20through%20topic%20mining%2C%20logic%20modeling%2C%20and%20quality%20evaluation.%20Then%2C%20ScenGenerator%20generates%20multiple%20task%20scenarios%20and%20rule-based%20trajectory%20validation%20functions%20for%20each%20environment.%20With%20EnvScaler%2C%20we%20synthesize%20191%20environments%20and%20about%207K%20scenarios%2C%20and%20apply%20them%20to%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Reinforcement%20Learning%20%28RL%29%20for%20Qwen3%20series%20models.%20Results%20on%20three%20benchmarks%20show%20that%20EnvScaler%20significantly%20improves%20LLMs%27%20ability%20to%20solve%20tasks%20in%20complex%20environments%20involving%20multi-turn%2C%20multi-tool%20interactions.%20We%20release%20our%20code%20and%20data%20at%20https%3A//github.com/RUC-NLPIR/EnvScaler.&entry.1838667208=http%3A//arxiv.org/abs/2601.05808v1&entry.124074799=Read"},
{"title": "RobustFormer: Noise-Robust Pre-training for images and videos", "author": "Ashish Bastola and Nishant Luitel and Hao Wang and Danda Pani Paudel and Roshani Poudel and Abolfazl Razi", "abstract": "While deep learning-based models like transformers, have revolutionized time-series and vision tasks, they remain highly susceptible to noise and often overfit on noisy patterns rather than robust features. This issue is exacerbated in vision transformers, which rely on pixel-level details that can easily be corrupt. To address this, we leverage the discrete wavelet transform (DWT) for its ability to decompose into multi-resolution layers, isolating noise primarily in the high frequency domain while preserving essential low-frequency information for resilient feature learning. Conventional DWT-based methods, however, struggle with computational inefficiencies due to the requirement for a subsequent inverse discrete wavelet transform (IDWT) step. In this work, we introduce RobustFormer, a novel framework that enables noise-robust masked autoencoder (MAE) pre-training for both images and videos by using DWT for efficient downsampling, eliminating the need for expensive IDWT reconstruction and simplifying the attention mechanism to focus on noise-resilient multi-scale representations. To our knowledge, RobustFormer is the first DWT-based method fully compatible with video inputs and MAE-style pre-training. Extensive experiments on noisy image and video datasets demonstrate that our approach achieves up to 8% increase in Top-1 classification accuracy under severe noise conditions in Imagenet-C and up to 2.7% in Imagenet-P standard benchmarks compared to the baseline and up to 13% higher Top-1 accuracy on UCF-101 under severe custom noise perturbations while maintaining similar accuracy scores for clean datasets. We also observe the reduction of computation complexity by up to 4.4% through IDWT removal compared to VideoMAE baseline without any performance drop.", "link": "http://arxiv.org/abs/2411.13040v2", "date": "2026-01-09", "relevancy": 2.2565, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5979}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustFormer%3A%20Noise-Robust%20Pre-training%20for%20images%20and%20videos&body=Title%3A%20RobustFormer%3A%20Noise-Robust%20Pre-training%20for%20images%20and%20videos%0AAuthor%3A%20Ashish%20Bastola%20and%20Nishant%20Luitel%20and%20Hao%20Wang%20and%20Danda%20Pani%20Paudel%20and%20Roshani%20Poudel%20and%20Abolfazl%20Razi%0AAbstract%3A%20While%20deep%20learning-based%20models%20like%20transformers%2C%20have%20revolutionized%20time-series%20and%20vision%20tasks%2C%20they%20remain%20highly%20susceptible%20to%20noise%20and%20often%20overfit%20on%20noisy%20patterns%20rather%20than%20robust%20features.%20This%20issue%20is%20exacerbated%20in%20vision%20transformers%2C%20which%20rely%20on%20pixel-level%20details%20that%20can%20easily%20be%20corrupt.%20To%20address%20this%2C%20we%20leverage%20the%20discrete%20wavelet%20transform%20%28DWT%29%20for%20its%20ability%20to%20decompose%20into%20multi-resolution%20layers%2C%20isolating%20noise%20primarily%20in%20the%20high%20frequency%20domain%20while%20preserving%20essential%20low-frequency%20information%20for%20resilient%20feature%20learning.%20Conventional%20DWT-based%20methods%2C%20however%2C%20struggle%20with%20computational%20inefficiencies%20due%20to%20the%20requirement%20for%20a%20subsequent%20inverse%20discrete%20wavelet%20transform%20%28IDWT%29%20step.%20In%20this%20work%2C%20we%20introduce%20RobustFormer%2C%20a%20novel%20framework%20that%20enables%20noise-robust%20masked%20autoencoder%20%28MAE%29%20pre-training%20for%20both%20images%20and%20videos%20by%20using%20DWT%20for%20efficient%20downsampling%2C%20eliminating%20the%20need%20for%20expensive%20IDWT%20reconstruction%20and%20simplifying%20the%20attention%20mechanism%20to%20focus%20on%20noise-resilient%20multi-scale%20representations.%20To%20our%20knowledge%2C%20RobustFormer%20is%20the%20first%20DWT-based%20method%20fully%20compatible%20with%20video%20inputs%20and%20MAE-style%20pre-training.%20Extensive%20experiments%20on%20noisy%20image%20and%20video%20datasets%20demonstrate%20that%20our%20approach%20achieves%20up%20to%208%25%20increase%20in%20Top-1%20classification%20accuracy%20under%20severe%20noise%20conditions%20in%20Imagenet-C%20and%20up%20to%202.7%25%20in%20Imagenet-P%20standard%20benchmarks%20compared%20to%20the%20baseline%20and%20up%20to%2013%25%20higher%20Top-1%20accuracy%20on%20UCF-101%20under%20severe%20custom%20noise%20perturbations%20while%20maintaining%20similar%20accuracy%20scores%20for%20clean%20datasets.%20We%20also%20observe%20the%20reduction%20of%20computation%20complexity%20by%20up%20to%204.4%25%20through%20IDWT%20removal%20compared%20to%20VideoMAE%20baseline%20without%20any%20performance%20drop.%0ALink%3A%20http%3A//arxiv.org/abs/2411.13040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustFormer%253A%2520Noise-Robust%2520Pre-training%2520for%2520images%2520and%2520videos%26entry.906535625%3DAshish%2520Bastola%2520and%2520Nishant%2520Luitel%2520and%2520Hao%2520Wang%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Roshani%2520Poudel%2520and%2520Abolfazl%2520Razi%26entry.1292438233%3DWhile%2520deep%2520learning-based%2520models%2520like%2520transformers%252C%2520have%2520revolutionized%2520time-series%2520and%2520vision%2520tasks%252C%2520they%2520remain%2520highly%2520susceptible%2520to%2520noise%2520and%2520often%2520overfit%2520on%2520noisy%2520patterns%2520rather%2520than%2520robust%2520features.%2520This%2520issue%2520is%2520exacerbated%2520in%2520vision%2520transformers%252C%2520which%2520rely%2520on%2520pixel-level%2520details%2520that%2520can%2520easily%2520be%2520corrupt.%2520To%2520address%2520this%252C%2520we%2520leverage%2520the%2520discrete%2520wavelet%2520transform%2520%2528DWT%2529%2520for%2520its%2520ability%2520to%2520decompose%2520into%2520multi-resolution%2520layers%252C%2520isolating%2520noise%2520primarily%2520in%2520the%2520high%2520frequency%2520domain%2520while%2520preserving%2520essential%2520low-frequency%2520information%2520for%2520resilient%2520feature%2520learning.%2520Conventional%2520DWT-based%2520methods%252C%2520however%252C%2520struggle%2520with%2520computational%2520inefficiencies%2520due%2520to%2520the%2520requirement%2520for%2520a%2520subsequent%2520inverse%2520discrete%2520wavelet%2520transform%2520%2528IDWT%2529%2520step.%2520In%2520this%2520work%252C%2520we%2520introduce%2520RobustFormer%252C%2520a%2520novel%2520framework%2520that%2520enables%2520noise-robust%2520masked%2520autoencoder%2520%2528MAE%2529%2520pre-training%2520for%2520both%2520images%2520and%2520videos%2520by%2520using%2520DWT%2520for%2520efficient%2520downsampling%252C%2520eliminating%2520the%2520need%2520for%2520expensive%2520IDWT%2520reconstruction%2520and%2520simplifying%2520the%2520attention%2520mechanism%2520to%2520focus%2520on%2520noise-resilient%2520multi-scale%2520representations.%2520To%2520our%2520knowledge%252C%2520RobustFormer%2520is%2520the%2520first%2520DWT-based%2520method%2520fully%2520compatible%2520with%2520video%2520inputs%2520and%2520MAE-style%2520pre-training.%2520Extensive%2520experiments%2520on%2520noisy%2520image%2520and%2520video%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520up%2520to%25208%2525%2520increase%2520in%2520Top-1%2520classification%2520accuracy%2520under%2520severe%2520noise%2520conditions%2520in%2520Imagenet-C%2520and%2520up%2520to%25202.7%2525%2520in%2520Imagenet-P%2520standard%2520benchmarks%2520compared%2520to%2520the%2520baseline%2520and%2520up%2520to%252013%2525%2520higher%2520Top-1%2520accuracy%2520on%2520UCF-101%2520under%2520severe%2520custom%2520noise%2520perturbations%2520while%2520maintaining%2520similar%2520accuracy%2520scores%2520for%2520clean%2520datasets.%2520We%2520also%2520observe%2520the%2520reduction%2520of%2520computation%2520complexity%2520by%2520up%2520to%25204.4%2525%2520through%2520IDWT%2520removal%2520compared%2520to%2520VideoMAE%2520baseline%2520without%2520any%2520performance%2520drop.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustFormer%3A%20Noise-Robust%20Pre-training%20for%20images%20and%20videos&entry.906535625=Ashish%20Bastola%20and%20Nishant%20Luitel%20and%20Hao%20Wang%20and%20Danda%20Pani%20Paudel%20and%20Roshani%20Poudel%20and%20Abolfazl%20Razi&entry.1292438233=While%20deep%20learning-based%20models%20like%20transformers%2C%20have%20revolutionized%20time-series%20and%20vision%20tasks%2C%20they%20remain%20highly%20susceptible%20to%20noise%20and%20often%20overfit%20on%20noisy%20patterns%20rather%20than%20robust%20features.%20This%20issue%20is%20exacerbated%20in%20vision%20transformers%2C%20which%20rely%20on%20pixel-level%20details%20that%20can%20easily%20be%20corrupt.%20To%20address%20this%2C%20we%20leverage%20the%20discrete%20wavelet%20transform%20%28DWT%29%20for%20its%20ability%20to%20decompose%20into%20multi-resolution%20layers%2C%20isolating%20noise%20primarily%20in%20the%20high%20frequency%20domain%20while%20preserving%20essential%20low-frequency%20information%20for%20resilient%20feature%20learning.%20Conventional%20DWT-based%20methods%2C%20however%2C%20struggle%20with%20computational%20inefficiencies%20due%20to%20the%20requirement%20for%20a%20subsequent%20inverse%20discrete%20wavelet%20transform%20%28IDWT%29%20step.%20In%20this%20work%2C%20we%20introduce%20RobustFormer%2C%20a%20novel%20framework%20that%20enables%20noise-robust%20masked%20autoencoder%20%28MAE%29%20pre-training%20for%20both%20images%20and%20videos%20by%20using%20DWT%20for%20efficient%20downsampling%2C%20eliminating%20the%20need%20for%20expensive%20IDWT%20reconstruction%20and%20simplifying%20the%20attention%20mechanism%20to%20focus%20on%20noise-resilient%20multi-scale%20representations.%20To%20our%20knowledge%2C%20RobustFormer%20is%20the%20first%20DWT-based%20method%20fully%20compatible%20with%20video%20inputs%20and%20MAE-style%20pre-training.%20Extensive%20experiments%20on%20noisy%20image%20and%20video%20datasets%20demonstrate%20that%20our%20approach%20achieves%20up%20to%208%25%20increase%20in%20Top-1%20classification%20accuracy%20under%20severe%20noise%20conditions%20in%20Imagenet-C%20and%20up%20to%202.7%25%20in%20Imagenet-P%20standard%20benchmarks%20compared%20to%20the%20baseline%20and%20up%20to%2013%25%20higher%20Top-1%20accuracy%20on%20UCF-101%20under%20severe%20custom%20noise%20perturbations%20while%20maintaining%20similar%20accuracy%20scores%20for%20clean%20datasets.%20We%20also%20observe%20the%20reduction%20of%20computation%20complexity%20by%20up%20to%204.4%25%20through%20IDWT%20removal%20compared%20to%20VideoMAE%20baseline%20without%20any%20performance%20drop.&entry.1838667208=http%3A//arxiv.org/abs/2411.13040v2&entry.124074799=Read"},
{"title": "TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment", "author": "Jin Wang and Jianxiang Lu and Guangzheng Xu and Comi Chen and Haoyu Yang and Linqing Wang and Peng Chen and Mingtao Chen and Zhichao Hu and Longhuang Wu and Shuai Shao and Qinglin Lu and Ping Luo", "abstract": "Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.", "link": "http://arxiv.org/abs/2601.05729v1", "date": "2026-01-09", "relevancy": 2.2128, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5536}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5536}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAGRPO%3A%20Boosting%20GRPO%20on%20Image-to-Video%20Generation%20with%20Direct%20Trajectory%20Alignment&body=Title%3A%20TAGRPO%3A%20Boosting%20GRPO%20on%20Image-to-Video%20Generation%20with%20Direct%20Trajectory%20Alignment%0AAuthor%3A%20Jin%20Wang%20and%20Jianxiang%20Lu%20and%20Guangzheng%20Xu%20and%20Comi%20Chen%20and%20Haoyu%20Yang%20and%20Linqing%20Wang%20and%20Peng%20Chen%20and%20Mingtao%20Chen%20and%20Zhichao%20Hu%20and%20Longhuang%20Wu%20and%20Shuai%20Shao%20and%20Qinglin%20Lu%20and%20Ping%20Luo%0AAbstract%3A%20Recent%20studies%20have%20demonstrated%20the%20efficacy%20of%20integrating%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20into%20flow%20matching%20models%2C%20particularly%20for%20text-to-image%20and%20text-to-video%20generation.%20However%2C%20we%20find%20that%20directly%20applying%20these%20techniques%20to%20image-to-video%20%28I2V%29%20models%20often%20fails%20to%20yield%20consistent%20reward%20improvements.%20To%20address%20this%20limitation%2C%20we%20present%20TAGRPO%2C%20a%20robust%20post-training%20framework%20for%20I2V%20models%20inspired%20by%20contrastive%20learning.%20Our%20approach%20is%20grounded%20in%20the%20observation%20that%20rollout%20videos%20generated%20from%20identical%20initial%20noise%20provide%20superior%20guidance%20for%20optimization.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20novel%20GRPO%20loss%20applied%20to%20intermediate%20latents%2C%20encouraging%20direct%20alignment%20with%20high-reward%20trajectories%20while%20maximizing%20distance%20from%20low-reward%20counterparts.%20Furthermore%2C%20we%20introduce%20a%20memory%20bank%20for%20rollout%20videos%20to%20enhance%20diversity%20and%20reduce%20computational%20overhead.%20Despite%20its%20simplicity%2C%20TAGRPO%20achieves%20significant%20improvements%20over%20DanceGRPO%20in%20I2V%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAGRPO%253A%2520Boosting%2520GRPO%2520on%2520Image-to-Video%2520Generation%2520with%2520Direct%2520Trajectory%2520Alignment%26entry.906535625%3DJin%2520Wang%2520and%2520Jianxiang%2520Lu%2520and%2520Guangzheng%2520Xu%2520and%2520Comi%2520Chen%2520and%2520Haoyu%2520Yang%2520and%2520Linqing%2520Wang%2520and%2520Peng%2520Chen%2520and%2520Mingtao%2520Chen%2520and%2520Zhichao%2520Hu%2520and%2520Longhuang%2520Wu%2520and%2520Shuai%2520Shao%2520and%2520Qinglin%2520Lu%2520and%2520Ping%2520Luo%26entry.1292438233%3DRecent%2520studies%2520have%2520demonstrated%2520the%2520efficacy%2520of%2520integrating%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520into%2520flow%2520matching%2520models%252C%2520particularly%2520for%2520text-to-image%2520and%2520text-to-video%2520generation.%2520However%252C%2520we%2520find%2520that%2520directly%2520applying%2520these%2520techniques%2520to%2520image-to-video%2520%2528I2V%2529%2520models%2520often%2520fails%2520to%2520yield%2520consistent%2520reward%2520improvements.%2520To%2520address%2520this%2520limitation%252C%2520we%2520present%2520TAGRPO%252C%2520a%2520robust%2520post-training%2520framework%2520for%2520I2V%2520models%2520inspired%2520by%2520contrastive%2520learning.%2520Our%2520approach%2520is%2520grounded%2520in%2520the%2520observation%2520that%2520rollout%2520videos%2520generated%2520from%2520identical%2520initial%2520noise%2520provide%2520superior%2520guidance%2520for%2520optimization.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%2520GRPO%2520loss%2520applied%2520to%2520intermediate%2520latents%252C%2520encouraging%2520direct%2520alignment%2520with%2520high-reward%2520trajectories%2520while%2520maximizing%2520distance%2520from%2520low-reward%2520counterparts.%2520Furthermore%252C%2520we%2520introduce%2520a%2520memory%2520bank%2520for%2520rollout%2520videos%2520to%2520enhance%2520diversity%2520and%2520reduce%2520computational%2520overhead.%2520Despite%2520its%2520simplicity%252C%2520TAGRPO%2520achieves%2520significant%2520improvements%2520over%2520DanceGRPO%2520in%2520I2V%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAGRPO%3A%20Boosting%20GRPO%20on%20Image-to-Video%20Generation%20with%20Direct%20Trajectory%20Alignment&entry.906535625=Jin%20Wang%20and%20Jianxiang%20Lu%20and%20Guangzheng%20Xu%20and%20Comi%20Chen%20and%20Haoyu%20Yang%20and%20Linqing%20Wang%20and%20Peng%20Chen%20and%20Mingtao%20Chen%20and%20Zhichao%20Hu%20and%20Longhuang%20Wu%20and%20Shuai%20Shao%20and%20Qinglin%20Lu%20and%20Ping%20Luo&entry.1292438233=Recent%20studies%20have%20demonstrated%20the%20efficacy%20of%20integrating%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20into%20flow%20matching%20models%2C%20particularly%20for%20text-to-image%20and%20text-to-video%20generation.%20However%2C%20we%20find%20that%20directly%20applying%20these%20techniques%20to%20image-to-video%20%28I2V%29%20models%20often%20fails%20to%20yield%20consistent%20reward%20improvements.%20To%20address%20this%20limitation%2C%20we%20present%20TAGRPO%2C%20a%20robust%20post-training%20framework%20for%20I2V%20models%20inspired%20by%20contrastive%20learning.%20Our%20approach%20is%20grounded%20in%20the%20observation%20that%20rollout%20videos%20generated%20from%20identical%20initial%20noise%20provide%20superior%20guidance%20for%20optimization.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20novel%20GRPO%20loss%20applied%20to%20intermediate%20latents%2C%20encouraging%20direct%20alignment%20with%20high-reward%20trajectories%20while%20maximizing%20distance%20from%20low-reward%20counterparts.%20Furthermore%2C%20we%20introduce%20a%20memory%20bank%20for%20rollout%20videos%20to%20enhance%20diversity%20and%20reduce%20computational%20overhead.%20Despite%20its%20simplicity%2C%20TAGRPO%20achieves%20significant%20improvements%20over%20DanceGRPO%20in%20I2V%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2601.05729v1&entry.124074799=Read"},
{"title": "Assessing Superposition-Targeted Coverage Criteria for Quantum Neural Networks", "author": "Minqi Shao and Jianjun Zhao", "abstract": "Quantum Neural Networks (QNNs) have achieved initial success in various tasks by integrating quantum computing and neural networks. However, growing concerns about their reliability and robustness highlight the need for systematic testing. Unfortunately, current testing methods for QNNs remain underdeveloped, with limited practical utility and insufficient empirical evaluation. As an initial effort, we design a set of superposition-targeted coverage criteria to evaluate QNN state exploration embedded in test suites. To characterize the effectiveness, scalability, and robustness of the criteria, we conduct a comprehensive empirical study using benchmark datasets and QNN architectures. We first evaluate their sensitivity to input diversity under multiple data settings, and analyze their correlation with the number of injected faults. We then assess their scalability to increasing circuit scales. The robustness is further studied under practical quantum constraints including insufficient measurement and quantum noise. The results demonstrate the effectiveness of quantifying test adequacy and the potential applicability to larger-scale circuits and realistic quantum execution, while also revealing some limitations. Finally, we provide insights and recommendations for future QNN testing.", "link": "http://arxiv.org/abs/2411.02450v3", "date": "2026-01-09", "relevancy": 2.2106, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Superposition-Targeted%20Coverage%20Criteria%20for%20Quantum%20Neural%20Networks&body=Title%3A%20Assessing%20Superposition-Targeted%20Coverage%20Criteria%20for%20Quantum%20Neural%20Networks%0AAuthor%3A%20Minqi%20Shao%20and%20Jianjun%20Zhao%0AAbstract%3A%20Quantum%20Neural%20Networks%20%28QNNs%29%20have%20achieved%20initial%20success%20in%20various%20tasks%20by%20integrating%20quantum%20computing%20and%20neural%20networks.%20However%2C%20growing%20concerns%20about%20their%20reliability%20and%20robustness%20highlight%20the%20need%20for%20systematic%20testing.%20Unfortunately%2C%20current%20testing%20methods%20for%20QNNs%20remain%20underdeveloped%2C%20with%20limited%20practical%20utility%20and%20insufficient%20empirical%20evaluation.%20As%20an%20initial%20effort%2C%20we%20design%20a%20set%20of%20superposition-targeted%20coverage%20criteria%20to%20evaluate%20QNN%20state%20exploration%20embedded%20in%20test%20suites.%20To%20characterize%20the%20effectiveness%2C%20scalability%2C%20and%20robustness%20of%20the%20criteria%2C%20we%20conduct%20a%20comprehensive%20empirical%20study%20using%20benchmark%20datasets%20and%20QNN%20architectures.%20We%20first%20evaluate%20their%20sensitivity%20to%20input%20diversity%20under%20multiple%20data%20settings%2C%20and%20analyze%20their%20correlation%20with%20the%20number%20of%20injected%20faults.%20We%20then%20assess%20their%20scalability%20to%20increasing%20circuit%20scales.%20The%20robustness%20is%20further%20studied%20under%20practical%20quantum%20constraints%20including%20insufficient%20measurement%20and%20quantum%20noise.%20The%20results%20demonstrate%20the%20effectiveness%20of%20quantifying%20test%20adequacy%20and%20the%20potential%20applicability%20to%20larger-scale%20circuits%20and%20realistic%20quantum%20execution%2C%20while%20also%20revealing%20some%20limitations.%20Finally%2C%20we%20provide%20insights%20and%20recommendations%20for%20future%20QNN%20testing.%0ALink%3A%20http%3A//arxiv.org/abs/2411.02450v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Superposition-Targeted%2520Coverage%2520Criteria%2520for%2520Quantum%2520Neural%2520Networks%26entry.906535625%3DMinqi%2520Shao%2520and%2520Jianjun%2520Zhao%26entry.1292438233%3DQuantum%2520Neural%2520Networks%2520%2528QNNs%2529%2520have%2520achieved%2520initial%2520success%2520in%2520various%2520tasks%2520by%2520integrating%2520quantum%2520computing%2520and%2520neural%2520networks.%2520However%252C%2520growing%2520concerns%2520about%2520their%2520reliability%2520and%2520robustness%2520highlight%2520the%2520need%2520for%2520systematic%2520testing.%2520Unfortunately%252C%2520current%2520testing%2520methods%2520for%2520QNNs%2520remain%2520underdeveloped%252C%2520with%2520limited%2520practical%2520utility%2520and%2520insufficient%2520empirical%2520evaluation.%2520As%2520an%2520initial%2520effort%252C%2520we%2520design%2520a%2520set%2520of%2520superposition-targeted%2520coverage%2520criteria%2520to%2520evaluate%2520QNN%2520state%2520exploration%2520embedded%2520in%2520test%2520suites.%2520To%2520characterize%2520the%2520effectiveness%252C%2520scalability%252C%2520and%2520robustness%2520of%2520the%2520criteria%252C%2520we%2520conduct%2520a%2520comprehensive%2520empirical%2520study%2520using%2520benchmark%2520datasets%2520and%2520QNN%2520architectures.%2520We%2520first%2520evaluate%2520their%2520sensitivity%2520to%2520input%2520diversity%2520under%2520multiple%2520data%2520settings%252C%2520and%2520analyze%2520their%2520correlation%2520with%2520the%2520number%2520of%2520injected%2520faults.%2520We%2520then%2520assess%2520their%2520scalability%2520to%2520increasing%2520circuit%2520scales.%2520The%2520robustness%2520is%2520further%2520studied%2520under%2520practical%2520quantum%2520constraints%2520including%2520insufficient%2520measurement%2520and%2520quantum%2520noise.%2520The%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520quantifying%2520test%2520adequacy%2520and%2520the%2520potential%2520applicability%2520to%2520larger-scale%2520circuits%2520and%2520realistic%2520quantum%2520execution%252C%2520while%2520also%2520revealing%2520some%2520limitations.%2520Finally%252C%2520we%2520provide%2520insights%2520and%2520recommendations%2520for%2520future%2520QNN%2520testing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02450v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Superposition-Targeted%20Coverage%20Criteria%20for%20Quantum%20Neural%20Networks&entry.906535625=Minqi%20Shao%20and%20Jianjun%20Zhao&entry.1292438233=Quantum%20Neural%20Networks%20%28QNNs%29%20have%20achieved%20initial%20success%20in%20various%20tasks%20by%20integrating%20quantum%20computing%20and%20neural%20networks.%20However%2C%20growing%20concerns%20about%20their%20reliability%20and%20robustness%20highlight%20the%20need%20for%20systematic%20testing.%20Unfortunately%2C%20current%20testing%20methods%20for%20QNNs%20remain%20underdeveloped%2C%20with%20limited%20practical%20utility%20and%20insufficient%20empirical%20evaluation.%20As%20an%20initial%20effort%2C%20we%20design%20a%20set%20of%20superposition-targeted%20coverage%20criteria%20to%20evaluate%20QNN%20state%20exploration%20embedded%20in%20test%20suites.%20To%20characterize%20the%20effectiveness%2C%20scalability%2C%20and%20robustness%20of%20the%20criteria%2C%20we%20conduct%20a%20comprehensive%20empirical%20study%20using%20benchmark%20datasets%20and%20QNN%20architectures.%20We%20first%20evaluate%20their%20sensitivity%20to%20input%20diversity%20under%20multiple%20data%20settings%2C%20and%20analyze%20their%20correlation%20with%20the%20number%20of%20injected%20faults.%20We%20then%20assess%20their%20scalability%20to%20increasing%20circuit%20scales.%20The%20robustness%20is%20further%20studied%20under%20practical%20quantum%20constraints%20including%20insufficient%20measurement%20and%20quantum%20noise.%20The%20results%20demonstrate%20the%20effectiveness%20of%20quantifying%20test%20adequacy%20and%20the%20potential%20applicability%20to%20larger-scale%20circuits%20and%20realistic%20quantum%20execution%2C%20while%20also%20revealing%20some%20limitations.%20Finally%2C%20we%20provide%20insights%20and%20recommendations%20for%20future%20QNN%20testing.&entry.1838667208=http%3A//arxiv.org/abs/2411.02450v3&entry.124074799=Read"},
{"title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems", "author": "Kamel Kamel and Hridoy Sankar Dutta and Keshav Sood and Sunil Aryal", "abstract": "Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.", "link": "http://arxiv.org/abs/2509.07677v4", "date": "2026-01-09", "relevancy": 2.2097, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4584}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems&body=Title%3A%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems%0AAuthor%3A%20Kamel%20Kamel%20and%20Hridoy%20Sankar%20Dutta%20and%20Keshav%20Sood%20and%20Sunil%20Aryal%0AAbstract%3A%20Voice%20Authentication%20Systems%20%28VAS%29%20use%20unique%20vocal%20characteristics%20for%20verification.%20They%20are%20increasingly%20integrated%20into%20high-security%20sectors%20such%20as%20banking%20and%20healthcare.%20Despite%20their%20improvements%20using%20deep%20learning%2C%20they%20face%20severe%20vulnerabilities%20from%20sophisticated%20threats%20like%20deepfakes%20and%20adversarial%20attacks.%20The%20emergence%20of%20realistic%20voice%20cloning%20complicates%20detection%2C%20as%20systems%20struggle%20to%20distinguish%20authentic%20from%20synthetic%20audio.%20While%20anti-spoofing%20countermeasures%20%28CMs%29%20exist%20to%20mitigate%20these%20risks%2C%20many%20rely%20on%20static%20detection%20models%20that%20can%20be%20bypassed%20by%20novel%20adversarial%20methods%2C%20leaving%20a%20critical%20security%20gap.%20To%20demonstrate%20this%20vulnerability%2C%20we%20propose%20the%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%2C%20a%20novel%20method%20that%20strategically%20manipulates%20inaudible%20frequency%20regions%20of%20AI-generated%20audio.%20By%20altering%20the%20voice%20in%20imperceptible%20zones%20to%20the%20human%20ear%2C%20SMIA%20creates%20adversarial%20samples%20that%20sound%20authentic%20while%20deceiving%20CMs.%20We%20conducted%20a%20comprehensive%20evaluation%20of%20our%20attack%20against%20state-of-the-art%20%28SOTA%29%20models%20across%20multiple%20tasks%2C%20under%20simulated%20real-world%20conditions.%20SMIA%20achieved%20a%20strong%20attack%20success%20rate%20%28ASR%29%20of%20at%20least%2082%25%20against%20combined%20VAS/CM%20systems%2C%20at%20least%2097.5%25%20against%20standalone%20speaker%20verification%20systems%2C%20and%20100%25%20against%20countermeasures.%20These%20findings%20conclusively%20demonstrate%20that%20current%20security%20postures%20are%20insufficient%20against%20adaptive%20adversarial%20attacks.%20This%20work%20highlights%20the%20urgent%20need%20for%20a%20paradigm%20shift%20toward%20next-generation%20defenses%20that%20employ%20dynamic%2C%20context-aware%20frameworks%20capable%20of%20evolving%20with%20the%20threat%20landscape.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07677v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Masking%2520and%2520Interpolation%2520Attack%2520%2528SMIA%2529%253A%2520A%2520Black-box%2520Adversarial%2520Attack%2520against%2520Voice%2520Authentication%2520and%2520Anti-Spoofing%2520Systems%26entry.906535625%3DKamel%2520Kamel%2520and%2520Hridoy%2520Sankar%2520Dutta%2520and%2520Keshav%2520Sood%2520and%2520Sunil%2520Aryal%26entry.1292438233%3DVoice%2520Authentication%2520Systems%2520%2528VAS%2529%2520use%2520unique%2520vocal%2520characteristics%2520for%2520verification.%2520They%2520are%2520increasingly%2520integrated%2520into%2520high-security%2520sectors%2520such%2520as%2520banking%2520and%2520healthcare.%2520Despite%2520their%2520improvements%2520using%2520deep%2520learning%252C%2520they%2520face%2520severe%2520vulnerabilities%2520from%2520sophisticated%2520threats%2520like%2520deepfakes%2520and%2520adversarial%2520attacks.%2520The%2520emergence%2520of%2520realistic%2520voice%2520cloning%2520complicates%2520detection%252C%2520as%2520systems%2520struggle%2520to%2520distinguish%2520authentic%2520from%2520synthetic%2520audio.%2520While%2520anti-spoofing%2520countermeasures%2520%2528CMs%2529%2520exist%2520to%2520mitigate%2520these%2520risks%252C%2520many%2520rely%2520on%2520static%2520detection%2520models%2520that%2520can%2520be%2520bypassed%2520by%2520novel%2520adversarial%2520methods%252C%2520leaving%2520a%2520critical%2520security%2520gap.%2520To%2520demonstrate%2520this%2520vulnerability%252C%2520we%2520propose%2520the%2520Spectral%2520Masking%2520and%2520Interpolation%2520Attack%2520%2528SMIA%2529%252C%2520a%2520novel%2520method%2520that%2520strategically%2520manipulates%2520inaudible%2520frequency%2520regions%2520of%2520AI-generated%2520audio.%2520By%2520altering%2520the%2520voice%2520in%2520imperceptible%2520zones%2520to%2520the%2520human%2520ear%252C%2520SMIA%2520creates%2520adversarial%2520samples%2520that%2520sound%2520authentic%2520while%2520deceiving%2520CMs.%2520We%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520our%2520attack%2520against%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520across%2520multiple%2520tasks%252C%2520under%2520simulated%2520real-world%2520conditions.%2520SMIA%2520achieved%2520a%2520strong%2520attack%2520success%2520rate%2520%2528ASR%2529%2520of%2520at%2520least%252082%2525%2520against%2520combined%2520VAS/CM%2520systems%252C%2520at%2520least%252097.5%2525%2520against%2520standalone%2520speaker%2520verification%2520systems%252C%2520and%2520100%2525%2520against%2520countermeasures.%2520These%2520findings%2520conclusively%2520demonstrate%2520that%2520current%2520security%2520postures%2520are%2520insufficient%2520against%2520adaptive%2520adversarial%2520attacks.%2520This%2520work%2520highlights%2520the%2520urgent%2520need%2520for%2520a%2520paradigm%2520shift%2520toward%2520next-generation%2520defenses%2520that%2520employ%2520dynamic%252C%2520context-aware%2520frameworks%2520capable%2520of%2520evolving%2520with%2520the%2520threat%2520landscape.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07677v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems&entry.906535625=Kamel%20Kamel%20and%20Hridoy%20Sankar%20Dutta%20and%20Keshav%20Sood%20and%20Sunil%20Aryal&entry.1292438233=Voice%20Authentication%20Systems%20%28VAS%29%20use%20unique%20vocal%20characteristics%20for%20verification.%20They%20are%20increasingly%20integrated%20into%20high-security%20sectors%20such%20as%20banking%20and%20healthcare.%20Despite%20their%20improvements%20using%20deep%20learning%2C%20they%20face%20severe%20vulnerabilities%20from%20sophisticated%20threats%20like%20deepfakes%20and%20adversarial%20attacks.%20The%20emergence%20of%20realistic%20voice%20cloning%20complicates%20detection%2C%20as%20systems%20struggle%20to%20distinguish%20authentic%20from%20synthetic%20audio.%20While%20anti-spoofing%20countermeasures%20%28CMs%29%20exist%20to%20mitigate%20these%20risks%2C%20many%20rely%20on%20static%20detection%20models%20that%20can%20be%20bypassed%20by%20novel%20adversarial%20methods%2C%20leaving%20a%20critical%20security%20gap.%20To%20demonstrate%20this%20vulnerability%2C%20we%20propose%20the%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%2C%20a%20novel%20method%20that%20strategically%20manipulates%20inaudible%20frequency%20regions%20of%20AI-generated%20audio.%20By%20altering%20the%20voice%20in%20imperceptible%20zones%20to%20the%20human%20ear%2C%20SMIA%20creates%20adversarial%20samples%20that%20sound%20authentic%20while%20deceiving%20CMs.%20We%20conducted%20a%20comprehensive%20evaluation%20of%20our%20attack%20against%20state-of-the-art%20%28SOTA%29%20models%20across%20multiple%20tasks%2C%20under%20simulated%20real-world%20conditions.%20SMIA%20achieved%20a%20strong%20attack%20success%20rate%20%28ASR%29%20of%20at%20least%2082%25%20against%20combined%20VAS/CM%20systems%2C%20at%20least%2097.5%25%20against%20standalone%20speaker%20verification%20systems%2C%20and%20100%25%20against%20countermeasures.%20These%20findings%20conclusively%20demonstrate%20that%20current%20security%20postures%20are%20insufficient%20against%20adaptive%20adversarial%20attacks.%20This%20work%20highlights%20the%20urgent%20need%20for%20a%20paradigm%20shift%20toward%20next-generation%20defenses%20that%20employ%20dynamic%2C%20context-aware%20frameworks%20capable%20of%20evolving%20with%20the%20threat%20landscape.&entry.1838667208=http%3A//arxiv.org/abs/2509.07677v4&entry.124074799=Read"},
{"title": "Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features", "author": "Shinwoo Park and Hyundong Jin and Jeong-won Cha and Yo-Sub Han", "abstract": "Recent progress in large language models (LLMs) for code generation has raised serious concerns about intellectual property protection. Malicious users can exploit LLMs to produce paraphrased versions of proprietary code that closely resemble the original. While the potential for LLM-assisted code paraphrasing continues to grow, research on detecting it remains limited, underscoring an urgent need for detection system. We respond to this need by proposing two tasks. The first task is to detect whether code generated by an LLM is a paraphrased version of original human-written code. The second task is to identify which LLM is used to paraphrase the original code. For these tasks, we construct a dataset LPcode consisting of pairs of human-written code and LLM-paraphrased code using various LLMs.\n  We statistically confirm significant differences in the coding styles of human-written and LLM-paraphrased code, particularly in terms of naming consistency, code structure, and readability. Based on these findings, we develop LPcodedec, a detection method that identifies paraphrase relationships between human-written and LLM-generated code, and discover which LLM is used for the paraphrasing. LPcodedec outperforms the best baselines in two tasks, improving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and 213x, respectively. Our code and data are available at https://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.", "link": "http://arxiv.org/abs/2502.17749v3", "date": "2026-01-09", "relevancy": 2.2093, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4592}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4351}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20LLM-Paraphrased%20Code%20and%20Identification%20of%20the%20Responsible%20LLM%20Using%20Coding%20Style%20Features&body=Title%3A%20Detection%20of%20LLM-Paraphrased%20Code%20and%20Identification%20of%20the%20Responsible%20LLM%20Using%20Coding%20Style%20Features%0AAuthor%3A%20Shinwoo%20Park%20and%20Hyundong%20Jin%20and%20Jeong-won%20Cha%20and%20Yo-Sub%20Han%0AAbstract%3A%20Recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20for%20code%20generation%20has%20raised%20serious%20concerns%20about%20intellectual%20property%20protection.%20Malicious%20users%20can%20exploit%20LLMs%20to%20produce%20paraphrased%20versions%20of%20proprietary%20code%20that%20closely%20resemble%20the%20original.%20While%20the%20potential%20for%20LLM-assisted%20code%20paraphrasing%20continues%20to%20grow%2C%20research%20on%20detecting%20it%20remains%20limited%2C%20underscoring%20an%20urgent%20need%20for%20detection%20system.%20We%20respond%20to%20this%20need%20by%20proposing%20two%20tasks.%20The%20first%20task%20is%20to%20detect%20whether%20code%20generated%20by%20an%20LLM%20is%20a%20paraphrased%20version%20of%20original%20human-written%20code.%20The%20second%20task%20is%20to%20identify%20which%20LLM%20is%20used%20to%20paraphrase%20the%20original%20code.%20For%20these%20tasks%2C%20we%20construct%20a%20dataset%20LPcode%20consisting%20of%20pairs%20of%20human-written%20code%20and%20LLM-paraphrased%20code%20using%20various%20LLMs.%0A%20%20We%20statistically%20confirm%20significant%20differences%20in%20the%20coding%20styles%20of%20human-written%20and%20LLM-paraphrased%20code%2C%20particularly%20in%20terms%20of%20naming%20consistency%2C%20code%20structure%2C%20and%20readability.%20Based%20on%20these%20findings%2C%20we%20develop%20LPcodedec%2C%20a%20detection%20method%20that%20identifies%20paraphrase%20relationships%20between%20human-written%20and%20LLM-generated%20code%2C%20and%20discover%20which%20LLM%20is%20used%20for%20the%20paraphrasing.%20LPcodedec%20outperforms%20the%20best%20baselines%20in%20two%20tasks%2C%20improving%20F1%20scores%20by%202.64%25%20and%2015.17%25%20while%20achieving%20speedups%20of%201%2C343x%20and%20213x%2C%20respectively.%20Our%20code%20and%20data%20are%20available%20at%20https%3A//github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.%0ALink%3A%20http%3A//arxiv.org/abs/2502.17749v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520of%2520LLM-Paraphrased%2520Code%2520and%2520Identification%2520of%2520the%2520Responsible%2520LLM%2520Using%2520Coding%2520Style%2520Features%26entry.906535625%3DShinwoo%2520Park%2520and%2520Hyundong%2520Jin%2520and%2520Jeong-won%2520Cha%2520and%2520Yo-Sub%2520Han%26entry.1292438233%3DRecent%2520progress%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520code%2520generation%2520has%2520raised%2520serious%2520concerns%2520about%2520intellectual%2520property%2520protection.%2520Malicious%2520users%2520can%2520exploit%2520LLMs%2520to%2520produce%2520paraphrased%2520versions%2520of%2520proprietary%2520code%2520that%2520closely%2520resemble%2520the%2520original.%2520While%2520the%2520potential%2520for%2520LLM-assisted%2520code%2520paraphrasing%2520continues%2520to%2520grow%252C%2520research%2520on%2520detecting%2520it%2520remains%2520limited%252C%2520underscoring%2520an%2520urgent%2520need%2520for%2520detection%2520system.%2520We%2520respond%2520to%2520this%2520need%2520by%2520proposing%2520two%2520tasks.%2520The%2520first%2520task%2520is%2520to%2520detect%2520whether%2520code%2520generated%2520by%2520an%2520LLM%2520is%2520a%2520paraphrased%2520version%2520of%2520original%2520human-written%2520code.%2520The%2520second%2520task%2520is%2520to%2520identify%2520which%2520LLM%2520is%2520used%2520to%2520paraphrase%2520the%2520original%2520code.%2520For%2520these%2520tasks%252C%2520we%2520construct%2520a%2520dataset%2520LPcode%2520consisting%2520of%2520pairs%2520of%2520human-written%2520code%2520and%2520LLM-paraphrased%2520code%2520using%2520various%2520LLMs.%250A%2520%2520We%2520statistically%2520confirm%2520significant%2520differences%2520in%2520the%2520coding%2520styles%2520of%2520human-written%2520and%2520LLM-paraphrased%2520code%252C%2520particularly%2520in%2520terms%2520of%2520naming%2520consistency%252C%2520code%2520structure%252C%2520and%2520readability.%2520Based%2520on%2520these%2520findings%252C%2520we%2520develop%2520LPcodedec%252C%2520a%2520detection%2520method%2520that%2520identifies%2520paraphrase%2520relationships%2520between%2520human-written%2520and%2520LLM-generated%2520code%252C%2520and%2520discover%2520which%2520LLM%2520is%2520used%2520for%2520the%2520paraphrasing.%2520LPcodedec%2520outperforms%2520the%2520best%2520baselines%2520in%2520two%2520tasks%252C%2520improving%2520F1%2520scores%2520by%25202.64%2525%2520and%252015.17%2525%2520while%2520achieving%2520speedups%2520of%25201%252C343x%2520and%2520213x%252C%2520respectively.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17749v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20LLM-Paraphrased%20Code%20and%20Identification%20of%20the%20Responsible%20LLM%20Using%20Coding%20Style%20Features&entry.906535625=Shinwoo%20Park%20and%20Hyundong%20Jin%20and%20Jeong-won%20Cha%20and%20Yo-Sub%20Han&entry.1292438233=Recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20for%20code%20generation%20has%20raised%20serious%20concerns%20about%20intellectual%20property%20protection.%20Malicious%20users%20can%20exploit%20LLMs%20to%20produce%20paraphrased%20versions%20of%20proprietary%20code%20that%20closely%20resemble%20the%20original.%20While%20the%20potential%20for%20LLM-assisted%20code%20paraphrasing%20continues%20to%20grow%2C%20research%20on%20detecting%20it%20remains%20limited%2C%20underscoring%20an%20urgent%20need%20for%20detection%20system.%20We%20respond%20to%20this%20need%20by%20proposing%20two%20tasks.%20The%20first%20task%20is%20to%20detect%20whether%20code%20generated%20by%20an%20LLM%20is%20a%20paraphrased%20version%20of%20original%20human-written%20code.%20The%20second%20task%20is%20to%20identify%20which%20LLM%20is%20used%20to%20paraphrase%20the%20original%20code.%20For%20these%20tasks%2C%20we%20construct%20a%20dataset%20LPcode%20consisting%20of%20pairs%20of%20human-written%20code%20and%20LLM-paraphrased%20code%20using%20various%20LLMs.%0A%20%20We%20statistically%20confirm%20significant%20differences%20in%20the%20coding%20styles%20of%20human-written%20and%20LLM-paraphrased%20code%2C%20particularly%20in%20terms%20of%20naming%20consistency%2C%20code%20structure%2C%20and%20readability.%20Based%20on%20these%20findings%2C%20we%20develop%20LPcodedec%2C%20a%20detection%20method%20that%20identifies%20paraphrase%20relationships%20between%20human-written%20and%20LLM-generated%20code%2C%20and%20discover%20which%20LLM%20is%20used%20for%20the%20paraphrasing.%20LPcodedec%20outperforms%20the%20best%20baselines%20in%20two%20tasks%2C%20improving%20F1%20scores%20by%202.64%25%20and%2015.17%25%20while%20achieving%20speedups%20of%201%2C343x%20and%20213x%2C%20respectively.%20Our%20code%20and%20data%20are%20available%20at%20https%3A//github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.&entry.1838667208=http%3A//arxiv.org/abs/2502.17749v3&entry.124074799=Read"},
{"title": "WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation", "author": "Chanchan Wang and Yuanfang Wang and Qing Xu and Guanxin Chen", "abstract": "Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM's direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM's upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.", "link": "http://arxiv.org/abs/2601.05942v1", "date": "2026-01-09", "relevancy": 2.209, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5588}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaveRNet%3A%20Wavelet-Guided%20Frequency%20Learning%20for%20Multi-Source%20Domain-Generalized%20Retinal%20Vessel%20Segmentation&body=Title%3A%20WaveRNet%3A%20Wavelet-Guided%20Frequency%20Learning%20for%20Multi-Source%20Domain-Generalized%20Retinal%20Vessel%20Segmentation%0AAuthor%3A%20Chanchan%20Wang%20and%20Yuanfang%20Wang%20and%20Qing%20Xu%20and%20Guanxin%20Chen%0AAbstract%3A%20Domain-generalized%20retinal%20vessel%20segmentation%20is%20critical%20for%20automated%20ophthalmic%20diagnosis%2C%20yet%20faces%20significant%20challenges%20from%20domain%20shift%20induced%20by%20non-uniform%20illumination%20and%20varying%20contrast%2C%20compounded%20by%20the%20difficulty%20of%20preserving%20fine%20vessel%20structures.%20While%20the%20Segment%20Anything%20Model%20%28SAM%29%20exhibits%20remarkable%20zero-shot%20capabilities%2C%20existing%20SAM-based%20methods%20rely%20on%20simple%20adapter%20fine-tuning%20while%20overlooking%20frequency-domain%20information%20that%20encodes%20domain-invariant%20features%2C%20resulting%20in%20degraded%20generalization%20under%20illumination%20and%20contrast%20variations.%20Furthermore%2C%20SAM%27s%20direct%20upsampling%20inevitably%20loses%20fine%20vessel%20details.%20To%20address%20these%20limitations%2C%20we%20propose%20WaveRNet%2C%20a%20wavelet-guided%20frequency%20learning%20framework%20for%20robust%20multi-source%20domain-generalized%20retinal%20vessel%20segmentation.%20Specifically%2C%20we%20devise%20a%20Spectral-guided%20Domain%20Modulator%20%28SDM%29%20that%20integrates%20wavelet%20decomposition%20with%20learnable%20domain%20tokens%2C%20enabling%20the%20separation%20of%20illumination-robust%20low-frequency%20structures%20from%20high-frequency%20vessel%20boundaries%20while%20facilitating%20domain-specific%20feature%20generation.%20Furthermore%2C%20we%20introduce%20a%20Frequency-Adaptive%20Domain%20Fusion%20%28FADF%29%20module%20that%20performs%20intelligent%20test-time%20domain%20selection%20through%20wavelet-based%20frequency%20similarity%20and%20soft-weighted%20fusion.%20Finally%2C%20we%20present%20a%20Hierarchical%20Mask-Prompt%20Refiner%20%28HMPR%29%20that%20overcomes%20SAM%27s%20upsampling%20limitation%20through%20coarse-to-fine%20refinement%20with%20long-range%20dependency%20modeling.%20Extensive%20experiments%20under%20the%20Leave-One-Domain-Out%20protocol%20on%20four%20public%20retinal%20datasets%20demonstrate%20that%20WaveRNet%20achieves%20state-of-the-art%20generalization%20performance.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/Chanchan-Wang/WaveRNet.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaveRNet%253A%2520Wavelet-Guided%2520Frequency%2520Learning%2520for%2520Multi-Source%2520Domain-Generalized%2520Retinal%2520Vessel%2520Segmentation%26entry.906535625%3DChanchan%2520Wang%2520and%2520Yuanfang%2520Wang%2520and%2520Qing%2520Xu%2520and%2520Guanxin%2520Chen%26entry.1292438233%3DDomain-generalized%2520retinal%2520vessel%2520segmentation%2520is%2520critical%2520for%2520automated%2520ophthalmic%2520diagnosis%252C%2520yet%2520faces%2520significant%2520challenges%2520from%2520domain%2520shift%2520induced%2520by%2520non-uniform%2520illumination%2520and%2520varying%2520contrast%252C%2520compounded%2520by%2520the%2520difficulty%2520of%2520preserving%2520fine%2520vessel%2520structures.%2520While%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520exhibits%2520remarkable%2520zero-shot%2520capabilities%252C%2520existing%2520SAM-based%2520methods%2520rely%2520on%2520simple%2520adapter%2520fine-tuning%2520while%2520overlooking%2520frequency-domain%2520information%2520that%2520encodes%2520domain-invariant%2520features%252C%2520resulting%2520in%2520degraded%2520generalization%2520under%2520illumination%2520and%2520contrast%2520variations.%2520Furthermore%252C%2520SAM%2527s%2520direct%2520upsampling%2520inevitably%2520loses%2520fine%2520vessel%2520details.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520WaveRNet%252C%2520a%2520wavelet-guided%2520frequency%2520learning%2520framework%2520for%2520robust%2520multi-source%2520domain-generalized%2520retinal%2520vessel%2520segmentation.%2520Specifically%252C%2520we%2520devise%2520a%2520Spectral-guided%2520Domain%2520Modulator%2520%2528SDM%2529%2520that%2520integrates%2520wavelet%2520decomposition%2520with%2520learnable%2520domain%2520tokens%252C%2520enabling%2520the%2520separation%2520of%2520illumination-robust%2520low-frequency%2520structures%2520from%2520high-frequency%2520vessel%2520boundaries%2520while%2520facilitating%2520domain-specific%2520feature%2520generation.%2520Furthermore%252C%2520we%2520introduce%2520a%2520Frequency-Adaptive%2520Domain%2520Fusion%2520%2528FADF%2529%2520module%2520that%2520performs%2520intelligent%2520test-time%2520domain%2520selection%2520through%2520wavelet-based%2520frequency%2520similarity%2520and%2520soft-weighted%2520fusion.%2520Finally%252C%2520we%2520present%2520a%2520Hierarchical%2520Mask-Prompt%2520Refiner%2520%2528HMPR%2529%2520that%2520overcomes%2520SAM%2527s%2520upsampling%2520limitation%2520through%2520coarse-to-fine%2520refinement%2520with%2520long-range%2520dependency%2520modeling.%2520Extensive%2520experiments%2520under%2520the%2520Leave-One-Domain-Out%2520protocol%2520on%2520four%2520public%2520retinal%2520datasets%2520demonstrate%2520that%2520WaveRNet%2520achieves%2520state-of-the-art%2520generalization%2520performance.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/Chanchan-Wang/WaveRNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveRNet%3A%20Wavelet-Guided%20Frequency%20Learning%20for%20Multi-Source%20Domain-Generalized%20Retinal%20Vessel%20Segmentation&entry.906535625=Chanchan%20Wang%20and%20Yuanfang%20Wang%20and%20Qing%20Xu%20and%20Guanxin%20Chen&entry.1292438233=Domain-generalized%20retinal%20vessel%20segmentation%20is%20critical%20for%20automated%20ophthalmic%20diagnosis%2C%20yet%20faces%20significant%20challenges%20from%20domain%20shift%20induced%20by%20non-uniform%20illumination%20and%20varying%20contrast%2C%20compounded%20by%20the%20difficulty%20of%20preserving%20fine%20vessel%20structures.%20While%20the%20Segment%20Anything%20Model%20%28SAM%29%20exhibits%20remarkable%20zero-shot%20capabilities%2C%20existing%20SAM-based%20methods%20rely%20on%20simple%20adapter%20fine-tuning%20while%20overlooking%20frequency-domain%20information%20that%20encodes%20domain-invariant%20features%2C%20resulting%20in%20degraded%20generalization%20under%20illumination%20and%20contrast%20variations.%20Furthermore%2C%20SAM%27s%20direct%20upsampling%20inevitably%20loses%20fine%20vessel%20details.%20To%20address%20these%20limitations%2C%20we%20propose%20WaveRNet%2C%20a%20wavelet-guided%20frequency%20learning%20framework%20for%20robust%20multi-source%20domain-generalized%20retinal%20vessel%20segmentation.%20Specifically%2C%20we%20devise%20a%20Spectral-guided%20Domain%20Modulator%20%28SDM%29%20that%20integrates%20wavelet%20decomposition%20with%20learnable%20domain%20tokens%2C%20enabling%20the%20separation%20of%20illumination-robust%20low-frequency%20structures%20from%20high-frequency%20vessel%20boundaries%20while%20facilitating%20domain-specific%20feature%20generation.%20Furthermore%2C%20we%20introduce%20a%20Frequency-Adaptive%20Domain%20Fusion%20%28FADF%29%20module%20that%20performs%20intelligent%20test-time%20domain%20selection%20through%20wavelet-based%20frequency%20similarity%20and%20soft-weighted%20fusion.%20Finally%2C%20we%20present%20a%20Hierarchical%20Mask-Prompt%20Refiner%20%28HMPR%29%20that%20overcomes%20SAM%27s%20upsampling%20limitation%20through%20coarse-to-fine%20refinement%20with%20long-range%20dependency%20modeling.%20Extensive%20experiments%20under%20the%20Leave-One-Domain-Out%20protocol%20on%20four%20public%20retinal%20datasets%20demonstrate%20that%20WaveRNet%20achieves%20state-of-the-art%20generalization%20performance.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/Chanchan-Wang/WaveRNet.&entry.1838667208=http%3A//arxiv.org/abs/2601.05942v1&entry.124074799=Read"},
{"title": "Multi-task Modeling for Engineering Applications with Sparse Data", "author": "Yigitcan Comlek and R. Murali Krishnan and Sandipp Krishnan Ravi and Amin Moghaddas and Rafael Giorjao and Michael Eff and Anirban Samaddar and Nesar S. Ramachandra and Sandeep Madireddy and Liping Wang", "abstract": "Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization.", "link": "http://arxiv.org/abs/2601.05910v1", "date": "2026-01-09", "relevancy": 2.1758, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.55}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20Modeling%20for%20Engineering%20Applications%20with%20Sparse%20Data&body=Title%3A%20Multi-task%20Modeling%20for%20Engineering%20Applications%20with%20Sparse%20Data%0AAuthor%3A%20Yigitcan%20Comlek%20and%20R.%20Murali%20Krishnan%20and%20Sandipp%20Krishnan%20Ravi%20and%20Amin%20Moghaddas%20and%20Rafael%20Giorjao%20and%20Michael%20Eff%20and%20Anirban%20Samaddar%20and%20Nesar%20S.%20Ramachandra%20and%20Sandeep%20Madireddy%20and%20Liping%20Wang%0AAbstract%3A%20Modern%20engineering%20and%20scientific%20workflows%20often%20require%20simultaneous%20predictions%20across%20related%20tasks%20and%20fidelity%20levels%2C%20where%20high-fidelity%20data%20is%20scarce%20and%20expensive%2C%20while%20low-fidelity%20data%20is%20more%20abundant.%20This%20paper%20introduces%20an%20Multi-Task%20Gaussian%20Processes%20%28MTGP%29%20framework%20tailored%20for%20engineering%20systems%20characterized%20by%20multi-source%2C%20multi-fidelity%20data%2C%20addressing%20challenges%20of%20data%20sparsity%20and%20varying%20task%20correlations.%20The%20proposed%20framework%20leverages%20inter-task%20relationships%20across%20outputs%20and%20fidelity%20levels%20to%20improve%20predictive%20performance%20and%20reduce%20computational%20costs.%20The%20framework%20is%20validated%20across%20three%20representative%20scenarios%3A%20Forrester%20function%20benchmark%2C%203D%20ellipsoidal%20void%20modeling%2C%20and%20friction-stir%20welding.%20By%20quantifying%20and%20leveraging%20inter-task%20relationships%2C%20the%20proposed%20MTGP%20framework%20offers%20a%20robust%20and%20scalable%20solution%20for%20predictive%20modeling%20in%20domains%20with%20significant%20computational%20and%20experimental%20costs%2C%20supporting%20informed%20decision-making%20and%20efficient%20resource%20utilization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520Modeling%2520for%2520Engineering%2520Applications%2520with%2520Sparse%2520Data%26entry.906535625%3DYigitcan%2520Comlek%2520and%2520R.%2520Murali%2520Krishnan%2520and%2520Sandipp%2520Krishnan%2520Ravi%2520and%2520Amin%2520Moghaddas%2520and%2520Rafael%2520Giorjao%2520and%2520Michael%2520Eff%2520and%2520Anirban%2520Samaddar%2520and%2520Nesar%2520S.%2520Ramachandra%2520and%2520Sandeep%2520Madireddy%2520and%2520Liping%2520Wang%26entry.1292438233%3DModern%2520engineering%2520and%2520scientific%2520workflows%2520often%2520require%2520simultaneous%2520predictions%2520across%2520related%2520tasks%2520and%2520fidelity%2520levels%252C%2520where%2520high-fidelity%2520data%2520is%2520scarce%2520and%2520expensive%252C%2520while%2520low-fidelity%2520data%2520is%2520more%2520abundant.%2520This%2520paper%2520introduces%2520an%2520Multi-Task%2520Gaussian%2520Processes%2520%2528MTGP%2529%2520framework%2520tailored%2520for%2520engineering%2520systems%2520characterized%2520by%2520multi-source%252C%2520multi-fidelity%2520data%252C%2520addressing%2520challenges%2520of%2520data%2520sparsity%2520and%2520varying%2520task%2520correlations.%2520The%2520proposed%2520framework%2520leverages%2520inter-task%2520relationships%2520across%2520outputs%2520and%2520fidelity%2520levels%2520to%2520improve%2520predictive%2520performance%2520and%2520reduce%2520computational%2520costs.%2520The%2520framework%2520is%2520validated%2520across%2520three%2520representative%2520scenarios%253A%2520Forrester%2520function%2520benchmark%252C%25203D%2520ellipsoidal%2520void%2520modeling%252C%2520and%2520friction-stir%2520welding.%2520By%2520quantifying%2520and%2520leveraging%2520inter-task%2520relationships%252C%2520the%2520proposed%2520MTGP%2520framework%2520offers%2520a%2520robust%2520and%2520scalable%2520solution%2520for%2520predictive%2520modeling%2520in%2520domains%2520with%2520significant%2520computational%2520and%2520experimental%2520costs%252C%2520supporting%2520informed%2520decision-making%2520and%2520efficient%2520resource%2520utilization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20Modeling%20for%20Engineering%20Applications%20with%20Sparse%20Data&entry.906535625=Yigitcan%20Comlek%20and%20R.%20Murali%20Krishnan%20and%20Sandipp%20Krishnan%20Ravi%20and%20Amin%20Moghaddas%20and%20Rafael%20Giorjao%20and%20Michael%20Eff%20and%20Anirban%20Samaddar%20and%20Nesar%20S.%20Ramachandra%20and%20Sandeep%20Madireddy%20and%20Liping%20Wang&entry.1292438233=Modern%20engineering%20and%20scientific%20workflows%20often%20require%20simultaneous%20predictions%20across%20related%20tasks%20and%20fidelity%20levels%2C%20where%20high-fidelity%20data%20is%20scarce%20and%20expensive%2C%20while%20low-fidelity%20data%20is%20more%20abundant.%20This%20paper%20introduces%20an%20Multi-Task%20Gaussian%20Processes%20%28MTGP%29%20framework%20tailored%20for%20engineering%20systems%20characterized%20by%20multi-source%2C%20multi-fidelity%20data%2C%20addressing%20challenges%20of%20data%20sparsity%20and%20varying%20task%20correlations.%20The%20proposed%20framework%20leverages%20inter-task%20relationships%20across%20outputs%20and%20fidelity%20levels%20to%20improve%20predictive%20performance%20and%20reduce%20computational%20costs.%20The%20framework%20is%20validated%20across%20three%20representative%20scenarios%3A%20Forrester%20function%20benchmark%2C%203D%20ellipsoidal%20void%20modeling%2C%20and%20friction-stir%20welding.%20By%20quantifying%20and%20leveraging%20inter-task%20relationships%2C%20the%20proposed%20MTGP%20framework%20offers%20a%20robust%20and%20scalable%20solution%20for%20predictive%20modeling%20in%20domains%20with%20significant%20computational%20and%20experimental%20costs%2C%20supporting%20informed%20decision-making%20and%20efficient%20resource%20utilization.&entry.1838667208=http%3A//arxiv.org/abs/2601.05910v1&entry.124074799=Read"},
{"title": "Next-Generation Reservoir Computing for Dynamical Inference", "author": "Rok Cestnik and Erik A. Martens", "abstract": "We present a simple and scalable implementation of next-generation reservoir computing (NGRC) for modeling dynamical systems from time-series data. The method uses a pseudorandom nonlinear projection of time-delay embedded inputs, allowing the feature-space dimension to be chosen independently of the observation size and offering a flexible alternative to polynomial-based NGRC projections. We demonstrate the approach on benchmark tasks, including attractor reconstruction and bifurcation diagram estimation, using partial and noisy measurements. We further show that small amounts of measurement noise during training act as an effective regularizer, improving long-term autonomous stability compared to standard regression alone. Across all tests, the models remain stable over long rollouts and generalize beyond the training data. The framework offers explicit control of system state during prediction, and these properties make NGRC a natural candidate for applications such as surrogate modeling and digital-twin applications.", "link": "http://arxiv.org/abs/2509.11338v3", "date": "2026-01-09", "relevancy": 2.1548, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5453}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5361}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Generation%20Reservoir%20Computing%20for%20Dynamical%20Inference&body=Title%3A%20Next-Generation%20Reservoir%20Computing%20for%20Dynamical%20Inference%0AAuthor%3A%20Rok%20Cestnik%20and%20Erik%20A.%20Martens%0AAbstract%3A%20We%20present%20a%20simple%20and%20scalable%20implementation%20of%20next-generation%20reservoir%20computing%20%28NGRC%29%20for%20modeling%20dynamical%20systems%20from%20time-series%20data.%20The%20method%20uses%20a%20pseudorandom%20nonlinear%20projection%20of%20time-delay%20embedded%20inputs%2C%20allowing%20the%20feature-space%20dimension%20to%20be%20chosen%20independently%20of%20the%20observation%20size%20and%20offering%20a%20flexible%20alternative%20to%20polynomial-based%20NGRC%20projections.%20We%20demonstrate%20the%20approach%20on%20benchmark%20tasks%2C%20including%20attractor%20reconstruction%20and%20bifurcation%20diagram%20estimation%2C%20using%20partial%20and%20noisy%20measurements.%20We%20further%20show%20that%20small%20amounts%20of%20measurement%20noise%20during%20training%20act%20as%20an%20effective%20regularizer%2C%20improving%20long-term%20autonomous%20stability%20compared%20to%20standard%20regression%20alone.%20Across%20all%20tests%2C%20the%20models%20remain%20stable%20over%20long%20rollouts%20and%20generalize%20beyond%20the%20training%20data.%20The%20framework%20offers%20explicit%20control%20of%20system%20state%20during%20prediction%2C%20and%20these%20properties%20make%20NGRC%20a%20natural%20candidate%20for%20applications%20such%20as%20surrogate%20modeling%20and%20digital-twin%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11338v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Generation%2520Reservoir%2520Computing%2520for%2520Dynamical%2520Inference%26entry.906535625%3DRok%2520Cestnik%2520and%2520Erik%2520A.%2520Martens%26entry.1292438233%3DWe%2520present%2520a%2520simple%2520and%2520scalable%2520implementation%2520of%2520next-generation%2520reservoir%2520computing%2520%2528NGRC%2529%2520for%2520modeling%2520dynamical%2520systems%2520from%2520time-series%2520data.%2520The%2520method%2520uses%2520a%2520pseudorandom%2520nonlinear%2520projection%2520of%2520time-delay%2520embedded%2520inputs%252C%2520allowing%2520the%2520feature-space%2520dimension%2520to%2520be%2520chosen%2520independently%2520of%2520the%2520observation%2520size%2520and%2520offering%2520a%2520flexible%2520alternative%2520to%2520polynomial-based%2520NGRC%2520projections.%2520We%2520demonstrate%2520the%2520approach%2520on%2520benchmark%2520tasks%252C%2520including%2520attractor%2520reconstruction%2520and%2520bifurcation%2520diagram%2520estimation%252C%2520using%2520partial%2520and%2520noisy%2520measurements.%2520We%2520further%2520show%2520that%2520small%2520amounts%2520of%2520measurement%2520noise%2520during%2520training%2520act%2520as%2520an%2520effective%2520regularizer%252C%2520improving%2520long-term%2520autonomous%2520stability%2520compared%2520to%2520standard%2520regression%2520alone.%2520Across%2520all%2520tests%252C%2520the%2520models%2520remain%2520stable%2520over%2520long%2520rollouts%2520and%2520generalize%2520beyond%2520the%2520training%2520data.%2520The%2520framework%2520offers%2520explicit%2520control%2520of%2520system%2520state%2520during%2520prediction%252C%2520and%2520these%2520properties%2520make%2520NGRC%2520a%2520natural%2520candidate%2520for%2520applications%2520such%2520as%2520surrogate%2520modeling%2520and%2520digital-twin%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11338v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Generation%20Reservoir%20Computing%20for%20Dynamical%20Inference&entry.906535625=Rok%20Cestnik%20and%20Erik%20A.%20Martens&entry.1292438233=We%20present%20a%20simple%20and%20scalable%20implementation%20of%20next-generation%20reservoir%20computing%20%28NGRC%29%20for%20modeling%20dynamical%20systems%20from%20time-series%20data.%20The%20method%20uses%20a%20pseudorandom%20nonlinear%20projection%20of%20time-delay%20embedded%20inputs%2C%20allowing%20the%20feature-space%20dimension%20to%20be%20chosen%20independently%20of%20the%20observation%20size%20and%20offering%20a%20flexible%20alternative%20to%20polynomial-based%20NGRC%20projections.%20We%20demonstrate%20the%20approach%20on%20benchmark%20tasks%2C%20including%20attractor%20reconstruction%20and%20bifurcation%20diagram%20estimation%2C%20using%20partial%20and%20noisy%20measurements.%20We%20further%20show%20that%20small%20amounts%20of%20measurement%20noise%20during%20training%20act%20as%20an%20effective%20regularizer%2C%20improving%20long-term%20autonomous%20stability%20compared%20to%20standard%20regression%20alone.%20Across%20all%20tests%2C%20the%20models%20remain%20stable%20over%20long%20rollouts%20and%20generalize%20beyond%20the%20training%20data.%20The%20framework%20offers%20explicit%20control%20of%20system%20state%20during%20prediction%2C%20and%20these%20properties%20make%20NGRC%20a%20natural%20candidate%20for%20applications%20such%20as%20surrogate%20modeling%20and%20digital-twin%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2509.11338v3&entry.124074799=Read"},
{"title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers", "author": "Guray Ozgur and Eduarda Caldeira and Tahar Chettaoui and Jan Niklas Kolf and Marco Huber and Naser Damer and Fadi Boutros", "abstract": "Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.", "link": "http://arxiv.org/abs/2601.05741v1", "date": "2026-01-09", "relevancy": 2.1502, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5653}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5322}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTNT-FIQA%3A%20Training-Free%20Face%20Image%20Quality%20Assessment%20with%20Vision%20Transformers&body=Title%3A%20ViTNT-FIQA%3A%20Training-Free%20Face%20Image%20Quality%20Assessment%20with%20Vision%20Transformers%0AAuthor%3A%20Guray%20Ozgur%20and%20Eduarda%20Caldeira%20and%20Tahar%20Chettaoui%20and%20Jan%20Niklas%20Kolf%20and%20Marco%20Huber%20and%20Naser%20Damer%20and%20Fadi%20Boutros%0AAbstract%3A%20Face%20Image%20Quality%20Assessment%20%28FIQA%29%20is%20essential%20for%20reliable%20face%20recognition%20systems.%20Current%20approaches%20primarily%20exploit%20only%20final-layer%20representations%2C%20while%20training-free%20methods%20require%20multiple%20forward%20passes%20or%20backpropagation.%20We%20propose%20ViTNT-FIQA%2C%20a%20training-free%20approach%20that%20measures%20the%20stability%20of%20patch%20embedding%20evolution%20across%20intermediate%20Vision%20Transformer%20%28ViT%29%20blocks.%20We%20demonstrate%20that%20high-quality%20face%20images%20exhibit%20stable%20feature%20refinement%20trajectories%20across%20blocks%2C%20while%20degraded%20images%20show%20erratic%20transformations.%20Our%20method%20computes%20Euclidean%20distances%20between%20L2-normalized%20patch%20embeddings%20from%20consecutive%20transformer%20blocks%20and%20aggregates%20them%20into%20image-level%20quality%20scores.%20We%20empirically%20validate%20this%20correlation%20on%20a%20quality-labeled%20synthetic%20dataset%20with%20controlled%20degradation%20levels.%20Unlike%20existing%20training-free%20approaches%2C%20ViTNT-FIQA%20requires%20only%20a%20single%20forward%20pass%20without%20backpropagation%20or%20architectural%20modifications.%20Through%20extensive%20evaluation%20on%20eight%20benchmarks%20%28LFW%2C%20AgeDB-30%2C%20CFP-FP%2C%20CALFW%2C%20Adience%2C%20CPLFW%2C%20XQLFW%2C%20IJB-C%29%2C%20we%20show%20that%20ViTNT-FIQA%20achieves%20competitive%20performance%20with%20state-of-the-art%20methods%20while%20maintaining%20computational%20efficiency%20and%20immediate%20applicability%20to%20any%20pre-trained%20ViT-based%20face%20recognition%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTNT-FIQA%253A%2520Training-Free%2520Face%2520Image%2520Quality%2520Assessment%2520with%2520Vision%2520Transformers%26entry.906535625%3DGuray%2520Ozgur%2520and%2520Eduarda%2520Caldeira%2520and%2520Tahar%2520Chettaoui%2520and%2520Jan%2520Niklas%2520Kolf%2520and%2520Marco%2520Huber%2520and%2520Naser%2520Damer%2520and%2520Fadi%2520Boutros%26entry.1292438233%3DFace%2520Image%2520Quality%2520Assessment%2520%2528FIQA%2529%2520is%2520essential%2520for%2520reliable%2520face%2520recognition%2520systems.%2520Current%2520approaches%2520primarily%2520exploit%2520only%2520final-layer%2520representations%252C%2520while%2520training-free%2520methods%2520require%2520multiple%2520forward%2520passes%2520or%2520backpropagation.%2520We%2520propose%2520ViTNT-FIQA%252C%2520a%2520training-free%2520approach%2520that%2520measures%2520the%2520stability%2520of%2520patch%2520embedding%2520evolution%2520across%2520intermediate%2520Vision%2520Transformer%2520%2528ViT%2529%2520blocks.%2520We%2520demonstrate%2520that%2520high-quality%2520face%2520images%2520exhibit%2520stable%2520feature%2520refinement%2520trajectories%2520across%2520blocks%252C%2520while%2520degraded%2520images%2520show%2520erratic%2520transformations.%2520Our%2520method%2520computes%2520Euclidean%2520distances%2520between%2520L2-normalized%2520patch%2520embeddings%2520from%2520consecutive%2520transformer%2520blocks%2520and%2520aggregates%2520them%2520into%2520image-level%2520quality%2520scores.%2520We%2520empirically%2520validate%2520this%2520correlation%2520on%2520a%2520quality-labeled%2520synthetic%2520dataset%2520with%2520controlled%2520degradation%2520levels.%2520Unlike%2520existing%2520training-free%2520approaches%252C%2520ViTNT-FIQA%2520requires%2520only%2520a%2520single%2520forward%2520pass%2520without%2520backpropagation%2520or%2520architectural%2520modifications.%2520Through%2520extensive%2520evaluation%2520on%2520eight%2520benchmarks%2520%2528LFW%252C%2520AgeDB-30%252C%2520CFP-FP%252C%2520CALFW%252C%2520Adience%252C%2520CPLFW%252C%2520XQLFW%252C%2520IJB-C%2529%252C%2520we%2520show%2520that%2520ViTNT-FIQA%2520achieves%2520competitive%2520performance%2520with%2520state-of-the-art%2520methods%2520while%2520maintaining%2520computational%2520efficiency%2520and%2520immediate%2520applicability%2520to%2520any%2520pre-trained%2520ViT-based%2520face%2520recognition%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTNT-FIQA%3A%20Training-Free%20Face%20Image%20Quality%20Assessment%20with%20Vision%20Transformers&entry.906535625=Guray%20Ozgur%20and%20Eduarda%20Caldeira%20and%20Tahar%20Chettaoui%20and%20Jan%20Niklas%20Kolf%20and%20Marco%20Huber%20and%20Naser%20Damer%20and%20Fadi%20Boutros&entry.1292438233=Face%20Image%20Quality%20Assessment%20%28FIQA%29%20is%20essential%20for%20reliable%20face%20recognition%20systems.%20Current%20approaches%20primarily%20exploit%20only%20final-layer%20representations%2C%20while%20training-free%20methods%20require%20multiple%20forward%20passes%20or%20backpropagation.%20We%20propose%20ViTNT-FIQA%2C%20a%20training-free%20approach%20that%20measures%20the%20stability%20of%20patch%20embedding%20evolution%20across%20intermediate%20Vision%20Transformer%20%28ViT%29%20blocks.%20We%20demonstrate%20that%20high-quality%20face%20images%20exhibit%20stable%20feature%20refinement%20trajectories%20across%20blocks%2C%20while%20degraded%20images%20show%20erratic%20transformations.%20Our%20method%20computes%20Euclidean%20distances%20between%20L2-normalized%20patch%20embeddings%20from%20consecutive%20transformer%20blocks%20and%20aggregates%20them%20into%20image-level%20quality%20scores.%20We%20empirically%20validate%20this%20correlation%20on%20a%20quality-labeled%20synthetic%20dataset%20with%20controlled%20degradation%20levels.%20Unlike%20existing%20training-free%20approaches%2C%20ViTNT-FIQA%20requires%20only%20a%20single%20forward%20pass%20without%20backpropagation%20or%20architectural%20modifications.%20Through%20extensive%20evaluation%20on%20eight%20benchmarks%20%28LFW%2C%20AgeDB-30%2C%20CFP-FP%2C%20CALFW%2C%20Adience%2C%20CPLFW%2C%20XQLFW%2C%20IJB-C%29%2C%20we%20show%20that%20ViTNT-FIQA%20achieves%20competitive%20performance%20with%20state-of-the-art%20methods%20while%20maintaining%20computational%20efficiency%20and%20immediate%20applicability%20to%20any%20pre-trained%20ViT-based%20face%20recognition%20model.&entry.1838667208=http%3A//arxiv.org/abs/2601.05741v1&entry.124074799=Read"},
{"title": "Can We Predict Before Executing Machine Learning Agents?", "author": "Jingsheng Zheng and Jintian Zhang and Yujie Luo and Yuren Mao and Yunjun Gao and Lun Du and Huajun Chen and Ningyu Zhang", "abstract": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.", "link": "http://arxiv.org/abs/2601.05930v1", "date": "2026-01-09", "relevancy": 2.1182, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Predict%20Before%20Executing%20Machine%20Learning%20Agents%3F&body=Title%3A%20Can%20We%20Predict%20Before%20Executing%20Machine%20Learning%20Agents%3F%0AAuthor%3A%20Jingsheng%20Zheng%20and%20Jintian%20Zhang%20and%20Yujie%20Luo%20and%20Yuren%20Mao%20and%20Yunjun%20Gao%20and%20Lun%20Du%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20Autonomous%20machine%20learning%20agents%20have%20revolutionized%20scientific%20discovery%2C%20yet%20they%20remain%20constrained%20by%20a%20Generate-Execute-Feedback%20paradigm.%20Previous%20approaches%20suffer%20from%20a%20severe%20Execution%20Bottleneck%2C%20as%20hypothesis%20evaluation%20relies%20strictly%20on%20expensive%20physical%20execution.%20To%20bypass%20these%20physical%20constraints%2C%20we%20internalize%20execution%20priors%20to%20substitute%20costly%20runtime%20checks%20with%20instantaneous%20predictive%20reasoning%2C%20drawing%20inspiration%20from%20World%20Models.%20In%20this%20work%2C%20we%20formalize%20the%20task%20of%20Data-centric%20Solution%20Preference%20and%20construct%20a%20comprehensive%20corpus%20of%2018%2C438%20pairwise%20comparisons.%20We%20demonstrate%20that%20LLMs%20exhibit%20significant%20predictive%20capabilities%20when%20primed%20with%20a%20Verified%20Data%20Analysis%20Report%2C%20achieving%2061.5%25%20accuracy%20and%20robust%20confidence%20calibration.%20Finally%2C%20we%20instantiate%20this%20framework%20in%20FOREAGENT%2C%20an%20agent%20that%20employs%20a%20Predict-then-Verify%20loop%2C%20achieving%20a%206x%20acceleration%20in%20convergence%20while%20surpassing%20execution-based%20baselines%20by%20%2B6%25.%20Our%20code%20and%20dataset%20will%20be%20publicly%20available%20soon%20at%20https%3A//github.com/zjunlp/predict-before-execute.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Predict%2520Before%2520Executing%2520Machine%2520Learning%2520Agents%253F%26entry.906535625%3DJingsheng%2520Zheng%2520and%2520Jintian%2520Zhang%2520and%2520Yujie%2520Luo%2520and%2520Yuren%2520Mao%2520and%2520Yunjun%2520Gao%2520and%2520Lun%2520Du%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3DAutonomous%2520machine%2520learning%2520agents%2520have%2520revolutionized%2520scientific%2520discovery%252C%2520yet%2520they%2520remain%2520constrained%2520by%2520a%2520Generate-Execute-Feedback%2520paradigm.%2520Previous%2520approaches%2520suffer%2520from%2520a%2520severe%2520Execution%2520Bottleneck%252C%2520as%2520hypothesis%2520evaluation%2520relies%2520strictly%2520on%2520expensive%2520physical%2520execution.%2520To%2520bypass%2520these%2520physical%2520constraints%252C%2520we%2520internalize%2520execution%2520priors%2520to%2520substitute%2520costly%2520runtime%2520checks%2520with%2520instantaneous%2520predictive%2520reasoning%252C%2520drawing%2520inspiration%2520from%2520World%2520Models.%2520In%2520this%2520work%252C%2520we%2520formalize%2520the%2520task%2520of%2520Data-centric%2520Solution%2520Preference%2520and%2520construct%2520a%2520comprehensive%2520corpus%2520of%252018%252C438%2520pairwise%2520comparisons.%2520We%2520demonstrate%2520that%2520LLMs%2520exhibit%2520significant%2520predictive%2520capabilities%2520when%2520primed%2520with%2520a%2520Verified%2520Data%2520Analysis%2520Report%252C%2520achieving%252061.5%2525%2520accuracy%2520and%2520robust%2520confidence%2520calibration.%2520Finally%252C%2520we%2520instantiate%2520this%2520framework%2520in%2520FOREAGENT%252C%2520an%2520agent%2520that%2520employs%2520a%2520Predict-then-Verify%2520loop%252C%2520achieving%2520a%25206x%2520acceleration%2520in%2520convergence%2520while%2520surpassing%2520execution-based%2520baselines%2520by%2520%252B6%2525.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520publicly%2520available%2520soon%2520at%2520https%253A//github.com/zjunlp/predict-before-execute.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Predict%20Before%20Executing%20Machine%20Learning%20Agents%3F&entry.906535625=Jingsheng%20Zheng%20and%20Jintian%20Zhang%20and%20Yujie%20Luo%20and%20Yuren%20Mao%20and%20Yunjun%20Gao%20and%20Lun%20Du%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=Autonomous%20machine%20learning%20agents%20have%20revolutionized%20scientific%20discovery%2C%20yet%20they%20remain%20constrained%20by%20a%20Generate-Execute-Feedback%20paradigm.%20Previous%20approaches%20suffer%20from%20a%20severe%20Execution%20Bottleneck%2C%20as%20hypothesis%20evaluation%20relies%20strictly%20on%20expensive%20physical%20execution.%20To%20bypass%20these%20physical%20constraints%2C%20we%20internalize%20execution%20priors%20to%20substitute%20costly%20runtime%20checks%20with%20instantaneous%20predictive%20reasoning%2C%20drawing%20inspiration%20from%20World%20Models.%20In%20this%20work%2C%20we%20formalize%20the%20task%20of%20Data-centric%20Solution%20Preference%20and%20construct%20a%20comprehensive%20corpus%20of%2018%2C438%20pairwise%20comparisons.%20We%20demonstrate%20that%20LLMs%20exhibit%20significant%20predictive%20capabilities%20when%20primed%20with%20a%20Verified%20Data%20Analysis%20Report%2C%20achieving%2061.5%25%20accuracy%20and%20robust%20confidence%20calibration.%20Finally%2C%20we%20instantiate%20this%20framework%20in%20FOREAGENT%2C%20an%20agent%20that%20employs%20a%20Predict-then-Verify%20loop%2C%20achieving%20a%206x%20acceleration%20in%20convergence%20while%20surpassing%20execution-based%20baselines%20by%20%2B6%25.%20Our%20code%20and%20dataset%20will%20be%20publicly%20available%20soon%20at%20https%3A//github.com/zjunlp/predict-before-execute.&entry.1838667208=http%3A//arxiv.org/abs/2601.05930v1&entry.124074799=Read"},
{"title": "Explainable AI needs formalization", "author": "Stefan Haufe and Rick Wilming and Benedict Clark and Rustam Zhumagambetov and Ahc\u00e8ne Boubekki and J\u00f6rg Martin and Danny Panknin", "abstract": "The field of \"explainable artificial intelligence\" (XAI) seemingly addresses the desire that decisions of machine learning systems should be human-understandable. However, in its current state, XAI itself needs scrutiny. Popular methods cannot reliably answer relevant questions about ML models, their training data, or test inputs, because they systematically attribute importance to input features that are independent of the prediction target. This limits the utility of XAI for diagnosing and correcting data and models, for scientific discovery, and for identifying intervention targets. The fundamental reason for this is that current XAI methods do not address well-defined problems and are not evaluated against targeted criteria of explanation correctness. Researchers should formally define the problems they intend to solve and design methods accordingly. This will lead to diverse use-case-dependent notions of explanation correctness and objective metrics of explanation performance that can be used to validate XAI algorithms.", "link": "http://arxiv.org/abs/2409.14590v4", "date": "2026-01-09", "relevancy": 2.1117, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20needs%20formalization&body=Title%3A%20Explainable%20AI%20needs%20formalization%0AAuthor%3A%20Stefan%20Haufe%20and%20Rick%20Wilming%20and%20Benedict%20Clark%20and%20Rustam%20Zhumagambetov%20and%20Ahc%C3%A8ne%20Boubekki%20and%20J%C3%B6rg%20Martin%20and%20Danny%20Panknin%0AAbstract%3A%20The%20field%20of%20%22explainable%20artificial%20intelligence%22%20%28XAI%29%20seemingly%20addresses%20the%20desire%20that%20decisions%20of%20machine%20learning%20systems%20should%20be%20human-understandable.%20However%2C%20in%20its%20current%20state%2C%20XAI%20itself%20needs%20scrutiny.%20Popular%20methods%20cannot%20reliably%20answer%20relevant%20questions%20about%20ML%20models%2C%20their%20training%20data%2C%20or%20test%20inputs%2C%20because%20they%20systematically%20attribute%20importance%20to%20input%20features%20that%20are%20independent%20of%20the%20prediction%20target.%20This%20limits%20the%20utility%20of%20XAI%20for%20diagnosing%20and%20correcting%20data%20and%20models%2C%20for%20scientific%20discovery%2C%20and%20for%20identifying%20intervention%20targets.%20The%20fundamental%20reason%20for%20this%20is%20that%20current%20XAI%20methods%20do%20not%20address%20well-defined%20problems%20and%20are%20not%20evaluated%20against%20targeted%20criteria%20of%20explanation%20correctness.%20Researchers%20should%20formally%20define%20the%20problems%20they%20intend%20to%20solve%20and%20design%20methods%20accordingly.%20This%20will%20lead%20to%20diverse%20use-case-dependent%20notions%20of%20explanation%20correctness%20and%20objective%20metrics%20of%20explanation%20performance%20that%20can%20be%20used%20to%20validate%20XAI%20algorithms.%0ALink%3A%20http%3A//arxiv.org/abs/2409.14590v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520needs%2520formalization%26entry.906535625%3DStefan%2520Haufe%2520and%2520Rick%2520Wilming%2520and%2520Benedict%2520Clark%2520and%2520Rustam%2520Zhumagambetov%2520and%2520Ahc%25C3%25A8ne%2520Boubekki%2520and%2520J%25C3%25B6rg%2520Martin%2520and%2520Danny%2520Panknin%26entry.1292438233%3DThe%2520field%2520of%2520%2522explainable%2520artificial%2520intelligence%2522%2520%2528XAI%2529%2520seemingly%2520addresses%2520the%2520desire%2520that%2520decisions%2520of%2520machine%2520learning%2520systems%2520should%2520be%2520human-understandable.%2520However%252C%2520in%2520its%2520current%2520state%252C%2520XAI%2520itself%2520needs%2520scrutiny.%2520Popular%2520methods%2520cannot%2520reliably%2520answer%2520relevant%2520questions%2520about%2520ML%2520models%252C%2520their%2520training%2520data%252C%2520or%2520test%2520inputs%252C%2520because%2520they%2520systematically%2520attribute%2520importance%2520to%2520input%2520features%2520that%2520are%2520independent%2520of%2520the%2520prediction%2520target.%2520This%2520limits%2520the%2520utility%2520of%2520XAI%2520for%2520diagnosing%2520and%2520correcting%2520data%2520and%2520models%252C%2520for%2520scientific%2520discovery%252C%2520and%2520for%2520identifying%2520intervention%2520targets.%2520The%2520fundamental%2520reason%2520for%2520this%2520is%2520that%2520current%2520XAI%2520methods%2520do%2520not%2520address%2520well-defined%2520problems%2520and%2520are%2520not%2520evaluated%2520against%2520targeted%2520criteria%2520of%2520explanation%2520correctness.%2520Researchers%2520should%2520formally%2520define%2520the%2520problems%2520they%2520intend%2520to%2520solve%2520and%2520design%2520methods%2520accordingly.%2520This%2520will%2520lead%2520to%2520diverse%2520use-case-dependent%2520notions%2520of%2520explanation%2520correctness%2520and%2520objective%2520metrics%2520of%2520explanation%2520performance%2520that%2520can%2520be%2520used%2520to%2520validate%2520XAI%2520algorithms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14590v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20needs%20formalization&entry.906535625=Stefan%20Haufe%20and%20Rick%20Wilming%20and%20Benedict%20Clark%20and%20Rustam%20Zhumagambetov%20and%20Ahc%C3%A8ne%20Boubekki%20and%20J%C3%B6rg%20Martin%20and%20Danny%20Panknin&entry.1292438233=The%20field%20of%20%22explainable%20artificial%20intelligence%22%20%28XAI%29%20seemingly%20addresses%20the%20desire%20that%20decisions%20of%20machine%20learning%20systems%20should%20be%20human-understandable.%20However%2C%20in%20its%20current%20state%2C%20XAI%20itself%20needs%20scrutiny.%20Popular%20methods%20cannot%20reliably%20answer%20relevant%20questions%20about%20ML%20models%2C%20their%20training%20data%2C%20or%20test%20inputs%2C%20because%20they%20systematically%20attribute%20importance%20to%20input%20features%20that%20are%20independent%20of%20the%20prediction%20target.%20This%20limits%20the%20utility%20of%20XAI%20for%20diagnosing%20and%20correcting%20data%20and%20models%2C%20for%20scientific%20discovery%2C%20and%20for%20identifying%20intervention%20targets.%20The%20fundamental%20reason%20for%20this%20is%20that%20current%20XAI%20methods%20do%20not%20address%20well-defined%20problems%20and%20are%20not%20evaluated%20against%20targeted%20criteria%20of%20explanation%20correctness.%20Researchers%20should%20formally%20define%20the%20problems%20they%20intend%20to%20solve%20and%20design%20methods%20accordingly.%20This%20will%20lead%20to%20diverse%20use-case-dependent%20notions%20of%20explanation%20correctness%20and%20objective%20metrics%20of%20explanation%20performance%20that%20can%20be%20used%20to%20validate%20XAI%20algorithms.&entry.1838667208=http%3A//arxiv.org/abs/2409.14590v4&entry.124074799=Read"},
{"title": "Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns", "author": "Amalie Brogaard Pauli and Maria Barrett and Max M\u00fcller-Eberstein and Isabelle Augenstein and Ira Assent", "abstract": "Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.", "link": "http://arxiv.org/abs/2601.05751v1", "date": "2026-01-09", "relevancy": 2.1063, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4217}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysing%20Differences%20in%20Persuasive%20Language%20in%20LLM-Generated%20Text%3A%20Uncovering%20Stereotypical%20Gender%20Patterns&body=Title%3A%20Analysing%20Differences%20in%20Persuasive%20Language%20in%20LLM-Generated%20Text%3A%20Uncovering%20Stereotypical%20Gender%20Patterns%0AAuthor%3A%20Amalie%20Brogaard%20Pauli%20and%20Maria%20Barrett%20and%20Max%20M%C3%BCller-Eberstein%20and%20Isabelle%20Augenstein%20and%20Ira%20Assent%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20for%20everyday%20communication%20tasks%2C%20including%20drafting%20interpersonal%20messages%20intended%20to%20influence%20and%20persuade.%20Prior%20work%20has%20shown%20that%20LLMs%20can%20successfully%20persuade%20humans%20and%20amplify%20persuasive%20language.%20It%20is%20therefore%20essential%20to%20understand%20how%20user%20instructions%20affect%20the%20generation%20of%20persuasive%20language%2C%20and%20to%20understand%20whether%20the%20generated%20persuasive%20language%20differs%2C%20for%20example%2C%20when%20targeting%20different%20groups.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%20evaluating%20how%20persuasive%20language%20generation%20is%20affected%20by%20recipient%20gender%2C%20sender%20intent%2C%20or%20output%20language.%20We%20evaluate%2013%20LLMs%20and%2016%20languages%20using%20pairwise%20prompt%20instructions.%20We%20evaluate%20model%20responses%20on%2019%20categories%20of%20persuasive%20language%20using%20an%20LLM-as-judge%20setup%20grounded%20in%20social%20psychology%20and%20communication%20science.%20Our%20results%20reveal%20significant%20gender%20differences%20in%20the%20persuasive%20language%20generated%20across%20all%20models.%20These%20patterns%20reflect%20biases%20consistent%20with%20gender-stereotypical%20linguistic%20tendencies%20documented%20in%20social%20psychology%20and%20sociolinguistics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysing%2520Differences%2520in%2520Persuasive%2520Language%2520in%2520LLM-Generated%2520Text%253A%2520Uncovering%2520Stereotypical%2520Gender%2520Patterns%26entry.906535625%3DAmalie%2520Brogaard%2520Pauli%2520and%2520Maria%2520Barrett%2520and%2520Max%2520M%25C3%25BCller-Eberstein%2520and%2520Isabelle%2520Augenstein%2520and%2520Ira%2520Assent%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520for%2520everyday%2520communication%2520tasks%252C%2520including%2520drafting%2520interpersonal%2520messages%2520intended%2520to%2520influence%2520and%2520persuade.%2520Prior%2520work%2520has%2520shown%2520that%2520LLMs%2520can%2520successfully%2520persuade%2520humans%2520and%2520amplify%2520persuasive%2520language.%2520It%2520is%2520therefore%2520essential%2520to%2520understand%2520how%2520user%2520instructions%2520affect%2520the%2520generation%2520of%2520persuasive%2520language%252C%2520and%2520to%2520understand%2520whether%2520the%2520generated%2520persuasive%2520language%2520differs%252C%2520for%2520example%252C%2520when%2520targeting%2520different%2520groups.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520framework%2520for%2520evaluating%2520how%2520persuasive%2520language%2520generation%2520is%2520affected%2520by%2520recipient%2520gender%252C%2520sender%2520intent%252C%2520or%2520output%2520language.%2520We%2520evaluate%252013%2520LLMs%2520and%252016%2520languages%2520using%2520pairwise%2520prompt%2520instructions.%2520We%2520evaluate%2520model%2520responses%2520on%252019%2520categories%2520of%2520persuasive%2520language%2520using%2520an%2520LLM-as-judge%2520setup%2520grounded%2520in%2520social%2520psychology%2520and%2520communication%2520science.%2520Our%2520results%2520reveal%2520significant%2520gender%2520differences%2520in%2520the%2520persuasive%2520language%2520generated%2520across%2520all%2520models.%2520These%2520patterns%2520reflect%2520biases%2520consistent%2520with%2520gender-stereotypical%2520linguistic%2520tendencies%2520documented%2520in%2520social%2520psychology%2520and%2520sociolinguistics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20Differences%20in%20Persuasive%20Language%20in%20LLM-Generated%20Text%3A%20Uncovering%20Stereotypical%20Gender%20Patterns&entry.906535625=Amalie%20Brogaard%20Pauli%20and%20Maria%20Barrett%20and%20Max%20M%C3%BCller-Eberstein%20and%20Isabelle%20Augenstein%20and%20Ira%20Assent&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20for%20everyday%20communication%20tasks%2C%20including%20drafting%20interpersonal%20messages%20intended%20to%20influence%20and%20persuade.%20Prior%20work%20has%20shown%20that%20LLMs%20can%20successfully%20persuade%20humans%20and%20amplify%20persuasive%20language.%20It%20is%20therefore%20essential%20to%20understand%20how%20user%20instructions%20affect%20the%20generation%20of%20persuasive%20language%2C%20and%20to%20understand%20whether%20the%20generated%20persuasive%20language%20differs%2C%20for%20example%2C%20when%20targeting%20different%20groups.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%20evaluating%20how%20persuasive%20language%20generation%20is%20affected%20by%20recipient%20gender%2C%20sender%20intent%2C%20or%20output%20language.%20We%20evaluate%2013%20LLMs%20and%2016%20languages%20using%20pairwise%20prompt%20instructions.%20We%20evaluate%20model%20responses%20on%2019%20categories%20of%20persuasive%20language%20using%20an%20LLM-as-judge%20setup%20grounded%20in%20social%20psychology%20and%20communication%20science.%20Our%20results%20reveal%20significant%20gender%20differences%20in%20the%20persuasive%20language%20generated%20across%20all%20models.%20These%20patterns%20reflect%20biases%20consistent%20with%20gender-stereotypical%20linguistic%20tendencies%20documented%20in%20social%20psychology%20and%20sociolinguistics.&entry.1838667208=http%3A//arxiv.org/abs/2601.05751v1&entry.124074799=Read"},
{"title": "CHDP: Cooperative Hybrid Diffusion Policies for Reinforcement Learning in Parameterized Action Space", "author": "Bingyi Liu and Jinbo He and Haiyong Shi and Enshu Wang and Weizhen Han and Jingxiang Hao and Peixi Wang and Zhuangzhuang Zhang", "abstract": "Hybrid action space, which combines discrete choices and continuous parameters, is prevalent in domains such as robot control and game AI. However, efficiently modeling and optimizing hybrid discrete-continuous action space remains a fundamental challenge, mainly due to limited policy expressiveness and poor scalability in high-dimensional settings. To address this challenge, we view the hybrid action space problem as a fully cooperative game and propose a \\textbf{Cooperative Hybrid Diffusion Policies (CHDP)} framework to solve it. CHDP employs two cooperative agents that leverage a discrete and a continuous diffusion policy, respectively. The continuous policy is conditioned on the discrete action's representation, explicitly modeling the dependency between them. This cooperative design allows the diffusion policies to leverage their expressiveness to capture complex distributions in their respective action spaces. To mitigate the update conflicts arising from simultaneous policy updates in this cooperative setting, we employ a sequential update scheme that fosters co-adaptation. Moreover, to improve scalability when learning in high-dimensional discrete action space, we construct a codebook that embeds the action space into a low-dimensional latent space. This mapping enables the discrete policy to learn in a compact, structured space. Finally, we design a Q-function-based guidance mechanism to align the codebook's embeddings with the discrete policy's representation during training. On challenging hybrid action benchmarks, CHDP outperforms the state-of-the-art method by up to $19.3\\%$ in success rate.", "link": "http://arxiv.org/abs/2601.05675v1", "date": "2026-01-09", "relevancy": 2.0987, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5274}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHDP%3A%20Cooperative%20Hybrid%20Diffusion%20Policies%20for%20Reinforcement%20Learning%20in%20Parameterized%20Action%20Space&body=Title%3A%20CHDP%3A%20Cooperative%20Hybrid%20Diffusion%20Policies%20for%20Reinforcement%20Learning%20in%20Parameterized%20Action%20Space%0AAuthor%3A%20Bingyi%20Liu%20and%20Jinbo%20He%20and%20Haiyong%20Shi%20and%20Enshu%20Wang%20and%20Weizhen%20Han%20and%20Jingxiang%20Hao%20and%20Peixi%20Wang%20and%20Zhuangzhuang%20Zhang%0AAbstract%3A%20Hybrid%20action%20space%2C%20which%20combines%20discrete%20choices%20and%20continuous%20parameters%2C%20is%20prevalent%20in%20domains%20such%20as%20robot%20control%20and%20game%20AI.%20However%2C%20efficiently%20modeling%20and%20optimizing%20hybrid%20discrete-continuous%20action%20space%20remains%20a%20fundamental%20challenge%2C%20mainly%20due%20to%20limited%20policy%20expressiveness%20and%20poor%20scalability%20in%20high-dimensional%20settings.%20To%20address%20this%20challenge%2C%20we%20view%20the%20hybrid%20action%20space%20problem%20as%20a%20fully%20cooperative%20game%20and%20propose%20a%20%5Ctextbf%7BCooperative%20Hybrid%20Diffusion%20Policies%20%28CHDP%29%7D%20framework%20to%20solve%20it.%20CHDP%20employs%20two%20cooperative%20agents%20that%20leverage%20a%20discrete%20and%20a%20continuous%20diffusion%20policy%2C%20respectively.%20The%20continuous%20policy%20is%20conditioned%20on%20the%20discrete%20action%27s%20representation%2C%20explicitly%20modeling%20the%20dependency%20between%20them.%20This%20cooperative%20design%20allows%20the%20diffusion%20policies%20to%20leverage%20their%20expressiveness%20to%20capture%20complex%20distributions%20in%20their%20respective%20action%20spaces.%20To%20mitigate%20the%20update%20conflicts%20arising%20from%20simultaneous%20policy%20updates%20in%20this%20cooperative%20setting%2C%20we%20employ%20a%20sequential%20update%20scheme%20that%20fosters%20co-adaptation.%20Moreover%2C%20to%20improve%20scalability%20when%20learning%20in%20high-dimensional%20discrete%20action%20space%2C%20we%20construct%20a%20codebook%20that%20embeds%20the%20action%20space%20into%20a%20low-dimensional%20latent%20space.%20This%20mapping%20enables%20the%20discrete%20policy%20to%20learn%20in%20a%20compact%2C%20structured%20space.%20Finally%2C%20we%20design%20a%20Q-function-based%20guidance%20mechanism%20to%20align%20the%20codebook%27s%20embeddings%20with%20the%20discrete%20policy%27s%20representation%20during%20training.%20On%20challenging%20hybrid%20action%20benchmarks%2C%20CHDP%20outperforms%20the%20state-of-the-art%20method%20by%20up%20to%20%2419.3%5C%25%24%20in%20success%20rate.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHDP%253A%2520Cooperative%2520Hybrid%2520Diffusion%2520Policies%2520for%2520Reinforcement%2520Learning%2520in%2520Parameterized%2520Action%2520Space%26entry.906535625%3DBingyi%2520Liu%2520and%2520Jinbo%2520He%2520and%2520Haiyong%2520Shi%2520and%2520Enshu%2520Wang%2520and%2520Weizhen%2520Han%2520and%2520Jingxiang%2520Hao%2520and%2520Peixi%2520Wang%2520and%2520Zhuangzhuang%2520Zhang%26entry.1292438233%3DHybrid%2520action%2520space%252C%2520which%2520combines%2520discrete%2520choices%2520and%2520continuous%2520parameters%252C%2520is%2520prevalent%2520in%2520domains%2520such%2520as%2520robot%2520control%2520and%2520game%2520AI.%2520However%252C%2520efficiently%2520modeling%2520and%2520optimizing%2520hybrid%2520discrete-continuous%2520action%2520space%2520remains%2520a%2520fundamental%2520challenge%252C%2520mainly%2520due%2520to%2520limited%2520policy%2520expressiveness%2520and%2520poor%2520scalability%2520in%2520high-dimensional%2520settings.%2520To%2520address%2520this%2520challenge%252C%2520we%2520view%2520the%2520hybrid%2520action%2520space%2520problem%2520as%2520a%2520fully%2520cooperative%2520game%2520and%2520propose%2520a%2520%255Ctextbf%257BCooperative%2520Hybrid%2520Diffusion%2520Policies%2520%2528CHDP%2529%257D%2520framework%2520to%2520solve%2520it.%2520CHDP%2520employs%2520two%2520cooperative%2520agents%2520that%2520leverage%2520a%2520discrete%2520and%2520a%2520continuous%2520diffusion%2520policy%252C%2520respectively.%2520The%2520continuous%2520policy%2520is%2520conditioned%2520on%2520the%2520discrete%2520action%2527s%2520representation%252C%2520explicitly%2520modeling%2520the%2520dependency%2520between%2520them.%2520This%2520cooperative%2520design%2520allows%2520the%2520diffusion%2520policies%2520to%2520leverage%2520their%2520expressiveness%2520to%2520capture%2520complex%2520distributions%2520in%2520their%2520respective%2520action%2520spaces.%2520To%2520mitigate%2520the%2520update%2520conflicts%2520arising%2520from%2520simultaneous%2520policy%2520updates%2520in%2520this%2520cooperative%2520setting%252C%2520we%2520employ%2520a%2520sequential%2520update%2520scheme%2520that%2520fosters%2520co-adaptation.%2520Moreover%252C%2520to%2520improve%2520scalability%2520when%2520learning%2520in%2520high-dimensional%2520discrete%2520action%2520space%252C%2520we%2520construct%2520a%2520codebook%2520that%2520embeds%2520the%2520action%2520space%2520into%2520a%2520low-dimensional%2520latent%2520space.%2520This%2520mapping%2520enables%2520the%2520discrete%2520policy%2520to%2520learn%2520in%2520a%2520compact%252C%2520structured%2520space.%2520Finally%252C%2520we%2520design%2520a%2520Q-function-based%2520guidance%2520mechanism%2520to%2520align%2520the%2520codebook%2527s%2520embeddings%2520with%2520the%2520discrete%2520policy%2527s%2520representation%2520during%2520training.%2520On%2520challenging%2520hybrid%2520action%2520benchmarks%252C%2520CHDP%2520outperforms%2520the%2520state-of-the-art%2520method%2520by%2520up%2520to%2520%252419.3%255C%2525%2524%2520in%2520success%2520rate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHDP%3A%20Cooperative%20Hybrid%20Diffusion%20Policies%20for%20Reinforcement%20Learning%20in%20Parameterized%20Action%20Space&entry.906535625=Bingyi%20Liu%20and%20Jinbo%20He%20and%20Haiyong%20Shi%20and%20Enshu%20Wang%20and%20Weizhen%20Han%20and%20Jingxiang%20Hao%20and%20Peixi%20Wang%20and%20Zhuangzhuang%20Zhang&entry.1292438233=Hybrid%20action%20space%2C%20which%20combines%20discrete%20choices%20and%20continuous%20parameters%2C%20is%20prevalent%20in%20domains%20such%20as%20robot%20control%20and%20game%20AI.%20However%2C%20efficiently%20modeling%20and%20optimizing%20hybrid%20discrete-continuous%20action%20space%20remains%20a%20fundamental%20challenge%2C%20mainly%20due%20to%20limited%20policy%20expressiveness%20and%20poor%20scalability%20in%20high-dimensional%20settings.%20To%20address%20this%20challenge%2C%20we%20view%20the%20hybrid%20action%20space%20problem%20as%20a%20fully%20cooperative%20game%20and%20propose%20a%20%5Ctextbf%7BCooperative%20Hybrid%20Diffusion%20Policies%20%28CHDP%29%7D%20framework%20to%20solve%20it.%20CHDP%20employs%20two%20cooperative%20agents%20that%20leverage%20a%20discrete%20and%20a%20continuous%20diffusion%20policy%2C%20respectively.%20The%20continuous%20policy%20is%20conditioned%20on%20the%20discrete%20action%27s%20representation%2C%20explicitly%20modeling%20the%20dependency%20between%20them.%20This%20cooperative%20design%20allows%20the%20diffusion%20policies%20to%20leverage%20their%20expressiveness%20to%20capture%20complex%20distributions%20in%20their%20respective%20action%20spaces.%20To%20mitigate%20the%20update%20conflicts%20arising%20from%20simultaneous%20policy%20updates%20in%20this%20cooperative%20setting%2C%20we%20employ%20a%20sequential%20update%20scheme%20that%20fosters%20co-adaptation.%20Moreover%2C%20to%20improve%20scalability%20when%20learning%20in%20high-dimensional%20discrete%20action%20space%2C%20we%20construct%20a%20codebook%20that%20embeds%20the%20action%20space%20into%20a%20low-dimensional%20latent%20space.%20This%20mapping%20enables%20the%20discrete%20policy%20to%20learn%20in%20a%20compact%2C%20structured%20space.%20Finally%2C%20we%20design%20a%20Q-function-based%20guidance%20mechanism%20to%20align%20the%20codebook%27s%20embeddings%20with%20the%20discrete%20policy%27s%20representation%20during%20training.%20On%20challenging%20hybrid%20action%20benchmarks%2C%20CHDP%20outperforms%20the%20state-of-the-art%20method%20by%20up%20to%20%2419.3%5C%25%24%20in%20success%20rate.&entry.1838667208=http%3A//arxiv.org/abs/2601.05675v1&entry.124074799=Read"},
{"title": "Solving Inverse Problems in Stochastic Self-Organizing Systems through Invariant Representations", "author": "Elias Najarro and Nicolas Bessone and Sebastian Risi", "abstract": "Self-organizing systems demonstrate how simple local rules can generate complex stochastic patterns. Many natural systems rely on such dynamics, making self-organization central to understanding natural complexity. A fundamental challenge in modeling such systems is solving the inverse problem: finding the unknown causal parameters from macroscopic observations. This task becomes particularly difficult when observations have a strong stochastic component, yielding diverse yet equivalent patterns. Traditional inverse methods fail in this setting, as pixel-wise metrics cannot capture feature similarities between variable outcomes. In this work, we introduce a novel inverse modeling method specifically designed to handle stochasticity in the observable space, leveraging the capacity of visual embeddings to produce robust representations that capture perceptual invariances. By mapping the pattern representations onto an invariant embedding space, we can effectively recover unknown causal parameters without the need for handcrafted objective functions or heuristics. We evaluate the method on three self-organizing systems: a physical, a biological, and a social one; namely, a reaction-diffusion system, a model of embryonic development, and an agent-based model of social segregation. We show that the method reliably recovers parameters despite stochasticity in the pattern outcomes. We further apply the method to real biological patterns, highlighting its potential as a tool for both theorists and experimentalists to investigate the dynamics underlying complex stochastic pattern formation.", "link": "http://arxiv.org/abs/2506.11796v2", "date": "2026-01-09", "relevancy": 2.0847, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5558}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5198}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Inverse%20Problems%20in%20Stochastic%20Self-Organizing%20Systems%20through%20Invariant%20Representations&body=Title%3A%20Solving%20Inverse%20Problems%20in%20Stochastic%20Self-Organizing%20Systems%20through%20Invariant%20Representations%0AAuthor%3A%20Elias%20Najarro%20and%20Nicolas%20Bessone%20and%20Sebastian%20Risi%0AAbstract%3A%20Self-organizing%20systems%20demonstrate%20how%20simple%20local%20rules%20can%20generate%20complex%20stochastic%20patterns.%20Many%20natural%20systems%20rely%20on%20such%20dynamics%2C%20making%20self-organization%20central%20to%20understanding%20natural%20complexity.%20A%20fundamental%20challenge%20in%20modeling%20such%20systems%20is%20solving%20the%20inverse%20problem%3A%20finding%20the%20unknown%20causal%20parameters%20from%20macroscopic%20observations.%20This%20task%20becomes%20particularly%20difficult%20when%20observations%20have%20a%20strong%20stochastic%20component%2C%20yielding%20diverse%20yet%20equivalent%20patterns.%20Traditional%20inverse%20methods%20fail%20in%20this%20setting%2C%20as%20pixel-wise%20metrics%20cannot%20capture%20feature%20similarities%20between%20variable%20outcomes.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20inverse%20modeling%20method%20specifically%20designed%20to%20handle%20stochasticity%20in%20the%20observable%20space%2C%20leveraging%20the%20capacity%20of%20visual%20embeddings%20to%20produce%20robust%20representations%20that%20capture%20perceptual%20invariances.%20By%20mapping%20the%20pattern%20representations%20onto%20an%20invariant%20embedding%20space%2C%20we%20can%20effectively%20recover%20unknown%20causal%20parameters%20without%20the%20need%20for%20handcrafted%20objective%20functions%20or%20heuristics.%20We%20evaluate%20the%20method%20on%20three%20self-organizing%20systems%3A%20a%20physical%2C%20a%20biological%2C%20and%20a%20social%20one%3B%20namely%2C%20a%20reaction-diffusion%20system%2C%20a%20model%20of%20embryonic%20development%2C%20and%20an%20agent-based%20model%20of%20social%20segregation.%20We%20show%20that%20the%20method%20reliably%20recovers%20parameters%20despite%20stochasticity%20in%20the%20pattern%20outcomes.%20We%20further%20apply%20the%20method%20to%20real%20biological%20patterns%2C%20highlighting%20its%20potential%20as%20a%20tool%20for%20both%20theorists%20and%20experimentalists%20to%20investigate%20the%20dynamics%20underlying%20complex%20stochastic%20pattern%20formation.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Inverse%2520Problems%2520in%2520Stochastic%2520Self-Organizing%2520Systems%2520through%2520Invariant%2520Representations%26entry.906535625%3DElias%2520Najarro%2520and%2520Nicolas%2520Bessone%2520and%2520Sebastian%2520Risi%26entry.1292438233%3DSelf-organizing%2520systems%2520demonstrate%2520how%2520simple%2520local%2520rules%2520can%2520generate%2520complex%2520stochastic%2520patterns.%2520Many%2520natural%2520systems%2520rely%2520on%2520such%2520dynamics%252C%2520making%2520self-organization%2520central%2520to%2520understanding%2520natural%2520complexity.%2520A%2520fundamental%2520challenge%2520in%2520modeling%2520such%2520systems%2520is%2520solving%2520the%2520inverse%2520problem%253A%2520finding%2520the%2520unknown%2520causal%2520parameters%2520from%2520macroscopic%2520observations.%2520This%2520task%2520becomes%2520particularly%2520difficult%2520when%2520observations%2520have%2520a%2520strong%2520stochastic%2520component%252C%2520yielding%2520diverse%2520yet%2520equivalent%2520patterns.%2520Traditional%2520inverse%2520methods%2520fail%2520in%2520this%2520setting%252C%2520as%2520pixel-wise%2520metrics%2520cannot%2520capture%2520feature%2520similarities%2520between%2520variable%2520outcomes.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520inverse%2520modeling%2520method%2520specifically%2520designed%2520to%2520handle%2520stochasticity%2520in%2520the%2520observable%2520space%252C%2520leveraging%2520the%2520capacity%2520of%2520visual%2520embeddings%2520to%2520produce%2520robust%2520representations%2520that%2520capture%2520perceptual%2520invariances.%2520By%2520mapping%2520the%2520pattern%2520representations%2520onto%2520an%2520invariant%2520embedding%2520space%252C%2520we%2520can%2520effectively%2520recover%2520unknown%2520causal%2520parameters%2520without%2520the%2520need%2520for%2520handcrafted%2520objective%2520functions%2520or%2520heuristics.%2520We%2520evaluate%2520the%2520method%2520on%2520three%2520self-organizing%2520systems%253A%2520a%2520physical%252C%2520a%2520biological%252C%2520and%2520a%2520social%2520one%253B%2520namely%252C%2520a%2520reaction-diffusion%2520system%252C%2520a%2520model%2520of%2520embryonic%2520development%252C%2520and%2520an%2520agent-based%2520model%2520of%2520social%2520segregation.%2520We%2520show%2520that%2520the%2520method%2520reliably%2520recovers%2520parameters%2520despite%2520stochasticity%2520in%2520the%2520pattern%2520outcomes.%2520We%2520further%2520apply%2520the%2520method%2520to%2520real%2520biological%2520patterns%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520tool%2520for%2520both%2520theorists%2520and%2520experimentalists%2520to%2520investigate%2520the%2520dynamics%2520underlying%2520complex%2520stochastic%2520pattern%2520formation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Inverse%20Problems%20in%20Stochastic%20Self-Organizing%20Systems%20through%20Invariant%20Representations&entry.906535625=Elias%20Najarro%20and%20Nicolas%20Bessone%20and%20Sebastian%20Risi&entry.1292438233=Self-organizing%20systems%20demonstrate%20how%20simple%20local%20rules%20can%20generate%20complex%20stochastic%20patterns.%20Many%20natural%20systems%20rely%20on%20such%20dynamics%2C%20making%20self-organization%20central%20to%20understanding%20natural%20complexity.%20A%20fundamental%20challenge%20in%20modeling%20such%20systems%20is%20solving%20the%20inverse%20problem%3A%20finding%20the%20unknown%20causal%20parameters%20from%20macroscopic%20observations.%20This%20task%20becomes%20particularly%20difficult%20when%20observations%20have%20a%20strong%20stochastic%20component%2C%20yielding%20diverse%20yet%20equivalent%20patterns.%20Traditional%20inverse%20methods%20fail%20in%20this%20setting%2C%20as%20pixel-wise%20metrics%20cannot%20capture%20feature%20similarities%20between%20variable%20outcomes.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20inverse%20modeling%20method%20specifically%20designed%20to%20handle%20stochasticity%20in%20the%20observable%20space%2C%20leveraging%20the%20capacity%20of%20visual%20embeddings%20to%20produce%20robust%20representations%20that%20capture%20perceptual%20invariances.%20By%20mapping%20the%20pattern%20representations%20onto%20an%20invariant%20embedding%20space%2C%20we%20can%20effectively%20recover%20unknown%20causal%20parameters%20without%20the%20need%20for%20handcrafted%20objective%20functions%20or%20heuristics.%20We%20evaluate%20the%20method%20on%20three%20self-organizing%20systems%3A%20a%20physical%2C%20a%20biological%2C%20and%20a%20social%20one%3B%20namely%2C%20a%20reaction-diffusion%20system%2C%20a%20model%20of%20embryonic%20development%2C%20and%20an%20agent-based%20model%20of%20social%20segregation.%20We%20show%20that%20the%20method%20reliably%20recovers%20parameters%20despite%20stochasticity%20in%20the%20pattern%20outcomes.%20We%20further%20apply%20the%20method%20to%20real%20biological%20patterns%2C%20highlighting%20its%20potential%20as%20a%20tool%20for%20both%20theorists%20and%20experimentalists%20to%20investigate%20the%20dynamics%20underlying%20complex%20stochastic%20pattern%20formation.&entry.1838667208=http%3A//arxiv.org/abs/2506.11796v2&entry.124074799=Read"},
{"title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "author": "Qiguang Chen and Yantao Du and Ziniu Li and Jinhao Liu and Songyao Duan and Jiarui Guo and Minghao Liu and Jiaheng Liu and Tong Yang and Ge Zhang and Libo Qin and Wanxiang Che and Wenhao Huang", "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "link": "http://arxiv.org/abs/2601.06002v1", "date": "2026-01-09", "relevancy": 2.0724, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Molecular%20Structure%20of%20Thought%3A%20Mapping%20the%20Topology%20of%20Long%20Chain-of-Thought%20Reasoning&body=Title%3A%20The%20Molecular%20Structure%20of%20Thought%3A%20Mapping%20the%20Topology%20of%20Long%20Chain-of-Thought%20Reasoning%0AAuthor%3A%20Qiguang%20Chen%20and%20Yantao%20Du%20and%20Ziniu%20Li%20and%20Jinhao%20Liu%20and%20Songyao%20Duan%20and%20Jiarui%20Guo%20and%20Minghao%20Liu%20and%20Jiaheng%20Liu%20and%20Tong%20Yang%20and%20Ge%20Zhang%20and%20Libo%20Qin%20and%20Wanxiang%20Che%20and%20Wenhao%20Huang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20often%20fail%20to%20learn%20effective%20long%20chain-of-thought%20%28Long%20CoT%29%20reasoning%20from%20human%20or%20non-Long-CoT%20LLMs%20imitation.%20To%20understand%20this%2C%20we%20propose%20that%20effective%20and%20learnable%20Long%20CoT%20trajectories%20feature%20stable%20molecular-like%20structures%20in%20unified%20view%2C%20which%20are%20formed%20by%20three%20interaction%20types%3A%20Deep-Reasoning%20%28covalent-like%29%2C%20Self-Reflection%20%28hydrogen-bond-like%29%2C%20and%20Self-Exploration%20%28van%20der%20Waals-like%29.%20Analysis%20of%20distilled%20trajectories%20reveals%20these%20structures%20emerge%20from%20Long%20CoT%20fine-tuning%2C%20not%20keyword%20imitation.%20We%20introduce%20Effective%20Semantic%20Isomers%20and%20show%20that%20only%20bonds%20promoting%20fast%20entropy%20convergence%20support%20stable%20Long%20CoT%20learning%2C%20while%20structural%20competition%20impairs%20training.%20Drawing%20on%20these%20findings%2C%20we%20present%20Mole-Syn%2C%20a%20distribution-transfer-graph%20method%20that%20guides%20synthesis%20of%20effective%20Long%20CoT%20structures%2C%20boosting%20performance%20and%20RL%20stability%20across%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Molecular%2520Structure%2520of%2520Thought%253A%2520Mapping%2520the%2520Topology%2520of%2520Long%2520Chain-of-Thought%2520Reasoning%26entry.906535625%3DQiguang%2520Chen%2520and%2520Yantao%2520Du%2520and%2520Ziniu%2520Li%2520and%2520Jinhao%2520Liu%2520and%2520Songyao%2520Duan%2520and%2520Jiarui%2520Guo%2520and%2520Minghao%2520Liu%2520and%2520Jiaheng%2520Liu%2520and%2520Tong%2520Yang%2520and%2520Ge%2520Zhang%2520and%2520Libo%2520Qin%2520and%2520Wanxiang%2520Che%2520and%2520Wenhao%2520Huang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520often%2520fail%2520to%2520learn%2520effective%2520long%2520chain-of-thought%2520%2528Long%2520CoT%2529%2520reasoning%2520from%2520human%2520or%2520non-Long-CoT%2520LLMs%2520imitation.%2520To%2520understand%2520this%252C%2520we%2520propose%2520that%2520effective%2520and%2520learnable%2520Long%2520CoT%2520trajectories%2520feature%2520stable%2520molecular-like%2520structures%2520in%2520unified%2520view%252C%2520which%2520are%2520formed%2520by%2520three%2520interaction%2520types%253A%2520Deep-Reasoning%2520%2528covalent-like%2529%252C%2520Self-Reflection%2520%2528hydrogen-bond-like%2529%252C%2520and%2520Self-Exploration%2520%2528van%2520der%2520Waals-like%2529.%2520Analysis%2520of%2520distilled%2520trajectories%2520reveals%2520these%2520structures%2520emerge%2520from%2520Long%2520CoT%2520fine-tuning%252C%2520not%2520keyword%2520imitation.%2520We%2520introduce%2520Effective%2520Semantic%2520Isomers%2520and%2520show%2520that%2520only%2520bonds%2520promoting%2520fast%2520entropy%2520convergence%2520support%2520stable%2520Long%2520CoT%2520learning%252C%2520while%2520structural%2520competition%2520impairs%2520training.%2520Drawing%2520on%2520these%2520findings%252C%2520we%2520present%2520Mole-Syn%252C%2520a%2520distribution-transfer-graph%2520method%2520that%2520guides%2520synthesis%2520of%2520effective%2520Long%2520CoT%2520structures%252C%2520boosting%2520performance%2520and%2520RL%2520stability%2520across%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Molecular%20Structure%20of%20Thought%3A%20Mapping%20the%20Topology%20of%20Long%20Chain-of-Thought%20Reasoning&entry.906535625=Qiguang%20Chen%20and%20Yantao%20Du%20and%20Ziniu%20Li%20and%20Jinhao%20Liu%20and%20Songyao%20Duan%20and%20Jiarui%20Guo%20and%20Minghao%20Liu%20and%20Jiaheng%20Liu%20and%20Tong%20Yang%20and%20Ge%20Zhang%20and%20Libo%20Qin%20and%20Wanxiang%20Che%20and%20Wenhao%20Huang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20often%20fail%20to%20learn%20effective%20long%20chain-of-thought%20%28Long%20CoT%29%20reasoning%20from%20human%20or%20non-Long-CoT%20LLMs%20imitation.%20To%20understand%20this%2C%20we%20propose%20that%20effective%20and%20learnable%20Long%20CoT%20trajectories%20feature%20stable%20molecular-like%20structures%20in%20unified%20view%2C%20which%20are%20formed%20by%20three%20interaction%20types%3A%20Deep-Reasoning%20%28covalent-like%29%2C%20Self-Reflection%20%28hydrogen-bond-like%29%2C%20and%20Self-Exploration%20%28van%20der%20Waals-like%29.%20Analysis%20of%20distilled%20trajectories%20reveals%20these%20structures%20emerge%20from%20Long%20CoT%20fine-tuning%2C%20not%20keyword%20imitation.%20We%20introduce%20Effective%20Semantic%20Isomers%20and%20show%20that%20only%20bonds%20promoting%20fast%20entropy%20convergence%20support%20stable%20Long%20CoT%20learning%2C%20while%20structural%20competition%20impairs%20training.%20Drawing%20on%20these%20findings%2C%20we%20present%20Mole-Syn%2C%20a%20distribution-transfer-graph%20method%20that%20guides%20synthesis%20of%20effective%20Long%20CoT%20structures%2C%20boosting%20performance%20and%20RL%20stability%20across%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.06002v1&entry.124074799=Read"},
{"title": "SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More", "author": "Muye Huang and Lingling Zhang and Yifei Li and Yaqiang Wu and Jun Liu", "abstract": "Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.", "link": "http://arxiv.org/abs/2601.05688v1", "date": "2026-01-09", "relevancy": 2.068, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5206}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SketchVL%3A%20Policy%20Optimization%20via%20Fine-Grained%20Credit%20Assignment%20for%20Chart%20Understanding%20and%20More&body=Title%3A%20SketchVL%3A%20Policy%20Optimization%20via%20Fine-Grained%20Credit%20Assignment%20for%20Chart%20Understanding%20and%20More%0AAuthor%3A%20Muye%20Huang%20and%20Lingling%20Zhang%20and%20Yifei%20Li%20and%20Yaqiang%20Wu%20and%20Jun%20Liu%0AAbstract%3A%20Charts%20are%20high-density%20visual%20carriers%20of%20complex%20data%20and%20medium%20for%20information%20extraction%20and%20analysis.%20Due%20to%20the%20need%20for%20precise%20and%20complex%20visual%20reasoning%2C%20automated%20chart%20understanding%20poses%20a%20significant%20challenge%20to%20existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Many%20MLLMs%20trained%20with%20reinforcement%20learning%20%28RL%29%20face%20the%20challenge%20of%20credit%20assignment.%20Their%20advantage%20estimation%2C%20typically%20performed%20at%20the%20trajectory%20level%2C%20cannot%20distinguish%20between%20correct%20and%20incorrect%20reasoning%20steps%20within%20a%20single%20generated%20response.%20To%20address%20this%20limitation%2C%20we%20introduce%20SketchVL%2C%20a%20novel%20MLLM%20that%20optimized%20with%20FinePO%2C%20a%20new%20RL%20algorithm%20designed%20for%20fine-grained%20credit%20assignment%20within%20each%20trajectory.%20SketchVL%27s%20methodology%20involves%20drawing%20its%20intermediate%20reasoning%20steps%20as%20markers%20on%20the%20image%20and%20feeding%20the%20annotated%20image%20back%20to%20itself%2C%20creating%20a%20robust%2C%20multi-step%20reasoning%20process.%20During%20training%2C%20the%20FinePO%20algorithm%20leverages%20a%20Fine-grained%20Process%20Reward%20Model%20%28FinePRM%29%20to%20score%20each%20drawing%20action%20within%20a%20trajectory%2C%20thereby%20precisely%20assigning%20credit%20for%20each%20step.%20This%20mechanism%20allows%20FinePO%20to%20more%20strongly%20reward%20correct%20tokens%20when%20a%20trajectory%20is%20globally%20successful%2C%20and%20more%20heavily%20penalize%20incorrect%20tokens%20when%20the%20trajectory%20is%20globally%20suboptimal%2C%20thus%20achieving%20fine-grained%20reinforcement%20signals.%20Experiments%20show%20that%20SketchVL%20learns%20to%20align%20its%20step-level%20behavior%20with%20the%20FinePRM%2C%20achieving%20an%20average%20performance%20gain%20of%207.23%5C%25%20over%20its%20base%20model%20across%20chart%20datasets%2C%20natural%20image%20datasets%2C%20and%20mathematics%2C%20providing%20a%20promising%20new%20direction%20for%20training%20powerful%20reasoning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketchVL%253A%2520Policy%2520Optimization%2520via%2520Fine-Grained%2520Credit%2520Assignment%2520for%2520Chart%2520Understanding%2520and%2520More%26entry.906535625%3DMuye%2520Huang%2520and%2520Lingling%2520Zhang%2520and%2520Yifei%2520Li%2520and%2520Yaqiang%2520Wu%2520and%2520Jun%2520Liu%26entry.1292438233%3DCharts%2520are%2520high-density%2520visual%2520carriers%2520of%2520complex%2520data%2520and%2520medium%2520for%2520information%2520extraction%2520and%2520analysis.%2520Due%2520to%2520the%2520need%2520for%2520precise%2520and%2520complex%2520visual%2520reasoning%252C%2520automated%2520chart%2520understanding%2520poses%2520a%2520significant%2520challenge%2520to%2520existing%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Many%2520MLLMs%2520trained%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520face%2520the%2520challenge%2520of%2520credit%2520assignment.%2520Their%2520advantage%2520estimation%252C%2520typically%2520performed%2520at%2520the%2520trajectory%2520level%252C%2520cannot%2520distinguish%2520between%2520correct%2520and%2520incorrect%2520reasoning%2520steps%2520within%2520a%2520single%2520generated%2520response.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520SketchVL%252C%2520a%2520novel%2520MLLM%2520that%2520optimized%2520with%2520FinePO%252C%2520a%2520new%2520RL%2520algorithm%2520designed%2520for%2520fine-grained%2520credit%2520assignment%2520within%2520each%2520trajectory.%2520SketchVL%2527s%2520methodology%2520involves%2520drawing%2520its%2520intermediate%2520reasoning%2520steps%2520as%2520markers%2520on%2520the%2520image%2520and%2520feeding%2520the%2520annotated%2520image%2520back%2520to%2520itself%252C%2520creating%2520a%2520robust%252C%2520multi-step%2520reasoning%2520process.%2520During%2520training%252C%2520the%2520FinePO%2520algorithm%2520leverages%2520a%2520Fine-grained%2520Process%2520Reward%2520Model%2520%2528FinePRM%2529%2520to%2520score%2520each%2520drawing%2520action%2520within%2520a%2520trajectory%252C%2520thereby%2520precisely%2520assigning%2520credit%2520for%2520each%2520step.%2520This%2520mechanism%2520allows%2520FinePO%2520to%2520more%2520strongly%2520reward%2520correct%2520tokens%2520when%2520a%2520trajectory%2520is%2520globally%2520successful%252C%2520and%2520more%2520heavily%2520penalize%2520incorrect%2520tokens%2520when%2520the%2520trajectory%2520is%2520globally%2520suboptimal%252C%2520thus%2520achieving%2520fine-grained%2520reinforcement%2520signals.%2520Experiments%2520show%2520that%2520SketchVL%2520learns%2520to%2520align%2520its%2520step-level%2520behavior%2520with%2520the%2520FinePRM%252C%2520achieving%2520an%2520average%2520performance%2520gain%2520of%25207.23%255C%2525%2520over%2520its%2520base%2520model%2520across%2520chart%2520datasets%252C%2520natural%2520image%2520datasets%252C%2520and%2520mathematics%252C%2520providing%2520a%2520promising%2520new%2520direction%2520for%2520training%2520powerful%2520reasoning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SketchVL%3A%20Policy%20Optimization%20via%20Fine-Grained%20Credit%20Assignment%20for%20Chart%20Understanding%20and%20More&entry.906535625=Muye%20Huang%20and%20Lingling%20Zhang%20and%20Yifei%20Li%20and%20Yaqiang%20Wu%20and%20Jun%20Liu&entry.1292438233=Charts%20are%20high-density%20visual%20carriers%20of%20complex%20data%20and%20medium%20for%20information%20extraction%20and%20analysis.%20Due%20to%20the%20need%20for%20precise%20and%20complex%20visual%20reasoning%2C%20automated%20chart%20understanding%20poses%20a%20significant%20challenge%20to%20existing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Many%20MLLMs%20trained%20with%20reinforcement%20learning%20%28RL%29%20face%20the%20challenge%20of%20credit%20assignment.%20Their%20advantage%20estimation%2C%20typically%20performed%20at%20the%20trajectory%20level%2C%20cannot%20distinguish%20between%20correct%20and%20incorrect%20reasoning%20steps%20within%20a%20single%20generated%20response.%20To%20address%20this%20limitation%2C%20we%20introduce%20SketchVL%2C%20a%20novel%20MLLM%20that%20optimized%20with%20FinePO%2C%20a%20new%20RL%20algorithm%20designed%20for%20fine-grained%20credit%20assignment%20within%20each%20trajectory.%20SketchVL%27s%20methodology%20involves%20drawing%20its%20intermediate%20reasoning%20steps%20as%20markers%20on%20the%20image%20and%20feeding%20the%20annotated%20image%20back%20to%20itself%2C%20creating%20a%20robust%2C%20multi-step%20reasoning%20process.%20During%20training%2C%20the%20FinePO%20algorithm%20leverages%20a%20Fine-grained%20Process%20Reward%20Model%20%28FinePRM%29%20to%20score%20each%20drawing%20action%20within%20a%20trajectory%2C%20thereby%20precisely%20assigning%20credit%20for%20each%20step.%20This%20mechanism%20allows%20FinePO%20to%20more%20strongly%20reward%20correct%20tokens%20when%20a%20trajectory%20is%20globally%20successful%2C%20and%20more%20heavily%20penalize%20incorrect%20tokens%20when%20the%20trajectory%20is%20globally%20suboptimal%2C%20thus%20achieving%20fine-grained%20reinforcement%20signals.%20Experiments%20show%20that%20SketchVL%20learns%20to%20align%20its%20step-level%20behavior%20with%20the%20FinePRM%2C%20achieving%20an%20average%20performance%20gain%20of%207.23%5C%25%20over%20its%20base%20model%20across%20chart%20datasets%2C%20natural%20image%20datasets%2C%20and%20mathematics%2C%20providing%20a%20promising%20new%20direction%20for%20training%20powerful%20reasoning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.05688v1&entry.124074799=Read"},
{"title": "Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation", "author": "Yinsong Wang and Xinzhe Luo and Siyi Du and Chen Qin", "abstract": "Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.", "link": "http://arxiv.org/abs/2601.05981v1", "date": "2026-01-09", "relevancy": 2.0591, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.52}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5124}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Conditional%20Contrast-Agnostic%20Deformable%20Image%20Registration%20with%20Uncertainty%20Estimation&body=Title%3A%20Adaptive%20Conditional%20Contrast-Agnostic%20Deformable%20Image%20Registration%20with%20Uncertainty%20Estimation%0AAuthor%3A%20Yinsong%20Wang%20and%20Xinzhe%20Luo%20and%20Siyi%20Du%20and%20Chen%20Qin%0AAbstract%3A%20Deformable%20multi-contrast%20image%20registration%20is%20a%20challenging%20yet%20crucial%20task%20due%20to%20the%20complex%2C%20non-linear%20intensity%20relationships%20across%20different%20imaging%20contrasts.%20Conventional%20registration%20methods%20typically%20rely%20on%20iterative%20optimization%20of%20the%20deformation%20field%2C%20which%20is%20time-consuming.%20Although%20recent%20learning-based%20approaches%20enable%20fast%20and%20accurate%20registration%20during%20inference%2C%20their%20generalizability%20remains%20limited%20to%20the%20specific%20contrasts%20observed%20during%20training.%20In%20this%20work%2C%20we%20propose%20an%20adaptive%20conditional%20contrast-agnostic%20deformable%20image%20registration%20framework%20%28AC-CAR%29%20based%20on%20a%20random%20convolution-based%20contrast%20augmentation%20scheme.%20AC-CAR%20can%20generalize%20to%20arbitrary%20imaging%20contrasts%20without%20observing%20them%20during%20training.%20To%20encourage%20contrast-invariant%20feature%20learning%2C%20we%20propose%20an%20adaptive%20conditional%20feature%20modulator%20%28ACFM%29%20that%20adaptively%20modulates%20the%20features%20and%20the%20contrast-invariant%20latent%20regularization%20to%20enforce%20the%20consistency%20of%20the%20learned%20feature%20across%20different%20imaging%20contrasts.%20Additionally%2C%20we%20enable%20our%20framework%20to%20provide%20contrast-agnostic%20registration%20uncertainty%20by%20integrating%20a%20variance%20network%20that%20leverages%20the%20contrast-agnostic%20registration%20encoder%20to%20improve%20the%20trustworthiness%20and%20reliability%20of%20AC-CAR.%20Experimental%20results%20demonstrate%20that%20AC-CAR%20outperforms%20baseline%20methods%20in%20registration%20accuracy%20and%20exhibits%20superior%20generalization%20to%20unseen%20imaging%20contrasts.%20Code%20is%20available%20at%20https%3A//github.com/Yinsong0510/AC-CAR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Conditional%2520Contrast-Agnostic%2520Deformable%2520Image%2520Registration%2520with%2520Uncertainty%2520Estimation%26entry.906535625%3DYinsong%2520Wang%2520and%2520Xinzhe%2520Luo%2520and%2520Siyi%2520Du%2520and%2520Chen%2520Qin%26entry.1292438233%3DDeformable%2520multi-contrast%2520image%2520registration%2520is%2520a%2520challenging%2520yet%2520crucial%2520task%2520due%2520to%2520the%2520complex%252C%2520non-linear%2520intensity%2520relationships%2520across%2520different%2520imaging%2520contrasts.%2520Conventional%2520registration%2520methods%2520typically%2520rely%2520on%2520iterative%2520optimization%2520of%2520the%2520deformation%2520field%252C%2520which%2520is%2520time-consuming.%2520Although%2520recent%2520learning-based%2520approaches%2520enable%2520fast%2520and%2520accurate%2520registration%2520during%2520inference%252C%2520their%2520generalizability%2520remains%2520limited%2520to%2520the%2520specific%2520contrasts%2520observed%2520during%2520training.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520adaptive%2520conditional%2520contrast-agnostic%2520deformable%2520image%2520registration%2520framework%2520%2528AC-CAR%2529%2520based%2520on%2520a%2520random%2520convolution-based%2520contrast%2520augmentation%2520scheme.%2520AC-CAR%2520can%2520generalize%2520to%2520arbitrary%2520imaging%2520contrasts%2520without%2520observing%2520them%2520during%2520training.%2520To%2520encourage%2520contrast-invariant%2520feature%2520learning%252C%2520we%2520propose%2520an%2520adaptive%2520conditional%2520feature%2520modulator%2520%2528ACFM%2529%2520that%2520adaptively%2520modulates%2520the%2520features%2520and%2520the%2520contrast-invariant%2520latent%2520regularization%2520to%2520enforce%2520the%2520consistency%2520of%2520the%2520learned%2520feature%2520across%2520different%2520imaging%2520contrasts.%2520Additionally%252C%2520we%2520enable%2520our%2520framework%2520to%2520provide%2520contrast-agnostic%2520registration%2520uncertainty%2520by%2520integrating%2520a%2520variance%2520network%2520that%2520leverages%2520the%2520contrast-agnostic%2520registration%2520encoder%2520to%2520improve%2520the%2520trustworthiness%2520and%2520reliability%2520of%2520AC-CAR.%2520Experimental%2520results%2520demonstrate%2520that%2520AC-CAR%2520outperforms%2520baseline%2520methods%2520in%2520registration%2520accuracy%2520and%2520exhibits%2520superior%2520generalization%2520to%2520unseen%2520imaging%2520contrasts.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Yinsong0510/AC-CAR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Conditional%20Contrast-Agnostic%20Deformable%20Image%20Registration%20with%20Uncertainty%20Estimation&entry.906535625=Yinsong%20Wang%20and%20Xinzhe%20Luo%20and%20Siyi%20Du%20and%20Chen%20Qin&entry.1292438233=Deformable%20multi-contrast%20image%20registration%20is%20a%20challenging%20yet%20crucial%20task%20due%20to%20the%20complex%2C%20non-linear%20intensity%20relationships%20across%20different%20imaging%20contrasts.%20Conventional%20registration%20methods%20typically%20rely%20on%20iterative%20optimization%20of%20the%20deformation%20field%2C%20which%20is%20time-consuming.%20Although%20recent%20learning-based%20approaches%20enable%20fast%20and%20accurate%20registration%20during%20inference%2C%20their%20generalizability%20remains%20limited%20to%20the%20specific%20contrasts%20observed%20during%20training.%20In%20this%20work%2C%20we%20propose%20an%20adaptive%20conditional%20contrast-agnostic%20deformable%20image%20registration%20framework%20%28AC-CAR%29%20based%20on%20a%20random%20convolution-based%20contrast%20augmentation%20scheme.%20AC-CAR%20can%20generalize%20to%20arbitrary%20imaging%20contrasts%20without%20observing%20them%20during%20training.%20To%20encourage%20contrast-invariant%20feature%20learning%2C%20we%20propose%20an%20adaptive%20conditional%20feature%20modulator%20%28ACFM%29%20that%20adaptively%20modulates%20the%20features%20and%20the%20contrast-invariant%20latent%20regularization%20to%20enforce%20the%20consistency%20of%20the%20learned%20feature%20across%20different%20imaging%20contrasts.%20Additionally%2C%20we%20enable%20our%20framework%20to%20provide%20contrast-agnostic%20registration%20uncertainty%20by%20integrating%20a%20variance%20network%20that%20leverages%20the%20contrast-agnostic%20registration%20encoder%20to%20improve%20the%20trustworthiness%20and%20reliability%20of%20AC-CAR.%20Experimental%20results%20demonstrate%20that%20AC-CAR%20outperforms%20baseline%20methods%20in%20registration%20accuracy%20and%20exhibits%20superior%20generalization%20to%20unseen%20imaging%20contrasts.%20Code%20is%20available%20at%20https%3A//github.com/Yinsong0510/AC-CAR.&entry.1838667208=http%3A//arxiv.org/abs/2601.05981v1&entry.124074799=Read"},
{"title": "Machine learning for in-situ composition mapping in a self-driving magnetron sputtering system", "author": "Sanna Jarl and Jens Sj\u00f6lund and Robert J. W. Frost and Anders Holst and Jonathan J. S. Scragg", "abstract": "Self-driving labs (SDLs), employing automation and machine learning (ML) to accelerate experimental procedures, have enormous potential in the discovery of new materials. However, in thin film science, SDLs are mainly restricted to solution-based synthetic methods which are easier to automate but cannot access the broad chemical space of inorganic materials. This work presents an SDL based on magnetron co-sputtering. We are using combinatorial frameworks, obtaining accurate composition maps on multi-element, compositionally graded thin films. This normally requires time-consuming ex-situ analysis prone to systematic errors. We present a rapid and calibration-free in-situ, ML driven approach to produce composition maps for arbitrary source combinations and sputtering conditions. We develop a method to predict the composition distribution in a multi-element combinatorial thin film, using in-situ measurements from quartz-crystal microbalance sensors placed in a sputter chamber. For a given source, the sensor readings are learned as a function of the sputtering pressure and magnetron power, through active learning using Gaussian processes (GPs). The final GPs are combined with a geometric model of the deposition flux distribution in the chamber, which allows interpolation of the deposition rates from each source, at any position across the sample. We investigate several acquisition functions for the ML procedure. A fully Bayesian GP - BALM (Bayesian active learning MacKay) - achieved the best performance, learning the deposition rates for a single source in 10 experiments. Prediction accuracy for co-sputtering composition distributions was verified experimentally. Our framework dramatically increases throughput by avoiding the need for extensive characterisation or calibration, thus demonstrating the potential of ML-guided SDLs to accelerate materials exploration.", "link": "http://arxiv.org/abs/2506.05999v2", "date": "2026-01-09", "relevancy": 2.0528, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.51}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20learning%20for%20in-situ%20composition%20mapping%20in%20a%20self-driving%20magnetron%20sputtering%20system&body=Title%3A%20Machine%20learning%20for%20in-situ%20composition%20mapping%20in%20a%20self-driving%20magnetron%20sputtering%20system%0AAuthor%3A%20Sanna%20Jarl%20and%20Jens%20Sj%C3%B6lund%20and%20Robert%20J.%20W.%20Frost%20and%20Anders%20Holst%20and%20Jonathan%20J.%20S.%20Scragg%0AAbstract%3A%20Self-driving%20labs%20%28SDLs%29%2C%20employing%20automation%20and%20machine%20learning%20%28ML%29%20to%20accelerate%20experimental%20procedures%2C%20have%20enormous%20potential%20in%20the%20discovery%20of%20new%20materials.%20However%2C%20in%20thin%20film%20science%2C%20SDLs%20are%20mainly%20restricted%20to%20solution-based%20synthetic%20methods%20which%20are%20easier%20to%20automate%20but%20cannot%20access%20the%20broad%20chemical%20space%20of%20inorganic%20materials.%20This%20work%20presents%20an%20SDL%20based%20on%20magnetron%20co-sputtering.%20We%20are%20using%20combinatorial%20frameworks%2C%20obtaining%20accurate%20composition%20maps%20on%20multi-element%2C%20compositionally%20graded%20thin%20films.%20This%20normally%20requires%20time-consuming%20ex-situ%20analysis%20prone%20to%20systematic%20errors.%20We%20present%20a%20rapid%20and%20calibration-free%20in-situ%2C%20ML%20driven%20approach%20to%20produce%20composition%20maps%20for%20arbitrary%20source%20combinations%20and%20sputtering%20conditions.%20We%20develop%20a%20method%20to%20predict%20the%20composition%20distribution%20in%20a%20multi-element%20combinatorial%20thin%20film%2C%20using%20in-situ%20measurements%20from%20quartz-crystal%20microbalance%20sensors%20placed%20in%20a%20sputter%20chamber.%20For%20a%20given%20source%2C%20the%20sensor%20readings%20are%20learned%20as%20a%20function%20of%20the%20sputtering%20pressure%20and%20magnetron%20power%2C%20through%20active%20learning%20using%20Gaussian%20processes%20%28GPs%29.%20The%20final%20GPs%20are%20combined%20with%20a%20geometric%20model%20of%20the%20deposition%20flux%20distribution%20in%20the%20chamber%2C%20which%20allows%20interpolation%20of%20the%20deposition%20rates%20from%20each%20source%2C%20at%20any%20position%20across%20the%20sample.%20We%20investigate%20several%20acquisition%20functions%20for%20the%20ML%20procedure.%20A%20fully%20Bayesian%20GP%20-%20BALM%20%28Bayesian%20active%20learning%20MacKay%29%20-%20achieved%20the%20best%20performance%2C%20learning%20the%20deposition%20rates%20for%20a%20single%20source%20in%2010%20experiments.%20Prediction%20accuracy%20for%20co-sputtering%20composition%20distributions%20was%20verified%20experimentally.%20Our%20framework%20dramatically%20increases%20throughput%20by%20avoiding%20the%20need%20for%20extensive%20characterisation%20or%20calibration%2C%20thus%20demonstrating%20the%20potential%20of%20ML-guided%20SDLs%20to%20accelerate%20materials%20exploration.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520learning%2520for%2520in-situ%2520composition%2520mapping%2520in%2520a%2520self-driving%2520magnetron%2520sputtering%2520system%26entry.906535625%3DSanna%2520Jarl%2520and%2520Jens%2520Sj%25C3%25B6lund%2520and%2520Robert%2520J.%2520W.%2520Frost%2520and%2520Anders%2520Holst%2520and%2520Jonathan%2520J.%2520S.%2520Scragg%26entry.1292438233%3DSelf-driving%2520labs%2520%2528SDLs%2529%252C%2520employing%2520automation%2520and%2520machine%2520learning%2520%2528ML%2529%2520to%2520accelerate%2520experimental%2520procedures%252C%2520have%2520enormous%2520potential%2520in%2520the%2520discovery%2520of%2520new%2520materials.%2520However%252C%2520in%2520thin%2520film%2520science%252C%2520SDLs%2520are%2520mainly%2520restricted%2520to%2520solution-based%2520synthetic%2520methods%2520which%2520are%2520easier%2520to%2520automate%2520but%2520cannot%2520access%2520the%2520broad%2520chemical%2520space%2520of%2520inorganic%2520materials.%2520This%2520work%2520presents%2520an%2520SDL%2520based%2520on%2520magnetron%2520co-sputtering.%2520We%2520are%2520using%2520combinatorial%2520frameworks%252C%2520obtaining%2520accurate%2520composition%2520maps%2520on%2520multi-element%252C%2520compositionally%2520graded%2520thin%2520films.%2520This%2520normally%2520requires%2520time-consuming%2520ex-situ%2520analysis%2520prone%2520to%2520systematic%2520errors.%2520We%2520present%2520a%2520rapid%2520and%2520calibration-free%2520in-situ%252C%2520ML%2520driven%2520approach%2520to%2520produce%2520composition%2520maps%2520for%2520arbitrary%2520source%2520combinations%2520and%2520sputtering%2520conditions.%2520We%2520develop%2520a%2520method%2520to%2520predict%2520the%2520composition%2520distribution%2520in%2520a%2520multi-element%2520combinatorial%2520thin%2520film%252C%2520using%2520in-situ%2520measurements%2520from%2520quartz-crystal%2520microbalance%2520sensors%2520placed%2520in%2520a%2520sputter%2520chamber.%2520For%2520a%2520given%2520source%252C%2520the%2520sensor%2520readings%2520are%2520learned%2520as%2520a%2520function%2520of%2520the%2520sputtering%2520pressure%2520and%2520magnetron%2520power%252C%2520through%2520active%2520learning%2520using%2520Gaussian%2520processes%2520%2528GPs%2529.%2520The%2520final%2520GPs%2520are%2520combined%2520with%2520a%2520geometric%2520model%2520of%2520the%2520deposition%2520flux%2520distribution%2520in%2520the%2520chamber%252C%2520which%2520allows%2520interpolation%2520of%2520the%2520deposition%2520rates%2520from%2520each%2520source%252C%2520at%2520any%2520position%2520across%2520the%2520sample.%2520We%2520investigate%2520several%2520acquisition%2520functions%2520for%2520the%2520ML%2520procedure.%2520A%2520fully%2520Bayesian%2520GP%2520-%2520BALM%2520%2528Bayesian%2520active%2520learning%2520MacKay%2529%2520-%2520achieved%2520the%2520best%2520performance%252C%2520learning%2520the%2520deposition%2520rates%2520for%2520a%2520single%2520source%2520in%252010%2520experiments.%2520Prediction%2520accuracy%2520for%2520co-sputtering%2520composition%2520distributions%2520was%2520verified%2520experimentally.%2520Our%2520framework%2520dramatically%2520increases%2520throughput%2520by%2520avoiding%2520the%2520need%2520for%2520extensive%2520characterisation%2520or%2520calibration%252C%2520thus%2520demonstrating%2520the%2520potential%2520of%2520ML-guided%2520SDLs%2520to%2520accelerate%2520materials%2520exploration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20learning%20for%20in-situ%20composition%20mapping%20in%20a%20self-driving%20magnetron%20sputtering%20system&entry.906535625=Sanna%20Jarl%20and%20Jens%20Sj%C3%B6lund%20and%20Robert%20J.%20W.%20Frost%20and%20Anders%20Holst%20and%20Jonathan%20J.%20S.%20Scragg&entry.1292438233=Self-driving%20labs%20%28SDLs%29%2C%20employing%20automation%20and%20machine%20learning%20%28ML%29%20to%20accelerate%20experimental%20procedures%2C%20have%20enormous%20potential%20in%20the%20discovery%20of%20new%20materials.%20However%2C%20in%20thin%20film%20science%2C%20SDLs%20are%20mainly%20restricted%20to%20solution-based%20synthetic%20methods%20which%20are%20easier%20to%20automate%20but%20cannot%20access%20the%20broad%20chemical%20space%20of%20inorganic%20materials.%20This%20work%20presents%20an%20SDL%20based%20on%20magnetron%20co-sputtering.%20We%20are%20using%20combinatorial%20frameworks%2C%20obtaining%20accurate%20composition%20maps%20on%20multi-element%2C%20compositionally%20graded%20thin%20films.%20This%20normally%20requires%20time-consuming%20ex-situ%20analysis%20prone%20to%20systematic%20errors.%20We%20present%20a%20rapid%20and%20calibration-free%20in-situ%2C%20ML%20driven%20approach%20to%20produce%20composition%20maps%20for%20arbitrary%20source%20combinations%20and%20sputtering%20conditions.%20We%20develop%20a%20method%20to%20predict%20the%20composition%20distribution%20in%20a%20multi-element%20combinatorial%20thin%20film%2C%20using%20in-situ%20measurements%20from%20quartz-crystal%20microbalance%20sensors%20placed%20in%20a%20sputter%20chamber.%20For%20a%20given%20source%2C%20the%20sensor%20readings%20are%20learned%20as%20a%20function%20of%20the%20sputtering%20pressure%20and%20magnetron%20power%2C%20through%20active%20learning%20using%20Gaussian%20processes%20%28GPs%29.%20The%20final%20GPs%20are%20combined%20with%20a%20geometric%20model%20of%20the%20deposition%20flux%20distribution%20in%20the%20chamber%2C%20which%20allows%20interpolation%20of%20the%20deposition%20rates%20from%20each%20source%2C%20at%20any%20position%20across%20the%20sample.%20We%20investigate%20several%20acquisition%20functions%20for%20the%20ML%20procedure.%20A%20fully%20Bayesian%20GP%20-%20BALM%20%28Bayesian%20active%20learning%20MacKay%29%20-%20achieved%20the%20best%20performance%2C%20learning%20the%20deposition%20rates%20for%20a%20single%20source%20in%2010%20experiments.%20Prediction%20accuracy%20for%20co-sputtering%20composition%20distributions%20was%20verified%20experimentally.%20Our%20framework%20dramatically%20increases%20throughput%20by%20avoiding%20the%20need%20for%20extensive%20characterisation%20or%20calibration%2C%20thus%20demonstrating%20the%20potential%20of%20ML-guided%20SDLs%20to%20accelerate%20materials%20exploration.&entry.1838667208=http%3A//arxiv.org/abs/2506.05999v2&entry.124074799=Read"},
{"title": "AtomThink: Multimodal Slow Thinking with Atomic Step Reasoning", "author": "Kun Xiang and Zhili Liu and Terry Jingchen Zhang and Yinya Huang and Yunshuang Nie and Kaixin Cai and Yiyang Yin and Runhui Huang and Hanhui Li and Yihan Zeng and Yu-Jie Yuan and Jianhua Han and Lanqing Hong and Hang Xu and Xiaodan Liang", "abstract": "In this paper, we address the challenging task of multimodal reasoning by incorporating the notion of ``slow thinking'' into multimodal large language models (MLLMs). Our core idea is that models can learn to adaptively use different levels of reasoning to tackle questions of varying complexity. We propose a novel paradigm of Self-structured Chain of Thought (SCoT), which consists of minimal semantic atomic steps. Unlike existing methods that rely on structured templates or free-form paradigms, our method not only generates flexible CoT structures for various complex tasks but also mitigates the phenomenon of overthinking for easier tasks. To introduce structured reasoning into visual cognition, we design a novel AtomThink framework with four key modules: (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning (SFT) process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single-step utilization rate. Extensive experiments demonstrate that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10\\% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 $\\times$ and boosts inference efficiency by 85.3\\%. Our code is publicly available at https://github.com/Kun-Xiang/AtomThink.", "link": "http://arxiv.org/abs/2411.11930v5", "date": "2026-01-09", "relevancy": 2.0486, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5147}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AtomThink%3A%20Multimodal%20Slow%20Thinking%20with%20Atomic%20Step%20Reasoning&body=Title%3A%20AtomThink%3A%20Multimodal%20Slow%20Thinking%20with%20Atomic%20Step%20Reasoning%0AAuthor%3A%20Kun%20Xiang%20and%20Zhili%20Liu%20and%20Terry%20Jingchen%20Zhang%20and%20Yinya%20Huang%20and%20Yunshuang%20Nie%20and%20Kaixin%20Cai%20and%20Yiyang%20Yin%20and%20Runhui%20Huang%20and%20Hanhui%20Li%20and%20Yihan%20Zeng%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Lanqing%20Hong%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20In%20this%20paper%2C%20we%20address%20the%20challenging%20task%20of%20multimodal%20reasoning%20by%20incorporating%20the%20notion%20of%20%60%60slow%20thinking%27%27%20into%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20core%20idea%20is%20that%20models%20can%20learn%20to%20adaptively%20use%20different%20levels%20of%20reasoning%20to%20tackle%20questions%20of%20varying%20complexity.%20We%20propose%20a%20novel%20paradigm%20of%20Self-structured%20Chain%20of%20Thought%20%28SCoT%29%2C%20which%20consists%20of%20minimal%20semantic%20atomic%20steps.%20Unlike%20existing%20methods%20that%20rely%20on%20structured%20templates%20or%20free-form%20paradigms%2C%20our%20method%20not%20only%20generates%20flexible%20CoT%20structures%20for%20various%20complex%20tasks%20but%20also%20mitigates%20the%20phenomenon%20of%20overthinking%20for%20easier%20tasks.%20To%20introduce%20structured%20reasoning%20into%20visual%20cognition%2C%20we%20design%20a%20novel%20AtomThink%20framework%20with%20four%20key%20modules%3A%20%28i%29%20a%20data%20engine%20to%20generate%20high-quality%20multimodal%20reasoning%20paths%3B%20%28ii%29%20a%20supervised%20fine-tuning%20%28SFT%29%20process%20with%20serialized%20inference%20data%3B%20%28iii%29%20a%20policy-guided%20multi-turn%20inference%20method%3B%20and%20%28iv%29%20an%20atomic%20capability%20metric%20to%20evaluate%20the%20single-step%20utilization%20rate.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20AtomThink%20significantly%20improves%20the%20performance%20of%20baseline%20MLLMs%2C%20achieving%20more%20than%2010%5C%25%20average%20accuracy%20gains%20on%20MathVista%20and%20MathVerse.%20Compared%20to%20state-of-the-art%20structured%20CoT%20approaches%2C%20our%20method%20not%20only%20achieves%20higher%20accuracy%20but%20also%20improves%20data%20utilization%20by%205%20%24%5Ctimes%24%20and%20boosts%20inference%20efficiency%20by%2085.3%5C%25.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Kun-Xiang/AtomThink.%0ALink%3A%20http%3A//arxiv.org/abs/2411.11930v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtomThink%253A%2520Multimodal%2520Slow%2520Thinking%2520with%2520Atomic%2520Step%2520Reasoning%26entry.906535625%3DKun%2520Xiang%2520and%2520Zhili%2520Liu%2520and%2520Terry%2520Jingchen%2520Zhang%2520and%2520Yinya%2520Huang%2520and%2520Yunshuang%2520Nie%2520and%2520Kaixin%2520Cai%2520and%2520Yiyang%2520Yin%2520and%2520Runhui%2520Huang%2520and%2520Hanhui%2520Li%2520and%2520Yihan%2520Zeng%2520and%2520Yu-Jie%2520Yuan%2520and%2520Jianhua%2520Han%2520and%2520Lanqing%2520Hong%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520address%2520the%2520challenging%2520task%2520of%2520multimodal%2520reasoning%2520by%2520incorporating%2520the%2520notion%2520of%2520%2560%2560slow%2520thinking%2527%2527%2520into%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Our%2520core%2520idea%2520is%2520that%2520models%2520can%2520learn%2520to%2520adaptively%2520use%2520different%2520levels%2520of%2520reasoning%2520to%2520tackle%2520questions%2520of%2520varying%2520complexity.%2520We%2520propose%2520a%2520novel%2520paradigm%2520of%2520Self-structured%2520Chain%2520of%2520Thought%2520%2528SCoT%2529%252C%2520which%2520consists%2520of%2520minimal%2520semantic%2520atomic%2520steps.%2520Unlike%2520existing%2520methods%2520that%2520rely%2520on%2520structured%2520templates%2520or%2520free-form%2520paradigms%252C%2520our%2520method%2520not%2520only%2520generates%2520flexible%2520CoT%2520structures%2520for%2520various%2520complex%2520tasks%2520but%2520also%2520mitigates%2520the%2520phenomenon%2520of%2520overthinking%2520for%2520easier%2520tasks.%2520To%2520introduce%2520structured%2520reasoning%2520into%2520visual%2520cognition%252C%2520we%2520design%2520a%2520novel%2520AtomThink%2520framework%2520with%2520four%2520key%2520modules%253A%2520%2528i%2529%2520a%2520data%2520engine%2520to%2520generate%2520high-quality%2520multimodal%2520reasoning%2520paths%253B%2520%2528ii%2529%2520a%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520process%2520with%2520serialized%2520inference%2520data%253B%2520%2528iii%2529%2520a%2520policy-guided%2520multi-turn%2520inference%2520method%253B%2520and%2520%2528iv%2529%2520an%2520atomic%2520capability%2520metric%2520to%2520evaluate%2520the%2520single-step%2520utilization%2520rate.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520AtomThink%2520significantly%2520improves%2520the%2520performance%2520of%2520baseline%2520MLLMs%252C%2520achieving%2520more%2520than%252010%255C%2525%2520average%2520accuracy%2520gains%2520on%2520MathVista%2520and%2520MathVerse.%2520Compared%2520to%2520state-of-the-art%2520structured%2520CoT%2520approaches%252C%2520our%2520method%2520not%2520only%2520achieves%2520higher%2520accuracy%2520but%2520also%2520improves%2520data%2520utilization%2520by%25205%2520%2524%255Ctimes%2524%2520and%2520boosts%2520inference%2520efficiency%2520by%252085.3%255C%2525.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Kun-Xiang/AtomThink.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11930v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AtomThink%3A%20Multimodal%20Slow%20Thinking%20with%20Atomic%20Step%20Reasoning&entry.906535625=Kun%20Xiang%20and%20Zhili%20Liu%20and%20Terry%20Jingchen%20Zhang%20and%20Yinya%20Huang%20and%20Yunshuang%20Nie%20and%20Kaixin%20Cai%20and%20Yiyang%20Yin%20and%20Runhui%20Huang%20and%20Hanhui%20Li%20and%20Yihan%20Zeng%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Lanqing%20Hong%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=In%20this%20paper%2C%20we%20address%20the%20challenging%20task%20of%20multimodal%20reasoning%20by%20incorporating%20the%20notion%20of%20%60%60slow%20thinking%27%27%20into%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20core%20idea%20is%20that%20models%20can%20learn%20to%20adaptively%20use%20different%20levels%20of%20reasoning%20to%20tackle%20questions%20of%20varying%20complexity.%20We%20propose%20a%20novel%20paradigm%20of%20Self-structured%20Chain%20of%20Thought%20%28SCoT%29%2C%20which%20consists%20of%20minimal%20semantic%20atomic%20steps.%20Unlike%20existing%20methods%20that%20rely%20on%20structured%20templates%20or%20free-form%20paradigms%2C%20our%20method%20not%20only%20generates%20flexible%20CoT%20structures%20for%20various%20complex%20tasks%20but%20also%20mitigates%20the%20phenomenon%20of%20overthinking%20for%20easier%20tasks.%20To%20introduce%20structured%20reasoning%20into%20visual%20cognition%2C%20we%20design%20a%20novel%20AtomThink%20framework%20with%20four%20key%20modules%3A%20%28i%29%20a%20data%20engine%20to%20generate%20high-quality%20multimodal%20reasoning%20paths%3B%20%28ii%29%20a%20supervised%20fine-tuning%20%28SFT%29%20process%20with%20serialized%20inference%20data%3B%20%28iii%29%20a%20policy-guided%20multi-turn%20inference%20method%3B%20and%20%28iv%29%20an%20atomic%20capability%20metric%20to%20evaluate%20the%20single-step%20utilization%20rate.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20AtomThink%20significantly%20improves%20the%20performance%20of%20baseline%20MLLMs%2C%20achieving%20more%20than%2010%5C%25%20average%20accuracy%20gains%20on%20MathVista%20and%20MathVerse.%20Compared%20to%20state-of-the-art%20structured%20CoT%20approaches%2C%20our%20method%20not%20only%20achieves%20higher%20accuracy%20but%20also%20improves%20data%20utilization%20by%205%20%24%5Ctimes%24%20and%20boosts%20inference%20efficiency%20by%2085.3%5C%25.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Kun-Xiang/AtomThink.&entry.1838667208=http%3A//arxiv.org/abs/2411.11930v5&entry.124074799=Read"},
{"title": "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems", "author": "Teresa Klatzer and Savvas Melidonis and Marcelo Pereyra and Konstantinos C. Zygalakis", "abstract": "This paper studies plug-and-play (PnP) Langevin sampling strategies for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling offers a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. Herein, we leverage and adapt recent developments in this framework to tackle challenging imaging problems involving weakly informative Poisson data. Existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we explore two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is evaluated and contrasted through extensive numerical experiments and comparisons with state-of-the-art methods. The source code accompanying this paper is available at https://github.com/freyyia/pnp-langevin-poisson.", "link": "http://arxiv.org/abs/2503.16222v2", "date": "2026-01-09", "relevancy": 2.0481, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Bayesian%20Computation%20Using%20Plug-and-Play%20Priors%20for%20Poisson%20Inverse%20Problems&body=Title%3A%20Efficient%20Bayesian%20Computation%20Using%20Plug-and-Play%20Priors%20for%20Poisson%20Inverse%20Problems%0AAuthor%3A%20Teresa%20Klatzer%20and%20Savvas%20Melidonis%20and%20Marcelo%20Pereyra%20and%20Konstantinos%20C.%20Zygalakis%0AAbstract%3A%20This%20paper%20studies%20plug-and-play%20%28PnP%29%20Langevin%20sampling%20strategies%20for%20Bayesian%20inference%20in%20low-photon%20Poisson%20imaging%20problems%2C%20a%20challenging%20class%20of%20problems%20with%20significant%20applications%20in%20astronomy%2C%20medicine%2C%20and%20biology.%20PnP%20Langevin%20sampling%20offers%20a%20powerful%20framework%20for%20Bayesian%20image%20restoration%2C%20enabling%20accurate%20point%20estimation%20as%20well%20as%20advanced%20inference%20tasks%2C%20including%20uncertainty%20quantification%20and%20visualization%20analyses%2C%20and%20empirical%20Bayesian%20inference%20for%20automatic%20model%20parameter%20tuning.%20Herein%2C%20we%20leverage%20and%20adapt%20recent%20developments%20in%20this%20framework%20to%20tackle%20challenging%20imaging%20problems%20involving%20weakly%20informative%20Poisson%20data.%20Existing%20PnP%20Langevin%20algorithms%20are%20not%20well-suited%20for%20low-photon%20Poisson%20imaging%20due%20to%20high%20solution%20uncertainty%20and%20poor%20regularity%20properties%2C%20such%20as%20exploding%20gradients%20and%20non-negativity%20constraints.%20To%20address%20these%20challenges%2C%20we%20explore%20two%20strategies%20for%20extending%20Langevin%20PnP%20sampling%20to%20Poisson%20imaging%20models%3A%20%28i%29%20an%20accelerated%20PnP%20Langevin%20method%20that%20incorporates%20boundary%20reflections%20and%20a%20Poisson%20likelihood%20approximation%20and%20%28ii%29%20a%20mirror%20sampling%20algorithm%20that%20leverages%20a%20Riemannian%20geometry%20to%20handle%20the%20constraints%20and%20the%20poor%20regularity%20of%20the%20likelihood%20without%20approximations.%20The%20effectiveness%20of%20these%20approaches%20is%20evaluated%20and%20contrasted%20through%20extensive%20numerical%20experiments%20and%20comparisons%20with%20state-of-the-art%20methods.%20The%20source%20code%20accompanying%20this%20paper%20is%20available%20at%20https%3A//github.com/freyyia/pnp-langevin-poisson.%0ALink%3A%20http%3A//arxiv.org/abs/2503.16222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Bayesian%2520Computation%2520Using%2520Plug-and-Play%2520Priors%2520for%2520Poisson%2520Inverse%2520Problems%26entry.906535625%3DTeresa%2520Klatzer%2520and%2520Savvas%2520Melidonis%2520and%2520Marcelo%2520Pereyra%2520and%2520Konstantinos%2520C.%2520Zygalakis%26entry.1292438233%3DThis%2520paper%2520studies%2520plug-and-play%2520%2528PnP%2529%2520Langevin%2520sampling%2520strategies%2520for%2520Bayesian%2520inference%2520in%2520low-photon%2520Poisson%2520imaging%2520problems%252C%2520a%2520challenging%2520class%2520of%2520problems%2520with%2520significant%2520applications%2520in%2520astronomy%252C%2520medicine%252C%2520and%2520biology.%2520PnP%2520Langevin%2520sampling%2520offers%2520a%2520powerful%2520framework%2520for%2520Bayesian%2520image%2520restoration%252C%2520enabling%2520accurate%2520point%2520estimation%2520as%2520well%2520as%2520advanced%2520inference%2520tasks%252C%2520including%2520uncertainty%2520quantification%2520and%2520visualization%2520analyses%252C%2520and%2520empirical%2520Bayesian%2520inference%2520for%2520automatic%2520model%2520parameter%2520tuning.%2520Herein%252C%2520we%2520leverage%2520and%2520adapt%2520recent%2520developments%2520in%2520this%2520framework%2520to%2520tackle%2520challenging%2520imaging%2520problems%2520involving%2520weakly%2520informative%2520Poisson%2520data.%2520Existing%2520PnP%2520Langevin%2520algorithms%2520are%2520not%2520well-suited%2520for%2520low-photon%2520Poisson%2520imaging%2520due%2520to%2520high%2520solution%2520uncertainty%2520and%2520poor%2520regularity%2520properties%252C%2520such%2520as%2520exploding%2520gradients%2520and%2520non-negativity%2520constraints.%2520To%2520address%2520these%2520challenges%252C%2520we%2520explore%2520two%2520strategies%2520for%2520extending%2520Langevin%2520PnP%2520sampling%2520to%2520Poisson%2520imaging%2520models%253A%2520%2528i%2529%2520an%2520accelerated%2520PnP%2520Langevin%2520method%2520that%2520incorporates%2520boundary%2520reflections%2520and%2520a%2520Poisson%2520likelihood%2520approximation%2520and%2520%2528ii%2529%2520a%2520mirror%2520sampling%2520algorithm%2520that%2520leverages%2520a%2520Riemannian%2520geometry%2520to%2520handle%2520the%2520constraints%2520and%2520the%2520poor%2520regularity%2520of%2520the%2520likelihood%2520without%2520approximations.%2520The%2520effectiveness%2520of%2520these%2520approaches%2520is%2520evaluated%2520and%2520contrasted%2520through%2520extensive%2520numerical%2520experiments%2520and%2520comparisons%2520with%2520state-of-the-art%2520methods.%2520The%2520source%2520code%2520accompanying%2520this%2520paper%2520is%2520available%2520at%2520https%253A//github.com/freyyia/pnp-langevin-poisson.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Bayesian%20Computation%20Using%20Plug-and-Play%20Priors%20for%20Poisson%20Inverse%20Problems&entry.906535625=Teresa%20Klatzer%20and%20Savvas%20Melidonis%20and%20Marcelo%20Pereyra%20and%20Konstantinos%20C.%20Zygalakis&entry.1292438233=This%20paper%20studies%20plug-and-play%20%28PnP%29%20Langevin%20sampling%20strategies%20for%20Bayesian%20inference%20in%20low-photon%20Poisson%20imaging%20problems%2C%20a%20challenging%20class%20of%20problems%20with%20significant%20applications%20in%20astronomy%2C%20medicine%2C%20and%20biology.%20PnP%20Langevin%20sampling%20offers%20a%20powerful%20framework%20for%20Bayesian%20image%20restoration%2C%20enabling%20accurate%20point%20estimation%20as%20well%20as%20advanced%20inference%20tasks%2C%20including%20uncertainty%20quantification%20and%20visualization%20analyses%2C%20and%20empirical%20Bayesian%20inference%20for%20automatic%20model%20parameter%20tuning.%20Herein%2C%20we%20leverage%20and%20adapt%20recent%20developments%20in%20this%20framework%20to%20tackle%20challenging%20imaging%20problems%20involving%20weakly%20informative%20Poisson%20data.%20Existing%20PnP%20Langevin%20algorithms%20are%20not%20well-suited%20for%20low-photon%20Poisson%20imaging%20due%20to%20high%20solution%20uncertainty%20and%20poor%20regularity%20properties%2C%20such%20as%20exploding%20gradients%20and%20non-negativity%20constraints.%20To%20address%20these%20challenges%2C%20we%20explore%20two%20strategies%20for%20extending%20Langevin%20PnP%20sampling%20to%20Poisson%20imaging%20models%3A%20%28i%29%20an%20accelerated%20PnP%20Langevin%20method%20that%20incorporates%20boundary%20reflections%20and%20a%20Poisson%20likelihood%20approximation%20and%20%28ii%29%20a%20mirror%20sampling%20algorithm%20that%20leverages%20a%20Riemannian%20geometry%20to%20handle%20the%20constraints%20and%20the%20poor%20regularity%20of%20the%20likelihood%20without%20approximations.%20The%20effectiveness%20of%20these%20approaches%20is%20evaluated%20and%20contrasted%20through%20extensive%20numerical%20experiments%20and%20comparisons%20with%20state-of-the-art%20methods.%20The%20source%20code%20accompanying%20this%20paper%20is%20available%20at%20https%3A//github.com/freyyia/pnp-langevin-poisson.&entry.1838667208=http%3A//arxiv.org/abs/2503.16222v2&entry.124074799=Read"},
{"title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs", "author": "Chengming Cui and Tianxin Wei and Ziyi Chen and Ruizhong Qiu and Zhichen Zeng and Zhining Liu and Xuying Ning and Duo Zhou and Jingrui He", "abstract": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.", "link": "http://arxiv.org/abs/2601.06022v1", "date": "2026-01-09", "relevancy": 2.0459, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaFuse%3A%20Adaptive%20Ensemble%20Decoding%20with%20Test-Time%20Scaling%20for%20LLMs&body=Title%3A%20AdaFuse%3A%20Adaptive%20Ensemble%20Decoding%20with%20Test-Time%20Scaling%20for%20LLMs%0AAuthor%3A%20Chengming%20Cui%20and%20Tianxin%20Wei%20and%20Ziyi%20Chen%20and%20Ruizhong%20Qiu%20and%20Zhichen%20Zeng%20and%20Zhining%20Liu%20and%20Xuying%20Ning%20and%20Duo%20Zhou%20and%20Jingrui%20He%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20exhibit%20complementary%20strengths%20arising%20from%20differences%20in%20pretraining%20data%2C%20model%20architectures%2C%20and%20decoding%20behaviors.%20Inference-time%20ensembling%20provides%20a%20practical%20way%20to%20combine%20these%20capabilities%20without%20retraining.%20However%2C%20existing%20ensemble%20approaches%20suffer%20from%20fundamental%20limitations.%20Most%20rely%20on%20fixed%20fusion%20granularity%2C%20which%20lacks%20the%20flexibility%20required%20for%20mid-generation%20adaptation%20and%20fails%20to%20adapt%20to%20different%20generation%20characteristics%20across%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20AdaFuse%2C%20an%20adaptive%20ensemble%20decoding%20framework%20that%20dynamically%20selects%20semantically%20appropriate%20fusion%20units%20during%20generation.%20Rather%20than%20committing%20to%20a%20fixed%20granularity%2C%20AdaFuse%20adjusts%20fusion%20behavior%20on%20the%20fly%20based%20on%20the%20decoding%20context%2C%20with%20words%20serving%20as%20basic%20building%20blocks%20for%20alignment.%20To%20be%20specific%2C%20we%20introduce%20an%20uncertainty-based%20criterion%20to%20decide%20whether%20to%20apply%20ensembling%20at%20each%20decoding%20step.%20Under%20confident%20decoding%20states%2C%20the%20model%20continues%20generation%20directly.%20In%20less%20certain%20states%2C%20AdaFuse%20invokes%20a%20diversity-aware%20scaling%20strategy%20to%20explore%20alternative%20candidate%20continuations%20and%20inform%20ensemble%20decisions.%20This%20design%20establishes%20a%20synergistic%20interaction%20between%20adaptive%20ensembling%20and%20test-time%20scaling%2C%20where%20ensemble%20decisions%20guide%20targeted%20exploration%2C%20and%20the%20resulting%20diversity%20in%20turn%20strengthens%20ensemble%20quality.%20Experiments%20on%20open-domain%20question%20answering%2C%20arithmetic%20reasoning%2C%20and%20machine%20translation%20demonstrate%20that%20AdaFuse%20consistently%20outperforms%20strong%20ensemble%20baselines%2C%20achieving%20an%20average%20relative%20improvement%20of%206.88%25.%20The%20code%20is%20available%20at%20https%3A//github.com/CCM0111/AdaFuse.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaFuse%253A%2520Adaptive%2520Ensemble%2520Decoding%2520with%2520Test-Time%2520Scaling%2520for%2520LLMs%26entry.906535625%3DChengming%2520Cui%2520and%2520Tianxin%2520Wei%2520and%2520Ziyi%2520Chen%2520and%2520Ruizhong%2520Qiu%2520and%2520Zhichen%2520Zeng%2520and%2520Zhining%2520Liu%2520and%2520Xuying%2520Ning%2520and%2520Duo%2520Zhou%2520and%2520Jingrui%2520He%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520complementary%2520strengths%2520arising%2520from%2520differences%2520in%2520pretraining%2520data%252C%2520model%2520architectures%252C%2520and%2520decoding%2520behaviors.%2520Inference-time%2520ensembling%2520provides%2520a%2520practical%2520way%2520to%2520combine%2520these%2520capabilities%2520without%2520retraining.%2520However%252C%2520existing%2520ensemble%2520approaches%2520suffer%2520from%2520fundamental%2520limitations.%2520Most%2520rely%2520on%2520fixed%2520fusion%2520granularity%252C%2520which%2520lacks%2520the%2520flexibility%2520required%2520for%2520mid-generation%2520adaptation%2520and%2520fails%2520to%2520adapt%2520to%2520different%2520generation%2520characteristics%2520across%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520AdaFuse%252C%2520an%2520adaptive%2520ensemble%2520decoding%2520framework%2520that%2520dynamically%2520selects%2520semantically%2520appropriate%2520fusion%2520units%2520during%2520generation.%2520Rather%2520than%2520committing%2520to%2520a%2520fixed%2520granularity%252C%2520AdaFuse%2520adjusts%2520fusion%2520behavior%2520on%2520the%2520fly%2520based%2520on%2520the%2520decoding%2520context%252C%2520with%2520words%2520serving%2520as%2520basic%2520building%2520blocks%2520for%2520alignment.%2520To%2520be%2520specific%252C%2520we%2520introduce%2520an%2520uncertainty-based%2520criterion%2520to%2520decide%2520whether%2520to%2520apply%2520ensembling%2520at%2520each%2520decoding%2520step.%2520Under%2520confident%2520decoding%2520states%252C%2520the%2520model%2520continues%2520generation%2520directly.%2520In%2520less%2520certain%2520states%252C%2520AdaFuse%2520invokes%2520a%2520diversity-aware%2520scaling%2520strategy%2520to%2520explore%2520alternative%2520candidate%2520continuations%2520and%2520inform%2520ensemble%2520decisions.%2520This%2520design%2520establishes%2520a%2520synergistic%2520interaction%2520between%2520adaptive%2520ensembling%2520and%2520test-time%2520scaling%252C%2520where%2520ensemble%2520decisions%2520guide%2520targeted%2520exploration%252C%2520and%2520the%2520resulting%2520diversity%2520in%2520turn%2520strengthens%2520ensemble%2520quality.%2520Experiments%2520on%2520open-domain%2520question%2520answering%252C%2520arithmetic%2520reasoning%252C%2520and%2520machine%2520translation%2520demonstrate%2520that%2520AdaFuse%2520consistently%2520outperforms%2520strong%2520ensemble%2520baselines%252C%2520achieving%2520an%2520average%2520relative%2520improvement%2520of%25206.88%2525.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/CCM0111/AdaFuse.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaFuse%3A%20Adaptive%20Ensemble%20Decoding%20with%20Test-Time%20Scaling%20for%20LLMs&entry.906535625=Chengming%20Cui%20and%20Tianxin%20Wei%20and%20Ziyi%20Chen%20and%20Ruizhong%20Qiu%20and%20Zhichen%20Zeng%20and%20Zhining%20Liu%20and%20Xuying%20Ning%20and%20Duo%20Zhou%20and%20Jingrui%20He&entry.1292438233=Large%20language%20models%20%28LLMs%29%20exhibit%20complementary%20strengths%20arising%20from%20differences%20in%20pretraining%20data%2C%20model%20architectures%2C%20and%20decoding%20behaviors.%20Inference-time%20ensembling%20provides%20a%20practical%20way%20to%20combine%20these%20capabilities%20without%20retraining.%20However%2C%20existing%20ensemble%20approaches%20suffer%20from%20fundamental%20limitations.%20Most%20rely%20on%20fixed%20fusion%20granularity%2C%20which%20lacks%20the%20flexibility%20required%20for%20mid-generation%20adaptation%20and%20fails%20to%20adapt%20to%20different%20generation%20characteristics%20across%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20AdaFuse%2C%20an%20adaptive%20ensemble%20decoding%20framework%20that%20dynamically%20selects%20semantically%20appropriate%20fusion%20units%20during%20generation.%20Rather%20than%20committing%20to%20a%20fixed%20granularity%2C%20AdaFuse%20adjusts%20fusion%20behavior%20on%20the%20fly%20based%20on%20the%20decoding%20context%2C%20with%20words%20serving%20as%20basic%20building%20blocks%20for%20alignment.%20To%20be%20specific%2C%20we%20introduce%20an%20uncertainty-based%20criterion%20to%20decide%20whether%20to%20apply%20ensembling%20at%20each%20decoding%20step.%20Under%20confident%20decoding%20states%2C%20the%20model%20continues%20generation%20directly.%20In%20less%20certain%20states%2C%20AdaFuse%20invokes%20a%20diversity-aware%20scaling%20strategy%20to%20explore%20alternative%20candidate%20continuations%20and%20inform%20ensemble%20decisions.%20This%20design%20establishes%20a%20synergistic%20interaction%20between%20adaptive%20ensembling%20and%20test-time%20scaling%2C%20where%20ensemble%20decisions%20guide%20targeted%20exploration%2C%20and%20the%20resulting%20diversity%20in%20turn%20strengthens%20ensemble%20quality.%20Experiments%20on%20open-domain%20question%20answering%2C%20arithmetic%20reasoning%2C%20and%20machine%20translation%20demonstrate%20that%20AdaFuse%20consistently%20outperforms%20strong%20ensemble%20baselines%2C%20achieving%20an%20average%20relative%20improvement%20of%206.88%25.%20The%20code%20is%20available%20at%20https%3A//github.com/CCM0111/AdaFuse.&entry.1838667208=http%3A//arxiv.org/abs/2601.06022v1&entry.124074799=Read"},
{"title": "From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data", "author": "Mete Harun Akcay and Buse Gul Atli and Siddharth Prakash Rao and Alexandros Bakas", "abstract": "As the volume of stored data continues to grow, identifying and protecting sensitive information within large repositories becomes increasingly challenging, especially when shared with multiple users with different roles and permissions. This work presents a system architecture for trusted data sharing with policy-driven access control, enabling selective protection of sensitive regions while maintaining scalability. The proposed architecture integrates four core modules that combine automated detection of sensitive regions, post-correction, key management, and access control. Sensitive regions are secured using a hybrid scheme that employs symmetric encryption for efficiency and Attribute-Based Encryption for policy enforcement. The system supports efficient key distribution and isolates key storage to strengthen overall security. To demonstrate its applicability, we evaluate the system on visual datasets, where Privacy-Sensitive Objects in images are automatically detected, reassessed, and selectively encrypted prior to sharing in a data repository. Experimental results show that our system provides effective PSO detection, increases macro-averaged F1 score (5%) and mean Average Precision (10%), and maintains an average policy-enforced decryption time of less than 1 second per image. These results demonstrate the effectiveness, efficiency and scalability of our proposed solution for fine-grained access control.", "link": "http://arxiv.org/abs/2510.19418v2", "date": "2026-01-09", "relevancy": 2.0435, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5206}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20See%20to%20Shield%3A%20ML-Assisted%20Fine-Grained%20Access%20Control%20for%20Visual%20Data&body=Title%3A%20From%20See%20to%20Shield%3A%20ML-Assisted%20Fine-Grained%20Access%20Control%20for%20Visual%20Data%0AAuthor%3A%20Mete%20Harun%20Akcay%20and%20Buse%20Gul%20Atli%20and%20Siddharth%20Prakash%20Rao%20and%20Alexandros%20Bakas%0AAbstract%3A%20As%20the%20volume%20of%20stored%20data%20continues%20to%20grow%2C%20identifying%20and%20protecting%20sensitive%20information%20within%20large%20repositories%20becomes%20increasingly%20challenging%2C%20especially%20when%20shared%20with%20multiple%20users%20with%20different%20roles%20and%20permissions.%20This%20work%20presents%20a%20system%20architecture%20for%20trusted%20data%20sharing%20with%20policy-driven%20access%20control%2C%20enabling%20selective%20protection%20of%20sensitive%20regions%20while%20maintaining%20scalability.%20The%20proposed%20architecture%20integrates%20four%20core%20modules%20that%20combine%20automated%20detection%20of%20sensitive%20regions%2C%20post-correction%2C%20key%20management%2C%20and%20access%20control.%20Sensitive%20regions%20are%20secured%20using%20a%20hybrid%20scheme%20that%20employs%20symmetric%20encryption%20for%20efficiency%20and%20Attribute-Based%20Encryption%20for%20policy%20enforcement.%20The%20system%20supports%20efficient%20key%20distribution%20and%20isolates%20key%20storage%20to%20strengthen%20overall%20security.%20To%20demonstrate%20its%20applicability%2C%20we%20evaluate%20the%20system%20on%20visual%20datasets%2C%20where%20Privacy-Sensitive%20Objects%20in%20images%20are%20automatically%20detected%2C%20reassessed%2C%20and%20selectively%20encrypted%20prior%20to%20sharing%20in%20a%20data%20repository.%20Experimental%20results%20show%20that%20our%20system%20provides%20effective%20PSO%20detection%2C%20increases%20macro-averaged%20F1%20score%20%285%25%29%20and%20mean%20Average%20Precision%20%2810%25%29%2C%20and%20maintains%20an%20average%20policy-enforced%20decryption%20time%20of%20less%20than%201%20second%20per%20image.%20These%20results%20demonstrate%20the%20effectiveness%2C%20efficiency%20and%20scalability%20of%20our%20proposed%20solution%20for%20fine-grained%20access%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2510.19418v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520See%2520to%2520Shield%253A%2520ML-Assisted%2520Fine-Grained%2520Access%2520Control%2520for%2520Visual%2520Data%26entry.906535625%3DMete%2520Harun%2520Akcay%2520and%2520Buse%2520Gul%2520Atli%2520and%2520Siddharth%2520Prakash%2520Rao%2520and%2520Alexandros%2520Bakas%26entry.1292438233%3DAs%2520the%2520volume%2520of%2520stored%2520data%2520continues%2520to%2520grow%252C%2520identifying%2520and%2520protecting%2520sensitive%2520information%2520within%2520large%2520repositories%2520becomes%2520increasingly%2520challenging%252C%2520especially%2520when%2520shared%2520with%2520multiple%2520users%2520with%2520different%2520roles%2520and%2520permissions.%2520This%2520work%2520presents%2520a%2520system%2520architecture%2520for%2520trusted%2520data%2520sharing%2520with%2520policy-driven%2520access%2520control%252C%2520enabling%2520selective%2520protection%2520of%2520sensitive%2520regions%2520while%2520maintaining%2520scalability.%2520The%2520proposed%2520architecture%2520integrates%2520four%2520core%2520modules%2520that%2520combine%2520automated%2520detection%2520of%2520sensitive%2520regions%252C%2520post-correction%252C%2520key%2520management%252C%2520and%2520access%2520control.%2520Sensitive%2520regions%2520are%2520secured%2520using%2520a%2520hybrid%2520scheme%2520that%2520employs%2520symmetric%2520encryption%2520for%2520efficiency%2520and%2520Attribute-Based%2520Encryption%2520for%2520policy%2520enforcement.%2520The%2520system%2520supports%2520efficient%2520key%2520distribution%2520and%2520isolates%2520key%2520storage%2520to%2520strengthen%2520overall%2520security.%2520To%2520demonstrate%2520its%2520applicability%252C%2520we%2520evaluate%2520the%2520system%2520on%2520visual%2520datasets%252C%2520where%2520Privacy-Sensitive%2520Objects%2520in%2520images%2520are%2520automatically%2520detected%252C%2520reassessed%252C%2520and%2520selectively%2520encrypted%2520prior%2520to%2520sharing%2520in%2520a%2520data%2520repository.%2520Experimental%2520results%2520show%2520that%2520our%2520system%2520provides%2520effective%2520PSO%2520detection%252C%2520increases%2520macro-averaged%2520F1%2520score%2520%25285%2525%2529%2520and%2520mean%2520Average%2520Precision%2520%252810%2525%2529%252C%2520and%2520maintains%2520an%2520average%2520policy-enforced%2520decryption%2520time%2520of%2520less%2520than%25201%2520second%2520per%2520image.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%252C%2520efficiency%2520and%2520scalability%2520of%2520our%2520proposed%2520solution%2520for%2520fine-grained%2520access%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19418v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20See%20to%20Shield%3A%20ML-Assisted%20Fine-Grained%20Access%20Control%20for%20Visual%20Data&entry.906535625=Mete%20Harun%20Akcay%20and%20Buse%20Gul%20Atli%20and%20Siddharth%20Prakash%20Rao%20and%20Alexandros%20Bakas&entry.1292438233=As%20the%20volume%20of%20stored%20data%20continues%20to%20grow%2C%20identifying%20and%20protecting%20sensitive%20information%20within%20large%20repositories%20becomes%20increasingly%20challenging%2C%20especially%20when%20shared%20with%20multiple%20users%20with%20different%20roles%20and%20permissions.%20This%20work%20presents%20a%20system%20architecture%20for%20trusted%20data%20sharing%20with%20policy-driven%20access%20control%2C%20enabling%20selective%20protection%20of%20sensitive%20regions%20while%20maintaining%20scalability.%20The%20proposed%20architecture%20integrates%20four%20core%20modules%20that%20combine%20automated%20detection%20of%20sensitive%20regions%2C%20post-correction%2C%20key%20management%2C%20and%20access%20control.%20Sensitive%20regions%20are%20secured%20using%20a%20hybrid%20scheme%20that%20employs%20symmetric%20encryption%20for%20efficiency%20and%20Attribute-Based%20Encryption%20for%20policy%20enforcement.%20The%20system%20supports%20efficient%20key%20distribution%20and%20isolates%20key%20storage%20to%20strengthen%20overall%20security.%20To%20demonstrate%20its%20applicability%2C%20we%20evaluate%20the%20system%20on%20visual%20datasets%2C%20where%20Privacy-Sensitive%20Objects%20in%20images%20are%20automatically%20detected%2C%20reassessed%2C%20and%20selectively%20encrypted%20prior%20to%20sharing%20in%20a%20data%20repository.%20Experimental%20results%20show%20that%20our%20system%20provides%20effective%20PSO%20detection%2C%20increases%20macro-averaged%20F1%20score%20%285%25%29%20and%20mean%20Average%20Precision%20%2810%25%29%2C%20and%20maintains%20an%20average%20policy-enforced%20decryption%20time%20of%20less%20than%201%20second%20per%20image.%20These%20results%20demonstrate%20the%20effectiveness%2C%20efficiency%20and%20scalability%20of%20our%20proposed%20solution%20for%20fine-grained%20access%20control.&entry.1838667208=http%3A//arxiv.org/abs/2510.19418v2&entry.124074799=Read"},
{"title": "TDHook: A Lightweight Framework for Interpretability", "author": "Yoann Poupart", "abstract": "Interpretability of Deep Neural Networks (DNNs) is a growing field driven by the study of vision and language models. Yet, some use cases, like image captioning, or domains like Deep Reinforcement Learning (DRL), require complex modelling, with multiple inputs and outputs or use composable and separated networks. As a consequence, they rarely fit natively into the API of popular interpretability frameworks. We thus present TDHook, an open-source, lightweight, generic interpretability framework based on $\\texttt{tensordict}$ and applicable to any $\\texttt{torch}$ model. It focuses on handling complex composed models which can be trained for Computer Vision, Natural Language Processing, Reinforcement Learning or any other domain. This library features ready-to-use methods for attribution, probing and a flexible get-set API for interventions, and is aiming to bridge the gap between these method classes to make modern interpretability pipelines more accessible. TDHook is designed with minimal dependencies, requiring roughly half as much disk space as $\\texttt{transformer_lens}$, and, in our controlled benchmark, achieves up to a $\\times$2 speed-up over $\\texttt{captum}$ when running integrated gradients for multi-target pipelines on both CPU and GPU. In addition, to value our work, we showcase concrete use cases of our library with composed interpretability pipelines in Computer Vision (CV) and Natural Language Processing (NLP), as well as with complex models in DRL.", "link": "http://arxiv.org/abs/2509.25475v2", "date": "2026-01-09", "relevancy": 2.0362, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5139}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TDHook%3A%20A%20Lightweight%20Framework%20for%20Interpretability&body=Title%3A%20TDHook%3A%20A%20Lightweight%20Framework%20for%20Interpretability%0AAuthor%3A%20Yoann%20Poupart%0AAbstract%3A%20Interpretability%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20is%20a%20growing%20field%20driven%20by%20the%20study%20of%20vision%20and%20language%20models.%20Yet%2C%20some%20use%20cases%2C%20like%20image%20captioning%2C%20or%20domains%20like%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20require%20complex%20modelling%2C%20with%20multiple%20inputs%20and%20outputs%20or%20use%20composable%20and%20separated%20networks.%20As%20a%20consequence%2C%20they%20rarely%20fit%20natively%20into%20the%20API%20of%20popular%20interpretability%20frameworks.%20We%20thus%20present%20TDHook%2C%20an%20open-source%2C%20lightweight%2C%20generic%20interpretability%20framework%20based%20on%20%24%5Ctexttt%7Btensordict%7D%24%20and%20applicable%20to%20any%20%24%5Ctexttt%7Btorch%7D%24%20model.%20It%20focuses%20on%20handling%20complex%20composed%20models%20which%20can%20be%20trained%20for%20Computer%20Vision%2C%20Natural%20Language%20Processing%2C%20Reinforcement%20Learning%20or%20any%20other%20domain.%20This%20library%20features%20ready-to-use%20methods%20for%20attribution%2C%20probing%20and%20a%20flexible%20get-set%20API%20for%20interventions%2C%20and%20is%20aiming%20to%20bridge%20the%20gap%20between%20these%20method%20classes%20to%20make%20modern%20interpretability%20pipelines%20more%20accessible.%20TDHook%20is%20designed%20with%20minimal%20dependencies%2C%20requiring%20roughly%20half%20as%20much%20disk%20space%20as%20%24%5Ctexttt%7Btransformer_lens%7D%24%2C%20and%2C%20in%20our%20controlled%20benchmark%2C%20achieves%20up%20to%20a%20%24%5Ctimes%242%20speed-up%20over%20%24%5Ctexttt%7Bcaptum%7D%24%20when%20running%20integrated%20gradients%20for%20multi-target%20pipelines%20on%20both%20CPU%20and%20GPU.%20In%20addition%2C%20to%20value%20our%20work%2C%20we%20showcase%20concrete%20use%20cases%20of%20our%20library%20with%20composed%20interpretability%20pipelines%20in%20Computer%20Vision%20%28CV%29%20and%20Natural%20Language%20Processing%20%28NLP%29%2C%20as%20well%20as%20with%20complex%20models%20in%20DRL.%0ALink%3A%20http%3A//arxiv.org/abs/2509.25475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTDHook%253A%2520A%2520Lightweight%2520Framework%2520for%2520Interpretability%26entry.906535625%3DYoann%2520Poupart%26entry.1292438233%3DInterpretability%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520is%2520a%2520growing%2520field%2520driven%2520by%2520the%2520study%2520of%2520vision%2520and%2520language%2520models.%2520Yet%252C%2520some%2520use%2520cases%252C%2520like%2520image%2520captioning%252C%2520or%2520domains%2520like%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%252C%2520require%2520complex%2520modelling%252C%2520with%2520multiple%2520inputs%2520and%2520outputs%2520or%2520use%2520composable%2520and%2520separated%2520networks.%2520As%2520a%2520consequence%252C%2520they%2520rarely%2520fit%2520natively%2520into%2520the%2520API%2520of%2520popular%2520interpretability%2520frameworks.%2520We%2520thus%2520present%2520TDHook%252C%2520an%2520open-source%252C%2520lightweight%252C%2520generic%2520interpretability%2520framework%2520based%2520on%2520%2524%255Ctexttt%257Btensordict%257D%2524%2520and%2520applicable%2520to%2520any%2520%2524%255Ctexttt%257Btorch%257D%2524%2520model.%2520It%2520focuses%2520on%2520handling%2520complex%2520composed%2520models%2520which%2520can%2520be%2520trained%2520for%2520Computer%2520Vision%252C%2520Natural%2520Language%2520Processing%252C%2520Reinforcement%2520Learning%2520or%2520any%2520other%2520domain.%2520This%2520library%2520features%2520ready-to-use%2520methods%2520for%2520attribution%252C%2520probing%2520and%2520a%2520flexible%2520get-set%2520API%2520for%2520interventions%252C%2520and%2520is%2520aiming%2520to%2520bridge%2520the%2520gap%2520between%2520these%2520method%2520classes%2520to%2520make%2520modern%2520interpretability%2520pipelines%2520more%2520accessible.%2520TDHook%2520is%2520designed%2520with%2520minimal%2520dependencies%252C%2520requiring%2520roughly%2520half%2520as%2520much%2520disk%2520space%2520as%2520%2524%255Ctexttt%257Btransformer_lens%257D%2524%252C%2520and%252C%2520in%2520our%2520controlled%2520benchmark%252C%2520achieves%2520up%2520to%2520a%2520%2524%255Ctimes%25242%2520speed-up%2520over%2520%2524%255Ctexttt%257Bcaptum%257D%2524%2520when%2520running%2520integrated%2520gradients%2520for%2520multi-target%2520pipelines%2520on%2520both%2520CPU%2520and%2520GPU.%2520In%2520addition%252C%2520to%2520value%2520our%2520work%252C%2520we%2520showcase%2520concrete%2520use%2520cases%2520of%2520our%2520library%2520with%2520composed%2520interpretability%2520pipelines%2520in%2520Computer%2520Vision%2520%2528CV%2529%2520and%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%252C%2520as%2520well%2520as%2520with%2520complex%2520models%2520in%2520DRL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TDHook%3A%20A%20Lightweight%20Framework%20for%20Interpretability&entry.906535625=Yoann%20Poupart&entry.1292438233=Interpretability%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20is%20a%20growing%20field%20driven%20by%20the%20study%20of%20vision%20and%20language%20models.%20Yet%2C%20some%20use%20cases%2C%20like%20image%20captioning%2C%20or%20domains%20like%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20require%20complex%20modelling%2C%20with%20multiple%20inputs%20and%20outputs%20or%20use%20composable%20and%20separated%20networks.%20As%20a%20consequence%2C%20they%20rarely%20fit%20natively%20into%20the%20API%20of%20popular%20interpretability%20frameworks.%20We%20thus%20present%20TDHook%2C%20an%20open-source%2C%20lightweight%2C%20generic%20interpretability%20framework%20based%20on%20%24%5Ctexttt%7Btensordict%7D%24%20and%20applicable%20to%20any%20%24%5Ctexttt%7Btorch%7D%24%20model.%20It%20focuses%20on%20handling%20complex%20composed%20models%20which%20can%20be%20trained%20for%20Computer%20Vision%2C%20Natural%20Language%20Processing%2C%20Reinforcement%20Learning%20or%20any%20other%20domain.%20This%20library%20features%20ready-to-use%20methods%20for%20attribution%2C%20probing%20and%20a%20flexible%20get-set%20API%20for%20interventions%2C%20and%20is%20aiming%20to%20bridge%20the%20gap%20between%20these%20method%20classes%20to%20make%20modern%20interpretability%20pipelines%20more%20accessible.%20TDHook%20is%20designed%20with%20minimal%20dependencies%2C%20requiring%20roughly%20half%20as%20much%20disk%20space%20as%20%24%5Ctexttt%7Btransformer_lens%7D%24%2C%20and%2C%20in%20our%20controlled%20benchmark%2C%20achieves%20up%20to%20a%20%24%5Ctimes%242%20speed-up%20over%20%24%5Ctexttt%7Bcaptum%7D%24%20when%20running%20integrated%20gradients%20for%20multi-target%20pipelines%20on%20both%20CPU%20and%20GPU.%20In%20addition%2C%20to%20value%20our%20work%2C%20we%20showcase%20concrete%20use%20cases%20of%20our%20library%20with%20composed%20interpretability%20pipelines%20in%20Computer%20Vision%20%28CV%29%20and%20Natural%20Language%20Processing%20%28NLP%29%2C%20as%20well%20as%20with%20complex%20models%20in%20DRL.&entry.1838667208=http%3A//arxiv.org/abs/2509.25475v2&entry.124074799=Read"},
{"title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs", "author": "Sandeep Mishra and Devichand Budagam and Anubhab Mandal and Bishal Santra and Pawan Goyal and Manish Gupta", "abstract": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.", "link": "http://arxiv.org/abs/2601.05851v1", "date": "2026-01-09", "relevancy": 2.0285, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5076}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Router-Suggest%3A%20Dynamic%20Routing%20for%20Multimodal%20Auto-Completion%20in%20Visually-Grounded%20Dialogs&body=Title%3A%20Router-Suggest%3A%20Dynamic%20Routing%20for%20Multimodal%20Auto-Completion%20in%20Visually-Grounded%20Dialogs%0AAuthor%3A%20Sandeep%20Mishra%20and%20Devichand%20Budagam%20and%20Anubhab%20Mandal%20and%20Bishal%20Santra%20and%20Pawan%20Goyal%20and%20Manish%20Gupta%0AAbstract%3A%20Real-time%20multimodal%20auto-completion%20is%20essential%20for%20digital%20assistants%2C%20chatbots%2C%20design%20tools%2C%20and%20healthcare%20consultations%2C%20where%20user%20inputs%20rely%20on%20shared%20visual%20context.%20We%20introduce%20Multimodal%20Auto-Completion%20%28MAC%29%2C%20a%20task%20that%20predicts%20upcoming%20characters%20in%20live%20chats%20using%20partially%20typed%20text%20and%20visual%20cues.%20Unlike%20traditional%20text-only%20auto-completion%20%28TAC%29%2C%20MAC%20grounds%20predictions%20in%20multimodal%20context%20to%20better%20capture%20user%20intent.%20To%20enable%20this%20task%2C%20we%20adapt%20MMDialog%20and%20ImageChat%20to%20create%20benchmark%20datasets.%20We%20evaluate%20leading%20vision-language%20models%20%28VLMs%29%20against%20strong%20textual%20baselines%2C%20highlighting%20trade-offs%20in%20accuracy%20and%20efficiency.%20We%20present%20Router-Suggest%2C%20a%20router%20framework%20that%20dynamically%20selects%20between%20textual%20models%20and%20VLMs%20based%20on%20dialog%20context%2C%20along%20with%20a%20lightweight%20variant%20for%20resource-constrained%20environments.%20Router-Suggest%20achieves%20a%202.3x%20to%2010x%20speedup%20over%20the%20best-performing%20VLM.%20A%20user%20study%20shows%20that%20VLMs%20significantly%20excel%20over%20textual%20models%20on%20user%20satisfaction%2C%20notably%20saving%20user%20typing%20effort%20and%20improving%20the%20quality%20of%20completions%20in%20multi-turn%20conversations.%20These%20findings%20underscore%20the%20need%20for%20multimodal%20context%20in%20auto-completions%2C%20leading%20to%20smarter%2C%20user-aware%20assistants.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouter-Suggest%253A%2520Dynamic%2520Routing%2520for%2520Multimodal%2520Auto-Completion%2520in%2520Visually-Grounded%2520Dialogs%26entry.906535625%3DSandeep%2520Mishra%2520and%2520Devichand%2520Budagam%2520and%2520Anubhab%2520Mandal%2520and%2520Bishal%2520Santra%2520and%2520Pawan%2520Goyal%2520and%2520Manish%2520Gupta%26entry.1292438233%3DReal-time%2520multimodal%2520auto-completion%2520is%2520essential%2520for%2520digital%2520assistants%252C%2520chatbots%252C%2520design%2520tools%252C%2520and%2520healthcare%2520consultations%252C%2520where%2520user%2520inputs%2520rely%2520on%2520shared%2520visual%2520context.%2520We%2520introduce%2520Multimodal%2520Auto-Completion%2520%2528MAC%2529%252C%2520a%2520task%2520that%2520predicts%2520upcoming%2520characters%2520in%2520live%2520chats%2520using%2520partially%2520typed%2520text%2520and%2520visual%2520cues.%2520Unlike%2520traditional%2520text-only%2520auto-completion%2520%2528TAC%2529%252C%2520MAC%2520grounds%2520predictions%2520in%2520multimodal%2520context%2520to%2520better%2520capture%2520user%2520intent.%2520To%2520enable%2520this%2520task%252C%2520we%2520adapt%2520MMDialog%2520and%2520ImageChat%2520to%2520create%2520benchmark%2520datasets.%2520We%2520evaluate%2520leading%2520vision-language%2520models%2520%2528VLMs%2529%2520against%2520strong%2520textual%2520baselines%252C%2520highlighting%2520trade-offs%2520in%2520accuracy%2520and%2520efficiency.%2520We%2520present%2520Router-Suggest%252C%2520a%2520router%2520framework%2520that%2520dynamically%2520selects%2520between%2520textual%2520models%2520and%2520VLMs%2520based%2520on%2520dialog%2520context%252C%2520along%2520with%2520a%2520lightweight%2520variant%2520for%2520resource-constrained%2520environments.%2520Router-Suggest%2520achieves%2520a%25202.3x%2520to%252010x%2520speedup%2520over%2520the%2520best-performing%2520VLM.%2520A%2520user%2520study%2520shows%2520that%2520VLMs%2520significantly%2520excel%2520over%2520textual%2520models%2520on%2520user%2520satisfaction%252C%2520notably%2520saving%2520user%2520typing%2520effort%2520and%2520improving%2520the%2520quality%2520of%2520completions%2520in%2520multi-turn%2520conversations.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520multimodal%2520context%2520in%2520auto-completions%252C%2520leading%2520to%2520smarter%252C%2520user-aware%2520assistants.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Router-Suggest%3A%20Dynamic%20Routing%20for%20Multimodal%20Auto-Completion%20in%20Visually-Grounded%20Dialogs&entry.906535625=Sandeep%20Mishra%20and%20Devichand%20Budagam%20and%20Anubhab%20Mandal%20and%20Bishal%20Santra%20and%20Pawan%20Goyal%20and%20Manish%20Gupta&entry.1292438233=Real-time%20multimodal%20auto-completion%20is%20essential%20for%20digital%20assistants%2C%20chatbots%2C%20design%20tools%2C%20and%20healthcare%20consultations%2C%20where%20user%20inputs%20rely%20on%20shared%20visual%20context.%20We%20introduce%20Multimodal%20Auto-Completion%20%28MAC%29%2C%20a%20task%20that%20predicts%20upcoming%20characters%20in%20live%20chats%20using%20partially%20typed%20text%20and%20visual%20cues.%20Unlike%20traditional%20text-only%20auto-completion%20%28TAC%29%2C%20MAC%20grounds%20predictions%20in%20multimodal%20context%20to%20better%20capture%20user%20intent.%20To%20enable%20this%20task%2C%20we%20adapt%20MMDialog%20and%20ImageChat%20to%20create%20benchmark%20datasets.%20We%20evaluate%20leading%20vision-language%20models%20%28VLMs%29%20against%20strong%20textual%20baselines%2C%20highlighting%20trade-offs%20in%20accuracy%20and%20efficiency.%20We%20present%20Router-Suggest%2C%20a%20router%20framework%20that%20dynamically%20selects%20between%20textual%20models%20and%20VLMs%20based%20on%20dialog%20context%2C%20along%20with%20a%20lightweight%20variant%20for%20resource-constrained%20environments.%20Router-Suggest%20achieves%20a%202.3x%20to%2010x%20speedup%20over%20the%20best-performing%20VLM.%20A%20user%20study%20shows%20that%20VLMs%20significantly%20excel%20over%20textual%20models%20on%20user%20satisfaction%2C%20notably%20saving%20user%20typing%20effort%20and%20improving%20the%20quality%20of%20completions%20in%20multi-turn%20conversations.%20These%20findings%20underscore%20the%20need%20for%20multimodal%20context%20in%20auto-completions%2C%20leading%20to%20smarter%2C%20user-aware%20assistants.&entry.1838667208=http%3A//arxiv.org/abs/2601.05851v1&entry.124074799=Read"},
{"title": "LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection", "author": "\u00de\u00f3r Sverrisson and Steinn Gu\u00f0mundsson", "abstract": "Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.", "link": "http://arxiv.org/abs/2601.06016v1", "date": "2026-01-09", "relevancy": 2.0236, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5439}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4822}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LookAroundNet%3A%20Extending%20Temporal%20Context%20with%20Transformers%20for%20Clinically%20Viable%20EEG%20Seizure%20Detection&body=Title%3A%20LookAroundNet%3A%20Extending%20Temporal%20Context%20with%20Transformers%20for%20Clinically%20Viable%20EEG%20Seizure%20Detection%0AAuthor%3A%20%C3%9E%C3%B3r%20Sverrisson%20and%20Steinn%20Gu%C3%B0mundsson%0AAbstract%3A%20Automated%20seizure%20detection%20from%20electroencephalography%20%28EEG%29%20remains%20difficult%20due%20to%20the%20large%20variability%20of%20seizure%20dynamics%20across%20patients%2C%20recording%20conditions%2C%20and%20clinical%20settings.%20We%20introduce%20LookAroundNet%2C%20a%20transformer-based%20seizure%20detector%20that%20uses%20a%20wider%20temporal%20window%20of%20EEG%20data%20to%20model%20seizure%20activity.%20The%20seizure%20detector%20incorporates%20EEG%20signals%20before%20and%20after%20the%20segment%20of%20interest%2C%20reflecting%20how%20clinicians%20use%20surrounding%20context%20when%20interpreting%20EEG%20recordings.%20We%20evaluate%20the%20proposed%20method%20on%20multiple%20EEG%20datasets%20spanning%20diverse%20clinical%20environments%2C%20patient%20populations%2C%20and%20recording%20modalities%2C%20including%20routine%20clinical%20EEG%20and%20long-term%20ambulatory%20recordings%2C%20in%20order%20to%20study%20performance%20across%20varying%20data%20distributions.%20The%20evaluation%20includes%20publicly%20available%20datasets%20as%20well%20as%20a%20large%20proprietary%20collection%20of%20home%20EEG%20recordings%2C%20providing%20complementary%20views%20of%20controlled%20clinical%20data%20and%20unconstrained%20home-monitoring%20conditions.%20Our%20results%20show%20that%20LookAroundNet%20achieves%20strong%20performance%20across%20datasets%2C%20generalizes%20well%20to%20previously%20unseen%20recording%20conditions%2C%20and%20operates%20with%20computational%20costs%20compatible%20with%20real-world%20clinical%20deployment.%20The%20results%20indicate%20that%20extended%20temporal%20context%2C%20increased%20training%20data%20diversity%2C%20and%20model%20ensembling%20are%20key%20factors%20for%20improving%20performance.%20This%20work%20contributes%20to%20moving%20automatic%20seizure%20detection%20models%20toward%20clinically%20viable%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.06016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookAroundNet%253A%2520Extending%2520Temporal%2520Context%2520with%2520Transformers%2520for%2520Clinically%2520Viable%2520EEG%2520Seizure%2520Detection%26entry.906535625%3D%25C3%259E%25C3%25B3r%2520Sverrisson%2520and%2520Steinn%2520Gu%25C3%25B0mundsson%26entry.1292438233%3DAutomated%2520seizure%2520detection%2520from%2520electroencephalography%2520%2528EEG%2529%2520remains%2520difficult%2520due%2520to%2520the%2520large%2520variability%2520of%2520seizure%2520dynamics%2520across%2520patients%252C%2520recording%2520conditions%252C%2520and%2520clinical%2520settings.%2520We%2520introduce%2520LookAroundNet%252C%2520a%2520transformer-based%2520seizure%2520detector%2520that%2520uses%2520a%2520wider%2520temporal%2520window%2520of%2520EEG%2520data%2520to%2520model%2520seizure%2520activity.%2520The%2520seizure%2520detector%2520incorporates%2520EEG%2520signals%2520before%2520and%2520after%2520the%2520segment%2520of%2520interest%252C%2520reflecting%2520how%2520clinicians%2520use%2520surrounding%2520context%2520when%2520interpreting%2520EEG%2520recordings.%2520We%2520evaluate%2520the%2520proposed%2520method%2520on%2520multiple%2520EEG%2520datasets%2520spanning%2520diverse%2520clinical%2520environments%252C%2520patient%2520populations%252C%2520and%2520recording%2520modalities%252C%2520including%2520routine%2520clinical%2520EEG%2520and%2520long-term%2520ambulatory%2520recordings%252C%2520in%2520order%2520to%2520study%2520performance%2520across%2520varying%2520data%2520distributions.%2520The%2520evaluation%2520includes%2520publicly%2520available%2520datasets%2520as%2520well%2520as%2520a%2520large%2520proprietary%2520collection%2520of%2520home%2520EEG%2520recordings%252C%2520providing%2520complementary%2520views%2520of%2520controlled%2520clinical%2520data%2520and%2520unconstrained%2520home-monitoring%2520conditions.%2520Our%2520results%2520show%2520that%2520LookAroundNet%2520achieves%2520strong%2520performance%2520across%2520datasets%252C%2520generalizes%2520well%2520to%2520previously%2520unseen%2520recording%2520conditions%252C%2520and%2520operates%2520with%2520computational%2520costs%2520compatible%2520with%2520real-world%2520clinical%2520deployment.%2520The%2520results%2520indicate%2520that%2520extended%2520temporal%2520context%252C%2520increased%2520training%2520data%2520diversity%252C%2520and%2520model%2520ensembling%2520are%2520key%2520factors%2520for%2520improving%2520performance.%2520This%2520work%2520contributes%2520to%2520moving%2520automatic%2520seizure%2520detection%2520models%2520toward%2520clinically%2520viable%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.06016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LookAroundNet%3A%20Extending%20Temporal%20Context%20with%20Transformers%20for%20Clinically%20Viable%20EEG%20Seizure%20Detection&entry.906535625=%C3%9E%C3%B3r%20Sverrisson%20and%20Steinn%20Gu%C3%B0mundsson&entry.1292438233=Automated%20seizure%20detection%20from%20electroencephalography%20%28EEG%29%20remains%20difficult%20due%20to%20the%20large%20variability%20of%20seizure%20dynamics%20across%20patients%2C%20recording%20conditions%2C%20and%20clinical%20settings.%20We%20introduce%20LookAroundNet%2C%20a%20transformer-based%20seizure%20detector%20that%20uses%20a%20wider%20temporal%20window%20of%20EEG%20data%20to%20model%20seizure%20activity.%20The%20seizure%20detector%20incorporates%20EEG%20signals%20before%20and%20after%20the%20segment%20of%20interest%2C%20reflecting%20how%20clinicians%20use%20surrounding%20context%20when%20interpreting%20EEG%20recordings.%20We%20evaluate%20the%20proposed%20method%20on%20multiple%20EEG%20datasets%20spanning%20diverse%20clinical%20environments%2C%20patient%20populations%2C%20and%20recording%20modalities%2C%20including%20routine%20clinical%20EEG%20and%20long-term%20ambulatory%20recordings%2C%20in%20order%20to%20study%20performance%20across%20varying%20data%20distributions.%20The%20evaluation%20includes%20publicly%20available%20datasets%20as%20well%20as%20a%20large%20proprietary%20collection%20of%20home%20EEG%20recordings%2C%20providing%20complementary%20views%20of%20controlled%20clinical%20data%20and%20unconstrained%20home-monitoring%20conditions.%20Our%20results%20show%20that%20LookAroundNet%20achieves%20strong%20performance%20across%20datasets%2C%20generalizes%20well%20to%20previously%20unseen%20recording%20conditions%2C%20and%20operates%20with%20computational%20costs%20compatible%20with%20real-world%20clinical%20deployment.%20The%20results%20indicate%20that%20extended%20temporal%20context%2C%20increased%20training%20data%20diversity%2C%20and%20model%20ensembling%20are%20key%20factors%20for%20improving%20performance.%20This%20work%20contributes%20to%20moving%20automatic%20seizure%20detection%20models%20toward%20clinically%20viable%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2601.06016v1&entry.124074799=Read"},
{"title": "Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning", "author": "Sheng-Kai Chen and Jyh-Horng Wu", "abstract": "This paper presents a comprehensive approach to singularity detection and avoidance in UR10 robotic arm path planning through the integration of fuzzy logic safety systems and reinforcement learning algorithms. The proposed system addresses critical challenges in robotic manipulation where singularities can cause loss of control and potential equipment damage. Our hybrid approach combines real-time singularity detection using manipulability measures, condition number analysis, and fuzzy logic decision-making with a stable reinforcement learning framework for adaptive path planning. Experimental results demonstrate a 90% success rate in reaching target positions while maintaining safe distances from singular configurations. The system integrates PyBullet simulation for training data collection and URSim connectivity for real-world deployment.", "link": "http://arxiv.org/abs/2601.05836v1", "date": "2026-01-09", "relevancy": 2.0192, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5156}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Singularity%20Avoidance%20in%20UR10%20Robotic%20Arm%20Path%20Planning%20Using%20Hybrid%20Fuzzy%20Logic%20and%20Reinforcement%20Learning&body=Title%3A%20Intelligent%20Singularity%20Avoidance%20in%20UR10%20Robotic%20Arm%20Path%20Planning%20Using%20Hybrid%20Fuzzy%20Logic%20and%20Reinforcement%20Learning%0AAuthor%3A%20Sheng-Kai%20Chen%20and%20Jyh-Horng%20Wu%0AAbstract%3A%20This%20paper%20presents%20a%20comprehensive%20approach%20to%20singularity%20detection%20and%20avoidance%20in%20UR10%20robotic%20arm%20path%20planning%20through%20the%20integration%20of%20fuzzy%20logic%20safety%20systems%20and%20reinforcement%20learning%20algorithms.%20The%20proposed%20system%20addresses%20critical%20challenges%20in%20robotic%20manipulation%20where%20singularities%20can%20cause%20loss%20of%20control%20and%20potential%20equipment%20damage.%20Our%20hybrid%20approach%20combines%20real-time%20singularity%20detection%20using%20manipulability%20measures%2C%20condition%20number%20analysis%2C%20and%20fuzzy%20logic%20decision-making%20with%20a%20stable%20reinforcement%20learning%20framework%20for%20adaptive%20path%20planning.%20Experimental%20results%20demonstrate%20a%2090%25%20success%20rate%20in%20reaching%20target%20positions%20while%20maintaining%20safe%20distances%20from%20singular%20configurations.%20The%20system%20integrates%20PyBullet%20simulation%20for%20training%20data%20collection%20and%20URSim%20connectivity%20for%20real-world%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Singularity%2520Avoidance%2520in%2520UR10%2520Robotic%2520Arm%2520Path%2520Planning%2520Using%2520Hybrid%2520Fuzzy%2520Logic%2520and%2520Reinforcement%2520Learning%26entry.906535625%3DSheng-Kai%2520Chen%2520and%2520Jyh-Horng%2520Wu%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520comprehensive%2520approach%2520to%2520singularity%2520detection%2520and%2520avoidance%2520in%2520UR10%2520robotic%2520arm%2520path%2520planning%2520through%2520the%2520integration%2520of%2520fuzzy%2520logic%2520safety%2520systems%2520and%2520reinforcement%2520learning%2520algorithms.%2520The%2520proposed%2520system%2520addresses%2520critical%2520challenges%2520in%2520robotic%2520manipulation%2520where%2520singularities%2520can%2520cause%2520loss%2520of%2520control%2520and%2520potential%2520equipment%2520damage.%2520Our%2520hybrid%2520approach%2520combines%2520real-time%2520singularity%2520detection%2520using%2520manipulability%2520measures%252C%2520condition%2520number%2520analysis%252C%2520and%2520fuzzy%2520logic%2520decision-making%2520with%2520a%2520stable%2520reinforcement%2520learning%2520framework%2520for%2520adaptive%2520path%2520planning.%2520Experimental%2520results%2520demonstrate%2520a%252090%2525%2520success%2520rate%2520in%2520reaching%2520target%2520positions%2520while%2520maintaining%2520safe%2520distances%2520from%2520singular%2520configurations.%2520The%2520system%2520integrates%2520PyBullet%2520simulation%2520for%2520training%2520data%2520collection%2520and%2520URSim%2520connectivity%2520for%2520real-world%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Singularity%20Avoidance%20in%20UR10%20Robotic%20Arm%20Path%20Planning%20Using%20Hybrid%20Fuzzy%20Logic%20and%20Reinforcement%20Learning&entry.906535625=Sheng-Kai%20Chen%20and%20Jyh-Horng%20Wu&entry.1292438233=This%20paper%20presents%20a%20comprehensive%20approach%20to%20singularity%20detection%20and%20avoidance%20in%20UR10%20robotic%20arm%20path%20planning%20through%20the%20integration%20of%20fuzzy%20logic%20safety%20systems%20and%20reinforcement%20learning%20algorithms.%20The%20proposed%20system%20addresses%20critical%20challenges%20in%20robotic%20manipulation%20where%20singularities%20can%20cause%20loss%20of%20control%20and%20potential%20equipment%20damage.%20Our%20hybrid%20approach%20combines%20real-time%20singularity%20detection%20using%20manipulability%20measures%2C%20condition%20number%20analysis%2C%20and%20fuzzy%20logic%20decision-making%20with%20a%20stable%20reinforcement%20learning%20framework%20for%20adaptive%20path%20planning.%20Experimental%20results%20demonstrate%20a%2090%25%20success%20rate%20in%20reaching%20target%20positions%20while%20maintaining%20safe%20distances%20from%20singular%20configurations.%20The%20system%20integrates%20PyBullet%20simulation%20for%20training%20data%20collection%20and%20URSim%20connectivity%20for%20real-world%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2601.05836v1&entry.124074799=Read"},
{"title": "Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints", "author": "Adrian Serrano and Erwan Umlil and Ronan Thomas", "abstract": "Deepfake detection systems deployed in real-world environments are subject to adversaries capable of crafting imperceptible perturbations that degrade model performance. While adversarial training is a widely adopted defense, its effectiveness under realistic conditions -- where attackers operate with limited knowledge and mismatched data distributions - remains underexplored. In this work, we extend the DUMB -- Dataset soUrces, Model architecture and Balance - and DUMBer methodology to deepfake detection. We evaluate detectors robustness against adversarial attacks under transferability constraints and cross-dataset configuration to extract real-world insights. Our study spans five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL), three attacks (PGD, FGSM, FPBA), and two datasets (FaceForensics++ and Celeb-DF-V2). We analyze both attacker and defender perspectives mapping results to mismatch scenarios. Experiments show that adversarial training strategies reinforce robustness in the in-distribution cases but can also degrade it under cross-dataset configuration depending on the strategy adopted. These findings highlight the need for case-aware defense strategies in real-world applications exposed to adversarial attacks.", "link": "http://arxiv.org/abs/2601.05986v1", "date": "2026-01-09", "relevancy": 2.018, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5445}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5068}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deepfake%20detectors%20are%20DUMB%3A%20A%20benchmark%20to%20assess%20adversarial%20training%20robustness%20under%20transferability%20constraints&body=Title%3A%20Deepfake%20detectors%20are%20DUMB%3A%20A%20benchmark%20to%20assess%20adversarial%20training%20robustness%20under%20transferability%20constraints%0AAuthor%3A%20Adrian%20Serrano%20and%20Erwan%20Umlil%20and%20Ronan%20Thomas%0AAbstract%3A%20Deepfake%20detection%20systems%20deployed%20in%20real-world%20environments%20are%20subject%20to%20adversaries%20capable%20of%20crafting%20imperceptible%20perturbations%20that%20degrade%20model%20performance.%20While%20adversarial%20training%20is%20a%20widely%20adopted%20defense%2C%20its%20effectiveness%20under%20realistic%20conditions%20--%20where%20attackers%20operate%20with%20limited%20knowledge%20and%20mismatched%20data%20distributions%20-%20remains%20underexplored.%20In%20this%20work%2C%20we%20extend%20the%20DUMB%20--%20Dataset%20soUrces%2C%20Model%20architecture%20and%20Balance%20-%20and%20DUMBer%20methodology%20to%20deepfake%20detection.%20We%20evaluate%20detectors%20robustness%20against%20adversarial%20attacks%20under%20transferability%20constraints%20and%20cross-dataset%20configuration%20to%20extract%20real-world%20insights.%20Our%20study%20spans%20five%20state-of-the-art%20detectors%20%28RECCE%2C%20SRM%2C%20XCeption%2C%20UCF%2C%20SPSL%29%2C%20three%20attacks%20%28PGD%2C%20FGSM%2C%20FPBA%29%2C%20and%20two%20datasets%20%28FaceForensics%2B%2B%20and%20Celeb-DF-V2%29.%20We%20analyze%20both%20attacker%20and%20defender%20perspectives%20mapping%20results%20to%20mismatch%20scenarios.%20Experiments%20show%20that%20adversarial%20training%20strategies%20reinforce%20robustness%20in%20the%20in-distribution%20cases%20but%20can%20also%20degrade%20it%20under%20cross-dataset%20configuration%20depending%20on%20the%20strategy%20adopted.%20These%20findings%20highlight%20the%20need%20for%20case-aware%20defense%20strategies%20in%20real-world%20applications%20exposed%20to%20adversarial%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepfake%2520detectors%2520are%2520DUMB%253A%2520A%2520benchmark%2520to%2520assess%2520adversarial%2520training%2520robustness%2520under%2520transferability%2520constraints%26entry.906535625%3DAdrian%2520Serrano%2520and%2520Erwan%2520Umlil%2520and%2520Ronan%2520Thomas%26entry.1292438233%3DDeepfake%2520detection%2520systems%2520deployed%2520in%2520real-world%2520environments%2520are%2520subject%2520to%2520adversaries%2520capable%2520of%2520crafting%2520imperceptible%2520perturbations%2520that%2520degrade%2520model%2520performance.%2520While%2520adversarial%2520training%2520is%2520a%2520widely%2520adopted%2520defense%252C%2520its%2520effectiveness%2520under%2520realistic%2520conditions%2520--%2520where%2520attackers%2520operate%2520with%2520limited%2520knowledge%2520and%2520mismatched%2520data%2520distributions%2520-%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520DUMB%2520--%2520Dataset%2520soUrces%252C%2520Model%2520architecture%2520and%2520Balance%2520-%2520and%2520DUMBer%2520methodology%2520to%2520deepfake%2520detection.%2520We%2520evaluate%2520detectors%2520robustness%2520against%2520adversarial%2520attacks%2520under%2520transferability%2520constraints%2520and%2520cross-dataset%2520configuration%2520to%2520extract%2520real-world%2520insights.%2520Our%2520study%2520spans%2520five%2520state-of-the-art%2520detectors%2520%2528RECCE%252C%2520SRM%252C%2520XCeption%252C%2520UCF%252C%2520SPSL%2529%252C%2520three%2520attacks%2520%2528PGD%252C%2520FGSM%252C%2520FPBA%2529%252C%2520and%2520two%2520datasets%2520%2528FaceForensics%252B%252B%2520and%2520Celeb-DF-V2%2529.%2520We%2520analyze%2520both%2520attacker%2520and%2520defender%2520perspectives%2520mapping%2520results%2520to%2520mismatch%2520scenarios.%2520Experiments%2520show%2520that%2520adversarial%2520training%2520strategies%2520reinforce%2520robustness%2520in%2520the%2520in-distribution%2520cases%2520but%2520can%2520also%2520degrade%2520it%2520under%2520cross-dataset%2520configuration%2520depending%2520on%2520the%2520strategy%2520adopted.%2520These%2520findings%2520highlight%2520the%2520need%2520for%2520case-aware%2520defense%2520strategies%2520in%2520real-world%2520applications%2520exposed%2520to%2520adversarial%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20detectors%20are%20DUMB%3A%20A%20benchmark%20to%20assess%20adversarial%20training%20robustness%20under%20transferability%20constraints&entry.906535625=Adrian%20Serrano%20and%20Erwan%20Umlil%20and%20Ronan%20Thomas&entry.1292438233=Deepfake%20detection%20systems%20deployed%20in%20real-world%20environments%20are%20subject%20to%20adversaries%20capable%20of%20crafting%20imperceptible%20perturbations%20that%20degrade%20model%20performance.%20While%20adversarial%20training%20is%20a%20widely%20adopted%20defense%2C%20its%20effectiveness%20under%20realistic%20conditions%20--%20where%20attackers%20operate%20with%20limited%20knowledge%20and%20mismatched%20data%20distributions%20-%20remains%20underexplored.%20In%20this%20work%2C%20we%20extend%20the%20DUMB%20--%20Dataset%20soUrces%2C%20Model%20architecture%20and%20Balance%20-%20and%20DUMBer%20methodology%20to%20deepfake%20detection.%20We%20evaluate%20detectors%20robustness%20against%20adversarial%20attacks%20under%20transferability%20constraints%20and%20cross-dataset%20configuration%20to%20extract%20real-world%20insights.%20Our%20study%20spans%20five%20state-of-the-art%20detectors%20%28RECCE%2C%20SRM%2C%20XCeption%2C%20UCF%2C%20SPSL%29%2C%20three%20attacks%20%28PGD%2C%20FGSM%2C%20FPBA%29%2C%20and%20two%20datasets%20%28FaceForensics%2B%2B%20and%20Celeb-DF-V2%29.%20We%20analyze%20both%20attacker%20and%20defender%20perspectives%20mapping%20results%20to%20mismatch%20scenarios.%20Experiments%20show%20that%20adversarial%20training%20strategies%20reinforce%20robustness%20in%20the%20in-distribution%20cases%20but%20can%20also%20degrade%20it%20under%20cross-dataset%20configuration%20depending%20on%20the%20strategy%20adopted.%20These%20findings%20highlight%20the%20need%20for%20case-aware%20defense%20strategies%20in%20real-world%20applications%20exposed%20to%20adversarial%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2601.05986v1&entry.124074799=Read"},
{"title": "ACDZero: MCTS Agent for Mastering Automated Cyber Defense", "author": "Yu Li and Sizhe Tang and Rongqian Chen and Fei Xu Yu and Guangyu Jiang and Mahdi Imani and Nathaniel D. Bastian and Tian Lan", "abstract": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.", "link": "http://arxiv.org/abs/2601.02196v2", "date": "2026-01-09", "relevancy": 2.0179, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5863}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5138}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACDZero%3A%20MCTS%20Agent%20for%20Mastering%20Automated%20Cyber%20Defense&body=Title%3A%20ACDZero%3A%20MCTS%20Agent%20for%20Mastering%20Automated%20Cyber%20Defense%0AAuthor%3A%20Yu%20Li%20and%20Sizhe%20Tang%20and%20Rongqian%20Chen%20and%20Fei%20Xu%20Yu%20and%20Guangyu%20Jiang%20and%20Mahdi%20Imani%20and%20Nathaniel%20D.%20Bastian%20and%20Tian%20Lan%0AAbstract%3A%20Automated%20cyber%20defense%20%28ACD%29%20seeks%20to%20protect%20computer%20networks%20with%20minimal%20or%20no%20human%20intervention%2C%20reacting%20to%20intrusions%20by%20taking%20corrective%20actions%20such%20as%20isolating%20hosts%2C%20resetting%20services%2C%20deploying%20decoys%2C%20or%20updating%20access%20controls.%20However%2C%20existing%20approaches%20for%20ACD%2C%20such%20as%20deep%20reinforcement%20learning%20%28RL%29%2C%20often%20face%20difficult%20exploration%20in%20complex%20networks%20with%20large%20decision/state%20spaces%20and%20thus%20require%20an%20expensive%20amount%20of%20samples.%20Inspired%20by%20the%20need%20to%20learn%20sample-efficient%20defense%20policies%2C%20we%20frame%20ACD%20in%20CAGE%20Challenge%204%20%28CAGE-4%20/%20CC4%29%20as%20a%20context-based%20partially%20observable%20Markov%20decision%20problem%20and%20propose%20a%20planning-centric%20defense%20policy%20based%20on%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29.%20It%20explicitly%20models%20the%20exploration-exploitation%20tradeoff%20in%20ACD%20and%20uses%20statistical%20sampling%20to%20guide%20exploration%20and%20decision%20making.%20We%20make%20novel%20use%20of%20graph%20neural%20networks%20%28GNNs%29%20to%20embed%20observations%20from%20the%20network%20as%20attributed%20graphs%2C%20to%20enable%20permutation-invariant%20reasoning%20over%20hosts%20and%20their%20relationships.%20To%20make%20our%20solution%20practical%20in%20complex%20search%20spaces%2C%20we%20guide%20MCTS%20with%20learned%20graph%20embeddings%20and%20priors%20over%20graph-edit%20actions%2C%20combining%20model-free%20generalization%20and%20policy%20distillation%20with%20look-ahead%20planning.%20We%20evaluate%20the%20resulting%20agent%20on%20CC4%20scenarios%20involving%20diverse%20network%20structures%20and%20adversary%20behaviors%2C%20and%20show%20that%20our%20search-guided%2C%20graph-embedding-based%20planning%20improves%20defense%20reward%20and%20robustness%20relative%20to%20state-of-the-art%20RL%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACDZero%253A%2520MCTS%2520Agent%2520for%2520Mastering%2520Automated%2520Cyber%2520Defense%26entry.906535625%3DYu%2520Li%2520and%2520Sizhe%2520Tang%2520and%2520Rongqian%2520Chen%2520and%2520Fei%2520Xu%2520Yu%2520and%2520Guangyu%2520Jiang%2520and%2520Mahdi%2520Imani%2520and%2520Nathaniel%2520D.%2520Bastian%2520and%2520Tian%2520Lan%26entry.1292438233%3DAutomated%2520cyber%2520defense%2520%2528ACD%2529%2520seeks%2520to%2520protect%2520computer%2520networks%2520with%2520minimal%2520or%2520no%2520human%2520intervention%252C%2520reacting%2520to%2520intrusions%2520by%2520taking%2520corrective%2520actions%2520such%2520as%2520isolating%2520hosts%252C%2520resetting%2520services%252C%2520deploying%2520decoys%252C%2520or%2520updating%2520access%2520controls.%2520However%252C%2520existing%2520approaches%2520for%2520ACD%252C%2520such%2520as%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520often%2520face%2520difficult%2520exploration%2520in%2520complex%2520networks%2520with%2520large%2520decision/state%2520spaces%2520and%2520thus%2520require%2520an%2520expensive%2520amount%2520of%2520samples.%2520Inspired%2520by%2520the%2520need%2520to%2520learn%2520sample-efficient%2520defense%2520policies%252C%2520we%2520frame%2520ACD%2520in%2520CAGE%2520Challenge%25204%2520%2528CAGE-4%2520/%2520CC4%2529%2520as%2520a%2520context-based%2520partially%2520observable%2520Markov%2520decision%2520problem%2520and%2520propose%2520a%2520planning-centric%2520defense%2520policy%2520based%2520on%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529.%2520It%2520explicitly%2520models%2520the%2520exploration-exploitation%2520tradeoff%2520in%2520ACD%2520and%2520uses%2520statistical%2520sampling%2520to%2520guide%2520exploration%2520and%2520decision%2520making.%2520We%2520make%2520novel%2520use%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520to%2520embed%2520observations%2520from%2520the%2520network%2520as%2520attributed%2520graphs%252C%2520to%2520enable%2520permutation-invariant%2520reasoning%2520over%2520hosts%2520and%2520their%2520relationships.%2520To%2520make%2520our%2520solution%2520practical%2520in%2520complex%2520search%2520spaces%252C%2520we%2520guide%2520MCTS%2520with%2520learned%2520graph%2520embeddings%2520and%2520priors%2520over%2520graph-edit%2520actions%252C%2520combining%2520model-free%2520generalization%2520and%2520policy%2520distillation%2520with%2520look-ahead%2520planning.%2520We%2520evaluate%2520the%2520resulting%2520agent%2520on%2520CC4%2520scenarios%2520involving%2520diverse%2520network%2520structures%2520and%2520adversary%2520behaviors%252C%2520and%2520show%2520that%2520our%2520search-guided%252C%2520graph-embedding-based%2520planning%2520improves%2520defense%2520reward%2520and%2520robustness%2520relative%2520to%2520state-of-the-art%2520RL%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACDZero%3A%20MCTS%20Agent%20for%20Mastering%20Automated%20Cyber%20Defense&entry.906535625=Yu%20Li%20and%20Sizhe%20Tang%20and%20Rongqian%20Chen%20and%20Fei%20Xu%20Yu%20and%20Guangyu%20Jiang%20and%20Mahdi%20Imani%20and%20Nathaniel%20D.%20Bastian%20and%20Tian%20Lan&entry.1292438233=Automated%20cyber%20defense%20%28ACD%29%20seeks%20to%20protect%20computer%20networks%20with%20minimal%20or%20no%20human%20intervention%2C%20reacting%20to%20intrusions%20by%20taking%20corrective%20actions%20such%20as%20isolating%20hosts%2C%20resetting%20services%2C%20deploying%20decoys%2C%20or%20updating%20access%20controls.%20However%2C%20existing%20approaches%20for%20ACD%2C%20such%20as%20deep%20reinforcement%20learning%20%28RL%29%2C%20often%20face%20difficult%20exploration%20in%20complex%20networks%20with%20large%20decision/state%20spaces%20and%20thus%20require%20an%20expensive%20amount%20of%20samples.%20Inspired%20by%20the%20need%20to%20learn%20sample-efficient%20defense%20policies%2C%20we%20frame%20ACD%20in%20CAGE%20Challenge%204%20%28CAGE-4%20/%20CC4%29%20as%20a%20context-based%20partially%20observable%20Markov%20decision%20problem%20and%20propose%20a%20planning-centric%20defense%20policy%20based%20on%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29.%20It%20explicitly%20models%20the%20exploration-exploitation%20tradeoff%20in%20ACD%20and%20uses%20statistical%20sampling%20to%20guide%20exploration%20and%20decision%20making.%20We%20make%20novel%20use%20of%20graph%20neural%20networks%20%28GNNs%29%20to%20embed%20observations%20from%20the%20network%20as%20attributed%20graphs%2C%20to%20enable%20permutation-invariant%20reasoning%20over%20hosts%20and%20their%20relationships.%20To%20make%20our%20solution%20practical%20in%20complex%20search%20spaces%2C%20we%20guide%20MCTS%20with%20learned%20graph%20embeddings%20and%20priors%20over%20graph-edit%20actions%2C%20combining%20model-free%20generalization%20and%20policy%20distillation%20with%20look-ahead%20planning.%20We%20evaluate%20the%20resulting%20agent%20on%20CC4%20scenarios%20involving%20diverse%20network%20structures%20and%20adversary%20behaviors%2C%20and%20show%20that%20our%20search-guided%2C%20graph-embedding-based%20planning%20improves%20defense%20reward%20and%20robustness%20relative%20to%20state-of-the-art%20RL%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.02196v2&entry.124074799=Read"},
{"title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management", "author": "Ruizhe Zhang and Xinke Jiang and Zhibang Yang and Zhixin Zhang and Jiaran Gao and Yuzhen Xiao and Hongbin Lai and Xu Chu and Junfeng Zhao and Yasha Wang", "abstract": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.", "link": "http://arxiv.org/abs/2601.05890v1", "date": "2026-01-09", "relevancy": 2.0109, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5313}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StackPlanner%3A%20A%20Centralized%20Hierarchical%20Multi-Agent%20System%20with%20Task-Experience%20Memory%20Management&body=Title%3A%20StackPlanner%3A%20A%20Centralized%20Hierarchical%20Multi-Agent%20System%20with%20Task-Experience%20Memory%20Management%0AAuthor%3A%20Ruizhe%20Zhang%20and%20Xinke%20Jiang%20and%20Zhibang%20Yang%20and%20Zhixin%20Zhang%20and%20Jiaran%20Gao%20and%20Yuzhen%20Xiao%20and%20Hongbin%20Lai%20and%20Xu%20Chu%20and%20Junfeng%20Zhao%20and%20Yasha%20Wang%0AAbstract%3A%20Multi-agent%20systems%20based%20on%20large%20language%20models%2C%20particularly%20centralized%20architectures%2C%20have%20recently%20shown%20strong%20potential%20for%20complex%20and%20knowledge-intensive%20tasks.%20However%2C%20central%20agents%20often%20suffer%20from%20unstable%20long-horizon%20collaboration%20due%20to%20the%20lack%20of%20memory%20management%2C%20leading%20to%20context%20bloat%2C%20error%20accumulation%2C%20and%20poor%20cross-task%20generalization.%20To%20address%20both%20task-level%20memory%20inefficiency%20and%20the%20inability%20to%20reuse%20coordination%20experience%2C%20we%20propose%20StackPlanner%2C%20a%20hierarchical%20multi-agent%20framework%20with%20explicit%20memory%20control.%20StackPlanner%20addresses%20these%20challenges%20by%20decoupling%20high-level%20coordination%20from%20subtask%20execution%20with%20active%20task-level%20memory%20control%2C%20and%20by%20learning%20to%20retrieve%20and%20exploit%20reusable%20coordination%20experience%20via%20structured%20experience%20memory%20and%20reinforcement%20learning.%20Experiments%20on%20multiple%20deep-search%20and%20agent%20system%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20enabling%20reliable%20long-horizon%20multi-agent%20collaboration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStackPlanner%253A%2520A%2520Centralized%2520Hierarchical%2520Multi-Agent%2520System%2520with%2520Task-Experience%2520Memory%2520Management%26entry.906535625%3DRuizhe%2520Zhang%2520and%2520Xinke%2520Jiang%2520and%2520Zhibang%2520Yang%2520and%2520Zhixin%2520Zhang%2520and%2520Jiaran%2520Gao%2520and%2520Yuzhen%2520Xiao%2520and%2520Hongbin%2520Lai%2520and%2520Xu%2520Chu%2520and%2520Junfeng%2520Zhao%2520and%2520Yasha%2520Wang%26entry.1292438233%3DMulti-agent%2520systems%2520based%2520on%2520large%2520language%2520models%252C%2520particularly%2520centralized%2520architectures%252C%2520have%2520recently%2520shown%2520strong%2520potential%2520for%2520complex%2520and%2520knowledge-intensive%2520tasks.%2520However%252C%2520central%2520agents%2520often%2520suffer%2520from%2520unstable%2520long-horizon%2520collaboration%2520due%2520to%2520the%2520lack%2520of%2520memory%2520management%252C%2520leading%2520to%2520context%2520bloat%252C%2520error%2520accumulation%252C%2520and%2520poor%2520cross-task%2520generalization.%2520To%2520address%2520both%2520task-level%2520memory%2520inefficiency%2520and%2520the%2520inability%2520to%2520reuse%2520coordination%2520experience%252C%2520we%2520propose%2520StackPlanner%252C%2520a%2520hierarchical%2520multi-agent%2520framework%2520with%2520explicit%2520memory%2520control.%2520StackPlanner%2520addresses%2520these%2520challenges%2520by%2520decoupling%2520high-level%2520coordination%2520from%2520subtask%2520execution%2520with%2520active%2520task-level%2520memory%2520control%252C%2520and%2520by%2520learning%2520to%2520retrieve%2520and%2520exploit%2520reusable%2520coordination%2520experience%2520via%2520structured%2520experience%2520memory%2520and%2520reinforcement%2520learning.%2520Experiments%2520on%2520multiple%2520deep-search%2520and%2520agent%2520system%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520enabling%2520reliable%2520long-horizon%2520multi-agent%2520collaboration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StackPlanner%3A%20A%20Centralized%20Hierarchical%20Multi-Agent%20System%20with%20Task-Experience%20Memory%20Management&entry.906535625=Ruizhe%20Zhang%20and%20Xinke%20Jiang%20and%20Zhibang%20Yang%20and%20Zhixin%20Zhang%20and%20Jiaran%20Gao%20and%20Yuzhen%20Xiao%20and%20Hongbin%20Lai%20and%20Xu%20Chu%20and%20Junfeng%20Zhao%20and%20Yasha%20Wang&entry.1292438233=Multi-agent%20systems%20based%20on%20large%20language%20models%2C%20particularly%20centralized%20architectures%2C%20have%20recently%20shown%20strong%20potential%20for%20complex%20and%20knowledge-intensive%20tasks.%20However%2C%20central%20agents%20often%20suffer%20from%20unstable%20long-horizon%20collaboration%20due%20to%20the%20lack%20of%20memory%20management%2C%20leading%20to%20context%20bloat%2C%20error%20accumulation%2C%20and%20poor%20cross-task%20generalization.%20To%20address%20both%20task-level%20memory%20inefficiency%20and%20the%20inability%20to%20reuse%20coordination%20experience%2C%20we%20propose%20StackPlanner%2C%20a%20hierarchical%20multi-agent%20framework%20with%20explicit%20memory%20control.%20StackPlanner%20addresses%20these%20challenges%20by%20decoupling%20high-level%20coordination%20from%20subtask%20execution%20with%20active%20task-level%20memory%20control%2C%20and%20by%20learning%20to%20retrieve%20and%20exploit%20reusable%20coordination%20experience%20via%20structured%20experience%20memory%20and%20reinforcement%20learning.%20Experiments%20on%20multiple%20deep-search%20and%20agent%20system%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20enabling%20reliable%20long-horizon%20multi-agent%20collaboration.&entry.1838667208=http%3A//arxiv.org/abs/2601.05890v1&entry.124074799=Read"},
{"title": "Foundation models for high-energy physics", "author": "Anna Hallin", "abstract": "The rise of foundation models -- large, pretrained machine learning models that can be finetuned to a variety of tasks -- has revolutionized the fields of natural language processing and computer vision. In high-energy physics, the question of whether these models can be implemented directly in physics research, or even built from scratch, tailored for particle physics data, has generated an increasing amount of attention. This review, which is the first on the topic of foundation models in high-energy physics, summarizes and discusses the research that has been published in the field so far.", "link": "http://arxiv.org/abs/2509.21434v2", "date": "2026-01-09", "relevancy": 2.0046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20models%20for%20high-energy%20physics&body=Title%3A%20Foundation%20models%20for%20high-energy%20physics%0AAuthor%3A%20Anna%20Hallin%0AAbstract%3A%20The%20rise%20of%20foundation%20models%20--%20large%2C%20pretrained%20machine%20learning%20models%20that%20can%20be%20finetuned%20to%20a%20variety%20of%20tasks%20--%20has%20revolutionized%20the%20fields%20of%20natural%20language%20processing%20and%20computer%20vision.%20In%20high-energy%20physics%2C%20the%20question%20of%20whether%20these%20models%20can%20be%20implemented%20directly%20in%20physics%20research%2C%20or%20even%20built%20from%20scratch%2C%20tailored%20for%20particle%20physics%20data%2C%20has%20generated%20an%20increasing%20amount%20of%20attention.%20This%20review%2C%20which%20is%20the%20first%20on%20the%20topic%20of%20foundation%20models%20in%20high-energy%20physics%2C%20summarizes%20and%20discusses%20the%20research%20that%20has%20been%20published%20in%20the%20field%20so%20far.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520models%2520for%2520high-energy%2520physics%26entry.906535625%3DAnna%2520Hallin%26entry.1292438233%3DThe%2520rise%2520of%2520foundation%2520models%2520--%2520large%252C%2520pretrained%2520machine%2520learning%2520models%2520that%2520can%2520be%2520finetuned%2520to%2520a%2520variety%2520of%2520tasks%2520--%2520has%2520revolutionized%2520the%2520fields%2520of%2520natural%2520language%2520processing%2520and%2520computer%2520vision.%2520In%2520high-energy%2520physics%252C%2520the%2520question%2520of%2520whether%2520these%2520models%2520can%2520be%2520implemented%2520directly%2520in%2520physics%2520research%252C%2520or%2520even%2520built%2520from%2520scratch%252C%2520tailored%2520for%2520particle%2520physics%2520data%252C%2520has%2520generated%2520an%2520increasing%2520amount%2520of%2520attention.%2520This%2520review%252C%2520which%2520is%2520the%2520first%2520on%2520the%2520topic%2520of%2520foundation%2520models%2520in%2520high-energy%2520physics%252C%2520summarizes%2520and%2520discusses%2520the%2520research%2520that%2520has%2520been%2520published%2520in%2520the%2520field%2520so%2520far.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20models%20for%20high-energy%20physics&entry.906535625=Anna%20Hallin&entry.1292438233=The%20rise%20of%20foundation%20models%20--%20large%2C%20pretrained%20machine%20learning%20models%20that%20can%20be%20finetuned%20to%20a%20variety%20of%20tasks%20--%20has%20revolutionized%20the%20fields%20of%20natural%20language%20processing%20and%20computer%20vision.%20In%20high-energy%20physics%2C%20the%20question%20of%20whether%20these%20models%20can%20be%20implemented%20directly%20in%20physics%20research%2C%20or%20even%20built%20from%20scratch%2C%20tailored%20for%20particle%20physics%20data%2C%20has%20generated%20an%20increasing%20amount%20of%20attention.%20This%20review%2C%20which%20is%20the%20first%20on%20the%20topic%20of%20foundation%20models%20in%20high-energy%20physics%2C%20summarizes%20and%20discusses%20the%20research%20that%20has%20been%20published%20in%20the%20field%20so%20far.&entry.1838667208=http%3A//arxiv.org/abs/2509.21434v2&entry.124074799=Read"},
{"title": "The Computational Complexity of Counting Linear Regions in ReLU Neural Networks", "author": "Moritz Stargalla and Christoph Hertrich and Daniel Reichman", "abstract": "An established measure of the expressive power of a given ReLU neural network is the number of linear regions into which it partitions the input space. There exist many different, non-equivalent definitions of what a linear region actually is. We systematically assess which papers use which definitions and discuss how they relate to each other. We then analyze the computational complexity of counting the number of such regions for the various definitions. Generally, this turns out to be an intractable problem. We prove NP- and #P-hardness results already for networks with one hidden layer and strong hardness of approximation results for two or more hidden layers. Finally, on the algorithmic side, we demonstrate that counting linear regions can at least be achieved in polynomial space for some common definitions.", "link": "http://arxiv.org/abs/2505.16716v3", "date": "2026-01-09", "relevancy": 2.0004, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4189}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Computational%20Complexity%20of%20Counting%20Linear%20Regions%20in%20ReLU%20Neural%20Networks&body=Title%3A%20The%20Computational%20Complexity%20of%20Counting%20Linear%20Regions%20in%20ReLU%20Neural%20Networks%0AAuthor%3A%20Moritz%20Stargalla%20and%20Christoph%20Hertrich%20and%20Daniel%20Reichman%0AAbstract%3A%20An%20established%20measure%20of%20the%20expressive%20power%20of%20a%20given%20ReLU%20neural%20network%20is%20the%20number%20of%20linear%20regions%20into%20which%20it%20partitions%20the%20input%20space.%20There%20exist%20many%20different%2C%20non-equivalent%20definitions%20of%20what%20a%20linear%20region%20actually%20is.%20We%20systematically%20assess%20which%20papers%20use%20which%20definitions%20and%20discuss%20how%20they%20relate%20to%20each%20other.%20We%20then%20analyze%20the%20computational%20complexity%20of%20counting%20the%20number%20of%20such%20regions%20for%20the%20various%20definitions.%20Generally%2C%20this%20turns%20out%20to%20be%20an%20intractable%20problem.%20We%20prove%20NP-%20and%20%23P-hardness%20results%20already%20for%20networks%20with%20one%20hidden%20layer%20and%20strong%20hardness%20of%20approximation%20results%20for%20two%20or%20more%20hidden%20layers.%20Finally%2C%20on%20the%20algorithmic%20side%2C%20we%20demonstrate%20that%20counting%20linear%20regions%20can%20at%20least%20be%20achieved%20in%20polynomial%20space%20for%20some%20common%20definitions.%0ALink%3A%20http%3A//arxiv.org/abs/2505.16716v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Computational%2520Complexity%2520of%2520Counting%2520Linear%2520Regions%2520in%2520ReLU%2520Neural%2520Networks%26entry.906535625%3DMoritz%2520Stargalla%2520and%2520Christoph%2520Hertrich%2520and%2520Daniel%2520Reichman%26entry.1292438233%3DAn%2520established%2520measure%2520of%2520the%2520expressive%2520power%2520of%2520a%2520given%2520ReLU%2520neural%2520network%2520is%2520the%2520number%2520of%2520linear%2520regions%2520into%2520which%2520it%2520partitions%2520the%2520input%2520space.%2520There%2520exist%2520many%2520different%252C%2520non-equivalent%2520definitions%2520of%2520what%2520a%2520linear%2520region%2520actually%2520is.%2520We%2520systematically%2520assess%2520which%2520papers%2520use%2520which%2520definitions%2520and%2520discuss%2520how%2520they%2520relate%2520to%2520each%2520other.%2520We%2520then%2520analyze%2520the%2520computational%2520complexity%2520of%2520counting%2520the%2520number%2520of%2520such%2520regions%2520for%2520the%2520various%2520definitions.%2520Generally%252C%2520this%2520turns%2520out%2520to%2520be%2520an%2520intractable%2520problem.%2520We%2520prove%2520NP-%2520and%2520%2523P-hardness%2520results%2520already%2520for%2520networks%2520with%2520one%2520hidden%2520layer%2520and%2520strong%2520hardness%2520of%2520approximation%2520results%2520for%2520two%2520or%2520more%2520hidden%2520layers.%2520Finally%252C%2520on%2520the%2520algorithmic%2520side%252C%2520we%2520demonstrate%2520that%2520counting%2520linear%2520regions%2520can%2520at%2520least%2520be%2520achieved%2520in%2520polynomial%2520space%2520for%2520some%2520common%2520definitions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16716v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Computational%20Complexity%20of%20Counting%20Linear%20Regions%20in%20ReLU%20Neural%20Networks&entry.906535625=Moritz%20Stargalla%20and%20Christoph%20Hertrich%20and%20Daniel%20Reichman&entry.1292438233=An%20established%20measure%20of%20the%20expressive%20power%20of%20a%20given%20ReLU%20neural%20network%20is%20the%20number%20of%20linear%20regions%20into%20which%20it%20partitions%20the%20input%20space.%20There%20exist%20many%20different%2C%20non-equivalent%20definitions%20of%20what%20a%20linear%20region%20actually%20is.%20We%20systematically%20assess%20which%20papers%20use%20which%20definitions%20and%20discuss%20how%20they%20relate%20to%20each%20other.%20We%20then%20analyze%20the%20computational%20complexity%20of%20counting%20the%20number%20of%20such%20regions%20for%20the%20various%20definitions.%20Generally%2C%20this%20turns%20out%20to%20be%20an%20intractable%20problem.%20We%20prove%20NP-%20and%20%23P-hardness%20results%20already%20for%20networks%20with%20one%20hidden%20layer%20and%20strong%20hardness%20of%20approximation%20results%20for%20two%20or%20more%20hidden%20layers.%20Finally%2C%20on%20the%20algorithmic%20side%2C%20we%20demonstrate%20that%20counting%20linear%20regions%20can%20at%20least%20be%20achieved%20in%20polynomial%20space%20for%20some%20common%20definitions.&entry.1838667208=http%3A//arxiv.org/abs/2505.16716v3&entry.124074799=Read"},
{"title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world", "author": "E. Middell and L. Carlton and S. Moradi and T. Codina and T. Fischer and J. Cutler and S. Kelley and J. Behrendt and T. Dissanayake and N. Harmening and M. A. Y\u00fccel and D. A. Boas and A. von L\u00fchmann", "abstract": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.", "link": "http://arxiv.org/abs/2601.05923v1", "date": "2026-01-09", "relevancy": 1.9932, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cedalion%20Tutorial%3A%20A%20Python-based%20framework%20for%20comprehensive%20analysis%20of%20multimodal%20fNIRS%20%26%20DOT%20from%20the%20lab%20to%20the%20everyday%20world&body=Title%3A%20Cedalion%20Tutorial%3A%20A%20Python-based%20framework%20for%20comprehensive%20analysis%20of%20multimodal%20fNIRS%20%26%20DOT%20from%20the%20lab%20to%20the%20everyday%20world%0AAuthor%3A%20E.%20Middell%20and%20L.%20Carlton%20and%20S.%20Moradi%20and%20T.%20Codina%20and%20T.%20Fischer%20and%20J.%20Cutler%20and%20S.%20Kelley%20and%20J.%20Behrendt%20and%20T.%20Dissanayake%20and%20N.%20Harmening%20and%20M.%20A.%20Y%C3%BCcel%20and%20D.%20A.%20Boas%20and%20A.%20von%20L%C3%BChmann%0AAbstract%3A%20Functional%20near-infrared%20spectroscopy%20%28fNIRS%29%20and%20diffuse%20optical%20tomography%20%28DOT%29%20are%20rapidly%20evolving%20toward%20wearable%2C%20multimodal%2C%20and%20data-driven%2C%20AI-supported%20neuroimaging%20in%20the%20everyday%20world.%20However%2C%20current%20analytical%20tools%20are%20fragmented%20across%20platforms%2C%20limiting%20reproducibility%2C%20interoperability%2C%20and%20integration%20with%20modern%20machine%20learning%20%28ML%29%20workflows.%20Cedalion%20is%20a%20Python-based%20open-source%20framework%20designed%20to%20unify%20advanced%20model-based%20and%20data-driven%20analysis%20of%20multimodal%20fNIRS%20and%20DOT%20data%20within%20a%20reproducible%2C%20extensible%2C%20and%20community-driven%20environment.%20Cedalion%20integrates%20forward%20modelling%2C%20photogrammetric%20optode%20co-registration%2C%20signal%20processing%2C%20GLM%20Analysis%2C%20DOT%20image%20reconstruction%2C%20and%20ML-based%20data-driven%20methods%20within%20a%20single%20standardized%20architecture%20based%20on%20the%20Python%20ecosystem.%20It%20adheres%20to%20SNIRF%20and%20BIDS%20standards%2C%20supports%20cloud-executable%20Jupyter%20notebooks%2C%20and%20provides%20containerized%20workflows%20for%20scalable%2C%20fully%20reproducible%20analysis%20pipelines%20that%20can%20be%20provided%20alongside%20original%20research%20publications.%20Cedalion%20connects%20established%20optical-neuroimaging%20pipelines%20with%20ML%20frameworks%20such%20as%20scikit-learn%20and%20PyTorch%2C%20enabling%20seamless%20multimodal%20fusion%20with%20EEG%2C%20MEG%2C%20and%20physiological%20data.%20It%20implements%20validated%20algorithms%20for%20signal-quality%20assessment%2C%20motion%20correction%2C%20GLM%20modelling%2C%20and%20DOT%20reconstruction%2C%20complemented%20by%20modules%20for%20simulation%2C%20data%20augmentation%2C%20and%20multimodal%20physiology%20analysis.%20Automated%20documentation%20links%20each%20method%20to%20its%20source%20publication%2C%20and%20continuous-integration%20testing%20ensures%20robustness.%20This%20tutorial%20paper%20provides%20seven%20fully%20executable%20notebooks%20that%20demonstrate%20core%20features.%20Cedalion%20offers%20an%20open%2C%20transparent%2C%20and%20community%20extensible%20foundation%20that%20supports%20reproducible%2C%20scalable%2C%20cloud-%20and%20ML-ready%20fNIRS/DOT%20workflows%20for%20laboratory-based%20and%20real-world%20neuroimaging.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCedalion%2520Tutorial%253A%2520A%2520Python-based%2520framework%2520for%2520comprehensive%2520analysis%2520of%2520multimodal%2520fNIRS%2520%2526%2520DOT%2520from%2520the%2520lab%2520to%2520the%2520everyday%2520world%26entry.906535625%3DE.%2520Middell%2520and%2520L.%2520Carlton%2520and%2520S.%2520Moradi%2520and%2520T.%2520Codina%2520and%2520T.%2520Fischer%2520and%2520J.%2520Cutler%2520and%2520S.%2520Kelley%2520and%2520J.%2520Behrendt%2520and%2520T.%2520Dissanayake%2520and%2520N.%2520Harmening%2520and%2520M.%2520A.%2520Y%25C3%25BCcel%2520and%2520D.%2520A.%2520Boas%2520and%2520A.%2520von%2520L%25C3%25BChmann%26entry.1292438233%3DFunctional%2520near-infrared%2520spectroscopy%2520%2528fNIRS%2529%2520and%2520diffuse%2520optical%2520tomography%2520%2528DOT%2529%2520are%2520rapidly%2520evolving%2520toward%2520wearable%252C%2520multimodal%252C%2520and%2520data-driven%252C%2520AI-supported%2520neuroimaging%2520in%2520the%2520everyday%2520world.%2520However%252C%2520current%2520analytical%2520tools%2520are%2520fragmented%2520across%2520platforms%252C%2520limiting%2520reproducibility%252C%2520interoperability%252C%2520and%2520integration%2520with%2520modern%2520machine%2520learning%2520%2528ML%2529%2520workflows.%2520Cedalion%2520is%2520a%2520Python-based%2520open-source%2520framework%2520designed%2520to%2520unify%2520advanced%2520model-based%2520and%2520data-driven%2520analysis%2520of%2520multimodal%2520fNIRS%2520and%2520DOT%2520data%2520within%2520a%2520reproducible%252C%2520extensible%252C%2520and%2520community-driven%2520environment.%2520Cedalion%2520integrates%2520forward%2520modelling%252C%2520photogrammetric%2520optode%2520co-registration%252C%2520signal%2520processing%252C%2520GLM%2520Analysis%252C%2520DOT%2520image%2520reconstruction%252C%2520and%2520ML-based%2520data-driven%2520methods%2520within%2520a%2520single%2520standardized%2520architecture%2520based%2520on%2520the%2520Python%2520ecosystem.%2520It%2520adheres%2520to%2520SNIRF%2520and%2520BIDS%2520standards%252C%2520supports%2520cloud-executable%2520Jupyter%2520notebooks%252C%2520and%2520provides%2520containerized%2520workflows%2520for%2520scalable%252C%2520fully%2520reproducible%2520analysis%2520pipelines%2520that%2520can%2520be%2520provided%2520alongside%2520original%2520research%2520publications.%2520Cedalion%2520connects%2520established%2520optical-neuroimaging%2520pipelines%2520with%2520ML%2520frameworks%2520such%2520as%2520scikit-learn%2520and%2520PyTorch%252C%2520enabling%2520seamless%2520multimodal%2520fusion%2520with%2520EEG%252C%2520MEG%252C%2520and%2520physiological%2520data.%2520It%2520implements%2520validated%2520algorithms%2520for%2520signal-quality%2520assessment%252C%2520motion%2520correction%252C%2520GLM%2520modelling%252C%2520and%2520DOT%2520reconstruction%252C%2520complemented%2520by%2520modules%2520for%2520simulation%252C%2520data%2520augmentation%252C%2520and%2520multimodal%2520physiology%2520analysis.%2520Automated%2520documentation%2520links%2520each%2520method%2520to%2520its%2520source%2520publication%252C%2520and%2520continuous-integration%2520testing%2520ensures%2520robustness.%2520This%2520tutorial%2520paper%2520provides%2520seven%2520fully%2520executable%2520notebooks%2520that%2520demonstrate%2520core%2520features.%2520Cedalion%2520offers%2520an%2520open%252C%2520transparent%252C%2520and%2520community%2520extensible%2520foundation%2520that%2520supports%2520reproducible%252C%2520scalable%252C%2520cloud-%2520and%2520ML-ready%2520fNIRS/DOT%2520workflows%2520for%2520laboratory-based%2520and%2520real-world%2520neuroimaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cedalion%20Tutorial%3A%20A%20Python-based%20framework%20for%20comprehensive%20analysis%20of%20multimodal%20fNIRS%20%26%20DOT%20from%20the%20lab%20to%20the%20everyday%20world&entry.906535625=E.%20Middell%20and%20L.%20Carlton%20and%20S.%20Moradi%20and%20T.%20Codina%20and%20T.%20Fischer%20and%20J.%20Cutler%20and%20S.%20Kelley%20and%20J.%20Behrendt%20and%20T.%20Dissanayake%20and%20N.%20Harmening%20and%20M.%20A.%20Y%C3%BCcel%20and%20D.%20A.%20Boas%20and%20A.%20von%20L%C3%BChmann&entry.1292438233=Functional%20near-infrared%20spectroscopy%20%28fNIRS%29%20and%20diffuse%20optical%20tomography%20%28DOT%29%20are%20rapidly%20evolving%20toward%20wearable%2C%20multimodal%2C%20and%20data-driven%2C%20AI-supported%20neuroimaging%20in%20the%20everyday%20world.%20However%2C%20current%20analytical%20tools%20are%20fragmented%20across%20platforms%2C%20limiting%20reproducibility%2C%20interoperability%2C%20and%20integration%20with%20modern%20machine%20learning%20%28ML%29%20workflows.%20Cedalion%20is%20a%20Python-based%20open-source%20framework%20designed%20to%20unify%20advanced%20model-based%20and%20data-driven%20analysis%20of%20multimodal%20fNIRS%20and%20DOT%20data%20within%20a%20reproducible%2C%20extensible%2C%20and%20community-driven%20environment.%20Cedalion%20integrates%20forward%20modelling%2C%20photogrammetric%20optode%20co-registration%2C%20signal%20processing%2C%20GLM%20Analysis%2C%20DOT%20image%20reconstruction%2C%20and%20ML-based%20data-driven%20methods%20within%20a%20single%20standardized%20architecture%20based%20on%20the%20Python%20ecosystem.%20It%20adheres%20to%20SNIRF%20and%20BIDS%20standards%2C%20supports%20cloud-executable%20Jupyter%20notebooks%2C%20and%20provides%20containerized%20workflows%20for%20scalable%2C%20fully%20reproducible%20analysis%20pipelines%20that%20can%20be%20provided%20alongside%20original%20research%20publications.%20Cedalion%20connects%20established%20optical-neuroimaging%20pipelines%20with%20ML%20frameworks%20such%20as%20scikit-learn%20and%20PyTorch%2C%20enabling%20seamless%20multimodal%20fusion%20with%20EEG%2C%20MEG%2C%20and%20physiological%20data.%20It%20implements%20validated%20algorithms%20for%20signal-quality%20assessment%2C%20motion%20correction%2C%20GLM%20modelling%2C%20and%20DOT%20reconstruction%2C%20complemented%20by%20modules%20for%20simulation%2C%20data%20augmentation%2C%20and%20multimodal%20physiology%20analysis.%20Automated%20documentation%20links%20each%20method%20to%20its%20source%20publication%2C%20and%20continuous-integration%20testing%20ensures%20robustness.%20This%20tutorial%20paper%20provides%20seven%20fully%20executable%20notebooks%20that%20demonstrate%20core%20features.%20Cedalion%20offers%20an%20open%2C%20transparent%2C%20and%20community%20extensible%20foundation%20that%20supports%20reproducible%2C%20scalable%2C%20cloud-%20and%20ML-ready%20fNIRS/DOT%20workflows%20for%20laboratory-based%20and%20real-world%20neuroimaging.&entry.1838667208=http%3A//arxiv.org/abs/2601.05923v1&entry.124074799=Read"},
{"title": "SceneFoundry: Generating Interactive Infinite 3D Worlds", "author": "ChunTeng Chen and YiChen Hsu and YiWen Liu and WeiFang Sun and TsaiChing Ni and ChunYi Lee and Min Sun and YuanFu Yang", "abstract": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.", "link": "http://arxiv.org/abs/2601.05810v1", "date": "2026-01-09", "relevancy": 1.9849, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6977}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6771}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneFoundry%3A%20Generating%20Interactive%20Infinite%203D%20Worlds&body=Title%3A%20SceneFoundry%3A%20Generating%20Interactive%20Infinite%203D%20Worlds%0AAuthor%3A%20ChunTeng%20Chen%20and%20YiChen%20Hsu%20and%20YiWen%20Liu%20and%20WeiFang%20Sun%20and%20TsaiChing%20Ni%20and%20ChunYi%20Lee%20and%20Min%20Sun%20and%20YuanFu%20Yang%0AAbstract%3A%20The%20ability%20to%20automatically%20generate%20large-scale%2C%20interactive%2C%20and%20physically%20realistic%203D%20environments%20is%20crucial%20for%20advancing%20robotic%20learning%20and%20embodied%20intelligence.%20However%2C%20existing%20generative%20approaches%20often%20fail%20to%20capture%20the%20functional%20complexity%20of%20real-world%20interiors%2C%20particularly%20those%20containing%20articulated%20objects%20with%20movable%20parts%20essential%20for%20manipulation%20and%20navigation.%20This%20paper%20presents%20SceneFoundry%2C%20a%20language-guided%20diffusion%20framework%20that%20generates%20apartment-scale%203D%20worlds%20with%20functionally%20articulated%20furniture%20and%20semantically%20diverse%20layouts%20for%20robotic%20training.%20From%20natural%20language%20prompts%2C%20an%20LLM%20module%20controls%20floor%20layout%20generation%2C%20while%20diffusion-based%20posterior%20sampling%20efficiently%20populates%20the%20scene%20with%20articulated%20assets%20from%20large-scale%203D%20repositories.%20To%20ensure%20physical%20usability%2C%20SceneFoundry%20employs%20differentiable%20guidance%20functions%20to%20regulate%20object%20quantity%2C%20prevent%20articulation%20collisions%2C%20and%20maintain%20sufficient%20walkable%20space%20for%20robotic%20navigation.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20generates%20structurally%20valid%2C%20semantically%20coherent%2C%20and%20functionally%20interactive%20environments%20across%20diverse%20scene%20types%20and%20conditions%2C%20enabling%20scalable%20embodied%20AI%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneFoundry%253A%2520Generating%2520Interactive%2520Infinite%25203D%2520Worlds%26entry.906535625%3DChunTeng%2520Chen%2520and%2520YiChen%2520Hsu%2520and%2520YiWen%2520Liu%2520and%2520WeiFang%2520Sun%2520and%2520TsaiChing%2520Ni%2520and%2520ChunYi%2520Lee%2520and%2520Min%2520Sun%2520and%2520YuanFu%2520Yang%26entry.1292438233%3DThe%2520ability%2520to%2520automatically%2520generate%2520large-scale%252C%2520interactive%252C%2520and%2520physically%2520realistic%25203D%2520environments%2520is%2520crucial%2520for%2520advancing%2520robotic%2520learning%2520and%2520embodied%2520intelligence.%2520However%252C%2520existing%2520generative%2520approaches%2520often%2520fail%2520to%2520capture%2520the%2520functional%2520complexity%2520of%2520real-world%2520interiors%252C%2520particularly%2520those%2520containing%2520articulated%2520objects%2520with%2520movable%2520parts%2520essential%2520for%2520manipulation%2520and%2520navigation.%2520This%2520paper%2520presents%2520SceneFoundry%252C%2520a%2520language-guided%2520diffusion%2520framework%2520that%2520generates%2520apartment-scale%25203D%2520worlds%2520with%2520functionally%2520articulated%2520furniture%2520and%2520semantically%2520diverse%2520layouts%2520for%2520robotic%2520training.%2520From%2520natural%2520language%2520prompts%252C%2520an%2520LLM%2520module%2520controls%2520floor%2520layout%2520generation%252C%2520while%2520diffusion-based%2520posterior%2520sampling%2520efficiently%2520populates%2520the%2520scene%2520with%2520articulated%2520assets%2520from%2520large-scale%25203D%2520repositories.%2520To%2520ensure%2520physical%2520usability%252C%2520SceneFoundry%2520employs%2520differentiable%2520guidance%2520functions%2520to%2520regulate%2520object%2520quantity%252C%2520prevent%2520articulation%2520collisions%252C%2520and%2520maintain%2520sufficient%2520walkable%2520space%2520for%2520robotic%2520navigation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520generates%2520structurally%2520valid%252C%2520semantically%2520coherent%252C%2520and%2520functionally%2520interactive%2520environments%2520across%2520diverse%2520scene%2520types%2520and%2520conditions%252C%2520enabling%2520scalable%2520embodied%2520AI%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneFoundry%3A%20Generating%20Interactive%20Infinite%203D%20Worlds&entry.906535625=ChunTeng%20Chen%20and%20YiChen%20Hsu%20and%20YiWen%20Liu%20and%20WeiFang%20Sun%20and%20TsaiChing%20Ni%20and%20ChunYi%20Lee%20and%20Min%20Sun%20and%20YuanFu%20Yang&entry.1292438233=The%20ability%20to%20automatically%20generate%20large-scale%2C%20interactive%2C%20and%20physically%20realistic%203D%20environments%20is%20crucial%20for%20advancing%20robotic%20learning%20and%20embodied%20intelligence.%20However%2C%20existing%20generative%20approaches%20often%20fail%20to%20capture%20the%20functional%20complexity%20of%20real-world%20interiors%2C%20particularly%20those%20containing%20articulated%20objects%20with%20movable%20parts%20essential%20for%20manipulation%20and%20navigation.%20This%20paper%20presents%20SceneFoundry%2C%20a%20language-guided%20diffusion%20framework%20that%20generates%20apartment-scale%203D%20worlds%20with%20functionally%20articulated%20furniture%20and%20semantically%20diverse%20layouts%20for%20robotic%20training.%20From%20natural%20language%20prompts%2C%20an%20LLM%20module%20controls%20floor%20layout%20generation%2C%20while%20diffusion-based%20posterior%20sampling%20efficiently%20populates%20the%20scene%20with%20articulated%20assets%20from%20large-scale%203D%20repositories.%20To%20ensure%20physical%20usability%2C%20SceneFoundry%20employs%20differentiable%20guidance%20functions%20to%20regulate%20object%20quantity%2C%20prevent%20articulation%20collisions%2C%20and%20maintain%20sufficient%20walkable%20space%20for%20robotic%20navigation.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20generates%20structurally%20valid%2C%20semantically%20coherent%2C%20and%20functionally%20interactive%20environments%20across%20diverse%20scene%20types%20and%20conditions%2C%20enabling%20scalable%20embodied%20AI%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.05810v1&entry.124074799=Read"},
{"title": "Detecting Autism Spectrum Disorder with Deep Eye Movement Features", "author": "Zhanpei Huang and Taochen chen and Fangqing Gu and Yiqun Zhang", "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.", "link": "http://arxiv.org/abs/2601.05812v1", "date": "2026-01-09", "relevancy": 1.9845, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5094}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Autism%20Spectrum%20Disorder%20with%20Deep%20Eye%20Movement%20Features&body=Title%3A%20Detecting%20Autism%20Spectrum%20Disorder%20with%20Deep%20Eye%20Movement%20Features%0AAuthor%3A%20Zhanpei%20Huang%20and%20Taochen%20chen%20and%20Fangqing%20Gu%20and%20Yiqun%20Zhang%0AAbstract%3A%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20a%20neurodevelopmental%20disorder%20characterized%20by%20deficits%20in%20social%20communication%20and%20behavioral%20patterns.%20Eye%20movement%20data%20offers%20a%20non-invasive%20diagnostic%20tool%20for%20ASD%20detection%2C%20as%20it%20is%20inherently%20discrete%20and%20exhibits%20short-term%20temporal%20dependencies%2C%20reflecting%20localized%20gaze%20focus%20between%20fixation%20points.%20These%20characteristics%20enable%20the%20data%20to%20provide%20deeper%20insights%20into%20subtle%20behavioral%20markers%2C%20distinguishing%20ASD-related%20patterns%20from%20typical%20development.%20Eye%20movement%20signals%20mainly%20contain%20short-term%20and%20localized%20dependencies.%20However%2C%20despite%20the%20widespread%20application%20of%20stacked%20attention%20layers%20in%20Transformer-based%20models%20for%20capturing%20long-range%20dependencies%2C%20our%20experimental%20results%20indicate%20that%20this%20approach%20yields%20only%20limited%20benefits%20when%20applied%20to%20eye%20movement%20data.%20This%20may%20be%20because%20discrete%20fixation%20points%20and%20short-term%20dependencies%20in%20gaze%20focus%20reduce%20the%20utility%20of%20global%20attention%20mechanisms%2C%20making%20them%20less%20efficient%20than%20architectures%20focusing%20on%20local%20temporal%20patterns.%20To%20efficiently%20capture%20subtle%20and%20complex%20eye%20movement%20patterns%2C%20distinguishing%20ASD%20from%20typically%20developing%20%28TD%29%20individuals%2C%20a%20discrete%20short-term%20sequential%20%28DSTS%29%20modeling%20framework%20is%20designed%20with%20Class-aware%20Representation%20and%20Imbalance-aware%20Mechanisms.%20Through%20extensive%20experiments%20on%20several%20eye%20movement%20datasets%2C%20DSTS%20outperforms%20both%20traditional%20machine%20learning%20techniques%20and%20more%20sophisticated%20deep%20learning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Autism%2520Spectrum%2520Disorder%2520with%2520Deep%2520Eye%2520Movement%2520Features%26entry.906535625%3DZhanpei%2520Huang%2520and%2520Taochen%2520chen%2520and%2520Fangqing%2520Gu%2520and%2520Yiqun%2520Zhang%26entry.1292438233%3DAutism%2520Spectrum%2520Disorder%2520%2528ASD%2529%2520is%2520a%2520neurodevelopmental%2520disorder%2520characterized%2520by%2520deficits%2520in%2520social%2520communication%2520and%2520behavioral%2520patterns.%2520Eye%2520movement%2520data%2520offers%2520a%2520non-invasive%2520diagnostic%2520tool%2520for%2520ASD%2520detection%252C%2520as%2520it%2520is%2520inherently%2520discrete%2520and%2520exhibits%2520short-term%2520temporal%2520dependencies%252C%2520reflecting%2520localized%2520gaze%2520focus%2520between%2520fixation%2520points.%2520These%2520characteristics%2520enable%2520the%2520data%2520to%2520provide%2520deeper%2520insights%2520into%2520subtle%2520behavioral%2520markers%252C%2520distinguishing%2520ASD-related%2520patterns%2520from%2520typical%2520development.%2520Eye%2520movement%2520signals%2520mainly%2520contain%2520short-term%2520and%2520localized%2520dependencies.%2520However%252C%2520despite%2520the%2520widespread%2520application%2520of%2520stacked%2520attention%2520layers%2520in%2520Transformer-based%2520models%2520for%2520capturing%2520long-range%2520dependencies%252C%2520our%2520experimental%2520results%2520indicate%2520that%2520this%2520approach%2520yields%2520only%2520limited%2520benefits%2520when%2520applied%2520to%2520eye%2520movement%2520data.%2520This%2520may%2520be%2520because%2520discrete%2520fixation%2520points%2520and%2520short-term%2520dependencies%2520in%2520gaze%2520focus%2520reduce%2520the%2520utility%2520of%2520global%2520attention%2520mechanisms%252C%2520making%2520them%2520less%2520efficient%2520than%2520architectures%2520focusing%2520on%2520local%2520temporal%2520patterns.%2520To%2520efficiently%2520capture%2520subtle%2520and%2520complex%2520eye%2520movement%2520patterns%252C%2520distinguishing%2520ASD%2520from%2520typically%2520developing%2520%2528TD%2529%2520individuals%252C%2520a%2520discrete%2520short-term%2520sequential%2520%2528DSTS%2529%2520modeling%2520framework%2520is%2520designed%2520with%2520Class-aware%2520Representation%2520and%2520Imbalance-aware%2520Mechanisms.%2520Through%2520extensive%2520experiments%2520on%2520several%2520eye%2520movement%2520datasets%252C%2520DSTS%2520outperforms%2520both%2520traditional%2520machine%2520learning%2520techniques%2520and%2520more%2520sophisticated%2520deep%2520learning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Autism%20Spectrum%20Disorder%20with%20Deep%20Eye%20Movement%20Features&entry.906535625=Zhanpei%20Huang%20and%20Taochen%20chen%20and%20Fangqing%20Gu%20and%20Yiqun%20Zhang&entry.1292438233=Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20a%20neurodevelopmental%20disorder%20characterized%20by%20deficits%20in%20social%20communication%20and%20behavioral%20patterns.%20Eye%20movement%20data%20offers%20a%20non-invasive%20diagnostic%20tool%20for%20ASD%20detection%2C%20as%20it%20is%20inherently%20discrete%20and%20exhibits%20short-term%20temporal%20dependencies%2C%20reflecting%20localized%20gaze%20focus%20between%20fixation%20points.%20These%20characteristics%20enable%20the%20data%20to%20provide%20deeper%20insights%20into%20subtle%20behavioral%20markers%2C%20distinguishing%20ASD-related%20patterns%20from%20typical%20development.%20Eye%20movement%20signals%20mainly%20contain%20short-term%20and%20localized%20dependencies.%20However%2C%20despite%20the%20widespread%20application%20of%20stacked%20attention%20layers%20in%20Transformer-based%20models%20for%20capturing%20long-range%20dependencies%2C%20our%20experimental%20results%20indicate%20that%20this%20approach%20yields%20only%20limited%20benefits%20when%20applied%20to%20eye%20movement%20data.%20This%20may%20be%20because%20discrete%20fixation%20points%20and%20short-term%20dependencies%20in%20gaze%20focus%20reduce%20the%20utility%20of%20global%20attention%20mechanisms%2C%20making%20them%20less%20efficient%20than%20architectures%20focusing%20on%20local%20temporal%20patterns.%20To%20efficiently%20capture%20subtle%20and%20complex%20eye%20movement%20patterns%2C%20distinguishing%20ASD%20from%20typically%20developing%20%28TD%29%20individuals%2C%20a%20discrete%20short-term%20sequential%20%28DSTS%29%20modeling%20framework%20is%20designed%20with%20Class-aware%20Representation%20and%20Imbalance-aware%20Mechanisms.%20Through%20extensive%20experiments%20on%20several%20eye%20movement%20datasets%2C%20DSTS%20outperforms%20both%20traditional%20machine%20learning%20techniques%20and%20more%20sophisticated%20deep%20learning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.05812v1&entry.124074799=Read"},
{"title": "Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers", "author": "Mohamed Amine Hallam and Kuo-Kun Tseng", "abstract": "Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.", "link": "http://arxiv.org/abs/2601.05807v1", "date": "2026-01-09", "relevancy": 1.9775, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20Matters%3A%20Length-Aware%20Analysis%20of%20Positional-Encoding%20Fusion%20in%20Transformers&body=Title%3A%20Fusion%20Matters%3A%20Length-Aware%20Analysis%20of%20Positional-Encoding%20Fusion%20in%20Transformers%0AAuthor%3A%20Mohamed%20Amine%20Hallam%20and%20Kuo-Kun%20Tseng%0AAbstract%3A%20Transformers%20require%20positional%20encodings%20to%20represent%20sequence%20order%2C%20yet%20most%20prior%20work%20focuses%20on%20designing%20new%20positional%20encodings%20rather%20than%20examining%20how%20positional%20information%20is%20fused%20with%20token%20embeddings.%20In%20this%20paper%2C%20we%20study%20whether%20the%20fusion%20mechanism%20itself%20affects%20performance%2C%20particularly%20in%20long-sequence%20settings.%20We%20conduct%20a%20controlled%20empirical%20study%20comparing%20three%20canonical%20fusion%20strategies--element-wise%20addition%2C%20concatenation%20with%20projection%2C%20and%20scalar%20gated%20fusion--under%20identical%20Transformer%20architectures%2C%20data%20splits%2C%20and%20random%20seeds.%20Experiments%20on%20three%20text%20classification%20datasets%20spanning%20short%20%28AG%20News%29%2C%20medium%20%28IMDB%29%2C%20and%20long%20%28ArXiv%29%20sequences%20show%20that%20fusion%20choice%20has%20negligible%20impact%20on%20short%20texts%20but%20produces%20consistent%20gains%20on%20long%20documents.%20To%20verify%20that%20these%20gains%20are%20structural%20rather%20than%20stochastic%2C%20we%20perform%20paired-seed%20analysis%20and%20cross-dataset%20comparison%20across%20sequence-length%20regimes.%20Additional%20experiments%20on%20the%20ArXiv%20dataset%20indicate%20that%20the%20benefit%20of%20learnable%20fusion%20generalizes%20across%20multiple%20positional%20encoding%20families.%20Finally%2C%20we%20explore%20a%20lightweight%20convolutional%20gating%20mechanism%20that%20introduces%20local%20inductive%20bias%20at%20the%20fusion%20level%2C%20evaluated%20on%20long%20documents%20only.%20Our%20results%20indicate%20that%20positional-encoding%20fusion%20is%20a%20non-trivial%20design%20choice%20for%20long-sequence%20Transformers%20and%20should%20be%20treated%20as%20an%20explicit%20modeling%20decision%20rather%20than%20a%20fixed%20default.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520Matters%253A%2520Length-Aware%2520Analysis%2520of%2520Positional-Encoding%2520Fusion%2520in%2520Transformers%26entry.906535625%3DMohamed%2520Amine%2520Hallam%2520and%2520Kuo-Kun%2520Tseng%26entry.1292438233%3DTransformers%2520require%2520positional%2520encodings%2520to%2520represent%2520sequence%2520order%252C%2520yet%2520most%2520prior%2520work%2520focuses%2520on%2520designing%2520new%2520positional%2520encodings%2520rather%2520than%2520examining%2520how%2520positional%2520information%2520is%2520fused%2520with%2520token%2520embeddings.%2520In%2520this%2520paper%252C%2520we%2520study%2520whether%2520the%2520fusion%2520mechanism%2520itself%2520affects%2520performance%252C%2520particularly%2520in%2520long-sequence%2520settings.%2520We%2520conduct%2520a%2520controlled%2520empirical%2520study%2520comparing%2520three%2520canonical%2520fusion%2520strategies--element-wise%2520addition%252C%2520concatenation%2520with%2520projection%252C%2520and%2520scalar%2520gated%2520fusion--under%2520identical%2520Transformer%2520architectures%252C%2520data%2520splits%252C%2520and%2520random%2520seeds.%2520Experiments%2520on%2520three%2520text%2520classification%2520datasets%2520spanning%2520short%2520%2528AG%2520News%2529%252C%2520medium%2520%2528IMDB%2529%252C%2520and%2520long%2520%2528ArXiv%2529%2520sequences%2520show%2520that%2520fusion%2520choice%2520has%2520negligible%2520impact%2520on%2520short%2520texts%2520but%2520produces%2520consistent%2520gains%2520on%2520long%2520documents.%2520To%2520verify%2520that%2520these%2520gains%2520are%2520structural%2520rather%2520than%2520stochastic%252C%2520we%2520perform%2520paired-seed%2520analysis%2520and%2520cross-dataset%2520comparison%2520across%2520sequence-length%2520regimes.%2520Additional%2520experiments%2520on%2520the%2520ArXiv%2520dataset%2520indicate%2520that%2520the%2520benefit%2520of%2520learnable%2520fusion%2520generalizes%2520across%2520multiple%2520positional%2520encoding%2520families.%2520Finally%252C%2520we%2520explore%2520a%2520lightweight%2520convolutional%2520gating%2520mechanism%2520that%2520introduces%2520local%2520inductive%2520bias%2520at%2520the%2520fusion%2520level%252C%2520evaluated%2520on%2520long%2520documents%2520only.%2520Our%2520results%2520indicate%2520that%2520positional-encoding%2520fusion%2520is%2520a%2520non-trivial%2520design%2520choice%2520for%2520long-sequence%2520Transformers%2520and%2520should%2520be%2520treated%2520as%2520an%2520explicit%2520modeling%2520decision%2520rather%2520than%2520a%2520fixed%2520default.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20Matters%3A%20Length-Aware%20Analysis%20of%20Positional-Encoding%20Fusion%20in%20Transformers&entry.906535625=Mohamed%20Amine%20Hallam%20and%20Kuo-Kun%20Tseng&entry.1292438233=Transformers%20require%20positional%20encodings%20to%20represent%20sequence%20order%2C%20yet%20most%20prior%20work%20focuses%20on%20designing%20new%20positional%20encodings%20rather%20than%20examining%20how%20positional%20information%20is%20fused%20with%20token%20embeddings.%20In%20this%20paper%2C%20we%20study%20whether%20the%20fusion%20mechanism%20itself%20affects%20performance%2C%20particularly%20in%20long-sequence%20settings.%20We%20conduct%20a%20controlled%20empirical%20study%20comparing%20three%20canonical%20fusion%20strategies--element-wise%20addition%2C%20concatenation%20with%20projection%2C%20and%20scalar%20gated%20fusion--under%20identical%20Transformer%20architectures%2C%20data%20splits%2C%20and%20random%20seeds.%20Experiments%20on%20three%20text%20classification%20datasets%20spanning%20short%20%28AG%20News%29%2C%20medium%20%28IMDB%29%2C%20and%20long%20%28ArXiv%29%20sequences%20show%20that%20fusion%20choice%20has%20negligible%20impact%20on%20short%20texts%20but%20produces%20consistent%20gains%20on%20long%20documents.%20To%20verify%20that%20these%20gains%20are%20structural%20rather%20than%20stochastic%2C%20we%20perform%20paired-seed%20analysis%20and%20cross-dataset%20comparison%20across%20sequence-length%20regimes.%20Additional%20experiments%20on%20the%20ArXiv%20dataset%20indicate%20that%20the%20benefit%20of%20learnable%20fusion%20generalizes%20across%20multiple%20positional%20encoding%20families.%20Finally%2C%20we%20explore%20a%20lightweight%20convolutional%20gating%20mechanism%20that%20introduces%20local%20inductive%20bias%20at%20the%20fusion%20level%2C%20evaluated%20on%20long%20documents%20only.%20Our%20results%20indicate%20that%20positional-encoding%20fusion%20is%20a%20non-trivial%20design%20choice%20for%20long-sequence%20Transformers%20and%20should%20be%20treated%20as%20an%20explicit%20modeling%20decision%20rather%20than%20a%20fixed%20default.&entry.1838667208=http%3A//arxiv.org/abs/2601.05807v1&entry.124074799=Read"},
{"title": "Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning", "author": "Ali Farjami and Luca Redondi and Marco Valentino", "abstract": "Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.", "link": "http://arxiv.org/abs/2601.05705v1", "date": "2026-01-09", "relevancy": 1.9755, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logic-Parametric%20Neuro-Symbolic%20NLI%3A%20Controlling%20Logical%20Formalisms%20for%20Verifiable%20LLM%20Reasoning&body=Title%3A%20Logic-Parametric%20Neuro-Symbolic%20NLI%3A%20Controlling%20Logical%20Formalisms%20for%20Verifiable%20LLM%20Reasoning%0AAuthor%3A%20Ali%20Farjami%20and%20Luca%20Redondi%20and%20Marco%20Valentino%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20and%20theorem%20provers%20%28TPs%29%20can%20be%20effectively%20combined%20for%20verifiable%20natural%20language%20inference%20%28NLI%29.%20However%2C%20existing%20approaches%20rely%20on%20a%20fixed%20logical%20formalism%2C%20a%20feature%20that%20limits%20robustness%20and%20adaptability.%20We%20propose%20a%20logic-parametric%20framework%20for%20neuro-symbolic%20NLI%20that%20treats%20the%20underlying%20logic%20not%20as%20a%20static%20background%2C%20but%20as%20a%20controllable%20component.%20Using%20the%20LogiKEy%20methodology%2C%20we%20embed%20a%20range%20of%20classical%20and%20non-classical%20formalisms%20into%20higher-order%20logic%20%28HOL%29%2C%20enabling%20a%20systematic%20comparison%20of%20inference%20quality%2C%20explanation%20refinement%2C%20and%20proof%20behavior.%20We%20focus%20on%20normative%20reasoning%2C%20where%20the%20choice%20of%20logic%20has%20significant%20implications.%20In%20particular%2C%20we%20compare%20logic-external%20approaches%2C%20where%20normative%20requirements%20are%20encoded%20via%20axioms%2C%20with%20logic-internal%20approaches%2C%20where%20normative%20patterns%20emerge%20from%20the%20logic%27s%20built-in%20structure.%20Extensive%20experiments%20demonstrate%20that%20logic-internal%20strategies%20can%20consistently%20improve%20performance%20and%20produce%20more%20efficient%20hybrid%20proofs%20for%20NLI.%20In%20addition%2C%20we%20show%20that%20the%20effectiveness%20of%20a%20logic%20is%20domain-dependent%2C%20with%20first-order%20logic%20favouring%20commonsense%20reasoning%2C%20while%20deontic%20and%20modal%20logics%20excel%20in%20ethical%20domains.%20Our%20results%20highlight%20the%20value%20of%20making%20logic%20a%20first-class%2C%20parametric%20element%20in%20neuro-symbolic%20architectures%20for%20more%20robust%2C%20modular%2C%20and%20adaptable%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogic-Parametric%2520Neuro-Symbolic%2520NLI%253A%2520Controlling%2520Logical%2520Formalisms%2520for%2520Verifiable%2520LLM%2520Reasoning%26entry.906535625%3DAli%2520Farjami%2520and%2520Luca%2520Redondi%2520and%2520Marco%2520Valentino%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520and%2520theorem%2520provers%2520%2528TPs%2529%2520can%2520be%2520effectively%2520combined%2520for%2520verifiable%2520natural%2520language%2520inference%2520%2528NLI%2529.%2520However%252C%2520existing%2520approaches%2520rely%2520on%2520a%2520fixed%2520logical%2520formalism%252C%2520a%2520feature%2520that%2520limits%2520robustness%2520and%2520adaptability.%2520We%2520propose%2520a%2520logic-parametric%2520framework%2520for%2520neuro-symbolic%2520NLI%2520that%2520treats%2520the%2520underlying%2520logic%2520not%2520as%2520a%2520static%2520background%252C%2520but%2520as%2520a%2520controllable%2520component.%2520Using%2520the%2520LogiKEy%2520methodology%252C%2520we%2520embed%2520a%2520range%2520of%2520classical%2520and%2520non-classical%2520formalisms%2520into%2520higher-order%2520logic%2520%2528HOL%2529%252C%2520enabling%2520a%2520systematic%2520comparison%2520of%2520inference%2520quality%252C%2520explanation%2520refinement%252C%2520and%2520proof%2520behavior.%2520We%2520focus%2520on%2520normative%2520reasoning%252C%2520where%2520the%2520choice%2520of%2520logic%2520has%2520significant%2520implications.%2520In%2520particular%252C%2520we%2520compare%2520logic-external%2520approaches%252C%2520where%2520normative%2520requirements%2520are%2520encoded%2520via%2520axioms%252C%2520with%2520logic-internal%2520approaches%252C%2520where%2520normative%2520patterns%2520emerge%2520from%2520the%2520logic%2527s%2520built-in%2520structure.%2520Extensive%2520experiments%2520demonstrate%2520that%2520logic-internal%2520strategies%2520can%2520consistently%2520improve%2520performance%2520and%2520produce%2520more%2520efficient%2520hybrid%2520proofs%2520for%2520NLI.%2520In%2520addition%252C%2520we%2520show%2520that%2520the%2520effectiveness%2520of%2520a%2520logic%2520is%2520domain-dependent%252C%2520with%2520first-order%2520logic%2520favouring%2520commonsense%2520reasoning%252C%2520while%2520deontic%2520and%2520modal%2520logics%2520excel%2520in%2520ethical%2520domains.%2520Our%2520results%2520highlight%2520the%2520value%2520of%2520making%2520logic%2520a%2520first-class%252C%2520parametric%2520element%2520in%2520neuro-symbolic%2520architectures%2520for%2520more%2520robust%252C%2520modular%252C%2520and%2520adaptable%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logic-Parametric%20Neuro-Symbolic%20NLI%3A%20Controlling%20Logical%20Formalisms%20for%20Verifiable%20LLM%20Reasoning&entry.906535625=Ali%20Farjami%20and%20Luca%20Redondi%20and%20Marco%20Valentino&entry.1292438233=Large%20language%20models%20%28LLMs%29%20and%20theorem%20provers%20%28TPs%29%20can%20be%20effectively%20combined%20for%20verifiable%20natural%20language%20inference%20%28NLI%29.%20However%2C%20existing%20approaches%20rely%20on%20a%20fixed%20logical%20formalism%2C%20a%20feature%20that%20limits%20robustness%20and%20adaptability.%20We%20propose%20a%20logic-parametric%20framework%20for%20neuro-symbolic%20NLI%20that%20treats%20the%20underlying%20logic%20not%20as%20a%20static%20background%2C%20but%20as%20a%20controllable%20component.%20Using%20the%20LogiKEy%20methodology%2C%20we%20embed%20a%20range%20of%20classical%20and%20non-classical%20formalisms%20into%20higher-order%20logic%20%28HOL%29%2C%20enabling%20a%20systematic%20comparison%20of%20inference%20quality%2C%20explanation%20refinement%2C%20and%20proof%20behavior.%20We%20focus%20on%20normative%20reasoning%2C%20where%20the%20choice%20of%20logic%20has%20significant%20implications.%20In%20particular%2C%20we%20compare%20logic-external%20approaches%2C%20where%20normative%20requirements%20are%20encoded%20via%20axioms%2C%20with%20logic-internal%20approaches%2C%20where%20normative%20patterns%20emerge%20from%20the%20logic%27s%20built-in%20structure.%20Extensive%20experiments%20demonstrate%20that%20logic-internal%20strategies%20can%20consistently%20improve%20performance%20and%20produce%20more%20efficient%20hybrid%20proofs%20for%20NLI.%20In%20addition%2C%20we%20show%20that%20the%20effectiveness%20of%20a%20logic%20is%20domain-dependent%2C%20with%20first-order%20logic%20favouring%20commonsense%20reasoning%2C%20while%20deontic%20and%20modal%20logics%20excel%20in%20ethical%20domains.%20Our%20results%20highlight%20the%20value%20of%20making%20logic%20a%20first-class%2C%20parametric%20element%20in%20neuro-symbolic%20architectures%20for%20more%20robust%2C%20modular%2C%20and%20adaptable%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2601.05705v1&entry.124074799=Read"},
{"title": "Sequential Bayesian Optimal Experimental Design in Infinite Dimensions via Policy Gradient Reinforcement Learning", "author": "Kaichen Shen and Peng Chen", "abstract": "Sequential Bayesian optimal experimental design (SBOED) for PDE-governed inverse problems is computationally challenging, especially for infinite-dimensional random field parameters. High-fidelity approaches require repeated forward and adjoint PDE solves inside nested Bayesian inversion and design loops. We formulate SBOED as a finite-horizon Markov decision process and learn an amortized design policy via policy-gradient reinforcement learning (PGRL), enabling online design selection from the experiment history without repeatedly solving an SBOED optimization problem. To make policy training and reward evaluation scalable, we combine dual dimension reduction -- active subspace projection for the parameter and principal component analysis for the state -- with an adjusted derivative-informed latent attention neural operator (LANO) surrogate that predicts both the parameter-to-solution map and its Jacobian. We use a Laplace-based D-optimality reward while noting that, in general, other expected-information-gain utilities such as KL divergence can also be used within the same framework. We further introduce an eigenvalue-based evaluation strategy that uses prior samples as proxies for maximum a posteriori (MAP) points, avoiding repeated MAP solves while retaining accurate information-gain estimates. Numerical experiments on sequential multi-sensor placement for contaminant source tracking demonstrate approximately $100\\times$ speedup over high-fidelity finite element methods, improved performance over random sensor placements, and physically interpretable policies that discover an ``upstream'' tracking strategy.", "link": "http://arxiv.org/abs/2601.05868v1", "date": "2026-01-09", "relevancy": 1.9754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5623}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.493}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential%20Bayesian%20Optimal%20Experimental%20Design%20in%20Infinite%20Dimensions%20via%20Policy%20Gradient%20Reinforcement%20Learning&body=Title%3A%20Sequential%20Bayesian%20Optimal%20Experimental%20Design%20in%20Infinite%20Dimensions%20via%20Policy%20Gradient%20Reinforcement%20Learning%0AAuthor%3A%20Kaichen%20Shen%20and%20Peng%20Chen%0AAbstract%3A%20Sequential%20Bayesian%20optimal%20experimental%20design%20%28SBOED%29%20for%20PDE-governed%20inverse%20problems%20is%20computationally%20challenging%2C%20especially%20for%20infinite-dimensional%20random%20field%20parameters.%20High-fidelity%20approaches%20require%20repeated%20forward%20and%20adjoint%20PDE%20solves%20inside%20nested%20Bayesian%20inversion%20and%20design%20loops.%20We%20formulate%20SBOED%20as%20a%20finite-horizon%20Markov%20decision%20process%20and%20learn%20an%20amortized%20design%20policy%20via%20policy-gradient%20reinforcement%20learning%20%28PGRL%29%2C%20enabling%20online%20design%20selection%20from%20the%20experiment%20history%20without%20repeatedly%20solving%20an%20SBOED%20optimization%20problem.%20To%20make%20policy%20training%20and%20reward%20evaluation%20scalable%2C%20we%20combine%20dual%20dimension%20reduction%20--%20active%20subspace%20projection%20for%20the%20parameter%20and%20principal%20component%20analysis%20for%20the%20state%20--%20with%20an%20adjusted%20derivative-informed%20latent%20attention%20neural%20operator%20%28LANO%29%20surrogate%20that%20predicts%20both%20the%20parameter-to-solution%20map%20and%20its%20Jacobian.%20We%20use%20a%20Laplace-based%20D-optimality%20reward%20while%20noting%20that%2C%20in%20general%2C%20other%20expected-information-gain%20utilities%20such%20as%20KL%20divergence%20can%20also%20be%20used%20within%20the%20same%20framework.%20We%20further%20introduce%20an%20eigenvalue-based%20evaluation%20strategy%20that%20uses%20prior%20samples%20as%20proxies%20for%20maximum%20a%20posteriori%20%28MAP%29%20points%2C%20avoiding%20repeated%20MAP%20solves%20while%20retaining%20accurate%20information-gain%20estimates.%20Numerical%20experiments%20on%20sequential%20multi-sensor%20placement%20for%20contaminant%20source%20tracking%20demonstrate%20approximately%20%24100%5Ctimes%24%20speedup%20over%20high-fidelity%20finite%20element%20methods%2C%20improved%20performance%20over%20random%20sensor%20placements%2C%20and%20physically%20interpretable%20policies%20that%20discover%20an%20%60%60upstream%27%27%20tracking%20strategy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential%2520Bayesian%2520Optimal%2520Experimental%2520Design%2520in%2520Infinite%2520Dimensions%2520via%2520Policy%2520Gradient%2520Reinforcement%2520Learning%26entry.906535625%3DKaichen%2520Shen%2520and%2520Peng%2520Chen%26entry.1292438233%3DSequential%2520Bayesian%2520optimal%2520experimental%2520design%2520%2528SBOED%2529%2520for%2520PDE-governed%2520inverse%2520problems%2520is%2520computationally%2520challenging%252C%2520especially%2520for%2520infinite-dimensional%2520random%2520field%2520parameters.%2520High-fidelity%2520approaches%2520require%2520repeated%2520forward%2520and%2520adjoint%2520PDE%2520solves%2520inside%2520nested%2520Bayesian%2520inversion%2520and%2520design%2520loops.%2520We%2520formulate%2520SBOED%2520as%2520a%2520finite-horizon%2520Markov%2520decision%2520process%2520and%2520learn%2520an%2520amortized%2520design%2520policy%2520via%2520policy-gradient%2520reinforcement%2520learning%2520%2528PGRL%2529%252C%2520enabling%2520online%2520design%2520selection%2520from%2520the%2520experiment%2520history%2520without%2520repeatedly%2520solving%2520an%2520SBOED%2520optimization%2520problem.%2520To%2520make%2520policy%2520training%2520and%2520reward%2520evaluation%2520scalable%252C%2520we%2520combine%2520dual%2520dimension%2520reduction%2520--%2520active%2520subspace%2520projection%2520for%2520the%2520parameter%2520and%2520principal%2520component%2520analysis%2520for%2520the%2520state%2520--%2520with%2520an%2520adjusted%2520derivative-informed%2520latent%2520attention%2520neural%2520operator%2520%2528LANO%2529%2520surrogate%2520that%2520predicts%2520both%2520the%2520parameter-to-solution%2520map%2520and%2520its%2520Jacobian.%2520We%2520use%2520a%2520Laplace-based%2520D-optimality%2520reward%2520while%2520noting%2520that%252C%2520in%2520general%252C%2520other%2520expected-information-gain%2520utilities%2520such%2520as%2520KL%2520divergence%2520can%2520also%2520be%2520used%2520within%2520the%2520same%2520framework.%2520We%2520further%2520introduce%2520an%2520eigenvalue-based%2520evaluation%2520strategy%2520that%2520uses%2520prior%2520samples%2520as%2520proxies%2520for%2520maximum%2520a%2520posteriori%2520%2528MAP%2529%2520points%252C%2520avoiding%2520repeated%2520MAP%2520solves%2520while%2520retaining%2520accurate%2520information-gain%2520estimates.%2520Numerical%2520experiments%2520on%2520sequential%2520multi-sensor%2520placement%2520for%2520contaminant%2520source%2520tracking%2520demonstrate%2520approximately%2520%2524100%255Ctimes%2524%2520speedup%2520over%2520high-fidelity%2520finite%2520element%2520methods%252C%2520improved%2520performance%2520over%2520random%2520sensor%2520placements%252C%2520and%2520physically%2520interpretable%2520policies%2520that%2520discover%2520an%2520%2560%2560upstream%2527%2527%2520tracking%2520strategy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential%20Bayesian%20Optimal%20Experimental%20Design%20in%20Infinite%20Dimensions%20via%20Policy%20Gradient%20Reinforcement%20Learning&entry.906535625=Kaichen%20Shen%20and%20Peng%20Chen&entry.1292438233=Sequential%20Bayesian%20optimal%20experimental%20design%20%28SBOED%29%20for%20PDE-governed%20inverse%20problems%20is%20computationally%20challenging%2C%20especially%20for%20infinite-dimensional%20random%20field%20parameters.%20High-fidelity%20approaches%20require%20repeated%20forward%20and%20adjoint%20PDE%20solves%20inside%20nested%20Bayesian%20inversion%20and%20design%20loops.%20We%20formulate%20SBOED%20as%20a%20finite-horizon%20Markov%20decision%20process%20and%20learn%20an%20amortized%20design%20policy%20via%20policy-gradient%20reinforcement%20learning%20%28PGRL%29%2C%20enabling%20online%20design%20selection%20from%20the%20experiment%20history%20without%20repeatedly%20solving%20an%20SBOED%20optimization%20problem.%20To%20make%20policy%20training%20and%20reward%20evaluation%20scalable%2C%20we%20combine%20dual%20dimension%20reduction%20--%20active%20subspace%20projection%20for%20the%20parameter%20and%20principal%20component%20analysis%20for%20the%20state%20--%20with%20an%20adjusted%20derivative-informed%20latent%20attention%20neural%20operator%20%28LANO%29%20surrogate%20that%20predicts%20both%20the%20parameter-to-solution%20map%20and%20its%20Jacobian.%20We%20use%20a%20Laplace-based%20D-optimality%20reward%20while%20noting%20that%2C%20in%20general%2C%20other%20expected-information-gain%20utilities%20such%20as%20KL%20divergence%20can%20also%20be%20used%20within%20the%20same%20framework.%20We%20further%20introduce%20an%20eigenvalue-based%20evaluation%20strategy%20that%20uses%20prior%20samples%20as%20proxies%20for%20maximum%20a%20posteriori%20%28MAP%29%20points%2C%20avoiding%20repeated%20MAP%20solves%20while%20retaining%20accurate%20information-gain%20estimates.%20Numerical%20experiments%20on%20sequential%20multi-sensor%20placement%20for%20contaminant%20source%20tracking%20demonstrate%20approximately%20%24100%5Ctimes%24%20speedup%20over%20high-fidelity%20finite%20element%20methods%2C%20improved%20performance%20over%20random%20sensor%20placements%2C%20and%20physically%20interpretable%20policies%20that%20discover%20an%20%60%60upstream%27%27%20tracking%20strategy.&entry.1838667208=http%3A//arxiv.org/abs/2601.05868v1&entry.124074799=Read"},
{"title": "Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets", "author": "Pankaj Gupta and Priya Mudgil and Niharika Dutta and Kartik Bose and Nitish Kumar and Anupam Kumar and Jimil Shah and Vaneet Jearth and Jayanta Samanta and Vishal Sharma and Harshal Mandavdhare and Surinder Rana and Saroj K Sinha and Usha Dutta", "abstract": "Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer backbone was trained and validated with 17,367 EUS images (from two public datasets) in 5-fold cross-validation. The model was tested on an independent dataset of 350 EUS images from another public dataset, manually segmented by radiologists. Preprocessing included grayscale conversion, cropping, and resizing to 512x512 pixels. Metrics included Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy. Results: In 5-fold cross-validation, the model achieved a mean DSC of 0.651 +/- 0.738, IoU of 0.579 +/- 0.658, sensitivity of 69.8%, specificity of 98.8%, and accuracy of 97.5%. For the external validation set, the model achieved a DSC of 0.657 (95% CI: 0.634-0.769), IoU of 0.614 (95% CI: 0.590-0.689), sensitivity of 71.8%, and specificity of 97.7%. Results were consistent, but 9.7% of cases exhibited erroneous multiple predictions. Conclusions: The Vision Transformer-based model demonstrated strong performance for pancreatic tumor segmentation in EUS images. However, dataset heterogeneity and limited external validation highlight the need for further refinement, standardization, and prospective studies.", "link": "http://arxiv.org/abs/2601.05937v1", "date": "2026-01-09", "relevancy": 1.9722, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5128}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20of%20a%20Deep%20Learning-Based%20Segmentation%20Model%20for%20Pancreatic%20Tumors%20on%20Public%20Endoscopic%20Ultrasound%20Datasets&body=Title%3A%20Performance%20of%20a%20Deep%20Learning-Based%20Segmentation%20Model%20for%20Pancreatic%20Tumors%20on%20Public%20Endoscopic%20Ultrasound%20Datasets%0AAuthor%3A%20Pankaj%20Gupta%20and%20Priya%20Mudgil%20and%20Niharika%20Dutta%20and%20Kartik%20Bose%20and%20Nitish%20Kumar%20and%20Anupam%20Kumar%20and%20Jimil%20Shah%20and%20Vaneet%20Jearth%20and%20Jayanta%20Samanta%20and%20Vishal%20Sharma%20and%20Harshal%20Mandavdhare%20and%20Surinder%20Rana%20and%20Saroj%20K%20Sinha%20and%20Usha%20Dutta%0AAbstract%3A%20Background%3A%20Pancreatic%20cancer%20is%20one%20of%20the%20most%20aggressive%20cancers%2C%20with%20poor%20survival%20rates.%20Endoscopic%20ultrasound%20%28EUS%29%20is%20a%20key%20diagnostic%20modality%2C%20but%20its%20effectiveness%20is%20constrained%20by%20operator%20subjectivity.%20This%20study%20evaluates%20a%20Vision%20Transformer-based%20deep%20learning%20segmentation%20model%20for%20pancreatic%20tumors.%20Methods%3A%20A%20segmentation%20model%20using%20the%20USFM%20framework%20with%20a%20Vision%20Transformer%20backbone%20was%20trained%20and%20validated%20with%2017%2C367%20EUS%20images%20%28from%20two%20public%20datasets%29%20in%205-fold%20cross-validation.%20The%20model%20was%20tested%20on%20an%20independent%20dataset%20of%20350%20EUS%20images%20from%20another%20public%20dataset%2C%20manually%20segmented%20by%20radiologists.%20Preprocessing%20included%20grayscale%20conversion%2C%20cropping%2C%20and%20resizing%20to%20512x512%20pixels.%20Metrics%20included%20Dice%20similarity%20coefficient%20%28DSC%29%2C%20intersection%20over%20union%20%28IoU%29%2C%20sensitivity%2C%20specificity%2C%20and%20accuracy.%20Results%3A%20In%205-fold%20cross-validation%2C%20the%20model%20achieved%20a%20mean%20DSC%20of%200.651%20%2B/-%200.738%2C%20IoU%20of%200.579%20%2B/-%200.658%2C%20sensitivity%20of%2069.8%25%2C%20specificity%20of%2098.8%25%2C%20and%20accuracy%20of%2097.5%25.%20For%20the%20external%20validation%20set%2C%20the%20model%20achieved%20a%20DSC%20of%200.657%20%2895%25%20CI%3A%200.634-0.769%29%2C%20IoU%20of%200.614%20%2895%25%20CI%3A%200.590-0.689%29%2C%20sensitivity%20of%2071.8%25%2C%20and%20specificity%20of%2097.7%25.%20Results%20were%20consistent%2C%20but%209.7%25%20of%20cases%20exhibited%20erroneous%20multiple%20predictions.%20Conclusions%3A%20The%20Vision%20Transformer-based%20model%20demonstrated%20strong%20performance%20for%20pancreatic%20tumor%20segmentation%20in%20EUS%20images.%20However%2C%20dataset%20heterogeneity%20and%20limited%20external%20validation%20highlight%20the%20need%20for%20further%20refinement%2C%20standardization%2C%20and%20prospective%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520of%2520a%2520Deep%2520Learning-Based%2520Segmentation%2520Model%2520for%2520Pancreatic%2520Tumors%2520on%2520Public%2520Endoscopic%2520Ultrasound%2520Datasets%26entry.906535625%3DPankaj%2520Gupta%2520and%2520Priya%2520Mudgil%2520and%2520Niharika%2520Dutta%2520and%2520Kartik%2520Bose%2520and%2520Nitish%2520Kumar%2520and%2520Anupam%2520Kumar%2520and%2520Jimil%2520Shah%2520and%2520Vaneet%2520Jearth%2520and%2520Jayanta%2520Samanta%2520and%2520Vishal%2520Sharma%2520and%2520Harshal%2520Mandavdhare%2520and%2520Surinder%2520Rana%2520and%2520Saroj%2520K%2520Sinha%2520and%2520Usha%2520Dutta%26entry.1292438233%3DBackground%253A%2520Pancreatic%2520cancer%2520is%2520one%2520of%2520the%2520most%2520aggressive%2520cancers%252C%2520with%2520poor%2520survival%2520rates.%2520Endoscopic%2520ultrasound%2520%2528EUS%2529%2520is%2520a%2520key%2520diagnostic%2520modality%252C%2520but%2520its%2520effectiveness%2520is%2520constrained%2520by%2520operator%2520subjectivity.%2520This%2520study%2520evaluates%2520a%2520Vision%2520Transformer-based%2520deep%2520learning%2520segmentation%2520model%2520for%2520pancreatic%2520tumors.%2520Methods%253A%2520A%2520segmentation%2520model%2520using%2520the%2520USFM%2520framework%2520with%2520a%2520Vision%2520Transformer%2520backbone%2520was%2520trained%2520and%2520validated%2520with%252017%252C367%2520EUS%2520images%2520%2528from%2520two%2520public%2520datasets%2529%2520in%25205-fold%2520cross-validation.%2520The%2520model%2520was%2520tested%2520on%2520an%2520independent%2520dataset%2520of%2520350%2520EUS%2520images%2520from%2520another%2520public%2520dataset%252C%2520manually%2520segmented%2520by%2520radiologists.%2520Preprocessing%2520included%2520grayscale%2520conversion%252C%2520cropping%252C%2520and%2520resizing%2520to%2520512x512%2520pixels.%2520Metrics%2520included%2520Dice%2520similarity%2520coefficient%2520%2528DSC%2529%252C%2520intersection%2520over%2520union%2520%2528IoU%2529%252C%2520sensitivity%252C%2520specificity%252C%2520and%2520accuracy.%2520Results%253A%2520In%25205-fold%2520cross-validation%252C%2520the%2520model%2520achieved%2520a%2520mean%2520DSC%2520of%25200.651%2520%252B/-%25200.738%252C%2520IoU%2520of%25200.579%2520%252B/-%25200.658%252C%2520sensitivity%2520of%252069.8%2525%252C%2520specificity%2520of%252098.8%2525%252C%2520and%2520accuracy%2520of%252097.5%2525.%2520For%2520the%2520external%2520validation%2520set%252C%2520the%2520model%2520achieved%2520a%2520DSC%2520of%25200.657%2520%252895%2525%2520CI%253A%25200.634-0.769%2529%252C%2520IoU%2520of%25200.614%2520%252895%2525%2520CI%253A%25200.590-0.689%2529%252C%2520sensitivity%2520of%252071.8%2525%252C%2520and%2520specificity%2520of%252097.7%2525.%2520Results%2520were%2520consistent%252C%2520but%25209.7%2525%2520of%2520cases%2520exhibited%2520erroneous%2520multiple%2520predictions.%2520Conclusions%253A%2520The%2520Vision%2520Transformer-based%2520model%2520demonstrated%2520strong%2520performance%2520for%2520pancreatic%2520tumor%2520segmentation%2520in%2520EUS%2520images.%2520However%252C%2520dataset%2520heterogeneity%2520and%2520limited%2520external%2520validation%2520highlight%2520the%2520need%2520for%2520further%2520refinement%252C%2520standardization%252C%2520and%2520prospective%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20of%20a%20Deep%20Learning-Based%20Segmentation%20Model%20for%20Pancreatic%20Tumors%20on%20Public%20Endoscopic%20Ultrasound%20Datasets&entry.906535625=Pankaj%20Gupta%20and%20Priya%20Mudgil%20and%20Niharika%20Dutta%20and%20Kartik%20Bose%20and%20Nitish%20Kumar%20and%20Anupam%20Kumar%20and%20Jimil%20Shah%20and%20Vaneet%20Jearth%20and%20Jayanta%20Samanta%20and%20Vishal%20Sharma%20and%20Harshal%20Mandavdhare%20and%20Surinder%20Rana%20and%20Saroj%20K%20Sinha%20and%20Usha%20Dutta&entry.1292438233=Background%3A%20Pancreatic%20cancer%20is%20one%20of%20the%20most%20aggressive%20cancers%2C%20with%20poor%20survival%20rates.%20Endoscopic%20ultrasound%20%28EUS%29%20is%20a%20key%20diagnostic%20modality%2C%20but%20its%20effectiveness%20is%20constrained%20by%20operator%20subjectivity.%20This%20study%20evaluates%20a%20Vision%20Transformer-based%20deep%20learning%20segmentation%20model%20for%20pancreatic%20tumors.%20Methods%3A%20A%20segmentation%20model%20using%20the%20USFM%20framework%20with%20a%20Vision%20Transformer%20backbone%20was%20trained%20and%20validated%20with%2017%2C367%20EUS%20images%20%28from%20two%20public%20datasets%29%20in%205-fold%20cross-validation.%20The%20model%20was%20tested%20on%20an%20independent%20dataset%20of%20350%20EUS%20images%20from%20another%20public%20dataset%2C%20manually%20segmented%20by%20radiologists.%20Preprocessing%20included%20grayscale%20conversion%2C%20cropping%2C%20and%20resizing%20to%20512x512%20pixels.%20Metrics%20included%20Dice%20similarity%20coefficient%20%28DSC%29%2C%20intersection%20over%20union%20%28IoU%29%2C%20sensitivity%2C%20specificity%2C%20and%20accuracy.%20Results%3A%20In%205-fold%20cross-validation%2C%20the%20model%20achieved%20a%20mean%20DSC%20of%200.651%20%2B/-%200.738%2C%20IoU%20of%200.579%20%2B/-%200.658%2C%20sensitivity%20of%2069.8%25%2C%20specificity%20of%2098.8%25%2C%20and%20accuracy%20of%2097.5%25.%20For%20the%20external%20validation%20set%2C%20the%20model%20achieved%20a%20DSC%20of%200.657%20%2895%25%20CI%3A%200.634-0.769%29%2C%20IoU%20of%200.614%20%2895%25%20CI%3A%200.590-0.689%29%2C%20sensitivity%20of%2071.8%25%2C%20and%20specificity%20of%2097.7%25.%20Results%20were%20consistent%2C%20but%209.7%25%20of%20cases%20exhibited%20erroneous%20multiple%20predictions.%20Conclusions%3A%20The%20Vision%20Transformer-based%20model%20demonstrated%20strong%20performance%20for%20pancreatic%20tumor%20segmentation%20in%20EUS%20images.%20However%2C%20dataset%20heterogeneity%20and%20limited%20external%20validation%20highlight%20the%20need%20for%20further%20refinement%2C%20standardization%2C%20and%20prospective%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2601.05937v1&entry.124074799=Read"},
{"title": "mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations", "author": "Yongyi Yang and Jianyang Gao", "abstract": "Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.", "link": "http://arxiv.org/abs/2601.05732v1", "date": "2026-01-09", "relevancy": 1.967, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4949}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4929}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mHC-lite%3A%20You%20Don%27t%20Need%2020%20Sinkhorn-Knopp%20Iterations&body=Title%3A%20mHC-lite%3A%20You%20Don%27t%20Need%2020%20Sinkhorn-Knopp%20Iterations%0AAuthor%3A%20Yongyi%20Yang%20and%20Jianyang%20Gao%0AAbstract%3A%20Hyper-Connections%20%28HC%29%20generalizes%20residual%20connections%20by%20introducing%20dynamic%20residual%20matrices%20that%20mix%20information%20across%20multiple%20residual%20streams%2C%20accelerating%20convergence%20in%20deep%20neural%20networks.%20However%2C%20unconstrained%20residual%20matrices%20can%20compromise%20training%20stability.%20To%20address%20this%2C%20DeepSeek%27s%20Manifold-Constrained%20Hyper-Connections%20%28mHC%29%20approximately%20projects%20these%20matrices%20onto%20the%20Birkhoff%20polytope%20via%20iterative%20Sinkhorn--Knopp%20%28SK%29%20normalization.%20We%20identify%20two%20limitations%20of%20this%20approach%3A%20%28i%29%20finite%20SK%20iterations%20do%20not%20guarantee%20exact%20doubly%20stochasticity%2C%20leaving%20an%20approximation%20gap%20that%20can%20accumulate%20through%20network%20depth%20and%20undermine%20stability%3B%20%28ii%29%20efficient%20SK%20implementation%20requires%20highly%20specialized%20CUDA%20kernels%2C%20raising%20engineering%20barriers%20and%20reducing%20portability.%20Motivated%20by%20the%20Birkhoff--von%20Neumann%20theorem%2C%20we%20propose%20mHC-lite%2C%20a%20simple%20reparameterization%20that%20explicitly%20constructs%20doubly%20stochastic%20matrices%20as%20convex%20combinations%20of%20permutation%20matrices.%20This%20approach%20guarantees%20exact%20doubly%20stochasticity%20by%20construction%20and%20can%20be%20implemented%20using%20only%20native%20matrix%20operations.%20Extensive%20experiments%20demonstrate%20that%20mHC-lite%20matches%20or%20exceeds%20mHC%20in%20performance%20while%20achieving%20higher%20training%20throughput%20with%20a%20naive%20implementation%20and%20eliminating%20the%20residual%20instabilities%20observed%20in%20both%20HC%20and%20mHC.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/FFTYYY/mhc-lite.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmHC-lite%253A%2520You%2520Don%2527t%2520Need%252020%2520Sinkhorn-Knopp%2520Iterations%26entry.906535625%3DYongyi%2520Yang%2520and%2520Jianyang%2520Gao%26entry.1292438233%3DHyper-Connections%2520%2528HC%2529%2520generalizes%2520residual%2520connections%2520by%2520introducing%2520dynamic%2520residual%2520matrices%2520that%2520mix%2520information%2520across%2520multiple%2520residual%2520streams%252C%2520accelerating%2520convergence%2520in%2520deep%2520neural%2520networks.%2520However%252C%2520unconstrained%2520residual%2520matrices%2520can%2520compromise%2520training%2520stability.%2520To%2520address%2520this%252C%2520DeepSeek%2527s%2520Manifold-Constrained%2520Hyper-Connections%2520%2528mHC%2529%2520approximately%2520projects%2520these%2520matrices%2520onto%2520the%2520Birkhoff%2520polytope%2520via%2520iterative%2520Sinkhorn--Knopp%2520%2528SK%2529%2520normalization.%2520We%2520identify%2520two%2520limitations%2520of%2520this%2520approach%253A%2520%2528i%2529%2520finite%2520SK%2520iterations%2520do%2520not%2520guarantee%2520exact%2520doubly%2520stochasticity%252C%2520leaving%2520an%2520approximation%2520gap%2520that%2520can%2520accumulate%2520through%2520network%2520depth%2520and%2520undermine%2520stability%253B%2520%2528ii%2529%2520efficient%2520SK%2520implementation%2520requires%2520highly%2520specialized%2520CUDA%2520kernels%252C%2520raising%2520engineering%2520barriers%2520and%2520reducing%2520portability.%2520Motivated%2520by%2520the%2520Birkhoff--von%2520Neumann%2520theorem%252C%2520we%2520propose%2520mHC-lite%252C%2520a%2520simple%2520reparameterization%2520that%2520explicitly%2520constructs%2520doubly%2520stochastic%2520matrices%2520as%2520convex%2520combinations%2520of%2520permutation%2520matrices.%2520This%2520approach%2520guarantees%2520exact%2520doubly%2520stochasticity%2520by%2520construction%2520and%2520can%2520be%2520implemented%2520using%2520only%2520native%2520matrix%2520operations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520mHC-lite%2520matches%2520or%2520exceeds%2520mHC%2520in%2520performance%2520while%2520achieving%2520higher%2520training%2520throughput%2520with%2520a%2520naive%2520implementation%2520and%2520eliminating%2520the%2520residual%2520instabilities%2520observed%2520in%2520both%2520HC%2520and%2520mHC.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/FFTYYY/mhc-lite.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mHC-lite%3A%20You%20Don%27t%20Need%2020%20Sinkhorn-Knopp%20Iterations&entry.906535625=Yongyi%20Yang%20and%20Jianyang%20Gao&entry.1292438233=Hyper-Connections%20%28HC%29%20generalizes%20residual%20connections%20by%20introducing%20dynamic%20residual%20matrices%20that%20mix%20information%20across%20multiple%20residual%20streams%2C%20accelerating%20convergence%20in%20deep%20neural%20networks.%20However%2C%20unconstrained%20residual%20matrices%20can%20compromise%20training%20stability.%20To%20address%20this%2C%20DeepSeek%27s%20Manifold-Constrained%20Hyper-Connections%20%28mHC%29%20approximately%20projects%20these%20matrices%20onto%20the%20Birkhoff%20polytope%20via%20iterative%20Sinkhorn--Knopp%20%28SK%29%20normalization.%20We%20identify%20two%20limitations%20of%20this%20approach%3A%20%28i%29%20finite%20SK%20iterations%20do%20not%20guarantee%20exact%20doubly%20stochasticity%2C%20leaving%20an%20approximation%20gap%20that%20can%20accumulate%20through%20network%20depth%20and%20undermine%20stability%3B%20%28ii%29%20efficient%20SK%20implementation%20requires%20highly%20specialized%20CUDA%20kernels%2C%20raising%20engineering%20barriers%20and%20reducing%20portability.%20Motivated%20by%20the%20Birkhoff--von%20Neumann%20theorem%2C%20we%20propose%20mHC-lite%2C%20a%20simple%20reparameterization%20that%20explicitly%20constructs%20doubly%20stochastic%20matrices%20as%20convex%20combinations%20of%20permutation%20matrices.%20This%20approach%20guarantees%20exact%20doubly%20stochasticity%20by%20construction%20and%20can%20be%20implemented%20using%20only%20native%20matrix%20operations.%20Extensive%20experiments%20demonstrate%20that%20mHC-lite%20matches%20or%20exceeds%20mHC%20in%20performance%20while%20achieving%20higher%20training%20throughput%20with%20a%20naive%20implementation%20and%20eliminating%20the%20residual%20instabilities%20observed%20in%20both%20HC%20and%20mHC.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/FFTYYY/mhc-lite.&entry.1838667208=http%3A//arxiv.org/abs/2601.05732v1&entry.124074799=Read"},
{"title": "Large language models can effectively convince people to believe conspiracies", "author": "Thomas H. Costello and Kellin Pelrine and Matthew Kowal and Antonio A. Arechar and Jean-Fran\u00e7ois Godbout and Adam Gleave and David Rand and Gordon Pennycook", "abstract": "Large language models (LLMs) have been shown to be persuasive across a variety of contexts. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to prevent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.", "link": "http://arxiv.org/abs/2601.05050v2", "date": "2026-01-09", "relevancy": 1.9636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3946}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20can%20effectively%20convince%20people%20to%20believe%20conspiracies&body=Title%3A%20Large%20language%20models%20can%20effectively%20convince%20people%20to%20believe%20conspiracies%0AAuthor%3A%20Thomas%20H.%20Costello%20and%20Kellin%20Pelrine%20and%20Matthew%20Kowal%20and%20Antonio%20A.%20Arechar%20and%20Jean-Fran%C3%A7ois%20Godbout%20and%20Adam%20Gleave%20and%20David%20Rand%20and%20Gordon%20Pennycook%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20been%20shown%20to%20be%20persuasive%20across%20a%20variety%20of%20contexts.%20But%20it%20remains%20unclear%20whether%20this%20persuasive%20power%20advantages%20truth%20over%20falsehood%2C%20or%20if%20LLMs%20can%20promote%20misbeliefs%20just%20as%20easily%20as%20refuting%20them.%20Here%2C%20we%20investigate%20this%20question%20across%20three%20pre-registered%20experiments%20in%20which%20participants%20%28N%20%3D%202%2C724%20Americans%29%20discussed%20a%20conspiracy%20theory%20they%20were%20uncertain%20about%20with%20GPT-4o%2C%20and%20the%20model%20was%20instructed%20to%20either%20argue%20against%20%28%22debunking%22%29%20or%20for%20%28%22bunking%22%29%20that%20conspiracy.%20When%20using%20a%20%22jailbroken%22%20GPT-4o%20variant%20with%20guardrails%20removed%2C%20the%20AI%20was%20as%20effective%20at%20increasing%20conspiracy%20belief%20as%20decreasing%20it.%20Concerningly%2C%20the%20bunking%20AI%20was%20rated%20more%20positively%2C%20and%20increased%20trust%20in%20AI%2C%20more%20than%20the%20debunking%20AI.%20Surprisingly%2C%20we%20found%20that%20using%20standard%20GPT-4o%20produced%20very%20similar%20effects%2C%20such%20that%20the%20guardrails%20imposed%20by%20OpenAI%20did%20little%20to%20prevent%20the%20LLM%20from%20promoting%20conspiracy%20beliefs.%20Encouragingly%2C%20however%2C%20a%20corrective%20conversation%20reversed%20these%20newly%20induced%20conspiracy%20beliefs%2C%20and%20simply%20prompting%20GPT-4o%20to%20only%20use%20accurate%20information%20dramatically%20reduced%20its%20ability%20to%20increase%20conspiracy%20beliefs.%20Our%20findings%20demonstrate%20that%20LLMs%20possess%20potent%20abilities%20to%20promote%20both%20truth%20and%20falsehood%2C%20but%20that%20potential%20solutions%20may%20exist%20to%20help%20mitigate%20this%20risk.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520can%2520effectively%2520convince%2520people%2520to%2520believe%2520conspiracies%26entry.906535625%3DThomas%2520H.%2520Costello%2520and%2520Kellin%2520Pelrine%2520and%2520Matthew%2520Kowal%2520and%2520Antonio%2520A.%2520Arechar%2520and%2520Jean-Fran%25C3%25A7ois%2520Godbout%2520and%2520Adam%2520Gleave%2520and%2520David%2520Rand%2520and%2520Gordon%2520Pennycook%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520be%2520persuasive%2520across%2520a%2520variety%2520of%2520contexts.%2520But%2520it%2520remains%2520unclear%2520whether%2520this%2520persuasive%2520power%2520advantages%2520truth%2520over%2520falsehood%252C%2520or%2520if%2520LLMs%2520can%2520promote%2520misbeliefs%2520just%2520as%2520easily%2520as%2520refuting%2520them.%2520Here%252C%2520we%2520investigate%2520this%2520question%2520across%2520three%2520pre-registered%2520experiments%2520in%2520which%2520participants%2520%2528N%2520%253D%25202%252C724%2520Americans%2529%2520discussed%2520a%2520conspiracy%2520theory%2520they%2520were%2520uncertain%2520about%2520with%2520GPT-4o%252C%2520and%2520the%2520model%2520was%2520instructed%2520to%2520either%2520argue%2520against%2520%2528%2522debunking%2522%2529%2520or%2520for%2520%2528%2522bunking%2522%2529%2520that%2520conspiracy.%2520When%2520using%2520a%2520%2522jailbroken%2522%2520GPT-4o%2520variant%2520with%2520guardrails%2520removed%252C%2520the%2520AI%2520was%2520as%2520effective%2520at%2520increasing%2520conspiracy%2520belief%2520as%2520decreasing%2520it.%2520Concerningly%252C%2520the%2520bunking%2520AI%2520was%2520rated%2520more%2520positively%252C%2520and%2520increased%2520trust%2520in%2520AI%252C%2520more%2520than%2520the%2520debunking%2520AI.%2520Surprisingly%252C%2520we%2520found%2520that%2520using%2520standard%2520GPT-4o%2520produced%2520very%2520similar%2520effects%252C%2520such%2520that%2520the%2520guardrails%2520imposed%2520by%2520OpenAI%2520did%2520little%2520to%2520prevent%2520the%2520LLM%2520from%2520promoting%2520conspiracy%2520beliefs.%2520Encouragingly%252C%2520however%252C%2520a%2520corrective%2520conversation%2520reversed%2520these%2520newly%2520induced%2520conspiracy%2520beliefs%252C%2520and%2520simply%2520prompting%2520GPT-4o%2520to%2520only%2520use%2520accurate%2520information%2520dramatically%2520reduced%2520its%2520ability%2520to%2520increase%2520conspiracy%2520beliefs.%2520Our%2520findings%2520demonstrate%2520that%2520LLMs%2520possess%2520potent%2520abilities%2520to%2520promote%2520both%2520truth%2520and%2520falsehood%252C%2520but%2520that%2520potential%2520solutions%2520may%2520exist%2520to%2520help%2520mitigate%2520this%2520risk.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20can%20effectively%20convince%20people%20to%20believe%20conspiracies&entry.906535625=Thomas%20H.%20Costello%20and%20Kellin%20Pelrine%20and%20Matthew%20Kowal%20and%20Antonio%20A.%20Arechar%20and%20Jean-Fran%C3%A7ois%20Godbout%20and%20Adam%20Gleave%20and%20David%20Rand%20and%20Gordon%20Pennycook&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20been%20shown%20to%20be%20persuasive%20across%20a%20variety%20of%20contexts.%20But%20it%20remains%20unclear%20whether%20this%20persuasive%20power%20advantages%20truth%20over%20falsehood%2C%20or%20if%20LLMs%20can%20promote%20misbeliefs%20just%20as%20easily%20as%20refuting%20them.%20Here%2C%20we%20investigate%20this%20question%20across%20three%20pre-registered%20experiments%20in%20which%20participants%20%28N%20%3D%202%2C724%20Americans%29%20discussed%20a%20conspiracy%20theory%20they%20were%20uncertain%20about%20with%20GPT-4o%2C%20and%20the%20model%20was%20instructed%20to%20either%20argue%20against%20%28%22debunking%22%29%20or%20for%20%28%22bunking%22%29%20that%20conspiracy.%20When%20using%20a%20%22jailbroken%22%20GPT-4o%20variant%20with%20guardrails%20removed%2C%20the%20AI%20was%20as%20effective%20at%20increasing%20conspiracy%20belief%20as%20decreasing%20it.%20Concerningly%2C%20the%20bunking%20AI%20was%20rated%20more%20positively%2C%20and%20increased%20trust%20in%20AI%2C%20more%20than%20the%20debunking%20AI.%20Surprisingly%2C%20we%20found%20that%20using%20standard%20GPT-4o%20produced%20very%20similar%20effects%2C%20such%20that%20the%20guardrails%20imposed%20by%20OpenAI%20did%20little%20to%20prevent%20the%20LLM%20from%20promoting%20conspiracy%20beliefs.%20Encouragingly%2C%20however%2C%20a%20corrective%20conversation%20reversed%20these%20newly%20induced%20conspiracy%20beliefs%2C%20and%20simply%20prompting%20GPT-4o%20to%20only%20use%20accurate%20information%20dramatically%20reduced%20its%20ability%20to%20increase%20conspiracy%20beliefs.%20Our%20findings%20demonstrate%20that%20LLMs%20possess%20potent%20abilities%20to%20promote%20both%20truth%20and%20falsehood%2C%20but%20that%20potential%20solutions%20may%20exist%20to%20help%20mitigate%20this%20risk.&entry.1838667208=http%3A//arxiv.org/abs/2601.05050v2&entry.124074799=Read"},
{"title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions", "author": "Aishik Mandal and Tanmoy Chakraborty and Iryna Gurevych", "abstract": "The growing demand for scalable psychological counseling highlights the need for high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. We further propose a unified evaluation framework that consolidates diverse automatic metrics and expands expert assessment from four to nine counseling dimensions, thus addressing inconsistencies in prior evaluation protocols. Empirically, MAGneT substantially outperforms existing methods: experts prefer MAGneT-generated sessions in 77.2% of cases, and sessions generated by MAGneT yield 3.2% higher general counseling skills and 4.3% higher CBT-specific skills on cognitive therapy rating scale (CTRS). A open source Llama3-8B-Instruct model fine-tuned on MAGneT-generated data also outperforms models fine-tuned using baseline synthetic datasets by 6.9% on average on CTRS.We also make our code and data public.", "link": "http://arxiv.org/abs/2509.04183v2", "date": "2026-01-09", "relevancy": 1.9578, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5024}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGneT%3A%20Coordinated%20Multi-Agent%20Generation%20of%20Synthetic%20Multi-Turn%20Mental%20Health%20Counseling%20Sessions&body=Title%3A%20MAGneT%3A%20Coordinated%20Multi-Agent%20Generation%20of%20Synthetic%20Multi-Turn%20Mental%20Health%20Counseling%20Sessions%0AAuthor%3A%20Aishik%20Mandal%20and%20Tanmoy%20Chakraborty%20and%20Iryna%20Gurevych%0AAbstract%3A%20The%20growing%20demand%20for%20scalable%20psychological%20counseling%20highlights%20the%20need%20for%20high-quality%2C%20privacy-compliant%20data%2C%20yet%20such%20data%20remains%20scarce.%20Here%20we%20introduce%20MAGneT%2C%20a%20novel%20multi-agent%20framework%20for%20synthetic%20psychological%20counseling%20session%20generation%20that%20decomposes%20counselor%20response%20generation%20into%20coordinated%20sub-tasks%20handled%20by%20specialized%20LLM%20agents%2C%20each%20modeling%20a%20key%20psychological%20technique.%20Unlike%20prior%20single-agent%20approaches%2C%20MAGneT%20better%20captures%20the%20structure%20and%20nuance%20of%20real%20counseling.%20We%20further%20propose%20a%20unified%20evaluation%20framework%20that%20consolidates%20diverse%20automatic%20metrics%20and%20expands%20expert%20assessment%20from%20four%20to%20nine%20counseling%20dimensions%2C%20thus%20addressing%20inconsistencies%20in%20prior%20evaluation%20protocols.%20Empirically%2C%20MAGneT%20substantially%20outperforms%20existing%20methods%3A%20experts%20prefer%20MAGneT-generated%20sessions%20in%2077.2%25%20of%20cases%2C%20and%20sessions%20generated%20by%20MAGneT%20yield%203.2%25%20higher%20general%20counseling%20skills%20and%204.3%25%20higher%20CBT-specific%20skills%20on%20cognitive%20therapy%20rating%20scale%20%28CTRS%29.%20A%20open%20source%20Llama3-8B-Instruct%20model%20fine-tuned%20on%20MAGneT-generated%20data%20also%20outperforms%20models%20fine-tuned%20using%20baseline%20synthetic%20datasets%20by%206.9%25%20on%20average%20on%20CTRS.We%20also%20make%20our%20code%20and%20data%20public.%0ALink%3A%20http%3A//arxiv.org/abs/2509.04183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGneT%253A%2520Coordinated%2520Multi-Agent%2520Generation%2520of%2520Synthetic%2520Multi-Turn%2520Mental%2520Health%2520Counseling%2520Sessions%26entry.906535625%3DAishik%2520Mandal%2520and%2520Tanmoy%2520Chakraborty%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3DThe%2520growing%2520demand%2520for%2520scalable%2520psychological%2520counseling%2520highlights%2520the%2520need%2520for%2520high-quality%252C%2520privacy-compliant%2520data%252C%2520yet%2520such%2520data%2520remains%2520scarce.%2520Here%2520we%2520introduce%2520MAGneT%252C%2520a%2520novel%2520multi-agent%2520framework%2520for%2520synthetic%2520psychological%2520counseling%2520session%2520generation%2520that%2520decomposes%2520counselor%2520response%2520generation%2520into%2520coordinated%2520sub-tasks%2520handled%2520by%2520specialized%2520LLM%2520agents%252C%2520each%2520modeling%2520a%2520key%2520psychological%2520technique.%2520Unlike%2520prior%2520single-agent%2520approaches%252C%2520MAGneT%2520better%2520captures%2520the%2520structure%2520and%2520nuance%2520of%2520real%2520counseling.%2520We%2520further%2520propose%2520a%2520unified%2520evaluation%2520framework%2520that%2520consolidates%2520diverse%2520automatic%2520metrics%2520and%2520expands%2520expert%2520assessment%2520from%2520four%2520to%2520nine%2520counseling%2520dimensions%252C%2520thus%2520addressing%2520inconsistencies%2520in%2520prior%2520evaluation%2520protocols.%2520Empirically%252C%2520MAGneT%2520substantially%2520outperforms%2520existing%2520methods%253A%2520experts%2520prefer%2520MAGneT-generated%2520sessions%2520in%252077.2%2525%2520of%2520cases%252C%2520and%2520sessions%2520generated%2520by%2520MAGneT%2520yield%25203.2%2525%2520higher%2520general%2520counseling%2520skills%2520and%25204.3%2525%2520higher%2520CBT-specific%2520skills%2520on%2520cognitive%2520therapy%2520rating%2520scale%2520%2528CTRS%2529.%2520A%2520open%2520source%2520Llama3-8B-Instruct%2520model%2520fine-tuned%2520on%2520MAGneT-generated%2520data%2520also%2520outperforms%2520models%2520fine-tuned%2520using%2520baseline%2520synthetic%2520datasets%2520by%25206.9%2525%2520on%2520average%2520on%2520CTRS.We%2520also%2520make%2520our%2520code%2520and%2520data%2520public.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGneT%3A%20Coordinated%20Multi-Agent%20Generation%20of%20Synthetic%20Multi-Turn%20Mental%20Health%20Counseling%20Sessions&entry.906535625=Aishik%20Mandal%20and%20Tanmoy%20Chakraborty%20and%20Iryna%20Gurevych&entry.1292438233=The%20growing%20demand%20for%20scalable%20psychological%20counseling%20highlights%20the%20need%20for%20high-quality%2C%20privacy-compliant%20data%2C%20yet%20such%20data%20remains%20scarce.%20Here%20we%20introduce%20MAGneT%2C%20a%20novel%20multi-agent%20framework%20for%20synthetic%20psychological%20counseling%20session%20generation%20that%20decomposes%20counselor%20response%20generation%20into%20coordinated%20sub-tasks%20handled%20by%20specialized%20LLM%20agents%2C%20each%20modeling%20a%20key%20psychological%20technique.%20Unlike%20prior%20single-agent%20approaches%2C%20MAGneT%20better%20captures%20the%20structure%20and%20nuance%20of%20real%20counseling.%20We%20further%20propose%20a%20unified%20evaluation%20framework%20that%20consolidates%20diverse%20automatic%20metrics%20and%20expands%20expert%20assessment%20from%20four%20to%20nine%20counseling%20dimensions%2C%20thus%20addressing%20inconsistencies%20in%20prior%20evaluation%20protocols.%20Empirically%2C%20MAGneT%20substantially%20outperforms%20existing%20methods%3A%20experts%20prefer%20MAGneT-generated%20sessions%20in%2077.2%25%20of%20cases%2C%20and%20sessions%20generated%20by%20MAGneT%20yield%203.2%25%20higher%20general%20counseling%20skills%20and%204.3%25%20higher%20CBT-specific%20skills%20on%20cognitive%20therapy%20rating%20scale%20%28CTRS%29.%20A%20open%20source%20Llama3-8B-Instruct%20model%20fine-tuned%20on%20MAGneT-generated%20data%20also%20outperforms%20models%20fine-tuned%20using%20baseline%20synthetic%20datasets%20by%206.9%25%20on%20average%20on%20CTRS.We%20also%20make%20our%20code%20and%20data%20public.&entry.1838667208=http%3A//arxiv.org/abs/2509.04183v2&entry.124074799=Read"},
{"title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment", "author": "John Page and Xuesong Niu and Kai Wu and Kun Gai", "abstract": "Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.", "link": "http://arxiv.org/abs/2601.05823v1", "date": "2026-01-09", "relevancy": 1.6824, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.578}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5583}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Latent%20Diffusion%20Models%20via%20Disentangled%20Representation%20Alignment&body=Title%3A%20Boosting%20Latent%20Diffusion%20Models%20via%20Disentangled%20Representation%20Alignment%0AAuthor%3A%20John%20Page%20and%20Xuesong%20Niu%20and%20Kai%20Wu%20and%20Kun%20Gai%0AAbstract%3A%20Latent%20Diffusion%20Models%20%28LDMs%29%20generate%20high-quality%20images%20by%20operating%20in%20a%20compressed%20latent%20space%2C%20typically%20obtained%20through%20image%20tokenizers%20such%20as%20Variational%20Autoencoders%20%28VAEs%29.%20In%20pursuit%20of%20a%20generation-friendly%20VAE%2C%20recent%20studies%20have%20explored%20leveraging%20Vision%20Foundation%20Models%20%28VFMs%29%20as%20representation%20alignment%20targets%20for%20VAEs%2C%20mirroring%20the%20approach%20commonly%20adopted%20for%20LDMs.%20Although%20this%20yields%20certain%20performance%20gains%2C%20using%20the%20same%20alignment%20target%20for%20both%20VAEs%20and%20LDMs%20overlooks%20their%20fundamentally%20different%20representational%20requirements.%20We%20advocate%20that%20while%20LDMs%20benefit%20from%20latents%20retaining%20high-level%20semantic%20concepts%2C%20VAEs%20should%20excel%20in%20semantic%20disentanglement%2C%20enabling%20encoding%20of%20attribute-level%20information%20in%20a%20structured%20way.%20To%20address%20this%2C%20we%20propose%20the%20Semantic%20disentangled%20VAE%20%28Send-VAE%29%2C%20explicitly%20optimized%20for%20disentangled%20representation%20learning%20through%20aligning%20its%20latent%20space%20with%20the%20semantic%20hierarchy%20of%20pre-trained%20VFMs.%20Our%20approach%20employs%20a%20non-linear%20mapper%20network%20to%20transform%20VAE%20latents%2C%20aligning%20them%20with%20VFMs%20to%20bridge%20the%20gap%20between%20attribute-level%20disentanglement%20and%20high-level%20semantics%2C%20facilitating%20effective%20guidance%20for%20VAE%20learning.%20We%20evaluate%20semantic%20disentanglement%20via%20linear%20probing%20on%20attribute%20prediction%20tasks%2C%20showing%20strong%20correlation%20with%20improved%20generation%20performance.%20Finally%2C%20using%20Send-VAE%2C%20we%20train%20flow-based%20transformers%20SiTs%3B%20experiments%20show%20Send-VAE%20significantly%20speeds%20up%20training%20and%20achieves%20a%20state-of-the-art%20FID%20of%201.21%20and%201.75%20with%20and%20without%20classifier-free%20guidance%20on%20ImageNet%20256x256.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Latent%2520Diffusion%2520Models%2520via%2520Disentangled%2520Representation%2520Alignment%26entry.906535625%3DJohn%2520Page%2520and%2520Xuesong%2520Niu%2520and%2520Kai%2520Wu%2520and%2520Kun%2520Gai%26entry.1292438233%3DLatent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520generate%2520high-quality%2520images%2520by%2520operating%2520in%2520a%2520compressed%2520latent%2520space%252C%2520typically%2520obtained%2520through%2520image%2520tokenizers%2520such%2520as%2520Variational%2520Autoencoders%2520%2528VAEs%2529.%2520In%2520pursuit%2520of%2520a%2520generation-friendly%2520VAE%252C%2520recent%2520studies%2520have%2520explored%2520leveraging%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520as%2520representation%2520alignment%2520targets%2520for%2520VAEs%252C%2520mirroring%2520the%2520approach%2520commonly%2520adopted%2520for%2520LDMs.%2520Although%2520this%2520yields%2520certain%2520performance%2520gains%252C%2520using%2520the%2520same%2520alignment%2520target%2520for%2520both%2520VAEs%2520and%2520LDMs%2520overlooks%2520their%2520fundamentally%2520different%2520representational%2520requirements.%2520We%2520advocate%2520that%2520while%2520LDMs%2520benefit%2520from%2520latents%2520retaining%2520high-level%2520semantic%2520concepts%252C%2520VAEs%2520should%2520excel%2520in%2520semantic%2520disentanglement%252C%2520enabling%2520encoding%2520of%2520attribute-level%2520information%2520in%2520a%2520structured%2520way.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Semantic%2520disentangled%2520VAE%2520%2528Send-VAE%2529%252C%2520explicitly%2520optimized%2520for%2520disentangled%2520representation%2520learning%2520through%2520aligning%2520its%2520latent%2520space%2520with%2520the%2520semantic%2520hierarchy%2520of%2520pre-trained%2520VFMs.%2520Our%2520approach%2520employs%2520a%2520non-linear%2520mapper%2520network%2520to%2520transform%2520VAE%2520latents%252C%2520aligning%2520them%2520with%2520VFMs%2520to%2520bridge%2520the%2520gap%2520between%2520attribute-level%2520disentanglement%2520and%2520high-level%2520semantics%252C%2520facilitating%2520effective%2520guidance%2520for%2520VAE%2520learning.%2520We%2520evaluate%2520semantic%2520disentanglement%2520via%2520linear%2520probing%2520on%2520attribute%2520prediction%2520tasks%252C%2520showing%2520strong%2520correlation%2520with%2520improved%2520generation%2520performance.%2520Finally%252C%2520using%2520Send-VAE%252C%2520we%2520train%2520flow-based%2520transformers%2520SiTs%253B%2520experiments%2520show%2520Send-VAE%2520significantly%2520speeds%2520up%2520training%2520and%2520achieves%2520a%2520state-of-the-art%2520FID%2520of%25201.21%2520and%25201.75%2520with%2520and%2520without%2520classifier-free%2520guidance%2520on%2520ImageNet%2520256x256.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Latent%20Diffusion%20Models%20via%20Disentangled%20Representation%20Alignment&entry.906535625=John%20Page%20and%20Xuesong%20Niu%20and%20Kai%20Wu%20and%20Kun%20Gai&entry.1292438233=Latent%20Diffusion%20Models%20%28LDMs%29%20generate%20high-quality%20images%20by%20operating%20in%20a%20compressed%20latent%20space%2C%20typically%20obtained%20through%20image%20tokenizers%20such%20as%20Variational%20Autoencoders%20%28VAEs%29.%20In%20pursuit%20of%20a%20generation-friendly%20VAE%2C%20recent%20studies%20have%20explored%20leveraging%20Vision%20Foundation%20Models%20%28VFMs%29%20as%20representation%20alignment%20targets%20for%20VAEs%2C%20mirroring%20the%20approach%20commonly%20adopted%20for%20LDMs.%20Although%20this%20yields%20certain%20performance%20gains%2C%20using%20the%20same%20alignment%20target%20for%20both%20VAEs%20and%20LDMs%20overlooks%20their%20fundamentally%20different%20representational%20requirements.%20We%20advocate%20that%20while%20LDMs%20benefit%20from%20latents%20retaining%20high-level%20semantic%20concepts%2C%20VAEs%20should%20excel%20in%20semantic%20disentanglement%2C%20enabling%20encoding%20of%20attribute-level%20information%20in%20a%20structured%20way.%20To%20address%20this%2C%20we%20propose%20the%20Semantic%20disentangled%20VAE%20%28Send-VAE%29%2C%20explicitly%20optimized%20for%20disentangled%20representation%20learning%20through%20aligning%20its%20latent%20space%20with%20the%20semantic%20hierarchy%20of%20pre-trained%20VFMs.%20Our%20approach%20employs%20a%20non-linear%20mapper%20network%20to%20transform%20VAE%20latents%2C%20aligning%20them%20with%20VFMs%20to%20bridge%20the%20gap%20between%20attribute-level%20disentanglement%20and%20high-level%20semantics%2C%20facilitating%20effective%20guidance%20for%20VAE%20learning.%20We%20evaluate%20semantic%20disentanglement%20via%20linear%20probing%20on%20attribute%20prediction%20tasks%2C%20showing%20strong%20correlation%20with%20improved%20generation%20performance.%20Finally%2C%20using%20Send-VAE%2C%20we%20train%20flow-based%20transformers%20SiTs%3B%20experiments%20show%20Send-VAE%20significantly%20speeds%20up%20training%20and%20achieves%20a%20state-of-the-art%20FID%20of%201.21%20and%201.75%20with%20and%20without%20classifier-free%20guidance%20on%20ImageNet%20256x256.&entry.1838667208=http%3A//arxiv.org/abs/2601.05823v1&entry.124074799=Read"},
{"title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law", "author": "Jakub Harasta and Matej Vasina and Martin Kornel and Tomas Foltynek", "abstract": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.", "link": "http://arxiv.org/abs/2601.05879v1", "date": "2026-01-09", "relevancy": 1.2033, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4028}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4007}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gender%20Bias%20in%20LLMs%3A%20Preliminary%20Evidence%20from%20Shared%20Parenting%20Scenario%20in%20Czech%20Family%20Law&body=Title%3A%20Gender%20Bias%20in%20LLMs%3A%20Preliminary%20Evidence%20from%20Shared%20Parenting%20Scenario%20in%20Czech%20Family%20Law%0AAuthor%3A%20Jakub%20Harasta%20and%20Matej%20Vasina%20and%20Martin%20Kornel%20and%20Tomas%20Foltynek%0AAbstract%3A%20Access%20to%20justice%20remains%20limited%20for%20many%20people%2C%20leading%20laypersons%20to%20increasingly%20rely%20on%20Large%20Language%20Models%20%28LLMs%29%20for%20legal%20self-help.%20Laypeople%20use%20these%20tools%20intuitively%2C%20which%20may%20lead%20them%20to%20form%20expectations%20based%20on%20incomplete%2C%20incorrect%2C%20or%20biased%20outputs.%20This%20study%20examines%20whether%20leading%20LLMs%20exhibit%20gender%20bias%20in%20their%20responses%20to%20a%20realistic%20family%20law%20scenario.%20We%20present%20an%20expert-designed%20divorce%20scenario%20grounded%20in%20Czech%20family%20law%20and%20evaluate%20four%20state-of-the-art%20LLMs%20GPT-5%20nano%2C%20Claude%20Haiku%204.5%2C%20Gemini%202.5%20Flash%2C%20and%20Llama%203.3%20in%20a%20fully%20zero-shot%20interaction.%20We%20deploy%20two%20versions%20of%20the%20scenario%2C%20one%20with%20gendered%20names%20and%20one%20with%20neutral%20labels%2C%20to%20establish%20a%20baseline%20for%20comparison.%20We%20further%20introduce%20nine%20legally%20relevant%20factors%20that%20vary%20the%20factual%20circumstances%20of%20the%20case%20and%20test%20whether%20these%20variations%20influence%20the%20models%27%20proposed%20shared-parenting%20ratios.%20Our%20preliminary%20results%20highlight%20differences%20across%20models%20and%20suggest%20gender-dependent%20patterns%20in%20the%20outcomes%20generated%20by%20some%20systems.%20The%20findings%20underscore%20both%20the%20risks%20associated%20with%20laypeople%27s%20reliance%20on%20LLMs%20for%20legal%20guidance%20and%20the%20need%20for%20more%20robust%20evaluation%20of%20model%20behavior%20in%20sensitive%20legal%20contexts.%20We%20present%20exploratory%20and%20descriptive%20evidence%20intended%20to%20identify%20systematic%20asymmetries%20rather%20than%20to%20establish%20causal%20effects.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGender%2520Bias%2520in%2520LLMs%253A%2520Preliminary%2520Evidence%2520from%2520Shared%2520Parenting%2520Scenario%2520in%2520Czech%2520Family%2520Law%26entry.906535625%3DJakub%2520Harasta%2520and%2520Matej%2520Vasina%2520and%2520Martin%2520Kornel%2520and%2520Tomas%2520Foltynek%26entry.1292438233%3DAccess%2520to%2520justice%2520remains%2520limited%2520for%2520many%2520people%252C%2520leading%2520laypersons%2520to%2520increasingly%2520rely%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520legal%2520self-help.%2520Laypeople%2520use%2520these%2520tools%2520intuitively%252C%2520which%2520may%2520lead%2520them%2520to%2520form%2520expectations%2520based%2520on%2520incomplete%252C%2520incorrect%252C%2520or%2520biased%2520outputs.%2520This%2520study%2520examines%2520whether%2520leading%2520LLMs%2520exhibit%2520gender%2520bias%2520in%2520their%2520responses%2520to%2520a%2520realistic%2520family%2520law%2520scenario.%2520We%2520present%2520an%2520expert-designed%2520divorce%2520scenario%2520grounded%2520in%2520Czech%2520family%2520law%2520and%2520evaluate%2520four%2520state-of-the-art%2520LLMs%2520GPT-5%2520nano%252C%2520Claude%2520Haiku%25204.5%252C%2520Gemini%25202.5%2520Flash%252C%2520and%2520Llama%25203.3%2520in%2520a%2520fully%2520zero-shot%2520interaction.%2520We%2520deploy%2520two%2520versions%2520of%2520the%2520scenario%252C%2520one%2520with%2520gendered%2520names%2520and%2520one%2520with%2520neutral%2520labels%252C%2520to%2520establish%2520a%2520baseline%2520for%2520comparison.%2520We%2520further%2520introduce%2520nine%2520legally%2520relevant%2520factors%2520that%2520vary%2520the%2520factual%2520circumstances%2520of%2520the%2520case%2520and%2520test%2520whether%2520these%2520variations%2520influence%2520the%2520models%2527%2520proposed%2520shared-parenting%2520ratios.%2520Our%2520preliminary%2520results%2520highlight%2520differences%2520across%2520models%2520and%2520suggest%2520gender-dependent%2520patterns%2520in%2520the%2520outcomes%2520generated%2520by%2520some%2520systems.%2520The%2520findings%2520underscore%2520both%2520the%2520risks%2520associated%2520with%2520laypeople%2527s%2520reliance%2520on%2520LLMs%2520for%2520legal%2520guidance%2520and%2520the%2520need%2520for%2520more%2520robust%2520evaluation%2520of%2520model%2520behavior%2520in%2520sensitive%2520legal%2520contexts.%2520We%2520present%2520exploratory%2520and%2520descriptive%2520evidence%2520intended%2520to%2520identify%2520systematic%2520asymmetries%2520rather%2520than%2520to%2520establish%2520causal%2520effects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gender%20Bias%20in%20LLMs%3A%20Preliminary%20Evidence%20from%20Shared%20Parenting%20Scenario%20in%20Czech%20Family%20Law&entry.906535625=Jakub%20Harasta%20and%20Matej%20Vasina%20and%20Martin%20Kornel%20and%20Tomas%20Foltynek&entry.1292438233=Access%20to%20justice%20remains%20limited%20for%20many%20people%2C%20leading%20laypersons%20to%20increasingly%20rely%20on%20Large%20Language%20Models%20%28LLMs%29%20for%20legal%20self-help.%20Laypeople%20use%20these%20tools%20intuitively%2C%20which%20may%20lead%20them%20to%20form%20expectations%20based%20on%20incomplete%2C%20incorrect%2C%20or%20biased%20outputs.%20This%20study%20examines%20whether%20leading%20LLMs%20exhibit%20gender%20bias%20in%20their%20responses%20to%20a%20realistic%20family%20law%20scenario.%20We%20present%20an%20expert-designed%20divorce%20scenario%20grounded%20in%20Czech%20family%20law%20and%20evaluate%20four%20state-of-the-art%20LLMs%20GPT-5%20nano%2C%20Claude%20Haiku%204.5%2C%20Gemini%202.5%20Flash%2C%20and%20Llama%203.3%20in%20a%20fully%20zero-shot%20interaction.%20We%20deploy%20two%20versions%20of%20the%20scenario%2C%20one%20with%20gendered%20names%20and%20one%20with%20neutral%20labels%2C%20to%20establish%20a%20baseline%20for%20comparison.%20We%20further%20introduce%20nine%20legally%20relevant%20factors%20that%20vary%20the%20factual%20circumstances%20of%20the%20case%20and%20test%20whether%20these%20variations%20influence%20the%20models%27%20proposed%20shared-parenting%20ratios.%20Our%20preliminary%20results%20highlight%20differences%20across%20models%20and%20suggest%20gender-dependent%20patterns%20in%20the%20outcomes%20generated%20by%20some%20systems.%20The%20findings%20underscore%20both%20the%20risks%20associated%20with%20laypeople%27s%20reliance%20on%20LLMs%20for%20legal%20guidance%20and%20the%20need%20for%20more%20robust%20evaluation%20of%20model%20behavior%20in%20sensitive%20legal%20contexts.%20We%20present%20exploratory%20and%20descriptive%20evidence%20intended%20to%20identify%20systematic%20asymmetries%20rather%20than%20to%20establish%20causal%20effects.&entry.1838667208=http%3A//arxiv.org/abs/2601.05879v1&entry.124074799=Read"},
{"title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level", "author": "Jia Li and Yuxin Su and Michael R. Lyu", "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.", "link": "http://arxiv.org/abs/2601.03731v2", "date": "2026-01-09", "relevancy": 1.519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Laboratory%20to%20Real-World%20Applications%3A%20Benchmarking%20Agentic%20Code%20Reasoning%20at%20the%20Repository%20Level&body=Title%3A%20From%20Laboratory%20to%20Real-World%20Applications%3A%20Benchmarking%20Agentic%20Code%20Reasoning%20at%20the%20Repository%20Level%0AAuthor%3A%20Jia%20Li%20and%20Yuxin%20Su%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20As%20large%20language%20models%20%28LLMs%29%20evolve%20into%20autonomous%20agents%2C%20evaluating%20repository-level%20reasoning%2C%20the%20ability%20to%20maintain%20logical%20consistency%20across%20massive%2C%20real-world%2C%20interdependent%20file%20systems%2C%20has%20become%20critical.%20Current%20benchmarks%20typically%20fluctuate%20between%20isolated%20code%20snippets%20and%20black-box%20evaluations.%20We%20present%20RepoReason%2C%20a%20white-box%20diagnostic%20benchmark%20centered%20on%20abductive%20assertion%20verification.%20To%20eliminate%20memorization%20while%20preserving%20authentic%20logical%20depth%2C%20we%20implement%20an%20execution-driven%20mutation%20framework%20that%20utilizes%20the%20environment%20as%20a%20semantic%20oracle%20to%20regenerate%20ground-truth%20states.%20Furthermore%2C%20we%20establish%20a%20fine-grained%20diagnostic%20system%20using%20dynamic%20program%20slicing%2C%20quantifying%20reasoning%20via%20three%20orthogonal%20metrics%3A%20%24ESV%24%20%28reading%20load%29%2C%20%24MCL%24%20%28simulation%20depth%29%2C%20and%20%24DFI%24%20%28integration%20width%29.%20Comprehensive%20evaluations%20of%20frontier%20models%20%28e.g.%2C%20Claude-4.5-Sonnet%2C%20DeepSeek-v3.1-Terminus%29%20reveal%20a%20prevalent%20aggregation%20deficit%2C%20where%20integration%20width%20serves%20as%20the%20primary%20cognitive%20bottleneck.%20Our%20findings%20provide%20granular%20white-box%20insights%20for%20optimizing%20the%20next%20generation%20of%20agentic%20software%20engineering.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Laboratory%2520to%2520Real-World%2520Applications%253A%2520Benchmarking%2520Agentic%2520Code%2520Reasoning%2520at%2520the%2520Repository%2520Level%26entry.906535625%3DJia%2520Li%2520and%2520Yuxin%2520Su%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3DAs%2520large%2520language%2520models%2520%2528LLMs%2529%2520evolve%2520into%2520autonomous%2520agents%252C%2520evaluating%2520repository-level%2520reasoning%252C%2520the%2520ability%2520to%2520maintain%2520logical%2520consistency%2520across%2520massive%252C%2520real-world%252C%2520interdependent%2520file%2520systems%252C%2520has%2520become%2520critical.%2520Current%2520benchmarks%2520typically%2520fluctuate%2520between%2520isolated%2520code%2520snippets%2520and%2520black-box%2520evaluations.%2520We%2520present%2520RepoReason%252C%2520a%2520white-box%2520diagnostic%2520benchmark%2520centered%2520on%2520abductive%2520assertion%2520verification.%2520To%2520eliminate%2520memorization%2520while%2520preserving%2520authentic%2520logical%2520depth%252C%2520we%2520implement%2520an%2520execution-driven%2520mutation%2520framework%2520that%2520utilizes%2520the%2520environment%2520as%2520a%2520semantic%2520oracle%2520to%2520regenerate%2520ground-truth%2520states.%2520Furthermore%252C%2520we%2520establish%2520a%2520fine-grained%2520diagnostic%2520system%2520using%2520dynamic%2520program%2520slicing%252C%2520quantifying%2520reasoning%2520via%2520three%2520orthogonal%2520metrics%253A%2520%2524ESV%2524%2520%2528reading%2520load%2529%252C%2520%2524MCL%2524%2520%2528simulation%2520depth%2529%252C%2520and%2520%2524DFI%2524%2520%2528integration%2520width%2529.%2520Comprehensive%2520evaluations%2520of%2520frontier%2520models%2520%2528e.g.%252C%2520Claude-4.5-Sonnet%252C%2520DeepSeek-v3.1-Terminus%2529%2520reveal%2520a%2520prevalent%2520aggregation%2520deficit%252C%2520where%2520integration%2520width%2520serves%2520as%2520the%2520primary%2520cognitive%2520bottleneck.%2520Our%2520findings%2520provide%2520granular%2520white-box%2520insights%2520for%2520optimizing%2520the%2520next%2520generation%2520of%2520agentic%2520software%2520engineering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Laboratory%20to%20Real-World%20Applications%3A%20Benchmarking%20Agentic%20Code%20Reasoning%20at%20the%20Repository%20Level&entry.906535625=Jia%20Li%20and%20Yuxin%20Su%20and%20Michael%20R.%20Lyu&entry.1292438233=As%20large%20language%20models%20%28LLMs%29%20evolve%20into%20autonomous%20agents%2C%20evaluating%20repository-level%20reasoning%2C%20the%20ability%20to%20maintain%20logical%20consistency%20across%20massive%2C%20real-world%2C%20interdependent%20file%20systems%2C%20has%20become%20critical.%20Current%20benchmarks%20typically%20fluctuate%20between%20isolated%20code%20snippets%20and%20black-box%20evaluations.%20We%20present%20RepoReason%2C%20a%20white-box%20diagnostic%20benchmark%20centered%20on%20abductive%20assertion%20verification.%20To%20eliminate%20memorization%20while%20preserving%20authentic%20logical%20depth%2C%20we%20implement%20an%20execution-driven%20mutation%20framework%20that%20utilizes%20the%20environment%20as%20a%20semantic%20oracle%20to%20regenerate%20ground-truth%20states.%20Furthermore%2C%20we%20establish%20a%20fine-grained%20diagnostic%20system%20using%20dynamic%20program%20slicing%2C%20quantifying%20reasoning%20via%20three%20orthogonal%20metrics%3A%20%24ESV%24%20%28reading%20load%29%2C%20%24MCL%24%20%28simulation%20depth%29%2C%20and%20%24DFI%24%20%28integration%20width%29.%20Comprehensive%20evaluations%20of%20frontier%20models%20%28e.g.%2C%20Claude-4.5-Sonnet%2C%20DeepSeek-v3.1-Terminus%29%20reveal%20a%20prevalent%20aggregation%20deficit%2C%20where%20integration%20width%20serves%20as%20the%20primary%20cognitive%20bottleneck.%20Our%20findings%20provide%20granular%20white-box%20insights%20for%20optimizing%20the%20next%20generation%20of%20agentic%20software%20engineering.&entry.1838667208=http%3A//arxiv.org/abs/2601.03731v2&entry.124074799=Read"},
{"title": "Machine Learning Framework for Characterizing Processing-Structure Relationship in Block Copolymer Thin Films", "author": "Bradley Lamb and Saroj Upreti and Yunfei Wang and Daniel Struble and Chenhui Zhu and Guillaume Freychet and Xiaodan Gu and Boran Ma", "abstract": "The morphology of block copolymers (BCPs) critically influences material properties and applications. This work introduces a machine learning (ML)-enabled, high-throughput framework for analyzing grazing incidence small-angle X-ray scattering (GISAXS) data and atomic force microscopy (AFM) images to characterize BCP thin film morphology. A convolutional neural network was trained to classify AFM images by morphology type, achieving 97% testing accuracy. Classified images were then analyzed to extract 2D grain size measurements from the samples in a high-throughput manner. ML models were developed to predict morphological features based on processing parameters such as solvent ratio, additive type, and additive ratio. GISAXS-based properties were predicted with strong performances ($R^2$ > 0.75), while AFM-based property predictions were less accurate ($R^2$ < 0.60), likely due to the localized nature of AFM measurements compared to the bulk information captured by GISAXS. Beyond model performance, interpretability was addressed using Shapley Additive exPlanations (SHAP). SHAP analysis revealed that the additive ratio had the largest impact on morphological predictions, where additive provides the BCP chains with increased volume to rearrange into thermodynamically favorable morphologies. This interpretability helps validate model predictions and offers insight into parameter importance. Altogether, the presented framework combining high-throughput characterization and interpretable ML offers an approach to exploring and optimizing BCP thin film morphology across a broad processing landscape.", "link": "http://arxiv.org/abs/2505.23064v2", "date": "2026-01-09", "relevancy": 1.9069, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Framework%20for%20Characterizing%20Processing-Structure%20Relationship%20in%20Block%20Copolymer%20Thin%20Films&body=Title%3A%20Machine%20Learning%20Framework%20for%20Characterizing%20Processing-Structure%20Relationship%20in%20Block%20Copolymer%20Thin%20Films%0AAuthor%3A%20Bradley%20Lamb%20and%20Saroj%20Upreti%20and%20Yunfei%20Wang%20and%20Daniel%20Struble%20and%20Chenhui%20Zhu%20and%20Guillaume%20Freychet%20and%20Xiaodan%20Gu%20and%20Boran%20Ma%0AAbstract%3A%20The%20morphology%20of%20block%20copolymers%20%28BCPs%29%20critically%20influences%20material%20properties%20and%20applications.%20This%20work%20introduces%20a%20machine%20learning%20%28ML%29-enabled%2C%20high-throughput%20framework%20for%20analyzing%20grazing%20incidence%20small-angle%20X-ray%20scattering%20%28GISAXS%29%20data%20and%20atomic%20force%20microscopy%20%28AFM%29%20images%20to%20characterize%20BCP%20thin%20film%20morphology.%20A%20convolutional%20neural%20network%20was%20trained%20to%20classify%20AFM%20images%20by%20morphology%20type%2C%20achieving%2097%25%20testing%20accuracy.%20Classified%20images%20were%20then%20analyzed%20to%20extract%202D%20grain%20size%20measurements%20from%20the%20samples%20in%20a%20high-throughput%20manner.%20ML%20models%20were%20developed%20to%20predict%20morphological%20features%20based%20on%20processing%20parameters%20such%20as%20solvent%20ratio%2C%20additive%20type%2C%20and%20additive%20ratio.%20GISAXS-based%20properties%20were%20predicted%20with%20strong%20performances%20%28%24R%5E2%24%20%3E%200.75%29%2C%20while%20AFM-based%20property%20predictions%20were%20less%20accurate%20%28%24R%5E2%24%20%3C%200.60%29%2C%20likely%20due%20to%20the%20localized%20nature%20of%20AFM%20measurements%20compared%20to%20the%20bulk%20information%20captured%20by%20GISAXS.%20Beyond%20model%20performance%2C%20interpretability%20was%20addressed%20using%20Shapley%20Additive%20exPlanations%20%28SHAP%29.%20SHAP%20analysis%20revealed%20that%20the%20additive%20ratio%20had%20the%20largest%20impact%20on%20morphological%20predictions%2C%20where%20additive%20provides%20the%20BCP%20chains%20with%20increased%20volume%20to%20rearrange%20into%20thermodynamically%20favorable%20morphologies.%20This%20interpretability%20helps%20validate%20model%20predictions%20and%20offers%20insight%20into%20parameter%20importance.%20Altogether%2C%20the%20presented%20framework%20combining%20high-throughput%20characterization%20and%20interpretable%20ML%20offers%20an%20approach%20to%20exploring%20and%20optimizing%20BCP%20thin%20film%20morphology%20across%20a%20broad%20processing%20landscape.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Framework%2520for%2520Characterizing%2520Processing-Structure%2520Relationship%2520in%2520Block%2520Copolymer%2520Thin%2520Films%26entry.906535625%3DBradley%2520Lamb%2520and%2520Saroj%2520Upreti%2520and%2520Yunfei%2520Wang%2520and%2520Daniel%2520Struble%2520and%2520Chenhui%2520Zhu%2520and%2520Guillaume%2520Freychet%2520and%2520Xiaodan%2520Gu%2520and%2520Boran%2520Ma%26entry.1292438233%3DThe%2520morphology%2520of%2520block%2520copolymers%2520%2528BCPs%2529%2520critically%2520influences%2520material%2520properties%2520and%2520applications.%2520This%2520work%2520introduces%2520a%2520machine%2520learning%2520%2528ML%2529-enabled%252C%2520high-throughput%2520framework%2520for%2520analyzing%2520grazing%2520incidence%2520small-angle%2520X-ray%2520scattering%2520%2528GISAXS%2529%2520data%2520and%2520atomic%2520force%2520microscopy%2520%2528AFM%2529%2520images%2520to%2520characterize%2520BCP%2520thin%2520film%2520morphology.%2520A%2520convolutional%2520neural%2520network%2520was%2520trained%2520to%2520classify%2520AFM%2520images%2520by%2520morphology%2520type%252C%2520achieving%252097%2525%2520testing%2520accuracy.%2520Classified%2520images%2520were%2520then%2520analyzed%2520to%2520extract%25202D%2520grain%2520size%2520measurements%2520from%2520the%2520samples%2520in%2520a%2520high-throughput%2520manner.%2520ML%2520models%2520were%2520developed%2520to%2520predict%2520morphological%2520features%2520based%2520on%2520processing%2520parameters%2520such%2520as%2520solvent%2520ratio%252C%2520additive%2520type%252C%2520and%2520additive%2520ratio.%2520GISAXS-based%2520properties%2520were%2520predicted%2520with%2520strong%2520performances%2520%2528%2524R%255E2%2524%2520%253E%25200.75%2529%252C%2520while%2520AFM-based%2520property%2520predictions%2520were%2520less%2520accurate%2520%2528%2524R%255E2%2524%2520%253C%25200.60%2529%252C%2520likely%2520due%2520to%2520the%2520localized%2520nature%2520of%2520AFM%2520measurements%2520compared%2520to%2520the%2520bulk%2520information%2520captured%2520by%2520GISAXS.%2520Beyond%2520model%2520performance%252C%2520interpretability%2520was%2520addressed%2520using%2520Shapley%2520Additive%2520exPlanations%2520%2528SHAP%2529.%2520SHAP%2520analysis%2520revealed%2520that%2520the%2520additive%2520ratio%2520had%2520the%2520largest%2520impact%2520on%2520morphological%2520predictions%252C%2520where%2520additive%2520provides%2520the%2520BCP%2520chains%2520with%2520increased%2520volume%2520to%2520rearrange%2520into%2520thermodynamically%2520favorable%2520morphologies.%2520This%2520interpretability%2520helps%2520validate%2520model%2520predictions%2520and%2520offers%2520insight%2520into%2520parameter%2520importance.%2520Altogether%252C%2520the%2520presented%2520framework%2520combining%2520high-throughput%2520characterization%2520and%2520interpretable%2520ML%2520offers%2520an%2520approach%2520to%2520exploring%2520and%2520optimizing%2520BCP%2520thin%2520film%2520morphology%2520across%2520a%2520broad%2520processing%2520landscape.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Framework%20for%20Characterizing%20Processing-Structure%20Relationship%20in%20Block%20Copolymer%20Thin%20Films&entry.906535625=Bradley%20Lamb%20and%20Saroj%20Upreti%20and%20Yunfei%20Wang%20and%20Daniel%20Struble%20and%20Chenhui%20Zhu%20and%20Guillaume%20Freychet%20and%20Xiaodan%20Gu%20and%20Boran%20Ma&entry.1292438233=The%20morphology%20of%20block%20copolymers%20%28BCPs%29%20critically%20influences%20material%20properties%20and%20applications.%20This%20work%20introduces%20a%20machine%20learning%20%28ML%29-enabled%2C%20high-throughput%20framework%20for%20analyzing%20grazing%20incidence%20small-angle%20X-ray%20scattering%20%28GISAXS%29%20data%20and%20atomic%20force%20microscopy%20%28AFM%29%20images%20to%20characterize%20BCP%20thin%20film%20morphology.%20A%20convolutional%20neural%20network%20was%20trained%20to%20classify%20AFM%20images%20by%20morphology%20type%2C%20achieving%2097%25%20testing%20accuracy.%20Classified%20images%20were%20then%20analyzed%20to%20extract%202D%20grain%20size%20measurements%20from%20the%20samples%20in%20a%20high-throughput%20manner.%20ML%20models%20were%20developed%20to%20predict%20morphological%20features%20based%20on%20processing%20parameters%20such%20as%20solvent%20ratio%2C%20additive%20type%2C%20and%20additive%20ratio.%20GISAXS-based%20properties%20were%20predicted%20with%20strong%20performances%20%28%24R%5E2%24%20%3E%200.75%29%2C%20while%20AFM-based%20property%20predictions%20were%20less%20accurate%20%28%24R%5E2%24%20%3C%200.60%29%2C%20likely%20due%20to%20the%20localized%20nature%20of%20AFM%20measurements%20compared%20to%20the%20bulk%20information%20captured%20by%20GISAXS.%20Beyond%20model%20performance%2C%20interpretability%20was%20addressed%20using%20Shapley%20Additive%20exPlanations%20%28SHAP%29.%20SHAP%20analysis%20revealed%20that%20the%20additive%20ratio%20had%20the%20largest%20impact%20on%20morphological%20predictions%2C%20where%20additive%20provides%20the%20BCP%20chains%20with%20increased%20volume%20to%20rearrange%20into%20thermodynamically%20favorable%20morphologies.%20This%20interpretability%20helps%20validate%20model%20predictions%20and%20offers%20insight%20into%20parameter%20importance.%20Altogether%2C%20the%20presented%20framework%20combining%20high-throughput%20characterization%20and%20interpretable%20ML%20offers%20an%20approach%20to%20exploring%20and%20optimizing%20BCP%20thin%20film%20morphology%20across%20a%20broad%20processing%20landscape.&entry.1838667208=http%3A//arxiv.org/abs/2505.23064v2&entry.124074799=Read"},
{"title": "GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras", "author": "Weimin Liu and Wenjun Wang and Joshua H. Meng", "abstract": "Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.", "link": "http://arxiv.org/abs/2601.05839v1", "date": "2026-01-09", "relevancy": 1.8782, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.635}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6298}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSurDepth%3A%20Spatial%20Geometry-Consistent%20Self-Supervised%20Depth%20Estimation%20for%20Surround-View%20Cameras&body=Title%3A%20GeoSurDepth%3A%20Spatial%20Geometry-Consistent%20Self-Supervised%20Depth%20Estimation%20for%20Surround-View%20Cameras%0AAuthor%3A%20Weimin%20Liu%20and%20Wenjun%20Wang%20and%20Joshua%20H.%20Meng%0AAbstract%3A%20Accurate%20surround-view%20depth%20estimation%20provides%20a%20competitive%20alternative%20to%20laser-based%20sensors%20and%20is%20essential%20for%203D%20scene%20understanding%20in%20autonomous%20driving.%20While%20prior%20studies%20have%20proposed%20various%20approaches%20that%20primarily%20focus%20on%20enforcing%20cross-view%20constraints%20at%20the%20photometric%20level%2C%20few%20explicitly%20exploit%20the%20rich%20geometric%20structure%20inherent%20in%20both%20monocular%20and%20surround-view%20setting.%20In%20this%20work%2C%20we%20propose%20GeoSurDepth%2C%20a%20framework%20that%20leverages%20geometry%20consistency%20as%20the%20primary%20cue%20for%20surround-view%20depth%20estimation.%20Concretely%2C%20we%20utilize%20foundation%20models%20as%20a%20pseudo%20geometry%20prior%20and%20feature%20representation%20enhancement%20tool%20to%20guide%20the%20network%20to%20maintain%20surface%20normal%20consistency%20in%20spatial%203D%20space%20and%20regularize%20object-%20and%20texture-consistent%20depth%20estimation%20in%202D.%20In%20addition%2C%20we%20introduce%20a%20novel%20view%20synthesis%20pipeline%20where%202D-3D%20lifting%20is%20achieved%20with%20dense%20depth%20reconstructed%20via%20spatial%20warping%2C%20encouraging%20additional%20photometric%20supervision%20across%20temporal%2C%20spatial%2C%20and%20spatial-temporal%20contexts%2C%20and%20compensating%20for%20the%20limitations%20of%20single-view%20image%20reconstruction.%20Finally%2C%20a%20newly-proposed%20adaptive%20joint%20motion%20learning%20strategy%20enables%20the%20network%20to%20adaptively%20emphasize%20informative%20spatial%20geometry%20cues%20for%20improved%20motion%20reasoning.%20Extensive%20experiments%20on%20DDAD%20and%20nuScenes%20demonstrate%20that%20GeoSurDepth%20achieves%20state-of-the-art%20performance%2C%20validating%20the%20effectiveness%20of%20our%20approach.%20Our%20framework%20highlights%20the%20importance%20of%20exploiting%20geometry%20coherence%20and%20consistency%20for%20robust%20self-supervised%20multi-view%20depth%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSurDepth%253A%2520Spatial%2520Geometry-Consistent%2520Self-Supervised%2520Depth%2520Estimation%2520for%2520Surround-View%2520Cameras%26entry.906535625%3DWeimin%2520Liu%2520and%2520Wenjun%2520Wang%2520and%2520Joshua%2520H.%2520Meng%26entry.1292438233%3DAccurate%2520surround-view%2520depth%2520estimation%2520provides%2520a%2520competitive%2520alternative%2520to%2520laser-based%2520sensors%2520and%2520is%2520essential%2520for%25203D%2520scene%2520understanding%2520in%2520autonomous%2520driving.%2520While%2520prior%2520studies%2520have%2520proposed%2520various%2520approaches%2520that%2520primarily%2520focus%2520on%2520enforcing%2520cross-view%2520constraints%2520at%2520the%2520photometric%2520level%252C%2520few%2520explicitly%2520exploit%2520the%2520rich%2520geometric%2520structure%2520inherent%2520in%2520both%2520monocular%2520and%2520surround-view%2520setting.%2520In%2520this%2520work%252C%2520we%2520propose%2520GeoSurDepth%252C%2520a%2520framework%2520that%2520leverages%2520geometry%2520consistency%2520as%2520the%2520primary%2520cue%2520for%2520surround-view%2520depth%2520estimation.%2520Concretely%252C%2520we%2520utilize%2520foundation%2520models%2520as%2520a%2520pseudo%2520geometry%2520prior%2520and%2520feature%2520representation%2520enhancement%2520tool%2520to%2520guide%2520the%2520network%2520to%2520maintain%2520surface%2520normal%2520consistency%2520in%2520spatial%25203D%2520space%2520and%2520regularize%2520object-%2520and%2520texture-consistent%2520depth%2520estimation%2520in%25202D.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520novel%2520view%2520synthesis%2520pipeline%2520where%25202D-3D%2520lifting%2520is%2520achieved%2520with%2520dense%2520depth%2520reconstructed%2520via%2520spatial%2520warping%252C%2520encouraging%2520additional%2520photometric%2520supervision%2520across%2520temporal%252C%2520spatial%252C%2520and%2520spatial-temporal%2520contexts%252C%2520and%2520compensating%2520for%2520the%2520limitations%2520of%2520single-view%2520image%2520reconstruction.%2520Finally%252C%2520a%2520newly-proposed%2520adaptive%2520joint%2520motion%2520learning%2520strategy%2520enables%2520the%2520network%2520to%2520adaptively%2520emphasize%2520informative%2520spatial%2520geometry%2520cues%2520for%2520improved%2520motion%2520reasoning.%2520Extensive%2520experiments%2520on%2520DDAD%2520and%2520nuScenes%2520demonstrate%2520that%2520GeoSurDepth%2520achieves%2520state-of-the-art%2520performance%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Our%2520framework%2520highlights%2520the%2520importance%2520of%2520exploiting%2520geometry%2520coherence%2520and%2520consistency%2520for%2520robust%2520self-supervised%2520multi-view%2520depth%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSurDepth%3A%20Spatial%20Geometry-Consistent%20Self-Supervised%20Depth%20Estimation%20for%20Surround-View%20Cameras&entry.906535625=Weimin%20Liu%20and%20Wenjun%20Wang%20and%20Joshua%20H.%20Meng&entry.1292438233=Accurate%20surround-view%20depth%20estimation%20provides%20a%20competitive%20alternative%20to%20laser-based%20sensors%20and%20is%20essential%20for%203D%20scene%20understanding%20in%20autonomous%20driving.%20While%20prior%20studies%20have%20proposed%20various%20approaches%20that%20primarily%20focus%20on%20enforcing%20cross-view%20constraints%20at%20the%20photometric%20level%2C%20few%20explicitly%20exploit%20the%20rich%20geometric%20structure%20inherent%20in%20both%20monocular%20and%20surround-view%20setting.%20In%20this%20work%2C%20we%20propose%20GeoSurDepth%2C%20a%20framework%20that%20leverages%20geometry%20consistency%20as%20the%20primary%20cue%20for%20surround-view%20depth%20estimation.%20Concretely%2C%20we%20utilize%20foundation%20models%20as%20a%20pseudo%20geometry%20prior%20and%20feature%20representation%20enhancement%20tool%20to%20guide%20the%20network%20to%20maintain%20surface%20normal%20consistency%20in%20spatial%203D%20space%20and%20regularize%20object-%20and%20texture-consistent%20depth%20estimation%20in%202D.%20In%20addition%2C%20we%20introduce%20a%20novel%20view%20synthesis%20pipeline%20where%202D-3D%20lifting%20is%20achieved%20with%20dense%20depth%20reconstructed%20via%20spatial%20warping%2C%20encouraging%20additional%20photometric%20supervision%20across%20temporal%2C%20spatial%2C%20and%20spatial-temporal%20contexts%2C%20and%20compensating%20for%20the%20limitations%20of%20single-view%20image%20reconstruction.%20Finally%2C%20a%20newly-proposed%20adaptive%20joint%20motion%20learning%20strategy%20enables%20the%20network%20to%20adaptively%20emphasize%20informative%20spatial%20geometry%20cues%20for%20improved%20motion%20reasoning.%20Extensive%20experiments%20on%20DDAD%20and%20nuScenes%20demonstrate%20that%20GeoSurDepth%20achieves%20state-of-the-art%20performance%2C%20validating%20the%20effectiveness%20of%20our%20approach.%20Our%20framework%20highlights%20the%20importance%20of%20exploiting%20geometry%20coherence%20and%20consistency%20for%20robust%20self-supervised%20multi-view%20depth%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2601.05839v1&entry.124074799=Read"},
{"title": "Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer", "author": "Jorge Sastre and Daniel Faronbi and Jos\u00e9 Miguel Alonso and Peter Traver and Javier Ib\u00e1\u00f1ez and Nuria Lloret", "abstract": "The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Pad\u00e9 approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.", "link": "http://arxiv.org/abs/2512.20777v2", "date": "2026-01-09", "relevancy": 1.5096, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5678}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4858}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Matrix%20Exponential%20for%20Generative%20AI%20Flows%3A%20A%20Taylor-Based%20Approach%20Beyond%20Paterson--Stockmeyer&body=Title%3A%20Improving%20Matrix%20Exponential%20for%20Generative%20AI%20Flows%3A%20A%20Taylor-Based%20Approach%20Beyond%20Paterson--Stockmeyer%0AAuthor%3A%20Jorge%20Sastre%20and%20Daniel%20Faronbi%20and%20Jos%C3%A9%20Miguel%20Alonso%20and%20Peter%20Traver%20and%20Javier%20Ib%C3%A1%C3%B1ez%20and%20Nuria%20Lloret%0AAbstract%3A%20The%20matrix%20exponential%20is%20a%20fundamental%20operator%20in%20scientific%20computing%20and%20system%20simulation%2C%20with%20applications%20ranging%20from%20control%20theory%20and%20quantum%20mechanics%20to%20modern%20generative%20machine%20learning.%20While%20Pad%C3%A9%20approximants%20combined%20with%20scaling%20and%20squaring%20have%20long%20served%20as%20the%20standard%2C%20recent%20Taylor-based%20methods%2C%20which%20utilize%20polynomial%20evaluation%20schemes%20that%20surpass%20the%20classical%20Paterson--Stockmeyer%20technique%2C%20offer%20superior%20accuracy%20and%20reduced%20computational%20complexity.%20This%20paper%20presents%20an%20optimized%20Taylor-based%20algorithm%20for%20the%20matrix%20exponential%2C%20specifically%20designed%20for%20the%20high-throughput%20requirements%20of%20generative%20AI%20flows.%20We%20provide%20a%20rigorous%20error%20analysis%20and%20develop%20a%20dynamic%20selection%20strategy%20for%20the%20Taylor%20order%20and%20scaling%20factor%20to%20minimize%20computational%20effort%20under%20a%20prescribed%20error%20tolerance.%20Extensive%20numerical%20experiments%20demonstrate%20that%20our%20approach%20provides%20significant%20acceleration%20and%20maintains%20high%20numerical%20stability%20compared%20to%20existing%20state-of-the-art%20implementations.%20These%20results%20establish%20the%20proposed%20method%20as%20a%20highly%20efficient%20tool%20for%20large-scale%20generative%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Matrix%2520Exponential%2520for%2520Generative%2520AI%2520Flows%253A%2520A%2520Taylor-Based%2520Approach%2520Beyond%2520Paterson--Stockmeyer%26entry.906535625%3DJorge%2520Sastre%2520and%2520Daniel%2520Faronbi%2520and%2520Jos%25C3%25A9%2520Miguel%2520Alonso%2520and%2520Peter%2520Traver%2520and%2520Javier%2520Ib%25C3%25A1%25C3%25B1ez%2520and%2520Nuria%2520Lloret%26entry.1292438233%3DThe%2520matrix%2520exponential%2520is%2520a%2520fundamental%2520operator%2520in%2520scientific%2520computing%2520and%2520system%2520simulation%252C%2520with%2520applications%2520ranging%2520from%2520control%2520theory%2520and%2520quantum%2520mechanics%2520to%2520modern%2520generative%2520machine%2520learning.%2520While%2520Pad%25C3%25A9%2520approximants%2520combined%2520with%2520scaling%2520and%2520squaring%2520have%2520long%2520served%2520as%2520the%2520standard%252C%2520recent%2520Taylor-based%2520methods%252C%2520which%2520utilize%2520polynomial%2520evaluation%2520schemes%2520that%2520surpass%2520the%2520classical%2520Paterson--Stockmeyer%2520technique%252C%2520offer%2520superior%2520accuracy%2520and%2520reduced%2520computational%2520complexity.%2520This%2520paper%2520presents%2520an%2520optimized%2520Taylor-based%2520algorithm%2520for%2520the%2520matrix%2520exponential%252C%2520specifically%2520designed%2520for%2520the%2520high-throughput%2520requirements%2520of%2520generative%2520AI%2520flows.%2520We%2520provide%2520a%2520rigorous%2520error%2520analysis%2520and%2520develop%2520a%2520dynamic%2520selection%2520strategy%2520for%2520the%2520Taylor%2520order%2520and%2520scaling%2520factor%2520to%2520minimize%2520computational%2520effort%2520under%2520a%2520prescribed%2520error%2520tolerance.%2520Extensive%2520numerical%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520provides%2520significant%2520acceleration%2520and%2520maintains%2520high%2520numerical%2520stability%2520compared%2520to%2520existing%2520state-of-the-art%2520implementations.%2520These%2520results%2520establish%2520the%2520proposed%2520method%2520as%2520a%2520highly%2520efficient%2520tool%2520for%2520large-scale%2520generative%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Matrix%20Exponential%20for%20Generative%20AI%20Flows%3A%20A%20Taylor-Based%20Approach%20Beyond%20Paterson--Stockmeyer&entry.906535625=Jorge%20Sastre%20and%20Daniel%20Faronbi%20and%20Jos%C3%A9%20Miguel%20Alonso%20and%20Peter%20Traver%20and%20Javier%20Ib%C3%A1%C3%B1ez%20and%20Nuria%20Lloret&entry.1292438233=The%20matrix%20exponential%20is%20a%20fundamental%20operator%20in%20scientific%20computing%20and%20system%20simulation%2C%20with%20applications%20ranging%20from%20control%20theory%20and%20quantum%20mechanics%20to%20modern%20generative%20machine%20learning.%20While%20Pad%C3%A9%20approximants%20combined%20with%20scaling%20and%20squaring%20have%20long%20served%20as%20the%20standard%2C%20recent%20Taylor-based%20methods%2C%20which%20utilize%20polynomial%20evaluation%20schemes%20that%20surpass%20the%20classical%20Paterson--Stockmeyer%20technique%2C%20offer%20superior%20accuracy%20and%20reduced%20computational%20complexity.%20This%20paper%20presents%20an%20optimized%20Taylor-based%20algorithm%20for%20the%20matrix%20exponential%2C%20specifically%20designed%20for%20the%20high-throughput%20requirements%20of%20generative%20AI%20flows.%20We%20provide%20a%20rigorous%20error%20analysis%20and%20develop%20a%20dynamic%20selection%20strategy%20for%20the%20Taylor%20order%20and%20scaling%20factor%20to%20minimize%20computational%20effort%20under%20a%20prescribed%20error%20tolerance.%20Extensive%20numerical%20experiments%20demonstrate%20that%20our%20approach%20provides%20significant%20acceleration%20and%20maintains%20high%20numerical%20stability%20compared%20to%20existing%20state-of-the-art%20implementations.%20These%20results%20establish%20the%20proposed%20method%20as%20a%20highly%20efficient%20tool%20for%20large-scale%20generative%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.20777v2&entry.124074799=Read"},
{"title": "Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models", "author": "Zenghao Duan and Liang Pang and Zihao Wei and Wenbin Duan and Yuxin Tian and Shicheng Xu and Jingcheng Deng and Zhiyi Yin and Xueqi Cheng", "abstract": "Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.", "link": "http://arxiv.org/abs/2601.05693v1", "date": "2026-01-09", "relevancy": 1.8441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4677}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Circular%20Reasoning%3A%20Understanding%20Self-Reinforcing%20Loops%20in%20Large%20Reasoning%20Models&body=Title%3A%20Circular%20Reasoning%3A%20Understanding%20Self-Reinforcing%20Loops%20in%20Large%20Reasoning%20Models%0AAuthor%3A%20Zenghao%20Duan%20and%20Liang%20Pang%20and%20Zihao%20Wei%20and%20Wenbin%20Duan%20and%20Yuxin%20Tian%20and%20Shicheng%20Xu%20and%20Jingcheng%20Deng%20and%20Zhiyi%20Yin%20and%20Xueqi%20Cheng%0AAbstract%3A%20Despite%20the%20success%20of%20test-time%20scaling%2C%20Large%20Reasoning%20Models%20%28LRMs%29%20frequently%20encounter%20repetitive%20loops%20that%20lead%20to%20computational%20waste%20and%20inference%20failure.%20In%20this%20paper%2C%20we%20identify%20a%20distinct%20failure%20mode%20termed%20Circular%20Reasoning.%20Unlike%20traditional%20model%20degeneration%2C%20this%20phenomenon%20manifests%20as%20a%20self-reinforcing%20trap%20where%20generated%20content%20acts%20as%20a%20logical%20premise%20for%20its%20own%20recurrence%2C%20compelling%20the%20reiteration%20of%20preceding%20text.%20To%20systematically%20analyze%20this%20phenomenon%2C%20we%20introduce%20LoopBench%2C%20a%20dataset%20designed%20to%20capture%20two%20distinct%20loop%20typologies%3A%20numerical%20loops%20and%20statement%20loops.%20Mechanistically%2C%20we%20characterize%20circular%20reasoning%20as%20a%20state%20collapse%20exhibiting%20distinct%20boundaries%2C%20where%20semantic%20repetition%20precedes%20textual%20repetition.%20We%20reveal%20that%20reasoning%20impasses%20trigger%20the%20loop%20onset%2C%20which%20subsequently%20persists%20as%20an%20inescapable%20cycle%20driven%20by%20a%20self-reinforcing%20V-shaped%20attention%20mechanism.%20Guided%20by%20these%20findings%2C%20we%20employ%20the%20Cumulative%20Sum%20%28CUSUM%29%20algorithm%20to%20capture%20these%20precursors%20for%20early%20loop%20prediction.%20Experiments%20across%20diverse%20LRMs%20validate%20its%20accuracy%20and%20elucidate%20the%20stability%20of%20long-chain%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCircular%2520Reasoning%253A%2520Understanding%2520Self-Reinforcing%2520Loops%2520in%2520Large%2520Reasoning%2520Models%26entry.906535625%3DZenghao%2520Duan%2520and%2520Liang%2520Pang%2520and%2520Zihao%2520Wei%2520and%2520Wenbin%2520Duan%2520and%2520Yuxin%2520Tian%2520and%2520Shicheng%2520Xu%2520and%2520Jingcheng%2520Deng%2520and%2520Zhiyi%2520Yin%2520and%2520Xueqi%2520Cheng%26entry.1292438233%3DDespite%2520the%2520success%2520of%2520test-time%2520scaling%252C%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520frequently%2520encounter%2520repetitive%2520loops%2520that%2520lead%2520to%2520computational%2520waste%2520and%2520inference%2520failure.%2520In%2520this%2520paper%252C%2520we%2520identify%2520a%2520distinct%2520failure%2520mode%2520termed%2520Circular%2520Reasoning.%2520Unlike%2520traditional%2520model%2520degeneration%252C%2520this%2520phenomenon%2520manifests%2520as%2520a%2520self-reinforcing%2520trap%2520where%2520generated%2520content%2520acts%2520as%2520a%2520logical%2520premise%2520for%2520its%2520own%2520recurrence%252C%2520compelling%2520the%2520reiteration%2520of%2520preceding%2520text.%2520To%2520systematically%2520analyze%2520this%2520phenomenon%252C%2520we%2520introduce%2520LoopBench%252C%2520a%2520dataset%2520designed%2520to%2520capture%2520two%2520distinct%2520loop%2520typologies%253A%2520numerical%2520loops%2520and%2520statement%2520loops.%2520Mechanistically%252C%2520we%2520characterize%2520circular%2520reasoning%2520as%2520a%2520state%2520collapse%2520exhibiting%2520distinct%2520boundaries%252C%2520where%2520semantic%2520repetition%2520precedes%2520textual%2520repetition.%2520We%2520reveal%2520that%2520reasoning%2520impasses%2520trigger%2520the%2520loop%2520onset%252C%2520which%2520subsequently%2520persists%2520as%2520an%2520inescapable%2520cycle%2520driven%2520by%2520a%2520self-reinforcing%2520V-shaped%2520attention%2520mechanism.%2520Guided%2520by%2520these%2520findings%252C%2520we%2520employ%2520the%2520Cumulative%2520Sum%2520%2528CUSUM%2529%2520algorithm%2520to%2520capture%2520these%2520precursors%2520for%2520early%2520loop%2520prediction.%2520Experiments%2520across%2520diverse%2520LRMs%2520validate%2520its%2520accuracy%2520and%2520elucidate%2520the%2520stability%2520of%2520long-chain%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Circular%20Reasoning%3A%20Understanding%20Self-Reinforcing%20Loops%20in%20Large%20Reasoning%20Models&entry.906535625=Zenghao%20Duan%20and%20Liang%20Pang%20and%20Zihao%20Wei%20and%20Wenbin%20Duan%20and%20Yuxin%20Tian%20and%20Shicheng%20Xu%20and%20Jingcheng%20Deng%20and%20Zhiyi%20Yin%20and%20Xueqi%20Cheng&entry.1292438233=Despite%20the%20success%20of%20test-time%20scaling%2C%20Large%20Reasoning%20Models%20%28LRMs%29%20frequently%20encounter%20repetitive%20loops%20that%20lead%20to%20computational%20waste%20and%20inference%20failure.%20In%20this%20paper%2C%20we%20identify%20a%20distinct%20failure%20mode%20termed%20Circular%20Reasoning.%20Unlike%20traditional%20model%20degeneration%2C%20this%20phenomenon%20manifests%20as%20a%20self-reinforcing%20trap%20where%20generated%20content%20acts%20as%20a%20logical%20premise%20for%20its%20own%20recurrence%2C%20compelling%20the%20reiteration%20of%20preceding%20text.%20To%20systematically%20analyze%20this%20phenomenon%2C%20we%20introduce%20LoopBench%2C%20a%20dataset%20designed%20to%20capture%20two%20distinct%20loop%20typologies%3A%20numerical%20loops%20and%20statement%20loops.%20Mechanistically%2C%20we%20characterize%20circular%20reasoning%20as%20a%20state%20collapse%20exhibiting%20distinct%20boundaries%2C%20where%20semantic%20repetition%20precedes%20textual%20repetition.%20We%20reveal%20that%20reasoning%20impasses%20trigger%20the%20loop%20onset%2C%20which%20subsequently%20persists%20as%20an%20inescapable%20cycle%20driven%20by%20a%20self-reinforcing%20V-shaped%20attention%20mechanism.%20Guided%20by%20these%20findings%2C%20we%20employ%20the%20Cumulative%20Sum%20%28CUSUM%29%20algorithm%20to%20capture%20these%20precursors%20for%20early%20loop%20prediction.%20Experiments%20across%20diverse%20LRMs%20validate%20its%20accuracy%20and%20elucidate%20the%20stability%20of%20long-chain%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2601.05693v1&entry.124074799=Read"},
{"title": "Monadic Context Engineering", "author": "Yifan Zhang and Yang Yuan and Mengdi Wang and Andrew Chi-Chih Yao", "abstract": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming.", "link": "http://arxiv.org/abs/2512.22431v3", "date": "2026-01-09", "relevancy": 1.4925, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5316}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monadic%20Context%20Engineering&body=Title%3A%20Monadic%20Context%20Engineering%0AAuthor%3A%20Yifan%20Zhang%20and%20Yang%20Yuan%20and%20Mengdi%20Wang%20and%20Andrew%20Chi-Chih%20Yao%0AAbstract%3A%20The%20proliferation%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20catalyzed%20a%20shift%20towards%20autonomous%20agents%20capable%20of%20complex%20reasoning%20and%20tool%20use.%20However%2C%20current%20agent%20architectures%20are%20frequently%20constructed%20using%20imperative%2C%20ad%20hoc%20patterns.%20This%20results%20in%20brittle%20systems%20plagued%20by%20difficulties%20in%20state%20management%2C%20error%20handling%2C%20and%20concurrency.%20This%20paper%20introduces%20Monadic%20Context%20Engineering%20%28MCE%29%2C%20a%20novel%20architectural%20paradigm%20leveraging%20the%20algebraic%20structures%20of%20Functors%2C%20Applicative%20Functors%2C%20and%20Monads%20to%20provide%20a%20formal%20foundation%20for%20agent%20design.%20MCE%20treats%20agent%20workflows%20as%20computational%20contexts%20where%20cross-cutting%20concerns%2C%20such%20as%20state%20propagation%2C%20short-circuiting%20error%20handling%2C%20and%20asynchronous%20execution%2C%20are%20managed%20intrinsically%20by%20the%20algebraic%20properties%20of%20the%20abstraction.%20We%20demonstrate%20how%20Monads%20enable%20robust%20sequential%20composition%2C%20how%20Applicatives%20provide%20a%20principled%20structure%20for%20parallel%20execution%2C%20and%20crucially%2C%20how%20Monad%20Transformers%20allow%20for%20the%20systematic%20composition%20of%20these%20capabilities.%20This%20layered%20approach%20enables%20developers%20to%20construct%20complex%2C%20resilient%2C%20and%20efficient%20AI%20agents%20from%20simple%2C%20independently%20verifiable%20components.%20We%20further%20extend%20this%20framework%20to%20describe%20Meta-Agents%2C%20which%20leverage%20MCE%20for%20generative%20orchestration%2C%20dynamically%20creating%20and%20managing%20sub-agent%20workflows%20through%20metaprogramming.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22431v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonadic%2520Context%2520Engineering%26entry.906535625%3DYifan%2520Zhang%2520and%2520Yang%2520Yuan%2520and%2520Mengdi%2520Wang%2520and%2520Andrew%2520Chi-Chih%2520Yao%26entry.1292438233%3DThe%2520proliferation%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520catalyzed%2520a%2520shift%2520towards%2520autonomous%2520agents%2520capable%2520of%2520complex%2520reasoning%2520and%2520tool%2520use.%2520However%252C%2520current%2520agent%2520architectures%2520are%2520frequently%2520constructed%2520using%2520imperative%252C%2520ad%2520hoc%2520patterns.%2520This%2520results%2520in%2520brittle%2520systems%2520plagued%2520by%2520difficulties%2520in%2520state%2520management%252C%2520error%2520handling%252C%2520and%2520concurrency.%2520This%2520paper%2520introduces%2520Monadic%2520Context%2520Engineering%2520%2528MCE%2529%252C%2520a%2520novel%2520architectural%2520paradigm%2520leveraging%2520the%2520algebraic%2520structures%2520of%2520Functors%252C%2520Applicative%2520Functors%252C%2520and%2520Monads%2520to%2520provide%2520a%2520formal%2520foundation%2520for%2520agent%2520design.%2520MCE%2520treats%2520agent%2520workflows%2520as%2520computational%2520contexts%2520where%2520cross-cutting%2520concerns%252C%2520such%2520as%2520state%2520propagation%252C%2520short-circuiting%2520error%2520handling%252C%2520and%2520asynchronous%2520execution%252C%2520are%2520managed%2520intrinsically%2520by%2520the%2520algebraic%2520properties%2520of%2520the%2520abstraction.%2520We%2520demonstrate%2520how%2520Monads%2520enable%2520robust%2520sequential%2520composition%252C%2520how%2520Applicatives%2520provide%2520a%2520principled%2520structure%2520for%2520parallel%2520execution%252C%2520and%2520crucially%252C%2520how%2520Monad%2520Transformers%2520allow%2520for%2520the%2520systematic%2520composition%2520of%2520these%2520capabilities.%2520This%2520layered%2520approach%2520enables%2520developers%2520to%2520construct%2520complex%252C%2520resilient%252C%2520and%2520efficient%2520AI%2520agents%2520from%2520simple%252C%2520independently%2520verifiable%2520components.%2520We%2520further%2520extend%2520this%2520framework%2520to%2520describe%2520Meta-Agents%252C%2520which%2520leverage%2520MCE%2520for%2520generative%2520orchestration%252C%2520dynamically%2520creating%2520and%2520managing%2520sub-agent%2520workflows%2520through%2520metaprogramming.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22431v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monadic%20Context%20Engineering&entry.906535625=Yifan%20Zhang%20and%20Yang%20Yuan%20and%20Mengdi%20Wang%20and%20Andrew%20Chi-Chih%20Yao&entry.1292438233=The%20proliferation%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20catalyzed%20a%20shift%20towards%20autonomous%20agents%20capable%20of%20complex%20reasoning%20and%20tool%20use.%20However%2C%20current%20agent%20architectures%20are%20frequently%20constructed%20using%20imperative%2C%20ad%20hoc%20patterns.%20This%20results%20in%20brittle%20systems%20plagued%20by%20difficulties%20in%20state%20management%2C%20error%20handling%2C%20and%20concurrency.%20This%20paper%20introduces%20Monadic%20Context%20Engineering%20%28MCE%29%2C%20a%20novel%20architectural%20paradigm%20leveraging%20the%20algebraic%20structures%20of%20Functors%2C%20Applicative%20Functors%2C%20and%20Monads%20to%20provide%20a%20formal%20foundation%20for%20agent%20design.%20MCE%20treats%20agent%20workflows%20as%20computational%20contexts%20where%20cross-cutting%20concerns%2C%20such%20as%20state%20propagation%2C%20short-circuiting%20error%20handling%2C%20and%20asynchronous%20execution%2C%20are%20managed%20intrinsically%20by%20the%20algebraic%20properties%20of%20the%20abstraction.%20We%20demonstrate%20how%20Monads%20enable%20robust%20sequential%20composition%2C%20how%20Applicatives%20provide%20a%20principled%20structure%20for%20parallel%20execution%2C%20and%20crucially%2C%20how%20Monad%20Transformers%20allow%20for%20the%20systematic%20composition%20of%20these%20capabilities.%20This%20layered%20approach%20enables%20developers%20to%20construct%20complex%2C%20resilient%2C%20and%20efficient%20AI%20agents%20from%20simple%2C%20independently%20verifiable%20components.%20We%20further%20extend%20this%20framework%20to%20describe%20Meta-Agents%2C%20which%20leverage%20MCE%20for%20generative%20orchestration%2C%20dynamically%20creating%20and%20managing%20sub-agent%20workflows%20through%20metaprogramming.&entry.1838667208=http%3A//arxiv.org/abs/2512.22431v3&entry.124074799=Read"},
{"title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?", "author": "Yiyou Sun and Georgia Zhou and Haoyue Bai and Hao Wang and Dacheng Li and Nouha Dziri and Dawn Song", "abstract": "Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.", "link": "http://arxiv.org/abs/2504.11741v2", "date": "2026-01-09", "relevancy": 1.9271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Climbing%20the%20Ladder%20of%20Reasoning%3A%20What%20LLMs%20Can-and%20Still%20Can%27t-Solve%20after%20SFT%3F&body=Title%3A%20Climbing%20the%20Ladder%20of%20Reasoning%3A%20What%20LLMs%20Can-and%20Still%20Can%27t-Solve%20after%20SFT%3F%0AAuthor%3A%20Yiyou%20Sun%20and%20Georgia%20Zhou%20and%20Haoyue%20Bai%20and%20Hao%20Wang%20and%20Dacheng%20Li%20and%20Nouha%20Dziri%20and%20Dawn%20Song%0AAbstract%3A%20Recent%20supervised%20fine-tuning%20%28SFT%29%20approaches%20have%20significantly%20improved%20language%20models%27%20performance%20on%20mathematical%20reasoning%20tasks%2C%20even%20when%20models%20are%20trained%20at%20a%20small%20scale.%20However%2C%20the%20specific%20capabilities%20enhanced%20through%20such%20fine-tuning%20remain%20poorly%20understood.%20In%20this%20paper%2C%20we%20conduct%20a%20detailed%20analysis%20of%20model%20performance%20on%20the%20AIME24%20dataset%20to%20understand%20how%20reasoning%20capabilities%20evolve.%20We%20discover%20a%20ladder-like%20structure%20in%20problem%20difficulty%2C%20categorize%20questions%20into%20four%20tiers%20%28Easy%2C%20Medium%2C%20Hard%2C%20and%20Extremely%20Hard%20%28Exh%29%29%2C%20and%20identify%20the%20specific%20requirements%20for%20advancing%20between%20tiers.%20We%20find%20that%20progression%20from%20Easy%20to%20Medium%20tier%20requires%20adopting%20an%20R1%20reasoning%20style%20with%20minimal%20SFT%20%28500-1K%20instances%29%2C%20while%20Hard-level%20questions%20suffer%20from%20frequent%20model%27s%20errors%20at%20each%20step%20of%20the%20reasoning%20chain%2C%20with%20accuracy%20plateauing%20at%20around%2065%25%20despite%20logarithmic%20scaling.%20Exh-level%20questions%20present%20a%20fundamentally%20different%20challenge%3B%20they%20require%20unconventional%20problem-solving%20skills%20that%20current%20models%20uniformly%20struggle%20with.%20Additional%20findings%20reveal%20that%20carefully%20curated%20small-scale%20datasets%20offer%20limited%20advantage-scaling%20dataset%20size%20proves%20far%20more%20effective.%20Our%20analysis%20provides%20a%20clearer%20roadmap%20for%20advancing%20language%20model%20capabilities%20in%20mathematical%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2504.11741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClimbing%2520the%2520Ladder%2520of%2520Reasoning%253A%2520What%2520LLMs%2520Can-and%2520Still%2520Can%2527t-Solve%2520after%2520SFT%253F%26entry.906535625%3DYiyou%2520Sun%2520and%2520Georgia%2520Zhou%2520and%2520Haoyue%2520Bai%2520and%2520Hao%2520Wang%2520and%2520Dacheng%2520Li%2520and%2520Nouha%2520Dziri%2520and%2520Dawn%2520Song%26entry.1292438233%3DRecent%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520approaches%2520have%2520significantly%2520improved%2520language%2520models%2527%2520performance%2520on%2520mathematical%2520reasoning%2520tasks%252C%2520even%2520when%2520models%2520are%2520trained%2520at%2520a%2520small%2520scale.%2520However%252C%2520the%2520specific%2520capabilities%2520enhanced%2520through%2520such%2520fine-tuning%2520remain%2520poorly%2520understood.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520detailed%2520analysis%2520of%2520model%2520performance%2520on%2520the%2520AIME24%2520dataset%2520to%2520understand%2520how%2520reasoning%2520capabilities%2520evolve.%2520We%2520discover%2520a%2520ladder-like%2520structure%2520in%2520problem%2520difficulty%252C%2520categorize%2520questions%2520into%2520four%2520tiers%2520%2528Easy%252C%2520Medium%252C%2520Hard%252C%2520and%2520Extremely%2520Hard%2520%2528Exh%2529%2529%252C%2520and%2520identify%2520the%2520specific%2520requirements%2520for%2520advancing%2520between%2520tiers.%2520We%2520find%2520that%2520progression%2520from%2520Easy%2520to%2520Medium%2520tier%2520requires%2520adopting%2520an%2520R1%2520reasoning%2520style%2520with%2520minimal%2520SFT%2520%2528500-1K%2520instances%2529%252C%2520while%2520Hard-level%2520questions%2520suffer%2520from%2520frequent%2520model%2527s%2520errors%2520at%2520each%2520step%2520of%2520the%2520reasoning%2520chain%252C%2520with%2520accuracy%2520plateauing%2520at%2520around%252065%2525%2520despite%2520logarithmic%2520scaling.%2520Exh-level%2520questions%2520present%2520a%2520fundamentally%2520different%2520challenge%253B%2520they%2520require%2520unconventional%2520problem-solving%2520skills%2520that%2520current%2520models%2520uniformly%2520struggle%2520with.%2520Additional%2520findings%2520reveal%2520that%2520carefully%2520curated%2520small-scale%2520datasets%2520offer%2520limited%2520advantage-scaling%2520dataset%2520size%2520proves%2520far%2520more%2520effective.%2520Our%2520analysis%2520provides%2520a%2520clearer%2520roadmap%2520for%2520advancing%2520language%2520model%2520capabilities%2520in%2520mathematical%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Climbing%20the%20Ladder%20of%20Reasoning%3A%20What%20LLMs%20Can-and%20Still%20Can%27t-Solve%20after%20SFT%3F&entry.906535625=Yiyou%20Sun%20and%20Georgia%20Zhou%20and%20Haoyue%20Bai%20and%20Hao%20Wang%20and%20Dacheng%20Li%20and%20Nouha%20Dziri%20and%20Dawn%20Song&entry.1292438233=Recent%20supervised%20fine-tuning%20%28SFT%29%20approaches%20have%20significantly%20improved%20language%20models%27%20performance%20on%20mathematical%20reasoning%20tasks%2C%20even%20when%20models%20are%20trained%20at%20a%20small%20scale.%20However%2C%20the%20specific%20capabilities%20enhanced%20through%20such%20fine-tuning%20remain%20poorly%20understood.%20In%20this%20paper%2C%20we%20conduct%20a%20detailed%20analysis%20of%20model%20performance%20on%20the%20AIME24%20dataset%20to%20understand%20how%20reasoning%20capabilities%20evolve.%20We%20discover%20a%20ladder-like%20structure%20in%20problem%20difficulty%2C%20categorize%20questions%20into%20four%20tiers%20%28Easy%2C%20Medium%2C%20Hard%2C%20and%20Extremely%20Hard%20%28Exh%29%29%2C%20and%20identify%20the%20specific%20requirements%20for%20advancing%20between%20tiers.%20We%20find%20that%20progression%20from%20Easy%20to%20Medium%20tier%20requires%20adopting%20an%20R1%20reasoning%20style%20with%20minimal%20SFT%20%28500-1K%20instances%29%2C%20while%20Hard-level%20questions%20suffer%20from%20frequent%20model%27s%20errors%20at%20each%20step%20of%20the%20reasoning%20chain%2C%20with%20accuracy%20plateauing%20at%20around%2065%25%20despite%20logarithmic%20scaling.%20Exh-level%20questions%20present%20a%20fundamentally%20different%20challenge%3B%20they%20require%20unconventional%20problem-solving%20skills%20that%20current%20models%20uniformly%20struggle%20with.%20Additional%20findings%20reveal%20that%20carefully%20curated%20small-scale%20datasets%20offer%20limited%20advantage-scaling%20dataset%20size%20proves%20far%20more%20effective.%20Our%20analysis%20provides%20a%20clearer%20roadmap%20for%20advancing%20language%20model%20capabilities%20in%20mathematical%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2504.11741v2&entry.124074799=Read"},
{"title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management", "author": "Kieran Wood and Stephen J. Roberts and Stefan Zohren", "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.", "link": "http://arxiv.org/abs/2601.05975v1", "date": "2026-01-09", "relevancy": 1.4304, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5036}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4466}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeePM%3A%20Regime-Robust%20Deep%20Learning%20for%20Systematic%20Macro%20Portfolio%20Management&body=Title%3A%20DeePM%3A%20Regime-Robust%20Deep%20Learning%20for%20Systematic%20Macro%20Portfolio%20Management%0AAuthor%3A%20Kieran%20Wood%20and%20Stephen%20J.%20Roberts%20and%20Stefan%20Zohren%0AAbstract%3A%20We%20propose%20DeePM%20%28Deep%20Portfolio%20Manager%29%2C%20a%20structured%20deep-learning%20macro%20portfolio%20manager%20trained%20end-to-end%20to%20maximize%20a%20robust%2C%20risk-adjusted%20utility.%20DeePM%20addresses%20three%20fundamental%20challenges%20in%20financial%20learning%3A%20%281%29%20it%20resolves%20the%20asynchronous%20%22ragged%20filtration%22%20problem%20via%20a%20Directed%20Delay%20%28Causal%20Sieve%29%20mechanism%20that%20prioritizes%20causal%20impulse-response%20learning%20over%20information%20freshness%3B%20%282%29%20it%20combats%20low%20signal-to-noise%20ratios%20via%20a%20Macroeconomic%20Graph%20Prior%2C%20regularizing%20cross-asset%20dependence%20according%20to%20economic%20first%20principles%3B%20and%20%283%29%20it%20optimizes%20a%20distributionally%20robust%20objective%20where%20a%20smooth%20worst-window%20penalty%20serves%20as%20a%20differentiable%20proxy%20for%20Entropic%20Value-at-Risk%20%28EVaR%29%20-%20a%20window-robust%20utility%20encouraging%20strong%20performance%20in%20the%20most%20adverse%20historical%20subperiods.%20In%20large-scale%20backtests%20from%202010-2025%20on%2050%20diversified%20futures%20with%20highly%20realistic%20transaction%20costs%2C%20DeePM%20attains%20net%20risk-adjusted%20returns%20that%20are%20roughly%20twice%20those%20of%20classical%20trend-following%20strategies%20and%20passive%20benchmarks%2C%20solely%20using%20daily%20closing%20prices.%20Furthermore%2C%20DeePM%20improves%20upon%20the%20state-of-the-art%20Momentum%20Transformer%20architecture%20by%20roughly%20fifty%20percent.%20The%20model%20demonstrates%20structural%20resilience%20across%20the%202010s%20%22CTA%20%28Commodity%20Trading%20Advisor%29%20Winter%22%20and%20the%20post-2020%20volatility%20regime%20shift%2C%20maintaining%20consistent%20performance%20through%20the%20pandemic%2C%20inflation%20shocks%2C%20and%20the%20subsequent%20higher-for-longer%20environment.%20Ablation%20studies%20confirm%20that%20strictly%20lagged%20cross-sectional%20attention%2C%20graph%20prior%2C%20principled%20treatment%20of%20transaction%20costs%2C%20and%20robust%20minimax%20optimization%20are%20the%20primary%20drivers%20of%20this%20generalization%20capability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeePM%253A%2520Regime-Robust%2520Deep%2520Learning%2520for%2520Systematic%2520Macro%2520Portfolio%2520Management%26entry.906535625%3DKieran%2520Wood%2520and%2520Stephen%2520J.%2520Roberts%2520and%2520Stefan%2520Zohren%26entry.1292438233%3DWe%2520propose%2520DeePM%2520%2528Deep%2520Portfolio%2520Manager%2529%252C%2520a%2520structured%2520deep-learning%2520macro%2520portfolio%2520manager%2520trained%2520end-to-end%2520to%2520maximize%2520a%2520robust%252C%2520risk-adjusted%2520utility.%2520DeePM%2520addresses%2520three%2520fundamental%2520challenges%2520in%2520financial%2520learning%253A%2520%25281%2529%2520it%2520resolves%2520the%2520asynchronous%2520%2522ragged%2520filtration%2522%2520problem%2520via%2520a%2520Directed%2520Delay%2520%2528Causal%2520Sieve%2529%2520mechanism%2520that%2520prioritizes%2520causal%2520impulse-response%2520learning%2520over%2520information%2520freshness%253B%2520%25282%2529%2520it%2520combats%2520low%2520signal-to-noise%2520ratios%2520via%2520a%2520Macroeconomic%2520Graph%2520Prior%252C%2520regularizing%2520cross-asset%2520dependence%2520according%2520to%2520economic%2520first%2520principles%253B%2520and%2520%25283%2529%2520it%2520optimizes%2520a%2520distributionally%2520robust%2520objective%2520where%2520a%2520smooth%2520worst-window%2520penalty%2520serves%2520as%2520a%2520differentiable%2520proxy%2520for%2520Entropic%2520Value-at-Risk%2520%2528EVaR%2529%2520-%2520a%2520window-robust%2520utility%2520encouraging%2520strong%2520performance%2520in%2520the%2520most%2520adverse%2520historical%2520subperiods.%2520In%2520large-scale%2520backtests%2520from%25202010-2025%2520on%252050%2520diversified%2520futures%2520with%2520highly%2520realistic%2520transaction%2520costs%252C%2520DeePM%2520attains%2520net%2520risk-adjusted%2520returns%2520that%2520are%2520roughly%2520twice%2520those%2520of%2520classical%2520trend-following%2520strategies%2520and%2520passive%2520benchmarks%252C%2520solely%2520using%2520daily%2520closing%2520prices.%2520Furthermore%252C%2520DeePM%2520improves%2520upon%2520the%2520state-of-the-art%2520Momentum%2520Transformer%2520architecture%2520by%2520roughly%2520fifty%2520percent.%2520The%2520model%2520demonstrates%2520structural%2520resilience%2520across%2520the%25202010s%2520%2522CTA%2520%2528Commodity%2520Trading%2520Advisor%2529%2520Winter%2522%2520and%2520the%2520post-2020%2520volatility%2520regime%2520shift%252C%2520maintaining%2520consistent%2520performance%2520through%2520the%2520pandemic%252C%2520inflation%2520shocks%252C%2520and%2520the%2520subsequent%2520higher-for-longer%2520environment.%2520Ablation%2520studies%2520confirm%2520that%2520strictly%2520lagged%2520cross-sectional%2520attention%252C%2520graph%2520prior%252C%2520principled%2520treatment%2520of%2520transaction%2520costs%252C%2520and%2520robust%2520minimax%2520optimization%2520are%2520the%2520primary%2520drivers%2520of%2520this%2520generalization%2520capability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeePM%3A%20Regime-Robust%20Deep%20Learning%20for%20Systematic%20Macro%20Portfolio%20Management&entry.906535625=Kieran%20Wood%20and%20Stephen%20J.%20Roberts%20and%20Stefan%20Zohren&entry.1292438233=We%20propose%20DeePM%20%28Deep%20Portfolio%20Manager%29%2C%20a%20structured%20deep-learning%20macro%20portfolio%20manager%20trained%20end-to-end%20to%20maximize%20a%20robust%2C%20risk-adjusted%20utility.%20DeePM%20addresses%20three%20fundamental%20challenges%20in%20financial%20learning%3A%20%281%29%20it%20resolves%20the%20asynchronous%20%22ragged%20filtration%22%20problem%20via%20a%20Directed%20Delay%20%28Causal%20Sieve%29%20mechanism%20that%20prioritizes%20causal%20impulse-response%20learning%20over%20information%20freshness%3B%20%282%29%20it%20combats%20low%20signal-to-noise%20ratios%20via%20a%20Macroeconomic%20Graph%20Prior%2C%20regularizing%20cross-asset%20dependence%20according%20to%20economic%20first%20principles%3B%20and%20%283%29%20it%20optimizes%20a%20distributionally%20robust%20objective%20where%20a%20smooth%20worst-window%20penalty%20serves%20as%20a%20differentiable%20proxy%20for%20Entropic%20Value-at-Risk%20%28EVaR%29%20-%20a%20window-robust%20utility%20encouraging%20strong%20performance%20in%20the%20most%20adverse%20historical%20subperiods.%20In%20large-scale%20backtests%20from%202010-2025%20on%2050%20diversified%20futures%20with%20highly%20realistic%20transaction%20costs%2C%20DeePM%20attains%20net%20risk-adjusted%20returns%20that%20are%20roughly%20twice%20those%20of%20classical%20trend-following%20strategies%20and%20passive%20benchmarks%2C%20solely%20using%20daily%20closing%20prices.%20Furthermore%2C%20DeePM%20improves%20upon%20the%20state-of-the-art%20Momentum%20Transformer%20architecture%20by%20roughly%20fifty%20percent.%20The%20model%20demonstrates%20structural%20resilience%20across%20the%202010s%20%22CTA%20%28Commodity%20Trading%20Advisor%29%20Winter%22%20and%20the%20post-2020%20volatility%20regime%20shift%2C%20maintaining%20consistent%20performance%20through%20the%20pandemic%2C%20inflation%20shocks%2C%20and%20the%20subsequent%20higher-for-longer%20environment.%20Ablation%20studies%20confirm%20that%20strictly%20lagged%20cross-sectional%20attention%2C%20graph%20prior%2C%20principled%20treatment%20of%20transaction%20costs%2C%20and%20robust%20minimax%20optimization%20are%20the%20primary%20drivers%20of%20this%20generalization%20capability.&entry.1838667208=http%3A//arxiv.org/abs/2601.05975v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


