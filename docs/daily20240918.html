<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240917.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction", "author": "Marko Mihajlovic and Sergey Prokudin and Siyu Tang and Robert Maier and Federica Bogo and Tony Tung and Edmond Boyer", "abstract": "  Digitizing 3D static scenes and 4D dynamic events from multi-view images has\nlong been a challenge in computer vision and graphics. Recently, 3D Gaussian\nSplatting (3DGS) has emerged as a practical and scalable reconstruction method,\ngaining popularity due to its impressive reconstruction quality, real-time\nrendering capabilities, and compatibility with widely used visualization tools.\nHowever, the method requires a substantial number of input views to achieve\nhigh-quality scene reconstruction, introducing a significant practical\nbottleneck. This challenge is especially severe in capturing dynamic scenes,\nwhere deploying an extensive camera array can be prohibitively costly. In this\nwork, we identify the lack of spatial autocorrelation of splat features as one\nof the factors contributing to the suboptimal performance of the 3DGS technique\nin sparse reconstruction settings. To address the issue, we propose an\noptimization strategy that effectively regularizes splat features by modeling\nthem as the outputs of a corresponding implicit neural field. This results in a\nconsistent enhancement of reconstruction quality across various scenarios. Our\napproach effectively handles static and dynamic cases, as demonstrated by\nextensive testing across different setups and scene complexities.\n", "link": "http://arxiv.org/abs/2409.11211v1", "date": "2024-09-17", "relevancy": 3.6623, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7745}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7251}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatFields%3A%20Neural%20Gaussian%20Splats%20for%20Sparse%203D%20and%204D%20Reconstruction&body=Title%3A%20SplatFields%3A%20Neural%20Gaussian%20Splats%20for%20Sparse%203D%20and%204D%20Reconstruction%0AAuthor%3A%20Marko%20Mihajlovic%20and%20Sergey%20Prokudin%20and%20Siyu%20Tang%20and%20Robert%20Maier%20and%20Federica%20Bogo%20and%20Tony%20Tung%20and%20Edmond%20Boyer%0AAbstract%3A%20%20%20Digitizing%203D%20static%20scenes%20and%204D%20dynamic%20events%20from%20multi-view%20images%20has%0Along%20been%20a%20challenge%20in%20computer%20vision%20and%20graphics.%20Recently%2C%203D%20Gaussian%0ASplatting%20%283DGS%29%20has%20emerged%20as%20a%20practical%20and%20scalable%20reconstruction%20method%2C%0Againing%20popularity%20due%20to%20its%20impressive%20reconstruction%20quality%2C%20real-time%0Arendering%20capabilities%2C%20and%20compatibility%20with%20widely%20used%20visualization%20tools.%0AHowever%2C%20the%20method%20requires%20a%20substantial%20number%20of%20input%20views%20to%20achieve%0Ahigh-quality%20scene%20reconstruction%2C%20introducing%20a%20significant%20practical%0Abottleneck.%20This%20challenge%20is%20especially%20severe%20in%20capturing%20dynamic%20scenes%2C%0Awhere%20deploying%20an%20extensive%20camera%20array%20can%20be%20prohibitively%20costly.%20In%20this%0Awork%2C%20we%20identify%20the%20lack%20of%20spatial%20autocorrelation%20of%20splat%20features%20as%20one%0Aof%20the%20factors%20contributing%20to%20the%20suboptimal%20performance%20of%20the%203DGS%20technique%0Ain%20sparse%20reconstruction%20settings.%20To%20address%20the%20issue%2C%20we%20propose%20an%0Aoptimization%20strategy%20that%20effectively%20regularizes%20splat%20features%20by%20modeling%0Athem%20as%20the%20outputs%20of%20a%20corresponding%20implicit%20neural%20field.%20This%20results%20in%20a%0Aconsistent%20enhancement%20of%20reconstruction%20quality%20across%20various%20scenarios.%20Our%0Aapproach%20effectively%20handles%20static%20and%20dynamic%20cases%2C%20as%20demonstrated%20by%0Aextensive%20testing%20across%20different%20setups%20and%20scene%20complexities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatFields%253A%2520Neural%2520Gaussian%2520Splats%2520for%2520Sparse%25203D%2520and%25204D%2520Reconstruction%26entry.906535625%3DMarko%2520Mihajlovic%2520and%2520Sergey%2520Prokudin%2520and%2520Siyu%2520Tang%2520and%2520Robert%2520Maier%2520and%2520Federica%2520Bogo%2520and%2520Tony%2520Tung%2520and%2520Edmond%2520Boyer%26entry.1292438233%3D%2520%2520Digitizing%25203D%2520static%2520scenes%2520and%25204D%2520dynamic%2520events%2520from%2520multi-view%2520images%2520has%250Along%2520been%2520a%2520challenge%2520in%2520computer%2520vision%2520and%2520graphics.%2520Recently%252C%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520practical%2520and%2520scalable%2520reconstruction%2520method%252C%250Againing%2520popularity%2520due%2520to%2520its%2520impressive%2520reconstruction%2520quality%252C%2520real-time%250Arendering%2520capabilities%252C%2520and%2520compatibility%2520with%2520widely%2520used%2520visualization%2520tools.%250AHowever%252C%2520the%2520method%2520requires%2520a%2520substantial%2520number%2520of%2520input%2520views%2520to%2520achieve%250Ahigh-quality%2520scene%2520reconstruction%252C%2520introducing%2520a%2520significant%2520practical%250Abottleneck.%2520This%2520challenge%2520is%2520especially%2520severe%2520in%2520capturing%2520dynamic%2520scenes%252C%250Awhere%2520deploying%2520an%2520extensive%2520camera%2520array%2520can%2520be%2520prohibitively%2520costly.%2520In%2520this%250Awork%252C%2520we%2520identify%2520the%2520lack%2520of%2520spatial%2520autocorrelation%2520of%2520splat%2520features%2520as%2520one%250Aof%2520the%2520factors%2520contributing%2520to%2520the%2520suboptimal%2520performance%2520of%2520the%25203DGS%2520technique%250Ain%2520sparse%2520reconstruction%2520settings.%2520To%2520address%2520the%2520issue%252C%2520we%2520propose%2520an%250Aoptimization%2520strategy%2520that%2520effectively%2520regularizes%2520splat%2520features%2520by%2520modeling%250Athem%2520as%2520the%2520outputs%2520of%2520a%2520corresponding%2520implicit%2520neural%2520field.%2520This%2520results%2520in%2520a%250Aconsistent%2520enhancement%2520of%2520reconstruction%2520quality%2520across%2520various%2520scenarios.%2520Our%250Aapproach%2520effectively%2520handles%2520static%2520and%2520dynamic%2520cases%252C%2520as%2520demonstrated%2520by%250Aextensive%2520testing%2520across%2520different%2520setups%2520and%2520scene%2520complexities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatFields%3A%20Neural%20Gaussian%20Splats%20for%20Sparse%203D%20and%204D%20Reconstruction&entry.906535625=Marko%20Mihajlovic%20and%20Sergey%20Prokudin%20and%20Siyu%20Tang%20and%20Robert%20Maier%20and%20Federica%20Bogo%20and%20Tony%20Tung%20and%20Edmond%20Boyer&entry.1292438233=%20%20Digitizing%203D%20static%20scenes%20and%204D%20dynamic%20events%20from%20multi-view%20images%20has%0Along%20been%20a%20challenge%20in%20computer%20vision%20and%20graphics.%20Recently%2C%203D%20Gaussian%0ASplatting%20%283DGS%29%20has%20emerged%20as%20a%20practical%20and%20scalable%20reconstruction%20method%2C%0Againing%20popularity%20due%20to%20its%20impressive%20reconstruction%20quality%2C%20real-time%0Arendering%20capabilities%2C%20and%20compatibility%20with%20widely%20used%20visualization%20tools.%0AHowever%2C%20the%20method%20requires%20a%20substantial%20number%20of%20input%20views%20to%20achieve%0Ahigh-quality%20scene%20reconstruction%2C%20introducing%20a%20significant%20practical%0Abottleneck.%20This%20challenge%20is%20especially%20severe%20in%20capturing%20dynamic%20scenes%2C%0Awhere%20deploying%20an%20extensive%20camera%20array%20can%20be%20prohibitively%20costly.%20In%20this%0Awork%2C%20we%20identify%20the%20lack%20of%20spatial%20autocorrelation%20of%20splat%20features%20as%20one%0Aof%20the%20factors%20contributing%20to%20the%20suboptimal%20performance%20of%20the%203DGS%20technique%0Ain%20sparse%20reconstruction%20settings.%20To%20address%20the%20issue%2C%20we%20propose%20an%0Aoptimization%20strategy%20that%20effectively%20regularizes%20splat%20features%20by%20modeling%0Athem%20as%20the%20outputs%20of%20a%20corresponding%20implicit%20neural%20field.%20This%20results%20in%20a%0Aconsistent%20enhancement%20of%20reconstruction%20quality%20across%20various%20scenarios.%20Our%0Aapproach%20effectively%20handles%20static%20and%20dynamic%20cases%2C%20as%20demonstrated%20by%0Aextensive%20testing%20across%20different%20setups%20and%20scene%20complexities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11211v1&entry.124074799=Read"},
{"title": "GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module", "author": "Yichen Zhang and Zihan Wang and Jiali Han and Peilin Li and Jiaxun Zhang and Jianqiang Wang and Lei He and Keqiang Li", "abstract": "  3D Gaussian Splatting (3DGS) integrates the strengths of primitive-based\nrepresentations and volumetric rendering techniques, enabling real-time,\nhigh-quality rendering. However, 3DGS models typically overfit to single-scene\ntraining and are highly sensitive to the initialization of Gaussian ellipsoids,\nheuristically derived from Structure from Motion (SfM) point clouds, which\nlimits both generalization and practicality. To address these limitations, we\npropose GS-Net, a generalizable, plug-and-play 3DGS module that densifies\nGaussian ellipsoids from sparse SfM point clouds, enhancing geometric structure\nrepresentation. To the best of our knowledge, GS-Net is the first plug-and-play\n3DGS module with cross-scene generalization capabilities. Additionally, we\nintroduce the CARLA-NVS dataset, which incorporates additional camera\nviewpoints to thoroughly evaluate reconstruction and rendering quality.\nExtensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNR\nimprovement of 2.08 dB for conventional viewpoints and 1.86 dB for novel\nviewpoints, confirming the method's effectiveness and robustness.\n", "link": "http://arxiv.org/abs/2409.11307v1", "date": "2024-09-17", "relevancy": 3.3739, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7029}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6837}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Net%3A%20Generalizable%20Plug-and-Play%203D%20Gaussian%20Splatting%20Module&body=Title%3A%20GS-Net%3A%20Generalizable%20Plug-and-Play%203D%20Gaussian%20Splatting%20Module%0AAuthor%3A%20Yichen%20Zhang%20and%20Zihan%20Wang%20and%20Jiali%20Han%20and%20Peilin%20Li%20and%20Jiaxun%20Zhang%20and%20Jianqiang%20Wang%20and%20Lei%20He%20and%20Keqiang%20Li%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20integrates%20the%20strengths%20of%20primitive-based%0Arepresentations%20and%20volumetric%20rendering%20techniques%2C%20enabling%20real-time%2C%0Ahigh-quality%20rendering.%20However%2C%203DGS%20models%20typically%20overfit%20to%20single-scene%0Atraining%20and%20are%20highly%20sensitive%20to%20the%20initialization%20of%20Gaussian%20ellipsoids%2C%0Aheuristically%20derived%20from%20Structure%20from%20Motion%20%28SfM%29%20point%20clouds%2C%20which%0Alimits%20both%20generalization%20and%20practicality.%20To%20address%20these%20limitations%2C%20we%0Apropose%20GS-Net%2C%20a%20generalizable%2C%20plug-and-play%203DGS%20module%20that%20densifies%0AGaussian%20ellipsoids%20from%20sparse%20SfM%20point%20clouds%2C%20enhancing%20geometric%20structure%0Arepresentation.%20To%20the%20best%20of%20our%20knowledge%2C%20GS-Net%20is%20the%20first%20plug-and-play%0A3DGS%20module%20with%20cross-scene%20generalization%20capabilities.%20Additionally%2C%20we%0Aintroduce%20the%20CARLA-NVS%20dataset%2C%20which%20incorporates%20additional%20camera%0Aviewpoints%20to%20thoroughly%20evaluate%20reconstruction%20and%20rendering%20quality.%0AExtensive%20experiments%20demonstrate%20that%20applying%20GS-Net%20to%203DGS%20yields%20a%20PSNR%0Aimprovement%20of%202.08%20dB%20for%20conventional%20viewpoints%20and%201.86%20dB%20for%20novel%0Aviewpoints%2C%20confirming%20the%20method%27s%20effectiveness%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Net%253A%2520Generalizable%2520Plug-and-Play%25203D%2520Gaussian%2520Splatting%2520Module%26entry.906535625%3DYichen%2520Zhang%2520and%2520Zihan%2520Wang%2520and%2520Jiali%2520Han%2520and%2520Peilin%2520Li%2520and%2520Jiaxun%2520Zhang%2520and%2520Jianqiang%2520Wang%2520and%2520Lei%2520He%2520and%2520Keqiang%2520Li%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520integrates%2520the%2520strengths%2520of%2520primitive-based%250Arepresentations%2520and%2520volumetric%2520rendering%2520techniques%252C%2520enabling%2520real-time%252C%250Ahigh-quality%2520rendering.%2520However%252C%25203DGS%2520models%2520typically%2520overfit%2520to%2520single-scene%250Atraining%2520and%2520are%2520highly%2520sensitive%2520to%2520the%2520initialization%2520of%2520Gaussian%2520ellipsoids%252C%250Aheuristically%2520derived%2520from%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520point%2520clouds%252C%2520which%250Alimits%2520both%2520generalization%2520and%2520practicality.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520GS-Net%252C%2520a%2520generalizable%252C%2520plug-and-play%25203DGS%2520module%2520that%2520densifies%250AGaussian%2520ellipsoids%2520from%2520sparse%2520SfM%2520point%2520clouds%252C%2520enhancing%2520geometric%2520structure%250Arepresentation.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520GS-Net%2520is%2520the%2520first%2520plug-and-play%250A3DGS%2520module%2520with%2520cross-scene%2520generalization%2520capabilities.%2520Additionally%252C%2520we%250Aintroduce%2520the%2520CARLA-NVS%2520dataset%252C%2520which%2520incorporates%2520additional%2520camera%250Aviewpoints%2520to%2520thoroughly%2520evaluate%2520reconstruction%2520and%2520rendering%2520quality.%250AExtensive%2520experiments%2520demonstrate%2520that%2520applying%2520GS-Net%2520to%25203DGS%2520yields%2520a%2520PSNR%250Aimprovement%2520of%25202.08%2520dB%2520for%2520conventional%2520viewpoints%2520and%25201.86%2520dB%2520for%2520novel%250Aviewpoints%252C%2520confirming%2520the%2520method%2527s%2520effectiveness%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Net%3A%20Generalizable%20Plug-and-Play%203D%20Gaussian%20Splatting%20Module&entry.906535625=Yichen%20Zhang%20and%20Zihan%20Wang%20and%20Jiali%20Han%20and%20Peilin%20Li%20and%20Jiaxun%20Zhang%20and%20Jianqiang%20Wang%20and%20Lei%20He%20and%20Keqiang%20Li&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20integrates%20the%20strengths%20of%20primitive-based%0Arepresentations%20and%20volumetric%20rendering%20techniques%2C%20enabling%20real-time%2C%0Ahigh-quality%20rendering.%20However%2C%203DGS%20models%20typically%20overfit%20to%20single-scene%0Atraining%20and%20are%20highly%20sensitive%20to%20the%20initialization%20of%20Gaussian%20ellipsoids%2C%0Aheuristically%20derived%20from%20Structure%20from%20Motion%20%28SfM%29%20point%20clouds%2C%20which%0Alimits%20both%20generalization%20and%20practicality.%20To%20address%20these%20limitations%2C%20we%0Apropose%20GS-Net%2C%20a%20generalizable%2C%20plug-and-play%203DGS%20module%20that%20densifies%0AGaussian%20ellipsoids%20from%20sparse%20SfM%20point%20clouds%2C%20enhancing%20geometric%20structure%0Arepresentation.%20To%20the%20best%20of%20our%20knowledge%2C%20GS-Net%20is%20the%20first%20plug-and-play%0A3DGS%20module%20with%20cross-scene%20generalization%20capabilities.%20Additionally%2C%20we%0Aintroduce%20the%20CARLA-NVS%20dataset%2C%20which%20incorporates%20additional%20camera%0Aviewpoints%20to%20thoroughly%20evaluate%20reconstruction%20and%20rendering%20quality.%0AExtensive%20experiments%20demonstrate%20that%20applying%20GS-Net%20to%203DGS%20yields%20a%20PSNR%0Aimprovement%20of%202.08%20dB%20for%20conventional%20viewpoints%20and%201.86%20dB%20for%20novel%0Aviewpoints%2C%20confirming%20the%20method%27s%20effectiveness%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11307v1&entry.124074799=Read"},
{"title": "fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D\n  Reconstruction", "author": "Jianxiong Gao and Yuqian Fu and Yun Wang and Xuelin Qian and Jianfeng Feng and Yanwei Fu", "abstract": "  Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI)\ndata, introduced as Recon3DMind in our conference work, is of significant\ninterest to both cognitive neuroscience and computer vision. To advance this\ntask, we present the fMRI-3D dataset, which includes data from 15 participants\nand showcases a total of 4768 3D objects. The dataset comprises two components:\nfMRI-Shape, previously introduced and accessible at\nhttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse,\nproposed in this paper and available at\nhttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse\nincludes data from 5 subjects, 4 of whom are also part of the Core set in\nfMRI-Shape, with each subject viewing 3142 3D objects across 117 categories,\nall accompanied by text captions. This significantly enhances the diversity and\npotential applications of the dataset. Additionally, we propose MinD-3D, a\nnovel framework designed to decode 3D visual information from fMRI signals. The\nframework first extracts and aggregates features from fMRI data using a\nneuro-fusion encoder, then employs a feature-bridge diffusion model to generate\nvisual features, and finally reconstructs the 3D object using a generative\ntransformer decoder. We establish new benchmarks by designing metrics at both\nsemantic and structural levels to evaluate model performance. Furthermore, we\nassess our model's effectiveness in an Out-of-Distribution setting and analyze\nthe attribution of the extracted features and the visual ROIs in fMRI signals.\nOur experiments demonstrate that MinD-3D not only reconstructs 3D objects with\nhigh semantic and spatial accuracy but also deepens our understanding of how\nhuman brain processes 3D visual information. Project page at:\nhttps://jianxgao.github.io/MinD-3D.\n", "link": "http://arxiv.org/abs/2409.11315v1", "date": "2024-09-17", "relevancy": 3.1335, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6464}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20fMRI-3D%3A%20A%20Comprehensive%20Dataset%20for%20Enhancing%20fMRI-based%203D%0A%20%20Reconstruction&body=Title%3A%20fMRI-3D%3A%20A%20Comprehensive%20Dataset%20for%20Enhancing%20fMRI-based%203D%0A%20%20Reconstruction%0AAuthor%3A%20Jianxiong%20Gao%20and%20Yuqian%20Fu%20and%20Yun%20Wang%20and%20Xuelin%20Qian%20and%20Jianfeng%20Feng%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Reconstructing%203D%20visuals%20from%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Adata%2C%20introduced%20as%20Recon3DMind%20in%20our%20conference%20work%2C%20is%20of%20significant%0Ainterest%20to%20both%20cognitive%20neuroscience%20and%20computer%20vision.%20To%20advance%20this%0Atask%2C%20we%20present%20the%20fMRI-3D%20dataset%2C%20which%20includes%20data%20from%2015%20participants%0Aand%20showcases%20a%20total%20of%204768%203D%20objects.%20The%20dataset%20comprises%20two%20components%3A%0AfMRI-Shape%2C%20previously%20introduced%20and%20accessible%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Shape%2C%20and%20fMRI-Objaverse%2C%0Aproposed%20in%20this%20paper%20and%20available%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse.%20fMRI-Objaverse%0Aincludes%20data%20from%205%20subjects%2C%204%20of%20whom%20are%20also%20part%20of%20the%20Core%20set%20in%0AfMRI-Shape%2C%20with%20each%20subject%20viewing%203142%203D%20objects%20across%20117%20categories%2C%0Aall%20accompanied%20by%20text%20captions.%20This%20significantly%20enhances%20the%20diversity%20and%0Apotential%20applications%20of%20the%20dataset.%20Additionally%2C%20we%20propose%20MinD-3D%2C%20a%0Anovel%20framework%20designed%20to%20decode%203D%20visual%20information%20from%20fMRI%20signals.%20The%0Aframework%20first%20extracts%20and%20aggregates%20features%20from%20fMRI%20data%20using%20a%0Aneuro-fusion%20encoder%2C%20then%20employs%20a%20feature-bridge%20diffusion%20model%20to%20generate%0Avisual%20features%2C%20and%20finally%20reconstructs%20the%203D%20object%20using%20a%20generative%0Atransformer%20decoder.%20We%20establish%20new%20benchmarks%20by%20designing%20metrics%20at%20both%0Asemantic%20and%20structural%20levels%20to%20evaluate%20model%20performance.%20Furthermore%2C%20we%0Aassess%20our%20model%27s%20effectiveness%20in%20an%20Out-of-Distribution%20setting%20and%20analyze%0Athe%20attribution%20of%20the%20extracted%20features%20and%20the%20visual%20ROIs%20in%20fMRI%20signals.%0AOur%20experiments%20demonstrate%20that%20MinD-3D%20not%20only%20reconstructs%203D%20objects%20with%0Ahigh%20semantic%20and%20spatial%20accuracy%20but%20also%20deepens%20our%20understanding%20of%20how%0Ahuman%20brain%20processes%203D%20visual%20information.%20Project%20page%20at%3A%0Ahttps%3A//jianxgao.github.io/MinD-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DfMRI-3D%253A%2520A%2520Comprehensive%2520Dataset%2520for%2520Enhancing%2520fMRI-based%25203D%250A%2520%2520Reconstruction%26entry.906535625%3DJianxiong%2520Gao%2520and%2520Yuqian%2520Fu%2520and%2520Yun%2520Wang%2520and%2520Xuelin%2520Qian%2520and%2520Jianfeng%2520Feng%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520visuals%2520from%2520functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%250Adata%252C%2520introduced%2520as%2520Recon3DMind%2520in%2520our%2520conference%2520work%252C%2520is%2520of%2520significant%250Ainterest%2520to%2520both%2520cognitive%2520neuroscience%2520and%2520computer%2520vision.%2520To%2520advance%2520this%250Atask%252C%2520we%2520present%2520the%2520fMRI-3D%2520dataset%252C%2520which%2520includes%2520data%2520from%252015%2520participants%250Aand%2520showcases%2520a%2520total%2520of%25204768%25203D%2520objects.%2520The%2520dataset%2520comprises%2520two%2520components%253A%250AfMRI-Shape%252C%2520previously%2520introduced%2520and%2520accessible%2520at%250Ahttps%253A//huggingface.co/datasets/Fudan-fMRI/fMRI-Shape%252C%2520and%2520fMRI-Objaverse%252C%250Aproposed%2520in%2520this%2520paper%2520and%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse.%2520fMRI-Objaverse%250Aincludes%2520data%2520from%25205%2520subjects%252C%25204%2520of%2520whom%2520are%2520also%2520part%2520of%2520the%2520Core%2520set%2520in%250AfMRI-Shape%252C%2520with%2520each%2520subject%2520viewing%25203142%25203D%2520objects%2520across%2520117%2520categories%252C%250Aall%2520accompanied%2520by%2520text%2520captions.%2520This%2520significantly%2520enhances%2520the%2520diversity%2520and%250Apotential%2520applications%2520of%2520the%2520dataset.%2520Additionally%252C%2520we%2520propose%2520MinD-3D%252C%2520a%250Anovel%2520framework%2520designed%2520to%2520decode%25203D%2520visual%2520information%2520from%2520fMRI%2520signals.%2520The%250Aframework%2520first%2520extracts%2520and%2520aggregates%2520features%2520from%2520fMRI%2520data%2520using%2520a%250Aneuro-fusion%2520encoder%252C%2520then%2520employs%2520a%2520feature-bridge%2520diffusion%2520model%2520to%2520generate%250Avisual%2520features%252C%2520and%2520finally%2520reconstructs%2520the%25203D%2520object%2520using%2520a%2520generative%250Atransformer%2520decoder.%2520We%2520establish%2520new%2520benchmarks%2520by%2520designing%2520metrics%2520at%2520both%250Asemantic%2520and%2520structural%2520levels%2520to%2520evaluate%2520model%2520performance.%2520Furthermore%252C%2520we%250Aassess%2520our%2520model%2527s%2520effectiveness%2520in%2520an%2520Out-of-Distribution%2520setting%2520and%2520analyze%250Athe%2520attribution%2520of%2520the%2520extracted%2520features%2520and%2520the%2520visual%2520ROIs%2520in%2520fMRI%2520signals.%250AOur%2520experiments%2520demonstrate%2520that%2520MinD-3D%2520not%2520only%2520reconstructs%25203D%2520objects%2520with%250Ahigh%2520semantic%2520and%2520spatial%2520accuracy%2520but%2520also%2520deepens%2520our%2520understanding%2520of%2520how%250Ahuman%2520brain%2520processes%25203D%2520visual%2520information.%2520Project%2520page%2520at%253A%250Ahttps%253A//jianxgao.github.io/MinD-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=fMRI-3D%3A%20A%20Comprehensive%20Dataset%20for%20Enhancing%20fMRI-based%203D%0A%20%20Reconstruction&entry.906535625=Jianxiong%20Gao%20and%20Yuqian%20Fu%20and%20Yun%20Wang%20and%20Xuelin%20Qian%20and%20Jianfeng%20Feng%20and%20Yanwei%20Fu&entry.1292438233=%20%20Reconstructing%203D%20visuals%20from%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Adata%2C%20introduced%20as%20Recon3DMind%20in%20our%20conference%20work%2C%20is%20of%20significant%0Ainterest%20to%20both%20cognitive%20neuroscience%20and%20computer%20vision.%20To%20advance%20this%0Atask%2C%20we%20present%20the%20fMRI-3D%20dataset%2C%20which%20includes%20data%20from%2015%20participants%0Aand%20showcases%20a%20total%20of%204768%203D%20objects.%20The%20dataset%20comprises%20two%20components%3A%0AfMRI-Shape%2C%20previously%20introduced%20and%20accessible%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Shape%2C%20and%20fMRI-Objaverse%2C%0Aproposed%20in%20this%20paper%20and%20available%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse.%20fMRI-Objaverse%0Aincludes%20data%20from%205%20subjects%2C%204%20of%20whom%20are%20also%20part%20of%20the%20Core%20set%20in%0AfMRI-Shape%2C%20with%20each%20subject%20viewing%203142%203D%20objects%20across%20117%20categories%2C%0Aall%20accompanied%20by%20text%20captions.%20This%20significantly%20enhances%20the%20diversity%20and%0Apotential%20applications%20of%20the%20dataset.%20Additionally%2C%20we%20propose%20MinD-3D%2C%20a%0Anovel%20framework%20designed%20to%20decode%203D%20visual%20information%20from%20fMRI%20signals.%20The%0Aframework%20first%20extracts%20and%20aggregates%20features%20from%20fMRI%20data%20using%20a%0Aneuro-fusion%20encoder%2C%20then%20employs%20a%20feature-bridge%20diffusion%20model%20to%20generate%0Avisual%20features%2C%20and%20finally%20reconstructs%20the%203D%20object%20using%20a%20generative%0Atransformer%20decoder.%20We%20establish%20new%20benchmarks%20by%20designing%20metrics%20at%20both%0Asemantic%20and%20structural%20levels%20to%20evaluate%20model%20performance.%20Furthermore%2C%20we%0Aassess%20our%20model%27s%20effectiveness%20in%20an%20Out-of-Distribution%20setting%20and%20analyze%0Athe%20attribution%20of%20the%20extracted%20features%20and%20the%20visual%20ROIs%20in%20fMRI%20signals.%0AOur%20experiments%20demonstrate%20that%20MinD-3D%20not%20only%20reconstructs%203D%20objects%20with%0Ahigh%20semantic%20and%20spatial%20accuracy%20but%20also%20deepens%20our%20understanding%20of%20how%0Ahuman%20brain%20processes%203D%20visual%20information.%20Project%20page%20at%3A%0Ahttps%3A//jianxgao.github.io/MinD-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11315v1&entry.124074799=Read"},
{"title": "TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields", "author": "Chengyao Duan and Zhiliu Yang", "abstract": "  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. This paper proposes a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, a tracking process and a mapping process, are\nmaintained simultaneously in our framework. In the tracking process, all input\nimages are uniformly sampled and then progressively trained in a\nself-supervised paradigm. In the mapping process, we leverage motion masks to\ndistinguish dynamic objects from the static background, and sample more pixels\nfrom dynamic areas. Secondly, the parameter optimization for both processes is\ncomprised of two stages: the first stage associates time with 3D positions to\nconvert the deformation field to the canonical field. The second stage\nassociates time with the embeddings of the canonical field to obtain colors and\na Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection\nstrategy based on the overlapping rate. Our approach is evaluated on two\nsynthetic datasets and one real-world dataset, and the experiments validate\nthat our method achieves competitive results in both tracking and mapping when\ncompared to existing state-of-the-art NeRF-based dynamic SLAM systems.\n", "link": "http://arxiv.org/abs/2310.18917v6", "date": "2024-09-17", "relevancy": 3.0941, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields&body=Title%3A%20TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields%0AAuthor%3A%20Chengyao%20Duan%20and%20Zhiliu%20Yang%0AAbstract%3A%20%20%20Previous%20attempts%20to%20integrate%20Neural%20Radiance%20Fields%20%28NeRF%29%20into%20the%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20framework%20either%20rely%20on%20the%0Aassumption%20of%20static%20scenes%20or%20require%20the%20ground%20truth%20camera%20poses%2C%20which%0Aimpedes%20their%20application%20in%20real-world%20scenarios.%20This%20paper%20proposes%20a%0Atime-varying%20representation%20to%20track%20and%20reconstruct%20the%20dynamic%20scenes.%0AFirstly%2C%20two%20processes%2C%20a%20tracking%20process%20and%20a%20mapping%20process%2C%20are%0Amaintained%20simultaneously%20in%20our%20framework.%20In%20the%20tracking%20process%2C%20all%20input%0Aimages%20are%20uniformly%20sampled%20and%20then%20progressively%20trained%20in%20a%0Aself-supervised%20paradigm.%20In%20the%20mapping%20process%2C%20we%20leverage%20motion%20masks%20to%0Adistinguish%20dynamic%20objects%20from%20the%20static%20background%2C%20and%20sample%20more%20pixels%0Afrom%20dynamic%20areas.%20Secondly%2C%20the%20parameter%20optimization%20for%20both%20processes%20is%0Acomprised%20of%20two%20stages%3A%20the%20first%20stage%20associates%20time%20with%203D%20positions%20to%0Aconvert%20the%20deformation%20field%20to%20the%20canonical%20field.%20The%20second%20stage%0Aassociates%20time%20with%20the%20embeddings%20of%20the%20canonical%20field%20to%20obtain%20colors%20and%0Aa%20Signed%20Distance%20Function%20%28SDF%29.%20Lastly%2C%20we%20propose%20a%20novel%20keyframe%20selection%0Astrategy%20based%20on%20the%20overlapping%20rate.%20Our%20approach%20is%20evaluated%20on%20two%0Asynthetic%20datasets%20and%20one%20real-world%20dataset%2C%20and%20the%20experiments%20validate%0Athat%20our%20method%20achieves%20competitive%20results%20in%20both%20tracking%20and%20mapping%20when%0Acompared%20to%20existing%20state-of-the-art%20NeRF-based%20dynamic%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18917v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTivNe-SLAM%253A%2520Dynamic%2520Mapping%2520and%2520Tracking%2520via%2520Time-Varying%2520Neural%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DChengyao%2520Duan%2520and%2520Zhiliu%2520Yang%26entry.1292438233%3D%2520%2520Previous%2520attempts%2520to%2520integrate%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520into%2520the%250ASimultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520framework%2520either%2520rely%2520on%2520the%250Aassumption%2520of%2520static%2520scenes%2520or%2520require%2520the%2520ground%2520truth%2520camera%2520poses%252C%2520which%250Aimpedes%2520their%2520application%2520in%2520real-world%2520scenarios.%2520This%2520paper%2520proposes%2520a%250Atime-varying%2520representation%2520to%2520track%2520and%2520reconstruct%2520the%2520dynamic%2520scenes.%250AFirstly%252C%2520two%2520processes%252C%2520a%2520tracking%2520process%2520and%2520a%2520mapping%2520process%252C%2520are%250Amaintained%2520simultaneously%2520in%2520our%2520framework.%2520In%2520the%2520tracking%2520process%252C%2520all%2520input%250Aimages%2520are%2520uniformly%2520sampled%2520and%2520then%2520progressively%2520trained%2520in%2520a%250Aself-supervised%2520paradigm.%2520In%2520the%2520mapping%2520process%252C%2520we%2520leverage%2520motion%2520masks%2520to%250Adistinguish%2520dynamic%2520objects%2520from%2520the%2520static%2520background%252C%2520and%2520sample%2520more%2520pixels%250Afrom%2520dynamic%2520areas.%2520Secondly%252C%2520the%2520parameter%2520optimization%2520for%2520both%2520processes%2520is%250Acomprised%2520of%2520two%2520stages%253A%2520the%2520first%2520stage%2520associates%2520time%2520with%25203D%2520positions%2520to%250Aconvert%2520the%2520deformation%2520field%2520to%2520the%2520canonical%2520field.%2520The%2520second%2520stage%250Aassociates%2520time%2520with%2520the%2520embeddings%2520of%2520the%2520canonical%2520field%2520to%2520obtain%2520colors%2520and%250Aa%2520Signed%2520Distance%2520Function%2520%2528SDF%2529.%2520Lastly%252C%2520we%2520propose%2520a%2520novel%2520keyframe%2520selection%250Astrategy%2520based%2520on%2520the%2520overlapping%2520rate.%2520Our%2520approach%2520is%2520evaluated%2520on%2520two%250Asynthetic%2520datasets%2520and%2520one%2520real-world%2520dataset%252C%2520and%2520the%2520experiments%2520validate%250Athat%2520our%2520method%2520achieves%2520competitive%2520results%2520in%2520both%2520tracking%2520and%2520mapping%2520when%250Acompared%2520to%2520existing%2520state-of-the-art%2520NeRF-based%2520dynamic%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18917v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TivNe-SLAM%3A%20Dynamic%20Mapping%20and%20Tracking%20via%20Time-Varying%20Neural%0A%20%20Radiance%20Fields&entry.906535625=Chengyao%20Duan%20and%20Zhiliu%20Yang&entry.1292438233=%20%20Previous%20attempts%20to%20integrate%20Neural%20Radiance%20Fields%20%28NeRF%29%20into%20the%0ASimultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20framework%20either%20rely%20on%20the%0Aassumption%20of%20static%20scenes%20or%20require%20the%20ground%20truth%20camera%20poses%2C%20which%0Aimpedes%20their%20application%20in%20real-world%20scenarios.%20This%20paper%20proposes%20a%0Atime-varying%20representation%20to%20track%20and%20reconstruct%20the%20dynamic%20scenes.%0AFirstly%2C%20two%20processes%2C%20a%20tracking%20process%20and%20a%20mapping%20process%2C%20are%0Amaintained%20simultaneously%20in%20our%20framework.%20In%20the%20tracking%20process%2C%20all%20input%0Aimages%20are%20uniformly%20sampled%20and%20then%20progressively%20trained%20in%20a%0Aself-supervised%20paradigm.%20In%20the%20mapping%20process%2C%20we%20leverage%20motion%20masks%20to%0Adistinguish%20dynamic%20objects%20from%20the%20static%20background%2C%20and%20sample%20more%20pixels%0Afrom%20dynamic%20areas.%20Secondly%2C%20the%20parameter%20optimization%20for%20both%20processes%20is%0Acomprised%20of%20two%20stages%3A%20the%20first%20stage%20associates%20time%20with%203D%20positions%20to%0Aconvert%20the%20deformation%20field%20to%20the%20canonical%20field.%20The%20second%20stage%0Aassociates%20time%20with%20the%20embeddings%20of%20the%20canonical%20field%20to%20obtain%20colors%20and%0Aa%20Signed%20Distance%20Function%20%28SDF%29.%20Lastly%2C%20we%20propose%20a%20novel%20keyframe%20selection%0Astrategy%20based%20on%20the%20overlapping%20rate.%20Our%20approach%20is%20evaluated%20on%20two%0Asynthetic%20datasets%20and%20one%20real-world%20dataset%2C%20and%20the%20experiments%20validate%0Athat%20our%20method%20achieves%20competitive%20results%20in%20both%20tracking%20and%20mapping%20when%0Acompared%20to%20existing%20state-of-the-art%20NeRF-based%20dynamic%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18917v6&entry.124074799=Read"},
{"title": "UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D\n  Object Detection via Channel2height", "author": "Zichen Yu and Changyong Shu", "abstract": "  Occupancy and 3D object detection are characterized as two standard tasks in\nmodern autonomous driving system. In order to deploy them on a series of edge\nchips with better precision and time-consuming trade-off, contemporary\napproaches either deploy standalone models for individual tasks, or design a\nmulti-task paradigm with separate heads. However, they might suffer from\ndeployment difficulties (i.e., 3D convolution, transformer and so on) or\ndeficiencies in task coordination. Instead, we argue that a favorable framework\nshould be devised in pursuit of ease deployment on diverse chips and high\nprecision with little time-consuming. Oriented at this, we revisit the paradigm\nfor interaction between 3D object detection and occupancy prediction,\nreformulate the model with 2D convolution and prioritize the tasks such that\neach contributes to other. Thus, we propose a method to achieve fast 3D object\ndetection and occupancy prediction (UltimateDO), wherein the light occupancy\nprediction head in FlashOcc is married to 3D object detection network, with\nnegligible additional timeconsuming of only 1.1ms while facilitating each\nother. We instantiate UltimateDO on the challenging nuScenes-series benchmarks.\n", "link": "http://arxiv.org/abs/2409.11160v1", "date": "2024-09-17", "relevancy": 2.9726, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltimateDO%3A%20An%20Efficient%20Framework%20to%20Marry%20Occupancy%20Prediction%20with%203D%0A%20%20Object%20Detection%20via%20Channel2height&body=Title%3A%20UltimateDO%3A%20An%20Efficient%20Framework%20to%20Marry%20Occupancy%20Prediction%20with%203D%0A%20%20Object%20Detection%20via%20Channel2height%0AAuthor%3A%20Zichen%20Yu%20and%20Changyong%20Shu%0AAbstract%3A%20%20%20Occupancy%20and%203D%20object%20detection%20are%20characterized%20as%20two%20standard%20tasks%20in%0Amodern%20autonomous%20driving%20system.%20In%20order%20to%20deploy%20them%20on%20a%20series%20of%20edge%0Achips%20with%20better%20precision%20and%20time-consuming%20trade-off%2C%20contemporary%0Aapproaches%20either%20deploy%20standalone%20models%20for%20individual%20tasks%2C%20or%20design%20a%0Amulti-task%20paradigm%20with%20separate%20heads.%20However%2C%20they%20might%20suffer%20from%0Adeployment%20difficulties%20%28i.e.%2C%203D%20convolution%2C%20transformer%20and%20so%20on%29%20or%0Adeficiencies%20in%20task%20coordination.%20Instead%2C%20we%20argue%20that%20a%20favorable%20framework%0Ashould%20be%20devised%20in%20pursuit%20of%20ease%20deployment%20on%20diverse%20chips%20and%20high%0Aprecision%20with%20little%20time-consuming.%20Oriented%20at%20this%2C%20we%20revisit%20the%20paradigm%0Afor%20interaction%20between%203D%20object%20detection%20and%20occupancy%20prediction%2C%0Areformulate%20the%20model%20with%202D%20convolution%20and%20prioritize%20the%20tasks%20such%20that%0Aeach%20contributes%20to%20other.%20Thus%2C%20we%20propose%20a%20method%20to%20achieve%20fast%203D%20object%0Adetection%20and%20occupancy%20prediction%20%28UltimateDO%29%2C%20wherein%20the%20light%20occupancy%0Aprediction%20head%20in%20FlashOcc%20is%20married%20to%203D%20object%20detection%20network%2C%20with%0Anegligible%20additional%20timeconsuming%20of%20only%201.1ms%20while%20facilitating%20each%0Aother.%20We%20instantiate%20UltimateDO%20on%20the%20challenging%20nuScenes-series%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltimateDO%253A%2520An%2520Efficient%2520Framework%2520to%2520Marry%2520Occupancy%2520Prediction%2520with%25203D%250A%2520%2520Object%2520Detection%2520via%2520Channel2height%26entry.906535625%3DZichen%2520Yu%2520and%2520Changyong%2520Shu%26entry.1292438233%3D%2520%2520Occupancy%2520and%25203D%2520object%2520detection%2520are%2520characterized%2520as%2520two%2520standard%2520tasks%2520in%250Amodern%2520autonomous%2520driving%2520system.%2520In%2520order%2520to%2520deploy%2520them%2520on%2520a%2520series%2520of%2520edge%250Achips%2520with%2520better%2520precision%2520and%2520time-consuming%2520trade-off%252C%2520contemporary%250Aapproaches%2520either%2520deploy%2520standalone%2520models%2520for%2520individual%2520tasks%252C%2520or%2520design%2520a%250Amulti-task%2520paradigm%2520with%2520separate%2520heads.%2520However%252C%2520they%2520might%2520suffer%2520from%250Adeployment%2520difficulties%2520%2528i.e.%252C%25203D%2520convolution%252C%2520transformer%2520and%2520so%2520on%2529%2520or%250Adeficiencies%2520in%2520task%2520coordination.%2520Instead%252C%2520we%2520argue%2520that%2520a%2520favorable%2520framework%250Ashould%2520be%2520devised%2520in%2520pursuit%2520of%2520ease%2520deployment%2520on%2520diverse%2520chips%2520and%2520high%250Aprecision%2520with%2520little%2520time-consuming.%2520Oriented%2520at%2520this%252C%2520we%2520revisit%2520the%2520paradigm%250Afor%2520interaction%2520between%25203D%2520object%2520detection%2520and%2520occupancy%2520prediction%252C%250Areformulate%2520the%2520model%2520with%25202D%2520convolution%2520and%2520prioritize%2520the%2520tasks%2520such%2520that%250Aeach%2520contributes%2520to%2520other.%2520Thus%252C%2520we%2520propose%2520a%2520method%2520to%2520achieve%2520fast%25203D%2520object%250Adetection%2520and%2520occupancy%2520prediction%2520%2528UltimateDO%2529%252C%2520wherein%2520the%2520light%2520occupancy%250Aprediction%2520head%2520in%2520FlashOcc%2520is%2520married%2520to%25203D%2520object%2520detection%2520network%252C%2520with%250Anegligible%2520additional%2520timeconsuming%2520of%2520only%25201.1ms%2520while%2520facilitating%2520each%250Aother.%2520We%2520instantiate%2520UltimateDO%2520on%2520the%2520challenging%2520nuScenes-series%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltimateDO%3A%20An%20Efficient%20Framework%20to%20Marry%20Occupancy%20Prediction%20with%203D%0A%20%20Object%20Detection%20via%20Channel2height&entry.906535625=Zichen%20Yu%20and%20Changyong%20Shu&entry.1292438233=%20%20Occupancy%20and%203D%20object%20detection%20are%20characterized%20as%20two%20standard%20tasks%20in%0Amodern%20autonomous%20driving%20system.%20In%20order%20to%20deploy%20them%20on%20a%20series%20of%20edge%0Achips%20with%20better%20precision%20and%20time-consuming%20trade-off%2C%20contemporary%0Aapproaches%20either%20deploy%20standalone%20models%20for%20individual%20tasks%2C%20or%20design%20a%0Amulti-task%20paradigm%20with%20separate%20heads.%20However%2C%20they%20might%20suffer%20from%0Adeployment%20difficulties%20%28i.e.%2C%203D%20convolution%2C%20transformer%20and%20so%20on%29%20or%0Adeficiencies%20in%20task%20coordination.%20Instead%2C%20we%20argue%20that%20a%20favorable%20framework%0Ashould%20be%20devised%20in%20pursuit%20of%20ease%20deployment%20on%20diverse%20chips%20and%20high%0Aprecision%20with%20little%20time-consuming.%20Oriented%20at%20this%2C%20we%20revisit%20the%20paradigm%0Afor%20interaction%20between%203D%20object%20detection%20and%20occupancy%20prediction%2C%0Areformulate%20the%20model%20with%202D%20convolution%20and%20prioritize%20the%20tasks%20such%20that%0Aeach%20contributes%20to%20other.%20Thus%2C%20we%20propose%20a%20method%20to%20achieve%20fast%203D%20object%0Adetection%20and%20occupancy%20prediction%20%28UltimateDO%29%2C%20wherein%20the%20light%20occupancy%0Aprediction%20head%20in%20FlashOcc%20is%20married%20to%203D%20object%20detection%20network%2C%20with%0Anegligible%20additional%20timeconsuming%20of%20only%201.1ms%20while%20facilitating%20each%0Aother.%20We%20instantiate%20UltimateDO%20on%20the%20challenging%20nuScenes-series%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11160v1&entry.124074799=Read"},
{"title": "OneEncoder: A Lightweight Framework for Progressive Alignment of\n  Modalities", "author": "Bilal Faye and Hanane Azzag and Mustapha Lebbah", "abstract": "  Cross-modal alignment Learning integrates information from different\nmodalities like text, image, audio and video to create unified models. This\napproach develops shared representations and learns correlations between\nmodalities, enabling applications such as visual question answering and\naudiovisual content analysis. Current techniques rely on large\nmodality-specific encoders, necessitating fine-tuning or training from scratch\non vast aligned datasets (e.g., text-image, text-audio, image-audio). This\napproach has limitations: (i) it is very expensive due to the need for training\nlarge encoders on extensive datasets, (ii) acquiring aligned large paired\ndatasets is challenging, and (iii) adding new modalities requires retraining\nthe entire framework to incorporate these modalities. To address these issues,\nwe propose OneEncoder, a lightweight framework that progressively represents\nand aligns four modalities (image, text, audio, video). Initially, we train a\nlightweight Universal Projection module (UP) to align image and text\nmodalities. Then, we freeze the pretrained UP and progressively align future\nmodalities to those already aligned. OneEncoder operates efficiently and\ncost-effectively, even in scenarios where vast aligned datasets are\nunavailable, due to its lightweight design. Trained on small paired datasets,\nit shows strong performance in tasks like classification, querying, and visual\nquestion answering, surpassing methods that rely on large datasets and\nspecialized encoders.\n", "link": "http://arxiv.org/abs/2409.11059v1", "date": "2024-09-17", "relevancy": 2.949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.59}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.59}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneEncoder%3A%20A%20Lightweight%20Framework%20for%20Progressive%20Alignment%20of%0A%20%20Modalities&body=Title%3A%20OneEncoder%3A%20A%20Lightweight%20Framework%20for%20Progressive%20Alignment%20of%0A%20%20Modalities%0AAuthor%3A%20Bilal%20Faye%20and%20Hanane%20Azzag%20and%20Mustapha%20Lebbah%0AAbstract%3A%20%20%20Cross-modal%20alignment%20Learning%20integrates%20information%20from%20different%0Amodalities%20like%20text%2C%20image%2C%20audio%20and%20video%20to%20create%20unified%20models.%20This%0Aapproach%20develops%20shared%20representations%20and%20learns%20correlations%20between%0Amodalities%2C%20enabling%20applications%20such%20as%20visual%20question%20answering%20and%0Aaudiovisual%20content%20analysis.%20Current%20techniques%20rely%20on%20large%0Amodality-specific%20encoders%2C%20necessitating%20fine-tuning%20or%20training%20from%20scratch%0Aon%20vast%20aligned%20datasets%20%28e.g.%2C%20text-image%2C%20text-audio%2C%20image-audio%29.%20This%0Aapproach%20has%20limitations%3A%20%28i%29%20it%20is%20very%20expensive%20due%20to%20the%20need%20for%20training%0Alarge%20encoders%20on%20extensive%20datasets%2C%20%28ii%29%20acquiring%20aligned%20large%20paired%0Adatasets%20is%20challenging%2C%20and%20%28iii%29%20adding%20new%20modalities%20requires%20retraining%0Athe%20entire%20framework%20to%20incorporate%20these%20modalities.%20To%20address%20these%20issues%2C%0Awe%20propose%20OneEncoder%2C%20a%20lightweight%20framework%20that%20progressively%20represents%0Aand%20aligns%20four%20modalities%20%28image%2C%20text%2C%20audio%2C%20video%29.%20Initially%2C%20we%20train%20a%0Alightweight%20Universal%20Projection%20module%20%28UP%29%20to%20align%20image%20and%20text%0Amodalities.%20Then%2C%20we%20freeze%20the%20pretrained%20UP%20and%20progressively%20align%20future%0Amodalities%20to%20those%20already%20aligned.%20OneEncoder%20operates%20efficiently%20and%0Acost-effectively%2C%20even%20in%20scenarios%20where%20vast%20aligned%20datasets%20are%0Aunavailable%2C%20due%20to%20its%20lightweight%20design.%20Trained%20on%20small%20paired%20datasets%2C%0Ait%20shows%20strong%20performance%20in%20tasks%20like%20classification%2C%20querying%2C%20and%20visual%0Aquestion%20answering%2C%20surpassing%20methods%20that%20rely%20on%20large%20datasets%20and%0Aspecialized%20encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneEncoder%253A%2520A%2520Lightweight%2520Framework%2520for%2520Progressive%2520Alignment%2520of%250A%2520%2520Modalities%26entry.906535625%3DBilal%2520Faye%2520and%2520Hanane%2520Azzag%2520and%2520Mustapha%2520Lebbah%26entry.1292438233%3D%2520%2520Cross-modal%2520alignment%2520Learning%2520integrates%2520information%2520from%2520different%250Amodalities%2520like%2520text%252C%2520image%252C%2520audio%2520and%2520video%2520to%2520create%2520unified%2520models.%2520This%250Aapproach%2520develops%2520shared%2520representations%2520and%2520learns%2520correlations%2520between%250Amodalities%252C%2520enabling%2520applications%2520such%2520as%2520visual%2520question%2520answering%2520and%250Aaudiovisual%2520content%2520analysis.%2520Current%2520techniques%2520rely%2520on%2520large%250Amodality-specific%2520encoders%252C%2520necessitating%2520fine-tuning%2520or%2520training%2520from%2520scratch%250Aon%2520vast%2520aligned%2520datasets%2520%2528e.g.%252C%2520text-image%252C%2520text-audio%252C%2520image-audio%2529.%2520This%250Aapproach%2520has%2520limitations%253A%2520%2528i%2529%2520it%2520is%2520very%2520expensive%2520due%2520to%2520the%2520need%2520for%2520training%250Alarge%2520encoders%2520on%2520extensive%2520datasets%252C%2520%2528ii%2529%2520acquiring%2520aligned%2520large%2520paired%250Adatasets%2520is%2520challenging%252C%2520and%2520%2528iii%2529%2520adding%2520new%2520modalities%2520requires%2520retraining%250Athe%2520entire%2520framework%2520to%2520incorporate%2520these%2520modalities.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520OneEncoder%252C%2520a%2520lightweight%2520framework%2520that%2520progressively%2520represents%250Aand%2520aligns%2520four%2520modalities%2520%2528image%252C%2520text%252C%2520audio%252C%2520video%2529.%2520Initially%252C%2520we%2520train%2520a%250Alightweight%2520Universal%2520Projection%2520module%2520%2528UP%2529%2520to%2520align%2520image%2520and%2520text%250Amodalities.%2520Then%252C%2520we%2520freeze%2520the%2520pretrained%2520UP%2520and%2520progressively%2520align%2520future%250Amodalities%2520to%2520those%2520already%2520aligned.%2520OneEncoder%2520operates%2520efficiently%2520and%250Acost-effectively%252C%2520even%2520in%2520scenarios%2520where%2520vast%2520aligned%2520datasets%2520are%250Aunavailable%252C%2520due%2520to%2520its%2520lightweight%2520design.%2520Trained%2520on%2520small%2520paired%2520datasets%252C%250Ait%2520shows%2520strong%2520performance%2520in%2520tasks%2520like%2520classification%252C%2520querying%252C%2520and%2520visual%250Aquestion%2520answering%252C%2520surpassing%2520methods%2520that%2520rely%2520on%2520large%2520datasets%2520and%250Aspecialized%2520encoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneEncoder%3A%20A%20Lightweight%20Framework%20for%20Progressive%20Alignment%20of%0A%20%20Modalities&entry.906535625=Bilal%20Faye%20and%20Hanane%20Azzag%20and%20Mustapha%20Lebbah&entry.1292438233=%20%20Cross-modal%20alignment%20Learning%20integrates%20information%20from%20different%0Amodalities%20like%20text%2C%20image%2C%20audio%20and%20video%20to%20create%20unified%20models.%20This%0Aapproach%20develops%20shared%20representations%20and%20learns%20correlations%20between%0Amodalities%2C%20enabling%20applications%20such%20as%20visual%20question%20answering%20and%0Aaudiovisual%20content%20analysis.%20Current%20techniques%20rely%20on%20large%0Amodality-specific%20encoders%2C%20necessitating%20fine-tuning%20or%20training%20from%20scratch%0Aon%20vast%20aligned%20datasets%20%28e.g.%2C%20text-image%2C%20text-audio%2C%20image-audio%29.%20This%0Aapproach%20has%20limitations%3A%20%28i%29%20it%20is%20very%20expensive%20due%20to%20the%20need%20for%20training%0Alarge%20encoders%20on%20extensive%20datasets%2C%20%28ii%29%20acquiring%20aligned%20large%20paired%0Adatasets%20is%20challenging%2C%20and%20%28iii%29%20adding%20new%20modalities%20requires%20retraining%0Athe%20entire%20framework%20to%20incorporate%20these%20modalities.%20To%20address%20these%20issues%2C%0Awe%20propose%20OneEncoder%2C%20a%20lightweight%20framework%20that%20progressively%20represents%0Aand%20aligns%20four%20modalities%20%28image%2C%20text%2C%20audio%2C%20video%29.%20Initially%2C%20we%20train%20a%0Alightweight%20Universal%20Projection%20module%20%28UP%29%20to%20align%20image%20and%20text%0Amodalities.%20Then%2C%20we%20freeze%20the%20pretrained%20UP%20and%20progressively%20align%20future%0Amodalities%20to%20those%20already%20aligned.%20OneEncoder%20operates%20efficiently%20and%0Acost-effectively%2C%20even%20in%20scenarios%20where%20vast%20aligned%20datasets%20are%0Aunavailable%2C%20due%20to%20its%20lightweight%20design.%20Trained%20on%20small%20paired%20datasets%2C%0Ait%20shows%20strong%20performance%20in%20tasks%20like%20classification%2C%20querying%2C%20and%20visual%0Aquestion%20answering%2C%20surpassing%20methods%20that%20rely%20on%20large%20datasets%20and%0Aspecialized%20encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11059v1&entry.124074799=Read"},
{"title": "GenQ: Quantization in Low Data Regimes with Generative Synthetic Data", "author": "Yuhang Li and Youngeun Kim and Donghyun Lee and Souvik Kundu and Priyadarshini Panda", "abstract": "  In the realm of deep neural network deployment, low-bit quantization presents\na promising avenue for enhancing computational efficiency. However, it often\nhinges on the availability of training data to mitigate quantization errors, a\nsignificant challenge when data availability is scarce or restricted due to\nprivacy or copyright concerns. Addressing this, we introduce GenQ, a novel\napproach employing an advanced Generative AI model to generate photorealistic,\nhigh-resolution synthetic data, overcoming the limitations of traditional\nmethods that struggle to accurately mimic complex objects in extensive datasets\nlike ImageNet. Our methodology is underscored by two robust filtering\nmechanisms designed to ensure the synthetic data closely aligns with the\nintrinsic characteristics of the actual training data. In case of limited data\navailability, the actual data is used to guide the synthetic data generation\nprocess, enhancing fidelity through the inversion of learnable token\nembeddings. Through rigorous experimentation, GenQ establishes new benchmarks\nin data-free and data-scarce quantization, significantly outperforming existing\nmethods in accuracy and efficiency, thereby setting a new standard for\nquantization in low data regimes. Code is released at\n\\url{https://github.com/Intelligent-Computing-Lab-Yale/GenQ}.\n", "link": "http://arxiv.org/abs/2312.05272v3", "date": "2024-09-17", "relevancy": 2.9446, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5977}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5887}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenQ%3A%20Quantization%20in%20Low%20Data%20Regimes%20with%20Generative%20Synthetic%20Data&body=Title%3A%20GenQ%3A%20Quantization%20in%20Low%20Data%20Regimes%20with%20Generative%20Synthetic%20Data%0AAuthor%3A%20Yuhang%20Li%20and%20Youngeun%20Kim%20and%20Donghyun%20Lee%20and%20Souvik%20Kundu%20and%20Priyadarshini%20Panda%0AAbstract%3A%20%20%20In%20the%20realm%20of%20deep%20neural%20network%20deployment%2C%20low-bit%20quantization%20presents%0Aa%20promising%20avenue%20for%20enhancing%20computational%20efficiency.%20However%2C%20it%20often%0Ahinges%20on%20the%20availability%20of%20training%20data%20to%20mitigate%20quantization%20errors%2C%20a%0Asignificant%20challenge%20when%20data%20availability%20is%20scarce%20or%20restricted%20due%20to%0Aprivacy%20or%20copyright%20concerns.%20Addressing%20this%2C%20we%20introduce%20GenQ%2C%20a%20novel%0Aapproach%20employing%20an%20advanced%20Generative%20AI%20model%20to%20generate%20photorealistic%2C%0Ahigh-resolution%20synthetic%20data%2C%20overcoming%20the%20limitations%20of%20traditional%0Amethods%20that%20struggle%20to%20accurately%20mimic%20complex%20objects%20in%20extensive%20datasets%0Alike%20ImageNet.%20Our%20methodology%20is%20underscored%20by%20two%20robust%20filtering%0Amechanisms%20designed%20to%20ensure%20the%20synthetic%20data%20closely%20aligns%20with%20the%0Aintrinsic%20characteristics%20of%20the%20actual%20training%20data.%20In%20case%20of%20limited%20data%0Aavailability%2C%20the%20actual%20data%20is%20used%20to%20guide%20the%20synthetic%20data%20generation%0Aprocess%2C%20enhancing%20fidelity%20through%20the%20inversion%20of%20learnable%20token%0Aembeddings.%20Through%20rigorous%20experimentation%2C%20GenQ%20establishes%20new%20benchmarks%0Ain%20data-free%20and%20data-scarce%20quantization%2C%20significantly%20outperforming%20existing%0Amethods%20in%20accuracy%20and%20efficiency%2C%20thereby%20setting%20a%20new%20standard%20for%0Aquantization%20in%20low%20data%20regimes.%20Code%20is%20released%20at%0A%5Curl%7Bhttps%3A//github.com/Intelligent-Computing-Lab-Yale/GenQ%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05272v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenQ%253A%2520Quantization%2520in%2520Low%2520Data%2520Regimes%2520with%2520Generative%2520Synthetic%2520Data%26entry.906535625%3DYuhang%2520Li%2520and%2520Youngeun%2520Kim%2520and%2520Donghyun%2520Lee%2520and%2520Souvik%2520Kundu%2520and%2520Priyadarshini%2520Panda%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520deep%2520neural%2520network%2520deployment%252C%2520low-bit%2520quantization%2520presents%250Aa%2520promising%2520avenue%2520for%2520enhancing%2520computational%2520efficiency.%2520However%252C%2520it%2520often%250Ahinges%2520on%2520the%2520availability%2520of%2520training%2520data%2520to%2520mitigate%2520quantization%2520errors%252C%2520a%250Asignificant%2520challenge%2520when%2520data%2520availability%2520is%2520scarce%2520or%2520restricted%2520due%2520to%250Aprivacy%2520or%2520copyright%2520concerns.%2520Addressing%2520this%252C%2520we%2520introduce%2520GenQ%252C%2520a%2520novel%250Aapproach%2520employing%2520an%2520advanced%2520Generative%2520AI%2520model%2520to%2520generate%2520photorealistic%252C%250Ahigh-resolution%2520synthetic%2520data%252C%2520overcoming%2520the%2520limitations%2520of%2520traditional%250Amethods%2520that%2520struggle%2520to%2520accurately%2520mimic%2520complex%2520objects%2520in%2520extensive%2520datasets%250Alike%2520ImageNet.%2520Our%2520methodology%2520is%2520underscored%2520by%2520two%2520robust%2520filtering%250Amechanisms%2520designed%2520to%2520ensure%2520the%2520synthetic%2520data%2520closely%2520aligns%2520with%2520the%250Aintrinsic%2520characteristics%2520of%2520the%2520actual%2520training%2520data.%2520In%2520case%2520of%2520limited%2520data%250Aavailability%252C%2520the%2520actual%2520data%2520is%2520used%2520to%2520guide%2520the%2520synthetic%2520data%2520generation%250Aprocess%252C%2520enhancing%2520fidelity%2520through%2520the%2520inversion%2520of%2520learnable%2520token%250Aembeddings.%2520Through%2520rigorous%2520experimentation%252C%2520GenQ%2520establishes%2520new%2520benchmarks%250Ain%2520data-free%2520and%2520data-scarce%2520quantization%252C%2520significantly%2520outperforming%2520existing%250Amethods%2520in%2520accuracy%2520and%2520efficiency%252C%2520thereby%2520setting%2520a%2520new%2520standard%2520for%250Aquantization%2520in%2520low%2520data%2520regimes.%2520Code%2520is%2520released%2520at%250A%255Curl%257Bhttps%253A//github.com/Intelligent-Computing-Lab-Yale/GenQ%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05272v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenQ%3A%20Quantization%20in%20Low%20Data%20Regimes%20with%20Generative%20Synthetic%20Data&entry.906535625=Yuhang%20Li%20and%20Youngeun%20Kim%20and%20Donghyun%20Lee%20and%20Souvik%20Kundu%20and%20Priyadarshini%20Panda&entry.1292438233=%20%20In%20the%20realm%20of%20deep%20neural%20network%20deployment%2C%20low-bit%20quantization%20presents%0Aa%20promising%20avenue%20for%20enhancing%20computational%20efficiency.%20However%2C%20it%20often%0Ahinges%20on%20the%20availability%20of%20training%20data%20to%20mitigate%20quantization%20errors%2C%20a%0Asignificant%20challenge%20when%20data%20availability%20is%20scarce%20or%20restricted%20due%20to%0Aprivacy%20or%20copyright%20concerns.%20Addressing%20this%2C%20we%20introduce%20GenQ%2C%20a%20novel%0Aapproach%20employing%20an%20advanced%20Generative%20AI%20model%20to%20generate%20photorealistic%2C%0Ahigh-resolution%20synthetic%20data%2C%20overcoming%20the%20limitations%20of%20traditional%0Amethods%20that%20struggle%20to%20accurately%20mimic%20complex%20objects%20in%20extensive%20datasets%0Alike%20ImageNet.%20Our%20methodology%20is%20underscored%20by%20two%20robust%20filtering%0Amechanisms%20designed%20to%20ensure%20the%20synthetic%20data%20closely%20aligns%20with%20the%0Aintrinsic%20characteristics%20of%20the%20actual%20training%20data.%20In%20case%20of%20limited%20data%0Aavailability%2C%20the%20actual%20data%20is%20used%20to%20guide%20the%20synthetic%20data%20generation%0Aprocess%2C%20enhancing%20fidelity%20through%20the%20inversion%20of%20learnable%20token%0Aembeddings.%20Through%20rigorous%20experimentation%2C%20GenQ%20establishes%20new%20benchmarks%0Ain%20data-free%20and%20data-scarce%20quantization%2C%20significantly%20outperforming%20existing%0Amethods%20in%20accuracy%20and%20efficiency%2C%20thereby%20setting%20a%20new%20standard%20for%0Aquantization%20in%20low%20data%20regimes.%20Code%20is%20released%20at%0A%5Curl%7Bhttps%3A//github.com/Intelligent-Computing-Lab-Yale/GenQ%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05272v3&entry.124074799=Read"},
{"title": "Improving the Efficiency of Visually Augmented Language Models", "author": "Paula Ontalvilla and Aitor Ormazabal and Gorka Azkune", "abstract": "  Despite the impressive performance of autoregressive Language Models (LM) it\nhas been shown that due to reporting bias, LMs lack visual knowledge, i.e. they\ndo not know much about the visual world and its properties. To augment LMs with\nvisual knowledge, existing solutions often rely on explicit images, requiring\ntime-consuming retrieval or image generation systems. This paper shows that\nexplicit images are not necessary to visually augment an LM. Instead, we use\nvisually-grounded text representations obtained from the well-known CLIP\nmultimodal system. For a fair comparison, we modify VALM, a visually-augmented\nLM which uses image retrieval and representation, to work directly with\nvisually-grounded text representations. We name this new model BLIND-VALM. We\nshow that BLIND-VALM performs on par with VALM for Visual Language\nUnderstanding (VLU), Natural Language Understanding (NLU) and Language Modeling\ntasks, despite being significantly more efficient and simpler. We also show\nthat scaling up our model within the compute budget of VALM, either increasing\nthe model or pre-training corpus size, we outperform VALM for all the\nevaluation tasks.\n", "link": "http://arxiv.org/abs/2409.11148v1", "date": "2024-09-17", "relevancy": 2.8761, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Efficiency%20of%20Visually%20Augmented%20Language%20Models&body=Title%3A%20Improving%20the%20Efficiency%20of%20Visually%20Augmented%20Language%20Models%0AAuthor%3A%20Paula%20Ontalvilla%20and%20Aitor%20Ormazabal%20and%20Gorka%20Azkune%0AAbstract%3A%20%20%20Despite%20the%20impressive%20performance%20of%20autoregressive%20Language%20Models%20%28LM%29%20it%0Ahas%20been%20shown%20that%20due%20to%20reporting%20bias%2C%20LMs%20lack%20visual%20knowledge%2C%20i.e.%20they%0Ado%20not%20know%20much%20about%20the%20visual%20world%20and%20its%20properties.%20To%20augment%20LMs%20with%0Avisual%20knowledge%2C%20existing%20solutions%20often%20rely%20on%20explicit%20images%2C%20requiring%0Atime-consuming%20retrieval%20or%20image%20generation%20systems.%20This%20paper%20shows%20that%0Aexplicit%20images%20are%20not%20necessary%20to%20visually%20augment%20an%20LM.%20Instead%2C%20we%20use%0Avisually-grounded%20text%20representations%20obtained%20from%20the%20well-known%20CLIP%0Amultimodal%20system.%20For%20a%20fair%20comparison%2C%20we%20modify%20VALM%2C%20a%20visually-augmented%0ALM%20which%20uses%20image%20retrieval%20and%20representation%2C%20to%20work%20directly%20with%0Avisually-grounded%20text%20representations.%20We%20name%20this%20new%20model%20BLIND-VALM.%20We%0Ashow%20that%20BLIND-VALM%20performs%20on%20par%20with%20VALM%20for%20Visual%20Language%0AUnderstanding%20%28VLU%29%2C%20Natural%20Language%20Understanding%20%28NLU%29%20and%20Language%20Modeling%0Atasks%2C%20despite%20being%20significantly%20more%20efficient%20and%20simpler.%20We%20also%20show%0Athat%20scaling%20up%20our%20model%20within%20the%20compute%20budget%20of%20VALM%2C%20either%20increasing%0Athe%20model%20or%20pre-training%20corpus%20size%2C%20we%20outperform%20VALM%20for%20all%20the%0Aevaluation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Efficiency%2520of%2520Visually%2520Augmented%2520Language%2520Models%26entry.906535625%3DPaula%2520Ontalvilla%2520and%2520Aitor%2520Ormazabal%2520and%2520Gorka%2520Azkune%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520performance%2520of%2520autoregressive%2520Language%2520Models%2520%2528LM%2529%2520it%250Ahas%2520been%2520shown%2520that%2520due%2520to%2520reporting%2520bias%252C%2520LMs%2520lack%2520visual%2520knowledge%252C%2520i.e.%2520they%250Ado%2520not%2520know%2520much%2520about%2520the%2520visual%2520world%2520and%2520its%2520properties.%2520To%2520augment%2520LMs%2520with%250Avisual%2520knowledge%252C%2520existing%2520solutions%2520often%2520rely%2520on%2520explicit%2520images%252C%2520requiring%250Atime-consuming%2520retrieval%2520or%2520image%2520generation%2520systems.%2520This%2520paper%2520shows%2520that%250Aexplicit%2520images%2520are%2520not%2520necessary%2520to%2520visually%2520augment%2520an%2520LM.%2520Instead%252C%2520we%2520use%250Avisually-grounded%2520text%2520representations%2520obtained%2520from%2520the%2520well-known%2520CLIP%250Amultimodal%2520system.%2520For%2520a%2520fair%2520comparison%252C%2520we%2520modify%2520VALM%252C%2520a%2520visually-augmented%250ALM%2520which%2520uses%2520image%2520retrieval%2520and%2520representation%252C%2520to%2520work%2520directly%2520with%250Avisually-grounded%2520text%2520representations.%2520We%2520name%2520this%2520new%2520model%2520BLIND-VALM.%2520We%250Ashow%2520that%2520BLIND-VALM%2520performs%2520on%2520par%2520with%2520VALM%2520for%2520Visual%2520Language%250AUnderstanding%2520%2528VLU%2529%252C%2520Natural%2520Language%2520Understanding%2520%2528NLU%2529%2520and%2520Language%2520Modeling%250Atasks%252C%2520despite%2520being%2520significantly%2520more%2520efficient%2520and%2520simpler.%2520We%2520also%2520show%250Athat%2520scaling%2520up%2520our%2520model%2520within%2520the%2520compute%2520budget%2520of%2520VALM%252C%2520either%2520increasing%250Athe%2520model%2520or%2520pre-training%2520corpus%2520size%252C%2520we%2520outperform%2520VALM%2520for%2520all%2520the%250Aevaluation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Efficiency%20of%20Visually%20Augmented%20Language%20Models&entry.906535625=Paula%20Ontalvilla%20and%20Aitor%20Ormazabal%20and%20Gorka%20Azkune&entry.1292438233=%20%20Despite%20the%20impressive%20performance%20of%20autoregressive%20Language%20Models%20%28LM%29%20it%0Ahas%20been%20shown%20that%20due%20to%20reporting%20bias%2C%20LMs%20lack%20visual%20knowledge%2C%20i.e.%20they%0Ado%20not%20know%20much%20about%20the%20visual%20world%20and%20its%20properties.%20To%20augment%20LMs%20with%0Avisual%20knowledge%2C%20existing%20solutions%20often%20rely%20on%20explicit%20images%2C%20requiring%0Atime-consuming%20retrieval%20or%20image%20generation%20systems.%20This%20paper%20shows%20that%0Aexplicit%20images%20are%20not%20necessary%20to%20visually%20augment%20an%20LM.%20Instead%2C%20we%20use%0Avisually-grounded%20text%20representations%20obtained%20from%20the%20well-known%20CLIP%0Amultimodal%20system.%20For%20a%20fair%20comparison%2C%20we%20modify%20VALM%2C%20a%20visually-augmented%0ALM%20which%20uses%20image%20retrieval%20and%20representation%2C%20to%20work%20directly%20with%0Avisually-grounded%20text%20representations.%20We%20name%20this%20new%20model%20BLIND-VALM.%20We%0Ashow%20that%20BLIND-VALM%20performs%20on%20par%20with%20VALM%20for%20Visual%20Language%0AUnderstanding%20%28VLU%29%2C%20Natural%20Language%20Understanding%20%28NLU%29%20and%20Language%20Modeling%0Atasks%2C%20despite%20being%20significantly%20more%20efficient%20and%20simpler.%20We%20also%20show%0Athat%20scaling%20up%20our%20model%20within%20the%20compute%20budget%20of%20VALM%2C%20either%20increasing%0Athe%20model%20or%20pre-training%20corpus%20size%2C%20we%20outperform%20VALM%20for%20all%20the%0Aevaluation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11148v1&entry.124074799=Read"},
{"title": "Learning Spatially-Aware Language and Audio Embedding", "author": "Bhavika Devnani and Skyler Seto and Zakaria Aldeneh and Alessandro Toso and Elena Menyaylenko and Barry-John Theobald and Jonathan Sheaffer and Miguel Sarabia", "abstract": "  Humans can picture a sound scene given an imprecise natural language\ndescription. For example, it is easy to imagine an acoustic environment given a\nphrase like \"the lion roar came from right behind me!\". For a machine to have\nthe same degree of comprehension, the machine must know what a lion is\n(semantic attribute), what the concept of \"behind\" is (spatial attribute) and\nhow these pieces of linguistic information align with the semantic and spatial\nattributes of the sound (what a roar sounds like when its coming from behind).\nState-of-the-art audio foundation models which learn to map between audio\nscenes and natural textual descriptions, are trained on non-spatial audio and\ntext pairs, and hence lack spatial awareness. In contrast, sound event\nlocalization and detection models are limited to recognizing sounds from a\nfixed number of classes, and they localize the source to absolute position\n(e.g., 0.2m) rather than a position described using natural language (e.g.,\n\"next to me\"). To address these gaps, we present ELSA a spatially aware-audio\nand text embedding model trained using multimodal contrastive learning. ELSA\nsupports non-spatial audio, spatial audio, and open vocabulary text captions\ndescribing both the spatial and semantic components of sound. To train ELSA:\n(a) we spatially augment the audio and captions of three open-source audio\ndatasets totaling 4,738 hours of audio, and (b) we design an encoder to capture\nthe semantics of non-spatial audio, and the semantics and spatial attributes of\nspatial audio using contrastive learning. ELSA is competitive with\nstate-of-the-art for both semantic retrieval and 3D source localization. In\nparticular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 above\nthe baseline, and outperforms by -11.6{\\deg} mean-absolute-error in 3D source\nlocalization over the baseline.\n", "link": "http://arxiv.org/abs/2409.11369v1", "date": "2024-09-17", "relevancy": 2.8641, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Spatially-Aware%20Language%20and%20Audio%20Embedding&body=Title%3A%20Learning%20Spatially-Aware%20Language%20and%20Audio%20Embedding%0AAuthor%3A%20Bhavika%20Devnani%20and%20Skyler%20Seto%20and%20Zakaria%20Aldeneh%20and%20Alessandro%20Toso%20and%20Elena%20Menyaylenko%20and%20Barry-John%20Theobald%20and%20Jonathan%20Sheaffer%20and%20Miguel%20Sarabia%0AAbstract%3A%20%20%20Humans%20can%20picture%20a%20sound%20scene%20given%20an%20imprecise%20natural%20language%0Adescription.%20For%20example%2C%20it%20is%20easy%20to%20imagine%20an%20acoustic%20environment%20given%20a%0Aphrase%20like%20%22the%20lion%20roar%20came%20from%20right%20behind%20me%21%22.%20For%20a%20machine%20to%20have%0Athe%20same%20degree%20of%20comprehension%2C%20the%20machine%20must%20know%20what%20a%20lion%20is%0A%28semantic%20attribute%29%2C%20what%20the%20concept%20of%20%22behind%22%20is%20%28spatial%20attribute%29%20and%0Ahow%20these%20pieces%20of%20linguistic%20information%20align%20with%20the%20semantic%20and%20spatial%0Aattributes%20of%20the%20sound%20%28what%20a%20roar%20sounds%20like%20when%20its%20coming%20from%20behind%29.%0AState-of-the-art%20audio%20foundation%20models%20which%20learn%20to%20map%20between%20audio%0Ascenes%20and%20natural%20textual%20descriptions%2C%20are%20trained%20on%20non-spatial%20audio%20and%0Atext%20pairs%2C%20and%20hence%20lack%20spatial%20awareness.%20In%20contrast%2C%20sound%20event%0Alocalization%20and%20detection%20models%20are%20limited%20to%20recognizing%20sounds%20from%20a%0Afixed%20number%20of%20classes%2C%20and%20they%20localize%20the%20source%20to%20absolute%20position%0A%28e.g.%2C%200.2m%29%20rather%20than%20a%20position%20described%20using%20natural%20language%20%28e.g.%2C%0A%22next%20to%20me%22%29.%20To%20address%20these%20gaps%2C%20we%20present%20ELSA%20a%20spatially%20aware-audio%0Aand%20text%20embedding%20model%20trained%20using%20multimodal%20contrastive%20learning.%20ELSA%0Asupports%20non-spatial%20audio%2C%20spatial%20audio%2C%20and%20open%20vocabulary%20text%20captions%0Adescribing%20both%20the%20spatial%20and%20semantic%20components%20of%20sound.%20To%20train%20ELSA%3A%0A%28a%29%20we%20spatially%20augment%20the%20audio%20and%20captions%20of%20three%20open-source%20audio%0Adatasets%20totaling%204%2C738%20hours%20of%20audio%2C%20and%20%28b%29%20we%20design%20an%20encoder%20to%20capture%0Athe%20semantics%20of%20non-spatial%20audio%2C%20and%20the%20semantics%20and%20spatial%20attributes%20of%0Aspatial%20audio%20using%20contrastive%20learning.%20ELSA%20is%20competitive%20with%0Astate-of-the-art%20for%20both%20semantic%20retrieval%20and%203D%20source%20localization.%20In%0Aparticular%2C%20ELSA%20achieves%20%2B2.8%25%20mean%20audio-to-text%20and%20text-to-audio%20R%401%20above%0Athe%20baseline%2C%20and%20outperforms%20by%20-11.6%7B%5Cdeg%7D%20mean-absolute-error%20in%203D%20source%0Alocalization%20over%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Spatially-Aware%2520Language%2520and%2520Audio%2520Embedding%26entry.906535625%3DBhavika%2520Devnani%2520and%2520Skyler%2520Seto%2520and%2520Zakaria%2520Aldeneh%2520and%2520Alessandro%2520Toso%2520and%2520Elena%2520Menyaylenko%2520and%2520Barry-John%2520Theobald%2520and%2520Jonathan%2520Sheaffer%2520and%2520Miguel%2520Sarabia%26entry.1292438233%3D%2520%2520Humans%2520can%2520picture%2520a%2520sound%2520scene%2520given%2520an%2520imprecise%2520natural%2520language%250Adescription.%2520For%2520example%252C%2520it%2520is%2520easy%2520to%2520imagine%2520an%2520acoustic%2520environment%2520given%2520a%250Aphrase%2520like%2520%2522the%2520lion%2520roar%2520came%2520from%2520right%2520behind%2520me%2521%2522.%2520For%2520a%2520machine%2520to%2520have%250Athe%2520same%2520degree%2520of%2520comprehension%252C%2520the%2520machine%2520must%2520know%2520what%2520a%2520lion%2520is%250A%2528semantic%2520attribute%2529%252C%2520what%2520the%2520concept%2520of%2520%2522behind%2522%2520is%2520%2528spatial%2520attribute%2529%2520and%250Ahow%2520these%2520pieces%2520of%2520linguistic%2520information%2520align%2520with%2520the%2520semantic%2520and%2520spatial%250Aattributes%2520of%2520the%2520sound%2520%2528what%2520a%2520roar%2520sounds%2520like%2520when%2520its%2520coming%2520from%2520behind%2529.%250AState-of-the-art%2520audio%2520foundation%2520models%2520which%2520learn%2520to%2520map%2520between%2520audio%250Ascenes%2520and%2520natural%2520textual%2520descriptions%252C%2520are%2520trained%2520on%2520non-spatial%2520audio%2520and%250Atext%2520pairs%252C%2520and%2520hence%2520lack%2520spatial%2520awareness.%2520In%2520contrast%252C%2520sound%2520event%250Alocalization%2520and%2520detection%2520models%2520are%2520limited%2520to%2520recognizing%2520sounds%2520from%2520a%250Afixed%2520number%2520of%2520classes%252C%2520and%2520they%2520localize%2520the%2520source%2520to%2520absolute%2520position%250A%2528e.g.%252C%25200.2m%2529%2520rather%2520than%2520a%2520position%2520described%2520using%2520natural%2520language%2520%2528e.g.%252C%250A%2522next%2520to%2520me%2522%2529.%2520To%2520address%2520these%2520gaps%252C%2520we%2520present%2520ELSA%2520a%2520spatially%2520aware-audio%250Aand%2520text%2520embedding%2520model%2520trained%2520using%2520multimodal%2520contrastive%2520learning.%2520ELSA%250Asupports%2520non-spatial%2520audio%252C%2520spatial%2520audio%252C%2520and%2520open%2520vocabulary%2520text%2520captions%250Adescribing%2520both%2520the%2520spatial%2520and%2520semantic%2520components%2520of%2520sound.%2520To%2520train%2520ELSA%253A%250A%2528a%2529%2520we%2520spatially%2520augment%2520the%2520audio%2520and%2520captions%2520of%2520three%2520open-source%2520audio%250Adatasets%2520totaling%25204%252C738%2520hours%2520of%2520audio%252C%2520and%2520%2528b%2529%2520we%2520design%2520an%2520encoder%2520to%2520capture%250Athe%2520semantics%2520of%2520non-spatial%2520audio%252C%2520and%2520the%2520semantics%2520and%2520spatial%2520attributes%2520of%250Aspatial%2520audio%2520using%2520contrastive%2520learning.%2520ELSA%2520is%2520competitive%2520with%250Astate-of-the-art%2520for%2520both%2520semantic%2520retrieval%2520and%25203D%2520source%2520localization.%2520In%250Aparticular%252C%2520ELSA%2520achieves%2520%252B2.8%2525%2520mean%2520audio-to-text%2520and%2520text-to-audio%2520R%25401%2520above%250Athe%2520baseline%252C%2520and%2520outperforms%2520by%2520-11.6%257B%255Cdeg%257D%2520mean-absolute-error%2520in%25203D%2520source%250Alocalization%2520over%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Spatially-Aware%20Language%20and%20Audio%20Embedding&entry.906535625=Bhavika%20Devnani%20and%20Skyler%20Seto%20and%20Zakaria%20Aldeneh%20and%20Alessandro%20Toso%20and%20Elena%20Menyaylenko%20and%20Barry-John%20Theobald%20and%20Jonathan%20Sheaffer%20and%20Miguel%20Sarabia&entry.1292438233=%20%20Humans%20can%20picture%20a%20sound%20scene%20given%20an%20imprecise%20natural%20language%0Adescription.%20For%20example%2C%20it%20is%20easy%20to%20imagine%20an%20acoustic%20environment%20given%20a%0Aphrase%20like%20%22the%20lion%20roar%20came%20from%20right%20behind%20me%21%22.%20For%20a%20machine%20to%20have%0Athe%20same%20degree%20of%20comprehension%2C%20the%20machine%20must%20know%20what%20a%20lion%20is%0A%28semantic%20attribute%29%2C%20what%20the%20concept%20of%20%22behind%22%20is%20%28spatial%20attribute%29%20and%0Ahow%20these%20pieces%20of%20linguistic%20information%20align%20with%20the%20semantic%20and%20spatial%0Aattributes%20of%20the%20sound%20%28what%20a%20roar%20sounds%20like%20when%20its%20coming%20from%20behind%29.%0AState-of-the-art%20audio%20foundation%20models%20which%20learn%20to%20map%20between%20audio%0Ascenes%20and%20natural%20textual%20descriptions%2C%20are%20trained%20on%20non-spatial%20audio%20and%0Atext%20pairs%2C%20and%20hence%20lack%20spatial%20awareness.%20In%20contrast%2C%20sound%20event%0Alocalization%20and%20detection%20models%20are%20limited%20to%20recognizing%20sounds%20from%20a%0Afixed%20number%20of%20classes%2C%20and%20they%20localize%20the%20source%20to%20absolute%20position%0A%28e.g.%2C%200.2m%29%20rather%20than%20a%20position%20described%20using%20natural%20language%20%28e.g.%2C%0A%22next%20to%20me%22%29.%20To%20address%20these%20gaps%2C%20we%20present%20ELSA%20a%20spatially%20aware-audio%0Aand%20text%20embedding%20model%20trained%20using%20multimodal%20contrastive%20learning.%20ELSA%0Asupports%20non-spatial%20audio%2C%20spatial%20audio%2C%20and%20open%20vocabulary%20text%20captions%0Adescribing%20both%20the%20spatial%20and%20semantic%20components%20of%20sound.%20To%20train%20ELSA%3A%0A%28a%29%20we%20spatially%20augment%20the%20audio%20and%20captions%20of%20three%20open-source%20audio%0Adatasets%20totaling%204%2C738%20hours%20of%20audio%2C%20and%20%28b%29%20we%20design%20an%20encoder%20to%20capture%0Athe%20semantics%20of%20non-spatial%20audio%2C%20and%20the%20semantics%20and%20spatial%20attributes%20of%0Aspatial%20audio%20using%20contrastive%20learning.%20ELSA%20is%20competitive%20with%0Astate-of-the-art%20for%20both%20semantic%20retrieval%20and%203D%20source%20localization.%20In%0Aparticular%2C%20ELSA%20achieves%20%2B2.8%25%20mean%20audio-to-text%20and%20text-to-audio%20R%401%20above%0Athe%20baseline%2C%20and%20outperforms%20by%20-11.6%7B%5Cdeg%7D%20mean-absolute-error%20in%203D%20source%0Alocalization%20over%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11369v1&entry.124074799=Read"},
{"title": "NVLM: Open Frontier-Class Multimodal LLMs", "author": "Wenliang Dai and Nayeon Lee and Boxin Wang and Zhuoling Yang and Zihan Liu and Jon Barker and Tuomas Rintamaki and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping", "abstract": "  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we are releasing the model weights and will open-source the code for the\ncommunity: https://nvlm-project.github.io/.\n", "link": "http://arxiv.org/abs/2409.11402v1", "date": "2024-09-17", "relevancy": 2.8052, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NVLM%3A%20Open%20Frontier-Class%20Multimodal%20LLMs&body=Title%3A%20NVLM%3A%20Open%20Frontier-Class%20Multimodal%20LLMs%0AAuthor%3A%20Wenliang%20Dai%20and%20Nayeon%20Lee%20and%20Boxin%20Wang%20and%20Zhuoling%20Yang%20and%20Zihan%20Liu%20and%20Jon%20Barker%20and%20Tuomas%20Rintamaki%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%20and%20Wei%20Ping%0AAbstract%3A%20%20%20We%20introduce%20NVLM%201.0%2C%20a%20family%20of%20frontier-class%20multimodal%20large%20language%0Amodels%20%28LLMs%29%20that%20achieve%20state-of-the-art%20results%20on%20vision-language%20tasks%2C%0Arivaling%20the%20leading%20proprietary%20models%20%28e.g.%2C%20GPT-4o%29%20and%20open-access%20models%0A%28e.g.%2C%20Llama%203-V%20405B%20and%20InternVL%202%29.%20Remarkably%2C%20NVLM%201.0%20shows%20improved%0Atext-only%20performance%20over%20its%20LLM%20backbone%20after%20multimodal%20training.%20In%20terms%0Aof%20model%20design%2C%20we%20perform%20a%20comprehensive%20comparison%20between%20decoder-only%0Amultimodal%20LLMs%20%28e.g.%2C%20LLaVA%29%20and%20cross-attention-based%20models%20%28e.g.%2C%0AFlamingo%29.%20Based%20on%20the%20strengths%20and%20weaknesses%20of%20both%20approaches%2C%20we%20propose%0Aa%20novel%20architecture%20that%20enhances%20both%20training%20efficiency%20and%20multimodal%0Areasoning%20capabilities.%20Furthermore%2C%20we%20introduce%20a%201-D%20tile-tagging%20design%20for%0Atile-based%20dynamic%20high-resolution%20images%2C%20which%20significantly%20boosts%0Aperformance%20on%20multimodal%20reasoning%20and%20OCR-related%20tasks.%20Regarding%20training%0Adata%2C%20we%20meticulously%20curate%20and%20provide%20detailed%20information%20on%20our%20multimodal%0Apretraining%20and%20supervised%20fine-tuning%20datasets.%20Our%20findings%20indicate%20that%0Adataset%20quality%20and%20task%20diversity%20are%20more%20important%20than%20scale%2C%20even%20during%0Athe%20pretraining%20phase%2C%20across%20all%20architectures.%20Notably%2C%20we%20develop%0Aproduction-grade%20multimodality%20for%20the%20NVLM-1.0%20models%2C%20enabling%20them%20to%20excel%0Ain%20vision-language%20tasks%20while%20maintaining%20and%20even%20improving%20text-only%0Aperformance%20compared%20to%20their%20LLM%20backbones.%20To%20achieve%20this%2C%20we%20craft%20and%0Aintegrate%20a%20high-quality%20text-only%20dataset%20into%20multimodal%20training%2C%20alongside%0Aa%20substantial%20amount%20of%20multimodal%20math%20and%20reasoning%20data%2C%20leading%20to%20enhanced%0Amath%20and%20coding%20capabilities%20across%20modalities.%20To%20advance%20research%20in%20the%0Afield%2C%20we%20are%20releasing%20the%20model%20weights%20and%20will%20open-source%20the%20code%20for%20the%0Acommunity%3A%20https%3A//nvlm-project.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNVLM%253A%2520Open%2520Frontier-Class%2520Multimodal%2520LLMs%26entry.906535625%3DWenliang%2520Dai%2520and%2520Nayeon%2520Lee%2520and%2520Boxin%2520Wang%2520and%2520Zhuoling%2520Yang%2520and%2520Zihan%2520Liu%2520and%2520Jon%2520Barker%2520and%2520Tuomas%2520Rintamaki%2520and%2520Mohammad%2520Shoeybi%2520and%2520Bryan%2520Catanzaro%2520and%2520Wei%2520Ping%26entry.1292438233%3D%2520%2520We%2520introduce%2520NVLM%25201.0%252C%2520a%2520family%2520of%2520frontier-class%2520multimodal%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520that%2520achieve%2520state-of-the-art%2520results%2520on%2520vision-language%2520tasks%252C%250Arivaling%2520the%2520leading%2520proprietary%2520models%2520%2528e.g.%252C%2520GPT-4o%2529%2520and%2520open-access%2520models%250A%2528e.g.%252C%2520Llama%25203-V%2520405B%2520and%2520InternVL%25202%2529.%2520Remarkably%252C%2520NVLM%25201.0%2520shows%2520improved%250Atext-only%2520performance%2520over%2520its%2520LLM%2520backbone%2520after%2520multimodal%2520training.%2520In%2520terms%250Aof%2520model%2520design%252C%2520we%2520perform%2520a%2520comprehensive%2520comparison%2520between%2520decoder-only%250Amultimodal%2520LLMs%2520%2528e.g.%252C%2520LLaVA%2529%2520and%2520cross-attention-based%2520models%2520%2528e.g.%252C%250AFlamingo%2529.%2520Based%2520on%2520the%2520strengths%2520and%2520weaknesses%2520of%2520both%2520approaches%252C%2520we%2520propose%250Aa%2520novel%2520architecture%2520that%2520enhances%2520both%2520training%2520efficiency%2520and%2520multimodal%250Areasoning%2520capabilities.%2520Furthermore%252C%2520we%2520introduce%2520a%25201-D%2520tile-tagging%2520design%2520for%250Atile-based%2520dynamic%2520high-resolution%2520images%252C%2520which%2520significantly%2520boosts%250Aperformance%2520on%2520multimodal%2520reasoning%2520and%2520OCR-related%2520tasks.%2520Regarding%2520training%250Adata%252C%2520we%2520meticulously%2520curate%2520and%2520provide%2520detailed%2520information%2520on%2520our%2520multimodal%250Apretraining%2520and%2520supervised%2520fine-tuning%2520datasets.%2520Our%2520findings%2520indicate%2520that%250Adataset%2520quality%2520and%2520task%2520diversity%2520are%2520more%2520important%2520than%2520scale%252C%2520even%2520during%250Athe%2520pretraining%2520phase%252C%2520across%2520all%2520architectures.%2520Notably%252C%2520we%2520develop%250Aproduction-grade%2520multimodality%2520for%2520the%2520NVLM-1.0%2520models%252C%2520enabling%2520them%2520to%2520excel%250Ain%2520vision-language%2520tasks%2520while%2520maintaining%2520and%2520even%2520improving%2520text-only%250Aperformance%2520compared%2520to%2520their%2520LLM%2520backbones.%2520To%2520achieve%2520this%252C%2520we%2520craft%2520and%250Aintegrate%2520a%2520high-quality%2520text-only%2520dataset%2520into%2520multimodal%2520training%252C%2520alongside%250Aa%2520substantial%2520amount%2520of%2520multimodal%2520math%2520and%2520reasoning%2520data%252C%2520leading%2520to%2520enhanced%250Amath%2520and%2520coding%2520capabilities%2520across%2520modalities.%2520To%2520advance%2520research%2520in%2520the%250Afield%252C%2520we%2520are%2520releasing%2520the%2520model%2520weights%2520and%2520will%2520open-source%2520the%2520code%2520for%2520the%250Acommunity%253A%2520https%253A//nvlm-project.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NVLM%3A%20Open%20Frontier-Class%20Multimodal%20LLMs&entry.906535625=Wenliang%20Dai%20and%20Nayeon%20Lee%20and%20Boxin%20Wang%20and%20Zhuoling%20Yang%20and%20Zihan%20Liu%20and%20Jon%20Barker%20and%20Tuomas%20Rintamaki%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%20and%20Wei%20Ping&entry.1292438233=%20%20We%20introduce%20NVLM%201.0%2C%20a%20family%20of%20frontier-class%20multimodal%20large%20language%0Amodels%20%28LLMs%29%20that%20achieve%20state-of-the-art%20results%20on%20vision-language%20tasks%2C%0Arivaling%20the%20leading%20proprietary%20models%20%28e.g.%2C%20GPT-4o%29%20and%20open-access%20models%0A%28e.g.%2C%20Llama%203-V%20405B%20and%20InternVL%202%29.%20Remarkably%2C%20NVLM%201.0%20shows%20improved%0Atext-only%20performance%20over%20its%20LLM%20backbone%20after%20multimodal%20training.%20In%20terms%0Aof%20model%20design%2C%20we%20perform%20a%20comprehensive%20comparison%20between%20decoder-only%0Amultimodal%20LLMs%20%28e.g.%2C%20LLaVA%29%20and%20cross-attention-based%20models%20%28e.g.%2C%0AFlamingo%29.%20Based%20on%20the%20strengths%20and%20weaknesses%20of%20both%20approaches%2C%20we%20propose%0Aa%20novel%20architecture%20that%20enhances%20both%20training%20efficiency%20and%20multimodal%0Areasoning%20capabilities.%20Furthermore%2C%20we%20introduce%20a%201-D%20tile-tagging%20design%20for%0Atile-based%20dynamic%20high-resolution%20images%2C%20which%20significantly%20boosts%0Aperformance%20on%20multimodal%20reasoning%20and%20OCR-related%20tasks.%20Regarding%20training%0Adata%2C%20we%20meticulously%20curate%20and%20provide%20detailed%20information%20on%20our%20multimodal%0Apretraining%20and%20supervised%20fine-tuning%20datasets.%20Our%20findings%20indicate%20that%0Adataset%20quality%20and%20task%20diversity%20are%20more%20important%20than%20scale%2C%20even%20during%0Athe%20pretraining%20phase%2C%20across%20all%20architectures.%20Notably%2C%20we%20develop%0Aproduction-grade%20multimodality%20for%20the%20NVLM-1.0%20models%2C%20enabling%20them%20to%20excel%0Ain%20vision-language%20tasks%20while%20maintaining%20and%20even%20improving%20text-only%0Aperformance%20compared%20to%20their%20LLM%20backbones.%20To%20achieve%20this%2C%20we%20craft%20and%0Aintegrate%20a%20high-quality%20text-only%20dataset%20into%20multimodal%20training%2C%20alongside%0Aa%20substantial%20amount%20of%20multimodal%20math%20and%20reasoning%20data%2C%20leading%20to%20enhanced%0Amath%20and%20coding%20capabilities%20across%20modalities.%20To%20advance%20research%20in%20the%0Afield%2C%20we%20are%20releasing%20the%20model%20weights%20and%20will%20open-source%20the%20code%20for%20the%0Acommunity%3A%20https%3A//nvlm-project.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11402v1&entry.124074799=Read"},
{"title": "RenderWorld: World Model with Self-Supervised 3D Label", "author": "Ziyang Yan and Wenzhen Dong and Yihua Shao and Yuhang Lu and Liu Haiyang and Jingwen Liu and Haozhe Wang and Zhe Wang and Yan Wang and Fabio Remondino and Yuexin Ma", "abstract": "  End-to-end autonomous driving with vision-only is not only more\ncost-effective compared to LiDAR-vision fusion but also more reliable than\ntraditional methods. To achieve a economical and robust purely visual\nautonomous driving system, we propose RenderWorld, a vision-only end-to-end\nautonomous driving framework, which generates 3D occupancy labels using a\nself-supervised gaussian-based Img2Occ Module, then encodes the labels by\nAM-VAE, and uses world model for forecasting and planning. RenderWorld employs\nGaussian Splatting to represent 3D scenes and render 2D images greatly improves\nsegmentation accuracy and reduces GPU memory consumption compared with\nNeRF-based methods. By applying AM-VAE to encode air and non-air separately,\nRenderWorld achieves more fine-grained scene element representation, leading to\nstate-of-the-art performance in both 4D occupancy forecasting and motion\nplanning from autoregressive world model.\n", "link": "http://arxiv.org/abs/2409.11356v1", "date": "2024-09-17", "relevancy": 2.7817, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5748}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RenderWorld%3A%20World%20Model%20with%20Self-Supervised%203D%20Label&body=Title%3A%20RenderWorld%3A%20World%20Model%20with%20Self-Supervised%203D%20Label%0AAuthor%3A%20Ziyang%20Yan%20and%20Wenzhen%20Dong%20and%20Yihua%20Shao%20and%20Yuhang%20Lu%20and%20Liu%20Haiyang%20and%20Jingwen%20Liu%20and%20Haozhe%20Wang%20and%20Zhe%20Wang%20and%20Yan%20Wang%20and%20Fabio%20Remondino%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20with%20vision-only%20is%20not%20only%20more%0Acost-effective%20compared%20to%20LiDAR-vision%20fusion%20but%20also%20more%20reliable%20than%0Atraditional%20methods.%20To%20achieve%20a%20economical%20and%20robust%20purely%20visual%0Aautonomous%20driving%20system%2C%20we%20propose%20RenderWorld%2C%20a%20vision-only%20end-to-end%0Aautonomous%20driving%20framework%2C%20which%20generates%203D%20occupancy%20labels%20using%20a%0Aself-supervised%20gaussian-based%20Img2Occ%20Module%2C%20then%20encodes%20the%20labels%20by%0AAM-VAE%2C%20and%20uses%20world%20model%20for%20forecasting%20and%20planning.%20RenderWorld%20employs%0AGaussian%20Splatting%20to%20represent%203D%20scenes%20and%20render%202D%20images%20greatly%20improves%0Asegmentation%20accuracy%20and%20reduces%20GPU%20memory%20consumption%20compared%20with%0ANeRF-based%20methods.%20By%20applying%20AM-VAE%20to%20encode%20air%20and%20non-air%20separately%2C%0ARenderWorld%20achieves%20more%20fine-grained%20scene%20element%20representation%2C%20leading%20to%0Astate-of-the-art%20performance%20in%20both%204D%20occupancy%20forecasting%20and%20motion%0Aplanning%20from%20autoregressive%20world%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRenderWorld%253A%2520World%2520Model%2520with%2520Self-Supervised%25203D%2520Label%26entry.906535625%3DZiyang%2520Yan%2520and%2520Wenzhen%2520Dong%2520and%2520Yihua%2520Shao%2520and%2520Yuhang%2520Lu%2520and%2520Liu%2520Haiyang%2520and%2520Jingwen%2520Liu%2520and%2520Haozhe%2520Wang%2520and%2520Zhe%2520Wang%2520and%2520Yan%2520Wang%2520and%2520Fabio%2520Remondino%2520and%2520Yuexin%2520Ma%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520with%2520vision-only%2520is%2520not%2520only%2520more%250Acost-effective%2520compared%2520to%2520LiDAR-vision%2520fusion%2520but%2520also%2520more%2520reliable%2520than%250Atraditional%2520methods.%2520To%2520achieve%2520a%2520economical%2520and%2520robust%2520purely%2520visual%250Aautonomous%2520driving%2520system%252C%2520we%2520propose%2520RenderWorld%252C%2520a%2520vision-only%2520end-to-end%250Aautonomous%2520driving%2520framework%252C%2520which%2520generates%25203D%2520occupancy%2520labels%2520using%2520a%250Aself-supervised%2520gaussian-based%2520Img2Occ%2520Module%252C%2520then%2520encodes%2520the%2520labels%2520by%250AAM-VAE%252C%2520and%2520uses%2520world%2520model%2520for%2520forecasting%2520and%2520planning.%2520RenderWorld%2520employs%250AGaussian%2520Splatting%2520to%2520represent%25203D%2520scenes%2520and%2520render%25202D%2520images%2520greatly%2520improves%250Asegmentation%2520accuracy%2520and%2520reduces%2520GPU%2520memory%2520consumption%2520compared%2520with%250ANeRF-based%2520methods.%2520By%2520applying%2520AM-VAE%2520to%2520encode%2520air%2520and%2520non-air%2520separately%252C%250ARenderWorld%2520achieves%2520more%2520fine-grained%2520scene%2520element%2520representation%252C%2520leading%2520to%250Astate-of-the-art%2520performance%2520in%2520both%25204D%2520occupancy%2520forecasting%2520and%2520motion%250Aplanning%2520from%2520autoregressive%2520world%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RenderWorld%3A%20World%20Model%20with%20Self-Supervised%203D%20Label&entry.906535625=Ziyang%20Yan%20and%20Wenzhen%20Dong%20and%20Yihua%20Shao%20and%20Yuhang%20Lu%20and%20Liu%20Haiyang%20and%20Jingwen%20Liu%20and%20Haozhe%20Wang%20and%20Zhe%20Wang%20and%20Yan%20Wang%20and%20Fabio%20Remondino%20and%20Yuexin%20Ma&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20with%20vision-only%20is%20not%20only%20more%0Acost-effective%20compared%20to%20LiDAR-vision%20fusion%20but%20also%20more%20reliable%20than%0Atraditional%20methods.%20To%20achieve%20a%20economical%20and%20robust%20purely%20visual%0Aautonomous%20driving%20system%2C%20we%20propose%20RenderWorld%2C%20a%20vision-only%20end-to-end%0Aautonomous%20driving%20framework%2C%20which%20generates%203D%20occupancy%20labels%20using%20a%0Aself-supervised%20gaussian-based%20Img2Occ%20Module%2C%20then%20encodes%20the%20labels%20by%0AAM-VAE%2C%20and%20uses%20world%20model%20for%20forecasting%20and%20planning.%20RenderWorld%20employs%0AGaussian%20Splatting%20to%20represent%203D%20scenes%20and%20render%202D%20images%20greatly%20improves%0Asegmentation%20accuracy%20and%20reduces%20GPU%20memory%20consumption%20compared%20with%0ANeRF-based%20methods.%20By%20applying%20AM-VAE%20to%20encode%20air%20and%20non-air%20separately%2C%0ARenderWorld%20achieves%20more%20fine-grained%20scene%20element%20representation%2C%20leading%20to%0Astate-of-the-art%20performance%20in%20both%204D%20occupancy%20forecasting%20and%20motion%0Aplanning%20from%20autoregressive%20world%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11356v1&entry.124074799=Read"},
{"title": "Towards flexible perception with visual memory", "author": "Robert Geirhos and Priyank Jaini and Austin Stone and Sourabh Medapati and Xi Yi and George Toderici and Abhijit Ogale and Jonathon Shlens", "abstract": "  Training a neural network is a monolithic endeavor, akin to carving knowledge\ninto stone: once the process is completed, editing the knowledge in a network\nis nearly impossible, since all information is distributed across the network's\nweights. We here explore a simple, compelling alternative by marrying the\nrepresentational power of deep neural networks with the flexibility of a\ndatabase. Decomposing the task of image classification into image similarity\n(from a pre-trained embedding) and search (via fast nearest neighbor retrieval\nfrom a knowledge database), we build a simple and flexible visual memory that\nhas the following key capabilities: (1.) The ability to flexibly add data\nacross scales: from individual samples all the way to entire classes and\nbillion-scale data; (2.) The ability to remove data through unlearning and\nmemory pruning; (3.) An interpretable decision-mechanism on which we can\nintervene to control its behavior. Taken together, these capabilities\ncomprehensively demonstrate the benefits of an explicit visual memory. We hope\nthat it might contribute to a conversation on how knowledge should be\nrepresented in deep vision models -- beyond carving it in \"stone\" weights.\n", "link": "http://arxiv.org/abs/2408.08172v2", "date": "2024-09-17", "relevancy": 2.7646, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5822}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20flexible%20perception%20with%20visual%20memory&body=Title%3A%20Towards%20flexible%20perception%20with%20visual%20memory%0AAuthor%3A%20Robert%20Geirhos%20and%20Priyank%20Jaini%20and%20Austin%20Stone%20and%20Sourabh%20Medapati%20and%20Xi%20Yi%20and%20George%20Toderici%20and%20Abhijit%20Ogale%20and%20Jonathon%20Shlens%0AAbstract%3A%20%20%20Training%20a%20neural%20network%20is%20a%20monolithic%20endeavor%2C%20akin%20to%20carving%20knowledge%0Ainto%20stone%3A%20once%20the%20process%20is%20completed%2C%20editing%20the%20knowledge%20in%20a%20network%0Ais%20nearly%20impossible%2C%20since%20all%20information%20is%20distributed%20across%20the%20network%27s%0Aweights.%20We%20here%20explore%20a%20simple%2C%20compelling%20alternative%20by%20marrying%20the%0Arepresentational%20power%20of%20deep%20neural%20networks%20with%20the%20flexibility%20of%20a%0Adatabase.%20Decomposing%20the%20task%20of%20image%20classification%20into%20image%20similarity%0A%28from%20a%20pre-trained%20embedding%29%20and%20search%20%28via%20fast%20nearest%20neighbor%20retrieval%0Afrom%20a%20knowledge%20database%29%2C%20we%20build%20a%20simple%20and%20flexible%20visual%20memory%20that%0Ahas%20the%20following%20key%20capabilities%3A%20%281.%29%20The%20ability%20to%20flexibly%20add%20data%0Aacross%20scales%3A%20from%20individual%20samples%20all%20the%20way%20to%20entire%20classes%20and%0Abillion-scale%20data%3B%20%282.%29%20The%20ability%20to%20remove%20data%20through%20unlearning%20and%0Amemory%20pruning%3B%20%283.%29%20An%20interpretable%20decision-mechanism%20on%20which%20we%20can%0Aintervene%20to%20control%20its%20behavior.%20Taken%20together%2C%20these%20capabilities%0Acomprehensively%20demonstrate%20the%20benefits%20of%20an%20explicit%20visual%20memory.%20We%20hope%0Athat%20it%20might%20contribute%20to%20a%20conversation%20on%20how%20knowledge%20should%20be%0Arepresented%20in%20deep%20vision%20models%20--%20beyond%20carving%20it%20in%20%22stone%22%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520flexible%2520perception%2520with%2520visual%2520memory%26entry.906535625%3DRobert%2520Geirhos%2520and%2520Priyank%2520Jaini%2520and%2520Austin%2520Stone%2520and%2520Sourabh%2520Medapati%2520and%2520Xi%2520Yi%2520and%2520George%2520Toderici%2520and%2520Abhijit%2520Ogale%2520and%2520Jonathon%2520Shlens%26entry.1292438233%3D%2520%2520Training%2520a%2520neural%2520network%2520is%2520a%2520monolithic%2520endeavor%252C%2520akin%2520to%2520carving%2520knowledge%250Ainto%2520stone%253A%2520once%2520the%2520process%2520is%2520completed%252C%2520editing%2520the%2520knowledge%2520in%2520a%2520network%250Ais%2520nearly%2520impossible%252C%2520since%2520all%2520information%2520is%2520distributed%2520across%2520the%2520network%2527s%250Aweights.%2520We%2520here%2520explore%2520a%2520simple%252C%2520compelling%2520alternative%2520by%2520marrying%2520the%250Arepresentational%2520power%2520of%2520deep%2520neural%2520networks%2520with%2520the%2520flexibility%2520of%2520a%250Adatabase.%2520Decomposing%2520the%2520task%2520of%2520image%2520classification%2520into%2520image%2520similarity%250A%2528from%2520a%2520pre-trained%2520embedding%2529%2520and%2520search%2520%2528via%2520fast%2520nearest%2520neighbor%2520retrieval%250Afrom%2520a%2520knowledge%2520database%2529%252C%2520we%2520build%2520a%2520simple%2520and%2520flexible%2520visual%2520memory%2520that%250Ahas%2520the%2520following%2520key%2520capabilities%253A%2520%25281.%2529%2520The%2520ability%2520to%2520flexibly%2520add%2520data%250Aacross%2520scales%253A%2520from%2520individual%2520samples%2520all%2520the%2520way%2520to%2520entire%2520classes%2520and%250Abillion-scale%2520data%253B%2520%25282.%2529%2520The%2520ability%2520to%2520remove%2520data%2520through%2520unlearning%2520and%250Amemory%2520pruning%253B%2520%25283.%2529%2520An%2520interpretable%2520decision-mechanism%2520on%2520which%2520we%2520can%250Aintervene%2520to%2520control%2520its%2520behavior.%2520Taken%2520together%252C%2520these%2520capabilities%250Acomprehensively%2520demonstrate%2520the%2520benefits%2520of%2520an%2520explicit%2520visual%2520memory.%2520We%2520hope%250Athat%2520it%2520might%2520contribute%2520to%2520a%2520conversation%2520on%2520how%2520knowledge%2520should%2520be%250Arepresented%2520in%2520deep%2520vision%2520models%2520--%2520beyond%2520carving%2520it%2520in%2520%2522stone%2522%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20flexible%20perception%20with%20visual%20memory&entry.906535625=Robert%20Geirhos%20and%20Priyank%20Jaini%20and%20Austin%20Stone%20and%20Sourabh%20Medapati%20and%20Xi%20Yi%20and%20George%20Toderici%20and%20Abhijit%20Ogale%20and%20Jonathon%20Shlens&entry.1292438233=%20%20Training%20a%20neural%20network%20is%20a%20monolithic%20endeavor%2C%20akin%20to%20carving%20knowledge%0Ainto%20stone%3A%20once%20the%20process%20is%20completed%2C%20editing%20the%20knowledge%20in%20a%20network%0Ais%20nearly%20impossible%2C%20since%20all%20information%20is%20distributed%20across%20the%20network%27s%0Aweights.%20We%20here%20explore%20a%20simple%2C%20compelling%20alternative%20by%20marrying%20the%0Arepresentational%20power%20of%20deep%20neural%20networks%20with%20the%20flexibility%20of%20a%0Adatabase.%20Decomposing%20the%20task%20of%20image%20classification%20into%20image%20similarity%0A%28from%20a%20pre-trained%20embedding%29%20and%20search%20%28via%20fast%20nearest%20neighbor%20retrieval%0Afrom%20a%20knowledge%20database%29%2C%20we%20build%20a%20simple%20and%20flexible%20visual%20memory%20that%0Ahas%20the%20following%20key%20capabilities%3A%20%281.%29%20The%20ability%20to%20flexibly%20add%20data%0Aacross%20scales%3A%20from%20individual%20samples%20all%20the%20way%20to%20entire%20classes%20and%0Abillion-scale%20data%3B%20%282.%29%20The%20ability%20to%20remove%20data%20through%20unlearning%20and%0Amemory%20pruning%3B%20%283.%29%20An%20interpretable%20decision-mechanism%20on%20which%20we%20can%0Aintervene%20to%20control%20its%20behavior.%20Taken%20together%2C%20these%20capabilities%0Acomprehensively%20demonstrate%20the%20benefits%20of%20an%20explicit%20visual%20memory.%20We%20hope%0Athat%20it%20might%20contribute%20to%20a%20conversation%20on%20how%20knowledge%20should%20be%0Arepresented%20in%20deep%20vision%20models%20--%20beyond%20carving%20it%20in%20%22stone%22%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08172v2&entry.124074799=Read"},
{"title": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with\n  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease\n  Classification", "author": "Fatema-E- Jannat and Sina Gholami and Jennifer I. Lim and Theodore Leng and Minhaj Nur Alam and Hamed Tabkhi", "abstract": "  In the medical domain, acquiring large datasets poses significant challenges\ndue to privacy concerns. Nonetheless, the development of a robust deep-learning\nmodel for retinal disease diagnosis necessitates a substantial dataset for\ntraining. The capacity to generalize effectively on smaller datasets remains a\npersistent challenge. The scarcity of data presents a significant barrier to\nthe practical implementation of scalable medical AI solutions. To address this\nissue, we've combined a wide range of data sources to improve performance and\ngeneralization to new data by giving it a deeper understanding of the data\nrepresentation from multi-modal datasets and developed a self-supervised\nframework based on large language models (LLMs), SwinV2 to gain a deeper\nunderstanding of multi-modal dataset representations, enhancing the model's\nability to extrapolate to new data for the detection of eye diseases using\noptical coherence tomography (OCT) images. We adopt a two-phase training\nmethodology, self-supervised pre-training, and fine-tuning on a downstream\nsupervised classifier. An ablation study conducted across three datasets\nemploying various encoder backbones, without data fusion, with low data\navailability setting, and without self-supervised pre-training scenarios,\nhighlights the robustness of our method. Our findings demonstrate consistent\nperformance across these diverse conditions, showcasing superior generalization\ncapabilities compared to the baseline model, ResNet-50.\n", "link": "http://arxiv.org/abs/2409.11375v1", "date": "2024-09-17", "relevancy": 2.718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5388}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-OCT-SelfNet%3A%20Integrating%20Self-Supervised%20Learning%20with%0A%20%20Multi-Source%20Data%20Fusion%20for%20Enhanced%20Multi-Class%20Retinal%20Disease%0A%20%20Classification&body=Title%3A%20Multi-OCT-SelfNet%3A%20Integrating%20Self-Supervised%20Learning%20with%0A%20%20Multi-Source%20Data%20Fusion%20for%20Enhanced%20Multi-Class%20Retinal%20Disease%0A%20%20Classification%0AAuthor%3A%20Fatema-E-%20Jannat%20and%20Sina%20Gholami%20and%20Jennifer%20I.%20Lim%20and%20Theodore%20Leng%20and%20Minhaj%20Nur%20Alam%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20In%20the%20medical%20domain%2C%20acquiring%20large%20datasets%20poses%20significant%20challenges%0Adue%20to%20privacy%20concerns.%20Nonetheless%2C%20the%20development%20of%20a%20robust%20deep-learning%0Amodel%20for%20retinal%20disease%20diagnosis%20necessitates%20a%20substantial%20dataset%20for%0Atraining.%20The%20capacity%20to%20generalize%20effectively%20on%20smaller%20datasets%20remains%20a%0Apersistent%20challenge.%20The%20scarcity%20of%20data%20presents%20a%20significant%20barrier%20to%0Athe%20practical%20implementation%20of%20scalable%20medical%20AI%20solutions.%20To%20address%20this%0Aissue%2C%20we%27ve%20combined%20a%20wide%20range%20of%20data%20sources%20to%20improve%20performance%20and%0Ageneralization%20to%20new%20data%20by%20giving%20it%20a%20deeper%20understanding%20of%20the%20data%0Arepresentation%20from%20multi-modal%20datasets%20and%20developed%20a%20self-supervised%0Aframework%20based%20on%20large%20language%20models%20%28LLMs%29%2C%20SwinV2%20to%20gain%20a%20deeper%0Aunderstanding%20of%20multi-modal%20dataset%20representations%2C%20enhancing%20the%20model%27s%0Aability%20to%20extrapolate%20to%20new%20data%20for%20the%20detection%20of%20eye%20diseases%20using%0Aoptical%20coherence%20tomography%20%28OCT%29%20images.%20We%20adopt%20a%20two-phase%20training%0Amethodology%2C%20self-supervised%20pre-training%2C%20and%20fine-tuning%20on%20a%20downstream%0Asupervised%20classifier.%20An%20ablation%20study%20conducted%20across%20three%20datasets%0Aemploying%20various%20encoder%20backbones%2C%20without%20data%20fusion%2C%20with%20low%20data%0Aavailability%20setting%2C%20and%20without%20self-supervised%20pre-training%20scenarios%2C%0Ahighlights%20the%20robustness%20of%20our%20method.%20Our%20findings%20demonstrate%20consistent%0Aperformance%20across%20these%20diverse%20conditions%2C%20showcasing%20superior%20generalization%0Acapabilities%20compared%20to%20the%20baseline%20model%2C%20ResNet-50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-OCT-SelfNet%253A%2520Integrating%2520Self-Supervised%2520Learning%2520with%250A%2520%2520Multi-Source%2520Data%2520Fusion%2520for%2520Enhanced%2520Multi-Class%2520Retinal%2520Disease%250A%2520%2520Classification%26entry.906535625%3DFatema-E-%2520Jannat%2520and%2520Sina%2520Gholami%2520and%2520Jennifer%2520I.%2520Lim%2520and%2520Theodore%2520Leng%2520and%2520Minhaj%2520Nur%2520Alam%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520In%2520the%2520medical%2520domain%252C%2520acquiring%2520large%2520datasets%2520poses%2520significant%2520challenges%250Adue%2520to%2520privacy%2520concerns.%2520Nonetheless%252C%2520the%2520development%2520of%2520a%2520robust%2520deep-learning%250Amodel%2520for%2520retinal%2520disease%2520diagnosis%2520necessitates%2520a%2520substantial%2520dataset%2520for%250Atraining.%2520The%2520capacity%2520to%2520generalize%2520effectively%2520on%2520smaller%2520datasets%2520remains%2520a%250Apersistent%2520challenge.%2520The%2520scarcity%2520of%2520data%2520presents%2520a%2520significant%2520barrier%2520to%250Athe%2520practical%2520implementation%2520of%2520scalable%2520medical%2520AI%2520solutions.%2520To%2520address%2520this%250Aissue%252C%2520we%2527ve%2520combined%2520a%2520wide%2520range%2520of%2520data%2520sources%2520to%2520improve%2520performance%2520and%250Ageneralization%2520to%2520new%2520data%2520by%2520giving%2520it%2520a%2520deeper%2520understanding%2520of%2520the%2520data%250Arepresentation%2520from%2520multi-modal%2520datasets%2520and%2520developed%2520a%2520self-supervised%250Aframework%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520SwinV2%2520to%2520gain%2520a%2520deeper%250Aunderstanding%2520of%2520multi-modal%2520dataset%2520representations%252C%2520enhancing%2520the%2520model%2527s%250Aability%2520to%2520extrapolate%2520to%2520new%2520data%2520for%2520the%2520detection%2520of%2520eye%2520diseases%2520using%250Aoptical%2520coherence%2520tomography%2520%2528OCT%2529%2520images.%2520We%2520adopt%2520a%2520two-phase%2520training%250Amethodology%252C%2520self-supervised%2520pre-training%252C%2520and%2520fine-tuning%2520on%2520a%2520downstream%250Asupervised%2520classifier.%2520An%2520ablation%2520study%2520conducted%2520across%2520three%2520datasets%250Aemploying%2520various%2520encoder%2520backbones%252C%2520without%2520data%2520fusion%252C%2520with%2520low%2520data%250Aavailability%2520setting%252C%2520and%2520without%2520self-supervised%2520pre-training%2520scenarios%252C%250Ahighlights%2520the%2520robustness%2520of%2520our%2520method.%2520Our%2520findings%2520demonstrate%2520consistent%250Aperformance%2520across%2520these%2520diverse%2520conditions%252C%2520showcasing%2520superior%2520generalization%250Acapabilities%2520compared%2520to%2520the%2520baseline%2520model%252C%2520ResNet-50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-OCT-SelfNet%3A%20Integrating%20Self-Supervised%20Learning%20with%0A%20%20Multi-Source%20Data%20Fusion%20for%20Enhanced%20Multi-Class%20Retinal%20Disease%0A%20%20Classification&entry.906535625=Fatema-E-%20Jannat%20and%20Sina%20Gholami%20and%20Jennifer%20I.%20Lim%20and%20Theodore%20Leng%20and%20Minhaj%20Nur%20Alam%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20In%20the%20medical%20domain%2C%20acquiring%20large%20datasets%20poses%20significant%20challenges%0Adue%20to%20privacy%20concerns.%20Nonetheless%2C%20the%20development%20of%20a%20robust%20deep-learning%0Amodel%20for%20retinal%20disease%20diagnosis%20necessitates%20a%20substantial%20dataset%20for%0Atraining.%20The%20capacity%20to%20generalize%20effectively%20on%20smaller%20datasets%20remains%20a%0Apersistent%20challenge.%20The%20scarcity%20of%20data%20presents%20a%20significant%20barrier%20to%0Athe%20practical%20implementation%20of%20scalable%20medical%20AI%20solutions.%20To%20address%20this%0Aissue%2C%20we%27ve%20combined%20a%20wide%20range%20of%20data%20sources%20to%20improve%20performance%20and%0Ageneralization%20to%20new%20data%20by%20giving%20it%20a%20deeper%20understanding%20of%20the%20data%0Arepresentation%20from%20multi-modal%20datasets%20and%20developed%20a%20self-supervised%0Aframework%20based%20on%20large%20language%20models%20%28LLMs%29%2C%20SwinV2%20to%20gain%20a%20deeper%0Aunderstanding%20of%20multi-modal%20dataset%20representations%2C%20enhancing%20the%20model%27s%0Aability%20to%20extrapolate%20to%20new%20data%20for%20the%20detection%20of%20eye%20diseases%20using%0Aoptical%20coherence%20tomography%20%28OCT%29%20images.%20We%20adopt%20a%20two-phase%20training%0Amethodology%2C%20self-supervised%20pre-training%2C%20and%20fine-tuning%20on%20a%20downstream%0Asupervised%20classifier.%20An%20ablation%20study%20conducted%20across%20three%20datasets%0Aemploying%20various%20encoder%20backbones%2C%20without%20data%20fusion%2C%20with%20low%20data%0Aavailability%20setting%2C%20and%20without%20self-supervised%20pre-training%20scenarios%2C%0Ahighlights%20the%20robustness%20of%20our%20method.%20Our%20findings%20demonstrate%20consistent%0Aperformance%20across%20these%20diverse%20conditions%2C%20showcasing%20superior%20generalization%0Acapabilities%20compared%20to%20the%20baseline%20model%2C%20ResNet-50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11375v1&entry.124074799=Read"},
{"title": "Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation", "author": "J\u00e9r\u00e9my Lebreton and Ingo Ahrns and Roland Brochard and Christoph Haskamp and Matthieu Le Goff and Nicolas Menga and Nicolas Ollagnier and Ralf Regele and Francesco Capolupo and Massimo Casasco", "abstract": "  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n", "link": "http://arxiv.org/abs/2409.11383v1", "date": "2024-09-17", "relevancy": 2.71, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Datasets%20Generation%20for%20Machine%20Learning%3A%20Application%20to%20Vision%0A%20%20Based%20Navigation&body=Title%3A%20Training%20Datasets%20Generation%20for%20Machine%20Learning%3A%20Application%20to%20Vision%0A%20%20Based%20Navigation%0AAuthor%3A%20J%C3%A9r%C3%A9my%20Lebreton%20and%20Ingo%20Ahrns%20and%20Roland%20Brochard%20and%20Christoph%20Haskamp%20and%20Matthieu%20Le%20Goff%20and%20Nicolas%20Menga%20and%20Nicolas%20Ollagnier%20and%20Ralf%20Regele%20and%20Francesco%20Capolupo%20and%20Massimo%20Casasco%0AAbstract%3A%20%20%20Vision%20Based%20Navigation%20consists%20in%20utilizing%20cameras%20as%20precision%20sensors%0Afor%20GNC%20after%20extracting%20information%20from%20images.%20To%20enable%20the%20adoption%20of%0Amachine%20learning%20for%20space%20applications%2C%20one%20of%20obstacles%20is%20the%20demonstration%0Athat%20available%20training%20datasets%20are%20adequate%20to%20validate%20the%20algorithms.%20The%0Aobjective%20of%20the%20study%20is%20to%20generate%20datasets%20of%20images%20and%20metadata%20suitable%0Afor%20training%20machine%20learning%20algorithms.%20Two%20use%20cases%20were%20selected%20and%20a%0Arobust%20methodology%20was%20developed%20to%20validate%20the%20datasets%20including%20the%20ground%0Atruth.%20The%20first%20use%20case%20is%20in-orbit%20rendezvous%20with%20a%20man-made%20object%3A%20a%0Amockup%20of%20satellite%20ENVISAT.%20The%20second%20use%20case%20is%20a%20Lunar%20landing%20scenario.%0ADatasets%20were%20produced%20from%20archival%20datasets%20%28Chang%27e%203%29%2C%20from%20the%20laboratory%0Aat%20DLR%20TRON%20facility%20and%20at%20Airbus%20Robotic%20laboratory%2C%20from%20SurRender%20software%0Ahigh%20fidelity%20image%20simulator%20using%20Model%20Capture%20and%20from%20Generative%0AAdversarial%20Networks.%20The%20use%20case%20definition%20included%20the%20selection%20of%0Aalgorithms%20as%20benchmark%3A%20an%20AI-based%20pose%20estimation%20algorithm%20and%20a%20dense%0Aoptical%20flow%20algorithm%20were%20selected.%20Eventually%20it%20is%20demonstrated%20that%0Adatasets%20produced%20with%20SurRender%20and%20selected%20laboratory%20facilities%20are%0Aadequate%20to%20train%20machine%20learning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Datasets%2520Generation%2520for%2520Machine%2520Learning%253A%2520Application%2520to%2520Vision%250A%2520%2520Based%2520Navigation%26entry.906535625%3DJ%25C3%25A9r%25C3%25A9my%2520Lebreton%2520and%2520Ingo%2520Ahrns%2520and%2520Roland%2520Brochard%2520and%2520Christoph%2520Haskamp%2520and%2520Matthieu%2520Le%2520Goff%2520and%2520Nicolas%2520Menga%2520and%2520Nicolas%2520Ollagnier%2520and%2520Ralf%2520Regele%2520and%2520Francesco%2520Capolupo%2520and%2520Massimo%2520Casasco%26entry.1292438233%3D%2520%2520Vision%2520Based%2520Navigation%2520consists%2520in%2520utilizing%2520cameras%2520as%2520precision%2520sensors%250Afor%2520GNC%2520after%2520extracting%2520information%2520from%2520images.%2520To%2520enable%2520the%2520adoption%2520of%250Amachine%2520learning%2520for%2520space%2520applications%252C%2520one%2520of%2520obstacles%2520is%2520the%2520demonstration%250Athat%2520available%2520training%2520datasets%2520are%2520adequate%2520to%2520validate%2520the%2520algorithms.%2520The%250Aobjective%2520of%2520the%2520study%2520is%2520to%2520generate%2520datasets%2520of%2520images%2520and%2520metadata%2520suitable%250Afor%2520training%2520machine%2520learning%2520algorithms.%2520Two%2520use%2520cases%2520were%2520selected%2520and%2520a%250Arobust%2520methodology%2520was%2520developed%2520to%2520validate%2520the%2520datasets%2520including%2520the%2520ground%250Atruth.%2520The%2520first%2520use%2520case%2520is%2520in-orbit%2520rendezvous%2520with%2520a%2520man-made%2520object%253A%2520a%250Amockup%2520of%2520satellite%2520ENVISAT.%2520The%2520second%2520use%2520case%2520is%2520a%2520Lunar%2520landing%2520scenario.%250ADatasets%2520were%2520produced%2520from%2520archival%2520datasets%2520%2528Chang%2527e%25203%2529%252C%2520from%2520the%2520laboratory%250Aat%2520DLR%2520TRON%2520facility%2520and%2520at%2520Airbus%2520Robotic%2520laboratory%252C%2520from%2520SurRender%2520software%250Ahigh%2520fidelity%2520image%2520simulator%2520using%2520Model%2520Capture%2520and%2520from%2520Generative%250AAdversarial%2520Networks.%2520The%2520use%2520case%2520definition%2520included%2520the%2520selection%2520of%250Aalgorithms%2520as%2520benchmark%253A%2520an%2520AI-based%2520pose%2520estimation%2520algorithm%2520and%2520a%2520dense%250Aoptical%2520flow%2520algorithm%2520were%2520selected.%2520Eventually%2520it%2520is%2520demonstrated%2520that%250Adatasets%2520produced%2520with%2520SurRender%2520and%2520selected%2520laboratory%2520facilities%2520are%250Aadequate%2520to%2520train%2520machine%2520learning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Datasets%20Generation%20for%20Machine%20Learning%3A%20Application%20to%20Vision%0A%20%20Based%20Navigation&entry.906535625=J%C3%A9r%C3%A9my%20Lebreton%20and%20Ingo%20Ahrns%20and%20Roland%20Brochard%20and%20Christoph%20Haskamp%20and%20Matthieu%20Le%20Goff%20and%20Nicolas%20Menga%20and%20Nicolas%20Ollagnier%20and%20Ralf%20Regele%20and%20Francesco%20Capolupo%20and%20Massimo%20Casasco&entry.1292438233=%20%20Vision%20Based%20Navigation%20consists%20in%20utilizing%20cameras%20as%20precision%20sensors%0Afor%20GNC%20after%20extracting%20information%20from%20images.%20To%20enable%20the%20adoption%20of%0Amachine%20learning%20for%20space%20applications%2C%20one%20of%20obstacles%20is%20the%20demonstration%0Athat%20available%20training%20datasets%20are%20adequate%20to%20validate%20the%20algorithms.%20The%0Aobjective%20of%20the%20study%20is%20to%20generate%20datasets%20of%20images%20and%20metadata%20suitable%0Afor%20training%20machine%20learning%20algorithms.%20Two%20use%20cases%20were%20selected%20and%20a%0Arobust%20methodology%20was%20developed%20to%20validate%20the%20datasets%20including%20the%20ground%0Atruth.%20The%20first%20use%20case%20is%20in-orbit%20rendezvous%20with%20a%20man-made%20object%3A%20a%0Amockup%20of%20satellite%20ENVISAT.%20The%20second%20use%20case%20is%20a%20Lunar%20landing%20scenario.%0ADatasets%20were%20produced%20from%20archival%20datasets%20%28Chang%27e%203%29%2C%20from%20the%20laboratory%0Aat%20DLR%20TRON%20facility%20and%20at%20Airbus%20Robotic%20laboratory%2C%20from%20SurRender%20software%0Ahigh%20fidelity%20image%20simulator%20using%20Model%20Capture%20and%20from%20Generative%0AAdversarial%20Networks.%20The%20use%20case%20definition%20included%20the%20selection%20of%0Aalgorithms%20as%20benchmark%3A%20an%20AI-based%20pose%20estimation%20algorithm%20and%20a%20dense%0Aoptical%20flow%20algorithm%20were%20selected.%20Eventually%20it%20is%20demonstrated%20that%0Adatasets%20produced%20with%20SurRender%20and%20selected%20laboratory%20facilities%20are%0Aadequate%20to%20train%20machine%20learning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11383v1&entry.124074799=Read"},
{"title": "OSV: One Step is Enough for High-Quality Image to Video Generation", "author": "Xiaofeng Mao and Zhengkai Jiang and Fu-Yun Wang and Wenbing Zhu and Jiangning Zhang and Hao Chen and Mingmin Chi and Yabiao Wang", "abstract": "  Video diffusion models have shown great potential in generating high-quality\nvideos, making them an increasingly popular focus. However, their inherent\niterative nature leads to substantial computational and time costs. While\nefforts have been made to accelerate video diffusion by reducing inference\nsteps (through techniques like consistency distillation) and GAN training\n(these approaches often fall short in either performance or training\nstability). In this work, we introduce a two-stage training framework that\neffectively combines consistency distillation with GAN training to address\nthese challenges. Additionally, we propose a novel video discriminator design,\nwhich eliminates the need for decoding the video latents and improves the final\nperformance. Our model is capable of producing high-quality videos in merely\none-step, with the flexibility to perform multi-step refinement for further\nperformance enhancement. Our quantitative evaluation on the OpenWebVid-1M\nbenchmark shows that our model significantly outperforms existing methods.\nNotably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of\nthe consistency distillation based method, AnimateLCM (FVD 184.79), and\napproaches the 25-step performance of advanced Stable Video Diffusion (FVD\n156.94).\n", "link": "http://arxiv.org/abs/2409.11367v1", "date": "2024-09-17", "relevancy": 2.6809, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6734}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6699}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSV%3A%20One%20Step%20is%20Enough%20for%20High-Quality%20Image%20to%20Video%20Generation&body=Title%3A%20OSV%3A%20One%20Step%20is%20Enough%20for%20High-Quality%20Image%20to%20Video%20Generation%0AAuthor%3A%20Xiaofeng%20Mao%20and%20Zhengkai%20Jiang%20and%20Fu-Yun%20Wang%20and%20Wenbing%20Zhu%20and%20Jiangning%20Zhang%20and%20Hao%20Chen%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang%0AAbstract%3A%20%20%20Video%20diffusion%20models%20have%20shown%20great%20potential%20in%20generating%20high-quality%0Avideos%2C%20making%20them%20an%20increasingly%20popular%20focus.%20However%2C%20their%20inherent%0Aiterative%20nature%20leads%20to%20substantial%20computational%20and%20time%20costs.%20While%0Aefforts%20have%20been%20made%20to%20accelerate%20video%20diffusion%20by%20reducing%20inference%0Asteps%20%28through%20techniques%20like%20consistency%20distillation%29%20and%20GAN%20training%0A%28these%20approaches%20often%20fall%20short%20in%20either%20performance%20or%20training%0Astability%29.%20In%20this%20work%2C%20we%20introduce%20a%20two-stage%20training%20framework%20that%0Aeffectively%20combines%20consistency%20distillation%20with%20GAN%20training%20to%20address%0Athese%20challenges.%20Additionally%2C%20we%20propose%20a%20novel%20video%20discriminator%20design%2C%0Awhich%20eliminates%20the%20need%20for%20decoding%20the%20video%20latents%20and%20improves%20the%20final%0Aperformance.%20Our%20model%20is%20capable%20of%20producing%20high-quality%20videos%20in%20merely%0Aone-step%2C%20with%20the%20flexibility%20to%20perform%20multi-step%20refinement%20for%20further%0Aperformance%20enhancement.%20Our%20quantitative%20evaluation%20on%20the%20OpenWebVid-1M%0Abenchmark%20shows%20that%20our%20model%20significantly%20outperforms%20existing%20methods.%0ANotably%2C%20our%201-step%20performance%28FVD%20171.15%29%20exceeds%20the%208-step%20performance%20of%0Athe%20consistency%20distillation%20based%20method%2C%20AnimateLCM%20%28FVD%20184.79%29%2C%20and%0Aapproaches%20the%2025-step%20performance%20of%20advanced%20Stable%20Video%20Diffusion%20%28FVD%0A156.94%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSV%253A%2520One%2520Step%2520is%2520Enough%2520for%2520High-Quality%2520Image%2520to%2520Video%2520Generation%26entry.906535625%3DXiaofeng%2520Mao%2520and%2520Zhengkai%2520Jiang%2520and%2520Fu-Yun%2520Wang%2520and%2520Wenbing%2520Zhu%2520and%2520Jiangning%2520Zhang%2520and%2520Hao%2520Chen%2520and%2520Mingmin%2520Chi%2520and%2520Yabiao%2520Wang%26entry.1292438233%3D%2520%2520Video%2520diffusion%2520models%2520have%2520shown%2520great%2520potential%2520in%2520generating%2520high-quality%250Avideos%252C%2520making%2520them%2520an%2520increasingly%2520popular%2520focus.%2520However%252C%2520their%2520inherent%250Aiterative%2520nature%2520leads%2520to%2520substantial%2520computational%2520and%2520time%2520costs.%2520While%250Aefforts%2520have%2520been%2520made%2520to%2520accelerate%2520video%2520diffusion%2520by%2520reducing%2520inference%250Asteps%2520%2528through%2520techniques%2520like%2520consistency%2520distillation%2529%2520and%2520GAN%2520training%250A%2528these%2520approaches%2520often%2520fall%2520short%2520in%2520either%2520performance%2520or%2520training%250Astability%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520two-stage%2520training%2520framework%2520that%250Aeffectively%2520combines%2520consistency%2520distillation%2520with%2520GAN%2520training%2520to%2520address%250Athese%2520challenges.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520video%2520discriminator%2520design%252C%250Awhich%2520eliminates%2520the%2520need%2520for%2520decoding%2520the%2520video%2520latents%2520and%2520improves%2520the%2520final%250Aperformance.%2520Our%2520model%2520is%2520capable%2520of%2520producing%2520high-quality%2520videos%2520in%2520merely%250Aone-step%252C%2520with%2520the%2520flexibility%2520to%2520perform%2520multi-step%2520refinement%2520for%2520further%250Aperformance%2520enhancement.%2520Our%2520quantitative%2520evaluation%2520on%2520the%2520OpenWebVid-1M%250Abenchmark%2520shows%2520that%2520our%2520model%2520significantly%2520outperforms%2520existing%2520methods.%250ANotably%252C%2520our%25201-step%2520performance%2528FVD%2520171.15%2529%2520exceeds%2520the%25208-step%2520performance%2520of%250Athe%2520consistency%2520distillation%2520based%2520method%252C%2520AnimateLCM%2520%2528FVD%2520184.79%2529%252C%2520and%250Aapproaches%2520the%252025-step%2520performance%2520of%2520advanced%2520Stable%2520Video%2520Diffusion%2520%2528FVD%250A156.94%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSV%3A%20One%20Step%20is%20Enough%20for%20High-Quality%20Image%20to%20Video%20Generation&entry.906535625=Xiaofeng%20Mao%20and%20Zhengkai%20Jiang%20and%20Fu-Yun%20Wang%20and%20Wenbing%20Zhu%20and%20Jiangning%20Zhang%20and%20Hao%20Chen%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang&entry.1292438233=%20%20Video%20diffusion%20models%20have%20shown%20great%20potential%20in%20generating%20high-quality%0Avideos%2C%20making%20them%20an%20increasingly%20popular%20focus.%20However%2C%20their%20inherent%0Aiterative%20nature%20leads%20to%20substantial%20computational%20and%20time%20costs.%20While%0Aefforts%20have%20been%20made%20to%20accelerate%20video%20diffusion%20by%20reducing%20inference%0Asteps%20%28through%20techniques%20like%20consistency%20distillation%29%20and%20GAN%20training%0A%28these%20approaches%20often%20fall%20short%20in%20either%20performance%20or%20training%0Astability%29.%20In%20this%20work%2C%20we%20introduce%20a%20two-stage%20training%20framework%20that%0Aeffectively%20combines%20consistency%20distillation%20with%20GAN%20training%20to%20address%0Athese%20challenges.%20Additionally%2C%20we%20propose%20a%20novel%20video%20discriminator%20design%2C%0Awhich%20eliminates%20the%20need%20for%20decoding%20the%20video%20latents%20and%20improves%20the%20final%0Aperformance.%20Our%20model%20is%20capable%20of%20producing%20high-quality%20videos%20in%20merely%0Aone-step%2C%20with%20the%20flexibility%20to%20perform%20multi-step%20refinement%20for%20further%0Aperformance%20enhancement.%20Our%20quantitative%20evaluation%20on%20the%20OpenWebVid-1M%0Abenchmark%20shows%20that%20our%20model%20significantly%20outperforms%20existing%20methods.%0ANotably%2C%20our%201-step%20performance%28FVD%20171.15%29%20exceeds%20the%208-step%20performance%20of%0Athe%20consistency%20distillation%20based%20method%2C%20AnimateLCM%20%28FVD%20184.79%29%2C%20and%0Aapproaches%20the%2025-step%20performance%20of%20advanced%20Stable%20Video%20Diffusion%20%28FVD%0A156.94%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11367v1&entry.124074799=Read"},
{"title": "Visual grounding for desktop graphical user interfaces", "author": "Tassnim Dardouri and Laura Minkova and Jessica L\u00f3pez Espejel and Walid Dahhane and El Hassane Ettifouri", "abstract": "  Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel.\n", "link": "http://arxiv.org/abs/2407.01558v2", "date": "2024-09-17", "relevancy": 2.6608, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5553}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20grounding%20for%20desktop%20graphical%20user%20interfaces&body=Title%3A%20Visual%20grounding%20for%20desktop%20graphical%20user%20interfaces%0AAuthor%3A%20Tassnim%20Dardouri%20and%20Laura%20Minkova%20and%20Jessica%20L%C3%B3pez%20Espejel%20and%20Walid%20Dahhane%20and%20El%20Hassane%20Ettifouri%0AAbstract%3A%20%20%20Most%20instance%20perception%20and%20image%20understanding%20solutions%20focus%20mainly%20on%0Anatural%20images.%20However%2C%20applications%20for%20synthetic%20images%2C%20and%20more%0Aspecifically%2C%20images%20of%20Graphical%20User%20Interfaces%20%28GUI%29%20remain%20limited.%20This%0Ahinders%20the%20development%20of%20autonomous%20computer-vision-powered%20Artificial%0AIntelligence%20%28AI%29%20agents.%20In%20this%20work%2C%20we%20present%20Instruction%20Visual%20Grounding%0Aor%20IVG%2C%20a%20multi-modal%20solution%20for%20object%20identification%20in%20a%20GUI.%20More%0Aprecisely%2C%20given%20a%20natural%20language%20instruction%20and%20GUI%20screen%2C%20IVG%20locates%20the%0Acoordinates%20of%20the%20element%20on%20the%20screen%20where%20the%20instruction%20would%20be%0Aexecuted.%20To%20this%20end%2C%20we%20develop%20two%20methods.%20The%20first%20method%20is%20a%20three-part%0Aarchitecture%20that%20relies%20on%20a%20combination%20of%20a%20Large%20Language%20Model%20%28LLM%29%20and%0Aan%20object%20detection%20model.%20The%20second%20approach%20uses%20a%20multi-modal%20foundation%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520grounding%2520for%2520desktop%2520graphical%2520user%2520interfaces%26entry.906535625%3DTassnim%2520Dardouri%2520and%2520Laura%2520Minkova%2520and%2520Jessica%2520L%25C3%25B3pez%2520Espejel%2520and%2520Walid%2520Dahhane%2520and%2520El%2520Hassane%2520Ettifouri%26entry.1292438233%3D%2520%2520Most%2520instance%2520perception%2520and%2520image%2520understanding%2520solutions%2520focus%2520mainly%2520on%250Anatural%2520images.%2520However%252C%2520applications%2520for%2520synthetic%2520images%252C%2520and%2520more%250Aspecifically%252C%2520images%2520of%2520Graphical%2520User%2520Interfaces%2520%2528GUI%2529%2520remain%2520limited.%2520This%250Ahinders%2520the%2520development%2520of%2520autonomous%2520computer-vision-powered%2520Artificial%250AIntelligence%2520%2528AI%2529%2520agents.%2520In%2520this%2520work%252C%2520we%2520present%2520Instruction%2520Visual%2520Grounding%250Aor%2520IVG%252C%2520a%2520multi-modal%2520solution%2520for%2520object%2520identification%2520in%2520a%2520GUI.%2520More%250Aprecisely%252C%2520given%2520a%2520natural%2520language%2520instruction%2520and%2520GUI%2520screen%252C%2520IVG%2520locates%2520the%250Acoordinates%2520of%2520the%2520element%2520on%2520the%2520screen%2520where%2520the%2520instruction%2520would%2520be%250Aexecuted.%2520To%2520this%2520end%252C%2520we%2520develop%2520two%2520methods.%2520The%2520first%2520method%2520is%2520a%2520three-part%250Aarchitecture%2520that%2520relies%2520on%2520a%2520combination%2520of%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520and%250Aan%2520object%2520detection%2520model.%2520The%2520second%2520approach%2520uses%2520a%2520multi-modal%2520foundation%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20grounding%20for%20desktop%20graphical%20user%20interfaces&entry.906535625=Tassnim%20Dardouri%20and%20Laura%20Minkova%20and%20Jessica%20L%C3%B3pez%20Espejel%20and%20Walid%20Dahhane%20and%20El%20Hassane%20Ettifouri&entry.1292438233=%20%20Most%20instance%20perception%20and%20image%20understanding%20solutions%20focus%20mainly%20on%0Anatural%20images.%20However%2C%20applications%20for%20synthetic%20images%2C%20and%20more%0Aspecifically%2C%20images%20of%20Graphical%20User%20Interfaces%20%28GUI%29%20remain%20limited.%20This%0Ahinders%20the%20development%20of%20autonomous%20computer-vision-powered%20Artificial%0AIntelligence%20%28AI%29%20agents.%20In%20this%20work%2C%20we%20present%20Instruction%20Visual%20Grounding%0Aor%20IVG%2C%20a%20multi-modal%20solution%20for%20object%20identification%20in%20a%20GUI.%20More%0Aprecisely%2C%20given%20a%20natural%20language%20instruction%20and%20GUI%20screen%2C%20IVG%20locates%20the%0Acoordinates%20of%20the%20element%20on%20the%20screen%20where%20the%20instruction%20would%20be%0Aexecuted.%20To%20this%20end%2C%20we%20develop%20two%20methods.%20The%20first%20method%20is%20a%20three-part%0Aarchitecture%20that%20relies%20on%20a%20combination%20of%20a%20Large%20Language%20Model%20%28LLM%29%20and%0Aan%20object%20detection%20model.%20The%20second%20approach%20uses%20a%20multi-modal%20foundation%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01558v2&entry.124074799=Read"},
{"title": "BoostDream: Efficient Refining for High-Quality Text-to-3D Generation\n  from Multi-View Diffusion", "author": "Yonghao Yu and Shunan Zhu and Huai Qin and Haorui Li", "abstract": "  Witnessing the evolution of text-to-image diffusion models, significant\nstrides have been made in text-to-3D generation. Currently, two primary\nparadigms dominate the field of text-to-3D: the feed-forward generation\nsolutions, capable of swiftly producing 3D assets but often yielding coarse\nresults, and the Score Distillation Sampling (SDS) based solutions, known for\ngenerating high-fidelity 3D assets albeit at a slower pace. The synergistic\nintegration of these methods holds substantial promise for advancing 3D\ngeneration techniques. In this paper, we present BoostDream, a highly efficient\nplug-and-play 3D refining method designed to transform coarse 3D assets into\nhigh-quality. The BoostDream framework comprises three distinct processes: (1)\nWe introduce 3D model distillation that fits differentiable representations\nfrom the 3D assets obtained through feed-forward generation. (2) A novel\nmulti-view SDS loss is designed, which utilizes a multi-view aware 2D diffusion\nmodel to refine the 3D assets. (3) We propose to use prompt and multi-view\nconsistent normal maps as guidance in refinement.Our extensive experiment is\nconducted on different differentiable 3D representations, revealing that\nBoostDream excels in generating high-quality 3D assets rapidly, overcoming the\nJanus problem compared to conventional SDS-based methods. This breakthrough\nsignifies a substantial advancement in both the efficiency and quality of 3D\ngeneration processes.\n", "link": "http://arxiv.org/abs/2401.16764v3", "date": "2024-09-17", "relevancy": 2.6543, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6668}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6668}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoostDream%3A%20Efficient%20Refining%20for%20High-Quality%20Text-to-3D%20Generation%0A%20%20from%20Multi-View%20Diffusion&body=Title%3A%20BoostDream%3A%20Efficient%20Refining%20for%20High-Quality%20Text-to-3D%20Generation%0A%20%20from%20Multi-View%20Diffusion%0AAuthor%3A%20Yonghao%20Yu%20and%20Shunan%20Zhu%20and%20Huai%20Qin%20and%20Haorui%20Li%0AAbstract%3A%20%20%20Witnessing%20the%20evolution%20of%20text-to-image%20diffusion%20models%2C%20significant%0Astrides%20have%20been%20made%20in%20text-to-3D%20generation.%20Currently%2C%20two%20primary%0Aparadigms%20dominate%20the%20field%20of%20text-to-3D%3A%20the%20feed-forward%20generation%0Asolutions%2C%20capable%20of%20swiftly%20producing%203D%20assets%20but%20often%20yielding%20coarse%0Aresults%2C%20and%20the%20Score%20Distillation%20Sampling%20%28SDS%29%20based%20solutions%2C%20known%20for%0Agenerating%20high-fidelity%203D%20assets%20albeit%20at%20a%20slower%20pace.%20The%20synergistic%0Aintegration%20of%20these%20methods%20holds%20substantial%20promise%20for%20advancing%203D%0Ageneration%20techniques.%20In%20this%20paper%2C%20we%20present%20BoostDream%2C%20a%20highly%20efficient%0Aplug-and-play%203D%20refining%20method%20designed%20to%20transform%20coarse%203D%20assets%20into%0Ahigh-quality.%20The%20BoostDream%20framework%20comprises%20three%20distinct%20processes%3A%20%281%29%0AWe%20introduce%203D%20model%20distillation%20that%20fits%20differentiable%20representations%0Afrom%20the%203D%20assets%20obtained%20through%20feed-forward%20generation.%20%282%29%20A%20novel%0Amulti-view%20SDS%20loss%20is%20designed%2C%20which%20utilizes%20a%20multi-view%20aware%202D%20diffusion%0Amodel%20to%20refine%20the%203D%20assets.%20%283%29%20We%20propose%20to%20use%20prompt%20and%20multi-view%0Aconsistent%20normal%20maps%20as%20guidance%20in%20refinement.Our%20extensive%20experiment%20is%0Aconducted%20on%20different%20differentiable%203D%20representations%2C%20revealing%20that%0ABoostDream%20excels%20in%20generating%20high-quality%203D%20assets%20rapidly%2C%20overcoming%20the%0AJanus%20problem%20compared%20to%20conventional%20SDS-based%20methods.%20This%20breakthrough%0Asignifies%20a%20substantial%20advancement%20in%20both%20the%20efficiency%20and%20quality%20of%203D%0Ageneration%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16764v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoostDream%253A%2520Efficient%2520Refining%2520for%2520High-Quality%2520Text-to-3D%2520Generation%250A%2520%2520from%2520Multi-View%2520Diffusion%26entry.906535625%3DYonghao%2520Yu%2520and%2520Shunan%2520Zhu%2520and%2520Huai%2520Qin%2520and%2520Haorui%2520Li%26entry.1292438233%3D%2520%2520Witnessing%2520the%2520evolution%2520of%2520text-to-image%2520diffusion%2520models%252C%2520significant%250Astrides%2520have%2520been%2520made%2520in%2520text-to-3D%2520generation.%2520Currently%252C%2520two%2520primary%250Aparadigms%2520dominate%2520the%2520field%2520of%2520text-to-3D%253A%2520the%2520feed-forward%2520generation%250Asolutions%252C%2520capable%2520of%2520swiftly%2520producing%25203D%2520assets%2520but%2520often%2520yielding%2520coarse%250Aresults%252C%2520and%2520the%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520based%2520solutions%252C%2520known%2520for%250Agenerating%2520high-fidelity%25203D%2520assets%2520albeit%2520at%2520a%2520slower%2520pace.%2520The%2520synergistic%250Aintegration%2520of%2520these%2520methods%2520holds%2520substantial%2520promise%2520for%2520advancing%25203D%250Ageneration%2520techniques.%2520In%2520this%2520paper%252C%2520we%2520present%2520BoostDream%252C%2520a%2520highly%2520efficient%250Aplug-and-play%25203D%2520refining%2520method%2520designed%2520to%2520transform%2520coarse%25203D%2520assets%2520into%250Ahigh-quality.%2520The%2520BoostDream%2520framework%2520comprises%2520three%2520distinct%2520processes%253A%2520%25281%2529%250AWe%2520introduce%25203D%2520model%2520distillation%2520that%2520fits%2520differentiable%2520representations%250Afrom%2520the%25203D%2520assets%2520obtained%2520through%2520feed-forward%2520generation.%2520%25282%2529%2520A%2520novel%250Amulti-view%2520SDS%2520loss%2520is%2520designed%252C%2520which%2520utilizes%2520a%2520multi-view%2520aware%25202D%2520diffusion%250Amodel%2520to%2520refine%2520the%25203D%2520assets.%2520%25283%2529%2520We%2520propose%2520to%2520use%2520prompt%2520and%2520multi-view%250Aconsistent%2520normal%2520maps%2520as%2520guidance%2520in%2520refinement.Our%2520extensive%2520experiment%2520is%250Aconducted%2520on%2520different%2520differentiable%25203D%2520representations%252C%2520revealing%2520that%250ABoostDream%2520excels%2520in%2520generating%2520high-quality%25203D%2520assets%2520rapidly%252C%2520overcoming%2520the%250AJanus%2520problem%2520compared%2520to%2520conventional%2520SDS-based%2520methods.%2520This%2520breakthrough%250Asignifies%2520a%2520substantial%2520advancement%2520in%2520both%2520the%2520efficiency%2520and%2520quality%2520of%25203D%250Ageneration%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16764v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoostDream%3A%20Efficient%20Refining%20for%20High-Quality%20Text-to-3D%20Generation%0A%20%20from%20Multi-View%20Diffusion&entry.906535625=Yonghao%20Yu%20and%20Shunan%20Zhu%20and%20Huai%20Qin%20and%20Haorui%20Li&entry.1292438233=%20%20Witnessing%20the%20evolution%20of%20text-to-image%20diffusion%20models%2C%20significant%0Astrides%20have%20been%20made%20in%20text-to-3D%20generation.%20Currently%2C%20two%20primary%0Aparadigms%20dominate%20the%20field%20of%20text-to-3D%3A%20the%20feed-forward%20generation%0Asolutions%2C%20capable%20of%20swiftly%20producing%203D%20assets%20but%20often%20yielding%20coarse%0Aresults%2C%20and%20the%20Score%20Distillation%20Sampling%20%28SDS%29%20based%20solutions%2C%20known%20for%0Agenerating%20high-fidelity%203D%20assets%20albeit%20at%20a%20slower%20pace.%20The%20synergistic%0Aintegration%20of%20these%20methods%20holds%20substantial%20promise%20for%20advancing%203D%0Ageneration%20techniques.%20In%20this%20paper%2C%20we%20present%20BoostDream%2C%20a%20highly%20efficient%0Aplug-and-play%203D%20refining%20method%20designed%20to%20transform%20coarse%203D%20assets%20into%0Ahigh-quality.%20The%20BoostDream%20framework%20comprises%20three%20distinct%20processes%3A%20%281%29%0AWe%20introduce%203D%20model%20distillation%20that%20fits%20differentiable%20representations%0Afrom%20the%203D%20assets%20obtained%20through%20feed-forward%20generation.%20%282%29%20A%20novel%0Amulti-view%20SDS%20loss%20is%20designed%2C%20which%20utilizes%20a%20multi-view%20aware%202D%20diffusion%0Amodel%20to%20refine%20the%203D%20assets.%20%283%29%20We%20propose%20to%20use%20prompt%20and%20multi-view%0Aconsistent%20normal%20maps%20as%20guidance%20in%20refinement.Our%20extensive%20experiment%20is%0Aconducted%20on%20different%20differentiable%203D%20representations%2C%20revealing%20that%0ABoostDream%20excels%20in%20generating%20high-quality%203D%20assets%20rapidly%2C%20overcoming%20the%0AJanus%20problem%20compared%20to%20conventional%20SDS-based%20methods.%20This%20breakthrough%0Asignifies%20a%20substantial%20advancement%20in%20both%20the%20efficiency%20and%20quality%20of%203D%0Ageneration%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16764v3&entry.124074799=Read"},
{"title": "Says Who? Effective Zero-Shot Annotation of Focalization", "author": "Rebecca M. M. Hicke and Yuri Bizzoni and Pascale Feldkamp and Ross Deans Kristensen-McLachlan", "abstract": "  Focalization, the perspective through which narrative is presented, is\nencoded via a wide range of lexico-grammatical features and is subject to\nreader interpretation. Moreover, trained readers regularly disagree on\ninterpretations, suggesting that this problem may be computationally\nintractable. In this paper, we provide experiments to test how well\ncontemporary Large Language Models (LLMs) perform when annotating literary\ntexts for focalization mode. Despite the challenging nature of the task, LLMs\nshow comparable performance to trained human annotators in our experiments. We\nprovide a case study working with the novels of Stephen King to demonstrate the\nusefulness of this approach for computational literary studies, illustrating\nhow focalization can be studied at scale.\n", "link": "http://arxiv.org/abs/2409.11390v1", "date": "2024-09-17", "relevancy": 2.637, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Says%20Who%3F%20Effective%20Zero-Shot%20Annotation%20of%20Focalization&body=Title%3A%20Says%20Who%3F%20Effective%20Zero-Shot%20Annotation%20of%20Focalization%0AAuthor%3A%20Rebecca%20M.%20M.%20Hicke%20and%20Yuri%20Bizzoni%20and%20Pascale%20Feldkamp%20and%20Ross%20Deans%20Kristensen-McLachlan%0AAbstract%3A%20%20%20Focalization%2C%20the%20perspective%20through%20which%20narrative%20is%20presented%2C%20is%0Aencoded%20via%20a%20wide%20range%20of%20lexico-grammatical%20features%20and%20is%20subject%20to%0Areader%20interpretation.%20Moreover%2C%20trained%20readers%20regularly%20disagree%20on%0Ainterpretations%2C%20suggesting%20that%20this%20problem%20may%20be%20computationally%0Aintractable.%20In%20this%20paper%2C%20we%20provide%20experiments%20to%20test%20how%20well%0Acontemporary%20Large%20Language%20Models%20%28LLMs%29%20perform%20when%20annotating%20literary%0Atexts%20for%20focalization%20mode.%20Despite%20the%20challenging%20nature%20of%20the%20task%2C%20LLMs%0Ashow%20comparable%20performance%20to%20trained%20human%20annotators%20in%20our%20experiments.%20We%0Aprovide%20a%20case%20study%20working%20with%20the%20novels%20of%20Stephen%20King%20to%20demonstrate%20the%0Ausefulness%20of%20this%20approach%20for%20computational%20literary%20studies%2C%20illustrating%0Ahow%20focalization%20can%20be%20studied%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSays%2520Who%253F%2520Effective%2520Zero-Shot%2520Annotation%2520of%2520Focalization%26entry.906535625%3DRebecca%2520M.%2520M.%2520Hicke%2520and%2520Yuri%2520Bizzoni%2520and%2520Pascale%2520Feldkamp%2520and%2520Ross%2520Deans%2520Kristensen-McLachlan%26entry.1292438233%3D%2520%2520Focalization%252C%2520the%2520perspective%2520through%2520which%2520narrative%2520is%2520presented%252C%2520is%250Aencoded%2520via%2520a%2520wide%2520range%2520of%2520lexico-grammatical%2520features%2520and%2520is%2520subject%2520to%250Areader%2520interpretation.%2520Moreover%252C%2520trained%2520readers%2520regularly%2520disagree%2520on%250Ainterpretations%252C%2520suggesting%2520that%2520this%2520problem%2520may%2520be%2520computationally%250Aintractable.%2520In%2520this%2520paper%252C%2520we%2520provide%2520experiments%2520to%2520test%2520how%2520well%250Acontemporary%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520perform%2520when%2520annotating%2520literary%250Atexts%2520for%2520focalization%2520mode.%2520Despite%2520the%2520challenging%2520nature%2520of%2520the%2520task%252C%2520LLMs%250Ashow%2520comparable%2520performance%2520to%2520trained%2520human%2520annotators%2520in%2520our%2520experiments.%2520We%250Aprovide%2520a%2520case%2520study%2520working%2520with%2520the%2520novels%2520of%2520Stephen%2520King%2520to%2520demonstrate%2520the%250Ausefulness%2520of%2520this%2520approach%2520for%2520computational%2520literary%2520studies%252C%2520illustrating%250Ahow%2520focalization%2520can%2520be%2520studied%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Says%20Who%3F%20Effective%20Zero-Shot%20Annotation%20of%20Focalization&entry.906535625=Rebecca%20M.%20M.%20Hicke%20and%20Yuri%20Bizzoni%20and%20Pascale%20Feldkamp%20and%20Ross%20Deans%20Kristensen-McLachlan&entry.1292438233=%20%20Focalization%2C%20the%20perspective%20through%20which%20narrative%20is%20presented%2C%20is%0Aencoded%20via%20a%20wide%20range%20of%20lexico-grammatical%20features%20and%20is%20subject%20to%0Areader%20interpretation.%20Moreover%2C%20trained%20readers%20regularly%20disagree%20on%0Ainterpretations%2C%20suggesting%20that%20this%20problem%20may%20be%20computationally%0Aintractable.%20In%20this%20paper%2C%20we%20provide%20experiments%20to%20test%20how%20well%0Acontemporary%20Large%20Language%20Models%20%28LLMs%29%20perform%20when%20annotating%20literary%0Atexts%20for%20focalization%20mode.%20Despite%20the%20challenging%20nature%20of%20the%20task%2C%20LLMs%0Ashow%20comparable%20performance%20to%20trained%20human%20annotators%20in%20our%20experiments.%20We%0Aprovide%20a%20case%20study%20working%20with%20the%20novels%20of%20Stephen%20King%20to%20demonstrate%20the%0Ausefulness%20of%20this%20approach%20for%20computational%20literary%20studies%2C%20illustrating%0Ahow%20focalization%20can%20be%20studied%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11390v1&entry.124074799=Read"},
{"title": "Air-FAR: Fast and Adaptable Routing for Aerial Navigation in Large-scale\n  Complex Unknown Environments", "author": "Botao He and Guofei Chen and Cornelia Fermuller and Yiannis Aloimonos and Ji Zhang", "abstract": "  This paper presents a novel method for real-time 3D navigation in\nlarge-scale, complex environments using a hierarchical 3D visibility graph\n(V-graph). The proposed algorithm addresses the computational challenges of\nV-graph construction and shortest path search on the graph simultaneously. By\nintroducing hierarchical 3D V-graph construction with heuristic visibility\nupdate, the 3D V-graph is constructed in O(K*n^2logn) time, which guarantees\nreal-time performance. The proposed iterative divide-and-conquer path search\nmethod can achieve near-optimal path solutions within the constraints of\nreal-time operations. The algorithm ensures efficient 3D V-graph construction\nand path search. Extensive simulated and real-world environments validated that\nour algorithm reduces the travel time by 42%, achieves up to 24.8% higher\ntrajectory efficiency, and runs faster than most benchmarks by orders of\nmagnitude in complex environments. The code and developed simulator have been\nopen-sourced to facilitate future research.\n", "link": "http://arxiv.org/abs/2409.11188v1", "date": "2024-09-17", "relevancy": 2.6212, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5124}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Air-FAR%3A%20Fast%20and%20Adaptable%20Routing%20for%20Aerial%20Navigation%20in%20Large-scale%0A%20%20Complex%20Unknown%20Environments&body=Title%3A%20Air-FAR%3A%20Fast%20and%20Adaptable%20Routing%20for%20Aerial%20Navigation%20in%20Large-scale%0A%20%20Complex%20Unknown%20Environments%0AAuthor%3A%20Botao%20He%20and%20Guofei%20Chen%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos%20and%20Ji%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20method%20for%20real-time%203D%20navigation%20in%0Alarge-scale%2C%20complex%20environments%20using%20a%20hierarchical%203D%20visibility%20graph%0A%28V-graph%29.%20The%20proposed%20algorithm%20addresses%20the%20computational%20challenges%20of%0AV-graph%20construction%20and%20shortest%20path%20search%20on%20the%20graph%20simultaneously.%20By%0Aintroducing%20hierarchical%203D%20V-graph%20construction%20with%20heuristic%20visibility%0Aupdate%2C%20the%203D%20V-graph%20is%20constructed%20in%20O%28K%2An%5E2logn%29%20time%2C%20which%20guarantees%0Areal-time%20performance.%20The%20proposed%20iterative%20divide-and-conquer%20path%20search%0Amethod%20can%20achieve%20near-optimal%20path%20solutions%20within%20the%20constraints%20of%0Areal-time%20operations.%20The%20algorithm%20ensures%20efficient%203D%20V-graph%20construction%0Aand%20path%20search.%20Extensive%20simulated%20and%20real-world%20environments%20validated%20that%0Aour%20algorithm%20reduces%20the%20travel%20time%20by%2042%25%2C%20achieves%20up%20to%2024.8%25%20higher%0Atrajectory%20efficiency%2C%20and%20runs%20faster%20than%20most%20benchmarks%20by%20orders%20of%0Amagnitude%20in%20complex%20environments.%20The%20code%20and%20developed%20simulator%20have%20been%0Aopen-sourced%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAir-FAR%253A%2520Fast%2520and%2520Adaptable%2520Routing%2520for%2520Aerial%2520Navigation%2520in%2520Large-scale%250A%2520%2520Complex%2520Unknown%2520Environments%26entry.906535625%3DBotao%2520He%2520and%2520Guofei%2520Chen%2520and%2520Cornelia%2520Fermuller%2520and%2520Yiannis%2520Aloimonos%2520and%2520Ji%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520method%2520for%2520real-time%25203D%2520navigation%2520in%250Alarge-scale%252C%2520complex%2520environments%2520using%2520a%2520hierarchical%25203D%2520visibility%2520graph%250A%2528V-graph%2529.%2520The%2520proposed%2520algorithm%2520addresses%2520the%2520computational%2520challenges%2520of%250AV-graph%2520construction%2520and%2520shortest%2520path%2520search%2520on%2520the%2520graph%2520simultaneously.%2520By%250Aintroducing%2520hierarchical%25203D%2520V-graph%2520construction%2520with%2520heuristic%2520visibility%250Aupdate%252C%2520the%25203D%2520V-graph%2520is%2520constructed%2520in%2520O%2528K%252An%255E2logn%2529%2520time%252C%2520which%2520guarantees%250Areal-time%2520performance.%2520The%2520proposed%2520iterative%2520divide-and-conquer%2520path%2520search%250Amethod%2520can%2520achieve%2520near-optimal%2520path%2520solutions%2520within%2520the%2520constraints%2520of%250Areal-time%2520operations.%2520The%2520algorithm%2520ensures%2520efficient%25203D%2520V-graph%2520construction%250Aand%2520path%2520search.%2520Extensive%2520simulated%2520and%2520real-world%2520environments%2520validated%2520that%250Aour%2520algorithm%2520reduces%2520the%2520travel%2520time%2520by%252042%2525%252C%2520achieves%2520up%2520to%252024.8%2525%2520higher%250Atrajectory%2520efficiency%252C%2520and%2520runs%2520faster%2520than%2520most%2520benchmarks%2520by%2520orders%2520of%250Amagnitude%2520in%2520complex%2520environments.%2520The%2520code%2520and%2520developed%2520simulator%2520have%2520been%250Aopen-sourced%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Air-FAR%3A%20Fast%20and%20Adaptable%20Routing%20for%20Aerial%20Navigation%20in%20Large-scale%0A%20%20Complex%20Unknown%20Environments&entry.906535625=Botao%20He%20and%20Guofei%20Chen%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos%20and%20Ji%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20method%20for%20real-time%203D%20navigation%20in%0Alarge-scale%2C%20complex%20environments%20using%20a%20hierarchical%203D%20visibility%20graph%0A%28V-graph%29.%20The%20proposed%20algorithm%20addresses%20the%20computational%20challenges%20of%0AV-graph%20construction%20and%20shortest%20path%20search%20on%20the%20graph%20simultaneously.%20By%0Aintroducing%20hierarchical%203D%20V-graph%20construction%20with%20heuristic%20visibility%0Aupdate%2C%20the%203D%20V-graph%20is%20constructed%20in%20O%28K%2An%5E2logn%29%20time%2C%20which%20guarantees%0Areal-time%20performance.%20The%20proposed%20iterative%20divide-and-conquer%20path%20search%0Amethod%20can%20achieve%20near-optimal%20path%20solutions%20within%20the%20constraints%20of%0Areal-time%20operations.%20The%20algorithm%20ensures%20efficient%203D%20V-graph%20construction%0Aand%20path%20search.%20Extensive%20simulated%20and%20real-world%20environments%20validated%20that%0Aour%20algorithm%20reduces%20the%20travel%20time%20by%2042%25%2C%20achieves%20up%20to%2024.8%25%20higher%0Atrajectory%20efficiency%2C%20and%20runs%20faster%20than%20most%20benchmarks%20by%20orders%20of%0Amagnitude%20in%20complex%20environments.%20The%20code%20and%20developed%20simulator%20have%20been%0Aopen-sourced%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11188v1&entry.124074799=Read"},
{"title": "Leveraging Distillation Techniques for Document Understanding: A Case\n  Study with FLAN-T5", "author": "Marcel Lamott and Muhammad Armaghan Shakir", "abstract": "  The surge of digital documents in various formats, including less\nstandardized documents such as business reports and environmental assessments,\nunderscores the growing importance of Document Understanding. While Large\nLanguage Models (LLMs) have showcased prowess across diverse natural language\nprocessing tasks, their direct application to Document Understanding remains a\nchallenge. Previous research has demonstrated the utility of LLMs in this\ndomain, yet their significant computational demands make them challenging to\ndeploy effectively. Additionally, proprietary Blackbox LLMs often outperform\ntheir open-source counterparts, posing a barrier to widespread accessibility.\nIn this paper, we delve into the realm of document understanding, leveraging\ndistillation methods to harness the power of large LLMs while accommodating\ncomputational limitations. Specifically, we present a novel approach wherein we\ndistill document understanding knowledge from the proprietary LLM ChatGPT into\nFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms\nto facilitate efficient knowledge transfer. This work contributes to the\nadvancement of document understanding methodologies by offering a scalable\nsolution that bridges the gap between resource-intensive LLMs and practical\napplications. Our findings underscore the potential of distillation techniques\nin facilitating the deployment of sophisticated language models in real-world\nscenarios, thereby fostering advancements in natural language processing and\ndocument comprehension domains.\n", "link": "http://arxiv.org/abs/2409.11282v1", "date": "2024-09-17", "relevancy": 2.5939, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Distillation%20Techniques%20for%20Document%20Understanding%3A%20A%20Case%0A%20%20Study%20with%20FLAN-T5&body=Title%3A%20Leveraging%20Distillation%20Techniques%20for%20Document%20Understanding%3A%20A%20Case%0A%20%20Study%20with%20FLAN-T5%0AAuthor%3A%20Marcel%20Lamott%20and%20Muhammad%20Armaghan%20Shakir%0AAbstract%3A%20%20%20The%20surge%20of%20digital%20documents%20in%20various%20formats%2C%20including%20less%0Astandardized%20documents%20such%20as%20business%20reports%20and%20environmental%20assessments%2C%0Aunderscores%20the%20growing%20importance%20of%20Document%20Understanding.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20showcased%20prowess%20across%20diverse%20natural%20language%0Aprocessing%20tasks%2C%20their%20direct%20application%20to%20Document%20Understanding%20remains%20a%0Achallenge.%20Previous%20research%20has%20demonstrated%20the%20utility%20of%20LLMs%20in%20this%0Adomain%2C%20yet%20their%20significant%20computational%20demands%20make%20them%20challenging%20to%0Adeploy%20effectively.%20Additionally%2C%20proprietary%20Blackbox%20LLMs%20often%20outperform%0Atheir%20open-source%20counterparts%2C%20posing%20a%20barrier%20to%20widespread%20accessibility.%0AIn%20this%20paper%2C%20we%20delve%20into%20the%20realm%20of%20document%20understanding%2C%20leveraging%0Adistillation%20methods%20to%20harness%20the%20power%20of%20large%20LLMs%20while%20accommodating%0Acomputational%20limitations.%20Specifically%2C%20we%20present%20a%20novel%20approach%20wherein%20we%0Adistill%20document%20understanding%20knowledge%20from%20the%20proprietary%20LLM%20ChatGPT%20into%0AFLAN-T5.%20Our%20methodology%20integrates%20labeling%20and%20curriculum-learning%20mechanisms%0Ato%20facilitate%20efficient%20knowledge%20transfer.%20This%20work%20contributes%20to%20the%0Aadvancement%20of%20document%20understanding%20methodologies%20by%20offering%20a%20scalable%0Asolution%20that%20bridges%20the%20gap%20between%20resource-intensive%20LLMs%20and%20practical%0Aapplications.%20Our%20findings%20underscore%20the%20potential%20of%20distillation%20techniques%0Ain%20facilitating%20the%20deployment%20of%20sophisticated%20language%20models%20in%20real-world%0Ascenarios%2C%20thereby%20fostering%20advancements%20in%20natural%20language%20processing%20and%0Adocument%20comprehension%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Distillation%2520Techniques%2520for%2520Document%2520Understanding%253A%2520A%2520Case%250A%2520%2520Study%2520with%2520FLAN-T5%26entry.906535625%3DMarcel%2520Lamott%2520and%2520Muhammad%2520Armaghan%2520Shakir%26entry.1292438233%3D%2520%2520The%2520surge%2520of%2520digital%2520documents%2520in%2520various%2520formats%252C%2520including%2520less%250Astandardized%2520documents%2520such%2520as%2520business%2520reports%2520and%2520environmental%2520assessments%252C%250Aunderscores%2520the%2520growing%2520importance%2520of%2520Document%2520Understanding.%2520While%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520have%2520showcased%2520prowess%2520across%2520diverse%2520natural%2520language%250Aprocessing%2520tasks%252C%2520their%2520direct%2520application%2520to%2520Document%2520Understanding%2520remains%2520a%250Achallenge.%2520Previous%2520research%2520has%2520demonstrated%2520the%2520utility%2520of%2520LLMs%2520in%2520this%250Adomain%252C%2520yet%2520their%2520significant%2520computational%2520demands%2520make%2520them%2520challenging%2520to%250Adeploy%2520effectively.%2520Additionally%252C%2520proprietary%2520Blackbox%2520LLMs%2520often%2520outperform%250Atheir%2520open-source%2520counterparts%252C%2520posing%2520a%2520barrier%2520to%2520widespread%2520accessibility.%250AIn%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%2520realm%2520of%2520document%2520understanding%252C%2520leveraging%250Adistillation%2520methods%2520to%2520harness%2520the%2520power%2520of%2520large%2520LLMs%2520while%2520accommodating%250Acomputational%2520limitations.%2520Specifically%252C%2520we%2520present%2520a%2520novel%2520approach%2520wherein%2520we%250Adistill%2520document%2520understanding%2520knowledge%2520from%2520the%2520proprietary%2520LLM%2520ChatGPT%2520into%250AFLAN-T5.%2520Our%2520methodology%2520integrates%2520labeling%2520and%2520curriculum-learning%2520mechanisms%250Ato%2520facilitate%2520efficient%2520knowledge%2520transfer.%2520This%2520work%2520contributes%2520to%2520the%250Aadvancement%2520of%2520document%2520understanding%2520methodologies%2520by%2520offering%2520a%2520scalable%250Asolution%2520that%2520bridges%2520the%2520gap%2520between%2520resource-intensive%2520LLMs%2520and%2520practical%250Aapplications.%2520Our%2520findings%2520underscore%2520the%2520potential%2520of%2520distillation%2520techniques%250Ain%2520facilitating%2520the%2520deployment%2520of%2520sophisticated%2520language%2520models%2520in%2520real-world%250Ascenarios%252C%2520thereby%2520fostering%2520advancements%2520in%2520natural%2520language%2520processing%2520and%250Adocument%2520comprehension%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Distillation%20Techniques%20for%20Document%20Understanding%3A%20A%20Case%0A%20%20Study%20with%20FLAN-T5&entry.906535625=Marcel%20Lamott%20and%20Muhammad%20Armaghan%20Shakir&entry.1292438233=%20%20The%20surge%20of%20digital%20documents%20in%20various%20formats%2C%20including%20less%0Astandardized%20documents%20such%20as%20business%20reports%20and%20environmental%20assessments%2C%0Aunderscores%20the%20growing%20importance%20of%20Document%20Understanding.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20showcased%20prowess%20across%20diverse%20natural%20language%0Aprocessing%20tasks%2C%20their%20direct%20application%20to%20Document%20Understanding%20remains%20a%0Achallenge.%20Previous%20research%20has%20demonstrated%20the%20utility%20of%20LLMs%20in%20this%0Adomain%2C%20yet%20their%20significant%20computational%20demands%20make%20them%20challenging%20to%0Adeploy%20effectively.%20Additionally%2C%20proprietary%20Blackbox%20LLMs%20often%20outperform%0Atheir%20open-source%20counterparts%2C%20posing%20a%20barrier%20to%20widespread%20accessibility.%0AIn%20this%20paper%2C%20we%20delve%20into%20the%20realm%20of%20document%20understanding%2C%20leveraging%0Adistillation%20methods%20to%20harness%20the%20power%20of%20large%20LLMs%20while%20accommodating%0Acomputational%20limitations.%20Specifically%2C%20we%20present%20a%20novel%20approach%20wherein%20we%0Adistill%20document%20understanding%20knowledge%20from%20the%20proprietary%20LLM%20ChatGPT%20into%0AFLAN-T5.%20Our%20methodology%20integrates%20labeling%20and%20curriculum-learning%20mechanisms%0Ato%20facilitate%20efficient%20knowledge%20transfer.%20This%20work%20contributes%20to%20the%0Aadvancement%20of%20document%20understanding%20methodologies%20by%20offering%20a%20scalable%0Asolution%20that%20bridges%20the%20gap%20between%20resource-intensive%20LLMs%20and%20practical%0Aapplications.%20Our%20findings%20underscore%20the%20potential%20of%20distillation%20techniques%0Ain%20facilitating%20the%20deployment%20of%20sophisticated%20language%20models%20in%20real-world%0Ascenarios%2C%20thereby%20fostering%20advancements%20in%20natural%20language%20processing%20and%0Adocument%20comprehension%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11282v1&entry.124074799=Read"},
{"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks", "author": "Georgios Chochlakis and Niyantha Maruthu Pandiyan and Kristina Lerman and Shrikanth Narayanan", "abstract": "  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.\n", "link": "http://arxiv.org/abs/2409.06173v2", "date": "2024-09-17", "relevancy": 2.59, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Larger%20Language%20Models%20Don%27t%20Care%20How%20You%20Think%3A%20Why%20Chain-of-Thought%0A%20%20Prompting%20Fails%20in%20Subjective%20Tasks&body=Title%3A%20Larger%20Language%20Models%20Don%27t%20Care%20How%20You%20Think%3A%20Why%20Chain-of-Thought%0A%20%20Prompting%20Fails%20in%20Subjective%20Tasks%0AAuthor%3A%20Georgios%20Chochlakis%20and%20Niyantha%20Maruthu%20Pandiyan%20and%20Kristina%20Lerman%20and%20Shrikanth%20Narayanan%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20in%20Large%20Language%20Models%20%28LLM%29%20has%20emerged%20as%20the%0Adominant%20technique%20for%20performing%20natural%20language%20tasks%2C%20as%20it%20does%20not%0Arequire%20updating%20the%20model%20parameters%20with%20gradient-based%20methods.%20ICL%20promises%0Ato%20%22adapt%22%20the%20LLM%20to%20perform%20the%20present%20task%20at%20a%20competitive%20or%0Astate-of-the-art%20level%20at%20a%20fraction%20of%20the%20computational%20cost.%20ICL%20can%20be%0Aaugmented%20by%20incorporating%20the%20reasoning%20process%20to%20arrive%20at%20the%20final%20label%0Aexplicitly%20in%20the%20prompt%2C%20a%20technique%20called%20Chain-of-Thought%20%28CoT%29%20prompting.%0AHowever%2C%20recent%20work%20has%20found%20that%20ICL%20relies%20mostly%20on%20the%20retrieval%20of%20task%0Apriors%20and%20less%20so%20on%20%22learning%22%20to%20perform%20tasks%2C%20especially%20for%20complex%0Asubjective%20domains%20like%20emotion%20and%20morality%2C%20where%20priors%20ossify%20posterior%0Apredictions.%20In%20this%20work%2C%20we%20examine%20whether%20%22enabling%22%20reasoning%20also%20creates%0Athe%20same%20behavior%20in%20LLMs%2C%20wherein%20the%20format%20of%20CoT%20retrieves%20reasoning%20priors%0Athat%20remain%20relatively%20unchanged%20despite%20the%20evidence%20in%20the%20prompt.%20We%20find%0Athat%2C%20surprisingly%2C%20CoT%20indeed%20suffers%20from%20the%20same%20posterior%20collapse%20as%20ICL%0Afor%20larger%20language%20models.%20Code%20is%20avalaible%20at%0Ahttps%3A//github.com/gchochla/cot-priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarger%2520Language%2520Models%2520Don%2527t%2520Care%2520How%2520You%2520Think%253A%2520Why%2520Chain-of-Thought%250A%2520%2520Prompting%2520Fails%2520in%2520Subjective%2520Tasks%26entry.906535625%3DGeorgios%2520Chochlakis%2520and%2520Niyantha%2520Maruthu%2520Pandiyan%2520and%2520Kristina%2520Lerman%2520and%2520Shrikanth%2520Narayanan%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520in%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520has%2520emerged%2520as%2520the%250Adominant%2520technique%2520for%2520performing%2520natural%2520language%2520tasks%252C%2520as%2520it%2520does%2520not%250Arequire%2520updating%2520the%2520model%2520parameters%2520with%2520gradient-based%2520methods.%2520ICL%2520promises%250Ato%2520%2522adapt%2522%2520the%2520LLM%2520to%2520perform%2520the%2520present%2520task%2520at%2520a%2520competitive%2520or%250Astate-of-the-art%2520level%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520cost.%2520ICL%2520can%2520be%250Aaugmented%2520by%2520incorporating%2520the%2520reasoning%2520process%2520to%2520arrive%2520at%2520the%2520final%2520label%250Aexplicitly%2520in%2520the%2520prompt%252C%2520a%2520technique%2520called%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting.%250AHowever%252C%2520recent%2520work%2520has%2520found%2520that%2520ICL%2520relies%2520mostly%2520on%2520the%2520retrieval%2520of%2520task%250Apriors%2520and%2520less%2520so%2520on%2520%2522learning%2522%2520to%2520perform%2520tasks%252C%2520especially%2520for%2520complex%250Asubjective%2520domains%2520like%2520emotion%2520and%2520morality%252C%2520where%2520priors%2520ossify%2520posterior%250Apredictions.%2520In%2520this%2520work%252C%2520we%2520examine%2520whether%2520%2522enabling%2522%2520reasoning%2520also%2520creates%250Athe%2520same%2520behavior%2520in%2520LLMs%252C%2520wherein%2520the%2520format%2520of%2520CoT%2520retrieves%2520reasoning%2520priors%250Athat%2520remain%2520relatively%2520unchanged%2520despite%2520the%2520evidence%2520in%2520the%2520prompt.%2520We%2520find%250Athat%252C%2520surprisingly%252C%2520CoT%2520indeed%2520suffers%2520from%2520the%2520same%2520posterior%2520collapse%2520as%2520ICL%250Afor%2520larger%2520language%2520models.%2520Code%2520is%2520avalaible%2520at%250Ahttps%253A//github.com/gchochla/cot-priors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Larger%20Language%20Models%20Don%27t%20Care%20How%20You%20Think%3A%20Why%20Chain-of-Thought%0A%20%20Prompting%20Fails%20in%20Subjective%20Tasks&entry.906535625=Georgios%20Chochlakis%20and%20Niyantha%20Maruthu%20Pandiyan%20and%20Kristina%20Lerman%20and%20Shrikanth%20Narayanan&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20in%20Large%20Language%20Models%20%28LLM%29%20has%20emerged%20as%20the%0Adominant%20technique%20for%20performing%20natural%20language%20tasks%2C%20as%20it%20does%20not%0Arequire%20updating%20the%20model%20parameters%20with%20gradient-based%20methods.%20ICL%20promises%0Ato%20%22adapt%22%20the%20LLM%20to%20perform%20the%20present%20task%20at%20a%20competitive%20or%0Astate-of-the-art%20level%20at%20a%20fraction%20of%20the%20computational%20cost.%20ICL%20can%20be%0Aaugmented%20by%20incorporating%20the%20reasoning%20process%20to%20arrive%20at%20the%20final%20label%0Aexplicitly%20in%20the%20prompt%2C%20a%20technique%20called%20Chain-of-Thought%20%28CoT%29%20prompting.%0AHowever%2C%20recent%20work%20has%20found%20that%20ICL%20relies%20mostly%20on%20the%20retrieval%20of%20task%0Apriors%20and%20less%20so%20on%20%22learning%22%20to%20perform%20tasks%2C%20especially%20for%20complex%0Asubjective%20domains%20like%20emotion%20and%20morality%2C%20where%20priors%20ossify%20posterior%0Apredictions.%20In%20this%20work%2C%20we%20examine%20whether%20%22enabling%22%20reasoning%20also%20creates%0Athe%20same%20behavior%20in%20LLMs%2C%20wherein%20the%20format%20of%20CoT%20retrieves%20reasoning%20priors%0Athat%20remain%20relatively%20unchanged%20despite%20the%20evidence%20in%20the%20prompt.%20We%20find%0Athat%2C%20surprisingly%2C%20CoT%20indeed%20suffers%20from%20the%20same%20posterior%20collapse%20as%20ICL%0Afor%20larger%20language%20models.%20Code%20is%20avalaible%20at%0Ahttps%3A//github.com/gchochla/cot-priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06173v2&entry.124074799=Read"},
{"title": "Phidias: A Generative Model for Creating 3D Content from Text, Image,\n  and 3D Conditions with Reference-Augmented Diffusion", "author": "Zhenwei Wang and Tengfei Wang and Zexin He and Gerhard Hancke and Ziwei Liu and Rynson W. H. Lau", "abstract": "  In 3D modeling, designers often use an existing 3D model as a reference to\ncreate new ones. This practice has inspired the development of Phidias, a novel\ngenerative model that uses diffusion for reference-augmented 3D generation.\nGiven an image, our method leverages a retrieved or user-provided 3D reference\nmodel to guide the generation process, thereby enhancing the generation\nquality, generalization ability, and controllability. Our model integrates\nthree key components: 1) meta-ControlNet that dynamically modulates the\nconditioning strength, 2) dynamic reference routing that mitigates misalignment\nbetween the input image and 3D reference, and 3) self-reference augmentations\nthat enable self-supervised training with a progressive curriculum.\nCollectively, these designs result in a clear improvement over existing\nmethods. Phidias establishes a unified framework for 3D generation using text,\nimage, and 3D conditions with versatile applications.\n", "link": "http://arxiv.org/abs/2409.11406v1", "date": "2024-09-17", "relevancy": 2.5529, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6411}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6411}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phidias%3A%20A%20Generative%20Model%20for%20Creating%203D%20Content%20from%20Text%2C%20Image%2C%0A%20%20and%203D%20Conditions%20with%20Reference-Augmented%20Diffusion&body=Title%3A%20Phidias%3A%20A%20Generative%20Model%20for%20Creating%203D%20Content%20from%20Text%2C%20Image%2C%0A%20%20and%203D%20Conditions%20with%20Reference-Augmented%20Diffusion%0AAuthor%3A%20Zhenwei%20Wang%20and%20Tengfei%20Wang%20and%20Zexin%20He%20and%20Gerhard%20Hancke%20and%20Ziwei%20Liu%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20In%203D%20modeling%2C%20designers%20often%20use%20an%20existing%203D%20model%20as%20a%20reference%20to%0Acreate%20new%20ones.%20This%20practice%20has%20inspired%20the%20development%20of%20Phidias%2C%20a%20novel%0Agenerative%20model%20that%20uses%20diffusion%20for%20reference-augmented%203D%20generation.%0AGiven%20an%20image%2C%20our%20method%20leverages%20a%20retrieved%20or%20user-provided%203D%20reference%0Amodel%20to%20guide%20the%20generation%20process%2C%20thereby%20enhancing%20the%20generation%0Aquality%2C%20generalization%20ability%2C%20and%20controllability.%20Our%20model%20integrates%0Athree%20key%20components%3A%201%29%20meta-ControlNet%20that%20dynamically%20modulates%20the%0Aconditioning%20strength%2C%202%29%20dynamic%20reference%20routing%20that%20mitigates%20misalignment%0Abetween%20the%20input%20image%20and%203D%20reference%2C%20and%203%29%20self-reference%20augmentations%0Athat%20enable%20self-supervised%20training%20with%20a%20progressive%20curriculum.%0ACollectively%2C%20these%20designs%20result%20in%20a%20clear%20improvement%20over%20existing%0Amethods.%20Phidias%20establishes%20a%20unified%20framework%20for%203D%20generation%20using%20text%2C%0Aimage%2C%20and%203D%20conditions%20with%20versatile%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhidias%253A%2520A%2520Generative%2520Model%2520for%2520Creating%25203D%2520Content%2520from%2520Text%252C%2520Image%252C%250A%2520%2520and%25203D%2520Conditions%2520with%2520Reference-Augmented%2520Diffusion%26entry.906535625%3DZhenwei%2520Wang%2520and%2520Tengfei%2520Wang%2520and%2520Zexin%2520He%2520and%2520Gerhard%2520Hancke%2520and%2520Ziwei%2520Liu%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520In%25203D%2520modeling%252C%2520designers%2520often%2520use%2520an%2520existing%25203D%2520model%2520as%2520a%2520reference%2520to%250Acreate%2520new%2520ones.%2520This%2520practice%2520has%2520inspired%2520the%2520development%2520of%2520Phidias%252C%2520a%2520novel%250Agenerative%2520model%2520that%2520uses%2520diffusion%2520for%2520reference-augmented%25203D%2520generation.%250AGiven%2520an%2520image%252C%2520our%2520method%2520leverages%2520a%2520retrieved%2520or%2520user-provided%25203D%2520reference%250Amodel%2520to%2520guide%2520the%2520generation%2520process%252C%2520thereby%2520enhancing%2520the%2520generation%250Aquality%252C%2520generalization%2520ability%252C%2520and%2520controllability.%2520Our%2520model%2520integrates%250Athree%2520key%2520components%253A%25201%2529%2520meta-ControlNet%2520that%2520dynamically%2520modulates%2520the%250Aconditioning%2520strength%252C%25202%2529%2520dynamic%2520reference%2520routing%2520that%2520mitigates%2520misalignment%250Abetween%2520the%2520input%2520image%2520and%25203D%2520reference%252C%2520and%25203%2529%2520self-reference%2520augmentations%250Athat%2520enable%2520self-supervised%2520training%2520with%2520a%2520progressive%2520curriculum.%250ACollectively%252C%2520these%2520designs%2520result%2520in%2520a%2520clear%2520improvement%2520over%2520existing%250Amethods.%2520Phidias%2520establishes%2520a%2520unified%2520framework%2520for%25203D%2520generation%2520using%2520text%252C%250Aimage%252C%2520and%25203D%2520conditions%2520with%2520versatile%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phidias%3A%20A%20Generative%20Model%20for%20Creating%203D%20Content%20from%20Text%2C%20Image%2C%0A%20%20and%203D%20Conditions%20with%20Reference-Augmented%20Diffusion&entry.906535625=Zhenwei%20Wang%20and%20Tengfei%20Wang%20and%20Zexin%20He%20and%20Gerhard%20Hancke%20and%20Ziwei%20Liu%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20In%203D%20modeling%2C%20designers%20often%20use%20an%20existing%203D%20model%20as%20a%20reference%20to%0Acreate%20new%20ones.%20This%20practice%20has%20inspired%20the%20development%20of%20Phidias%2C%20a%20novel%0Agenerative%20model%20that%20uses%20diffusion%20for%20reference-augmented%203D%20generation.%0AGiven%20an%20image%2C%20our%20method%20leverages%20a%20retrieved%20or%20user-provided%203D%20reference%0Amodel%20to%20guide%20the%20generation%20process%2C%20thereby%20enhancing%20the%20generation%0Aquality%2C%20generalization%20ability%2C%20and%20controllability.%20Our%20model%20integrates%0Athree%20key%20components%3A%201%29%20meta-ControlNet%20that%20dynamically%20modulates%20the%0Aconditioning%20strength%2C%202%29%20dynamic%20reference%20routing%20that%20mitigates%20misalignment%0Abetween%20the%20input%20image%20and%203D%20reference%2C%20and%203%29%20self-reference%20augmentations%0Athat%20enable%20self-supervised%20training%20with%20a%20progressive%20curriculum.%0ACollectively%2C%20these%20designs%20result%20in%20a%20clear%20improvement%20over%20existing%0Amethods.%20Phidias%20establishes%20a%20unified%20framework%20for%203D%20generation%20using%20text%2C%0Aimage%2C%20and%203D%20conditions%20with%20versatile%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11406v1&entry.124074799=Read"},
{"title": "LC-Protonets: Multi-label Few-shot learning for world music audio\n  tagging", "author": "Charilaos Papaioannou and Emmanouil Benetos and Alexandros Potamianos", "abstract": "  We introduce Label-Combination Prototypical Networks (LC-Protonets) to\naddress the problem of multi-label few-shot classification, where a model must\ngeneralize to new classes based on only a few available examples. Extending\nPrototypical Networks, LC-Protonets generate one prototype per label\ncombination, derived from the power set of labels present in the limited\ntraining items, rather than one prototype per label. Our method is applied to\nautomatic audio tagging across diverse music datasets, covering various\ncultures and including both modern and traditional music, and is evaluated\nagainst existing approaches in the literature. The results demonstrate a\nsignificant performance improvement in almost all domains and training setups\nwhen using LC-Protonets for multi-label classification. In addition to training\na few-shot learning model from scratch, we explore the use of a pre-trained\nmodel, obtained via supervised learning, to embed items in the feature space.\nFine-tuning improves the generalization ability of all methods, yet\nLC-Protonets achieve high-level performance even without fine-tuning, in\ncontrast to the comparative approaches. We finally analyze the scalability of\nthe proposed method, providing detailed quantitative metrics from our\nexperiments. The implementation and experimental setup are made publicly\navailable, offering a benchmark for future research.\n", "link": "http://arxiv.org/abs/2409.11264v1", "date": "2024-09-17", "relevancy": 2.5323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LC-Protonets%3A%20Multi-label%20Few-shot%20learning%20for%20world%20music%20audio%0A%20%20tagging&body=Title%3A%20LC-Protonets%3A%20Multi-label%20Few-shot%20learning%20for%20world%20music%20audio%0A%20%20tagging%0AAuthor%3A%20Charilaos%20Papaioannou%20and%20Emmanouil%20Benetos%20and%20Alexandros%20Potamianos%0AAbstract%3A%20%20%20We%20introduce%20Label-Combination%20Prototypical%20Networks%20%28LC-Protonets%29%20to%0Aaddress%20the%20problem%20of%20multi-label%20few-shot%20classification%2C%20where%20a%20model%20must%0Ageneralize%20to%20new%20classes%20based%20on%20only%20a%20few%20available%20examples.%20Extending%0APrototypical%20Networks%2C%20LC-Protonets%20generate%20one%20prototype%20per%20label%0Acombination%2C%20derived%20from%20the%20power%20set%20of%20labels%20present%20in%20the%20limited%0Atraining%20items%2C%20rather%20than%20one%20prototype%20per%20label.%20Our%20method%20is%20applied%20to%0Aautomatic%20audio%20tagging%20across%20diverse%20music%20datasets%2C%20covering%20various%0Acultures%20and%20including%20both%20modern%20and%20traditional%20music%2C%20and%20is%20evaluated%0Aagainst%20existing%20approaches%20in%20the%20literature.%20The%20results%20demonstrate%20a%0Asignificant%20performance%20improvement%20in%20almost%20all%20domains%20and%20training%20setups%0Awhen%20using%20LC-Protonets%20for%20multi-label%20classification.%20In%20addition%20to%20training%0Aa%20few-shot%20learning%20model%20from%20scratch%2C%20we%20explore%20the%20use%20of%20a%20pre-trained%0Amodel%2C%20obtained%20via%20supervised%20learning%2C%20to%20embed%20items%20in%20the%20feature%20space.%0AFine-tuning%20improves%20the%20generalization%20ability%20of%20all%20methods%2C%20yet%0ALC-Protonets%20achieve%20high-level%20performance%20even%20without%20fine-tuning%2C%20in%0Acontrast%20to%20the%20comparative%20approaches.%20We%20finally%20analyze%20the%20scalability%20of%0Athe%20proposed%20method%2C%20providing%20detailed%20quantitative%20metrics%20from%20our%0Aexperiments.%20The%20implementation%20and%20experimental%20setup%20are%20made%20publicly%0Aavailable%2C%20offering%20a%20benchmark%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLC-Protonets%253A%2520Multi-label%2520Few-shot%2520learning%2520for%2520world%2520music%2520audio%250A%2520%2520tagging%26entry.906535625%3DCharilaos%2520Papaioannou%2520and%2520Emmanouil%2520Benetos%2520and%2520Alexandros%2520Potamianos%26entry.1292438233%3D%2520%2520We%2520introduce%2520Label-Combination%2520Prototypical%2520Networks%2520%2528LC-Protonets%2529%2520to%250Aaddress%2520the%2520problem%2520of%2520multi-label%2520few-shot%2520classification%252C%2520where%2520a%2520model%2520must%250Ageneralize%2520to%2520new%2520classes%2520based%2520on%2520only%2520a%2520few%2520available%2520examples.%2520Extending%250APrototypical%2520Networks%252C%2520LC-Protonets%2520generate%2520one%2520prototype%2520per%2520label%250Acombination%252C%2520derived%2520from%2520the%2520power%2520set%2520of%2520labels%2520present%2520in%2520the%2520limited%250Atraining%2520items%252C%2520rather%2520than%2520one%2520prototype%2520per%2520label.%2520Our%2520method%2520is%2520applied%2520to%250Aautomatic%2520audio%2520tagging%2520across%2520diverse%2520music%2520datasets%252C%2520covering%2520various%250Acultures%2520and%2520including%2520both%2520modern%2520and%2520traditional%2520music%252C%2520and%2520is%2520evaluated%250Aagainst%2520existing%2520approaches%2520in%2520the%2520literature.%2520The%2520results%2520demonstrate%2520a%250Asignificant%2520performance%2520improvement%2520in%2520almost%2520all%2520domains%2520and%2520training%2520setups%250Awhen%2520using%2520LC-Protonets%2520for%2520multi-label%2520classification.%2520In%2520addition%2520to%2520training%250Aa%2520few-shot%2520learning%2520model%2520from%2520scratch%252C%2520we%2520explore%2520the%2520use%2520of%2520a%2520pre-trained%250Amodel%252C%2520obtained%2520via%2520supervised%2520learning%252C%2520to%2520embed%2520items%2520in%2520the%2520feature%2520space.%250AFine-tuning%2520improves%2520the%2520generalization%2520ability%2520of%2520all%2520methods%252C%2520yet%250ALC-Protonets%2520achieve%2520high-level%2520performance%2520even%2520without%2520fine-tuning%252C%2520in%250Acontrast%2520to%2520the%2520comparative%2520approaches.%2520We%2520finally%2520analyze%2520the%2520scalability%2520of%250Athe%2520proposed%2520method%252C%2520providing%2520detailed%2520quantitative%2520metrics%2520from%2520our%250Aexperiments.%2520The%2520implementation%2520and%2520experimental%2520setup%2520are%2520made%2520publicly%250Aavailable%252C%2520offering%2520a%2520benchmark%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LC-Protonets%3A%20Multi-label%20Few-shot%20learning%20for%20world%20music%20audio%0A%20%20tagging&entry.906535625=Charilaos%20Papaioannou%20and%20Emmanouil%20Benetos%20and%20Alexandros%20Potamianos&entry.1292438233=%20%20We%20introduce%20Label-Combination%20Prototypical%20Networks%20%28LC-Protonets%29%20to%0Aaddress%20the%20problem%20of%20multi-label%20few-shot%20classification%2C%20where%20a%20model%20must%0Ageneralize%20to%20new%20classes%20based%20on%20only%20a%20few%20available%20examples.%20Extending%0APrototypical%20Networks%2C%20LC-Protonets%20generate%20one%20prototype%20per%20label%0Acombination%2C%20derived%20from%20the%20power%20set%20of%20labels%20present%20in%20the%20limited%0Atraining%20items%2C%20rather%20than%20one%20prototype%20per%20label.%20Our%20method%20is%20applied%20to%0Aautomatic%20audio%20tagging%20across%20diverse%20music%20datasets%2C%20covering%20various%0Acultures%20and%20including%20both%20modern%20and%20traditional%20music%2C%20and%20is%20evaluated%0Aagainst%20existing%20approaches%20in%20the%20literature.%20The%20results%20demonstrate%20a%0Asignificant%20performance%20improvement%20in%20almost%20all%20domains%20and%20training%20setups%0Awhen%20using%20LC-Protonets%20for%20multi-label%20classification.%20In%20addition%20to%20training%0Aa%20few-shot%20learning%20model%20from%20scratch%2C%20we%20explore%20the%20use%20of%20a%20pre-trained%0Amodel%2C%20obtained%20via%20supervised%20learning%2C%20to%20embed%20items%20in%20the%20feature%20space.%0AFine-tuning%20improves%20the%20generalization%20ability%20of%20all%20methods%2C%20yet%0ALC-Protonets%20achieve%20high-level%20performance%20even%20without%20fine-tuning%2C%20in%0Acontrast%20to%20the%20comparative%20approaches.%20We%20finally%20analyze%20the%20scalability%20of%0Athe%20proposed%20method%2C%20providing%20detailed%20quantitative%20metrics%20from%20our%0Aexperiments.%20The%20implementation%20and%20experimental%20setup%20are%20made%20publicly%0Aavailable%2C%20offering%20a%20benchmark%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11264v1&entry.124074799=Read"},
{"title": "S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton\n  Sketching", "author": "Xue Wang and Tian Zhou and Jianqing Zhu and Jialin Liu and Kun Yuan and Tao Yao and Wotao Yin and Rong Jin and HanQin Cai", "abstract": "  Attention based models have achieved many remarkable breakthroughs in\nnumerous applications. However, the quadratic complexity of Attention makes the\nvanilla Attention based models hard to apply to long sequence tasks. Various\nimproved Attention structures are proposed to reduce the computation cost by\ninducing low rankness and approximating the whole sequence by sub-sequences.\nThe most challenging part of those approaches is maintaining the proper balance\nbetween information preservation and computation reduction: the longer\nsub-sequences used, the better information is preserved, but at the price of\nintroducing more noise and computational costs. In this paper, we propose a\nsmoothed skeleton sketching based Attention structure, coined S$^3$Attention,\nwhich significantly improves upon the previous attempts to negotiate this\ntrade-off. S$^3$Attention has two mechanisms to effectively minimize the impact\nof noise while keeping the linear complexity to the sequence length: a\nsmoothing block to mix information over long sequences and a matrix sketching\nmethod that simultaneously selects columns and rows from the input matrix. We\nverify the effectiveness of S$^3$Attention both theoretically and empirically.\nExtensive studies over Long Range Arena (LRA) datasets and six time-series\nforecasting show that S$^3$Attention significantly outperforms both vanilla\nAttention and other state-of-the-art variants of Attention structures.\n", "link": "http://arxiv.org/abs/2408.08567v3", "date": "2024-09-17", "relevancy": 2.5299, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5178}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5054}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%24%5E3%24Attention%3A%20Improving%20Long%20Sequence%20Attention%20with%20Smoothed%20Skeleton%0A%20%20Sketching&body=Title%3A%20S%24%5E3%24Attention%3A%20Improving%20Long%20Sequence%20Attention%20with%20Smoothed%20Skeleton%0A%20%20Sketching%0AAuthor%3A%20Xue%20Wang%20and%20Tian%20Zhou%20and%20Jianqing%20Zhu%20and%20Jialin%20Liu%20and%20Kun%20Yuan%20and%20Tao%20Yao%20and%20Wotao%20Yin%20and%20Rong%20Jin%20and%20HanQin%20Cai%0AAbstract%3A%20%20%20Attention%20based%20models%20have%20achieved%20many%20remarkable%20breakthroughs%20in%0Anumerous%20applications.%20However%2C%20the%20quadratic%20complexity%20of%20Attention%20makes%20the%0Avanilla%20Attention%20based%20models%20hard%20to%20apply%20to%20long%20sequence%20tasks.%20Various%0Aimproved%20Attention%20structures%20are%20proposed%20to%20reduce%20the%20computation%20cost%20by%0Ainducing%20low%20rankness%20and%20approximating%20the%20whole%20sequence%20by%20sub-sequences.%0AThe%20most%20challenging%20part%20of%20those%20approaches%20is%20maintaining%20the%20proper%20balance%0Abetween%20information%20preservation%20and%20computation%20reduction%3A%20the%20longer%0Asub-sequences%20used%2C%20the%20better%20information%20is%20preserved%2C%20but%20at%20the%20price%20of%0Aintroducing%20more%20noise%20and%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20a%0Asmoothed%20skeleton%20sketching%20based%20Attention%20structure%2C%20coined%20S%24%5E3%24Attention%2C%0Awhich%20significantly%20improves%20upon%20the%20previous%20attempts%20to%20negotiate%20this%0Atrade-off.%20S%24%5E3%24Attention%20has%20two%20mechanisms%20to%20effectively%20minimize%20the%20impact%0Aof%20noise%20while%20keeping%20the%20linear%20complexity%20to%20the%20sequence%20length%3A%20a%0Asmoothing%20block%20to%20mix%20information%20over%20long%20sequences%20and%20a%20matrix%20sketching%0Amethod%20that%20simultaneously%20selects%20columns%20and%20rows%20from%20the%20input%20matrix.%20We%0Averify%20the%20effectiveness%20of%20S%24%5E3%24Attention%20both%20theoretically%20and%20empirically.%0AExtensive%20studies%20over%20Long%20Range%20Arena%20%28LRA%29%20datasets%20and%20six%20time-series%0Aforecasting%20show%20that%20S%24%5E3%24Attention%20significantly%20outperforms%20both%20vanilla%0AAttention%20and%20other%20state-of-the-art%20variants%20of%20Attention%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08567v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%2524%255E3%2524Attention%253A%2520Improving%2520Long%2520Sequence%2520Attention%2520with%2520Smoothed%2520Skeleton%250A%2520%2520Sketching%26entry.906535625%3DXue%2520Wang%2520and%2520Tian%2520Zhou%2520and%2520Jianqing%2520Zhu%2520and%2520Jialin%2520Liu%2520and%2520Kun%2520Yuan%2520and%2520Tao%2520Yao%2520and%2520Wotao%2520Yin%2520and%2520Rong%2520Jin%2520and%2520HanQin%2520Cai%26entry.1292438233%3D%2520%2520Attention%2520based%2520models%2520have%2520achieved%2520many%2520remarkable%2520breakthroughs%2520in%250Anumerous%2520applications.%2520However%252C%2520the%2520quadratic%2520complexity%2520of%2520Attention%2520makes%2520the%250Avanilla%2520Attention%2520based%2520models%2520hard%2520to%2520apply%2520to%2520long%2520sequence%2520tasks.%2520Various%250Aimproved%2520Attention%2520structures%2520are%2520proposed%2520to%2520reduce%2520the%2520computation%2520cost%2520by%250Ainducing%2520low%2520rankness%2520and%2520approximating%2520the%2520whole%2520sequence%2520by%2520sub-sequences.%250AThe%2520most%2520challenging%2520part%2520of%2520those%2520approaches%2520is%2520maintaining%2520the%2520proper%2520balance%250Abetween%2520information%2520preservation%2520and%2520computation%2520reduction%253A%2520the%2520longer%250Asub-sequences%2520used%252C%2520the%2520better%2520information%2520is%2520preserved%252C%2520but%2520at%2520the%2520price%2520of%250Aintroducing%2520more%2520noise%2520and%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Asmoothed%2520skeleton%2520sketching%2520based%2520Attention%2520structure%252C%2520coined%2520S%2524%255E3%2524Attention%252C%250Awhich%2520significantly%2520improves%2520upon%2520the%2520previous%2520attempts%2520to%2520negotiate%2520this%250Atrade-off.%2520S%2524%255E3%2524Attention%2520has%2520two%2520mechanisms%2520to%2520effectively%2520minimize%2520the%2520impact%250Aof%2520noise%2520while%2520keeping%2520the%2520linear%2520complexity%2520to%2520the%2520sequence%2520length%253A%2520a%250Asmoothing%2520block%2520to%2520mix%2520information%2520over%2520long%2520sequences%2520and%2520a%2520matrix%2520sketching%250Amethod%2520that%2520simultaneously%2520selects%2520columns%2520and%2520rows%2520from%2520the%2520input%2520matrix.%2520We%250Averify%2520the%2520effectiveness%2520of%2520S%2524%255E3%2524Attention%2520both%2520theoretically%2520and%2520empirically.%250AExtensive%2520studies%2520over%2520Long%2520Range%2520Arena%2520%2528LRA%2529%2520datasets%2520and%2520six%2520time-series%250Aforecasting%2520show%2520that%2520S%2524%255E3%2524Attention%2520significantly%2520outperforms%2520both%2520vanilla%250AAttention%2520and%2520other%2520state-of-the-art%2520variants%2520of%2520Attention%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08567v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%24%5E3%24Attention%3A%20Improving%20Long%20Sequence%20Attention%20with%20Smoothed%20Skeleton%0A%20%20Sketching&entry.906535625=Xue%20Wang%20and%20Tian%20Zhou%20and%20Jianqing%20Zhu%20and%20Jialin%20Liu%20and%20Kun%20Yuan%20and%20Tao%20Yao%20and%20Wotao%20Yin%20and%20Rong%20Jin%20and%20HanQin%20Cai&entry.1292438233=%20%20Attention%20based%20models%20have%20achieved%20many%20remarkable%20breakthroughs%20in%0Anumerous%20applications.%20However%2C%20the%20quadratic%20complexity%20of%20Attention%20makes%20the%0Avanilla%20Attention%20based%20models%20hard%20to%20apply%20to%20long%20sequence%20tasks.%20Various%0Aimproved%20Attention%20structures%20are%20proposed%20to%20reduce%20the%20computation%20cost%20by%0Ainducing%20low%20rankness%20and%20approximating%20the%20whole%20sequence%20by%20sub-sequences.%0AThe%20most%20challenging%20part%20of%20those%20approaches%20is%20maintaining%20the%20proper%20balance%0Abetween%20information%20preservation%20and%20computation%20reduction%3A%20the%20longer%0Asub-sequences%20used%2C%20the%20better%20information%20is%20preserved%2C%20but%20at%20the%20price%20of%0Aintroducing%20more%20noise%20and%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20a%0Asmoothed%20skeleton%20sketching%20based%20Attention%20structure%2C%20coined%20S%24%5E3%24Attention%2C%0Awhich%20significantly%20improves%20upon%20the%20previous%20attempts%20to%20negotiate%20this%0Atrade-off.%20S%24%5E3%24Attention%20has%20two%20mechanisms%20to%20effectively%20minimize%20the%20impact%0Aof%20noise%20while%20keeping%20the%20linear%20complexity%20to%20the%20sequence%20length%3A%20a%0Asmoothing%20block%20to%20mix%20information%20over%20long%20sequences%20and%20a%20matrix%20sketching%0Amethod%20that%20simultaneously%20selects%20columns%20and%20rows%20from%20the%20input%20matrix.%20We%0Averify%20the%20effectiveness%20of%20S%24%5E3%24Attention%20both%20theoretically%20and%20empirically.%0AExtensive%20studies%20over%20Long%20Range%20Arena%20%28LRA%29%20datasets%20and%20six%20time-series%0Aforecasting%20show%20that%20S%24%5E3%24Attention%20significantly%20outperforms%20both%20vanilla%0AAttention%20and%20other%20state-of-the-art%20variants%20of%20Attention%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08567v3&entry.124074799=Read"},
{"title": "OmniGen: Unified Image Generation", "author": "Shitao Xiao and Yueze Wang and Junjie Zhou and Huaying Yuan and Xingrun Xing and Ruiran Yan and Shuting Wang and Tiejun Huang and Zheng Liu", "abstract": "  In this work, we introduce OmniGen, a new diffusion model for unified image\ngeneration. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen\nno longer requires additional modules such as ControlNet or IP-Adapter to\nprocess diverse control conditions. OmniGenis characterized by the following\nfeatures: 1) Unification: OmniGen not only demonstrates text-to-image\ngeneration capabilities but also inherently supports other downstream tasks,\nsuch as image editing, subject-driven generation, and visual-conditional\ngeneration. Additionally, OmniGen can handle classical computer vision tasks by\ntransforming them into image generation tasks, such as edge detection and human\npose recognition. 2) Simplicity: The architecture of OmniGen is highly\nsimplified, eliminating the need for additional text encoders. Moreover, it is\nmore user-friendly compared to existing diffusion models, enabling complex\ntasks to be accomplished through instructions without the need for extra\npreprocessing steps (e.g., human pose estimation), thereby significantly\nsimplifying the workflow of image generation. 3) Knowledge Transfer: Through\nlearning in a unified format, OmniGen effectively transfers knowledge across\ndifferent tasks, manages unseen tasks and domains, and exhibits novel\ncapabilities. We also explore the model's reasoning capabilities and potential\napplications of chain-of-thought mechanism. This work represents the first\nattempt at a general-purpose image generation model, and there remain several\nunresolved issues. We will open-source the related resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.\n", "link": "http://arxiv.org/abs/2409.11340v1", "date": "2024-09-17", "relevancy": 2.5183, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6454}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6343}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniGen%3A%20Unified%20Image%20Generation&body=Title%3A%20OmniGen%3A%20Unified%20Image%20Generation%0AAuthor%3A%20Shitao%20Xiao%20and%20Yueze%20Wang%20and%20Junjie%20Zhou%20and%20Huaying%20Yuan%20and%20Xingrun%20Xing%20and%20Ruiran%20Yan%20and%20Shuting%20Wang%20and%20Tiejun%20Huang%20and%20Zheng%20Liu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20OmniGen%2C%20a%20new%20diffusion%20model%20for%20unified%20image%0Ageneration.%20Unlike%20popular%20diffusion%20models%20%28e.g.%2C%20Stable%20Diffusion%29%2C%20OmniGen%0Ano%20longer%20requires%20additional%20modules%20such%20as%20ControlNet%20or%20IP-Adapter%20to%0Aprocess%20diverse%20control%20conditions.%20OmniGenis%20characterized%20by%20the%20following%0Afeatures%3A%201%29%20Unification%3A%20OmniGen%20not%20only%20demonstrates%20text-to-image%0Ageneration%20capabilities%20but%20also%20inherently%20supports%20other%20downstream%20tasks%2C%0Asuch%20as%20image%20editing%2C%20subject-driven%20generation%2C%20and%20visual-conditional%0Ageneration.%20Additionally%2C%20OmniGen%20can%20handle%20classical%20computer%20vision%20tasks%20by%0Atransforming%20them%20into%20image%20generation%20tasks%2C%20such%20as%20edge%20detection%20and%20human%0Apose%20recognition.%202%29%20Simplicity%3A%20The%20architecture%20of%20OmniGen%20is%20highly%0Asimplified%2C%20eliminating%20the%20need%20for%20additional%20text%20encoders.%20Moreover%2C%20it%20is%0Amore%20user-friendly%20compared%20to%20existing%20diffusion%20models%2C%20enabling%20complex%0Atasks%20to%20be%20accomplished%20through%20instructions%20without%20the%20need%20for%20extra%0Apreprocessing%20steps%20%28e.g.%2C%20human%20pose%20estimation%29%2C%20thereby%20significantly%0Asimplifying%20the%20workflow%20of%20image%20generation.%203%29%20Knowledge%20Transfer%3A%20Through%0Alearning%20in%20a%20unified%20format%2C%20OmniGen%20effectively%20transfers%20knowledge%20across%0Adifferent%20tasks%2C%20manages%20unseen%20tasks%20and%20domains%2C%20and%20exhibits%20novel%0Acapabilities.%20We%20also%20explore%20the%20model%27s%20reasoning%20capabilities%20and%20potential%0Aapplications%20of%20chain-of-thought%20mechanism.%20This%20work%20represents%20the%20first%0Aattempt%20at%20a%20general-purpose%20image%20generation%20model%2C%20and%20there%20remain%20several%0Aunresolved%20issues.%20We%20will%20open-source%20the%20related%20resources%20at%0Ahttps%3A//github.com/VectorSpaceLab/OmniGen%20to%20foster%20advancements%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniGen%253A%2520Unified%2520Image%2520Generation%26entry.906535625%3DShitao%2520Xiao%2520and%2520Yueze%2520Wang%2520and%2520Junjie%2520Zhou%2520and%2520Huaying%2520Yuan%2520and%2520Xingrun%2520Xing%2520and%2520Ruiran%2520Yan%2520and%2520Shuting%2520Wang%2520and%2520Tiejun%2520Huang%2520and%2520Zheng%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520OmniGen%252C%2520a%2520new%2520diffusion%2520model%2520for%2520unified%2520image%250Ageneration.%2520Unlike%2520popular%2520diffusion%2520models%2520%2528e.g.%252C%2520Stable%2520Diffusion%2529%252C%2520OmniGen%250Ano%2520longer%2520requires%2520additional%2520modules%2520such%2520as%2520ControlNet%2520or%2520IP-Adapter%2520to%250Aprocess%2520diverse%2520control%2520conditions.%2520OmniGenis%2520characterized%2520by%2520the%2520following%250Afeatures%253A%25201%2529%2520Unification%253A%2520OmniGen%2520not%2520only%2520demonstrates%2520text-to-image%250Ageneration%2520capabilities%2520but%2520also%2520inherently%2520supports%2520other%2520downstream%2520tasks%252C%250Asuch%2520as%2520image%2520editing%252C%2520subject-driven%2520generation%252C%2520and%2520visual-conditional%250Ageneration.%2520Additionally%252C%2520OmniGen%2520can%2520handle%2520classical%2520computer%2520vision%2520tasks%2520by%250Atransforming%2520them%2520into%2520image%2520generation%2520tasks%252C%2520such%2520as%2520edge%2520detection%2520and%2520human%250Apose%2520recognition.%25202%2529%2520Simplicity%253A%2520The%2520architecture%2520of%2520OmniGen%2520is%2520highly%250Asimplified%252C%2520eliminating%2520the%2520need%2520for%2520additional%2520text%2520encoders.%2520Moreover%252C%2520it%2520is%250Amore%2520user-friendly%2520compared%2520to%2520existing%2520diffusion%2520models%252C%2520enabling%2520complex%250Atasks%2520to%2520be%2520accomplished%2520through%2520instructions%2520without%2520the%2520need%2520for%2520extra%250Apreprocessing%2520steps%2520%2528e.g.%252C%2520human%2520pose%2520estimation%2529%252C%2520thereby%2520significantly%250Asimplifying%2520the%2520workflow%2520of%2520image%2520generation.%25203%2529%2520Knowledge%2520Transfer%253A%2520Through%250Alearning%2520in%2520a%2520unified%2520format%252C%2520OmniGen%2520effectively%2520transfers%2520knowledge%2520across%250Adifferent%2520tasks%252C%2520manages%2520unseen%2520tasks%2520and%2520domains%252C%2520and%2520exhibits%2520novel%250Acapabilities.%2520We%2520also%2520explore%2520the%2520model%2527s%2520reasoning%2520capabilities%2520and%2520potential%250Aapplications%2520of%2520chain-of-thought%2520mechanism.%2520This%2520work%2520represents%2520the%2520first%250Aattempt%2520at%2520a%2520general-purpose%2520image%2520generation%2520model%252C%2520and%2520there%2520remain%2520several%250Aunresolved%2520issues.%2520We%2520will%2520open-source%2520the%2520related%2520resources%2520at%250Ahttps%253A//github.com/VectorSpaceLab/OmniGen%2520to%2520foster%2520advancements%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniGen%3A%20Unified%20Image%20Generation&entry.906535625=Shitao%20Xiao%20and%20Yueze%20Wang%20and%20Junjie%20Zhou%20and%20Huaying%20Yuan%20and%20Xingrun%20Xing%20and%20Ruiran%20Yan%20and%20Shuting%20Wang%20and%20Tiejun%20Huang%20and%20Zheng%20Liu&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20OmniGen%2C%20a%20new%20diffusion%20model%20for%20unified%20image%0Ageneration.%20Unlike%20popular%20diffusion%20models%20%28e.g.%2C%20Stable%20Diffusion%29%2C%20OmniGen%0Ano%20longer%20requires%20additional%20modules%20such%20as%20ControlNet%20or%20IP-Adapter%20to%0Aprocess%20diverse%20control%20conditions.%20OmniGenis%20characterized%20by%20the%20following%0Afeatures%3A%201%29%20Unification%3A%20OmniGen%20not%20only%20demonstrates%20text-to-image%0Ageneration%20capabilities%20but%20also%20inherently%20supports%20other%20downstream%20tasks%2C%0Asuch%20as%20image%20editing%2C%20subject-driven%20generation%2C%20and%20visual-conditional%0Ageneration.%20Additionally%2C%20OmniGen%20can%20handle%20classical%20computer%20vision%20tasks%20by%0Atransforming%20them%20into%20image%20generation%20tasks%2C%20such%20as%20edge%20detection%20and%20human%0Apose%20recognition.%202%29%20Simplicity%3A%20The%20architecture%20of%20OmniGen%20is%20highly%0Asimplified%2C%20eliminating%20the%20need%20for%20additional%20text%20encoders.%20Moreover%2C%20it%20is%0Amore%20user-friendly%20compared%20to%20existing%20diffusion%20models%2C%20enabling%20complex%0Atasks%20to%20be%20accomplished%20through%20instructions%20without%20the%20need%20for%20extra%0Apreprocessing%20steps%20%28e.g.%2C%20human%20pose%20estimation%29%2C%20thereby%20significantly%0Asimplifying%20the%20workflow%20of%20image%20generation.%203%29%20Knowledge%20Transfer%3A%20Through%0Alearning%20in%20a%20unified%20format%2C%20OmniGen%20effectively%20transfers%20knowledge%20across%0Adifferent%20tasks%2C%20manages%20unseen%20tasks%20and%20domains%2C%20and%20exhibits%20novel%0Acapabilities.%20We%20also%20explore%20the%20model%27s%20reasoning%20capabilities%20and%20potential%0Aapplications%20of%20chain-of-thought%20mechanism.%20This%20work%20represents%20the%20first%0Aattempt%20at%20a%20general-purpose%20image%20generation%20model%2C%20and%20there%20remain%20several%0Aunresolved%20issues.%20We%20will%20open-source%20the%20related%20resources%20at%0Ahttps%3A//github.com/VectorSpaceLab/OmniGen%20to%20foster%20advancements%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11340v1&entry.124074799=Read"},
{"title": "LOLA -- An Open-Source Massively Multilingual Large Language Model", "author": "Nikit Srivastava and Denis Kuchelev and Tatiana Moteu and Kshitij Shetty and Michael Roeder and Diego Moussallem and Hamada Zahera and Axel-Cyrille Ngonga Ngomo", "abstract": "  This paper presents LOLA, a massively multilingual large language model\ntrained on more than 160 languages using a sparse Mixture-of-Experts\nTransformer architecture. Our architectural and implementation choices address\nthe challenge of harnessing linguistic diversity while maintaining efficiency\nand avoiding the common pitfalls of multilinguality. Our analysis of the\nevaluation results shows competitive performance in natural language generation\nand understanding tasks. Additionally, we demonstrate how the learned\nexpert-routing mechanism exploits implicit phylogenetic linguistic patterns to\npotentially alleviate the curse of multilinguality. We provide an in-depth look\nat the training process, an analysis of the datasets, and a balanced\nexploration of the model's strengths and limitations. As an open-source model,\nLOLA promotes reproducibility and serves as a robust foundation for future\nresearch. Our findings enable the development of compute-efficient multilingual\nmodels with strong, scalable performance across languages.\n", "link": "http://arxiv.org/abs/2409.11272v1", "date": "2024-09-17", "relevancy": 2.5153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model&body=Title%3A%20LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model%0AAuthor%3A%20Nikit%20Srivastava%20and%20Denis%20Kuchelev%20and%20Tatiana%20Moteu%20and%20Kshitij%20Shetty%20and%20Michael%20Roeder%20and%20Diego%20Moussallem%20and%20Hamada%20Zahera%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20This%20paper%20presents%20LOLA%2C%20a%20massively%20multilingual%20large%20language%20model%0Atrained%20on%20more%20than%20160%20languages%20using%20a%20sparse%20Mixture-of-Experts%0ATransformer%20architecture.%20Our%20architectural%20and%20implementation%20choices%20address%0Athe%20challenge%20of%20harnessing%20linguistic%20diversity%20while%20maintaining%20efficiency%0Aand%20avoiding%20the%20common%20pitfalls%20of%20multilinguality.%20Our%20analysis%20of%20the%0Aevaluation%20results%20shows%20competitive%20performance%20in%20natural%20language%20generation%0Aand%20understanding%20tasks.%20Additionally%2C%20we%20demonstrate%20how%20the%20learned%0Aexpert-routing%20mechanism%20exploits%20implicit%20phylogenetic%20linguistic%20patterns%20to%0Apotentially%20alleviate%20the%20curse%20of%20multilinguality.%20We%20provide%20an%20in-depth%20look%0Aat%20the%20training%20process%2C%20an%20analysis%20of%20the%20datasets%2C%20and%20a%20balanced%0Aexploration%20of%20the%20model%27s%20strengths%20and%20limitations.%20As%20an%20open-source%20model%2C%0ALOLA%20promotes%20reproducibility%20and%20serves%20as%20a%20robust%20foundation%20for%20future%0Aresearch.%20Our%20findings%20enable%20the%20development%20of%20compute-efficient%20multilingual%0Amodels%20with%20strong%2C%20scalable%20performance%20across%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOLA%2520--%2520An%2520Open-Source%2520Massively%2520Multilingual%2520Large%2520Language%2520Model%26entry.906535625%3DNikit%2520Srivastava%2520and%2520Denis%2520Kuchelev%2520and%2520Tatiana%2520Moteu%2520and%2520Kshitij%2520Shetty%2520and%2520Michael%2520Roeder%2520and%2520Diego%2520Moussallem%2520and%2520Hamada%2520Zahera%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LOLA%252C%2520a%2520massively%2520multilingual%2520large%2520language%2520model%250Atrained%2520on%2520more%2520than%2520160%2520languages%2520using%2520a%2520sparse%2520Mixture-of-Experts%250ATransformer%2520architecture.%2520Our%2520architectural%2520and%2520implementation%2520choices%2520address%250Athe%2520challenge%2520of%2520harnessing%2520linguistic%2520diversity%2520while%2520maintaining%2520efficiency%250Aand%2520avoiding%2520the%2520common%2520pitfalls%2520of%2520multilinguality.%2520Our%2520analysis%2520of%2520the%250Aevaluation%2520results%2520shows%2520competitive%2520performance%2520in%2520natural%2520language%2520generation%250Aand%2520understanding%2520tasks.%2520Additionally%252C%2520we%2520demonstrate%2520how%2520the%2520learned%250Aexpert-routing%2520mechanism%2520exploits%2520implicit%2520phylogenetic%2520linguistic%2520patterns%2520to%250Apotentially%2520alleviate%2520the%2520curse%2520of%2520multilinguality.%2520We%2520provide%2520an%2520in-depth%2520look%250Aat%2520the%2520training%2520process%252C%2520an%2520analysis%2520of%2520the%2520datasets%252C%2520and%2520a%2520balanced%250Aexploration%2520of%2520the%2520model%2527s%2520strengths%2520and%2520limitations.%2520As%2520an%2520open-source%2520model%252C%250ALOLA%2520promotes%2520reproducibility%2520and%2520serves%2520as%2520a%2520robust%2520foundation%2520for%2520future%250Aresearch.%2520Our%2520findings%2520enable%2520the%2520development%2520of%2520compute-efficient%2520multilingual%250Amodels%2520with%2520strong%252C%2520scalable%2520performance%2520across%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model&entry.906535625=Nikit%20Srivastava%20and%20Denis%20Kuchelev%20and%20Tatiana%20Moteu%20and%20Kshitij%20Shetty%20and%20Michael%20Roeder%20and%20Diego%20Moussallem%20and%20Hamada%20Zahera%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20This%20paper%20presents%20LOLA%2C%20a%20massively%20multilingual%20large%20language%20model%0Atrained%20on%20more%20than%20160%20languages%20using%20a%20sparse%20Mixture-of-Experts%0ATransformer%20architecture.%20Our%20architectural%20and%20implementation%20choices%20address%0Athe%20challenge%20of%20harnessing%20linguistic%20diversity%20while%20maintaining%20efficiency%0Aand%20avoiding%20the%20common%20pitfalls%20of%20multilinguality.%20Our%20analysis%20of%20the%0Aevaluation%20results%20shows%20competitive%20performance%20in%20natural%20language%20generation%0Aand%20understanding%20tasks.%20Additionally%2C%20we%20demonstrate%20how%20the%20learned%0Aexpert-routing%20mechanism%20exploits%20implicit%20phylogenetic%20linguistic%20patterns%20to%0Apotentially%20alleviate%20the%20curse%20of%20multilinguality.%20We%20provide%20an%20in-depth%20look%0Aat%20the%20training%20process%2C%20an%20analysis%20of%20the%20datasets%2C%20and%20a%20balanced%0Aexploration%20of%20the%20model%27s%20strengths%20and%20limitations.%20As%20an%20open-source%20model%2C%0ALOLA%20promotes%20reproducibility%20and%20serves%20as%20a%20robust%20foundation%20for%20future%0Aresearch.%20Our%20findings%20enable%20the%20development%20of%20compute-efficient%20multilingual%0Amodels%20with%20strong%2C%20scalable%20performance%20across%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11272v1&entry.124074799=Read"},
{"title": "High-Order Evolving Graphs for Enhanced Representation of Traffic\n  Dynamics", "author": "Aditya Humnabadkar and Arindam Sikdar and Benjamin Cave and Huaizhong Zhang and Paul Bakaki and Ardhendu Behera", "abstract": "  We present an innovative framework for traffic dynamics analysis using\nHigh-Order Evolving Graphs, designed to improve spatio-temporal representations\nin autonomous driving contexts. Our approach constructs temporal bidirectional\nbipartite graphs that effectively model the complex interactions within traffic\nscenes in real-time. By integrating Graph Neural Networks (GNNs) with\nhigh-order multi-aggregation strategies, we significantly enhance the modeling\nof traffic scene dynamics, providing a more accurate and detailed analysis of\nthese interactions. Additionally, we incorporate inductive learning techniques\ninspired by the GraphSAGE framework, enabling our model to adapt to new and\nunseen traffic scenarios without the need for retraining, thus ensuring robust\ngeneralization. Through extensive experiments on the ROAD and ROAD Waymo\ndatasets, we establish a comprehensive baseline for further developments,\ndemonstrating the potential of our method in accurately capturing traffic\nbehavior. Our results emphasize the value of high-order statistical moments and\nfeature-gated attention mechanisms in improving traffic behavior analysis,\nlaying the groundwork for advancing autonomous driving technologies. Our source\ncode is available at: https://github.com/Addy-1998/High\\_Order\\_Graphs\n", "link": "http://arxiv.org/abs/2409.11206v1", "date": "2024-09-17", "relevancy": 2.5133, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5041}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5021}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Order%20Evolving%20Graphs%20for%20Enhanced%20Representation%20of%20Traffic%0A%20%20Dynamics&body=Title%3A%20High-Order%20Evolving%20Graphs%20for%20Enhanced%20Representation%20of%20Traffic%0A%20%20Dynamics%0AAuthor%3A%20Aditya%20Humnabadkar%20and%20Arindam%20Sikdar%20and%20Benjamin%20Cave%20and%20Huaizhong%20Zhang%20and%20Paul%20Bakaki%20and%20Ardhendu%20Behera%0AAbstract%3A%20%20%20We%20present%20an%20innovative%20framework%20for%20traffic%20dynamics%20analysis%20using%0AHigh-Order%20Evolving%20Graphs%2C%20designed%20to%20improve%20spatio-temporal%20representations%0Ain%20autonomous%20driving%20contexts.%20Our%20approach%20constructs%20temporal%20bidirectional%0Abipartite%20graphs%20that%20effectively%20model%20the%20complex%20interactions%20within%20traffic%0Ascenes%20in%20real-time.%20By%20integrating%20Graph%20Neural%20Networks%20%28GNNs%29%20with%0Ahigh-order%20multi-aggregation%20strategies%2C%20we%20significantly%20enhance%20the%20modeling%0Aof%20traffic%20scene%20dynamics%2C%20providing%20a%20more%20accurate%20and%20detailed%20analysis%20of%0Athese%20interactions.%20Additionally%2C%20we%20incorporate%20inductive%20learning%20techniques%0Ainspired%20by%20the%20GraphSAGE%20framework%2C%20enabling%20our%20model%20to%20adapt%20to%20new%20and%0Aunseen%20traffic%20scenarios%20without%20the%20need%20for%20retraining%2C%20thus%20ensuring%20robust%0Ageneralization.%20Through%20extensive%20experiments%20on%20the%20ROAD%20and%20ROAD%20Waymo%0Adatasets%2C%20we%20establish%20a%20comprehensive%20baseline%20for%20further%20developments%2C%0Ademonstrating%20the%20potential%20of%20our%20method%20in%20accurately%20capturing%20traffic%0Abehavior.%20Our%20results%20emphasize%20the%20value%20of%20high-order%20statistical%20moments%20and%0Afeature-gated%20attention%20mechanisms%20in%20improving%20traffic%20behavior%20analysis%2C%0Alaying%20the%20groundwork%20for%20advancing%20autonomous%20driving%20technologies.%20Our%20source%0Acode%20is%20available%20at%3A%20https%3A//github.com/Addy-1998/High%5C_Order%5C_Graphs%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Order%2520Evolving%2520Graphs%2520for%2520Enhanced%2520Representation%2520of%2520Traffic%250A%2520%2520Dynamics%26entry.906535625%3DAditya%2520Humnabadkar%2520and%2520Arindam%2520Sikdar%2520and%2520Benjamin%2520Cave%2520and%2520Huaizhong%2520Zhang%2520and%2520Paul%2520Bakaki%2520and%2520Ardhendu%2520Behera%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520innovative%2520framework%2520for%2520traffic%2520dynamics%2520analysis%2520using%250AHigh-Order%2520Evolving%2520Graphs%252C%2520designed%2520to%2520improve%2520spatio-temporal%2520representations%250Ain%2520autonomous%2520driving%2520contexts.%2520Our%2520approach%2520constructs%2520temporal%2520bidirectional%250Abipartite%2520graphs%2520that%2520effectively%2520model%2520the%2520complex%2520interactions%2520within%2520traffic%250Ascenes%2520in%2520real-time.%2520By%2520integrating%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520with%250Ahigh-order%2520multi-aggregation%2520strategies%252C%2520we%2520significantly%2520enhance%2520the%2520modeling%250Aof%2520traffic%2520scene%2520dynamics%252C%2520providing%2520a%2520more%2520accurate%2520and%2520detailed%2520analysis%2520of%250Athese%2520interactions.%2520Additionally%252C%2520we%2520incorporate%2520inductive%2520learning%2520techniques%250Ainspired%2520by%2520the%2520GraphSAGE%2520framework%252C%2520enabling%2520our%2520model%2520to%2520adapt%2520to%2520new%2520and%250Aunseen%2520traffic%2520scenarios%2520without%2520the%2520need%2520for%2520retraining%252C%2520thus%2520ensuring%2520robust%250Ageneralization.%2520Through%2520extensive%2520experiments%2520on%2520the%2520ROAD%2520and%2520ROAD%2520Waymo%250Adatasets%252C%2520we%2520establish%2520a%2520comprehensive%2520baseline%2520for%2520further%2520developments%252C%250Ademonstrating%2520the%2520potential%2520of%2520our%2520method%2520in%2520accurately%2520capturing%2520traffic%250Abehavior.%2520Our%2520results%2520emphasize%2520the%2520value%2520of%2520high-order%2520statistical%2520moments%2520and%250Afeature-gated%2520attention%2520mechanisms%2520in%2520improving%2520traffic%2520behavior%2520analysis%252C%250Alaying%2520the%2520groundwork%2520for%2520advancing%2520autonomous%2520driving%2520technologies.%2520Our%2520source%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/Addy-1998/High%255C_Order%255C_Graphs%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Order%20Evolving%20Graphs%20for%20Enhanced%20Representation%20of%20Traffic%0A%20%20Dynamics&entry.906535625=Aditya%20Humnabadkar%20and%20Arindam%20Sikdar%20and%20Benjamin%20Cave%20and%20Huaizhong%20Zhang%20and%20Paul%20Bakaki%20and%20Ardhendu%20Behera&entry.1292438233=%20%20We%20present%20an%20innovative%20framework%20for%20traffic%20dynamics%20analysis%20using%0AHigh-Order%20Evolving%20Graphs%2C%20designed%20to%20improve%20spatio-temporal%20representations%0Ain%20autonomous%20driving%20contexts.%20Our%20approach%20constructs%20temporal%20bidirectional%0Abipartite%20graphs%20that%20effectively%20model%20the%20complex%20interactions%20within%20traffic%0Ascenes%20in%20real-time.%20By%20integrating%20Graph%20Neural%20Networks%20%28GNNs%29%20with%0Ahigh-order%20multi-aggregation%20strategies%2C%20we%20significantly%20enhance%20the%20modeling%0Aof%20traffic%20scene%20dynamics%2C%20providing%20a%20more%20accurate%20and%20detailed%20analysis%20of%0Athese%20interactions.%20Additionally%2C%20we%20incorporate%20inductive%20learning%20techniques%0Ainspired%20by%20the%20GraphSAGE%20framework%2C%20enabling%20our%20model%20to%20adapt%20to%20new%20and%0Aunseen%20traffic%20scenarios%20without%20the%20need%20for%20retraining%2C%20thus%20ensuring%20robust%0Ageneralization.%20Through%20extensive%20experiments%20on%20the%20ROAD%20and%20ROAD%20Waymo%0Adatasets%2C%20we%20establish%20a%20comprehensive%20baseline%20for%20further%20developments%2C%0Ademonstrating%20the%20potential%20of%20our%20method%20in%20accurately%20capturing%20traffic%0Abehavior.%20Our%20results%20emphasize%20the%20value%20of%20high-order%20statistical%20moments%20and%0Afeature-gated%20attention%20mechanisms%20in%20improving%20traffic%20behavior%20analysis%2C%0Alaying%20the%20groundwork%20for%20advancing%20autonomous%20driving%20technologies.%20Our%20source%0Acode%20is%20available%20at%3A%20https%3A//github.com/Addy-1998/High%5C_Order%5C_Graphs%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11206v1&entry.124074799=Read"},
{"title": "Generalized Few-Shot Semantic Segmentation in Remote Sensing: Challenge\n  and Benchmark", "author": "Clifford Broni-Bediako and Junshi Xia and Jian Song and Hongruixuan Chen and Mennatullah Siam and Naoto Yokoya", "abstract": "  Learning with limited labelled data is a challenging problem in various\napplications, including remote sensing. Few-shot semantic segmentation is one\napproach that can encourage deep learning models to learn from few labelled\nexamples for novel classes not seen during the training. The generalized\nfew-shot segmentation setting has an additional challenge which encourages\nmodels not only to adapt to the novel classes but also to maintain strong\nperformance on the training base classes. While previous datasets and\nbenchmarks discussed the few-shot segmentation setting in remote sensing, we\nare the first to propose a generalized few-shot segmentation benchmark for\nremote sensing. The generalized setting is more realistic and challenging,\nwhich necessitates exploring it within the remote sensing context. We release\nthe dataset augmenting OpenEarthMap with additional classes labelled for the\ngeneralized few-shot evaluation setting. The dataset is released during the\nOpenEarthMap land cover mapping generalized few-shot challenge in the L3D-IVU\nworkshop in conjunction with CVPR 2024. In this work, we summarize the dataset\nand challenge details in addition to providing the benchmark results on the two\nphases of the challenge for the validation and test sets.\n", "link": "http://arxiv.org/abs/2409.11227v1", "date": "2024-09-17", "relevancy": 2.5064, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5039}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Few-Shot%20Semantic%20Segmentation%20in%20Remote%20Sensing%3A%20Challenge%0A%20%20and%20Benchmark&body=Title%3A%20Generalized%20Few-Shot%20Semantic%20Segmentation%20in%20Remote%20Sensing%3A%20Challenge%0A%20%20and%20Benchmark%0AAuthor%3A%20Clifford%20Broni-Bediako%20and%20Junshi%20Xia%20and%20Jian%20Song%20and%20Hongruixuan%20Chen%20and%20Mennatullah%20Siam%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Learning%20with%20limited%20labelled%20data%20is%20a%20challenging%20problem%20in%20various%0Aapplications%2C%20including%20remote%20sensing.%20Few-shot%20semantic%20segmentation%20is%20one%0Aapproach%20that%20can%20encourage%20deep%20learning%20models%20to%20learn%20from%20few%20labelled%0Aexamples%20for%20novel%20classes%20not%20seen%20during%20the%20training.%20The%20generalized%0Afew-shot%20segmentation%20setting%20has%20an%20additional%20challenge%20which%20encourages%0Amodels%20not%20only%20to%20adapt%20to%20the%20novel%20classes%20but%20also%20to%20maintain%20strong%0Aperformance%20on%20the%20training%20base%20classes.%20While%20previous%20datasets%20and%0Abenchmarks%20discussed%20the%20few-shot%20segmentation%20setting%20in%20remote%20sensing%2C%20we%0Aare%20the%20first%20to%20propose%20a%20generalized%20few-shot%20segmentation%20benchmark%20for%0Aremote%20sensing.%20The%20generalized%20setting%20is%20more%20realistic%20and%20challenging%2C%0Awhich%20necessitates%20exploring%20it%20within%20the%20remote%20sensing%20context.%20We%20release%0Athe%20dataset%20augmenting%20OpenEarthMap%20with%20additional%20classes%20labelled%20for%20the%0Ageneralized%20few-shot%20evaluation%20setting.%20The%20dataset%20is%20released%20during%20the%0AOpenEarthMap%20land%20cover%20mapping%20generalized%20few-shot%20challenge%20in%20the%20L3D-IVU%0Aworkshop%20in%20conjunction%20with%20CVPR%202024.%20In%20this%20work%2C%20we%20summarize%20the%20dataset%0Aand%20challenge%20details%20in%20addition%20to%20providing%20the%20benchmark%20results%20on%20the%20two%0Aphases%20of%20the%20challenge%20for%20the%20validation%20and%20test%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Few-Shot%2520Semantic%2520Segmentation%2520in%2520Remote%2520Sensing%253A%2520Challenge%250A%2520%2520and%2520Benchmark%26entry.906535625%3DClifford%2520Broni-Bediako%2520and%2520Junshi%2520Xia%2520and%2520Jian%2520Song%2520and%2520Hongruixuan%2520Chen%2520and%2520Mennatullah%2520Siam%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Learning%2520with%2520limited%2520labelled%2520data%2520is%2520a%2520challenging%2520problem%2520in%2520various%250Aapplications%252C%2520including%2520remote%2520sensing.%2520Few-shot%2520semantic%2520segmentation%2520is%2520one%250Aapproach%2520that%2520can%2520encourage%2520deep%2520learning%2520models%2520to%2520learn%2520from%2520few%2520labelled%250Aexamples%2520for%2520novel%2520classes%2520not%2520seen%2520during%2520the%2520training.%2520The%2520generalized%250Afew-shot%2520segmentation%2520setting%2520has%2520an%2520additional%2520challenge%2520which%2520encourages%250Amodels%2520not%2520only%2520to%2520adapt%2520to%2520the%2520novel%2520classes%2520but%2520also%2520to%2520maintain%2520strong%250Aperformance%2520on%2520the%2520training%2520base%2520classes.%2520While%2520previous%2520datasets%2520and%250Abenchmarks%2520discussed%2520the%2520few-shot%2520segmentation%2520setting%2520in%2520remote%2520sensing%252C%2520we%250Aare%2520the%2520first%2520to%2520propose%2520a%2520generalized%2520few-shot%2520segmentation%2520benchmark%2520for%250Aremote%2520sensing.%2520The%2520generalized%2520setting%2520is%2520more%2520realistic%2520and%2520challenging%252C%250Awhich%2520necessitates%2520exploring%2520it%2520within%2520the%2520remote%2520sensing%2520context.%2520We%2520release%250Athe%2520dataset%2520augmenting%2520OpenEarthMap%2520with%2520additional%2520classes%2520labelled%2520for%2520the%250Ageneralized%2520few-shot%2520evaluation%2520setting.%2520The%2520dataset%2520is%2520released%2520during%2520the%250AOpenEarthMap%2520land%2520cover%2520mapping%2520generalized%2520few-shot%2520challenge%2520in%2520the%2520L3D-IVU%250Aworkshop%2520in%2520conjunction%2520with%2520CVPR%25202024.%2520In%2520this%2520work%252C%2520we%2520summarize%2520the%2520dataset%250Aand%2520challenge%2520details%2520in%2520addition%2520to%2520providing%2520the%2520benchmark%2520results%2520on%2520the%2520two%250Aphases%2520of%2520the%2520challenge%2520for%2520the%2520validation%2520and%2520test%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Few-Shot%20Semantic%20Segmentation%20in%20Remote%20Sensing%3A%20Challenge%0A%20%20and%20Benchmark&entry.906535625=Clifford%20Broni-Bediako%20and%20Junshi%20Xia%20and%20Jian%20Song%20and%20Hongruixuan%20Chen%20and%20Mennatullah%20Siam%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Learning%20with%20limited%20labelled%20data%20is%20a%20challenging%20problem%20in%20various%0Aapplications%2C%20including%20remote%20sensing.%20Few-shot%20semantic%20segmentation%20is%20one%0Aapproach%20that%20can%20encourage%20deep%20learning%20models%20to%20learn%20from%20few%20labelled%0Aexamples%20for%20novel%20classes%20not%20seen%20during%20the%20training.%20The%20generalized%0Afew-shot%20segmentation%20setting%20has%20an%20additional%20challenge%20which%20encourages%0Amodels%20not%20only%20to%20adapt%20to%20the%20novel%20classes%20but%20also%20to%20maintain%20strong%0Aperformance%20on%20the%20training%20base%20classes.%20While%20previous%20datasets%20and%0Abenchmarks%20discussed%20the%20few-shot%20segmentation%20setting%20in%20remote%20sensing%2C%20we%0Aare%20the%20first%20to%20propose%20a%20generalized%20few-shot%20segmentation%20benchmark%20for%0Aremote%20sensing.%20The%20generalized%20setting%20is%20more%20realistic%20and%20challenging%2C%0Awhich%20necessitates%20exploring%20it%20within%20the%20remote%20sensing%20context.%20We%20release%0Athe%20dataset%20augmenting%20OpenEarthMap%20with%20additional%20classes%20labelled%20for%20the%0Ageneralized%20few-shot%20evaluation%20setting.%20The%20dataset%20is%20released%20during%20the%0AOpenEarthMap%20land%20cover%20mapping%20generalized%20few-shot%20challenge%20in%20the%20L3D-IVU%0Aworkshop%20in%20conjunction%20with%20CVPR%202024.%20In%20this%20work%2C%20we%20summarize%20the%20dataset%0Aand%20challenge%20details%20in%20addition%20to%20providing%20the%20benchmark%20results%20on%20the%20two%0Aphases%20of%20the%20challenge%20for%20the%20validation%20and%20test%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11227v1&entry.124074799=Read"},
{"title": "Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm\n  with Hybrid Representation", "author": "Haozhe Lou and Yurong Liu and Yike Pan and Yiran Geng and Jianteng Chen and Wenlong Ma and Chenglong Li and Lin Wang and Hengzhen Feng and Lu Shi and Liyi Luo and Yongliang Shi", "abstract": "  Real2Sim2Real plays a critical role in robotic arm control and reinforcement\nlearning, yet bridging this gap remains a significant challenge due to the\ncomplex physical properties of robots and the objects they manipulate. Existing\nmethods lack a comprehensive solution to accurately reconstruct real-world\nobjects with spatial representations and their associated physics attributes.\n  We propose a Real2Sim pipeline with a hybrid representation model that\nintegrates mesh geometry, 3D Gaussian kernels, and physics attributes to\nenhance the digital asset representation of robotic arms.\n  This hybrid representation is implemented through a Gaussian-Mesh-Pixel\nbinding technique, which establishes an isomorphic mapping between mesh\nvertices and Gaussian models. This enables a fully differentiable rendering\npipeline that can be optimized through numerical solvers, achieves\nhigh-fidelity rendering via Gaussian Splatting, and facilitates physically\nplausible simulation of the robotic arm's interaction with its environment\nusing mesh-based methods.\n  The code,full presentation and datasets will be made publicly available at\nour website https://robostudioapp.com\n", "link": "http://arxiv.org/abs/2408.14873v2", "date": "2024-09-17", "relevancy": 2.46, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6174}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6148}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robo-GS%3A%20A%20Physics%20Consistent%20Spatial-Temporal%20Model%20for%20Robotic%20Arm%0A%20%20with%20Hybrid%20Representation&body=Title%3A%20Robo-GS%3A%20A%20Physics%20Consistent%20Spatial-Temporal%20Model%20for%20Robotic%20Arm%0A%20%20with%20Hybrid%20Representation%0AAuthor%3A%20Haozhe%20Lou%20and%20Yurong%20Liu%20and%20Yike%20Pan%20and%20Yiran%20Geng%20and%20Jianteng%20Chen%20and%20Wenlong%20Ma%20and%20Chenglong%20Li%20and%20Lin%20Wang%20and%20Hengzhen%20Feng%20and%20Lu%20Shi%20and%20Liyi%20Luo%20and%20Yongliang%20Shi%0AAbstract%3A%20%20%20Real2Sim2Real%20plays%20a%20critical%20role%20in%20robotic%20arm%20control%20and%20reinforcement%0Alearning%2C%20yet%20bridging%20this%20gap%20remains%20a%20significant%20challenge%20due%20to%20the%0Acomplex%20physical%20properties%20of%20robots%20and%20the%20objects%20they%20manipulate.%20Existing%0Amethods%20lack%20a%20comprehensive%20solution%20to%20accurately%20reconstruct%20real-world%0Aobjects%20with%20spatial%20representations%20and%20their%20associated%20physics%20attributes.%0A%20%20We%20propose%20a%20Real2Sim%20pipeline%20with%20a%20hybrid%20representation%20model%20that%0Aintegrates%20mesh%20geometry%2C%203D%20Gaussian%20kernels%2C%20and%20physics%20attributes%20to%0Aenhance%20the%20digital%20asset%20representation%20of%20robotic%20arms.%0A%20%20This%20hybrid%20representation%20is%20implemented%20through%20a%20Gaussian-Mesh-Pixel%0Abinding%20technique%2C%20which%20establishes%20an%20isomorphic%20mapping%20between%20mesh%0Avertices%20and%20Gaussian%20models.%20This%20enables%20a%20fully%20differentiable%20rendering%0Apipeline%20that%20can%20be%20optimized%20through%20numerical%20solvers%2C%20achieves%0Ahigh-fidelity%20rendering%20via%20Gaussian%20Splatting%2C%20and%20facilitates%20physically%0Aplausible%20simulation%20of%20the%20robotic%20arm%27s%20interaction%20with%20its%20environment%0Ausing%20mesh-based%20methods.%0A%20%20The%20code%2Cfull%20presentation%20and%20datasets%20will%20be%20made%20publicly%20available%20at%0Aour%20website%20https%3A//robostudioapp.com%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobo-GS%253A%2520A%2520Physics%2520Consistent%2520Spatial-Temporal%2520Model%2520for%2520Robotic%2520Arm%250A%2520%2520with%2520Hybrid%2520Representation%26entry.906535625%3DHaozhe%2520Lou%2520and%2520Yurong%2520Liu%2520and%2520Yike%2520Pan%2520and%2520Yiran%2520Geng%2520and%2520Jianteng%2520Chen%2520and%2520Wenlong%2520Ma%2520and%2520Chenglong%2520Li%2520and%2520Lin%2520Wang%2520and%2520Hengzhen%2520Feng%2520and%2520Lu%2520Shi%2520and%2520Liyi%2520Luo%2520and%2520Yongliang%2520Shi%26entry.1292438233%3D%2520%2520Real2Sim2Real%2520plays%2520a%2520critical%2520role%2520in%2520robotic%2520arm%2520control%2520and%2520reinforcement%250Alearning%252C%2520yet%2520bridging%2520this%2520gap%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%250Acomplex%2520physical%2520properties%2520of%2520robots%2520and%2520the%2520objects%2520they%2520manipulate.%2520Existing%250Amethods%2520lack%2520a%2520comprehensive%2520solution%2520to%2520accurately%2520reconstruct%2520real-world%250Aobjects%2520with%2520spatial%2520representations%2520and%2520their%2520associated%2520physics%2520attributes.%250A%2520%2520We%2520propose%2520a%2520Real2Sim%2520pipeline%2520with%2520a%2520hybrid%2520representation%2520model%2520that%250Aintegrates%2520mesh%2520geometry%252C%25203D%2520Gaussian%2520kernels%252C%2520and%2520physics%2520attributes%2520to%250Aenhance%2520the%2520digital%2520asset%2520representation%2520of%2520robotic%2520arms.%250A%2520%2520This%2520hybrid%2520representation%2520is%2520implemented%2520through%2520a%2520Gaussian-Mesh-Pixel%250Abinding%2520technique%252C%2520which%2520establishes%2520an%2520isomorphic%2520mapping%2520between%2520mesh%250Avertices%2520and%2520Gaussian%2520models.%2520This%2520enables%2520a%2520fully%2520differentiable%2520rendering%250Apipeline%2520that%2520can%2520be%2520optimized%2520through%2520numerical%2520solvers%252C%2520achieves%250Ahigh-fidelity%2520rendering%2520via%2520Gaussian%2520Splatting%252C%2520and%2520facilitates%2520physically%250Aplausible%2520simulation%2520of%2520the%2520robotic%2520arm%2527s%2520interaction%2520with%2520its%2520environment%250Ausing%2520mesh-based%2520methods.%250A%2520%2520The%2520code%252Cfull%2520presentation%2520and%2520datasets%2520will%2520be%2520made%2520publicly%2520available%2520at%250Aour%2520website%2520https%253A//robostudioapp.com%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robo-GS%3A%20A%20Physics%20Consistent%20Spatial-Temporal%20Model%20for%20Robotic%20Arm%0A%20%20with%20Hybrid%20Representation&entry.906535625=Haozhe%20Lou%20and%20Yurong%20Liu%20and%20Yike%20Pan%20and%20Yiran%20Geng%20and%20Jianteng%20Chen%20and%20Wenlong%20Ma%20and%20Chenglong%20Li%20and%20Lin%20Wang%20and%20Hengzhen%20Feng%20and%20Lu%20Shi%20and%20Liyi%20Luo%20and%20Yongliang%20Shi&entry.1292438233=%20%20Real2Sim2Real%20plays%20a%20critical%20role%20in%20robotic%20arm%20control%20and%20reinforcement%0Alearning%2C%20yet%20bridging%20this%20gap%20remains%20a%20significant%20challenge%20due%20to%20the%0Acomplex%20physical%20properties%20of%20robots%20and%20the%20objects%20they%20manipulate.%20Existing%0Amethods%20lack%20a%20comprehensive%20solution%20to%20accurately%20reconstruct%20real-world%0Aobjects%20with%20spatial%20representations%20and%20their%20associated%20physics%20attributes.%0A%20%20We%20propose%20a%20Real2Sim%20pipeline%20with%20a%20hybrid%20representation%20model%20that%0Aintegrates%20mesh%20geometry%2C%203D%20Gaussian%20kernels%2C%20and%20physics%20attributes%20to%0Aenhance%20the%20digital%20asset%20representation%20of%20robotic%20arms.%0A%20%20This%20hybrid%20representation%20is%20implemented%20through%20a%20Gaussian-Mesh-Pixel%0Abinding%20technique%2C%20which%20establishes%20an%20isomorphic%20mapping%20between%20mesh%0Avertices%20and%20Gaussian%20models.%20This%20enables%20a%20fully%20differentiable%20rendering%0Apipeline%20that%20can%20be%20optimized%20through%20numerical%20solvers%2C%20achieves%0Ahigh-fidelity%20rendering%20via%20Gaussian%20Splatting%2C%20and%20facilitates%20physically%0Aplausible%20simulation%20of%20the%20robotic%20arm%27s%20interaction%20with%20its%20environment%0Ausing%20mesh-based%20methods.%0A%20%20The%20code%2Cfull%20presentation%20and%20datasets%20will%20be%20made%20publicly%20available%20at%0Aour%20website%20https%3A//robostudioapp.com%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14873v2&entry.124074799=Read"},
{"title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on\n  Supervised Regression (Preprint)", "author": "Yansel Gonzalez Tejeda and Helmut A. Mayer", "abstract": "  In this tutorial, we present a compact and holistic discussion of Deep\nLearning with a focus on Convolutional Neural Networks (CNNs) and supervised\nregression. While there are numerous books and articles on the individual\ntopics we cover, comprehensive and detailed tutorials that address Deep\nLearning from a foundational yet rigorous and accessible perspective are rare.\nMost resources on CNNs are either too advanced, focusing on cutting-edge\narchitectures, or too narrow, addressing only specific applications like image\nclassification.This tutorial not only summarizes the most relevant concepts but\nalso provides an in-depth exploration of each, offering a complete yet agile\nset of ideas. Moreover, we highlight the powerful synergy between learning\ntheory, statistic, and machine learning, which together underpin the Deep\nLearning and CNN frameworks. We aim for this tutorial to serve as an optimal\nresource for students, professors, and anyone interested in understanding the\nfoundations of Deep Learning. Upon acceptance we will provide an accompanying\nrepository under\n\\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}\n  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine\nLearning.\n", "link": "http://arxiv.org/abs/2408.12308v2", "date": "2024-09-17", "relevancy": 2.4431, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4823}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29&body=Title%3A%20Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29%0AAuthor%3A%20Yansel%20Gonzalez%20Tejeda%20and%20Helmut%20A.%20Mayer%0AAbstract%3A%20%20%20In%20this%20tutorial%2C%20we%20present%20a%20compact%20and%20holistic%20discussion%20of%20Deep%0ALearning%20with%20a%20focus%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20supervised%0Aregression.%20While%20there%20are%20numerous%20books%20and%20articles%20on%20the%20individual%0Atopics%20we%20cover%2C%20comprehensive%20and%20detailed%20tutorials%20that%20address%20Deep%0ALearning%20from%20a%20foundational%20yet%20rigorous%20and%20accessible%20perspective%20are%20rare.%0AMost%20resources%20on%20CNNs%20are%20either%20too%20advanced%2C%20focusing%20on%20cutting-edge%0Aarchitectures%2C%20or%20too%20narrow%2C%20addressing%20only%20specific%20applications%20like%20image%0Aclassification.This%20tutorial%20not%20only%20summarizes%20the%20most%20relevant%20concepts%20but%0Aalso%20provides%20an%20in-depth%20exploration%20of%20each%2C%20offering%20a%20complete%20yet%20agile%0Aset%20of%20ideas.%20Moreover%2C%20we%20highlight%20the%20powerful%20synergy%20between%20learning%0Atheory%2C%20statistic%2C%20and%20machine%20learning%2C%20which%20together%20underpin%20the%20Deep%0ALearning%20and%20CNN%20frameworks.%20We%20aim%20for%20this%20tutorial%20to%20serve%20as%20an%20optimal%0Aresource%20for%20students%2C%20professors%2C%20and%20anyone%20interested%20in%20understanding%20the%0Afoundations%20of%20Deep%20Learning.%20Upon%20acceptance%20we%20will%20provide%20an%20accompanying%0Arepository%20under%0A%5Chref%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%0A%20%20Keywords%3A%20Tutorial%2C%20Deep%20Learning%2C%20Convolutional%20Neural%20Networks%2C%20Machine%0ALearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520with%2520CNNs%253A%2520A%2520Compact%2520Holistic%2520Tutorial%2520with%2520Focus%2520on%250A%2520%2520Supervised%2520Regression%2520%2528Preprint%2529%26entry.906535625%3DYansel%2520Gonzalez%2520Tejeda%2520and%2520Helmut%2520A.%2520Mayer%26entry.1292438233%3D%2520%2520In%2520this%2520tutorial%252C%2520we%2520present%2520a%2520compact%2520and%2520holistic%2520discussion%2520of%2520Deep%250ALearning%2520with%2520a%2520focus%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520supervised%250Aregression.%2520While%2520there%2520are%2520numerous%2520books%2520and%2520articles%2520on%2520the%2520individual%250Atopics%2520we%2520cover%252C%2520comprehensive%2520and%2520detailed%2520tutorials%2520that%2520address%2520Deep%250ALearning%2520from%2520a%2520foundational%2520yet%2520rigorous%2520and%2520accessible%2520perspective%2520are%2520rare.%250AMost%2520resources%2520on%2520CNNs%2520are%2520either%2520too%2520advanced%252C%2520focusing%2520on%2520cutting-edge%250Aarchitectures%252C%2520or%2520too%2520narrow%252C%2520addressing%2520only%2520specific%2520applications%2520like%2520image%250Aclassification.This%2520tutorial%2520not%2520only%2520summarizes%2520the%2520most%2520relevant%2520concepts%2520but%250Aalso%2520provides%2520an%2520in-depth%2520exploration%2520of%2520each%252C%2520offering%2520a%2520complete%2520yet%2520agile%250Aset%2520of%2520ideas.%2520Moreover%252C%2520we%2520highlight%2520the%2520powerful%2520synergy%2520between%2520learning%250Atheory%252C%2520statistic%252C%2520and%2520machine%2520learning%252C%2520which%2520together%2520underpin%2520the%2520Deep%250ALearning%2520and%2520CNN%2520frameworks.%2520We%2520aim%2520for%2520this%2520tutorial%2520to%2520serve%2520as%2520an%2520optimal%250Aresource%2520for%2520students%252C%2520professors%252C%2520and%2520anyone%2520interested%2520in%2520understanding%2520the%250Afoundations%2520of%2520Deep%2520Learning.%2520Upon%2520acceptance%2520we%2520will%2520provide%2520an%2520accompanying%250Arepository%2520under%250A%255Chref%257Bhttps%253A//github.com/neoglez/deep-learning-tutorial%257D%257Bhttps%253A//github.com/neoglez/deep-learning-tutorial%257D%250A%2520%2520Keywords%253A%2520Tutorial%252C%2520Deep%2520Learning%252C%2520Convolutional%2520Neural%2520Networks%252C%2520Machine%250ALearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29&entry.906535625=Yansel%20Gonzalez%20Tejeda%20and%20Helmut%20A.%20Mayer&entry.1292438233=%20%20In%20this%20tutorial%2C%20we%20present%20a%20compact%20and%20holistic%20discussion%20of%20Deep%0ALearning%20with%20a%20focus%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20supervised%0Aregression.%20While%20there%20are%20numerous%20books%20and%20articles%20on%20the%20individual%0Atopics%20we%20cover%2C%20comprehensive%20and%20detailed%20tutorials%20that%20address%20Deep%0ALearning%20from%20a%20foundational%20yet%20rigorous%20and%20accessible%20perspective%20are%20rare.%0AMost%20resources%20on%20CNNs%20are%20either%20too%20advanced%2C%20focusing%20on%20cutting-edge%0Aarchitectures%2C%20or%20too%20narrow%2C%20addressing%20only%20specific%20applications%20like%20image%0Aclassification.This%20tutorial%20not%20only%20summarizes%20the%20most%20relevant%20concepts%20but%0Aalso%20provides%20an%20in-depth%20exploration%20of%20each%2C%20offering%20a%20complete%20yet%20agile%0Aset%20of%20ideas.%20Moreover%2C%20we%20highlight%20the%20powerful%20synergy%20between%20learning%0Atheory%2C%20statistic%2C%20and%20machine%20learning%2C%20which%20together%20underpin%20the%20Deep%0ALearning%20and%20CNN%20frameworks.%20We%20aim%20for%20this%20tutorial%20to%20serve%20as%20an%20optimal%0Aresource%20for%20students%2C%20professors%2C%20and%20anyone%20interested%20in%20understanding%20the%0Afoundations%20of%20Deep%20Learning.%20Upon%20acceptance%20we%20will%20provide%20an%20accompanying%0Arepository%20under%0A%5Chref%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%0A%20%20Keywords%3A%20Tutorial%2C%20Deep%20Learning%2C%20Convolutional%20Neural%20Networks%2C%20Machine%0ALearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12308v2&entry.124074799=Read"},
{"title": "Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach", "author": "Kyle Stein and Andrew A. Mahyari and Guillermo Francia III and Eman El-Sheikh", "abstract": "  As the complexity and connectivity of networks increase, the need for novel\nmalware detection approaches becomes imperative. Traditional security defenses\nare becoming less effective against the advanced tactics of today's\ncyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in\nstrengthening network security, offering detailed analysis of network traffic\nthat goes beyond simple metadata analysis. DPI examines not only the packet\nheaders but also the payload content within, offering a thorough insight into\nthe data traversing the network. This study proposes a novel approach that\nleverages a large language model (LLM) and few-shot learning to accurately\nrecognizes novel, unseen malware types with few labels samples. Our proposed\napproach uses a pretrained LLM on known malware types to extract the embeddings\nfrom packets. The embeddings are then used alongside few labeled samples of an\nunseen malware type. This technique is designed to acclimate the model to\ndifferent malware representations, further enabling it to generate robust\nembeddings for each trained and unseen classes. Following the extraction of\nembeddings from the LLM, few-shot learning is utilized to enhance performance\nwith minimal labeled data. Our evaluation, which utilized two renowned\ndatasets, focused on identifying malware types within network traffic and\nInternet of Things (IoT) environments. Our approach shows promising results\nwith an average accuracy of 86.35% and F1-Score of 86.40% on different malware\ntypes across the two datasets.\n", "link": "http://arxiv.org/abs/2409.11254v1", "date": "2024-09-17", "relevancy": 2.442, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4973}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4841}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Novel%20Malicious%20Packet%20Recognition%3A%20A%20Few-Shot%20Learning%20Approach&body=Title%3A%20Towards%20Novel%20Malicious%20Packet%20Recognition%3A%20A%20Few-Shot%20Learning%20Approach%0AAuthor%3A%20Kyle%20Stein%20and%20Andrew%20A.%20Mahyari%20and%20Guillermo%20Francia%20III%20and%20Eman%20El-Sheikh%0AAbstract%3A%20%20%20As%20the%20complexity%20and%20connectivity%20of%20networks%20increase%2C%20the%20need%20for%20novel%0Amalware%20detection%20approaches%20becomes%20imperative.%20Traditional%20security%20defenses%0Aare%20becoming%20less%20effective%20against%20the%20advanced%20tactics%20of%20today%27s%0Acyberattacks.%20Deep%20Packet%20Inspection%20%28DPI%29%20has%20emerged%20as%20a%20key%20technology%20in%0Astrengthening%20network%20security%2C%20offering%20detailed%20analysis%20of%20network%20traffic%0Athat%20goes%20beyond%20simple%20metadata%20analysis.%20DPI%20examines%20not%20only%20the%20packet%0Aheaders%20but%20also%20the%20payload%20content%20within%2C%20offering%20a%20thorough%20insight%20into%0Athe%20data%20traversing%20the%20network.%20This%20study%20proposes%20a%20novel%20approach%20that%0Aleverages%20a%20large%20language%20model%20%28LLM%29%20and%20few-shot%20learning%20to%20accurately%0Arecognizes%20novel%2C%20unseen%20malware%20types%20with%20few%20labels%20samples.%20Our%20proposed%0Aapproach%20uses%20a%20pretrained%20LLM%20on%20known%20malware%20types%20to%20extract%20the%20embeddings%0Afrom%20packets.%20The%20embeddings%20are%20then%20used%20alongside%20few%20labeled%20samples%20of%20an%0Aunseen%20malware%20type.%20This%20technique%20is%20designed%20to%20acclimate%20the%20model%20to%0Adifferent%20malware%20representations%2C%20further%20enabling%20it%20to%20generate%20robust%0Aembeddings%20for%20each%20trained%20and%20unseen%20classes.%20Following%20the%20extraction%20of%0Aembeddings%20from%20the%20LLM%2C%20few-shot%20learning%20is%20utilized%20to%20enhance%20performance%0Awith%20minimal%20labeled%20data.%20Our%20evaluation%2C%20which%20utilized%20two%20renowned%0Adatasets%2C%20focused%20on%20identifying%20malware%20types%20within%20network%20traffic%20and%0AInternet%20of%20Things%20%28IoT%29%20environments.%20Our%20approach%20shows%20promising%20results%0Awith%20an%20average%20accuracy%20of%2086.35%25%20and%20F1-Score%20of%2086.40%25%20on%20different%20malware%0Atypes%20across%20the%20two%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Novel%2520Malicious%2520Packet%2520Recognition%253A%2520A%2520Few-Shot%2520Learning%2520Approach%26entry.906535625%3DKyle%2520Stein%2520and%2520Andrew%2520A.%2520Mahyari%2520and%2520Guillermo%2520Francia%2520III%2520and%2520Eman%2520El-Sheikh%26entry.1292438233%3D%2520%2520As%2520the%2520complexity%2520and%2520connectivity%2520of%2520networks%2520increase%252C%2520the%2520need%2520for%2520novel%250Amalware%2520detection%2520approaches%2520becomes%2520imperative.%2520Traditional%2520security%2520defenses%250Aare%2520becoming%2520less%2520effective%2520against%2520the%2520advanced%2520tactics%2520of%2520today%2527s%250Acyberattacks.%2520Deep%2520Packet%2520Inspection%2520%2528DPI%2529%2520has%2520emerged%2520as%2520a%2520key%2520technology%2520in%250Astrengthening%2520network%2520security%252C%2520offering%2520detailed%2520analysis%2520of%2520network%2520traffic%250Athat%2520goes%2520beyond%2520simple%2520metadata%2520analysis.%2520DPI%2520examines%2520not%2520only%2520the%2520packet%250Aheaders%2520but%2520also%2520the%2520payload%2520content%2520within%252C%2520offering%2520a%2520thorough%2520insight%2520into%250Athe%2520data%2520traversing%2520the%2520network.%2520This%2520study%2520proposes%2520a%2520novel%2520approach%2520that%250Aleverages%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520and%2520few-shot%2520learning%2520to%2520accurately%250Arecognizes%2520novel%252C%2520unseen%2520malware%2520types%2520with%2520few%2520labels%2520samples.%2520Our%2520proposed%250Aapproach%2520uses%2520a%2520pretrained%2520LLM%2520on%2520known%2520malware%2520types%2520to%2520extract%2520the%2520embeddings%250Afrom%2520packets.%2520The%2520embeddings%2520are%2520then%2520used%2520alongside%2520few%2520labeled%2520samples%2520of%2520an%250Aunseen%2520malware%2520type.%2520This%2520technique%2520is%2520designed%2520to%2520acclimate%2520the%2520model%2520to%250Adifferent%2520malware%2520representations%252C%2520further%2520enabling%2520it%2520to%2520generate%2520robust%250Aembeddings%2520for%2520each%2520trained%2520and%2520unseen%2520classes.%2520Following%2520the%2520extraction%2520of%250Aembeddings%2520from%2520the%2520LLM%252C%2520few-shot%2520learning%2520is%2520utilized%2520to%2520enhance%2520performance%250Awith%2520minimal%2520labeled%2520data.%2520Our%2520evaluation%252C%2520which%2520utilized%2520two%2520renowned%250Adatasets%252C%2520focused%2520on%2520identifying%2520malware%2520types%2520within%2520network%2520traffic%2520and%250AInternet%2520of%2520Things%2520%2528IoT%2529%2520environments.%2520Our%2520approach%2520shows%2520promising%2520results%250Awith%2520an%2520average%2520accuracy%2520of%252086.35%2525%2520and%2520F1-Score%2520of%252086.40%2525%2520on%2520different%2520malware%250Atypes%2520across%2520the%2520two%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Novel%20Malicious%20Packet%20Recognition%3A%20A%20Few-Shot%20Learning%20Approach&entry.906535625=Kyle%20Stein%20and%20Andrew%20A.%20Mahyari%20and%20Guillermo%20Francia%20III%20and%20Eman%20El-Sheikh&entry.1292438233=%20%20As%20the%20complexity%20and%20connectivity%20of%20networks%20increase%2C%20the%20need%20for%20novel%0Amalware%20detection%20approaches%20becomes%20imperative.%20Traditional%20security%20defenses%0Aare%20becoming%20less%20effective%20against%20the%20advanced%20tactics%20of%20today%27s%0Acyberattacks.%20Deep%20Packet%20Inspection%20%28DPI%29%20has%20emerged%20as%20a%20key%20technology%20in%0Astrengthening%20network%20security%2C%20offering%20detailed%20analysis%20of%20network%20traffic%0Athat%20goes%20beyond%20simple%20metadata%20analysis.%20DPI%20examines%20not%20only%20the%20packet%0Aheaders%20but%20also%20the%20payload%20content%20within%2C%20offering%20a%20thorough%20insight%20into%0Athe%20data%20traversing%20the%20network.%20This%20study%20proposes%20a%20novel%20approach%20that%0Aleverages%20a%20large%20language%20model%20%28LLM%29%20and%20few-shot%20learning%20to%20accurately%0Arecognizes%20novel%2C%20unseen%20malware%20types%20with%20few%20labels%20samples.%20Our%20proposed%0Aapproach%20uses%20a%20pretrained%20LLM%20on%20known%20malware%20types%20to%20extract%20the%20embeddings%0Afrom%20packets.%20The%20embeddings%20are%20then%20used%20alongside%20few%20labeled%20samples%20of%20an%0Aunseen%20malware%20type.%20This%20technique%20is%20designed%20to%20acclimate%20the%20model%20to%0Adifferent%20malware%20representations%2C%20further%20enabling%20it%20to%20generate%20robust%0Aembeddings%20for%20each%20trained%20and%20unseen%20classes.%20Following%20the%20extraction%20of%0Aembeddings%20from%20the%20LLM%2C%20few-shot%20learning%20is%20utilized%20to%20enhance%20performance%0Awith%20minimal%20labeled%20data.%20Our%20evaluation%2C%20which%20utilized%20two%20renowned%0Adatasets%2C%20focused%20on%20identifying%20malware%20types%20within%20network%20traffic%20and%0AInternet%20of%20Things%20%28IoT%29%20environments.%20Our%20approach%20shows%20promising%20results%0Awith%20an%20average%20accuracy%20of%2086.35%25%20and%20F1-Score%20of%2086.40%25%20on%20different%20malware%0Atypes%20across%20the%20two%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11254v1&entry.124074799=Read"},
{"title": "IRIS: Interactive Responsive Intelligent Segmentation for 3D Affordance\n  Analysis", "author": "Meng Chu and Xuan Zhang", "abstract": "  Recent advancements in large language and vision-language models have\nsignificantly enhanced multimodal understanding, yet translating high-level\nlinguistic instructions into precise robotic actions in 3D space remains\nchallenging. This paper introduces IRIS (Interactive Responsive Intelligent\nSegmentation), a novel training-free multimodal system for 3D affordance\nsegmentation, alongside a benchmark for evaluating interactive language-guided\naffordance in everyday environments. IRIS integrates a large multimodal model\nwith a specialized 3D vision network, enabling seamless fusion of 2D and 3D\nvisual understanding with language comprehension. To facilitate evaluation, we\npresent a dataset of 10 typical indoor environments, each with 50 images\nannotated for object actions and 3D affordance segmentation. Extensive\nexperiments demonstrate IRIS's capability in handling interactive 3D affordance\nsegmentation tasks across diverse settings, showcasing competitive performance\nacross various metrics. Our results highlight IRIS's potential for enhancing\nhuman-robot interaction based on affordance understanding in complex indoor\nenvironments, advancing the development of more intuitive and efficient robotic\nsystems for real-world applications.\n", "link": "http://arxiv.org/abs/2409.10078v2", "date": "2024-09-17", "relevancy": 2.44, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRIS%3A%20Interactive%20Responsive%20Intelligent%20Segmentation%20for%203D%20Affordance%0A%20%20Analysis&body=Title%3A%20IRIS%3A%20Interactive%20Responsive%20Intelligent%20Segmentation%20for%203D%20Affordance%0A%20%20Analysis%0AAuthor%3A%20Meng%20Chu%20and%20Xuan%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20and%20vision-language%20models%20have%0Asignificantly%20enhanced%20multimodal%20understanding%2C%20yet%20translating%20high-level%0Alinguistic%20instructions%20into%20precise%20robotic%20actions%20in%203D%20space%20remains%0Achallenging.%20This%20paper%20introduces%20IRIS%20%28Interactive%20Responsive%20Intelligent%0ASegmentation%29%2C%20a%20novel%20training-free%20multimodal%20system%20for%203D%20affordance%0Asegmentation%2C%20alongside%20a%20benchmark%20for%20evaluating%20interactive%20language-guided%0Aaffordance%20in%20everyday%20environments.%20IRIS%20integrates%20a%20large%20multimodal%20model%0Awith%20a%20specialized%203D%20vision%20network%2C%20enabling%20seamless%20fusion%20of%202D%20and%203D%0Avisual%20understanding%20with%20language%20comprehension.%20To%20facilitate%20evaluation%2C%20we%0Apresent%20a%20dataset%20of%2010%20typical%20indoor%20environments%2C%20each%20with%2050%20images%0Aannotated%20for%20object%20actions%20and%203D%20affordance%20segmentation.%20Extensive%0Aexperiments%20demonstrate%20IRIS%27s%20capability%20in%20handling%20interactive%203D%20affordance%0Asegmentation%20tasks%20across%20diverse%20settings%2C%20showcasing%20competitive%20performance%0Aacross%20various%20metrics.%20Our%20results%20highlight%20IRIS%27s%20potential%20for%20enhancing%0Ahuman-robot%20interaction%20based%20on%20affordance%20understanding%20in%20complex%20indoor%0Aenvironments%2C%20advancing%20the%20development%20of%20more%20intuitive%20and%20efficient%20robotic%0Asystems%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRIS%253A%2520Interactive%2520Responsive%2520Intelligent%2520Segmentation%2520for%25203D%2520Affordance%250A%2520%2520Analysis%26entry.906535625%3DMeng%2520Chu%2520and%2520Xuan%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520and%2520vision-language%2520models%2520have%250Asignificantly%2520enhanced%2520multimodal%2520understanding%252C%2520yet%2520translating%2520high-level%250Alinguistic%2520instructions%2520into%2520precise%2520robotic%2520actions%2520in%25203D%2520space%2520remains%250Achallenging.%2520This%2520paper%2520introduces%2520IRIS%2520%2528Interactive%2520Responsive%2520Intelligent%250ASegmentation%2529%252C%2520a%2520novel%2520training-free%2520multimodal%2520system%2520for%25203D%2520affordance%250Asegmentation%252C%2520alongside%2520a%2520benchmark%2520for%2520evaluating%2520interactive%2520language-guided%250Aaffordance%2520in%2520everyday%2520environments.%2520IRIS%2520integrates%2520a%2520large%2520multimodal%2520model%250Awith%2520a%2520specialized%25203D%2520vision%2520network%252C%2520enabling%2520seamless%2520fusion%2520of%25202D%2520and%25203D%250Avisual%2520understanding%2520with%2520language%2520comprehension.%2520To%2520facilitate%2520evaluation%252C%2520we%250Apresent%2520a%2520dataset%2520of%252010%2520typical%2520indoor%2520environments%252C%2520each%2520with%252050%2520images%250Aannotated%2520for%2520object%2520actions%2520and%25203D%2520affordance%2520segmentation.%2520Extensive%250Aexperiments%2520demonstrate%2520IRIS%2527s%2520capability%2520in%2520handling%2520interactive%25203D%2520affordance%250Asegmentation%2520tasks%2520across%2520diverse%2520settings%252C%2520showcasing%2520competitive%2520performance%250Aacross%2520various%2520metrics.%2520Our%2520results%2520highlight%2520IRIS%2527s%2520potential%2520for%2520enhancing%250Ahuman-robot%2520interaction%2520based%2520on%2520affordance%2520understanding%2520in%2520complex%2520indoor%250Aenvironments%252C%2520advancing%2520the%2520development%2520of%2520more%2520intuitive%2520and%2520efficient%2520robotic%250Asystems%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRIS%3A%20Interactive%20Responsive%20Intelligent%20Segmentation%20for%203D%20Affordance%0A%20%20Analysis&entry.906535625=Meng%20Chu%20and%20Xuan%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20and%20vision-language%20models%20have%0Asignificantly%20enhanced%20multimodal%20understanding%2C%20yet%20translating%20high-level%0Alinguistic%20instructions%20into%20precise%20robotic%20actions%20in%203D%20space%20remains%0Achallenging.%20This%20paper%20introduces%20IRIS%20%28Interactive%20Responsive%20Intelligent%0ASegmentation%29%2C%20a%20novel%20training-free%20multimodal%20system%20for%203D%20affordance%0Asegmentation%2C%20alongside%20a%20benchmark%20for%20evaluating%20interactive%20language-guided%0Aaffordance%20in%20everyday%20environments.%20IRIS%20integrates%20a%20large%20multimodal%20model%0Awith%20a%20specialized%203D%20vision%20network%2C%20enabling%20seamless%20fusion%20of%202D%20and%203D%0Avisual%20understanding%20with%20language%20comprehension.%20To%20facilitate%20evaluation%2C%20we%0Apresent%20a%20dataset%20of%2010%20typical%20indoor%20environments%2C%20each%20with%2050%20images%0Aannotated%20for%20object%20actions%20and%203D%20affordance%20segmentation.%20Extensive%0Aexperiments%20demonstrate%20IRIS%27s%20capability%20in%20handling%20interactive%203D%20affordance%0Asegmentation%20tasks%20across%20diverse%20settings%2C%20showcasing%20competitive%20performance%0Aacross%20various%20metrics.%20Our%20results%20highlight%20IRIS%27s%20potential%20for%20enhancing%0Ahuman-robot%20interaction%20based%20on%20affordance%20understanding%20in%20complex%20indoor%0Aenvironments%2C%20advancing%20the%20development%20of%20more%20intuitive%20and%20efficient%20robotic%0Asystems%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10078v2&entry.124074799=Read"},
{"title": "SpatialBot: Precise Spatial Understanding with Vision Language Models", "author": "Wenxiao Cai and Iaroslav Ponomarenko and Jianhao Yuan and Xiaoqi Li and Wankou Yang and Hao Dong and Bo Zhao", "abstract": "  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n", "link": "http://arxiv.org/abs/2406.13642v6", "date": "2024-09-17", "relevancy": 2.4275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6133}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialBot%3A%20Precise%20Spatial%20Understanding%20with%20Vision%20Language%20Models&body=Title%3A%20SpatialBot%3A%20Precise%20Spatial%20Understanding%20with%20Vision%20Language%20Models%0AAuthor%3A%20Wenxiao%20Cai%20and%20Iaroslav%20Ponomarenko%20and%20Jianhao%20Yuan%20and%20Xiaoqi%20Li%20and%20Wankou%20Yang%20and%20Hao%20Dong%20and%20Bo%20Zhao%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20in%202D%0Aimage%20understanding%2C%20however%20they%20are%20still%20struggling%20with%20spatial%0Aunderstanding%20which%20is%20the%20foundation%20of%20Embodied%20AI.%20In%20this%20paper%2C%20we%20propose%0ASpatialBot%20for%20better%20spatial%20understanding%20by%20feeding%20both%20RGB%20and%20depth%0Aimages.%20Additionally%2C%20we%20have%20constructed%20the%20SpatialQA%20dataset%2C%20which%20involves%0Amulti-level%20depth-related%20questions%20to%20train%20VLMs%20for%20depth%20understanding.%0AFinally%2C%20we%20present%20SpatialBench%20to%20comprehensively%20evaluate%20VLMs%27%20capabilities%0Ain%20spatial%20understanding%20at%20different%20levels.%20Extensive%20experiments%20on%20our%0Aspatial-understanding%20benchmark%2C%20general%20VLM%20benchmarks%20and%20Embodied%20AI%20tasks%2C%0Ademonstrate%20the%20remarkable%20improvements%20of%20SpatialBot%20trained%20on%20SpatialQA.%20The%0Amodel%2C%20code%20and%20data%20are%20available%20at%20https%3A//github.com/BAAI-DCAI/SpatialBot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13642v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialBot%253A%2520Precise%2520Spatial%2520Understanding%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DWenxiao%2520Cai%2520and%2520Iaroslav%2520Ponomarenko%2520and%2520Jianhao%2520Yuan%2520and%2520Xiaoqi%2520Li%2520and%2520Wankou%2520Yang%2520and%2520Hao%2520Dong%2520and%2520Bo%2520Zhao%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520in%25202D%250Aimage%2520understanding%252C%2520however%2520they%2520are%2520still%2520struggling%2520with%2520spatial%250Aunderstanding%2520which%2520is%2520the%2520foundation%2520of%2520Embodied%2520AI.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASpatialBot%2520for%2520better%2520spatial%2520understanding%2520by%2520feeding%2520both%2520RGB%2520and%2520depth%250Aimages.%2520Additionally%252C%2520we%2520have%2520constructed%2520the%2520SpatialQA%2520dataset%252C%2520which%2520involves%250Amulti-level%2520depth-related%2520questions%2520to%2520train%2520VLMs%2520for%2520depth%2520understanding.%250AFinally%252C%2520we%2520present%2520SpatialBench%2520to%2520comprehensively%2520evaluate%2520VLMs%2527%2520capabilities%250Ain%2520spatial%2520understanding%2520at%2520different%2520levels.%2520Extensive%2520experiments%2520on%2520our%250Aspatial-understanding%2520benchmark%252C%2520general%2520VLM%2520benchmarks%2520and%2520Embodied%2520AI%2520tasks%252C%250Ademonstrate%2520the%2520remarkable%2520improvements%2520of%2520SpatialBot%2520trained%2520on%2520SpatialQA.%2520The%250Amodel%252C%2520code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/BAAI-DCAI/SpatialBot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13642v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialBot%3A%20Precise%20Spatial%20Understanding%20with%20Vision%20Language%20Models&entry.906535625=Wenxiao%20Cai%20and%20Iaroslav%20Ponomarenko%20and%20Jianhao%20Yuan%20and%20Xiaoqi%20Li%20and%20Wankou%20Yang%20and%20Hao%20Dong%20and%20Bo%20Zhao&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20in%202D%0Aimage%20understanding%2C%20however%20they%20are%20still%20struggling%20with%20spatial%0Aunderstanding%20which%20is%20the%20foundation%20of%20Embodied%20AI.%20In%20this%20paper%2C%20we%20propose%0ASpatialBot%20for%20better%20spatial%20understanding%20by%20feeding%20both%20RGB%20and%20depth%0Aimages.%20Additionally%2C%20we%20have%20constructed%20the%20SpatialQA%20dataset%2C%20which%20involves%0Amulti-level%20depth-related%20questions%20to%20train%20VLMs%20for%20depth%20understanding.%0AFinally%2C%20we%20present%20SpatialBench%20to%20comprehensively%20evaluate%20VLMs%27%20capabilities%0Ain%20spatial%20understanding%20at%20different%20levels.%20Extensive%20experiments%20on%20our%0Aspatial-understanding%20benchmark%2C%20general%20VLM%20benchmarks%20and%20Embodied%20AI%20tasks%2C%0Ademonstrate%20the%20remarkable%20improvements%20of%20SpatialBot%20trained%20on%20SpatialQA.%20The%0Amodel%2C%20code%20and%20data%20are%20available%20at%20https%3A//github.com/BAAI-DCAI/SpatialBot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13642v6&entry.124074799=Read"},
{"title": "MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks", "author": "Tal Kadosh and Niranjan Hasabnis and Vy A. Vo and Nadav Schneider and Neva Krien and Mihai Capota and Abdul Wasay and Nesreen Ahmed and Ted Willke and Guy Tamir and Yuval Pinter and Timothy Mattson and Gal Oren", "abstract": "  With easier access to powerful compute resources, there is a growing trend in\nAI for software development to develop large language models (LLMs) to address\na variety of programming tasks. Even LLMs applied to tasks from the\nhigh-performance computing (HPC) domain are huge in size and demand expensive\ncompute resources for training. This is partly because LLMs for HPC tasks are\nobtained by finetuning existing LLMs that support several natural and/or\nprogramming languages. We found this design choice confusing - why do we need\nLLMs trained on natural languages and programming languages unrelated to HPC\nfor HPC-specific tasks? In this line of work, we aim to question choices made\nby existing LLMs by developing smaller language models (LMs) for specific\ndomains - we call them domain-specific LMs. Specifically, we start with HPC as\na domain and build an HPC-specific LM, named MonoCoder, which is orders of\nmagnitude smaller than existing LMs but delivers better performance on non-HPC\nand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific\ndataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated\nthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.\nResults demonstrate that MonoCoder, although much smaller than existing LMs,\noutperforms other LLMs on normalized-perplexity tests (in relation to model\nsize) while also delivering competing CodeBLEU scores for high-performance and\nparallel code generations. In other words, results suggest that MonoCoder\nunderstands HPC code better than state-of-the-art LLMs.\n", "link": "http://arxiv.org/abs/2312.13322v2", "date": "2024-09-17", "relevancy": 2.4187, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoCoder%3A%20Domain-Specific%20Code%20Language%20Model%20for%20HPC%20Codes%20and%20Tasks&body=Title%3A%20MonoCoder%3A%20Domain-Specific%20Code%20Language%20Model%20for%20HPC%20Codes%20and%20Tasks%0AAuthor%3A%20Tal%20Kadosh%20and%20Niranjan%20Hasabnis%20and%20Vy%20A.%20Vo%20and%20Nadav%20Schneider%20and%20Neva%20Krien%20and%20Mihai%20Capota%20and%20Abdul%20Wasay%20and%20Nesreen%20Ahmed%20and%20Ted%20Willke%20and%20Guy%20Tamir%20and%20Yuval%20Pinter%20and%20Timothy%20Mattson%20and%20Gal%20Oren%0AAbstract%3A%20%20%20With%20easier%20access%20to%20powerful%20compute%20resources%2C%20there%20is%20a%20growing%20trend%20in%0AAI%20for%20software%20development%20to%20develop%20large%20language%20models%20%28LLMs%29%20to%20address%0Aa%20variety%20of%20programming%20tasks.%20Even%20LLMs%20applied%20to%20tasks%20from%20the%0Ahigh-performance%20computing%20%28HPC%29%20domain%20are%20huge%20in%20size%20and%20demand%20expensive%0Acompute%20resources%20for%20training.%20This%20is%20partly%20because%20LLMs%20for%20HPC%20tasks%20are%0Aobtained%20by%20finetuning%20existing%20LLMs%20that%20support%20several%20natural%20and/or%0Aprogramming%20languages.%20We%20found%20this%20design%20choice%20confusing%20-%20why%20do%20we%20need%0ALLMs%20trained%20on%20natural%20languages%20and%20programming%20languages%20unrelated%20to%20HPC%0Afor%20HPC-specific%20tasks%3F%20In%20this%20line%20of%20work%2C%20we%20aim%20to%20question%20choices%20made%0Aby%20existing%20LLMs%20by%20developing%20smaller%20language%20models%20%28LMs%29%20for%20specific%0Adomains%20-%20we%20call%20them%20domain-specific%20LMs.%20Specifically%2C%20we%20start%20with%20HPC%20as%0Aa%20domain%20and%20build%20an%20HPC-specific%20LM%2C%20named%20MonoCoder%2C%20which%20is%20orders%20of%0Amagnitude%20smaller%20than%20existing%20LMs%20but%20delivers%20better%20performance%20on%20non-HPC%0Aand%20HPC%20codes.%20Specifically%2C%20we%20pre-trained%20MonoCoder%20on%20an%20HPC-specific%0Adataset%20%28named%20HPCorpus%29%20of%20C%20and%20C%2B%2B%20programs%20mined%20from%20GitHub.%20We%20evaluated%0Athe%20performance%20of%20MonoCoder%20against%20state-of-the-art%20multi-lingual%20LLMs.%0AResults%20demonstrate%20that%20MonoCoder%2C%20although%20much%20smaller%20than%20existing%20LMs%2C%0Aoutperforms%20other%20LLMs%20on%20normalized-perplexity%20tests%20%28in%20relation%20to%20model%0Asize%29%20while%20also%20delivering%20competing%20CodeBLEU%20scores%20for%20high-performance%20and%0Aparallel%20code%20generations.%20In%20other%20words%2C%20results%20suggest%20that%20MonoCoder%0Aunderstands%20HPC%20code%20better%20than%20state-of-the-art%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoCoder%253A%2520Domain-Specific%2520Code%2520Language%2520Model%2520for%2520HPC%2520Codes%2520and%2520Tasks%26entry.906535625%3DTal%2520Kadosh%2520and%2520Niranjan%2520Hasabnis%2520and%2520Vy%2520A.%2520Vo%2520and%2520Nadav%2520Schneider%2520and%2520Neva%2520Krien%2520and%2520Mihai%2520Capota%2520and%2520Abdul%2520Wasay%2520and%2520Nesreen%2520Ahmed%2520and%2520Ted%2520Willke%2520and%2520Guy%2520Tamir%2520and%2520Yuval%2520Pinter%2520and%2520Timothy%2520Mattson%2520and%2520Gal%2520Oren%26entry.1292438233%3D%2520%2520With%2520easier%2520access%2520to%2520powerful%2520compute%2520resources%252C%2520there%2520is%2520a%2520growing%2520trend%2520in%250AAI%2520for%2520software%2520development%2520to%2520develop%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520address%250Aa%2520variety%2520of%2520programming%2520tasks.%2520Even%2520LLMs%2520applied%2520to%2520tasks%2520from%2520the%250Ahigh-performance%2520computing%2520%2528HPC%2529%2520domain%2520are%2520huge%2520in%2520size%2520and%2520demand%2520expensive%250Acompute%2520resources%2520for%2520training.%2520This%2520is%2520partly%2520because%2520LLMs%2520for%2520HPC%2520tasks%2520are%250Aobtained%2520by%2520finetuning%2520existing%2520LLMs%2520that%2520support%2520several%2520natural%2520and/or%250Aprogramming%2520languages.%2520We%2520found%2520this%2520design%2520choice%2520confusing%2520-%2520why%2520do%2520we%2520need%250ALLMs%2520trained%2520on%2520natural%2520languages%2520and%2520programming%2520languages%2520unrelated%2520to%2520HPC%250Afor%2520HPC-specific%2520tasks%253F%2520In%2520this%2520line%2520of%2520work%252C%2520we%2520aim%2520to%2520question%2520choices%2520made%250Aby%2520existing%2520LLMs%2520by%2520developing%2520smaller%2520language%2520models%2520%2528LMs%2529%2520for%2520specific%250Adomains%2520-%2520we%2520call%2520them%2520domain-specific%2520LMs.%2520Specifically%252C%2520we%2520start%2520with%2520HPC%2520as%250Aa%2520domain%2520and%2520build%2520an%2520HPC-specific%2520LM%252C%2520named%2520MonoCoder%252C%2520which%2520is%2520orders%2520of%250Amagnitude%2520smaller%2520than%2520existing%2520LMs%2520but%2520delivers%2520better%2520performance%2520on%2520non-HPC%250Aand%2520HPC%2520codes.%2520Specifically%252C%2520we%2520pre-trained%2520MonoCoder%2520on%2520an%2520HPC-specific%250Adataset%2520%2528named%2520HPCorpus%2529%2520of%2520C%2520and%2520C%252B%252B%2520programs%2520mined%2520from%2520GitHub.%2520We%2520evaluated%250Athe%2520performance%2520of%2520MonoCoder%2520against%2520state-of-the-art%2520multi-lingual%2520LLMs.%250AResults%2520demonstrate%2520that%2520MonoCoder%252C%2520although%2520much%2520smaller%2520than%2520existing%2520LMs%252C%250Aoutperforms%2520other%2520LLMs%2520on%2520normalized-perplexity%2520tests%2520%2528in%2520relation%2520to%2520model%250Asize%2529%2520while%2520also%2520delivering%2520competing%2520CodeBLEU%2520scores%2520for%2520high-performance%2520and%250Aparallel%2520code%2520generations.%2520In%2520other%2520words%252C%2520results%2520suggest%2520that%2520MonoCoder%250Aunderstands%2520HPC%2520code%2520better%2520than%2520state-of-the-art%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoCoder%3A%20Domain-Specific%20Code%20Language%20Model%20for%20HPC%20Codes%20and%20Tasks&entry.906535625=Tal%20Kadosh%20and%20Niranjan%20Hasabnis%20and%20Vy%20A.%20Vo%20and%20Nadav%20Schneider%20and%20Neva%20Krien%20and%20Mihai%20Capota%20and%20Abdul%20Wasay%20and%20Nesreen%20Ahmed%20and%20Ted%20Willke%20and%20Guy%20Tamir%20and%20Yuval%20Pinter%20and%20Timothy%20Mattson%20and%20Gal%20Oren&entry.1292438233=%20%20With%20easier%20access%20to%20powerful%20compute%20resources%2C%20there%20is%20a%20growing%20trend%20in%0AAI%20for%20software%20development%20to%20develop%20large%20language%20models%20%28LLMs%29%20to%20address%0Aa%20variety%20of%20programming%20tasks.%20Even%20LLMs%20applied%20to%20tasks%20from%20the%0Ahigh-performance%20computing%20%28HPC%29%20domain%20are%20huge%20in%20size%20and%20demand%20expensive%0Acompute%20resources%20for%20training.%20This%20is%20partly%20because%20LLMs%20for%20HPC%20tasks%20are%0Aobtained%20by%20finetuning%20existing%20LLMs%20that%20support%20several%20natural%20and/or%0Aprogramming%20languages.%20We%20found%20this%20design%20choice%20confusing%20-%20why%20do%20we%20need%0ALLMs%20trained%20on%20natural%20languages%20and%20programming%20languages%20unrelated%20to%20HPC%0Afor%20HPC-specific%20tasks%3F%20In%20this%20line%20of%20work%2C%20we%20aim%20to%20question%20choices%20made%0Aby%20existing%20LLMs%20by%20developing%20smaller%20language%20models%20%28LMs%29%20for%20specific%0Adomains%20-%20we%20call%20them%20domain-specific%20LMs.%20Specifically%2C%20we%20start%20with%20HPC%20as%0Aa%20domain%20and%20build%20an%20HPC-specific%20LM%2C%20named%20MonoCoder%2C%20which%20is%20orders%20of%0Amagnitude%20smaller%20than%20existing%20LMs%20but%20delivers%20better%20performance%20on%20non-HPC%0Aand%20HPC%20codes.%20Specifically%2C%20we%20pre-trained%20MonoCoder%20on%20an%20HPC-specific%0Adataset%20%28named%20HPCorpus%29%20of%20C%20and%20C%2B%2B%20programs%20mined%20from%20GitHub.%20We%20evaluated%0Athe%20performance%20of%20MonoCoder%20against%20state-of-the-art%20multi-lingual%20LLMs.%0AResults%20demonstrate%20that%20MonoCoder%2C%20although%20much%20smaller%20than%20existing%20LMs%2C%0Aoutperforms%20other%20LLMs%20on%20normalized-perplexity%20tests%20%28in%20relation%20to%20model%0Asize%29%20while%20also%20delivering%20competing%20CodeBLEU%20scores%20for%20high-performance%20and%0Aparallel%20code%20generations.%20In%20other%20words%2C%20results%20suggest%20that%20MonoCoder%0Aunderstands%20HPC%20code%20better%20than%20state-of-the-art%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13322v2&entry.124074799=Read"},
{"title": "Towards Optimal Branching of Linear and Semidefinite Relaxations for\n  Neural Network Robustness Certification", "author": "Brendon G. Anderson and Ziye Ma and Jingqi Li and Somayeh Sojoudi", "abstract": "  In this paper, we study certifying the robustness of ReLU neural networks\nagainst adversarial input perturbations. To diminish the relaxation error\nsuffered by the popular linear programming (LP) and semidefinite programming\n(SDP) certification methods, we take a branch-and-bound approach to propose\npartitioning the input uncertainty set and solving the relaxations on each part\nseparately. We show that this approach reduces relaxation error, and that the\nerror is eliminated entirely upon performing an LP relaxation with a partition\nintelligently designed to exploit the nature of the ReLU activations. To scale\nthis approach to large networks, we consider using a coarser partition whereby\nthe number of parts in the partition is reduced. We prove that computing such a\ncoarse partition that directly minimizes the LP relaxation error is NP-hard. By\ninstead minimizing the worst-case LP relaxation error, we develop a closed-form\nbranching scheme in the single-hidden layer case. We extend the analysis to the\nSDP, where the feasible set geometry is exploited to design a branching scheme\nthat minimizes the worst-case SDP relaxation error. Experiments on MNIST,\nCIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstrate\nsignificant increases in the percentages of test samples certified. By\nindependently increasing the input size and the number of layers, we\nempirically illustrate under which regimes the branched LP and branched SDP are\nbest applied. Finally, we extend our LP branching method into a multi-layer\nbranching heuristic, which attains comparable performance to prior\nstate-of-the-art heuristics on large-scale, deep neural network certification\nbenchmarks.\n", "link": "http://arxiv.org/abs/2101.09306v3", "date": "2024-09-17", "relevancy": 2.4179, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4894}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4887}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Optimal%20Branching%20of%20Linear%20and%20Semidefinite%20Relaxations%20for%0A%20%20Neural%20Network%20Robustness%20Certification&body=Title%3A%20Towards%20Optimal%20Branching%20of%20Linear%20and%20Semidefinite%20Relaxations%20for%0A%20%20Neural%20Network%20Robustness%20Certification%0AAuthor%3A%20Brendon%20G.%20Anderson%20and%20Ziye%20Ma%20and%20Jingqi%20Li%20and%20Somayeh%20Sojoudi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20certifying%20the%20robustness%20of%20ReLU%20neural%20networks%0Aagainst%20adversarial%20input%20perturbations.%20To%20diminish%20the%20relaxation%20error%0Asuffered%20by%20the%20popular%20linear%20programming%20%28LP%29%20and%20semidefinite%20programming%0A%28SDP%29%20certification%20methods%2C%20we%20take%20a%20branch-and-bound%20approach%20to%20propose%0Apartitioning%20the%20input%20uncertainty%20set%20and%20solving%20the%20relaxations%20on%20each%20part%0Aseparately.%20We%20show%20that%20this%20approach%20reduces%20relaxation%20error%2C%20and%20that%20the%0Aerror%20is%20eliminated%20entirely%20upon%20performing%20an%20LP%20relaxation%20with%20a%20partition%0Aintelligently%20designed%20to%20exploit%20the%20nature%20of%20the%20ReLU%20activations.%20To%20scale%0Athis%20approach%20to%20large%20networks%2C%20we%20consider%20using%20a%20coarser%20partition%20whereby%0Athe%20number%20of%20parts%20in%20the%20partition%20is%20reduced.%20We%20prove%20that%20computing%20such%20a%0Acoarse%20partition%20that%20directly%20minimizes%20the%20LP%20relaxation%20error%20is%20NP-hard.%20By%0Ainstead%20minimizing%20the%20worst-case%20LP%20relaxation%20error%2C%20we%20develop%20a%20closed-form%0Abranching%20scheme%20in%20the%20single-hidden%20layer%20case.%20We%20extend%20the%20analysis%20to%20the%0ASDP%2C%20where%20the%20feasible%20set%20geometry%20is%20exploited%20to%20design%20a%20branching%20scheme%0Athat%20minimizes%20the%20worst-case%20SDP%20relaxation%20error.%20Experiments%20on%20MNIST%2C%0ACIFAR-10%2C%20and%20Wisconsin%20breast%20cancer%20diagnosis%20classifiers%20demonstrate%0Asignificant%20increases%20in%20the%20percentages%20of%20test%20samples%20certified.%20By%0Aindependently%20increasing%20the%20input%20size%20and%20the%20number%20of%20layers%2C%20we%0Aempirically%20illustrate%20under%20which%20regimes%20the%20branched%20LP%20and%20branched%20SDP%20are%0Abest%20applied.%20Finally%2C%20we%20extend%20our%20LP%20branching%20method%20into%20a%20multi-layer%0Abranching%20heuristic%2C%20which%20attains%20comparable%20performance%20to%20prior%0Astate-of-the-art%20heuristics%20on%20large-scale%2C%20deep%20neural%20network%20certification%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2101.09306v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Optimal%2520Branching%2520of%2520Linear%2520and%2520Semidefinite%2520Relaxations%2520for%250A%2520%2520Neural%2520Network%2520Robustness%2520Certification%26entry.906535625%3DBrendon%2520G.%2520Anderson%2520and%2520Ziye%2520Ma%2520and%2520Jingqi%2520Li%2520and%2520Somayeh%2520Sojoudi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520certifying%2520the%2520robustness%2520of%2520ReLU%2520neural%2520networks%250Aagainst%2520adversarial%2520input%2520perturbations.%2520To%2520diminish%2520the%2520relaxation%2520error%250Asuffered%2520by%2520the%2520popular%2520linear%2520programming%2520%2528LP%2529%2520and%2520semidefinite%2520programming%250A%2528SDP%2529%2520certification%2520methods%252C%2520we%2520take%2520a%2520branch-and-bound%2520approach%2520to%2520propose%250Apartitioning%2520the%2520input%2520uncertainty%2520set%2520and%2520solving%2520the%2520relaxations%2520on%2520each%2520part%250Aseparately.%2520We%2520show%2520that%2520this%2520approach%2520reduces%2520relaxation%2520error%252C%2520and%2520that%2520the%250Aerror%2520is%2520eliminated%2520entirely%2520upon%2520performing%2520an%2520LP%2520relaxation%2520with%2520a%2520partition%250Aintelligently%2520designed%2520to%2520exploit%2520the%2520nature%2520of%2520the%2520ReLU%2520activations.%2520To%2520scale%250Athis%2520approach%2520to%2520large%2520networks%252C%2520we%2520consider%2520using%2520a%2520coarser%2520partition%2520whereby%250Athe%2520number%2520of%2520parts%2520in%2520the%2520partition%2520is%2520reduced.%2520We%2520prove%2520that%2520computing%2520such%2520a%250Acoarse%2520partition%2520that%2520directly%2520minimizes%2520the%2520LP%2520relaxation%2520error%2520is%2520NP-hard.%2520By%250Ainstead%2520minimizing%2520the%2520worst-case%2520LP%2520relaxation%2520error%252C%2520we%2520develop%2520a%2520closed-form%250Abranching%2520scheme%2520in%2520the%2520single-hidden%2520layer%2520case.%2520We%2520extend%2520the%2520analysis%2520to%2520the%250ASDP%252C%2520where%2520the%2520feasible%2520set%2520geometry%2520is%2520exploited%2520to%2520design%2520a%2520branching%2520scheme%250Athat%2520minimizes%2520the%2520worst-case%2520SDP%2520relaxation%2520error.%2520Experiments%2520on%2520MNIST%252C%250ACIFAR-10%252C%2520and%2520Wisconsin%2520breast%2520cancer%2520diagnosis%2520classifiers%2520demonstrate%250Asignificant%2520increases%2520in%2520the%2520percentages%2520of%2520test%2520samples%2520certified.%2520By%250Aindependently%2520increasing%2520the%2520input%2520size%2520and%2520the%2520number%2520of%2520layers%252C%2520we%250Aempirically%2520illustrate%2520under%2520which%2520regimes%2520the%2520branched%2520LP%2520and%2520branched%2520SDP%2520are%250Abest%2520applied.%2520Finally%252C%2520we%2520extend%2520our%2520LP%2520branching%2520method%2520into%2520a%2520multi-layer%250Abranching%2520heuristic%252C%2520which%2520attains%2520comparable%2520performance%2520to%2520prior%250Astate-of-the-art%2520heuristics%2520on%2520large-scale%252C%2520deep%2520neural%2520network%2520certification%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2101.09306v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Optimal%20Branching%20of%20Linear%20and%20Semidefinite%20Relaxations%20for%0A%20%20Neural%20Network%20Robustness%20Certification&entry.906535625=Brendon%20G.%20Anderson%20and%20Ziye%20Ma%20and%20Jingqi%20Li%20and%20Somayeh%20Sojoudi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20certifying%20the%20robustness%20of%20ReLU%20neural%20networks%0Aagainst%20adversarial%20input%20perturbations.%20To%20diminish%20the%20relaxation%20error%0Asuffered%20by%20the%20popular%20linear%20programming%20%28LP%29%20and%20semidefinite%20programming%0A%28SDP%29%20certification%20methods%2C%20we%20take%20a%20branch-and-bound%20approach%20to%20propose%0Apartitioning%20the%20input%20uncertainty%20set%20and%20solving%20the%20relaxations%20on%20each%20part%0Aseparately.%20We%20show%20that%20this%20approach%20reduces%20relaxation%20error%2C%20and%20that%20the%0Aerror%20is%20eliminated%20entirely%20upon%20performing%20an%20LP%20relaxation%20with%20a%20partition%0Aintelligently%20designed%20to%20exploit%20the%20nature%20of%20the%20ReLU%20activations.%20To%20scale%0Athis%20approach%20to%20large%20networks%2C%20we%20consider%20using%20a%20coarser%20partition%20whereby%0Athe%20number%20of%20parts%20in%20the%20partition%20is%20reduced.%20We%20prove%20that%20computing%20such%20a%0Acoarse%20partition%20that%20directly%20minimizes%20the%20LP%20relaxation%20error%20is%20NP-hard.%20By%0Ainstead%20minimizing%20the%20worst-case%20LP%20relaxation%20error%2C%20we%20develop%20a%20closed-form%0Abranching%20scheme%20in%20the%20single-hidden%20layer%20case.%20We%20extend%20the%20analysis%20to%20the%0ASDP%2C%20where%20the%20feasible%20set%20geometry%20is%20exploited%20to%20design%20a%20branching%20scheme%0Athat%20minimizes%20the%20worst-case%20SDP%20relaxation%20error.%20Experiments%20on%20MNIST%2C%0ACIFAR-10%2C%20and%20Wisconsin%20breast%20cancer%20diagnosis%20classifiers%20demonstrate%0Asignificant%20increases%20in%20the%20percentages%20of%20test%20samples%20certified.%20By%0Aindependently%20increasing%20the%20input%20size%20and%20the%20number%20of%20layers%2C%20we%0Aempirically%20illustrate%20under%20which%20regimes%20the%20branched%20LP%20and%20branched%20SDP%20are%0Abest%20applied.%20Finally%2C%20we%20extend%20our%20LP%20branching%20method%20into%20a%20multi-layer%0Abranching%20heuristic%2C%20which%20attains%20comparable%20performance%20to%20prior%0Astate-of-the-art%20heuristics%20on%20large-scale%2C%20deep%20neural%20network%20certification%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2101.09306v3&entry.124074799=Read"},
{"title": "Reducing Catastrophic Forgetting in Online Class Incremental Learning\n  Using Self-Distillation", "author": "Kotaro Nagata and Hiromu Ono and Kazuhiro Hotta", "abstract": "  In continual learning, there is a serious problem of catastrophic forgetting,\nin which previous knowledge is forgotten when a model learns new tasks. Various\nmethods have been proposed to solve this problem. Replay methods which replay\ndata from previous tasks in later training, have shown good accuracy. However,\nreplay methods have a generalizability problem from a limited memory buffer. In\nthis paper, we tried to solve this problem by acquiring transferable knowledge\nthrough self-distillation using highly generalizable output in shallow layer as\na teacher. Furthermore, when we deal with a large number of classes or\nchallenging data, there is a risk of learning not converging and not\nexperiencing overfitting. Therefore, we attempted to achieve more efficient and\nthorough learning by prioritizing the storage of easily misclassified samples\nthrough a new method of memory update. We confirmed that our proposed method\noutperformed conventional methods by experiments on CIFAR10, CIFAR100, and\nMiniimageNet datasets.\n", "link": "http://arxiv.org/abs/2409.11329v1", "date": "2024-09-17", "relevancy": 2.413, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5006}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Catastrophic%20Forgetting%20in%20Online%20Class%20Incremental%20Learning%0A%20%20Using%20Self-Distillation&body=Title%3A%20Reducing%20Catastrophic%20Forgetting%20in%20Online%20Class%20Incremental%20Learning%0A%20%20Using%20Self-Distillation%0AAuthor%3A%20Kotaro%20Nagata%20and%20Hiromu%20Ono%20and%20Kazuhiro%20Hotta%0AAbstract%3A%20%20%20In%20continual%20learning%2C%20there%20is%20a%20serious%20problem%20of%20catastrophic%20forgetting%2C%0Ain%20which%20previous%20knowledge%20is%20forgotten%20when%20a%20model%20learns%20new%20tasks.%20Various%0Amethods%20have%20been%20proposed%20to%20solve%20this%20problem.%20Replay%20methods%20which%20replay%0Adata%20from%20previous%20tasks%20in%20later%20training%2C%20have%20shown%20good%20accuracy.%20However%2C%0Areplay%20methods%20have%20a%20generalizability%20problem%20from%20a%20limited%20memory%20buffer.%20In%0Athis%20paper%2C%20we%20tried%20to%20solve%20this%20problem%20by%20acquiring%20transferable%20knowledge%0Athrough%20self-distillation%20using%20highly%20generalizable%20output%20in%20shallow%20layer%20as%0Aa%20teacher.%20Furthermore%2C%20when%20we%20deal%20with%20a%20large%20number%20of%20classes%20or%0Achallenging%20data%2C%20there%20is%20a%20risk%20of%20learning%20not%20converging%20and%20not%0Aexperiencing%20overfitting.%20Therefore%2C%20we%20attempted%20to%20achieve%20more%20efficient%20and%0Athorough%20learning%20by%20prioritizing%20the%20storage%20of%20easily%20misclassified%20samples%0Athrough%20a%20new%20method%20of%20memory%20update.%20We%20confirmed%20that%20our%20proposed%20method%0Aoutperformed%20conventional%20methods%20by%20experiments%20on%20CIFAR10%2C%20CIFAR100%2C%20and%0AMiniimageNet%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Catastrophic%2520Forgetting%2520in%2520Online%2520Class%2520Incremental%2520Learning%250A%2520%2520Using%2520Self-Distillation%26entry.906535625%3DKotaro%2520Nagata%2520and%2520Hiromu%2520Ono%2520and%2520Kazuhiro%2520Hotta%26entry.1292438233%3D%2520%2520In%2520continual%2520learning%252C%2520there%2520is%2520a%2520serious%2520problem%2520of%2520catastrophic%2520forgetting%252C%250Ain%2520which%2520previous%2520knowledge%2520is%2520forgotten%2520when%2520a%2520model%2520learns%2520new%2520tasks.%2520Various%250Amethods%2520have%2520been%2520proposed%2520to%2520solve%2520this%2520problem.%2520Replay%2520methods%2520which%2520replay%250Adata%2520from%2520previous%2520tasks%2520in%2520later%2520training%252C%2520have%2520shown%2520good%2520accuracy.%2520However%252C%250Areplay%2520methods%2520have%2520a%2520generalizability%2520problem%2520from%2520a%2520limited%2520memory%2520buffer.%2520In%250Athis%2520paper%252C%2520we%2520tried%2520to%2520solve%2520this%2520problem%2520by%2520acquiring%2520transferable%2520knowledge%250Athrough%2520self-distillation%2520using%2520highly%2520generalizable%2520output%2520in%2520shallow%2520layer%2520as%250Aa%2520teacher.%2520Furthermore%252C%2520when%2520we%2520deal%2520with%2520a%2520large%2520number%2520of%2520classes%2520or%250Achallenging%2520data%252C%2520there%2520is%2520a%2520risk%2520of%2520learning%2520not%2520converging%2520and%2520not%250Aexperiencing%2520overfitting.%2520Therefore%252C%2520we%2520attempted%2520to%2520achieve%2520more%2520efficient%2520and%250Athorough%2520learning%2520by%2520prioritizing%2520the%2520storage%2520of%2520easily%2520misclassified%2520samples%250Athrough%2520a%2520new%2520method%2520of%2520memory%2520update.%2520We%2520confirmed%2520that%2520our%2520proposed%2520method%250Aoutperformed%2520conventional%2520methods%2520by%2520experiments%2520on%2520CIFAR10%252C%2520CIFAR100%252C%2520and%250AMiniimageNet%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Catastrophic%20Forgetting%20in%20Online%20Class%20Incremental%20Learning%0A%20%20Using%20Self-Distillation&entry.906535625=Kotaro%20Nagata%20and%20Hiromu%20Ono%20and%20Kazuhiro%20Hotta&entry.1292438233=%20%20In%20continual%20learning%2C%20there%20is%20a%20serious%20problem%20of%20catastrophic%20forgetting%2C%0Ain%20which%20previous%20knowledge%20is%20forgotten%20when%20a%20model%20learns%20new%20tasks.%20Various%0Amethods%20have%20been%20proposed%20to%20solve%20this%20problem.%20Replay%20methods%20which%20replay%0Adata%20from%20previous%20tasks%20in%20later%20training%2C%20have%20shown%20good%20accuracy.%20However%2C%0Areplay%20methods%20have%20a%20generalizability%20problem%20from%20a%20limited%20memory%20buffer.%20In%0Athis%20paper%2C%20we%20tried%20to%20solve%20this%20problem%20by%20acquiring%20transferable%20knowledge%0Athrough%20self-distillation%20using%20highly%20generalizable%20output%20in%20shallow%20layer%20as%0Aa%20teacher.%20Furthermore%2C%20when%20we%20deal%20with%20a%20large%20number%20of%20classes%20or%0Achallenging%20data%2C%20there%20is%20a%20risk%20of%20learning%20not%20converging%20and%20not%0Aexperiencing%20overfitting.%20Therefore%2C%20we%20attempted%20to%20achieve%20more%20efficient%20and%0Athorough%20learning%20by%20prioritizing%20the%20storage%20of%20easily%20misclassified%20samples%0Athrough%20a%20new%20method%20of%20memory%20update.%20We%20confirmed%20that%20our%20proposed%20method%0Aoutperformed%20conventional%20methods%20by%20experiments%20on%20CIFAR10%2C%20CIFAR100%2C%20and%0AMiniimageNet%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11329v1&entry.124074799=Read"},
{"title": "Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained\n  Image Denoisers", "author": "Zixuan Fu and Lanqing Guo and Chong Wang and Yufei Wang and Zhihao Li and Bihan Wen", "abstract": "  Recent advancements in deep learning have shown impressive results in image\nand video denoising, leveraging extensive pairs of noisy and noise-free data\nfor supervision. However, the challenge of acquiring paired videos for dynamic\nscenes hampers the practical deployment of deep video denoising techniques. In\ncontrast, this obstacle is less pronounced in image denoising, where paired\ndata is more readily available. Thus, a well-trained image denoiser could serve\nas a reliable spatial prior for video denoising. In this paper, we propose a\nnovel unsupervised video denoising framework, named ``Temporal As a Plugin''\n(TAP), which integrates tunable temporal modules into a pre-trained image\ndenoiser. By incorporating temporal modules, our method can harness temporal\ninformation across noisy frames, complementing its power of spatial denoising.\nFurthermore, we introduce a progressive fine-tuning strategy that refines each\ntemporal module using the generated pseudo clean video frames, progressively\nenhancing the network's denoising performance. Compared to other unsupervised\nvideo denoising methods, our framework demonstrates superior performance on\nboth sRGB and raw video denoising datasets.\n", "link": "http://arxiv.org/abs/2409.11256v1", "date": "2024-09-17", "relevancy": 2.4091, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.619}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6044}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20As%20a%20Plugin%3A%20Unsupervised%20Video%20Denoising%20with%20Pre-Trained%0A%20%20Image%20Denoisers&body=Title%3A%20Temporal%20As%20a%20Plugin%3A%20Unsupervised%20Video%20Denoising%20with%20Pre-Trained%0A%20%20Image%20Denoisers%0AAuthor%3A%20Zixuan%20Fu%20and%20Lanqing%20Guo%20and%20Chong%20Wang%20and%20Yufei%20Wang%20and%20Zhihao%20Li%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20have%20shown%20impressive%20results%20in%20image%0Aand%20video%20denoising%2C%20leveraging%20extensive%20pairs%20of%20noisy%20and%20noise-free%20data%0Afor%20supervision.%20However%2C%20the%20challenge%20of%20acquiring%20paired%20videos%20for%20dynamic%0Ascenes%20hampers%20the%20practical%20deployment%20of%20deep%20video%20denoising%20techniques.%20In%0Acontrast%2C%20this%20obstacle%20is%20less%20pronounced%20in%20image%20denoising%2C%20where%20paired%0Adata%20is%20more%20readily%20available.%20Thus%2C%20a%20well-trained%20image%20denoiser%20could%20serve%0Aas%20a%20reliable%20spatial%20prior%20for%20video%20denoising.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20unsupervised%20video%20denoising%20framework%2C%20named%20%60%60Temporal%20As%20a%20Plugin%27%27%0A%28TAP%29%2C%20which%20integrates%20tunable%20temporal%20modules%20into%20a%20pre-trained%20image%0Adenoiser.%20By%20incorporating%20temporal%20modules%2C%20our%20method%20can%20harness%20temporal%0Ainformation%20across%20noisy%20frames%2C%20complementing%20its%20power%20of%20spatial%20denoising.%0AFurthermore%2C%20we%20introduce%20a%20progressive%20fine-tuning%20strategy%20that%20refines%20each%0Atemporal%20module%20using%20the%20generated%20pseudo%20clean%20video%20frames%2C%20progressively%0Aenhancing%20the%20network%27s%20denoising%20performance.%20Compared%20to%20other%20unsupervised%0Avideo%20denoising%20methods%2C%20our%20framework%20demonstrates%20superior%20performance%20on%0Aboth%20sRGB%20and%20raw%20video%20denoising%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520As%2520a%2520Plugin%253A%2520Unsupervised%2520Video%2520Denoising%2520with%2520Pre-Trained%250A%2520%2520Image%2520Denoisers%26entry.906535625%3DZixuan%2520Fu%2520and%2520Lanqing%2520Guo%2520and%2520Chong%2520Wang%2520and%2520Yufei%2520Wang%2520and%2520Zhihao%2520Li%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520shown%2520impressive%2520results%2520in%2520image%250Aand%2520video%2520denoising%252C%2520leveraging%2520extensive%2520pairs%2520of%2520noisy%2520and%2520noise-free%2520data%250Afor%2520supervision.%2520However%252C%2520the%2520challenge%2520of%2520acquiring%2520paired%2520videos%2520for%2520dynamic%250Ascenes%2520hampers%2520the%2520practical%2520deployment%2520of%2520deep%2520video%2520denoising%2520techniques.%2520In%250Acontrast%252C%2520this%2520obstacle%2520is%2520less%2520pronounced%2520in%2520image%2520denoising%252C%2520where%2520paired%250Adata%2520is%2520more%2520readily%2520available.%2520Thus%252C%2520a%2520well-trained%2520image%2520denoiser%2520could%2520serve%250Aas%2520a%2520reliable%2520spatial%2520prior%2520for%2520video%2520denoising.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520unsupervised%2520video%2520denoising%2520framework%252C%2520named%2520%2560%2560Temporal%2520As%2520a%2520Plugin%2527%2527%250A%2528TAP%2529%252C%2520which%2520integrates%2520tunable%2520temporal%2520modules%2520into%2520a%2520pre-trained%2520image%250Adenoiser.%2520By%2520incorporating%2520temporal%2520modules%252C%2520our%2520method%2520can%2520harness%2520temporal%250Ainformation%2520across%2520noisy%2520frames%252C%2520complementing%2520its%2520power%2520of%2520spatial%2520denoising.%250AFurthermore%252C%2520we%2520introduce%2520a%2520progressive%2520fine-tuning%2520strategy%2520that%2520refines%2520each%250Atemporal%2520module%2520using%2520the%2520generated%2520pseudo%2520clean%2520video%2520frames%252C%2520progressively%250Aenhancing%2520the%2520network%2527s%2520denoising%2520performance.%2520Compared%2520to%2520other%2520unsupervised%250Avideo%2520denoising%2520methods%252C%2520our%2520framework%2520demonstrates%2520superior%2520performance%2520on%250Aboth%2520sRGB%2520and%2520raw%2520video%2520denoising%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20As%20a%20Plugin%3A%20Unsupervised%20Video%20Denoising%20with%20Pre-Trained%0A%20%20Image%20Denoisers&entry.906535625=Zixuan%20Fu%20and%20Lanqing%20Guo%20and%20Chong%20Wang%20and%20Yufei%20Wang%20and%20Zhihao%20Li%20and%20Bihan%20Wen&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20shown%20impressive%20results%20in%20image%0Aand%20video%20denoising%2C%20leveraging%20extensive%20pairs%20of%20noisy%20and%20noise-free%20data%0Afor%20supervision.%20However%2C%20the%20challenge%20of%20acquiring%20paired%20videos%20for%20dynamic%0Ascenes%20hampers%20the%20practical%20deployment%20of%20deep%20video%20denoising%20techniques.%20In%0Acontrast%2C%20this%20obstacle%20is%20less%20pronounced%20in%20image%20denoising%2C%20where%20paired%0Adata%20is%20more%20readily%20available.%20Thus%2C%20a%20well-trained%20image%20denoiser%20could%20serve%0Aas%20a%20reliable%20spatial%20prior%20for%20video%20denoising.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20unsupervised%20video%20denoising%20framework%2C%20named%20%60%60Temporal%20As%20a%20Plugin%27%27%0A%28TAP%29%2C%20which%20integrates%20tunable%20temporal%20modules%20into%20a%20pre-trained%20image%0Adenoiser.%20By%20incorporating%20temporal%20modules%2C%20our%20method%20can%20harness%20temporal%0Ainformation%20across%20noisy%20frames%2C%20complementing%20its%20power%20of%20spatial%20denoising.%0AFurthermore%2C%20we%20introduce%20a%20progressive%20fine-tuning%20strategy%20that%20refines%20each%0Atemporal%20module%20using%20the%20generated%20pseudo%20clean%20video%20frames%2C%20progressively%0Aenhancing%20the%20network%27s%20denoising%20performance.%20Compared%20to%20other%20unsupervised%0Avideo%20denoising%20methods%2C%20our%20framework%20demonstrates%20superior%20performance%20on%0Aboth%20sRGB%20and%20raw%20video%20denoising%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11256v1&entry.124074799=Read"},
{"title": "Transferable-guided Attention Is All You Need for Video Domain\n  Adaptation", "author": "Andr\u00e9 Sacilotti and Samuel Felipe dos Santos and Nicu Sebe and Jurandy Almeida", "abstract": "  Unsupervised domain adaptation (UDA) in videos is a challenging task that\nremains not well explored compared to image-based UDA techniques. Although\nvision transformers (ViT) achieve state-of-the-art performance in many computer\nvision tasks, their use in video UDA has been little explored. Our key idea is\nto use transformer layers as a feature encoder and incorporate spatial and\ntemporal transferability relationships into the attention mechanism. A\nTransferable-guided Attention (TransferAttn) framework is then developed to\nexploit the capacity of the transformer to adapt cross-domain knowledge across\ndifferent backbones. To improve the transferability of ViT, we introduce a\nnovel and effective module, named Domain Transferable-guided Attention Block\n(DTAB). DTAB compels ViT to focus on the spatio-temporal transferability\nrelationship among video frames by changing the self-attention mechanism to a\ntransferability attention mechanism. Extensive experiments were conducted on\nUCF-HMDB, Kinetics-Gameplay, and Kinetics-NEC Drone datasets, with different\nbackbones, like ResNet101, I3D, and STAM, to verify the effectiveness of\nTransferAttn compared with state-of-the-art approaches. Also, we demonstrate\nthat DTAB yields performance gains when applied to other state-of-the-art\ntransformer-based UDA methods from both video and image domains. Our code is\navailable at https://github.com/Andre-Sacilotti/transferattn-project-code.\n", "link": "http://arxiv.org/abs/2407.01375v2", "date": "2024-09-17", "relevancy": 2.3797, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6294}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable-guided%20Attention%20Is%20All%20You%20Need%20for%20Video%20Domain%0A%20%20Adaptation&body=Title%3A%20Transferable-guided%20Attention%20Is%20All%20You%20Need%20for%20Video%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Andr%C3%A9%20Sacilotti%20and%20Samuel%20Felipe%20dos%20Santos%20and%20Nicu%20Sebe%20and%20Jurandy%20Almeida%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20in%20videos%20is%20a%20challenging%20task%20that%0Aremains%20not%20well%20explored%20compared%20to%20image-based%20UDA%20techniques.%20Although%0Avision%20transformers%20%28ViT%29%20achieve%20state-of-the-art%20performance%20in%20many%20computer%0Avision%20tasks%2C%20their%20use%20in%20video%20UDA%20has%20been%20little%20explored.%20Our%20key%20idea%20is%0Ato%20use%20transformer%20layers%20as%20a%20feature%20encoder%20and%20incorporate%20spatial%20and%0Atemporal%20transferability%20relationships%20into%20the%20attention%20mechanism.%20A%0ATransferable-guided%20Attention%20%28TransferAttn%29%20framework%20is%20then%20developed%20to%0Aexploit%20the%20capacity%20of%20the%20transformer%20to%20adapt%20cross-domain%20knowledge%20across%0Adifferent%20backbones.%20To%20improve%20the%20transferability%20of%20ViT%2C%20we%20introduce%20a%0Anovel%20and%20effective%20module%2C%20named%20Domain%20Transferable-guided%20Attention%20Block%0A%28DTAB%29.%20DTAB%20compels%20ViT%20to%20focus%20on%20the%20spatio-temporal%20transferability%0Arelationship%20among%20video%20frames%20by%20changing%20the%20self-attention%20mechanism%20to%20a%0Atransferability%20attention%20mechanism.%20Extensive%20experiments%20were%20conducted%20on%0AUCF-HMDB%2C%20Kinetics-Gameplay%2C%20and%20Kinetics-NEC%20Drone%20datasets%2C%20with%20different%0Abackbones%2C%20like%20ResNet101%2C%20I3D%2C%20and%20STAM%2C%20to%20verify%20the%20effectiveness%20of%0ATransferAttn%20compared%20with%20state-of-the-art%20approaches.%20Also%2C%20we%20demonstrate%0Athat%20DTAB%20yields%20performance%20gains%20when%20applied%20to%20other%20state-of-the-art%0Atransformer-based%20UDA%20methods%20from%20both%20video%20and%20image%20domains.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Andre-Sacilotti/transferattn-project-code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable-guided%2520Attention%2520Is%2520All%2520You%2520Need%2520for%2520Video%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DAndr%25C3%25A9%2520Sacilotti%2520and%2520Samuel%2520Felipe%2520dos%2520Santos%2520and%2520Nicu%2520Sebe%2520and%2520Jurandy%2520Almeida%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520in%2520videos%2520is%2520a%2520challenging%2520task%2520that%250Aremains%2520not%2520well%2520explored%2520compared%2520to%2520image-based%2520UDA%2520techniques.%2520Although%250Avision%2520transformers%2520%2528ViT%2529%2520achieve%2520state-of-the-art%2520performance%2520in%2520many%2520computer%250Avision%2520tasks%252C%2520their%2520use%2520in%2520video%2520UDA%2520has%2520been%2520little%2520explored.%2520Our%2520key%2520idea%2520is%250Ato%2520use%2520transformer%2520layers%2520as%2520a%2520feature%2520encoder%2520and%2520incorporate%2520spatial%2520and%250Atemporal%2520transferability%2520relationships%2520into%2520the%2520attention%2520mechanism.%2520A%250ATransferable-guided%2520Attention%2520%2528TransferAttn%2529%2520framework%2520is%2520then%2520developed%2520to%250Aexploit%2520the%2520capacity%2520of%2520the%2520transformer%2520to%2520adapt%2520cross-domain%2520knowledge%2520across%250Adifferent%2520backbones.%2520To%2520improve%2520the%2520transferability%2520of%2520ViT%252C%2520we%2520introduce%2520a%250Anovel%2520and%2520effective%2520module%252C%2520named%2520Domain%2520Transferable-guided%2520Attention%2520Block%250A%2528DTAB%2529.%2520DTAB%2520compels%2520ViT%2520to%2520focus%2520on%2520the%2520spatio-temporal%2520transferability%250Arelationship%2520among%2520video%2520frames%2520by%2520changing%2520the%2520self-attention%2520mechanism%2520to%2520a%250Atransferability%2520attention%2520mechanism.%2520Extensive%2520experiments%2520were%2520conducted%2520on%250AUCF-HMDB%252C%2520Kinetics-Gameplay%252C%2520and%2520Kinetics-NEC%2520Drone%2520datasets%252C%2520with%2520different%250Abackbones%252C%2520like%2520ResNet101%252C%2520I3D%252C%2520and%2520STAM%252C%2520to%2520verify%2520the%2520effectiveness%2520of%250ATransferAttn%2520compared%2520with%2520state-of-the-art%2520approaches.%2520Also%252C%2520we%2520demonstrate%250Athat%2520DTAB%2520yields%2520performance%2520gains%2520when%2520applied%2520to%2520other%2520state-of-the-art%250Atransformer-based%2520UDA%2520methods%2520from%2520both%2520video%2520and%2520image%2520domains.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Andre-Sacilotti/transferattn-project-code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable-guided%20Attention%20Is%20All%20You%20Need%20for%20Video%20Domain%0A%20%20Adaptation&entry.906535625=Andr%C3%A9%20Sacilotti%20and%20Samuel%20Felipe%20dos%20Santos%20and%20Nicu%20Sebe%20and%20Jurandy%20Almeida&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20in%20videos%20is%20a%20challenging%20task%20that%0Aremains%20not%20well%20explored%20compared%20to%20image-based%20UDA%20techniques.%20Although%0Avision%20transformers%20%28ViT%29%20achieve%20state-of-the-art%20performance%20in%20many%20computer%0Avision%20tasks%2C%20their%20use%20in%20video%20UDA%20has%20been%20little%20explored.%20Our%20key%20idea%20is%0Ato%20use%20transformer%20layers%20as%20a%20feature%20encoder%20and%20incorporate%20spatial%20and%0Atemporal%20transferability%20relationships%20into%20the%20attention%20mechanism.%20A%0ATransferable-guided%20Attention%20%28TransferAttn%29%20framework%20is%20then%20developed%20to%0Aexploit%20the%20capacity%20of%20the%20transformer%20to%20adapt%20cross-domain%20knowledge%20across%0Adifferent%20backbones.%20To%20improve%20the%20transferability%20of%20ViT%2C%20we%20introduce%20a%0Anovel%20and%20effective%20module%2C%20named%20Domain%20Transferable-guided%20Attention%20Block%0A%28DTAB%29.%20DTAB%20compels%20ViT%20to%20focus%20on%20the%20spatio-temporal%20transferability%0Arelationship%20among%20video%20frames%20by%20changing%20the%20self-attention%20mechanism%20to%20a%0Atransferability%20attention%20mechanism.%20Extensive%20experiments%20were%20conducted%20on%0AUCF-HMDB%2C%20Kinetics-Gameplay%2C%20and%20Kinetics-NEC%20Drone%20datasets%2C%20with%20different%0Abackbones%2C%20like%20ResNet101%2C%20I3D%2C%20and%20STAM%2C%20to%20verify%20the%20effectiveness%20of%0ATransferAttn%20compared%20with%20state-of-the-art%20approaches.%20Also%2C%20we%20demonstrate%0Athat%20DTAB%20yields%20performance%20gains%20when%20applied%20to%20other%20state-of-the-art%0Atransformer-based%20UDA%20methods%20from%20both%20video%20and%20image%20domains.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Andre-Sacilotti/transferattn-project-code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01375v2&entry.124074799=Read"},
{"title": "A Diffusion Approach to Radiance Field Relighting using\n  Multi-Illumination Synthesis", "author": "Yohan Poirier-Ginter and Alban Gauthier and Julien Philip and Jean-Francois Lalonde and George Drettakis", "abstract": "  Relighting radiance fields is severely underconstrained for multi-view data,\nwhich is most often captured under a single illumination condition; It is\nespecially hard for full scenes containing multiple objects. We introduce a\nmethod to create relightable radiance fields using such single-illumination\ndata by exploiting priors extracted from 2D image diffusion models. We first\nfine-tune a 2D diffusion model on a multi-illumination dataset conditioned by\nlight direction, allowing us to augment a single-illumination capture into a\nrealistic -- but possibly inconsistent -- multi-illumination dataset from\ndirectly defined light directions. We use this augmented data to create a\nrelightable radiance field represented by 3D Gaussian splats. To allow direct\ncontrol of light direction for low-frequency lighting, we represent appearance\nwith a multi-layer perceptron parameterized on light direction. To enforce\nmulti-view consistency and overcome inaccuracies we optimize a per-image\nauxiliary feature vector. We show results on synthetic and real multi-view data\nunder single illumination, demonstrating that our method successfully exploits\n2D diffusion model priors to allow realistic 3D relighting for complete scenes.\nProject site\nhttps://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/\n", "link": "http://arxiv.org/abs/2409.08947v2", "date": "2024-09-17", "relevancy": 2.3643, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5921}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5921}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Diffusion%20Approach%20to%20Radiance%20Field%20Relighting%20using%0A%20%20Multi-Illumination%20Synthesis&body=Title%3A%20A%20Diffusion%20Approach%20to%20Radiance%20Field%20Relighting%20using%0A%20%20Multi-Illumination%20Synthesis%0AAuthor%3A%20Yohan%20Poirier-Ginter%20and%20Alban%20Gauthier%20and%20Julien%20Philip%20and%20Jean-Francois%20Lalonde%20and%20George%20Drettakis%0AAbstract%3A%20%20%20Relighting%20radiance%20fields%20is%20severely%20underconstrained%20for%20multi-view%20data%2C%0Awhich%20is%20most%20often%20captured%20under%20a%20single%20illumination%20condition%3B%20It%20is%0Aespecially%20hard%20for%20full%20scenes%20containing%20multiple%20objects.%20We%20introduce%20a%0Amethod%20to%20create%20relightable%20radiance%20fields%20using%20such%20single-illumination%0Adata%20by%20exploiting%20priors%20extracted%20from%202D%20image%20diffusion%20models.%20We%20first%0Afine-tune%20a%202D%20diffusion%20model%20on%20a%20multi-illumination%20dataset%20conditioned%20by%0Alight%20direction%2C%20allowing%20us%20to%20augment%20a%20single-illumination%20capture%20into%20a%0Arealistic%20--%20but%20possibly%20inconsistent%20--%20multi-illumination%20dataset%20from%0Adirectly%20defined%20light%20directions.%20We%20use%20this%20augmented%20data%20to%20create%20a%0Arelightable%20radiance%20field%20represented%20by%203D%20Gaussian%20splats.%20To%20allow%20direct%0Acontrol%20of%20light%20direction%20for%20low-frequency%20lighting%2C%20we%20represent%20appearance%0Awith%20a%20multi-layer%20perceptron%20parameterized%20on%20light%20direction.%20To%20enforce%0Amulti-view%20consistency%20and%20overcome%20inaccuracies%20we%20optimize%20a%20per-image%0Aauxiliary%20feature%20vector.%20We%20show%20results%20on%20synthetic%20and%20real%20multi-view%20data%0Aunder%20single%20illumination%2C%20demonstrating%20that%20our%20method%20successfully%20exploits%0A2D%20diffusion%20model%20priors%20to%20allow%20realistic%203D%20relighting%20for%20complete%20scenes.%0AProject%20site%0Ahttps%3A//repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Diffusion%2520Approach%2520to%2520Radiance%2520Field%2520Relighting%2520using%250A%2520%2520Multi-Illumination%2520Synthesis%26entry.906535625%3DYohan%2520Poirier-Ginter%2520and%2520Alban%2520Gauthier%2520and%2520Julien%2520Philip%2520and%2520Jean-Francois%2520Lalonde%2520and%2520George%2520Drettakis%26entry.1292438233%3D%2520%2520Relighting%2520radiance%2520fields%2520is%2520severely%2520underconstrained%2520for%2520multi-view%2520data%252C%250Awhich%2520is%2520most%2520often%2520captured%2520under%2520a%2520single%2520illumination%2520condition%253B%2520It%2520is%250Aespecially%2520hard%2520for%2520full%2520scenes%2520containing%2520multiple%2520objects.%2520We%2520introduce%2520a%250Amethod%2520to%2520create%2520relightable%2520radiance%2520fields%2520using%2520such%2520single-illumination%250Adata%2520by%2520exploiting%2520priors%2520extracted%2520from%25202D%2520image%2520diffusion%2520models.%2520We%2520first%250Afine-tune%2520a%25202D%2520diffusion%2520model%2520on%2520a%2520multi-illumination%2520dataset%2520conditioned%2520by%250Alight%2520direction%252C%2520allowing%2520us%2520to%2520augment%2520a%2520single-illumination%2520capture%2520into%2520a%250Arealistic%2520--%2520but%2520possibly%2520inconsistent%2520--%2520multi-illumination%2520dataset%2520from%250Adirectly%2520defined%2520light%2520directions.%2520We%2520use%2520this%2520augmented%2520data%2520to%2520create%2520a%250Arelightable%2520radiance%2520field%2520represented%2520by%25203D%2520Gaussian%2520splats.%2520To%2520allow%2520direct%250Acontrol%2520of%2520light%2520direction%2520for%2520low-frequency%2520lighting%252C%2520we%2520represent%2520appearance%250Awith%2520a%2520multi-layer%2520perceptron%2520parameterized%2520on%2520light%2520direction.%2520To%2520enforce%250Amulti-view%2520consistency%2520and%2520overcome%2520inaccuracies%2520we%2520optimize%2520a%2520per-image%250Aauxiliary%2520feature%2520vector.%2520We%2520show%2520results%2520on%2520synthetic%2520and%2520real%2520multi-view%2520data%250Aunder%2520single%2520illumination%252C%2520demonstrating%2520that%2520our%2520method%2520successfully%2520exploits%250A2D%2520diffusion%2520model%2520priors%2520to%2520allow%2520realistic%25203D%2520relighting%2520for%2520complete%2520scenes.%250AProject%2520site%250Ahttps%253A//repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Diffusion%20Approach%20to%20Radiance%20Field%20Relighting%20using%0A%20%20Multi-Illumination%20Synthesis&entry.906535625=Yohan%20Poirier-Ginter%20and%20Alban%20Gauthier%20and%20Julien%20Philip%20and%20Jean-Francois%20Lalonde%20and%20George%20Drettakis&entry.1292438233=%20%20Relighting%20radiance%20fields%20is%20severely%20underconstrained%20for%20multi-view%20data%2C%0Awhich%20is%20most%20often%20captured%20under%20a%20single%20illumination%20condition%3B%20It%20is%0Aespecially%20hard%20for%20full%20scenes%20containing%20multiple%20objects.%20We%20introduce%20a%0Amethod%20to%20create%20relightable%20radiance%20fields%20using%20such%20single-illumination%0Adata%20by%20exploiting%20priors%20extracted%20from%202D%20image%20diffusion%20models.%20We%20first%0Afine-tune%20a%202D%20diffusion%20model%20on%20a%20multi-illumination%20dataset%20conditioned%20by%0Alight%20direction%2C%20allowing%20us%20to%20augment%20a%20single-illumination%20capture%20into%20a%0Arealistic%20--%20but%20possibly%20inconsistent%20--%20multi-illumination%20dataset%20from%0Adirectly%20defined%20light%20directions.%20We%20use%20this%20augmented%20data%20to%20create%20a%0Arelightable%20radiance%20field%20represented%20by%203D%20Gaussian%20splats.%20To%20allow%20direct%0Acontrol%20of%20light%20direction%20for%20low-frequency%20lighting%2C%20we%20represent%20appearance%0Awith%20a%20multi-layer%20perceptron%20parameterized%20on%20light%20direction.%20To%20enforce%0Amulti-view%20consistency%20and%20overcome%20inaccuracies%20we%20optimize%20a%20per-image%0Aauxiliary%20feature%20vector.%20We%20show%20results%20on%20synthetic%20and%20real%20multi-view%20data%0Aunder%20single%20illumination%2C%20demonstrating%20that%20our%20method%20successfully%20exploits%0A2D%20diffusion%20model%20priors%20to%20allow%20realistic%203D%20relighting%20for%20complete%20scenes.%0AProject%20site%0Ahttps%3A//repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08947v2&entry.124074799=Read"},
{"title": "UniMODE: Unified Monocular 3D Object Detection", "author": "Zhuoling Li and Xiaogang Xu and SerNam Lim and Hengshuang Zhao", "abstract": "  Realizing unified monocular 3D object detection, including both indoor and\noutdoor scenes, holds great importance in applications like robot navigation.\nHowever, involving various scenarios of data to train models poses challenges\ndue to their significantly different characteristics, e.g., diverse geometry\nproperties and heterogeneous domain distributions. To address these challenges,\nwe build a detector based on the bird's-eye-view (BEV) detection paradigm,\nwhere the explicit feature projection is beneficial to addressing the geometry\nlearning ambiguity when employing multiple scenarios of data to train\ndetectors. Then, we split the classical BEV detection architecture into two\nstages and propose an uneven BEV grid design to handle the convergence\ninstability caused by the aforementioned challenges. Moreover, we develop a\nsparse BEV feature projection strategy to reduce computational cost and a\nunified domain alignment method to handle heterogeneous domains. Combining\nthese techniques, a unified detector UniMODE is derived, which surpasses the\nprevious state-of-the-art on the challenging Omni3D dataset (a large-scale\ndataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the\nfirst successful generalization of a BEV detector to unified 3D object\ndetection.\n", "link": "http://arxiv.org/abs/2402.18573v4", "date": "2024-09-17", "relevancy": 2.3548, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6052}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMODE%3A%20Unified%20Monocular%203D%20Object%20Detection&body=Title%3A%20UniMODE%3A%20Unified%20Monocular%203D%20Object%20Detection%0AAuthor%3A%20Zhuoling%20Li%20and%20Xiaogang%20Xu%20and%20SerNam%20Lim%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Realizing%20unified%20monocular%203D%20object%20detection%2C%20including%20both%20indoor%20and%0Aoutdoor%20scenes%2C%20holds%20great%20importance%20in%20applications%20like%20robot%20navigation.%0AHowever%2C%20involving%20various%20scenarios%20of%20data%20to%20train%20models%20poses%20challenges%0Adue%20to%20their%20significantly%20different%20characteristics%2C%20e.g.%2C%20diverse%20geometry%0Aproperties%20and%20heterogeneous%20domain%20distributions.%20To%20address%20these%20challenges%2C%0Awe%20build%20a%20detector%20based%20on%20the%20bird%27s-eye-view%20%28BEV%29%20detection%20paradigm%2C%0Awhere%20the%20explicit%20feature%20projection%20is%20beneficial%20to%20addressing%20the%20geometry%0Alearning%20ambiguity%20when%20employing%20multiple%20scenarios%20of%20data%20to%20train%0Adetectors.%20Then%2C%20we%20split%20the%20classical%20BEV%20detection%20architecture%20into%20two%0Astages%20and%20propose%20an%20uneven%20BEV%20grid%20design%20to%20handle%20the%20convergence%0Ainstability%20caused%20by%20the%20aforementioned%20challenges.%20Moreover%2C%20we%20develop%20a%0Asparse%20BEV%20feature%20projection%20strategy%20to%20reduce%20computational%20cost%20and%20a%0Aunified%20domain%20alignment%20method%20to%20handle%20heterogeneous%20domains.%20Combining%0Athese%20techniques%2C%20a%20unified%20detector%20UniMODE%20is%20derived%2C%20which%20surpasses%20the%0Aprevious%20state-of-the-art%20on%20the%20challenging%20Omni3D%20dataset%20%28a%20large-scale%0Adataset%20including%20both%20indoor%20and%20outdoor%20scenes%29%20by%204.9%25%20AP_3D%2C%20revealing%20the%0Afirst%20successful%20generalization%20of%20a%20BEV%20detector%20to%20unified%203D%20object%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18573v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMODE%253A%2520Unified%2520Monocular%25203D%2520Object%2520Detection%26entry.906535625%3DZhuoling%2520Li%2520and%2520Xiaogang%2520Xu%2520and%2520SerNam%2520Lim%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Realizing%2520unified%2520monocular%25203D%2520object%2520detection%252C%2520including%2520both%2520indoor%2520and%250Aoutdoor%2520scenes%252C%2520holds%2520great%2520importance%2520in%2520applications%2520like%2520robot%2520navigation.%250AHowever%252C%2520involving%2520various%2520scenarios%2520of%2520data%2520to%2520train%2520models%2520poses%2520challenges%250Adue%2520to%2520their%2520significantly%2520different%2520characteristics%252C%2520e.g.%252C%2520diverse%2520geometry%250Aproperties%2520and%2520heterogeneous%2520domain%2520distributions.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520build%2520a%2520detector%2520based%2520on%2520the%2520bird%2527s-eye-view%2520%2528BEV%2529%2520detection%2520paradigm%252C%250Awhere%2520the%2520explicit%2520feature%2520projection%2520is%2520beneficial%2520to%2520addressing%2520the%2520geometry%250Alearning%2520ambiguity%2520when%2520employing%2520multiple%2520scenarios%2520of%2520data%2520to%2520train%250Adetectors.%2520Then%252C%2520we%2520split%2520the%2520classical%2520BEV%2520detection%2520architecture%2520into%2520two%250Astages%2520and%2520propose%2520an%2520uneven%2520BEV%2520grid%2520design%2520to%2520handle%2520the%2520convergence%250Ainstability%2520caused%2520by%2520the%2520aforementioned%2520challenges.%2520Moreover%252C%2520we%2520develop%2520a%250Asparse%2520BEV%2520feature%2520projection%2520strategy%2520to%2520reduce%2520computational%2520cost%2520and%2520a%250Aunified%2520domain%2520alignment%2520method%2520to%2520handle%2520heterogeneous%2520domains.%2520Combining%250Athese%2520techniques%252C%2520a%2520unified%2520detector%2520UniMODE%2520is%2520derived%252C%2520which%2520surpasses%2520the%250Aprevious%2520state-of-the-art%2520on%2520the%2520challenging%2520Omni3D%2520dataset%2520%2528a%2520large-scale%250Adataset%2520including%2520both%2520indoor%2520and%2520outdoor%2520scenes%2529%2520by%25204.9%2525%2520AP_3D%252C%2520revealing%2520the%250Afirst%2520successful%2520generalization%2520of%2520a%2520BEV%2520detector%2520to%2520unified%25203D%2520object%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18573v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMODE%3A%20Unified%20Monocular%203D%20Object%20Detection&entry.906535625=Zhuoling%20Li%20and%20Xiaogang%20Xu%20and%20SerNam%20Lim%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Realizing%20unified%20monocular%203D%20object%20detection%2C%20including%20both%20indoor%20and%0Aoutdoor%20scenes%2C%20holds%20great%20importance%20in%20applications%20like%20robot%20navigation.%0AHowever%2C%20involving%20various%20scenarios%20of%20data%20to%20train%20models%20poses%20challenges%0Adue%20to%20their%20significantly%20different%20characteristics%2C%20e.g.%2C%20diverse%20geometry%0Aproperties%20and%20heterogeneous%20domain%20distributions.%20To%20address%20these%20challenges%2C%0Awe%20build%20a%20detector%20based%20on%20the%20bird%27s-eye-view%20%28BEV%29%20detection%20paradigm%2C%0Awhere%20the%20explicit%20feature%20projection%20is%20beneficial%20to%20addressing%20the%20geometry%0Alearning%20ambiguity%20when%20employing%20multiple%20scenarios%20of%20data%20to%20train%0Adetectors.%20Then%2C%20we%20split%20the%20classical%20BEV%20detection%20architecture%20into%20two%0Astages%20and%20propose%20an%20uneven%20BEV%20grid%20design%20to%20handle%20the%20convergence%0Ainstability%20caused%20by%20the%20aforementioned%20challenges.%20Moreover%2C%20we%20develop%20a%0Asparse%20BEV%20feature%20projection%20strategy%20to%20reduce%20computational%20cost%20and%20a%0Aunified%20domain%20alignment%20method%20to%20handle%20heterogeneous%20domains.%20Combining%0Athese%20techniques%2C%20a%20unified%20detector%20UniMODE%20is%20derived%2C%20which%20surpasses%20the%0Aprevious%20state-of-the-art%20on%20the%20challenging%20Omni3D%20dataset%20%28a%20large-scale%0Adataset%20including%20both%20indoor%20and%20outdoor%20scenes%29%20by%204.9%25%20AP_3D%2C%20revealing%20the%0Afirst%20successful%20generalization%20of%20a%20BEV%20detector%20to%20unified%203D%20object%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18573v4&entry.124074799=Read"},
{"title": "ULOC: Learning to Localize in Complex Large-Scale Environments with\n  Ultra-Wideband Ranges", "author": "Thien-Minh Nguyen and Yizhuo Yang and Tien-Dat Nguyen and Shenghai Yuan and Lihua Xie", "abstract": "  While UWB-based methods can achieve high localization accuracy in small-scale\nareas, their accuracy and reliability are significantly challenged in\nlarge-scale environments. In this paper, we propose a learning-based framework\nnamed ULOC for Ultra-Wideband (UWB) based localization in such complex\nlarge-scale environments. First, anchors are deployed in the environment\nwithout knowledge of their actual position. Then, UWB observations are\ncollected when the vehicle travels in the environment. At the same time,\nmap-consistent pose estimates are developed from registering (onboard\nself-localization) data with the prior map to provide the training labels. We\nthen propose a network based on MAMBA that learns the ranging patterns of UWBs\nover a complex large-scale environment. The experiment demonstrates that our\nsolution can ensure high localization accuracy on a large scale compared to the\nstate-of-the-art. We release our source code to benefit the community at\nhttps://github.com/brytsknguyen/uloc.\n", "link": "http://arxiv.org/abs/2409.11122v1", "date": "2024-09-17", "relevancy": 2.3534, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6405}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ULOC%3A%20Learning%20to%20Localize%20in%20Complex%20Large-Scale%20Environments%20with%0A%20%20Ultra-Wideband%20Ranges&body=Title%3A%20ULOC%3A%20Learning%20to%20Localize%20in%20Complex%20Large-Scale%20Environments%20with%0A%20%20Ultra-Wideband%20Ranges%0AAuthor%3A%20Thien-Minh%20Nguyen%20and%20Yizhuo%20Yang%20and%20Tien-Dat%20Nguyen%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20While%20UWB-based%20methods%20can%20achieve%20high%20localization%20accuracy%20in%20small-scale%0Aareas%2C%20their%20accuracy%20and%20reliability%20are%20significantly%20challenged%20in%0Alarge-scale%20environments.%20In%20this%20paper%2C%20we%20propose%20a%20learning-based%20framework%0Anamed%20ULOC%20for%20Ultra-Wideband%20%28UWB%29%20based%20localization%20in%20such%20complex%0Alarge-scale%20environments.%20First%2C%20anchors%20are%20deployed%20in%20the%20environment%0Awithout%20knowledge%20of%20their%20actual%20position.%20Then%2C%20UWB%20observations%20are%0Acollected%20when%20the%20vehicle%20travels%20in%20the%20environment.%20At%20the%20same%20time%2C%0Amap-consistent%20pose%20estimates%20are%20developed%20from%20registering%20%28onboard%0Aself-localization%29%20data%20with%20the%20prior%20map%20to%20provide%20the%20training%20labels.%20We%0Athen%20propose%20a%20network%20based%20on%20MAMBA%20that%20learns%20the%20ranging%20patterns%20of%20UWBs%0Aover%20a%20complex%20large-scale%20environment.%20The%20experiment%20demonstrates%20that%20our%0Asolution%20can%20ensure%20high%20localization%20accuracy%20on%20a%20large%20scale%20compared%20to%20the%0Astate-of-the-art.%20We%20release%20our%20source%20code%20to%20benefit%20the%20community%20at%0Ahttps%3A//github.com/brytsknguyen/uloc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DULOC%253A%2520Learning%2520to%2520Localize%2520in%2520Complex%2520Large-Scale%2520Environments%2520with%250A%2520%2520Ultra-Wideband%2520Ranges%26entry.906535625%3DThien-Minh%2520Nguyen%2520and%2520Yizhuo%2520Yang%2520and%2520Tien-Dat%2520Nguyen%2520and%2520Shenghai%2520Yuan%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520While%2520UWB-based%2520methods%2520can%2520achieve%2520high%2520localization%2520accuracy%2520in%2520small-scale%250Aareas%252C%2520their%2520accuracy%2520and%2520reliability%2520are%2520significantly%2520challenged%2520in%250Alarge-scale%2520environments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520learning-based%2520framework%250Anamed%2520ULOC%2520for%2520Ultra-Wideband%2520%2528UWB%2529%2520based%2520localization%2520in%2520such%2520complex%250Alarge-scale%2520environments.%2520First%252C%2520anchors%2520are%2520deployed%2520in%2520the%2520environment%250Awithout%2520knowledge%2520of%2520their%2520actual%2520position.%2520Then%252C%2520UWB%2520observations%2520are%250Acollected%2520when%2520the%2520vehicle%2520travels%2520in%2520the%2520environment.%2520At%2520the%2520same%2520time%252C%250Amap-consistent%2520pose%2520estimates%2520are%2520developed%2520from%2520registering%2520%2528onboard%250Aself-localization%2529%2520data%2520with%2520the%2520prior%2520map%2520to%2520provide%2520the%2520training%2520labels.%2520We%250Athen%2520propose%2520a%2520network%2520based%2520on%2520MAMBA%2520that%2520learns%2520the%2520ranging%2520patterns%2520of%2520UWBs%250Aover%2520a%2520complex%2520large-scale%2520environment.%2520The%2520experiment%2520demonstrates%2520that%2520our%250Asolution%2520can%2520ensure%2520high%2520localization%2520accuracy%2520on%2520a%2520large%2520scale%2520compared%2520to%2520the%250Astate-of-the-art.%2520We%2520release%2520our%2520source%2520code%2520to%2520benefit%2520the%2520community%2520at%250Ahttps%253A//github.com/brytsknguyen/uloc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ULOC%3A%20Learning%20to%20Localize%20in%20Complex%20Large-Scale%20Environments%20with%0A%20%20Ultra-Wideband%20Ranges&entry.906535625=Thien-Minh%20Nguyen%20and%20Yizhuo%20Yang%20and%20Tien-Dat%20Nguyen%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie&entry.1292438233=%20%20While%20UWB-based%20methods%20can%20achieve%20high%20localization%20accuracy%20in%20small-scale%0Aareas%2C%20their%20accuracy%20and%20reliability%20are%20significantly%20challenged%20in%0Alarge-scale%20environments.%20In%20this%20paper%2C%20we%20propose%20a%20learning-based%20framework%0Anamed%20ULOC%20for%20Ultra-Wideband%20%28UWB%29%20based%20localization%20in%20such%20complex%0Alarge-scale%20environments.%20First%2C%20anchors%20are%20deployed%20in%20the%20environment%0Awithout%20knowledge%20of%20their%20actual%20position.%20Then%2C%20UWB%20observations%20are%0Acollected%20when%20the%20vehicle%20travels%20in%20the%20environment.%20At%20the%20same%20time%2C%0Amap-consistent%20pose%20estimates%20are%20developed%20from%20registering%20%28onboard%0Aself-localization%29%20data%20with%20the%20prior%20map%20to%20provide%20the%20training%20labels.%20We%0Athen%20propose%20a%20network%20based%20on%20MAMBA%20that%20learns%20the%20ranging%20patterns%20of%20UWBs%0Aover%20a%20complex%20large-scale%20environment.%20The%20experiment%20demonstrates%20that%20our%0Asolution%20can%20ensure%20high%20localization%20accuracy%20on%20a%20large%20scale%20compared%20to%20the%0Astate-of-the-art.%20We%20release%20our%20source%20code%20to%20benefit%20the%20community%20at%0Ahttps%3A//github.com/brytsknguyen/uloc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11122v1&entry.124074799=Read"},
{"title": "Task Arithmetic for Language Expansion in Speech Translation", "author": "Yao-Fei Cheng and Hayato Futami and Yosuke Kashiwagi and Emiru Tsunoo and Wen Shen Teo and Siddhant Arora and Shinji Watanabe", "abstract": "  Recent advances in large language models (LLMs) have gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-based speech translation (ST). However, expanding language pairs\nfrom an existing instruction-tuned ST system is costly due to the necessity of\nre-training on a combination of new and previous datasets. We propose to expand\nnew language pairs by merging the model trained on new language pairs and the\nexisting model, using task arithmetic. We find that the direct application of\ntask arithmetic for ST causes the merged model to fail to follow instructions;\nthus, generating translation in incorrect languages. To eliminate language\nconfusion, we propose an augmented task arithmetic method that merges an\nadditional language control model. It is trained to generate the correct target\nlanguage token following the instructions. Our experiments demonstrate that our\nproposed language control model can achieve language expansion by eliminating\nlanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66\nand 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the\nuse of our task arithmetic framework can expand to a language pair where\nneither paired ST training data nor a pre-trained ST model is available. We\nfirst synthesize the ST system from machine translation (MT) systems via task\nanalogy, then merge the synthesized ST system to the existing ST model.\n", "link": "http://arxiv.org/abs/2409.11274v1", "date": "2024-09-17", "relevancy": 2.3466, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Arithmetic%20for%20Language%20Expansion%20in%20Speech%20Translation&body=Title%3A%20Task%20Arithmetic%20for%20Language%20Expansion%20in%20Speech%20Translation%0AAuthor%3A%20Yao-Fei%20Cheng%20and%20Hayato%20Futami%20and%20Yosuke%20Kashiwagi%20and%20Emiru%20Tsunoo%20and%20Wen%20Shen%20Teo%20and%20Siddhant%20Arora%20and%20Shinji%20Watanabe%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20gained%20interest%20in%0Aspeech-text%20multimodal%20foundation%20models%2C%20achieving%20strong%20performance%20on%0Ainstruction-based%20speech%20translation%20%28ST%29.%20However%2C%20expanding%20language%20pairs%0Afrom%20an%20existing%20instruction-tuned%20ST%20system%20is%20costly%20due%20to%20the%20necessity%20of%0Are-training%20on%20a%20combination%20of%20new%20and%20previous%20datasets.%20We%20propose%20to%20expand%0Anew%20language%20pairs%20by%20merging%20the%20model%20trained%20on%20new%20language%20pairs%20and%20the%0Aexisting%20model%2C%20using%20task%20arithmetic.%20We%20find%20that%20the%20direct%20application%20of%0Atask%20arithmetic%20for%20ST%20causes%20the%20merged%20model%20to%20fail%20to%20follow%20instructions%3B%0Athus%2C%20generating%20translation%20in%20incorrect%20languages.%20To%20eliminate%20language%0Aconfusion%2C%20we%20propose%20an%20augmented%20task%20arithmetic%20method%20that%20merges%20an%0Aadditional%20language%20control%20model.%20It%20is%20trained%20to%20generate%20the%20correct%20target%0Alanguage%20token%20following%20the%20instructions.%20Our%20experiments%20demonstrate%20that%20our%0Aproposed%20language%20control%20model%20can%20achieve%20language%20expansion%20by%20eliminating%0Alanguage%20confusion.%20In%20our%20MuST-C%20and%20CoVoST-2%20experiments%2C%20it%20shows%20up%20to%204.66%0Aand%204.92%20BLEU%20scores%20improvement%2C%20respectively.%20In%20addition%2C%20we%20demonstrate%20the%0Ause%20of%20our%20task%20arithmetic%20framework%20can%20expand%20to%20a%20language%20pair%20where%0Aneither%20paired%20ST%20training%20data%20nor%20a%20pre-trained%20ST%20model%20is%20available.%20We%0Afirst%20synthesize%20the%20ST%20system%20from%20machine%20translation%20%28MT%29%20systems%20via%20task%0Aanalogy%2C%20then%20merge%20the%20synthesized%20ST%20system%20to%20the%20existing%20ST%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Arithmetic%2520for%2520Language%2520Expansion%2520in%2520Speech%2520Translation%26entry.906535625%3DYao-Fei%2520Cheng%2520and%2520Hayato%2520Futami%2520and%2520Yosuke%2520Kashiwagi%2520and%2520Emiru%2520Tsunoo%2520and%2520Wen%2520Shen%2520Teo%2520and%2520Siddhant%2520Arora%2520and%2520Shinji%2520Watanabe%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520gained%2520interest%2520in%250Aspeech-text%2520multimodal%2520foundation%2520models%252C%2520achieving%2520strong%2520performance%2520on%250Ainstruction-based%2520speech%2520translation%2520%2528ST%2529.%2520However%252C%2520expanding%2520language%2520pairs%250Afrom%2520an%2520existing%2520instruction-tuned%2520ST%2520system%2520is%2520costly%2520due%2520to%2520the%2520necessity%2520of%250Are-training%2520on%2520a%2520combination%2520of%2520new%2520and%2520previous%2520datasets.%2520We%2520propose%2520to%2520expand%250Anew%2520language%2520pairs%2520by%2520merging%2520the%2520model%2520trained%2520on%2520new%2520language%2520pairs%2520and%2520the%250Aexisting%2520model%252C%2520using%2520task%2520arithmetic.%2520We%2520find%2520that%2520the%2520direct%2520application%2520of%250Atask%2520arithmetic%2520for%2520ST%2520causes%2520the%2520merged%2520model%2520to%2520fail%2520to%2520follow%2520instructions%253B%250Athus%252C%2520generating%2520translation%2520in%2520incorrect%2520languages.%2520To%2520eliminate%2520language%250Aconfusion%252C%2520we%2520propose%2520an%2520augmented%2520task%2520arithmetic%2520method%2520that%2520merges%2520an%250Aadditional%2520language%2520control%2520model.%2520It%2520is%2520trained%2520to%2520generate%2520the%2520correct%2520target%250Alanguage%2520token%2520following%2520the%2520instructions.%2520Our%2520experiments%2520demonstrate%2520that%2520our%250Aproposed%2520language%2520control%2520model%2520can%2520achieve%2520language%2520expansion%2520by%2520eliminating%250Alanguage%2520confusion.%2520In%2520our%2520MuST-C%2520and%2520CoVoST-2%2520experiments%252C%2520it%2520shows%2520up%2520to%25204.66%250Aand%25204.92%2520BLEU%2520scores%2520improvement%252C%2520respectively.%2520In%2520addition%252C%2520we%2520demonstrate%2520the%250Ause%2520of%2520our%2520task%2520arithmetic%2520framework%2520can%2520expand%2520to%2520a%2520language%2520pair%2520where%250Aneither%2520paired%2520ST%2520training%2520data%2520nor%2520a%2520pre-trained%2520ST%2520model%2520is%2520available.%2520We%250Afirst%2520synthesize%2520the%2520ST%2520system%2520from%2520machine%2520translation%2520%2528MT%2529%2520systems%2520via%2520task%250Aanalogy%252C%2520then%2520merge%2520the%2520synthesized%2520ST%2520system%2520to%2520the%2520existing%2520ST%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Arithmetic%20for%20Language%20Expansion%20in%20Speech%20Translation&entry.906535625=Yao-Fei%20Cheng%20and%20Hayato%20Futami%20and%20Yosuke%20Kashiwagi%20and%20Emiru%20Tsunoo%20and%20Wen%20Shen%20Teo%20and%20Siddhant%20Arora%20and%20Shinji%20Watanabe&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20gained%20interest%20in%0Aspeech-text%20multimodal%20foundation%20models%2C%20achieving%20strong%20performance%20on%0Ainstruction-based%20speech%20translation%20%28ST%29.%20However%2C%20expanding%20language%20pairs%0Afrom%20an%20existing%20instruction-tuned%20ST%20system%20is%20costly%20due%20to%20the%20necessity%20of%0Are-training%20on%20a%20combination%20of%20new%20and%20previous%20datasets.%20We%20propose%20to%20expand%0Anew%20language%20pairs%20by%20merging%20the%20model%20trained%20on%20new%20language%20pairs%20and%20the%0Aexisting%20model%2C%20using%20task%20arithmetic.%20We%20find%20that%20the%20direct%20application%20of%0Atask%20arithmetic%20for%20ST%20causes%20the%20merged%20model%20to%20fail%20to%20follow%20instructions%3B%0Athus%2C%20generating%20translation%20in%20incorrect%20languages.%20To%20eliminate%20language%0Aconfusion%2C%20we%20propose%20an%20augmented%20task%20arithmetic%20method%20that%20merges%20an%0Aadditional%20language%20control%20model.%20It%20is%20trained%20to%20generate%20the%20correct%20target%0Alanguage%20token%20following%20the%20instructions.%20Our%20experiments%20demonstrate%20that%20our%0Aproposed%20language%20control%20model%20can%20achieve%20language%20expansion%20by%20eliminating%0Alanguage%20confusion.%20In%20our%20MuST-C%20and%20CoVoST-2%20experiments%2C%20it%20shows%20up%20to%204.66%0Aand%204.92%20BLEU%20scores%20improvement%2C%20respectively.%20In%20addition%2C%20we%20demonstrate%20the%0Ause%20of%20our%20task%20arithmetic%20framework%20can%20expand%20to%20a%20language%20pair%20where%0Aneither%20paired%20ST%20training%20data%20nor%20a%20pre-trained%20ST%20model%20is%20available.%20We%0Afirst%20synthesize%20the%20ST%20system%20from%20machine%20translation%20%28MT%29%20systems%20via%20task%0Aanalogy%2C%20then%20merge%20the%20synthesized%20ST%20system%20to%20the%20existing%20ST%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11274v1&entry.124074799=Read"},
{"title": "Autoregressive Image Diffusion: Generation of Image Sequence and\n  Application in MRI", "author": "Guanxiong Luo and Shoujin Huang and Martin Uecker", "abstract": "  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging\nmodality. However, a persistent challenge lies in balancing image quality with\nimaging speed. This trade-off is primarily constrained by k-space measurements,\nwhich traverse specific trajectories in the spatial Fourier domain (k-space).\nThese measurements are often undersampled to shorten acquisition times,\nresulting in image artifacts and compromised quality. Generative models learn\nimage distributions and can be used to reconstruct high-quality images from\nundersampled k-space data. In this work, we present the autoregressive image\ndiffusion (AID) model for image sequences and use it to sample the posterior\nfor accelerated MRI reconstruction. The algorithm incorporates both\nundersampled k-space and pre-existing information. Models trained with fastMRI\ndataset are evaluated comprehensively. The results show that the AID model can\nrobustly generate sequentially coherent image sequences. In 3D and dynamic MRI,\nthe AID can outperform the standard diffusion model and reduce hallucinations,\ndue to the learned inter-image dependencies.\n", "link": "http://arxiv.org/abs/2405.14327v3", "date": "2024-09-17", "relevancy": 2.3276, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6021}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5779}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&body=Title%3A%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI%0AAuthor%3A%20Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%203D%20and%20dynamic%20MRI%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14327v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Image%2520Diffusion%253A%2520Generation%2520of%2520Image%2520Sequence%2520and%250A%2520%2520Application%2520in%2520MRI%26entry.906535625%3DGuanxiong%2520Luo%2520and%2520Shoujin%2520Huang%2520and%2520Martin%2520Uecker%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520a%2520widely%2520used%2520non-invasive%2520imaging%250Amodality.%2520However%252C%2520a%2520persistent%2520challenge%2520lies%2520in%2520balancing%2520image%2520quality%2520with%250Aimaging%2520speed.%2520This%2520trade-off%2520is%2520primarily%2520constrained%2520by%2520k-space%2520measurements%252C%250Awhich%2520traverse%2520specific%2520trajectories%2520in%2520the%2520spatial%2520Fourier%2520domain%2520%2528k-space%2529.%250AThese%2520measurements%2520are%2520often%2520undersampled%2520to%2520shorten%2520acquisition%2520times%252C%250Aresulting%2520in%2520image%2520artifacts%2520and%2520compromised%2520quality.%2520Generative%2520models%2520learn%250Aimage%2520distributions%2520and%2520can%2520be%2520used%2520to%2520reconstruct%2520high-quality%2520images%2520from%250Aundersampled%2520k-space%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520autoregressive%2520image%250Adiffusion%2520%2528AID%2529%2520model%2520for%2520image%2520sequences%2520and%2520use%2520it%2520to%2520sample%2520the%2520posterior%250Afor%2520accelerated%2520MRI%2520reconstruction.%2520The%2520algorithm%2520incorporates%2520both%250Aundersampled%2520k-space%2520and%2520pre-existing%2520information.%2520Models%2520trained%2520with%2520fastMRI%250Adataset%2520are%2520evaluated%2520comprehensively.%2520The%2520results%2520show%2520that%2520the%2520AID%2520model%2520can%250Arobustly%2520generate%2520sequentially%2520coherent%2520image%2520sequences.%2520In%25203D%2520and%2520dynamic%2520MRI%252C%250Athe%2520AID%2520can%2520outperform%2520the%2520standard%2520diffusion%2520model%2520and%2520reduce%2520hallucinations%252C%250Adue%2520to%2520the%2520learned%2520inter-image%2520dependencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14327v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&entry.906535625=Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%203D%20and%20dynamic%20MRI%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14327v3&entry.124074799=Read"},
{"title": "RING#: PR-by-PE Global Localization with Roto-translation Equivariant\n  Gram Learning", "author": "Sha Lu and Xuecheng Xu and Yuxuan Wu and Haojian Lu and Xieyuanli Chen and Rong Xiong and Yue Wang", "abstract": "  Global localization using onboard perception sensors, such as cameras and\nLiDARs, is crucial in autonomous driving and robotics applications when GPS\nsignals are unreliable. Most approaches achieve global localization by\nsequential place recognition (PR) and pose estimation (PE). Some methods train\nseparate models for each task, while others employ a single model with dual\nheads, trained jointly with separate task-specific losses. However, the\naccuracy of localization heavily depends on the success of place recognition,\nwhich often fails in scenarios with significant changes in viewpoint or\nenvironmental appearance. Consequently, this renders the final pose estimation\nof localization ineffective. To address this, we introduce a new paradigm,\nPR-by-PE localization, which bypasses the need for separate place recognition\nby directly deriving it from pose estimation. We propose RING#, an end-to-end\nPR-by-PE localization network that operates in the bird's-eye-view (BEV) space,\ncompatible with both vision and LiDAR sensors. RING# incorporates a novel\ndesign that learns two equivariant representations from BEV features, enabling\nglobally convergent and computationally efficient pose estimation.\nComprehensive experiments on the NCLT and Oxford datasets show that RING#\noutperforms state-of-the-art methods in both vision and LiDAR modalities,\nvalidating the effectiveness of the proposed approach. The code will be\npublicly released.\n", "link": "http://arxiv.org/abs/2409.00206v2", "date": "2024-09-17", "relevancy": 2.3094, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5882}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RING%23%3A%20PR-by-PE%20Global%20Localization%20with%20Roto-translation%20Equivariant%0A%20%20Gram%20Learning&body=Title%3A%20RING%23%3A%20PR-by-PE%20Global%20Localization%20with%20Roto-translation%20Equivariant%0A%20%20Gram%20Learning%0AAuthor%3A%20Sha%20Lu%20and%20Xuecheng%20Xu%20and%20Yuxuan%20Wu%20and%20Haojian%20Lu%20and%20Xieyuanli%20Chen%20and%20Rong%20Xiong%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Global%20localization%20using%20onboard%20perception%20sensors%2C%20such%20as%20cameras%20and%0ALiDARs%2C%20is%20crucial%20in%20autonomous%20driving%20and%20robotics%20applications%20when%20GPS%0Asignals%20are%20unreliable.%20Most%20approaches%20achieve%20global%20localization%20by%0Asequential%20place%20recognition%20%28PR%29%20and%20pose%20estimation%20%28PE%29.%20Some%20methods%20train%0Aseparate%20models%20for%20each%20task%2C%20while%20others%20employ%20a%20single%20model%20with%20dual%0Aheads%2C%20trained%20jointly%20with%20separate%20task-specific%20losses.%20However%2C%20the%0Aaccuracy%20of%20localization%20heavily%20depends%20on%20the%20success%20of%20place%20recognition%2C%0Awhich%20often%20fails%20in%20scenarios%20with%20significant%20changes%20in%20viewpoint%20or%0Aenvironmental%20appearance.%20Consequently%2C%20this%20renders%20the%20final%20pose%20estimation%0Aof%20localization%20ineffective.%20To%20address%20this%2C%20we%20introduce%20a%20new%20paradigm%2C%0APR-by-PE%20localization%2C%20which%20bypasses%20the%20need%20for%20separate%20place%20recognition%0Aby%20directly%20deriving%20it%20from%20pose%20estimation.%20We%20propose%20RING%23%2C%20an%20end-to-end%0APR-by-PE%20localization%20network%20that%20operates%20in%20the%20bird%27s-eye-view%20%28BEV%29%20space%2C%0Acompatible%20with%20both%20vision%20and%20LiDAR%20sensors.%20RING%23%20incorporates%20a%20novel%0Adesign%20that%20learns%20two%20equivariant%20representations%20from%20BEV%20features%2C%20enabling%0Aglobally%20convergent%20and%20computationally%20efficient%20pose%20estimation.%0AComprehensive%20experiments%20on%20the%20NCLT%20and%20Oxford%20datasets%20show%20that%20RING%23%0Aoutperforms%20state-of-the-art%20methods%20in%20both%20vision%20and%20LiDAR%20modalities%2C%0Avalidating%20the%20effectiveness%20of%20the%20proposed%20approach.%20The%20code%20will%20be%0Apublicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRING%2523%253A%2520PR-by-PE%2520Global%2520Localization%2520with%2520Roto-translation%2520Equivariant%250A%2520%2520Gram%2520Learning%26entry.906535625%3DSha%2520Lu%2520and%2520Xuecheng%2520Xu%2520and%2520Yuxuan%2520Wu%2520and%2520Haojian%2520Lu%2520and%2520Xieyuanli%2520Chen%2520and%2520Rong%2520Xiong%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Global%2520localization%2520using%2520onboard%2520perception%2520sensors%252C%2520such%2520as%2520cameras%2520and%250ALiDARs%252C%2520is%2520crucial%2520in%2520autonomous%2520driving%2520and%2520robotics%2520applications%2520when%2520GPS%250Asignals%2520are%2520unreliable.%2520Most%2520approaches%2520achieve%2520global%2520localization%2520by%250Asequential%2520place%2520recognition%2520%2528PR%2529%2520and%2520pose%2520estimation%2520%2528PE%2529.%2520Some%2520methods%2520train%250Aseparate%2520models%2520for%2520each%2520task%252C%2520while%2520others%2520employ%2520a%2520single%2520model%2520with%2520dual%250Aheads%252C%2520trained%2520jointly%2520with%2520separate%2520task-specific%2520losses.%2520However%252C%2520the%250Aaccuracy%2520of%2520localization%2520heavily%2520depends%2520on%2520the%2520success%2520of%2520place%2520recognition%252C%250Awhich%2520often%2520fails%2520in%2520scenarios%2520with%2520significant%2520changes%2520in%2520viewpoint%2520or%250Aenvironmental%2520appearance.%2520Consequently%252C%2520this%2520renders%2520the%2520final%2520pose%2520estimation%250Aof%2520localization%2520ineffective.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520new%2520paradigm%252C%250APR-by-PE%2520localization%252C%2520which%2520bypasses%2520the%2520need%2520for%2520separate%2520place%2520recognition%250Aby%2520directly%2520deriving%2520it%2520from%2520pose%2520estimation.%2520We%2520propose%2520RING%2523%252C%2520an%2520end-to-end%250APR-by-PE%2520localization%2520network%2520that%2520operates%2520in%2520the%2520bird%2527s-eye-view%2520%2528BEV%2529%2520space%252C%250Acompatible%2520with%2520both%2520vision%2520and%2520LiDAR%2520sensors.%2520RING%2523%2520incorporates%2520a%2520novel%250Adesign%2520that%2520learns%2520two%2520equivariant%2520representations%2520from%2520BEV%2520features%252C%2520enabling%250Aglobally%2520convergent%2520and%2520computationally%2520efficient%2520pose%2520estimation.%250AComprehensive%2520experiments%2520on%2520the%2520NCLT%2520and%2520Oxford%2520datasets%2520show%2520that%2520RING%2523%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520both%2520vision%2520and%2520LiDAR%2520modalities%252C%250Avalidating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%2520The%2520code%2520will%2520be%250Apublicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RING%23%3A%20PR-by-PE%20Global%20Localization%20with%20Roto-translation%20Equivariant%0A%20%20Gram%20Learning&entry.906535625=Sha%20Lu%20and%20Xuecheng%20Xu%20and%20Yuxuan%20Wu%20and%20Haojian%20Lu%20and%20Xieyuanli%20Chen%20and%20Rong%20Xiong%20and%20Yue%20Wang&entry.1292438233=%20%20Global%20localization%20using%20onboard%20perception%20sensors%2C%20such%20as%20cameras%20and%0ALiDARs%2C%20is%20crucial%20in%20autonomous%20driving%20and%20robotics%20applications%20when%20GPS%0Asignals%20are%20unreliable.%20Most%20approaches%20achieve%20global%20localization%20by%0Asequential%20place%20recognition%20%28PR%29%20and%20pose%20estimation%20%28PE%29.%20Some%20methods%20train%0Aseparate%20models%20for%20each%20task%2C%20while%20others%20employ%20a%20single%20model%20with%20dual%0Aheads%2C%20trained%20jointly%20with%20separate%20task-specific%20losses.%20However%2C%20the%0Aaccuracy%20of%20localization%20heavily%20depends%20on%20the%20success%20of%20place%20recognition%2C%0Awhich%20often%20fails%20in%20scenarios%20with%20significant%20changes%20in%20viewpoint%20or%0Aenvironmental%20appearance.%20Consequently%2C%20this%20renders%20the%20final%20pose%20estimation%0Aof%20localization%20ineffective.%20To%20address%20this%2C%20we%20introduce%20a%20new%20paradigm%2C%0APR-by-PE%20localization%2C%20which%20bypasses%20the%20need%20for%20separate%20place%20recognition%0Aby%20directly%20deriving%20it%20from%20pose%20estimation.%20We%20propose%20RING%23%2C%20an%20end-to-end%0APR-by-PE%20localization%20network%20that%20operates%20in%20the%20bird%27s-eye-view%20%28BEV%29%20space%2C%0Acompatible%20with%20both%20vision%20and%20LiDAR%20sensors.%20RING%23%20incorporates%20a%20novel%0Adesign%20that%20learns%20two%20equivariant%20representations%20from%20BEV%20features%2C%20enabling%0Aglobally%20convergent%20and%20computationally%20efficient%20pose%20estimation.%0AComprehensive%20experiments%20on%20the%20NCLT%20and%20Oxford%20datasets%20show%20that%20RING%23%0Aoutperforms%20state-of-the-art%20methods%20in%20both%20vision%20and%20LiDAR%20modalities%2C%0Avalidating%20the%20effectiveness%20of%20the%20proposed%20approach.%20The%20code%20will%20be%0Apublicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00206v2&entry.124074799=Read"},
{"title": "MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via\n  Transformer-Guided Prototyping", "author": "Amirreza Fateh and Mohammad Reza Mohammadi and Mohammad Reza Jahed Motlagh", "abstract": "  Few-shot Semantic Segmentation addresses the challenge of segmenting objects\nin query images with only a handful of annotated examples. However, many\nprevious state-of-the-art methods either have to discard intricate local\nsemantic features or suffer from high computational complexity. To address\nthese challenges, we propose a new Few-shot Semantic Segmentation framework\nbased on the transformer architecture. Our approach introduces the spatial\ntransformer decoder and the contextual mask generation module to improve the\nrelational understanding between support and query images. Moreover, we\nintroduce a multi-scale decoder to refine the segmentation mask by\nincorporating features from different resolutions in a hierarchical manner.\nAdditionally, our approach integrates global features from intermediate encoder\nstages to improve contextual understanding, while maintaining a lightweight\nstructure to reduce complexity. This balance between performance and efficiency\nenables our method to achieve state-of-the-art results on benchmark datasets\nsuch as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings.\nNotably, our model with only 1.5 million parameters demonstrates competitive\nperformance while overcoming limitations of existing methodologies.\nhttps://github.com/amirrezafateh/MSDNet\n", "link": "http://arxiv.org/abs/2409.11316v1", "date": "2024-09-17", "relevancy": 2.3037, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.599}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.572}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSDNet%3A%20Multi-Scale%20Decoder%20for%20Few-Shot%20Semantic%20Segmentation%20via%0A%20%20Transformer-Guided%20Prototyping&body=Title%3A%20MSDNet%3A%20Multi-Scale%20Decoder%20for%20Few-Shot%20Semantic%20Segmentation%20via%0A%20%20Transformer-Guided%20Prototyping%0AAuthor%3A%20Amirreza%20Fateh%20and%20Mohammad%20Reza%20Mohammadi%20and%20Mohammad%20Reza%20Jahed%20Motlagh%0AAbstract%3A%20%20%20Few-shot%20Semantic%20Segmentation%20addresses%20the%20challenge%20of%20segmenting%20objects%0Ain%20query%20images%20with%20only%20a%20handful%20of%20annotated%20examples.%20However%2C%20many%0Aprevious%20state-of-the-art%20methods%20either%20have%20to%20discard%20intricate%20local%0Asemantic%20features%20or%20suffer%20from%20high%20computational%20complexity.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20new%20Few-shot%20Semantic%20Segmentation%20framework%0Abased%20on%20the%20transformer%20architecture.%20Our%20approach%20introduces%20the%20spatial%0Atransformer%20decoder%20and%20the%20contextual%20mask%20generation%20module%20to%20improve%20the%0Arelational%20understanding%20between%20support%20and%20query%20images.%20Moreover%2C%20we%0Aintroduce%20a%20multi-scale%20decoder%20to%20refine%20the%20segmentation%20mask%20by%0Aincorporating%20features%20from%20different%20resolutions%20in%20a%20hierarchical%20manner.%0AAdditionally%2C%20our%20approach%20integrates%20global%20features%20from%20intermediate%20encoder%0Astages%20to%20improve%20contextual%20understanding%2C%20while%20maintaining%20a%20lightweight%0Astructure%20to%20reduce%20complexity.%20This%20balance%20between%20performance%20and%20efficiency%0Aenables%20our%20method%20to%20achieve%20state-of-the-art%20results%20on%20benchmark%20datasets%0Asuch%20as%20%24PASCAL-5%5Ei%24%20and%20%24COCO-20%5Ei%24%20in%20both%201-shot%20and%205-shot%20settings.%0ANotably%2C%20our%20model%20with%20only%201.5%20million%20parameters%20demonstrates%20competitive%0Aperformance%20while%20overcoming%20limitations%20of%20existing%20methodologies.%0Ahttps%3A//github.com/amirrezafateh/MSDNet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSDNet%253A%2520Multi-Scale%2520Decoder%2520for%2520Few-Shot%2520Semantic%2520Segmentation%2520via%250A%2520%2520Transformer-Guided%2520Prototyping%26entry.906535625%3DAmirreza%2520Fateh%2520and%2520Mohammad%2520Reza%2520Mohammadi%2520and%2520Mohammad%2520Reza%2520Jahed%2520Motlagh%26entry.1292438233%3D%2520%2520Few-shot%2520Semantic%2520Segmentation%2520addresses%2520the%2520challenge%2520of%2520segmenting%2520objects%250Ain%2520query%2520images%2520with%2520only%2520a%2520handful%2520of%2520annotated%2520examples.%2520However%252C%2520many%250Aprevious%2520state-of-the-art%2520methods%2520either%2520have%2520to%2520discard%2520intricate%2520local%250Asemantic%2520features%2520or%2520suffer%2520from%2520high%2520computational%2520complexity.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520new%2520Few-shot%2520Semantic%2520Segmentation%2520framework%250Abased%2520on%2520the%2520transformer%2520architecture.%2520Our%2520approach%2520introduces%2520the%2520spatial%250Atransformer%2520decoder%2520and%2520the%2520contextual%2520mask%2520generation%2520module%2520to%2520improve%2520the%250Arelational%2520understanding%2520between%2520support%2520and%2520query%2520images.%2520Moreover%252C%2520we%250Aintroduce%2520a%2520multi-scale%2520decoder%2520to%2520refine%2520the%2520segmentation%2520mask%2520by%250Aincorporating%2520features%2520from%2520different%2520resolutions%2520in%2520a%2520hierarchical%2520manner.%250AAdditionally%252C%2520our%2520approach%2520integrates%2520global%2520features%2520from%2520intermediate%2520encoder%250Astages%2520to%2520improve%2520contextual%2520understanding%252C%2520while%2520maintaining%2520a%2520lightweight%250Astructure%2520to%2520reduce%2520complexity.%2520This%2520balance%2520between%2520performance%2520and%2520efficiency%250Aenables%2520our%2520method%2520to%2520achieve%2520state-of-the-art%2520results%2520on%2520benchmark%2520datasets%250Asuch%2520as%2520%2524PASCAL-5%255Ei%2524%2520and%2520%2524COCO-20%255Ei%2524%2520in%2520both%25201-shot%2520and%25205-shot%2520settings.%250ANotably%252C%2520our%2520model%2520with%2520only%25201.5%2520million%2520parameters%2520demonstrates%2520competitive%250Aperformance%2520while%2520overcoming%2520limitations%2520of%2520existing%2520methodologies.%250Ahttps%253A//github.com/amirrezafateh/MSDNet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSDNet%3A%20Multi-Scale%20Decoder%20for%20Few-Shot%20Semantic%20Segmentation%20via%0A%20%20Transformer-Guided%20Prototyping&entry.906535625=Amirreza%20Fateh%20and%20Mohammad%20Reza%20Mohammadi%20and%20Mohammad%20Reza%20Jahed%20Motlagh&entry.1292438233=%20%20Few-shot%20Semantic%20Segmentation%20addresses%20the%20challenge%20of%20segmenting%20objects%0Ain%20query%20images%20with%20only%20a%20handful%20of%20annotated%20examples.%20However%2C%20many%0Aprevious%20state-of-the-art%20methods%20either%20have%20to%20discard%20intricate%20local%0Asemantic%20features%20or%20suffer%20from%20high%20computational%20complexity.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20new%20Few-shot%20Semantic%20Segmentation%20framework%0Abased%20on%20the%20transformer%20architecture.%20Our%20approach%20introduces%20the%20spatial%0Atransformer%20decoder%20and%20the%20contextual%20mask%20generation%20module%20to%20improve%20the%0Arelational%20understanding%20between%20support%20and%20query%20images.%20Moreover%2C%20we%0Aintroduce%20a%20multi-scale%20decoder%20to%20refine%20the%20segmentation%20mask%20by%0Aincorporating%20features%20from%20different%20resolutions%20in%20a%20hierarchical%20manner.%0AAdditionally%2C%20our%20approach%20integrates%20global%20features%20from%20intermediate%20encoder%0Astages%20to%20improve%20contextual%20understanding%2C%20while%20maintaining%20a%20lightweight%0Astructure%20to%20reduce%20complexity.%20This%20balance%20between%20performance%20and%20efficiency%0Aenables%20our%20method%20to%20achieve%20state-of-the-art%20results%20on%20benchmark%20datasets%0Asuch%20as%20%24PASCAL-5%5Ei%24%20and%20%24COCO-20%5Ei%24%20in%20both%201-shot%20and%205-shot%20settings.%0ANotably%2C%20our%20model%20with%20only%201.5%20million%20parameters%20demonstrates%20competitive%0Aperformance%20while%20overcoming%20limitations%20of%20existing%20methodologies.%0Ahttps%3A//github.com/amirrezafateh/MSDNet%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11316v1&entry.124074799=Read"},
{"title": "Can Graph Reordering Speed Up Graph Neural Network Training? An\n  Experimental Study", "author": "Nikolai Merkel and Pierre Toussing and Ruben Mayer and Hans-Arno Jacobsen", "abstract": "  Graph neural networks (GNNs) are a type of neural network capable of learning\non graph-structured data. However, training GNNs on large-scale graphs is\nchallenging due to iterative aggregations of high-dimensional features from\nneighboring vertices within sparse graph structures combined with neural\nnetwork operations. The sparsity of graphs frequently results in suboptimal\nmemory access patterns and longer training time. Graph reordering is an\noptimization strategy aiming to improve the graph data layout. It has shown to\nbe effective to speed up graph analytics workloads, but its effect on the\nperformance of GNN training has not been investigated yet. The generalization\nof reordering to GNN performance is nontrivial, as multiple aspects must be\nconsidered: GNN hyper-parameters such as the number of layers, the number of\nhidden dimensions, and the feature size used in the GNN model, neural network\noperations, large intermediate vertex states, and GPU acceleration.\n  In our work, we close this gap by performing an empirical evaluation of 12\nreordering strategies in two state-of-the-art GNN systems, PyTorch Geometric\nand Deep Graph Library. Our results show that graph reordering is effective in\nreducing training time for CPU- and GPU-based training, respectively. Further,\nwe find that GNN hyper-parameters influence the effectiveness of reordering,\nthat reordering metrics play an important role in selecting a reordering\nstrategy, that lightweight reordering performs better for GPU-based than for\nCPU-based training, and that invested reordering time can in many cases be\namortized.\n", "link": "http://arxiv.org/abs/2409.11129v1", "date": "2024-09-17", "relevancy": 2.2979, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4691}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4566}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Graph%20Reordering%20Speed%20Up%20Graph%20Neural%20Network%20Training%3F%20An%0A%20%20Experimental%20Study&body=Title%3A%20Can%20Graph%20Reordering%20Speed%20Up%20Graph%20Neural%20Network%20Training%3F%20An%0A%20%20Experimental%20Study%0AAuthor%3A%20Nikolai%20Merkel%20and%20Pierre%20Toussing%20and%20Ruben%20Mayer%20and%20Hans-Arno%20Jacobsen%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20a%20type%20of%20neural%20network%20capable%20of%20learning%0Aon%20graph-structured%20data.%20However%2C%20training%20GNNs%20on%20large-scale%20graphs%20is%0Achallenging%20due%20to%20iterative%20aggregations%20of%20high-dimensional%20features%20from%0Aneighboring%20vertices%20within%20sparse%20graph%20structures%20combined%20with%20neural%0Anetwork%20operations.%20The%20sparsity%20of%20graphs%20frequently%20results%20in%20suboptimal%0Amemory%20access%20patterns%20and%20longer%20training%20time.%20Graph%20reordering%20is%20an%0Aoptimization%20strategy%20aiming%20to%20improve%20the%20graph%20data%20layout.%20It%20has%20shown%20to%0Abe%20effective%20to%20speed%20up%20graph%20analytics%20workloads%2C%20but%20its%20effect%20on%20the%0Aperformance%20of%20GNN%20training%20has%20not%20been%20investigated%20yet.%20The%20generalization%0Aof%20reordering%20to%20GNN%20performance%20is%20nontrivial%2C%20as%20multiple%20aspects%20must%20be%0Aconsidered%3A%20GNN%20hyper-parameters%20such%20as%20the%20number%20of%20layers%2C%20the%20number%20of%0Ahidden%20dimensions%2C%20and%20the%20feature%20size%20used%20in%20the%20GNN%20model%2C%20neural%20network%0Aoperations%2C%20large%20intermediate%20vertex%20states%2C%20and%20GPU%20acceleration.%0A%20%20In%20our%20work%2C%20we%20close%20this%20gap%20by%20performing%20an%20empirical%20evaluation%20of%2012%0Areordering%20strategies%20in%20two%20state-of-the-art%20GNN%20systems%2C%20PyTorch%20Geometric%0Aand%20Deep%20Graph%20Library.%20Our%20results%20show%20that%20graph%20reordering%20is%20effective%20in%0Areducing%20training%20time%20for%20CPU-%20and%20GPU-based%20training%2C%20respectively.%20Further%2C%0Awe%20find%20that%20GNN%20hyper-parameters%20influence%20the%20effectiveness%20of%20reordering%2C%0Athat%20reordering%20metrics%20play%20an%20important%20role%20in%20selecting%20a%20reordering%0Astrategy%2C%20that%20lightweight%20reordering%20performs%20better%20for%20GPU-based%20than%20for%0ACPU-based%20training%2C%20and%20that%20invested%20reordering%20time%20can%20in%20many%20cases%20be%0Aamortized.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Graph%2520Reordering%2520Speed%2520Up%2520Graph%2520Neural%2520Network%2520Training%253F%2520An%250A%2520%2520Experimental%2520Study%26entry.906535625%3DNikolai%2520Merkel%2520and%2520Pierre%2520Toussing%2520and%2520Ruben%2520Mayer%2520and%2520Hans-Arno%2520Jacobsen%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520a%2520type%2520of%2520neural%2520network%2520capable%2520of%2520learning%250Aon%2520graph-structured%2520data.%2520However%252C%2520training%2520GNNs%2520on%2520large-scale%2520graphs%2520is%250Achallenging%2520due%2520to%2520iterative%2520aggregations%2520of%2520high-dimensional%2520features%2520from%250Aneighboring%2520vertices%2520within%2520sparse%2520graph%2520structures%2520combined%2520with%2520neural%250Anetwork%2520operations.%2520The%2520sparsity%2520of%2520graphs%2520frequently%2520results%2520in%2520suboptimal%250Amemory%2520access%2520patterns%2520and%2520longer%2520training%2520time.%2520Graph%2520reordering%2520is%2520an%250Aoptimization%2520strategy%2520aiming%2520to%2520improve%2520the%2520graph%2520data%2520layout.%2520It%2520has%2520shown%2520to%250Abe%2520effective%2520to%2520speed%2520up%2520graph%2520analytics%2520workloads%252C%2520but%2520its%2520effect%2520on%2520the%250Aperformance%2520of%2520GNN%2520training%2520has%2520not%2520been%2520investigated%2520yet.%2520The%2520generalization%250Aof%2520reordering%2520to%2520GNN%2520performance%2520is%2520nontrivial%252C%2520as%2520multiple%2520aspects%2520must%2520be%250Aconsidered%253A%2520GNN%2520hyper-parameters%2520such%2520as%2520the%2520number%2520of%2520layers%252C%2520the%2520number%2520of%250Ahidden%2520dimensions%252C%2520and%2520the%2520feature%2520size%2520used%2520in%2520the%2520GNN%2520model%252C%2520neural%2520network%250Aoperations%252C%2520large%2520intermediate%2520vertex%2520states%252C%2520and%2520GPU%2520acceleration.%250A%2520%2520In%2520our%2520work%252C%2520we%2520close%2520this%2520gap%2520by%2520performing%2520an%2520empirical%2520evaluation%2520of%252012%250Areordering%2520strategies%2520in%2520two%2520state-of-the-art%2520GNN%2520systems%252C%2520PyTorch%2520Geometric%250Aand%2520Deep%2520Graph%2520Library.%2520Our%2520results%2520show%2520that%2520graph%2520reordering%2520is%2520effective%2520in%250Areducing%2520training%2520time%2520for%2520CPU-%2520and%2520GPU-based%2520training%252C%2520respectively.%2520Further%252C%250Awe%2520find%2520that%2520GNN%2520hyper-parameters%2520influence%2520the%2520effectiveness%2520of%2520reordering%252C%250Athat%2520reordering%2520metrics%2520play%2520an%2520important%2520role%2520in%2520selecting%2520a%2520reordering%250Astrategy%252C%2520that%2520lightweight%2520reordering%2520performs%2520better%2520for%2520GPU-based%2520than%2520for%250ACPU-based%2520training%252C%2520and%2520that%2520invested%2520reordering%2520time%2520can%2520in%2520many%2520cases%2520be%250Aamortized.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Graph%20Reordering%20Speed%20Up%20Graph%20Neural%20Network%20Training%3F%20An%0A%20%20Experimental%20Study&entry.906535625=Nikolai%20Merkel%20and%20Pierre%20Toussing%20and%20Ruben%20Mayer%20and%20Hans-Arno%20Jacobsen&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20a%20type%20of%20neural%20network%20capable%20of%20learning%0Aon%20graph-structured%20data.%20However%2C%20training%20GNNs%20on%20large-scale%20graphs%20is%0Achallenging%20due%20to%20iterative%20aggregations%20of%20high-dimensional%20features%20from%0Aneighboring%20vertices%20within%20sparse%20graph%20structures%20combined%20with%20neural%0Anetwork%20operations.%20The%20sparsity%20of%20graphs%20frequently%20results%20in%20suboptimal%0Amemory%20access%20patterns%20and%20longer%20training%20time.%20Graph%20reordering%20is%20an%0Aoptimization%20strategy%20aiming%20to%20improve%20the%20graph%20data%20layout.%20It%20has%20shown%20to%0Abe%20effective%20to%20speed%20up%20graph%20analytics%20workloads%2C%20but%20its%20effect%20on%20the%0Aperformance%20of%20GNN%20training%20has%20not%20been%20investigated%20yet.%20The%20generalization%0Aof%20reordering%20to%20GNN%20performance%20is%20nontrivial%2C%20as%20multiple%20aspects%20must%20be%0Aconsidered%3A%20GNN%20hyper-parameters%20such%20as%20the%20number%20of%20layers%2C%20the%20number%20of%0Ahidden%20dimensions%2C%20and%20the%20feature%20size%20used%20in%20the%20GNN%20model%2C%20neural%20network%0Aoperations%2C%20large%20intermediate%20vertex%20states%2C%20and%20GPU%20acceleration.%0A%20%20In%20our%20work%2C%20we%20close%20this%20gap%20by%20performing%20an%20empirical%20evaluation%20of%2012%0Areordering%20strategies%20in%20two%20state-of-the-art%20GNN%20systems%2C%20PyTorch%20Geometric%0Aand%20Deep%20Graph%20Library.%20Our%20results%20show%20that%20graph%20reordering%20is%20effective%20in%0Areducing%20training%20time%20for%20CPU-%20and%20GPU-based%20training%2C%20respectively.%20Further%2C%0Awe%20find%20that%20GNN%20hyper-parameters%20influence%20the%20effectiveness%20of%20reordering%2C%0Athat%20reordering%20metrics%20play%20an%20important%20role%20in%20selecting%20a%20reordering%0Astrategy%2C%20that%20lightweight%20reordering%20performs%20better%20for%20GPU-based%20than%20for%0ACPU-based%20training%2C%20and%20that%20invested%20reordering%20time%20can%20in%20many%20cases%20be%0Aamortized.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11129v1&entry.124074799=Read"},
{"title": "Depth-based Privileged Information for Boosting 3D Human Pose Estimation\n  on RGB", "author": "Alessandro Simoni and Francesco Marchetti and Guido Borghi and Federico Becattini and Davide Davoli and Lorenzo Garattoni and Gianpiero Francesca and Lorenzo Seidenari and Roberto Vezzani", "abstract": "  Despite the recent advances in computer vision research, estimating the 3D\nhuman pose from single RGB images remains a challenging task, as multiple 3D\nposes can correspond to the same 2D projection on the image. In this context,\ndepth data could help to disambiguate the 2D information by providing\nadditional constraints about the distance between objects in the scene and the\ncamera. Unfortunately, the acquisition of accurate depth data is limited to\nindoor spaces and usually is tied to specific depth technologies and devices,\nthus limiting generalization capabilities. In this paper, we propose a method\nable to leverage the benefits of depth information without compromising its\nbroader applicability and adaptability in a predominantly RGB-camera-centric\nlandscape. Our approach consists of a heatmap-based 3D pose estimator that,\nleveraging the paradigm of Privileged Information, is able to hallucinate depth\ninformation from the RGB frames given at inference time. More precisely, depth\ninformation is used exclusively during training by enforcing our RGB-based\nhallucination network to learn similar features to a backbone pre-trained only\non depth data. This approach proves to be effective even when dealing with\nlimited and small datasets. Experimental results reveal that the paradigm of\nPrivileged Information significantly enhances the model's performance, enabling\nefficient extraction of depth information by using only RGB images.\n", "link": "http://arxiv.org/abs/2409.11104v1", "date": "2024-09-17", "relevancy": 2.2973, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.594}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5631}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-based%20Privileged%20Information%20for%20Boosting%203D%20Human%20Pose%20Estimation%0A%20%20on%20RGB&body=Title%3A%20Depth-based%20Privileged%20Information%20for%20Boosting%203D%20Human%20Pose%20Estimation%0A%20%20on%20RGB%0AAuthor%3A%20Alessandro%20Simoni%20and%20Francesco%20Marchetti%20and%20Guido%20Borghi%20and%20Federico%20Becattini%20and%20Davide%20Davoli%20and%20Lorenzo%20Garattoni%20and%20Gianpiero%20Francesca%20and%20Lorenzo%20Seidenari%20and%20Roberto%20Vezzani%0AAbstract%3A%20%20%20Despite%20the%20recent%20advances%20in%20computer%20vision%20research%2C%20estimating%20the%203D%0Ahuman%20pose%20from%20single%20RGB%20images%20remains%20a%20challenging%20task%2C%20as%20multiple%203D%0Aposes%20can%20correspond%20to%20the%20same%202D%20projection%20on%20the%20image.%20In%20this%20context%2C%0Adepth%20data%20could%20help%20to%20disambiguate%20the%202D%20information%20by%20providing%0Aadditional%20constraints%20about%20the%20distance%20between%20objects%20in%20the%20scene%20and%20the%0Acamera.%20Unfortunately%2C%20the%20acquisition%20of%20accurate%20depth%20data%20is%20limited%20to%0Aindoor%20spaces%20and%20usually%20is%20tied%20to%20specific%20depth%20technologies%20and%20devices%2C%0Athus%20limiting%20generalization%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20method%0Aable%20to%20leverage%20the%20benefits%20of%20depth%20information%20without%20compromising%20its%0Abroader%20applicability%20and%20adaptability%20in%20a%20predominantly%20RGB-camera-centric%0Alandscape.%20Our%20approach%20consists%20of%20a%20heatmap-based%203D%20pose%20estimator%20that%2C%0Aleveraging%20the%20paradigm%20of%20Privileged%20Information%2C%20is%20able%20to%20hallucinate%20depth%0Ainformation%20from%20the%20RGB%20frames%20given%20at%20inference%20time.%20More%20precisely%2C%20depth%0Ainformation%20is%20used%20exclusively%20during%20training%20by%20enforcing%20our%20RGB-based%0Ahallucination%20network%20to%20learn%20similar%20features%20to%20a%20backbone%20pre-trained%20only%0Aon%20depth%20data.%20This%20approach%20proves%20to%20be%20effective%20even%20when%20dealing%20with%0Alimited%20and%20small%20datasets.%20Experimental%20results%20reveal%20that%20the%20paradigm%20of%0APrivileged%20Information%20significantly%20enhances%20the%20model%27s%20performance%2C%20enabling%0Aefficient%20extraction%20of%20depth%20information%20by%20using%20only%20RGB%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-based%2520Privileged%2520Information%2520for%2520Boosting%25203D%2520Human%2520Pose%2520Estimation%250A%2520%2520on%2520RGB%26entry.906535625%3DAlessandro%2520Simoni%2520and%2520Francesco%2520Marchetti%2520and%2520Guido%2520Borghi%2520and%2520Federico%2520Becattini%2520and%2520Davide%2520Davoli%2520and%2520Lorenzo%2520Garattoni%2520and%2520Gianpiero%2520Francesca%2520and%2520Lorenzo%2520Seidenari%2520and%2520Roberto%2520Vezzani%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520advances%2520in%2520computer%2520vision%2520research%252C%2520estimating%2520the%25203D%250Ahuman%2520pose%2520from%2520single%2520RGB%2520images%2520remains%2520a%2520challenging%2520task%252C%2520as%2520multiple%25203D%250Aposes%2520can%2520correspond%2520to%2520the%2520same%25202D%2520projection%2520on%2520the%2520image.%2520In%2520this%2520context%252C%250Adepth%2520data%2520could%2520help%2520to%2520disambiguate%2520the%25202D%2520information%2520by%2520providing%250Aadditional%2520constraints%2520about%2520the%2520distance%2520between%2520objects%2520in%2520the%2520scene%2520and%2520the%250Acamera.%2520Unfortunately%252C%2520the%2520acquisition%2520of%2520accurate%2520depth%2520data%2520is%2520limited%2520to%250Aindoor%2520spaces%2520and%2520usually%2520is%2520tied%2520to%2520specific%2520depth%2520technologies%2520and%2520devices%252C%250Athus%2520limiting%2520generalization%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%250Aable%2520to%2520leverage%2520the%2520benefits%2520of%2520depth%2520information%2520without%2520compromising%2520its%250Abroader%2520applicability%2520and%2520adaptability%2520in%2520a%2520predominantly%2520RGB-camera-centric%250Alandscape.%2520Our%2520approach%2520consists%2520of%2520a%2520heatmap-based%25203D%2520pose%2520estimator%2520that%252C%250Aleveraging%2520the%2520paradigm%2520of%2520Privileged%2520Information%252C%2520is%2520able%2520to%2520hallucinate%2520depth%250Ainformation%2520from%2520the%2520RGB%2520frames%2520given%2520at%2520inference%2520time.%2520More%2520precisely%252C%2520depth%250Ainformation%2520is%2520used%2520exclusively%2520during%2520training%2520by%2520enforcing%2520our%2520RGB-based%250Ahallucination%2520network%2520to%2520learn%2520similar%2520features%2520to%2520a%2520backbone%2520pre-trained%2520only%250Aon%2520depth%2520data.%2520This%2520approach%2520proves%2520to%2520be%2520effective%2520even%2520when%2520dealing%2520with%250Alimited%2520and%2520small%2520datasets.%2520Experimental%2520results%2520reveal%2520that%2520the%2520paradigm%2520of%250APrivileged%2520Information%2520significantly%2520enhances%2520the%2520model%2527s%2520performance%252C%2520enabling%250Aefficient%2520extraction%2520of%2520depth%2520information%2520by%2520using%2520only%2520RGB%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-based%20Privileged%20Information%20for%20Boosting%203D%20Human%20Pose%20Estimation%0A%20%20on%20RGB&entry.906535625=Alessandro%20Simoni%20and%20Francesco%20Marchetti%20and%20Guido%20Borghi%20and%20Federico%20Becattini%20and%20Davide%20Davoli%20and%20Lorenzo%20Garattoni%20and%20Gianpiero%20Francesca%20and%20Lorenzo%20Seidenari%20and%20Roberto%20Vezzani&entry.1292438233=%20%20Despite%20the%20recent%20advances%20in%20computer%20vision%20research%2C%20estimating%20the%203D%0Ahuman%20pose%20from%20single%20RGB%20images%20remains%20a%20challenging%20task%2C%20as%20multiple%203D%0Aposes%20can%20correspond%20to%20the%20same%202D%20projection%20on%20the%20image.%20In%20this%20context%2C%0Adepth%20data%20could%20help%20to%20disambiguate%20the%202D%20information%20by%20providing%0Aadditional%20constraints%20about%20the%20distance%20between%20objects%20in%20the%20scene%20and%20the%0Acamera.%20Unfortunately%2C%20the%20acquisition%20of%20accurate%20depth%20data%20is%20limited%20to%0Aindoor%20spaces%20and%20usually%20is%20tied%20to%20specific%20depth%20technologies%20and%20devices%2C%0Athus%20limiting%20generalization%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20method%0Aable%20to%20leverage%20the%20benefits%20of%20depth%20information%20without%20compromising%20its%0Abroader%20applicability%20and%20adaptability%20in%20a%20predominantly%20RGB-camera-centric%0Alandscape.%20Our%20approach%20consists%20of%20a%20heatmap-based%203D%20pose%20estimator%20that%2C%0Aleveraging%20the%20paradigm%20of%20Privileged%20Information%2C%20is%20able%20to%20hallucinate%20depth%0Ainformation%20from%20the%20RGB%20frames%20given%20at%20inference%20time.%20More%20precisely%2C%20depth%0Ainformation%20is%20used%20exclusively%20during%20training%20by%20enforcing%20our%20RGB-based%0Ahallucination%20network%20to%20learn%20similar%20features%20to%20a%20backbone%20pre-trained%20only%0Aon%20depth%20data.%20This%20approach%20proves%20to%20be%20effective%20even%20when%20dealing%20with%0Alimited%20and%20small%20datasets.%20Experimental%20results%20reveal%20that%20the%20paradigm%20of%0APrivileged%20Information%20significantly%20enhances%20the%20model%27s%20performance%2C%20enabling%0Aefficient%20extraction%20of%20depth%20information%20by%20using%20only%20RGB%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11104v1&entry.124074799=Read"},
{"title": "Promptriever: Instruction-Trained Retrievers Can Be Prompted Like\n  Language Models", "author": "Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel", "abstract": "  Instruction-tuned language models (LM) are able to respond to imperative\ncommands, providing a more natural user interface compared to their base\ncounterparts. In this work, we present Promptriever, the first retrieval model\nable to be prompted like an LM. To train Promptriever, we curate and release a\nnew instance-level instruction training set from MS MARCO, spanning nearly 500k\ninstances. Promptriever not only achieves strong performance on standard\nretrieval tasks, but also follows instructions. We observe: (1) large gains\n(reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR /\n+3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical\nchoices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR),\nand (3) the ability to perform hyperparameter search via prompting to reliably\nimprove retrieval performance (+1.4 average increase on BEIR). Promptriever\ndemonstrates that retrieval models can be controlled with prompts on a\nper-query basis, setting the stage for future work aligning LM prompting\ntechniques with information retrieval.\n", "link": "http://arxiv.org/abs/2409.11136v1", "date": "2024-09-17", "relevancy": 2.2935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Promptriever%3A%20Instruction-Trained%20Retrievers%20Can%20Be%20Prompted%20Like%0A%20%20Language%20Models&body=Title%3A%20Promptriever%3A%20Instruction-Trained%20Retrievers%20Can%20Be%20Prompted%20Like%0A%20%20Language%20Models%0AAuthor%3A%20Orion%20Weller%20and%20Benjamin%20Van%20Durme%20and%20Dawn%20Lawrie%20and%20Ashwin%20Paranjape%20and%20Yuhao%20Zhang%20and%20Jack%20Hessel%0AAbstract%3A%20%20%20Instruction-tuned%20language%20models%20%28LM%29%20are%20able%20to%20respond%20to%20imperative%0Acommands%2C%20providing%20a%20more%20natural%20user%20interface%20compared%20to%20their%20base%0Acounterparts.%20In%20this%20work%2C%20we%20present%20Promptriever%2C%20the%20first%20retrieval%20model%0Aable%20to%20be%20prompted%20like%20an%20LM.%20To%20train%20Promptriever%2C%20we%20curate%20and%20release%20a%0Anew%20instance-level%20instruction%20training%20set%20from%20MS%20MARCO%2C%20spanning%20nearly%20500k%0Ainstances.%20Promptriever%20not%20only%20achieves%20strong%20performance%20on%20standard%0Aretrieval%20tasks%2C%20but%20also%20follows%20instructions.%20We%20observe%3A%20%281%29%20large%20gains%0A%28reaching%20SoTA%29%20on%20following%20detailed%20relevance%20instructions%20%28%2B14.3%20p-MRR%20/%0A%2B3.1%20nDCG%20on%20FollowIR%29%2C%20%282%29%20significantly%20increased%20robustness%20to%20lexical%0Achoices/phrasing%20in%20the%20query%2Binstruction%20%28%2B12.9%20Robustness%4010%20on%20InstructIR%29%2C%0Aand%20%283%29%20the%20ability%20to%20perform%20hyperparameter%20search%20via%20prompting%20to%20reliably%0Aimprove%20retrieval%20performance%20%28%2B1.4%20average%20increase%20on%20BEIR%29.%20Promptriever%0Ademonstrates%20that%20retrieval%20models%20can%20be%20controlled%20with%20prompts%20on%20a%0Aper-query%20basis%2C%20setting%20the%20stage%20for%20future%20work%20aligning%20LM%20prompting%0Atechniques%20with%20information%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptriever%253A%2520Instruction-Trained%2520Retrievers%2520Can%2520Be%2520Prompted%2520Like%250A%2520%2520Language%2520Models%26entry.906535625%3DOrion%2520Weller%2520and%2520Benjamin%2520Van%2520Durme%2520and%2520Dawn%2520Lawrie%2520and%2520Ashwin%2520Paranjape%2520and%2520Yuhao%2520Zhang%2520and%2520Jack%2520Hessel%26entry.1292438233%3D%2520%2520Instruction-tuned%2520language%2520models%2520%2528LM%2529%2520are%2520able%2520to%2520respond%2520to%2520imperative%250Acommands%252C%2520providing%2520a%2520more%2520natural%2520user%2520interface%2520compared%2520to%2520their%2520base%250Acounterparts.%2520In%2520this%2520work%252C%2520we%2520present%2520Promptriever%252C%2520the%2520first%2520retrieval%2520model%250Aable%2520to%2520be%2520prompted%2520like%2520an%2520LM.%2520To%2520train%2520Promptriever%252C%2520we%2520curate%2520and%2520release%2520a%250Anew%2520instance-level%2520instruction%2520training%2520set%2520from%2520MS%2520MARCO%252C%2520spanning%2520nearly%2520500k%250Ainstances.%2520Promptriever%2520not%2520only%2520achieves%2520strong%2520performance%2520on%2520standard%250Aretrieval%2520tasks%252C%2520but%2520also%2520follows%2520instructions.%2520We%2520observe%253A%2520%25281%2529%2520large%2520gains%250A%2528reaching%2520SoTA%2529%2520on%2520following%2520detailed%2520relevance%2520instructions%2520%2528%252B14.3%2520p-MRR%2520/%250A%252B3.1%2520nDCG%2520on%2520FollowIR%2529%252C%2520%25282%2529%2520significantly%2520increased%2520robustness%2520to%2520lexical%250Achoices/phrasing%2520in%2520the%2520query%252Binstruction%2520%2528%252B12.9%2520Robustness%254010%2520on%2520InstructIR%2529%252C%250Aand%2520%25283%2529%2520the%2520ability%2520to%2520perform%2520hyperparameter%2520search%2520via%2520prompting%2520to%2520reliably%250Aimprove%2520retrieval%2520performance%2520%2528%252B1.4%2520average%2520increase%2520on%2520BEIR%2529.%2520Promptriever%250Ademonstrates%2520that%2520retrieval%2520models%2520can%2520be%2520controlled%2520with%2520prompts%2520on%2520a%250Aper-query%2520basis%252C%2520setting%2520the%2520stage%2520for%2520future%2520work%2520aligning%2520LM%2520prompting%250Atechniques%2520with%2520information%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Promptriever%3A%20Instruction-Trained%20Retrievers%20Can%20Be%20Prompted%20Like%0A%20%20Language%20Models&entry.906535625=Orion%20Weller%20and%20Benjamin%20Van%20Durme%20and%20Dawn%20Lawrie%20and%20Ashwin%20Paranjape%20and%20Yuhao%20Zhang%20and%20Jack%20Hessel&entry.1292438233=%20%20Instruction-tuned%20language%20models%20%28LM%29%20are%20able%20to%20respond%20to%20imperative%0Acommands%2C%20providing%20a%20more%20natural%20user%20interface%20compared%20to%20their%20base%0Acounterparts.%20In%20this%20work%2C%20we%20present%20Promptriever%2C%20the%20first%20retrieval%20model%0Aable%20to%20be%20prompted%20like%20an%20LM.%20To%20train%20Promptriever%2C%20we%20curate%20and%20release%20a%0Anew%20instance-level%20instruction%20training%20set%20from%20MS%20MARCO%2C%20spanning%20nearly%20500k%0Ainstances.%20Promptriever%20not%20only%20achieves%20strong%20performance%20on%20standard%0Aretrieval%20tasks%2C%20but%20also%20follows%20instructions.%20We%20observe%3A%20%281%29%20large%20gains%0A%28reaching%20SoTA%29%20on%20following%20detailed%20relevance%20instructions%20%28%2B14.3%20p-MRR%20/%0A%2B3.1%20nDCG%20on%20FollowIR%29%2C%20%282%29%20significantly%20increased%20robustness%20to%20lexical%0Achoices/phrasing%20in%20the%20query%2Binstruction%20%28%2B12.9%20Robustness%4010%20on%20InstructIR%29%2C%0Aand%20%283%29%20the%20ability%20to%20perform%20hyperparameter%20search%20via%20prompting%20to%20reliably%0Aimprove%20retrieval%20performance%20%28%2B1.4%20average%20increase%20on%20BEIR%29.%20Promptriever%0Ademonstrates%20that%20retrieval%20models%20can%20be%20controlled%20with%20prompts%20on%20a%0Aper-query%20basis%2C%20setting%20the%20stage%20for%20future%20work%20aligning%20LM%20prompting%0Atechniques%20with%20information%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11136v1&entry.124074799=Read"},
{"title": "Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series\n  Foundational Models", "author": "Divij Gupta and Anubhav Bhatti and Surajsinh Parmar", "abstract": "  Time Series Foundation Models (TSFMs) have recently garnered attention for\ntheir ability to model complex, large-scale time series data across domains\nsuch as retail, finance, and transportation. However, their application to\nsensitive, domain-specific fields like healthcare remains challenging,\nprimarily due to the difficulty of fine-tuning these models for specialized,\nout-of-domain tasks with scarce publicly available datasets. In this work, we\nexplore the use of Parameter-Efficient Fine-Tuning (PEFT) techniques to address\nthese limitations, focusing on healthcare applications, particularly ICU vitals\nforecasting for sepsis patients. We introduce and evaluate two selective\n(BitFit and LayerNorm Tuning) and two additive (VeRA and FourierFT) PEFT\ntechniques on multiple configurations of the Chronos TSFM for forecasting vital\nsigns of sepsis patients. Our comparative analysis demonstrates that some of\nthese PEFT methods outperform LoRA in terms of parameter efficiency and domain\nadaptation, establishing state-of-the-art (SOTA) results in ICU vital\nforecasting tasks. Interestingly, FourierFT applied to the Chronos (Tiny)\nvariant surpasses the SOTA model while fine-tuning only 2,400 parameters\ncompared to the 700K parameters of the benchmark.\n", "link": "http://arxiv.org/abs/2409.11302v1", "date": "2024-09-17", "relevancy": 2.2887, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20LoRA%3A%20Exploring%20Efficient%20Fine-Tuning%20Techniques%20for%20Time%20Series%0A%20%20Foundational%20Models&body=Title%3A%20Beyond%20LoRA%3A%20Exploring%20Efficient%20Fine-Tuning%20Techniques%20for%20Time%20Series%0A%20%20Foundational%20Models%0AAuthor%3A%20Divij%20Gupta%20and%20Anubhav%20Bhatti%20and%20Surajsinh%20Parmar%0AAbstract%3A%20%20%20Time%20Series%20Foundation%20Models%20%28TSFMs%29%20have%20recently%20garnered%20attention%20for%0Atheir%20ability%20to%20model%20complex%2C%20large-scale%20time%20series%20data%20across%20domains%0Asuch%20as%20retail%2C%20finance%2C%20and%20transportation.%20However%2C%20their%20application%20to%0Asensitive%2C%20domain-specific%20fields%20like%20healthcare%20remains%20challenging%2C%0Aprimarily%20due%20to%20the%20difficulty%20of%20fine-tuning%20these%20models%20for%20specialized%2C%0Aout-of-domain%20tasks%20with%20scarce%20publicly%20available%20datasets.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20techniques%20to%20address%0Athese%20limitations%2C%20focusing%20on%20healthcare%20applications%2C%20particularly%20ICU%20vitals%0Aforecasting%20for%20sepsis%20patients.%20We%20introduce%20and%20evaluate%20two%20selective%0A%28BitFit%20and%20LayerNorm%20Tuning%29%20and%20two%20additive%20%28VeRA%20and%20FourierFT%29%20PEFT%0Atechniques%20on%20multiple%20configurations%20of%20the%20Chronos%20TSFM%20for%20forecasting%20vital%0Asigns%20of%20sepsis%20patients.%20Our%20comparative%20analysis%20demonstrates%20that%20some%20of%0Athese%20PEFT%20methods%20outperform%20LoRA%20in%20terms%20of%20parameter%20efficiency%20and%20domain%0Aadaptation%2C%20establishing%20state-of-the-art%20%28SOTA%29%20results%20in%20ICU%20vital%0Aforecasting%20tasks.%20Interestingly%2C%20FourierFT%20applied%20to%20the%20Chronos%20%28Tiny%29%0Avariant%20surpasses%20the%20SOTA%20model%20while%20fine-tuning%20only%202%2C400%20parameters%0Acompared%20to%20the%20700K%20parameters%20of%20the%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520LoRA%253A%2520Exploring%2520Efficient%2520Fine-Tuning%2520Techniques%2520for%2520Time%2520Series%250A%2520%2520Foundational%2520Models%26entry.906535625%3DDivij%2520Gupta%2520and%2520Anubhav%2520Bhatti%2520and%2520Surajsinh%2520Parmar%26entry.1292438233%3D%2520%2520Time%2520Series%2520Foundation%2520Models%2520%2528TSFMs%2529%2520have%2520recently%2520garnered%2520attention%2520for%250Atheir%2520ability%2520to%2520model%2520complex%252C%2520large-scale%2520time%2520series%2520data%2520across%2520domains%250Asuch%2520as%2520retail%252C%2520finance%252C%2520and%2520transportation.%2520However%252C%2520their%2520application%2520to%250Asensitive%252C%2520domain-specific%2520fields%2520like%2520healthcare%2520remains%2520challenging%252C%250Aprimarily%2520due%2520to%2520the%2520difficulty%2520of%2520fine-tuning%2520these%2520models%2520for%2520specialized%252C%250Aout-of-domain%2520tasks%2520with%2520scarce%2520publicly%2520available%2520datasets.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520the%2520use%2520of%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520techniques%2520to%2520address%250Athese%2520limitations%252C%2520focusing%2520on%2520healthcare%2520applications%252C%2520particularly%2520ICU%2520vitals%250Aforecasting%2520for%2520sepsis%2520patients.%2520We%2520introduce%2520and%2520evaluate%2520two%2520selective%250A%2528BitFit%2520and%2520LayerNorm%2520Tuning%2529%2520and%2520two%2520additive%2520%2528VeRA%2520and%2520FourierFT%2529%2520PEFT%250Atechniques%2520on%2520multiple%2520configurations%2520of%2520the%2520Chronos%2520TSFM%2520for%2520forecasting%2520vital%250Asigns%2520of%2520sepsis%2520patients.%2520Our%2520comparative%2520analysis%2520demonstrates%2520that%2520some%2520of%250Athese%2520PEFT%2520methods%2520outperform%2520LoRA%2520in%2520terms%2520of%2520parameter%2520efficiency%2520and%2520domain%250Aadaptation%252C%2520establishing%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520in%2520ICU%2520vital%250Aforecasting%2520tasks.%2520Interestingly%252C%2520FourierFT%2520applied%2520to%2520the%2520Chronos%2520%2528Tiny%2529%250Avariant%2520surpasses%2520the%2520SOTA%2520model%2520while%2520fine-tuning%2520only%25202%252C400%2520parameters%250Acompared%2520to%2520the%2520700K%2520parameters%2520of%2520the%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20LoRA%3A%20Exploring%20Efficient%20Fine-Tuning%20Techniques%20for%20Time%20Series%0A%20%20Foundational%20Models&entry.906535625=Divij%20Gupta%20and%20Anubhav%20Bhatti%20and%20Surajsinh%20Parmar&entry.1292438233=%20%20Time%20Series%20Foundation%20Models%20%28TSFMs%29%20have%20recently%20garnered%20attention%20for%0Atheir%20ability%20to%20model%20complex%2C%20large-scale%20time%20series%20data%20across%20domains%0Asuch%20as%20retail%2C%20finance%2C%20and%20transportation.%20However%2C%20their%20application%20to%0Asensitive%2C%20domain-specific%20fields%20like%20healthcare%20remains%20challenging%2C%0Aprimarily%20due%20to%20the%20difficulty%20of%20fine-tuning%20these%20models%20for%20specialized%2C%0Aout-of-domain%20tasks%20with%20scarce%20publicly%20available%20datasets.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20techniques%20to%20address%0Athese%20limitations%2C%20focusing%20on%20healthcare%20applications%2C%20particularly%20ICU%20vitals%0Aforecasting%20for%20sepsis%20patients.%20We%20introduce%20and%20evaluate%20two%20selective%0A%28BitFit%20and%20LayerNorm%20Tuning%29%20and%20two%20additive%20%28VeRA%20and%20FourierFT%29%20PEFT%0Atechniques%20on%20multiple%20configurations%20of%20the%20Chronos%20TSFM%20for%20forecasting%20vital%0Asigns%20of%20sepsis%20patients.%20Our%20comparative%20analysis%20demonstrates%20that%20some%20of%0Athese%20PEFT%20methods%20outperform%20LoRA%20in%20terms%20of%20parameter%20efficiency%20and%20domain%0Aadaptation%2C%20establishing%20state-of-the-art%20%28SOTA%29%20results%20in%20ICU%20vital%0Aforecasting%20tasks.%20Interestingly%2C%20FourierFT%20applied%20to%20the%20Chronos%20%28Tiny%29%0Avariant%20surpasses%20the%20SOTA%20model%20while%20fine-tuning%20only%202%2C400%20parameters%0Acompared%20to%20the%20700K%20parameters%20of%20the%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11302v1&entry.124074799=Read"},
{"title": "SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking", "author": "Siyuan Li and Lei Ke and Yung-Hsu Yang and Luigi Piccinelli and Mattia Seg\u00f9 and Martin Danelljan and Luc Van Gool", "abstract": "  Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to\nnovel categories not in the training set. Currently, the best-performing\nmethods are mainly based on pure appearance matching. Due to the complexity of\nmotion patterns in the large-vocabulary scenarios and unstable classification\nof the novel objects, the motion and semantics cues are either ignored or\napplied based on heuristics in the final matching steps by existing methods. In\nthis paper, we present a unified framework SLAck that jointly considers\nsemantics, location, and appearance priors in the early steps of association\nand learns how to integrate all valuable information through a lightweight\nspatial and temporal object graph. Our method eliminates complex\npost-processing heuristics for fusing different cues and boosts the association\nperformance significantly for large-scale open-vocabulary tracking. Without\nbells and whistles, we outperform previous state-of-the-art methods for novel\nclasses tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code\nis available at\n\\href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.\n", "link": "http://arxiv.org/abs/2409.11235v1", "date": "2024-09-17", "relevancy": 2.271, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAck%3A%20Semantic%2C%20Location%2C%20and%20Appearance%20Aware%20Open-Vocabulary%20Tracking&body=Title%3A%20SLAck%3A%20Semantic%2C%20Location%2C%20and%20Appearance%20Aware%20Open-Vocabulary%20Tracking%0AAuthor%3A%20Siyuan%20Li%20and%20Lei%20Ke%20and%20Yung-Hsu%20Yang%20and%20Luigi%20Piccinelli%20and%20Mattia%20Seg%C3%B9%20and%20Martin%20Danelljan%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Open-vocabulary%20Multiple%20Object%20Tracking%20%28MOT%29%20aims%20to%20generalize%20trackers%20to%0Anovel%20categories%20not%20in%20the%20training%20set.%20Currently%2C%20the%20best-performing%0Amethods%20are%20mainly%20based%20on%20pure%20appearance%20matching.%20Due%20to%20the%20complexity%20of%0Amotion%20patterns%20in%20the%20large-vocabulary%20scenarios%20and%20unstable%20classification%0Aof%20the%20novel%20objects%2C%20the%20motion%20and%20semantics%20cues%20are%20either%20ignored%20or%0Aapplied%20based%20on%20heuristics%20in%20the%20final%20matching%20steps%20by%20existing%20methods.%20In%0Athis%20paper%2C%20we%20present%20a%20unified%20framework%20SLAck%20that%20jointly%20considers%0Asemantics%2C%20location%2C%20and%20appearance%20priors%20in%20the%20early%20steps%20of%20association%0Aand%20learns%20how%20to%20integrate%20all%20valuable%20information%20through%20a%20lightweight%0Aspatial%20and%20temporal%20object%20graph.%20Our%20method%20eliminates%20complex%0Apost-processing%20heuristics%20for%20fusing%20different%20cues%20and%20boosts%20the%20association%0Aperformance%20significantly%20for%20large-scale%20open-vocabulary%20tracking.%20Without%0Abells%20and%20whistles%2C%20we%20outperform%20previous%20state-of-the-art%20methods%20for%20novel%0Aclasses%20tracking%20on%20the%20open-vocabulary%20MOT%20and%20TAO%20TETA%20benchmarks.%20Our%20code%0Ais%20available%20at%0A%5Chref%7Bhttps%3A//github.com/siyuanliii/SLAck%7D%7Bgithub.com/siyuanliii/SLAck%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAck%253A%2520Semantic%252C%2520Location%252C%2520and%2520Appearance%2520Aware%2520Open-Vocabulary%2520Tracking%26entry.906535625%3DSiyuan%2520Li%2520and%2520Lei%2520Ke%2520and%2520Yung-Hsu%2520Yang%2520and%2520Luigi%2520Piccinelli%2520and%2520Mattia%2520Seg%25C3%25B9%2520and%2520Martin%2520Danelljan%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Open-vocabulary%2520Multiple%2520Object%2520Tracking%2520%2528MOT%2529%2520aims%2520to%2520generalize%2520trackers%2520to%250Anovel%2520categories%2520not%2520in%2520the%2520training%2520set.%2520Currently%252C%2520the%2520best-performing%250Amethods%2520are%2520mainly%2520based%2520on%2520pure%2520appearance%2520matching.%2520Due%2520to%2520the%2520complexity%2520of%250Amotion%2520patterns%2520in%2520the%2520large-vocabulary%2520scenarios%2520and%2520unstable%2520classification%250Aof%2520the%2520novel%2520objects%252C%2520the%2520motion%2520and%2520semantics%2520cues%2520are%2520either%2520ignored%2520or%250Aapplied%2520based%2520on%2520heuristics%2520in%2520the%2520final%2520matching%2520steps%2520by%2520existing%2520methods.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520unified%2520framework%2520SLAck%2520that%2520jointly%2520considers%250Asemantics%252C%2520location%252C%2520and%2520appearance%2520priors%2520in%2520the%2520early%2520steps%2520of%2520association%250Aand%2520learns%2520how%2520to%2520integrate%2520all%2520valuable%2520information%2520through%2520a%2520lightweight%250Aspatial%2520and%2520temporal%2520object%2520graph.%2520Our%2520method%2520eliminates%2520complex%250Apost-processing%2520heuristics%2520for%2520fusing%2520different%2520cues%2520and%2520boosts%2520the%2520association%250Aperformance%2520significantly%2520for%2520large-scale%2520open-vocabulary%2520tracking.%2520Without%250Abells%2520and%2520whistles%252C%2520we%2520outperform%2520previous%2520state-of-the-art%2520methods%2520for%2520novel%250Aclasses%2520tracking%2520on%2520the%2520open-vocabulary%2520MOT%2520and%2520TAO%2520TETA%2520benchmarks.%2520Our%2520code%250Ais%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/siyuanliii/SLAck%257D%257Bgithub.com/siyuanliii/SLAck%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAck%3A%20Semantic%2C%20Location%2C%20and%20Appearance%20Aware%20Open-Vocabulary%20Tracking&entry.906535625=Siyuan%20Li%20and%20Lei%20Ke%20and%20Yung-Hsu%20Yang%20and%20Luigi%20Piccinelli%20and%20Mattia%20Seg%C3%B9%20and%20Martin%20Danelljan%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Open-vocabulary%20Multiple%20Object%20Tracking%20%28MOT%29%20aims%20to%20generalize%20trackers%20to%0Anovel%20categories%20not%20in%20the%20training%20set.%20Currently%2C%20the%20best-performing%0Amethods%20are%20mainly%20based%20on%20pure%20appearance%20matching.%20Due%20to%20the%20complexity%20of%0Amotion%20patterns%20in%20the%20large-vocabulary%20scenarios%20and%20unstable%20classification%0Aof%20the%20novel%20objects%2C%20the%20motion%20and%20semantics%20cues%20are%20either%20ignored%20or%0Aapplied%20based%20on%20heuristics%20in%20the%20final%20matching%20steps%20by%20existing%20methods.%20In%0Athis%20paper%2C%20we%20present%20a%20unified%20framework%20SLAck%20that%20jointly%20considers%0Asemantics%2C%20location%2C%20and%20appearance%20priors%20in%20the%20early%20steps%20of%20association%0Aand%20learns%20how%20to%20integrate%20all%20valuable%20information%20through%20a%20lightweight%0Aspatial%20and%20temporal%20object%20graph.%20Our%20method%20eliminates%20complex%0Apost-processing%20heuristics%20for%20fusing%20different%20cues%20and%20boosts%20the%20association%0Aperformance%20significantly%20for%20large-scale%20open-vocabulary%20tracking.%20Without%0Abells%20and%20whistles%2C%20we%20outperform%20previous%20state-of-the-art%20methods%20for%20novel%0Aclasses%20tracking%20on%20the%20open-vocabulary%20MOT%20and%20TAO%20TETA%20benchmarks.%20Our%20code%0Ais%20available%20at%0A%5Chref%7Bhttps%3A//github.com/siyuanliii/SLAck%7D%7Bgithub.com/siyuanliii/SLAck%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11235v1&entry.124074799=Read"},
{"title": "A Simple Generative Network", "author": "Daniel N. Nissani", "abstract": "  Generative neural networks are able to mimic intricate probability\ndistributions such as those of handwritten text, natural images, etc. Since\ntheir inception several models were proposed. The most successful of these were\nbased on adversarial (GAN), auto-encoding (VAE) and maximum mean discrepancy\n(MMD) relatively complex architectures and schemes. Surprisingly, a very simple\narchitecture (a single feed-forward neural network) in conjunction with an\nobvious optimization goal (Kullback_Leibler divergence) was apparently\noverlooked. This paper demonstrates that such a model (denoted SGN for its\nsimplicity) is able to generate samples visually and quantitatively competitive\nas compared with the fore-mentioned state of the art methods.\n", "link": "http://arxiv.org/abs/2106.09330v6", "date": "2024-09-17", "relevancy": 2.2566, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5841}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5579}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Generative%20Network&body=Title%3A%20A%20Simple%20Generative%20Network%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20Generative%20neural%20networks%20are%20able%20to%20mimic%20intricate%20probability%0Adistributions%20such%20as%20those%20of%20handwritten%20text%2C%20natural%20images%2C%20etc.%20Since%0Atheir%20inception%20several%20models%20were%20proposed.%20The%20most%20successful%20of%20these%20were%0Abased%20on%20adversarial%20%28GAN%29%2C%20auto-encoding%20%28VAE%29%20and%20maximum%20mean%20discrepancy%0A%28MMD%29%20relatively%20complex%20architectures%20and%20schemes.%20Surprisingly%2C%20a%20very%20simple%0Aarchitecture%20%28a%20single%20feed-forward%20neural%20network%29%20in%20conjunction%20with%20an%0Aobvious%20optimization%20goal%20%28Kullback_Leibler%20divergence%29%20was%20apparently%0Aoverlooked.%20This%20paper%20demonstrates%20that%20such%20a%20model%20%28denoted%20SGN%20for%20its%0Asimplicity%29%20is%20able%20to%20generate%20samples%20visually%20and%20quantitatively%20competitive%0Aas%20compared%20with%20the%20fore-mentioned%20state%20of%20the%20art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2106.09330v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Generative%2520Network%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520Generative%2520neural%2520networks%2520are%2520able%2520to%2520mimic%2520intricate%2520probability%250Adistributions%2520such%2520as%2520those%2520of%2520handwritten%2520text%252C%2520natural%2520images%252C%2520etc.%2520Since%250Atheir%2520inception%2520several%2520models%2520were%2520proposed.%2520The%2520most%2520successful%2520of%2520these%2520were%250Abased%2520on%2520adversarial%2520%2528GAN%2529%252C%2520auto-encoding%2520%2528VAE%2529%2520and%2520maximum%2520mean%2520discrepancy%250A%2528MMD%2529%2520relatively%2520complex%2520architectures%2520and%2520schemes.%2520Surprisingly%252C%2520a%2520very%2520simple%250Aarchitecture%2520%2528a%2520single%2520feed-forward%2520neural%2520network%2529%2520in%2520conjunction%2520with%2520an%250Aobvious%2520optimization%2520goal%2520%2528Kullback_Leibler%2520divergence%2529%2520was%2520apparently%250Aoverlooked.%2520This%2520paper%2520demonstrates%2520that%2520such%2520a%2520model%2520%2528denoted%2520SGN%2520for%2520its%250Asimplicity%2529%2520is%2520able%2520to%2520generate%2520samples%2520visually%2520and%2520quantitatively%2520competitive%250Aas%2520compared%2520with%2520the%2520fore-mentioned%2520state%2520of%2520the%2520art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2106.09330v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Generative%20Network&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20Generative%20neural%20networks%20are%20able%20to%20mimic%20intricate%20probability%0Adistributions%20such%20as%20those%20of%20handwritten%20text%2C%20natural%20images%2C%20etc.%20Since%0Atheir%20inception%20several%20models%20were%20proposed.%20The%20most%20successful%20of%20these%20were%0Abased%20on%20adversarial%20%28GAN%29%2C%20auto-encoding%20%28VAE%29%20and%20maximum%20mean%20discrepancy%0A%28MMD%29%20relatively%20complex%20architectures%20and%20schemes.%20Surprisingly%2C%20a%20very%20simple%0Aarchitecture%20%28a%20single%20feed-forward%20neural%20network%29%20in%20conjunction%20with%20an%0Aobvious%20optimization%20goal%20%28Kullback_Leibler%20divergence%29%20was%20apparently%0Aoverlooked.%20This%20paper%20demonstrates%20that%20such%20a%20model%20%28denoted%20SGN%20for%20its%0Asimplicity%29%20is%20able%20to%20generate%20samples%20visually%20and%20quantitatively%20competitive%0Aas%20compared%20with%20the%20fore-mentioned%20state%20of%20the%20art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.09330v6&entry.124074799=Read"},
{"title": "Neural Networks for Vehicle Routing Problem", "author": "L\u00e1szl\u00f3 Kov\u00e1cs and Ali Jlidi", "abstract": "  The Vehicle Routing Problem is about optimizing the routes of vehicles to\nmeet the needs of customers at specific locations. The route graph consists of\ndepots on several levels and customer positions. Several optimization methods\nhave been developed over the years, most of which are based on some type of\nclassic heuristic: genetic algorithm, simulated annealing, tabu search, ant\ncolony optimization, firefly algorithm. Recent developments in machine learning\nprovide a new toolset, the rich family of neural networks, for tackling complex\nproblems. The main area of application of neural networks is the area of\nclassification and regression. Route optimization can be viewed as a new\nchallenge for neural networks. The article first presents an analysis of the\napplicability of neural network tools, then a novel graphical neural network\nmodel is presented in detail. The efficiency analysis based on test experiments\nshows the applicability of the proposed NN architecture.\n", "link": "http://arxiv.org/abs/2409.11290v1", "date": "2024-09-17", "relevancy": 2.256, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4567}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Networks%20for%20Vehicle%20Routing%20Problem&body=Title%3A%20Neural%20Networks%20for%20Vehicle%20Routing%20Problem%0AAuthor%3A%20L%C3%A1szl%C3%B3%20Kov%C3%A1cs%20and%20Ali%20Jlidi%0AAbstract%3A%20%20%20The%20Vehicle%20Routing%20Problem%20is%20about%20optimizing%20the%20routes%20of%20vehicles%20to%0Ameet%20the%20needs%20of%20customers%20at%20specific%20locations.%20The%20route%20graph%20consists%20of%0Adepots%20on%20several%20levels%20and%20customer%20positions.%20Several%20optimization%20methods%0Ahave%20been%20developed%20over%20the%20years%2C%20most%20of%20which%20are%20based%20on%20some%20type%20of%0Aclassic%20heuristic%3A%20genetic%20algorithm%2C%20simulated%20annealing%2C%20tabu%20search%2C%20ant%0Acolony%20optimization%2C%20firefly%20algorithm.%20Recent%20developments%20in%20machine%20learning%0Aprovide%20a%20new%20toolset%2C%20the%20rich%20family%20of%20neural%20networks%2C%20for%20tackling%20complex%0Aproblems.%20The%20main%20area%20of%20application%20of%20neural%20networks%20is%20the%20area%20of%0Aclassification%20and%20regression.%20Route%20optimization%20can%20be%20viewed%20as%20a%20new%0Achallenge%20for%20neural%20networks.%20The%20article%20first%20presents%20an%20analysis%20of%20the%0Aapplicability%20of%20neural%20network%20tools%2C%20then%20a%20novel%20graphical%20neural%20network%0Amodel%20is%20presented%20in%20detail.%20The%20efficiency%20analysis%20based%20on%20test%20experiments%0Ashows%20the%20applicability%20of%20the%20proposed%20NN%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Networks%2520for%2520Vehicle%2520Routing%2520Problem%26entry.906535625%3DL%25C3%25A1szl%25C3%25B3%2520Kov%25C3%25A1cs%2520and%2520Ali%2520Jlidi%26entry.1292438233%3D%2520%2520The%2520Vehicle%2520Routing%2520Problem%2520is%2520about%2520optimizing%2520the%2520routes%2520of%2520vehicles%2520to%250Ameet%2520the%2520needs%2520of%2520customers%2520at%2520specific%2520locations.%2520The%2520route%2520graph%2520consists%2520of%250Adepots%2520on%2520several%2520levels%2520and%2520customer%2520positions.%2520Several%2520optimization%2520methods%250Ahave%2520been%2520developed%2520over%2520the%2520years%252C%2520most%2520of%2520which%2520are%2520based%2520on%2520some%2520type%2520of%250Aclassic%2520heuristic%253A%2520genetic%2520algorithm%252C%2520simulated%2520annealing%252C%2520tabu%2520search%252C%2520ant%250Acolony%2520optimization%252C%2520firefly%2520algorithm.%2520Recent%2520developments%2520in%2520machine%2520learning%250Aprovide%2520a%2520new%2520toolset%252C%2520the%2520rich%2520family%2520of%2520neural%2520networks%252C%2520for%2520tackling%2520complex%250Aproblems.%2520The%2520main%2520area%2520of%2520application%2520of%2520neural%2520networks%2520is%2520the%2520area%2520of%250Aclassification%2520and%2520regression.%2520Route%2520optimization%2520can%2520be%2520viewed%2520as%2520a%2520new%250Achallenge%2520for%2520neural%2520networks.%2520The%2520article%2520first%2520presents%2520an%2520analysis%2520of%2520the%250Aapplicability%2520of%2520neural%2520network%2520tools%252C%2520then%2520a%2520novel%2520graphical%2520neural%2520network%250Amodel%2520is%2520presented%2520in%2520detail.%2520The%2520efficiency%2520analysis%2520based%2520on%2520test%2520experiments%250Ashows%2520the%2520applicability%2520of%2520the%2520proposed%2520NN%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20for%20Vehicle%20Routing%20Problem&entry.906535625=L%C3%A1szl%C3%B3%20Kov%C3%A1cs%20and%20Ali%20Jlidi&entry.1292438233=%20%20The%20Vehicle%20Routing%20Problem%20is%20about%20optimizing%20the%20routes%20of%20vehicles%20to%0Ameet%20the%20needs%20of%20customers%20at%20specific%20locations.%20The%20route%20graph%20consists%20of%0Adepots%20on%20several%20levels%20and%20customer%20positions.%20Several%20optimization%20methods%0Ahave%20been%20developed%20over%20the%20years%2C%20most%20of%20which%20are%20based%20on%20some%20type%20of%0Aclassic%20heuristic%3A%20genetic%20algorithm%2C%20simulated%20annealing%2C%20tabu%20search%2C%20ant%0Acolony%20optimization%2C%20firefly%20algorithm.%20Recent%20developments%20in%20machine%20learning%0Aprovide%20a%20new%20toolset%2C%20the%20rich%20family%20of%20neural%20networks%2C%20for%20tackling%20complex%0Aproblems.%20The%20main%20area%20of%20application%20of%20neural%20networks%20is%20the%20area%20of%0Aclassification%20and%20regression.%20Route%20optimization%20can%20be%20viewed%20as%20a%20new%0Achallenge%20for%20neural%20networks.%20The%20article%20first%20presents%20an%20analysis%20of%20the%0Aapplicability%20of%20neural%20network%20tools%2C%20then%20a%20novel%20graphical%20neural%20network%0Amodel%20is%20presented%20in%20detail.%20The%20efficiency%20analysis%20based%20on%20test%20experiments%0Ashows%20the%20applicability%20of%20the%20proposed%20NN%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11290v1&entry.124074799=Read"},
{"title": "CLIP Adaptation by Intra-modal Overlap Reduction", "author": "Alexey Kravets and Vinay Namboodiri", "abstract": "  Numerous methods have been proposed to adapt a pre-trained foundational CLIP\nmodel for few-shot classification. As CLIP is trained on a large corpus, it\ngeneralises well through adaptation to few-shot classification. In this work,\nwe analyse the intra-modal overlap in image space in terms of embedding\nrepresentation. Our analysis shows that, due to contrastive learning,\nembeddings from CLIP model exhibit high cosine similarity distribution overlap\nin the image space between paired and unpaired examples affecting the\nperformance of few-shot training-free classification methods which rely on\nsimilarity in the image space for their predictions. To tackle intra-modal\noverlap we propose to train a lightweight adapter on a generic set of samples\nfrom the Google Open Images dataset demonstrating that this improves accuracy\nfor few-shot training-free classification. We validate our contribution through\nextensive empirical analysis and demonstrate that reducing the intra-modal\noverlap leads to a) improved performance on a number of standard datasets, b)\nincreased robustness to distribution shift and c) higher feature variance\nrendering the features more discriminative for downstream tasks.\n", "link": "http://arxiv.org/abs/2409.11338v1", "date": "2024-09-17", "relevancy": 2.2493, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6343}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5209}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20Adaptation%20by%20Intra-modal%20Overlap%20Reduction&body=Title%3A%20CLIP%20Adaptation%20by%20Intra-modal%20Overlap%20Reduction%0AAuthor%3A%20Alexey%20Kravets%20and%20Vinay%20Namboodiri%0AAbstract%3A%20%20%20Numerous%20methods%20have%20been%20proposed%20to%20adapt%20a%20pre-trained%20foundational%20CLIP%0Amodel%20for%20few-shot%20classification.%20As%20CLIP%20is%20trained%20on%20a%20large%20corpus%2C%20it%0Ageneralises%20well%20through%20adaptation%20to%20few-shot%20classification.%20In%20this%20work%2C%0Awe%20analyse%20the%20intra-modal%20overlap%20in%20image%20space%20in%20terms%20of%20embedding%0Arepresentation.%20Our%20analysis%20shows%20that%2C%20due%20to%20contrastive%20learning%2C%0Aembeddings%20from%20CLIP%20model%20exhibit%20high%20cosine%20similarity%20distribution%20overlap%0Ain%20the%20image%20space%20between%20paired%20and%20unpaired%20examples%20affecting%20the%0Aperformance%20of%20few-shot%20training-free%20classification%20methods%20which%20rely%20on%0Asimilarity%20in%20the%20image%20space%20for%20their%20predictions.%20To%20tackle%20intra-modal%0Aoverlap%20we%20propose%20to%20train%20a%20lightweight%20adapter%20on%20a%20generic%20set%20of%20samples%0Afrom%20the%20Google%20Open%20Images%20dataset%20demonstrating%20that%20this%20improves%20accuracy%0Afor%20few-shot%20training-free%20classification.%20We%20validate%20our%20contribution%20through%0Aextensive%20empirical%20analysis%20and%20demonstrate%20that%20reducing%20the%20intra-modal%0Aoverlap%20leads%20to%20a%29%20improved%20performance%20on%20a%20number%20of%20standard%20datasets%2C%20b%29%0Aincreased%20robustness%20to%20distribution%20shift%20and%20c%29%20higher%20feature%20variance%0Arendering%20the%20features%20more%20discriminative%20for%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520Adaptation%2520by%2520Intra-modal%2520Overlap%2520Reduction%26entry.906535625%3DAlexey%2520Kravets%2520and%2520Vinay%2520Namboodiri%26entry.1292438233%3D%2520%2520Numerous%2520methods%2520have%2520been%2520proposed%2520to%2520adapt%2520a%2520pre-trained%2520foundational%2520CLIP%250Amodel%2520for%2520few-shot%2520classification.%2520As%2520CLIP%2520is%2520trained%2520on%2520a%2520large%2520corpus%252C%2520it%250Ageneralises%2520well%2520through%2520adaptation%2520to%2520few-shot%2520classification.%2520In%2520this%2520work%252C%250Awe%2520analyse%2520the%2520intra-modal%2520overlap%2520in%2520image%2520space%2520in%2520terms%2520of%2520embedding%250Arepresentation.%2520Our%2520analysis%2520shows%2520that%252C%2520due%2520to%2520contrastive%2520learning%252C%250Aembeddings%2520from%2520CLIP%2520model%2520exhibit%2520high%2520cosine%2520similarity%2520distribution%2520overlap%250Ain%2520the%2520image%2520space%2520between%2520paired%2520and%2520unpaired%2520examples%2520affecting%2520the%250Aperformance%2520of%2520few-shot%2520training-free%2520classification%2520methods%2520which%2520rely%2520on%250Asimilarity%2520in%2520the%2520image%2520space%2520for%2520their%2520predictions.%2520To%2520tackle%2520intra-modal%250Aoverlap%2520we%2520propose%2520to%2520train%2520a%2520lightweight%2520adapter%2520on%2520a%2520generic%2520set%2520of%2520samples%250Afrom%2520the%2520Google%2520Open%2520Images%2520dataset%2520demonstrating%2520that%2520this%2520improves%2520accuracy%250Afor%2520few-shot%2520training-free%2520classification.%2520We%2520validate%2520our%2520contribution%2520through%250Aextensive%2520empirical%2520analysis%2520and%2520demonstrate%2520that%2520reducing%2520the%2520intra-modal%250Aoverlap%2520leads%2520to%2520a%2529%2520improved%2520performance%2520on%2520a%2520number%2520of%2520standard%2520datasets%252C%2520b%2529%250Aincreased%2520robustness%2520to%2520distribution%2520shift%2520and%2520c%2529%2520higher%2520feature%2520variance%250Arendering%2520the%2520features%2520more%2520discriminative%2520for%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20Adaptation%20by%20Intra-modal%20Overlap%20Reduction&entry.906535625=Alexey%20Kravets%20and%20Vinay%20Namboodiri&entry.1292438233=%20%20Numerous%20methods%20have%20been%20proposed%20to%20adapt%20a%20pre-trained%20foundational%20CLIP%0Amodel%20for%20few-shot%20classification.%20As%20CLIP%20is%20trained%20on%20a%20large%20corpus%2C%20it%0Ageneralises%20well%20through%20adaptation%20to%20few-shot%20classification.%20In%20this%20work%2C%0Awe%20analyse%20the%20intra-modal%20overlap%20in%20image%20space%20in%20terms%20of%20embedding%0Arepresentation.%20Our%20analysis%20shows%20that%2C%20due%20to%20contrastive%20learning%2C%0Aembeddings%20from%20CLIP%20model%20exhibit%20high%20cosine%20similarity%20distribution%20overlap%0Ain%20the%20image%20space%20between%20paired%20and%20unpaired%20examples%20affecting%20the%0Aperformance%20of%20few-shot%20training-free%20classification%20methods%20which%20rely%20on%0Asimilarity%20in%20the%20image%20space%20for%20their%20predictions.%20To%20tackle%20intra-modal%0Aoverlap%20we%20propose%20to%20train%20a%20lightweight%20adapter%20on%20a%20generic%20set%20of%20samples%0Afrom%20the%20Google%20Open%20Images%20dataset%20demonstrating%20that%20this%20improves%20accuracy%0Afor%20few-shot%20training-free%20classification.%20We%20validate%20our%20contribution%20through%0Aextensive%20empirical%20analysis%20and%20demonstrate%20that%20reducing%20the%20intra-modal%0Aoverlap%20leads%20to%20a%29%20improved%20performance%20on%20a%20number%20of%20standard%20datasets%2C%20b%29%0Aincreased%20robustness%20to%20distribution%20shift%20and%20c%29%20higher%20feature%20variance%0Arendering%20the%20features%20more%20discriminative%20for%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11338v1&entry.124074799=Read"},
{"title": "SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot\n  Object Detection", "author": "Anay Majee and Ryan Sharp and Rishabh Iyer", "abstract": "  Confusion and forgetting of object classes have been challenges of prime\ninterest in Few-Shot Object Detection (FSOD). To overcome these pitfalls in\nmetric learning based FSOD techniques, we introduce a novel Submodular Mutual\nInformation Learning (SMILe) framework which adopts combinatorial mutual\ninformation functions to enforce the creation of tighter and discriminative\nfeature clusters in FSOD. Our proposed approach generalizes to several existing\napproaches in FSOD, agnostic of the backbone architecture demonstrating\nelevated performance gains. A paradigm shift from instance based objective\nfunctions to combinatorial objectives in SMILe naturally preserves the\ndiversity within an object class resulting in reduced forgetting when subjected\nto few training examples. Furthermore, the application of mutual information\nbetween the already learnt (base) and newly added (novel) objects ensures\nsufficient separation between base and novel classes, minimizing the effect of\nclass confusion. Experiments on popular FSOD benchmarks, PASCAL-VOC and MS-COCO\nshow that our approach generalizes to State-of-the-Art (SoTA) approaches\nimproving their novel class performance by up to 5.7% (3.3 mAP points) and 5.4%\n(2.6 mAP points) on the 10-shot setting of VOC (split 3) and 30-shot setting of\nCOCO datasets respectively. Our experiments also demonstrate better retention\nof base class performance and up to 2x faster convergence over existing\napproaches agnostic of the underlying architecture.\n", "link": "http://arxiv.org/abs/2407.02665v2", "date": "2024-09-17", "relevancy": 2.2269, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5729}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMILe%3A%20Leveraging%20Submodular%20Mutual%20Information%20For%20Robust%20Few-Shot%0A%20%20Object%20Detection&body=Title%3A%20SMILe%3A%20Leveraging%20Submodular%20Mutual%20Information%20For%20Robust%20Few-Shot%0A%20%20Object%20Detection%0AAuthor%3A%20Anay%20Majee%20and%20Ryan%20Sharp%20and%20Rishabh%20Iyer%0AAbstract%3A%20%20%20Confusion%20and%20forgetting%20of%20object%20classes%20have%20been%20challenges%20of%20prime%0Ainterest%20in%20Few-Shot%20Object%20Detection%20%28FSOD%29.%20To%20overcome%20these%20pitfalls%20in%0Ametric%20learning%20based%20FSOD%20techniques%2C%20we%20introduce%20a%20novel%20Submodular%20Mutual%0AInformation%20Learning%20%28SMILe%29%20framework%20which%20adopts%20combinatorial%20mutual%0Ainformation%20functions%20to%20enforce%20the%20creation%20of%20tighter%20and%20discriminative%0Afeature%20clusters%20in%20FSOD.%20Our%20proposed%20approach%20generalizes%20to%20several%20existing%0Aapproaches%20in%20FSOD%2C%20agnostic%20of%20the%20backbone%20architecture%20demonstrating%0Aelevated%20performance%20gains.%20A%20paradigm%20shift%20from%20instance%20based%20objective%0Afunctions%20to%20combinatorial%20objectives%20in%20SMILe%20naturally%20preserves%20the%0Adiversity%20within%20an%20object%20class%20resulting%20in%20reduced%20forgetting%20when%20subjected%0Ato%20few%20training%20examples.%20Furthermore%2C%20the%20application%20of%20mutual%20information%0Abetween%20the%20already%20learnt%20%28base%29%20and%20newly%20added%20%28novel%29%20objects%20ensures%0Asufficient%20separation%20between%20base%20and%20novel%20classes%2C%20minimizing%20the%20effect%20of%0Aclass%20confusion.%20Experiments%20on%20popular%20FSOD%20benchmarks%2C%20PASCAL-VOC%20and%20MS-COCO%0Ashow%20that%20our%20approach%20generalizes%20to%20State-of-the-Art%20%28SoTA%29%20approaches%0Aimproving%20their%20novel%20class%20performance%20by%20up%20to%205.7%25%20%283.3%20mAP%20points%29%20and%205.4%25%0A%282.6%20mAP%20points%29%20on%20the%2010-shot%20setting%20of%20VOC%20%28split%203%29%20and%2030-shot%20setting%20of%0ACOCO%20datasets%20respectively.%20Our%20experiments%20also%20demonstrate%20better%20retention%0Aof%20base%20class%20performance%20and%20up%20to%202x%20faster%20convergence%20over%20existing%0Aapproaches%20agnostic%20of%20the%20underlying%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02665v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMILe%253A%2520Leveraging%2520Submodular%2520Mutual%2520Information%2520For%2520Robust%2520Few-Shot%250A%2520%2520Object%2520Detection%26entry.906535625%3DAnay%2520Majee%2520and%2520Ryan%2520Sharp%2520and%2520Rishabh%2520Iyer%26entry.1292438233%3D%2520%2520Confusion%2520and%2520forgetting%2520of%2520object%2520classes%2520have%2520been%2520challenges%2520of%2520prime%250Ainterest%2520in%2520Few-Shot%2520Object%2520Detection%2520%2528FSOD%2529.%2520To%2520overcome%2520these%2520pitfalls%2520in%250Ametric%2520learning%2520based%2520FSOD%2520techniques%252C%2520we%2520introduce%2520a%2520novel%2520Submodular%2520Mutual%250AInformation%2520Learning%2520%2528SMILe%2529%2520framework%2520which%2520adopts%2520combinatorial%2520mutual%250Ainformation%2520functions%2520to%2520enforce%2520the%2520creation%2520of%2520tighter%2520and%2520discriminative%250Afeature%2520clusters%2520in%2520FSOD.%2520Our%2520proposed%2520approach%2520generalizes%2520to%2520several%2520existing%250Aapproaches%2520in%2520FSOD%252C%2520agnostic%2520of%2520the%2520backbone%2520architecture%2520demonstrating%250Aelevated%2520performance%2520gains.%2520A%2520paradigm%2520shift%2520from%2520instance%2520based%2520objective%250Afunctions%2520to%2520combinatorial%2520objectives%2520in%2520SMILe%2520naturally%2520preserves%2520the%250Adiversity%2520within%2520an%2520object%2520class%2520resulting%2520in%2520reduced%2520forgetting%2520when%2520subjected%250Ato%2520few%2520training%2520examples.%2520Furthermore%252C%2520the%2520application%2520of%2520mutual%2520information%250Abetween%2520the%2520already%2520learnt%2520%2528base%2529%2520and%2520newly%2520added%2520%2528novel%2529%2520objects%2520ensures%250Asufficient%2520separation%2520between%2520base%2520and%2520novel%2520classes%252C%2520minimizing%2520the%2520effect%2520of%250Aclass%2520confusion.%2520Experiments%2520on%2520popular%2520FSOD%2520benchmarks%252C%2520PASCAL-VOC%2520and%2520MS-COCO%250Ashow%2520that%2520our%2520approach%2520generalizes%2520to%2520State-of-the-Art%2520%2528SoTA%2529%2520approaches%250Aimproving%2520their%2520novel%2520class%2520performance%2520by%2520up%2520to%25205.7%2525%2520%25283.3%2520mAP%2520points%2529%2520and%25205.4%2525%250A%25282.6%2520mAP%2520points%2529%2520on%2520the%252010-shot%2520setting%2520of%2520VOC%2520%2528split%25203%2529%2520and%252030-shot%2520setting%2520of%250ACOCO%2520datasets%2520respectively.%2520Our%2520experiments%2520also%2520demonstrate%2520better%2520retention%250Aof%2520base%2520class%2520performance%2520and%2520up%2520to%25202x%2520faster%2520convergence%2520over%2520existing%250Aapproaches%2520agnostic%2520of%2520the%2520underlying%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02665v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMILe%3A%20Leveraging%20Submodular%20Mutual%20Information%20For%20Robust%20Few-Shot%0A%20%20Object%20Detection&entry.906535625=Anay%20Majee%20and%20Ryan%20Sharp%20and%20Rishabh%20Iyer&entry.1292438233=%20%20Confusion%20and%20forgetting%20of%20object%20classes%20have%20been%20challenges%20of%20prime%0Ainterest%20in%20Few-Shot%20Object%20Detection%20%28FSOD%29.%20To%20overcome%20these%20pitfalls%20in%0Ametric%20learning%20based%20FSOD%20techniques%2C%20we%20introduce%20a%20novel%20Submodular%20Mutual%0AInformation%20Learning%20%28SMILe%29%20framework%20which%20adopts%20combinatorial%20mutual%0Ainformation%20functions%20to%20enforce%20the%20creation%20of%20tighter%20and%20discriminative%0Afeature%20clusters%20in%20FSOD.%20Our%20proposed%20approach%20generalizes%20to%20several%20existing%0Aapproaches%20in%20FSOD%2C%20agnostic%20of%20the%20backbone%20architecture%20demonstrating%0Aelevated%20performance%20gains.%20A%20paradigm%20shift%20from%20instance%20based%20objective%0Afunctions%20to%20combinatorial%20objectives%20in%20SMILe%20naturally%20preserves%20the%0Adiversity%20within%20an%20object%20class%20resulting%20in%20reduced%20forgetting%20when%20subjected%0Ato%20few%20training%20examples.%20Furthermore%2C%20the%20application%20of%20mutual%20information%0Abetween%20the%20already%20learnt%20%28base%29%20and%20newly%20added%20%28novel%29%20objects%20ensures%0Asufficient%20separation%20between%20base%20and%20novel%20classes%2C%20minimizing%20the%20effect%20of%0Aclass%20confusion.%20Experiments%20on%20popular%20FSOD%20benchmarks%2C%20PASCAL-VOC%20and%20MS-COCO%0Ashow%20that%20our%20approach%20generalizes%20to%20State-of-the-Art%20%28SoTA%29%20approaches%0Aimproving%20their%20novel%20class%20performance%20by%20up%20to%205.7%25%20%283.3%20mAP%20points%29%20and%205.4%25%0A%282.6%20mAP%20points%29%20on%20the%2010-shot%20setting%20of%20VOC%20%28split%203%29%20and%2030-shot%20setting%20of%0ACOCO%20datasets%20respectively.%20Our%20experiments%20also%20demonstrate%20better%20retention%0Aof%20base%20class%20performance%20and%20up%20to%202x%20faster%20convergence%20over%20existing%0Aapproaches%20agnostic%20of%20the%20underlying%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02665v2&entry.124074799=Read"},
{"title": "HS3-Bench: A Benchmark and Strong Baseline for Hyperspectral Semantic\n  Segmentation in Driving Scenarios", "author": "Nick Theisen and Robin Bartsch and Dietrich Paulus and Peer Neubert", "abstract": "  Semantic segmentation is an essential step for many vision applications in\norder to understand a scene and the objects within. Recent progress in\nhyperspectral imaging technology enables the application in driving scenarios\nand the hope is that the devices perceptive abilities provide an advantage over\nRGB-cameras. Even though some datasets exist, there is no standard benchmark\navailable to systematically measure progress on this task and evaluate the\nbenefit of hyperspectral data. In this paper, we work towards closing this gap\nby providing the HyperSpectral Semantic Segmentation benchmark (HS3-Bench). It\ncombines annotated hyperspectral images from three driving scenario datasets\nand provides standardized metrics, implementations, and evaluation protocols.\nWe use the benchmark to derive two strong baseline models that surpass the\nprevious state-of-the-art performances with and without pre-training on the\nindividual datasets. Further, our results indicate that the existing\nlearning-based methods benefit more from leveraging additional RGB training\ndata than from leveraging the additional hyperspectral channels. This poses\nimportant questions for future research on hyperspectral imaging for semantic\nsegmentation in driving scenarios. Code to run the benchmark and the strong\nbaseline approaches are available under\nhttps://github.com/nickstheisen/hyperseg.\n", "link": "http://arxiv.org/abs/2409.11205v1", "date": "2024-09-17", "relevancy": 2.2264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HS3-Bench%3A%20A%20Benchmark%20and%20Strong%20Baseline%20for%20Hyperspectral%20Semantic%0A%20%20Segmentation%20in%20Driving%20Scenarios&body=Title%3A%20HS3-Bench%3A%20A%20Benchmark%20and%20Strong%20Baseline%20for%20Hyperspectral%20Semantic%0A%20%20Segmentation%20in%20Driving%20Scenarios%0AAuthor%3A%20Nick%20Theisen%20and%20Robin%20Bartsch%20and%20Dietrich%20Paulus%20and%20Peer%20Neubert%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20an%20essential%20step%20for%20many%20vision%20applications%20in%0Aorder%20to%20understand%20a%20scene%20and%20the%20objects%20within.%20Recent%20progress%20in%0Ahyperspectral%20imaging%20technology%20enables%20the%20application%20in%20driving%20scenarios%0Aand%20the%20hope%20is%20that%20the%20devices%20perceptive%20abilities%20provide%20an%20advantage%20over%0ARGB-cameras.%20Even%20though%20some%20datasets%20exist%2C%20there%20is%20no%20standard%20benchmark%0Aavailable%20to%20systematically%20measure%20progress%20on%20this%20task%20and%20evaluate%20the%0Abenefit%20of%20hyperspectral%20data.%20In%20this%20paper%2C%20we%20work%20towards%20closing%20this%20gap%0Aby%20providing%20the%20HyperSpectral%20Semantic%20Segmentation%20benchmark%20%28HS3-Bench%29.%20It%0Acombines%20annotated%20hyperspectral%20images%20from%20three%20driving%20scenario%20datasets%0Aand%20provides%20standardized%20metrics%2C%20implementations%2C%20and%20evaluation%20protocols.%0AWe%20use%20the%20benchmark%20to%20derive%20two%20strong%20baseline%20models%20that%20surpass%20the%0Aprevious%20state-of-the-art%20performances%20with%20and%20without%20pre-training%20on%20the%0Aindividual%20datasets.%20Further%2C%20our%20results%20indicate%20that%20the%20existing%0Alearning-based%20methods%20benefit%20more%20from%20leveraging%20additional%20RGB%20training%0Adata%20than%20from%20leveraging%20the%20additional%20hyperspectral%20channels.%20This%20poses%0Aimportant%20questions%20for%20future%20research%20on%20hyperspectral%20imaging%20for%20semantic%0Asegmentation%20in%20driving%20scenarios.%20Code%20to%20run%20the%20benchmark%20and%20the%20strong%0Abaseline%20approaches%20are%20available%20under%0Ahttps%3A//github.com/nickstheisen/hyperseg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHS3-Bench%253A%2520A%2520Benchmark%2520and%2520Strong%2520Baseline%2520for%2520Hyperspectral%2520Semantic%250A%2520%2520Segmentation%2520in%2520Driving%2520Scenarios%26entry.906535625%3DNick%2520Theisen%2520and%2520Robin%2520Bartsch%2520and%2520Dietrich%2520Paulus%2520and%2520Peer%2520Neubert%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520an%2520essential%2520step%2520for%2520many%2520vision%2520applications%2520in%250Aorder%2520to%2520understand%2520a%2520scene%2520and%2520the%2520objects%2520within.%2520Recent%2520progress%2520in%250Ahyperspectral%2520imaging%2520technology%2520enables%2520the%2520application%2520in%2520driving%2520scenarios%250Aand%2520the%2520hope%2520is%2520that%2520the%2520devices%2520perceptive%2520abilities%2520provide%2520an%2520advantage%2520over%250ARGB-cameras.%2520Even%2520though%2520some%2520datasets%2520exist%252C%2520there%2520is%2520no%2520standard%2520benchmark%250Aavailable%2520to%2520systematically%2520measure%2520progress%2520on%2520this%2520task%2520and%2520evaluate%2520the%250Abenefit%2520of%2520hyperspectral%2520data.%2520In%2520this%2520paper%252C%2520we%2520work%2520towards%2520closing%2520this%2520gap%250Aby%2520providing%2520the%2520HyperSpectral%2520Semantic%2520Segmentation%2520benchmark%2520%2528HS3-Bench%2529.%2520It%250Acombines%2520annotated%2520hyperspectral%2520images%2520from%2520three%2520driving%2520scenario%2520datasets%250Aand%2520provides%2520standardized%2520metrics%252C%2520implementations%252C%2520and%2520evaluation%2520protocols.%250AWe%2520use%2520the%2520benchmark%2520to%2520derive%2520two%2520strong%2520baseline%2520models%2520that%2520surpass%2520the%250Aprevious%2520state-of-the-art%2520performances%2520with%2520and%2520without%2520pre-training%2520on%2520the%250Aindividual%2520datasets.%2520Further%252C%2520our%2520results%2520indicate%2520that%2520the%2520existing%250Alearning-based%2520methods%2520benefit%2520more%2520from%2520leveraging%2520additional%2520RGB%2520training%250Adata%2520than%2520from%2520leveraging%2520the%2520additional%2520hyperspectral%2520channels.%2520This%2520poses%250Aimportant%2520questions%2520for%2520future%2520research%2520on%2520hyperspectral%2520imaging%2520for%2520semantic%250Asegmentation%2520in%2520driving%2520scenarios.%2520Code%2520to%2520run%2520the%2520benchmark%2520and%2520the%2520strong%250Abaseline%2520approaches%2520are%2520available%2520under%250Ahttps%253A//github.com/nickstheisen/hyperseg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HS3-Bench%3A%20A%20Benchmark%20and%20Strong%20Baseline%20for%20Hyperspectral%20Semantic%0A%20%20Segmentation%20in%20Driving%20Scenarios&entry.906535625=Nick%20Theisen%20and%20Robin%20Bartsch%20and%20Dietrich%20Paulus%20and%20Peer%20Neubert&entry.1292438233=%20%20Semantic%20segmentation%20is%20an%20essential%20step%20for%20many%20vision%20applications%20in%0Aorder%20to%20understand%20a%20scene%20and%20the%20objects%20within.%20Recent%20progress%20in%0Ahyperspectral%20imaging%20technology%20enables%20the%20application%20in%20driving%20scenarios%0Aand%20the%20hope%20is%20that%20the%20devices%20perceptive%20abilities%20provide%20an%20advantage%20over%0ARGB-cameras.%20Even%20though%20some%20datasets%20exist%2C%20there%20is%20no%20standard%20benchmark%0Aavailable%20to%20systematically%20measure%20progress%20on%20this%20task%20and%20evaluate%20the%0Abenefit%20of%20hyperspectral%20data.%20In%20this%20paper%2C%20we%20work%20towards%20closing%20this%20gap%0Aby%20providing%20the%20HyperSpectral%20Semantic%20Segmentation%20benchmark%20%28HS3-Bench%29.%20It%0Acombines%20annotated%20hyperspectral%20images%20from%20three%20driving%20scenario%20datasets%0Aand%20provides%20standardized%20metrics%2C%20implementations%2C%20and%20evaluation%20protocols.%0AWe%20use%20the%20benchmark%20to%20derive%20two%20strong%20baseline%20models%20that%20surpass%20the%0Aprevious%20state-of-the-art%20performances%20with%20and%20without%20pre-training%20on%20the%0Aindividual%20datasets.%20Further%2C%20our%20results%20indicate%20that%20the%20existing%0Alearning-based%20methods%20benefit%20more%20from%20leveraging%20additional%20RGB%20training%0Adata%20than%20from%20leveraging%20the%20additional%20hyperspectral%20channels.%20This%20poses%0Aimportant%20questions%20for%20future%20research%20on%20hyperspectral%20imaging%20for%20semantic%0Asegmentation%20in%20driving%20scenarios.%20Code%20to%20run%20the%20benchmark%20and%20the%20strong%0Abaseline%20approaches%20are%20available%20under%0Ahttps%3A//github.com/nickstheisen/hyperseg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11205v1&entry.124074799=Read"},
{"title": "Backdoor Attacks in Peer-to-Peer Federated Learning", "author": "Georgios Syros and Gokberk Yar and Simona Boboila and Cristina Nita-Rotaru and Alina Oprea", "abstract": "  Most machine learning applications rely on centralized learning processes,\nopening up the risk of exposure of their training datasets. While federated\nlearning (FL) mitigates to some extent these privacy risks, it relies on a\ntrusted aggregation server for training a shared global model. Recently, new\ndistributed learning architectures based on Peer-to-Peer Federated Learning\n(P2PFL) offer advantages in terms of both privacy and reliability. Still, their\nresilience to poisoning attacks during training has not been investigated. In\nthis paper, we propose new backdoor attacks for P2PFL that leverage structural\ngraph properties to select the malicious nodes, and achieve high attack\nsuccess, while remaining stealthy. We evaluate our attacks under various\nrealistic conditions, including multiple graph topologies, limited adversarial\nvisibility of the network, and clients with non-IID data. Finally, we show the\nlimitations of existing defenses adapted from FL and design a new defense that\nsuccessfully mitigates the backdoor attacks, without an impact on model\naccuracy.\n", "link": "http://arxiv.org/abs/2301.09732v4", "date": "2024-09-17", "relevancy": 2.2218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4591}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4472}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdoor%20Attacks%20in%20Peer-to-Peer%20Federated%20Learning&body=Title%3A%20Backdoor%20Attacks%20in%20Peer-to-Peer%20Federated%20Learning%0AAuthor%3A%20Georgios%20Syros%20and%20Gokberk%20Yar%20and%20Simona%20Boboila%20and%20Cristina%20Nita-Rotaru%20and%20Alina%20Oprea%0AAbstract%3A%20%20%20Most%20machine%20learning%20applications%20rely%20on%20centralized%20learning%20processes%2C%0Aopening%20up%20the%20risk%20of%20exposure%20of%20their%20training%20datasets.%20While%20federated%0Alearning%20%28FL%29%20mitigates%20to%20some%20extent%20these%20privacy%20risks%2C%20it%20relies%20on%20a%0Atrusted%20aggregation%20server%20for%20training%20a%20shared%20global%20model.%20Recently%2C%20new%0Adistributed%20learning%20architectures%20based%20on%20Peer-to-Peer%20Federated%20Learning%0A%28P2PFL%29%20offer%20advantages%20in%20terms%20of%20both%20privacy%20and%20reliability.%20Still%2C%20their%0Aresilience%20to%20poisoning%20attacks%20during%20training%20has%20not%20been%20investigated.%20In%0Athis%20paper%2C%20we%20propose%20new%20backdoor%20attacks%20for%20P2PFL%20that%20leverage%20structural%0Agraph%20properties%20to%20select%20the%20malicious%20nodes%2C%20and%20achieve%20high%20attack%0Asuccess%2C%20while%20remaining%20stealthy.%20We%20evaluate%20our%20attacks%20under%20various%0Arealistic%20conditions%2C%20including%20multiple%20graph%20topologies%2C%20limited%20adversarial%0Avisibility%20of%20the%20network%2C%20and%20clients%20with%20non-IID%20data.%20Finally%2C%20we%20show%20the%0Alimitations%20of%20existing%20defenses%20adapted%20from%20FL%20and%20design%20a%20new%20defense%20that%0Asuccessfully%20mitigates%20the%20backdoor%20attacks%2C%20without%20an%20impact%20on%20model%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.09732v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoor%2520Attacks%2520in%2520Peer-to-Peer%2520Federated%2520Learning%26entry.906535625%3DGeorgios%2520Syros%2520and%2520Gokberk%2520Yar%2520and%2520Simona%2520Boboila%2520and%2520Cristina%2520Nita-Rotaru%2520and%2520Alina%2520Oprea%26entry.1292438233%3D%2520%2520Most%2520machine%2520learning%2520applications%2520rely%2520on%2520centralized%2520learning%2520processes%252C%250Aopening%2520up%2520the%2520risk%2520of%2520exposure%2520of%2520their%2520training%2520datasets.%2520While%2520federated%250Alearning%2520%2528FL%2529%2520mitigates%2520to%2520some%2520extent%2520these%2520privacy%2520risks%252C%2520it%2520relies%2520on%2520a%250Atrusted%2520aggregation%2520server%2520for%2520training%2520a%2520shared%2520global%2520model.%2520Recently%252C%2520new%250Adistributed%2520learning%2520architectures%2520based%2520on%2520Peer-to-Peer%2520Federated%2520Learning%250A%2528P2PFL%2529%2520offer%2520advantages%2520in%2520terms%2520of%2520both%2520privacy%2520and%2520reliability.%2520Still%252C%2520their%250Aresilience%2520to%2520poisoning%2520attacks%2520during%2520training%2520has%2520not%2520been%2520investigated.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520new%2520backdoor%2520attacks%2520for%2520P2PFL%2520that%2520leverage%2520structural%250Agraph%2520properties%2520to%2520select%2520the%2520malicious%2520nodes%252C%2520and%2520achieve%2520high%2520attack%250Asuccess%252C%2520while%2520remaining%2520stealthy.%2520We%2520evaluate%2520our%2520attacks%2520under%2520various%250Arealistic%2520conditions%252C%2520including%2520multiple%2520graph%2520topologies%252C%2520limited%2520adversarial%250Avisibility%2520of%2520the%2520network%252C%2520and%2520clients%2520with%2520non-IID%2520data.%2520Finally%252C%2520we%2520show%2520the%250Alimitations%2520of%2520existing%2520defenses%2520adapted%2520from%2520FL%2520and%2520design%2520a%2520new%2520defense%2520that%250Asuccessfully%2520mitigates%2520the%2520backdoor%2520attacks%252C%2520without%2520an%2520impact%2520on%2520model%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.09732v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdoor%20Attacks%20in%20Peer-to-Peer%20Federated%20Learning&entry.906535625=Georgios%20Syros%20and%20Gokberk%20Yar%20and%20Simona%20Boboila%20and%20Cristina%20Nita-Rotaru%20and%20Alina%20Oprea&entry.1292438233=%20%20Most%20machine%20learning%20applications%20rely%20on%20centralized%20learning%20processes%2C%0Aopening%20up%20the%20risk%20of%20exposure%20of%20their%20training%20datasets.%20While%20federated%0Alearning%20%28FL%29%20mitigates%20to%20some%20extent%20these%20privacy%20risks%2C%20it%20relies%20on%20a%0Atrusted%20aggregation%20server%20for%20training%20a%20shared%20global%20model.%20Recently%2C%20new%0Adistributed%20learning%20architectures%20based%20on%20Peer-to-Peer%20Federated%20Learning%0A%28P2PFL%29%20offer%20advantages%20in%20terms%20of%20both%20privacy%20and%20reliability.%20Still%2C%20their%0Aresilience%20to%20poisoning%20attacks%20during%20training%20has%20not%20been%20investigated.%20In%0Athis%20paper%2C%20we%20propose%20new%20backdoor%20attacks%20for%20P2PFL%20that%20leverage%20structural%0Agraph%20properties%20to%20select%20the%20malicious%20nodes%2C%20and%20achieve%20high%20attack%0Asuccess%2C%20while%20remaining%20stealthy.%20We%20evaluate%20our%20attacks%20under%20various%0Arealistic%20conditions%2C%20including%20multiple%20graph%20topologies%2C%20limited%20adversarial%0Avisibility%20of%20the%20network%2C%20and%20clients%20with%20non-IID%20data.%20Finally%2C%20we%20show%20the%0Alimitations%20of%20existing%20defenses%20adapted%20from%20FL%20and%20design%20a%20new%20defense%20that%0Asuccessfully%20mitigates%20the%20backdoor%20attacks%2C%20without%20an%20impact%20on%20model%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.09732v4&entry.124074799=Read"},
{"title": "MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for\n  Legged Robot Contact Perception", "author": "Daniel Butterfield and Sandilya Sai Garimella and Nai-Jen Cheng and Lu Gan", "abstract": "  We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN)\nfor learning-based contact perception. The architecture and connectivity of the\nMI-HGNN are constructed from the robot morphology, in which nodes and edges are\nrobot joints and links, respectively. By incorporating the morphology-informed\nconstraints into a neural network, we improve a learning-based approach using\nmodel-based knowledge. We apply the proposed MI-HGNN to two contact perception\nproblems, and conduct extensive experiments using both real-world and simulated\ndata collected using two quadruped robots. Our experiments demonstrate the\nsuperiority of our method in terms of effectiveness, generalization ability,\nmodel efficiency, and sample efficiency. Our MI-HGNN improved the performance\nof a state-of-the-art model that leverages robot morphological symmetry by 8.4%\nwith only 0.21% of its parameters. Although MI-HGNN is applied to contact\nperception problems for legged robots in this work, it can be seamlessly\napplied to other types of multi-body dynamical systems and has the potential to\nimprove other robot learning frameworks. Our code is made publicly available at\nhttps://github.com/lunarlab-gatech/Morphology-Informed-HGNN.\n", "link": "http://arxiv.org/abs/2409.11146v1", "date": "2024-09-17", "relevancy": 2.2155, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6024}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5672}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MI-HGNN%3A%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20for%0A%20%20Legged%20Robot%20Contact%20Perception&body=Title%3A%20MI-HGNN%3A%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20for%0A%20%20Legged%20Robot%20Contact%20Perception%0AAuthor%3A%20Daniel%20Butterfield%20and%20Sandilya%20Sai%20Garimella%20and%20Nai-Jen%20Cheng%20and%20Lu%20Gan%0AAbstract%3A%20%20%20We%20present%20a%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20%28MI-HGNN%29%0Afor%20learning-based%20contact%20perception.%20The%20architecture%20and%20connectivity%20of%20the%0AMI-HGNN%20are%20constructed%20from%20the%20robot%20morphology%2C%20in%20which%20nodes%20and%20edges%20are%0Arobot%20joints%20and%20links%2C%20respectively.%20By%20incorporating%20the%20morphology-informed%0Aconstraints%20into%20a%20neural%20network%2C%20we%20improve%20a%20learning-based%20approach%20using%0Amodel-based%20knowledge.%20We%20apply%20the%20proposed%20MI-HGNN%20to%20two%20contact%20perception%0Aproblems%2C%20and%20conduct%20extensive%20experiments%20using%20both%20real-world%20and%20simulated%0Adata%20collected%20using%20two%20quadruped%20robots.%20Our%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20terms%20of%20effectiveness%2C%20generalization%20ability%2C%0Amodel%20efficiency%2C%20and%20sample%20efficiency.%20Our%20MI-HGNN%20improved%20the%20performance%0Aof%20a%20state-of-the-art%20model%20that%20leverages%20robot%20morphological%20symmetry%20by%208.4%25%0Awith%20only%200.21%25%20of%20its%20parameters.%20Although%20MI-HGNN%20is%20applied%20to%20contact%0Aperception%20problems%20for%20legged%20robots%20in%20this%20work%2C%20it%20can%20be%20seamlessly%0Aapplied%20to%20other%20types%20of%20multi-body%20dynamical%20systems%20and%20has%20the%20potential%20to%0Aimprove%20other%20robot%20learning%20frameworks.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lunarlab-gatech/Morphology-Informed-HGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMI-HGNN%253A%2520Morphology-Informed%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520for%250A%2520%2520Legged%2520Robot%2520Contact%2520Perception%26entry.906535625%3DDaniel%2520Butterfield%2520and%2520Sandilya%2520Sai%2520Garimella%2520and%2520Nai-Jen%2520Cheng%2520and%2520Lu%2520Gan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520Morphology-Informed%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520%2528MI-HGNN%2529%250Afor%2520learning-based%2520contact%2520perception.%2520The%2520architecture%2520and%2520connectivity%2520of%2520the%250AMI-HGNN%2520are%2520constructed%2520from%2520the%2520robot%2520morphology%252C%2520in%2520which%2520nodes%2520and%2520edges%2520are%250Arobot%2520joints%2520and%2520links%252C%2520respectively.%2520By%2520incorporating%2520the%2520morphology-informed%250Aconstraints%2520into%2520a%2520neural%2520network%252C%2520we%2520improve%2520a%2520learning-based%2520approach%2520using%250Amodel-based%2520knowledge.%2520We%2520apply%2520the%2520proposed%2520MI-HGNN%2520to%2520two%2520contact%2520perception%250Aproblems%252C%2520and%2520conduct%2520extensive%2520experiments%2520using%2520both%2520real-world%2520and%2520simulated%250Adata%2520collected%2520using%2520two%2520quadruped%2520robots.%2520Our%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520in%2520terms%2520of%2520effectiveness%252C%2520generalization%2520ability%252C%250Amodel%2520efficiency%252C%2520and%2520sample%2520efficiency.%2520Our%2520MI-HGNN%2520improved%2520the%2520performance%250Aof%2520a%2520state-of-the-art%2520model%2520that%2520leverages%2520robot%2520morphological%2520symmetry%2520by%25208.4%2525%250Awith%2520only%25200.21%2525%2520of%2520its%2520parameters.%2520Although%2520MI-HGNN%2520is%2520applied%2520to%2520contact%250Aperception%2520problems%2520for%2520legged%2520robots%2520in%2520this%2520work%252C%2520it%2520can%2520be%2520seamlessly%250Aapplied%2520to%2520other%2520types%2520of%2520multi-body%2520dynamical%2520systems%2520and%2520has%2520the%2520potential%2520to%250Aimprove%2520other%2520robot%2520learning%2520frameworks.%2520Our%2520code%2520is%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lunarlab-gatech/Morphology-Informed-HGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MI-HGNN%3A%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20for%0A%20%20Legged%20Robot%20Contact%20Perception&entry.906535625=Daniel%20Butterfield%20and%20Sandilya%20Sai%20Garimella%20and%20Nai-Jen%20Cheng%20and%20Lu%20Gan&entry.1292438233=%20%20We%20present%20a%20Morphology-Informed%20Heterogeneous%20Graph%20Neural%20Network%20%28MI-HGNN%29%0Afor%20learning-based%20contact%20perception.%20The%20architecture%20and%20connectivity%20of%20the%0AMI-HGNN%20are%20constructed%20from%20the%20robot%20morphology%2C%20in%20which%20nodes%20and%20edges%20are%0Arobot%20joints%20and%20links%2C%20respectively.%20By%20incorporating%20the%20morphology-informed%0Aconstraints%20into%20a%20neural%20network%2C%20we%20improve%20a%20learning-based%20approach%20using%0Amodel-based%20knowledge.%20We%20apply%20the%20proposed%20MI-HGNN%20to%20two%20contact%20perception%0Aproblems%2C%20and%20conduct%20extensive%20experiments%20using%20both%20real-world%20and%20simulated%0Adata%20collected%20using%20two%20quadruped%20robots.%20Our%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20terms%20of%20effectiveness%2C%20generalization%20ability%2C%0Amodel%20efficiency%2C%20and%20sample%20efficiency.%20Our%20MI-HGNN%20improved%20the%20performance%0Aof%20a%20state-of-the-art%20model%20that%20leverages%20robot%20morphological%20symmetry%20by%208.4%25%0Awith%20only%200.21%25%20of%20its%20parameters.%20Although%20MI-HGNN%20is%20applied%20to%20contact%0Aperception%20problems%20for%20legged%20robots%20in%20this%20work%2C%20it%20can%20be%20seamlessly%0Aapplied%20to%20other%20types%20of%20multi-body%20dynamical%20systems%20and%20has%20the%20potential%20to%0Aimprove%20other%20robot%20learning%20frameworks.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lunarlab-gatech/Morphology-Informed-HGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11146v1&entry.124074799=Read"},
{"title": "Distributed Perception Aware Safe Leader Follower System via Control\n  Barrier Methods", "author": "Richie R. Suganda and Tony Tran and Miao Pan and Lei Fan and Qin Lin and Bin Hu", "abstract": "  This paper addresses a distributed leader-follower formation control problem\nfor a group of agents, each using a body-fixed camera with a limited field of\nview (FOV) for state estimation. The main challenge arises from the need to\ncoordinate the agents' movements with their cameras' FOV to maintain visibility\nof the leader for accurate and reliable state estimation. To address this\nchallenge, we propose a novel perception-aware distributed leader-follower safe\ncontrol scheme that incorporates FOV limits as state constraints. A Control\nBarrier Function (CBF) based quadratic program is employed to ensure the\nforward invariance of a safety set defined by these constraints. Furthermore,\nnew neural network based and double bounding boxes based estimators, combined\nwith temporal filters, are developed to estimate system states directly from\nreal-time image data, providing consistent performance across various\nenvironments. Comparison results in the Gazebo simulator demonstrate the\neffectiveness and robustness of the proposed framework in two distinct\nenvironments.\n", "link": "http://arxiv.org/abs/2409.11394v1", "date": "2024-09-17", "relevancy": 2.2152, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5721}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Perception%20Aware%20Safe%20Leader%20Follower%20System%20via%20Control%0A%20%20Barrier%20Methods&body=Title%3A%20Distributed%20Perception%20Aware%20Safe%20Leader%20Follower%20System%20via%20Control%0A%20%20Barrier%20Methods%0AAuthor%3A%20Richie%20R.%20Suganda%20and%20Tony%20Tran%20and%20Miao%20Pan%20and%20Lei%20Fan%20and%20Qin%20Lin%20and%20Bin%20Hu%0AAbstract%3A%20%20%20This%20paper%20addresses%20a%20distributed%20leader-follower%20formation%20control%20problem%0Afor%20a%20group%20of%20agents%2C%20each%20using%20a%20body-fixed%20camera%20with%20a%20limited%20field%20of%0Aview%20%28FOV%29%20for%20state%20estimation.%20The%20main%20challenge%20arises%20from%20the%20need%20to%0Acoordinate%20the%20agents%27%20movements%20with%20their%20cameras%27%20FOV%20to%20maintain%20visibility%0Aof%20the%20leader%20for%20accurate%20and%20reliable%20state%20estimation.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20novel%20perception-aware%20distributed%20leader-follower%20safe%0Acontrol%20scheme%20that%20incorporates%20FOV%20limits%20as%20state%20constraints.%20A%20Control%0ABarrier%20Function%20%28CBF%29%20based%20quadratic%20program%20is%20employed%20to%20ensure%20the%0Aforward%20invariance%20of%20a%20safety%20set%20defined%20by%20these%20constraints.%20Furthermore%2C%0Anew%20neural%20network%20based%20and%20double%20bounding%20boxes%20based%20estimators%2C%20combined%0Awith%20temporal%20filters%2C%20are%20developed%20to%20estimate%20system%20states%20directly%20from%0Areal-time%20image%20data%2C%20providing%20consistent%20performance%20across%20various%0Aenvironments.%20Comparison%20results%20in%20the%20Gazebo%20simulator%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20the%20proposed%20framework%20in%20two%20distinct%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Perception%2520Aware%2520Safe%2520Leader%2520Follower%2520System%2520via%2520Control%250A%2520%2520Barrier%2520Methods%26entry.906535625%3DRichie%2520R.%2520Suganda%2520and%2520Tony%2520Tran%2520and%2520Miao%2520Pan%2520and%2520Lei%2520Fan%2520and%2520Qin%2520Lin%2520and%2520Bin%2520Hu%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520a%2520distributed%2520leader-follower%2520formation%2520control%2520problem%250Afor%2520a%2520group%2520of%2520agents%252C%2520each%2520using%2520a%2520body-fixed%2520camera%2520with%2520a%2520limited%2520field%2520of%250Aview%2520%2528FOV%2529%2520for%2520state%2520estimation.%2520The%2520main%2520challenge%2520arises%2520from%2520the%2520need%2520to%250Acoordinate%2520the%2520agents%2527%2520movements%2520with%2520their%2520cameras%2527%2520FOV%2520to%2520maintain%2520visibility%250Aof%2520the%2520leader%2520for%2520accurate%2520and%2520reliable%2520state%2520estimation.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520novel%2520perception-aware%2520distributed%2520leader-follower%2520safe%250Acontrol%2520scheme%2520that%2520incorporates%2520FOV%2520limits%2520as%2520state%2520constraints.%2520A%2520Control%250ABarrier%2520Function%2520%2528CBF%2529%2520based%2520quadratic%2520program%2520is%2520employed%2520to%2520ensure%2520the%250Aforward%2520invariance%2520of%2520a%2520safety%2520set%2520defined%2520by%2520these%2520constraints.%2520Furthermore%252C%250Anew%2520neural%2520network%2520based%2520and%2520double%2520bounding%2520boxes%2520based%2520estimators%252C%2520combined%250Awith%2520temporal%2520filters%252C%2520are%2520developed%2520to%2520estimate%2520system%2520states%2520directly%2520from%250Areal-time%2520image%2520data%252C%2520providing%2520consistent%2520performance%2520across%2520various%250Aenvironments.%2520Comparison%2520results%2520in%2520the%2520Gazebo%2520simulator%2520demonstrate%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520framework%2520in%2520two%2520distinct%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Perception%20Aware%20Safe%20Leader%20Follower%20System%20via%20Control%0A%20%20Barrier%20Methods&entry.906535625=Richie%20R.%20Suganda%20and%20Tony%20Tran%20and%20Miao%20Pan%20and%20Lei%20Fan%20and%20Qin%20Lin%20and%20Bin%20Hu&entry.1292438233=%20%20This%20paper%20addresses%20a%20distributed%20leader-follower%20formation%20control%20problem%0Afor%20a%20group%20of%20agents%2C%20each%20using%20a%20body-fixed%20camera%20with%20a%20limited%20field%20of%0Aview%20%28FOV%29%20for%20state%20estimation.%20The%20main%20challenge%20arises%20from%20the%20need%20to%0Acoordinate%20the%20agents%27%20movements%20with%20their%20cameras%27%20FOV%20to%20maintain%20visibility%0Aof%20the%20leader%20for%20accurate%20and%20reliable%20state%20estimation.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20novel%20perception-aware%20distributed%20leader-follower%20safe%0Acontrol%20scheme%20that%20incorporates%20FOV%20limits%20as%20state%20constraints.%20A%20Control%0ABarrier%20Function%20%28CBF%29%20based%20quadratic%20program%20is%20employed%20to%20ensure%20the%0Aforward%20invariance%20of%20a%20safety%20set%20defined%20by%20these%20constraints.%20Furthermore%2C%0Anew%20neural%20network%20based%20and%20double%20bounding%20boxes%20based%20estimators%2C%20combined%0Awith%20temporal%20filters%2C%20are%20developed%20to%20estimate%20system%20states%20directly%20from%0Areal-time%20image%20data%2C%20providing%20consistent%20performance%20across%20various%0Aenvironments.%20Comparison%20results%20in%20the%20Gazebo%20simulator%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20the%20proposed%20framework%20in%20two%20distinct%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11394v1&entry.124074799=Read"},
{"title": "Few-Shot Domain Adaptation for Learned Image Compression", "author": "Tianyu Zhang and Haotian Zhang and Yuqi Li and Li Li and Dong Liu", "abstract": "  Learned image compression (LIC) has achieved state-of-the-art rate-distortion\nperformance, deemed promising for next-generation image compression techniques.\nHowever, pre-trained LIC models usually suffer from significant performance\ndegradation when applied to out-of-training-domain images, implying their poor\ngeneralization capabilities. To tackle this problem, we propose a few-shot\ndomain adaptation method for LIC by integrating plug-and-play adapters into\npre-trained models. Drawing inspiration from the analogy between latent\nchannels and frequency components, we examine domain gaps in LIC and observe\nthat out-of-training-domain images disrupt pre-trained channel-wise\ndecomposition. Consequently, we introduce a method for channel-wise\nre-allocation using convolution-based adapters and low-rank adapters, which are\nlightweight and compatible to mainstream LIC schemes. Extensive experiments\nacross multiple domains and multiple representative LIC schemes demonstrate\nthat our method significantly enhances pre-trained models, achieving comparable\nperformance to H.266/VVC intra coding with merely 25 target-domain samples.\nAdditionally, our method matches the performance of full-model finetune while\ntransmitting fewer than $2\\%$ of the parameters.\n", "link": "http://arxiv.org/abs/2409.11111v1", "date": "2024-09-17", "relevancy": 2.1994, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Domain%20Adaptation%20for%20Learned%20Image%20Compression&body=Title%3A%20Few-Shot%20Domain%20Adaptation%20for%20Learned%20Image%20Compression%0AAuthor%3A%20Tianyu%20Zhang%20and%20Haotian%20Zhang%20and%20Yuqi%20Li%20and%20Li%20Li%20and%20Dong%20Liu%0AAbstract%3A%20%20%20Learned%20image%20compression%20%28LIC%29%20has%20achieved%20state-of-the-art%20rate-distortion%0Aperformance%2C%20deemed%20promising%20for%20next-generation%20image%20compression%20techniques.%0AHowever%2C%20pre-trained%20LIC%20models%20usually%20suffer%20from%20significant%20performance%0Adegradation%20when%20applied%20to%20out-of-training-domain%20images%2C%20implying%20their%20poor%0Ageneralization%20capabilities.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20few-shot%0Adomain%20adaptation%20method%20for%20LIC%20by%20integrating%20plug-and-play%20adapters%20into%0Apre-trained%20models.%20Drawing%20inspiration%20from%20the%20analogy%20between%20latent%0Achannels%20and%20frequency%20components%2C%20we%20examine%20domain%20gaps%20in%20LIC%20and%20observe%0Athat%20out-of-training-domain%20images%20disrupt%20pre-trained%20channel-wise%0Adecomposition.%20Consequently%2C%20we%20introduce%20a%20method%20for%20channel-wise%0Are-allocation%20using%20convolution-based%20adapters%20and%20low-rank%20adapters%2C%20which%20are%0Alightweight%20and%20compatible%20to%20mainstream%20LIC%20schemes.%20Extensive%20experiments%0Aacross%20multiple%20domains%20and%20multiple%20representative%20LIC%20schemes%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20pre-trained%20models%2C%20achieving%20comparable%0Aperformance%20to%20H.266/VVC%20intra%20coding%20with%20merely%2025%20target-domain%20samples.%0AAdditionally%2C%20our%20method%20matches%20the%20performance%20of%20full-model%20finetune%20while%0Atransmitting%20fewer%20than%20%242%5C%25%24%20of%20the%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Domain%2520Adaptation%2520for%2520Learned%2520Image%2520Compression%26entry.906535625%3DTianyu%2520Zhang%2520and%2520Haotian%2520Zhang%2520and%2520Yuqi%2520Li%2520and%2520Li%2520Li%2520and%2520Dong%2520Liu%26entry.1292438233%3D%2520%2520Learned%2520image%2520compression%2520%2528LIC%2529%2520has%2520achieved%2520state-of-the-art%2520rate-distortion%250Aperformance%252C%2520deemed%2520promising%2520for%2520next-generation%2520image%2520compression%2520techniques.%250AHowever%252C%2520pre-trained%2520LIC%2520models%2520usually%2520suffer%2520from%2520significant%2520performance%250Adegradation%2520when%2520applied%2520to%2520out-of-training-domain%2520images%252C%2520implying%2520their%2520poor%250Ageneralization%2520capabilities.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520few-shot%250Adomain%2520adaptation%2520method%2520for%2520LIC%2520by%2520integrating%2520plug-and-play%2520adapters%2520into%250Apre-trained%2520models.%2520Drawing%2520inspiration%2520from%2520the%2520analogy%2520between%2520latent%250Achannels%2520and%2520frequency%2520components%252C%2520we%2520examine%2520domain%2520gaps%2520in%2520LIC%2520and%2520observe%250Athat%2520out-of-training-domain%2520images%2520disrupt%2520pre-trained%2520channel-wise%250Adecomposition.%2520Consequently%252C%2520we%2520introduce%2520a%2520method%2520for%2520channel-wise%250Are-allocation%2520using%2520convolution-based%2520adapters%2520and%2520low-rank%2520adapters%252C%2520which%2520are%250Alightweight%2520and%2520compatible%2520to%2520mainstream%2520LIC%2520schemes.%2520Extensive%2520experiments%250Aacross%2520multiple%2520domains%2520and%2520multiple%2520representative%2520LIC%2520schemes%2520demonstrate%250Athat%2520our%2520method%2520significantly%2520enhances%2520pre-trained%2520models%252C%2520achieving%2520comparable%250Aperformance%2520to%2520H.266/VVC%2520intra%2520coding%2520with%2520merely%252025%2520target-domain%2520samples.%250AAdditionally%252C%2520our%2520method%2520matches%2520the%2520performance%2520of%2520full-model%2520finetune%2520while%250Atransmitting%2520fewer%2520than%2520%25242%255C%2525%2524%2520of%2520the%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Domain%20Adaptation%20for%20Learned%20Image%20Compression&entry.906535625=Tianyu%20Zhang%20and%20Haotian%20Zhang%20and%20Yuqi%20Li%20and%20Li%20Li%20and%20Dong%20Liu&entry.1292438233=%20%20Learned%20image%20compression%20%28LIC%29%20has%20achieved%20state-of-the-art%20rate-distortion%0Aperformance%2C%20deemed%20promising%20for%20next-generation%20image%20compression%20techniques.%0AHowever%2C%20pre-trained%20LIC%20models%20usually%20suffer%20from%20significant%20performance%0Adegradation%20when%20applied%20to%20out-of-training-domain%20images%2C%20implying%20their%20poor%0Ageneralization%20capabilities.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20few-shot%0Adomain%20adaptation%20method%20for%20LIC%20by%20integrating%20plug-and-play%20adapters%20into%0Apre-trained%20models.%20Drawing%20inspiration%20from%20the%20analogy%20between%20latent%0Achannels%20and%20frequency%20components%2C%20we%20examine%20domain%20gaps%20in%20LIC%20and%20observe%0Athat%20out-of-training-domain%20images%20disrupt%20pre-trained%20channel-wise%0Adecomposition.%20Consequently%2C%20we%20introduce%20a%20method%20for%20channel-wise%0Are-allocation%20using%20convolution-based%20adapters%20and%20low-rank%20adapters%2C%20which%20are%0Alightweight%20and%20compatible%20to%20mainstream%20LIC%20schemes.%20Extensive%20experiments%0Aacross%20multiple%20domains%20and%20multiple%20representative%20LIC%20schemes%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20pre-trained%20models%2C%20achieving%20comparable%0Aperformance%20to%20H.266/VVC%20intra%20coding%20with%20merely%2025%20target-domain%20samples.%0AAdditionally%2C%20our%20method%20matches%20the%20performance%20of%20full-model%20finetune%20while%0Atransmitting%20fewer%20than%20%242%5C%25%24%20of%20the%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11111v1&entry.124074799=Read"},
{"title": "Down-Sampling Inter-Layer Adapter for Parameter and Computation\n  Efficient Ultra-Fine-Grained Image Recognition", "author": "Edwin Arkel Rios and Femiloye Oyerinde and Min-Chun Hu and Bo-Cheng Lai", "abstract": "  Ultra-fine-grained image recognition (UFGIR) categorizes objects with\nextremely small differences between classes, such as distinguishing between\ncultivars within the same species, as opposed to species-level classification\nin fine-grained image recognition (FGIR). The difficulty of this task is\nexacerbated due to the scarcity of samples per category. To tackle these\nchallenges we introduce a novel approach employing down-sampling inter-layer\nadapters in a parameter-efficient setting, where the backbone parameters are\nfrozen and we only fine-tune a small set of additional modules. By integrating\ndual-branch down-sampling, we significantly reduce the number of parameters and\nfloating-point operations (FLOPs) required, making our method highly efficient.\nComprehensive experiments on ten datasets demonstrate that our approach obtains\noutstanding accuracy-cost performance, highlighting its potential for practical\napplications in resource-constrained environments. In particular, our method\nincreases the average accuracy by at least 6.8\\% compared to other methods in\nthe parameter-efficient setting while requiring at least 123x less trainable\nparameters compared to current state-of-the-art UFGIR methods and reducing the\nFLOPs by 30\\% in average compared to other methods.\n", "link": "http://arxiv.org/abs/2409.11051v1", "date": "2024-09-17", "relevancy": 2.1925, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5839}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.541}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Down-Sampling%20Inter-Layer%20Adapter%20for%20Parameter%20and%20Computation%0A%20%20Efficient%20Ultra-Fine-Grained%20Image%20Recognition&body=Title%3A%20Down-Sampling%20Inter-Layer%20Adapter%20for%20Parameter%20and%20Computation%0A%20%20Efficient%20Ultra-Fine-Grained%20Image%20Recognition%0AAuthor%3A%20Edwin%20Arkel%20Rios%20and%20Femiloye%20Oyerinde%20and%20Min-Chun%20Hu%20and%20Bo-Cheng%20Lai%0AAbstract%3A%20%20%20Ultra-fine-grained%20image%20recognition%20%28UFGIR%29%20categorizes%20objects%20with%0Aextremely%20small%20differences%20between%20classes%2C%20such%20as%20distinguishing%20between%0Acultivars%20within%20the%20same%20species%2C%20as%20opposed%20to%20species-level%20classification%0Ain%20fine-grained%20image%20recognition%20%28FGIR%29.%20The%20difficulty%20of%20this%20task%20is%0Aexacerbated%20due%20to%20the%20scarcity%20of%20samples%20per%20category.%20To%20tackle%20these%0Achallenges%20we%20introduce%20a%20novel%20approach%20employing%20down-sampling%20inter-layer%0Aadapters%20in%20a%20parameter-efficient%20setting%2C%20where%20the%20backbone%20parameters%20are%0Afrozen%20and%20we%20only%20fine-tune%20a%20small%20set%20of%20additional%20modules.%20By%20integrating%0Adual-branch%20down-sampling%2C%20we%20significantly%20reduce%20the%20number%20of%20parameters%20and%0Afloating-point%20operations%20%28FLOPs%29%20required%2C%20making%20our%20method%20highly%20efficient.%0AComprehensive%20experiments%20on%20ten%20datasets%20demonstrate%20that%20our%20approach%20obtains%0Aoutstanding%20accuracy-cost%20performance%2C%20highlighting%20its%20potential%20for%20practical%0Aapplications%20in%20resource-constrained%20environments.%20In%20particular%2C%20our%20method%0Aincreases%20the%20average%20accuracy%20by%20at%20least%206.8%5C%25%20compared%20to%20other%20methods%20in%0Athe%20parameter-efficient%20setting%20while%20requiring%20at%20least%20123x%20less%20trainable%0Aparameters%20compared%20to%20current%20state-of-the-art%20UFGIR%20methods%20and%20reducing%20the%0AFLOPs%20by%2030%5C%25%20in%20average%20compared%20to%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDown-Sampling%2520Inter-Layer%2520Adapter%2520for%2520Parameter%2520and%2520Computation%250A%2520%2520Efficient%2520Ultra-Fine-Grained%2520Image%2520Recognition%26entry.906535625%3DEdwin%2520Arkel%2520Rios%2520and%2520Femiloye%2520Oyerinde%2520and%2520Min-Chun%2520Hu%2520and%2520Bo-Cheng%2520Lai%26entry.1292438233%3D%2520%2520Ultra-fine-grained%2520image%2520recognition%2520%2528UFGIR%2529%2520categorizes%2520objects%2520with%250Aextremely%2520small%2520differences%2520between%2520classes%252C%2520such%2520as%2520distinguishing%2520between%250Acultivars%2520within%2520the%2520same%2520species%252C%2520as%2520opposed%2520to%2520species-level%2520classification%250Ain%2520fine-grained%2520image%2520recognition%2520%2528FGIR%2529.%2520The%2520difficulty%2520of%2520this%2520task%2520is%250Aexacerbated%2520due%2520to%2520the%2520scarcity%2520of%2520samples%2520per%2520category.%2520To%2520tackle%2520these%250Achallenges%2520we%2520introduce%2520a%2520novel%2520approach%2520employing%2520down-sampling%2520inter-layer%250Aadapters%2520in%2520a%2520parameter-efficient%2520setting%252C%2520where%2520the%2520backbone%2520parameters%2520are%250Afrozen%2520and%2520we%2520only%2520fine-tune%2520a%2520small%2520set%2520of%2520additional%2520modules.%2520By%2520integrating%250Adual-branch%2520down-sampling%252C%2520we%2520significantly%2520reduce%2520the%2520number%2520of%2520parameters%2520and%250Afloating-point%2520operations%2520%2528FLOPs%2529%2520required%252C%2520making%2520our%2520method%2520highly%2520efficient.%250AComprehensive%2520experiments%2520on%2520ten%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520obtains%250Aoutstanding%2520accuracy-cost%2520performance%252C%2520highlighting%2520its%2520potential%2520for%2520practical%250Aapplications%2520in%2520resource-constrained%2520environments.%2520In%2520particular%252C%2520our%2520method%250Aincreases%2520the%2520average%2520accuracy%2520by%2520at%2520least%25206.8%255C%2525%2520compared%2520to%2520other%2520methods%2520in%250Athe%2520parameter-efficient%2520setting%2520while%2520requiring%2520at%2520least%2520123x%2520less%2520trainable%250Aparameters%2520compared%2520to%2520current%2520state-of-the-art%2520UFGIR%2520methods%2520and%2520reducing%2520the%250AFLOPs%2520by%252030%255C%2525%2520in%2520average%2520compared%2520to%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Down-Sampling%20Inter-Layer%20Adapter%20for%20Parameter%20and%20Computation%0A%20%20Efficient%20Ultra-Fine-Grained%20Image%20Recognition&entry.906535625=Edwin%20Arkel%20Rios%20and%20Femiloye%20Oyerinde%20and%20Min-Chun%20Hu%20and%20Bo-Cheng%20Lai&entry.1292438233=%20%20Ultra-fine-grained%20image%20recognition%20%28UFGIR%29%20categorizes%20objects%20with%0Aextremely%20small%20differences%20between%20classes%2C%20such%20as%20distinguishing%20between%0Acultivars%20within%20the%20same%20species%2C%20as%20opposed%20to%20species-level%20classification%0Ain%20fine-grained%20image%20recognition%20%28FGIR%29.%20The%20difficulty%20of%20this%20task%20is%0Aexacerbated%20due%20to%20the%20scarcity%20of%20samples%20per%20category.%20To%20tackle%20these%0Achallenges%20we%20introduce%20a%20novel%20approach%20employing%20down-sampling%20inter-layer%0Aadapters%20in%20a%20parameter-efficient%20setting%2C%20where%20the%20backbone%20parameters%20are%0Afrozen%20and%20we%20only%20fine-tune%20a%20small%20set%20of%20additional%20modules.%20By%20integrating%0Adual-branch%20down-sampling%2C%20we%20significantly%20reduce%20the%20number%20of%20parameters%20and%0Afloating-point%20operations%20%28FLOPs%29%20required%2C%20making%20our%20method%20highly%20efficient.%0AComprehensive%20experiments%20on%20ten%20datasets%20demonstrate%20that%20our%20approach%20obtains%0Aoutstanding%20accuracy-cost%20performance%2C%20highlighting%20its%20potential%20for%20practical%0Aapplications%20in%20resource-constrained%20environments.%20In%20particular%2C%20our%20method%0Aincreases%20the%20average%20accuracy%20by%20at%20least%206.8%5C%25%20compared%20to%20other%20methods%20in%0Athe%20parameter-efficient%20setting%20while%20requiring%20at%20least%20123x%20less%20trainable%0Aparameters%20compared%20to%20current%20state-of-the-art%20UFGIR%20methods%20and%20reducing%20the%0AFLOPs%20by%2030%5C%25%20in%20average%20compared%20to%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11051v1&entry.124074799=Read"},
{"title": "LPT++: Efficient Training on Mixture of Long-tailed Experts", "author": "Bowen Dong and Pan Zhou and Wangmeng Zuo", "abstract": "  We introduce LPT++, a comprehensive framework for long-tailed classification\nthat combines parameter-efficient fine-tuning (PEFT) with a learnable model\nensemble. LPT++ enhances frozen Vision Transformers (ViTs) through the\nintegration of three core components. The first is a universal long-tailed\nadaptation module, which aggregates long-tailed prompts and visual adapters to\nadapt the pretrained model to the target domain, meanwhile improving its\ndiscriminative ability. The second is the mixture of long-tailed experts\nframework with a mixture-of-experts (MoE) scorer, which adaptively calculates\nreweighting coefficients for confidence scores from both visual-only and\nvisual-language (VL) model experts to generate more accurate predictions.\nFinally, LPT++ employs a three-phase training framework, wherein each critical\nmodule is learned separately, resulting in a stable and effective long-tailed\nclassification training paradigm. Besides, we also propose the simple version\nof LPT++ namely LPT, which only integrates visual-only pretrained ViT and\nlong-tailed prompts to formulate a single model method. LPT can clearly\nillustrate how long-tailed prompts works meanwhile achieving comparable\nperformance without VL pretrained models. Experiments show that, with only ~1%\nextra trainable parameters, LPT++ achieves comparable accuracy against all the\ncounterparts.\n", "link": "http://arxiv.org/abs/2409.11323v1", "date": "2024-09-17", "relevancy": 2.1896, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5354}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPT%2B%2B%3A%20Efficient%20Training%20on%20Mixture%20of%20Long-tailed%20Experts&body=Title%3A%20LPT%2B%2B%3A%20Efficient%20Training%20on%20Mixture%20of%20Long-tailed%20Experts%0AAuthor%3A%20Bowen%20Dong%20and%20Pan%20Zhou%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20We%20introduce%20LPT%2B%2B%2C%20a%20comprehensive%20framework%20for%20long-tailed%20classification%0Athat%20combines%20parameter-efficient%20fine-tuning%20%28PEFT%29%20with%20a%20learnable%20model%0Aensemble.%20LPT%2B%2B%20enhances%20frozen%20Vision%20Transformers%20%28ViTs%29%20through%20the%0Aintegration%20of%20three%20core%20components.%20The%20first%20is%20a%20universal%20long-tailed%0Aadaptation%20module%2C%20which%20aggregates%20long-tailed%20prompts%20and%20visual%20adapters%20to%0Aadapt%20the%20pretrained%20model%20to%20the%20target%20domain%2C%20meanwhile%20improving%20its%0Adiscriminative%20ability.%20The%20second%20is%20the%20mixture%20of%20long-tailed%20experts%0Aframework%20with%20a%20mixture-of-experts%20%28MoE%29%20scorer%2C%20which%20adaptively%20calculates%0Areweighting%20coefficients%20for%20confidence%20scores%20from%20both%20visual-only%20and%0Avisual-language%20%28VL%29%20model%20experts%20to%20generate%20more%20accurate%20predictions.%0AFinally%2C%20LPT%2B%2B%20employs%20a%20three-phase%20training%20framework%2C%20wherein%20each%20critical%0Amodule%20is%20learned%20separately%2C%20resulting%20in%20a%20stable%20and%20effective%20long-tailed%0Aclassification%20training%20paradigm.%20Besides%2C%20we%20also%20propose%20the%20simple%20version%0Aof%20LPT%2B%2B%20namely%20LPT%2C%20which%20only%20integrates%20visual-only%20pretrained%20ViT%20and%0Along-tailed%20prompts%20to%20formulate%20a%20single%20model%20method.%20LPT%20can%20clearly%0Aillustrate%20how%20long-tailed%20prompts%20works%20meanwhile%20achieving%20comparable%0Aperformance%20without%20VL%20pretrained%20models.%20Experiments%20show%20that%2C%20with%20only%20~1%25%0Aextra%20trainable%20parameters%2C%20LPT%2B%2B%20achieves%20comparable%20accuracy%20against%20all%20the%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPT%252B%252B%253A%2520Efficient%2520Training%2520on%2520Mixture%2520of%2520Long-tailed%2520Experts%26entry.906535625%3DBowen%2520Dong%2520and%2520Pan%2520Zhou%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520We%2520introduce%2520LPT%252B%252B%252C%2520a%2520comprehensive%2520framework%2520for%2520long-tailed%2520classification%250Athat%2520combines%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520with%2520a%2520learnable%2520model%250Aensemble.%2520LPT%252B%252B%2520enhances%2520frozen%2520Vision%2520Transformers%2520%2528ViTs%2529%2520through%2520the%250Aintegration%2520of%2520three%2520core%2520components.%2520The%2520first%2520is%2520a%2520universal%2520long-tailed%250Aadaptation%2520module%252C%2520which%2520aggregates%2520long-tailed%2520prompts%2520and%2520visual%2520adapters%2520to%250Aadapt%2520the%2520pretrained%2520model%2520to%2520the%2520target%2520domain%252C%2520meanwhile%2520improving%2520its%250Adiscriminative%2520ability.%2520The%2520second%2520is%2520the%2520mixture%2520of%2520long-tailed%2520experts%250Aframework%2520with%2520a%2520mixture-of-experts%2520%2528MoE%2529%2520scorer%252C%2520which%2520adaptively%2520calculates%250Areweighting%2520coefficients%2520for%2520confidence%2520scores%2520from%2520both%2520visual-only%2520and%250Avisual-language%2520%2528VL%2529%2520model%2520experts%2520to%2520generate%2520more%2520accurate%2520predictions.%250AFinally%252C%2520LPT%252B%252B%2520employs%2520a%2520three-phase%2520training%2520framework%252C%2520wherein%2520each%2520critical%250Amodule%2520is%2520learned%2520separately%252C%2520resulting%2520in%2520a%2520stable%2520and%2520effective%2520long-tailed%250Aclassification%2520training%2520paradigm.%2520Besides%252C%2520we%2520also%2520propose%2520the%2520simple%2520version%250Aof%2520LPT%252B%252B%2520namely%2520LPT%252C%2520which%2520only%2520integrates%2520visual-only%2520pretrained%2520ViT%2520and%250Along-tailed%2520prompts%2520to%2520formulate%2520a%2520single%2520model%2520method.%2520LPT%2520can%2520clearly%250Aillustrate%2520how%2520long-tailed%2520prompts%2520works%2520meanwhile%2520achieving%2520comparable%250Aperformance%2520without%2520VL%2520pretrained%2520models.%2520Experiments%2520show%2520that%252C%2520with%2520only%2520~1%2525%250Aextra%2520trainable%2520parameters%252C%2520LPT%252B%252B%2520achieves%2520comparable%2520accuracy%2520against%2520all%2520the%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPT%2B%2B%3A%20Efficient%20Training%20on%20Mixture%20of%20Long-tailed%20Experts&entry.906535625=Bowen%20Dong%20and%20Pan%20Zhou%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20We%20introduce%20LPT%2B%2B%2C%20a%20comprehensive%20framework%20for%20long-tailed%20classification%0Athat%20combines%20parameter-efficient%20fine-tuning%20%28PEFT%29%20with%20a%20learnable%20model%0Aensemble.%20LPT%2B%2B%20enhances%20frozen%20Vision%20Transformers%20%28ViTs%29%20through%20the%0Aintegration%20of%20three%20core%20components.%20The%20first%20is%20a%20universal%20long-tailed%0Aadaptation%20module%2C%20which%20aggregates%20long-tailed%20prompts%20and%20visual%20adapters%20to%0Aadapt%20the%20pretrained%20model%20to%20the%20target%20domain%2C%20meanwhile%20improving%20its%0Adiscriminative%20ability.%20The%20second%20is%20the%20mixture%20of%20long-tailed%20experts%0Aframework%20with%20a%20mixture-of-experts%20%28MoE%29%20scorer%2C%20which%20adaptively%20calculates%0Areweighting%20coefficients%20for%20confidence%20scores%20from%20both%20visual-only%20and%0Avisual-language%20%28VL%29%20model%20experts%20to%20generate%20more%20accurate%20predictions.%0AFinally%2C%20LPT%2B%2B%20employs%20a%20three-phase%20training%20framework%2C%20wherein%20each%20critical%0Amodule%20is%20learned%20separately%2C%20resulting%20in%20a%20stable%20and%20effective%20long-tailed%0Aclassification%20training%20paradigm.%20Besides%2C%20we%20also%20propose%20the%20simple%20version%0Aof%20LPT%2B%2B%20namely%20LPT%2C%20which%20only%20integrates%20visual-only%20pretrained%20ViT%20and%0Along-tailed%20prompts%20to%20formulate%20a%20single%20model%20method.%20LPT%20can%20clearly%0Aillustrate%20how%20long-tailed%20prompts%20works%20meanwhile%20achieving%20comparable%0Aperformance%20without%20VL%20pretrained%20models.%20Experiments%20show%20that%2C%20with%20only%20~1%25%0Aextra%20trainable%20parameters%2C%20LPT%2B%2B%20achieves%20comparable%20accuracy%20against%20all%20the%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11323v1&entry.124074799=Read"},
{"title": "ShapeAug++: More Realistic Shape Augmentation for Event Data", "author": "Katharina Bendig and Ren\u00e9 Schuster and Didier Stricker", "abstract": "  The novel Dynamic Vision Sensors (DVSs) gained a great amount of attention\nrecently as they are superior compared to RGB cameras in terms of latency,\ndynamic range and energy consumption. This is particularly of interest for\nautonomous applications since event cameras are able to alleviate motion blur\nand allow for night vision. One challenge in real-world autonomous settings is\nocclusion where foreground objects hinder the view on traffic participants in\nthe background. The ShapeAug method addresses this problem by using simulated\nevents resulting from objects moving on linear paths for event data\naugmentation. However, the shapes and movements lack complexity, making the\nsimulation fail to resemble the behavior of objects in the real world.\nTherefore in this paper, we propose ShapeAug++, an extended version of ShapeAug\nwhich involves randomly generated polygons as well as curved movements. We show\nthe superiority of our method on multiple DVS classification datasets,\nimproving the top-1 accuracy by up to 3.7% compared to ShapeAug.\n", "link": "http://arxiv.org/abs/2409.11075v1", "date": "2024-09-17", "relevancy": 2.1811, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeAug%2B%2B%3A%20More%20Realistic%20Shape%20Augmentation%20for%20Event%20Data&body=Title%3A%20ShapeAug%2B%2B%3A%20More%20Realistic%20Shape%20Augmentation%20for%20Event%20Data%0AAuthor%3A%20Katharina%20Bendig%20and%20Ren%C3%A9%20Schuster%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20The%20novel%20Dynamic%20Vision%20Sensors%20%28DVSs%29%20gained%20a%20great%20amount%20of%20attention%0Arecently%20as%20they%20are%20superior%20compared%20to%20RGB%20cameras%20in%20terms%20of%20latency%2C%0Adynamic%20range%20and%20energy%20consumption.%20This%20is%20particularly%20of%20interest%20for%0Aautonomous%20applications%20since%20event%20cameras%20are%20able%20to%20alleviate%20motion%20blur%0Aand%20allow%20for%20night%20vision.%20One%20challenge%20in%20real-world%20autonomous%20settings%20is%0Aocclusion%20where%20foreground%20objects%20hinder%20the%20view%20on%20traffic%20participants%20in%0Athe%20background.%20The%20ShapeAug%20method%20addresses%20this%20problem%20by%20using%20simulated%0Aevents%20resulting%20from%20objects%20moving%20on%20linear%20paths%20for%20event%20data%0Aaugmentation.%20However%2C%20the%20shapes%20and%20movements%20lack%20complexity%2C%20making%20the%0Asimulation%20fail%20to%20resemble%20the%20behavior%20of%20objects%20in%20the%20real%20world.%0ATherefore%20in%20this%20paper%2C%20we%20propose%20ShapeAug%2B%2B%2C%20an%20extended%20version%20of%20ShapeAug%0Awhich%20involves%20randomly%20generated%20polygons%20as%20well%20as%20curved%20movements.%20We%20show%0Athe%20superiority%20of%20our%20method%20on%20multiple%20DVS%20classification%20datasets%2C%0Aimproving%20the%20top-1%20accuracy%20by%20up%20to%203.7%25%20compared%20to%20ShapeAug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeAug%252B%252B%253A%2520More%2520Realistic%2520Shape%2520Augmentation%2520for%2520Event%2520Data%26entry.906535625%3DKatharina%2520Bendig%2520and%2520Ren%25C3%25A9%2520Schuster%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520The%2520novel%2520Dynamic%2520Vision%2520Sensors%2520%2528DVSs%2529%2520gained%2520a%2520great%2520amount%2520of%2520attention%250Arecently%2520as%2520they%2520are%2520superior%2520compared%2520to%2520RGB%2520cameras%2520in%2520terms%2520of%2520latency%252C%250Adynamic%2520range%2520and%2520energy%2520consumption.%2520This%2520is%2520particularly%2520of%2520interest%2520for%250Aautonomous%2520applications%2520since%2520event%2520cameras%2520are%2520able%2520to%2520alleviate%2520motion%2520blur%250Aand%2520allow%2520for%2520night%2520vision.%2520One%2520challenge%2520in%2520real-world%2520autonomous%2520settings%2520is%250Aocclusion%2520where%2520foreground%2520objects%2520hinder%2520the%2520view%2520on%2520traffic%2520participants%2520in%250Athe%2520background.%2520The%2520ShapeAug%2520method%2520addresses%2520this%2520problem%2520by%2520using%2520simulated%250Aevents%2520resulting%2520from%2520objects%2520moving%2520on%2520linear%2520paths%2520for%2520event%2520data%250Aaugmentation.%2520However%252C%2520the%2520shapes%2520and%2520movements%2520lack%2520complexity%252C%2520making%2520the%250Asimulation%2520fail%2520to%2520resemble%2520the%2520behavior%2520of%2520objects%2520in%2520the%2520real%2520world.%250ATherefore%2520in%2520this%2520paper%252C%2520we%2520propose%2520ShapeAug%252B%252B%252C%2520an%2520extended%2520version%2520of%2520ShapeAug%250Awhich%2520involves%2520randomly%2520generated%2520polygons%2520as%2520well%2520as%2520curved%2520movements.%2520We%2520show%250Athe%2520superiority%2520of%2520our%2520method%2520on%2520multiple%2520DVS%2520classification%2520datasets%252C%250Aimproving%2520the%2520top-1%2520accuracy%2520by%2520up%2520to%25203.7%2525%2520compared%2520to%2520ShapeAug.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeAug%2B%2B%3A%20More%20Realistic%20Shape%20Augmentation%20for%20Event%20Data&entry.906535625=Katharina%20Bendig%20and%20Ren%C3%A9%20Schuster%20and%20Didier%20Stricker&entry.1292438233=%20%20The%20novel%20Dynamic%20Vision%20Sensors%20%28DVSs%29%20gained%20a%20great%20amount%20of%20attention%0Arecently%20as%20they%20are%20superior%20compared%20to%20RGB%20cameras%20in%20terms%20of%20latency%2C%0Adynamic%20range%20and%20energy%20consumption.%20This%20is%20particularly%20of%20interest%20for%0Aautonomous%20applications%20since%20event%20cameras%20are%20able%20to%20alleviate%20motion%20blur%0Aand%20allow%20for%20night%20vision.%20One%20challenge%20in%20real-world%20autonomous%20settings%20is%0Aocclusion%20where%20foreground%20objects%20hinder%20the%20view%20on%20traffic%20participants%20in%0Athe%20background.%20The%20ShapeAug%20method%20addresses%20this%20problem%20by%20using%20simulated%0Aevents%20resulting%20from%20objects%20moving%20on%20linear%20paths%20for%20event%20data%0Aaugmentation.%20However%2C%20the%20shapes%20and%20movements%20lack%20complexity%2C%20making%20the%0Asimulation%20fail%20to%20resemble%20the%20behavior%20of%20objects%20in%20the%20real%20world.%0ATherefore%20in%20this%20paper%2C%20we%20propose%20ShapeAug%2B%2B%2C%20an%20extended%20version%20of%20ShapeAug%0Awhich%20involves%20randomly%20generated%20polygons%20as%20well%20as%20curved%20movements.%20We%20show%0Athe%20superiority%20of%20our%20method%20on%20multiple%20DVS%20classification%20datasets%2C%0Aimproving%20the%20top-1%20accuracy%20by%20up%20to%203.7%25%20compared%20to%20ShapeAug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11075v1&entry.124074799=Read"},
{"title": "Benchmarking Multimodal Variational Autoencoders: CdSprites+ Dataset and\n  Toolkit", "author": "Gabriela Sejnova and Michal Vavrecka and Karla Stepanova and Tadahiro Taniguchi", "abstract": "  Multimodal Variational Autoencoders (VAEs) have been the subject of intense\nresearch in the past years as they can integrate multiple modalities into a\njoint representation and can thus serve as a promising tool for both data\nclassification and generation. Several approaches toward multimodal VAE\nlearning have been proposed so far, their comparison and evaluation have\nhowever been rather inconsistent. One reason is that the models differ at the\nimplementation level, another problem is that the datasets commonly used in\nthese cases were not initially designed to evaluate multimodal generative\nmodels. This paper addresses both mentioned issues. First, we propose a toolkit\nfor systematic multimodal VAE training and comparison. The toolkit currently\ncomprises 4 existing multimodal VAEs and 6 commonly used benchmark datasets\nalong with instructions on how to easily add a new model or a dataset. Second,\nwe present a disentangled bimodal dataset designed to comprehensively evaluate\nthe joint generation and cross-generation capabilities across multiple\ndifficulty levels. We demonstrate the utility of our dataset by comparing the\nimplemented state-of-the-art models.\n", "link": "http://arxiv.org/abs/2209.03048v3", "date": "2024-09-17", "relevancy": 2.1733, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5735}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Multimodal%20Variational%20Autoencoders%3A%20CdSprites%2B%20Dataset%20and%0A%20%20Toolkit&body=Title%3A%20Benchmarking%20Multimodal%20Variational%20Autoencoders%3A%20CdSprites%2B%20Dataset%20and%0A%20%20Toolkit%0AAuthor%3A%20Gabriela%20Sejnova%20and%20Michal%20Vavrecka%20and%20Karla%20Stepanova%20and%20Tadahiro%20Taniguchi%0AAbstract%3A%20%20%20Multimodal%20Variational%20Autoencoders%20%28VAEs%29%20have%20been%20the%20subject%20of%20intense%0Aresearch%20in%20the%20past%20years%20as%20they%20can%20integrate%20multiple%20modalities%20into%20a%0Ajoint%20representation%20and%20can%20thus%20serve%20as%20a%20promising%20tool%20for%20both%20data%0Aclassification%20and%20generation.%20Several%20approaches%20toward%20multimodal%20VAE%0Alearning%20have%20been%20proposed%20so%20far%2C%20their%20comparison%20and%20evaluation%20have%0Ahowever%20been%20rather%20inconsistent.%20One%20reason%20is%20that%20the%20models%20differ%20at%20the%0Aimplementation%20level%2C%20another%20problem%20is%20that%20the%20datasets%20commonly%20used%20in%0Athese%20cases%20were%20not%20initially%20designed%20to%20evaluate%20multimodal%20generative%0Amodels.%20This%20paper%20addresses%20both%20mentioned%20issues.%20First%2C%20we%20propose%20a%20toolkit%0Afor%20systematic%20multimodal%20VAE%20training%20and%20comparison.%20The%20toolkit%20currently%0Acomprises%204%20existing%20multimodal%20VAEs%20and%206%20commonly%20used%20benchmark%20datasets%0Aalong%20with%20instructions%20on%20how%20to%20easily%20add%20a%20new%20model%20or%20a%20dataset.%20Second%2C%0Awe%20present%20a%20disentangled%20bimodal%20dataset%20designed%20to%20comprehensively%20evaluate%0Athe%20joint%20generation%20and%20cross-generation%20capabilities%20across%20multiple%0Adifficulty%20levels.%20We%20demonstrate%20the%20utility%20of%20our%20dataset%20by%20comparing%20the%0Aimplemented%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.03048v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Multimodal%2520Variational%2520Autoencoders%253A%2520CdSprites%252B%2520Dataset%2520and%250A%2520%2520Toolkit%26entry.906535625%3DGabriela%2520Sejnova%2520and%2520Michal%2520Vavrecka%2520and%2520Karla%2520Stepanova%2520and%2520Tadahiro%2520Taniguchi%26entry.1292438233%3D%2520%2520Multimodal%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520have%2520been%2520the%2520subject%2520of%2520intense%250Aresearch%2520in%2520the%2520past%2520years%2520as%2520they%2520can%2520integrate%2520multiple%2520modalities%2520into%2520a%250Ajoint%2520representation%2520and%2520can%2520thus%2520serve%2520as%2520a%2520promising%2520tool%2520for%2520both%2520data%250Aclassification%2520and%2520generation.%2520Several%2520approaches%2520toward%2520multimodal%2520VAE%250Alearning%2520have%2520been%2520proposed%2520so%2520far%252C%2520their%2520comparison%2520and%2520evaluation%2520have%250Ahowever%2520been%2520rather%2520inconsistent.%2520One%2520reason%2520is%2520that%2520the%2520models%2520differ%2520at%2520the%250Aimplementation%2520level%252C%2520another%2520problem%2520is%2520that%2520the%2520datasets%2520commonly%2520used%2520in%250Athese%2520cases%2520were%2520not%2520initially%2520designed%2520to%2520evaluate%2520multimodal%2520generative%250Amodels.%2520This%2520paper%2520addresses%2520both%2520mentioned%2520issues.%2520First%252C%2520we%2520propose%2520a%2520toolkit%250Afor%2520systematic%2520multimodal%2520VAE%2520training%2520and%2520comparison.%2520The%2520toolkit%2520currently%250Acomprises%25204%2520existing%2520multimodal%2520VAEs%2520and%25206%2520commonly%2520used%2520benchmark%2520datasets%250Aalong%2520with%2520instructions%2520on%2520how%2520to%2520easily%2520add%2520a%2520new%2520model%2520or%2520a%2520dataset.%2520Second%252C%250Awe%2520present%2520a%2520disentangled%2520bimodal%2520dataset%2520designed%2520to%2520comprehensively%2520evaluate%250Athe%2520joint%2520generation%2520and%2520cross-generation%2520capabilities%2520across%2520multiple%250Adifficulty%2520levels.%2520We%2520demonstrate%2520the%2520utility%2520of%2520our%2520dataset%2520by%2520comparing%2520the%250Aimplemented%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.03048v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Multimodal%20Variational%20Autoencoders%3A%20CdSprites%2B%20Dataset%20and%0A%20%20Toolkit&entry.906535625=Gabriela%20Sejnova%20and%20Michal%20Vavrecka%20and%20Karla%20Stepanova%20and%20Tadahiro%20Taniguchi&entry.1292438233=%20%20Multimodal%20Variational%20Autoencoders%20%28VAEs%29%20have%20been%20the%20subject%20of%20intense%0Aresearch%20in%20the%20past%20years%20as%20they%20can%20integrate%20multiple%20modalities%20into%20a%0Ajoint%20representation%20and%20can%20thus%20serve%20as%20a%20promising%20tool%20for%20both%20data%0Aclassification%20and%20generation.%20Several%20approaches%20toward%20multimodal%20VAE%0Alearning%20have%20been%20proposed%20so%20far%2C%20their%20comparison%20and%20evaluation%20have%0Ahowever%20been%20rather%20inconsistent.%20One%20reason%20is%20that%20the%20models%20differ%20at%20the%0Aimplementation%20level%2C%20another%20problem%20is%20that%20the%20datasets%20commonly%20used%20in%0Athese%20cases%20were%20not%20initially%20designed%20to%20evaluate%20multimodal%20generative%0Amodels.%20This%20paper%20addresses%20both%20mentioned%20issues.%20First%2C%20we%20propose%20a%20toolkit%0Afor%20systematic%20multimodal%20VAE%20training%20and%20comparison.%20The%20toolkit%20currently%0Acomprises%204%20existing%20multimodal%20VAEs%20and%206%20commonly%20used%20benchmark%20datasets%0Aalong%20with%20instructions%20on%20how%20to%20easily%20add%20a%20new%20model%20or%20a%20dataset.%20Second%2C%0Awe%20present%20a%20disentangled%20bimodal%20dataset%20designed%20to%20comprehensively%20evaluate%0Athe%20joint%20generation%20and%20cross-generation%20capabilities%20across%20multiple%0Adifficulty%20levels.%20We%20demonstrate%20the%20utility%20of%20our%20dataset%20by%20comparing%20the%0Aimplemented%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.03048v3&entry.124074799=Read"},
{"title": "Learning by Self-Explaining", "author": "Wolfgang Stammer and Felix Friedrich and David Steinmann and Manuel Brack and Hikaru Shindo and Kristian Kersting", "abstract": "  Much of explainable AI research treats explanations as a means for model\ninspection. Yet, this neglects findings from human psychology that describe the\nbenefit of self-explanations in an agent's learning process. Motivated by this,\nwe introduce a novel workflow in the context of image classification, termed\nLearning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI and\nhuman-guided explanatory machine learning. The underlying idea is that a\nlearner model, in addition to optimizing for the original predictive task, is\nfurther optimized based on explanatory feedback from an internal critic model.\nIntuitively, a learner's explanations are considered \"useful\" if the internal\ncritic can perform the same task given these explanations. We provide an\noverview of important components of LSX and, based on this, perform extensive\nexperimental evaluations via three different example instantiations. Our\nresults indicate improvements via Learning by Self-Explaining on several\nlevels: in terms of model generalization, reducing the influence of confounding\nfactors, and providing more task-relevant and faithful model explanations.\nOverall, our work provides evidence for the potential of self-explaining within\nthe learning phase of an AI model.\n", "link": "http://arxiv.org/abs/2309.08395v3", "date": "2024-09-17", "relevancy": 2.1395, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5554}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20by%20Self-Explaining&body=Title%3A%20Learning%20by%20Self-Explaining%0AAuthor%3A%20Wolfgang%20Stammer%20and%20Felix%20Friedrich%20and%20David%20Steinmann%20and%20Manuel%20Brack%20and%20Hikaru%20Shindo%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Much%20of%20explainable%20AI%20research%20treats%20explanations%20as%20a%20means%20for%20model%0Ainspection.%20Yet%2C%20this%20neglects%20findings%20from%20human%20psychology%20that%20describe%20the%0Abenefit%20of%20self-explanations%20in%20an%20agent%27s%20learning%20process.%20Motivated%20by%20this%2C%0Awe%20introduce%20a%20novel%20workflow%20in%20the%20context%20of%20image%20classification%2C%20termed%0ALearning%20by%20Self-Explaining%20%28LSX%29.%20LSX%20utilizes%20aspects%20of%20self-refining%20AI%20and%0Ahuman-guided%20explanatory%20machine%20learning.%20The%20underlying%20idea%20is%20that%20a%0Alearner%20model%2C%20in%20addition%20to%20optimizing%20for%20the%20original%20predictive%20task%2C%20is%0Afurther%20optimized%20based%20on%20explanatory%20feedback%20from%20an%20internal%20critic%20model.%0AIntuitively%2C%20a%20learner%27s%20explanations%20are%20considered%20%22useful%22%20if%20the%20internal%0Acritic%20can%20perform%20the%20same%20task%20given%20these%20explanations.%20We%20provide%20an%0Aoverview%20of%20important%20components%20of%20LSX%20and%2C%20based%20on%20this%2C%20perform%20extensive%0Aexperimental%20evaluations%20via%20three%20different%20example%20instantiations.%20Our%0Aresults%20indicate%20improvements%20via%20Learning%20by%20Self-Explaining%20on%20several%0Alevels%3A%20in%20terms%20of%20model%20generalization%2C%20reducing%20the%20influence%20of%20confounding%0Afactors%2C%20and%20providing%20more%20task-relevant%20and%20faithful%20model%20explanations.%0AOverall%2C%20our%20work%20provides%20evidence%20for%20the%20potential%20of%20self-explaining%20within%0Athe%20learning%20phase%20of%20an%20AI%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08395v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520by%2520Self-Explaining%26entry.906535625%3DWolfgang%2520Stammer%2520and%2520Felix%2520Friedrich%2520and%2520David%2520Steinmann%2520and%2520Manuel%2520Brack%2520and%2520Hikaru%2520Shindo%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Much%2520of%2520explainable%2520AI%2520research%2520treats%2520explanations%2520as%2520a%2520means%2520for%2520model%250Ainspection.%2520Yet%252C%2520this%2520neglects%2520findings%2520from%2520human%2520psychology%2520that%2520describe%2520the%250Abenefit%2520of%2520self-explanations%2520in%2520an%2520agent%2527s%2520learning%2520process.%2520Motivated%2520by%2520this%252C%250Awe%2520introduce%2520a%2520novel%2520workflow%2520in%2520the%2520context%2520of%2520image%2520classification%252C%2520termed%250ALearning%2520by%2520Self-Explaining%2520%2528LSX%2529.%2520LSX%2520utilizes%2520aspects%2520of%2520self-refining%2520AI%2520and%250Ahuman-guided%2520explanatory%2520machine%2520learning.%2520The%2520underlying%2520idea%2520is%2520that%2520a%250Alearner%2520model%252C%2520in%2520addition%2520to%2520optimizing%2520for%2520the%2520original%2520predictive%2520task%252C%2520is%250Afurther%2520optimized%2520based%2520on%2520explanatory%2520feedback%2520from%2520an%2520internal%2520critic%2520model.%250AIntuitively%252C%2520a%2520learner%2527s%2520explanations%2520are%2520considered%2520%2522useful%2522%2520if%2520the%2520internal%250Acritic%2520can%2520perform%2520the%2520same%2520task%2520given%2520these%2520explanations.%2520We%2520provide%2520an%250Aoverview%2520of%2520important%2520components%2520of%2520LSX%2520and%252C%2520based%2520on%2520this%252C%2520perform%2520extensive%250Aexperimental%2520evaluations%2520via%2520three%2520different%2520example%2520instantiations.%2520Our%250Aresults%2520indicate%2520improvements%2520via%2520Learning%2520by%2520Self-Explaining%2520on%2520several%250Alevels%253A%2520in%2520terms%2520of%2520model%2520generalization%252C%2520reducing%2520the%2520influence%2520of%2520confounding%250Afactors%252C%2520and%2520providing%2520more%2520task-relevant%2520and%2520faithful%2520model%2520explanations.%250AOverall%252C%2520our%2520work%2520provides%2520evidence%2520for%2520the%2520potential%2520of%2520self-explaining%2520within%250Athe%2520learning%2520phase%2520of%2520an%2520AI%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08395v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20by%20Self-Explaining&entry.906535625=Wolfgang%20Stammer%20and%20Felix%20Friedrich%20and%20David%20Steinmann%20and%20Manuel%20Brack%20and%20Hikaru%20Shindo%20and%20Kristian%20Kersting&entry.1292438233=%20%20Much%20of%20explainable%20AI%20research%20treats%20explanations%20as%20a%20means%20for%20model%0Ainspection.%20Yet%2C%20this%20neglects%20findings%20from%20human%20psychology%20that%20describe%20the%0Abenefit%20of%20self-explanations%20in%20an%20agent%27s%20learning%20process.%20Motivated%20by%20this%2C%0Awe%20introduce%20a%20novel%20workflow%20in%20the%20context%20of%20image%20classification%2C%20termed%0ALearning%20by%20Self-Explaining%20%28LSX%29.%20LSX%20utilizes%20aspects%20of%20self-refining%20AI%20and%0Ahuman-guided%20explanatory%20machine%20learning.%20The%20underlying%20idea%20is%20that%20a%0Alearner%20model%2C%20in%20addition%20to%20optimizing%20for%20the%20original%20predictive%20task%2C%20is%0Afurther%20optimized%20based%20on%20explanatory%20feedback%20from%20an%20internal%20critic%20model.%0AIntuitively%2C%20a%20learner%27s%20explanations%20are%20considered%20%22useful%22%20if%20the%20internal%0Acritic%20can%20perform%20the%20same%20task%20given%20these%20explanations.%20We%20provide%20an%0Aoverview%20of%20important%20components%20of%20LSX%20and%2C%20based%20on%20this%2C%20perform%20extensive%0Aexperimental%20evaluations%20via%20three%20different%20example%20instantiations.%20Our%0Aresults%20indicate%20improvements%20via%20Learning%20by%20Self-Explaining%20on%20several%0Alevels%3A%20in%20terms%20of%20model%20generalization%2C%20reducing%20the%20influence%20of%20confounding%0Afactors%2C%20and%20providing%20more%20task-relevant%20and%20faithful%20model%20explanations.%0AOverall%2C%20our%20work%20provides%20evidence%20for%20the%20potential%20of%20self-explaining%20within%0Athe%20learning%20phase%20of%20an%20AI%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08395v3&entry.124074799=Read"},
{"title": "TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical\n  image segmentation", "author": "Rong Zhou and Zhengqing Yuan and Zhiling Yan and Weixiang Sun and Kai Zhang and Yiwei Li and Yanfang Ye and Xiang Li and Lifang He and Lichao Sun", "abstract": "  Biomedical image segmentation is crucial for accurately diagnosing and\nanalyzing various diseases. However, Convolutional Neural Networks (CNNs) and\nTransformers, the most commonly used architectures for this task, struggle to\neffectively capture long-range dependencies due to the inherent locality of\nCNNs and the computational complexity of Transformers. To address this\nlimitation, we introduce TTT-Unet, a novel framework that integrates Test-Time\nTraining (TTT) layers into the traditional U-Net architecture for biomedical\nimage segmentation. TTT-Unet dynamically adjusts model parameters during the\ntesting time, enhancing the model's ability to capture both local and\nlong-range features. We evaluate TTT-Unet on multiple medical imaging datasets,\nincluding 3D abdominal organ segmentation in CT and MR images, instrument\nsegmentation in endoscopy images, and cell segmentation in microscopy images.\nThe results demonstrate that TTT-Unet consistently outperforms state-of-the-art\nCNN-based and Transformer-based segmentation models across all tasks. The code\nis available at https://github.com/rongzhou7/TTT-Unet.\n", "link": "http://arxiv.org/abs/2409.11299v1", "date": "2024-09-17", "relevancy": 2.1386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5406}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTT-Unet%3A%20Enhancing%20U-Net%20with%20Test-Time%20Training%20Layers%20for%20biomedical%0A%20%20image%20segmentation&body=Title%3A%20TTT-Unet%3A%20Enhancing%20U-Net%20with%20Test-Time%20Training%20Layers%20for%20biomedical%0A%20%20image%20segmentation%0AAuthor%3A%20Rong%20Zhou%20and%20Zhengqing%20Yuan%20and%20Zhiling%20Yan%20and%20Weixiang%20Sun%20and%20Kai%20Zhang%20and%20Yiwei%20Li%20and%20Yanfang%20Ye%20and%20Xiang%20Li%20and%20Lifang%20He%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Biomedical%20image%20segmentation%20is%20crucial%20for%20accurately%20diagnosing%20and%0Aanalyzing%20various%20diseases.%20However%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%0ATransformers%2C%20the%20most%20commonly%20used%20architectures%20for%20this%20task%2C%20struggle%20to%0Aeffectively%20capture%20long-range%20dependencies%20due%20to%20the%20inherent%20locality%20of%0ACNNs%20and%20the%20computational%20complexity%20of%20Transformers.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20TTT-Unet%2C%20a%20novel%20framework%20that%20integrates%20Test-Time%0ATraining%20%28TTT%29%20layers%20into%20the%20traditional%20U-Net%20architecture%20for%20biomedical%0Aimage%20segmentation.%20TTT-Unet%20dynamically%20adjusts%20model%20parameters%20during%20the%0Atesting%20time%2C%20enhancing%20the%20model%27s%20ability%20to%20capture%20both%20local%20and%0Along-range%20features.%20We%20evaluate%20TTT-Unet%20on%20multiple%20medical%20imaging%20datasets%2C%0Aincluding%203D%20abdominal%20organ%20segmentation%20in%20CT%20and%20MR%20images%2C%20instrument%0Asegmentation%20in%20endoscopy%20images%2C%20and%20cell%20segmentation%20in%20microscopy%20images.%0AThe%20results%20demonstrate%20that%20TTT-Unet%20consistently%20outperforms%20state-of-the-art%0ACNN-based%20and%20Transformer-based%20segmentation%20models%20across%20all%20tasks.%20The%20code%0Ais%20available%20at%20https%3A//github.com/rongzhou7/TTT-Unet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTT-Unet%253A%2520Enhancing%2520U-Net%2520with%2520Test-Time%2520Training%2520Layers%2520for%2520biomedical%250A%2520%2520image%2520segmentation%26entry.906535625%3DRong%2520Zhou%2520and%2520Zhengqing%2520Yuan%2520and%2520Zhiling%2520Yan%2520and%2520Weixiang%2520Sun%2520and%2520Kai%2520Zhang%2520and%2520Yiwei%2520Li%2520and%2520Yanfang%2520Ye%2520and%2520Xiang%2520Li%2520and%2520Lifang%2520He%2520and%2520Lichao%2520Sun%26entry.1292438233%3D%2520%2520Biomedical%2520image%2520segmentation%2520is%2520crucial%2520for%2520accurately%2520diagnosing%2520and%250Aanalyzing%2520various%2520diseases.%2520However%252C%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%250ATransformers%252C%2520the%2520most%2520commonly%2520used%2520architectures%2520for%2520this%2520task%252C%2520struggle%2520to%250Aeffectively%2520capture%2520long-range%2520dependencies%2520due%2520to%2520the%2520inherent%2520locality%2520of%250ACNNs%2520and%2520the%2520computational%2520complexity%2520of%2520Transformers.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520TTT-Unet%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520Test-Time%250ATraining%2520%2528TTT%2529%2520layers%2520into%2520the%2520traditional%2520U-Net%2520architecture%2520for%2520biomedical%250Aimage%2520segmentation.%2520TTT-Unet%2520dynamically%2520adjusts%2520model%2520parameters%2520during%2520the%250Atesting%2520time%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520capture%2520both%2520local%2520and%250Along-range%2520features.%2520We%2520evaluate%2520TTT-Unet%2520on%2520multiple%2520medical%2520imaging%2520datasets%252C%250Aincluding%25203D%2520abdominal%2520organ%2520segmentation%2520in%2520CT%2520and%2520MR%2520images%252C%2520instrument%250Asegmentation%2520in%2520endoscopy%2520images%252C%2520and%2520cell%2520segmentation%2520in%2520microscopy%2520images.%250AThe%2520results%2520demonstrate%2520that%2520TTT-Unet%2520consistently%2520outperforms%2520state-of-the-art%250ACNN-based%2520and%2520Transformer-based%2520segmentation%2520models%2520across%2520all%2520tasks.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/rongzhou7/TTT-Unet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTT-Unet%3A%20Enhancing%20U-Net%20with%20Test-Time%20Training%20Layers%20for%20biomedical%0A%20%20image%20segmentation&entry.906535625=Rong%20Zhou%20and%20Zhengqing%20Yuan%20and%20Zhiling%20Yan%20and%20Weixiang%20Sun%20and%20Kai%20Zhang%20and%20Yiwei%20Li%20and%20Yanfang%20Ye%20and%20Xiang%20Li%20and%20Lifang%20He%20and%20Lichao%20Sun&entry.1292438233=%20%20Biomedical%20image%20segmentation%20is%20crucial%20for%20accurately%20diagnosing%20and%0Aanalyzing%20various%20diseases.%20However%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%0ATransformers%2C%20the%20most%20commonly%20used%20architectures%20for%20this%20task%2C%20struggle%20to%0Aeffectively%20capture%20long-range%20dependencies%20due%20to%20the%20inherent%20locality%20of%0ACNNs%20and%20the%20computational%20complexity%20of%20Transformers.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20TTT-Unet%2C%20a%20novel%20framework%20that%20integrates%20Test-Time%0ATraining%20%28TTT%29%20layers%20into%20the%20traditional%20U-Net%20architecture%20for%20biomedical%0Aimage%20segmentation.%20TTT-Unet%20dynamically%20adjusts%20model%20parameters%20during%20the%0Atesting%20time%2C%20enhancing%20the%20model%27s%20ability%20to%20capture%20both%20local%20and%0Along-range%20features.%20We%20evaluate%20TTT-Unet%20on%20multiple%20medical%20imaging%20datasets%2C%0Aincluding%203D%20abdominal%20organ%20segmentation%20in%20CT%20and%20MR%20images%2C%20instrument%0Asegmentation%20in%20endoscopy%20images%2C%20and%20cell%20segmentation%20in%20microscopy%20images.%0AThe%20results%20demonstrate%20that%20TTT-Unet%20consistently%20outperforms%20state-of-the-art%0ACNN-based%20and%20Transformer-based%20segmentation%20models%20across%20all%20tasks.%20The%20code%0Ais%20available%20at%20https%3A//github.com/rongzhou7/TTT-Unet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11299v1&entry.124074799=Read"},
{"title": "Gradient-free Post-hoc Explainability Using Distillation Aided Learnable\n  Approach", "author": "Debarpan Bhattacharya and Amir H. Poorjam and Deepak Mittal and Sriram Ganapathy", "abstract": "  The recent advancements in artificial intelligence (AI), with the release of\nseveral large models having only query access, make a strong case for\nexplainability of deep models in a post-hoc gradient free manner. In this\npaper, we propose a framework, named distillation aided explainability (DAX),\nthat attempts to generate a saliency-based explanation in a model agnostic\ngradient free application. The DAX approach poses the problem of explanation in\na learnable setting with a mask generation network and a distillation network.\nThe mask generation network learns to generate the multiplier mask that finds\nthe salient regions of the input, while the student distillation network aims\nto approximate the local behavior of the black-box model. We propose a joint\noptimization of the two networks in the DAX framework using the locally\nperturbed input samples, with the targets derived from input-output access to\nthe black-box model. We extensively evaluate DAX across different modalities\n(image and audio), in a classification setting, using a diverse set of\nevaluations (intersection over union with ground truth, deletion based and\nsubjective human evaluation based measures) and benchmark it with respect to\n$9$ different methods. In these evaluations, the DAX significantly outperforms\nthe existing approaches on all modalities and evaluation metrics.\n", "link": "http://arxiv.org/abs/2409.11123v1", "date": "2024-09-17", "relevancy": 2.0871, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5322}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5213}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-free%20Post-hoc%20Explainability%20Using%20Distillation%20Aided%20Learnable%0A%20%20Approach&body=Title%3A%20Gradient-free%20Post-hoc%20Explainability%20Using%20Distillation%20Aided%20Learnable%0A%20%20Approach%0AAuthor%3A%20Debarpan%20Bhattacharya%20and%20Amir%20H.%20Poorjam%20and%20Deepak%20Mittal%20and%20Sriram%20Ganapathy%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20artificial%20intelligence%20%28AI%29%2C%20with%20the%20release%20of%0Aseveral%20large%20models%20having%20only%20query%20access%2C%20make%20a%20strong%20case%20for%0Aexplainability%20of%20deep%20models%20in%20a%20post-hoc%20gradient%20free%20manner.%20In%20this%0Apaper%2C%20we%20propose%20a%20framework%2C%20named%20distillation%20aided%20explainability%20%28DAX%29%2C%0Athat%20attempts%20to%20generate%20a%20saliency-based%20explanation%20in%20a%20model%20agnostic%0Agradient%20free%20application.%20The%20DAX%20approach%20poses%20the%20problem%20of%20explanation%20in%0Aa%20learnable%20setting%20with%20a%20mask%20generation%20network%20and%20a%20distillation%20network.%0AThe%20mask%20generation%20network%20learns%20to%20generate%20the%20multiplier%20mask%20that%20finds%0Athe%20salient%20regions%20of%20the%20input%2C%20while%20the%20student%20distillation%20network%20aims%0Ato%20approximate%20the%20local%20behavior%20of%20the%20black-box%20model.%20We%20propose%20a%20joint%0Aoptimization%20of%20the%20two%20networks%20in%20the%20DAX%20framework%20using%20the%20locally%0Aperturbed%20input%20samples%2C%20with%20the%20targets%20derived%20from%20input-output%20access%20to%0Athe%20black-box%20model.%20We%20extensively%20evaluate%20DAX%20across%20different%20modalities%0A%28image%20and%20audio%29%2C%20in%20a%20classification%20setting%2C%20using%20a%20diverse%20set%20of%0Aevaluations%20%28intersection%20over%20union%20with%20ground%20truth%2C%20deletion%20based%20and%0Asubjective%20human%20evaluation%20based%20measures%29%20and%20benchmark%20it%20with%20respect%20to%0A%249%24%20different%20methods.%20In%20these%20evaluations%2C%20the%20DAX%20significantly%20outperforms%0Athe%20existing%20approaches%20on%20all%20modalities%20and%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-free%2520Post-hoc%2520Explainability%2520Using%2520Distillation%2520Aided%2520Learnable%250A%2520%2520Approach%26entry.906535625%3DDebarpan%2520Bhattacharya%2520and%2520Amir%2520H.%2520Poorjam%2520and%2520Deepak%2520Mittal%2520and%2520Sriram%2520Ganapathy%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520with%2520the%2520release%2520of%250Aseveral%2520large%2520models%2520having%2520only%2520query%2520access%252C%2520make%2520a%2520strong%2520case%2520for%250Aexplainability%2520of%2520deep%2520models%2520in%2520a%2520post-hoc%2520gradient%2520free%2520manner.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520framework%252C%2520named%2520distillation%2520aided%2520explainability%2520%2528DAX%2529%252C%250Athat%2520attempts%2520to%2520generate%2520a%2520saliency-based%2520explanation%2520in%2520a%2520model%2520agnostic%250Agradient%2520free%2520application.%2520The%2520DAX%2520approach%2520poses%2520the%2520problem%2520of%2520explanation%2520in%250Aa%2520learnable%2520setting%2520with%2520a%2520mask%2520generation%2520network%2520and%2520a%2520distillation%2520network.%250AThe%2520mask%2520generation%2520network%2520learns%2520to%2520generate%2520the%2520multiplier%2520mask%2520that%2520finds%250Athe%2520salient%2520regions%2520of%2520the%2520input%252C%2520while%2520the%2520student%2520distillation%2520network%2520aims%250Ato%2520approximate%2520the%2520local%2520behavior%2520of%2520the%2520black-box%2520model.%2520We%2520propose%2520a%2520joint%250Aoptimization%2520of%2520the%2520two%2520networks%2520in%2520the%2520DAX%2520framework%2520using%2520the%2520locally%250Aperturbed%2520input%2520samples%252C%2520with%2520the%2520targets%2520derived%2520from%2520input-output%2520access%2520to%250Athe%2520black-box%2520model.%2520We%2520extensively%2520evaluate%2520DAX%2520across%2520different%2520modalities%250A%2528image%2520and%2520audio%2529%252C%2520in%2520a%2520classification%2520setting%252C%2520using%2520a%2520diverse%2520set%2520of%250Aevaluations%2520%2528intersection%2520over%2520union%2520with%2520ground%2520truth%252C%2520deletion%2520based%2520and%250Asubjective%2520human%2520evaluation%2520based%2520measures%2529%2520and%2520benchmark%2520it%2520with%2520respect%2520to%250A%25249%2524%2520different%2520methods.%2520In%2520these%2520evaluations%252C%2520the%2520DAX%2520significantly%2520outperforms%250Athe%2520existing%2520approaches%2520on%2520all%2520modalities%2520and%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-free%20Post-hoc%20Explainability%20Using%20Distillation%20Aided%20Learnable%0A%20%20Approach&entry.906535625=Debarpan%20Bhattacharya%20and%20Amir%20H.%20Poorjam%20and%20Deepak%20Mittal%20and%20Sriram%20Ganapathy&entry.1292438233=%20%20The%20recent%20advancements%20in%20artificial%20intelligence%20%28AI%29%2C%20with%20the%20release%20of%0Aseveral%20large%20models%20having%20only%20query%20access%2C%20make%20a%20strong%20case%20for%0Aexplainability%20of%20deep%20models%20in%20a%20post-hoc%20gradient%20free%20manner.%20In%20this%0Apaper%2C%20we%20propose%20a%20framework%2C%20named%20distillation%20aided%20explainability%20%28DAX%29%2C%0Athat%20attempts%20to%20generate%20a%20saliency-based%20explanation%20in%20a%20model%20agnostic%0Agradient%20free%20application.%20The%20DAX%20approach%20poses%20the%20problem%20of%20explanation%20in%0Aa%20learnable%20setting%20with%20a%20mask%20generation%20network%20and%20a%20distillation%20network.%0AThe%20mask%20generation%20network%20learns%20to%20generate%20the%20multiplier%20mask%20that%20finds%0Athe%20salient%20regions%20of%20the%20input%2C%20while%20the%20student%20distillation%20network%20aims%0Ato%20approximate%20the%20local%20behavior%20of%20the%20black-box%20model.%20We%20propose%20a%20joint%0Aoptimization%20of%20the%20two%20networks%20in%20the%20DAX%20framework%20using%20the%20locally%0Aperturbed%20input%20samples%2C%20with%20the%20targets%20derived%20from%20input-output%20access%20to%0Athe%20black-box%20model.%20We%20extensively%20evaluate%20DAX%20across%20different%20modalities%0A%28image%20and%20audio%29%2C%20in%20a%20classification%20setting%2C%20using%20a%20diverse%20set%20of%0Aevaluations%20%28intersection%20over%20union%20with%20ground%20truth%2C%20deletion%20based%20and%0Asubjective%20human%20evaluation%20based%20measures%29%20and%20benchmark%20it%20with%20respect%20to%0A%249%24%20different%20methods.%20In%20these%20evaluations%2C%20the%20DAX%20significantly%20outperforms%0Athe%20existing%20approaches%20on%20all%20modalities%20and%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11123v1&entry.124074799=Read"},
{"title": "Compact Implicit Neural Representations for Plane Wave Images", "author": "Mathilde Monvoisin and Yuxin Zhang and Diana Mateus", "abstract": "  Ultrafast Plane-Wave (PW) imaging often produces artifacts and shadows that\nvary with insonification angles. We propose a novel approach using Implicit\nNeural Representations (INRs) to compactly encode multi-planar sequences while\npreserving crucial orientation-dependent information. To our knowledge, this is\nthe first application of INRs for PW angular interpolation. Our method employs\na Multi-Layer Perceptron (MLP)-based model with a concise physics-enhanced\nrendering technique. Quantitative evaluations using SSIM, PSNR, and standard\nultrasound metrics, along with qualitative visual assessments, confirm the\neffectiveness of our approach. Additionally, our method demonstrates\nsignificant storage efficiency, with model weights requiring 530 KB compared to\n8 MB for directly storing the 75 PW images, achieving a notable compression\nratio of approximately 15:1.\n", "link": "http://arxiv.org/abs/2409.11370v1", "date": "2024-09-17", "relevancy": 2.0851, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5421}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5278}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compact%20Implicit%20Neural%20Representations%20for%20Plane%20Wave%20Images&body=Title%3A%20Compact%20Implicit%20Neural%20Representations%20for%20Plane%20Wave%20Images%0AAuthor%3A%20Mathilde%20Monvoisin%20and%20Yuxin%20Zhang%20and%20Diana%20Mateus%0AAbstract%3A%20%20%20Ultrafast%20Plane-Wave%20%28PW%29%20imaging%20often%20produces%20artifacts%20and%20shadows%20that%0Avary%20with%20insonification%20angles.%20We%20propose%20a%20novel%20approach%20using%20Implicit%0ANeural%20Representations%20%28INRs%29%20to%20compactly%20encode%20multi-planar%20sequences%20while%0Apreserving%20crucial%20orientation-dependent%20information.%20To%20our%20knowledge%2C%20this%20is%0Athe%20first%20application%20of%20INRs%20for%20PW%20angular%20interpolation.%20Our%20method%20employs%0Aa%20Multi-Layer%20Perceptron%20%28MLP%29-based%20model%20with%20a%20concise%20physics-enhanced%0Arendering%20technique.%20Quantitative%20evaluations%20using%20SSIM%2C%20PSNR%2C%20and%20standard%0Aultrasound%20metrics%2C%20along%20with%20qualitative%20visual%20assessments%2C%20confirm%20the%0Aeffectiveness%20of%20our%20approach.%20Additionally%2C%20our%20method%20demonstrates%0Asignificant%20storage%20efficiency%2C%20with%20model%20weights%20requiring%20530%20KB%20compared%20to%0A8%20MB%20for%20directly%20storing%20the%2075%20PW%20images%2C%20achieving%20a%20notable%20compression%0Aratio%20of%20approximately%2015%3A1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompact%2520Implicit%2520Neural%2520Representations%2520for%2520Plane%2520Wave%2520Images%26entry.906535625%3DMathilde%2520Monvoisin%2520and%2520Yuxin%2520Zhang%2520and%2520Diana%2520Mateus%26entry.1292438233%3D%2520%2520Ultrafast%2520Plane-Wave%2520%2528PW%2529%2520imaging%2520often%2520produces%2520artifacts%2520and%2520shadows%2520that%250Avary%2520with%2520insonification%2520angles.%2520We%2520propose%2520a%2520novel%2520approach%2520using%2520Implicit%250ANeural%2520Representations%2520%2528INRs%2529%2520to%2520compactly%2520encode%2520multi-planar%2520sequences%2520while%250Apreserving%2520crucial%2520orientation-dependent%2520information.%2520To%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520application%2520of%2520INRs%2520for%2520PW%2520angular%2520interpolation.%2520Our%2520method%2520employs%250Aa%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529-based%2520model%2520with%2520a%2520concise%2520physics-enhanced%250Arendering%2520technique.%2520Quantitative%2520evaluations%2520using%2520SSIM%252C%2520PSNR%252C%2520and%2520standard%250Aultrasound%2520metrics%252C%2520along%2520with%2520qualitative%2520visual%2520assessments%252C%2520confirm%2520the%250Aeffectiveness%2520of%2520our%2520approach.%2520Additionally%252C%2520our%2520method%2520demonstrates%250Asignificant%2520storage%2520efficiency%252C%2520with%2520model%2520weights%2520requiring%2520530%2520KB%2520compared%2520to%250A8%2520MB%2520for%2520directly%2520storing%2520the%252075%2520PW%2520images%252C%2520achieving%2520a%2520notable%2520compression%250Aratio%2520of%2520approximately%252015%253A1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%20Implicit%20Neural%20Representations%20for%20Plane%20Wave%20Images&entry.906535625=Mathilde%20Monvoisin%20and%20Yuxin%20Zhang%20and%20Diana%20Mateus&entry.1292438233=%20%20Ultrafast%20Plane-Wave%20%28PW%29%20imaging%20often%20produces%20artifacts%20and%20shadows%20that%0Avary%20with%20insonification%20angles.%20We%20propose%20a%20novel%20approach%20using%20Implicit%0ANeural%20Representations%20%28INRs%29%20to%20compactly%20encode%20multi-planar%20sequences%20while%0Apreserving%20crucial%20orientation-dependent%20information.%20To%20our%20knowledge%2C%20this%20is%0Athe%20first%20application%20of%20INRs%20for%20PW%20angular%20interpolation.%20Our%20method%20employs%0Aa%20Multi-Layer%20Perceptron%20%28MLP%29-based%20model%20with%20a%20concise%20physics-enhanced%0Arendering%20technique.%20Quantitative%20evaluations%20using%20SSIM%2C%20PSNR%2C%20and%20standard%0Aultrasound%20metrics%2C%20along%20with%20qualitative%20visual%20assessments%2C%20confirm%20the%0Aeffectiveness%20of%20our%20approach.%20Additionally%2C%20our%20method%20demonstrates%0Asignificant%20storage%20efficiency%2C%20with%20model%20weights%20requiring%20530%20KB%20compared%20to%0A8%20MB%20for%20directly%20storing%20the%2075%20PW%20images%2C%20achieving%20a%20notable%20compression%0Aratio%20of%20approximately%2015%3A1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11370v1&entry.124074799=Read"},
{"title": "STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object\n  Tracking", "author": "Jianbo Ma and Chuanming Tang and Fei Wu and Can Zhao and Jianlin Zhang and Zhiyong Xu", "abstract": "  Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is\nimportant for diverse applications in computer vision. Current MOT trackers\nrely on accurate object detection results and precise matching of target\nreidentification (ReID). These methods focus on optimizing target spatial\nattributes while overlooking temporal cues in modelling object relationships,\nespecially for challenging tracking conditions such as object deformation and\nblurring, etc. To address the above-mentioned issues, we propose a novel\nSpatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which\nutilizes historical embedding features to model the representation of ReID and\ndetection features in a sequential order. Concretely, a temporal embedding\nboosting module is introduced to enhance the discriminability of individual\nembedding based on adjacent frame cooperation. While the trajectory embedding\nis then propagated by a temporal detection refinement module to mine salient\ntarget locations in the temporal field. Extensive experiments on the\nVisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new\nstate-of-the-art performance in MOTA and IDF1 metrics. The source codes are\nreleased at https://github.com/ydhcg-BoBo/STCMOT.\n", "link": "http://arxiv.org/abs/2409.11234v1", "date": "2024-09-17", "relevancy": 2.0848, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5247}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STCMOT%3A%20Spatio-Temporal%20Cohesion%20Learning%20for%20UAV-Based%20Multiple%20Object%0A%20%20Tracking&body=Title%3A%20STCMOT%3A%20Spatio-Temporal%20Cohesion%20Learning%20for%20UAV-Based%20Multiple%20Object%0A%20%20Tracking%0AAuthor%3A%20Jianbo%20Ma%20and%20Chuanming%20Tang%20and%20Fei%20Wu%20and%20Can%20Zhao%20and%20Jianlin%20Zhang%20and%20Zhiyong%20Xu%0AAbstract%3A%20%20%20Multiple%20object%20tracking%20%28MOT%29%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20videos%20is%0Aimportant%20for%20diverse%20applications%20in%20computer%20vision.%20Current%20MOT%20trackers%0Arely%20on%20accurate%20object%20detection%20results%20and%20precise%20matching%20of%20target%0Areidentification%20%28ReID%29.%20These%20methods%20focus%20on%20optimizing%20target%20spatial%0Aattributes%20while%20overlooking%20temporal%20cues%20in%20modelling%20object%20relationships%2C%0Aespecially%20for%20challenging%20tracking%20conditions%20such%20as%20object%20deformation%20and%0Ablurring%2C%20etc.%20To%20address%20the%20above-mentioned%20issues%2C%20we%20propose%20a%20novel%0ASpatio-Temporal%20Cohesion%20Multiple%20Object%20Tracking%20framework%20%28STCMOT%29%2C%20which%0Autilizes%20historical%20embedding%20features%20to%20model%20the%20representation%20of%20ReID%20and%0Adetection%20features%20in%20a%20sequential%20order.%20Concretely%2C%20a%20temporal%20embedding%0Aboosting%20module%20is%20introduced%20to%20enhance%20the%20discriminability%20of%20individual%0Aembedding%20based%20on%20adjacent%20frame%20cooperation.%20While%20the%20trajectory%20embedding%0Ais%20then%20propagated%20by%20a%20temporal%20detection%20refinement%20module%20to%20mine%20salient%0Atarget%20locations%20in%20the%20temporal%20field.%20Extensive%20experiments%20on%20the%0AVisDrone2019%20and%20UAVDT%20datasets%20demonstrate%20our%20STCMOT%20sets%20a%20new%0Astate-of-the-art%20performance%20in%20MOTA%20and%20IDF1%20metrics.%20The%20source%20codes%20are%0Areleased%20at%20https%3A//github.com/ydhcg-BoBo/STCMOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTCMOT%253A%2520Spatio-Temporal%2520Cohesion%2520Learning%2520for%2520UAV-Based%2520Multiple%2520Object%250A%2520%2520Tracking%26entry.906535625%3DJianbo%2520Ma%2520and%2520Chuanming%2520Tang%2520and%2520Fei%2520Wu%2520and%2520Can%2520Zhao%2520and%2520Jianlin%2520Zhang%2520and%2520Zhiyong%2520Xu%26entry.1292438233%3D%2520%2520Multiple%2520object%2520tracking%2520%2528MOT%2529%2520in%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520videos%2520is%250Aimportant%2520for%2520diverse%2520applications%2520in%2520computer%2520vision.%2520Current%2520MOT%2520trackers%250Arely%2520on%2520accurate%2520object%2520detection%2520results%2520and%2520precise%2520matching%2520of%2520target%250Areidentification%2520%2528ReID%2529.%2520These%2520methods%2520focus%2520on%2520optimizing%2520target%2520spatial%250Aattributes%2520while%2520overlooking%2520temporal%2520cues%2520in%2520modelling%2520object%2520relationships%252C%250Aespecially%2520for%2520challenging%2520tracking%2520conditions%2520such%2520as%2520object%2520deformation%2520and%250Ablurring%252C%2520etc.%2520To%2520address%2520the%2520above-mentioned%2520issues%252C%2520we%2520propose%2520a%2520novel%250ASpatio-Temporal%2520Cohesion%2520Multiple%2520Object%2520Tracking%2520framework%2520%2528STCMOT%2529%252C%2520which%250Autilizes%2520historical%2520embedding%2520features%2520to%2520model%2520the%2520representation%2520of%2520ReID%2520and%250Adetection%2520features%2520in%2520a%2520sequential%2520order.%2520Concretely%252C%2520a%2520temporal%2520embedding%250Aboosting%2520module%2520is%2520introduced%2520to%2520enhance%2520the%2520discriminability%2520of%2520individual%250Aembedding%2520based%2520on%2520adjacent%2520frame%2520cooperation.%2520While%2520the%2520trajectory%2520embedding%250Ais%2520then%2520propagated%2520by%2520a%2520temporal%2520detection%2520refinement%2520module%2520to%2520mine%2520salient%250Atarget%2520locations%2520in%2520the%2520temporal%2520field.%2520Extensive%2520experiments%2520on%2520the%250AVisDrone2019%2520and%2520UAVDT%2520datasets%2520demonstrate%2520our%2520STCMOT%2520sets%2520a%2520new%250Astate-of-the-art%2520performance%2520in%2520MOTA%2520and%2520IDF1%2520metrics.%2520The%2520source%2520codes%2520are%250Areleased%2520at%2520https%253A//github.com/ydhcg-BoBo/STCMOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STCMOT%3A%20Spatio-Temporal%20Cohesion%20Learning%20for%20UAV-Based%20Multiple%20Object%0A%20%20Tracking&entry.906535625=Jianbo%20Ma%20and%20Chuanming%20Tang%20and%20Fei%20Wu%20and%20Can%20Zhao%20and%20Jianlin%20Zhang%20and%20Zhiyong%20Xu&entry.1292438233=%20%20Multiple%20object%20tracking%20%28MOT%29%20in%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20videos%20is%0Aimportant%20for%20diverse%20applications%20in%20computer%20vision.%20Current%20MOT%20trackers%0Arely%20on%20accurate%20object%20detection%20results%20and%20precise%20matching%20of%20target%0Areidentification%20%28ReID%29.%20These%20methods%20focus%20on%20optimizing%20target%20spatial%0Aattributes%20while%20overlooking%20temporal%20cues%20in%20modelling%20object%20relationships%2C%0Aespecially%20for%20challenging%20tracking%20conditions%20such%20as%20object%20deformation%20and%0Ablurring%2C%20etc.%20To%20address%20the%20above-mentioned%20issues%2C%20we%20propose%20a%20novel%0ASpatio-Temporal%20Cohesion%20Multiple%20Object%20Tracking%20framework%20%28STCMOT%29%2C%20which%0Autilizes%20historical%20embedding%20features%20to%20model%20the%20representation%20of%20ReID%20and%0Adetection%20features%20in%20a%20sequential%20order.%20Concretely%2C%20a%20temporal%20embedding%0Aboosting%20module%20is%20introduced%20to%20enhance%20the%20discriminability%20of%20individual%0Aembedding%20based%20on%20adjacent%20frame%20cooperation.%20While%20the%20trajectory%20embedding%0Ais%20then%20propagated%20by%20a%20temporal%20detection%20refinement%20module%20to%20mine%20salient%0Atarget%20locations%20in%20the%20temporal%20field.%20Extensive%20experiments%20on%20the%0AVisDrone2019%20and%20UAVDT%20datasets%20demonstrate%20our%20STCMOT%20sets%20a%20new%0Astate-of-the-art%20performance%20in%20MOTA%20and%20IDF1%20metrics.%20The%20source%20codes%20are%0Areleased%20at%20https%3A//github.com/ydhcg-BoBo/STCMOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11234v1&entry.124074799=Read"},
{"title": "When Cars meet Drones: Hyperbolic Federated Learning for Source-Free\n  Domain Adaptation in Adverse Weather", "author": "Giulia Rizzoli and Matteo Caligiuri and Donald Shenaj and Francesco Barbato and Pietro Zanuttigh", "abstract": "  In Federated Learning (FL), multiple clients collaboratively train a global\nmodel without sharing private data. In semantic segmentation, the Federated\nsource Free Domain Adaptation (FFreeDA) setting is of particular interest,\nwhere clients undergo unsupervised training after supervised pretraining at the\nserver side. While few recent works address FL for autonomous vehicles,\nintrinsic real-world challenges such as the presence of adverse weather\nconditions and the existence of different autonomous agents are still\nunexplored. To bridge this gap, we address both problems and introduce a new\nfederated semantic segmentation setting where both car and drone clients\nco-exist and collaborate. Specifically, we propose a novel approach for this\nsetting which exploits a batch-norm weather-aware strategy to dynamically adapt\nthe model to the different weather conditions, while hyperbolic space\nprototypes are used to align the heterogeneous client representations. Finally,\nwe introduce FLYAWARE, the first semantic segmentation dataset with adverse\nweather data for aerial vehicles.\n", "link": "http://arxiv.org/abs/2403.13762v2", "date": "2024-09-17", "relevancy": 2.0831, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Cars%20meet%20Drones%3A%20Hyperbolic%20Federated%20Learning%20for%20Source-Free%0A%20%20Domain%20Adaptation%20in%20Adverse%20Weather&body=Title%3A%20When%20Cars%20meet%20Drones%3A%20Hyperbolic%20Federated%20Learning%20for%20Source-Free%0A%20%20Domain%20Adaptation%20in%20Adverse%20Weather%0AAuthor%3A%20Giulia%20Rizzoli%20and%20Matteo%20Caligiuri%20and%20Donald%20Shenaj%20and%20Francesco%20Barbato%20and%20Pietro%20Zanuttigh%0AAbstract%3A%20%20%20In%20Federated%20Learning%20%28FL%29%2C%20multiple%20clients%20collaboratively%20train%20a%20global%0Amodel%20without%20sharing%20private%20data.%20In%20semantic%20segmentation%2C%20the%20Federated%0Asource%20Free%20Domain%20Adaptation%20%28FFreeDA%29%20setting%20is%20of%20particular%20interest%2C%0Awhere%20clients%20undergo%20unsupervised%20training%20after%20supervised%20pretraining%20at%20the%0Aserver%20side.%20While%20few%20recent%20works%20address%20FL%20for%20autonomous%20vehicles%2C%0Aintrinsic%20real-world%20challenges%20such%20as%20the%20presence%20of%20adverse%20weather%0Aconditions%20and%20the%20existence%20of%20different%20autonomous%20agents%20are%20still%0Aunexplored.%20To%20bridge%20this%20gap%2C%20we%20address%20both%20problems%20and%20introduce%20a%20new%0Afederated%20semantic%20segmentation%20setting%20where%20both%20car%20and%20drone%20clients%0Aco-exist%20and%20collaborate.%20Specifically%2C%20we%20propose%20a%20novel%20approach%20for%20this%0Asetting%20which%20exploits%20a%20batch-norm%20weather-aware%20strategy%20to%20dynamically%20adapt%0Athe%20model%20to%20the%20different%20weather%20conditions%2C%20while%20hyperbolic%20space%0Aprototypes%20are%20used%20to%20align%20the%20heterogeneous%20client%20representations.%20Finally%2C%0Awe%20introduce%20FLYAWARE%2C%20the%20first%20semantic%20segmentation%20dataset%20with%20adverse%0Aweather%20data%20for%20aerial%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Cars%2520meet%2520Drones%253A%2520Hyperbolic%2520Federated%2520Learning%2520for%2520Source-Free%250A%2520%2520Domain%2520Adaptation%2520in%2520Adverse%2520Weather%26entry.906535625%3DGiulia%2520Rizzoli%2520and%2520Matteo%2520Caligiuri%2520and%2520Donald%2520Shenaj%2520and%2520Francesco%2520Barbato%2520and%2520Pietro%2520Zanuttigh%26entry.1292438233%3D%2520%2520In%2520Federated%2520Learning%2520%2528FL%2529%252C%2520multiple%2520clients%2520collaboratively%2520train%2520a%2520global%250Amodel%2520without%2520sharing%2520private%2520data.%2520In%2520semantic%2520segmentation%252C%2520the%2520Federated%250Asource%2520Free%2520Domain%2520Adaptation%2520%2528FFreeDA%2529%2520setting%2520is%2520of%2520particular%2520interest%252C%250Awhere%2520clients%2520undergo%2520unsupervised%2520training%2520after%2520supervised%2520pretraining%2520at%2520the%250Aserver%2520side.%2520While%2520few%2520recent%2520works%2520address%2520FL%2520for%2520autonomous%2520vehicles%252C%250Aintrinsic%2520real-world%2520challenges%2520such%2520as%2520the%2520presence%2520of%2520adverse%2520weather%250Aconditions%2520and%2520the%2520existence%2520of%2520different%2520autonomous%2520agents%2520are%2520still%250Aunexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520address%2520both%2520problems%2520and%2520introduce%2520a%2520new%250Afederated%2520semantic%2520segmentation%2520setting%2520where%2520both%2520car%2520and%2520drone%2520clients%250Aco-exist%2520and%2520collaborate.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520this%250Asetting%2520which%2520exploits%2520a%2520batch-norm%2520weather-aware%2520strategy%2520to%2520dynamically%2520adapt%250Athe%2520model%2520to%2520the%2520different%2520weather%2520conditions%252C%2520while%2520hyperbolic%2520space%250Aprototypes%2520are%2520used%2520to%2520align%2520the%2520heterogeneous%2520client%2520representations.%2520Finally%252C%250Awe%2520introduce%2520FLYAWARE%252C%2520the%2520first%2520semantic%2520segmentation%2520dataset%2520with%2520adverse%250Aweather%2520data%2520for%2520aerial%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Cars%20meet%20Drones%3A%20Hyperbolic%20Federated%20Learning%20for%20Source-Free%0A%20%20Domain%20Adaptation%20in%20Adverse%20Weather&entry.906535625=Giulia%20Rizzoli%20and%20Matteo%20Caligiuri%20and%20Donald%20Shenaj%20and%20Francesco%20Barbato%20and%20Pietro%20Zanuttigh&entry.1292438233=%20%20In%20Federated%20Learning%20%28FL%29%2C%20multiple%20clients%20collaboratively%20train%20a%20global%0Amodel%20without%20sharing%20private%20data.%20In%20semantic%20segmentation%2C%20the%20Federated%0Asource%20Free%20Domain%20Adaptation%20%28FFreeDA%29%20setting%20is%20of%20particular%20interest%2C%0Awhere%20clients%20undergo%20unsupervised%20training%20after%20supervised%20pretraining%20at%20the%0Aserver%20side.%20While%20few%20recent%20works%20address%20FL%20for%20autonomous%20vehicles%2C%0Aintrinsic%20real-world%20challenges%20such%20as%20the%20presence%20of%20adverse%20weather%0Aconditions%20and%20the%20existence%20of%20different%20autonomous%20agents%20are%20still%0Aunexplored.%20To%20bridge%20this%20gap%2C%20we%20address%20both%20problems%20and%20introduce%20a%20new%0Afederated%20semantic%20segmentation%20setting%20where%20both%20car%20and%20drone%20clients%0Aco-exist%20and%20collaborate.%20Specifically%2C%20we%20propose%20a%20novel%20approach%20for%20this%0Asetting%20which%20exploits%20a%20batch-norm%20weather-aware%20strategy%20to%20dynamically%20adapt%0Athe%20model%20to%20the%20different%20weather%20conditions%2C%20while%20hyperbolic%20space%0Aprototypes%20are%20used%20to%20align%20the%20heterogeneous%20client%20representations.%20Finally%2C%0Awe%20introduce%20FLYAWARE%2C%20the%20first%20semantic%20segmentation%20dataset%20with%20adverse%0Aweather%20data%20for%20aerial%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13762v2&entry.124074799=Read"},
{"title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative\n  Refinement", "author": "Simon Yu and Liangyu Chen and Sara Ahmadian and Marzieh Fadaee", "abstract": "  Finetuning large language models on instruction data is crucial for enhancing\npre-trained knowledge and improving instruction-following capabilities. As\ninstruction datasets proliferate, selecting optimal data for effective training\nbecomes increasingly important. This work addresses the question: How can we\ndetermine the optimal subset of data for effective training? While existing\nresearch often emphasizes local criteria like instance quality for subset\nselection, we argue that a global approach focused on data diversity is more\ncritical. Our method employs k-means clustering to ensure the selected subset\neffectively represents the full dataset. We propose an iterative refinement\nmethod inspired by active learning techniques to resample instances from\nclusters, reassessing each cluster's importance and sampling weight in every\ntraining iteration. This approach reduces the effect of outliers and\nautomatically filters out clusters containing low-quality data. Through\nextensive evaluation across natural language reasoning, general world\nknowledge, code and math reasoning tasks, and by fine-tuning models from\nvarious families, we observe consistent improvements, achieving a 7% increase\nover random selection and a 3.8% improvement over state-of-the-art sampling\nmethods. Our work highlights the significance of diversity-first sampling when\nfinetuning LLMs to enhance performance across a broad array of evaluation\ntasks. Our code is available at\nhttps://github.com/for-ai/iterative-data-selection.\n", "link": "http://arxiv.org/abs/2409.11378v1", "date": "2024-09-17", "relevancy": 2.0788, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5157}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversify%20and%20Conquer%3A%20Diversity-Centric%20Data%20Selection%20with%20Iterative%0A%20%20Refinement&body=Title%3A%20Diversify%20and%20Conquer%3A%20Diversity-Centric%20Data%20Selection%20with%20Iterative%0A%20%20Refinement%0AAuthor%3A%20Simon%20Yu%20and%20Liangyu%20Chen%20and%20Sara%20Ahmadian%20and%20Marzieh%20Fadaee%0AAbstract%3A%20%20%20Finetuning%20large%20language%20models%20on%20instruction%20data%20is%20crucial%20for%20enhancing%0Apre-trained%20knowledge%20and%20improving%20instruction-following%20capabilities.%20As%0Ainstruction%20datasets%20proliferate%2C%20selecting%20optimal%20data%20for%20effective%20training%0Abecomes%20increasingly%20important.%20This%20work%20addresses%20the%20question%3A%20How%20can%20we%0Adetermine%20the%20optimal%20subset%20of%20data%20for%20effective%20training%3F%20While%20existing%0Aresearch%20often%20emphasizes%20local%20criteria%20like%20instance%20quality%20for%20subset%0Aselection%2C%20we%20argue%20that%20a%20global%20approach%20focused%20on%20data%20diversity%20is%20more%0Acritical.%20Our%20method%20employs%20k-means%20clustering%20to%20ensure%20the%20selected%20subset%0Aeffectively%20represents%20the%20full%20dataset.%20We%20propose%20an%20iterative%20refinement%0Amethod%20inspired%20by%20active%20learning%20techniques%20to%20resample%20instances%20from%0Aclusters%2C%20reassessing%20each%20cluster%27s%20importance%20and%20sampling%20weight%20in%20every%0Atraining%20iteration.%20This%20approach%20reduces%20the%20effect%20of%20outliers%20and%0Aautomatically%20filters%20out%20clusters%20containing%20low-quality%20data.%20Through%0Aextensive%20evaluation%20across%20natural%20language%20reasoning%2C%20general%20world%0Aknowledge%2C%20code%20and%20math%20reasoning%20tasks%2C%20and%20by%20fine-tuning%20models%20from%0Avarious%20families%2C%20we%20observe%20consistent%20improvements%2C%20achieving%20a%207%25%20increase%0Aover%20random%20selection%20and%20a%203.8%25%20improvement%20over%20state-of-the-art%20sampling%0Amethods.%20Our%20work%20highlights%20the%20significance%20of%20diversity-first%20sampling%20when%0Afinetuning%20LLMs%20to%20enhance%20performance%20across%20a%20broad%20array%20of%20evaluation%0Atasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/for-ai/iterative-data-selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversify%2520and%2520Conquer%253A%2520Diversity-Centric%2520Data%2520Selection%2520with%2520Iterative%250A%2520%2520Refinement%26entry.906535625%3DSimon%2520Yu%2520and%2520Liangyu%2520Chen%2520and%2520Sara%2520Ahmadian%2520and%2520Marzieh%2520Fadaee%26entry.1292438233%3D%2520%2520Finetuning%2520large%2520language%2520models%2520on%2520instruction%2520data%2520is%2520crucial%2520for%2520enhancing%250Apre-trained%2520knowledge%2520and%2520improving%2520instruction-following%2520capabilities.%2520As%250Ainstruction%2520datasets%2520proliferate%252C%2520selecting%2520optimal%2520data%2520for%2520effective%2520training%250Abecomes%2520increasingly%2520important.%2520This%2520work%2520addresses%2520the%2520question%253A%2520How%2520can%2520we%250Adetermine%2520the%2520optimal%2520subset%2520of%2520data%2520for%2520effective%2520training%253F%2520While%2520existing%250Aresearch%2520often%2520emphasizes%2520local%2520criteria%2520like%2520instance%2520quality%2520for%2520subset%250Aselection%252C%2520we%2520argue%2520that%2520a%2520global%2520approach%2520focused%2520on%2520data%2520diversity%2520is%2520more%250Acritical.%2520Our%2520method%2520employs%2520k-means%2520clustering%2520to%2520ensure%2520the%2520selected%2520subset%250Aeffectively%2520represents%2520the%2520full%2520dataset.%2520We%2520propose%2520an%2520iterative%2520refinement%250Amethod%2520inspired%2520by%2520active%2520learning%2520techniques%2520to%2520resample%2520instances%2520from%250Aclusters%252C%2520reassessing%2520each%2520cluster%2527s%2520importance%2520and%2520sampling%2520weight%2520in%2520every%250Atraining%2520iteration.%2520This%2520approach%2520reduces%2520the%2520effect%2520of%2520outliers%2520and%250Aautomatically%2520filters%2520out%2520clusters%2520containing%2520low-quality%2520data.%2520Through%250Aextensive%2520evaluation%2520across%2520natural%2520language%2520reasoning%252C%2520general%2520world%250Aknowledge%252C%2520code%2520and%2520math%2520reasoning%2520tasks%252C%2520and%2520by%2520fine-tuning%2520models%2520from%250Avarious%2520families%252C%2520we%2520observe%2520consistent%2520improvements%252C%2520achieving%2520a%25207%2525%2520increase%250Aover%2520random%2520selection%2520and%2520a%25203.8%2525%2520improvement%2520over%2520state-of-the-art%2520sampling%250Amethods.%2520Our%2520work%2520highlights%2520the%2520significance%2520of%2520diversity-first%2520sampling%2520when%250Afinetuning%2520LLMs%2520to%2520enhance%2520performance%2520across%2520a%2520broad%2520array%2520of%2520evaluation%250Atasks.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/for-ai/iterative-data-selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversify%20and%20Conquer%3A%20Diversity-Centric%20Data%20Selection%20with%20Iterative%0A%20%20Refinement&entry.906535625=Simon%20Yu%20and%20Liangyu%20Chen%20and%20Sara%20Ahmadian%20and%20Marzieh%20Fadaee&entry.1292438233=%20%20Finetuning%20large%20language%20models%20on%20instruction%20data%20is%20crucial%20for%20enhancing%0Apre-trained%20knowledge%20and%20improving%20instruction-following%20capabilities.%20As%0Ainstruction%20datasets%20proliferate%2C%20selecting%20optimal%20data%20for%20effective%20training%0Abecomes%20increasingly%20important.%20This%20work%20addresses%20the%20question%3A%20How%20can%20we%0Adetermine%20the%20optimal%20subset%20of%20data%20for%20effective%20training%3F%20While%20existing%0Aresearch%20often%20emphasizes%20local%20criteria%20like%20instance%20quality%20for%20subset%0Aselection%2C%20we%20argue%20that%20a%20global%20approach%20focused%20on%20data%20diversity%20is%20more%0Acritical.%20Our%20method%20employs%20k-means%20clustering%20to%20ensure%20the%20selected%20subset%0Aeffectively%20represents%20the%20full%20dataset.%20We%20propose%20an%20iterative%20refinement%0Amethod%20inspired%20by%20active%20learning%20techniques%20to%20resample%20instances%20from%0Aclusters%2C%20reassessing%20each%20cluster%27s%20importance%20and%20sampling%20weight%20in%20every%0Atraining%20iteration.%20This%20approach%20reduces%20the%20effect%20of%20outliers%20and%0Aautomatically%20filters%20out%20clusters%20containing%20low-quality%20data.%20Through%0Aextensive%20evaluation%20across%20natural%20language%20reasoning%2C%20general%20world%0Aknowledge%2C%20code%20and%20math%20reasoning%20tasks%2C%20and%20by%20fine-tuning%20models%20from%0Avarious%20families%2C%20we%20observe%20consistent%20improvements%2C%20achieving%20a%207%25%20increase%0Aover%20random%20selection%20and%20a%203.8%25%20improvement%20over%20state-of-the-art%20sampling%0Amethods.%20Our%20work%20highlights%20the%20significance%20of%20diversity-first%20sampling%20when%0Afinetuning%20LLMs%20to%20enhance%20performance%20across%20a%20broad%20array%20of%20evaluation%0Atasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/for-ai/iterative-data-selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11378v1&entry.124074799=Read"},
{"title": "Scale generalisation properties of extended scale-covariant and\n  scale-invariant Gaussian derivative networks on image datasets with spatial\n  scaling variations", "author": "Andrzej Perzanowski and Tony Lindeberg", "abstract": "  This paper presents an in-depth analysis of the scale generalisation\nproperties of the scale-covariant and scale-invariant Gaussian derivative\nnetworks, complemented with both conceptual and algorithmic extensions. For\nthis purpose, Gaussian derivative networks are evaluated on new rescaled\nversions of the Fashion-MNIST and the CIFAR-10 datasets, with spatial scaling\nvariations over a factor of 4 in the testing data, that are not present in the\ntraining data. Additionally, evaluations on the previously existing STIR\ndatasets show that the Gaussian derivative networks achieve better scale\ngeneralisation than previously reported for these datasets for other types of\ndeep networks.\n  We first experimentally demonstrate that the Gaussian derivative networks\nhave quite good scale generalisation properties on the new datasets, and that\naverage pooling of feature responses over scales may sometimes also lead to\nbetter results than the previously used approach of max pooling over scales.\nThen, we demonstrate that using a spatial max pooling mechanism after the final\nlayer enables localisation of non-centred objects in image domain, with\nmaintained scale generalisation properties. We also show that regularisation\nduring training, by applying dropout across the scale channels, referred to as\nscale-channel dropout, improves both the performance and the scale\ngeneralisation.\n  In additional ablation studies, we demonstrate that discretisations of\nGaussian derivative networks, based on the discrete analogue of the Gaussian\nkernel in combination with central difference operators, perform best or among\nthe best, compared to a set of other discrete approximations of the Gaussian\nderivative kernels.\n  Finally, by visualising the activation maps and the learned receptive fields,\nwe demonstrate that the Gaussian derivative networks have very good\nexplainability properties.\n", "link": "http://arxiv.org/abs/2409.11140v1", "date": "2024-09-17", "relevancy": 2.0786, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5635}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5146}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale%20generalisation%20properties%20of%20extended%20scale-covariant%20and%0A%20%20scale-invariant%20Gaussian%20derivative%20networks%20on%20image%20datasets%20with%20spatial%0A%20%20scaling%20variations&body=Title%3A%20Scale%20generalisation%20properties%20of%20extended%20scale-covariant%20and%0A%20%20scale-invariant%20Gaussian%20derivative%20networks%20on%20image%20datasets%20with%20spatial%0A%20%20scaling%20variations%0AAuthor%3A%20Andrzej%20Perzanowski%20and%20Tony%20Lindeberg%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20in-depth%20analysis%20of%20the%20scale%20generalisation%0Aproperties%20of%20the%20scale-covariant%20and%20scale-invariant%20Gaussian%20derivative%0Anetworks%2C%20complemented%20with%20both%20conceptual%20and%20algorithmic%20extensions.%20For%0Athis%20purpose%2C%20Gaussian%20derivative%20networks%20are%20evaluated%20on%20new%20rescaled%0Aversions%20of%20the%20Fashion-MNIST%20and%20the%20CIFAR-10%20datasets%2C%20with%20spatial%20scaling%0Avariations%20over%20a%20factor%20of%204%20in%20the%20testing%20data%2C%20that%20are%20not%20present%20in%20the%0Atraining%20data.%20Additionally%2C%20evaluations%20on%20the%20previously%20existing%20STIR%0Adatasets%20show%20that%20the%20Gaussian%20derivative%20networks%20achieve%20better%20scale%0Ageneralisation%20than%20previously%20reported%20for%20these%20datasets%20for%20other%20types%20of%0Adeep%20networks.%0A%20%20We%20first%20experimentally%20demonstrate%20that%20the%20Gaussian%20derivative%20networks%0Ahave%20quite%20good%20scale%20generalisation%20properties%20on%20the%20new%20datasets%2C%20and%20that%0Aaverage%20pooling%20of%20feature%20responses%20over%20scales%20may%20sometimes%20also%20lead%20to%0Abetter%20results%20than%20the%20previously%20used%20approach%20of%20max%20pooling%20over%20scales.%0AThen%2C%20we%20demonstrate%20that%20using%20a%20spatial%20max%20pooling%20mechanism%20after%20the%20final%0Alayer%20enables%20localisation%20of%20non-centred%20objects%20in%20image%20domain%2C%20with%0Amaintained%20scale%20generalisation%20properties.%20We%20also%20show%20that%20regularisation%0Aduring%20training%2C%20by%20applying%20dropout%20across%20the%20scale%20channels%2C%20referred%20to%20as%0Ascale-channel%20dropout%2C%20improves%20both%20the%20performance%20and%20the%20scale%0Ageneralisation.%0A%20%20In%20additional%20ablation%20studies%2C%20we%20demonstrate%20that%20discretisations%20of%0AGaussian%20derivative%20networks%2C%20based%20on%20the%20discrete%20analogue%20of%20the%20Gaussian%0Akernel%20in%20combination%20with%20central%20difference%20operators%2C%20perform%20best%20or%20among%0Athe%20best%2C%20compared%20to%20a%20set%20of%20other%20discrete%20approximations%20of%20the%20Gaussian%0Aderivative%20kernels.%0A%20%20Finally%2C%20by%20visualising%20the%20activation%20maps%20and%20the%20learned%20receptive%20fields%2C%0Awe%20demonstrate%20that%20the%20Gaussian%20derivative%20networks%20have%20very%20good%0Aexplainability%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale%2520generalisation%2520properties%2520of%2520extended%2520scale-covariant%2520and%250A%2520%2520scale-invariant%2520Gaussian%2520derivative%2520networks%2520on%2520image%2520datasets%2520with%2520spatial%250A%2520%2520scaling%2520variations%26entry.906535625%3DAndrzej%2520Perzanowski%2520and%2520Tony%2520Lindeberg%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520in-depth%2520analysis%2520of%2520the%2520scale%2520generalisation%250Aproperties%2520of%2520the%2520scale-covariant%2520and%2520scale-invariant%2520Gaussian%2520derivative%250Anetworks%252C%2520complemented%2520with%2520both%2520conceptual%2520and%2520algorithmic%2520extensions.%2520For%250Athis%2520purpose%252C%2520Gaussian%2520derivative%2520networks%2520are%2520evaluated%2520on%2520new%2520rescaled%250Aversions%2520of%2520the%2520Fashion-MNIST%2520and%2520the%2520CIFAR-10%2520datasets%252C%2520with%2520spatial%2520scaling%250Avariations%2520over%2520a%2520factor%2520of%25204%2520in%2520the%2520testing%2520data%252C%2520that%2520are%2520not%2520present%2520in%2520the%250Atraining%2520data.%2520Additionally%252C%2520evaluations%2520on%2520the%2520previously%2520existing%2520STIR%250Adatasets%2520show%2520that%2520the%2520Gaussian%2520derivative%2520networks%2520achieve%2520better%2520scale%250Ageneralisation%2520than%2520previously%2520reported%2520for%2520these%2520datasets%2520for%2520other%2520types%2520of%250Adeep%2520networks.%250A%2520%2520We%2520first%2520experimentally%2520demonstrate%2520that%2520the%2520Gaussian%2520derivative%2520networks%250Ahave%2520quite%2520good%2520scale%2520generalisation%2520properties%2520on%2520the%2520new%2520datasets%252C%2520and%2520that%250Aaverage%2520pooling%2520of%2520feature%2520responses%2520over%2520scales%2520may%2520sometimes%2520also%2520lead%2520to%250Abetter%2520results%2520than%2520the%2520previously%2520used%2520approach%2520of%2520max%2520pooling%2520over%2520scales.%250AThen%252C%2520we%2520demonstrate%2520that%2520using%2520a%2520spatial%2520max%2520pooling%2520mechanism%2520after%2520the%2520final%250Alayer%2520enables%2520localisation%2520of%2520non-centred%2520objects%2520in%2520image%2520domain%252C%2520with%250Amaintained%2520scale%2520generalisation%2520properties.%2520We%2520also%2520show%2520that%2520regularisation%250Aduring%2520training%252C%2520by%2520applying%2520dropout%2520across%2520the%2520scale%2520channels%252C%2520referred%2520to%2520as%250Ascale-channel%2520dropout%252C%2520improves%2520both%2520the%2520performance%2520and%2520the%2520scale%250Ageneralisation.%250A%2520%2520In%2520additional%2520ablation%2520studies%252C%2520we%2520demonstrate%2520that%2520discretisations%2520of%250AGaussian%2520derivative%2520networks%252C%2520based%2520on%2520the%2520discrete%2520analogue%2520of%2520the%2520Gaussian%250Akernel%2520in%2520combination%2520with%2520central%2520difference%2520operators%252C%2520perform%2520best%2520or%2520among%250Athe%2520best%252C%2520compared%2520to%2520a%2520set%2520of%2520other%2520discrete%2520approximations%2520of%2520the%2520Gaussian%250Aderivative%2520kernels.%250A%2520%2520Finally%252C%2520by%2520visualising%2520the%2520activation%2520maps%2520and%2520the%2520learned%2520receptive%2520fields%252C%250Awe%2520demonstrate%2520that%2520the%2520Gaussian%2520derivative%2520networks%2520have%2520very%2520good%250Aexplainability%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale%20generalisation%20properties%20of%20extended%20scale-covariant%20and%0A%20%20scale-invariant%20Gaussian%20derivative%20networks%20on%20image%20datasets%20with%20spatial%0A%20%20scaling%20variations&entry.906535625=Andrzej%20Perzanowski%20and%20Tony%20Lindeberg&entry.1292438233=%20%20This%20paper%20presents%20an%20in-depth%20analysis%20of%20the%20scale%20generalisation%0Aproperties%20of%20the%20scale-covariant%20and%20scale-invariant%20Gaussian%20derivative%0Anetworks%2C%20complemented%20with%20both%20conceptual%20and%20algorithmic%20extensions.%20For%0Athis%20purpose%2C%20Gaussian%20derivative%20networks%20are%20evaluated%20on%20new%20rescaled%0Aversions%20of%20the%20Fashion-MNIST%20and%20the%20CIFAR-10%20datasets%2C%20with%20spatial%20scaling%0Avariations%20over%20a%20factor%20of%204%20in%20the%20testing%20data%2C%20that%20are%20not%20present%20in%20the%0Atraining%20data.%20Additionally%2C%20evaluations%20on%20the%20previously%20existing%20STIR%0Adatasets%20show%20that%20the%20Gaussian%20derivative%20networks%20achieve%20better%20scale%0Ageneralisation%20than%20previously%20reported%20for%20these%20datasets%20for%20other%20types%20of%0Adeep%20networks.%0A%20%20We%20first%20experimentally%20demonstrate%20that%20the%20Gaussian%20derivative%20networks%0Ahave%20quite%20good%20scale%20generalisation%20properties%20on%20the%20new%20datasets%2C%20and%20that%0Aaverage%20pooling%20of%20feature%20responses%20over%20scales%20may%20sometimes%20also%20lead%20to%0Abetter%20results%20than%20the%20previously%20used%20approach%20of%20max%20pooling%20over%20scales.%0AThen%2C%20we%20demonstrate%20that%20using%20a%20spatial%20max%20pooling%20mechanism%20after%20the%20final%0Alayer%20enables%20localisation%20of%20non-centred%20objects%20in%20image%20domain%2C%20with%0Amaintained%20scale%20generalisation%20properties.%20We%20also%20show%20that%20regularisation%0Aduring%20training%2C%20by%20applying%20dropout%20across%20the%20scale%20channels%2C%20referred%20to%20as%0Ascale-channel%20dropout%2C%20improves%20both%20the%20performance%20and%20the%20scale%0Ageneralisation.%0A%20%20In%20additional%20ablation%20studies%2C%20we%20demonstrate%20that%20discretisations%20of%0AGaussian%20derivative%20networks%2C%20based%20on%20the%20discrete%20analogue%20of%20the%20Gaussian%0Akernel%20in%20combination%20with%20central%20difference%20operators%2C%20perform%20best%20or%20among%0Athe%20best%2C%20compared%20to%20a%20set%20of%20other%20discrete%20approximations%20of%20the%20Gaussian%0Aderivative%20kernels.%0A%20%20Finally%2C%20by%20visualising%20the%20activation%20maps%20and%20the%20learned%20receptive%20fields%2C%0Awe%20demonstrate%20that%20the%20Gaussian%20derivative%20networks%20have%20very%20good%0Aexplainability%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11140v1&entry.124074799=Read"},
{"title": "Diversity-grounded Channel Prototypical Learning for Out-of-Distribution\n  Intent Detection", "author": "Bo Liu and Liming Zhan and Yujie Feng and Zexin Lu and Chengqiang Xie and Lei Xue and Xiao-Ming Wu and Albert Y. S. Lam", "abstract": "  In the realm of task-oriented dialogue systems, a robust intent detection\nmechanism must effectively handle malformed utterances encountered in\nreal-world scenarios. This study presents a novel fine-tuning framework for\nlarge language models (LLMs) aimed at enhancing in-distribution (ID) intent\nclassification and out-of-distribution (OOD) intent detection, which utilizes\nsemantic matching with prototypes derived from ID class names. By harnessing\nthe highly distinguishable representations of LLMs, we construct semantic\nprototypes for each ID class using a diversity-grounded prompt tuning approach.\nWe rigorously test our framework in a challenging OOD context, where ID and OOD\nclasses are semantically close yet distinct, referred to as \\emph{near} OOD\ndetection. For a thorough assessment, we benchmark our method against the\nprevalent fine-tuning approaches. The experimental findings reveal that our\nmethod demonstrates superior performance in both few-shot ID intent\nclassification and near-OOD intent detection tasks.\n", "link": "http://arxiv.org/abs/2409.11114v1", "date": "2024-09-17", "relevancy": 2.0651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversity-grounded%20Channel%20Prototypical%20Learning%20for%20Out-of-Distribution%0A%20%20Intent%20Detection&body=Title%3A%20Diversity-grounded%20Channel%20Prototypical%20Learning%20for%20Out-of-Distribution%0A%20%20Intent%20Detection%0AAuthor%3A%20Bo%20Liu%20and%20Liming%20Zhan%20and%20Yujie%20Feng%20and%20Zexin%20Lu%20and%20Chengqiang%20Xie%20and%20Lei%20Xue%20and%20Xiao-Ming%20Wu%20and%20Albert%20Y.%20S.%20Lam%0AAbstract%3A%20%20%20In%20the%20realm%20of%20task-oriented%20dialogue%20systems%2C%20a%20robust%20intent%20detection%0Amechanism%20must%20effectively%20handle%20malformed%20utterances%20encountered%20in%0Areal-world%20scenarios.%20This%20study%20presents%20a%20novel%20fine-tuning%20framework%20for%0Alarge%20language%20models%20%28LLMs%29%20aimed%20at%20enhancing%20in-distribution%20%28ID%29%20intent%0Aclassification%20and%20out-of-distribution%20%28OOD%29%20intent%20detection%2C%20which%20utilizes%0Asemantic%20matching%20with%20prototypes%20derived%20from%20ID%20class%20names.%20By%20harnessing%0Athe%20highly%20distinguishable%20representations%20of%20LLMs%2C%20we%20construct%20semantic%0Aprototypes%20for%20each%20ID%20class%20using%20a%20diversity-grounded%20prompt%20tuning%20approach.%0AWe%20rigorously%20test%20our%20framework%20in%20a%20challenging%20OOD%20context%2C%20where%20ID%20and%20OOD%0Aclasses%20are%20semantically%20close%20yet%20distinct%2C%20referred%20to%20as%20%5Cemph%7Bnear%7D%20OOD%0Adetection.%20For%20a%20thorough%20assessment%2C%20we%20benchmark%20our%20method%20against%20the%0Aprevalent%20fine-tuning%20approaches.%20The%20experimental%20findings%20reveal%20that%20our%0Amethod%20demonstrates%20superior%20performance%20in%20both%20few-shot%20ID%20intent%0Aclassification%20and%20near-OOD%20intent%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversity-grounded%2520Channel%2520Prototypical%2520Learning%2520for%2520Out-of-Distribution%250A%2520%2520Intent%2520Detection%26entry.906535625%3DBo%2520Liu%2520and%2520Liming%2520Zhan%2520and%2520Yujie%2520Feng%2520and%2520Zexin%2520Lu%2520and%2520Chengqiang%2520Xie%2520and%2520Lei%2520Xue%2520and%2520Xiao-Ming%2520Wu%2520and%2520Albert%2520Y.%2520S.%2520Lam%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520task-oriented%2520dialogue%2520systems%252C%2520a%2520robust%2520intent%2520detection%250Amechanism%2520must%2520effectively%2520handle%2520malformed%2520utterances%2520encountered%2520in%250Areal-world%2520scenarios.%2520This%2520study%2520presents%2520a%2520novel%2520fine-tuning%2520framework%2520for%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520aimed%2520at%2520enhancing%2520in-distribution%2520%2528ID%2529%2520intent%250Aclassification%2520and%2520out-of-distribution%2520%2528OOD%2529%2520intent%2520detection%252C%2520which%2520utilizes%250Asemantic%2520matching%2520with%2520prototypes%2520derived%2520from%2520ID%2520class%2520names.%2520By%2520harnessing%250Athe%2520highly%2520distinguishable%2520representations%2520of%2520LLMs%252C%2520we%2520construct%2520semantic%250Aprototypes%2520for%2520each%2520ID%2520class%2520using%2520a%2520diversity-grounded%2520prompt%2520tuning%2520approach.%250AWe%2520rigorously%2520test%2520our%2520framework%2520in%2520a%2520challenging%2520OOD%2520context%252C%2520where%2520ID%2520and%2520OOD%250Aclasses%2520are%2520semantically%2520close%2520yet%2520distinct%252C%2520referred%2520to%2520as%2520%255Cemph%257Bnear%257D%2520OOD%250Adetection.%2520For%2520a%2520thorough%2520assessment%252C%2520we%2520benchmark%2520our%2520method%2520against%2520the%250Aprevalent%2520fine-tuning%2520approaches.%2520The%2520experimental%2520findings%2520reveal%2520that%2520our%250Amethod%2520demonstrates%2520superior%2520performance%2520in%2520both%2520few-shot%2520ID%2520intent%250Aclassification%2520and%2520near-OOD%2520intent%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversity-grounded%20Channel%20Prototypical%20Learning%20for%20Out-of-Distribution%0A%20%20Intent%20Detection&entry.906535625=Bo%20Liu%20and%20Liming%20Zhan%20and%20Yujie%20Feng%20and%20Zexin%20Lu%20and%20Chengqiang%20Xie%20and%20Lei%20Xue%20and%20Xiao-Ming%20Wu%20and%20Albert%20Y.%20S.%20Lam&entry.1292438233=%20%20In%20the%20realm%20of%20task-oriented%20dialogue%20systems%2C%20a%20robust%20intent%20detection%0Amechanism%20must%20effectively%20handle%20malformed%20utterances%20encountered%20in%0Areal-world%20scenarios.%20This%20study%20presents%20a%20novel%20fine-tuning%20framework%20for%0Alarge%20language%20models%20%28LLMs%29%20aimed%20at%20enhancing%20in-distribution%20%28ID%29%20intent%0Aclassification%20and%20out-of-distribution%20%28OOD%29%20intent%20detection%2C%20which%20utilizes%0Asemantic%20matching%20with%20prototypes%20derived%20from%20ID%20class%20names.%20By%20harnessing%0Athe%20highly%20distinguishable%20representations%20of%20LLMs%2C%20we%20construct%20semantic%0Aprototypes%20for%20each%20ID%20class%20using%20a%20diversity-grounded%20prompt%20tuning%20approach.%0AWe%20rigorously%20test%20our%20framework%20in%20a%20challenging%20OOD%20context%2C%20where%20ID%20and%20OOD%0Aclasses%20are%20semantically%20close%20yet%20distinct%2C%20referred%20to%20as%20%5Cemph%7Bnear%7D%20OOD%0Adetection.%20For%20a%20thorough%20assessment%2C%20we%20benchmark%20our%20method%20against%20the%0Aprevalent%20fine-tuning%20approaches.%20The%20experimental%20findings%20reveal%20that%20our%0Amethod%20demonstrates%20superior%20performance%20in%20both%20few-shot%20ID%20intent%0Aclassification%20and%20near-OOD%20intent%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11114v1&entry.124074799=Read"},
{"title": "TopoMaskV2: Enhanced Instance-Mask-Based Formulation for the Road\n  Topology Problem", "author": "M. Esat Kalfaoglu and Halil Ibrahim Ozturk and Ozsel Kilinc and Alptekin Temizel", "abstract": "  Recently, the centerline has become a popular representation of lanes due to\nits advantages in solving the road topology problem. To enhance centerline\nprediction, we have developed a new approach called TopoMask. Unlike previous\nmethods that rely on keypoints or parametric methods, TopoMask utilizes an\ninstance-mask-based formulation coupled with a masked-attention-based\ntransformer architecture. We introduce a quad-direction label representation to\nenrich the mask instances with flow information and design a corresponding\npost-processing technique for mask-to-centerline conversion. Additionally, we\ndemonstrate that the instance-mask formulation provides complementary\ninformation to parametric Bezier regressions, and fusing both outputs leads to\nimproved detection and topology performance. Moreover, we analyze the\nshortcomings of the pillar assumption in the Lift Splat technique and adapt a\nmulti-height bin configuration. Experimental results show that TopoMask\nachieves state-of-the-art performance in the OpenLane-V2 dataset, increasing\nfrom 44.1 to 49.4 for Subset-A and 44.7 to 51.8 for Subset-B in the V1.1 OLS\nbaseline.\n", "link": "http://arxiv.org/abs/2409.11325v1", "date": "2024-09-17", "relevancy": 2.0594, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoMaskV2%3A%20Enhanced%20Instance-Mask-Based%20Formulation%20for%20the%20Road%0A%20%20Topology%20Problem&body=Title%3A%20TopoMaskV2%3A%20Enhanced%20Instance-Mask-Based%20Formulation%20for%20the%20Road%0A%20%20Topology%20Problem%0AAuthor%3A%20M.%20Esat%20Kalfaoglu%20and%20Halil%20Ibrahim%20Ozturk%20and%20Ozsel%20Kilinc%20and%20Alptekin%20Temizel%0AAbstract%3A%20%20%20Recently%2C%20the%20centerline%20has%20become%20a%20popular%20representation%20of%20lanes%20due%20to%0Aits%20advantages%20in%20solving%20the%20road%20topology%20problem.%20To%20enhance%20centerline%0Aprediction%2C%20we%20have%20developed%20a%20new%20approach%20called%20TopoMask.%20Unlike%20previous%0Amethods%20that%20rely%20on%20keypoints%20or%20parametric%20methods%2C%20TopoMask%20utilizes%20an%0Ainstance-mask-based%20formulation%20coupled%20with%20a%20masked-attention-based%0Atransformer%20architecture.%20We%20introduce%20a%20quad-direction%20label%20representation%20to%0Aenrich%20the%20mask%20instances%20with%20flow%20information%20and%20design%20a%20corresponding%0Apost-processing%20technique%20for%20mask-to-centerline%20conversion.%20Additionally%2C%20we%0Ademonstrate%20that%20the%20instance-mask%20formulation%20provides%20complementary%0Ainformation%20to%20parametric%20Bezier%20regressions%2C%20and%20fusing%20both%20outputs%20leads%20to%0Aimproved%20detection%20and%20topology%20performance.%20Moreover%2C%20we%20analyze%20the%0Ashortcomings%20of%20the%20pillar%20assumption%20in%20the%20Lift%20Splat%20technique%20and%20adapt%20a%0Amulti-height%20bin%20configuration.%20Experimental%20results%20show%20that%20TopoMask%0Aachieves%20state-of-the-art%20performance%20in%20the%20OpenLane-V2%20dataset%2C%20increasing%0Afrom%2044.1%20to%2049.4%20for%20Subset-A%20and%2044.7%20to%2051.8%20for%20Subset-B%20in%20the%20V1.1%20OLS%0Abaseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoMaskV2%253A%2520Enhanced%2520Instance-Mask-Based%2520Formulation%2520for%2520the%2520Road%250A%2520%2520Topology%2520Problem%26entry.906535625%3DM.%2520Esat%2520Kalfaoglu%2520and%2520Halil%2520Ibrahim%2520Ozturk%2520and%2520Ozsel%2520Kilinc%2520and%2520Alptekin%2520Temizel%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520centerline%2520has%2520become%2520a%2520popular%2520representation%2520of%2520lanes%2520due%2520to%250Aits%2520advantages%2520in%2520solving%2520the%2520road%2520topology%2520problem.%2520To%2520enhance%2520centerline%250Aprediction%252C%2520we%2520have%2520developed%2520a%2520new%2520approach%2520called%2520TopoMask.%2520Unlike%2520previous%250Amethods%2520that%2520rely%2520on%2520keypoints%2520or%2520parametric%2520methods%252C%2520TopoMask%2520utilizes%2520an%250Ainstance-mask-based%2520formulation%2520coupled%2520with%2520a%2520masked-attention-based%250Atransformer%2520architecture.%2520We%2520introduce%2520a%2520quad-direction%2520label%2520representation%2520to%250Aenrich%2520the%2520mask%2520instances%2520with%2520flow%2520information%2520and%2520design%2520a%2520corresponding%250Apost-processing%2520technique%2520for%2520mask-to-centerline%2520conversion.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520the%2520instance-mask%2520formulation%2520provides%2520complementary%250Ainformation%2520to%2520parametric%2520Bezier%2520regressions%252C%2520and%2520fusing%2520both%2520outputs%2520leads%2520to%250Aimproved%2520detection%2520and%2520topology%2520performance.%2520Moreover%252C%2520we%2520analyze%2520the%250Ashortcomings%2520of%2520the%2520pillar%2520assumption%2520in%2520the%2520Lift%2520Splat%2520technique%2520and%2520adapt%2520a%250Amulti-height%2520bin%2520configuration.%2520Experimental%2520results%2520show%2520that%2520TopoMask%250Aachieves%2520state-of-the-art%2520performance%2520in%2520the%2520OpenLane-V2%2520dataset%252C%2520increasing%250Afrom%252044.1%2520to%252049.4%2520for%2520Subset-A%2520and%252044.7%2520to%252051.8%2520for%2520Subset-B%2520in%2520the%2520V1.1%2520OLS%250Abaseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoMaskV2%3A%20Enhanced%20Instance-Mask-Based%20Formulation%20for%20the%20Road%0A%20%20Topology%20Problem&entry.906535625=M.%20Esat%20Kalfaoglu%20and%20Halil%20Ibrahim%20Ozturk%20and%20Ozsel%20Kilinc%20and%20Alptekin%20Temizel&entry.1292438233=%20%20Recently%2C%20the%20centerline%20has%20become%20a%20popular%20representation%20of%20lanes%20due%20to%0Aits%20advantages%20in%20solving%20the%20road%20topology%20problem.%20To%20enhance%20centerline%0Aprediction%2C%20we%20have%20developed%20a%20new%20approach%20called%20TopoMask.%20Unlike%20previous%0Amethods%20that%20rely%20on%20keypoints%20or%20parametric%20methods%2C%20TopoMask%20utilizes%20an%0Ainstance-mask-based%20formulation%20coupled%20with%20a%20masked-attention-based%0Atransformer%20architecture.%20We%20introduce%20a%20quad-direction%20label%20representation%20to%0Aenrich%20the%20mask%20instances%20with%20flow%20information%20and%20design%20a%20corresponding%0Apost-processing%20technique%20for%20mask-to-centerline%20conversion.%20Additionally%2C%20we%0Ademonstrate%20that%20the%20instance-mask%20formulation%20provides%20complementary%0Ainformation%20to%20parametric%20Bezier%20regressions%2C%20and%20fusing%20both%20outputs%20leads%20to%0Aimproved%20detection%20and%20topology%20performance.%20Moreover%2C%20we%20analyze%20the%0Ashortcomings%20of%20the%20pillar%20assumption%20in%20the%20Lift%20Splat%20technique%20and%20adapt%20a%0Amulti-height%20bin%20configuration.%20Experimental%20results%20show%20that%20TopoMask%0Aachieves%20state-of-the-art%20performance%20in%20the%20OpenLane-V2%20dataset%2C%20increasing%0Afrom%2044.1%20to%2049.4%20for%20Subset-A%20and%2044.7%20to%2051.8%20for%20Subset-B%20in%20the%20V1.1%20OLS%0Abaseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11325v1&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language\n  Models: An Experimental Analysis up to 405B", "author": "Jemin Lee and Sihyeong Park and Jinse Kwon and Jihun Oh and Yongin Kwon", "abstract": "  Prior research works have evaluated quantized LLMs using limited metrics such\nas perplexity or a few basic knowledge tasks and old datasets. Additionally,\nrecent large-scale models such as Llama 3.1 with up to 405B have not been\nthoroughly examined. This paper evaluates the performance of instruction-tuned\nLLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on\nmodels ranging from 7B to 405B. Using 13 benchmarks, we assess performance\nacross six task types: commonsense Q\\&A, knowledge and language understanding,\ninstruction following, hallucination detection, mathematics, and dialogue. Our\nkey findings reveal that (1) quantizing a larger LLM to a similar size as a\nsmaller FP16 LLM generally performs better across most benchmarks, except for\nhallucination detection and instruction following; (2) performance varies\nsignificantly with different quantization methods, model size, and bit-width,\nwith weight-only methods often yielding better results in larger models; (3)\ntask difficulty does not significantly impact accuracy degradation due to\nquantization; and (4) the MT-Bench evaluation method has limited discriminatory\npower among recent high-performing LLMs.\n", "link": "http://arxiv.org/abs/2409.11055v1", "date": "2024-09-17", "relevancy": 2.0561, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20of%20Quantized%20Instruction-Tuned%20Large%20Language%0A%20%20Models%3A%20An%20Experimental%20Analysis%20up%20to%20405B&body=Title%3A%20A%20Comprehensive%20Evaluation%20of%20Quantized%20Instruction-Tuned%20Large%20Language%0A%20%20Models%3A%20An%20Experimental%20Analysis%20up%20to%20405B%0AAuthor%3A%20Jemin%20Lee%20and%20Sihyeong%20Park%20and%20Jinse%20Kwon%20and%20Jihun%20Oh%20and%20Yongin%20Kwon%0AAbstract%3A%20%20%20Prior%20research%20works%20have%20evaluated%20quantized%20LLMs%20using%20limited%20metrics%20such%0Aas%20perplexity%20or%20a%20few%20basic%20knowledge%20tasks%20and%20old%20datasets.%20Additionally%2C%0Arecent%20large-scale%20models%20such%20as%20Llama%203.1%20with%20up%20to%20405B%20have%20not%20been%0Athoroughly%20examined.%20This%20paper%20evaluates%20the%20performance%20of%20instruction-tuned%0ALLMs%20across%20various%20quantization%20methods%20%28GPTQ%2C%20AWQ%2C%20SmoothQuant%2C%20and%20FP8%29%20on%0Amodels%20ranging%20from%207B%20to%20405B.%20Using%2013%20benchmarks%2C%20we%20assess%20performance%0Aacross%20six%20task%20types%3A%20commonsense%20Q%5C%26A%2C%20knowledge%20and%20language%20understanding%2C%0Ainstruction%20following%2C%20hallucination%20detection%2C%20mathematics%2C%20and%20dialogue.%20Our%0Akey%20findings%20reveal%20that%20%281%29%20quantizing%20a%20larger%20LLM%20to%20a%20similar%20size%20as%20a%0Asmaller%20FP16%20LLM%20generally%20performs%20better%20across%20most%20benchmarks%2C%20except%20for%0Ahallucination%20detection%20and%20instruction%20following%3B%20%282%29%20performance%20varies%0Asignificantly%20with%20different%20quantization%20methods%2C%20model%20size%2C%20and%20bit-width%2C%0Awith%20weight-only%20methods%20often%20yielding%20better%20results%20in%20larger%20models%3B%20%283%29%0Atask%20difficulty%20does%20not%20significantly%20impact%20accuracy%20degradation%20due%20to%0Aquantization%3B%20and%20%284%29%20the%20MT-Bench%20evaluation%20method%20has%20limited%20discriminatory%0Apower%20among%20recent%20high-performing%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520of%2520Quantized%2520Instruction-Tuned%2520Large%2520Language%250A%2520%2520Models%253A%2520An%2520Experimental%2520Analysis%2520up%2520to%2520405B%26entry.906535625%3DJemin%2520Lee%2520and%2520Sihyeong%2520Park%2520and%2520Jinse%2520Kwon%2520and%2520Jihun%2520Oh%2520and%2520Yongin%2520Kwon%26entry.1292438233%3D%2520%2520Prior%2520research%2520works%2520have%2520evaluated%2520quantized%2520LLMs%2520using%2520limited%2520metrics%2520such%250Aas%2520perplexity%2520or%2520a%2520few%2520basic%2520knowledge%2520tasks%2520and%2520old%2520datasets.%2520Additionally%252C%250Arecent%2520large-scale%2520models%2520such%2520as%2520Llama%25203.1%2520with%2520up%2520to%2520405B%2520have%2520not%2520been%250Athoroughly%2520examined.%2520This%2520paper%2520evaluates%2520the%2520performance%2520of%2520instruction-tuned%250ALLMs%2520across%2520various%2520quantization%2520methods%2520%2528GPTQ%252C%2520AWQ%252C%2520SmoothQuant%252C%2520and%2520FP8%2529%2520on%250Amodels%2520ranging%2520from%25207B%2520to%2520405B.%2520Using%252013%2520benchmarks%252C%2520we%2520assess%2520performance%250Aacross%2520six%2520task%2520types%253A%2520commonsense%2520Q%255C%2526A%252C%2520knowledge%2520and%2520language%2520understanding%252C%250Ainstruction%2520following%252C%2520hallucination%2520detection%252C%2520mathematics%252C%2520and%2520dialogue.%2520Our%250Akey%2520findings%2520reveal%2520that%2520%25281%2529%2520quantizing%2520a%2520larger%2520LLM%2520to%2520a%2520similar%2520size%2520as%2520a%250Asmaller%2520FP16%2520LLM%2520generally%2520performs%2520better%2520across%2520most%2520benchmarks%252C%2520except%2520for%250Ahallucination%2520detection%2520and%2520instruction%2520following%253B%2520%25282%2529%2520performance%2520varies%250Asignificantly%2520with%2520different%2520quantization%2520methods%252C%2520model%2520size%252C%2520and%2520bit-width%252C%250Awith%2520weight-only%2520methods%2520often%2520yielding%2520better%2520results%2520in%2520larger%2520models%253B%2520%25283%2529%250Atask%2520difficulty%2520does%2520not%2520significantly%2520impact%2520accuracy%2520degradation%2520due%2520to%250Aquantization%253B%2520and%2520%25284%2529%2520the%2520MT-Bench%2520evaluation%2520method%2520has%2520limited%2520discriminatory%250Apower%2520among%2520recent%2520high-performing%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20of%20Quantized%20Instruction-Tuned%20Large%20Language%0A%20%20Models%3A%20An%20Experimental%20Analysis%20up%20to%20405B&entry.906535625=Jemin%20Lee%20and%20Sihyeong%20Park%20and%20Jinse%20Kwon%20and%20Jihun%20Oh%20and%20Yongin%20Kwon&entry.1292438233=%20%20Prior%20research%20works%20have%20evaluated%20quantized%20LLMs%20using%20limited%20metrics%20such%0Aas%20perplexity%20or%20a%20few%20basic%20knowledge%20tasks%20and%20old%20datasets.%20Additionally%2C%0Arecent%20large-scale%20models%20such%20as%20Llama%203.1%20with%20up%20to%20405B%20have%20not%20been%0Athoroughly%20examined.%20This%20paper%20evaluates%20the%20performance%20of%20instruction-tuned%0ALLMs%20across%20various%20quantization%20methods%20%28GPTQ%2C%20AWQ%2C%20SmoothQuant%2C%20and%20FP8%29%20on%0Amodels%20ranging%20from%207B%20to%20405B.%20Using%2013%20benchmarks%2C%20we%20assess%20performance%0Aacross%20six%20task%20types%3A%20commonsense%20Q%5C%26A%2C%20knowledge%20and%20language%20understanding%2C%0Ainstruction%20following%2C%20hallucination%20detection%2C%20mathematics%2C%20and%20dialogue.%20Our%0Akey%20findings%20reveal%20that%20%281%29%20quantizing%20a%20larger%20LLM%20to%20a%20similar%20size%20as%20a%0Asmaller%20FP16%20LLM%20generally%20performs%20better%20across%20most%20benchmarks%2C%20except%20for%0Ahallucination%20detection%20and%20instruction%20following%3B%20%282%29%20performance%20varies%0Asignificantly%20with%20different%20quantization%20methods%2C%20model%20size%2C%20and%20bit-width%2C%0Awith%20weight-only%20methods%20often%20yielding%20better%20results%20in%20larger%20models%3B%20%283%29%0Atask%20difficulty%20does%20not%20significantly%20impact%20accuracy%20degradation%20due%20to%0Aquantization%3B%20and%20%284%29%20the%20MT-Bench%20evaluation%20method%20has%20limited%20discriminatory%0Apower%20among%20recent%20high-performing%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11055v1&entry.124074799=Read"},
{"title": "Machine Learning on Dynamic Functional Connectivity: Promise, Pitfalls,\n  and Interpretations", "author": "Jiaqi Ding and Tingting Dan and Ziquan Wei and Hyuna Cho and Paul J. Laurienti and Won Hwa Kim and Guorong Wu", "abstract": "  An unprecedented amount of existing functional Magnetic Resonance Imaging\n(fMRI) data provides a new opportunity to understand the relationship between\nfunctional fluctuation and human cognition/behavior using a data-driven\napproach. To that end, tremendous efforts have been made in machine learning to\npredict cognitive states from evolving volumetric images of\nblood-oxygen-level-dependent (BOLD) signals. Due to the complex nature of brain\nfunction, however, the evaluation on learning performance and discoveries are\nnot often consistent across current state-of-the-arts (SOTA). By capitalizing\non large-scale existing neuroimaging data (34,887 data samples from six public\ndatabases), we seek to establish a well-founded empirical guideline for\ndesigning deep models for functional neuroimages by linking the methodology\nunderpinning with knowledge from the neuroscience domain. Specifically, we put\nthe spotlight on (1) What is the current SOTA performance in cognitive task\nrecognition and disease diagnosis using fMRI? (2) What are the limitations of\ncurrent deep models? and (3) What is the general guideline for selecting the\nsuitable machine learning backbone for new neuroimaging applications? We have\nconducted a comprehensive evaluation and statistical analysis, in various\nsettings, to answer the above outstanding questions.\n", "link": "http://arxiv.org/abs/2409.11377v1", "date": "2024-09-17", "relevancy": 2.0548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20on%20Dynamic%20Functional%20Connectivity%3A%20Promise%2C%20Pitfalls%2C%0A%20%20and%20Interpretations&body=Title%3A%20Machine%20Learning%20on%20Dynamic%20Functional%20Connectivity%3A%20Promise%2C%20Pitfalls%2C%0A%20%20and%20Interpretations%0AAuthor%3A%20Jiaqi%20Ding%20and%20Tingting%20Dan%20and%20Ziquan%20Wei%20and%20Hyuna%20Cho%20and%20Paul%20J.%20Laurienti%20and%20Won%20Hwa%20Kim%20and%20Guorong%20Wu%0AAbstract%3A%20%20%20An%20unprecedented%20amount%20of%20existing%20functional%20Magnetic%20Resonance%20Imaging%0A%28fMRI%29%20data%20provides%20a%20new%20opportunity%20to%20understand%20the%20relationship%20between%0Afunctional%20fluctuation%20and%20human%20cognition/behavior%20using%20a%20data-driven%0Aapproach.%20To%20that%20end%2C%20tremendous%20efforts%20have%20been%20made%20in%20machine%20learning%20to%0Apredict%20cognitive%20states%20from%20evolving%20volumetric%20images%20of%0Ablood-oxygen-level-dependent%20%28BOLD%29%20signals.%20Due%20to%20the%20complex%20nature%20of%20brain%0Afunction%2C%20however%2C%20the%20evaluation%20on%20learning%20performance%20and%20discoveries%20are%0Anot%20often%20consistent%20across%20current%20state-of-the-arts%20%28SOTA%29.%20By%20capitalizing%0Aon%20large-scale%20existing%20neuroimaging%20data%20%2834%2C887%20data%20samples%20from%20six%20public%0Adatabases%29%2C%20we%20seek%20to%20establish%20a%20well-founded%20empirical%20guideline%20for%0Adesigning%20deep%20models%20for%20functional%20neuroimages%20by%20linking%20the%20methodology%0Aunderpinning%20with%20knowledge%20from%20the%20neuroscience%20domain.%20Specifically%2C%20we%20put%0Athe%20spotlight%20on%20%281%29%20What%20is%20the%20current%20SOTA%20performance%20in%20cognitive%20task%0Arecognition%20and%20disease%20diagnosis%20using%20fMRI%3F%20%282%29%20What%20are%20the%20limitations%20of%0Acurrent%20deep%20models%3F%20and%20%283%29%20What%20is%20the%20general%20guideline%20for%20selecting%20the%0Asuitable%20machine%20learning%20backbone%20for%20new%20neuroimaging%20applications%3F%20We%20have%0Aconducted%20a%20comprehensive%20evaluation%20and%20statistical%20analysis%2C%20in%20various%0Asettings%2C%20to%20answer%20the%20above%20outstanding%20questions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520on%2520Dynamic%2520Functional%2520Connectivity%253A%2520Promise%252C%2520Pitfalls%252C%250A%2520%2520and%2520Interpretations%26entry.906535625%3DJiaqi%2520Ding%2520and%2520Tingting%2520Dan%2520and%2520Ziquan%2520Wei%2520and%2520Hyuna%2520Cho%2520and%2520Paul%2520J.%2520Laurienti%2520and%2520Won%2520Hwa%2520Kim%2520and%2520Guorong%2520Wu%26entry.1292438233%3D%2520%2520An%2520unprecedented%2520amount%2520of%2520existing%2520functional%2520Magnetic%2520Resonance%2520Imaging%250A%2528fMRI%2529%2520data%2520provides%2520a%2520new%2520opportunity%2520to%2520understand%2520the%2520relationship%2520between%250Afunctional%2520fluctuation%2520and%2520human%2520cognition/behavior%2520using%2520a%2520data-driven%250Aapproach.%2520To%2520that%2520end%252C%2520tremendous%2520efforts%2520have%2520been%2520made%2520in%2520machine%2520learning%2520to%250Apredict%2520cognitive%2520states%2520from%2520evolving%2520volumetric%2520images%2520of%250Ablood-oxygen-level-dependent%2520%2528BOLD%2529%2520signals.%2520Due%2520to%2520the%2520complex%2520nature%2520of%2520brain%250Afunction%252C%2520however%252C%2520the%2520evaluation%2520on%2520learning%2520performance%2520and%2520discoveries%2520are%250Anot%2520often%2520consistent%2520across%2520current%2520state-of-the-arts%2520%2528SOTA%2529.%2520By%2520capitalizing%250Aon%2520large-scale%2520existing%2520neuroimaging%2520data%2520%252834%252C887%2520data%2520samples%2520from%2520six%2520public%250Adatabases%2529%252C%2520we%2520seek%2520to%2520establish%2520a%2520well-founded%2520empirical%2520guideline%2520for%250Adesigning%2520deep%2520models%2520for%2520functional%2520neuroimages%2520by%2520linking%2520the%2520methodology%250Aunderpinning%2520with%2520knowledge%2520from%2520the%2520neuroscience%2520domain.%2520Specifically%252C%2520we%2520put%250Athe%2520spotlight%2520on%2520%25281%2529%2520What%2520is%2520the%2520current%2520SOTA%2520performance%2520in%2520cognitive%2520task%250Arecognition%2520and%2520disease%2520diagnosis%2520using%2520fMRI%253F%2520%25282%2529%2520What%2520are%2520the%2520limitations%2520of%250Acurrent%2520deep%2520models%253F%2520and%2520%25283%2529%2520What%2520is%2520the%2520general%2520guideline%2520for%2520selecting%2520the%250Asuitable%2520machine%2520learning%2520backbone%2520for%2520new%2520neuroimaging%2520applications%253F%2520We%2520have%250Aconducted%2520a%2520comprehensive%2520evaluation%2520and%2520statistical%2520analysis%252C%2520in%2520various%250Asettings%252C%2520to%2520answer%2520the%2520above%2520outstanding%2520questions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20on%20Dynamic%20Functional%20Connectivity%3A%20Promise%2C%20Pitfalls%2C%0A%20%20and%20Interpretations&entry.906535625=Jiaqi%20Ding%20and%20Tingting%20Dan%20and%20Ziquan%20Wei%20and%20Hyuna%20Cho%20and%20Paul%20J.%20Laurienti%20and%20Won%20Hwa%20Kim%20and%20Guorong%20Wu&entry.1292438233=%20%20An%20unprecedented%20amount%20of%20existing%20functional%20Magnetic%20Resonance%20Imaging%0A%28fMRI%29%20data%20provides%20a%20new%20opportunity%20to%20understand%20the%20relationship%20between%0Afunctional%20fluctuation%20and%20human%20cognition/behavior%20using%20a%20data-driven%0Aapproach.%20To%20that%20end%2C%20tremendous%20efforts%20have%20been%20made%20in%20machine%20learning%20to%0Apredict%20cognitive%20states%20from%20evolving%20volumetric%20images%20of%0Ablood-oxygen-level-dependent%20%28BOLD%29%20signals.%20Due%20to%20the%20complex%20nature%20of%20brain%0Afunction%2C%20however%2C%20the%20evaluation%20on%20learning%20performance%20and%20discoveries%20are%0Anot%20often%20consistent%20across%20current%20state-of-the-arts%20%28SOTA%29.%20By%20capitalizing%0Aon%20large-scale%20existing%20neuroimaging%20data%20%2834%2C887%20data%20samples%20from%20six%20public%0Adatabases%29%2C%20we%20seek%20to%20establish%20a%20well-founded%20empirical%20guideline%20for%0Adesigning%20deep%20models%20for%20functional%20neuroimages%20by%20linking%20the%20methodology%0Aunderpinning%20with%20knowledge%20from%20the%20neuroscience%20domain.%20Specifically%2C%20we%20put%0Athe%20spotlight%20on%20%281%29%20What%20is%20the%20current%20SOTA%20performance%20in%20cognitive%20task%0Arecognition%20and%20disease%20diagnosis%20using%20fMRI%3F%20%282%29%20What%20are%20the%20limitations%20of%0Acurrent%20deep%20models%3F%20and%20%283%29%20What%20is%20the%20general%20guideline%20for%20selecting%20the%0Asuitable%20machine%20learning%20backbone%20for%20new%20neuroimaging%20applications%3F%20We%20have%0Aconducted%20a%20comprehensive%20evaluation%20and%20statistical%20analysis%2C%20in%20various%0Asettings%2C%20to%20answer%20the%20above%20outstanding%20questions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11377v1&entry.124074799=Read"},
{"title": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts", "author": "Zeliang Zhang and Xiaodong Liu and Hao Cheng and Chenliang Xu and Jianfeng Gao", "abstract": "  By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research.\n", "link": "http://arxiv.org/abs/2407.09590v2", "date": "2024-09-17", "relevancy": 2.0515, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversifying%20the%20Expert%20Knowledge%20for%20Task-Agnostic%20Pruning%20in%20Sparse%0A%20%20Mixture-of-Experts&body=Title%3A%20Diversifying%20the%20Expert%20Knowledge%20for%20Task-Agnostic%20Pruning%20in%20Sparse%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Zeliang%20Zhang%20and%20Xiaodong%20Liu%20and%20Hao%20Cheng%20and%20Chenliang%20Xu%20and%20Jianfeng%20Gao%0AAbstract%3A%20%20%20By%20increasing%20model%20parameters%20but%20activating%20them%20sparsely%20when%20performing%20a%0Atask%2C%20the%20use%20of%20Mixture-of-Experts%20%28MoE%29%20architecture%20significantly%20improves%0Athe%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20without%20increasing%20the%0Ainference%20cost.%20However%2C%20the%20memory%20consumption%20due%20to%20the%20growing%20number%20of%0Aexperts%20presents%20a%20challenge%20to%20the%20deployment%20of%20these%20models%20in%20many%20real%0Aworld%20settings.%20Our%20empirical%20study%20reveals%20that%20some%20experts%20encode%20redundant%0Aknowledge%20during%20pre-training.%20We%20thus%20propose%20a%20method%20of%20grouping%20and%20pruning%0Asimilar%20experts%20to%20improve%20the%20model%27s%20parameter%20efficiency.%20We%20validate%20the%0Aeffectiveness%20of%20our%20method%20by%20pruning%20three%20state-of-the-art%20MoE%0Aarchitectures%2C%20including%20Mixtral%2C%20Deepseek-MoE%2C%20and%20Qwen.%20The%20evaluation%20shows%0Athat%20our%20method%20outperforms%20other%20model%20pruning%20methods%20on%20a%20range%20of%20natural%0Alanguage%20tasks.%20We%20will%20release%20our%20code%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09590v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversifying%2520the%2520Expert%2520Knowledge%2520for%2520Task-Agnostic%2520Pruning%2520in%2520Sparse%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DZeliang%2520Zhang%2520and%2520Xiaodong%2520Liu%2520and%2520Hao%2520Cheng%2520and%2520Chenliang%2520Xu%2520and%2520Jianfeng%2520Gao%26entry.1292438233%3D%2520%2520By%2520increasing%2520model%2520parameters%2520but%2520activating%2520them%2520sparsely%2520when%2520performing%2520a%250Atask%252C%2520the%2520use%2520of%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%2520significantly%2520improves%250Athe%2520performance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520without%2520increasing%2520the%250Ainference%2520cost.%2520However%252C%2520the%2520memory%2520consumption%2520due%2520to%2520the%2520growing%2520number%2520of%250Aexperts%2520presents%2520a%2520challenge%2520to%2520the%2520deployment%2520of%2520these%2520models%2520in%2520many%2520real%250Aworld%2520settings.%2520Our%2520empirical%2520study%2520reveals%2520that%2520some%2520experts%2520encode%2520redundant%250Aknowledge%2520during%2520pre-training.%2520We%2520thus%2520propose%2520a%2520method%2520of%2520grouping%2520and%2520pruning%250Asimilar%2520experts%2520to%2520improve%2520the%2520model%2527s%2520parameter%2520efficiency.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520by%2520pruning%2520three%2520state-of-the-art%2520MoE%250Aarchitectures%252C%2520including%2520Mixtral%252C%2520Deepseek-MoE%252C%2520and%2520Qwen.%2520The%2520evaluation%2520shows%250Athat%2520our%2520method%2520outperforms%2520other%2520model%2520pruning%2520methods%2520on%2520a%2520range%2520of%2520natural%250Alanguage%2520tasks.%2520We%2520will%2520release%2520our%2520code%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09590v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversifying%20the%20Expert%20Knowledge%20for%20Task-Agnostic%20Pruning%20in%20Sparse%0A%20%20Mixture-of-Experts&entry.906535625=Zeliang%20Zhang%20and%20Xiaodong%20Liu%20and%20Hao%20Cheng%20and%20Chenliang%20Xu%20and%20Jianfeng%20Gao&entry.1292438233=%20%20By%20increasing%20model%20parameters%20but%20activating%20them%20sparsely%20when%20performing%20a%0Atask%2C%20the%20use%20of%20Mixture-of-Experts%20%28MoE%29%20architecture%20significantly%20improves%0Athe%20performance%20of%20Large%20Language%20Models%20%28LLMs%29%20without%20increasing%20the%0Ainference%20cost.%20However%2C%20the%20memory%20consumption%20due%20to%20the%20growing%20number%20of%0Aexperts%20presents%20a%20challenge%20to%20the%20deployment%20of%20these%20models%20in%20many%20real%0Aworld%20settings.%20Our%20empirical%20study%20reveals%20that%20some%20experts%20encode%20redundant%0Aknowledge%20during%20pre-training.%20We%20thus%20propose%20a%20method%20of%20grouping%20and%20pruning%0Asimilar%20experts%20to%20improve%20the%20model%27s%20parameter%20efficiency.%20We%20validate%20the%0Aeffectiveness%20of%20our%20method%20by%20pruning%20three%20state-of-the-art%20MoE%0Aarchitectures%2C%20including%20Mixtral%2C%20Deepseek-MoE%2C%20and%20Qwen.%20The%20evaluation%20shows%0Athat%20our%20method%20outperforms%20other%20model%20pruning%20methods%20on%20a%20range%20of%20natural%0Alanguage%20tasks.%20We%20will%20release%20our%20code%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09590v2&entry.124074799=Read"},
{"title": "CalTag: Robust calibration of mmWave Radar and LiDAR using backscatter\n  tags", "author": "Junyi Xu and Kshitiz Bansal and Dinesh Bharadia", "abstract": "  The rise of automation in robotics necessitates the use of high-quality\nperception systems, often through the use of multiple sensors. A crucial aspect\nof a successfully deployed multi-sensor system is the calibration with a known\nobject typically named fiducial. In this work, we propose a novel fiducial\nsystem for millimeter wave radars, termed as CalTag. CalTag addresses the\nlimitations of traditional corner reflector-based calibration methods in\nextremely cluttered environments. CalTag leverages millimeter wave backscatter\ntechnology to achieve more reliable calibration than corner reflectors,\nenhancing the overall performance of multi-sensor perception systems. We\ncompare the performance in several real-world environments and show the\nimprovement achieved by using CalTag as the radar fiducial over a corner\nreflector.\n", "link": "http://arxiv.org/abs/2408.16867v2", "date": "2024-09-17", "relevancy": 2.0502, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5136}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CalTag%3A%20Robust%20calibration%20of%20mmWave%20Radar%20and%20LiDAR%20using%20backscatter%0A%20%20tags&body=Title%3A%20CalTag%3A%20Robust%20calibration%20of%20mmWave%20Radar%20and%20LiDAR%20using%20backscatter%0A%20%20tags%0AAuthor%3A%20Junyi%20Xu%20and%20Kshitiz%20Bansal%20and%20Dinesh%20Bharadia%0AAbstract%3A%20%20%20The%20rise%20of%20automation%20in%20robotics%20necessitates%20the%20use%20of%20high-quality%0Aperception%20systems%2C%20often%20through%20the%20use%20of%20multiple%20sensors.%20A%20crucial%20aspect%0Aof%20a%20successfully%20deployed%20multi-sensor%20system%20is%20the%20calibration%20with%20a%20known%0Aobject%20typically%20named%20fiducial.%20In%20this%20work%2C%20we%20propose%20a%20novel%20fiducial%0Asystem%20for%20millimeter%20wave%20radars%2C%20termed%20as%20CalTag.%20CalTag%20addresses%20the%0Alimitations%20of%20traditional%20corner%20reflector-based%20calibration%20methods%20in%0Aextremely%20cluttered%20environments.%20CalTag%20leverages%20millimeter%20wave%20backscatter%0Atechnology%20to%20achieve%20more%20reliable%20calibration%20than%20corner%20reflectors%2C%0Aenhancing%20the%20overall%20performance%20of%20multi-sensor%20perception%20systems.%20We%0Acompare%20the%20performance%20in%20several%20real-world%20environments%20and%20show%20the%0Aimprovement%20achieved%20by%20using%20CalTag%20as%20the%20radar%20fiducial%20over%20a%20corner%0Areflector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalTag%253A%2520Robust%2520calibration%2520of%2520mmWave%2520Radar%2520and%2520LiDAR%2520using%2520backscatter%250A%2520%2520tags%26entry.906535625%3DJunyi%2520Xu%2520and%2520Kshitiz%2520Bansal%2520and%2520Dinesh%2520Bharadia%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520automation%2520in%2520robotics%2520necessitates%2520the%2520use%2520of%2520high-quality%250Aperception%2520systems%252C%2520often%2520through%2520the%2520use%2520of%2520multiple%2520sensors.%2520A%2520crucial%2520aspect%250Aof%2520a%2520successfully%2520deployed%2520multi-sensor%2520system%2520is%2520the%2520calibration%2520with%2520a%2520known%250Aobject%2520typically%2520named%2520fiducial.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520fiducial%250Asystem%2520for%2520millimeter%2520wave%2520radars%252C%2520termed%2520as%2520CalTag.%2520CalTag%2520addresses%2520the%250Alimitations%2520of%2520traditional%2520corner%2520reflector-based%2520calibration%2520methods%2520in%250Aextremely%2520cluttered%2520environments.%2520CalTag%2520leverages%2520millimeter%2520wave%2520backscatter%250Atechnology%2520to%2520achieve%2520more%2520reliable%2520calibration%2520than%2520corner%2520reflectors%252C%250Aenhancing%2520the%2520overall%2520performance%2520of%2520multi-sensor%2520perception%2520systems.%2520We%250Acompare%2520the%2520performance%2520in%2520several%2520real-world%2520environments%2520and%2520show%2520the%250Aimprovement%2520achieved%2520by%2520using%2520CalTag%2520as%2520the%2520radar%2520fiducial%2520over%2520a%2520corner%250Areflector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CalTag%3A%20Robust%20calibration%20of%20mmWave%20Radar%20and%20LiDAR%20using%20backscatter%0A%20%20tags&entry.906535625=Junyi%20Xu%20and%20Kshitiz%20Bansal%20and%20Dinesh%20Bharadia&entry.1292438233=%20%20The%20rise%20of%20automation%20in%20robotics%20necessitates%20the%20use%20of%20high-quality%0Aperception%20systems%2C%20often%20through%20the%20use%20of%20multiple%20sensors.%20A%20crucial%20aspect%0Aof%20a%20successfully%20deployed%20multi-sensor%20system%20is%20the%20calibration%20with%20a%20known%0Aobject%20typically%20named%20fiducial.%20In%20this%20work%2C%20we%20propose%20a%20novel%20fiducial%0Asystem%20for%20millimeter%20wave%20radars%2C%20termed%20as%20CalTag.%20CalTag%20addresses%20the%0Alimitations%20of%20traditional%20corner%20reflector-based%20calibration%20methods%20in%0Aextremely%20cluttered%20environments.%20CalTag%20leverages%20millimeter%20wave%20backscatter%0Atechnology%20to%20achieve%20more%20reliable%20calibration%20than%20corner%20reflectors%2C%0Aenhancing%20the%20overall%20performance%20of%20multi-sensor%20perception%20systems.%20We%0Acompare%20the%20performance%20in%20several%20real-world%20environments%20and%20show%20the%0Aimprovement%20achieved%20by%20using%20CalTag%20as%20the%20radar%20fiducial%20over%20a%20corner%0Areflector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16867v2&entry.124074799=Read"},
{"title": "Force Myography based Torque Estimation in Human Knee and Ankle Joints", "author": "Charlotte Marquardt and Arne Schulz and Miha Dezman and Gunther Kurz and Thorsten Stein and Tamim Asfour", "abstract": "  Online adaptation of exoskeleton control based on muscle activity sensing is\na promising way to personalize exoskeletons based on the user's biosignals.\nWhile several electromyography (EMG) based methods have been shown to improve\njoint torque estimation, EMG sensors require direct skin contact and complex\npost-processing. In contrast, force myography (FMG) measures normal forces from\nchanges in muscle volume due to muscle activity. We propose an FMG-based method\nto estimate knee and ankle joint torques by combining joint angles and\nvelocities with muscle activity information. We learn a model for joint torque\nestimation using Gaussian process regression (GPR). The effectiveness of the\nproposed FMG-based method is validated on isokinetic motions performed by two\nsubjects. The model is compared to a baseline model using only joint angle and\nvelocity, as well as a model augmented by EMG data. The results show that\nintegrating FMG into exoskeleton control improves the joint torque estimation\nfor the ankle and knee and is therefore a promising way to improve adaptability\nto different exoskeleton users.\n", "link": "http://arxiv.org/abs/2409.11061v1", "date": "2024-09-17", "relevancy": 2.0437, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5579}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5059}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Force%20Myography%20based%20Torque%20Estimation%20in%20Human%20Knee%20and%20Ankle%20Joints&body=Title%3A%20Force%20Myography%20based%20Torque%20Estimation%20in%20Human%20Knee%20and%20Ankle%20Joints%0AAuthor%3A%20Charlotte%20Marquardt%20and%20Arne%20Schulz%20and%20Miha%20Dezman%20and%20Gunther%20Kurz%20and%20Thorsten%20Stein%20and%20Tamim%20Asfour%0AAbstract%3A%20%20%20Online%20adaptation%20of%20exoskeleton%20control%20based%20on%20muscle%20activity%20sensing%20is%0Aa%20promising%20way%20to%20personalize%20exoskeletons%20based%20on%20the%20user%27s%20biosignals.%0AWhile%20several%20electromyography%20%28EMG%29%20based%20methods%20have%20been%20shown%20to%20improve%0Ajoint%20torque%20estimation%2C%20EMG%20sensors%20require%20direct%20skin%20contact%20and%20complex%0Apost-processing.%20In%20contrast%2C%20force%20myography%20%28FMG%29%20measures%20normal%20forces%20from%0Achanges%20in%20muscle%20volume%20due%20to%20muscle%20activity.%20We%20propose%20an%20FMG-based%20method%0Ato%20estimate%20knee%20and%20ankle%20joint%20torques%20by%20combining%20joint%20angles%20and%0Avelocities%20with%20muscle%20activity%20information.%20We%20learn%20a%20model%20for%20joint%20torque%0Aestimation%20using%20Gaussian%20process%20regression%20%28GPR%29.%20The%20effectiveness%20of%20the%0Aproposed%20FMG-based%20method%20is%20validated%20on%20isokinetic%20motions%20performed%20by%20two%0Asubjects.%20The%20model%20is%20compared%20to%20a%20baseline%20model%20using%20only%20joint%20angle%20and%0Avelocity%2C%20as%20well%20as%20a%20model%20augmented%20by%20EMG%20data.%20The%20results%20show%20that%0Aintegrating%20FMG%20into%20exoskeleton%20control%20improves%20the%20joint%20torque%20estimation%0Afor%20the%20ankle%20and%20knee%20and%20is%20therefore%20a%20promising%20way%20to%20improve%20adaptability%0Ato%20different%20exoskeleton%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForce%2520Myography%2520based%2520Torque%2520Estimation%2520in%2520Human%2520Knee%2520and%2520Ankle%2520Joints%26entry.906535625%3DCharlotte%2520Marquardt%2520and%2520Arne%2520Schulz%2520and%2520Miha%2520Dezman%2520and%2520Gunther%2520Kurz%2520and%2520Thorsten%2520Stein%2520and%2520Tamim%2520Asfour%26entry.1292438233%3D%2520%2520Online%2520adaptation%2520of%2520exoskeleton%2520control%2520based%2520on%2520muscle%2520activity%2520sensing%2520is%250Aa%2520promising%2520way%2520to%2520personalize%2520exoskeletons%2520based%2520on%2520the%2520user%2527s%2520biosignals.%250AWhile%2520several%2520electromyography%2520%2528EMG%2529%2520based%2520methods%2520have%2520been%2520shown%2520to%2520improve%250Ajoint%2520torque%2520estimation%252C%2520EMG%2520sensors%2520require%2520direct%2520skin%2520contact%2520and%2520complex%250Apost-processing.%2520In%2520contrast%252C%2520force%2520myography%2520%2528FMG%2529%2520measures%2520normal%2520forces%2520from%250Achanges%2520in%2520muscle%2520volume%2520due%2520to%2520muscle%2520activity.%2520We%2520propose%2520an%2520FMG-based%2520method%250Ato%2520estimate%2520knee%2520and%2520ankle%2520joint%2520torques%2520by%2520combining%2520joint%2520angles%2520and%250Avelocities%2520with%2520muscle%2520activity%2520information.%2520We%2520learn%2520a%2520model%2520for%2520joint%2520torque%250Aestimation%2520using%2520Gaussian%2520process%2520regression%2520%2528GPR%2529.%2520The%2520effectiveness%2520of%2520the%250Aproposed%2520FMG-based%2520method%2520is%2520validated%2520on%2520isokinetic%2520motions%2520performed%2520by%2520two%250Asubjects.%2520The%2520model%2520is%2520compared%2520to%2520a%2520baseline%2520model%2520using%2520only%2520joint%2520angle%2520and%250Avelocity%252C%2520as%2520well%2520as%2520a%2520model%2520augmented%2520by%2520EMG%2520data.%2520The%2520results%2520show%2520that%250Aintegrating%2520FMG%2520into%2520exoskeleton%2520control%2520improves%2520the%2520joint%2520torque%2520estimation%250Afor%2520the%2520ankle%2520and%2520knee%2520and%2520is%2520therefore%2520a%2520promising%2520way%2520to%2520improve%2520adaptability%250Ato%2520different%2520exoskeleton%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Force%20Myography%20based%20Torque%20Estimation%20in%20Human%20Knee%20and%20Ankle%20Joints&entry.906535625=Charlotte%20Marquardt%20and%20Arne%20Schulz%20and%20Miha%20Dezman%20and%20Gunther%20Kurz%20and%20Thorsten%20Stein%20and%20Tamim%20Asfour&entry.1292438233=%20%20Online%20adaptation%20of%20exoskeleton%20control%20based%20on%20muscle%20activity%20sensing%20is%0Aa%20promising%20way%20to%20personalize%20exoskeletons%20based%20on%20the%20user%27s%20biosignals.%0AWhile%20several%20electromyography%20%28EMG%29%20based%20methods%20have%20been%20shown%20to%20improve%0Ajoint%20torque%20estimation%2C%20EMG%20sensors%20require%20direct%20skin%20contact%20and%20complex%0Apost-processing.%20In%20contrast%2C%20force%20myography%20%28FMG%29%20measures%20normal%20forces%20from%0Achanges%20in%20muscle%20volume%20due%20to%20muscle%20activity.%20We%20propose%20an%20FMG-based%20method%0Ato%20estimate%20knee%20and%20ankle%20joint%20torques%20by%20combining%20joint%20angles%20and%0Avelocities%20with%20muscle%20activity%20information.%20We%20learn%20a%20model%20for%20joint%20torque%0Aestimation%20using%20Gaussian%20process%20regression%20%28GPR%29.%20The%20effectiveness%20of%20the%0Aproposed%20FMG-based%20method%20is%20validated%20on%20isokinetic%20motions%20performed%20by%20two%0Asubjects.%20The%20model%20is%20compared%20to%20a%20baseline%20model%20using%20only%20joint%20angle%20and%0Avelocity%2C%20as%20well%20as%20a%20model%20augmented%20by%20EMG%20data.%20The%20results%20show%20that%0Aintegrating%20FMG%20into%20exoskeleton%20control%20improves%20the%20joint%20torque%20estimation%0Afor%20the%20ankle%20and%20knee%20and%20is%20therefore%20a%20promising%20way%20to%20improve%20adaptability%0Ato%20different%20exoskeleton%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11061v1&entry.124074799=Read"},
{"title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions", "author": "Ninghan Zhong and Alessandro Potenza and Stephen L. Smith", "abstract": "  Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.\n", "link": "http://arxiv.org/abs/2409.11326v1", "date": "2024-09-17", "relevancy": 2.035, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5083}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Navigation%20in%20Ice-Covered%20Waters%20with%20Learned%20Predictions%20on%0A%20%20Ship-Ice%20Interactions&body=Title%3A%20Autonomous%20Navigation%20in%20Ice-Covered%20Waters%20with%20Learned%20Predictions%20on%0A%20%20Ship-Ice%20Interactions%0AAuthor%3A%20Ninghan%20Zhong%20and%20Alessandro%20Potenza%20and%20Stephen%20L.%20Smith%0AAbstract%3A%20%20%20Autonomous%20navigation%20in%20ice-covered%20waters%20poses%20significant%20challenges%20due%0Ato%20the%20frequent%20lack%20of%20viable%20collision-free%20trajectories.%20When%20complete%0Aobstacle%20avoidance%20is%20infeasible%2C%20it%20becomes%20imperative%20for%20the%20navigation%0Astrategy%20to%20minimize%20collisions.%20Additionally%2C%20the%20dynamic%20nature%20of%20ice%2C%20which%0Amoves%20in%20response%20to%20ship%20maneuvers%2C%20complicates%20the%20path%20planning%20process.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20deep%20learning%20model%20to%20estimate%0Athe%20coarse%20dynamics%20of%20ice%20movements%20triggered%20by%20ship%20actions%20through%0Aoccupancy%20estimation.%20To%20ensure%20real-time%20applicability%2C%20we%20propose%20a%20novel%0Aapproach%20that%20caches%20intermediate%20prediction%20results%20and%20seamlessly%20integrates%0Athe%20predictive%20model%20into%20a%20graph%20search%20planner.%20We%20evaluate%20the%20proposed%0Aplanner%20both%20in%20simulation%20and%20in%20a%20physical%20testbed%20against%20existing%0Aapproaches%20and%20show%20that%20our%20planner%20significantly%20reduces%20collisions%20with%20ice%0Awhen%20compared%20to%20the%20state-of-the-art.%20Codes%20and%20demos%20of%20this%20work%20are%0Aavailable%20at%20https%3A//github.com/IvanIZ/predictive-asv-planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Navigation%2520in%2520Ice-Covered%2520Waters%2520with%2520Learned%2520Predictions%2520on%250A%2520%2520Ship-Ice%2520Interactions%26entry.906535625%3DNinghan%2520Zhong%2520and%2520Alessandro%2520Potenza%2520and%2520Stephen%2520L.%2520Smith%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520in%2520ice-covered%2520waters%2520poses%2520significant%2520challenges%2520due%250Ato%2520the%2520frequent%2520lack%2520of%2520viable%2520collision-free%2520trajectories.%2520When%2520complete%250Aobstacle%2520avoidance%2520is%2520infeasible%252C%2520it%2520becomes%2520imperative%2520for%2520the%2520navigation%250Astrategy%2520to%2520minimize%2520collisions.%2520Additionally%252C%2520the%2520dynamic%2520nature%2520of%2520ice%252C%2520which%250Amoves%2520in%2520response%2520to%2520ship%2520maneuvers%252C%2520complicates%2520the%2520path%2520planning%2520process.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520deep%2520learning%2520model%2520to%2520estimate%250Athe%2520coarse%2520dynamics%2520of%2520ice%2520movements%2520triggered%2520by%2520ship%2520actions%2520through%250Aoccupancy%2520estimation.%2520To%2520ensure%2520real-time%2520applicability%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520that%2520caches%2520intermediate%2520prediction%2520results%2520and%2520seamlessly%2520integrates%250Athe%2520predictive%2520model%2520into%2520a%2520graph%2520search%2520planner.%2520We%2520evaluate%2520the%2520proposed%250Aplanner%2520both%2520in%2520simulation%2520and%2520in%2520a%2520physical%2520testbed%2520against%2520existing%250Aapproaches%2520and%2520show%2520that%2520our%2520planner%2520significantly%2520reduces%2520collisions%2520with%2520ice%250Awhen%2520compared%2520to%2520the%2520state-of-the-art.%2520Codes%2520and%2520demos%2520of%2520this%2520work%2520are%250Aavailable%2520at%2520https%253A//github.com/IvanIZ/predictive-asv-planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Navigation%20in%20Ice-Covered%20Waters%20with%20Learned%20Predictions%20on%0A%20%20Ship-Ice%20Interactions&entry.906535625=Ninghan%20Zhong%20and%20Alessandro%20Potenza%20and%20Stephen%20L.%20Smith&entry.1292438233=%20%20Autonomous%20navigation%20in%20ice-covered%20waters%20poses%20significant%20challenges%20due%0Ato%20the%20frequent%20lack%20of%20viable%20collision-free%20trajectories.%20When%20complete%0Aobstacle%20avoidance%20is%20infeasible%2C%20it%20becomes%20imperative%20for%20the%20navigation%0Astrategy%20to%20minimize%20collisions.%20Additionally%2C%20the%20dynamic%20nature%20of%20ice%2C%20which%0Amoves%20in%20response%20to%20ship%20maneuvers%2C%20complicates%20the%20path%20planning%20process.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20deep%20learning%20model%20to%20estimate%0Athe%20coarse%20dynamics%20of%20ice%20movements%20triggered%20by%20ship%20actions%20through%0Aoccupancy%20estimation.%20To%20ensure%20real-time%20applicability%2C%20we%20propose%20a%20novel%0Aapproach%20that%20caches%20intermediate%20prediction%20results%20and%20seamlessly%20integrates%0Athe%20predictive%20model%20into%20a%20graph%20search%20planner.%20We%20evaluate%20the%20proposed%0Aplanner%20both%20in%20simulation%20and%20in%20a%20physical%20testbed%20against%20existing%0Aapproaches%20and%20show%20that%20our%20planner%20significantly%20reduces%20collisions%20with%20ice%0Awhen%20compared%20to%20the%20state-of-the-art.%20Codes%20and%20demos%20of%20this%20work%20are%0Aavailable%20at%20https%3A//github.com/IvanIZ/predictive-asv-planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11326v1&entry.124074799=Read"},
{"title": "A Dynamical System View of Langevin-Based Non-Convex Sampling", "author": "Mohammad Reza Karimi and Ya-Ping Hsieh and Andreas Krause", "abstract": "  Non-convex sampling is a key challenge in machine learning, central to\nnon-convex optimization in deep learning as well as to approximate\nprobabilistic inference. Despite its significance, theoretically there remain\nmany important challenges: Existing guarantees (1) typically only hold for the\naveraged iterates rather than the more desirable last iterates, (2) lack\nconvergence metrics that capture the scales of the variables such as\nWasserstein distances, and (3) mainly apply to elementary schemes such as\nstochastic gradient Langevin dynamics. In this paper, we develop a new\nframework that lifts the above issues by harnessing several tools from the\ntheory of dynamical systems. Our key result is that, for a large class of\nstate-of-the-art sampling schemes, their last-iterate convergence in\nWasserstein distances can be reduced to the study of their continuous-time\ncounterparts, which is much better understood. Coupled with standard\nassumptions of MCMC sampling, our theory immediately yields the last-iterate\nWasserstein convergence of many advanced sampling schemes such as proximal,\nrandomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our\nframework also motivates more efficient schemes that enjoy the same rigorous\nguarantees.\n", "link": "http://arxiv.org/abs/2210.13867v3", "date": "2024-09-17", "relevancy": 2.0303, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dynamical%20System%20View%20of%20Langevin-Based%20Non-Convex%20Sampling&body=Title%3A%20A%20Dynamical%20System%20View%20of%20Langevin-Based%20Non-Convex%20Sampling%0AAuthor%3A%20Mohammad%20Reza%20Karimi%20and%20Ya-Ping%20Hsieh%20and%20Andreas%20Krause%0AAbstract%3A%20%20%20Non-convex%20sampling%20is%20a%20key%20challenge%20in%20machine%20learning%2C%20central%20to%0Anon-convex%20optimization%20in%20deep%20learning%20as%20well%20as%20to%20approximate%0Aprobabilistic%20inference.%20Despite%20its%20significance%2C%20theoretically%20there%20remain%0Amany%20important%20challenges%3A%20Existing%20guarantees%20%281%29%20typically%20only%20hold%20for%20the%0Aaveraged%20iterates%20rather%20than%20the%20more%20desirable%20last%20iterates%2C%20%282%29%20lack%0Aconvergence%20metrics%20that%20capture%20the%20scales%20of%20the%20variables%20such%20as%0AWasserstein%20distances%2C%20and%20%283%29%20mainly%20apply%20to%20elementary%20schemes%20such%20as%0Astochastic%20gradient%20Langevin%20dynamics.%20In%20this%20paper%2C%20we%20develop%20a%20new%0Aframework%20that%20lifts%20the%20above%20issues%20by%20harnessing%20several%20tools%20from%20the%0Atheory%20of%20dynamical%20systems.%20Our%20key%20result%20is%20that%2C%20for%20a%20large%20class%20of%0Astate-of-the-art%20sampling%20schemes%2C%20their%20last-iterate%20convergence%20in%0AWasserstein%20distances%20can%20be%20reduced%20to%20the%20study%20of%20their%20continuous-time%0Acounterparts%2C%20which%20is%20much%20better%20understood.%20Coupled%20with%20standard%0Aassumptions%20of%20MCMC%20sampling%2C%20our%20theory%20immediately%20yields%20the%20last-iterate%0AWasserstein%20convergence%20of%20many%20advanced%20sampling%20schemes%20such%20as%20proximal%2C%0Arandomized%20mid-point%2C%20and%20Runge-Kutta%20integrators.%20Beyond%20existing%20methods%2C%20our%0Aframework%20also%20motivates%20more%20efficient%20schemes%20that%20enjoy%20the%20same%20rigorous%0Aguarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.13867v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dynamical%2520System%2520View%2520of%2520Langevin-Based%2520Non-Convex%2520Sampling%26entry.906535625%3DMohammad%2520Reza%2520Karimi%2520and%2520Ya-Ping%2520Hsieh%2520and%2520Andreas%2520Krause%26entry.1292438233%3D%2520%2520Non-convex%2520sampling%2520is%2520a%2520key%2520challenge%2520in%2520machine%2520learning%252C%2520central%2520to%250Anon-convex%2520optimization%2520in%2520deep%2520learning%2520as%2520well%2520as%2520to%2520approximate%250Aprobabilistic%2520inference.%2520Despite%2520its%2520significance%252C%2520theoretically%2520there%2520remain%250Amany%2520important%2520challenges%253A%2520Existing%2520guarantees%2520%25281%2529%2520typically%2520only%2520hold%2520for%2520the%250Aaveraged%2520iterates%2520rather%2520than%2520the%2520more%2520desirable%2520last%2520iterates%252C%2520%25282%2529%2520lack%250Aconvergence%2520metrics%2520that%2520capture%2520the%2520scales%2520of%2520the%2520variables%2520such%2520as%250AWasserstein%2520distances%252C%2520and%2520%25283%2529%2520mainly%2520apply%2520to%2520elementary%2520schemes%2520such%2520as%250Astochastic%2520gradient%2520Langevin%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520new%250Aframework%2520that%2520lifts%2520the%2520above%2520issues%2520by%2520harnessing%2520several%2520tools%2520from%2520the%250Atheory%2520of%2520dynamical%2520systems.%2520Our%2520key%2520result%2520is%2520that%252C%2520for%2520a%2520large%2520class%2520of%250Astate-of-the-art%2520sampling%2520schemes%252C%2520their%2520last-iterate%2520convergence%2520in%250AWasserstein%2520distances%2520can%2520be%2520reduced%2520to%2520the%2520study%2520of%2520their%2520continuous-time%250Acounterparts%252C%2520which%2520is%2520much%2520better%2520understood.%2520Coupled%2520with%2520standard%250Aassumptions%2520of%2520MCMC%2520sampling%252C%2520our%2520theory%2520immediately%2520yields%2520the%2520last-iterate%250AWasserstein%2520convergence%2520of%2520many%2520advanced%2520sampling%2520schemes%2520such%2520as%2520proximal%252C%250Arandomized%2520mid-point%252C%2520and%2520Runge-Kutta%2520integrators.%2520Beyond%2520existing%2520methods%252C%2520our%250Aframework%2520also%2520motivates%2520more%2520efficient%2520schemes%2520that%2520enjoy%2520the%2520same%2520rigorous%250Aguarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.13867v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dynamical%20System%20View%20of%20Langevin-Based%20Non-Convex%20Sampling&entry.906535625=Mohammad%20Reza%20Karimi%20and%20Ya-Ping%20Hsieh%20and%20Andreas%20Krause&entry.1292438233=%20%20Non-convex%20sampling%20is%20a%20key%20challenge%20in%20machine%20learning%2C%20central%20to%0Anon-convex%20optimization%20in%20deep%20learning%20as%20well%20as%20to%20approximate%0Aprobabilistic%20inference.%20Despite%20its%20significance%2C%20theoretically%20there%20remain%0Amany%20important%20challenges%3A%20Existing%20guarantees%20%281%29%20typically%20only%20hold%20for%20the%0Aaveraged%20iterates%20rather%20than%20the%20more%20desirable%20last%20iterates%2C%20%282%29%20lack%0Aconvergence%20metrics%20that%20capture%20the%20scales%20of%20the%20variables%20such%20as%0AWasserstein%20distances%2C%20and%20%283%29%20mainly%20apply%20to%20elementary%20schemes%20such%20as%0Astochastic%20gradient%20Langevin%20dynamics.%20In%20this%20paper%2C%20we%20develop%20a%20new%0Aframework%20that%20lifts%20the%20above%20issues%20by%20harnessing%20several%20tools%20from%20the%0Atheory%20of%20dynamical%20systems.%20Our%20key%20result%20is%20that%2C%20for%20a%20large%20class%20of%0Astate-of-the-art%20sampling%20schemes%2C%20their%20last-iterate%20convergence%20in%0AWasserstein%20distances%20can%20be%20reduced%20to%20the%20study%20of%20their%20continuous-time%0Acounterparts%2C%20which%20is%20much%20better%20understood.%20Coupled%20with%20standard%0Aassumptions%20of%20MCMC%20sampling%2C%20our%20theory%20immediately%20yields%20the%20last-iterate%0AWasserstein%20convergence%20of%20many%20advanced%20sampling%20schemes%20such%20as%20proximal%2C%0Arandomized%20mid-point%2C%20and%20Runge-Kutta%20integrators.%20Beyond%20existing%20methods%2C%20our%0Aframework%20also%20motivates%20more%20efficient%20schemes%20that%20enjoy%20the%20same%20rigorous%0Aguarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.13867v3&entry.124074799=Read"},
{"title": "Fast Long-Term Multi-Scenario Prediction for Maneuver Planning at\n  Unsignalized Intersections", "author": "Max Bastian Mertens and Jona Ruof and Jan Strohbeck and Michael Buchholz", "abstract": "  Motion prediction for intelligent vehicles typically focuses on estimating\nthe most probable future evolutions of a traffic scenario. Estimating the gap\nacceptance, i.e., whether a vehicle merges or crosses before another vehicle\nwith the right of way, is often handled implicitly in the prediction. However,\nan infrastructure-based maneuver planning can assign artificial priorities\nbetween cooperative vehicles, so it needs to evaluate many more potential\nscenarios. Additionally, the prediction horizon has to be long enough to assess\nthe impact of a maneuver. We, therefore, present a novel long-term prediction\napproach handling the gap acceptance estimation and the velocity prediction in\ntwo separate stages. Thereby, the behavior of regular vehicles as well as\npriority assignments of cooperative vehicles can be considered. We train both\nstages on real-world traffic observations to achieve realistic prediction\nresults. Our method has a competitive accuracy and is fast enough to predict a\nmultitude of scenarios in a short time, making it suitable to be used in a\nmaneuver planning framework.\n", "link": "http://arxiv.org/abs/2401.14879v2", "date": "2024-09-17", "relevancy": 2.0293, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5817}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5088}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Long-Term%20Multi-Scenario%20Prediction%20for%20Maneuver%20Planning%20at%0A%20%20Unsignalized%20Intersections&body=Title%3A%20Fast%20Long-Term%20Multi-Scenario%20Prediction%20for%20Maneuver%20Planning%20at%0A%20%20Unsignalized%20Intersections%0AAuthor%3A%20Max%20Bastian%20Mertens%20and%20Jona%20Ruof%20and%20Jan%20Strohbeck%20and%20Michael%20Buchholz%0AAbstract%3A%20%20%20Motion%20prediction%20for%20intelligent%20vehicles%20typically%20focuses%20on%20estimating%0Athe%20most%20probable%20future%20evolutions%20of%20a%20traffic%20scenario.%20Estimating%20the%20gap%0Aacceptance%2C%20i.e.%2C%20whether%20a%20vehicle%20merges%20or%20crosses%20before%20another%20vehicle%0Awith%20the%20right%20of%20way%2C%20is%20often%20handled%20implicitly%20in%20the%20prediction.%20However%2C%0Aan%20infrastructure-based%20maneuver%20planning%20can%20assign%20artificial%20priorities%0Abetween%20cooperative%20vehicles%2C%20so%20it%20needs%20to%20evaluate%20many%20more%20potential%0Ascenarios.%20Additionally%2C%20the%20prediction%20horizon%20has%20to%20be%20long%20enough%20to%20assess%0Athe%20impact%20of%20a%20maneuver.%20We%2C%20therefore%2C%20present%20a%20novel%20long-term%20prediction%0Aapproach%20handling%20the%20gap%20acceptance%20estimation%20and%20the%20velocity%20prediction%20in%0Atwo%20separate%20stages.%20Thereby%2C%20the%20behavior%20of%20regular%20vehicles%20as%20well%20as%0Apriority%20assignments%20of%20cooperative%20vehicles%20can%20be%20considered.%20We%20train%20both%0Astages%20on%20real-world%20traffic%20observations%20to%20achieve%20realistic%20prediction%0Aresults.%20Our%20method%20has%20a%20competitive%20accuracy%20and%20is%20fast%20enough%20to%20predict%20a%0Amultitude%20of%20scenarios%20in%20a%20short%20time%2C%20making%20it%20suitable%20to%20be%20used%20in%20a%0Amaneuver%20planning%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Long-Term%2520Multi-Scenario%2520Prediction%2520for%2520Maneuver%2520Planning%2520at%250A%2520%2520Unsignalized%2520Intersections%26entry.906535625%3DMax%2520Bastian%2520Mertens%2520and%2520Jona%2520Ruof%2520and%2520Jan%2520Strohbeck%2520and%2520Michael%2520Buchholz%26entry.1292438233%3D%2520%2520Motion%2520prediction%2520for%2520intelligent%2520vehicles%2520typically%2520focuses%2520on%2520estimating%250Athe%2520most%2520probable%2520future%2520evolutions%2520of%2520a%2520traffic%2520scenario.%2520Estimating%2520the%2520gap%250Aacceptance%252C%2520i.e.%252C%2520whether%2520a%2520vehicle%2520merges%2520or%2520crosses%2520before%2520another%2520vehicle%250Awith%2520the%2520right%2520of%2520way%252C%2520is%2520often%2520handled%2520implicitly%2520in%2520the%2520prediction.%2520However%252C%250Aan%2520infrastructure-based%2520maneuver%2520planning%2520can%2520assign%2520artificial%2520priorities%250Abetween%2520cooperative%2520vehicles%252C%2520so%2520it%2520needs%2520to%2520evaluate%2520many%2520more%2520potential%250Ascenarios.%2520Additionally%252C%2520the%2520prediction%2520horizon%2520has%2520to%2520be%2520long%2520enough%2520to%2520assess%250Athe%2520impact%2520of%2520a%2520maneuver.%2520We%252C%2520therefore%252C%2520present%2520a%2520novel%2520long-term%2520prediction%250Aapproach%2520handling%2520the%2520gap%2520acceptance%2520estimation%2520and%2520the%2520velocity%2520prediction%2520in%250Atwo%2520separate%2520stages.%2520Thereby%252C%2520the%2520behavior%2520of%2520regular%2520vehicles%2520as%2520well%2520as%250Apriority%2520assignments%2520of%2520cooperative%2520vehicles%2520can%2520be%2520considered.%2520We%2520train%2520both%250Astages%2520on%2520real-world%2520traffic%2520observations%2520to%2520achieve%2520realistic%2520prediction%250Aresults.%2520Our%2520method%2520has%2520a%2520competitive%2520accuracy%2520and%2520is%2520fast%2520enough%2520to%2520predict%2520a%250Amultitude%2520of%2520scenarios%2520in%2520a%2520short%2520time%252C%2520making%2520it%2520suitable%2520to%2520be%2520used%2520in%2520a%250Amaneuver%2520planning%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Long-Term%20Multi-Scenario%20Prediction%20for%20Maneuver%20Planning%20at%0A%20%20Unsignalized%20Intersections&entry.906535625=Max%20Bastian%20Mertens%20and%20Jona%20Ruof%20and%20Jan%20Strohbeck%20and%20Michael%20Buchholz&entry.1292438233=%20%20Motion%20prediction%20for%20intelligent%20vehicles%20typically%20focuses%20on%20estimating%0Athe%20most%20probable%20future%20evolutions%20of%20a%20traffic%20scenario.%20Estimating%20the%20gap%0Aacceptance%2C%20i.e.%2C%20whether%20a%20vehicle%20merges%20or%20crosses%20before%20another%20vehicle%0Awith%20the%20right%20of%20way%2C%20is%20often%20handled%20implicitly%20in%20the%20prediction.%20However%2C%0Aan%20infrastructure-based%20maneuver%20planning%20can%20assign%20artificial%20priorities%0Abetween%20cooperative%20vehicles%2C%20so%20it%20needs%20to%20evaluate%20many%20more%20potential%0Ascenarios.%20Additionally%2C%20the%20prediction%20horizon%20has%20to%20be%20long%20enough%20to%20assess%0Athe%20impact%20of%20a%20maneuver.%20We%2C%20therefore%2C%20present%20a%20novel%20long-term%20prediction%0Aapproach%20handling%20the%20gap%20acceptance%20estimation%20and%20the%20velocity%20prediction%20in%0Atwo%20separate%20stages.%20Thereby%2C%20the%20behavior%20of%20regular%20vehicles%20as%20well%20as%0Apriority%20assignments%20of%20cooperative%20vehicles%20can%20be%20considered.%20We%20train%20both%0Astages%20on%20real-world%20traffic%20observations%20to%20achieve%20realistic%20prediction%0Aresults.%20Our%20method%20has%20a%20competitive%20accuracy%20and%20is%20fast%20enough%20to%20predict%20a%0Amultitude%20of%20scenarios%20in%20a%20short%20time%2C%20making%20it%20suitable%20to%20be%20used%20in%20a%0Amaneuver%20planning%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14879v2&entry.124074799=Read"},
{"title": "Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think", "author": "Gonzalo Martin Garcia and Karim Abou Zeid and Christian Schmidt and Daan de Geus and Alexander Hermans and Bastian Leibe", "abstract": "  Recent work showed that large diffusion models can be reused as highly\nprecise monocular depth estimators by casting depth estimation as an\nimage-conditional image generation task. While the proposed model achieved\nstate-of-the-art results, high computational demands due to multi-step\ninference limited its use in many scenarios. In this paper, we show that the\nperceived inefficiency was caused by a flaw in the inference pipeline that has\nso far gone unnoticed. The fixed model performs comparably to the best\npreviously reported configuration while being more than 200$\\times$ faster. To\noptimize for downstream task performance, we perform end-to-end fine-tuning on\ntop of the single-step model with task-specific losses and get a deterministic\nmodel that outperforms all other diffusion-based depth and normal estimation\nmodels on common zero-shot benchmarks. We surprisingly find that this\nfine-tuning protocol also works directly on Stable Diffusion and achieves\ncomparable performance to current state-of-the-art diffusion-based depth and\nnormal estimation models, calling into question some of the conclusions drawn\nfrom prior works.\n", "link": "http://arxiv.org/abs/2409.11355v1", "date": "2024-09-17", "relevancy": 2.0011, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7344}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.656}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20Image-Conditional%20Diffusion%20Models%20is%20Easier%20than%20You%20Think&body=Title%3A%20Fine-Tuning%20Image-Conditional%20Diffusion%20Models%20is%20Easier%20than%20You%20Think%0AAuthor%3A%20Gonzalo%20Martin%20Garcia%20and%20Karim%20Abou%20Zeid%20and%20Christian%20Schmidt%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20Bastian%20Leibe%0AAbstract%3A%20%20%20Recent%20work%20showed%20that%20large%20diffusion%20models%20can%20be%20reused%20as%20highly%0Aprecise%20monocular%20depth%20estimators%20by%20casting%20depth%20estimation%20as%20an%0Aimage-conditional%20image%20generation%20task.%20While%20the%20proposed%20model%20achieved%0Astate-of-the-art%20results%2C%20high%20computational%20demands%20due%20to%20multi-step%0Ainference%20limited%20its%20use%20in%20many%20scenarios.%20In%20this%20paper%2C%20we%20show%20that%20the%0Aperceived%20inefficiency%20was%20caused%20by%20a%20flaw%20in%20the%20inference%20pipeline%20that%20has%0Aso%20far%20gone%20unnoticed.%20The%20fixed%20model%20performs%20comparably%20to%20the%20best%0Apreviously%20reported%20configuration%20while%20being%20more%20than%20200%24%5Ctimes%24%20faster.%20To%0Aoptimize%20for%20downstream%20task%20performance%2C%20we%20perform%20end-to-end%20fine-tuning%20on%0Atop%20of%20the%20single-step%20model%20with%20task-specific%20losses%20and%20get%20a%20deterministic%0Amodel%20that%20outperforms%20all%20other%20diffusion-based%20depth%20and%20normal%20estimation%0Amodels%20on%20common%20zero-shot%20benchmarks.%20We%20surprisingly%20find%20that%20this%0Afine-tuning%20protocol%20also%20works%20directly%20on%20Stable%20Diffusion%20and%20achieves%0Acomparable%20performance%20to%20current%20state-of-the-art%20diffusion-based%20depth%20and%0Anormal%20estimation%20models%2C%20calling%20into%20question%20some%20of%20the%20conclusions%20drawn%0Afrom%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520Image-Conditional%2520Diffusion%2520Models%2520is%2520Easier%2520than%2520You%2520Think%26entry.906535625%3DGonzalo%2520Martin%2520Garcia%2520and%2520Karim%2520Abou%2520Zeid%2520and%2520Christian%2520Schmidt%2520and%2520Daan%2520de%2520Geus%2520and%2520Alexander%2520Hermans%2520and%2520Bastian%2520Leibe%26entry.1292438233%3D%2520%2520Recent%2520work%2520showed%2520that%2520large%2520diffusion%2520models%2520can%2520be%2520reused%2520as%2520highly%250Aprecise%2520monocular%2520depth%2520estimators%2520by%2520casting%2520depth%2520estimation%2520as%2520an%250Aimage-conditional%2520image%2520generation%2520task.%2520While%2520the%2520proposed%2520model%2520achieved%250Astate-of-the-art%2520results%252C%2520high%2520computational%2520demands%2520due%2520to%2520multi-step%250Ainference%2520limited%2520its%2520use%2520in%2520many%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%250Aperceived%2520inefficiency%2520was%2520caused%2520by%2520a%2520flaw%2520in%2520the%2520inference%2520pipeline%2520that%2520has%250Aso%2520far%2520gone%2520unnoticed.%2520The%2520fixed%2520model%2520performs%2520comparably%2520to%2520the%2520best%250Apreviously%2520reported%2520configuration%2520while%2520being%2520more%2520than%2520200%2524%255Ctimes%2524%2520faster.%2520To%250Aoptimize%2520for%2520downstream%2520task%2520performance%252C%2520we%2520perform%2520end-to-end%2520fine-tuning%2520on%250Atop%2520of%2520the%2520single-step%2520model%2520with%2520task-specific%2520losses%2520and%2520get%2520a%2520deterministic%250Amodel%2520that%2520outperforms%2520all%2520other%2520diffusion-based%2520depth%2520and%2520normal%2520estimation%250Amodels%2520on%2520common%2520zero-shot%2520benchmarks.%2520We%2520surprisingly%2520find%2520that%2520this%250Afine-tuning%2520protocol%2520also%2520works%2520directly%2520on%2520Stable%2520Diffusion%2520and%2520achieves%250Acomparable%2520performance%2520to%2520current%2520state-of-the-art%2520diffusion-based%2520depth%2520and%250Anormal%2520estimation%2520models%252C%2520calling%2520into%2520question%2520some%2520of%2520the%2520conclusions%2520drawn%250Afrom%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20Image-Conditional%20Diffusion%20Models%20is%20Easier%20than%20You%20Think&entry.906535625=Gonzalo%20Martin%20Garcia%20and%20Karim%20Abou%20Zeid%20and%20Christian%20Schmidt%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20Bastian%20Leibe&entry.1292438233=%20%20Recent%20work%20showed%20that%20large%20diffusion%20models%20can%20be%20reused%20as%20highly%0Aprecise%20monocular%20depth%20estimators%20by%20casting%20depth%20estimation%20as%20an%0Aimage-conditional%20image%20generation%20task.%20While%20the%20proposed%20model%20achieved%0Astate-of-the-art%20results%2C%20high%20computational%20demands%20due%20to%20multi-step%0Ainference%20limited%20its%20use%20in%20many%20scenarios.%20In%20this%20paper%2C%20we%20show%20that%20the%0Aperceived%20inefficiency%20was%20caused%20by%20a%20flaw%20in%20the%20inference%20pipeline%20that%20has%0Aso%20far%20gone%20unnoticed.%20The%20fixed%20model%20performs%20comparably%20to%20the%20best%0Apreviously%20reported%20configuration%20while%20being%20more%20than%20200%24%5Ctimes%24%20faster.%20To%0Aoptimize%20for%20downstream%20task%20performance%2C%20we%20perform%20end-to-end%20fine-tuning%20on%0Atop%20of%20the%20single-step%20model%20with%20task-specific%20losses%20and%20get%20a%20deterministic%0Amodel%20that%20outperforms%20all%20other%20diffusion-based%20depth%20and%20normal%20estimation%0Amodels%20on%20common%20zero-shot%20benchmarks.%20We%20surprisingly%20find%20that%20this%0Afine-tuning%20protocol%20also%20works%20directly%20on%20Stable%20Diffusion%20and%20achieves%0Acomparable%20performance%20to%20current%20state-of-the-art%20diffusion-based%20depth%20and%0Anormal%20estimation%20models%2C%20calling%20into%20question%20some%20of%20the%20conclusions%20drawn%0Afrom%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11355v1&entry.124074799=Read"},
{"title": "Learning Source Disentanglement in Neural Audio Codec", "author": "Xiaoyu Bie and Xubo Liu and Ga\u00ebl Richard", "abstract": "  Neural audio codecs have significantly advanced audio compression by\nefficiently converting continuous audio signals into discrete tokens. These\ncodecs preserve high-quality sound and enable sophisticated sound generation\nthrough generative models trained on these tokens. However, existing neural\ncodec models are typically trained on large, undifferentiated audio datasets,\nneglecting the essential discrepancies between sound domains like speech,\nmusic, and environmental sound effects. This oversight complicates data\nmodeling and poses additional challenges to the controllability of sound\ngeneration. To tackle these issues, we introduce the Source-Disentangled Neural\nAudio Codec (SD-Codec), a novel approach that combines audio coding and source\nseparation. By jointly learning audio resynthesis and separation, SD-Codec\nexplicitly assigns audio signals from different domains to distinct codebooks,\nsets of discrete representations. Experimental results indicate that SD-Codec\nnot only maintains competitive resynthesis quality but also, supported by the\nseparation results, demonstrates successful disentanglement of different\nsources in the latent space, thereby enhancing interpretability in audio codec\nand providing potential finer control over the audio generation process.\n", "link": "http://arxiv.org/abs/2409.11228v1", "date": "2024-09-17", "relevancy": 1.9971, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Source%20Disentanglement%20in%20Neural%20Audio%20Codec&body=Title%3A%20Learning%20Source%20Disentanglement%20in%20Neural%20Audio%20Codec%0AAuthor%3A%20Xiaoyu%20Bie%20and%20Xubo%20Liu%20and%20Ga%C3%ABl%20Richard%0AAbstract%3A%20%20%20Neural%20audio%20codecs%20have%20significantly%20advanced%20audio%20compression%20by%0Aefficiently%20converting%20continuous%20audio%20signals%20into%20discrete%20tokens.%20These%0Acodecs%20preserve%20high-quality%20sound%20and%20enable%20sophisticated%20sound%20generation%0Athrough%20generative%20models%20trained%20on%20these%20tokens.%20However%2C%20existing%20neural%0Acodec%20models%20are%20typically%20trained%20on%20large%2C%20undifferentiated%20audio%20datasets%2C%0Aneglecting%20the%20essential%20discrepancies%20between%20sound%20domains%20like%20speech%2C%0Amusic%2C%20and%20environmental%20sound%20effects.%20This%20oversight%20complicates%20data%0Amodeling%20and%20poses%20additional%20challenges%20to%20the%20controllability%20of%20sound%0Ageneration.%20To%20tackle%20these%20issues%2C%20we%20introduce%20the%20Source-Disentangled%20Neural%0AAudio%20Codec%20%28SD-Codec%29%2C%20a%20novel%20approach%20that%20combines%20audio%20coding%20and%20source%0Aseparation.%20By%20jointly%20learning%20audio%20resynthesis%20and%20separation%2C%20SD-Codec%0Aexplicitly%20assigns%20audio%20signals%20from%20different%20domains%20to%20distinct%20codebooks%2C%0Asets%20of%20discrete%20representations.%20Experimental%20results%20indicate%20that%20SD-Codec%0Anot%20only%20maintains%20competitive%20resynthesis%20quality%20but%20also%2C%20supported%20by%20the%0Aseparation%20results%2C%20demonstrates%20successful%20disentanglement%20of%20different%0Asources%20in%20the%20latent%20space%2C%20thereby%20enhancing%20interpretability%20in%20audio%20codec%0Aand%20providing%20potential%20finer%20control%20over%20the%20audio%20generation%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Source%2520Disentanglement%2520in%2520Neural%2520Audio%2520Codec%26entry.906535625%3DXiaoyu%2520Bie%2520and%2520Xubo%2520Liu%2520and%2520Ga%25C3%25ABl%2520Richard%26entry.1292438233%3D%2520%2520Neural%2520audio%2520codecs%2520have%2520significantly%2520advanced%2520audio%2520compression%2520by%250Aefficiently%2520converting%2520continuous%2520audio%2520signals%2520into%2520discrete%2520tokens.%2520These%250Acodecs%2520preserve%2520high-quality%2520sound%2520and%2520enable%2520sophisticated%2520sound%2520generation%250Athrough%2520generative%2520models%2520trained%2520on%2520these%2520tokens.%2520However%252C%2520existing%2520neural%250Acodec%2520models%2520are%2520typically%2520trained%2520on%2520large%252C%2520undifferentiated%2520audio%2520datasets%252C%250Aneglecting%2520the%2520essential%2520discrepancies%2520between%2520sound%2520domains%2520like%2520speech%252C%250Amusic%252C%2520and%2520environmental%2520sound%2520effects.%2520This%2520oversight%2520complicates%2520data%250Amodeling%2520and%2520poses%2520additional%2520challenges%2520to%2520the%2520controllability%2520of%2520sound%250Ageneration.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520introduce%2520the%2520Source-Disentangled%2520Neural%250AAudio%2520Codec%2520%2528SD-Codec%2529%252C%2520a%2520novel%2520approach%2520that%2520combines%2520audio%2520coding%2520and%2520source%250Aseparation.%2520By%2520jointly%2520learning%2520audio%2520resynthesis%2520and%2520separation%252C%2520SD-Codec%250Aexplicitly%2520assigns%2520audio%2520signals%2520from%2520different%2520domains%2520to%2520distinct%2520codebooks%252C%250Asets%2520of%2520discrete%2520representations.%2520Experimental%2520results%2520indicate%2520that%2520SD-Codec%250Anot%2520only%2520maintains%2520competitive%2520resynthesis%2520quality%2520but%2520also%252C%2520supported%2520by%2520the%250Aseparation%2520results%252C%2520demonstrates%2520successful%2520disentanglement%2520of%2520different%250Asources%2520in%2520the%2520latent%2520space%252C%2520thereby%2520enhancing%2520interpretability%2520in%2520audio%2520codec%250Aand%2520providing%2520potential%2520finer%2520control%2520over%2520the%2520audio%2520generation%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Source%20Disentanglement%20in%20Neural%20Audio%20Codec&entry.906535625=Xiaoyu%20Bie%20and%20Xubo%20Liu%20and%20Ga%C3%ABl%20Richard&entry.1292438233=%20%20Neural%20audio%20codecs%20have%20significantly%20advanced%20audio%20compression%20by%0Aefficiently%20converting%20continuous%20audio%20signals%20into%20discrete%20tokens.%20These%0Acodecs%20preserve%20high-quality%20sound%20and%20enable%20sophisticated%20sound%20generation%0Athrough%20generative%20models%20trained%20on%20these%20tokens.%20However%2C%20existing%20neural%0Acodec%20models%20are%20typically%20trained%20on%20large%2C%20undifferentiated%20audio%20datasets%2C%0Aneglecting%20the%20essential%20discrepancies%20between%20sound%20domains%20like%20speech%2C%0Amusic%2C%20and%20environmental%20sound%20effects.%20This%20oversight%20complicates%20data%0Amodeling%20and%20poses%20additional%20challenges%20to%20the%20controllability%20of%20sound%0Ageneration.%20To%20tackle%20these%20issues%2C%20we%20introduce%20the%20Source-Disentangled%20Neural%0AAudio%20Codec%20%28SD-Codec%29%2C%20a%20novel%20approach%20that%20combines%20audio%20coding%20and%20source%0Aseparation.%20By%20jointly%20learning%20audio%20resynthesis%20and%20separation%2C%20SD-Codec%0Aexplicitly%20assigns%20audio%20signals%20from%20different%20domains%20to%20distinct%20codebooks%2C%0Asets%20of%20discrete%20representations.%20Experimental%20results%20indicate%20that%20SD-Codec%0Anot%20only%20maintains%20competitive%20resynthesis%20quality%20but%20also%2C%20supported%20by%20the%0Aseparation%20results%2C%20demonstrates%20successful%20disentanglement%20of%20different%0Asources%20in%20the%20latent%20space%2C%20thereby%20enhancing%20interpretability%20in%20audio%20codec%0Aand%20providing%20potential%20finer%20control%20over%20the%20audio%20generation%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11228v1&entry.124074799=Read"},
{"title": "Towards Time Series Reasoning with LLMs", "author": "Winnie Chow and Lauren Gardiner and Haraldur T. Hallgr\u00edmsson and Maxwell A. Xu and Shirley You Ren", "abstract": "  Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains.\n", "link": "http://arxiv.org/abs/2409.11376v1", "date": "2024-09-17", "relevancy": 1.994, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Time%20Series%20Reasoning%20with%20LLMs&body=Title%3A%20Towards%20Time%20Series%20Reasoning%20with%20LLMs%0AAuthor%3A%20Winnie%20Chow%20and%20Lauren%20Gardiner%20and%20Haraldur%20T.%20Hallgr%C3%ADmsson%20and%20Maxwell%20A.%20Xu%20and%20Shirley%20You%20Ren%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20numerous%20advances%20in%0Aunderstanding%20and%20reasoning%20in%20domains%20like%20vision%2C%20but%20we%20have%20not%20yet%20seen%0Athis%20broad%20success%20for%20time-series.%20Although%20prior%20works%20on%20time-series%20MLLMs%0Ahave%20shown%20promising%20performance%20in%20time-series%20forecasting%2C%20very%20few%20works%0Ashow%20how%20an%20LLM%20could%20be%20used%20for%20time-series%20reasoning%20in%20natural%20language.%20We%0Apropose%20a%20novel%20multi-modal%20time-series%20LLM%20approach%20that%20learns%20generalizable%0Ainformation%20across%20various%20domains%20with%20powerful%20zero-shot%20performance.%20First%2C%0Awe%20train%20a%20lightweight%20time-series%20encoder%20on%20top%20of%20an%20LLM%20to%20directly%20extract%0Atime-series%20information.%20Then%2C%20we%20fine-tune%20our%20model%20with%20chain-of-thought%0Aaugmented%20time-series%20tasks%20to%20encourage%20the%20model%20to%20generate%20reasoning%20paths.%0AWe%20show%20that%20our%20model%20learns%20a%20latent%20representation%20that%20reflects%20specific%0Atime-series%20features%20%28e.g.%20slope%2C%20frequency%29%2C%20as%20well%20as%20outperforming%20GPT-4o%0Aon%20a%20set%20of%20zero-shot%20reasoning%20tasks%20on%20a%20variety%20of%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Time%2520Series%2520Reasoning%2520with%2520LLMs%26entry.906535625%3DWinnie%2520Chow%2520and%2520Lauren%2520Gardiner%2520and%2520Haraldur%2520T.%2520Hallgr%25C3%25ADmsson%2520and%2520Maxwell%2520A.%2520Xu%2520and%2520Shirley%2520You%2520Ren%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520enabled%2520numerous%2520advances%2520in%250Aunderstanding%2520and%2520reasoning%2520in%2520domains%2520like%2520vision%252C%2520but%2520we%2520have%2520not%2520yet%2520seen%250Athis%2520broad%2520success%2520for%2520time-series.%2520Although%2520prior%2520works%2520on%2520time-series%2520MLLMs%250Ahave%2520shown%2520promising%2520performance%2520in%2520time-series%2520forecasting%252C%2520very%2520few%2520works%250Ashow%2520how%2520an%2520LLM%2520could%2520be%2520used%2520for%2520time-series%2520reasoning%2520in%2520natural%2520language.%2520We%250Apropose%2520a%2520novel%2520multi-modal%2520time-series%2520LLM%2520approach%2520that%2520learns%2520generalizable%250Ainformation%2520across%2520various%2520domains%2520with%2520powerful%2520zero-shot%2520performance.%2520First%252C%250Awe%2520train%2520a%2520lightweight%2520time-series%2520encoder%2520on%2520top%2520of%2520an%2520LLM%2520to%2520directly%2520extract%250Atime-series%2520information.%2520Then%252C%2520we%2520fine-tune%2520our%2520model%2520with%2520chain-of-thought%250Aaugmented%2520time-series%2520tasks%2520to%2520encourage%2520the%2520model%2520to%2520generate%2520reasoning%2520paths.%250AWe%2520show%2520that%2520our%2520model%2520learns%2520a%2520latent%2520representation%2520that%2520reflects%2520specific%250Atime-series%2520features%2520%2528e.g.%2520slope%252C%2520frequency%2529%252C%2520as%2520well%2520as%2520outperforming%2520GPT-4o%250Aon%2520a%2520set%2520of%2520zero-shot%2520reasoning%2520tasks%2520on%2520a%2520variety%2520of%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Time%20Series%20Reasoning%20with%20LLMs&entry.906535625=Winnie%20Chow%20and%20Lauren%20Gardiner%20and%20Haraldur%20T.%20Hallgr%C3%ADmsson%20and%20Maxwell%20A.%20Xu%20and%20Shirley%20You%20Ren&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20numerous%20advances%20in%0Aunderstanding%20and%20reasoning%20in%20domains%20like%20vision%2C%20but%20we%20have%20not%20yet%20seen%0Athis%20broad%20success%20for%20time-series.%20Although%20prior%20works%20on%20time-series%20MLLMs%0Ahave%20shown%20promising%20performance%20in%20time-series%20forecasting%2C%20very%20few%20works%0Ashow%20how%20an%20LLM%20could%20be%20used%20for%20time-series%20reasoning%20in%20natural%20language.%20We%0Apropose%20a%20novel%20multi-modal%20time-series%20LLM%20approach%20that%20learns%20generalizable%0Ainformation%20across%20various%20domains%20with%20powerful%20zero-shot%20performance.%20First%2C%0Awe%20train%20a%20lightweight%20time-series%20encoder%20on%20top%20of%20an%20LLM%20to%20directly%20extract%0Atime-series%20information.%20Then%2C%20we%20fine-tune%20our%20model%20with%20chain-of-thought%0Aaugmented%20time-series%20tasks%20to%20encourage%20the%20model%20to%20generate%20reasoning%20paths.%0AWe%20show%20that%20our%20model%20learns%20a%20latent%20representation%20that%20reflects%20specific%0Atime-series%20features%20%28e.g.%20slope%2C%20frequency%29%2C%20as%20well%20as%20outperforming%20GPT-4o%0Aon%20a%20set%20of%20zero-shot%20reasoning%20tasks%20on%20a%20variety%20of%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11376v1&entry.124074799=Read"},
{"title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning", "author": "Xinzhe Li", "abstract": "  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles (policy models, evaluators, and dynamic models), and\nuniversally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on LMPR implementations and workflow usage\nacross different agent paradigms.\n", "link": "http://arxiv.org/abs/2406.05804v3", "date": "2024-09-17", "relevancy": 1.972, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Prominent%20Paradigms%20for%20LLM-Based%20Agents%3A%20Tool%20Use%0A%20%20%28Including%20RAG%29%2C%20Planning%2C%20and%20Feedback%20Learning&body=Title%3A%20A%20Review%20of%20Prominent%20Paradigms%20for%20LLM-Based%20Agents%3A%20Tool%20Use%0A%20%20%28Including%20RAG%29%2C%20Planning%2C%20and%20Feedback%20Learning%0AAuthor%3A%20Xinzhe%20Li%0AAbstract%3A%20%20%20Tool%20use%2C%20planning%2C%20and%20feedback%20learning%20are%20currently%20three%20prominent%0Aparadigms%20for%20developing%20Large%20Language%20Model%20%28LLM%29-based%20agents%20across%20various%0Atasks.%20Although%20numerous%20frameworks%20have%20been%20devised%20for%20each%20paradigm%2C%20their%0Aintricate%20workflows%20and%20inconsistent%20taxonomy%20create%20challenges%20in%0Aunderstanding%20and%20reviewing%20the%20frameworks%20across%20different%20paradigms.%20This%0Asurvey%20introduces%20a%20unified%20taxonomy%20to%20systematically%20review%20and%20discuss%20these%0Aframeworks.%20Specifically%2C%201%29%20the%20taxonomy%20defines%20environments/tasks%2C%20common%0ALLM-profiled%20roles%20%28policy%20models%2C%20evaluators%2C%20and%20dynamic%20models%29%2C%20and%0Auniversally%20applicable%20workflows%20found%20in%20prior%20work%2C%20and%202%29%20it%20enables%20a%0Acomparison%20of%20key%20perspectives%20on%20LMPR%20implementations%20and%20workflow%20usage%0Aacross%20different%20agent%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05804v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Prominent%2520Paradigms%2520for%2520LLM-Based%2520Agents%253A%2520Tool%2520Use%250A%2520%2520%2528Including%2520RAG%2529%252C%2520Planning%252C%2520and%2520Feedback%2520Learning%26entry.906535625%3DXinzhe%2520Li%26entry.1292438233%3D%2520%2520Tool%2520use%252C%2520planning%252C%2520and%2520feedback%2520learning%2520are%2520currently%2520three%2520prominent%250Aparadigms%2520for%2520developing%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520agents%2520across%2520various%250Atasks.%2520Although%2520numerous%2520frameworks%2520have%2520been%2520devised%2520for%2520each%2520paradigm%252C%2520their%250Aintricate%2520workflows%2520and%2520inconsistent%2520taxonomy%2520create%2520challenges%2520in%250Aunderstanding%2520and%2520reviewing%2520the%2520frameworks%2520across%2520different%2520paradigms.%2520This%250Asurvey%2520introduces%2520a%2520unified%2520taxonomy%2520to%2520systematically%2520review%2520and%2520discuss%2520these%250Aframeworks.%2520Specifically%252C%25201%2529%2520the%2520taxonomy%2520defines%2520environments/tasks%252C%2520common%250ALLM-profiled%2520roles%2520%2528policy%2520models%252C%2520evaluators%252C%2520and%2520dynamic%2520models%2529%252C%2520and%250Auniversally%2520applicable%2520workflows%2520found%2520in%2520prior%2520work%252C%2520and%25202%2529%2520it%2520enables%2520a%250Acomparison%2520of%2520key%2520perspectives%2520on%2520LMPR%2520implementations%2520and%2520workflow%2520usage%250Aacross%2520different%2520agent%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05804v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Prominent%20Paradigms%20for%20LLM-Based%20Agents%3A%20Tool%20Use%0A%20%20%28Including%20RAG%29%2C%20Planning%2C%20and%20Feedback%20Learning&entry.906535625=Xinzhe%20Li&entry.1292438233=%20%20Tool%20use%2C%20planning%2C%20and%20feedback%20learning%20are%20currently%20three%20prominent%0Aparadigms%20for%20developing%20Large%20Language%20Model%20%28LLM%29-based%20agents%20across%20various%0Atasks.%20Although%20numerous%20frameworks%20have%20been%20devised%20for%20each%20paradigm%2C%20their%0Aintricate%20workflows%20and%20inconsistent%20taxonomy%20create%20challenges%20in%0Aunderstanding%20and%20reviewing%20the%20frameworks%20across%20different%20paradigms.%20This%0Asurvey%20introduces%20a%20unified%20taxonomy%20to%20systematically%20review%20and%20discuss%20these%0Aframeworks.%20Specifically%2C%201%29%20the%20taxonomy%20defines%20environments/tasks%2C%20common%0ALLM-profiled%20roles%20%28policy%20models%2C%20evaluators%2C%20and%20dynamic%20models%29%2C%20and%0Auniversally%20applicable%20workflows%20found%20in%20prior%20work%2C%20and%202%29%20it%20enables%20a%0Acomparison%20of%20key%20perspectives%20on%20LMPR%20implementations%20and%20workflow%20usage%0Aacross%20different%20agent%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05804v3&entry.124074799=Read"},
{"title": "Large language models can replicate cross-cultural differences in\n  personality", "author": "Pawe\u0142 Niszczota and Mateusz Janczak and Micha\u0142 Misiak", "abstract": "  We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners.\n", "link": "http://arxiv.org/abs/2310.10679v2", "date": "2024-09-17", "relevancy": 1.9695, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3954}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20can%20replicate%20cross-cultural%20differences%20in%0A%20%20personality&body=Title%3A%20Large%20language%20models%20can%20replicate%20cross-cultural%20differences%20in%0A%20%20personality%0AAuthor%3A%20Pawe%C5%82%20Niszczota%20and%20Mateusz%20Janczak%20and%20Micha%C5%82%20Misiak%0AAbstract%3A%20%20%20We%20use%20a%20large-scale%20experiment%20%28N%3D8000%29%20to%20determine%20whether%20GPT-4%20can%0Areplicate%20cross-cultural%20differences%20in%20the%20Big%20Five%2C%20measured%20using%20the%0ATen-Item%20Personality%20Inventory.%20We%20used%20the%20US%20and%20South%20Korea%20as%20the%20cultural%0Apair%2C%20given%20that%20prior%20research%20suggests%20substantial%20personality%20differences%0Abetween%20people%20from%20these%20two%20countries.%20We%20manipulated%20the%20target%20of%20the%0Asimulation%20%28US%20vs.%20Korean%29%2C%20the%20language%20of%20the%20inventory%20%28English%20vs.%20Korean%29%2C%0Aand%20the%20language%20model%20%28GPT-4%20vs.%20GPT-3.5%29.%20Our%20results%20show%20that%20GPT-4%0Areplicated%20the%20cross-cultural%20differences%20for%20each%20factor.%20However%2C%20mean%0Aratings%20had%20an%20upward%20bias%20and%20exhibited%20lower%20variation%20than%20in%20the%20human%0Asamples%2C%20as%20well%20as%20lower%20structural%20validity.%20We%20provide%20preliminary%20evidence%0Athat%20LLMs%20can%20aid%20cross-cultural%20researchers%20and%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520can%2520replicate%2520cross-cultural%2520differences%2520in%250A%2520%2520personality%26entry.906535625%3DPawe%25C5%2582%2520Niszczota%2520and%2520Mateusz%2520Janczak%2520and%2520Micha%25C5%2582%2520Misiak%26entry.1292438233%3D%2520%2520We%2520use%2520a%2520large-scale%2520experiment%2520%2528N%253D8000%2529%2520to%2520determine%2520whether%2520GPT-4%2520can%250Areplicate%2520cross-cultural%2520differences%2520in%2520the%2520Big%2520Five%252C%2520measured%2520using%2520the%250ATen-Item%2520Personality%2520Inventory.%2520We%2520used%2520the%2520US%2520and%2520South%2520Korea%2520as%2520the%2520cultural%250Apair%252C%2520given%2520that%2520prior%2520research%2520suggests%2520substantial%2520personality%2520differences%250Abetween%2520people%2520from%2520these%2520two%2520countries.%2520We%2520manipulated%2520the%2520target%2520of%2520the%250Asimulation%2520%2528US%2520vs.%2520Korean%2529%252C%2520the%2520language%2520of%2520the%2520inventory%2520%2528English%2520vs.%2520Korean%2529%252C%250Aand%2520the%2520language%2520model%2520%2528GPT-4%2520vs.%2520GPT-3.5%2529.%2520Our%2520results%2520show%2520that%2520GPT-4%250Areplicated%2520the%2520cross-cultural%2520differences%2520for%2520each%2520factor.%2520However%252C%2520mean%250Aratings%2520had%2520an%2520upward%2520bias%2520and%2520exhibited%2520lower%2520variation%2520than%2520in%2520the%2520human%250Asamples%252C%2520as%2520well%2520as%2520lower%2520structural%2520validity.%2520We%2520provide%2520preliminary%2520evidence%250Athat%2520LLMs%2520can%2520aid%2520cross-cultural%2520researchers%2520and%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20can%20replicate%20cross-cultural%20differences%20in%0A%20%20personality&entry.906535625=Pawe%C5%82%20Niszczota%20and%20Mateusz%20Janczak%20and%20Micha%C5%82%20Misiak&entry.1292438233=%20%20We%20use%20a%20large-scale%20experiment%20%28N%3D8000%29%20to%20determine%20whether%20GPT-4%20can%0Areplicate%20cross-cultural%20differences%20in%20the%20Big%20Five%2C%20measured%20using%20the%0ATen-Item%20Personality%20Inventory.%20We%20used%20the%20US%20and%20South%20Korea%20as%20the%20cultural%0Apair%2C%20given%20that%20prior%20research%20suggests%20substantial%20personality%20differences%0Abetween%20people%20from%20these%20two%20countries.%20We%20manipulated%20the%20target%20of%20the%0Asimulation%20%28US%20vs.%20Korean%29%2C%20the%20language%20of%20the%20inventory%20%28English%20vs.%20Korean%29%2C%0Aand%20the%20language%20model%20%28GPT-4%20vs.%20GPT-3.5%29.%20Our%20results%20show%20that%20GPT-4%0Areplicated%20the%20cross-cultural%20differences%20for%20each%20factor.%20However%2C%20mean%0Aratings%20had%20an%20upward%20bias%20and%20exhibited%20lower%20variation%20than%20in%20the%20human%0Asamples%2C%20as%20well%20as%20lower%20structural%20validity.%20We%20provide%20preliminary%20evidence%0Athat%20LLMs%20can%20aid%20cross-cultural%20researchers%20and%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10679v2&entry.124074799=Read"},
{"title": "A Human-Centered Risk Evaluation of Biometric Systems Using Conjoint\n  Analysis", "author": "Tetsushi Ohki and Narishige Abe and Hidetsugu Uchida and Shigefumi Yamada", "abstract": "  Biometric recognition systems, known for their convenience, are widely\nadopted across various fields. However, their security faces risks depending on\nthe authentication algorithm and deployment environment. Current risk\nassessment methods faces significant challenges in incorporating the crucial\nfactor of attacker's motivation, leading to incomplete evaluations. This paper\npresents a novel human-centered risk evaluation framework using conjoint\nanalysis to quantify the impact of risk factors, such as surveillance cameras,\non attacker's motivation. Our framework calculates risk values incorporating\nthe False Acceptance Rate (FAR) and attack probability, allowing comprehensive\ncomparisons across use cases. A survey of 600 Japanese participants\ndemonstrates our method's effectiveness, showing how security measures\ninfluence attacker's motivation. This approach helps decision-makers customize\nbiometric systems to enhance security while maintaining usability.\n", "link": "http://arxiv.org/abs/2409.11224v1", "date": "2024-09-17", "relevancy": 1.9394, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5165}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4823}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Human-Centered%20Risk%20Evaluation%20of%20Biometric%20Systems%20Using%20Conjoint%0A%20%20Analysis&body=Title%3A%20A%20Human-Centered%20Risk%20Evaluation%20of%20Biometric%20Systems%20Using%20Conjoint%0A%20%20Analysis%0AAuthor%3A%20Tetsushi%20Ohki%20and%20Narishige%20Abe%20and%20Hidetsugu%20Uchida%20and%20Shigefumi%20Yamada%0AAbstract%3A%20%20%20Biometric%20recognition%20systems%2C%20known%20for%20their%20convenience%2C%20are%20widely%0Aadopted%20across%20various%20fields.%20However%2C%20their%20security%20faces%20risks%20depending%20on%0Athe%20authentication%20algorithm%20and%20deployment%20environment.%20Current%20risk%0Aassessment%20methods%20faces%20significant%20challenges%20in%20incorporating%20the%20crucial%0Afactor%20of%20attacker%27s%20motivation%2C%20leading%20to%20incomplete%20evaluations.%20This%20paper%0Apresents%20a%20novel%20human-centered%20risk%20evaluation%20framework%20using%20conjoint%0Aanalysis%20to%20quantify%20the%20impact%20of%20risk%20factors%2C%20such%20as%20surveillance%20cameras%2C%0Aon%20attacker%27s%20motivation.%20Our%20framework%20calculates%20risk%20values%20incorporating%0Athe%20False%20Acceptance%20Rate%20%28FAR%29%20and%20attack%20probability%2C%20allowing%20comprehensive%0Acomparisons%20across%20use%20cases.%20A%20survey%20of%20600%20Japanese%20participants%0Ademonstrates%20our%20method%27s%20effectiveness%2C%20showing%20how%20security%20measures%0Ainfluence%20attacker%27s%20motivation.%20This%20approach%20helps%20decision-makers%20customize%0Abiometric%20systems%20to%20enhance%20security%20while%20maintaining%20usability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Human-Centered%2520Risk%2520Evaluation%2520of%2520Biometric%2520Systems%2520Using%2520Conjoint%250A%2520%2520Analysis%26entry.906535625%3DTetsushi%2520Ohki%2520and%2520Narishige%2520Abe%2520and%2520Hidetsugu%2520Uchida%2520and%2520Shigefumi%2520Yamada%26entry.1292438233%3D%2520%2520Biometric%2520recognition%2520systems%252C%2520known%2520for%2520their%2520convenience%252C%2520are%2520widely%250Aadopted%2520across%2520various%2520fields.%2520However%252C%2520their%2520security%2520faces%2520risks%2520depending%2520on%250Athe%2520authentication%2520algorithm%2520and%2520deployment%2520environment.%2520Current%2520risk%250Aassessment%2520methods%2520faces%2520significant%2520challenges%2520in%2520incorporating%2520the%2520crucial%250Afactor%2520of%2520attacker%2527s%2520motivation%252C%2520leading%2520to%2520incomplete%2520evaluations.%2520This%2520paper%250Apresents%2520a%2520novel%2520human-centered%2520risk%2520evaluation%2520framework%2520using%2520conjoint%250Aanalysis%2520to%2520quantify%2520the%2520impact%2520of%2520risk%2520factors%252C%2520such%2520as%2520surveillance%2520cameras%252C%250Aon%2520attacker%2527s%2520motivation.%2520Our%2520framework%2520calculates%2520risk%2520values%2520incorporating%250Athe%2520False%2520Acceptance%2520Rate%2520%2528FAR%2529%2520and%2520attack%2520probability%252C%2520allowing%2520comprehensive%250Acomparisons%2520across%2520use%2520cases.%2520A%2520survey%2520of%2520600%2520Japanese%2520participants%250Ademonstrates%2520our%2520method%2527s%2520effectiveness%252C%2520showing%2520how%2520security%2520measures%250Ainfluence%2520attacker%2527s%2520motivation.%2520This%2520approach%2520helps%2520decision-makers%2520customize%250Abiometric%2520systems%2520to%2520enhance%2520security%2520while%2520maintaining%2520usability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Human-Centered%20Risk%20Evaluation%20of%20Biometric%20Systems%20Using%20Conjoint%0A%20%20Analysis&entry.906535625=Tetsushi%20Ohki%20and%20Narishige%20Abe%20and%20Hidetsugu%20Uchida%20and%20Shigefumi%20Yamada&entry.1292438233=%20%20Biometric%20recognition%20systems%2C%20known%20for%20their%20convenience%2C%20are%20widely%0Aadopted%20across%20various%20fields.%20However%2C%20their%20security%20faces%20risks%20depending%20on%0Athe%20authentication%20algorithm%20and%20deployment%20environment.%20Current%20risk%0Aassessment%20methods%20faces%20significant%20challenges%20in%20incorporating%20the%20crucial%0Afactor%20of%20attacker%27s%20motivation%2C%20leading%20to%20incomplete%20evaluations.%20This%20paper%0Apresents%20a%20novel%20human-centered%20risk%20evaluation%20framework%20using%20conjoint%0Aanalysis%20to%20quantify%20the%20impact%20of%20risk%20factors%2C%20such%20as%20surveillance%20cameras%2C%0Aon%20attacker%27s%20motivation.%20Our%20framework%20calculates%20risk%20values%20incorporating%0Athe%20False%20Acceptance%20Rate%20%28FAR%29%20and%20attack%20probability%2C%20allowing%20comprehensive%0Acomparisons%20across%20use%20cases.%20A%20survey%20of%20600%20Japanese%20participants%0Ademonstrates%20our%20method%27s%20effectiveness%2C%20showing%20how%20security%20measures%0Ainfluence%20attacker%27s%20motivation.%20This%20approach%20helps%20decision-makers%20customize%0Abiometric%20systems%20to%20enhance%20security%20while%20maintaining%20usability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11224v1&entry.124074799=Read"},
{"title": "Generating Synthetic Free-text Medical Records with Low\n  Re-identification Risk using Masked Language Modeling", "author": "Samuel Belkadi and Libo Ren and Nicolo Micheletti and Lifeng Han and Goran Nenadic", "abstract": "  In this paper, we present a system that generates synthetic free-text medical\nrecords, such as discharge summaries, admission notes and doctor\ncorrespondences, using Masked Language Modeling (MLM). Our system is designed\nto preserve the critical information of the records while introducing\nsignificant diversity and minimizing re-identification risk. The system\nincorporates a de-identification component that uses Philter to mask Protected\nHealth Information (PHI), followed by a Medical Entity Recognition (NER) model\nto retain key medical information. We explore various masking ratios and\nmask-filling techniques to balance the trade-off between diversity and fidelity\nin the synthetic outputs without affecting overall readability. Our results\ndemonstrate that the system can produce high-quality synthetic data with\nsignificant diversity while achieving a HIPAA-compliant PHI recall rate of 0.96\nand a low re-identification risk of 0.035. Furthermore, downstream evaluations\nusing a NER task reveal that the synthetic data can be effectively used to\ntrain models with performance comparable to those trained on real data. The\nflexibility of the system allows it to be adapted for specific use cases,\nmaking it a valuable tool for privacy-preserving data generation in medical\nresearch and healthcare applications.\n", "link": "http://arxiv.org/abs/2409.09831v2", "date": "2024-09-17", "relevancy": 1.9229, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.485}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4829}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Synthetic%20Free-text%20Medical%20Records%20with%20Low%0A%20%20Re-identification%20Risk%20using%20Masked%20Language%20Modeling&body=Title%3A%20Generating%20Synthetic%20Free-text%20Medical%20Records%20with%20Low%0A%20%20Re-identification%20Risk%20using%20Masked%20Language%20Modeling%0AAuthor%3A%20Samuel%20Belkadi%20and%20Libo%20Ren%20and%20Nicolo%20Micheletti%20and%20Lifeng%20Han%20and%20Goran%20Nenadic%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20system%20that%20generates%20synthetic%20free-text%20medical%0Arecords%2C%20such%20as%20discharge%20summaries%2C%20admission%20notes%20and%20doctor%0Acorrespondences%2C%20using%20Masked%20Language%20Modeling%20%28MLM%29.%20Our%20system%20is%20designed%0Ato%20preserve%20the%20critical%20information%20of%20the%20records%20while%20introducing%0Asignificant%20diversity%20and%20minimizing%20re-identification%20risk.%20The%20system%0Aincorporates%20a%20de-identification%20component%20that%20uses%20Philter%20to%20mask%20Protected%0AHealth%20Information%20%28PHI%29%2C%20followed%20by%20a%20Medical%20Entity%20Recognition%20%28NER%29%20model%0Ato%20retain%20key%20medical%20information.%20We%20explore%20various%20masking%20ratios%20and%0Amask-filling%20techniques%20to%20balance%20the%20trade-off%20between%20diversity%20and%20fidelity%0Ain%20the%20synthetic%20outputs%20without%20affecting%20overall%20readability.%20Our%20results%0Ademonstrate%20that%20the%20system%20can%20produce%20high-quality%20synthetic%20data%20with%0Asignificant%20diversity%20while%20achieving%20a%20HIPAA-compliant%20PHI%20recall%20rate%20of%200.96%0Aand%20a%20low%20re-identification%20risk%20of%200.035.%20Furthermore%2C%20downstream%20evaluations%0Ausing%20a%20NER%20task%20reveal%20that%20the%20synthetic%20data%20can%20be%20effectively%20used%20to%0Atrain%20models%20with%20performance%20comparable%20to%20those%20trained%20on%20real%20data.%20The%0Aflexibility%20of%20the%20system%20allows%20it%20to%20be%20adapted%20for%20specific%20use%20cases%2C%0Amaking%20it%20a%20valuable%20tool%20for%20privacy-preserving%20data%20generation%20in%20medical%0Aresearch%20and%20healthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Synthetic%2520Free-text%2520Medical%2520Records%2520with%2520Low%250A%2520%2520Re-identification%2520Risk%2520using%2520Masked%2520Language%2520Modeling%26entry.906535625%3DSamuel%2520Belkadi%2520and%2520Libo%2520Ren%2520and%2520Nicolo%2520Micheletti%2520and%2520Lifeng%2520Han%2520and%2520Goran%2520Nenadic%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520system%2520that%2520generates%2520synthetic%2520free-text%2520medical%250Arecords%252C%2520such%2520as%2520discharge%2520summaries%252C%2520admission%2520notes%2520and%2520doctor%250Acorrespondences%252C%2520using%2520Masked%2520Language%2520Modeling%2520%2528MLM%2529.%2520Our%2520system%2520is%2520designed%250Ato%2520preserve%2520the%2520critical%2520information%2520of%2520the%2520records%2520while%2520introducing%250Asignificant%2520diversity%2520and%2520minimizing%2520re-identification%2520risk.%2520The%2520system%250Aincorporates%2520a%2520de-identification%2520component%2520that%2520uses%2520Philter%2520to%2520mask%2520Protected%250AHealth%2520Information%2520%2528PHI%2529%252C%2520followed%2520by%2520a%2520Medical%2520Entity%2520Recognition%2520%2528NER%2529%2520model%250Ato%2520retain%2520key%2520medical%2520information.%2520We%2520explore%2520various%2520masking%2520ratios%2520and%250Amask-filling%2520techniques%2520to%2520balance%2520the%2520trade-off%2520between%2520diversity%2520and%2520fidelity%250Ain%2520the%2520synthetic%2520outputs%2520without%2520affecting%2520overall%2520readability.%2520Our%2520results%250Ademonstrate%2520that%2520the%2520system%2520can%2520produce%2520high-quality%2520synthetic%2520data%2520with%250Asignificant%2520diversity%2520while%2520achieving%2520a%2520HIPAA-compliant%2520PHI%2520recall%2520rate%2520of%25200.96%250Aand%2520a%2520low%2520re-identification%2520risk%2520of%25200.035.%2520Furthermore%252C%2520downstream%2520evaluations%250Ausing%2520a%2520NER%2520task%2520reveal%2520that%2520the%2520synthetic%2520data%2520can%2520be%2520effectively%2520used%2520to%250Atrain%2520models%2520with%2520performance%2520comparable%2520to%2520those%2520trained%2520on%2520real%2520data.%2520The%250Aflexibility%2520of%2520the%2520system%2520allows%2520it%2520to%2520be%2520adapted%2520for%2520specific%2520use%2520cases%252C%250Amaking%2520it%2520a%2520valuable%2520tool%2520for%2520privacy-preserving%2520data%2520generation%2520in%2520medical%250Aresearch%2520and%2520healthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Synthetic%20Free-text%20Medical%20Records%20with%20Low%0A%20%20Re-identification%20Risk%20using%20Masked%20Language%20Modeling&entry.906535625=Samuel%20Belkadi%20and%20Libo%20Ren%20and%20Nicolo%20Micheletti%20and%20Lifeng%20Han%20and%20Goran%20Nenadic&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20system%20that%20generates%20synthetic%20free-text%20medical%0Arecords%2C%20such%20as%20discharge%20summaries%2C%20admission%20notes%20and%20doctor%0Acorrespondences%2C%20using%20Masked%20Language%20Modeling%20%28MLM%29.%20Our%20system%20is%20designed%0Ato%20preserve%20the%20critical%20information%20of%20the%20records%20while%20introducing%0Asignificant%20diversity%20and%20minimizing%20re-identification%20risk.%20The%20system%0Aincorporates%20a%20de-identification%20component%20that%20uses%20Philter%20to%20mask%20Protected%0AHealth%20Information%20%28PHI%29%2C%20followed%20by%20a%20Medical%20Entity%20Recognition%20%28NER%29%20model%0Ato%20retain%20key%20medical%20information.%20We%20explore%20various%20masking%20ratios%20and%0Amask-filling%20techniques%20to%20balance%20the%20trade-off%20between%20diversity%20and%20fidelity%0Ain%20the%20synthetic%20outputs%20without%20affecting%20overall%20readability.%20Our%20results%0Ademonstrate%20that%20the%20system%20can%20produce%20high-quality%20synthetic%20data%20with%0Asignificant%20diversity%20while%20achieving%20a%20HIPAA-compliant%20PHI%20recall%20rate%20of%200.96%0Aand%20a%20low%20re-identification%20risk%20of%200.035.%20Furthermore%2C%20downstream%20evaluations%0Ausing%20a%20NER%20task%20reveal%20that%20the%20synthetic%20data%20can%20be%20effectively%20used%20to%0Atrain%20models%20with%20performance%20comparable%20to%20those%20trained%20on%20real%20data.%20The%0Aflexibility%20of%20the%20system%20allows%20it%20to%20be%20adapted%20for%20specific%20use%20cases%2C%0Amaking%20it%20a%20valuable%20tool%20for%20privacy-preserving%20data%20generation%20in%20medical%0Aresearch%20and%20healthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09831v2&entry.124074799=Read"},
{"title": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling", "author": "Xinyue Fang and Zhen Huang and Zhiliang Tian and Minghui Fang and Ziyi Pan and Quntian Fang and Zhihua Wen and Hengyue Pan and Dongsheng Li", "abstract": "  LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.\n", "link": "http://arxiv.org/abs/2409.11283v1", "date": "2024-09-17", "relevancy": 1.9227, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-resource%20Hallucination%20Detection%20for%20Text%20Generation%20via%0A%20%20Graph-based%20Contextual%20Knowledge%20Triples%20Modeling&body=Title%3A%20Zero-resource%20Hallucination%20Detection%20for%20Text%20Generation%20via%0A%20%20Graph-based%20Contextual%20Knowledge%20Triples%20Modeling%0AAuthor%3A%20Xinyue%20Fang%20and%20Zhen%20Huang%20and%20Zhiliang%20Tian%20and%20Minghui%20Fang%20and%20Ziyi%20Pan%20and%20Quntian%20Fang%20and%20Zhihua%20Wen%20and%20Hengyue%20Pan%20and%20Dongsheng%20Li%0AAbstract%3A%20%20%20LLMs%20obtain%20remarkable%20performance%20but%20suffer%20from%20hallucinations.%20Most%0Aresearch%20on%20detecting%20hallucination%20focuses%20on%20the%20questions%20with%20short%20and%0Aconcrete%20correct%20answers%20that%20are%20easy%20to%20check%20the%20faithfulness.%20Hallucination%0Adetections%20for%20text%20generation%20with%20open-ended%20answers%20are%20more%20challenging.%0ASome%20researchers%20use%20external%20knowledge%20to%20detect%20hallucinations%20in%20generated%0Atexts%2C%20but%20external%20resources%20for%20specific%20scenarios%20are%20hard%20to%20access.%20Recent%0Astudies%20on%20detecting%20hallucinations%20in%20long%20text%20without%20external%20resources%0Aconduct%20consistency%20comparison%20among%20multiple%20sampled%20outputs.%20To%20handle%20long%0Atexts%2C%20researchers%20split%20long%20texts%20into%20multiple%20facts%20and%20individually%0Acompare%20the%20consistency%20of%20each%20pairs%20of%20facts.%20However%2C%20these%20methods%20%281%29%0Ahardly%20achieve%20alignment%20among%20multiple%20facts%3B%20%282%29%20overlook%20dependencies%0Abetween%20multiple%20contextual%20facts.%20In%20this%20paper%2C%20we%20propose%20a%20graph-based%0Acontext-aware%20%28GCA%29%20hallucination%20detection%20for%20text%20generations%2C%20which%20aligns%0Aknowledge%20facts%20and%20considers%20the%20dependencies%20between%20contextual%20knowledge%0Atriples%20in%20consistency%20comparison.%20Particularly%2C%20to%20align%20multiple%20facts%2C%20we%0Aconduct%20a%20triple-oriented%20response%20segmentation%20to%20extract%20multiple%20knowledge%0Atriples.%20To%20model%20dependencies%20among%20contextual%20knowledge%20triple%20%28facts%29%2C%20we%0Aconstruct%20contextual%20triple%20into%20a%20graph%20and%20enhance%20triples%27%20interactions%20via%0Amessage%20passing%20and%20aggregating%20via%20RGCN.%20To%20avoid%20the%20omission%20of%20knowledge%0Atriples%20in%20long%20text%2C%20we%20conduct%20a%20LLM-based%20reverse%20verification%20via%0Areconstructing%20the%20knowledge%20triples.%20Experiments%20show%20that%20our%20model%20enhances%0Ahallucination%20detection%20and%20excels%20all%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-resource%2520Hallucination%2520Detection%2520for%2520Text%2520Generation%2520via%250A%2520%2520Graph-based%2520Contextual%2520Knowledge%2520Triples%2520Modeling%26entry.906535625%3DXinyue%2520Fang%2520and%2520Zhen%2520Huang%2520and%2520Zhiliang%2520Tian%2520and%2520Minghui%2520Fang%2520and%2520Ziyi%2520Pan%2520and%2520Quntian%2520Fang%2520and%2520Zhihua%2520Wen%2520and%2520Hengyue%2520Pan%2520and%2520Dongsheng%2520Li%26entry.1292438233%3D%2520%2520LLMs%2520obtain%2520remarkable%2520performance%2520but%2520suffer%2520from%2520hallucinations.%2520Most%250Aresearch%2520on%2520detecting%2520hallucination%2520focuses%2520on%2520the%2520questions%2520with%2520short%2520and%250Aconcrete%2520correct%2520answers%2520that%2520are%2520easy%2520to%2520check%2520the%2520faithfulness.%2520Hallucination%250Adetections%2520for%2520text%2520generation%2520with%2520open-ended%2520answers%2520are%2520more%2520challenging.%250ASome%2520researchers%2520use%2520external%2520knowledge%2520to%2520detect%2520hallucinations%2520in%2520generated%250Atexts%252C%2520but%2520external%2520resources%2520for%2520specific%2520scenarios%2520are%2520hard%2520to%2520access.%2520Recent%250Astudies%2520on%2520detecting%2520hallucinations%2520in%2520long%2520text%2520without%2520external%2520resources%250Aconduct%2520consistency%2520comparison%2520among%2520multiple%2520sampled%2520outputs.%2520To%2520handle%2520long%250Atexts%252C%2520researchers%2520split%2520long%2520texts%2520into%2520multiple%2520facts%2520and%2520individually%250Acompare%2520the%2520consistency%2520of%2520each%2520pairs%2520of%2520facts.%2520However%252C%2520these%2520methods%2520%25281%2529%250Ahardly%2520achieve%2520alignment%2520among%2520multiple%2520facts%253B%2520%25282%2529%2520overlook%2520dependencies%250Abetween%2520multiple%2520contextual%2520facts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520graph-based%250Acontext-aware%2520%2528GCA%2529%2520hallucination%2520detection%2520for%2520text%2520generations%252C%2520which%2520aligns%250Aknowledge%2520facts%2520and%2520considers%2520the%2520dependencies%2520between%2520contextual%2520knowledge%250Atriples%2520in%2520consistency%2520comparison.%2520Particularly%252C%2520to%2520align%2520multiple%2520facts%252C%2520we%250Aconduct%2520a%2520triple-oriented%2520response%2520segmentation%2520to%2520extract%2520multiple%2520knowledge%250Atriples.%2520To%2520model%2520dependencies%2520among%2520contextual%2520knowledge%2520triple%2520%2528facts%2529%252C%2520we%250Aconstruct%2520contextual%2520triple%2520into%2520a%2520graph%2520and%2520enhance%2520triples%2527%2520interactions%2520via%250Amessage%2520passing%2520and%2520aggregating%2520via%2520RGCN.%2520To%2520avoid%2520the%2520omission%2520of%2520knowledge%250Atriples%2520in%2520long%2520text%252C%2520we%2520conduct%2520a%2520LLM-based%2520reverse%2520verification%2520via%250Areconstructing%2520the%2520knowledge%2520triples.%2520Experiments%2520show%2520that%2520our%2520model%2520enhances%250Ahallucination%2520detection%2520and%2520excels%2520all%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-resource%20Hallucination%20Detection%20for%20Text%20Generation%20via%0A%20%20Graph-based%20Contextual%20Knowledge%20Triples%20Modeling&entry.906535625=Xinyue%20Fang%20and%20Zhen%20Huang%20and%20Zhiliang%20Tian%20and%20Minghui%20Fang%20and%20Ziyi%20Pan%20and%20Quntian%20Fang%20and%20Zhihua%20Wen%20and%20Hengyue%20Pan%20and%20Dongsheng%20Li&entry.1292438233=%20%20LLMs%20obtain%20remarkable%20performance%20but%20suffer%20from%20hallucinations.%20Most%0Aresearch%20on%20detecting%20hallucination%20focuses%20on%20the%20questions%20with%20short%20and%0Aconcrete%20correct%20answers%20that%20are%20easy%20to%20check%20the%20faithfulness.%20Hallucination%0Adetections%20for%20text%20generation%20with%20open-ended%20answers%20are%20more%20challenging.%0ASome%20researchers%20use%20external%20knowledge%20to%20detect%20hallucinations%20in%20generated%0Atexts%2C%20but%20external%20resources%20for%20specific%20scenarios%20are%20hard%20to%20access.%20Recent%0Astudies%20on%20detecting%20hallucinations%20in%20long%20text%20without%20external%20resources%0Aconduct%20consistency%20comparison%20among%20multiple%20sampled%20outputs.%20To%20handle%20long%0Atexts%2C%20researchers%20split%20long%20texts%20into%20multiple%20facts%20and%20individually%0Acompare%20the%20consistency%20of%20each%20pairs%20of%20facts.%20However%2C%20these%20methods%20%281%29%0Ahardly%20achieve%20alignment%20among%20multiple%20facts%3B%20%282%29%20overlook%20dependencies%0Abetween%20multiple%20contextual%20facts.%20In%20this%20paper%2C%20we%20propose%20a%20graph-based%0Acontext-aware%20%28GCA%29%20hallucination%20detection%20for%20text%20generations%2C%20which%20aligns%0Aknowledge%20facts%20and%20considers%20the%20dependencies%20between%20contextual%20knowledge%0Atriples%20in%20consistency%20comparison.%20Particularly%2C%20to%20align%20multiple%20facts%2C%20we%0Aconduct%20a%20triple-oriented%20response%20segmentation%20to%20extract%20multiple%20knowledge%0Atriples.%20To%20model%20dependencies%20among%20contextual%20knowledge%20triple%20%28facts%29%2C%20we%0Aconstruct%20contextual%20triple%20into%20a%20graph%20and%20enhance%20triples%27%20interactions%20via%0Amessage%20passing%20and%20aggregating%20via%20RGCN.%20To%20avoid%20the%20omission%20of%20knowledge%0Atriples%20in%20long%20text%2C%20we%20conduct%20a%20LLM-based%20reverse%20verification%20via%0Areconstructing%20the%20knowledge%20triples.%20Experiments%20show%20that%20our%20model%20enhances%0Ahallucination%20detection%20and%20excels%20all%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11283v1&entry.124074799=Read"},
{"title": "Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder\n  Optimization in RIS", "author": "Dahlia Devapriya and Sheetal Kalyani", "abstract": "  In reconfigurable intelligent surface (RIS) aided systems, the joint\noptimization of the precoder matrix at the base station and the phase shifts of\nthe RIS elements involves significant complexity. In this paper, we propose a\ncomplex-valued, geometry aware meta-learning neural network that maximizes the\nweighted sum rate in a multi-user multiple input single output system. By\nleveraging the complex circle geometry for phase shifts and spherical geometry\nfor the precoder, the optimization occurs on Riemannian manifolds, leading to\nfaster convergence. We use a complex-valued neural network for phase shifts and\nan Euler inspired update for the precoder network. Our approach outperforms\nexisting neural network-based algorithms, offering higher weighted sum rates,\nlower power consumption, and significantly faster convergence. Specifically, it\nconverges faster by nearly 100 epochs, with a 0.7 bps improvement in weighted\nsum rate and a 1.8 dBm power gain when compared with existing work.\n", "link": "http://arxiv.org/abs/2409.11270v1", "date": "2024-09-17", "relevancy": 1.9182, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4849}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4814}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Aware%20Meta-Learning%20Neural%20Network%20for%20Joint%20Phase%20and%20Precoder%0A%20%20Optimization%20in%20RIS&body=Title%3A%20Geometry%20Aware%20Meta-Learning%20Neural%20Network%20for%20Joint%20Phase%20and%20Precoder%0A%20%20Optimization%20in%20RIS%0AAuthor%3A%20Dahlia%20Devapriya%20and%20Sheetal%20Kalyani%0AAbstract%3A%20%20%20In%20reconfigurable%20intelligent%20surface%20%28RIS%29%20aided%20systems%2C%20the%20joint%0Aoptimization%20of%20the%20precoder%20matrix%20at%20the%20base%20station%20and%20the%20phase%20shifts%20of%0Athe%20RIS%20elements%20involves%20significant%20complexity.%20In%20this%20paper%2C%20we%20propose%20a%0Acomplex-valued%2C%20geometry%20aware%20meta-learning%20neural%20network%20that%20maximizes%20the%0Aweighted%20sum%20rate%20in%20a%20multi-user%20multiple%20input%20single%20output%20system.%20By%0Aleveraging%20the%20complex%20circle%20geometry%20for%20phase%20shifts%20and%20spherical%20geometry%0Afor%20the%20precoder%2C%20the%20optimization%20occurs%20on%20Riemannian%20manifolds%2C%20leading%20to%0Afaster%20convergence.%20We%20use%20a%20complex-valued%20neural%20network%20for%20phase%20shifts%20and%0Aan%20Euler%20inspired%20update%20for%20the%20precoder%20network.%20Our%20approach%20outperforms%0Aexisting%20neural%20network-based%20algorithms%2C%20offering%20higher%20weighted%20sum%20rates%2C%0Alower%20power%20consumption%2C%20and%20significantly%20faster%20convergence.%20Specifically%2C%20it%0Aconverges%20faster%20by%20nearly%20100%20epochs%2C%20with%20a%200.7%20bps%20improvement%20in%20weighted%0Asum%20rate%20and%20a%201.8%20dBm%20power%20gain%20when%20compared%20with%20existing%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Aware%2520Meta-Learning%2520Neural%2520Network%2520for%2520Joint%2520Phase%2520and%2520Precoder%250A%2520%2520Optimization%2520in%2520RIS%26entry.906535625%3DDahlia%2520Devapriya%2520and%2520Sheetal%2520Kalyani%26entry.1292438233%3D%2520%2520In%2520reconfigurable%2520intelligent%2520surface%2520%2528RIS%2529%2520aided%2520systems%252C%2520the%2520joint%250Aoptimization%2520of%2520the%2520precoder%2520matrix%2520at%2520the%2520base%2520station%2520and%2520the%2520phase%2520shifts%2520of%250Athe%2520RIS%2520elements%2520involves%2520significant%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Acomplex-valued%252C%2520geometry%2520aware%2520meta-learning%2520neural%2520network%2520that%2520maximizes%2520the%250Aweighted%2520sum%2520rate%2520in%2520a%2520multi-user%2520multiple%2520input%2520single%2520output%2520system.%2520By%250Aleveraging%2520the%2520complex%2520circle%2520geometry%2520for%2520phase%2520shifts%2520and%2520spherical%2520geometry%250Afor%2520the%2520precoder%252C%2520the%2520optimization%2520occurs%2520on%2520Riemannian%2520manifolds%252C%2520leading%2520to%250Afaster%2520convergence.%2520We%2520use%2520a%2520complex-valued%2520neural%2520network%2520for%2520phase%2520shifts%2520and%250Aan%2520Euler%2520inspired%2520update%2520for%2520the%2520precoder%2520network.%2520Our%2520approach%2520outperforms%250Aexisting%2520neural%2520network-based%2520algorithms%252C%2520offering%2520higher%2520weighted%2520sum%2520rates%252C%250Alower%2520power%2520consumption%252C%2520and%2520significantly%2520faster%2520convergence.%2520Specifically%252C%2520it%250Aconverges%2520faster%2520by%2520nearly%2520100%2520epochs%252C%2520with%2520a%25200.7%2520bps%2520improvement%2520in%2520weighted%250Asum%2520rate%2520and%2520a%25201.8%2520dBm%2520power%2520gain%2520when%2520compared%2520with%2520existing%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Aware%20Meta-Learning%20Neural%20Network%20for%20Joint%20Phase%20and%20Precoder%0A%20%20Optimization%20in%20RIS&entry.906535625=Dahlia%20Devapriya%20and%20Sheetal%20Kalyani&entry.1292438233=%20%20In%20reconfigurable%20intelligent%20surface%20%28RIS%29%20aided%20systems%2C%20the%20joint%0Aoptimization%20of%20the%20precoder%20matrix%20at%20the%20base%20station%20and%20the%20phase%20shifts%20of%0Athe%20RIS%20elements%20involves%20significant%20complexity.%20In%20this%20paper%2C%20we%20propose%20a%0Acomplex-valued%2C%20geometry%20aware%20meta-learning%20neural%20network%20that%20maximizes%20the%0Aweighted%20sum%20rate%20in%20a%20multi-user%20multiple%20input%20single%20output%20system.%20By%0Aleveraging%20the%20complex%20circle%20geometry%20for%20phase%20shifts%20and%20spherical%20geometry%0Afor%20the%20precoder%2C%20the%20optimization%20occurs%20on%20Riemannian%20manifolds%2C%20leading%20to%0Afaster%20convergence.%20We%20use%20a%20complex-valued%20neural%20network%20for%20phase%20shifts%20and%0Aan%20Euler%20inspired%20update%20for%20the%20precoder%20network.%20Our%20approach%20outperforms%0Aexisting%20neural%20network-based%20algorithms%2C%20offering%20higher%20weighted%20sum%20rates%2C%0Alower%20power%20consumption%2C%20and%20significantly%20faster%20convergence.%20Specifically%2C%20it%0Aconverges%20faster%20by%20nearly%20100%20epochs%2C%20with%20a%200.7%20bps%20improvement%20in%20weighted%0Asum%20rate%20and%20a%201.8%20dBm%20power%20gain%20when%20compared%20with%20existing%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11270v1&entry.124074799=Read"},
{"title": "SOAP: Improving and Stabilizing Shampoo using Adam", "author": "Nikhil Vyas and Depen Morwani and Rosie Zhao and Itai Shapira and David Brandfonbrener and Lucas Janson and Sham Kakade", "abstract": "  There is growing evidence of the effectiveness of Shampoo, a higher-order\npreconditioning method, over Adam in deep learning optimization tasks. However,\nShampoo's drawbacks include additional hyperparameters and computational\noverhead when compared to Adam, which only updates running averages of first-\nand second-moment quantities. This work establishes a formal connection between\nShampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient\napproximation of Adam -- showing that Shampoo is equivalent to running\nAdafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to\nthe design of a simpler and computationally efficient algorithm:\n$\\textbf{S}$hampo$\\textbf{O}$ with $\\textbf{A}$dam in the\n$\\textbf{P}$reconditioner's eigenbasis (SOAP).\n  With regards to improving Shampoo's computational efficiency, the most\nstraightforward approach would be to simply compute Shampoo's\neigendecomposition less frequently. Unfortunately, as our empirical results\nshow, this leads to performance degradation that worsens with this frequency.\nSOAP mitigates this degradation by continually updating the running average of\nthe second moment, just as Adam does, but in the current (slowly changing)\ncoordinate basis. Furthermore, since SOAP is equivalent to running Adam in a\nrotated space, it introduces only one additional hyperparameter (the\npreconditioning frequency) compared to Adam. We empirically evaluate SOAP on\nlanguage model pre-training with 360m and 660m sized models. In the large batch\nregime, SOAP reduces the number of iterations by over 40% and wall clock time\nby over 35% compared to AdamW, with approximately 20% improvements in both\nmetrics compared to Shampoo. An implementation of SOAP is available at\nhttps://github.com/nikhilvyas/SOAP.\n", "link": "http://arxiv.org/abs/2409.11321v1", "date": "2024-09-17", "relevancy": 1.6931, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4396}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4124}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOAP%3A%20Improving%20and%20Stabilizing%20Shampoo%20using%20Adam&body=Title%3A%20SOAP%3A%20Improving%20and%20Stabilizing%20Shampoo%20using%20Adam%0AAuthor%3A%20Nikhil%20Vyas%20and%20Depen%20Morwani%20and%20Rosie%20Zhao%20and%20Itai%20Shapira%20and%20David%20Brandfonbrener%20and%20Lucas%20Janson%20and%20Sham%20Kakade%0AAbstract%3A%20%20%20There%20is%20growing%20evidence%20of%20the%20effectiveness%20of%20Shampoo%2C%20a%20higher-order%0Apreconditioning%20method%2C%20over%20Adam%20in%20deep%20learning%20optimization%20tasks.%20However%2C%0AShampoo%27s%20drawbacks%20include%20additional%20hyperparameters%20and%20computational%0Aoverhead%20when%20compared%20to%20Adam%2C%20which%20only%20updates%20running%20averages%20of%20first-%0Aand%20second-moment%20quantities.%20This%20work%20establishes%20a%20formal%20connection%20between%0AShampoo%20%28implemented%20with%20the%201/2%20power%29%20and%20Adafactor%20--%20a%20memory-efficient%0Aapproximation%20of%20Adam%20--%20showing%20that%20Shampoo%20is%20equivalent%20to%20running%0AAdafactor%20in%20the%20eigenbasis%20of%20Shampoo%27s%20preconditioner.%20This%20insight%20leads%20to%0Athe%20design%20of%20a%20simpler%20and%20computationally%20efficient%20algorithm%3A%0A%24%5Ctextbf%7BS%7D%24hampo%24%5Ctextbf%7BO%7D%24%20with%20%24%5Ctextbf%7BA%7D%24dam%20in%20the%0A%24%5Ctextbf%7BP%7D%24reconditioner%27s%20eigenbasis%20%28SOAP%29.%0A%20%20With%20regards%20to%20improving%20Shampoo%27s%20computational%20efficiency%2C%20the%20most%0Astraightforward%20approach%20would%20be%20to%20simply%20compute%20Shampoo%27s%0Aeigendecomposition%20less%20frequently.%20Unfortunately%2C%20as%20our%20empirical%20results%0Ashow%2C%20this%20leads%20to%20performance%20degradation%20that%20worsens%20with%20this%20frequency.%0ASOAP%20mitigates%20this%20degradation%20by%20continually%20updating%20the%20running%20average%20of%0Athe%20second%20moment%2C%20just%20as%20Adam%20does%2C%20but%20in%20the%20current%20%28slowly%20changing%29%0Acoordinate%20basis.%20Furthermore%2C%20since%20SOAP%20is%20equivalent%20to%20running%20Adam%20in%20a%0Arotated%20space%2C%20it%20introduces%20only%20one%20additional%20hyperparameter%20%28the%0Apreconditioning%20frequency%29%20compared%20to%20Adam.%20We%20empirically%20evaluate%20SOAP%20on%0Alanguage%20model%20pre-training%20with%20360m%20and%20660m%20sized%20models.%20In%20the%20large%20batch%0Aregime%2C%20SOAP%20reduces%20the%20number%20of%20iterations%20by%20over%2040%25%20and%20wall%20clock%20time%0Aby%20over%2035%25%20compared%20to%20AdamW%2C%20with%20approximately%2020%25%20improvements%20in%20both%0Ametrics%20compared%20to%20Shampoo.%20An%20implementation%20of%20SOAP%20is%20available%20at%0Ahttps%3A//github.com/nikhilvyas/SOAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOAP%253A%2520Improving%2520and%2520Stabilizing%2520Shampoo%2520using%2520Adam%26entry.906535625%3DNikhil%2520Vyas%2520and%2520Depen%2520Morwani%2520and%2520Rosie%2520Zhao%2520and%2520Itai%2520Shapira%2520and%2520David%2520Brandfonbrener%2520and%2520Lucas%2520Janson%2520and%2520Sham%2520Kakade%26entry.1292438233%3D%2520%2520There%2520is%2520growing%2520evidence%2520of%2520the%2520effectiveness%2520of%2520Shampoo%252C%2520a%2520higher-order%250Apreconditioning%2520method%252C%2520over%2520Adam%2520in%2520deep%2520learning%2520optimization%2520tasks.%2520However%252C%250AShampoo%2527s%2520drawbacks%2520include%2520additional%2520hyperparameters%2520and%2520computational%250Aoverhead%2520when%2520compared%2520to%2520Adam%252C%2520which%2520only%2520updates%2520running%2520averages%2520of%2520first-%250Aand%2520second-moment%2520quantities.%2520This%2520work%2520establishes%2520a%2520formal%2520connection%2520between%250AShampoo%2520%2528implemented%2520with%2520the%25201/2%2520power%2529%2520and%2520Adafactor%2520--%2520a%2520memory-efficient%250Aapproximation%2520of%2520Adam%2520--%2520showing%2520that%2520Shampoo%2520is%2520equivalent%2520to%2520running%250AAdafactor%2520in%2520the%2520eigenbasis%2520of%2520Shampoo%2527s%2520preconditioner.%2520This%2520insight%2520leads%2520to%250Athe%2520design%2520of%2520a%2520simpler%2520and%2520computationally%2520efficient%2520algorithm%253A%250A%2524%255Ctextbf%257BS%257D%2524hampo%2524%255Ctextbf%257BO%257D%2524%2520with%2520%2524%255Ctextbf%257BA%257D%2524dam%2520in%2520the%250A%2524%255Ctextbf%257BP%257D%2524reconditioner%2527s%2520eigenbasis%2520%2528SOAP%2529.%250A%2520%2520With%2520regards%2520to%2520improving%2520Shampoo%2527s%2520computational%2520efficiency%252C%2520the%2520most%250Astraightforward%2520approach%2520would%2520be%2520to%2520simply%2520compute%2520Shampoo%2527s%250Aeigendecomposition%2520less%2520frequently.%2520Unfortunately%252C%2520as%2520our%2520empirical%2520results%250Ashow%252C%2520this%2520leads%2520to%2520performance%2520degradation%2520that%2520worsens%2520with%2520this%2520frequency.%250ASOAP%2520mitigates%2520this%2520degradation%2520by%2520continually%2520updating%2520the%2520running%2520average%2520of%250Athe%2520second%2520moment%252C%2520just%2520as%2520Adam%2520does%252C%2520but%2520in%2520the%2520current%2520%2528slowly%2520changing%2529%250Acoordinate%2520basis.%2520Furthermore%252C%2520since%2520SOAP%2520is%2520equivalent%2520to%2520running%2520Adam%2520in%2520a%250Arotated%2520space%252C%2520it%2520introduces%2520only%2520one%2520additional%2520hyperparameter%2520%2528the%250Apreconditioning%2520frequency%2529%2520compared%2520to%2520Adam.%2520We%2520empirically%2520evaluate%2520SOAP%2520on%250Alanguage%2520model%2520pre-training%2520with%2520360m%2520and%2520660m%2520sized%2520models.%2520In%2520the%2520large%2520batch%250Aregime%252C%2520SOAP%2520reduces%2520the%2520number%2520of%2520iterations%2520by%2520over%252040%2525%2520and%2520wall%2520clock%2520time%250Aby%2520over%252035%2525%2520compared%2520to%2520AdamW%252C%2520with%2520approximately%252020%2525%2520improvements%2520in%2520both%250Ametrics%2520compared%2520to%2520Shampoo.%2520An%2520implementation%2520of%2520SOAP%2520is%2520available%2520at%250Ahttps%253A//github.com/nikhilvyas/SOAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOAP%3A%20Improving%20and%20Stabilizing%20Shampoo%20using%20Adam&entry.906535625=Nikhil%20Vyas%20and%20Depen%20Morwani%20and%20Rosie%20Zhao%20and%20Itai%20Shapira%20and%20David%20Brandfonbrener%20and%20Lucas%20Janson%20and%20Sham%20Kakade&entry.1292438233=%20%20There%20is%20growing%20evidence%20of%20the%20effectiveness%20of%20Shampoo%2C%20a%20higher-order%0Apreconditioning%20method%2C%20over%20Adam%20in%20deep%20learning%20optimization%20tasks.%20However%2C%0AShampoo%27s%20drawbacks%20include%20additional%20hyperparameters%20and%20computational%0Aoverhead%20when%20compared%20to%20Adam%2C%20which%20only%20updates%20running%20averages%20of%20first-%0Aand%20second-moment%20quantities.%20This%20work%20establishes%20a%20formal%20connection%20between%0AShampoo%20%28implemented%20with%20the%201/2%20power%29%20and%20Adafactor%20--%20a%20memory-efficient%0Aapproximation%20of%20Adam%20--%20showing%20that%20Shampoo%20is%20equivalent%20to%20running%0AAdafactor%20in%20the%20eigenbasis%20of%20Shampoo%27s%20preconditioner.%20This%20insight%20leads%20to%0Athe%20design%20of%20a%20simpler%20and%20computationally%20efficient%20algorithm%3A%0A%24%5Ctextbf%7BS%7D%24hampo%24%5Ctextbf%7BO%7D%24%20with%20%24%5Ctextbf%7BA%7D%24dam%20in%20the%0A%24%5Ctextbf%7BP%7D%24reconditioner%27s%20eigenbasis%20%28SOAP%29.%0A%20%20With%20regards%20to%20improving%20Shampoo%27s%20computational%20efficiency%2C%20the%20most%0Astraightforward%20approach%20would%20be%20to%20simply%20compute%20Shampoo%27s%0Aeigendecomposition%20less%20frequently.%20Unfortunately%2C%20as%20our%20empirical%20results%0Ashow%2C%20this%20leads%20to%20performance%20degradation%20that%20worsens%20with%20this%20frequency.%0ASOAP%20mitigates%20this%20degradation%20by%20continually%20updating%20the%20running%20average%20of%0Athe%20second%20moment%2C%20just%20as%20Adam%20does%2C%20but%20in%20the%20current%20%28slowly%20changing%29%0Acoordinate%20basis.%20Furthermore%2C%20since%20SOAP%20is%20equivalent%20to%20running%20Adam%20in%20a%0Arotated%20space%2C%20it%20introduces%20only%20one%20additional%20hyperparameter%20%28the%0Apreconditioning%20frequency%29%20compared%20to%20Adam.%20We%20empirically%20evaluate%20SOAP%20on%0Alanguage%20model%20pre-training%20with%20360m%20and%20660m%20sized%20models.%20In%20the%20large%20batch%0Aregime%2C%20SOAP%20reduces%20the%20number%20of%20iterations%20by%20over%2040%25%20and%20wall%20clock%20time%0Aby%20over%2035%25%20compared%20to%20AdamW%2C%20with%20approximately%2020%25%20improvements%20in%20both%0Ametrics%20compared%20to%20Shampoo.%20An%20implementation%20of%20SOAP%20is%20available%20at%0Ahttps%3A//github.com/nikhilvyas/SOAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11321v1&entry.124074799=Read"},
{"title": "Flash STU: Fast Spectral Transform Units", "author": "Y. Isabel Liu and Windsor Nguyen and Yagiz Devre and Evan Dogariu and Anirudha Majumdar and Elad Hazan", "abstract": "  This paper describes an efficient, open source PyTorch implementation of the\nSpectral Transform Unit. We investigate sequence prediction tasks over several\nmodalities including language, robotics, and simulated dynamical systems. We\nfind that for the same parameter count, the STU and its variants outperform the\nTransformer as well as other leading state space models across various\nmodalities.\n", "link": "http://arxiv.org/abs/2409.10489v2", "date": "2024-09-17", "relevancy": 1.3729, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.476}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4528}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flash%20STU%3A%20Fast%20Spectral%20Transform%20Units&body=Title%3A%20Flash%20STU%3A%20Fast%20Spectral%20Transform%20Units%0AAuthor%3A%20Y.%20Isabel%20Liu%20and%20Windsor%20Nguyen%20and%20Yagiz%20Devre%20and%20Evan%20Dogariu%20and%20Anirudha%20Majumdar%20and%20Elad%20Hazan%0AAbstract%3A%20%20%20This%20paper%20describes%20an%20efficient%2C%20open%20source%20PyTorch%20implementation%20of%20the%0ASpectral%20Transform%20Unit.%20We%20investigate%20sequence%20prediction%20tasks%20over%20several%0Amodalities%20including%20language%2C%20robotics%2C%20and%20simulated%20dynamical%20systems.%20We%0Afind%20that%20for%20the%20same%20parameter%20count%2C%20the%20STU%20and%20its%20variants%20outperform%20the%0ATransformer%20as%20well%20as%20other%20leading%20state%20space%20models%20across%20various%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlash%2520STU%253A%2520Fast%2520Spectral%2520Transform%2520Units%26entry.906535625%3DY.%2520Isabel%2520Liu%2520and%2520Windsor%2520Nguyen%2520and%2520Yagiz%2520Devre%2520and%2520Evan%2520Dogariu%2520and%2520Anirudha%2520Majumdar%2520and%2520Elad%2520Hazan%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520an%2520efficient%252C%2520open%2520source%2520PyTorch%2520implementation%2520of%2520the%250ASpectral%2520Transform%2520Unit.%2520We%2520investigate%2520sequence%2520prediction%2520tasks%2520over%2520several%250Amodalities%2520including%2520language%252C%2520robotics%252C%2520and%2520simulated%2520dynamical%2520systems.%2520We%250Afind%2520that%2520for%2520the%2520same%2520parameter%2520count%252C%2520the%2520STU%2520and%2520its%2520variants%2520outperform%2520the%250ATransformer%2520as%2520well%2520as%2520other%2520leading%2520state%2520space%2520models%2520across%2520various%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flash%20STU%3A%20Fast%20Spectral%20Transform%20Units&entry.906535625=Y.%20Isabel%20Liu%20and%20Windsor%20Nguyen%20and%20Yagiz%20Devre%20and%20Evan%20Dogariu%20and%20Anirudha%20Majumdar%20and%20Elad%20Hazan&entry.1292438233=%20%20This%20paper%20describes%20an%20efficient%2C%20open%20source%20PyTorch%20implementation%20of%20the%0ASpectral%20Transform%20Unit.%20We%20investigate%20sequence%20prediction%20tasks%20over%20several%0Amodalities%20including%20language%2C%20robotics%2C%20and%20simulated%20dynamical%20systems.%20We%0Afind%20that%20for%20the%20same%20parameter%20count%2C%20the%20STU%20and%20its%20variants%20outperform%20the%0ATransformer%20as%20well%20as%20other%20leading%20state%20space%20models%20across%20various%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10489v2&entry.124074799=Read"},
{"title": "LYT-NET: Lightweight YUV Transformer-based Network for Low-light Image\n  Enhancement", "author": "A. Brateanu and R. Balmez and A. Avram and C. Orhei and C. Ancuti", "abstract": "  This letter introduces LYT-Net, a novel lightweight transformer-based model\nfor low-light image enhancement (LLIE). LYT-Net consists of several layers and\ndetachable blocks, including our novel blocks--Channel-Wise Denoiser (CWD) and\nMulti-Stage Squeeze & Excite Fusion (MSEF)--along with the traditional\nTransformer block, Multi-Headed Self-Attention (MHSA). In our method we adopt a\ndual-path approach, treating chrominance channels U and V and luminance channel\nY as separate entities to help the model better handle illumination adjustment\nand corruption restoration. Our comprehensive evaluation on established LLIE\ndatasets demonstrates that, despite its low complexity, our model outperforms\nrecent LLIE methods. The source code and pre-trained models are available at\nhttps://github.com/albrateanu/LYT-Net\n", "link": "http://arxiv.org/abs/2401.15204v6", "date": "2024-09-17", "relevancy": 1.6794, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5683}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5666}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LYT-NET%3A%20Lightweight%20YUV%20Transformer-based%20Network%20for%20Low-light%20Image%0A%20%20Enhancement&body=Title%3A%20LYT-NET%3A%20Lightweight%20YUV%20Transformer-based%20Network%20for%20Low-light%20Image%0A%20%20Enhancement%0AAuthor%3A%20A.%20Brateanu%20and%20R.%20Balmez%20and%20A.%20Avram%20and%20C.%20Orhei%20and%20C.%20Ancuti%0AAbstract%3A%20%20%20This%20letter%20introduces%20LYT-Net%2C%20a%20novel%20lightweight%20transformer-based%20model%0Afor%20low-light%20image%20enhancement%20%28LLIE%29.%20LYT-Net%20consists%20of%20several%20layers%20and%0Adetachable%20blocks%2C%20including%20our%20novel%20blocks--Channel-Wise%20Denoiser%20%28CWD%29%20and%0AMulti-Stage%20Squeeze%20%26%20Excite%20Fusion%20%28MSEF%29--along%20with%20the%20traditional%0ATransformer%20block%2C%20Multi-Headed%20Self-Attention%20%28MHSA%29.%20In%20our%20method%20we%20adopt%20a%0Adual-path%20approach%2C%20treating%20chrominance%20channels%20U%20and%20V%20and%20luminance%20channel%0AY%20as%20separate%20entities%20to%20help%20the%20model%20better%20handle%20illumination%20adjustment%0Aand%20corruption%20restoration.%20Our%20comprehensive%20evaluation%20on%20established%20LLIE%0Adatasets%20demonstrates%20that%2C%20despite%20its%20low%20complexity%2C%20our%20model%20outperforms%0Arecent%20LLIE%20methods.%20The%20source%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/albrateanu/LYT-Net%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15204v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLYT-NET%253A%2520Lightweight%2520YUV%2520Transformer-based%2520Network%2520for%2520Low-light%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DA.%2520Brateanu%2520and%2520R.%2520Balmez%2520and%2520A.%2520Avram%2520and%2520C.%2520Orhei%2520and%2520C.%2520Ancuti%26entry.1292438233%3D%2520%2520This%2520letter%2520introduces%2520LYT-Net%252C%2520a%2520novel%2520lightweight%2520transformer-based%2520model%250Afor%2520low-light%2520image%2520enhancement%2520%2528LLIE%2529.%2520LYT-Net%2520consists%2520of%2520several%2520layers%2520and%250Adetachable%2520blocks%252C%2520including%2520our%2520novel%2520blocks--Channel-Wise%2520Denoiser%2520%2528CWD%2529%2520and%250AMulti-Stage%2520Squeeze%2520%2526%2520Excite%2520Fusion%2520%2528MSEF%2529--along%2520with%2520the%2520traditional%250ATransformer%2520block%252C%2520Multi-Headed%2520Self-Attention%2520%2528MHSA%2529.%2520In%2520our%2520method%2520we%2520adopt%2520a%250Adual-path%2520approach%252C%2520treating%2520chrominance%2520channels%2520U%2520and%2520V%2520and%2520luminance%2520channel%250AY%2520as%2520separate%2520entities%2520to%2520help%2520the%2520model%2520better%2520handle%2520illumination%2520adjustment%250Aand%2520corruption%2520restoration.%2520Our%2520comprehensive%2520evaluation%2520on%2520established%2520LLIE%250Adatasets%2520demonstrates%2520that%252C%2520despite%2520its%2520low%2520complexity%252C%2520our%2520model%2520outperforms%250Arecent%2520LLIE%2520methods.%2520The%2520source%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/albrateanu/LYT-Net%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15204v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LYT-NET%3A%20Lightweight%20YUV%20Transformer-based%20Network%20for%20Low-light%20Image%0A%20%20Enhancement&entry.906535625=A.%20Brateanu%20and%20R.%20Balmez%20and%20A.%20Avram%20and%20C.%20Orhei%20and%20C.%20Ancuti&entry.1292438233=%20%20This%20letter%20introduces%20LYT-Net%2C%20a%20novel%20lightweight%20transformer-based%20model%0Afor%20low-light%20image%20enhancement%20%28LLIE%29.%20LYT-Net%20consists%20of%20several%20layers%20and%0Adetachable%20blocks%2C%20including%20our%20novel%20blocks--Channel-Wise%20Denoiser%20%28CWD%29%20and%0AMulti-Stage%20Squeeze%20%26%20Excite%20Fusion%20%28MSEF%29--along%20with%20the%20traditional%0ATransformer%20block%2C%20Multi-Headed%20Self-Attention%20%28MHSA%29.%20In%20our%20method%20we%20adopt%20a%0Adual-path%20approach%2C%20treating%20chrominance%20channels%20U%20and%20V%20and%20luminance%20channel%0AY%20as%20separate%20entities%20to%20help%20the%20model%20better%20handle%20illumination%20adjustment%0Aand%20corruption%20restoration.%20Our%20comprehensive%20evaluation%20on%20established%20LLIE%0Adatasets%20demonstrates%20that%2C%20despite%20its%20low%20complexity%2C%20our%20model%20outperforms%0Arecent%20LLIE%20methods.%20The%20source%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/albrateanu/LYT-Net%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15204v6&entry.124074799=Read"},
{"title": "Fault Detection for agents on power grid topology optimization: A\n  Comprehensive analysis", "author": "Malte Lehna and Mohamed Hassouna and Dmitry Degtyar and Sven Tomforde and Christoph Scholz", "abstract": "  Optimizing the topology of transmission networks using Deep Reinforcement\nLearning (DRL) has increasingly come into focus. Various DRL agents have been\nproposed, which are mostly benchmarked on the Grid2Op environment from the\nLearning to Run a Power Network (L2RPN) challenges. The environments have many\nadvantages with their realistic grid scenarios and underlying power flow\nbackends. However, the interpretation of agent survival or failure is not\nalways clear, as there are a variety of potential causes. In this work, we\nfocus on the failures of the power grid simulation to identify patterns and\ndetect them in advance. We collect the failed scenarios of three different\nagents on the WCCI 2022 L2RPN environment, totaling about 40k data points. By\nclustering, we are able to detect five distinct clusters, identifying common\nfailure types. Further, we propose a multi-class prediction approach to detect\nfailures beforehand and evaluate five different prediction models. Here, the\nLight Gradient-Boosting Machine (LightGBM) shows the best failure prediction\nperformance, with an accuracy of 82%. It also accurately classifies whether a\nthe grid survives or fails in 87% of cases. Finally, we provide a detailed\nfeature importance analysis that identifies critical features and regions in\nthe grid.\n", "link": "http://arxiv.org/abs/2406.16426v3", "date": "2024-09-17", "relevancy": 1.8694, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4934}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis&body=Title%3A%20Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis%0AAuthor%3A%20Malte%20Lehna%20and%20Mohamed%20Hassouna%20and%20Dmitry%20Degtyar%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Optimizing%20the%20topology%20of%20transmission%20networks%20using%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20has%20increasingly%20come%20into%20focus.%20Various%20DRL%20agents%20have%20been%0Aproposed%2C%20which%20are%20mostly%20benchmarked%20on%20the%20Grid2Op%20environment%20from%20the%0ALearning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%20The%20environments%20have%20many%0Aadvantages%20with%20their%20realistic%20grid%20scenarios%20and%20underlying%20power%20flow%0Abackends.%20However%2C%20the%20interpretation%20of%20agent%20survival%20or%20failure%20is%20not%0Aalways%20clear%2C%20as%20there%20are%20a%20variety%20of%20potential%20causes.%20In%20this%20work%2C%20we%0Afocus%20on%20the%20failures%20of%20the%20power%20grid%20simulation%20to%20identify%20patterns%20and%0Adetect%20them%20in%20advance.%20We%20collect%20the%20failed%20scenarios%20of%20three%20different%0Aagents%20on%20the%20WCCI%202022%20L2RPN%20environment%2C%20totaling%20about%2040k%20data%20points.%20By%0Aclustering%2C%20we%20are%20able%20to%20detect%20five%20distinct%20clusters%2C%20identifying%20common%0Afailure%20types.%20Further%2C%20we%20propose%20a%20multi-class%20prediction%20approach%20to%20detect%0Afailures%20beforehand%20and%20evaluate%20five%20different%20prediction%20models.%20Here%2C%20the%0ALight%20Gradient-Boosting%20Machine%20%28LightGBM%29%20shows%20the%20best%20failure%20prediction%0Aperformance%2C%20with%20an%20accuracy%20of%2082%25.%20It%20also%20accurately%20classifies%20whether%20a%0Athe%20grid%20survives%20or%20fails%20in%2087%25%20of%20cases.%20Finally%2C%20we%20provide%20a%20detailed%0Afeature%20importance%20analysis%20that%20identifies%20critical%20features%20and%20regions%20in%0Athe%20grid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16426v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Detection%2520for%2520agents%2520on%2520power%2520grid%2520topology%2520optimization%253A%2520A%250A%2520%2520Comprehensive%2520analysis%26entry.906535625%3DMalte%2520Lehna%2520and%2520Mohamed%2520Hassouna%2520and%2520Dmitry%2520Degtyar%2520and%2520Sven%2520Tomforde%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Optimizing%2520the%2520topology%2520of%2520transmission%2520networks%2520using%2520Deep%2520Reinforcement%250ALearning%2520%2528DRL%2529%2520has%2520increasingly%2520come%2520into%2520focus.%2520Various%2520DRL%2520agents%2520have%2520been%250Aproposed%252C%2520which%2520are%2520mostly%2520benchmarked%2520on%2520the%2520Grid2Op%2520environment%2520from%2520the%250ALearning%2520to%2520Run%2520a%2520Power%2520Network%2520%2528L2RPN%2529%2520challenges.%2520The%2520environments%2520have%2520many%250Aadvantages%2520with%2520their%2520realistic%2520grid%2520scenarios%2520and%2520underlying%2520power%2520flow%250Abackends.%2520However%252C%2520the%2520interpretation%2520of%2520agent%2520survival%2520or%2520failure%2520is%2520not%250Aalways%2520clear%252C%2520as%2520there%2520are%2520a%2520variety%2520of%2520potential%2520causes.%2520In%2520this%2520work%252C%2520we%250Afocus%2520on%2520the%2520failures%2520of%2520the%2520power%2520grid%2520simulation%2520to%2520identify%2520patterns%2520and%250Adetect%2520them%2520in%2520advance.%2520We%2520collect%2520the%2520failed%2520scenarios%2520of%2520three%2520different%250Aagents%2520on%2520the%2520WCCI%25202022%2520L2RPN%2520environment%252C%2520totaling%2520about%252040k%2520data%2520points.%2520By%250Aclustering%252C%2520we%2520are%2520able%2520to%2520detect%2520five%2520distinct%2520clusters%252C%2520identifying%2520common%250Afailure%2520types.%2520Further%252C%2520we%2520propose%2520a%2520multi-class%2520prediction%2520approach%2520to%2520detect%250Afailures%2520beforehand%2520and%2520evaluate%2520five%2520different%2520prediction%2520models.%2520Here%252C%2520the%250ALight%2520Gradient-Boosting%2520Machine%2520%2528LightGBM%2529%2520shows%2520the%2520best%2520failure%2520prediction%250Aperformance%252C%2520with%2520an%2520accuracy%2520of%252082%2525.%2520It%2520also%2520accurately%2520classifies%2520whether%2520a%250Athe%2520grid%2520survives%2520or%2520fails%2520in%252087%2525%2520of%2520cases.%2520Finally%252C%2520we%2520provide%2520a%2520detailed%250Afeature%2520importance%2520analysis%2520that%2520identifies%2520critical%2520features%2520and%2520regions%2520in%250Athe%2520grid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16426v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis&entry.906535625=Malte%20Lehna%20and%20Mohamed%20Hassouna%20and%20Dmitry%20Degtyar%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz&entry.1292438233=%20%20Optimizing%20the%20topology%20of%20transmission%20networks%20using%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20has%20increasingly%20come%20into%20focus.%20Various%20DRL%20agents%20have%20been%0Aproposed%2C%20which%20are%20mostly%20benchmarked%20on%20the%20Grid2Op%20environment%20from%20the%0ALearning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%20The%20environments%20have%20many%0Aadvantages%20with%20their%20realistic%20grid%20scenarios%20and%20underlying%20power%20flow%0Abackends.%20However%2C%20the%20interpretation%20of%20agent%20survival%20or%20failure%20is%20not%0Aalways%20clear%2C%20as%20there%20are%20a%20variety%20of%20potential%20causes.%20In%20this%20work%2C%20we%0Afocus%20on%20the%20failures%20of%20the%20power%20grid%20simulation%20to%20identify%20patterns%20and%0Adetect%20them%20in%20advance.%20We%20collect%20the%20failed%20scenarios%20of%20three%20different%0Aagents%20on%20the%20WCCI%202022%20L2RPN%20environment%2C%20totaling%20about%2040k%20data%20points.%20By%0Aclustering%2C%20we%20are%20able%20to%20detect%20five%20distinct%20clusters%2C%20identifying%20common%0Afailure%20types.%20Further%2C%20we%20propose%20a%20multi-class%20prediction%20approach%20to%20detect%0Afailures%20beforehand%20and%20evaluate%20five%20different%20prediction%20models.%20Here%2C%20the%0ALight%20Gradient-Boosting%20Machine%20%28LightGBM%29%20shows%20the%20best%20failure%20prediction%0Aperformance%2C%20with%20an%20accuracy%20of%2082%25.%20It%20also%20accurately%20classifies%20whether%20a%0Athe%20grid%20survives%20or%20fails%20in%2087%25%20of%20cases.%20Finally%2C%20we%20provide%20a%20detailed%0Afeature%20importance%20analysis%20that%20identifies%20critical%20features%20and%20regions%20in%0Athe%20grid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16426v3&entry.124074799=Read"},
{"title": "UniLCD: Unified Local-Cloud Decision-Making via Reinforcement Learning", "author": "Kathakoli Sengupta and Zhongkai Shagguan and Sandesh Bharadwaj and Sanjay Arora and Eshed Ohn-Bar and Renato Mancuso", "abstract": "  Embodied vision-based real-world systems, such as mobile robots, require a\ncareful balance between energy consumption, compute latency, and safety\nconstraints to optimize operation across dynamic tasks and contexts. As local\ncomputation tends to be restricted, offloading the computation, ie, to a remote\nserver, can save local resources while providing access to high-quality\npredictions from powerful and large models. However, the resulting\ncommunication and latency overhead has led to limited usability of cloud models\nin dynamic, safety-critical, real-time settings. To effectively address this\ntrade-off, we introduce UniLCD, a novel hybrid inference framework for enabling\nflexible local-cloud collaboration. By efficiently optimizing a flexible\nrouting module via reinforcement learning and a suitable multi-task objective,\nUniLCD is specifically designed to support the multiple constraints of\nsafety-critical end-to-end mobile systems. We validate the proposed approach\nusing a challenging, crowded navigation task requiring frequent and timely\nswitching between local and cloud operations. UniLCD demonstrates improved\noverall performance and efficiency, by over 35% compared to state-of-the-art\nbaselines based on various split computing and early exit strategies.\n", "link": "http://arxiv.org/abs/2409.11403v1", "date": "2024-09-17", "relevancy": 1.7385, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5658}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLCD%3A%20Unified%20Local-Cloud%20Decision-Making%20via%20Reinforcement%20Learning&body=Title%3A%20UniLCD%3A%20Unified%20Local-Cloud%20Decision-Making%20via%20Reinforcement%20Learning%0AAuthor%3A%20Kathakoli%20Sengupta%20and%20Zhongkai%20Shagguan%20and%20Sandesh%20Bharadwaj%20and%20Sanjay%20Arora%20and%20Eshed%20Ohn-Bar%20and%20Renato%20Mancuso%0AAbstract%3A%20%20%20Embodied%20vision-based%20real-world%20systems%2C%20such%20as%20mobile%20robots%2C%20require%20a%0Acareful%20balance%20between%20energy%20consumption%2C%20compute%20latency%2C%20and%20safety%0Aconstraints%20to%20optimize%20operation%20across%20dynamic%20tasks%20and%20contexts.%20As%20local%0Acomputation%20tends%20to%20be%20restricted%2C%20offloading%20the%20computation%2C%20ie%2C%20to%20a%20remote%0Aserver%2C%20can%20save%20local%20resources%20while%20providing%20access%20to%20high-quality%0Apredictions%20from%20powerful%20and%20large%20models.%20However%2C%20the%20resulting%0Acommunication%20and%20latency%20overhead%20has%20led%20to%20limited%20usability%20of%20cloud%20models%0Ain%20dynamic%2C%20safety-critical%2C%20real-time%20settings.%20To%20effectively%20address%20this%0Atrade-off%2C%20we%20introduce%20UniLCD%2C%20a%20novel%20hybrid%20inference%20framework%20for%20enabling%0Aflexible%20local-cloud%20collaboration.%20By%20efficiently%20optimizing%20a%20flexible%0Arouting%20module%20via%20reinforcement%20learning%20and%20a%20suitable%20multi-task%20objective%2C%0AUniLCD%20is%20specifically%20designed%20to%20support%20the%20multiple%20constraints%20of%0Asafety-critical%20end-to-end%20mobile%20systems.%20We%20validate%20the%20proposed%20approach%0Ausing%20a%20challenging%2C%20crowded%20navigation%20task%20requiring%20frequent%20and%20timely%0Aswitching%20between%20local%20and%20cloud%20operations.%20UniLCD%20demonstrates%20improved%0Aoverall%20performance%20and%20efficiency%2C%20by%20over%2035%25%20compared%20to%20state-of-the-art%0Abaselines%20based%20on%20various%20split%20computing%20and%20early%20exit%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLCD%253A%2520Unified%2520Local-Cloud%2520Decision-Making%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DKathakoli%2520Sengupta%2520and%2520Zhongkai%2520Shagguan%2520and%2520Sandesh%2520Bharadwaj%2520and%2520Sanjay%2520Arora%2520and%2520Eshed%2520Ohn-Bar%2520and%2520Renato%2520Mancuso%26entry.1292438233%3D%2520%2520Embodied%2520vision-based%2520real-world%2520systems%252C%2520such%2520as%2520mobile%2520robots%252C%2520require%2520a%250Acareful%2520balance%2520between%2520energy%2520consumption%252C%2520compute%2520latency%252C%2520and%2520safety%250Aconstraints%2520to%2520optimize%2520operation%2520across%2520dynamic%2520tasks%2520and%2520contexts.%2520As%2520local%250Acomputation%2520tends%2520to%2520be%2520restricted%252C%2520offloading%2520the%2520computation%252C%2520ie%252C%2520to%2520a%2520remote%250Aserver%252C%2520can%2520save%2520local%2520resources%2520while%2520providing%2520access%2520to%2520high-quality%250Apredictions%2520from%2520powerful%2520and%2520large%2520models.%2520However%252C%2520the%2520resulting%250Acommunication%2520and%2520latency%2520overhead%2520has%2520led%2520to%2520limited%2520usability%2520of%2520cloud%2520models%250Ain%2520dynamic%252C%2520safety-critical%252C%2520real-time%2520settings.%2520To%2520effectively%2520address%2520this%250Atrade-off%252C%2520we%2520introduce%2520UniLCD%252C%2520a%2520novel%2520hybrid%2520inference%2520framework%2520for%2520enabling%250Aflexible%2520local-cloud%2520collaboration.%2520By%2520efficiently%2520optimizing%2520a%2520flexible%250Arouting%2520module%2520via%2520reinforcement%2520learning%2520and%2520a%2520suitable%2520multi-task%2520objective%252C%250AUniLCD%2520is%2520specifically%2520designed%2520to%2520support%2520the%2520multiple%2520constraints%2520of%250Asafety-critical%2520end-to-end%2520mobile%2520systems.%2520We%2520validate%2520the%2520proposed%2520approach%250Ausing%2520a%2520challenging%252C%2520crowded%2520navigation%2520task%2520requiring%2520frequent%2520and%2520timely%250Aswitching%2520between%2520local%2520and%2520cloud%2520operations.%2520UniLCD%2520demonstrates%2520improved%250Aoverall%2520performance%2520and%2520efficiency%252C%2520by%2520over%252035%2525%2520compared%2520to%2520state-of-the-art%250Abaselines%2520based%2520on%2520various%2520split%2520computing%2520and%2520early%2520exit%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLCD%3A%20Unified%20Local-Cloud%20Decision-Making%20via%20Reinforcement%20Learning&entry.906535625=Kathakoli%20Sengupta%20and%20Zhongkai%20Shagguan%20and%20Sandesh%20Bharadwaj%20and%20Sanjay%20Arora%20and%20Eshed%20Ohn-Bar%20and%20Renato%20Mancuso&entry.1292438233=%20%20Embodied%20vision-based%20real-world%20systems%2C%20such%20as%20mobile%20robots%2C%20require%20a%0Acareful%20balance%20between%20energy%20consumption%2C%20compute%20latency%2C%20and%20safety%0Aconstraints%20to%20optimize%20operation%20across%20dynamic%20tasks%20and%20contexts.%20As%20local%0Acomputation%20tends%20to%20be%20restricted%2C%20offloading%20the%20computation%2C%20ie%2C%20to%20a%20remote%0Aserver%2C%20can%20save%20local%20resources%20while%20providing%20access%20to%20high-quality%0Apredictions%20from%20powerful%20and%20large%20models.%20However%2C%20the%20resulting%0Acommunication%20and%20latency%20overhead%20has%20led%20to%20limited%20usability%20of%20cloud%20models%0Ain%20dynamic%2C%20safety-critical%2C%20real-time%20settings.%20To%20effectively%20address%20this%0Atrade-off%2C%20we%20introduce%20UniLCD%2C%20a%20novel%20hybrid%20inference%20framework%20for%20enabling%0Aflexible%20local-cloud%20collaboration.%20By%20efficiently%20optimizing%20a%20flexible%0Arouting%20module%20via%20reinforcement%20learning%20and%20a%20suitable%20multi-task%20objective%2C%0AUniLCD%20is%20specifically%20designed%20to%20support%20the%20multiple%20constraints%20of%0Asafety-critical%20end-to-end%20mobile%20systems.%20We%20validate%20the%20proposed%20approach%0Ausing%20a%20challenging%2C%20crowded%20navigation%20task%20requiring%20frequent%20and%20timely%0Aswitching%20between%20local%20and%20cloud%20operations.%20UniLCD%20demonstrates%20improved%0Aoverall%20performance%20and%20efficiency%2C%20by%20over%2035%25%20compared%20to%20state-of-the-art%0Abaselines%20based%20on%20various%20split%20computing%20and%20early%20exit%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11403v1&entry.124074799=Read"},
{"title": "Sample Complexity Bounds for Linear System Identification from a Finite\n  Set", "author": "Nicolas Chatzikiriakos and Andrea Iannelli", "abstract": "  This paper considers a finite sample perspective on the problem of\nidentifying an LTI system from a finite set of possible systems using\ntrajectory data. To this end, we use the maximum likelihood estimator to\nidentify the true system and provide an upper bound for its sample complexity.\nCrucially, the derived bound does not rely on a potentially restrictive\nstability assumption. Additionally, we leverage tools from information theory\nto provide a lower bound to the sample complexity that holds independently of\nthe used estimator. The derived sample complexity bounds are analyzed\nanalytically and numerically.\n", "link": "http://arxiv.org/abs/2409.11141v1", "date": "2024-09-17", "relevancy": 1.5407, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.394}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3799}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20Complexity%20Bounds%20for%20Linear%20System%20Identification%20from%20a%20Finite%0A%20%20Set&body=Title%3A%20Sample%20Complexity%20Bounds%20for%20Linear%20System%20Identification%20from%20a%20Finite%0A%20%20Set%0AAuthor%3A%20Nicolas%20Chatzikiriakos%20and%20Andrea%20Iannelli%0AAbstract%3A%20%20%20This%20paper%20considers%20a%20finite%20sample%20perspective%20on%20the%20problem%20of%0Aidentifying%20an%20LTI%20system%20from%20a%20finite%20set%20of%20possible%20systems%20using%0Atrajectory%20data.%20To%20this%20end%2C%20we%20use%20the%20maximum%20likelihood%20estimator%20to%0Aidentify%20the%20true%20system%20and%20provide%20an%20upper%20bound%20for%20its%20sample%20complexity.%0ACrucially%2C%20the%20derived%20bound%20does%20not%20rely%20on%20a%20potentially%20restrictive%0Astability%20assumption.%20Additionally%2C%20we%20leverage%20tools%20from%20information%20theory%0Ato%20provide%20a%20lower%20bound%20to%20the%20sample%20complexity%20that%20holds%20independently%20of%0Athe%20used%20estimator.%20The%20derived%20sample%20complexity%20bounds%20are%20analyzed%0Aanalytically%20and%20numerically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520Complexity%2520Bounds%2520for%2520Linear%2520System%2520Identification%2520from%2520a%2520Finite%250A%2520%2520Set%26entry.906535625%3DNicolas%2520Chatzikiriakos%2520and%2520Andrea%2520Iannelli%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520a%2520finite%2520sample%2520perspective%2520on%2520the%2520problem%2520of%250Aidentifying%2520an%2520LTI%2520system%2520from%2520a%2520finite%2520set%2520of%2520possible%2520systems%2520using%250Atrajectory%2520data.%2520To%2520this%2520end%252C%2520we%2520use%2520the%2520maximum%2520likelihood%2520estimator%2520to%250Aidentify%2520the%2520true%2520system%2520and%2520provide%2520an%2520upper%2520bound%2520for%2520its%2520sample%2520complexity.%250ACrucially%252C%2520the%2520derived%2520bound%2520does%2520not%2520rely%2520on%2520a%2520potentially%2520restrictive%250Astability%2520assumption.%2520Additionally%252C%2520we%2520leverage%2520tools%2520from%2520information%2520theory%250Ato%2520provide%2520a%2520lower%2520bound%2520to%2520the%2520sample%2520complexity%2520that%2520holds%2520independently%2520of%250Athe%2520used%2520estimator.%2520The%2520derived%2520sample%2520complexity%2520bounds%2520are%2520analyzed%250Aanalytically%2520and%2520numerically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20Complexity%20Bounds%20for%20Linear%20System%20Identification%20from%20a%20Finite%0A%20%20Set&entry.906535625=Nicolas%20Chatzikiriakos%20and%20Andrea%20Iannelli&entry.1292438233=%20%20This%20paper%20considers%20a%20finite%20sample%20perspective%20on%20the%20problem%20of%0Aidentifying%20an%20LTI%20system%20from%20a%20finite%20set%20of%20possible%20systems%20using%0Atrajectory%20data.%20To%20this%20end%2C%20we%20use%20the%20maximum%20likelihood%20estimator%20to%0Aidentify%20the%20true%20system%20and%20provide%20an%20upper%20bound%20for%20its%20sample%20complexity.%0ACrucially%2C%20the%20derived%20bound%20does%20not%20rely%20on%20a%20potentially%20restrictive%0Astability%20assumption.%20Additionally%2C%20we%20leverage%20tools%20from%20information%20theory%0Ato%20provide%20a%20lower%20bound%20to%20the%20sample%20complexity%20that%20holds%20independently%20of%0Athe%20used%20estimator.%20The%20derived%20sample%20complexity%20bounds%20are%20analyzed%0Aanalytically%20and%20numerically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11141v1&entry.124074799=Read"},
{"title": "On-policy Actor-Critic Reinforcement Learning for Multi-UAV Exploration", "author": "Ali Moltajaei Farid and Jafar Roshanian and Malek Mouhoub", "abstract": "  Unmanned aerial vehicles (UAVs) have become increasingly popular in various\nfields, including precision agriculture, search and rescue, and remote sensing.\nHowever, exploring unknown environments remains a significant challenge. This\nstudy aims to address this challenge by utilizing on-policy Reinforcement\nLearning (RL) with Proximal Policy Optimization (PPO) to explore the {two\ndimensional} area of interest with multiple UAVs. The UAVs will avoid collision\nwith obstacles and each other and do the exploration in a distributed manner.\nThe proposed solution includes actor-critic networks using deep convolutional\nneural networks {(CNN)} and long short-term memory (LSTM) for identifying the\nUAVs and areas that have already been covered. Compared to other RL techniques,\nsuch as policy gradient (PG) and asynchronous advantage actor-critic (A3C), the\nsimulation results demonstrate the superiority of the proposed PPO approach.\nAlso, the results show that combining LSTM with CNN in critic can improve\nexploration. Since the proposed exploration has to work in unknown\nenvironments, the results showed that the proposed setup can complete the\ncoverage when we have new maps that differ from the trained maps. Finally, we\nshowed how tuning hyper parameters may affect the overall performance.\n", "link": "http://arxiv.org/abs/2409.11058v1", "date": "2024-09-17", "relevancy": 1.6719, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5464}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-policy%20Actor-Critic%20Reinforcement%20Learning%20for%20Multi-UAV%20Exploration&body=Title%3A%20On-policy%20Actor-Critic%20Reinforcement%20Learning%20for%20Multi-UAV%20Exploration%0AAuthor%3A%20Ali%20Moltajaei%20Farid%20and%20Jafar%20Roshanian%20and%20Malek%20Mouhoub%0AAbstract%3A%20%20%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20have%20become%20increasingly%20popular%20in%20various%0Afields%2C%20including%20precision%20agriculture%2C%20search%20and%20rescue%2C%20and%20remote%20sensing.%0AHowever%2C%20exploring%20unknown%20environments%20remains%20a%20significant%20challenge.%20This%0Astudy%20aims%20to%20address%20this%20challenge%20by%20utilizing%20on-policy%20Reinforcement%0ALearning%20%28RL%29%20with%20Proximal%20Policy%20Optimization%20%28PPO%29%20to%20explore%20the%20%7Btwo%0Adimensional%7D%20area%20of%20interest%20with%20multiple%20UAVs.%20The%20UAVs%20will%20avoid%20collision%0Awith%20obstacles%20and%20each%20other%20and%20do%20the%20exploration%20in%20a%20distributed%20manner.%0AThe%20proposed%20solution%20includes%20actor-critic%20networks%20using%20deep%20convolutional%0Aneural%20networks%20%7B%28CNN%29%7D%20and%20long%20short-term%20memory%20%28LSTM%29%20for%20identifying%20the%0AUAVs%20and%20areas%20that%20have%20already%20been%20covered.%20Compared%20to%20other%20RL%20techniques%2C%0Asuch%20as%20policy%20gradient%20%28PG%29%20and%20asynchronous%20advantage%20actor-critic%20%28A3C%29%2C%20the%0Asimulation%20results%20demonstrate%20the%20superiority%20of%20the%20proposed%20PPO%20approach.%0AAlso%2C%20the%20results%20show%20that%20combining%20LSTM%20with%20CNN%20in%20critic%20can%20improve%0Aexploration.%20Since%20the%20proposed%20exploration%20has%20to%20work%20in%20unknown%0Aenvironments%2C%20the%20results%20showed%20that%20the%20proposed%20setup%20can%20complete%20the%0Acoverage%20when%20we%20have%20new%20maps%20that%20differ%20from%20the%20trained%20maps.%20Finally%2C%20we%0Ashowed%20how%20tuning%20hyper%20parameters%20may%20affect%20the%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-policy%2520Actor-Critic%2520Reinforcement%2520Learning%2520for%2520Multi-UAV%2520Exploration%26entry.906535625%3DAli%2520Moltajaei%2520Farid%2520and%2520Jafar%2520Roshanian%2520and%2520Malek%2520Mouhoub%26entry.1292438233%3D%2520%2520Unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520have%2520become%2520increasingly%2520popular%2520in%2520various%250Afields%252C%2520including%2520precision%2520agriculture%252C%2520search%2520and%2520rescue%252C%2520and%2520remote%2520sensing.%250AHowever%252C%2520exploring%2520unknown%2520environments%2520remains%2520a%2520significant%2520challenge.%2520This%250Astudy%2520aims%2520to%2520address%2520this%2520challenge%2520by%2520utilizing%2520on-policy%2520Reinforcement%250ALearning%2520%2528RL%2529%2520with%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520to%2520explore%2520the%2520%257Btwo%250Adimensional%257D%2520area%2520of%2520interest%2520with%2520multiple%2520UAVs.%2520The%2520UAVs%2520will%2520avoid%2520collision%250Awith%2520obstacles%2520and%2520each%2520other%2520and%2520do%2520the%2520exploration%2520in%2520a%2520distributed%2520manner.%250AThe%2520proposed%2520solution%2520includes%2520actor-critic%2520networks%2520using%2520deep%2520convolutional%250Aneural%2520networks%2520%257B%2528CNN%2529%257D%2520and%2520long%2520short-term%2520memory%2520%2528LSTM%2529%2520for%2520identifying%2520the%250AUAVs%2520and%2520areas%2520that%2520have%2520already%2520been%2520covered.%2520Compared%2520to%2520other%2520RL%2520techniques%252C%250Asuch%2520as%2520policy%2520gradient%2520%2528PG%2529%2520and%2520asynchronous%2520advantage%2520actor-critic%2520%2528A3C%2529%252C%2520the%250Asimulation%2520results%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520PPO%2520approach.%250AAlso%252C%2520the%2520results%2520show%2520that%2520combining%2520LSTM%2520with%2520CNN%2520in%2520critic%2520can%2520improve%250Aexploration.%2520Since%2520the%2520proposed%2520exploration%2520has%2520to%2520work%2520in%2520unknown%250Aenvironments%252C%2520the%2520results%2520showed%2520that%2520the%2520proposed%2520setup%2520can%2520complete%2520the%250Acoverage%2520when%2520we%2520have%2520new%2520maps%2520that%2520differ%2520from%2520the%2520trained%2520maps.%2520Finally%252C%2520we%250Ashowed%2520how%2520tuning%2520hyper%2520parameters%2520may%2520affect%2520the%2520overall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-policy%20Actor-Critic%20Reinforcement%20Learning%20for%20Multi-UAV%20Exploration&entry.906535625=Ali%20Moltajaei%20Farid%20and%20Jafar%20Roshanian%20and%20Malek%20Mouhoub&entry.1292438233=%20%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20have%20become%20increasingly%20popular%20in%20various%0Afields%2C%20including%20precision%20agriculture%2C%20search%20and%20rescue%2C%20and%20remote%20sensing.%0AHowever%2C%20exploring%20unknown%20environments%20remains%20a%20significant%20challenge.%20This%0Astudy%20aims%20to%20address%20this%20challenge%20by%20utilizing%20on-policy%20Reinforcement%0ALearning%20%28RL%29%20with%20Proximal%20Policy%20Optimization%20%28PPO%29%20to%20explore%20the%20%7Btwo%0Adimensional%7D%20area%20of%20interest%20with%20multiple%20UAVs.%20The%20UAVs%20will%20avoid%20collision%0Awith%20obstacles%20and%20each%20other%20and%20do%20the%20exploration%20in%20a%20distributed%20manner.%0AThe%20proposed%20solution%20includes%20actor-critic%20networks%20using%20deep%20convolutional%0Aneural%20networks%20%7B%28CNN%29%7D%20and%20long%20short-term%20memory%20%28LSTM%29%20for%20identifying%20the%0AUAVs%20and%20areas%20that%20have%20already%20been%20covered.%20Compared%20to%20other%20RL%20techniques%2C%0Asuch%20as%20policy%20gradient%20%28PG%29%20and%20asynchronous%20advantage%20actor-critic%20%28A3C%29%2C%20the%0Asimulation%20results%20demonstrate%20the%20superiority%20of%20the%20proposed%20PPO%20approach.%0AAlso%2C%20the%20results%20show%20that%20combining%20LSTM%20with%20CNN%20in%20critic%20can%20improve%0Aexploration.%20Since%20the%20proposed%20exploration%20has%20to%20work%20in%20unknown%0Aenvironments%2C%20the%20results%20showed%20that%20the%20proposed%20setup%20can%20complete%20the%0Acoverage%20when%20we%20have%20new%20maps%20that%20differ%20from%20the%20trained%20maps.%20Finally%2C%20we%0Ashowed%20how%20tuning%20hyper%20parameters%20may%20affect%20the%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11058v1&entry.124074799=Read"},
{"title": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation", "author": "Dasom Choi and SoHyun Park and Kyungah Lee and Hwajung Hong and Young-Ho Kim", "abstract": "  As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction.\n", "link": "http://arxiv.org/abs/2409.09641v2", "date": "2024-09-17", "relevancy": 1.8226, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4599}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4587}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AACessTalk%3A%20Fostering%20Communication%20between%20Minimally%20Verbal%20Autistic%0A%20%20Children%20and%20Parents%20with%20Contextual%20Guidance%20and%20Card%20Recommendation&body=Title%3A%20AACessTalk%3A%20Fostering%20Communication%20between%20Minimally%20Verbal%20Autistic%0A%20%20Children%20and%20Parents%20with%20Contextual%20Guidance%20and%20Card%20Recommendation%0AAuthor%3A%20Dasom%20Choi%20and%20SoHyun%20Park%20and%20Kyungah%20Lee%20and%20Hwajung%20Hong%20and%20Young-Ho%20Kim%0AAbstract%3A%20%20%20As%20minimally%20verbal%20autistic%20%28MVA%29%20children%20communicate%20with%20parents%20through%0Afew%20words%20and%20nonverbal%20cues%2C%20parents%20often%20struggle%20to%20encourage%20their%0Achildren%20to%20express%20subtle%20emotions%20and%20needs%20and%20to%20grasp%20their%20nuanced%0Asignals.%20We%20present%20AACessTalk%2C%20a%20tablet-based%2C%20AI-mediated%20communication%0Asystem%20that%20facilitates%20meaningful%20exchanges%20between%20an%20MVA%20child%20and%20a%20parent.%0AAACessTalk%20provides%20real-time%20guides%20to%20the%20parent%20to%20engage%20the%20child%20in%0Aconversation%20and%2C%20in%20turn%2C%20recommends%20contextual%20vocabulary%20cards%20to%20the%20child.%0AThrough%20a%20two-week%20deployment%20study%20with%2011%20MVA%20child-parent%20dyads%2C%20we%20examine%0Ahow%20AACessTalk%20fosters%20everyday%20conversation%20practice%20and%20mutual%20engagement.%0AOur%20findings%20show%20high%20engagement%20from%20all%20dyads%2C%20leading%20to%20increased%0Afrequency%20of%20conversation%20and%20turn-taking.%20AACessTalk%20also%20encouraged%20parents%0Ato%20explore%20their%20own%20interaction%20strategies%20and%20empowered%20the%20children%20to%20have%0Amore%20agency%20in%20communication.%20We%20discuss%20the%20implications%20of%20designing%0Atechnologies%20for%20balanced%20communication%20dynamics%20in%20parent-MVA%20child%0Ainteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAACessTalk%253A%2520Fostering%2520Communication%2520between%2520Minimally%2520Verbal%2520Autistic%250A%2520%2520Children%2520and%2520Parents%2520with%2520Contextual%2520Guidance%2520and%2520Card%2520Recommendation%26entry.906535625%3DDasom%2520Choi%2520and%2520SoHyun%2520Park%2520and%2520Kyungah%2520Lee%2520and%2520Hwajung%2520Hong%2520and%2520Young-Ho%2520Kim%26entry.1292438233%3D%2520%2520As%2520minimally%2520verbal%2520autistic%2520%2528MVA%2529%2520children%2520communicate%2520with%2520parents%2520through%250Afew%2520words%2520and%2520nonverbal%2520cues%252C%2520parents%2520often%2520struggle%2520to%2520encourage%2520their%250Achildren%2520to%2520express%2520subtle%2520emotions%2520and%2520needs%2520and%2520to%2520grasp%2520their%2520nuanced%250Asignals.%2520We%2520present%2520AACessTalk%252C%2520a%2520tablet-based%252C%2520AI-mediated%2520communication%250Asystem%2520that%2520facilitates%2520meaningful%2520exchanges%2520between%2520an%2520MVA%2520child%2520and%2520a%2520parent.%250AAACessTalk%2520provides%2520real-time%2520guides%2520to%2520the%2520parent%2520to%2520engage%2520the%2520child%2520in%250Aconversation%2520and%252C%2520in%2520turn%252C%2520recommends%2520contextual%2520vocabulary%2520cards%2520to%2520the%2520child.%250AThrough%2520a%2520two-week%2520deployment%2520study%2520with%252011%2520MVA%2520child-parent%2520dyads%252C%2520we%2520examine%250Ahow%2520AACessTalk%2520fosters%2520everyday%2520conversation%2520practice%2520and%2520mutual%2520engagement.%250AOur%2520findings%2520show%2520high%2520engagement%2520from%2520all%2520dyads%252C%2520leading%2520to%2520increased%250Afrequency%2520of%2520conversation%2520and%2520turn-taking.%2520AACessTalk%2520also%2520encouraged%2520parents%250Ato%2520explore%2520their%2520own%2520interaction%2520strategies%2520and%2520empowered%2520the%2520children%2520to%2520have%250Amore%2520agency%2520in%2520communication.%2520We%2520discuss%2520the%2520implications%2520of%2520designing%250Atechnologies%2520for%2520balanced%2520communication%2520dynamics%2520in%2520parent-MVA%2520child%250Ainteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AACessTalk%3A%20Fostering%20Communication%20between%20Minimally%20Verbal%20Autistic%0A%20%20Children%20and%20Parents%20with%20Contextual%20Guidance%20and%20Card%20Recommendation&entry.906535625=Dasom%20Choi%20and%20SoHyun%20Park%20and%20Kyungah%20Lee%20and%20Hwajung%20Hong%20and%20Young-Ho%20Kim&entry.1292438233=%20%20As%20minimally%20verbal%20autistic%20%28MVA%29%20children%20communicate%20with%20parents%20through%0Afew%20words%20and%20nonverbal%20cues%2C%20parents%20often%20struggle%20to%20encourage%20their%0Achildren%20to%20express%20subtle%20emotions%20and%20needs%20and%20to%20grasp%20their%20nuanced%0Asignals.%20We%20present%20AACessTalk%2C%20a%20tablet-based%2C%20AI-mediated%20communication%0Asystem%20that%20facilitates%20meaningful%20exchanges%20between%20an%20MVA%20child%20and%20a%20parent.%0AAACessTalk%20provides%20real-time%20guides%20to%20the%20parent%20to%20engage%20the%20child%20in%0Aconversation%20and%2C%20in%20turn%2C%20recommends%20contextual%20vocabulary%20cards%20to%20the%20child.%0AThrough%20a%20two-week%20deployment%20study%20with%2011%20MVA%20child-parent%20dyads%2C%20we%20examine%0Ahow%20AACessTalk%20fosters%20everyday%20conversation%20practice%20and%20mutual%20engagement.%0AOur%20findings%20show%20high%20engagement%20from%20all%20dyads%2C%20leading%20to%20increased%0Afrequency%20of%20conversation%20and%20turn-taking.%20AACessTalk%20also%20encouraged%20parents%0Ato%20explore%20their%20own%20interaction%20strategies%20and%20empowered%20the%20children%20to%20have%0Amore%20agency%20in%20communication.%20We%20discuss%20the%20implications%20of%20designing%0Atechnologies%20for%20balanced%20communication%20dynamics%20in%20parent-MVA%20child%0Ainteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09641v2&entry.124074799=Read"},
{"title": "Fractional Naive Bayes (FNB): non-convex optimization for a parsimonious\n  weighted selective naive Bayes classifier", "author": "Carine Hue and Marc Boull\u00e9", "abstract": "  We study supervised classification for datasets with a very large number of\ninput variables. The na\\\"ive Bayes classifier is attractive for its simplicity,\nscalability and effectiveness in many real data applications. When the strong\nna\\\"ive Bayes assumption of conditional independence of the input variables\ngiven the target variable is not valid, variable selection and model averaging\nare two common ways to improve the performance. In the case of the na\\\"ive\nBayes classifier, the resulting weighting scheme on the models reduces to a\nweighting scheme on the variables. Here we focus on direct estimation of\nvariable weights in such a weighted na\\\"ive Bayes classifier. We propose a\nsparse regularization of the model log-likelihood, which takes into account\nprior penalization costs related to each input variable. Compared to averaging\nbased classifiers used up until now, our main goal is to obtain parsimonious\nrobust models with less variables and equivalent performance. The direct\nestimation of the variable weights amounts to a non-convex optimization problem\nfor which we propose and compare several two-stage algorithms. First, the\ncriterion obtained by convex relaxation is minimized using several variants of\nstandard gradient methods. Then, the initial non-convex optimization problem is\nsolved using local optimization methods initialized with the result of the\nfirst stage. The various proposed algorithms result in optimization-based\nweighted na\\\"ive Bayes classifiers, that are evaluated on benchmark datasets\nand positioned w.r.t. to a reference averaging-based classifier.\n", "link": "http://arxiv.org/abs/2409.11100v1", "date": "2024-09-17", "relevancy": 1.8256, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4535}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fractional%20Naive%20Bayes%20%28FNB%29%3A%20non-convex%20optimization%20for%20a%20parsimonious%0A%20%20weighted%20selective%20naive%20Bayes%20classifier&body=Title%3A%20Fractional%20Naive%20Bayes%20%28FNB%29%3A%20non-convex%20optimization%20for%20a%20parsimonious%0A%20%20weighted%20selective%20naive%20Bayes%20classifier%0AAuthor%3A%20Carine%20Hue%20and%20Marc%20Boull%C3%A9%0AAbstract%3A%20%20%20We%20study%20supervised%20classification%20for%20datasets%20with%20a%20very%20large%20number%20of%0Ainput%20variables.%20The%20na%5C%22ive%20Bayes%20classifier%20is%20attractive%20for%20its%20simplicity%2C%0Ascalability%20and%20effectiveness%20in%20many%20real%20data%20applications.%20When%20the%20strong%0Ana%5C%22ive%20Bayes%20assumption%20of%20conditional%20independence%20of%20the%20input%20variables%0Agiven%20the%20target%20variable%20is%20not%20valid%2C%20variable%20selection%20and%20model%20averaging%0Aare%20two%20common%20ways%20to%20improve%20the%20performance.%20In%20the%20case%20of%20the%20na%5C%22ive%0ABayes%20classifier%2C%20the%20resulting%20weighting%20scheme%20on%20the%20models%20reduces%20to%20a%0Aweighting%20scheme%20on%20the%20variables.%20Here%20we%20focus%20on%20direct%20estimation%20of%0Avariable%20weights%20in%20such%20a%20weighted%20na%5C%22ive%20Bayes%20classifier.%20We%20propose%20a%0Asparse%20regularization%20of%20the%20model%20log-likelihood%2C%20which%20takes%20into%20account%0Aprior%20penalization%20costs%20related%20to%20each%20input%20variable.%20Compared%20to%20averaging%0Abased%20classifiers%20used%20up%20until%20now%2C%20our%20main%20goal%20is%20to%20obtain%20parsimonious%0Arobust%20models%20with%20less%20variables%20and%20equivalent%20performance.%20The%20direct%0Aestimation%20of%20the%20variable%20weights%20amounts%20to%20a%20non-convex%20optimization%20problem%0Afor%20which%20we%20propose%20and%20compare%20several%20two-stage%20algorithms.%20First%2C%20the%0Acriterion%20obtained%20by%20convex%20relaxation%20is%20minimized%20using%20several%20variants%20of%0Astandard%20gradient%20methods.%20Then%2C%20the%20initial%20non-convex%20optimization%20problem%20is%0Asolved%20using%20local%20optimization%20methods%20initialized%20with%20the%20result%20of%20the%0Afirst%20stage.%20The%20various%20proposed%20algorithms%20result%20in%20optimization-based%0Aweighted%20na%5C%22ive%20Bayes%20classifiers%2C%20that%20are%20evaluated%20on%20benchmark%20datasets%0Aand%20positioned%20w.r.t.%20to%20a%20reference%20averaging-based%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFractional%2520Naive%2520Bayes%2520%2528FNB%2529%253A%2520non-convex%2520optimization%2520for%2520a%2520parsimonious%250A%2520%2520weighted%2520selective%2520naive%2520Bayes%2520classifier%26entry.906535625%3DCarine%2520Hue%2520and%2520Marc%2520Boull%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520study%2520supervised%2520classification%2520for%2520datasets%2520with%2520a%2520very%2520large%2520number%2520of%250Ainput%2520variables.%2520The%2520na%255C%2522ive%2520Bayes%2520classifier%2520is%2520attractive%2520for%2520its%2520simplicity%252C%250Ascalability%2520and%2520effectiveness%2520in%2520many%2520real%2520data%2520applications.%2520When%2520the%2520strong%250Ana%255C%2522ive%2520Bayes%2520assumption%2520of%2520conditional%2520independence%2520of%2520the%2520input%2520variables%250Agiven%2520the%2520target%2520variable%2520is%2520not%2520valid%252C%2520variable%2520selection%2520and%2520model%2520averaging%250Aare%2520two%2520common%2520ways%2520to%2520improve%2520the%2520performance.%2520In%2520the%2520case%2520of%2520the%2520na%255C%2522ive%250ABayes%2520classifier%252C%2520the%2520resulting%2520weighting%2520scheme%2520on%2520the%2520models%2520reduces%2520to%2520a%250Aweighting%2520scheme%2520on%2520the%2520variables.%2520Here%2520we%2520focus%2520on%2520direct%2520estimation%2520of%250Avariable%2520weights%2520in%2520such%2520a%2520weighted%2520na%255C%2522ive%2520Bayes%2520classifier.%2520We%2520propose%2520a%250Asparse%2520regularization%2520of%2520the%2520model%2520log-likelihood%252C%2520which%2520takes%2520into%2520account%250Aprior%2520penalization%2520costs%2520related%2520to%2520each%2520input%2520variable.%2520Compared%2520to%2520averaging%250Abased%2520classifiers%2520used%2520up%2520until%2520now%252C%2520our%2520main%2520goal%2520is%2520to%2520obtain%2520parsimonious%250Arobust%2520models%2520with%2520less%2520variables%2520and%2520equivalent%2520performance.%2520The%2520direct%250Aestimation%2520of%2520the%2520variable%2520weights%2520amounts%2520to%2520a%2520non-convex%2520optimization%2520problem%250Afor%2520which%2520we%2520propose%2520and%2520compare%2520several%2520two-stage%2520algorithms.%2520First%252C%2520the%250Acriterion%2520obtained%2520by%2520convex%2520relaxation%2520is%2520minimized%2520using%2520several%2520variants%2520of%250Astandard%2520gradient%2520methods.%2520Then%252C%2520the%2520initial%2520non-convex%2520optimization%2520problem%2520is%250Asolved%2520using%2520local%2520optimization%2520methods%2520initialized%2520with%2520the%2520result%2520of%2520the%250Afirst%2520stage.%2520The%2520various%2520proposed%2520algorithms%2520result%2520in%2520optimization-based%250Aweighted%2520na%255C%2522ive%2520Bayes%2520classifiers%252C%2520that%2520are%2520evaluated%2520on%2520benchmark%2520datasets%250Aand%2520positioned%2520w.r.t.%2520to%2520a%2520reference%2520averaging-based%2520classifier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fractional%20Naive%20Bayes%20%28FNB%29%3A%20non-convex%20optimization%20for%20a%20parsimonious%0A%20%20weighted%20selective%20naive%20Bayes%20classifier&entry.906535625=Carine%20Hue%20and%20Marc%20Boull%C3%A9&entry.1292438233=%20%20We%20study%20supervised%20classification%20for%20datasets%20with%20a%20very%20large%20number%20of%0Ainput%20variables.%20The%20na%5C%22ive%20Bayes%20classifier%20is%20attractive%20for%20its%20simplicity%2C%0Ascalability%20and%20effectiveness%20in%20many%20real%20data%20applications.%20When%20the%20strong%0Ana%5C%22ive%20Bayes%20assumption%20of%20conditional%20independence%20of%20the%20input%20variables%0Agiven%20the%20target%20variable%20is%20not%20valid%2C%20variable%20selection%20and%20model%20averaging%0Aare%20two%20common%20ways%20to%20improve%20the%20performance.%20In%20the%20case%20of%20the%20na%5C%22ive%0ABayes%20classifier%2C%20the%20resulting%20weighting%20scheme%20on%20the%20models%20reduces%20to%20a%0Aweighting%20scheme%20on%20the%20variables.%20Here%20we%20focus%20on%20direct%20estimation%20of%0Avariable%20weights%20in%20such%20a%20weighted%20na%5C%22ive%20Bayes%20classifier.%20We%20propose%20a%0Asparse%20regularization%20of%20the%20model%20log-likelihood%2C%20which%20takes%20into%20account%0Aprior%20penalization%20costs%20related%20to%20each%20input%20variable.%20Compared%20to%20averaging%0Abased%20classifiers%20used%20up%20until%20now%2C%20our%20main%20goal%20is%20to%20obtain%20parsimonious%0Arobust%20models%20with%20less%20variables%20and%20equivalent%20performance.%20The%20direct%0Aestimation%20of%20the%20variable%20weights%20amounts%20to%20a%20non-convex%20optimization%20problem%0Afor%20which%20we%20propose%20and%20compare%20several%20two-stage%20algorithms.%20First%2C%20the%0Acriterion%20obtained%20by%20convex%20relaxation%20is%20minimized%20using%20several%20variants%20of%0Astandard%20gradient%20methods.%20Then%2C%20the%20initial%20non-convex%20optimization%20problem%20is%0Asolved%20using%20local%20optimization%20methods%20initialized%20with%20the%20result%20of%20the%0Afirst%20stage.%20The%20various%20proposed%20algorithms%20result%20in%20optimization-based%0Aweighted%20na%5C%22ive%20Bayes%20classifiers%2C%20that%20are%20evaluated%20on%20benchmark%20datasets%0Aand%20positioned%20w.r.t.%20to%20a%20reference%20averaging-based%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11100v1&entry.124074799=Read"},
{"title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack", "author": "Wei Shao and Chandra Thapa and Rayne Holland and Sarah Ali Siddiqui and Seyit Camtepe", "abstract": "  Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.\n", "link": "http://arxiv.org/abs/2409.11258v1", "date": "2024-09-17", "relevancy": 1.6002, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4214}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3963}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attacking%20Slicing%20Network%20via%20Side-channel%20Reinforcement%20Learning%20Attack&body=Title%3A%20Attacking%20Slicing%20Network%20via%20Side-channel%20Reinforcement%20Learning%20Attack%0AAuthor%3A%20Wei%20Shao%20and%20Chandra%20Thapa%20and%20Rayne%20Holland%20and%20Sarah%20Ali%20Siddiqui%20and%20Seyit%20Camtepe%0AAbstract%3A%20%20%20Network%20slicing%20in%205G%20and%20the%20future%206G%20networks%20will%20enable%20the%20creation%20of%0Amultiple%20virtualized%20networks%20on%20a%20shared%20physical%20infrastructure.%20This%0Ainnovative%20approach%20enables%20the%20provision%20of%20tailored%20networks%20to%20accommodate%0Aspecific%20business%20types%20or%20industry%20users%2C%20thus%20delivering%20more%20customized%20and%0Aefficient%20services.%20However%2C%20the%20shared%20memory%20and%20cache%20in%20network%20slicing%0Aintroduce%20security%20vulnerabilities%20that%20have%20yet%20to%20be%20fully%20addressed.%20In%20this%0Apaper%2C%20we%20introduce%20a%20reinforcement%20learning-based%20side-channel%20cache%20attack%0Aframework%20specifically%20designed%20for%20network%20slicing%20environments.%20Unlike%0Atraditional%20cache%20attack%20methods%2C%20our%20framework%20leverages%20reinforcement%0Alearning%20to%20dynamically%20identify%20and%20exploit%20cache%20locations%20storing%20sensitive%0Ainformation%2C%20such%20as%20authentication%20keys%20and%20user%20registration%20data.%20We%20assume%0Athat%20one%20slice%20network%20is%20compromised%20and%20demonstrate%20how%20the%20attacker%20can%0Ainduce%20another%20shared%20slice%20to%20send%20registration%20requests%2C%20thereby%20estimating%0Athe%20cache%20locations%20of%20critical%20data.%20By%20formulating%20the%20cache%20timing%20channel%0Aattack%20as%20a%20reinforcement%20learning-driven%20guessing%20game%20between%20the%20attack%0Aslice%20and%20the%20victim%20slice%2C%20our%20model%20efficiently%20explores%20possible%20actions%20to%0Apinpoint%20memory%20blocks%20containing%20sensitive%20information.%20Experimental%20results%0Ashowcase%20the%20superiority%20of%20our%20approach%2C%20achieving%20a%20success%20rate%20of%0Aapproximately%2095%5C%25%20to%2098%5C%25%20in%20accurately%20identifying%20the%20storage%20locations%20of%0Asensitive%20data.%20This%20high%20level%20of%20accuracy%20underscores%20the%20potential%20risks%20in%0Ashared%20network%20slicing%20environments%20and%20highlights%20the%20need%20for%20robust%20security%0Ameasures%20to%20safeguard%20against%20such%20advanced%20side-channel%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttacking%2520Slicing%2520Network%2520via%2520Side-channel%2520Reinforcement%2520Learning%2520Attack%26entry.906535625%3DWei%2520Shao%2520and%2520Chandra%2520Thapa%2520and%2520Rayne%2520Holland%2520and%2520Sarah%2520Ali%2520Siddiqui%2520and%2520Seyit%2520Camtepe%26entry.1292438233%3D%2520%2520Network%2520slicing%2520in%25205G%2520and%2520the%2520future%25206G%2520networks%2520will%2520enable%2520the%2520creation%2520of%250Amultiple%2520virtualized%2520networks%2520on%2520a%2520shared%2520physical%2520infrastructure.%2520This%250Ainnovative%2520approach%2520enables%2520the%2520provision%2520of%2520tailored%2520networks%2520to%2520accommodate%250Aspecific%2520business%2520types%2520or%2520industry%2520users%252C%2520thus%2520delivering%2520more%2520customized%2520and%250Aefficient%2520services.%2520However%252C%2520the%2520shared%2520memory%2520and%2520cache%2520in%2520network%2520slicing%250Aintroduce%2520security%2520vulnerabilities%2520that%2520have%2520yet%2520to%2520be%2520fully%2520addressed.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520reinforcement%2520learning-based%2520side-channel%2520cache%2520attack%250Aframework%2520specifically%2520designed%2520for%2520network%2520slicing%2520environments.%2520Unlike%250Atraditional%2520cache%2520attack%2520methods%252C%2520our%2520framework%2520leverages%2520reinforcement%250Alearning%2520to%2520dynamically%2520identify%2520and%2520exploit%2520cache%2520locations%2520storing%2520sensitive%250Ainformation%252C%2520such%2520as%2520authentication%2520keys%2520and%2520user%2520registration%2520data.%2520We%2520assume%250Athat%2520one%2520slice%2520network%2520is%2520compromised%2520and%2520demonstrate%2520how%2520the%2520attacker%2520can%250Ainduce%2520another%2520shared%2520slice%2520to%2520send%2520registration%2520requests%252C%2520thereby%2520estimating%250Athe%2520cache%2520locations%2520of%2520critical%2520data.%2520By%2520formulating%2520the%2520cache%2520timing%2520channel%250Aattack%2520as%2520a%2520reinforcement%2520learning-driven%2520guessing%2520game%2520between%2520the%2520attack%250Aslice%2520and%2520the%2520victim%2520slice%252C%2520our%2520model%2520efficiently%2520explores%2520possible%2520actions%2520to%250Apinpoint%2520memory%2520blocks%2520containing%2520sensitive%2520information.%2520Experimental%2520results%250Ashowcase%2520the%2520superiority%2520of%2520our%2520approach%252C%2520achieving%2520a%2520success%2520rate%2520of%250Aapproximately%252095%255C%2525%2520to%252098%255C%2525%2520in%2520accurately%2520identifying%2520the%2520storage%2520locations%2520of%250Asensitive%2520data.%2520This%2520high%2520level%2520of%2520accuracy%2520underscores%2520the%2520potential%2520risks%2520in%250Ashared%2520network%2520slicing%2520environments%2520and%2520highlights%2520the%2520need%2520for%2520robust%2520security%250Ameasures%2520to%2520safeguard%2520against%2520such%2520advanced%2520side-channel%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attacking%20Slicing%20Network%20via%20Side-channel%20Reinforcement%20Learning%20Attack&entry.906535625=Wei%20Shao%20and%20Chandra%20Thapa%20and%20Rayne%20Holland%20and%20Sarah%20Ali%20Siddiqui%20and%20Seyit%20Camtepe&entry.1292438233=%20%20Network%20slicing%20in%205G%20and%20the%20future%206G%20networks%20will%20enable%20the%20creation%20of%0Amultiple%20virtualized%20networks%20on%20a%20shared%20physical%20infrastructure.%20This%0Ainnovative%20approach%20enables%20the%20provision%20of%20tailored%20networks%20to%20accommodate%0Aspecific%20business%20types%20or%20industry%20users%2C%20thus%20delivering%20more%20customized%20and%0Aefficient%20services.%20However%2C%20the%20shared%20memory%20and%20cache%20in%20network%20slicing%0Aintroduce%20security%20vulnerabilities%20that%20have%20yet%20to%20be%20fully%20addressed.%20In%20this%0Apaper%2C%20we%20introduce%20a%20reinforcement%20learning-based%20side-channel%20cache%20attack%0Aframework%20specifically%20designed%20for%20network%20slicing%20environments.%20Unlike%0Atraditional%20cache%20attack%20methods%2C%20our%20framework%20leverages%20reinforcement%0Alearning%20to%20dynamically%20identify%20and%20exploit%20cache%20locations%20storing%20sensitive%0Ainformation%2C%20such%20as%20authentication%20keys%20and%20user%20registration%20data.%20We%20assume%0Athat%20one%20slice%20network%20is%20compromised%20and%20demonstrate%20how%20the%20attacker%20can%0Ainduce%20another%20shared%20slice%20to%20send%20registration%20requests%2C%20thereby%20estimating%0Athe%20cache%20locations%20of%20critical%20data.%20By%20formulating%20the%20cache%20timing%20channel%0Aattack%20as%20a%20reinforcement%20learning-driven%20guessing%20game%20between%20the%20attack%0Aslice%20and%20the%20victim%20slice%2C%20our%20model%20efficiently%20explores%20possible%20actions%20to%0Apinpoint%20memory%20blocks%20containing%20sensitive%20information.%20Experimental%20results%0Ashowcase%20the%20superiority%20of%20our%20approach%2C%20achieving%20a%20success%20rate%20of%0Aapproximately%2095%5C%25%20to%2098%5C%25%20in%20accurately%20identifying%20the%20storage%20locations%20of%0Asensitive%20data.%20This%20high%20level%20of%20accuracy%20underscores%20the%20potential%20risks%20in%0Ashared%20network%20slicing%20environments%20and%20highlights%20the%20need%20for%20robust%20security%0Ameasures%20to%20safeguard%20against%20such%20advanced%20side-channel%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11258v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


