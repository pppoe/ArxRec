<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Robust 3D Object Detection from LiDAR-Radar Point Clouds via Cross-Modal\n  Feature Augmentation", "author": "Jianning Deng and Gabriel Chan and Hantao Zhong and Chris Xiaoxuan Lu", "abstract": "  This paper presents a novel framework for robust 3D object detection from\npoint clouds via cross-modal hallucination. Our proposed approach is agnostic\nto either hallucination direction between LiDAR and 4D radar. We introduce\nmultiple alignments on both spatial and feature levels to achieve simultaneous\nbackbone refinement and hallucination generation. Specifically, spatial\nalignment is proposed to deal with the geometry discrepancy for better instance\nmatching between LiDAR and radar. The feature alignment step further bridges\nthe intrinsic attribute gap between the sensing modalities and stabilizes the\ntraining. The trained object detection models can deal with difficult detection\ncases better, even though only single-modal data is used as the input during\nthe inference stage. Extensive experiments on the View-of-Delft (VoD) dataset\nshow that our proposed method outperforms the state-of-the-art (SOTA) methods\nfor both radar and LiDAR object detection while maintaining competitive\nefficiency in runtime.\n", "link": "http://arxiv.org/abs/2309.17336v2", "date": "2024-03-11", "relevancy": 2.9763, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5908}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5738}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Robust%203D%20Object%20Detection%20from%20LiDAR-Radar%20Point%20Clouds%20via%20Cross-Modal%0A%20%20Feature%20Augmentation&body=Title%3A%20Robust%203D%20Object%20Detection%20from%20LiDAR-Radar%20Point%20Clouds%20via%20Cross-Modal%0A%20%20Feature%20Augmentation%0AAuthor%3A%20Jianning%20Deng%20and%20Gabriel%20Chan%20and%20Hantao%20Zhong%20and%20Chris%20Xiaoxuan%20Lu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20robust%203D%20object%20detection%20from%0Apoint%20clouds%20via%20cross-modal%20hallucination.%20Our%20proposed%20approach%20is%20agnostic%0Ato%20either%20hallucination%20direction%20between%20LiDAR%20and%204D%20radar.%20We%20introduce%0Amultiple%20alignments%20on%20both%20spatial%20and%20feature%20levels%20to%20achieve%20simultaneous%0Abackbone%20refinement%20and%20hallucination%20generation.%20Specifically%2C%20spatial%0Aalignment%20is%20proposed%20to%20deal%20with%20the%20geometry%20discrepancy%20for%20better%20instance%0Amatching%20between%20LiDAR%20and%20radar.%20The%20feature%20alignment%20step%20further%20bridges%0Athe%20intrinsic%20attribute%20gap%20between%20the%20sensing%20modalities%20and%20stabilizes%20the%0Atraining.%20The%20trained%20object%20detection%20models%20can%20deal%20with%20difficult%20detection%0Acases%20better%2C%20even%20though%20only%20single-modal%20data%20is%20used%20as%20the%20input%20during%0Athe%20inference%20stage.%20Extensive%20experiments%20on%20the%20View-of-Delft%20%28VoD%29%20dataset%0Ashow%20that%20our%20proposed%20method%20outperforms%20the%20state-of-the-art%20%28SOTA%29%20methods%0Afor%20both%20radar%20and%20LiDAR%20object%20detection%20while%20maintaining%20competitive%0Aefficiency%20in%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17336v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%203D%20Object%20Detection%20from%20LiDAR-Radar%20Point%20Clouds%20via%20Cross-Modal%0A%20%20Feature%20Augmentation&entry.906535625=Jianning%20Deng%20and%20Gabriel%20Chan%20and%20Hantao%20Zhong%20and%20Chris%20Xiaoxuan%20Lu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20robust%203D%20object%20detection%20from%0Apoint%20clouds%20via%20cross-modal%20hallucination.%20Our%20proposed%20approach%20is%20agnostic%0Ato%20either%20hallucination%20direction%20between%20LiDAR%20and%204D%20radar.%20We%20introduce%0Amultiple%20alignments%20on%20both%20spatial%20and%20feature%20levels%20to%20achieve%20simultaneous%0Abackbone%20refinement%20and%20hallucination%20generation.%20Specifically%2C%20spatial%0Aalignment%20is%20proposed%20to%20deal%20with%20the%20geometry%20discrepancy%20for%20better%20instance%0Amatching%20between%20LiDAR%20and%20radar.%20The%20feature%20alignment%20step%20further%20bridges%0Athe%20intrinsic%20attribute%20gap%20between%20the%20sensing%20modalities%20and%20stabilizes%20the%0Atraining.%20The%20trained%20object%20detection%20models%20can%20deal%20with%20difficult%20detection%0Acases%20better%2C%20even%20though%20only%20single-modal%20data%20is%20used%20as%20the%20input%20during%0Athe%20inference%20stage.%20Extensive%20experiments%20on%20the%20View-of-Delft%20%28VoD%29%20dataset%0Ashow%20that%20our%20proposed%20method%20outperforms%20the%20state-of-the-art%20%28SOTA%29%20methods%0Afor%20both%20radar%20and%20LiDAR%20object%20detection%20while%20maintaining%20competitive%0Aefficiency%20in%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17336v2&entry.124074799=Read"},
{"title": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization", "author": "Jinlu Zhang and Yiyi Zhou and Qiancheng Zheng and Xiaoxiong Du and Gen Luo and Jun Peng and Xiaoshuai Sun and Rongrong Ji", "abstract": "  Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code are released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet.\n", "link": "http://arxiv.org/abs/2403.06702v1", "date": "2024-03-11", "relevancy": 2.9586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5983}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5897}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5872}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Fast%20Text-to-3D-Aware%20Face%20Generation%20and%20Manipulation%20via%20Direct%0A%20%20Cross-modal%20Mapping%20and%20Geometric%20Regularization&body=Title%3A%20Fast%20Text-to-3D-Aware%20Face%20Generation%20and%20Manipulation%20via%20Direct%0A%20%20Cross-modal%20Mapping%20and%20Geometric%20Regularization%0AAuthor%3A%20Jinlu%20Zhang%20and%20Yiyi%20Zhou%20and%20Qiancheng%20Zheng%20and%20Xiaoxiong%20Du%20and%20Gen%20Luo%20and%20Jun%20Peng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Text-to-3D-aware%20face%20%28T3D%20Face%29%20generation%20and%20manipulation%20is%20an%20emerging%0Aresearch%20hot%20spot%20in%20machine%20learning%2C%20which%20still%20suffers%20from%20low%20efficiency%0Aand%20poor%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20End-to-End%20Efficient%20and%0AEffective%20network%20for%20fast%20and%20accurate%20T3D%20face%20generation%20and%20manipulation%2C%0Atermed%20%24E%5E3%24-FaceNet.%20Different%20from%20existing%20complex%20generation%20paradigms%2C%0A%24E%5E3%24-FaceNet%20resorts%20to%20a%20direct%20mapping%20from%20text%20instructions%20to%203D-aware%0Avisual%20space.%20We%20introduce%20a%20novel%20Style%20Code%20Enhancer%20to%20enhance%20cross-modal%0Asemantic%20alignment%2C%20alongside%20an%20innovative%20Geometric%20Regularization%20objective%0Ato%20maintain%20consistency%20across%20multi-view%20generations.%20Extensive%20experiments%20on%0Athree%20benchmark%20datasets%20demonstrate%20that%20%24E%5E3%24-FaceNet%20can%20not%20only%20achieve%0Apicture-like%203D%20face%20generation%20and%20manipulation%2C%20but%20also%20improve%20inference%0Aspeed%20by%20orders%20of%20magnitudes.%20For%20instance%2C%20compared%20with%20Latent3D%2C%0A%24E%5E3%24-FaceNet%20speeds%20up%20the%20five-view%20generations%20by%20almost%20470%20times%2C%20while%0Astill%20exceeding%20in%20generation%20quality.%20Our%20code%20are%20released%20at%0Ahttps%3A//github.com/Aria-Zhangjl/E3-FaceNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06702v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Text-to-3D-Aware%20Face%20Generation%20and%20Manipulation%20via%20Direct%0A%20%20Cross-modal%20Mapping%20and%20Geometric%20Regularization&entry.906535625=Jinlu%20Zhang%20and%20Yiyi%20Zhou%20and%20Qiancheng%20Zheng%20and%20Xiaoxiong%20Du%20and%20Gen%20Luo%20and%20Jun%20Peng%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Text-to-3D-aware%20face%20%28T3D%20Face%29%20generation%20and%20manipulation%20is%20an%20emerging%0Aresearch%20hot%20spot%20in%20machine%20learning%2C%20which%20still%20suffers%20from%20low%20efficiency%0Aand%20poor%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20End-to-End%20Efficient%20and%0AEffective%20network%20for%20fast%20and%20accurate%20T3D%20face%20generation%20and%20manipulation%2C%0Atermed%20%24E%5E3%24-FaceNet.%20Different%20from%20existing%20complex%20generation%20paradigms%2C%0A%24E%5E3%24-FaceNet%20resorts%20to%20a%20direct%20mapping%20from%20text%20instructions%20to%203D-aware%0Avisual%20space.%20We%20introduce%20a%20novel%20Style%20Code%20Enhancer%20to%20enhance%20cross-modal%0Asemantic%20alignment%2C%20alongside%20an%20innovative%20Geometric%20Regularization%20objective%0Ato%20maintain%20consistency%20across%20multi-view%20generations.%20Extensive%20experiments%20on%0Athree%20benchmark%20datasets%20demonstrate%20that%20%24E%5E3%24-FaceNet%20can%20not%20only%20achieve%0Apicture-like%203D%20face%20generation%20and%20manipulation%2C%20but%20also%20improve%20inference%0Aspeed%20by%20orders%20of%20magnitudes.%20For%20instance%2C%20compared%20with%20Latent3D%2C%0A%24E%5E3%24-FaceNet%20speeds%20up%20the%20five-view%20generations%20by%20almost%20470%20times%2C%20while%0Astill%20exceeding%20in%20generation%20quality.%20Our%20code%20are%20released%20at%0Ahttps%3A//github.com/Aria-Zhangjl/E3-FaceNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06702v1&entry.124074799=Read"},
{"title": "HDA-LVIO: A High-Precision LiDAR-Visual-Inertial Odometry in Urban\n  Environments with Hybrid Data Association", "author": "Jian Shi and Wei Wang and Mingyang Qi and Xin Li and Ye Yan", "abstract": "  To enhance localization accuracy in urban environments, an innovative\nLiDAR-Visual-Inertial odometry, named HDA-LVIO, is proposed by employing hybrid\ndata association. The proposed HDA_LVIO system can be divided into two\nsubsystems: the LiDAR-Inertial subsystem (LIS) and the Visual-Inertial\nsubsystem (VIS). In the LIS, the LiDAR pointcloud is utilized to calculate the\nIterative Closest Point (ICP) error, serving as the measurement value of Error\nState Iterated Kalman Filter (ESIKF) to construct the global map. In the VIS,\nan incremental method is firstly employed to adaptively extract planes from the\nglobal map. And the centroids of these planes are projected onto the image to\nobtain projection points. Then, feature points are extracted from the image and\ntracked along with projection points using Lucas-Kanade (LK) optical flow.\nNext, leveraging the vehicle states from previous intervals, sliding window\noptimization is performed to estimate the depth of feature points.\nConcurrently, a method based on epipolar geometric constraints is proposed to\naddress tracking failures for feature points, which can improve the accuracy of\ndepth estimation for feature points by ensuring sufficient parallax within the\nsliding window. Subsequently, the feature points and projection points are\nhybridly associated to construct reprojection error, serving as the measurement\nvalue of ESIKF to estimate vehicle states. Finally, the localization accuracy\nof the proposed HDA-LVIO is validated using public datasets and data from our\nequipment. The results demonstrate that the proposed algorithm achieves\nobviously improvement in localization accuracy compared to various existing\nalgorithms.\n", "link": "http://arxiv.org/abs/2403.06590v1", "date": "2024-03-11", "relevancy": 2.9573, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6141}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6002}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5601}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20HDA-LVIO%3A%20A%20High-Precision%20LiDAR-Visual-Inertial%20Odometry%20in%20Urban%0A%20%20Environments%20with%20Hybrid%20Data%20Association&body=Title%3A%20HDA-LVIO%3A%20A%20High-Precision%20LiDAR-Visual-Inertial%20Odometry%20in%20Urban%0A%20%20Environments%20with%20Hybrid%20Data%20Association%0AAuthor%3A%20Jian%20Shi%20and%20Wei%20Wang%20and%20Mingyang%20Qi%20and%20Xin%20Li%20and%20Ye%20Yan%0AAbstract%3A%20%20%20To%20enhance%20localization%20accuracy%20in%20urban%20environments%2C%20an%20innovative%0ALiDAR-Visual-Inertial%20odometry%2C%20named%20HDA-LVIO%2C%20is%20proposed%20by%20employing%20hybrid%0Adata%20association.%20The%20proposed%20HDA_LVIO%20system%20can%20be%20divided%20into%20two%0Asubsystems%3A%20the%20LiDAR-Inertial%20subsystem%20%28LIS%29%20and%20the%20Visual-Inertial%0Asubsystem%20%28VIS%29.%20In%20the%20LIS%2C%20the%20LiDAR%20pointcloud%20is%20utilized%20to%20calculate%20the%0AIterative%20Closest%20Point%20%28ICP%29%20error%2C%20serving%20as%20the%20measurement%20value%20of%20Error%0AState%20Iterated%20Kalman%20Filter%20%28ESIKF%29%20to%20construct%20the%20global%20map.%20In%20the%20VIS%2C%0Aan%20incremental%20method%20is%20firstly%20employed%20to%20adaptively%20extract%20planes%20from%20the%0Aglobal%20map.%20And%20the%20centroids%20of%20these%20planes%20are%20projected%20onto%20the%20image%20to%0Aobtain%20projection%20points.%20Then%2C%20feature%20points%20are%20extracted%20from%20the%20image%20and%0Atracked%20along%20with%20projection%20points%20using%20Lucas-Kanade%20%28LK%29%20optical%20flow.%0ANext%2C%20leveraging%20the%20vehicle%20states%20from%20previous%20intervals%2C%20sliding%20window%0Aoptimization%20is%20performed%20to%20estimate%20the%20depth%20of%20feature%20points.%0AConcurrently%2C%20a%20method%20based%20on%20epipolar%20geometric%20constraints%20is%20proposed%20to%0Aaddress%20tracking%20failures%20for%20feature%20points%2C%20which%20can%20improve%20the%20accuracy%20of%0Adepth%20estimation%20for%20feature%20points%20by%20ensuring%20sufficient%20parallax%20within%20the%0Asliding%20window.%20Subsequently%2C%20the%20feature%20points%20and%20projection%20points%20are%0Ahybridly%20associated%20to%20construct%20reprojection%20error%2C%20serving%20as%20the%20measurement%0Avalue%20of%20ESIKF%20to%20estimate%20vehicle%20states.%20Finally%2C%20the%20localization%20accuracy%0Aof%20the%20proposed%20HDA-LVIO%20is%20validated%20using%20public%20datasets%20and%20data%20from%20our%0Aequipment.%20The%20results%20demonstrate%20that%20the%20proposed%20algorithm%20achieves%0Aobviously%20improvement%20in%20localization%20accuracy%20compared%20to%20various%20existing%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06590v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDA-LVIO%3A%20A%20High-Precision%20LiDAR-Visual-Inertial%20Odometry%20in%20Urban%0A%20%20Environments%20with%20Hybrid%20Data%20Association&entry.906535625=Jian%20Shi%20and%20Wei%20Wang%20and%20Mingyang%20Qi%20and%20Xin%20Li%20and%20Ye%20Yan&entry.1292438233=%20%20To%20enhance%20localization%20accuracy%20in%20urban%20environments%2C%20an%20innovative%0ALiDAR-Visual-Inertial%20odometry%2C%20named%20HDA-LVIO%2C%20is%20proposed%20by%20employing%20hybrid%0Adata%20association.%20The%20proposed%20HDA_LVIO%20system%20can%20be%20divided%20into%20two%0Asubsystems%3A%20the%20LiDAR-Inertial%20subsystem%20%28LIS%29%20and%20the%20Visual-Inertial%0Asubsystem%20%28VIS%29.%20In%20the%20LIS%2C%20the%20LiDAR%20pointcloud%20is%20utilized%20to%20calculate%20the%0AIterative%20Closest%20Point%20%28ICP%29%20error%2C%20serving%20as%20the%20measurement%20value%20of%20Error%0AState%20Iterated%20Kalman%20Filter%20%28ESIKF%29%20to%20construct%20the%20global%20map.%20In%20the%20VIS%2C%0Aan%20incremental%20method%20is%20firstly%20employed%20to%20adaptively%20extract%20planes%20from%20the%0Aglobal%20map.%20And%20the%20centroids%20of%20these%20planes%20are%20projected%20onto%20the%20image%20to%0Aobtain%20projection%20points.%20Then%2C%20feature%20points%20are%20extracted%20from%20the%20image%20and%0Atracked%20along%20with%20projection%20points%20using%20Lucas-Kanade%20%28LK%29%20optical%20flow.%0ANext%2C%20leveraging%20the%20vehicle%20states%20from%20previous%20intervals%2C%20sliding%20window%0Aoptimization%20is%20performed%20to%20estimate%20the%20depth%20of%20feature%20points.%0AConcurrently%2C%20a%20method%20based%20on%20epipolar%20geometric%20constraints%20is%20proposed%20to%0Aaddress%20tracking%20failures%20for%20feature%20points%2C%20which%20can%20improve%20the%20accuracy%20of%0Adepth%20estimation%20for%20feature%20points%20by%20ensuring%20sufficient%20parallax%20within%20the%0Asliding%20window.%20Subsequently%2C%20the%20feature%20points%20and%20projection%20points%20are%0Ahybridly%20associated%20to%20construct%20reprojection%20error%2C%20serving%20as%20the%20measurement%0Avalue%20of%20ESIKF%20to%20estimate%20vehicle%20states.%20Finally%2C%20the%20localization%20accuracy%0Aof%20the%20proposed%20HDA-LVIO%20is%20validated%20using%20public%20datasets%20and%20data%20from%20our%0Aequipment.%20The%20results%20demonstrate%20that%20the%20proposed%20algorithm%20achieves%0Aobviously%20improvement%20in%20localization%20accuracy%20compared%20to%20various%20existing%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06590v1&entry.124074799=Read"},
{"title": "Probabilistic Contrastive Learning for Long-Tailed Visual Recognition", "author": "Chaoqun Du and Yulin Wang and Shiji Song and Gao Huang", "abstract": "  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n", "link": "http://arxiv.org/abs/2403.06726v1", "date": "2024-03-11", "relevancy": 2.8621, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.618}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5555}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5437}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition&body=Title%3A%20Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition%0AAuthor%3A%20Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Long-tailed%20distributions%20frequently%20emerge%20in%20real-world%20data%2C%20where%20a%20large%0Anumber%20of%20minority%20categories%20contain%20a%20limited%20number%20of%20samples.%20Such%0Aimbalance%20issue%20considerably%20impairs%20the%20performance%20of%20standard%20supervised%0Alearning%20algorithms%2C%20which%20are%20mainly%20designed%20for%20balanced%20training%20sets.%0ARecent%20investigations%20have%20revealed%20that%20supervised%20contrastive%20learning%0Aexhibits%20promising%20potential%20in%20alleviating%20the%20data%20imbalance.%20However%2C%20the%0Aperformance%20of%20supervised%20contrastive%20learning%20is%20plagued%20by%20an%20inherent%0Achallenge%3A%20it%20necessitates%20sufficiently%20large%20batches%20of%20training%20data%20to%0Aconstruct%20contrastive%20pairs%20that%20cover%20all%20categories%2C%20yet%20this%20requirement%20is%0Adifficult%20to%20meet%20in%20the%20context%20of%20class-imbalanced%20data.%20To%20overcome%20this%0Aobstacle%2C%20we%20propose%20a%20novel%20probabilistic%20contrastive%20%28ProCo%29%20learning%0Aalgorithm%20that%20estimates%20the%20data%20distribution%20of%20the%20samples%20from%20each%20class%0Ain%20the%20feature%20space%2C%20and%20samples%20contrastive%20pairs%20accordingly.%20In%20fact%2C%0Aestimating%20the%20distributions%20of%20all%20classes%20using%20features%20in%20a%20small%20batch%2C%0Aparticularly%20for%20imbalanced%20data%2C%20is%20not%20feasible.%20Our%20key%20idea%20is%20to%20introduce%0Aa%20reasonable%20and%20simple%20assumption%20that%20the%20normalized%20features%20in%20contrastive%0Alearning%20follow%20a%20mixture%20of%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20unit%0Aspace%2C%20which%20brings%20two-fold%20benefits.%20First%2C%20the%20distribution%20parameters%20can%0Abe%20estimated%20using%20only%20the%20first%20sample%20moment%2C%20which%20can%20be%20efficiently%0Acomputed%20in%20an%20online%20manner%20across%20different%20batches.%20Second%2C%20based%20on%20the%0Aestimated%20distribution%2C%20the%20vMF%20distribution%20allows%20us%20to%20sample%20an%20infinite%0Anumber%20of%20contrastive%20pairs%20and%20derive%20a%20closed%20form%20of%20the%20expected%0Acontrastive%20loss%20for%20efficient%20optimization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/ProCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06726v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition&entry.906535625=Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Long-tailed%20distributions%20frequently%20emerge%20in%20real-world%20data%2C%20where%20a%20large%0Anumber%20of%20minority%20categories%20contain%20a%20limited%20number%20of%20samples.%20Such%0Aimbalance%20issue%20considerably%20impairs%20the%20performance%20of%20standard%20supervised%0Alearning%20algorithms%2C%20which%20are%20mainly%20designed%20for%20balanced%20training%20sets.%0ARecent%20investigations%20have%20revealed%20that%20supervised%20contrastive%20learning%0Aexhibits%20promising%20potential%20in%20alleviating%20the%20data%20imbalance.%20However%2C%20the%0Aperformance%20of%20supervised%20contrastive%20learning%20is%20plagued%20by%20an%20inherent%0Achallenge%3A%20it%20necessitates%20sufficiently%20large%20batches%20of%20training%20data%20to%0Aconstruct%20contrastive%20pairs%20that%20cover%20all%20categories%2C%20yet%20this%20requirement%20is%0Adifficult%20to%20meet%20in%20the%20context%20of%20class-imbalanced%20data.%20To%20overcome%20this%0Aobstacle%2C%20we%20propose%20a%20novel%20probabilistic%20contrastive%20%28ProCo%29%20learning%0Aalgorithm%20that%20estimates%20the%20data%20distribution%20of%20the%20samples%20from%20each%20class%0Ain%20the%20feature%20space%2C%20and%20samples%20contrastive%20pairs%20accordingly.%20In%20fact%2C%0Aestimating%20the%20distributions%20of%20all%20classes%20using%20features%20in%20a%20small%20batch%2C%0Aparticularly%20for%20imbalanced%20data%2C%20is%20not%20feasible.%20Our%20key%20idea%20is%20to%20introduce%0Aa%20reasonable%20and%20simple%20assumption%20that%20the%20normalized%20features%20in%20contrastive%0Alearning%20follow%20a%20mixture%20of%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20unit%0Aspace%2C%20which%20brings%20two-fold%20benefits.%20First%2C%20the%20distribution%20parameters%20can%0Abe%20estimated%20using%20only%20the%20first%20sample%20moment%2C%20which%20can%20be%20efficiently%0Acomputed%20in%20an%20online%20manner%20across%20different%20batches.%20Second%2C%20based%20on%20the%0Aestimated%20distribution%2C%20the%20vMF%20distribution%20allows%20us%20to%20sample%20an%20infinite%0Anumber%20of%20contrastive%20pairs%20and%20derive%20a%20closed%20form%20of%20the%20expected%0Acontrastive%20loss%20for%20efficient%20optimization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/ProCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06726v1&entry.124074799=Read"},
{"title": "CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar\n  Class-Incremental Learnin", "author": "Xinyuan Gao and Songlin Dong and Yuhang He and Xing Wei and Yihong Gong", "abstract": "  In real-world applications, dynamic scenarios require the models to possess\nthe capability to learn new tasks continuously without forgetting the old\nknowledge. Experience-Replay methods store a subset of the old images for joint\ntraining. In the scenario of more strict privacy protection, storing the old\nimages becomes infeasible, which leads to a more severe plasticity-stability\ndilemma and classifier bias. To meet the above challenges, we propose a new\narchitecture, named continual expansion and absorption transformer~(CEAT). The\nmodel can learn the novel knowledge by extending the expanded-fusion layers in\nparallel with the frozen previous parameters. After the task ends, we\nlosslessly absorb the extended parameters into the backbone to ensure that the\nnumber of parameters remains constant. To improve the learning ability of the\nmodel, we designed a novel prototype contrastive loss to reduce the overlap\nbetween old and new classes in the feature space. Besides, to address the\nclassifier bias towards the new classes, we propose a novel approach to\ngenerate the pseudo-features to correct the classifier. We experiment with our\nmethods on three standard Non-Exemplar Class-Incremental Learning~(NECIL)\nbenchmarks. Extensive experiments demonstrate that our model gets a significant\nimprovement compared with the previous works and achieves 5.38%, 5.20%, and\n4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.\n", "link": "http://arxiv.org/abs/2403.06670v1", "date": "2024-03-11", "relevancy": 2.8551, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5927}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5656}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5548}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20CEAT%3A%20Continual%20Expansion%20and%20Absorption%20Transformer%20for%20Non-Exemplar%0A%20%20Class-Incremental%20Learnin&body=Title%3A%20CEAT%3A%20Continual%20Expansion%20and%20Absorption%20Transformer%20for%20Non-Exemplar%0A%20%20Class-Incremental%20Learnin%0AAuthor%3A%20Xinyuan%20Gao%20and%20Songlin%20Dong%20and%20Yuhang%20He%20and%20Xing%20Wei%20and%20Yihong%20Gong%0AAbstract%3A%20%20%20In%20real-world%20applications%2C%20dynamic%20scenarios%20require%20the%20models%20to%20possess%0Athe%20capability%20to%20learn%20new%20tasks%20continuously%20without%20forgetting%20the%20old%0Aknowledge.%20Experience-Replay%20methods%20store%20a%20subset%20of%20the%20old%20images%20for%20joint%0Atraining.%20In%20the%20scenario%20of%20more%20strict%20privacy%20protection%2C%20storing%20the%20old%0Aimages%20becomes%20infeasible%2C%20which%20leads%20to%20a%20more%20severe%20plasticity-stability%0Adilemma%20and%20classifier%20bias.%20To%20meet%20the%20above%20challenges%2C%20we%20propose%20a%20new%0Aarchitecture%2C%20named%20continual%20expansion%20and%20absorption%20transformer~%28CEAT%29.%20The%0Amodel%20can%20learn%20the%20novel%20knowledge%20by%20extending%20the%20expanded-fusion%20layers%20in%0Aparallel%20with%20the%20frozen%20previous%20parameters.%20After%20the%20task%20ends%2C%20we%0Alosslessly%20absorb%20the%20extended%20parameters%20into%20the%20backbone%20to%20ensure%20that%20the%0Anumber%20of%20parameters%20remains%20constant.%20To%20improve%20the%20learning%20ability%20of%20the%0Amodel%2C%20we%20designed%20a%20novel%20prototype%20contrastive%20loss%20to%20reduce%20the%20overlap%0Abetween%20old%20and%20new%20classes%20in%20the%20feature%20space.%20Besides%2C%20to%20address%20the%0Aclassifier%20bias%20towards%20the%20new%20classes%2C%20we%20propose%20a%20novel%20approach%20to%0Agenerate%20the%20pseudo-features%20to%20correct%20the%20classifier.%20We%20experiment%20with%20our%0Amethods%20on%20three%20standard%20Non-Exemplar%20Class-Incremental%20Learning~%28NECIL%29%0Abenchmarks.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20gets%20a%20significant%0Aimprovement%20compared%20with%20the%20previous%20works%20and%20achieves%205.38%25%2C%205.20%25%2C%20and%0A4.92%25%20improvement%20on%20CIFAR-100%2C%20TinyImageNet%2C%20and%20ImageNet-Subset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06670v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CEAT%3A%20Continual%20Expansion%20and%20Absorption%20Transformer%20for%20Non-Exemplar%0A%20%20Class-Incremental%20Learnin&entry.906535625=Xinyuan%20Gao%20and%20Songlin%20Dong%20and%20Yuhang%20He%20and%20Xing%20Wei%20and%20Yihong%20Gong&entry.1292438233=%20%20In%20real-world%20applications%2C%20dynamic%20scenarios%20require%20the%20models%20to%20possess%0Athe%20capability%20to%20learn%20new%20tasks%20continuously%20without%20forgetting%20the%20old%0Aknowledge.%20Experience-Replay%20methods%20store%20a%20subset%20of%20the%20old%20images%20for%20joint%0Atraining.%20In%20the%20scenario%20of%20more%20strict%20privacy%20protection%2C%20storing%20the%20old%0Aimages%20becomes%20infeasible%2C%20which%20leads%20to%20a%20more%20severe%20plasticity-stability%0Adilemma%20and%20classifier%20bias.%20To%20meet%20the%20above%20challenges%2C%20we%20propose%20a%20new%0Aarchitecture%2C%20named%20continual%20expansion%20and%20absorption%20transformer~%28CEAT%29.%20The%0Amodel%20can%20learn%20the%20novel%20knowledge%20by%20extending%20the%20expanded-fusion%20layers%20in%0Aparallel%20with%20the%20frozen%20previous%20parameters.%20After%20the%20task%20ends%2C%20we%0Alosslessly%20absorb%20the%20extended%20parameters%20into%20the%20backbone%20to%20ensure%20that%20the%0Anumber%20of%20parameters%20remains%20constant.%20To%20improve%20the%20learning%20ability%20of%20the%0Amodel%2C%20we%20designed%20a%20novel%20prototype%20contrastive%20loss%20to%20reduce%20the%20overlap%0Abetween%20old%20and%20new%20classes%20in%20the%20feature%20space.%20Besides%2C%20to%20address%20the%0Aclassifier%20bias%20towards%20the%20new%20classes%2C%20we%20propose%20a%20novel%20approach%20to%0Agenerate%20the%20pseudo-features%20to%20correct%20the%20classifier.%20We%20experiment%20with%20our%0Amethods%20on%20three%20standard%20Non-Exemplar%20Class-Incremental%20Learning~%28NECIL%29%0Abenchmarks.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20gets%20a%20significant%0Aimprovement%20compared%20with%20the%20previous%20works%20and%20achieves%205.38%25%2C%205.20%25%2C%20and%0A4.92%25%20improvement%20on%20CIFAR-100%2C%20TinyImageNet%2C%20and%20ImageNet-Subset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06670v1&entry.124074799=Read"},
{"title": "Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration\n  Framework", "author": "Henrique Jesus and Hugo Proen\u00e7a", "abstract": "  Large vision models based in deep learning architectures have been\nconsistently advancing the state-of-the-art in biometric recognition. However,\nthree weaknesses are commonly reported for such kind of approaches: 1) their\nextreme demands in terms of learning data; 2) the difficulties in generalising\nbetween different domains; and 3) the lack of interpretability/explainability,\nwith biometrics being of particular interest, as it is important to provide\nevidence able to be used for forensics/legal purposes (e.g., in courts). To the\nbest of our knowledge, this paper describes the first recognition\nframework/strategy that aims at addressing the three weaknesses simultaneously.\nAt first, it relies exclusively in synthetic samples for learning purposes.\nInstead of requiring a large amount and variety of samples for each subject,\nthe idea is to exclusively enroll a 3D point cloud per identity. Then, using\ngenerative strategies, we synthesize a very large (potentially infinite) number\nof samples, containing all the desired covariates (poses, clothing, distances,\nperspectives, lighting, occlusions,...). Upon the synthesizing method used, it\nis possible to adapt precisely to different kind of domains, which accounts for\ngeneralization purposes. Such data are then used to learn a model that performs\nlocal registration between image pairs, establishing positive correspondences\nbetween body parts that are the key, not only to recognition (according to\ncardinality and distribution), but also to provide an interpretable description\nof the response (e.g.: \"both samples are from the same person, as they have\nsimilar facial shape, hair color and legs thickness\").\n", "link": "http://arxiv.org/abs/2403.06658v1", "date": "2024-03-11", "relevancy": 2.8448, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5537}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework&body=Title%3A%20Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework%0AAuthor%3A%20Henrique%20Jesus%20and%20Hugo%20Proen%C3%A7a%0AAbstract%3A%20%20%20Large%20vision%20models%20based%20in%20deep%20learning%20architectures%20have%20been%0Aconsistently%20advancing%20the%20state-of-the-art%20in%20biometric%20recognition.%20However%2C%0Athree%20weaknesses%20are%20commonly%20reported%20for%20such%20kind%20of%20approaches%3A%201%29%20their%0Aextreme%20demands%20in%20terms%20of%20learning%20data%3B%202%29%20the%20difficulties%20in%20generalising%0Abetween%20different%20domains%3B%20and%203%29%20the%20lack%20of%20interpretability/explainability%2C%0Awith%20biometrics%20being%20of%20particular%20interest%2C%20as%20it%20is%20important%20to%20provide%0Aevidence%20able%20to%20be%20used%20for%20forensics/legal%20purposes%20%28e.g.%2C%20in%20courts%29.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20paper%20describes%20the%20first%20recognition%0Aframework/strategy%20that%20aims%20at%20addressing%20the%20three%20weaknesses%20simultaneously.%0AAt%20first%2C%20it%20relies%20exclusively%20in%20synthetic%20samples%20for%20learning%20purposes.%0AInstead%20of%20requiring%20a%20large%20amount%20and%20variety%20of%20samples%20for%20each%20subject%2C%0Athe%20idea%20is%20to%20exclusively%20enroll%20a%203D%20point%20cloud%20per%20identity.%20Then%2C%20using%0Agenerative%20strategies%2C%20we%20synthesize%20a%20very%20large%20%28potentially%20infinite%29%20number%0Aof%20samples%2C%20containing%20all%20the%20desired%20covariates%20%28poses%2C%20clothing%2C%20distances%2C%0Aperspectives%2C%20lighting%2C%20occlusions%2C...%29.%20Upon%20the%20synthesizing%20method%20used%2C%20it%0Ais%20possible%20to%20adapt%20precisely%20to%20different%20kind%20of%20domains%2C%20which%20accounts%20for%0Ageneralization%20purposes.%20Such%20data%20are%20then%20used%20to%20learn%20a%20model%20that%20performs%0Alocal%20registration%20between%20image%20pairs%2C%20establishing%20positive%20correspondences%0Abetween%20body%20parts%20that%20are%20the%20key%2C%20not%20only%20to%20recognition%20%28according%20to%0Acardinality%20and%20distribution%29%2C%20but%20also%20to%20provide%20an%20interpretable%20description%0Aof%20the%20response%20%28e.g.%3A%20%22both%20samples%20are%20from%20the%20same%20person%2C%20as%20they%20have%0Asimilar%20facial%20shape%2C%20hair%20color%20and%20legs%20thickness%22%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06658v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Zero-Shot%20Interpretable%20Human%20Recognition%3A%20A%202D-3D%20Registration%0A%20%20Framework&entry.906535625=Henrique%20Jesus%20and%20Hugo%20Proen%C3%A7a&entry.1292438233=%20%20Large%20vision%20models%20based%20in%20deep%20learning%20architectures%20have%20been%0Aconsistently%20advancing%20the%20state-of-the-art%20in%20biometric%20recognition.%20However%2C%0Athree%20weaknesses%20are%20commonly%20reported%20for%20such%20kind%20of%20approaches%3A%201%29%20their%0Aextreme%20demands%20in%20terms%20of%20learning%20data%3B%202%29%20the%20difficulties%20in%20generalising%0Abetween%20different%20domains%3B%20and%203%29%20the%20lack%20of%20interpretability/explainability%2C%0Awith%20biometrics%20being%20of%20particular%20interest%2C%20as%20it%20is%20important%20to%20provide%0Aevidence%20able%20to%20be%20used%20for%20forensics/legal%20purposes%20%28e.g.%2C%20in%20courts%29.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20paper%20describes%20the%20first%20recognition%0Aframework/strategy%20that%20aims%20at%20addressing%20the%20three%20weaknesses%20simultaneously.%0AAt%20first%2C%20it%20relies%20exclusively%20in%20synthetic%20samples%20for%20learning%20purposes.%0AInstead%20of%20requiring%20a%20large%20amount%20and%20variety%20of%20samples%20for%20each%20subject%2C%0Athe%20idea%20is%20to%20exclusively%20enroll%20a%203D%20point%20cloud%20per%20identity.%20Then%2C%20using%0Agenerative%20strategies%2C%20we%20synthesize%20a%20very%20large%20%28potentially%20infinite%29%20number%0Aof%20samples%2C%20containing%20all%20the%20desired%20covariates%20%28poses%2C%20clothing%2C%20distances%2C%0Aperspectives%2C%20lighting%2C%20occlusions%2C...%29.%20Upon%20the%20synthesizing%20method%20used%2C%20it%0Ais%20possible%20to%20adapt%20precisely%20to%20different%20kind%20of%20domains%2C%20which%20accounts%20for%0Ageneralization%20purposes.%20Such%20data%20are%20then%20used%20to%20learn%20a%20model%20that%20performs%0Alocal%20registration%20between%20image%20pairs%2C%20establishing%20positive%20correspondences%0Abetween%20body%20parts%20that%20are%20the%20key%2C%20not%20only%20to%20recognition%20%28according%20to%0Acardinality%20and%20distribution%29%2C%20but%20also%20to%20provide%20an%20interpretable%20description%0Aof%20the%20response%20%28e.g.%3A%20%22both%20samples%20are%20from%20the%20same%20person%2C%20as%20they%20have%0Asimilar%20facial%20shape%2C%20hair%20color%20and%20legs%20thickness%22%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06658v1&entry.124074799=Read"},
{"title": "CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object\n  Localization Perspective", "author": "Shunsuke Yasuki and Masato Taki", "abstract": "  Recently, convolutional neural networks (CNNs) with large size kernels have\nattracted much attention in the computer vision field, following the success of\nthe Vision Transformers. Large kernel CNNs have been reported to perform well\nin downstream vision tasks as well as in classification performance. The reason\nfor the high-performance of large kernel CNNs in downstream tasks has been\nattributed to the large effective receptive field (ERF) produced by large size\nkernels, but this view has not been fully tested. We therefore revisit the\nperformance of large kernel CNNs in downstream task, focusing on the weakly\nsupervised object localization (WSOL) task. WSOL, a difficult downstream task\nthat is not fully supervised, provides a new angle to explore the capabilities\nof the large kernel CNNs. Our study compares the modern large kernel CNNs\nConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that\nERF size is important for improving downstream task performance. Our analysis\nof the factors contributing to high performance provides a different\nperspective, in which the main factor is feature map improvement. Furthermore,\nwe find that modern CNNs are robust to the CAM problems of local regions of\nobjects being activated, which has long been discussed in WSOL. CAM is the most\nclassic WSOL method, but because of the above-mentioned problems, it is often\nused as a baseline method for comparison. However, experiments on the\nCUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and\nsimple data augmentation methods can achieve performance (90.99% MaxBoxAcc)\ncomparable to the latest WSOL method, which is CNN-based and requires special\ntraining or complex post-processing. The code is available at\nhttps://github.com/snskysk/CAM-Back-Again.\n", "link": "http://arxiv.org/abs/2403.06676v1", "date": "2024-03-11", "relevancy": 2.8301, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6054}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5711}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20CAM%20Back%20Again%3A%20Large%20Kernel%20CNNs%20from%20a%20Weakly%20Supervised%20Object%0A%20%20Localization%20Perspective&body=Title%3A%20CAM%20Back%20Again%3A%20Large%20Kernel%20CNNs%20from%20a%20Weakly%20Supervised%20Object%0A%20%20Localization%20Perspective%0AAuthor%3A%20Shunsuke%20Yasuki%20and%20Masato%20Taki%0AAbstract%3A%20%20%20Recently%2C%20convolutional%20neural%20networks%20%28CNNs%29%20with%20large%20size%20kernels%20have%0Aattracted%20much%20attention%20in%20the%20computer%20vision%20field%2C%20following%20the%20success%20of%0Athe%20Vision%20Transformers.%20Large%20kernel%20CNNs%20have%20been%20reported%20to%20perform%20well%0Ain%20downstream%20vision%20tasks%20as%20well%20as%20in%20classification%20performance.%20The%20reason%0Afor%20the%20high-performance%20of%20large%20kernel%20CNNs%20in%20downstream%20tasks%20has%20been%0Aattributed%20to%20the%20large%20effective%20receptive%20field%20%28ERF%29%20produced%20by%20large%20size%0Akernels%2C%20but%20this%20view%20has%20not%20been%20fully%20tested.%20We%20therefore%20revisit%20the%0Aperformance%20of%20large%20kernel%20CNNs%20in%20downstream%20task%2C%20focusing%20on%20the%20weakly%0Asupervised%20object%20localization%20%28WSOL%29%20task.%20WSOL%2C%20a%20difficult%20downstream%20task%0Athat%20is%20not%20fully%20supervised%2C%20provides%20a%20new%20angle%20to%20explore%20the%20capabilities%0Aof%20the%20large%20kernel%20CNNs.%20Our%20study%20compares%20the%20modern%20large%20kernel%20CNNs%0AConvNeXt%2C%20RepLKNet%2C%20and%20SLaK%20to%20test%20the%20validity%20of%20the%20naive%20expectation%20that%0AERF%20size%20is%20important%20for%20improving%20downstream%20task%20performance.%20Our%20analysis%0Aof%20the%20factors%20contributing%20to%20high%20performance%20provides%20a%20different%0Aperspective%2C%20in%20which%20the%20main%20factor%20is%20feature%20map%20improvement.%20Furthermore%2C%0Awe%20find%20that%20modern%20CNNs%20are%20robust%20to%20the%20CAM%20problems%20of%20local%20regions%20of%0Aobjects%20being%20activated%2C%20which%20has%20long%20been%20discussed%20in%20WSOL.%20CAM%20is%20the%20most%0Aclassic%20WSOL%20method%2C%20but%20because%20of%20the%20above-mentioned%20problems%2C%20it%20is%20often%0Aused%20as%20a%20baseline%20method%20for%20comparison.%20However%2C%20experiments%20on%20the%0ACUB-200-2011%20dataset%20show%20that%20simply%20combining%20a%20large%20kernel%20CNN%2C%20CAM%2C%20and%0Asimple%20data%20augmentation%20methods%20can%20achieve%20performance%20%2890.99%25%20MaxBoxAcc%29%0Acomparable%20to%20the%20latest%20WSOL%20method%2C%20which%20is%20CNN-based%20and%20requires%20special%0Atraining%20or%20complex%20post-processing.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/snskysk/CAM-Back-Again.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06676v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAM%20Back%20Again%3A%20Large%20Kernel%20CNNs%20from%20a%20Weakly%20Supervised%20Object%0A%20%20Localization%20Perspective&entry.906535625=Shunsuke%20Yasuki%20and%20Masato%20Taki&entry.1292438233=%20%20Recently%2C%20convolutional%20neural%20networks%20%28CNNs%29%20with%20large%20size%20kernels%20have%0Aattracted%20much%20attention%20in%20the%20computer%20vision%20field%2C%20following%20the%20success%20of%0Athe%20Vision%20Transformers.%20Large%20kernel%20CNNs%20have%20been%20reported%20to%20perform%20well%0Ain%20downstream%20vision%20tasks%20as%20well%20as%20in%20classification%20performance.%20The%20reason%0Afor%20the%20high-performance%20of%20large%20kernel%20CNNs%20in%20downstream%20tasks%20has%20been%0Aattributed%20to%20the%20large%20effective%20receptive%20field%20%28ERF%29%20produced%20by%20large%20size%0Akernels%2C%20but%20this%20view%20has%20not%20been%20fully%20tested.%20We%20therefore%20revisit%20the%0Aperformance%20of%20large%20kernel%20CNNs%20in%20downstream%20task%2C%20focusing%20on%20the%20weakly%0Asupervised%20object%20localization%20%28WSOL%29%20task.%20WSOL%2C%20a%20difficult%20downstream%20task%0Athat%20is%20not%20fully%20supervised%2C%20provides%20a%20new%20angle%20to%20explore%20the%20capabilities%0Aof%20the%20large%20kernel%20CNNs.%20Our%20study%20compares%20the%20modern%20large%20kernel%20CNNs%0AConvNeXt%2C%20RepLKNet%2C%20and%20SLaK%20to%20test%20the%20validity%20of%20the%20naive%20expectation%20that%0AERF%20size%20is%20important%20for%20improving%20downstream%20task%20performance.%20Our%20analysis%0Aof%20the%20factors%20contributing%20to%20high%20performance%20provides%20a%20different%0Aperspective%2C%20in%20which%20the%20main%20factor%20is%20feature%20map%20improvement.%20Furthermore%2C%0Awe%20find%20that%20modern%20CNNs%20are%20robust%20to%20the%20CAM%20problems%20of%20local%20regions%20of%0Aobjects%20being%20activated%2C%20which%20has%20long%20been%20discussed%20in%20WSOL.%20CAM%20is%20the%20most%0Aclassic%20WSOL%20method%2C%20but%20because%20of%20the%20above-mentioned%20problems%2C%20it%20is%20often%0Aused%20as%20a%20baseline%20method%20for%20comparison.%20However%2C%20experiments%20on%20the%0ACUB-200-2011%20dataset%20show%20that%20simply%20combining%20a%20large%20kernel%20CNN%2C%20CAM%2C%20and%0Asimple%20data%20augmentation%20methods%20can%20achieve%20performance%20%2890.99%25%20MaxBoxAcc%29%0Acomparable%20to%20the%20latest%20WSOL%20method%2C%20which%20is%20CNN-based%20and%20requires%20special%0Atraining%20or%20complex%20post-processing.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/snskysk/CAM-Back-Again.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06676v1&entry.124074799=Read"},
{"title": "FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in\n  Human-Centric Tasks", "author": "Muhammad Saif Ullah Khan and Muhammad Ferjad Naeem and Federico Tombari and Luc Van Gool and Didier Stricker and Muhammad Zeshan Afzal", "abstract": "  We propose FocusCLIP, integrating subject-level guidance--a specialized\nmechanism for target-specific supervision--into the CLIP framework for improved\nzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP\non both the vision and text sides. On the vision side, we incorporate ROI\nheatmaps emulating human visual attention mechanisms to emphasize\nsubject-relevant image regions. On the text side, we introduce human pose\ndescriptions to provide rich contextual information. For human-centric tasks,\nFocusCLIP is trained with images from the MPII Human Pose dataset. The proposed\napproach surpassed CLIP by an average of 8.61% across five previously unseen\ndatasets covering three human-centric tasks. FocusCLIP achieved an average\naccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement\nin activity recognition, a 14.78% improvement in age classification, and a\n7.06% improvement in emotion recognition. Moreover, using our proposed\nsingle-shot LLM prompting strategy, we release a high-quality MPII Pose\nDescriptions dataset to encourage further research in multimodal learning for\nhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of our\nsubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%\nimprovement over CLIP in zero-shot bird classification using the CUB dataset.\nOur findings emphasize the potential of integrating subject-level guidance with\ngeneral pretraining methods for enhanced downstream performance.\n", "link": "http://arxiv.org/abs/2403.06904v1", "date": "2024-03-11", "relevancy": 2.8224, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6113}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.543}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20FocusCLIP%3A%20Multimodal%20Subject-Level%20Guidance%20for%20Zero-Shot%20Transfer%20in%0A%20%20Human-Centric%20Tasks&body=Title%3A%20FocusCLIP%3A%20Multimodal%20Subject-Level%20Guidance%20for%20Zero-Shot%20Transfer%20in%0A%20%20Human-Centric%20Tasks%0AAuthor%3A%20Muhammad%20Saif%20Ullah%20Khan%20and%20Muhammad%20Ferjad%20Naeem%20and%20Federico%20Tombari%20and%20Luc%20Van%20Gool%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal%0AAbstract%3A%20%20%20We%20propose%20FocusCLIP%2C%20integrating%20subject-level%20guidance--a%20specialized%0Amechanism%20for%20target-specific%20supervision--into%20the%20CLIP%20framework%20for%20improved%0Azero-shot%20transfer%20on%20human-centric%20tasks.%20Our%20novel%20contributions%20enhance%20CLIP%0Aon%20both%20the%20vision%20and%20text%20sides.%20On%20the%20vision%20side%2C%20we%20incorporate%20ROI%0Aheatmaps%20emulating%20human%20visual%20attention%20mechanisms%20to%20emphasize%0Asubject-relevant%20image%20regions.%20On%20the%20text%20side%2C%20we%20introduce%20human%20pose%0Adescriptions%20to%20provide%20rich%20contextual%20information.%20For%20human-centric%20tasks%2C%0AFocusCLIP%20is%20trained%20with%20images%20from%20the%20MPII%20Human%20Pose%20dataset.%20The%20proposed%0Aapproach%20surpassed%20CLIP%20by%20an%20average%20of%208.61%25%20across%20five%20previously%20unseen%0Adatasets%20covering%20three%20human-centric%20tasks.%20FocusCLIP%20achieved%20an%20average%0Aaccuracy%20of%2033.65%25%20compared%20to%2025.04%25%20by%20CLIP.%20We%20observed%20a%203.98%25%20improvement%0Ain%20activity%20recognition%2C%20a%2014.78%25%20improvement%20in%20age%20classification%2C%20and%20a%0A7.06%25%20improvement%20in%20emotion%20recognition.%20Moreover%2C%20using%20our%20proposed%0Asingle-shot%20LLM%20prompting%20strategy%2C%20we%20release%20a%20high-quality%20MPII%20Pose%0ADescriptions%20dataset%20to%20encourage%20further%20research%20in%20multimodal%20learning%20for%0Ahuman-centric%20tasks.%20Furthermore%2C%20we%20also%20demonstrate%20the%20effectiveness%20of%20our%0Asubject-level%20supervision%20on%20non-human-centric%20tasks.%20FocusCLIP%20shows%20a%202.47%25%0Aimprovement%20over%20CLIP%20in%20zero-shot%20bird%20classification%20using%20the%20CUB%20dataset.%0AOur%20findings%20emphasize%20the%20potential%20of%20integrating%20subject-level%20guidance%20with%0Ageneral%20pretraining%20methods%20for%20enhanced%20downstream%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06904v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusCLIP%3A%20Multimodal%20Subject-Level%20Guidance%20for%20Zero-Shot%20Transfer%20in%0A%20%20Human-Centric%20Tasks&entry.906535625=Muhammad%20Saif%20Ullah%20Khan%20and%20Muhammad%20Ferjad%20Naeem%20and%20Federico%20Tombari%20and%20Luc%20Van%20Gool%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal&entry.1292438233=%20%20We%20propose%20FocusCLIP%2C%20integrating%20subject-level%20guidance--a%20specialized%0Amechanism%20for%20target-specific%20supervision--into%20the%20CLIP%20framework%20for%20improved%0Azero-shot%20transfer%20on%20human-centric%20tasks.%20Our%20novel%20contributions%20enhance%20CLIP%0Aon%20both%20the%20vision%20and%20text%20sides.%20On%20the%20vision%20side%2C%20we%20incorporate%20ROI%0Aheatmaps%20emulating%20human%20visual%20attention%20mechanisms%20to%20emphasize%0Asubject-relevant%20image%20regions.%20On%20the%20text%20side%2C%20we%20introduce%20human%20pose%0Adescriptions%20to%20provide%20rich%20contextual%20information.%20For%20human-centric%20tasks%2C%0AFocusCLIP%20is%20trained%20with%20images%20from%20the%20MPII%20Human%20Pose%20dataset.%20The%20proposed%0Aapproach%20surpassed%20CLIP%20by%20an%20average%20of%208.61%25%20across%20five%20previously%20unseen%0Adatasets%20covering%20three%20human-centric%20tasks.%20FocusCLIP%20achieved%20an%20average%0Aaccuracy%20of%2033.65%25%20compared%20to%2025.04%25%20by%20CLIP.%20We%20observed%20a%203.98%25%20improvement%0Ain%20activity%20recognition%2C%20a%2014.78%25%20improvement%20in%20age%20classification%2C%20and%20a%0A7.06%25%20improvement%20in%20emotion%20recognition.%20Moreover%2C%20using%20our%20proposed%0Asingle-shot%20LLM%20prompting%20strategy%2C%20we%20release%20a%20high-quality%20MPII%20Pose%0ADescriptions%20dataset%20to%20encourage%20further%20research%20in%20multimodal%20learning%20for%0Ahuman-centric%20tasks.%20Furthermore%2C%20we%20also%20demonstrate%20the%20effectiveness%20of%20our%0Asubject-level%20supervision%20on%20non-human-centric%20tasks.%20FocusCLIP%20shows%20a%202.47%25%0Aimprovement%20over%20CLIP%20in%20zero-shot%20bird%20classification%20using%20the%20CUB%20dataset.%0AOur%20findings%20emphasize%20the%20potential%20of%20integrating%20subject-level%20guidance%20with%0Ageneral%20pretraining%20methods%20for%20enhanced%20downstream%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06904v1&entry.124074799=Read"},
{"title": "DiaLoc: An Iterative Approach to Embodied Dialog Localization", "author": "Chao Zhang and Mohan Li and Ignas Budvytis and Stephan Liwicki", "abstract": "  Multimodal learning has advanced the performance for many vision-language\ntasks. However, most existing works in embodied dialog research focus on\nnavigation and leave the localization task understudied. The few existing\ndialog-based localization approaches assume the availability of entire dialog\nprior to localizaiton, which is impractical for deployed dialog-based\nlocalization. In this paper, we propose DiaLoc, a new dialog-based localization\nframework which aligns with a real human operator behavior. Specifically, we\nproduce an iterative refinement of location predictions which can visualize\ncurrent pose believes after each dialog turn. DiaLoc effectively utilizes the\nmultimodal data for multi-shot localization, where a fusion encoder fuses\nvision and dialog information iteratively. We achieve state-of-the-art results\non embodied dialog-based localization task, in single-shot (+7.08% in\nAcc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc\nnarrows the gap between simulation and real-world applications, opening doors\nfor future research on collaborative localization and navigation.\n", "link": "http://arxiv.org/abs/2403.06846v1", "date": "2024-03-11", "relevancy": 2.7952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6125}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5394}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DiaLoc%3A%20An%20Iterative%20Approach%20to%20Embodied%20Dialog%20Localization&body=Title%3A%20DiaLoc%3A%20An%20Iterative%20Approach%20to%20Embodied%20Dialog%20Localization%0AAuthor%3A%20Chao%20Zhang%20and%20Mohan%20Li%20and%20Ignas%20Budvytis%20and%20Stephan%20Liwicki%0AAbstract%3A%20%20%20Multimodal%20learning%20has%20advanced%20the%20performance%20for%20many%20vision-language%0Atasks.%20However%2C%20most%20existing%20works%20in%20embodied%20dialog%20research%20focus%20on%0Anavigation%20and%20leave%20the%20localization%20task%20understudied.%20The%20few%20existing%0Adialog-based%20localization%20approaches%20assume%20the%20availability%20of%20entire%20dialog%0Aprior%20to%20localizaiton%2C%20which%20is%20impractical%20for%20deployed%20dialog-based%0Alocalization.%20In%20this%20paper%2C%20we%20propose%20DiaLoc%2C%20a%20new%20dialog-based%20localization%0Aframework%20which%20aligns%20with%20a%20real%20human%20operator%20behavior.%20Specifically%2C%20we%0Aproduce%20an%20iterative%20refinement%20of%20location%20predictions%20which%20can%20visualize%0Acurrent%20pose%20believes%20after%20each%20dialog%20turn.%20DiaLoc%20effectively%20utilizes%20the%0Amultimodal%20data%20for%20multi-shot%20localization%2C%20where%20a%20fusion%20encoder%20fuses%0Avision%20and%20dialog%20information%20iteratively.%20We%20achieve%20state-of-the-art%20results%0Aon%20embodied%20dialog-based%20localization%20task%2C%20in%20single-shot%20%28%2B7.08%25%20in%0AAcc5%40valUnseen%29%20and%20multi-shot%20settings%20%28%2B10.85%25%20in%20Acc5%40valUnseen%29.%20DiaLoc%0Anarrows%20the%20gap%20between%20simulation%20and%20real-world%20applications%2C%20opening%20doors%0Afor%20future%20research%20on%20collaborative%20localization%20and%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06846v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiaLoc%3A%20An%20Iterative%20Approach%20to%20Embodied%20Dialog%20Localization&entry.906535625=Chao%20Zhang%20and%20Mohan%20Li%20and%20Ignas%20Budvytis%20and%20Stephan%20Liwicki&entry.1292438233=%20%20Multimodal%20learning%20has%20advanced%20the%20performance%20for%20many%20vision-language%0Atasks.%20However%2C%20most%20existing%20works%20in%20embodied%20dialog%20research%20focus%20on%0Anavigation%20and%20leave%20the%20localization%20task%20understudied.%20The%20few%20existing%0Adialog-based%20localization%20approaches%20assume%20the%20availability%20of%20entire%20dialog%0Aprior%20to%20localizaiton%2C%20which%20is%20impractical%20for%20deployed%20dialog-based%0Alocalization.%20In%20this%20paper%2C%20we%20propose%20DiaLoc%2C%20a%20new%20dialog-based%20localization%0Aframework%20which%20aligns%20with%20a%20real%20human%20operator%20behavior.%20Specifically%2C%20we%0Aproduce%20an%20iterative%20refinement%20of%20location%20predictions%20which%20can%20visualize%0Acurrent%20pose%20believes%20after%20each%20dialog%20turn.%20DiaLoc%20effectively%20utilizes%20the%0Amultimodal%20data%20for%20multi-shot%20localization%2C%20where%20a%20fusion%20encoder%20fuses%0Avision%20and%20dialog%20information%20iteratively.%20We%20achieve%20state-of-the-art%20results%0Aon%20embodied%20dialog-based%20localization%20task%2C%20in%20single-shot%20%28%2B7.08%25%20in%0AAcc5%40valUnseen%29%20and%20multi-shot%20settings%20%28%2B10.85%25%20in%20Acc5%40valUnseen%29.%20DiaLoc%0Anarrows%20the%20gap%20between%20simulation%20and%20real-world%20applications%2C%20opening%20doors%0Afor%20future%20research%20on%20collaborative%20localization%20and%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06846v1&entry.124074799=Read"},
{"title": "PRIOR: Prototype Representation Joint Learning from Medical Images and\n  Reports", "author": "Pujin Cheng and Li Lin and Junyan Lyu and Yijin Huang and Wenhan Luo and Xiaoying Tang", "abstract": "  Contrastive learning based vision-language joint pre-training has emerged as\na successful representation learning strategy. In this paper, we present a\nprototype representation learning framework incorporating both global and local\nalignment between medical images and reports. In contrast to standard global\nmulti-modality alignment methods, we employ a local alignment module for\nfine-grained representation. Furthermore, a cross-modality conditional\nreconstruction module is designed to interchange information across modalities\nin the training phase by reconstructing masked images and reports. For\nreconstructing long reports, a sentence-wise prototype memory bank is\nconstructed, enabling the network to focus on low-level localized visual and\nhigh-level clinical linguistic features. Additionally, a non-auto-regressive\ngeneration paradigm is proposed for reconstructing non-sequential reports.\nExperimental results on five downstream tasks, including supervised\nclassification, zero-shot classification, image-to-text retrieval, semantic\nsegmentation, and object detection, show the proposed method outperforms other\nstate-of-the-art methods across multiple datasets and under different dataset\nsize settings. The code is available at https://github.com/QtacierP/PRIOR.\n", "link": "http://arxiv.org/abs/2307.12577v3", "date": "2024-03-11", "relevancy": 2.7895, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5689}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20PRIOR%3A%20Prototype%20Representation%20Joint%20Learning%20from%20Medical%20Images%20and%0A%20%20Reports&body=Title%3A%20PRIOR%3A%20Prototype%20Representation%20Joint%20Learning%20from%20Medical%20Images%20and%0A%20%20Reports%0AAuthor%3A%20Pujin%20Cheng%20and%20Li%20Lin%20and%20Junyan%20Lyu%20and%20Yijin%20Huang%20and%20Wenhan%20Luo%20and%20Xiaoying%20Tang%0AAbstract%3A%20%20%20Contrastive%20learning%20based%20vision-language%20joint%20pre-training%20has%20emerged%20as%0Aa%20successful%20representation%20learning%20strategy.%20In%20this%20paper%2C%20we%20present%20a%0Aprototype%20representation%20learning%20framework%20incorporating%20both%20global%20and%20local%0Aalignment%20between%20medical%20images%20and%20reports.%20In%20contrast%20to%20standard%20global%0Amulti-modality%20alignment%20methods%2C%20we%20employ%20a%20local%20alignment%20module%20for%0Afine-grained%20representation.%20Furthermore%2C%20a%20cross-modality%20conditional%0Areconstruction%20module%20is%20designed%20to%20interchange%20information%20across%20modalities%0Ain%20the%20training%20phase%20by%20reconstructing%20masked%20images%20and%20reports.%20For%0Areconstructing%20long%20reports%2C%20a%20sentence-wise%20prototype%20memory%20bank%20is%0Aconstructed%2C%20enabling%20the%20network%20to%20focus%20on%20low-level%20localized%20visual%20and%0Ahigh-level%20clinical%20linguistic%20features.%20Additionally%2C%20a%20non-auto-regressive%0Ageneration%20paradigm%20is%20proposed%20for%20reconstructing%20non-sequential%20reports.%0AExperimental%20results%20on%20five%20downstream%20tasks%2C%20including%20supervised%0Aclassification%2C%20zero-shot%20classification%2C%20image-to-text%20retrieval%2C%20semantic%0Asegmentation%2C%20and%20object%20detection%2C%20show%20the%20proposed%20method%20outperforms%20other%0Astate-of-the-art%20methods%20across%20multiple%20datasets%20and%20under%20different%20dataset%0Asize%20settings.%20The%20code%20is%20available%20at%20https%3A//github.com/QtacierP/PRIOR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12577v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIOR%3A%20Prototype%20Representation%20Joint%20Learning%20from%20Medical%20Images%20and%0A%20%20Reports&entry.906535625=Pujin%20Cheng%20and%20Li%20Lin%20and%20Junyan%20Lyu%20and%20Yijin%20Huang%20and%20Wenhan%20Luo%20and%20Xiaoying%20Tang&entry.1292438233=%20%20Contrastive%20learning%20based%20vision-language%20joint%20pre-training%20has%20emerged%20as%0Aa%20successful%20representation%20learning%20strategy.%20In%20this%20paper%2C%20we%20present%20a%0Aprototype%20representation%20learning%20framework%20incorporating%20both%20global%20and%20local%0Aalignment%20between%20medical%20images%20and%20reports.%20In%20contrast%20to%20standard%20global%0Amulti-modality%20alignment%20methods%2C%20we%20employ%20a%20local%20alignment%20module%20for%0Afine-grained%20representation.%20Furthermore%2C%20a%20cross-modality%20conditional%0Areconstruction%20module%20is%20designed%20to%20interchange%20information%20across%20modalities%0Ain%20the%20training%20phase%20by%20reconstructing%20masked%20images%20and%20reports.%20For%0Areconstructing%20long%20reports%2C%20a%20sentence-wise%20prototype%20memory%20bank%20is%0Aconstructed%2C%20enabling%20the%20network%20to%20focus%20on%20low-level%20localized%20visual%20and%0Ahigh-level%20clinical%20linguistic%20features.%20Additionally%2C%20a%20non-auto-regressive%0Ageneration%20paradigm%20is%20proposed%20for%20reconstructing%20non-sequential%20reports.%0AExperimental%20results%20on%20five%20downstream%20tasks%2C%20including%20supervised%0Aclassification%2C%20zero-shot%20classification%2C%20image-to-text%20retrieval%2C%20semantic%0Asegmentation%2C%20and%20object%20detection%2C%20show%20the%20proposed%20method%20outperforms%20other%0Astate-of-the-art%20methods%20across%20multiple%20datasets%20and%20under%20different%20dataset%0Asize%20settings.%20The%20code%20is%20available%20at%20https%3A//github.com/QtacierP/PRIOR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12577v3&entry.124074799=Read"},
{"title": "Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment", "author": "Hanxiao Chen", "abstract": "  Most computer vision applications aim to identify pixels in a scene and use\nthem for diverse purposes. One intriguing application is car damage detection\nfor insurance carriers which tends to detect all car damages by comparing both\npre-trip and post-trip images, even requiring two components: (i) car damage\ndetection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to\ndetect car damages on custom images. Whereas for the image alignment section,\nwe especially propose a novel self-supervised Patch-to-Patch SimCLR inspired\nalignment approach to find perspective transformations between custom pre/post\ncar rental images except for traditional computer vision methods.\n", "link": "http://arxiv.org/abs/2403.06674v1", "date": "2024-03-11", "relevancy": 2.7374, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5648}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5616}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Car%20Damage%20Detection%20and%20Patch-to-Patch%20Self-supervised%20Image%20Alignment&body=Title%3A%20Car%20Damage%20Detection%20and%20Patch-to-Patch%20Self-supervised%20Image%20Alignment%0AAuthor%3A%20Hanxiao%20Chen%0AAbstract%3A%20%20%20Most%20computer%20vision%20applications%20aim%20to%20identify%20pixels%20in%20a%20scene%20and%20use%0Athem%20for%20diverse%20purposes.%20One%20intriguing%20application%20is%20car%20damage%20detection%0Afor%20insurance%20carriers%20which%20tends%20to%20detect%20all%20car%20damages%20by%20comparing%20both%0Apre-trip%20and%20post-trip%20images%2C%20even%20requiring%20two%20components%3A%20%28i%29%20car%20damage%0Adetection%3B%20%28ii%29%20image%20alignment.%20Firstly%2C%20we%20implemented%20a%20Mask%20R-CNN%20model%20to%0Adetect%20car%20damages%20on%20custom%20images.%20Whereas%20for%20the%20image%20alignment%20section%2C%0Awe%20especially%20propose%20a%20novel%20self-supervised%20Patch-to-Patch%20SimCLR%20inspired%0Aalignment%20approach%20to%20find%20perspective%20transformations%20between%20custom%20pre/post%0Acar%20rental%20images%20except%20for%20traditional%20computer%20vision%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06674v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Car%20Damage%20Detection%20and%20Patch-to-Patch%20Self-supervised%20Image%20Alignment&entry.906535625=Hanxiao%20Chen&entry.1292438233=%20%20Most%20computer%20vision%20applications%20aim%20to%20identify%20pixels%20in%20a%20scene%20and%20use%0Athem%20for%20diverse%20purposes.%20One%20intriguing%20application%20is%20car%20damage%20detection%0Afor%20insurance%20carriers%20which%20tends%20to%20detect%20all%20car%20damages%20by%20comparing%20both%0Apre-trip%20and%20post-trip%20images%2C%20even%20requiring%20two%20components%3A%20%28i%29%20car%20damage%0Adetection%3B%20%28ii%29%20image%20alignment.%20Firstly%2C%20we%20implemented%20a%20Mask%20R-CNN%20model%20to%0Adetect%20car%20damages%20on%20custom%20images.%20Whereas%20for%20the%20image%20alignment%20section%2C%0Awe%20especially%20propose%20a%20novel%20self-supervised%20Patch-to-Patch%20SimCLR%20inspired%0Aalignment%20approach%20to%20find%20perspective%20transformations%20between%20custom%20pre/post%0Acar%20rental%20images%20except%20for%20traditional%20computer%20vision%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06674v1&entry.124074799=Read"},
{"title": "BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning\n  of SAM", "author": "Li Zhang and Youwei Liang and Ruiyi Zhang and Amirhosein Javadi and Pengtao Xie", "abstract": "  The Segment Anything Model (SAM), a foundation model pretrained on millions\nof images and segmentation masks, has significantly advanced semantic\nsegmentation, a fundamental task in computer vision. Despite its strengths, SAM\nencounters two major challenges. Firstly, it struggles with segmenting specific\nobjects autonomously, as it relies on users to manually input prompts like\npoints or bounding boxes to identify targeted objects. Secondly, SAM faces\nchallenges in excelling at specific downstream tasks, like medical imaging, due\nto a disparity between the distribution of its pretraining data, which\npredominantly consists of general-domain images, and the data used in\ndownstream tasks. Current solutions to these problems, which involve finetuning\nSAM, often lead to overfitting, a notable issue in scenarios with very limited\ndata, like in medical imaging. To overcome these limitations, we introduce\nBLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach\nallows for automatic image segmentation without the need for manual prompts, by\noptimizing a learnable prompt embedding. Furthermore, it significantly reduces\nthe risk of overfitting by training the model's weight parameters and the\nprompt embedding on two separate subsets of the training dataset, each at a\ndifferent level of optimization. We apply BLO-SAM to diverse semantic\nsegmentation tasks in general and medical domains. The results demonstrate\nBLO-SAM's superior performance over various state-of-the-art image semantic\nsegmentation methods.\n", "link": "http://arxiv.org/abs/2402.16338v4", "date": "2024-03-11", "relevancy": 2.7238, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5213}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20BLO-SAM%3A%20Bi-level%20Optimization%20Based%20Overfitting-Preventing%20Finetuning%0A%20%20of%20SAM&body=Title%3A%20BLO-SAM%3A%20Bi-level%20Optimization%20Based%20Overfitting-Preventing%20Finetuning%0A%20%20of%20SAM%0AAuthor%3A%20Li%20Zhang%20and%20Youwei%20Liang%20and%20Ruiyi%20Zhang%20and%20Amirhosein%20Javadi%20and%20Pengtao%20Xie%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20foundation%20model%20pretrained%20on%20millions%0Aof%20images%20and%20segmentation%20masks%2C%20has%20significantly%20advanced%20semantic%0Asegmentation%2C%20a%20fundamental%20task%20in%20computer%20vision.%20Despite%20its%20strengths%2C%20SAM%0Aencounters%20two%20major%20challenges.%20Firstly%2C%20it%20struggles%20with%20segmenting%20specific%0Aobjects%20autonomously%2C%20as%20it%20relies%20on%20users%20to%20manually%20input%20prompts%20like%0Apoints%20or%20bounding%20boxes%20to%20identify%20targeted%20objects.%20Secondly%2C%20SAM%20faces%0Achallenges%20in%20excelling%20at%20specific%20downstream%20tasks%2C%20like%20medical%20imaging%2C%20due%0Ato%20a%20disparity%20between%20the%20distribution%20of%20its%20pretraining%20data%2C%20which%0Apredominantly%20consists%20of%20general-domain%20images%2C%20and%20the%20data%20used%20in%0Adownstream%20tasks.%20Current%20solutions%20to%20these%20problems%2C%20which%20involve%20finetuning%0ASAM%2C%20often%20lead%20to%20overfitting%2C%20a%20notable%20issue%20in%20scenarios%20with%20very%20limited%0Adata%2C%20like%20in%20medical%20imaging.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ABLO-SAM%2C%20which%20finetunes%20SAM%20based%20on%20bi-level%20optimization%20%28BLO%29.%20Our%20approach%0Aallows%20for%20automatic%20image%20segmentation%20without%20the%20need%20for%20manual%20prompts%2C%20by%0Aoptimizing%20a%20learnable%20prompt%20embedding.%20Furthermore%2C%20it%20significantly%20reduces%0Athe%20risk%20of%20overfitting%20by%20training%20the%20model%27s%20weight%20parameters%20and%20the%0Aprompt%20embedding%20on%20two%20separate%20subsets%20of%20the%20training%20dataset%2C%20each%20at%20a%0Adifferent%20level%20of%20optimization.%20We%20apply%20BLO-SAM%20to%20diverse%20semantic%0Asegmentation%20tasks%20in%20general%20and%20medical%20domains.%20The%20results%20demonstrate%0ABLO-SAM%27s%20superior%20performance%20over%20various%20state-of-the-art%20image%20semantic%0Asegmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16338v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLO-SAM%3A%20Bi-level%20Optimization%20Based%20Overfitting-Preventing%20Finetuning%0A%20%20of%20SAM&entry.906535625=Li%20Zhang%20and%20Youwei%20Liang%20and%20Ruiyi%20Zhang%20and%20Amirhosein%20Javadi%20and%20Pengtao%20Xie&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20foundation%20model%20pretrained%20on%20millions%0Aof%20images%20and%20segmentation%20masks%2C%20has%20significantly%20advanced%20semantic%0Asegmentation%2C%20a%20fundamental%20task%20in%20computer%20vision.%20Despite%20its%20strengths%2C%20SAM%0Aencounters%20two%20major%20challenges.%20Firstly%2C%20it%20struggles%20with%20segmenting%20specific%0Aobjects%20autonomously%2C%20as%20it%20relies%20on%20users%20to%20manually%20input%20prompts%20like%0Apoints%20or%20bounding%20boxes%20to%20identify%20targeted%20objects.%20Secondly%2C%20SAM%20faces%0Achallenges%20in%20excelling%20at%20specific%20downstream%20tasks%2C%20like%20medical%20imaging%2C%20due%0Ato%20a%20disparity%20between%20the%20distribution%20of%20its%20pretraining%20data%2C%20which%0Apredominantly%20consists%20of%20general-domain%20images%2C%20and%20the%20data%20used%20in%0Adownstream%20tasks.%20Current%20solutions%20to%20these%20problems%2C%20which%20involve%20finetuning%0ASAM%2C%20often%20lead%20to%20overfitting%2C%20a%20notable%20issue%20in%20scenarios%20with%20very%20limited%0Adata%2C%20like%20in%20medical%20imaging.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ABLO-SAM%2C%20which%20finetunes%20SAM%20based%20on%20bi-level%20optimization%20%28BLO%29.%20Our%20approach%0Aallows%20for%20automatic%20image%20segmentation%20without%20the%20need%20for%20manual%20prompts%2C%20by%0Aoptimizing%20a%20learnable%20prompt%20embedding.%20Furthermore%2C%20it%20significantly%20reduces%0Athe%20risk%20of%20overfitting%20by%20training%20the%20model%27s%20weight%20parameters%20and%20the%0Aprompt%20embedding%20on%20two%20separate%20subsets%20of%20the%20training%20dataset%2C%20each%20at%20a%0Adifferent%20level%20of%20optimization.%20We%20apply%20BLO-SAM%20to%20diverse%20semantic%0Asegmentation%20tasks%20in%20general%20and%20medical%20domains.%20The%20results%20demonstrate%0ABLO-SAM%27s%20superior%20performance%20over%20various%20state-of-the-art%20image%20semantic%0Asegmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16338v4&entry.124074799=Read"},
{"title": "Density-Guided Label Smoothing for Temporal Localization of Driving\n  Actions", "author": "Tunc Alkanat and Erkut Akdag and Egor Bondarev and Peter H. N. De With", "abstract": "  Temporal localization of driving actions plays a crucial role in advanced\ndriver-assistance systems and naturalistic driving studies. However, this is a\nchallenging task due to strict requirements for robustness, reliability and\naccurate localization. In this work, we focus on improving the overall\nperformance by efficiently utilizing video action recognition networks and\nadapting these to the problem of action localization. To this end, we first\ndevelop a density-guided label smoothing technique based on label probability\ndistributions to facilitate better learning from boundary video-segments that\ntypically include multiple labels. Second, we design a post-processing step to\nefficiently fuse information from video-segments and multiple camera views into\nscene-level predictions, which facilitates elimination of false positives. Our\nmethodology yields a competitive performance on the A2 test set of the\nnaturalistic driving action recognition track of the 2022 NVIDIA AI City\nChallenge with an F1 score of 0.271.\n", "link": "http://arxiv.org/abs/2403.06616v1", "date": "2024-03-11", "relevancy": 2.7182, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5471}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5293}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Density-Guided%20Label%20Smoothing%20for%20Temporal%20Localization%20of%20Driving%0A%20%20Actions&body=Title%3A%20Density-Guided%20Label%20Smoothing%20for%20Temporal%20Localization%20of%20Driving%0A%20%20Actions%0AAuthor%3A%20Tunc%20Alkanat%20and%20Erkut%20Akdag%20and%20Egor%20Bondarev%20and%20Peter%20H.%20N.%20De%20With%0AAbstract%3A%20%20%20Temporal%20localization%20of%20driving%20actions%20plays%20a%20crucial%20role%20in%20advanced%0Adriver-assistance%20systems%20and%20naturalistic%20driving%20studies.%20However%2C%20this%20is%20a%0Achallenging%20task%20due%20to%20strict%20requirements%20for%20robustness%2C%20reliability%20and%0Aaccurate%20localization.%20In%20this%20work%2C%20we%20focus%20on%20improving%20the%20overall%0Aperformance%20by%20efficiently%20utilizing%20video%20action%20recognition%20networks%20and%0Aadapting%20these%20to%20the%20problem%20of%20action%20localization.%20To%20this%20end%2C%20we%20first%0Adevelop%20a%20density-guided%20label%20smoothing%20technique%20based%20on%20label%20probability%0Adistributions%20to%20facilitate%20better%20learning%20from%20boundary%20video-segments%20that%0Atypically%20include%20multiple%20labels.%20Second%2C%20we%20design%20a%20post-processing%20step%20to%0Aefficiently%20fuse%20information%20from%20video-segments%20and%20multiple%20camera%20views%20into%0Ascene-level%20predictions%2C%20which%20facilitates%20elimination%20of%20false%20positives.%20Our%0Amethodology%20yields%20a%20competitive%20performance%20on%20the%20A2%20test%20set%20of%20the%0Anaturalistic%20driving%20action%20recognition%20track%20of%20the%202022%20NVIDIA%20AI%20City%0AChallenge%20with%20an%20F1%20score%20of%200.271.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06616v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Density-Guided%20Label%20Smoothing%20for%20Temporal%20Localization%20of%20Driving%0A%20%20Actions&entry.906535625=Tunc%20Alkanat%20and%20Erkut%20Akdag%20and%20Egor%20Bondarev%20and%20Peter%20H.%20N.%20De%20With&entry.1292438233=%20%20Temporal%20localization%20of%20driving%20actions%20plays%20a%20crucial%20role%20in%20advanced%0Adriver-assistance%20systems%20and%20naturalistic%20driving%20studies.%20However%2C%20this%20is%20a%0Achallenging%20task%20due%20to%20strict%20requirements%20for%20robustness%2C%20reliability%20and%0Aaccurate%20localization.%20In%20this%20work%2C%20we%20focus%20on%20improving%20the%20overall%0Aperformance%20by%20efficiently%20utilizing%20video%20action%20recognition%20networks%20and%0Aadapting%20these%20to%20the%20problem%20of%20action%20localization.%20To%20this%20end%2C%20we%20first%0Adevelop%20a%20density-guided%20label%20smoothing%20technique%20based%20on%20label%20probability%0Adistributions%20to%20facilitate%20better%20learning%20from%20boundary%20video-segments%20that%0Atypically%20include%20multiple%20labels.%20Second%2C%20we%20design%20a%20post-processing%20step%20to%0Aefficiently%20fuse%20information%20from%20video-segments%20and%20multiple%20camera%20views%20into%0Ascene-level%20predictions%2C%20which%20facilitates%20elimination%20of%20false%20positives.%20Our%0Amethodology%20yields%20a%20competitive%20performance%20on%20the%20A2%20test%20set%20of%20the%0Anaturalistic%20driving%20action%20recognition%20track%20of%20the%202022%20NVIDIA%20AI%20City%0AChallenge%20with%20an%20F1%20score%20of%200.271.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06616v1&entry.124074799=Read"},
{"title": "Data-Independent Operator: A Training-Free Artifact Representation\n  Extractor for Generalizable Deepfake Detection", "author": "Chuangchuang Tan and Ping Liu and RenShuai Tao and Huan Liu and Yao Zhao and Baoyuan Wu and Yunchao Wei", "abstract": "  Recently, the proliferation of increasingly realistic synthetic images\ngenerated by various generative adversarial networks has increased the risk of\nmisuse. Consequently, there is a pressing need to develop a generalizable\ndetector for accurately recognizing fake images. The conventional methods rely\non generating diverse training sources or large pretrained models. In this\nwork, we show that, on the contrary, the small and training-free filter is\nsufficient to capture more general artifact representations. Due to its unbias\ntowards both the training and test sources, we define it as Data-Independent\nOperator (DIO) to achieve appealing improvements on unseen sources. In our\nframework, handcrafted filters and the randomly-initialized convolutional layer\ncan be used as the training-free artifact representations extractor with\nexcellent results. With the data-independent operator of a popular classifier,\nsuch as Resnet50, one could already reach a new state-of-the-art without bells\nand whistles. We evaluate the effectiveness of the DIO on 33 generation models,\neven DALLE and Midjourney. Our detector achieves a remarkable improvement of\n$13.3\\%$, establishing a new state-of-the-art performance. The DIO and its\nextension can serve as strong baselines for future methods. The code is\navailable at\n\\url{https://github.com/chuangchuangtan/Data-Independent-Operator}.\n", "link": "http://arxiv.org/abs/2403.06803v1", "date": "2024-03-11", "relevancy": 2.712, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5549}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5291}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Data-Independent%20Operator%3A%20A%20Training-Free%20Artifact%20Representation%0A%20%20Extractor%20for%20Generalizable%20Deepfake%20Detection&body=Title%3A%20Data-Independent%20Operator%3A%20A%20Training-Free%20Artifact%20Representation%0A%20%20Extractor%20for%20Generalizable%20Deepfake%20Detection%0AAuthor%3A%20Chuangchuang%20Tan%20and%20Ping%20Liu%20and%20RenShuai%20Tao%20and%20Huan%20Liu%20and%20Yao%20Zhao%20and%20Baoyuan%20Wu%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Recently%2C%20the%20proliferation%20of%20increasingly%20realistic%20synthetic%20images%0Agenerated%20by%20various%20generative%20adversarial%20networks%20has%20increased%20the%20risk%20of%0Amisuse.%20Consequently%2C%20there%20is%20a%20pressing%20need%20to%20develop%20a%20generalizable%0Adetector%20for%20accurately%20recognizing%20fake%20images.%20The%20conventional%20methods%20rely%0Aon%20generating%20diverse%20training%20sources%20or%20large%20pretrained%20models.%20In%20this%0Awork%2C%20we%20show%20that%2C%20on%20the%20contrary%2C%20the%20small%20and%20training-free%20filter%20is%0Asufficient%20to%20capture%20more%20general%20artifact%20representations.%20Due%20to%20its%20unbias%0Atowards%20both%20the%20training%20and%20test%20sources%2C%20we%20define%20it%20as%20Data-Independent%0AOperator%20%28DIO%29%20to%20achieve%20appealing%20improvements%20on%20unseen%20sources.%20In%20our%0Aframework%2C%20handcrafted%20filters%20and%20the%20randomly-initialized%20convolutional%20layer%0Acan%20be%20used%20as%20the%20training-free%20artifact%20representations%20extractor%20with%0Aexcellent%20results.%20With%20the%20data-independent%20operator%20of%20a%20popular%20classifier%2C%0Asuch%20as%20Resnet50%2C%20one%20could%20already%20reach%20a%20new%20state-of-the-art%20without%20bells%0Aand%20whistles.%20We%20evaluate%20the%20effectiveness%20of%20the%20DIO%20on%2033%20generation%20models%2C%0Aeven%20DALLE%20and%20Midjourney.%20Our%20detector%20achieves%20a%20remarkable%20improvement%20of%0A%2413.3%5C%25%24%2C%20establishing%20a%20new%20state-of-the-art%20performance.%20The%20DIO%20and%20its%0Aextension%20can%20serve%20as%20strong%20baselines%20for%20future%20methods.%20The%20code%20is%0Aavailable%20at%0A%5Curl%7Bhttps%3A//github.com/chuangchuangtan/Data-Independent-Operator%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06803v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Independent%20Operator%3A%20A%20Training-Free%20Artifact%20Representation%0A%20%20Extractor%20for%20Generalizable%20Deepfake%20Detection&entry.906535625=Chuangchuang%20Tan%20and%20Ping%20Liu%20and%20RenShuai%20Tao%20and%20Huan%20Liu%20and%20Yao%20Zhao%20and%20Baoyuan%20Wu%20and%20Yunchao%20Wei&entry.1292438233=%20%20Recently%2C%20the%20proliferation%20of%20increasingly%20realistic%20synthetic%20images%0Agenerated%20by%20various%20generative%20adversarial%20networks%20has%20increased%20the%20risk%20of%0Amisuse.%20Consequently%2C%20there%20is%20a%20pressing%20need%20to%20develop%20a%20generalizable%0Adetector%20for%20accurately%20recognizing%20fake%20images.%20The%20conventional%20methods%20rely%0Aon%20generating%20diverse%20training%20sources%20or%20large%20pretrained%20models.%20In%20this%0Awork%2C%20we%20show%20that%2C%20on%20the%20contrary%2C%20the%20small%20and%20training-free%20filter%20is%0Asufficient%20to%20capture%20more%20general%20artifact%20representations.%20Due%20to%20its%20unbias%0Atowards%20both%20the%20training%20and%20test%20sources%2C%20we%20define%20it%20as%20Data-Independent%0AOperator%20%28DIO%29%20to%20achieve%20appealing%20improvements%20on%20unseen%20sources.%20In%20our%0Aframework%2C%20handcrafted%20filters%20and%20the%20randomly-initialized%20convolutional%20layer%0Acan%20be%20used%20as%20the%20training-free%20artifact%20representations%20extractor%20with%0Aexcellent%20results.%20With%20the%20data-independent%20operator%20of%20a%20popular%20classifier%2C%0Asuch%20as%20Resnet50%2C%20one%20could%20already%20reach%20a%20new%20state-of-the-art%20without%20bells%0Aand%20whistles.%20We%20evaluate%20the%20effectiveness%20of%20the%20DIO%20on%2033%20generation%20models%2C%0Aeven%20DALLE%20and%20Midjourney.%20Our%20detector%20achieves%20a%20remarkable%20improvement%20of%0A%2413.3%5C%25%24%2C%20establishing%20a%20new%20state-of-the-art%20performance.%20The%20DIO%20and%20its%0Aextension%20can%20serve%20as%20strong%20baselines%20for%20future%20methods.%20The%20code%20is%0Aavailable%20at%0A%5Curl%7Bhttps%3A//github.com/chuangchuangtan/Data-Independent-Operator%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06803v1&entry.124074799=Read"},
{"title": "Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and\n  Attention Mechanism Approach for Heterogeneous Graph-Structured Data", "author": "Jinghan Huang and Qiufeng Chen and Yijun Bian and Pengli Zhu and Nanguang Chen and Moo K. Chung and Anqi Qiu", "abstract": "  Graph neural networks (GNNs) have proven effective in capturing relationships\namong nodes in a graph. This study introduces a novel perspective by\nconsidering a graph as a simplicial complex, encompassing nodes, edges,\ntriangles, and $k$-simplices, enabling the definition of graph-structured data\non any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous\ngraph attention network (HL-HGAT), designed to learn heterogeneous signal\nrepresentations across $k$-simplices. The HL-HGAT incorporates three key\ncomponents: HL convolutional filters (HL-filters), simplicial projection (SP),\nand simplicial attention pooling (SAP) operators, applied to $k$-simplices.\nHL-filters leverage the unique topology of $k$-simplices encoded by the\nHodge-Laplacian (HL) operator, operating within the spectral domain of the\n$k$-th HL operator. To address computation challenges, we introduce a\npolynomial approximation for HL-filters, exhibiting spatial localization\nproperties. Additionally, we propose a pooling operator to coarsen\n$k$-simplices, combining features through simplicial attention mechanisms of\nself-attention and cross-attention via transformers and SP operators, capturing\ntopological interconnections across multiple dimensions of simplices. The\nHL-HGAT is comprehensively evaluated across diverse graph applications,\nincluding NP-hard problems, graph multi-label and classification challenges,\nand graph regression tasks in logistics, computer vision, biology, chemistry,\nand neuroscience. The results demonstrate the model's efficacy and versatility\nin handling a wide range of graph-based scenarios.\n", "link": "http://arxiv.org/abs/2403.06687v1", "date": "2024-03-11", "relevancy": 2.7005, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5779}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5301}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Advancing%20Graph%20Neural%20Networks%20with%20HL-HGAT%3A%20A%20Hodge-Laplacian%20and%0A%20%20Attention%20Mechanism%20Approach%20for%20Heterogeneous%20Graph-Structured%20Data&body=Title%3A%20Advancing%20Graph%20Neural%20Networks%20with%20HL-HGAT%3A%20A%20Hodge-Laplacian%20and%0A%20%20Attention%20Mechanism%20Approach%20for%20Heterogeneous%20Graph-Structured%20Data%0AAuthor%3A%20Jinghan%20Huang%20and%20Qiufeng%20Chen%20and%20Yijun%20Bian%20and%20Pengli%20Zhu%20and%20Nanguang%20Chen%20and%20Moo%20K.%20Chung%20and%20Anqi%20Qiu%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20proven%20effective%20in%20capturing%20relationships%0Aamong%20nodes%20in%20a%20graph.%20This%20study%20introduces%20a%20novel%20perspective%20by%0Aconsidering%20a%20graph%20as%20a%20simplicial%20complex%2C%20encompassing%20nodes%2C%20edges%2C%0Atriangles%2C%20and%20%24k%24-simplices%2C%20enabling%20the%20definition%20of%20graph-structured%20data%0Aon%20any%20%24k%24-simplices.%20Our%20contribution%20is%20the%20Hodge-Laplacian%20heterogeneous%0Agraph%20attention%20network%20%28HL-HGAT%29%2C%20designed%20to%20learn%20heterogeneous%20signal%0Arepresentations%20across%20%24k%24-simplices.%20The%20HL-HGAT%20incorporates%20three%20key%0Acomponents%3A%20HL%20convolutional%20filters%20%28HL-filters%29%2C%20simplicial%20projection%20%28SP%29%2C%0Aand%20simplicial%20attention%20pooling%20%28SAP%29%20operators%2C%20applied%20to%20%24k%24-simplices.%0AHL-filters%20leverage%20the%20unique%20topology%20of%20%24k%24-simplices%20encoded%20by%20the%0AHodge-Laplacian%20%28HL%29%20operator%2C%20operating%20within%20the%20spectral%20domain%20of%20the%0A%24k%24-th%20HL%20operator.%20To%20address%20computation%20challenges%2C%20we%20introduce%20a%0Apolynomial%20approximation%20for%20HL-filters%2C%20exhibiting%20spatial%20localization%0Aproperties.%20Additionally%2C%20we%20propose%20a%20pooling%20operator%20to%20coarsen%0A%24k%24-simplices%2C%20combining%20features%20through%20simplicial%20attention%20mechanisms%20of%0Aself-attention%20and%20cross-attention%20via%20transformers%20and%20SP%20operators%2C%20capturing%0Atopological%20interconnections%20across%20multiple%20dimensions%20of%20simplices.%20The%0AHL-HGAT%20is%20comprehensively%20evaluated%20across%20diverse%20graph%20applications%2C%0Aincluding%20NP-hard%20problems%2C%20graph%20multi-label%20and%20classification%20challenges%2C%0Aand%20graph%20regression%20tasks%20in%20logistics%2C%20computer%20vision%2C%20biology%2C%20chemistry%2C%0Aand%20neuroscience.%20The%20results%20demonstrate%20the%20model%27s%20efficacy%20and%20versatility%0Ain%20handling%20a%20wide%20range%20of%20graph-based%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Graph%20Neural%20Networks%20with%20HL-HGAT%3A%20A%20Hodge-Laplacian%20and%0A%20%20Attention%20Mechanism%20Approach%20for%20Heterogeneous%20Graph-Structured%20Data&entry.906535625=Jinghan%20Huang%20and%20Qiufeng%20Chen%20and%20Yijun%20Bian%20and%20Pengli%20Zhu%20and%20Nanguang%20Chen%20and%20Moo%20K.%20Chung%20and%20Anqi%20Qiu&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20proven%20effective%20in%20capturing%20relationships%0Aamong%20nodes%20in%20a%20graph.%20This%20study%20introduces%20a%20novel%20perspective%20by%0Aconsidering%20a%20graph%20as%20a%20simplicial%20complex%2C%20encompassing%20nodes%2C%20edges%2C%0Atriangles%2C%20and%20%24k%24-simplices%2C%20enabling%20the%20definition%20of%20graph-structured%20data%0Aon%20any%20%24k%24-simplices.%20Our%20contribution%20is%20the%20Hodge-Laplacian%20heterogeneous%0Agraph%20attention%20network%20%28HL-HGAT%29%2C%20designed%20to%20learn%20heterogeneous%20signal%0Arepresentations%20across%20%24k%24-simplices.%20The%20HL-HGAT%20incorporates%20three%20key%0Acomponents%3A%20HL%20convolutional%20filters%20%28HL-filters%29%2C%20simplicial%20projection%20%28SP%29%2C%0Aand%20simplicial%20attention%20pooling%20%28SAP%29%20operators%2C%20applied%20to%20%24k%24-simplices.%0AHL-filters%20leverage%20the%20unique%20topology%20of%20%24k%24-simplices%20encoded%20by%20the%0AHodge-Laplacian%20%28HL%29%20operator%2C%20operating%20within%20the%20spectral%20domain%20of%20the%0A%24k%24-th%20HL%20operator.%20To%20address%20computation%20challenges%2C%20we%20introduce%20a%0Apolynomial%20approximation%20for%20HL-filters%2C%20exhibiting%20spatial%20localization%0Aproperties.%20Additionally%2C%20we%20propose%20a%20pooling%20operator%20to%20coarsen%0A%24k%24-simplices%2C%20combining%20features%20through%20simplicial%20attention%20mechanisms%20of%0Aself-attention%20and%20cross-attention%20via%20transformers%20and%20SP%20operators%2C%20capturing%0Atopological%20interconnections%20across%20multiple%20dimensions%20of%20simplices.%20The%0AHL-HGAT%20is%20comprehensively%20evaluated%20across%20diverse%20graph%20applications%2C%0Aincluding%20NP-hard%20problems%2C%20graph%20multi-label%20and%20classification%20challenges%2C%0Aand%20graph%20regression%20tasks%20in%20logistics%2C%20computer%20vision%2C%20biology%2C%20chemistry%2C%0Aand%20neuroscience.%20The%20results%20demonstrate%20the%20model%27s%20efficacy%20and%20versatility%0Ain%20handling%20a%20wide%20range%20of%20graph-based%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06687v1&entry.124074799=Read"},
{"title": "FaceChain-SuDe: Building Derived Class to Inherit Category Attributes\n  for One-shot Subject-Driven Generation", "author": "Pengchong Qiao and Lei Shang and Chang Liu and Baigui Sun and Xiangyang Ji and Jie Chen", "abstract": "  Subject-driven generation has garnered significant interest recently due to\nits ability to personalize text-to-image generation. Typical works focus on\nlearning the new subject's private attributes. However, an important fact has\nnot been taken seriously that a subject is not an isolated new concept but\nshould be a specialization of a certain category in the pre-trained model. This\nresults in the subject failing to comprehensively inherit the attributes in its\ncategory, causing poor attribute-related generations. In this paper, motivated\nby object-oriented programming, we model the subject as a derived class whose\nbase class is its semantic category. This modeling enables the subject to\ninherit public attributes from its category while learning its private\nattributes from the user-provided example. Specifically, we propose a\nplug-and-play method, Subject-Derived regularization (SuDe). It constructs the\nbase-derived class modeling by constraining the subject-driven generated images\nto semantically belong to the subject's category. Extensive experiments under\nthree baselines and two backbones on various subjects show that our SuDe\nenables imaginative attribute-related generations while maintaining subject\nfidelity. Codes will be open sourced soon at FaceChain\n(https://github.com/modelscope/facechain).\n", "link": "http://arxiv.org/abs/2403.06775v1", "date": "2024-03-11", "relevancy": 2.6748, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5553}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5527}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4969}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20FaceChain-SuDe%3A%20Building%20Derived%20Class%20to%20Inherit%20Category%20Attributes%0A%20%20for%20One-shot%20Subject-Driven%20Generation&body=Title%3A%20FaceChain-SuDe%3A%20Building%20Derived%20Class%20to%20Inherit%20Category%20Attributes%0A%20%20for%20One-shot%20Subject-Driven%20Generation%0AAuthor%3A%20Pengchong%20Qiao%20and%20Lei%20Shang%20and%20Chang%20Liu%20and%20Baigui%20Sun%20and%20Xiangyang%20Ji%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Subject-driven%20generation%20has%20garnered%20significant%20interest%20recently%20due%20to%0Aits%20ability%20to%20personalize%20text-to-image%20generation.%20Typical%20works%20focus%20on%0Alearning%20the%20new%20subject%27s%20private%20attributes.%20However%2C%20an%20important%20fact%20has%0Anot%20been%20taken%20seriously%20that%20a%20subject%20is%20not%20an%20isolated%20new%20concept%20but%0Ashould%20be%20a%20specialization%20of%20a%20certain%20category%20in%20the%20pre-trained%20model.%20This%0Aresults%20in%20the%20subject%20failing%20to%20comprehensively%20inherit%20the%20attributes%20in%20its%0Acategory%2C%20causing%20poor%20attribute-related%20generations.%20In%20this%20paper%2C%20motivated%0Aby%20object-oriented%20programming%2C%20we%20model%20the%20subject%20as%20a%20derived%20class%20whose%0Abase%20class%20is%20its%20semantic%20category.%20This%20modeling%20enables%20the%20subject%20to%0Ainherit%20public%20attributes%20from%20its%20category%20while%20learning%20its%20private%0Aattributes%20from%20the%20user-provided%20example.%20Specifically%2C%20we%20propose%20a%0Aplug-and-play%20method%2C%20Subject-Derived%20regularization%20%28SuDe%29.%20It%20constructs%20the%0Abase-derived%20class%20modeling%20by%20constraining%20the%20subject-driven%20generated%20images%0Ato%20semantically%20belong%20to%20the%20subject%27s%20category.%20Extensive%20experiments%20under%0Athree%20baselines%20and%20two%20backbones%20on%20various%20subjects%20show%20that%20our%20SuDe%0Aenables%20imaginative%20attribute-related%20generations%20while%20maintaining%20subject%0Afidelity.%20Codes%20will%20be%20open%20sourced%20soon%20at%20FaceChain%0A%28https%3A//github.com/modelscope/facechain%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06775v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceChain-SuDe%3A%20Building%20Derived%20Class%20to%20Inherit%20Category%20Attributes%0A%20%20for%20One-shot%20Subject-Driven%20Generation&entry.906535625=Pengchong%20Qiao%20and%20Lei%20Shang%20and%20Chang%20Liu%20and%20Baigui%20Sun%20and%20Xiangyang%20Ji%20and%20Jie%20Chen&entry.1292438233=%20%20Subject-driven%20generation%20has%20garnered%20significant%20interest%20recently%20due%20to%0Aits%20ability%20to%20personalize%20text-to-image%20generation.%20Typical%20works%20focus%20on%0Alearning%20the%20new%20subject%27s%20private%20attributes.%20However%2C%20an%20important%20fact%20has%0Anot%20been%20taken%20seriously%20that%20a%20subject%20is%20not%20an%20isolated%20new%20concept%20but%0Ashould%20be%20a%20specialization%20of%20a%20certain%20category%20in%20the%20pre-trained%20model.%20This%0Aresults%20in%20the%20subject%20failing%20to%20comprehensively%20inherit%20the%20attributes%20in%20its%0Acategory%2C%20causing%20poor%20attribute-related%20generations.%20In%20this%20paper%2C%20motivated%0Aby%20object-oriented%20programming%2C%20we%20model%20the%20subject%20as%20a%20derived%20class%20whose%0Abase%20class%20is%20its%20semantic%20category.%20This%20modeling%20enables%20the%20subject%20to%0Ainherit%20public%20attributes%20from%20its%20category%20while%20learning%20its%20private%0Aattributes%20from%20the%20user-provided%20example.%20Specifically%2C%20we%20propose%20a%0Aplug-and-play%20method%2C%20Subject-Derived%20regularization%20%28SuDe%29.%20It%20constructs%20the%0Abase-derived%20class%20modeling%20by%20constraining%20the%20subject-driven%20generated%20images%0Ato%20semantically%20belong%20to%20the%20subject%27s%20category.%20Extensive%20experiments%20under%0Athree%20baselines%20and%20two%20backbones%20on%20various%20subjects%20show%20that%20our%20SuDe%0Aenables%20imaginative%20attribute-related%20generations%20while%20maintaining%20subject%0Afidelity.%20Codes%20will%20be%20open%20sourced%20soon%20at%20FaceChain%0A%28https%3A//github.com/modelscope/facechain%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06775v1&entry.124074799=Read"},
{"title": "LeOCLR: Leveraging Original Images for Contrastive Learning of Visual\n  Representations", "author": "Mohammad Alkhalefi and Georgios Leontidis and Mingjun Zhong", "abstract": "  Contrastive instance discrimination outperforms supervised learning in\ndownstream tasks like image classification and object detection. However, this\napproach heavily relies on data augmentation during representation learning,\nwhich may result in inferior results if not properly implemented. Random\ncropping followed by resizing is a common form of data augmentation used in\ncontrastive learning, but it can lead to degraded representation learning if\nthe two random crops contain distinct semantic content. To address this issue,\nthis paper introduces LeOCLR (Leveraging Original Images for Contrastive\nLearning of Visual Representations), a framework that employs a new instance\ndiscrimination approach and an adapted loss function that ensures the shared\nregion between positive pairs is semantically correct. The experimental results\nshow that our approach consistently improves representation learning across\ndifferent datasets compared to baseline models. For example, our approach\noutperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several\nother methods on transfer learning tasks.\n", "link": "http://arxiv.org/abs/2403.06813v1", "date": "2024-03-11", "relevancy": 2.6726, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.57}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5064}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations&body=Title%3A%20LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations%0AAuthor%3A%20Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong%0AAbstract%3A%20%20%20Contrastive%20instance%20discrimination%20outperforms%20supervised%20learning%20in%0Adownstream%20tasks%20like%20image%20classification%20and%20object%20detection.%20However%2C%20this%0Aapproach%20heavily%20relies%20on%20data%20augmentation%20during%20representation%20learning%2C%0Awhich%20may%20result%20in%20inferior%20results%20if%20not%20properly%20implemented.%20Random%0Acropping%20followed%20by%20resizing%20is%20a%20common%20form%20of%20data%20augmentation%20used%20in%0Acontrastive%20learning%2C%20but%20it%20can%20lead%20to%20degraded%20representation%20learning%20if%0Athe%20two%20random%20crops%20contain%20distinct%20semantic%20content.%20To%20address%20this%20issue%2C%0Athis%20paper%20introduces%20LeOCLR%20%28Leveraging%20Original%20Images%20for%20Contrastive%0ALearning%20of%20Visual%20Representations%29%2C%20a%20framework%20that%20employs%20a%20new%20instance%0Adiscrimination%20approach%20and%20an%20adapted%20loss%20function%20that%20ensures%20the%20shared%0Aregion%20between%20positive%20pairs%20is%20semantically%20correct.%20The%20experimental%20results%0Ashow%20that%20our%20approach%20consistently%20improves%20representation%20learning%20across%0Adifferent%20datasets%20compared%20to%20baseline%20models.%20For%20example%2C%20our%20approach%0Aoutperforms%20MoCo-v2%20by%205.1%25%20on%20ImageNet-1K%20in%20linear%20evaluation%20and%20several%0Aother%20methods%20on%20transfer%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06813v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations&entry.906535625=Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong&entry.1292438233=%20%20Contrastive%20instance%20discrimination%20outperforms%20supervised%20learning%20in%0Adownstream%20tasks%20like%20image%20classification%20and%20object%20detection.%20However%2C%20this%0Aapproach%20heavily%20relies%20on%20data%20augmentation%20during%20representation%20learning%2C%0Awhich%20may%20result%20in%20inferior%20results%20if%20not%20properly%20implemented.%20Random%0Acropping%20followed%20by%20resizing%20is%20a%20common%20form%20of%20data%20augmentation%20used%20in%0Acontrastive%20learning%2C%20but%20it%20can%20lead%20to%20degraded%20representation%20learning%20if%0Athe%20two%20random%20crops%20contain%20distinct%20semantic%20content.%20To%20address%20this%20issue%2C%0Athis%20paper%20introduces%20LeOCLR%20%28Leveraging%20Original%20Images%20for%20Contrastive%0ALearning%20of%20Visual%20Representations%29%2C%20a%20framework%20that%20employs%20a%20new%20instance%0Adiscrimination%20approach%20and%20an%20adapted%20loss%20function%20that%20ensures%20the%20shared%0Aregion%20between%20positive%20pairs%20is%20semantically%20correct.%20The%20experimental%20results%0Ashow%20that%20our%20approach%20consistently%20improves%20representation%20learning%20across%0Adifferent%20datasets%20compared%20to%20baseline%20models.%20For%20example%2C%20our%20approach%0Aoutperforms%20MoCo-v2%20by%205.1%25%20on%20ImageNet-1K%20in%20linear%20evaluation%20and%20several%0Aother%20methods%20on%20transfer%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06813v1&entry.124074799=Read"},
{"title": "Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification", "author": "Fengda Zhang and Qianpei He and Kun Kuang and Jiashuo Liu and Long Chen and Chao Wu and Jun Xiao and Hanwang Zhang", "abstract": "  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n", "link": "http://arxiv.org/abs/2403.06606v1", "date": "2024-03-11", "relevancy": 2.6407, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5322}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5277}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5245}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Distributionally%20Generative%20Augmentation%20for%20Fair%20Facial%20Attribute%0A%20%20Classification&body=Title%3A%20Distributionally%20Generative%20Augmentation%20for%20Fair%20Facial%20Attribute%0A%20%20Classification%0AAuthor%3A%20Fengda%20Zhang%20and%20Qianpei%20He%20and%20Kun%20Kuang%20and%20Jiashuo%20Liu%20and%20Long%20Chen%20and%20Chao%20Wu%20and%20Jun%20Xiao%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Facial%20Attribute%20Classification%20%28FAC%29%20holds%20substantial%20promise%20in%20widespread%0Aapplications.%20However%2C%20FAC%20models%20trained%20by%20traditional%20methodologies%20can%20be%0Aunfair%20by%20exhibiting%20accuracy%20inconsistencies%20across%20varied%20data%0Asubpopulations.%20This%20unfairness%20is%20largely%20attributed%20to%20bias%20in%20data%2C%20where%0Asome%20spurious%20attributes%20%28e.g.%2C%20Male%29%20statistically%20correlate%20with%20the%20target%0Aattribute%20%28e.g.%2C%20Smiling%29.%20Most%20of%20existing%20fairness-aware%20methods%20rely%20on%20the%0Alabels%20of%20spurious%20attributes%2C%20which%20may%20be%20unavailable%20in%20practice.%20This%20work%0Aproposes%20a%20novel%2C%20generation-based%20two-stage%20framework%20to%20train%20a%20fair%20FAC%0Amodel%20on%20biased%20data%20without%20additional%20annotation.%20Initially%2C%20we%20identify%20the%0Apotential%20spurious%20attributes%20based%20on%20generative%20models.%20Notably%2C%20it%20enhances%0Ainterpretability%20by%20explicitly%20showing%20the%20spurious%20attributes%20in%20image%20space.%0AFollowing%20this%2C%20for%20each%20image%2C%20we%20first%20edit%20the%20spurious%20attributes%20with%20a%0Arandom%20degree%20sampled%20from%20a%20uniform%20distribution%2C%20while%20keeping%20target%0Aattribute%20unchanged.%20Then%20we%20train%20a%20fair%20FAC%20model%20by%20fostering%20model%0Ainvariance%20to%20these%20augmentation.%20Extensive%20experiments%20on%20three%20common%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20promoting%20fairness%20in%0AFAC%20without%20compromising%20accuracy.%20Codes%20are%20in%0Ahttps%3A//github.com/heqianpei/DiGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06606v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributionally%20Generative%20Augmentation%20for%20Fair%20Facial%20Attribute%0A%20%20Classification&entry.906535625=Fengda%20Zhang%20and%20Qianpei%20He%20and%20Kun%20Kuang%20and%20Jiashuo%20Liu%20and%20Long%20Chen%20and%20Chao%20Wu%20and%20Jun%20Xiao%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Facial%20Attribute%20Classification%20%28FAC%29%20holds%20substantial%20promise%20in%20widespread%0Aapplications.%20However%2C%20FAC%20models%20trained%20by%20traditional%20methodologies%20can%20be%0Aunfair%20by%20exhibiting%20accuracy%20inconsistencies%20across%20varied%20data%0Asubpopulations.%20This%20unfairness%20is%20largely%20attributed%20to%20bias%20in%20data%2C%20where%0Asome%20spurious%20attributes%20%28e.g.%2C%20Male%29%20statistically%20correlate%20with%20the%20target%0Aattribute%20%28e.g.%2C%20Smiling%29.%20Most%20of%20existing%20fairness-aware%20methods%20rely%20on%20the%0Alabels%20of%20spurious%20attributes%2C%20which%20may%20be%20unavailable%20in%20practice.%20This%20work%0Aproposes%20a%20novel%2C%20generation-based%20two-stage%20framework%20to%20train%20a%20fair%20FAC%0Amodel%20on%20biased%20data%20without%20additional%20annotation.%20Initially%2C%20we%20identify%20the%0Apotential%20spurious%20attributes%20based%20on%20generative%20models.%20Notably%2C%20it%20enhances%0Ainterpretability%20by%20explicitly%20showing%20the%20spurious%20attributes%20in%20image%20space.%0AFollowing%20this%2C%20for%20each%20image%2C%20we%20first%20edit%20the%20spurious%20attributes%20with%20a%0Arandom%20degree%20sampled%20from%20a%20uniform%20distribution%2C%20while%20keeping%20target%0Aattribute%20unchanged.%20Then%20we%20train%20a%20fair%20FAC%20model%20by%20fostering%20model%0Ainvariance%20to%20these%20augmentation.%20Extensive%20experiments%20on%20three%20common%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20promoting%20fairness%20in%0AFAC%20without%20compromising%20accuracy.%20Codes%20are%20in%0Ahttps%3A//github.com/heqianpei/DiGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06606v1&entry.124074799=Read"},
{"title": "Guiding Masked Representation Learning to Capture Spatio-Temporal\n  Relationship of Electrocardiogram", "author": "Yeongyeon Na and Minje Park and Yunwon Tae and Sunghoon Joo", "abstract": "  Electrocardiograms (ECG) are widely employed as a diagnostic tool for\nmonitoring electrical signals originating from a heart. Recent machine learning\nresearch efforts have focused on the application of screening various diseases\nusing ECG signals. However, adapting to the application of screening disease is\nchallenging in that labeled ECG data are limited. Achieving general\nrepresentation through self-supervised learning (SSL) is a well-known approach\nto overcome the scarcity of labeled data; however, a naive application of SSL\nto ECG data, without considering the spatial-temporal relationships inherent in\nECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM\n(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn\nspatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM\noutperforms other SSL baseline methods in various experimental settings for\narrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is\nadaptable to various lead combinations. Through quantitative and qualitative\nanalysis, we show a spatio-temporal relationship within ECG data. Our code is\navailable at https://github.com/bakqui/ST-MEM.\n", "link": "http://arxiv.org/abs/2402.09450v2", "date": "2024-03-11", "relevancy": 2.6055, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5404}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Guiding%20Masked%20Representation%20Learning%20to%20Capture%20Spatio-Temporal%0A%20%20Relationship%20of%20Electrocardiogram&body=Title%3A%20Guiding%20Masked%20Representation%20Learning%20to%20Capture%20Spatio-Temporal%0A%20%20Relationship%20of%20Electrocardiogram%0AAuthor%3A%20Yeongyeon%20Na%20and%20Minje%20Park%20and%20Yunwon%20Tae%20and%20Sunghoon%20Joo%0AAbstract%3A%20%20%20Electrocardiograms%20%28ECG%29%20are%20widely%20employed%20as%20a%20diagnostic%20tool%20for%0Amonitoring%20electrical%20signals%20originating%20from%20a%20heart.%20Recent%20machine%20learning%0Aresearch%20efforts%20have%20focused%20on%20the%20application%20of%20screening%20various%20diseases%0Ausing%20ECG%20signals.%20However%2C%20adapting%20to%20the%20application%20of%20screening%20disease%20is%0Achallenging%20in%20that%20labeled%20ECG%20data%20are%20limited.%20Achieving%20general%0Arepresentation%20through%20self-supervised%20learning%20%28SSL%29%20is%20a%20well-known%20approach%0Ato%20overcome%20the%20scarcity%20of%20labeled%20data%3B%20however%2C%20a%20naive%20application%20of%20SSL%0Ato%20ECG%20data%2C%20without%20considering%20the%20spatial-temporal%20relationships%20inherent%20in%0AECG%20signals%2C%20may%20yield%20suboptimal%20results.%20In%20this%20paper%2C%20we%20introduce%20ST-MEM%0A%28Spatio-Temporal%20Masked%20Electrocardiogram%20Modeling%29%2C%20designed%20to%20learn%0Aspatio-temporal%20features%20by%20reconstructing%20masked%2012-lead%20ECG%20data.%20ST-MEM%0Aoutperforms%20other%20SSL%20baseline%20methods%20in%20various%20experimental%20settings%20for%0Aarrhythmia%20classification%20tasks.%20Moreover%2C%20we%20demonstrate%20that%20ST-MEM%20is%0Aadaptable%20to%20various%20lead%20combinations.%20Through%20quantitative%20and%20qualitative%0Aanalysis%2C%20we%20show%20a%20spatio-temporal%20relationship%20within%20ECG%20data.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/bakqui/ST-MEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09450v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Masked%20Representation%20Learning%20to%20Capture%20Spatio-Temporal%0A%20%20Relationship%20of%20Electrocardiogram&entry.906535625=Yeongyeon%20Na%20and%20Minje%20Park%20and%20Yunwon%20Tae%20and%20Sunghoon%20Joo&entry.1292438233=%20%20Electrocardiograms%20%28ECG%29%20are%20widely%20employed%20as%20a%20diagnostic%20tool%20for%0Amonitoring%20electrical%20signals%20originating%20from%20a%20heart.%20Recent%20machine%20learning%0Aresearch%20efforts%20have%20focused%20on%20the%20application%20of%20screening%20various%20diseases%0Ausing%20ECG%20signals.%20However%2C%20adapting%20to%20the%20application%20of%20screening%20disease%20is%0Achallenging%20in%20that%20labeled%20ECG%20data%20are%20limited.%20Achieving%20general%0Arepresentation%20through%20self-supervised%20learning%20%28SSL%29%20is%20a%20well-known%20approach%0Ato%20overcome%20the%20scarcity%20of%20labeled%20data%3B%20however%2C%20a%20naive%20application%20of%20SSL%0Ato%20ECG%20data%2C%20without%20considering%20the%20spatial-temporal%20relationships%20inherent%20in%0AECG%20signals%2C%20may%20yield%20suboptimal%20results.%20In%20this%20paper%2C%20we%20introduce%20ST-MEM%0A%28Spatio-Temporal%20Masked%20Electrocardiogram%20Modeling%29%2C%20designed%20to%20learn%0Aspatio-temporal%20features%20by%20reconstructing%20masked%2012-lead%20ECG%20data.%20ST-MEM%0Aoutperforms%20other%20SSL%20baseline%20methods%20in%20various%20experimental%20settings%20for%0Aarrhythmia%20classification%20tasks.%20Moreover%2C%20we%20demonstrate%20that%20ST-MEM%20is%0Aadaptable%20to%20various%20lead%20combinations.%20Through%20quantitative%20and%20qualitative%0Aanalysis%2C%20we%20show%20a%20spatio-temporal%20relationship%20within%20ECG%20data.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/bakqui/ST-MEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09450v2&entry.124074799=Read"},
{"title": "V3D: Video Diffusion Models are Effective 3D Generators", "author": "Zilong Chen and Yikai Wang and Feng Wang and Zhengyi Wang and Huaping Liu", "abstract": "  Automatic 3D generation has recently attracted widespread attention. Recent\nmethods have greatly accelerated the generation speed, but usually produce\nless-detailed objects due to limited model capacity or 3D data. Motivated by\nrecent advancements in video diffusion models, we introduce V3D, which\nleverages the world simulation capacity of pre-trained video diffusion models\nto facilitate 3D generation. To fully unleash the potential of video diffusion\nto perceive the 3D world, we further introduce geometrical consistency prior\nand extend the video diffusion model to a multi-view consistent 3D generator.\nBenefiting from this, the state-of-the-art video diffusion model could be\nfine-tuned to generate 360degree orbit frames surrounding an object given a\nsingle image. With our tailored reconstruction pipelines, we can generate\nhigh-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method\ncan be extended to scene-level novel view synthesis, achieving precise control\nover the camera path with sparse input views. Extensive experiments demonstrate\nthe superior performance of the proposed approach, especially in terms of\ngeneration quality and multi-view consistency. Our code is available at\nhttps://github.com/heheyas/V3D\n", "link": "http://arxiv.org/abs/2403.06738v1", "date": "2024-03-11", "relevancy": 2.604, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6803}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6345}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.619}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20V3D%3A%20Video%20Diffusion%20Models%20are%20Effective%203D%20Generators&body=Title%3A%20V3D%3A%20Video%20Diffusion%20Models%20are%20Effective%203D%20Generators%0AAuthor%3A%20Zilong%20Chen%20and%20Yikai%20Wang%20and%20Feng%20Wang%20and%20Zhengyi%20Wang%20and%20Huaping%20Liu%0AAbstract%3A%20%20%20Automatic%203D%20generation%20has%20recently%20attracted%20widespread%20attention.%20Recent%0Amethods%20have%20greatly%20accelerated%20the%20generation%20speed%2C%20but%20usually%20produce%0Aless-detailed%20objects%20due%20to%20limited%20model%20capacity%20or%203D%20data.%20Motivated%20by%0Arecent%20advancements%20in%20video%20diffusion%20models%2C%20we%20introduce%20V3D%2C%20which%0Aleverages%20the%20world%20simulation%20capacity%20of%20pre-trained%20video%20diffusion%20models%0Ato%20facilitate%203D%20generation.%20To%20fully%20unleash%20the%20potential%20of%20video%20diffusion%0Ato%20perceive%20the%203D%20world%2C%20we%20further%20introduce%20geometrical%20consistency%20prior%0Aand%20extend%20the%20video%20diffusion%20model%20to%20a%20multi-view%20consistent%203D%20generator.%0ABenefiting%20from%20this%2C%20the%20state-of-the-art%20video%20diffusion%20model%20could%20be%0Afine-tuned%20to%20generate%20360degree%20orbit%20frames%20surrounding%20an%20object%20given%20a%0Asingle%20image.%20With%20our%20tailored%20reconstruction%20pipelines%2C%20we%20can%20generate%0Ahigh-quality%20meshes%20or%203D%20Gaussians%20within%203%20minutes.%20Furthermore%2C%20our%20method%0Acan%20be%20extended%20to%20scene-level%20novel%20view%20synthesis%2C%20achieving%20precise%20control%0Aover%20the%20camera%20path%20with%20sparse%20input%20views.%20Extensive%20experiments%20demonstrate%0Athe%20superior%20performance%20of%20the%20proposed%20approach%2C%20especially%20in%20terms%20of%0Ageneration%20quality%20and%20multi-view%20consistency.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/heheyas/V3D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06738v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V3D%3A%20Video%20Diffusion%20Models%20are%20Effective%203D%20Generators&entry.906535625=Zilong%20Chen%20and%20Yikai%20Wang%20and%20Feng%20Wang%20and%20Zhengyi%20Wang%20and%20Huaping%20Liu&entry.1292438233=%20%20Automatic%203D%20generation%20has%20recently%20attracted%20widespread%20attention.%20Recent%0Amethods%20have%20greatly%20accelerated%20the%20generation%20speed%2C%20but%20usually%20produce%0Aless-detailed%20objects%20due%20to%20limited%20model%20capacity%20or%203D%20data.%20Motivated%20by%0Arecent%20advancements%20in%20video%20diffusion%20models%2C%20we%20introduce%20V3D%2C%20which%0Aleverages%20the%20world%20simulation%20capacity%20of%20pre-trained%20video%20diffusion%20models%0Ato%20facilitate%203D%20generation.%20To%20fully%20unleash%20the%20potential%20of%20video%20diffusion%0Ato%20perceive%20the%203D%20world%2C%20we%20further%20introduce%20geometrical%20consistency%20prior%0Aand%20extend%20the%20video%20diffusion%20model%20to%20a%20multi-view%20consistent%203D%20generator.%0ABenefiting%20from%20this%2C%20the%20state-of-the-art%20video%20diffusion%20model%20could%20be%0Afine-tuned%20to%20generate%20360degree%20orbit%20frames%20surrounding%20an%20object%20given%20a%0Asingle%20image.%20With%20our%20tailored%20reconstruction%20pipelines%2C%20we%20can%20generate%0Ahigh-quality%20meshes%20or%203D%20Gaussians%20within%203%20minutes.%20Furthermore%2C%20our%20method%0Acan%20be%20extended%20to%20scene-level%20novel%20view%20synthesis%2C%20achieving%20precise%20control%0Aover%20the%20camera%20path%20with%20sparse%20input%20views.%20Extensive%20experiments%20demonstrate%0Athe%20superior%20performance%20of%20the%20proposed%20approach%2C%20especially%20in%20terms%20of%0Ageneration%20quality%20and%20multi-view%20consistency.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/heheyas/V3D%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06738v1&entry.124074799=Read"},
{"title": "Dual-path Frequency Discriminators for Few-shot Anomaly Detection", "author": "Yuhu Bai and Jiangning Zhang and Yuhang Dong and Guanzhong Tian and Liang Liu and Yunkang Cao and Yabiao Wang and Chengjie Wang", "abstract": "  Few-shot anomaly detection (FSAD) is essential in industrial manufacturing.\nHowever, existing FSAD methods struggle to effectively leverage a limited\nnumber of normal samples, and they may fail to detect and locate inconspicuous\nanomalies in the spatial domain. We further discover that these subtle\nanomalies would be more noticeable in the frequency domain. In this paper, we\npropose a Dual-Path Frequency Discriminators (DFD) network from a frequency\nperspective to tackle these issues. Specifically, we generate anomalies at both\nimage-level and feature-level. Differential frequency components are extracted\nby the multi-frequency information construction module and supplied into the\nfine-grained feature construction module to provide adapted features. We\nconsider anomaly detection as a discriminative classification problem,\nwherefore the dual-path feature discrimination module is employed to detect and\nlocate the image-level and feature-level anomalies in the feature space. The\ndiscriminators aim to learn a joint representation of anomalous features and\nnormal features in the latent space. Extensive experiments conducted on MVTec\nAD and VisA benchmarks demonstrate that our DFD surpasses current\nstate-of-the-art methods. Source code will be available.\n", "link": "http://arxiv.org/abs/2403.04151v2", "date": "2024-03-11", "relevancy": 2.5967, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5422}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5078}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Dual-path%20Frequency%20Discriminators%20for%20Few-shot%20Anomaly%20Detection&body=Title%3A%20Dual-path%20Frequency%20Discriminators%20for%20Few-shot%20Anomaly%20Detection%0AAuthor%3A%20Yuhu%20Bai%20and%20Jiangning%20Zhang%20and%20Yuhang%20Dong%20and%20Guanzhong%20Tian%20and%20Liang%20Liu%20and%20Yunkang%20Cao%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Few-shot%20anomaly%20detection%20%28FSAD%29%20is%20essential%20in%20industrial%20manufacturing.%0AHowever%2C%20existing%20FSAD%20methods%20struggle%20to%20effectively%20leverage%20a%20limited%0Anumber%20of%20normal%20samples%2C%20and%20they%20may%20fail%20to%20detect%20and%20locate%20inconspicuous%0Aanomalies%20in%20the%20spatial%20domain.%20We%20further%20discover%20that%20these%20subtle%0Aanomalies%20would%20be%20more%20noticeable%20in%20the%20frequency%20domain.%20In%20this%20paper%2C%20we%0Apropose%20a%20Dual-Path%20Frequency%20Discriminators%20%28DFD%29%20network%20from%20a%20frequency%0Aperspective%20to%20tackle%20these%20issues.%20Specifically%2C%20we%20generate%20anomalies%20at%20both%0Aimage-level%20and%20feature-level.%20Differential%20frequency%20components%20are%20extracted%0Aby%20the%20multi-frequency%20information%20construction%20module%20and%20supplied%20into%20the%0Afine-grained%20feature%20construction%20module%20to%20provide%20adapted%20features.%20We%0Aconsider%20anomaly%20detection%20as%20a%20discriminative%20classification%20problem%2C%0Awherefore%20the%20dual-path%20feature%20discrimination%20module%20is%20employed%20to%20detect%20and%0Alocate%20the%20image-level%20and%20feature-level%20anomalies%20in%20the%20feature%20space.%20The%0Adiscriminators%20aim%20to%20learn%20a%20joint%20representation%20of%20anomalous%20features%20and%0Anormal%20features%20in%20the%20latent%20space.%20Extensive%20experiments%20conducted%20on%20MVTec%0AAD%20and%20VisA%20benchmarks%20demonstrate%20that%20our%20DFD%20surpasses%20current%0Astate-of-the-art%20methods.%20Source%20code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04151v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-path%20Frequency%20Discriminators%20for%20Few-shot%20Anomaly%20Detection&entry.906535625=Yuhu%20Bai%20and%20Jiangning%20Zhang%20and%20Yuhang%20Dong%20and%20Guanzhong%20Tian%20and%20Liang%20Liu%20and%20Yunkang%20Cao%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang&entry.1292438233=%20%20Few-shot%20anomaly%20detection%20%28FSAD%29%20is%20essential%20in%20industrial%20manufacturing.%0AHowever%2C%20existing%20FSAD%20methods%20struggle%20to%20effectively%20leverage%20a%20limited%0Anumber%20of%20normal%20samples%2C%20and%20they%20may%20fail%20to%20detect%20and%20locate%20inconspicuous%0Aanomalies%20in%20the%20spatial%20domain.%20We%20further%20discover%20that%20these%20subtle%0Aanomalies%20would%20be%20more%20noticeable%20in%20the%20frequency%20domain.%20In%20this%20paper%2C%20we%0Apropose%20a%20Dual-Path%20Frequency%20Discriminators%20%28DFD%29%20network%20from%20a%20frequency%0Aperspective%20to%20tackle%20these%20issues.%20Specifically%2C%20we%20generate%20anomalies%20at%20both%0Aimage-level%20and%20feature-level.%20Differential%20frequency%20components%20are%20extracted%0Aby%20the%20multi-frequency%20information%20construction%20module%20and%20supplied%20into%20the%0Afine-grained%20feature%20construction%20module%20to%20provide%20adapted%20features.%20We%0Aconsider%20anomaly%20detection%20as%20a%20discriminative%20classification%20problem%2C%0Awherefore%20the%20dual-path%20feature%20discrimination%20module%20is%20employed%20to%20detect%20and%0Alocate%20the%20image-level%20and%20feature-level%20anomalies%20in%20the%20feature%20space.%20The%0Adiscriminators%20aim%20to%20learn%20a%20joint%20representation%20of%20anomalous%20features%20and%0Anormal%20features%20in%20the%20latent%20space.%20Extensive%20experiments%20conducted%20on%20MVTec%0AAD%20and%20VisA%20benchmarks%20demonstrate%20that%20our%20DFD%20surpasses%20current%0Astate-of-the-art%20methods.%20Source%20code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04151v2&entry.124074799=Read"},
{"title": "Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape\n  Matching via Unsupervised Functional Map Regularized Reconstruction", "author": "Souhaib Attaiki and Maks Ovsjanikov", "abstract": "  We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for\nnon-rigid shape matching that eliminates the need for extensive training or\nground truth data. SNK operates on a single pair of shapes, and employs a\nreconstruction-based strategy using an encoder-decoder architecture, which\ndeforms the source shape to closely match the target shape. During the process,\nan unsupervised functional map is predicted and converted into a point-to-point\nmap, serving as a supervisory mechanism for the reconstruction. To aid in\ntraining, we have designed a new decoder architecture that generates smooth,\nrealistic deformations. SNK demonstrates competitive results on traditional\nbenchmarks, simplifying the shape-matching process without compromising\naccuracy. Our code can be found online: https://github.com/pvnieo/SNK\n", "link": "http://arxiv.org/abs/2403.06804v1", "date": "2024-03-11", "relevancy": 2.5699, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5165}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4874}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Shape%20Non-rigid%20Kinematics%20%28SNK%29%3A%20A%20Zero-Shot%20Method%20for%20Non-Rigid%20Shape%0A%20%20Matching%20via%20Unsupervised%20Functional%20Map%20Regularized%20Reconstruction&body=Title%3A%20Shape%20Non-rigid%20Kinematics%20%28SNK%29%3A%20A%20Zero-Shot%20Method%20for%20Non-Rigid%20Shape%0A%20%20Matching%20via%20Unsupervised%20Functional%20Map%20Regularized%20Reconstruction%0AAuthor%3A%20Souhaib%20Attaiki%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20We%20present%20Shape%20Non-rigid%20Kinematics%20%28SNK%29%2C%20a%20novel%20zero-shot%20method%20for%0Anon-rigid%20shape%20matching%20that%20eliminates%20the%20need%20for%20extensive%20training%20or%0Aground%20truth%20data.%20SNK%20operates%20on%20a%20single%20pair%20of%20shapes%2C%20and%20employs%20a%0Areconstruction-based%20strategy%20using%20an%20encoder-decoder%20architecture%2C%20which%0Adeforms%20the%20source%20shape%20to%20closely%20match%20the%20target%20shape.%20During%20the%20process%2C%0Aan%20unsupervised%20functional%20map%20is%20predicted%20and%20converted%20into%20a%20point-to-point%0Amap%2C%20serving%20as%20a%20supervisory%20mechanism%20for%20the%20reconstruction.%20To%20aid%20in%0Atraining%2C%20we%20have%20designed%20a%20new%20decoder%20architecture%20that%20generates%20smooth%2C%0Arealistic%20deformations.%20SNK%20demonstrates%20competitive%20results%20on%20traditional%0Abenchmarks%2C%20simplifying%20the%20shape-matching%20process%20without%20compromising%0Aaccuracy.%20Our%20code%20can%20be%20found%20online%3A%20https%3A//github.com/pvnieo/SNK%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06804v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape%20Non-rigid%20Kinematics%20%28SNK%29%3A%20A%20Zero-Shot%20Method%20for%20Non-Rigid%20Shape%0A%20%20Matching%20via%20Unsupervised%20Functional%20Map%20Regularized%20Reconstruction&entry.906535625=Souhaib%20Attaiki%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20We%20present%20Shape%20Non-rigid%20Kinematics%20%28SNK%29%2C%20a%20novel%20zero-shot%20method%20for%0Anon-rigid%20shape%20matching%20that%20eliminates%20the%20need%20for%20extensive%20training%20or%0Aground%20truth%20data.%20SNK%20operates%20on%20a%20single%20pair%20of%20shapes%2C%20and%20employs%20a%0Areconstruction-based%20strategy%20using%20an%20encoder-decoder%20architecture%2C%20which%0Adeforms%20the%20source%20shape%20to%20closely%20match%20the%20target%20shape.%20During%20the%20process%2C%0Aan%20unsupervised%20functional%20map%20is%20predicted%20and%20converted%20into%20a%20point-to-point%0Amap%2C%20serving%20as%20a%20supervisory%20mechanism%20for%20the%20reconstruction.%20To%20aid%20in%0Atraining%2C%20we%20have%20designed%20a%20new%20decoder%20architecture%20that%20generates%20smooth%2C%0Arealistic%20deformations.%20SNK%20demonstrates%20competitive%20results%20on%20traditional%0Abenchmarks%2C%20simplifying%20the%20shape-matching%20process%20without%20compromising%0Aaccuracy.%20Our%20code%20can%20be%20found%20online%3A%20https%3A//github.com/pvnieo/SNK%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06804v1&entry.124074799=Read"},
{"title": "MatchXML: An Efficient Text-label Matching Framework for Extreme\n  Multi-label Text Classification", "author": "Hui Ye and Rajshekhar Sunderraman and Shihao Ji", "abstract": "  The eXtreme Multi-label text Classification(XMC) refers to training a\nclassifier that assigns a text sample with relevant labels from an extremely\nlarge-scale label set (e.g., millions of labels). We propose MatchXML, an\nefficient text-label matching framework for XMC. We observe that the label\nembeddings generated from the sparse Term Frequency-Inverse Document\nFrequency(TF-IDF) features have several limitations. We thus propose label2vec\nto effectively train the semantic dense label embeddings by the Skip-gram\nmodel. The dense label embeddings are then used to build a Hierarchical Label\nTree by clustering. In fine-tuning the pre-trained encoder Transformer, we\nformulate the multi-label text classification as a text-label matching problem\nin a bipartite graph. We then extract the dense text representations from the\nfine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also\nextract the static dense sentence embeddings from a pre-trained Sentence\nTransformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF\nfeatures, the fine-tuned dense text representations and static dense sentence\nfeatures. Experimental results demonstrate that MatchXML achieves\nstate-of-the-art accuracy on five out of six datasets. As for the speed,\nMatchXML outperforms the competing methods on all the six datasets. Our source\ncode is publicly available at https://github.com/huiyegit/MatchXML.\n", "link": "http://arxiv.org/abs/2308.13139v2", "date": "2024-03-11", "relevancy": 2.5625, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5078}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MatchXML%3A%20An%20Efficient%20Text-label%20Matching%20Framework%20for%20Extreme%0A%20%20Multi-label%20Text%20Classification&body=Title%3A%20MatchXML%3A%20An%20Efficient%20Text-label%20Matching%20Framework%20for%20Extreme%0A%20%20Multi-label%20Text%20Classification%0AAuthor%3A%20Hui%20Ye%20and%20Rajshekhar%20Sunderraman%20and%20Shihao%20Ji%0AAbstract%3A%20%20%20The%20eXtreme%20Multi-label%20text%20Classification%28XMC%29%20refers%20to%20training%20a%0Aclassifier%20that%20assigns%20a%20text%20sample%20with%20relevant%20labels%20from%20an%20extremely%0Alarge-scale%20label%20set%20%28e.g.%2C%20millions%20of%20labels%29.%20We%20propose%20MatchXML%2C%20an%0Aefficient%20text-label%20matching%20framework%20for%20XMC.%20We%20observe%20that%20the%20label%0Aembeddings%20generated%20from%20the%20sparse%20Term%20Frequency-Inverse%20Document%0AFrequency%28TF-IDF%29%20features%20have%20several%20limitations.%20We%20thus%20propose%20label2vec%0Ato%20effectively%20train%20the%20semantic%20dense%20label%20embeddings%20by%20the%20Skip-gram%0Amodel.%20The%20dense%20label%20embeddings%20are%20then%20used%20to%20build%20a%20Hierarchical%20Label%0ATree%20by%20clustering.%20In%20fine-tuning%20the%20pre-trained%20encoder%20Transformer%2C%20we%0Aformulate%20the%20multi-label%20text%20classification%20as%20a%20text-label%20matching%20problem%0Ain%20a%20bipartite%20graph.%20We%20then%20extract%20the%20dense%20text%20representations%20from%20the%0Afine-tuned%20Transformer.%20Besides%20the%20fine-tuned%20dense%20text%20embeddings%2C%20we%20also%0Aextract%20the%20static%20dense%20sentence%20embeddings%20from%20a%20pre-trained%20Sentence%0ATransformer.%20Finally%2C%20a%20linear%20ranker%20is%20trained%20by%20utilizing%20the%20sparse%20TF-IDF%0Afeatures%2C%20the%20fine-tuned%20dense%20text%20representations%20and%20static%20dense%20sentence%0Afeatures.%20Experimental%20results%20demonstrate%20that%20MatchXML%20achieves%0Astate-of-the-art%20accuracy%20on%20five%20out%20of%20six%20datasets.%20As%20for%20the%20speed%2C%0AMatchXML%20outperforms%20the%20competing%20methods%20on%20all%20the%20six%20datasets.%20Our%20source%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/huiyegit/MatchXML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13139v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatchXML%3A%20An%20Efficient%20Text-label%20Matching%20Framework%20for%20Extreme%0A%20%20Multi-label%20Text%20Classification&entry.906535625=Hui%20Ye%20and%20Rajshekhar%20Sunderraman%20and%20Shihao%20Ji&entry.1292438233=%20%20The%20eXtreme%20Multi-label%20text%20Classification%28XMC%29%20refers%20to%20training%20a%0Aclassifier%20that%20assigns%20a%20text%20sample%20with%20relevant%20labels%20from%20an%20extremely%0Alarge-scale%20label%20set%20%28e.g.%2C%20millions%20of%20labels%29.%20We%20propose%20MatchXML%2C%20an%0Aefficient%20text-label%20matching%20framework%20for%20XMC.%20We%20observe%20that%20the%20label%0Aembeddings%20generated%20from%20the%20sparse%20Term%20Frequency-Inverse%20Document%0AFrequency%28TF-IDF%29%20features%20have%20several%20limitations.%20We%20thus%20propose%20label2vec%0Ato%20effectively%20train%20the%20semantic%20dense%20label%20embeddings%20by%20the%20Skip-gram%0Amodel.%20The%20dense%20label%20embeddings%20are%20then%20used%20to%20build%20a%20Hierarchical%20Label%0ATree%20by%20clustering.%20In%20fine-tuning%20the%20pre-trained%20encoder%20Transformer%2C%20we%0Aformulate%20the%20multi-label%20text%20classification%20as%20a%20text-label%20matching%20problem%0Ain%20a%20bipartite%20graph.%20We%20then%20extract%20the%20dense%20text%20representations%20from%20the%0Afine-tuned%20Transformer.%20Besides%20the%20fine-tuned%20dense%20text%20embeddings%2C%20we%20also%0Aextract%20the%20static%20dense%20sentence%20embeddings%20from%20a%20pre-trained%20Sentence%0ATransformer.%20Finally%2C%20a%20linear%20ranker%20is%20trained%20by%20utilizing%20the%20sparse%20TF-IDF%0Afeatures%2C%20the%20fine-tuned%20dense%20text%20representations%20and%20static%20dense%20sentence%0Afeatures.%20Experimental%20results%20demonstrate%20that%20MatchXML%20achieves%0Astate-of-the-art%20accuracy%20on%20five%20out%20of%20six%20datasets.%20As%20for%20the%20speed%2C%0AMatchXML%20outperforms%20the%20competing%20methods%20on%20all%20the%20six%20datasets.%20Our%20source%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/huiyegit/MatchXML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13139v2&entry.124074799=Read"},
{"title": "On the Generalization Ability of Unsupervised Pretraining", "author": "Yuyang Deng and Junyuan Hong and Jiayu Zhou and Mehrdad Mahdavi", "abstract": "  Recent advances in unsupervised learning have shown that unsupervised\npre-training, followed by fine-tuning, can improve model generalization.\nHowever, a rigorous understanding of how the representation function learned on\nan unlabeled dataset affects the generalization of the fine-tuned model is\nlacking. Existing theoretical research does not adequately account for the\nheterogeneity of the distribution and tasks in pre-training and fine-tuning\nstage. To bridge this gap, this paper introduces a novel theoretical framework\nthat illuminates the critical factor influencing the transferability of\nknowledge acquired during unsupervised pre-training to the subsequent\nfine-tuning phase, ultimately affecting the generalization capabilities of the\nfine-tuned model on downstream tasks. We apply our theoretical framework to\nanalyze generalization bound of two distinct scenarios: Context Encoder\npre-training with deep neural networks and Masked Autoencoder pre-training with\ndeep transformers, followed by fine-tuning on a binary classification task.\nFinally, inspired by our findings, we propose a novel regularization method\nduring pre-training to further enhances the generalization of fine-tuned model.\nOverall, our results contribute to a better understanding of unsupervised\npre-training and fine-tuning paradigm, and can shed light on the design of more\neffective pre-training algorithms.\n", "link": "http://arxiv.org/abs/2403.06871v1", "date": "2024-03-11", "relevancy": 2.5366, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5243}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5007}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20On%20the%20Generalization%20Ability%20of%20Unsupervised%20Pretraining&body=Title%3A%20On%20the%20Generalization%20Ability%20of%20Unsupervised%20Pretraining%0AAuthor%3A%20Yuyang%20Deng%20and%20Junyuan%20Hong%20and%20Jiayu%20Zhou%20and%20Mehrdad%20Mahdavi%0AAbstract%3A%20%20%20Recent%20advances%20in%20unsupervised%20learning%20have%20shown%20that%20unsupervised%0Apre-training%2C%20followed%20by%20fine-tuning%2C%20can%20improve%20model%20generalization.%0AHowever%2C%20a%20rigorous%20understanding%20of%20how%20the%20representation%20function%20learned%20on%0Aan%20unlabeled%20dataset%20affects%20the%20generalization%20of%20the%20fine-tuned%20model%20is%0Alacking.%20Existing%20theoretical%20research%20does%20not%20adequately%20account%20for%20the%0Aheterogeneity%20of%20the%20distribution%20and%20tasks%20in%20pre-training%20and%20fine-tuning%0Astage.%20To%20bridge%20this%20gap%2C%20this%20paper%20introduces%20a%20novel%20theoretical%20framework%0Athat%20illuminates%20the%20critical%20factor%20influencing%20the%20transferability%20of%0Aknowledge%20acquired%20during%20unsupervised%20pre-training%20to%20the%20subsequent%0Afine-tuning%20phase%2C%20ultimately%20affecting%20the%20generalization%20capabilities%20of%20the%0Afine-tuned%20model%20on%20downstream%20tasks.%20We%20apply%20our%20theoretical%20framework%20to%0Aanalyze%20generalization%20bound%20of%20two%20distinct%20scenarios%3A%20Context%20Encoder%0Apre-training%20with%20deep%20neural%20networks%20and%20Masked%20Autoencoder%20pre-training%20with%0Adeep%20transformers%2C%20followed%20by%20fine-tuning%20on%20a%20binary%20classification%20task.%0AFinally%2C%20inspired%20by%20our%20findings%2C%20we%20propose%20a%20novel%20regularization%20method%0Aduring%20pre-training%20to%20further%20enhances%20the%20generalization%20of%20fine-tuned%20model.%0AOverall%2C%20our%20results%20contribute%20to%20a%20better%20understanding%20of%20unsupervised%0Apre-training%20and%20fine-tuning%20paradigm%2C%20and%20can%20shed%20light%20on%20the%20design%20of%20more%0Aeffective%20pre-training%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06871v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Generalization%20Ability%20of%20Unsupervised%20Pretraining&entry.906535625=Yuyang%20Deng%20and%20Junyuan%20Hong%20and%20Jiayu%20Zhou%20and%20Mehrdad%20Mahdavi&entry.1292438233=%20%20Recent%20advances%20in%20unsupervised%20learning%20have%20shown%20that%20unsupervised%0Apre-training%2C%20followed%20by%20fine-tuning%2C%20can%20improve%20model%20generalization.%0AHowever%2C%20a%20rigorous%20understanding%20of%20how%20the%20representation%20function%20learned%20on%0Aan%20unlabeled%20dataset%20affects%20the%20generalization%20of%20the%20fine-tuned%20model%20is%0Alacking.%20Existing%20theoretical%20research%20does%20not%20adequately%20account%20for%20the%0Aheterogeneity%20of%20the%20distribution%20and%20tasks%20in%20pre-training%20and%20fine-tuning%0Astage.%20To%20bridge%20this%20gap%2C%20this%20paper%20introduces%20a%20novel%20theoretical%20framework%0Athat%20illuminates%20the%20critical%20factor%20influencing%20the%20transferability%20of%0Aknowledge%20acquired%20during%20unsupervised%20pre-training%20to%20the%20subsequent%0Afine-tuning%20phase%2C%20ultimately%20affecting%20the%20generalization%20capabilities%20of%20the%0Afine-tuned%20model%20on%20downstream%20tasks.%20We%20apply%20our%20theoretical%20framework%20to%0Aanalyze%20generalization%20bound%20of%20two%20distinct%20scenarios%3A%20Context%20Encoder%0Apre-training%20with%20deep%20neural%20networks%20and%20Masked%20Autoencoder%20pre-training%20with%0Adeep%20transformers%2C%20followed%20by%20fine-tuning%20on%20a%20binary%20classification%20task.%0AFinally%2C%20inspired%20by%20our%20findings%2C%20we%20propose%20a%20novel%20regularization%20method%0Aduring%20pre-training%20to%20further%20enhances%20the%20generalization%20of%20fine-tuned%20model.%0AOverall%2C%20our%20results%20contribute%20to%20a%20better%20understanding%20of%20unsupervised%0Apre-training%20and%20fine-tuning%20paradigm%2C%20and%20can%20shed%20light%20on%20the%20design%20of%20more%0Aeffective%20pre-training%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06871v1&entry.124074799=Read"},
{"title": "NeuPAN: Direct Point Robot Navigation with End-to-End Model-based\n  Learning", "author": "Ruihua Han and Shuai Wang and Shuaijun Wang and Zeqing Zhang and Jianjun Chen and Shijie Lin and Chengyang Li and Chengzhong Xu and Yonina C. Eldar and Qi Hao and Jia Pan", "abstract": "  Navigating a nonholonomic robot in a cluttered environment requires extremely\naccurate perception and locomotion for collision avoidance. This paper presents\nNeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and\nenvironment-invariant robot navigation solution. Leveraging a tightly-coupled\nperception-locomotion framework, NeuPAN has two key innovations compared to\nexisting approaches: 1) it directly maps raw points to a learned multi-frame\ndistance space, avoiding error propagation from perception to control; 2) it is\ninterpretable from an end-to-end model-based learning perspective, enabling\nprovable convergence. The crux of NeuPAN is to solve a high-dimensional\nend-to-end mathematical model with various point-level constraints using the\nplug-and-play (PnP) proximal alternating-minimization network (PAN) with\nneurons in the loop. This allows NeuPAN to generate real-time, end-to-end,\nphysically-interpretable motions directly from point clouds, which seamlessly\nintegrates data- and knowledge-engines, where its network parameters are\nadjusted via back propagation. We evaluate NeuPAN on car-like robot,\nwheel-legged robot, and passenger autonomous vehicle, in both simulated and\nreal-world environments. Experiments demonstrate that NeuPAN outperforms\nvarious benchmarks, in terms of accuracy, efficiency, robustness, and\ngeneralization capability across various environments, including the cluttered\nsandbox, office, corridor, and parking lot. We show that NeuPAN works well in\nunstructured environments with arbitrary-shape undetectable objects, making\nimpassable ways passable.\n", "link": "http://arxiv.org/abs/2403.06828v1", "date": "2024-03-11", "relevancy": 2.5352, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6702}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6104}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6068}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20NeuPAN%3A%20Direct%20Point%20Robot%20Navigation%20with%20End-to-End%20Model-based%0A%20%20Learning&body=Title%3A%20NeuPAN%3A%20Direct%20Point%20Robot%20Navigation%20with%20End-to-End%20Model-based%0A%20%20Learning%0AAuthor%3A%20Ruihua%20Han%20and%20Shuai%20Wang%20and%20Shuaijun%20Wang%20and%20Zeqing%20Zhang%20and%20Jianjun%20Chen%20and%20Shijie%20Lin%20and%20Chengyang%20Li%20and%20Chengzhong%20Xu%20and%20Yonina%20C.%20Eldar%20and%20Qi%20Hao%20and%20Jia%20Pan%0AAbstract%3A%20%20%20Navigating%20a%20nonholonomic%20robot%20in%20a%20cluttered%20environment%20requires%20extremely%0Aaccurate%20perception%20and%20locomotion%20for%20collision%20avoidance.%20This%20paper%20presents%0ANeuPAN%3A%20a%20real-time%2C%20highly-accurate%2C%20map-free%2C%20robot-agnostic%2C%20and%0Aenvironment-invariant%20robot%20navigation%20solution.%20Leveraging%20a%20tightly-coupled%0Aperception-locomotion%20framework%2C%20NeuPAN%20has%20two%20key%20innovations%20compared%20to%0Aexisting%20approaches%3A%201%29%20it%20directly%20maps%20raw%20points%20to%20a%20learned%20multi-frame%0Adistance%20space%2C%20avoiding%20error%20propagation%20from%20perception%20to%20control%3B%202%29%20it%20is%0Ainterpretable%20from%20an%20end-to-end%20model-based%20learning%20perspective%2C%20enabling%0Aprovable%20convergence.%20The%20crux%20of%20NeuPAN%20is%20to%20solve%20a%20high-dimensional%0Aend-to-end%20mathematical%20model%20with%20various%20point-level%20constraints%20using%20the%0Aplug-and-play%20%28PnP%29%20proximal%20alternating-minimization%20network%20%28PAN%29%20with%0Aneurons%20in%20the%20loop.%20This%20allows%20NeuPAN%20to%20generate%20real-time%2C%20end-to-end%2C%0Aphysically-interpretable%20motions%20directly%20from%20point%20clouds%2C%20which%20seamlessly%0Aintegrates%20data-%20and%20knowledge-engines%2C%20where%20its%20network%20parameters%20are%0Aadjusted%20via%20back%20propagation.%20We%20evaluate%20NeuPAN%20on%20car-like%20robot%2C%0Awheel-legged%20robot%2C%20and%20passenger%20autonomous%20vehicle%2C%20in%20both%20simulated%20and%0Areal-world%20environments.%20Experiments%20demonstrate%20that%20NeuPAN%20outperforms%0Avarious%20benchmarks%2C%20in%20terms%20of%20accuracy%2C%20efficiency%2C%20robustness%2C%20and%0Ageneralization%20capability%20across%20various%20environments%2C%20including%20the%20cluttered%0Asandbox%2C%20office%2C%20corridor%2C%20and%20parking%20lot.%20We%20show%20that%20NeuPAN%20works%20well%20in%0Aunstructured%20environments%20with%20arbitrary-shape%20undetectable%20objects%2C%20making%0Aimpassable%20ways%20passable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06828v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuPAN%3A%20Direct%20Point%20Robot%20Navigation%20with%20End-to-End%20Model-based%0A%20%20Learning&entry.906535625=Ruihua%20Han%20and%20Shuai%20Wang%20and%20Shuaijun%20Wang%20and%20Zeqing%20Zhang%20and%20Jianjun%20Chen%20and%20Shijie%20Lin%20and%20Chengyang%20Li%20and%20Chengzhong%20Xu%20and%20Yonina%20C.%20Eldar%20and%20Qi%20Hao%20and%20Jia%20Pan&entry.1292438233=%20%20Navigating%20a%20nonholonomic%20robot%20in%20a%20cluttered%20environment%20requires%20extremely%0Aaccurate%20perception%20and%20locomotion%20for%20collision%20avoidance.%20This%20paper%20presents%0ANeuPAN%3A%20a%20real-time%2C%20highly-accurate%2C%20map-free%2C%20robot-agnostic%2C%20and%0Aenvironment-invariant%20robot%20navigation%20solution.%20Leveraging%20a%20tightly-coupled%0Aperception-locomotion%20framework%2C%20NeuPAN%20has%20two%20key%20innovations%20compared%20to%0Aexisting%20approaches%3A%201%29%20it%20directly%20maps%20raw%20points%20to%20a%20learned%20multi-frame%0Adistance%20space%2C%20avoiding%20error%20propagation%20from%20perception%20to%20control%3B%202%29%20it%20is%0Ainterpretable%20from%20an%20end-to-end%20model-based%20learning%20perspective%2C%20enabling%0Aprovable%20convergence.%20The%20crux%20of%20NeuPAN%20is%20to%20solve%20a%20high-dimensional%0Aend-to-end%20mathematical%20model%20with%20various%20point-level%20constraints%20using%20the%0Aplug-and-play%20%28PnP%29%20proximal%20alternating-minimization%20network%20%28PAN%29%20with%0Aneurons%20in%20the%20loop.%20This%20allows%20NeuPAN%20to%20generate%20real-time%2C%20end-to-end%2C%0Aphysically-interpretable%20motions%20directly%20from%20point%20clouds%2C%20which%20seamlessly%0Aintegrates%20data-%20and%20knowledge-engines%2C%20where%20its%20network%20parameters%20are%0Aadjusted%20via%20back%20propagation.%20We%20evaluate%20NeuPAN%20on%20car-like%20robot%2C%0Awheel-legged%20robot%2C%20and%20passenger%20autonomous%20vehicle%2C%20in%20both%20simulated%20and%0Areal-world%20environments.%20Experiments%20demonstrate%20that%20NeuPAN%20outperforms%0Avarious%20benchmarks%2C%20in%20terms%20of%20accuracy%2C%20efficiency%2C%20robustness%2C%20and%0Ageneralization%20capability%20across%20various%20environments%2C%20including%20the%20cluttered%0Asandbox%2C%20office%2C%20corridor%2C%20and%20parking%20lot.%20We%20show%20that%20NeuPAN%20works%20well%20in%0Aunstructured%20environments%20with%20arbitrary-shape%20undetectable%20objects%2C%20making%0Aimpassable%20ways%20passable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06828v1&entry.124074799=Read"},
{"title": "A transductive few-shot learning approach for classification of digital\n  histopathological slides from liver cancer", "author": "Aymen Sadraoui and S\u00e9gol\u00e8ne Martin and Eliott Barbot and Astrid Laurent-Bellue and Jean-Christophe Pesquet and Catherine Guettier and Ismail Ben Ayed", "abstract": "  This paper presents a new approach for classifying 2D histopathology patches\nusing few-shot learning. The method is designed to tackle a significant\nchallenge in histopathology, which is the limited availability of labeled data.\nBy applying a sliding window technique to histopathology slides, we illustrate\nthe practical benefits of transductive learning (i.e., making joint predictions\non patches) to achieve consistent and accurate classification. Our approach\ninvolves an optimization-based strategy that actively penalizes the prediction\nof a large number of distinct classes within each window. We conducted\nexperiments on histopathological data to classify tissue classes in digital\nslides of liver cancer, specifically hepatocellular carcinoma. The initial\nresults show the effectiveness of our method and its potential to enhance the\nprocess of automated cancer diagnosis and treatment, all while reducing the\ntime and effort required for expert annotation.\n", "link": "http://arxiv.org/abs/2311.17740v2", "date": "2024-03-11", "relevancy": 2.516, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.479}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20transductive%20few-shot%20learning%20approach%20for%20classification%20of%20digital%0A%20%20histopathological%20slides%20from%20liver%20cancer&body=Title%3A%20A%20transductive%20few-shot%20learning%20approach%20for%20classification%20of%20digital%0A%20%20histopathological%20slides%20from%20liver%20cancer%0AAuthor%3A%20Aymen%20Sadraoui%20and%20S%C3%A9gol%C3%A8ne%20Martin%20and%20Eliott%20Barbot%20and%20Astrid%20Laurent-Bellue%20and%20Jean-Christophe%20Pesquet%20and%20Catherine%20Guettier%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20for%20classifying%202D%20histopathology%20patches%0Ausing%20few-shot%20learning.%20The%20method%20is%20designed%20to%20tackle%20a%20significant%0Achallenge%20in%20histopathology%2C%20which%20is%20the%20limited%20availability%20of%20labeled%20data.%0ABy%20applying%20a%20sliding%20window%20technique%20to%20histopathology%20slides%2C%20we%20illustrate%0Athe%20practical%20benefits%20of%20transductive%20learning%20%28i.e.%2C%20making%20joint%20predictions%0Aon%20patches%29%20to%20achieve%20consistent%20and%20accurate%20classification.%20Our%20approach%0Ainvolves%20an%20optimization-based%20strategy%20that%20actively%20penalizes%20the%20prediction%0Aof%20a%20large%20number%20of%20distinct%20classes%20within%20each%20window.%20We%20conducted%0Aexperiments%20on%20histopathological%20data%20to%20classify%20tissue%20classes%20in%20digital%0Aslides%20of%20liver%20cancer%2C%20specifically%20hepatocellular%20carcinoma.%20The%20initial%0Aresults%20show%20the%20effectiveness%20of%20our%20method%20and%20its%20potential%20to%20enhance%20the%0Aprocess%20of%20automated%20cancer%20diagnosis%20and%20treatment%2C%20all%20while%20reducing%20the%0Atime%20and%20effort%20required%20for%20expert%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17740v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20transductive%20few-shot%20learning%20approach%20for%20classification%20of%20digital%0A%20%20histopathological%20slides%20from%20liver%20cancer&entry.906535625=Aymen%20Sadraoui%20and%20S%C3%A9gol%C3%A8ne%20Martin%20and%20Eliott%20Barbot%20and%20Astrid%20Laurent-Bellue%20and%20Jean-Christophe%20Pesquet%20and%20Catherine%20Guettier%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20for%20classifying%202D%20histopathology%20patches%0Ausing%20few-shot%20learning.%20The%20method%20is%20designed%20to%20tackle%20a%20significant%0Achallenge%20in%20histopathology%2C%20which%20is%20the%20limited%20availability%20of%20labeled%20data.%0ABy%20applying%20a%20sliding%20window%20technique%20to%20histopathology%20slides%2C%20we%20illustrate%0Athe%20practical%20benefits%20of%20transductive%20learning%20%28i.e.%2C%20making%20joint%20predictions%0Aon%20patches%29%20to%20achieve%20consistent%20and%20accurate%20classification.%20Our%20approach%0Ainvolves%20an%20optimization-based%20strategy%20that%20actively%20penalizes%20the%20prediction%0Aof%20a%20large%20number%20of%20distinct%20classes%20within%20each%20window.%20We%20conducted%0Aexperiments%20on%20histopathological%20data%20to%20classify%20tissue%20classes%20in%20digital%0Aslides%20of%20liver%20cancer%2C%20specifically%20hepatocellular%20carcinoma.%20The%20initial%0Aresults%20show%20the%20effectiveness%20of%20our%20method%20and%20its%20potential%20to%20enhance%20the%0Aprocess%20of%20automated%20cancer%20diagnosis%20and%20treatment%2C%20all%20while%20reducing%20the%0Atime%20and%20effort%20required%20for%20expert%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17740v2&entry.124074799=Read"},
{"title": "Shortcut Learning in Medical Image Segmentation", "author": "Manxi Lin and Nina Weng and Kamil Mikolaj and Zahra Bashir and Morten Bo S\u00f8ndergaard Svendsen and Martin Tolsgaard and Anders Nymark Christensen and Aasa Feragen", "abstract": "  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation.\n", "link": "http://arxiv.org/abs/2403.06748v1", "date": "2024-03-11", "relevancy": 2.5103, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5117}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4795}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Shortcut%20Learning%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Shortcut%20Learning%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Manxi%20Lin%20and%20Nina%20Weng%20and%20Kamil%20Mikolaj%20and%20Zahra%20Bashir%20and%20Morten%20Bo%20S%C3%B8ndergaard%20Svendsen%20and%20Martin%20Tolsgaard%20and%20Anders%20Nymark%20Christensen%20and%20Aasa%20Feragen%0AAbstract%3A%20%20%20Shortcut%20learning%20is%20a%20phenomenon%20where%20machine%20learning%20models%20prioritize%0Alearning%20simple%2C%20potentially%20misleading%20cues%20from%20data%20that%20do%20not%20generalize%0Awell%20beyond%20the%20training%20set.%20While%20existing%20research%20primarily%20investigates%0Athis%20in%20the%20realm%20of%20image%20classification%2C%20this%20study%20extends%20the%20exploration%0Aof%20shortcut%20learning%20into%20medical%20image%20segmentation.%20We%20demonstrate%20that%0Aclinical%20annotations%20such%20as%20calipers%2C%20and%20the%20combination%20of%20zero-padded%0Aconvolutions%20and%20center-cropped%20training%20sets%20in%20the%20dataset%20can%20inadvertently%0Aserve%20as%20shortcuts%2C%20impacting%20segmentation%20accuracy.%20We%20identify%20and%20evaluate%0Athe%20shortcut%20learning%20on%20two%20different%20but%20common%20medical%20image%20segmentation%0Atasks.%20In%20addition%2C%20we%20suggest%20strategies%20to%20mitigate%20the%20influence%20of%20shortcut%0Alearning%20and%20improve%20the%20generalizability%20of%20the%20segmentation%20models.%20By%0Auncovering%20the%20presence%20and%20implications%20of%20shortcuts%20in%20medical%20image%0Asegmentation%2C%20we%20provide%20insights%20and%20methodologies%20for%20evaluating%20and%0Aovercoming%20this%20pervasive%20challenge%20and%20call%20for%20attention%20in%20the%20community%20for%0Ashortcuts%20in%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06748v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shortcut%20Learning%20in%20Medical%20Image%20Segmentation&entry.906535625=Manxi%20Lin%20and%20Nina%20Weng%20and%20Kamil%20Mikolaj%20and%20Zahra%20Bashir%20and%20Morten%20Bo%20S%C3%B8ndergaard%20Svendsen%20and%20Martin%20Tolsgaard%20and%20Anders%20Nymark%20Christensen%20and%20Aasa%20Feragen&entry.1292438233=%20%20Shortcut%20learning%20is%20a%20phenomenon%20where%20machine%20learning%20models%20prioritize%0Alearning%20simple%2C%20potentially%20misleading%20cues%20from%20data%20that%20do%20not%20generalize%0Awell%20beyond%20the%20training%20set.%20While%20existing%20research%20primarily%20investigates%0Athis%20in%20the%20realm%20of%20image%20classification%2C%20this%20study%20extends%20the%20exploration%0Aof%20shortcut%20learning%20into%20medical%20image%20segmentation.%20We%20demonstrate%20that%0Aclinical%20annotations%20such%20as%20calipers%2C%20and%20the%20combination%20of%20zero-padded%0Aconvolutions%20and%20center-cropped%20training%20sets%20in%20the%20dataset%20can%20inadvertently%0Aserve%20as%20shortcuts%2C%20impacting%20segmentation%20accuracy.%20We%20identify%20and%20evaluate%0Athe%20shortcut%20learning%20on%20two%20different%20but%20common%20medical%20image%20segmentation%0Atasks.%20In%20addition%2C%20we%20suggest%20strategies%20to%20mitigate%20the%20influence%20of%20shortcut%0Alearning%20and%20improve%20the%20generalizability%20of%20the%20segmentation%20models.%20By%0Auncovering%20the%20presence%20and%20implications%20of%20shortcuts%20in%20medical%20image%0Asegmentation%2C%20we%20provide%20insights%20and%20methodologies%20for%20evaluating%20and%0Aovercoming%20this%20pervasive%20challenge%20and%20call%20for%20attention%20in%20the%20community%20for%0Ashortcuts%20in%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06748v1&entry.124074799=Read"},
{"title": "Deep Classifier Mimicry without Data Access", "author": "Steven Braun and Martin Mundt and Kristian Kersting", "abstract": "  Access to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.\n", "link": "http://arxiv.org/abs/2306.02090v2", "date": "2024-03-11", "relevancy": 2.4929, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5123}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4893}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Deep%20Classifier%20Mimicry%20without%20Data%20Access&body=Title%3A%20Deep%20Classifier%20Mimicry%20without%20Data%20Access%0AAuthor%3A%20Steven%20Braun%20and%20Martin%20Mundt%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Access%20to%20pre-trained%20models%20has%20recently%20emerged%20as%20a%20standard%20across%0Anumerous%20machine%20learning%20domains.%20Unfortunately%2C%20access%20to%20the%20original%20data%0Athe%20models%20were%20trained%20on%20may%20not%20equally%20be%20granted.%20This%20makes%20it%0Atremendously%20challenging%20to%20fine-tune%2C%20compress%20models%2C%20adapt%20continually%2C%20or%0Ato%20do%20any%20other%20type%20of%20data-driven%20update.%20We%20posit%20that%20original%20data%20access%0Amay%20however%20not%20be%20required.%20Specifically%2C%20we%20propose%20Contrastive%20Abductive%0AKnowledge%20Extraction%20%28CAKE%29%2C%20a%20model-agnostic%20knowledge%20distillation%20procedure%0Athat%20mimics%20deep%20classifiers%20without%20access%20to%20the%20original%20data.%20To%20this%20end%2C%0ACAKE%20generates%20pairs%20of%20noisy%20synthetic%20samples%20and%20diffuses%20them%20contrastively%0Atoward%20a%20model%27s%20decision%20boundary.%20We%20empirically%20corroborate%20CAKE%27s%0Aeffectiveness%20using%20several%20benchmark%20datasets%20and%20various%20architectural%0Achoices%2C%20paving%20the%20way%20for%20broad%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02090v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Classifier%20Mimicry%20without%20Data%20Access&entry.906535625=Steven%20Braun%20and%20Martin%20Mundt%20and%20Kristian%20Kersting&entry.1292438233=%20%20Access%20to%20pre-trained%20models%20has%20recently%20emerged%20as%20a%20standard%20across%0Anumerous%20machine%20learning%20domains.%20Unfortunately%2C%20access%20to%20the%20original%20data%0Athe%20models%20were%20trained%20on%20may%20not%20equally%20be%20granted.%20This%20makes%20it%0Atremendously%20challenging%20to%20fine-tune%2C%20compress%20models%2C%20adapt%20continually%2C%20or%0Ato%20do%20any%20other%20type%20of%20data-driven%20update.%20We%20posit%20that%20original%20data%20access%0Amay%20however%20not%20be%20required.%20Specifically%2C%20we%20propose%20Contrastive%20Abductive%0AKnowledge%20Extraction%20%28CAKE%29%2C%20a%20model-agnostic%20knowledge%20distillation%20procedure%0Athat%20mimics%20deep%20classifiers%20without%20access%20to%20the%20original%20data.%20To%20this%20end%2C%0ACAKE%20generates%20pairs%20of%20noisy%20synthetic%20samples%20and%20diffuses%20them%20contrastively%0Atoward%20a%20model%27s%20decision%20boundary.%20We%20empirically%20corroborate%20CAKE%27s%0Aeffectiveness%20using%20several%20benchmark%20datasets%20and%20various%20architectural%0Achoices%2C%20paving%20the%20way%20for%20broad%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02090v2&entry.124074799=Read"},
{"title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation\n  Extraction", "author": "Da Luo and Yanglei Gan and Rui Hou and Run Lin and Qiao Liu and Yuxiang Cai and Wannian Gao", "abstract": "  Few-shot Relation Extraction (FSRE) aims to extract relational facts from a\nsparse set of labeled corpora. Recent studies have shown promising results in\nFSRE by employing Pre-trained Language Models (PLMs) within the framework of\nsupervised contrastive learning, which considers both instances and label\nfacts. However, how to effectively harness massive instance-label pairs to\nencompass the learned representation with semantic richness in this learning\nparadigm is not fully explored. To address this gap, we introduce a novel\nsynergistic anchored contrastive pre-training framework. This framework is\nmotivated by the insight that the diverse viewpoints conveyed through\ninstance-label pairs capture incomplete yet complementary intrinsic textual\nsemantics. Specifically, our framework involves a symmetrical contrastive\nobjective that encompasses both sentence-anchored and label-anchored\ncontrastive losses. By combining these two losses, the model establishes a\nrobust and uniform representation space. This space effectively captures the\nreciprocal alignment of feature distributions among instances and relational\nfacts, simultaneously enhancing the maximization of mutual information across\ndiverse perspectives within the same relation. Experimental results demonstrate\nthat our framework achieves significant performance enhancements compared to\nbaseline models in downstream FSRE tasks. Furthermore, our approach exhibits\nsuperior adaptability to handle the challenges of domain shift and zero-shot\nrelation extraction. Our code is available online at\nhttps://github.com/AONE-NLP/FSRE-SaCon.\n", "link": "http://arxiv.org/abs/2312.12021v3", "date": "2024-03-11", "relevancy": 2.4904, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5045}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4943}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Synergistic%20Anchored%20Contrastive%20Pre-training%20for%20Few-Shot%20Relation%0A%20%20Extraction&body=Title%3A%20Synergistic%20Anchored%20Contrastive%20Pre-training%20for%20Few-Shot%20Relation%0A%20%20Extraction%0AAuthor%3A%20Da%20Luo%20and%20Yanglei%20Gan%20and%20Rui%20Hou%20and%20Run%20Lin%20and%20Qiao%20Liu%20and%20Yuxiang%20Cai%20and%20Wannian%20Gao%0AAbstract%3A%20%20%20Few-shot%20Relation%20Extraction%20%28FSRE%29%20aims%20to%20extract%20relational%20facts%20from%20a%0Asparse%20set%20of%20labeled%20corpora.%20Recent%20studies%20have%20shown%20promising%20results%20in%0AFSRE%20by%20employing%20Pre-trained%20Language%20Models%20%28PLMs%29%20within%20the%20framework%20of%0Asupervised%20contrastive%20learning%2C%20which%20considers%20both%20instances%20and%20label%0Afacts.%20However%2C%20how%20to%20effectively%20harness%20massive%20instance-label%20pairs%20to%0Aencompass%20the%20learned%20representation%20with%20semantic%20richness%20in%20this%20learning%0Aparadigm%20is%20not%20fully%20explored.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20novel%0Asynergistic%20anchored%20contrastive%20pre-training%20framework.%20This%20framework%20is%0Amotivated%20by%20the%20insight%20that%20the%20diverse%20viewpoints%20conveyed%20through%0Ainstance-label%20pairs%20capture%20incomplete%20yet%20complementary%20intrinsic%20textual%0Asemantics.%20Specifically%2C%20our%20framework%20involves%20a%20symmetrical%20contrastive%0Aobjective%20that%20encompasses%20both%20sentence-anchored%20and%20label-anchored%0Acontrastive%20losses.%20By%20combining%20these%20two%20losses%2C%20the%20model%20establishes%20a%0Arobust%20and%20uniform%20representation%20space.%20This%20space%20effectively%20captures%20the%0Areciprocal%20alignment%20of%20feature%20distributions%20among%20instances%20and%20relational%0Afacts%2C%20simultaneously%20enhancing%20the%20maximization%20of%20mutual%20information%20across%0Adiverse%20perspectives%20within%20the%20same%20relation.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20achieves%20significant%20performance%20enhancements%20compared%20to%0Abaseline%20models%20in%20downstream%20FSRE%20tasks.%20Furthermore%2C%20our%20approach%20exhibits%0Asuperior%20adaptability%20to%20handle%20the%20challenges%20of%20domain%20shift%20and%20zero-shot%0Arelation%20extraction.%20Our%20code%20is%20available%20online%20at%0Ahttps%3A//github.com/AONE-NLP/FSRE-SaCon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12021v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergistic%20Anchored%20Contrastive%20Pre-training%20for%20Few-Shot%20Relation%0A%20%20Extraction&entry.906535625=Da%20Luo%20and%20Yanglei%20Gan%20and%20Rui%20Hou%20and%20Run%20Lin%20and%20Qiao%20Liu%20and%20Yuxiang%20Cai%20and%20Wannian%20Gao&entry.1292438233=%20%20Few-shot%20Relation%20Extraction%20%28FSRE%29%20aims%20to%20extract%20relational%20facts%20from%20a%0Asparse%20set%20of%20labeled%20corpora.%20Recent%20studies%20have%20shown%20promising%20results%20in%0AFSRE%20by%20employing%20Pre-trained%20Language%20Models%20%28PLMs%29%20within%20the%20framework%20of%0Asupervised%20contrastive%20learning%2C%20which%20considers%20both%20instances%20and%20label%0Afacts.%20However%2C%20how%20to%20effectively%20harness%20massive%20instance-label%20pairs%20to%0Aencompass%20the%20learned%20representation%20with%20semantic%20richness%20in%20this%20learning%0Aparadigm%20is%20not%20fully%20explored.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20novel%0Asynergistic%20anchored%20contrastive%20pre-training%20framework.%20This%20framework%20is%0Amotivated%20by%20the%20insight%20that%20the%20diverse%20viewpoints%20conveyed%20through%0Ainstance-label%20pairs%20capture%20incomplete%20yet%20complementary%20intrinsic%20textual%0Asemantics.%20Specifically%2C%20our%20framework%20involves%20a%20symmetrical%20contrastive%0Aobjective%20that%20encompasses%20both%20sentence-anchored%20and%20label-anchored%0Acontrastive%20losses.%20By%20combining%20these%20two%20losses%2C%20the%20model%20establishes%20a%0Arobust%20and%20uniform%20representation%20space.%20This%20space%20effectively%20captures%20the%0Areciprocal%20alignment%20of%20feature%20distributions%20among%20instances%20and%20relational%0Afacts%2C%20simultaneously%20enhancing%20the%20maximization%20of%20mutual%20information%20across%0Adiverse%20perspectives%20within%20the%20same%20relation.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20achieves%20significant%20performance%20enhancements%20compared%20to%0Abaseline%20models%20in%20downstream%20FSRE%20tasks.%20Furthermore%2C%20our%20approach%20exhibits%0Asuperior%20adaptability%20to%20handle%20the%20challenges%20of%20domain%20shift%20and%20zero-shot%0Arelation%20extraction.%20Our%20code%20is%20available%20online%20at%0Ahttps%3A//github.com/AONE-NLP/FSRE-SaCon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12021v3&entry.124074799=Read"},
{"title": "MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual\n  Grounding", "author": "Chun-Peng Chang and Shaoxiang Wang and Alain Pagani and Didier Stricker", "abstract": "  3D visual grounding involves matching natural language descriptions with\ntheir corresponding objects in 3D spaces. Existing methods often face\nchallenges with accuracy in object recognition and struggle in interpreting\ncomplex linguistic queries, particularly with descriptions that involve\nmultiple anchors or are view-dependent. In response, we present the MiKASA\n(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model\nintegrates a self-attention-based scene-aware object encoder and an original\nmulti-key-anchor technique, enhancing object recognition accuracy and the\nunderstanding of spatial relationships. Furthermore, MiKASA improves the\nexplainability of decision-making, facilitating error diagnosis. Our model\nachieves the highest overall accuracy in the Referit3D challenge for both the\nSr3D and Nr3D datasets, particularly excelling by a large margin in categories\nthat require viewpoint-dependent descriptions.\n  The source code and additional resources for this project are available on\nGitHub: https://github.com/birdy666/MiKASA-3DVG\n", "link": "http://arxiv.org/abs/2403.03077v2", "date": "2024-03-11", "relevancy": 2.4361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6418}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5919}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5698}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MiKASA%3A%20Multi-Key-Anchor%20%26%20Scene-Aware%20Transformer%20for%203D%20Visual%0A%20%20Grounding&body=Title%3A%20MiKASA%3A%20Multi-Key-Anchor%20%26%20Scene-Aware%20Transformer%20for%203D%20Visual%0A%20%20Grounding%0AAuthor%3A%20Chun-Peng%20Chang%20and%20Shaoxiang%20Wang%20and%20Alain%20Pagani%20and%20Didier%20Stricker%0AAbstract%3A%20%20%203D%20visual%20grounding%20involves%20matching%20natural%20language%20descriptions%20with%0Atheir%20corresponding%20objects%20in%203D%20spaces.%20Existing%20methods%20often%20face%0Achallenges%20with%20accuracy%20in%20object%20recognition%20and%20struggle%20in%20interpreting%0Acomplex%20linguistic%20queries%2C%20particularly%20with%20descriptions%20that%20involve%0Amultiple%20anchors%20or%20are%20view-dependent.%20In%20response%2C%20we%20present%20the%20MiKASA%0A%28Multi-Key-Anchor%20Scene-Aware%29%20Transformer.%20Our%20novel%20end-to-end%20trained%20model%0Aintegrates%20a%20self-attention-based%20scene-aware%20object%20encoder%20and%20an%20original%0Amulti-key-anchor%20technique%2C%20enhancing%20object%20recognition%20accuracy%20and%20the%0Aunderstanding%20of%20spatial%20relationships.%20Furthermore%2C%20MiKASA%20improves%20the%0Aexplainability%20of%20decision-making%2C%20facilitating%20error%20diagnosis.%20Our%20model%0Aachieves%20the%20highest%20overall%20accuracy%20in%20the%20Referit3D%20challenge%20for%20both%20the%0ASr3D%20and%20Nr3D%20datasets%2C%20particularly%20excelling%20by%20a%20large%20margin%20in%20categories%0Athat%20require%20viewpoint-dependent%20descriptions.%0A%20%20The%20source%20code%20and%20additional%20resources%20for%20this%20project%20are%20available%20on%0AGitHub%3A%20https%3A//github.com/birdy666/MiKASA-3DVG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03077v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiKASA%3A%20Multi-Key-Anchor%20%26%20Scene-Aware%20Transformer%20for%203D%20Visual%0A%20%20Grounding&entry.906535625=Chun-Peng%20Chang%20and%20Shaoxiang%20Wang%20and%20Alain%20Pagani%20and%20Didier%20Stricker&entry.1292438233=%20%203D%20visual%20grounding%20involves%20matching%20natural%20language%20descriptions%20with%0Atheir%20corresponding%20objects%20in%203D%20spaces.%20Existing%20methods%20often%20face%0Achallenges%20with%20accuracy%20in%20object%20recognition%20and%20struggle%20in%20interpreting%0Acomplex%20linguistic%20queries%2C%20particularly%20with%20descriptions%20that%20involve%0Amultiple%20anchors%20or%20are%20view-dependent.%20In%20response%2C%20we%20present%20the%20MiKASA%0A%28Multi-Key-Anchor%20Scene-Aware%29%20Transformer.%20Our%20novel%20end-to-end%20trained%20model%0Aintegrates%20a%20self-attention-based%20scene-aware%20object%20encoder%20and%20an%20original%0Amulti-key-anchor%20technique%2C%20enhancing%20object%20recognition%20accuracy%20and%20the%0Aunderstanding%20of%20spatial%20relationships.%20Furthermore%2C%20MiKASA%20improves%20the%0Aexplainability%20of%20decision-making%2C%20facilitating%20error%20diagnosis.%20Our%20model%0Aachieves%20the%20highest%20overall%20accuracy%20in%20the%20Referit3D%20challenge%20for%20both%20the%0ASr3D%20and%20Nr3D%20datasets%2C%20particularly%20excelling%20by%20a%20large%20margin%20in%20categories%0Athat%20require%20viewpoint-dependent%20descriptions.%0A%20%20The%20source%20code%20and%20additional%20resources%20for%20this%20project%20are%20available%20on%0AGitHub%3A%20https%3A//github.com/birdy666/MiKASA-3DVG%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03077v2&entry.124074799=Read"},
{"title": "Contact-Implicit Model Predictive Control for Dexterous In-hand\n  Manipulation: A Long-Horizon and Robust Approach", "author": "Yongpeng Jiang and Mingrui Yu and Xinghao Zhu and Masayoshi Tomizuka and Xiang Li", "abstract": "  Dexterous in-hand manipulation is an essential skill of production and life.\nNevertheless, the highly stiff and mutable features of contacts cause\nlimitations to real-time contact discovery and inference, which degrades the\nperformance of model-based methods. Inspired by recent advancements in\ncontact-rich locomotion and manipulation, this paper proposes a novel\nmodel-based approach to control dexterous in-hand manipulation and overcome the\ncurrent limitations. The proposed approach has the attractive feature, which\nallows the robot to robustly execute long-horizon in-hand manipulation without\npre-defined contact sequences or separated planning procedures. Specifically,\nwe design a contact-implicit model predictive controller at high-level to\ngenerate real-time contact plans, which are executed by the low-level tracking\ncontroller. Compared with other model-based methods, such a long-horizon\nfeature enables replanning and robust execution of contact-rich motions to\nachieve large-displacement in-hand tasks more efficiently; Compared with\nexisting learning-based methods, the proposed approach achieves the dexterity\nand also generalizes to different objects without any pre-training. Detailed\nsimulations and ablation studies demonstrate the efficiency and effectiveness\nof our method. It runs at 20Hz on the 23-degree-of-freedom long-horizon in-hand\nobject rotation task.\n", "link": "http://arxiv.org/abs/2402.18897v2", "date": "2024-03-11", "relevancy": 2.4325, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.64}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.591}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5831}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Contact-Implicit%20Model%20Predictive%20Control%20for%20Dexterous%20In-hand%0A%20%20Manipulation%3A%20A%20Long-Horizon%20and%20Robust%20Approach&body=Title%3A%20Contact-Implicit%20Model%20Predictive%20Control%20for%20Dexterous%20In-hand%0A%20%20Manipulation%3A%20A%20Long-Horizon%20and%20Robust%20Approach%0AAuthor%3A%20Yongpeng%20Jiang%20and%20Mingrui%20Yu%20and%20Xinghao%20Zhu%20and%20Masayoshi%20Tomizuka%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Dexterous%20in-hand%20manipulation%20is%20an%20essential%20skill%20of%20production%20and%20life.%0ANevertheless%2C%20the%20highly%20stiff%20and%20mutable%20features%20of%20contacts%20cause%0Alimitations%20to%20real-time%20contact%20discovery%20and%20inference%2C%20which%20degrades%20the%0Aperformance%20of%20model-based%20methods.%20Inspired%20by%20recent%20advancements%20in%0Acontact-rich%20locomotion%20and%20manipulation%2C%20this%20paper%20proposes%20a%20novel%0Amodel-based%20approach%20to%20control%20dexterous%20in-hand%20manipulation%20and%20overcome%20the%0Acurrent%20limitations.%20The%20proposed%20approach%20has%20the%20attractive%20feature%2C%20which%0Aallows%20the%20robot%20to%20robustly%20execute%20long-horizon%20in-hand%20manipulation%20without%0Apre-defined%20contact%20sequences%20or%20separated%20planning%20procedures.%20Specifically%2C%0Awe%20design%20a%20contact-implicit%20model%20predictive%20controller%20at%20high-level%20to%0Agenerate%20real-time%20contact%20plans%2C%20which%20are%20executed%20by%20the%20low-level%20tracking%0Acontroller.%20Compared%20with%20other%20model-based%20methods%2C%20such%20a%20long-horizon%0Afeature%20enables%20replanning%20and%20robust%20execution%20of%20contact-rich%20motions%20to%0Aachieve%20large-displacement%20in-hand%20tasks%20more%20efficiently%3B%20Compared%20with%0Aexisting%20learning-based%20methods%2C%20the%20proposed%20approach%20achieves%20the%20dexterity%0Aand%20also%20generalizes%20to%20different%20objects%20without%20any%20pre-training.%20Detailed%0Asimulations%20and%20ablation%20studies%20demonstrate%20the%20efficiency%20and%20effectiveness%0Aof%20our%20method.%20It%20runs%20at%2020Hz%20on%20the%2023-degree-of-freedom%20long-horizon%20in-hand%0Aobject%20rotation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18897v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contact-Implicit%20Model%20Predictive%20Control%20for%20Dexterous%20In-hand%0A%20%20Manipulation%3A%20A%20Long-Horizon%20and%20Robust%20Approach&entry.906535625=Yongpeng%20Jiang%20and%20Mingrui%20Yu%20and%20Xinghao%20Zhu%20and%20Masayoshi%20Tomizuka%20and%20Xiang%20Li&entry.1292438233=%20%20Dexterous%20in-hand%20manipulation%20is%20an%20essential%20skill%20of%20production%20and%20life.%0ANevertheless%2C%20the%20highly%20stiff%20and%20mutable%20features%20of%20contacts%20cause%0Alimitations%20to%20real-time%20contact%20discovery%20and%20inference%2C%20which%20degrades%20the%0Aperformance%20of%20model-based%20methods.%20Inspired%20by%20recent%20advancements%20in%0Acontact-rich%20locomotion%20and%20manipulation%2C%20this%20paper%20proposes%20a%20novel%0Amodel-based%20approach%20to%20control%20dexterous%20in-hand%20manipulation%20and%20overcome%20the%0Acurrent%20limitations.%20The%20proposed%20approach%20has%20the%20attractive%20feature%2C%20which%0Aallows%20the%20robot%20to%20robustly%20execute%20long-horizon%20in-hand%20manipulation%20without%0Apre-defined%20contact%20sequences%20or%20separated%20planning%20procedures.%20Specifically%2C%0Awe%20design%20a%20contact-implicit%20model%20predictive%20controller%20at%20high-level%20to%0Agenerate%20real-time%20contact%20plans%2C%20which%20are%20executed%20by%20the%20low-level%20tracking%0Acontroller.%20Compared%20with%20other%20model-based%20methods%2C%20such%20a%20long-horizon%0Afeature%20enables%20replanning%20and%20robust%20execution%20of%20contact-rich%20motions%20to%0Aachieve%20large-displacement%20in-hand%20tasks%20more%20efficiently%3B%20Compared%20with%0Aexisting%20learning-based%20methods%2C%20the%20proposed%20approach%20achieves%20the%20dexterity%0Aand%20also%20generalizes%20to%20different%20objects%20without%20any%20pre-training.%20Detailed%0Asimulations%20and%20ablation%20studies%20demonstrate%20the%20efficiency%20and%20effectiveness%0Aof%20our%20method.%20It%20runs%20at%2020Hz%20on%20the%2023-degree-of-freedom%20long-horizon%20in-hand%0Aobject%20rotation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18897v2&entry.124074799=Read"},
{"title": "Memory-based Adapters for Online 3D Scene Perception", "author": "Xiuwei Xu and Chong Xia and Ziwei Wang and Linqing Zhao and Yueqi Duan and Jie Zhou and Jiwen Lu", "abstract": "  In this paper, we propose a new framework for online 3D scene perception.\nConventional 3D scene perception methods are offline, i.e., take an already\nreconstructed 3D scene geometry as input, which is not applicable in robotic\napplications where the input data is streaming RGB-D videos rather than a\ncomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal with\nonline 3D scene perception tasks where data collection and perception should be\nperformed simultaneously, the model should be able to process 3D scenes frame\nby frame and make use of the temporal information. To this end, we propose an\nadapter-based plug-and-play module for the backbone of 3D scene perception\nmodel, which constructs memory to cache and aggregate the extracted RGB-D\nfeatures to empower offline models with temporal learning ability.\nSpecifically, we propose a queued memory mechanism to cache the supporting\npoint cloud and image features. Then we devise aggregation modules which\ndirectly perform on the memory and pass temporal information to current frame.\nWe further propose 3D-to-2D adapter to enhance image features with strong\nglobal context. Our adapters can be easily inserted into mainstream offline\narchitectures of different tasks and significantly boost their performance on\nonline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate\nour approach achieves leading performance on three 3D scene perception tasks\ncompared with state-of-the-art online methods by simply finetuning existing\noffline models, without any model and task-specific designs.\n\\href{https://xuxw98.github.io/Online3D/}{Project page}.\n", "link": "http://arxiv.org/abs/2403.06974v1", "date": "2024-03-11", "relevancy": 2.4312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6453}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5996}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5344}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Memory-based%20Adapters%20for%20Online%203D%20Scene%20Perception&body=Title%3A%20Memory-based%20Adapters%20for%20Online%203D%20Scene%20Perception%0AAuthor%3A%20Xiuwei%20Xu%20and%20Chong%20Xia%20and%20Ziwei%20Wang%20and%20Linqing%20Zhao%20and%20Yueqi%20Duan%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20for%20online%203D%20scene%20perception.%0AConventional%203D%20scene%20perception%20methods%20are%20offline%2C%20i.e.%2C%20take%20an%20already%0Areconstructed%203D%20scene%20geometry%20as%20input%2C%20which%20is%20not%20applicable%20in%20robotic%0Aapplications%20where%20the%20input%20data%20is%20streaming%20RGB-D%20videos%20rather%20than%20a%0Acomplete%203D%20scene%20reconstructed%20from%20pre-collected%20RGB-D%20videos.%20To%20deal%20with%0Aonline%203D%20scene%20perception%20tasks%20where%20data%20collection%20and%20perception%20should%20be%0Aperformed%20simultaneously%2C%20the%20model%20should%20be%20able%20to%20process%203D%20scenes%20frame%0Aby%20frame%20and%20make%20use%20of%20the%20temporal%20information.%20To%20this%20end%2C%20we%20propose%20an%0Aadapter-based%20plug-and-play%20module%20for%20the%20backbone%20of%203D%20scene%20perception%0Amodel%2C%20which%20constructs%20memory%20to%20cache%20and%20aggregate%20the%20extracted%20RGB-D%0Afeatures%20to%20empower%20offline%20models%20with%20temporal%20learning%20ability.%0ASpecifically%2C%20we%20propose%20a%20queued%20memory%20mechanism%20to%20cache%20the%20supporting%0Apoint%20cloud%20and%20image%20features.%20Then%20we%20devise%20aggregation%20modules%20which%0Adirectly%20perform%20on%20the%20memory%20and%20pass%20temporal%20information%20to%20current%20frame.%0AWe%20further%20propose%203D-to-2D%20adapter%20to%20enhance%20image%20features%20with%20strong%0Aglobal%20context.%20Our%20adapters%20can%20be%20easily%20inserted%20into%20mainstream%20offline%0Aarchitectures%20of%20different%20tasks%20and%20significantly%20boost%20their%20performance%20on%0Aonline%20tasks.%20Extensive%20experiments%20on%20ScanNet%20and%20SceneNN%20datasets%20demonstrate%0Aour%20approach%20achieves%20leading%20performance%20on%20three%203D%20scene%20perception%20tasks%0Acompared%20with%20state-of-the-art%20online%20methods%20by%20simply%20finetuning%20existing%0Aoffline%20models%2C%20without%20any%20model%20and%20task-specific%20designs.%0A%5Chref%7Bhttps%3A//xuxw98.github.io/Online3D/%7D%7BProject%20page%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06974v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-based%20Adapters%20for%20Online%203D%20Scene%20Perception&entry.906535625=Xiuwei%20Xu%20and%20Chong%20Xia%20and%20Ziwei%20Wang%20and%20Linqing%20Zhao%20and%20Yueqi%20Duan%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20for%20online%203D%20scene%20perception.%0AConventional%203D%20scene%20perception%20methods%20are%20offline%2C%20i.e.%2C%20take%20an%20already%0Areconstructed%203D%20scene%20geometry%20as%20input%2C%20which%20is%20not%20applicable%20in%20robotic%0Aapplications%20where%20the%20input%20data%20is%20streaming%20RGB-D%20videos%20rather%20than%20a%0Acomplete%203D%20scene%20reconstructed%20from%20pre-collected%20RGB-D%20videos.%20To%20deal%20with%0Aonline%203D%20scene%20perception%20tasks%20where%20data%20collection%20and%20perception%20should%20be%0Aperformed%20simultaneously%2C%20the%20model%20should%20be%20able%20to%20process%203D%20scenes%20frame%0Aby%20frame%20and%20make%20use%20of%20the%20temporal%20information.%20To%20this%20end%2C%20we%20propose%20an%0Aadapter-based%20plug-and-play%20module%20for%20the%20backbone%20of%203D%20scene%20perception%0Amodel%2C%20which%20constructs%20memory%20to%20cache%20and%20aggregate%20the%20extracted%20RGB-D%0Afeatures%20to%20empower%20offline%20models%20with%20temporal%20learning%20ability.%0ASpecifically%2C%20we%20propose%20a%20queued%20memory%20mechanism%20to%20cache%20the%20supporting%0Apoint%20cloud%20and%20image%20features.%20Then%20we%20devise%20aggregation%20modules%20which%0Adirectly%20perform%20on%20the%20memory%20and%20pass%20temporal%20information%20to%20current%20frame.%0AWe%20further%20propose%203D-to-2D%20adapter%20to%20enhance%20image%20features%20with%20strong%0Aglobal%20context.%20Our%20adapters%20can%20be%20easily%20inserted%20into%20mainstream%20offline%0Aarchitectures%20of%20different%20tasks%20and%20significantly%20boost%20their%20performance%20on%0Aonline%20tasks.%20Extensive%20experiments%20on%20ScanNet%20and%20SceneNN%20datasets%20demonstrate%0Aour%20approach%20achieves%20leading%20performance%20on%20three%203D%20scene%20perception%20tasks%0Acompared%20with%20state-of-the-art%20online%20methods%20by%20simply%20finetuning%20existing%0Aoffline%20models%2C%20without%20any%20model%20and%20task-specific%20designs.%0A%5Chref%7Bhttps%3A//xuxw98.github.io/Online3D/%7D%7BProject%20page%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06974v1&entry.124074799=Read"},
{"title": "TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models", "author": "Yushi Huang and Ruihao Gong and Jing Liu and Tianlong Chen and Xianglong Liu", "abstract": "  The Diffusion model, a prevalent framework for image generation, encounters\nsignificant challenges in terms of broad applicability due to its extended\ninference times and substantial memory requirements. Efficient Post-training\nQuantization (PTQ) is pivotal for addressing these issues in traditional\nmodels. Different from traditional models, diffusion models heavily depend on\nthe time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$\nfrom the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a\nfew modules totally irrespective of the sampling data. However, existing PTQ\nmethods do not optimize these modules separately. They adopt inappropriate\nreconstruction targets and complex calibration methods, resulting in a severe\ndisturbance of the temporal feature and denoising trajectory, as well as a low\ncompression efficiency. To solve these, we propose a Temporal Feature\nMaintenance Quantization (TFMQ) framework building upon a Temporal Information\nBlock which is just related to the time-step $t$ and unrelated to the sampling\ndata. Powered by the pioneering block design, we devise temporal information\naware reconstruction (TIAR) and finite set calibration (FSC) to align the\nfull-precision temporal features in a limited time. Equipped with the\nframework, we can maintain the most temporal information and ensure the\nend-to-end generation quality. Extensive experiments on various datasets and\ndiffusion models prove our state-of-the-art results. Remarkably, our\nquantization approach, for the first time, achieves model performance nearly on\npar with the full-precision model under 4-bit weight quantization.\nAdditionally, our method incurs almost no extra computational cost and\naccelerates quantization time by $2.0 \\times$ on LSUN-Bedrooms $256 \\times 256$\ncompared to previous works. Our code is publicly available at\nhttps://github.com/ModelTC/TFMQ-DM.\n", "link": "http://arxiv.org/abs/2311.16503v3", "date": "2024-03-11", "relevancy": 2.4233, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6232}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5997}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5909}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20TFMQ-DM%3A%20Temporal%20Feature%20Maintenance%20Quantization%20for%20Diffusion%20Models&body=Title%3A%20TFMQ-DM%3A%20Temporal%20Feature%20Maintenance%20Quantization%20for%20Diffusion%20Models%0AAuthor%3A%20Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Tianlong%20Chen%20and%20Xianglong%20Liu%0AAbstract%3A%20%20%20The%20Diffusion%20model%2C%20a%20prevalent%20framework%20for%20image%20generation%2C%20encounters%0Asignificant%20challenges%20in%20terms%20of%20broad%20applicability%20due%20to%20its%20extended%0Ainference%20times%20and%20substantial%20memory%20requirements.%20Efficient%20Post-training%0AQuantization%20%28PTQ%29%20is%20pivotal%20for%20addressing%20these%20issues%20in%20traditional%0Amodels.%20Different%20from%20traditional%20models%2C%20diffusion%20models%20heavily%20depend%20on%0Athe%20time-step%20%24t%24%20to%20achieve%20satisfactory%20multi-round%20denoising.%20Usually%2C%20%24t%24%0Afrom%20the%20finite%20set%20%24%5C%7B1%2C%20%5Cldots%2C%20T%5C%7D%24%20is%20encoded%20to%20a%20temporal%20feature%20by%20a%0Afew%20modules%20totally%20irrespective%20of%20the%20sampling%20data.%20However%2C%20existing%20PTQ%0Amethods%20do%20not%20optimize%20these%20modules%20separately.%20They%20adopt%20inappropriate%0Areconstruction%20targets%20and%20complex%20calibration%20methods%2C%20resulting%20in%20a%20severe%0Adisturbance%20of%20the%20temporal%20feature%20and%20denoising%20trajectory%2C%20as%20well%20as%20a%20low%0Acompression%20efficiency.%20To%20solve%20these%2C%20we%20propose%20a%20Temporal%20Feature%0AMaintenance%20Quantization%20%28TFMQ%29%20framework%20building%20upon%20a%20Temporal%20Information%0ABlock%20which%20is%20just%20related%20to%20the%20time-step%20%24t%24%20and%20unrelated%20to%20the%20sampling%0Adata.%20Powered%20by%20the%20pioneering%20block%20design%2C%20we%20devise%20temporal%20information%0Aaware%20reconstruction%20%28TIAR%29%20and%20finite%20set%20calibration%20%28FSC%29%20to%20align%20the%0Afull-precision%20temporal%20features%20in%20a%20limited%20time.%20Equipped%20with%20the%0Aframework%2C%20we%20can%20maintain%20the%20most%20temporal%20information%20and%20ensure%20the%0Aend-to-end%20generation%20quality.%20Extensive%20experiments%20on%20various%20datasets%20and%0Adiffusion%20models%20prove%20our%20state-of-the-art%20results.%20Remarkably%2C%20our%0Aquantization%20approach%2C%20for%20the%20first%20time%2C%20achieves%20model%20performance%20nearly%20on%0Apar%20with%20the%20full-precision%20model%20under%204-bit%20weight%20quantization.%0AAdditionally%2C%20our%20method%20incurs%20almost%20no%20extra%20computational%20cost%20and%0Aaccelerates%20quantization%20time%20by%20%242.0%20%5Ctimes%24%20on%20LSUN-Bedrooms%20%24256%20%5Ctimes%20256%24%0Acompared%20to%20previous%20works.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ModelTC/TFMQ-DM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16503v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TFMQ-DM%3A%20Temporal%20Feature%20Maintenance%20Quantization%20for%20Diffusion%20Models&entry.906535625=Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Tianlong%20Chen%20and%20Xianglong%20Liu&entry.1292438233=%20%20The%20Diffusion%20model%2C%20a%20prevalent%20framework%20for%20image%20generation%2C%20encounters%0Asignificant%20challenges%20in%20terms%20of%20broad%20applicability%20due%20to%20its%20extended%0Ainference%20times%20and%20substantial%20memory%20requirements.%20Efficient%20Post-training%0AQuantization%20%28PTQ%29%20is%20pivotal%20for%20addressing%20these%20issues%20in%20traditional%0Amodels.%20Different%20from%20traditional%20models%2C%20diffusion%20models%20heavily%20depend%20on%0Athe%20time-step%20%24t%24%20to%20achieve%20satisfactory%20multi-round%20denoising.%20Usually%2C%20%24t%24%0Afrom%20the%20finite%20set%20%24%5C%7B1%2C%20%5Cldots%2C%20T%5C%7D%24%20is%20encoded%20to%20a%20temporal%20feature%20by%20a%0Afew%20modules%20totally%20irrespective%20of%20the%20sampling%20data.%20However%2C%20existing%20PTQ%0Amethods%20do%20not%20optimize%20these%20modules%20separately.%20They%20adopt%20inappropriate%0Areconstruction%20targets%20and%20complex%20calibration%20methods%2C%20resulting%20in%20a%20severe%0Adisturbance%20of%20the%20temporal%20feature%20and%20denoising%20trajectory%2C%20as%20well%20as%20a%20low%0Acompression%20efficiency.%20To%20solve%20these%2C%20we%20propose%20a%20Temporal%20Feature%0AMaintenance%20Quantization%20%28TFMQ%29%20framework%20building%20upon%20a%20Temporal%20Information%0ABlock%20which%20is%20just%20related%20to%20the%20time-step%20%24t%24%20and%20unrelated%20to%20the%20sampling%0Adata.%20Powered%20by%20the%20pioneering%20block%20design%2C%20we%20devise%20temporal%20information%0Aaware%20reconstruction%20%28TIAR%29%20and%20finite%20set%20calibration%20%28FSC%29%20to%20align%20the%0Afull-precision%20temporal%20features%20in%20a%20limited%20time.%20Equipped%20with%20the%0Aframework%2C%20we%20can%20maintain%20the%20most%20temporal%20information%20and%20ensure%20the%0Aend-to-end%20generation%20quality.%20Extensive%20experiments%20on%20various%20datasets%20and%0Adiffusion%20models%20prove%20our%20state-of-the-art%20results.%20Remarkably%2C%20our%0Aquantization%20approach%2C%20for%20the%20first%20time%2C%20achieves%20model%20performance%20nearly%20on%0Apar%20with%20the%20full-precision%20model%20under%204-bit%20weight%20quantization.%0AAdditionally%2C%20our%20method%20incurs%20almost%20no%20extra%20computational%20cost%20and%0Aaccelerates%20quantization%20time%20by%20%242.0%20%5Ctimes%24%20on%20LSUN-Bedrooms%20%24256%20%5Ctimes%20256%24%0Acompared%20to%20previous%20works.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ModelTC/TFMQ-DM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16503v3&entry.124074799=Read"},
{"title": "Unlink to Unlearn: Simplifying Edge Unlearning in GNNs", "author": "Jiajun Tan and Fei Sun and Ruichen Qiu and Du Su and Huawei Shen", "abstract": "  As concerns over data privacy intensify, unlearning in Graph Neural Networks\n(GNNs) has emerged as a prominent research frontier in academia. This concept\nis pivotal in enforcing the \\textit{right to be forgotten}, which entails the\nselective removal of specific data from trained GNNs upon user request. Our\nresearch focuses on edge unlearning, a process of particular relevance to\nreal-world applications. Current state-of-the-art approaches like GNNDelete can\neliminate the influence of specific edges yet suffer from\n\\textit{over-forgetting}, which means the unlearning process inadvertently\nremoves excessive information beyond needed, leading to a significant\nperformance decline for remaining edges. Our analysis identifies the loss\nfunctions of GNNDelete as the primary source of over-forgetting and also\nsuggests that loss functions may be redundant for effective edge unlearning.\nBuilding on these insights, we simplify GNNDelete to develop \\textbf{Unlink to\nUnlearn} (UtU), a novel method that facilitates unlearning exclusively through\nunlinking the forget edges from graph structure. Our extensive experiments\ndemonstrate that UtU delivers privacy protection on par with that of a\nretrained model while preserving high accuracy in downstream tasks, by\nupholding over 97.3\\% of the retrained model's privacy protection capabilities\nand 99.8\\% of its link prediction accuracy. Meanwhile, UtU requires only\nconstant computational demands, underscoring its advantage as a highly\nlightweight and practical edge unlearning solution.\n", "link": "http://arxiv.org/abs/2402.10695v2", "date": "2024-03-11", "relevancy": 2.4095, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4876}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4811}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.477}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Unlink%20to%20Unlearn%3A%20Simplifying%20Edge%20Unlearning%20in%20GNNs&body=Title%3A%20Unlink%20to%20Unlearn%3A%20Simplifying%20Edge%20Unlearning%20in%20GNNs%0AAuthor%3A%20Jiajun%20Tan%20and%20Fei%20Sun%20and%20Ruichen%20Qiu%20and%20Du%20Su%20and%20Huawei%20Shen%0AAbstract%3A%20%20%20As%20concerns%20over%20data%20privacy%20intensify%2C%20unlearning%20in%20Graph%20Neural%20Networks%0A%28GNNs%29%20has%20emerged%20as%20a%20prominent%20research%20frontier%20in%20academia.%20This%20concept%0Ais%20pivotal%20in%20enforcing%20the%20%5Ctextit%7Bright%20to%20be%20forgotten%7D%2C%20which%20entails%20the%0Aselective%20removal%20of%20specific%20data%20from%20trained%20GNNs%20upon%20user%20request.%20Our%0Aresearch%20focuses%20on%20edge%20unlearning%2C%20a%20process%20of%20particular%20relevance%20to%0Areal-world%20applications.%20Current%20state-of-the-art%20approaches%20like%20GNNDelete%20can%0Aeliminate%20the%20influence%20of%20specific%20edges%20yet%20suffer%20from%0A%5Ctextit%7Bover-forgetting%7D%2C%20which%20means%20the%20unlearning%20process%20inadvertently%0Aremoves%20excessive%20information%20beyond%20needed%2C%20leading%20to%20a%20significant%0Aperformance%20decline%20for%20remaining%20edges.%20Our%20analysis%20identifies%20the%20loss%0Afunctions%20of%20GNNDelete%20as%20the%20primary%20source%20of%20over-forgetting%20and%20also%0Asuggests%20that%20loss%20functions%20may%20be%20redundant%20for%20effective%20edge%20unlearning.%0ABuilding%20on%20these%20insights%2C%20we%20simplify%20GNNDelete%20to%20develop%20%5Ctextbf%7BUnlink%20to%0AUnlearn%7D%20%28UtU%29%2C%20a%20novel%20method%20that%20facilitates%20unlearning%20exclusively%20through%0Aunlinking%20the%20forget%20edges%20from%20graph%20structure.%20Our%20extensive%20experiments%0Ademonstrate%20that%20UtU%20delivers%20privacy%20protection%20on%20par%20with%20that%20of%20a%0Aretrained%20model%20while%20preserving%20high%20accuracy%20in%20downstream%20tasks%2C%20by%0Aupholding%20over%2097.3%5C%25%20of%20the%20retrained%20model%27s%20privacy%20protection%20capabilities%0Aand%2099.8%5C%25%20of%20its%20link%20prediction%20accuracy.%20Meanwhile%2C%20UtU%20requires%20only%0Aconstant%20computational%20demands%2C%20underscoring%20its%20advantage%20as%20a%20highly%0Alightweight%20and%20practical%20edge%20unlearning%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10695v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlink%20to%20Unlearn%3A%20Simplifying%20Edge%20Unlearning%20in%20GNNs&entry.906535625=Jiajun%20Tan%20and%20Fei%20Sun%20and%20Ruichen%20Qiu%20and%20Du%20Su%20and%20Huawei%20Shen&entry.1292438233=%20%20As%20concerns%20over%20data%20privacy%20intensify%2C%20unlearning%20in%20Graph%20Neural%20Networks%0A%28GNNs%29%20has%20emerged%20as%20a%20prominent%20research%20frontier%20in%20academia.%20This%20concept%0Ais%20pivotal%20in%20enforcing%20the%20%5Ctextit%7Bright%20to%20be%20forgotten%7D%2C%20which%20entails%20the%0Aselective%20removal%20of%20specific%20data%20from%20trained%20GNNs%20upon%20user%20request.%20Our%0Aresearch%20focuses%20on%20edge%20unlearning%2C%20a%20process%20of%20particular%20relevance%20to%0Areal-world%20applications.%20Current%20state-of-the-art%20approaches%20like%20GNNDelete%20can%0Aeliminate%20the%20influence%20of%20specific%20edges%20yet%20suffer%20from%0A%5Ctextit%7Bover-forgetting%7D%2C%20which%20means%20the%20unlearning%20process%20inadvertently%0Aremoves%20excessive%20information%20beyond%20needed%2C%20leading%20to%20a%20significant%0Aperformance%20decline%20for%20remaining%20edges.%20Our%20analysis%20identifies%20the%20loss%0Afunctions%20of%20GNNDelete%20as%20the%20primary%20source%20of%20over-forgetting%20and%20also%0Asuggests%20that%20loss%20functions%20may%20be%20redundant%20for%20effective%20edge%20unlearning.%0ABuilding%20on%20these%20insights%2C%20we%20simplify%20GNNDelete%20to%20develop%20%5Ctextbf%7BUnlink%20to%0AUnlearn%7D%20%28UtU%29%2C%20a%20novel%20method%20that%20facilitates%20unlearning%20exclusively%20through%0Aunlinking%20the%20forget%20edges%20from%20graph%20structure.%20Our%20extensive%20experiments%0Ademonstrate%20that%20UtU%20delivers%20privacy%20protection%20on%20par%20with%20that%20of%20a%0Aretrained%20model%20while%20preserving%20high%20accuracy%20in%20downstream%20tasks%2C%20by%0Aupholding%20over%2097.3%5C%25%20of%20the%20retrained%20model%27s%20privacy%20protection%20capabilities%0Aand%2099.8%5C%25%20of%20its%20link%20prediction%20accuracy.%20Meanwhile%2C%20UtU%20requires%20only%0Aconstant%20computational%20demands%2C%20underscoring%20its%20advantage%20as%20a%20highly%0Alightweight%20and%20practical%20edge%20unlearning%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10695v2&entry.124074799=Read"},
{"title": "Cross-domain and Cross-dimension Learning for Image-to-Graph\n  Transformers", "author": "Alexander H. Berger and Laurin Lux and Suprosanna Shit and Ivan Ezhov and Georgios Kaissis and Martin J. Menten and Daniel Rueckert and Johannes C. Paetzold", "abstract": "  Direct image-to-graph transformation is a challenging task that solves object\ndetection and relationship prediction in a single model. Due to the complexity\nof this task, large training datasets are rare in many domains, which makes the\ntraining of large networks challenging. This data sparsity necessitates the\nestablishment of pre-training strategies akin to the state-of-the-art in\ncomputer vision. In this work, we introduce a set of methods enabling\ncross-domain and cross-dimension transfer learning for image-to-graph\ntransformers. We propose (1) a regularized edge sampling loss for sampling the\noptimal number of object relationships (edges) across domains, (2) a domain\nadaptation framework for image-to-graph transformers that aligns features from\ndifferent domains, and (3) a simple projection function that allows us to\npretrain 3D transformers on 2D input data. We demonstrate our method's utility\nin cross-domain and cross-dimension experiments, where we pretrain our models\non 2D satellite images before applying them to vastly different target domains\nin 2D and 3D. Our method consistently outperforms a series of baselines on\nchallenging benchmarks, such as retinal or whole-brain vessel graph extraction.\n", "link": "http://arxiv.org/abs/2403.06601v1", "date": "2024-03-11", "relevancy": 2.3989, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.617}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6056}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5869}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Cross-domain%20and%20Cross-dimension%20Learning%20for%20Image-to-Graph%0A%20%20Transformers&body=Title%3A%20Cross-domain%20and%20Cross-dimension%20Learning%20for%20Image-to-Graph%0A%20%20Transformers%0AAuthor%3A%20Alexander%20H.%20Berger%20and%20Laurin%20Lux%20and%20Suprosanna%20Shit%20and%20Ivan%20Ezhov%20and%20Georgios%20Kaissis%20and%20Martin%20J.%20Menten%20and%20Daniel%20Rueckert%20and%20Johannes%20C.%20Paetzold%0AAbstract%3A%20%20%20Direct%20image-to-graph%20transformation%20is%20a%20challenging%20task%20that%20solves%20object%0Adetection%20and%20relationship%20prediction%20in%20a%20single%20model.%20Due%20to%20the%20complexity%0Aof%20this%20task%2C%20large%20training%20datasets%20are%20rare%20in%20many%20domains%2C%20which%20makes%20the%0Atraining%20of%20large%20networks%20challenging.%20This%20data%20sparsity%20necessitates%20the%0Aestablishment%20of%20pre-training%20strategies%20akin%20to%20the%20state-of-the-art%20in%0Acomputer%20vision.%20In%20this%20work%2C%20we%20introduce%20a%20set%20of%20methods%20enabling%0Across-domain%20and%20cross-dimension%20transfer%20learning%20for%20image-to-graph%0Atransformers.%20We%20propose%20%281%29%20a%20regularized%20edge%20sampling%20loss%20for%20sampling%20the%0Aoptimal%20number%20of%20object%20relationships%20%28edges%29%20across%20domains%2C%20%282%29%20a%20domain%0Aadaptation%20framework%20for%20image-to-graph%20transformers%20that%20aligns%20features%20from%0Adifferent%20domains%2C%20and%20%283%29%20a%20simple%20projection%20function%20that%20allows%20us%20to%0Apretrain%203D%20transformers%20on%202D%20input%20data.%20We%20demonstrate%20our%20method%27s%20utility%0Ain%20cross-domain%20and%20cross-dimension%20experiments%2C%20where%20we%20pretrain%20our%20models%0Aon%202D%20satellite%20images%20before%20applying%20them%20to%20vastly%20different%20target%20domains%0Ain%202D%20and%203D.%20Our%20method%20consistently%20outperforms%20a%20series%20of%20baselines%20on%0Achallenging%20benchmarks%2C%20such%20as%20retinal%20or%20whole-brain%20vessel%20graph%20extraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06601v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-domain%20and%20Cross-dimension%20Learning%20for%20Image-to-Graph%0A%20%20Transformers&entry.906535625=Alexander%20H.%20Berger%20and%20Laurin%20Lux%20and%20Suprosanna%20Shit%20and%20Ivan%20Ezhov%20and%20Georgios%20Kaissis%20and%20Martin%20J.%20Menten%20and%20Daniel%20Rueckert%20and%20Johannes%20C.%20Paetzold&entry.1292438233=%20%20Direct%20image-to-graph%20transformation%20is%20a%20challenging%20task%20that%20solves%20object%0Adetection%20and%20relationship%20prediction%20in%20a%20single%20model.%20Due%20to%20the%20complexity%0Aof%20this%20task%2C%20large%20training%20datasets%20are%20rare%20in%20many%20domains%2C%20which%20makes%20the%0Atraining%20of%20large%20networks%20challenging.%20This%20data%20sparsity%20necessitates%20the%0Aestablishment%20of%20pre-training%20strategies%20akin%20to%20the%20state-of-the-art%20in%0Acomputer%20vision.%20In%20this%20work%2C%20we%20introduce%20a%20set%20of%20methods%20enabling%0Across-domain%20and%20cross-dimension%20transfer%20learning%20for%20image-to-graph%0Atransformers.%20We%20propose%20%281%29%20a%20regularized%20edge%20sampling%20loss%20for%20sampling%20the%0Aoptimal%20number%20of%20object%20relationships%20%28edges%29%20across%20domains%2C%20%282%29%20a%20domain%0Aadaptation%20framework%20for%20image-to-graph%20transformers%20that%20aligns%20features%20from%0Adifferent%20domains%2C%20and%20%283%29%20a%20simple%20projection%20function%20that%20allows%20us%20to%0Apretrain%203D%20transformers%20on%202D%20input%20data.%20We%20demonstrate%20our%20method%27s%20utility%0Ain%20cross-domain%20and%20cross-dimension%20experiments%2C%20where%20we%20pretrain%20our%20models%0Aon%202D%20satellite%20images%20before%20applying%20them%20to%20vastly%20different%20target%20domains%0Ain%202D%20and%203D.%20Our%20method%20consistently%20outperforms%20a%20series%20of%20baselines%20on%0Achallenging%20benchmarks%2C%20such%20as%20retinal%20or%20whole-brain%20vessel%20graph%20extraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06601v1&entry.124074799=Read"},
{"title": "COOD: Combined out-of-distribution detection using multiple measures for\n  anomaly & novel class detection in large-scale hierarchical classification", "author": "L. E. Hogeweg and R. Gangireddy and D. Brunink and V. J. Kalkman and L. Cornelissen and J. W. Kamminga", "abstract": "  High-performing out-of-distribution (OOD) detection, both anomaly and novel\nclass, is an important prerequisite for the practical use of classification\nmodels. In this paper, we focus on the species recognition task in images\nconcerned with large databases, a large number of fine-grained hierarchical\nclasses, severe class imbalance, and varying image quality. We propose a\nframework for combining individual OOD measures into one combined OOD (COOD)\nmeasure using a supervised model. The individual measures are several existing\nstate-of-the-art measures and several novel OOD measures developed with novel\nclass detection and hierarchical class structure in mind. COOD was extensively\nevaluated on three large-scale (500k+ images) biodiversity datasets in the\ncontext of anomaly and novel class detection. We show that COOD outperforms\nindividual, including state-of-the-art, OOD measures by a large margin in terms\nof TPR@1% FPR in the majority of experiments, e.g., improving detecting\nImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.\nSHAP (feature contribution) analysis shows that different individual OOD\nmeasures are essential for various tasks, indicating that multiple OOD measures\nand combinations are needed to generalize. Additionally, we show that\nexplicitly considering ID images that are incorrectly classified for the\noriginal (species) recognition task is important for constructing\nhigh-performing OOD detection methods and for practical applicability. The\nframework can easily be extended or adapted to other tasks and media\nmodalities.\n", "link": "http://arxiv.org/abs/2403.06874v1", "date": "2024-03-11", "relevancy": 2.3684, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.486}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4621}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20COOD%3A%20Combined%20out-of-distribution%20detection%20using%20multiple%20measures%20for%0A%20%20anomaly%20%26%20novel%20class%20detection%20in%20large-scale%20hierarchical%20classification&body=Title%3A%20COOD%3A%20Combined%20out-of-distribution%20detection%20using%20multiple%20measures%20for%0A%20%20anomaly%20%26%20novel%20class%20detection%20in%20large-scale%20hierarchical%20classification%0AAuthor%3A%20L.%20E.%20Hogeweg%20and%20R.%20Gangireddy%20and%20D.%20Brunink%20and%20V.%20J.%20Kalkman%20and%20L.%20Cornelissen%20and%20J.%20W.%20Kamminga%0AAbstract%3A%20%20%20High-performing%20out-of-distribution%20%28OOD%29%20detection%2C%20both%20anomaly%20and%20novel%0Aclass%2C%20is%20an%20important%20prerequisite%20for%20the%20practical%20use%20of%20classification%0Amodels.%20In%20this%20paper%2C%20we%20focus%20on%20the%20species%20recognition%20task%20in%20images%0Aconcerned%20with%20large%20databases%2C%20a%20large%20number%20of%20fine-grained%20hierarchical%0Aclasses%2C%20severe%20class%20imbalance%2C%20and%20varying%20image%20quality.%20We%20propose%20a%0Aframework%20for%20combining%20individual%20OOD%20measures%20into%20one%20combined%20OOD%20%28COOD%29%0Ameasure%20using%20a%20supervised%20model.%20The%20individual%20measures%20are%20several%20existing%0Astate-of-the-art%20measures%20and%20several%20novel%20OOD%20measures%20developed%20with%20novel%0Aclass%20detection%20and%20hierarchical%20class%20structure%20in%20mind.%20COOD%20was%20extensively%0Aevaluated%20on%20three%20large-scale%20%28500k%2B%20images%29%20biodiversity%20datasets%20in%20the%0Acontext%20of%20anomaly%20and%20novel%20class%20detection.%20We%20show%20that%20COOD%20outperforms%0Aindividual%2C%20including%20state-of-the-art%2C%20OOD%20measures%20by%20a%20large%20margin%20in%20terms%0Aof%20TPR%401%25%20FPR%20in%20the%20majority%20of%20experiments%2C%20e.g.%2C%20improving%20detecting%0AImageNet%20images%20%28OOD%29%20from%2054.3%25%20to%2085.4%25%20for%20the%20iNaturalist%202018%20dataset.%0ASHAP%20%28feature%20contribution%29%20analysis%20shows%20that%20different%20individual%20OOD%0Ameasures%20are%20essential%20for%20various%20tasks%2C%20indicating%20that%20multiple%20OOD%20measures%0Aand%20combinations%20are%20needed%20to%20generalize.%20Additionally%2C%20we%20show%20that%0Aexplicitly%20considering%20ID%20images%20that%20are%20incorrectly%20classified%20for%20the%0Aoriginal%20%28species%29%20recognition%20task%20is%20important%20for%20constructing%0Ahigh-performing%20OOD%20detection%20methods%20and%20for%20practical%20applicability.%20The%0Aframework%20can%20easily%20be%20extended%20or%20adapted%20to%20other%20tasks%20and%20media%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06874v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COOD%3A%20Combined%20out-of-distribution%20detection%20using%20multiple%20measures%20for%0A%20%20anomaly%20%26%20novel%20class%20detection%20in%20large-scale%20hierarchical%20classification&entry.906535625=L.%20E.%20Hogeweg%20and%20R.%20Gangireddy%20and%20D.%20Brunink%20and%20V.%20J.%20Kalkman%20and%20L.%20Cornelissen%20and%20J.%20W.%20Kamminga&entry.1292438233=%20%20High-performing%20out-of-distribution%20%28OOD%29%20detection%2C%20both%20anomaly%20and%20novel%0Aclass%2C%20is%20an%20important%20prerequisite%20for%20the%20practical%20use%20of%20classification%0Amodels.%20In%20this%20paper%2C%20we%20focus%20on%20the%20species%20recognition%20task%20in%20images%0Aconcerned%20with%20large%20databases%2C%20a%20large%20number%20of%20fine-grained%20hierarchical%0Aclasses%2C%20severe%20class%20imbalance%2C%20and%20varying%20image%20quality.%20We%20propose%20a%0Aframework%20for%20combining%20individual%20OOD%20measures%20into%20one%20combined%20OOD%20%28COOD%29%0Ameasure%20using%20a%20supervised%20model.%20The%20individual%20measures%20are%20several%20existing%0Astate-of-the-art%20measures%20and%20several%20novel%20OOD%20measures%20developed%20with%20novel%0Aclass%20detection%20and%20hierarchical%20class%20structure%20in%20mind.%20COOD%20was%20extensively%0Aevaluated%20on%20three%20large-scale%20%28500k%2B%20images%29%20biodiversity%20datasets%20in%20the%0Acontext%20of%20anomaly%20and%20novel%20class%20detection.%20We%20show%20that%20COOD%20outperforms%0Aindividual%2C%20including%20state-of-the-art%2C%20OOD%20measures%20by%20a%20large%20margin%20in%20terms%0Aof%20TPR%401%25%20FPR%20in%20the%20majority%20of%20experiments%2C%20e.g.%2C%20improving%20detecting%0AImageNet%20images%20%28OOD%29%20from%2054.3%25%20to%2085.4%25%20for%20the%20iNaturalist%202018%20dataset.%0ASHAP%20%28feature%20contribution%29%20analysis%20shows%20that%20different%20individual%20OOD%0Ameasures%20are%20essential%20for%20various%20tasks%2C%20indicating%20that%20multiple%20OOD%20measures%0Aand%20combinations%20are%20needed%20to%20generalize.%20Additionally%2C%20we%20show%20that%0Aexplicitly%20considering%20ID%20images%20that%20are%20incorrectly%20classified%20for%20the%0Aoriginal%20%28species%29%20recognition%20task%20is%20important%20for%20constructing%0Ahigh-performing%20OOD%20detection%20methods%20and%20for%20practical%20applicability.%20The%0Aframework%20can%20easily%20be%20extended%20or%20adapted%20to%20other%20tasks%20and%20media%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06874v1&entry.124074799=Read"},
{"title": "Zero-Shot Image Harmonization with Generative Model Prior", "author": "Jianqi Chen and Yilan Zhang and Zhengxia Zou and Keyan Chen and Zhenwei Shi", "abstract": "  We propose a zero-shot approach to image harmonization, aiming to overcome\nthe reliance on large amounts of synthetic composite images in existing\nmethods. These methods, while showing promising results, involve significant\ntraining expenses and often struggle with generalization to unseen images. To\nthis end, we introduce a fully modularized framework inspired by human\nbehavior. Leveraging the reasoning capabilities of recent foundation models in\nlanguage and vision, our approach comprises three main stages. Initially, we\nemploy a pretrained vision-language model (VLM) to generate descriptions for\nthe composite image. Subsequently, these descriptions guide the foreground\nharmonization direction of a text-to-image generative model (T2I). We refine\ntext embeddings for enhanced representation of imaging conditions and employ\nself-attention and edge maps for structure preservation. Following each\nharmonization iteration, an evaluator determines whether to conclude or modify\nthe harmonization direction. The resulting framework, mirroring human behavior,\nachieves harmonious results without the need for extensive training. We present\ncompelling visual results across diverse scenes and objects, along with a user\nstudy validating the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2307.08182v2", "date": "2024-03-11", "relevancy": 2.3568, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5992}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5641}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Image%20Harmonization%20with%20Generative%20Model%20Prior&body=Title%3A%20Zero-Shot%20Image%20Harmonization%20with%20Generative%20Model%20Prior%0AAuthor%3A%20Jianqi%20Chen%20and%20Yilan%20Zhang%20and%20Zhengxia%20Zou%20and%20Keyan%20Chen%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20We%20propose%20a%20zero-shot%20approach%20to%20image%20harmonization%2C%20aiming%20to%20overcome%0Athe%20reliance%20on%20large%20amounts%20of%20synthetic%20composite%20images%20in%20existing%0Amethods.%20These%20methods%2C%20while%20showing%20promising%20results%2C%20involve%20significant%0Atraining%20expenses%20and%20often%20struggle%20with%20generalization%20to%20unseen%20images.%20To%0Athis%20end%2C%20we%20introduce%20a%20fully%20modularized%20framework%20inspired%20by%20human%0Abehavior.%20Leveraging%20the%20reasoning%20capabilities%20of%20recent%20foundation%20models%20in%0Alanguage%20and%20vision%2C%20our%20approach%20comprises%20three%20main%20stages.%20Initially%2C%20we%0Aemploy%20a%20pretrained%20vision-language%20model%20%28VLM%29%20to%20generate%20descriptions%20for%0Athe%20composite%20image.%20Subsequently%2C%20these%20descriptions%20guide%20the%20foreground%0Aharmonization%20direction%20of%20a%20text-to-image%20generative%20model%20%28T2I%29.%20We%20refine%0Atext%20embeddings%20for%20enhanced%20representation%20of%20imaging%20conditions%20and%20employ%0Aself-attention%20and%20edge%20maps%20for%20structure%20preservation.%20Following%20each%0Aharmonization%20iteration%2C%20an%20evaluator%20determines%20whether%20to%20conclude%20or%20modify%0Athe%20harmonization%20direction.%20The%20resulting%20framework%2C%20mirroring%20human%20behavior%2C%0Aachieves%20harmonious%20results%20without%20the%20need%20for%20extensive%20training.%20We%20present%0Acompelling%20visual%20results%20across%20diverse%20scenes%20and%20objects%2C%20along%20with%20a%20user%0Astudy%20validating%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08182v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Image%20Harmonization%20with%20Generative%20Model%20Prior&entry.906535625=Jianqi%20Chen%20and%20Yilan%20Zhang%20and%20Zhengxia%20Zou%20and%20Keyan%20Chen%20and%20Zhenwei%20Shi&entry.1292438233=%20%20We%20propose%20a%20zero-shot%20approach%20to%20image%20harmonization%2C%20aiming%20to%20overcome%0Athe%20reliance%20on%20large%20amounts%20of%20synthetic%20composite%20images%20in%20existing%0Amethods.%20These%20methods%2C%20while%20showing%20promising%20results%2C%20involve%20significant%0Atraining%20expenses%20and%20often%20struggle%20with%20generalization%20to%20unseen%20images.%20To%0Athis%20end%2C%20we%20introduce%20a%20fully%20modularized%20framework%20inspired%20by%20human%0Abehavior.%20Leveraging%20the%20reasoning%20capabilities%20of%20recent%20foundation%20models%20in%0Alanguage%20and%20vision%2C%20our%20approach%20comprises%20three%20main%20stages.%20Initially%2C%20we%0Aemploy%20a%20pretrained%20vision-language%20model%20%28VLM%29%20to%20generate%20descriptions%20for%0Athe%20composite%20image.%20Subsequently%2C%20these%20descriptions%20guide%20the%20foreground%0Aharmonization%20direction%20of%20a%20text-to-image%20generative%20model%20%28T2I%29.%20We%20refine%0Atext%20embeddings%20for%20enhanced%20representation%20of%20imaging%20conditions%20and%20employ%0Aself-attention%20and%20edge%20maps%20for%20structure%20preservation.%20Following%20each%0Aharmonization%20iteration%2C%20an%20evaluator%20determines%20whether%20to%20conclude%20or%20modify%0Athe%20harmonization%20direction.%20The%20resulting%20framework%2C%20mirroring%20human%20behavior%2C%0Aachieves%20harmonious%20results%20without%20the%20need%20for%20extensive%20training.%20We%20present%0Acompelling%20visual%20results%20across%20diverse%20scenes%20and%20objects%2C%20along%20with%20a%20user%0Astudy%20validating%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08182v2&entry.124074799=Read"},
{"title": "PeerAiD: Improving Adversarial Distillation from a Specialized Peer\n  Tutor", "author": "Jaewon Jung and Hongsun Jang and Jaeyong Song and Jinho Lee", "abstract": "  Adversarial robustness of the neural network is a significant concern when it\nis applied to security-critical domains. In this situation, adversarial\ndistillation is a promising option which aims to distill the robustness of the\nteacher network to improve the robustness of a small student network. Previous\nworks pretrain the teacher network to make it robust to the adversarial\nexamples aimed at itself. However, the adversarial examples are dependent on\nthe parameters of the target network. The fixed teacher network inevitably\ndegrades its robustness against the unseen transferred adversarial examples\nwhich targets the parameters of the student network in the adversarial\ndistillation process. We propose PeerAiD to make a peer network learn the\nadversarial examples of the student network instead of adversarial examples\naimed at itself. PeerAiD is an adversarial distillation that trains the peer\nnetwork and the student network simultaneously in order to make the peer\nnetwork specialized for defending the student network. We observe that such\npeer networks surpass the robustness of pretrained robust teacher network\nagainst student-attacked adversarial samples. With this peer network and\nadversarial distillation, PeerAiD achieves significantly higher robustness of\nthe student network with AutoAttack (AA) accuracy up to 1.66%p and improves the\nnatural accuracy of the student network up to 4.72%p with ResNet-18 and\nTinyImageNet dataset.\n", "link": "http://arxiv.org/abs/2403.06668v1", "date": "2024-03-11", "relevancy": 2.3489, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4967}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4609}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4517}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20PeerAiD%3A%20Improving%20Adversarial%20Distillation%20from%20a%20Specialized%20Peer%0A%20%20Tutor&body=Title%3A%20PeerAiD%3A%20Improving%20Adversarial%20Distillation%20from%20a%20Specialized%20Peer%0A%20%20Tutor%0AAuthor%3A%20Jaewon%20Jung%20and%20Hongsun%20Jang%20and%20Jaeyong%20Song%20and%20Jinho%20Lee%0AAbstract%3A%20%20%20Adversarial%20robustness%20of%20the%20neural%20network%20is%20a%20significant%20concern%20when%20it%0Ais%20applied%20to%20security-critical%20domains.%20In%20this%20situation%2C%20adversarial%0Adistillation%20is%20a%20promising%20option%20which%20aims%20to%20distill%20the%20robustness%20of%20the%0Ateacher%20network%20to%20improve%20the%20robustness%20of%20a%20small%20student%20network.%20Previous%0Aworks%20pretrain%20the%20teacher%20network%20to%20make%20it%20robust%20to%20the%20adversarial%0Aexamples%20aimed%20at%20itself.%20However%2C%20the%20adversarial%20examples%20are%20dependent%20on%0Athe%20parameters%20of%20the%20target%20network.%20The%20fixed%20teacher%20network%20inevitably%0Adegrades%20its%20robustness%20against%20the%20unseen%20transferred%20adversarial%20examples%0Awhich%20targets%20the%20parameters%20of%20the%20student%20network%20in%20the%20adversarial%0Adistillation%20process.%20We%20propose%20PeerAiD%20to%20make%20a%20peer%20network%20learn%20the%0Aadversarial%20examples%20of%20the%20student%20network%20instead%20of%20adversarial%20examples%0Aaimed%20at%20itself.%20PeerAiD%20is%20an%20adversarial%20distillation%20that%20trains%20the%20peer%0Anetwork%20and%20the%20student%20network%20simultaneously%20in%20order%20to%20make%20the%20peer%0Anetwork%20specialized%20for%20defending%20the%20student%20network.%20We%20observe%20that%20such%0Apeer%20networks%20surpass%20the%20robustness%20of%20pretrained%20robust%20teacher%20network%0Aagainst%20student-attacked%20adversarial%20samples.%20With%20this%20peer%20network%20and%0Aadversarial%20distillation%2C%20PeerAiD%20achieves%20significantly%20higher%20robustness%20of%0Athe%20student%20network%20with%20AutoAttack%20%28AA%29%20accuracy%20up%20to%201.66%25p%20and%20improves%20the%0Anatural%20accuracy%20of%20the%20student%20network%20up%20to%204.72%25p%20with%20ResNet-18%20and%0ATinyImageNet%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06668v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PeerAiD%3A%20Improving%20Adversarial%20Distillation%20from%20a%20Specialized%20Peer%0A%20%20Tutor&entry.906535625=Jaewon%20Jung%20and%20Hongsun%20Jang%20and%20Jaeyong%20Song%20and%20Jinho%20Lee&entry.1292438233=%20%20Adversarial%20robustness%20of%20the%20neural%20network%20is%20a%20significant%20concern%20when%20it%0Ais%20applied%20to%20security-critical%20domains.%20In%20this%20situation%2C%20adversarial%0Adistillation%20is%20a%20promising%20option%20which%20aims%20to%20distill%20the%20robustness%20of%20the%0Ateacher%20network%20to%20improve%20the%20robustness%20of%20a%20small%20student%20network.%20Previous%0Aworks%20pretrain%20the%20teacher%20network%20to%20make%20it%20robust%20to%20the%20adversarial%0Aexamples%20aimed%20at%20itself.%20However%2C%20the%20adversarial%20examples%20are%20dependent%20on%0Athe%20parameters%20of%20the%20target%20network.%20The%20fixed%20teacher%20network%20inevitably%0Adegrades%20its%20robustness%20against%20the%20unseen%20transferred%20adversarial%20examples%0Awhich%20targets%20the%20parameters%20of%20the%20student%20network%20in%20the%20adversarial%0Adistillation%20process.%20We%20propose%20PeerAiD%20to%20make%20a%20peer%20network%20learn%20the%0Aadversarial%20examples%20of%20the%20student%20network%20instead%20of%20adversarial%20examples%0Aaimed%20at%20itself.%20PeerAiD%20is%20an%20adversarial%20distillation%20that%20trains%20the%20peer%0Anetwork%20and%20the%20student%20network%20simultaneously%20in%20order%20to%20make%20the%20peer%0Anetwork%20specialized%20for%20defending%20the%20student%20network.%20We%20observe%20that%20such%0Apeer%20networks%20surpass%20the%20robustness%20of%20pretrained%20robust%20teacher%20network%0Aagainst%20student-attacked%20adversarial%20samples.%20With%20this%20peer%20network%20and%0Aadversarial%20distillation%2C%20PeerAiD%20achieves%20significantly%20higher%20robustness%20of%0Athe%20student%20network%20with%20AutoAttack%20%28AA%29%20accuracy%20up%20to%201.66%25p%20and%20improves%20the%0Anatural%20accuracy%20of%20the%20student%20network%20up%20to%204.72%25p%20with%20ResNet-18%20and%0ATinyImageNet%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06668v1&entry.124074799=Read"},
{"title": "DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video\n  Generation", "author": "Guosheng Zhao and Xiaofeng Wang and Zheng Zhu and Xinze Chen and Guan Huang and Xiaoyi Bao and Xingang Wang", "abstract": "  World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.\n", "link": "http://arxiv.org/abs/2403.06845v1", "date": "2024-03-11", "relevancy": 2.3003, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5879}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5721}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5635}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DriveDreamer-2%3A%20LLM-Enhanced%20World%20Models%20for%20Diverse%20Driving%20Video%0A%20%20Generation&body=Title%3A%20DriveDreamer-2%3A%20LLM-Enhanced%20World%20Models%20for%20Diverse%20Driving%20Video%0A%20%20Generation%0AAuthor%3A%20Guosheng%20Zhao%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Xinze%20Chen%20and%20Guan%20Huang%20and%20Xiaoyi%20Bao%20and%20Xingang%20Wang%0AAbstract%3A%20%20%20World%20models%20have%20demonstrated%20superiority%20in%20autonomous%20driving%2C%0Aparticularly%20in%20the%20generation%20of%20multi-view%20driving%20videos.%20However%2C%0Asignificant%20challenges%20still%20exist%20in%20generating%20customized%20driving%20videos.%20In%0Athis%20paper%2C%20we%20propose%20DriveDreamer-2%2C%20which%20builds%20upon%20the%20framework%20of%0ADriveDreamer%20and%20incorporates%20a%20Large%20Language%20Model%20%28LLM%29%20to%20generate%0Auser-defined%20driving%20videos.%20Specifically%2C%20an%20LLM%20interface%20is%20initially%0Aincorporated%20to%20convert%20a%20user%27s%20query%20into%20agent%20trajectories.%20Subsequently%2C%20a%0AHDMap%2C%20adhering%20to%20traffic%20regulations%2C%20is%20generated%20based%20on%20the%20trajectories.%0AUltimately%2C%20we%20propose%20the%20Unified%20Multi-View%20Model%20to%20enhance%20temporal%20and%0Aspatial%20coherence%20in%20the%20generated%20driving%20videos.%20DriveDreamer-2%20is%20the%20first%0Aworld%20model%20to%20generate%20customized%20driving%20videos%2C%20it%20can%20generate%20uncommon%0Adriving%20videos%20%28e.g.%2C%20vehicles%20abruptly%20cut%20in%29%20in%20a%20user-friendly%20manner.%0ABesides%2C%20experimental%20results%20demonstrate%20that%20the%20generated%20videos%20enhance%20the%0Atraining%20of%20driving%20perception%20methods%20%28e.g.%2C%203D%20detection%20and%20tracking%29.%0AFurthermore%2C%20video%20generation%20quality%20of%20DriveDreamer-2%20surpasses%20other%0Astate-of-the-art%20methods%2C%20showcasing%20FID%20and%20FVD%20scores%20of%2011.2%20and%2055.7%2C%0Arepresenting%20relative%20improvements%20of%2030%25%20and%2050%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06845v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveDreamer-2%3A%20LLM-Enhanced%20World%20Models%20for%20Diverse%20Driving%20Video%0A%20%20Generation&entry.906535625=Guosheng%20Zhao%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Xinze%20Chen%20and%20Guan%20Huang%20and%20Xiaoyi%20Bao%20and%20Xingang%20Wang&entry.1292438233=%20%20World%20models%20have%20demonstrated%20superiority%20in%20autonomous%20driving%2C%0Aparticularly%20in%20the%20generation%20of%20multi-view%20driving%20videos.%20However%2C%0Asignificant%20challenges%20still%20exist%20in%20generating%20customized%20driving%20videos.%20In%0Athis%20paper%2C%20we%20propose%20DriveDreamer-2%2C%20which%20builds%20upon%20the%20framework%20of%0ADriveDreamer%20and%20incorporates%20a%20Large%20Language%20Model%20%28LLM%29%20to%20generate%0Auser-defined%20driving%20videos.%20Specifically%2C%20an%20LLM%20interface%20is%20initially%0Aincorporated%20to%20convert%20a%20user%27s%20query%20into%20agent%20trajectories.%20Subsequently%2C%20a%0AHDMap%2C%20adhering%20to%20traffic%20regulations%2C%20is%20generated%20based%20on%20the%20trajectories.%0AUltimately%2C%20we%20propose%20the%20Unified%20Multi-View%20Model%20to%20enhance%20temporal%20and%0Aspatial%20coherence%20in%20the%20generated%20driving%20videos.%20DriveDreamer-2%20is%20the%20first%0Aworld%20model%20to%20generate%20customized%20driving%20videos%2C%20it%20can%20generate%20uncommon%0Adriving%20videos%20%28e.g.%2C%20vehicles%20abruptly%20cut%20in%29%20in%20a%20user-friendly%20manner.%0ABesides%2C%20experimental%20results%20demonstrate%20that%20the%20generated%20videos%20enhance%20the%0Atraining%20of%20driving%20perception%20methods%20%28e.g.%2C%203D%20detection%20and%20tracking%29.%0AFurthermore%2C%20video%20generation%20quality%20of%20DriveDreamer-2%20surpasses%20other%0Astate-of-the-art%20methods%2C%20showcasing%20FID%20and%20FVD%20scores%20of%2011.2%20and%2055.7%2C%0Arepresenting%20relative%20improvements%20of%2030%25%20and%2050%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06845v1&entry.124074799=Read"},
{"title": "Sharpened Lazy Incremental Quasi-Newton Method", "author": "Aakash Lahoti and Spandan Senapati and Ketan Rajawat and Alec Koppel", "abstract": "  The problem of minimizing the sum of $n$ functions in $d$ dimensions is\nubiquitous in machine learning and statistics. In many applications where the\nnumber of observations $n$ is large, it is necessary to use incremental or\nstochastic methods, as their per-iteration cost is independent of $n$. Of\nthese, Quasi-Newton (QN) methods strike a balance between the per-iteration\ncost and the convergence rate. Specifically, they exhibit a superlinear rate\nwith $O(d^2)$ cost in contrast to the linear rate of first-order methods with\n$O(d)$ cost and the quadratic rate of second-order methods with $O(d^3)$ cost.\nHowever, existing incremental methods have notable shortcomings: Incremental\nQuasi-Newton (IQN) only exhibits asymptotic superlinear convergence. In\ncontrast, Incremental Greedy BFGS (IGS) offers explicit superlinear convergence\nbut suffers from poor empirical performance and has a per-iteration cost of\n$O(d^3)$. To address these issues, we introduce the Sharpened Lazy Incremental\nQuasi-Newton Method (SLIQN) that achieves the best of both worlds: an explicit\nsuperlinear convergence rate, and superior empirical performance at a\nper-iteration $O(d^2)$ cost. SLIQN features two key changes: first, it\nincorporates a hybrid strategy of using both classic and greedy BFGS updates,\nallowing it to empirically outperform both IQN and IGS. Second, it employs a\nclever constant multiplicative factor along with a lazy propagation strategy,\nwhich enables it to have a cost of $O(d^2)$. Additionally, our experiments\ndemonstrate the superiority of SLIQN over other incremental and stochastic\nQuasi-Newton variants and establish its competitiveness with second-order\nincremental methods.\n", "link": "http://arxiv.org/abs/2305.17283v2", "date": "2024-03-11", "relevancy": 2.2988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4552}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4328}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Sharpened%20Lazy%20Incremental%20Quasi-Newton%20Method&body=Title%3A%20Sharpened%20Lazy%20Incremental%20Quasi-Newton%20Method%0AAuthor%3A%20Aakash%20Lahoti%20and%20Spandan%20Senapati%20and%20Ketan%20Rajawat%20and%20Alec%20Koppel%0AAbstract%3A%20%20%20The%20problem%20of%20minimizing%20the%20sum%20of%20%24n%24%20functions%20in%20%24d%24%20dimensions%20is%0Aubiquitous%20in%20machine%20learning%20and%20statistics.%20In%20many%20applications%20where%20the%0Anumber%20of%20observations%20%24n%24%20is%20large%2C%20it%20is%20necessary%20to%20use%20incremental%20or%0Astochastic%20methods%2C%20as%20their%20per-iteration%20cost%20is%20independent%20of%20%24n%24.%20Of%0Athese%2C%20Quasi-Newton%20%28QN%29%20methods%20strike%20a%20balance%20between%20the%20per-iteration%0Acost%20and%20the%20convergence%20rate.%20Specifically%2C%20they%20exhibit%20a%20superlinear%20rate%0Awith%20%24O%28d%5E2%29%24%20cost%20in%20contrast%20to%20the%20linear%20rate%20of%20first-order%20methods%20with%0A%24O%28d%29%24%20cost%20and%20the%20quadratic%20rate%20of%20second-order%20methods%20with%20%24O%28d%5E3%29%24%20cost.%0AHowever%2C%20existing%20incremental%20methods%20have%20notable%20shortcomings%3A%20Incremental%0AQuasi-Newton%20%28IQN%29%20only%20exhibits%20asymptotic%20superlinear%20convergence.%20In%0Acontrast%2C%20Incremental%20Greedy%20BFGS%20%28IGS%29%20offers%20explicit%20superlinear%20convergence%0Abut%20suffers%20from%20poor%20empirical%20performance%20and%20has%20a%20per-iteration%20cost%20of%0A%24O%28d%5E3%29%24.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20Sharpened%20Lazy%20Incremental%0AQuasi-Newton%20Method%20%28SLIQN%29%20that%20achieves%20the%20best%20of%20both%20worlds%3A%20an%20explicit%0Asuperlinear%20convergence%20rate%2C%20and%20superior%20empirical%20performance%20at%20a%0Aper-iteration%20%24O%28d%5E2%29%24%20cost.%20SLIQN%20features%20two%20key%20changes%3A%20first%2C%20it%0Aincorporates%20a%20hybrid%20strategy%20of%20using%20both%20classic%20and%20greedy%20BFGS%20updates%2C%0Aallowing%20it%20to%20empirically%20outperform%20both%20IQN%20and%20IGS.%20Second%2C%20it%20employs%20a%0Aclever%20constant%20multiplicative%20factor%20along%20with%20a%20lazy%20propagation%20strategy%2C%0Awhich%20enables%20it%20to%20have%20a%20cost%20of%20%24O%28d%5E2%29%24.%20Additionally%2C%20our%20experiments%0Ademonstrate%20the%20superiority%20of%20SLIQN%20over%20other%20incremental%20and%20stochastic%0AQuasi-Newton%20variants%20and%20establish%20its%20competitiveness%20with%20second-order%0Aincremental%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17283v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharpened%20Lazy%20Incremental%20Quasi-Newton%20Method&entry.906535625=Aakash%20Lahoti%20and%20Spandan%20Senapati%20and%20Ketan%20Rajawat%20and%20Alec%20Koppel&entry.1292438233=%20%20The%20problem%20of%20minimizing%20the%20sum%20of%20%24n%24%20functions%20in%20%24d%24%20dimensions%20is%0Aubiquitous%20in%20machine%20learning%20and%20statistics.%20In%20many%20applications%20where%20the%0Anumber%20of%20observations%20%24n%24%20is%20large%2C%20it%20is%20necessary%20to%20use%20incremental%20or%0Astochastic%20methods%2C%20as%20their%20per-iteration%20cost%20is%20independent%20of%20%24n%24.%20Of%0Athese%2C%20Quasi-Newton%20%28QN%29%20methods%20strike%20a%20balance%20between%20the%20per-iteration%0Acost%20and%20the%20convergence%20rate.%20Specifically%2C%20they%20exhibit%20a%20superlinear%20rate%0Awith%20%24O%28d%5E2%29%24%20cost%20in%20contrast%20to%20the%20linear%20rate%20of%20first-order%20methods%20with%0A%24O%28d%29%24%20cost%20and%20the%20quadratic%20rate%20of%20second-order%20methods%20with%20%24O%28d%5E3%29%24%20cost.%0AHowever%2C%20existing%20incremental%20methods%20have%20notable%20shortcomings%3A%20Incremental%0AQuasi-Newton%20%28IQN%29%20only%20exhibits%20asymptotic%20superlinear%20convergence.%20In%0Acontrast%2C%20Incremental%20Greedy%20BFGS%20%28IGS%29%20offers%20explicit%20superlinear%20convergence%0Abut%20suffers%20from%20poor%20empirical%20performance%20and%20has%20a%20per-iteration%20cost%20of%0A%24O%28d%5E3%29%24.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20Sharpened%20Lazy%20Incremental%0AQuasi-Newton%20Method%20%28SLIQN%29%20that%20achieves%20the%20best%20of%20both%20worlds%3A%20an%20explicit%0Asuperlinear%20convergence%20rate%2C%20and%20superior%20empirical%20performance%20at%20a%0Aper-iteration%20%24O%28d%5E2%29%24%20cost.%20SLIQN%20features%20two%20key%20changes%3A%20first%2C%20it%0Aincorporates%20a%20hybrid%20strategy%20of%20using%20both%20classic%20and%20greedy%20BFGS%20updates%2C%0Aallowing%20it%20to%20empirically%20outperform%20both%20IQN%20and%20IGS.%20Second%2C%20it%20employs%20a%0Aclever%20constant%20multiplicative%20factor%20along%20with%20a%20lazy%20propagation%20strategy%2C%0Awhich%20enables%20it%20to%20have%20a%20cost%20of%20%24O%28d%5E2%29%24.%20Additionally%2C%20our%20experiments%0Ademonstrate%20the%20superiority%20of%20SLIQN%20over%20other%20incremental%20and%20stochastic%0AQuasi-Newton%20variants%20and%20establish%20its%20competitiveness%20with%20second-order%0Aincremental%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17283v2&entry.124074799=Read"},
{"title": "Split to Merge: Unifying Separated Modalities for Unsupervised Domain\n  Adaptation", "author": "Xinyao Li and Yuke Li and Zhekai Du and Fengling Li and Ke Lu and Jingjing Li", "abstract": "  Large vision-language models (VLMs) like CLIP have demonstrated good\nzero-shot learning performance in the unsupervised domain adaptation task. Yet,\nmost transfer approaches for VLMs focus on either the language or visual\nbranches, overlooking the nuanced interplay between both modalities. In this\nwork, we introduce a Unified Modality Separation (UniMoS) framework for\nunsupervised domain adaptation. Leveraging insights from modality gap studies,\nwe craft a nimble modality separation network that distinctly disentangles\nCLIP's features into language-associated and vision-associated components. Our\nproposed Modality-Ensemble Training (MET) method fosters the exchange of\nmodality-agnostic information while maintaining modality-specific nuances. We\nalign features across domains using a modality discriminator. Comprehensive\nevaluations on three benchmarks reveal our approach sets a new state-of-the-art\nwith minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS\n", "link": "http://arxiv.org/abs/2403.06946v1", "date": "2024-03-11", "relevancy": 2.296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6457}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5251}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5219}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Split%20to%20Merge%3A%20Unifying%20Separated%20Modalities%20for%20Unsupervised%20Domain%0A%20%20Adaptation&body=Title%3A%20Split%20to%20Merge%3A%20Unifying%20Separated%20Modalities%20for%20Unsupervised%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Xinyao%20Li%20and%20Yuke%20Li%20and%20Zhekai%20Du%20and%20Fengling%20Li%20and%20Ke%20Lu%20and%20Jingjing%20Li%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20have%20demonstrated%20good%0Azero-shot%20learning%20performance%20in%20the%20unsupervised%20domain%20adaptation%20task.%20Yet%2C%0Amost%20transfer%20approaches%20for%20VLMs%20focus%20on%20either%20the%20language%20or%20visual%0Abranches%2C%20overlooking%20the%20nuanced%20interplay%20between%20both%20modalities.%20In%20this%0Awork%2C%20we%20introduce%20a%20Unified%20Modality%20Separation%20%28UniMoS%29%20framework%20for%0Aunsupervised%20domain%20adaptation.%20Leveraging%20insights%20from%20modality%20gap%20studies%2C%0Awe%20craft%20a%20nimble%20modality%20separation%20network%20that%20distinctly%20disentangles%0ACLIP%27s%20features%20into%20language-associated%20and%20vision-associated%20components.%20Our%0Aproposed%20Modality-Ensemble%20Training%20%28MET%29%20method%20fosters%20the%20exchange%20of%0Amodality-agnostic%20information%20while%20maintaining%20modality-specific%20nuances.%20We%0Aalign%20features%20across%20domains%20using%20a%20modality%20discriminator.%20Comprehensive%0Aevaluations%20on%20three%20benchmarks%20reveal%20our%20approach%20sets%20a%20new%20state-of-the-art%0Awith%20minimal%20computational%20costs.%20Code%3A%20https%3A//github.com/TL-UESTC/UniMoS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06946v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Split%20to%20Merge%3A%20Unifying%20Separated%20Modalities%20for%20Unsupervised%20Domain%0A%20%20Adaptation&entry.906535625=Xinyao%20Li%20and%20Yuke%20Li%20and%20Zhekai%20Du%20and%20Fengling%20Li%20and%20Ke%20Lu%20and%20Jingjing%20Li&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20have%20demonstrated%20good%0Azero-shot%20learning%20performance%20in%20the%20unsupervised%20domain%20adaptation%20task.%20Yet%2C%0Amost%20transfer%20approaches%20for%20VLMs%20focus%20on%20either%20the%20language%20or%20visual%0Abranches%2C%20overlooking%20the%20nuanced%20interplay%20between%20both%20modalities.%20In%20this%0Awork%2C%20we%20introduce%20a%20Unified%20Modality%20Separation%20%28UniMoS%29%20framework%20for%0Aunsupervised%20domain%20adaptation.%20Leveraging%20insights%20from%20modality%20gap%20studies%2C%0Awe%20craft%20a%20nimble%20modality%20separation%20network%20that%20distinctly%20disentangles%0ACLIP%27s%20features%20into%20language-associated%20and%20vision-associated%20components.%20Our%0Aproposed%20Modality-Ensemble%20Training%20%28MET%29%20method%20fosters%20the%20exchange%20of%0Amodality-agnostic%20information%20while%20maintaining%20modality-specific%20nuances.%20We%0Aalign%20features%20across%20domains%20using%20a%20modality%20discriminator.%20Comprehensive%0Aevaluations%20on%20three%20benchmarks%20reveal%20our%20approach%20sets%20a%20new%20state-of-the-art%0Awith%20minimal%20computational%20costs.%20Code%3A%20https%3A//github.com/TL-UESTC/UniMoS%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06946v1&entry.124074799=Read"},
{"title": "VideoMamba: State Space Model for Efficient Video Understanding", "author": "Kunchang Li and Xinhao Li and Yi Wang and Yinan He and Yali Wang and Limin Wang and Yu Qiao", "abstract": "  Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.\n", "link": "http://arxiv.org/abs/2403.06977v1", "date": "2024-03-11", "relevancy": 2.2956, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.593}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20VideoMamba%3A%20State%20Space%20Model%20for%20Efficient%20Video%20Understanding&body=Title%3A%20VideoMamba%3A%20State%20Space%20Model%20for%20Efficient%20Video%20Understanding%0AAuthor%3A%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Yi%20Wang%20and%20Yinan%20He%20and%20Yali%20Wang%20and%20Limin%20Wang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Addressing%20the%20dual%20challenges%20of%20local%20redundancy%20and%20global%20dependencies%20in%0Avideo%20understanding%2C%20this%20work%20innovatively%20adapts%20the%20Mamba%20to%20the%20video%0Adomain.%20The%20proposed%20VideoMamba%20overcomes%20the%20limitations%20of%20existing%203D%0Aconvolution%20neural%20networks%20and%20video%20transformers.%20Its%20linear-complexity%0Aoperator%20enables%20efficient%20long-term%20modeling%2C%20which%20is%20crucial%20for%0Ahigh-resolution%20long%20video%20understanding.%20Extensive%20evaluations%20reveal%0AVideoMamba%27s%20four%20core%20abilities%3A%20%281%29%20Scalability%20in%20the%20visual%20domain%20without%0Aextensive%20dataset%20pretraining%2C%20thanks%20to%20a%20novel%20self-distillation%20technique%3B%0A%282%29%20Sensitivity%20for%20recognizing%20short-term%20actions%20even%20with%20fine-grained%0Amotion%20differences%3B%20%283%29%20Superiority%20in%20long-term%20video%20understanding%2C%0Ashowcasing%20significant%20advancements%20over%20traditional%20feature-based%20models%3B%20and%0A%284%29%20Compatibility%20with%20other%20modalities%2C%20demonstrating%20robustness%20in%0Amulti-modal%20contexts.%20Through%20these%20distinct%20advantages%2C%20VideoMamba%20sets%20a%20new%0Abenchmark%20for%20video%20understanding%2C%20offering%20a%20scalable%20and%20efficient%20solution%0Afor%20comprehensive%20video%20understanding.%20All%20the%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/VideoMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06977v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMamba%3A%20State%20Space%20Model%20for%20Efficient%20Video%20Understanding&entry.906535625=Kunchang%20Li%20and%20Xinhao%20Li%20and%20Yi%20Wang%20and%20Yinan%20He%20and%20Yali%20Wang%20and%20Limin%20Wang%20and%20Yu%20Qiao&entry.1292438233=%20%20Addressing%20the%20dual%20challenges%20of%20local%20redundancy%20and%20global%20dependencies%20in%0Avideo%20understanding%2C%20this%20work%20innovatively%20adapts%20the%20Mamba%20to%20the%20video%0Adomain.%20The%20proposed%20VideoMamba%20overcomes%20the%20limitations%20of%20existing%203D%0Aconvolution%20neural%20networks%20and%20video%20transformers.%20Its%20linear-complexity%0Aoperator%20enables%20efficient%20long-term%20modeling%2C%20which%20is%20crucial%20for%0Ahigh-resolution%20long%20video%20understanding.%20Extensive%20evaluations%20reveal%0AVideoMamba%27s%20four%20core%20abilities%3A%20%281%29%20Scalability%20in%20the%20visual%20domain%20without%0Aextensive%20dataset%20pretraining%2C%20thanks%20to%20a%20novel%20self-distillation%20technique%3B%0A%282%29%20Sensitivity%20for%20recognizing%20short-term%20actions%20even%20with%20fine-grained%0Amotion%20differences%3B%20%283%29%20Superiority%20in%20long-term%20video%20understanding%2C%0Ashowcasing%20significant%20advancements%20over%20traditional%20feature-based%20models%3B%20and%0A%284%29%20Compatibility%20with%20other%20modalities%2C%20demonstrating%20robustness%20in%0Amulti-modal%20contexts.%20Through%20these%20distinct%20advantages%2C%20VideoMamba%20sets%20a%20new%0Abenchmark%20for%20video%20understanding%2C%20offering%20a%20scalable%20and%20efficient%20solution%0Afor%20comprehensive%20video%20understanding.%20All%20the%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/VideoMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06977v1&entry.124074799=Read"},
{"title": "Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained\n  Models for Spatiotemporal Modeling", "author": "Wele Gedara Chaminda Bandara and Vishal M. Patel", "abstract": "  In this paper, we introduce Attention Prompt Tuning (APT) - a computationally\nefficient variant of prompt tuning for video-based applications such as action\nrecognition. Prompt tuning approaches involve injecting a set of learnable\nprompts along with data tokens during fine-tuning while keeping the backbone\nfrozen. This approach greatly reduces the number of learnable parameters\ncompared to full tuning. For image-based downstream tasks, normally a couple of\nlearnable prompts achieve results close to those of full tuning. However,\nvideos, which contain more complex spatiotemporal information, require hundreds\nof tunable prompts to achieve reasonably good results. This reduces the\nparameter efficiency observed in images and significantly increases latency and\nthe number of floating-point operations (FLOPs) during inference. To tackle\nthese issues, we directly inject the prompts into the keys and values of the\nnon-local attention mechanism within the transformer block. Additionally, we\nintroduce a novel prompt reparameterization technique to make APT more robust\nagainst hyperparameter selection. The proposed APT approach greatly reduces the\nnumber of FLOPs and latency while achieving a significant performance boost\nover the existing parameter-efficient tuning methods on UCF101, HMDB51, and\nSSv2 datasets for action recognition. The code and pre-trained models are\navailable at https://github.com/wgcban/apt\n", "link": "http://arxiv.org/abs/2403.06978v1", "date": "2024-03-11", "relevancy": 2.2935, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5983}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5766}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5472}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Attention%20Prompt%20Tuning%3A%20Parameter-efficient%20Adaptation%20of%20Pre-trained%0A%20%20Models%20for%20Spatiotemporal%20Modeling&body=Title%3A%20Attention%20Prompt%20Tuning%3A%20Parameter-efficient%20Adaptation%20of%20Pre-trained%0A%20%20Models%20for%20Spatiotemporal%20Modeling%0AAuthor%3A%20Wele%20Gedara%20Chaminda%20Bandara%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Attention%20Prompt%20Tuning%20%28APT%29%20-%20a%20computationally%0Aefficient%20variant%20of%20prompt%20tuning%20for%20video-based%20applications%20such%20as%20action%0Arecognition.%20Prompt%20tuning%20approaches%20involve%20injecting%20a%20set%20of%20learnable%0Aprompts%20along%20with%20data%20tokens%20during%20fine-tuning%20while%20keeping%20the%20backbone%0Afrozen.%20This%20approach%20greatly%20reduces%20the%20number%20of%20learnable%20parameters%0Acompared%20to%20full%20tuning.%20For%20image-based%20downstream%20tasks%2C%20normally%20a%20couple%20of%0Alearnable%20prompts%20achieve%20results%20close%20to%20those%20of%20full%20tuning.%20However%2C%0Avideos%2C%20which%20contain%20more%20complex%20spatiotemporal%20information%2C%20require%20hundreds%0Aof%20tunable%20prompts%20to%20achieve%20reasonably%20good%20results.%20This%20reduces%20the%0Aparameter%20efficiency%20observed%20in%20images%20and%20significantly%20increases%20latency%20and%0Athe%20number%20of%20floating-point%20operations%20%28FLOPs%29%20during%20inference.%20To%20tackle%0Athese%20issues%2C%20we%20directly%20inject%20the%20prompts%20into%20the%20keys%20and%20values%20of%20the%0Anon-local%20attention%20mechanism%20within%20the%20transformer%20block.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20prompt%20reparameterization%20technique%20to%20make%20APT%20more%20robust%0Aagainst%20hyperparameter%20selection.%20The%20proposed%20APT%20approach%20greatly%20reduces%20the%0Anumber%20of%20FLOPs%20and%20latency%20while%20achieving%20a%20significant%20performance%20boost%0Aover%20the%20existing%20parameter-efficient%20tuning%20methods%20on%20UCF101%2C%20HMDB51%2C%20and%0ASSv2%20datasets%20for%20action%20recognition.%20The%20code%20and%20pre-trained%20models%20are%0Aavailable%20at%20https%3A//github.com/wgcban/apt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06978v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Prompt%20Tuning%3A%20Parameter-efficient%20Adaptation%20of%20Pre-trained%0A%20%20Models%20for%20Spatiotemporal%20Modeling&entry.906535625=Wele%20Gedara%20Chaminda%20Bandara%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Attention%20Prompt%20Tuning%20%28APT%29%20-%20a%20computationally%0Aefficient%20variant%20of%20prompt%20tuning%20for%20video-based%20applications%20such%20as%20action%0Arecognition.%20Prompt%20tuning%20approaches%20involve%20injecting%20a%20set%20of%20learnable%0Aprompts%20along%20with%20data%20tokens%20during%20fine-tuning%20while%20keeping%20the%20backbone%0Afrozen.%20This%20approach%20greatly%20reduces%20the%20number%20of%20learnable%20parameters%0Acompared%20to%20full%20tuning.%20For%20image-based%20downstream%20tasks%2C%20normally%20a%20couple%20of%0Alearnable%20prompts%20achieve%20results%20close%20to%20those%20of%20full%20tuning.%20However%2C%0Avideos%2C%20which%20contain%20more%20complex%20spatiotemporal%20information%2C%20require%20hundreds%0Aof%20tunable%20prompts%20to%20achieve%20reasonably%20good%20results.%20This%20reduces%20the%0Aparameter%20efficiency%20observed%20in%20images%20and%20significantly%20increases%20latency%20and%0Athe%20number%20of%20floating-point%20operations%20%28FLOPs%29%20during%20inference.%20To%20tackle%0Athese%20issues%2C%20we%20directly%20inject%20the%20prompts%20into%20the%20keys%20and%20values%20of%20the%0Anon-local%20attention%20mechanism%20within%20the%20transformer%20block.%20Additionally%2C%20we%0Aintroduce%20a%20novel%20prompt%20reparameterization%20technique%20to%20make%20APT%20more%20robust%0Aagainst%20hyperparameter%20selection.%20The%20proposed%20APT%20approach%20greatly%20reduces%20the%0Anumber%20of%20FLOPs%20and%20latency%20while%20achieving%20a%20significant%20performance%20boost%0Aover%20the%20existing%20parameter-efficient%20tuning%20methods%20on%20UCF101%2C%20HMDB51%2C%20and%0ASSv2%20datasets%20for%20action%20recognition.%20The%20code%20and%20pre-trained%20models%20are%0Aavailable%20at%20https%3A//github.com/wgcban/apt%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06978v1&entry.124074799=Read"},
{"title": "Bayesian Diffusion Models for 3D Shape Reconstruction", "author": "Haiyang Xu and Yu Lei and Zeyuan Chen and Xiang Zhang and Yue Zhao and Yilin Wang and Zhuowen Tu", "abstract": "  We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.\n", "link": "http://arxiv.org/abs/2403.06973v1", "date": "2024-03-11", "relevancy": 2.2731, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Diffusion%20Models%20for%203D%20Shape%20Reconstruction&body=Title%3A%20Bayesian%20Diffusion%20Models%20for%203D%20Shape%20Reconstruction%0AAuthor%3A%20Haiyang%20Xu%20and%20Yu%20Lei%20and%20Zeyuan%20Chen%20and%20Xiang%20Zhang%20and%20Yue%20Zhao%20and%20Yilin%20Wang%20and%20Zhuowen%20Tu%0AAbstract%3A%20%20%20We%20present%20Bayesian%20Diffusion%20Models%20%28BDM%29%2C%20a%20prediction%20algorithm%20that%0Aperforms%20effective%20Bayesian%20inference%20by%20tightly%20coupling%20the%20top-down%20%28prior%29%0Ainformation%20with%20the%20bottom-up%20%28data-driven%29%20procedure%20via%20joint%20diffusion%0Aprocesses.%20We%20show%20the%20effectiveness%20of%20BDM%20on%20the%203D%20shape%20reconstruction%0Atask.%20Compared%20to%20prototypical%20deep%20learning%20data-driven%20approaches%20trained%20on%0Apaired%20%28supervised%29%20data-labels%20%28e.g.%20image-point%20clouds%29%20datasets%2C%20our%20BDM%0Abrings%20in%20rich%20prior%20information%20from%20standalone%20labels%20%28e.g.%20point%20clouds%29%20to%0Aimprove%20the%20bottom-up%203D%20reconstruction.%20As%20opposed%20to%20the%20standard%20Bayesian%0Aframeworks%20where%20explicit%20prior%20and%20likelihood%20are%20required%20for%20the%20inference%2C%0ABDM%20performs%20seamless%20information%20fusion%20via%20coupled%20diffusion%20processes%20with%0Alearned%20gradient%20computation%20networks.%20The%20specialty%20of%20our%20BDM%20lies%20in%20its%0Acapability%20to%20engage%20the%20active%20and%20effective%20information%20exchange%20and%20fusion%0Aof%20the%20top-down%20and%20bottom-up%20processes%20where%20each%20itself%20is%20a%20diffusion%0Aprocess.%20We%20demonstrate%20state-of-the-art%20results%20on%20both%20synthetic%20and%0Areal-world%20benchmarks%20for%203D%20shape%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06973v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Diffusion%20Models%20for%203D%20Shape%20Reconstruction&entry.906535625=Haiyang%20Xu%20and%20Yu%20Lei%20and%20Zeyuan%20Chen%20and%20Xiang%20Zhang%20and%20Yue%20Zhao%20and%20Yilin%20Wang%20and%20Zhuowen%20Tu&entry.1292438233=%20%20We%20present%20Bayesian%20Diffusion%20Models%20%28BDM%29%2C%20a%20prediction%20algorithm%20that%0Aperforms%20effective%20Bayesian%20inference%20by%20tightly%20coupling%20the%20top-down%20%28prior%29%0Ainformation%20with%20the%20bottom-up%20%28data-driven%29%20procedure%20via%20joint%20diffusion%0Aprocesses.%20We%20show%20the%20effectiveness%20of%20BDM%20on%20the%203D%20shape%20reconstruction%0Atask.%20Compared%20to%20prototypical%20deep%20learning%20data-driven%20approaches%20trained%20on%0Apaired%20%28supervised%29%20data-labels%20%28e.g.%20image-point%20clouds%29%20datasets%2C%20our%20BDM%0Abrings%20in%20rich%20prior%20information%20from%20standalone%20labels%20%28e.g.%20point%20clouds%29%20to%0Aimprove%20the%20bottom-up%203D%20reconstruction.%20As%20opposed%20to%20the%20standard%20Bayesian%0Aframeworks%20where%20explicit%20prior%20and%20likelihood%20are%20required%20for%20the%20inference%2C%0ABDM%20performs%20seamless%20information%20fusion%20via%20coupled%20diffusion%20processes%20with%0Alearned%20gradient%20computation%20networks.%20The%20specialty%20of%20our%20BDM%20lies%20in%20its%0Acapability%20to%20engage%20the%20active%20and%20effective%20information%20exchange%20and%20fusion%0Aof%20the%20top-down%20and%20bottom-up%20processes%20where%20each%20itself%20is%20a%20diffusion%0Aprocess.%20We%20demonstrate%20state-of-the-art%20results%20on%20both%20synthetic%20and%0Areal-world%20benchmarks%20for%203D%20shape%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06973v1&entry.124074799=Read"},
{"title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data", "author": "Jialu Li and Jaemin Cho and Yi-Lin Sung and Jaehong Yoon and Mohit Bansal", "abstract": "  Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.\n", "link": "http://arxiv.org/abs/2403.06952v1", "date": "2024-03-11", "relevancy": 2.2635, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.585}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5596}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5493}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20SELMA%3A%20Learning%20and%20Merging%20Skill-Specific%20Text-to-Image%20Experts%20with%0A%20%20Auto-Generated%20Data&body=Title%3A%20SELMA%3A%20Learning%20and%20Merging%20Skill-Specific%20Text-to-Image%20Experts%20with%0A%20%20Auto-Generated%20Data%0AAuthor%3A%20Jialu%20Li%20and%20Jaemin%20Cho%20and%20Yi-Lin%20Sung%20and%20Jaehong%20Yoon%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Recent%20text-to-image%20%28T2I%29%20generation%20models%20have%20demonstrated%20impressive%0Acapabilities%20in%20creating%20images%20from%20text%20descriptions.%20However%2C%20these%20T2I%0Ageneration%20models%20often%20fall%20short%20of%20generating%20images%20that%20precisely%20match%0Athe%20details%20of%20the%20text%20inputs%2C%20such%20as%20incorrect%20spatial%20relationship%20or%0Amissing%20objects.%20In%20this%20paper%2C%20we%20introduce%20SELMA%3A%20Skill-Specific%20Expert%0ALearning%20and%20Merging%20with%20Auto-Generated%20Data%2C%20a%20novel%20paradigm%20to%20improve%20the%0Afaithfulness%20of%20T2I%20models%20by%20fine-tuning%20models%20on%20automatically%20generated%2C%0Amulti-skill%20image-text%20datasets%2C%20with%20skill-specific%20expert%20learning%20and%0Amerging.%20First%2C%20SELMA%20leverages%20an%20LLM%27s%20in-context%20learning%20capability%20to%0Agenerate%20multiple%20datasets%20of%20text%20prompts%20that%20can%20teach%20different%20skills%2C%20and%0Athen%20generates%20the%20images%20with%20a%20T2I%20model%20based%20on%20the%20prompts.%20Next%2C%20SELMA%0Aadapts%20the%20T2I%20model%20to%20the%20new%20skills%20by%20learning%20multiple%20single-skill%20LoRA%0A%28low-rank%20adaptation%29%20experts%20followed%20by%20expert%20merging.%20Our%20independent%0Aexpert%20fine-tuning%20specializes%20multiple%20models%20for%20different%20skills%2C%20and%20expert%0Amerging%20helps%20build%20a%20joint%20multi-skill%20T2I%20model%20that%20can%20generate%20faithful%0Aimages%20given%20diverse%20text%20prompts%2C%20while%20mitigating%20the%20knowledge%20conflict%20from%0Adifferent%20datasets.%20We%20empirically%20demonstrate%20that%20SELMA%20significantly%0Aimproves%20the%20semantic%20alignment%20and%20text%20faithfulness%20of%20state-of-the-art%20T2I%0Adiffusion%20models%20on%20multiple%20benchmarks%20%28%2B2.1%25%20on%20TIFA%20and%20%2B6.9%25%20on%20DSG%29%2C%20human%0Apreference%20metrics%20%28PickScore%2C%20ImageReward%2C%20and%20HPS%29%2C%20as%20well%20as%20human%0Aevaluation.%20Moreover%2C%20fine-tuning%20with%20image-text%20pairs%20auto-collected%20via%0ASELMA%20shows%20comparable%20performance%20to%20fine-tuning%20with%20ground%20truth%20data.%0ALastly%2C%20we%20show%20that%20fine-tuning%20with%20images%20from%20a%20weaker%20T2I%20model%20can%20help%0Aimprove%20the%20generation%20quality%20of%20a%20stronger%20T2I%20model%2C%20suggesting%20promising%0Aweak-to-strong%20generalization%20in%20T2I%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06952v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELMA%3A%20Learning%20and%20Merging%20Skill-Specific%20Text-to-Image%20Experts%20with%0A%20%20Auto-Generated%20Data&entry.906535625=Jialu%20Li%20and%20Jaemin%20Cho%20and%20Yi-Lin%20Sung%20and%20Jaehong%20Yoon%20and%20Mohit%20Bansal&entry.1292438233=%20%20Recent%20text-to-image%20%28T2I%29%20generation%20models%20have%20demonstrated%20impressive%0Acapabilities%20in%20creating%20images%20from%20text%20descriptions.%20However%2C%20these%20T2I%0Ageneration%20models%20often%20fall%20short%20of%20generating%20images%20that%20precisely%20match%0Athe%20details%20of%20the%20text%20inputs%2C%20such%20as%20incorrect%20spatial%20relationship%20or%0Amissing%20objects.%20In%20this%20paper%2C%20we%20introduce%20SELMA%3A%20Skill-Specific%20Expert%0ALearning%20and%20Merging%20with%20Auto-Generated%20Data%2C%20a%20novel%20paradigm%20to%20improve%20the%0Afaithfulness%20of%20T2I%20models%20by%20fine-tuning%20models%20on%20automatically%20generated%2C%0Amulti-skill%20image-text%20datasets%2C%20with%20skill-specific%20expert%20learning%20and%0Amerging.%20First%2C%20SELMA%20leverages%20an%20LLM%27s%20in-context%20learning%20capability%20to%0Agenerate%20multiple%20datasets%20of%20text%20prompts%20that%20can%20teach%20different%20skills%2C%20and%0Athen%20generates%20the%20images%20with%20a%20T2I%20model%20based%20on%20the%20prompts.%20Next%2C%20SELMA%0Aadapts%20the%20T2I%20model%20to%20the%20new%20skills%20by%20learning%20multiple%20single-skill%20LoRA%0A%28low-rank%20adaptation%29%20experts%20followed%20by%20expert%20merging.%20Our%20independent%0Aexpert%20fine-tuning%20specializes%20multiple%20models%20for%20different%20skills%2C%20and%20expert%0Amerging%20helps%20build%20a%20joint%20multi-skill%20T2I%20model%20that%20can%20generate%20faithful%0Aimages%20given%20diverse%20text%20prompts%2C%20while%20mitigating%20the%20knowledge%20conflict%20from%0Adifferent%20datasets.%20We%20empirically%20demonstrate%20that%20SELMA%20significantly%0Aimproves%20the%20semantic%20alignment%20and%20text%20faithfulness%20of%20state-of-the-art%20T2I%0Adiffusion%20models%20on%20multiple%20benchmarks%20%28%2B2.1%25%20on%20TIFA%20and%20%2B6.9%25%20on%20DSG%29%2C%20human%0Apreference%20metrics%20%28PickScore%2C%20ImageReward%2C%20and%20HPS%29%2C%20as%20well%20as%20human%0Aevaluation.%20Moreover%2C%20fine-tuning%20with%20image-text%20pairs%20auto-collected%20via%0ASELMA%20shows%20comparable%20performance%20to%20fine-tuning%20with%20ground%20truth%20data.%0ALastly%2C%20we%20show%20that%20fine-tuning%20with%20images%20from%20a%20weaker%20T2I%20model%20can%20help%0Aimprove%20the%20generation%20quality%20of%20a%20stronger%20T2I%20model%2C%20suggesting%20promising%0Aweak-to-strong%20generalization%20in%20T2I%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06952v1&entry.124074799=Read"},
{"title": "An Autonomous Driving Model Integrated with BEV-V2X Perception, Fusion\n  Prediction of Motion and Occupancy, and Driving Planning, in Complex Traffic\n  Intersections", "author": "Fukang Li and Wenlin Ou and Kunpeng Gao and Yuwen Pang and Yifei Li and Henry Fan", "abstract": "  The comprehensiveness of vehicle-to-everything (V2X) recognition enriches and\nholistically shapes the global Birds-Eye-View (BEV) perception, incorporating\nrich semantics and integrating driving scene information, thereby serving\nfeatures of vehicle state prediction, decision-making and driving planning.\nUtilizing V2X message sets to form BEV map proves to be an effective perception\nmethod for connected and automated vehicles (CAVs). Specifically, Map Msg.\n(MAP), Signal Phase And Timing (SPAT) and Roadside Information (RSI)\ncontributes to the achievement of road connectivity, synchronized traffic\nsignal navigation and obstacle warning. Moreover, harnessing time-sequential\nBasic Safety Msg. (BSM) data from multiple vehicles allows for the real-time\nperception and future state prediction. Therefore, this paper develops a\ncomprehensive autonomous driving model that relies on BEV-V2X perception,\nInteracting Multiple model Unscented Kalman Filter (IMM-UKF)-based fusion\nprediction, and deep reinforcement learning (DRL)-based decision making and\nplanning. We integrated them into a DRL environment to develop an optimal set\nof unified driving behaviors that encompass obstacle avoidance, lane changes,\novertaking, turning maneuver, and synchronized traffic signal navigation.\nConsequently, a complex traffic intersection scenario was simulated, and the\nwell-trained model was applied for driving planning. The observed driving\nbehavior closely resembled that of an experienced driver, exhibiting\nanticipatory actions and revealing notable operational highlights of driving\npolicy.\n", "link": "http://arxiv.org/abs/2312.05104v2", "date": "2024-03-11", "relevancy": 2.2617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5668}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5633}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20An%20Autonomous%20Driving%20Model%20Integrated%20with%20BEV-V2X%20Perception%2C%20Fusion%0A%20%20Prediction%20of%20Motion%20and%20Occupancy%2C%20and%20Driving%20Planning%2C%20in%20Complex%20Traffic%0A%20%20Intersections&body=Title%3A%20An%20Autonomous%20Driving%20Model%20Integrated%20with%20BEV-V2X%20Perception%2C%20Fusion%0A%20%20Prediction%20of%20Motion%20and%20Occupancy%2C%20and%20Driving%20Planning%2C%20in%20Complex%20Traffic%0A%20%20Intersections%0AAuthor%3A%20Fukang%20Li%20and%20Wenlin%20Ou%20and%20Kunpeng%20Gao%20and%20Yuwen%20Pang%20and%20Yifei%20Li%20and%20Henry%20Fan%0AAbstract%3A%20%20%20The%20comprehensiveness%20of%20vehicle-to-everything%20%28V2X%29%20recognition%20enriches%20and%0Aholistically%20shapes%20the%20global%20Birds-Eye-View%20%28BEV%29%20perception%2C%20incorporating%0Arich%20semantics%20and%20integrating%20driving%20scene%20information%2C%20thereby%20serving%0Afeatures%20of%20vehicle%20state%20prediction%2C%20decision-making%20and%20driving%20planning.%0AUtilizing%20V2X%20message%20sets%20to%20form%20BEV%20map%20proves%20to%20be%20an%20effective%20perception%0Amethod%20for%20connected%20and%20automated%20vehicles%20%28CAVs%29.%20Specifically%2C%20Map%20Msg.%0A%28MAP%29%2C%20Signal%20Phase%20And%20Timing%20%28SPAT%29%20and%20Roadside%20Information%20%28RSI%29%0Acontributes%20to%20the%20achievement%20of%20road%20connectivity%2C%20synchronized%20traffic%0Asignal%20navigation%20and%20obstacle%20warning.%20Moreover%2C%20harnessing%20time-sequential%0ABasic%20Safety%20Msg.%20%28BSM%29%20data%20from%20multiple%20vehicles%20allows%20for%20the%20real-time%0Aperception%20and%20future%20state%20prediction.%20Therefore%2C%20this%20paper%20develops%20a%0Acomprehensive%20autonomous%20driving%20model%20that%20relies%20on%20BEV-V2X%20perception%2C%0AInteracting%20Multiple%20model%20Unscented%20Kalman%20Filter%20%28IMM-UKF%29-based%20fusion%0Aprediction%2C%20and%20deep%20reinforcement%20learning%20%28DRL%29-based%20decision%20making%20and%0Aplanning.%20We%20integrated%20them%20into%20a%20DRL%20environment%20to%20develop%20an%20optimal%20set%0Aof%20unified%20driving%20behaviors%20that%20encompass%20obstacle%20avoidance%2C%20lane%20changes%2C%0Aovertaking%2C%20turning%20maneuver%2C%20and%20synchronized%20traffic%20signal%20navigation.%0AConsequently%2C%20a%20complex%20traffic%20intersection%20scenario%20was%20simulated%2C%20and%20the%0Awell-trained%20model%20was%20applied%20for%20driving%20planning.%20The%20observed%20driving%0Abehavior%20closely%20resembled%20that%20of%20an%20experienced%20driver%2C%20exhibiting%0Aanticipatory%20actions%20and%20revealing%20notable%20operational%20highlights%20of%20driving%0Apolicy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05104v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Autonomous%20Driving%20Model%20Integrated%20with%20BEV-V2X%20Perception%2C%20Fusion%0A%20%20Prediction%20of%20Motion%20and%20Occupancy%2C%20and%20Driving%20Planning%2C%20in%20Complex%20Traffic%0A%20%20Intersections&entry.906535625=Fukang%20Li%20and%20Wenlin%20Ou%20and%20Kunpeng%20Gao%20and%20Yuwen%20Pang%20and%20Yifei%20Li%20and%20Henry%20Fan&entry.1292438233=%20%20The%20comprehensiveness%20of%20vehicle-to-everything%20%28V2X%29%20recognition%20enriches%20and%0Aholistically%20shapes%20the%20global%20Birds-Eye-View%20%28BEV%29%20perception%2C%20incorporating%0Arich%20semantics%20and%20integrating%20driving%20scene%20information%2C%20thereby%20serving%0Afeatures%20of%20vehicle%20state%20prediction%2C%20decision-making%20and%20driving%20planning.%0AUtilizing%20V2X%20message%20sets%20to%20form%20BEV%20map%20proves%20to%20be%20an%20effective%20perception%0Amethod%20for%20connected%20and%20automated%20vehicles%20%28CAVs%29.%20Specifically%2C%20Map%20Msg.%0A%28MAP%29%2C%20Signal%20Phase%20And%20Timing%20%28SPAT%29%20and%20Roadside%20Information%20%28RSI%29%0Acontributes%20to%20the%20achievement%20of%20road%20connectivity%2C%20synchronized%20traffic%0Asignal%20navigation%20and%20obstacle%20warning.%20Moreover%2C%20harnessing%20time-sequential%0ABasic%20Safety%20Msg.%20%28BSM%29%20data%20from%20multiple%20vehicles%20allows%20for%20the%20real-time%0Aperception%20and%20future%20state%20prediction.%20Therefore%2C%20this%20paper%20develops%20a%0Acomprehensive%20autonomous%20driving%20model%20that%20relies%20on%20BEV-V2X%20perception%2C%0AInteracting%20Multiple%20model%20Unscented%20Kalman%20Filter%20%28IMM-UKF%29-based%20fusion%0Aprediction%2C%20and%20deep%20reinforcement%20learning%20%28DRL%29-based%20decision%20making%20and%0Aplanning.%20We%20integrated%20them%20into%20a%20DRL%20environment%20to%20develop%20an%20optimal%20set%0Aof%20unified%20driving%20behaviors%20that%20encompass%20obstacle%20avoidance%2C%20lane%20changes%2C%0Aovertaking%2C%20turning%20maneuver%2C%20and%20synchronized%20traffic%20signal%20navigation.%0AConsequently%2C%20a%20complex%20traffic%20intersection%20scenario%20was%20simulated%2C%20and%20the%0Awell-trained%20model%20was%20applied%20for%20driving%20planning.%20The%20observed%20driving%0Abehavior%20closely%20resembled%20that%20of%20an%20experienced%20driver%2C%20exhibiting%0Aanticipatory%20actions%20and%20revealing%20notable%20operational%20highlights%20of%20driving%0Apolicy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05104v2&entry.124074799=Read"},
{"title": "Answering Diverse Questions via Text Attached with Key Audio-Visual\n  Clues", "author": "Qilang Ye and Zitong Yu and Xin Liu", "abstract": "  Audio-visual question answering (AVQA) requires reference to video content\nand auditory information, followed by correlating the question to predict the\nmost precise answer. Although mining deeper layers of audio-visual information\nto interact with questions facilitates the multimodal fusion process, the\nredundancy of audio-visual parameters tends to reduce the generalization of the\ninference engine to multiple question-answer pairs in a single video. Indeed,\nthe natural heterogeneous relationship between audiovisuals and text makes the\nperfect fusion challenging, to prevent high-level audio-visual semantics from\nweakening the network's adaptability to diverse question types, we propose a\nframework for performing mutual correlation distillation (MCD) to aid question\ninference. MCD is divided into three main steps: 1) firstly, the residual\nstructure is utilized to enhance the audio-visual soft associations based on\nself-attention, then key local audio-visual features relevant to the question\ncontext are captured hierarchically by shared aggregators and coupled in the\nform of clues with specific question vectors. 2) Secondly, knowledge\ndistillation is enforced to align audio-visual-text pairs in a shared latent\nspace to narrow the cross-modal semantic gap. 3) And finally, the audio-visual\ndependencies are decoupled by discarding the decision-level integrations. We\nevaluate the proposed method on two publicly available datasets containing\nmultiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show\nthat our method outperforms other state-of-the-art methods, and one interesting\nfinding behind is that removing deep audio-visual features during inference can\neffectively mitigate overfitting. The source code is released at\nhttp://github.com/rikeilong/MCD-forAVQA.\n", "link": "http://arxiv.org/abs/2403.06679v1", "date": "2024-03-11", "relevancy": 2.2579, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5832}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Answering%20Diverse%20Questions%20via%20Text%20Attached%20with%20Key%20Audio-Visual%0A%20%20Clues&body=Title%3A%20Answering%20Diverse%20Questions%20via%20Text%20Attached%20with%20Key%20Audio-Visual%0A%20%20Clues%0AAuthor%3A%20Qilang%20Ye%20and%20Zitong%20Yu%20and%20Xin%20Liu%0AAbstract%3A%20%20%20Audio-visual%20question%20answering%20%28AVQA%29%20requires%20reference%20to%20video%20content%0Aand%20auditory%20information%2C%20followed%20by%20correlating%20the%20question%20to%20predict%20the%0Amost%20precise%20answer.%20Although%20mining%20deeper%20layers%20of%20audio-visual%20information%0Ato%20interact%20with%20questions%20facilitates%20the%20multimodal%20fusion%20process%2C%20the%0Aredundancy%20of%20audio-visual%20parameters%20tends%20to%20reduce%20the%20generalization%20of%20the%0Ainference%20engine%20to%20multiple%20question-answer%20pairs%20in%20a%20single%20video.%20Indeed%2C%0Athe%20natural%20heterogeneous%20relationship%20between%20audiovisuals%20and%20text%20makes%20the%0Aperfect%20fusion%20challenging%2C%20to%20prevent%20high-level%20audio-visual%20semantics%20from%0Aweakening%20the%20network%27s%20adaptability%20to%20diverse%20question%20types%2C%20we%20propose%20a%0Aframework%20for%20performing%20mutual%20correlation%20distillation%20%28MCD%29%20to%20aid%20question%0Ainference.%20MCD%20is%20divided%20into%20three%20main%20steps%3A%201%29%20firstly%2C%20the%20residual%0Astructure%20is%20utilized%20to%20enhance%20the%20audio-visual%20soft%20associations%20based%20on%0Aself-attention%2C%20then%20key%20local%20audio-visual%20features%20relevant%20to%20the%20question%0Acontext%20are%20captured%20hierarchically%20by%20shared%20aggregators%20and%20coupled%20in%20the%0Aform%20of%20clues%20with%20specific%20question%20vectors.%202%29%20Secondly%2C%20knowledge%0Adistillation%20is%20enforced%20to%20align%20audio-visual-text%20pairs%20in%20a%20shared%20latent%0Aspace%20to%20narrow%20the%20cross-modal%20semantic%20gap.%203%29%20And%20finally%2C%20the%20audio-visual%0Adependencies%20are%20decoupled%20by%20discarding%20the%20decision-level%20integrations.%20We%0Aevaluate%20the%20proposed%20method%20on%20two%20publicly%20available%20datasets%20containing%0Amultiple%20question-and-answer%20pairs%2C%20i.e.%2C%20Music-AVQA%20and%20AVQA.%20Experiments%20show%0Athat%20our%20method%20outperforms%20other%20state-of-the-art%20methods%2C%20and%20one%20interesting%0Afinding%20behind%20is%20that%20removing%20deep%20audio-visual%20features%20during%20inference%20can%0Aeffectively%20mitigate%20overfitting.%20The%20source%20code%20is%20released%20at%0Ahttp%3A//github.com/rikeilong/MCD-forAVQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06679v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Answering%20Diverse%20Questions%20via%20Text%20Attached%20with%20Key%20Audio-Visual%0A%20%20Clues&entry.906535625=Qilang%20Ye%20and%20Zitong%20Yu%20and%20Xin%20Liu&entry.1292438233=%20%20Audio-visual%20question%20answering%20%28AVQA%29%20requires%20reference%20to%20video%20content%0Aand%20auditory%20information%2C%20followed%20by%20correlating%20the%20question%20to%20predict%20the%0Amost%20precise%20answer.%20Although%20mining%20deeper%20layers%20of%20audio-visual%20information%0Ato%20interact%20with%20questions%20facilitates%20the%20multimodal%20fusion%20process%2C%20the%0Aredundancy%20of%20audio-visual%20parameters%20tends%20to%20reduce%20the%20generalization%20of%20the%0Ainference%20engine%20to%20multiple%20question-answer%20pairs%20in%20a%20single%20video.%20Indeed%2C%0Athe%20natural%20heterogeneous%20relationship%20between%20audiovisuals%20and%20text%20makes%20the%0Aperfect%20fusion%20challenging%2C%20to%20prevent%20high-level%20audio-visual%20semantics%20from%0Aweakening%20the%20network%27s%20adaptability%20to%20diverse%20question%20types%2C%20we%20propose%20a%0Aframework%20for%20performing%20mutual%20correlation%20distillation%20%28MCD%29%20to%20aid%20question%0Ainference.%20MCD%20is%20divided%20into%20three%20main%20steps%3A%201%29%20firstly%2C%20the%20residual%0Astructure%20is%20utilized%20to%20enhance%20the%20audio-visual%20soft%20associations%20based%20on%0Aself-attention%2C%20then%20key%20local%20audio-visual%20features%20relevant%20to%20the%20question%0Acontext%20are%20captured%20hierarchically%20by%20shared%20aggregators%20and%20coupled%20in%20the%0Aform%20of%20clues%20with%20specific%20question%20vectors.%202%29%20Secondly%2C%20knowledge%0Adistillation%20is%20enforced%20to%20align%20audio-visual-text%20pairs%20in%20a%20shared%20latent%0Aspace%20to%20narrow%20the%20cross-modal%20semantic%20gap.%203%29%20And%20finally%2C%20the%20audio-visual%0Adependencies%20are%20decoupled%20by%20discarding%20the%20decision-level%20integrations.%20We%0Aevaluate%20the%20proposed%20method%20on%20two%20publicly%20available%20datasets%20containing%0Amultiple%20question-and-answer%20pairs%2C%20i.e.%2C%20Music-AVQA%20and%20AVQA.%20Experiments%20show%0Athat%20our%20method%20outperforms%20other%20state-of-the-art%20methods%2C%20and%20one%20interesting%0Afinding%20behind%20is%20that%20removing%20deep%20audio-visual%20features%20during%20inference%20can%0Aeffectively%20mitigate%20overfitting.%20The%20source%20code%20is%20released%20at%0Ahttp%3A//github.com/rikeilong/MCD-forAVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06679v1&entry.124074799=Read"},
{"title": "Multimodal Transformers for Real-Time Surgical Activity Prediction", "author": "Keshara Weerasinghe and Seyed Hamid Reza Roodabeh and Kay Hutchinson and Homa Alemzadeh", "abstract": "  Real-time recognition and prediction of surgical activities are fundamental\nto advancing safety and autonomy in robot-assisted surgery. This paper presents\na multimodal transformer architecture for real-time recognition and prediction\nof surgical gestures and trajectories based on short segments of kinematic and\nvideo data. We conduct an ablation study to evaluate the impact of fusing\ndifferent input modalities and their representations on gesture recognition and\nprediction performance. We perform an end-to-end assessment of the proposed\narchitecture using the JHU-ISI Gesture and Skill Assessment Working Set\n(JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with\n89.5\\% accuracy for gesture prediction through effective fusion of kinematic\nfeatures with spatial and contextual video features. It achieves the real-time\nperformance of 1.1-1.3ms for processing a 1-second input window by relying on a\ncomputationally efficient model.\n", "link": "http://arxiv.org/abs/2403.06705v1", "date": "2024-03-11", "relevancy": 2.2533, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5602}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5583}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Transformers%20for%20Real-Time%20Surgical%20Activity%20Prediction&body=Title%3A%20Multimodal%20Transformers%20for%20Real-Time%20Surgical%20Activity%20Prediction%0AAuthor%3A%20Keshara%20Weerasinghe%20and%20Seyed%20Hamid%20Reza%20Roodabeh%20and%20Kay%20Hutchinson%20and%20Homa%20Alemzadeh%0AAbstract%3A%20%20%20Real-time%20recognition%20and%20prediction%20of%20surgical%20activities%20are%20fundamental%0Ato%20advancing%20safety%20and%20autonomy%20in%20robot-assisted%20surgery.%20This%20paper%20presents%0Aa%20multimodal%20transformer%20architecture%20for%20real-time%20recognition%20and%20prediction%0Aof%20surgical%20gestures%20and%20trajectories%20based%20on%20short%20segments%20of%20kinematic%20and%0Avideo%20data.%20We%20conduct%20an%20ablation%20study%20to%20evaluate%20the%20impact%20of%20fusing%0Adifferent%20input%20modalities%20and%20their%20representations%20on%20gesture%20recognition%20and%0Aprediction%20performance.%20We%20perform%20an%20end-to-end%20assessment%20of%20the%20proposed%0Aarchitecture%20using%20the%20JHU-ISI%20Gesture%20and%20Skill%20Assessment%20Working%20Set%0A%28JIGSAWS%29%20dataset.%20Our%20model%20outperforms%20the%20state-of-the-art%20%28SOTA%29%20with%0A89.5%5C%25%20accuracy%20for%20gesture%20prediction%20through%20effective%20fusion%20of%20kinematic%0Afeatures%20with%20spatial%20and%20contextual%20video%20features.%20It%20achieves%20the%20real-time%0Aperformance%20of%201.1-1.3ms%20for%20processing%20a%201-second%20input%20window%20by%20relying%20on%20a%0Acomputationally%20efficient%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06705v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Transformers%20for%20Real-Time%20Surgical%20Activity%20Prediction&entry.906535625=Keshara%20Weerasinghe%20and%20Seyed%20Hamid%20Reza%20Roodabeh%20and%20Kay%20Hutchinson%20and%20Homa%20Alemzadeh&entry.1292438233=%20%20Real-time%20recognition%20and%20prediction%20of%20surgical%20activities%20are%20fundamental%0Ato%20advancing%20safety%20and%20autonomy%20in%20robot-assisted%20surgery.%20This%20paper%20presents%0Aa%20multimodal%20transformer%20architecture%20for%20real-time%20recognition%20and%20prediction%0Aof%20surgical%20gestures%20and%20trajectories%20based%20on%20short%20segments%20of%20kinematic%20and%0Avideo%20data.%20We%20conduct%20an%20ablation%20study%20to%20evaluate%20the%20impact%20of%20fusing%0Adifferent%20input%20modalities%20and%20their%20representations%20on%20gesture%20recognition%20and%0Aprediction%20performance.%20We%20perform%20an%20end-to-end%20assessment%20of%20the%20proposed%0Aarchitecture%20using%20the%20JHU-ISI%20Gesture%20and%20Skill%20Assessment%20Working%20Set%0A%28JIGSAWS%29%20dataset.%20Our%20model%20outperforms%20the%20state-of-the-art%20%28SOTA%29%20with%0A89.5%5C%25%20accuracy%20for%20gesture%20prediction%20through%20effective%20fusion%20of%20kinematic%0Afeatures%20with%20spatial%20and%20contextual%20video%20features.%20It%20achieves%20the%20real-time%0Aperformance%20of%201.1-1.3ms%20for%20processing%20a%201-second%20input%20window%20by%20relying%20on%20a%0Acomputationally%20efficient%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06705v1&entry.124074799=Read"},
{"title": "Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for\n  Distracted Driver Action Recognition", "author": "Erkut Akdag and Zeqi Zhu and Egor Bondarev and Peter H. N. De With", "abstract": "  Classification and localization of driving actions over time is important for\nadvanced driver-assistance systems and naturalistic driving studies. Temporal\nlocalization is challenging because it requires robustness, reliability, and\naccuracy. In this study, we aim to improve the temporal localization and\nclassification accuracy performance by adapting video action recognition and 2D\nhuman-pose estimation networks to one model. Therefore, we design a\ntransformer-based fusion architecture to effectively combine 2D-pose features\nand spatio-temporal features. The model uses 2D-pose features as the positional\nembedding of the transformer architecture and spatio-temporal features as the\nmain input to the encoder of the transformer. The proposed solution is generic\nand independent of the camera numbers and positions, giving frame-based class\nprobabilities as output. Finally, the post-processing step combines information\nfrom different camera views to obtain final predictions and eliminate false\npositives. The model performs well on the A2 test set of the 2023 NVIDIA AI\nCity Challenge for naturalistic driving action recognition, achieving the\noverlap score of the organizer-defined distracted driver behaviour metric of\n0.5079.\n", "link": "http://arxiv.org/abs/2403.06577v1", "date": "2024-03-11", "relevancy": 2.2428, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5681}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5577}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5497}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Transformer-based%20Fusion%20of%202D-pose%20and%20Spatio-temporal%20Embeddings%20for%0A%20%20Distracted%20Driver%20Action%20Recognition&body=Title%3A%20Transformer-based%20Fusion%20of%202D-pose%20and%20Spatio-temporal%20Embeddings%20for%0A%20%20Distracted%20Driver%20Action%20Recognition%0AAuthor%3A%20Erkut%20Akdag%20and%20Zeqi%20Zhu%20and%20Egor%20Bondarev%20and%20Peter%20H.%20N.%20De%20With%0AAbstract%3A%20%20%20Classification%20and%20localization%20of%20driving%20actions%20over%20time%20is%20important%20for%0Aadvanced%20driver-assistance%20systems%20and%20naturalistic%20driving%20studies.%20Temporal%0Alocalization%20is%20challenging%20because%20it%20requires%20robustness%2C%20reliability%2C%20and%0Aaccuracy.%20In%20this%20study%2C%20we%20aim%20to%20improve%20the%20temporal%20localization%20and%0Aclassification%20accuracy%20performance%20by%20adapting%20video%20action%20recognition%20and%202D%0Ahuman-pose%20estimation%20networks%20to%20one%20model.%20Therefore%2C%20we%20design%20a%0Atransformer-based%20fusion%20architecture%20to%20effectively%20combine%202D-pose%20features%0Aand%20spatio-temporal%20features.%20The%20model%20uses%202D-pose%20features%20as%20the%20positional%0Aembedding%20of%20the%20transformer%20architecture%20and%20spatio-temporal%20features%20as%20the%0Amain%20input%20to%20the%20encoder%20of%20the%20transformer.%20The%20proposed%20solution%20is%20generic%0Aand%20independent%20of%20the%20camera%20numbers%20and%20positions%2C%20giving%20frame-based%20class%0Aprobabilities%20as%20output.%20Finally%2C%20the%20post-processing%20step%20combines%20information%0Afrom%20different%20camera%20views%20to%20obtain%20final%20predictions%20and%20eliminate%20false%0Apositives.%20The%20model%20performs%20well%20on%20the%20A2%20test%20set%20of%20the%202023%20NVIDIA%20AI%0ACity%20Challenge%20for%20naturalistic%20driving%20action%20recognition%2C%20achieving%20the%0Aoverlap%20score%20of%20the%20organizer-defined%20distracted%20driver%20behaviour%20metric%20of%0A0.5079.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06577v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-based%20Fusion%20of%202D-pose%20and%20Spatio-temporal%20Embeddings%20for%0A%20%20Distracted%20Driver%20Action%20Recognition&entry.906535625=Erkut%20Akdag%20and%20Zeqi%20Zhu%20and%20Egor%20Bondarev%20and%20Peter%20H.%20N.%20De%20With&entry.1292438233=%20%20Classification%20and%20localization%20of%20driving%20actions%20over%20time%20is%20important%20for%0Aadvanced%20driver-assistance%20systems%20and%20naturalistic%20driving%20studies.%20Temporal%0Alocalization%20is%20challenging%20because%20it%20requires%20robustness%2C%20reliability%2C%20and%0Aaccuracy.%20In%20this%20study%2C%20we%20aim%20to%20improve%20the%20temporal%20localization%20and%0Aclassification%20accuracy%20performance%20by%20adapting%20video%20action%20recognition%20and%202D%0Ahuman-pose%20estimation%20networks%20to%20one%20model.%20Therefore%2C%20we%20design%20a%0Atransformer-based%20fusion%20architecture%20to%20effectively%20combine%202D-pose%20features%0Aand%20spatio-temporal%20features.%20The%20model%20uses%202D-pose%20features%20as%20the%20positional%0Aembedding%20of%20the%20transformer%20architecture%20and%20spatio-temporal%20features%20as%20the%0Amain%20input%20to%20the%20encoder%20of%20the%20transformer.%20The%20proposed%20solution%20is%20generic%0Aand%20independent%20of%20the%20camera%20numbers%20and%20positions%2C%20giving%20frame-based%20class%0Aprobabilities%20as%20output.%20Finally%2C%20the%20post-processing%20step%20combines%20information%0Afrom%20different%20camera%20views%20to%20obtain%20final%20predictions%20and%20eliminate%20false%0Apositives.%20The%20model%20performs%20well%20on%20the%20A2%20test%20set%20of%20the%202023%20NVIDIA%20AI%0ACity%20Challenge%20for%20naturalistic%20driving%20action%20recognition%2C%20achieving%20the%0Aoverlap%20score%20of%20the%20organizer-defined%20distracted%20driver%20behaviour%20metric%20of%0A0.5079.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06577v1&entry.124074799=Read"},
{"title": "Learning with Noisy Foundation Models", "author": "Hao Chen and Jindong Wang and Zihan Wang and Ran Tao and Hongxin Wei and Xing Xie and Masashi Sugiyama and Bhiksha Raj", "abstract": "  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n", "link": "http://arxiv.org/abs/2403.06869v1", "date": "2024-03-11", "relevancy": 2.2352, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5923}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5522}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5279}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Noisy%20Foundation%20Models&body=Title%3A%20Learning%20with%20Noisy%20Foundation%20Models%0AAuthor%3A%20Hao%20Chen%20and%20Jindong%20Wang%20and%20Zihan%20Wang%20and%20Ran%20Tao%20and%20Hongxin%20Wei%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20Foundation%20models%20are%20usually%20pre-trained%20on%20large-scale%20datasets%20and%20then%0Aadapted%20to%20downstream%20tasks%20through%20tuning.%20However%2C%20the%20large-scale%0Apre-training%20datasets%2C%20often%20inaccessible%20or%20too%20expensive%20to%20handle%2C%20can%0Acontain%20label%20noise%20that%20may%20adversely%20affect%20the%20generalization%20of%20the%20model%0Aand%20pose%20unexpected%20risks.%20This%20paper%20stands%20out%20as%20the%20first%20work%20to%0Acomprehensively%20understand%20and%20analyze%20the%20nature%20of%20noise%20in%20pre-training%0Adatasets%20and%20then%20effectively%20mitigate%20its%20impacts%20on%20downstream%20tasks.%0ASpecifically%2C%20through%20extensive%20experiments%20of%20fully-supervised%20and%20image-text%0Acontrastive%20pre-training%20on%20synthetic%20noisy%20ImageNet-1K%2C%20YFCC15M%2C%20and%20CC12M%0Adatasets%2C%20we%20demonstrate%20that%2C%20while%20slight%20noise%20in%20pre-training%20can%20benefit%0Ain-domain%20%28ID%29%20performance%2C%20where%20the%20training%20and%20testing%20data%20share%20a%20similar%0Adistribution%2C%20it%20always%20deteriorates%20out-of-domain%20%28OOD%29%20performance%2C%20where%0Atraining%20and%20testing%20distributions%20are%20significantly%20different.%20These%0Aobservations%20are%20agnostic%20to%20scales%20of%20pre-training%20datasets%2C%20pre-training%0Anoise%20types%2C%20model%20architectures%2C%20pre-training%20objectives%2C%20downstream%20tuning%0Amethods%2C%20and%20downstream%20applications.%20We%20empirically%20ascertain%20that%20the%20reason%0Abehind%20this%20is%20that%20the%20pre-training%20noise%20shapes%20the%20feature%20space%0Adifferently.%20We%20then%20propose%20a%20tuning%20method%20%28NMTune%29%20to%20affine%20the%20feature%0Aspace%20to%20mitigate%20the%20malignant%20effect%20of%20noise%20and%20improve%20generalization%2C%0Awhich%20is%20applicable%20in%20both%20parameter-efficient%20and%20black-box%20tuning%20manners.%0AWe%20additionally%20conduct%20extensive%20experiments%20on%20popular%20vision%20and%20language%0Amodels%2C%20including%20APIs%2C%20which%20are%20supervised%20and%20self-supervised%20pre-trained%20on%0Arealistic%20noisy%20data%20for%20evaluation.%20Our%20analysis%20and%20results%20demonstrate%20the%0Aimportance%20of%20this%20novel%20and%20fundamental%20research%20direction%2C%20which%20we%20term%20as%0ANoisy%20Model%20Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06869v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Noisy%20Foundation%20Models&entry.906535625=Hao%20Chen%20and%20Jindong%20Wang%20and%20Zihan%20Wang%20and%20Ran%20Tao%20and%20Hongxin%20Wei%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Bhiksha%20Raj&entry.1292438233=%20%20Foundation%20models%20are%20usually%20pre-trained%20on%20large-scale%20datasets%20and%20then%0Aadapted%20to%20downstream%20tasks%20through%20tuning.%20However%2C%20the%20large-scale%0Apre-training%20datasets%2C%20often%20inaccessible%20or%20too%20expensive%20to%20handle%2C%20can%0Acontain%20label%20noise%20that%20may%20adversely%20affect%20the%20generalization%20of%20the%20model%0Aand%20pose%20unexpected%20risks.%20This%20paper%20stands%20out%20as%20the%20first%20work%20to%0Acomprehensively%20understand%20and%20analyze%20the%20nature%20of%20noise%20in%20pre-training%0Adatasets%20and%20then%20effectively%20mitigate%20its%20impacts%20on%20downstream%20tasks.%0ASpecifically%2C%20through%20extensive%20experiments%20of%20fully-supervised%20and%20image-text%0Acontrastive%20pre-training%20on%20synthetic%20noisy%20ImageNet-1K%2C%20YFCC15M%2C%20and%20CC12M%0Adatasets%2C%20we%20demonstrate%20that%2C%20while%20slight%20noise%20in%20pre-training%20can%20benefit%0Ain-domain%20%28ID%29%20performance%2C%20where%20the%20training%20and%20testing%20data%20share%20a%20similar%0Adistribution%2C%20it%20always%20deteriorates%20out-of-domain%20%28OOD%29%20performance%2C%20where%0Atraining%20and%20testing%20distributions%20are%20significantly%20different.%20These%0Aobservations%20are%20agnostic%20to%20scales%20of%20pre-training%20datasets%2C%20pre-training%0Anoise%20types%2C%20model%20architectures%2C%20pre-training%20objectives%2C%20downstream%20tuning%0Amethods%2C%20and%20downstream%20applications.%20We%20empirically%20ascertain%20that%20the%20reason%0Abehind%20this%20is%20that%20the%20pre-training%20noise%20shapes%20the%20feature%20space%0Adifferently.%20We%20then%20propose%20a%20tuning%20method%20%28NMTune%29%20to%20affine%20the%20feature%0Aspace%20to%20mitigate%20the%20malignant%20effect%20of%20noise%20and%20improve%20generalization%2C%0Awhich%20is%20applicable%20in%20both%20parameter-efficient%20and%20black-box%20tuning%20manners.%0AWe%20additionally%20conduct%20extensive%20experiments%20on%20popular%20vision%20and%20language%0Amodels%2C%20including%20APIs%2C%20which%20are%20supervised%20and%20self-supervised%20pre-trained%20on%0Arealistic%20noisy%20data%20for%20evaluation.%20Our%20analysis%20and%20results%20demonstrate%20the%0Aimportance%20of%20this%20novel%20and%20fundamental%20research%20direction%2C%20which%20we%20term%20as%0ANoisy%20Model%20Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06869v1&entry.124074799=Read"},
{"title": "Benign overfitting in leaky ReLU networks with moderate input dimension", "author": "Kedar Karhadkar and Erin George and Michael Murray and Guido Mont\u00fafar and Deanna Needell", "abstract": "  The problem of benign overfitting asks whether it is possible for a model to\nperfectly fit noisy training data and still generalize well. We study benign\noverfitting in two-layer leaky ReLU networks trained with the hinge loss on a\nbinary classification task. We consider input data which can be decomposed into\nthe sum of a common signal and a random noise component, which lie on subspaces\northogonal to one another. We characterize conditions on the signal to noise\nratio (SNR) of the model parameters giving rise to benign versus non-benign, or\nharmful, overfitting: in particular, if the SNR is high then benign overfitting\noccurs, conversely if the SNR is low then harmful overfitting occurs. We\nattribute both benign and non-benign overfitting to an approximate margin\nmaximization property and show that leaky ReLU networks trained on hinge loss\nwith Gradient Descent (GD) satisfy this property. In contrast to prior work we\ndo not require near orthogonality conditions on the training data: notably, for\ninput dimension $d$ and training sample size $n$, while prior work shows\nasymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require\nonly $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within\n$\\epsilon$ of optimal.\n", "link": "http://arxiv.org/abs/2403.06903v1", "date": "2024-03-11", "relevancy": 2.2321, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.438}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4361}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Benign%20overfitting%20in%20leaky%20ReLU%20networks%20with%20moderate%20input%20dimension&body=Title%3A%20Benign%20overfitting%20in%20leaky%20ReLU%20networks%20with%20moderate%20input%20dimension%0AAuthor%3A%20Kedar%20Karhadkar%20and%20Erin%20George%20and%20Michael%20Murray%20and%20Guido%20Mont%C3%BAfar%20and%20Deanna%20Needell%0AAbstract%3A%20%20%20The%20problem%20of%20benign%20overfitting%20asks%20whether%20it%20is%20possible%20for%20a%20model%20to%0Aperfectly%20fit%20noisy%20training%20data%20and%20still%20generalize%20well.%20We%20study%20benign%0Aoverfitting%20in%20two-layer%20leaky%20ReLU%20networks%20trained%20with%20the%20hinge%20loss%20on%20a%0Abinary%20classification%20task.%20We%20consider%20input%20data%20which%20can%20be%20decomposed%20into%0Athe%20sum%20of%20a%20common%20signal%20and%20a%20random%20noise%20component%2C%20which%20lie%20on%20subspaces%0Aorthogonal%20to%20one%20another.%20We%20characterize%20conditions%20on%20the%20signal%20to%20noise%0Aratio%20%28SNR%29%20of%20the%20model%20parameters%20giving%20rise%20to%20benign%20versus%20non-benign%2C%20or%0Aharmful%2C%20overfitting%3A%20in%20particular%2C%20if%20the%20SNR%20is%20high%20then%20benign%20overfitting%0Aoccurs%2C%20conversely%20if%20the%20SNR%20is%20low%20then%20harmful%20overfitting%20occurs.%20We%0Aattribute%20both%20benign%20and%20non-benign%20overfitting%20to%20an%20approximate%20margin%0Amaximization%20property%20and%20show%20that%20leaky%20ReLU%20networks%20trained%20on%20hinge%20loss%0Awith%20Gradient%20Descent%20%28GD%29%20satisfy%20this%20property.%20In%20contrast%20to%20prior%20work%20we%0Ado%20not%20require%20near%20orthogonality%20conditions%20on%20the%20training%20data%3A%20notably%2C%20for%0Ainput%20dimension%20%24d%24%20and%20training%20sample%20size%20%24n%24%2C%20while%20prior%20work%20shows%0Aasymptotically%20optimal%20error%20when%20%24d%20%3D%20%5COmega%28n%5E2%20%5Clog%20n%29%24%2C%20here%20we%20require%0Aonly%20%24d%20%3D%20%5COmega%5Cleft%28n%20%5Clog%20%5Cfrac%7B1%7D%7B%5Cepsilon%7D%5Cright%29%24%20to%20obtain%20error%20within%0A%24%5Cepsilon%24%20of%20optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06903v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benign%20overfitting%20in%20leaky%20ReLU%20networks%20with%20moderate%20input%20dimension&entry.906535625=Kedar%20Karhadkar%20and%20Erin%20George%20and%20Michael%20Murray%20and%20Guido%20Mont%C3%BAfar%20and%20Deanna%20Needell&entry.1292438233=%20%20The%20problem%20of%20benign%20overfitting%20asks%20whether%20it%20is%20possible%20for%20a%20model%20to%0Aperfectly%20fit%20noisy%20training%20data%20and%20still%20generalize%20well.%20We%20study%20benign%0Aoverfitting%20in%20two-layer%20leaky%20ReLU%20networks%20trained%20with%20the%20hinge%20loss%20on%20a%0Abinary%20classification%20task.%20We%20consider%20input%20data%20which%20can%20be%20decomposed%20into%0Athe%20sum%20of%20a%20common%20signal%20and%20a%20random%20noise%20component%2C%20which%20lie%20on%20subspaces%0Aorthogonal%20to%20one%20another.%20We%20characterize%20conditions%20on%20the%20signal%20to%20noise%0Aratio%20%28SNR%29%20of%20the%20model%20parameters%20giving%20rise%20to%20benign%20versus%20non-benign%2C%20or%0Aharmful%2C%20overfitting%3A%20in%20particular%2C%20if%20the%20SNR%20is%20high%20then%20benign%20overfitting%0Aoccurs%2C%20conversely%20if%20the%20SNR%20is%20low%20then%20harmful%20overfitting%20occurs.%20We%0Aattribute%20both%20benign%20and%20non-benign%20overfitting%20to%20an%20approximate%20margin%0Amaximization%20property%20and%20show%20that%20leaky%20ReLU%20networks%20trained%20on%20hinge%20loss%0Awith%20Gradient%20Descent%20%28GD%29%20satisfy%20this%20property.%20In%20contrast%20to%20prior%20work%20we%0Ado%20not%20require%20near%20orthogonality%20conditions%20on%20the%20training%20data%3A%20notably%2C%20for%0Ainput%20dimension%20%24d%24%20and%20training%20sample%20size%20%24n%24%2C%20while%20prior%20work%20shows%0Aasymptotically%20optimal%20error%20when%20%24d%20%3D%20%5COmega%28n%5E2%20%5Clog%20n%29%24%2C%20here%20we%20require%0Aonly%20%24d%20%3D%20%5COmega%5Cleft%28n%20%5Clog%20%5Cfrac%7B1%7D%7B%5Cepsilon%7D%5Cright%29%24%20to%20obtain%20error%20within%0A%24%5Cepsilon%24%20of%20optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06903v1&entry.124074799=Read"},
{"title": "AmodalSynthDrive: A Synthetic Amodal Perception Dataset for Autonomous\n  Driving", "author": "Ahmed Rida Sekkat and Rohit Mohan and Oliver Sawade and Elmar Matthes and Abhinav Valada", "abstract": "  Unlike humans, who can effortlessly estimate the entirety of objects even\nwhen partially occluded, modern computer vision algorithms still find this\naspect extremely challenging. Leveraging this amodal perception for autonomous\ndriving remains largely untapped due to the lack of suitable datasets. The\ncuration of these datasets is primarily hindered by significant annotation\ncosts and mitigating annotator subjectivity in accurately labeling occluded\nregions. To address these limitations, we introduce AmodalSynthDrive, a\nsynthetic multi-task multi-modal amodal perception dataset. The dataset\nprovides multi-view camera images, 3D bounding boxes, LiDAR data, and odometry\nfor 150 driving sequences with over 1M object annotations in diverse traffic,\nweather, and lighting conditions. AmodalSynthDrive supports multiple amodal\nscene understanding tasks including the introduced amodal depth estimation for\nenhanced spatial understanding. We evaluate several baselines for each of these\ntasks to illustrate the challenges and set up public benchmarking servers. The\ndataset is available at http://amodalsynthdrive.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2309.06547v2", "date": "2024-03-11", "relevancy": 2.2175, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5472}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20AmodalSynthDrive%3A%20A%20Synthetic%20Amodal%20Perception%20Dataset%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20AmodalSynthDrive%3A%20A%20Synthetic%20Amodal%20Perception%20Dataset%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Ahmed%20Rida%20Sekkat%20and%20Rohit%20Mohan%20and%20Oliver%20Sawade%20and%20Elmar%20Matthes%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Unlike%20humans%2C%20who%20can%20effortlessly%20estimate%20the%20entirety%20of%20objects%20even%0Awhen%20partially%20occluded%2C%20modern%20computer%20vision%20algorithms%20still%20find%20this%0Aaspect%20extremely%20challenging.%20Leveraging%20this%20amodal%20perception%20for%20autonomous%0Adriving%20remains%20largely%20untapped%20due%20to%20the%20lack%20of%20suitable%20datasets.%20The%0Acuration%20of%20these%20datasets%20is%20primarily%20hindered%20by%20significant%20annotation%0Acosts%20and%20mitigating%20annotator%20subjectivity%20in%20accurately%20labeling%20occluded%0Aregions.%20To%20address%20these%20limitations%2C%20we%20introduce%20AmodalSynthDrive%2C%20a%0Asynthetic%20multi-task%20multi-modal%20amodal%20perception%20dataset.%20The%20dataset%0Aprovides%20multi-view%20camera%20images%2C%203D%20bounding%20boxes%2C%20LiDAR%20data%2C%20and%20odometry%0Afor%20150%20driving%20sequences%20with%20over%201M%20object%20annotations%20in%20diverse%20traffic%2C%0Aweather%2C%20and%20lighting%20conditions.%20AmodalSynthDrive%20supports%20multiple%20amodal%0Ascene%20understanding%20tasks%20including%20the%20introduced%20amodal%20depth%20estimation%20for%0Aenhanced%20spatial%20understanding.%20We%20evaluate%20several%20baselines%20for%20each%20of%20these%0Atasks%20to%20illustrate%20the%20challenges%20and%20set%20up%20public%20benchmarking%20servers.%20The%0Adataset%20is%20available%20at%20http%3A//amodalsynthdrive.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06547v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AmodalSynthDrive%3A%20A%20Synthetic%20Amodal%20Perception%20Dataset%20for%20Autonomous%0A%20%20Driving&entry.906535625=Ahmed%20Rida%20Sekkat%20and%20Rohit%20Mohan%20and%20Oliver%20Sawade%20and%20Elmar%20Matthes%20and%20Abhinav%20Valada&entry.1292438233=%20%20Unlike%20humans%2C%20who%20can%20effortlessly%20estimate%20the%20entirety%20of%20objects%20even%0Awhen%20partially%20occluded%2C%20modern%20computer%20vision%20algorithms%20still%20find%20this%0Aaspect%20extremely%20challenging.%20Leveraging%20this%20amodal%20perception%20for%20autonomous%0Adriving%20remains%20largely%20untapped%20due%20to%20the%20lack%20of%20suitable%20datasets.%20The%0Acuration%20of%20these%20datasets%20is%20primarily%20hindered%20by%20significant%20annotation%0Acosts%20and%20mitigating%20annotator%20subjectivity%20in%20accurately%20labeling%20occluded%0Aregions.%20To%20address%20these%20limitations%2C%20we%20introduce%20AmodalSynthDrive%2C%20a%0Asynthetic%20multi-task%20multi-modal%20amodal%20perception%20dataset.%20The%20dataset%0Aprovides%20multi-view%20camera%20images%2C%203D%20bounding%20boxes%2C%20LiDAR%20data%2C%20and%20odometry%0Afor%20150%20driving%20sequences%20with%20over%201M%20object%20annotations%20in%20diverse%20traffic%2C%0Aweather%2C%20and%20lighting%20conditions.%20AmodalSynthDrive%20supports%20multiple%20amodal%0Ascene%20understanding%20tasks%20including%20the%20introduced%20amodal%20depth%20estimation%20for%0Aenhanced%20spatial%20understanding.%20We%20evaluate%20several%20baselines%20for%20each%20of%20these%0Atasks%20to%20illustrate%20the%20challenges%20and%20set%20up%20public%20benchmarking%20servers.%20The%0Adataset%20is%20available%20at%20http%3A//amodalsynthdrive.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06547v2&entry.124074799=Read"},
{"title": "Anatomically-Controllable Medical Image Generation with\n  Segmentation-Guided Diffusion Models", "author": "Nicholas Konz and Yuwen Chen and Haoyu Dong and Maciej A. Mazurowski", "abstract": "  Diffusion models have enabled remarkably high-quality medical image\ngeneration, yet it is challenging to enforce anatomical constraints in\ngenerated images. This hampers many useful applications, including\npre-registered image generation, counterfactual scenarios, and others. To this\nend, we propose a diffusion model-based method that supports\nanatomically-controllable medical image generation, by following a multi-class\nanatomical segmentation mask at each sampling step. We additionally introduce a\nrandom mask ablation training algorithm to enable conditioning on a selected\ncombination of anatomical constraints while allowing flexibility in other\nanatomical areas. We compare our model (\"Seg-Diff\") to existing methods on\nbreast MRI and abdominal/neck-to-pelvis CT datasets with a wide range of\nanatomical objects. Results show that it reaches a new state-of-the-art in the\nfaithfulness of generated images to input anatomical masks on both datasets,\nand is on par for general anatomical realism. Finally, our model also enjoys\nthe extra benefit of being able to adjust the anatomical similarity of\ngenerated images to real images of choice through interpolation in its latent\nspace.\n", "link": "http://arxiv.org/abs/2402.05210v3", "date": "2024-03-11", "relevancy": 2.2171, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5607}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5565}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5495}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Anatomically-Controllable%20Medical%20Image%20Generation%20with%0A%20%20Segmentation-Guided%20Diffusion%20Models&body=Title%3A%20Anatomically-Controllable%20Medical%20Image%20Generation%20with%0A%20%20Segmentation-Guided%20Diffusion%20Models%0AAuthor%3A%20Nicholas%20Konz%20and%20Yuwen%20Chen%20and%20Haoyu%20Dong%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Diffusion%20models%20have%20enabled%20remarkably%20high-quality%20medical%20image%0Ageneration%2C%20yet%20it%20is%20challenging%20to%20enforce%20anatomical%20constraints%20in%0Agenerated%20images.%20This%20hampers%20many%20useful%20applications%2C%20including%0Apre-registered%20image%20generation%2C%20counterfactual%20scenarios%2C%20and%20others.%20To%20this%0Aend%2C%20we%20propose%20a%20diffusion%20model-based%20method%20that%20supports%0Aanatomically-controllable%20medical%20image%20generation%2C%20by%20following%20a%20multi-class%0Aanatomical%20segmentation%20mask%20at%20each%20sampling%20step.%20We%20additionally%20introduce%20a%0Arandom%20mask%20ablation%20training%20algorithm%20to%20enable%20conditioning%20on%20a%20selected%0Acombination%20of%20anatomical%20constraints%20while%20allowing%20flexibility%20in%20other%0Aanatomical%20areas.%20We%20compare%20our%20model%20%28%22Seg-Diff%22%29%20to%20existing%20methods%20on%0Abreast%20MRI%20and%20abdominal/neck-to-pelvis%20CT%20datasets%20with%20a%20wide%20range%20of%0Aanatomical%20objects.%20Results%20show%20that%20it%20reaches%20a%20new%20state-of-the-art%20in%20the%0Afaithfulness%20of%20generated%20images%20to%20input%20anatomical%20masks%20on%20both%20datasets%2C%0Aand%20is%20on%20par%20for%20general%20anatomical%20realism.%20Finally%2C%20our%20model%20also%20enjoys%0Athe%20extra%20benefit%20of%20being%20able%20to%20adjust%20the%20anatomical%20similarity%20of%0Agenerated%20images%20to%20real%20images%20of%20choice%20through%20interpolation%20in%20its%20latent%0Aspace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05210v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomically-Controllable%20Medical%20Image%20Generation%20with%0A%20%20Segmentation-Guided%20Diffusion%20Models&entry.906535625=Nicholas%20Konz%20and%20Yuwen%20Chen%20and%20Haoyu%20Dong%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Diffusion%20models%20have%20enabled%20remarkably%20high-quality%20medical%20image%0Ageneration%2C%20yet%20it%20is%20challenging%20to%20enforce%20anatomical%20constraints%20in%0Agenerated%20images.%20This%20hampers%20many%20useful%20applications%2C%20including%0Apre-registered%20image%20generation%2C%20counterfactual%20scenarios%2C%20and%20others.%20To%20this%0Aend%2C%20we%20propose%20a%20diffusion%20model-based%20method%20that%20supports%0Aanatomically-controllable%20medical%20image%20generation%2C%20by%20following%20a%20multi-class%0Aanatomical%20segmentation%20mask%20at%20each%20sampling%20step.%20We%20additionally%20introduce%20a%0Arandom%20mask%20ablation%20training%20algorithm%20to%20enable%20conditioning%20on%20a%20selected%0Acombination%20of%20anatomical%20constraints%20while%20allowing%20flexibility%20in%20other%0Aanatomical%20areas.%20We%20compare%20our%20model%20%28%22Seg-Diff%22%29%20to%20existing%20methods%20on%0Abreast%20MRI%20and%20abdominal/neck-to-pelvis%20CT%20datasets%20with%20a%20wide%20range%20of%0Aanatomical%20objects.%20Results%20show%20that%20it%20reaches%20a%20new%20state-of-the-art%20in%20the%0Afaithfulness%20of%20generated%20images%20to%20input%20anatomical%20masks%20on%20both%20datasets%2C%0Aand%20is%20on%20par%20for%20general%20anatomical%20realism.%20Finally%2C%20our%20model%20also%20enjoys%0Athe%20extra%20benefit%20of%20being%20able%20to%20adjust%20the%20anatomical%20similarity%20of%0Agenerated%20images%20to%20real%20images%20of%20choice%20through%20interpolation%20in%20its%20latent%0Aspace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05210v3&entry.124074799=Read"},
{"title": "Understanding and Mitigating the Label Noise in Pre-training on\n  Downstream Tasks", "author": "Hao Chen and Jindong Wang and Ankit Shah and Ran Tao and Hongxin Wei and Xing Xie and Masashi Sugiyama and Bhiksha Raj", "abstract": "  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlight-weight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n", "link": "http://arxiv.org/abs/2309.17002v2", "date": "2024-03-11", "relevancy": 2.2168, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5594}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5556}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5377}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Mitigating%20the%20Label%20Noise%20in%20Pre-training%20on%0A%20%20Downstream%20Tasks&body=Title%3A%20Understanding%20and%20Mitigating%20the%20Label%20Noise%20in%20Pre-training%20on%0A%20%20Downstream%20Tasks%0AAuthor%3A%20Hao%20Chen%20and%20Jindong%20Wang%20and%20Ankit%20Shah%20and%20Ran%20Tao%20and%20Hongxin%20Wei%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20Pre-training%20on%20large-scale%20datasets%20and%20then%20fine-tuning%20on%20downstream%20tasks%0Ahave%20become%20a%20standard%20practice%20in%20deep%20learning.%20However%2C%20pre-training%20data%0Aoften%20contain%20label%20noise%20that%20may%20adversely%20affect%20the%20generalization%20of%20the%0Amodel.%20This%20paper%20aims%20to%20understand%20the%20nature%20of%20noise%20in%20pre-training%0Adatasets%20and%20to%20mitigate%20its%20impact%20on%20downstream%20tasks.%20More%20specifically%2C%0Athrough%20extensive%20experiments%20of%20supervised%20pre-training%20models%20on%20synthetic%0Anoisy%20ImageNet-1K%20and%20YFCC15M%20datasets%2C%20we%20demonstrate%20that%20while%20slight%20noise%0Ain%20pre-training%20can%20benefit%20in-domain%20%28ID%29%20transfer%20performance%2C%20where%20the%0Atraining%20and%20testing%20data%20share%20the%20same%20distribution%2C%20it%20always%20deteriorates%0Aout-of-domain%20%28OOD%29%20performance%2C%20where%20training%20and%20testing%20data%20distribution%0Aare%20different.%20We%20empirically%20verify%20that%20the%20reason%20behind%20is%20noise%20in%0Apre-training%20shapes%20the%20feature%20space%20differently.%20We%20then%20propose%20a%0Alight-weight%20black-box%20tuning%20method%20%28NMTune%29%20to%20affine%20the%20feature%20space%20to%0Amitigate%20the%20malignant%20effect%20of%20noise%20and%20improve%20generalization%20on%20both%20ID%0Aand%20OOD%20tasks%2C%20considering%20one%20may%20not%20be%20able%20to%20fully%20fine-tune%20or%20even%0Aaccess%20the%20pre-trained%20models.%20We%20conduct%20practical%20experiments%20on%20popular%0Avision%20and%20language%20models%20that%20are%20pre-trained%20on%20noisy%20data%20for%20evaluation%20of%0Aour%20approach.%20Our%20analysis%20and%20results%20show%20the%20importance%20of%20this%20interesting%0Aand%20novel%20research%20direction%2C%20which%20we%20term%20Noisy%20Model%20Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17002v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Mitigating%20the%20Label%20Noise%20in%20Pre-training%20on%0A%20%20Downstream%20Tasks&entry.906535625=Hao%20Chen%20and%20Jindong%20Wang%20and%20Ankit%20Shah%20and%20Ran%20Tao%20and%20Hongxin%20Wei%20and%20Xing%20Xie%20and%20Masashi%20Sugiyama%20and%20Bhiksha%20Raj&entry.1292438233=%20%20Pre-training%20on%20large-scale%20datasets%20and%20then%20fine-tuning%20on%20downstream%20tasks%0Ahave%20become%20a%20standard%20practice%20in%20deep%20learning.%20However%2C%20pre-training%20data%0Aoften%20contain%20label%20noise%20that%20may%20adversely%20affect%20the%20generalization%20of%20the%0Amodel.%20This%20paper%20aims%20to%20understand%20the%20nature%20of%20noise%20in%20pre-training%0Adatasets%20and%20to%20mitigate%20its%20impact%20on%20downstream%20tasks.%20More%20specifically%2C%0Athrough%20extensive%20experiments%20of%20supervised%20pre-training%20models%20on%20synthetic%0Anoisy%20ImageNet-1K%20and%20YFCC15M%20datasets%2C%20we%20demonstrate%20that%20while%20slight%20noise%0Ain%20pre-training%20can%20benefit%20in-domain%20%28ID%29%20transfer%20performance%2C%20where%20the%0Atraining%20and%20testing%20data%20share%20the%20same%20distribution%2C%20it%20always%20deteriorates%0Aout-of-domain%20%28OOD%29%20performance%2C%20where%20training%20and%20testing%20data%20distribution%0Aare%20different.%20We%20empirically%20verify%20that%20the%20reason%20behind%20is%20noise%20in%0Apre-training%20shapes%20the%20feature%20space%20differently.%20We%20then%20propose%20a%0Alight-weight%20black-box%20tuning%20method%20%28NMTune%29%20to%20affine%20the%20feature%20space%20to%0Amitigate%20the%20malignant%20effect%20of%20noise%20and%20improve%20generalization%20on%20both%20ID%0Aand%20OOD%20tasks%2C%20considering%20one%20may%20not%20be%20able%20to%20fully%20fine-tune%20or%20even%0Aaccess%20the%20pre-trained%20models.%20We%20conduct%20practical%20experiments%20on%20popular%0Avision%20and%20language%20models%20that%20are%20pre-trained%20on%20noisy%20data%20for%20evaluation%20of%0Aour%20approach.%20Our%20analysis%20and%20results%20show%20the%20importance%20of%20this%20interesting%0Aand%20novel%20research%20direction%2C%20which%20we%20term%20Noisy%20Model%20Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17002v2&entry.124074799=Read"},
{"title": "Boosting Image Restoration via Priors from Pre-trained Models", "author": "Xiaogang Xu and Shu Kong and Tao Hu and Zhe Liu and Hujun Bao", "abstract": "  Pre-trained models with large-scale training data, such as CLIP and Stable\nDiffusion, have demonstrated remarkable performance in various high-level\ncomputer vision tasks such as image understanding and generation from language\ndescriptions. Yet, their potential for low-level tasks such as image\nrestoration remains relatively unexplored. In this paper, we explore such\nmodels to enhance image restoration. As off-the-shelf features (OSF) from\npre-trained models do not directly serve image restoration, we propose to learn\nan additional lightweight module called Pre-Train-Guided Refinement Module\n(PTG-RM) to refine restoration results of a target restoration network with\nOSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying\nEnhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention\n(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,\nwhile PTG-CSA enhances spatial-channel attention for restoration-related\nlearning. Extensive experiments demonstrate that PTG-RM, with its compact size\n($<$1M parameters), effectively enhances restoration performance of various\nmodels across different tasks, including low-light enhancement, deraining,\ndeblurring, and denoising.\n", "link": "http://arxiv.org/abs/2403.06793v1", "date": "2024-03-11", "relevancy": 2.2147, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5728}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5683}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5287}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Boosting%20Image%20Restoration%20via%20Priors%20from%20Pre-trained%20Models&body=Title%3A%20Boosting%20Image%20Restoration%20via%20Priors%20from%20Pre-trained%20Models%0AAuthor%3A%20Xiaogang%20Xu%20and%20Shu%20Kong%20and%20Tao%20Hu%20and%20Zhe%20Liu%20and%20Hujun%20Bao%0AAbstract%3A%20%20%20Pre-trained%20models%20with%20large-scale%20training%20data%2C%20such%20as%20CLIP%20and%20Stable%0ADiffusion%2C%20have%20demonstrated%20remarkable%20performance%20in%20various%20high-level%0Acomputer%20vision%20tasks%20such%20as%20image%20understanding%20and%20generation%20from%20language%0Adescriptions.%20Yet%2C%20their%20potential%20for%20low-level%20tasks%20such%20as%20image%0Arestoration%20remains%20relatively%20unexplored.%20In%20this%20paper%2C%20we%20explore%20such%0Amodels%20to%20enhance%20image%20restoration.%20As%20off-the-shelf%20features%20%28OSF%29%20from%0Apre-trained%20models%20do%20not%20directly%20serve%20image%20restoration%2C%20we%20propose%20to%20learn%0Aan%20additional%20lightweight%20module%20called%20Pre-Train-Guided%20Refinement%20Module%0A%28PTG-RM%29%20to%20refine%20restoration%20results%20of%20a%20target%20restoration%20network%20with%0AOSF.%20PTG-RM%20consists%20of%20two%20components%2C%20Pre-Train-Guided%20Spatial-Varying%0AEnhancement%20%28PTG-SVE%29%2C%20and%20Pre-Train-Guided%20Channel-Spatial%20Attention%0A%28PTG-CSA%29.%20PTG-SVE%20enables%20optimal%20short-%20and%20long-range%20neural%20operations%2C%0Awhile%20PTG-CSA%20enhances%20spatial-channel%20attention%20for%20restoration-related%0Alearning.%20Extensive%20experiments%20demonstrate%20that%20PTG-RM%2C%20with%20its%20compact%20size%0A%28%24%3C%241M%20parameters%29%2C%20effectively%20enhances%20restoration%20performance%20of%20various%0Amodels%20across%20different%20tasks%2C%20including%20low-light%20enhancement%2C%20deraining%2C%0Adeblurring%2C%20and%20denoising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06793v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Image%20Restoration%20via%20Priors%20from%20Pre-trained%20Models&entry.906535625=Xiaogang%20Xu%20and%20Shu%20Kong%20and%20Tao%20Hu%20and%20Zhe%20Liu%20and%20Hujun%20Bao&entry.1292438233=%20%20Pre-trained%20models%20with%20large-scale%20training%20data%2C%20such%20as%20CLIP%20and%20Stable%0ADiffusion%2C%20have%20demonstrated%20remarkable%20performance%20in%20various%20high-level%0Acomputer%20vision%20tasks%20such%20as%20image%20understanding%20and%20generation%20from%20language%0Adescriptions.%20Yet%2C%20their%20potential%20for%20low-level%20tasks%20such%20as%20image%0Arestoration%20remains%20relatively%20unexplored.%20In%20this%20paper%2C%20we%20explore%20such%0Amodels%20to%20enhance%20image%20restoration.%20As%20off-the-shelf%20features%20%28OSF%29%20from%0Apre-trained%20models%20do%20not%20directly%20serve%20image%20restoration%2C%20we%20propose%20to%20learn%0Aan%20additional%20lightweight%20module%20called%20Pre-Train-Guided%20Refinement%20Module%0A%28PTG-RM%29%20to%20refine%20restoration%20results%20of%20a%20target%20restoration%20network%20with%0AOSF.%20PTG-RM%20consists%20of%20two%20components%2C%20Pre-Train-Guided%20Spatial-Varying%0AEnhancement%20%28PTG-SVE%29%2C%20and%20Pre-Train-Guided%20Channel-Spatial%20Attention%0A%28PTG-CSA%29.%20PTG-SVE%20enables%20optimal%20short-%20and%20long-range%20neural%20operations%2C%0Awhile%20PTG-CSA%20enhances%20spatial-channel%20attention%20for%20restoration-related%0Alearning.%20Extensive%20experiments%20demonstrate%20that%20PTG-RM%2C%20with%20its%20compact%20size%0A%28%24%3C%241M%20parameters%29%2C%20effectively%20enhances%20restoration%20performance%20of%20various%0Amodels%20across%20different%20tasks%2C%20including%20low-light%20enhancement%2C%20deraining%2C%0Adeblurring%2C%20and%20denoising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06793v1&entry.124074799=Read"},
{"title": "Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot\n  Domain Transfer", "author": "Siddhant Satyanaik and Aditya Murali and Deepak Alapatt and Xin Wang and Pietro Mascagni and Nicolas Padoy", "abstract": "  Purpose: Advances in deep learning have resulted in effective models for\nsurgical video analysis; however, these models often fail to generalize across\nmedical centers due to domain shift caused by variations in surgical workflow,\ncamera setups, and patient demographics. Recently, object-centric learning has\nemerged as a promising approach for improved surgical scene understanding,\ncapturing and disentangling visual and semantic properties of surgical tools\nand anatomy to improve downstream task performance. In this work, we conduct a\nmulti-centric performance benchmark of object-centric approaches, focusing on\nCritical View of Safety assessment in laparoscopic cholecystectomy, then\npropose an improved approach for unseen domain generalization.\n  Methods: We evaluate four object-centric approaches for domain\ngeneralization, establishing baseline performance. Next, leveraging the\ndisentangled nature of object-centric representations, we dissect one of these\nmethods through a series of ablations (e.g. ignoring either visual or semantic\nfeatures for downstream classification). Finally, based on the results of these\nablations, we develop an optimized method specifically tailored for domain\ngeneralization, LG-DG, that includes a novel disentanglement loss function.\n  Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over\nthe best baseline approach. More broadly, we show that object-centric\napproaches are highly effective for domain generalization thanks to their\nmodular approach to representation learning.\n  Conclusion: We investigate the use of object-centric methods for unseen\ndomain generalization, identify method-agnostic factors critical for\nperformance, and present an optimized approach that substantially outperforms\nexisting methods.\n", "link": "http://arxiv.org/abs/2403.06953v1", "date": "2024-03-11", "relevancy": 2.2117, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5716}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5487}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5359}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Latent%20Graph%20Representations%20of%20Surgical%20Scenes%20for%20Zero-Shot%0A%20%20Domain%20Transfer&body=Title%3A%20Optimizing%20Latent%20Graph%20Representations%20of%20Surgical%20Scenes%20for%20Zero-Shot%0A%20%20Domain%20Transfer%0AAuthor%3A%20Siddhant%20Satyanaik%20and%20Aditya%20Murali%20and%20Deepak%20Alapatt%20and%20Xin%20Wang%20and%20Pietro%20Mascagni%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Purpose%3A%20Advances%20in%20deep%20learning%20have%20resulted%20in%20effective%20models%20for%0Asurgical%20video%20analysis%3B%20however%2C%20these%20models%20often%20fail%20to%20generalize%20across%0Amedical%20centers%20due%20to%20domain%20shift%20caused%20by%20variations%20in%20surgical%20workflow%2C%0Acamera%20setups%2C%20and%20patient%20demographics.%20Recently%2C%20object-centric%20learning%20has%0Aemerged%20as%20a%20promising%20approach%20for%20improved%20surgical%20scene%20understanding%2C%0Acapturing%20and%20disentangling%20visual%20and%20semantic%20properties%20of%20surgical%20tools%0Aand%20anatomy%20to%20improve%20downstream%20task%20performance.%20In%20this%20work%2C%20we%20conduct%20a%0Amulti-centric%20performance%20benchmark%20of%20object-centric%20approaches%2C%20focusing%20on%0ACritical%20View%20of%20Safety%20assessment%20in%20laparoscopic%20cholecystectomy%2C%20then%0Apropose%20an%20improved%20approach%20for%20unseen%20domain%20generalization.%0A%20%20Methods%3A%20We%20evaluate%20four%20object-centric%20approaches%20for%20domain%0Ageneralization%2C%20establishing%20baseline%20performance.%20Next%2C%20leveraging%20the%0Adisentangled%20nature%20of%20object-centric%20representations%2C%20we%20dissect%20one%20of%20these%0Amethods%20through%20a%20series%20of%20ablations%20%28e.g.%20ignoring%20either%20visual%20or%20semantic%0Afeatures%20for%20downstream%20classification%29.%20Finally%2C%20based%20on%20the%20results%20of%20these%0Aablations%2C%20we%20develop%20an%20optimized%20method%20specifically%20tailored%20for%20domain%0Ageneralization%2C%20LG-DG%2C%20that%20includes%20a%20novel%20disentanglement%20loss%20function.%0A%20%20Results%3A%20Our%20optimized%20approach%2C%20LG-DG%2C%20achieves%20an%20improvement%20of%209.28%25%20over%0Athe%20best%20baseline%20approach.%20More%20broadly%2C%20we%20show%20that%20object-centric%0Aapproaches%20are%20highly%20effective%20for%20domain%20generalization%20thanks%20to%20their%0Amodular%20approach%20to%20representation%20learning.%0A%20%20Conclusion%3A%20We%20investigate%20the%20use%20of%20object-centric%20methods%20for%20unseen%0Adomain%20generalization%2C%20identify%20method-agnostic%20factors%20critical%20for%0Aperformance%2C%20and%20present%20an%20optimized%20approach%20that%20substantially%20outperforms%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06953v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Latent%20Graph%20Representations%20of%20Surgical%20Scenes%20for%20Zero-Shot%0A%20%20Domain%20Transfer&entry.906535625=Siddhant%20Satyanaik%20and%20Aditya%20Murali%20and%20Deepak%20Alapatt%20and%20Xin%20Wang%20and%20Pietro%20Mascagni%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Purpose%3A%20Advances%20in%20deep%20learning%20have%20resulted%20in%20effective%20models%20for%0Asurgical%20video%20analysis%3B%20however%2C%20these%20models%20often%20fail%20to%20generalize%20across%0Amedical%20centers%20due%20to%20domain%20shift%20caused%20by%20variations%20in%20surgical%20workflow%2C%0Acamera%20setups%2C%20and%20patient%20demographics.%20Recently%2C%20object-centric%20learning%20has%0Aemerged%20as%20a%20promising%20approach%20for%20improved%20surgical%20scene%20understanding%2C%0Acapturing%20and%20disentangling%20visual%20and%20semantic%20properties%20of%20surgical%20tools%0Aand%20anatomy%20to%20improve%20downstream%20task%20performance.%20In%20this%20work%2C%20we%20conduct%20a%0Amulti-centric%20performance%20benchmark%20of%20object-centric%20approaches%2C%20focusing%20on%0ACritical%20View%20of%20Safety%20assessment%20in%20laparoscopic%20cholecystectomy%2C%20then%0Apropose%20an%20improved%20approach%20for%20unseen%20domain%20generalization.%0A%20%20Methods%3A%20We%20evaluate%20four%20object-centric%20approaches%20for%20domain%0Ageneralization%2C%20establishing%20baseline%20performance.%20Next%2C%20leveraging%20the%0Adisentangled%20nature%20of%20object-centric%20representations%2C%20we%20dissect%20one%20of%20these%0Amethods%20through%20a%20series%20of%20ablations%20%28e.g.%20ignoring%20either%20visual%20or%20semantic%0Afeatures%20for%20downstream%20classification%29.%20Finally%2C%20based%20on%20the%20results%20of%20these%0Aablations%2C%20we%20develop%20an%20optimized%20method%20specifically%20tailored%20for%20domain%0Ageneralization%2C%20LG-DG%2C%20that%20includes%20a%20novel%20disentanglement%20loss%20function.%0A%20%20Results%3A%20Our%20optimized%20approach%2C%20LG-DG%2C%20achieves%20an%20improvement%20of%209.28%25%20over%0Athe%20best%20baseline%20approach.%20More%20broadly%2C%20we%20show%20that%20object-centric%0Aapproaches%20are%20highly%20effective%20for%20domain%20generalization%20thanks%20to%20their%0Amodular%20approach%20to%20representation%20learning.%0A%20%20Conclusion%3A%20We%20investigate%20the%20use%20of%20object-centric%20methods%20for%20unseen%0Adomain%20generalization%2C%20identify%20method-agnostic%20factors%20critical%20for%0Aperformance%2C%20and%20present%20an%20optimized%20approach%20that%20substantially%20outperforms%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06953v1&entry.124074799=Read"},
{"title": "The Power of Noise: Toward a Unified Multi-modal Knowledge Graph\n  Representation Framework", "author": "Zhuo Chen and Yin Fang and Yichi Zhang and Lingbing Guo and Jiaoyan Chen and Huajun Chen and Wen Zhang", "abstract": "  The advancement of Multi-modal Pre-training highlights the necessity for a\nrobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.\nThis framework is crucial for integrating structured knowledge into multi-modal\nLarge Language Models (LLMs) at scale, aiming to alleviate issues like\nknowledge misconceptions and multi-modal hallucinations. In this work, to\nevaluate models' ability to accurately embed entities within MMKGs, we focus on\ntwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and\nMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose a\nnovel SNAG method that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking for the robust integration of multi-modal entity\nfeatures in KGs. By incorporating specific training objectives for both MKGC\nand MMEA, our approach achieves SOTA performance across a total of ten datasets\n(three for MKGC and seven for MEMA), demonstrating its robustness and\nversatility. Besides, SNAG can not only function as a standalone model but also\nenhance other existing methods, providing stable performance improvements. Our\ncode and data are available at: https://github.com/zjukg/SNAG.\n", "link": "http://arxiv.org/abs/2403.06832v1", "date": "2024-03-11", "relevancy": 2.2073, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5888}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5286}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5242}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Power%20of%20Noise%3A%20Toward%20a%20Unified%20Multi-modal%20Knowledge%20Graph%0A%20%20Representation%20Framework&body=Title%3A%20The%20Power%20of%20Noise%3A%20Toward%20a%20Unified%20Multi-modal%20Knowledge%20Graph%0A%20%20Representation%20Framework%0AAuthor%3A%20Zhuo%20Chen%20and%20Yin%20Fang%20and%20Yichi%20Zhang%20and%20Lingbing%20Guo%20and%20Jiaoyan%20Chen%20and%20Huajun%20Chen%20and%20Wen%20Zhang%0AAbstract%3A%20%20%20The%20advancement%20of%20Multi-modal%20Pre-training%20highlights%20the%20necessity%20for%20a%0Arobust%20Multi-Modal%20Knowledge%20Graph%20%28MMKG%29%20representation%20learning%20framework.%0AThis%20framework%20is%20crucial%20for%20integrating%20structured%20knowledge%20into%20multi-modal%0ALarge%20Language%20Models%20%28LLMs%29%20at%20scale%2C%20aiming%20to%20alleviate%20issues%20like%0Aknowledge%20misconceptions%20and%20multi-modal%20hallucinations.%20In%20this%20work%2C%20to%0Aevaluate%20models%27%20ability%20to%20accurately%20embed%20entities%20within%20MMKGs%2C%20we%20focus%20on%0Atwo%20widely%20researched%20tasks%3A%20Multi-modal%20Knowledge%20Graph%20Completion%20%28MKGC%29%20and%0AMulti-modal%20Entity%20Alignment%20%28MMEA%29.%20Building%20on%20this%20foundation%2C%20we%20propose%20a%0Anovel%20SNAG%20method%20that%20utilizes%20a%20Transformer-based%20architecture%20equipped%20with%0Amodality-level%20noise%20masking%20for%20the%20robust%20integration%20of%20multi-modal%20entity%0Afeatures%20in%20KGs.%20By%20incorporating%20specific%20training%20objectives%20for%20both%20MKGC%0Aand%20MMEA%2C%20our%20approach%20achieves%20SOTA%20performance%20across%20a%20total%20of%20ten%20datasets%0A%28three%20for%20MKGC%20and%20seven%20for%20MEMA%29%2C%20demonstrating%20its%20robustness%20and%0Aversatility.%20Besides%2C%20SNAG%20can%20not%20only%20function%20as%20a%20standalone%20model%20but%20also%0Aenhance%20other%20existing%20methods%2C%20providing%20stable%20performance%20improvements.%20Our%0Acode%20and%20data%20are%20available%20at%3A%20https%3A//github.com/zjukg/SNAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06832v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Power%20of%20Noise%3A%20Toward%20a%20Unified%20Multi-modal%20Knowledge%20Graph%0A%20%20Representation%20Framework&entry.906535625=Zhuo%20Chen%20and%20Yin%20Fang%20and%20Yichi%20Zhang%20and%20Lingbing%20Guo%20and%20Jiaoyan%20Chen%20and%20Huajun%20Chen%20and%20Wen%20Zhang&entry.1292438233=%20%20The%20advancement%20of%20Multi-modal%20Pre-training%20highlights%20the%20necessity%20for%20a%0Arobust%20Multi-Modal%20Knowledge%20Graph%20%28MMKG%29%20representation%20learning%20framework.%0AThis%20framework%20is%20crucial%20for%20integrating%20structured%20knowledge%20into%20multi-modal%0ALarge%20Language%20Models%20%28LLMs%29%20at%20scale%2C%20aiming%20to%20alleviate%20issues%20like%0Aknowledge%20misconceptions%20and%20multi-modal%20hallucinations.%20In%20this%20work%2C%20to%0Aevaluate%20models%27%20ability%20to%20accurately%20embed%20entities%20within%20MMKGs%2C%20we%20focus%20on%0Atwo%20widely%20researched%20tasks%3A%20Multi-modal%20Knowledge%20Graph%20Completion%20%28MKGC%29%20and%0AMulti-modal%20Entity%20Alignment%20%28MMEA%29.%20Building%20on%20this%20foundation%2C%20we%20propose%20a%0Anovel%20SNAG%20method%20that%20utilizes%20a%20Transformer-based%20architecture%20equipped%20with%0Amodality-level%20noise%20masking%20for%20the%20robust%20integration%20of%20multi-modal%20entity%0Afeatures%20in%20KGs.%20By%20incorporating%20specific%20training%20objectives%20for%20both%20MKGC%0Aand%20MMEA%2C%20our%20approach%20achieves%20SOTA%20performance%20across%20a%20total%20of%20ten%20datasets%0A%28three%20for%20MKGC%20and%20seven%20for%20MEMA%29%2C%20demonstrating%20its%20robustness%20and%0Aversatility.%20Besides%2C%20SNAG%20can%20not%20only%20function%20as%20a%20standalone%20model%20but%20also%0Aenhance%20other%20existing%20methods%2C%20providing%20stable%20performance%20improvements.%20Our%0Acode%20and%20data%20are%20available%20at%3A%20https%3A//github.com/zjukg/SNAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06832v1&entry.124074799=Read"},
{"title": "Enhancing Image Caption Generation Using Reinforcement Learning with\n  Human Feedback", "author": "Adarsh N L and Arun P V and Aravindh N L", "abstract": "  Research on generative models to produce human-aligned / human-preferred\noutputs has seen significant recent contributions. Between text and\nimage-generative models, we narrowed our focus to text-based generative models,\nparticularly to produce captions for images that align with human preferences.\nIn this research, we explored a potential method to amplify the performance of\nthe Deep Neural Network Model to generate captions that are preferred by\nhumans. This was achieved by integrating Supervised Learning and Reinforcement\nLearning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel\nloss function that is capable of optimizing the model based on human feedback\nis introduced. In this paper, we provide a concise sketch of our approach and\nresults, hoping to contribute to the ongoing advances in the field of\nhuman-aligned generative AI models.\n", "link": "http://arxiv.org/abs/2403.06735v1", "date": "2024-03-11", "relevancy": 2.2057, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5673}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.561}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5355}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Image%20Caption%20Generation%20Using%20Reinforcement%20Learning%20with%0A%20%20Human%20Feedback&body=Title%3A%20Enhancing%20Image%20Caption%20Generation%20Using%20Reinforcement%20Learning%20with%0A%20%20Human%20Feedback%0AAuthor%3A%20Adarsh%20N%20L%20and%20Arun%20P%20V%20and%20Aravindh%20N%20L%0AAbstract%3A%20%20%20Research%20on%20generative%20models%20to%20produce%20human-aligned%20/%20human-preferred%0Aoutputs%20has%20seen%20significant%20recent%20contributions.%20Between%20text%20and%0Aimage-generative%20models%2C%20we%20narrowed%20our%20focus%20to%20text-based%20generative%20models%2C%0Aparticularly%20to%20produce%20captions%20for%20images%20that%20align%20with%20human%20preferences.%0AIn%20this%20research%2C%20we%20explored%20a%20potential%20method%20to%20amplify%20the%20performance%20of%0Athe%20Deep%20Neural%20Network%20Model%20to%20generate%20captions%20that%20are%20preferred%20by%0Ahumans.%20This%20was%20achieved%20by%20integrating%20Supervised%20Learning%20and%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20using%20the%20Flickr8k%20dataset.%20Also%2C%20a%20novel%0Aloss%20function%20that%20is%20capable%20of%20optimizing%20the%20model%20based%20on%20human%20feedback%0Ais%20introduced.%20In%20this%20paper%2C%20we%20provide%20a%20concise%20sketch%20of%20our%20approach%20and%0Aresults%2C%20hoping%20to%20contribute%20to%20the%20ongoing%20advances%20in%20the%20field%20of%0Ahuman-aligned%20generative%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06735v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Image%20Caption%20Generation%20Using%20Reinforcement%20Learning%20with%0A%20%20Human%20Feedback&entry.906535625=Adarsh%20N%20L%20and%20Arun%20P%20V%20and%20Aravindh%20N%20L&entry.1292438233=%20%20Research%20on%20generative%20models%20to%20produce%20human-aligned%20/%20human-preferred%0Aoutputs%20has%20seen%20significant%20recent%20contributions.%20Between%20text%20and%0Aimage-generative%20models%2C%20we%20narrowed%20our%20focus%20to%20text-based%20generative%20models%2C%0Aparticularly%20to%20produce%20captions%20for%20images%20that%20align%20with%20human%20preferences.%0AIn%20this%20research%2C%20we%20explored%20a%20potential%20method%20to%20amplify%20the%20performance%20of%0Athe%20Deep%20Neural%20Network%20Model%20to%20generate%20captions%20that%20are%20preferred%20by%0Ahumans.%20This%20was%20achieved%20by%20integrating%20Supervised%20Learning%20and%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20using%20the%20Flickr8k%20dataset.%20Also%2C%20a%20novel%0Aloss%20function%20that%20is%20capable%20of%20optimizing%20the%20model%20based%20on%20human%20feedback%0Ais%20introduced.%20In%20this%20paper%2C%20we%20provide%20a%20concise%20sketch%20of%20our%20approach%20and%0Aresults%2C%20hoping%20to%20contribute%20to%20the%20ongoing%20advances%20in%20the%20field%20of%0Ahuman-aligned%20generative%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06735v1&entry.124074799=Read"},
{"title": "Genetic Learning for Designing Sim-to-Real Data Augmentations", "author": "Bram Vanherle and Nick Michiels and Frank Van Reeth", "abstract": "  Data augmentations are useful in closing the sim-to-real domain gap when\ntraining on synthetic data. This is because they widen the training data\ndistribution, thus encouraging the model to generalize better to other domains.\nMany image augmentation techniques exist, parametrized by different settings,\nsuch as strength and probability. This leads to a large space of different\npossible augmentation policies. Some policies work better than others for\novercoming the sim-to-real gap for specific datasets, and it is unclear why.\nThis paper presents two different interpretable metrics that can be combined to\npredict how well a certain augmentation policy will work for a specific\nsim-to-real setting, focusing on object detection. We validate our metrics by\ntraining many models with different augmentation policies and showing a strong\ncorrelation with performance on real data. Additionally, we introduce\nGeneticAugment, a genetic programming method that can leverage these metrics to\nautomatically design an augmentation policy for a specific dataset without\nneeding to train a model.\n", "link": "http://arxiv.org/abs/2403.06786v1", "date": "2024-03-11", "relevancy": 2.2047, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5599}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5528}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Genetic%20Learning%20for%20Designing%20Sim-to-Real%20Data%20Augmentations&body=Title%3A%20Genetic%20Learning%20for%20Designing%20Sim-to-Real%20Data%20Augmentations%0AAuthor%3A%20Bram%20Vanherle%20and%20Nick%20Michiels%20and%20Frank%20Van%20Reeth%0AAbstract%3A%20%20%20Data%20augmentations%20are%20useful%20in%20closing%20the%20sim-to-real%20domain%20gap%20when%0Atraining%20on%20synthetic%20data.%20This%20is%20because%20they%20widen%20the%20training%20data%0Adistribution%2C%20thus%20encouraging%20the%20model%20to%20generalize%20better%20to%20other%20domains.%0AMany%20image%20augmentation%20techniques%20exist%2C%20parametrized%20by%20different%20settings%2C%0Asuch%20as%20strength%20and%20probability.%20This%20leads%20to%20a%20large%20space%20of%20different%0Apossible%20augmentation%20policies.%20Some%20policies%20work%20better%20than%20others%20for%0Aovercoming%20the%20sim-to-real%20gap%20for%20specific%20datasets%2C%20and%20it%20is%20unclear%20why.%0AThis%20paper%20presents%20two%20different%20interpretable%20metrics%20that%20can%20be%20combined%20to%0Apredict%20how%20well%20a%20certain%20augmentation%20policy%20will%20work%20for%20a%20specific%0Asim-to-real%20setting%2C%20focusing%20on%20object%20detection.%20We%20validate%20our%20metrics%20by%0Atraining%20many%20models%20with%20different%20augmentation%20policies%20and%20showing%20a%20strong%0Acorrelation%20with%20performance%20on%20real%20data.%20Additionally%2C%20we%20introduce%0AGeneticAugment%2C%20a%20genetic%20programming%20method%20that%20can%20leverage%20these%20metrics%20to%0Aautomatically%20design%20an%20augmentation%20policy%20for%20a%20specific%20dataset%20without%0Aneeding%20to%20train%20a%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06786v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genetic%20Learning%20for%20Designing%20Sim-to-Real%20Data%20Augmentations&entry.906535625=Bram%20Vanherle%20and%20Nick%20Michiels%20and%20Frank%20Van%20Reeth&entry.1292438233=%20%20Data%20augmentations%20are%20useful%20in%20closing%20the%20sim-to-real%20domain%20gap%20when%0Atraining%20on%20synthetic%20data.%20This%20is%20because%20they%20widen%20the%20training%20data%0Adistribution%2C%20thus%20encouraging%20the%20model%20to%20generalize%20better%20to%20other%20domains.%0AMany%20image%20augmentation%20techniques%20exist%2C%20parametrized%20by%20different%20settings%2C%0Asuch%20as%20strength%20and%20probability.%20This%20leads%20to%20a%20large%20space%20of%20different%0Apossible%20augmentation%20policies.%20Some%20policies%20work%20better%20than%20others%20for%0Aovercoming%20the%20sim-to-real%20gap%20for%20specific%20datasets%2C%20and%20it%20is%20unclear%20why.%0AThis%20paper%20presents%20two%20different%20interpretable%20metrics%20that%20can%20be%20combined%20to%0Apredict%20how%20well%20a%20certain%20augmentation%20policy%20will%20work%20for%20a%20specific%0Asim-to-real%20setting%2C%20focusing%20on%20object%20detection.%20We%20validate%20our%20metrics%20by%0Atraining%20many%20models%20with%20different%20augmentation%20policies%20and%20showing%20a%20strong%0Acorrelation%20with%20performance%20on%20real%20data.%20Additionally%2C%20we%20introduce%0AGeneticAugment%2C%20a%20genetic%20programming%20method%20that%20can%20leverage%20these%20metrics%20to%0Aautomatically%20design%20an%20augmentation%20policy%20for%20a%20specific%20dataset%20without%0Aneeding%20to%20train%20a%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06786v1&entry.124074799=Read"},
{"title": "Transferring Relative Monocular Depth to Surgical Vision with Temporal\n  Consistency", "author": "Charlie Budd and Tom Vercauteren", "abstract": "  Relative monocular depth, inferring depth up to shift and scale from a single\nimage, is an active research topic. Recent deep learning models, trained on\nlarge and varied meta-datasets, now provide excellent performance in the domain\nof natural images. However, few datasets exist which provide ground truth depth\nfor endoscopic images, making training such models from scratch unfeasible.\nThis work investigates the transfer of these models into the surgical domain,\nand presents an effective and simple way to improve on standard supervision\nthrough the use of temporal consistency self-supervision. We show temporal\nconsistency significantly improves supervised training alone when transferring\nto the low-data regime of endoscopy, and outperforms the prevalent\nself-supervision technique for this task. In addition we show our method\ndrastically outperforms the state-of-the-art method from within the domain of\nendoscopy. We also release our code, model and ensembled meta-dataset,\nMeta-MED, establishing a strong benchmark for future work.\n", "link": "http://arxiv.org/abs/2403.06683v1", "date": "2024-03-11", "relevancy": 2.1994, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5633}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.535}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Transferring%20Relative%20Monocular%20Depth%20to%20Surgical%20Vision%20with%20Temporal%0A%20%20Consistency&body=Title%3A%20Transferring%20Relative%20Monocular%20Depth%20to%20Surgical%20Vision%20with%20Temporal%0A%20%20Consistency%0AAuthor%3A%20Charlie%20Budd%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Relative%20monocular%20depth%2C%20inferring%20depth%20up%20to%20shift%20and%20scale%20from%20a%20single%0Aimage%2C%20is%20an%20active%20research%20topic.%20Recent%20deep%20learning%20models%2C%20trained%20on%0Alarge%20and%20varied%20meta-datasets%2C%20now%20provide%20excellent%20performance%20in%20the%20domain%0Aof%20natural%20images.%20However%2C%20few%20datasets%20exist%20which%20provide%20ground%20truth%20depth%0Afor%20endoscopic%20images%2C%20making%20training%20such%20models%20from%20scratch%20unfeasible.%0AThis%20work%20investigates%20the%20transfer%20of%20these%20models%20into%20the%20surgical%20domain%2C%0Aand%20presents%20an%20effective%20and%20simple%20way%20to%20improve%20on%20standard%20supervision%0Athrough%20the%20use%20of%20temporal%20consistency%20self-supervision.%20We%20show%20temporal%0Aconsistency%20significantly%20improves%20supervised%20training%20alone%20when%20transferring%0Ato%20the%20low-data%20regime%20of%20endoscopy%2C%20and%20outperforms%20the%20prevalent%0Aself-supervision%20technique%20for%20this%20task.%20In%20addition%20we%20show%20our%20method%0Adrastically%20outperforms%20the%20state-of-the-art%20method%20from%20within%20the%20domain%20of%0Aendoscopy.%20We%20also%20release%20our%20code%2C%20model%20and%20ensembled%20meta-dataset%2C%0AMeta-MED%2C%20establishing%20a%20strong%20benchmark%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06683v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferring%20Relative%20Monocular%20Depth%20to%20Surgical%20Vision%20with%20Temporal%0A%20%20Consistency&entry.906535625=Charlie%20Budd%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Relative%20monocular%20depth%2C%20inferring%20depth%20up%20to%20shift%20and%20scale%20from%20a%20single%0Aimage%2C%20is%20an%20active%20research%20topic.%20Recent%20deep%20learning%20models%2C%20trained%20on%0Alarge%20and%20varied%20meta-datasets%2C%20now%20provide%20excellent%20performance%20in%20the%20domain%0Aof%20natural%20images.%20However%2C%20few%20datasets%20exist%20which%20provide%20ground%20truth%20depth%0Afor%20endoscopic%20images%2C%20making%20training%20such%20models%20from%20scratch%20unfeasible.%0AThis%20work%20investigates%20the%20transfer%20of%20these%20models%20into%20the%20surgical%20domain%2C%0Aand%20presents%20an%20effective%20and%20simple%20way%20to%20improve%20on%20standard%20supervision%0Athrough%20the%20use%20of%20temporal%20consistency%20self-supervision.%20We%20show%20temporal%0Aconsistency%20significantly%20improves%20supervised%20training%20alone%20when%20transferring%0Ato%20the%20low-data%20regime%20of%20endoscopy%2C%20and%20outperforms%20the%20prevalent%0Aself-supervision%20technique%20for%20this%20task.%20In%20addition%20we%20show%20our%20method%0Adrastically%20outperforms%20the%20state-of-the-art%20method%20from%20within%20the%20domain%20of%0Aendoscopy.%20We%20also%20release%20our%20code%2C%20model%20and%20ensembled%20meta-dataset%2C%0AMeta-MED%2C%20establishing%20a%20strong%20benchmark%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06683v1&entry.124074799=Read"},
{"title": "SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields\n  for Robotic Inspection", "author": "Yifu Tao and Yash Bhalgat and Lanke Frank Tarimo Fu and Matias Mattamala and Nived Chebrolu and Maurice Fallon", "abstract": "  We present a neural-field-based large-scale reconstruction system that fuses\nlidar and vision data to generate high-quality reconstructions that are\ngeometrically accurate and capture photo-realistic textures. This system adapts\nthe state-of-the-art neural radiance field (NeRF) representation to also\nincorporate lidar data which adds strong geometric constraints on the depth and\nsurface normals. We exploit the trajectory from a real-time lidar SLAM system\nto bootstrap a Structure-from-Motion (SfM) procedure to both significantly\nreduce the computation time and to provide metric scale which is crucial for\nlidar depth loss. We use submapping to scale the system to large-scale\nenvironments captured over long trajectories. We demonstrate the reconstruction\nsystem with data from a multi-camera, lidar sensor suite onboard a legged\nrobot, hand-held while scanning building scenes for 600 metres, and onboard an\naerial robot surveying a multi-storey mock disaster site-building. Website:\nhttps://ori-drs.github.io/projects/silvr/\n", "link": "http://arxiv.org/abs/2403.06877v1", "date": "2024-03-11", "relevancy": 2.186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5658}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5337}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20SiLVR%3A%20Scalable%20Lidar-Visual%20Reconstruction%20with%20Neural%20Radiance%20Fields%0A%20%20for%20Robotic%20Inspection&body=Title%3A%20SiLVR%3A%20Scalable%20Lidar-Visual%20Reconstruction%20with%20Neural%20Radiance%20Fields%0A%20%20for%20Robotic%20Inspection%0AAuthor%3A%20Yifu%20Tao%20and%20Yash%20Bhalgat%20and%20Lanke%20Frank%20Tarimo%20Fu%20and%20Matias%20Mattamala%20and%20Nived%20Chebrolu%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20We%20present%20a%20neural-field-based%20large-scale%20reconstruction%20system%20that%20fuses%0Alidar%20and%20vision%20data%20to%20generate%20high-quality%20reconstructions%20that%20are%0Ageometrically%20accurate%20and%20capture%20photo-realistic%20textures.%20This%20system%20adapts%0Athe%20state-of-the-art%20neural%20radiance%20field%20%28NeRF%29%20representation%20to%20also%0Aincorporate%20lidar%20data%20which%20adds%20strong%20geometric%20constraints%20on%20the%20depth%20and%0Asurface%20normals.%20We%20exploit%20the%20trajectory%20from%20a%20real-time%20lidar%20SLAM%20system%0Ato%20bootstrap%20a%20Structure-from-Motion%20%28SfM%29%20procedure%20to%20both%20significantly%0Areduce%20the%20computation%20time%20and%20to%20provide%20metric%20scale%20which%20is%20crucial%20for%0Alidar%20depth%20loss.%20We%20use%20submapping%20to%20scale%20the%20system%20to%20large-scale%0Aenvironments%20captured%20over%20long%20trajectories.%20We%20demonstrate%20the%20reconstruction%0Asystem%20with%20data%20from%20a%20multi-camera%2C%20lidar%20sensor%20suite%20onboard%20a%20legged%0Arobot%2C%20hand-held%20while%20scanning%20building%20scenes%20for%20600%20metres%2C%20and%20onboard%20an%0Aaerial%20robot%20surveying%20a%20multi-storey%20mock%20disaster%20site-building.%20Website%3A%0Ahttps%3A//ori-drs.github.io/projects/silvr/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06877v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiLVR%3A%20Scalable%20Lidar-Visual%20Reconstruction%20with%20Neural%20Radiance%20Fields%0A%20%20for%20Robotic%20Inspection&entry.906535625=Yifu%20Tao%20and%20Yash%20Bhalgat%20and%20Lanke%20Frank%20Tarimo%20Fu%20and%20Matias%20Mattamala%20and%20Nived%20Chebrolu%20and%20Maurice%20Fallon&entry.1292438233=%20%20We%20present%20a%20neural-field-based%20large-scale%20reconstruction%20system%20that%20fuses%0Alidar%20and%20vision%20data%20to%20generate%20high-quality%20reconstructions%20that%20are%0Ageometrically%20accurate%20and%20capture%20photo-realistic%20textures.%20This%20system%20adapts%0Athe%20state-of-the-art%20neural%20radiance%20field%20%28NeRF%29%20representation%20to%20also%0Aincorporate%20lidar%20data%20which%20adds%20strong%20geometric%20constraints%20on%20the%20depth%20and%0Asurface%20normals.%20We%20exploit%20the%20trajectory%20from%20a%20real-time%20lidar%20SLAM%20system%0Ato%20bootstrap%20a%20Structure-from-Motion%20%28SfM%29%20procedure%20to%20both%20significantly%0Areduce%20the%20computation%20time%20and%20to%20provide%20metric%20scale%20which%20is%20crucial%20for%0Alidar%20depth%20loss.%20We%20use%20submapping%20to%20scale%20the%20system%20to%20large-scale%0Aenvironments%20captured%20over%20long%20trajectories.%20We%20demonstrate%20the%20reconstruction%0Asystem%20with%20data%20from%20a%20multi-camera%2C%20lidar%20sensor%20suite%20onboard%20a%20legged%0Arobot%2C%20hand-held%20while%20scanning%20building%20scenes%20for%20600%20metres%2C%20and%20onboard%20an%0Aaerial%20robot%20surveying%20a%20multi-storey%20mock%20disaster%20site-building.%20Website%3A%0Ahttps%3A//ori-drs.github.io/projects/silvr/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06877v1&entry.124074799=Read"},
{"title": "Exploiting Style Latent Flows for Generalizing Deepfake Detection Video\n  Detection", "author": "Jongwook Choi and Taehoon Kim and Yonghyun Jeong and Seungryul Baek and Jongwon Choi", "abstract": "  This paper presents a new approach for the detection of fake videos, based on\nthe analysis of style latent vectors and their abnormal behavior in temporal\nchanges in the generated videos. We discovered that the generated facial videos\nsuffer from the temporal distinctiveness in the temporal changes of style\nlatent vectors, which are inevitable during the generation of temporally stable\nvideos with various facial expressions and geometric transformations. Our\nframework utilizes the StyleGRU module, trained by contrastive learning, to\nrepresent the dynamic properties of style latent vectors. Additionally, we\nintroduce a style attention module that integrates StyleGRU-generated features\nwith content-based features, enabling the detection of visual and temporal\nartifacts. We demonstrate our approach across various benchmark scenarios in\ndeepfake detection, showing its superiority in cross-dataset and\ncross-manipulation scenarios. Through further analysis, we also validate the\nimportance of using temporal changes of style latent vectors to improve the\ngenerality of deepfake video detection.\n", "link": "http://arxiv.org/abs/2403.06592v1", "date": "2024-03-11", "relevancy": 2.1799, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.555}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5505}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5354}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Style%20Latent%20Flows%20for%20Generalizing%20Deepfake%20Detection%20Video%0A%20%20Detection&body=Title%3A%20Exploiting%20Style%20Latent%20Flows%20for%20Generalizing%20Deepfake%20Detection%20Video%0A%20%20Detection%0AAuthor%3A%20Jongwook%20Choi%20and%20Taehoon%20Kim%20and%20Yonghyun%20Jeong%20and%20Seungryul%20Baek%20and%20Jongwon%20Choi%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20for%20the%20detection%20of%20fake%20videos%2C%20based%20on%0Athe%20analysis%20of%20style%20latent%20vectors%20and%20their%20abnormal%20behavior%20in%20temporal%0Achanges%20in%20the%20generated%20videos.%20We%20discovered%20that%20the%20generated%20facial%20videos%0Asuffer%20from%20the%20temporal%20distinctiveness%20in%20the%20temporal%20changes%20of%20style%0Alatent%20vectors%2C%20which%20are%20inevitable%20during%20the%20generation%20of%20temporally%20stable%0Avideos%20with%20various%20facial%20expressions%20and%20geometric%20transformations.%20Our%0Aframework%20utilizes%20the%20StyleGRU%20module%2C%20trained%20by%20contrastive%20learning%2C%20to%0Arepresent%20the%20dynamic%20properties%20of%20style%20latent%20vectors.%20Additionally%2C%20we%0Aintroduce%20a%20style%20attention%20module%20that%20integrates%20StyleGRU-generated%20features%0Awith%20content-based%20features%2C%20enabling%20the%20detection%20of%20visual%20and%20temporal%0Aartifacts.%20We%20demonstrate%20our%20approach%20across%20various%20benchmark%20scenarios%20in%0Adeepfake%20detection%2C%20showing%20its%20superiority%20in%20cross-dataset%20and%0Across-manipulation%20scenarios.%20Through%20further%20analysis%2C%20we%20also%20validate%20the%0Aimportance%20of%20using%20temporal%20changes%20of%20style%20latent%20vectors%20to%20improve%20the%0Agenerality%20of%20deepfake%20video%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06592v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Style%20Latent%20Flows%20for%20Generalizing%20Deepfake%20Detection%20Video%0A%20%20Detection&entry.906535625=Jongwook%20Choi%20and%20Taehoon%20Kim%20and%20Yonghyun%20Jeong%20and%20Seungryul%20Baek%20and%20Jongwon%20Choi&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20for%20the%20detection%20of%20fake%20videos%2C%20based%20on%0Athe%20analysis%20of%20style%20latent%20vectors%20and%20their%20abnormal%20behavior%20in%20temporal%0Achanges%20in%20the%20generated%20videos.%20We%20discovered%20that%20the%20generated%20facial%20videos%0Asuffer%20from%20the%20temporal%20distinctiveness%20in%20the%20temporal%20changes%20of%20style%0Alatent%20vectors%2C%20which%20are%20inevitable%20during%20the%20generation%20of%20temporally%20stable%0Avideos%20with%20various%20facial%20expressions%20and%20geometric%20transformations.%20Our%0Aframework%20utilizes%20the%20StyleGRU%20module%2C%20trained%20by%20contrastive%20learning%2C%20to%0Arepresent%20the%20dynamic%20properties%20of%20style%20latent%20vectors.%20Additionally%2C%20we%0Aintroduce%20a%20style%20attention%20module%20that%20integrates%20StyleGRU-generated%20features%0Awith%20content-based%20features%2C%20enabling%20the%20detection%20of%20visual%20and%20temporal%0Aartifacts.%20We%20demonstrate%20our%20approach%20across%20various%20benchmark%20scenarios%20in%0Adeepfake%20detection%2C%20showing%20its%20superiority%20in%20cross-dataset%20and%0Across-manipulation%20scenarios.%20Through%20further%20analysis%2C%20we%20also%20validate%20the%0Aimportance%20of%20using%20temporal%20changes%20of%20style%20latent%20vectors%20to%20improve%20the%0Agenerality%20of%20deepfake%20video%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06592v1&entry.124074799=Read"},
{"title": "LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video\n  Compression", "author": "Wei Jiang and Junru Li and Kai Zhang and Li Zhang", "abstract": "  Existing learned video compression models employ flow net or deformable\nconvolutional networks (DCN) to estimate motion information. However, the\nlimited receptive fields of flow net and DCN inherently direct their\nattentiveness towards the local contexts. Global contexts, such as large-scale\nmotions and global correlations among frames are ignored, presenting a\nsignificant bottleneck for capturing accurate motions. To address this issue,\nwe propose a joint local and global motion compensation module (LGMC) for\nleaned video coding. More specifically, we adopt flow net for local motion\ncompensation. To capture global context, we employ the cross attention in\nfeature domain for motion compensation. In addition, to avoid the quadratic\ncomplexity of vanilla cross attention, we divide the softmax operations in\nattention into two independent softmax operations, leading to linear\ncomplexity. To validate the effectiveness of our proposed LGMC, we integrate it\nwith DCVC-TCM and obtain learned video compression with joint local and global\nmotion compensation (LVC-LGMC). Extensive experiments demonstrate that our\nLVC-LGMC has significant rate-distortion performance improvements over baseline\nDCVC-TCM.\n", "link": "http://arxiv.org/abs/2402.00680v3", "date": "2024-03-11", "relevancy": 2.1792, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5658}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5586}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5226}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LVC-LGMC%3A%20Joint%20Local%20and%20Global%20Motion%20Compensation%20for%20Learned%20Video%0A%20%20Compression&body=Title%3A%20LVC-LGMC%3A%20Joint%20Local%20and%20Global%20Motion%20Compensation%20for%20Learned%20Video%0A%20%20Compression%0AAuthor%3A%20Wei%20Jiang%20and%20Junru%20Li%20and%20Kai%20Zhang%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Existing%20learned%20video%20compression%20models%20employ%20flow%20net%20or%20deformable%0Aconvolutional%20networks%20%28DCN%29%20to%20estimate%20motion%20information.%20However%2C%20the%0Alimited%20receptive%20fields%20of%20flow%20net%20and%20DCN%20inherently%20direct%20their%0Aattentiveness%20towards%20the%20local%20contexts.%20Global%20contexts%2C%20such%20as%20large-scale%0Amotions%20and%20global%20correlations%20among%20frames%20are%20ignored%2C%20presenting%20a%0Asignificant%20bottleneck%20for%20capturing%20accurate%20motions.%20To%20address%20this%20issue%2C%0Awe%20propose%20a%20joint%20local%20and%20global%20motion%20compensation%20module%20%28LGMC%29%20for%0Aleaned%20video%20coding.%20More%20specifically%2C%20we%20adopt%20flow%20net%20for%20local%20motion%0Acompensation.%20To%20capture%20global%20context%2C%20we%20employ%20the%20cross%20attention%20in%0Afeature%20domain%20for%20motion%20compensation.%20In%20addition%2C%20to%20avoid%20the%20quadratic%0Acomplexity%20of%20vanilla%20cross%20attention%2C%20we%20divide%20the%20softmax%20operations%20in%0Aattention%20into%20two%20independent%20softmax%20operations%2C%20leading%20to%20linear%0Acomplexity.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%20LGMC%2C%20we%20integrate%20it%0Awith%20DCVC-TCM%20and%20obtain%20learned%20video%20compression%20with%20joint%20local%20and%20global%0Amotion%20compensation%20%28LVC-LGMC%29.%20Extensive%20experiments%20demonstrate%20that%20our%0ALVC-LGMC%20has%20significant%20rate-distortion%20performance%20improvements%20over%20baseline%0ADCVC-TCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00680v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVC-LGMC%3A%20Joint%20Local%20and%20Global%20Motion%20Compensation%20for%20Learned%20Video%0A%20%20Compression&entry.906535625=Wei%20Jiang%20and%20Junru%20Li%20and%20Kai%20Zhang%20and%20Li%20Zhang&entry.1292438233=%20%20Existing%20learned%20video%20compression%20models%20employ%20flow%20net%20or%20deformable%0Aconvolutional%20networks%20%28DCN%29%20to%20estimate%20motion%20information.%20However%2C%20the%0Alimited%20receptive%20fields%20of%20flow%20net%20and%20DCN%20inherently%20direct%20their%0Aattentiveness%20towards%20the%20local%20contexts.%20Global%20contexts%2C%20such%20as%20large-scale%0Amotions%20and%20global%20correlations%20among%20frames%20are%20ignored%2C%20presenting%20a%0Asignificant%20bottleneck%20for%20capturing%20accurate%20motions.%20To%20address%20this%20issue%2C%0Awe%20propose%20a%20joint%20local%20and%20global%20motion%20compensation%20module%20%28LGMC%29%20for%0Aleaned%20video%20coding.%20More%20specifically%2C%20we%20adopt%20flow%20net%20for%20local%20motion%0Acompensation.%20To%20capture%20global%20context%2C%20we%20employ%20the%20cross%20attention%20in%0Afeature%20domain%20for%20motion%20compensation.%20In%20addition%2C%20to%20avoid%20the%20quadratic%0Acomplexity%20of%20vanilla%20cross%20attention%2C%20we%20divide%20the%20softmax%20operations%20in%0Aattention%20into%20two%20independent%20softmax%20operations%2C%20leading%20to%20linear%0Acomplexity.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%20LGMC%2C%20we%20integrate%20it%0Awith%20DCVC-TCM%20and%20obtain%20learned%20video%20compression%20with%20joint%20local%20and%20global%0Amotion%20compensation%20%28LVC-LGMC%29.%20Extensive%20experiments%20demonstrate%20that%20our%0ALVC-LGMC%20has%20significant%20rate-distortion%20performance%20improvements%20over%20baseline%0ADCVC-TCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00680v3&entry.124074799=Read"},
{"title": "Last Iterate Convergence of Incremental Methods and Applications in\n  Continual Learning", "author": "Xufeng Cai and Jelena Diakonikolas", "abstract": "  Incremental gradient methods and incremental proximal methods are a\nfundamental class of optimization algorithms used for solving finite sum\nproblems, broadly studied in the literature. Yet, when it comes to their\nconvergence guarantees, nonasymptotic (first-order or proximal) oracle\ncomplexity bounds have been obtained fairly recently, almost exclusively\napplying to the average iterate. Motivated by applications in continual\nlearning, we obtain the first convergence guarantees for the last iterate of\nboth incremental gradient and incremental proximal methods, in general convex\nsmooth (for both) and convex Lipschitz (for the proximal variants) settings.\nOur oracle complexity bounds for the last iterate nearly match (i.e., match up\nto a square-root-log or a log factor) the best known oracle complexity bounds\nfor the average iterate, for both classes of methods. We further obtain\ngeneralizations of our results to weighted averaging of the iterates with\nincreasing weights, which can be seen as interpolating between the last iterate\nand the average iterate guarantees. Additionally, we discuss how our results\ncan be generalized to variants of studied incremental methods with permuted\nordering of updates. Our results generalize last iterate guarantees for\nincremental methods compared to state of the art, as such results were\npreviously known only for overparameterized linear models, which correspond to\nconvex quadratic problems with infinitely many solutions.\n", "link": "http://arxiv.org/abs/2403.06873v1", "date": "2024-03-11", "relevancy": 2.1783, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4384}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4355}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.433}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Last%20Iterate%20Convergence%20of%20Incremental%20Methods%20and%20Applications%20in%0A%20%20Continual%20Learning&body=Title%3A%20Last%20Iterate%20Convergence%20of%20Incremental%20Methods%20and%20Applications%20in%0A%20%20Continual%20Learning%0AAuthor%3A%20Xufeng%20Cai%20and%20Jelena%20Diakonikolas%0AAbstract%3A%20%20%20Incremental%20gradient%20methods%20and%20incremental%20proximal%20methods%20are%20a%0Afundamental%20class%20of%20optimization%20algorithms%20used%20for%20solving%20finite%20sum%0Aproblems%2C%20broadly%20studied%20in%20the%20literature.%20Yet%2C%20when%20it%20comes%20to%20their%0Aconvergence%20guarantees%2C%20nonasymptotic%20%28first-order%20or%20proximal%29%20oracle%0Acomplexity%20bounds%20have%20been%20obtained%20fairly%20recently%2C%20almost%20exclusively%0Aapplying%20to%20the%20average%20iterate.%20Motivated%20by%20applications%20in%20continual%0Alearning%2C%20we%20obtain%20the%20first%20convergence%20guarantees%20for%20the%20last%20iterate%20of%0Aboth%20incremental%20gradient%20and%20incremental%20proximal%20methods%2C%20in%20general%20convex%0Asmooth%20%28for%20both%29%20and%20convex%20Lipschitz%20%28for%20the%20proximal%20variants%29%20settings.%0AOur%20oracle%20complexity%20bounds%20for%20the%20last%20iterate%20nearly%20match%20%28i.e.%2C%20match%20up%0Ato%20a%20square-root-log%20or%20a%20log%20factor%29%20the%20best%20known%20oracle%20complexity%20bounds%0Afor%20the%20average%20iterate%2C%20for%20both%20classes%20of%20methods.%20We%20further%20obtain%0Ageneralizations%20of%20our%20results%20to%20weighted%20averaging%20of%20the%20iterates%20with%0Aincreasing%20weights%2C%20which%20can%20be%20seen%20as%20interpolating%20between%20the%20last%20iterate%0Aand%20the%20average%20iterate%20guarantees.%20Additionally%2C%20we%20discuss%20how%20our%20results%0Acan%20be%20generalized%20to%20variants%20of%20studied%20incremental%20methods%20with%20permuted%0Aordering%20of%20updates.%20Our%20results%20generalize%20last%20iterate%20guarantees%20for%0Aincremental%20methods%20compared%20to%20state%20of%20the%20art%2C%20as%20such%20results%20were%0Apreviously%20known%20only%20for%20overparameterized%20linear%20models%2C%20which%20correspond%20to%0Aconvex%20quadratic%20problems%20with%20infinitely%20many%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06873v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Last%20Iterate%20Convergence%20of%20Incremental%20Methods%20and%20Applications%20in%0A%20%20Continual%20Learning&entry.906535625=Xufeng%20Cai%20and%20Jelena%20Diakonikolas&entry.1292438233=%20%20Incremental%20gradient%20methods%20and%20incremental%20proximal%20methods%20are%20a%0Afundamental%20class%20of%20optimization%20algorithms%20used%20for%20solving%20finite%20sum%0Aproblems%2C%20broadly%20studied%20in%20the%20literature.%20Yet%2C%20when%20it%20comes%20to%20their%0Aconvergence%20guarantees%2C%20nonasymptotic%20%28first-order%20or%20proximal%29%20oracle%0Acomplexity%20bounds%20have%20been%20obtained%20fairly%20recently%2C%20almost%20exclusively%0Aapplying%20to%20the%20average%20iterate.%20Motivated%20by%20applications%20in%20continual%0Alearning%2C%20we%20obtain%20the%20first%20convergence%20guarantees%20for%20the%20last%20iterate%20of%0Aboth%20incremental%20gradient%20and%20incremental%20proximal%20methods%2C%20in%20general%20convex%0Asmooth%20%28for%20both%29%20and%20convex%20Lipschitz%20%28for%20the%20proximal%20variants%29%20settings.%0AOur%20oracle%20complexity%20bounds%20for%20the%20last%20iterate%20nearly%20match%20%28i.e.%2C%20match%20up%0Ato%20a%20square-root-log%20or%20a%20log%20factor%29%20the%20best%20known%20oracle%20complexity%20bounds%0Afor%20the%20average%20iterate%2C%20for%20both%20classes%20of%20methods.%20We%20further%20obtain%0Ageneralizations%20of%20our%20results%20to%20weighted%20averaging%20of%20the%20iterates%20with%0Aincreasing%20weights%2C%20which%20can%20be%20seen%20as%20interpolating%20between%20the%20last%20iterate%0Aand%20the%20average%20iterate%20guarantees.%20Additionally%2C%20we%20discuss%20how%20our%20results%0Acan%20be%20generalized%20to%20variants%20of%20studied%20incremental%20methods%20with%20permuted%0Aordering%20of%20updates.%20Our%20results%20generalize%20last%20iterate%20guarantees%20for%0Aincremental%20methods%20compared%20to%20state%20of%20the%20art%2C%20as%20such%20results%20were%0Apreviously%20known%20only%20for%20overparameterized%20linear%20models%2C%20which%20correspond%20to%0Aconvex%20quadratic%20problems%20with%20infinitely%20many%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06873v1&entry.124074799=Read"},
{"title": "LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression", "author": "Wei Jiang and Peirong Ning and Jiayu Yang and Yongqi Zhai and Feng Gao and Ronggang Wang", "abstract": "  Effective Receptive field (ERF) plays an important role in transform coding,\nwhich determines how much redundancy can be removed at most during transform\nand how many spatial priors can be utilized to synthesize textures during\ninverse transform. Existing methods rely on stacks of small kernels, whose ERF\nremains not large enough instead, or heavy non-local attention mechanisms,\nwhich limit the potential of high resolution image coding. To tackle this\nissue, we propose Large Receptive Field Transform Coding with Adaptive Weights\nfor Learned Image Compression (LLIC). Specifically, for the first time in\nlearned image compression community, we introduce a few large kernel-based\ndepth-wise convolutions to reduce more redundancy while maintaining modest\ncomplexity. Due to wide range of image diversity, we propose to enhance the\nadaptability of convolutions via generating weights in a self-conditioned\nmanner. The large kernels cooperate with non-linear embedding and gate\nmechanisms for better expressiveness and lighter point-wise interactions. We\nalso investigate improved training techniques to fully exploit the potential of\nlarge kernels. In addition, to enhance the interactions among channels, we\npropose the adaptive channel-wise bit allocation via generating channel\nimportance factor in a self-conditioned manner. To demonstrate the\neffectiveness of proposed transform coding, we align the entropy model to\ncompare with existing transform methods and obtain models LLIC-STF, LLIC-ELIC,\nLLIC-TCM. Extensive experiments demonstrate our proposed LLIC models have\nsignificant improvements over corresponding baselines and achieve\nstate-of-the-art performances and better trade-off between performance and\ncomplexity.\n", "link": "http://arxiv.org/abs/2304.09571v6", "date": "2024-03-11", "relevancy": 2.1753, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5609}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5294}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression&body=Title%3A%20LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression%0AAuthor%3A%20Wei%20Jiang%20and%20Peirong%20Ning%20and%20Jiayu%20Yang%20and%20Yongqi%20Zhai%20and%20Feng%20Gao%20and%20Ronggang%20Wang%0AAbstract%3A%20%20%20Effective%20Receptive%20field%20%28ERF%29%20plays%20an%20important%20role%20in%20transform%20coding%2C%0Awhich%20determines%20how%20much%20redundancy%20can%20be%20removed%20at%20most%20during%20transform%0Aand%20how%20many%20spatial%20priors%20can%20be%20utilized%20to%20synthesize%20textures%20during%0Ainverse%20transform.%20Existing%20methods%20rely%20on%20stacks%20of%20small%20kernels%2C%20whose%20ERF%0Aremains%20not%20large%20enough%20instead%2C%20or%20heavy%20non-local%20attention%20mechanisms%2C%0Awhich%20limit%20the%20potential%20of%20high%20resolution%20image%20coding.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%0Afor%20Learned%20Image%20Compression%20%28LLIC%29.%20Specifically%2C%20for%20the%20first%20time%20in%0Alearned%20image%20compression%20community%2C%20we%20introduce%20a%20few%20large%20kernel-based%0Adepth-wise%20convolutions%20to%20reduce%20more%20redundancy%20while%20maintaining%20modest%0Acomplexity.%20Due%20to%20wide%20range%20of%20image%20diversity%2C%20we%20propose%20to%20enhance%20the%0Aadaptability%20of%20convolutions%20via%20generating%20weights%20in%20a%20self-conditioned%0Amanner.%20The%20large%20kernels%20cooperate%20with%20non-linear%20embedding%20and%20gate%0Amechanisms%20for%20better%20expressiveness%20and%20lighter%20point-wise%20interactions.%20We%0Aalso%20investigate%20improved%20training%20techniques%20to%20fully%20exploit%20the%20potential%20of%0Alarge%20kernels.%20In%20addition%2C%20to%20enhance%20the%20interactions%20among%20channels%2C%20we%0Apropose%20the%20adaptive%20channel-wise%20bit%20allocation%20via%20generating%20channel%0Aimportance%20factor%20in%20a%20self-conditioned%20manner.%20To%20demonstrate%20the%0Aeffectiveness%20of%20proposed%20transform%20coding%2C%20we%20align%20the%20entropy%20model%20to%0Acompare%20with%20existing%20transform%20methods%20and%20obtain%20models%20LLIC-STF%2C%20LLIC-ELIC%2C%0ALLIC-TCM.%20Extensive%20experiments%20demonstrate%20our%20proposed%20LLIC%20models%20have%0Asignificant%20improvements%20over%20corresponding%20baselines%20and%20achieve%0Astate-of-the-art%20performances%20and%20better%20trade-off%20between%20performance%20and%0Acomplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09571v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLIC%3A%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%20for%0A%20%20Learned%20Image%20Compression&entry.906535625=Wei%20Jiang%20and%20Peirong%20Ning%20and%20Jiayu%20Yang%20and%20Yongqi%20Zhai%20and%20Feng%20Gao%20and%20Ronggang%20Wang&entry.1292438233=%20%20Effective%20Receptive%20field%20%28ERF%29%20plays%20an%20important%20role%20in%20transform%20coding%2C%0Awhich%20determines%20how%20much%20redundancy%20can%20be%20removed%20at%20most%20during%20transform%0Aand%20how%20many%20spatial%20priors%20can%20be%20utilized%20to%20synthesize%20textures%20during%0Ainverse%20transform.%20Existing%20methods%20rely%20on%20stacks%20of%20small%20kernels%2C%20whose%20ERF%0Aremains%20not%20large%20enough%20instead%2C%20or%20heavy%20non-local%20attention%20mechanisms%2C%0Awhich%20limit%20the%20potential%20of%20high%20resolution%20image%20coding.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20Large%20Receptive%20Field%20Transform%20Coding%20with%20Adaptive%20Weights%0Afor%20Learned%20Image%20Compression%20%28LLIC%29.%20Specifically%2C%20for%20the%20first%20time%20in%0Alearned%20image%20compression%20community%2C%20we%20introduce%20a%20few%20large%20kernel-based%0Adepth-wise%20convolutions%20to%20reduce%20more%20redundancy%20while%20maintaining%20modest%0Acomplexity.%20Due%20to%20wide%20range%20of%20image%20diversity%2C%20we%20propose%20to%20enhance%20the%0Aadaptability%20of%20convolutions%20via%20generating%20weights%20in%20a%20self-conditioned%0Amanner.%20The%20large%20kernels%20cooperate%20with%20non-linear%20embedding%20and%20gate%0Amechanisms%20for%20better%20expressiveness%20and%20lighter%20point-wise%20interactions.%20We%0Aalso%20investigate%20improved%20training%20techniques%20to%20fully%20exploit%20the%20potential%20of%0Alarge%20kernels.%20In%20addition%2C%20to%20enhance%20the%20interactions%20among%20channels%2C%20we%0Apropose%20the%20adaptive%20channel-wise%20bit%20allocation%20via%20generating%20channel%0Aimportance%20factor%20in%20a%20self-conditioned%20manner.%20To%20demonstrate%20the%0Aeffectiveness%20of%20proposed%20transform%20coding%2C%20we%20align%20the%20entropy%20model%20to%0Acompare%20with%20existing%20transform%20methods%20and%20obtain%20models%20LLIC-STF%2C%20LLIC-ELIC%2C%0ALLIC-TCM.%20Extensive%20experiments%20demonstrate%20our%20proposed%20LLIC%20models%20have%0Asignificant%20improvements%20over%20corresponding%20baselines%20and%20achieve%0Astate-of-the-art%20performances%20and%20better%20trade-off%20between%20performance%20and%0Acomplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09571v6&entry.124074799=Read"},
{"title": "Dynamic Perturbation-Adaptive Adversarial Training on Medical Image\n  Classification", "author": "Shuai Li and Xiaoguang Ma and Shancheng Jiang and Lu Meng", "abstract": "  Remarkable successes were made in Medical Image Classification (MIC)\nrecently, mainly due to wide applications of convolutional neural networks\n(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity\nwith raw data, raising serious concerns on network robustness. Although\nadversarial training (AT), in responding to malevolent AEs, was recognized as\nan effective approach to improve robustness, it was challenging to overcome\ngeneralization decline of networks caused by the AT. In this paper, in order to\nreserve high generalization while improving robustness, we proposed a dynamic\nperturbation-adaptive adversarial training (DPAAT) method, which placed AT in a\ndynamic learning environment to generate adaptive data-level perturbations and\nprovided a dynamically updated criterion by loss information collections to\nhandle the disadvantage of fixed perturbation sizes in conventional AT methods\nand the dependence on external transference. Comprehensive testing on\ndermatology HAM10000 dataset showed that the DPAAT not only achieved better\nrobustness improvement and generalization preservation but also significantly\nenhanced mean average precision and interpretability on various CNNs,\nindicating its great potential as a generic adversarial training method on the\nMIC.\n", "link": "http://arxiv.org/abs/2403.06798v1", "date": "2024-03-11", "relevancy": 2.1729, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5656}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5375}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5231}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Perturbation-Adaptive%20Adversarial%20Training%20on%20Medical%20Image%0A%20%20Classification&body=Title%3A%20Dynamic%20Perturbation-Adaptive%20Adversarial%20Training%20on%20Medical%20Image%0A%20%20Classification%0AAuthor%3A%20Shuai%20Li%20and%20Xiaoguang%20Ma%20and%20Shancheng%20Jiang%20and%20Lu%20Meng%0AAbstract%3A%20%20%20Remarkable%20successes%20were%20made%20in%20Medical%20Image%20Classification%20%28MIC%29%0Arecently%2C%20mainly%20due%20to%20wide%20applications%20of%20convolutional%20neural%20networks%0A%28CNNs%29.%20However%2C%20adversarial%20examples%20%28AEs%29%20exhibited%20imperceptible%20similarity%0Awith%20raw%20data%2C%20raising%20serious%20concerns%20on%20network%20robustness.%20Although%0Aadversarial%20training%20%28AT%29%2C%20in%20responding%20to%20malevolent%20AEs%2C%20was%20recognized%20as%0Aan%20effective%20approach%20to%20improve%20robustness%2C%20it%20was%20challenging%20to%20overcome%0Ageneralization%20decline%20of%20networks%20caused%20by%20the%20AT.%20In%20this%20paper%2C%20in%20order%20to%0Areserve%20high%20generalization%20while%20improving%20robustness%2C%20we%20proposed%20a%20dynamic%0Aperturbation-adaptive%20adversarial%20training%20%28DPAAT%29%20method%2C%20which%20placed%20AT%20in%20a%0Adynamic%20learning%20environment%20to%20generate%20adaptive%20data-level%20perturbations%20and%0Aprovided%20a%20dynamically%20updated%20criterion%20by%20loss%20information%20collections%20to%0Ahandle%20the%20disadvantage%20of%20fixed%20perturbation%20sizes%20in%20conventional%20AT%20methods%0Aand%20the%20dependence%20on%20external%20transference.%20Comprehensive%20testing%20on%0Adermatology%20HAM10000%20dataset%20showed%20that%20the%20DPAAT%20not%20only%20achieved%20better%0Arobustness%20improvement%20and%20generalization%20preservation%20but%20also%20significantly%0Aenhanced%20mean%20average%20precision%20and%20interpretability%20on%20various%20CNNs%2C%0Aindicating%20its%20great%20potential%20as%20a%20generic%20adversarial%20training%20method%20on%20the%0AMIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06798v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Perturbation-Adaptive%20Adversarial%20Training%20on%20Medical%20Image%0A%20%20Classification&entry.906535625=Shuai%20Li%20and%20Xiaoguang%20Ma%20and%20Shancheng%20Jiang%20and%20Lu%20Meng&entry.1292438233=%20%20Remarkable%20successes%20were%20made%20in%20Medical%20Image%20Classification%20%28MIC%29%0Arecently%2C%20mainly%20due%20to%20wide%20applications%20of%20convolutional%20neural%20networks%0A%28CNNs%29.%20However%2C%20adversarial%20examples%20%28AEs%29%20exhibited%20imperceptible%20similarity%0Awith%20raw%20data%2C%20raising%20serious%20concerns%20on%20network%20robustness.%20Although%0Aadversarial%20training%20%28AT%29%2C%20in%20responding%20to%20malevolent%20AEs%2C%20was%20recognized%20as%0Aan%20effective%20approach%20to%20improve%20robustness%2C%20it%20was%20challenging%20to%20overcome%0Ageneralization%20decline%20of%20networks%20caused%20by%20the%20AT.%20In%20this%20paper%2C%20in%20order%20to%0Areserve%20high%20generalization%20while%20improving%20robustness%2C%20we%20proposed%20a%20dynamic%0Aperturbation-adaptive%20adversarial%20training%20%28DPAAT%29%20method%2C%20which%20placed%20AT%20in%20a%0Adynamic%20learning%20environment%20to%20generate%20adaptive%20data-level%20perturbations%20and%0Aprovided%20a%20dynamically%20updated%20criterion%20by%20loss%20information%20collections%20to%0Ahandle%20the%20disadvantage%20of%20fixed%20perturbation%20sizes%20in%20conventional%20AT%20methods%0Aand%20the%20dependence%20on%20external%20transference.%20Comprehensive%20testing%20on%0Adermatology%20HAM10000%20dataset%20showed%20that%20the%20DPAAT%20not%20only%20achieved%20better%0Arobustness%20improvement%20and%20generalization%20preservation%20but%20also%20significantly%0Aenhanced%20mean%20average%20precision%20and%20interpretability%20on%20various%20CNNs%2C%0Aindicating%20its%20great%20potential%20as%20a%20generic%20adversarial%20training%20method%20on%20the%0AMIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06798v1&entry.124074799=Read"},
{"title": "DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with\n  Global-Local Depth Normalization", "author": "Jiahe Li and Jiawei Zhang and Xiao Bai and Jin Zheng and Xin Ning and Jun Zhou and Lin Gu", "abstract": "  Radiance fields have demonstrated impressive performance in synthesizing\nnovel views from sparse input views, yet prevailing methods suffer from high\ntraining costs and slow inference speed. This paper introduces DNGaussian, a\ndepth-regularized framework based on 3D Gaussian radiance fields, offering\nreal-time and high-quality few-shot novel view synthesis at low costs. Our\nmotivation stems from the highly efficient representation and surprising\nquality of the recent 3D Gaussian Splatting, despite it will encounter a\ngeometry degradation when input views decrease. In the Gaussian radiance\nfields, we find this degradation in scene geometry primarily lined to the\npositioning of Gaussian primitives and can be mitigated by depth constraint.\nConsequently, we propose a Hard and Soft Depth Regularization to restore\naccurate scene geometry under coarse monocular depth supervision while\nmaintaining a fine-grained color appearance. To further refine detailed\ngeometry reshaping, we introduce Global-Local Depth Normalization, enhancing\nthe focus on small local depth changes. Extensive experiments on LLFF, DTU, and\nBlender datasets demonstrate that DNGaussian outperforms state-of-the-art\nmethods, achieving comparable or better results with significantly reduced\nmemory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$\nfaster rendering speed.\n", "link": "http://arxiv.org/abs/2403.06912v1", "date": "2024-03-11", "relevancy": 2.1722, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5673}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5351}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5023}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DNGaussian%3A%20Optimizing%20Sparse-View%203D%20Gaussian%20Radiance%20Fields%20with%0A%20%20Global-Local%20Depth%20Normalization&body=Title%3A%20DNGaussian%3A%20Optimizing%20Sparse-View%203D%20Gaussian%20Radiance%20Fields%20with%0A%20%20Global-Local%20Depth%20Normalization%0AAuthor%3A%20Jiahe%20Li%20and%20Jiawei%20Zhang%20and%20Xiao%20Bai%20and%20Jin%20Zheng%20and%20Xin%20Ning%20and%20Jun%20Zhou%20and%20Lin%20Gu%0AAbstract%3A%20%20%20Radiance%20fields%20have%20demonstrated%20impressive%20performance%20in%20synthesizing%0Anovel%20views%20from%20sparse%20input%20views%2C%20yet%20prevailing%20methods%20suffer%20from%20high%0Atraining%20costs%20and%20slow%20inference%20speed.%20This%20paper%20introduces%20DNGaussian%2C%20a%0Adepth-regularized%20framework%20based%20on%203D%20Gaussian%20radiance%20fields%2C%20offering%0Areal-time%20and%20high-quality%20few-shot%20novel%20view%20synthesis%20at%20low%20costs.%20Our%0Amotivation%20stems%20from%20the%20highly%20efficient%20representation%20and%20surprising%0Aquality%20of%20the%20recent%203D%20Gaussian%20Splatting%2C%20despite%20it%20will%20encounter%20a%0Ageometry%20degradation%20when%20input%20views%20decrease.%20In%20the%20Gaussian%20radiance%0Afields%2C%20we%20find%20this%20degradation%20in%20scene%20geometry%20primarily%20lined%20to%20the%0Apositioning%20of%20Gaussian%20primitives%20and%20can%20be%20mitigated%20by%20depth%20constraint.%0AConsequently%2C%20we%20propose%20a%20Hard%20and%20Soft%20Depth%20Regularization%20to%20restore%0Aaccurate%20scene%20geometry%20under%20coarse%20monocular%20depth%20supervision%20while%0Amaintaining%20a%20fine-grained%20color%20appearance.%20To%20further%20refine%20detailed%0Ageometry%20reshaping%2C%20we%20introduce%20Global-Local%20Depth%20Normalization%2C%20enhancing%0Athe%20focus%20on%20small%20local%20depth%20changes.%20Extensive%20experiments%20on%20LLFF%2C%20DTU%2C%20and%0ABlender%20datasets%20demonstrate%20that%20DNGaussian%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20comparable%20or%20better%20results%20with%20significantly%20reduced%0Amemory%20cost%2C%20a%20%2425%20%5Ctimes%24%20reduction%20in%20training%20time%2C%20and%20over%20%243000%20%5Ctimes%24%0Afaster%20rendering%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06912v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNGaussian%3A%20Optimizing%20Sparse-View%203D%20Gaussian%20Radiance%20Fields%20with%0A%20%20Global-Local%20Depth%20Normalization&entry.906535625=Jiahe%20Li%20and%20Jiawei%20Zhang%20and%20Xiao%20Bai%20and%20Jin%20Zheng%20and%20Xin%20Ning%20and%20Jun%20Zhou%20and%20Lin%20Gu&entry.1292438233=%20%20Radiance%20fields%20have%20demonstrated%20impressive%20performance%20in%20synthesizing%0Anovel%20views%20from%20sparse%20input%20views%2C%20yet%20prevailing%20methods%20suffer%20from%20high%0Atraining%20costs%20and%20slow%20inference%20speed.%20This%20paper%20introduces%20DNGaussian%2C%20a%0Adepth-regularized%20framework%20based%20on%203D%20Gaussian%20radiance%20fields%2C%20offering%0Areal-time%20and%20high-quality%20few-shot%20novel%20view%20synthesis%20at%20low%20costs.%20Our%0Amotivation%20stems%20from%20the%20highly%20efficient%20representation%20and%20surprising%0Aquality%20of%20the%20recent%203D%20Gaussian%20Splatting%2C%20despite%20it%20will%20encounter%20a%0Ageometry%20degradation%20when%20input%20views%20decrease.%20In%20the%20Gaussian%20radiance%0Afields%2C%20we%20find%20this%20degradation%20in%20scene%20geometry%20primarily%20lined%20to%20the%0Apositioning%20of%20Gaussian%20primitives%20and%20can%20be%20mitigated%20by%20depth%20constraint.%0AConsequently%2C%20we%20propose%20a%20Hard%20and%20Soft%20Depth%20Regularization%20to%20restore%0Aaccurate%20scene%20geometry%20under%20coarse%20monocular%20depth%20supervision%20while%0Amaintaining%20a%20fine-grained%20color%20appearance.%20To%20further%20refine%20detailed%0Ageometry%20reshaping%2C%20we%20introduce%20Global-Local%20Depth%20Normalization%2C%20enhancing%0Athe%20focus%20on%20small%20local%20depth%20changes.%20Extensive%20experiments%20on%20LLFF%2C%20DTU%2C%20and%0ABlender%20datasets%20demonstrate%20that%20DNGaussian%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20comparable%20or%20better%20results%20with%20significantly%20reduced%0Amemory%20cost%2C%20a%20%2425%20%5Ctimes%24%20reduction%20in%20training%20time%2C%20and%20over%20%243000%20%5Ctimes%24%0Afaster%20rendering%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06912v1&entry.124074799=Read"},
{"title": "Node-weighted Graph Convolutional Network for Depression Detection in\n  Transcribed Clinical Interviews", "author": "Sergio Burdisso and Esa\u00fa Villatoro-Tello and Srikanth Madikeri and Petr Motlicek", "abstract": "  We propose a simple approach for weighting self-connecting edges in a Graph\nConvolutional Network (GCN) and show its impact on depression detection from\ntranscribed clinical interviews. To this end, we use a GCN for modeling\nnon-consecutive and long-distance semantics to classify the transcriptions into\ndepressed or control subjects. The proposed method aims to mitigate the\nlimiting assumptions of locality and the equal importance of self-connections\nvs. edges to neighboring nodes in GCNs, while preserving attractive features\nsuch as low computational cost, data agnostic, and interpretability\ncapabilities. We perform an exhaustive evaluation in two benchmark datasets.\nResults show that our approach consistently outperforms the vanilla GCN model\nas well as previously reported results, achieving an F1=0.84 on both datasets.\nFinally, a qualitative analysis illustrates the interpretability capabilities\nof the proposed approach and its alignment with previous findings in\npsychology.\n", "link": "http://arxiv.org/abs/2307.00920v2", "date": "2024-03-11", "relevancy": 2.165, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4698}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4189}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4103}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Node-weighted%20Graph%20Convolutional%20Network%20for%20Depression%20Detection%20in%0A%20%20Transcribed%20Clinical%20Interviews&body=Title%3A%20Node-weighted%20Graph%20Convolutional%20Network%20for%20Depression%20Detection%20in%0A%20%20Transcribed%20Clinical%20Interviews%0AAuthor%3A%20Sergio%20Burdisso%20and%20Esa%C3%BA%20Villatoro-Tello%20and%20Srikanth%20Madikeri%20and%20Petr%20Motlicek%0AAbstract%3A%20%20%20We%20propose%20a%20simple%20approach%20for%20weighting%20self-connecting%20edges%20in%20a%20Graph%0AConvolutional%20Network%20%28GCN%29%20and%20show%20its%20impact%20on%20depression%20detection%20from%0Atranscribed%20clinical%20interviews.%20To%20this%20end%2C%20we%20use%20a%20GCN%20for%20modeling%0Anon-consecutive%20and%20long-distance%20semantics%20to%20classify%20the%20transcriptions%20into%0Adepressed%20or%20control%20subjects.%20The%20proposed%20method%20aims%20to%20mitigate%20the%0Alimiting%20assumptions%20of%20locality%20and%20the%20equal%20importance%20of%20self-connections%0Avs.%20edges%20to%20neighboring%20nodes%20in%20GCNs%2C%20while%20preserving%20attractive%20features%0Asuch%20as%20low%20computational%20cost%2C%20data%20agnostic%2C%20and%20interpretability%0Acapabilities.%20We%20perform%20an%20exhaustive%20evaluation%20in%20two%20benchmark%20datasets.%0AResults%20show%20that%20our%20approach%20consistently%20outperforms%20the%20vanilla%20GCN%20model%0Aas%20well%20as%20previously%20reported%20results%2C%20achieving%20an%20F1%3D0.84%20on%20both%20datasets.%0AFinally%2C%20a%20qualitative%20analysis%20illustrates%20the%20interpretability%20capabilities%0Aof%20the%20proposed%20approach%20and%20its%20alignment%20with%20previous%20findings%20in%0Apsychology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.00920v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Node-weighted%20Graph%20Convolutional%20Network%20for%20Depression%20Detection%20in%0A%20%20Transcribed%20Clinical%20Interviews&entry.906535625=Sergio%20Burdisso%20and%20Esa%C3%BA%20Villatoro-Tello%20and%20Srikanth%20Madikeri%20and%20Petr%20Motlicek&entry.1292438233=%20%20We%20propose%20a%20simple%20approach%20for%20weighting%20self-connecting%20edges%20in%20a%20Graph%0AConvolutional%20Network%20%28GCN%29%20and%20show%20its%20impact%20on%20depression%20detection%20from%0Atranscribed%20clinical%20interviews.%20To%20this%20end%2C%20we%20use%20a%20GCN%20for%20modeling%0Anon-consecutive%20and%20long-distance%20semantics%20to%20classify%20the%20transcriptions%20into%0Adepressed%20or%20control%20subjects.%20The%20proposed%20method%20aims%20to%20mitigate%20the%0Alimiting%20assumptions%20of%20locality%20and%20the%20equal%20importance%20of%20self-connections%0Avs.%20edges%20to%20neighboring%20nodes%20in%20GCNs%2C%20while%20preserving%20attractive%20features%0Asuch%20as%20low%20computational%20cost%2C%20data%20agnostic%2C%20and%20interpretability%0Acapabilities.%20We%20perform%20an%20exhaustive%20evaluation%20in%20two%20benchmark%20datasets.%0AResults%20show%20that%20our%20approach%20consistently%20outperforms%20the%20vanilla%20GCN%20model%0Aas%20well%20as%20previously%20reported%20results%2C%20achieving%20an%20F1%3D0.84%20on%20both%20datasets.%0AFinally%2C%20a%20qualitative%20analysis%20illustrates%20the%20interpretability%20capabilities%0Aof%20the%20proposed%20approach%20and%20its%20alignment%20with%20previous%20findings%20in%0Apsychology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00920v2&entry.124074799=Read"},
{"title": "BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed\n  Dual-Branch Diffusion", "author": "Xuan Ju and Xian Liu and Xintao Wang and Yuxuan Bian and Ying Shan and Qiang Xu", "abstract": "  Image inpainting, the process of restoring corrupted images, has seen\nsignificant advancements with the advent of diffusion models (DMs). Despite\nthese advancements, current DM adaptations for inpainting, which involve\nmodifications to the sampling strategy or the development of\ninpainting-specific DMs, frequently suffer from semantic inconsistencies and\nreduced image quality. Addressing these challenges, our work introduces a novel\nparadigm: the division of masked image features and noisy latent into separate\nbranches. This division dramatically diminishes the model's learning load,\nfacilitating a nuanced incorporation of essential masked image information in a\nhierarchical fashion. Herein, we present BrushNet, a novel plug-and-play\ndual-branch model engineered to embed pixel-level masked image features into\nany pre-trained DM, guaranteeing coherent and enhanced image inpainting\noutcomes. Additionally, we introduce BrushData and BrushBench to facilitate\nsegmentation-based inpainting training and performance assessment. Our\nextensive experimental analysis demonstrates BrushNet's superior performance\nover existing models across seven key metrics, including image quality, mask\nregion preservation, and textual coherence.\n", "link": "http://arxiv.org/abs/2403.06976v1", "date": "2024-03-11", "relevancy": 2.1608, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5389}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5386}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20BrushNet%3A%20A%20Plug-and-Play%20Image%20Inpainting%20Model%20with%20Decomposed%0A%20%20Dual-Branch%20Diffusion&body=Title%3A%20BrushNet%3A%20A%20Plug-and-Play%20Image%20Inpainting%20Model%20with%20Decomposed%0A%20%20Dual-Branch%20Diffusion%0AAuthor%3A%20Xuan%20Ju%20and%20Xian%20Liu%20and%20Xintao%20Wang%20and%20Yuxuan%20Bian%20and%20Ying%20Shan%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20Image%20inpainting%2C%20the%20process%20of%20restoring%20corrupted%20images%2C%20has%20seen%0Asignificant%20advancements%20with%20the%20advent%20of%20diffusion%20models%20%28DMs%29.%20Despite%0Athese%20advancements%2C%20current%20DM%20adaptations%20for%20inpainting%2C%20which%20involve%0Amodifications%20to%20the%20sampling%20strategy%20or%20the%20development%20of%0Ainpainting-specific%20DMs%2C%20frequently%20suffer%20from%20semantic%20inconsistencies%20and%0Areduced%20image%20quality.%20Addressing%20these%20challenges%2C%20our%20work%20introduces%20a%20novel%0Aparadigm%3A%20the%20division%20of%20masked%20image%20features%20and%20noisy%20latent%20into%20separate%0Abranches.%20This%20division%20dramatically%20diminishes%20the%20model%27s%20learning%20load%2C%0Afacilitating%20a%20nuanced%20incorporation%20of%20essential%20masked%20image%20information%20in%20a%0Ahierarchical%20fashion.%20Herein%2C%20we%20present%20BrushNet%2C%20a%20novel%20plug-and-play%0Adual-branch%20model%20engineered%20to%20embed%20pixel-level%20masked%20image%20features%20into%0Aany%20pre-trained%20DM%2C%20guaranteeing%20coherent%20and%20enhanced%20image%20inpainting%0Aoutcomes.%20Additionally%2C%20we%20introduce%20BrushData%20and%20BrushBench%20to%20facilitate%0Asegmentation-based%20inpainting%20training%20and%20performance%20assessment.%20Our%0Aextensive%20experimental%20analysis%20demonstrates%20BrushNet%27s%20superior%20performance%0Aover%20existing%20models%20across%20seven%20key%20metrics%2C%20including%20image%20quality%2C%20mask%0Aregion%20preservation%2C%20and%20textual%20coherence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06976v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrushNet%3A%20A%20Plug-and-Play%20Image%20Inpainting%20Model%20with%20Decomposed%0A%20%20Dual-Branch%20Diffusion&entry.906535625=Xuan%20Ju%20and%20Xian%20Liu%20and%20Xintao%20Wang%20and%20Yuxuan%20Bian%20and%20Ying%20Shan%20and%20Qiang%20Xu&entry.1292438233=%20%20Image%20inpainting%2C%20the%20process%20of%20restoring%20corrupted%20images%2C%20has%20seen%0Asignificant%20advancements%20with%20the%20advent%20of%20diffusion%20models%20%28DMs%29.%20Despite%0Athese%20advancements%2C%20current%20DM%20adaptations%20for%20inpainting%2C%20which%20involve%0Amodifications%20to%20the%20sampling%20strategy%20or%20the%20development%20of%0Ainpainting-specific%20DMs%2C%20frequently%20suffer%20from%20semantic%20inconsistencies%20and%0Areduced%20image%20quality.%20Addressing%20these%20challenges%2C%20our%20work%20introduces%20a%20novel%0Aparadigm%3A%20the%20division%20of%20masked%20image%20features%20and%20noisy%20latent%20into%20separate%0Abranches.%20This%20division%20dramatically%20diminishes%20the%20model%27s%20learning%20load%2C%0Afacilitating%20a%20nuanced%20incorporation%20of%20essential%20masked%20image%20information%20in%20a%0Ahierarchical%20fashion.%20Herein%2C%20we%20present%20BrushNet%2C%20a%20novel%20plug-and-play%0Adual-branch%20model%20engineered%20to%20embed%20pixel-level%20masked%20image%20features%20into%0Aany%20pre-trained%20DM%2C%20guaranteeing%20coherent%20and%20enhanced%20image%20inpainting%0Aoutcomes.%20Additionally%2C%20we%20introduce%20BrushData%20and%20BrushBench%20to%20facilitate%0Asegmentation-based%20inpainting%20training%20and%20performance%20assessment.%20Our%0Aextensive%20experimental%20analysis%20demonstrates%20BrushNet%27s%20superior%20performance%0Aover%20existing%20models%20across%20seven%20key%20metrics%2C%20including%20image%20quality%2C%20mask%0Aregion%20preservation%2C%20and%20textual%20coherence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06976v1&entry.124074799=Read"},
{"title": "Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation", "author": "Theodore Barfoot and Luis Garcia-Peraza-Herrera and Ben Glocker and Tom Vercauteren", "abstract": "  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n", "link": "http://arxiv.org/abs/2403.06759v1", "date": "2024-03-11", "relevancy": 2.149, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5637}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.508}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Average%20Calibration%20Error%3A%20A%20Differentiable%20Loss%20for%20Improved%0A%20%20Reliability%20in%20Image%20Segmentation&body=Title%3A%20Average%20Calibration%20Error%3A%20A%20Differentiable%20Loss%20for%20Improved%0A%20%20Reliability%20in%20Image%20Segmentation%0AAuthor%3A%20Theodore%20Barfoot%20and%20Luis%20Garcia-Peraza-Herrera%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Deep%20neural%20networks%20for%20medical%20image%20segmentation%20often%20produce%0Aoverconfident%20results%20misaligned%20with%20empirical%20observations.%20Such%0Amiscalibration%2C%20challenges%20their%20clinical%20translation.%20We%20propose%20to%20use%0Amarginal%20L1%20average%20calibration%20error%20%28mL1-ACE%29%20as%20a%20novel%20auxiliary%20loss%0Afunction%20to%20improve%20pixel-wise%20calibration%20without%20compromising%20segmentation%0Aquality.%20We%20show%20that%20this%20loss%2C%20despite%20using%20hard%20binning%2C%20is%20directly%0Adifferentiable%2C%20bypassing%20the%20need%20for%20approximate%20but%20differentiable%20surrogate%0Aor%20soft%20binning%20approaches.%20Our%20work%20also%20introduces%20the%20concept%20of%20dataset%0Areliability%20histograms%20which%20generalises%20standard%20reliability%20diagrams%20for%0Arefined%20visual%20assessment%20of%20calibration%20in%20semantic%20segmentation%20aggregated%20at%0Athe%20dataset%20level.%20Using%20mL1-ACE%2C%20we%20reduce%20average%20and%20maximum%20calibration%0Aerror%20by%2045%25%20and%2055%25%20respectively%2C%20maintaining%20a%20Dice%20score%20of%2087%25%20on%20the%20BraTS%0A2021%20dataset.%20We%20share%20our%20code%20here%3A%20https%3A//github.com/cai4cai/ACE-DLIRIS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06759v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Average%20Calibration%20Error%3A%20A%20Differentiable%20Loss%20for%20Improved%0A%20%20Reliability%20in%20Image%20Segmentation&entry.906535625=Theodore%20Barfoot%20and%20Luis%20Garcia-Peraza-Herrera%20and%20Ben%20Glocker%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Deep%20neural%20networks%20for%20medical%20image%20segmentation%20often%20produce%0Aoverconfident%20results%20misaligned%20with%20empirical%20observations.%20Such%0Amiscalibration%2C%20challenges%20their%20clinical%20translation.%20We%20propose%20to%20use%0Amarginal%20L1%20average%20calibration%20error%20%28mL1-ACE%29%20as%20a%20novel%20auxiliary%20loss%0Afunction%20to%20improve%20pixel-wise%20calibration%20without%20compromising%20segmentation%0Aquality.%20We%20show%20that%20this%20loss%2C%20despite%20using%20hard%20binning%2C%20is%20directly%0Adifferentiable%2C%20bypassing%20the%20need%20for%20approximate%20but%20differentiable%20surrogate%0Aor%20soft%20binning%20approaches.%20Our%20work%20also%20introduces%20the%20concept%20of%20dataset%0Areliability%20histograms%20which%20generalises%20standard%20reliability%20diagrams%20for%0Arefined%20visual%20assessment%20of%20calibration%20in%20semantic%20segmentation%20aggregated%20at%0Athe%20dataset%20level.%20Using%20mL1-ACE%2C%20we%20reduce%20average%20and%20maximum%20calibration%0Aerror%20by%2045%25%20and%2055%25%20respectively%2C%20maintaining%20a%20Dice%20score%20of%2087%25%20on%20the%20BraTS%0A2021%20dataset.%20We%20share%20our%20code%20here%3A%20https%3A//github.com/cai4cai/ACE-DLIRIS%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06759v1&entry.124074799=Read"},
{"title": "PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification", "author": "Mert Gulsen and Batuhan Cengiz and Yusuf H. Sahin and Gozde Unal", "abstract": "  Point clouds are extensively employed in a variety of real-world applications\nsuch as robotics, autonomous driving and augmented reality. Despite the recent\nsuccess of point cloud neural networks, especially for safety-critical tasks,\nit is essential to also ensure the robustness of the model. A typical way to\nassess a model's robustness is through adversarial attacks, where test-time\nexamples are generated based on gradients to deceive the model. While many\ndifferent defense mechanisms are studied in 2D, studies on 3D point clouds have\nbeen relatively limited in the academic field. Inspired from PointDP, which\ndenoises the network inputs by diffusion, we propose Point Cloud Layerwise\nDiffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy.\nUnlike PointDP, we propagated the diffusion denoising after each layer to\nincrementally enhance the results. We apply our defense method to different\ntypes of commonly used point cloud models and adversarial attacks to evaluate\nits robustness. Our experiments demonstrate that the proposed defense method\nachieved results that are comparable to or surpass those of existing\nmethodologies, establishing robustness through a novel technique. Code is\navailable at https://github.com/batuceng/diffusion-layer-robustness-pc.\n", "link": "http://arxiv.org/abs/2403.06698v1", "date": "2024-03-11", "relevancy": 2.1448, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5348}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20PCLD%3A%20Point%20Cloud%20Layerwise%20Diffusion%20for%20Adversarial%20Purification&body=Title%3A%20PCLD%3A%20Point%20Cloud%20Layerwise%20Diffusion%20for%20Adversarial%20Purification%0AAuthor%3A%20Mert%20Gulsen%20and%20Batuhan%20Cengiz%20and%20Yusuf%20H.%20Sahin%20and%20Gozde%20Unal%0AAbstract%3A%20%20%20Point%20clouds%20are%20extensively%20employed%20in%20a%20variety%20of%20real-world%20applications%0Asuch%20as%20robotics%2C%20autonomous%20driving%20and%20augmented%20reality.%20Despite%20the%20recent%0Asuccess%20of%20point%20cloud%20neural%20networks%2C%20especially%20for%20safety-critical%20tasks%2C%0Ait%20is%20essential%20to%20also%20ensure%20the%20robustness%20of%20the%20model.%20A%20typical%20way%20to%0Aassess%20a%20model%27s%20robustness%20is%20through%20adversarial%20attacks%2C%20where%20test-time%0Aexamples%20are%20generated%20based%20on%20gradients%20to%20deceive%20the%20model.%20While%20many%0Adifferent%20defense%20mechanisms%20are%20studied%20in%202D%2C%20studies%20on%203D%20point%20clouds%20have%0Abeen%20relatively%20limited%20in%20the%20academic%20field.%20Inspired%20from%20PointDP%2C%20which%0Adenoises%20the%20network%20inputs%20by%20diffusion%2C%20we%20propose%20Point%20Cloud%20Layerwise%0ADiffusion%20%28PCLD%29%2C%20a%20layerwise%20diffusion%20based%203D%20point%20cloud%20defense%20strategy.%0AUnlike%20PointDP%2C%20we%20propagated%20the%20diffusion%20denoising%20after%20each%20layer%20to%0Aincrementally%20enhance%20the%20results.%20We%20apply%20our%20defense%20method%20to%20different%0Atypes%20of%20commonly%20used%20point%20cloud%20models%20and%20adversarial%20attacks%20to%20evaluate%0Aits%20robustness.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20defense%20method%0Aachieved%20results%20that%20are%20comparable%20to%20or%20surpass%20those%20of%20existing%0Amethodologies%2C%20establishing%20robustness%20through%20a%20novel%20technique.%20Code%20is%0Aavailable%20at%20https%3A//github.com/batuceng/diffusion-layer-robustness-pc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06698v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCLD%3A%20Point%20Cloud%20Layerwise%20Diffusion%20for%20Adversarial%20Purification&entry.906535625=Mert%20Gulsen%20and%20Batuhan%20Cengiz%20and%20Yusuf%20H.%20Sahin%20and%20Gozde%20Unal&entry.1292438233=%20%20Point%20clouds%20are%20extensively%20employed%20in%20a%20variety%20of%20real-world%20applications%0Asuch%20as%20robotics%2C%20autonomous%20driving%20and%20augmented%20reality.%20Despite%20the%20recent%0Asuccess%20of%20point%20cloud%20neural%20networks%2C%20especially%20for%20safety-critical%20tasks%2C%0Ait%20is%20essential%20to%20also%20ensure%20the%20robustness%20of%20the%20model.%20A%20typical%20way%20to%0Aassess%20a%20model%27s%20robustness%20is%20through%20adversarial%20attacks%2C%20where%20test-time%0Aexamples%20are%20generated%20based%20on%20gradients%20to%20deceive%20the%20model.%20While%20many%0Adifferent%20defense%20mechanisms%20are%20studied%20in%202D%2C%20studies%20on%203D%20point%20clouds%20have%0Abeen%20relatively%20limited%20in%20the%20academic%20field.%20Inspired%20from%20PointDP%2C%20which%0Adenoises%20the%20network%20inputs%20by%20diffusion%2C%20we%20propose%20Point%20Cloud%20Layerwise%0ADiffusion%20%28PCLD%29%2C%20a%20layerwise%20diffusion%20based%203D%20point%20cloud%20defense%20strategy.%0AUnlike%20PointDP%2C%20we%20propagated%20the%20diffusion%20denoising%20after%20each%20layer%20to%0Aincrementally%20enhance%20the%20results.%20We%20apply%20our%20defense%20method%20to%20different%0Atypes%20of%20commonly%20used%20point%20cloud%20models%20and%20adversarial%20attacks%20to%20evaluate%0Aits%20robustness.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20defense%20method%0Aachieved%20results%20that%20are%20comparable%20to%20or%20surpass%20those%20of%20existing%0Amethodologies%2C%20establishing%20robustness%20through%20a%20novel%20technique.%20Code%20is%0Aavailable%20at%20https%3A//github.com/batuceng/diffusion-layer-robustness-pc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06698v1&entry.124074799=Read"},
{"title": "Improving and generalizing flow-based generative models with minibatch\n  optimal transport", "author": "Alexander Tong and Kilian Fatras and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio", "abstract": "  Continuous normalizing flows (CNFs) are an attractive generative modeling\ntechnique, but they have been held back by limitations in their\nsimulation-based maximum likelihood training. We introduce the generalized\nconditional flow matching (CFM) technique, a family of simulation-free training\nobjectives for CNFs. CFM features a stable regression objective like that used\nto train the stochastic flow in diffusion models but enjoys the efficient\ninference of deterministic flow models. In contrast to both diffusion models\nand prior CNF training algorithms, CFM does not require the source distribution\nto be Gaussian or require evaluation of its density. A variant of our objective\nis optimal transport CFM (OT-CFM), which creates simpler flows that are more\nstable to train and lead to faster inference, as evaluated in our experiments.\nFurthermore, we show that when the true OT plan is available, our OT-CFM method\napproximates dynamic OT. Training CNFs with CFM improves results on a variety\nof conditional and unconditional generation tasks, such as inferring single\ncell dynamics, unsupervised image translation, and Schr\\\"odinger bridge\ninference.\n", "link": "http://arxiv.org/abs/2302.00482v4", "date": "2024-03-11", "relevancy": 2.1411, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6512}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5122}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.512}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Improving%20and%20generalizing%20flow-based%20generative%20models%20with%20minibatch%0A%20%20optimal%20transport&body=Title%3A%20Improving%20and%20generalizing%20flow-based%20generative%20models%20with%20minibatch%0A%20%20optimal%20transport%0AAuthor%3A%20Alexander%20Tong%20and%20Kilian%20Fatras%20and%20Nikolay%20Malkin%20and%20Guillaume%20Huguet%20and%20Yanlei%20Zhang%20and%20Jarrid%20Rector-Brooks%20and%20Guy%20Wolf%20and%20Yoshua%20Bengio%0AAbstract%3A%20%20%20Continuous%20normalizing%20flows%20%28CNFs%29%20are%20an%20attractive%20generative%20modeling%0Atechnique%2C%20but%20they%20have%20been%20held%20back%20by%20limitations%20in%20their%0Asimulation-based%20maximum%20likelihood%20training.%20We%20introduce%20the%20generalized%0Aconditional%20flow%20matching%20%28CFM%29%20technique%2C%20a%20family%20of%20simulation-free%20training%0Aobjectives%20for%20CNFs.%20CFM%20features%20a%20stable%20regression%20objective%20like%20that%20used%0Ato%20train%20the%20stochastic%20flow%20in%20diffusion%20models%20but%20enjoys%20the%20efficient%0Ainference%20of%20deterministic%20flow%20models.%20In%20contrast%20to%20both%20diffusion%20models%0Aand%20prior%20CNF%20training%20algorithms%2C%20CFM%20does%20not%20require%20the%20source%20distribution%0Ato%20be%20Gaussian%20or%20require%20evaluation%20of%20its%20density.%20A%20variant%20of%20our%20objective%0Ais%20optimal%20transport%20CFM%20%28OT-CFM%29%2C%20which%20creates%20simpler%20flows%20that%20are%20more%0Astable%20to%20train%20and%20lead%20to%20faster%20inference%2C%20as%20evaluated%20in%20our%20experiments.%0AFurthermore%2C%20we%20show%20that%20when%20the%20true%20OT%20plan%20is%20available%2C%20our%20OT-CFM%20method%0Aapproximates%20dynamic%20OT.%20Training%20CNFs%20with%20CFM%20improves%20results%20on%20a%20variety%0Aof%20conditional%20and%20unconditional%20generation%20tasks%2C%20such%20as%20inferring%20single%0Acell%20dynamics%2C%20unsupervised%20image%20translation%2C%20and%20Schr%5C%22odinger%20bridge%0Ainference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.00482v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20and%20generalizing%20flow-based%20generative%20models%20with%20minibatch%0A%20%20optimal%20transport&entry.906535625=Alexander%20Tong%20and%20Kilian%20Fatras%20and%20Nikolay%20Malkin%20and%20Guillaume%20Huguet%20and%20Yanlei%20Zhang%20and%20Jarrid%20Rector-Brooks%20and%20Guy%20Wolf%20and%20Yoshua%20Bengio&entry.1292438233=%20%20Continuous%20normalizing%20flows%20%28CNFs%29%20are%20an%20attractive%20generative%20modeling%0Atechnique%2C%20but%20they%20have%20been%20held%20back%20by%20limitations%20in%20their%0Asimulation-based%20maximum%20likelihood%20training.%20We%20introduce%20the%20generalized%0Aconditional%20flow%20matching%20%28CFM%29%20technique%2C%20a%20family%20of%20simulation-free%20training%0Aobjectives%20for%20CNFs.%20CFM%20features%20a%20stable%20regression%20objective%20like%20that%20used%0Ato%20train%20the%20stochastic%20flow%20in%20diffusion%20models%20but%20enjoys%20the%20efficient%0Ainference%20of%20deterministic%20flow%20models.%20In%20contrast%20to%20both%20diffusion%20models%0Aand%20prior%20CNF%20training%20algorithms%2C%20CFM%20does%20not%20require%20the%20source%20distribution%0Ato%20be%20Gaussian%20or%20require%20evaluation%20of%20its%20density.%20A%20variant%20of%20our%20objective%0Ais%20optimal%20transport%20CFM%20%28OT-CFM%29%2C%20which%20creates%20simpler%20flows%20that%20are%20more%0Astable%20to%20train%20and%20lead%20to%20faster%20inference%2C%20as%20evaluated%20in%20our%20experiments.%0AFurthermore%2C%20we%20show%20that%20when%20the%20true%20OT%20plan%20is%20available%2C%20our%20OT-CFM%20method%0Aapproximates%20dynamic%20OT.%20Training%20CNFs%20with%20CFM%20improves%20results%20on%20a%20variety%0Aof%20conditional%20and%20unconditional%20generation%20tasks%2C%20such%20as%20inferring%20single%0Acell%20dynamics%2C%20unsupervised%20image%20translation%2C%20and%20Schr%5C%22odinger%20bridge%0Ainference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.00482v4&entry.124074799=Read"},
{"title": "MEND: Meta dEmonstratioN Distillation for Efficient and Effective\n  In-Context Learning", "author": "Yichuan Li and Xiyao Ma and Sixing Lu and Kyumin Lee and Xiaohu Liu and Chenlei Guo", "abstract": "  Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models\n", "link": "http://arxiv.org/abs/2403.06914v1", "date": "2024-03-11", "relevancy": 2.1404, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5833}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5251}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MEND%3A%20Meta%20dEmonstratioN%20Distillation%20for%20Efficient%20and%20Effective%0A%20%20In-Context%20Learning&body=Title%3A%20MEND%3A%20Meta%20dEmonstratioN%20Distillation%20for%20Efficient%20and%20Effective%0A%20%20In-Context%20Learning%0AAuthor%3A%20Yichuan%20Li%20and%20Xiyao%20Ma%20and%20Sixing%20Lu%20and%20Kyumin%20Lee%20and%20Xiaohu%20Liu%20and%20Chenlei%20Guo%0AAbstract%3A%20%20%20Large%20Language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20in-context%20learning%0A%28ICL%29%20capabilities%2C%20where%20a%20LLM%20makes%20predictions%20for%20a%20given%20test%20input%0Atogether%20with%20a%20few%20input-output%20pairs%20%28demonstrations%29.%20Nevertheless%2C%20the%0Ainclusion%20of%20demonstrations%20leads%20to%20a%20quadratic%20increase%20in%20the%20computational%0Aoverhead%20of%20the%20self-attention%20mechanism.%20Existing%20solutions%20attempt%20to%20distill%0Alengthy%20demonstrations%20into%20compact%20vectors.%20However%2C%20they%20often%20require%0Atask-specific%20retraining%20or%20compromise%20LLM%27s%20in-context%20learning%20performance.%0ATo%20mitigate%20these%20challenges%2C%20we%20present%20Meta%20dEmonstratioN%20Distillation%0A%28MEND%29%2C%20where%20a%20language%20model%20learns%20to%20distill%20any%20lengthy%20demonstrations%0Ainto%20vectors%20without%20retraining%20for%20a%20new%20downstream%20task.%20We%20exploit%20the%0Aknowledge%20distillation%20to%20enhance%20alignment%20between%20MEND%20and%20LLM%2C%20achieving%0Aboth%20efficiency%20and%20effectiveness%20simultaneously.%20MEND%20is%20endowed%20with%20the%0Ameta-knowledge%20of%20distilling%20demonstrations%20through%20a%20two-stage%20training%0Aprocess%2C%20which%20includes%20meta-distillation%20pretraining%20and%20fine-tuning.%0AComprehensive%20evaluations%20across%20seven%20diverse%20ICL%20task%20partitions%20using%0Adecoder-only%20%28GPT-2%29%20and%20encoder-decoder%20%28T5%29%20attest%20to%20MEND%27s%20prowess.%20It%20not%0Aonly%20matches%20but%20often%20outperforms%20the%20Vanilla%20ICL%20as%20well%20as%20other%0Astate-of-the-art%20distillation%20models%2C%20while%20significantly%20reducing%20the%0Acomputational%20demands.%20This%20innovation%20promises%20enhanced%20scalability%20and%0Aefficiency%20for%20the%20practical%20deployment%20of%20large%20language%20models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06914v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEND%3A%20Meta%20dEmonstratioN%20Distillation%20for%20Efficient%20and%20Effective%0A%20%20In-Context%20Learning&entry.906535625=Yichuan%20Li%20and%20Xiyao%20Ma%20and%20Sixing%20Lu%20and%20Kyumin%20Lee%20and%20Xiaohu%20Liu%20and%20Chenlei%20Guo&entry.1292438233=%20%20Large%20Language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20in-context%20learning%0A%28ICL%29%20capabilities%2C%20where%20a%20LLM%20makes%20predictions%20for%20a%20given%20test%20input%0Atogether%20with%20a%20few%20input-output%20pairs%20%28demonstrations%29.%20Nevertheless%2C%20the%0Ainclusion%20of%20demonstrations%20leads%20to%20a%20quadratic%20increase%20in%20the%20computational%0Aoverhead%20of%20the%20self-attention%20mechanism.%20Existing%20solutions%20attempt%20to%20distill%0Alengthy%20demonstrations%20into%20compact%20vectors.%20However%2C%20they%20often%20require%0Atask-specific%20retraining%20or%20compromise%20LLM%27s%20in-context%20learning%20performance.%0ATo%20mitigate%20these%20challenges%2C%20we%20present%20Meta%20dEmonstratioN%20Distillation%0A%28MEND%29%2C%20where%20a%20language%20model%20learns%20to%20distill%20any%20lengthy%20demonstrations%0Ainto%20vectors%20without%20retraining%20for%20a%20new%20downstream%20task.%20We%20exploit%20the%0Aknowledge%20distillation%20to%20enhance%20alignment%20between%20MEND%20and%20LLM%2C%20achieving%0Aboth%20efficiency%20and%20effectiveness%20simultaneously.%20MEND%20is%20endowed%20with%20the%0Ameta-knowledge%20of%20distilling%20demonstrations%20through%20a%20two-stage%20training%0Aprocess%2C%20which%20includes%20meta-distillation%20pretraining%20and%20fine-tuning.%0AComprehensive%20evaluations%20across%20seven%20diverse%20ICL%20task%20partitions%20using%0Adecoder-only%20%28GPT-2%29%20and%20encoder-decoder%20%28T5%29%20attest%20to%20MEND%27s%20prowess.%20It%20not%0Aonly%20matches%20but%20often%20outperforms%20the%20Vanilla%20ICL%20as%20well%20as%20other%0Astate-of-the-art%20distillation%20models%2C%20while%20significantly%20reducing%20the%0Acomputational%20demands.%20This%20innovation%20promises%20enhanced%20scalability%20and%0Aefficiency%20for%20the%20practical%20deployment%20of%20large%20language%20models%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06914v1&entry.124074799=Read"},
{"title": "Poisoning Programs by Un-Repairing Code: Security Concerns of\n  AI-generated Code", "author": "Cristina Improta", "abstract": "  AI-based code generators have gained a fundamental role in assisting\ndevelopers in writing software starting from natural language (NL). However,\nsince these large language models are trained on massive volumes of data\ncollected from unreliable online sources (e.g., GitHub, Hugging Face), AI\nmodels become an easy target for data poisoning attacks, in which an attacker\ncorrupts the training data by injecting a small amount of poison into it, i.e.,\nastutely crafted malicious samples. In this position paper, we address the\nsecurity of AI code generators by identifying a novel data poisoning attack\nthat results in the generation of vulnerable code. Next, we devise an extensive\nevaluation of how these attacks impact state-of-the-art models for code\ngeneration. Lastly, we discuss potential solutions to overcome this threat.\n", "link": "http://arxiv.org/abs/2403.06675v1", "date": "2024-03-11", "relevancy": 2.1401, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.476}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4133}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3947}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Poisoning%20Programs%20by%20Un-Repairing%20Code%3A%20Security%20Concerns%20of%0A%20%20AI-generated%20Code&body=Title%3A%20Poisoning%20Programs%20by%20Un-Repairing%20Code%3A%20Security%20Concerns%20of%0A%20%20AI-generated%20Code%0AAuthor%3A%20Cristina%20Improta%0AAbstract%3A%20%20%20AI-based%20code%20generators%20have%20gained%20a%20fundamental%20role%20in%20assisting%0Adevelopers%20in%20writing%20software%20starting%20from%20natural%20language%20%28NL%29.%20However%2C%0Asince%20these%20large%20language%20models%20are%20trained%20on%20massive%20volumes%20of%20data%0Acollected%20from%20unreliable%20online%20sources%20%28e.g.%2C%20GitHub%2C%20Hugging%20Face%29%2C%20AI%0Amodels%20become%20an%20easy%20target%20for%20data%20poisoning%20attacks%2C%20in%20which%20an%20attacker%0Acorrupts%20the%20training%20data%20by%20injecting%20a%20small%20amount%20of%20poison%20into%20it%2C%20i.e.%2C%0Aastutely%20crafted%20malicious%20samples.%20In%20this%20position%20paper%2C%20we%20address%20the%0Asecurity%20of%20AI%20code%20generators%20by%20identifying%20a%20novel%20data%20poisoning%20attack%0Athat%20results%20in%20the%20generation%20of%20vulnerable%20code.%20Next%2C%20we%20devise%20an%20extensive%0Aevaluation%20of%20how%20these%20attacks%20impact%20state-of-the-art%20models%20for%20code%0Ageneration.%20Lastly%2C%20we%20discuss%20potential%20solutions%20to%20overcome%20this%20threat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06675v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poisoning%20Programs%20by%20Un-Repairing%20Code%3A%20Security%20Concerns%20of%0A%20%20AI-generated%20Code&entry.906535625=Cristina%20Improta&entry.1292438233=%20%20AI-based%20code%20generators%20have%20gained%20a%20fundamental%20role%20in%20assisting%0Adevelopers%20in%20writing%20software%20starting%20from%20natural%20language%20%28NL%29.%20However%2C%0Asince%20these%20large%20language%20models%20are%20trained%20on%20massive%20volumes%20of%20data%0Acollected%20from%20unreliable%20online%20sources%20%28e.g.%2C%20GitHub%2C%20Hugging%20Face%29%2C%20AI%0Amodels%20become%20an%20easy%20target%20for%20data%20poisoning%20attacks%2C%20in%20which%20an%20attacker%0Acorrupts%20the%20training%20data%20by%20injecting%20a%20small%20amount%20of%20poison%20into%20it%2C%20i.e.%2C%0Aastutely%20crafted%20malicious%20samples.%20In%20this%20position%20paper%2C%20we%20address%20the%0Asecurity%20of%20AI%20code%20generators%20by%20identifying%20a%20novel%20data%20poisoning%20attack%0Athat%20results%20in%20the%20generation%20of%20vulnerable%20code.%20Next%2C%20we%20devise%20an%20extensive%0Aevaluation%20of%20how%20these%20attacks%20impact%20state-of-the-art%20models%20for%20code%0Ageneration.%20Lastly%2C%20we%20discuss%20potential%20solutions%20to%20overcome%20this%20threat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06675v1&entry.124074799=Read"},
{"title": "Advancing Generalizable Remote Physiological Measurement through the\n  Integration of Explicit and Implicit Prior Knowledge", "author": "Yuting Zhang and Hao Lu and Xin Liu and Yingcong Chen and Kaishun Wu", "abstract": "  Remote photoplethysmography (rPPG) is a promising technology that captures\nphysiological signals from face videos, with potential applications in medical\nhealth, emotional computing, and biosecurity recognition. The demand for rPPG\ntasks has expanded from demonstrating good performance on intra-dataset testing\nto cross-dataset testing (i.e., domain generalization). However, most existing\nmethods have overlooked the prior knowledge of rPPG, resulting in poor\ngeneralization ability. In this paper, we propose a novel framework that\nsimultaneously utilizes explicit and implicit prior knowledge in the rPPG task.\nSpecifically, we systematically analyze the causes of noise sources (e.g.,\ndifferent camera, lighting, skin types, and movement) across different domains\nand incorporate these prior knowledge into the network. Additionally, we\nleverage a two-branch network to disentangle the physiological feature\ndistribution from noises through implicit label correlation. Our extensive\nexperiments demonstrate that the proposed method not only outperforms\nstate-of-the-art methods on RGB cross-dataset evaluation but also generalizes\nwell from RGB datasets to NIR datasets. The code is available at\nhttps://github.com/keke-nice/Greip.\n", "link": "http://arxiv.org/abs/2403.06947v1", "date": "2024-03-11", "relevancy": 2.1398, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5402}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5361}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Advancing%20Generalizable%20Remote%20Physiological%20Measurement%20through%20the%0A%20%20Integration%20of%20Explicit%20and%20Implicit%20Prior%20Knowledge&body=Title%3A%20Advancing%20Generalizable%20Remote%20Physiological%20Measurement%20through%20the%0A%20%20Integration%20of%20Explicit%20and%20Implicit%20Prior%20Knowledge%0AAuthor%3A%20Yuting%20Zhang%20and%20Hao%20Lu%20and%20Xin%20Liu%20and%20Yingcong%20Chen%20and%20Kaishun%20Wu%0AAbstract%3A%20%20%20Remote%20photoplethysmography%20%28rPPG%29%20is%20a%20promising%20technology%20that%20captures%0Aphysiological%20signals%20from%20face%20videos%2C%20with%20potential%20applications%20in%20medical%0Ahealth%2C%20emotional%20computing%2C%20and%20biosecurity%20recognition.%20The%20demand%20for%20rPPG%0Atasks%20has%20expanded%20from%20demonstrating%20good%20performance%20on%20intra-dataset%20testing%0Ato%20cross-dataset%20testing%20%28i.e.%2C%20domain%20generalization%29.%20However%2C%20most%20existing%0Amethods%20have%20overlooked%20the%20prior%20knowledge%20of%20rPPG%2C%20resulting%20in%20poor%0Ageneralization%20ability.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%0Asimultaneously%20utilizes%20explicit%20and%20implicit%20prior%20knowledge%20in%20the%20rPPG%20task.%0ASpecifically%2C%20we%20systematically%20analyze%20the%20causes%20of%20noise%20sources%20%28e.g.%2C%0Adifferent%20camera%2C%20lighting%2C%20skin%20types%2C%20and%20movement%29%20across%20different%20domains%0Aand%20incorporate%20these%20prior%20knowledge%20into%20the%20network.%20Additionally%2C%20we%0Aleverage%20a%20two-branch%20network%20to%20disentangle%20the%20physiological%20feature%0Adistribution%20from%20noises%20through%20implicit%20label%20correlation.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20method%20not%20only%20outperforms%0Astate-of-the-art%20methods%20on%20RGB%20cross-dataset%20evaluation%20but%20also%20generalizes%0Awell%20from%20RGB%20datasets%20to%20NIR%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/keke-nice/Greip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06947v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Generalizable%20Remote%20Physiological%20Measurement%20through%20the%0A%20%20Integration%20of%20Explicit%20and%20Implicit%20Prior%20Knowledge&entry.906535625=Yuting%20Zhang%20and%20Hao%20Lu%20and%20Xin%20Liu%20and%20Yingcong%20Chen%20and%20Kaishun%20Wu&entry.1292438233=%20%20Remote%20photoplethysmography%20%28rPPG%29%20is%20a%20promising%20technology%20that%20captures%0Aphysiological%20signals%20from%20face%20videos%2C%20with%20potential%20applications%20in%20medical%0Ahealth%2C%20emotional%20computing%2C%20and%20biosecurity%20recognition.%20The%20demand%20for%20rPPG%0Atasks%20has%20expanded%20from%20demonstrating%20good%20performance%20on%20intra-dataset%20testing%0Ato%20cross-dataset%20testing%20%28i.e.%2C%20domain%20generalization%29.%20However%2C%20most%20existing%0Amethods%20have%20overlooked%20the%20prior%20knowledge%20of%20rPPG%2C%20resulting%20in%20poor%0Ageneralization%20ability.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%0Asimultaneously%20utilizes%20explicit%20and%20implicit%20prior%20knowledge%20in%20the%20rPPG%20task.%0ASpecifically%2C%20we%20systematically%20analyze%20the%20causes%20of%20noise%20sources%20%28e.g.%2C%0Adifferent%20camera%2C%20lighting%2C%20skin%20types%2C%20and%20movement%29%20across%20different%20domains%0Aand%20incorporate%20these%20prior%20knowledge%20into%20the%20network.%20Additionally%2C%20we%0Aleverage%20a%20two-branch%20network%20to%20disentangle%20the%20physiological%20feature%0Adistribution%20from%20noises%20through%20implicit%20label%20correlation.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20method%20not%20only%20outperforms%0Astate-of-the-art%20methods%20on%20RGB%20cross-dataset%20evaluation%20but%20also%20generalizes%0Awell%20from%20RGB%20datasets%20to%20NIR%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/keke-nice/Greip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06947v1&entry.124074799=Read"},
{"title": "In-context Exploration-Exploitation for Reinforcement Learning", "author": "Zhenwen Dai and Federico Tomasi and Sina Ghiassian", "abstract": "  In-context learning is a promising approach for online policy learning of\noffline reinforcement learning (RL) methods, which can be achieved at inference\ntime without gradient optimization. However, this method is hindered by\nsignificant computational costs resulting from the gathering of large training\ntrajectory sets and the need to train large Transformer models. We address this\nchallenge by introducing an In-context Exploration-Exploitation (ICEE)\nalgorithm, designed to optimize the efficiency of in-context policy learning.\nUnlike existing models, ICEE performs an exploration-exploitation trade-off at\ninference time within a Transformer model, without the need for explicit\nBayesian inference. Consequently, ICEE can solve Bayesian optimization problems\nas efficiently as Gaussian process biased methods do, but in significantly less\ntime. Through experiments in grid world environments, we demonstrate that ICEE\ncan learn to solve new RL tasks using only tens of episodes, marking a\nsubstantial improvement over the hundreds of episodes needed by the previous\nin-context learning method.\n", "link": "http://arxiv.org/abs/2403.06826v1", "date": "2024-03-11", "relevancy": 2.1356, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5635}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5312}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5248}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20In-context%20Exploration-Exploitation%20for%20Reinforcement%20Learning&body=Title%3A%20In-context%20Exploration-Exploitation%20for%20Reinforcement%20Learning%0AAuthor%3A%20Zhenwen%20Dai%20and%20Federico%20Tomasi%20and%20Sina%20Ghiassian%0AAbstract%3A%20%20%20In-context%20learning%20is%20a%20promising%20approach%20for%20online%20policy%20learning%20of%0Aoffline%20reinforcement%20learning%20%28RL%29%20methods%2C%20which%20can%20be%20achieved%20at%20inference%0Atime%20without%20gradient%20optimization.%20However%2C%20this%20method%20is%20hindered%20by%0Asignificant%20computational%20costs%20resulting%20from%20the%20gathering%20of%20large%20training%0Atrajectory%20sets%20and%20the%20need%20to%20train%20large%20Transformer%20models.%20We%20address%20this%0Achallenge%20by%20introducing%20an%20In-context%20Exploration-Exploitation%20%28ICEE%29%0Aalgorithm%2C%20designed%20to%20optimize%20the%20efficiency%20of%20in-context%20policy%20learning.%0AUnlike%20existing%20models%2C%20ICEE%20performs%20an%20exploration-exploitation%20trade-off%20at%0Ainference%20time%20within%20a%20Transformer%20model%2C%20without%20the%20need%20for%20explicit%0ABayesian%20inference.%20Consequently%2C%20ICEE%20can%20solve%20Bayesian%20optimization%20problems%0Aas%20efficiently%20as%20Gaussian%20process%20biased%20methods%20do%2C%20but%20in%20significantly%20less%0Atime.%20Through%20experiments%20in%20grid%20world%20environments%2C%20we%20demonstrate%20that%20ICEE%0Acan%20learn%20to%20solve%20new%20RL%20tasks%20using%20only%20tens%20of%20episodes%2C%20marking%20a%0Asubstantial%20improvement%20over%20the%20hundreds%20of%20episodes%20needed%20by%20the%20previous%0Ain-context%20learning%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06826v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-context%20Exploration-Exploitation%20for%20Reinforcement%20Learning&entry.906535625=Zhenwen%20Dai%20and%20Federico%20Tomasi%20and%20Sina%20Ghiassian&entry.1292438233=%20%20In-context%20learning%20is%20a%20promising%20approach%20for%20online%20policy%20learning%20of%0Aoffline%20reinforcement%20learning%20%28RL%29%20methods%2C%20which%20can%20be%20achieved%20at%20inference%0Atime%20without%20gradient%20optimization.%20However%2C%20this%20method%20is%20hindered%20by%0Asignificant%20computational%20costs%20resulting%20from%20the%20gathering%20of%20large%20training%0Atrajectory%20sets%20and%20the%20need%20to%20train%20large%20Transformer%20models.%20We%20address%20this%0Achallenge%20by%20introducing%20an%20In-context%20Exploration-Exploitation%20%28ICEE%29%0Aalgorithm%2C%20designed%20to%20optimize%20the%20efficiency%20of%20in-context%20policy%20learning.%0AUnlike%20existing%20models%2C%20ICEE%20performs%20an%20exploration-exploitation%20trade-off%20at%0Ainference%20time%20within%20a%20Transformer%20model%2C%20without%20the%20need%20for%20explicit%0ABayesian%20inference.%20Consequently%2C%20ICEE%20can%20solve%20Bayesian%20optimization%20problems%0Aas%20efficiently%20as%20Gaussian%20process%20biased%20methods%20do%2C%20but%20in%20significantly%20less%0Atime.%20Through%20experiments%20in%20grid%20world%20environments%2C%20we%20demonstrate%20that%20ICEE%0Acan%20learn%20to%20solve%20new%20RL%20tasks%20using%20only%20tens%20of%20episodes%2C%20marking%20a%0Asubstantial%20improvement%20over%20the%20hundreds%20of%20episodes%20needed%20by%20the%20previous%0Ain-context%20learning%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06826v1&entry.124074799=Read"},
{"title": "EarthLoc: Astronaut Photography Localization by Indexing Earth from\n  Space", "author": "Gabriele Berton and Alex Stoken and Barbara Caputo and Carlo Masone", "abstract": "  Astronaut photography, spanning six decades of human spaceflight, presents a\nunique Earth observations dataset with immense value for both scientific\nresearch and disaster response. Despite its significance, accurately localizing\nthe geographical extent of these images, crucial for effective utilization,\nposes substantial challenges. Current manual localization efforts are\ntime-consuming, motivating the need for automated solutions. We propose a novel\napproach - leveraging image retrieval - to address this challenge efficiently.\nWe introduce innovative training techniques, including Year-Wise Data\nAugmentation and a Neutral-Aware Multi-Similarity Loss, which contribute to the\ndevelopment of a high-performance model, EarthLoc. We develop six evaluation\ndatasets and perform a comprehensive benchmark comparing EarthLoc to existing\nmethods, showcasing its superior efficiency and accuracy. Our approach marks a\nsignificant advancement in automating the localization of astronaut\nphotography, which will help bridge a critical gap in Earth observations data.\nCode and datasets are available at https://github.com/gmberton/EarthLoc\n", "link": "http://arxiv.org/abs/2403.06758v1", "date": "2024-03-11", "relevancy": 2.1311, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5616}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.523}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4853}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20EarthLoc%3A%20Astronaut%20Photography%20Localization%20by%20Indexing%20Earth%20from%0A%20%20Space&body=Title%3A%20EarthLoc%3A%20Astronaut%20Photography%20Localization%20by%20Indexing%20Earth%20from%0A%20%20Space%0AAuthor%3A%20Gabriele%20Berton%20and%20Alex%20Stoken%20and%20Barbara%20Caputo%20and%20Carlo%20Masone%0AAbstract%3A%20%20%20Astronaut%20photography%2C%20spanning%20six%20decades%20of%20human%20spaceflight%2C%20presents%20a%0Aunique%20Earth%20observations%20dataset%20with%20immense%20value%20for%20both%20scientific%0Aresearch%20and%20disaster%20response.%20Despite%20its%20significance%2C%20accurately%20localizing%0Athe%20geographical%20extent%20of%20these%20images%2C%20crucial%20for%20effective%20utilization%2C%0Aposes%20substantial%20challenges.%20Current%20manual%20localization%20efforts%20are%0Atime-consuming%2C%20motivating%20the%20need%20for%20automated%20solutions.%20We%20propose%20a%20novel%0Aapproach%20-%20leveraging%20image%20retrieval%20-%20to%20address%20this%20challenge%20efficiently.%0AWe%20introduce%20innovative%20training%20techniques%2C%20including%20Year-Wise%20Data%0AAugmentation%20and%20a%20Neutral-Aware%20Multi-Similarity%20Loss%2C%20which%20contribute%20to%20the%0Adevelopment%20of%20a%20high-performance%20model%2C%20EarthLoc.%20We%20develop%20six%20evaluation%0Adatasets%20and%20perform%20a%20comprehensive%20benchmark%20comparing%20EarthLoc%20to%20existing%0Amethods%2C%20showcasing%20its%20superior%20efficiency%20and%20accuracy.%20Our%20approach%20marks%20a%0Asignificant%20advancement%20in%20automating%20the%20localization%20of%20astronaut%0Aphotography%2C%20which%20will%20help%20bridge%20a%20critical%20gap%20in%20Earth%20observations%20data.%0ACode%20and%20datasets%20are%20available%20at%20https%3A//github.com/gmberton/EarthLoc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06758v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthLoc%3A%20Astronaut%20Photography%20Localization%20by%20Indexing%20Earth%20from%0A%20%20Space&entry.906535625=Gabriele%20Berton%20and%20Alex%20Stoken%20and%20Barbara%20Caputo%20and%20Carlo%20Masone&entry.1292438233=%20%20Astronaut%20photography%2C%20spanning%20six%20decades%20of%20human%20spaceflight%2C%20presents%20a%0Aunique%20Earth%20observations%20dataset%20with%20immense%20value%20for%20both%20scientific%0Aresearch%20and%20disaster%20response.%20Despite%20its%20significance%2C%20accurately%20localizing%0Athe%20geographical%20extent%20of%20these%20images%2C%20crucial%20for%20effective%20utilization%2C%0Aposes%20substantial%20challenges.%20Current%20manual%20localization%20efforts%20are%0Atime-consuming%2C%20motivating%20the%20need%20for%20automated%20solutions.%20We%20propose%20a%20novel%0Aapproach%20-%20leveraging%20image%20retrieval%20-%20to%20address%20this%20challenge%20efficiently.%0AWe%20introduce%20innovative%20training%20techniques%2C%20including%20Year-Wise%20Data%0AAugmentation%20and%20a%20Neutral-Aware%20Multi-Similarity%20Loss%2C%20which%20contribute%20to%20the%0Adevelopment%20of%20a%20high-performance%20model%2C%20EarthLoc.%20We%20develop%20six%20evaluation%0Adatasets%20and%20perform%20a%20comprehensive%20benchmark%20comparing%20EarthLoc%20to%20existing%0Amethods%2C%20showcasing%20its%20superior%20efficiency%20and%20accuracy.%20Our%20approach%20marks%20a%0Asignificant%20advancement%20in%20automating%20the%20localization%20of%20astronaut%0Aphotography%2C%20which%20will%20help%20bridge%20a%20critical%20gap%20in%20Earth%20observations%20data.%0ACode%20and%20datasets%20are%20available%20at%20https%3A//github.com/gmberton/EarthLoc%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06758v1&entry.124074799=Read"},
{"title": "Monotone Individual Fairness", "author": "Yahav Bechavod", "abstract": "  We revisit the problem of online learning with individual fairness, where an\nonline learner strives to maximize predictive accuracy while ensuring that\nsimilar individuals are treated similarly. We first extend the frameworks of\nGillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human\nauditors regarding fairness violations, as we consider auditing schemes that\nare capable of aggregating feedback from any number of auditors, using a rich\nclass we term monotone aggregation functions. We then prove a characterization\nfor such auditing schemes, practically reducing the analysis of auditing for\nindividual fairness by multiple auditors to that of auditing by\n(instance-specific) single auditors. Using our generalized framework, we\npresent an oracle-efficient algorithm achieving an upper bound frontier of\n$(\\mathcal{O}(T^{1/2+2b}),\\mathcal{O}(T^{3/4-b}))$ respectively for regret,\nnumber of fairness violations, for $0\\leq b \\leq 1/4$. We then study an online\nclassification setting where label feedback is available for\npositively-predicted individuals only, and present an oracle-efficient\nalgorithm achieving an upper bound frontier of\n$(\\mathcal{O}(T^{2/3+2b}),\\mathcal{O}(T^{5/6-b}))$ for regret, number of\nfairness violations, for $0\\leq b \\leq 1/6$. In both settings, our algorithms\nimprove on the best known bounds for oracle-efficient algorithms. Furthermore,\nour algorithms offer significant improvements in computational efficiency,\ngreatly reducing the number of required calls to an (offline) optimization\noracle per round, to $\\tilde{\\mathcal{O}}(\\alpha^{-2})$ in the full information\nsetting, and $\\tilde{\\mathcal{O}}(\\alpha^{-2} + k^2T^{1/3})$ in the partial\ninformation setting, where $\\alpha$ is the sensitivity for reporting fairness\nviolations, and $k$ is the number of individuals in a round.\n", "link": "http://arxiv.org/abs/2403.06812v1", "date": "2024-03-11", "relevancy": 2.1268, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4386}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4188}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4186}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Monotone%20Individual%20Fairness&body=Title%3A%20Monotone%20Individual%20Fairness%0AAuthor%3A%20Yahav%20Bechavod%0AAbstract%3A%20%20%20We%20revisit%20the%20problem%20of%20online%20learning%20with%20individual%20fairness%2C%20where%20an%0Aonline%20learner%20strives%20to%20maximize%20predictive%20accuracy%20while%20ensuring%20that%0Asimilar%20individuals%20are%20treated%20similarly.%20We%20first%20extend%20the%20frameworks%20of%0AGillen%20et%20al.%20%282018%29%3B%20Bechavod%20et%20al.%20%282020%29%2C%20which%20rely%20on%20feedback%20from%20human%0Aauditors%20regarding%20fairness%20violations%2C%20as%20we%20consider%20auditing%20schemes%20that%0Aare%20capable%20of%20aggregating%20feedback%20from%20any%20number%20of%20auditors%2C%20using%20a%20rich%0Aclass%20we%20term%20monotone%20aggregation%20functions.%20We%20then%20prove%20a%20characterization%0Afor%20such%20auditing%20schemes%2C%20practically%20reducing%20the%20analysis%20of%20auditing%20for%0Aindividual%20fairness%20by%20multiple%20auditors%20to%20that%20of%20auditing%20by%0A%28instance-specific%29%20single%20auditors.%20Using%20our%20generalized%20framework%2C%20we%0Apresent%20an%20oracle-efficient%20algorithm%20achieving%20an%20upper%20bound%20frontier%20of%0A%24%28%5Cmathcal%7BO%7D%28T%5E%7B1/2%2B2b%7D%29%2C%5Cmathcal%7BO%7D%28T%5E%7B3/4-b%7D%29%29%24%20respectively%20for%20regret%2C%0Anumber%20of%20fairness%20violations%2C%20for%20%240%5Cleq%20b%20%5Cleq%201/4%24.%20We%20then%20study%20an%20online%0Aclassification%20setting%20where%20label%20feedback%20is%20available%20for%0Apositively-predicted%20individuals%20only%2C%20and%20present%20an%20oracle-efficient%0Aalgorithm%20achieving%20an%20upper%20bound%20frontier%20of%0A%24%28%5Cmathcal%7BO%7D%28T%5E%7B2/3%2B2b%7D%29%2C%5Cmathcal%7BO%7D%28T%5E%7B5/6-b%7D%29%29%24%20for%20regret%2C%20number%20of%0Afairness%20violations%2C%20for%20%240%5Cleq%20b%20%5Cleq%201/6%24.%20In%20both%20settings%2C%20our%20algorithms%0Aimprove%20on%20the%20best%20known%20bounds%20for%20oracle-efficient%20algorithms.%20Furthermore%2C%0Aour%20algorithms%20offer%20significant%20improvements%20in%20computational%20efficiency%2C%0Agreatly%20reducing%20the%20number%20of%20required%20calls%20to%20an%20%28offline%29%20optimization%0Aoracle%20per%20round%2C%20to%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Calpha%5E%7B-2%7D%29%24%20in%20the%20full%20information%0Asetting%2C%20and%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Calpha%5E%7B-2%7D%20%2B%20k%5E2T%5E%7B1/3%7D%29%24%20in%20the%20partial%0Ainformation%20setting%2C%20where%20%24%5Calpha%24%20is%20the%20sensitivity%20for%20reporting%20fairness%0Aviolations%2C%20and%20%24k%24%20is%20the%20number%20of%20individuals%20in%20a%20round.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06812v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monotone%20Individual%20Fairness&entry.906535625=Yahav%20Bechavod&entry.1292438233=%20%20We%20revisit%20the%20problem%20of%20online%20learning%20with%20individual%20fairness%2C%20where%20an%0Aonline%20learner%20strives%20to%20maximize%20predictive%20accuracy%20while%20ensuring%20that%0Asimilar%20individuals%20are%20treated%20similarly.%20We%20first%20extend%20the%20frameworks%20of%0AGillen%20et%20al.%20%282018%29%3B%20Bechavod%20et%20al.%20%282020%29%2C%20which%20rely%20on%20feedback%20from%20human%0Aauditors%20regarding%20fairness%20violations%2C%20as%20we%20consider%20auditing%20schemes%20that%0Aare%20capable%20of%20aggregating%20feedback%20from%20any%20number%20of%20auditors%2C%20using%20a%20rich%0Aclass%20we%20term%20monotone%20aggregation%20functions.%20We%20then%20prove%20a%20characterization%0Afor%20such%20auditing%20schemes%2C%20practically%20reducing%20the%20analysis%20of%20auditing%20for%0Aindividual%20fairness%20by%20multiple%20auditors%20to%20that%20of%20auditing%20by%0A%28instance-specific%29%20single%20auditors.%20Using%20our%20generalized%20framework%2C%20we%0Apresent%20an%20oracle-efficient%20algorithm%20achieving%20an%20upper%20bound%20frontier%20of%0A%24%28%5Cmathcal%7BO%7D%28T%5E%7B1/2%2B2b%7D%29%2C%5Cmathcal%7BO%7D%28T%5E%7B3/4-b%7D%29%29%24%20respectively%20for%20regret%2C%0Anumber%20of%20fairness%20violations%2C%20for%20%240%5Cleq%20b%20%5Cleq%201/4%24.%20We%20then%20study%20an%20online%0Aclassification%20setting%20where%20label%20feedback%20is%20available%20for%0Apositively-predicted%20individuals%20only%2C%20and%20present%20an%20oracle-efficient%0Aalgorithm%20achieving%20an%20upper%20bound%20frontier%20of%0A%24%28%5Cmathcal%7BO%7D%28T%5E%7B2/3%2B2b%7D%29%2C%5Cmathcal%7BO%7D%28T%5E%7B5/6-b%7D%29%29%24%20for%20regret%2C%20number%20of%0Afairness%20violations%2C%20for%20%240%5Cleq%20b%20%5Cleq%201/6%24.%20In%20both%20settings%2C%20our%20algorithms%0Aimprove%20on%20the%20best%20known%20bounds%20for%20oracle-efficient%20algorithms.%20Furthermore%2C%0Aour%20algorithms%20offer%20significant%20improvements%20in%20computational%20efficiency%2C%0Agreatly%20reducing%20the%20number%20of%20required%20calls%20to%20an%20%28offline%29%20optimization%0Aoracle%20per%20round%2C%20to%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Calpha%5E%7B-2%7D%29%24%20in%20the%20full%20information%0Asetting%2C%20and%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Calpha%5E%7B-2%7D%20%2B%20k%5E2T%5E%7B1/3%7D%29%24%20in%20the%20partial%0Ainformation%20setting%2C%20where%20%24%5Calpha%24%20is%20the%20sensitivity%20for%20reporting%20fairness%0Aviolations%2C%20and%20%24k%24%20is%20the%20number%20of%20individuals%20in%20a%20round.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06812v1&entry.124074799=Read"},
{"title": "UniPT: Universal Parallel Tuning for Transfer Learning with Efficient\n  Parameter and Memory", "author": "Haiwen Diao and Bo Wan and Ying Zhang and Xu Jia and Huchuan Lu and Long Chen", "abstract": "  Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small\nportion of parameters, is an effective strategy for adapting pre-trained models\nto downstream domains. To further reduce the memory demand, recent PETL works\nfocus on the more valuable memory-efficient characteristic. In this paper, we\nargue that the scalability, adaptability, and generalizability of\nstate-of-the-art methods are hindered by structural dependency and pertinency\non specific pre-trained backbones. To this end, we propose a new\nmemory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate\nthese weaknesses. Specifically, we facilitate the transfer process via a\nlightweight and learnable parallel network, which consists of: 1) A parallel\ninteraction module that decouples the sequential connections and processes the\nintermediate activations detachedly from the pre-trained network. 2) A\nconfidence aggregation module that learns optimal strategies adaptively for\nintegrating cross-layer features. We evaluate UniPT with different backbones\n(e.g., T5, VSE$\\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various\nvision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have\nvalidated that UniPT can not only dramatically reduce memory consumption and\noutperform the best competitor, but also achieve competitive performance over\nother plain PETL methods with lower training memory overhead. Our code is\npublicly available at: https://github.com/Paranioar/UniPT.\n", "link": "http://arxiv.org/abs/2308.14316v2", "date": "2024-03-11", "relevancy": 2.1189, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5216}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5155}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20UniPT%3A%20Universal%20Parallel%20Tuning%20for%20Transfer%20Learning%20with%20Efficient%0A%20%20Parameter%20and%20Memory&body=Title%3A%20UniPT%3A%20Universal%20Parallel%20Tuning%20for%20Transfer%20Learning%20with%20Efficient%0A%20%20Parameter%20and%20Memory%0AAuthor%3A%20Haiwen%20Diao%20and%20Bo%20Wan%20and%20Ying%20Zhang%20and%20Xu%20Jia%20and%20Huchuan%20Lu%20and%20Long%20Chen%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%2C%20i.e.%2C%20fine-tuning%20a%20small%0Aportion%20of%20parameters%2C%20is%20an%20effective%20strategy%20for%20adapting%20pre-trained%20models%0Ato%20downstream%20domains.%20To%20further%20reduce%20the%20memory%20demand%2C%20recent%20PETL%20works%0Afocus%20on%20the%20more%20valuable%20memory-efficient%20characteristic.%20In%20this%20paper%2C%20we%0Aargue%20that%20the%20scalability%2C%20adaptability%2C%20and%20generalizability%20of%0Astate-of-the-art%20methods%20are%20hindered%20by%20structural%20dependency%20and%20pertinency%0Aon%20specific%20pre-trained%20backbones.%20To%20this%20end%2C%20we%20propose%20a%20new%0Amemory-efficient%20PETL%20strategy%2C%20Universal%20Parallel%20Tuning%20%28UniPT%29%2C%20to%20mitigate%0Athese%20weaknesses.%20Specifically%2C%20we%20facilitate%20the%20transfer%20process%20via%20a%0Alightweight%20and%20learnable%20parallel%20network%2C%20which%20consists%20of%3A%201%29%20A%20parallel%0Ainteraction%20module%20that%20decouples%20the%20sequential%20connections%20and%20processes%20the%0Aintermediate%20activations%20detachedly%20from%20the%20pre-trained%20network.%202%29%20A%0Aconfidence%20aggregation%20module%20that%20learns%20optimal%20strategies%20adaptively%20for%0Aintegrating%20cross-layer%20features.%20We%20evaluate%20UniPT%20with%20different%20backbones%0A%28e.g.%2C%20T5%2C%20VSE%24%5Cinfty%24%2C%20CLIP4Clip%2C%20Clip-ViL%2C%20and%20MDETR%29%20on%20various%0Avision-and-language%20and%20pure%20NLP%20tasks.%20Extensive%20ablations%20on%2018%20datasets%20have%0Avalidated%20that%20UniPT%20can%20not%20only%20dramatically%20reduce%20memory%20consumption%20and%0Aoutperform%20the%20best%20competitor%2C%20but%20also%20achieve%20competitive%20performance%20over%0Aother%20plain%20PETL%20methods%20with%20lower%20training%20memory%20overhead.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/Paranioar/UniPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14316v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPT%3A%20Universal%20Parallel%20Tuning%20for%20Transfer%20Learning%20with%20Efficient%0A%20%20Parameter%20and%20Memory&entry.906535625=Haiwen%20Diao%20and%20Bo%20Wan%20and%20Ying%20Zhang%20and%20Xu%20Jia%20and%20Huchuan%20Lu%20and%20Long%20Chen&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%2C%20i.e.%2C%20fine-tuning%20a%20small%0Aportion%20of%20parameters%2C%20is%20an%20effective%20strategy%20for%20adapting%20pre-trained%20models%0Ato%20downstream%20domains.%20To%20further%20reduce%20the%20memory%20demand%2C%20recent%20PETL%20works%0Afocus%20on%20the%20more%20valuable%20memory-efficient%20characteristic.%20In%20this%20paper%2C%20we%0Aargue%20that%20the%20scalability%2C%20adaptability%2C%20and%20generalizability%20of%0Astate-of-the-art%20methods%20are%20hindered%20by%20structural%20dependency%20and%20pertinency%0Aon%20specific%20pre-trained%20backbones.%20To%20this%20end%2C%20we%20propose%20a%20new%0Amemory-efficient%20PETL%20strategy%2C%20Universal%20Parallel%20Tuning%20%28UniPT%29%2C%20to%20mitigate%0Athese%20weaknesses.%20Specifically%2C%20we%20facilitate%20the%20transfer%20process%20via%20a%0Alightweight%20and%20learnable%20parallel%20network%2C%20which%20consists%20of%3A%201%29%20A%20parallel%0Ainteraction%20module%20that%20decouples%20the%20sequential%20connections%20and%20processes%20the%0Aintermediate%20activations%20detachedly%20from%20the%20pre-trained%20network.%202%29%20A%0Aconfidence%20aggregation%20module%20that%20learns%20optimal%20strategies%20adaptively%20for%0Aintegrating%20cross-layer%20features.%20We%20evaluate%20UniPT%20with%20different%20backbones%0A%28e.g.%2C%20T5%2C%20VSE%24%5Cinfty%24%2C%20CLIP4Clip%2C%20Clip-ViL%2C%20and%20MDETR%29%20on%20various%0Avision-and-language%20and%20pure%20NLP%20tasks.%20Extensive%20ablations%20on%2018%20datasets%20have%0Avalidated%20that%20UniPT%20can%20not%20only%20dramatically%20reduce%20memory%20consumption%20and%0Aoutperform%20the%20best%20competitor%2C%20but%20also%20achieve%20competitive%20performance%20over%0Aother%20plain%20PETL%20methods%20with%20lower%20training%20memory%20overhead.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/Paranioar/UniPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14316v2&entry.124074799=Read"},
{"title": "Which Models have Perceptually-Aligned Gradients? An Explanation via\n  Off-Manifold Robustness", "author": "Suraj Srinivas and Sebastian Bordt and Hima Lakkaraju", "abstract": "  One of the remarkable properties of robust computer vision models is that\ntheir input-gradients are often aligned with human perception, referred to in\nthe literature as perceptually-aligned gradients (PAGs). Despite only being\ntrained for classification, PAGs cause robust models to have rudimentary\ngenerative capabilities, including image generation, denoising, and\nin-painting. However, the underlying mechanisms behind these phenomena remain\nunknown. In this work, we provide a first explanation of PAGs via\n\\emph{off-manifold robustness}, which states that models must be more robust\noff- the data manifold than they are on-manifold. We first demonstrate\ntheoretically that off-manifold robustness leads input gradients to lie\napproximately on the data manifold, explaining their perceptual alignment. We\nthen show that Bayes optimal models satisfy off-manifold robustness, and\nconfirm the same empirically for robust models trained via gradient norm\nregularization, randomized smoothing, and adversarial training with projected\ngradient descent. Quantifying the perceptual alignment of model gradients via\ntheir similarity with the gradients of generative models, we show that\noff-manifold robustness correlates well with perceptual alignment. Finally,\nbased on the levels of on- and off-manifold robustness, we identify three\ndifferent regimes of robustness that affect both perceptual alignment and model\naccuracy: weak robustness, bayes-aligned robustness, and excessive robustness.\nCode is available at \\url{https://github.com/tml-tuebingen/pags}.\n", "link": "http://arxiv.org/abs/2305.19101v2", "date": "2024-03-11", "relevancy": 2.1178, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5157}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Which%20Models%20have%20Perceptually-Aligned%20Gradients%3F%20An%20Explanation%20via%0A%20%20Off-Manifold%20Robustness&body=Title%3A%20Which%20Models%20have%20Perceptually-Aligned%20Gradients%3F%20An%20Explanation%20via%0A%20%20Off-Manifold%20Robustness%0AAuthor%3A%20Suraj%20Srinivas%20and%20Sebastian%20Bordt%20and%20Hima%20Lakkaraju%0AAbstract%3A%20%20%20One%20of%20the%20remarkable%20properties%20of%20robust%20computer%20vision%20models%20is%20that%0Atheir%20input-gradients%20are%20often%20aligned%20with%20human%20perception%2C%20referred%20to%20in%0Athe%20literature%20as%20perceptually-aligned%20gradients%20%28PAGs%29.%20Despite%20only%20being%0Atrained%20for%20classification%2C%20PAGs%20cause%20robust%20models%20to%20have%20rudimentary%0Agenerative%20capabilities%2C%20including%20image%20generation%2C%20denoising%2C%20and%0Ain-painting.%20However%2C%20the%20underlying%20mechanisms%20behind%20these%20phenomena%20remain%0Aunknown.%20In%20this%20work%2C%20we%20provide%20a%20first%20explanation%20of%20PAGs%20via%0A%5Cemph%7Boff-manifold%20robustness%7D%2C%20which%20states%20that%20models%20must%20be%20more%20robust%0Aoff-%20the%20data%20manifold%20than%20they%20are%20on-manifold.%20We%20first%20demonstrate%0Atheoretically%20that%20off-manifold%20robustness%20leads%20input%20gradients%20to%20lie%0Aapproximately%20on%20the%20data%20manifold%2C%20explaining%20their%20perceptual%20alignment.%20We%0Athen%20show%20that%20Bayes%20optimal%20models%20satisfy%20off-manifold%20robustness%2C%20and%0Aconfirm%20the%20same%20empirically%20for%20robust%20models%20trained%20via%20gradient%20norm%0Aregularization%2C%20randomized%20smoothing%2C%20and%20adversarial%20training%20with%20projected%0Agradient%20descent.%20Quantifying%20the%20perceptual%20alignment%20of%20model%20gradients%20via%0Atheir%20similarity%20with%20the%20gradients%20of%20generative%20models%2C%20we%20show%20that%0Aoff-manifold%20robustness%20correlates%20well%20with%20perceptual%20alignment.%20Finally%2C%0Abased%20on%20the%20levels%20of%20on-%20and%20off-manifold%20robustness%2C%20we%20identify%20three%0Adifferent%20regimes%20of%20robustness%20that%20affect%20both%20perceptual%20alignment%20and%20model%0Aaccuracy%3A%20weak%20robustness%2C%20bayes-aligned%20robustness%2C%20and%20excessive%20robustness.%0ACode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/tml-tuebingen/pags%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.19101v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Models%20have%20Perceptually-Aligned%20Gradients%3F%20An%20Explanation%20via%0A%20%20Off-Manifold%20Robustness&entry.906535625=Suraj%20Srinivas%20and%20Sebastian%20Bordt%20and%20Hima%20Lakkaraju&entry.1292438233=%20%20One%20of%20the%20remarkable%20properties%20of%20robust%20computer%20vision%20models%20is%20that%0Atheir%20input-gradients%20are%20often%20aligned%20with%20human%20perception%2C%20referred%20to%20in%0Athe%20literature%20as%20perceptually-aligned%20gradients%20%28PAGs%29.%20Despite%20only%20being%0Atrained%20for%20classification%2C%20PAGs%20cause%20robust%20models%20to%20have%20rudimentary%0Agenerative%20capabilities%2C%20including%20image%20generation%2C%20denoising%2C%20and%0Ain-painting.%20However%2C%20the%20underlying%20mechanisms%20behind%20these%20phenomena%20remain%0Aunknown.%20In%20this%20work%2C%20we%20provide%20a%20first%20explanation%20of%20PAGs%20via%0A%5Cemph%7Boff-manifold%20robustness%7D%2C%20which%20states%20that%20models%20must%20be%20more%20robust%0Aoff-%20the%20data%20manifold%20than%20they%20are%20on-manifold.%20We%20first%20demonstrate%0Atheoretically%20that%20off-manifold%20robustness%20leads%20input%20gradients%20to%20lie%0Aapproximately%20on%20the%20data%20manifold%2C%20explaining%20their%20perceptual%20alignment.%20We%0Athen%20show%20that%20Bayes%20optimal%20models%20satisfy%20off-manifold%20robustness%2C%20and%0Aconfirm%20the%20same%20empirically%20for%20robust%20models%20trained%20via%20gradient%20norm%0Aregularization%2C%20randomized%20smoothing%2C%20and%20adversarial%20training%20with%20projected%0Agradient%20descent.%20Quantifying%20the%20perceptual%20alignment%20of%20model%20gradients%20via%0Atheir%20similarity%20with%20the%20gradients%20of%20generative%20models%2C%20we%20show%20that%0Aoff-manifold%20robustness%20correlates%20well%20with%20perceptual%20alignment.%20Finally%2C%0Abased%20on%20the%20levels%20of%20on-%20and%20off-manifold%20robustness%2C%20we%20identify%20three%0Adifferent%20regimes%20of%20robustness%20that%20affect%20both%20perceptual%20alignment%20and%20model%0Aaccuracy%3A%20weak%20robustness%2C%20bayes-aligned%20robustness%2C%20and%20excessive%20robustness.%0ACode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/tml-tuebingen/pags%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19101v2&entry.124074799=Read"},
{"title": "GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes", "author": "Ibrahim Ethem Hamamci and Sezgin Er and Anjany Sekuboyina and Enis Simsar and Alperen Tezcan and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Furkan Almas and Irem Dogan and Muhammed Furkan Dasdelen and Chinmay Prabhakar and Hadrien Reynaud and Sarthak Pati and Christian Bluethgen and Mehmet Kemal Ozdemir and Bjoern Menze", "abstract": "  GenerateCT, the first approach to generating 3D medical imaging conditioned\non free-form medical text prompts, incorporates a text encoder and three key\ncomponents: a novel causal vision transformer for encoding 3D CT volumes, a\ntext-image transformer for aligning CT and text tokens, and a text-conditional\nsuper-resolution diffusion model. Given the absence of directly comparable\nmethods in 3D medical imaging, we established baselines with cutting-edge\nmethods to demonstrate our method's effectiveness. GenerateCT significantly\noutperforms these methods across all key metrics. Importantly, we explored\nGenerateCT's clinical applications by evaluating its utility in a\nmulti-abnormality classification task. First, we established a baseline by\ntraining a multi-abnormality classifier on our real dataset. To further assess\nthe model's generalization to external datasets and its performance with unseen\nprompts in a zero-shot scenario, we employed an external dataset to train the\nclassifier, setting an additional benchmark. We conducted two experiments in\nwhich we doubled the training datasets by synthesizing an equal number of\nvolumes for each set using GenerateCT. The first experiment demonstrated an 11%\nimprovement in the AP score when training the classifier jointly on real and\ngenerated volumes. The second experiment showed a 7% improvement when training\non both real and generated volumes based on unseen prompts. Moreover,\nGenerateCT enables the scaling of synthetic training datasets to arbitrary\nsizes. As an example, we generated 100,000 3D CT volumes, fivefold the number\nin our real dataset, and trained the classifier exclusively on these synthetic\nvolumes. Impressively, this classifier surpassed the performance of the one\ntrained on all available real data by a margin of 8%. Lastly, domain experts\nevaluated the generated volumes, confirming a high degree of alignment with the\ntext prompt.\n", "link": "http://arxiv.org/abs/2305.16037v4", "date": "2024-03-11", "relevancy": 2.1158, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5696}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5198}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20GenerateCT%3A%20Text-Conditional%20Generation%20of%203D%20Chest%20CT%20Volumes&body=Title%3A%20GenerateCT%3A%20Text-Conditional%20Generation%20of%203D%20Chest%20CT%20Volumes%0AAuthor%3A%20Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Anjany%20Sekuboyina%20and%20Enis%20Simsar%20and%20Alperen%20Tezcan%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Furkan%20Almas%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Chinmay%20Prabhakar%20and%20Hadrien%20Reynaud%20and%20Sarthak%20Pati%20and%20Christian%20Bluethgen%20and%20Mehmet%20Kemal%20Ozdemir%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20GenerateCT%2C%20the%20first%20approach%20to%20generating%203D%20medical%20imaging%20conditioned%0Aon%20free-form%20medical%20text%20prompts%2C%20incorporates%20a%20text%20encoder%20and%20three%20key%0Acomponents%3A%20a%20novel%20causal%20vision%20transformer%20for%20encoding%203D%20CT%20volumes%2C%20a%0Atext-image%20transformer%20for%20aligning%20CT%20and%20text%20tokens%2C%20and%20a%20text-conditional%0Asuper-resolution%20diffusion%20model.%20Given%20the%20absence%20of%20directly%20comparable%0Amethods%20in%203D%20medical%20imaging%2C%20we%20established%20baselines%20with%20cutting-edge%0Amethods%20to%20demonstrate%20our%20method%27s%20effectiveness.%20GenerateCT%20significantly%0Aoutperforms%20these%20methods%20across%20all%20key%20metrics.%20Importantly%2C%20we%20explored%0AGenerateCT%27s%20clinical%20applications%20by%20evaluating%20its%20utility%20in%20a%0Amulti-abnormality%20classification%20task.%20First%2C%20we%20established%20a%20baseline%20by%0Atraining%20a%20multi-abnormality%20classifier%20on%20our%20real%20dataset.%20To%20further%20assess%0Athe%20model%27s%20generalization%20to%20external%20datasets%20and%20its%20performance%20with%20unseen%0Aprompts%20in%20a%20zero-shot%20scenario%2C%20we%20employed%20an%20external%20dataset%20to%20train%20the%0Aclassifier%2C%20setting%20an%20additional%20benchmark.%20We%20conducted%20two%20experiments%20in%0Awhich%20we%20doubled%20the%20training%20datasets%20by%20synthesizing%20an%20equal%20number%20of%0Avolumes%20for%20each%20set%20using%20GenerateCT.%20The%20first%20experiment%20demonstrated%20an%2011%25%0Aimprovement%20in%20the%20AP%20score%20when%20training%20the%20classifier%20jointly%20on%20real%20and%0Agenerated%20volumes.%20The%20second%20experiment%20showed%20a%207%25%20improvement%20when%20training%0Aon%20both%20real%20and%20generated%20volumes%20based%20on%20unseen%20prompts.%20Moreover%2C%0AGenerateCT%20enables%20the%20scaling%20of%20synthetic%20training%20datasets%20to%20arbitrary%0Asizes.%20As%20an%20example%2C%20we%20generated%20100%2C000%203D%20CT%20volumes%2C%20fivefold%20the%20number%0Ain%20our%20real%20dataset%2C%20and%20trained%20the%20classifier%20exclusively%20on%20these%20synthetic%0Avolumes.%20Impressively%2C%20this%20classifier%20surpassed%20the%20performance%20of%20the%20one%0Atrained%20on%20all%20available%20real%20data%20by%20a%20margin%20of%208%25.%20Lastly%2C%20domain%20experts%0Aevaluated%20the%20generated%20volumes%2C%20confirming%20a%20high%20degree%20of%20alignment%20with%20the%0Atext%20prompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16037v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenerateCT%3A%20Text-Conditional%20Generation%20of%203D%20Chest%20CT%20Volumes&entry.906535625=Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Anjany%20Sekuboyina%20and%20Enis%20Simsar%20and%20Alperen%20Tezcan%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Furkan%20Almas%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Chinmay%20Prabhakar%20and%20Hadrien%20Reynaud%20and%20Sarthak%20Pati%20and%20Christian%20Bluethgen%20and%20Mehmet%20Kemal%20Ozdemir%20and%20Bjoern%20Menze&entry.1292438233=%20%20GenerateCT%2C%20the%20first%20approach%20to%20generating%203D%20medical%20imaging%20conditioned%0Aon%20free-form%20medical%20text%20prompts%2C%20incorporates%20a%20text%20encoder%20and%20three%20key%0Acomponents%3A%20a%20novel%20causal%20vision%20transformer%20for%20encoding%203D%20CT%20volumes%2C%20a%0Atext-image%20transformer%20for%20aligning%20CT%20and%20text%20tokens%2C%20and%20a%20text-conditional%0Asuper-resolution%20diffusion%20model.%20Given%20the%20absence%20of%20directly%20comparable%0Amethods%20in%203D%20medical%20imaging%2C%20we%20established%20baselines%20with%20cutting-edge%0Amethods%20to%20demonstrate%20our%20method%27s%20effectiveness.%20GenerateCT%20significantly%0Aoutperforms%20these%20methods%20across%20all%20key%20metrics.%20Importantly%2C%20we%20explored%0AGenerateCT%27s%20clinical%20applications%20by%20evaluating%20its%20utility%20in%20a%0Amulti-abnormality%20classification%20task.%20First%2C%20we%20established%20a%20baseline%20by%0Atraining%20a%20multi-abnormality%20classifier%20on%20our%20real%20dataset.%20To%20further%20assess%0Athe%20model%27s%20generalization%20to%20external%20datasets%20and%20its%20performance%20with%20unseen%0Aprompts%20in%20a%20zero-shot%20scenario%2C%20we%20employed%20an%20external%20dataset%20to%20train%20the%0Aclassifier%2C%20setting%20an%20additional%20benchmark.%20We%20conducted%20two%20experiments%20in%0Awhich%20we%20doubled%20the%20training%20datasets%20by%20synthesizing%20an%20equal%20number%20of%0Avolumes%20for%20each%20set%20using%20GenerateCT.%20The%20first%20experiment%20demonstrated%20an%2011%25%0Aimprovement%20in%20the%20AP%20score%20when%20training%20the%20classifier%20jointly%20on%20real%20and%0Agenerated%20volumes.%20The%20second%20experiment%20showed%20a%207%25%20improvement%20when%20training%0Aon%20both%20real%20and%20generated%20volumes%20based%20on%20unseen%20prompts.%20Moreover%2C%0AGenerateCT%20enables%20the%20scaling%20of%20synthetic%20training%20datasets%20to%20arbitrary%0Asizes.%20As%20an%20example%2C%20we%20generated%20100%2C000%203D%20CT%20volumes%2C%20fivefold%20the%20number%0Ain%20our%20real%20dataset%2C%20and%20trained%20the%20classifier%20exclusively%20on%20these%20synthetic%0Avolumes.%20Impressively%2C%20this%20classifier%20surpassed%20the%20performance%20of%20the%20one%0Atrained%20on%20all%20available%20real%20data%20by%20a%20margin%20of%208%25.%20Lastly%2C%20domain%20experts%0Aevaluated%20the%20generated%20volumes%2C%20confirming%20a%20high%20degree%20of%20alignment%20with%20the%0Atext%20prompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16037v4&entry.124074799=Read"},
{"title": "BEV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues", "author": "Fudong Ge and Yiwei Zhang and Shuhan Shen and Yue Wang and Weiming Hu and Jin Gao", "abstract": "  In this paper, we propose a new image-based visual place recognition (VPR)\nframework by exploiting the structural cues in bird's-eye view (BEV) from a\nsingle monocular camera. The motivation arises from two key observations about\nVPR: 1) For the methods based on both camera and LiDAR sensors, the integration\nof LiDAR in robotic systems has led to increased expenses, while the alignment\nof data between different sensors is also a major challenge. 2) Other\nimage-/camera-based methods, involving integrating RGB images and their derived\nvariants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several\nlimitations, such as the failure to effectively exploit the explicit spatial\nrelationships between different objects. To tackle the above issues, we design\na new BEV-enhanced VPR framework, nemely BEV2PR, which can generate a composite\ndescriptor with both visual cues and spatial awareness solely based on a single\ncamera. For the visual cues, any popular aggregation module for RGB global\nfeatures can be integrated into our framework. The key points lie in: 1) We use\nBEV segmentation features as an explicit source of structural knowledge in\nconstructing global features. 2) The lower layers of the pre-trained backbone\nfrom BEV map generation are shared for visual and structural streams in VPR,\nfacilitating the learning of fine-grained local features in the visual stream.\n3) The complementary visual features and structural features can jointly\nenhance VPR performance. Our BEV2PR framework enables consistent performance\nimprovements over several popular camera-based VPR aggregation modules when\nintegrating them. The experiments on our collected VPR-NuScenes dataset\ndemonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP\nbaseline to achieve the best performance in our setting, and notably, a 18.06%\ngain on the hard set.\n", "link": "http://arxiv.org/abs/2403.06600v1", "date": "2024-03-11", "relevancy": 2.11, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5335}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5257}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.517}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20BEV2PR%3A%20BEV-Enhanced%20Visual%20Place%20Recognition%20with%20Structural%20Cues&body=Title%3A%20BEV2PR%3A%20BEV-Enhanced%20Visual%20Place%20Recognition%20with%20Structural%20Cues%0AAuthor%3A%20Fudong%20Ge%20and%20Yiwei%20Zhang%20and%20Shuhan%20Shen%20and%20Yue%20Wang%20and%20Weiming%20Hu%20and%20Jin%20Gao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20image-based%20visual%20place%20recognition%20%28VPR%29%0Aframework%20by%20exploiting%20the%20structural%20cues%20in%20bird%27s-eye%20view%20%28BEV%29%20from%20a%0Asingle%20monocular%20camera.%20The%20motivation%20arises%20from%20two%20key%20observations%20about%0AVPR%3A%201%29%20For%20the%20methods%20based%20on%20both%20camera%20and%20LiDAR%20sensors%2C%20the%20integration%0Aof%20LiDAR%20in%20robotic%20systems%20has%20led%20to%20increased%20expenses%2C%20while%20the%20alignment%0Aof%20data%20between%20different%20sensors%20is%20also%20a%20major%20challenge.%202%29%20Other%0Aimage-/camera-based%20methods%2C%20involving%20integrating%20RGB%20images%20and%20their%20derived%0Avariants%20%28e.g.%2C%20pseudo%20depth%20images%2C%20pseudo%203D%20point%20clouds%29%2C%20exhibit%20several%0Alimitations%2C%20such%20as%20the%20failure%20to%20effectively%20exploit%20the%20explicit%20spatial%0Arelationships%20between%20different%20objects.%20To%20tackle%20the%20above%20issues%2C%20we%20design%0Aa%20new%20BEV-enhanced%20VPR%20framework%2C%20nemely%20BEV2PR%2C%20which%20can%20generate%20a%20composite%0Adescriptor%20with%20both%20visual%20cues%20and%20spatial%20awareness%20solely%20based%20on%20a%20single%0Acamera.%20For%20the%20visual%20cues%2C%20any%20popular%20aggregation%20module%20for%20RGB%20global%0Afeatures%20can%20be%20integrated%20into%20our%20framework.%20The%20key%20points%20lie%20in%3A%201%29%20We%20use%0ABEV%20segmentation%20features%20as%20an%20explicit%20source%20of%20structural%20knowledge%20in%0Aconstructing%20global%20features.%202%29%20The%20lower%20layers%20of%20the%20pre-trained%20backbone%0Afrom%20BEV%20map%20generation%20are%20shared%20for%20visual%20and%20structural%20streams%20in%20VPR%2C%0Afacilitating%20the%20learning%20of%20fine-grained%20local%20features%20in%20the%20visual%20stream.%0A3%29%20The%20complementary%20visual%20features%20and%20structural%20features%20can%20jointly%0Aenhance%20VPR%20performance.%20Our%20BEV2PR%20framework%20enables%20consistent%20performance%0Aimprovements%20over%20several%20popular%20camera-based%20VPR%20aggregation%20modules%20when%0Aintegrating%20them.%20The%20experiments%20on%20our%20collected%20VPR-NuScenes%20dataset%0Ademonstrate%20an%20absolute%20gain%20of%202.47%25%20on%20Recall%401%20for%20the%20strong%20Conv-AP%0Abaseline%20to%20achieve%20the%20best%20performance%20in%20our%20setting%2C%20and%20notably%2C%20a%2018.06%25%0Again%20on%20the%20hard%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06600v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEV2PR%3A%20BEV-Enhanced%20Visual%20Place%20Recognition%20with%20Structural%20Cues&entry.906535625=Fudong%20Ge%20and%20Yiwei%20Zhang%20and%20Shuhan%20Shen%20and%20Yue%20Wang%20and%20Weiming%20Hu%20and%20Jin%20Gao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20image-based%20visual%20place%20recognition%20%28VPR%29%0Aframework%20by%20exploiting%20the%20structural%20cues%20in%20bird%27s-eye%20view%20%28BEV%29%20from%20a%0Asingle%20monocular%20camera.%20The%20motivation%20arises%20from%20two%20key%20observations%20about%0AVPR%3A%201%29%20For%20the%20methods%20based%20on%20both%20camera%20and%20LiDAR%20sensors%2C%20the%20integration%0Aof%20LiDAR%20in%20robotic%20systems%20has%20led%20to%20increased%20expenses%2C%20while%20the%20alignment%0Aof%20data%20between%20different%20sensors%20is%20also%20a%20major%20challenge.%202%29%20Other%0Aimage-/camera-based%20methods%2C%20involving%20integrating%20RGB%20images%20and%20their%20derived%0Avariants%20%28e.g.%2C%20pseudo%20depth%20images%2C%20pseudo%203D%20point%20clouds%29%2C%20exhibit%20several%0Alimitations%2C%20such%20as%20the%20failure%20to%20effectively%20exploit%20the%20explicit%20spatial%0Arelationships%20between%20different%20objects.%20To%20tackle%20the%20above%20issues%2C%20we%20design%0Aa%20new%20BEV-enhanced%20VPR%20framework%2C%20nemely%20BEV2PR%2C%20which%20can%20generate%20a%20composite%0Adescriptor%20with%20both%20visual%20cues%20and%20spatial%20awareness%20solely%20based%20on%20a%20single%0Acamera.%20For%20the%20visual%20cues%2C%20any%20popular%20aggregation%20module%20for%20RGB%20global%0Afeatures%20can%20be%20integrated%20into%20our%20framework.%20The%20key%20points%20lie%20in%3A%201%29%20We%20use%0ABEV%20segmentation%20features%20as%20an%20explicit%20source%20of%20structural%20knowledge%20in%0Aconstructing%20global%20features.%202%29%20The%20lower%20layers%20of%20the%20pre-trained%20backbone%0Afrom%20BEV%20map%20generation%20are%20shared%20for%20visual%20and%20structural%20streams%20in%20VPR%2C%0Afacilitating%20the%20learning%20of%20fine-grained%20local%20features%20in%20the%20visual%20stream.%0A3%29%20The%20complementary%20visual%20features%20and%20structural%20features%20can%20jointly%0Aenhance%20VPR%20performance.%20Our%20BEV2PR%20framework%20enables%20consistent%20performance%0Aimprovements%20over%20several%20popular%20camera-based%20VPR%20aggregation%20modules%20when%0Aintegrating%20them.%20The%20experiments%20on%20our%20collected%20VPR-NuScenes%20dataset%0Ademonstrate%20an%20absolute%20gain%20of%202.47%25%20on%20Recall%401%20for%20the%20strong%20Conv-AP%0Abaseline%20to%20achieve%20the%20best%20performance%20in%20our%20setting%2C%20and%20notably%2C%20a%2018.06%25%0Again%20on%20the%20hard%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06600v1&entry.124074799=Read"},
{"title": "From Posterior Sampling to Meaningful Diversity in Image Restoration", "author": "Noa Cohen and Hila Manor and Yuval Bahat and Tomer Michaeli", "abstract": "  Image restoration problems are typically ill-posed in the sense that each\ndegraded image can be restored in infinitely many valid ways. To accommodate\nthis, many works generate a diverse set of outputs by attempting to randomly\nsample from the posterior distribution of natural images given the degraded\ninput. Here we argue that this strategy is commonly of limited practical value\nbecause of the heavy tail of the posterior distribution. Consider for example\ninpainting a missing region of the sky in an image. Since there is a high\nprobability that the missing region contains no object but clouds, any set of\nsamples from the posterior would be entirely dominated by (practically\nidentical) completions of sky. However, arguably, presenting users with only\none clear sky completion, along with several alternative solutions such as\nairships, birds, and balloons, would better outline the set of possibilities.\nIn this paper, we initiate the study of meaningfully diverse image restoration.\nWe explore several post-processing approaches that can be combined with any\ndiverse image restoration method to yield semantically meaningful diversity.\nMoreover, we propose a practical approach for allowing diffusion based image\nrestoration methods to generate meaningfully diverse outputs, while incurring\nonly negligent computational overhead. We conduct extensive user studies to\nanalyze the proposed techniques, and find the strategy of reducing similarity\nbetween outputs to be significantly favorable over posterior sampling. Code and\nexamples are available at https://noa-cohen.github.io/MeaningfulDiversityInIR.\n", "link": "http://arxiv.org/abs/2310.16047v2", "date": "2024-03-11", "relevancy": 2.1055, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5118}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5088}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20From%20Posterior%20Sampling%20to%20Meaningful%20Diversity%20in%20Image%20Restoration&body=Title%3A%20From%20Posterior%20Sampling%20to%20Meaningful%20Diversity%20in%20Image%20Restoration%0AAuthor%3A%20Noa%20Cohen%20and%20Hila%20Manor%20and%20Yuval%20Bahat%20and%20Tomer%20Michaeli%0AAbstract%3A%20%20%20Image%20restoration%20problems%20are%20typically%20ill-posed%20in%20the%20sense%20that%20each%0Adegraded%20image%20can%20be%20restored%20in%20infinitely%20many%20valid%20ways.%20To%20accommodate%0Athis%2C%20many%20works%20generate%20a%20diverse%20set%20of%20outputs%20by%20attempting%20to%20randomly%0Asample%20from%20the%20posterior%20distribution%20of%20natural%20images%20given%20the%20degraded%0Ainput.%20Here%20we%20argue%20that%20this%20strategy%20is%20commonly%20of%20limited%20practical%20value%0Abecause%20of%20the%20heavy%20tail%20of%20the%20posterior%20distribution.%20Consider%20for%20example%0Ainpainting%20a%20missing%20region%20of%20the%20sky%20in%20an%20image.%20Since%20there%20is%20a%20high%0Aprobability%20that%20the%20missing%20region%20contains%20no%20object%20but%20clouds%2C%20any%20set%20of%0Asamples%20from%20the%20posterior%20would%20be%20entirely%20dominated%20by%20%28practically%0Aidentical%29%20completions%20of%20sky.%20However%2C%20arguably%2C%20presenting%20users%20with%20only%0Aone%20clear%20sky%20completion%2C%20along%20with%20several%20alternative%20solutions%20such%20as%0Aairships%2C%20birds%2C%20and%20balloons%2C%20would%20better%20outline%20the%20set%20of%20possibilities.%0AIn%20this%20paper%2C%20we%20initiate%20the%20study%20of%20meaningfully%20diverse%20image%20restoration.%0AWe%20explore%20several%20post-processing%20approaches%20that%20can%20be%20combined%20with%20any%0Adiverse%20image%20restoration%20method%20to%20yield%20semantically%20meaningful%20diversity.%0AMoreover%2C%20we%20propose%20a%20practical%20approach%20for%20allowing%20diffusion%20based%20image%0Arestoration%20methods%20to%20generate%20meaningfully%20diverse%20outputs%2C%20while%20incurring%0Aonly%20negligent%20computational%20overhead.%20We%20conduct%20extensive%20user%20studies%20to%0Aanalyze%20the%20proposed%20techniques%2C%20and%20find%20the%20strategy%20of%20reducing%20similarity%0Abetween%20outputs%20to%20be%20significantly%20favorable%20over%20posterior%20sampling.%20Code%20and%0Aexamples%20are%20available%20at%20https%3A//noa-cohen.github.io/MeaningfulDiversityInIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16047v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Posterior%20Sampling%20to%20Meaningful%20Diversity%20in%20Image%20Restoration&entry.906535625=Noa%20Cohen%20and%20Hila%20Manor%20and%20Yuval%20Bahat%20and%20Tomer%20Michaeli&entry.1292438233=%20%20Image%20restoration%20problems%20are%20typically%20ill-posed%20in%20the%20sense%20that%20each%0Adegraded%20image%20can%20be%20restored%20in%20infinitely%20many%20valid%20ways.%20To%20accommodate%0Athis%2C%20many%20works%20generate%20a%20diverse%20set%20of%20outputs%20by%20attempting%20to%20randomly%0Asample%20from%20the%20posterior%20distribution%20of%20natural%20images%20given%20the%20degraded%0Ainput.%20Here%20we%20argue%20that%20this%20strategy%20is%20commonly%20of%20limited%20practical%20value%0Abecause%20of%20the%20heavy%20tail%20of%20the%20posterior%20distribution.%20Consider%20for%20example%0Ainpainting%20a%20missing%20region%20of%20the%20sky%20in%20an%20image.%20Since%20there%20is%20a%20high%0Aprobability%20that%20the%20missing%20region%20contains%20no%20object%20but%20clouds%2C%20any%20set%20of%0Asamples%20from%20the%20posterior%20would%20be%20entirely%20dominated%20by%20%28practically%0Aidentical%29%20completions%20of%20sky.%20However%2C%20arguably%2C%20presenting%20users%20with%20only%0Aone%20clear%20sky%20completion%2C%20along%20with%20several%20alternative%20solutions%20such%20as%0Aairships%2C%20birds%2C%20and%20balloons%2C%20would%20better%20outline%20the%20set%20of%20possibilities.%0AIn%20this%20paper%2C%20we%20initiate%20the%20study%20of%20meaningfully%20diverse%20image%20restoration.%0AWe%20explore%20several%20post-processing%20approaches%20that%20can%20be%20combined%20with%20any%0Adiverse%20image%20restoration%20method%20to%20yield%20semantically%20meaningful%20diversity.%0AMoreover%2C%20we%20propose%20a%20practical%20approach%20for%20allowing%20diffusion%20based%20image%0Arestoration%20methods%20to%20generate%20meaningfully%20diverse%20outputs%2C%20while%20incurring%0Aonly%20negligent%20computational%20overhead.%20We%20conduct%20extensive%20user%20studies%20to%0Aanalyze%20the%20proposed%20techniques%2C%20and%20find%20the%20strategy%20of%20reducing%20similarity%0Abetween%20outputs%20to%20be%20significantly%20favorable%20over%20posterior%20sampling.%20Code%20and%0Aexamples%20are%20available%20at%20https%3A//noa-cohen.github.io/MeaningfulDiversityInIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16047v2&entry.124074799=Read"},
{"title": "DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for\n  CAVs", "author": "Ghazal Asemian and Mohammadreza Amini and Burak Kantarci and Melike Erol-Kantarci", "abstract": "  The Synchronization Signal Block (SSB) is a fundamental component of the 5G\nNew Radio (NR) air interface, crucial for the initial access procedure of\nConnected and Automated Vehicles (CAVs), and serves several key purposes in the\nnetwork's operation. However, due to the predictable nature of SSB\ntransmission, including the Primary and Secondary Synchronization Signals (PSS\nand SSS), jamming attacks are critical threats. These attacks, which can be\nexecuted without requiring high power or complex equipment, pose substantial\nrisks to the 5G network, particularly as a result of the unencrypted\ntransmission of control signals. Leveraging RF domain knowledge, this work\npresents a novel deep learning-based technique for detecting jammers in CAV\nnetworks. Unlike the existing jamming detection algorithms that mostly rely on\nnetwork parameters, we introduce a double-threshold deep learning jamming\ndetector by focusing on the SSB. The detection method is focused on RF domain\nfeatures and improves the robustness of the network without requiring\nintegration with the pre-existing network infrastructure. By integrating a\npreprocessing block to extract PSS correlation and energy per null resource\nelements (EPNRE) characteristics, our method distinguishes between normal and\njammed received signals with high precision. Additionally, by incorporating of\nDiscrete Wavelet Transform (DWT), the efficacy of training and detection are\noptimized. A double-threshold double Deep Neural Network (DT-DDNN) is also\nintroduced to the architecture complemented by a deep cascade learning model to\nincrease the sensitivity of the model to variations of signal-to-jamming noise\nratio (SJNR). Results show that the proposed method achieves 96.4% detection\nrate in extra low jamming power, i.e., SJNR between 15 to 30 dB. Further,\nperformance of DT-DDNN is validated by analyzing real 5G signals obtained from\na practical testbed.\n", "link": "http://arxiv.org/abs/2403.02645v2", "date": "2024-03-11", "relevancy": 2.1018, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4307}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4195}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4109}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DT-DDNN%3A%20A%20Physical%20Layer%20Security%20Attack%20Detector%20in%205G%20RF%20Domain%20for%0A%20%20CAVs&body=Title%3A%20DT-DDNN%3A%20A%20Physical%20Layer%20Security%20Attack%20Detector%20in%205G%20RF%20Domain%20for%0A%20%20CAVs%0AAuthor%3A%20Ghazal%20Asemian%20and%20Mohammadreza%20Amini%20and%20Burak%20Kantarci%20and%20Melike%20Erol-Kantarci%0AAbstract%3A%20%20%20The%20Synchronization%20Signal%20Block%20%28SSB%29%20is%20a%20fundamental%20component%20of%20the%205G%0ANew%20Radio%20%28NR%29%20air%20interface%2C%20crucial%20for%20the%20initial%20access%20procedure%20of%0AConnected%20and%20Automated%20Vehicles%20%28CAVs%29%2C%20and%20serves%20several%20key%20purposes%20in%20the%0Anetwork%27s%20operation.%20However%2C%20due%20to%20the%20predictable%20nature%20of%20SSB%0Atransmission%2C%20including%20the%20Primary%20and%20Secondary%20Synchronization%20Signals%20%28PSS%0Aand%20SSS%29%2C%20jamming%20attacks%20are%20critical%20threats.%20These%20attacks%2C%20which%20can%20be%0Aexecuted%20without%20requiring%20high%20power%20or%20complex%20equipment%2C%20pose%20substantial%0Arisks%20to%20the%205G%20network%2C%20particularly%20as%20a%20result%20of%20the%20unencrypted%0Atransmission%20of%20control%20signals.%20Leveraging%20RF%20domain%20knowledge%2C%20this%20work%0Apresents%20a%20novel%20deep%20learning-based%20technique%20for%20detecting%20jammers%20in%20CAV%0Anetworks.%20Unlike%20the%20existing%20jamming%20detection%20algorithms%20that%20mostly%20rely%20on%0Anetwork%20parameters%2C%20we%20introduce%20a%20double-threshold%20deep%20learning%20jamming%0Adetector%20by%20focusing%20on%20the%20SSB.%20The%20detection%20method%20is%20focused%20on%20RF%20domain%0Afeatures%20and%20improves%20the%20robustness%20of%20the%20network%20without%20requiring%0Aintegration%20with%20the%20pre-existing%20network%20infrastructure.%20By%20integrating%20a%0Apreprocessing%20block%20to%20extract%20PSS%20correlation%20and%20energy%20per%20null%20resource%0Aelements%20%28EPNRE%29%20characteristics%2C%20our%20method%20distinguishes%20between%20normal%20and%0Ajammed%20received%20signals%20with%20high%20precision.%20Additionally%2C%20by%20incorporating%20of%0ADiscrete%20Wavelet%20Transform%20%28DWT%29%2C%20the%20efficacy%20of%20training%20and%20detection%20are%0Aoptimized.%20A%20double-threshold%20double%20Deep%20Neural%20Network%20%28DT-DDNN%29%20is%20also%0Aintroduced%20to%20the%20architecture%20complemented%20by%20a%20deep%20cascade%20learning%20model%20to%0Aincrease%20the%20sensitivity%20of%20the%20model%20to%20variations%20of%20signal-to-jamming%20noise%0Aratio%20%28SJNR%29.%20Results%20show%20that%20the%20proposed%20method%20achieves%2096.4%25%20detection%0Arate%20in%20extra%20low%20jamming%20power%2C%20i.e.%2C%20SJNR%20between%2015%20to%2030%20dB.%20Further%2C%0Aperformance%20of%20DT-DDNN%20is%20validated%20by%20analyzing%20real%205G%20signals%20obtained%20from%0Aa%20practical%20testbed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02645v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DT-DDNN%3A%20A%20Physical%20Layer%20Security%20Attack%20Detector%20in%205G%20RF%20Domain%20for%0A%20%20CAVs&entry.906535625=Ghazal%20Asemian%20and%20Mohammadreza%20Amini%20and%20Burak%20Kantarci%20and%20Melike%20Erol-Kantarci&entry.1292438233=%20%20The%20Synchronization%20Signal%20Block%20%28SSB%29%20is%20a%20fundamental%20component%20of%20the%205G%0ANew%20Radio%20%28NR%29%20air%20interface%2C%20crucial%20for%20the%20initial%20access%20procedure%20of%0AConnected%20and%20Automated%20Vehicles%20%28CAVs%29%2C%20and%20serves%20several%20key%20purposes%20in%20the%0Anetwork%27s%20operation.%20However%2C%20due%20to%20the%20predictable%20nature%20of%20SSB%0Atransmission%2C%20including%20the%20Primary%20and%20Secondary%20Synchronization%20Signals%20%28PSS%0Aand%20SSS%29%2C%20jamming%20attacks%20are%20critical%20threats.%20These%20attacks%2C%20which%20can%20be%0Aexecuted%20without%20requiring%20high%20power%20or%20complex%20equipment%2C%20pose%20substantial%0Arisks%20to%20the%205G%20network%2C%20particularly%20as%20a%20result%20of%20the%20unencrypted%0Atransmission%20of%20control%20signals.%20Leveraging%20RF%20domain%20knowledge%2C%20this%20work%0Apresents%20a%20novel%20deep%20learning-based%20technique%20for%20detecting%20jammers%20in%20CAV%0Anetworks.%20Unlike%20the%20existing%20jamming%20detection%20algorithms%20that%20mostly%20rely%20on%0Anetwork%20parameters%2C%20we%20introduce%20a%20double-threshold%20deep%20learning%20jamming%0Adetector%20by%20focusing%20on%20the%20SSB.%20The%20detection%20method%20is%20focused%20on%20RF%20domain%0Afeatures%20and%20improves%20the%20robustness%20of%20the%20network%20without%20requiring%0Aintegration%20with%20the%20pre-existing%20network%20infrastructure.%20By%20integrating%20a%0Apreprocessing%20block%20to%20extract%20PSS%20correlation%20and%20energy%20per%20null%20resource%0Aelements%20%28EPNRE%29%20characteristics%2C%20our%20method%20distinguishes%20between%20normal%20and%0Ajammed%20received%20signals%20with%20high%20precision.%20Additionally%2C%20by%20incorporating%20of%0ADiscrete%20Wavelet%20Transform%20%28DWT%29%2C%20the%20efficacy%20of%20training%20and%20detection%20are%0Aoptimized.%20A%20double-threshold%20double%20Deep%20Neural%20Network%20%28DT-DDNN%29%20is%20also%0Aintroduced%20to%20the%20architecture%20complemented%20by%20a%20deep%20cascade%20learning%20model%20to%0Aincrease%20the%20sensitivity%20of%20the%20model%20to%20variations%20of%20signal-to-jamming%20noise%0Aratio%20%28SJNR%29.%20Results%20show%20that%20the%20proposed%20method%20achieves%2096.4%25%20detection%0Arate%20in%20extra%20low%20jamming%20power%2C%20i.e.%2C%20SJNR%20between%2015%20to%2030%20dB.%20Further%2C%0Aperformance%20of%20DT-DDNN%20is%20validated%20by%20analyzing%20real%205G%20signals%20obtained%20from%0Aa%20practical%20testbed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02645v2&entry.124074799=Read"},
{"title": "Trustworthy Partial Label Learning with Out-of-distribution Detection", "author": "Jintao Huang and Yiu-Ming Cheung", "abstract": "  Partial Label Learning (PLL) grapples with learning from ambiguously labelled\ndata, and it has been successfully applied in fields such as image recognition.\nNevertheless, traditional PLL methods rely on the closed-world assumption,\nwhich can be limiting in open-world scenarios and negatively impact model\nperformance and generalization. To tackle these challenges, our study\nintroduces a novel method called PLL-OOD, which is the first to incorporate\nOut-of-Distribution (OOD) detection into the PLL framework. PLL-OOD\nsignificantly enhances model adaptability and accuracy by merging\nself-supervised learning with partial label loss and pioneering the\nPartial-Energy (PE) score for OOD detection. This approach improves data\nfeature representation and effectively disambiguates candidate labels, using a\ndynamic label confidence matrix to refine predictions. The PE score, adjusted\nby label confidence, precisely identifies OOD instances, optimizing model\ntraining towards in-distribution data. This innovative method markedly boosts\nPLL model robustness and performance in open-world settings. To validate our\napproach, we conducted a comprehensive comparative experiment combining the\nexisting state-of-the-art PLL model with multiple OOD scores on the CIFAR-10\nand CIFAR-100 datasets with various OOD datasets. The results demonstrate that\nthe proposed PLL-OOD framework is highly effective and effectiveness\noutperforms existing models, showcasing its superiority and effectiveness.\n", "link": "http://arxiv.org/abs/2403.06681v1", "date": "2024-03-11", "relevancy": 2.1005, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5673}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.52}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5134}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Partial%20Label%20Learning%20with%20Out-of-distribution%20Detection&body=Title%3A%20Trustworthy%20Partial%20Label%20Learning%20with%20Out-of-distribution%20Detection%0AAuthor%3A%20Jintao%20Huang%20and%20Yiu-Ming%20Cheung%0AAbstract%3A%20%20%20Partial%20Label%20Learning%20%28PLL%29%20grapples%20with%20learning%20from%20ambiguously%20labelled%0Adata%2C%20and%20it%20has%20been%20successfully%20applied%20in%20fields%20such%20as%20image%20recognition.%0ANevertheless%2C%20traditional%20PLL%20methods%20rely%20on%20the%20closed-world%20assumption%2C%0Awhich%20can%20be%20limiting%20in%20open-world%20scenarios%20and%20negatively%20impact%20model%0Aperformance%20and%20generalization.%20To%20tackle%20these%20challenges%2C%20our%20study%0Aintroduces%20a%20novel%20method%20called%20PLL-OOD%2C%20which%20is%20the%20first%20to%20incorporate%0AOut-of-Distribution%20%28OOD%29%20detection%20into%20the%20PLL%20framework.%20PLL-OOD%0Asignificantly%20enhances%20model%20adaptability%20and%20accuracy%20by%20merging%0Aself-supervised%20learning%20with%20partial%20label%20loss%20and%20pioneering%20the%0APartial-Energy%20%28PE%29%20score%20for%20OOD%20detection.%20This%20approach%20improves%20data%0Afeature%20representation%20and%20effectively%20disambiguates%20candidate%20labels%2C%20using%20a%0Adynamic%20label%20confidence%20matrix%20to%20refine%20predictions.%20The%20PE%20score%2C%20adjusted%0Aby%20label%20confidence%2C%20precisely%20identifies%20OOD%20instances%2C%20optimizing%20model%0Atraining%20towards%20in-distribution%20data.%20This%20innovative%20method%20markedly%20boosts%0APLL%20model%20robustness%20and%20performance%20in%20open-world%20settings.%20To%20validate%20our%0Aapproach%2C%20we%20conducted%20a%20comprehensive%20comparative%20experiment%20combining%20the%0Aexisting%20state-of-the-art%20PLL%20model%20with%20multiple%20OOD%20scores%20on%20the%20CIFAR-10%0Aand%20CIFAR-100%20datasets%20with%20various%20OOD%20datasets.%20The%20results%20demonstrate%20that%0Athe%20proposed%20PLL-OOD%20framework%20is%20highly%20effective%20and%20effectiveness%0Aoutperforms%20existing%20models%2C%20showcasing%20its%20superiority%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06681v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Partial%20Label%20Learning%20with%20Out-of-distribution%20Detection&entry.906535625=Jintao%20Huang%20and%20Yiu-Ming%20Cheung&entry.1292438233=%20%20Partial%20Label%20Learning%20%28PLL%29%20grapples%20with%20learning%20from%20ambiguously%20labelled%0Adata%2C%20and%20it%20has%20been%20successfully%20applied%20in%20fields%20such%20as%20image%20recognition.%0ANevertheless%2C%20traditional%20PLL%20methods%20rely%20on%20the%20closed-world%20assumption%2C%0Awhich%20can%20be%20limiting%20in%20open-world%20scenarios%20and%20negatively%20impact%20model%0Aperformance%20and%20generalization.%20To%20tackle%20these%20challenges%2C%20our%20study%0Aintroduces%20a%20novel%20method%20called%20PLL-OOD%2C%20which%20is%20the%20first%20to%20incorporate%0AOut-of-Distribution%20%28OOD%29%20detection%20into%20the%20PLL%20framework.%20PLL-OOD%0Asignificantly%20enhances%20model%20adaptability%20and%20accuracy%20by%20merging%0Aself-supervised%20learning%20with%20partial%20label%20loss%20and%20pioneering%20the%0APartial-Energy%20%28PE%29%20score%20for%20OOD%20detection.%20This%20approach%20improves%20data%0Afeature%20representation%20and%20effectively%20disambiguates%20candidate%20labels%2C%20using%20a%0Adynamic%20label%20confidence%20matrix%20to%20refine%20predictions.%20The%20PE%20score%2C%20adjusted%0Aby%20label%20confidence%2C%20precisely%20identifies%20OOD%20instances%2C%20optimizing%20model%0Atraining%20towards%20in-distribution%20data.%20This%20innovative%20method%20markedly%20boosts%0APLL%20model%20robustness%20and%20performance%20in%20open-world%20settings.%20To%20validate%20our%0Aapproach%2C%20we%20conducted%20a%20comprehensive%20comparative%20experiment%20combining%20the%0Aexisting%20state-of-the-art%20PLL%20model%20with%20multiple%20OOD%20scores%20on%20the%20CIFAR-10%0Aand%20CIFAR-100%20datasets%20with%20various%20OOD%20datasets.%20The%20results%20demonstrate%20that%0Athe%20proposed%20PLL-OOD%20framework%20is%20highly%20effective%20and%20effectiveness%0Aoutperforms%20existing%20models%2C%20showcasing%20its%20superiority%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06681v1&entry.124074799=Read"},
{"title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding", "author": "Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Hao Yang and Yaofeng Sun and Chengqi Deng and Hanwei Xu and Zhenda Xie and Chong Ruan", "abstract": "  We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed\nfor real-world vision and language understanding applications. Our approach is\nstructured around three key dimensions:\n  We strive to ensure our data is diverse, scalable, and extensively covers\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and\nknowledge-based content, aiming for a comprehensive representation of practical\ncontexts. Further, we create a use case taxonomy from real user scenarios and\nconstruct an instruction tuning dataset accordingly. The fine-tuning with this\ndataset substantially improves the model's user experience in practical\napplications. Considering efficiency and the demands of most real-world\nscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently\nprocesses high-resolution images (1024 x 1024), while maintaining a relatively\nlow computational overhead. This design choice ensures the model's ability to\ncapture critical semantic and detailed information across various visual tasks.\nWe posit that a proficient Vision-Language Model should, foremost, possess\nstrong language abilities. To ensure the preservation of LLM capabilities\nduring pretraining, we investigate an effective VL pretraining strategy by\nintegrating LLM training from the beginning and carefully managing the\ncompetitive dynamics observed between vision and language modalities.\n  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user\nexperiences as a vision-language chatbot in real-world applications, achieving\nstate-of-the-art or competitive performance across a wide range of\nvisual-language benchmarks at the same model size while maintaining robust\nperformance on language-centric benchmarks. We have made both 1.3B and 7B\nmodels publicly accessible to foster innovations based on this foundation\nmodel.\n", "link": "http://arxiv.org/abs/2403.05525v2", "date": "2024-03-11", "relevancy": 2.0994, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5315}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5021}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DeepSeek-VL%3A%20Towards%20Real-World%20Vision-Language%20Understanding&body=Title%3A%20DeepSeek-VL%3A%20Towards%20Real-World%20Vision-Language%20Understanding%0AAuthor%3A%20Haoyu%20Lu%20and%20Wen%20Liu%20and%20Bo%20Zhang%20and%20Bingxuan%20Wang%20and%20Kai%20Dong%20and%20Bo%20Liu%20and%20Jingxiang%20Sun%20and%20Tongzheng%20Ren%20and%20Zhuoshu%20Li%20and%20Hao%20Yang%20and%20Yaofeng%20Sun%20and%20Chengqi%20Deng%20and%20Hanwei%20Xu%20and%20Zhenda%20Xie%20and%20Chong%20Ruan%0AAbstract%3A%20%20%20We%20present%20DeepSeek-VL%2C%20an%20open-source%20Vision-Language%20%28VL%29%20Model%20designed%0Afor%20real-world%20vision%20and%20language%20understanding%20applications.%20Our%20approach%20is%0Astructured%20around%20three%20key%20dimensions%3A%0A%20%20We%20strive%20to%20ensure%20our%20data%20is%20diverse%2C%20scalable%2C%20and%20extensively%20covers%0Areal-world%20scenarios%20including%20web%20screenshots%2C%20PDFs%2C%20OCR%2C%20charts%2C%20and%0Aknowledge-based%20content%2C%20aiming%20for%20a%20comprehensive%20representation%20of%20practical%0Acontexts.%20Further%2C%20we%20create%20a%20use%20case%20taxonomy%20from%20real%20user%20scenarios%20and%0Aconstruct%20an%20instruction%20tuning%20dataset%20accordingly.%20The%20fine-tuning%20with%20this%0Adataset%20substantially%20improves%20the%20model%27s%20user%20experience%20in%20practical%0Aapplications.%20Considering%20efficiency%20and%20the%20demands%20of%20most%20real-world%0Ascenarios%2C%20DeepSeek-VL%20incorporates%20a%20hybrid%20vision%20encoder%20that%20efficiently%0Aprocesses%20high-resolution%20images%20%281024%20x%201024%29%2C%20while%20maintaining%20a%20relatively%0Alow%20computational%20overhead.%20This%20design%20choice%20ensures%20the%20model%27s%20ability%20to%0Acapture%20critical%20semantic%20and%20detailed%20information%20across%20various%20visual%20tasks.%0AWe%20posit%20that%20a%20proficient%20Vision-Language%20Model%20should%2C%20foremost%2C%20possess%0Astrong%20language%20abilities.%20To%20ensure%20the%20preservation%20of%20LLM%20capabilities%0Aduring%20pretraining%2C%20we%20investigate%20an%20effective%20VL%20pretraining%20strategy%20by%0Aintegrating%20LLM%20training%20from%20the%20beginning%20and%20carefully%20managing%20the%0Acompetitive%20dynamics%20observed%20between%20vision%20and%20language%20modalities.%0A%20%20The%20DeepSeek-VL%20family%20%28both%201.3B%20and%207B%20models%29%20showcases%20superior%20user%0Aexperiences%20as%20a%20vision-language%20chatbot%20in%20real-world%20applications%2C%20achieving%0Astate-of-the-art%20or%20competitive%20performance%20across%20a%20wide%20range%20of%0Avisual-language%20benchmarks%20at%20the%20same%20model%20size%20while%20maintaining%20robust%0Aperformance%20on%20language-centric%20benchmarks.%20We%20have%20made%20both%201.3B%20and%207B%0Amodels%20publicly%20accessible%20to%20foster%20innovations%20based%20on%20this%20foundation%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05525v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-VL%3A%20Towards%20Real-World%20Vision-Language%20Understanding&entry.906535625=Haoyu%20Lu%20and%20Wen%20Liu%20and%20Bo%20Zhang%20and%20Bingxuan%20Wang%20and%20Kai%20Dong%20and%20Bo%20Liu%20and%20Jingxiang%20Sun%20and%20Tongzheng%20Ren%20and%20Zhuoshu%20Li%20and%20Hao%20Yang%20and%20Yaofeng%20Sun%20and%20Chengqi%20Deng%20and%20Hanwei%20Xu%20and%20Zhenda%20Xie%20and%20Chong%20Ruan&entry.1292438233=%20%20We%20present%20DeepSeek-VL%2C%20an%20open-source%20Vision-Language%20%28VL%29%20Model%20designed%0Afor%20real-world%20vision%20and%20language%20understanding%20applications.%20Our%20approach%20is%0Astructured%20around%20three%20key%20dimensions%3A%0A%20%20We%20strive%20to%20ensure%20our%20data%20is%20diverse%2C%20scalable%2C%20and%20extensively%20covers%0Areal-world%20scenarios%20including%20web%20screenshots%2C%20PDFs%2C%20OCR%2C%20charts%2C%20and%0Aknowledge-based%20content%2C%20aiming%20for%20a%20comprehensive%20representation%20of%20practical%0Acontexts.%20Further%2C%20we%20create%20a%20use%20case%20taxonomy%20from%20real%20user%20scenarios%20and%0Aconstruct%20an%20instruction%20tuning%20dataset%20accordingly.%20The%20fine-tuning%20with%20this%0Adataset%20substantially%20improves%20the%20model%27s%20user%20experience%20in%20practical%0Aapplications.%20Considering%20efficiency%20and%20the%20demands%20of%20most%20real-world%0Ascenarios%2C%20DeepSeek-VL%20incorporates%20a%20hybrid%20vision%20encoder%20that%20efficiently%0Aprocesses%20high-resolution%20images%20%281024%20x%201024%29%2C%20while%20maintaining%20a%20relatively%0Alow%20computational%20overhead.%20This%20design%20choice%20ensures%20the%20model%27s%20ability%20to%0Acapture%20critical%20semantic%20and%20detailed%20information%20across%20various%20visual%20tasks.%0AWe%20posit%20that%20a%20proficient%20Vision-Language%20Model%20should%2C%20foremost%2C%20possess%0Astrong%20language%20abilities.%20To%20ensure%20the%20preservation%20of%20LLM%20capabilities%0Aduring%20pretraining%2C%20we%20investigate%20an%20effective%20VL%20pretraining%20strategy%20by%0Aintegrating%20LLM%20training%20from%20the%20beginning%20and%20carefully%20managing%20the%0Acompetitive%20dynamics%20observed%20between%20vision%20and%20language%20modalities.%0A%20%20The%20DeepSeek-VL%20family%20%28both%201.3B%20and%207B%20models%29%20showcases%20superior%20user%0Aexperiences%20as%20a%20vision-language%20chatbot%20in%20real-world%20applications%2C%20achieving%0Astate-of-the-art%20or%20competitive%20performance%20across%20a%20wide%20range%20of%0Avisual-language%20benchmarks%20at%20the%20same%20model%20size%20while%20maintaining%20robust%0Aperformance%20on%20language-centric%20benchmarks.%20We%20have%20made%20both%201.3B%20and%207B%0Amodels%20publicly%20accessible%20to%20foster%20innovations%20based%20on%20this%20foundation%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05525v2&entry.124074799=Read"},
{"title": "Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation", "author": "Luca Eyring and Dominik Klein and Th\u00e9o Uscidda and Giovanni Palla and Niki Kilbertus and Zeynep Akata and Fabian Theis", "abstract": "  In optimal transport (OT), a Monge map is known as a mapping that transports\na source distribution to a target distribution in the most cost-efficient way.\nRecently, multiple neural estimators for Monge maps have been developed and\napplied in diverse unpaired domain translation tasks, e.g. in single-cell\nbiology and computer vision. However, the classic OT framework enforces mass\nconservation, which makes it prone to outliers and limits its applicability in\nreal-world scenarios. The latter can be particularly harmful in OT domain\ntranslation tasks, where the relative position of a sample within a\ndistribution is explicitly taken into account. While unbalanced OT tackles this\nchallenge in the discrete setting, its integration into neural Monge map\nestimators has received limited attention. We propose a theoretically grounded\nmethod to incorporate unbalancedness into any Monge map estimator. We improve\nexisting estimators to model cell trajectories over time and to predict\ncellular responses to perturbations. Moreover, our approach seamlessly\nintegrates with the OT flow matching (OT-FM) framework. While we show that\nOT-FM performs competitively in image translation, we further improve\nperformance by incorporating unbalancedness (UOT-FM), which better preserves\nrelevant features. We hence establish UOT-FM as a principled method for\nunpaired image translation.\n", "link": "http://arxiv.org/abs/2311.15100v2", "date": "2024-03-11", "relevancy": 2.0993, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5516}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5057}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5056}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Unbalancedness%20in%20Neural%20Monge%20Maps%20Improves%20Unpaired%20Domain%20Translation&body=Title%3A%20Unbalancedness%20in%20Neural%20Monge%20Maps%20Improves%20Unpaired%20Domain%20Translation%0AAuthor%3A%20Luca%20Eyring%20and%20Dominik%20Klein%20and%20Th%C3%A9o%20Uscidda%20and%20Giovanni%20Palla%20and%20Niki%20Kilbertus%20and%20Zeynep%20Akata%20and%20Fabian%20Theis%0AAbstract%3A%20%20%20In%20optimal%20transport%20%28OT%29%2C%20a%20Monge%20map%20is%20known%20as%20a%20mapping%20that%20transports%0Aa%20source%20distribution%20to%20a%20target%20distribution%20in%20the%20most%20cost-efficient%20way.%0ARecently%2C%20multiple%20neural%20estimators%20for%20Monge%20maps%20have%20been%20developed%20and%0Aapplied%20in%20diverse%20unpaired%20domain%20translation%20tasks%2C%20e.g.%20in%20single-cell%0Abiology%20and%20computer%20vision.%20However%2C%20the%20classic%20OT%20framework%20enforces%20mass%0Aconservation%2C%20which%20makes%20it%20prone%20to%20outliers%20and%20limits%20its%20applicability%20in%0Areal-world%20scenarios.%20The%20latter%20can%20be%20particularly%20harmful%20in%20OT%20domain%0Atranslation%20tasks%2C%20where%20the%20relative%20position%20of%20a%20sample%20within%20a%0Adistribution%20is%20explicitly%20taken%20into%20account.%20While%20unbalanced%20OT%20tackles%20this%0Achallenge%20in%20the%20discrete%20setting%2C%20its%20integration%20into%20neural%20Monge%20map%0Aestimators%20has%20received%20limited%20attention.%20We%20propose%20a%20theoretically%20grounded%0Amethod%20to%20incorporate%20unbalancedness%20into%20any%20Monge%20map%20estimator.%20We%20improve%0Aexisting%20estimators%20to%20model%20cell%20trajectories%20over%20time%20and%20to%20predict%0Acellular%20responses%20to%20perturbations.%20Moreover%2C%20our%20approach%20seamlessly%0Aintegrates%20with%20the%20OT%20flow%20matching%20%28OT-FM%29%20framework.%20While%20we%20show%20that%0AOT-FM%20performs%20competitively%20in%20image%20translation%2C%20we%20further%20improve%0Aperformance%20by%20incorporating%20unbalancedness%20%28UOT-FM%29%2C%20which%20better%20preserves%0Arelevant%20features.%20We%20hence%20establish%20UOT-FM%20as%20a%20principled%20method%20for%0Aunpaired%20image%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15100v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbalancedness%20in%20Neural%20Monge%20Maps%20Improves%20Unpaired%20Domain%20Translation&entry.906535625=Luca%20Eyring%20and%20Dominik%20Klein%20and%20Th%C3%A9o%20Uscidda%20and%20Giovanni%20Palla%20and%20Niki%20Kilbertus%20and%20Zeynep%20Akata%20and%20Fabian%20Theis&entry.1292438233=%20%20In%20optimal%20transport%20%28OT%29%2C%20a%20Monge%20map%20is%20known%20as%20a%20mapping%20that%20transports%0Aa%20source%20distribution%20to%20a%20target%20distribution%20in%20the%20most%20cost-efficient%20way.%0ARecently%2C%20multiple%20neural%20estimators%20for%20Monge%20maps%20have%20been%20developed%20and%0Aapplied%20in%20diverse%20unpaired%20domain%20translation%20tasks%2C%20e.g.%20in%20single-cell%0Abiology%20and%20computer%20vision.%20However%2C%20the%20classic%20OT%20framework%20enforces%20mass%0Aconservation%2C%20which%20makes%20it%20prone%20to%20outliers%20and%20limits%20its%20applicability%20in%0Areal-world%20scenarios.%20The%20latter%20can%20be%20particularly%20harmful%20in%20OT%20domain%0Atranslation%20tasks%2C%20where%20the%20relative%20position%20of%20a%20sample%20within%20a%0Adistribution%20is%20explicitly%20taken%20into%20account.%20While%20unbalanced%20OT%20tackles%20this%0Achallenge%20in%20the%20discrete%20setting%2C%20its%20integration%20into%20neural%20Monge%20map%0Aestimators%20has%20received%20limited%20attention.%20We%20propose%20a%20theoretically%20grounded%0Amethod%20to%20incorporate%20unbalancedness%20into%20any%20Monge%20map%20estimator.%20We%20improve%0Aexisting%20estimators%20to%20model%20cell%20trajectories%20over%20time%20and%20to%20predict%0Acellular%20responses%20to%20perturbations.%20Moreover%2C%20our%20approach%20seamlessly%0Aintegrates%20with%20the%20OT%20flow%20matching%20%28OT-FM%29%20framework.%20While%20we%20show%20that%0AOT-FM%20performs%20competitively%20in%20image%20translation%2C%20we%20further%20improve%0Aperformance%20by%20incorporating%20unbalancedness%20%28UOT-FM%29%2C%20which%20better%20preserves%0Arelevant%20features.%20We%20hence%20establish%20UOT-FM%20as%20a%20principled%20method%20for%0Aunpaired%20image%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15100v2&entry.124074799=Read"},
{"title": "A Survey on the Robustness of Computer Vision Models against Common\n  Corruptions", "author": "Shunxin Wang and Raymond Veldhuis and Christoph Brune and Nicola Strisciuglio", "abstract": "  The performance of computer vision models are susceptible to unexpected\nchanges in input images, known as common corruptions (e.g. noise, blur,\nillumination changes, etc.), that can hinder their reliability when deployed in\nreal scenarios. These corruptions are not always considered to test model\ngeneralization and robustness. In this survey, we present a comprehensive\noverview of methods that improve the robustness of computer vision models\nagainst common corruptions. We categorize methods into four groups based on the\nmodel part and training method addressed: data augmentation, representation\nlearning, knowledge distillation, and network components. We also cover\nindirect methods for generalization and mitigation of shortcut learning,\npotentially useful for corruption robustness. We release a unified benchmark\nframework to compare robustness performance on several datasets, and address\nthe inconsistencies of evaluation in the literature. We provide an experimental\noverview of the base corruption robustness of popular vision backbones, and\nshow that corruption robustness does not necessarily scale with model size. The\nvery large models (above 100M parameters) gain negligible robustness,\nconsidering the increased computational requirements. To achieve generalizable\nand robust computer vision models, we foresee the need of developing new\nlearning strategies to efficiently exploit limited data and mitigate unwanted\nor unreliable learning behaviors.\n", "link": "http://arxiv.org/abs/2305.06024v3", "date": "2024-03-11", "relevancy": 2.097, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5431}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5162}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5086}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20the%20Robustness%20of%20Computer%20Vision%20Models%20against%20Common%0A%20%20Corruptions&body=Title%3A%20A%20Survey%20on%20the%20Robustness%20of%20Computer%20Vision%20Models%20against%20Common%0A%20%20Corruptions%0AAuthor%3A%20Shunxin%20Wang%20and%20Raymond%20Veldhuis%20and%20Christoph%20Brune%20and%20Nicola%20Strisciuglio%0AAbstract%3A%20%20%20The%20performance%20of%20computer%20vision%20models%20are%20susceptible%20to%20unexpected%0Achanges%20in%20input%20images%2C%20known%20as%20common%20corruptions%20%28e.g.%20noise%2C%20blur%2C%0Aillumination%20changes%2C%20etc.%29%2C%20that%20can%20hinder%20their%20reliability%20when%20deployed%20in%0Areal%20scenarios.%20These%20corruptions%20are%20not%20always%20considered%20to%20test%20model%0Ageneralization%20and%20robustness.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%0Aoverview%20of%20methods%20that%20improve%20the%20robustness%20of%20computer%20vision%20models%0Aagainst%20common%20corruptions.%20We%20categorize%20methods%20into%20four%20groups%20based%20on%20the%0Amodel%20part%20and%20training%20method%20addressed%3A%20data%20augmentation%2C%20representation%0Alearning%2C%20knowledge%20distillation%2C%20and%20network%20components.%20We%20also%20cover%0Aindirect%20methods%20for%20generalization%20and%20mitigation%20of%20shortcut%20learning%2C%0Apotentially%20useful%20for%20corruption%20robustness.%20We%20release%20a%20unified%20benchmark%0Aframework%20to%20compare%20robustness%20performance%20on%20several%20datasets%2C%20and%20address%0Athe%20inconsistencies%20of%20evaluation%20in%20the%20literature.%20We%20provide%20an%20experimental%0Aoverview%20of%20the%20base%20corruption%20robustness%20of%20popular%20vision%20backbones%2C%20and%0Ashow%20that%20corruption%20robustness%20does%20not%20necessarily%20scale%20with%20model%20size.%20The%0Avery%20large%20models%20%28above%20100M%20parameters%29%20gain%20negligible%20robustness%2C%0Aconsidering%20the%20increased%20computational%20requirements.%20To%20achieve%20generalizable%0Aand%20robust%20computer%20vision%20models%2C%20we%20foresee%20the%20need%20of%20developing%20new%0Alearning%20strategies%20to%20efficiently%20exploit%20limited%20data%20and%20mitigate%20unwanted%0Aor%20unreliable%20learning%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.06024v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20the%20Robustness%20of%20Computer%20Vision%20Models%20against%20Common%0A%20%20Corruptions&entry.906535625=Shunxin%20Wang%20and%20Raymond%20Veldhuis%20and%20Christoph%20Brune%20and%20Nicola%20Strisciuglio&entry.1292438233=%20%20The%20performance%20of%20computer%20vision%20models%20are%20susceptible%20to%20unexpected%0Achanges%20in%20input%20images%2C%20known%20as%20common%20corruptions%20%28e.g.%20noise%2C%20blur%2C%0Aillumination%20changes%2C%20etc.%29%2C%20that%20can%20hinder%20their%20reliability%20when%20deployed%20in%0Areal%20scenarios.%20These%20corruptions%20are%20not%20always%20considered%20to%20test%20model%0Ageneralization%20and%20robustness.%20In%20this%20survey%2C%20we%20present%20a%20comprehensive%0Aoverview%20of%20methods%20that%20improve%20the%20robustness%20of%20computer%20vision%20models%0Aagainst%20common%20corruptions.%20We%20categorize%20methods%20into%20four%20groups%20based%20on%20the%0Amodel%20part%20and%20training%20method%20addressed%3A%20data%20augmentation%2C%20representation%0Alearning%2C%20knowledge%20distillation%2C%20and%20network%20components.%20We%20also%20cover%0Aindirect%20methods%20for%20generalization%20and%20mitigation%20of%20shortcut%20learning%2C%0Apotentially%20useful%20for%20corruption%20robustness.%20We%20release%20a%20unified%20benchmark%0Aframework%20to%20compare%20robustness%20performance%20on%20several%20datasets%2C%20and%20address%0Athe%20inconsistencies%20of%20evaluation%20in%20the%20literature.%20We%20provide%20an%20experimental%0Aoverview%20of%20the%20base%20corruption%20robustness%20of%20popular%20vision%20backbones%2C%20and%0Ashow%20that%20corruption%20robustness%20does%20not%20necessarily%20scale%20with%20model%20size.%20The%0Avery%20large%20models%20%28above%20100M%20parameters%29%20gain%20negligible%20robustness%2C%0Aconsidering%20the%20increased%20computational%20requirements.%20To%20achieve%20generalizable%0Aand%20robust%20computer%20vision%20models%2C%20we%20foresee%20the%20need%20of%20developing%20new%0Alearning%20strategies%20to%20efficiently%20exploit%20limited%20data%20and%20mitigate%20unwanted%0Aor%20unreliable%20learning%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.06024v3&entry.124074799=Read"},
{"title": "Medical Image Synthesis via Fine-Grained Image-Text Alignment and\n  Anatomy-Pathology Prompting", "author": "Wenting Chen and Pengyu Wang and Hui Ren and Lichao Sun and Quanzheng Li and Yixuan Yuan and Xiang Li", "abstract": "  Data scarcity and privacy concerns limit the availability of high-quality\nmedical images for public use, which can be mitigated through medical image\nsynthesis. However, current medical image synthesis methods often struggle to\naccurately capture the complexity of detailed anatomical structures and\npathological conditions. To address these challenges, we propose a novel\nmedical image synthesis model that leverages fine-grained image-text alignment\nand anatomy-pathology prompts to generate highly detailed and accurate\nsynthetic medical images. Our method integrates advanced natural language\nprocessing techniques with image generative modeling, enabling precise\nalignment between descriptive text prompts and the synthesized images'\nanatomical and pathological details. The proposed approach consists of two key\ncomponents: an anatomy-pathology prompting module and a fine-grained\nalignment-based synthesis module. The anatomy-pathology prompting module\nautomatically generates descriptive prompts for high-quality medical images. To\nfurther synthesize high-quality medical images from the generated prompts, the\nfine-grained alignment-based synthesis module pre-defines a visual codebook for\nthe radiology dataset and performs fine-grained alignment between the codebook\nand generated prompts to obtain key patches as visual clues, facilitating\naccurate image synthesis. We validate the superiority of our method through\nexperiments on public chest X-ray datasets and demonstrate that our synthetic\nimages preserve accurate semantic information, making them valuable for various\nmedical applications.\n", "link": "http://arxiv.org/abs/2403.06835v1", "date": "2024-03-11", "relevancy": 2.095, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5275}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5269}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5188}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Medical%20Image%20Synthesis%20via%20Fine-Grained%20Image-Text%20Alignment%20and%0A%20%20Anatomy-Pathology%20Prompting&body=Title%3A%20Medical%20Image%20Synthesis%20via%20Fine-Grained%20Image-Text%20Alignment%20and%0A%20%20Anatomy-Pathology%20Prompting%0AAuthor%3A%20Wenting%20Chen%20and%20Pengyu%20Wang%20and%20Hui%20Ren%20and%20Lichao%20Sun%20and%20Quanzheng%20Li%20and%20Yixuan%20Yuan%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Data%20scarcity%20and%20privacy%20concerns%20limit%20the%20availability%20of%20high-quality%0Amedical%20images%20for%20public%20use%2C%20which%20can%20be%20mitigated%20through%20medical%20image%0Asynthesis.%20However%2C%20current%20medical%20image%20synthesis%20methods%20often%20struggle%20to%0Aaccurately%20capture%20the%20complexity%20of%20detailed%20anatomical%20structures%20and%0Apathological%20conditions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Amedical%20image%20synthesis%20model%20that%20leverages%20fine-grained%20image-text%20alignment%0Aand%20anatomy-pathology%20prompts%20to%20generate%20highly%20detailed%20and%20accurate%0Asynthetic%20medical%20images.%20Our%20method%20integrates%20advanced%20natural%20language%0Aprocessing%20techniques%20with%20image%20generative%20modeling%2C%20enabling%20precise%0Aalignment%20between%20descriptive%20text%20prompts%20and%20the%20synthesized%20images%27%0Aanatomical%20and%20pathological%20details.%20The%20proposed%20approach%20consists%20of%20two%20key%0Acomponents%3A%20an%20anatomy-pathology%20prompting%20module%20and%20a%20fine-grained%0Aalignment-based%20synthesis%20module.%20The%20anatomy-pathology%20prompting%20module%0Aautomatically%20generates%20descriptive%20prompts%20for%20high-quality%20medical%20images.%20To%0Afurther%20synthesize%20high-quality%20medical%20images%20from%20the%20generated%20prompts%2C%20the%0Afine-grained%20alignment-based%20synthesis%20module%20pre-defines%20a%20visual%20codebook%20for%0Athe%20radiology%20dataset%20and%20performs%20fine-grained%20alignment%20between%20the%20codebook%0Aand%20generated%20prompts%20to%20obtain%20key%20patches%20as%20visual%20clues%2C%20facilitating%0Aaccurate%20image%20synthesis.%20We%20validate%20the%20superiority%20of%20our%20method%20through%0Aexperiments%20on%20public%20chest%20X-ray%20datasets%20and%20demonstrate%20that%20our%20synthetic%0Aimages%20preserve%20accurate%20semantic%20information%2C%20making%20them%20valuable%20for%20various%0Amedical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06835v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Image%20Synthesis%20via%20Fine-Grained%20Image-Text%20Alignment%20and%0A%20%20Anatomy-Pathology%20Prompting&entry.906535625=Wenting%20Chen%20and%20Pengyu%20Wang%20and%20Hui%20Ren%20and%20Lichao%20Sun%20and%20Quanzheng%20Li%20and%20Yixuan%20Yuan%20and%20Xiang%20Li&entry.1292438233=%20%20Data%20scarcity%20and%20privacy%20concerns%20limit%20the%20availability%20of%20high-quality%0Amedical%20images%20for%20public%20use%2C%20which%20can%20be%20mitigated%20through%20medical%20image%0Asynthesis.%20However%2C%20current%20medical%20image%20synthesis%20methods%20often%20struggle%20to%0Aaccurately%20capture%20the%20complexity%20of%20detailed%20anatomical%20structures%20and%0Apathological%20conditions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Amedical%20image%20synthesis%20model%20that%20leverages%20fine-grained%20image-text%20alignment%0Aand%20anatomy-pathology%20prompts%20to%20generate%20highly%20detailed%20and%20accurate%0Asynthetic%20medical%20images.%20Our%20method%20integrates%20advanced%20natural%20language%0Aprocessing%20techniques%20with%20image%20generative%20modeling%2C%20enabling%20precise%0Aalignment%20between%20descriptive%20text%20prompts%20and%20the%20synthesized%20images%27%0Aanatomical%20and%20pathological%20details.%20The%20proposed%20approach%20consists%20of%20two%20key%0Acomponents%3A%20an%20anatomy-pathology%20prompting%20module%20and%20a%20fine-grained%0Aalignment-based%20synthesis%20module.%20The%20anatomy-pathology%20prompting%20module%0Aautomatically%20generates%20descriptive%20prompts%20for%20high-quality%20medical%20images.%20To%0Afurther%20synthesize%20high-quality%20medical%20images%20from%20the%20generated%20prompts%2C%20the%0Afine-grained%20alignment-based%20synthesis%20module%20pre-defines%20a%20visual%20codebook%20for%0Athe%20radiology%20dataset%20and%20performs%20fine-grained%20alignment%20between%20the%20codebook%0Aand%20generated%20prompts%20to%20obtain%20key%20patches%20as%20visual%20clues%2C%20facilitating%0Aaccurate%20image%20synthesis.%20We%20validate%20the%20superiority%20of%20our%20method%20through%0Aexperiments%20on%20public%20chest%20X-ray%20datasets%20and%20demonstrate%20that%20our%20synthetic%0Aimages%20preserve%20accurate%20semantic%20information%2C%20making%20them%20valuable%20for%20various%0Amedical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06835v1&entry.124074799=Read"},
{"title": "Fast Minimization of Expected Logarithmic Loss via Stochastic Dual\n  Averaging", "author": "Chung-En Tsai and Hao-Chung Cheng and Yen-Huan Li", "abstract": "  Consider the problem of minimizing an expected logarithmic loss over either\nthe probability simplex or the set of quantum density matrices. This problem\nincludes tasks such as solving the Poisson inverse problem, computing the\nmaximum-likelihood estimate for quantum state tomography, and approximating\npositive semi-definite matrix permanents with the currently tightest\napproximation ratio. Although the optimization problem is convex, standard\niteration complexity guarantees for first-order methods do not directly apply\ndue to the absence of Lipschitz continuity and smoothness in the loss function.\n  In this work, we propose a stochastic first-order algorithm named $B$-sample\nstochastic dual averaging with the logarithmic barrier. For the Poisson inverse\nproblem, our algorithm attains an $\\varepsilon$-optimal solution in\n$\\smash{\\tilde{O}}(d^2/\\varepsilon^2)$ time, matching the state of the art,\nwhere $d$ denotes the dimension. When computing the maximum-likelihood estimate\nfor quantum state tomography, our algorithm yields an $\\varepsilon$-optimal\nsolution in $\\smash{\\tilde{O}}(d^3/\\varepsilon^2)$ time. This improves on the\ntime complexities of existing stochastic first-order methods by a factor of\n$d^{\\omega-2}$ and those of batch methods by a factor of $d^2$, where $\\omega$\ndenotes the matrix multiplication exponent. Numerical experiments demonstrate\nthat empirically, our algorithm outperforms existing methods with explicit\ncomplexity guarantees.\n", "link": "http://arxiv.org/abs/2311.02557v2", "date": "2024-03-11", "relevancy": 2.0867, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4305}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4113}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4102}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Fast%20Minimization%20of%20Expected%20Logarithmic%20Loss%20via%20Stochastic%20Dual%0A%20%20Averaging&body=Title%3A%20Fast%20Minimization%20of%20Expected%20Logarithmic%20Loss%20via%20Stochastic%20Dual%0A%20%20Averaging%0AAuthor%3A%20Chung-En%20Tsai%20and%20Hao-Chung%20Cheng%20and%20Yen-Huan%20Li%0AAbstract%3A%20%20%20Consider%20the%20problem%20of%20minimizing%20an%20expected%20logarithmic%20loss%20over%20either%0Athe%20probability%20simplex%20or%20the%20set%20of%20quantum%20density%20matrices.%20This%20problem%0Aincludes%20tasks%20such%20as%20solving%20the%20Poisson%20inverse%20problem%2C%20computing%20the%0Amaximum-likelihood%20estimate%20for%20quantum%20state%20tomography%2C%20and%20approximating%0Apositive%20semi-definite%20matrix%20permanents%20with%20the%20currently%20tightest%0Aapproximation%20ratio.%20Although%20the%20optimization%20problem%20is%20convex%2C%20standard%0Aiteration%20complexity%20guarantees%20for%20first-order%20methods%20do%20not%20directly%20apply%0Adue%20to%20the%20absence%20of%20Lipschitz%20continuity%20and%20smoothness%20in%20the%20loss%20function.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20stochastic%20first-order%20algorithm%20named%20%24B%24-sample%0Astochastic%20dual%20averaging%20with%20the%20logarithmic%20barrier.%20For%20the%20Poisson%20inverse%0Aproblem%2C%20our%20algorithm%20attains%20an%20%24%5Cvarepsilon%24-optimal%20solution%20in%0A%24%5Csmash%7B%5Ctilde%7BO%7D%7D%28d%5E2/%5Cvarepsilon%5E2%29%24%20time%2C%20matching%20the%20state%20of%20the%20art%2C%0Awhere%20%24d%24%20denotes%20the%20dimension.%20When%20computing%20the%20maximum-likelihood%20estimate%0Afor%20quantum%20state%20tomography%2C%20our%20algorithm%20yields%20an%20%24%5Cvarepsilon%24-optimal%0Asolution%20in%20%24%5Csmash%7B%5Ctilde%7BO%7D%7D%28d%5E3/%5Cvarepsilon%5E2%29%24%20time.%20This%20improves%20on%20the%0Atime%20complexities%20of%20existing%20stochastic%20first-order%20methods%20by%20a%20factor%20of%0A%24d%5E%7B%5Comega-2%7D%24%20and%20those%20of%20batch%20methods%20by%20a%20factor%20of%20%24d%5E2%24%2C%20where%20%24%5Comega%24%0Adenotes%20the%20matrix%20multiplication%20exponent.%20Numerical%20experiments%20demonstrate%0Athat%20empirically%2C%20our%20algorithm%20outperforms%20existing%20methods%20with%20explicit%0Acomplexity%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02557v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Minimization%20of%20Expected%20Logarithmic%20Loss%20via%20Stochastic%20Dual%0A%20%20Averaging&entry.906535625=Chung-En%20Tsai%20and%20Hao-Chung%20Cheng%20and%20Yen-Huan%20Li&entry.1292438233=%20%20Consider%20the%20problem%20of%20minimizing%20an%20expected%20logarithmic%20loss%20over%20either%0Athe%20probability%20simplex%20or%20the%20set%20of%20quantum%20density%20matrices.%20This%20problem%0Aincludes%20tasks%20such%20as%20solving%20the%20Poisson%20inverse%20problem%2C%20computing%20the%0Amaximum-likelihood%20estimate%20for%20quantum%20state%20tomography%2C%20and%20approximating%0Apositive%20semi-definite%20matrix%20permanents%20with%20the%20currently%20tightest%0Aapproximation%20ratio.%20Although%20the%20optimization%20problem%20is%20convex%2C%20standard%0Aiteration%20complexity%20guarantees%20for%20first-order%20methods%20do%20not%20directly%20apply%0Adue%20to%20the%20absence%20of%20Lipschitz%20continuity%20and%20smoothness%20in%20the%20loss%20function.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20stochastic%20first-order%20algorithm%20named%20%24B%24-sample%0Astochastic%20dual%20averaging%20with%20the%20logarithmic%20barrier.%20For%20the%20Poisson%20inverse%0Aproblem%2C%20our%20algorithm%20attains%20an%20%24%5Cvarepsilon%24-optimal%20solution%20in%0A%24%5Csmash%7B%5Ctilde%7BO%7D%7D%28d%5E2/%5Cvarepsilon%5E2%29%24%20time%2C%20matching%20the%20state%20of%20the%20art%2C%0Awhere%20%24d%24%20denotes%20the%20dimension.%20When%20computing%20the%20maximum-likelihood%20estimate%0Afor%20quantum%20state%20tomography%2C%20our%20algorithm%20yields%20an%20%24%5Cvarepsilon%24-optimal%0Asolution%20in%20%24%5Csmash%7B%5Ctilde%7BO%7D%7D%28d%5E3/%5Cvarepsilon%5E2%29%24%20time.%20This%20improves%20on%20the%0Atime%20complexities%20of%20existing%20stochastic%20first-order%20methods%20by%20a%20factor%20of%0A%24d%5E%7B%5Comega-2%7D%24%20and%20those%20of%20batch%20methods%20by%20a%20factor%20of%20%24d%5E2%24%2C%20where%20%24%5Comega%24%0Adenotes%20the%20matrix%20multiplication%20exponent.%20Numerical%20experiments%20demonstrate%0Athat%20empirically%2C%20our%20algorithm%20outperforms%20existing%20methods%20with%20explicit%0Acomplexity%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02557v2&entry.124074799=Read"},
{"title": "Real-Time Simulated Avatar from Head-Mounted Sensors", "author": "Zhengyi Luo and Jinkun Cao and Rawal Khirodkar and Alexander Winkler and Kris Kitani and Weipeng Xu", "abstract": "  We present SimXR, a method for controlling a simulated avatar from\ninformation (headset pose and cameras) obtained from AR / VR headsets. Due to\nthe challenging viewpoint of head-mounted cameras, the human body is often\nclipped out of view, making traditional image-based egocentric pose estimation\nchallenging. On the other hand, headset poses provide valuable information\nabout overall body motion, but lack fine-grained details about the hands and\nfeet. To synergize headset poses with cameras, we control a humanoid to track\nheadset movement while analyzing input images to decide body movement. When\nbody parts are seen, the movements of hands and feet will be guided by the\nimages; when unseen, the laws of physics guide the controller to generate\nplausible motion. We design an end-to-end method that does not rely on any\nintermediate representations and learns to directly map from images and headset\nposes to humanoid control signals. To train our method, we also propose a\nlarge-scale synthetic dataset created using camera configurations compatible\nwith a commercially available VR headset (Quest 2) and show promising results\non real-world captures. To demonstrate the applicability of our framework, we\nalso test it on an AR headset with a forward-facing camera.\n", "link": "http://arxiv.org/abs/2403.06862v1", "date": "2024-03-11", "relevancy": 1.6886, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5873}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5324}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Simulated%20Avatar%20from%20Head-Mounted%20Sensors&body=Title%3A%20Real-Time%20Simulated%20Avatar%20from%20Head-Mounted%20Sensors%0AAuthor%3A%20Zhengyi%20Luo%20and%20Jinkun%20Cao%20and%20Rawal%20Khirodkar%20and%20Alexander%20Winkler%20and%20Kris%20Kitani%20and%20Weipeng%20Xu%0AAbstract%3A%20%20%20We%20present%20SimXR%2C%20a%20method%20for%20controlling%20a%20simulated%20avatar%20from%0Ainformation%20%28headset%20pose%20and%20cameras%29%20obtained%20from%20AR%20/%20VR%20headsets.%20Due%20to%0Athe%20challenging%20viewpoint%20of%20head-mounted%20cameras%2C%20the%20human%20body%20is%20often%0Aclipped%20out%20of%20view%2C%20making%20traditional%20image-based%20egocentric%20pose%20estimation%0Achallenging.%20On%20the%20other%20hand%2C%20headset%20poses%20provide%20valuable%20information%0Aabout%20overall%20body%20motion%2C%20but%20lack%20fine-grained%20details%20about%20the%20hands%20and%0Afeet.%20To%20synergize%20headset%20poses%20with%20cameras%2C%20we%20control%20a%20humanoid%20to%20track%0Aheadset%20movement%20while%20analyzing%20input%20images%20to%20decide%20body%20movement.%20When%0Abody%20parts%20are%20seen%2C%20the%20movements%20of%20hands%20and%20feet%20will%20be%20guided%20by%20the%0Aimages%3B%20when%20unseen%2C%20the%20laws%20of%20physics%20guide%20the%20controller%20to%20generate%0Aplausible%20motion.%20We%20design%20an%20end-to-end%20method%20that%20does%20not%20rely%20on%20any%0Aintermediate%20representations%20and%20learns%20to%20directly%20map%20from%20images%20and%20headset%0Aposes%20to%20humanoid%20control%20signals.%20To%20train%20our%20method%2C%20we%20also%20propose%20a%0Alarge-scale%20synthetic%20dataset%20created%20using%20camera%20configurations%20compatible%0Awith%20a%20commercially%20available%20VR%20headset%20%28Quest%202%29%20and%20show%20promising%20results%0Aon%20real-world%20captures.%20To%20demonstrate%20the%20applicability%20of%20our%20framework%2C%20we%0Aalso%20test%20it%20on%20an%20AR%20headset%20with%20a%20forward-facing%20camera.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06862v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Simulated%20Avatar%20from%20Head-Mounted%20Sensors&entry.906535625=Zhengyi%20Luo%20and%20Jinkun%20Cao%20and%20Rawal%20Khirodkar%20and%20Alexander%20Winkler%20and%20Kris%20Kitani%20and%20Weipeng%20Xu&entry.1292438233=%20%20We%20present%20SimXR%2C%20a%20method%20for%20controlling%20a%20simulated%20avatar%20from%0Ainformation%20%28headset%20pose%20and%20cameras%29%20obtained%20from%20AR%20/%20VR%20headsets.%20Due%20to%0Athe%20challenging%20viewpoint%20of%20head-mounted%20cameras%2C%20the%20human%20body%20is%20often%0Aclipped%20out%20of%20view%2C%20making%20traditional%20image-based%20egocentric%20pose%20estimation%0Achallenging.%20On%20the%20other%20hand%2C%20headset%20poses%20provide%20valuable%20information%0Aabout%20overall%20body%20motion%2C%20but%20lack%20fine-grained%20details%20about%20the%20hands%20and%0Afeet.%20To%20synergize%20headset%20poses%20with%20cameras%2C%20we%20control%20a%20humanoid%20to%20track%0Aheadset%20movement%20while%20analyzing%20input%20images%20to%20decide%20body%20movement.%20When%0Abody%20parts%20are%20seen%2C%20the%20movements%20of%20hands%20and%20feet%20will%20be%20guided%20by%20the%0Aimages%3B%20when%20unseen%2C%20the%20laws%20of%20physics%20guide%20the%20controller%20to%20generate%0Aplausible%20motion.%20We%20design%20an%20end-to-end%20method%20that%20does%20not%20rely%20on%20any%0Aintermediate%20representations%20and%20learns%20to%20directly%20map%20from%20images%20and%20headset%0Aposes%20to%20humanoid%20control%20signals.%20To%20train%20our%20method%2C%20we%20also%20propose%20a%0Alarge-scale%20synthetic%20dataset%20created%20using%20camera%20configurations%20compatible%0Awith%20a%20commercially%20available%20VR%20headset%20%28Quest%202%29%20and%20show%20promising%20results%0Aon%20real-world%20captures.%20To%20demonstrate%20the%20applicability%20of%20our%20framework%2C%20we%0Aalso%20test%20it%20on%20an%20AR%20headset%20with%20a%20forward-facing%20camera.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06862v1&entry.124074799=Read"},
{"title": "DEADiff: An Efficient Stylization Diffusion Model with Disentangled\n  Representations", "author": "Tianhao Qi and Shancheng Fang and Yanze Wu and Hongtao Xie and Jiawei Liu and Lang Chen and Qian He and Yongdong Zhang", "abstract": "  The diffusion-based text-to-image model harbors immense potential in\ntransferring reference style. However, current encoder-based approaches\nsignificantly impair the text controllability of text-to-image models while\ntransferring styles. In this paper, we introduce \\textit{DEADiff} to address\nthis issue using the following two strategies: 1) a mechanism to decouple the\nstyle and semantics of reference images. The decoupled feature representations\nare first extracted by Q-Formers which are instructed by different text\ndescriptions. Then they are injected into mutually exclusive subsets of\ncross-attention layers for better disentanglement. 2) A non-reconstructive\nlearning method. The Q-Formers are trained using paired images rather than the\nidentical target, in which the reference image and the ground-truth image are\nwith the same style or semantics. We show that DEADiff attains the best visual\nstylization results and optimal balance between the text controllability\ninherent in the text-to-image model and style similarity to the reference\nimage, as demonstrated both quantitatively and qualitatively. Our project page\nis~\\href{https://tianhao-qi.github.io/DEADiff/}{https://tianhao-qi.github.io/DEADiff/}.\n", "link": "http://arxiv.org/abs/2403.06951v1", "date": "2024-03-11", "relevancy": 1.7864, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6205}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5946}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5858}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DEADiff%3A%20An%20Efficient%20Stylization%20Diffusion%20Model%20with%20Disentangled%0A%20%20Representations&body=Title%3A%20DEADiff%3A%20An%20Efficient%20Stylization%20Diffusion%20Model%20with%20Disentangled%0A%20%20Representations%0AAuthor%3A%20Tianhao%20Qi%20and%20Shancheng%20Fang%20and%20Yanze%20Wu%20and%20Hongtao%20Xie%20and%20Jiawei%20Liu%20and%20Lang%20Chen%20and%20Qian%20He%20and%20Yongdong%20Zhang%0AAbstract%3A%20%20%20The%20diffusion-based%20text-to-image%20model%20harbors%20immense%20potential%20in%0Atransferring%20reference%20style.%20However%2C%20current%20encoder-based%20approaches%0Asignificantly%20impair%20the%20text%20controllability%20of%20text-to-image%20models%20while%0Atransferring%20styles.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextit%7BDEADiff%7D%20to%20address%0Athis%20issue%20using%20the%20following%20two%20strategies%3A%201%29%20a%20mechanism%20to%20decouple%20the%0Astyle%20and%20semantics%20of%20reference%20images.%20The%20decoupled%20feature%20representations%0Aare%20first%20extracted%20by%20Q-Formers%20which%20are%20instructed%20by%20different%20text%0Adescriptions.%20Then%20they%20are%20injected%20into%20mutually%20exclusive%20subsets%20of%0Across-attention%20layers%20for%20better%20disentanglement.%202%29%20A%20non-reconstructive%0Alearning%20method.%20The%20Q-Formers%20are%20trained%20using%20paired%20images%20rather%20than%20the%0Aidentical%20target%2C%20in%20which%20the%20reference%20image%20and%20the%20ground-truth%20image%20are%0Awith%20the%20same%20style%20or%20semantics.%20We%20show%20that%20DEADiff%20attains%20the%20best%20visual%0Astylization%20results%20and%20optimal%20balance%20between%20the%20text%20controllability%0Ainherent%20in%20the%20text-to-image%20model%20and%20style%20similarity%20to%20the%20reference%0Aimage%2C%20as%20demonstrated%20both%20quantitatively%20and%20qualitatively.%20Our%20project%20page%0Ais~%5Chref%7Bhttps%3A//tianhao-qi.github.io/DEADiff/%7D%7Bhttps%3A//tianhao-qi.github.io/DEADiff/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06951v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEADiff%3A%20An%20Efficient%20Stylization%20Diffusion%20Model%20with%20Disentangled%0A%20%20Representations&entry.906535625=Tianhao%20Qi%20and%20Shancheng%20Fang%20and%20Yanze%20Wu%20and%20Hongtao%20Xie%20and%20Jiawei%20Liu%20and%20Lang%20Chen%20and%20Qian%20He%20and%20Yongdong%20Zhang&entry.1292438233=%20%20The%20diffusion-based%20text-to-image%20model%20harbors%20immense%20potential%20in%0Atransferring%20reference%20style.%20However%2C%20current%20encoder-based%20approaches%0Asignificantly%20impair%20the%20text%20controllability%20of%20text-to-image%20models%20while%0Atransferring%20styles.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextit%7BDEADiff%7D%20to%20address%0Athis%20issue%20using%20the%20following%20two%20strategies%3A%201%29%20a%20mechanism%20to%20decouple%20the%0Astyle%20and%20semantics%20of%20reference%20images.%20The%20decoupled%20feature%20representations%0Aare%20first%20extracted%20by%20Q-Formers%20which%20are%20instructed%20by%20different%20text%0Adescriptions.%20Then%20they%20are%20injected%20into%20mutually%20exclusive%20subsets%20of%0Across-attention%20layers%20for%20better%20disentanglement.%202%29%20A%20non-reconstructive%0Alearning%20method.%20The%20Q-Formers%20are%20trained%20using%20paired%20images%20rather%20than%20the%0Aidentical%20target%2C%20in%20which%20the%20reference%20image%20and%20the%20ground-truth%20image%20are%0Awith%20the%20same%20style%20or%20semantics.%20We%20show%20that%20DEADiff%20attains%20the%20best%20visual%0Astylization%20results%20and%20optimal%20balance%20between%20the%20text%20controllability%0Ainherent%20in%20the%20text-to-image%20model%20and%20style%20similarity%20to%20the%20reference%0Aimage%2C%20as%20demonstrated%20both%20quantitatively%20and%20qualitatively.%20Our%20project%20page%0Ais~%5Chref%7Bhttps%3A//tianhao-qi.github.io/DEADiff/%7D%7Bhttps%3A//tianhao-qi.github.io/DEADiff/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06951v1&entry.124074799=Read"},
{"title": "A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with\n  an Arbitrary Opponent", "author": "Mehdi Jafarnia-Jahromi and Rahul Jain and Ashutosh Nayyar", "abstract": "  In this paper, we propose Posterior Sampling Reinforcement Learning for\nZero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that\nachieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon\nzero-sum stochastic games with average-reward criterion. Here $H$ is an upper\nbound on the span of the bias function, $S$ is the number of states, $A$ is the\nnumber of joint actions and $T$ is the horizon. We consider the online setting\nwhere the opponent can not be controlled and can take any arbitrary\ntime-adaptive history-dependent strategy. Our regret bound improves on the best\nexisting regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the\nsame assumption and matches the theoretical lower bound in $T$.\n", "link": "http://arxiv.org/abs/2109.03396v3", "date": "2024-03-11", "relevancy": 1.6953, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4706}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4154}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4135}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Bayesian%20Learning%20Algorithm%20for%20Unknown%20Zero-sum%20Stochastic%20Games%20with%0A%20%20an%20Arbitrary%20Opponent&body=Title%3A%20A%20Bayesian%20Learning%20Algorithm%20for%20Unknown%20Zero-sum%20Stochastic%20Games%20with%0A%20%20an%20Arbitrary%20Opponent%0AAuthor%3A%20Mehdi%20Jafarnia-Jahromi%20and%20Rahul%20Jain%20and%20Ashutosh%20Nayyar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Posterior%20Sampling%20Reinforcement%20Learning%20for%0AZero-sum%20Stochastic%20Games%20%28PSRL-ZSG%29%2C%20the%20first%20online%20learning%20algorithm%20that%0Aachieves%20Bayesian%20regret%20bound%20of%20%24O%28HS%5Csqrt%7BAT%7D%29%24%20in%20the%20infinite-horizon%0Azero-sum%20stochastic%20games%20with%20average-reward%20criterion.%20Here%20%24H%24%20is%20an%20upper%0Abound%20on%20the%20span%20of%20the%20bias%20function%2C%20%24S%24%20is%20the%20number%20of%20states%2C%20%24A%24%20is%20the%0Anumber%20of%20joint%20actions%20and%20%24T%24%20is%20the%20horizon.%20We%20consider%20the%20online%20setting%0Awhere%20the%20opponent%20can%20not%20be%20controlled%20and%20can%20take%20any%20arbitrary%0Atime-adaptive%20history-dependent%20strategy.%20Our%20regret%20bound%20improves%20on%20the%20best%0Aexisting%20regret%20bound%20of%20%24O%28%5Csqrt%5B3%5D%7BDS%5E2AT%5E2%7D%29%24%20by%20Wei%20et%20al.%20%282017%29%20under%20the%0Asame%20assumption%20and%20matches%20the%20theoretical%20lower%20bound%20in%20%24T%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.03396v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bayesian%20Learning%20Algorithm%20for%20Unknown%20Zero-sum%20Stochastic%20Games%20with%0A%20%20an%20Arbitrary%20Opponent&entry.906535625=Mehdi%20Jafarnia-Jahromi%20and%20Rahul%20Jain%20and%20Ashutosh%20Nayyar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Posterior%20Sampling%20Reinforcement%20Learning%20for%0AZero-sum%20Stochastic%20Games%20%28PSRL-ZSG%29%2C%20the%20first%20online%20learning%20algorithm%20that%0Aachieves%20Bayesian%20regret%20bound%20of%20%24O%28HS%5Csqrt%7BAT%7D%29%24%20in%20the%20infinite-horizon%0Azero-sum%20stochastic%20games%20with%20average-reward%20criterion.%20Here%20%24H%24%20is%20an%20upper%0Abound%20on%20the%20span%20of%20the%20bias%20function%2C%20%24S%24%20is%20the%20number%20of%20states%2C%20%24A%24%20is%20the%0Anumber%20of%20joint%20actions%20and%20%24T%24%20is%20the%20horizon.%20We%20consider%20the%20online%20setting%0Awhere%20the%20opponent%20can%20not%20be%20controlled%20and%20can%20take%20any%20arbitrary%0Atime-adaptive%20history-dependent%20strategy.%20Our%20regret%20bound%20improves%20on%20the%20best%0Aexisting%20regret%20bound%20of%20%24O%28%5Csqrt%5B3%5D%7BDS%5E2AT%5E2%7D%29%24%20by%20Wei%20et%20al.%20%282017%29%20under%20the%0Asame%20assumption%20and%20matches%20the%20theoretical%20lower%20bound%20in%20%24T%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.03396v3&entry.124074799=Read"},
{"title": "Spatial features of CO2 for occupancy detection in a naturally\n  ventilated school building", "author": "Qirui Huang and Marc Syndicus and J\u00e9r\u00f4me Frisch and Christoph van Treeck", "abstract": "  Accurate occupancy information helps to improve building energy efficiency\nand occupant comfort. Occupancy detection methods based on CO2 sensors have\nreceived attention due to their low cost and low intrusiveness. In naturally\nventilated buildings, the accuracy of CO2-based occupancy detection is\ngenerally low in related studies due to the complex ventilation behavior and\nthe difficulty in measuring the actual air exchange through windows. In this\nstudy, we present two novel features for occupancy detection based on the\nspatial distribution of the CO2 concentration. After a quantitative analysis\nwith Support Vector Machine (SVM) as classifier, it was found that the accuracy\nof occupancy state detection in naturally ventilated rooms could be improved by\nup to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1\nscore 0.84) without any ventilation information. With ventilation information,\nthe accuracy reached 87.6 % (F1 score 0.89). The performance of occupancy\nquantity detection was significantly improved by up to 25.3 percentage points\nversus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44\noccupants, using only CO2-related features. Additional ventilation information\nfurther enhanced the performance to 61.8 % (RMSE 9.02 occupants). By\nincorporating spatial features, the model using only CO2-related features\nrevealed similar performance as the model containing additional ventilation\ninformation, resulting in a better low-cost occupancy detection method for\nnaturally ventilated buildings.\n", "link": "http://arxiv.org/abs/2403.06643v1", "date": "2024-03-11", "relevancy": 1.7638, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4325}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Spatial%20features%20of%20CO2%20for%20occupancy%20detection%20in%20a%20naturally%0A%20%20ventilated%20school%20building&body=Title%3A%20Spatial%20features%20of%20CO2%20for%20occupancy%20detection%20in%20a%20naturally%0A%20%20ventilated%20school%20building%0AAuthor%3A%20Qirui%20Huang%20and%20Marc%20Syndicus%20and%20J%C3%A9r%C3%B4me%20Frisch%20and%20Christoph%20van%20Treeck%0AAbstract%3A%20%20%20Accurate%20occupancy%20information%20helps%20to%20improve%20building%20energy%20efficiency%0Aand%20occupant%20comfort.%20Occupancy%20detection%20methods%20based%20on%20CO2%20sensors%20have%0Areceived%20attention%20due%20to%20their%20low%20cost%20and%20low%20intrusiveness.%20In%20naturally%0Aventilated%20buildings%2C%20the%20accuracy%20of%20CO2-based%20occupancy%20detection%20is%0Agenerally%20low%20in%20related%20studies%20due%20to%20the%20complex%20ventilation%20behavior%20and%0Athe%20difficulty%20in%20measuring%20the%20actual%20air%20exchange%20through%20windows.%20In%20this%0Astudy%2C%20we%20present%20two%20novel%20features%20for%20occupancy%20detection%20based%20on%20the%0Aspatial%20distribution%20of%20the%20CO2%20concentration.%20After%20a%20quantitative%20analysis%0Awith%20Support%20Vector%20Machine%20%28SVM%29%20as%20classifier%2C%20it%20was%20found%20that%20the%20accuracy%0Aof%20occupancy%20state%20detection%20in%20naturally%20ventilated%20rooms%20could%20be%20improved%20by%0Aup%20to%2014.8%20percentage%20points%20compared%20to%20the%20baseline%2C%20reaching%2083.2%20%25%20%28F1%0Ascore%200.84%29%20without%20any%20ventilation%20information.%20With%20ventilation%20information%2C%0Athe%20accuracy%20reached%2087.6%20%25%20%28F1%20score%200.89%29.%20The%20performance%20of%20occupancy%0Aquantity%20detection%20was%20significantly%20improved%20by%20up%20to%2025.3%20percentage%20points%0Aversus%20baseline%2C%20reaching%2056%20%25%2C%20with%20root%20mean%20square%20error%20%28RMSE%29%20of%2011.44%0Aoccupants%2C%20using%20only%20CO2-related%20features.%20Additional%20ventilation%20information%0Afurther%20enhanced%20the%20performance%20to%2061.8%20%25%20%28RMSE%209.02%20occupants%29.%20By%0Aincorporating%20spatial%20features%2C%20the%20model%20using%20only%20CO2-related%20features%0Arevealed%20similar%20performance%20as%20the%20model%20containing%20additional%20ventilation%0Ainformation%2C%20resulting%20in%20a%20better%20low-cost%20occupancy%20detection%20method%20for%0Anaturally%20ventilated%20buildings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06643v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20features%20of%20CO2%20for%20occupancy%20detection%20in%20a%20naturally%0A%20%20ventilated%20school%20building&entry.906535625=Qirui%20Huang%20and%20Marc%20Syndicus%20and%20J%C3%A9r%C3%B4me%20Frisch%20and%20Christoph%20van%20Treeck&entry.1292438233=%20%20Accurate%20occupancy%20information%20helps%20to%20improve%20building%20energy%20efficiency%0Aand%20occupant%20comfort.%20Occupancy%20detection%20methods%20based%20on%20CO2%20sensors%20have%0Areceived%20attention%20due%20to%20their%20low%20cost%20and%20low%20intrusiveness.%20In%20naturally%0Aventilated%20buildings%2C%20the%20accuracy%20of%20CO2-based%20occupancy%20detection%20is%0Agenerally%20low%20in%20related%20studies%20due%20to%20the%20complex%20ventilation%20behavior%20and%0Athe%20difficulty%20in%20measuring%20the%20actual%20air%20exchange%20through%20windows.%20In%20this%0Astudy%2C%20we%20present%20two%20novel%20features%20for%20occupancy%20detection%20based%20on%20the%0Aspatial%20distribution%20of%20the%20CO2%20concentration.%20After%20a%20quantitative%20analysis%0Awith%20Support%20Vector%20Machine%20%28SVM%29%20as%20classifier%2C%20it%20was%20found%20that%20the%20accuracy%0Aof%20occupancy%20state%20detection%20in%20naturally%20ventilated%20rooms%20could%20be%20improved%20by%0Aup%20to%2014.8%20percentage%20points%20compared%20to%20the%20baseline%2C%20reaching%2083.2%20%25%20%28F1%0Ascore%200.84%29%20without%20any%20ventilation%20information.%20With%20ventilation%20information%2C%0Athe%20accuracy%20reached%2087.6%20%25%20%28F1%20score%200.89%29.%20The%20performance%20of%20occupancy%0Aquantity%20detection%20was%20significantly%20improved%20by%20up%20to%2025.3%20percentage%20points%0Aversus%20baseline%2C%20reaching%2056%20%25%2C%20with%20root%20mean%20square%20error%20%28RMSE%29%20of%2011.44%0Aoccupants%2C%20using%20only%20CO2-related%20features.%20Additional%20ventilation%20information%0Afurther%20enhanced%20the%20performance%20to%2061.8%20%25%20%28RMSE%209.02%20occupants%29.%20By%0Aincorporating%20spatial%20features%2C%20the%20model%20using%20only%20CO2-related%20features%0Arevealed%20similar%20performance%20as%20the%20model%20containing%20additional%20ventilation%0Ainformation%2C%20resulting%20in%20a%20better%20low-cost%20occupancy%20detection%20method%20for%0Anaturally%20ventilated%20buildings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06643v1&entry.124074799=Read"},
{"title": "Elephants Never Forget: Testing Language Models for Memorization of\n  Tabular Data", "author": "Sebastian Bordt and Harsha Nori and Rich Caruana", "abstract": "  While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Starting with simple qualitative tests for whether an LLM knows\nthe names and values of features, we introduce a variety of different\ntechniques to assess the degrees of contamination, including statistical tests\nfor conditional distribution modeling and four tests that identify\nmemorization. Our investigation reveals that LLMs are pre-trained on many\npopular tabular datasets. This exposure can lead to invalid performance\nevaluation on downstream tasks because the LLMs have, in effect, been fit to\nthe test set. Interestingly, we also identify a regime where the language model\nreproduces important statistics of the data, but fails to reproduce the dataset\nverbatim. On these datasets, although seen during training, good performance on\ndownstream tasks might not be due to overfitting. Our findings underscore the\nneed for ensuring data integrity in machine learning tasks with LLMs. To\nfacilitate future research, we release an open-source tool that can perform\nvarious tests for memorization\n\\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.\n", "link": "http://arxiv.org/abs/2403.06644v1", "date": "2024-03-11", "relevancy": 1.8948, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.465}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Elephants%20Never%20Forget%3A%20Testing%20Language%20Models%20for%20Memorization%20of%0A%20%20Tabular%20Data&body=Title%3A%20Elephants%20Never%20Forget%3A%20Testing%20Language%20Models%20for%20Memorization%20of%0A%20%20Tabular%20Data%0AAuthor%3A%20Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Rich%20Caruana%0AAbstract%3A%20%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Starting%20with%20simple%20qualitative%20tests%20for%20whether%20an%20LLM%20knows%0Athe%20names%20and%20values%20of%20features%2C%20we%20introduce%20a%20variety%20of%20different%0Atechniques%20to%20assess%20the%20degrees%20of%20contamination%2C%20including%20statistical%20tests%0Afor%20conditional%20distribution%20modeling%20and%20four%20tests%20that%20identify%0Amemorization.%20Our%20investigation%20reveals%20that%20LLMs%20are%20pre-trained%20on%20many%0Apopular%20tabular%20datasets.%20This%20exposure%20can%20lead%20to%20invalid%20performance%0Aevaluation%20on%20downstream%20tasks%20because%20the%20LLMs%20have%2C%20in%20effect%2C%20been%20fit%20to%0Athe%20test%20set.%20Interestingly%2C%20we%20also%20identify%20a%20regime%20where%20the%20language%20model%0Areproduces%20important%20statistics%20of%20the%20data%2C%20but%20fails%20to%20reproduce%20the%20dataset%0Averbatim.%20On%20these%20datasets%2C%20although%20seen%20during%20training%2C%20good%20performance%20on%0Adownstream%20tasks%20might%20not%20be%20due%20to%20overfitting.%20Our%20findings%20underscore%20the%0Aneed%20for%20ensuring%20data%20integrity%20in%20machine%20learning%20tasks%20with%20LLMs.%20To%0Afacilitate%20future%20research%2C%20we%20release%20an%20open-source%20tool%20that%20can%20perform%0Avarious%20tests%20for%20memorization%0A%5Curl%7Bhttps%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elephants%20Never%20Forget%3A%20Testing%20Language%20Models%20for%20Memorization%20of%0A%20%20Tabular%20Data&entry.906535625=Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Rich%20Caruana&entry.1292438233=%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Starting%20with%20simple%20qualitative%20tests%20for%20whether%20an%20LLM%20knows%0Athe%20names%20and%20values%20of%20features%2C%20we%20introduce%20a%20variety%20of%20different%0Atechniques%20to%20assess%20the%20degrees%20of%20contamination%2C%20including%20statistical%20tests%0Afor%20conditional%20distribution%20modeling%20and%20four%20tests%20that%20identify%0Amemorization.%20Our%20investigation%20reveals%20that%20LLMs%20are%20pre-trained%20on%20many%0Apopular%20tabular%20datasets.%20This%20exposure%20can%20lead%20to%20invalid%20performance%0Aevaluation%20on%20downstream%20tasks%20because%20the%20LLMs%20have%2C%20in%20effect%2C%20been%20fit%20to%0Athe%20test%20set.%20Interestingly%2C%20we%20also%20identify%20a%20regime%20where%20the%20language%20model%0Areproduces%20important%20statistics%20of%20the%20data%2C%20but%20fails%20to%20reproduce%20the%20dataset%0Averbatim.%20On%20these%20datasets%2C%20although%20seen%20during%20training%2C%20good%20performance%20on%0Adownstream%20tasks%20might%20not%20be%20due%20to%20overfitting.%20Our%20findings%20underscore%20the%0Aneed%20for%20ensuring%20data%20integrity%20in%20machine%20learning%20tasks%20with%20LLMs.%20To%0Afacilitate%20future%20research%2C%20we%20release%20an%20open-source%20tool%20that%20can%20perform%0Avarious%20tests%20for%20memorization%0A%5Curl%7Bhttps%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06644v1&entry.124074799=Read"},
{"title": "The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism,\n  AI, and Health in Africa", "author": "Mercy Asiedu and Awa Dieng and Iskandar Haykel and Negar Rostamzadeh and Stephen Pfohl and Chirag Nagpal and Maria Nagawa and Abigail Oppong and Sanmi Koyejo and Katherine Heller", "abstract": "  With growing application of machine learning (ML) technologies in healthcare,\nthere have been calls for developing techniques to understand and mitigate\nbiases these systems may exhibit. Fair-ness considerations in the development\nof ML-based solutions for health have particular implications for Africa, which\nalready faces inequitable power imbalances between the Global North and\nSouth.This paper seeks to explore fairness for global health, with Africa as a\ncase study. We conduct a scoping review to propose axes of disparities for\nfairness consideration in the African context and delineate where they may come\ninto play in different ML-enabled medical modalities. We then conduct\nqualitative research studies with 672 general population study participants and\n28 experts inML, health, and policy focused on Africa to obtain corroborative\nevidence on the proposed axes of disparities. Our analysis focuses on\ncolonialism as the attribute of interest and examines the interplay between\nartificial intelligence (AI), health, and colonialism. Among the pre-identified\nattributes, we found that colonial history, country of origin, and national\nincome level were specific axes of disparities that participants believed would\ncause an AI system to be biased.However, there was also divergence of opinion\nbetween experts and general population participants. Whereas experts generally\nexpressed a shared view about the relevance of colonial history for the\ndevelopment and implementation of AI technologies in Africa, the majority of\nthe general population participants surveyed did not think there was a direct\nlink between AI and colonialism. Based on these findings, we provide practical\nrecommendations for developing fairness-aware ML solutions for health in\nAfrica.\n", "link": "http://arxiv.org/abs/2403.03357v2", "date": "2024-03-11", "relevancy": 1.3716, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.345}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3421}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3411}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Case%20for%20Globalizing%20Fairness%3A%20A%20Mixed%20Methods%20Study%20on%20Colonialism%2C%0A%20%20AI%2C%20and%20Health%20in%20Africa&body=Title%3A%20The%20Case%20for%20Globalizing%20Fairness%3A%20A%20Mixed%20Methods%20Study%20on%20Colonialism%2C%0A%20%20AI%2C%20and%20Health%20in%20Africa%0AAuthor%3A%20Mercy%20Asiedu%20and%20Awa%20Dieng%20and%20Iskandar%20Haykel%20and%20Negar%20Rostamzadeh%20and%20Stephen%20Pfohl%20and%20Chirag%20Nagpal%20and%20Maria%20Nagawa%20and%20Abigail%20Oppong%20and%20Sanmi%20Koyejo%20and%20Katherine%20Heller%0AAbstract%3A%20%20%20With%20growing%20application%20of%20machine%20learning%20%28ML%29%20technologies%20in%20healthcare%2C%0Athere%20have%20been%20calls%20for%20developing%20techniques%20to%20understand%20and%20mitigate%0Abiases%20these%20systems%20may%20exhibit.%20Fair-ness%20considerations%20in%20the%20development%0Aof%20ML-based%20solutions%20for%20health%20have%20particular%20implications%20for%20Africa%2C%20which%0Aalready%20faces%20inequitable%20power%20imbalances%20between%20the%20Global%20North%20and%0ASouth.This%20paper%20seeks%20to%20explore%20fairness%20for%20global%20health%2C%20with%20Africa%20as%20a%0Acase%20study.%20We%20conduct%20a%20scoping%20review%20to%20propose%20axes%20of%20disparities%20for%0Afairness%20consideration%20in%20the%20African%20context%20and%20delineate%20where%20they%20may%20come%0Ainto%20play%20in%20different%20ML-enabled%20medical%20modalities.%20We%20then%20conduct%0Aqualitative%20research%20studies%20with%20672%20general%20population%20study%20participants%20and%0A28%20experts%20inML%2C%20health%2C%20and%20policy%20focused%20on%20Africa%20to%20obtain%20corroborative%0Aevidence%20on%20the%20proposed%20axes%20of%20disparities.%20Our%20analysis%20focuses%20on%0Acolonialism%20as%20the%20attribute%20of%20interest%20and%20examines%20the%20interplay%20between%0Aartificial%20intelligence%20%28AI%29%2C%20health%2C%20and%20colonialism.%20Among%20the%20pre-identified%0Aattributes%2C%20we%20found%20that%20colonial%20history%2C%20country%20of%20origin%2C%20and%20national%0Aincome%20level%20were%20specific%20axes%20of%20disparities%20that%20participants%20believed%20would%0Acause%20an%20AI%20system%20to%20be%20biased.However%2C%20there%20was%20also%20divergence%20of%20opinion%0Abetween%20experts%20and%20general%20population%20participants.%20Whereas%20experts%20generally%0Aexpressed%20a%20shared%20view%20about%20the%20relevance%20of%20colonial%20history%20for%20the%0Adevelopment%20and%20implementation%20of%20AI%20technologies%20in%20Africa%2C%20the%20majority%20of%0Athe%20general%20population%20participants%20surveyed%20did%20not%20think%20there%20was%20a%20direct%0Alink%20between%20AI%20and%20colonialism.%20Based%20on%20these%20findings%2C%20we%20provide%20practical%0Arecommendations%20for%20developing%20fairness-aware%20ML%20solutions%20for%20health%20in%0AAfrica.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03357v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Case%20for%20Globalizing%20Fairness%3A%20A%20Mixed%20Methods%20Study%20on%20Colonialism%2C%0A%20%20AI%2C%20and%20Health%20in%20Africa&entry.906535625=Mercy%20Asiedu%20and%20Awa%20Dieng%20and%20Iskandar%20Haykel%20and%20Negar%20Rostamzadeh%20and%20Stephen%20Pfohl%20and%20Chirag%20Nagpal%20and%20Maria%20Nagawa%20and%20Abigail%20Oppong%20and%20Sanmi%20Koyejo%20and%20Katherine%20Heller&entry.1292438233=%20%20With%20growing%20application%20of%20machine%20learning%20%28ML%29%20technologies%20in%20healthcare%2C%0Athere%20have%20been%20calls%20for%20developing%20techniques%20to%20understand%20and%20mitigate%0Abiases%20these%20systems%20may%20exhibit.%20Fair-ness%20considerations%20in%20the%20development%0Aof%20ML-based%20solutions%20for%20health%20have%20particular%20implications%20for%20Africa%2C%20which%0Aalready%20faces%20inequitable%20power%20imbalances%20between%20the%20Global%20North%20and%0ASouth.This%20paper%20seeks%20to%20explore%20fairness%20for%20global%20health%2C%20with%20Africa%20as%20a%0Acase%20study.%20We%20conduct%20a%20scoping%20review%20to%20propose%20axes%20of%20disparities%20for%0Afairness%20consideration%20in%20the%20African%20context%20and%20delineate%20where%20they%20may%20come%0Ainto%20play%20in%20different%20ML-enabled%20medical%20modalities.%20We%20then%20conduct%0Aqualitative%20research%20studies%20with%20672%20general%20population%20study%20participants%20and%0A28%20experts%20inML%2C%20health%2C%20and%20policy%20focused%20on%20Africa%20to%20obtain%20corroborative%0Aevidence%20on%20the%20proposed%20axes%20of%20disparities.%20Our%20analysis%20focuses%20on%0Acolonialism%20as%20the%20attribute%20of%20interest%20and%20examines%20the%20interplay%20between%0Aartificial%20intelligence%20%28AI%29%2C%20health%2C%20and%20colonialism.%20Among%20the%20pre-identified%0Aattributes%2C%20we%20found%20that%20colonial%20history%2C%20country%20of%20origin%2C%20and%20national%0Aincome%20level%20were%20specific%20axes%20of%20disparities%20that%20participants%20believed%20would%0Acause%20an%20AI%20system%20to%20be%20biased.However%2C%20there%20was%20also%20divergence%20of%20opinion%0Abetween%20experts%20and%20general%20population%20participants.%20Whereas%20experts%20generally%0Aexpressed%20a%20shared%20view%20about%20the%20relevance%20of%20colonial%20history%20for%20the%0Adevelopment%20and%20implementation%20of%20AI%20technologies%20in%20Africa%2C%20the%20majority%20of%0Athe%20general%20population%20participants%20surveyed%20did%20not%20think%20there%20was%20a%20direct%0Alink%20between%20AI%20and%20colonialism.%20Based%20on%20these%20findings%2C%20we%20provide%20practical%0Arecommendations%20for%20developing%20fairness-aware%20ML%20solutions%20for%20health%20in%0AAfrica.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03357v2&entry.124074799=Read"},
{"title": "Parameter Efficient Multi-task Model Fusion with Partial Linearization", "author": "Anke Tang and Li Shen and Yong Luo and Yibing Zhan and Han Hu and Bo Du and Yixin Chen and Dacheng Tao", "abstract": "  Large pre-trained models have enabled significant advances in machine\nlearning and served as foundation components. Model fusion methods, such as\ntask arithmetic, have been proven to be powerful and scalable to incorporate\nfine-tuned weights from different tasks into a multi-task model. However,\nefficiently fine-tuning large pre-trained models on multiple downstream tasks\nremains challenging, leading to inefficient multi-task model fusion. In this\nwork, we propose a novel method to improve multi-task fusion for\nparameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically,\nour approach partially linearizes only the adapter modules and applies task\narithmetic over the linearized adapters. This allows us to leverage the the\nadvantages of model fusion over linearized fine-tuning, while still performing\nfine-tuning and inference efficiently. We demonstrate that our partial\nlinearization technique enables a more effective fusion of multiple tasks into\na single model, outperforming standard adapter tuning and task arithmetic\nalone. Experimental results demonstrate the capabilities of our proposed\npartial linearization technique to effectively construct unified multi-task\nmodels via the fusion of fine-tuned task vectors. We evaluate performance over\nan increasing number of tasks and find that our approach outperforms standard\nparameter-efficient fine-tuning techniques. The results highlight the benefits\nof partial linearization for scalable and efficient multi-task model fusion.\nThe code is available at https://github.com/tanganke/peta\n", "link": "http://arxiv.org/abs/2310.04742v3", "date": "2024-03-11", "relevancy": 1.543, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5081}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4871}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Parameter%20Efficient%20Multi-task%20Model%20Fusion%20with%20Partial%20Linearization&body=Title%3A%20Parameter%20Efficient%20Multi-task%20Model%20Fusion%20with%20Partial%20Linearization%0AAuthor%3A%20Anke%20Tang%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Yibing%20Zhan%20and%20Han%20Hu%20and%20Bo%20Du%20and%20Yixin%20Chen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20have%20enabled%20significant%20advances%20in%20machine%0Alearning%20and%20served%20as%20foundation%20components.%20Model%20fusion%20methods%2C%20such%20as%0Atask%20arithmetic%2C%20have%20been%20proven%20to%20be%20powerful%20and%20scalable%20to%20incorporate%0Afine-tuned%20weights%20from%20different%20tasks%20into%20a%20multi-task%20model.%20However%2C%0Aefficiently%20fine-tuning%20large%20pre-trained%20models%20on%20multiple%20downstream%20tasks%0Aremains%20challenging%2C%20leading%20to%20inefficient%20multi-task%20model%20fusion.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20method%20to%20improve%20multi-task%20fusion%20for%0Aparameter-efficient%20fine-tuning%20techniques%20like%20LoRA%20fine-tuning.%20Specifically%2C%0Aour%20approach%20partially%20linearizes%20only%20the%20adapter%20modules%20and%20applies%20task%0Aarithmetic%20over%20the%20linearized%20adapters.%20This%20allows%20us%20to%20leverage%20the%20the%0Aadvantages%20of%20model%20fusion%20over%20linearized%20fine-tuning%2C%20while%20still%20performing%0Afine-tuning%20and%20inference%20efficiently.%20We%20demonstrate%20that%20our%20partial%0Alinearization%20technique%20enables%20a%20more%20effective%20fusion%20of%20multiple%20tasks%20into%0Aa%20single%20model%2C%20outperforming%20standard%20adapter%20tuning%20and%20task%20arithmetic%0Aalone.%20Experimental%20results%20demonstrate%20the%20capabilities%20of%20our%20proposed%0Apartial%20linearization%20technique%20to%20effectively%20construct%20unified%20multi-task%0Amodels%20via%20the%20fusion%20of%20fine-tuned%20task%20vectors.%20We%20evaluate%20performance%20over%0Aan%20increasing%20number%20of%20tasks%20and%20find%20that%20our%20approach%20outperforms%20standard%0Aparameter-efficient%20fine-tuning%20techniques.%20The%20results%20highlight%20the%20benefits%0Aof%20partial%20linearization%20for%20scalable%20and%20efficient%20multi-task%20model%20fusion.%0AThe%20code%20is%20available%20at%20https%3A//github.com/tanganke/peta%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04742v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Efficient%20Multi-task%20Model%20Fusion%20with%20Partial%20Linearization&entry.906535625=Anke%20Tang%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Yibing%20Zhan%20and%20Han%20Hu%20and%20Bo%20Du%20and%20Yixin%20Chen%20and%20Dacheng%20Tao&entry.1292438233=%20%20Large%20pre-trained%20models%20have%20enabled%20significant%20advances%20in%20machine%0Alearning%20and%20served%20as%20foundation%20components.%20Model%20fusion%20methods%2C%20such%20as%0Atask%20arithmetic%2C%20have%20been%20proven%20to%20be%20powerful%20and%20scalable%20to%20incorporate%0Afine-tuned%20weights%20from%20different%20tasks%20into%20a%20multi-task%20model.%20However%2C%0Aefficiently%20fine-tuning%20large%20pre-trained%20models%20on%20multiple%20downstream%20tasks%0Aremains%20challenging%2C%20leading%20to%20inefficient%20multi-task%20model%20fusion.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20method%20to%20improve%20multi-task%20fusion%20for%0Aparameter-efficient%20fine-tuning%20techniques%20like%20LoRA%20fine-tuning.%20Specifically%2C%0Aour%20approach%20partially%20linearizes%20only%20the%20adapter%20modules%20and%20applies%20task%0Aarithmetic%20over%20the%20linearized%20adapters.%20This%20allows%20us%20to%20leverage%20the%20the%0Aadvantages%20of%20model%20fusion%20over%20linearized%20fine-tuning%2C%20while%20still%20performing%0Afine-tuning%20and%20inference%20efficiently.%20We%20demonstrate%20that%20our%20partial%0Alinearization%20technique%20enables%20a%20more%20effective%20fusion%20of%20multiple%20tasks%20into%0Aa%20single%20model%2C%20outperforming%20standard%20adapter%20tuning%20and%20task%20arithmetic%0Aalone.%20Experimental%20results%20demonstrate%20the%20capabilities%20of%20our%20proposed%0Apartial%20linearization%20technique%20to%20effectively%20construct%20unified%20multi-task%0Amodels%20via%20the%20fusion%20of%20fine-tuned%20task%20vectors.%20We%20evaluate%20performance%20over%0Aan%20increasing%20number%20of%20tasks%20and%20find%20that%20our%20approach%20outperforms%20standard%0Aparameter-efficient%20fine-tuning%20techniques.%20The%20results%20highlight%20the%20benefits%0Aof%20partial%20linearization%20for%20scalable%20and%20efficient%20multi-task%20model%20fusion.%0AThe%20code%20is%20available%20at%20https%3A//github.com/tanganke/peta%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04742v3&entry.124074799=Read"},
{"title": "RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained\n  Scenarios", "author": "Xiaolei Liu and Ming Yi and Kangyi Ding and Bangzhou Xin and Yixiao Xu and Li Yan and Chao Shen", "abstract": "  Learning-based systems have been demonstrated to be vulnerable to backdoor\nattacks, wherein malicious users manipulate model performance by injecting\nbackdoors into the target model and activating them with specific triggers.\nPrevious backdoor attack methods primarily focused on two key metrics: attack\nsuccess rate and stealthiness. However, these methods often necessitate\nsignificant privileges over the target model, such as control over the training\nprocess, making them challenging to implement in real-world scenarios.\nMoreover, the robustness of existing backdoor attacks is not guaranteed, as\nthey prove sensitive to defenses such as image augmentations and model\ndistillation. In this paper, we address these two limitations and introduce\nRSBA (Robust Statistical Backdoor Attack under Privilege-constrained\nScenarios). The key insight of RSBA is that statistical features can naturally\ndivide images into different groups, offering a potential implementation of\ntriggers. This type of trigger is more robust than manually designed ones, as\nit is widely distributed in normal images. By leveraging these statistical\ntriggers, RSBA enables attackers to conduct black-box attacks by solely\npoisoning the labels or the images. We empirically and theoretically\ndemonstrate the robustness of RSBA against image augmentations and model\ndistillation. Experimental results show that RSBA achieves a 99.83\\% attack\nsuccess rate in black-box scenarios. Remarkably, it maintains a high success\nrate even after model distillation, where attackers lack access to the training\ndataset of the student model (1.39\\% success rate for baseline methods on\naverage).\n", "link": "http://arxiv.org/abs/2304.10985v2", "date": "2024-03-11", "relevancy": 1.4418, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4952}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4686}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4562}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20RSBA%3A%20Robust%20Statistical%20Backdoor%20Attack%20under%20Privilege-Constrained%0A%20%20Scenarios&body=Title%3A%20RSBA%3A%20Robust%20Statistical%20Backdoor%20Attack%20under%20Privilege-Constrained%0A%20%20Scenarios%0AAuthor%3A%20Xiaolei%20Liu%20and%20Ming%20Yi%20and%20Kangyi%20Ding%20and%20Bangzhou%20Xin%20and%20Yixiao%20Xu%20and%20Li%20Yan%20and%20Chao%20Shen%0AAbstract%3A%20%20%20Learning-based%20systems%20have%20been%20demonstrated%20to%20be%20vulnerable%20to%20backdoor%0Aattacks%2C%20wherein%20malicious%20users%20manipulate%20model%20performance%20by%20injecting%0Abackdoors%20into%20the%20target%20model%20and%20activating%20them%20with%20specific%20triggers.%0APrevious%20backdoor%20attack%20methods%20primarily%20focused%20on%20two%20key%20metrics%3A%20attack%0Asuccess%20rate%20and%20stealthiness.%20However%2C%20these%20methods%20often%20necessitate%0Asignificant%20privileges%20over%20the%20target%20model%2C%20such%20as%20control%20over%20the%20training%0Aprocess%2C%20making%20them%20challenging%20to%20implement%20in%20real-world%20scenarios.%0AMoreover%2C%20the%20robustness%20of%20existing%20backdoor%20attacks%20is%20not%20guaranteed%2C%20as%0Athey%20prove%20sensitive%20to%20defenses%20such%20as%20image%20augmentations%20and%20model%0Adistillation.%20In%20this%20paper%2C%20we%20address%20these%20two%20limitations%20and%20introduce%0ARSBA%20%28Robust%20Statistical%20Backdoor%20Attack%20under%20Privilege-constrained%0AScenarios%29.%20The%20key%20insight%20of%20RSBA%20is%20that%20statistical%20features%20can%20naturally%0Adivide%20images%20into%20different%20groups%2C%20offering%20a%20potential%20implementation%20of%0Atriggers.%20This%20type%20of%20trigger%20is%20more%20robust%20than%20manually%20designed%20ones%2C%20as%0Ait%20is%20widely%20distributed%20in%20normal%20images.%20By%20leveraging%20these%20statistical%0Atriggers%2C%20RSBA%20enables%20attackers%20to%20conduct%20black-box%20attacks%20by%20solely%0Apoisoning%20the%20labels%20or%20the%20images.%20We%20empirically%20and%20theoretically%0Ademonstrate%20the%20robustness%20of%20RSBA%20against%20image%20augmentations%20and%20model%0Adistillation.%20Experimental%20results%20show%20that%20RSBA%20achieves%20a%2099.83%5C%25%20attack%0Asuccess%20rate%20in%20black-box%20scenarios.%20Remarkably%2C%20it%20maintains%20a%20high%20success%0Arate%20even%20after%20model%20distillation%2C%20where%20attackers%20lack%20access%20to%20the%20training%0Adataset%20of%20the%20student%20model%20%281.39%5C%25%20success%20rate%20for%20baseline%20methods%20on%0Aaverage%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.10985v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSBA%3A%20Robust%20Statistical%20Backdoor%20Attack%20under%20Privilege-Constrained%0A%20%20Scenarios&entry.906535625=Xiaolei%20Liu%20and%20Ming%20Yi%20and%20Kangyi%20Ding%20and%20Bangzhou%20Xin%20and%20Yixiao%20Xu%20and%20Li%20Yan%20and%20Chao%20Shen&entry.1292438233=%20%20Learning-based%20systems%20have%20been%20demonstrated%20to%20be%20vulnerable%20to%20backdoor%0Aattacks%2C%20wherein%20malicious%20users%20manipulate%20model%20performance%20by%20injecting%0Abackdoors%20into%20the%20target%20model%20and%20activating%20them%20with%20specific%20triggers.%0APrevious%20backdoor%20attack%20methods%20primarily%20focused%20on%20two%20key%20metrics%3A%20attack%0Asuccess%20rate%20and%20stealthiness.%20However%2C%20these%20methods%20often%20necessitate%0Asignificant%20privileges%20over%20the%20target%20model%2C%20such%20as%20control%20over%20the%20training%0Aprocess%2C%20making%20them%20challenging%20to%20implement%20in%20real-world%20scenarios.%0AMoreover%2C%20the%20robustness%20of%20existing%20backdoor%20attacks%20is%20not%20guaranteed%2C%20as%0Athey%20prove%20sensitive%20to%20defenses%20such%20as%20image%20augmentations%20and%20model%0Adistillation.%20In%20this%20paper%2C%20we%20address%20these%20two%20limitations%20and%20introduce%0ARSBA%20%28Robust%20Statistical%20Backdoor%20Attack%20under%20Privilege-constrained%0AScenarios%29.%20The%20key%20insight%20of%20RSBA%20is%20that%20statistical%20features%20can%20naturally%0Adivide%20images%20into%20different%20groups%2C%20offering%20a%20potential%20implementation%20of%0Atriggers.%20This%20type%20of%20trigger%20is%20more%20robust%20than%20manually%20designed%20ones%2C%20as%0Ait%20is%20widely%20distributed%20in%20normal%20images.%20By%20leveraging%20these%20statistical%0Atriggers%2C%20RSBA%20enables%20attackers%20to%20conduct%20black-box%20attacks%20by%20solely%0Apoisoning%20the%20labels%20or%20the%20images.%20We%20empirically%20and%20theoretically%0Ademonstrate%20the%20robustness%20of%20RSBA%20against%20image%20augmentations%20and%20model%0Adistillation.%20Experimental%20results%20show%20that%20RSBA%20achieves%20a%2099.83%5C%25%20attack%0Asuccess%20rate%20in%20black-box%20scenarios.%20Remarkably%2C%20it%20maintains%20a%20high%20success%0Arate%20even%20after%20model%20distillation%2C%20where%20attackers%20lack%20access%20to%20the%20training%0Adataset%20of%20the%20student%20model%20%281.39%5C%25%20success%20rate%20for%20baseline%20methods%20on%0Aaverage%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.10985v2&entry.124074799=Read"},
{"title": "Grid Monitoring and Protection with Continuous Point-on-Wave\n  Measurements and Generative AI", "author": "Lang Tong and Xinyi Wang and Qing Zhao", "abstract": "  Purpose This article presents a case for a next-generation grid monitoring\nand control system, leveraging recent advances in generative artificial\nintelligence (AI), machine learning, and statistical inference. Advancing\nbeyond earlier generations of wide-area monitoring systems built upon\nsupervisory control and data acquisition (SCADA) and synchrophasor\ntechnologies, we argue for a monitoring and control framework based on the\nstreaming of continuous point-on-wave (CPOW) measurements with AI-powered data\ncompression and fault detection.\n  Methods and Results: The architecture of the proposed design originates from\nthe Wiener-Kallianpur innovation representation of a random process that\ntransforms causally a stationary random process into an innovation sequence\nwith independent and identically distributed random variables. This work\npresents a generative AI approach that (i) learns an innovation autoencoder\nthat extracts innovation sequence from CPOW time series, (ii) compresses the\nCPOW streaming data with innovation autoencoder and subband coding, and (iii)\ndetects unknown faults and novel trends via nonparametric sequential hypothesis\ntesting.\n  Conclusion: This work argues that conventional monitoring using SCADA and\nphasor measurement unit (PMU) technologies is ill-suited for a future grid with\ndeep penetration of inverter-based renewable generations and distributed energy\nresources. A monitoring system based on CPOW data streaming and AI data\nanalytics should be the basic building blocks for situational awareness of a\nhighly dynamic future grid.\n", "link": "http://arxiv.org/abs/2403.06942v1", "date": "2024-03-11", "relevancy": 1.4319, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4909}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4491}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Grid%20Monitoring%20and%20Protection%20with%20Continuous%20Point-on-Wave%0A%20%20Measurements%20and%20Generative%20AI&body=Title%3A%20Grid%20Monitoring%20and%20Protection%20with%20Continuous%20Point-on-Wave%0A%20%20Measurements%20and%20Generative%20AI%0AAuthor%3A%20Lang%20Tong%20and%20Xinyi%20Wang%20and%20Qing%20Zhao%0AAbstract%3A%20%20%20Purpose%20This%20article%20presents%20a%20case%20for%20a%20next-generation%20grid%20monitoring%0Aand%20control%20system%2C%20leveraging%20recent%20advances%20in%20generative%20artificial%0Aintelligence%20%28AI%29%2C%20machine%20learning%2C%20and%20statistical%20inference.%20Advancing%0Abeyond%20earlier%20generations%20of%20wide-area%20monitoring%20systems%20built%20upon%0Asupervisory%20control%20and%20data%20acquisition%20%28SCADA%29%20and%20synchrophasor%0Atechnologies%2C%20we%20argue%20for%20a%20monitoring%20and%20control%20framework%20based%20on%20the%0Astreaming%20of%20continuous%20point-on-wave%20%28CPOW%29%20measurements%20with%20AI-powered%20data%0Acompression%20and%20fault%20detection.%0A%20%20Methods%20and%20Results%3A%20The%20architecture%20of%20the%20proposed%20design%20originates%20from%0Athe%20Wiener-Kallianpur%20innovation%20representation%20of%20a%20random%20process%20that%0Atransforms%20causally%20a%20stationary%20random%20process%20into%20an%20innovation%20sequence%0Awith%20independent%20and%20identically%20distributed%20random%20variables.%20This%20work%0Apresents%20a%20generative%20AI%20approach%20that%20%28i%29%20learns%20an%20innovation%20autoencoder%0Athat%20extracts%20innovation%20sequence%20from%20CPOW%20time%20series%2C%20%28ii%29%20compresses%20the%0ACPOW%20streaming%20data%20with%20innovation%20autoencoder%20and%20subband%20coding%2C%20and%20%28iii%29%0Adetects%20unknown%20faults%20and%20novel%20trends%20via%20nonparametric%20sequential%20hypothesis%0Atesting.%0A%20%20Conclusion%3A%20This%20work%20argues%20that%20conventional%20monitoring%20using%20SCADA%20and%0Aphasor%20measurement%20unit%20%28PMU%29%20technologies%20is%20ill-suited%20for%20a%20future%20grid%20with%0Adeep%20penetration%20of%20inverter-based%20renewable%20generations%20and%20distributed%20energy%0Aresources.%20A%20monitoring%20system%20based%20on%20CPOW%20data%20streaming%20and%20AI%20data%0Aanalytics%20should%20be%20the%20basic%20building%20blocks%20for%20situational%20awareness%20of%20a%0Ahighly%20dynamic%20future%20grid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06942v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grid%20Monitoring%20and%20Protection%20with%20Continuous%20Point-on-Wave%0A%20%20Measurements%20and%20Generative%20AI&entry.906535625=Lang%20Tong%20and%20Xinyi%20Wang%20and%20Qing%20Zhao&entry.1292438233=%20%20Purpose%20This%20article%20presents%20a%20case%20for%20a%20next-generation%20grid%20monitoring%0Aand%20control%20system%2C%20leveraging%20recent%20advances%20in%20generative%20artificial%0Aintelligence%20%28AI%29%2C%20machine%20learning%2C%20and%20statistical%20inference.%20Advancing%0Abeyond%20earlier%20generations%20of%20wide-area%20monitoring%20systems%20built%20upon%0Asupervisory%20control%20and%20data%20acquisition%20%28SCADA%29%20and%20synchrophasor%0Atechnologies%2C%20we%20argue%20for%20a%20monitoring%20and%20control%20framework%20based%20on%20the%0Astreaming%20of%20continuous%20point-on-wave%20%28CPOW%29%20measurements%20with%20AI-powered%20data%0Acompression%20and%20fault%20detection.%0A%20%20Methods%20and%20Results%3A%20The%20architecture%20of%20the%20proposed%20design%20originates%20from%0Athe%20Wiener-Kallianpur%20innovation%20representation%20of%20a%20random%20process%20that%0Atransforms%20causally%20a%20stationary%20random%20process%20into%20an%20innovation%20sequence%0Awith%20independent%20and%20identically%20distributed%20random%20variables.%20This%20work%0Apresents%20a%20generative%20AI%20approach%20that%20%28i%29%20learns%20an%20innovation%20autoencoder%0Athat%20extracts%20innovation%20sequence%20from%20CPOW%20time%20series%2C%20%28ii%29%20compresses%20the%0ACPOW%20streaming%20data%20with%20innovation%20autoencoder%20and%20subband%20coding%2C%20and%20%28iii%29%0Adetects%20unknown%20faults%20and%20novel%20trends%20via%20nonparametric%20sequential%20hypothesis%0Atesting.%0A%20%20Conclusion%3A%20This%20work%20argues%20that%20conventional%20monitoring%20using%20SCADA%20and%0Aphasor%20measurement%20unit%20%28PMU%29%20technologies%20is%20ill-suited%20for%20a%20future%20grid%20with%0Adeep%20penetration%20of%20inverter-based%20renewable%20generations%20and%20distributed%20energy%0Aresources.%20A%20monitoring%20system%20based%20on%20CPOW%20data%20streaming%20and%20AI%20data%0Aanalytics%20should%20be%20the%20basic%20building%20blocks%20for%20situational%20awareness%20of%20a%0Ahighly%20dynamic%20future%20grid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06942v1&entry.124074799=Read"},
{"title": "SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers", "author": "Stefan Baumgartner and Oliver Lang and Mario Huemer", "abstract": "  In recent years data-driven machine learning approaches have been extensively\nstudied to replace or enhance traditionally model-based processing in digital\ncommunication systems. In this work, we focus on equalization and propose a\nnovel neural network (NN-)based approach, referred to as SICNN. SICNN is\ndesigned by deep unfolding a model-based iterative soft interference\ncancellation (SIC) method. It eliminates the main disadvantages of its\nmodel-based counterpart, which suffers from high computational complexity and\nperformance degradation due to required approximations. We present different\nvariants of SICNN. SICNNv1 is specifically tailored to single carrier frequency\ndomain equalization (SC-FDE) systems, the communication system mainly regarded\nin this work. SICNNv2 is more universal and is applicable as an equalizer in\nany communication system with a block-based data transmission scheme. Moreover,\nfor both SICNNv1 and SICNNv2, we present versions with highly reduced numbers\nof learnable parameters. Another contribution of this work is a novel approach\nfor generating training datasets for NN-based equalizers, which significantly\nimproves their performance at high signal-to-noise ratios. We compare the bit\nerror ratio performance of the proposed NN-based equalizers with\nstate-of-the-art model-based and NN-based approaches, highlighting the\nsuperiority of SICNNv1 over all other methods for SC-FDE. Exemplarily, to\nemphasize its universality, SICNNv2 is additionally applied to a unique word\northogonal frequency division multiplexing (UW-OFDM) system, where it achieves\nstate-of-the-art performance. Furthermore, we present a thorough complexity\nanalysis of the proposed NN-based equalization approaches, and we investigate\nthe influence of the training set size on the performance of NN-based\nequalizers.\n", "link": "http://arxiv.org/abs/2308.12591v2", "date": "2024-03-11", "relevancy": 1.9158, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4807}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4326}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20SICNN%3A%20Soft%20Interference%20Cancellation%20Inspired%20Neural%20Network%20Equalizers&body=Title%3A%20SICNN%3A%20Soft%20Interference%20Cancellation%20Inspired%20Neural%20Network%20Equalizers%0AAuthor%3A%20Stefan%20Baumgartner%20and%20Oliver%20Lang%20and%20Mario%20Huemer%0AAbstract%3A%20%20%20In%20recent%20years%20data-driven%20machine%20learning%20approaches%20have%20been%20extensively%0Astudied%20to%20replace%20or%20enhance%20traditionally%20model-based%20processing%20in%20digital%0Acommunication%20systems.%20In%20this%20work%2C%20we%20focus%20on%20equalization%20and%20propose%20a%0Anovel%20neural%20network%20%28NN-%29based%20approach%2C%20referred%20to%20as%20SICNN.%20SICNN%20is%0Adesigned%20by%20deep%20unfolding%20a%20model-based%20iterative%20soft%20interference%0Acancellation%20%28SIC%29%20method.%20It%20eliminates%20the%20main%20disadvantages%20of%20its%0Amodel-based%20counterpart%2C%20which%20suffers%20from%20high%20computational%20complexity%20and%0Aperformance%20degradation%20due%20to%20required%20approximations.%20We%20present%20different%0Avariants%20of%20SICNN.%20SICNNv1%20is%20specifically%20tailored%20to%20single%20carrier%20frequency%0Adomain%20equalization%20%28SC-FDE%29%20systems%2C%20the%20communication%20system%20mainly%20regarded%0Ain%20this%20work.%20SICNNv2%20is%20more%20universal%20and%20is%20applicable%20as%20an%20equalizer%20in%0Aany%20communication%20system%20with%20a%20block-based%20data%20transmission%20scheme.%20Moreover%2C%0Afor%20both%20SICNNv1%20and%20SICNNv2%2C%20we%20present%20versions%20with%20highly%20reduced%20numbers%0Aof%20learnable%20parameters.%20Another%20contribution%20of%20this%20work%20is%20a%20novel%20approach%0Afor%20generating%20training%20datasets%20for%20NN-based%20equalizers%2C%20which%20significantly%0Aimproves%20their%20performance%20at%20high%20signal-to-noise%20ratios.%20We%20compare%20the%20bit%0Aerror%20ratio%20performance%20of%20the%20proposed%20NN-based%20equalizers%20with%0Astate-of-the-art%20model-based%20and%20NN-based%20approaches%2C%20highlighting%20the%0Asuperiority%20of%20SICNNv1%20over%20all%20other%20methods%20for%20SC-FDE.%20Exemplarily%2C%20to%0Aemphasize%20its%20universality%2C%20SICNNv2%20is%20additionally%20applied%20to%20a%20unique%20word%0Aorthogonal%20frequency%20division%20multiplexing%20%28UW-OFDM%29%20system%2C%20where%20it%20achieves%0Astate-of-the-art%20performance.%20Furthermore%2C%20we%20present%20a%20thorough%20complexity%0Aanalysis%20of%20the%20proposed%20NN-based%20equalization%20approaches%2C%20and%20we%20investigate%0Athe%20influence%20of%20the%20training%20set%20size%20on%20the%20performance%20of%20NN-based%0Aequalizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12591v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SICNN%3A%20Soft%20Interference%20Cancellation%20Inspired%20Neural%20Network%20Equalizers&entry.906535625=Stefan%20Baumgartner%20and%20Oliver%20Lang%20and%20Mario%20Huemer&entry.1292438233=%20%20In%20recent%20years%20data-driven%20machine%20learning%20approaches%20have%20been%20extensively%0Astudied%20to%20replace%20or%20enhance%20traditionally%20model-based%20processing%20in%20digital%0Acommunication%20systems.%20In%20this%20work%2C%20we%20focus%20on%20equalization%20and%20propose%20a%0Anovel%20neural%20network%20%28NN-%29based%20approach%2C%20referred%20to%20as%20SICNN.%20SICNN%20is%0Adesigned%20by%20deep%20unfolding%20a%20model-based%20iterative%20soft%20interference%0Acancellation%20%28SIC%29%20method.%20It%20eliminates%20the%20main%20disadvantages%20of%20its%0Amodel-based%20counterpart%2C%20which%20suffers%20from%20high%20computational%20complexity%20and%0Aperformance%20degradation%20due%20to%20required%20approximations.%20We%20present%20different%0Avariants%20of%20SICNN.%20SICNNv1%20is%20specifically%20tailored%20to%20single%20carrier%20frequency%0Adomain%20equalization%20%28SC-FDE%29%20systems%2C%20the%20communication%20system%20mainly%20regarded%0Ain%20this%20work.%20SICNNv2%20is%20more%20universal%20and%20is%20applicable%20as%20an%20equalizer%20in%0Aany%20communication%20system%20with%20a%20block-based%20data%20transmission%20scheme.%20Moreover%2C%0Afor%20both%20SICNNv1%20and%20SICNNv2%2C%20we%20present%20versions%20with%20highly%20reduced%20numbers%0Aof%20learnable%20parameters.%20Another%20contribution%20of%20this%20work%20is%20a%20novel%20approach%0Afor%20generating%20training%20datasets%20for%20NN-based%20equalizers%2C%20which%20significantly%0Aimproves%20their%20performance%20at%20high%20signal-to-noise%20ratios.%20We%20compare%20the%20bit%0Aerror%20ratio%20performance%20of%20the%20proposed%20NN-based%20equalizers%20with%0Astate-of-the-art%20model-based%20and%20NN-based%20approaches%2C%20highlighting%20the%0Asuperiority%20of%20SICNNv1%20over%20all%20other%20methods%20for%20SC-FDE.%20Exemplarily%2C%20to%0Aemphasize%20its%20universality%2C%20SICNNv2%20is%20additionally%20applied%20to%20a%20unique%20word%0Aorthogonal%20frequency%20division%20multiplexing%20%28UW-OFDM%29%20system%2C%20where%20it%20achieves%0Astate-of-the-art%20performance.%20Furthermore%2C%20we%20present%20a%20thorough%20complexity%0Aanalysis%20of%20the%20proposed%20NN-based%20equalization%20approaches%2C%20and%20we%20investigate%0Athe%20influence%20of%20the%20training%20set%20size%20on%20the%20performance%20of%20NN-based%0Aequalizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12591v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


