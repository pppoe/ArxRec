<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250501.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face\n  Reconstruction", "author": "Simon Giebenhain and Tobias Kirschstein and Martin R\u00fcnz and Lourdes Agapito and Matthias Nie\u00dfner", "abstract": "  We address the 3D reconstruction of human faces from a single RGB image. To\nthis end, we propose Pixel3DMM, a set of highly-generalized vision transformers\nwhich predict per-pixel geometric cues in order to constrain the optimization\nof a 3D morphable face model (3DMM). We exploit the latent features of the DINO\nfoundation model, and introduce a tailored surface normal and uv-coordinate\nprediction head. We train our model by registering three high-quality 3D face\ndatasets against the FLAME mesh topology, which results in a total of over\n1,000 identities and 976K images. For 3D face reconstruction, we propose a\nFLAME fitting opitmization that solves for the 3DMM parameters from the\nuv-coordinate and normal estimates. To evaluate our method, we introduce a new\nbenchmark for single-image face reconstruction, which features high diversity\nfacial expressions, viewing angles, and ethnicities. Crucially, our benchmark\nis the first to evaluate both posed and neutral facial geometry. Ultimately,\nour method outperforms the most competitive baselines by over 15% in terms of\ngeometric accuracy for posed facial expressions.\n", "link": "http://arxiv.org/abs/2505.00615v1", "date": "2025-05-01", "relevancy": 3.151, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6393}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6256}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel3DMM%3A%20Versatile%20Screen-Space%20Priors%20for%20Single-Image%203D%20Face%0A%20%20Reconstruction&body=Title%3A%20Pixel3DMM%3A%20Versatile%20Screen-Space%20Priors%20for%20Single-Image%203D%20Face%0A%20%20Reconstruction%0AAuthor%3A%20Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Martin%20R%C3%BCnz%20and%20Lourdes%20Agapito%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20We%20address%20the%203D%20reconstruction%20of%20human%20faces%20from%20a%20single%20RGB%20image.%20To%0Athis%20end%2C%20we%20propose%20Pixel3DMM%2C%20a%20set%20of%20highly-generalized%20vision%20transformers%0Awhich%20predict%20per-pixel%20geometric%20cues%20in%20order%20to%20constrain%20the%20optimization%0Aof%20a%203D%20morphable%20face%20model%20%283DMM%29.%20We%20exploit%20the%20latent%20features%20of%20the%20DINO%0Afoundation%20model%2C%20and%20introduce%20a%20tailored%20surface%20normal%20and%20uv-coordinate%0Aprediction%20head.%20We%20train%20our%20model%20by%20registering%20three%20high-quality%203D%20face%0Adatasets%20against%20the%20FLAME%20mesh%20topology%2C%20which%20results%20in%20a%20total%20of%20over%0A1%2C000%20identities%20and%20976K%20images.%20For%203D%20face%20reconstruction%2C%20we%20propose%20a%0AFLAME%20fitting%20opitmization%20that%20solves%20for%20the%203DMM%20parameters%20from%20the%0Auv-coordinate%20and%20normal%20estimates.%20To%20evaluate%20our%20method%2C%20we%20introduce%20a%20new%0Abenchmark%20for%20single-image%20face%20reconstruction%2C%20which%20features%20high%20diversity%0Afacial%20expressions%2C%20viewing%20angles%2C%20and%20ethnicities.%20Crucially%2C%20our%20benchmark%0Ais%20the%20first%20to%20evaluate%20both%20posed%20and%20neutral%20facial%20geometry.%20Ultimately%2C%0Aour%20method%20outperforms%20the%20most%20competitive%20baselines%20by%20over%2015%25%20in%20terms%20of%0Ageometric%20accuracy%20for%20posed%20facial%20expressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel3DMM%253A%2520Versatile%2520Screen-Space%2520Priors%2520for%2520Single-Image%25203D%2520Face%250A%2520%2520Reconstruction%26entry.906535625%3DSimon%2520Giebenhain%2520and%2520Tobias%2520Kirschstein%2520and%2520Martin%2520R%25C3%25BCnz%2520and%2520Lourdes%2520Agapito%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520We%2520address%2520the%25203D%2520reconstruction%2520of%2520human%2520faces%2520from%2520a%2520single%2520RGB%2520image.%2520To%250Athis%2520end%252C%2520we%2520propose%2520Pixel3DMM%252C%2520a%2520set%2520of%2520highly-generalized%2520vision%2520transformers%250Awhich%2520predict%2520per-pixel%2520geometric%2520cues%2520in%2520order%2520to%2520constrain%2520the%2520optimization%250Aof%2520a%25203D%2520morphable%2520face%2520model%2520%25283DMM%2529.%2520We%2520exploit%2520the%2520latent%2520features%2520of%2520the%2520DINO%250Afoundation%2520model%252C%2520and%2520introduce%2520a%2520tailored%2520surface%2520normal%2520and%2520uv-coordinate%250Aprediction%2520head.%2520We%2520train%2520our%2520model%2520by%2520registering%2520three%2520high-quality%25203D%2520face%250Adatasets%2520against%2520the%2520FLAME%2520mesh%2520topology%252C%2520which%2520results%2520in%2520a%2520total%2520of%2520over%250A1%252C000%2520identities%2520and%2520976K%2520images.%2520For%25203D%2520face%2520reconstruction%252C%2520we%2520propose%2520a%250AFLAME%2520fitting%2520opitmization%2520that%2520solves%2520for%2520the%25203DMM%2520parameters%2520from%2520the%250Auv-coordinate%2520and%2520normal%2520estimates.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520introduce%2520a%2520new%250Abenchmark%2520for%2520single-image%2520face%2520reconstruction%252C%2520which%2520features%2520high%2520diversity%250Afacial%2520expressions%252C%2520viewing%2520angles%252C%2520and%2520ethnicities.%2520Crucially%252C%2520our%2520benchmark%250Ais%2520the%2520first%2520to%2520evaluate%2520both%2520posed%2520and%2520neutral%2520facial%2520geometry.%2520Ultimately%252C%250Aour%2520method%2520outperforms%2520the%2520most%2520competitive%2520baselines%2520by%2520over%252015%2525%2520in%2520terms%2520of%250Ageometric%2520accuracy%2520for%2520posed%2520facial%2520expressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel3DMM%3A%20Versatile%20Screen-Space%20Priors%20for%20Single-Image%203D%20Face%0A%20%20Reconstruction&entry.906535625=Simon%20Giebenhain%20and%20Tobias%20Kirschstein%20and%20Martin%20R%C3%BCnz%20and%20Lourdes%20Agapito%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20We%20address%20the%203D%20reconstruction%20of%20human%20faces%20from%20a%20single%20RGB%20image.%20To%0Athis%20end%2C%20we%20propose%20Pixel3DMM%2C%20a%20set%20of%20highly-generalized%20vision%20transformers%0Awhich%20predict%20per-pixel%20geometric%20cues%20in%20order%20to%20constrain%20the%20optimization%0Aof%20a%203D%20morphable%20face%20model%20%283DMM%29.%20We%20exploit%20the%20latent%20features%20of%20the%20DINO%0Afoundation%20model%2C%20and%20introduce%20a%20tailored%20surface%20normal%20and%20uv-coordinate%0Aprediction%20head.%20We%20train%20our%20model%20by%20registering%20three%20high-quality%203D%20face%0Adatasets%20against%20the%20FLAME%20mesh%20topology%2C%20which%20results%20in%20a%20total%20of%20over%0A1%2C000%20identities%20and%20976K%20images.%20For%203D%20face%20reconstruction%2C%20we%20propose%20a%0AFLAME%20fitting%20opitmization%20that%20solves%20for%20the%203DMM%20parameters%20from%20the%0Auv-coordinate%20and%20normal%20estimates.%20To%20evaluate%20our%20method%2C%20we%20introduce%20a%20new%0Abenchmark%20for%20single-image%20face%20reconstruction%2C%20which%20features%20high%20diversity%0Afacial%20expressions%2C%20viewing%20angles%2C%20and%20ethnicities.%20Crucially%2C%20our%20benchmark%0Ais%20the%20first%20to%20evaluate%20both%20posed%20and%20neutral%20facial%20geometry.%20Ultimately%2C%0Aour%20method%20outperforms%20the%20most%20competitive%20baselines%20by%20over%2015%25%20in%20terms%20of%0Ageometric%20accuracy%20for%20posed%20facial%20expressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00615v1&entry.124074799=Read"},
{"title": "Towards Global Localization using Multi-Modal Object-Instance\n  Re-Identification", "author": "Aneesh Chavan and Vaibhav Agrawal and Vineeth Bhat and Sarthak Chittawar and Siddharth Srivastava and Chetan Arora and K Madhava Krishna", "abstract": "  Re-identification (ReID) is a critical challenge in computer vision,\npredominantly studied in the context of pedestrians and vehicles. However,\nrobust object-instance ReID, which has significant implications for tasks such\nas autonomous exploration, long-term perception, and scene understanding,\nremains underexplored. In this work, we address this gap by proposing a novel\ndual-path object-instance re-identification transformer architecture that\nintegrates multimodal RGB and depth information. By leveraging depth data, we\ndemonstrate improvements in ReID across scenes that are cluttered or have\nvarying illumination conditions. Additionally, we develop a ReID-based\nlocalization framework that enables accurate camera localization and pose\nidentification across different viewpoints. We validate our methods using two\ncustom-built RGB-D datasets, as well as multiple sequences from the open-source\nTUM RGB-D datasets. Our approach demonstrates significant improvements in both\nobject instance ReID (mAP of 75.18) and localization accuracy (success rate of\n83% on TUM-RGBD), highlighting the essential role of object ReID in advancing\nrobotic perception. Our models, frameworks, and datasets have been made\npublicly available.\n", "link": "http://arxiv.org/abs/2409.12002v2", "date": "2025-05-01", "relevancy": 2.9905, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.603}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Global%20Localization%20using%20Multi-Modal%20Object-Instance%0A%20%20Re-Identification&body=Title%3A%20Towards%20Global%20Localization%20using%20Multi-Modal%20Object-Instance%0A%20%20Re-Identification%0AAuthor%3A%20Aneesh%20Chavan%20and%20Vaibhav%20Agrawal%20and%20Vineeth%20Bhat%20and%20Sarthak%20Chittawar%20and%20Siddharth%20Srivastava%20and%20Chetan%20Arora%20and%20K%20Madhava%20Krishna%0AAbstract%3A%20%20%20Re-identification%20%28ReID%29%20is%20a%20critical%20challenge%20in%20computer%20vision%2C%0Apredominantly%20studied%20in%20the%20context%20of%20pedestrians%20and%20vehicles.%20However%2C%0Arobust%20object-instance%20ReID%2C%20which%20has%20significant%20implications%20for%20tasks%20such%0Aas%20autonomous%20exploration%2C%20long-term%20perception%2C%20and%20scene%20understanding%2C%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20proposing%20a%20novel%0Adual-path%20object-instance%20re-identification%20transformer%20architecture%20that%0Aintegrates%20multimodal%20RGB%20and%20depth%20information.%20By%20leveraging%20depth%20data%2C%20we%0Ademonstrate%20improvements%20in%20ReID%20across%20scenes%20that%20are%20cluttered%20or%20have%0Avarying%20illumination%20conditions.%20Additionally%2C%20we%20develop%20a%20ReID-based%0Alocalization%20framework%20that%20enables%20accurate%20camera%20localization%20and%20pose%0Aidentification%20across%20different%20viewpoints.%20We%20validate%20our%20methods%20using%20two%0Acustom-built%20RGB-D%20datasets%2C%20as%20well%20as%20multiple%20sequences%20from%20the%20open-source%0ATUM%20RGB-D%20datasets.%20Our%20approach%20demonstrates%20significant%20improvements%20in%20both%0Aobject%20instance%20ReID%20%28mAP%20of%2075.18%29%20and%20localization%20accuracy%20%28success%20rate%20of%0A83%25%20on%20TUM-RGBD%29%2C%20highlighting%20the%20essential%20role%20of%20object%20ReID%20in%20advancing%0Arobotic%20perception.%20Our%20models%2C%20frameworks%2C%20and%20datasets%20have%20been%20made%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Global%2520Localization%2520using%2520Multi-Modal%2520Object-Instance%250A%2520%2520Re-Identification%26entry.906535625%3DAneesh%2520Chavan%2520and%2520Vaibhav%2520Agrawal%2520and%2520Vineeth%2520Bhat%2520and%2520Sarthak%2520Chittawar%2520and%2520Siddharth%2520Srivastava%2520and%2520Chetan%2520Arora%2520and%2520K%2520Madhava%2520Krishna%26entry.1292438233%3D%2520%2520Re-identification%2520%2528ReID%2529%2520is%2520a%2520critical%2520challenge%2520in%2520computer%2520vision%252C%250Apredominantly%2520studied%2520in%2520the%2520context%2520of%2520pedestrians%2520and%2520vehicles.%2520However%252C%250Arobust%2520object-instance%2520ReID%252C%2520which%2520has%2520significant%2520implications%2520for%2520tasks%2520such%250Aas%2520autonomous%2520exploration%252C%2520long-term%2520perception%252C%2520and%2520scene%2520understanding%252C%250Aremains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520proposing%2520a%2520novel%250Adual-path%2520object-instance%2520re-identification%2520transformer%2520architecture%2520that%250Aintegrates%2520multimodal%2520RGB%2520and%2520depth%2520information.%2520By%2520leveraging%2520depth%2520data%252C%2520we%250Ademonstrate%2520improvements%2520in%2520ReID%2520across%2520scenes%2520that%2520are%2520cluttered%2520or%2520have%250Avarying%2520illumination%2520conditions.%2520Additionally%252C%2520we%2520develop%2520a%2520ReID-based%250Alocalization%2520framework%2520that%2520enables%2520accurate%2520camera%2520localization%2520and%2520pose%250Aidentification%2520across%2520different%2520viewpoints.%2520We%2520validate%2520our%2520methods%2520using%2520two%250Acustom-built%2520RGB-D%2520datasets%252C%2520as%2520well%2520as%2520multiple%2520sequences%2520from%2520the%2520open-source%250ATUM%2520RGB-D%2520datasets.%2520Our%2520approach%2520demonstrates%2520significant%2520improvements%2520in%2520both%250Aobject%2520instance%2520ReID%2520%2528mAP%2520of%252075.18%2529%2520and%2520localization%2520accuracy%2520%2528success%2520rate%2520of%250A83%2525%2520on%2520TUM-RGBD%2529%252C%2520highlighting%2520the%2520essential%2520role%2520of%2520object%2520ReID%2520in%2520advancing%250Arobotic%2520perception.%2520Our%2520models%252C%2520frameworks%252C%2520and%2520datasets%2520have%2520been%2520made%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Global%20Localization%20using%20Multi-Modal%20Object-Instance%0A%20%20Re-Identification&entry.906535625=Aneesh%20Chavan%20and%20Vaibhav%20Agrawal%20and%20Vineeth%20Bhat%20and%20Sarthak%20Chittawar%20and%20Siddharth%20Srivastava%20and%20Chetan%20Arora%20and%20K%20Madhava%20Krishna&entry.1292438233=%20%20Re-identification%20%28ReID%29%20is%20a%20critical%20challenge%20in%20computer%20vision%2C%0Apredominantly%20studied%20in%20the%20context%20of%20pedestrians%20and%20vehicles.%20However%2C%0Arobust%20object-instance%20ReID%2C%20which%20has%20significant%20implications%20for%20tasks%20such%0Aas%20autonomous%20exploration%2C%20long-term%20perception%2C%20and%20scene%20understanding%2C%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20proposing%20a%20novel%0Adual-path%20object-instance%20re-identification%20transformer%20architecture%20that%0Aintegrates%20multimodal%20RGB%20and%20depth%20information.%20By%20leveraging%20depth%20data%2C%20we%0Ademonstrate%20improvements%20in%20ReID%20across%20scenes%20that%20are%20cluttered%20or%20have%0Avarying%20illumination%20conditions.%20Additionally%2C%20we%20develop%20a%20ReID-based%0Alocalization%20framework%20that%20enables%20accurate%20camera%20localization%20and%20pose%0Aidentification%20across%20different%20viewpoints.%20We%20validate%20our%20methods%20using%20two%0Acustom-built%20RGB-D%20datasets%2C%20as%20well%20as%20multiple%20sequences%20from%20the%20open-source%0ATUM%20RGB-D%20datasets.%20Our%20approach%20demonstrates%20significant%20improvements%20in%20both%0Aobject%20instance%20ReID%20%28mAP%20of%2075.18%29%20and%20localization%20accuracy%20%28success%20rate%20of%0A83%25%20on%20TUM-RGBD%29%2C%20highlighting%20the%20essential%20role%20of%20object%20ReID%20in%20advancing%0Arobotic%20perception.%20Our%20models%2C%20frameworks%2C%20and%20datasets%20have%20been%20made%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12002v2&entry.124074799=Read"},
{"title": "Dietary Intake Estimation via Continuous 3D Reconstruction of Food", "author": "Wallace Lee and YuHao Chen", "abstract": "  Monitoring dietary habits is crucial for preventing health risks associated\nwith overeating and undereating, including obesity, diabetes, and\ncardiovascular diseases. Traditional methods for tracking food intake rely on\nself-reported data before or after the eating, which are prone to inaccuracies.\nThis study proposes an approach to accurately monitor ingest behaviours by\nleveraging 3D food models constructed from monocular 2D video. Using COLMAP and\npose estimation algorithms, we generate detailed 3D representations of food,\nallowing us to observe changes in food volume as it is consumed. Experiments\nwith toy models and real food items demonstrate the approach's potential.\nMeanwhile, we have proposed a new methodology for automated state recognition\nchallenges to accurately detect state changes and maintain model fidelity. The\n3D reconstruction approach shows promise in capturing comprehensive dietary\nbehaviour insights, ultimately contributing to the development of automated and\naccurate dietary monitoring tools.\n", "link": "http://arxiv.org/abs/2505.00606v1", "date": "2025-05-01", "relevancy": 2.8659, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.575}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dietary%20Intake%20Estimation%20via%20Continuous%203D%20Reconstruction%20of%20Food&body=Title%3A%20Dietary%20Intake%20Estimation%20via%20Continuous%203D%20Reconstruction%20of%20Food%0AAuthor%3A%20Wallace%20Lee%20and%20YuHao%20Chen%0AAbstract%3A%20%20%20Monitoring%20dietary%20habits%20is%20crucial%20for%20preventing%20health%20risks%20associated%0Awith%20overeating%20and%20undereating%2C%20including%20obesity%2C%20diabetes%2C%20and%0Acardiovascular%20diseases.%20Traditional%20methods%20for%20tracking%20food%20intake%20rely%20on%0Aself-reported%20data%20before%20or%20after%20the%20eating%2C%20which%20are%20prone%20to%20inaccuracies.%0AThis%20study%20proposes%20an%20approach%20to%20accurately%20monitor%20ingest%20behaviours%20by%0Aleveraging%203D%20food%20models%20constructed%20from%20monocular%202D%20video.%20Using%20COLMAP%20and%0Apose%20estimation%20algorithms%2C%20we%20generate%20detailed%203D%20representations%20of%20food%2C%0Aallowing%20us%20to%20observe%20changes%20in%20food%20volume%20as%20it%20is%20consumed.%20Experiments%0Awith%20toy%20models%20and%20real%20food%20items%20demonstrate%20the%20approach%27s%20potential.%0AMeanwhile%2C%20we%20have%20proposed%20a%20new%20methodology%20for%20automated%20state%20recognition%0Achallenges%20to%20accurately%20detect%20state%20changes%20and%20maintain%20model%20fidelity.%20The%0A3D%20reconstruction%20approach%20shows%20promise%20in%20capturing%20comprehensive%20dietary%0Abehaviour%20insights%2C%20ultimately%20contributing%20to%20the%20development%20of%20automated%20and%0Aaccurate%20dietary%20monitoring%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDietary%2520Intake%2520Estimation%2520via%2520Continuous%25203D%2520Reconstruction%2520of%2520Food%26entry.906535625%3DWallace%2520Lee%2520and%2520YuHao%2520Chen%26entry.1292438233%3D%2520%2520Monitoring%2520dietary%2520habits%2520is%2520crucial%2520for%2520preventing%2520health%2520risks%2520associated%250Awith%2520overeating%2520and%2520undereating%252C%2520including%2520obesity%252C%2520diabetes%252C%2520and%250Acardiovascular%2520diseases.%2520Traditional%2520methods%2520for%2520tracking%2520food%2520intake%2520rely%2520on%250Aself-reported%2520data%2520before%2520or%2520after%2520the%2520eating%252C%2520which%2520are%2520prone%2520to%2520inaccuracies.%250AThis%2520study%2520proposes%2520an%2520approach%2520to%2520accurately%2520monitor%2520ingest%2520behaviours%2520by%250Aleveraging%25203D%2520food%2520models%2520constructed%2520from%2520monocular%25202D%2520video.%2520Using%2520COLMAP%2520and%250Apose%2520estimation%2520algorithms%252C%2520we%2520generate%2520detailed%25203D%2520representations%2520of%2520food%252C%250Aallowing%2520us%2520to%2520observe%2520changes%2520in%2520food%2520volume%2520as%2520it%2520is%2520consumed.%2520Experiments%250Awith%2520toy%2520models%2520and%2520real%2520food%2520items%2520demonstrate%2520the%2520approach%2527s%2520potential.%250AMeanwhile%252C%2520we%2520have%2520proposed%2520a%2520new%2520methodology%2520for%2520automated%2520state%2520recognition%250Achallenges%2520to%2520accurately%2520detect%2520state%2520changes%2520and%2520maintain%2520model%2520fidelity.%2520The%250A3D%2520reconstruction%2520approach%2520shows%2520promise%2520in%2520capturing%2520comprehensive%2520dietary%250Abehaviour%2520insights%252C%2520ultimately%2520contributing%2520to%2520the%2520development%2520of%2520automated%2520and%250Aaccurate%2520dietary%2520monitoring%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dietary%20Intake%20Estimation%20via%20Continuous%203D%20Reconstruction%20of%20Food&entry.906535625=Wallace%20Lee%20and%20YuHao%20Chen&entry.1292438233=%20%20Monitoring%20dietary%20habits%20is%20crucial%20for%20preventing%20health%20risks%20associated%0Awith%20overeating%20and%20undereating%2C%20including%20obesity%2C%20diabetes%2C%20and%0Acardiovascular%20diseases.%20Traditional%20methods%20for%20tracking%20food%20intake%20rely%20on%0Aself-reported%20data%20before%20or%20after%20the%20eating%2C%20which%20are%20prone%20to%20inaccuracies.%0AThis%20study%20proposes%20an%20approach%20to%20accurately%20monitor%20ingest%20behaviours%20by%0Aleveraging%203D%20food%20models%20constructed%20from%20monocular%202D%20video.%20Using%20COLMAP%20and%0Apose%20estimation%20algorithms%2C%20we%20generate%20detailed%203D%20representations%20of%20food%2C%0Aallowing%20us%20to%20observe%20changes%20in%20food%20volume%20as%20it%20is%20consumed.%20Experiments%0Awith%20toy%20models%20and%20real%20food%20items%20demonstrate%20the%20approach%27s%20potential.%0AMeanwhile%2C%20we%20have%20proposed%20a%20new%20methodology%20for%20automated%20state%20recognition%0Achallenges%20to%20accurately%20detect%20state%20changes%20and%20maintain%20model%20fidelity.%20The%0A3D%20reconstruction%20approach%20shows%20promise%20in%20capturing%20comprehensive%20dietary%0Abehaviour%20insights%2C%20ultimately%20contributing%20to%20the%20development%20of%20automated%20and%0Aaccurate%20dietary%20monitoring%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00606v1&entry.124074799=Read"},
{"title": "Diverse Semantics-Guided Feature Alignment and Decoupling for\n  Visible-Infrared Person Re-Identification", "author": "Neng Dong and Shuanglin Yan and Liyan Zhang and Jinhui Tang", "abstract": "  Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due\nto the large modality discrepancy between visible and infrared images, which\ncomplicates the alignment of their features into a suitable common space.\nMoreover, style noise, such as illumination and color contrast, reduces the\nidentity discriminability and modality invariance of features. To address these\nchallenges, we propose a novel Diverse Semantics-guided Feature Alignment and\nDecoupling (DSFAD) network to align identity-relevant features from different\nmodalities into a textual embedding space and disentangle identity-irrelevant\nfeatures within each modality. Specifically, we develop a Diverse\nSemantics-guided Feature Alignment (DSFA) module, which generates pedestrian\ndescriptions with diverse sentence structures to guide the cross-modality\nalignment of visual features. Furthermore, to filter out style information, we\npropose a Semantic Margin-guided Feature Decoupling (SMFD) module, which\ndecomposes visual features into pedestrian-related and style-related\ncomponents, and then constrains the similarity between the former and the\ntextual embeddings to be at least a margin higher than that between the latter\nand the textual embeddings. Additionally, to prevent the loss of pedestrian\nsemantics during feature decoupling, we design a Semantic Consistency-guided\nFeature Restitution (SCFR) module, which further excavates useful information\nfor identification from the style-related features and restores it back into\nthe pedestrian-related features, and then constrains the similarity between the\nfeatures after restitution and the textual embeddings to be consistent with\nthat between the features before decoupling and the textual embeddings.\nExtensive experiments on three VI-ReID datasets demonstrate the superiority of\nour DSFAD.\n", "link": "http://arxiv.org/abs/2505.00619v1", "date": "2025-05-01", "relevancy": 2.8599, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Semantics-Guided%20Feature%20Alignment%20and%20Decoupling%20for%0A%20%20Visible-Infrared%20Person%20Re-Identification&body=Title%3A%20Diverse%20Semantics-Guided%20Feature%20Alignment%20and%20Decoupling%20for%0A%20%20Visible-Infrared%20Person%20Re-Identification%0AAuthor%3A%20Neng%20Dong%20and%20Shuanglin%20Yan%20and%20Liyan%20Zhang%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Visible-Infrared%20Person%20Re-Identification%20%28VI-ReID%29%20is%20a%20challenging%20task%20due%0Ato%20the%20large%20modality%20discrepancy%20between%20visible%20and%20infrared%20images%2C%20which%0Acomplicates%20the%20alignment%20of%20their%20features%20into%20a%20suitable%20common%20space.%0AMoreover%2C%20style%20noise%2C%20such%20as%20illumination%20and%20color%20contrast%2C%20reduces%20the%0Aidentity%20discriminability%20and%20modality%20invariance%20of%20features.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20Diverse%20Semantics-guided%20Feature%20Alignment%20and%0ADecoupling%20%28DSFAD%29%20network%20to%20align%20identity-relevant%20features%20from%20different%0Amodalities%20into%20a%20textual%20embedding%20space%20and%20disentangle%20identity-irrelevant%0Afeatures%20within%20each%20modality.%20Specifically%2C%20we%20develop%20a%20Diverse%0ASemantics-guided%20Feature%20Alignment%20%28DSFA%29%20module%2C%20which%20generates%20pedestrian%0Adescriptions%20with%20diverse%20sentence%20structures%20to%20guide%20the%20cross-modality%0Aalignment%20of%20visual%20features.%20Furthermore%2C%20to%20filter%20out%20style%20information%2C%20we%0Apropose%20a%20Semantic%20Margin-guided%20Feature%20Decoupling%20%28SMFD%29%20module%2C%20which%0Adecomposes%20visual%20features%20into%20pedestrian-related%20and%20style-related%0Acomponents%2C%20and%20then%20constrains%20the%20similarity%20between%20the%20former%20and%20the%0Atextual%20embeddings%20to%20be%20at%20least%20a%20margin%20higher%20than%20that%20between%20the%20latter%0Aand%20the%20textual%20embeddings.%20Additionally%2C%20to%20prevent%20the%20loss%20of%20pedestrian%0Asemantics%20during%20feature%20decoupling%2C%20we%20design%20a%20Semantic%20Consistency-guided%0AFeature%20Restitution%20%28SCFR%29%20module%2C%20which%20further%20excavates%20useful%20information%0Afor%20identification%20from%20the%20style-related%20features%20and%20restores%20it%20back%20into%0Athe%20pedestrian-related%20features%2C%20and%20then%20constrains%20the%20similarity%20between%20the%0Afeatures%20after%20restitution%20and%20the%20textual%20embeddings%20to%20be%20consistent%20with%0Athat%20between%20the%20features%20before%20decoupling%20and%20the%20textual%20embeddings.%0AExtensive%20experiments%20on%20three%20VI-ReID%20datasets%20demonstrate%20the%20superiority%20of%0Aour%20DSFAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Semantics-Guided%2520Feature%2520Alignment%2520and%2520Decoupling%2520for%250A%2520%2520Visible-Infrared%2520Person%2520Re-Identification%26entry.906535625%3DNeng%2520Dong%2520and%2520Shuanglin%2520Yan%2520and%2520Liyan%2520Zhang%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Visible-Infrared%2520Person%2520Re-Identification%2520%2528VI-ReID%2529%2520is%2520a%2520challenging%2520task%2520due%250Ato%2520the%2520large%2520modality%2520discrepancy%2520between%2520visible%2520and%2520infrared%2520images%252C%2520which%250Acomplicates%2520the%2520alignment%2520of%2520their%2520features%2520into%2520a%2520suitable%2520common%2520space.%250AMoreover%252C%2520style%2520noise%252C%2520such%2520as%2520illumination%2520and%2520color%2520contrast%252C%2520reduces%2520the%250Aidentity%2520discriminability%2520and%2520modality%2520invariance%2520of%2520features.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520Diverse%2520Semantics-guided%2520Feature%2520Alignment%2520and%250ADecoupling%2520%2528DSFAD%2529%2520network%2520to%2520align%2520identity-relevant%2520features%2520from%2520different%250Amodalities%2520into%2520a%2520textual%2520embedding%2520space%2520and%2520disentangle%2520identity-irrelevant%250Afeatures%2520within%2520each%2520modality.%2520Specifically%252C%2520we%2520develop%2520a%2520Diverse%250ASemantics-guided%2520Feature%2520Alignment%2520%2528DSFA%2529%2520module%252C%2520which%2520generates%2520pedestrian%250Adescriptions%2520with%2520diverse%2520sentence%2520structures%2520to%2520guide%2520the%2520cross-modality%250Aalignment%2520of%2520visual%2520features.%2520Furthermore%252C%2520to%2520filter%2520out%2520style%2520information%252C%2520we%250Apropose%2520a%2520Semantic%2520Margin-guided%2520Feature%2520Decoupling%2520%2528SMFD%2529%2520module%252C%2520which%250Adecomposes%2520visual%2520features%2520into%2520pedestrian-related%2520and%2520style-related%250Acomponents%252C%2520and%2520then%2520constrains%2520the%2520similarity%2520between%2520the%2520former%2520and%2520the%250Atextual%2520embeddings%2520to%2520be%2520at%2520least%2520a%2520margin%2520higher%2520than%2520that%2520between%2520the%2520latter%250Aand%2520the%2520textual%2520embeddings.%2520Additionally%252C%2520to%2520prevent%2520the%2520loss%2520of%2520pedestrian%250Asemantics%2520during%2520feature%2520decoupling%252C%2520we%2520design%2520a%2520Semantic%2520Consistency-guided%250AFeature%2520Restitution%2520%2528SCFR%2529%2520module%252C%2520which%2520further%2520excavates%2520useful%2520information%250Afor%2520identification%2520from%2520the%2520style-related%2520features%2520and%2520restores%2520it%2520back%2520into%250Athe%2520pedestrian-related%2520features%252C%2520and%2520then%2520constrains%2520the%2520similarity%2520between%2520the%250Afeatures%2520after%2520restitution%2520and%2520the%2520textual%2520embeddings%2520to%2520be%2520consistent%2520with%250Athat%2520between%2520the%2520features%2520before%2520decoupling%2520and%2520the%2520textual%2520embeddings.%250AExtensive%2520experiments%2520on%2520three%2520VI-ReID%2520datasets%2520demonstrate%2520the%2520superiority%2520of%250Aour%2520DSFAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Semantics-Guided%20Feature%20Alignment%20and%20Decoupling%20for%0A%20%20Visible-Infrared%20Person%20Re-Identification&entry.906535625=Neng%20Dong%20and%20Shuanglin%20Yan%20and%20Liyan%20Zhang%20and%20Jinhui%20Tang&entry.1292438233=%20%20Visible-Infrared%20Person%20Re-Identification%20%28VI-ReID%29%20is%20a%20challenging%20task%20due%0Ato%20the%20large%20modality%20discrepancy%20between%20visible%20and%20infrared%20images%2C%20which%0Acomplicates%20the%20alignment%20of%20their%20features%20into%20a%20suitable%20common%20space.%0AMoreover%2C%20style%20noise%2C%20such%20as%20illumination%20and%20color%20contrast%2C%20reduces%20the%0Aidentity%20discriminability%20and%20modality%20invariance%20of%20features.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20Diverse%20Semantics-guided%20Feature%20Alignment%20and%0ADecoupling%20%28DSFAD%29%20network%20to%20align%20identity-relevant%20features%20from%20different%0Amodalities%20into%20a%20textual%20embedding%20space%20and%20disentangle%20identity-irrelevant%0Afeatures%20within%20each%20modality.%20Specifically%2C%20we%20develop%20a%20Diverse%0ASemantics-guided%20Feature%20Alignment%20%28DSFA%29%20module%2C%20which%20generates%20pedestrian%0Adescriptions%20with%20diverse%20sentence%20structures%20to%20guide%20the%20cross-modality%0Aalignment%20of%20visual%20features.%20Furthermore%2C%20to%20filter%20out%20style%20information%2C%20we%0Apropose%20a%20Semantic%20Margin-guided%20Feature%20Decoupling%20%28SMFD%29%20module%2C%20which%0Adecomposes%20visual%20features%20into%20pedestrian-related%20and%20style-related%0Acomponents%2C%20and%20then%20constrains%20the%20similarity%20between%20the%20former%20and%20the%0Atextual%20embeddings%20to%20be%20at%20least%20a%20margin%20higher%20than%20that%20between%20the%20latter%0Aand%20the%20textual%20embeddings.%20Additionally%2C%20to%20prevent%20the%20loss%20of%20pedestrian%0Asemantics%20during%20feature%20decoupling%2C%20we%20design%20a%20Semantic%20Consistency-guided%0AFeature%20Restitution%20%28SCFR%29%20module%2C%20which%20further%20excavates%20useful%20information%0Afor%20identification%20from%20the%20style-related%20features%20and%20restores%20it%20back%20into%0Athe%20pedestrian-related%20features%2C%20and%20then%20constrains%20the%20similarity%20between%20the%0Afeatures%20after%20restitution%20and%20the%20textual%20embeddings%20to%20be%20consistent%20with%0Athat%20between%20the%20features%20before%20decoupling%20and%20the%20textual%20embeddings.%0AExtensive%20experiments%20on%20three%20VI-ReID%20datasets%20demonstrate%20the%20superiority%20of%0Aour%20DSFAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00619v1&entry.124074799=Read"},
{"title": "Brain Foundation Models with Hypergraph Dynamic Adapter for Brain\n  Disease Analysis", "author": "Zhongying Deng and Haoyu Wang and Ziyan Huang and Lipei Zhang and Angelica I. Aviles-Rivero and Chaoyu Liu and Junjun He and Zoe Kourtzi and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Brain diseases, such as Alzheimer's disease and brain tumors, present\nprofound challenges due to their complexity and societal impact. Recent\nadvancements in brain foundation models have shown significant promise in\naddressing a range of brain-related tasks. However, current brain foundation\nmodels are limited by task and data homogeneity, restricted generalization\nbeyond segmentation or classification, and inefficient adaptation to diverse\nclinical tasks. In this work, we propose SAM-Brain3D, a brain-specific\nfoundation model trained on over 66,000 brain image-label pairs across 14 MRI\nsub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter\nfor efficient and effective downstream adaptation. SAM-Brain3D captures\ndetailed brain-specific anatomical and modality priors for segmenting diverse\nbrain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse\ncomplementary multi-modal data and dynamically generate patient-specific\nconvolutional kernels for multi-scale feature fusion and personalized\npatient-wise adaptation. Together, our framework excels across a broad spectrum\nof brain disease segmentation and classification tasks. Extensive experiments\ndemonstrate that our method consistently outperforms existing state-of-the-art\napproaches, offering a new paradigm for brain disease analysis through\nmulti-modal, multi-scale, and dynamic foundation modeling.\n", "link": "http://arxiv.org/abs/2505.00627v1", "date": "2025-05-01", "relevancy": 2.8458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Foundation%20Models%20with%20Hypergraph%20Dynamic%20Adapter%20for%20Brain%0A%20%20Disease%20Analysis&body=Title%3A%20Brain%20Foundation%20Models%20with%20Hypergraph%20Dynamic%20Adapter%20for%20Brain%0A%20%20Disease%20Analysis%0AAuthor%3A%20Zhongying%20Deng%20and%20Haoyu%20Wang%20and%20Ziyan%20Huang%20and%20Lipei%20Zhang%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Chaoyu%20Liu%20and%20Junjun%20He%20and%20Zoe%20Kourtzi%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Brain%20diseases%2C%20such%20as%20Alzheimer%27s%20disease%20and%20brain%20tumors%2C%20present%0Aprofound%20challenges%20due%20to%20their%20complexity%20and%20societal%20impact.%20Recent%0Aadvancements%20in%20brain%20foundation%20models%20have%20shown%20significant%20promise%20in%0Aaddressing%20a%20range%20of%20brain-related%20tasks.%20However%2C%20current%20brain%20foundation%0Amodels%20are%20limited%20by%20task%20and%20data%20homogeneity%2C%20restricted%20generalization%0Abeyond%20segmentation%20or%20classification%2C%20and%20inefficient%20adaptation%20to%20diverse%0Aclinical%20tasks.%20In%20this%20work%2C%20we%20propose%20SAM-Brain3D%2C%20a%20brain-specific%0Afoundation%20model%20trained%20on%20over%2066%2C000%20brain%20image-label%20pairs%20across%2014%20MRI%0Asub-modalities%2C%20and%20Hypergraph%20Dynamic%20Adapter%20%28HyDA%29%2C%20a%20lightweight%20adapter%0Afor%20efficient%20and%20effective%20downstream%20adaptation.%20SAM-Brain3D%20captures%0Adetailed%20brain-specific%20anatomical%20and%20modality%20priors%20for%20segmenting%20diverse%0Abrain%20targets%20and%20broader%20downstream%20tasks.%20HyDA%20leverages%20hypergraphs%20to%20fuse%0Acomplementary%20multi-modal%20data%20and%20dynamically%20generate%20patient-specific%0Aconvolutional%20kernels%20for%20multi-scale%20feature%20fusion%20and%20personalized%0Apatient-wise%20adaptation.%20Together%2C%20our%20framework%20excels%20across%20a%20broad%20spectrum%0Aof%20brain%20disease%20segmentation%20and%20classification%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20consistently%20outperforms%20existing%20state-of-the-art%0Aapproaches%2C%20offering%20a%20new%20paradigm%20for%20brain%20disease%20analysis%20through%0Amulti-modal%2C%20multi-scale%2C%20and%20dynamic%20foundation%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Foundation%2520Models%2520with%2520Hypergraph%2520Dynamic%2520Adapter%2520for%2520Brain%250A%2520%2520Disease%2520Analysis%26entry.906535625%3DZhongying%2520Deng%2520and%2520Haoyu%2520Wang%2520and%2520Ziyan%2520Huang%2520and%2520Lipei%2520Zhang%2520and%2520Angelica%2520I.%2520Aviles-Rivero%2520and%2520Chaoyu%2520Liu%2520and%2520Junjun%2520He%2520and%2520Zoe%2520Kourtzi%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%26entry.1292438233%3D%2520%2520Brain%2520diseases%252C%2520such%2520as%2520Alzheimer%2527s%2520disease%2520and%2520brain%2520tumors%252C%2520present%250Aprofound%2520challenges%2520due%2520to%2520their%2520complexity%2520and%2520societal%2520impact.%2520Recent%250Aadvancements%2520in%2520brain%2520foundation%2520models%2520have%2520shown%2520significant%2520promise%2520in%250Aaddressing%2520a%2520range%2520of%2520brain-related%2520tasks.%2520However%252C%2520current%2520brain%2520foundation%250Amodels%2520are%2520limited%2520by%2520task%2520and%2520data%2520homogeneity%252C%2520restricted%2520generalization%250Abeyond%2520segmentation%2520or%2520classification%252C%2520and%2520inefficient%2520adaptation%2520to%2520diverse%250Aclinical%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520SAM-Brain3D%252C%2520a%2520brain-specific%250Afoundation%2520model%2520trained%2520on%2520over%252066%252C000%2520brain%2520image-label%2520pairs%2520across%252014%2520MRI%250Asub-modalities%252C%2520and%2520Hypergraph%2520Dynamic%2520Adapter%2520%2528HyDA%2529%252C%2520a%2520lightweight%2520adapter%250Afor%2520efficient%2520and%2520effective%2520downstream%2520adaptation.%2520SAM-Brain3D%2520captures%250Adetailed%2520brain-specific%2520anatomical%2520and%2520modality%2520priors%2520for%2520segmenting%2520diverse%250Abrain%2520targets%2520and%2520broader%2520downstream%2520tasks.%2520HyDA%2520leverages%2520hypergraphs%2520to%2520fuse%250Acomplementary%2520multi-modal%2520data%2520and%2520dynamically%2520generate%2520patient-specific%250Aconvolutional%2520kernels%2520for%2520multi-scale%2520feature%2520fusion%2520and%2520personalized%250Apatient-wise%2520adaptation.%2520Together%252C%2520our%2520framework%2520excels%2520across%2520a%2520broad%2520spectrum%250Aof%2520brain%2520disease%2520segmentation%2520and%2520classification%2520tasks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520existing%2520state-of-the-art%250Aapproaches%252C%2520offering%2520a%2520new%2520paradigm%2520for%2520brain%2520disease%2520analysis%2520through%250Amulti-modal%252C%2520multi-scale%252C%2520and%2520dynamic%2520foundation%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Foundation%20Models%20with%20Hypergraph%20Dynamic%20Adapter%20for%20Brain%0A%20%20Disease%20Analysis&entry.906535625=Zhongying%20Deng%20and%20Haoyu%20Wang%20and%20Ziyan%20Huang%20and%20Lipei%20Zhang%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Chaoyu%20Liu%20and%20Junjun%20He%20and%20Zoe%20Kourtzi%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Brain%20diseases%2C%20such%20as%20Alzheimer%27s%20disease%20and%20brain%20tumors%2C%20present%0Aprofound%20challenges%20due%20to%20their%20complexity%20and%20societal%20impact.%20Recent%0Aadvancements%20in%20brain%20foundation%20models%20have%20shown%20significant%20promise%20in%0Aaddressing%20a%20range%20of%20brain-related%20tasks.%20However%2C%20current%20brain%20foundation%0Amodels%20are%20limited%20by%20task%20and%20data%20homogeneity%2C%20restricted%20generalization%0Abeyond%20segmentation%20or%20classification%2C%20and%20inefficient%20adaptation%20to%20diverse%0Aclinical%20tasks.%20In%20this%20work%2C%20we%20propose%20SAM-Brain3D%2C%20a%20brain-specific%0Afoundation%20model%20trained%20on%20over%2066%2C000%20brain%20image-label%20pairs%20across%2014%20MRI%0Asub-modalities%2C%20and%20Hypergraph%20Dynamic%20Adapter%20%28HyDA%29%2C%20a%20lightweight%20adapter%0Afor%20efficient%20and%20effective%20downstream%20adaptation.%20SAM-Brain3D%20captures%0Adetailed%20brain-specific%20anatomical%20and%20modality%20priors%20for%20segmenting%20diverse%0Abrain%20targets%20and%20broader%20downstream%20tasks.%20HyDA%20leverages%20hypergraphs%20to%20fuse%0Acomplementary%20multi-modal%20data%20and%20dynamically%20generate%20patient-specific%0Aconvolutional%20kernels%20for%20multi-scale%20feature%20fusion%20and%20personalized%0Apatient-wise%20adaptation.%20Together%2C%20our%20framework%20excels%20across%20a%20broad%20spectrum%0Aof%20brain%20disease%20segmentation%20and%20classification%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20consistently%20outperforms%20existing%20state-of-the-art%0Aapproaches%2C%20offering%20a%20new%20paradigm%20for%20brain%20disease%20analysis%20through%0Amulti-modal%2C%20multi-scale%2C%20and%20dynamic%20foundation%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00627v1&entry.124074799=Read"},
{"title": "Variational Self-Supervised Learning", "author": "Mehmet Can Yavuz and Berrin Yanikoglu", "abstract": "  We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques.\n", "link": "http://arxiv.org/abs/2504.04318v3", "date": "2025-05-01", "relevancy": 2.8222, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6028}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5847}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Self-Supervised%20Learning&body=Title%3A%20Variational%20Self-Supervised%20Learning%0AAuthor%3A%20Mehmet%20Can%20Yavuz%20and%20Berrin%20Yanikoglu%0AAbstract%3A%20%20%20We%20present%20Variational%20Self-Supervised%20Learning%20%28VSSL%29%2C%20a%20novel%20framework%0Athat%20combines%20variational%20inference%20with%20self-supervised%20learning%20to%20enable%0Aefficient%2C%20decoder-free%20representation%20learning.%20Unlike%20traditional%20VAEs%20that%0Arely%20on%20input%20reconstruction%20via%20a%20decoder%2C%20VSSL%20symmetrically%20couples%20two%0Aencoders%20with%20Gaussian%20outputs.%20A%20momentum-updated%20teacher%20network%20defines%20a%0Adynamic%2C%20data-dependent%20prior%2C%20while%20the%20student%20encoder%20produces%20an%0Aapproximate%20posterior%20from%20augmented%20views.%20The%20reconstruction%20term%20in%20the%20ELBO%0Ais%20replaced%20with%20a%20cross-view%20denoising%20objective%2C%20preserving%20the%20analytical%0Atractability%20of%20Gaussian%20KL%20divergence.%20We%20further%20introduce%20cosine-based%0Aformulations%20of%20KL%20and%20log-likelihood%20terms%20to%20enhance%20semantic%20alignment%20in%0Ahigh-dimensional%20latent%20spaces.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%0AImageNet-100%20show%20that%20VSSL%20achieves%20competitive%20or%20superior%20performance%20to%0Aleading%20self-supervised%20methods%2C%20including%20BYOL%20and%20MoCo%20V3.%20VSSL%20offers%20a%0Ascalable%2C%20probabilistically%20grounded%20approach%20to%20learning%20transferable%0Arepresentations%20without%20generative%20reconstruction%2C%20bridging%20the%20gap%20between%0Avariational%20modeling%20and%20modern%20self-supervised%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04318v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Self-Supervised%2520Learning%26entry.906535625%3DMehmet%2520Can%2520Yavuz%2520and%2520Berrin%2520Yanikoglu%26entry.1292438233%3D%2520%2520We%2520present%2520Variational%2520Self-Supervised%2520Learning%2520%2528VSSL%2529%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520variational%2520inference%2520with%2520self-supervised%2520learning%2520to%2520enable%250Aefficient%252C%2520decoder-free%2520representation%2520learning.%2520Unlike%2520traditional%2520VAEs%2520that%250Arely%2520on%2520input%2520reconstruction%2520via%2520a%2520decoder%252C%2520VSSL%2520symmetrically%2520couples%2520two%250Aencoders%2520with%2520Gaussian%2520outputs.%2520A%2520momentum-updated%2520teacher%2520network%2520defines%2520a%250Adynamic%252C%2520data-dependent%2520prior%252C%2520while%2520the%2520student%2520encoder%2520produces%2520an%250Aapproximate%2520posterior%2520from%2520augmented%2520views.%2520The%2520reconstruction%2520term%2520in%2520the%2520ELBO%250Ais%2520replaced%2520with%2520a%2520cross-view%2520denoising%2520objective%252C%2520preserving%2520the%2520analytical%250Atractability%2520of%2520Gaussian%2520KL%2520divergence.%2520We%2520further%2520introduce%2520cosine-based%250Aformulations%2520of%2520KL%2520and%2520log-likelihood%2520terms%2520to%2520enhance%2520semantic%2520alignment%2520in%250Ahigh-dimensional%2520latent%2520spaces.%2520Experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%250AImageNet-100%2520show%2520that%2520VSSL%2520achieves%2520competitive%2520or%2520superior%2520performance%2520to%250Aleading%2520self-supervised%2520methods%252C%2520including%2520BYOL%2520and%2520MoCo%2520V3.%2520VSSL%2520offers%2520a%250Ascalable%252C%2520probabilistically%2520grounded%2520approach%2520to%2520learning%2520transferable%250Arepresentations%2520without%2520generative%2520reconstruction%252C%2520bridging%2520the%2520gap%2520between%250Avariational%2520modeling%2520and%2520modern%2520self-supervised%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04318v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Self-Supervised%20Learning&entry.906535625=Mehmet%20Can%20Yavuz%20and%20Berrin%20Yanikoglu&entry.1292438233=%20%20We%20present%20Variational%20Self-Supervised%20Learning%20%28VSSL%29%2C%20a%20novel%20framework%0Athat%20combines%20variational%20inference%20with%20self-supervised%20learning%20to%20enable%0Aefficient%2C%20decoder-free%20representation%20learning.%20Unlike%20traditional%20VAEs%20that%0Arely%20on%20input%20reconstruction%20via%20a%20decoder%2C%20VSSL%20symmetrically%20couples%20two%0Aencoders%20with%20Gaussian%20outputs.%20A%20momentum-updated%20teacher%20network%20defines%20a%0Adynamic%2C%20data-dependent%20prior%2C%20while%20the%20student%20encoder%20produces%20an%0Aapproximate%20posterior%20from%20augmented%20views.%20The%20reconstruction%20term%20in%20the%20ELBO%0Ais%20replaced%20with%20a%20cross-view%20denoising%20objective%2C%20preserving%20the%20analytical%0Atractability%20of%20Gaussian%20KL%20divergence.%20We%20further%20introduce%20cosine-based%0Aformulations%20of%20KL%20and%20log-likelihood%20terms%20to%20enhance%20semantic%20alignment%20in%0Ahigh-dimensional%20latent%20spaces.%20Experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%0AImageNet-100%20show%20that%20VSSL%20achieves%20competitive%20or%20superior%20performance%20to%0Aleading%20self-supervised%20methods%2C%20including%20BYOL%20and%20MoCo%20V3.%20VSSL%20offers%20a%0Ascalable%2C%20probabilistically%20grounded%20approach%20to%20learning%20transferable%0Arepresentations%20without%20generative%20reconstruction%2C%20bridging%20the%20gap%20between%0Avariational%20modeling%20and%20modern%20self-supervised%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04318v3&entry.124074799=Read"},
{"title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion", "author": "Hao Wen and Zehuan Huang and Yaohui Wang and Xinyuan Chen and Lu Sheng", "abstract": "  Existing single image-to-3D creation methods typically involve a two-stage\nprocess, first generating multi-view images, and then using these images for 3D\nreconstruction. However, training these two stages separately leads to\nsignificant data bias in the inference phase, thus affecting the quality of\nreconstructed results. We introduce a unified 3D generation framework, named\nOuroboros3D, which integrates diffusion-based multi-view image generation and\n3D reconstruction into a recursive diffusion process. In our framework, these\ntwo modules are jointly trained through a self-conditioning mechanism, allowing\nthem to adapt to each other's characteristics for robust inference. During the\nmulti-view denoising process, the multi-view diffusion model uses the 3D-aware\nmaps rendered by the reconstruction module at the previous timestep as\nadditional conditions. The recursive diffusion framework with 3D-aware feedback\nunites the entire process and improves geometric consistency.Experiments show\nthat our framework outperforms separation of these two stages and existing\nmethods that combine them at the inference phase. Project page:\nhttps://costwen.github.io/Ouroboros3D/\n", "link": "http://arxiv.org/abs/2406.03184v2", "date": "2025-05-01", "relevancy": 2.797, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7124}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7124}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ouroboros3D%3A%20Image-to-3D%20Generation%20via%203D-aware%20Recursive%20Diffusion&body=Title%3A%20Ouroboros3D%3A%20Image-to-3D%20Generation%20via%203D-aware%20Recursive%20Diffusion%0AAuthor%3A%20Hao%20Wen%20and%20Zehuan%20Huang%20and%20Yaohui%20Wang%20and%20Xinyuan%20Chen%20and%20Lu%20Sheng%0AAbstract%3A%20%20%20Existing%20single%20image-to-3D%20creation%20methods%20typically%20involve%20a%20two-stage%0Aprocess%2C%20first%20generating%20multi-view%20images%2C%20and%20then%20using%20these%20images%20for%203D%0Areconstruction.%20However%2C%20training%20these%20two%20stages%20separately%20leads%20to%0Asignificant%20data%20bias%20in%20the%20inference%20phase%2C%20thus%20affecting%20the%20quality%20of%0Areconstructed%20results.%20We%20introduce%20a%20unified%203D%20generation%20framework%2C%20named%0AOuroboros3D%2C%20which%20integrates%20diffusion-based%20multi-view%20image%20generation%20and%0A3D%20reconstruction%20into%20a%20recursive%20diffusion%20process.%20In%20our%20framework%2C%20these%0Atwo%20modules%20are%20jointly%20trained%20through%20a%20self-conditioning%20mechanism%2C%20allowing%0Athem%20to%20adapt%20to%20each%20other%27s%20characteristics%20for%20robust%20inference.%20During%20the%0Amulti-view%20denoising%20process%2C%20the%20multi-view%20diffusion%20model%20uses%20the%203D-aware%0Amaps%20rendered%20by%20the%20reconstruction%20module%20at%20the%20previous%20timestep%20as%0Aadditional%20conditions.%20The%20recursive%20diffusion%20framework%20with%203D-aware%20feedback%0Aunites%20the%20entire%20process%20and%20improves%20geometric%20consistency.Experiments%20show%0Athat%20our%20framework%20outperforms%20separation%20of%20these%20two%20stages%20and%20existing%0Amethods%20that%20combine%20them%20at%20the%20inference%20phase.%20Project%20page%3A%0Ahttps%3A//costwen.github.io/Ouroboros3D/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOuroboros3D%253A%2520Image-to-3D%2520Generation%2520via%25203D-aware%2520Recursive%2520Diffusion%26entry.906535625%3DHao%2520Wen%2520and%2520Zehuan%2520Huang%2520and%2520Yaohui%2520Wang%2520and%2520Xinyuan%2520Chen%2520and%2520Lu%2520Sheng%26entry.1292438233%3D%2520%2520Existing%2520single%2520image-to-3D%2520creation%2520methods%2520typically%2520involve%2520a%2520two-stage%250Aprocess%252C%2520first%2520generating%2520multi-view%2520images%252C%2520and%2520then%2520using%2520these%2520images%2520for%25203D%250Areconstruction.%2520However%252C%2520training%2520these%2520two%2520stages%2520separately%2520leads%2520to%250Asignificant%2520data%2520bias%2520in%2520the%2520inference%2520phase%252C%2520thus%2520affecting%2520the%2520quality%2520of%250Areconstructed%2520results.%2520We%2520introduce%2520a%2520unified%25203D%2520generation%2520framework%252C%2520named%250AOuroboros3D%252C%2520which%2520integrates%2520diffusion-based%2520multi-view%2520image%2520generation%2520and%250A3D%2520reconstruction%2520into%2520a%2520recursive%2520diffusion%2520process.%2520In%2520our%2520framework%252C%2520these%250Atwo%2520modules%2520are%2520jointly%2520trained%2520through%2520a%2520self-conditioning%2520mechanism%252C%2520allowing%250Athem%2520to%2520adapt%2520to%2520each%2520other%2527s%2520characteristics%2520for%2520robust%2520inference.%2520During%2520the%250Amulti-view%2520denoising%2520process%252C%2520the%2520multi-view%2520diffusion%2520model%2520uses%2520the%25203D-aware%250Amaps%2520rendered%2520by%2520the%2520reconstruction%2520module%2520at%2520the%2520previous%2520timestep%2520as%250Aadditional%2520conditions.%2520The%2520recursive%2520diffusion%2520framework%2520with%25203D-aware%2520feedback%250Aunites%2520the%2520entire%2520process%2520and%2520improves%2520geometric%2520consistency.Experiments%2520show%250Athat%2520our%2520framework%2520outperforms%2520separation%2520of%2520these%2520two%2520stages%2520and%2520existing%250Amethods%2520that%2520combine%2520them%2520at%2520the%2520inference%2520phase.%2520Project%2520page%253A%250Ahttps%253A//costwen.github.io/Ouroboros3D/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ouroboros3D%3A%20Image-to-3D%20Generation%20via%203D-aware%20Recursive%20Diffusion&entry.906535625=Hao%20Wen%20and%20Zehuan%20Huang%20and%20Yaohui%20Wang%20and%20Xinyuan%20Chen%20and%20Lu%20Sheng&entry.1292438233=%20%20Existing%20single%20image-to-3D%20creation%20methods%20typically%20involve%20a%20two-stage%0Aprocess%2C%20first%20generating%20multi-view%20images%2C%20and%20then%20using%20these%20images%20for%203D%0Areconstruction.%20However%2C%20training%20these%20two%20stages%20separately%20leads%20to%0Asignificant%20data%20bias%20in%20the%20inference%20phase%2C%20thus%20affecting%20the%20quality%20of%0Areconstructed%20results.%20We%20introduce%20a%20unified%203D%20generation%20framework%2C%20named%0AOuroboros3D%2C%20which%20integrates%20diffusion-based%20multi-view%20image%20generation%20and%0A3D%20reconstruction%20into%20a%20recursive%20diffusion%20process.%20In%20our%20framework%2C%20these%0Atwo%20modules%20are%20jointly%20trained%20through%20a%20self-conditioning%20mechanism%2C%20allowing%0Athem%20to%20adapt%20to%20each%20other%27s%20characteristics%20for%20robust%20inference.%20During%20the%0Amulti-view%20denoising%20process%2C%20the%20multi-view%20diffusion%20model%20uses%20the%203D-aware%0Amaps%20rendered%20by%20the%20reconstruction%20module%20at%20the%20previous%20timestep%20as%0Aadditional%20conditions.%20The%20recursive%20diffusion%20framework%20with%203D-aware%20feedback%0Aunites%20the%20entire%20process%20and%20improves%20geometric%20consistency.Experiments%20show%0Athat%20our%20framework%20outperforms%20separation%20of%20these%20two%20stages%20and%20existing%0Amethods%20that%20combine%20them%20at%20the%20inference%20phase.%20Project%20page%3A%0Ahttps%3A//costwen.github.io/Ouroboros3D/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03184v2&entry.124074799=Read"},
{"title": "Geometry-aware Active Learning of Spatiotemporal Dynamic Systems", "author": "Xizhuo Zhang and Bing Yao", "abstract": "  Rapid developments in advanced sensing and imaging have significantly\nenhanced information visibility, opening opportunities for predictive modeling\nof complex dynamic systems. However, sensing signals acquired from such complex\nsystems are often distributed across 3D geometries and rapidly evolving over\ntime, posing significant challenges in spatiotemporal predictive modeling. This\npaper proposes a geometry-aware active learning framework for modeling\nspatiotemporal dynamic systems. Specifically, we propose a geometry-aware\nspatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal\ncorrelations and geometric manifold features for reliable prediction of\nhigh-dimensional dynamic behaviors. In addition, we develop an adaptive active\nlearning strategy to strategically identify informative spatial locations for\ndata collection and further maximize the prediction accuracy. This strategy\nachieves the adaptive trade-off between the prediction uncertainty in the\nG-ST-GP model and the space-filling design guided by the geodesic distance\nacross the 3D geometry. We implement the proposed framework to model the\nspatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments\nshow that our framework outperforms traditional methods lacking the mechanism\nof geometric information incorporation or effective data collection.\n", "link": "http://arxiv.org/abs/2504.19012v2", "date": "2025-05-01", "relevancy": 2.782, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5704}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-aware%20Active%20Learning%20of%20Spatiotemporal%20Dynamic%20Systems&body=Title%3A%20Geometry-aware%20Active%20Learning%20of%20Spatiotemporal%20Dynamic%20Systems%0AAuthor%3A%20Xizhuo%20Zhang%20and%20Bing%20Yao%0AAbstract%3A%20%20%20Rapid%20developments%20in%20advanced%20sensing%20and%20imaging%20have%20significantly%0Aenhanced%20information%20visibility%2C%20opening%20opportunities%20for%20predictive%20modeling%0Aof%20complex%20dynamic%20systems.%20However%2C%20sensing%20signals%20acquired%20from%20such%20complex%0Asystems%20are%20often%20distributed%20across%203D%20geometries%20and%20rapidly%20evolving%20over%0Atime%2C%20posing%20significant%20challenges%20in%20spatiotemporal%20predictive%20modeling.%20This%0Apaper%20proposes%20a%20geometry-aware%20active%20learning%20framework%20for%20modeling%0Aspatiotemporal%20dynamic%20systems.%20Specifically%2C%20we%20propose%20a%20geometry-aware%0Aspatiotemporal%20Gaussian%20Process%20%28G-ST-GP%29%20to%20effectively%20integrate%20the%20temporal%0Acorrelations%20and%20geometric%20manifold%20features%20for%20reliable%20prediction%20of%0Ahigh-dimensional%20dynamic%20behaviors.%20In%20addition%2C%20we%20develop%20an%20adaptive%20active%0Alearning%20strategy%20to%20strategically%20identify%20informative%20spatial%20locations%20for%0Adata%20collection%20and%20further%20maximize%20the%20prediction%20accuracy.%20This%20strategy%0Aachieves%20the%20adaptive%20trade-off%20between%20the%20prediction%20uncertainty%20in%20the%0AG-ST-GP%20model%20and%20the%20space-filling%20design%20guided%20by%20the%20geodesic%20distance%0Aacross%20the%203D%20geometry.%20We%20implement%20the%20proposed%20framework%20to%20model%20the%0Aspatiotemporal%20electrodynamics%20in%20a%203D%20heart%20geometry.%20Numerical%20experiments%0Ashow%20that%20our%20framework%20outperforms%20traditional%20methods%20lacking%20the%20mechanism%0Aof%20geometric%20information%20incorporation%20or%20effective%20data%20collection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-aware%2520Active%2520Learning%2520of%2520Spatiotemporal%2520Dynamic%2520Systems%26entry.906535625%3DXizhuo%2520Zhang%2520and%2520Bing%2520Yao%26entry.1292438233%3D%2520%2520Rapid%2520developments%2520in%2520advanced%2520sensing%2520and%2520imaging%2520have%2520significantly%250Aenhanced%2520information%2520visibility%252C%2520opening%2520opportunities%2520for%2520predictive%2520modeling%250Aof%2520complex%2520dynamic%2520systems.%2520However%252C%2520sensing%2520signals%2520acquired%2520from%2520such%2520complex%250Asystems%2520are%2520often%2520distributed%2520across%25203D%2520geometries%2520and%2520rapidly%2520evolving%2520over%250Atime%252C%2520posing%2520significant%2520challenges%2520in%2520spatiotemporal%2520predictive%2520modeling.%2520This%250Apaper%2520proposes%2520a%2520geometry-aware%2520active%2520learning%2520framework%2520for%2520modeling%250Aspatiotemporal%2520dynamic%2520systems.%2520Specifically%252C%2520we%2520propose%2520a%2520geometry-aware%250Aspatiotemporal%2520Gaussian%2520Process%2520%2528G-ST-GP%2529%2520to%2520effectively%2520integrate%2520the%2520temporal%250Acorrelations%2520and%2520geometric%2520manifold%2520features%2520for%2520reliable%2520prediction%2520of%250Ahigh-dimensional%2520dynamic%2520behaviors.%2520In%2520addition%252C%2520we%2520develop%2520an%2520adaptive%2520active%250Alearning%2520strategy%2520to%2520strategically%2520identify%2520informative%2520spatial%2520locations%2520for%250Adata%2520collection%2520and%2520further%2520maximize%2520the%2520prediction%2520accuracy.%2520This%2520strategy%250Aachieves%2520the%2520adaptive%2520trade-off%2520between%2520the%2520prediction%2520uncertainty%2520in%2520the%250AG-ST-GP%2520model%2520and%2520the%2520space-filling%2520design%2520guided%2520by%2520the%2520geodesic%2520distance%250Aacross%2520the%25203D%2520geometry.%2520We%2520implement%2520the%2520proposed%2520framework%2520to%2520model%2520the%250Aspatiotemporal%2520electrodynamics%2520in%2520a%25203D%2520heart%2520geometry.%2520Numerical%2520experiments%250Ashow%2520that%2520our%2520framework%2520outperforms%2520traditional%2520methods%2520lacking%2520the%2520mechanism%250Aof%2520geometric%2520information%2520incorporation%2520or%2520effective%2520data%2520collection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-aware%20Active%20Learning%20of%20Spatiotemporal%20Dynamic%20Systems&entry.906535625=Xizhuo%20Zhang%20and%20Bing%20Yao&entry.1292438233=%20%20Rapid%20developments%20in%20advanced%20sensing%20and%20imaging%20have%20significantly%0Aenhanced%20information%20visibility%2C%20opening%20opportunities%20for%20predictive%20modeling%0Aof%20complex%20dynamic%20systems.%20However%2C%20sensing%20signals%20acquired%20from%20such%20complex%0Asystems%20are%20often%20distributed%20across%203D%20geometries%20and%20rapidly%20evolving%20over%0Atime%2C%20posing%20significant%20challenges%20in%20spatiotemporal%20predictive%20modeling.%20This%0Apaper%20proposes%20a%20geometry-aware%20active%20learning%20framework%20for%20modeling%0Aspatiotemporal%20dynamic%20systems.%20Specifically%2C%20we%20propose%20a%20geometry-aware%0Aspatiotemporal%20Gaussian%20Process%20%28G-ST-GP%29%20to%20effectively%20integrate%20the%20temporal%0Acorrelations%20and%20geometric%20manifold%20features%20for%20reliable%20prediction%20of%0Ahigh-dimensional%20dynamic%20behaviors.%20In%20addition%2C%20we%20develop%20an%20adaptive%20active%0Alearning%20strategy%20to%20strategically%20identify%20informative%20spatial%20locations%20for%0Adata%20collection%20and%20further%20maximize%20the%20prediction%20accuracy.%20This%20strategy%0Aachieves%20the%20adaptive%20trade-off%20between%20the%20prediction%20uncertainty%20in%20the%0AG-ST-GP%20model%20and%20the%20space-filling%20design%20guided%20by%20the%20geodesic%20distance%0Aacross%20the%203D%20geometry.%20We%20implement%20the%20proposed%20framework%20to%20model%20the%0Aspatiotemporal%20electrodynamics%20in%20a%203D%20heart%20geometry.%20Numerical%20experiments%0Ashow%20that%20our%20framework%20outperforms%20traditional%20methods%20lacking%20the%20mechanism%0Aof%20geometric%20information%20incorporation%20or%20effective%20data%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19012v2&entry.124074799=Read"},
{"title": "MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling", "author": "Fadeel Sher Khan and Joshua Ebenezer and Hamid Sheikh and Seok-Jun Lee", "abstract": "  Smartphone cameras have become ubiquitous imaging tools, yet their small\nsensors and compact optics often limit spatial resolution and introduce\ndistortions. Combining information from multiple low-resolution (LR) frames to\nproduce a high-resolution (HR) image has been explored to overcome the inherent\nlimitations of smartphone cameras. Despite the promise of multi-frame\nsuper-resolution (MFSR), current approaches are hindered by datasets that fail\nto capture the characteristic noise and motion patterns found in real-world\nhandheld burst images. In this work, we address this gap by introducing a novel\nsynthetic data engine that uses multi-exposure static images to synthesize\nLR-HR training pairs while preserving sensor-specific noise characteristics and\nimage motion found during handheld burst photography. We also propose MFSR-GAN:\na multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches,\nMFSR-GAN emphasizes a \"base frame\" throughout its architecture to mitigate\nartifacts. Experimental results on both synthetic and real data demonstrates\nthat MFSR-GAN trained with our synthetic engine yields sharper, more realistic\nreconstructions than existing methods for real-world MFSR.\n", "link": "http://arxiv.org/abs/2502.20824v2", "date": "2025-05-01", "relevancy": 2.6903, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5795}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5182}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MFSR-GAN%3A%20Multi-Frame%20Super-Resolution%20with%20Handheld%20Motion%20Modeling&body=Title%3A%20MFSR-GAN%3A%20Multi-Frame%20Super-Resolution%20with%20Handheld%20Motion%20Modeling%0AAuthor%3A%20Fadeel%20Sher%20Khan%20and%20Joshua%20Ebenezer%20and%20Hamid%20Sheikh%20and%20Seok-Jun%20Lee%0AAbstract%3A%20%20%20Smartphone%20cameras%20have%20become%20ubiquitous%20imaging%20tools%2C%20yet%20their%20small%0Asensors%20and%20compact%20optics%20often%20limit%20spatial%20resolution%20and%20introduce%0Adistortions.%20Combining%20information%20from%20multiple%20low-resolution%20%28LR%29%20frames%20to%0Aproduce%20a%20high-resolution%20%28HR%29%20image%20has%20been%20explored%20to%20overcome%20the%20inherent%0Alimitations%20of%20smartphone%20cameras.%20Despite%20the%20promise%20of%20multi-frame%0Asuper-resolution%20%28MFSR%29%2C%20current%20approaches%20are%20hindered%20by%20datasets%20that%20fail%0Ato%20capture%20the%20characteristic%20noise%20and%20motion%20patterns%20found%20in%20real-world%0Ahandheld%20burst%20images.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20a%20novel%0Asynthetic%20data%20engine%20that%20uses%20multi-exposure%20static%20images%20to%20synthesize%0ALR-HR%20training%20pairs%20while%20preserving%20sensor-specific%20noise%20characteristics%20and%0Aimage%20motion%20found%20during%20handheld%20burst%20photography.%20We%20also%20propose%20MFSR-GAN%3A%0Aa%20multi-scale%20RAW-to-RGB%20network%20for%20MFSR.%20Compared%20to%20prior%20approaches%2C%0AMFSR-GAN%20emphasizes%20a%20%22base%20frame%22%20throughout%20its%20architecture%20to%20mitigate%0Aartifacts.%20Experimental%20results%20on%20both%20synthetic%20and%20real%20data%20demonstrates%0Athat%20MFSR-GAN%20trained%20with%20our%20synthetic%20engine%20yields%20sharper%2C%20more%20realistic%0Areconstructions%20than%20existing%20methods%20for%20real-world%20MFSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMFSR-GAN%253A%2520Multi-Frame%2520Super-Resolution%2520with%2520Handheld%2520Motion%2520Modeling%26entry.906535625%3DFadeel%2520Sher%2520Khan%2520and%2520Joshua%2520Ebenezer%2520and%2520Hamid%2520Sheikh%2520and%2520Seok-Jun%2520Lee%26entry.1292438233%3D%2520%2520Smartphone%2520cameras%2520have%2520become%2520ubiquitous%2520imaging%2520tools%252C%2520yet%2520their%2520small%250Asensors%2520and%2520compact%2520optics%2520often%2520limit%2520spatial%2520resolution%2520and%2520introduce%250Adistortions.%2520Combining%2520information%2520from%2520multiple%2520low-resolution%2520%2528LR%2529%2520frames%2520to%250Aproduce%2520a%2520high-resolution%2520%2528HR%2529%2520image%2520has%2520been%2520explored%2520to%2520overcome%2520the%2520inherent%250Alimitations%2520of%2520smartphone%2520cameras.%2520Despite%2520the%2520promise%2520of%2520multi-frame%250Asuper-resolution%2520%2528MFSR%2529%252C%2520current%2520approaches%2520are%2520hindered%2520by%2520datasets%2520that%2520fail%250Ato%2520capture%2520the%2520characteristic%2520noise%2520and%2520motion%2520patterns%2520found%2520in%2520real-world%250Ahandheld%2520burst%2520images.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520a%2520novel%250Asynthetic%2520data%2520engine%2520that%2520uses%2520multi-exposure%2520static%2520images%2520to%2520synthesize%250ALR-HR%2520training%2520pairs%2520while%2520preserving%2520sensor-specific%2520noise%2520characteristics%2520and%250Aimage%2520motion%2520found%2520during%2520handheld%2520burst%2520photography.%2520We%2520also%2520propose%2520MFSR-GAN%253A%250Aa%2520multi-scale%2520RAW-to-RGB%2520network%2520for%2520MFSR.%2520Compared%2520to%2520prior%2520approaches%252C%250AMFSR-GAN%2520emphasizes%2520a%2520%2522base%2520frame%2522%2520throughout%2520its%2520architecture%2520to%2520mitigate%250Aartifacts.%2520Experimental%2520results%2520on%2520both%2520synthetic%2520and%2520real%2520data%2520demonstrates%250Athat%2520MFSR-GAN%2520trained%2520with%2520our%2520synthetic%2520engine%2520yields%2520sharper%252C%2520more%2520realistic%250Areconstructions%2520than%2520existing%2520methods%2520for%2520real-world%2520MFSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MFSR-GAN%3A%20Multi-Frame%20Super-Resolution%20with%20Handheld%20Motion%20Modeling&entry.906535625=Fadeel%20Sher%20Khan%20and%20Joshua%20Ebenezer%20and%20Hamid%20Sheikh%20and%20Seok-Jun%20Lee&entry.1292438233=%20%20Smartphone%20cameras%20have%20become%20ubiquitous%20imaging%20tools%2C%20yet%20their%20small%0Asensors%20and%20compact%20optics%20often%20limit%20spatial%20resolution%20and%20introduce%0Adistortions.%20Combining%20information%20from%20multiple%20low-resolution%20%28LR%29%20frames%20to%0Aproduce%20a%20high-resolution%20%28HR%29%20image%20has%20been%20explored%20to%20overcome%20the%20inherent%0Alimitations%20of%20smartphone%20cameras.%20Despite%20the%20promise%20of%20multi-frame%0Asuper-resolution%20%28MFSR%29%2C%20current%20approaches%20are%20hindered%20by%20datasets%20that%20fail%0Ato%20capture%20the%20characteristic%20noise%20and%20motion%20patterns%20found%20in%20real-world%0Ahandheld%20burst%20images.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20a%20novel%0Asynthetic%20data%20engine%20that%20uses%20multi-exposure%20static%20images%20to%20synthesize%0ALR-HR%20training%20pairs%20while%20preserving%20sensor-specific%20noise%20characteristics%20and%0Aimage%20motion%20found%20during%20handheld%20burst%20photography.%20We%20also%20propose%20MFSR-GAN%3A%0Aa%20multi-scale%20RAW-to-RGB%20network%20for%20MFSR.%20Compared%20to%20prior%20approaches%2C%0AMFSR-GAN%20emphasizes%20a%20%22base%20frame%22%20throughout%20its%20architecture%20to%20mitigate%0Aartifacts.%20Experimental%20results%20on%20both%20synthetic%20and%20real%20data%20demonstrates%0Athat%20MFSR-GAN%20trained%20with%20our%20synthetic%20engine%20yields%20sharper%2C%20more%20realistic%0Areconstructions%20than%20existing%20methods%20for%20real-world%20MFSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20824v2&entry.124074799=Read"},
{"title": "Cognitive Neural Architecture Search Reveals Hierarchical Entailment", "author": "Lukas Kuhn and Sari Saba-Sadiya and Gemma Roig", "abstract": "  Recent research has suggested that the brain is more shallow than previously\nthought, challenging the traditionally assumed hierarchical structure of the\nventral visual pathway. Here, we demonstrate that optimizing convolutional\nnetwork architectures for brain-alignment via evolutionary neural architecture\nsearch results in models with clear representational hierarchies. Despite\nhaving random weights, the identified models achieve brain-alignment scores\nsurpassing even those of pretrained classification models - as measured by both\nregression and representational similarity analysis. Furthermore, through\ntraditional supervised training, architectures optimized for alignment with\nlate ventral regions become competitive classification models. These findings\nsuggest that hierarchical structure is a fundamental mechanism of primate\nvisual processing. Finally, this work demonstrates the potential of neural\narchitecture search as a framework for computational cognitive neuroscience\nresearch that could reduce the field's reliance on manually designed\nconvolutional networks.\n", "link": "http://arxiv.org/abs/2502.11141v2", "date": "2025-05-01", "relevancy": 2.6377, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Neural%20Architecture%20Search%20Reveals%20Hierarchical%20Entailment&body=Title%3A%20Cognitive%20Neural%20Architecture%20Search%20Reveals%20Hierarchical%20Entailment%0AAuthor%3A%20Lukas%20Kuhn%20and%20Sari%20Saba-Sadiya%20and%20Gemma%20Roig%0AAbstract%3A%20%20%20Recent%20research%20has%20suggested%20that%20the%20brain%20is%20more%20shallow%20than%20previously%0Athought%2C%20challenging%20the%20traditionally%20assumed%20hierarchical%20structure%20of%20the%0Aventral%20visual%20pathway.%20Here%2C%20we%20demonstrate%20that%20optimizing%20convolutional%0Anetwork%20architectures%20for%20brain-alignment%20via%20evolutionary%20neural%20architecture%0Asearch%20results%20in%20models%20with%20clear%20representational%20hierarchies.%20Despite%0Ahaving%20random%20weights%2C%20the%20identified%20models%20achieve%20brain-alignment%20scores%0Asurpassing%20even%20those%20of%20pretrained%20classification%20models%20-%20as%20measured%20by%20both%0Aregression%20and%20representational%20similarity%20analysis.%20Furthermore%2C%20through%0Atraditional%20supervised%20training%2C%20architectures%20optimized%20for%20alignment%20with%0Alate%20ventral%20regions%20become%20competitive%20classification%20models.%20These%20findings%0Asuggest%20that%20hierarchical%20structure%20is%20a%20fundamental%20mechanism%20of%20primate%0Avisual%20processing.%20Finally%2C%20this%20work%20demonstrates%20the%20potential%20of%20neural%0Aarchitecture%20search%20as%20a%20framework%20for%20computational%20cognitive%20neuroscience%0Aresearch%20that%20could%20reduce%20the%20field%27s%20reliance%20on%20manually%20designed%0Aconvolutional%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Neural%2520Architecture%2520Search%2520Reveals%2520Hierarchical%2520Entailment%26entry.906535625%3DLukas%2520Kuhn%2520and%2520Sari%2520Saba-Sadiya%2520and%2520Gemma%2520Roig%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520suggested%2520that%2520the%2520brain%2520is%2520more%2520shallow%2520than%2520previously%250Athought%252C%2520challenging%2520the%2520traditionally%2520assumed%2520hierarchical%2520structure%2520of%2520the%250Aventral%2520visual%2520pathway.%2520Here%252C%2520we%2520demonstrate%2520that%2520optimizing%2520convolutional%250Anetwork%2520architectures%2520for%2520brain-alignment%2520via%2520evolutionary%2520neural%2520architecture%250Asearch%2520results%2520in%2520models%2520with%2520clear%2520representational%2520hierarchies.%2520Despite%250Ahaving%2520random%2520weights%252C%2520the%2520identified%2520models%2520achieve%2520brain-alignment%2520scores%250Asurpassing%2520even%2520those%2520of%2520pretrained%2520classification%2520models%2520-%2520as%2520measured%2520by%2520both%250Aregression%2520and%2520representational%2520similarity%2520analysis.%2520Furthermore%252C%2520through%250Atraditional%2520supervised%2520training%252C%2520architectures%2520optimized%2520for%2520alignment%2520with%250Alate%2520ventral%2520regions%2520become%2520competitive%2520classification%2520models.%2520These%2520findings%250Asuggest%2520that%2520hierarchical%2520structure%2520is%2520a%2520fundamental%2520mechanism%2520of%2520primate%250Avisual%2520processing.%2520Finally%252C%2520this%2520work%2520demonstrates%2520the%2520potential%2520of%2520neural%250Aarchitecture%2520search%2520as%2520a%2520framework%2520for%2520computational%2520cognitive%2520neuroscience%250Aresearch%2520that%2520could%2520reduce%2520the%2520field%2527s%2520reliance%2520on%2520manually%2520designed%250Aconvolutional%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Neural%20Architecture%20Search%20Reveals%20Hierarchical%20Entailment&entry.906535625=Lukas%20Kuhn%20and%20Sari%20Saba-Sadiya%20and%20Gemma%20Roig&entry.1292438233=%20%20Recent%20research%20has%20suggested%20that%20the%20brain%20is%20more%20shallow%20than%20previously%0Athought%2C%20challenging%20the%20traditionally%20assumed%20hierarchical%20structure%20of%20the%0Aventral%20visual%20pathway.%20Here%2C%20we%20demonstrate%20that%20optimizing%20convolutional%0Anetwork%20architectures%20for%20brain-alignment%20via%20evolutionary%20neural%20architecture%0Asearch%20results%20in%20models%20with%20clear%20representational%20hierarchies.%20Despite%0Ahaving%20random%20weights%2C%20the%20identified%20models%20achieve%20brain-alignment%20scores%0Asurpassing%20even%20those%20of%20pretrained%20classification%20models%20-%20as%20measured%20by%20both%0Aregression%20and%20representational%20similarity%20analysis.%20Furthermore%2C%20through%0Atraditional%20supervised%20training%2C%20architectures%20optimized%20for%20alignment%20with%0Alate%20ventral%20regions%20become%20competitive%20classification%20models.%20These%20findings%0Asuggest%20that%20hierarchical%20structure%20is%20a%20fundamental%20mechanism%20of%20primate%0Avisual%20processing.%20Finally%2C%20this%20work%20demonstrates%20the%20potential%20of%20neural%0Aarchitecture%20search%20as%20a%20framework%20for%20computational%20cognitive%20neuroscience%0Aresearch%20that%20could%20reduce%20the%20field%27s%20reliance%20on%20manually%20designed%0Aconvolutional%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11141v2&entry.124074799=Read"},
{"title": "Scene-Conditional 3D Object Stylization and Composition", "author": "Jinghao Zhou and Tomas Jakab and Philip Torr and Christian Rupprecht", "abstract": "  Recently, 3D generative models have made impressive progress, enabling the\ngeneration of almost arbitrary 3D assets from text or image inputs. However,\nthese approaches generate objects in isolation without any consideration for\nthe scene where they will eventually be placed. In this paper, we propose a\nframework that allows for the stylization of an existing 3D asset to fit into a\ngiven 2D scene, and additionally produce a photorealistic composition as if the\nasset was placed within the environment. This not only opens up a new level of\ncontrol for object stylization, for example, the same assets can be stylized to\nreflect changes in the environment, such as summer to winter or fantasy versus\nfuturistic settings-but also makes the object-scene composition more\ncontrollable. We achieve this by combining modeling and optimizing the object's\ntexture and environmental lighting through differentiable ray tracing with\nimage priors from pre-trained text-to-image diffusion models. We demonstrate\nthat our method is applicable to a wide variety of indoor and outdoor scenes\nand arbitrary objects. Project page:\nhttps://jensenzhoujh.github.io/scene-cond-3d/.\n", "link": "http://arxiv.org/abs/2312.12419v2", "date": "2025-05-01", "relevancy": 2.6169, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6689}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6513}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene-Conditional%203D%20Object%20Stylization%20and%20Composition&body=Title%3A%20Scene-Conditional%203D%20Object%20Stylization%20and%20Composition%0AAuthor%3A%20Jinghao%20Zhou%20and%20Tomas%20Jakab%20and%20Philip%20Torr%20and%20Christian%20Rupprecht%0AAbstract%3A%20%20%20Recently%2C%203D%20generative%20models%20have%20made%20impressive%20progress%2C%20enabling%20the%0Ageneration%20of%20almost%20arbitrary%203D%20assets%20from%20text%20or%20image%20inputs.%20However%2C%0Athese%20approaches%20generate%20objects%20in%20isolation%20without%20any%20consideration%20for%0Athe%20scene%20where%20they%20will%20eventually%20be%20placed.%20In%20this%20paper%2C%20we%20propose%20a%0Aframework%20that%20allows%20for%20the%20stylization%20of%20an%20existing%203D%20asset%20to%20fit%20into%20a%0Agiven%202D%20scene%2C%20and%20additionally%20produce%20a%20photorealistic%20composition%20as%20if%20the%0Aasset%20was%20placed%20within%20the%20environment.%20This%20not%20only%20opens%20up%20a%20new%20level%20of%0Acontrol%20for%20object%20stylization%2C%20for%20example%2C%20the%20same%20assets%20can%20be%20stylized%20to%0Areflect%20changes%20in%20the%20environment%2C%20such%20as%20summer%20to%20winter%20or%20fantasy%20versus%0Afuturistic%20settings-but%20also%20makes%20the%20object-scene%20composition%20more%0Acontrollable.%20We%20achieve%20this%20by%20combining%20modeling%20and%20optimizing%20the%20object%27s%0Atexture%20and%20environmental%20lighting%20through%20differentiable%20ray%20tracing%20with%0Aimage%20priors%20from%20pre-trained%20text-to-image%20diffusion%20models.%20We%20demonstrate%0Athat%20our%20method%20is%20applicable%20to%20a%20wide%20variety%20of%20indoor%20and%20outdoor%20scenes%0Aand%20arbitrary%20objects.%20Project%20page%3A%0Ahttps%3A//jensenzhoujh.github.io/scene-cond-3d/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene-Conditional%25203D%2520Object%2520Stylization%2520and%2520Composition%26entry.906535625%3DJinghao%2520Zhou%2520and%2520Tomas%2520Jakab%2520and%2520Philip%2520Torr%2520and%2520Christian%2520Rupprecht%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520generative%2520models%2520have%2520made%2520impressive%2520progress%252C%2520enabling%2520the%250Ageneration%2520of%2520almost%2520arbitrary%25203D%2520assets%2520from%2520text%2520or%2520image%2520inputs.%2520However%252C%250Athese%2520approaches%2520generate%2520objects%2520in%2520isolation%2520without%2520any%2520consideration%2520for%250Athe%2520scene%2520where%2520they%2520will%2520eventually%2520be%2520placed.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aframework%2520that%2520allows%2520for%2520the%2520stylization%2520of%2520an%2520existing%25203D%2520asset%2520to%2520fit%2520into%2520a%250Agiven%25202D%2520scene%252C%2520and%2520additionally%2520produce%2520a%2520photorealistic%2520composition%2520as%2520if%2520the%250Aasset%2520was%2520placed%2520within%2520the%2520environment.%2520This%2520not%2520only%2520opens%2520up%2520a%2520new%2520level%2520of%250Acontrol%2520for%2520object%2520stylization%252C%2520for%2520example%252C%2520the%2520same%2520assets%2520can%2520be%2520stylized%2520to%250Areflect%2520changes%2520in%2520the%2520environment%252C%2520such%2520as%2520summer%2520to%2520winter%2520or%2520fantasy%2520versus%250Afuturistic%2520settings-but%2520also%2520makes%2520the%2520object-scene%2520composition%2520more%250Acontrollable.%2520We%2520achieve%2520this%2520by%2520combining%2520modeling%2520and%2520optimizing%2520the%2520object%2527s%250Atexture%2520and%2520environmental%2520lighting%2520through%2520differentiable%2520ray%2520tracing%2520with%250Aimage%2520priors%2520from%2520pre-trained%2520text-to-image%2520diffusion%2520models.%2520We%2520demonstrate%250Athat%2520our%2520method%2520is%2520applicable%2520to%2520a%2520wide%2520variety%2520of%2520indoor%2520and%2520outdoor%2520scenes%250Aand%2520arbitrary%2520objects.%2520Project%2520page%253A%250Ahttps%253A//jensenzhoujh.github.io/scene-cond-3d/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene-Conditional%203D%20Object%20Stylization%20and%20Composition&entry.906535625=Jinghao%20Zhou%20and%20Tomas%20Jakab%20and%20Philip%20Torr%20and%20Christian%20Rupprecht&entry.1292438233=%20%20Recently%2C%203D%20generative%20models%20have%20made%20impressive%20progress%2C%20enabling%20the%0Ageneration%20of%20almost%20arbitrary%203D%20assets%20from%20text%20or%20image%20inputs.%20However%2C%0Athese%20approaches%20generate%20objects%20in%20isolation%20without%20any%20consideration%20for%0Athe%20scene%20where%20they%20will%20eventually%20be%20placed.%20In%20this%20paper%2C%20we%20propose%20a%0Aframework%20that%20allows%20for%20the%20stylization%20of%20an%20existing%203D%20asset%20to%20fit%20into%20a%0Agiven%202D%20scene%2C%20and%20additionally%20produce%20a%20photorealistic%20composition%20as%20if%20the%0Aasset%20was%20placed%20within%20the%20environment.%20This%20not%20only%20opens%20up%20a%20new%20level%20of%0Acontrol%20for%20object%20stylization%2C%20for%20example%2C%20the%20same%20assets%20can%20be%20stylized%20to%0Areflect%20changes%20in%20the%20environment%2C%20such%20as%20summer%20to%20winter%20or%20fantasy%20versus%0Afuturistic%20settings-but%20also%20makes%20the%20object-scene%20composition%20more%0Acontrollable.%20We%20achieve%20this%20by%20combining%20modeling%20and%20optimizing%20the%20object%27s%0Atexture%20and%20environmental%20lighting%20through%20differentiable%20ray%20tracing%20with%0Aimage%20priors%20from%20pre-trained%20text-to-image%20diffusion%20models.%20We%20demonstrate%0Athat%20our%20method%20is%20applicable%20to%20a%20wide%20variety%20of%20indoor%20and%20outdoor%20scenes%0Aand%20arbitrary%20objects.%20Project%20page%3A%0Ahttps%3A//jensenzhoujh.github.io/scene-cond-3d/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12419v2&entry.124074799=Read"},
{"title": "OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation\n  Learning", "author": "Anirudhan Badrinath and Alex Yang and Kousik Rajesh and Prabhat Agarwal and Jaewon Yang and Haoyu Chen and Jiajing Xu and Charles Rosenberg", "abstract": "  Representation learning, a task of learning latent vectors to represent\nentities, is a key task in improving search and recommender systems in web\napplications. Various representation learning methods have been developed,\nincluding graph-based approaches for relationships among entities,\nsequence-based methods for capturing the temporal evolution of user activities,\nand content-based models for leveraging text and visual content. However, the\ndevelopment of a unifying framework that integrates these diverse techniques to\nsupport multiple applications remains a significant challenge. This paper\npresents OmniSage, a large-scale representation framework that learns universal\nrepresentations for a variety of applications at Pinterest. OmniSage integrates\ngraph neural networks with content-based models and user sequence models by\nemploying multiple contrastive learning tasks to effectively process graph\ndata, user sequence data, and content signals. To support the training and\ninference of OmniSage, we developed an efficient infrastructure capable of\nsupporting Pinterest graphs with billions of nodes. The universal\nrepresentations generated by OmniSage have significantly enhanced user\nexperiences on Pinterest, leading to an approximate 2.5% increase in sitewide\nrepins (saves) across five applications. This paper highlights the impact of\nunifying representation learning methods, and we will open source the OmniSage\ncode by the time of publication.\n", "link": "http://arxiv.org/abs/2504.17811v2", "date": "2025-05-01", "relevancy": 2.6143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSage%3A%20Large%20Scale%2C%20Multi-Entity%20Heterogeneous%20Graph%20Representation%0A%20%20Learning&body=Title%3A%20OmniSage%3A%20Large%20Scale%2C%20Multi-Entity%20Heterogeneous%20Graph%20Representation%0A%20%20Learning%0AAuthor%3A%20Anirudhan%20Badrinath%20and%20Alex%20Yang%20and%20Kousik%20Rajesh%20and%20Prabhat%20Agarwal%20and%20Jaewon%20Yang%20and%20Haoyu%20Chen%20and%20Jiajing%20Xu%20and%20Charles%20Rosenberg%0AAbstract%3A%20%20%20Representation%20learning%2C%20a%20task%20of%20learning%20latent%20vectors%20to%20represent%0Aentities%2C%20is%20a%20key%20task%20in%20improving%20search%20and%20recommender%20systems%20in%20web%0Aapplications.%20Various%20representation%20learning%20methods%20have%20been%20developed%2C%0Aincluding%20graph-based%20approaches%20for%20relationships%20among%20entities%2C%0Asequence-based%20methods%20for%20capturing%20the%20temporal%20evolution%20of%20user%20activities%2C%0Aand%20content-based%20models%20for%20leveraging%20text%20and%20visual%20content.%20However%2C%20the%0Adevelopment%20of%20a%20unifying%20framework%20that%20integrates%20these%20diverse%20techniques%20to%0Asupport%20multiple%20applications%20remains%20a%20significant%20challenge.%20This%20paper%0Apresents%20OmniSage%2C%20a%20large-scale%20representation%20framework%20that%20learns%20universal%0Arepresentations%20for%20a%20variety%20of%20applications%20at%20Pinterest.%20OmniSage%20integrates%0Agraph%20neural%20networks%20with%20content-based%20models%20and%20user%20sequence%20models%20by%0Aemploying%20multiple%20contrastive%20learning%20tasks%20to%20effectively%20process%20graph%0Adata%2C%20user%20sequence%20data%2C%20and%20content%20signals.%20To%20support%20the%20training%20and%0Ainference%20of%20OmniSage%2C%20we%20developed%20an%20efficient%20infrastructure%20capable%20of%0Asupporting%20Pinterest%20graphs%20with%20billions%20of%20nodes.%20The%20universal%0Arepresentations%20generated%20by%20OmniSage%20have%20significantly%20enhanced%20user%0Aexperiences%20on%20Pinterest%2C%20leading%20to%20an%20approximate%202.5%25%20increase%20in%20sitewide%0Arepins%20%28saves%29%20across%20five%20applications.%20This%20paper%20highlights%20the%20impact%20of%0Aunifying%20representation%20learning%20methods%2C%20and%20we%20will%20open%20source%20the%20OmniSage%0Acode%20by%20the%20time%20of%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17811v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSage%253A%2520Large%2520Scale%252C%2520Multi-Entity%2520Heterogeneous%2520Graph%2520Representation%250A%2520%2520Learning%26entry.906535625%3DAnirudhan%2520Badrinath%2520and%2520Alex%2520Yang%2520and%2520Kousik%2520Rajesh%2520and%2520Prabhat%2520Agarwal%2520and%2520Jaewon%2520Yang%2520and%2520Haoyu%2520Chen%2520and%2520Jiajing%2520Xu%2520and%2520Charles%2520Rosenberg%26entry.1292438233%3D%2520%2520Representation%2520learning%252C%2520a%2520task%2520of%2520learning%2520latent%2520vectors%2520to%2520represent%250Aentities%252C%2520is%2520a%2520key%2520task%2520in%2520improving%2520search%2520and%2520recommender%2520systems%2520in%2520web%250Aapplications.%2520Various%2520representation%2520learning%2520methods%2520have%2520been%2520developed%252C%250Aincluding%2520graph-based%2520approaches%2520for%2520relationships%2520among%2520entities%252C%250Asequence-based%2520methods%2520for%2520capturing%2520the%2520temporal%2520evolution%2520of%2520user%2520activities%252C%250Aand%2520content-based%2520models%2520for%2520leveraging%2520text%2520and%2520visual%2520content.%2520However%252C%2520the%250Adevelopment%2520of%2520a%2520unifying%2520framework%2520that%2520integrates%2520these%2520diverse%2520techniques%2520to%250Asupport%2520multiple%2520applications%2520remains%2520a%2520significant%2520challenge.%2520This%2520paper%250Apresents%2520OmniSage%252C%2520a%2520large-scale%2520representation%2520framework%2520that%2520learns%2520universal%250Arepresentations%2520for%2520a%2520variety%2520of%2520applications%2520at%2520Pinterest.%2520OmniSage%2520integrates%250Agraph%2520neural%2520networks%2520with%2520content-based%2520models%2520and%2520user%2520sequence%2520models%2520by%250Aemploying%2520multiple%2520contrastive%2520learning%2520tasks%2520to%2520effectively%2520process%2520graph%250Adata%252C%2520user%2520sequence%2520data%252C%2520and%2520content%2520signals.%2520To%2520support%2520the%2520training%2520and%250Ainference%2520of%2520OmniSage%252C%2520we%2520developed%2520an%2520efficient%2520infrastructure%2520capable%2520of%250Asupporting%2520Pinterest%2520graphs%2520with%2520billions%2520of%2520nodes.%2520The%2520universal%250Arepresentations%2520generated%2520by%2520OmniSage%2520have%2520significantly%2520enhanced%2520user%250Aexperiences%2520on%2520Pinterest%252C%2520leading%2520to%2520an%2520approximate%25202.5%2525%2520increase%2520in%2520sitewide%250Arepins%2520%2528saves%2529%2520across%2520five%2520applications.%2520This%2520paper%2520highlights%2520the%2520impact%2520of%250Aunifying%2520representation%2520learning%2520methods%252C%2520and%2520we%2520will%2520open%2520source%2520the%2520OmniSage%250Acode%2520by%2520the%2520time%2520of%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17811v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSage%3A%20Large%20Scale%2C%20Multi-Entity%20Heterogeneous%20Graph%20Representation%0A%20%20Learning&entry.906535625=Anirudhan%20Badrinath%20and%20Alex%20Yang%20and%20Kousik%20Rajesh%20and%20Prabhat%20Agarwal%20and%20Jaewon%20Yang%20and%20Haoyu%20Chen%20and%20Jiajing%20Xu%20and%20Charles%20Rosenberg&entry.1292438233=%20%20Representation%20learning%2C%20a%20task%20of%20learning%20latent%20vectors%20to%20represent%0Aentities%2C%20is%20a%20key%20task%20in%20improving%20search%20and%20recommender%20systems%20in%20web%0Aapplications.%20Various%20representation%20learning%20methods%20have%20been%20developed%2C%0Aincluding%20graph-based%20approaches%20for%20relationships%20among%20entities%2C%0Asequence-based%20methods%20for%20capturing%20the%20temporal%20evolution%20of%20user%20activities%2C%0Aand%20content-based%20models%20for%20leveraging%20text%20and%20visual%20content.%20However%2C%20the%0Adevelopment%20of%20a%20unifying%20framework%20that%20integrates%20these%20diverse%20techniques%20to%0Asupport%20multiple%20applications%20remains%20a%20significant%20challenge.%20This%20paper%0Apresents%20OmniSage%2C%20a%20large-scale%20representation%20framework%20that%20learns%20universal%0Arepresentations%20for%20a%20variety%20of%20applications%20at%20Pinterest.%20OmniSage%20integrates%0Agraph%20neural%20networks%20with%20content-based%20models%20and%20user%20sequence%20models%20by%0Aemploying%20multiple%20contrastive%20learning%20tasks%20to%20effectively%20process%20graph%0Adata%2C%20user%20sequence%20data%2C%20and%20content%20signals.%20To%20support%20the%20training%20and%0Ainference%20of%20OmniSage%2C%20we%20developed%20an%20efficient%20infrastructure%20capable%20of%0Asupporting%20Pinterest%20graphs%20with%20billions%20of%20nodes.%20The%20universal%0Arepresentations%20generated%20by%20OmniSage%20have%20significantly%20enhanced%20user%0Aexperiences%20on%20Pinterest%2C%20leading%20to%20an%20approximate%202.5%25%20increase%20in%20sitewide%0Arepins%20%28saves%29%20across%20five%20applications.%20This%20paper%20highlights%20the%20impact%20of%0Aunifying%20representation%20learning%20methods%2C%20and%20we%20will%20open%20source%20the%20OmniSage%0Acode%20by%20the%20time%20of%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17811v2&entry.124074799=Read"},
{"title": "LT3SD: Latent Trees for 3D Scene Diffusion", "author": "Quan Meng and Lei Li and Matthias Nie\u00dfner and Angela Dai", "abstract": "  We present LT3SD, a novel latent diffusion model for large-scale 3D scene\ngeneration. Recent advances in diffusion models have shown impressive results\nin 3D object generation, but are limited in spatial extent and quality when\nextended to 3D scenes. To generate complex and diverse 3D scene structures, we\nintroduce a latent tree representation to effectively encode both\nlower-frequency geometry and higher-frequency detail in a coarse-to-fine\nhierarchy. We can then learn a generative diffusion process in this latent 3D\nscene space, modeling the latent components of a scene at each resolution\nlevel. To synthesize large-scale scenes with varying sizes, we train our\ndiffusion model on scene patches and synthesize arbitrary-sized output 3D\nscenes through shared diffusion generation across multiple scene patches.\nThrough extensive experiments, we demonstrate the efficacy and benefits of\nLT3SD for large-scale, high-quality unconditional 3D scene generation and for\nprobabilistic completion for partial scene observations.\n", "link": "http://arxiv.org/abs/2409.08215v2", "date": "2025-05-01", "relevancy": 2.598, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6573}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.648}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LT3SD%3A%20Latent%20Trees%20for%203D%20Scene%20Diffusion&body=Title%3A%20LT3SD%3A%20Latent%20Trees%20for%203D%20Scene%20Diffusion%0AAuthor%3A%20Quan%20Meng%20and%20Lei%20Li%20and%20Matthias%20Nie%C3%9Fner%20and%20Angela%20Dai%0AAbstract%3A%20%20%20We%20present%20LT3SD%2C%20a%20novel%20latent%20diffusion%20model%20for%20large-scale%203D%20scene%0Ageneration.%20Recent%20advances%20in%20diffusion%20models%20have%20shown%20impressive%20results%0Ain%203D%20object%20generation%2C%20but%20are%20limited%20in%20spatial%20extent%20and%20quality%20when%0Aextended%20to%203D%20scenes.%20To%20generate%20complex%20and%20diverse%203D%20scene%20structures%2C%20we%0Aintroduce%20a%20latent%20tree%20representation%20to%20effectively%20encode%20both%0Alower-frequency%20geometry%20and%20higher-frequency%20detail%20in%20a%20coarse-to-fine%0Ahierarchy.%20We%20can%20then%20learn%20a%20generative%20diffusion%20process%20in%20this%20latent%203D%0Ascene%20space%2C%20modeling%20the%20latent%20components%20of%20a%20scene%20at%20each%20resolution%0Alevel.%20To%20synthesize%20large-scale%20scenes%20with%20varying%20sizes%2C%20we%20train%20our%0Adiffusion%20model%20on%20scene%20patches%20and%20synthesize%20arbitrary-sized%20output%203D%0Ascenes%20through%20shared%20diffusion%20generation%20across%20multiple%20scene%20patches.%0AThrough%20extensive%20experiments%2C%20we%20demonstrate%20the%20efficacy%20and%20benefits%20of%0ALT3SD%20for%20large-scale%2C%20high-quality%20unconditional%203D%20scene%20generation%20and%20for%0Aprobabilistic%20completion%20for%20partial%20scene%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLT3SD%253A%2520Latent%2520Trees%2520for%25203D%2520Scene%2520Diffusion%26entry.906535625%3DQuan%2520Meng%2520and%2520Lei%2520Li%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520We%2520present%2520LT3SD%252C%2520a%2520novel%2520latent%2520diffusion%2520model%2520for%2520large-scale%25203D%2520scene%250Ageneration.%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520shown%2520impressive%2520results%250Ain%25203D%2520object%2520generation%252C%2520but%2520are%2520limited%2520in%2520spatial%2520extent%2520and%2520quality%2520when%250Aextended%2520to%25203D%2520scenes.%2520To%2520generate%2520complex%2520and%2520diverse%25203D%2520scene%2520structures%252C%2520we%250Aintroduce%2520a%2520latent%2520tree%2520representation%2520to%2520effectively%2520encode%2520both%250Alower-frequency%2520geometry%2520and%2520higher-frequency%2520detail%2520in%2520a%2520coarse-to-fine%250Ahierarchy.%2520We%2520can%2520then%2520learn%2520a%2520generative%2520diffusion%2520process%2520in%2520this%2520latent%25203D%250Ascene%2520space%252C%2520modeling%2520the%2520latent%2520components%2520of%2520a%2520scene%2520at%2520each%2520resolution%250Alevel.%2520To%2520synthesize%2520large-scale%2520scenes%2520with%2520varying%2520sizes%252C%2520we%2520train%2520our%250Adiffusion%2520model%2520on%2520scene%2520patches%2520and%2520synthesize%2520arbitrary-sized%2520output%25203D%250Ascenes%2520through%2520shared%2520diffusion%2520generation%2520across%2520multiple%2520scene%2520patches.%250AThrough%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520efficacy%2520and%2520benefits%2520of%250ALT3SD%2520for%2520large-scale%252C%2520high-quality%2520unconditional%25203D%2520scene%2520generation%2520and%2520for%250Aprobabilistic%2520completion%2520for%2520partial%2520scene%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LT3SD%3A%20Latent%20Trees%20for%203D%20Scene%20Diffusion&entry.906535625=Quan%20Meng%20and%20Lei%20Li%20and%20Matthias%20Nie%C3%9Fner%20and%20Angela%20Dai&entry.1292438233=%20%20We%20present%20LT3SD%2C%20a%20novel%20latent%20diffusion%20model%20for%20large-scale%203D%20scene%0Ageneration.%20Recent%20advances%20in%20diffusion%20models%20have%20shown%20impressive%20results%0Ain%203D%20object%20generation%2C%20but%20are%20limited%20in%20spatial%20extent%20and%20quality%20when%0Aextended%20to%203D%20scenes.%20To%20generate%20complex%20and%20diverse%203D%20scene%20structures%2C%20we%0Aintroduce%20a%20latent%20tree%20representation%20to%20effectively%20encode%20both%0Alower-frequency%20geometry%20and%20higher-frequency%20detail%20in%20a%20coarse-to-fine%0Ahierarchy.%20We%20can%20then%20learn%20a%20generative%20diffusion%20process%20in%20this%20latent%203D%0Ascene%20space%2C%20modeling%20the%20latent%20components%20of%20a%20scene%20at%20each%20resolution%0Alevel.%20To%20synthesize%20large-scale%20scenes%20with%20varying%20sizes%2C%20we%20train%20our%0Adiffusion%20model%20on%20scene%20patches%20and%20synthesize%20arbitrary-sized%20output%203D%0Ascenes%20through%20shared%20diffusion%20generation%20across%20multiple%20scene%20patches.%0AThrough%20extensive%20experiments%2C%20we%20demonstrate%20the%20efficacy%20and%20benefits%20of%0ALT3SD%20for%20large-scale%2C%20high-quality%20unconditional%203D%20scene%20generation%20and%20for%0Aprobabilistic%20completion%20for%20partial%20scene%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08215v2&entry.124074799=Read"},
{"title": "FineScope : Precision Pruning for Domain-Specialized Large Language\n  Models Using SAE-Guided Self-Data Cultivation", "author": "Chaitali Bhattacharyya and Yeseong Kim", "abstract": "  Training large language models (LLMs) from scratch requires significant\ncomputational resources, driving interest in developing smaller,\ndomain-specific LLMs that maintain both efficiency and strong task performance.\nMedium-sized models such as LLaMA, llama} have served as starting points for\ndomain-specific adaptation, but they often suffer from accuracy degradation\nwhen tested on specialized datasets. We introduce FineScope, a framework for\nderiving compact, domain-optimized LLMs from larger pretrained models.\nFineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its\nability to produce interpretable feature representations, to extract\ndomain-specific subsets from large datasets. We apply structured pruning with\ndomain-specific constraints, ensuring that the resulting pruned models retain\nessential knowledge for the target domain. To further enhance performance,\nthese pruned models undergo self-data distillation, leveraging SAE-curated\ndatasets to restore key domain-specific information lost during pruning.\nExtensive experiments and ablation studies demonstrate that FineScope achieves\nhighly competitive performance, outperforming several large-scale\nstate-of-the-art LLMs in domain-specific tasks. Additionally, our results show\nthat FineScope enables pruned models to regain a substantial portion of their\noriginal performance when fine-tuned with SAE-curated datasets. Furthermore,\napplying these datasets to fine-tune pretrained LLMs without pruning also\nimproves their domain-specific accuracy, highlighting the robustness of our\napproach. The code will be released.\n", "link": "http://arxiv.org/abs/2505.00624v1", "date": "2025-05-01", "relevancy": 2.5351, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineScope%20%3A%20Precision%20Pruning%20for%20Domain-Specialized%20Large%20Language%0A%20%20Models%20Using%20SAE-Guided%20Self-Data%20Cultivation&body=Title%3A%20FineScope%20%3A%20Precision%20Pruning%20for%20Domain-Specialized%20Large%20Language%0A%20%20Models%20Using%20SAE-Guided%20Self-Data%20Cultivation%0AAuthor%3A%20Chaitali%20Bhattacharyya%20and%20Yeseong%20Kim%0AAbstract%3A%20%20%20Training%20large%20language%20models%20%28LLMs%29%20from%20scratch%20requires%20significant%0Acomputational%20resources%2C%20driving%20interest%20in%20developing%20smaller%2C%0Adomain-specific%20LLMs%20that%20maintain%20both%20efficiency%20and%20strong%20task%20performance.%0AMedium-sized%20models%20such%20as%20LLaMA%2C%20llama%7D%20have%20served%20as%20starting%20points%20for%0Adomain-specific%20adaptation%2C%20but%20they%20often%20suffer%20from%20accuracy%20degradation%0Awhen%20tested%20on%20specialized%20datasets.%20We%20introduce%20FineScope%2C%20a%20framework%20for%0Aderiving%20compact%2C%20domain-optimized%20LLMs%20from%20larger%20pretrained%20models.%0AFineScope%20leverages%20the%20Sparse%20Autoencoder%20%28SAE%29%20framework%2C%20inspired%20by%20its%0Aability%20to%20produce%20interpretable%20feature%20representations%2C%20to%20extract%0Adomain-specific%20subsets%20from%20large%20datasets.%20We%20apply%20structured%20pruning%20with%0Adomain-specific%20constraints%2C%20ensuring%20that%20the%20resulting%20pruned%20models%20retain%0Aessential%20knowledge%20for%20the%20target%20domain.%20To%20further%20enhance%20performance%2C%0Athese%20pruned%20models%20undergo%20self-data%20distillation%2C%20leveraging%20SAE-curated%0Adatasets%20to%20restore%20key%20domain-specific%20information%20lost%20during%20pruning.%0AExtensive%20experiments%20and%20ablation%20studies%20demonstrate%20that%20FineScope%20achieves%0Ahighly%20competitive%20performance%2C%20outperforming%20several%20large-scale%0Astate-of-the-art%20LLMs%20in%20domain-specific%20tasks.%20Additionally%2C%20our%20results%20show%0Athat%20FineScope%20enables%20pruned%20models%20to%20regain%20a%20substantial%20portion%20of%20their%0Aoriginal%20performance%20when%20fine-tuned%20with%20SAE-curated%20datasets.%20Furthermore%2C%0Aapplying%20these%20datasets%20to%20fine-tune%20pretrained%20LLMs%20without%20pruning%20also%0Aimproves%20their%20domain-specific%20accuracy%2C%20highlighting%20the%20robustness%20of%20our%0Aapproach.%20The%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineScope%2520%253A%2520Precision%2520Pruning%2520for%2520Domain-Specialized%2520Large%2520Language%250A%2520%2520Models%2520Using%2520SAE-Guided%2520Self-Data%2520Cultivation%26entry.906535625%3DChaitali%2520Bhattacharyya%2520and%2520Yeseong%2520Kim%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520%2528LLMs%2529%2520from%2520scratch%2520requires%2520significant%250Acomputational%2520resources%252C%2520driving%2520interest%2520in%2520developing%2520smaller%252C%250Adomain-specific%2520LLMs%2520that%2520maintain%2520both%2520efficiency%2520and%2520strong%2520task%2520performance.%250AMedium-sized%2520models%2520such%2520as%2520LLaMA%252C%2520llama%257D%2520have%2520served%2520as%2520starting%2520points%2520for%250Adomain-specific%2520adaptation%252C%2520but%2520they%2520often%2520suffer%2520from%2520accuracy%2520degradation%250Awhen%2520tested%2520on%2520specialized%2520datasets.%2520We%2520introduce%2520FineScope%252C%2520a%2520framework%2520for%250Aderiving%2520compact%252C%2520domain-optimized%2520LLMs%2520from%2520larger%2520pretrained%2520models.%250AFineScope%2520leverages%2520the%2520Sparse%2520Autoencoder%2520%2528SAE%2529%2520framework%252C%2520inspired%2520by%2520its%250Aability%2520to%2520produce%2520interpretable%2520feature%2520representations%252C%2520to%2520extract%250Adomain-specific%2520subsets%2520from%2520large%2520datasets.%2520We%2520apply%2520structured%2520pruning%2520with%250Adomain-specific%2520constraints%252C%2520ensuring%2520that%2520the%2520resulting%2520pruned%2520models%2520retain%250Aessential%2520knowledge%2520for%2520the%2520target%2520domain.%2520To%2520further%2520enhance%2520performance%252C%250Athese%2520pruned%2520models%2520undergo%2520self-data%2520distillation%252C%2520leveraging%2520SAE-curated%250Adatasets%2520to%2520restore%2520key%2520domain-specific%2520information%2520lost%2520during%2520pruning.%250AExtensive%2520experiments%2520and%2520ablation%2520studies%2520demonstrate%2520that%2520FineScope%2520achieves%250Ahighly%2520competitive%2520performance%252C%2520outperforming%2520several%2520large-scale%250Astate-of-the-art%2520LLMs%2520in%2520domain-specific%2520tasks.%2520Additionally%252C%2520our%2520results%2520show%250Athat%2520FineScope%2520enables%2520pruned%2520models%2520to%2520regain%2520a%2520substantial%2520portion%2520of%2520their%250Aoriginal%2520performance%2520when%2520fine-tuned%2520with%2520SAE-curated%2520datasets.%2520Furthermore%252C%250Aapplying%2520these%2520datasets%2520to%2520fine-tune%2520pretrained%2520LLMs%2520without%2520pruning%2520also%250Aimproves%2520their%2520domain-specific%2520accuracy%252C%2520highlighting%2520the%2520robustness%2520of%2520our%250Aapproach.%2520The%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineScope%20%3A%20Precision%20Pruning%20for%20Domain-Specialized%20Large%20Language%0A%20%20Models%20Using%20SAE-Guided%20Self-Data%20Cultivation&entry.906535625=Chaitali%20Bhattacharyya%20and%20Yeseong%20Kim&entry.1292438233=%20%20Training%20large%20language%20models%20%28LLMs%29%20from%20scratch%20requires%20significant%0Acomputational%20resources%2C%20driving%20interest%20in%20developing%20smaller%2C%0Adomain-specific%20LLMs%20that%20maintain%20both%20efficiency%20and%20strong%20task%20performance.%0AMedium-sized%20models%20such%20as%20LLaMA%2C%20llama%7D%20have%20served%20as%20starting%20points%20for%0Adomain-specific%20adaptation%2C%20but%20they%20often%20suffer%20from%20accuracy%20degradation%0Awhen%20tested%20on%20specialized%20datasets.%20We%20introduce%20FineScope%2C%20a%20framework%20for%0Aderiving%20compact%2C%20domain-optimized%20LLMs%20from%20larger%20pretrained%20models.%0AFineScope%20leverages%20the%20Sparse%20Autoencoder%20%28SAE%29%20framework%2C%20inspired%20by%20its%0Aability%20to%20produce%20interpretable%20feature%20representations%2C%20to%20extract%0Adomain-specific%20subsets%20from%20large%20datasets.%20We%20apply%20structured%20pruning%20with%0Adomain-specific%20constraints%2C%20ensuring%20that%20the%20resulting%20pruned%20models%20retain%0Aessential%20knowledge%20for%20the%20target%20domain.%20To%20further%20enhance%20performance%2C%0Athese%20pruned%20models%20undergo%20self-data%20distillation%2C%20leveraging%20SAE-curated%0Adatasets%20to%20restore%20key%20domain-specific%20information%20lost%20during%20pruning.%0AExtensive%20experiments%20and%20ablation%20studies%20demonstrate%20that%20FineScope%20achieves%0Ahighly%20competitive%20performance%2C%20outperforming%20several%20large-scale%0Astate-of-the-art%20LLMs%20in%20domain-specific%20tasks.%20Additionally%2C%20our%20results%20show%0Athat%20FineScope%20enables%20pruned%20models%20to%20regain%20a%20substantial%20portion%20of%20their%0Aoriginal%20performance%20when%20fine-tuned%20with%20SAE-curated%20datasets.%20Furthermore%2C%0Aapplying%20these%20datasets%20to%20fine-tune%20pretrained%20LLMs%20without%20pruning%20also%0Aimproves%20their%20domain-specific%20accuracy%2C%20highlighting%20the%20robustness%20of%20our%0Aapproach.%20The%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00624v1&entry.124074799=Read"},
{"title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation", "author": "Thanet Markchom and Tong Wu and Liting Huang and Huizhi Liang", "abstract": "  SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.\n", "link": "http://arxiv.org/abs/2502.20984v3", "date": "2025-05-01", "relevancy": 2.5293, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UoR-NCL%20at%20SemEval-2025%20Task%201%3A%20Using%20Generative%20LLMs%20and%20CLIP%20Models%0A%20%20for%20Multilingual%20Multimodal%20Idiomaticity%20Representation&body=Title%3A%20UoR-NCL%20at%20SemEval-2025%20Task%201%3A%20Using%20Generative%20LLMs%20and%20CLIP%20Models%0A%20%20for%20Multilingual%20Multimodal%20Idiomaticity%20Representation%0AAuthor%3A%20Thanet%20Markchom%20and%20Tong%20Wu%20and%20Liting%20Huang%20and%20Huizhi%20Liang%0AAbstract%3A%20%20%20SemEval-2025%20Task%201%20focuses%20on%20ranking%20images%20based%20on%20their%20alignment%20with%20a%0Agiven%20nominal%20compound%20that%20may%20carry%20idiomatic%20meaning%20in%20both%20English%20and%0ABrazilian%20Portuguese.%20To%20address%20this%20challenge%2C%20this%20work%20uses%20generative%0Alarge%20language%20models%20%28LLMs%29%20and%20multilingual%20CLIP%20models%20to%20enhance%20idiomatic%0Acompound%20representations.%20LLMs%20generate%20idiomatic%20meanings%20for%20potentially%0Aidiomatic%20compounds%2C%20enriching%20their%20semantic%20interpretation.%20These%20meanings%0Aare%20then%20encoded%20using%20multilingual%20CLIP%20models%2C%20serving%20as%20representations%20for%0Aimage%20ranking.%20Contrastive%20learning%20and%20data%20augmentation%20techniques%20are%0Aapplied%20to%20fine-tune%20these%20embeddings%20for%20improved%20performance.%20Experimental%0Aresults%20show%20that%20multimodal%20representations%20extracted%20through%20this%20method%0Aoutperformed%20those%20based%20solely%20on%20the%20original%20nominal%20compounds.%20The%0Afine-tuning%20approach%20shows%20promising%20outcomes%20but%20is%20less%20effective%20than%20using%0Aembeddings%20without%20fine-tuning.%20The%20source%20code%20used%20in%20this%20paper%20is%20available%0Aat%20https%3A//github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20984v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUoR-NCL%2520at%2520SemEval-2025%2520Task%25201%253A%2520Using%2520Generative%2520LLMs%2520and%2520CLIP%2520Models%250A%2520%2520for%2520Multilingual%2520Multimodal%2520Idiomaticity%2520Representation%26entry.906535625%3DThanet%2520Markchom%2520and%2520Tong%2520Wu%2520and%2520Liting%2520Huang%2520and%2520Huizhi%2520Liang%26entry.1292438233%3D%2520%2520SemEval-2025%2520Task%25201%2520focuses%2520on%2520ranking%2520images%2520based%2520on%2520their%2520alignment%2520with%2520a%250Agiven%2520nominal%2520compound%2520that%2520may%2520carry%2520idiomatic%2520meaning%2520in%2520both%2520English%2520and%250ABrazilian%2520Portuguese.%2520To%2520address%2520this%2520challenge%252C%2520this%2520work%2520uses%2520generative%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520and%2520multilingual%2520CLIP%2520models%2520to%2520enhance%2520idiomatic%250Acompound%2520representations.%2520LLMs%2520generate%2520idiomatic%2520meanings%2520for%2520potentially%250Aidiomatic%2520compounds%252C%2520enriching%2520their%2520semantic%2520interpretation.%2520These%2520meanings%250Aare%2520then%2520encoded%2520using%2520multilingual%2520CLIP%2520models%252C%2520serving%2520as%2520representations%2520for%250Aimage%2520ranking.%2520Contrastive%2520learning%2520and%2520data%2520augmentation%2520techniques%2520are%250Aapplied%2520to%2520fine-tune%2520these%2520embeddings%2520for%2520improved%2520performance.%2520Experimental%250Aresults%2520show%2520that%2520multimodal%2520representations%2520extracted%2520through%2520this%2520method%250Aoutperformed%2520those%2520based%2520solely%2520on%2520the%2520original%2520nominal%2520compounds.%2520The%250Afine-tuning%2520approach%2520shows%2520promising%2520outcomes%2520but%2520is%2520less%2520effective%2520than%2520using%250Aembeddings%2520without%2520fine-tuning.%2520The%2520source%2520code%2520used%2520in%2520this%2520paper%2520is%2520available%250Aat%2520https%253A//github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20984v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UoR-NCL%20at%20SemEval-2025%20Task%201%3A%20Using%20Generative%20LLMs%20and%20CLIP%20Models%0A%20%20for%20Multilingual%20Multimodal%20Idiomaticity%20Representation&entry.906535625=Thanet%20Markchom%20and%20Tong%20Wu%20and%20Liting%20Huang%20and%20Huizhi%20Liang&entry.1292438233=%20%20SemEval-2025%20Task%201%20focuses%20on%20ranking%20images%20based%20on%20their%20alignment%20with%20a%0Agiven%20nominal%20compound%20that%20may%20carry%20idiomatic%20meaning%20in%20both%20English%20and%0ABrazilian%20Portuguese.%20To%20address%20this%20challenge%2C%20this%20work%20uses%20generative%0Alarge%20language%20models%20%28LLMs%29%20and%20multilingual%20CLIP%20models%20to%20enhance%20idiomatic%0Acompound%20representations.%20LLMs%20generate%20idiomatic%20meanings%20for%20potentially%0Aidiomatic%20compounds%2C%20enriching%20their%20semantic%20interpretation.%20These%20meanings%0Aare%20then%20encoded%20using%20multilingual%20CLIP%20models%2C%20serving%20as%20representations%20for%0Aimage%20ranking.%20Contrastive%20learning%20and%20data%20augmentation%20techniques%20are%0Aapplied%20to%20fine-tune%20these%20embeddings%20for%20improved%20performance.%20Experimental%0Aresults%20show%20that%20multimodal%20representations%20extracted%20through%20this%20method%0Aoutperformed%20those%20based%20solely%20on%20the%20original%20nominal%20compounds.%20The%0Afine-tuning%20approach%20shows%20promising%20outcomes%20but%20is%20less%20effective%20than%20using%0Aembeddings%20without%20fine-tuning.%20The%20source%20code%20used%20in%20this%20paper%20is%20available%0Aat%20https%3A//github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20984v3&entry.124074799=Read"},
{"title": "Hypencoder: Hypernetworks for Information Retrieval", "author": "Julian Killingback and Hansi Zeng and Hamed Zamani", "abstract": "  Existing information retrieval systems are largely constrained by their\nreliance on vector inner products to assess query-document relevance, which\nnaturally limits the expressiveness of the relevance score they can produce. We\npropose a new paradigm; instead of representing a query as a vector, we use a\nsmall neural network that acts as a learned query-specific relevance function.\nThis small neural network takes a document representation as input (in this\nwork we use a single vector) and produces a scalar relevance score. To produce\nthe small neural network we use a hypernetwork, a network that produces the\nweights of other networks, as our query encoder. We name this category of\nencoder models Hypencoders. Experiments on in-domain search tasks show that\nHypencoders significantly outperform strong dense retrieval models and even\nsurpass reranking models and retrieval models with an order of magnitude more\nparameters. To assess the extent of Hypencoders' capabilities, we evaluate on a\nset of hard retrieval tasks including tip-of-the-tongue and\ninstruction-following retrieval tasks. On harder tasks, we find that the\nperformance gap widens substantially compared to standard retrieval tasks.\nFurthermore, to demonstrate the practicality of our method, we implement an\napproximate search algorithm and show that our model is able to retrieve from a\ncorpus of 8.8M documents in under 60 milliseconds.\n", "link": "http://arxiv.org/abs/2502.05364v2", "date": "2025-05-01", "relevancy": 2.4795, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypencoder%3A%20Hypernetworks%20for%20Information%20Retrieval&body=Title%3A%20Hypencoder%3A%20Hypernetworks%20for%20Information%20Retrieval%0AAuthor%3A%20Julian%20Killingback%20and%20Hansi%20Zeng%20and%20Hamed%20Zamani%0AAbstract%3A%20%20%20Existing%20information%20retrieval%20systems%20are%20largely%20constrained%20by%20their%0Areliance%20on%20vector%20inner%20products%20to%20assess%20query-document%20relevance%2C%20which%0Anaturally%20limits%20the%20expressiveness%20of%20the%20relevance%20score%20they%20can%20produce.%20We%0Apropose%20a%20new%20paradigm%3B%20instead%20of%20representing%20a%20query%20as%20a%20vector%2C%20we%20use%20a%0Asmall%20neural%20network%20that%20acts%20as%20a%20learned%20query-specific%20relevance%20function.%0AThis%20small%20neural%20network%20takes%20a%20document%20representation%20as%20input%20%28in%20this%0Awork%20we%20use%20a%20single%20vector%29%20and%20produces%20a%20scalar%20relevance%20score.%20To%20produce%0Athe%20small%20neural%20network%20we%20use%20a%20hypernetwork%2C%20a%20network%20that%20produces%20the%0Aweights%20of%20other%20networks%2C%20as%20our%20query%20encoder.%20We%20name%20this%20category%20of%0Aencoder%20models%20Hypencoders.%20Experiments%20on%20in-domain%20search%20tasks%20show%20that%0AHypencoders%20significantly%20outperform%20strong%20dense%20retrieval%20models%20and%20even%0Asurpass%20reranking%20models%20and%20retrieval%20models%20with%20an%20order%20of%20magnitude%20more%0Aparameters.%20To%20assess%20the%20extent%20of%20Hypencoders%27%20capabilities%2C%20we%20evaluate%20on%20a%0Aset%20of%20hard%20retrieval%20tasks%20including%20tip-of-the-tongue%20and%0Ainstruction-following%20retrieval%20tasks.%20On%20harder%20tasks%2C%20we%20find%20that%20the%0Aperformance%20gap%20widens%20substantially%20compared%20to%20standard%20retrieval%20tasks.%0AFurthermore%2C%20to%20demonstrate%20the%20practicality%20of%20our%20method%2C%20we%20implement%20an%0Aapproximate%20search%20algorithm%20and%20show%20that%20our%20model%20is%20able%20to%20retrieve%20from%20a%0Acorpus%20of%208.8M%20documents%20in%20under%2060%20milliseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypencoder%253A%2520Hypernetworks%2520for%2520Information%2520Retrieval%26entry.906535625%3DJulian%2520Killingback%2520and%2520Hansi%2520Zeng%2520and%2520Hamed%2520Zamani%26entry.1292438233%3D%2520%2520Existing%2520information%2520retrieval%2520systems%2520are%2520largely%2520constrained%2520by%2520their%250Areliance%2520on%2520vector%2520inner%2520products%2520to%2520assess%2520query-document%2520relevance%252C%2520which%250Anaturally%2520limits%2520the%2520expressiveness%2520of%2520the%2520relevance%2520score%2520they%2520can%2520produce.%2520We%250Apropose%2520a%2520new%2520paradigm%253B%2520instead%2520of%2520representing%2520a%2520query%2520as%2520a%2520vector%252C%2520we%2520use%2520a%250Asmall%2520neural%2520network%2520that%2520acts%2520as%2520a%2520learned%2520query-specific%2520relevance%2520function.%250AThis%2520small%2520neural%2520network%2520takes%2520a%2520document%2520representation%2520as%2520input%2520%2528in%2520this%250Awork%2520we%2520use%2520a%2520single%2520vector%2529%2520and%2520produces%2520a%2520scalar%2520relevance%2520score.%2520To%2520produce%250Athe%2520small%2520neural%2520network%2520we%2520use%2520a%2520hypernetwork%252C%2520a%2520network%2520that%2520produces%2520the%250Aweights%2520of%2520other%2520networks%252C%2520as%2520our%2520query%2520encoder.%2520We%2520name%2520this%2520category%2520of%250Aencoder%2520models%2520Hypencoders.%2520Experiments%2520on%2520in-domain%2520search%2520tasks%2520show%2520that%250AHypencoders%2520significantly%2520outperform%2520strong%2520dense%2520retrieval%2520models%2520and%2520even%250Asurpass%2520reranking%2520models%2520and%2520retrieval%2520models%2520with%2520an%2520order%2520of%2520magnitude%2520more%250Aparameters.%2520To%2520assess%2520the%2520extent%2520of%2520Hypencoders%2527%2520capabilities%252C%2520we%2520evaluate%2520on%2520a%250Aset%2520of%2520hard%2520retrieval%2520tasks%2520including%2520tip-of-the-tongue%2520and%250Ainstruction-following%2520retrieval%2520tasks.%2520On%2520harder%2520tasks%252C%2520we%2520find%2520that%2520the%250Aperformance%2520gap%2520widens%2520substantially%2520compared%2520to%2520standard%2520retrieval%2520tasks.%250AFurthermore%252C%2520to%2520demonstrate%2520the%2520practicality%2520of%2520our%2520method%252C%2520we%2520implement%2520an%250Aapproximate%2520search%2520algorithm%2520and%2520show%2520that%2520our%2520model%2520is%2520able%2520to%2520retrieve%2520from%2520a%250Acorpus%2520of%25208.8M%2520documents%2520in%2520under%252060%2520milliseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypencoder%3A%20Hypernetworks%20for%20Information%20Retrieval&entry.906535625=Julian%20Killingback%20and%20Hansi%20Zeng%20and%20Hamed%20Zamani&entry.1292438233=%20%20Existing%20information%20retrieval%20systems%20are%20largely%20constrained%20by%20their%0Areliance%20on%20vector%20inner%20products%20to%20assess%20query-document%20relevance%2C%20which%0Anaturally%20limits%20the%20expressiveness%20of%20the%20relevance%20score%20they%20can%20produce.%20We%0Apropose%20a%20new%20paradigm%3B%20instead%20of%20representing%20a%20query%20as%20a%20vector%2C%20we%20use%20a%0Asmall%20neural%20network%20that%20acts%20as%20a%20learned%20query-specific%20relevance%20function.%0AThis%20small%20neural%20network%20takes%20a%20document%20representation%20as%20input%20%28in%20this%0Awork%20we%20use%20a%20single%20vector%29%20and%20produces%20a%20scalar%20relevance%20score.%20To%20produce%0Athe%20small%20neural%20network%20we%20use%20a%20hypernetwork%2C%20a%20network%20that%20produces%20the%0Aweights%20of%20other%20networks%2C%20as%20our%20query%20encoder.%20We%20name%20this%20category%20of%0Aencoder%20models%20Hypencoders.%20Experiments%20on%20in-domain%20search%20tasks%20show%20that%0AHypencoders%20significantly%20outperform%20strong%20dense%20retrieval%20models%20and%20even%0Asurpass%20reranking%20models%20and%20retrieval%20models%20with%20an%20order%20of%20magnitude%20more%0Aparameters.%20To%20assess%20the%20extent%20of%20Hypencoders%27%20capabilities%2C%20we%20evaluate%20on%20a%0Aset%20of%20hard%20retrieval%20tasks%20including%20tip-of-the-tongue%20and%0Ainstruction-following%20retrieval%20tasks.%20On%20harder%20tasks%2C%20we%20find%20that%20the%0Aperformance%20gap%20widens%20substantially%20compared%20to%20standard%20retrieval%20tasks.%0AFurthermore%2C%20to%20demonstrate%20the%20practicality%20of%20our%20method%2C%20we%20implement%20an%0Aapproximate%20search%20algorithm%20and%20show%20that%20our%20model%20is%20able%20to%20retrieve%20from%20a%0Acorpus%20of%208.8M%20documents%20in%20under%2060%20milliseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05364v2&entry.124074799=Read"},
{"title": "Investigating Task Arithmetic for Zero-Shot Information Retrieval", "author": "Marco Braga and Pranav Kasela and Alessandro Raganato and Gabriella Pasi", "abstract": "  Large Language Models (LLMs) have shown impressive zero-shot performance\nacross a variety of Natural Language Processing tasks, including document\nre-ranking. However, their effectiveness degrades on unseen tasks and domains,\nlargely due to shifts in vocabulary and word distributions. In this paper, we\ninvestigate Task Arithmetic, a technique that combines the weights of LLMs\npre-trained on different tasks or domains via simple mathematical operations,\nsuch as addition or subtraction, to adapt retrieval models without requiring\nadditional fine-tuning. Our method is able to synthesize diverse tasks and\ndomain knowledge into a single model, enabling effective zero-shot adaptation\nin different retrieval contexts. Extensive experiments on publicly available\nscientific, biomedical, and multilingual datasets show that our method improves\nstate-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in\nP@10. In addition to these empirical gains, our analysis provides insights into\nthe strengths and limitations of Task Arithmetic as a practical strategy for\nzero-shot learning and model adaptation. We make our code publicly available at\nhttps://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.\n", "link": "http://arxiv.org/abs/2505.00649v1", "date": "2025-05-01", "relevancy": 2.4764, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Task%20Arithmetic%20for%20Zero-Shot%20Information%20Retrieval&body=Title%3A%20Investigating%20Task%20Arithmetic%20for%20Zero-Shot%20Information%20Retrieval%0AAuthor%3A%20Marco%20Braga%20and%20Pranav%20Kasela%20and%20Alessandro%20Raganato%20and%20Gabriella%20Pasi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20zero-shot%20performance%0Aacross%20a%20variety%20of%20Natural%20Language%20Processing%20tasks%2C%20including%20document%0Are-ranking.%20However%2C%20their%20effectiveness%20degrades%20on%20unseen%20tasks%20and%20domains%2C%0Alargely%20due%20to%20shifts%20in%20vocabulary%20and%20word%20distributions.%20In%20this%20paper%2C%20we%0Ainvestigate%20Task%20Arithmetic%2C%20a%20technique%20that%20combines%20the%20weights%20of%20LLMs%0Apre-trained%20on%20different%20tasks%20or%20domains%20via%20simple%20mathematical%20operations%2C%0Asuch%20as%20addition%20or%20subtraction%2C%20to%20adapt%20retrieval%20models%20without%20requiring%0Aadditional%20fine-tuning.%20Our%20method%20is%20able%20to%20synthesize%20diverse%20tasks%20and%0Adomain%20knowledge%20into%20a%20single%20model%2C%20enabling%20effective%20zero-shot%20adaptation%0Ain%20different%20retrieval%20contexts.%20Extensive%20experiments%20on%20publicly%20available%0Ascientific%2C%20biomedical%2C%20and%20multilingual%20datasets%20show%20that%20our%20method%20improves%0Astate-of-the-art%20re-ranking%20performance%20by%20up%20to%2018%25%20in%20NDCG%4010%20and%2015%25%20in%0AP%4010.%20In%20addition%20to%20these%20empirical%20gains%2C%20our%20analysis%20provides%20insights%20into%0Athe%20strengths%20and%20limitations%20of%20Task%20Arithmetic%20as%20a%20practical%20strategy%20for%0Azero-shot%20learning%20and%20model%20adaptation.%20We%20make%20our%20code%20publicly%20available%20at%0Ahttps%3A//github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Task%2520Arithmetic%2520for%2520Zero-Shot%2520Information%2520Retrieval%26entry.906535625%3DMarco%2520Braga%2520and%2520Pranav%2520Kasela%2520and%2520Alessandro%2520Raganato%2520and%2520Gabriella%2520Pasi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520zero-shot%2520performance%250Aacross%2520a%2520variety%2520of%2520Natural%2520Language%2520Processing%2520tasks%252C%2520including%2520document%250Are-ranking.%2520However%252C%2520their%2520effectiveness%2520degrades%2520on%2520unseen%2520tasks%2520and%2520domains%252C%250Alargely%2520due%2520to%2520shifts%2520in%2520vocabulary%2520and%2520word%2520distributions.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520Task%2520Arithmetic%252C%2520a%2520technique%2520that%2520combines%2520the%2520weights%2520of%2520LLMs%250Apre-trained%2520on%2520different%2520tasks%2520or%2520domains%2520via%2520simple%2520mathematical%2520operations%252C%250Asuch%2520as%2520addition%2520or%2520subtraction%252C%2520to%2520adapt%2520retrieval%2520models%2520without%2520requiring%250Aadditional%2520fine-tuning.%2520Our%2520method%2520is%2520able%2520to%2520synthesize%2520diverse%2520tasks%2520and%250Adomain%2520knowledge%2520into%2520a%2520single%2520model%252C%2520enabling%2520effective%2520zero-shot%2520adaptation%250Ain%2520different%2520retrieval%2520contexts.%2520Extensive%2520experiments%2520on%2520publicly%2520available%250Ascientific%252C%2520biomedical%252C%2520and%2520multilingual%2520datasets%2520show%2520that%2520our%2520method%2520improves%250Astate-of-the-art%2520re-ranking%2520performance%2520by%2520up%2520to%252018%2525%2520in%2520NDCG%254010%2520and%252015%2525%2520in%250AP%254010.%2520In%2520addition%2520to%2520these%2520empirical%2520gains%252C%2520our%2520analysis%2520provides%2520insights%2520into%250Athe%2520strengths%2520and%2520limitations%2520of%2520Task%2520Arithmetic%2520as%2520a%2520practical%2520strategy%2520for%250Azero-shot%2520learning%2520and%2520model%2520adaptation.%2520We%2520make%2520our%2520code%2520publicly%2520available%2520at%250Ahttps%253A//github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Task%20Arithmetic%20for%20Zero-Shot%20Information%20Retrieval&entry.906535625=Marco%20Braga%20and%20Pranav%20Kasela%20and%20Alessandro%20Raganato%20and%20Gabriella%20Pasi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20zero-shot%20performance%0Aacross%20a%20variety%20of%20Natural%20Language%20Processing%20tasks%2C%20including%20document%0Are-ranking.%20However%2C%20their%20effectiveness%20degrades%20on%20unseen%20tasks%20and%20domains%2C%0Alargely%20due%20to%20shifts%20in%20vocabulary%20and%20word%20distributions.%20In%20this%20paper%2C%20we%0Ainvestigate%20Task%20Arithmetic%2C%20a%20technique%20that%20combines%20the%20weights%20of%20LLMs%0Apre-trained%20on%20different%20tasks%20or%20domains%20via%20simple%20mathematical%20operations%2C%0Asuch%20as%20addition%20or%20subtraction%2C%20to%20adapt%20retrieval%20models%20without%20requiring%0Aadditional%20fine-tuning.%20Our%20method%20is%20able%20to%20synthesize%20diverse%20tasks%20and%0Adomain%20knowledge%20into%20a%20single%20model%2C%20enabling%20effective%20zero-shot%20adaptation%0Ain%20different%20retrieval%20contexts.%20Extensive%20experiments%20on%20publicly%20available%0Ascientific%2C%20biomedical%2C%20and%20multilingual%20datasets%20show%20that%20our%20method%20improves%0Astate-of-the-art%20re-ranking%20performance%20by%20up%20to%2018%25%20in%20NDCG%4010%20and%2015%25%20in%0AP%4010.%20In%20addition%20to%20these%20empirical%20gains%2C%20our%20analysis%20provides%20insights%20into%0Athe%20strengths%20and%20limitations%20of%20Task%20Arithmetic%20as%20a%20practical%20strategy%20for%0Azero-shot%20learning%20and%20model%20adaptation.%20We%20make%20our%20code%20publicly%20available%20at%0Ahttps%3A//github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00649v1&entry.124074799=Read"},
{"title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension", "author": "Jushi Kai and Boyi Zeng and Yixuan Wang and Haoli Bai and Bo Jiang and Zhouhan Lin", "abstract": "  Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.\n", "link": "http://arxiv.org/abs/2505.00570v1", "date": "2025-05-01", "relevancy": 2.4317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreqKV%3A%20Frequency%20Domain%20Key-Value%20Compression%20for%20Efficient%20Context%0A%20%20Window%20Extension&body=Title%3A%20FreqKV%3A%20Frequency%20Domain%20Key-Value%20Compression%20for%20Efficient%20Context%0A%20%20Window%20Extension%0AAuthor%3A%20Jushi%20Kai%20and%20Boyi%20Zeng%20and%20Yixuan%20Wang%20and%20Haoli%20Bai%20and%20Bo%20Jiang%20and%20Zhouhan%20Lin%0AAbstract%3A%20%20%20Extending%20the%20context%20window%20in%20large%20language%20models%20%28LLMs%29%20is%20essential%20for%0Aapplications%20involving%20long-form%20content%20generation.%20However%2C%20the%20linear%0Aincrease%20in%20key-value%20%28KV%29%20cache%20memory%20requirements%20and%20the%20quadratic%0Acomplexity%20of%20self-attention%20with%20respect%20to%20sequence%20length%20present%0Asignificant%20challenges%20during%20fine-tuning%20and%20inference.%20Existing%20methods%0Asuffer%20from%20performance%20degradation%20when%20extending%20to%20longer%20contexts.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20context%20extension%20method%20that%20optimizes%20both%0Afine-tuning%20and%20inference%20efficiency.%20Our%20method%20exploits%20a%20key%20observation%3A%20in%0Athe%20frequency%20domain%2C%20the%20energy%20distribution%20of%20the%20KV%20cache%20is%20primarily%0Aconcentrated%20in%20low-frequency%20components.%20By%20filtering%20out%20the%20high-frequency%0Acomponents%2C%20the%20KV%20cache%20can%20be%20effectively%20compressed%20with%20minimal%20information%0Aloss.%20Building%20on%20this%20insight%2C%20we%20propose%20an%20efficient%20compression%20technique%2C%0AFreqKV%2C%20that%20iteratively%20compresses%20the%20increasing%20KV%20cache%20to%20a%20fixed%20size%20in%0Athe%20frequency%20domain%2C%20applicable%20to%20both%20fine-tuning%20and%20inference.%20FreqKV%0Aintroduces%20no%20additional%20parameters%20or%20architectural%20modifications.%20With%0Aminimal%20fine-tuning%2C%20LLMs%20can%20learn%20to%20leverage%20the%20limited%20cache%20that%20is%0Acompressed%20in%20the%20frequency%20domain%20and%20extend%20the%20context%20window%20efficiently.%0AExperiments%20on%20various%20long%20context%20language%20modeling%20and%20understanding%20tasks%0Ademonstrate%20the%20efficiency%20and%20efficacy%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreqKV%253A%2520Frequency%2520Domain%2520Key-Value%2520Compression%2520for%2520Efficient%2520Context%250A%2520%2520Window%2520Extension%26entry.906535625%3DJushi%2520Kai%2520and%2520Boyi%2520Zeng%2520and%2520Yixuan%2520Wang%2520and%2520Haoli%2520Bai%2520and%2520Bo%2520Jiang%2520and%2520Zhouhan%2520Lin%26entry.1292438233%3D%2520%2520Extending%2520the%2520context%2520window%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520essential%2520for%250Aapplications%2520involving%2520long-form%2520content%2520generation.%2520However%252C%2520the%2520linear%250Aincrease%2520in%2520key-value%2520%2528KV%2529%2520cache%2520memory%2520requirements%2520and%2520the%2520quadratic%250Acomplexity%2520of%2520self-attention%2520with%2520respect%2520to%2520sequence%2520length%2520present%250Asignificant%2520challenges%2520during%2520fine-tuning%2520and%2520inference.%2520Existing%2520methods%250Asuffer%2520from%2520performance%2520degradation%2520when%2520extending%2520to%2520longer%2520contexts.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520context%2520extension%2520method%2520that%2520optimizes%2520both%250Afine-tuning%2520and%2520inference%2520efficiency.%2520Our%2520method%2520exploits%2520a%2520key%2520observation%253A%2520in%250Athe%2520frequency%2520domain%252C%2520the%2520energy%2520distribution%2520of%2520the%2520KV%2520cache%2520is%2520primarily%250Aconcentrated%2520in%2520low-frequency%2520components.%2520By%2520filtering%2520out%2520the%2520high-frequency%250Acomponents%252C%2520the%2520KV%2520cache%2520can%2520be%2520effectively%2520compressed%2520with%2520minimal%2520information%250Aloss.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520an%2520efficient%2520compression%2520technique%252C%250AFreqKV%252C%2520that%2520iteratively%2520compresses%2520the%2520increasing%2520KV%2520cache%2520to%2520a%2520fixed%2520size%2520in%250Athe%2520frequency%2520domain%252C%2520applicable%2520to%2520both%2520fine-tuning%2520and%2520inference.%2520FreqKV%250Aintroduces%2520no%2520additional%2520parameters%2520or%2520architectural%2520modifications.%2520With%250Aminimal%2520fine-tuning%252C%2520LLMs%2520can%2520learn%2520to%2520leverage%2520the%2520limited%2520cache%2520that%2520is%250Acompressed%2520in%2520the%2520frequency%2520domain%2520and%2520extend%2520the%2520context%2520window%2520efficiently.%250AExperiments%2520on%2520various%2520long%2520context%2520language%2520modeling%2520and%2520understanding%2520tasks%250Ademonstrate%2520the%2520efficiency%2520and%2520efficacy%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreqKV%3A%20Frequency%20Domain%20Key-Value%20Compression%20for%20Efficient%20Context%0A%20%20Window%20Extension&entry.906535625=Jushi%20Kai%20and%20Boyi%20Zeng%20and%20Yixuan%20Wang%20and%20Haoli%20Bai%20and%20Bo%20Jiang%20and%20Zhouhan%20Lin&entry.1292438233=%20%20Extending%20the%20context%20window%20in%20large%20language%20models%20%28LLMs%29%20is%20essential%20for%0Aapplications%20involving%20long-form%20content%20generation.%20However%2C%20the%20linear%0Aincrease%20in%20key-value%20%28KV%29%20cache%20memory%20requirements%20and%20the%20quadratic%0Acomplexity%20of%20self-attention%20with%20respect%20to%20sequence%20length%20present%0Asignificant%20challenges%20during%20fine-tuning%20and%20inference.%20Existing%20methods%0Asuffer%20from%20performance%20degradation%20when%20extending%20to%20longer%20contexts.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20context%20extension%20method%20that%20optimizes%20both%0Afine-tuning%20and%20inference%20efficiency.%20Our%20method%20exploits%20a%20key%20observation%3A%20in%0Athe%20frequency%20domain%2C%20the%20energy%20distribution%20of%20the%20KV%20cache%20is%20primarily%0Aconcentrated%20in%20low-frequency%20components.%20By%20filtering%20out%20the%20high-frequency%0Acomponents%2C%20the%20KV%20cache%20can%20be%20effectively%20compressed%20with%20minimal%20information%0Aloss.%20Building%20on%20this%20insight%2C%20we%20propose%20an%20efficient%20compression%20technique%2C%0AFreqKV%2C%20that%20iteratively%20compresses%20the%20increasing%20KV%20cache%20to%20a%20fixed%20size%20in%0Athe%20frequency%20domain%2C%20applicable%20to%20both%20fine-tuning%20and%20inference.%20FreqKV%0Aintroduces%20no%20additional%20parameters%20or%20architectural%20modifications.%20With%0Aminimal%20fine-tuning%2C%20LLMs%20can%20learn%20to%20leverage%20the%20limited%20cache%20that%20is%0Acompressed%20in%20the%20frequency%20domain%20and%20extend%20the%20context%20window%20efficiently.%0AExperiments%20on%20various%20long%20context%20language%20modeling%20and%20understanding%20tasks%0Ademonstrate%20the%20efficiency%20and%20efficacy%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00570v1&entry.124074799=Read"},
{"title": "RayZer: A Self-supervised Large View Synthesis Model", "author": "Hanwen Jiang and Hao Tan and Peng Wang and Haian Jin and Yue Zhao and Sai Bi and Kai Zhang and Fujun Luan and Kalyan Sunkavalli and Qixing Huang and Georgios Pavlakos", "abstract": "  We present RayZer, a self-supervised multi-view 3D Vision model trained\nwithout any 3D supervision, i.e., camera poses and scene geometry, while\nexhibiting emerging 3D awareness. Concretely, RayZer takes unposed and\nuncalibrated images as input, recovers camera parameters, reconstructs a scene\nrepresentation, and synthesizes novel views. During training, RayZer relies\nsolely on its self-predicted camera poses to render target views, eliminating\nthe need for any ground-truth camera annotations and allowing RayZer to be\ntrained with 2D image supervision. The emerging 3D awareness of RayZer is\nattributed to two key factors. First, we design a self-supervised framework,\nwhich achieves 3D-aware auto-encoding of input images by disentangling camera\nand scene representations. Second, we design a transformer-based model in which\nthe only 3D prior is the ray structure, connecting camera, pixel, and scene\nsimultaneously. RayZer demonstrates comparable or even superior novel view\nsynthesis performance than ``oracle'' methods that rely on pose annotations in\nboth training and testing. Project: https://hwjiang1510.github.io/RayZer/\n", "link": "http://arxiv.org/abs/2505.00702v1", "date": "2025-05-01", "relevancy": 2.416, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RayZer%3A%20A%20Self-supervised%20Large%20View%20Synthesis%20Model&body=Title%3A%20RayZer%3A%20A%20Self-supervised%20Large%20View%20Synthesis%20Model%0AAuthor%3A%20Hanwen%20Jiang%20and%20Hao%20Tan%20and%20Peng%20Wang%20and%20Haian%20Jin%20and%20Yue%20Zhao%20and%20Sai%20Bi%20and%20Kai%20Zhang%20and%20Fujun%20Luan%20and%20Kalyan%20Sunkavalli%20and%20Qixing%20Huang%20and%20Georgios%20Pavlakos%0AAbstract%3A%20%20%20We%20present%20RayZer%2C%20a%20self-supervised%20multi-view%203D%20Vision%20model%20trained%0Awithout%20any%203D%20supervision%2C%20i.e.%2C%20camera%20poses%20and%20scene%20geometry%2C%20while%0Aexhibiting%20emerging%203D%20awareness.%20Concretely%2C%20RayZer%20takes%20unposed%20and%0Auncalibrated%20images%20as%20input%2C%20recovers%20camera%20parameters%2C%20reconstructs%20a%20scene%0Arepresentation%2C%20and%20synthesizes%20novel%20views.%20During%20training%2C%20RayZer%20relies%0Asolely%20on%20its%20self-predicted%20camera%20poses%20to%20render%20target%20views%2C%20eliminating%0Athe%20need%20for%20any%20ground-truth%20camera%20annotations%20and%20allowing%20RayZer%20to%20be%0Atrained%20with%202D%20image%20supervision.%20The%20emerging%203D%20awareness%20of%20RayZer%20is%0Aattributed%20to%20two%20key%20factors.%20First%2C%20we%20design%20a%20self-supervised%20framework%2C%0Awhich%20achieves%203D-aware%20auto-encoding%20of%20input%20images%20by%20disentangling%20camera%0Aand%20scene%20representations.%20Second%2C%20we%20design%20a%20transformer-based%20model%20in%20which%0Athe%20only%203D%20prior%20is%20the%20ray%20structure%2C%20connecting%20camera%2C%20pixel%2C%20and%20scene%0Asimultaneously.%20RayZer%20demonstrates%20comparable%20or%20even%20superior%20novel%20view%0Asynthesis%20performance%20than%20%60%60oracle%27%27%20methods%20that%20rely%20on%20pose%20annotations%20in%0Aboth%20training%20and%20testing.%20Project%3A%20https%3A//hwjiang1510.github.io/RayZer/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRayZer%253A%2520A%2520Self-supervised%2520Large%2520View%2520Synthesis%2520Model%26entry.906535625%3DHanwen%2520Jiang%2520and%2520Hao%2520Tan%2520and%2520Peng%2520Wang%2520and%2520Haian%2520Jin%2520and%2520Yue%2520Zhao%2520and%2520Sai%2520Bi%2520and%2520Kai%2520Zhang%2520and%2520Fujun%2520Luan%2520and%2520Kalyan%2520Sunkavalli%2520and%2520Qixing%2520Huang%2520and%2520Georgios%2520Pavlakos%26entry.1292438233%3D%2520%2520We%2520present%2520RayZer%252C%2520a%2520self-supervised%2520multi-view%25203D%2520Vision%2520model%2520trained%250Awithout%2520any%25203D%2520supervision%252C%2520i.e.%252C%2520camera%2520poses%2520and%2520scene%2520geometry%252C%2520while%250Aexhibiting%2520emerging%25203D%2520awareness.%2520Concretely%252C%2520RayZer%2520takes%2520unposed%2520and%250Auncalibrated%2520images%2520as%2520input%252C%2520recovers%2520camera%2520parameters%252C%2520reconstructs%2520a%2520scene%250Arepresentation%252C%2520and%2520synthesizes%2520novel%2520views.%2520During%2520training%252C%2520RayZer%2520relies%250Asolely%2520on%2520its%2520self-predicted%2520camera%2520poses%2520to%2520render%2520target%2520views%252C%2520eliminating%250Athe%2520need%2520for%2520any%2520ground-truth%2520camera%2520annotations%2520and%2520allowing%2520RayZer%2520to%2520be%250Atrained%2520with%25202D%2520image%2520supervision.%2520The%2520emerging%25203D%2520awareness%2520of%2520RayZer%2520is%250Aattributed%2520to%2520two%2520key%2520factors.%2520First%252C%2520we%2520design%2520a%2520self-supervised%2520framework%252C%250Awhich%2520achieves%25203D-aware%2520auto-encoding%2520of%2520input%2520images%2520by%2520disentangling%2520camera%250Aand%2520scene%2520representations.%2520Second%252C%2520we%2520design%2520a%2520transformer-based%2520model%2520in%2520which%250Athe%2520only%25203D%2520prior%2520is%2520the%2520ray%2520structure%252C%2520connecting%2520camera%252C%2520pixel%252C%2520and%2520scene%250Asimultaneously.%2520RayZer%2520demonstrates%2520comparable%2520or%2520even%2520superior%2520novel%2520view%250Asynthesis%2520performance%2520than%2520%2560%2560oracle%2527%2527%2520methods%2520that%2520rely%2520on%2520pose%2520annotations%2520in%250Aboth%2520training%2520and%2520testing.%2520Project%253A%2520https%253A//hwjiang1510.github.io/RayZer/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RayZer%3A%20A%20Self-supervised%20Large%20View%20Synthesis%20Model&entry.906535625=Hanwen%20Jiang%20and%20Hao%20Tan%20and%20Peng%20Wang%20and%20Haian%20Jin%20and%20Yue%20Zhao%20and%20Sai%20Bi%20and%20Kai%20Zhang%20and%20Fujun%20Luan%20and%20Kalyan%20Sunkavalli%20and%20Qixing%20Huang%20and%20Georgios%20Pavlakos&entry.1292438233=%20%20We%20present%20RayZer%2C%20a%20self-supervised%20multi-view%203D%20Vision%20model%20trained%0Awithout%20any%203D%20supervision%2C%20i.e.%2C%20camera%20poses%20and%20scene%20geometry%2C%20while%0Aexhibiting%20emerging%203D%20awareness.%20Concretely%2C%20RayZer%20takes%20unposed%20and%0Auncalibrated%20images%20as%20input%2C%20recovers%20camera%20parameters%2C%20reconstructs%20a%20scene%0Arepresentation%2C%20and%20synthesizes%20novel%20views.%20During%20training%2C%20RayZer%20relies%0Asolely%20on%20its%20self-predicted%20camera%20poses%20to%20render%20target%20views%2C%20eliminating%0Athe%20need%20for%20any%20ground-truth%20camera%20annotations%20and%20allowing%20RayZer%20to%20be%0Atrained%20with%202D%20image%20supervision.%20The%20emerging%203D%20awareness%20of%20RayZer%20is%0Aattributed%20to%20two%20key%20factors.%20First%2C%20we%20design%20a%20self-supervised%20framework%2C%0Awhich%20achieves%203D-aware%20auto-encoding%20of%20input%20images%20by%20disentangling%20camera%0Aand%20scene%20representations.%20Second%2C%20we%20design%20a%20transformer-based%20model%20in%20which%0Athe%20only%203D%20prior%20is%20the%20ray%20structure%2C%20connecting%20camera%2C%20pixel%2C%20and%20scene%0Asimultaneously.%20RayZer%20demonstrates%20comparable%20or%20even%20superior%20novel%20view%0Asynthesis%20performance%20than%20%60%60oracle%27%27%20methods%20that%20rely%20on%20pose%20annotations%20in%0Aboth%20training%20and%20testing.%20Project%3A%20https%3A//hwjiang1510.github.io/RayZer/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00702v1&entry.124074799=Read"},
{"title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning\n  (and How to Fix Them)", "author": "Zihao Wang and Yibo Jiang and Jiahao Yu and Heqing Huang", "abstract": "  Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers.\n", "link": "http://arxiv.org/abs/2505.00626v1", "date": "2025-05-01", "relevancy": 2.3906, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Illusion%20of%20Role%20Separation%3A%20Hidden%20Shortcuts%20in%20LLM%20Role%20Learning%0A%20%20%28and%20How%20to%20Fix%20Them%29&body=Title%3A%20The%20Illusion%20of%20Role%20Separation%3A%20Hidden%20Shortcuts%20in%20LLM%20Role%20Learning%0A%20%20%28and%20How%20to%20Fix%20Them%29%0AAuthor%3A%20Zihao%20Wang%20and%20Yibo%20Jiang%20and%20Jiahao%20Yu%20and%20Heqing%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20that%20integrate%20multiple%20input%20roles%20%28e.g.%2C%0Asystem%20instructions%2C%20user%20queries%2C%20external%20tool%20outputs%29%20are%20increasingly%0Aprevalent%20in%20practice.%20Ensuring%20that%20the%20model%20accurately%20distinguishes%0Amessages%20from%20each%20role%20--%20a%20concept%20we%20call%20%5Cemph%7Brole%20separation%7D%20--%20is%0Acrucial%20for%20consistent%20multi-role%20behavior.%20Although%20recent%20work%20often%20targets%0Astate-of-the-art%20prompt%20injection%20defenses%2C%20it%20remains%20unclear%20whether%20such%0Amethods%20truly%20teach%20LLMs%20to%20differentiate%20roles%20or%20merely%20memorize%20known%0Atriggers.%20In%20this%20paper%2C%20we%20examine%20%5Cemph%7Brole-separation%20learning%7D%3A%20the%0Aprocess%20of%20teaching%20LLMs%20to%20robustly%20distinguish%20system%20and%20user%20tokens.%0AThrough%20a%20%5Cemph%7Bsimple%2C%20controlled%20experimental%20framework%7D%2C%20we%20find%20that%0Afine-tuned%20models%20often%20rely%20on%20two%20proxies%20for%20role%20identification%3A%20%281%29%20task%0Atype%20exploitation%2C%20and%20%282%29%20proximity%20to%20begin-of-text.%20Although%20data%0Aaugmentation%20can%20partially%20mitigate%20these%20shortcuts%2C%20it%20generally%20leads%20to%0Aiterative%20patching%20rather%20than%20a%20deeper%20fix.%20To%20address%20this%2C%20we%20propose%0Areinforcing%20%5Cemph%7Binvariant%20signals%7D%20that%20mark%20role%20boundaries%20by%20adjusting%0Atoken-wise%20cues%20in%20the%20model%27s%20input%20encoding.%20In%20particular%2C%20manipulating%0Aposition%20IDs%20helps%20the%20model%20learn%20clearer%20distinctions%20and%20reduces%20reliance%20on%0Asuperficial%20proxies.%20By%20focusing%20on%20this%20mechanism-centered%20perspective%2C%20our%0Awork%20illuminates%20how%20LLMs%20can%20more%20reliably%20maintain%20consistent%20multi-role%0Abehavior%20without%20merely%20memorizing%20known%20prompts%20or%20triggers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Illusion%2520of%2520Role%2520Separation%253A%2520Hidden%2520Shortcuts%2520in%2520LLM%2520Role%2520Learning%250A%2520%2520%2528and%2520How%2520to%2520Fix%2520Them%2529%26entry.906535625%3DZihao%2520Wang%2520and%2520Yibo%2520Jiang%2520and%2520Jiahao%2520Yu%2520and%2520Heqing%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520that%2520integrate%2520multiple%2520input%2520roles%2520%2528e.g.%252C%250Asystem%2520instructions%252C%2520user%2520queries%252C%2520external%2520tool%2520outputs%2529%2520are%2520increasingly%250Aprevalent%2520in%2520practice.%2520Ensuring%2520that%2520the%2520model%2520accurately%2520distinguishes%250Amessages%2520from%2520each%2520role%2520--%2520a%2520concept%2520we%2520call%2520%255Cemph%257Brole%2520separation%257D%2520--%2520is%250Acrucial%2520for%2520consistent%2520multi-role%2520behavior.%2520Although%2520recent%2520work%2520often%2520targets%250Astate-of-the-art%2520prompt%2520injection%2520defenses%252C%2520it%2520remains%2520unclear%2520whether%2520such%250Amethods%2520truly%2520teach%2520LLMs%2520to%2520differentiate%2520roles%2520or%2520merely%2520memorize%2520known%250Atriggers.%2520In%2520this%2520paper%252C%2520we%2520examine%2520%255Cemph%257Brole-separation%2520learning%257D%253A%2520the%250Aprocess%2520of%2520teaching%2520LLMs%2520to%2520robustly%2520distinguish%2520system%2520and%2520user%2520tokens.%250AThrough%2520a%2520%255Cemph%257Bsimple%252C%2520controlled%2520experimental%2520framework%257D%252C%2520we%2520find%2520that%250Afine-tuned%2520models%2520often%2520rely%2520on%2520two%2520proxies%2520for%2520role%2520identification%253A%2520%25281%2529%2520task%250Atype%2520exploitation%252C%2520and%2520%25282%2529%2520proximity%2520to%2520begin-of-text.%2520Although%2520data%250Aaugmentation%2520can%2520partially%2520mitigate%2520these%2520shortcuts%252C%2520it%2520generally%2520leads%2520to%250Aiterative%2520patching%2520rather%2520than%2520a%2520deeper%2520fix.%2520To%2520address%2520this%252C%2520we%2520propose%250Areinforcing%2520%255Cemph%257Binvariant%2520signals%257D%2520that%2520mark%2520role%2520boundaries%2520by%2520adjusting%250Atoken-wise%2520cues%2520in%2520the%2520model%2527s%2520input%2520encoding.%2520In%2520particular%252C%2520manipulating%250Aposition%2520IDs%2520helps%2520the%2520model%2520learn%2520clearer%2520distinctions%2520and%2520reduces%2520reliance%2520on%250Asuperficial%2520proxies.%2520By%2520focusing%2520on%2520this%2520mechanism-centered%2520perspective%252C%2520our%250Awork%2520illuminates%2520how%2520LLMs%2520can%2520more%2520reliably%2520maintain%2520consistent%2520multi-role%250Abehavior%2520without%2520merely%2520memorizing%2520known%2520prompts%2520or%2520triggers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Illusion%20of%20Role%20Separation%3A%20Hidden%20Shortcuts%20in%20LLM%20Role%20Learning%0A%20%20%28and%20How%20to%20Fix%20Them%29&entry.906535625=Zihao%20Wang%20and%20Yibo%20Jiang%20and%20Jiahao%20Yu%20and%20Heqing%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20that%20integrate%20multiple%20input%20roles%20%28e.g.%2C%0Asystem%20instructions%2C%20user%20queries%2C%20external%20tool%20outputs%29%20are%20increasingly%0Aprevalent%20in%20practice.%20Ensuring%20that%20the%20model%20accurately%20distinguishes%0Amessages%20from%20each%20role%20--%20a%20concept%20we%20call%20%5Cemph%7Brole%20separation%7D%20--%20is%0Acrucial%20for%20consistent%20multi-role%20behavior.%20Although%20recent%20work%20often%20targets%0Astate-of-the-art%20prompt%20injection%20defenses%2C%20it%20remains%20unclear%20whether%20such%0Amethods%20truly%20teach%20LLMs%20to%20differentiate%20roles%20or%20merely%20memorize%20known%0Atriggers.%20In%20this%20paper%2C%20we%20examine%20%5Cemph%7Brole-separation%20learning%7D%3A%20the%0Aprocess%20of%20teaching%20LLMs%20to%20robustly%20distinguish%20system%20and%20user%20tokens.%0AThrough%20a%20%5Cemph%7Bsimple%2C%20controlled%20experimental%20framework%7D%2C%20we%20find%20that%0Afine-tuned%20models%20often%20rely%20on%20two%20proxies%20for%20role%20identification%3A%20%281%29%20task%0Atype%20exploitation%2C%20and%20%282%29%20proximity%20to%20begin-of-text.%20Although%20data%0Aaugmentation%20can%20partially%20mitigate%20these%20shortcuts%2C%20it%20generally%20leads%20to%0Aiterative%20patching%20rather%20than%20a%20deeper%20fix.%20To%20address%20this%2C%20we%20propose%0Areinforcing%20%5Cemph%7Binvariant%20signals%7D%20that%20mark%20role%20boundaries%20by%20adjusting%0Atoken-wise%20cues%20in%20the%20model%27s%20input%20encoding.%20In%20particular%2C%20manipulating%0Aposition%20IDs%20helps%20the%20model%20learn%20clearer%20distinctions%20and%20reduces%20reliance%20on%0Asuperficial%20proxies.%20By%20focusing%20on%20this%20mechanism-centered%20perspective%2C%20our%0Awork%20illuminates%20how%20LLMs%20can%20more%20reliably%20maintain%20consistent%20multi-role%0Abehavior%20without%20merely%20memorizing%20known%20prompts%20or%20triggers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00626v1&entry.124074799=Read"},
{"title": "Fast and Low-Cost Genomic Foundation Models via Outlier Removal", "author": "Haozheng Luo and Chenghao Qiu and Maojiang Su and Zhihan Zhou and Zoe Mehta and Guo Ye and Jerry Yao-Chieh Hu and Han Liu", "abstract": "  We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM\noffers the first comprehensive evaluation framework to systematically assess\nthe vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate\nthe adversarial robustness of five state-of-the-art GFMs using four widely\nadopted attack algorithms and three defense strategies. Importantly, our\nbenchmark provides an accessible and comprehensive framework to analyze GFM\nvulnerabilities with respect to model architecture, quantization schemes, and\ntraining datasets. Empirically, transformer-based models exhibit greater\nrobustness to adversarial perturbations compared to HyenaDNA, highlighting the\nimpact of architectural design on vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features.\n", "link": "http://arxiv.org/abs/2505.00598v1", "date": "2025-05-01", "relevancy": 2.3791, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.473}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Low-Cost%20Genomic%20Foundation%20Models%20via%20Outlier%20Removal&body=Title%3A%20Fast%20and%20Low-Cost%20Genomic%20Foundation%20Models%20via%20Outlier%20Removal%0AAuthor%3A%20Haozheng%20Luo%20and%20Chenghao%20Qiu%20and%20Maojiang%20Su%20and%20Zhihan%20Zhou%20and%20Zoe%20Mehta%20and%20Guo%20Ye%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Han%20Liu%0AAbstract%3A%20%20%20We%20propose%20the%20first%20unified%20adversarial%20attack%20benchmark%20for%20Genomic%0AFoundation%20Models%20%28GFMs%29%2C%20named%20GERM.%20Unlike%20existing%20GFM%20benchmarks%2C%20GERM%0Aoffers%20the%20first%20comprehensive%20evaluation%20framework%20to%20systematically%20assess%0Athe%20vulnerability%20of%20GFMs%20to%20adversarial%20attacks.%20Methodologically%2C%20we%20evaluate%0Athe%20adversarial%20robustness%20of%20five%20state-of-the-art%20GFMs%20using%20four%20widely%0Aadopted%20attack%20algorithms%20and%20three%20defense%20strategies.%20Importantly%2C%20our%0Abenchmark%20provides%20an%20accessible%20and%20comprehensive%20framework%20to%20analyze%20GFM%0Avulnerabilities%20with%20respect%20to%20model%20architecture%2C%20quantization%20schemes%2C%20and%0Atraining%20datasets.%20Empirically%2C%20transformer-based%20models%20exhibit%20greater%0Arobustness%20to%20adversarial%20perturbations%20compared%20to%20HyenaDNA%2C%20highlighting%20the%0Aimpact%20of%20architectural%20design%20on%20vulnerability.%20Moreover%2C%20adversarial%20attacks%0Afrequently%20target%20biologically%20significant%20genomic%20regions%2C%20suggesting%20that%0Athese%20models%20effectively%20capture%20meaningful%20sequence%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Low-Cost%2520Genomic%2520Foundation%2520Models%2520via%2520Outlier%2520Removal%26entry.906535625%3DHaozheng%2520Luo%2520and%2520Chenghao%2520Qiu%2520and%2520Maojiang%2520Su%2520and%2520Zhihan%2520Zhou%2520and%2520Zoe%2520Mehta%2520and%2520Guo%2520Ye%2520and%2520Jerry%2520Yao-Chieh%2520Hu%2520and%2520Han%2520Liu%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520first%2520unified%2520adversarial%2520attack%2520benchmark%2520for%2520Genomic%250AFoundation%2520Models%2520%2528GFMs%2529%252C%2520named%2520GERM.%2520Unlike%2520existing%2520GFM%2520benchmarks%252C%2520GERM%250Aoffers%2520the%2520first%2520comprehensive%2520evaluation%2520framework%2520to%2520systematically%2520assess%250Athe%2520vulnerability%2520of%2520GFMs%2520to%2520adversarial%2520attacks.%2520Methodologically%252C%2520we%2520evaluate%250Athe%2520adversarial%2520robustness%2520of%2520five%2520state-of-the-art%2520GFMs%2520using%2520four%2520widely%250Aadopted%2520attack%2520algorithms%2520and%2520three%2520defense%2520strategies.%2520Importantly%252C%2520our%250Abenchmark%2520provides%2520an%2520accessible%2520and%2520comprehensive%2520framework%2520to%2520analyze%2520GFM%250Avulnerabilities%2520with%2520respect%2520to%2520model%2520architecture%252C%2520quantization%2520schemes%252C%2520and%250Atraining%2520datasets.%2520Empirically%252C%2520transformer-based%2520models%2520exhibit%2520greater%250Arobustness%2520to%2520adversarial%2520perturbations%2520compared%2520to%2520HyenaDNA%252C%2520highlighting%2520the%250Aimpact%2520of%2520architectural%2520design%2520on%2520vulnerability.%2520Moreover%252C%2520adversarial%2520attacks%250Afrequently%2520target%2520biologically%2520significant%2520genomic%2520regions%252C%2520suggesting%2520that%250Athese%2520models%2520effectively%2520capture%2520meaningful%2520sequence%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Low-Cost%20Genomic%20Foundation%20Models%20via%20Outlier%20Removal&entry.906535625=Haozheng%20Luo%20and%20Chenghao%20Qiu%20and%20Maojiang%20Su%20and%20Zhihan%20Zhou%20and%20Zoe%20Mehta%20and%20Guo%20Ye%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Han%20Liu&entry.1292438233=%20%20We%20propose%20the%20first%20unified%20adversarial%20attack%20benchmark%20for%20Genomic%0AFoundation%20Models%20%28GFMs%29%2C%20named%20GERM.%20Unlike%20existing%20GFM%20benchmarks%2C%20GERM%0Aoffers%20the%20first%20comprehensive%20evaluation%20framework%20to%20systematically%20assess%0Athe%20vulnerability%20of%20GFMs%20to%20adversarial%20attacks.%20Methodologically%2C%20we%20evaluate%0Athe%20adversarial%20robustness%20of%20five%20state-of-the-art%20GFMs%20using%20four%20widely%0Aadopted%20attack%20algorithms%20and%20three%20defense%20strategies.%20Importantly%2C%20our%0Abenchmark%20provides%20an%20accessible%20and%20comprehensive%20framework%20to%20analyze%20GFM%0Avulnerabilities%20with%20respect%20to%20model%20architecture%2C%20quantization%20schemes%2C%20and%0Atraining%20datasets.%20Empirically%2C%20transformer-based%20models%20exhibit%20greater%0Arobustness%20to%20adversarial%20perturbations%20compared%20to%20HyenaDNA%2C%20highlighting%20the%0Aimpact%20of%20architectural%20design%20on%20vulnerability.%20Moreover%2C%20adversarial%20attacks%0Afrequently%20target%20biologically%20significant%20genomic%20regions%2C%20suggesting%20that%0Athese%20models%20effectively%20capture%20meaningful%20sequence%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00598v1&entry.124074799=Read"},
{"title": "OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery\n  and Survival Stratification", "author": "Atahan Karagoz", "abstract": "  Unsupervised learning of disease subtypes from multi-omics data presents a\nsignificant opportunity for advancing personalized medicine. We introduce\nOmicsCL, a modular contrastive learning framework that jointly embeds\nheterogeneous omics modalities-such as gene expression, DNA methylation, and\nmiRNA expression-into a unified latent space. Our method incorporates a\nsurvival-aware contrastive loss that encourages the model to learn\nrepresentations aligned with survival-related patterns, without relying on\nlabeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers\nclinically meaningful clusters and achieves strong unsupervised concordance\nwith patient survival. The framework demonstrates robustness across\nhyperparameter configurations and can be tuned to prioritize either subtype\ncoherence or survival stratification. Ablation studies confirm that integrating\nsurvival-aware loss significantly enhances the predictive power of learned\nembeddings. These results highlight the promise of contrastive objectives for\nbiological insight discovery in high-dimensional, heterogeneous omics data.\n", "link": "http://arxiv.org/abs/2505.00650v1", "date": "2025-05-01", "relevancy": 2.3715, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmicsCL%3A%20Unsupervised%20Contrastive%20Learning%20for%20Cancer%20Subtype%20Discovery%0A%20%20and%20Survival%20Stratification&body=Title%3A%20OmicsCL%3A%20Unsupervised%20Contrastive%20Learning%20for%20Cancer%20Subtype%20Discovery%0A%20%20and%20Survival%20Stratification%0AAuthor%3A%20Atahan%20Karagoz%0AAbstract%3A%20%20%20Unsupervised%20learning%20of%20disease%20subtypes%20from%20multi-omics%20data%20presents%20a%0Asignificant%20opportunity%20for%20advancing%20personalized%20medicine.%20We%20introduce%0AOmicsCL%2C%20a%20modular%20contrastive%20learning%20framework%20that%20jointly%20embeds%0Aheterogeneous%20omics%20modalities-such%20as%20gene%20expression%2C%20DNA%20methylation%2C%20and%0AmiRNA%20expression-into%20a%20unified%20latent%20space.%20Our%20method%20incorporates%20a%0Asurvival-aware%20contrastive%20loss%20that%20encourages%20the%20model%20to%20learn%0Arepresentations%20aligned%20with%20survival-related%20patterns%2C%20without%20relying%20on%0Alabeled%20outcomes.%20Evaluated%20on%20the%20TCGA%20BRCA%20dataset%2C%20OmicsCL%20uncovers%0Aclinically%20meaningful%20clusters%20and%20achieves%20strong%20unsupervised%20concordance%0Awith%20patient%20survival.%20The%20framework%20demonstrates%20robustness%20across%0Ahyperparameter%20configurations%20and%20can%20be%20tuned%20to%20prioritize%20either%20subtype%0Acoherence%20or%20survival%20stratification.%20Ablation%20studies%20confirm%20that%20integrating%0Asurvival-aware%20loss%20significantly%20enhances%20the%20predictive%20power%20of%20learned%0Aembeddings.%20These%20results%20highlight%20the%20promise%20of%20contrastive%20objectives%20for%0Abiological%20insight%20discovery%20in%20high-dimensional%2C%20heterogeneous%20omics%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmicsCL%253A%2520Unsupervised%2520Contrastive%2520Learning%2520for%2520Cancer%2520Subtype%2520Discovery%250A%2520%2520and%2520Survival%2520Stratification%26entry.906535625%3DAtahan%2520Karagoz%26entry.1292438233%3D%2520%2520Unsupervised%2520learning%2520of%2520disease%2520subtypes%2520from%2520multi-omics%2520data%2520presents%2520a%250Asignificant%2520opportunity%2520for%2520advancing%2520personalized%2520medicine.%2520We%2520introduce%250AOmicsCL%252C%2520a%2520modular%2520contrastive%2520learning%2520framework%2520that%2520jointly%2520embeds%250Aheterogeneous%2520omics%2520modalities-such%2520as%2520gene%2520expression%252C%2520DNA%2520methylation%252C%2520and%250AmiRNA%2520expression-into%2520a%2520unified%2520latent%2520space.%2520Our%2520method%2520incorporates%2520a%250Asurvival-aware%2520contrastive%2520loss%2520that%2520encourages%2520the%2520model%2520to%2520learn%250Arepresentations%2520aligned%2520with%2520survival-related%2520patterns%252C%2520without%2520relying%2520on%250Alabeled%2520outcomes.%2520Evaluated%2520on%2520the%2520TCGA%2520BRCA%2520dataset%252C%2520OmicsCL%2520uncovers%250Aclinically%2520meaningful%2520clusters%2520and%2520achieves%2520strong%2520unsupervised%2520concordance%250Awith%2520patient%2520survival.%2520The%2520framework%2520demonstrates%2520robustness%2520across%250Ahyperparameter%2520configurations%2520and%2520can%2520be%2520tuned%2520to%2520prioritize%2520either%2520subtype%250Acoherence%2520or%2520survival%2520stratification.%2520Ablation%2520studies%2520confirm%2520that%2520integrating%250Asurvival-aware%2520loss%2520significantly%2520enhances%2520the%2520predictive%2520power%2520of%2520learned%250Aembeddings.%2520These%2520results%2520highlight%2520the%2520promise%2520of%2520contrastive%2520objectives%2520for%250Abiological%2520insight%2520discovery%2520in%2520high-dimensional%252C%2520heterogeneous%2520omics%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmicsCL%3A%20Unsupervised%20Contrastive%20Learning%20for%20Cancer%20Subtype%20Discovery%0A%20%20and%20Survival%20Stratification&entry.906535625=Atahan%20Karagoz&entry.1292438233=%20%20Unsupervised%20learning%20of%20disease%20subtypes%20from%20multi-omics%20data%20presents%20a%0Asignificant%20opportunity%20for%20advancing%20personalized%20medicine.%20We%20introduce%0AOmicsCL%2C%20a%20modular%20contrastive%20learning%20framework%20that%20jointly%20embeds%0Aheterogeneous%20omics%20modalities-such%20as%20gene%20expression%2C%20DNA%20methylation%2C%20and%0AmiRNA%20expression-into%20a%20unified%20latent%20space.%20Our%20method%20incorporates%20a%0Asurvival-aware%20contrastive%20loss%20that%20encourages%20the%20model%20to%20learn%0Arepresentations%20aligned%20with%20survival-related%20patterns%2C%20without%20relying%20on%0Alabeled%20outcomes.%20Evaluated%20on%20the%20TCGA%20BRCA%20dataset%2C%20OmicsCL%20uncovers%0Aclinically%20meaningful%20clusters%20and%20achieves%20strong%20unsupervised%20concordance%0Awith%20patient%20survival.%20The%20framework%20demonstrates%20robustness%20across%0Ahyperparameter%20configurations%20and%20can%20be%20tuned%20to%20prioritize%20either%20subtype%0Acoherence%20or%20survival%20stratification.%20Ablation%20studies%20confirm%20that%20integrating%0Asurvival-aware%20loss%20significantly%20enhances%20the%20predictive%20power%20of%20learned%0Aembeddings.%20These%20results%20highlight%20the%20promise%20of%20contrastive%20objectives%20for%0Abiological%20insight%20discovery%20in%20high-dimensional%2C%20heterogeneous%20omics%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00650v1&entry.124074799=Read"},
{"title": "HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection", "author": "Esteban Rivera and Surya Prabhakaran and Markus Lienkamp", "abstract": "  Active Learning has proved to be a relevant approach to perform sample\nselection for training models for Autonomous Driving. Particularly, previous\nworks on active learning for 3D object detection have shown that selection of\nsamples in uncontrolled scenarios is challenging. Furthermore, current\napproaches focus exclusively on the theoretical aspects of the sample selection\nproblem but neglect the practical insights that can be obtained from the\nextensive literature and application of 3D detection models. In this paper, we\nintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)\nwhich integrates those heuristical features together with Localization and\nClassification to deliver the most contributing samples to the model's\ntraining. In contrast to previous works, our approach integrates heuristical\nfeatures such as object distance and point-quantity to estimate the\nuncertainty, which enhance the usefulness of selected samples to train\ndetection models. Our quantitative evaluation on KITTI shows that HeAL presents\ncompetitive mAP with respect to the State-of-the-Art, and achieves the same mAP\nas the full-supervised baseline with only 24% of the samples.\n", "link": "http://arxiv.org/abs/2505.00507v1", "date": "2025-05-01", "relevancy": 2.3648, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6448}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6054}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeAL3D%3A%20Heuristical-enhanced%20Active%20Learning%20for%203D%20Object%20Detection&body=Title%3A%20HeAL3D%3A%20Heuristical-enhanced%20Active%20Learning%20for%203D%20Object%20Detection%0AAuthor%3A%20Esteban%20Rivera%20and%20Surya%20Prabhakaran%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20Active%20Learning%20has%20proved%20to%20be%20a%20relevant%20approach%20to%20perform%20sample%0Aselection%20for%20training%20models%20for%20Autonomous%20Driving.%20Particularly%2C%20previous%0Aworks%20on%20active%20learning%20for%203D%20object%20detection%20have%20shown%20that%20selection%20of%0Asamples%20in%20uncontrolled%20scenarios%20is%20challenging.%20Furthermore%2C%20current%0Aapproaches%20focus%20exclusively%20on%20the%20theoretical%20aspects%20of%20the%20sample%20selection%0Aproblem%20but%20neglect%20the%20practical%20insights%20that%20can%20be%20obtained%20from%20the%0Aextensive%20literature%20and%20application%20of%203D%20detection%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20HeAL%20%28Heuristical-enhanced%20Active%20Learning%20for%203D%20Object%20Detection%29%0Awhich%20integrates%20those%20heuristical%20features%20together%20with%20Localization%20and%0AClassification%20to%20deliver%20the%20most%20contributing%20samples%20to%20the%20model%27s%0Atraining.%20In%20contrast%20to%20previous%20works%2C%20our%20approach%20integrates%20heuristical%0Afeatures%20such%20as%20object%20distance%20and%20point-quantity%20to%20estimate%20the%0Auncertainty%2C%20which%20enhance%20the%20usefulness%20of%20selected%20samples%20to%20train%0Adetection%20models.%20Our%20quantitative%20evaluation%20on%20KITTI%20shows%20that%20HeAL%20presents%0Acompetitive%20mAP%20with%20respect%20to%20the%20State-of-the-Art%2C%20and%20achieves%20the%20same%20mAP%0Aas%20the%20full-supervised%20baseline%20with%20only%2024%25%20of%20the%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeAL3D%253A%2520Heuristical-enhanced%2520Active%2520Learning%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DEsteban%2520Rivera%2520and%2520Surya%2520Prabhakaran%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520Active%2520Learning%2520has%2520proved%2520to%2520be%2520a%2520relevant%2520approach%2520to%2520perform%2520sample%250Aselection%2520for%2520training%2520models%2520for%2520Autonomous%2520Driving.%2520Particularly%252C%2520previous%250Aworks%2520on%2520active%2520learning%2520for%25203D%2520object%2520detection%2520have%2520shown%2520that%2520selection%2520of%250Asamples%2520in%2520uncontrolled%2520scenarios%2520is%2520challenging.%2520Furthermore%252C%2520current%250Aapproaches%2520focus%2520exclusively%2520on%2520the%2520theoretical%2520aspects%2520of%2520the%2520sample%2520selection%250Aproblem%2520but%2520neglect%2520the%2520practical%2520insights%2520that%2520can%2520be%2520obtained%2520from%2520the%250Aextensive%2520literature%2520and%2520application%2520of%25203D%2520detection%2520models.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520HeAL%2520%2528Heuristical-enhanced%2520Active%2520Learning%2520for%25203D%2520Object%2520Detection%2529%250Awhich%2520integrates%2520those%2520heuristical%2520features%2520together%2520with%2520Localization%2520and%250AClassification%2520to%2520deliver%2520the%2520most%2520contributing%2520samples%2520to%2520the%2520model%2527s%250Atraining.%2520In%2520contrast%2520to%2520previous%2520works%252C%2520our%2520approach%2520integrates%2520heuristical%250Afeatures%2520such%2520as%2520object%2520distance%2520and%2520point-quantity%2520to%2520estimate%2520the%250Auncertainty%252C%2520which%2520enhance%2520the%2520usefulness%2520of%2520selected%2520samples%2520to%2520train%250Adetection%2520models.%2520Our%2520quantitative%2520evaluation%2520on%2520KITTI%2520shows%2520that%2520HeAL%2520presents%250Acompetitive%2520mAP%2520with%2520respect%2520to%2520the%2520State-of-the-Art%252C%2520and%2520achieves%2520the%2520same%2520mAP%250Aas%2520the%2520full-supervised%2520baseline%2520with%2520only%252024%2525%2520of%2520the%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeAL3D%3A%20Heuristical-enhanced%20Active%20Learning%20for%203D%20Object%20Detection&entry.906535625=Esteban%20Rivera%20and%20Surya%20Prabhakaran%20and%20Markus%20Lienkamp&entry.1292438233=%20%20Active%20Learning%20has%20proved%20to%20be%20a%20relevant%20approach%20to%20perform%20sample%0Aselection%20for%20training%20models%20for%20Autonomous%20Driving.%20Particularly%2C%20previous%0Aworks%20on%20active%20learning%20for%203D%20object%20detection%20have%20shown%20that%20selection%20of%0Asamples%20in%20uncontrolled%20scenarios%20is%20challenging.%20Furthermore%2C%20current%0Aapproaches%20focus%20exclusively%20on%20the%20theoretical%20aspects%20of%20the%20sample%20selection%0Aproblem%20but%20neglect%20the%20practical%20insights%20that%20can%20be%20obtained%20from%20the%0Aextensive%20literature%20and%20application%20of%203D%20detection%20models.%20In%20this%20paper%2C%20we%0Aintroduce%20HeAL%20%28Heuristical-enhanced%20Active%20Learning%20for%203D%20Object%20Detection%29%0Awhich%20integrates%20those%20heuristical%20features%20together%20with%20Localization%20and%0AClassification%20to%20deliver%20the%20most%20contributing%20samples%20to%20the%20model%27s%0Atraining.%20In%20contrast%20to%20previous%20works%2C%20our%20approach%20integrates%20heuristical%0Afeatures%20such%20as%20object%20distance%20and%20point-quantity%20to%20estimate%20the%0Auncertainty%2C%20which%20enhance%20the%20usefulness%20of%20selected%20samples%20to%20train%0Adetection%20models.%20Our%20quantitative%20evaluation%20on%20KITTI%20shows%20that%20HeAL%20presents%0Acompetitive%20mAP%20with%20respect%20to%20the%20State-of-the-Art%2C%20and%20achieves%20the%20same%20mAP%0Aas%20the%20full-supervised%20baseline%20with%20only%2024%25%20of%20the%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00507v1&entry.124074799=Read"},
{"title": "LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image\n  Segmentation", "author": "Jiachen Li and Qing Xie and Renshu Gu and Jinyu Xu and Yongjian Liu and Xiaohan Yu", "abstract": "  Zero-shot referring image segmentation aims to locate and segment the target\nregion based on a referring expression, with the primary challenge of aligning\nand matching semantics across visual and textual modalities without training.\nPrevious works address this challenge by utilizing Vision-Language Models and\nmask proposal networks for region-text matching. However, this paradigm may\nlead to incorrect target localization due to the inherent ambiguity and\ndiversity of free-form referring expressions. To alleviate this issue, we\npresent LGD (Leveraging Generative Descriptions), a framework that utilizes the\nadvanced language generation capabilities of Multi-Modal Large Language Models\nto enhance region-text matching performance in Vision-Language Models.\nSpecifically, we first design two kinds of prompts, the attribute prompt and\nthe surrounding prompt, to guide the Multi-Modal Large Language Models in\ngenerating descriptions related to the crucial attributes of the referent\nobject and the details of surrounding objects, referred to as attribute\ndescription and surrounding description, respectively. Secondly, three\nvisual-text matching scores are introduced to evaluate the similarity between\ninstance-level visual features and textual features, which determines the mask\nmost associated with the referring expression. The proposed method achieves new\nstate-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and\nRefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU\ncompared to previous methods.\n", "link": "http://arxiv.org/abs/2504.14467v2", "date": "2025-05-01", "relevancy": 2.3506, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGD%3A%20Leveraging%20Generative%20Descriptions%20for%20Zero-Shot%20Referring%20Image%0A%20%20Segmentation&body=Title%3A%20LGD%3A%20Leveraging%20Generative%20Descriptions%20for%20Zero-Shot%20Referring%20Image%0A%20%20Segmentation%0AAuthor%3A%20Jiachen%20Li%20and%20Qing%20Xie%20and%20Renshu%20Gu%20and%20Jinyu%20Xu%20and%20Yongjian%20Liu%20and%20Xiaohan%20Yu%0AAbstract%3A%20%20%20Zero-shot%20referring%20image%20segmentation%20aims%20to%20locate%20and%20segment%20the%20target%0Aregion%20based%20on%20a%20referring%20expression%2C%20with%20the%20primary%20challenge%20of%20aligning%0Aand%20matching%20semantics%20across%20visual%20and%20textual%20modalities%20without%20training.%0APrevious%20works%20address%20this%20challenge%20by%20utilizing%20Vision-Language%20Models%20and%0Amask%20proposal%20networks%20for%20region-text%20matching.%20However%2C%20this%20paradigm%20may%0Alead%20to%20incorrect%20target%20localization%20due%20to%20the%20inherent%20ambiguity%20and%0Adiversity%20of%20free-form%20referring%20expressions.%20To%20alleviate%20this%20issue%2C%20we%0Apresent%20LGD%20%28Leveraging%20Generative%20Descriptions%29%2C%20a%20framework%20that%20utilizes%20the%0Aadvanced%20language%20generation%20capabilities%20of%20Multi-Modal%20Large%20Language%20Models%0Ato%20enhance%20region-text%20matching%20performance%20in%20Vision-Language%20Models.%0ASpecifically%2C%20we%20first%20design%20two%20kinds%20of%20prompts%2C%20the%20attribute%20prompt%20and%0Athe%20surrounding%20prompt%2C%20to%20guide%20the%20Multi-Modal%20Large%20Language%20Models%20in%0Agenerating%20descriptions%20related%20to%20the%20crucial%20attributes%20of%20the%20referent%0Aobject%20and%20the%20details%20of%20surrounding%20objects%2C%20referred%20to%20as%20attribute%0Adescription%20and%20surrounding%20description%2C%20respectively.%20Secondly%2C%20three%0Avisual-text%20matching%20scores%20are%20introduced%20to%20evaluate%20the%20similarity%20between%0Ainstance-level%20visual%20features%20and%20textual%20features%2C%20which%20determines%20the%20mask%0Amost%20associated%20with%20the%20referring%20expression.%20The%20proposed%20method%20achieves%20new%0Astate-of-the-art%20performance%20on%20three%20public%20datasets%20RefCOCO%2C%20RefCOCO%2B%20and%0ARefCOCOg%2C%20with%20maximum%20improvements%20of%209.97%25%20in%20oIoU%20and%2011.29%25%20in%20mIoU%0Acompared%20to%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGD%253A%2520Leveraging%2520Generative%2520Descriptions%2520for%2520Zero-Shot%2520Referring%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DJiachen%2520Li%2520and%2520Qing%2520Xie%2520and%2520Renshu%2520Gu%2520and%2520Jinyu%2520Xu%2520and%2520Yongjian%2520Liu%2520and%2520Xiaohan%2520Yu%26entry.1292438233%3D%2520%2520Zero-shot%2520referring%2520image%2520segmentation%2520aims%2520to%2520locate%2520and%2520segment%2520the%2520target%250Aregion%2520based%2520on%2520a%2520referring%2520expression%252C%2520with%2520the%2520primary%2520challenge%2520of%2520aligning%250Aand%2520matching%2520semantics%2520across%2520visual%2520and%2520textual%2520modalities%2520without%2520training.%250APrevious%2520works%2520address%2520this%2520challenge%2520by%2520utilizing%2520Vision-Language%2520Models%2520and%250Amask%2520proposal%2520networks%2520for%2520region-text%2520matching.%2520However%252C%2520this%2520paradigm%2520may%250Alead%2520to%2520incorrect%2520target%2520localization%2520due%2520to%2520the%2520inherent%2520ambiguity%2520and%250Adiversity%2520of%2520free-form%2520referring%2520expressions.%2520To%2520alleviate%2520this%2520issue%252C%2520we%250Apresent%2520LGD%2520%2528Leveraging%2520Generative%2520Descriptions%2529%252C%2520a%2520framework%2520that%2520utilizes%2520the%250Aadvanced%2520language%2520generation%2520capabilities%2520of%2520Multi-Modal%2520Large%2520Language%2520Models%250Ato%2520enhance%2520region-text%2520matching%2520performance%2520in%2520Vision-Language%2520Models.%250ASpecifically%252C%2520we%2520first%2520design%2520two%2520kinds%2520of%2520prompts%252C%2520the%2520attribute%2520prompt%2520and%250Athe%2520surrounding%2520prompt%252C%2520to%2520guide%2520the%2520Multi-Modal%2520Large%2520Language%2520Models%2520in%250Agenerating%2520descriptions%2520related%2520to%2520the%2520crucial%2520attributes%2520of%2520the%2520referent%250Aobject%2520and%2520the%2520details%2520of%2520surrounding%2520objects%252C%2520referred%2520to%2520as%2520attribute%250Adescription%2520and%2520surrounding%2520description%252C%2520respectively.%2520Secondly%252C%2520three%250Avisual-text%2520matching%2520scores%2520are%2520introduced%2520to%2520evaluate%2520the%2520similarity%2520between%250Ainstance-level%2520visual%2520features%2520and%2520textual%2520features%252C%2520which%2520determines%2520the%2520mask%250Amost%2520associated%2520with%2520the%2520referring%2520expression.%2520The%2520proposed%2520method%2520achieves%2520new%250Astate-of-the-art%2520performance%2520on%2520three%2520public%2520datasets%2520RefCOCO%252C%2520RefCOCO%252B%2520and%250ARefCOCOg%252C%2520with%2520maximum%2520improvements%2520of%25209.97%2525%2520in%2520oIoU%2520and%252011.29%2525%2520in%2520mIoU%250Acompared%2520to%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGD%3A%20Leveraging%20Generative%20Descriptions%20for%20Zero-Shot%20Referring%20Image%0A%20%20Segmentation&entry.906535625=Jiachen%20Li%20and%20Qing%20Xie%20and%20Renshu%20Gu%20and%20Jinyu%20Xu%20and%20Yongjian%20Liu%20and%20Xiaohan%20Yu&entry.1292438233=%20%20Zero-shot%20referring%20image%20segmentation%20aims%20to%20locate%20and%20segment%20the%20target%0Aregion%20based%20on%20a%20referring%20expression%2C%20with%20the%20primary%20challenge%20of%20aligning%0Aand%20matching%20semantics%20across%20visual%20and%20textual%20modalities%20without%20training.%0APrevious%20works%20address%20this%20challenge%20by%20utilizing%20Vision-Language%20Models%20and%0Amask%20proposal%20networks%20for%20region-text%20matching.%20However%2C%20this%20paradigm%20may%0Alead%20to%20incorrect%20target%20localization%20due%20to%20the%20inherent%20ambiguity%20and%0Adiversity%20of%20free-form%20referring%20expressions.%20To%20alleviate%20this%20issue%2C%20we%0Apresent%20LGD%20%28Leveraging%20Generative%20Descriptions%29%2C%20a%20framework%20that%20utilizes%20the%0Aadvanced%20language%20generation%20capabilities%20of%20Multi-Modal%20Large%20Language%20Models%0Ato%20enhance%20region-text%20matching%20performance%20in%20Vision-Language%20Models.%0ASpecifically%2C%20we%20first%20design%20two%20kinds%20of%20prompts%2C%20the%20attribute%20prompt%20and%0Athe%20surrounding%20prompt%2C%20to%20guide%20the%20Multi-Modal%20Large%20Language%20Models%20in%0Agenerating%20descriptions%20related%20to%20the%20crucial%20attributes%20of%20the%20referent%0Aobject%20and%20the%20details%20of%20surrounding%20objects%2C%20referred%20to%20as%20attribute%0Adescription%20and%20surrounding%20description%2C%20respectively.%20Secondly%2C%20three%0Avisual-text%20matching%20scores%20are%20introduced%20to%20evaluate%20the%20similarity%20between%0Ainstance-level%20visual%20features%20and%20textual%20features%2C%20which%20determines%20the%20mask%0Amost%20associated%20with%20the%20referring%20expression.%20The%20proposed%20method%20achieves%20new%0Astate-of-the-art%20performance%20on%20three%20public%20datasets%20RefCOCO%2C%20RefCOCO%2B%20and%0ARefCOCOg%2C%20with%20maximum%20improvements%20of%209.97%25%20in%20oIoU%20and%2011.29%25%20in%20mIoU%0Acompared%20to%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14467v2&entry.124074799=Read"},
{"title": "MINERVA: Evaluating Complex Video Reasoning", "author": "Arsha Nagrani and Sachit Menon and Ahmet Iscen and Shyamal Buch and Ramin Mehran and Nilpa Jha and Anja Hauth and Yukun Zhu and Carl Vondrick and Mikhail Sirotenko and Cordelia Schmid and Tobias Weyand", "abstract": "  Multimodal LLMs are turning their focus to video benchmarks, however most\nvideo benchmarks only provide outcome supervision, with no intermediate or\ninterpretable reasoning steps. This makes it challenging to assess if models\nare truly able to combine perceptual and temporal information to reason about\nvideos, or simply get the correct answer by chance or by exploiting linguistic\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\nfor modern multimodal models. Each question in the dataset comes with 5 answer\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\nmultimodal, diverse in terms of video domain and length, and consists of\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\nprovides a challenge for frontier open-source and proprietary models. We\nperform fine-grained error analysis to identify common failure modes across\nvarious models, and create a taxonomy of reasoning errors. We use this to\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\ntraces, and find that failure modes are primarily related to temporal\nlocalization, followed by visual perception errors, as opposed to logical or\ncompleteness errors. The dataset, along with questions, answer candidates and\nreasoning traces will be publicly available under\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva.\n", "link": "http://arxiv.org/abs/2505.00681v1", "date": "2025-05-01", "relevancy": 2.3446, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MINERVA%3A%20Evaluating%20Complex%20Video%20Reasoning&body=Title%3A%20MINERVA%3A%20Evaluating%20Complex%20Video%20Reasoning%0AAuthor%3A%20Arsha%20Nagrani%20and%20Sachit%20Menon%20and%20Ahmet%20Iscen%20and%20Shyamal%20Buch%20and%20Ramin%20Mehran%20and%20Nilpa%20Jha%20and%20Anja%20Hauth%20and%20Yukun%20Zhu%20and%20Carl%20Vondrick%20and%20Mikhail%20Sirotenko%20and%20Cordelia%20Schmid%20and%20Tobias%20Weyand%0AAbstract%3A%20%20%20Multimodal%20LLMs%20are%20turning%20their%20focus%20to%20video%20benchmarks%2C%20however%20most%0Avideo%20benchmarks%20only%20provide%20outcome%20supervision%2C%20with%20no%20intermediate%20or%0Ainterpretable%20reasoning%20steps.%20This%20makes%20it%20challenging%20to%20assess%20if%20models%0Aare%20truly%20able%20to%20combine%20perceptual%20and%20temporal%20information%20to%20reason%20about%0Avideos%2C%20or%20simply%20get%20the%20correct%20answer%20by%20chance%20or%20by%20exploiting%20linguistic%0Abiases.%20To%20remedy%20this%2C%20we%20provide%20a%20new%20video%20reasoning%20dataset%20called%20MINERVA%0Afor%20modern%20multimodal%20models.%20Each%20question%20in%20the%20dataset%20comes%20with%205%20answer%0Achoices%2C%20as%20well%20as%20detailed%2C%20hand-crafted%20reasoning%20traces.%20Our%20dataset%20is%0Amultimodal%2C%20diverse%20in%20terms%20of%20video%20domain%20and%20length%2C%20and%20consists%20of%0Acomplex%20multi-step%20questions.%20Extensive%20benchmarking%20shows%20that%20our%20dataset%0Aprovides%20a%20challenge%20for%20frontier%20open-source%20and%20proprietary%20models.%20We%0Aperform%20fine-grained%20error%20analysis%20to%20identify%20common%20failure%20modes%20across%0Avarious%20models%2C%20and%20create%20a%20taxonomy%20of%20reasoning%20errors.%20We%20use%20this%20to%0Aexplore%20both%20human%20and%20LLM-as-a-judge%20methods%20for%20scoring%20video%20reasoning%0Atraces%2C%20and%20find%20that%20failure%20modes%20are%20primarily%20related%20to%20temporal%0Alocalization%2C%20followed%20by%20visual%20perception%20errors%2C%20as%20opposed%20to%20logical%20or%0Acompleteness%20errors.%20The%20dataset%2C%20along%20with%20questions%2C%20answer%20candidates%20and%0Areasoning%20traces%20will%20be%20publicly%20available%20under%0Ahttps%3A//github.com/google-deepmind/neptune%3Ftab%3Dreadme-ov-file%5C%23minerva.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMINERVA%253A%2520Evaluating%2520Complex%2520Video%2520Reasoning%26entry.906535625%3DArsha%2520Nagrani%2520and%2520Sachit%2520Menon%2520and%2520Ahmet%2520Iscen%2520and%2520Shyamal%2520Buch%2520and%2520Ramin%2520Mehran%2520and%2520Nilpa%2520Jha%2520and%2520Anja%2520Hauth%2520and%2520Yukun%2520Zhu%2520and%2520Carl%2520Vondrick%2520and%2520Mikhail%2520Sirotenko%2520and%2520Cordelia%2520Schmid%2520and%2520Tobias%2520Weyand%26entry.1292438233%3D%2520%2520Multimodal%2520LLMs%2520are%2520turning%2520their%2520focus%2520to%2520video%2520benchmarks%252C%2520however%2520most%250Avideo%2520benchmarks%2520only%2520provide%2520outcome%2520supervision%252C%2520with%2520no%2520intermediate%2520or%250Ainterpretable%2520reasoning%2520steps.%2520This%2520makes%2520it%2520challenging%2520to%2520assess%2520if%2520models%250Aare%2520truly%2520able%2520to%2520combine%2520perceptual%2520and%2520temporal%2520information%2520to%2520reason%2520about%250Avideos%252C%2520or%2520simply%2520get%2520the%2520correct%2520answer%2520by%2520chance%2520or%2520by%2520exploiting%2520linguistic%250Abiases.%2520To%2520remedy%2520this%252C%2520we%2520provide%2520a%2520new%2520video%2520reasoning%2520dataset%2520called%2520MINERVA%250Afor%2520modern%2520multimodal%2520models.%2520Each%2520question%2520in%2520the%2520dataset%2520comes%2520with%25205%2520answer%250Achoices%252C%2520as%2520well%2520as%2520detailed%252C%2520hand-crafted%2520reasoning%2520traces.%2520Our%2520dataset%2520is%250Amultimodal%252C%2520diverse%2520in%2520terms%2520of%2520video%2520domain%2520and%2520length%252C%2520and%2520consists%2520of%250Acomplex%2520multi-step%2520questions.%2520Extensive%2520benchmarking%2520shows%2520that%2520our%2520dataset%250Aprovides%2520a%2520challenge%2520for%2520frontier%2520open-source%2520and%2520proprietary%2520models.%2520We%250Aperform%2520fine-grained%2520error%2520analysis%2520to%2520identify%2520common%2520failure%2520modes%2520across%250Avarious%2520models%252C%2520and%2520create%2520a%2520taxonomy%2520of%2520reasoning%2520errors.%2520We%2520use%2520this%2520to%250Aexplore%2520both%2520human%2520and%2520LLM-as-a-judge%2520methods%2520for%2520scoring%2520video%2520reasoning%250Atraces%252C%2520and%2520find%2520that%2520failure%2520modes%2520are%2520primarily%2520related%2520to%2520temporal%250Alocalization%252C%2520followed%2520by%2520visual%2520perception%2520errors%252C%2520as%2520opposed%2520to%2520logical%2520or%250Acompleteness%2520errors.%2520The%2520dataset%252C%2520along%2520with%2520questions%252C%2520answer%2520candidates%2520and%250Areasoning%2520traces%2520will%2520be%2520publicly%2520available%2520under%250Ahttps%253A//github.com/google-deepmind/neptune%253Ftab%253Dreadme-ov-file%255C%2523minerva.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MINERVA%3A%20Evaluating%20Complex%20Video%20Reasoning&entry.906535625=Arsha%20Nagrani%20and%20Sachit%20Menon%20and%20Ahmet%20Iscen%20and%20Shyamal%20Buch%20and%20Ramin%20Mehran%20and%20Nilpa%20Jha%20and%20Anja%20Hauth%20and%20Yukun%20Zhu%20and%20Carl%20Vondrick%20and%20Mikhail%20Sirotenko%20and%20Cordelia%20Schmid%20and%20Tobias%20Weyand&entry.1292438233=%20%20Multimodal%20LLMs%20are%20turning%20their%20focus%20to%20video%20benchmarks%2C%20however%20most%0Avideo%20benchmarks%20only%20provide%20outcome%20supervision%2C%20with%20no%20intermediate%20or%0Ainterpretable%20reasoning%20steps.%20This%20makes%20it%20challenging%20to%20assess%20if%20models%0Aare%20truly%20able%20to%20combine%20perceptual%20and%20temporal%20information%20to%20reason%20about%0Avideos%2C%20or%20simply%20get%20the%20correct%20answer%20by%20chance%20or%20by%20exploiting%20linguistic%0Abiases.%20To%20remedy%20this%2C%20we%20provide%20a%20new%20video%20reasoning%20dataset%20called%20MINERVA%0Afor%20modern%20multimodal%20models.%20Each%20question%20in%20the%20dataset%20comes%20with%205%20answer%0Achoices%2C%20as%20well%20as%20detailed%2C%20hand-crafted%20reasoning%20traces.%20Our%20dataset%20is%0Amultimodal%2C%20diverse%20in%20terms%20of%20video%20domain%20and%20length%2C%20and%20consists%20of%0Acomplex%20multi-step%20questions.%20Extensive%20benchmarking%20shows%20that%20our%20dataset%0Aprovides%20a%20challenge%20for%20frontier%20open-source%20and%20proprietary%20models.%20We%0Aperform%20fine-grained%20error%20analysis%20to%20identify%20common%20failure%20modes%20across%0Avarious%20models%2C%20and%20create%20a%20taxonomy%20of%20reasoning%20errors.%20We%20use%20this%20to%0Aexplore%20both%20human%20and%20LLM-as-a-judge%20methods%20for%20scoring%20video%20reasoning%0Atraces%2C%20and%20find%20that%20failure%20modes%20are%20primarily%20related%20to%20temporal%0Alocalization%2C%20followed%20by%20visual%20perception%20errors%2C%20as%20opposed%20to%20logical%20or%0Acompleteness%20errors.%20The%20dataset%2C%20along%20with%20questions%2C%20answer%20candidates%20and%0Areasoning%20traces%20will%20be%20publicly%20available%20under%0Ahttps%3A//github.com/google-deepmind/neptune%3Ftab%3Dreadme-ov-file%5C%23minerva.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00681v1&entry.124074799=Read"},
{"title": "Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor\n  Analysis with Missing Modalities", "author": "Lucas Robinet and Ahmad Berjaoui and Elizabeth Cohen-Jonathan Moyal", "abstract": "  Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/bmmae\n", "link": "http://arxiv.org/abs/2505.00568v1", "date": "2025-05-01", "relevancy": 2.3354, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6241}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Masked%20Autoencoder%20Pre-training%20for%203D%20MRI-Based%20Brain%20Tumor%0A%20%20Analysis%20with%20Missing%20Modalities&body=Title%3A%20Multimodal%20Masked%20Autoencoder%20Pre-training%20for%203D%20MRI-Based%20Brain%20Tumor%0A%20%20Analysis%20with%20Missing%20Modalities%0AAuthor%3A%20Lucas%20Robinet%20and%20Ahmad%20Berjaoui%20and%20Elizabeth%20Cohen-Jonathan%20Moyal%0AAbstract%3A%20%20%20Multimodal%20magnetic%20resonance%20imaging%20%28MRI%29%20constitutes%20the%20first%20line%20of%0Ainvestigation%20for%20clinicians%20in%20the%20care%20of%20brain%20tumors%2C%20providing%20crucial%0Ainsights%20for%20surgery%20planning%2C%20treatment%20monitoring%2C%20and%20biomarker%0Aidentification.%20Pre-training%20on%20large%20datasets%20have%20been%20shown%20to%20help%20models%0Alearn%20transferable%20representations%20and%20adapt%20with%20minimal%20labeled%20data.%20This%0Abehavior%20is%20especially%20valuable%20in%20medical%20imaging%2C%20where%20annotations%20are%20often%0Ascarce.%20However%2C%20applying%20this%20paradigm%20to%20multimodal%20medical%20data%20introduces%20a%0Achallenge%3A%20most%20existing%20approaches%20assume%20that%20all%20imaging%20modalities%20are%0Aavailable%20during%20both%20pre-training%20and%20fine-tuning.%20In%20practice%2C%20missing%0Amodalities%20often%20occur%20due%20to%20acquisition%20issues%2C%20specialist%20unavailability%2C%20or%0Aspecific%20experimental%20designs%20on%20small%20in-house%20datasets.%20Consequently%2C%20a%0Acommon%20approach%20involves%20training%20a%20separate%20model%20for%20each%20desired%20modality%0Acombination%2C%20making%20the%20process%20both%20resource-intensive%20and%20impractical%20for%0Aclinical%20use.%20Therefore%2C%20we%20introduce%20BM-MAE%2C%20a%20masked%20image%20modeling%0Apre-training%20strategy%20tailored%20for%20multimodal%20MRI%20data.%20The%20same%20pre-trained%0Amodel%20seamlessly%20adapts%20to%20any%20combination%20of%20available%20modalities%2C%20extracting%0Arich%20representations%20that%20capture%20both%20intra-%20and%20inter-modal%20information.%20This%0Aallows%20fine-tuning%20on%20any%20subset%20of%20modalities%20without%20requiring%20architectural%0Achanges%2C%20while%20still%20benefiting%20from%20a%20model%20pre-trained%20on%20the%20full%20set%20of%0Amodalities.%20Extensive%20experiments%20show%20that%20the%20proposed%20pre-training%20strategy%0Aoutperforms%20or%20remains%20competitive%20with%20baselines%20that%20require%20separate%0Apre-training%20for%20each%20modality%20subset%2C%20while%20substantially%20surpassing%20training%0Afrom%20scratch%20on%20several%20downstream%20tasks.%20Additionally%2C%20it%20can%20quickly%20and%0Aefficiently%20reconstruct%20missing%20modalities%2C%20highlighting%20its%20practical%20value.%0ACode%20and%20trained%20models%20are%20available%20at%3A%20https%3A//github.com/Lucas-rbnt/bmmae%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Masked%2520Autoencoder%2520Pre-training%2520for%25203D%2520MRI-Based%2520Brain%2520Tumor%250A%2520%2520Analysis%2520with%2520Missing%2520Modalities%26entry.906535625%3DLucas%2520Robinet%2520and%2520Ahmad%2520Berjaoui%2520and%2520Elizabeth%2520Cohen-Jonathan%2520Moyal%26entry.1292438233%3D%2520%2520Multimodal%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520constitutes%2520the%2520first%2520line%2520of%250Ainvestigation%2520for%2520clinicians%2520in%2520the%2520care%2520of%2520brain%2520tumors%252C%2520providing%2520crucial%250Ainsights%2520for%2520surgery%2520planning%252C%2520treatment%2520monitoring%252C%2520and%2520biomarker%250Aidentification.%2520Pre-training%2520on%2520large%2520datasets%2520have%2520been%2520shown%2520to%2520help%2520models%250Alearn%2520transferable%2520representations%2520and%2520adapt%2520with%2520minimal%2520labeled%2520data.%2520This%250Abehavior%2520is%2520especially%2520valuable%2520in%2520medical%2520imaging%252C%2520where%2520annotations%2520are%2520often%250Ascarce.%2520However%252C%2520applying%2520this%2520paradigm%2520to%2520multimodal%2520medical%2520data%2520introduces%2520a%250Achallenge%253A%2520most%2520existing%2520approaches%2520assume%2520that%2520all%2520imaging%2520modalities%2520are%250Aavailable%2520during%2520both%2520pre-training%2520and%2520fine-tuning.%2520In%2520practice%252C%2520missing%250Amodalities%2520often%2520occur%2520due%2520to%2520acquisition%2520issues%252C%2520specialist%2520unavailability%252C%2520or%250Aspecific%2520experimental%2520designs%2520on%2520small%2520in-house%2520datasets.%2520Consequently%252C%2520a%250Acommon%2520approach%2520involves%2520training%2520a%2520separate%2520model%2520for%2520each%2520desired%2520modality%250Acombination%252C%2520making%2520the%2520process%2520both%2520resource-intensive%2520and%2520impractical%2520for%250Aclinical%2520use.%2520Therefore%252C%2520we%2520introduce%2520BM-MAE%252C%2520a%2520masked%2520image%2520modeling%250Apre-training%2520strategy%2520tailored%2520for%2520multimodal%2520MRI%2520data.%2520The%2520same%2520pre-trained%250Amodel%2520seamlessly%2520adapts%2520to%2520any%2520combination%2520of%2520available%2520modalities%252C%2520extracting%250Arich%2520representations%2520that%2520capture%2520both%2520intra-%2520and%2520inter-modal%2520information.%2520This%250Aallows%2520fine-tuning%2520on%2520any%2520subset%2520of%2520modalities%2520without%2520requiring%2520architectural%250Achanges%252C%2520while%2520still%2520benefiting%2520from%2520a%2520model%2520pre-trained%2520on%2520the%2520full%2520set%2520of%250Amodalities.%2520Extensive%2520experiments%2520show%2520that%2520the%2520proposed%2520pre-training%2520strategy%250Aoutperforms%2520or%2520remains%2520competitive%2520with%2520baselines%2520that%2520require%2520separate%250Apre-training%2520for%2520each%2520modality%2520subset%252C%2520while%2520substantially%2520surpassing%2520training%250Afrom%2520scratch%2520on%2520several%2520downstream%2520tasks.%2520Additionally%252C%2520it%2520can%2520quickly%2520and%250Aefficiently%2520reconstruct%2520missing%2520modalities%252C%2520highlighting%2520its%2520practical%2520value.%250ACode%2520and%2520trained%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/Lucas-rbnt/bmmae%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Masked%20Autoencoder%20Pre-training%20for%203D%20MRI-Based%20Brain%20Tumor%0A%20%20Analysis%20with%20Missing%20Modalities&entry.906535625=Lucas%20Robinet%20and%20Ahmad%20Berjaoui%20and%20Elizabeth%20Cohen-Jonathan%20Moyal&entry.1292438233=%20%20Multimodal%20magnetic%20resonance%20imaging%20%28MRI%29%20constitutes%20the%20first%20line%20of%0Ainvestigation%20for%20clinicians%20in%20the%20care%20of%20brain%20tumors%2C%20providing%20crucial%0Ainsights%20for%20surgery%20planning%2C%20treatment%20monitoring%2C%20and%20biomarker%0Aidentification.%20Pre-training%20on%20large%20datasets%20have%20been%20shown%20to%20help%20models%0Alearn%20transferable%20representations%20and%20adapt%20with%20minimal%20labeled%20data.%20This%0Abehavior%20is%20especially%20valuable%20in%20medical%20imaging%2C%20where%20annotations%20are%20often%0Ascarce.%20However%2C%20applying%20this%20paradigm%20to%20multimodal%20medical%20data%20introduces%20a%0Achallenge%3A%20most%20existing%20approaches%20assume%20that%20all%20imaging%20modalities%20are%0Aavailable%20during%20both%20pre-training%20and%20fine-tuning.%20In%20practice%2C%20missing%0Amodalities%20often%20occur%20due%20to%20acquisition%20issues%2C%20specialist%20unavailability%2C%20or%0Aspecific%20experimental%20designs%20on%20small%20in-house%20datasets.%20Consequently%2C%20a%0Acommon%20approach%20involves%20training%20a%20separate%20model%20for%20each%20desired%20modality%0Acombination%2C%20making%20the%20process%20both%20resource-intensive%20and%20impractical%20for%0Aclinical%20use.%20Therefore%2C%20we%20introduce%20BM-MAE%2C%20a%20masked%20image%20modeling%0Apre-training%20strategy%20tailored%20for%20multimodal%20MRI%20data.%20The%20same%20pre-trained%0Amodel%20seamlessly%20adapts%20to%20any%20combination%20of%20available%20modalities%2C%20extracting%0Arich%20representations%20that%20capture%20both%20intra-%20and%20inter-modal%20information.%20This%0Aallows%20fine-tuning%20on%20any%20subset%20of%20modalities%20without%20requiring%20architectural%0Achanges%2C%20while%20still%20benefiting%20from%20a%20model%20pre-trained%20on%20the%20full%20set%20of%0Amodalities.%20Extensive%20experiments%20show%20that%20the%20proposed%20pre-training%20strategy%0Aoutperforms%20or%20remains%20competitive%20with%20baselines%20that%20require%20separate%0Apre-training%20for%20each%20modality%20subset%2C%20while%20substantially%20surpassing%20training%0Afrom%20scratch%20on%20several%20downstream%20tasks.%20Additionally%2C%20it%20can%20quickly%20and%0Aefficiently%20reconstruct%20missing%20modalities%2C%20highlighting%20its%20practical%20value.%0ACode%20and%20trained%20models%20are%20available%20at%3A%20https%3A//github.com/Lucas-rbnt/bmmae%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00568v1&entry.124074799=Read"},
{"title": "Efficient IoT Intrusion Detection with an Improved Attention-Based\n  CNN-BiLSTM Architecture", "author": "Amna Naeem and Muazzam A. Khan and Nada Alasbali and Jawad Ahmad and Aizaz Ahmad Khattak and Muhammad Shahbaz Khan", "abstract": "  The ever-increasing security vulnerabilities in the Internet-of-Things (IoT)\nsystems require improved threat detection approaches. This paper presents a\ncompact and efficient approach to detect botnet attacks by employing an\nintegrated approach that consists of traffic pattern analysis, temporal support\nlearning, and focused feature extraction. The proposed attention-based model\nbenefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification\naccuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while\nmaintaining high precision and recall across various scenarios. The proposed\nmodel's performance is further validated by key parameters, such as Mathews\nCorrelation Coefficient and Cohen's kappa Correlation Coefficient. The\nclose-to-ideal results for these parameters demonstrate the proposed model's\nability to detect botnet attacks accurately and efficiently in practical\nsettings and on unseen data. The proposed model proved to be a powerful defence\nmechanism for IoT networks to face emerging security challenges.\n", "link": "http://arxiv.org/abs/2503.19339v3", "date": "2025-05-01", "relevancy": 2.3298, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4747}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4632}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20IoT%20Intrusion%20Detection%20with%20an%20Improved%20Attention-Based%0A%20%20CNN-BiLSTM%20Architecture&body=Title%3A%20Efficient%20IoT%20Intrusion%20Detection%20with%20an%20Improved%20Attention-Based%0A%20%20CNN-BiLSTM%20Architecture%0AAuthor%3A%20Amna%20Naeem%20and%20Muazzam%20A.%20Khan%20and%20Nada%20Alasbali%20and%20Jawad%20Ahmad%20and%20Aizaz%20Ahmad%20Khattak%20and%20Muhammad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20The%20ever-increasing%20security%20vulnerabilities%20in%20the%20Internet-of-Things%20%28IoT%29%0Asystems%20require%20improved%20threat%20detection%20approaches.%20This%20paper%20presents%20a%0Acompact%20and%20efficient%20approach%20to%20detect%20botnet%20attacks%20by%20employing%20an%0Aintegrated%20approach%20that%20consists%20of%20traffic%20pattern%20analysis%2C%20temporal%20support%0Alearning%2C%20and%20focused%20feature%20extraction.%20The%20proposed%20attention-based%20model%0Abenefits%20from%20a%20hybrid%20CNN-BiLSTM%20architecture%20and%20achieves%2099%25%20classification%0Aaccuracy%20in%20detecting%20botnet%20attacks%20utilizing%20the%20N-BaIoT%20dataset%2C%20while%0Amaintaining%20high%20precision%20and%20recall%20across%20various%20scenarios.%20The%20proposed%0Amodel%27s%20performance%20is%20further%20validated%20by%20key%20parameters%2C%20such%20as%20Mathews%0ACorrelation%20Coefficient%20and%20Cohen%27s%20kappa%20Correlation%20Coefficient.%20The%0Aclose-to-ideal%20results%20for%20these%20parameters%20demonstrate%20the%20proposed%20model%27s%0Aability%20to%20detect%20botnet%20attacks%20accurately%20and%20efficiently%20in%20practical%0Asettings%20and%20on%20unseen%20data.%20The%20proposed%20model%20proved%20to%20be%20a%20powerful%20defence%0Amechanism%20for%20IoT%20networks%20to%20face%20emerging%20security%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19339v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520IoT%2520Intrusion%2520Detection%2520with%2520an%2520Improved%2520Attention-Based%250A%2520%2520CNN-BiLSTM%2520Architecture%26entry.906535625%3DAmna%2520Naeem%2520and%2520Muazzam%2520A.%2520Khan%2520and%2520Nada%2520Alasbali%2520and%2520Jawad%2520Ahmad%2520and%2520Aizaz%2520Ahmad%2520Khattak%2520and%2520Muhammad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520The%2520ever-increasing%2520security%2520vulnerabilities%2520in%2520the%2520Internet-of-Things%2520%2528IoT%2529%250Asystems%2520require%2520improved%2520threat%2520detection%2520approaches.%2520This%2520paper%2520presents%2520a%250Acompact%2520and%2520efficient%2520approach%2520to%2520detect%2520botnet%2520attacks%2520by%2520employing%2520an%250Aintegrated%2520approach%2520that%2520consists%2520of%2520traffic%2520pattern%2520analysis%252C%2520temporal%2520support%250Alearning%252C%2520and%2520focused%2520feature%2520extraction.%2520The%2520proposed%2520attention-based%2520model%250Abenefits%2520from%2520a%2520hybrid%2520CNN-BiLSTM%2520architecture%2520and%2520achieves%252099%2525%2520classification%250Aaccuracy%2520in%2520detecting%2520botnet%2520attacks%2520utilizing%2520the%2520N-BaIoT%2520dataset%252C%2520while%250Amaintaining%2520high%2520precision%2520and%2520recall%2520across%2520various%2520scenarios.%2520The%2520proposed%250Amodel%2527s%2520performance%2520is%2520further%2520validated%2520by%2520key%2520parameters%252C%2520such%2520as%2520Mathews%250ACorrelation%2520Coefficient%2520and%2520Cohen%2527s%2520kappa%2520Correlation%2520Coefficient.%2520The%250Aclose-to-ideal%2520results%2520for%2520these%2520parameters%2520demonstrate%2520the%2520proposed%2520model%2527s%250Aability%2520to%2520detect%2520botnet%2520attacks%2520accurately%2520and%2520efficiently%2520in%2520practical%250Asettings%2520and%2520on%2520unseen%2520data.%2520The%2520proposed%2520model%2520proved%2520to%2520be%2520a%2520powerful%2520defence%250Amechanism%2520for%2520IoT%2520networks%2520to%2520face%2520emerging%2520security%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19339v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20IoT%20Intrusion%20Detection%20with%20an%20Improved%20Attention-Based%0A%20%20CNN-BiLSTM%20Architecture&entry.906535625=Amna%20Naeem%20and%20Muazzam%20A.%20Khan%20and%20Nada%20Alasbali%20and%20Jawad%20Ahmad%20and%20Aizaz%20Ahmad%20Khattak%20and%20Muhammad%20Shahbaz%20Khan&entry.1292438233=%20%20The%20ever-increasing%20security%20vulnerabilities%20in%20the%20Internet-of-Things%20%28IoT%29%0Asystems%20require%20improved%20threat%20detection%20approaches.%20This%20paper%20presents%20a%0Acompact%20and%20efficient%20approach%20to%20detect%20botnet%20attacks%20by%20employing%20an%0Aintegrated%20approach%20that%20consists%20of%20traffic%20pattern%20analysis%2C%20temporal%20support%0Alearning%2C%20and%20focused%20feature%20extraction.%20The%20proposed%20attention-based%20model%0Abenefits%20from%20a%20hybrid%20CNN-BiLSTM%20architecture%20and%20achieves%2099%25%20classification%0Aaccuracy%20in%20detecting%20botnet%20attacks%20utilizing%20the%20N-BaIoT%20dataset%2C%20while%0Amaintaining%20high%20precision%20and%20recall%20across%20various%20scenarios.%20The%20proposed%0Amodel%27s%20performance%20is%20further%20validated%20by%20key%20parameters%2C%20such%20as%20Mathews%0ACorrelation%20Coefficient%20and%20Cohen%27s%20kappa%20Correlation%20Coefficient.%20The%0Aclose-to-ideal%20results%20for%20these%20parameters%20demonstrate%20the%20proposed%20model%27s%0Aability%20to%20detect%20botnet%20attacks%20accurately%20and%20efficiently%20in%20practical%0Asettings%20and%20on%20unseen%20data.%20The%20proposed%20model%20proved%20to%20be%20a%20powerful%20defence%0Amechanism%20for%20IoT%20networks%20to%20face%20emerging%20security%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19339v3&entry.124074799=Read"},
{"title": "Commute Graph Neural Networks", "author": "Wei Zhuo and Han Yu and Guang Tan and Xiaoxiao Li", "abstract": "  Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such as\nasymmetrical shortest paths typically found in digraphs. Recognizing this gap,\nwe introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly\nintegrates node-wise commute time into the message passing scheme. The\ncornerstone of CGNN is an efficient method for computing commute time using a\nnewly formulated digraph Laplacian. Commute time is then integrated into the\nneighborhood aggregation process, with neighbor contributions weighted\naccording to their respective commute time to the central node in each layer.\nIt enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs. Extensive experiments on 8 benchmarking datasets confirm the\nsuperiority of CGNN against 13 state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2407.01635v5", "date": "2025-05-01", "relevancy": 2.3252, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4776}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4723}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commute%20Graph%20Neural%20Networks&body=Title%3A%20Commute%20Graph%20Neural%20Networks%0AAuthor%3A%20Wei%20Zhuo%20and%20Han%20Yu%20and%20Guang%20Tan%20and%20Xiaoxiao%20Li%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20success%20in%20learning%20from%0Agraph-structured%20data.%20However%2C%20their%20application%20to%20directed%20graphs%20%28digraphs%29%0Apresents%20unique%20challenges%2C%20primarily%20due%20to%20the%20inherent%20asymmetry%20in%20node%0Arelationships.%20Traditional%20GNNs%20are%20adept%20at%20capturing%20unidirectional%20relations%0Abut%20fall%20short%20in%20encoding%20the%20mutual%20path%20dependencies%20between%20nodes%2C%20such%20as%0Aasymmetrical%20shortest%20paths%20typically%20found%20in%20digraphs.%20Recognizing%20this%20gap%2C%0Awe%20introduce%20Commute%20Graph%20Neural%20Networks%20%28CGNN%29%2C%20an%20approach%20that%20seamlessly%0Aintegrates%20node-wise%20commute%20time%20into%20the%20message%20passing%20scheme.%20The%0Acornerstone%20of%20CGNN%20is%20an%20efficient%20method%20for%20computing%20commute%20time%20using%20a%0Anewly%20formulated%20digraph%20Laplacian.%20Commute%20time%20is%20then%20integrated%20into%20the%0Aneighborhood%20aggregation%20process%2C%20with%20neighbor%20contributions%20weighted%0Aaccording%20to%20their%20respective%20commute%20time%20to%20the%20central%20node%20in%20each%20layer.%0AIt%20enables%20CGNN%20to%20directly%20capture%20the%20mutual%2C%20asymmetric%20relationships%20in%0Adigraphs.%20Extensive%20experiments%20on%208%20benchmarking%20datasets%20confirm%20the%0Asuperiority%20of%20CGNN%20against%2013%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01635v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommute%2520Graph%2520Neural%2520Networks%26entry.906535625%3DWei%2520Zhuo%2520and%2520Han%2520Yu%2520and%2520Guang%2520Tan%2520and%2520Xiaoxiao%2520Li%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520remarkable%2520success%2520in%2520learning%2520from%250Agraph-structured%2520data.%2520However%252C%2520their%2520application%2520to%2520directed%2520graphs%2520%2528digraphs%2529%250Apresents%2520unique%2520challenges%252C%2520primarily%2520due%2520to%2520the%2520inherent%2520asymmetry%2520in%2520node%250Arelationships.%2520Traditional%2520GNNs%2520are%2520adept%2520at%2520capturing%2520unidirectional%2520relations%250Abut%2520fall%2520short%2520in%2520encoding%2520the%2520mutual%2520path%2520dependencies%2520between%2520nodes%252C%2520such%2520as%250Aasymmetrical%2520shortest%2520paths%2520typically%2520found%2520in%2520digraphs.%2520Recognizing%2520this%2520gap%252C%250Awe%2520introduce%2520Commute%2520Graph%2520Neural%2520Networks%2520%2528CGNN%2529%252C%2520an%2520approach%2520that%2520seamlessly%250Aintegrates%2520node-wise%2520commute%2520time%2520into%2520the%2520message%2520passing%2520scheme.%2520The%250Acornerstone%2520of%2520CGNN%2520is%2520an%2520efficient%2520method%2520for%2520computing%2520commute%2520time%2520using%2520a%250Anewly%2520formulated%2520digraph%2520Laplacian.%2520Commute%2520time%2520is%2520then%2520integrated%2520into%2520the%250Aneighborhood%2520aggregation%2520process%252C%2520with%2520neighbor%2520contributions%2520weighted%250Aaccording%2520to%2520their%2520respective%2520commute%2520time%2520to%2520the%2520central%2520node%2520in%2520each%2520layer.%250AIt%2520enables%2520CGNN%2520to%2520directly%2520capture%2520the%2520mutual%252C%2520asymmetric%2520relationships%2520in%250Adigraphs.%2520Extensive%2520experiments%2520on%25208%2520benchmarking%2520datasets%2520confirm%2520the%250Asuperiority%2520of%2520CGNN%2520against%252013%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01635v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commute%20Graph%20Neural%20Networks&entry.906535625=Wei%20Zhuo%20and%20Han%20Yu%20and%20Guang%20Tan%20and%20Xiaoxiao%20Li&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20success%20in%20learning%20from%0Agraph-structured%20data.%20However%2C%20their%20application%20to%20directed%20graphs%20%28digraphs%29%0Apresents%20unique%20challenges%2C%20primarily%20due%20to%20the%20inherent%20asymmetry%20in%20node%0Arelationships.%20Traditional%20GNNs%20are%20adept%20at%20capturing%20unidirectional%20relations%0Abut%20fall%20short%20in%20encoding%20the%20mutual%20path%20dependencies%20between%20nodes%2C%20such%20as%0Aasymmetrical%20shortest%20paths%20typically%20found%20in%20digraphs.%20Recognizing%20this%20gap%2C%0Awe%20introduce%20Commute%20Graph%20Neural%20Networks%20%28CGNN%29%2C%20an%20approach%20that%20seamlessly%0Aintegrates%20node-wise%20commute%20time%20into%20the%20message%20passing%20scheme.%20The%0Acornerstone%20of%20CGNN%20is%20an%20efficient%20method%20for%20computing%20commute%20time%20using%20a%0Anewly%20formulated%20digraph%20Laplacian.%20Commute%20time%20is%20then%20integrated%20into%20the%0Aneighborhood%20aggregation%20process%2C%20with%20neighbor%20contributions%20weighted%0Aaccording%20to%20their%20respective%20commute%20time%20to%20the%20central%20node%20in%20each%20layer.%0AIt%20enables%20CGNN%20to%20directly%20capture%20the%20mutual%2C%20asymmetric%20relationships%20in%0Adigraphs.%20Extensive%20experiments%20on%208%20benchmarking%20datasets%20confirm%20the%0Asuperiority%20of%20CGNN%20against%2013%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01635v5&entry.124074799=Read"},
{"title": "InterLoc: LiDAR-based Intersection Localization using Road Segmentation\n  with Automated Evaluation Method", "author": "Nguyen Hoang Khoi Tran and Julie Stephany Berrio and Mao Shan and Zhenxing Ming and Stewart Worrall", "abstract": "  Intersections are geometric and functional key points in every road network.\nThey offer strong landmarks to correct GNSS dropouts and anchor new sensor data\nin up-to-date maps. Despite that importance, intersection detectors either\nignore the rich semantic information already computed onboard or depend on\nscarce, hand-labeled intersection datasets. To close that gap, this paper\npresents a LiDAR-based method for intersection detection that (i) fuses\nsemantic road segmentation with vehicle localization to detect intersection\ncandidates in a bird's eye view (BEV) representation and (ii) refines those\ncandidates by analyzing branch topology with a least squares formulation. To\nevaluate our method, we introduce an automated benchmarking pipeline that pairs\ndetections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS\nground-truth poses. Tested on eight SemanticKITTI sequences, the approach\nachieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a\n5 m tolerance, outperforming the latest learning-based baseline. Moreover, the\nmethod is robust to segmentation errors higher than those of the benchmark\nmodel, demonstrating its applicability in the real world.\n", "link": "http://arxiv.org/abs/2505.00512v1", "date": "2025-05-01", "relevancy": 2.3029, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5963}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5704}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterLoc%3A%20LiDAR-based%20Intersection%20Localization%20using%20Road%20Segmentation%0A%20%20with%20Automated%20Evaluation%20Method&body=Title%3A%20InterLoc%3A%20LiDAR-based%20Intersection%20Localization%20using%20Road%20Segmentation%0A%20%20with%20Automated%20Evaluation%20Method%0AAuthor%3A%20Nguyen%20Hoang%20Khoi%20Tran%20and%20Julie%20Stephany%20Berrio%20and%20Mao%20Shan%20and%20Zhenxing%20Ming%20and%20Stewart%20Worrall%0AAbstract%3A%20%20%20Intersections%20are%20geometric%20and%20functional%20key%20points%20in%20every%20road%20network.%0AThey%20offer%20strong%20landmarks%20to%20correct%20GNSS%20dropouts%20and%20anchor%20new%20sensor%20data%0Ain%20up-to-date%20maps.%20Despite%20that%20importance%2C%20intersection%20detectors%20either%0Aignore%20the%20rich%20semantic%20information%20already%20computed%20onboard%20or%20depend%20on%0Ascarce%2C%20hand-labeled%20intersection%20datasets.%20To%20close%20that%20gap%2C%20this%20paper%0Apresents%20a%20LiDAR-based%20method%20for%20intersection%20detection%20that%20%28i%29%20fuses%0Asemantic%20road%20segmentation%20with%20vehicle%20localization%20to%20detect%20intersection%0Acandidates%20in%20a%20bird%27s%20eye%20view%20%28BEV%29%20representation%20and%20%28ii%29%20refines%20those%0Acandidates%20by%20analyzing%20branch%20topology%20with%20a%20least%20squares%20formulation.%20To%0Aevaluate%20our%20method%2C%20we%20introduce%20an%20automated%20benchmarking%20pipeline%20that%20pairs%0Adetections%20with%20OpenStreetMap%20%28OSM%29%20intersection%20nodes%20using%20precise%20GNSS/INS%0Aground-truth%20poses.%20Tested%20on%20eight%20SemanticKITTI%20sequences%2C%20the%20approach%0Aachieves%20a%20mean%20localization%20error%20of%201.9%20m%2C%2089%25%20precision%2C%20and%2077%25%20recall%20at%20a%0A5%20m%20tolerance%2C%20outperforming%20the%20latest%20learning-based%20baseline.%20Moreover%2C%20the%0Amethod%20is%20robust%20to%20segmentation%20errors%20higher%20than%20those%20of%20the%20benchmark%0Amodel%2C%20demonstrating%20its%20applicability%20in%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterLoc%253A%2520LiDAR-based%2520Intersection%2520Localization%2520using%2520Road%2520Segmentation%250A%2520%2520with%2520Automated%2520Evaluation%2520Method%26entry.906535625%3DNguyen%2520Hoang%2520Khoi%2520Tran%2520and%2520Julie%2520Stephany%2520Berrio%2520and%2520Mao%2520Shan%2520and%2520Zhenxing%2520Ming%2520and%2520Stewart%2520Worrall%26entry.1292438233%3D%2520%2520Intersections%2520are%2520geometric%2520and%2520functional%2520key%2520points%2520in%2520every%2520road%2520network.%250AThey%2520offer%2520strong%2520landmarks%2520to%2520correct%2520GNSS%2520dropouts%2520and%2520anchor%2520new%2520sensor%2520data%250Ain%2520up-to-date%2520maps.%2520Despite%2520that%2520importance%252C%2520intersection%2520detectors%2520either%250Aignore%2520the%2520rich%2520semantic%2520information%2520already%2520computed%2520onboard%2520or%2520depend%2520on%250Ascarce%252C%2520hand-labeled%2520intersection%2520datasets.%2520To%2520close%2520that%2520gap%252C%2520this%2520paper%250Apresents%2520a%2520LiDAR-based%2520method%2520for%2520intersection%2520detection%2520that%2520%2528i%2529%2520fuses%250Asemantic%2520road%2520segmentation%2520with%2520vehicle%2520localization%2520to%2520detect%2520intersection%250Acandidates%2520in%2520a%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529%2520representation%2520and%2520%2528ii%2529%2520refines%2520those%250Acandidates%2520by%2520analyzing%2520branch%2520topology%2520with%2520a%2520least%2520squares%2520formulation.%2520To%250Aevaluate%2520our%2520method%252C%2520we%2520introduce%2520an%2520automated%2520benchmarking%2520pipeline%2520that%2520pairs%250Adetections%2520with%2520OpenStreetMap%2520%2528OSM%2529%2520intersection%2520nodes%2520using%2520precise%2520GNSS/INS%250Aground-truth%2520poses.%2520Tested%2520on%2520eight%2520SemanticKITTI%2520sequences%252C%2520the%2520approach%250Aachieves%2520a%2520mean%2520localization%2520error%2520of%25201.9%2520m%252C%252089%2525%2520precision%252C%2520and%252077%2525%2520recall%2520at%2520a%250A5%2520m%2520tolerance%252C%2520outperforming%2520the%2520latest%2520learning-based%2520baseline.%2520Moreover%252C%2520the%250Amethod%2520is%2520robust%2520to%2520segmentation%2520errors%2520higher%2520than%2520those%2520of%2520the%2520benchmark%250Amodel%252C%2520demonstrating%2520its%2520applicability%2520in%2520the%2520real%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterLoc%3A%20LiDAR-based%20Intersection%20Localization%20using%20Road%20Segmentation%0A%20%20with%20Automated%20Evaluation%20Method&entry.906535625=Nguyen%20Hoang%20Khoi%20Tran%20and%20Julie%20Stephany%20Berrio%20and%20Mao%20Shan%20and%20Zhenxing%20Ming%20and%20Stewart%20Worrall&entry.1292438233=%20%20Intersections%20are%20geometric%20and%20functional%20key%20points%20in%20every%20road%20network.%0AThey%20offer%20strong%20landmarks%20to%20correct%20GNSS%20dropouts%20and%20anchor%20new%20sensor%20data%0Ain%20up-to-date%20maps.%20Despite%20that%20importance%2C%20intersection%20detectors%20either%0Aignore%20the%20rich%20semantic%20information%20already%20computed%20onboard%20or%20depend%20on%0Ascarce%2C%20hand-labeled%20intersection%20datasets.%20To%20close%20that%20gap%2C%20this%20paper%0Apresents%20a%20LiDAR-based%20method%20for%20intersection%20detection%20that%20%28i%29%20fuses%0Asemantic%20road%20segmentation%20with%20vehicle%20localization%20to%20detect%20intersection%0Acandidates%20in%20a%20bird%27s%20eye%20view%20%28BEV%29%20representation%20and%20%28ii%29%20refines%20those%0Acandidates%20by%20analyzing%20branch%20topology%20with%20a%20least%20squares%20formulation.%20To%0Aevaluate%20our%20method%2C%20we%20introduce%20an%20automated%20benchmarking%20pipeline%20that%20pairs%0Adetections%20with%20OpenStreetMap%20%28OSM%29%20intersection%20nodes%20using%20precise%20GNSS/INS%0Aground-truth%20poses.%20Tested%20on%20eight%20SemanticKITTI%20sequences%2C%20the%20approach%0Aachieves%20a%20mean%20localization%20error%20of%201.9%20m%2C%2089%25%20precision%2C%20and%2077%25%20recall%20at%20a%0A5%20m%20tolerance%2C%20outperforming%20the%20latest%20learning-based%20baseline.%20Moreover%2C%20the%0Amethod%20is%20robust%20to%20segmentation%20errors%20higher%20than%20those%20of%20the%20benchmark%0Amodel%2C%20demonstrating%20its%20applicability%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00512v1&entry.124074799=Read"},
{"title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic", "author": "Muhammad Imran Zaman and Usama Ijaz Bajwa and Gulshan Saleem and Rana Hammad Raza", "abstract": "  Vision sensors are becoming more important in Intelligent Transportation\nSystems (ITS) for traffic monitoring, management, and optimization as the\nnumber of network cameras continues to rise. However, manual object tracking\nand matching across multiple non-overlapping cameras pose significant\nchallenges in city-scale urban traffic scenarios. These challenges include\nhandling diverse vehicle attributes, occlusions, illumination variations,\nshadows, and varying video resolutions. To address these issues, we propose an\nefficient and cost-effective deep learning-based framework for Multi-Object\nMulti-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for\nobject detection and employs Non-Maximum Suppression (NMS) to select target\nobjects from overlapping detections. Transfer learning is employed for\nre-identification, enabling the association and generation of vehicle tracklets\nacross multiple cameras. Moreover, we leverage appropriate loss functions and\ndistance measures to handle occlusion, illumination, and shadow challenges. The\nfinal solution identification module performs feature extraction using\nResNet-152 coupled with Deep SORT based vehicle tracking. The proposed\nframework is evaluated on the 5th AI City Challenge dataset (Track 3),\ncomprising 46 camera feeds. Among these 46 camera streams, 40 are used for\nmodel training and validation, while the remaining six are utilized for model\ntesting. The proposed framework achieves competitive performance with an IDF1\nscore of 0.8289, and precision and recall scores of 0.9026 and 0.8527\nrespectively, demonstrating its effectiveness in robust and accurate vehicle\ntracking.\n", "link": "http://arxiv.org/abs/2505.00534v1", "date": "2025-05-01", "relevancy": 2.2981, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5813}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20Deep%20Networks%20based%20Multi-Object%20MultiCamera%20Tracking%20System%0A%20%20for%20City%20Scale%20Traffic&body=Title%3A%20A%20Robust%20Deep%20Networks%20based%20Multi-Object%20MultiCamera%20Tracking%20System%0A%20%20for%20City%20Scale%20Traffic%0AAuthor%3A%20Muhammad%20Imran%20Zaman%20and%20Usama%20Ijaz%20Bajwa%20and%20Gulshan%20Saleem%20and%20Rana%20Hammad%20Raza%0AAbstract%3A%20%20%20Vision%20sensors%20are%20becoming%20more%20important%20in%20Intelligent%20Transportation%0ASystems%20%28ITS%29%20for%20traffic%20monitoring%2C%20management%2C%20and%20optimization%20as%20the%0Anumber%20of%20network%20cameras%20continues%20to%20rise.%20However%2C%20manual%20object%20tracking%0Aand%20matching%20across%20multiple%20non-overlapping%20cameras%20pose%20significant%0Achallenges%20in%20city-scale%20urban%20traffic%20scenarios.%20These%20challenges%20include%0Ahandling%20diverse%20vehicle%20attributes%2C%20occlusions%2C%20illumination%20variations%2C%0Ashadows%2C%20and%20varying%20video%20resolutions.%20To%20address%20these%20issues%2C%20we%20propose%20an%0Aefficient%20and%20cost-effective%20deep%20learning-based%20framework%20for%20Multi-Object%0AMulti-Camera%20Tracking%20%28MO-MCT%29.%20The%20proposed%20framework%20utilizes%20Mask%20R-CNN%20for%0Aobject%20detection%20and%20employs%20Non-Maximum%20Suppression%20%28NMS%29%20to%20select%20target%0Aobjects%20from%20overlapping%20detections.%20Transfer%20learning%20is%20employed%20for%0Are-identification%2C%20enabling%20the%20association%20and%20generation%20of%20vehicle%20tracklets%0Aacross%20multiple%20cameras.%20Moreover%2C%20we%20leverage%20appropriate%20loss%20functions%20and%0Adistance%20measures%20to%20handle%20occlusion%2C%20illumination%2C%20and%20shadow%20challenges.%20The%0Afinal%20solution%20identification%20module%20performs%20feature%20extraction%20using%0AResNet-152%20coupled%20with%20Deep%20SORT%20based%20vehicle%20tracking.%20The%20proposed%0Aframework%20is%20evaluated%20on%20the%205th%20AI%20City%20Challenge%20dataset%20%28Track%203%29%2C%0Acomprising%2046%20camera%20feeds.%20Among%20these%2046%20camera%20streams%2C%2040%20are%20used%20for%0Amodel%20training%20and%20validation%2C%20while%20the%20remaining%20six%20are%20utilized%20for%20model%0Atesting.%20The%20proposed%20framework%20achieves%20competitive%20performance%20with%20an%20IDF1%0Ascore%20of%200.8289%2C%20and%20precision%20and%20recall%20scores%20of%200.9026%20and%200.8527%0Arespectively%2C%20demonstrating%20its%20effectiveness%20in%20robust%20and%20accurate%20vehicle%0Atracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520Deep%2520Networks%2520based%2520Multi-Object%2520MultiCamera%2520Tracking%2520System%250A%2520%2520for%2520City%2520Scale%2520Traffic%26entry.906535625%3DMuhammad%2520Imran%2520Zaman%2520and%2520Usama%2520Ijaz%2520Bajwa%2520and%2520Gulshan%2520Saleem%2520and%2520Rana%2520Hammad%2520Raza%26entry.1292438233%3D%2520%2520Vision%2520sensors%2520are%2520becoming%2520more%2520important%2520in%2520Intelligent%2520Transportation%250ASystems%2520%2528ITS%2529%2520for%2520traffic%2520monitoring%252C%2520management%252C%2520and%2520optimization%2520as%2520the%250Anumber%2520of%2520network%2520cameras%2520continues%2520to%2520rise.%2520However%252C%2520manual%2520object%2520tracking%250Aand%2520matching%2520across%2520multiple%2520non-overlapping%2520cameras%2520pose%2520significant%250Achallenges%2520in%2520city-scale%2520urban%2520traffic%2520scenarios.%2520These%2520challenges%2520include%250Ahandling%2520diverse%2520vehicle%2520attributes%252C%2520occlusions%252C%2520illumination%2520variations%252C%250Ashadows%252C%2520and%2520varying%2520video%2520resolutions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%250Aefficient%2520and%2520cost-effective%2520deep%2520learning-based%2520framework%2520for%2520Multi-Object%250AMulti-Camera%2520Tracking%2520%2528MO-MCT%2529.%2520The%2520proposed%2520framework%2520utilizes%2520Mask%2520R-CNN%2520for%250Aobject%2520detection%2520and%2520employs%2520Non-Maximum%2520Suppression%2520%2528NMS%2529%2520to%2520select%2520target%250Aobjects%2520from%2520overlapping%2520detections.%2520Transfer%2520learning%2520is%2520employed%2520for%250Are-identification%252C%2520enabling%2520the%2520association%2520and%2520generation%2520of%2520vehicle%2520tracklets%250Aacross%2520multiple%2520cameras.%2520Moreover%252C%2520we%2520leverage%2520appropriate%2520loss%2520functions%2520and%250Adistance%2520measures%2520to%2520handle%2520occlusion%252C%2520illumination%252C%2520and%2520shadow%2520challenges.%2520The%250Afinal%2520solution%2520identification%2520module%2520performs%2520feature%2520extraction%2520using%250AResNet-152%2520coupled%2520with%2520Deep%2520SORT%2520based%2520vehicle%2520tracking.%2520The%2520proposed%250Aframework%2520is%2520evaluated%2520on%2520the%25205th%2520AI%2520City%2520Challenge%2520dataset%2520%2528Track%25203%2529%252C%250Acomprising%252046%2520camera%2520feeds.%2520Among%2520these%252046%2520camera%2520streams%252C%252040%2520are%2520used%2520for%250Amodel%2520training%2520and%2520validation%252C%2520while%2520the%2520remaining%2520six%2520are%2520utilized%2520for%2520model%250Atesting.%2520The%2520proposed%2520framework%2520achieves%2520competitive%2520performance%2520with%2520an%2520IDF1%250Ascore%2520of%25200.8289%252C%2520and%2520precision%2520and%2520recall%2520scores%2520of%25200.9026%2520and%25200.8527%250Arespectively%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520robust%2520and%2520accurate%2520vehicle%250Atracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20Deep%20Networks%20based%20Multi-Object%20MultiCamera%20Tracking%20System%0A%20%20for%20City%20Scale%20Traffic&entry.906535625=Muhammad%20Imran%20Zaman%20and%20Usama%20Ijaz%20Bajwa%20and%20Gulshan%20Saleem%20and%20Rana%20Hammad%20Raza&entry.1292438233=%20%20Vision%20sensors%20are%20becoming%20more%20important%20in%20Intelligent%20Transportation%0ASystems%20%28ITS%29%20for%20traffic%20monitoring%2C%20management%2C%20and%20optimization%20as%20the%0Anumber%20of%20network%20cameras%20continues%20to%20rise.%20However%2C%20manual%20object%20tracking%0Aand%20matching%20across%20multiple%20non-overlapping%20cameras%20pose%20significant%0Achallenges%20in%20city-scale%20urban%20traffic%20scenarios.%20These%20challenges%20include%0Ahandling%20diverse%20vehicle%20attributes%2C%20occlusions%2C%20illumination%20variations%2C%0Ashadows%2C%20and%20varying%20video%20resolutions.%20To%20address%20these%20issues%2C%20we%20propose%20an%0Aefficient%20and%20cost-effective%20deep%20learning-based%20framework%20for%20Multi-Object%0AMulti-Camera%20Tracking%20%28MO-MCT%29.%20The%20proposed%20framework%20utilizes%20Mask%20R-CNN%20for%0Aobject%20detection%20and%20employs%20Non-Maximum%20Suppression%20%28NMS%29%20to%20select%20target%0Aobjects%20from%20overlapping%20detections.%20Transfer%20learning%20is%20employed%20for%0Are-identification%2C%20enabling%20the%20association%20and%20generation%20of%20vehicle%20tracklets%0Aacross%20multiple%20cameras.%20Moreover%2C%20we%20leverage%20appropriate%20loss%20functions%20and%0Adistance%20measures%20to%20handle%20occlusion%2C%20illumination%2C%20and%20shadow%20challenges.%20The%0Afinal%20solution%20identification%20module%20performs%20feature%20extraction%20using%0AResNet-152%20coupled%20with%20Deep%20SORT%20based%20vehicle%20tracking.%20The%20proposed%0Aframework%20is%20evaluated%20on%20the%205th%20AI%20City%20Challenge%20dataset%20%28Track%203%29%2C%0Acomprising%2046%20camera%20feeds.%20Among%20these%2046%20camera%20streams%2C%2040%20are%20used%20for%0Amodel%20training%20and%20validation%2C%20while%20the%20remaining%20six%20are%20utilized%20for%20model%0Atesting.%20The%20proposed%20framework%20achieves%20competitive%20performance%20with%20an%20IDF1%0Ascore%20of%200.8289%2C%20and%20precision%20and%20recall%20scores%20of%200.9026%20and%200.8527%0Arespectively%2C%20demonstrating%20its%20effectiveness%20in%20robust%20and%20accurate%20vehicle%0Atracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00534v1&entry.124074799=Read"},
{"title": "Artificial Scientific Discovery", "author": "Antonio Norelli", "abstract": "  Rooted in the explosion of deep learning over the past decade, this thesis\nspans from AlphaGo to ChatGPT to empirically examine the fundamental concepts\nneeded to realize the vision of an artificial scientist: a machine with the\ncapacity to autonomously generate original research and contribute to the\nexpansion of human knowledge. The investigation begins with Olivaw, an AlphaGo\nZero-like agent that discovers Othello knowledge from scratch but is unable to\ncommunicate it. This realization leads to the development of the Explanatory\nLearning (EL) framework, a formalization of the problem faced by a scientist\nwhen trying to explain a new phenomenon to their peers. The effective EL\nprescriptions allow us to crack Zendo, a popular board game simulating the\nscientific endeavor. This success comes with a fundamental insight: an\nartificial scientist must develop its own interpretation of the language used\nto explain its findings, and not rely on a rigid existing interpreter.\nQuestioning the very process of learning an interpreter, we turn our attention\nto the inner functioning of modern multimodal models. This culminates in a\nsimple idea to build CLIP-like models where interpretation and perception are\nexplicitly disentangled: a cost-effective approach that couples two unimodal\nmodels using little multimodal data and no further training. Finally, we\ndiscuss what ChatGPT and its siblings are still missing to become artificial\nscientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark\nabout interpreting Zendo-like explanations that sees LLMs going no further than\nrandom chance while being instead fully solved by humans.\n", "link": "http://arxiv.org/abs/2411.11672v2", "date": "2025-05-01", "relevancy": 2.2713, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5776}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Scientific%20Discovery&body=Title%3A%20Artificial%20Scientific%20Discovery%0AAuthor%3A%20Antonio%20Norelli%0AAbstract%3A%20%20%20Rooted%20in%20the%20explosion%20of%20deep%20learning%20over%20the%20past%20decade%2C%20this%20thesis%0Aspans%20from%20AlphaGo%20to%20ChatGPT%20to%20empirically%20examine%20the%20fundamental%20concepts%0Aneeded%20to%20realize%20the%20vision%20of%20an%20artificial%20scientist%3A%20a%20machine%20with%20the%0Acapacity%20to%20autonomously%20generate%20original%20research%20and%20contribute%20to%20the%0Aexpansion%20of%20human%20knowledge.%20The%20investigation%20begins%20with%20Olivaw%2C%20an%20AlphaGo%0AZero-like%20agent%20that%20discovers%20Othello%20knowledge%20from%20scratch%20but%20is%20unable%20to%0Acommunicate%20it.%20This%20realization%20leads%20to%20the%20development%20of%20the%20Explanatory%0ALearning%20%28EL%29%20framework%2C%20a%20formalization%20of%20the%20problem%20faced%20by%20a%20scientist%0Awhen%20trying%20to%20explain%20a%20new%20phenomenon%20to%20their%20peers.%20The%20effective%20EL%0Aprescriptions%20allow%20us%20to%20crack%20Zendo%2C%20a%20popular%20board%20game%20simulating%20the%0Ascientific%20endeavor.%20This%20success%20comes%20with%20a%20fundamental%20insight%3A%20an%0Aartificial%20scientist%20must%20develop%20its%20own%20interpretation%20of%20the%20language%20used%0Ato%20explain%20its%20findings%2C%20and%20not%20rely%20on%20a%20rigid%20existing%20interpreter.%0AQuestioning%20the%20very%20process%20of%20learning%20an%20interpreter%2C%20we%20turn%20our%20attention%0Ato%20the%20inner%20functioning%20of%20modern%20multimodal%20models.%20This%20culminates%20in%20a%0Asimple%20idea%20to%20build%20CLIP-like%20models%20where%20interpretation%20and%20perception%20are%0Aexplicitly%20disentangled%3A%20a%20cost-effective%20approach%20that%20couples%20two%20unimodal%0Amodels%20using%20little%20multimodal%20data%20and%20no%20further%20training.%20Finally%2C%20we%0Adiscuss%20what%20ChatGPT%20and%20its%20siblings%20are%20still%20missing%20to%20become%20artificial%0Ascientists%2C%20and%20introduce%20the%20Big-Bench%20Symbol%20Interpretation%20Task%2C%20a%20benchmark%0Aabout%20interpreting%20Zendo-like%20explanations%20that%20sees%20LLMs%20going%20no%20further%20than%0Arandom%20chance%20while%20being%20instead%20fully%20solved%20by%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Scientific%2520Discovery%26entry.906535625%3DAntonio%2520Norelli%26entry.1292438233%3D%2520%2520Rooted%2520in%2520the%2520explosion%2520of%2520deep%2520learning%2520over%2520the%2520past%2520decade%252C%2520this%2520thesis%250Aspans%2520from%2520AlphaGo%2520to%2520ChatGPT%2520to%2520empirically%2520examine%2520the%2520fundamental%2520concepts%250Aneeded%2520to%2520realize%2520the%2520vision%2520of%2520an%2520artificial%2520scientist%253A%2520a%2520machine%2520with%2520the%250Acapacity%2520to%2520autonomously%2520generate%2520original%2520research%2520and%2520contribute%2520to%2520the%250Aexpansion%2520of%2520human%2520knowledge.%2520The%2520investigation%2520begins%2520with%2520Olivaw%252C%2520an%2520AlphaGo%250AZero-like%2520agent%2520that%2520discovers%2520Othello%2520knowledge%2520from%2520scratch%2520but%2520is%2520unable%2520to%250Acommunicate%2520it.%2520This%2520realization%2520leads%2520to%2520the%2520development%2520of%2520the%2520Explanatory%250ALearning%2520%2528EL%2529%2520framework%252C%2520a%2520formalization%2520of%2520the%2520problem%2520faced%2520by%2520a%2520scientist%250Awhen%2520trying%2520to%2520explain%2520a%2520new%2520phenomenon%2520to%2520their%2520peers.%2520The%2520effective%2520EL%250Aprescriptions%2520allow%2520us%2520to%2520crack%2520Zendo%252C%2520a%2520popular%2520board%2520game%2520simulating%2520the%250Ascientific%2520endeavor.%2520This%2520success%2520comes%2520with%2520a%2520fundamental%2520insight%253A%2520an%250Aartificial%2520scientist%2520must%2520develop%2520its%2520own%2520interpretation%2520of%2520the%2520language%2520used%250Ato%2520explain%2520its%2520findings%252C%2520and%2520not%2520rely%2520on%2520a%2520rigid%2520existing%2520interpreter.%250AQuestioning%2520the%2520very%2520process%2520of%2520learning%2520an%2520interpreter%252C%2520we%2520turn%2520our%2520attention%250Ato%2520the%2520inner%2520functioning%2520of%2520modern%2520multimodal%2520models.%2520This%2520culminates%2520in%2520a%250Asimple%2520idea%2520to%2520build%2520CLIP-like%2520models%2520where%2520interpretation%2520and%2520perception%2520are%250Aexplicitly%2520disentangled%253A%2520a%2520cost-effective%2520approach%2520that%2520couples%2520two%2520unimodal%250Amodels%2520using%2520little%2520multimodal%2520data%2520and%2520no%2520further%2520training.%2520Finally%252C%2520we%250Adiscuss%2520what%2520ChatGPT%2520and%2520its%2520siblings%2520are%2520still%2520missing%2520to%2520become%2520artificial%250Ascientists%252C%2520and%2520introduce%2520the%2520Big-Bench%2520Symbol%2520Interpretation%2520Task%252C%2520a%2520benchmark%250Aabout%2520interpreting%2520Zendo-like%2520explanations%2520that%2520sees%2520LLMs%2520going%2520no%2520further%2520than%250Arandom%2520chance%2520while%2520being%2520instead%2520fully%2520solved%2520by%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Scientific%20Discovery&entry.906535625=Antonio%20Norelli&entry.1292438233=%20%20Rooted%20in%20the%20explosion%20of%20deep%20learning%20over%20the%20past%20decade%2C%20this%20thesis%0Aspans%20from%20AlphaGo%20to%20ChatGPT%20to%20empirically%20examine%20the%20fundamental%20concepts%0Aneeded%20to%20realize%20the%20vision%20of%20an%20artificial%20scientist%3A%20a%20machine%20with%20the%0Acapacity%20to%20autonomously%20generate%20original%20research%20and%20contribute%20to%20the%0Aexpansion%20of%20human%20knowledge.%20The%20investigation%20begins%20with%20Olivaw%2C%20an%20AlphaGo%0AZero-like%20agent%20that%20discovers%20Othello%20knowledge%20from%20scratch%20but%20is%20unable%20to%0Acommunicate%20it.%20This%20realization%20leads%20to%20the%20development%20of%20the%20Explanatory%0ALearning%20%28EL%29%20framework%2C%20a%20formalization%20of%20the%20problem%20faced%20by%20a%20scientist%0Awhen%20trying%20to%20explain%20a%20new%20phenomenon%20to%20their%20peers.%20The%20effective%20EL%0Aprescriptions%20allow%20us%20to%20crack%20Zendo%2C%20a%20popular%20board%20game%20simulating%20the%0Ascientific%20endeavor.%20This%20success%20comes%20with%20a%20fundamental%20insight%3A%20an%0Aartificial%20scientist%20must%20develop%20its%20own%20interpretation%20of%20the%20language%20used%0Ato%20explain%20its%20findings%2C%20and%20not%20rely%20on%20a%20rigid%20existing%20interpreter.%0AQuestioning%20the%20very%20process%20of%20learning%20an%20interpreter%2C%20we%20turn%20our%20attention%0Ato%20the%20inner%20functioning%20of%20modern%20multimodal%20models.%20This%20culminates%20in%20a%0Asimple%20idea%20to%20build%20CLIP-like%20models%20where%20interpretation%20and%20perception%20are%0Aexplicitly%20disentangled%3A%20a%20cost-effective%20approach%20that%20couples%20two%20unimodal%0Amodels%20using%20little%20multimodal%20data%20and%20no%20further%20training.%20Finally%2C%20we%0Adiscuss%20what%20ChatGPT%20and%20its%20siblings%20are%20still%20missing%20to%20become%20artificial%0Ascientists%2C%20and%20introduce%20the%20Big-Bench%20Symbol%20Interpretation%20Task%2C%20a%20benchmark%0Aabout%20interpreting%20Zendo-like%20explanations%20that%20sees%20LLMs%20going%20no%20further%20than%0Arandom%20chance%20while%20being%20instead%20fully%20solved%20by%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11672v2&entry.124074799=Read"},
{"title": "CORSTITCH - A free, open source software for stitching and\n  georeferencing underwater coral reef videos", "author": "Julian Christopher L. Maya and Johnenn R. Manalang and Maricor N. Soriano", "abstract": "  CorStitch is an open-source software developed to automate the creation of\naccurate georeferenced reef mosaics from video transects obtained through\nAutomated Rapid Reef Assessment System surveys. We utilized a Fourier-based\nimage correlation algorithm to stitch sequential video frames, aligning them\nwith synchronized GNSS timestamps. The resulting compressed Keyhole Markup\nLanguage files, compatible with geographic information systems such as Google\nEarth, enable detailed spatial analysis. Validation through comparative\nanalysis of mosaics from two temporally distinct surveys of the same reef\ndemonstrated the software's consistent and reliable performance.\n", "link": "http://arxiv.org/abs/2505.00462v1", "date": "2025-05-01", "relevancy": 2.257, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4528}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4528}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CORSTITCH%20-%20A%20free%2C%20open%20source%20software%20for%20stitching%20and%0A%20%20georeferencing%20underwater%20coral%20reef%20videos&body=Title%3A%20CORSTITCH%20-%20A%20free%2C%20open%20source%20software%20for%20stitching%20and%0A%20%20georeferencing%20underwater%20coral%20reef%20videos%0AAuthor%3A%20Julian%20Christopher%20L.%20Maya%20and%20Johnenn%20R.%20Manalang%20and%20Maricor%20N.%20Soriano%0AAbstract%3A%20%20%20CorStitch%20is%20an%20open-source%20software%20developed%20to%20automate%20the%20creation%20of%0Aaccurate%20georeferenced%20reef%20mosaics%20from%20video%20transects%20obtained%20through%0AAutomated%20Rapid%20Reef%20Assessment%20System%20surveys.%20We%20utilized%20a%20Fourier-based%0Aimage%20correlation%20algorithm%20to%20stitch%20sequential%20video%20frames%2C%20aligning%20them%0Awith%20synchronized%20GNSS%20timestamps.%20The%20resulting%20compressed%20Keyhole%20Markup%0ALanguage%20files%2C%20compatible%20with%20geographic%20information%20systems%20such%20as%20Google%0AEarth%2C%20enable%20detailed%20spatial%20analysis.%20Validation%20through%20comparative%0Aanalysis%20of%20mosaics%20from%20two%20temporally%20distinct%20surveys%20of%20the%20same%20reef%0Ademonstrated%20the%20software%27s%20consistent%20and%20reliable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCORSTITCH%2520-%2520A%2520free%252C%2520open%2520source%2520software%2520for%2520stitching%2520and%250A%2520%2520georeferencing%2520underwater%2520coral%2520reef%2520videos%26entry.906535625%3DJulian%2520Christopher%2520L.%2520Maya%2520and%2520Johnenn%2520R.%2520Manalang%2520and%2520Maricor%2520N.%2520Soriano%26entry.1292438233%3D%2520%2520CorStitch%2520is%2520an%2520open-source%2520software%2520developed%2520to%2520automate%2520the%2520creation%2520of%250Aaccurate%2520georeferenced%2520reef%2520mosaics%2520from%2520video%2520transects%2520obtained%2520through%250AAutomated%2520Rapid%2520Reef%2520Assessment%2520System%2520surveys.%2520We%2520utilized%2520a%2520Fourier-based%250Aimage%2520correlation%2520algorithm%2520to%2520stitch%2520sequential%2520video%2520frames%252C%2520aligning%2520them%250Awith%2520synchronized%2520GNSS%2520timestamps.%2520The%2520resulting%2520compressed%2520Keyhole%2520Markup%250ALanguage%2520files%252C%2520compatible%2520with%2520geographic%2520information%2520systems%2520such%2520as%2520Google%250AEarth%252C%2520enable%2520detailed%2520spatial%2520analysis.%2520Validation%2520through%2520comparative%250Aanalysis%2520of%2520mosaics%2520from%2520two%2520temporally%2520distinct%2520surveys%2520of%2520the%2520same%2520reef%250Ademonstrated%2520the%2520software%2527s%2520consistent%2520and%2520reliable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CORSTITCH%20-%20A%20free%2C%20open%20source%20software%20for%20stitching%20and%0A%20%20georeferencing%20underwater%20coral%20reef%20videos&entry.906535625=Julian%20Christopher%20L.%20Maya%20and%20Johnenn%20R.%20Manalang%20and%20Maricor%20N.%20Soriano&entry.1292438233=%20%20CorStitch%20is%20an%20open-source%20software%20developed%20to%20automate%20the%20creation%20of%0Aaccurate%20georeferenced%20reef%20mosaics%20from%20video%20transects%20obtained%20through%0AAutomated%20Rapid%20Reef%20Assessment%20System%20surveys.%20We%20utilized%20a%20Fourier-based%0Aimage%20correlation%20algorithm%20to%20stitch%20sequential%20video%20frames%2C%20aligning%20them%0Awith%20synchronized%20GNSS%20timestamps.%20The%20resulting%20compressed%20Keyhole%20Markup%0ALanguage%20files%2C%20compatible%20with%20geographic%20information%20systems%20such%20as%20Google%0AEarth%2C%20enable%20detailed%20spatial%20analysis.%20Validation%20through%20comparative%0Aanalysis%20of%20mosaics%20from%20two%20temporally%20distinct%20surveys%20of%20the%20same%20reef%0Ademonstrated%20the%20software%27s%20consistent%20and%20reliable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00462v1&entry.124074799=Read"},
{"title": "DeCo: Task Decomposition and Skill Composition for Zero-Shot\n  Generalization in Long-Horizon 3D Manipulation", "author": "Zixuan Chen and Junhui Yin and Yangtao Chen and Jing Huo and Pinzhuo Tian and Jieqi Shi and Yiwen Hou and Yinchuan Li and Yang Gao", "abstract": "  Generalizing language-conditioned multi-task imitation learning (IL) models\nto novel long-horizon 3D manipulation tasks remains a significant challenge. To\naddress this, we propose DeCo (Task Decomposition and Skill Composition), a\nmodel-agnostic framework compatible with various multi-task IL models, designed\nto enhance their zero-shot generalization to novel, compositional, long-horizon\n3D manipulation tasks. DeCo first decomposes IL demonstrations into a set of\nmodular atomic tasks based on the physical interaction between the gripper and\nobjects, and constructs an atomic training dataset that enables models to learn\na diverse set of reusable atomic skills during imitation learning. At inference\ntime, DeCo leverages a vision-language model (VLM) to parse high-level\ninstructions for novel long-horizon tasks, retrieve the relevant atomic skills,\nand dynamically schedule their execution; a spatially-aware skill-chaining\nmodule then ensures smooth, collision-free transitions between sequential\nskills. We evaluate DeCo in simulation using DeCoBench, a benchmark\nspecifically designed to assess zero-shot generalization of multi-task IL\nmodels in compositional long-horizon 3D manipulation. Across three\nrepresentative multi-task IL models (RVT-2, 3DDA, and ARP), DeCo achieves\nsuccess rate improvements of 66.67%, 21.53%, and 57.92%, respectively, on 12\nnovel compositional tasks. Moreover, in real-world experiments, a DeCo-enhanced\nmodel trained on only 6 atomic tasks successfully completes 9 novel\nlong-horizon tasks, yielding an average success rate improvement of 53.33% over\nthe base multi-task IL model. Video demonstrations are available at:\nhttps://deco226.github.io.\n", "link": "http://arxiv.org/abs/2505.00527v1", "date": "2025-05-01", "relevancy": 2.2556, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCo%3A%20Task%20Decomposition%20and%20Skill%20Composition%20for%20Zero-Shot%0A%20%20Generalization%20in%20Long-Horizon%203D%20Manipulation&body=Title%3A%20DeCo%3A%20Task%20Decomposition%20and%20Skill%20Composition%20for%20Zero-Shot%0A%20%20Generalization%20in%20Long-Horizon%203D%20Manipulation%0AAuthor%3A%20Zixuan%20Chen%20and%20Junhui%20Yin%20and%20Yangtao%20Chen%20and%20Jing%20Huo%20and%20Pinzhuo%20Tian%20and%20Jieqi%20Shi%20and%20Yiwen%20Hou%20and%20Yinchuan%20Li%20and%20Yang%20Gao%0AAbstract%3A%20%20%20Generalizing%20language-conditioned%20multi-task%20imitation%20learning%20%28IL%29%20models%0Ato%20novel%20long-horizon%203D%20manipulation%20tasks%20remains%20a%20significant%20challenge.%20To%0Aaddress%20this%2C%20we%20propose%20DeCo%20%28Task%20Decomposition%20and%20Skill%20Composition%29%2C%20a%0Amodel-agnostic%20framework%20compatible%20with%20various%20multi-task%20IL%20models%2C%20designed%0Ato%20enhance%20their%20zero-shot%20generalization%20to%20novel%2C%20compositional%2C%20long-horizon%0A3D%20manipulation%20tasks.%20DeCo%20first%20decomposes%20IL%20demonstrations%20into%20a%20set%20of%0Amodular%20atomic%20tasks%20based%20on%20the%20physical%20interaction%20between%20the%20gripper%20and%0Aobjects%2C%20and%20constructs%20an%20atomic%20training%20dataset%20that%20enables%20models%20to%20learn%0Aa%20diverse%20set%20of%20reusable%20atomic%20skills%20during%20imitation%20learning.%20At%20inference%0Atime%2C%20DeCo%20leverages%20a%20vision-language%20model%20%28VLM%29%20to%20parse%20high-level%0Ainstructions%20for%20novel%20long-horizon%20tasks%2C%20retrieve%20the%20relevant%20atomic%20skills%2C%0Aand%20dynamically%20schedule%20their%20execution%3B%20a%20spatially-aware%20skill-chaining%0Amodule%20then%20ensures%20smooth%2C%20collision-free%20transitions%20between%20sequential%0Askills.%20We%20evaluate%20DeCo%20in%20simulation%20using%20DeCoBench%2C%20a%20benchmark%0Aspecifically%20designed%20to%20assess%20zero-shot%20generalization%20of%20multi-task%20IL%0Amodels%20in%20compositional%20long-horizon%203D%20manipulation.%20Across%20three%0Arepresentative%20multi-task%20IL%20models%20%28RVT-2%2C%203DDA%2C%20and%20ARP%29%2C%20DeCo%20achieves%0Asuccess%20rate%20improvements%20of%2066.67%25%2C%2021.53%25%2C%20and%2057.92%25%2C%20respectively%2C%20on%2012%0Anovel%20compositional%20tasks.%20Moreover%2C%20in%20real-world%20experiments%2C%20a%20DeCo-enhanced%0Amodel%20trained%20on%20only%206%20atomic%20tasks%20successfully%20completes%209%20novel%0Along-horizon%20tasks%2C%20yielding%20an%20average%20success%20rate%20improvement%20of%2053.33%25%20over%0Athe%20base%20multi-task%20IL%20model.%20Video%20demonstrations%20are%20available%20at%3A%0Ahttps%3A//deco226.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCo%253A%2520Task%2520Decomposition%2520and%2520Skill%2520Composition%2520for%2520Zero-Shot%250A%2520%2520Generalization%2520in%2520Long-Horizon%25203D%2520Manipulation%26entry.906535625%3DZixuan%2520Chen%2520and%2520Junhui%2520Yin%2520and%2520Yangtao%2520Chen%2520and%2520Jing%2520Huo%2520and%2520Pinzhuo%2520Tian%2520and%2520Jieqi%2520Shi%2520and%2520Yiwen%2520Hou%2520and%2520Yinchuan%2520Li%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520Generalizing%2520language-conditioned%2520multi-task%2520imitation%2520learning%2520%2528IL%2529%2520models%250Ato%2520novel%2520long-horizon%25203D%2520manipulation%2520tasks%2520remains%2520a%2520significant%2520challenge.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520DeCo%2520%2528Task%2520Decomposition%2520and%2520Skill%2520Composition%2529%252C%2520a%250Amodel-agnostic%2520framework%2520compatible%2520with%2520various%2520multi-task%2520IL%2520models%252C%2520designed%250Ato%2520enhance%2520their%2520zero-shot%2520generalization%2520to%2520novel%252C%2520compositional%252C%2520long-horizon%250A3D%2520manipulation%2520tasks.%2520DeCo%2520first%2520decomposes%2520IL%2520demonstrations%2520into%2520a%2520set%2520of%250Amodular%2520atomic%2520tasks%2520based%2520on%2520the%2520physical%2520interaction%2520between%2520the%2520gripper%2520and%250Aobjects%252C%2520and%2520constructs%2520an%2520atomic%2520training%2520dataset%2520that%2520enables%2520models%2520to%2520learn%250Aa%2520diverse%2520set%2520of%2520reusable%2520atomic%2520skills%2520during%2520imitation%2520learning.%2520At%2520inference%250Atime%252C%2520DeCo%2520leverages%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520parse%2520high-level%250Ainstructions%2520for%2520novel%2520long-horizon%2520tasks%252C%2520retrieve%2520the%2520relevant%2520atomic%2520skills%252C%250Aand%2520dynamically%2520schedule%2520their%2520execution%253B%2520a%2520spatially-aware%2520skill-chaining%250Amodule%2520then%2520ensures%2520smooth%252C%2520collision-free%2520transitions%2520between%2520sequential%250Askills.%2520We%2520evaluate%2520DeCo%2520in%2520simulation%2520using%2520DeCoBench%252C%2520a%2520benchmark%250Aspecifically%2520designed%2520to%2520assess%2520zero-shot%2520generalization%2520of%2520multi-task%2520IL%250Amodels%2520in%2520compositional%2520long-horizon%25203D%2520manipulation.%2520Across%2520three%250Arepresentative%2520multi-task%2520IL%2520models%2520%2528RVT-2%252C%25203DDA%252C%2520and%2520ARP%2529%252C%2520DeCo%2520achieves%250Asuccess%2520rate%2520improvements%2520of%252066.67%2525%252C%252021.53%2525%252C%2520and%252057.92%2525%252C%2520respectively%252C%2520on%252012%250Anovel%2520compositional%2520tasks.%2520Moreover%252C%2520in%2520real-world%2520experiments%252C%2520a%2520DeCo-enhanced%250Amodel%2520trained%2520on%2520only%25206%2520atomic%2520tasks%2520successfully%2520completes%25209%2520novel%250Along-horizon%2520tasks%252C%2520yielding%2520an%2520average%2520success%2520rate%2520improvement%2520of%252053.33%2525%2520over%250Athe%2520base%2520multi-task%2520IL%2520model.%2520Video%2520demonstrations%2520are%2520available%2520at%253A%250Ahttps%253A//deco226.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCo%3A%20Task%20Decomposition%20and%20Skill%20Composition%20for%20Zero-Shot%0A%20%20Generalization%20in%20Long-Horizon%203D%20Manipulation&entry.906535625=Zixuan%20Chen%20and%20Junhui%20Yin%20and%20Yangtao%20Chen%20and%20Jing%20Huo%20and%20Pinzhuo%20Tian%20and%20Jieqi%20Shi%20and%20Yiwen%20Hou%20and%20Yinchuan%20Li%20and%20Yang%20Gao&entry.1292438233=%20%20Generalizing%20language-conditioned%20multi-task%20imitation%20learning%20%28IL%29%20models%0Ato%20novel%20long-horizon%203D%20manipulation%20tasks%20remains%20a%20significant%20challenge.%20To%0Aaddress%20this%2C%20we%20propose%20DeCo%20%28Task%20Decomposition%20and%20Skill%20Composition%29%2C%20a%0Amodel-agnostic%20framework%20compatible%20with%20various%20multi-task%20IL%20models%2C%20designed%0Ato%20enhance%20their%20zero-shot%20generalization%20to%20novel%2C%20compositional%2C%20long-horizon%0A3D%20manipulation%20tasks.%20DeCo%20first%20decomposes%20IL%20demonstrations%20into%20a%20set%20of%0Amodular%20atomic%20tasks%20based%20on%20the%20physical%20interaction%20between%20the%20gripper%20and%0Aobjects%2C%20and%20constructs%20an%20atomic%20training%20dataset%20that%20enables%20models%20to%20learn%0Aa%20diverse%20set%20of%20reusable%20atomic%20skills%20during%20imitation%20learning.%20At%20inference%0Atime%2C%20DeCo%20leverages%20a%20vision-language%20model%20%28VLM%29%20to%20parse%20high-level%0Ainstructions%20for%20novel%20long-horizon%20tasks%2C%20retrieve%20the%20relevant%20atomic%20skills%2C%0Aand%20dynamically%20schedule%20their%20execution%3B%20a%20spatially-aware%20skill-chaining%0Amodule%20then%20ensures%20smooth%2C%20collision-free%20transitions%20between%20sequential%0Askills.%20We%20evaluate%20DeCo%20in%20simulation%20using%20DeCoBench%2C%20a%20benchmark%0Aspecifically%20designed%20to%20assess%20zero-shot%20generalization%20of%20multi-task%20IL%0Amodels%20in%20compositional%20long-horizon%203D%20manipulation.%20Across%20three%0Arepresentative%20multi-task%20IL%20models%20%28RVT-2%2C%203DDA%2C%20and%20ARP%29%2C%20DeCo%20achieves%0Asuccess%20rate%20improvements%20of%2066.67%25%2C%2021.53%25%2C%20and%2057.92%25%2C%20respectively%2C%20on%2012%0Anovel%20compositional%20tasks.%20Moreover%2C%20in%20real-world%20experiments%2C%20a%20DeCo-enhanced%0Amodel%20trained%20on%20only%206%20atomic%20tasks%20successfully%20completes%209%20novel%0Along-horizon%20tasks%2C%20yielding%20an%20average%20success%20rate%20improvement%20of%2053.33%25%20over%0Athe%20base%20multi-task%20IL%20model.%20Video%20demonstrations%20are%20available%20at%3A%0Ahttps%3A//deco226.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00527v1&entry.124074799=Read"},
{"title": "Voice Cloning: Comprehensive Survey", "author": "Hussam Azzuni and Abdulmotaleb El Saddik", "abstract": "  Voice Cloning has rapidly advanced in today's digital world, with many\nresearchers and corporations working to improve these algorithms for various\napplications. This article aims to establish a standardized terminology for\nvoice cloning and explore its different variations. It will cover speaker\nadaptation as the fundamental concept and then delve deeper into topics such as\nfew-shot, zero-shot, and multilingual TTS within that context. Finally, we will\nexplore the evaluation metrics commonly used in voice cloning research and\nrelated datasets. This survey compiles the available voice cloning algorithms\nto encourage research toward its generation and detection to limit its misuse.\n", "link": "http://arxiv.org/abs/2505.00579v1", "date": "2025-05-01", "relevancy": 2.2529, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voice%20Cloning%3A%20Comprehensive%20Survey&body=Title%3A%20Voice%20Cloning%3A%20Comprehensive%20Survey%0AAuthor%3A%20Hussam%20Azzuni%20and%20Abdulmotaleb%20El%20Saddik%0AAbstract%3A%20%20%20Voice%20Cloning%20has%20rapidly%20advanced%20in%20today%27s%20digital%20world%2C%20with%20many%0Aresearchers%20and%20corporations%20working%20to%20improve%20these%20algorithms%20for%20various%0Aapplications.%20This%20article%20aims%20to%20establish%20a%20standardized%20terminology%20for%0Avoice%20cloning%20and%20explore%20its%20different%20variations.%20It%20will%20cover%20speaker%0Aadaptation%20as%20the%20fundamental%20concept%20and%20then%20delve%20deeper%20into%20topics%20such%20as%0Afew-shot%2C%20zero-shot%2C%20and%20multilingual%20TTS%20within%20that%20context.%20Finally%2C%20we%20will%0Aexplore%20the%20evaluation%20metrics%20commonly%20used%20in%20voice%20cloning%20research%20and%0Arelated%20datasets.%20This%20survey%20compiles%20the%20available%20voice%20cloning%20algorithms%0Ato%20encourage%20research%20toward%20its%20generation%20and%20detection%20to%20limit%20its%20misuse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoice%2520Cloning%253A%2520Comprehensive%2520Survey%26entry.906535625%3DHussam%2520Azzuni%2520and%2520Abdulmotaleb%2520El%2520Saddik%26entry.1292438233%3D%2520%2520Voice%2520Cloning%2520has%2520rapidly%2520advanced%2520in%2520today%2527s%2520digital%2520world%252C%2520with%2520many%250Aresearchers%2520and%2520corporations%2520working%2520to%2520improve%2520these%2520algorithms%2520for%2520various%250Aapplications.%2520This%2520article%2520aims%2520to%2520establish%2520a%2520standardized%2520terminology%2520for%250Avoice%2520cloning%2520and%2520explore%2520its%2520different%2520variations.%2520It%2520will%2520cover%2520speaker%250Aadaptation%2520as%2520the%2520fundamental%2520concept%2520and%2520then%2520delve%2520deeper%2520into%2520topics%2520such%2520as%250Afew-shot%252C%2520zero-shot%252C%2520and%2520multilingual%2520TTS%2520within%2520that%2520context.%2520Finally%252C%2520we%2520will%250Aexplore%2520the%2520evaluation%2520metrics%2520commonly%2520used%2520in%2520voice%2520cloning%2520research%2520and%250Arelated%2520datasets.%2520This%2520survey%2520compiles%2520the%2520available%2520voice%2520cloning%2520algorithms%250Ato%2520encourage%2520research%2520toward%2520its%2520generation%2520and%2520detection%2520to%2520limit%2520its%2520misuse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voice%20Cloning%3A%20Comprehensive%20Survey&entry.906535625=Hussam%20Azzuni%20and%20Abdulmotaleb%20El%20Saddik&entry.1292438233=%20%20Voice%20Cloning%20has%20rapidly%20advanced%20in%20today%27s%20digital%20world%2C%20with%20many%0Aresearchers%20and%20corporations%20working%20to%20improve%20these%20algorithms%20for%20various%0Aapplications.%20This%20article%20aims%20to%20establish%20a%20standardized%20terminology%20for%0Avoice%20cloning%20and%20explore%20its%20different%20variations.%20It%20will%20cover%20speaker%0Aadaptation%20as%20the%20fundamental%20concept%20and%20then%20delve%20deeper%20into%20topics%20such%20as%0Afew-shot%2C%20zero-shot%2C%20and%20multilingual%20TTS%20within%20that%20context.%20Finally%2C%20we%20will%0Aexplore%20the%20evaluation%20metrics%20commonly%20used%20in%20voice%20cloning%20research%20and%0Arelated%20datasets.%20This%20survey%20compiles%20the%20available%20voice%20cloning%20algorithms%0Ato%20encourage%20research%20toward%20its%20generation%20and%20detection%20to%20limit%20its%20misuse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00579v1&entry.124074799=Read"},
{"title": "Rule-based Classifier Models", "author": "Cecilia Di Florio and Huimin Dong and Antonino Rotolo", "abstract": "  We extend the formal framework of classifier models used in the legal domain.\nWhile the existing classifier framework characterises cases solely through the\nfacts involved, legal reasoning fundamentally relies on both facts and rules,\nparticularly the ratio decidendi. This paper presents an initial approach to\nincorporating sets of rules within a classifier. Our work is built on the work\nof Canavotto et al. (2023), which has developed the rule-based reason model of\nprecedential constraint within a hierarchy of factors. We demonstrate how\ndecisions for new cases can be inferred using this enriched rule-based\nclassifier framework. Additionally, we provide an example of how the time\nelement and the hierarchy of courts can be used in the new classifier\nframework.\n", "link": "http://arxiv.org/abs/2505.00474v1", "date": "2025-05-01", "relevancy": 2.2457, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rule-based%20Classifier%20Models&body=Title%3A%20Rule-based%20Classifier%20Models%0AAuthor%3A%20Cecilia%20Di%20Florio%20and%20Huimin%20Dong%20and%20Antonino%20Rotolo%0AAbstract%3A%20%20%20We%20extend%20the%20formal%20framework%20of%20classifier%20models%20used%20in%20the%20legal%20domain.%0AWhile%20the%20existing%20classifier%20framework%20characterises%20cases%20solely%20through%20the%0Afacts%20involved%2C%20legal%20reasoning%20fundamentally%20relies%20on%20both%20facts%20and%20rules%2C%0Aparticularly%20the%20ratio%20decidendi.%20This%20paper%20presents%20an%20initial%20approach%20to%0Aincorporating%20sets%20of%20rules%20within%20a%20classifier.%20Our%20work%20is%20built%20on%20the%20work%0Aof%20Canavotto%20et%20al.%20%282023%29%2C%20which%20has%20developed%20the%20rule-based%20reason%20model%20of%0Aprecedential%20constraint%20within%20a%20hierarchy%20of%20factors.%20We%20demonstrate%20how%0Adecisions%20for%20new%20cases%20can%20be%20inferred%20using%20this%20enriched%20rule-based%0Aclassifier%20framework.%20Additionally%2C%20we%20provide%20an%20example%20of%20how%20the%20time%0Aelement%20and%20the%20hierarchy%20of%20courts%20can%20be%20used%20in%20the%20new%20classifier%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRule-based%2520Classifier%2520Models%26entry.906535625%3DCecilia%2520Di%2520Florio%2520and%2520Huimin%2520Dong%2520and%2520Antonino%2520Rotolo%26entry.1292438233%3D%2520%2520We%2520extend%2520the%2520formal%2520framework%2520of%2520classifier%2520models%2520used%2520in%2520the%2520legal%2520domain.%250AWhile%2520the%2520existing%2520classifier%2520framework%2520characterises%2520cases%2520solely%2520through%2520the%250Afacts%2520involved%252C%2520legal%2520reasoning%2520fundamentally%2520relies%2520on%2520both%2520facts%2520and%2520rules%252C%250Aparticularly%2520the%2520ratio%2520decidendi.%2520This%2520paper%2520presents%2520an%2520initial%2520approach%2520to%250Aincorporating%2520sets%2520of%2520rules%2520within%2520a%2520classifier.%2520Our%2520work%2520is%2520built%2520on%2520the%2520work%250Aof%2520Canavotto%2520et%2520al.%2520%25282023%2529%252C%2520which%2520has%2520developed%2520the%2520rule-based%2520reason%2520model%2520of%250Aprecedential%2520constraint%2520within%2520a%2520hierarchy%2520of%2520factors.%2520We%2520demonstrate%2520how%250Adecisions%2520for%2520new%2520cases%2520can%2520be%2520inferred%2520using%2520this%2520enriched%2520rule-based%250Aclassifier%2520framework.%2520Additionally%252C%2520we%2520provide%2520an%2520example%2520of%2520how%2520the%2520time%250Aelement%2520and%2520the%2520hierarchy%2520of%2520courts%2520can%2520be%2520used%2520in%2520the%2520new%2520classifier%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rule-based%20Classifier%20Models&entry.906535625=Cecilia%20Di%20Florio%20and%20Huimin%20Dong%20and%20Antonino%20Rotolo&entry.1292438233=%20%20We%20extend%20the%20formal%20framework%20of%20classifier%20models%20used%20in%20the%20legal%20domain.%0AWhile%20the%20existing%20classifier%20framework%20characterises%20cases%20solely%20through%20the%0Afacts%20involved%2C%20legal%20reasoning%20fundamentally%20relies%20on%20both%20facts%20and%20rules%2C%0Aparticularly%20the%20ratio%20decidendi.%20This%20paper%20presents%20an%20initial%20approach%20to%0Aincorporating%20sets%20of%20rules%20within%20a%20classifier.%20Our%20work%20is%20built%20on%20the%20work%0Aof%20Canavotto%20et%20al.%20%282023%29%2C%20which%20has%20developed%20the%20rule-based%20reason%20model%20of%0Aprecedential%20constraint%20within%20a%20hierarchy%20of%20factors.%20We%20demonstrate%20how%0Adecisions%20for%20new%20cases%20can%20be%20inferred%20using%20this%20enriched%20rule-based%0Aclassifier%20framework.%20Additionally%2C%20we%20provide%20an%20example%20of%20how%20the%20time%0Aelement%20and%20the%20hierarchy%20of%20courts%20can%20be%20used%20in%20the%20new%20classifier%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00474v1&entry.124074799=Read"},
{"title": "Inconsistency-based Active Learning for LiDAR Object Detection", "author": "Esteban Rivera and Loic Stratil and Markus Lienkamp", "abstract": "  Deep learning models for object detection in autonomous driving have recently\nachieved impressive performance gains and are already being deployed in\nvehicles worldwide. However, current models require increasingly large datasets\nfor training. Acquiring and labeling such data is costly, necessitating the\ndevelopment of new strategies to optimize this process. Active learning is a\npromising approach that has been extensively researched in the image domain. In\nour work, we extend this concept to the LiDAR domain by developing several\ninconsistency-based sample selection strategies and evaluate their\neffectiveness in various settings. Our results show that using a naive\ninconsistency approach based on the number of detected boxes, we achieve the\nsame mAP as the random sampling strategy with 50% of the labeled data.\n", "link": "http://arxiv.org/abs/2505.00511v1", "date": "2025-05-01", "relevancy": 2.2325, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5573}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inconsistency-based%20Active%20Learning%20for%20LiDAR%20Object%20Detection&body=Title%3A%20Inconsistency-based%20Active%20Learning%20for%20LiDAR%20Object%20Detection%0AAuthor%3A%20Esteban%20Rivera%20and%20Loic%20Stratil%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20Deep%20learning%20models%20for%20object%20detection%20in%20autonomous%20driving%20have%20recently%0Aachieved%20impressive%20performance%20gains%20and%20are%20already%20being%20deployed%20in%0Avehicles%20worldwide.%20However%2C%20current%20models%20require%20increasingly%20large%20datasets%0Afor%20training.%20Acquiring%20and%20labeling%20such%20data%20is%20costly%2C%20necessitating%20the%0Adevelopment%20of%20new%20strategies%20to%20optimize%20this%20process.%20Active%20learning%20is%20a%0Apromising%20approach%20that%20has%20been%20extensively%20researched%20in%20the%20image%20domain.%20In%0Aour%20work%2C%20we%20extend%20this%20concept%20to%20the%20LiDAR%20domain%20by%20developing%20several%0Ainconsistency-based%20sample%20selection%20strategies%20and%20evaluate%20their%0Aeffectiveness%20in%20various%20settings.%20Our%20results%20show%20that%20using%20a%20naive%0Ainconsistency%20approach%20based%20on%20the%20number%20of%20detected%20boxes%2C%20we%20achieve%20the%0Asame%20mAP%20as%20the%20random%20sampling%20strategy%20with%2050%25%20of%20the%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInconsistency-based%2520Active%2520Learning%2520for%2520LiDAR%2520Object%2520Detection%26entry.906535625%3DEsteban%2520Rivera%2520and%2520Loic%2520Stratil%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520for%2520object%2520detection%2520in%2520autonomous%2520driving%2520have%2520recently%250Aachieved%2520impressive%2520performance%2520gains%2520and%2520are%2520already%2520being%2520deployed%2520in%250Avehicles%2520worldwide.%2520However%252C%2520current%2520models%2520require%2520increasingly%2520large%2520datasets%250Afor%2520training.%2520Acquiring%2520and%2520labeling%2520such%2520data%2520is%2520costly%252C%2520necessitating%2520the%250Adevelopment%2520of%2520new%2520strategies%2520to%2520optimize%2520this%2520process.%2520Active%2520learning%2520is%2520a%250Apromising%2520approach%2520that%2520has%2520been%2520extensively%2520researched%2520in%2520the%2520image%2520domain.%2520In%250Aour%2520work%252C%2520we%2520extend%2520this%2520concept%2520to%2520the%2520LiDAR%2520domain%2520by%2520developing%2520several%250Ainconsistency-based%2520sample%2520selection%2520strategies%2520and%2520evaluate%2520their%250Aeffectiveness%2520in%2520various%2520settings.%2520Our%2520results%2520show%2520that%2520using%2520a%2520naive%250Ainconsistency%2520approach%2520based%2520on%2520the%2520number%2520of%2520detected%2520boxes%252C%2520we%2520achieve%2520the%250Asame%2520mAP%2520as%2520the%2520random%2520sampling%2520strategy%2520with%252050%2525%2520of%2520the%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inconsistency-based%20Active%20Learning%20for%20LiDAR%20Object%20Detection&entry.906535625=Esteban%20Rivera%20and%20Loic%20Stratil%20and%20Markus%20Lienkamp&entry.1292438233=%20%20Deep%20learning%20models%20for%20object%20detection%20in%20autonomous%20driving%20have%20recently%0Aachieved%20impressive%20performance%20gains%20and%20are%20already%20being%20deployed%20in%0Avehicles%20worldwide.%20However%2C%20current%20models%20require%20increasingly%20large%20datasets%0Afor%20training.%20Acquiring%20and%20labeling%20such%20data%20is%20costly%2C%20necessitating%20the%0Adevelopment%20of%20new%20strategies%20to%20optimize%20this%20process.%20Active%20learning%20is%20a%0Apromising%20approach%20that%20has%20been%20extensively%20researched%20in%20the%20image%20domain.%20In%0Aour%20work%2C%20we%20extend%20this%20concept%20to%20the%20LiDAR%20domain%20by%20developing%20several%0Ainconsistency-based%20sample%20selection%20strategies%20and%20evaluate%20their%0Aeffectiveness%20in%20various%20settings.%20Our%20results%20show%20that%20using%20a%20naive%0Ainconsistency%20approach%20based%20on%20the%20number%20of%20detected%20boxes%2C%20we%20achieve%20the%0Asame%20mAP%20as%20the%20random%20sampling%20strategy%20with%2050%25%20of%20the%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00511v1&entry.124074799=Read"},
{"title": "Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly", "author": "Ruiyuan Zhang and Qi Wang and Jiaxiang Liu and Yu Zhang and Yuchi Huo and Chao Wu", "abstract": "  3D part assembly aims to understand part relationships and predict their\n6-DoF poses to construct realistic 3D shapes, addressing the growing demand for\nautonomous assembly, which is crucial for robots. Existing methods mainly\nestimate the transformation of each part by training neural networks under\nsupervision, which requires a substantial quantity of manually labeled data.\nHowever, the high cost of data collection and the immense variability of\nreal-world shapes and parts make traditional methods impractical for\nlarge-scale applications. In this paper, we propose first a zero-shot part\nassembly method that utilizes pre-trained point cloud diffusion models as\ndiscriminators in the assembly process, guiding the manipulation of parts to\nform realistic shapes. Specifically, we theoretically demonstrate that\nutilizing a diffusion model for zero-shot part assembly can be transformed into\nan Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away\nstrategy to address the overlap parts, thereby further enhancing the robustness\nof the method. To verify our work, we conduct extensive experiments and\nquantitative comparisons to several strong baseline methods, demonstrating the\neffectiveness of the proposed approach, which even surpasses the supervised\nlearning method. The code has been released on\nhttps://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.\n", "link": "http://arxiv.org/abs/2505.00426v1", "date": "2025-05-01", "relevancy": 2.2304, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5676}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5505}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Pretrained%20Diffusion%20Models%20for%20Zero-Shot%20Part%20Assembly&body=Title%3A%20Leveraging%20Pretrained%20Diffusion%20Models%20for%20Zero-Shot%20Part%20Assembly%0AAuthor%3A%20Ruiyuan%20Zhang%20and%20Qi%20Wang%20and%20Jiaxiang%20Liu%20and%20Yu%20Zhang%20and%20Yuchi%20Huo%20and%20Chao%20Wu%0AAbstract%3A%20%20%203D%20part%20assembly%20aims%20to%20understand%20part%20relationships%20and%20predict%20their%0A6-DoF%20poses%20to%20construct%20realistic%203D%20shapes%2C%20addressing%20the%20growing%20demand%20for%0Aautonomous%20assembly%2C%20which%20is%20crucial%20for%20robots.%20Existing%20methods%20mainly%0Aestimate%20the%20transformation%20of%20each%20part%20by%20training%20neural%20networks%20under%0Asupervision%2C%20which%20requires%20a%20substantial%20quantity%20of%20manually%20labeled%20data.%0AHowever%2C%20the%20high%20cost%20of%20data%20collection%20and%20the%20immense%20variability%20of%0Areal-world%20shapes%20and%20parts%20make%20traditional%20methods%20impractical%20for%0Alarge-scale%20applications.%20In%20this%20paper%2C%20we%20propose%20first%20a%20zero-shot%20part%0Aassembly%20method%20that%20utilizes%20pre-trained%20point%20cloud%20diffusion%20models%20as%0Adiscriminators%20in%20the%20assembly%20process%2C%20guiding%20the%20manipulation%20of%20parts%20to%0Aform%20realistic%20shapes.%20Specifically%2C%20we%20theoretically%20demonstrate%20that%0Autilizing%20a%20diffusion%20model%20for%20zero-shot%20part%20assembly%20can%20be%20transformed%20into%0Aan%20Iterative%20Closest%20Point%20%28ICP%29%20process.%20Then%2C%20we%20propose%20a%20novel%20pushing-away%0Astrategy%20to%20address%20the%20overlap%20parts%2C%20thereby%20further%20enhancing%20the%20robustness%0Aof%20the%20method.%20To%20verify%20our%20work%2C%20we%20conduct%20extensive%20experiments%20and%0Aquantitative%20comparisons%20to%20several%20strong%20baseline%20methods%2C%20demonstrating%20the%0Aeffectiveness%20of%20the%20proposed%20approach%2C%20which%20even%20surpasses%20the%20supervised%0Alearning%20method.%20The%20code%20has%20been%20released%20on%0Ahttps%3A//github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Pretrained%2520Diffusion%2520Models%2520for%2520Zero-Shot%2520Part%2520Assembly%26entry.906535625%3DRuiyuan%2520Zhang%2520and%2520Qi%2520Wang%2520and%2520Jiaxiang%2520Liu%2520and%2520Yu%2520Zhang%2520and%2520Yuchi%2520Huo%2520and%2520Chao%2520Wu%26entry.1292438233%3D%2520%25203D%2520part%2520assembly%2520aims%2520to%2520understand%2520part%2520relationships%2520and%2520predict%2520their%250A6-DoF%2520poses%2520to%2520construct%2520realistic%25203D%2520shapes%252C%2520addressing%2520the%2520growing%2520demand%2520for%250Aautonomous%2520assembly%252C%2520which%2520is%2520crucial%2520for%2520robots.%2520Existing%2520methods%2520mainly%250Aestimate%2520the%2520transformation%2520of%2520each%2520part%2520by%2520training%2520neural%2520networks%2520under%250Asupervision%252C%2520which%2520requires%2520a%2520substantial%2520quantity%2520of%2520manually%2520labeled%2520data.%250AHowever%252C%2520the%2520high%2520cost%2520of%2520data%2520collection%2520and%2520the%2520immense%2520variability%2520of%250Areal-world%2520shapes%2520and%2520parts%2520make%2520traditional%2520methods%2520impractical%2520for%250Alarge-scale%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520first%2520a%2520zero-shot%2520part%250Aassembly%2520method%2520that%2520utilizes%2520pre-trained%2520point%2520cloud%2520diffusion%2520models%2520as%250Adiscriminators%2520in%2520the%2520assembly%2520process%252C%2520guiding%2520the%2520manipulation%2520of%2520parts%2520to%250Aform%2520realistic%2520shapes.%2520Specifically%252C%2520we%2520theoretically%2520demonstrate%2520that%250Autilizing%2520a%2520diffusion%2520model%2520for%2520zero-shot%2520part%2520assembly%2520can%2520be%2520transformed%2520into%250Aan%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%2520process.%2520Then%252C%2520we%2520propose%2520a%2520novel%2520pushing-away%250Astrategy%2520to%2520address%2520the%2520overlap%2520parts%252C%2520thereby%2520further%2520enhancing%2520the%2520robustness%250Aof%2520the%2520method.%2520To%2520verify%2520our%2520work%252C%2520we%2520conduct%2520extensive%2520experiments%2520and%250Aquantitative%2520comparisons%2520to%2520several%2520strong%2520baseline%2520methods%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%252C%2520which%2520even%2520surpasses%2520the%2520supervised%250Alearning%2520method.%2520The%2520code%2520has%2520been%2520released%2520on%250Ahttps%253A//github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Pretrained%20Diffusion%20Models%20for%20Zero-Shot%20Part%20Assembly&entry.906535625=Ruiyuan%20Zhang%20and%20Qi%20Wang%20and%20Jiaxiang%20Liu%20and%20Yu%20Zhang%20and%20Yuchi%20Huo%20and%20Chao%20Wu&entry.1292438233=%20%203D%20part%20assembly%20aims%20to%20understand%20part%20relationships%20and%20predict%20their%0A6-DoF%20poses%20to%20construct%20realistic%203D%20shapes%2C%20addressing%20the%20growing%20demand%20for%0Aautonomous%20assembly%2C%20which%20is%20crucial%20for%20robots.%20Existing%20methods%20mainly%0Aestimate%20the%20transformation%20of%20each%20part%20by%20training%20neural%20networks%20under%0Asupervision%2C%20which%20requires%20a%20substantial%20quantity%20of%20manually%20labeled%20data.%0AHowever%2C%20the%20high%20cost%20of%20data%20collection%20and%20the%20immense%20variability%20of%0Areal-world%20shapes%20and%20parts%20make%20traditional%20methods%20impractical%20for%0Alarge-scale%20applications.%20In%20this%20paper%2C%20we%20propose%20first%20a%20zero-shot%20part%0Aassembly%20method%20that%20utilizes%20pre-trained%20point%20cloud%20diffusion%20models%20as%0Adiscriminators%20in%20the%20assembly%20process%2C%20guiding%20the%20manipulation%20of%20parts%20to%0Aform%20realistic%20shapes.%20Specifically%2C%20we%20theoretically%20demonstrate%20that%0Autilizing%20a%20diffusion%20model%20for%20zero-shot%20part%20assembly%20can%20be%20transformed%20into%0Aan%20Iterative%20Closest%20Point%20%28ICP%29%20process.%20Then%2C%20we%20propose%20a%20novel%20pushing-away%0Astrategy%20to%20address%20the%20overlap%20parts%2C%20thereby%20further%20enhancing%20the%20robustness%0Aof%20the%20method.%20To%20verify%20our%20work%2C%20we%20conduct%20extensive%20experiments%20and%0Aquantitative%20comparisons%20to%20several%20strong%20baseline%20methods%2C%20demonstrating%20the%0Aeffectiveness%20of%20the%20proposed%20approach%2C%20which%20even%20surpasses%20the%20supervised%0Alearning%20method.%20The%20code%20has%20been%20released%20on%0Ahttps%3A//github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00426v1&entry.124074799=Read"},
{"title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "author": "Daniel N. Nissani", "abstract": "  A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.\n", "link": "http://arxiv.org/abs/2505.00654v1", "date": "2025-05-01", "relevancy": 2.2161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier&body=Title%3A%20Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier%0AAuthor%3A%20Daniel%20N.%20Nissani%0AAbstract%3A%20%20%20A%20lively%20ongoing%20debate%20is%20taking%20place%2C%20since%20the%20extraordinary%20emergence%20of%0ALarge%20Language%20Models%20%28LLMs%29%20with%20regards%20to%20their%20capability%20to%20understand%20the%0Aworld%20and%20capture%20the%20meaning%20of%20the%20dialogues%20in%20which%20they%20are%20involved.%0AArguments%20and%20counter-arguments%20have%20been%20proposed%20based%20upon%20thought%0Aexperiments%2C%20anecdotal%20conversations%20between%20LLMs%20and%20humans%2C%20statistical%0Alinguistic%20analysis%2C%20philosophical%20considerations%2C%20and%20more.%20In%20this%20brief%0Apaper%20we%20present%20a%20counter-argument%20based%20upon%20a%20thought%20experiment%20and%0Asemi-formal%20considerations%20leading%20to%20an%20inherent%20ambiguity%20barrier%20which%0Aprevents%20LLMs%20from%20having%20any%20understanding%20of%20what%20their%20amazingly%20fluent%0Adialogues%20mean.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Understanding%253A%2520an%2520Inherent%2520Ambiguity%2520Barrier%26entry.906535625%3DDaniel%2520N.%2520Nissani%26entry.1292438233%3D%2520%2520A%2520lively%2520ongoing%2520debate%2520is%2520taking%2520place%252C%2520since%2520the%2520extraordinary%2520emergence%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520regards%2520to%2520their%2520capability%2520to%2520understand%2520the%250Aworld%2520and%2520capture%2520the%2520meaning%2520of%2520the%2520dialogues%2520in%2520which%2520they%2520are%2520involved.%250AArguments%2520and%2520counter-arguments%2520have%2520been%2520proposed%2520based%2520upon%2520thought%250Aexperiments%252C%2520anecdotal%2520conversations%2520between%2520LLMs%2520and%2520humans%252C%2520statistical%250Alinguistic%2520analysis%252C%2520philosophical%2520considerations%252C%2520and%2520more.%2520In%2520this%2520brief%250Apaper%2520we%2520present%2520a%2520counter-argument%2520based%2520upon%2520a%2520thought%2520experiment%2520and%250Asemi-formal%2520considerations%2520leading%2520to%2520an%2520inherent%2520ambiguity%2520barrier%2520which%250Aprevents%2520LLMs%2520from%2520having%2520any%2520understanding%2520of%2520what%2520their%2520amazingly%2520fluent%250Adialogues%2520mean.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Understanding%3A%20an%20Inherent%20Ambiguity%20Barrier&entry.906535625=Daniel%20N.%20Nissani&entry.1292438233=%20%20A%20lively%20ongoing%20debate%20is%20taking%20place%2C%20since%20the%20extraordinary%20emergence%20of%0ALarge%20Language%20Models%20%28LLMs%29%20with%20regards%20to%20their%20capability%20to%20understand%20the%0Aworld%20and%20capture%20the%20meaning%20of%20the%20dialogues%20in%20which%20they%20are%20involved.%0AArguments%20and%20counter-arguments%20have%20been%20proposed%20based%20upon%20thought%0Aexperiments%2C%20anecdotal%20conversations%20between%20LLMs%20and%20humans%2C%20statistical%0Alinguistic%20analysis%2C%20philosophical%20considerations%2C%20and%20more.%20In%20this%20brief%0Apaper%20we%20present%20a%20counter-argument%20based%20upon%20a%20thought%20experiment%20and%0Asemi-formal%20considerations%20leading%20to%20an%20inherent%20ambiguity%20barrier%20which%0Aprevents%20LLMs%20from%20having%20any%20understanding%20of%20what%20their%20amazingly%20fluent%0Adialogues%20mean.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00654v1&entry.124074799=Read"},
{"title": "Advanced Physics-Informed Neural Network with Residuals for Solving\n  Complex Integral Equations", "author": "Mahdi Movahedian Moghaddam and Kourosh Parand and Saeed Reza Kheradpisheh", "abstract": "  In this paper, we present the Residual Integral Solver Network (RISN), a\nnovel neural network architecture designed to solve a wide range of integral\nand integro-differential equations, including one-dimensional,\nmulti-dimensional, ordinary and partial integro-differential, systems,\nfractional types, and Helmholtz-type integral equations involving oscillatory\nkernels. RISN integrates residual connections with high-accuracy numerical\nmethods such as Gaussian quadrature and fractional derivative operational\nmatrices, enabling it to achieve higher accuracy and stability than traditional\nPhysics-Informed Neural Networks (PINN). The residual connections help mitigate\nvanishing gradient issues, allowing RISN to handle deeper networks and more\ncomplex kernels, particularly in multi-dimensional problems. Through extensive\nexperiments, we demonstrate that RISN consistently outperforms not only\nclassical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and\nSelf-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute\nErrors (MAE) across various types of equations. These results highlight RISN's\nrobustness and efficiency in solving challenging integral and\nintegro-differential problems, making it a valuable tool for real-world\napplications where traditional methods often struggle.\n", "link": "http://arxiv.org/abs/2501.16370v2", "date": "2025-05-01", "relevancy": 2.2114, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4465}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4457}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Physics-Informed%20Neural%20Network%20with%20Residuals%20for%20Solving%0A%20%20Complex%20Integral%20Equations&body=Title%3A%20Advanced%20Physics-Informed%20Neural%20Network%20with%20Residuals%20for%20Solving%0A%20%20Complex%20Integral%20Equations%0AAuthor%3A%20Mahdi%20Movahedian%20Moghaddam%20and%20Kourosh%20Parand%20and%20Saeed%20Reza%20Kheradpisheh%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20Residual%20Integral%20Solver%20Network%20%28RISN%29%2C%20a%0Anovel%20neural%20network%20architecture%20designed%20to%20solve%20a%20wide%20range%20of%20integral%0Aand%20integro-differential%20equations%2C%20including%20one-dimensional%2C%0Amulti-dimensional%2C%20ordinary%20and%20partial%20integro-differential%2C%20systems%2C%0Afractional%20types%2C%20and%20Helmholtz-type%20integral%20equations%20involving%20oscillatory%0Akernels.%20RISN%20integrates%20residual%20connections%20with%20high-accuracy%20numerical%0Amethods%20such%20as%20Gaussian%20quadrature%20and%20fractional%20derivative%20operational%0Amatrices%2C%20enabling%20it%20to%20achieve%20higher%20accuracy%20and%20stability%20than%20traditional%0APhysics-Informed%20Neural%20Networks%20%28PINN%29.%20The%20residual%20connections%20help%20mitigate%0Avanishing%20gradient%20issues%2C%20allowing%20RISN%20to%20handle%20deeper%20networks%20and%20more%0Acomplex%20kernels%2C%20particularly%20in%20multi-dimensional%20problems.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20RISN%20consistently%20outperforms%20not%20only%0Aclassical%20PINNs%20but%20also%20advanced%20variants%20such%20as%20Auxiliary%20PINN%20%28A-PINN%29%20and%0ASelf-Adaptive%20PINN%20%28SA-PINN%29%2C%20achieving%20significantly%20lower%20Mean%20Absolute%0AErrors%20%28MAE%29%20across%20various%20types%20of%20equations.%20These%20results%20highlight%20RISN%27s%0Arobustness%20and%20efficiency%20in%20solving%20challenging%20integral%20and%0Aintegro-differential%20problems%2C%20making%20it%20a%20valuable%20tool%20for%20real-world%0Aapplications%20where%20traditional%20methods%20often%20struggle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Physics-Informed%2520Neural%2520Network%2520with%2520Residuals%2520for%2520Solving%250A%2520%2520Complex%2520Integral%2520Equations%26entry.906535625%3DMahdi%2520Movahedian%2520Moghaddam%2520and%2520Kourosh%2520Parand%2520and%2520Saeed%2520Reza%2520Kheradpisheh%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Residual%2520Integral%2520Solver%2520Network%2520%2528RISN%2529%252C%2520a%250Anovel%2520neural%2520network%2520architecture%2520designed%2520to%2520solve%2520a%2520wide%2520range%2520of%2520integral%250Aand%2520integro-differential%2520equations%252C%2520including%2520one-dimensional%252C%250Amulti-dimensional%252C%2520ordinary%2520and%2520partial%2520integro-differential%252C%2520systems%252C%250Afractional%2520types%252C%2520and%2520Helmholtz-type%2520integral%2520equations%2520involving%2520oscillatory%250Akernels.%2520RISN%2520integrates%2520residual%2520connections%2520with%2520high-accuracy%2520numerical%250Amethods%2520such%2520as%2520Gaussian%2520quadrature%2520and%2520fractional%2520derivative%2520operational%250Amatrices%252C%2520enabling%2520it%2520to%2520achieve%2520higher%2520accuracy%2520and%2520stability%2520than%2520traditional%250APhysics-Informed%2520Neural%2520Networks%2520%2528PINN%2529.%2520The%2520residual%2520connections%2520help%2520mitigate%250Avanishing%2520gradient%2520issues%252C%2520allowing%2520RISN%2520to%2520handle%2520deeper%2520networks%2520and%2520more%250Acomplex%2520kernels%252C%2520particularly%2520in%2520multi-dimensional%2520problems.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520RISN%2520consistently%2520outperforms%2520not%2520only%250Aclassical%2520PINNs%2520but%2520also%2520advanced%2520variants%2520such%2520as%2520Auxiliary%2520PINN%2520%2528A-PINN%2529%2520and%250ASelf-Adaptive%2520PINN%2520%2528SA-PINN%2529%252C%2520achieving%2520significantly%2520lower%2520Mean%2520Absolute%250AErrors%2520%2528MAE%2529%2520across%2520various%2520types%2520of%2520equations.%2520These%2520results%2520highlight%2520RISN%2527s%250Arobustness%2520and%2520efficiency%2520in%2520solving%2520challenging%2520integral%2520and%250Aintegro-differential%2520problems%252C%2520making%2520it%2520a%2520valuable%2520tool%2520for%2520real-world%250Aapplications%2520where%2520traditional%2520methods%2520often%2520struggle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Physics-Informed%20Neural%20Network%20with%20Residuals%20for%20Solving%0A%20%20Complex%20Integral%20Equations&entry.906535625=Mahdi%20Movahedian%20Moghaddam%20and%20Kourosh%20Parand%20and%20Saeed%20Reza%20Kheradpisheh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20Residual%20Integral%20Solver%20Network%20%28RISN%29%2C%20a%0Anovel%20neural%20network%20architecture%20designed%20to%20solve%20a%20wide%20range%20of%20integral%0Aand%20integro-differential%20equations%2C%20including%20one-dimensional%2C%0Amulti-dimensional%2C%20ordinary%20and%20partial%20integro-differential%2C%20systems%2C%0Afractional%20types%2C%20and%20Helmholtz-type%20integral%20equations%20involving%20oscillatory%0Akernels.%20RISN%20integrates%20residual%20connections%20with%20high-accuracy%20numerical%0Amethods%20such%20as%20Gaussian%20quadrature%20and%20fractional%20derivative%20operational%0Amatrices%2C%20enabling%20it%20to%20achieve%20higher%20accuracy%20and%20stability%20than%20traditional%0APhysics-Informed%20Neural%20Networks%20%28PINN%29.%20The%20residual%20connections%20help%20mitigate%0Avanishing%20gradient%20issues%2C%20allowing%20RISN%20to%20handle%20deeper%20networks%20and%20more%0Acomplex%20kernels%2C%20particularly%20in%20multi-dimensional%20problems.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20RISN%20consistently%20outperforms%20not%20only%0Aclassical%20PINNs%20but%20also%20advanced%20variants%20such%20as%20Auxiliary%20PINN%20%28A-PINN%29%20and%0ASelf-Adaptive%20PINN%20%28SA-PINN%29%2C%20achieving%20significantly%20lower%20Mean%20Absolute%0AErrors%20%28MAE%29%20across%20various%20types%20of%20equations.%20These%20results%20highlight%20RISN%27s%0Arobustness%20and%20efficiency%20in%20solving%20challenging%20integral%20and%0Aintegro-differential%20problems%2C%20making%20it%20a%20valuable%20tool%20for%20real-world%0Aapplications%20where%20traditional%20methods%20often%20struggle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16370v2&entry.124074799=Read"},
{"title": "Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera\n  Radar Datasets", "author": "Mathis Morales and Golnaz Habibi", "abstract": "  Detecting and tracking objects is a crucial component of any autonomous\nnavigation method. For the past decades, object detection has yielded promising\nresults using neural networks on various datasets. While many methods focus on\nperformance metrics, few projects focus on improving the robustness of these\ndetection and tracking pipelines, notably to sensor failures. In this paper we\nattempt to address this issue by creating a realistic synthetic data\naugmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our\ngoal is to accurately simulate sensor failures and data deterioration due to\nreal-world interferences. We also present our results of a baseline lightweight\nNoise Recognition neural network trained and tested on our augmented dataset,\nreaching an overall recognition accuracy of 54.4\\% on 11 categories across\n10086 images and 2145 radar point-clouds.\n", "link": "http://arxiv.org/abs/2505.00584v1", "date": "2025-05-01", "relevancy": 2.203, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5661}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5519}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20and%20Identifying%20Noise%20Levels%20in%20Autonomous%20Vehicle%20Camera%0A%20%20Radar%20Datasets&body=Title%3A%20Synthesizing%20and%20Identifying%20Noise%20Levels%20in%20Autonomous%20Vehicle%20Camera%0A%20%20Radar%20Datasets%0AAuthor%3A%20Mathis%20Morales%20and%20Golnaz%20Habibi%0AAbstract%3A%20%20%20Detecting%20and%20tracking%20objects%20is%20a%20crucial%20component%20of%20any%20autonomous%0Anavigation%20method.%20For%20the%20past%20decades%2C%20object%20detection%20has%20yielded%20promising%0Aresults%20using%20neural%20networks%20on%20various%20datasets.%20While%20many%20methods%20focus%20on%0Aperformance%20metrics%2C%20few%20projects%20focus%20on%20improving%20the%20robustness%20of%20these%0Adetection%20and%20tracking%20pipelines%2C%20notably%20to%20sensor%20failures.%20In%20this%20paper%20we%0Aattempt%20to%20address%20this%20issue%20by%20creating%20a%20realistic%20synthetic%20data%0Aaugmentation%20pipeline%20for%20camera-radar%20Autonomous%20Vehicle%20%28AV%29%20datasets.%20Our%0Agoal%20is%20to%20accurately%20simulate%20sensor%20failures%20and%20data%20deterioration%20due%20to%0Areal-world%20interferences.%20We%20also%20present%20our%20results%20of%20a%20baseline%20lightweight%0ANoise%20Recognition%20neural%20network%20trained%20and%20tested%20on%20our%20augmented%20dataset%2C%0Areaching%20an%20overall%20recognition%20accuracy%20of%2054.4%5C%25%20on%2011%20categories%20across%0A10086%20images%20and%202145%20radar%20point-clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520and%2520Identifying%2520Noise%2520Levels%2520in%2520Autonomous%2520Vehicle%2520Camera%250A%2520%2520Radar%2520Datasets%26entry.906535625%3DMathis%2520Morales%2520and%2520Golnaz%2520Habibi%26entry.1292438233%3D%2520%2520Detecting%2520and%2520tracking%2520objects%2520is%2520a%2520crucial%2520component%2520of%2520any%2520autonomous%250Anavigation%2520method.%2520For%2520the%2520past%2520decades%252C%2520object%2520detection%2520has%2520yielded%2520promising%250Aresults%2520using%2520neural%2520networks%2520on%2520various%2520datasets.%2520While%2520many%2520methods%2520focus%2520on%250Aperformance%2520metrics%252C%2520few%2520projects%2520focus%2520on%2520improving%2520the%2520robustness%2520of%2520these%250Adetection%2520and%2520tracking%2520pipelines%252C%2520notably%2520to%2520sensor%2520failures.%2520In%2520this%2520paper%2520we%250Aattempt%2520to%2520address%2520this%2520issue%2520by%2520creating%2520a%2520realistic%2520synthetic%2520data%250Aaugmentation%2520pipeline%2520for%2520camera-radar%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520datasets.%2520Our%250Agoal%2520is%2520to%2520accurately%2520simulate%2520sensor%2520failures%2520and%2520data%2520deterioration%2520due%2520to%250Areal-world%2520interferences.%2520We%2520also%2520present%2520our%2520results%2520of%2520a%2520baseline%2520lightweight%250ANoise%2520Recognition%2520neural%2520network%2520trained%2520and%2520tested%2520on%2520our%2520augmented%2520dataset%252C%250Areaching%2520an%2520overall%2520recognition%2520accuracy%2520of%252054.4%255C%2525%2520on%252011%2520categories%2520across%250A10086%2520images%2520and%25202145%2520radar%2520point-clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20and%20Identifying%20Noise%20Levels%20in%20Autonomous%20Vehicle%20Camera%0A%20%20Radar%20Datasets&entry.906535625=Mathis%20Morales%20and%20Golnaz%20Habibi&entry.1292438233=%20%20Detecting%20and%20tracking%20objects%20is%20a%20crucial%20component%20of%20any%20autonomous%0Anavigation%20method.%20For%20the%20past%20decades%2C%20object%20detection%20has%20yielded%20promising%0Aresults%20using%20neural%20networks%20on%20various%20datasets.%20While%20many%20methods%20focus%20on%0Aperformance%20metrics%2C%20few%20projects%20focus%20on%20improving%20the%20robustness%20of%20these%0Adetection%20and%20tracking%20pipelines%2C%20notably%20to%20sensor%20failures.%20In%20this%20paper%20we%0Aattempt%20to%20address%20this%20issue%20by%20creating%20a%20realistic%20synthetic%20data%0Aaugmentation%20pipeline%20for%20camera-radar%20Autonomous%20Vehicle%20%28AV%29%20datasets.%20Our%0Agoal%20is%20to%20accurately%20simulate%20sensor%20failures%20and%20data%20deterioration%20due%20to%0Areal-world%20interferences.%20We%20also%20present%20our%20results%20of%20a%20baseline%20lightweight%0ANoise%20Recognition%20neural%20network%20trained%20and%20tested%20on%20our%20augmented%20dataset%2C%0Areaching%20an%20overall%20recognition%20accuracy%20of%2054.4%5C%25%20on%2011%20categories%20across%0A10086%20images%20and%202145%20radar%20point-clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00584v1&entry.124074799=Read"},
{"title": "Learning An Active Inference Model of Driver Perception and Control:\n  Application to Vehicle Car-Following", "author": "Ran Wei and Anthony D. McDonald and Alfredo Garcia and Gustav Markkula and Johan Engstrom and Matthew O'Kelly", "abstract": "  In this paper we introduce a general estimation methodology for learning a\nmodel of human perception and control in a sensorimotor control task based upon\na finite set of demonstrations. The model's structure consists of i the agent's\ninternal representation of how the environment and associated observations\nevolve as a result of control actions and ii the agent's preferences over\nobservable outcomes. We consider a model's structure specification consistent\nwith active inference, a theory of human perception and behavior from cognitive\nscience. According to active inference, the agent acts upon the world so as to\nminimize surprise defined as a measure of the extent to which an agent's\ncurrent sensory observations differ from its preferred sensory observations. We\npropose a bi-level optimization approach to estimation which relies on a\nstructural assumption on prior distributions that parameterize the statistical\naccuracy of the human agent's model of the environment. To illustrate the\nproposed methodology, we present the estimation of a model for car-following\nbehavior based upon a naturalistic dataset. Overall, the results indicate that\nlearning active inference models of human perception and control from data is a\npromising alternative to black-box models of driving.\n", "link": "http://arxiv.org/abs/2303.15201v2", "date": "2025-05-01", "relevancy": 2.1941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6232}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5349}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20An%20Active%20Inference%20Model%20of%20Driver%20Perception%20and%20Control%3A%0A%20%20Application%20to%20Vehicle%20Car-Following&body=Title%3A%20Learning%20An%20Active%20Inference%20Model%20of%20Driver%20Perception%20and%20Control%3A%0A%20%20Application%20to%20Vehicle%20Car-Following%0AAuthor%3A%20Ran%20Wei%20and%20Anthony%20D.%20McDonald%20and%20Alfredo%20Garcia%20and%20Gustav%20Markkula%20and%20Johan%20Engstrom%20and%20Matthew%20O%27Kelly%0AAbstract%3A%20%20%20In%20this%20paper%20we%20introduce%20a%20general%20estimation%20methodology%20for%20learning%20a%0Amodel%20of%20human%20perception%20and%20control%20in%20a%20sensorimotor%20control%20task%20based%20upon%0Aa%20finite%20set%20of%20demonstrations.%20The%20model%27s%20structure%20consists%20of%20i%20the%20agent%27s%0Ainternal%20representation%20of%20how%20the%20environment%20and%20associated%20observations%0Aevolve%20as%20a%20result%20of%20control%20actions%20and%20ii%20the%20agent%27s%20preferences%20over%0Aobservable%20outcomes.%20We%20consider%20a%20model%27s%20structure%20specification%20consistent%0Awith%20active%20inference%2C%20a%20theory%20of%20human%20perception%20and%20behavior%20from%20cognitive%0Ascience.%20According%20to%20active%20inference%2C%20the%20agent%20acts%20upon%20the%20world%20so%20as%20to%0Aminimize%20surprise%20defined%20as%20a%20measure%20of%20the%20extent%20to%20which%20an%20agent%27s%0Acurrent%20sensory%20observations%20differ%20from%20its%20preferred%20sensory%20observations.%20We%0Apropose%20a%20bi-level%20optimization%20approach%20to%20estimation%20which%20relies%20on%20a%0Astructural%20assumption%20on%20prior%20distributions%20that%20parameterize%20the%20statistical%0Aaccuracy%20of%20the%20human%20agent%27s%20model%20of%20the%20environment.%20To%20illustrate%20the%0Aproposed%20methodology%2C%20we%20present%20the%20estimation%20of%20a%20model%20for%20car-following%0Abehavior%20based%20upon%20a%20naturalistic%20dataset.%20Overall%2C%20the%20results%20indicate%20that%0Alearning%20active%20inference%20models%20of%20human%20perception%20and%20control%20from%20data%20is%20a%0Apromising%20alternative%20to%20black-box%20models%20of%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520An%2520Active%2520Inference%2520Model%2520of%2520Driver%2520Perception%2520and%2520Control%253A%250A%2520%2520Application%2520to%2520Vehicle%2520Car-Following%26entry.906535625%3DRan%2520Wei%2520and%2520Anthony%2520D.%2520McDonald%2520and%2520Alfredo%2520Garcia%2520and%2520Gustav%2520Markkula%2520and%2520Johan%2520Engstrom%2520and%2520Matthew%2520O%2527Kelly%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520introduce%2520a%2520general%2520estimation%2520methodology%2520for%2520learning%2520a%250Amodel%2520of%2520human%2520perception%2520and%2520control%2520in%2520a%2520sensorimotor%2520control%2520task%2520based%2520upon%250Aa%2520finite%2520set%2520of%2520demonstrations.%2520The%2520model%2527s%2520structure%2520consists%2520of%2520i%2520the%2520agent%2527s%250Ainternal%2520representation%2520of%2520how%2520the%2520environment%2520and%2520associated%2520observations%250Aevolve%2520as%2520a%2520result%2520of%2520control%2520actions%2520and%2520ii%2520the%2520agent%2527s%2520preferences%2520over%250Aobservable%2520outcomes.%2520We%2520consider%2520a%2520model%2527s%2520structure%2520specification%2520consistent%250Awith%2520active%2520inference%252C%2520a%2520theory%2520of%2520human%2520perception%2520and%2520behavior%2520from%2520cognitive%250Ascience.%2520According%2520to%2520active%2520inference%252C%2520the%2520agent%2520acts%2520upon%2520the%2520world%2520so%2520as%2520to%250Aminimize%2520surprise%2520defined%2520as%2520a%2520measure%2520of%2520the%2520extent%2520to%2520which%2520an%2520agent%2527s%250Acurrent%2520sensory%2520observations%2520differ%2520from%2520its%2520preferred%2520sensory%2520observations.%2520We%250Apropose%2520a%2520bi-level%2520optimization%2520approach%2520to%2520estimation%2520which%2520relies%2520on%2520a%250Astructural%2520assumption%2520on%2520prior%2520distributions%2520that%2520parameterize%2520the%2520statistical%250Aaccuracy%2520of%2520the%2520human%2520agent%2527s%2520model%2520of%2520the%2520environment.%2520To%2520illustrate%2520the%250Aproposed%2520methodology%252C%2520we%2520present%2520the%2520estimation%2520of%2520a%2520model%2520for%2520car-following%250Abehavior%2520based%2520upon%2520a%2520naturalistic%2520dataset.%2520Overall%252C%2520the%2520results%2520indicate%2520that%250Alearning%2520active%2520inference%2520models%2520of%2520human%2520perception%2520and%2520control%2520from%2520data%2520is%2520a%250Apromising%2520alternative%2520to%2520black-box%2520models%2520of%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20An%20Active%20Inference%20Model%20of%20Driver%20Perception%20and%20Control%3A%0A%20%20Application%20to%20Vehicle%20Car-Following&entry.906535625=Ran%20Wei%20and%20Anthony%20D.%20McDonald%20and%20Alfredo%20Garcia%20and%20Gustav%20Markkula%20and%20Johan%20Engstrom%20and%20Matthew%20O%27Kelly&entry.1292438233=%20%20In%20this%20paper%20we%20introduce%20a%20general%20estimation%20methodology%20for%20learning%20a%0Amodel%20of%20human%20perception%20and%20control%20in%20a%20sensorimotor%20control%20task%20based%20upon%0Aa%20finite%20set%20of%20demonstrations.%20The%20model%27s%20structure%20consists%20of%20i%20the%20agent%27s%0Ainternal%20representation%20of%20how%20the%20environment%20and%20associated%20observations%0Aevolve%20as%20a%20result%20of%20control%20actions%20and%20ii%20the%20agent%27s%20preferences%20over%0Aobservable%20outcomes.%20We%20consider%20a%20model%27s%20structure%20specification%20consistent%0Awith%20active%20inference%2C%20a%20theory%20of%20human%20perception%20and%20behavior%20from%20cognitive%0Ascience.%20According%20to%20active%20inference%2C%20the%20agent%20acts%20upon%20the%20world%20so%20as%20to%0Aminimize%20surprise%20defined%20as%20a%20measure%20of%20the%20extent%20to%20which%20an%20agent%27s%0Acurrent%20sensory%20observations%20differ%20from%20its%20preferred%20sensory%20observations.%20We%0Apropose%20a%20bi-level%20optimization%20approach%20to%20estimation%20which%20relies%20on%20a%0Astructural%20assumption%20on%20prior%20distributions%20that%20parameterize%20the%20statistical%0Aaccuracy%20of%20the%20human%20agent%27s%20model%20of%20the%20environment.%20To%20illustrate%20the%0Aproposed%20methodology%2C%20we%20present%20the%20estimation%20of%20a%20model%20for%20car-following%0Abehavior%20based%20upon%20a%20naturalistic%20dataset.%20Overall%2C%20the%20results%20indicate%20that%0Alearning%20active%20inference%20models%20of%20human%20perception%20and%20control%20from%20data%20is%20a%0Apromising%20alternative%20to%20black-box%20models%20of%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15201v2&entry.124074799=Read"},
{"title": "Action-Minimization Meets Generative Modeling: Efficient Transition Path\n  Sampling with the Onsager-Machlup Functional", "author": "Sanjeev Raja and Martin \u0160\u00edpka and Michael Psenka and Tobias Kreiman and Michal Pavelka and Aditi S. Krishnapriyan", "abstract": "  Transition path sampling (TPS), which involves finding probable paths\nconnecting two points on an energy landscape, remains a challenge due to the\ncomplexity of real-world atomistic systems. Current machine learning approaches\nuse expensive, task-specific, and data-free training procedures, limiting their\nability to benefit from recent advances in atomistic machine learning, such as\nhigh-quality datasets and large-scale pre-trained models. In this work, we\naddress TPS by interpreting candidate paths as trajectories sampled from\nstochastic dynamics induced by the learned score function of pre-trained\ngenerative models, specifically denoising diffusion and flow matching. Under\nthese dynamics, finding high-likelihood transition paths becomes equivalent to\nminimizing the Onsager-Machlup (OM) action functional. This enables us to\nrepurpose pre-trained generative models for TPS in a zero-shot manner, in\ncontrast with bespoke, task-specific TPS models trained in previous work. We\ndemonstrate our approach on varied molecular systems, obtaining diverse,\nphysically realistic transition pathways and generalizing beyond the\npre-trained model's original training dataset. Our method can be easily\nincorporated into new generative models, making it practically relevant as\nmodels continue to scale and improve with increased data availability.\n", "link": "http://arxiv.org/abs/2504.18506v2", "date": "2025-05-01", "relevancy": 2.1923, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5547}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action-Minimization%20Meets%20Generative%20Modeling%3A%20Efficient%20Transition%20Path%0A%20%20Sampling%20with%20the%20Onsager-Machlup%20Functional&body=Title%3A%20Action-Minimization%20Meets%20Generative%20Modeling%3A%20Efficient%20Transition%20Path%0A%20%20Sampling%20with%20the%20Onsager-Machlup%20Functional%0AAuthor%3A%20Sanjeev%20Raja%20and%20Martin%20%C5%A0%C3%ADpka%20and%20Michael%20Psenka%20and%20Tobias%20Kreiman%20and%20Michal%20Pavelka%20and%20Aditi%20S.%20Krishnapriyan%0AAbstract%3A%20%20%20Transition%20path%20sampling%20%28TPS%29%2C%20which%20involves%20finding%20probable%20paths%0Aconnecting%20two%20points%20on%20an%20energy%20landscape%2C%20remains%20a%20challenge%20due%20to%20the%0Acomplexity%20of%20real-world%20atomistic%20systems.%20Current%20machine%20learning%20approaches%0Ause%20expensive%2C%20task-specific%2C%20and%20data-free%20training%20procedures%2C%20limiting%20their%0Aability%20to%20benefit%20from%20recent%20advances%20in%20atomistic%20machine%20learning%2C%20such%20as%0Ahigh-quality%20datasets%20and%20large-scale%20pre-trained%20models.%20In%20this%20work%2C%20we%0Aaddress%20TPS%20by%20interpreting%20candidate%20paths%20as%20trajectories%20sampled%20from%0Astochastic%20dynamics%20induced%20by%20the%20learned%20score%20function%20of%20pre-trained%0Agenerative%20models%2C%20specifically%20denoising%20diffusion%20and%20flow%20matching.%20Under%0Athese%20dynamics%2C%20finding%20high-likelihood%20transition%20paths%20becomes%20equivalent%20to%0Aminimizing%20the%20Onsager-Machlup%20%28OM%29%20action%20functional.%20This%20enables%20us%20to%0Arepurpose%20pre-trained%20generative%20models%20for%20TPS%20in%20a%20zero-shot%20manner%2C%20in%0Acontrast%20with%20bespoke%2C%20task-specific%20TPS%20models%20trained%20in%20previous%20work.%20We%0Ademonstrate%20our%20approach%20on%20varied%20molecular%20systems%2C%20obtaining%20diverse%2C%0Aphysically%20realistic%20transition%20pathways%20and%20generalizing%20beyond%20the%0Apre-trained%20model%27s%20original%20training%20dataset.%20Our%20method%20can%20be%20easily%0Aincorporated%20into%20new%20generative%20models%2C%20making%20it%20practically%20relevant%20as%0Amodels%20continue%20to%20scale%20and%20improve%20with%20increased%20data%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction-Minimization%2520Meets%2520Generative%2520Modeling%253A%2520Efficient%2520Transition%2520Path%250A%2520%2520Sampling%2520with%2520the%2520Onsager-Machlup%2520Functional%26entry.906535625%3DSanjeev%2520Raja%2520and%2520Martin%2520%25C5%25A0%25C3%25ADpka%2520and%2520Michael%2520Psenka%2520and%2520Tobias%2520Kreiman%2520and%2520Michal%2520Pavelka%2520and%2520Aditi%2520S.%2520Krishnapriyan%26entry.1292438233%3D%2520%2520Transition%2520path%2520sampling%2520%2528TPS%2529%252C%2520which%2520involves%2520finding%2520probable%2520paths%250Aconnecting%2520two%2520points%2520on%2520an%2520energy%2520landscape%252C%2520remains%2520a%2520challenge%2520due%2520to%2520the%250Acomplexity%2520of%2520real-world%2520atomistic%2520systems.%2520Current%2520machine%2520learning%2520approaches%250Ause%2520expensive%252C%2520task-specific%252C%2520and%2520data-free%2520training%2520procedures%252C%2520limiting%2520their%250Aability%2520to%2520benefit%2520from%2520recent%2520advances%2520in%2520atomistic%2520machine%2520learning%252C%2520such%2520as%250Ahigh-quality%2520datasets%2520and%2520large-scale%2520pre-trained%2520models.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520TPS%2520by%2520interpreting%2520candidate%2520paths%2520as%2520trajectories%2520sampled%2520from%250Astochastic%2520dynamics%2520induced%2520by%2520the%2520learned%2520score%2520function%2520of%2520pre-trained%250Agenerative%2520models%252C%2520specifically%2520denoising%2520diffusion%2520and%2520flow%2520matching.%2520Under%250Athese%2520dynamics%252C%2520finding%2520high-likelihood%2520transition%2520paths%2520becomes%2520equivalent%2520to%250Aminimizing%2520the%2520Onsager-Machlup%2520%2528OM%2529%2520action%2520functional.%2520This%2520enables%2520us%2520to%250Arepurpose%2520pre-trained%2520generative%2520models%2520for%2520TPS%2520in%2520a%2520zero-shot%2520manner%252C%2520in%250Acontrast%2520with%2520bespoke%252C%2520task-specific%2520TPS%2520models%2520trained%2520in%2520previous%2520work.%2520We%250Ademonstrate%2520our%2520approach%2520on%2520varied%2520molecular%2520systems%252C%2520obtaining%2520diverse%252C%250Aphysically%2520realistic%2520transition%2520pathways%2520and%2520generalizing%2520beyond%2520the%250Apre-trained%2520model%2527s%2520original%2520training%2520dataset.%2520Our%2520method%2520can%2520be%2520easily%250Aincorporated%2520into%2520new%2520generative%2520models%252C%2520making%2520it%2520practically%2520relevant%2520as%250Amodels%2520continue%2520to%2520scale%2520and%2520improve%2520with%2520increased%2520data%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action-Minimization%20Meets%20Generative%20Modeling%3A%20Efficient%20Transition%20Path%0A%20%20Sampling%20with%20the%20Onsager-Machlup%20Functional&entry.906535625=Sanjeev%20Raja%20and%20Martin%20%C5%A0%C3%ADpka%20and%20Michael%20Psenka%20and%20Tobias%20Kreiman%20and%20Michal%20Pavelka%20and%20Aditi%20S.%20Krishnapriyan&entry.1292438233=%20%20Transition%20path%20sampling%20%28TPS%29%2C%20which%20involves%20finding%20probable%20paths%0Aconnecting%20two%20points%20on%20an%20energy%20landscape%2C%20remains%20a%20challenge%20due%20to%20the%0Acomplexity%20of%20real-world%20atomistic%20systems.%20Current%20machine%20learning%20approaches%0Ause%20expensive%2C%20task-specific%2C%20and%20data-free%20training%20procedures%2C%20limiting%20their%0Aability%20to%20benefit%20from%20recent%20advances%20in%20atomistic%20machine%20learning%2C%20such%20as%0Ahigh-quality%20datasets%20and%20large-scale%20pre-trained%20models.%20In%20this%20work%2C%20we%0Aaddress%20TPS%20by%20interpreting%20candidate%20paths%20as%20trajectories%20sampled%20from%0Astochastic%20dynamics%20induced%20by%20the%20learned%20score%20function%20of%20pre-trained%0Agenerative%20models%2C%20specifically%20denoising%20diffusion%20and%20flow%20matching.%20Under%0Athese%20dynamics%2C%20finding%20high-likelihood%20transition%20paths%20becomes%20equivalent%20to%0Aminimizing%20the%20Onsager-Machlup%20%28OM%29%20action%20functional.%20This%20enables%20us%20to%0Arepurpose%20pre-trained%20generative%20models%20for%20TPS%20in%20a%20zero-shot%20manner%2C%20in%0Acontrast%20with%20bespoke%2C%20task-specific%20TPS%20models%20trained%20in%20previous%20work.%20We%0Ademonstrate%20our%20approach%20on%20varied%20molecular%20systems%2C%20obtaining%20diverse%2C%0Aphysically%20realistic%20transition%20pathways%20and%20generalizing%20beyond%20the%0Apre-trained%20model%27s%20original%20training%20dataset.%20Our%20method%20can%20be%20easily%0Aincorporated%20into%20new%20generative%20models%2C%20making%20it%20practically%20relevant%20as%0Amodels%20continue%20to%20scale%20and%20improve%20with%20increased%20data%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18506v2&entry.124074799=Read"},
{"title": "Deep Learning Assisted Outer Volume Removal for Highly-Accelerated\n  Real-Time Dynamic MRI", "author": "Merve G\u00fclle and Sebastian Weing\u00e4rtner and Mehmet Ak\u00e7akaya", "abstract": "  Real-time (RT) dynamic MRI plays a vital role in capturing rapid\nphysiological processes, offering unique insights into organ motion and\nfunction. Among these applications, RT cine MRI is particularly important for\nfunctional assessment of the heart with high temporal resolution. RT imaging\nenables free-breathing, ungated imaging of cardiac motion, making it a crucial\nalternative for patients who cannot tolerate conventional breath-hold,\nECG-gated acquisitions. However, achieving high acceleration rates in RT cine\nMRI is challenging due to aliasing artifacts from extra-cardiac tissues,\nparticularly at high undersampling factors. In this study, we propose a novel\nouter volume removal (OVR) method to address this challenge by eliminating\naliasing contributions from non-cardiac regions in a post-processing framework.\nOur approach estimates the outer volume signal for each timeframe using\ncomposite temporal images from time-interleaved undersampling patterns, which\ninherently contain pseudo-periodic ghosting artifacts. A deep learning (DL)\nmodel is trained to identify and remove these artifacts, producing a clean\nouter volume estimate that is subsequently subtracted from the corresponding\nk-space data. The final reconstruction is performed with a physics-driven DL\n(PD-DL) method trained using an OVR-specific loss function to restore high\nspatio-temporal resolution images. Experimental results show that the proposed\nmethod at high accelerations achieves image quality that is visually comparable\nto clinical baseline images, while outperforming conventional reconstruction\ntechniques, both qualitatively and quantitatively. The proposed approach\nprovides a practical and effective solution for artifact reduction in RT cine\nMRI without requiring acquisition modifications, offering a pathway to higher\nacceleration rates while preserving diagnostic quality.\n", "link": "http://arxiv.org/abs/2505.00643v1", "date": "2025-05-01", "relevancy": 2.1643, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5466}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5396}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Assisted%20Outer%20Volume%20Removal%20for%20Highly-Accelerated%0A%20%20Real-Time%20Dynamic%20MRI&body=Title%3A%20Deep%20Learning%20Assisted%20Outer%20Volume%20Removal%20for%20Highly-Accelerated%0A%20%20Real-Time%20Dynamic%20MRI%0AAuthor%3A%20Merve%20G%C3%BClle%20and%20Sebastian%20Weing%C3%A4rtner%20and%20Mehmet%20Ak%C3%A7akaya%0AAbstract%3A%20%20%20Real-time%20%28RT%29%20dynamic%20MRI%20plays%20a%20vital%20role%20in%20capturing%20rapid%0Aphysiological%20processes%2C%20offering%20unique%20insights%20into%20organ%20motion%20and%0Afunction.%20Among%20these%20applications%2C%20RT%20cine%20MRI%20is%20particularly%20important%20for%0Afunctional%20assessment%20of%20the%20heart%20with%20high%20temporal%20resolution.%20RT%20imaging%0Aenables%20free-breathing%2C%20ungated%20imaging%20of%20cardiac%20motion%2C%20making%20it%20a%20crucial%0Aalternative%20for%20patients%20who%20cannot%20tolerate%20conventional%20breath-hold%2C%0AECG-gated%20acquisitions.%20However%2C%20achieving%20high%20acceleration%20rates%20in%20RT%20cine%0AMRI%20is%20challenging%20due%20to%20aliasing%20artifacts%20from%20extra-cardiac%20tissues%2C%0Aparticularly%20at%20high%20undersampling%20factors.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Aouter%20volume%20removal%20%28OVR%29%20method%20to%20address%20this%20challenge%20by%20eliminating%0Aaliasing%20contributions%20from%20non-cardiac%20regions%20in%20a%20post-processing%20framework.%0AOur%20approach%20estimates%20the%20outer%20volume%20signal%20for%20each%20timeframe%20using%0Acomposite%20temporal%20images%20from%20time-interleaved%20undersampling%20patterns%2C%20which%0Ainherently%20contain%20pseudo-periodic%20ghosting%20artifacts.%20A%20deep%20learning%20%28DL%29%0Amodel%20is%20trained%20to%20identify%20and%20remove%20these%20artifacts%2C%20producing%20a%20clean%0Aouter%20volume%20estimate%20that%20is%20subsequently%20subtracted%20from%20the%20corresponding%0Ak-space%20data.%20The%20final%20reconstruction%20is%20performed%20with%20a%20physics-driven%20DL%0A%28PD-DL%29%20method%20trained%20using%20an%20OVR-specific%20loss%20function%20to%20restore%20high%0Aspatio-temporal%20resolution%20images.%20Experimental%20results%20show%20that%20the%20proposed%0Amethod%20at%20high%20accelerations%20achieves%20image%20quality%20that%20is%20visually%20comparable%0Ato%20clinical%20baseline%20images%2C%20while%20outperforming%20conventional%20reconstruction%0Atechniques%2C%20both%20qualitatively%20and%20quantitatively.%20The%20proposed%20approach%0Aprovides%20a%20practical%20and%20effective%20solution%20for%20artifact%20reduction%20in%20RT%20cine%0AMRI%20without%20requiring%20acquisition%20modifications%2C%20offering%20a%20pathway%20to%20higher%0Aacceleration%20rates%20while%20preserving%20diagnostic%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Assisted%2520Outer%2520Volume%2520Removal%2520for%2520Highly-Accelerated%250A%2520%2520Real-Time%2520Dynamic%2520MRI%26entry.906535625%3DMerve%2520G%25C3%25BClle%2520and%2520Sebastian%2520Weing%25C3%25A4rtner%2520and%2520Mehmet%2520Ak%25C3%25A7akaya%26entry.1292438233%3D%2520%2520Real-time%2520%2528RT%2529%2520dynamic%2520MRI%2520plays%2520a%2520vital%2520role%2520in%2520capturing%2520rapid%250Aphysiological%2520processes%252C%2520offering%2520unique%2520insights%2520into%2520organ%2520motion%2520and%250Afunction.%2520Among%2520these%2520applications%252C%2520RT%2520cine%2520MRI%2520is%2520particularly%2520important%2520for%250Afunctional%2520assessment%2520of%2520the%2520heart%2520with%2520high%2520temporal%2520resolution.%2520RT%2520imaging%250Aenables%2520free-breathing%252C%2520ungated%2520imaging%2520of%2520cardiac%2520motion%252C%2520making%2520it%2520a%2520crucial%250Aalternative%2520for%2520patients%2520who%2520cannot%2520tolerate%2520conventional%2520breath-hold%252C%250AECG-gated%2520acquisitions.%2520However%252C%2520achieving%2520high%2520acceleration%2520rates%2520in%2520RT%2520cine%250AMRI%2520is%2520challenging%2520due%2520to%2520aliasing%2520artifacts%2520from%2520extra-cardiac%2520tissues%252C%250Aparticularly%2520at%2520high%2520undersampling%2520factors.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250Aouter%2520volume%2520removal%2520%2528OVR%2529%2520method%2520to%2520address%2520this%2520challenge%2520by%2520eliminating%250Aaliasing%2520contributions%2520from%2520non-cardiac%2520regions%2520in%2520a%2520post-processing%2520framework.%250AOur%2520approach%2520estimates%2520the%2520outer%2520volume%2520signal%2520for%2520each%2520timeframe%2520using%250Acomposite%2520temporal%2520images%2520from%2520time-interleaved%2520undersampling%2520patterns%252C%2520which%250Ainherently%2520contain%2520pseudo-periodic%2520ghosting%2520artifacts.%2520A%2520deep%2520learning%2520%2528DL%2529%250Amodel%2520is%2520trained%2520to%2520identify%2520and%2520remove%2520these%2520artifacts%252C%2520producing%2520a%2520clean%250Aouter%2520volume%2520estimate%2520that%2520is%2520subsequently%2520subtracted%2520from%2520the%2520corresponding%250Ak-space%2520data.%2520The%2520final%2520reconstruction%2520is%2520performed%2520with%2520a%2520physics-driven%2520DL%250A%2528PD-DL%2529%2520method%2520trained%2520using%2520an%2520OVR-specific%2520loss%2520function%2520to%2520restore%2520high%250Aspatio-temporal%2520resolution%2520images.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%250Amethod%2520at%2520high%2520accelerations%2520achieves%2520image%2520quality%2520that%2520is%2520visually%2520comparable%250Ato%2520clinical%2520baseline%2520images%252C%2520while%2520outperforming%2520conventional%2520reconstruction%250Atechniques%252C%2520both%2520qualitatively%2520and%2520quantitatively.%2520The%2520proposed%2520approach%250Aprovides%2520a%2520practical%2520and%2520effective%2520solution%2520for%2520artifact%2520reduction%2520in%2520RT%2520cine%250AMRI%2520without%2520requiring%2520acquisition%2520modifications%252C%2520offering%2520a%2520pathway%2520to%2520higher%250Aacceleration%2520rates%2520while%2520preserving%2520diagnostic%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Assisted%20Outer%20Volume%20Removal%20for%20Highly-Accelerated%0A%20%20Real-Time%20Dynamic%20MRI&entry.906535625=Merve%20G%C3%BClle%20and%20Sebastian%20Weing%C3%A4rtner%20and%20Mehmet%20Ak%C3%A7akaya&entry.1292438233=%20%20Real-time%20%28RT%29%20dynamic%20MRI%20plays%20a%20vital%20role%20in%20capturing%20rapid%0Aphysiological%20processes%2C%20offering%20unique%20insights%20into%20organ%20motion%20and%0Afunction.%20Among%20these%20applications%2C%20RT%20cine%20MRI%20is%20particularly%20important%20for%0Afunctional%20assessment%20of%20the%20heart%20with%20high%20temporal%20resolution.%20RT%20imaging%0Aenables%20free-breathing%2C%20ungated%20imaging%20of%20cardiac%20motion%2C%20making%20it%20a%20crucial%0Aalternative%20for%20patients%20who%20cannot%20tolerate%20conventional%20breath-hold%2C%0AECG-gated%20acquisitions.%20However%2C%20achieving%20high%20acceleration%20rates%20in%20RT%20cine%0AMRI%20is%20challenging%20due%20to%20aliasing%20artifacts%20from%20extra-cardiac%20tissues%2C%0Aparticularly%20at%20high%20undersampling%20factors.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Aouter%20volume%20removal%20%28OVR%29%20method%20to%20address%20this%20challenge%20by%20eliminating%0Aaliasing%20contributions%20from%20non-cardiac%20regions%20in%20a%20post-processing%20framework.%0AOur%20approach%20estimates%20the%20outer%20volume%20signal%20for%20each%20timeframe%20using%0Acomposite%20temporal%20images%20from%20time-interleaved%20undersampling%20patterns%2C%20which%0Ainherently%20contain%20pseudo-periodic%20ghosting%20artifacts.%20A%20deep%20learning%20%28DL%29%0Amodel%20is%20trained%20to%20identify%20and%20remove%20these%20artifacts%2C%20producing%20a%20clean%0Aouter%20volume%20estimate%20that%20is%20subsequently%20subtracted%20from%20the%20corresponding%0Ak-space%20data.%20The%20final%20reconstruction%20is%20performed%20with%20a%20physics-driven%20DL%0A%28PD-DL%29%20method%20trained%20using%20an%20OVR-specific%20loss%20function%20to%20restore%20high%0Aspatio-temporal%20resolution%20images.%20Experimental%20results%20show%20that%20the%20proposed%0Amethod%20at%20high%20accelerations%20achieves%20image%20quality%20that%20is%20visually%20comparable%0Ato%20clinical%20baseline%20images%2C%20while%20outperforming%20conventional%20reconstruction%0Atechniques%2C%20both%20qualitatively%20and%20quantitatively.%20The%20proposed%20approach%0Aprovides%20a%20practical%20and%20effective%20solution%20for%20artifact%20reduction%20in%20RT%20cine%0AMRI%20without%20requiring%20acquisition%20modifications%2C%20offering%20a%20pathway%20to%20higher%0Aacceleration%20rates%20while%20preserving%20diagnostic%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00643v1&entry.124074799=Read"},
{"title": "Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques,\n  Applications and Outlook", "author": "Muyi Bao and Shuchang Lyu and Zhaoyang Xu and Huiyu Zhou and Jinchang Ren and Shiming Xiang and Xiangtai Li and Guangliang Cheng", "abstract": "  Deep learning has profoundly transformed remote sensing, yet prevailing\narchitectures like Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited\nreceptive fields, while ViTs grapple with quadratic computational complexity,\nhindering their scalability for high-resolution remote sensing data. State\nSpace Models (SSMs), particularly the recently proposed Mamba architecture,\nhave emerged as a paradigm-shifting solution, combining linear computational\nscaling with global context modeling. This survey presents a comprehensive\nreview of Mamba-based methodologies in remote sensing, systematically analyzing\nabout 120 studies to construct a holistic taxonomy of innovations and\napplications. Our contributions are structured across five dimensions: (i)\nfoundational principles of vision Mamba architectures, (ii) micro-architectural\nadvancements such as adaptive scan strategies and hybrid SSM formulations,\n(iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids\nand frequency-domain adaptations, (iv) rigorous benchmarking against\nstate-of-the-art methods in multiple application tasks, such as object\ndetection, semantic segmentation, change detection, etc. and (v) critical\nanalysis of unresolved challenges with actionable future directions. By\nbridging the gap between SSM theory and remote sensing practice, this survey\nestablishes Mamba as a transformative framework for remote sensing analysis. To\nour knowledge, this paper is the first systematic review of Mamba architectures\nin remote sensing. Our work provides a structured foundation for advancing\nresearch in remote sensing systems through SSM-based methods. We curate an\nopen-source repository\n(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster\ncommunity-driven advancements.\n", "link": "http://arxiv.org/abs/2505.00630v1", "date": "2025-05-01", "relevancy": 2.1643, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Mamba%20in%20Remote%20Sensing%3A%20A%20Comprehensive%20Survey%20of%20Techniques%2C%0A%20%20Applications%20and%20Outlook&body=Title%3A%20Vision%20Mamba%20in%20Remote%20Sensing%3A%20A%20Comprehensive%20Survey%20of%20Techniques%2C%0A%20%20Applications%20and%20Outlook%0AAuthor%3A%20Muyi%20Bao%20and%20Shuchang%20Lyu%20and%20Zhaoyang%20Xu%20and%20Huiyu%20Zhou%20and%20Jinchang%20Ren%20and%20Shiming%20Xiang%20and%20Xiangtai%20Li%20and%20Guangliang%20Cheng%0AAbstract%3A%20%20%20Deep%20learning%20has%20profoundly%20transformed%20remote%20sensing%2C%20yet%20prevailing%0Aarchitectures%20like%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%0A%28ViTs%29%20remain%20constrained%20by%20critical%20trade-offs%3A%20CNNs%20suffer%20from%20limited%0Areceptive%20fields%2C%20while%20ViTs%20grapple%20with%20quadratic%20computational%20complexity%2C%0Ahindering%20their%20scalability%20for%20high-resolution%20remote%20sensing%20data.%20State%0ASpace%20Models%20%28SSMs%29%2C%20particularly%20the%20recently%20proposed%20Mamba%20architecture%2C%0Ahave%20emerged%20as%20a%20paradigm-shifting%20solution%2C%20combining%20linear%20computational%0Ascaling%20with%20global%20context%20modeling.%20This%20survey%20presents%20a%20comprehensive%0Areview%20of%20Mamba-based%20methodologies%20in%20remote%20sensing%2C%20systematically%20analyzing%0Aabout%20120%20studies%20to%20construct%20a%20holistic%20taxonomy%20of%20innovations%20and%0Aapplications.%20Our%20contributions%20are%20structured%20across%20five%20dimensions%3A%20%28i%29%0Afoundational%20principles%20of%20vision%20Mamba%20architectures%2C%20%28ii%29%20micro-architectural%0Aadvancements%20such%20as%20adaptive%20scan%20strategies%20and%20hybrid%20SSM%20formulations%2C%0A%28iii%29%20macro-architectural%20integrations%2C%20including%20CNN-Transformer-Mamba%20hybrids%0Aand%20frequency-domain%20adaptations%2C%20%28iv%29%20rigorous%20benchmarking%20against%0Astate-of-the-art%20methods%20in%20multiple%20application%20tasks%2C%20such%20as%20object%0Adetection%2C%20semantic%20segmentation%2C%20change%20detection%2C%20etc.%20and%20%28v%29%20critical%0Aanalysis%20of%20unresolved%20challenges%20with%20actionable%20future%20directions.%20By%0Abridging%20the%20gap%20between%20SSM%20theory%20and%20remote%20sensing%20practice%2C%20this%20survey%0Aestablishes%20Mamba%20as%20a%20transformative%20framework%20for%20remote%20sensing%20analysis.%20To%0Aour%20knowledge%2C%20this%20paper%20is%20the%20first%20systematic%20review%20of%20Mamba%20architectures%0Ain%20remote%20sensing.%20Our%20work%20provides%20a%20structured%20foundation%20for%20advancing%0Aresearch%20in%20remote%20sensing%20systems%20through%20SSM-based%20methods.%20We%20curate%20an%0Aopen-source%20repository%0A%28https%3A//github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing%29%20to%20foster%0Acommunity-driven%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Mamba%2520in%2520Remote%2520Sensing%253A%2520A%2520Comprehensive%2520Survey%2520of%2520Techniques%252C%250A%2520%2520Applications%2520and%2520Outlook%26entry.906535625%3DMuyi%2520Bao%2520and%2520Shuchang%2520Lyu%2520and%2520Zhaoyang%2520Xu%2520and%2520Huiyu%2520Zhou%2520and%2520Jinchang%2520Ren%2520and%2520Shiming%2520Xiang%2520and%2520Xiangtai%2520Li%2520and%2520Guangliang%2520Cheng%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520profoundly%2520transformed%2520remote%2520sensing%252C%2520yet%2520prevailing%250Aarchitectures%2520like%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Vision%2520Transformers%250A%2528ViTs%2529%2520remain%2520constrained%2520by%2520critical%2520trade-offs%253A%2520CNNs%2520suffer%2520from%2520limited%250Areceptive%2520fields%252C%2520while%2520ViTs%2520grapple%2520with%2520quadratic%2520computational%2520complexity%252C%250Ahindering%2520their%2520scalability%2520for%2520high-resolution%2520remote%2520sensing%2520data.%2520State%250ASpace%2520Models%2520%2528SSMs%2529%252C%2520particularly%2520the%2520recently%2520proposed%2520Mamba%2520architecture%252C%250Ahave%2520emerged%2520as%2520a%2520paradigm-shifting%2520solution%252C%2520combining%2520linear%2520computational%250Ascaling%2520with%2520global%2520context%2520modeling.%2520This%2520survey%2520presents%2520a%2520comprehensive%250Areview%2520of%2520Mamba-based%2520methodologies%2520in%2520remote%2520sensing%252C%2520systematically%2520analyzing%250Aabout%2520120%2520studies%2520to%2520construct%2520a%2520holistic%2520taxonomy%2520of%2520innovations%2520and%250Aapplications.%2520Our%2520contributions%2520are%2520structured%2520across%2520five%2520dimensions%253A%2520%2528i%2529%250Afoundational%2520principles%2520of%2520vision%2520Mamba%2520architectures%252C%2520%2528ii%2529%2520micro-architectural%250Aadvancements%2520such%2520as%2520adaptive%2520scan%2520strategies%2520and%2520hybrid%2520SSM%2520formulations%252C%250A%2528iii%2529%2520macro-architectural%2520integrations%252C%2520including%2520CNN-Transformer-Mamba%2520hybrids%250Aand%2520frequency-domain%2520adaptations%252C%2520%2528iv%2529%2520rigorous%2520benchmarking%2520against%250Astate-of-the-art%2520methods%2520in%2520multiple%2520application%2520tasks%252C%2520such%2520as%2520object%250Adetection%252C%2520semantic%2520segmentation%252C%2520change%2520detection%252C%2520etc.%2520and%2520%2528v%2529%2520critical%250Aanalysis%2520of%2520unresolved%2520challenges%2520with%2520actionable%2520future%2520directions.%2520By%250Abridging%2520the%2520gap%2520between%2520SSM%2520theory%2520and%2520remote%2520sensing%2520practice%252C%2520this%2520survey%250Aestablishes%2520Mamba%2520as%2520a%2520transformative%2520framework%2520for%2520remote%2520sensing%2520analysis.%2520To%250Aour%2520knowledge%252C%2520this%2520paper%2520is%2520the%2520first%2520systematic%2520review%2520of%2520Mamba%2520architectures%250Ain%2520remote%2520sensing.%2520Our%2520work%2520provides%2520a%2520structured%2520foundation%2520for%2520advancing%250Aresearch%2520in%2520remote%2520sensing%2520systems%2520through%2520SSM-based%2520methods.%2520We%2520curate%2520an%250Aopen-source%2520repository%250A%2528https%253A//github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing%2529%2520to%2520foster%250Acommunity-driven%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Mamba%20in%20Remote%20Sensing%3A%20A%20Comprehensive%20Survey%20of%20Techniques%2C%0A%20%20Applications%20and%20Outlook&entry.906535625=Muyi%20Bao%20and%20Shuchang%20Lyu%20and%20Zhaoyang%20Xu%20and%20Huiyu%20Zhou%20and%20Jinchang%20Ren%20and%20Shiming%20Xiang%20and%20Xiangtai%20Li%20and%20Guangliang%20Cheng&entry.1292438233=%20%20Deep%20learning%20has%20profoundly%20transformed%20remote%20sensing%2C%20yet%20prevailing%0Aarchitectures%20like%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%0A%28ViTs%29%20remain%20constrained%20by%20critical%20trade-offs%3A%20CNNs%20suffer%20from%20limited%0Areceptive%20fields%2C%20while%20ViTs%20grapple%20with%20quadratic%20computational%20complexity%2C%0Ahindering%20their%20scalability%20for%20high-resolution%20remote%20sensing%20data.%20State%0ASpace%20Models%20%28SSMs%29%2C%20particularly%20the%20recently%20proposed%20Mamba%20architecture%2C%0Ahave%20emerged%20as%20a%20paradigm-shifting%20solution%2C%20combining%20linear%20computational%0Ascaling%20with%20global%20context%20modeling.%20This%20survey%20presents%20a%20comprehensive%0Areview%20of%20Mamba-based%20methodologies%20in%20remote%20sensing%2C%20systematically%20analyzing%0Aabout%20120%20studies%20to%20construct%20a%20holistic%20taxonomy%20of%20innovations%20and%0Aapplications.%20Our%20contributions%20are%20structured%20across%20five%20dimensions%3A%20%28i%29%0Afoundational%20principles%20of%20vision%20Mamba%20architectures%2C%20%28ii%29%20micro-architectural%0Aadvancements%20such%20as%20adaptive%20scan%20strategies%20and%20hybrid%20SSM%20formulations%2C%0A%28iii%29%20macro-architectural%20integrations%2C%20including%20CNN-Transformer-Mamba%20hybrids%0Aand%20frequency-domain%20adaptations%2C%20%28iv%29%20rigorous%20benchmarking%20against%0Astate-of-the-art%20methods%20in%20multiple%20application%20tasks%2C%20such%20as%20object%0Adetection%2C%20semantic%20segmentation%2C%20change%20detection%2C%20etc.%20and%20%28v%29%20critical%0Aanalysis%20of%20unresolved%20challenges%20with%20actionable%20future%20directions.%20By%0Abridging%20the%20gap%20between%20SSM%20theory%20and%20remote%20sensing%20practice%2C%20this%20survey%0Aestablishes%20Mamba%20as%20a%20transformative%20framework%20for%20remote%20sensing%20analysis.%20To%0Aour%20knowledge%2C%20this%20paper%20is%20the%20first%20systematic%20review%20of%20Mamba%20architectures%0Ain%20remote%20sensing.%20Our%20work%20provides%20a%20structured%20foundation%20for%20advancing%0Aresearch%20in%20remote%20sensing%20systems%20through%20SSM-based%20methods.%20We%20curate%20an%0Aopen-source%20repository%0A%28https%3A//github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing%29%20to%20foster%0Acommunity-driven%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00630v1&entry.124074799=Read"},
{"title": "X-ray illicit object detection using hybrid CNN-transformer neural\n  network architectures", "author": "Jorgen Cani and Christos Diou and Spyridon Evangelatos and Panagiotis Radoglou-Grammatikis and Vasileios Argyriou and Panagiotis Sarigiannidis and Iraklis Varlamis and Georgios Th. Papadopoulos", "abstract": "  In the field of X-ray security applications, even the smallest details can\nsignificantly impact outcomes. Objects that are heavily occluded or\nintentionally concealed pose a great challenge for detection, whether by human\nobservation or through advanced technological applications. While certain Deep\nLearning (DL) architectures demonstrate strong performance in processing local\ninformation, such as Convolutional Neural Networks (CNNs), others excel in\nhandling distant information, e.g., transformers. In X-ray security imaging the\nliterature has been dominated by the use of CNN-based methods, while the\nintegration of the two aforementioned leading architectures has not been\nsufficiently explored. In this paper, various hybrid CNN-transformer\narchitectures are evaluated against a common CNN object detection baseline,\nnamely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer\n(Next-ViT-S) backbone are combined with different CNN/transformer detection\nheads (YOLOv8 and RT-DETR). The resulting architectures are comparatively\nevaluated on three challenging public X-ray inspection datasets, namely EDS,\nHiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default\nbackbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray\nand PIDray datasets, when a domain distribution shift is incorporated in the\nX-ray images (as happens in the EDS datasets), hybrid CNN-transformer\narchitectures exhibit increased robustness. Detailed comparative evaluation\nresults, including object-level detection performance and object-size error\nanalysis, demonstrate the strengths and weaknesses of each architectural\ncombination and suggest guidelines for future research. The source code and\nnetwork weights of the models employed in this study are available at\nhttps://github.com/jgenc/xray-comparative-evaluation.\n", "link": "http://arxiv.org/abs/2505.00564v1", "date": "2025-05-01", "relevancy": 2.1482, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5412}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5399}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-ray%20illicit%20object%20detection%20using%20hybrid%20CNN-transformer%20neural%0A%20%20network%20architectures&body=Title%3A%20X-ray%20illicit%20object%20detection%20using%20hybrid%20CNN-transformer%20neural%0A%20%20network%20architectures%0AAuthor%3A%20Jorgen%20Cani%20and%20Christos%20Diou%20and%20Spyridon%20Evangelatos%20and%20Panagiotis%20Radoglou-Grammatikis%20and%20Vasileios%20Argyriou%20and%20Panagiotis%20Sarigiannidis%20and%20Iraklis%20Varlamis%20and%20Georgios%20Th.%20Papadopoulos%0AAbstract%3A%20%20%20In%20the%20field%20of%20X-ray%20security%20applications%2C%20even%20the%20smallest%20details%20can%0Asignificantly%20impact%20outcomes.%20Objects%20that%20are%20heavily%20occluded%20or%0Aintentionally%20concealed%20pose%20a%20great%20challenge%20for%20detection%2C%20whether%20by%20human%0Aobservation%20or%20through%20advanced%20technological%20applications.%20While%20certain%20Deep%0ALearning%20%28DL%29%20architectures%20demonstrate%20strong%20performance%20in%20processing%20local%0Ainformation%2C%20such%20as%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20others%20excel%20in%0Ahandling%20distant%20information%2C%20e.g.%2C%20transformers.%20In%20X-ray%20security%20imaging%20the%0Aliterature%20has%20been%20dominated%20by%20the%20use%20of%20CNN-based%20methods%2C%20while%20the%0Aintegration%20of%20the%20two%20aforementioned%20leading%20architectures%20has%20not%20been%0Asufficiently%20explored.%20In%20this%20paper%2C%20various%20hybrid%20CNN-transformer%0Aarchitectures%20are%20evaluated%20against%20a%20common%20CNN%20object%20detection%20baseline%2C%0Anamely%20YOLOv8.%20In%20particular%2C%20a%20CNN%20%28HGNetV2%29%20and%20a%20hybrid%20CNN-transformer%0A%28Next-ViT-S%29%20backbone%20are%20combined%20with%20different%20CNN/transformer%20detection%0Aheads%20%28YOLOv8%20and%20RT-DETR%29.%20The%20resulting%20architectures%20are%20comparatively%0Aevaluated%20on%20three%20challenging%20public%20X-ray%20inspection%20datasets%2C%20namely%20EDS%2C%0AHiXray%2C%20and%20PIDray.%20Interestingly%2C%20while%20the%20YOLOv8%20detector%20with%20its%20default%0Abackbone%20%28CSP-DarkNet53%29%20is%20generally%20shown%20to%20be%20advantageous%20on%20the%20HiXray%0Aand%20PIDray%20datasets%2C%20when%20a%20domain%20distribution%20shift%20is%20incorporated%20in%20the%0AX-ray%20images%20%28as%20happens%20in%20the%20EDS%20datasets%29%2C%20hybrid%20CNN-transformer%0Aarchitectures%20exhibit%20increased%20robustness.%20Detailed%20comparative%20evaluation%0Aresults%2C%20including%20object-level%20detection%20performance%20and%20object-size%20error%0Aanalysis%2C%20demonstrate%20the%20strengths%20and%20weaknesses%20of%20each%20architectural%0Acombination%20and%20suggest%20guidelines%20for%20future%20research.%20The%20source%20code%20and%0Anetwork%20weights%20of%20the%20models%20employed%20in%20this%20study%20are%20available%20at%0Ahttps%3A//github.com/jgenc/xray-comparative-evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-ray%2520illicit%2520object%2520detection%2520using%2520hybrid%2520CNN-transformer%2520neural%250A%2520%2520network%2520architectures%26entry.906535625%3DJorgen%2520Cani%2520and%2520Christos%2520Diou%2520and%2520Spyridon%2520Evangelatos%2520and%2520Panagiotis%2520Radoglou-Grammatikis%2520and%2520Vasileios%2520Argyriou%2520and%2520Panagiotis%2520Sarigiannidis%2520and%2520Iraklis%2520Varlamis%2520and%2520Georgios%2520Th.%2520Papadopoulos%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520X-ray%2520security%2520applications%252C%2520even%2520the%2520smallest%2520details%2520can%250Asignificantly%2520impact%2520outcomes.%2520Objects%2520that%2520are%2520heavily%2520occluded%2520or%250Aintentionally%2520concealed%2520pose%2520a%2520great%2520challenge%2520for%2520detection%252C%2520whether%2520by%2520human%250Aobservation%2520or%2520through%2520advanced%2520technological%2520applications.%2520While%2520certain%2520Deep%250ALearning%2520%2528DL%2529%2520architectures%2520demonstrate%2520strong%2520performance%2520in%2520processing%2520local%250Ainformation%252C%2520such%2520as%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520others%2520excel%2520in%250Ahandling%2520distant%2520information%252C%2520e.g.%252C%2520transformers.%2520In%2520X-ray%2520security%2520imaging%2520the%250Aliterature%2520has%2520been%2520dominated%2520by%2520the%2520use%2520of%2520CNN-based%2520methods%252C%2520while%2520the%250Aintegration%2520of%2520the%2520two%2520aforementioned%2520leading%2520architectures%2520has%2520not%2520been%250Asufficiently%2520explored.%2520In%2520this%2520paper%252C%2520various%2520hybrid%2520CNN-transformer%250Aarchitectures%2520are%2520evaluated%2520against%2520a%2520common%2520CNN%2520object%2520detection%2520baseline%252C%250Anamely%2520YOLOv8.%2520In%2520particular%252C%2520a%2520CNN%2520%2528HGNetV2%2529%2520and%2520a%2520hybrid%2520CNN-transformer%250A%2528Next-ViT-S%2529%2520backbone%2520are%2520combined%2520with%2520different%2520CNN/transformer%2520detection%250Aheads%2520%2528YOLOv8%2520and%2520RT-DETR%2529.%2520The%2520resulting%2520architectures%2520are%2520comparatively%250Aevaluated%2520on%2520three%2520challenging%2520public%2520X-ray%2520inspection%2520datasets%252C%2520namely%2520EDS%252C%250AHiXray%252C%2520and%2520PIDray.%2520Interestingly%252C%2520while%2520the%2520YOLOv8%2520detector%2520with%2520its%2520default%250Abackbone%2520%2528CSP-DarkNet53%2529%2520is%2520generally%2520shown%2520to%2520be%2520advantageous%2520on%2520the%2520HiXray%250Aand%2520PIDray%2520datasets%252C%2520when%2520a%2520domain%2520distribution%2520shift%2520is%2520incorporated%2520in%2520the%250AX-ray%2520images%2520%2528as%2520happens%2520in%2520the%2520EDS%2520datasets%2529%252C%2520hybrid%2520CNN-transformer%250Aarchitectures%2520exhibit%2520increased%2520robustness.%2520Detailed%2520comparative%2520evaluation%250Aresults%252C%2520including%2520object-level%2520detection%2520performance%2520and%2520object-size%2520error%250Aanalysis%252C%2520demonstrate%2520the%2520strengths%2520and%2520weaknesses%2520of%2520each%2520architectural%250Acombination%2520and%2520suggest%2520guidelines%2520for%2520future%2520research.%2520The%2520source%2520code%2520and%250Anetwork%2520weights%2520of%2520the%2520models%2520employed%2520in%2520this%2520study%2520are%2520available%2520at%250Ahttps%253A//github.com/jgenc/xray-comparative-evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-ray%20illicit%20object%20detection%20using%20hybrid%20CNN-transformer%20neural%0A%20%20network%20architectures&entry.906535625=Jorgen%20Cani%20and%20Christos%20Diou%20and%20Spyridon%20Evangelatos%20and%20Panagiotis%20Radoglou-Grammatikis%20and%20Vasileios%20Argyriou%20and%20Panagiotis%20Sarigiannidis%20and%20Iraklis%20Varlamis%20and%20Georgios%20Th.%20Papadopoulos&entry.1292438233=%20%20In%20the%20field%20of%20X-ray%20security%20applications%2C%20even%20the%20smallest%20details%20can%0Asignificantly%20impact%20outcomes.%20Objects%20that%20are%20heavily%20occluded%20or%0Aintentionally%20concealed%20pose%20a%20great%20challenge%20for%20detection%2C%20whether%20by%20human%0Aobservation%20or%20through%20advanced%20technological%20applications.%20While%20certain%20Deep%0ALearning%20%28DL%29%20architectures%20demonstrate%20strong%20performance%20in%20processing%20local%0Ainformation%2C%20such%20as%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20others%20excel%20in%0Ahandling%20distant%20information%2C%20e.g.%2C%20transformers.%20In%20X-ray%20security%20imaging%20the%0Aliterature%20has%20been%20dominated%20by%20the%20use%20of%20CNN-based%20methods%2C%20while%20the%0Aintegration%20of%20the%20two%20aforementioned%20leading%20architectures%20has%20not%20been%0Asufficiently%20explored.%20In%20this%20paper%2C%20various%20hybrid%20CNN-transformer%0Aarchitectures%20are%20evaluated%20against%20a%20common%20CNN%20object%20detection%20baseline%2C%0Anamely%20YOLOv8.%20In%20particular%2C%20a%20CNN%20%28HGNetV2%29%20and%20a%20hybrid%20CNN-transformer%0A%28Next-ViT-S%29%20backbone%20are%20combined%20with%20different%20CNN/transformer%20detection%0Aheads%20%28YOLOv8%20and%20RT-DETR%29.%20The%20resulting%20architectures%20are%20comparatively%0Aevaluated%20on%20three%20challenging%20public%20X-ray%20inspection%20datasets%2C%20namely%20EDS%2C%0AHiXray%2C%20and%20PIDray.%20Interestingly%2C%20while%20the%20YOLOv8%20detector%20with%20its%20default%0Abackbone%20%28CSP-DarkNet53%29%20is%20generally%20shown%20to%20be%20advantageous%20on%20the%20HiXray%0Aand%20PIDray%20datasets%2C%20when%20a%20domain%20distribution%20shift%20is%20incorporated%20in%20the%0AX-ray%20images%20%28as%20happens%20in%20the%20EDS%20datasets%29%2C%20hybrid%20CNN-transformer%0Aarchitectures%20exhibit%20increased%20robustness.%20Detailed%20comparative%20evaluation%0Aresults%2C%20including%20object-level%20detection%20performance%20and%20object-size%20error%0Aanalysis%2C%20demonstrate%20the%20strengths%20and%20weaknesses%20of%20each%20architectural%0Acombination%20and%20suggest%20guidelines%20for%20future%20research.%20The%20source%20code%20and%0Anetwork%20weights%20of%20the%20models%20employed%20in%20this%20study%20are%20available%20at%0Ahttps%3A//github.com/jgenc/xray-comparative-evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00564v1&entry.124074799=Read"},
{"title": "On the generalization of language models from in-context learning and\n  finetuning: a controlled study", "author": "Andrew K. Lampinen and Arslan Chaudhry and Stephanie C. Y. Chan and Cody Wild and Diane Wan and Alex Ku and J\u00f6rg Bornschein and Razvan Pascanu and Murray Shanahan and James L. McClelland", "abstract": "  Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning -- from failing to\ngeneralize to simple reversals of relations they are trained on, to missing\nlogical deductions that can be made from trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nHowever, language models' in-context learning shows different inductive biases,\nand can generalize better in some of these cases. Here, we explore these\ndifferences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' ability to generalize from finetuning data. The datasets are\nconstructed to isolate the knowledge in the dataset from that in pretraining,\nto create clean tests of generalization. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance.\n", "link": "http://arxiv.org/abs/2505.00661v1", "date": "2025-05-01", "relevancy": 2.1458, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20generalization%20of%20language%20models%20from%20in-context%20learning%20and%0A%20%20finetuning%3A%20a%20controlled%20study&body=Title%3A%20On%20the%20generalization%20of%20language%20models%20from%20in-context%20learning%20and%0A%20%20finetuning%3A%20a%20controlled%20study%0AAuthor%3A%20Andrew%20K.%20Lampinen%20and%20Arslan%20Chaudhry%20and%20Stephanie%20C.%20Y.%20Chan%20and%20Cody%20Wild%20and%20Diane%20Wan%20and%20Alex%20Ku%20and%20J%C3%B6rg%20Bornschein%20and%20Razvan%20Pascanu%20and%20Murray%20Shanahan%20and%20James%20L.%20McClelland%0AAbstract%3A%20%20%20Large%20language%20models%20exhibit%20exciting%20capabilities%2C%20yet%20can%20show%0Asurprisingly%20narrow%20generalization%20from%20finetuning%20--%20from%20failing%20to%0Ageneralize%20to%20simple%20reversals%20of%20relations%20they%20are%20trained%20on%2C%20to%20missing%0Alogical%20deductions%20that%20can%20be%20made%20from%20trained%20information.%20These%20failures%20to%0Ageneralize%20from%20fine-tuning%20can%20hinder%20practical%20application%20of%20these%20models.%0AHowever%2C%20language%20models%27%20in-context%20learning%20shows%20different%20inductive%20biases%2C%0Aand%20can%20generalize%20better%20in%20some%20of%20these%20cases.%20Here%2C%20we%20explore%20these%0Adifferences%20in%20generalization%20between%20in-context-%20and%20fine-tuning-based%0Alearning.%20To%20do%20so%2C%20we%20constructed%20several%20novel%20datasets%20to%20evaluate%20and%0Aimprove%20models%27%20ability%20to%20generalize%20from%20finetuning%20data.%20The%20datasets%20are%0Aconstructed%20to%20isolate%20the%20knowledge%20in%20the%20dataset%20from%20that%20in%20pretraining%2C%0Ato%20create%20clean%20tests%20of%20generalization.%20We%20expose%20pretrained%20large%20models%20to%0Acontrolled%20subsets%20of%20the%20information%20in%20these%20datasets%20--%20either%20in%20context%2C%0Aor%20through%20fine-tuning%20--%20and%20evaluate%20their%20performance%20on%20test%20sets%20that%0Arequire%20various%20types%20of%20generalization.%20We%20find%20overall%20that%20in%20data-matched%0Asettings%2C%20in-context%20learning%20can%20generalize%20more%20flexibly%20than%20fine-tuning%0A%28though%20we%20also%20find%20some%20qualifications%20of%20prior%20findings%2C%20such%20as%20cases%20when%0Afine-tuning%20can%20generalize%20to%20reversals%20embedded%20in%20a%20larger%20structure%20of%0Aknowledge%29.%20We%20build%20on%20these%20findings%20to%20propose%20a%20method%20to%20enable%20improved%0Ageneralization%20from%20fine-tuning%3A%20adding%20in-context%20inferences%20to%20finetuning%0Adata.%20We%20show%20that%20this%20method%20improves%20generalization%20across%20various%20splits%20of%0Aour%20datasets%20and%20other%20benchmarks.%20Our%20results%20have%20implications%20for%0Aunderstanding%20the%20inductive%20biases%20of%20different%20modes%20of%20learning%20in%20language%0Amodels%2C%20and%20practically%20improving%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520generalization%2520of%2520language%2520models%2520from%2520in-context%2520learning%2520and%250A%2520%2520finetuning%253A%2520a%2520controlled%2520study%26entry.906535625%3DAndrew%2520K.%2520Lampinen%2520and%2520Arslan%2520Chaudhry%2520and%2520Stephanie%2520C.%2520Y.%2520Chan%2520and%2520Cody%2520Wild%2520and%2520Diane%2520Wan%2520and%2520Alex%2520Ku%2520and%2520J%25C3%25B6rg%2520Bornschein%2520and%2520Razvan%2520Pascanu%2520and%2520Murray%2520Shanahan%2520and%2520James%2520L.%2520McClelland%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520exhibit%2520exciting%2520capabilities%252C%2520yet%2520can%2520show%250Asurprisingly%2520narrow%2520generalization%2520from%2520finetuning%2520--%2520from%2520failing%2520to%250Ageneralize%2520to%2520simple%2520reversals%2520of%2520relations%2520they%2520are%2520trained%2520on%252C%2520to%2520missing%250Alogical%2520deductions%2520that%2520can%2520be%2520made%2520from%2520trained%2520information.%2520These%2520failures%2520to%250Ageneralize%2520from%2520fine-tuning%2520can%2520hinder%2520practical%2520application%2520of%2520these%2520models.%250AHowever%252C%2520language%2520models%2527%2520in-context%2520learning%2520shows%2520different%2520inductive%2520biases%252C%250Aand%2520can%2520generalize%2520better%2520in%2520some%2520of%2520these%2520cases.%2520Here%252C%2520we%2520explore%2520these%250Adifferences%2520in%2520generalization%2520between%2520in-context-%2520and%2520fine-tuning-based%250Alearning.%2520To%2520do%2520so%252C%2520we%2520constructed%2520several%2520novel%2520datasets%2520to%2520evaluate%2520and%250Aimprove%2520models%2527%2520ability%2520to%2520generalize%2520from%2520finetuning%2520data.%2520The%2520datasets%2520are%250Aconstructed%2520to%2520isolate%2520the%2520knowledge%2520in%2520the%2520dataset%2520from%2520that%2520in%2520pretraining%252C%250Ato%2520create%2520clean%2520tests%2520of%2520generalization.%2520We%2520expose%2520pretrained%2520large%2520models%2520to%250Acontrolled%2520subsets%2520of%2520the%2520information%2520in%2520these%2520datasets%2520--%2520either%2520in%2520context%252C%250Aor%2520through%2520fine-tuning%2520--%2520and%2520evaluate%2520their%2520performance%2520on%2520test%2520sets%2520that%250Arequire%2520various%2520types%2520of%2520generalization.%2520We%2520find%2520overall%2520that%2520in%2520data-matched%250Asettings%252C%2520in-context%2520learning%2520can%2520generalize%2520more%2520flexibly%2520than%2520fine-tuning%250A%2528though%2520we%2520also%2520find%2520some%2520qualifications%2520of%2520prior%2520findings%252C%2520such%2520as%2520cases%2520when%250Afine-tuning%2520can%2520generalize%2520to%2520reversals%2520embedded%2520in%2520a%2520larger%2520structure%2520of%250Aknowledge%2529.%2520We%2520build%2520on%2520these%2520findings%2520to%2520propose%2520a%2520method%2520to%2520enable%2520improved%250Ageneralization%2520from%2520fine-tuning%253A%2520adding%2520in-context%2520inferences%2520to%2520finetuning%250Adata.%2520We%2520show%2520that%2520this%2520method%2520improves%2520generalization%2520across%2520various%2520splits%2520of%250Aour%2520datasets%2520and%2520other%2520benchmarks.%2520Our%2520results%2520have%2520implications%2520for%250Aunderstanding%2520the%2520inductive%2520biases%2520of%2520different%2520modes%2520of%2520learning%2520in%2520language%250Amodels%252C%2520and%2520practically%2520improving%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20generalization%20of%20language%20models%20from%20in-context%20learning%20and%0A%20%20finetuning%3A%20a%20controlled%20study&entry.906535625=Andrew%20K.%20Lampinen%20and%20Arslan%20Chaudhry%20and%20Stephanie%20C.%20Y.%20Chan%20and%20Cody%20Wild%20and%20Diane%20Wan%20and%20Alex%20Ku%20and%20J%C3%B6rg%20Bornschein%20and%20Razvan%20Pascanu%20and%20Murray%20Shanahan%20and%20James%20L.%20McClelland&entry.1292438233=%20%20Large%20language%20models%20exhibit%20exciting%20capabilities%2C%20yet%20can%20show%0Asurprisingly%20narrow%20generalization%20from%20finetuning%20--%20from%20failing%20to%0Ageneralize%20to%20simple%20reversals%20of%20relations%20they%20are%20trained%20on%2C%20to%20missing%0Alogical%20deductions%20that%20can%20be%20made%20from%20trained%20information.%20These%20failures%20to%0Ageneralize%20from%20fine-tuning%20can%20hinder%20practical%20application%20of%20these%20models.%0AHowever%2C%20language%20models%27%20in-context%20learning%20shows%20different%20inductive%20biases%2C%0Aand%20can%20generalize%20better%20in%20some%20of%20these%20cases.%20Here%2C%20we%20explore%20these%0Adifferences%20in%20generalization%20between%20in-context-%20and%20fine-tuning-based%0Alearning.%20To%20do%20so%2C%20we%20constructed%20several%20novel%20datasets%20to%20evaluate%20and%0Aimprove%20models%27%20ability%20to%20generalize%20from%20finetuning%20data.%20The%20datasets%20are%0Aconstructed%20to%20isolate%20the%20knowledge%20in%20the%20dataset%20from%20that%20in%20pretraining%2C%0Ato%20create%20clean%20tests%20of%20generalization.%20We%20expose%20pretrained%20large%20models%20to%0Acontrolled%20subsets%20of%20the%20information%20in%20these%20datasets%20--%20either%20in%20context%2C%0Aor%20through%20fine-tuning%20--%20and%20evaluate%20their%20performance%20on%20test%20sets%20that%0Arequire%20various%20types%20of%20generalization.%20We%20find%20overall%20that%20in%20data-matched%0Asettings%2C%20in-context%20learning%20can%20generalize%20more%20flexibly%20than%20fine-tuning%0A%28though%20we%20also%20find%20some%20qualifications%20of%20prior%20findings%2C%20such%20as%20cases%20when%0Afine-tuning%20can%20generalize%20to%20reversals%20embedded%20in%20a%20larger%20structure%20of%0Aknowledge%29.%20We%20build%20on%20these%20findings%20to%20propose%20a%20method%20to%20enable%20improved%0Ageneralization%20from%20fine-tuning%3A%20adding%20in-context%20inferences%20to%20finetuning%0Adata.%20We%20show%20that%20this%20method%20improves%20generalization%20across%20various%20splits%20of%0Aour%20datasets%20and%20other%20benchmarks.%20Our%20results%20have%20implications%20for%0Aunderstanding%20the%20inductive%20biases%20of%20different%20modes%20of%20learning%20in%20language%0Amodels%2C%20and%20practically%20improving%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00661v1&entry.124074799=Read"},
{"title": "Robot localization aided by quantum algorithms", "author": "Unai Antero and Basilio Sierra and Jon O\u00f1ativia and Alejandra Ruiz and Eneko Osaba", "abstract": "  Localization is a critical aspect of mobile robotics, enabling robots to\nnavigate their environment efficiently and avoid obstacles. Current\nprobabilistic localization methods, such as the Adaptive-Monte Carlo\nlocalization (AMCL) algorithm, are computationally intensive and may struggle\nwith large maps or high-resolution sensor data. This paper explores the\napplication of quantum computing in robotics, focusing on the use of Grover's\nsearch algorithm to improve the efficiency of localization in mobile robots. We\npropose a novel approach to utilize Grover's algorithm in a 2D map, enabling\nfaster and more efficient localization. Despite the limitations of current\nphysical quantum computers, our experimental results demonstrate a significant\nspeedup over classical methods, highlighting the potential of quantum computing\nto improve robotic localization. This work bridges the gap between quantum\ncomputing and robotics, providing a practical solution for robotic localization\nand paving the way for future research in quantum robotics.\n", "link": "http://arxiv.org/abs/2502.00077v2", "date": "2025-05-01", "relevancy": 2.1448, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5466}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5438}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot%20localization%20aided%20by%20quantum%20algorithms&body=Title%3A%20Robot%20localization%20aided%20by%20quantum%20algorithms%0AAuthor%3A%20Unai%20Antero%20and%20Basilio%20Sierra%20and%20Jon%20O%C3%B1ativia%20and%20Alejandra%20Ruiz%20and%20Eneko%20Osaba%0AAbstract%3A%20%20%20Localization%20is%20a%20critical%20aspect%20of%20mobile%20robotics%2C%20enabling%20robots%20to%0Anavigate%20their%20environment%20efficiently%20and%20avoid%20obstacles.%20Current%0Aprobabilistic%20localization%20methods%2C%20such%20as%20the%20Adaptive-Monte%20Carlo%0Alocalization%20%28AMCL%29%20algorithm%2C%20are%20computationally%20intensive%20and%20may%20struggle%0Awith%20large%20maps%20or%20high-resolution%20sensor%20data.%20This%20paper%20explores%20the%0Aapplication%20of%20quantum%20computing%20in%20robotics%2C%20focusing%20on%20the%20use%20of%20Grover%27s%0Asearch%20algorithm%20to%20improve%20the%20efficiency%20of%20localization%20in%20mobile%20robots.%20We%0Apropose%20a%20novel%20approach%20to%20utilize%20Grover%27s%20algorithm%20in%20a%202D%20map%2C%20enabling%0Afaster%20and%20more%20efficient%20localization.%20Despite%20the%20limitations%20of%20current%0Aphysical%20quantum%20computers%2C%20our%20experimental%20results%20demonstrate%20a%20significant%0Aspeedup%20over%20classical%20methods%2C%20highlighting%20the%20potential%20of%20quantum%20computing%0Ato%20improve%20robotic%20localization.%20This%20work%20bridges%20the%20gap%20between%20quantum%0Acomputing%20and%20robotics%2C%20providing%20a%20practical%20solution%20for%20robotic%20localization%0Aand%20paving%20the%20way%20for%20future%20research%20in%20quantum%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot%2520localization%2520aided%2520by%2520quantum%2520algorithms%26entry.906535625%3DUnai%2520Antero%2520and%2520Basilio%2520Sierra%2520and%2520Jon%2520O%25C3%25B1ativia%2520and%2520Alejandra%2520Ruiz%2520and%2520Eneko%2520Osaba%26entry.1292438233%3D%2520%2520Localization%2520is%2520a%2520critical%2520aspect%2520of%2520mobile%2520robotics%252C%2520enabling%2520robots%2520to%250Anavigate%2520their%2520environment%2520efficiently%2520and%2520avoid%2520obstacles.%2520Current%250Aprobabilistic%2520localization%2520methods%252C%2520such%2520as%2520the%2520Adaptive-Monte%2520Carlo%250Alocalization%2520%2528AMCL%2529%2520algorithm%252C%2520are%2520computationally%2520intensive%2520and%2520may%2520struggle%250Awith%2520large%2520maps%2520or%2520high-resolution%2520sensor%2520data.%2520This%2520paper%2520explores%2520the%250Aapplication%2520of%2520quantum%2520computing%2520in%2520robotics%252C%2520focusing%2520on%2520the%2520use%2520of%2520Grover%2527s%250Asearch%2520algorithm%2520to%2520improve%2520the%2520efficiency%2520of%2520localization%2520in%2520mobile%2520robots.%2520We%250Apropose%2520a%2520novel%2520approach%2520to%2520utilize%2520Grover%2527s%2520algorithm%2520in%2520a%25202D%2520map%252C%2520enabling%250Afaster%2520and%2520more%2520efficient%2520localization.%2520Despite%2520the%2520limitations%2520of%2520current%250Aphysical%2520quantum%2520computers%252C%2520our%2520experimental%2520results%2520demonstrate%2520a%2520significant%250Aspeedup%2520over%2520classical%2520methods%252C%2520highlighting%2520the%2520potential%2520of%2520quantum%2520computing%250Ato%2520improve%2520robotic%2520localization.%2520This%2520work%2520bridges%2520the%2520gap%2520between%2520quantum%250Acomputing%2520and%2520robotics%252C%2520providing%2520a%2520practical%2520solution%2520for%2520robotic%2520localization%250Aand%2520paving%2520the%2520way%2520for%2520future%2520research%2520in%2520quantum%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20localization%20aided%20by%20quantum%20algorithms&entry.906535625=Unai%20Antero%20and%20Basilio%20Sierra%20and%20Jon%20O%C3%B1ativia%20and%20Alejandra%20Ruiz%20and%20Eneko%20Osaba&entry.1292438233=%20%20Localization%20is%20a%20critical%20aspect%20of%20mobile%20robotics%2C%20enabling%20robots%20to%0Anavigate%20their%20environment%20efficiently%20and%20avoid%20obstacles.%20Current%0Aprobabilistic%20localization%20methods%2C%20such%20as%20the%20Adaptive-Monte%20Carlo%0Alocalization%20%28AMCL%29%20algorithm%2C%20are%20computationally%20intensive%20and%20may%20struggle%0Awith%20large%20maps%20or%20high-resolution%20sensor%20data.%20This%20paper%20explores%20the%0Aapplication%20of%20quantum%20computing%20in%20robotics%2C%20focusing%20on%20the%20use%20of%20Grover%27s%0Asearch%20algorithm%20to%20improve%20the%20efficiency%20of%20localization%20in%20mobile%20robots.%20We%0Apropose%20a%20novel%20approach%20to%20utilize%20Grover%27s%20algorithm%20in%20a%202D%20map%2C%20enabling%0Afaster%20and%20more%20efficient%20localization.%20Despite%20the%20limitations%20of%20current%0Aphysical%20quantum%20computers%2C%20our%20experimental%20results%20demonstrate%20a%20significant%0Aspeedup%20over%20classical%20methods%2C%20highlighting%20the%20potential%20of%20quantum%20computing%0Ato%20improve%20robotic%20localization.%20This%20work%20bridges%20the%20gap%20between%20quantum%0Acomputing%20and%20robotics%2C%20providing%20a%20practical%20solution%20for%20robotic%20localization%0Aand%20paving%20the%20way%20for%20future%20research%20in%20quantum%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00077v2&entry.124074799=Read"},
{"title": "CombAlign: Enhancing Model Expressiveness in Unsupervised Graph\n  Alignment", "author": "Songyang Chen and Yu Liu and Lei Zou and Zexuan Wang and Youfang Lin", "abstract": "  Unsupervised graph alignment finds the node correspondence between a pair of\nattributed graphs by only exploiting graph structure and node features. One\ncategory of recent studies first computes the node representation and then\nmatches nodes with the largest embedding-based similarity, while the other\ncategory reduces the problem to optimal transport (OT) via Gromov-Wasserstein\nlearning. However, it remains largely unexplored in the model expressiveness,\nas well as how theoretical expressivity impacts prediction accuracy. We\ninvestigate the model expressiveness from two aspects. First, we characterize\nthe model's discriminative power in distinguishing matched and unmatched node\npairs across two graphs.Second, we study the model's capability of guaranteeing\nnode matching properties such as one-to-one matching and mutual alignment.\nMotivated by our theoretical analysis, we put forward a hybrid approach named\nCombAlign with stronger expressive power. Specifically, we enable\ncross-dimensional feature interaction for OT-based learning and propose an\nembedding-based method inspired by the Weisfeiler-Lehman test. We also apply\nnon-uniform marginals obtained from the embedding-based modules to OT as priors\nfor more expressiveness. Based on that, we propose a traditional\nalgorithm-based refinement, which combines our OT and embedding-based\npredictions using the ensemble learning strategy and reduces the problem to\nmaximum weight matching. With carefully designed edge weights, we ensure those\nmatching properties and further enhance prediction accuracy. By extensive\nexperiments, we demonstrate a significant improvement of 14.5% in alignment\naccuracy compared to state-of-the-art approaches and confirm the soundness of\nour theoretical analysis.\n", "link": "http://arxiv.org/abs/2406.13216v2", "date": "2025-05-01", "relevancy": 2.1162, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5457}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5239}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CombAlign%3A%20Enhancing%20Model%20Expressiveness%20in%20Unsupervised%20Graph%0A%20%20Alignment&body=Title%3A%20CombAlign%3A%20Enhancing%20Model%20Expressiveness%20in%20Unsupervised%20Graph%0A%20%20Alignment%0AAuthor%3A%20Songyang%20Chen%20and%20Yu%20Liu%20and%20Lei%20Zou%20and%20Zexuan%20Wang%20and%20Youfang%20Lin%0AAbstract%3A%20%20%20Unsupervised%20graph%20alignment%20finds%20the%20node%20correspondence%20between%20a%20pair%20of%0Aattributed%20graphs%20by%20only%20exploiting%20graph%20structure%20and%20node%20features.%20One%0Acategory%20of%20recent%20studies%20first%20computes%20the%20node%20representation%20and%20then%0Amatches%20nodes%20with%20the%20largest%20embedding-based%20similarity%2C%20while%20the%20other%0Acategory%20reduces%20the%20problem%20to%20optimal%20transport%20%28OT%29%20via%20Gromov-Wasserstein%0Alearning.%20However%2C%20it%20remains%20largely%20unexplored%20in%20the%20model%20expressiveness%2C%0Aas%20well%20as%20how%20theoretical%20expressivity%20impacts%20prediction%20accuracy.%20We%0Ainvestigate%20the%20model%20expressiveness%20from%20two%20aspects.%20First%2C%20we%20characterize%0Athe%20model%27s%20discriminative%20power%20in%20distinguishing%20matched%20and%20unmatched%20node%0Apairs%20across%20two%20graphs.Second%2C%20we%20study%20the%20model%27s%20capability%20of%20guaranteeing%0Anode%20matching%20properties%20such%20as%20one-to-one%20matching%20and%20mutual%20alignment.%0AMotivated%20by%20our%20theoretical%20analysis%2C%20we%20put%20forward%20a%20hybrid%20approach%20named%0ACombAlign%20with%20stronger%20expressive%20power.%20Specifically%2C%20we%20enable%0Across-dimensional%20feature%20interaction%20for%20OT-based%20learning%20and%20propose%20an%0Aembedding-based%20method%20inspired%20by%20the%20Weisfeiler-Lehman%20test.%20We%20also%20apply%0Anon-uniform%20marginals%20obtained%20from%20the%20embedding-based%20modules%20to%20OT%20as%20priors%0Afor%20more%20expressiveness.%20Based%20on%20that%2C%20we%20propose%20a%20traditional%0Aalgorithm-based%20refinement%2C%20which%20combines%20our%20OT%20and%20embedding-based%0Apredictions%20using%20the%20ensemble%20learning%20strategy%20and%20reduces%20the%20problem%20to%0Amaximum%20weight%20matching.%20With%20carefully%20designed%20edge%20weights%2C%20we%20ensure%20those%0Amatching%20properties%20and%20further%20enhance%20prediction%20accuracy.%20By%20extensive%0Aexperiments%2C%20we%20demonstrate%20a%20significant%20improvement%20of%2014.5%25%20in%20alignment%0Aaccuracy%20compared%20to%20state-of-the-art%20approaches%20and%20confirm%20the%20soundness%20of%0Aour%20theoretical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombAlign%253A%2520Enhancing%2520Model%2520Expressiveness%2520in%2520Unsupervised%2520Graph%250A%2520%2520Alignment%26entry.906535625%3DSongyang%2520Chen%2520and%2520Yu%2520Liu%2520and%2520Lei%2520Zou%2520and%2520Zexuan%2520Wang%2520and%2520Youfang%2520Lin%26entry.1292438233%3D%2520%2520Unsupervised%2520graph%2520alignment%2520finds%2520the%2520node%2520correspondence%2520between%2520a%2520pair%2520of%250Aattributed%2520graphs%2520by%2520only%2520exploiting%2520graph%2520structure%2520and%2520node%2520features.%2520One%250Acategory%2520of%2520recent%2520studies%2520first%2520computes%2520the%2520node%2520representation%2520and%2520then%250Amatches%2520nodes%2520with%2520the%2520largest%2520embedding-based%2520similarity%252C%2520while%2520the%2520other%250Acategory%2520reduces%2520the%2520problem%2520to%2520optimal%2520transport%2520%2528OT%2529%2520via%2520Gromov-Wasserstein%250Alearning.%2520However%252C%2520it%2520remains%2520largely%2520unexplored%2520in%2520the%2520model%2520expressiveness%252C%250Aas%2520well%2520as%2520how%2520theoretical%2520expressivity%2520impacts%2520prediction%2520accuracy.%2520We%250Ainvestigate%2520the%2520model%2520expressiveness%2520from%2520two%2520aspects.%2520First%252C%2520we%2520characterize%250Athe%2520model%2527s%2520discriminative%2520power%2520in%2520distinguishing%2520matched%2520and%2520unmatched%2520node%250Apairs%2520across%2520two%2520graphs.Second%252C%2520we%2520study%2520the%2520model%2527s%2520capability%2520of%2520guaranteeing%250Anode%2520matching%2520properties%2520such%2520as%2520one-to-one%2520matching%2520and%2520mutual%2520alignment.%250AMotivated%2520by%2520our%2520theoretical%2520analysis%252C%2520we%2520put%2520forward%2520a%2520hybrid%2520approach%2520named%250ACombAlign%2520with%2520stronger%2520expressive%2520power.%2520Specifically%252C%2520we%2520enable%250Across-dimensional%2520feature%2520interaction%2520for%2520OT-based%2520learning%2520and%2520propose%2520an%250Aembedding-based%2520method%2520inspired%2520by%2520the%2520Weisfeiler-Lehman%2520test.%2520We%2520also%2520apply%250Anon-uniform%2520marginals%2520obtained%2520from%2520the%2520embedding-based%2520modules%2520to%2520OT%2520as%2520priors%250Afor%2520more%2520expressiveness.%2520Based%2520on%2520that%252C%2520we%2520propose%2520a%2520traditional%250Aalgorithm-based%2520refinement%252C%2520which%2520combines%2520our%2520OT%2520and%2520embedding-based%250Apredictions%2520using%2520the%2520ensemble%2520learning%2520strategy%2520and%2520reduces%2520the%2520problem%2520to%250Amaximum%2520weight%2520matching.%2520With%2520carefully%2520designed%2520edge%2520weights%252C%2520we%2520ensure%2520those%250Amatching%2520properties%2520and%2520further%2520enhance%2520prediction%2520accuracy.%2520By%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520a%2520significant%2520improvement%2520of%252014.5%2525%2520in%2520alignment%250Aaccuracy%2520compared%2520to%2520state-of-the-art%2520approaches%2520and%2520confirm%2520the%2520soundness%2520of%250Aour%2520theoretical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CombAlign%3A%20Enhancing%20Model%20Expressiveness%20in%20Unsupervised%20Graph%0A%20%20Alignment&entry.906535625=Songyang%20Chen%20and%20Yu%20Liu%20and%20Lei%20Zou%20and%20Zexuan%20Wang%20and%20Youfang%20Lin&entry.1292438233=%20%20Unsupervised%20graph%20alignment%20finds%20the%20node%20correspondence%20between%20a%20pair%20of%0Aattributed%20graphs%20by%20only%20exploiting%20graph%20structure%20and%20node%20features.%20One%0Acategory%20of%20recent%20studies%20first%20computes%20the%20node%20representation%20and%20then%0Amatches%20nodes%20with%20the%20largest%20embedding-based%20similarity%2C%20while%20the%20other%0Acategory%20reduces%20the%20problem%20to%20optimal%20transport%20%28OT%29%20via%20Gromov-Wasserstein%0Alearning.%20However%2C%20it%20remains%20largely%20unexplored%20in%20the%20model%20expressiveness%2C%0Aas%20well%20as%20how%20theoretical%20expressivity%20impacts%20prediction%20accuracy.%20We%0Ainvestigate%20the%20model%20expressiveness%20from%20two%20aspects.%20First%2C%20we%20characterize%0Athe%20model%27s%20discriminative%20power%20in%20distinguishing%20matched%20and%20unmatched%20node%0Apairs%20across%20two%20graphs.Second%2C%20we%20study%20the%20model%27s%20capability%20of%20guaranteeing%0Anode%20matching%20properties%20such%20as%20one-to-one%20matching%20and%20mutual%20alignment.%0AMotivated%20by%20our%20theoretical%20analysis%2C%20we%20put%20forward%20a%20hybrid%20approach%20named%0ACombAlign%20with%20stronger%20expressive%20power.%20Specifically%2C%20we%20enable%0Across-dimensional%20feature%20interaction%20for%20OT-based%20learning%20and%20propose%20an%0Aembedding-based%20method%20inspired%20by%20the%20Weisfeiler-Lehman%20test.%20We%20also%20apply%0Anon-uniform%20marginals%20obtained%20from%20the%20embedding-based%20modules%20to%20OT%20as%20priors%0Afor%20more%20expressiveness.%20Based%20on%20that%2C%20we%20propose%20a%20traditional%0Aalgorithm-based%20refinement%2C%20which%20combines%20our%20OT%20and%20embedding-based%0Apredictions%20using%20the%20ensemble%20learning%20strategy%20and%20reduces%20the%20problem%20to%0Amaximum%20weight%20matching.%20With%20carefully%20designed%20edge%20weights%2C%20we%20ensure%20those%0Amatching%20properties%20and%20further%20enhance%20prediction%20accuracy.%20By%20extensive%0Aexperiments%2C%20we%20demonstrate%20a%20significant%20improvement%20of%2014.5%25%20in%20alignment%0Aaccuracy%20compared%20to%20state-of-the-art%20approaches%20and%20confirm%20the%20soundness%20of%0Aour%20theoretical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13216v2&entry.124074799=Read"},
{"title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution", "author": "Antoni Bigata and Rodrigo Mira and Stella Bounareli and Micha\u0142 Stypu\u0142kowski and Konstantinos Vougioukas and Stavros Petridis and Maja Pantic", "abstract": "  Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.\n", "link": "http://arxiv.org/abs/2505.00497v1", "date": "2025-05-01", "relevancy": 2.1115, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5332}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5296}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KeySync%3A%20A%20Robust%20Approach%20for%20Leakage-free%20Lip%20Synchronization%20in%20High%0A%20%20Resolution&body=Title%3A%20KeySync%3A%20A%20Robust%20Approach%20for%20Leakage-free%20Lip%20Synchronization%20in%20High%0A%20%20Resolution%0AAuthor%3A%20Antoni%20Bigata%20and%20Rodrigo%20Mira%20and%20Stella%20Bounareli%20and%20Micha%C5%82%20Stypu%C5%82kowski%20and%20Konstantinos%20Vougioukas%20and%20Stavros%20Petridis%20and%20Maja%20Pantic%0AAbstract%3A%20%20%20Lip%20synchronization%2C%20known%20as%20the%20task%20of%20aligning%20lip%20movements%20in%20an%0Aexisting%20video%20with%20new%20input%20audio%2C%20is%20typically%20framed%20as%20a%20simpler%20variant%0Aof%20audio-driven%20facial%20animation.%20However%2C%20as%20well%20as%20suffering%20from%20the%20usual%0Aissues%20in%20talking%20head%20generation%20%28e.g.%2C%20temporal%20consistency%29%2C%20lip%0Asynchronization%20presents%20significant%20new%20challenges%20such%20as%20expression%20leakage%0Afrom%20the%20input%20video%20and%20facial%20occlusions%2C%20which%20can%20severely%20impact%0Areal-world%20applications%20like%20automated%20dubbing%2C%20but%20are%20often%20neglected%20in%0Aexisting%20works.%20To%20address%20these%20shortcomings%2C%20we%20present%20KeySync%2C%20a%20two-stage%0Aframework%20that%20succeeds%20in%20solving%20the%20issue%20of%20temporal%20consistency%2C%20while%0Aalso%20incorporating%20solutions%20for%20leakage%20and%20occlusions%20using%20a%20carefully%0Adesigned%20masking%20strategy.%20We%20show%20that%20KeySync%20achieves%20state-of-the-art%0Aresults%20in%20lip%20reconstruction%20and%20cross-synchronization%2C%20improving%20visual%0Aquality%20and%20reducing%20expression%20leakage%20according%20to%20LipLeak%2C%20our%20novel%20leakage%0Ametric.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20new%20masking%0Aapproach%20in%20handling%20occlusions%20and%20validate%20our%20architectural%20choices%20through%0Aseveral%20ablation%20studies.%20Code%20and%20model%20weights%20can%20be%20found%20at%0Ahttps%3A//antonibigata.github.io/KeySync.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeySync%253A%2520A%2520Robust%2520Approach%2520for%2520Leakage-free%2520Lip%2520Synchronization%2520in%2520High%250A%2520%2520Resolution%26entry.906535625%3DAntoni%2520Bigata%2520and%2520Rodrigo%2520Mira%2520and%2520Stella%2520Bounareli%2520and%2520Micha%25C5%2582%2520Stypu%25C5%2582kowski%2520and%2520Konstantinos%2520Vougioukas%2520and%2520Stavros%2520Petridis%2520and%2520Maja%2520Pantic%26entry.1292438233%3D%2520%2520Lip%2520synchronization%252C%2520known%2520as%2520the%2520task%2520of%2520aligning%2520lip%2520movements%2520in%2520an%250Aexisting%2520video%2520with%2520new%2520input%2520audio%252C%2520is%2520typically%2520framed%2520as%2520a%2520simpler%2520variant%250Aof%2520audio-driven%2520facial%2520animation.%2520However%252C%2520as%2520well%2520as%2520suffering%2520from%2520the%2520usual%250Aissues%2520in%2520talking%2520head%2520generation%2520%2528e.g.%252C%2520temporal%2520consistency%2529%252C%2520lip%250Asynchronization%2520presents%2520significant%2520new%2520challenges%2520such%2520as%2520expression%2520leakage%250Afrom%2520the%2520input%2520video%2520and%2520facial%2520occlusions%252C%2520which%2520can%2520severely%2520impact%250Areal-world%2520applications%2520like%2520automated%2520dubbing%252C%2520but%2520are%2520often%2520neglected%2520in%250Aexisting%2520works.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520present%2520KeySync%252C%2520a%2520two-stage%250Aframework%2520that%2520succeeds%2520in%2520solving%2520the%2520issue%2520of%2520temporal%2520consistency%252C%2520while%250Aalso%2520incorporating%2520solutions%2520for%2520leakage%2520and%2520occlusions%2520using%2520a%2520carefully%250Adesigned%2520masking%2520strategy.%2520We%2520show%2520that%2520KeySync%2520achieves%2520state-of-the-art%250Aresults%2520in%2520lip%2520reconstruction%2520and%2520cross-synchronization%252C%2520improving%2520visual%250Aquality%2520and%2520reducing%2520expression%2520leakage%2520according%2520to%2520LipLeak%252C%2520our%2520novel%2520leakage%250Ametric.%2520Furthermore%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520new%2520masking%250Aapproach%2520in%2520handling%2520occlusions%2520and%2520validate%2520our%2520architectural%2520choices%2520through%250Aseveral%2520ablation%2520studies.%2520Code%2520and%2520model%2520weights%2520can%2520be%2520found%2520at%250Ahttps%253A//antonibigata.github.io/KeySync.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KeySync%3A%20A%20Robust%20Approach%20for%20Leakage-free%20Lip%20Synchronization%20in%20High%0A%20%20Resolution&entry.906535625=Antoni%20Bigata%20and%20Rodrigo%20Mira%20and%20Stella%20Bounareli%20and%20Micha%C5%82%20Stypu%C5%82kowski%20and%20Konstantinos%20Vougioukas%20and%20Stavros%20Petridis%20and%20Maja%20Pantic&entry.1292438233=%20%20Lip%20synchronization%2C%20known%20as%20the%20task%20of%20aligning%20lip%20movements%20in%20an%0Aexisting%20video%20with%20new%20input%20audio%2C%20is%20typically%20framed%20as%20a%20simpler%20variant%0Aof%20audio-driven%20facial%20animation.%20However%2C%20as%20well%20as%20suffering%20from%20the%20usual%0Aissues%20in%20talking%20head%20generation%20%28e.g.%2C%20temporal%20consistency%29%2C%20lip%0Asynchronization%20presents%20significant%20new%20challenges%20such%20as%20expression%20leakage%0Afrom%20the%20input%20video%20and%20facial%20occlusions%2C%20which%20can%20severely%20impact%0Areal-world%20applications%20like%20automated%20dubbing%2C%20but%20are%20often%20neglected%20in%0Aexisting%20works.%20To%20address%20these%20shortcomings%2C%20we%20present%20KeySync%2C%20a%20two-stage%0Aframework%20that%20succeeds%20in%20solving%20the%20issue%20of%20temporal%20consistency%2C%20while%0Aalso%20incorporating%20solutions%20for%20leakage%20and%20occlusions%20using%20a%20carefully%0Adesigned%20masking%20strategy.%20We%20show%20that%20KeySync%20achieves%20state-of-the-art%0Aresults%20in%20lip%20reconstruction%20and%20cross-synchronization%2C%20improving%20visual%0Aquality%20and%20reducing%20expression%20leakage%20according%20to%20LipLeak%2C%20our%20novel%20leakage%0Ametric.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20new%20masking%0Aapproach%20in%20handling%20occlusions%20and%20validate%20our%20architectural%20choices%20through%0Aseveral%20ablation%20studies.%20Code%20and%20model%20weights%20can%20be%20found%20at%0Ahttps%3A//antonibigata.github.io/KeySync.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00497v1&entry.124074799=Read"},
{"title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs", "author": "Shenao Zhang and Zhihan Liu and Boyi Liu and Yufeng Zhang and Yingxiang Yang and Yongfei Liu and Liyu Chen and Tao Sun and Zhaoran Wang", "abstract": "  Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses, despite having access\nto preference data that includes reward scores from judge models during AI\nfeedback. Striving to maximize the implicit reward gap between the chosen and\nthe slightly inferior rejected responses can cause overfitting and unnecessary\nunlearning of the high-quality rejected responses. The unawareness of the\nreward scores also drives the LLM to indiscriminately favor the low-quality\nchosen responses and fail to generalize to optimal responses that are sparse in\ndata. To overcome these shortcomings, our study introduces reward-conditioned\nLLM policies that discern and learn from the entire spectrum of response\nquality within the dataset, helping extrapolate to more optimal regions. We\npropose an effective yet simple data relabeling method that conditions the\npreference pairs on quality scores to construct a reward-augmented dataset. The\nexperiments across various benchmarks and diverse models demonstrate that our\napproach consistently boosts DPO by a considerable margin. Through\ncomprehensive ablation studies, we demonstrate that our method not only\nmaximizes the utility of preference data but also mitigates the issue of\nunlearning, demonstrating its broad effectiveness beyond mere data expansion.\nOur code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.\n", "link": "http://arxiv.org/abs/2410.08067v4", "date": "2025-05-01", "relevancy": 2.1104, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5278}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward-Augmented%20Data%20Enhances%20Direct%20Preference%20Alignment%20of%20LLMs&body=Title%3A%20Reward-Augmented%20Data%20Enhances%20Direct%20Preference%20Alignment%20of%20LLMs%0AAuthor%3A%20Shenao%20Zhang%20and%20Zhihan%20Liu%20and%20Boyi%20Liu%20and%20Yufeng%20Zhang%20and%20Yingxiang%20Yang%20and%20Yongfei%20Liu%20and%20Liyu%20Chen%20and%20Tao%20Sun%20and%20Zhaoran%20Wang%0AAbstract%3A%20%20%20Preference%20alignment%20in%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Aimproved%20their%20ability%20to%20adhere%20to%20human%20instructions%20and%20intentions.%20However%2C%0Aexisting%20direct%20alignment%20algorithms%20primarily%20focus%20on%20relative%20preferences%0Aand%20often%20overlook%20the%20qualitative%20aspects%20of%20responses%2C%20despite%20having%20access%0Ato%20preference%20data%20that%20includes%20reward%20scores%20from%20judge%20models%20during%20AI%0Afeedback.%20Striving%20to%20maximize%20the%20implicit%20reward%20gap%20between%20the%20chosen%20and%0Athe%20slightly%20inferior%20rejected%20responses%20can%20cause%20overfitting%20and%20unnecessary%0Aunlearning%20of%20the%20high-quality%20rejected%20responses.%20The%20unawareness%20of%20the%0Areward%20scores%20also%20drives%20the%20LLM%20to%20indiscriminately%20favor%20the%20low-quality%0Achosen%20responses%20and%20fail%20to%20generalize%20to%20optimal%20responses%20that%20are%20sparse%20in%0Adata.%20To%20overcome%20these%20shortcomings%2C%20our%20study%20introduces%20reward-conditioned%0ALLM%20policies%20that%20discern%20and%20learn%20from%20the%20entire%20spectrum%20of%20response%0Aquality%20within%20the%20dataset%2C%20helping%20extrapolate%20to%20more%20optimal%20regions.%20We%0Apropose%20an%20effective%20yet%20simple%20data%20relabeling%20method%20that%20conditions%20the%0Apreference%20pairs%20on%20quality%20scores%20to%20construct%20a%20reward-augmented%20dataset.%20The%0Aexperiments%20across%20various%20benchmarks%20and%20diverse%20models%20demonstrate%20that%20our%0Aapproach%20consistently%20boosts%20DPO%20by%20a%20considerable%20margin.%20Through%0Acomprehensive%20ablation%20studies%2C%20we%20demonstrate%20that%20our%20method%20not%20only%0Amaximizes%20the%20utility%20of%20preference%20data%20but%20also%20mitigates%20the%20issue%20of%0Aunlearning%2C%20demonstrating%20its%20broad%20effectiveness%20beyond%20mere%20data%20expansion.%0AOur%20code%20is%20available%20at%0Ahttps%3A//github.com/shenao-zhang/reward-augmented-preference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08067v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward-Augmented%2520Data%2520Enhances%2520Direct%2520Preference%2520Alignment%2520of%2520LLMs%26entry.906535625%3DShenao%2520Zhang%2520and%2520Zhihan%2520Liu%2520and%2520Boyi%2520Liu%2520and%2520Yufeng%2520Zhang%2520and%2520Yingxiang%2520Yang%2520and%2520Yongfei%2520Liu%2520and%2520Liyu%2520Chen%2520and%2520Tao%2520Sun%2520and%2520Zhaoran%2520Wang%26entry.1292438233%3D%2520%2520Preference%2520alignment%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%250Aimproved%2520their%2520ability%2520to%2520adhere%2520to%2520human%2520instructions%2520and%2520intentions.%2520However%252C%250Aexisting%2520direct%2520alignment%2520algorithms%2520primarily%2520focus%2520on%2520relative%2520preferences%250Aand%2520often%2520overlook%2520the%2520qualitative%2520aspects%2520of%2520responses%252C%2520despite%2520having%2520access%250Ato%2520preference%2520data%2520that%2520includes%2520reward%2520scores%2520from%2520judge%2520models%2520during%2520AI%250Afeedback.%2520Striving%2520to%2520maximize%2520the%2520implicit%2520reward%2520gap%2520between%2520the%2520chosen%2520and%250Athe%2520slightly%2520inferior%2520rejected%2520responses%2520can%2520cause%2520overfitting%2520and%2520unnecessary%250Aunlearning%2520of%2520the%2520high-quality%2520rejected%2520responses.%2520The%2520unawareness%2520of%2520the%250Areward%2520scores%2520also%2520drives%2520the%2520LLM%2520to%2520indiscriminately%2520favor%2520the%2520low-quality%250Achosen%2520responses%2520and%2520fail%2520to%2520generalize%2520to%2520optimal%2520responses%2520that%2520are%2520sparse%2520in%250Adata.%2520To%2520overcome%2520these%2520shortcomings%252C%2520our%2520study%2520introduces%2520reward-conditioned%250ALLM%2520policies%2520that%2520discern%2520and%2520learn%2520from%2520the%2520entire%2520spectrum%2520of%2520response%250Aquality%2520within%2520the%2520dataset%252C%2520helping%2520extrapolate%2520to%2520more%2520optimal%2520regions.%2520We%250Apropose%2520an%2520effective%2520yet%2520simple%2520data%2520relabeling%2520method%2520that%2520conditions%2520the%250Apreference%2520pairs%2520on%2520quality%2520scores%2520to%2520construct%2520a%2520reward-augmented%2520dataset.%2520The%250Aexperiments%2520across%2520various%2520benchmarks%2520and%2520diverse%2520models%2520demonstrate%2520that%2520our%250Aapproach%2520consistently%2520boosts%2520DPO%2520by%2520a%2520considerable%2520margin.%2520Through%250Acomprehensive%2520ablation%2520studies%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520not%2520only%250Amaximizes%2520the%2520utility%2520of%2520preference%2520data%2520but%2520also%2520mitigates%2520the%2520issue%2520of%250Aunlearning%252C%2520demonstrating%2520its%2520broad%2520effectiveness%2520beyond%2520mere%2520data%2520expansion.%250AOur%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shenao-zhang/reward-augmented-preference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08067v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward-Augmented%20Data%20Enhances%20Direct%20Preference%20Alignment%20of%20LLMs&entry.906535625=Shenao%20Zhang%20and%20Zhihan%20Liu%20and%20Boyi%20Liu%20and%20Yufeng%20Zhang%20and%20Yingxiang%20Yang%20and%20Yongfei%20Liu%20and%20Liyu%20Chen%20and%20Tao%20Sun%20and%20Zhaoran%20Wang&entry.1292438233=%20%20Preference%20alignment%20in%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Aimproved%20their%20ability%20to%20adhere%20to%20human%20instructions%20and%20intentions.%20However%2C%0Aexisting%20direct%20alignment%20algorithms%20primarily%20focus%20on%20relative%20preferences%0Aand%20often%20overlook%20the%20qualitative%20aspects%20of%20responses%2C%20despite%20having%20access%0Ato%20preference%20data%20that%20includes%20reward%20scores%20from%20judge%20models%20during%20AI%0Afeedback.%20Striving%20to%20maximize%20the%20implicit%20reward%20gap%20between%20the%20chosen%20and%0Athe%20slightly%20inferior%20rejected%20responses%20can%20cause%20overfitting%20and%20unnecessary%0Aunlearning%20of%20the%20high-quality%20rejected%20responses.%20The%20unawareness%20of%20the%0Areward%20scores%20also%20drives%20the%20LLM%20to%20indiscriminately%20favor%20the%20low-quality%0Achosen%20responses%20and%20fail%20to%20generalize%20to%20optimal%20responses%20that%20are%20sparse%20in%0Adata.%20To%20overcome%20these%20shortcomings%2C%20our%20study%20introduces%20reward-conditioned%0ALLM%20policies%20that%20discern%20and%20learn%20from%20the%20entire%20spectrum%20of%20response%0Aquality%20within%20the%20dataset%2C%20helping%20extrapolate%20to%20more%20optimal%20regions.%20We%0Apropose%20an%20effective%20yet%20simple%20data%20relabeling%20method%20that%20conditions%20the%0Apreference%20pairs%20on%20quality%20scores%20to%20construct%20a%20reward-augmented%20dataset.%20The%0Aexperiments%20across%20various%20benchmarks%20and%20diverse%20models%20demonstrate%20that%20our%0Aapproach%20consistently%20boosts%20DPO%20by%20a%20considerable%20margin.%20Through%0Acomprehensive%20ablation%20studies%2C%20we%20demonstrate%20that%20our%20method%20not%20only%0Amaximizes%20the%20utility%20of%20preference%20data%20but%20also%20mitigates%20the%20issue%20of%0Aunlearning%2C%20demonstrating%20its%20broad%20effectiveness%20beyond%20mere%20data%20expansion.%0AOur%20code%20is%20available%20at%0Ahttps%3A//github.com/shenao-zhang/reward-augmented-preference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08067v4&entry.124074799=Read"},
{"title": "CLR-Wire: Towards Continuous Latent Representations for 3D Curve\n  Wireframe Generation", "author": "Xueqi Ma and Yilin Liu and Tianlong Gao and Qirui Huang and Hui Huang", "abstract": "  We introduce CLR-Wire, a novel framework for 3D curve-based wireframe\ngeneration that integrates geometry and topology into a unified Continuous\nLatent Representation. Unlike conventional methods that decouple vertices,\nedges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along\nwith their topological connectivity into a continuous and fixed-length latent\nspace using an attention-driven variational autoencoder (VAE). This unified\napproach facilitates joint learning and generation of both geometry and\ntopology. To generate wireframes, we employ a flow matching model to\nprogressively map Gaussian noise to these latents, which are subsequently\ndecoded into complete 3D wireframes. Our method provides fine-grained modeling\nof complex shapes and irregular topologies, and supports both unconditional\ngeneration and generation conditioned on point cloud or image inputs.\nExperimental results demonstrate that, compared with state-of-the-art\ngenerative approaches, our method achieves substantial improvements in\naccuracy, novelty, and diversity, offering an efficient and comprehensive\nsolution for CAD design, geometric reconstruction, and 3D content creation.\n", "link": "http://arxiv.org/abs/2504.19174v2", "date": "2025-05-01", "relevancy": 2.1035, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5269}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5269}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLR-Wire%3A%20Towards%20Continuous%20Latent%20Representations%20for%203D%20Curve%0A%20%20Wireframe%20Generation&body=Title%3A%20CLR-Wire%3A%20Towards%20Continuous%20Latent%20Representations%20for%203D%20Curve%0A%20%20Wireframe%20Generation%0AAuthor%3A%20Xueqi%20Ma%20and%20Yilin%20Liu%20and%20Tianlong%20Gao%20and%20Qirui%20Huang%20and%20Hui%20Huang%0AAbstract%3A%20%20%20We%20introduce%20CLR-Wire%2C%20a%20novel%20framework%20for%203D%20curve-based%20wireframe%0Ageneration%20that%20integrates%20geometry%20and%20topology%20into%20a%20unified%20Continuous%0ALatent%20Representation.%20Unlike%20conventional%20methods%20that%20decouple%20vertices%2C%0Aedges%2C%20and%20faces%2C%20CLR-Wire%20encodes%20curves%20as%20Neural%20Parametric%20Curves%20along%0Awith%20their%20topological%20connectivity%20into%20a%20continuous%20and%20fixed-length%20latent%0Aspace%20using%20an%20attention-driven%20variational%20autoencoder%20%28VAE%29.%20This%20unified%0Aapproach%20facilitates%20joint%20learning%20and%20generation%20of%20both%20geometry%20and%0Atopology.%20To%20generate%20wireframes%2C%20we%20employ%20a%20flow%20matching%20model%20to%0Aprogressively%20map%20Gaussian%20noise%20to%20these%20latents%2C%20which%20are%20subsequently%0Adecoded%20into%20complete%203D%20wireframes.%20Our%20method%20provides%20fine-grained%20modeling%0Aof%20complex%20shapes%20and%20irregular%20topologies%2C%20and%20supports%20both%20unconditional%0Ageneration%20and%20generation%20conditioned%20on%20point%20cloud%20or%20image%20inputs.%0AExperimental%20results%20demonstrate%20that%2C%20compared%20with%20state-of-the-art%0Agenerative%20approaches%2C%20our%20method%20achieves%20substantial%20improvements%20in%0Aaccuracy%2C%20novelty%2C%20and%20diversity%2C%20offering%20an%20efficient%20and%20comprehensive%0Asolution%20for%20CAD%20design%2C%20geometric%20reconstruction%2C%20and%203D%20content%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLR-Wire%253A%2520Towards%2520Continuous%2520Latent%2520Representations%2520for%25203D%2520Curve%250A%2520%2520Wireframe%2520Generation%26entry.906535625%3DXueqi%2520Ma%2520and%2520Yilin%2520Liu%2520and%2520Tianlong%2520Gao%2520and%2520Qirui%2520Huang%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520CLR-Wire%252C%2520a%2520novel%2520framework%2520for%25203D%2520curve-based%2520wireframe%250Ageneration%2520that%2520integrates%2520geometry%2520and%2520topology%2520into%2520a%2520unified%2520Continuous%250ALatent%2520Representation.%2520Unlike%2520conventional%2520methods%2520that%2520decouple%2520vertices%252C%250Aedges%252C%2520and%2520faces%252C%2520CLR-Wire%2520encodes%2520curves%2520as%2520Neural%2520Parametric%2520Curves%2520along%250Awith%2520their%2520topological%2520connectivity%2520into%2520a%2520continuous%2520and%2520fixed-length%2520latent%250Aspace%2520using%2520an%2520attention-driven%2520variational%2520autoencoder%2520%2528VAE%2529.%2520This%2520unified%250Aapproach%2520facilitates%2520joint%2520learning%2520and%2520generation%2520of%2520both%2520geometry%2520and%250Atopology.%2520To%2520generate%2520wireframes%252C%2520we%2520employ%2520a%2520flow%2520matching%2520model%2520to%250Aprogressively%2520map%2520Gaussian%2520noise%2520to%2520these%2520latents%252C%2520which%2520are%2520subsequently%250Adecoded%2520into%2520complete%25203D%2520wireframes.%2520Our%2520method%2520provides%2520fine-grained%2520modeling%250Aof%2520complex%2520shapes%2520and%2520irregular%2520topologies%252C%2520and%2520supports%2520both%2520unconditional%250Ageneration%2520and%2520generation%2520conditioned%2520on%2520point%2520cloud%2520or%2520image%2520inputs.%250AExperimental%2520results%2520demonstrate%2520that%252C%2520compared%2520with%2520state-of-the-art%250Agenerative%2520approaches%252C%2520our%2520method%2520achieves%2520substantial%2520improvements%2520in%250Aaccuracy%252C%2520novelty%252C%2520and%2520diversity%252C%2520offering%2520an%2520efficient%2520and%2520comprehensive%250Asolution%2520for%2520CAD%2520design%252C%2520geometric%2520reconstruction%252C%2520and%25203D%2520content%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLR-Wire%3A%20Towards%20Continuous%20Latent%20Representations%20for%203D%20Curve%0A%20%20Wireframe%20Generation&entry.906535625=Xueqi%20Ma%20and%20Yilin%20Liu%20and%20Tianlong%20Gao%20and%20Qirui%20Huang%20and%20Hui%20Huang&entry.1292438233=%20%20We%20introduce%20CLR-Wire%2C%20a%20novel%20framework%20for%203D%20curve-based%20wireframe%0Ageneration%20that%20integrates%20geometry%20and%20topology%20into%20a%20unified%20Continuous%0ALatent%20Representation.%20Unlike%20conventional%20methods%20that%20decouple%20vertices%2C%0Aedges%2C%20and%20faces%2C%20CLR-Wire%20encodes%20curves%20as%20Neural%20Parametric%20Curves%20along%0Awith%20their%20topological%20connectivity%20into%20a%20continuous%20and%20fixed-length%20latent%0Aspace%20using%20an%20attention-driven%20variational%20autoencoder%20%28VAE%29.%20This%20unified%0Aapproach%20facilitates%20joint%20learning%20and%20generation%20of%20both%20geometry%20and%0Atopology.%20To%20generate%20wireframes%2C%20we%20employ%20a%20flow%20matching%20model%20to%0Aprogressively%20map%20Gaussian%20noise%20to%20these%20latents%2C%20which%20are%20subsequently%0Adecoded%20into%20complete%203D%20wireframes.%20Our%20method%20provides%20fine-grained%20modeling%0Aof%20complex%20shapes%20and%20irregular%20topologies%2C%20and%20supports%20both%20unconditional%0Ageneration%20and%20generation%20conditioned%20on%20point%20cloud%20or%20image%20inputs.%0AExperimental%20results%20demonstrate%20that%2C%20compared%20with%20state-of-the-art%0Agenerative%20approaches%2C%20our%20method%20achieves%20substantial%20improvements%20in%0Aaccuracy%2C%20novelty%2C%20and%20diversity%2C%20offering%20an%20efficient%20and%20comprehensive%0Asolution%20for%20CAD%20design%2C%20geometric%20reconstruction%2C%20and%203D%20content%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19174v2&entry.124074799=Read"},
{"title": "Learning Against Distributional Uncertainty: On the Trade-off Between\n  Robustness and Specificity", "author": "Shixiong Wang and Haowei Wang and Xinke Li and Jean Honorio", "abstract": "  Trustworthy machine learning aims at combating distributional uncertainties\nin training data distributions compared to population distributions. Typical\ntreatment frameworks include the Bayesian approach, (min-max) distributionally\nrobust optimization (DRO), and regularization. However, three issues have to be\nraised: 1) the prior distribution in the Bayesian method and the regularizer in\nthe regularization method are difficult to specify; 2) the DRO method tends to\nbe overly conservative; 3) all the three methods are biased estimators of the\ntrue optimal cost. This paper studies a new framework that unifies the three\napproaches and addresses the three challenges above. The asymptotic properties\n(e.g., consistencies and asymptotic normalities), non-asymptotic properties\n(e.g., generalization bounds and unbiasedness), and solution methods of the\nproposed model are studied. The new model reveals the trade-off between the\nrobustness to the unseen data and the specificity to the training data.\nExperiments on various real-world tasks validate the superiority of the\nproposed learning framework.\n", "link": "http://arxiv.org/abs/2301.13565v2", "date": "2025-05-01", "relevancy": 2.0998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5583}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5261}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Against%20Distributional%20Uncertainty%3A%20On%20the%20Trade-off%20Between%0A%20%20Robustness%20and%20Specificity&body=Title%3A%20Learning%20Against%20Distributional%20Uncertainty%3A%20On%20the%20Trade-off%20Between%0A%20%20Robustness%20and%20Specificity%0AAuthor%3A%20Shixiong%20Wang%20and%20Haowei%20Wang%20and%20Xinke%20Li%20and%20Jean%20Honorio%0AAbstract%3A%20%20%20Trustworthy%20machine%20learning%20aims%20at%20combating%20distributional%20uncertainties%0Ain%20training%20data%20distributions%20compared%20to%20population%20distributions.%20Typical%0Atreatment%20frameworks%20include%20the%20Bayesian%20approach%2C%20%28min-max%29%20distributionally%0Arobust%20optimization%20%28DRO%29%2C%20and%20regularization.%20However%2C%20three%20issues%20have%20to%20be%0Araised%3A%201%29%20the%20prior%20distribution%20in%20the%20Bayesian%20method%20and%20the%20regularizer%20in%0Athe%20regularization%20method%20are%20difficult%20to%20specify%3B%202%29%20the%20DRO%20method%20tends%20to%0Abe%20overly%20conservative%3B%203%29%20all%20the%20three%20methods%20are%20biased%20estimators%20of%20the%0Atrue%20optimal%20cost.%20This%20paper%20studies%20a%20new%20framework%20that%20unifies%20the%20three%0Aapproaches%20and%20addresses%20the%20three%20challenges%20above.%20The%20asymptotic%20properties%0A%28e.g.%2C%20consistencies%20and%20asymptotic%20normalities%29%2C%20non-asymptotic%20properties%0A%28e.g.%2C%20generalization%20bounds%20and%20unbiasedness%29%2C%20and%20solution%20methods%20of%20the%0Aproposed%20model%20are%20studied.%20The%20new%20model%20reveals%20the%20trade-off%20between%20the%0Arobustness%20to%20the%20unseen%20data%20and%20the%20specificity%20to%20the%20training%20data.%0AExperiments%20on%20various%20real-world%20tasks%20validate%20the%20superiority%20of%20the%0Aproposed%20learning%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Against%2520Distributional%2520Uncertainty%253A%2520On%2520the%2520Trade-off%2520Between%250A%2520%2520Robustness%2520and%2520Specificity%26entry.906535625%3DShixiong%2520Wang%2520and%2520Haowei%2520Wang%2520and%2520Xinke%2520Li%2520and%2520Jean%2520Honorio%26entry.1292438233%3D%2520%2520Trustworthy%2520machine%2520learning%2520aims%2520at%2520combating%2520distributional%2520uncertainties%250Ain%2520training%2520data%2520distributions%2520compared%2520to%2520population%2520distributions.%2520Typical%250Atreatment%2520frameworks%2520include%2520the%2520Bayesian%2520approach%252C%2520%2528min-max%2529%2520distributionally%250Arobust%2520optimization%2520%2528DRO%2529%252C%2520and%2520regularization.%2520However%252C%2520three%2520issues%2520have%2520to%2520be%250Araised%253A%25201%2529%2520the%2520prior%2520distribution%2520in%2520the%2520Bayesian%2520method%2520and%2520the%2520regularizer%2520in%250Athe%2520regularization%2520method%2520are%2520difficult%2520to%2520specify%253B%25202%2529%2520the%2520DRO%2520method%2520tends%2520to%250Abe%2520overly%2520conservative%253B%25203%2529%2520all%2520the%2520three%2520methods%2520are%2520biased%2520estimators%2520of%2520the%250Atrue%2520optimal%2520cost.%2520This%2520paper%2520studies%2520a%2520new%2520framework%2520that%2520unifies%2520the%2520three%250Aapproaches%2520and%2520addresses%2520the%2520three%2520challenges%2520above.%2520The%2520asymptotic%2520properties%250A%2528e.g.%252C%2520consistencies%2520and%2520asymptotic%2520normalities%2529%252C%2520non-asymptotic%2520properties%250A%2528e.g.%252C%2520generalization%2520bounds%2520and%2520unbiasedness%2529%252C%2520and%2520solution%2520methods%2520of%2520the%250Aproposed%2520model%2520are%2520studied.%2520The%2520new%2520model%2520reveals%2520the%2520trade-off%2520between%2520the%250Arobustness%2520to%2520the%2520unseen%2520data%2520and%2520the%2520specificity%2520to%2520the%2520training%2520data.%250AExperiments%2520on%2520various%2520real-world%2520tasks%2520validate%2520the%2520superiority%2520of%2520the%250Aproposed%2520learning%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.13565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Against%20Distributional%20Uncertainty%3A%20On%20the%20Trade-off%20Between%0A%20%20Robustness%20and%20Specificity&entry.906535625=Shixiong%20Wang%20and%20Haowei%20Wang%20and%20Xinke%20Li%20and%20Jean%20Honorio&entry.1292438233=%20%20Trustworthy%20machine%20learning%20aims%20at%20combating%20distributional%20uncertainties%0Ain%20training%20data%20distributions%20compared%20to%20population%20distributions.%20Typical%0Atreatment%20frameworks%20include%20the%20Bayesian%20approach%2C%20%28min-max%29%20distributionally%0Arobust%20optimization%20%28DRO%29%2C%20and%20regularization.%20However%2C%20three%20issues%20have%20to%20be%0Araised%3A%201%29%20the%20prior%20distribution%20in%20the%20Bayesian%20method%20and%20the%20regularizer%20in%0Athe%20regularization%20method%20are%20difficult%20to%20specify%3B%202%29%20the%20DRO%20method%20tends%20to%0Abe%20overly%20conservative%3B%203%29%20all%20the%20three%20methods%20are%20biased%20estimators%20of%20the%0Atrue%20optimal%20cost.%20This%20paper%20studies%20a%20new%20framework%20that%20unifies%20the%20three%0Aapproaches%20and%20addresses%20the%20three%20challenges%20above.%20The%20asymptotic%20properties%0A%28e.g.%2C%20consistencies%20and%20asymptotic%20normalities%29%2C%20non-asymptotic%20properties%0A%28e.g.%2C%20generalization%20bounds%20and%20unbiasedness%29%2C%20and%20solution%20methods%20of%20the%0Aproposed%20model%20are%20studied.%20The%20new%20model%20reveals%20the%20trade-off%20between%20the%0Arobustness%20to%20the%20unseen%20data%20and%20the%20specificity%20to%20the%20training%20data.%0AExperiments%20on%20various%20real-world%20tasks%20validate%20the%20superiority%20of%20the%0Aproposed%20learning%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13565v2&entry.124074799=Read"},
{"title": "Interpretability-Aware Vision Transformer", "author": "Yao Qiang and Chengyin Li and Prashant Khanduri and Dongxiao Zhu", "abstract": "  Vision Transformers (ViTs) have become prominent models for solving various\nvision tasks. However, the interpretability of ViTs has not kept pace with\ntheir promising performance. While there has been a surge of interest in\ndeveloping {\\it post hoc} solutions to explain ViTs' outputs, these methods do\nnot generalize to different downstream tasks and various transformer\narchitectures. Furthermore, if ViTs are not properly trained with the given\ndata and do not prioritize the region of interest, the {\\it post hoc} methods\nwould be less effective. Instead of developing another {\\it post hoc} approach,\nwe introduce a novel training procedure that inherently enhances model\ninterpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration\nfrom a fresh insight: both the class patch and image patches consistently\ngenerate predicted distributions and attention maps. IA-ViT is composed of a\nfeature extractor, a predictor, and an interpreter, which are trained jointly\nwith an interpretability-aware training objective. Consequently, the\ninterpreter simulates the behavior of the predictor and provides a faithful\nexplanation through its single-head self-attention mechanism. Our comprehensive\nexperimental results demonstrate the effectiveness of IA-ViT in several image\nclassification tasks, with both qualitative and quantitative evaluations of\nmodel performance and interpretability. Source code is available from:\nhttps://github.com/qiangyao1988/IA-ViT.\n", "link": "http://arxiv.org/abs/2309.08035v2", "date": "2025-05-01", "relevancy": 2.0922, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.538}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5267}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretability-Aware%20Vision%20Transformer&body=Title%3A%20Interpretability-Aware%20Vision%20Transformer%0AAuthor%3A%20Yao%20Qiang%20and%20Chengyin%20Li%20and%20Prashant%20Khanduri%20and%20Dongxiao%20Zhu%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20become%20prominent%20models%20for%20solving%20various%0Avision%20tasks.%20However%2C%20the%20interpretability%20of%20ViTs%20has%20not%20kept%20pace%20with%0Atheir%20promising%20performance.%20While%20there%20has%20been%20a%20surge%20of%20interest%20in%0Adeveloping%20%7B%5Cit%20post%20hoc%7D%20solutions%20to%20explain%20ViTs%27%20outputs%2C%20these%20methods%20do%0Anot%20generalize%20to%20different%20downstream%20tasks%20and%20various%20transformer%0Aarchitectures.%20Furthermore%2C%20if%20ViTs%20are%20not%20properly%20trained%20with%20the%20given%0Adata%20and%20do%20not%20prioritize%20the%20region%20of%20interest%2C%20the%20%7B%5Cit%20post%20hoc%7D%20methods%0Awould%20be%20less%20effective.%20Instead%20of%20developing%20another%20%7B%5Cit%20post%20hoc%7D%20approach%2C%0Awe%20introduce%20a%20novel%20training%20procedure%20that%20inherently%20enhances%20model%0Ainterpretability.%20Our%20interpretability-aware%20ViT%20%28IA-ViT%29%20draws%20inspiration%0Afrom%20a%20fresh%20insight%3A%20both%20the%20class%20patch%20and%20image%20patches%20consistently%0Agenerate%20predicted%20distributions%20and%20attention%20maps.%20IA-ViT%20is%20composed%20of%20a%0Afeature%20extractor%2C%20a%20predictor%2C%20and%20an%20interpreter%2C%20which%20are%20trained%20jointly%0Awith%20an%20interpretability-aware%20training%20objective.%20Consequently%2C%20the%0Ainterpreter%20simulates%20the%20behavior%20of%20the%20predictor%20and%20provides%20a%20faithful%0Aexplanation%20through%20its%20single-head%20self-attention%20mechanism.%20Our%20comprehensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20IA-ViT%20in%20several%20image%0Aclassification%20tasks%2C%20with%20both%20qualitative%20and%20quantitative%20evaluations%20of%0Amodel%20performance%20and%20interpretability.%20Source%20code%20is%20available%20from%3A%0Ahttps%3A//github.com/qiangyao1988/IA-ViT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretability-Aware%2520Vision%2520Transformer%26entry.906535625%3DYao%2520Qiang%2520and%2520Chengyin%2520Li%2520and%2520Prashant%2520Khanduri%2520and%2520Dongxiao%2520Zhu%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520become%2520prominent%2520models%2520for%2520solving%2520various%250Avision%2520tasks.%2520However%252C%2520the%2520interpretability%2520of%2520ViTs%2520has%2520not%2520kept%2520pace%2520with%250Atheir%2520promising%2520performance.%2520While%2520there%2520has%2520been%2520a%2520surge%2520of%2520interest%2520in%250Adeveloping%2520%257B%255Cit%2520post%2520hoc%257D%2520solutions%2520to%2520explain%2520ViTs%2527%2520outputs%252C%2520these%2520methods%2520do%250Anot%2520generalize%2520to%2520different%2520downstream%2520tasks%2520and%2520various%2520transformer%250Aarchitectures.%2520Furthermore%252C%2520if%2520ViTs%2520are%2520not%2520properly%2520trained%2520with%2520the%2520given%250Adata%2520and%2520do%2520not%2520prioritize%2520the%2520region%2520of%2520interest%252C%2520the%2520%257B%255Cit%2520post%2520hoc%257D%2520methods%250Awould%2520be%2520less%2520effective.%2520Instead%2520of%2520developing%2520another%2520%257B%255Cit%2520post%2520hoc%257D%2520approach%252C%250Awe%2520introduce%2520a%2520novel%2520training%2520procedure%2520that%2520inherently%2520enhances%2520model%250Ainterpretability.%2520Our%2520interpretability-aware%2520ViT%2520%2528IA-ViT%2529%2520draws%2520inspiration%250Afrom%2520a%2520fresh%2520insight%253A%2520both%2520the%2520class%2520patch%2520and%2520image%2520patches%2520consistently%250Agenerate%2520predicted%2520distributions%2520and%2520attention%2520maps.%2520IA-ViT%2520is%2520composed%2520of%2520a%250Afeature%2520extractor%252C%2520a%2520predictor%252C%2520and%2520an%2520interpreter%252C%2520which%2520are%2520trained%2520jointly%250Awith%2520an%2520interpretability-aware%2520training%2520objective.%2520Consequently%252C%2520the%250Ainterpreter%2520simulates%2520the%2520behavior%2520of%2520the%2520predictor%2520and%2520provides%2520a%2520faithful%250Aexplanation%2520through%2520its%2520single-head%2520self-attention%2520mechanism.%2520Our%2520comprehensive%250Aexperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520IA-ViT%2520in%2520several%2520image%250Aclassification%2520tasks%252C%2520with%2520both%2520qualitative%2520and%2520quantitative%2520evaluations%2520of%250Amodel%2520performance%2520and%2520interpretability.%2520Source%2520code%2520is%2520available%2520from%253A%250Ahttps%253A//github.com/qiangyao1988/IA-ViT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretability-Aware%20Vision%20Transformer&entry.906535625=Yao%20Qiang%20and%20Chengyin%20Li%20and%20Prashant%20Khanduri%20and%20Dongxiao%20Zhu&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20become%20prominent%20models%20for%20solving%20various%0Avision%20tasks.%20However%2C%20the%20interpretability%20of%20ViTs%20has%20not%20kept%20pace%20with%0Atheir%20promising%20performance.%20While%20there%20has%20been%20a%20surge%20of%20interest%20in%0Adeveloping%20%7B%5Cit%20post%20hoc%7D%20solutions%20to%20explain%20ViTs%27%20outputs%2C%20these%20methods%20do%0Anot%20generalize%20to%20different%20downstream%20tasks%20and%20various%20transformer%0Aarchitectures.%20Furthermore%2C%20if%20ViTs%20are%20not%20properly%20trained%20with%20the%20given%0Adata%20and%20do%20not%20prioritize%20the%20region%20of%20interest%2C%20the%20%7B%5Cit%20post%20hoc%7D%20methods%0Awould%20be%20less%20effective.%20Instead%20of%20developing%20another%20%7B%5Cit%20post%20hoc%7D%20approach%2C%0Awe%20introduce%20a%20novel%20training%20procedure%20that%20inherently%20enhances%20model%0Ainterpretability.%20Our%20interpretability-aware%20ViT%20%28IA-ViT%29%20draws%20inspiration%0Afrom%20a%20fresh%20insight%3A%20both%20the%20class%20patch%20and%20image%20patches%20consistently%0Agenerate%20predicted%20distributions%20and%20attention%20maps.%20IA-ViT%20is%20composed%20of%20a%0Afeature%20extractor%2C%20a%20predictor%2C%20and%20an%20interpreter%2C%20which%20are%20trained%20jointly%0Awith%20an%20interpretability-aware%20training%20objective.%20Consequently%2C%20the%0Ainterpreter%20simulates%20the%20behavior%20of%20the%20predictor%20and%20provides%20a%20faithful%0Aexplanation%20through%20its%20single-head%20self-attention%20mechanism.%20Our%20comprehensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20IA-ViT%20in%20several%20image%0Aclassification%20tasks%2C%20with%20both%20qualitative%20and%20quantitative%20evaluations%20of%0Amodel%20performance%20and%20interpretability.%20Source%20code%20is%20available%20from%3A%0Ahttps%3A//github.com/qiangyao1988/IA-ViT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08035v2&entry.124074799=Read"},
{"title": "Disentangle Before Anonymize: A Two-stage Framework for\n  Attribute-preserved and Occlusion-robust De-identification", "author": "Mingrui Zhu and Dongxin Chen and Xin Wei and Nannan Wang and Xinbo Gao", "abstract": "  In an era where personal photos are easily leaked and collected, face\nde-identification is a crucial method for protecting identity privacy. However,\ncurrent face de-identification techniques face challenges in preserving\nattribute details and often produce anonymized results with reduced\nauthenticity. These shortcomings are particularly evident when handling\nocclusions,frequently resulting in noticeable editing artifacts. Our primary\nfinding in this work is that simultaneous training of identity disentanglement\nand anonymization hinders their respective effectiveness.Therefore, we propose\n\"Disentangle Before Anonymize\",a novel two-stage Framework(DBAF)designed for\nattributepreserved and occlusion-robust de-identification. This framework\nincludes a Contrastive Identity Disentanglement (CID) module and a\nKey-authorized Reversible Identity Anonymization (KRIA) module, achieving\nfaithful attribute preservation and high-quality identity anonymization edits.\nAdditionally, we introduce a Multiscale Attentional Attribute Retention (MAAR)\nmodule to address the issue of reduced anonymization quality under\nocclusions.Extensive experiments demonstrate that our method outperforms\nstate-of-the-art de-identification approaches, delivering superior quality,\nenhanced detail fidelity, improved attribute preservation performance, and\ngreater robustness to occlusions.\n", "link": "http://arxiv.org/abs/2311.08786v2", "date": "2025-05-01", "relevancy": 2.0898, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangle%20Before%20Anonymize%3A%20A%20Two-stage%20Framework%20for%0A%20%20Attribute-preserved%20and%20Occlusion-robust%20De-identification&body=Title%3A%20Disentangle%20Before%20Anonymize%3A%20A%20Two-stage%20Framework%20for%0A%20%20Attribute-preserved%20and%20Occlusion-robust%20De-identification%0AAuthor%3A%20Mingrui%20Zhu%20and%20Dongxin%20Chen%20and%20Xin%20Wei%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20In%20an%20era%20where%20personal%20photos%20are%20easily%20leaked%20and%20collected%2C%20face%0Ade-identification%20is%20a%20crucial%20method%20for%20protecting%20identity%20privacy.%20However%2C%0Acurrent%20face%20de-identification%20techniques%20face%20challenges%20in%20preserving%0Aattribute%20details%20and%20often%20produce%20anonymized%20results%20with%20reduced%0Aauthenticity.%20These%20shortcomings%20are%20particularly%20evident%20when%20handling%0Aocclusions%2Cfrequently%20resulting%20in%20noticeable%20editing%20artifacts.%20Our%20primary%0Afinding%20in%20this%20work%20is%20that%20simultaneous%20training%20of%20identity%20disentanglement%0Aand%20anonymization%20hinders%20their%20respective%20effectiveness.Therefore%2C%20we%20propose%0A%22Disentangle%20Before%20Anonymize%22%2Ca%20novel%20two-stage%20Framework%28DBAF%29designed%20for%0Aattributepreserved%20and%20occlusion-robust%20de-identification.%20This%20framework%0Aincludes%20a%20Contrastive%20Identity%20Disentanglement%20%28CID%29%20module%20and%20a%0AKey-authorized%20Reversible%20Identity%20Anonymization%20%28KRIA%29%20module%2C%20achieving%0Afaithful%20attribute%20preservation%20and%20high-quality%20identity%20anonymization%20edits.%0AAdditionally%2C%20we%20introduce%20a%20Multiscale%20Attentional%20Attribute%20Retention%20%28MAAR%29%0Amodule%20to%20address%20the%20issue%20of%20reduced%20anonymization%20quality%20under%0Aocclusions.Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Astate-of-the-art%20de-identification%20approaches%2C%20delivering%20superior%20quality%2C%0Aenhanced%20detail%20fidelity%2C%20improved%20attribute%20preservation%20performance%2C%20and%0Agreater%20robustness%20to%20occlusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangle%2520Before%2520Anonymize%253A%2520A%2520Two-stage%2520Framework%2520for%250A%2520%2520Attribute-preserved%2520and%2520Occlusion-robust%2520De-identification%26entry.906535625%3DMingrui%2520Zhu%2520and%2520Dongxin%2520Chen%2520and%2520Xin%2520Wei%2520and%2520Nannan%2520Wang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520where%2520personal%2520photos%2520are%2520easily%2520leaked%2520and%2520collected%252C%2520face%250Ade-identification%2520is%2520a%2520crucial%2520method%2520for%2520protecting%2520identity%2520privacy.%2520However%252C%250Acurrent%2520face%2520de-identification%2520techniques%2520face%2520challenges%2520in%2520preserving%250Aattribute%2520details%2520and%2520often%2520produce%2520anonymized%2520results%2520with%2520reduced%250Aauthenticity.%2520These%2520shortcomings%2520are%2520particularly%2520evident%2520when%2520handling%250Aocclusions%252Cfrequently%2520resulting%2520in%2520noticeable%2520editing%2520artifacts.%2520Our%2520primary%250Afinding%2520in%2520this%2520work%2520is%2520that%2520simultaneous%2520training%2520of%2520identity%2520disentanglement%250Aand%2520anonymization%2520hinders%2520their%2520respective%2520effectiveness.Therefore%252C%2520we%2520propose%250A%2522Disentangle%2520Before%2520Anonymize%2522%252Ca%2520novel%2520two-stage%2520Framework%2528DBAF%2529designed%2520for%250Aattributepreserved%2520and%2520occlusion-robust%2520de-identification.%2520This%2520framework%250Aincludes%2520a%2520Contrastive%2520Identity%2520Disentanglement%2520%2528CID%2529%2520module%2520and%2520a%250AKey-authorized%2520Reversible%2520Identity%2520Anonymization%2520%2528KRIA%2529%2520module%252C%2520achieving%250Afaithful%2520attribute%2520preservation%2520and%2520high-quality%2520identity%2520anonymization%2520edits.%250AAdditionally%252C%2520we%2520introduce%2520a%2520Multiscale%2520Attentional%2520Attribute%2520Retention%2520%2528MAAR%2529%250Amodule%2520to%2520address%2520the%2520issue%2520of%2520reduced%2520anonymization%2520quality%2520under%250Aocclusions.Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Astate-of-the-art%2520de-identification%2520approaches%252C%2520delivering%2520superior%2520quality%252C%250Aenhanced%2520detail%2520fidelity%252C%2520improved%2520attribute%2520preservation%2520performance%252C%2520and%250Agreater%2520robustness%2520to%2520occlusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangle%20Before%20Anonymize%3A%20A%20Two-stage%20Framework%20for%0A%20%20Attribute-preserved%20and%20Occlusion-robust%20De-identification&entry.906535625=Mingrui%20Zhu%20and%20Dongxin%20Chen%20and%20Xin%20Wei%20and%20Nannan%20Wang%20and%20Xinbo%20Gao&entry.1292438233=%20%20In%20an%20era%20where%20personal%20photos%20are%20easily%20leaked%20and%20collected%2C%20face%0Ade-identification%20is%20a%20crucial%20method%20for%20protecting%20identity%20privacy.%20However%2C%0Acurrent%20face%20de-identification%20techniques%20face%20challenges%20in%20preserving%0Aattribute%20details%20and%20often%20produce%20anonymized%20results%20with%20reduced%0Aauthenticity.%20These%20shortcomings%20are%20particularly%20evident%20when%20handling%0Aocclusions%2Cfrequently%20resulting%20in%20noticeable%20editing%20artifacts.%20Our%20primary%0Afinding%20in%20this%20work%20is%20that%20simultaneous%20training%20of%20identity%20disentanglement%0Aand%20anonymization%20hinders%20their%20respective%20effectiveness.Therefore%2C%20we%20propose%0A%22Disentangle%20Before%20Anonymize%22%2Ca%20novel%20two-stage%20Framework%28DBAF%29designed%20for%0Aattributepreserved%20and%20occlusion-robust%20de-identification.%20This%20framework%0Aincludes%20a%20Contrastive%20Identity%20Disentanglement%20%28CID%29%20module%20and%20a%0AKey-authorized%20Reversible%20Identity%20Anonymization%20%28KRIA%29%20module%2C%20achieving%0Afaithful%20attribute%20preservation%20and%20high-quality%20identity%20anonymization%20edits.%0AAdditionally%2C%20we%20introduce%20a%20Multiscale%20Attentional%20Attribute%20Retention%20%28MAAR%29%0Amodule%20to%20address%20the%20issue%20of%20reduced%20anonymization%20quality%20under%0Aocclusions.Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Astate-of-the-art%20de-identification%20approaches%2C%20delivering%20superior%20quality%2C%0Aenhanced%20detail%20fidelity%2C%20improved%20attribute%20preservation%20performance%2C%20and%0Agreater%20robustness%20to%20occlusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08786v2&entry.124074799=Read"},
{"title": "Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced\n  Disease Grading", "author": "Shuo Tong and Shangde Gao and Ke Liu and Zihang Huang and Hongxia Xu and Haochao Ying and Jian Wu", "abstract": "  Automatic disease image grading is a significant application of artificial\nintelligence for healthcare, enabling faster and more accurate patient\nassessments. However, domain shifts, which are exacerbated by data imbalance,\nintroduce bias into the model, posing deployment difficulties in clinical\napplications. To address the problem, we propose a novel\n\\textbf{U}ncertainty-aware \\textbf{M}ulti-experts \\textbf{K}nowledge\n\\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple\nexpert models to a single student model. Specifically, to extract\ndiscriminative features, UMKD decouples task-agnostic and task-specific\nfeatures with shallow and compact feature alignment in the feature space. At\nthe output space, an uncertainty-aware decoupled distillation (UDD) mechanism\ndynamically adjusts knowledge transfer weights based on expert model\nuncertainties, ensuring robust and reliable distillation. Additionally, UMKD\nalso tackles the problems of model architecture heterogeneity and distribution\ndiscrepancies between source and target domains, which are inadequately tackled\nby previous KD approaches. Extensive experiments on histology prostate grading\n(\\textit{SICAPv2}) and fundus image grading (\\textit{APTOS}) demonstrate that\nUMKD achieves a new state-of-the-art in both source-imbalanced and\ntarget-imbalanced scenarios, offering a robust and practical solution for\nreal-world disease image grading.\n", "link": "http://arxiv.org/abs/2505.00592v1", "date": "2025-05-01", "relevancy": 2.0861, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6115}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5108}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Multi-Expert%20Knowledge%20Distillation%20for%20Imbalanced%0A%20%20Disease%20Grading&body=Title%3A%20Uncertainty-Aware%20Multi-Expert%20Knowledge%20Distillation%20for%20Imbalanced%0A%20%20Disease%20Grading%0AAuthor%3A%20Shuo%20Tong%20and%20Shangde%20Gao%20and%20Ke%20Liu%20and%20Zihang%20Huang%20and%20Hongxia%20Xu%20and%20Haochao%20Ying%20and%20Jian%20Wu%0AAbstract%3A%20%20%20Automatic%20disease%20image%20grading%20is%20a%20significant%20application%20of%20artificial%0Aintelligence%20for%20healthcare%2C%20enabling%20faster%20and%20more%20accurate%20patient%0Aassessments.%20However%2C%20domain%20shifts%2C%20which%20are%20exacerbated%20by%20data%20imbalance%2C%0Aintroduce%20bias%20into%20the%20model%2C%20posing%20deployment%20difficulties%20in%20clinical%0Aapplications.%20To%20address%20the%20problem%2C%20we%20propose%20a%20novel%0A%5Ctextbf%7BU%7Dncertainty-aware%20%5Ctextbf%7BM%7Dulti-experts%20%5Ctextbf%7BK%7Dnowledge%0A%5Ctextbf%7BD%7Distillation%20%28UMKD%29%20framework%20to%20transfer%20knowledge%20from%20multiple%0Aexpert%20models%20to%20a%20single%20student%20model.%20Specifically%2C%20to%20extract%0Adiscriminative%20features%2C%20UMKD%20decouples%20task-agnostic%20and%20task-specific%0Afeatures%20with%20shallow%20and%20compact%20feature%20alignment%20in%20the%20feature%20space.%20At%0Athe%20output%20space%2C%20an%20uncertainty-aware%20decoupled%20distillation%20%28UDD%29%20mechanism%0Adynamically%20adjusts%20knowledge%20transfer%20weights%20based%20on%20expert%20model%0Auncertainties%2C%20ensuring%20robust%20and%20reliable%20distillation.%20Additionally%2C%20UMKD%0Aalso%20tackles%20the%20problems%20of%20model%20architecture%20heterogeneity%20and%20distribution%0Adiscrepancies%20between%20source%20and%20target%20domains%2C%20which%20are%20inadequately%20tackled%0Aby%20previous%20KD%20approaches.%20Extensive%20experiments%20on%20histology%20prostate%20grading%0A%28%5Ctextit%7BSICAPv2%7D%29%20and%20fundus%20image%20grading%20%28%5Ctextit%7BAPTOS%7D%29%20demonstrate%20that%0AUMKD%20achieves%20a%20new%20state-of-the-art%20in%20both%20source-imbalanced%20and%0Atarget-imbalanced%20scenarios%2C%20offering%20a%20robust%20and%20practical%20solution%20for%0Areal-world%20disease%20image%20grading.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Multi-Expert%2520Knowledge%2520Distillation%2520for%2520Imbalanced%250A%2520%2520Disease%2520Grading%26entry.906535625%3DShuo%2520Tong%2520and%2520Shangde%2520Gao%2520and%2520Ke%2520Liu%2520and%2520Zihang%2520Huang%2520and%2520Hongxia%2520Xu%2520and%2520Haochao%2520Ying%2520and%2520Jian%2520Wu%26entry.1292438233%3D%2520%2520Automatic%2520disease%2520image%2520grading%2520is%2520a%2520significant%2520application%2520of%2520artificial%250Aintelligence%2520for%2520healthcare%252C%2520enabling%2520faster%2520and%2520more%2520accurate%2520patient%250Aassessments.%2520However%252C%2520domain%2520shifts%252C%2520which%2520are%2520exacerbated%2520by%2520data%2520imbalance%252C%250Aintroduce%2520bias%2520into%2520the%2520model%252C%2520posing%2520deployment%2520difficulties%2520in%2520clinical%250Aapplications.%2520To%2520address%2520the%2520problem%252C%2520we%2520propose%2520a%2520novel%250A%255Ctextbf%257BU%257Dncertainty-aware%2520%255Ctextbf%257BM%257Dulti-experts%2520%255Ctextbf%257BK%257Dnowledge%250A%255Ctextbf%257BD%257Distillation%2520%2528UMKD%2529%2520framework%2520to%2520transfer%2520knowledge%2520from%2520multiple%250Aexpert%2520models%2520to%2520a%2520single%2520student%2520model.%2520Specifically%252C%2520to%2520extract%250Adiscriminative%2520features%252C%2520UMKD%2520decouples%2520task-agnostic%2520and%2520task-specific%250Afeatures%2520with%2520shallow%2520and%2520compact%2520feature%2520alignment%2520in%2520the%2520feature%2520space.%2520At%250Athe%2520output%2520space%252C%2520an%2520uncertainty-aware%2520decoupled%2520distillation%2520%2528UDD%2529%2520mechanism%250Adynamically%2520adjusts%2520knowledge%2520transfer%2520weights%2520based%2520on%2520expert%2520model%250Auncertainties%252C%2520ensuring%2520robust%2520and%2520reliable%2520distillation.%2520Additionally%252C%2520UMKD%250Aalso%2520tackles%2520the%2520problems%2520of%2520model%2520architecture%2520heterogeneity%2520and%2520distribution%250Adiscrepancies%2520between%2520source%2520and%2520target%2520domains%252C%2520which%2520are%2520inadequately%2520tackled%250Aby%2520previous%2520KD%2520approaches.%2520Extensive%2520experiments%2520on%2520histology%2520prostate%2520grading%250A%2528%255Ctextit%257BSICAPv2%257D%2529%2520and%2520fundus%2520image%2520grading%2520%2528%255Ctextit%257BAPTOS%257D%2529%2520demonstrate%2520that%250AUMKD%2520achieves%2520a%2520new%2520state-of-the-art%2520in%2520both%2520source-imbalanced%2520and%250Atarget-imbalanced%2520scenarios%252C%2520offering%2520a%2520robust%2520and%2520practical%2520solution%2520for%250Areal-world%2520disease%2520image%2520grading.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Multi-Expert%20Knowledge%20Distillation%20for%20Imbalanced%0A%20%20Disease%20Grading&entry.906535625=Shuo%20Tong%20and%20Shangde%20Gao%20and%20Ke%20Liu%20and%20Zihang%20Huang%20and%20Hongxia%20Xu%20and%20Haochao%20Ying%20and%20Jian%20Wu&entry.1292438233=%20%20Automatic%20disease%20image%20grading%20is%20a%20significant%20application%20of%20artificial%0Aintelligence%20for%20healthcare%2C%20enabling%20faster%20and%20more%20accurate%20patient%0Aassessments.%20However%2C%20domain%20shifts%2C%20which%20are%20exacerbated%20by%20data%20imbalance%2C%0Aintroduce%20bias%20into%20the%20model%2C%20posing%20deployment%20difficulties%20in%20clinical%0Aapplications.%20To%20address%20the%20problem%2C%20we%20propose%20a%20novel%0A%5Ctextbf%7BU%7Dncertainty-aware%20%5Ctextbf%7BM%7Dulti-experts%20%5Ctextbf%7BK%7Dnowledge%0A%5Ctextbf%7BD%7Distillation%20%28UMKD%29%20framework%20to%20transfer%20knowledge%20from%20multiple%0Aexpert%20models%20to%20a%20single%20student%20model.%20Specifically%2C%20to%20extract%0Adiscriminative%20features%2C%20UMKD%20decouples%20task-agnostic%20and%20task-specific%0Afeatures%20with%20shallow%20and%20compact%20feature%20alignment%20in%20the%20feature%20space.%20At%0Athe%20output%20space%2C%20an%20uncertainty-aware%20decoupled%20distillation%20%28UDD%29%20mechanism%0Adynamically%20adjusts%20knowledge%20transfer%20weights%20based%20on%20expert%20model%0Auncertainties%2C%20ensuring%20robust%20and%20reliable%20distillation.%20Additionally%2C%20UMKD%0Aalso%20tackles%20the%20problems%20of%20model%20architecture%20heterogeneity%20and%20distribution%0Adiscrepancies%20between%20source%20and%20target%20domains%2C%20which%20are%20inadequately%20tackled%0Aby%20previous%20KD%20approaches.%20Extensive%20experiments%20on%20histology%20prostate%20grading%0A%28%5Ctextit%7BSICAPv2%7D%29%20and%20fundus%20image%20grading%20%28%5Ctextit%7BAPTOS%7D%29%20demonstrate%20that%0AUMKD%20achieves%20a%20new%20state-of-the-art%20in%20both%20source-imbalanced%20and%0Atarget-imbalanced%20scenarios%2C%20offering%20a%20robust%20and%20practical%20solution%20for%0Areal-world%20disease%20image%20grading.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00592v1&entry.124074799=Read"},
{"title": "ClearLines - Camera Calibration from Straight Lines", "author": "Gregory Schroeder and Mohamed Sabry and Cristina Olaverri-Monreal", "abstract": "  The problem of calibration from straight lines is fundamental in geometric\ncomputer vision, with well-established theoretical foundations. However, its\npractical applicability remains limited, particularly in real-world outdoor\nscenarios. These environments pose significant challenges due to diverse and\ncluttered scenes, interrupted reprojections of straight 3D lines, and varying\nlighting conditions, making the task notoriously difficult. Furthermore, the\nfield lacks a dedicated dataset encouraging the development of respective\ndetection algorithms. In this study, we present a small dataset named\n\"ClearLines\", and by detailing its creation process, provide practical insights\nthat can serve as a guide for developing and refining straight 3D line\ndetection algorithms.\n", "link": "http://arxiv.org/abs/2505.00452v1", "date": "2025-05-01", "relevancy": 2.086, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5745}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClearLines%20-%20Camera%20Calibration%20from%20Straight%20Lines&body=Title%3A%20ClearLines%20-%20Camera%20Calibration%20from%20Straight%20Lines%0AAuthor%3A%20Gregory%20Schroeder%20and%20Mohamed%20Sabry%20and%20Cristina%20Olaverri-Monreal%0AAbstract%3A%20%20%20The%20problem%20of%20calibration%20from%20straight%20lines%20is%20fundamental%20in%20geometric%0Acomputer%20vision%2C%20with%20well-established%20theoretical%20foundations.%20However%2C%20its%0Apractical%20applicability%20remains%20limited%2C%20particularly%20in%20real-world%20outdoor%0Ascenarios.%20These%20environments%20pose%20significant%20challenges%20due%20to%20diverse%20and%0Acluttered%20scenes%2C%20interrupted%20reprojections%20of%20straight%203D%20lines%2C%20and%20varying%0Alighting%20conditions%2C%20making%20the%20task%20notoriously%20difficult.%20Furthermore%2C%20the%0Afield%20lacks%20a%20dedicated%20dataset%20encouraging%20the%20development%20of%20respective%0Adetection%20algorithms.%20In%20this%20study%2C%20we%20present%20a%20small%20dataset%20named%0A%22ClearLines%22%2C%20and%20by%20detailing%20its%20creation%20process%2C%20provide%20practical%20insights%0Athat%20can%20serve%20as%20a%20guide%20for%20developing%20and%20refining%20straight%203D%20line%0Adetection%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClearLines%2520-%2520Camera%2520Calibration%2520from%2520Straight%2520Lines%26entry.906535625%3DGregory%2520Schroeder%2520and%2520Mohamed%2520Sabry%2520and%2520Cristina%2520Olaverri-Monreal%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520calibration%2520from%2520straight%2520lines%2520is%2520fundamental%2520in%2520geometric%250Acomputer%2520vision%252C%2520with%2520well-established%2520theoretical%2520foundations.%2520However%252C%2520its%250Apractical%2520applicability%2520remains%2520limited%252C%2520particularly%2520in%2520real-world%2520outdoor%250Ascenarios.%2520These%2520environments%2520pose%2520significant%2520challenges%2520due%2520to%2520diverse%2520and%250Acluttered%2520scenes%252C%2520interrupted%2520reprojections%2520of%2520straight%25203D%2520lines%252C%2520and%2520varying%250Alighting%2520conditions%252C%2520making%2520the%2520task%2520notoriously%2520difficult.%2520Furthermore%252C%2520the%250Afield%2520lacks%2520a%2520dedicated%2520dataset%2520encouraging%2520the%2520development%2520of%2520respective%250Adetection%2520algorithms.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520small%2520dataset%2520named%250A%2522ClearLines%2522%252C%2520and%2520by%2520detailing%2520its%2520creation%2520process%252C%2520provide%2520practical%2520insights%250Athat%2520can%2520serve%2520as%2520a%2520guide%2520for%2520developing%2520and%2520refining%2520straight%25203D%2520line%250Adetection%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClearLines%20-%20Camera%20Calibration%20from%20Straight%20Lines&entry.906535625=Gregory%20Schroeder%20and%20Mohamed%20Sabry%20and%20Cristina%20Olaverri-Monreal&entry.1292438233=%20%20The%20problem%20of%20calibration%20from%20straight%20lines%20is%20fundamental%20in%20geometric%0Acomputer%20vision%2C%20with%20well-established%20theoretical%20foundations.%20However%2C%20its%0Apractical%20applicability%20remains%20limited%2C%20particularly%20in%20real-world%20outdoor%0Ascenarios.%20These%20environments%20pose%20significant%20challenges%20due%20to%20diverse%20and%0Acluttered%20scenes%2C%20interrupted%20reprojections%20of%20straight%203D%20lines%2C%20and%20varying%0Alighting%20conditions%2C%20making%20the%20task%20notoriously%20difficult.%20Furthermore%2C%20the%0Afield%20lacks%20a%20dedicated%20dataset%20encouraging%20the%20development%20of%20respective%0Adetection%20algorithms.%20In%20this%20study%2C%20we%20present%20a%20small%20dataset%20named%0A%22ClearLines%22%2C%20and%20by%20detailing%20its%20creation%20process%2C%20provide%20practical%20insights%0Athat%20can%20serve%20as%20a%20guide%20for%20developing%20and%20refining%20straight%203D%20line%0Adetection%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00452v1&entry.124074799=Read"},
{"title": "Visual Trajectory Prediction of Vessels for Inland Navigation", "author": "Alexander Puzicha and Konstantin W\u00fcstefeld and Kathrin Wilms and Frank Weichert", "abstract": "  The future of inland navigation increasingly relies on autonomous systems and\nremote operations, emphasizing the need for accurate vessel trajectory\nprediction. This study addresses the challenges of video-based vessel tracking\nand prediction by integrating advanced object detection methods, Kalman\nfilters, and spline-based interpolation. However, existing detection systems\noften misclassify objects in inland waterways due to complex surroundings. A\ncomparative evaluation of tracking algorithms, including BoT-SORT, Deep\nOC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in\nproviding smoothed trajectories. Experimental results from diverse scenarios\ndemonstrate improved accuracy in predicting vessel movements, which is\nessential for collision avoidance and situational awareness. The findings\nunderline the necessity of customized datasets and models for inland\nnavigation. Future work will expand the datasets and incorporate vessel\nclassification to refine predictions, supporting both autonomous systems and\nhuman operators in complex environments.\n", "link": "http://arxiv.org/abs/2505.00599v1", "date": "2025-05-01", "relevancy": 2.0833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Trajectory%20Prediction%20of%20Vessels%20for%20Inland%20Navigation&body=Title%3A%20Visual%20Trajectory%20Prediction%20of%20Vessels%20for%20Inland%20Navigation%0AAuthor%3A%20Alexander%20Puzicha%20and%20Konstantin%20W%C3%BCstefeld%20and%20Kathrin%20Wilms%20and%20Frank%20Weichert%0AAbstract%3A%20%20%20The%20future%20of%20inland%20navigation%20increasingly%20relies%20on%20autonomous%20systems%20and%0Aremote%20operations%2C%20emphasizing%20the%20need%20for%20accurate%20vessel%20trajectory%0Aprediction.%20This%20study%20addresses%20the%20challenges%20of%20video-based%20vessel%20tracking%0Aand%20prediction%20by%20integrating%20advanced%20object%20detection%20methods%2C%20Kalman%0Afilters%2C%20and%20spline-based%20interpolation.%20However%2C%20existing%20detection%20systems%0Aoften%20misclassify%20objects%20in%20inland%20waterways%20due%20to%20complex%20surroundings.%20A%0Acomparative%20evaluation%20of%20tracking%20algorithms%2C%20including%20BoT-SORT%2C%20Deep%0AOC-SORT%2C%20and%20ByeTrack%2C%20highlights%20the%20robustness%20of%20the%20Kalman%20filter%20in%0Aproviding%20smoothed%20trajectories.%20Experimental%20results%20from%20diverse%20scenarios%0Ademonstrate%20improved%20accuracy%20in%20predicting%20vessel%20movements%2C%20which%20is%0Aessential%20for%20collision%20avoidance%20and%20situational%20awareness.%20The%20findings%0Aunderline%20the%20necessity%20of%20customized%20datasets%20and%20models%20for%20inland%0Anavigation.%20Future%20work%20will%20expand%20the%20datasets%20and%20incorporate%20vessel%0Aclassification%20to%20refine%20predictions%2C%20supporting%20both%20autonomous%20systems%20and%0Ahuman%20operators%20in%20complex%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Trajectory%2520Prediction%2520of%2520Vessels%2520for%2520Inland%2520Navigation%26entry.906535625%3DAlexander%2520Puzicha%2520and%2520Konstantin%2520W%25C3%25BCstefeld%2520and%2520Kathrin%2520Wilms%2520and%2520Frank%2520Weichert%26entry.1292438233%3D%2520%2520The%2520future%2520of%2520inland%2520navigation%2520increasingly%2520relies%2520on%2520autonomous%2520systems%2520and%250Aremote%2520operations%252C%2520emphasizing%2520the%2520need%2520for%2520accurate%2520vessel%2520trajectory%250Aprediction.%2520This%2520study%2520addresses%2520the%2520challenges%2520of%2520video-based%2520vessel%2520tracking%250Aand%2520prediction%2520by%2520integrating%2520advanced%2520object%2520detection%2520methods%252C%2520Kalman%250Afilters%252C%2520and%2520spline-based%2520interpolation.%2520However%252C%2520existing%2520detection%2520systems%250Aoften%2520misclassify%2520objects%2520in%2520inland%2520waterways%2520due%2520to%2520complex%2520surroundings.%2520A%250Acomparative%2520evaluation%2520of%2520tracking%2520algorithms%252C%2520including%2520BoT-SORT%252C%2520Deep%250AOC-SORT%252C%2520and%2520ByeTrack%252C%2520highlights%2520the%2520robustness%2520of%2520the%2520Kalman%2520filter%2520in%250Aproviding%2520smoothed%2520trajectories.%2520Experimental%2520results%2520from%2520diverse%2520scenarios%250Ademonstrate%2520improved%2520accuracy%2520in%2520predicting%2520vessel%2520movements%252C%2520which%2520is%250Aessential%2520for%2520collision%2520avoidance%2520and%2520situational%2520awareness.%2520The%2520findings%250Aunderline%2520the%2520necessity%2520of%2520customized%2520datasets%2520and%2520models%2520for%2520inland%250Anavigation.%2520Future%2520work%2520will%2520expand%2520the%2520datasets%2520and%2520incorporate%2520vessel%250Aclassification%2520to%2520refine%2520predictions%252C%2520supporting%2520both%2520autonomous%2520systems%2520and%250Ahuman%2520operators%2520in%2520complex%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Trajectory%20Prediction%20of%20Vessels%20for%20Inland%20Navigation&entry.906535625=Alexander%20Puzicha%20and%20Konstantin%20W%C3%BCstefeld%20and%20Kathrin%20Wilms%20and%20Frank%20Weichert&entry.1292438233=%20%20The%20future%20of%20inland%20navigation%20increasingly%20relies%20on%20autonomous%20systems%20and%0Aremote%20operations%2C%20emphasizing%20the%20need%20for%20accurate%20vessel%20trajectory%0Aprediction.%20This%20study%20addresses%20the%20challenges%20of%20video-based%20vessel%20tracking%0Aand%20prediction%20by%20integrating%20advanced%20object%20detection%20methods%2C%20Kalman%0Afilters%2C%20and%20spline-based%20interpolation.%20However%2C%20existing%20detection%20systems%0Aoften%20misclassify%20objects%20in%20inland%20waterways%20due%20to%20complex%20surroundings.%20A%0Acomparative%20evaluation%20of%20tracking%20algorithms%2C%20including%20BoT-SORT%2C%20Deep%0AOC-SORT%2C%20and%20ByeTrack%2C%20highlights%20the%20robustness%20of%20the%20Kalman%20filter%20in%0Aproviding%20smoothed%20trajectories.%20Experimental%20results%20from%20diverse%20scenarios%0Ademonstrate%20improved%20accuracy%20in%20predicting%20vessel%20movements%2C%20which%20is%0Aessential%20for%20collision%20avoidance%20and%20situational%20awareness.%20The%20findings%0Aunderline%20the%20necessity%20of%20customized%20datasets%20and%20models%20for%20inland%0Anavigation.%20Future%20work%20will%20expand%20the%20datasets%20and%20incorporate%20vessel%0Aclassification%20to%20refine%20predictions%2C%20supporting%20both%20autonomous%20systems%20and%0Ahuman%20operators%20in%20complex%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00599v1&entry.124074799=Read"},
{"title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World\n  Hallucination Detection", "author": "Deanna Emery and Michael Goitia and Freddie Vargus and Iulia Neagu", "abstract": "  As large language models (LLMs) are increasingly deployed in high-stakes\ndomains, detecting hallucinated content$\\unicode{x2013}$text that is not\ngrounded in supporting evidence$\\unicode{x2013}$has become a critical\nchallenge. Existing benchmarks for hallucination detection are often\nsynthetically generated, narrowly focused on extractive question answering, and\nfail to capture the complexity of real-world scenarios involving multi-document\ncontexts and full-sentence outputs. We introduce the HalluMix Benchmark, a\ndiverse, task-agnostic dataset that includes examples from a range of domains\nand formats. Using this benchmark, we evaluate seven hallucination detection\nsystems$\\unicode{x2013}$both open and closed\nsource$\\unicode{x2013}$highlighting differences in performance across tasks,\ndocument lengths, and input representations. Our analysis highlights\nsubstantial performance disparities between short and long contexts, with\ncritical implications for real-world Retrieval Augmented Generation (RAG)\nimplementations. Quotient Detections achieves the best overall performance,\nwith an accuracy of 0.82 and an F1 score of 0.84.\n", "link": "http://arxiv.org/abs/2505.00506v1", "date": "2025-05-01", "relevancy": 2.0787, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HalluMix%3A%20A%20Task-Agnostic%2C%20Multi-Domain%20Benchmark%20for%20Real-World%0A%20%20Hallucination%20Detection&body=Title%3A%20HalluMix%3A%20A%20Task-Agnostic%2C%20Multi-Domain%20Benchmark%20for%20Real-World%0A%20%20Hallucination%20Detection%0AAuthor%3A%20Deanna%20Emery%20and%20Michael%20Goitia%20and%20Freddie%20Vargus%20and%20Iulia%20Neagu%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20high-stakes%0Adomains%2C%20detecting%20hallucinated%20content%24%5Cunicode%7Bx2013%7D%24text%20that%20is%20not%0Agrounded%20in%20supporting%20evidence%24%5Cunicode%7Bx2013%7D%24has%20become%20a%20critical%0Achallenge.%20Existing%20benchmarks%20for%20hallucination%20detection%20are%20often%0Asynthetically%20generated%2C%20narrowly%20focused%20on%20extractive%20question%20answering%2C%20and%0Afail%20to%20capture%20the%20complexity%20of%20real-world%20scenarios%20involving%20multi-document%0Acontexts%20and%20full-sentence%20outputs.%20We%20introduce%20the%20HalluMix%20Benchmark%2C%20a%0Adiverse%2C%20task-agnostic%20dataset%20that%20includes%20examples%20from%20a%20range%20of%20domains%0Aand%20formats.%20Using%20this%20benchmark%2C%20we%20evaluate%20seven%20hallucination%20detection%0Asystems%24%5Cunicode%7Bx2013%7D%24both%20open%20and%20closed%0Asource%24%5Cunicode%7Bx2013%7D%24highlighting%20differences%20in%20performance%20across%20tasks%2C%0Adocument%20lengths%2C%20and%20input%20representations.%20Our%20analysis%20highlights%0Asubstantial%20performance%20disparities%20between%20short%20and%20long%20contexts%2C%20with%0Acritical%20implications%20for%20real-world%20Retrieval%20Augmented%20Generation%20%28RAG%29%0Aimplementations.%20Quotient%20Detections%20achieves%20the%20best%20overall%20performance%2C%0Awith%20an%20accuracy%20of%200.82%20and%20an%20F1%20score%20of%200.84.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHalluMix%253A%2520A%2520Task-Agnostic%252C%2520Multi-Domain%2520Benchmark%2520for%2520Real-World%250A%2520%2520Hallucination%2520Detection%26entry.906535625%3DDeanna%2520Emery%2520and%2520Michael%2520Goitia%2520and%2520Freddie%2520Vargus%2520and%2520Iulia%2520Neagu%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520high-stakes%250Adomains%252C%2520detecting%2520hallucinated%2520content%2524%255Cunicode%257Bx2013%257D%2524text%2520that%2520is%2520not%250Agrounded%2520in%2520supporting%2520evidence%2524%255Cunicode%257Bx2013%257D%2524has%2520become%2520a%2520critical%250Achallenge.%2520Existing%2520benchmarks%2520for%2520hallucination%2520detection%2520are%2520often%250Asynthetically%2520generated%252C%2520narrowly%2520focused%2520on%2520extractive%2520question%2520answering%252C%2520and%250Afail%2520to%2520capture%2520the%2520complexity%2520of%2520real-world%2520scenarios%2520involving%2520multi-document%250Acontexts%2520and%2520full-sentence%2520outputs.%2520We%2520introduce%2520the%2520HalluMix%2520Benchmark%252C%2520a%250Adiverse%252C%2520task-agnostic%2520dataset%2520that%2520includes%2520examples%2520from%2520a%2520range%2520of%2520domains%250Aand%2520formats.%2520Using%2520this%2520benchmark%252C%2520we%2520evaluate%2520seven%2520hallucination%2520detection%250Asystems%2524%255Cunicode%257Bx2013%257D%2524both%2520open%2520and%2520closed%250Asource%2524%255Cunicode%257Bx2013%257D%2524highlighting%2520differences%2520in%2520performance%2520across%2520tasks%252C%250Adocument%2520lengths%252C%2520and%2520input%2520representations.%2520Our%2520analysis%2520highlights%250Asubstantial%2520performance%2520disparities%2520between%2520short%2520and%2520long%2520contexts%252C%2520with%250Acritical%2520implications%2520for%2520real-world%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%250Aimplementations.%2520Quotient%2520Detections%2520achieves%2520the%2520best%2520overall%2520performance%252C%250Awith%2520an%2520accuracy%2520of%25200.82%2520and%2520an%2520F1%2520score%2520of%25200.84.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HalluMix%3A%20A%20Task-Agnostic%2C%20Multi-Domain%20Benchmark%20for%20Real-World%0A%20%20Hallucination%20Detection&entry.906535625=Deanna%20Emery%20and%20Michael%20Goitia%20and%20Freddie%20Vargus%20and%20Iulia%20Neagu&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20high-stakes%0Adomains%2C%20detecting%20hallucinated%20content%24%5Cunicode%7Bx2013%7D%24text%20that%20is%20not%0Agrounded%20in%20supporting%20evidence%24%5Cunicode%7Bx2013%7D%24has%20become%20a%20critical%0Achallenge.%20Existing%20benchmarks%20for%20hallucination%20detection%20are%20often%0Asynthetically%20generated%2C%20narrowly%20focused%20on%20extractive%20question%20answering%2C%20and%0Afail%20to%20capture%20the%20complexity%20of%20real-world%20scenarios%20involving%20multi-document%0Acontexts%20and%20full-sentence%20outputs.%20We%20introduce%20the%20HalluMix%20Benchmark%2C%20a%0Adiverse%2C%20task-agnostic%20dataset%20that%20includes%20examples%20from%20a%20range%20of%20domains%0Aand%20formats.%20Using%20this%20benchmark%2C%20we%20evaluate%20seven%20hallucination%20detection%0Asystems%24%5Cunicode%7Bx2013%7D%24both%20open%20and%20closed%0Asource%24%5Cunicode%7Bx2013%7D%24highlighting%20differences%20in%20performance%20across%20tasks%2C%0Adocument%20lengths%2C%20and%20input%20representations.%20Our%20analysis%20highlights%0Asubstantial%20performance%20disparities%20between%20short%20and%20long%20contexts%2C%20with%0Acritical%20implications%20for%20real-world%20Retrieval%20Augmented%20Generation%20%28RAG%29%0Aimplementations.%20Quotient%20Detections%20achieves%20the%20best%20overall%20performance%2C%0Awith%20an%20accuracy%20of%200.82%20and%20an%20F1%20score%20of%200.84.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00506v1&entry.124074799=Read"},
{"title": "AMUN: Adversarial Machine UNlearning", "author": "Ali Ebrahimpour-Boroojeny and Hari Sundaram and Varun Chandrasekaran", "abstract": "  Machine unlearning, where users can request the deletion of a forget dataset,\nis becoming increasingly important because of numerous privacy regulations.\nInitial works on ``exact'' unlearning (e.g., retraining) incur large\ncomputational overheads. However, while computationally inexpensive,\n``approximate'' methods have fallen short of reaching the effectiveness of\nexact unlearning: models produced fail to obtain comparable accuracy and\nprediction confidence on both the forget and test (i.e., unseen) dataset.\nExploiting this observation, we propose a new unlearning method, Adversarial\nMachine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA)\nmethods for image classification. AMUN lowers the confidence of the model on\nthe forget samples by fine-tuning the model on their corresponding adversarial\nexamples. Adversarial examples naturally belong to the distribution imposed by\nthe model on the input space; fine-tuning the model on the adversarial examples\nclosest to the corresponding forget samples (a) localizes the changes to the\ndecision boundary of the model around each forget sample and (b) avoids drastic\nchanges to the global behavior of the model, thereby preserving the model's\naccuracy on test samples. Using AMUN for unlearning a random $10\\%$ of CIFAR-10\nsamples, we observe that even SOTA membership inference attacks cannot do\nbetter than random guessing.\n", "link": "http://arxiv.org/abs/2503.00917v2", "date": "2025-05-01", "relevancy": 2.0766, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.542}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5306}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMUN%3A%20Adversarial%20Machine%20UNlearning&body=Title%3A%20AMUN%3A%20Adversarial%20Machine%20UNlearning%0AAuthor%3A%20Ali%20Ebrahimpour-Boroojeny%20and%20Hari%20Sundaram%20and%20Varun%20Chandrasekaran%0AAbstract%3A%20%20%20Machine%20unlearning%2C%20where%20users%20can%20request%20the%20deletion%20of%20a%20forget%20dataset%2C%0Ais%20becoming%20increasingly%20important%20because%20of%20numerous%20privacy%20regulations.%0AInitial%20works%20on%20%60%60exact%27%27%20unlearning%20%28e.g.%2C%20retraining%29%20incur%20large%0Acomputational%20overheads.%20However%2C%20while%20computationally%20inexpensive%2C%0A%60%60approximate%27%27%20methods%20have%20fallen%20short%20of%20reaching%20the%20effectiveness%20of%0Aexact%20unlearning%3A%20models%20produced%20fail%20to%20obtain%20comparable%20accuracy%20and%0Aprediction%20confidence%20on%20both%20the%20forget%20and%20test%20%28i.e.%2C%20unseen%29%20dataset.%0AExploiting%20this%20observation%2C%20we%20propose%20a%20new%20unlearning%20method%2C%20Adversarial%0AMachine%20UNlearning%20%28AMUN%29%2C%20that%20outperforms%20prior%20state-of-the-art%20%28SOTA%29%0Amethods%20for%20image%20classification.%20AMUN%20lowers%20the%20confidence%20of%20the%20model%20on%0Athe%20forget%20samples%20by%20fine-tuning%20the%20model%20on%20their%20corresponding%20adversarial%0Aexamples.%20Adversarial%20examples%20naturally%20belong%20to%20the%20distribution%20imposed%20by%0Athe%20model%20on%20the%20input%20space%3B%20fine-tuning%20the%20model%20on%20the%20adversarial%20examples%0Aclosest%20to%20the%20corresponding%20forget%20samples%20%28a%29%20localizes%20the%20changes%20to%20the%0Adecision%20boundary%20of%20the%20model%20around%20each%20forget%20sample%20and%20%28b%29%20avoids%20drastic%0Achanges%20to%20the%20global%20behavior%20of%20the%20model%2C%20thereby%20preserving%20the%20model%27s%0Aaccuracy%20on%20test%20samples.%20Using%20AMUN%20for%20unlearning%20a%20random%20%2410%5C%25%24%20of%20CIFAR-10%0Asamples%2C%20we%20observe%20that%20even%20SOTA%20membership%20inference%20attacks%20cannot%20do%0Abetter%20than%20random%20guessing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMUN%253A%2520Adversarial%2520Machine%2520UNlearning%26entry.906535625%3DAli%2520Ebrahimpour-Boroojeny%2520and%2520Hari%2520Sundaram%2520and%2520Varun%2520Chandrasekaran%26entry.1292438233%3D%2520%2520Machine%2520unlearning%252C%2520where%2520users%2520can%2520request%2520the%2520deletion%2520of%2520a%2520forget%2520dataset%252C%250Ais%2520becoming%2520increasingly%2520important%2520because%2520of%2520numerous%2520privacy%2520regulations.%250AInitial%2520works%2520on%2520%2560%2560exact%2527%2527%2520unlearning%2520%2528e.g.%252C%2520retraining%2529%2520incur%2520large%250Acomputational%2520overheads.%2520However%252C%2520while%2520computationally%2520inexpensive%252C%250A%2560%2560approximate%2527%2527%2520methods%2520have%2520fallen%2520short%2520of%2520reaching%2520the%2520effectiveness%2520of%250Aexact%2520unlearning%253A%2520models%2520produced%2520fail%2520to%2520obtain%2520comparable%2520accuracy%2520and%250Aprediction%2520confidence%2520on%2520both%2520the%2520forget%2520and%2520test%2520%2528i.e.%252C%2520unseen%2529%2520dataset.%250AExploiting%2520this%2520observation%252C%2520we%2520propose%2520a%2520new%2520unlearning%2520method%252C%2520Adversarial%250AMachine%2520UNlearning%2520%2528AMUN%2529%252C%2520that%2520outperforms%2520prior%2520state-of-the-art%2520%2528SOTA%2529%250Amethods%2520for%2520image%2520classification.%2520AMUN%2520lowers%2520the%2520confidence%2520of%2520the%2520model%2520on%250Athe%2520forget%2520samples%2520by%2520fine-tuning%2520the%2520model%2520on%2520their%2520corresponding%2520adversarial%250Aexamples.%2520Adversarial%2520examples%2520naturally%2520belong%2520to%2520the%2520distribution%2520imposed%2520by%250Athe%2520model%2520on%2520the%2520input%2520space%253B%2520fine-tuning%2520the%2520model%2520on%2520the%2520adversarial%2520examples%250Aclosest%2520to%2520the%2520corresponding%2520forget%2520samples%2520%2528a%2529%2520localizes%2520the%2520changes%2520to%2520the%250Adecision%2520boundary%2520of%2520the%2520model%2520around%2520each%2520forget%2520sample%2520and%2520%2528b%2529%2520avoids%2520drastic%250Achanges%2520to%2520the%2520global%2520behavior%2520of%2520the%2520model%252C%2520thereby%2520preserving%2520the%2520model%2527s%250Aaccuracy%2520on%2520test%2520samples.%2520Using%2520AMUN%2520for%2520unlearning%2520a%2520random%2520%252410%255C%2525%2524%2520of%2520CIFAR-10%250Asamples%252C%2520we%2520observe%2520that%2520even%2520SOTA%2520membership%2520inference%2520attacks%2520cannot%2520do%250Abetter%2520than%2520random%2520guessing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMUN%3A%20Adversarial%20Machine%20UNlearning&entry.906535625=Ali%20Ebrahimpour-Boroojeny%20and%20Hari%20Sundaram%20and%20Varun%20Chandrasekaran&entry.1292438233=%20%20Machine%20unlearning%2C%20where%20users%20can%20request%20the%20deletion%20of%20a%20forget%20dataset%2C%0Ais%20becoming%20increasingly%20important%20because%20of%20numerous%20privacy%20regulations.%0AInitial%20works%20on%20%60%60exact%27%27%20unlearning%20%28e.g.%2C%20retraining%29%20incur%20large%0Acomputational%20overheads.%20However%2C%20while%20computationally%20inexpensive%2C%0A%60%60approximate%27%27%20methods%20have%20fallen%20short%20of%20reaching%20the%20effectiveness%20of%0Aexact%20unlearning%3A%20models%20produced%20fail%20to%20obtain%20comparable%20accuracy%20and%0Aprediction%20confidence%20on%20both%20the%20forget%20and%20test%20%28i.e.%2C%20unseen%29%20dataset.%0AExploiting%20this%20observation%2C%20we%20propose%20a%20new%20unlearning%20method%2C%20Adversarial%0AMachine%20UNlearning%20%28AMUN%29%2C%20that%20outperforms%20prior%20state-of-the-art%20%28SOTA%29%0Amethods%20for%20image%20classification.%20AMUN%20lowers%20the%20confidence%20of%20the%20model%20on%0Athe%20forget%20samples%20by%20fine-tuning%20the%20model%20on%20their%20corresponding%20adversarial%0Aexamples.%20Adversarial%20examples%20naturally%20belong%20to%20the%20distribution%20imposed%20by%0Athe%20model%20on%20the%20input%20space%3B%20fine-tuning%20the%20model%20on%20the%20adversarial%20examples%0Aclosest%20to%20the%20corresponding%20forget%20samples%20%28a%29%20localizes%20the%20changes%20to%20the%0Adecision%20boundary%20of%20the%20model%20around%20each%20forget%20sample%20and%20%28b%29%20avoids%20drastic%0Achanges%20to%20the%20global%20behavior%20of%20the%20model%2C%20thereby%20preserving%20the%20model%27s%0Aaccuracy%20on%20test%20samples.%20Using%20AMUN%20for%20unlearning%20a%20random%20%2410%5C%25%24%20of%20CIFAR-10%0Asamples%2C%20we%20observe%20that%20even%20SOTA%20membership%20inference%20attacks%20cannot%20do%0Abetter%20than%20random%20guessing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00917v2&entry.124074799=Read"},
{"title": "Visual Test-time Scaling for GUI Agent Grounding", "author": "Tiange Luo and Lajanugen Logeswaran and Justin Johnson and Honglak Lee", "abstract": "  We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.\n", "link": "http://arxiv.org/abs/2505.00684v1", "date": "2025-05-01", "relevancy": 2.0718, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Test-time%20Scaling%20for%20GUI%20Agent%20Grounding&body=Title%3A%20Visual%20Test-time%20Scaling%20for%20GUI%20Agent%20Grounding%0AAuthor%3A%20Tiange%20Luo%20and%20Lajanugen%20Logeswaran%20and%20Justin%20Johnson%20and%20Honglak%20Lee%0AAbstract%3A%20%20%20We%20introduce%20RegionFocus%2C%20a%20visual%20test-time%20scaling%20approach%20for%20Vision%0ALanguage%20Model%20Agents.%20Understanding%20webpages%20is%20challenging%20due%20to%20the%20visual%0Acomplexity%20of%20GUI%20images%20and%20the%20large%20number%20of%20interface%20elements%2C%20making%0Aaccurate%20action%20selection%20difficult.%20Our%20approach%20dynamically%20zooms%20in%20on%0Arelevant%20regions%2C%20reducing%20background%20clutter%20and%20improving%20grounding%20accuracy.%0ATo%20support%20this%20process%2C%20we%20propose%20an%20image-as-map%20mechanism%20that%20visualizes%0Akey%20landmarks%20at%20each%20step%2C%20providing%20a%20transparent%20action%20record%20and%20enables%0Athe%20agent%20to%20effectively%20choose%20among%20action%20candidates.%20Even%20with%20a%20simple%0Aregion%20selection%20strategy%2C%20we%20observe%20significant%20performance%20gains%20of%2028%2B%5C%25%20on%0AScreenspot-pro%20and%2024%2B%5C%25%20on%20WebVoyager%20benchmarks%20on%20top%20of%20two%0Astate-of-the-art%20open%20vision%20language%20model%20agents%2C%20UI-TARS%20and%20Qwen2.5-VL%2C%0Ahighlighting%20the%20effectiveness%20of%20visual%20test-time%20scaling%20in%20interactive%0Asettings.%20We%20achieve%20a%20new%20state-of-the-art%20grounding%20performance%20of%2061.6%5C%25%20on%0Athe%20ScreenSpot-Pro%20benchmark%20by%20applying%20RegionFocus%20to%20a%20Qwen2.5-VL-72B%20model.%0AOur%20code%20will%20be%20released%20publicly%20at%20https%3A//github.com/tiangeluo/RegionFocus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Test-time%2520Scaling%2520for%2520GUI%2520Agent%2520Grounding%26entry.906535625%3DTiange%2520Luo%2520and%2520Lajanugen%2520Logeswaran%2520and%2520Justin%2520Johnson%2520and%2520Honglak%2520Lee%26entry.1292438233%3D%2520%2520We%2520introduce%2520RegionFocus%252C%2520a%2520visual%2520test-time%2520scaling%2520approach%2520for%2520Vision%250ALanguage%2520Model%2520Agents.%2520Understanding%2520webpages%2520is%2520challenging%2520due%2520to%2520the%2520visual%250Acomplexity%2520of%2520GUI%2520images%2520and%2520the%2520large%2520number%2520of%2520interface%2520elements%252C%2520making%250Aaccurate%2520action%2520selection%2520difficult.%2520Our%2520approach%2520dynamically%2520zooms%2520in%2520on%250Arelevant%2520regions%252C%2520reducing%2520background%2520clutter%2520and%2520improving%2520grounding%2520accuracy.%250ATo%2520support%2520this%2520process%252C%2520we%2520propose%2520an%2520image-as-map%2520mechanism%2520that%2520visualizes%250Akey%2520landmarks%2520at%2520each%2520step%252C%2520providing%2520a%2520transparent%2520action%2520record%2520and%2520enables%250Athe%2520agent%2520to%2520effectively%2520choose%2520among%2520action%2520candidates.%2520Even%2520with%2520a%2520simple%250Aregion%2520selection%2520strategy%252C%2520we%2520observe%2520significant%2520performance%2520gains%2520of%252028%252B%255C%2525%2520on%250AScreenspot-pro%2520and%252024%252B%255C%2525%2520on%2520WebVoyager%2520benchmarks%2520on%2520top%2520of%2520two%250Astate-of-the-art%2520open%2520vision%2520language%2520model%2520agents%252C%2520UI-TARS%2520and%2520Qwen2.5-VL%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520visual%2520test-time%2520scaling%2520in%2520interactive%250Asettings.%2520We%2520achieve%2520a%2520new%2520state-of-the-art%2520grounding%2520performance%2520of%252061.6%255C%2525%2520on%250Athe%2520ScreenSpot-Pro%2520benchmark%2520by%2520applying%2520RegionFocus%2520to%2520a%2520Qwen2.5-VL-72B%2520model.%250AOur%2520code%2520will%2520be%2520released%2520publicly%2520at%2520https%253A//github.com/tiangeluo/RegionFocus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Test-time%20Scaling%20for%20GUI%20Agent%20Grounding&entry.906535625=Tiange%20Luo%20and%20Lajanugen%20Logeswaran%20and%20Justin%20Johnson%20and%20Honglak%20Lee&entry.1292438233=%20%20We%20introduce%20RegionFocus%2C%20a%20visual%20test-time%20scaling%20approach%20for%20Vision%0ALanguage%20Model%20Agents.%20Understanding%20webpages%20is%20challenging%20due%20to%20the%20visual%0Acomplexity%20of%20GUI%20images%20and%20the%20large%20number%20of%20interface%20elements%2C%20making%0Aaccurate%20action%20selection%20difficult.%20Our%20approach%20dynamically%20zooms%20in%20on%0Arelevant%20regions%2C%20reducing%20background%20clutter%20and%20improving%20grounding%20accuracy.%0ATo%20support%20this%20process%2C%20we%20propose%20an%20image-as-map%20mechanism%20that%20visualizes%0Akey%20landmarks%20at%20each%20step%2C%20providing%20a%20transparent%20action%20record%20and%20enables%0Athe%20agent%20to%20effectively%20choose%20among%20action%20candidates.%20Even%20with%20a%20simple%0Aregion%20selection%20strategy%2C%20we%20observe%20significant%20performance%20gains%20of%2028%2B%5C%25%20on%0AScreenspot-pro%20and%2024%2B%5C%25%20on%20WebVoyager%20benchmarks%20on%20top%20of%20two%0Astate-of-the-art%20open%20vision%20language%20model%20agents%2C%20UI-TARS%20and%20Qwen2.5-VL%2C%0Ahighlighting%20the%20effectiveness%20of%20visual%20test-time%20scaling%20in%20interactive%0Asettings.%20We%20achieve%20a%20new%20state-of-the-art%20grounding%20performance%20of%2061.6%5C%25%20on%0Athe%20ScreenSpot-Pro%20benchmark%20by%20applying%20RegionFocus%20to%20a%20Qwen2.5-VL-72B%20model.%0AOur%20code%20will%20be%20released%20publicly%20at%20https%3A//github.com/tiangeluo/RegionFocus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00684v1&entry.124074799=Read"},
{"title": "On the Importance of Gaussianizing Representations", "author": "Daniel Eftekhari and Vardan Papyan", "abstract": "  The normal distribution plays a central role in information theory - it is at\nthe same time the best-case signal and worst-case noise distribution, has the\ngreatest representational capacity of any distribution, and offers an\nequivalence between uncorrelatedness and independence for joint distributions.\nAccounting for the mean and variance of activations throughout the layers of\ndeep neural networks has had a significant effect on facilitating their\neffective training, but seldom has a prescription for precisely what\ndistribution these activations should take, and how this might be achieved,\nbeen offered. Motivated by the information-theoretic properties of the normal\ndistribution, we address this question and concurrently present normality\nnormalization: a novel normalization layer which encourages normality in the\nfeature representations of neural networks using the power transform and\nemploys additive Gaussian noise during training. Our experiments\ncomprehensively demonstrate the effectiveness of normality normalization, in\nregards to its generalization performance on an array of widely used model and\ndataset combinations, its strong performance across various common factors of\nvariation such as model width, depth, and training minibatch size, its\nsuitability for usage wherever existing normalization layers are conventionally\nused, and as a means to improving model robustness to random perturbations.\n", "link": "http://arxiv.org/abs/2505.00685v1", "date": "2025-05-01", "relevancy": 2.0682, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.54}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5092}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Importance%20of%20Gaussianizing%20Representations&body=Title%3A%20On%20the%20Importance%20of%20Gaussianizing%20Representations%0AAuthor%3A%20Daniel%20Eftekhari%20and%20Vardan%20Papyan%0AAbstract%3A%20%20%20The%20normal%20distribution%20plays%20a%20central%20role%20in%20information%20theory%20-%20it%20is%20at%0Athe%20same%20time%20the%20best-case%20signal%20and%20worst-case%20noise%20distribution%2C%20has%20the%0Agreatest%20representational%20capacity%20of%20any%20distribution%2C%20and%20offers%20an%0Aequivalence%20between%20uncorrelatedness%20and%20independence%20for%20joint%20distributions.%0AAccounting%20for%20the%20mean%20and%20variance%20of%20activations%20throughout%20the%20layers%20of%0Adeep%20neural%20networks%20has%20had%20a%20significant%20effect%20on%20facilitating%20their%0Aeffective%20training%2C%20but%20seldom%20has%20a%20prescription%20for%20precisely%20what%0Adistribution%20these%20activations%20should%20take%2C%20and%20how%20this%20might%20be%20achieved%2C%0Abeen%20offered.%20Motivated%20by%20the%20information-theoretic%20properties%20of%20the%20normal%0Adistribution%2C%20we%20address%20this%20question%20and%20concurrently%20present%20normality%0Anormalization%3A%20a%20novel%20normalization%20layer%20which%20encourages%20normality%20in%20the%0Afeature%20representations%20of%20neural%20networks%20using%20the%20power%20transform%20and%0Aemploys%20additive%20Gaussian%20noise%20during%20training.%20Our%20experiments%0Acomprehensively%20demonstrate%20the%20effectiveness%20of%20normality%20normalization%2C%20in%0Aregards%20to%20its%20generalization%20performance%20on%20an%20array%20of%20widely%20used%20model%20and%0Adataset%20combinations%2C%20its%20strong%20performance%20across%20various%20common%20factors%20of%0Avariation%20such%20as%20model%20width%2C%20depth%2C%20and%20training%20minibatch%20size%2C%20its%0Asuitability%20for%20usage%20wherever%20existing%20normalization%20layers%20are%20conventionally%0Aused%2C%20and%20as%20a%20means%20to%20improving%20model%20robustness%20to%20random%20perturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Importance%2520of%2520Gaussianizing%2520Representations%26entry.906535625%3DDaniel%2520Eftekhari%2520and%2520Vardan%2520Papyan%26entry.1292438233%3D%2520%2520The%2520normal%2520distribution%2520plays%2520a%2520central%2520role%2520in%2520information%2520theory%2520-%2520it%2520is%2520at%250Athe%2520same%2520time%2520the%2520best-case%2520signal%2520and%2520worst-case%2520noise%2520distribution%252C%2520has%2520the%250Agreatest%2520representational%2520capacity%2520of%2520any%2520distribution%252C%2520and%2520offers%2520an%250Aequivalence%2520between%2520uncorrelatedness%2520and%2520independence%2520for%2520joint%2520distributions.%250AAccounting%2520for%2520the%2520mean%2520and%2520variance%2520of%2520activations%2520throughout%2520the%2520layers%2520of%250Adeep%2520neural%2520networks%2520has%2520had%2520a%2520significant%2520effect%2520on%2520facilitating%2520their%250Aeffective%2520training%252C%2520but%2520seldom%2520has%2520a%2520prescription%2520for%2520precisely%2520what%250Adistribution%2520these%2520activations%2520should%2520take%252C%2520and%2520how%2520this%2520might%2520be%2520achieved%252C%250Abeen%2520offered.%2520Motivated%2520by%2520the%2520information-theoretic%2520properties%2520of%2520the%2520normal%250Adistribution%252C%2520we%2520address%2520this%2520question%2520and%2520concurrently%2520present%2520normality%250Anormalization%253A%2520a%2520novel%2520normalization%2520layer%2520which%2520encourages%2520normality%2520in%2520the%250Afeature%2520representations%2520of%2520neural%2520networks%2520using%2520the%2520power%2520transform%2520and%250Aemploys%2520additive%2520Gaussian%2520noise%2520during%2520training.%2520Our%2520experiments%250Acomprehensively%2520demonstrate%2520the%2520effectiveness%2520of%2520normality%2520normalization%252C%2520in%250Aregards%2520to%2520its%2520generalization%2520performance%2520on%2520an%2520array%2520of%2520widely%2520used%2520model%2520and%250Adataset%2520combinations%252C%2520its%2520strong%2520performance%2520across%2520various%2520common%2520factors%2520of%250Avariation%2520such%2520as%2520model%2520width%252C%2520depth%252C%2520and%2520training%2520minibatch%2520size%252C%2520its%250Asuitability%2520for%2520usage%2520wherever%2520existing%2520normalization%2520layers%2520are%2520conventionally%250Aused%252C%2520and%2520as%2520a%2520means%2520to%2520improving%2520model%2520robustness%2520to%2520random%2520perturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Importance%20of%20Gaussianizing%20Representations&entry.906535625=Daniel%20Eftekhari%20and%20Vardan%20Papyan&entry.1292438233=%20%20The%20normal%20distribution%20plays%20a%20central%20role%20in%20information%20theory%20-%20it%20is%20at%0Athe%20same%20time%20the%20best-case%20signal%20and%20worst-case%20noise%20distribution%2C%20has%20the%0Agreatest%20representational%20capacity%20of%20any%20distribution%2C%20and%20offers%20an%0Aequivalence%20between%20uncorrelatedness%20and%20independence%20for%20joint%20distributions.%0AAccounting%20for%20the%20mean%20and%20variance%20of%20activations%20throughout%20the%20layers%20of%0Adeep%20neural%20networks%20has%20had%20a%20significant%20effect%20on%20facilitating%20their%0Aeffective%20training%2C%20but%20seldom%20has%20a%20prescription%20for%20precisely%20what%0Adistribution%20these%20activations%20should%20take%2C%20and%20how%20this%20might%20be%20achieved%2C%0Abeen%20offered.%20Motivated%20by%20the%20information-theoretic%20properties%20of%20the%20normal%0Adistribution%2C%20we%20address%20this%20question%20and%20concurrently%20present%20normality%0Anormalization%3A%20a%20novel%20normalization%20layer%20which%20encourages%20normality%20in%20the%0Afeature%20representations%20of%20neural%20networks%20using%20the%20power%20transform%20and%0Aemploys%20additive%20Gaussian%20noise%20during%20training.%20Our%20experiments%0Acomprehensively%20demonstrate%20the%20effectiveness%20of%20normality%20normalization%2C%20in%0Aregards%20to%20its%20generalization%20performance%20on%20an%20array%20of%20widely%20used%20model%20and%0Adataset%20combinations%2C%20its%20strong%20performance%20across%20various%20common%20factors%20of%0Avariation%20such%20as%20model%20width%2C%20depth%2C%20and%20training%20minibatch%20size%2C%20its%0Asuitability%20for%20usage%20wherever%20existing%20normalization%20layers%20are%20conventionally%0Aused%2C%20and%20as%20a%20means%20to%20improving%20model%20robustness%20to%20random%20perturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00685v1&entry.124074799=Read"},
{"title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts\n  Using Large Language Models", "author": "Sungbok Shin and Hyeon Jeon and Sanghyun Hong and Niklas Elmqvist", "abstract": "  Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design.\n", "link": "http://arxiv.org/abs/2505.00455v1", "date": "2025-05-01", "relevancy": 2.0676, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Therapist%3A%20Eliciting%20Domain%20Knowledge%20from%20Subject%20Matter%20Experts%0A%20%20Using%20Large%20Language%20Models&body=Title%3A%20Data%20Therapist%3A%20Eliciting%20Domain%20Knowledge%20from%20Subject%20Matter%20Experts%0A%20%20Using%20Large%20Language%20Models%0AAuthor%3A%20Sungbok%20Shin%20and%20Hyeon%20Jeon%20and%20Sanghyun%20Hong%20and%20Niklas%20Elmqvist%0AAbstract%3A%20%20%20Effective%20data%20visualization%20requires%20not%20only%20technical%20proficiency%20but%20also%0Aa%20deep%20understanding%20of%20the%20domain-specific%20context%20in%20which%20data%20exists.%20This%0Acontext%20often%20includes%20tacit%20knowledge%20about%20data%20provenance%2C%20quality%2C%20and%0Aintended%20use%2C%20which%20is%20rarely%20explicit%20in%20the%20dataset%20itself.%20We%20present%20the%0AData%20Therapist%2C%20a%20web-based%20tool%20that%20helps%20domain%20experts%20externalize%20this%0Aimplicit%20knowledge%20through%20a%20mixed-initiative%20process%20combining%20iterative%20Q%26A%0Awith%20interactive%20annotation.%20Powered%20by%20a%20large%20language%20model%2C%20the%20system%0Aanalyzes%20user-supplied%20datasets%2C%20prompts%20users%20with%20targeted%20questions%2C%20and%0Aallows%20annotation%20at%20varying%20levels%20of%20granularity.%20The%20resulting%20structured%0Aknowledge%20base%20can%20inform%20both%20human%20and%20automated%20visualization%20design.%20We%0Aevaluated%20the%20tool%20in%20a%20qualitative%20study%20involving%20expert%20pairs%20from%20Molecular%0ABiology%2C%20Accounting%2C%20Political%20Science%2C%20and%20Usable%20Security.%20The%20study%20revealed%0Arecurring%20patterns%20in%20how%20experts%20reason%20about%20their%20data%20and%20highlights%20areas%0Awhere%20AI%20support%20can%20improve%20visualization%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Therapist%253A%2520Eliciting%2520Domain%2520Knowledge%2520from%2520Subject%2520Matter%2520Experts%250A%2520%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DSungbok%2520Shin%2520and%2520Hyeon%2520Jeon%2520and%2520Sanghyun%2520Hong%2520and%2520Niklas%2520Elmqvist%26entry.1292438233%3D%2520%2520Effective%2520data%2520visualization%2520requires%2520not%2520only%2520technical%2520proficiency%2520but%2520also%250Aa%2520deep%2520understanding%2520of%2520the%2520domain-specific%2520context%2520in%2520which%2520data%2520exists.%2520This%250Acontext%2520often%2520includes%2520tacit%2520knowledge%2520about%2520data%2520provenance%252C%2520quality%252C%2520and%250Aintended%2520use%252C%2520which%2520is%2520rarely%2520explicit%2520in%2520the%2520dataset%2520itself.%2520We%2520present%2520the%250AData%2520Therapist%252C%2520a%2520web-based%2520tool%2520that%2520helps%2520domain%2520experts%2520externalize%2520this%250Aimplicit%2520knowledge%2520through%2520a%2520mixed-initiative%2520process%2520combining%2520iterative%2520Q%2526A%250Awith%2520interactive%2520annotation.%2520Powered%2520by%2520a%2520large%2520language%2520model%252C%2520the%2520system%250Aanalyzes%2520user-supplied%2520datasets%252C%2520prompts%2520users%2520with%2520targeted%2520questions%252C%2520and%250Aallows%2520annotation%2520at%2520varying%2520levels%2520of%2520granularity.%2520The%2520resulting%2520structured%250Aknowledge%2520base%2520can%2520inform%2520both%2520human%2520and%2520automated%2520visualization%2520design.%2520We%250Aevaluated%2520the%2520tool%2520in%2520a%2520qualitative%2520study%2520involving%2520expert%2520pairs%2520from%2520Molecular%250ABiology%252C%2520Accounting%252C%2520Political%2520Science%252C%2520and%2520Usable%2520Security.%2520The%2520study%2520revealed%250Arecurring%2520patterns%2520in%2520how%2520experts%2520reason%2520about%2520their%2520data%2520and%2520highlights%2520areas%250Awhere%2520AI%2520support%2520can%2520improve%2520visualization%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Therapist%3A%20Eliciting%20Domain%20Knowledge%20from%20Subject%20Matter%20Experts%0A%20%20Using%20Large%20Language%20Models&entry.906535625=Sungbok%20Shin%20and%20Hyeon%20Jeon%20and%20Sanghyun%20Hong%20and%20Niklas%20Elmqvist&entry.1292438233=%20%20Effective%20data%20visualization%20requires%20not%20only%20technical%20proficiency%20but%20also%0Aa%20deep%20understanding%20of%20the%20domain-specific%20context%20in%20which%20data%20exists.%20This%0Acontext%20often%20includes%20tacit%20knowledge%20about%20data%20provenance%2C%20quality%2C%20and%0Aintended%20use%2C%20which%20is%20rarely%20explicit%20in%20the%20dataset%20itself.%20We%20present%20the%0AData%20Therapist%2C%20a%20web-based%20tool%20that%20helps%20domain%20experts%20externalize%20this%0Aimplicit%20knowledge%20through%20a%20mixed-initiative%20process%20combining%20iterative%20Q%26A%0Awith%20interactive%20annotation.%20Powered%20by%20a%20large%20language%20model%2C%20the%20system%0Aanalyzes%20user-supplied%20datasets%2C%20prompts%20users%20with%20targeted%20questions%2C%20and%0Aallows%20annotation%20at%20varying%20levels%20of%20granularity.%20The%20resulting%20structured%0Aknowledge%20base%20can%20inform%20both%20human%20and%20automated%20visualization%20design.%20We%0Aevaluated%20the%20tool%20in%20a%20qualitative%20study%20involving%20expert%20pairs%20from%20Molecular%0ABiology%2C%20Accounting%2C%20Political%20Science%2C%20and%20Usable%20Security.%20The%20study%20revealed%0Arecurring%20patterns%20in%20how%20experts%20reason%20about%20their%20data%20and%20highlights%20areas%0Awhere%20AI%20support%20can%20improve%20visualization%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00455v1&entry.124074799=Read"},
{"title": "SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic\n  Regression for high-fidelity material property prediction", "author": "Liu Junchi and Tang Ying and Tretiak Sergei and Duan Wenhui and Zhou Liujiang", "abstract": "  Recent advances in machine learning have demonstrated an enormous utility of\ndeep learning approaches, particularly Graph Neural Networks (GNNs) for\nmaterials science. These methods have emerged as powerful tools for\nhigh-throughput prediction of material properties, offering a compelling\nenhancement and alternative to traditional first-principles calculations. While\nthe community has predominantly focused on developing increasingly complex and\nuniversal models to enhance predictive accuracy, such approaches often lack\nphysical interpretability and insights into materials behavior. Here, we\nintroduce a novel computational paradigm, Self-Adaptable Graph Attention\nNetworks integrated with Symbolic Regression (SA-GAT-SR), that synergistically\ncombines the predictive capability of GNNs with the interpretative power of\nsymbolic regression. Our framework employs a self-adaptable encoding algorithm\nthat automatically identifies and adjust attention weights so as to screen\ncritical features from an expansive 180-dimensional feature space while\nmaintaining O(n) computational scaling. The integrated SR module subsequently\ndistills these features into compact analytical expressions that explicitly\nreveal quantum-mechanically meaningful relationships, achieving 23 times\nacceleration compared to conventional SR implementations that heavily rely on\nfirst principle calculations-derived features as input. This work suggests a\nnew framework in computational materials science, bridging the gap between\npredictive accuracy and physical interpretability, offering valuable physical\ninsights into material behavior.\n", "link": "http://arxiv.org/abs/2505.00625v1", "date": "2025-05-01", "relevancy": 2.0397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5038}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA-GAT-SR%3A%20Self-Adaptable%20Graph%20Attention%20Networks%20with%20Symbolic%0A%20%20Regression%20for%20high-fidelity%20material%20property%20prediction&body=Title%3A%20SA-GAT-SR%3A%20Self-Adaptable%20Graph%20Attention%20Networks%20with%20Symbolic%0A%20%20Regression%20for%20high-fidelity%20material%20property%20prediction%0AAuthor%3A%20Liu%20Junchi%20and%20Tang%20Ying%20and%20Tretiak%20Sergei%20and%20Duan%20Wenhui%20and%20Zhou%20Liujiang%0AAbstract%3A%20%20%20Recent%20advances%20in%20machine%20learning%20have%20demonstrated%20an%20enormous%20utility%20of%0Adeep%20learning%20approaches%2C%20particularly%20Graph%20Neural%20Networks%20%28GNNs%29%20for%0Amaterials%20science.%20These%20methods%20have%20emerged%20as%20powerful%20tools%20for%0Ahigh-throughput%20prediction%20of%20material%20properties%2C%20offering%20a%20compelling%0Aenhancement%20and%20alternative%20to%20traditional%20first-principles%20calculations.%20While%0Athe%20community%20has%20predominantly%20focused%20on%20developing%20increasingly%20complex%20and%0Auniversal%20models%20to%20enhance%20predictive%20accuracy%2C%20such%20approaches%20often%20lack%0Aphysical%20interpretability%20and%20insights%20into%20materials%20behavior.%20Here%2C%20we%0Aintroduce%20a%20novel%20computational%20paradigm%2C%20Self-Adaptable%20Graph%20Attention%0ANetworks%20integrated%20with%20Symbolic%20Regression%20%28SA-GAT-SR%29%2C%20that%20synergistically%0Acombines%20the%20predictive%20capability%20of%20GNNs%20with%20the%20interpretative%20power%20of%0Asymbolic%20regression.%20Our%20framework%20employs%20a%20self-adaptable%20encoding%20algorithm%0Athat%20automatically%20identifies%20and%20adjust%20attention%20weights%20so%20as%20to%20screen%0Acritical%20features%20from%20an%20expansive%20180-dimensional%20feature%20space%20while%0Amaintaining%20O%28n%29%20computational%20scaling.%20The%20integrated%20SR%20module%20subsequently%0Adistills%20these%20features%20into%20compact%20analytical%20expressions%20that%20explicitly%0Areveal%20quantum-mechanically%20meaningful%20relationships%2C%20achieving%2023%20times%0Aacceleration%20compared%20to%20conventional%20SR%20implementations%20that%20heavily%20rely%20on%0Afirst%20principle%20calculations-derived%20features%20as%20input.%20This%20work%20suggests%20a%0Anew%20framework%20in%20computational%20materials%20science%2C%20bridging%20the%20gap%20between%0Apredictive%20accuracy%20and%20physical%20interpretability%2C%20offering%20valuable%20physical%0Ainsights%20into%20material%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA-GAT-SR%253A%2520Self-Adaptable%2520Graph%2520Attention%2520Networks%2520with%2520Symbolic%250A%2520%2520Regression%2520for%2520high-fidelity%2520material%2520property%2520prediction%26entry.906535625%3DLiu%2520Junchi%2520and%2520Tang%2520Ying%2520and%2520Tretiak%2520Sergei%2520and%2520Duan%2520Wenhui%2520and%2520Zhou%2520Liujiang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520machine%2520learning%2520have%2520demonstrated%2520an%2520enormous%2520utility%2520of%250Adeep%2520learning%2520approaches%252C%2520particularly%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520for%250Amaterials%2520science.%2520These%2520methods%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%250Ahigh-throughput%2520prediction%2520of%2520material%2520properties%252C%2520offering%2520a%2520compelling%250Aenhancement%2520and%2520alternative%2520to%2520traditional%2520first-principles%2520calculations.%2520While%250Athe%2520community%2520has%2520predominantly%2520focused%2520on%2520developing%2520increasingly%2520complex%2520and%250Auniversal%2520models%2520to%2520enhance%2520predictive%2520accuracy%252C%2520such%2520approaches%2520often%2520lack%250Aphysical%2520interpretability%2520and%2520insights%2520into%2520materials%2520behavior.%2520Here%252C%2520we%250Aintroduce%2520a%2520novel%2520computational%2520paradigm%252C%2520Self-Adaptable%2520Graph%2520Attention%250ANetworks%2520integrated%2520with%2520Symbolic%2520Regression%2520%2528SA-GAT-SR%2529%252C%2520that%2520synergistically%250Acombines%2520the%2520predictive%2520capability%2520of%2520GNNs%2520with%2520the%2520interpretative%2520power%2520of%250Asymbolic%2520regression.%2520Our%2520framework%2520employs%2520a%2520self-adaptable%2520encoding%2520algorithm%250Athat%2520automatically%2520identifies%2520and%2520adjust%2520attention%2520weights%2520so%2520as%2520to%2520screen%250Acritical%2520features%2520from%2520an%2520expansive%2520180-dimensional%2520feature%2520space%2520while%250Amaintaining%2520O%2528n%2529%2520computational%2520scaling.%2520The%2520integrated%2520SR%2520module%2520subsequently%250Adistills%2520these%2520features%2520into%2520compact%2520analytical%2520expressions%2520that%2520explicitly%250Areveal%2520quantum-mechanically%2520meaningful%2520relationships%252C%2520achieving%252023%2520times%250Aacceleration%2520compared%2520to%2520conventional%2520SR%2520implementations%2520that%2520heavily%2520rely%2520on%250Afirst%2520principle%2520calculations-derived%2520features%2520as%2520input.%2520This%2520work%2520suggests%2520a%250Anew%2520framework%2520in%2520computational%2520materials%2520science%252C%2520bridging%2520the%2520gap%2520between%250Apredictive%2520accuracy%2520and%2520physical%2520interpretability%252C%2520offering%2520valuable%2520physical%250Ainsights%2520into%2520material%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA-GAT-SR%3A%20Self-Adaptable%20Graph%20Attention%20Networks%20with%20Symbolic%0A%20%20Regression%20for%20high-fidelity%20material%20property%20prediction&entry.906535625=Liu%20Junchi%20and%20Tang%20Ying%20and%20Tretiak%20Sergei%20and%20Duan%20Wenhui%20and%20Zhou%20Liujiang&entry.1292438233=%20%20Recent%20advances%20in%20machine%20learning%20have%20demonstrated%20an%20enormous%20utility%20of%0Adeep%20learning%20approaches%2C%20particularly%20Graph%20Neural%20Networks%20%28GNNs%29%20for%0Amaterials%20science.%20These%20methods%20have%20emerged%20as%20powerful%20tools%20for%0Ahigh-throughput%20prediction%20of%20material%20properties%2C%20offering%20a%20compelling%0Aenhancement%20and%20alternative%20to%20traditional%20first-principles%20calculations.%20While%0Athe%20community%20has%20predominantly%20focused%20on%20developing%20increasingly%20complex%20and%0Auniversal%20models%20to%20enhance%20predictive%20accuracy%2C%20such%20approaches%20often%20lack%0Aphysical%20interpretability%20and%20insights%20into%20materials%20behavior.%20Here%2C%20we%0Aintroduce%20a%20novel%20computational%20paradigm%2C%20Self-Adaptable%20Graph%20Attention%0ANetworks%20integrated%20with%20Symbolic%20Regression%20%28SA-GAT-SR%29%2C%20that%20synergistically%0Acombines%20the%20predictive%20capability%20of%20GNNs%20with%20the%20interpretative%20power%20of%0Asymbolic%20regression.%20Our%20framework%20employs%20a%20self-adaptable%20encoding%20algorithm%0Athat%20automatically%20identifies%20and%20adjust%20attention%20weights%20so%20as%20to%20screen%0Acritical%20features%20from%20an%20expansive%20180-dimensional%20feature%20space%20while%0Amaintaining%20O%28n%29%20computational%20scaling.%20The%20integrated%20SR%20module%20subsequently%0Adistills%20these%20features%20into%20compact%20analytical%20expressions%20that%20explicitly%0Areveal%20quantum-mechanically%20meaningful%20relationships%2C%20achieving%2023%20times%0Aacceleration%20compared%20to%20conventional%20SR%20implementations%20that%20heavily%20rely%20on%0Afirst%20principle%20calculations-derived%20features%20as%20input.%20This%20work%20suggests%20a%0Anew%20framework%20in%20computational%20materials%20science%2C%20bridging%20the%20gap%20between%0Apredictive%20accuracy%20and%20physical%20interpretability%2C%20offering%20valuable%20physical%0Ainsights%20into%20material%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00625v1&entry.124074799=Read"},
{"title": "Self-Ablating Transformers: More Interpretability, Less Sparsity", "author": "Jeremias Ferrao and Luhan Mikaelson and Keenan Pepper and Natalia Perez-Campanero Antolin", "abstract": "  A growing intuition in machine learning suggests a link between sparsity and\ninterpretability. We introduce a novel self-ablation mechanism to investigate\nthis connection ante-hoc in the context of language transformers. Our approach\ndynamically enforces a k-winner-takes-all constraint, forcing the model to\ndemonstrate selective activation across neuron and attention units. Unlike\npost-hoc methods that analyze already-trained models, our approach integrates\ninterpretability directly into model training, promoting feature localization\nfrom inception. Training small models on the TinyStories dataset and employing\ninterpretability tests, we find that self-ablation leads to more localized\ncircuits, concentrated feature representations, and increased neuron\nspecialization without compromising language modelling performance.\nSurprisingly, our method also decreased overall sparsity, indicating that\nself-ablation promotes specialization rather than widespread inactivity. This\nreveals a complex interplay between sparsity and interpretability, where\ndecreased global sparsity can coexist with increased local specialization,\nleading to enhanced interpretability. To facilitate reproducibility, we make\nour code available at\nhttps://github.com/keenanpepper/self-ablating-transformers.\n", "link": "http://arxiv.org/abs/2505.00509v1", "date": "2025-05-01", "relevancy": 2.0359, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Ablating%20Transformers%3A%20More%20Interpretability%2C%20Less%20Sparsity&body=Title%3A%20Self-Ablating%20Transformers%3A%20More%20Interpretability%2C%20Less%20Sparsity%0AAuthor%3A%20Jeremias%20Ferrao%20and%20Luhan%20Mikaelson%20and%20Keenan%20Pepper%20and%20Natalia%20Perez-Campanero%20Antolin%0AAbstract%3A%20%20%20A%20growing%20intuition%20in%20machine%20learning%20suggests%20a%20link%20between%20sparsity%20and%0Ainterpretability.%20We%20introduce%20a%20novel%20self-ablation%20mechanism%20to%20investigate%0Athis%20connection%20ante-hoc%20in%20the%20context%20of%20language%20transformers.%20Our%20approach%0Adynamically%20enforces%20a%20k-winner-takes-all%20constraint%2C%20forcing%20the%20model%20to%0Ademonstrate%20selective%20activation%20across%20neuron%20and%20attention%20units.%20Unlike%0Apost-hoc%20methods%20that%20analyze%20already-trained%20models%2C%20our%20approach%20integrates%0Ainterpretability%20directly%20into%20model%20training%2C%20promoting%20feature%20localization%0Afrom%20inception.%20Training%20small%20models%20on%20the%20TinyStories%20dataset%20and%20employing%0Ainterpretability%20tests%2C%20we%20find%20that%20self-ablation%20leads%20to%20more%20localized%0Acircuits%2C%20concentrated%20feature%20representations%2C%20and%20increased%20neuron%0Aspecialization%20without%20compromising%20language%20modelling%20performance.%0ASurprisingly%2C%20our%20method%20also%20decreased%20overall%20sparsity%2C%20indicating%20that%0Aself-ablation%20promotes%20specialization%20rather%20than%20widespread%20inactivity.%20This%0Areveals%20a%20complex%20interplay%20between%20sparsity%20and%20interpretability%2C%20where%0Adecreased%20global%20sparsity%20can%20coexist%20with%20increased%20local%20specialization%2C%0Aleading%20to%20enhanced%20interpretability.%20To%20facilitate%20reproducibility%2C%20we%20make%0Aour%20code%20available%20at%0Ahttps%3A//github.com/keenanpepper/self-ablating-transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Ablating%2520Transformers%253A%2520More%2520Interpretability%252C%2520Less%2520Sparsity%26entry.906535625%3DJeremias%2520Ferrao%2520and%2520Luhan%2520Mikaelson%2520and%2520Keenan%2520Pepper%2520and%2520Natalia%2520Perez-Campanero%2520Antolin%26entry.1292438233%3D%2520%2520A%2520growing%2520intuition%2520in%2520machine%2520learning%2520suggests%2520a%2520link%2520between%2520sparsity%2520and%250Ainterpretability.%2520We%2520introduce%2520a%2520novel%2520self-ablation%2520mechanism%2520to%2520investigate%250Athis%2520connection%2520ante-hoc%2520in%2520the%2520context%2520of%2520language%2520transformers.%2520Our%2520approach%250Adynamically%2520enforces%2520a%2520k-winner-takes-all%2520constraint%252C%2520forcing%2520the%2520model%2520to%250Ademonstrate%2520selective%2520activation%2520across%2520neuron%2520and%2520attention%2520units.%2520Unlike%250Apost-hoc%2520methods%2520that%2520analyze%2520already-trained%2520models%252C%2520our%2520approach%2520integrates%250Ainterpretability%2520directly%2520into%2520model%2520training%252C%2520promoting%2520feature%2520localization%250Afrom%2520inception.%2520Training%2520small%2520models%2520on%2520the%2520TinyStories%2520dataset%2520and%2520employing%250Ainterpretability%2520tests%252C%2520we%2520find%2520that%2520self-ablation%2520leads%2520to%2520more%2520localized%250Acircuits%252C%2520concentrated%2520feature%2520representations%252C%2520and%2520increased%2520neuron%250Aspecialization%2520without%2520compromising%2520language%2520modelling%2520performance.%250ASurprisingly%252C%2520our%2520method%2520also%2520decreased%2520overall%2520sparsity%252C%2520indicating%2520that%250Aself-ablation%2520promotes%2520specialization%2520rather%2520than%2520widespread%2520inactivity.%2520This%250Areveals%2520a%2520complex%2520interplay%2520between%2520sparsity%2520and%2520interpretability%252C%2520where%250Adecreased%2520global%2520sparsity%2520can%2520coexist%2520with%2520increased%2520local%2520specialization%252C%250Aleading%2520to%2520enhanced%2520interpretability.%2520To%2520facilitate%2520reproducibility%252C%2520we%2520make%250Aour%2520code%2520available%2520at%250Ahttps%253A//github.com/keenanpepper/self-ablating-transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Ablating%20Transformers%3A%20More%20Interpretability%2C%20Less%20Sparsity&entry.906535625=Jeremias%20Ferrao%20and%20Luhan%20Mikaelson%20and%20Keenan%20Pepper%20and%20Natalia%20Perez-Campanero%20Antolin&entry.1292438233=%20%20A%20growing%20intuition%20in%20machine%20learning%20suggests%20a%20link%20between%20sparsity%20and%0Ainterpretability.%20We%20introduce%20a%20novel%20self-ablation%20mechanism%20to%20investigate%0Athis%20connection%20ante-hoc%20in%20the%20context%20of%20language%20transformers.%20Our%20approach%0Adynamically%20enforces%20a%20k-winner-takes-all%20constraint%2C%20forcing%20the%20model%20to%0Ademonstrate%20selective%20activation%20across%20neuron%20and%20attention%20units.%20Unlike%0Apost-hoc%20methods%20that%20analyze%20already-trained%20models%2C%20our%20approach%20integrates%0Ainterpretability%20directly%20into%20model%20training%2C%20promoting%20feature%20localization%0Afrom%20inception.%20Training%20small%20models%20on%20the%20TinyStories%20dataset%20and%20employing%0Ainterpretability%20tests%2C%20we%20find%20that%20self-ablation%20leads%20to%20more%20localized%0Acircuits%2C%20concentrated%20feature%20representations%2C%20and%20increased%20neuron%0Aspecialization%20without%20compromising%20language%20modelling%20performance.%0ASurprisingly%2C%20our%20method%20also%20decreased%20overall%20sparsity%2C%20indicating%20that%0Aself-ablation%20promotes%20specialization%20rather%20than%20widespread%20inactivity.%20This%0Areveals%20a%20complex%20interplay%20between%20sparsity%20and%20interpretability%2C%20where%0Adecreased%20global%20sparsity%20can%20coexist%20with%20increased%20local%20specialization%2C%0Aleading%20to%20enhanced%20interpretability.%20To%20facilitate%20reproducibility%2C%20we%20make%0Aour%20code%20available%20at%0Ahttps%3A//github.com/keenanpepper/self-ablating-transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00509v1&entry.124074799=Read"},
{"title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase\n  Transition in Large Language Models", "author": "Makoto Sato", "abstract": "  What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)-either fused together or presented separately-by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept-a form of conceptual fusion-current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds.\n", "link": "http://arxiv.org/abs/2504.21012v2", "date": "2025-05-01", "relevancy": 2.0174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Waking%20Up%20an%20AI%3A%20A%20Quantitative%20Framework%20for%20Prompt-Induced%20Phase%0A%20%20Transition%20in%20Large%20Language%20Models&body=Title%3A%20Waking%20Up%20an%20AI%3A%20A%20Quantitative%20Framework%20for%20Prompt-Induced%20Phase%0A%20%20Transition%20in%20Large%20Language%20Models%0AAuthor%3A%20Makoto%20Sato%0AAbstract%3A%20%20%20What%20underlies%20intuitive%20human%20thinking%3F%20One%20approach%20to%20this%20question%20is%20to%0Acompare%20the%20cognitive%20dynamics%20of%20humans%20and%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20such%20a%20comparison%20requires%20a%20method%20to%20quantitatively%20analyze%20AI%0Acognitive%20behavior%20under%20controlled%20conditions.%20While%20anecdotal%20observations%0Asuggest%20that%20certain%20prompts%20can%20dramatically%20change%20LLM%20behavior%2C%20these%0Aobservations%20have%20remained%20largely%20qualitative.%20Here%2C%20we%20propose%20a%20two-part%0Aframework%20to%20investigate%20this%20phenomenon%3A%20a%20Transition-Inducing%20Prompt%20%28TIP%29%0Athat%20triggers%20a%20rapid%20shift%20in%20LLM%20responsiveness%2C%20and%20a%20Transition%20Quantifying%0APrompt%20%28TQP%29%20that%20evaluates%20this%20change%20using%20a%20separate%20LLM.%20Through%0Acontrolled%20experiments%2C%20we%20examined%20how%20LLMs%20react%20to%20prompts%20embedding%20two%0Asemantically%20distant%20concepts%20%28e.g.%2C%20mathematical%20aperiodicity%20and%20traditional%0Acrafts%29-either%20fused%20together%20or%20presented%20separately-by%20changing%20their%0Alinguistic%20quality%20and%20affective%20tone.%20Whereas%20humans%20tend%20to%20experience%0Aheightened%20engagement%20when%20such%20concepts%20are%20meaningfully%20blended%20producing%20a%0Anovel%20concept-a%20form%20of%20conceptual%20fusion-current%20LLMs%20showed%20no%20significant%0Adifference%20in%20responsiveness%20between%20semantically%20fused%20and%20non-fused%20prompts.%0AThis%20suggests%20that%20LLMs%20may%20not%20yet%20replicate%20the%20conceptual%20integration%0Aprocesses%20seen%20in%20human%20intuition.%20Our%20method%20enables%20fine-grained%2C%0Areproducible%20measurement%20of%20cognitive%20responsiveness%2C%20and%20may%20help%20illuminate%0Akey%20differences%20in%20how%20intuition%20and%20conceptual%20leaps%20emerge%20in%20artificial%0Aversus%20human%20minds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaking%2520Up%2520an%2520AI%253A%2520A%2520Quantitative%2520Framework%2520for%2520Prompt-Induced%2520Phase%250A%2520%2520Transition%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMakoto%2520Sato%26entry.1292438233%3D%2520%2520What%2520underlies%2520intuitive%2520human%2520thinking%253F%2520One%2520approach%2520to%2520this%2520question%2520is%2520to%250Acompare%2520the%2520cognitive%2520dynamics%2520of%2520humans%2520and%2520large%2520language%2520models%2520%2528LLMs%2529.%250AHowever%252C%2520such%2520a%2520comparison%2520requires%2520a%2520method%2520to%2520quantitatively%2520analyze%2520AI%250Acognitive%2520behavior%2520under%2520controlled%2520conditions.%2520While%2520anecdotal%2520observations%250Asuggest%2520that%2520certain%2520prompts%2520can%2520dramatically%2520change%2520LLM%2520behavior%252C%2520these%250Aobservations%2520have%2520remained%2520largely%2520qualitative.%2520Here%252C%2520we%2520propose%2520a%2520two-part%250Aframework%2520to%2520investigate%2520this%2520phenomenon%253A%2520a%2520Transition-Inducing%2520Prompt%2520%2528TIP%2529%250Athat%2520triggers%2520a%2520rapid%2520shift%2520in%2520LLM%2520responsiveness%252C%2520and%2520a%2520Transition%2520Quantifying%250APrompt%2520%2528TQP%2529%2520that%2520evaluates%2520this%2520change%2520using%2520a%2520separate%2520LLM.%2520Through%250Acontrolled%2520experiments%252C%2520we%2520examined%2520how%2520LLMs%2520react%2520to%2520prompts%2520embedding%2520two%250Asemantically%2520distant%2520concepts%2520%2528e.g.%252C%2520mathematical%2520aperiodicity%2520and%2520traditional%250Acrafts%2529-either%2520fused%2520together%2520or%2520presented%2520separately-by%2520changing%2520their%250Alinguistic%2520quality%2520and%2520affective%2520tone.%2520Whereas%2520humans%2520tend%2520to%2520experience%250Aheightened%2520engagement%2520when%2520such%2520concepts%2520are%2520meaningfully%2520blended%2520producing%2520a%250Anovel%2520concept-a%2520form%2520of%2520conceptual%2520fusion-current%2520LLMs%2520showed%2520no%2520significant%250Adifference%2520in%2520responsiveness%2520between%2520semantically%2520fused%2520and%2520non-fused%2520prompts.%250AThis%2520suggests%2520that%2520LLMs%2520may%2520not%2520yet%2520replicate%2520the%2520conceptual%2520integration%250Aprocesses%2520seen%2520in%2520human%2520intuition.%2520Our%2520method%2520enables%2520fine-grained%252C%250Areproducible%2520measurement%2520of%2520cognitive%2520responsiveness%252C%2520and%2520may%2520help%2520illuminate%250Akey%2520differences%2520in%2520how%2520intuition%2520and%2520conceptual%2520leaps%2520emerge%2520in%2520artificial%250Aversus%2520human%2520minds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Waking%20Up%20an%20AI%3A%20A%20Quantitative%20Framework%20for%20Prompt-Induced%20Phase%0A%20%20Transition%20in%20Large%20Language%20Models&entry.906535625=Makoto%20Sato&entry.1292438233=%20%20What%20underlies%20intuitive%20human%20thinking%3F%20One%20approach%20to%20this%20question%20is%20to%0Acompare%20the%20cognitive%20dynamics%20of%20humans%20and%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20such%20a%20comparison%20requires%20a%20method%20to%20quantitatively%20analyze%20AI%0Acognitive%20behavior%20under%20controlled%20conditions.%20While%20anecdotal%20observations%0Asuggest%20that%20certain%20prompts%20can%20dramatically%20change%20LLM%20behavior%2C%20these%0Aobservations%20have%20remained%20largely%20qualitative.%20Here%2C%20we%20propose%20a%20two-part%0Aframework%20to%20investigate%20this%20phenomenon%3A%20a%20Transition-Inducing%20Prompt%20%28TIP%29%0Athat%20triggers%20a%20rapid%20shift%20in%20LLM%20responsiveness%2C%20and%20a%20Transition%20Quantifying%0APrompt%20%28TQP%29%20that%20evaluates%20this%20change%20using%20a%20separate%20LLM.%20Through%0Acontrolled%20experiments%2C%20we%20examined%20how%20LLMs%20react%20to%20prompts%20embedding%20two%0Asemantically%20distant%20concepts%20%28e.g.%2C%20mathematical%20aperiodicity%20and%20traditional%0Acrafts%29-either%20fused%20together%20or%20presented%20separately-by%20changing%20their%0Alinguistic%20quality%20and%20affective%20tone.%20Whereas%20humans%20tend%20to%20experience%0Aheightened%20engagement%20when%20such%20concepts%20are%20meaningfully%20blended%20producing%20a%0Anovel%20concept-a%20form%20of%20conceptual%20fusion-current%20LLMs%20showed%20no%20significant%0Adifference%20in%20responsiveness%20between%20semantically%20fused%20and%20non-fused%20prompts.%0AThis%20suggests%20that%20LLMs%20may%20not%20yet%20replicate%20the%20conceptual%20integration%0Aprocesses%20seen%20in%20human%20intuition.%20Our%20method%20enables%20fine-grained%2C%0Areproducible%20measurement%20of%20cognitive%20responsiveness%2C%20and%20may%20help%20illuminate%0Akey%20differences%20in%20how%20intuition%20and%20conceptual%20leaps%20emerge%20in%20artificial%0Aversus%20human%20minds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21012v2&entry.124074799=Read"},
{"title": "Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors", "author": "Xinyu Ding and Lexuan Chen and Siyu Liao and Zhongfeng Wang", "abstract": "  Foundation models have achieved tremendous success in different domains.\nHowever, their huge computation and storage complexity make these models\ndifficult to fine-tune and also less applicable in practice. Recent study shows\ntraining in Fourier domain can be an effective fine-tuning method in terms of\nboth model performance and number of training parameters. In this work, we\npropose to further reduce the complexity by the factorization through the\nproduct of interleaved circulant and diagonal matrices. In addition, we address\nthe case of non-square fine-tuning weights by partitioning the circulant matrix\ninto blocks. Our method avoids the construction of weight change matrix and\nutilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental\nresults show that our method achieves similar or better performance across\nvarious tasks with much less floating-point operations (FLOPs) and the number\nof trainable parameters.\n", "link": "http://arxiv.org/abs/2505.00580v1", "date": "2025-05-01", "relevancy": 2.0136, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5104}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5035}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Fine-Tuning%20with%20Circulant%20and%20Diagonal%20Vectors&body=Title%3A%20Parameter-Efficient%20Fine-Tuning%20with%20Circulant%20and%20Diagonal%20Vectors%0AAuthor%3A%20Xinyu%20Ding%20and%20Lexuan%20Chen%20and%20Siyu%20Liao%20and%20Zhongfeng%20Wang%0AAbstract%3A%20%20%20Foundation%20models%20have%20achieved%20tremendous%20success%20in%20different%20domains.%0AHowever%2C%20their%20huge%20computation%20and%20storage%20complexity%20make%20these%20models%0Adifficult%20to%20fine-tune%20and%20also%20less%20applicable%20in%20practice.%20Recent%20study%20shows%0Atraining%20in%20Fourier%20domain%20can%20be%20an%20effective%20fine-tuning%20method%20in%20terms%20of%0Aboth%20model%20performance%20and%20number%20of%20training%20parameters.%20In%20this%20work%2C%20we%0Apropose%20to%20further%20reduce%20the%20complexity%20by%20the%20factorization%20through%20the%0Aproduct%20of%20interleaved%20circulant%20and%20diagonal%20matrices.%20In%20addition%2C%20we%20address%0Athe%20case%20of%20non-square%20fine-tuning%20weights%20by%20partitioning%20the%20circulant%20matrix%0Ainto%20blocks.%20Our%20method%20avoids%20the%20construction%20of%20weight%20change%20matrix%20and%0Autilizes%201D%20fast%20Fourier%20transform%20%28FFT%29%20instead%20of%202D%20FFT.%20Experimental%0Aresults%20show%20that%20our%20method%20achieves%20similar%20or%20better%20performance%20across%0Avarious%20tasks%20with%20much%20less%20floating-point%20operations%20%28FLOPs%29%20and%20the%20number%0Aof%20trainable%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Fine-Tuning%2520with%2520Circulant%2520and%2520Diagonal%2520Vectors%26entry.906535625%3DXinyu%2520Ding%2520and%2520Lexuan%2520Chen%2520and%2520Siyu%2520Liao%2520and%2520Zhongfeng%2520Wang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520achieved%2520tremendous%2520success%2520in%2520different%2520domains.%250AHowever%252C%2520their%2520huge%2520computation%2520and%2520storage%2520complexity%2520make%2520these%2520models%250Adifficult%2520to%2520fine-tune%2520and%2520also%2520less%2520applicable%2520in%2520practice.%2520Recent%2520study%2520shows%250Atraining%2520in%2520Fourier%2520domain%2520can%2520be%2520an%2520effective%2520fine-tuning%2520method%2520in%2520terms%2520of%250Aboth%2520model%2520performance%2520and%2520number%2520of%2520training%2520parameters.%2520In%2520this%2520work%252C%2520we%250Apropose%2520to%2520further%2520reduce%2520the%2520complexity%2520by%2520the%2520factorization%2520through%2520the%250Aproduct%2520of%2520interleaved%2520circulant%2520and%2520diagonal%2520matrices.%2520In%2520addition%252C%2520we%2520address%250Athe%2520case%2520of%2520non-square%2520fine-tuning%2520weights%2520by%2520partitioning%2520the%2520circulant%2520matrix%250Ainto%2520blocks.%2520Our%2520method%2520avoids%2520the%2520construction%2520of%2520weight%2520change%2520matrix%2520and%250Autilizes%25201D%2520fast%2520Fourier%2520transform%2520%2528FFT%2529%2520instead%2520of%25202D%2520FFT.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%2520achieves%2520similar%2520or%2520better%2520performance%2520across%250Avarious%2520tasks%2520with%2520much%2520less%2520floating-point%2520operations%2520%2528FLOPs%2529%2520and%2520the%2520number%250Aof%2520trainable%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Fine-Tuning%20with%20Circulant%20and%20Diagonal%20Vectors&entry.906535625=Xinyu%20Ding%20and%20Lexuan%20Chen%20and%20Siyu%20Liao%20and%20Zhongfeng%20Wang&entry.1292438233=%20%20Foundation%20models%20have%20achieved%20tremendous%20success%20in%20different%20domains.%0AHowever%2C%20their%20huge%20computation%20and%20storage%20complexity%20make%20these%20models%0Adifficult%20to%20fine-tune%20and%20also%20less%20applicable%20in%20practice.%20Recent%20study%20shows%0Atraining%20in%20Fourier%20domain%20can%20be%20an%20effective%20fine-tuning%20method%20in%20terms%20of%0Aboth%20model%20performance%20and%20number%20of%20training%20parameters.%20In%20this%20work%2C%20we%0Apropose%20to%20further%20reduce%20the%20complexity%20by%20the%20factorization%20through%20the%0Aproduct%20of%20interleaved%20circulant%20and%20diagonal%20matrices.%20In%20addition%2C%20we%20address%0Athe%20case%20of%20non-square%20fine-tuning%20weights%20by%20partitioning%20the%20circulant%20matrix%0Ainto%20blocks.%20Our%20method%20avoids%20the%20construction%20of%20weight%20change%20matrix%20and%0Autilizes%201D%20fast%20Fourier%20transform%20%28FFT%29%20instead%20of%202D%20FFT.%20Experimental%0Aresults%20show%20that%20our%20method%20achieves%20similar%20or%20better%20performance%20across%0Avarious%20tasks%20with%20much%20less%20floating-point%20operations%20%28FLOPs%29%20and%20the%20number%0Aof%20trainable%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00580v1&entry.124074799=Read"},
{"title": "Instantiation-based Formalization of Logical Reasoning Tasks using\n  Language Models and Logical Solvers", "author": "Mohammad Raza and Natasa Milic-Frayling", "abstract": "  Robustness of reasoning remains a significant challenge for large language\nmodels, and addressing it is essential for the practical applicability of\nAI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a\nnovel approach that addresses the key challenge in combining language models\nwith the rigor of logical solvers: to accurately formulate the reasoning\nproblem from natural language to the formal language of the solver. SSV uses a\nconsistency-based approach to produce strong abstract formalizations of\nproblems using concrete instantiations that are generated by the model and\nverified by the solver. In addition to significantly advancing the overall\nreasoning accuracy over the state-of-the-art, a key novelty that this approach\npresents is a feature of verification that has near-perfect precision over a\nsignificant coverage of cases, as we demonstrate on open reasoning benchmarks.\nWe propose such *near-certain reasoning* as a new approach to reduce the need\nfor manual verification in many cases, taking us closer to more dependable and\nautonomous AI reasoning systems.\n", "link": "http://arxiv.org/abs/2501.16961v2", "date": "2025-05-01", "relevancy": 2.004, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instantiation-based%20Formalization%20of%20Logical%20Reasoning%20Tasks%20using%0A%20%20Language%20Models%20and%20Logical%20Solvers&body=Title%3A%20Instantiation-based%20Formalization%20of%20Logical%20Reasoning%20Tasks%20using%0A%20%20Language%20Models%20and%20Logical%20Solvers%0AAuthor%3A%20Mohammad%20Raza%20and%20Natasa%20Milic-Frayling%0AAbstract%3A%20%20%20Robustness%20of%20reasoning%20remains%20a%20significant%20challenge%20for%20large%20language%0Amodels%2C%20and%20addressing%20it%20is%20essential%20for%20the%20practical%20applicability%20of%0AAI-driven%20reasoning%20systems.%20We%20introduce%20Semantic%20Self-Verification%20%28SSV%29%2C%20a%0Anovel%20approach%20that%20addresses%20the%20key%20challenge%20in%20combining%20language%20models%0Awith%20the%20rigor%20of%20logical%20solvers%3A%20to%20accurately%20formulate%20the%20reasoning%0Aproblem%20from%20natural%20language%20to%20the%20formal%20language%20of%20the%20solver.%20SSV%20uses%20a%0Aconsistency-based%20approach%20to%20produce%20strong%20abstract%20formalizations%20of%0Aproblems%20using%20concrete%20instantiations%20that%20are%20generated%20by%20the%20model%20and%0Averified%20by%20the%20solver.%20In%20addition%20to%20significantly%20advancing%20the%20overall%0Areasoning%20accuracy%20over%20the%20state-of-the-art%2C%20a%20key%20novelty%20that%20this%20approach%0Apresents%20is%20a%20feature%20of%20verification%20that%20has%20near-perfect%20precision%20over%20a%0Asignificant%20coverage%20of%20cases%2C%20as%20we%20demonstrate%20on%20open%20reasoning%20benchmarks.%0AWe%20propose%20such%20%2Anear-certain%20reasoning%2A%20as%20a%20new%20approach%20to%20reduce%20the%20need%0Afor%20manual%20verification%20in%20many%20cases%2C%20taking%20us%20closer%20to%20more%20dependable%20and%0Aautonomous%20AI%20reasoning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstantiation-based%2520Formalization%2520of%2520Logical%2520Reasoning%2520Tasks%2520using%250A%2520%2520Language%2520Models%2520and%2520Logical%2520Solvers%26entry.906535625%3DMohammad%2520Raza%2520and%2520Natasa%2520Milic-Frayling%26entry.1292438233%3D%2520%2520Robustness%2520of%2520reasoning%2520remains%2520a%2520significant%2520challenge%2520for%2520large%2520language%250Amodels%252C%2520and%2520addressing%2520it%2520is%2520essential%2520for%2520the%2520practical%2520applicability%2520of%250AAI-driven%2520reasoning%2520systems.%2520We%2520introduce%2520Semantic%2520Self-Verification%2520%2528SSV%2529%252C%2520a%250Anovel%2520approach%2520that%2520addresses%2520the%2520key%2520challenge%2520in%2520combining%2520language%2520models%250Awith%2520the%2520rigor%2520of%2520logical%2520solvers%253A%2520to%2520accurately%2520formulate%2520the%2520reasoning%250Aproblem%2520from%2520natural%2520language%2520to%2520the%2520formal%2520language%2520of%2520the%2520solver.%2520SSV%2520uses%2520a%250Aconsistency-based%2520approach%2520to%2520produce%2520strong%2520abstract%2520formalizations%2520of%250Aproblems%2520using%2520concrete%2520instantiations%2520that%2520are%2520generated%2520by%2520the%2520model%2520and%250Averified%2520by%2520the%2520solver.%2520In%2520addition%2520to%2520significantly%2520advancing%2520the%2520overall%250Areasoning%2520accuracy%2520over%2520the%2520state-of-the-art%252C%2520a%2520key%2520novelty%2520that%2520this%2520approach%250Apresents%2520is%2520a%2520feature%2520of%2520verification%2520that%2520has%2520near-perfect%2520precision%2520over%2520a%250Asignificant%2520coverage%2520of%2520cases%252C%2520as%2520we%2520demonstrate%2520on%2520open%2520reasoning%2520benchmarks.%250AWe%2520propose%2520such%2520%252Anear-certain%2520reasoning%252A%2520as%2520a%2520new%2520approach%2520to%2520reduce%2520the%2520need%250Afor%2520manual%2520verification%2520in%2520many%2520cases%252C%2520taking%2520us%2520closer%2520to%2520more%2520dependable%2520and%250Aautonomous%2520AI%2520reasoning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instantiation-based%20Formalization%20of%20Logical%20Reasoning%20Tasks%20using%0A%20%20Language%20Models%20and%20Logical%20Solvers&entry.906535625=Mohammad%20Raza%20and%20Natasa%20Milic-Frayling&entry.1292438233=%20%20Robustness%20of%20reasoning%20remains%20a%20significant%20challenge%20for%20large%20language%0Amodels%2C%20and%20addressing%20it%20is%20essential%20for%20the%20practical%20applicability%20of%0AAI-driven%20reasoning%20systems.%20We%20introduce%20Semantic%20Self-Verification%20%28SSV%29%2C%20a%0Anovel%20approach%20that%20addresses%20the%20key%20challenge%20in%20combining%20language%20models%0Awith%20the%20rigor%20of%20logical%20solvers%3A%20to%20accurately%20formulate%20the%20reasoning%0Aproblem%20from%20natural%20language%20to%20the%20formal%20language%20of%20the%20solver.%20SSV%20uses%20a%0Aconsistency-based%20approach%20to%20produce%20strong%20abstract%20formalizations%20of%0Aproblems%20using%20concrete%20instantiations%20that%20are%20generated%20by%20the%20model%20and%0Averified%20by%20the%20solver.%20In%20addition%20to%20significantly%20advancing%20the%20overall%0Areasoning%20accuracy%20over%20the%20state-of-the-art%2C%20a%20key%20novelty%20that%20this%20approach%0Apresents%20is%20a%20feature%20of%20verification%20that%20has%20near-perfect%20precision%20over%20a%0Asignificant%20coverage%20of%20cases%2C%20as%20we%20demonstrate%20on%20open%20reasoning%20benchmarks.%0AWe%20propose%20such%20%2Anear-certain%20reasoning%2A%20as%20a%20new%20approach%20to%20reduce%20the%20need%0Afor%20manual%20verification%20in%20many%20cases%2C%20taking%20us%20closer%20to%20more%20dependable%20and%0Aautonomous%20AI%20reasoning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16961v2&entry.124074799=Read"},
{"title": "Test-time Correlation Alignment", "author": "Linjing You and Jiabao Lu and Xiayuan Huang", "abstract": "  Deep neural networks often experience performance drops due to distribution\nshifts between training and test data. Although domain adaptation offers a\nsolution, privacy concerns restrict access to training data in many real-world\nscenarios. This restriction has spurred interest in Test-Time Adaptation (TTA),\nwhich adapts models using only unlabeled test data. However, current TTA\nmethods still face practical challenges: (1) a primary focus on instance-wise\nalignment, overlooking CORrelation ALignment (CORAL) due to missing source\ncorrelations; (2) complex backpropagation operations for model updating,\nresulting in overhead computation and (3) domain forgetting.\n  To address these challenges, we provide a theoretical analysis to investigate\nthe feasibility of Test-time Correlation Alignment (TCA), demonstrating that\ncorrelation alignment between high-certainty instances and test instances can\nenhance test performances with a theoretical guarantee. Based on this, we\npropose two simple yet effective algorithms: LinearTCA and LinearTCA+.\nLinearTCA applies a simple linear transformation to achieve both instance and\ncorrelation alignment without additional model updates, while LinearTCA+ serves\nas a plug-and-play module that can easily boost existing TTA methods. Extensive\nexperiments validate our theoretical insights and show that TCA methods\nsignificantly outperforms baselines across various tasks, benchmarks and\nbackbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on\nOfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6%\ncomputation time compared to the best baseline TTA method.\n", "link": "http://arxiv.org/abs/2505.00533v1", "date": "2025-05-01", "relevancy": 1.987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4827}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20Correlation%20Alignment&body=Title%3A%20Test-time%20Correlation%20Alignment%0AAuthor%3A%20Linjing%20You%20and%20Jiabao%20Lu%20and%20Xiayuan%20Huang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20experience%20performance%20drops%20due%20to%20distribution%0Ashifts%20between%20training%20and%20test%20data.%20Although%20domain%20adaptation%20offers%20a%0Asolution%2C%20privacy%20concerns%20restrict%20access%20to%20training%20data%20in%20many%20real-world%0Ascenarios.%20This%20restriction%20has%20spurred%20interest%20in%20Test-Time%20Adaptation%20%28TTA%29%2C%0Awhich%20adapts%20models%20using%20only%20unlabeled%20test%20data.%20However%2C%20current%20TTA%0Amethods%20still%20face%20practical%20challenges%3A%20%281%29%20a%20primary%20focus%20on%20instance-wise%0Aalignment%2C%20overlooking%20CORrelation%20ALignment%20%28CORAL%29%20due%20to%20missing%20source%0Acorrelations%3B%20%282%29%20complex%20backpropagation%20operations%20for%20model%20updating%2C%0Aresulting%20in%20overhead%20computation%20and%20%283%29%20domain%20forgetting.%0A%20%20To%20address%20these%20challenges%2C%20we%20provide%20a%20theoretical%20analysis%20to%20investigate%0Athe%20feasibility%20of%20Test-time%20Correlation%20Alignment%20%28TCA%29%2C%20demonstrating%20that%0Acorrelation%20alignment%20between%20high-certainty%20instances%20and%20test%20instances%20can%0Aenhance%20test%20performances%20with%20a%20theoretical%20guarantee.%20Based%20on%20this%2C%20we%0Apropose%20two%20simple%20yet%20effective%20algorithms%3A%20LinearTCA%20and%20LinearTCA%2B.%0ALinearTCA%20applies%20a%20simple%20linear%20transformation%20to%20achieve%20both%20instance%20and%0Acorrelation%20alignment%20without%20additional%20model%20updates%2C%20while%20LinearTCA%2B%20serves%0Aas%20a%20plug-and-play%20module%20that%20can%20easily%20boost%20existing%20TTA%20methods.%20Extensive%0Aexperiments%20validate%20our%20theoretical%20insights%20and%20show%20that%20TCA%20methods%0Asignificantly%20outperforms%20baselines%20across%20various%20tasks%2C%20benchmarks%20and%0Abackbones.%20Notably%2C%20LinearTCA%20improves%20adaptation%20accuracy%20by%205.88%25%20on%0AOfficeHome%20dataset%2C%20while%20using%20only%204%25%20maximum%20GPU%20memory%20usage%20and%200.6%25%0Acomputation%20time%20compared%20to%20the%20best%20baseline%20TTA%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520Correlation%2520Alignment%26entry.906535625%3DLinjing%2520You%2520and%2520Jiabao%2520Lu%2520and%2520Xiayuan%2520Huang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520often%2520experience%2520performance%2520drops%2520due%2520to%2520distribution%250Ashifts%2520between%2520training%2520and%2520test%2520data.%2520Although%2520domain%2520adaptation%2520offers%2520a%250Asolution%252C%2520privacy%2520concerns%2520restrict%2520access%2520to%2520training%2520data%2520in%2520many%2520real-world%250Ascenarios.%2520This%2520restriction%2520has%2520spurred%2520interest%2520in%2520Test-Time%2520Adaptation%2520%2528TTA%2529%252C%250Awhich%2520adapts%2520models%2520using%2520only%2520unlabeled%2520test%2520data.%2520However%252C%2520current%2520TTA%250Amethods%2520still%2520face%2520practical%2520challenges%253A%2520%25281%2529%2520a%2520primary%2520focus%2520on%2520instance-wise%250Aalignment%252C%2520overlooking%2520CORrelation%2520ALignment%2520%2528CORAL%2529%2520due%2520to%2520missing%2520source%250Acorrelations%253B%2520%25282%2529%2520complex%2520backpropagation%2520operations%2520for%2520model%2520updating%252C%250Aresulting%2520in%2520overhead%2520computation%2520and%2520%25283%2529%2520domain%2520forgetting.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520to%2520investigate%250Athe%2520feasibility%2520of%2520Test-time%2520Correlation%2520Alignment%2520%2528TCA%2529%252C%2520demonstrating%2520that%250Acorrelation%2520alignment%2520between%2520high-certainty%2520instances%2520and%2520test%2520instances%2520can%250Aenhance%2520test%2520performances%2520with%2520a%2520theoretical%2520guarantee.%2520Based%2520on%2520this%252C%2520we%250Apropose%2520two%2520simple%2520yet%2520effective%2520algorithms%253A%2520LinearTCA%2520and%2520LinearTCA%252B.%250ALinearTCA%2520applies%2520a%2520simple%2520linear%2520transformation%2520to%2520achieve%2520both%2520instance%2520and%250Acorrelation%2520alignment%2520without%2520additional%2520model%2520updates%252C%2520while%2520LinearTCA%252B%2520serves%250Aas%2520a%2520plug-and-play%2520module%2520that%2520can%2520easily%2520boost%2520existing%2520TTA%2520methods.%2520Extensive%250Aexperiments%2520validate%2520our%2520theoretical%2520insights%2520and%2520show%2520that%2520TCA%2520methods%250Asignificantly%2520outperforms%2520baselines%2520across%2520various%2520tasks%252C%2520benchmarks%2520and%250Abackbones.%2520Notably%252C%2520LinearTCA%2520improves%2520adaptation%2520accuracy%2520by%25205.88%2525%2520on%250AOfficeHome%2520dataset%252C%2520while%2520using%2520only%25204%2525%2520maximum%2520GPU%2520memory%2520usage%2520and%25200.6%2525%250Acomputation%2520time%2520compared%2520to%2520the%2520best%2520baseline%2520TTA%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20Correlation%20Alignment&entry.906535625=Linjing%20You%20and%20Jiabao%20Lu%20and%20Xiayuan%20Huang&entry.1292438233=%20%20Deep%20neural%20networks%20often%20experience%20performance%20drops%20due%20to%20distribution%0Ashifts%20between%20training%20and%20test%20data.%20Although%20domain%20adaptation%20offers%20a%0Asolution%2C%20privacy%20concerns%20restrict%20access%20to%20training%20data%20in%20many%20real-world%0Ascenarios.%20This%20restriction%20has%20spurred%20interest%20in%20Test-Time%20Adaptation%20%28TTA%29%2C%0Awhich%20adapts%20models%20using%20only%20unlabeled%20test%20data.%20However%2C%20current%20TTA%0Amethods%20still%20face%20practical%20challenges%3A%20%281%29%20a%20primary%20focus%20on%20instance-wise%0Aalignment%2C%20overlooking%20CORrelation%20ALignment%20%28CORAL%29%20due%20to%20missing%20source%0Acorrelations%3B%20%282%29%20complex%20backpropagation%20operations%20for%20model%20updating%2C%0Aresulting%20in%20overhead%20computation%20and%20%283%29%20domain%20forgetting.%0A%20%20To%20address%20these%20challenges%2C%20we%20provide%20a%20theoretical%20analysis%20to%20investigate%0Athe%20feasibility%20of%20Test-time%20Correlation%20Alignment%20%28TCA%29%2C%20demonstrating%20that%0Acorrelation%20alignment%20between%20high-certainty%20instances%20and%20test%20instances%20can%0Aenhance%20test%20performances%20with%20a%20theoretical%20guarantee.%20Based%20on%20this%2C%20we%0Apropose%20two%20simple%20yet%20effective%20algorithms%3A%20LinearTCA%20and%20LinearTCA%2B.%0ALinearTCA%20applies%20a%20simple%20linear%20transformation%20to%20achieve%20both%20instance%20and%0Acorrelation%20alignment%20without%20additional%20model%20updates%2C%20while%20LinearTCA%2B%20serves%0Aas%20a%20plug-and-play%20module%20that%20can%20easily%20boost%20existing%20TTA%20methods.%20Extensive%0Aexperiments%20validate%20our%20theoretical%20insights%20and%20show%20that%20TCA%20methods%0Asignificantly%20outperforms%20baselines%20across%20various%20tasks%2C%20benchmarks%20and%0Abackbones.%20Notably%2C%20LinearTCA%20improves%20adaptation%20accuracy%20by%205.88%25%20on%0AOfficeHome%20dataset%2C%20while%20using%20only%204%25%20maximum%20GPU%20memory%20usage%20and%200.6%25%0Acomputation%20time%20compared%20to%20the%20best%20baseline%20TTA%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00533v1&entry.124074799=Read"},
{"title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model\n  Reasoning", "author": "Xiangxiang Chu and Hailang Huang and Xiao Zhang and Fei Wei and Yong Wang", "abstract": "  Reinforcement Learning (RL) can directly enhance the reasoning capabilities\nof large language models without extensive reliance on Supervised Fine-Tuning\n(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism\nand propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike\nconventional methods, GPG directly optimize the original RL objective, thus\nobviating the need for surrogate loss functions. By eliminating the critic and\nreference models, avoiding KL divergence constraints, and addressing the\nadvantage and gradient estimation bias, our approach significantly simplifies\nthe training process compared to Group Relative Policy Optimization (GRPO). Our\napproach achieves superior performance without relying on auxiliary techniques\nor adjustments. As illustrated in Figure 1, extensive experiments demonstrate\nthat our method not only reduces computational costs but also consistently\noutperforms GRPO across various unimodal and multimodal tasks. Our code is\navailable at https://github.com/AMAP-ML/GPG.\n", "link": "http://arxiv.org/abs/2504.02546v3", "date": "2025-05-01", "relevancy": 1.9709, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5105}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPG%3A%20A%20Simple%20and%20Strong%20Reinforcement%20Learning%20Baseline%20for%20Model%0A%20%20Reasoning&body=Title%3A%20GPG%3A%20A%20Simple%20and%20Strong%20Reinforcement%20Learning%20Baseline%20for%20Model%0A%20%20Reasoning%0AAuthor%3A%20Xiangxiang%20Chu%20and%20Hailang%20Huang%20and%20Xiao%20Zhang%20and%20Fei%20Wei%20and%20Yong%20Wang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20can%20directly%20enhance%20the%20reasoning%20capabilities%0Aof%20large%20language%20models%20without%20extensive%20reliance%20on%20Supervised%20Fine-Tuning%0A%28SFT%29.%20In%20this%20work%2C%20we%20revisit%20the%20traditional%20Policy%20Gradient%20%28PG%29%20mechanism%0Aand%20propose%20a%20minimalist%20RL%20approach%20termed%20Group%20Policy%20Gradient%20%28GPG%29.%20Unlike%0Aconventional%20methods%2C%20GPG%20directly%20optimize%20the%20original%20RL%20objective%2C%20thus%0Aobviating%20the%20need%20for%20surrogate%20loss%20functions.%20By%20eliminating%20the%20critic%20and%0Areference%20models%2C%20avoiding%20KL%20divergence%20constraints%2C%20and%20addressing%20the%0Aadvantage%20and%20gradient%20estimation%20bias%2C%20our%20approach%20significantly%20simplifies%0Athe%20training%20process%20compared%20to%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Our%0Aapproach%20achieves%20superior%20performance%20without%20relying%20on%20auxiliary%20techniques%0Aor%20adjustments.%20As%20illustrated%20in%20Figure%201%2C%20extensive%20experiments%20demonstrate%0Athat%20our%20method%20not%20only%20reduces%20computational%20costs%20but%20also%20consistently%0Aoutperforms%20GRPO%20across%20various%20unimodal%20and%20multimodal%20tasks.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AMAP-ML/GPG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02546v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPG%253A%2520A%2520Simple%2520and%2520Strong%2520Reinforcement%2520Learning%2520Baseline%2520for%2520Model%250A%2520%2520Reasoning%26entry.906535625%3DXiangxiang%2520Chu%2520and%2520Hailang%2520Huang%2520and%2520Xiao%2520Zhang%2520and%2520Fei%2520Wei%2520and%2520Yong%2520Wang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520can%2520directly%2520enhance%2520the%2520reasoning%2520capabilities%250Aof%2520large%2520language%2520models%2520without%2520extensive%2520reliance%2520on%2520Supervised%2520Fine-Tuning%250A%2528SFT%2529.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520traditional%2520Policy%2520Gradient%2520%2528PG%2529%2520mechanism%250Aand%2520propose%2520a%2520minimalist%2520RL%2520approach%2520termed%2520Group%2520Policy%2520Gradient%2520%2528GPG%2529.%2520Unlike%250Aconventional%2520methods%252C%2520GPG%2520directly%2520optimize%2520the%2520original%2520RL%2520objective%252C%2520thus%250Aobviating%2520the%2520need%2520for%2520surrogate%2520loss%2520functions.%2520By%2520eliminating%2520the%2520critic%2520and%250Areference%2520models%252C%2520avoiding%2520KL%2520divergence%2520constraints%252C%2520and%2520addressing%2520the%250Aadvantage%2520and%2520gradient%2520estimation%2520bias%252C%2520our%2520approach%2520significantly%2520simplifies%250Athe%2520training%2520process%2520compared%2520to%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529.%2520Our%250Aapproach%2520achieves%2520superior%2520performance%2520without%2520relying%2520on%2520auxiliary%2520techniques%250Aor%2520adjustments.%2520As%2520illustrated%2520in%2520Figure%25201%252C%2520extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520not%2520only%2520reduces%2520computational%2520costs%2520but%2520also%2520consistently%250Aoutperforms%2520GRPO%2520across%2520various%2520unimodal%2520and%2520multimodal%2520tasks.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/AMAP-ML/GPG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02546v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPG%3A%20A%20Simple%20and%20Strong%20Reinforcement%20Learning%20Baseline%20for%20Model%0A%20%20Reasoning&entry.906535625=Xiangxiang%20Chu%20and%20Hailang%20Huang%20and%20Xiao%20Zhang%20and%20Fei%20Wei%20and%20Yong%20Wang&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20can%20directly%20enhance%20the%20reasoning%20capabilities%0Aof%20large%20language%20models%20without%20extensive%20reliance%20on%20Supervised%20Fine-Tuning%0A%28SFT%29.%20In%20this%20work%2C%20we%20revisit%20the%20traditional%20Policy%20Gradient%20%28PG%29%20mechanism%0Aand%20propose%20a%20minimalist%20RL%20approach%20termed%20Group%20Policy%20Gradient%20%28GPG%29.%20Unlike%0Aconventional%20methods%2C%20GPG%20directly%20optimize%20the%20original%20RL%20objective%2C%20thus%0Aobviating%20the%20need%20for%20surrogate%20loss%20functions.%20By%20eliminating%20the%20critic%20and%0Areference%20models%2C%20avoiding%20KL%20divergence%20constraints%2C%20and%20addressing%20the%0Aadvantage%20and%20gradient%20estimation%20bias%2C%20our%20approach%20significantly%20simplifies%0Athe%20training%20process%20compared%20to%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29.%20Our%0Aapproach%20achieves%20superior%20performance%20without%20relying%20on%20auxiliary%20techniques%0Aor%20adjustments.%20As%20illustrated%20in%20Figure%201%2C%20extensive%20experiments%20demonstrate%0Athat%20our%20method%20not%20only%20reduces%20computational%20costs%20but%20also%20consistently%0Aoutperforms%20GRPO%20across%20various%20unimodal%20and%20multimodal%20tasks.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AMAP-ML/GPG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02546v3&entry.124074799=Read"},
{"title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching", "author": "Yue Meng and Chuchu Fan", "abstract": "  Learning to solve complex tasks with signal temporal logic (STL)\nspecifications is crucial to many real-world applications. However, most\nprevious works only consider fixed or parametrized STL specifications due to\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\nencoder and flow-matching to learn solutions for general STL specifications. We\nidentify four commonly used STL templates and collect a total of 200K\nspecifications with paired demonstrations. We conduct extensive experiments in\nfive simulation environments ranging from simple dynamical models in the 2D\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\nnavigation. Results show that our method outperforms other baselines in the STL\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\nshow our graph-encoding method's capability to solve complex STLs and\nrobustness to out-distribution STL specifications. Code is available at\nhttps://github.com/mengyuest/TeLoGraF\n", "link": "http://arxiv.org/abs/2505.00562v1", "date": "2025-05-01", "relevancy": 1.9563, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5184}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeLoGraF%3A%20Temporal%20Logic%20Planning%20via%20Graph-encoded%20Flow%20Matching&body=Title%3A%20TeLoGraF%3A%20Temporal%20Logic%20Planning%20via%20Graph-encoded%20Flow%20Matching%0AAuthor%3A%20Yue%20Meng%20and%20Chuchu%20Fan%0AAbstract%3A%20%20%20Learning%20to%20solve%20complex%20tasks%20with%20signal%20temporal%20logic%20%28STL%29%0Aspecifications%20is%20crucial%20to%20many%20real-world%20applications.%20However%2C%20most%0Aprevious%20works%20only%20consider%20fixed%20or%20parametrized%20STL%20specifications%20due%20to%0Athe%20lack%20of%20a%20diverse%20STL%20dataset%20and%20encoders%20to%20effectively%20extract%20temporal%0Alogic%20information%20for%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20TeLoGraF%2C%0ATemporal%20Logic%20Graph-encoded%20Flow%2C%20which%20utilizes%20Graph%20Neural%20Networks%20%28GNN%29%0Aencoder%20and%20flow-matching%20to%20learn%20solutions%20for%20general%20STL%20specifications.%20We%0Aidentify%20four%20commonly%20used%20STL%20templates%20and%20collect%20a%20total%20of%20200K%0Aspecifications%20with%20paired%20demonstrations.%20We%20conduct%20extensive%20experiments%20in%0Afive%20simulation%20environments%20ranging%20from%20simple%20dynamical%20models%20in%20the%202D%0Aspace%20to%20high-dimensional%207DoF%20Franka%20Panda%20robot%20arm%20and%20Ant%20quadruped%0Anavigation.%20Results%20show%20that%20our%20method%20outperforms%20other%20baselines%20in%20the%20STL%0Asatisfaction%20rate.%20Compared%20to%20classical%20STL%20planning%20algorithms%2C%20our%20approach%0Ais%2010-100X%20faster%20in%20inference%20and%20can%20work%20on%20any%20system%20dynamics.%20Besides%2C%20we%0Ashow%20our%20graph-encoding%20method%27s%20capability%20to%20solve%20complex%20STLs%20and%0Arobustness%20to%20out-distribution%20STL%20specifications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mengyuest/TeLoGraF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeLoGraF%253A%2520Temporal%2520Logic%2520Planning%2520via%2520Graph-encoded%2520Flow%2520Matching%26entry.906535625%3DYue%2520Meng%2520and%2520Chuchu%2520Fan%26entry.1292438233%3D%2520%2520Learning%2520to%2520solve%2520complex%2520tasks%2520with%2520signal%2520temporal%2520logic%2520%2528STL%2529%250Aspecifications%2520is%2520crucial%2520to%2520many%2520real-world%2520applications.%2520However%252C%2520most%250Aprevious%2520works%2520only%2520consider%2520fixed%2520or%2520parametrized%2520STL%2520specifications%2520due%2520to%250Athe%2520lack%2520of%2520a%2520diverse%2520STL%2520dataset%2520and%2520encoders%2520to%2520effectively%2520extract%2520temporal%250Alogic%2520information%2520for%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TeLoGraF%252C%250ATemporal%2520Logic%2520Graph-encoded%2520Flow%252C%2520which%2520utilizes%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%250Aencoder%2520and%2520flow-matching%2520to%2520learn%2520solutions%2520for%2520general%2520STL%2520specifications.%2520We%250Aidentify%2520four%2520commonly%2520used%2520STL%2520templates%2520and%2520collect%2520a%2520total%2520of%2520200K%250Aspecifications%2520with%2520paired%2520demonstrations.%2520We%2520conduct%2520extensive%2520experiments%2520in%250Afive%2520simulation%2520environments%2520ranging%2520from%2520simple%2520dynamical%2520models%2520in%2520the%25202D%250Aspace%2520to%2520high-dimensional%25207DoF%2520Franka%2520Panda%2520robot%2520arm%2520and%2520Ant%2520quadruped%250Anavigation.%2520Results%2520show%2520that%2520our%2520method%2520outperforms%2520other%2520baselines%2520in%2520the%2520STL%250Asatisfaction%2520rate.%2520Compared%2520to%2520classical%2520STL%2520planning%2520algorithms%252C%2520our%2520approach%250Ais%252010-100X%2520faster%2520in%2520inference%2520and%2520can%2520work%2520on%2520any%2520system%2520dynamics.%2520Besides%252C%2520we%250Ashow%2520our%2520graph-encoding%2520method%2527s%2520capability%2520to%2520solve%2520complex%2520STLs%2520and%250Arobustness%2520to%2520out-distribution%2520STL%2520specifications.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mengyuest/TeLoGraF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeLoGraF%3A%20Temporal%20Logic%20Planning%20via%20Graph-encoded%20Flow%20Matching&entry.906535625=Yue%20Meng%20and%20Chuchu%20Fan&entry.1292438233=%20%20Learning%20to%20solve%20complex%20tasks%20with%20signal%20temporal%20logic%20%28STL%29%0Aspecifications%20is%20crucial%20to%20many%20real-world%20applications.%20However%2C%20most%0Aprevious%20works%20only%20consider%20fixed%20or%20parametrized%20STL%20specifications%20due%20to%0Athe%20lack%20of%20a%20diverse%20STL%20dataset%20and%20encoders%20to%20effectively%20extract%20temporal%0Alogic%20information%20for%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20TeLoGraF%2C%0ATemporal%20Logic%20Graph-encoded%20Flow%2C%20which%20utilizes%20Graph%20Neural%20Networks%20%28GNN%29%0Aencoder%20and%20flow-matching%20to%20learn%20solutions%20for%20general%20STL%20specifications.%20We%0Aidentify%20four%20commonly%20used%20STL%20templates%20and%20collect%20a%20total%20of%20200K%0Aspecifications%20with%20paired%20demonstrations.%20We%20conduct%20extensive%20experiments%20in%0Afive%20simulation%20environments%20ranging%20from%20simple%20dynamical%20models%20in%20the%202D%0Aspace%20to%20high-dimensional%207DoF%20Franka%20Panda%20robot%20arm%20and%20Ant%20quadruped%0Anavigation.%20Results%20show%20that%20our%20method%20outperforms%20other%20baselines%20in%20the%20STL%0Asatisfaction%20rate.%20Compared%20to%20classical%20STL%20planning%20algorithms%2C%20our%20approach%0Ais%2010-100X%20faster%20in%20inference%20and%20can%20work%20on%20any%20system%20dynamics.%20Besides%2C%20we%0Ashow%20our%20graph-encoding%20method%27s%20capability%20to%20solve%20complex%20STLs%20and%0Arobustness%20to%20out-distribution%20STL%20specifications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mengyuest/TeLoGraF%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00562v1&entry.124074799=Read"},
{"title": "FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression", "author": "Alireza Furutanpey and Qiyang Zhang and Philipp Raith and Tobias Pfandzelter and Shangguang Wang and Schahram Dustdar", "abstract": "  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks. This work\npresents FOOL, an OEC-native and task-agnostic feature compression method that\npreserves prediction performance. FOOL partitions high-resolution satellite\nimagery to maximize throughput. Further, it embeds context and leverages\ninter-tile dependencies to lower transfer costs with negligible overhead. While\nFOOL is a feature compressor, it can recover images with competitive scores on\nquality measures at lower bitrates. We extensively evaluate transfer cost\nreduction by including the peculiarity of intermittently available network\nconnections in low earth orbit. Lastly, we test the feasibility of our system\nfor standardized nanosatellite form factors. We demonstrate that FOOL permits\ndownlinking over 100x the data volume without relying on prior information on\nthe downstream tasks.\n", "link": "http://arxiv.org/abs/2403.16677v3", "date": "2025-05-01", "relevancy": 1.9501, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4818}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression&body=Title%3A%20FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression%0AAuthor%3A%20Alireza%20Furutanpey%20and%20Qiyang%20Zhang%20and%20Philipp%20Raith%20and%20Tobias%20Pfandzelter%20and%20Shangguang%20Wang%20and%20Schahram%20Dustdar%0AAbstract%3A%20%20%20Nanosatellite%20constellations%20equipped%20with%20sensors%20capturing%20large%20geographic%0Aregions%20provide%20unprecedented%20opportunities%20for%20Earth%20observation.%20As%0Aconstellation%20sizes%20increase%2C%20network%20contention%20poses%20a%20downlink%20bottleneck.%0AOrbital%20Edge%20Computing%20%28OEC%29%20leverages%20limited%20onboard%20compute%20resources%20to%0Areduce%20transfer%20costs%20by%20processing%20the%20raw%20captures%20at%20the%20source.%20However%2C%0Acurrent%20solutions%20have%20limited%20practicability%20due%20to%20reliance%20on%20crude%0Afiltering%20methods%20or%20over-prioritizing%20particular%20downstream%20tasks.%20This%20work%0Apresents%20FOOL%2C%20an%20OEC-native%20and%20task-agnostic%20feature%20compression%20method%20that%0Apreserves%20prediction%20performance.%20FOOL%20partitions%20high-resolution%20satellite%0Aimagery%20to%20maximize%20throughput.%20Further%2C%20it%20embeds%20context%20and%20leverages%0Ainter-tile%20dependencies%20to%20lower%20transfer%20costs%20with%20negligible%20overhead.%20While%0AFOOL%20is%20a%20feature%20compressor%2C%20it%20can%20recover%20images%20with%20competitive%20scores%20on%0Aquality%20measures%20at%20lower%20bitrates.%20We%20extensively%20evaluate%20transfer%20cost%0Areduction%20by%20including%20the%20peculiarity%20of%20intermittently%20available%20network%0Aconnections%20in%20low%20earth%20orbit.%20Lastly%2C%20we%20test%20the%20feasibility%20of%20our%20system%0Afor%20standardized%20nanosatellite%20form%20factors.%20We%20demonstrate%20that%20FOOL%20permits%0Adownlinking%20over%20100x%20the%20data%20volume%20without%20relying%20on%20prior%20information%20on%0Athe%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOOL%253A%2520Addressing%2520the%2520Downlink%2520Bottleneck%2520in%2520Satellite%2520Computing%2520with%250A%2520%2520Neural%2520Feature%2520Compression%26entry.906535625%3DAlireza%2520Furutanpey%2520and%2520Qiyang%2520Zhang%2520and%2520Philipp%2520Raith%2520and%2520Tobias%2520Pfandzelter%2520and%2520Shangguang%2520Wang%2520and%2520Schahram%2520Dustdar%26entry.1292438233%3D%2520%2520Nanosatellite%2520constellations%2520equipped%2520with%2520sensors%2520capturing%2520large%2520geographic%250Aregions%2520provide%2520unprecedented%2520opportunities%2520for%2520Earth%2520observation.%2520As%250Aconstellation%2520sizes%2520increase%252C%2520network%2520contention%2520poses%2520a%2520downlink%2520bottleneck.%250AOrbital%2520Edge%2520Computing%2520%2528OEC%2529%2520leverages%2520limited%2520onboard%2520compute%2520resources%2520to%250Areduce%2520transfer%2520costs%2520by%2520processing%2520the%2520raw%2520captures%2520at%2520the%2520source.%2520However%252C%250Acurrent%2520solutions%2520have%2520limited%2520practicability%2520due%2520to%2520reliance%2520on%2520crude%250Afiltering%2520methods%2520or%2520over-prioritizing%2520particular%2520downstream%2520tasks.%2520This%2520work%250Apresents%2520FOOL%252C%2520an%2520OEC-native%2520and%2520task-agnostic%2520feature%2520compression%2520method%2520that%250Apreserves%2520prediction%2520performance.%2520FOOL%2520partitions%2520high-resolution%2520satellite%250Aimagery%2520to%2520maximize%2520throughput.%2520Further%252C%2520it%2520embeds%2520context%2520and%2520leverages%250Ainter-tile%2520dependencies%2520to%2520lower%2520transfer%2520costs%2520with%2520negligible%2520overhead.%2520While%250AFOOL%2520is%2520a%2520feature%2520compressor%252C%2520it%2520can%2520recover%2520images%2520with%2520competitive%2520scores%2520on%250Aquality%2520measures%2520at%2520lower%2520bitrates.%2520We%2520extensively%2520evaluate%2520transfer%2520cost%250Areduction%2520by%2520including%2520the%2520peculiarity%2520of%2520intermittently%2520available%2520network%250Aconnections%2520in%2520low%2520earth%2520orbit.%2520Lastly%252C%2520we%2520test%2520the%2520feasibility%2520of%2520our%2520system%250Afor%2520standardized%2520nanosatellite%2520form%2520factors.%2520We%2520demonstrate%2520that%2520FOOL%2520permits%250Adownlinking%2520over%2520100x%2520the%2520data%2520volume%2520without%2520relying%2520on%2520prior%2520information%2520on%250Athe%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression&entry.906535625=Alireza%20Furutanpey%20and%20Qiyang%20Zhang%20and%20Philipp%20Raith%20and%20Tobias%20Pfandzelter%20and%20Shangguang%20Wang%20and%20Schahram%20Dustdar&entry.1292438233=%20%20Nanosatellite%20constellations%20equipped%20with%20sensors%20capturing%20large%20geographic%0Aregions%20provide%20unprecedented%20opportunities%20for%20Earth%20observation.%20As%0Aconstellation%20sizes%20increase%2C%20network%20contention%20poses%20a%20downlink%20bottleneck.%0AOrbital%20Edge%20Computing%20%28OEC%29%20leverages%20limited%20onboard%20compute%20resources%20to%0Areduce%20transfer%20costs%20by%20processing%20the%20raw%20captures%20at%20the%20source.%20However%2C%0Acurrent%20solutions%20have%20limited%20practicability%20due%20to%20reliance%20on%20crude%0Afiltering%20methods%20or%20over-prioritizing%20particular%20downstream%20tasks.%20This%20work%0Apresents%20FOOL%2C%20an%20OEC-native%20and%20task-agnostic%20feature%20compression%20method%20that%0Apreserves%20prediction%20performance.%20FOOL%20partitions%20high-resolution%20satellite%0Aimagery%20to%20maximize%20throughput.%20Further%2C%20it%20embeds%20context%20and%20leverages%0Ainter-tile%20dependencies%20to%20lower%20transfer%20costs%20with%20negligible%20overhead.%20While%0AFOOL%20is%20a%20feature%20compressor%2C%20it%20can%20recover%20images%20with%20competitive%20scores%20on%0Aquality%20measures%20at%20lower%20bitrates.%20We%20extensively%20evaluate%20transfer%20cost%0Areduction%20by%20including%20the%20peculiarity%20of%20intermittently%20available%20network%0Aconnections%20in%20low%20earth%20orbit.%20Lastly%2C%20we%20test%20the%20feasibility%20of%20our%20system%0Afor%20standardized%20nanosatellite%20form%20factors.%20We%20demonstrate%20that%20FOOL%20permits%0Adownlinking%20over%20100x%20the%20data%20volume%20without%20relying%20on%20prior%20information%20on%0Athe%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16677v3&entry.124074799=Read"},
{"title": "A Methodological and Structural Review of Parkinsons Disease Detection\n  Across Diverse Data Modalities", "author": "Abu Saleh Musa Miah and taro Suzuki and Jungpil Shin", "abstract": "  Parkinsons Disease (PD) is a progressive neurological disorder that primarily\naffects motor functions and can lead to mild cognitive impairment (MCI) and\ndementia in its advanced stages. With approximately 10 million people diagnosed\nglobally 1 to 1.8 per 1,000 individuals, according to reports by the Japan\nTimes and the Parkinson Foundation early and accurate diagnosis of PD is\ncrucial for improving patient outcomes. While numerous studies have utilized\nmachine learning (ML) and deep learning (DL) techniques for PD recognition,\nexisting surveys are limited in scope, often focusing on single data modalities\nand failing to capture the potential of multimodal approaches. To address these\ngaps, this study presents a comprehensive review of PD recognition systems\nacross diverse data modalities, including Magnetic Resonance Imaging (MRI),\ngait-based pose analysis, gait sensory data, handwriting analysis, speech test\ndata, Electroencephalography (EEG), and multimodal fusion techniques. Based on\nover 347 articles from leading scientific databases, this review examines key\naspects such as data collection methods, settings, feature representations, and\nsystem performance, with a focus on recognition accuracy and robustness. This\nsurvey aims to serve as a comprehensive resource for researchers, providing\nactionable guidance for the development of next generation PD recognition\nsystems. By leveraging diverse data modalities and cutting-edge machine\nlearning paradigms, this work contributes to advancing the state of PD\ndiagnostics and improving patient care through innovative, multimodal\napproaches.\n", "link": "http://arxiv.org/abs/2505.00525v1", "date": "2025-05-01", "relevancy": 1.9471, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4915}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Methodological%20and%20Structural%20Review%20of%20Parkinsons%20Disease%20Detection%0A%20%20Across%20Diverse%20Data%20Modalities&body=Title%3A%20A%20Methodological%20and%20Structural%20Review%20of%20Parkinsons%20Disease%20Detection%0A%20%20Across%20Diverse%20Data%20Modalities%0AAuthor%3A%20Abu%20Saleh%20Musa%20Miah%20and%20taro%20Suzuki%20and%20Jungpil%20Shin%0AAbstract%3A%20%20%20Parkinsons%20Disease%20%28PD%29%20is%20a%20progressive%20neurological%20disorder%20that%20primarily%0Aaffects%20motor%20functions%20and%20can%20lead%20to%20mild%20cognitive%20impairment%20%28MCI%29%20and%0Adementia%20in%20its%20advanced%20stages.%20With%20approximately%2010%20million%20people%20diagnosed%0Aglobally%201%20to%201.8%20per%201%2C000%20individuals%2C%20according%20to%20reports%20by%20the%20Japan%0ATimes%20and%20the%20Parkinson%20Foundation%20early%20and%20accurate%20diagnosis%20of%20PD%20is%0Acrucial%20for%20improving%20patient%20outcomes.%20While%20numerous%20studies%20have%20utilized%0Amachine%20learning%20%28ML%29%20and%20deep%20learning%20%28DL%29%20techniques%20for%20PD%20recognition%2C%0Aexisting%20surveys%20are%20limited%20in%20scope%2C%20often%20focusing%20on%20single%20data%20modalities%0Aand%20failing%20to%20capture%20the%20potential%20of%20multimodal%20approaches.%20To%20address%20these%0Agaps%2C%20this%20study%20presents%20a%20comprehensive%20review%20of%20PD%20recognition%20systems%0Aacross%20diverse%20data%20modalities%2C%20including%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%0Agait-based%20pose%20analysis%2C%20gait%20sensory%20data%2C%20handwriting%20analysis%2C%20speech%20test%0Adata%2C%20Electroencephalography%20%28EEG%29%2C%20and%20multimodal%20fusion%20techniques.%20Based%20on%0Aover%20347%20articles%20from%20leading%20scientific%20databases%2C%20this%20review%20examines%20key%0Aaspects%20such%20as%20data%20collection%20methods%2C%20settings%2C%20feature%20representations%2C%20and%0Asystem%20performance%2C%20with%20a%20focus%20on%20recognition%20accuracy%20and%20robustness.%20This%0Asurvey%20aims%20to%20serve%20as%20a%20comprehensive%20resource%20for%20researchers%2C%20providing%0Aactionable%20guidance%20for%20the%20development%20of%20next%20generation%20PD%20recognition%0Asystems.%20By%20leveraging%20diverse%20data%20modalities%20and%20cutting-edge%20machine%0Alearning%20paradigms%2C%20this%20work%20contributes%20to%20advancing%20the%20state%20of%20PD%0Adiagnostics%20and%20improving%20patient%20care%20through%20innovative%2C%20multimodal%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Methodological%2520and%2520Structural%2520Review%2520of%2520Parkinsons%2520Disease%2520Detection%250A%2520%2520Across%2520Diverse%2520Data%2520Modalities%26entry.906535625%3DAbu%2520Saleh%2520Musa%2520Miah%2520and%2520taro%2520Suzuki%2520and%2520Jungpil%2520Shin%26entry.1292438233%3D%2520%2520Parkinsons%2520Disease%2520%2528PD%2529%2520is%2520a%2520progressive%2520neurological%2520disorder%2520that%2520primarily%250Aaffects%2520motor%2520functions%2520and%2520can%2520lead%2520to%2520mild%2520cognitive%2520impairment%2520%2528MCI%2529%2520and%250Adementia%2520in%2520its%2520advanced%2520stages.%2520With%2520approximately%252010%2520million%2520people%2520diagnosed%250Aglobally%25201%2520to%25201.8%2520per%25201%252C000%2520individuals%252C%2520according%2520to%2520reports%2520by%2520the%2520Japan%250ATimes%2520and%2520the%2520Parkinson%2520Foundation%2520early%2520and%2520accurate%2520diagnosis%2520of%2520PD%2520is%250Acrucial%2520for%2520improving%2520patient%2520outcomes.%2520While%2520numerous%2520studies%2520have%2520utilized%250Amachine%2520learning%2520%2528ML%2529%2520and%2520deep%2520learning%2520%2528DL%2529%2520techniques%2520for%2520PD%2520recognition%252C%250Aexisting%2520surveys%2520are%2520limited%2520in%2520scope%252C%2520often%2520focusing%2520on%2520single%2520data%2520modalities%250Aand%2520failing%2520to%2520capture%2520the%2520potential%2520of%2520multimodal%2520approaches.%2520To%2520address%2520these%250Agaps%252C%2520this%2520study%2520presents%2520a%2520comprehensive%2520review%2520of%2520PD%2520recognition%2520systems%250Aacross%2520diverse%2520data%2520modalities%252C%2520including%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%250Agait-based%2520pose%2520analysis%252C%2520gait%2520sensory%2520data%252C%2520handwriting%2520analysis%252C%2520speech%2520test%250Adata%252C%2520Electroencephalography%2520%2528EEG%2529%252C%2520and%2520multimodal%2520fusion%2520techniques.%2520Based%2520on%250Aover%2520347%2520articles%2520from%2520leading%2520scientific%2520databases%252C%2520this%2520review%2520examines%2520key%250Aaspects%2520such%2520as%2520data%2520collection%2520methods%252C%2520settings%252C%2520feature%2520representations%252C%2520and%250Asystem%2520performance%252C%2520with%2520a%2520focus%2520on%2520recognition%2520accuracy%2520and%2520robustness.%2520This%250Asurvey%2520aims%2520to%2520serve%2520as%2520a%2520comprehensive%2520resource%2520for%2520researchers%252C%2520providing%250Aactionable%2520guidance%2520for%2520the%2520development%2520of%2520next%2520generation%2520PD%2520recognition%250Asystems.%2520By%2520leveraging%2520diverse%2520data%2520modalities%2520and%2520cutting-edge%2520machine%250Alearning%2520paradigms%252C%2520this%2520work%2520contributes%2520to%2520advancing%2520the%2520state%2520of%2520PD%250Adiagnostics%2520and%2520improving%2520patient%2520care%2520through%2520innovative%252C%2520multimodal%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Methodological%20and%20Structural%20Review%20of%20Parkinsons%20Disease%20Detection%0A%20%20Across%20Diverse%20Data%20Modalities&entry.906535625=Abu%20Saleh%20Musa%20Miah%20and%20taro%20Suzuki%20and%20Jungpil%20Shin&entry.1292438233=%20%20Parkinsons%20Disease%20%28PD%29%20is%20a%20progressive%20neurological%20disorder%20that%20primarily%0Aaffects%20motor%20functions%20and%20can%20lead%20to%20mild%20cognitive%20impairment%20%28MCI%29%20and%0Adementia%20in%20its%20advanced%20stages.%20With%20approximately%2010%20million%20people%20diagnosed%0Aglobally%201%20to%201.8%20per%201%2C000%20individuals%2C%20according%20to%20reports%20by%20the%20Japan%0ATimes%20and%20the%20Parkinson%20Foundation%20early%20and%20accurate%20diagnosis%20of%20PD%20is%0Acrucial%20for%20improving%20patient%20outcomes.%20While%20numerous%20studies%20have%20utilized%0Amachine%20learning%20%28ML%29%20and%20deep%20learning%20%28DL%29%20techniques%20for%20PD%20recognition%2C%0Aexisting%20surveys%20are%20limited%20in%20scope%2C%20often%20focusing%20on%20single%20data%20modalities%0Aand%20failing%20to%20capture%20the%20potential%20of%20multimodal%20approaches.%20To%20address%20these%0Agaps%2C%20this%20study%20presents%20a%20comprehensive%20review%20of%20PD%20recognition%20systems%0Aacross%20diverse%20data%20modalities%2C%20including%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%0Agait-based%20pose%20analysis%2C%20gait%20sensory%20data%2C%20handwriting%20analysis%2C%20speech%20test%0Adata%2C%20Electroencephalography%20%28EEG%29%2C%20and%20multimodal%20fusion%20techniques.%20Based%20on%0Aover%20347%20articles%20from%20leading%20scientific%20databases%2C%20this%20review%20examines%20key%0Aaspects%20such%20as%20data%20collection%20methods%2C%20settings%2C%20feature%20representations%2C%20and%0Asystem%20performance%2C%20with%20a%20focus%20on%20recognition%20accuracy%20and%20robustness.%20This%0Asurvey%20aims%20to%20serve%20as%20a%20comprehensive%20resource%20for%20researchers%2C%20providing%0Aactionable%20guidance%20for%20the%20development%20of%20next%20generation%20PD%20recognition%0Asystems.%20By%20leveraging%20diverse%20data%20modalities%20and%20cutting-edge%20machine%0Alearning%20paradigms%2C%20this%20work%20contributes%20to%20advancing%20the%20state%20of%20PD%0Adiagnostics%20and%20improving%20patient%20care%20through%20innovative%2C%20multimodal%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00525v1&entry.124074799=Read"},
{"title": "Over-the-Air Inference over Multi-hop MIMO Networks", "author": "Chenghong Bian and Meng Hua and Deniz Gunduz", "abstract": "  A novel over-the-air machine learning framework over multi-hop multiple-input\nand multiple-output (MIMO) networks is proposed. The core idea is to imitate\nfully connected (FC) neural network layers using multiple MIMO channels by\ncarefully designing the precoding matrices at the transmitting nodes. A neural\nnetwork dubbed PrototypeNet is employed consisting of multiple FC layers, with\nthe number of neurons of each layer equal to the number of antennas of the\ncorresponding terminal. To achieve satisfactory performance, we train\nPrototypeNet based on a customized loss function consisting of classification\nerror and the power of latent vectors to satisfy transmit power constraints,\nwith noise injection during training. Precoding matrices for each hop are then\nobtained by solving an optimization problem. We also propose a multiple-block\nextension when the number of antennas is limited. Numerical results verify that\nthe proposed over-the-air transmission scheme can achieve satisfactory\nclassification accuracy under a power constraint. The results also show that\nhigher classification accuracy can be achieved with an increasing number of\nhops at a modest signal-to-noise ratio (SNR).\n", "link": "http://arxiv.org/abs/2505.00430v1", "date": "2025-05-01", "relevancy": 1.9416, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Over-the-Air%20Inference%20over%20Multi-hop%20MIMO%20Networks&body=Title%3A%20Over-the-Air%20Inference%20over%20Multi-hop%20MIMO%20Networks%0AAuthor%3A%20Chenghong%20Bian%20and%20Meng%20Hua%20and%20Deniz%20Gunduz%0AAbstract%3A%20%20%20A%20novel%20over-the-air%20machine%20learning%20framework%20over%20multi-hop%20multiple-input%0Aand%20multiple-output%20%28MIMO%29%20networks%20is%20proposed.%20The%20core%20idea%20is%20to%20imitate%0Afully%20connected%20%28FC%29%20neural%20network%20layers%20using%20multiple%20MIMO%20channels%20by%0Acarefully%20designing%20the%20precoding%20matrices%20at%20the%20transmitting%20nodes.%20A%20neural%0Anetwork%20dubbed%20PrototypeNet%20is%20employed%20consisting%20of%20multiple%20FC%20layers%2C%20with%0Athe%20number%20of%20neurons%20of%20each%20layer%20equal%20to%20the%20number%20of%20antennas%20of%20the%0Acorresponding%20terminal.%20To%20achieve%20satisfactory%20performance%2C%20we%20train%0APrototypeNet%20based%20on%20a%20customized%20loss%20function%20consisting%20of%20classification%0Aerror%20and%20the%20power%20of%20latent%20vectors%20to%20satisfy%20transmit%20power%20constraints%2C%0Awith%20noise%20injection%20during%20training.%20Precoding%20matrices%20for%20each%20hop%20are%20then%0Aobtained%20by%20solving%20an%20optimization%20problem.%20We%20also%20propose%20a%20multiple-block%0Aextension%20when%20the%20number%20of%20antennas%20is%20limited.%20Numerical%20results%20verify%20that%0Athe%20proposed%20over-the-air%20transmission%20scheme%20can%20achieve%20satisfactory%0Aclassification%20accuracy%20under%20a%20power%20constraint.%20The%20results%20also%20show%20that%0Ahigher%20classification%20accuracy%20can%20be%20achieved%20with%20an%20increasing%20number%20of%0Ahops%20at%20a%20modest%20signal-to-noise%20ratio%20%28SNR%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOver-the-Air%2520Inference%2520over%2520Multi-hop%2520MIMO%2520Networks%26entry.906535625%3DChenghong%2520Bian%2520and%2520Meng%2520Hua%2520and%2520Deniz%2520Gunduz%26entry.1292438233%3D%2520%2520A%2520novel%2520over-the-air%2520machine%2520learning%2520framework%2520over%2520multi-hop%2520multiple-input%250Aand%2520multiple-output%2520%2528MIMO%2529%2520networks%2520is%2520proposed.%2520The%2520core%2520idea%2520is%2520to%2520imitate%250Afully%2520connected%2520%2528FC%2529%2520neural%2520network%2520layers%2520using%2520multiple%2520MIMO%2520channels%2520by%250Acarefully%2520designing%2520the%2520precoding%2520matrices%2520at%2520the%2520transmitting%2520nodes.%2520A%2520neural%250Anetwork%2520dubbed%2520PrototypeNet%2520is%2520employed%2520consisting%2520of%2520multiple%2520FC%2520layers%252C%2520with%250Athe%2520number%2520of%2520neurons%2520of%2520each%2520layer%2520equal%2520to%2520the%2520number%2520of%2520antennas%2520of%2520the%250Acorresponding%2520terminal.%2520To%2520achieve%2520satisfactory%2520performance%252C%2520we%2520train%250APrototypeNet%2520based%2520on%2520a%2520customized%2520loss%2520function%2520consisting%2520of%2520classification%250Aerror%2520and%2520the%2520power%2520of%2520latent%2520vectors%2520to%2520satisfy%2520transmit%2520power%2520constraints%252C%250Awith%2520noise%2520injection%2520during%2520training.%2520Precoding%2520matrices%2520for%2520each%2520hop%2520are%2520then%250Aobtained%2520by%2520solving%2520an%2520optimization%2520problem.%2520We%2520also%2520propose%2520a%2520multiple-block%250Aextension%2520when%2520the%2520number%2520of%2520antennas%2520is%2520limited.%2520Numerical%2520results%2520verify%2520that%250Athe%2520proposed%2520over-the-air%2520transmission%2520scheme%2520can%2520achieve%2520satisfactory%250Aclassification%2520accuracy%2520under%2520a%2520power%2520constraint.%2520The%2520results%2520also%2520show%2520that%250Ahigher%2520classification%2520accuracy%2520can%2520be%2520achieved%2520with%2520an%2520increasing%2520number%2520of%250Ahops%2520at%2520a%2520modest%2520signal-to-noise%2520ratio%2520%2528SNR%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over-the-Air%20Inference%20over%20Multi-hop%20MIMO%20Networks&entry.906535625=Chenghong%20Bian%20and%20Meng%20Hua%20and%20Deniz%20Gunduz&entry.1292438233=%20%20A%20novel%20over-the-air%20machine%20learning%20framework%20over%20multi-hop%20multiple-input%0Aand%20multiple-output%20%28MIMO%29%20networks%20is%20proposed.%20The%20core%20idea%20is%20to%20imitate%0Afully%20connected%20%28FC%29%20neural%20network%20layers%20using%20multiple%20MIMO%20channels%20by%0Acarefully%20designing%20the%20precoding%20matrices%20at%20the%20transmitting%20nodes.%20A%20neural%0Anetwork%20dubbed%20PrototypeNet%20is%20employed%20consisting%20of%20multiple%20FC%20layers%2C%20with%0Athe%20number%20of%20neurons%20of%20each%20layer%20equal%20to%20the%20number%20of%20antennas%20of%20the%0Acorresponding%20terminal.%20To%20achieve%20satisfactory%20performance%2C%20we%20train%0APrototypeNet%20based%20on%20a%20customized%20loss%20function%20consisting%20of%20classification%0Aerror%20and%20the%20power%20of%20latent%20vectors%20to%20satisfy%20transmit%20power%20constraints%2C%0Awith%20noise%20injection%20during%20training.%20Precoding%20matrices%20for%20each%20hop%20are%20then%0Aobtained%20by%20solving%20an%20optimization%20problem.%20We%20also%20propose%20a%20multiple-block%0Aextension%20when%20the%20number%20of%20antennas%20is%20limited.%20Numerical%20results%20verify%20that%0Athe%20proposed%20over-the-air%20transmission%20scheme%20can%20achieve%20satisfactory%0Aclassification%20accuracy%20under%20a%20power%20constraint.%20The%20results%20also%20show%20that%0Ahigher%20classification%20accuracy%20can%20be%20achieved%20with%20an%20increasing%20number%20of%0Ahops%20at%20a%20modest%20signal-to-noise%20ratio%20%28SNR%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00430v1&entry.124074799=Read"},
{"title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts\n  Models", "author": "Keisuke Kamahori and Tian Tang and Yile Gu and Kan Zhu and Baris Kasikci", "abstract": "  Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures\nhave shown promising performance on various tasks. However, due to the huge\nmodel sizes, running them in resource-constrained environments where the GPU\nmemory is not abundant is challenging. Some existing systems propose to use CPU\nresources to solve that, but they either suffer from the significant overhead\nof frequently moving data between CPU and GPU, or fail to consider distinct\ncharacteristics of CPUs and GPUs. This paper proposes Fiddler, a\nresource-efficient inference system for MoE models with limited GPU resources.\nFiddler strategically utilizes CPU and GPU resources by determining the optimal\nexecution strategy. Our evaluation shows that, unlike state-of-the-art systems\nthat optimize for specific scenarios such as single batch inference or long\nprefill, Fiddler performs better in all scenarios. Compared against different\nbaselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30\ntimes in long prefill processing, and 11.57 times in beam search inference. The\ncode of Fiddler is publicly available at https://github.com/efeslab/fiddler.\n", "link": "http://arxiv.org/abs/2402.07033v3", "date": "2025-05-01", "relevancy": 1.94, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fiddler%3A%20CPU-GPU%20Orchestration%20for%20Fast%20Inference%20of%20Mixture-of-Experts%0A%20%20Models&body=Title%3A%20Fiddler%3A%20CPU-GPU%20Orchestration%20for%20Fast%20Inference%20of%20Mixture-of-Experts%0A%20%20Models%0AAuthor%3A%20Keisuke%20Kamahori%20and%20Tian%20Tang%20and%20Yile%20Gu%20and%20Kan%20Zhu%20and%20Baris%20Kasikci%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20the%20Mixture-of-Experts%20%28MoE%29%20architectures%0Ahave%20shown%20promising%20performance%20on%20various%20tasks.%20However%2C%20due%20to%20the%20huge%0Amodel%20sizes%2C%20running%20them%20in%20resource-constrained%20environments%20where%20the%20GPU%0Amemory%20is%20not%20abundant%20is%20challenging.%20Some%20existing%20systems%20propose%20to%20use%20CPU%0Aresources%20to%20solve%20that%2C%20but%20they%20either%20suffer%20from%20the%20significant%20overhead%0Aof%20frequently%20moving%20data%20between%20CPU%20and%20GPU%2C%20or%20fail%20to%20consider%20distinct%0Acharacteristics%20of%20CPUs%20and%20GPUs.%20This%20paper%20proposes%20Fiddler%2C%20a%0Aresource-efficient%20inference%20system%20for%20MoE%20models%20with%20limited%20GPU%20resources.%0AFiddler%20strategically%20utilizes%20CPU%20and%20GPU%20resources%20by%20determining%20the%20optimal%0Aexecution%20strategy.%20Our%20evaluation%20shows%20that%2C%20unlike%20state-of-the-art%20systems%0Athat%20optimize%20for%20specific%20scenarios%20such%20as%20single%20batch%20inference%20or%20long%0Aprefill%2C%20Fiddler%20performs%20better%20in%20all%20scenarios.%20Compared%20against%20different%0Abaselines%2C%20Fiddler%20achieves%201.26%20times%20speed%20up%20in%20single%20batch%20inference%2C%201.30%0Atimes%20in%20long%20prefill%20processing%2C%20and%2011.57%20times%20in%20beam%20search%20inference.%20The%0Acode%20of%20Fiddler%20is%20publicly%20available%20at%20https%3A//github.com/efeslab/fiddler.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07033v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiddler%253A%2520CPU-GPU%2520Orchestration%2520for%2520Fast%2520Inference%2520of%2520Mixture-of-Experts%250A%2520%2520Models%26entry.906535625%3DKeisuke%2520Kamahori%2520and%2520Tian%2520Tang%2520and%2520Yile%2520Gu%2520and%2520Kan%2520Zhu%2520and%2520Baris%2520Kasikci%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520the%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%250Ahave%2520shown%2520promising%2520performance%2520on%2520various%2520tasks.%2520However%252C%2520due%2520to%2520the%2520huge%250Amodel%2520sizes%252C%2520running%2520them%2520in%2520resource-constrained%2520environments%2520where%2520the%2520GPU%250Amemory%2520is%2520not%2520abundant%2520is%2520challenging.%2520Some%2520existing%2520systems%2520propose%2520to%2520use%2520CPU%250Aresources%2520to%2520solve%2520that%252C%2520but%2520they%2520either%2520suffer%2520from%2520the%2520significant%2520overhead%250Aof%2520frequently%2520moving%2520data%2520between%2520CPU%2520and%2520GPU%252C%2520or%2520fail%2520to%2520consider%2520distinct%250Acharacteristics%2520of%2520CPUs%2520and%2520GPUs.%2520This%2520paper%2520proposes%2520Fiddler%252C%2520a%250Aresource-efficient%2520inference%2520system%2520for%2520MoE%2520models%2520with%2520limited%2520GPU%2520resources.%250AFiddler%2520strategically%2520utilizes%2520CPU%2520and%2520GPU%2520resources%2520by%2520determining%2520the%2520optimal%250Aexecution%2520strategy.%2520Our%2520evaluation%2520shows%2520that%252C%2520unlike%2520state-of-the-art%2520systems%250Athat%2520optimize%2520for%2520specific%2520scenarios%2520such%2520as%2520single%2520batch%2520inference%2520or%2520long%250Aprefill%252C%2520Fiddler%2520performs%2520better%2520in%2520all%2520scenarios.%2520Compared%2520against%2520different%250Abaselines%252C%2520Fiddler%2520achieves%25201.26%2520times%2520speed%2520up%2520in%2520single%2520batch%2520inference%252C%25201.30%250Atimes%2520in%2520long%2520prefill%2520processing%252C%2520and%252011.57%2520times%2520in%2520beam%2520search%2520inference.%2520The%250Acode%2520of%2520Fiddler%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/efeslab/fiddler.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07033v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fiddler%3A%20CPU-GPU%20Orchestration%20for%20Fast%20Inference%20of%20Mixture-of-Experts%0A%20%20Models&entry.906535625=Keisuke%20Kamahori%20and%20Tian%20Tang%20and%20Yile%20Gu%20and%20Kan%20Zhu%20and%20Baris%20Kasikci&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20the%20Mixture-of-Experts%20%28MoE%29%20architectures%0Ahave%20shown%20promising%20performance%20on%20various%20tasks.%20However%2C%20due%20to%20the%20huge%0Amodel%20sizes%2C%20running%20them%20in%20resource-constrained%20environments%20where%20the%20GPU%0Amemory%20is%20not%20abundant%20is%20challenging.%20Some%20existing%20systems%20propose%20to%20use%20CPU%0Aresources%20to%20solve%20that%2C%20but%20they%20either%20suffer%20from%20the%20significant%20overhead%0Aof%20frequently%20moving%20data%20between%20CPU%20and%20GPU%2C%20or%20fail%20to%20consider%20distinct%0Acharacteristics%20of%20CPUs%20and%20GPUs.%20This%20paper%20proposes%20Fiddler%2C%20a%0Aresource-efficient%20inference%20system%20for%20MoE%20models%20with%20limited%20GPU%20resources.%0AFiddler%20strategically%20utilizes%20CPU%20and%20GPU%20resources%20by%20determining%20the%20optimal%0Aexecution%20strategy.%20Our%20evaluation%20shows%20that%2C%20unlike%20state-of-the-art%20systems%0Athat%20optimize%20for%20specific%20scenarios%20such%20as%20single%20batch%20inference%20or%20long%0Aprefill%2C%20Fiddler%20performs%20better%20in%20all%20scenarios.%20Compared%20against%20different%0Abaselines%2C%20Fiddler%20achieves%201.26%20times%20speed%20up%20in%20single%20batch%20inference%2C%201.30%0Atimes%20in%20long%20prefill%20processing%2C%20and%2011.57%20times%20in%20beam%20search%20inference.%20The%0Acode%20of%20Fiddler%20is%20publicly%20available%20at%20https%3A//github.com/efeslab/fiddler.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07033v3&entry.124074799=Read"},
{"title": "Can LLMs Help Improve Analogical Reasoning For Strategic Decisions?\n  Experimental Evidence from Humans and GPT-4", "author": "Phanish Puranam and Prothit Sen and Maciej Workiewicz", "abstract": "  This study investigates whether large language models, specifically GPT4, can\nmatch human capabilities in analogical reasoning within strategic decision\nmaking contexts. Using a novel experimental design involving source to target\nmatching, we find that GPT4 achieves high recall by retrieving all plausible\nanalogies but suffers from low precision, frequently applying incorrect\nanalogies based on superficial similarities. In contrast, human participants\nexhibit high precision but low recall, selecting fewer analogies yet with\nstronger causal alignment. These findings advance theory by identifying\nmatching, the evaluative phase of analogical reasoning, as a distinct step that\nrequires accurate causal mapping beyond simple retrieval. While current LLMs\nare proficient in generating candidate analogies, humans maintain a comparative\nadvantage in recognizing deep structural similarities across domains. Error\nanalysis reveals that AI errors arise from surface level matching, whereas\nhuman errors stem from misinterpretations of causal structure. Taken together,\nthe results suggest a productive division of labor in AI assisted\norganizational decision making where LLMs may serve as broad analogy\ngenerators, while humans act as critical evaluators, applying the most\ncontextually appropriate analogies to strategic problems.\n", "link": "http://arxiv.org/abs/2505.00603v1", "date": "2025-05-01", "relevancy": 1.9111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Help%20Improve%20Analogical%20Reasoning%20For%20Strategic%20Decisions%3F%0A%20%20Experimental%20Evidence%20from%20Humans%20and%20GPT-4&body=Title%3A%20Can%20LLMs%20Help%20Improve%20Analogical%20Reasoning%20For%20Strategic%20Decisions%3F%0A%20%20Experimental%20Evidence%20from%20Humans%20and%20GPT-4%0AAuthor%3A%20Phanish%20Puranam%20and%20Prothit%20Sen%20and%20Maciej%20Workiewicz%0AAbstract%3A%20%20%20This%20study%20investigates%20whether%20large%20language%20models%2C%20specifically%20GPT4%2C%20can%0Amatch%20human%20capabilities%20in%20analogical%20reasoning%20within%20strategic%20decision%0Amaking%20contexts.%20Using%20a%20novel%20experimental%20design%20involving%20source%20to%20target%0Amatching%2C%20we%20find%20that%20GPT4%20achieves%20high%20recall%20by%20retrieving%20all%20plausible%0Aanalogies%20but%20suffers%20from%20low%20precision%2C%20frequently%20applying%20incorrect%0Aanalogies%20based%20on%20superficial%20similarities.%20In%20contrast%2C%20human%20participants%0Aexhibit%20high%20precision%20but%20low%20recall%2C%20selecting%20fewer%20analogies%20yet%20with%0Astronger%20causal%20alignment.%20These%20findings%20advance%20theory%20by%20identifying%0Amatching%2C%20the%20evaluative%20phase%20of%20analogical%20reasoning%2C%20as%20a%20distinct%20step%20that%0Arequires%20accurate%20causal%20mapping%20beyond%20simple%20retrieval.%20While%20current%20LLMs%0Aare%20proficient%20in%20generating%20candidate%20analogies%2C%20humans%20maintain%20a%20comparative%0Aadvantage%20in%20recognizing%20deep%20structural%20similarities%20across%20domains.%20Error%0Aanalysis%20reveals%20that%20AI%20errors%20arise%20from%20surface%20level%20matching%2C%20whereas%0Ahuman%20errors%20stem%20from%20misinterpretations%20of%20causal%20structure.%20Taken%20together%2C%0Athe%20results%20suggest%20a%20productive%20division%20of%20labor%20in%20AI%20assisted%0Aorganizational%20decision%20making%20where%20LLMs%20may%20serve%20as%20broad%20analogy%0Agenerators%2C%20while%20humans%20act%20as%20critical%20evaluators%2C%20applying%20the%20most%0Acontextually%20appropriate%20analogies%20to%20strategic%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Help%2520Improve%2520Analogical%2520Reasoning%2520For%2520Strategic%2520Decisions%253F%250A%2520%2520Experimental%2520Evidence%2520from%2520Humans%2520and%2520GPT-4%26entry.906535625%3DPhanish%2520Puranam%2520and%2520Prothit%2520Sen%2520and%2520Maciej%2520Workiewicz%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520whether%2520large%2520language%2520models%252C%2520specifically%2520GPT4%252C%2520can%250Amatch%2520human%2520capabilities%2520in%2520analogical%2520reasoning%2520within%2520strategic%2520decision%250Amaking%2520contexts.%2520Using%2520a%2520novel%2520experimental%2520design%2520involving%2520source%2520to%2520target%250Amatching%252C%2520we%2520find%2520that%2520GPT4%2520achieves%2520high%2520recall%2520by%2520retrieving%2520all%2520plausible%250Aanalogies%2520but%2520suffers%2520from%2520low%2520precision%252C%2520frequently%2520applying%2520incorrect%250Aanalogies%2520based%2520on%2520superficial%2520similarities.%2520In%2520contrast%252C%2520human%2520participants%250Aexhibit%2520high%2520precision%2520but%2520low%2520recall%252C%2520selecting%2520fewer%2520analogies%2520yet%2520with%250Astronger%2520causal%2520alignment.%2520These%2520findings%2520advance%2520theory%2520by%2520identifying%250Amatching%252C%2520the%2520evaluative%2520phase%2520of%2520analogical%2520reasoning%252C%2520as%2520a%2520distinct%2520step%2520that%250Arequires%2520accurate%2520causal%2520mapping%2520beyond%2520simple%2520retrieval.%2520While%2520current%2520LLMs%250Aare%2520proficient%2520in%2520generating%2520candidate%2520analogies%252C%2520humans%2520maintain%2520a%2520comparative%250Aadvantage%2520in%2520recognizing%2520deep%2520structural%2520similarities%2520across%2520domains.%2520Error%250Aanalysis%2520reveals%2520that%2520AI%2520errors%2520arise%2520from%2520surface%2520level%2520matching%252C%2520whereas%250Ahuman%2520errors%2520stem%2520from%2520misinterpretations%2520of%2520causal%2520structure.%2520Taken%2520together%252C%250Athe%2520results%2520suggest%2520a%2520productive%2520division%2520of%2520labor%2520in%2520AI%2520assisted%250Aorganizational%2520decision%2520making%2520where%2520LLMs%2520may%2520serve%2520as%2520broad%2520analogy%250Agenerators%252C%2520while%2520humans%2520act%2520as%2520critical%2520evaluators%252C%2520applying%2520the%2520most%250Acontextually%2520appropriate%2520analogies%2520to%2520strategic%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Help%20Improve%20Analogical%20Reasoning%20For%20Strategic%20Decisions%3F%0A%20%20Experimental%20Evidence%20from%20Humans%20and%20GPT-4&entry.906535625=Phanish%20Puranam%20and%20Prothit%20Sen%20and%20Maciej%20Workiewicz&entry.1292438233=%20%20This%20study%20investigates%20whether%20large%20language%20models%2C%20specifically%20GPT4%2C%20can%0Amatch%20human%20capabilities%20in%20analogical%20reasoning%20within%20strategic%20decision%0Amaking%20contexts.%20Using%20a%20novel%20experimental%20design%20involving%20source%20to%20target%0Amatching%2C%20we%20find%20that%20GPT4%20achieves%20high%20recall%20by%20retrieving%20all%20plausible%0Aanalogies%20but%20suffers%20from%20low%20precision%2C%20frequently%20applying%20incorrect%0Aanalogies%20based%20on%20superficial%20similarities.%20In%20contrast%2C%20human%20participants%0Aexhibit%20high%20precision%20but%20low%20recall%2C%20selecting%20fewer%20analogies%20yet%20with%0Astronger%20causal%20alignment.%20These%20findings%20advance%20theory%20by%20identifying%0Amatching%2C%20the%20evaluative%20phase%20of%20analogical%20reasoning%2C%20as%20a%20distinct%20step%20that%0Arequires%20accurate%20causal%20mapping%20beyond%20simple%20retrieval.%20While%20current%20LLMs%0Aare%20proficient%20in%20generating%20candidate%20analogies%2C%20humans%20maintain%20a%20comparative%0Aadvantage%20in%20recognizing%20deep%20structural%20similarities%20across%20domains.%20Error%0Aanalysis%20reveals%20that%20AI%20errors%20arise%20from%20surface%20level%20matching%2C%20whereas%0Ahuman%20errors%20stem%20from%20misinterpretations%20of%20causal%20structure.%20Taken%20together%2C%0Athe%20results%20suggest%20a%20productive%20division%20of%20labor%20in%20AI%20assisted%0Aorganizational%20decision%20making%20where%20LLMs%20may%20serve%20as%20broad%20analogy%0Agenerators%2C%20while%20humans%20act%20as%20critical%20evaluators%2C%20applying%20the%20most%0Acontextually%20appropriate%20analogies%20to%20strategic%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00603v1&entry.124074799=Read"},
{"title": "Per-Domain Generalizing Policies: On Validation Instances and Scaling\n  Behavior", "author": "Timo P. Gros and Nicola J. M\u00fcller and Daniel Fiser and Isabel Valera and Verena Wolf and J\u00f6rg Hoffmann", "abstract": "  Recent work has shown that successful per-domain generalizing action policies\ncan be learned. Scaling behavior, from small training instances to large test\ninstances, is the key objective; and the use of validation instances larger\nthan training instances is one key to achieve it. Prior work has used fixed\nvalidation sets. Here, we introduce a method generating the validation set\ndynamically, on the fly, increasing instance size so long as informative and\nfeasible.We also introduce refined methodology for evaluating scaling behavior,\ngenerating test instances systematically to guarantee a given confidence in\ncoverage performance for each instance size. In experiments, dynamic validation\nimproves scaling behavior of GNN policies in all 9 domains used.\n", "link": "http://arxiv.org/abs/2505.00439v1", "date": "2025-05-01", "relevancy": 1.9054, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5011}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4795}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Per-Domain%20Generalizing%20Policies%3A%20On%20Validation%20Instances%20and%20Scaling%0A%20%20Behavior&body=Title%3A%20Per-Domain%20Generalizing%20Policies%3A%20On%20Validation%20Instances%20and%20Scaling%0A%20%20Behavior%0AAuthor%3A%20Timo%20P.%20Gros%20and%20Nicola%20J.%20M%C3%BCller%20and%20Daniel%20Fiser%20and%20Isabel%20Valera%20and%20Verena%20Wolf%20and%20J%C3%B6rg%20Hoffmann%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20successful%20per-domain%20generalizing%20action%20policies%0Acan%20be%20learned.%20Scaling%20behavior%2C%20from%20small%20training%20instances%20to%20large%20test%0Ainstances%2C%20is%20the%20key%20objective%3B%20and%20the%20use%20of%20validation%20instances%20larger%0Athan%20training%20instances%20is%20one%20key%20to%20achieve%20it.%20Prior%20work%20has%20used%20fixed%0Avalidation%20sets.%20Here%2C%20we%20introduce%20a%20method%20generating%20the%20validation%20set%0Adynamically%2C%20on%20the%20fly%2C%20increasing%20instance%20size%20so%20long%20as%20informative%20and%0Afeasible.We%20also%20introduce%20refined%20methodology%20for%20evaluating%20scaling%20behavior%2C%0Agenerating%20test%20instances%20systematically%20to%20guarantee%20a%20given%20confidence%20in%0Acoverage%20performance%20for%20each%20instance%20size.%20In%20experiments%2C%20dynamic%20validation%0Aimproves%20scaling%20behavior%20of%20GNN%20policies%20in%20all%209%20domains%20used.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPer-Domain%2520Generalizing%2520Policies%253A%2520On%2520Validation%2520Instances%2520and%2520Scaling%250A%2520%2520Behavior%26entry.906535625%3DTimo%2520P.%2520Gros%2520and%2520Nicola%2520J.%2520M%25C3%25BCller%2520and%2520Daniel%2520Fiser%2520and%2520Isabel%2520Valera%2520and%2520Verena%2520Wolf%2520and%2520J%25C3%25B6rg%2520Hoffmann%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520successful%2520per-domain%2520generalizing%2520action%2520policies%250Acan%2520be%2520learned.%2520Scaling%2520behavior%252C%2520from%2520small%2520training%2520instances%2520to%2520large%2520test%250Ainstances%252C%2520is%2520the%2520key%2520objective%253B%2520and%2520the%2520use%2520of%2520validation%2520instances%2520larger%250Athan%2520training%2520instances%2520is%2520one%2520key%2520to%2520achieve%2520it.%2520Prior%2520work%2520has%2520used%2520fixed%250Avalidation%2520sets.%2520Here%252C%2520we%2520introduce%2520a%2520method%2520generating%2520the%2520validation%2520set%250Adynamically%252C%2520on%2520the%2520fly%252C%2520increasing%2520instance%2520size%2520so%2520long%2520as%2520informative%2520and%250Afeasible.We%2520also%2520introduce%2520refined%2520methodology%2520for%2520evaluating%2520scaling%2520behavior%252C%250Agenerating%2520test%2520instances%2520systematically%2520to%2520guarantee%2520a%2520given%2520confidence%2520in%250Acoverage%2520performance%2520for%2520each%2520instance%2520size.%2520In%2520experiments%252C%2520dynamic%2520validation%250Aimproves%2520scaling%2520behavior%2520of%2520GNN%2520policies%2520in%2520all%25209%2520domains%2520used.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Per-Domain%20Generalizing%20Policies%3A%20On%20Validation%20Instances%20and%20Scaling%0A%20%20Behavior&entry.906535625=Timo%20P.%20Gros%20and%20Nicola%20J.%20M%C3%BCller%20and%20Daniel%20Fiser%20and%20Isabel%20Valera%20and%20Verena%20Wolf%20and%20J%C3%B6rg%20Hoffmann&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20successful%20per-domain%20generalizing%20action%20policies%0Acan%20be%20learned.%20Scaling%20behavior%2C%20from%20small%20training%20instances%20to%20large%20test%0Ainstances%2C%20is%20the%20key%20objective%3B%20and%20the%20use%20of%20validation%20instances%20larger%0Athan%20training%20instances%20is%20one%20key%20to%20achieve%20it.%20Prior%20work%20has%20used%20fixed%0Avalidation%20sets.%20Here%2C%20we%20introduce%20a%20method%20generating%20the%20validation%20set%0Adynamically%2C%20on%20the%20fly%2C%20increasing%20instance%20size%20so%20long%20as%20informative%20and%0Afeasible.We%20also%20introduce%20refined%20methodology%20for%20evaluating%20scaling%20behavior%2C%0Agenerating%20test%20instances%20systematically%20to%20guarantee%20a%20given%20confidence%20in%0Acoverage%20performance%20for%20each%20instance%20size.%20In%20experiments%2C%20dynamic%20validation%0Aimproves%20scaling%20behavior%20of%20GNN%20policies%20in%20all%209%20domains%20used.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00439v1&entry.124074799=Read"},
{"title": "Bayes-Optimal Fair Classification with Multiple Sensitive Features", "author": "Yi Yang and Yinghui Huang and Xiangyu Chang", "abstract": "  Existing theoretical work on Bayes-optimal fair classifiers usually considers\na single (binary) sensitive feature. In practice, individuals are often defined\nby multiple sensitive features. In this paper, we characterize the\nBayes-optimal fair classifier for multiple sensitive features under general\napproximate fairness measures, including mean difference and mean ratio. We\nshow that these approximate measures for existing group fairness notions,\nincluding Demographic Parity, Equal Opportunity, Predictive Equality, and\nAccuracy Parity, are linear transformations of selection rates for specific\ngroups defined by both labels and sensitive features. We then characterize that\nBayes-optimal fair classifiers for multiple sensitive features become\ninstance-dependent thresholding rules that rely on a weighted sum of these\ngroup membership probabilities. Our framework applies to both attribute-aware\nand attribute-blind settings and can accommodate composite fairness notions\nlike Equalized Odds. Building on this, we propose two practical algorithms for\nBayes-optimal fair classification via in-processing and post-processing. We\nshow empirically that our methods compare favorably to existing methods.\n", "link": "http://arxiv.org/abs/2505.00631v1", "date": "2025-05-01", "relevancy": 1.902, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5044}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4731}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayes-Optimal%20Fair%20Classification%20with%20Multiple%20Sensitive%20Features&body=Title%3A%20Bayes-Optimal%20Fair%20Classification%20with%20Multiple%20Sensitive%20Features%0AAuthor%3A%20Yi%20Yang%20and%20Yinghui%20Huang%20and%20Xiangyu%20Chang%0AAbstract%3A%20%20%20Existing%20theoretical%20work%20on%20Bayes-optimal%20fair%20classifiers%20usually%20considers%0Aa%20single%20%28binary%29%20sensitive%20feature.%20In%20practice%2C%20individuals%20are%20often%20defined%0Aby%20multiple%20sensitive%20features.%20In%20this%20paper%2C%20we%20characterize%20the%0ABayes-optimal%20fair%20classifier%20for%20multiple%20sensitive%20features%20under%20general%0Aapproximate%20fairness%20measures%2C%20including%20mean%20difference%20and%20mean%20ratio.%20We%0Ashow%20that%20these%20approximate%20measures%20for%20existing%20group%20fairness%20notions%2C%0Aincluding%20Demographic%20Parity%2C%20Equal%20Opportunity%2C%20Predictive%20Equality%2C%20and%0AAccuracy%20Parity%2C%20are%20linear%20transformations%20of%20selection%20rates%20for%20specific%0Agroups%20defined%20by%20both%20labels%20and%20sensitive%20features.%20We%20then%20characterize%20that%0ABayes-optimal%20fair%20classifiers%20for%20multiple%20sensitive%20features%20become%0Ainstance-dependent%20thresholding%20rules%20that%20rely%20on%20a%20weighted%20sum%20of%20these%0Agroup%20membership%20probabilities.%20Our%20framework%20applies%20to%20both%20attribute-aware%0Aand%20attribute-blind%20settings%20and%20can%20accommodate%20composite%20fairness%20notions%0Alike%20Equalized%20Odds.%20Building%20on%20this%2C%20we%20propose%20two%20practical%20algorithms%20for%0ABayes-optimal%20fair%20classification%20via%20in-processing%20and%20post-processing.%20We%0Ashow%20empirically%20that%20our%20methods%20compare%20favorably%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayes-Optimal%2520Fair%2520Classification%2520with%2520Multiple%2520Sensitive%2520Features%26entry.906535625%3DYi%2520Yang%2520and%2520Yinghui%2520Huang%2520and%2520Xiangyu%2520Chang%26entry.1292438233%3D%2520%2520Existing%2520theoretical%2520work%2520on%2520Bayes-optimal%2520fair%2520classifiers%2520usually%2520considers%250Aa%2520single%2520%2528binary%2529%2520sensitive%2520feature.%2520In%2520practice%252C%2520individuals%2520are%2520often%2520defined%250Aby%2520multiple%2520sensitive%2520features.%2520In%2520this%2520paper%252C%2520we%2520characterize%2520the%250ABayes-optimal%2520fair%2520classifier%2520for%2520multiple%2520sensitive%2520features%2520under%2520general%250Aapproximate%2520fairness%2520measures%252C%2520including%2520mean%2520difference%2520and%2520mean%2520ratio.%2520We%250Ashow%2520that%2520these%2520approximate%2520measures%2520for%2520existing%2520group%2520fairness%2520notions%252C%250Aincluding%2520Demographic%2520Parity%252C%2520Equal%2520Opportunity%252C%2520Predictive%2520Equality%252C%2520and%250AAccuracy%2520Parity%252C%2520are%2520linear%2520transformations%2520of%2520selection%2520rates%2520for%2520specific%250Agroups%2520defined%2520by%2520both%2520labels%2520and%2520sensitive%2520features.%2520We%2520then%2520characterize%2520that%250ABayes-optimal%2520fair%2520classifiers%2520for%2520multiple%2520sensitive%2520features%2520become%250Ainstance-dependent%2520thresholding%2520rules%2520that%2520rely%2520on%2520a%2520weighted%2520sum%2520of%2520these%250Agroup%2520membership%2520probabilities.%2520Our%2520framework%2520applies%2520to%2520both%2520attribute-aware%250Aand%2520attribute-blind%2520settings%2520and%2520can%2520accommodate%2520composite%2520fairness%2520notions%250Alike%2520Equalized%2520Odds.%2520Building%2520on%2520this%252C%2520we%2520propose%2520two%2520practical%2520algorithms%2520for%250ABayes-optimal%2520fair%2520classification%2520via%2520in-processing%2520and%2520post-processing.%2520We%250Ashow%2520empirically%2520that%2520our%2520methods%2520compare%2520favorably%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayes-Optimal%20Fair%20Classification%20with%20Multiple%20Sensitive%20Features&entry.906535625=Yi%20Yang%20and%20Yinghui%20Huang%20and%20Xiangyu%20Chang&entry.1292438233=%20%20Existing%20theoretical%20work%20on%20Bayes-optimal%20fair%20classifiers%20usually%20considers%0Aa%20single%20%28binary%29%20sensitive%20feature.%20In%20practice%2C%20individuals%20are%20often%20defined%0Aby%20multiple%20sensitive%20features.%20In%20this%20paper%2C%20we%20characterize%20the%0ABayes-optimal%20fair%20classifier%20for%20multiple%20sensitive%20features%20under%20general%0Aapproximate%20fairness%20measures%2C%20including%20mean%20difference%20and%20mean%20ratio.%20We%0Ashow%20that%20these%20approximate%20measures%20for%20existing%20group%20fairness%20notions%2C%0Aincluding%20Demographic%20Parity%2C%20Equal%20Opportunity%2C%20Predictive%20Equality%2C%20and%0AAccuracy%20Parity%2C%20are%20linear%20transformations%20of%20selection%20rates%20for%20specific%0Agroups%20defined%20by%20both%20labels%20and%20sensitive%20features.%20We%20then%20characterize%20that%0ABayes-optimal%20fair%20classifiers%20for%20multiple%20sensitive%20features%20become%0Ainstance-dependent%20thresholding%20rules%20that%20rely%20on%20a%20weighted%20sum%20of%20these%0Agroup%20membership%20probabilities.%20Our%20framework%20applies%20to%20both%20attribute-aware%0Aand%20attribute-blind%20settings%20and%20can%20accommodate%20composite%20fairness%20notions%0Alike%20Equalized%20Odds.%20Building%20on%20this%2C%20we%20propose%20two%20practical%20algorithms%20for%0ABayes-optimal%20fair%20classification%20via%20in-processing%20and%20post-processing.%20We%0Ashow%20empirically%20that%20our%20methods%20compare%20favorably%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00631v1&entry.124074799=Read"},
{"title": "Triggering Hallucinations in LLMs: A Quantitative Study of\n  Prompt-Induced Hallucination in Large Language Models", "author": "Makoto Sato", "abstract": "  Hallucinations in large language models (LLMs) present a growing challenge\nacross real-world applications, from healthcare to law, where factual\nreliability is essential. Despite advances in alignment and instruction tuning,\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\nan open problem. In this study, we propose a prompt-based framework to\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\nperiodic table of elements and tarot divination) in a misleading way, and a\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\nconfidence, and coherence of the output. Controlled experiments across multiple\nLLMs revealed that HIPs consistently produced less coherent and more\nhallucinated responses than their null-fusion controls. These effects varied\nacross models, with reasoning-oriented LLMs showing distinct profiles from\ngeneral-purpose ones. Our framework provides a reproducible testbed for\nstudying hallucination vulnerability, and opens the door to developing safer,\nmore introspective LLMs that can detect and self-regulate the onset of\nconceptual instability.\n", "link": "http://arxiv.org/abs/2505.00557v1", "date": "2025-05-01", "relevancy": 1.8718, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triggering%20Hallucinations%20in%20LLMs%3A%20A%20Quantitative%20Study%20of%0A%20%20Prompt-Induced%20Hallucination%20in%20Large%20Language%20Models&body=Title%3A%20Triggering%20Hallucinations%20in%20LLMs%3A%20A%20Quantitative%20Study%20of%0A%20%20Prompt-Induced%20Hallucination%20in%20Large%20Language%20Models%0AAuthor%3A%20Makoto%20Sato%0AAbstract%3A%20%20%20Hallucinations%20in%20large%20language%20models%20%28LLMs%29%20present%20a%20growing%20challenge%0Aacross%20real-world%20applications%2C%20from%20healthcare%20to%20law%2C%20where%20factual%0Areliability%20is%20essential.%20Despite%20advances%20in%20alignment%20and%20instruction%20tuning%2C%0ALLMs%20can%20still%20generate%20outputs%20that%20are%20fluent%20yet%20fundamentally%20untrue.%0AUnderstanding%20the%20cognitive%20dynamics%20that%20underlie%20these%20hallucinations%20remains%0Aan%20open%20problem.%20In%20this%20study%2C%20we%20propose%20a%20prompt-based%20framework%20to%0Asystematically%20trigger%20and%20quantify%20hallucination%3A%20a%20Hallucination-Inducing%0APrompt%20%28HIP%29%2C%20which%20synthetically%20fuses%20semantically%20distant%20concepts%20%28e.g.%2C%0Aperiodic%20table%20of%20elements%20and%20tarot%20divination%29%20in%20a%20misleading%20way%2C%20and%20a%0AHallucination%20Quantifying%20Prompt%20%28HQP%29%2C%20which%20scores%20the%20plausibility%2C%0Aconfidence%2C%20and%20coherence%20of%20the%20output.%20Controlled%20experiments%20across%20multiple%0ALLMs%20revealed%20that%20HIPs%20consistently%20produced%20less%20coherent%20and%20more%0Ahallucinated%20responses%20than%20their%20null-fusion%20controls.%20These%20effects%20varied%0Aacross%20models%2C%20with%20reasoning-oriented%20LLMs%20showing%20distinct%20profiles%20from%0Ageneral-purpose%20ones.%20Our%20framework%20provides%20a%20reproducible%20testbed%20for%0Astudying%20hallucination%20vulnerability%2C%20and%20opens%20the%20door%20to%20developing%20safer%2C%0Amore%20introspective%20LLMs%20that%20can%20detect%20and%20self-regulate%20the%20onset%20of%0Aconceptual%20instability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriggering%2520Hallucinations%2520in%2520LLMs%253A%2520A%2520Quantitative%2520Study%2520of%250A%2520%2520Prompt-Induced%2520Hallucination%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMakoto%2520Sato%26entry.1292438233%3D%2520%2520Hallucinations%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520present%2520a%2520growing%2520challenge%250Aacross%2520real-world%2520applications%252C%2520from%2520healthcare%2520to%2520law%252C%2520where%2520factual%250Areliability%2520is%2520essential.%2520Despite%2520advances%2520in%2520alignment%2520and%2520instruction%2520tuning%252C%250ALLMs%2520can%2520still%2520generate%2520outputs%2520that%2520are%2520fluent%2520yet%2520fundamentally%2520untrue.%250AUnderstanding%2520the%2520cognitive%2520dynamics%2520that%2520underlie%2520these%2520hallucinations%2520remains%250Aan%2520open%2520problem.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520prompt-based%2520framework%2520to%250Asystematically%2520trigger%2520and%2520quantify%2520hallucination%253A%2520a%2520Hallucination-Inducing%250APrompt%2520%2528HIP%2529%252C%2520which%2520synthetically%2520fuses%2520semantically%2520distant%2520concepts%2520%2528e.g.%252C%250Aperiodic%2520table%2520of%2520elements%2520and%2520tarot%2520divination%2529%2520in%2520a%2520misleading%2520way%252C%2520and%2520a%250AHallucination%2520Quantifying%2520Prompt%2520%2528HQP%2529%252C%2520which%2520scores%2520the%2520plausibility%252C%250Aconfidence%252C%2520and%2520coherence%2520of%2520the%2520output.%2520Controlled%2520experiments%2520across%2520multiple%250ALLMs%2520revealed%2520that%2520HIPs%2520consistently%2520produced%2520less%2520coherent%2520and%2520more%250Ahallucinated%2520responses%2520than%2520their%2520null-fusion%2520controls.%2520These%2520effects%2520varied%250Aacross%2520models%252C%2520with%2520reasoning-oriented%2520LLMs%2520showing%2520distinct%2520profiles%2520from%250Ageneral-purpose%2520ones.%2520Our%2520framework%2520provides%2520a%2520reproducible%2520testbed%2520for%250Astudying%2520hallucination%2520vulnerability%252C%2520and%2520opens%2520the%2520door%2520to%2520developing%2520safer%252C%250Amore%2520introspective%2520LLMs%2520that%2520can%2520detect%2520and%2520self-regulate%2520the%2520onset%2520of%250Aconceptual%2520instability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triggering%20Hallucinations%20in%20LLMs%3A%20A%20Quantitative%20Study%20of%0A%20%20Prompt-Induced%20Hallucination%20in%20Large%20Language%20Models&entry.906535625=Makoto%20Sato&entry.1292438233=%20%20Hallucinations%20in%20large%20language%20models%20%28LLMs%29%20present%20a%20growing%20challenge%0Aacross%20real-world%20applications%2C%20from%20healthcare%20to%20law%2C%20where%20factual%0Areliability%20is%20essential.%20Despite%20advances%20in%20alignment%20and%20instruction%20tuning%2C%0ALLMs%20can%20still%20generate%20outputs%20that%20are%20fluent%20yet%20fundamentally%20untrue.%0AUnderstanding%20the%20cognitive%20dynamics%20that%20underlie%20these%20hallucinations%20remains%0Aan%20open%20problem.%20In%20this%20study%2C%20we%20propose%20a%20prompt-based%20framework%20to%0Asystematically%20trigger%20and%20quantify%20hallucination%3A%20a%20Hallucination-Inducing%0APrompt%20%28HIP%29%2C%20which%20synthetically%20fuses%20semantically%20distant%20concepts%20%28e.g.%2C%0Aperiodic%20table%20of%20elements%20and%20tarot%20divination%29%20in%20a%20misleading%20way%2C%20and%20a%0AHallucination%20Quantifying%20Prompt%20%28HQP%29%2C%20which%20scores%20the%20plausibility%2C%0Aconfidence%2C%20and%20coherence%20of%20the%20output.%20Controlled%20experiments%20across%20multiple%0ALLMs%20revealed%20that%20HIPs%20consistently%20produced%20less%20coherent%20and%20more%0Ahallucinated%20responses%20than%20their%20null-fusion%20controls.%20These%20effects%20varied%0Aacross%20models%2C%20with%20reasoning-oriented%20LLMs%20showing%20distinct%20profiles%20from%0Ageneral-purpose%20ones.%20Our%20framework%20provides%20a%20reproducible%20testbed%20for%0Astudying%20hallucination%20vulnerability%2C%20and%20opens%20the%20door%20to%20developing%20safer%2C%0Amore%20introspective%20LLMs%20that%20can%20detect%20and%20self-regulate%20the%20onset%20of%0Aconceptual%20instability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00557v1&entry.124074799=Read"},
{"title": "DeepCritic: Deliberate Critique with Large Language Models", "author": "Wenkai Yang and Jingwen Chen and Yankai Lin and Ji-Rong Wen", "abstract": "  As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.\n", "link": "http://arxiv.org/abs/2505.00662v1", "date": "2025-05-01", "relevancy": 1.8626, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepCritic%3A%20Deliberate%20Critique%20with%20Large%20Language%20Models&body=Title%3A%20DeepCritic%3A%20Deliberate%20Critique%20with%20Large%20Language%20Models%0AAuthor%3A%20Wenkai%20Yang%20and%20Jingwen%20Chen%20and%20Yankai%20Lin%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20rapidly%20evolving%2C%20providing%20accurate%0Afeedback%20and%20scalable%20oversight%20on%20their%20outputs%20becomes%20an%20urgent%20and%20critical%0Aproblem.%20Leveraging%20LLMs%20as%20critique%20models%20to%20achieve%20automated%20supervision%20is%0Aa%20promising%20solution.%20In%20this%20work%2C%20we%20focus%20on%20studying%20and%20enhancing%20the%20math%0Acritique%20ability%20of%20LLMs.%20Current%20LLM%20critics%20provide%20critiques%20that%20are%20too%0Ashallow%20and%20superficial%20on%20each%20step%2C%20leading%20to%20low%20judgment%20accuracy%20and%0Astruggling%20to%20offer%20sufficient%20feedback%20for%20the%20LLM%20generator%20to%20correct%0Amistakes.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20and%20effective%20two-stage%0Aframework%20to%20develop%20LLM%20critics%20that%20are%20capable%20of%20deliberately%20critiquing%20on%0Aeach%20reasoning%20step%20of%20math%20solutions.%20In%20the%20first%20stage%2C%20we%20utilize%0AQwen2.5-72B-Instruct%20to%20generate%204.5K%20long-form%20critiques%20as%20seed%20data%20for%0Asupervised%20fine-tuning.%20Each%20seed%20critique%20consists%20of%20deliberate%20step-wise%0Acritiques%20that%20includes%20multi-perspective%20verifications%20as%20well%20as%20in-depth%0Acritiques%20of%20initial%20critiques%20for%20each%20reasoning%20step.%20Then%2C%20we%20perform%0Areinforcement%20learning%20on%20the%20fine-tuned%20model%20with%20either%20existing%0Ahuman-labeled%20data%20from%20PRM800K%20or%20our%20automatically%20annotated%20data%20obtained%0Avia%20Monte%20Carlo%20sampling-based%20correctness%20estimation%2C%20to%20further%20incentivize%0Aits%20critique%20ability.%20Our%20developed%20critique%20model%20built%20on%20Qwen2.5-7B-Instruct%0Anot%20only%20significantly%20outperforms%20existing%20LLM%20critics%20%28including%20the%0Asame-sized%20DeepSeek-R1-distill%20models%20and%20GPT-4o%29%20on%20various%20error%0Aidentification%20benchmarks%2C%20but%20also%20more%20effectively%20helps%20the%20LLM%20generator%0Arefine%20erroneous%20steps%20through%20more%20detailed%20feedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepCritic%253A%2520Deliberate%2520Critique%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DWenkai%2520Yang%2520and%2520Jingwen%2520Chen%2520and%2520Yankai%2520Lin%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520rapidly%2520evolving%252C%2520providing%2520accurate%250Afeedback%2520and%2520scalable%2520oversight%2520on%2520their%2520outputs%2520becomes%2520an%2520urgent%2520and%2520critical%250Aproblem.%2520Leveraging%2520LLMs%2520as%2520critique%2520models%2520to%2520achieve%2520automated%2520supervision%2520is%250Aa%2520promising%2520solution.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520studying%2520and%2520enhancing%2520the%2520math%250Acritique%2520ability%2520of%2520LLMs.%2520Current%2520LLM%2520critics%2520provide%2520critiques%2520that%2520are%2520too%250Ashallow%2520and%2520superficial%2520on%2520each%2520step%252C%2520leading%2520to%2520low%2520judgment%2520accuracy%2520and%250Astruggling%2520to%2520offer%2520sufficient%2520feedback%2520for%2520the%2520LLM%2520generator%2520to%2520correct%250Amistakes.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520and%2520effective%2520two-stage%250Aframework%2520to%2520develop%2520LLM%2520critics%2520that%2520are%2520capable%2520of%2520deliberately%2520critiquing%2520on%250Aeach%2520reasoning%2520step%2520of%2520math%2520solutions.%2520In%2520the%2520first%2520stage%252C%2520we%2520utilize%250AQwen2.5-72B-Instruct%2520to%2520generate%25204.5K%2520long-form%2520critiques%2520as%2520seed%2520data%2520for%250Asupervised%2520fine-tuning.%2520Each%2520seed%2520critique%2520consists%2520of%2520deliberate%2520step-wise%250Acritiques%2520that%2520includes%2520multi-perspective%2520verifications%2520as%2520well%2520as%2520in-depth%250Acritiques%2520of%2520initial%2520critiques%2520for%2520each%2520reasoning%2520step.%2520Then%252C%2520we%2520perform%250Areinforcement%2520learning%2520on%2520the%2520fine-tuned%2520model%2520with%2520either%2520existing%250Ahuman-labeled%2520data%2520from%2520PRM800K%2520or%2520our%2520automatically%2520annotated%2520data%2520obtained%250Avia%2520Monte%2520Carlo%2520sampling-based%2520correctness%2520estimation%252C%2520to%2520further%2520incentivize%250Aits%2520critique%2520ability.%2520Our%2520developed%2520critique%2520model%2520built%2520on%2520Qwen2.5-7B-Instruct%250Anot%2520only%2520significantly%2520outperforms%2520existing%2520LLM%2520critics%2520%2528including%2520the%250Asame-sized%2520DeepSeek-R1-distill%2520models%2520and%2520GPT-4o%2529%2520on%2520various%2520error%250Aidentification%2520benchmarks%252C%2520but%2520also%2520more%2520effectively%2520helps%2520the%2520LLM%2520generator%250Arefine%2520erroneous%2520steps%2520through%2520more%2520detailed%2520feedback.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepCritic%3A%20Deliberate%20Critique%20with%20Large%20Language%20Models&entry.906535625=Wenkai%20Yang%20and%20Jingwen%20Chen%20and%20Yankai%20Lin%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20rapidly%20evolving%2C%20providing%20accurate%0Afeedback%20and%20scalable%20oversight%20on%20their%20outputs%20becomes%20an%20urgent%20and%20critical%0Aproblem.%20Leveraging%20LLMs%20as%20critique%20models%20to%20achieve%20automated%20supervision%20is%0Aa%20promising%20solution.%20In%20this%20work%2C%20we%20focus%20on%20studying%20and%20enhancing%20the%20math%0Acritique%20ability%20of%20LLMs.%20Current%20LLM%20critics%20provide%20critiques%20that%20are%20too%0Ashallow%20and%20superficial%20on%20each%20step%2C%20leading%20to%20low%20judgment%20accuracy%20and%0Astruggling%20to%20offer%20sufficient%20feedback%20for%20the%20LLM%20generator%20to%20correct%0Amistakes.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20and%20effective%20two-stage%0Aframework%20to%20develop%20LLM%20critics%20that%20are%20capable%20of%20deliberately%20critiquing%20on%0Aeach%20reasoning%20step%20of%20math%20solutions.%20In%20the%20first%20stage%2C%20we%20utilize%0AQwen2.5-72B-Instruct%20to%20generate%204.5K%20long-form%20critiques%20as%20seed%20data%20for%0Asupervised%20fine-tuning.%20Each%20seed%20critique%20consists%20of%20deliberate%20step-wise%0Acritiques%20that%20includes%20multi-perspective%20verifications%20as%20well%20as%20in-depth%0Acritiques%20of%20initial%20critiques%20for%20each%20reasoning%20step.%20Then%2C%20we%20perform%0Areinforcement%20learning%20on%20the%20fine-tuned%20model%20with%20either%20existing%0Ahuman-labeled%20data%20from%20PRM800K%20or%20our%20automatically%20annotated%20data%20obtained%0Avia%20Monte%20Carlo%20sampling-based%20correctness%20estimation%2C%20to%20further%20incentivize%0Aits%20critique%20ability.%20Our%20developed%20critique%20model%20built%20on%20Qwen2.5-7B-Instruct%0Anot%20only%20significantly%20outperforms%20existing%20LLM%20critics%20%28including%20the%0Asame-sized%20DeepSeek-R1-distill%20models%20and%20GPT-4o%29%20on%20various%20error%0Aidentification%20benchmarks%2C%20but%20also%20more%20effectively%20helps%20the%20LLM%20generator%0Arefine%20erroneous%20steps%20through%20more%20detailed%20feedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00662v1&entry.124074799=Read"},
{"title": "Automated Review Generation Method Based on Large Language Models", "author": "Shican Wu and Xiao Ma and Dehui Luo and Lulu Li and Xiangcheng Shi and Xin Chang and Xiaoyun Lin and Ran Luo and Chunlei Pei and Changying Du and Zhi-Jian Zhao and Jinlong Gong", "abstract": "  Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.\n", "link": "http://arxiv.org/abs/2407.20906v5", "date": "2025-05-01", "relevancy": 1.8553, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Review%20Generation%20Method%20Based%20on%20Large%20Language%20Models&body=Title%3A%20Automated%20Review%20Generation%20Method%20Based%20on%20Large%20Language%20Models%0AAuthor%3A%20Shican%20Wu%20and%20Xiao%20Ma%20and%20Dehui%20Luo%20and%20Lulu%20Li%20and%20Xiangcheng%20Shi%20and%20Xin%20Chang%20and%20Xiaoyun%20Lin%20and%20Ran%20Luo%20and%20Chunlei%20Pei%20and%20Changying%20Du%20and%20Zhi-Jian%20Zhao%20and%20Jinlong%20Gong%0AAbstract%3A%20%20%20Literature%20research%2C%20vital%20for%20scientific%20work%2C%20faces%20the%20challenge%20of%0Asurging%20information%20volumes%20exceeding%20researchers%27%20processing%20capabilities.%20We%0Apresent%20an%20automated%20review%20generation%20method%20based%20on%20large%20language%20models%0A%28LLMs%29%20to%20overcome%20efficiency%20bottlenecks%20and%20reduce%20cognitive%20load.%20Our%0Astatistically%20validated%20evaluation%20framework%20demonstrates%20that%20the%20generated%0Areviews%20match%20or%20exceed%20manual%20quality%2C%20offering%20broad%20applicability%20across%0Aresearch%20fields%20without%20requiring%20users%27%20domain%20knowledge.%20Applied%20to%20propane%0Adehydrogenation%20%28PDH%29%20catalysts%2C%20our%20method%20swiftly%20analyzed%20343%20articles%2C%0Aaveraging%20seconds%20per%20article%20per%20LLM%20account%2C%20producing%20comprehensive%20reviews%0Aspanning%2035%20topics%2C%20with%20extended%20analysis%20of%201041%20articles%20providing%20insights%0Ainto%20catalysts%27%20properties.%20Through%20multi-layered%20quality%20control%2C%20we%0Aeffectively%20mitigated%20LLMs%27%20hallucinations%2C%20with%20expert%20verification%20confirming%0Aaccuracy%20and%20citation%20integrity%20while%20demonstrating%20hallucination%20risks%20reduced%0Ato%20below%200.5%5C%25%20with%2095%5C%25%20confidence.%20Released%20Windows%20application%20enables%0Aone-click%20review%20generation%2C%20enhancing%20research%20productivity%20and%20literature%0Arecommendation%20efficiency%20while%20setting%20the%20stage%20for%20broader%20scientific%0Aexplorations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20906v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Review%2520Generation%2520Method%2520Based%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DShican%2520Wu%2520and%2520Xiao%2520Ma%2520and%2520Dehui%2520Luo%2520and%2520Lulu%2520Li%2520and%2520Xiangcheng%2520Shi%2520and%2520Xin%2520Chang%2520and%2520Xiaoyun%2520Lin%2520and%2520Ran%2520Luo%2520and%2520Chunlei%2520Pei%2520and%2520Changying%2520Du%2520and%2520Zhi-Jian%2520Zhao%2520and%2520Jinlong%2520Gong%26entry.1292438233%3D%2520%2520Literature%2520research%252C%2520vital%2520for%2520scientific%2520work%252C%2520faces%2520the%2520challenge%2520of%250Asurging%2520information%2520volumes%2520exceeding%2520researchers%2527%2520processing%2520capabilities.%2520We%250Apresent%2520an%2520automated%2520review%2520generation%2520method%2520based%2520on%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520overcome%2520efficiency%2520bottlenecks%2520and%2520reduce%2520cognitive%2520load.%2520Our%250Astatistically%2520validated%2520evaluation%2520framework%2520demonstrates%2520that%2520the%2520generated%250Areviews%2520match%2520or%2520exceed%2520manual%2520quality%252C%2520offering%2520broad%2520applicability%2520across%250Aresearch%2520fields%2520without%2520requiring%2520users%2527%2520domain%2520knowledge.%2520Applied%2520to%2520propane%250Adehydrogenation%2520%2528PDH%2529%2520catalysts%252C%2520our%2520method%2520swiftly%2520analyzed%2520343%2520articles%252C%250Aaveraging%2520seconds%2520per%2520article%2520per%2520LLM%2520account%252C%2520producing%2520comprehensive%2520reviews%250Aspanning%252035%2520topics%252C%2520with%2520extended%2520analysis%2520of%25201041%2520articles%2520providing%2520insights%250Ainto%2520catalysts%2527%2520properties.%2520Through%2520multi-layered%2520quality%2520control%252C%2520we%250Aeffectively%2520mitigated%2520LLMs%2527%2520hallucinations%252C%2520with%2520expert%2520verification%2520confirming%250Aaccuracy%2520and%2520citation%2520integrity%2520while%2520demonstrating%2520hallucination%2520risks%2520reduced%250Ato%2520below%25200.5%255C%2525%2520with%252095%255C%2525%2520confidence.%2520Released%2520Windows%2520application%2520enables%250Aone-click%2520review%2520generation%252C%2520enhancing%2520research%2520productivity%2520and%2520literature%250Arecommendation%2520efficiency%2520while%2520setting%2520the%2520stage%2520for%2520broader%2520scientific%250Aexplorations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20906v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Review%20Generation%20Method%20Based%20on%20Large%20Language%20Models&entry.906535625=Shican%20Wu%20and%20Xiao%20Ma%20and%20Dehui%20Luo%20and%20Lulu%20Li%20and%20Xiangcheng%20Shi%20and%20Xin%20Chang%20and%20Xiaoyun%20Lin%20and%20Ran%20Luo%20and%20Chunlei%20Pei%20and%20Changying%20Du%20and%20Zhi-Jian%20Zhao%20and%20Jinlong%20Gong&entry.1292438233=%20%20Literature%20research%2C%20vital%20for%20scientific%20work%2C%20faces%20the%20challenge%20of%0Asurging%20information%20volumes%20exceeding%20researchers%27%20processing%20capabilities.%20We%0Apresent%20an%20automated%20review%20generation%20method%20based%20on%20large%20language%20models%0A%28LLMs%29%20to%20overcome%20efficiency%20bottlenecks%20and%20reduce%20cognitive%20load.%20Our%0Astatistically%20validated%20evaluation%20framework%20demonstrates%20that%20the%20generated%0Areviews%20match%20or%20exceed%20manual%20quality%2C%20offering%20broad%20applicability%20across%0Aresearch%20fields%20without%20requiring%20users%27%20domain%20knowledge.%20Applied%20to%20propane%0Adehydrogenation%20%28PDH%29%20catalysts%2C%20our%20method%20swiftly%20analyzed%20343%20articles%2C%0Aaveraging%20seconds%20per%20article%20per%20LLM%20account%2C%20producing%20comprehensive%20reviews%0Aspanning%2035%20topics%2C%20with%20extended%20analysis%20of%201041%20articles%20providing%20insights%0Ainto%20catalysts%27%20properties.%20Through%20multi-layered%20quality%20control%2C%20we%0Aeffectively%20mitigated%20LLMs%27%20hallucinations%2C%20with%20expert%20verification%20confirming%0Aaccuracy%20and%20citation%20integrity%20while%20demonstrating%20hallucination%20risks%20reduced%0Ato%20below%200.5%5C%25%20with%2095%5C%25%20confidence.%20Released%20Windows%20application%20enables%0Aone-click%20review%20generation%2C%20enhancing%20research%20productivity%20and%20literature%0Arecommendation%20efficiency%20while%20setting%20the%20stage%20for%20broader%20scientific%0Aexplorations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20906v5&entry.124074799=Read"},
{"title": "Variational OOD State Correction for Offline Reinforcement Learning", "author": "Ke Jiang and Wen Jiang and Xiaoyang Tan", "abstract": "  The performance of Offline reinforcement learning is significantly impacted\nby the issue of state distributional shift, and out-of-distribution (OOD) state\ncorrection is a popular approach to address this problem. In this paper, we\npropose a novel method named Density-Aware Safety Perception (DASP) for OOD\nstate correction. Specifically, our method encourages the agent to prioritize\nactions that lead to outcomes with higher data density, thereby promoting its\noperation within or the return to in-distribution (safe) regions. To achieve\nthis, we optimize the objective within a variational framework that\nconcurrently considers both the potential outcomes of decision-making and their\ndensity, thus providing crucial contextual information for safe\ndecision-making. Finally, we validate the effectiveness and feasibility of our\nproposed method through extensive experimental evaluations on the offline\nMuJoCo and AntMaze suites.\n", "link": "http://arxiv.org/abs/2505.00503v1", "date": "2025-05-01", "relevancy": 1.85, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20OOD%20State%20Correction%20for%20Offline%20Reinforcement%20Learning&body=Title%3A%20Variational%20OOD%20State%20Correction%20for%20Offline%20Reinforcement%20Learning%0AAuthor%3A%20Ke%20Jiang%20and%20Wen%20Jiang%20and%20Xiaoyang%20Tan%0AAbstract%3A%20%20%20The%20performance%20of%20Offline%20reinforcement%20learning%20is%20significantly%20impacted%0Aby%20the%20issue%20of%20state%20distributional%20shift%2C%20and%20out-of-distribution%20%28OOD%29%20state%0Acorrection%20is%20a%20popular%20approach%20to%20address%20this%20problem.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20method%20named%20Density-Aware%20Safety%20Perception%20%28DASP%29%20for%20OOD%0Astate%20correction.%20Specifically%2C%20our%20method%20encourages%20the%20agent%20to%20prioritize%0Aactions%20that%20lead%20to%20outcomes%20with%20higher%20data%20density%2C%20thereby%20promoting%20its%0Aoperation%20within%20or%20the%20return%20to%20in-distribution%20%28safe%29%20regions.%20To%20achieve%0Athis%2C%20we%20optimize%20the%20objective%20within%20a%20variational%20framework%20that%0Aconcurrently%20considers%20both%20the%20potential%20outcomes%20of%20decision-making%20and%20their%0Adensity%2C%20thus%20providing%20crucial%20contextual%20information%20for%20safe%0Adecision-making.%20Finally%2C%20we%20validate%20the%20effectiveness%20and%20feasibility%20of%20our%0Aproposed%20method%20through%20extensive%20experimental%20evaluations%20on%20the%20offline%0AMuJoCo%20and%20AntMaze%20suites.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520OOD%2520State%2520Correction%2520for%2520Offline%2520Reinforcement%2520Learning%26entry.906535625%3DKe%2520Jiang%2520and%2520Wen%2520Jiang%2520and%2520Xiaoyang%2520Tan%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520Offline%2520reinforcement%2520learning%2520is%2520significantly%2520impacted%250Aby%2520the%2520issue%2520of%2520state%2520distributional%2520shift%252C%2520and%2520out-of-distribution%2520%2528OOD%2529%2520state%250Acorrection%2520is%2520a%2520popular%2520approach%2520to%2520address%2520this%2520problem.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520method%2520named%2520Density-Aware%2520Safety%2520Perception%2520%2528DASP%2529%2520for%2520OOD%250Astate%2520correction.%2520Specifically%252C%2520our%2520method%2520encourages%2520the%2520agent%2520to%2520prioritize%250Aactions%2520that%2520lead%2520to%2520outcomes%2520with%2520higher%2520data%2520density%252C%2520thereby%2520promoting%2520its%250Aoperation%2520within%2520or%2520the%2520return%2520to%2520in-distribution%2520%2528safe%2529%2520regions.%2520To%2520achieve%250Athis%252C%2520we%2520optimize%2520the%2520objective%2520within%2520a%2520variational%2520framework%2520that%250Aconcurrently%2520considers%2520both%2520the%2520potential%2520outcomes%2520of%2520decision-making%2520and%2520their%250Adensity%252C%2520thus%2520providing%2520crucial%2520contextual%2520information%2520for%2520safe%250Adecision-making.%2520Finally%252C%2520we%2520validate%2520the%2520effectiveness%2520and%2520feasibility%2520of%2520our%250Aproposed%2520method%2520through%2520extensive%2520experimental%2520evaluations%2520on%2520the%2520offline%250AMuJoCo%2520and%2520AntMaze%2520suites.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20OOD%20State%20Correction%20for%20Offline%20Reinforcement%20Learning&entry.906535625=Ke%20Jiang%20and%20Wen%20Jiang%20and%20Xiaoyang%20Tan&entry.1292438233=%20%20The%20performance%20of%20Offline%20reinforcement%20learning%20is%20significantly%20impacted%0Aby%20the%20issue%20of%20state%20distributional%20shift%2C%20and%20out-of-distribution%20%28OOD%29%20state%0Acorrection%20is%20a%20popular%20approach%20to%20address%20this%20problem.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20method%20named%20Density-Aware%20Safety%20Perception%20%28DASP%29%20for%20OOD%0Astate%20correction.%20Specifically%2C%20our%20method%20encourages%20the%20agent%20to%20prioritize%0Aactions%20that%20lead%20to%20outcomes%20with%20higher%20data%20density%2C%20thereby%20promoting%20its%0Aoperation%20within%20or%20the%20return%20to%20in-distribution%20%28safe%29%20regions.%20To%20achieve%0Athis%2C%20we%20optimize%20the%20objective%20within%20a%20variational%20framework%20that%0Aconcurrently%20considers%20both%20the%20potential%20outcomes%20of%20decision-making%20and%20their%0Adensity%2C%20thus%20providing%20crucial%20contextual%20information%20for%20safe%0Adecision-making.%20Finally%2C%20we%20validate%20the%20effectiveness%20and%20feasibility%20of%20our%0Aproposed%20method%20through%20extensive%20experimental%20evaluations%20on%20the%20offline%0AMuJoCo%20and%20AntMaze%20suites.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00503v1&entry.124074799=Read"},
{"title": "Robotic Visual Instruction", "author": "Yanbang Li and Ziyang Gong and Haoyang Li and Haoyang Li and Xiaoqi Huang and Haolan Kang and Guangping Bai and Xianzheng Ma", "abstract": "  Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision for robotic\ncontrol introduces challenges such as ambiguity and verbosity. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment, enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Code and\nDatasets in this paper will be released soon.\n", "link": "http://arxiv.org/abs/2505.00693v1", "date": "2025-05-01", "relevancy": 1.8494, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6625}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6056}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Visual%20Instruction&body=Title%3A%20Robotic%20Visual%20Instruction%0AAuthor%3A%20Yanbang%20Li%20and%20Ziyang%20Gong%20and%20Haoyang%20Li%20and%20Haoyang%20Li%20and%20Xiaoqi%20Huang%20and%20Haolan%20Kang%20and%20Guangping%20Bai%20and%20Xianzheng%20Ma%0AAbstract%3A%20%20%20Recently%2C%20natural%20language%20has%20been%20the%20primary%20medium%20for%20human-robot%0Ainteraction.%20However%2C%20its%20inherent%20lack%20of%20spatial%20precision%20for%20robotic%0Acontrol%20introduces%20challenges%20such%20as%20ambiguity%20and%20verbosity.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20the%20Robotic%20Visual%20Instruction%20%28RoVI%29%2C%20a%20novel%0Aparadigm%20to%20guide%20robotic%20tasks%20through%20an%20object-centric%2C%20hand-drawn%20symbolic%0Arepresentation.%20RoVI%20effectively%20encodes%20spatial-temporal%20information%20into%0Ahuman-interpretable%20visual%20instructions%20through%202D%20sketches%2C%20utilizing%20arrows%2C%0Acircles%2C%20colors%2C%20and%20numbers%20to%20direct%203D%20robotic%20manipulation.%20To%20enable%0Arobots%20to%20understand%20RoVI%20better%20and%20generate%20precise%20actions%20based%20on%20RoVI%2C%20we%0Apresent%20Visual%20Instruction%20Embodied%20Workflow%20%28VIEW%29%2C%20a%20pipeline%20formulated%20for%0ARoVI-conditioned%20policies.%20This%20approach%20leverages%20Vision-Language%20Models%0A%28VLMs%29%20to%20interpret%20RoVI%20inputs%2C%20decode%20spatial%20and%20temporal%20constraints%20from%0A2D%20pixel%20space%20via%20keypoint%20extraction%2C%20and%20then%20transform%20them%20into%20executable%0A3D%20action%20sequences.%20We%20additionally%20curate%20a%20specialized%20dataset%20of%2015K%0Ainstances%20to%20fine-tune%20small%20VLMs%20for%20edge%20deployment%2C%20enabling%20them%20to%0Aeffectively%20learn%20RoVI%20capabilities.%20Our%20approach%20is%20rigorously%20validated%0Aacross%2011%20novel%20tasks%20in%20both%20real%20and%20simulated%20environments%2C%20demonstrating%0Asignificant%20generalization%20capability.%20Notably%2C%20VIEW%20achieves%20an%2087.5%25%20success%0Arate%20in%20real-world%20scenarios%20involving%20unseen%20tasks%20that%20feature%20multi-step%0Aactions%2C%20with%20disturbances%2C%20and%20trajectory-following%20requirements.%20Code%20and%0ADatasets%20in%20this%20paper%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Visual%2520Instruction%26entry.906535625%3DYanbang%2520Li%2520and%2520Ziyang%2520Gong%2520and%2520Haoyang%2520Li%2520and%2520Haoyang%2520Li%2520and%2520Xiaoqi%2520Huang%2520and%2520Haolan%2520Kang%2520and%2520Guangping%2520Bai%2520and%2520Xianzheng%2520Ma%26entry.1292438233%3D%2520%2520Recently%252C%2520natural%2520language%2520has%2520been%2520the%2520primary%2520medium%2520for%2520human-robot%250Ainteraction.%2520However%252C%2520its%2520inherent%2520lack%2520of%2520spatial%2520precision%2520for%2520robotic%250Acontrol%2520introduces%2520challenges%2520such%2520as%2520ambiguity%2520and%2520verbosity.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520the%2520Robotic%2520Visual%2520Instruction%2520%2528RoVI%2529%252C%2520a%2520novel%250Aparadigm%2520to%2520guide%2520robotic%2520tasks%2520through%2520an%2520object-centric%252C%2520hand-drawn%2520symbolic%250Arepresentation.%2520RoVI%2520effectively%2520encodes%2520spatial-temporal%2520information%2520into%250Ahuman-interpretable%2520visual%2520instructions%2520through%25202D%2520sketches%252C%2520utilizing%2520arrows%252C%250Acircles%252C%2520colors%252C%2520and%2520numbers%2520to%2520direct%25203D%2520robotic%2520manipulation.%2520To%2520enable%250Arobots%2520to%2520understand%2520RoVI%2520better%2520and%2520generate%2520precise%2520actions%2520based%2520on%2520RoVI%252C%2520we%250Apresent%2520Visual%2520Instruction%2520Embodied%2520Workflow%2520%2528VIEW%2529%252C%2520a%2520pipeline%2520formulated%2520for%250ARoVI-conditioned%2520policies.%2520This%2520approach%2520leverages%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520to%2520interpret%2520RoVI%2520inputs%252C%2520decode%2520spatial%2520and%2520temporal%2520constraints%2520from%250A2D%2520pixel%2520space%2520via%2520keypoint%2520extraction%252C%2520and%2520then%2520transform%2520them%2520into%2520executable%250A3D%2520action%2520sequences.%2520We%2520additionally%2520curate%2520a%2520specialized%2520dataset%2520of%252015K%250Ainstances%2520to%2520fine-tune%2520small%2520VLMs%2520for%2520edge%2520deployment%252C%2520enabling%2520them%2520to%250Aeffectively%2520learn%2520RoVI%2520capabilities.%2520Our%2520approach%2520is%2520rigorously%2520validated%250Aacross%252011%2520novel%2520tasks%2520in%2520both%2520real%2520and%2520simulated%2520environments%252C%2520demonstrating%250Asignificant%2520generalization%2520capability.%2520Notably%252C%2520VIEW%2520achieves%2520an%252087.5%2525%2520success%250Arate%2520in%2520real-world%2520scenarios%2520involving%2520unseen%2520tasks%2520that%2520feature%2520multi-step%250Aactions%252C%2520with%2520disturbances%252C%2520and%2520trajectory-following%2520requirements.%2520Code%2520and%250ADatasets%2520in%2520this%2520paper%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Visual%20Instruction&entry.906535625=Yanbang%20Li%20and%20Ziyang%20Gong%20and%20Haoyang%20Li%20and%20Haoyang%20Li%20and%20Xiaoqi%20Huang%20and%20Haolan%20Kang%20and%20Guangping%20Bai%20and%20Xianzheng%20Ma&entry.1292438233=%20%20Recently%2C%20natural%20language%20has%20been%20the%20primary%20medium%20for%20human-robot%0Ainteraction.%20However%2C%20its%20inherent%20lack%20of%20spatial%20precision%20for%20robotic%0Acontrol%20introduces%20challenges%20such%20as%20ambiguity%20and%20verbosity.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20the%20Robotic%20Visual%20Instruction%20%28RoVI%29%2C%20a%20novel%0Aparadigm%20to%20guide%20robotic%20tasks%20through%20an%20object-centric%2C%20hand-drawn%20symbolic%0Arepresentation.%20RoVI%20effectively%20encodes%20spatial-temporal%20information%20into%0Ahuman-interpretable%20visual%20instructions%20through%202D%20sketches%2C%20utilizing%20arrows%2C%0Acircles%2C%20colors%2C%20and%20numbers%20to%20direct%203D%20robotic%20manipulation.%20To%20enable%0Arobots%20to%20understand%20RoVI%20better%20and%20generate%20precise%20actions%20based%20on%20RoVI%2C%20we%0Apresent%20Visual%20Instruction%20Embodied%20Workflow%20%28VIEW%29%2C%20a%20pipeline%20formulated%20for%0ARoVI-conditioned%20policies.%20This%20approach%20leverages%20Vision-Language%20Models%0A%28VLMs%29%20to%20interpret%20RoVI%20inputs%2C%20decode%20spatial%20and%20temporal%20constraints%20from%0A2D%20pixel%20space%20via%20keypoint%20extraction%2C%20and%20then%20transform%20them%20into%20executable%0A3D%20action%20sequences.%20We%20additionally%20curate%20a%20specialized%20dataset%20of%2015K%0Ainstances%20to%20fine-tune%20small%20VLMs%20for%20edge%20deployment%2C%20enabling%20them%20to%0Aeffectively%20learn%20RoVI%20capabilities.%20Our%20approach%20is%20rigorously%20validated%0Aacross%2011%20novel%20tasks%20in%20both%20real%20and%20simulated%20environments%2C%20demonstrating%0Asignificant%20generalization%20capability.%20Notably%2C%20VIEW%20achieves%20an%2087.5%25%20success%0Arate%20in%20real-world%20scenarios%20involving%20unseen%20tasks%20that%20feature%20multi-step%0Aactions%2C%20with%20disturbances%2C%20and%20trajectory-following%20requirements.%20Code%20and%0ADatasets%20in%20this%20paper%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00693v1&entry.124074799=Read"},
{"title": "Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield\n  Networks", "author": "Akira Tamamori", "abstract": "  Hopfield networks using Hebbian learning suffer from limited storage\ncapacity. While supervised methods like Linear Logistic Regression (LLR) offer\nsome improvement, kernel methods like Kernel Logistic Regression (KLR)\nsignificantly enhance capacity and noise robustness. However, KLR requires\ncomputationally expensive iterative learning. We propose Kernel Ridge\nRegression (KRR) as an efficient kernel-based alternative for learning\nhigh-capacity Hopfield networks. KRR utilizes the kernel trick and predicts\nbipolar states via regression, crucially offering a non-iterative, closed-form\nsolution for learning dual variables. We evaluate KRR and compare its\nperformance against Hebbian, LLR, and KLR. Our results demonstrate that KRR\nachieves state-of-the-art storage capacity (reaching $\\beta$=1.5) and noise\nrobustness, comparable to KLR. Crucially, KRR drastically reduces training\ntime, being orders of magnitude faster than LLR and significantly faster than\nKLR, especially at higher storage loads. This establishes KRR as a potent and\nhighly efficient method for building high-performance associative memories,\nproviding comparable performance to KLR with substantial training speed\nadvantages. This work provides the first empirical comparison between KRR and\nKLR in the context of Hopfield network learning.\n", "link": "http://arxiv.org/abs/2504.12561v2", "date": "2025-05-01", "relevancy": 1.8451, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4637}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Ridge%20Regression%20for%20Efficient%20Learning%20of%20High-Capacity%20Hopfield%0A%20%20Networks&body=Title%3A%20Kernel%20Ridge%20Regression%20for%20Efficient%20Learning%20of%20High-Capacity%20Hopfield%0A%20%20Networks%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20%20%20Hopfield%20networks%20using%20Hebbian%20learning%20suffer%20from%20limited%20storage%0Acapacity.%20While%20supervised%20methods%20like%20Linear%20Logistic%20Regression%20%28LLR%29%20offer%0Asome%20improvement%2C%20kernel%20methods%20like%20Kernel%20Logistic%20Regression%20%28KLR%29%0Asignificantly%20enhance%20capacity%20and%20noise%20robustness.%20However%2C%20KLR%20requires%0Acomputationally%20expensive%20iterative%20learning.%20We%20propose%20Kernel%20Ridge%0ARegression%20%28KRR%29%20as%20an%20efficient%20kernel-based%20alternative%20for%20learning%0Ahigh-capacity%20Hopfield%20networks.%20KRR%20utilizes%20the%20kernel%20trick%20and%20predicts%0Abipolar%20states%20via%20regression%2C%20crucially%20offering%20a%20non-iterative%2C%20closed-form%0Asolution%20for%20learning%20dual%20variables.%20We%20evaluate%20KRR%20and%20compare%20its%0Aperformance%20against%20Hebbian%2C%20LLR%2C%20and%20KLR.%20Our%20results%20demonstrate%20that%20KRR%0Aachieves%20state-of-the-art%20storage%20capacity%20%28reaching%20%24%5Cbeta%24%3D1.5%29%20and%20noise%0Arobustness%2C%20comparable%20to%20KLR.%20Crucially%2C%20KRR%20drastically%20reduces%20training%0Atime%2C%20being%20orders%20of%20magnitude%20faster%20than%20LLR%20and%20significantly%20faster%20than%0AKLR%2C%20especially%20at%20higher%20storage%20loads.%20This%20establishes%20KRR%20as%20a%20potent%20and%0Ahighly%20efficient%20method%20for%20building%20high-performance%20associative%20memories%2C%0Aproviding%20comparable%20performance%20to%20KLR%20with%20substantial%20training%20speed%0Aadvantages.%20This%20work%20provides%20the%20first%20empirical%20comparison%20between%20KRR%20and%0AKLR%20in%20the%20context%20of%20Hopfield%20network%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Ridge%2520Regression%2520for%2520Efficient%2520Learning%2520of%2520High-Capacity%2520Hopfield%250A%2520%2520Networks%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3D%2520%2520Hopfield%2520networks%2520using%2520Hebbian%2520learning%2520suffer%2520from%2520limited%2520storage%250Acapacity.%2520While%2520supervised%2520methods%2520like%2520Linear%2520Logistic%2520Regression%2520%2528LLR%2529%2520offer%250Asome%2520improvement%252C%2520kernel%2520methods%2520like%2520Kernel%2520Logistic%2520Regression%2520%2528KLR%2529%250Asignificantly%2520enhance%2520capacity%2520and%2520noise%2520robustness.%2520However%252C%2520KLR%2520requires%250Acomputationally%2520expensive%2520iterative%2520learning.%2520We%2520propose%2520Kernel%2520Ridge%250ARegression%2520%2528KRR%2529%2520as%2520an%2520efficient%2520kernel-based%2520alternative%2520for%2520learning%250Ahigh-capacity%2520Hopfield%2520networks.%2520KRR%2520utilizes%2520the%2520kernel%2520trick%2520and%2520predicts%250Abipolar%2520states%2520via%2520regression%252C%2520crucially%2520offering%2520a%2520non-iterative%252C%2520closed-form%250Asolution%2520for%2520learning%2520dual%2520variables.%2520We%2520evaluate%2520KRR%2520and%2520compare%2520its%250Aperformance%2520against%2520Hebbian%252C%2520LLR%252C%2520and%2520KLR.%2520Our%2520results%2520demonstrate%2520that%2520KRR%250Aachieves%2520state-of-the-art%2520storage%2520capacity%2520%2528reaching%2520%2524%255Cbeta%2524%253D1.5%2529%2520and%2520noise%250Arobustness%252C%2520comparable%2520to%2520KLR.%2520Crucially%252C%2520KRR%2520drastically%2520reduces%2520training%250Atime%252C%2520being%2520orders%2520of%2520magnitude%2520faster%2520than%2520LLR%2520and%2520significantly%2520faster%2520than%250AKLR%252C%2520especially%2520at%2520higher%2520storage%2520loads.%2520This%2520establishes%2520KRR%2520as%2520a%2520potent%2520and%250Ahighly%2520efficient%2520method%2520for%2520building%2520high-performance%2520associative%2520memories%252C%250Aproviding%2520comparable%2520performance%2520to%2520KLR%2520with%2520substantial%2520training%2520speed%250Aadvantages.%2520This%2520work%2520provides%2520the%2520first%2520empirical%2520comparison%2520between%2520KRR%2520and%250AKLR%2520in%2520the%2520context%2520of%2520Hopfield%2520network%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Ridge%20Regression%20for%20Efficient%20Learning%20of%20High-Capacity%20Hopfield%0A%20%20Networks&entry.906535625=Akira%20Tamamori&entry.1292438233=%20%20Hopfield%20networks%20using%20Hebbian%20learning%20suffer%20from%20limited%20storage%0Acapacity.%20While%20supervised%20methods%20like%20Linear%20Logistic%20Regression%20%28LLR%29%20offer%0Asome%20improvement%2C%20kernel%20methods%20like%20Kernel%20Logistic%20Regression%20%28KLR%29%0Asignificantly%20enhance%20capacity%20and%20noise%20robustness.%20However%2C%20KLR%20requires%0Acomputationally%20expensive%20iterative%20learning.%20We%20propose%20Kernel%20Ridge%0ARegression%20%28KRR%29%20as%20an%20efficient%20kernel-based%20alternative%20for%20learning%0Ahigh-capacity%20Hopfield%20networks.%20KRR%20utilizes%20the%20kernel%20trick%20and%20predicts%0Abipolar%20states%20via%20regression%2C%20crucially%20offering%20a%20non-iterative%2C%20closed-form%0Asolution%20for%20learning%20dual%20variables.%20We%20evaluate%20KRR%20and%20compare%20its%0Aperformance%20against%20Hebbian%2C%20LLR%2C%20and%20KLR.%20Our%20results%20demonstrate%20that%20KRR%0Aachieves%20state-of-the-art%20storage%20capacity%20%28reaching%20%24%5Cbeta%24%3D1.5%29%20and%20noise%0Arobustness%2C%20comparable%20to%20KLR.%20Crucially%2C%20KRR%20drastically%20reduces%20training%0Atime%2C%20being%20orders%20of%20magnitude%20faster%20than%20LLR%20and%20significantly%20faster%20than%0AKLR%2C%20especially%20at%20higher%20storage%20loads.%20This%20establishes%20KRR%20as%20a%20potent%20and%0Ahighly%20efficient%20method%20for%20building%20high-performance%20associative%20memories%2C%0Aproviding%20comparable%20performance%20to%20KLR%20with%20substantial%20training%20speed%0Aadvantages.%20This%20work%20provides%20the%20first%20empirical%20comparison%20between%20KRR%20and%0AKLR%20in%20the%20context%20of%20Hopfield%20network%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12561v2&entry.124074799=Read"},
{"title": "KnowEEG: Explainable Knowledge Driven EEG Classification", "author": "Amarpal Sahota and Navid Mohammadi Foumani and Raul Santos-Rodriguez and Zahraa S. Abdallah", "abstract": "  Electroencephalography (EEG) is a method of recording brain activity that\nshows significant promise in applications ranging from disease classification\nto emotion detection and brain-computer interfaces. Recent advances in deep\nlearning have improved EEG classification performance yet model explainability\nremains an issue. To address this key limitation of explainability we introduce\nKnowEEG; a novel explainable machine learning approach for EEG classification.\nKnowEEG extracts a comprehensive set of per-electrode features, filters them\nusing statistical tests, and integrates between-electrode connectivity\nstatistics. These features are then input to our modified Random Forest model\n(Fusion Forest) that balances per electrode statistics with between electrode\nconnectivity features in growing the trees of the forest. By incorporating\nknowledge from both the generalized time-series and EEG-specific domains,\nKnowEEG achieves performance comparable to or exceeding state-of-the-art deep\nlearning models across five different classification tasks: emotion detection,\nmental workload classification, eyes open/closed detection, abnormal EEG\nclassification, and event detection. In addition to high performance, KnowEEG\nprovides inherent explainability through feature importance scores for\nunderstandable features. We demonstrate by example on the eyes closed/open\nclassification task that this explainability can be used to discover knowledge\nabout the classes. This discovered knowledge for eyes open/closed\nclassification was proven to be correct by current neuroscience literature.\nTherefore, the impact of KnowEEG will be significant for domains where EEG\nexplainability is critical such as healthcare.\n", "link": "http://arxiv.org/abs/2505.00541v1", "date": "2025-05-01", "relevancy": 1.8394, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4586}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KnowEEG%3A%20Explainable%20Knowledge%20Driven%20EEG%20Classification&body=Title%3A%20KnowEEG%3A%20Explainable%20Knowledge%20Driven%20EEG%20Classification%0AAuthor%3A%20Amarpal%20Sahota%20and%20Navid%20Mohammadi%20Foumani%20and%20Raul%20Santos-Rodriguez%20and%20Zahraa%20S.%20Abdallah%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20is%20a%20method%20of%20recording%20brain%20activity%20that%0Ashows%20significant%20promise%20in%20applications%20ranging%20from%20disease%20classification%0Ato%20emotion%20detection%20and%20brain-computer%20interfaces.%20Recent%20advances%20in%20deep%0Alearning%20have%20improved%20EEG%20classification%20performance%20yet%20model%20explainability%0Aremains%20an%20issue.%20To%20address%20this%20key%20limitation%20of%20explainability%20we%20introduce%0AKnowEEG%3B%20a%20novel%20explainable%20machine%20learning%20approach%20for%20EEG%20classification.%0AKnowEEG%20extracts%20a%20comprehensive%20set%20of%20per-electrode%20features%2C%20filters%20them%0Ausing%20statistical%20tests%2C%20and%20integrates%20between-electrode%20connectivity%0Astatistics.%20These%20features%20are%20then%20input%20to%20our%20modified%20Random%20Forest%20model%0A%28Fusion%20Forest%29%20that%20balances%20per%20electrode%20statistics%20with%20between%20electrode%0Aconnectivity%20features%20in%20growing%20the%20trees%20of%20the%20forest.%20By%20incorporating%0Aknowledge%20from%20both%20the%20generalized%20time-series%20and%20EEG-specific%20domains%2C%0AKnowEEG%20achieves%20performance%20comparable%20to%20or%20exceeding%20state-of-the-art%20deep%0Alearning%20models%20across%20five%20different%20classification%20tasks%3A%20emotion%20detection%2C%0Amental%20workload%20classification%2C%20eyes%20open/closed%20detection%2C%20abnormal%20EEG%0Aclassification%2C%20and%20event%20detection.%20In%20addition%20to%20high%20performance%2C%20KnowEEG%0Aprovides%20inherent%20explainability%20through%20feature%20importance%20scores%20for%0Aunderstandable%20features.%20We%20demonstrate%20by%20example%20on%20the%20eyes%20closed/open%0Aclassification%20task%20that%20this%20explainability%20can%20be%20used%20to%20discover%20knowledge%0Aabout%20the%20classes.%20This%20discovered%20knowledge%20for%20eyes%20open/closed%0Aclassification%20was%20proven%20to%20be%20correct%20by%20current%20neuroscience%20literature.%0ATherefore%2C%20the%20impact%20of%20KnowEEG%20will%20be%20significant%20for%20domains%20where%20EEG%0Aexplainability%20is%20critical%20such%20as%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowEEG%253A%2520Explainable%2520Knowledge%2520Driven%2520EEG%2520Classification%26entry.906535625%3DAmarpal%2520Sahota%2520and%2520Navid%2520Mohammadi%2520Foumani%2520and%2520Raul%2520Santos-Rodriguez%2520and%2520Zahraa%2520S.%2520Abdallah%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%2520is%2520a%2520method%2520of%2520recording%2520brain%2520activity%2520that%250Ashows%2520significant%2520promise%2520in%2520applications%2520ranging%2520from%2520disease%2520classification%250Ato%2520emotion%2520detection%2520and%2520brain-computer%2520interfaces.%2520Recent%2520advances%2520in%2520deep%250Alearning%2520have%2520improved%2520EEG%2520classification%2520performance%2520yet%2520model%2520explainability%250Aremains%2520an%2520issue.%2520To%2520address%2520this%2520key%2520limitation%2520of%2520explainability%2520we%2520introduce%250AKnowEEG%253B%2520a%2520novel%2520explainable%2520machine%2520learning%2520approach%2520for%2520EEG%2520classification.%250AKnowEEG%2520extracts%2520a%2520comprehensive%2520set%2520of%2520per-electrode%2520features%252C%2520filters%2520them%250Ausing%2520statistical%2520tests%252C%2520and%2520integrates%2520between-electrode%2520connectivity%250Astatistics.%2520These%2520features%2520are%2520then%2520input%2520to%2520our%2520modified%2520Random%2520Forest%2520model%250A%2528Fusion%2520Forest%2529%2520that%2520balances%2520per%2520electrode%2520statistics%2520with%2520between%2520electrode%250Aconnectivity%2520features%2520in%2520growing%2520the%2520trees%2520of%2520the%2520forest.%2520By%2520incorporating%250Aknowledge%2520from%2520both%2520the%2520generalized%2520time-series%2520and%2520EEG-specific%2520domains%252C%250AKnowEEG%2520achieves%2520performance%2520comparable%2520to%2520or%2520exceeding%2520state-of-the-art%2520deep%250Alearning%2520models%2520across%2520five%2520different%2520classification%2520tasks%253A%2520emotion%2520detection%252C%250Amental%2520workload%2520classification%252C%2520eyes%2520open/closed%2520detection%252C%2520abnormal%2520EEG%250Aclassification%252C%2520and%2520event%2520detection.%2520In%2520addition%2520to%2520high%2520performance%252C%2520KnowEEG%250Aprovides%2520inherent%2520explainability%2520through%2520feature%2520importance%2520scores%2520for%250Aunderstandable%2520features.%2520We%2520demonstrate%2520by%2520example%2520on%2520the%2520eyes%2520closed/open%250Aclassification%2520task%2520that%2520this%2520explainability%2520can%2520be%2520used%2520to%2520discover%2520knowledge%250Aabout%2520the%2520classes.%2520This%2520discovered%2520knowledge%2520for%2520eyes%2520open/closed%250Aclassification%2520was%2520proven%2520to%2520be%2520correct%2520by%2520current%2520neuroscience%2520literature.%250ATherefore%252C%2520the%2520impact%2520of%2520KnowEEG%2520will%2520be%2520significant%2520for%2520domains%2520where%2520EEG%250Aexplainability%2520is%2520critical%2520such%2520as%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowEEG%3A%20Explainable%20Knowledge%20Driven%20EEG%20Classification&entry.906535625=Amarpal%20Sahota%20and%20Navid%20Mohammadi%20Foumani%20and%20Raul%20Santos-Rodriguez%20and%20Zahraa%20S.%20Abdallah&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20is%20a%20method%20of%20recording%20brain%20activity%20that%0Ashows%20significant%20promise%20in%20applications%20ranging%20from%20disease%20classification%0Ato%20emotion%20detection%20and%20brain-computer%20interfaces.%20Recent%20advances%20in%20deep%0Alearning%20have%20improved%20EEG%20classification%20performance%20yet%20model%20explainability%0Aremains%20an%20issue.%20To%20address%20this%20key%20limitation%20of%20explainability%20we%20introduce%0AKnowEEG%3B%20a%20novel%20explainable%20machine%20learning%20approach%20for%20EEG%20classification.%0AKnowEEG%20extracts%20a%20comprehensive%20set%20of%20per-electrode%20features%2C%20filters%20them%0Ausing%20statistical%20tests%2C%20and%20integrates%20between-electrode%20connectivity%0Astatistics.%20These%20features%20are%20then%20input%20to%20our%20modified%20Random%20Forest%20model%0A%28Fusion%20Forest%29%20that%20balances%20per%20electrode%20statistics%20with%20between%20electrode%0Aconnectivity%20features%20in%20growing%20the%20trees%20of%20the%20forest.%20By%20incorporating%0Aknowledge%20from%20both%20the%20generalized%20time-series%20and%20EEG-specific%20domains%2C%0AKnowEEG%20achieves%20performance%20comparable%20to%20or%20exceeding%20state-of-the-art%20deep%0Alearning%20models%20across%20five%20different%20classification%20tasks%3A%20emotion%20detection%2C%0Amental%20workload%20classification%2C%20eyes%20open/closed%20detection%2C%20abnormal%20EEG%0Aclassification%2C%20and%20event%20detection.%20In%20addition%20to%20high%20performance%2C%20KnowEEG%0Aprovides%20inherent%20explainability%20through%20feature%20importance%20scores%20for%0Aunderstandable%20features.%20We%20demonstrate%20by%20example%20on%20the%20eyes%20closed/open%0Aclassification%20task%20that%20this%20explainability%20can%20be%20used%20to%20discover%20knowledge%0Aabout%20the%20classes.%20This%20discovered%20knowledge%20for%20eyes%20open/closed%0Aclassification%20was%20proven%20to%20be%20correct%20by%20current%20neuroscience%20literature.%0ATherefore%2C%20the%20impact%20of%20KnowEEG%20will%20be%20significant%20for%20domains%20where%20EEG%0Aexplainability%20is%20critical%20such%20as%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00541v1&entry.124074799=Read"},
{"title": "Learning to Learn with Quantum Optimization via Quantum Neural Networks", "author": "Kuan-Cheng Chen and Hiromichi Matsuyama and Wei-Hao Huang", "abstract": "  Quantum Approximate Optimization Algorithms (QAOA) promise efficient\nsolutions to classically intractable combinatorial optimization problems by\nharnessing shallow-depth quantum circuits. Yet, their performance and\nscalability often hinge on effective parameter optimization, which remains\nnontrivial due to rugged energy landscapes and hardware noise. In this work, we\nintroduce a quantum meta-learning framework that combines quantum neural\nnetworks, specifically Quantum Long Short-Term Memory (QLSTM) architectures,\nwith QAOA. By training the QLSTM optimizer on smaller graph instances, our\napproach rapidly generalizes to larger, more complex problems, substantially\nreducing the number of iterations required for convergence. Through\ncomprehensive benchmarks on Max-Cut and Sherrington-Kirkpatrick model\ninstances, we demonstrate that QLSTM-based optimizers converge faster and\nachieve higher approximation ratios compared to classical baselines, thereby\noffering a robust pathway toward scalable quantum optimization in the NISQ era.\n", "link": "http://arxiv.org/abs/2505.00561v1", "date": "2025-05-01", "relevancy": 1.8347, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4718}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Learn%20with%20Quantum%20Optimization%20via%20Quantum%20Neural%20Networks&body=Title%3A%20Learning%20to%20Learn%20with%20Quantum%20Optimization%20via%20Quantum%20Neural%20Networks%0AAuthor%3A%20Kuan-Cheng%20Chen%20and%20Hiromichi%20Matsuyama%20and%20Wei-Hao%20Huang%0AAbstract%3A%20%20%20Quantum%20Approximate%20Optimization%20Algorithms%20%28QAOA%29%20promise%20efficient%0Asolutions%20to%20classically%20intractable%20combinatorial%20optimization%20problems%20by%0Aharnessing%20shallow-depth%20quantum%20circuits.%20Yet%2C%20their%20performance%20and%0Ascalability%20often%20hinge%20on%20effective%20parameter%20optimization%2C%20which%20remains%0Anontrivial%20due%20to%20rugged%20energy%20landscapes%20and%20hardware%20noise.%20In%20this%20work%2C%20we%0Aintroduce%20a%20quantum%20meta-learning%20framework%20that%20combines%20quantum%20neural%0Anetworks%2C%20specifically%20Quantum%20Long%20Short-Term%20Memory%20%28QLSTM%29%20architectures%2C%0Awith%20QAOA.%20By%20training%20the%20QLSTM%20optimizer%20on%20smaller%20graph%20instances%2C%20our%0Aapproach%20rapidly%20generalizes%20to%20larger%2C%20more%20complex%20problems%2C%20substantially%0Areducing%20the%20number%20of%20iterations%20required%20for%20convergence.%20Through%0Acomprehensive%20benchmarks%20on%20Max-Cut%20and%20Sherrington-Kirkpatrick%20model%0Ainstances%2C%20we%20demonstrate%20that%20QLSTM-based%20optimizers%20converge%20faster%20and%0Aachieve%20higher%20approximation%20ratios%20compared%20to%20classical%20baselines%2C%20thereby%0Aoffering%20a%20robust%20pathway%20toward%20scalable%20quantum%20optimization%20in%20the%20NISQ%20era.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Learn%2520with%2520Quantum%2520Optimization%2520via%2520Quantum%2520Neural%2520Networks%26entry.906535625%3DKuan-Cheng%2520Chen%2520and%2520Hiromichi%2520Matsuyama%2520and%2520Wei-Hao%2520Huang%26entry.1292438233%3D%2520%2520Quantum%2520Approximate%2520Optimization%2520Algorithms%2520%2528QAOA%2529%2520promise%2520efficient%250Asolutions%2520to%2520classically%2520intractable%2520combinatorial%2520optimization%2520problems%2520by%250Aharnessing%2520shallow-depth%2520quantum%2520circuits.%2520Yet%252C%2520their%2520performance%2520and%250Ascalability%2520often%2520hinge%2520on%2520effective%2520parameter%2520optimization%252C%2520which%2520remains%250Anontrivial%2520due%2520to%2520rugged%2520energy%2520landscapes%2520and%2520hardware%2520noise.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520quantum%2520meta-learning%2520framework%2520that%2520combines%2520quantum%2520neural%250Anetworks%252C%2520specifically%2520Quantum%2520Long%2520Short-Term%2520Memory%2520%2528QLSTM%2529%2520architectures%252C%250Awith%2520QAOA.%2520By%2520training%2520the%2520QLSTM%2520optimizer%2520on%2520smaller%2520graph%2520instances%252C%2520our%250Aapproach%2520rapidly%2520generalizes%2520to%2520larger%252C%2520more%2520complex%2520problems%252C%2520substantially%250Areducing%2520the%2520number%2520of%2520iterations%2520required%2520for%2520convergence.%2520Through%250Acomprehensive%2520benchmarks%2520on%2520Max-Cut%2520and%2520Sherrington-Kirkpatrick%2520model%250Ainstances%252C%2520we%2520demonstrate%2520that%2520QLSTM-based%2520optimizers%2520converge%2520faster%2520and%250Aachieve%2520higher%2520approximation%2520ratios%2520compared%2520to%2520classical%2520baselines%252C%2520thereby%250Aoffering%2520a%2520robust%2520pathway%2520toward%2520scalable%2520quantum%2520optimization%2520in%2520the%2520NISQ%2520era.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Learn%20with%20Quantum%20Optimization%20via%20Quantum%20Neural%20Networks&entry.906535625=Kuan-Cheng%20Chen%20and%20Hiromichi%20Matsuyama%20and%20Wei-Hao%20Huang&entry.1292438233=%20%20Quantum%20Approximate%20Optimization%20Algorithms%20%28QAOA%29%20promise%20efficient%0Asolutions%20to%20classically%20intractable%20combinatorial%20optimization%20problems%20by%0Aharnessing%20shallow-depth%20quantum%20circuits.%20Yet%2C%20their%20performance%20and%0Ascalability%20often%20hinge%20on%20effective%20parameter%20optimization%2C%20which%20remains%0Anontrivial%20due%20to%20rugged%20energy%20landscapes%20and%20hardware%20noise.%20In%20this%20work%2C%20we%0Aintroduce%20a%20quantum%20meta-learning%20framework%20that%20combines%20quantum%20neural%0Anetworks%2C%20specifically%20Quantum%20Long%20Short-Term%20Memory%20%28QLSTM%29%20architectures%2C%0Awith%20QAOA.%20By%20training%20the%20QLSTM%20optimizer%20on%20smaller%20graph%20instances%2C%20our%0Aapproach%20rapidly%20generalizes%20to%20larger%2C%20more%20complex%20problems%2C%20substantially%0Areducing%20the%20number%20of%20iterations%20required%20for%20convergence.%20Through%0Acomprehensive%20benchmarks%20on%20Max-Cut%20and%20Sherrington-Kirkpatrick%20model%0Ainstances%2C%20we%20demonstrate%20that%20QLSTM-based%20optimizers%20converge%20faster%20and%0Aachieve%20higher%20approximation%20ratios%20compared%20to%20classical%20baselines%2C%20thereby%0Aoffering%20a%20robust%20pathway%20toward%20scalable%20quantum%20optimization%20in%20the%20NISQ%20era.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00561v1&entry.124074799=Read"},
{"title": "TRIED: Truly Innovative and Effective AI Detection Benchmark, developed\n  by WITNESS", "author": "Shirin Anlen and Zuzanna Wojciak", "abstract": "  The proliferation of generative AI and deceptive synthetic media threatens\nthe global information ecosystem, especially across the Global Majority. This\nreport from WITNESS highlights the limitations of current AI detection tools,\nwhich often underperform in real-world scenarios due to challenges related to\nexplainability, fairness, accessibility, and contextual relevance. In response,\nWITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)\nBenchmark, a new framework for evaluating detection tools based on their\nreal-world impact and capacity for innovation. Drawing on frontline\nexperiences, deceptive AI cases, and global consultations, the report outlines\nhow detection tools must evolve to become truly innovative and relevant by\nmeeting diverse linguistic, cultural, and technological contexts. It offers\npractical guidance for developers, policy actors, and standards bodies to\ndesign accountable, transparent, and user-centered detection solutions, and\nincorporate sociotechnical considerations into future AI standards, procedures\nand evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can\ndrive innovation, safeguard public trust, strengthen AI literacy, and\ncontribute to a more resilient global information credibility.\n", "link": "http://arxiv.org/abs/2504.21489v2", "date": "2025-05-01", "relevancy": 1.8321, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.471}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4676}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIED%3A%20Truly%20Innovative%20and%20Effective%20AI%20Detection%20Benchmark%2C%20developed%0A%20%20by%20WITNESS&body=Title%3A%20TRIED%3A%20Truly%20Innovative%20and%20Effective%20AI%20Detection%20Benchmark%2C%20developed%0A%20%20by%20WITNESS%0AAuthor%3A%20Shirin%20Anlen%20and%20Zuzanna%20Wojciak%0AAbstract%3A%20%20%20The%20proliferation%20of%20generative%20AI%20and%20deceptive%20synthetic%20media%20threatens%0Athe%20global%20information%20ecosystem%2C%20especially%20across%20the%20Global%20Majority.%20This%0Areport%20from%20WITNESS%20highlights%20the%20limitations%20of%20current%20AI%20detection%20tools%2C%0Awhich%20often%20underperform%20in%20real-world%20scenarios%20due%20to%20challenges%20related%20to%0Aexplainability%2C%20fairness%2C%20accessibility%2C%20and%20contextual%20relevance.%20In%20response%2C%0AWITNESS%20introduces%20the%20Truly%20Innovative%20and%20Effective%20AI%20Detection%20%28TRIED%29%0ABenchmark%2C%20a%20new%20framework%20for%20evaluating%20detection%20tools%20based%20on%20their%0Areal-world%20impact%20and%20capacity%20for%20innovation.%20Drawing%20on%20frontline%0Aexperiences%2C%20deceptive%20AI%20cases%2C%20and%20global%20consultations%2C%20the%20report%20outlines%0Ahow%20detection%20tools%20must%20evolve%20to%20become%20truly%20innovative%20and%20relevant%20by%0Ameeting%20diverse%20linguistic%2C%20cultural%2C%20and%20technological%20contexts.%20It%20offers%0Apractical%20guidance%20for%20developers%2C%20policy%20actors%2C%20and%20standards%20bodies%20to%0Adesign%20accountable%2C%20transparent%2C%20and%20user-centered%20detection%20solutions%2C%20and%0Aincorporate%20sociotechnical%20considerations%20into%20future%20AI%20standards%2C%20procedures%0Aand%20evaluation%20frameworks.%20By%20adopting%20the%20TRIED%20Benchmark%2C%20stakeholders%20can%0Adrive%20innovation%2C%20safeguard%20public%20trust%2C%20strengthen%20AI%20literacy%2C%20and%0Acontribute%20to%20a%20more%20resilient%20global%20information%20credibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIED%253A%2520Truly%2520Innovative%2520and%2520Effective%2520AI%2520Detection%2520Benchmark%252C%2520developed%250A%2520%2520by%2520WITNESS%26entry.906535625%3DShirin%2520Anlen%2520and%2520Zuzanna%2520Wojciak%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520generative%2520AI%2520and%2520deceptive%2520synthetic%2520media%2520threatens%250Athe%2520global%2520information%2520ecosystem%252C%2520especially%2520across%2520the%2520Global%2520Majority.%2520This%250Areport%2520from%2520WITNESS%2520highlights%2520the%2520limitations%2520of%2520current%2520AI%2520detection%2520tools%252C%250Awhich%2520often%2520underperform%2520in%2520real-world%2520scenarios%2520due%2520to%2520challenges%2520related%2520to%250Aexplainability%252C%2520fairness%252C%2520accessibility%252C%2520and%2520contextual%2520relevance.%2520In%2520response%252C%250AWITNESS%2520introduces%2520the%2520Truly%2520Innovative%2520and%2520Effective%2520AI%2520Detection%2520%2528TRIED%2529%250ABenchmark%252C%2520a%2520new%2520framework%2520for%2520evaluating%2520detection%2520tools%2520based%2520on%2520their%250Areal-world%2520impact%2520and%2520capacity%2520for%2520innovation.%2520Drawing%2520on%2520frontline%250Aexperiences%252C%2520deceptive%2520AI%2520cases%252C%2520and%2520global%2520consultations%252C%2520the%2520report%2520outlines%250Ahow%2520detection%2520tools%2520must%2520evolve%2520to%2520become%2520truly%2520innovative%2520and%2520relevant%2520by%250Ameeting%2520diverse%2520linguistic%252C%2520cultural%252C%2520and%2520technological%2520contexts.%2520It%2520offers%250Apractical%2520guidance%2520for%2520developers%252C%2520policy%2520actors%252C%2520and%2520standards%2520bodies%2520to%250Adesign%2520accountable%252C%2520transparent%252C%2520and%2520user-centered%2520detection%2520solutions%252C%2520and%250Aincorporate%2520sociotechnical%2520considerations%2520into%2520future%2520AI%2520standards%252C%2520procedures%250Aand%2520evaluation%2520frameworks.%2520By%2520adopting%2520the%2520TRIED%2520Benchmark%252C%2520stakeholders%2520can%250Adrive%2520innovation%252C%2520safeguard%2520public%2520trust%252C%2520strengthen%2520AI%2520literacy%252C%2520and%250Acontribute%2520to%2520a%2520more%2520resilient%2520global%2520information%2520credibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIED%3A%20Truly%20Innovative%20and%20Effective%20AI%20Detection%20Benchmark%2C%20developed%0A%20%20by%20WITNESS&entry.906535625=Shirin%20Anlen%20and%20Zuzanna%20Wojciak&entry.1292438233=%20%20The%20proliferation%20of%20generative%20AI%20and%20deceptive%20synthetic%20media%20threatens%0Athe%20global%20information%20ecosystem%2C%20especially%20across%20the%20Global%20Majority.%20This%0Areport%20from%20WITNESS%20highlights%20the%20limitations%20of%20current%20AI%20detection%20tools%2C%0Awhich%20often%20underperform%20in%20real-world%20scenarios%20due%20to%20challenges%20related%20to%0Aexplainability%2C%20fairness%2C%20accessibility%2C%20and%20contextual%20relevance.%20In%20response%2C%0AWITNESS%20introduces%20the%20Truly%20Innovative%20and%20Effective%20AI%20Detection%20%28TRIED%29%0ABenchmark%2C%20a%20new%20framework%20for%20evaluating%20detection%20tools%20based%20on%20their%0Areal-world%20impact%20and%20capacity%20for%20innovation.%20Drawing%20on%20frontline%0Aexperiences%2C%20deceptive%20AI%20cases%2C%20and%20global%20consultations%2C%20the%20report%20outlines%0Ahow%20detection%20tools%20must%20evolve%20to%20become%20truly%20innovative%20and%20relevant%20by%0Ameeting%20diverse%20linguistic%2C%20cultural%2C%20and%20technological%20contexts.%20It%20offers%0Apractical%20guidance%20for%20developers%2C%20policy%20actors%2C%20and%20standards%20bodies%20to%0Adesign%20accountable%2C%20transparent%2C%20and%20user-centered%20detection%20solutions%2C%20and%0Aincorporate%20sociotechnical%20considerations%20into%20future%20AI%20standards%2C%20procedures%0Aand%20evaluation%20frameworks.%20By%20adopting%20the%20TRIED%20Benchmark%2C%20stakeholders%20can%0Adrive%20innovation%2C%20safeguard%20public%20trust%2C%20strengthen%20AI%20literacy%2C%20and%0Acontribute%20to%20a%20more%20resilient%20global%20information%20credibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21489v2&entry.124074799=Read"},
{"title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful\n  Prompt Optimizers", "author": "Qingyan Guo and Rui Wang and Junliang Guo and Bei Li and Kaitao Song and Xu Tan and Guoqing Liu and Jiang Bian and Yujiu Yang", "abstract": "  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.\n", "link": "http://arxiv.org/abs/2309.08532v3", "date": "2025-05-01", "relevancy": 1.8253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4571}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoPrompt%3A%20Connecting%20LLMs%20with%20Evolutionary%20Algorithms%20Yields%20Powerful%0A%20%20Prompt%20Optimizers&body=Title%3A%20EvoPrompt%3A%20Connecting%20LLMs%20with%20Evolutionary%20Algorithms%20Yields%20Powerful%0A%20%20Prompt%20Optimizers%0AAuthor%3A%20Qingyan%20Guo%20and%20Rui%20Wang%20and%20Junliang%20Guo%20and%20Bei%20Li%20and%20Kaitao%20Song%20and%20Xu%20Tan%20and%20Guoqing%20Liu%20and%20Jiang%20Bian%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20various%20tasks%2C%20but%20they%20rely%20on%0Acarefully%20crafted%20prompts%20that%20often%20demand%20substantial%20human%20effort.%20To%0Aautomate%20this%20process%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20discrete%0Aprompt%20optimization%2C%20called%20EvoPrompt%2C%20which%20borrows%20the%20idea%20of%20evolutionary%0Aalgorithms%20%28EAs%29%20as%20they%20exhibit%20good%20performance%20and%20fast%20convergence.%20To%0Aenable%20EAs%20to%20work%20on%20discrete%20prompts%2C%20which%20are%20natural%20language%20expressions%0Athat%20need%20to%20be%20coherent%20and%20human-readable%2C%20we%20connect%20LLMs%20with%20EAs.%20This%0Aapproach%20allows%20us%20to%20simultaneously%20leverage%20the%20powerful%20language%20processing%0Acapabilities%20of%20LLMs%20and%20the%20efficient%20optimization%20performance%20of%20EAs.%0ASpecifically%2C%20abstaining%20from%20any%20gradients%20or%20parameters%2C%20EvoPrompt%20starts%0Afrom%20a%20population%20of%20prompts%20and%20iteratively%20generates%20new%20prompts%20with%20LLMs%0Abased%20on%20the%20evolutionary%20operators%2C%20improving%20the%20population%20based%20on%20the%0Adevelopment%20set.%20We%20optimize%20prompts%20for%20both%20closed-%20and%20open-source%20LLMs%0Aincluding%20GPT-3.5%20and%20Alpaca%2C%20on%2031%20datasets%20covering%20language%20understanding%2C%0Ageneration%20tasks%2C%20as%20well%20as%20BIG-Bench%20Hard%20%28BBH%29%20tasks.%20EvoPrompt%0Asignificantly%20outperforms%20human-engineered%20prompts%20and%20existing%20methods%20for%0Aautomatic%20prompt%20generation%20%28e.g.%2C%20up%20to%2025%25%20on%20BBH%29.%20Furthermore%2C%20EvoPrompt%0Ademonstrates%20that%20connecting%20LLMs%20with%20EAs%20creates%20synergies%2C%20which%20could%0Ainspire%20further%20research%20on%20the%20combination%20of%20LLMs%20and%20conventional%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08532v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoPrompt%253A%2520Connecting%2520LLMs%2520with%2520Evolutionary%2520Algorithms%2520Yields%2520Powerful%250A%2520%2520Prompt%2520Optimizers%26entry.906535625%3DQingyan%2520Guo%2520and%2520Rui%2520Wang%2520and%2520Junliang%2520Guo%2520and%2520Bei%2520Li%2520and%2520Kaitao%2520Song%2520and%2520Xu%2520Tan%2520and%2520Guoqing%2520Liu%2520and%2520Jiang%2520Bian%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520various%2520tasks%252C%2520but%2520they%2520rely%2520on%250Acarefully%2520crafted%2520prompts%2520that%2520often%2520demand%2520substantial%2520human%2520effort.%2520To%250Aautomate%2520this%2520process%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520discrete%250Aprompt%2520optimization%252C%2520called%2520EvoPrompt%252C%2520which%2520borrows%2520the%2520idea%2520of%2520evolutionary%250Aalgorithms%2520%2528EAs%2529%2520as%2520they%2520exhibit%2520good%2520performance%2520and%2520fast%2520convergence.%2520To%250Aenable%2520EAs%2520to%2520work%2520on%2520discrete%2520prompts%252C%2520which%2520are%2520natural%2520language%2520expressions%250Athat%2520need%2520to%2520be%2520coherent%2520and%2520human-readable%252C%2520we%2520connect%2520LLMs%2520with%2520EAs.%2520This%250Aapproach%2520allows%2520us%2520to%2520simultaneously%2520leverage%2520the%2520powerful%2520language%2520processing%250Acapabilities%2520of%2520LLMs%2520and%2520the%2520efficient%2520optimization%2520performance%2520of%2520EAs.%250ASpecifically%252C%2520abstaining%2520from%2520any%2520gradients%2520or%2520parameters%252C%2520EvoPrompt%2520starts%250Afrom%2520a%2520population%2520of%2520prompts%2520and%2520iteratively%2520generates%2520new%2520prompts%2520with%2520LLMs%250Abased%2520on%2520the%2520evolutionary%2520operators%252C%2520improving%2520the%2520population%2520based%2520on%2520the%250Adevelopment%2520set.%2520We%2520optimize%2520prompts%2520for%2520both%2520closed-%2520and%2520open-source%2520LLMs%250Aincluding%2520GPT-3.5%2520and%2520Alpaca%252C%2520on%252031%2520datasets%2520covering%2520language%2520understanding%252C%250Ageneration%2520tasks%252C%2520as%2520well%2520as%2520BIG-Bench%2520Hard%2520%2528BBH%2529%2520tasks.%2520EvoPrompt%250Asignificantly%2520outperforms%2520human-engineered%2520prompts%2520and%2520existing%2520methods%2520for%250Aautomatic%2520prompt%2520generation%2520%2528e.g.%252C%2520up%2520to%252025%2525%2520on%2520BBH%2529.%2520Furthermore%252C%2520EvoPrompt%250Ademonstrates%2520that%2520connecting%2520LLMs%2520with%2520EAs%2520creates%2520synergies%252C%2520which%2520could%250Ainspire%2520further%2520research%2520on%2520the%2520combination%2520of%2520LLMs%2520and%2520conventional%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08532v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoPrompt%3A%20Connecting%20LLMs%20with%20Evolutionary%20Algorithms%20Yields%20Powerful%0A%20%20Prompt%20Optimizers&entry.906535625=Qingyan%20Guo%20and%20Rui%20Wang%20and%20Junliang%20Guo%20and%20Bei%20Li%20and%20Kaitao%20Song%20and%20Xu%20Tan%20and%20Guoqing%20Liu%20and%20Jiang%20Bian%20and%20Yujiu%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20various%20tasks%2C%20but%20they%20rely%20on%0Acarefully%20crafted%20prompts%20that%20often%20demand%20substantial%20human%20effort.%20To%0Aautomate%20this%20process%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20discrete%0Aprompt%20optimization%2C%20called%20EvoPrompt%2C%20which%20borrows%20the%20idea%20of%20evolutionary%0Aalgorithms%20%28EAs%29%20as%20they%20exhibit%20good%20performance%20and%20fast%20convergence.%20To%0Aenable%20EAs%20to%20work%20on%20discrete%20prompts%2C%20which%20are%20natural%20language%20expressions%0Athat%20need%20to%20be%20coherent%20and%20human-readable%2C%20we%20connect%20LLMs%20with%20EAs.%20This%0Aapproach%20allows%20us%20to%20simultaneously%20leverage%20the%20powerful%20language%20processing%0Acapabilities%20of%20LLMs%20and%20the%20efficient%20optimization%20performance%20of%20EAs.%0ASpecifically%2C%20abstaining%20from%20any%20gradients%20or%20parameters%2C%20EvoPrompt%20starts%0Afrom%20a%20population%20of%20prompts%20and%20iteratively%20generates%20new%20prompts%20with%20LLMs%0Abased%20on%20the%20evolutionary%20operators%2C%20improving%20the%20population%20based%20on%20the%0Adevelopment%20set.%20We%20optimize%20prompts%20for%20both%20closed-%20and%20open-source%20LLMs%0Aincluding%20GPT-3.5%20and%20Alpaca%2C%20on%2031%20datasets%20covering%20language%20understanding%2C%0Ageneration%20tasks%2C%20as%20well%20as%20BIG-Bench%20Hard%20%28BBH%29%20tasks.%20EvoPrompt%0Asignificantly%20outperforms%20human-engineered%20prompts%20and%20existing%20methods%20for%0Aautomatic%20prompt%20generation%20%28e.g.%2C%20up%20to%2025%25%20on%20BBH%29.%20Furthermore%2C%20EvoPrompt%0Ademonstrates%20that%20connecting%20LLMs%20with%20EAs%20creates%20synergies%2C%20which%20could%0Ainspire%20further%20research%20on%20the%20combination%20of%20LLMs%20and%20conventional%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08532v3&entry.124074799=Read"},
{"title": "Forward kinematics of a general Stewart-Gough platform by elimination\n  templates", "author": "Evgeniy Martyushev", "abstract": "  The paper proposes an efficient algebraic solution to the problem of forward\nkinematics for a general Stewart-Gough platform. The problem involves\ndetermining all possible postures of a mobile platform connected to a fixed\nbase by six legs, given the leg lengths and the internal geometries of the\nplatform and base. The problem is known to have 40 solutions (whether real or\ncomplex). The proposed algorithm consists of three main steps: (i) a specific\nsparse matrix of size 293x362 (the elimination template) is constructed from\nthe coefficients of the polynomial system describing the platform's kinematics;\n(ii) the PLU decomposition of this matrix is used to construct a pair of 69x69\nmatrices; (iii) all 40 solutions (including complex ones) are obtained by\ncomputing the generalized eigenvectors of this matrix pair. The proposed\nalgorithm is numerically robust, computationally efficient, and straightforward\nto implement - requiring only standard linear algebra decompositions. MATLAB,\nJulia, and Python implementations of the algorithm will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2505.00634v1", "date": "2025-05-01", "relevancy": 1.8217, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.463}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forward%20kinematics%20of%20a%20general%20Stewart-Gough%20platform%20by%20elimination%0A%20%20templates&body=Title%3A%20Forward%20kinematics%20of%20a%20general%20Stewart-Gough%20platform%20by%20elimination%0A%20%20templates%0AAuthor%3A%20Evgeniy%20Martyushev%0AAbstract%3A%20%20%20The%20paper%20proposes%20an%20efficient%20algebraic%20solution%20to%20the%20problem%20of%20forward%0Akinematics%20for%20a%20general%20Stewart-Gough%20platform.%20The%20problem%20involves%0Adetermining%20all%20possible%20postures%20of%20a%20mobile%20platform%20connected%20to%20a%20fixed%0Abase%20by%20six%20legs%2C%20given%20the%20leg%20lengths%20and%20the%20internal%20geometries%20of%20the%0Aplatform%20and%20base.%20The%20problem%20is%20known%20to%20have%2040%20solutions%20%28whether%20real%20or%0Acomplex%29.%20The%20proposed%20algorithm%20consists%20of%20three%20main%20steps%3A%20%28i%29%20a%20specific%0Asparse%20matrix%20of%20size%20293x362%20%28the%20elimination%20template%29%20is%20constructed%20from%0Athe%20coefficients%20of%20the%20polynomial%20system%20describing%20the%20platform%27s%20kinematics%3B%0A%28ii%29%20the%20PLU%20decomposition%20of%20this%20matrix%20is%20used%20to%20construct%20a%20pair%20of%2069x69%0Amatrices%3B%20%28iii%29%20all%2040%20solutions%20%28including%20complex%20ones%29%20are%20obtained%20by%0Acomputing%20the%20generalized%20eigenvectors%20of%20this%20matrix%20pair.%20The%20proposed%0Aalgorithm%20is%20numerically%20robust%2C%20computationally%20efficient%2C%20and%20straightforward%0Ato%20implement%20-%20requiring%20only%20standard%20linear%20algebra%20decompositions.%20MATLAB%2C%0AJulia%2C%20and%20Python%20implementations%20of%20the%20algorithm%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForward%2520kinematics%2520of%2520a%2520general%2520Stewart-Gough%2520platform%2520by%2520elimination%250A%2520%2520templates%26entry.906535625%3DEvgeniy%2520Martyushev%26entry.1292438233%3D%2520%2520The%2520paper%2520proposes%2520an%2520efficient%2520algebraic%2520solution%2520to%2520the%2520problem%2520of%2520forward%250Akinematics%2520for%2520a%2520general%2520Stewart-Gough%2520platform.%2520The%2520problem%2520involves%250Adetermining%2520all%2520possible%2520postures%2520of%2520a%2520mobile%2520platform%2520connected%2520to%2520a%2520fixed%250Abase%2520by%2520six%2520legs%252C%2520given%2520the%2520leg%2520lengths%2520and%2520the%2520internal%2520geometries%2520of%2520the%250Aplatform%2520and%2520base.%2520The%2520problem%2520is%2520known%2520to%2520have%252040%2520solutions%2520%2528whether%2520real%2520or%250Acomplex%2529.%2520The%2520proposed%2520algorithm%2520consists%2520of%2520three%2520main%2520steps%253A%2520%2528i%2529%2520a%2520specific%250Asparse%2520matrix%2520of%2520size%2520293x362%2520%2528the%2520elimination%2520template%2529%2520is%2520constructed%2520from%250Athe%2520coefficients%2520of%2520the%2520polynomial%2520system%2520describing%2520the%2520platform%2527s%2520kinematics%253B%250A%2528ii%2529%2520the%2520PLU%2520decomposition%2520of%2520this%2520matrix%2520is%2520used%2520to%2520construct%2520a%2520pair%2520of%252069x69%250Amatrices%253B%2520%2528iii%2529%2520all%252040%2520solutions%2520%2528including%2520complex%2520ones%2529%2520are%2520obtained%2520by%250Acomputing%2520the%2520generalized%2520eigenvectors%2520of%2520this%2520matrix%2520pair.%2520The%2520proposed%250Aalgorithm%2520is%2520numerically%2520robust%252C%2520computationally%2520efficient%252C%2520and%2520straightforward%250Ato%2520implement%2520-%2520requiring%2520only%2520standard%2520linear%2520algebra%2520decompositions.%2520MATLAB%252C%250AJulia%252C%2520and%2520Python%2520implementations%2520of%2520the%2520algorithm%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forward%20kinematics%20of%20a%20general%20Stewart-Gough%20platform%20by%20elimination%0A%20%20templates&entry.906535625=Evgeniy%20Martyushev&entry.1292438233=%20%20The%20paper%20proposes%20an%20efficient%20algebraic%20solution%20to%20the%20problem%20of%20forward%0Akinematics%20for%20a%20general%20Stewart-Gough%20platform.%20The%20problem%20involves%0Adetermining%20all%20possible%20postures%20of%20a%20mobile%20platform%20connected%20to%20a%20fixed%0Abase%20by%20six%20legs%2C%20given%20the%20leg%20lengths%20and%20the%20internal%20geometries%20of%20the%0Aplatform%20and%20base.%20The%20problem%20is%20known%20to%20have%2040%20solutions%20%28whether%20real%20or%0Acomplex%29.%20The%20proposed%20algorithm%20consists%20of%20three%20main%20steps%3A%20%28i%29%20a%20specific%0Asparse%20matrix%20of%20size%20293x362%20%28the%20elimination%20template%29%20is%20constructed%20from%0Athe%20coefficients%20of%20the%20polynomial%20system%20describing%20the%20platform%27s%20kinematics%3B%0A%28ii%29%20the%20PLU%20decomposition%20of%20this%20matrix%20is%20used%20to%20construct%20a%20pair%20of%2069x69%0Amatrices%3B%20%28iii%29%20all%2040%20solutions%20%28including%20complex%20ones%29%20are%20obtained%20by%0Acomputing%20the%20generalized%20eigenvectors%20of%20this%20matrix%20pair.%20The%20proposed%0Aalgorithm%20is%20numerically%20robust%2C%20computationally%20efficient%2C%20and%20straightforward%0Ato%20implement%20-%20requiring%20only%20standard%20linear%20algebra%20decompositions.%20MATLAB%2C%0AJulia%2C%20and%20Python%20implementations%20of%20the%20algorithm%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00634v1&entry.124074799=Read"},
{"title": "Belief Roadmaps with Uncertain Landmark Evanescence", "author": "Erick Fuentes and Jared Strader and Ethan Fahnestock and Nicholas Roy", "abstract": "  We would like a robot to navigate to a goal location while minimizing state\nuncertainty. To aid the robot in this endeavor, maps provide a prior belief\nover the location of objects and regions of interest. To localize itself within\nthe map, a robot identifies mapped landmarks using its sensors. However, as the\ntime between map creation and robot deployment increases, portions of the map\ncan become stale, and landmarks, once believed to be permanent, may disappear.\nWe refer to the propensity of a landmark to disappear as landmark evanescence.\nReasoning about landmark evanescence during path planning, and the associated\nimpact on localization accuracy, requires analyzing the presence or absence of\neach landmark, leading to an exponential number of possible outcomes of a given\nmotion plan. To address this complexity, we develop BRULE, an extension of the\nBelief Roadmap. During planning, we replace the belief over future robot poses\nwith a Gaussian mixture which is able to capture the effects of landmark\nevanescence. Furthermore, we show that belief updates can be made efficient,\nand that maintaining a random subset of mixture components is sufficient to\nfind high quality solutions. We demonstrate performance in simulated and\nreal-world experiments. Software is available at https://bit.ly/BRULE.\n", "link": "http://arxiv.org/abs/2501.17982v2", "date": "2025-05-01", "relevancy": 1.8204, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6253}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6132}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Belief%20Roadmaps%20with%20Uncertain%20Landmark%20Evanescence&body=Title%3A%20Belief%20Roadmaps%20with%20Uncertain%20Landmark%20Evanescence%0AAuthor%3A%20Erick%20Fuentes%20and%20Jared%20Strader%20and%20Ethan%20Fahnestock%20and%20Nicholas%20Roy%0AAbstract%3A%20%20%20We%20would%20like%20a%20robot%20to%20navigate%20to%20a%20goal%20location%20while%20minimizing%20state%0Auncertainty.%20To%20aid%20the%20robot%20in%20this%20endeavor%2C%20maps%20provide%20a%20prior%20belief%0Aover%20the%20location%20of%20objects%20and%20regions%20of%20interest.%20To%20localize%20itself%20within%0Athe%20map%2C%20a%20robot%20identifies%20mapped%20landmarks%20using%20its%20sensors.%20However%2C%20as%20the%0Atime%20between%20map%20creation%20and%20robot%20deployment%20increases%2C%20portions%20of%20the%20map%0Acan%20become%20stale%2C%20and%20landmarks%2C%20once%20believed%20to%20be%20permanent%2C%20may%20disappear.%0AWe%20refer%20to%20the%20propensity%20of%20a%20landmark%20to%20disappear%20as%20landmark%20evanescence.%0AReasoning%20about%20landmark%20evanescence%20during%20path%20planning%2C%20and%20the%20associated%0Aimpact%20on%20localization%20accuracy%2C%20requires%20analyzing%20the%20presence%20or%20absence%20of%0Aeach%20landmark%2C%20leading%20to%20an%20exponential%20number%20of%20possible%20outcomes%20of%20a%20given%0Amotion%20plan.%20To%20address%20this%20complexity%2C%20we%20develop%20BRULE%2C%20an%20extension%20of%20the%0ABelief%20Roadmap.%20During%20planning%2C%20we%20replace%20the%20belief%20over%20future%20robot%20poses%0Awith%20a%20Gaussian%20mixture%20which%20is%20able%20to%20capture%20the%20effects%20of%20landmark%0Aevanescence.%20Furthermore%2C%20we%20show%20that%20belief%20updates%20can%20be%20made%20efficient%2C%0Aand%20that%20maintaining%20a%20random%20subset%20of%20mixture%20components%20is%20sufficient%20to%0Afind%20high%20quality%20solutions.%20We%20demonstrate%20performance%20in%20simulated%20and%0Areal-world%20experiments.%20Software%20is%20available%20at%20https%3A//bit.ly/BRULE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBelief%2520Roadmaps%2520with%2520Uncertain%2520Landmark%2520Evanescence%26entry.906535625%3DErick%2520Fuentes%2520and%2520Jared%2520Strader%2520and%2520Ethan%2520Fahnestock%2520and%2520Nicholas%2520Roy%26entry.1292438233%3D%2520%2520We%2520would%2520like%2520a%2520robot%2520to%2520navigate%2520to%2520a%2520goal%2520location%2520while%2520minimizing%2520state%250Auncertainty.%2520To%2520aid%2520the%2520robot%2520in%2520this%2520endeavor%252C%2520maps%2520provide%2520a%2520prior%2520belief%250Aover%2520the%2520location%2520of%2520objects%2520and%2520regions%2520of%2520interest.%2520To%2520localize%2520itself%2520within%250Athe%2520map%252C%2520a%2520robot%2520identifies%2520mapped%2520landmarks%2520using%2520its%2520sensors.%2520However%252C%2520as%2520the%250Atime%2520between%2520map%2520creation%2520and%2520robot%2520deployment%2520increases%252C%2520portions%2520of%2520the%2520map%250Acan%2520become%2520stale%252C%2520and%2520landmarks%252C%2520once%2520believed%2520to%2520be%2520permanent%252C%2520may%2520disappear.%250AWe%2520refer%2520to%2520the%2520propensity%2520of%2520a%2520landmark%2520to%2520disappear%2520as%2520landmark%2520evanescence.%250AReasoning%2520about%2520landmark%2520evanescence%2520during%2520path%2520planning%252C%2520and%2520the%2520associated%250Aimpact%2520on%2520localization%2520accuracy%252C%2520requires%2520analyzing%2520the%2520presence%2520or%2520absence%2520of%250Aeach%2520landmark%252C%2520leading%2520to%2520an%2520exponential%2520number%2520of%2520possible%2520outcomes%2520of%2520a%2520given%250Amotion%2520plan.%2520To%2520address%2520this%2520complexity%252C%2520we%2520develop%2520BRULE%252C%2520an%2520extension%2520of%2520the%250ABelief%2520Roadmap.%2520During%2520planning%252C%2520we%2520replace%2520the%2520belief%2520over%2520future%2520robot%2520poses%250Awith%2520a%2520Gaussian%2520mixture%2520which%2520is%2520able%2520to%2520capture%2520the%2520effects%2520of%2520landmark%250Aevanescence.%2520Furthermore%252C%2520we%2520show%2520that%2520belief%2520updates%2520can%2520be%2520made%2520efficient%252C%250Aand%2520that%2520maintaining%2520a%2520random%2520subset%2520of%2520mixture%2520components%2520is%2520sufficient%2520to%250Afind%2520high%2520quality%2520solutions.%2520We%2520demonstrate%2520performance%2520in%2520simulated%2520and%250Areal-world%2520experiments.%2520Software%2520is%2520available%2520at%2520https%253A//bit.ly/BRULE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Belief%20Roadmaps%20with%20Uncertain%20Landmark%20Evanescence&entry.906535625=Erick%20Fuentes%20and%20Jared%20Strader%20and%20Ethan%20Fahnestock%20and%20Nicholas%20Roy&entry.1292438233=%20%20We%20would%20like%20a%20robot%20to%20navigate%20to%20a%20goal%20location%20while%20minimizing%20state%0Auncertainty.%20To%20aid%20the%20robot%20in%20this%20endeavor%2C%20maps%20provide%20a%20prior%20belief%0Aover%20the%20location%20of%20objects%20and%20regions%20of%20interest.%20To%20localize%20itself%20within%0Athe%20map%2C%20a%20robot%20identifies%20mapped%20landmarks%20using%20its%20sensors.%20However%2C%20as%20the%0Atime%20between%20map%20creation%20and%20robot%20deployment%20increases%2C%20portions%20of%20the%20map%0Acan%20become%20stale%2C%20and%20landmarks%2C%20once%20believed%20to%20be%20permanent%2C%20may%20disappear.%0AWe%20refer%20to%20the%20propensity%20of%20a%20landmark%20to%20disappear%20as%20landmark%20evanescence.%0AReasoning%20about%20landmark%20evanescence%20during%20path%20planning%2C%20and%20the%20associated%0Aimpact%20on%20localization%20accuracy%2C%20requires%20analyzing%20the%20presence%20or%20absence%20of%0Aeach%20landmark%2C%20leading%20to%20an%20exponential%20number%20of%20possible%20outcomes%20of%20a%20given%0Amotion%20plan.%20To%20address%20this%20complexity%2C%20we%20develop%20BRULE%2C%20an%20extension%20of%20the%0ABelief%20Roadmap.%20During%20planning%2C%20we%20replace%20the%20belief%20over%20future%20robot%20poses%0Awith%20a%20Gaussian%20mixture%20which%20is%20able%20to%20capture%20the%20effects%20of%20landmark%0Aevanescence.%20Furthermore%2C%20we%20show%20that%20belief%20updates%20can%20be%20made%20efficient%2C%0Aand%20that%20maintaining%20a%20random%20subset%20of%20mixture%20components%20is%20sufficient%20to%0Afind%20high%20quality%20solutions.%20We%20demonstrate%20performance%20in%20simulated%20and%0Areal-world%20experiments.%20Software%20is%20available%20at%20https%3A//bit.ly/BRULE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17982v2&entry.124074799=Read"},
{"title": "Graph Spectral Filtering with Chebyshev Interpolation for Recommendation", "author": "Chanwoo Kim and Jinkyu Sung and Yebonn Han and Joonseok Lee", "abstract": "  Graph convolutional networks have recently gained prominence in collaborative\nfiltering (CF) for recommendations. However, we identify potential bottlenecks\nin two foundational components. First, the embedding layer leads to a latent\nspace with limited capacity, overlooking locally observed but potentially\nvaluable preference patterns. Also, the widely-used neighborhood aggregation is\nlimited in its ability to leverage diverse preference patterns in a\nfine-grained manner. Building on spectral graph theory, we reveal that these\nlimitations stem from graph filtering with a cut-off in the frequency spectrum\nand a restricted linear form. To address these issues, we introduce ChebyCF, a\nCF framework based on graph spectral filtering. Instead of a learned embedding,\nit takes a user's raw interaction history to utilize the full spectrum of\nsignals contained in it. Also, it adopts Chebyshev interpolation to effectively\napproximate a flexible non-linear graph filter, and further enhances it by\nusing an additional ideal pass filter and degree-based normalization. Through\nextensive experiments, we verify that ChebyCF overcomes the aforementioned\nbottlenecks and achieves state-of-the-art performance across multiple\nbenchmarks and reasonably fast inference. Our code is available at\nhttps://github.com/chanwoo0806/ChebyCF.\n", "link": "http://arxiv.org/abs/2505.00552v1", "date": "2025-05-01", "relevancy": 1.8193, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4629}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4502}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Spectral%20Filtering%20with%20Chebyshev%20Interpolation%20for%20Recommendation&body=Title%3A%20Graph%20Spectral%20Filtering%20with%20Chebyshev%20Interpolation%20for%20Recommendation%0AAuthor%3A%20Chanwoo%20Kim%20and%20Jinkyu%20Sung%20and%20Yebonn%20Han%20and%20Joonseok%20Lee%0AAbstract%3A%20%20%20Graph%20convolutional%20networks%20have%20recently%20gained%20prominence%20in%20collaborative%0Afiltering%20%28CF%29%20for%20recommendations.%20However%2C%20we%20identify%20potential%20bottlenecks%0Ain%20two%20foundational%20components.%20First%2C%20the%20embedding%20layer%20leads%20to%20a%20latent%0Aspace%20with%20limited%20capacity%2C%20overlooking%20locally%20observed%20but%20potentially%0Avaluable%20preference%20patterns.%20Also%2C%20the%20widely-used%20neighborhood%20aggregation%20is%0Alimited%20in%20its%20ability%20to%20leverage%20diverse%20preference%20patterns%20in%20a%0Afine-grained%20manner.%20Building%20on%20spectral%20graph%20theory%2C%20we%20reveal%20that%20these%0Alimitations%20stem%20from%20graph%20filtering%20with%20a%20cut-off%20in%20the%20frequency%20spectrum%0Aand%20a%20restricted%20linear%20form.%20To%20address%20these%20issues%2C%20we%20introduce%20ChebyCF%2C%20a%0ACF%20framework%20based%20on%20graph%20spectral%20filtering.%20Instead%20of%20a%20learned%20embedding%2C%0Ait%20takes%20a%20user%27s%20raw%20interaction%20history%20to%20utilize%20the%20full%20spectrum%20of%0Asignals%20contained%20in%20it.%20Also%2C%20it%20adopts%20Chebyshev%20interpolation%20to%20effectively%0Aapproximate%20a%20flexible%20non-linear%20graph%20filter%2C%20and%20further%20enhances%20it%20by%0Ausing%20an%20additional%20ideal%20pass%20filter%20and%20degree-based%20normalization.%20Through%0Aextensive%20experiments%2C%20we%20verify%20that%20ChebyCF%20overcomes%20the%20aforementioned%0Abottlenecks%20and%20achieves%20state-of-the-art%20performance%20across%20multiple%0Abenchmarks%20and%20reasonably%20fast%20inference.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/chanwoo0806/ChebyCF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Spectral%2520Filtering%2520with%2520Chebyshev%2520Interpolation%2520for%2520Recommendation%26entry.906535625%3DChanwoo%2520Kim%2520and%2520Jinkyu%2520Sung%2520and%2520Yebonn%2520Han%2520and%2520Joonseok%2520Lee%26entry.1292438233%3D%2520%2520Graph%2520convolutional%2520networks%2520have%2520recently%2520gained%2520prominence%2520in%2520collaborative%250Afiltering%2520%2528CF%2529%2520for%2520recommendations.%2520However%252C%2520we%2520identify%2520potential%2520bottlenecks%250Ain%2520two%2520foundational%2520components.%2520First%252C%2520the%2520embedding%2520layer%2520leads%2520to%2520a%2520latent%250Aspace%2520with%2520limited%2520capacity%252C%2520overlooking%2520locally%2520observed%2520but%2520potentially%250Avaluable%2520preference%2520patterns.%2520Also%252C%2520the%2520widely-used%2520neighborhood%2520aggregation%2520is%250Alimited%2520in%2520its%2520ability%2520to%2520leverage%2520diverse%2520preference%2520patterns%2520in%2520a%250Afine-grained%2520manner.%2520Building%2520on%2520spectral%2520graph%2520theory%252C%2520we%2520reveal%2520that%2520these%250Alimitations%2520stem%2520from%2520graph%2520filtering%2520with%2520a%2520cut-off%2520in%2520the%2520frequency%2520spectrum%250Aand%2520a%2520restricted%2520linear%2520form.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520ChebyCF%252C%2520a%250ACF%2520framework%2520based%2520on%2520graph%2520spectral%2520filtering.%2520Instead%2520of%2520a%2520learned%2520embedding%252C%250Ait%2520takes%2520a%2520user%2527s%2520raw%2520interaction%2520history%2520to%2520utilize%2520the%2520full%2520spectrum%2520of%250Asignals%2520contained%2520in%2520it.%2520Also%252C%2520it%2520adopts%2520Chebyshev%2520interpolation%2520to%2520effectively%250Aapproximate%2520a%2520flexible%2520non-linear%2520graph%2520filter%252C%2520and%2520further%2520enhances%2520it%2520by%250Ausing%2520an%2520additional%2520ideal%2520pass%2520filter%2520and%2520degree-based%2520normalization.%2520Through%250Aextensive%2520experiments%252C%2520we%2520verify%2520that%2520ChebyCF%2520overcomes%2520the%2520aforementioned%250Abottlenecks%2520and%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%250Abenchmarks%2520and%2520reasonably%2520fast%2520inference.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/chanwoo0806/ChebyCF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Spectral%20Filtering%20with%20Chebyshev%20Interpolation%20for%20Recommendation&entry.906535625=Chanwoo%20Kim%20and%20Jinkyu%20Sung%20and%20Yebonn%20Han%20and%20Joonseok%20Lee&entry.1292438233=%20%20Graph%20convolutional%20networks%20have%20recently%20gained%20prominence%20in%20collaborative%0Afiltering%20%28CF%29%20for%20recommendations.%20However%2C%20we%20identify%20potential%20bottlenecks%0Ain%20two%20foundational%20components.%20First%2C%20the%20embedding%20layer%20leads%20to%20a%20latent%0Aspace%20with%20limited%20capacity%2C%20overlooking%20locally%20observed%20but%20potentially%0Avaluable%20preference%20patterns.%20Also%2C%20the%20widely-used%20neighborhood%20aggregation%20is%0Alimited%20in%20its%20ability%20to%20leverage%20diverse%20preference%20patterns%20in%20a%0Afine-grained%20manner.%20Building%20on%20spectral%20graph%20theory%2C%20we%20reveal%20that%20these%0Alimitations%20stem%20from%20graph%20filtering%20with%20a%20cut-off%20in%20the%20frequency%20spectrum%0Aand%20a%20restricted%20linear%20form.%20To%20address%20these%20issues%2C%20we%20introduce%20ChebyCF%2C%20a%0ACF%20framework%20based%20on%20graph%20spectral%20filtering.%20Instead%20of%20a%20learned%20embedding%2C%0Ait%20takes%20a%20user%27s%20raw%20interaction%20history%20to%20utilize%20the%20full%20spectrum%20of%0Asignals%20contained%20in%20it.%20Also%2C%20it%20adopts%20Chebyshev%20interpolation%20to%20effectively%0Aapproximate%20a%20flexible%20non-linear%20graph%20filter%2C%20and%20further%20enhances%20it%20by%0Ausing%20an%20additional%20ideal%20pass%20filter%20and%20degree-based%20normalization.%20Through%0Aextensive%20experiments%2C%20we%20verify%20that%20ChebyCF%20overcomes%20the%20aforementioned%0Abottlenecks%20and%20achieves%20state-of-the-art%20performance%20across%20multiple%0Abenchmarks%20and%20reasonably%20fast%20inference.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/chanwoo0806/ChebyCF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00552v1&entry.124074799=Read"},
{"title": "Explainable AI in Spatial Analysis", "author": "Ziqi Li", "abstract": "  This chapter discusses the opportunities of eXplainable Artificial\nIntelligence (XAI) within the realm of spatial analysis. A key objective in\nspatial analysis is to model spatial relationships and infer spatial processes\nto generate knowledge from spatial data, which has been largely based on\nspatial statistical methods. More recently, machine learning offers scalable\nand flexible approaches that complement traditional methods and has been\nincreasingly applied in spatial data science. Despite its advantages, machine\nlearning is often criticized for being a black box, which limits our\nunderstanding of model behavior and output. Recognizing this limitation, XAI\nhas emerged as a pivotal field in AI that provides methods to explain the\noutput of machine learning models to enhance transparency and understanding.\nThese methods are crucial for model diagnosis, bias detection, and ensuring the\nreliability of results obtained from machine learning models. This chapter\nintroduces key concepts and methods in XAI with a focus on Shapley value-based\napproaches, which is arguably the most popular XAI method, and their\nintegration with spatial analysis. An empirical example of county-level voting\nbehaviors in the 2020 Presidential election is presented to demonstrate the use\nof Shapley values and spatial analysis with a comparison to multi-scale\ngeographically weighted regression. The chapter concludes with a discussion on\nthe challenges and limitations of current XAI techniques and proposes new\ndirections.\n", "link": "http://arxiv.org/abs/2505.00591v1", "date": "2025-05-01", "relevancy": 1.7834, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20in%20Spatial%20Analysis&body=Title%3A%20Explainable%20AI%20in%20Spatial%20Analysis%0AAuthor%3A%20Ziqi%20Li%0AAbstract%3A%20%20%20This%20chapter%20discusses%20the%20opportunities%20of%20eXplainable%20Artificial%0AIntelligence%20%28XAI%29%20within%20the%20realm%20of%20spatial%20analysis.%20A%20key%20objective%20in%0Aspatial%20analysis%20is%20to%20model%20spatial%20relationships%20and%20infer%20spatial%20processes%0Ato%20generate%20knowledge%20from%20spatial%20data%2C%20which%20has%20been%20largely%20based%20on%0Aspatial%20statistical%20methods.%20More%20recently%2C%20machine%20learning%20offers%20scalable%0Aand%20flexible%20approaches%20that%20complement%20traditional%20methods%20and%20has%20been%0Aincreasingly%20applied%20in%20spatial%20data%20science.%20Despite%20its%20advantages%2C%20machine%0Alearning%20is%20often%20criticized%20for%20being%20a%20black%20box%2C%20which%20limits%20our%0Aunderstanding%20of%20model%20behavior%20and%20output.%20Recognizing%20this%20limitation%2C%20XAI%0Ahas%20emerged%20as%20a%20pivotal%20field%20in%20AI%20that%20provides%20methods%20to%20explain%20the%0Aoutput%20of%20machine%20learning%20models%20to%20enhance%20transparency%20and%20understanding.%0AThese%20methods%20are%20crucial%20for%20model%20diagnosis%2C%20bias%20detection%2C%20and%20ensuring%20the%0Areliability%20of%20results%20obtained%20from%20machine%20learning%20models.%20This%20chapter%0Aintroduces%20key%20concepts%20and%20methods%20in%20XAI%20with%20a%20focus%20on%20Shapley%20value-based%0Aapproaches%2C%20which%20is%20arguably%20the%20most%20popular%20XAI%20method%2C%20and%20their%0Aintegration%20with%20spatial%20analysis.%20An%20empirical%20example%20of%20county-level%20voting%0Abehaviors%20in%20the%202020%20Presidential%20election%20is%20presented%20to%20demonstrate%20the%20use%0Aof%20Shapley%20values%20and%20spatial%20analysis%20with%20a%20comparison%20to%20multi-scale%0Ageographically%20weighted%20regression.%20The%20chapter%20concludes%20with%20a%20discussion%20on%0Athe%20challenges%20and%20limitations%20of%20current%20XAI%20techniques%20and%20proposes%20new%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520in%2520Spatial%2520Analysis%26entry.906535625%3DZiqi%2520Li%26entry.1292438233%3D%2520%2520This%2520chapter%2520discusses%2520the%2520opportunities%2520of%2520eXplainable%2520Artificial%250AIntelligence%2520%2528XAI%2529%2520within%2520the%2520realm%2520of%2520spatial%2520analysis.%2520A%2520key%2520objective%2520in%250Aspatial%2520analysis%2520is%2520to%2520model%2520spatial%2520relationships%2520and%2520infer%2520spatial%2520processes%250Ato%2520generate%2520knowledge%2520from%2520spatial%2520data%252C%2520which%2520has%2520been%2520largely%2520based%2520on%250Aspatial%2520statistical%2520methods.%2520More%2520recently%252C%2520machine%2520learning%2520offers%2520scalable%250Aand%2520flexible%2520approaches%2520that%2520complement%2520traditional%2520methods%2520and%2520has%2520been%250Aincreasingly%2520applied%2520in%2520spatial%2520data%2520science.%2520Despite%2520its%2520advantages%252C%2520machine%250Alearning%2520is%2520often%2520criticized%2520for%2520being%2520a%2520black%2520box%252C%2520which%2520limits%2520our%250Aunderstanding%2520of%2520model%2520behavior%2520and%2520output.%2520Recognizing%2520this%2520limitation%252C%2520XAI%250Ahas%2520emerged%2520as%2520a%2520pivotal%2520field%2520in%2520AI%2520that%2520provides%2520methods%2520to%2520explain%2520the%250Aoutput%2520of%2520machine%2520learning%2520models%2520to%2520enhance%2520transparency%2520and%2520understanding.%250AThese%2520methods%2520are%2520crucial%2520for%2520model%2520diagnosis%252C%2520bias%2520detection%252C%2520and%2520ensuring%2520the%250Areliability%2520of%2520results%2520obtained%2520from%2520machine%2520learning%2520models.%2520This%2520chapter%250Aintroduces%2520key%2520concepts%2520and%2520methods%2520in%2520XAI%2520with%2520a%2520focus%2520on%2520Shapley%2520value-based%250Aapproaches%252C%2520which%2520is%2520arguably%2520the%2520most%2520popular%2520XAI%2520method%252C%2520and%2520their%250Aintegration%2520with%2520spatial%2520analysis.%2520An%2520empirical%2520example%2520of%2520county-level%2520voting%250Abehaviors%2520in%2520the%25202020%2520Presidential%2520election%2520is%2520presented%2520to%2520demonstrate%2520the%2520use%250Aof%2520Shapley%2520values%2520and%2520spatial%2520analysis%2520with%2520a%2520comparison%2520to%2520multi-scale%250Ageographically%2520weighted%2520regression.%2520The%2520chapter%2520concludes%2520with%2520a%2520discussion%2520on%250Athe%2520challenges%2520and%2520limitations%2520of%2520current%2520XAI%2520techniques%2520and%2520proposes%2520new%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20in%20Spatial%20Analysis&entry.906535625=Ziqi%20Li&entry.1292438233=%20%20This%20chapter%20discusses%20the%20opportunities%20of%20eXplainable%20Artificial%0AIntelligence%20%28XAI%29%20within%20the%20realm%20of%20spatial%20analysis.%20A%20key%20objective%20in%0Aspatial%20analysis%20is%20to%20model%20spatial%20relationships%20and%20infer%20spatial%20processes%0Ato%20generate%20knowledge%20from%20spatial%20data%2C%20which%20has%20been%20largely%20based%20on%0Aspatial%20statistical%20methods.%20More%20recently%2C%20machine%20learning%20offers%20scalable%0Aand%20flexible%20approaches%20that%20complement%20traditional%20methods%20and%20has%20been%0Aincreasingly%20applied%20in%20spatial%20data%20science.%20Despite%20its%20advantages%2C%20machine%0Alearning%20is%20often%20criticized%20for%20being%20a%20black%20box%2C%20which%20limits%20our%0Aunderstanding%20of%20model%20behavior%20and%20output.%20Recognizing%20this%20limitation%2C%20XAI%0Ahas%20emerged%20as%20a%20pivotal%20field%20in%20AI%20that%20provides%20methods%20to%20explain%20the%0Aoutput%20of%20machine%20learning%20models%20to%20enhance%20transparency%20and%20understanding.%0AThese%20methods%20are%20crucial%20for%20model%20diagnosis%2C%20bias%20detection%2C%20and%20ensuring%20the%0Areliability%20of%20results%20obtained%20from%20machine%20learning%20models.%20This%20chapter%0Aintroduces%20key%20concepts%20and%20methods%20in%20XAI%20with%20a%20focus%20on%20Shapley%20value-based%0Aapproaches%2C%20which%20is%20arguably%20the%20most%20popular%20XAI%20method%2C%20and%20their%0Aintegration%20with%20spatial%20analysis.%20An%20empirical%20example%20of%20county-level%20voting%0Abehaviors%20in%20the%202020%20Presidential%20election%20is%20presented%20to%20demonstrate%20the%20use%0Aof%20Shapley%20values%20and%20spatial%20analysis%20with%20a%20comparison%20to%20multi-scale%0Ageographically%20weighted%20regression.%20The%20chapter%20concludes%20with%20a%20discussion%20on%0Athe%20challenges%20and%20limitations%20of%20current%20XAI%20techniques%20and%20proposes%20new%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00591v1&entry.124074799=Read"},
{"title": "Multi-Objective Reinforcement Learning for Power Grid Topology Control", "author": "Thomas Lautenbacher and Ali Rajaei and Davide Barbieri and Jan Viebahn and Jochen L. Cremer", "abstract": "  Transmission grid congestion increases as the electrification of various\nsectors requires transmitting more power. Topology control, through substation\nreconfiguration, can reduce congestion but its potential remains\nunder-exploited in operations. A challenge is modeling the topology control\nproblem to align well with the objectives and constraints of operators.\nAddressing this challenge, this paper investigates the application of\nmulti-objective reinforcement learning (MORL) to integrate multiple conflicting\nobjectives for power grid topology control. We develop a MORL approach using\ndeep optimistic linear support (DOL) and multi-objective proximal policy\noptimization (MOPPO) to generate a set of Pareto-optimal policies that balance\nobjectives such as minimizing line loading, topological deviation, and\nswitching frequency. Initial case studies show that the MORL approach can\nprovide valuable insights into objective trade-offs and improve Pareto front\napproximation compared to a random search baseline. The generated\nmulti-objective RL policies are 30% more successful in preventing grid failure\nunder contingencies and 20% more effective when training budget is reduced -\ncompared to the common single objective RL policy.\n", "link": "http://arxiv.org/abs/2502.00040v2", "date": "2025-05-01", "relevancy": 1.4434, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5046}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4681}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Objective%20Reinforcement%20Learning%20for%20Power%20Grid%20Topology%20Control&body=Title%3A%20Multi-Objective%20Reinforcement%20Learning%20for%20Power%20Grid%20Topology%20Control%0AAuthor%3A%20Thomas%20Lautenbacher%20and%20Ali%20Rajaei%20and%20Davide%20Barbieri%20and%20Jan%20Viebahn%20and%20Jochen%20L.%20Cremer%0AAbstract%3A%20%20%20Transmission%20grid%20congestion%20increases%20as%20the%20electrification%20of%20various%0Asectors%20requires%20transmitting%20more%20power.%20Topology%20control%2C%20through%20substation%0Areconfiguration%2C%20can%20reduce%20congestion%20but%20its%20potential%20remains%0Aunder-exploited%20in%20operations.%20A%20challenge%20is%20modeling%20the%20topology%20control%0Aproblem%20to%20align%20well%20with%20the%20objectives%20and%20constraints%20of%20operators.%0AAddressing%20this%20challenge%2C%20this%20paper%20investigates%20the%20application%20of%0Amulti-objective%20reinforcement%20learning%20%28MORL%29%20to%20integrate%20multiple%20conflicting%0Aobjectives%20for%20power%20grid%20topology%20control.%20We%20develop%20a%20MORL%20approach%20using%0Adeep%20optimistic%20linear%20support%20%28DOL%29%20and%20multi-objective%20proximal%20policy%0Aoptimization%20%28MOPPO%29%20to%20generate%20a%20set%20of%20Pareto-optimal%20policies%20that%20balance%0Aobjectives%20such%20as%20minimizing%20line%20loading%2C%20topological%20deviation%2C%20and%0Aswitching%20frequency.%20Initial%20case%20studies%20show%20that%20the%20MORL%20approach%20can%0Aprovide%20valuable%20insights%20into%20objective%20trade-offs%20and%20improve%20Pareto%20front%0Aapproximation%20compared%20to%20a%20random%20search%20baseline.%20The%20generated%0Amulti-objective%20RL%20policies%20are%2030%25%20more%20successful%20in%20preventing%20grid%20failure%0Aunder%20contingencies%20and%2020%25%20more%20effective%20when%20training%20budget%20is%20reduced%20-%0Acompared%20to%20the%20common%20single%20objective%20RL%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Objective%2520Reinforcement%2520Learning%2520for%2520Power%2520Grid%2520Topology%2520Control%26entry.906535625%3DThomas%2520Lautenbacher%2520and%2520Ali%2520Rajaei%2520and%2520Davide%2520Barbieri%2520and%2520Jan%2520Viebahn%2520and%2520Jochen%2520L.%2520Cremer%26entry.1292438233%3D%2520%2520Transmission%2520grid%2520congestion%2520increases%2520as%2520the%2520electrification%2520of%2520various%250Asectors%2520requires%2520transmitting%2520more%2520power.%2520Topology%2520control%252C%2520through%2520substation%250Areconfiguration%252C%2520can%2520reduce%2520congestion%2520but%2520its%2520potential%2520remains%250Aunder-exploited%2520in%2520operations.%2520A%2520challenge%2520is%2520modeling%2520the%2520topology%2520control%250Aproblem%2520to%2520align%2520well%2520with%2520the%2520objectives%2520and%2520constraints%2520of%2520operators.%250AAddressing%2520this%2520challenge%252C%2520this%2520paper%2520investigates%2520the%2520application%2520of%250Amulti-objective%2520reinforcement%2520learning%2520%2528MORL%2529%2520to%2520integrate%2520multiple%2520conflicting%250Aobjectives%2520for%2520power%2520grid%2520topology%2520control.%2520We%2520develop%2520a%2520MORL%2520approach%2520using%250Adeep%2520optimistic%2520linear%2520support%2520%2528DOL%2529%2520and%2520multi-objective%2520proximal%2520policy%250Aoptimization%2520%2528MOPPO%2529%2520to%2520generate%2520a%2520set%2520of%2520Pareto-optimal%2520policies%2520that%2520balance%250Aobjectives%2520such%2520as%2520minimizing%2520line%2520loading%252C%2520topological%2520deviation%252C%2520and%250Aswitching%2520frequency.%2520Initial%2520case%2520studies%2520show%2520that%2520the%2520MORL%2520approach%2520can%250Aprovide%2520valuable%2520insights%2520into%2520objective%2520trade-offs%2520and%2520improve%2520Pareto%2520front%250Aapproximation%2520compared%2520to%2520a%2520random%2520search%2520baseline.%2520The%2520generated%250Amulti-objective%2520RL%2520policies%2520are%252030%2525%2520more%2520successful%2520in%2520preventing%2520grid%2520failure%250Aunder%2520contingencies%2520and%252020%2525%2520more%2520effective%2520when%2520training%2520budget%2520is%2520reduced%2520-%250Acompared%2520to%2520the%2520common%2520single%2520objective%2520RL%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Objective%20Reinforcement%20Learning%20for%20Power%20Grid%20Topology%20Control&entry.906535625=Thomas%20Lautenbacher%20and%20Ali%20Rajaei%20and%20Davide%20Barbieri%20and%20Jan%20Viebahn%20and%20Jochen%20L.%20Cremer&entry.1292438233=%20%20Transmission%20grid%20congestion%20increases%20as%20the%20electrification%20of%20various%0Asectors%20requires%20transmitting%20more%20power.%20Topology%20control%2C%20through%20substation%0Areconfiguration%2C%20can%20reduce%20congestion%20but%20its%20potential%20remains%0Aunder-exploited%20in%20operations.%20A%20challenge%20is%20modeling%20the%20topology%20control%0Aproblem%20to%20align%20well%20with%20the%20objectives%20and%20constraints%20of%20operators.%0AAddressing%20this%20challenge%2C%20this%20paper%20investigates%20the%20application%20of%0Amulti-objective%20reinforcement%20learning%20%28MORL%29%20to%20integrate%20multiple%20conflicting%0Aobjectives%20for%20power%20grid%20topology%20control.%20We%20develop%20a%20MORL%20approach%20using%0Adeep%20optimistic%20linear%20support%20%28DOL%29%20and%20multi-objective%20proximal%20policy%0Aoptimization%20%28MOPPO%29%20to%20generate%20a%20set%20of%20Pareto-optimal%20policies%20that%20balance%0Aobjectives%20such%20as%20minimizing%20line%20loading%2C%20topological%20deviation%2C%20and%0Aswitching%20frequency.%20Initial%20case%20studies%20show%20that%20the%20MORL%20approach%20can%0Aprovide%20valuable%20insights%20into%20objective%20trade-offs%20and%20improve%20Pareto%20front%0Aapproximation%20compared%20to%20a%20random%20search%20baseline.%20The%20generated%0Amulti-objective%20RL%20policies%20are%2030%25%20more%20successful%20in%20preventing%20grid%20failure%0Aunder%20contingencies%20and%2020%25%20more%20effective%20when%20training%20budget%20is%20reduced%20-%0Acompared%20to%20the%20common%20single%20objective%20RL%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00040v2&entry.124074799=Read"},
{"title": "MULE: Multi-terrain and Unknown Load Adaptation for Effective\n  Quadrupedal Locomotion", "author": "Vamshi Kumar Kurva and Shishir Kolathaya", "abstract": "  Quadrupedal robots are increasingly deployed for load-carrying tasks across\ndiverse terrains. While Model Predictive Control (MPC)-based methods can\naccount for payload variations, they often depend on predefined gait schedules\nor trajectory generators, limiting their adaptability in unstructured\nenvironments. To address these limitations, we propose an Adaptive\nReinforcement Learning (RL) framework that enables quadrupedal robots to\ndynamically adapt to both varying payloads and diverse terrains. The framework\nconsists of a nominal policy responsible for baseline locomotion and an\nadaptive policy that learns corrective actions to preserve stability and\nimprove command tracking under payload variations. We validate the proposed\napproach through large-scale simulation experiments in Isaac Gym and real-world\nhardware deployment on a Unitree Go1 quadruped. The controller was tested on\nflat ground, slopes, and stairs under both static and dynamic payload changes.\nAcross all settings, our adaptive controller consistently outperformed the\ncontroller in tracking body height and velocity commands, demonstrating\nenhanced robustness and adaptability without requiring explicit gait design or\nmanual tuning.\n", "link": "http://arxiv.org/abs/2505.00488v1", "date": "2025-05-01", "relevancy": 1.6735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5586}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MULE%3A%20Multi-terrain%20and%20Unknown%20Load%20Adaptation%20for%20Effective%0A%20%20Quadrupedal%20Locomotion&body=Title%3A%20MULE%3A%20Multi-terrain%20and%20Unknown%20Load%20Adaptation%20for%20Effective%0A%20%20Quadrupedal%20Locomotion%0AAuthor%3A%20Vamshi%20Kumar%20Kurva%20and%20Shishir%20Kolathaya%0AAbstract%3A%20%20%20Quadrupedal%20robots%20are%20increasingly%20deployed%20for%20load-carrying%20tasks%20across%0Adiverse%20terrains.%20While%20Model%20Predictive%20Control%20%28MPC%29-based%20methods%20can%0Aaccount%20for%20payload%20variations%2C%20they%20often%20depend%20on%20predefined%20gait%20schedules%0Aor%20trajectory%20generators%2C%20limiting%20their%20adaptability%20in%20unstructured%0Aenvironments.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20Adaptive%0AReinforcement%20Learning%20%28RL%29%20framework%20that%20enables%20quadrupedal%20robots%20to%0Adynamically%20adapt%20to%20both%20varying%20payloads%20and%20diverse%20terrains.%20The%20framework%0Aconsists%20of%20a%20nominal%20policy%20responsible%20for%20baseline%20locomotion%20and%20an%0Aadaptive%20policy%20that%20learns%20corrective%20actions%20to%20preserve%20stability%20and%0Aimprove%20command%20tracking%20under%20payload%20variations.%20We%20validate%20the%20proposed%0Aapproach%20through%20large-scale%20simulation%20experiments%20in%20Isaac%20Gym%20and%20real-world%0Ahardware%20deployment%20on%20a%20Unitree%20Go1%20quadruped.%20The%20controller%20was%20tested%20on%0Aflat%20ground%2C%20slopes%2C%20and%20stairs%20under%20both%20static%20and%20dynamic%20payload%20changes.%0AAcross%20all%20settings%2C%20our%20adaptive%20controller%20consistently%20outperformed%20the%0Acontroller%20in%20tracking%20body%20height%20and%20velocity%20commands%2C%20demonstrating%0Aenhanced%20robustness%20and%20adaptability%20without%20requiring%20explicit%20gait%20design%20or%0Amanual%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMULE%253A%2520Multi-terrain%2520and%2520Unknown%2520Load%2520Adaptation%2520for%2520Effective%250A%2520%2520Quadrupedal%2520Locomotion%26entry.906535625%3DVamshi%2520Kumar%2520Kurva%2520and%2520Shishir%2520Kolathaya%26entry.1292438233%3D%2520%2520Quadrupedal%2520robots%2520are%2520increasingly%2520deployed%2520for%2520load-carrying%2520tasks%2520across%250Adiverse%2520terrains.%2520While%2520Model%2520Predictive%2520Control%2520%2528MPC%2529-based%2520methods%2520can%250Aaccount%2520for%2520payload%2520variations%252C%2520they%2520often%2520depend%2520on%2520predefined%2520gait%2520schedules%250Aor%2520trajectory%2520generators%252C%2520limiting%2520their%2520adaptability%2520in%2520unstructured%250Aenvironments.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520Adaptive%250AReinforcement%2520Learning%2520%2528RL%2529%2520framework%2520that%2520enables%2520quadrupedal%2520robots%2520to%250Adynamically%2520adapt%2520to%2520both%2520varying%2520payloads%2520and%2520diverse%2520terrains.%2520The%2520framework%250Aconsists%2520of%2520a%2520nominal%2520policy%2520responsible%2520for%2520baseline%2520locomotion%2520and%2520an%250Aadaptive%2520policy%2520that%2520learns%2520corrective%2520actions%2520to%2520preserve%2520stability%2520and%250Aimprove%2520command%2520tracking%2520under%2520payload%2520variations.%2520We%2520validate%2520the%2520proposed%250Aapproach%2520through%2520large-scale%2520simulation%2520experiments%2520in%2520Isaac%2520Gym%2520and%2520real-world%250Ahardware%2520deployment%2520on%2520a%2520Unitree%2520Go1%2520quadruped.%2520The%2520controller%2520was%2520tested%2520on%250Aflat%2520ground%252C%2520slopes%252C%2520and%2520stairs%2520under%2520both%2520static%2520and%2520dynamic%2520payload%2520changes.%250AAcross%2520all%2520settings%252C%2520our%2520adaptive%2520controller%2520consistently%2520outperformed%2520the%250Acontroller%2520in%2520tracking%2520body%2520height%2520and%2520velocity%2520commands%252C%2520demonstrating%250Aenhanced%2520robustness%2520and%2520adaptability%2520without%2520requiring%2520explicit%2520gait%2520design%2520or%250Amanual%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULE%3A%20Multi-terrain%20and%20Unknown%20Load%20Adaptation%20for%20Effective%0A%20%20Quadrupedal%20Locomotion&entry.906535625=Vamshi%20Kumar%20Kurva%20and%20Shishir%20Kolathaya&entry.1292438233=%20%20Quadrupedal%20robots%20are%20increasingly%20deployed%20for%20load-carrying%20tasks%20across%0Adiverse%20terrains.%20While%20Model%20Predictive%20Control%20%28MPC%29-based%20methods%20can%0Aaccount%20for%20payload%20variations%2C%20they%20often%20depend%20on%20predefined%20gait%20schedules%0Aor%20trajectory%20generators%2C%20limiting%20their%20adaptability%20in%20unstructured%0Aenvironments.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20Adaptive%0AReinforcement%20Learning%20%28RL%29%20framework%20that%20enables%20quadrupedal%20robots%20to%0Adynamically%20adapt%20to%20both%20varying%20payloads%20and%20diverse%20terrains.%20The%20framework%0Aconsists%20of%20a%20nominal%20policy%20responsible%20for%20baseline%20locomotion%20and%20an%0Aadaptive%20policy%20that%20learns%20corrective%20actions%20to%20preserve%20stability%20and%0Aimprove%20command%20tracking%20under%20payload%20variations.%20We%20validate%20the%20proposed%0Aapproach%20through%20large-scale%20simulation%20experiments%20in%20Isaac%20Gym%20and%20real-world%0Ahardware%20deployment%20on%20a%20Unitree%20Go1%20quadruped.%20The%20controller%20was%20tested%20on%0Aflat%20ground%2C%20slopes%2C%20and%20stairs%20under%20both%20static%20and%20dynamic%20payload%20changes.%0AAcross%20all%20settings%2C%20our%20adaptive%20controller%20consistently%20outperformed%20the%0Acontroller%20in%20tracking%20body%20height%20and%20velocity%20commands%2C%20demonstrating%0Aenhanced%20robustness%20and%20adaptability%20without%20requiring%20explicit%20gait%20design%20or%0Amanual%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00488v1&entry.124074799=Read"},
{"title": "Towards Scalable Human-aligned Benchmark for Text-guided Image Editing", "author": "Suho Ryu and Kihyun Kim and Eugene Baek and Dongsoo Shin and Joonseok Lee", "abstract": "  A variety of text-guided image editing models have been proposed recently.\nHowever, there is no widely-accepted standard evaluation method mainly due to\nthe subjective nature of the task, letting researchers rely on manual user\nstudy. To address this, we introduce a novel Human-Aligned benchmark for\nText-guided Image Editing (HATIE). Providing a large-scale benchmark set\ncovering a wide range of editing tasks, it allows reliable evaluation, not\nlimited to specific easy-to-evaluate cases. Also, HATIE provides a\nfully-automated and omnidirectional evaluation pipeline. Particularly, we\ncombine multiple scores measuring various aspects of editing so as to align\nwith human perception. We empirically verify that the evaluation of HATIE is\nindeed human-aligned in various aspects, and provide benchmark results on\nseveral state-of-the-art models to provide deeper insights on their\nperformance.\n", "link": "http://arxiv.org/abs/2505.00502v1", "date": "2025-05-01", "relevancy": 1.5529, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5453}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20Human-aligned%20Benchmark%20for%20Text-guided%20Image%20Editing&body=Title%3A%20Towards%20Scalable%20Human-aligned%20Benchmark%20for%20Text-guided%20Image%20Editing%0AAuthor%3A%20Suho%20Ryu%20and%20Kihyun%20Kim%20and%20Eugene%20Baek%20and%20Dongsoo%20Shin%20and%20Joonseok%20Lee%0AAbstract%3A%20%20%20A%20variety%20of%20text-guided%20image%20editing%20models%20have%20been%20proposed%20recently.%0AHowever%2C%20there%20is%20no%20widely-accepted%20standard%20evaluation%20method%20mainly%20due%20to%0Athe%20subjective%20nature%20of%20the%20task%2C%20letting%20researchers%20rely%20on%20manual%20user%0Astudy.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20Human-Aligned%20benchmark%20for%0AText-guided%20Image%20Editing%20%28HATIE%29.%20Providing%20a%20large-scale%20benchmark%20set%0Acovering%20a%20wide%20range%20of%20editing%20tasks%2C%20it%20allows%20reliable%20evaluation%2C%20not%0Alimited%20to%20specific%20easy-to-evaluate%20cases.%20Also%2C%20HATIE%20provides%20a%0Afully-automated%20and%20omnidirectional%20evaluation%20pipeline.%20Particularly%2C%20we%0Acombine%20multiple%20scores%20measuring%20various%20aspects%20of%20editing%20so%20as%20to%20align%0Awith%20human%20perception.%20We%20empirically%20verify%20that%20the%20evaluation%20of%20HATIE%20is%0Aindeed%20human-aligned%20in%20various%20aspects%2C%20and%20provide%20benchmark%20results%20on%0Aseveral%20state-of-the-art%20models%20to%20provide%20deeper%20insights%20on%20their%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520Human-aligned%2520Benchmark%2520for%2520Text-guided%2520Image%2520Editing%26entry.906535625%3DSuho%2520Ryu%2520and%2520Kihyun%2520Kim%2520and%2520Eugene%2520Baek%2520and%2520Dongsoo%2520Shin%2520and%2520Joonseok%2520Lee%26entry.1292438233%3D%2520%2520A%2520variety%2520of%2520text-guided%2520image%2520editing%2520models%2520have%2520been%2520proposed%2520recently.%250AHowever%252C%2520there%2520is%2520no%2520widely-accepted%2520standard%2520evaluation%2520method%2520mainly%2520due%2520to%250Athe%2520subjective%2520nature%2520of%2520the%2520task%252C%2520letting%2520researchers%2520rely%2520on%2520manual%2520user%250Astudy.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520Human-Aligned%2520benchmark%2520for%250AText-guided%2520Image%2520Editing%2520%2528HATIE%2529.%2520Providing%2520a%2520large-scale%2520benchmark%2520set%250Acovering%2520a%2520wide%2520range%2520of%2520editing%2520tasks%252C%2520it%2520allows%2520reliable%2520evaluation%252C%2520not%250Alimited%2520to%2520specific%2520easy-to-evaluate%2520cases.%2520Also%252C%2520HATIE%2520provides%2520a%250Afully-automated%2520and%2520omnidirectional%2520evaluation%2520pipeline.%2520Particularly%252C%2520we%250Acombine%2520multiple%2520scores%2520measuring%2520various%2520aspects%2520of%2520editing%2520so%2520as%2520to%2520align%250Awith%2520human%2520perception.%2520We%2520empirically%2520verify%2520that%2520the%2520evaluation%2520of%2520HATIE%2520is%250Aindeed%2520human-aligned%2520in%2520various%2520aspects%252C%2520and%2520provide%2520benchmark%2520results%2520on%250Aseveral%2520state-of-the-art%2520models%2520to%2520provide%2520deeper%2520insights%2520on%2520their%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20Human-aligned%20Benchmark%20for%20Text-guided%20Image%20Editing&entry.906535625=Suho%20Ryu%20and%20Kihyun%20Kim%20and%20Eugene%20Baek%20and%20Dongsoo%20Shin%20and%20Joonseok%20Lee&entry.1292438233=%20%20A%20variety%20of%20text-guided%20image%20editing%20models%20have%20been%20proposed%20recently.%0AHowever%2C%20there%20is%20no%20widely-accepted%20standard%20evaluation%20method%20mainly%20due%20to%0Athe%20subjective%20nature%20of%20the%20task%2C%20letting%20researchers%20rely%20on%20manual%20user%0Astudy.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20Human-Aligned%20benchmark%20for%0AText-guided%20Image%20Editing%20%28HATIE%29.%20Providing%20a%20large-scale%20benchmark%20set%0Acovering%20a%20wide%20range%20of%20editing%20tasks%2C%20it%20allows%20reliable%20evaluation%2C%20not%0Alimited%20to%20specific%20easy-to-evaluate%20cases.%20Also%2C%20HATIE%20provides%20a%0Afully-automated%20and%20omnidirectional%20evaluation%20pipeline.%20Particularly%2C%20we%0Acombine%20multiple%20scores%20measuring%20various%20aspects%20of%20editing%20so%20as%20to%20align%0Awith%20human%20perception.%20We%20empirically%20verify%20that%20the%20evaluation%20of%20HATIE%20is%0Aindeed%20human-aligned%20in%20various%20aspects%2C%20and%20provide%20benchmark%20results%20on%0Aseveral%20state-of-the-art%20models%20to%20provide%20deeper%20insights%20on%20their%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00502v1&entry.124074799=Read"},
{"title": "Gaussian Mixture Flow Matching Models", "author": "Hansheng Chen and Kai Zhang and Hao Tan and Zexiang Xu and Fujun Luan and Leonidas Guibas and Gordon Wetzstein and Sai Bi", "abstract": "  Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an $L_2$ denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256$\\times$256.\n", "link": "http://arxiv.org/abs/2504.05304v2", "date": "2025-05-01", "relevancy": 1.677, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6107}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.553}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Mixture%20Flow%20Matching%20Models&body=Title%3A%20Gaussian%20Mixture%20Flow%20Matching%20Models%0AAuthor%3A%20Hansheng%20Chen%20and%20Kai%20Zhang%20and%20Hao%20Tan%20and%20Zexiang%20Xu%20and%20Fujun%20Luan%20and%20Leonidas%20Guibas%20and%20Gordon%20Wetzstein%20and%20Sai%20Bi%0AAbstract%3A%20%20%20Diffusion%20models%20approximate%20the%20denoising%20distribution%20as%20a%20Gaussian%20and%0Apredict%20its%20mean%2C%20whereas%20flow%20matching%20models%20reparameterize%20the%20Gaussian%20mean%0Aas%20flow%20velocity.%20However%2C%20they%20underperform%20in%20few-step%20sampling%20due%20to%0Adiscretization%20error%20and%20tend%20to%20produce%20over-saturated%20colors%20under%0Aclassifier-free%20guidance%20%28CFG%29.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anovel%20Gaussian%20mixture%20flow%20matching%20%28GMFlow%29%20model%3A%20instead%20of%20predicting%20the%0Amean%2C%20GMFlow%20predicts%20dynamic%20Gaussian%20mixture%20%28GM%29%20parameters%20to%20capture%20a%0Amulti-modal%20flow%20velocity%20distribution%2C%20which%20can%20be%20learned%20with%20a%20KL%0Adivergence%20loss.%20We%20demonstrate%20that%20GMFlow%20generalizes%20previous%20diffusion%20and%0Aflow%20matching%20models%20where%20a%20single%20Gaussian%20is%20learned%20with%20an%20%24L_2%24%20denoising%0Aloss.%20For%20inference%2C%20we%20derive%20GM-SDE/ODE%20solvers%20that%20leverage%20analytic%0Adenoising%20distributions%20and%20velocity%20fields%20for%20precise%20few-step%20sampling.%0AFurthermore%2C%20we%20introduce%20a%20novel%20probabilistic%20guidance%20scheme%20that%20mitigates%0Athe%20over-saturation%20issues%20of%20CFG%20and%20improves%20image%20generation%20quality.%0AExtensive%20experiments%20demonstrate%20that%20GMFlow%20consistently%20outperforms%20flow%0Amatching%20baselines%20in%20generation%20quality%2C%20achieving%20a%20Precision%20of%200.942%20with%0Aonly%206%20sampling%20steps%20on%20ImageNet%20256%24%5Ctimes%24256.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Mixture%2520Flow%2520Matching%2520Models%26entry.906535625%3DHansheng%2520Chen%2520and%2520Kai%2520Zhang%2520and%2520Hao%2520Tan%2520and%2520Zexiang%2520Xu%2520and%2520Fujun%2520Luan%2520and%2520Leonidas%2520Guibas%2520and%2520Gordon%2520Wetzstein%2520and%2520Sai%2520Bi%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520approximate%2520the%2520denoising%2520distribution%2520as%2520a%2520Gaussian%2520and%250Apredict%2520its%2520mean%252C%2520whereas%2520flow%2520matching%2520models%2520reparameterize%2520the%2520Gaussian%2520mean%250Aas%2520flow%2520velocity.%2520However%252C%2520they%2520underperform%2520in%2520few-step%2520sampling%2520due%2520to%250Adiscretization%2520error%2520and%2520tend%2520to%2520produce%2520over-saturated%2520colors%2520under%250Aclassifier-free%2520guidance%2520%2528CFG%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520Gaussian%2520mixture%2520flow%2520matching%2520%2528GMFlow%2529%2520model%253A%2520instead%2520of%2520predicting%2520the%250Amean%252C%2520GMFlow%2520predicts%2520dynamic%2520Gaussian%2520mixture%2520%2528GM%2529%2520parameters%2520to%2520capture%2520a%250Amulti-modal%2520flow%2520velocity%2520distribution%252C%2520which%2520can%2520be%2520learned%2520with%2520a%2520KL%250Adivergence%2520loss.%2520We%2520demonstrate%2520that%2520GMFlow%2520generalizes%2520previous%2520diffusion%2520and%250Aflow%2520matching%2520models%2520where%2520a%2520single%2520Gaussian%2520is%2520learned%2520with%2520an%2520%2524L_2%2524%2520denoising%250Aloss.%2520For%2520inference%252C%2520we%2520derive%2520GM-SDE/ODE%2520solvers%2520that%2520leverage%2520analytic%250Adenoising%2520distributions%2520and%2520velocity%2520fields%2520for%2520precise%2520few-step%2520sampling.%250AFurthermore%252C%2520we%2520introduce%2520a%2520novel%2520probabilistic%2520guidance%2520scheme%2520that%2520mitigates%250Athe%2520over-saturation%2520issues%2520of%2520CFG%2520and%2520improves%2520image%2520generation%2520quality.%250AExtensive%2520experiments%2520demonstrate%2520that%2520GMFlow%2520consistently%2520outperforms%2520flow%250Amatching%2520baselines%2520in%2520generation%2520quality%252C%2520achieving%2520a%2520Precision%2520of%25200.942%2520with%250Aonly%25206%2520sampling%2520steps%2520on%2520ImageNet%2520256%2524%255Ctimes%2524256.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Mixture%20Flow%20Matching%20Models&entry.906535625=Hansheng%20Chen%20and%20Kai%20Zhang%20and%20Hao%20Tan%20and%20Zexiang%20Xu%20and%20Fujun%20Luan%20and%20Leonidas%20Guibas%20and%20Gordon%20Wetzstein%20and%20Sai%20Bi&entry.1292438233=%20%20Diffusion%20models%20approximate%20the%20denoising%20distribution%20as%20a%20Gaussian%20and%0Apredict%20its%20mean%2C%20whereas%20flow%20matching%20models%20reparameterize%20the%20Gaussian%20mean%0Aas%20flow%20velocity.%20However%2C%20they%20underperform%20in%20few-step%20sampling%20due%20to%0Adiscretization%20error%20and%20tend%20to%20produce%20over-saturated%20colors%20under%0Aclassifier-free%20guidance%20%28CFG%29.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anovel%20Gaussian%20mixture%20flow%20matching%20%28GMFlow%29%20model%3A%20instead%20of%20predicting%20the%0Amean%2C%20GMFlow%20predicts%20dynamic%20Gaussian%20mixture%20%28GM%29%20parameters%20to%20capture%20a%0Amulti-modal%20flow%20velocity%20distribution%2C%20which%20can%20be%20learned%20with%20a%20KL%0Adivergence%20loss.%20We%20demonstrate%20that%20GMFlow%20generalizes%20previous%20diffusion%20and%0Aflow%20matching%20models%20where%20a%20single%20Gaussian%20is%20learned%20with%20an%20%24L_2%24%20denoising%0Aloss.%20For%20inference%2C%20we%20derive%20GM-SDE/ODE%20solvers%20that%20leverage%20analytic%0Adenoising%20distributions%20and%20velocity%20fields%20for%20precise%20few-step%20sampling.%0AFurthermore%2C%20we%20introduce%20a%20novel%20probabilistic%20guidance%20scheme%20that%20mitigates%0Athe%20over-saturation%20issues%20of%20CFG%20and%20improves%20image%20generation%20quality.%0AExtensive%20experiments%20demonstrate%20that%20GMFlow%20consistently%20outperforms%20flow%0Amatching%20baselines%20in%20generation%20quality%2C%20achieving%20a%20Precision%20of%200.942%20with%0Aonly%206%20sampling%20steps%20on%20ImageNet%20256%24%5Ctimes%24256.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05304v2&entry.124074799=Read"},
{"title": "Subspace-Distance-Enabled Active Learning for Efficient Data-Driven\n  Model Reduction of Parametric Dynamical Systems", "author": "Harshit Kapadia and Peter Benner and Lihong Feng", "abstract": "  In situations where the solution of a high-fidelity dynamical system needs to\nbe evaluated repeatedly, over a vast pool of parametric configurations and in\nabsence of access to the underlying governing equations, data-driven model\nreduction techniques are preferable. We propose a novel active learning\napproach to build a parametric data-driven reduced-order model (ROM) by\ngreedily picking the most important parameter samples from the parameter\ndomain. As a result, during the ROM construction phase, the number of\nhigh-fidelity solutions dynamically grow in a principled fashion. The\nhigh-fidelity solution snapshots are expressed in several parameter-specific\nlinear subspaces, with the help of proper orthogonal decomposition (POD), and\nthe relative distance between these subspaces is used as a guiding mechanism to\nperform active learning. For successfully achieving this, we provide a distance\nmeasure to evaluate the similarity between pairs of linear subspaces with\ndifferent dimensions, and also show that this distance measure is a metric. The\nusability of the proposed subspace-distance-enabled active learning (SDE-AL)\nframework is demonstrated by augmenting two existing non-intrusive\nreduced-order modeling approaches, and providing their active-learning-driven\n(ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN.\nFurthermore, we report positive results for two parametric physical models,\nhighlighting the efficiency of the proposed SDE-AL approach.\n", "link": "http://arxiv.org/abs/2505.00460v1", "date": "2025-05-01", "relevancy": 1.5251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subspace-Distance-Enabled%20Active%20Learning%20for%20Efficient%20Data-Driven%0A%20%20Model%20Reduction%20of%20Parametric%20Dynamical%20Systems&body=Title%3A%20Subspace-Distance-Enabled%20Active%20Learning%20for%20Efficient%20Data-Driven%0A%20%20Model%20Reduction%20of%20Parametric%20Dynamical%20Systems%0AAuthor%3A%20Harshit%20Kapadia%20and%20Peter%20Benner%20and%20Lihong%20Feng%0AAbstract%3A%20%20%20In%20situations%20where%20the%20solution%20of%20a%20high-fidelity%20dynamical%20system%20needs%20to%0Abe%20evaluated%20repeatedly%2C%20over%20a%20vast%20pool%20of%20parametric%20configurations%20and%20in%0Aabsence%20of%20access%20to%20the%20underlying%20governing%20equations%2C%20data-driven%20model%0Areduction%20techniques%20are%20preferable.%20We%20propose%20a%20novel%20active%20learning%0Aapproach%20to%20build%20a%20parametric%20data-driven%20reduced-order%20model%20%28ROM%29%20by%0Agreedily%20picking%20the%20most%20important%20parameter%20samples%20from%20the%20parameter%0Adomain.%20As%20a%20result%2C%20during%20the%20ROM%20construction%20phase%2C%20the%20number%20of%0Ahigh-fidelity%20solutions%20dynamically%20grow%20in%20a%20principled%20fashion.%20The%0Ahigh-fidelity%20solution%20snapshots%20are%20expressed%20in%20several%20parameter-specific%0Alinear%20subspaces%2C%20with%20the%20help%20of%20proper%20orthogonal%20decomposition%20%28POD%29%2C%20and%0Athe%20relative%20distance%20between%20these%20subspaces%20is%20used%20as%20a%20guiding%20mechanism%20to%0Aperform%20active%20learning.%20For%20successfully%20achieving%20this%2C%20we%20provide%20a%20distance%0Ameasure%20to%20evaluate%20the%20similarity%20between%20pairs%20of%20linear%20subspaces%20with%0Adifferent%20dimensions%2C%20and%20also%20show%20that%20this%20distance%20measure%20is%20a%20metric.%20The%0Ausability%20of%20the%20proposed%20subspace-distance-enabled%20active%20learning%20%28SDE-AL%29%0Aframework%20is%20demonstrated%20by%20augmenting%20two%20existing%20non-intrusive%0Areduced-order%20modeling%20approaches%2C%20and%20providing%20their%20active-learning-driven%0A%28ActLearn%29%20extensions%2C%20namely%2C%20SDE-ActLearn-POD-KSNN%2C%20and%20SDE-ActLearn-POD-NN.%0AFurthermore%2C%20we%20report%20positive%20results%20for%20two%20parametric%20physical%20models%2C%0Ahighlighting%20the%20efficiency%20of%20the%20proposed%20SDE-AL%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubspace-Distance-Enabled%2520Active%2520Learning%2520for%2520Efficient%2520Data-Driven%250A%2520%2520Model%2520Reduction%2520of%2520Parametric%2520Dynamical%2520Systems%26entry.906535625%3DHarshit%2520Kapadia%2520and%2520Peter%2520Benner%2520and%2520Lihong%2520Feng%26entry.1292438233%3D%2520%2520In%2520situations%2520where%2520the%2520solution%2520of%2520a%2520high-fidelity%2520dynamical%2520system%2520needs%2520to%250Abe%2520evaluated%2520repeatedly%252C%2520over%2520a%2520vast%2520pool%2520of%2520parametric%2520configurations%2520and%2520in%250Aabsence%2520of%2520access%2520to%2520the%2520underlying%2520governing%2520equations%252C%2520data-driven%2520model%250Areduction%2520techniques%2520are%2520preferable.%2520We%2520propose%2520a%2520novel%2520active%2520learning%250Aapproach%2520to%2520build%2520a%2520parametric%2520data-driven%2520reduced-order%2520model%2520%2528ROM%2529%2520by%250Agreedily%2520picking%2520the%2520most%2520important%2520parameter%2520samples%2520from%2520the%2520parameter%250Adomain.%2520As%2520a%2520result%252C%2520during%2520the%2520ROM%2520construction%2520phase%252C%2520the%2520number%2520of%250Ahigh-fidelity%2520solutions%2520dynamically%2520grow%2520in%2520a%2520principled%2520fashion.%2520The%250Ahigh-fidelity%2520solution%2520snapshots%2520are%2520expressed%2520in%2520several%2520parameter-specific%250Alinear%2520subspaces%252C%2520with%2520the%2520help%2520of%2520proper%2520orthogonal%2520decomposition%2520%2528POD%2529%252C%2520and%250Athe%2520relative%2520distance%2520between%2520these%2520subspaces%2520is%2520used%2520as%2520a%2520guiding%2520mechanism%2520to%250Aperform%2520active%2520learning.%2520For%2520successfully%2520achieving%2520this%252C%2520we%2520provide%2520a%2520distance%250Ameasure%2520to%2520evaluate%2520the%2520similarity%2520between%2520pairs%2520of%2520linear%2520subspaces%2520with%250Adifferent%2520dimensions%252C%2520and%2520also%2520show%2520that%2520this%2520distance%2520measure%2520is%2520a%2520metric.%2520The%250Ausability%2520of%2520the%2520proposed%2520subspace-distance-enabled%2520active%2520learning%2520%2528SDE-AL%2529%250Aframework%2520is%2520demonstrated%2520by%2520augmenting%2520two%2520existing%2520non-intrusive%250Areduced-order%2520modeling%2520approaches%252C%2520and%2520providing%2520their%2520active-learning-driven%250A%2528ActLearn%2529%2520extensions%252C%2520namely%252C%2520SDE-ActLearn-POD-KSNN%252C%2520and%2520SDE-ActLearn-POD-NN.%250AFurthermore%252C%2520we%2520report%2520positive%2520results%2520for%2520two%2520parametric%2520physical%2520models%252C%250Ahighlighting%2520the%2520efficiency%2520of%2520the%2520proposed%2520SDE-AL%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subspace-Distance-Enabled%20Active%20Learning%20for%20Efficient%20Data-Driven%0A%20%20Model%20Reduction%20of%20Parametric%20Dynamical%20Systems&entry.906535625=Harshit%20Kapadia%20and%20Peter%20Benner%20and%20Lihong%20Feng&entry.1292438233=%20%20In%20situations%20where%20the%20solution%20of%20a%20high-fidelity%20dynamical%20system%20needs%20to%0Abe%20evaluated%20repeatedly%2C%20over%20a%20vast%20pool%20of%20parametric%20configurations%20and%20in%0Aabsence%20of%20access%20to%20the%20underlying%20governing%20equations%2C%20data-driven%20model%0Areduction%20techniques%20are%20preferable.%20We%20propose%20a%20novel%20active%20learning%0Aapproach%20to%20build%20a%20parametric%20data-driven%20reduced-order%20model%20%28ROM%29%20by%0Agreedily%20picking%20the%20most%20important%20parameter%20samples%20from%20the%20parameter%0Adomain.%20As%20a%20result%2C%20during%20the%20ROM%20construction%20phase%2C%20the%20number%20of%0Ahigh-fidelity%20solutions%20dynamically%20grow%20in%20a%20principled%20fashion.%20The%0Ahigh-fidelity%20solution%20snapshots%20are%20expressed%20in%20several%20parameter-specific%0Alinear%20subspaces%2C%20with%20the%20help%20of%20proper%20orthogonal%20decomposition%20%28POD%29%2C%20and%0Athe%20relative%20distance%20between%20these%20subspaces%20is%20used%20as%20a%20guiding%20mechanism%20to%0Aperform%20active%20learning.%20For%20successfully%20achieving%20this%2C%20we%20provide%20a%20distance%0Ameasure%20to%20evaluate%20the%20similarity%20between%20pairs%20of%20linear%20subspaces%20with%0Adifferent%20dimensions%2C%20and%20also%20show%20that%20this%20distance%20measure%20is%20a%20metric.%20The%0Ausability%20of%20the%20proposed%20subspace-distance-enabled%20active%20learning%20%28SDE-AL%29%0Aframework%20is%20demonstrated%20by%20augmenting%20two%20existing%20non-intrusive%0Areduced-order%20modeling%20approaches%2C%20and%20providing%20their%20active-learning-driven%0A%28ActLearn%29%20extensions%2C%20namely%2C%20SDE-ActLearn-POD-KSNN%2C%20and%20SDE-ActLearn-POD-NN.%0AFurthermore%2C%20we%20report%20positive%20results%20for%20two%20parametric%20physical%20models%2C%0Ahighlighting%20the%20efficiency%20of%20the%20proposed%20SDE-AL%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00460v1&entry.124074799=Read"},
{"title": "ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory\n  Prediction for Automated Parking using Diffusion Models", "author": "Jiarong Wei and Niclas V\u00f6disch and Anna Rehr and Christian Feist and Abhinav Valada", "abstract": "  Automated parking is a critical feature of Advanced Driver Assistance Systems\n(ADAS), where accurate trajectory prediction is essential to bridge perception\nand planning modules. Despite its significance, research in this domain remains\nrelatively limited, with most existing studies concentrating on single-modal\ntrajectory prediction of vehicles. In this work, we propose ParkDiffusion, a\nnovel approach that predicts the trajectories of both vehicles and pedestrians\nin automated parking scenarios. ParkDiffusion employs diffusion models to\ncapture the inherent uncertainty and multi-modality of future trajectories,\nincorporating several key innovations. First, we propose a dual map encoder\nthat processes soft semantic cues and hard geometric constraints using a\ntwo-step cross-attention mechanism. Second, we introduce an adaptive agent type\nembedding module, which dynamically conditions the prediction process on the\ndistinct characteristics of vehicles and pedestrians. Third, to ensure\nkinematic feasibility, our model outputs control signals that are subsequently\nused within a kinematic framework to generate physically feasible trajectories.\nWe evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the\nIntersections Drone (inD) dataset. Our work establishes a new baseline for\nheterogeneous trajectory prediction in parking scenarios, outperforming\nexisting methods by a considerable margin.\n", "link": "http://arxiv.org/abs/2505.00586v1", "date": "2025-05-01", "relevancy": 1.7138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6202}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5656}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParkDiffusion%3A%20Heterogeneous%20Multi-Agent%20Multi-Modal%20Trajectory%0A%20%20Prediction%20for%20Automated%20Parking%20using%20Diffusion%20Models&body=Title%3A%20ParkDiffusion%3A%20Heterogeneous%20Multi-Agent%20Multi-Modal%20Trajectory%0A%20%20Prediction%20for%20Automated%20Parking%20using%20Diffusion%20Models%0AAuthor%3A%20Jiarong%20Wei%20and%20Niclas%20V%C3%B6disch%20and%20Anna%20Rehr%20and%20Christian%20Feist%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Automated%20parking%20is%20a%20critical%20feature%20of%20Advanced%20Driver%20Assistance%20Systems%0A%28ADAS%29%2C%20where%20accurate%20trajectory%20prediction%20is%20essential%20to%20bridge%20perception%0Aand%20planning%20modules.%20Despite%20its%20significance%2C%20research%20in%20this%20domain%20remains%0Arelatively%20limited%2C%20with%20most%20existing%20studies%20concentrating%20on%20single-modal%0Atrajectory%20prediction%20of%20vehicles.%20In%20this%20work%2C%20we%20propose%20ParkDiffusion%2C%20a%0Anovel%20approach%20that%20predicts%20the%20trajectories%20of%20both%20vehicles%20and%20pedestrians%0Ain%20automated%20parking%20scenarios.%20ParkDiffusion%20employs%20diffusion%20models%20to%0Acapture%20the%20inherent%20uncertainty%20and%20multi-modality%20of%20future%20trajectories%2C%0Aincorporating%20several%20key%20innovations.%20First%2C%20we%20propose%20a%20dual%20map%20encoder%0Athat%20processes%20soft%20semantic%20cues%20and%20hard%20geometric%20constraints%20using%20a%0Atwo-step%20cross-attention%20mechanism.%20Second%2C%20we%20introduce%20an%20adaptive%20agent%20type%0Aembedding%20module%2C%20which%20dynamically%20conditions%20the%20prediction%20process%20on%20the%0Adistinct%20characteristics%20of%20vehicles%20and%20pedestrians.%20Third%2C%20to%20ensure%0Akinematic%20feasibility%2C%20our%20model%20outputs%20control%20signals%20that%20are%20subsequently%0Aused%20within%20a%20kinematic%20framework%20to%20generate%20physically%20feasible%20trajectories.%0AWe%20evaluate%20ParkDiffusion%20on%20the%20Dragon%20Lake%20Parking%20%28DLP%29%20dataset%20and%20the%0AIntersections%20Drone%20%28inD%29%20dataset.%20Our%20work%20establishes%20a%20new%20baseline%20for%0Aheterogeneous%20trajectory%20prediction%20in%20parking%20scenarios%2C%20outperforming%0Aexisting%20methods%20by%20a%20considerable%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParkDiffusion%253A%2520Heterogeneous%2520Multi-Agent%2520Multi-Modal%2520Trajectory%250A%2520%2520Prediction%2520for%2520Automated%2520Parking%2520using%2520Diffusion%2520Models%26entry.906535625%3DJiarong%2520Wei%2520and%2520Niclas%2520V%25C3%25B6disch%2520and%2520Anna%2520Rehr%2520and%2520Christian%2520Feist%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Automated%2520parking%2520is%2520a%2520critical%2520feature%2520of%2520Advanced%2520Driver%2520Assistance%2520Systems%250A%2528ADAS%2529%252C%2520where%2520accurate%2520trajectory%2520prediction%2520is%2520essential%2520to%2520bridge%2520perception%250Aand%2520planning%2520modules.%2520Despite%2520its%2520significance%252C%2520research%2520in%2520this%2520domain%2520remains%250Arelatively%2520limited%252C%2520with%2520most%2520existing%2520studies%2520concentrating%2520on%2520single-modal%250Atrajectory%2520prediction%2520of%2520vehicles.%2520In%2520this%2520work%252C%2520we%2520propose%2520ParkDiffusion%252C%2520a%250Anovel%2520approach%2520that%2520predicts%2520the%2520trajectories%2520of%2520both%2520vehicles%2520and%2520pedestrians%250Ain%2520automated%2520parking%2520scenarios.%2520ParkDiffusion%2520employs%2520diffusion%2520models%2520to%250Acapture%2520the%2520inherent%2520uncertainty%2520and%2520multi-modality%2520of%2520future%2520trajectories%252C%250Aincorporating%2520several%2520key%2520innovations.%2520First%252C%2520we%2520propose%2520a%2520dual%2520map%2520encoder%250Athat%2520processes%2520soft%2520semantic%2520cues%2520and%2520hard%2520geometric%2520constraints%2520using%2520a%250Atwo-step%2520cross-attention%2520mechanism.%2520Second%252C%2520we%2520introduce%2520an%2520adaptive%2520agent%2520type%250Aembedding%2520module%252C%2520which%2520dynamically%2520conditions%2520the%2520prediction%2520process%2520on%2520the%250Adistinct%2520characteristics%2520of%2520vehicles%2520and%2520pedestrians.%2520Third%252C%2520to%2520ensure%250Akinematic%2520feasibility%252C%2520our%2520model%2520outputs%2520control%2520signals%2520that%2520are%2520subsequently%250Aused%2520within%2520a%2520kinematic%2520framework%2520to%2520generate%2520physically%2520feasible%2520trajectories.%250AWe%2520evaluate%2520ParkDiffusion%2520on%2520the%2520Dragon%2520Lake%2520Parking%2520%2528DLP%2529%2520dataset%2520and%2520the%250AIntersections%2520Drone%2520%2528inD%2529%2520dataset.%2520Our%2520work%2520establishes%2520a%2520new%2520baseline%2520for%250Aheterogeneous%2520trajectory%2520prediction%2520in%2520parking%2520scenarios%252C%2520outperforming%250Aexisting%2520methods%2520by%2520a%2520considerable%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParkDiffusion%3A%20Heterogeneous%20Multi-Agent%20Multi-Modal%20Trajectory%0A%20%20Prediction%20for%20Automated%20Parking%20using%20Diffusion%20Models&entry.906535625=Jiarong%20Wei%20and%20Niclas%20V%C3%B6disch%20and%20Anna%20Rehr%20and%20Christian%20Feist%20and%20Abhinav%20Valada&entry.1292438233=%20%20Automated%20parking%20is%20a%20critical%20feature%20of%20Advanced%20Driver%20Assistance%20Systems%0A%28ADAS%29%2C%20where%20accurate%20trajectory%20prediction%20is%20essential%20to%20bridge%20perception%0Aand%20planning%20modules.%20Despite%20its%20significance%2C%20research%20in%20this%20domain%20remains%0Arelatively%20limited%2C%20with%20most%20existing%20studies%20concentrating%20on%20single-modal%0Atrajectory%20prediction%20of%20vehicles.%20In%20this%20work%2C%20we%20propose%20ParkDiffusion%2C%20a%0Anovel%20approach%20that%20predicts%20the%20trajectories%20of%20both%20vehicles%20and%20pedestrians%0Ain%20automated%20parking%20scenarios.%20ParkDiffusion%20employs%20diffusion%20models%20to%0Acapture%20the%20inherent%20uncertainty%20and%20multi-modality%20of%20future%20trajectories%2C%0Aincorporating%20several%20key%20innovations.%20First%2C%20we%20propose%20a%20dual%20map%20encoder%0Athat%20processes%20soft%20semantic%20cues%20and%20hard%20geometric%20constraints%20using%20a%0Atwo-step%20cross-attention%20mechanism.%20Second%2C%20we%20introduce%20an%20adaptive%20agent%20type%0Aembedding%20module%2C%20which%20dynamically%20conditions%20the%20prediction%20process%20on%20the%0Adistinct%20characteristics%20of%20vehicles%20and%20pedestrians.%20Third%2C%20to%20ensure%0Akinematic%20feasibility%2C%20our%20model%20outputs%20control%20signals%20that%20are%20subsequently%0Aused%20within%20a%20kinematic%20framework%20to%20generate%20physically%20feasible%20trajectories.%0AWe%20evaluate%20ParkDiffusion%20on%20the%20Dragon%20Lake%20Parking%20%28DLP%29%20dataset%20and%20the%0AIntersections%20Drone%20%28inD%29%20dataset.%20Our%20work%20establishes%20a%20new%20baseline%20for%0Aheterogeneous%20trajectory%20prediction%20in%20parking%20scenarios%2C%20outperforming%0Aexisting%20methods%20by%20a%20considerable%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00586v1&entry.124074799=Read"},
{"title": "GeoDEx: A Unified Geometric Framework for Tactile Dexterous and\n  Extrinsic Manipulation under Force Uncertainty", "author": "Sirui Chen and Sergio Aguilera Marinovic and Soshi Iba and Rana Soltani Zarrin", "abstract": "  Sense of touch that allows robots to detect contact and measure interaction\nforces enables them to perform challenging tasks such as grasping fragile\nobjects or using tools. Tactile sensors in theory can equip the robots with\nsuch capabilities. However, accuracy of the measured forces is not on a par\nwith those of the force sensors due to the potential calibration challenges and\nnoise. This has limited the values these sensors can offer in manipulation\napplications that require force control. In this paper, we introduce GeoDEx, a\nunified estimation, planning, and control framework using geometric primitives\nsuch as plane, cone and ellipsoid, which enables dexterous as well as extrinsic\nmanipulation in the presence of uncertain force readings. Through various\nexperimental results, we show that while relying on direct inaccurate and noisy\nforce readings from tactile sensors results in unstable or failed manipulation,\nour method enables successful grasping and extrinsic manipulation of different\nobjects. Additionally, compared to directly running optimization using SOCP\n(Second Order Cone Programming), planning and force estimation using our\nframework achieves a 14x speed-up.\n", "link": "http://arxiv.org/abs/2505.00647v1", "date": "2025-05-01", "relevancy": 1.7336, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5876}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoDEx%3A%20A%20Unified%20Geometric%20Framework%20for%20Tactile%20Dexterous%20and%0A%20%20Extrinsic%20Manipulation%20under%20Force%20Uncertainty&body=Title%3A%20GeoDEx%3A%20A%20Unified%20Geometric%20Framework%20for%20Tactile%20Dexterous%20and%0A%20%20Extrinsic%20Manipulation%20under%20Force%20Uncertainty%0AAuthor%3A%20Sirui%20Chen%20and%20Sergio%20Aguilera%20Marinovic%20and%20Soshi%20Iba%20and%20Rana%20Soltani%20Zarrin%0AAbstract%3A%20%20%20Sense%20of%20touch%20that%20allows%20robots%20to%20detect%20contact%20and%20measure%20interaction%0Aforces%20enables%20them%20to%20perform%20challenging%20tasks%20such%20as%20grasping%20fragile%0Aobjects%20or%20using%20tools.%20Tactile%20sensors%20in%20theory%20can%20equip%20the%20robots%20with%0Asuch%20capabilities.%20However%2C%20accuracy%20of%20the%20measured%20forces%20is%20not%20on%20a%20par%0Awith%20those%20of%20the%20force%20sensors%20due%20to%20the%20potential%20calibration%20challenges%20and%0Anoise.%20This%20has%20limited%20the%20values%20these%20sensors%20can%20offer%20in%20manipulation%0Aapplications%20that%20require%20force%20control.%20In%20this%20paper%2C%20we%20introduce%20GeoDEx%2C%20a%0Aunified%20estimation%2C%20planning%2C%20and%20control%20framework%20using%20geometric%20primitives%0Asuch%20as%20plane%2C%20cone%20and%20ellipsoid%2C%20which%20enables%20dexterous%20as%20well%20as%20extrinsic%0Amanipulation%20in%20the%20presence%20of%20uncertain%20force%20readings.%20Through%20various%0Aexperimental%20results%2C%20we%20show%20that%20while%20relying%20on%20direct%20inaccurate%20and%20noisy%0Aforce%20readings%20from%20tactile%20sensors%20results%20in%20unstable%20or%20failed%20manipulation%2C%0Aour%20method%20enables%20successful%20grasping%20and%20extrinsic%20manipulation%20of%20different%0Aobjects.%20Additionally%2C%20compared%20to%20directly%20running%20optimization%20using%20SOCP%0A%28Second%20Order%20Cone%20Programming%29%2C%20planning%20and%20force%20estimation%20using%20our%0Aframework%20achieves%20a%2014x%20speed-up.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoDEx%253A%2520A%2520Unified%2520Geometric%2520Framework%2520for%2520Tactile%2520Dexterous%2520and%250A%2520%2520Extrinsic%2520Manipulation%2520under%2520Force%2520Uncertainty%26entry.906535625%3DSirui%2520Chen%2520and%2520Sergio%2520Aguilera%2520Marinovic%2520and%2520Soshi%2520Iba%2520and%2520Rana%2520Soltani%2520Zarrin%26entry.1292438233%3D%2520%2520Sense%2520of%2520touch%2520that%2520allows%2520robots%2520to%2520detect%2520contact%2520and%2520measure%2520interaction%250Aforces%2520enables%2520them%2520to%2520perform%2520challenging%2520tasks%2520such%2520as%2520grasping%2520fragile%250Aobjects%2520or%2520using%2520tools.%2520Tactile%2520sensors%2520in%2520theory%2520can%2520equip%2520the%2520robots%2520with%250Asuch%2520capabilities.%2520However%252C%2520accuracy%2520of%2520the%2520measured%2520forces%2520is%2520not%2520on%2520a%2520par%250Awith%2520those%2520of%2520the%2520force%2520sensors%2520due%2520to%2520the%2520potential%2520calibration%2520challenges%2520and%250Anoise.%2520This%2520has%2520limited%2520the%2520values%2520these%2520sensors%2520can%2520offer%2520in%2520manipulation%250Aapplications%2520that%2520require%2520force%2520control.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GeoDEx%252C%2520a%250Aunified%2520estimation%252C%2520planning%252C%2520and%2520control%2520framework%2520using%2520geometric%2520primitives%250Asuch%2520as%2520plane%252C%2520cone%2520and%2520ellipsoid%252C%2520which%2520enables%2520dexterous%2520as%2520well%2520as%2520extrinsic%250Amanipulation%2520in%2520the%2520presence%2520of%2520uncertain%2520force%2520readings.%2520Through%2520various%250Aexperimental%2520results%252C%2520we%2520show%2520that%2520while%2520relying%2520on%2520direct%2520inaccurate%2520and%2520noisy%250Aforce%2520readings%2520from%2520tactile%2520sensors%2520results%2520in%2520unstable%2520or%2520failed%2520manipulation%252C%250Aour%2520method%2520enables%2520successful%2520grasping%2520and%2520extrinsic%2520manipulation%2520of%2520different%250Aobjects.%2520Additionally%252C%2520compared%2520to%2520directly%2520running%2520optimization%2520using%2520SOCP%250A%2528Second%2520Order%2520Cone%2520Programming%2529%252C%2520planning%2520and%2520force%2520estimation%2520using%2520our%250Aframework%2520achieves%2520a%252014x%2520speed-up.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDEx%3A%20A%20Unified%20Geometric%20Framework%20for%20Tactile%20Dexterous%20and%0A%20%20Extrinsic%20Manipulation%20under%20Force%20Uncertainty&entry.906535625=Sirui%20Chen%20and%20Sergio%20Aguilera%20Marinovic%20and%20Soshi%20Iba%20and%20Rana%20Soltani%20Zarrin&entry.1292438233=%20%20Sense%20of%20touch%20that%20allows%20robots%20to%20detect%20contact%20and%20measure%20interaction%0Aforces%20enables%20them%20to%20perform%20challenging%20tasks%20such%20as%20grasping%20fragile%0Aobjects%20or%20using%20tools.%20Tactile%20sensors%20in%20theory%20can%20equip%20the%20robots%20with%0Asuch%20capabilities.%20However%2C%20accuracy%20of%20the%20measured%20forces%20is%20not%20on%20a%20par%0Awith%20those%20of%20the%20force%20sensors%20due%20to%20the%20potential%20calibration%20challenges%20and%0Anoise.%20This%20has%20limited%20the%20values%20these%20sensors%20can%20offer%20in%20manipulation%0Aapplications%20that%20require%20force%20control.%20In%20this%20paper%2C%20we%20introduce%20GeoDEx%2C%20a%0Aunified%20estimation%2C%20planning%2C%20and%20control%20framework%20using%20geometric%20primitives%0Asuch%20as%20plane%2C%20cone%20and%20ellipsoid%2C%20which%20enables%20dexterous%20as%20well%20as%20extrinsic%0Amanipulation%20in%20the%20presence%20of%20uncertain%20force%20readings.%20Through%20various%0Aexperimental%20results%2C%20we%20show%20that%20while%20relying%20on%20direct%20inaccurate%20and%20noisy%0Aforce%20readings%20from%20tactile%20sensors%20results%20in%20unstable%20or%20failed%20manipulation%2C%0Aour%20method%20enables%20successful%20grasping%20and%20extrinsic%20manipulation%20of%20different%0Aobjects.%20Additionally%2C%20compared%20to%20directly%20running%20optimization%20using%20SOCP%0A%28Second%20Order%20Cone%20Programming%29%2C%20planning%20and%20force%20estimation%20using%20our%0Aframework%20achieves%20a%2014x%20speed-up.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00647v1&entry.124074799=Read"},
{"title": "Hypothesis-free discovery from epidemiological data by automatic\n  detection and local inference for tree-based nonlinearities and interactions", "author": "Giorgio Spadaccini and Marjolein Fokkema and Mark A. van de Wiel", "abstract": "  In epidemiological settings, Machine Learning (ML) is gaining popularity for\nhypothesis-free discovery of risk (or protective) factors. Although ML is\nstrong at discovering non-linearities and interactions, this power is currently\ncompromised by a lack of reliable inference. Although local measures of feature\neffect can be combined with tree ensembles, uncertainty quantifications for\nthese measures remain only partially available and oftentimes unsatisfactory.\nWe propose RuleSHAP, a framework for using rule-based, hypothesis-free\ndiscovery that combines sparse Bayesian regression, tree ensembles and Shapley\nvalues in a one-step procedure that both detects and tests complex patterns at\nthe individual level. To ease computation, we derive a formula that computes\nmarginal Shapley values more efficiently for our setting. We demonstrate the\nvalidity of our framework on simulated data. To illustrate, we apply our\nmachinery to data from an epidemiological cohort to detect and infer several\neffects for high cholesterol and blood pressure, such as nonlinear interaction\neffects between features like age, sex, ethnicity, BMI and glucose level.\n", "link": "http://arxiv.org/abs/2505.00571v1", "date": "2025-05-01", "relevancy": 1.2743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4674}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4168}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypothesis-free%20discovery%20from%20epidemiological%20data%20by%20automatic%0A%20%20detection%20and%20local%20inference%20for%20tree-based%20nonlinearities%20and%20interactions&body=Title%3A%20Hypothesis-free%20discovery%20from%20epidemiological%20data%20by%20automatic%0A%20%20detection%20and%20local%20inference%20for%20tree-based%20nonlinearities%20and%20interactions%0AAuthor%3A%20Giorgio%20Spadaccini%20and%20Marjolein%20Fokkema%20and%20Mark%20A.%20van%20de%20Wiel%0AAbstract%3A%20%20%20In%20epidemiological%20settings%2C%20Machine%20Learning%20%28ML%29%20is%20gaining%20popularity%20for%0Ahypothesis-free%20discovery%20of%20risk%20%28or%20protective%29%20factors.%20Although%20ML%20is%0Astrong%20at%20discovering%20non-linearities%20and%20interactions%2C%20this%20power%20is%20currently%0Acompromised%20by%20a%20lack%20of%20reliable%20inference.%20Although%20local%20measures%20of%20feature%0Aeffect%20can%20be%20combined%20with%20tree%20ensembles%2C%20uncertainty%20quantifications%20for%0Athese%20measures%20remain%20only%20partially%20available%20and%20oftentimes%20unsatisfactory.%0AWe%20propose%20RuleSHAP%2C%20a%20framework%20for%20using%20rule-based%2C%20hypothesis-free%0Adiscovery%20that%20combines%20sparse%20Bayesian%20regression%2C%20tree%20ensembles%20and%20Shapley%0Avalues%20in%20a%20one-step%20procedure%20that%20both%20detects%20and%20tests%20complex%20patterns%20at%0Athe%20individual%20level.%20To%20ease%20computation%2C%20we%20derive%20a%20formula%20that%20computes%0Amarginal%20Shapley%20values%20more%20efficiently%20for%20our%20setting.%20We%20demonstrate%20the%0Avalidity%20of%20our%20framework%20on%20simulated%20data.%20To%20illustrate%2C%20we%20apply%20our%0Amachinery%20to%20data%20from%20an%20epidemiological%20cohort%20to%20detect%20and%20infer%20several%0Aeffects%20for%20high%20cholesterol%20and%20blood%20pressure%2C%20such%20as%20nonlinear%20interaction%0Aeffects%20between%20features%20like%20age%2C%20sex%2C%20ethnicity%2C%20BMI%20and%20glucose%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypothesis-free%2520discovery%2520from%2520epidemiological%2520data%2520by%2520automatic%250A%2520%2520detection%2520and%2520local%2520inference%2520for%2520tree-based%2520nonlinearities%2520and%2520interactions%26entry.906535625%3DGiorgio%2520Spadaccini%2520and%2520Marjolein%2520Fokkema%2520and%2520Mark%2520A.%2520van%2520de%2520Wiel%26entry.1292438233%3D%2520%2520In%2520epidemiological%2520settings%252C%2520Machine%2520Learning%2520%2528ML%2529%2520is%2520gaining%2520popularity%2520for%250Ahypothesis-free%2520discovery%2520of%2520risk%2520%2528or%2520protective%2529%2520factors.%2520Although%2520ML%2520is%250Astrong%2520at%2520discovering%2520non-linearities%2520and%2520interactions%252C%2520this%2520power%2520is%2520currently%250Acompromised%2520by%2520a%2520lack%2520of%2520reliable%2520inference.%2520Although%2520local%2520measures%2520of%2520feature%250Aeffect%2520can%2520be%2520combined%2520with%2520tree%2520ensembles%252C%2520uncertainty%2520quantifications%2520for%250Athese%2520measures%2520remain%2520only%2520partially%2520available%2520and%2520oftentimes%2520unsatisfactory.%250AWe%2520propose%2520RuleSHAP%252C%2520a%2520framework%2520for%2520using%2520rule-based%252C%2520hypothesis-free%250Adiscovery%2520that%2520combines%2520sparse%2520Bayesian%2520regression%252C%2520tree%2520ensembles%2520and%2520Shapley%250Avalues%2520in%2520a%2520one-step%2520procedure%2520that%2520both%2520detects%2520and%2520tests%2520complex%2520patterns%2520at%250Athe%2520individual%2520level.%2520To%2520ease%2520computation%252C%2520we%2520derive%2520a%2520formula%2520that%2520computes%250Amarginal%2520Shapley%2520values%2520more%2520efficiently%2520for%2520our%2520setting.%2520We%2520demonstrate%2520the%250Avalidity%2520of%2520our%2520framework%2520on%2520simulated%2520data.%2520To%2520illustrate%252C%2520we%2520apply%2520our%250Amachinery%2520to%2520data%2520from%2520an%2520epidemiological%2520cohort%2520to%2520detect%2520and%2520infer%2520several%250Aeffects%2520for%2520high%2520cholesterol%2520and%2520blood%2520pressure%252C%2520such%2520as%2520nonlinear%2520interaction%250Aeffects%2520between%2520features%2520like%2520age%252C%2520sex%252C%2520ethnicity%252C%2520BMI%2520and%2520glucose%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypothesis-free%20discovery%20from%20epidemiological%20data%20by%20automatic%0A%20%20detection%20and%20local%20inference%20for%20tree-based%20nonlinearities%20and%20interactions&entry.906535625=Giorgio%20Spadaccini%20and%20Marjolein%20Fokkema%20and%20Mark%20A.%20van%20de%20Wiel&entry.1292438233=%20%20In%20epidemiological%20settings%2C%20Machine%20Learning%20%28ML%29%20is%20gaining%20popularity%20for%0Ahypothesis-free%20discovery%20of%20risk%20%28or%20protective%29%20factors.%20Although%20ML%20is%0Astrong%20at%20discovering%20non-linearities%20and%20interactions%2C%20this%20power%20is%20currently%0Acompromised%20by%20a%20lack%20of%20reliable%20inference.%20Although%20local%20measures%20of%20feature%0Aeffect%20can%20be%20combined%20with%20tree%20ensembles%2C%20uncertainty%20quantifications%20for%0Athese%20measures%20remain%20only%20partially%20available%20and%20oftentimes%20unsatisfactory.%0AWe%20propose%20RuleSHAP%2C%20a%20framework%20for%20using%20rule-based%2C%20hypothesis-free%0Adiscovery%20that%20combines%20sparse%20Bayesian%20regression%2C%20tree%20ensembles%20and%20Shapley%0Avalues%20in%20a%20one-step%20procedure%20that%20both%20detects%20and%20tests%20complex%20patterns%20at%0Athe%20individual%20level.%20To%20ease%20computation%2C%20we%20derive%20a%20formula%20that%20computes%0Amarginal%20Shapley%20values%20more%20efficiently%20for%20our%20setting.%20We%20demonstrate%20the%0Avalidity%20of%20our%20framework%20on%20simulated%20data.%20To%20illustrate%2C%20we%20apply%20our%0Amachinery%20to%20data%20from%20an%20epidemiological%20cohort%20to%20detect%20and%20infer%20several%0Aeffects%20for%20high%20cholesterol%20and%20blood%20pressure%2C%20such%20as%20nonlinear%20interaction%0Aeffects%20between%20features%20like%20age%2C%20sex%2C%20ethnicity%2C%20BMI%20and%20glucose%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00571v1&entry.124074799=Read"},
{"title": "TabRep: Training Tabular Diffusion Models with a Simple and Effective\n  Continuous Representation", "author": "Jacob Si and Zijing Ou and Mike Qu and Zhengrui Xiang and Yingzhen Li", "abstract": "  Diffusion models have been the predominant generative model for tabular data\ngeneration. However, they face the conundrum of modeling under a separate\nversus a unified data representation. The former encounters the challenge of\njointly modeling all multi-modal distributions of tabular data in one model.\nWhile the latter alleviates this by learning a single representation for all\nfeatures, it currently leverages sparse suboptimal encoding heuristics and\nnecessitates additional computation costs. In this work, we address the latter\nby presenting TabRep, a tabular diffusion architecture trained with a unified\ncontinuous representation. To motivate the design of our representation, we\nprovide geometric insights into how the data manifold affects diffusion models.\nThe key attributes of our representation are composed of its density,\nflexibility to provide ample separability for nominal features, and ability to\npreserve intrinsic relationships. Ultimately, TabRep provides a simple yet\neffective approach for training tabular diffusion models under a continuous\ndata manifold. Our results showcase that TabRep achieves superior performance\nacross a broad suite of evaluations. It is the first to synthesize tabular data\nthat exceeds the downstream quality of the original datasets while preserving\nprivacy and remaining computationally efficient.\n", "link": "http://arxiv.org/abs/2504.04798v4", "date": "2025-05-01", "relevancy": 1.6352, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5637}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5589}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabRep%3A%20Training%20Tabular%20Diffusion%20Models%20with%20a%20Simple%20and%20Effective%0A%20%20Continuous%20Representation&body=Title%3A%20TabRep%3A%20Training%20Tabular%20Diffusion%20Models%20with%20a%20Simple%20and%20Effective%0A%20%20Continuous%20Representation%0AAuthor%3A%20Jacob%20Si%20and%20Zijing%20Ou%20and%20Mike%20Qu%20and%20Zhengrui%20Xiang%20and%20Yingzhen%20Li%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20the%20predominant%20generative%20model%20for%20tabular%20data%0Ageneration.%20However%2C%20they%20face%20the%20conundrum%20of%20modeling%20under%20a%20separate%0Aversus%20a%20unified%20data%20representation.%20The%20former%20encounters%20the%20challenge%20of%0Ajointly%20modeling%20all%20multi-modal%20distributions%20of%20tabular%20data%20in%20one%20model.%0AWhile%20the%20latter%20alleviates%20this%20by%20learning%20a%20single%20representation%20for%20all%0Afeatures%2C%20it%20currently%20leverages%20sparse%20suboptimal%20encoding%20heuristics%20and%0Anecessitates%20additional%20computation%20costs.%20In%20this%20work%2C%20we%20address%20the%20latter%0Aby%20presenting%20TabRep%2C%20a%20tabular%20diffusion%20architecture%20trained%20with%20a%20unified%0Acontinuous%20representation.%20To%20motivate%20the%20design%20of%20our%20representation%2C%20we%0Aprovide%20geometric%20insights%20into%20how%20the%20data%20manifold%20affects%20diffusion%20models.%0AThe%20key%20attributes%20of%20our%20representation%20are%20composed%20of%20its%20density%2C%0Aflexibility%20to%20provide%20ample%20separability%20for%20nominal%20features%2C%20and%20ability%20to%0Apreserve%20intrinsic%20relationships.%20Ultimately%2C%20TabRep%20provides%20a%20simple%20yet%0Aeffective%20approach%20for%20training%20tabular%20diffusion%20models%20under%20a%20continuous%0Adata%20manifold.%20Our%20results%20showcase%20that%20TabRep%20achieves%20superior%20performance%0Aacross%20a%20broad%20suite%20of%20evaluations.%20It%20is%20the%20first%20to%20synthesize%20tabular%20data%0Athat%20exceeds%20the%20downstream%20quality%20of%20the%20original%20datasets%20while%20preserving%0Aprivacy%20and%20remaining%20computationally%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04798v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabRep%253A%2520Training%2520Tabular%2520Diffusion%2520Models%2520with%2520a%2520Simple%2520and%2520Effective%250A%2520%2520Continuous%2520Representation%26entry.906535625%3DJacob%2520Si%2520and%2520Zijing%2520Ou%2520and%2520Mike%2520Qu%2520and%2520Zhengrui%2520Xiang%2520and%2520Yingzhen%2520Li%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520the%2520predominant%2520generative%2520model%2520for%2520tabular%2520data%250Ageneration.%2520However%252C%2520they%2520face%2520the%2520conundrum%2520of%2520modeling%2520under%2520a%2520separate%250Aversus%2520a%2520unified%2520data%2520representation.%2520The%2520former%2520encounters%2520the%2520challenge%2520of%250Ajointly%2520modeling%2520all%2520multi-modal%2520distributions%2520of%2520tabular%2520data%2520in%2520one%2520model.%250AWhile%2520the%2520latter%2520alleviates%2520this%2520by%2520learning%2520a%2520single%2520representation%2520for%2520all%250Afeatures%252C%2520it%2520currently%2520leverages%2520sparse%2520suboptimal%2520encoding%2520heuristics%2520and%250Anecessitates%2520additional%2520computation%2520costs.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520latter%250Aby%2520presenting%2520TabRep%252C%2520a%2520tabular%2520diffusion%2520architecture%2520trained%2520with%2520a%2520unified%250Acontinuous%2520representation.%2520To%2520motivate%2520the%2520design%2520of%2520our%2520representation%252C%2520we%250Aprovide%2520geometric%2520insights%2520into%2520how%2520the%2520data%2520manifold%2520affects%2520diffusion%2520models.%250AThe%2520key%2520attributes%2520of%2520our%2520representation%2520are%2520composed%2520of%2520its%2520density%252C%250Aflexibility%2520to%2520provide%2520ample%2520separability%2520for%2520nominal%2520features%252C%2520and%2520ability%2520to%250Apreserve%2520intrinsic%2520relationships.%2520Ultimately%252C%2520TabRep%2520provides%2520a%2520simple%2520yet%250Aeffective%2520approach%2520for%2520training%2520tabular%2520diffusion%2520models%2520under%2520a%2520continuous%250Adata%2520manifold.%2520Our%2520results%2520showcase%2520that%2520TabRep%2520achieves%2520superior%2520performance%250Aacross%2520a%2520broad%2520suite%2520of%2520evaluations.%2520It%2520is%2520the%2520first%2520to%2520synthesize%2520tabular%2520data%250Athat%2520exceeds%2520the%2520downstream%2520quality%2520of%2520the%2520original%2520datasets%2520while%2520preserving%250Aprivacy%2520and%2520remaining%2520computationally%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04798v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabRep%3A%20Training%20Tabular%20Diffusion%20Models%20with%20a%20Simple%20and%20Effective%0A%20%20Continuous%20Representation&entry.906535625=Jacob%20Si%20and%20Zijing%20Ou%20and%20Mike%20Qu%20and%20Zhengrui%20Xiang%20and%20Yingzhen%20Li&entry.1292438233=%20%20Diffusion%20models%20have%20been%20the%20predominant%20generative%20model%20for%20tabular%20data%0Ageneration.%20However%2C%20they%20face%20the%20conundrum%20of%20modeling%20under%20a%20separate%0Aversus%20a%20unified%20data%20representation.%20The%20former%20encounters%20the%20challenge%20of%0Ajointly%20modeling%20all%20multi-modal%20distributions%20of%20tabular%20data%20in%20one%20model.%0AWhile%20the%20latter%20alleviates%20this%20by%20learning%20a%20single%20representation%20for%20all%0Afeatures%2C%20it%20currently%20leverages%20sparse%20suboptimal%20encoding%20heuristics%20and%0Anecessitates%20additional%20computation%20costs.%20In%20this%20work%2C%20we%20address%20the%20latter%0Aby%20presenting%20TabRep%2C%20a%20tabular%20diffusion%20architecture%20trained%20with%20a%20unified%0Acontinuous%20representation.%20To%20motivate%20the%20design%20of%20our%20representation%2C%20we%0Aprovide%20geometric%20insights%20into%20how%20the%20data%20manifold%20affects%20diffusion%20models.%0AThe%20key%20attributes%20of%20our%20representation%20are%20composed%20of%20its%20density%2C%0Aflexibility%20to%20provide%20ample%20separability%20for%20nominal%20features%2C%20and%20ability%20to%0Apreserve%20intrinsic%20relationships.%20Ultimately%2C%20TabRep%20provides%20a%20simple%20yet%0Aeffective%20approach%20for%20training%20tabular%20diffusion%20models%20under%20a%20continuous%0Adata%20manifold.%20Our%20results%20showcase%20that%20TabRep%20achieves%20superior%20performance%0Aacross%20a%20broad%20suite%20of%20evaluations.%20It%20is%20the%20first%20to%20synthesize%20tabular%20data%0Athat%20exceeds%20the%20downstream%20quality%20of%20the%20original%20datasets%20while%20preserving%0Aprivacy%20and%20remaining%20computationally%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04798v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


