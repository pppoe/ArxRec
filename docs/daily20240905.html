<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240904.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video\n  Diffusion Models", "author": "Zhibin Liu and Haoye Dong and Aviral Chharia and Hefeng Wu", "abstract": "  Generating lifelike 3D humans from a single RGB image remains a challenging\ntask in computer vision, as it requires accurate modeling of geometry,\nhigh-quality texture, and plausible unseen parts. Existing methods typically\nuse multi-view diffusion models for 3D generation, but they often face\ninconsistent view issues, which hinder high-quality 3D human generation. To\naddress this, we propose Human-VDM, a novel method for generating 3D human from\na single RGB image using Video Diffusion Models. Human-VDM provides temporally\nconsistent views for 3D human generation using Gaussian Splatting. It consists\nof three modules: a view-consistent human video diffusion module, a video\naugmentation module, and a Gaussian Splatting module. First, a single image is\nfed into a human video diffusion module to generate a coherent human video.\nNext, the video augmentation module applies super-resolution and video\ninterpolation to enhance the textures and geometric smoothness of the generated\nvideo. Finally, the 3D Human Gaussian Splatting module learns lifelike humans\nunder the guidance of these high-resolution and view-consistent images.\nExperiments demonstrate that Human-VDM achieves high-quality 3D human from a\nsingle image, outperforming state-of-the-art methods in both generation quality\nand quantity. Project page: https://human-vdm.github.io/Human-VDM/\n", "link": "http://arxiv.org/abs/2409.02851v1", "date": "2024-09-04", "relevancy": 3.2774, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6567}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6567}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-VDM%3A%20Learning%20Single-Image%203D%20Human%20Gaussian%20Splatting%20from%20Video%0A%20%20Diffusion%20Models&body=Title%3A%20Human-VDM%3A%20Learning%20Single-Image%203D%20Human%20Gaussian%20Splatting%20from%20Video%0A%20%20Diffusion%20Models%0AAuthor%3A%20Zhibin%20Liu%20and%20Haoye%20Dong%20and%20Aviral%20Chharia%20and%20Hefeng%20Wu%0AAbstract%3A%20%20%20Generating%20lifelike%203D%20humans%20from%20a%20single%20RGB%20image%20remains%20a%20challenging%0Atask%20in%20computer%20vision%2C%20as%20it%20requires%20accurate%20modeling%20of%20geometry%2C%0Ahigh-quality%20texture%2C%20and%20plausible%20unseen%20parts.%20Existing%20methods%20typically%0Ause%20multi-view%20diffusion%20models%20for%203D%20generation%2C%20but%20they%20often%20face%0Ainconsistent%20view%20issues%2C%20which%20hinder%20high-quality%203D%20human%20generation.%20To%0Aaddress%20this%2C%20we%20propose%20Human-VDM%2C%20a%20novel%20method%20for%20generating%203D%20human%20from%0Aa%20single%20RGB%20image%20using%20Video%20Diffusion%20Models.%20Human-VDM%20provides%20temporally%0Aconsistent%20views%20for%203D%20human%20generation%20using%20Gaussian%20Splatting.%20It%20consists%0Aof%20three%20modules%3A%20a%20view-consistent%20human%20video%20diffusion%20module%2C%20a%20video%0Aaugmentation%20module%2C%20and%20a%20Gaussian%20Splatting%20module.%20First%2C%20a%20single%20image%20is%0Afed%20into%20a%20human%20video%20diffusion%20module%20to%20generate%20a%20coherent%20human%20video.%0ANext%2C%20the%20video%20augmentation%20module%20applies%20super-resolution%20and%20video%0Ainterpolation%20to%20enhance%20the%20textures%20and%20geometric%20smoothness%20of%20the%20generated%0Avideo.%20Finally%2C%20the%203D%20Human%20Gaussian%20Splatting%20module%20learns%20lifelike%20humans%0Aunder%20the%20guidance%20of%20these%20high-resolution%20and%20view-consistent%20images.%0AExperiments%20demonstrate%20that%20Human-VDM%20achieves%20high-quality%203D%20human%20from%20a%0Asingle%20image%2C%20outperforming%20state-of-the-art%20methods%20in%20both%20generation%20quality%0Aand%20quantity.%20Project%20page%3A%20https%3A//human-vdm.github.io/Human-VDM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-VDM%253A%2520Learning%2520Single-Image%25203D%2520Human%2520Gaussian%2520Splatting%2520from%2520Video%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DZhibin%2520Liu%2520and%2520Haoye%2520Dong%2520and%2520Aviral%2520Chharia%2520and%2520Hefeng%2520Wu%26entry.1292438233%3D%2520%2520Generating%2520lifelike%25203D%2520humans%2520from%2520a%2520single%2520RGB%2520image%2520remains%2520a%2520challenging%250Atask%2520in%2520computer%2520vision%252C%2520as%2520it%2520requires%2520accurate%2520modeling%2520of%2520geometry%252C%250Ahigh-quality%2520texture%252C%2520and%2520plausible%2520unseen%2520parts.%2520Existing%2520methods%2520typically%250Ause%2520multi-view%2520diffusion%2520models%2520for%25203D%2520generation%252C%2520but%2520they%2520often%2520face%250Ainconsistent%2520view%2520issues%252C%2520which%2520hinder%2520high-quality%25203D%2520human%2520generation.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520Human-VDM%252C%2520a%2520novel%2520method%2520for%2520generating%25203D%2520human%2520from%250Aa%2520single%2520RGB%2520image%2520using%2520Video%2520Diffusion%2520Models.%2520Human-VDM%2520provides%2520temporally%250Aconsistent%2520views%2520for%25203D%2520human%2520generation%2520using%2520Gaussian%2520Splatting.%2520It%2520consists%250Aof%2520three%2520modules%253A%2520a%2520view-consistent%2520human%2520video%2520diffusion%2520module%252C%2520a%2520video%250Aaugmentation%2520module%252C%2520and%2520a%2520Gaussian%2520Splatting%2520module.%2520First%252C%2520a%2520single%2520image%2520is%250Afed%2520into%2520a%2520human%2520video%2520diffusion%2520module%2520to%2520generate%2520a%2520coherent%2520human%2520video.%250ANext%252C%2520the%2520video%2520augmentation%2520module%2520applies%2520super-resolution%2520and%2520video%250Ainterpolation%2520to%2520enhance%2520the%2520textures%2520and%2520geometric%2520smoothness%2520of%2520the%2520generated%250Avideo.%2520Finally%252C%2520the%25203D%2520Human%2520Gaussian%2520Splatting%2520module%2520learns%2520lifelike%2520humans%250Aunder%2520the%2520guidance%2520of%2520these%2520high-resolution%2520and%2520view-consistent%2520images.%250AExperiments%2520demonstrate%2520that%2520Human-VDM%2520achieves%2520high-quality%25203D%2520human%2520from%2520a%250Asingle%2520image%252C%2520outperforming%2520state-of-the-art%2520methods%2520in%2520both%2520generation%2520quality%250Aand%2520quantity.%2520Project%2520page%253A%2520https%253A//human-vdm.github.io/Human-VDM/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-VDM%3A%20Learning%20Single-Image%203D%20Human%20Gaussian%20Splatting%20from%20Video%0A%20%20Diffusion%20Models&entry.906535625=Zhibin%20Liu%20and%20Haoye%20Dong%20and%20Aviral%20Chharia%20and%20Hefeng%20Wu&entry.1292438233=%20%20Generating%20lifelike%203D%20humans%20from%20a%20single%20RGB%20image%20remains%20a%20challenging%0Atask%20in%20computer%20vision%2C%20as%20it%20requires%20accurate%20modeling%20of%20geometry%2C%0Ahigh-quality%20texture%2C%20and%20plausible%20unseen%20parts.%20Existing%20methods%20typically%0Ause%20multi-view%20diffusion%20models%20for%203D%20generation%2C%20but%20they%20often%20face%0Ainconsistent%20view%20issues%2C%20which%20hinder%20high-quality%203D%20human%20generation.%20To%0Aaddress%20this%2C%20we%20propose%20Human-VDM%2C%20a%20novel%20method%20for%20generating%203D%20human%20from%0Aa%20single%20RGB%20image%20using%20Video%20Diffusion%20Models.%20Human-VDM%20provides%20temporally%0Aconsistent%20views%20for%203D%20human%20generation%20using%20Gaussian%20Splatting.%20It%20consists%0Aof%20three%20modules%3A%20a%20view-consistent%20human%20video%20diffusion%20module%2C%20a%20video%0Aaugmentation%20module%2C%20and%20a%20Gaussian%20Splatting%20module.%20First%2C%20a%20single%20image%20is%0Afed%20into%20a%20human%20video%20diffusion%20module%20to%20generate%20a%20coherent%20human%20video.%0ANext%2C%20the%20video%20augmentation%20module%20applies%20super-resolution%20and%20video%0Ainterpolation%20to%20enhance%20the%20textures%20and%20geometric%20smoothness%20of%20the%20generated%0Avideo.%20Finally%2C%20the%203D%20Human%20Gaussian%20Splatting%20module%20learns%20lifelike%20humans%0Aunder%20the%20guidance%20of%20these%20high-resolution%20and%20view-consistent%20images.%0AExperiments%20demonstrate%20that%20Human-VDM%20achieves%20high-quality%203D%20human%20from%20a%0Asingle%20image%2C%20outperforming%20state-of-the-art%20methods%20in%20both%20generation%20quality%0Aand%20quantity.%20Project%20page%3A%20https%3A//human-vdm.github.io/Human-VDM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02851v1&entry.124074799=Read"},
{"title": "Object Gaussian for Monocular 6D Pose Estimation from Sparse Views", "author": "Luqing Luo and Shichu Sun and Jiangang Yang and Linfang Zheng and Jinwei Du and Jian Liu", "abstract": "  Monocular object pose estimation, as a pivotal task in computer vision and\nrobotics, heavily depends on accurate 2D-3D correspondences, which often demand\ncostly CAD models that may not be readily available. Object 3D reconstruction\nmethods offer an alternative, among which recent advancements in 3D Gaussian\nSplatting (3DGS) afford a compelling potential. Yet its performance still\nsuffers and tends to overfit with fewer input views. Embracing this challenge,\nwe introduce SGPose, a novel framework for sparse view object pose estimation\nusing Gaussian-based methods. Given as few as ten views, SGPose generates a\ngeometric-aware representation by starting with a random cuboid initialization,\neschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as\nrequired by traditional 3DGS methods. SGPose removes the dependence on CAD\nmodels by regressing dense 2D-3D correspondences between images and the\nreconstructed model from sparse input and random initialization, while the\ngeometric-consistent depth supervision and online synthetic view warping are\nkey to the success. Experiments on typical benchmarks, especially on the\nOcclusion LM-O dataset, demonstrate that SGPose outperforms existing methods\neven under sparse view constraints, under-scoring its potential in real-world\napplications.\n", "link": "http://arxiv.org/abs/2409.02581v1", "date": "2024-09-04", "relevancy": 3.2608, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6779}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6725}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Gaussian%20for%20Monocular%206D%20Pose%20Estimation%20from%20Sparse%20Views&body=Title%3A%20Object%20Gaussian%20for%20Monocular%206D%20Pose%20Estimation%20from%20Sparse%20Views%0AAuthor%3A%20Luqing%20Luo%20and%20Shichu%20Sun%20and%20Jiangang%20Yang%20and%20Linfang%20Zheng%20and%20Jinwei%20Du%20and%20Jian%20Liu%0AAbstract%3A%20%20%20Monocular%20object%20pose%20estimation%2C%20as%20a%20pivotal%20task%20in%20computer%20vision%20and%0Arobotics%2C%20heavily%20depends%20on%20accurate%202D-3D%20correspondences%2C%20which%20often%20demand%0Acostly%20CAD%20models%20that%20may%20not%20be%20readily%20available.%20Object%203D%20reconstruction%0Amethods%20offer%20an%20alternative%2C%20among%20which%20recent%20advancements%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20afford%20a%20compelling%20potential.%20Yet%20its%20performance%20still%0Asuffers%20and%20tends%20to%20overfit%20with%20fewer%20input%20views.%20Embracing%20this%20challenge%2C%0Awe%20introduce%20SGPose%2C%20a%20novel%20framework%20for%20sparse%20view%20object%20pose%20estimation%0Ausing%20Gaussian-based%20methods.%20Given%20as%20few%20as%20ten%20views%2C%20SGPose%20generates%20a%0Ageometric-aware%20representation%20by%20starting%20with%20a%20random%20cuboid%20initialization%2C%0Aeschewing%20reliance%20on%20Structure-from-Motion%20%28SfM%29%20pipeline-derived%20geometry%20as%0Arequired%20by%20traditional%203DGS%20methods.%20SGPose%20removes%20the%20dependence%20on%20CAD%0Amodels%20by%20regressing%20dense%202D-3D%20correspondences%20between%20images%20and%20the%0Areconstructed%20model%20from%20sparse%20input%20and%20random%20initialization%2C%20while%20the%0Ageometric-consistent%20depth%20supervision%20and%20online%20synthetic%20view%20warping%20are%0Akey%20to%20the%20success.%20Experiments%20on%20typical%20benchmarks%2C%20especially%20on%20the%0AOcclusion%20LM-O%20dataset%2C%20demonstrate%20that%20SGPose%20outperforms%20existing%20methods%0Aeven%20under%20sparse%20view%20constraints%2C%20under-scoring%20its%20potential%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Gaussian%2520for%2520Monocular%25206D%2520Pose%2520Estimation%2520from%2520Sparse%2520Views%26entry.906535625%3DLuqing%2520Luo%2520and%2520Shichu%2520Sun%2520and%2520Jiangang%2520Yang%2520and%2520Linfang%2520Zheng%2520and%2520Jinwei%2520Du%2520and%2520Jian%2520Liu%26entry.1292438233%3D%2520%2520Monocular%2520object%2520pose%2520estimation%252C%2520as%2520a%2520pivotal%2520task%2520in%2520computer%2520vision%2520and%250Arobotics%252C%2520heavily%2520depends%2520on%2520accurate%25202D-3D%2520correspondences%252C%2520which%2520often%2520demand%250Acostly%2520CAD%2520models%2520that%2520may%2520not%2520be%2520readily%2520available.%2520Object%25203D%2520reconstruction%250Amethods%2520offer%2520an%2520alternative%252C%2520among%2520which%2520recent%2520advancements%2520in%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520afford%2520a%2520compelling%2520potential.%2520Yet%2520its%2520performance%2520still%250Asuffers%2520and%2520tends%2520to%2520overfit%2520with%2520fewer%2520input%2520views.%2520Embracing%2520this%2520challenge%252C%250Awe%2520introduce%2520SGPose%252C%2520a%2520novel%2520framework%2520for%2520sparse%2520view%2520object%2520pose%2520estimation%250Ausing%2520Gaussian-based%2520methods.%2520Given%2520as%2520few%2520as%2520ten%2520views%252C%2520SGPose%2520generates%2520a%250Ageometric-aware%2520representation%2520by%2520starting%2520with%2520a%2520random%2520cuboid%2520initialization%252C%250Aeschewing%2520reliance%2520on%2520Structure-from-Motion%2520%2528SfM%2529%2520pipeline-derived%2520geometry%2520as%250Arequired%2520by%2520traditional%25203DGS%2520methods.%2520SGPose%2520removes%2520the%2520dependence%2520on%2520CAD%250Amodels%2520by%2520regressing%2520dense%25202D-3D%2520correspondences%2520between%2520images%2520and%2520the%250Areconstructed%2520model%2520from%2520sparse%2520input%2520and%2520random%2520initialization%252C%2520while%2520the%250Ageometric-consistent%2520depth%2520supervision%2520and%2520online%2520synthetic%2520view%2520warping%2520are%250Akey%2520to%2520the%2520success.%2520Experiments%2520on%2520typical%2520benchmarks%252C%2520especially%2520on%2520the%250AOcclusion%2520LM-O%2520dataset%252C%2520demonstrate%2520that%2520SGPose%2520outperforms%2520existing%2520methods%250Aeven%2520under%2520sparse%2520view%2520constraints%252C%2520under-scoring%2520its%2520potential%2520in%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Gaussian%20for%20Monocular%206D%20Pose%20Estimation%20from%20Sparse%20Views&entry.906535625=Luqing%20Luo%20and%20Shichu%20Sun%20and%20Jiangang%20Yang%20and%20Linfang%20Zheng%20and%20Jinwei%20Du%20and%20Jian%20Liu&entry.1292438233=%20%20Monocular%20object%20pose%20estimation%2C%20as%20a%20pivotal%20task%20in%20computer%20vision%20and%0Arobotics%2C%20heavily%20depends%20on%20accurate%202D-3D%20correspondences%2C%20which%20often%20demand%0Acostly%20CAD%20models%20that%20may%20not%20be%20readily%20available.%20Object%203D%20reconstruction%0Amethods%20offer%20an%20alternative%2C%20among%20which%20recent%20advancements%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20afford%20a%20compelling%20potential.%20Yet%20its%20performance%20still%0Asuffers%20and%20tends%20to%20overfit%20with%20fewer%20input%20views.%20Embracing%20this%20challenge%2C%0Awe%20introduce%20SGPose%2C%20a%20novel%20framework%20for%20sparse%20view%20object%20pose%20estimation%0Ausing%20Gaussian-based%20methods.%20Given%20as%20few%20as%20ten%20views%2C%20SGPose%20generates%20a%0Ageometric-aware%20representation%20by%20starting%20with%20a%20random%20cuboid%20initialization%2C%0Aeschewing%20reliance%20on%20Structure-from-Motion%20%28SfM%29%20pipeline-derived%20geometry%20as%0Arequired%20by%20traditional%203DGS%20methods.%20SGPose%20removes%20the%20dependence%20on%20CAD%0Amodels%20by%20regressing%20dense%202D-3D%20correspondences%20between%20images%20and%20the%0Areconstructed%20model%20from%20sparse%20input%20and%20random%20initialization%2C%20while%20the%0Ageometric-consistent%20depth%20supervision%20and%20online%20synthetic%20view%20warping%20are%0Akey%20to%20the%20success.%20Experiments%20on%20typical%20benchmarks%2C%20especially%20on%20the%0AOcclusion%20LM-O%20dataset%2C%20demonstrate%20that%20SGPose%20outperforms%20existing%20methods%0Aeven%20under%20sparse%20view%20constraints%2C%20under-scoring%20its%20potential%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02581v1&entry.124074799=Read"},
{"title": "Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any\n  Objects", "author": "Kyungmin Jo and Jaegul Choo", "abstract": "  The emergence of diffusion models has enabled the generation of diverse\nhigh-quality images solely from text, prompting subsequent efforts to enhance\nthe controllability of these models. Despite the improvement in\ncontrollability, pose control remains limited to specific objects (e.g.,\nhumans) or poses (e.g., frontal view) due to the fact that pose is generally\ncontrolled via camera parameters (e.g., rotation angle) or keypoints (e.g.,\neyes, nose). Specifically, camera parameters-conditional pose control models\ngenerate unrealistic images depending on the object, owing to the small size of\n3D datasets for training. Also, keypoint-based approaches encounter challenges\nin acquiring reliable keypoints for various objects (e.g., church) or poses\n(e.g., back view). To address these limitations, we propose depth-based pose\ncontrol, as depth maps are easily obtainable from a single depth estimation\nmodel regardless of objects and poses, unlike camera parameters and keypoints.\nHowever, depth-based pose control confronts issues of shape dependency, as\ndepth maps influence not only the pose but also the shape of the generated\nimages. To tackle this issue, we propose Skip-and-Play (SnP), designed via\nanalysis of the impact of three components of depth-conditional ControlNet on\nthe pose and the shape of the generated images. To be specific, based on the\nanalysis, we selectively skip parts of the components to mitigate shape\ndependency on the depth map while preserving the pose. Through various\nexperiments, we demonstrate the superiority of SnP over baselines and showcase\nthe ability of SnP to generate images of diverse objects and poses. Remarkably,\nSnP exhibits the ability to generate images even when the objects in the\ncondition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each\nother.\n", "link": "http://arxiv.org/abs/2409.02653v1", "date": "2024-09-04", "relevancy": 2.9827, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6235}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5894}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skip-and-Play%3A%20Depth-Driven%20Pose-Preserved%20Image%20Generation%20for%20Any%0A%20%20Objects&body=Title%3A%20Skip-and-Play%3A%20Depth-Driven%20Pose-Preserved%20Image%20Generation%20for%20Any%0A%20%20Objects%0AAuthor%3A%20Kyungmin%20Jo%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20The%20emergence%20of%20diffusion%20models%20has%20enabled%20the%20generation%20of%20diverse%0Ahigh-quality%20images%20solely%20from%20text%2C%20prompting%20subsequent%20efforts%20to%20enhance%0Athe%20controllability%20of%20these%20models.%20Despite%20the%20improvement%20in%0Acontrollability%2C%20pose%20control%20remains%20limited%20to%20specific%20objects%20%28e.g.%2C%0Ahumans%29%20or%20poses%20%28e.g.%2C%20frontal%20view%29%20due%20to%20the%20fact%20that%20pose%20is%20generally%0Acontrolled%20via%20camera%20parameters%20%28e.g.%2C%20rotation%20angle%29%20or%20keypoints%20%28e.g.%2C%0Aeyes%2C%20nose%29.%20Specifically%2C%20camera%20parameters-conditional%20pose%20control%20models%0Agenerate%20unrealistic%20images%20depending%20on%20the%20object%2C%20owing%20to%20the%20small%20size%20of%0A3D%20datasets%20for%20training.%20Also%2C%20keypoint-based%20approaches%20encounter%20challenges%0Ain%20acquiring%20reliable%20keypoints%20for%20various%20objects%20%28e.g.%2C%20church%29%20or%20poses%0A%28e.g.%2C%20back%20view%29.%20To%20address%20these%20limitations%2C%20we%20propose%20depth-based%20pose%0Acontrol%2C%20as%20depth%20maps%20are%20easily%20obtainable%20from%20a%20single%20depth%20estimation%0Amodel%20regardless%20of%20objects%20and%20poses%2C%20unlike%20camera%20parameters%20and%20keypoints.%0AHowever%2C%20depth-based%20pose%20control%20confronts%20issues%20of%20shape%20dependency%2C%20as%0Adepth%20maps%20influence%20not%20only%20the%20pose%20but%20also%20the%20shape%20of%20the%20generated%0Aimages.%20To%20tackle%20this%20issue%2C%20we%20propose%20Skip-and-Play%20%28SnP%29%2C%20designed%20via%0Aanalysis%20of%20the%20impact%20of%20three%20components%20of%20depth-conditional%20ControlNet%20on%0Athe%20pose%20and%20the%20shape%20of%20the%20generated%20images.%20To%20be%20specific%2C%20based%20on%20the%0Aanalysis%2C%20we%20selectively%20skip%20parts%20of%20the%20components%20to%20mitigate%20shape%0Adependency%20on%20the%20depth%20map%20while%20preserving%20the%20pose.%20Through%20various%0Aexperiments%2C%20we%20demonstrate%20the%20superiority%20of%20SnP%20over%20baselines%20and%20showcase%0Athe%20ability%20of%20SnP%20to%20generate%20images%20of%20diverse%20objects%20and%20poses.%20Remarkably%2C%0ASnP%20exhibits%20the%20ability%20to%20generate%20images%20even%20when%20the%20objects%20in%20the%0Acondition%20%28e.g.%2C%20a%20horse%29%20and%20the%20prompt%20%28e.g.%2C%20a%20hedgehog%29%20differ%20from%20each%0Aother.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkip-and-Play%253A%2520Depth-Driven%2520Pose-Preserved%2520Image%2520Generation%2520for%2520Any%250A%2520%2520Objects%26entry.906535625%3DKyungmin%2520Jo%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520diffusion%2520models%2520has%2520enabled%2520the%2520generation%2520of%2520diverse%250Ahigh-quality%2520images%2520solely%2520from%2520text%252C%2520prompting%2520subsequent%2520efforts%2520to%2520enhance%250Athe%2520controllability%2520of%2520these%2520models.%2520Despite%2520the%2520improvement%2520in%250Acontrollability%252C%2520pose%2520control%2520remains%2520limited%2520to%2520specific%2520objects%2520%2528e.g.%252C%250Ahumans%2529%2520or%2520poses%2520%2528e.g.%252C%2520frontal%2520view%2529%2520due%2520to%2520the%2520fact%2520that%2520pose%2520is%2520generally%250Acontrolled%2520via%2520camera%2520parameters%2520%2528e.g.%252C%2520rotation%2520angle%2529%2520or%2520keypoints%2520%2528e.g.%252C%250Aeyes%252C%2520nose%2529.%2520Specifically%252C%2520camera%2520parameters-conditional%2520pose%2520control%2520models%250Agenerate%2520unrealistic%2520images%2520depending%2520on%2520the%2520object%252C%2520owing%2520to%2520the%2520small%2520size%2520of%250A3D%2520datasets%2520for%2520training.%2520Also%252C%2520keypoint-based%2520approaches%2520encounter%2520challenges%250Ain%2520acquiring%2520reliable%2520keypoints%2520for%2520various%2520objects%2520%2528e.g.%252C%2520church%2529%2520or%2520poses%250A%2528e.g.%252C%2520back%2520view%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520depth-based%2520pose%250Acontrol%252C%2520as%2520depth%2520maps%2520are%2520easily%2520obtainable%2520from%2520a%2520single%2520depth%2520estimation%250Amodel%2520regardless%2520of%2520objects%2520and%2520poses%252C%2520unlike%2520camera%2520parameters%2520and%2520keypoints.%250AHowever%252C%2520depth-based%2520pose%2520control%2520confronts%2520issues%2520of%2520shape%2520dependency%252C%2520as%250Adepth%2520maps%2520influence%2520not%2520only%2520the%2520pose%2520but%2520also%2520the%2520shape%2520of%2520the%2520generated%250Aimages.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520Skip-and-Play%2520%2528SnP%2529%252C%2520designed%2520via%250Aanalysis%2520of%2520the%2520impact%2520of%2520three%2520components%2520of%2520depth-conditional%2520ControlNet%2520on%250Athe%2520pose%2520and%2520the%2520shape%2520of%2520the%2520generated%2520images.%2520To%2520be%2520specific%252C%2520based%2520on%2520the%250Aanalysis%252C%2520we%2520selectively%2520skip%2520parts%2520of%2520the%2520components%2520to%2520mitigate%2520shape%250Adependency%2520on%2520the%2520depth%2520map%2520while%2520preserving%2520the%2520pose.%2520Through%2520various%250Aexperiments%252C%2520we%2520demonstrate%2520the%2520superiority%2520of%2520SnP%2520over%2520baselines%2520and%2520showcase%250Athe%2520ability%2520of%2520SnP%2520to%2520generate%2520images%2520of%2520diverse%2520objects%2520and%2520poses.%2520Remarkably%252C%250ASnP%2520exhibits%2520the%2520ability%2520to%2520generate%2520images%2520even%2520when%2520the%2520objects%2520in%2520the%250Acondition%2520%2528e.g.%252C%2520a%2520horse%2529%2520and%2520the%2520prompt%2520%2528e.g.%252C%2520a%2520hedgehog%2529%2520differ%2520from%2520each%250Aother.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skip-and-Play%3A%20Depth-Driven%20Pose-Preserved%20Image%20Generation%20for%20Any%0A%20%20Objects&entry.906535625=Kyungmin%20Jo%20and%20Jaegul%20Choo&entry.1292438233=%20%20The%20emergence%20of%20diffusion%20models%20has%20enabled%20the%20generation%20of%20diverse%0Ahigh-quality%20images%20solely%20from%20text%2C%20prompting%20subsequent%20efforts%20to%20enhance%0Athe%20controllability%20of%20these%20models.%20Despite%20the%20improvement%20in%0Acontrollability%2C%20pose%20control%20remains%20limited%20to%20specific%20objects%20%28e.g.%2C%0Ahumans%29%20or%20poses%20%28e.g.%2C%20frontal%20view%29%20due%20to%20the%20fact%20that%20pose%20is%20generally%0Acontrolled%20via%20camera%20parameters%20%28e.g.%2C%20rotation%20angle%29%20or%20keypoints%20%28e.g.%2C%0Aeyes%2C%20nose%29.%20Specifically%2C%20camera%20parameters-conditional%20pose%20control%20models%0Agenerate%20unrealistic%20images%20depending%20on%20the%20object%2C%20owing%20to%20the%20small%20size%20of%0A3D%20datasets%20for%20training.%20Also%2C%20keypoint-based%20approaches%20encounter%20challenges%0Ain%20acquiring%20reliable%20keypoints%20for%20various%20objects%20%28e.g.%2C%20church%29%20or%20poses%0A%28e.g.%2C%20back%20view%29.%20To%20address%20these%20limitations%2C%20we%20propose%20depth-based%20pose%0Acontrol%2C%20as%20depth%20maps%20are%20easily%20obtainable%20from%20a%20single%20depth%20estimation%0Amodel%20regardless%20of%20objects%20and%20poses%2C%20unlike%20camera%20parameters%20and%20keypoints.%0AHowever%2C%20depth-based%20pose%20control%20confronts%20issues%20of%20shape%20dependency%2C%20as%0Adepth%20maps%20influence%20not%20only%20the%20pose%20but%20also%20the%20shape%20of%20the%20generated%0Aimages.%20To%20tackle%20this%20issue%2C%20we%20propose%20Skip-and-Play%20%28SnP%29%2C%20designed%20via%0Aanalysis%20of%20the%20impact%20of%20three%20components%20of%20depth-conditional%20ControlNet%20on%0Athe%20pose%20and%20the%20shape%20of%20the%20generated%20images.%20To%20be%20specific%2C%20based%20on%20the%0Aanalysis%2C%20we%20selectively%20skip%20parts%20of%20the%20components%20to%20mitigate%20shape%0Adependency%20on%20the%20depth%20map%20while%20preserving%20the%20pose.%20Through%20various%0Aexperiments%2C%20we%20demonstrate%20the%20superiority%20of%20SnP%20over%20baselines%20and%20showcase%0Athe%20ability%20of%20SnP%20to%20generate%20images%20of%20diverse%20objects%20and%20poses.%20Remarkably%2C%0ASnP%20exhibits%20the%20ability%20to%20generate%20images%20even%20when%20the%20objects%20in%20the%0Acondition%20%28e.g.%2C%20a%20horse%29%20and%20the%20prompt%20%28e.g.%2C%20a%20hedgehog%29%20differ%20from%20each%0Aother.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02653v1&entry.124074799=Read"},
{"title": "Augmented Reality without Borders: Achieving Precise Localization\n  Without Maps", "author": "Albert Gassol Puigjaner and Irvin Aloise and Patrik Schmuck", "abstract": "  Visual localization is crucial for Computer Vision and Augmented Reality (AR)\napplications, where determining the camera or device's position and orientation\nis essential to accurately interact with the physical environment. Traditional\nmethods rely on detailed 3D maps constructed using Structure from Motion (SfM)\nor Simultaneous Localization and Mapping (SLAM), which is computationally\nexpensive and impractical for dynamic or large-scale environments. We introduce\nMARLoc, a novel localization framework for AR applications that uses known\nrelative transformations within image sequences to perform intra-sequence\ntriangulation, generating 3D-2D correspondences for pose estimation and\nrefinement. MARLoc eliminates the need for pre-built SfM maps, providing\naccurate and efficient localization suitable for dynamic outdoor environments.\nEvaluation with benchmark datasets and real-world experiments demonstrates\nMARLoc's state-of-the-art performance and robustness. By integrating MARLoc\ninto an AR device, we highlight its capability to achieve precise localization\nin real-world outdoor scenarios, showcasing its practical effectiveness and\npotential to enhance visual localization in AR applications.\n", "link": "http://arxiv.org/abs/2408.17373v3", "date": "2024-09-04", "relevancy": 2.9134, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.606}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5757}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmented%20Reality%20without%20Borders%3A%20Achieving%20Precise%20Localization%0A%20%20Without%20Maps&body=Title%3A%20Augmented%20Reality%20without%20Borders%3A%20Achieving%20Precise%20Localization%0A%20%20Without%20Maps%0AAuthor%3A%20Albert%20Gassol%20Puigjaner%20and%20Irvin%20Aloise%20and%20Patrik%20Schmuck%0AAbstract%3A%20%20%20Visual%20localization%20is%20crucial%20for%20Computer%20Vision%20and%20Augmented%20Reality%20%28AR%29%0Aapplications%2C%20where%20determining%20the%20camera%20or%20device%27s%20position%20and%20orientation%0Ais%20essential%20to%20accurately%20interact%20with%20the%20physical%20environment.%20Traditional%0Amethods%20rely%20on%20detailed%203D%20maps%20constructed%20using%20Structure%20from%20Motion%20%28SfM%29%0Aor%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20which%20is%20computationally%0Aexpensive%20and%20impractical%20for%20dynamic%20or%20large-scale%20environments.%20We%20introduce%0AMARLoc%2C%20a%20novel%20localization%20framework%20for%20AR%20applications%20that%20uses%20known%0Arelative%20transformations%20within%20image%20sequences%20to%20perform%20intra-sequence%0Atriangulation%2C%20generating%203D-2D%20correspondences%20for%20pose%20estimation%20and%0Arefinement.%20MARLoc%20eliminates%20the%20need%20for%20pre-built%20SfM%20maps%2C%20providing%0Aaccurate%20and%20efficient%20localization%20suitable%20for%20dynamic%20outdoor%20environments.%0AEvaluation%20with%20benchmark%20datasets%20and%20real-world%20experiments%20demonstrates%0AMARLoc%27s%20state-of-the-art%20performance%20and%20robustness.%20By%20integrating%20MARLoc%0Ainto%20an%20AR%20device%2C%20we%20highlight%20its%20capability%20to%20achieve%20precise%20localization%0Ain%20real-world%20outdoor%20scenarios%2C%20showcasing%20its%20practical%20effectiveness%20and%0Apotential%20to%20enhance%20visual%20localization%20in%20AR%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17373v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmented%2520Reality%2520without%2520Borders%253A%2520Achieving%2520Precise%2520Localization%250A%2520%2520Without%2520Maps%26entry.906535625%3DAlbert%2520Gassol%2520Puigjaner%2520and%2520Irvin%2520Aloise%2520and%2520Patrik%2520Schmuck%26entry.1292438233%3D%2520%2520Visual%2520localization%2520is%2520crucial%2520for%2520Computer%2520Vision%2520and%2520Augmented%2520Reality%2520%2528AR%2529%250Aapplications%252C%2520where%2520determining%2520the%2520camera%2520or%2520device%2527s%2520position%2520and%2520orientation%250Ais%2520essential%2520to%2520accurately%2520interact%2520with%2520the%2520physical%2520environment.%2520Traditional%250Amethods%2520rely%2520on%2520detailed%25203D%2520maps%2520constructed%2520using%2520Structure%2520from%2520Motion%2520%2528SfM%2529%250Aor%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%252C%2520which%2520is%2520computationally%250Aexpensive%2520and%2520impractical%2520for%2520dynamic%2520or%2520large-scale%2520environments.%2520We%2520introduce%250AMARLoc%252C%2520a%2520novel%2520localization%2520framework%2520for%2520AR%2520applications%2520that%2520uses%2520known%250Arelative%2520transformations%2520within%2520image%2520sequences%2520to%2520perform%2520intra-sequence%250Atriangulation%252C%2520generating%25203D-2D%2520correspondences%2520for%2520pose%2520estimation%2520and%250Arefinement.%2520MARLoc%2520eliminates%2520the%2520need%2520for%2520pre-built%2520SfM%2520maps%252C%2520providing%250Aaccurate%2520and%2520efficient%2520localization%2520suitable%2520for%2520dynamic%2520outdoor%2520environments.%250AEvaluation%2520with%2520benchmark%2520datasets%2520and%2520real-world%2520experiments%2520demonstrates%250AMARLoc%2527s%2520state-of-the-art%2520performance%2520and%2520robustness.%2520By%2520integrating%2520MARLoc%250Ainto%2520an%2520AR%2520device%252C%2520we%2520highlight%2520its%2520capability%2520to%2520achieve%2520precise%2520localization%250Ain%2520real-world%2520outdoor%2520scenarios%252C%2520showcasing%2520its%2520practical%2520effectiveness%2520and%250Apotential%2520to%2520enhance%2520visual%2520localization%2520in%2520AR%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17373v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Reality%20without%20Borders%3A%20Achieving%20Precise%20Localization%0A%20%20Without%20Maps&entry.906535625=Albert%20Gassol%20Puigjaner%20and%20Irvin%20Aloise%20and%20Patrik%20Schmuck&entry.1292438233=%20%20Visual%20localization%20is%20crucial%20for%20Computer%20Vision%20and%20Augmented%20Reality%20%28AR%29%0Aapplications%2C%20where%20determining%20the%20camera%20or%20device%27s%20position%20and%20orientation%0Ais%20essential%20to%20accurately%20interact%20with%20the%20physical%20environment.%20Traditional%0Amethods%20rely%20on%20detailed%203D%20maps%20constructed%20using%20Structure%20from%20Motion%20%28SfM%29%0Aor%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20which%20is%20computationally%0Aexpensive%20and%20impractical%20for%20dynamic%20or%20large-scale%20environments.%20We%20introduce%0AMARLoc%2C%20a%20novel%20localization%20framework%20for%20AR%20applications%20that%20uses%20known%0Arelative%20transformations%20within%20image%20sequences%20to%20perform%20intra-sequence%0Atriangulation%2C%20generating%203D-2D%20correspondences%20for%20pose%20estimation%20and%0Arefinement.%20MARLoc%20eliminates%20the%20need%20for%20pre-built%20SfM%20maps%2C%20providing%0Aaccurate%20and%20efficient%20localization%20suitable%20for%20dynamic%20outdoor%20environments.%0AEvaluation%20with%20benchmark%20datasets%20and%20real-world%20experiments%20demonstrates%0AMARLoc%27s%20state-of-the-art%20performance%20and%20robustness.%20By%20integrating%20MARLoc%0Ainto%20an%20AR%20device%2C%20we%20highlight%20its%20capability%20to%20achieve%20precise%20localization%0Ain%20real-world%20outdoor%20scenarios%2C%20showcasing%20its%20practical%20effectiveness%20and%0Apotential%20to%20enhance%20visual%20localization%20in%20AR%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17373v3&entry.124074799=Read"},
{"title": "Learning Local Pattern Modularization for Point Cloud Reconstruction\n  from Unseen Classes", "author": "Chao Chen and Yu-Shen Liu and Zhizhong Han", "abstract": "  It is challenging to reconstruct 3D point clouds in unseen classes from\nsingle 2D images. Instead of object-centered coordinate system, current methods\ngeneralized global priors learned in seen classes to reconstruct 3D shapes from\nunseen classes in viewer-centered coordinate system. However, the\nreconstruction accuracy and interpretability are still eager to get improved.\nTo resolve this issue, we introduce to learn local pattern modularization for\nreconstructing 3D shapes in unseen classes, which achieves both good\ngeneralization ability and high reconstruction accuracy. Our insight is to\nlearn a local prior which is class-agnostic and easy to generalize in\nobject-centered coordinate system. Specifically, the local prior is learned via\na process of learning and customizing local pattern modularization in seen\nclasses. During this process, we first learn a set of patterns in local\nregions, which is the basis in the object-centered coordinate system to\nrepresent an arbitrary region on shapes across different classes. Then, we\nmodularize each region on an initially reconstructed shape using the learned\nlocal patterns. Based on that, we customize the local pattern modularization\nusing the input image by refining the reconstruction with more details. Our\nmethod enables to reconstruct high fidelity point clouds from unseen classes in\nobject-centered coordinate system without requiring a large number of patterns\nor any additional information, such as segmentation supervision or camera\nposes. Our experimental results under widely used benchmarks show that our\nmethod achieves the state-of-the-art reconstruction accuracy for shapes from\nunseen classes. The code is available at https://github.com/chenchao15/Unseen.\n", "link": "http://arxiv.org/abs/2408.14279v2", "date": "2024-09-04", "relevancy": 2.8686, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Local%20Pattern%20Modularization%20for%20Point%20Cloud%20Reconstruction%0A%20%20from%20Unseen%20Classes&body=Title%3A%20Learning%20Local%20Pattern%20Modularization%20for%20Point%20Cloud%20Reconstruction%0A%20%20from%20Unseen%20Classes%0AAuthor%3A%20Chao%20Chen%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han%0AAbstract%3A%20%20%20It%20is%20challenging%20to%20reconstruct%203D%20point%20clouds%20in%20unseen%20classes%20from%0Asingle%202D%20images.%20Instead%20of%20object-centered%20coordinate%20system%2C%20current%20methods%0Ageneralized%20global%20priors%20learned%20in%20seen%20classes%20to%20reconstruct%203D%20shapes%20from%0Aunseen%20classes%20in%20viewer-centered%20coordinate%20system.%20However%2C%20the%0Areconstruction%20accuracy%20and%20interpretability%20are%20still%20eager%20to%20get%20improved.%0ATo%20resolve%20this%20issue%2C%20we%20introduce%20to%20learn%20local%20pattern%20modularization%20for%0Areconstructing%203D%20shapes%20in%20unseen%20classes%2C%20which%20achieves%20both%20good%0Ageneralization%20ability%20and%20high%20reconstruction%20accuracy.%20Our%20insight%20is%20to%0Alearn%20a%20local%20prior%20which%20is%20class-agnostic%20and%20easy%20to%20generalize%20in%0Aobject-centered%20coordinate%20system.%20Specifically%2C%20the%20local%20prior%20is%20learned%20via%0Aa%20process%20of%20learning%20and%20customizing%20local%20pattern%20modularization%20in%20seen%0Aclasses.%20During%20this%20process%2C%20we%20first%20learn%20a%20set%20of%20patterns%20in%20local%0Aregions%2C%20which%20is%20the%20basis%20in%20the%20object-centered%20coordinate%20system%20to%0Arepresent%20an%20arbitrary%20region%20on%20shapes%20across%20different%20classes.%20Then%2C%20we%0Amodularize%20each%20region%20on%20an%20initially%20reconstructed%20shape%20using%20the%20learned%0Alocal%20patterns.%20Based%20on%20that%2C%20we%20customize%20the%20local%20pattern%20modularization%0Ausing%20the%20input%20image%20by%20refining%20the%20reconstruction%20with%20more%20details.%20Our%0Amethod%20enables%20to%20reconstruct%20high%20fidelity%20point%20clouds%20from%20unseen%20classes%20in%0Aobject-centered%20coordinate%20system%20without%20requiring%20a%20large%20number%20of%20patterns%0Aor%20any%20additional%20information%2C%20such%20as%20segmentation%20supervision%20or%20camera%0Aposes.%20Our%20experimental%20results%20under%20widely%20used%20benchmarks%20show%20that%20our%0Amethod%20achieves%20the%20state-of-the-art%20reconstruction%20accuracy%20for%20shapes%20from%0Aunseen%20classes.%20The%20code%20is%20available%20at%20https%3A//github.com/chenchao15/Unseen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Local%2520Pattern%2520Modularization%2520for%2520Point%2520Cloud%2520Reconstruction%250A%2520%2520from%2520Unseen%2520Classes%26entry.906535625%3DChao%2520Chen%2520and%2520Yu-Shen%2520Liu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3D%2520%2520It%2520is%2520challenging%2520to%2520reconstruct%25203D%2520point%2520clouds%2520in%2520unseen%2520classes%2520from%250Asingle%25202D%2520images.%2520Instead%2520of%2520object-centered%2520coordinate%2520system%252C%2520current%2520methods%250Ageneralized%2520global%2520priors%2520learned%2520in%2520seen%2520classes%2520to%2520reconstruct%25203D%2520shapes%2520from%250Aunseen%2520classes%2520in%2520viewer-centered%2520coordinate%2520system.%2520However%252C%2520the%250Areconstruction%2520accuracy%2520and%2520interpretability%2520are%2520still%2520eager%2520to%2520get%2520improved.%250ATo%2520resolve%2520this%2520issue%252C%2520we%2520introduce%2520to%2520learn%2520local%2520pattern%2520modularization%2520for%250Areconstructing%25203D%2520shapes%2520in%2520unseen%2520classes%252C%2520which%2520achieves%2520both%2520good%250Ageneralization%2520ability%2520and%2520high%2520reconstruction%2520accuracy.%2520Our%2520insight%2520is%2520to%250Alearn%2520a%2520local%2520prior%2520which%2520is%2520class-agnostic%2520and%2520easy%2520to%2520generalize%2520in%250Aobject-centered%2520coordinate%2520system.%2520Specifically%252C%2520the%2520local%2520prior%2520is%2520learned%2520via%250Aa%2520process%2520of%2520learning%2520and%2520customizing%2520local%2520pattern%2520modularization%2520in%2520seen%250Aclasses.%2520During%2520this%2520process%252C%2520we%2520first%2520learn%2520a%2520set%2520of%2520patterns%2520in%2520local%250Aregions%252C%2520which%2520is%2520the%2520basis%2520in%2520the%2520object-centered%2520coordinate%2520system%2520to%250Arepresent%2520an%2520arbitrary%2520region%2520on%2520shapes%2520across%2520different%2520classes.%2520Then%252C%2520we%250Amodularize%2520each%2520region%2520on%2520an%2520initially%2520reconstructed%2520shape%2520using%2520the%2520learned%250Alocal%2520patterns.%2520Based%2520on%2520that%252C%2520we%2520customize%2520the%2520local%2520pattern%2520modularization%250Ausing%2520the%2520input%2520image%2520by%2520refining%2520the%2520reconstruction%2520with%2520more%2520details.%2520Our%250Amethod%2520enables%2520to%2520reconstruct%2520high%2520fidelity%2520point%2520clouds%2520from%2520unseen%2520classes%2520in%250Aobject-centered%2520coordinate%2520system%2520without%2520requiring%2520a%2520large%2520number%2520of%2520patterns%250Aor%2520any%2520additional%2520information%252C%2520such%2520as%2520segmentation%2520supervision%2520or%2520camera%250Aposes.%2520Our%2520experimental%2520results%2520under%2520widely%2520used%2520benchmarks%2520show%2520that%2520our%250Amethod%2520achieves%2520the%2520state-of-the-art%2520reconstruction%2520accuracy%2520for%2520shapes%2520from%250Aunseen%2520classes.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/chenchao15/Unseen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Local%20Pattern%20Modularization%20for%20Point%20Cloud%20Reconstruction%0A%20%20from%20Unseen%20Classes&entry.906535625=Chao%20Chen%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han&entry.1292438233=%20%20It%20is%20challenging%20to%20reconstruct%203D%20point%20clouds%20in%20unseen%20classes%20from%0Asingle%202D%20images.%20Instead%20of%20object-centered%20coordinate%20system%2C%20current%20methods%0Ageneralized%20global%20priors%20learned%20in%20seen%20classes%20to%20reconstruct%203D%20shapes%20from%0Aunseen%20classes%20in%20viewer-centered%20coordinate%20system.%20However%2C%20the%0Areconstruction%20accuracy%20and%20interpretability%20are%20still%20eager%20to%20get%20improved.%0ATo%20resolve%20this%20issue%2C%20we%20introduce%20to%20learn%20local%20pattern%20modularization%20for%0Areconstructing%203D%20shapes%20in%20unseen%20classes%2C%20which%20achieves%20both%20good%0Ageneralization%20ability%20and%20high%20reconstruction%20accuracy.%20Our%20insight%20is%20to%0Alearn%20a%20local%20prior%20which%20is%20class-agnostic%20and%20easy%20to%20generalize%20in%0Aobject-centered%20coordinate%20system.%20Specifically%2C%20the%20local%20prior%20is%20learned%20via%0Aa%20process%20of%20learning%20and%20customizing%20local%20pattern%20modularization%20in%20seen%0Aclasses.%20During%20this%20process%2C%20we%20first%20learn%20a%20set%20of%20patterns%20in%20local%0Aregions%2C%20which%20is%20the%20basis%20in%20the%20object-centered%20coordinate%20system%20to%0Arepresent%20an%20arbitrary%20region%20on%20shapes%20across%20different%20classes.%20Then%2C%20we%0Amodularize%20each%20region%20on%20an%20initially%20reconstructed%20shape%20using%20the%20learned%0Alocal%20patterns.%20Based%20on%20that%2C%20we%20customize%20the%20local%20pattern%20modularization%0Ausing%20the%20input%20image%20by%20refining%20the%20reconstruction%20with%20more%20details.%20Our%0Amethod%20enables%20to%20reconstruct%20high%20fidelity%20point%20clouds%20from%20unseen%20classes%20in%0Aobject-centered%20coordinate%20system%20without%20requiring%20a%20large%20number%20of%20patterns%0Aor%20any%20additional%20information%2C%20such%20as%20segmentation%20supervision%20or%20camera%0Aposes.%20Our%20experimental%20results%20under%20widely%20used%20benchmarks%20show%20that%20our%0Amethod%20achieves%20the%20state-of-the-art%20reconstruction%20accuracy%20for%20shapes%20from%0Aunseen%20classes.%20The%20code%20is%20available%20at%20https%3A//github.com/chenchao15/Unseen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14279v2&entry.124074799=Read"},
{"title": "Multi-Modal Experience Inspired AI Creation", "author": "Qian Cao and Xu Chen and Ruihua Song and Hao Jiang and Guang Yang and Zhao Cao", "abstract": "  AI creation, such as poem or lyrics generation, has attracted increasing\nattention from both industry and academic communities, with many promising\nmodels proposed in the past few years. Existing methods usually estimate the\noutputs based on single and independent visual or textual information. However,\nin reality, humans usually make creations according to their experiences, which\nmay involve different modalities and be sequentially correlated. To model such\nhuman capabilities, in this paper, we define and solve a novel AI creation\nproblem based on human experiences. More specifically, we study how to generate\ntexts based on sequential multi-modal information. Compared with the previous\nworks, this task is much more difficult because the designed model has to well\nunderstand and adapt the semantics among different modalities and effectively\nconvert them into the output in a sequential manner. To alleviate these\ndifficulties, we firstly design a multi-channel sequence-to-sequence\narchitecture equipped with a multi-modal attention network. For more effective\noptimization, we then propose a curriculum negative sampling strategy tailored\nfor the sequential inputs. To benchmark this problem and demonstrate the\neffectiveness of our model, we manually labeled a new multi-modal experience\ndataset. With this dataset, we conduct extensive experiments by comparing our\nmodel with a series of representative baselines, where we can demonstrate\nsignificant improvements in our model based on both automatic and\nhuman-centered metrics. The code and data are available at:\n\\url{https://github.com/Aman-4-Real/MMTG}.\n", "link": "http://arxiv.org/abs/2209.02427v2", "date": "2024-09-04", "relevancy": 2.8492, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5705}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Experience%20Inspired%20AI%20Creation&body=Title%3A%20Multi-Modal%20Experience%20Inspired%20AI%20Creation%0AAuthor%3A%20Qian%20Cao%20and%20Xu%20Chen%20and%20Ruihua%20Song%20and%20Hao%20Jiang%20and%20Guang%20Yang%20and%20Zhao%20Cao%0AAbstract%3A%20%20%20AI%20creation%2C%20such%20as%20poem%20or%20lyrics%20generation%2C%20has%20attracted%20increasing%0Aattention%20from%20both%20industry%20and%20academic%20communities%2C%20with%20many%20promising%0Amodels%20proposed%20in%20the%20past%20few%20years.%20Existing%20methods%20usually%20estimate%20the%0Aoutputs%20based%20on%20single%20and%20independent%20visual%20or%20textual%20information.%20However%2C%0Ain%20reality%2C%20humans%20usually%20make%20creations%20according%20to%20their%20experiences%2C%20which%0Amay%20involve%20different%20modalities%20and%20be%20sequentially%20correlated.%20To%20model%20such%0Ahuman%20capabilities%2C%20in%20this%20paper%2C%20we%20define%20and%20solve%20a%20novel%20AI%20creation%0Aproblem%20based%20on%20human%20experiences.%20More%20specifically%2C%20we%20study%20how%20to%20generate%0Atexts%20based%20on%20sequential%20multi-modal%20information.%20Compared%20with%20the%20previous%0Aworks%2C%20this%20task%20is%20much%20more%20difficult%20because%20the%20designed%20model%20has%20to%20well%0Aunderstand%20and%20adapt%20the%20semantics%20among%20different%20modalities%20and%20effectively%0Aconvert%20them%20into%20the%20output%20in%20a%20sequential%20manner.%20To%20alleviate%20these%0Adifficulties%2C%20we%20firstly%20design%20a%20multi-channel%20sequence-to-sequence%0Aarchitecture%20equipped%20with%20a%20multi-modal%20attention%20network.%20For%20more%20effective%0Aoptimization%2C%20we%20then%20propose%20a%20curriculum%20negative%20sampling%20strategy%20tailored%0Afor%20the%20sequential%20inputs.%20To%20benchmark%20this%20problem%20and%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%2C%20we%20manually%20labeled%20a%20new%20multi-modal%20experience%0Adataset.%20With%20this%20dataset%2C%20we%20conduct%20extensive%20experiments%20by%20comparing%20our%0Amodel%20with%20a%20series%20of%20representative%20baselines%2C%20where%20we%20can%20demonstrate%0Asignificant%20improvements%20in%20our%20model%20based%20on%20both%20automatic%20and%0Ahuman-centered%20metrics.%20The%20code%20and%20data%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Aman-4-Real/MMTG%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.02427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Experience%2520Inspired%2520AI%2520Creation%26entry.906535625%3DQian%2520Cao%2520and%2520Xu%2520Chen%2520and%2520Ruihua%2520Song%2520and%2520Hao%2520Jiang%2520and%2520Guang%2520Yang%2520and%2520Zhao%2520Cao%26entry.1292438233%3D%2520%2520AI%2520creation%252C%2520such%2520as%2520poem%2520or%2520lyrics%2520generation%252C%2520has%2520attracted%2520increasing%250Aattention%2520from%2520both%2520industry%2520and%2520academic%2520communities%252C%2520with%2520many%2520promising%250Amodels%2520proposed%2520in%2520the%2520past%2520few%2520years.%2520Existing%2520methods%2520usually%2520estimate%2520the%250Aoutputs%2520based%2520on%2520single%2520and%2520independent%2520visual%2520or%2520textual%2520information.%2520However%252C%250Ain%2520reality%252C%2520humans%2520usually%2520make%2520creations%2520according%2520to%2520their%2520experiences%252C%2520which%250Amay%2520involve%2520different%2520modalities%2520and%2520be%2520sequentially%2520correlated.%2520To%2520model%2520such%250Ahuman%2520capabilities%252C%2520in%2520this%2520paper%252C%2520we%2520define%2520and%2520solve%2520a%2520novel%2520AI%2520creation%250Aproblem%2520based%2520on%2520human%2520experiences.%2520More%2520specifically%252C%2520we%2520study%2520how%2520to%2520generate%250Atexts%2520based%2520on%2520sequential%2520multi-modal%2520information.%2520Compared%2520with%2520the%2520previous%250Aworks%252C%2520this%2520task%2520is%2520much%2520more%2520difficult%2520because%2520the%2520designed%2520model%2520has%2520to%2520well%250Aunderstand%2520and%2520adapt%2520the%2520semantics%2520among%2520different%2520modalities%2520and%2520effectively%250Aconvert%2520them%2520into%2520the%2520output%2520in%2520a%2520sequential%2520manner.%2520To%2520alleviate%2520these%250Adifficulties%252C%2520we%2520firstly%2520design%2520a%2520multi-channel%2520sequence-to-sequence%250Aarchitecture%2520equipped%2520with%2520a%2520multi-modal%2520attention%2520network.%2520For%2520more%2520effective%250Aoptimization%252C%2520we%2520then%2520propose%2520a%2520curriculum%2520negative%2520sampling%2520strategy%2520tailored%250Afor%2520the%2520sequential%2520inputs.%2520To%2520benchmark%2520this%2520problem%2520and%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520model%252C%2520we%2520manually%2520labeled%2520a%2520new%2520multi-modal%2520experience%250Adataset.%2520With%2520this%2520dataset%252C%2520we%2520conduct%2520extensive%2520experiments%2520by%2520comparing%2520our%250Amodel%2520with%2520a%2520series%2520of%2520representative%2520baselines%252C%2520where%2520we%2520can%2520demonstrate%250Asignificant%2520improvements%2520in%2520our%2520model%2520based%2520on%2520both%2520automatic%2520and%250Ahuman-centered%2520metrics.%2520The%2520code%2520and%2520data%2520are%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/Aman-4-Real/MMTG%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.02427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Experience%20Inspired%20AI%20Creation&entry.906535625=Qian%20Cao%20and%20Xu%20Chen%20and%20Ruihua%20Song%20and%20Hao%20Jiang%20and%20Guang%20Yang%20and%20Zhao%20Cao&entry.1292438233=%20%20AI%20creation%2C%20such%20as%20poem%20or%20lyrics%20generation%2C%20has%20attracted%20increasing%0Aattention%20from%20both%20industry%20and%20academic%20communities%2C%20with%20many%20promising%0Amodels%20proposed%20in%20the%20past%20few%20years.%20Existing%20methods%20usually%20estimate%20the%0Aoutputs%20based%20on%20single%20and%20independent%20visual%20or%20textual%20information.%20However%2C%0Ain%20reality%2C%20humans%20usually%20make%20creations%20according%20to%20their%20experiences%2C%20which%0Amay%20involve%20different%20modalities%20and%20be%20sequentially%20correlated.%20To%20model%20such%0Ahuman%20capabilities%2C%20in%20this%20paper%2C%20we%20define%20and%20solve%20a%20novel%20AI%20creation%0Aproblem%20based%20on%20human%20experiences.%20More%20specifically%2C%20we%20study%20how%20to%20generate%0Atexts%20based%20on%20sequential%20multi-modal%20information.%20Compared%20with%20the%20previous%0Aworks%2C%20this%20task%20is%20much%20more%20difficult%20because%20the%20designed%20model%20has%20to%20well%0Aunderstand%20and%20adapt%20the%20semantics%20among%20different%20modalities%20and%20effectively%0Aconvert%20them%20into%20the%20output%20in%20a%20sequential%20manner.%20To%20alleviate%20these%0Adifficulties%2C%20we%20firstly%20design%20a%20multi-channel%20sequence-to-sequence%0Aarchitecture%20equipped%20with%20a%20multi-modal%20attention%20network.%20For%20more%20effective%0Aoptimization%2C%20we%20then%20propose%20a%20curriculum%20negative%20sampling%20strategy%20tailored%0Afor%20the%20sequential%20inputs.%20To%20benchmark%20this%20problem%20and%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%2C%20we%20manually%20labeled%20a%20new%20multi-modal%20experience%0Adataset.%20With%20this%20dataset%2C%20we%20conduct%20extensive%20experiments%20by%20comparing%20our%0Amodel%20with%20a%20series%20of%20representative%20baselines%2C%20where%20we%20can%20demonstrate%0Asignificant%20improvements%20in%20our%20model%20based%20on%20both%20automatic%20and%0Ahuman-centered%20metrics.%20The%20code%20and%20data%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Aman-4-Real/MMTG%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.02427v2&entry.124074799=Read"},
{"title": "Multi-Head Attention Residual Unfolded Network for Model-Based\n  Pansharpening", "author": "Ivan Pereira-S\u00e1nchez and Eloi Sans and Julia Navarro and Joan Duran", "abstract": "  The objective of pansharpening and hypersharpening is to accurately combine a\nhigh-resolution panchromatic (PAN) image with a low-resolution multispectral\n(MS) or hyperspectral (HS) image, respectively. Unfolding fusion methods\nintegrate the powerful representation capabilities of deep learning with the\nrobustness of model-based approaches. These techniques involve unrolling the\nsteps of the optimization scheme derived from the minimization of an energy\ninto a deep learning framework, resulting in efficient and highly interpretable\narchitectures. In this paper, we propose a model-based deep unfolded method for\nsatellite image fusion. Our approach is based on a variational formulation that\nincorporates the classic observation model for MS/HS data, a high-frequency\ninjection constraint based on the PAN image, and an arbitrary convex prior. For\nthe unfolding stage, we introduce upsampling and downsampling layers that use\ngeometric information encoded in the PAN image through residual networks. The\nbackbone of our method is a multi-head attention residual network (MARNet),\nwhich replaces the proximity operator in the optimization scheme and combines\nmultiple head attentions with residual learning to exploit image\nself-similarities via nonlocal operators defined in terms of patches.\nAdditionally, we incorporate a post-processing module based on the MARNet\narchitecture to further enhance the quality of the fused images. Experimental\nresults on PRISMA, Quickbird, and WorldView2 datasets demonstrate the superior\nperformance of our method and its ability to generalize across different sensor\nconfigurations and varying spatial and spectral resolutions. The source code\nwill be available at https://github.com/TAMI-UIB/MARNet.\n", "link": "http://arxiv.org/abs/2409.02675v1", "date": "2024-09-04", "relevancy": 2.8318, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6092}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Head%20Attention%20Residual%20Unfolded%20Network%20for%20Model-Based%0A%20%20Pansharpening&body=Title%3A%20Multi-Head%20Attention%20Residual%20Unfolded%20Network%20for%20Model-Based%0A%20%20Pansharpening%0AAuthor%3A%20Ivan%20Pereira-S%C3%A1nchez%20and%20Eloi%20Sans%20and%20Julia%20Navarro%20and%20Joan%20Duran%0AAbstract%3A%20%20%20The%20objective%20of%20pansharpening%20and%20hypersharpening%20is%20to%20accurately%20combine%20a%0Ahigh-resolution%20panchromatic%20%28PAN%29%20image%20with%20a%20low-resolution%20multispectral%0A%28MS%29%20or%20hyperspectral%20%28HS%29%20image%2C%20respectively.%20Unfolding%20fusion%20methods%0Aintegrate%20the%20powerful%20representation%20capabilities%20of%20deep%20learning%20with%20the%0Arobustness%20of%20model-based%20approaches.%20These%20techniques%20involve%20unrolling%20the%0Asteps%20of%20the%20optimization%20scheme%20derived%20from%20the%20minimization%20of%20an%20energy%0Ainto%20a%20deep%20learning%20framework%2C%20resulting%20in%20efficient%20and%20highly%20interpretable%0Aarchitectures.%20In%20this%20paper%2C%20we%20propose%20a%20model-based%20deep%20unfolded%20method%20for%0Asatellite%20image%20fusion.%20Our%20approach%20is%20based%20on%20a%20variational%20formulation%20that%0Aincorporates%20the%20classic%20observation%20model%20for%20MS/HS%20data%2C%20a%20high-frequency%0Ainjection%20constraint%20based%20on%20the%20PAN%20image%2C%20and%20an%20arbitrary%20convex%20prior.%20For%0Athe%20unfolding%20stage%2C%20we%20introduce%20upsampling%20and%20downsampling%20layers%20that%20use%0Ageometric%20information%20encoded%20in%20the%20PAN%20image%20through%20residual%20networks.%20The%0Abackbone%20of%20our%20method%20is%20a%20multi-head%20attention%20residual%20network%20%28MARNet%29%2C%0Awhich%20replaces%20the%20proximity%20operator%20in%20the%20optimization%20scheme%20and%20combines%0Amultiple%20head%20attentions%20with%20residual%20learning%20to%20exploit%20image%0Aself-similarities%20via%20nonlocal%20operators%20defined%20in%20terms%20of%20patches.%0AAdditionally%2C%20we%20incorporate%20a%20post-processing%20module%20based%20on%20the%20MARNet%0Aarchitecture%20to%20further%20enhance%20the%20quality%20of%20the%20fused%20images.%20Experimental%0Aresults%20on%20PRISMA%2C%20Quickbird%2C%20and%20WorldView2%20datasets%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method%20and%20its%20ability%20to%20generalize%20across%20different%20sensor%0Aconfigurations%20and%20varying%20spatial%20and%20spectral%20resolutions.%20The%20source%20code%0Awill%20be%20available%20at%20https%3A//github.com/TAMI-UIB/MARNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Head%2520Attention%2520Residual%2520Unfolded%2520Network%2520for%2520Model-Based%250A%2520%2520Pansharpening%26entry.906535625%3DIvan%2520Pereira-S%25C3%25A1nchez%2520and%2520Eloi%2520Sans%2520and%2520Julia%2520Navarro%2520and%2520Joan%2520Duran%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520pansharpening%2520and%2520hypersharpening%2520is%2520to%2520accurately%2520combine%2520a%250Ahigh-resolution%2520panchromatic%2520%2528PAN%2529%2520image%2520with%2520a%2520low-resolution%2520multispectral%250A%2528MS%2529%2520or%2520hyperspectral%2520%2528HS%2529%2520image%252C%2520respectively.%2520Unfolding%2520fusion%2520methods%250Aintegrate%2520the%2520powerful%2520representation%2520capabilities%2520of%2520deep%2520learning%2520with%2520the%250Arobustness%2520of%2520model-based%2520approaches.%2520These%2520techniques%2520involve%2520unrolling%2520the%250Asteps%2520of%2520the%2520optimization%2520scheme%2520derived%2520from%2520the%2520minimization%2520of%2520an%2520energy%250Ainto%2520a%2520deep%2520learning%2520framework%252C%2520resulting%2520in%2520efficient%2520and%2520highly%2520interpretable%250Aarchitectures.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520model-based%2520deep%2520unfolded%2520method%2520for%250Asatellite%2520image%2520fusion.%2520Our%2520approach%2520is%2520based%2520on%2520a%2520variational%2520formulation%2520that%250Aincorporates%2520the%2520classic%2520observation%2520model%2520for%2520MS/HS%2520data%252C%2520a%2520high-frequency%250Ainjection%2520constraint%2520based%2520on%2520the%2520PAN%2520image%252C%2520and%2520an%2520arbitrary%2520convex%2520prior.%2520For%250Athe%2520unfolding%2520stage%252C%2520we%2520introduce%2520upsampling%2520and%2520downsampling%2520layers%2520that%2520use%250Ageometric%2520information%2520encoded%2520in%2520the%2520PAN%2520image%2520through%2520residual%2520networks.%2520The%250Abackbone%2520of%2520our%2520method%2520is%2520a%2520multi-head%2520attention%2520residual%2520network%2520%2528MARNet%2529%252C%250Awhich%2520replaces%2520the%2520proximity%2520operator%2520in%2520the%2520optimization%2520scheme%2520and%2520combines%250Amultiple%2520head%2520attentions%2520with%2520residual%2520learning%2520to%2520exploit%2520image%250Aself-similarities%2520via%2520nonlocal%2520operators%2520defined%2520in%2520terms%2520of%2520patches.%250AAdditionally%252C%2520we%2520incorporate%2520a%2520post-processing%2520module%2520based%2520on%2520the%2520MARNet%250Aarchitecture%2520to%2520further%2520enhance%2520the%2520quality%2520of%2520the%2520fused%2520images.%2520Experimental%250Aresults%2520on%2520PRISMA%252C%2520Quickbird%252C%2520and%2520WorldView2%2520datasets%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520and%2520its%2520ability%2520to%2520generalize%2520across%2520different%2520sensor%250Aconfigurations%2520and%2520varying%2520spatial%2520and%2520spectral%2520resolutions.%2520The%2520source%2520code%250Awill%2520be%2520available%2520at%2520https%253A//github.com/TAMI-UIB/MARNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Head%20Attention%20Residual%20Unfolded%20Network%20for%20Model-Based%0A%20%20Pansharpening&entry.906535625=Ivan%20Pereira-S%C3%A1nchez%20and%20Eloi%20Sans%20and%20Julia%20Navarro%20and%20Joan%20Duran&entry.1292438233=%20%20The%20objective%20of%20pansharpening%20and%20hypersharpening%20is%20to%20accurately%20combine%20a%0Ahigh-resolution%20panchromatic%20%28PAN%29%20image%20with%20a%20low-resolution%20multispectral%0A%28MS%29%20or%20hyperspectral%20%28HS%29%20image%2C%20respectively.%20Unfolding%20fusion%20methods%0Aintegrate%20the%20powerful%20representation%20capabilities%20of%20deep%20learning%20with%20the%0Arobustness%20of%20model-based%20approaches.%20These%20techniques%20involve%20unrolling%20the%0Asteps%20of%20the%20optimization%20scheme%20derived%20from%20the%20minimization%20of%20an%20energy%0Ainto%20a%20deep%20learning%20framework%2C%20resulting%20in%20efficient%20and%20highly%20interpretable%0Aarchitectures.%20In%20this%20paper%2C%20we%20propose%20a%20model-based%20deep%20unfolded%20method%20for%0Asatellite%20image%20fusion.%20Our%20approach%20is%20based%20on%20a%20variational%20formulation%20that%0Aincorporates%20the%20classic%20observation%20model%20for%20MS/HS%20data%2C%20a%20high-frequency%0Ainjection%20constraint%20based%20on%20the%20PAN%20image%2C%20and%20an%20arbitrary%20convex%20prior.%20For%0Athe%20unfolding%20stage%2C%20we%20introduce%20upsampling%20and%20downsampling%20layers%20that%20use%0Ageometric%20information%20encoded%20in%20the%20PAN%20image%20through%20residual%20networks.%20The%0Abackbone%20of%20our%20method%20is%20a%20multi-head%20attention%20residual%20network%20%28MARNet%29%2C%0Awhich%20replaces%20the%20proximity%20operator%20in%20the%20optimization%20scheme%20and%20combines%0Amultiple%20head%20attentions%20with%20residual%20learning%20to%20exploit%20image%0Aself-similarities%20via%20nonlocal%20operators%20defined%20in%20terms%20of%20patches.%0AAdditionally%2C%20we%20incorporate%20a%20post-processing%20module%20based%20on%20the%20MARNet%0Aarchitecture%20to%20further%20enhance%20the%20quality%20of%20the%20fused%20images.%20Experimental%0Aresults%20on%20PRISMA%2C%20Quickbird%2C%20and%20WorldView2%20datasets%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method%20and%20its%20ability%20to%20generalize%20across%20different%20sensor%0Aconfigurations%20and%20varying%20spatial%20and%20spectral%20resolutions.%20The%20source%20code%0Awill%20be%20available%20at%20https%3A//github.com/TAMI-UIB/MARNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02675v1&entry.124074799=Read"},
{"title": "SOAR: Simultaneous Exploration and Photographing with Heterogeneous UAVs\n  for Fast Autonomous Reconstruction", "author": "Mingjie Zhang and Chen Feng and Zengzhi Li and Guiyong Zheng and Yiming Luo and Zhu Wang and Jinni Zhou and Shaojie Shen and Boyu Zhou", "abstract": "  Unmanned Aerial Vehicles (UAVs) have gained significant popularity in scene\nreconstruction. This paper presents SOAR, a LiDAR-Visual heterogeneous\nmulti-UAV system specifically designed for fast autonomous reconstruction of\ncomplex environments. Our system comprises a LiDAR-equipped explorer with a\nlarge field-of-view (FoV), alongside photographers equipped with cameras. To\nensure rapid acquisition of the scene's surface geometry, we employ a surface\nfrontier-based exploration strategy for the explorer. As the surface is\nprogressively explored, we identify the uncovered areas and generate viewpoints\nincrementally. These viewpoints are then assigned to photographers through\nsolving a Consistent Multiple Depot Multiple Traveling Salesman Problem\n(Consistent-MDMTSP), which optimizes scanning efficiency while ensuring task\nconsistency. Finally, photographers utilize the assigned viewpoints to\ndetermine optimal coverage paths for acquiring images. We present extensive\nbenchmarks in the realistic simulator, which validates the performance of SOAR\ncompared with classical and state-of-the-art methods. For more details, please\nsee our project page at\nhttps://sysu-star.github.io/SOAR}{sysu-star.github.io/SOAR.\n", "link": "http://arxiv.org/abs/2409.02738v1", "date": "2024-09-04", "relevancy": 2.8021, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5743}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5611}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOAR%3A%20Simultaneous%20Exploration%20and%20Photographing%20with%20Heterogeneous%20UAVs%0A%20%20for%20Fast%20Autonomous%20Reconstruction&body=Title%3A%20SOAR%3A%20Simultaneous%20Exploration%20and%20Photographing%20with%20Heterogeneous%20UAVs%0A%20%20for%20Fast%20Autonomous%20Reconstruction%0AAuthor%3A%20Mingjie%20Zhang%20and%20Chen%20Feng%20and%20Zengzhi%20Li%20and%20Guiyong%20Zheng%20and%20Yiming%20Luo%20and%20Zhu%20Wang%20and%20Jinni%20Zhou%20and%20Shaojie%20Shen%20and%20Boyu%20Zhou%0AAbstract%3A%20%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20have%20gained%20significant%20popularity%20in%20scene%0Areconstruction.%20This%20paper%20presents%20SOAR%2C%20a%20LiDAR-Visual%20heterogeneous%0Amulti-UAV%20system%20specifically%20designed%20for%20fast%20autonomous%20reconstruction%20of%0Acomplex%20environments.%20Our%20system%20comprises%20a%20LiDAR-equipped%20explorer%20with%20a%0Alarge%20field-of-view%20%28FoV%29%2C%20alongside%20photographers%20equipped%20with%20cameras.%20To%0Aensure%20rapid%20acquisition%20of%20the%20scene%27s%20surface%20geometry%2C%20we%20employ%20a%20surface%0Afrontier-based%20exploration%20strategy%20for%20the%20explorer.%20As%20the%20surface%20is%0Aprogressively%20explored%2C%20we%20identify%20the%20uncovered%20areas%20and%20generate%20viewpoints%0Aincrementally.%20These%20viewpoints%20are%20then%20assigned%20to%20photographers%20through%0Asolving%20a%20Consistent%20Multiple%20Depot%20Multiple%20Traveling%20Salesman%20Problem%0A%28Consistent-MDMTSP%29%2C%20which%20optimizes%20scanning%20efficiency%20while%20ensuring%20task%0Aconsistency.%20Finally%2C%20photographers%20utilize%20the%20assigned%20viewpoints%20to%0Adetermine%20optimal%20coverage%20paths%20for%20acquiring%20images.%20We%20present%20extensive%0Abenchmarks%20in%20the%20realistic%20simulator%2C%20which%20validates%20the%20performance%20of%20SOAR%0Acompared%20with%20classical%20and%20state-of-the-art%20methods.%20For%20more%20details%2C%20please%0Asee%20our%20project%20page%20at%0Ahttps%3A//sysu-star.github.io/SOAR%7D%7Bsysu-star.github.io/SOAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOAR%253A%2520Simultaneous%2520Exploration%2520and%2520Photographing%2520with%2520Heterogeneous%2520UAVs%250A%2520%2520for%2520Fast%2520Autonomous%2520Reconstruction%26entry.906535625%3DMingjie%2520Zhang%2520and%2520Chen%2520Feng%2520and%2520Zengzhi%2520Li%2520and%2520Guiyong%2520Zheng%2520and%2520Yiming%2520Luo%2520and%2520Zhu%2520Wang%2520and%2520Jinni%2520Zhou%2520and%2520Shaojie%2520Shen%2520and%2520Boyu%2520Zhou%26entry.1292438233%3D%2520%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520have%2520gained%2520significant%2520popularity%2520in%2520scene%250Areconstruction.%2520This%2520paper%2520presents%2520SOAR%252C%2520a%2520LiDAR-Visual%2520heterogeneous%250Amulti-UAV%2520system%2520specifically%2520designed%2520for%2520fast%2520autonomous%2520reconstruction%2520of%250Acomplex%2520environments.%2520Our%2520system%2520comprises%2520a%2520LiDAR-equipped%2520explorer%2520with%2520a%250Alarge%2520field-of-view%2520%2528FoV%2529%252C%2520alongside%2520photographers%2520equipped%2520with%2520cameras.%2520To%250Aensure%2520rapid%2520acquisition%2520of%2520the%2520scene%2527s%2520surface%2520geometry%252C%2520we%2520employ%2520a%2520surface%250Afrontier-based%2520exploration%2520strategy%2520for%2520the%2520explorer.%2520As%2520the%2520surface%2520is%250Aprogressively%2520explored%252C%2520we%2520identify%2520the%2520uncovered%2520areas%2520and%2520generate%2520viewpoints%250Aincrementally.%2520These%2520viewpoints%2520are%2520then%2520assigned%2520to%2520photographers%2520through%250Asolving%2520a%2520Consistent%2520Multiple%2520Depot%2520Multiple%2520Traveling%2520Salesman%2520Problem%250A%2528Consistent-MDMTSP%2529%252C%2520which%2520optimizes%2520scanning%2520efficiency%2520while%2520ensuring%2520task%250Aconsistency.%2520Finally%252C%2520photographers%2520utilize%2520the%2520assigned%2520viewpoints%2520to%250Adetermine%2520optimal%2520coverage%2520paths%2520for%2520acquiring%2520images.%2520We%2520present%2520extensive%250Abenchmarks%2520in%2520the%2520realistic%2520simulator%252C%2520which%2520validates%2520the%2520performance%2520of%2520SOAR%250Acompared%2520with%2520classical%2520and%2520state-of-the-art%2520methods.%2520For%2520more%2520details%252C%2520please%250Asee%2520our%2520project%2520page%2520at%250Ahttps%253A//sysu-star.github.io/SOAR%257D%257Bsysu-star.github.io/SOAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOAR%3A%20Simultaneous%20Exploration%20and%20Photographing%20with%20Heterogeneous%20UAVs%0A%20%20for%20Fast%20Autonomous%20Reconstruction&entry.906535625=Mingjie%20Zhang%20and%20Chen%20Feng%20and%20Zengzhi%20Li%20and%20Guiyong%20Zheng%20and%20Yiming%20Luo%20and%20Zhu%20Wang%20and%20Jinni%20Zhou%20and%20Shaojie%20Shen%20and%20Boyu%20Zhou&entry.1292438233=%20%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20have%20gained%20significant%20popularity%20in%20scene%0Areconstruction.%20This%20paper%20presents%20SOAR%2C%20a%20LiDAR-Visual%20heterogeneous%0Amulti-UAV%20system%20specifically%20designed%20for%20fast%20autonomous%20reconstruction%20of%0Acomplex%20environments.%20Our%20system%20comprises%20a%20LiDAR-equipped%20explorer%20with%20a%0Alarge%20field-of-view%20%28FoV%29%2C%20alongside%20photographers%20equipped%20with%20cameras.%20To%0Aensure%20rapid%20acquisition%20of%20the%20scene%27s%20surface%20geometry%2C%20we%20employ%20a%20surface%0Afrontier-based%20exploration%20strategy%20for%20the%20explorer.%20As%20the%20surface%20is%0Aprogressively%20explored%2C%20we%20identify%20the%20uncovered%20areas%20and%20generate%20viewpoints%0Aincrementally.%20These%20viewpoints%20are%20then%20assigned%20to%20photographers%20through%0Asolving%20a%20Consistent%20Multiple%20Depot%20Multiple%20Traveling%20Salesman%20Problem%0A%28Consistent-MDMTSP%29%2C%20which%20optimizes%20scanning%20efficiency%20while%20ensuring%20task%0Aconsistency.%20Finally%2C%20photographers%20utilize%20the%20assigned%20viewpoints%20to%0Adetermine%20optimal%20coverage%20paths%20for%20acquiring%20images.%20We%20present%20extensive%0Abenchmarks%20in%20the%20realistic%20simulator%2C%20which%20validates%20the%20performance%20of%20SOAR%0Acompared%20with%20classical%20and%20state-of-the-art%20methods.%20For%20more%20details%2C%20please%0Asee%20our%20project%20page%20at%0Ahttps%3A//sysu-star.github.io/SOAR%7D%7Bsysu-star.github.io/SOAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02738v1&entry.124074799=Read"},
{"title": "Standing on the Shoulders of Giants: Reprogramming Visual-Language Model\n  for General Deepfake Detection", "author": "Kaiqing Lin and Yuzhen Lin and Weixiang Li and Taiping Yao and Bin Li", "abstract": "  The proliferation of deepfake faces poses huge potential negative impacts on\nour daily lives. Despite substantial advancements in deepfake detection over\nthese years, the generalizability of existing methods against forgeries from\nunseen datasets or created by emerging generative models remains constrained.\nIn this paper, inspired by the zero-shot advantages of Vision-Language Models\n(VLMs), we propose a novel approach that repurposes a well-trained VLM for\ngeneral deepfake detection. Motivated by the model reprogramming paradigm that\nmanipulates the model prediction via data perturbations, our method can\nreprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its\ninput without tuning the inner parameters. Furthermore, we insert a pseudo-word\nguided by facial identity into the text prompt. Extensive experiments on\nseveral popular benchmarks demonstrate that (1) the cross-dataset and\ncross-manipulation performances of deepfake detection can be significantly and\nconsistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to\nWildDeepfake) using a pre-trained CLIP model with our proposed reprogramming\nmethod; (2) our superior performances are at less cost of trainable parameters,\nmaking it a promising approach for real-world applications.\n", "link": "http://arxiv.org/abs/2409.02664v1", "date": "2024-09-04", "relevancy": 2.7488, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standing%20on%20the%20Shoulders%20of%20Giants%3A%20Reprogramming%20Visual-Language%20Model%0A%20%20for%20General%20Deepfake%20Detection&body=Title%3A%20Standing%20on%20the%20Shoulders%20of%20Giants%3A%20Reprogramming%20Visual-Language%20Model%0A%20%20for%20General%20Deepfake%20Detection%0AAuthor%3A%20Kaiqing%20Lin%20and%20Yuzhen%20Lin%20and%20Weixiang%20Li%20and%20Taiping%20Yao%20and%20Bin%20Li%0AAbstract%3A%20%20%20The%20proliferation%20of%20deepfake%20faces%20poses%20huge%20potential%20negative%20impacts%20on%0Aour%20daily%20lives.%20Despite%20substantial%20advancements%20in%20deepfake%20detection%20over%0Athese%20years%2C%20the%20generalizability%20of%20existing%20methods%20against%20forgeries%20from%0Aunseen%20datasets%20or%20created%20by%20emerging%20generative%20models%20remains%20constrained.%0AIn%20this%20paper%2C%20inspired%20by%20the%20zero-shot%20advantages%20of%20Vision-Language%20Models%0A%28VLMs%29%2C%20we%20propose%20a%20novel%20approach%20that%20repurposes%20a%20well-trained%20VLM%20for%0Ageneral%20deepfake%20detection.%20Motivated%20by%20the%20model%20reprogramming%20paradigm%20that%0Amanipulates%20the%20model%20prediction%20via%20data%20perturbations%2C%20our%20method%20can%0Areprogram%20a%20pretrained%20VLM%20model%20%28e.g.%2C%20CLIP%29%20solely%20based%20on%20manipulating%20its%0Ainput%20without%20tuning%20the%20inner%20parameters.%20Furthermore%2C%20we%20insert%20a%20pseudo-word%0Aguided%20by%20facial%20identity%20into%20the%20text%20prompt.%20Extensive%20experiments%20on%0Aseveral%20popular%20benchmarks%20demonstrate%20that%20%281%29%20the%20cross-dataset%20and%0Across-manipulation%20performances%20of%20deepfake%20detection%20can%20be%20significantly%20and%0Aconsistently%20improved%20%28e.g.%2C%20over%2088%25%20AUC%20in%20cross-dataset%20setting%20from%20FF%2B%2B%20to%0AWildDeepfake%29%20using%20a%20pre-trained%20CLIP%20model%20with%20our%20proposed%20reprogramming%0Amethod%3B%20%282%29%20our%20superior%20performances%20are%20at%20less%20cost%20of%20trainable%20parameters%2C%0Amaking%20it%20a%20promising%20approach%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStanding%2520on%2520the%2520Shoulders%2520of%2520Giants%253A%2520Reprogramming%2520Visual-Language%2520Model%250A%2520%2520for%2520General%2520Deepfake%2520Detection%26entry.906535625%3DKaiqing%2520Lin%2520and%2520Yuzhen%2520Lin%2520and%2520Weixiang%2520Li%2520and%2520Taiping%2520Yao%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520deepfake%2520faces%2520poses%2520huge%2520potential%2520negative%2520impacts%2520on%250Aour%2520daily%2520lives.%2520Despite%2520substantial%2520advancements%2520in%2520deepfake%2520detection%2520over%250Athese%2520years%252C%2520the%2520generalizability%2520of%2520existing%2520methods%2520against%2520forgeries%2520from%250Aunseen%2520datasets%2520or%2520created%2520by%2520emerging%2520generative%2520models%2520remains%2520constrained.%250AIn%2520this%2520paper%252C%2520inspired%2520by%2520the%2520zero-shot%2520advantages%2520of%2520Vision-Language%2520Models%250A%2528VLMs%2529%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520repurposes%2520a%2520well-trained%2520VLM%2520for%250Ageneral%2520deepfake%2520detection.%2520Motivated%2520by%2520the%2520model%2520reprogramming%2520paradigm%2520that%250Amanipulates%2520the%2520model%2520prediction%2520via%2520data%2520perturbations%252C%2520our%2520method%2520can%250Areprogram%2520a%2520pretrained%2520VLM%2520model%2520%2528e.g.%252C%2520CLIP%2529%2520solely%2520based%2520on%2520manipulating%2520its%250Ainput%2520without%2520tuning%2520the%2520inner%2520parameters.%2520Furthermore%252C%2520we%2520insert%2520a%2520pseudo-word%250Aguided%2520by%2520facial%2520identity%2520into%2520the%2520text%2520prompt.%2520Extensive%2520experiments%2520on%250Aseveral%2520popular%2520benchmarks%2520demonstrate%2520that%2520%25281%2529%2520the%2520cross-dataset%2520and%250Across-manipulation%2520performances%2520of%2520deepfake%2520detection%2520can%2520be%2520significantly%2520and%250Aconsistently%2520improved%2520%2528e.g.%252C%2520over%252088%2525%2520AUC%2520in%2520cross-dataset%2520setting%2520from%2520FF%252B%252B%2520to%250AWildDeepfake%2529%2520using%2520a%2520pre-trained%2520CLIP%2520model%2520with%2520our%2520proposed%2520reprogramming%250Amethod%253B%2520%25282%2529%2520our%2520superior%2520performances%2520are%2520at%2520less%2520cost%2520of%2520trainable%2520parameters%252C%250Amaking%2520it%2520a%2520promising%2520approach%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standing%20on%20the%20Shoulders%20of%20Giants%3A%20Reprogramming%20Visual-Language%20Model%0A%20%20for%20General%20Deepfake%20Detection&entry.906535625=Kaiqing%20Lin%20and%20Yuzhen%20Lin%20and%20Weixiang%20Li%20and%20Taiping%20Yao%20and%20Bin%20Li&entry.1292438233=%20%20The%20proliferation%20of%20deepfake%20faces%20poses%20huge%20potential%20negative%20impacts%20on%0Aour%20daily%20lives.%20Despite%20substantial%20advancements%20in%20deepfake%20detection%20over%0Athese%20years%2C%20the%20generalizability%20of%20existing%20methods%20against%20forgeries%20from%0Aunseen%20datasets%20or%20created%20by%20emerging%20generative%20models%20remains%20constrained.%0AIn%20this%20paper%2C%20inspired%20by%20the%20zero-shot%20advantages%20of%20Vision-Language%20Models%0A%28VLMs%29%2C%20we%20propose%20a%20novel%20approach%20that%20repurposes%20a%20well-trained%20VLM%20for%0Ageneral%20deepfake%20detection.%20Motivated%20by%20the%20model%20reprogramming%20paradigm%20that%0Amanipulates%20the%20model%20prediction%20via%20data%20perturbations%2C%20our%20method%20can%0Areprogram%20a%20pretrained%20VLM%20model%20%28e.g.%2C%20CLIP%29%20solely%20based%20on%20manipulating%20its%0Ainput%20without%20tuning%20the%20inner%20parameters.%20Furthermore%2C%20we%20insert%20a%20pseudo-word%0Aguided%20by%20facial%20identity%20into%20the%20text%20prompt.%20Extensive%20experiments%20on%0Aseveral%20popular%20benchmarks%20demonstrate%20that%20%281%29%20the%20cross-dataset%20and%0Across-manipulation%20performances%20of%20deepfake%20detection%20can%20be%20significantly%20and%0Aconsistently%20improved%20%28e.g.%2C%20over%2088%25%20AUC%20in%20cross-dataset%20setting%20from%20FF%2B%2B%20to%0AWildDeepfake%29%20using%20a%20pre-trained%20CLIP%20model%20with%20our%20proposed%20reprogramming%0Amethod%3B%20%282%29%20our%20superior%20performances%20are%20at%20less%20cost%20of%20trainable%20parameters%2C%0Amaking%20it%20a%20promising%20approach%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02664v1&entry.124074799=Read"},
{"title": "Deep Learning Meets Satellite Images -- An Evaluation on Handcrafted and\n  Learning-based Features for Multi-date Satellite Stereo Images", "author": "Shuang Song and Luca Morelli and Xinyi Wu and Rongjun Qin and Hessah Albanwan and Fabio Remondino", "abstract": "  A critical step in the digital surface models(DSM) generation is feature\nmatching. Off-track (or multi-date) satellite stereo images, in particular, can\nchallenge the performance of feature matching due to spectral distortions\nbetween images, long baseline, and wide intersection angles. Feature matching\nmethods have evolved over the years from handcrafted methods (e.g., SIFT) to\nlearning-based methods (e.g., SuperPoint and SuperGlue). In this paper, we\ncompare the performance of different features, also known as feature extraction\nand matching methods, applied to satellite imagery. A wide range of stereo\npairs(~500) covering two separate study sites are used. SIFT, as a widely used\nclassic feature extraction and matching algorithm, is compared with seven\ndeep-learning matching methods: SuperGlue, LightGlue, LoFTR, ASpanFormer, DKM,\nGIM-LightGlue, and GIM-DKM. Results demonstrate that traditional matching\nmethods are still competitive in this age of deep learning, although for\nparticular scenarios learning-based methods are very promising.\n", "link": "http://arxiv.org/abs/2409.02825v1", "date": "2024-09-04", "relevancy": 2.6957, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5644}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5331}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Meets%20Satellite%20Images%20--%20An%20Evaluation%20on%20Handcrafted%20and%0A%20%20Learning-based%20Features%20for%20Multi-date%20Satellite%20Stereo%20Images&body=Title%3A%20Deep%20Learning%20Meets%20Satellite%20Images%20--%20An%20Evaluation%20on%20Handcrafted%20and%0A%20%20Learning-based%20Features%20for%20Multi-date%20Satellite%20Stereo%20Images%0AAuthor%3A%20Shuang%20Song%20and%20Luca%20Morelli%20and%20Xinyi%20Wu%20and%20Rongjun%20Qin%20and%20Hessah%20Albanwan%20and%20Fabio%20Remondino%0AAbstract%3A%20%20%20A%20critical%20step%20in%20the%20digital%20surface%20models%28DSM%29%20generation%20is%20feature%0Amatching.%20Off-track%20%28or%20multi-date%29%20satellite%20stereo%20images%2C%20in%20particular%2C%20can%0Achallenge%20the%20performance%20of%20feature%20matching%20due%20to%20spectral%20distortions%0Abetween%20images%2C%20long%20baseline%2C%20and%20wide%20intersection%20angles.%20Feature%20matching%0Amethods%20have%20evolved%20over%20the%20years%20from%20handcrafted%20methods%20%28e.g.%2C%20SIFT%29%20to%0Alearning-based%20methods%20%28e.g.%2C%20SuperPoint%20and%20SuperGlue%29.%20In%20this%20paper%2C%20we%0Acompare%20the%20performance%20of%20different%20features%2C%20also%20known%20as%20feature%20extraction%0Aand%20matching%20methods%2C%20applied%20to%20satellite%20imagery.%20A%20wide%20range%20of%20stereo%0Apairs%28~500%29%20covering%20two%20separate%20study%20sites%20are%20used.%20SIFT%2C%20as%20a%20widely%20used%0Aclassic%20feature%20extraction%20and%20matching%20algorithm%2C%20is%20compared%20with%20seven%0Adeep-learning%20matching%20methods%3A%20SuperGlue%2C%20LightGlue%2C%20LoFTR%2C%20ASpanFormer%2C%20DKM%2C%0AGIM-LightGlue%2C%20and%20GIM-DKM.%20Results%20demonstrate%20that%20traditional%20matching%0Amethods%20are%20still%20competitive%20in%20this%20age%20of%20deep%20learning%2C%20although%20for%0Aparticular%20scenarios%20learning-based%20methods%20are%20very%20promising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Meets%2520Satellite%2520Images%2520--%2520An%2520Evaluation%2520on%2520Handcrafted%2520and%250A%2520%2520Learning-based%2520Features%2520for%2520Multi-date%2520Satellite%2520Stereo%2520Images%26entry.906535625%3DShuang%2520Song%2520and%2520Luca%2520Morelli%2520and%2520Xinyi%2520Wu%2520and%2520Rongjun%2520Qin%2520and%2520Hessah%2520Albanwan%2520and%2520Fabio%2520Remondino%26entry.1292438233%3D%2520%2520A%2520critical%2520step%2520in%2520the%2520digital%2520surface%2520models%2528DSM%2529%2520generation%2520is%2520feature%250Amatching.%2520Off-track%2520%2528or%2520multi-date%2529%2520satellite%2520stereo%2520images%252C%2520in%2520particular%252C%2520can%250Achallenge%2520the%2520performance%2520of%2520feature%2520matching%2520due%2520to%2520spectral%2520distortions%250Abetween%2520images%252C%2520long%2520baseline%252C%2520and%2520wide%2520intersection%2520angles.%2520Feature%2520matching%250Amethods%2520have%2520evolved%2520over%2520the%2520years%2520from%2520handcrafted%2520methods%2520%2528e.g.%252C%2520SIFT%2529%2520to%250Alearning-based%2520methods%2520%2528e.g.%252C%2520SuperPoint%2520and%2520SuperGlue%2529.%2520In%2520this%2520paper%252C%2520we%250Acompare%2520the%2520performance%2520of%2520different%2520features%252C%2520also%2520known%2520as%2520feature%2520extraction%250Aand%2520matching%2520methods%252C%2520applied%2520to%2520satellite%2520imagery.%2520A%2520wide%2520range%2520of%2520stereo%250Apairs%2528~500%2529%2520covering%2520two%2520separate%2520study%2520sites%2520are%2520used.%2520SIFT%252C%2520as%2520a%2520widely%2520used%250Aclassic%2520feature%2520extraction%2520and%2520matching%2520algorithm%252C%2520is%2520compared%2520with%2520seven%250Adeep-learning%2520matching%2520methods%253A%2520SuperGlue%252C%2520LightGlue%252C%2520LoFTR%252C%2520ASpanFormer%252C%2520DKM%252C%250AGIM-LightGlue%252C%2520and%2520GIM-DKM.%2520Results%2520demonstrate%2520that%2520traditional%2520matching%250Amethods%2520are%2520still%2520competitive%2520in%2520this%2520age%2520of%2520deep%2520learning%252C%2520although%2520for%250Aparticular%2520scenarios%2520learning-based%2520methods%2520are%2520very%2520promising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Meets%20Satellite%20Images%20--%20An%20Evaluation%20on%20Handcrafted%20and%0A%20%20Learning-based%20Features%20for%20Multi-date%20Satellite%20Stereo%20Images&entry.906535625=Shuang%20Song%20and%20Luca%20Morelli%20and%20Xinyi%20Wu%20and%20Rongjun%20Qin%20and%20Hessah%20Albanwan%20and%20Fabio%20Remondino&entry.1292438233=%20%20A%20critical%20step%20in%20the%20digital%20surface%20models%28DSM%29%20generation%20is%20feature%0Amatching.%20Off-track%20%28or%20multi-date%29%20satellite%20stereo%20images%2C%20in%20particular%2C%20can%0Achallenge%20the%20performance%20of%20feature%20matching%20due%20to%20spectral%20distortions%0Abetween%20images%2C%20long%20baseline%2C%20and%20wide%20intersection%20angles.%20Feature%20matching%0Amethods%20have%20evolved%20over%20the%20years%20from%20handcrafted%20methods%20%28e.g.%2C%20SIFT%29%20to%0Alearning-based%20methods%20%28e.g.%2C%20SuperPoint%20and%20SuperGlue%29.%20In%20this%20paper%2C%20we%0Acompare%20the%20performance%20of%20different%20features%2C%20also%20known%20as%20feature%20extraction%0Aand%20matching%20methods%2C%20applied%20to%20satellite%20imagery.%20A%20wide%20range%20of%20stereo%0Apairs%28~500%29%20covering%20two%20separate%20study%20sites%20are%20used.%20SIFT%2C%20as%20a%20widely%20used%0Aclassic%20feature%20extraction%20and%20matching%20algorithm%2C%20is%20compared%20with%20seven%0Adeep-learning%20matching%20methods%3A%20SuperGlue%2C%20LightGlue%2C%20LoFTR%2C%20ASpanFormer%2C%20DKM%2C%0AGIM-LightGlue%2C%20and%20GIM-DKM.%20Results%20demonstrate%20that%20traditional%20matching%0Amethods%20are%20still%20competitive%20in%20this%20age%20of%20deep%20learning%2C%20although%20for%0Aparticular%20scenarios%20learning-based%20methods%20are%20very%20promising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02825v1&entry.124074799=Read"},
{"title": "The Impact of Balancing Real and Synthetic Data on Accuracy and Fairness\n  in Face Recognition", "author": "Andrea Atzori and Pietro Cosseddu and Gianni Fenu and Mirko Marras", "abstract": "  Over the recent years, the advancements in deep face recognition have fueled\nan increasing demand for large and diverse datasets. Nevertheless, the\nauthentic data acquired to create those datasets is typically sourced from the\nweb, which, in many cases, can lead to significant privacy issues due to the\nlack of explicit user consent. Furthermore, obtaining a demographically\nbalanced, large dataset is even more difficult because of the natural imbalance\nin the distribution of images from different demographic groups. In this paper,\nwe investigate the impact of demographically balanced authentic and synthetic\ndata, both individually and in combination, on the accuracy and fairness of\nface recognition models. Initially, several generative methods were used to\nbalance the demographic representations of the corresponding synthetic\ndatasets. Then a state-of-the-art face encoder was trained and evaluated using\n(combinations of) synthetic and authentic images. Our findings emphasized two\nmain points: (i) the increased effectiveness of training data generated by\ndiffusion-based models in enhancing accuracy, whether used alone or combined\nwith subsets of authentic data, and (ii) the minimal impact of incorporating\nbalanced data from pre-trained generative methods on fairness (in nearly all\ntested scenarios using combined datasets, fairness scores remained either\nunchanged or worsened, even when compared to unbalanced authentic datasets).\nSource code and data are available at \\url{https://cutt.ly/AeQy1K5G} for\nreproducibility.\n", "link": "http://arxiv.org/abs/2409.02867v1", "date": "2024-09-04", "relevancy": 2.6894, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5441}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5359}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Balancing%20Real%20and%20Synthetic%20Data%20on%20Accuracy%20and%20Fairness%0A%20%20in%20Face%20Recognition&body=Title%3A%20The%20Impact%20of%20Balancing%20Real%20and%20Synthetic%20Data%20on%20Accuracy%20and%20Fairness%0A%20%20in%20Face%20Recognition%0AAuthor%3A%20Andrea%20Atzori%20and%20Pietro%20Cosseddu%20and%20Gianni%20Fenu%20and%20Mirko%20Marras%0AAbstract%3A%20%20%20Over%20the%20recent%20years%2C%20the%20advancements%20in%20deep%20face%20recognition%20have%20fueled%0Aan%20increasing%20demand%20for%20large%20and%20diverse%20datasets.%20Nevertheless%2C%20the%0Aauthentic%20data%20acquired%20to%20create%20those%20datasets%20is%20typically%20sourced%20from%20the%0Aweb%2C%20which%2C%20in%20many%20cases%2C%20can%20lead%20to%20significant%20privacy%20issues%20due%20to%20the%0Alack%20of%20explicit%20user%20consent.%20Furthermore%2C%20obtaining%20a%20demographically%0Abalanced%2C%20large%20dataset%20is%20even%20more%20difficult%20because%20of%20the%20natural%20imbalance%0Ain%20the%20distribution%20of%20images%20from%20different%20demographic%20groups.%20In%20this%20paper%2C%0Awe%20investigate%20the%20impact%20of%20demographically%20balanced%20authentic%20and%20synthetic%0Adata%2C%20both%20individually%20and%20in%20combination%2C%20on%20the%20accuracy%20and%20fairness%20of%0Aface%20recognition%20models.%20Initially%2C%20several%20generative%20methods%20were%20used%20to%0Abalance%20the%20demographic%20representations%20of%20the%20corresponding%20synthetic%0Adatasets.%20Then%20a%20state-of-the-art%20face%20encoder%20was%20trained%20and%20evaluated%20using%0A%28combinations%20of%29%20synthetic%20and%20authentic%20images.%20Our%20findings%20emphasized%20two%0Amain%20points%3A%20%28i%29%20the%20increased%20effectiveness%20of%20training%20data%20generated%20by%0Adiffusion-based%20models%20in%20enhancing%20accuracy%2C%20whether%20used%20alone%20or%20combined%0Awith%20subsets%20of%20authentic%20data%2C%20and%20%28ii%29%20the%20minimal%20impact%20of%20incorporating%0Abalanced%20data%20from%20pre-trained%20generative%20methods%20on%20fairness%20%28in%20nearly%20all%0Atested%20scenarios%20using%20combined%20datasets%2C%20fairness%20scores%20remained%20either%0Aunchanged%20or%20worsened%2C%20even%20when%20compared%20to%20unbalanced%20authentic%20datasets%29.%0ASource%20code%20and%20data%20are%20available%20at%20%5Curl%7Bhttps%3A//cutt.ly/AeQy1K5G%7D%20for%0Areproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Balancing%2520Real%2520and%2520Synthetic%2520Data%2520on%2520Accuracy%2520and%2520Fairness%250A%2520%2520in%2520Face%2520Recognition%26entry.906535625%3DAndrea%2520Atzori%2520and%2520Pietro%2520Cosseddu%2520and%2520Gianni%2520Fenu%2520and%2520Mirko%2520Marras%26entry.1292438233%3D%2520%2520Over%2520the%2520recent%2520years%252C%2520the%2520advancements%2520in%2520deep%2520face%2520recognition%2520have%2520fueled%250Aan%2520increasing%2520demand%2520for%2520large%2520and%2520diverse%2520datasets.%2520Nevertheless%252C%2520the%250Aauthentic%2520data%2520acquired%2520to%2520create%2520those%2520datasets%2520is%2520typically%2520sourced%2520from%2520the%250Aweb%252C%2520which%252C%2520in%2520many%2520cases%252C%2520can%2520lead%2520to%2520significant%2520privacy%2520issues%2520due%2520to%2520the%250Alack%2520of%2520explicit%2520user%2520consent.%2520Furthermore%252C%2520obtaining%2520a%2520demographically%250Abalanced%252C%2520large%2520dataset%2520is%2520even%2520more%2520difficult%2520because%2520of%2520the%2520natural%2520imbalance%250Ain%2520the%2520distribution%2520of%2520images%2520from%2520different%2520demographic%2520groups.%2520In%2520this%2520paper%252C%250Awe%2520investigate%2520the%2520impact%2520of%2520demographically%2520balanced%2520authentic%2520and%2520synthetic%250Adata%252C%2520both%2520individually%2520and%2520in%2520combination%252C%2520on%2520the%2520accuracy%2520and%2520fairness%2520of%250Aface%2520recognition%2520models.%2520Initially%252C%2520several%2520generative%2520methods%2520were%2520used%2520to%250Abalance%2520the%2520demographic%2520representations%2520of%2520the%2520corresponding%2520synthetic%250Adatasets.%2520Then%2520a%2520state-of-the-art%2520face%2520encoder%2520was%2520trained%2520and%2520evaluated%2520using%250A%2528combinations%2520of%2529%2520synthetic%2520and%2520authentic%2520images.%2520Our%2520findings%2520emphasized%2520two%250Amain%2520points%253A%2520%2528i%2529%2520the%2520increased%2520effectiveness%2520of%2520training%2520data%2520generated%2520by%250Adiffusion-based%2520models%2520in%2520enhancing%2520accuracy%252C%2520whether%2520used%2520alone%2520or%2520combined%250Awith%2520subsets%2520of%2520authentic%2520data%252C%2520and%2520%2528ii%2529%2520the%2520minimal%2520impact%2520of%2520incorporating%250Abalanced%2520data%2520from%2520pre-trained%2520generative%2520methods%2520on%2520fairness%2520%2528in%2520nearly%2520all%250Atested%2520scenarios%2520using%2520combined%2520datasets%252C%2520fairness%2520scores%2520remained%2520either%250Aunchanged%2520or%2520worsened%252C%2520even%2520when%2520compared%2520to%2520unbalanced%2520authentic%2520datasets%2529.%250ASource%2520code%2520and%2520data%2520are%2520available%2520at%2520%255Curl%257Bhttps%253A//cutt.ly/AeQy1K5G%257D%2520for%250Areproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Balancing%20Real%20and%20Synthetic%20Data%20on%20Accuracy%20and%20Fairness%0A%20%20in%20Face%20Recognition&entry.906535625=Andrea%20Atzori%20and%20Pietro%20Cosseddu%20and%20Gianni%20Fenu%20and%20Mirko%20Marras&entry.1292438233=%20%20Over%20the%20recent%20years%2C%20the%20advancements%20in%20deep%20face%20recognition%20have%20fueled%0Aan%20increasing%20demand%20for%20large%20and%20diverse%20datasets.%20Nevertheless%2C%20the%0Aauthentic%20data%20acquired%20to%20create%20those%20datasets%20is%20typically%20sourced%20from%20the%0Aweb%2C%20which%2C%20in%20many%20cases%2C%20can%20lead%20to%20significant%20privacy%20issues%20due%20to%20the%0Alack%20of%20explicit%20user%20consent.%20Furthermore%2C%20obtaining%20a%20demographically%0Abalanced%2C%20large%20dataset%20is%20even%20more%20difficult%20because%20of%20the%20natural%20imbalance%0Ain%20the%20distribution%20of%20images%20from%20different%20demographic%20groups.%20In%20this%20paper%2C%0Awe%20investigate%20the%20impact%20of%20demographically%20balanced%20authentic%20and%20synthetic%0Adata%2C%20both%20individually%20and%20in%20combination%2C%20on%20the%20accuracy%20and%20fairness%20of%0Aface%20recognition%20models.%20Initially%2C%20several%20generative%20methods%20were%20used%20to%0Abalance%20the%20demographic%20representations%20of%20the%20corresponding%20synthetic%0Adatasets.%20Then%20a%20state-of-the-art%20face%20encoder%20was%20trained%20and%20evaluated%20using%0A%28combinations%20of%29%20synthetic%20and%20authentic%20images.%20Our%20findings%20emphasized%20two%0Amain%20points%3A%20%28i%29%20the%20increased%20effectiveness%20of%20training%20data%20generated%20by%0Adiffusion-based%20models%20in%20enhancing%20accuracy%2C%20whether%20used%20alone%20or%20combined%0Awith%20subsets%20of%20authentic%20data%2C%20and%20%28ii%29%20the%20minimal%20impact%20of%20incorporating%0Abalanced%20data%20from%20pre-trained%20generative%20methods%20on%20fairness%20%28in%20nearly%20all%0Atested%20scenarios%20using%20combined%20datasets%2C%20fairness%20scores%20remained%20either%0Aunchanged%20or%20worsened%2C%20even%20when%20compared%20to%20unbalanced%20authentic%20datasets%29.%0ASource%20code%20and%20data%20are%20available%20at%20%5Curl%7Bhttps%3A//cutt.ly/AeQy1K5G%7D%20for%0Areproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02867v1&entry.124074799=Read"},
{"title": "In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data", "author": "Francisco Mena and Diego Arenas and Andreas Dengel", "abstract": "  Studying and analyzing cropland is a difficult task due to its dynamic and\nheterogeneous growth behavior. Usually, diverse data sources can be collected\nfor its estimation. Although deep learning models have proven to excel in the\ncrop classification task, they face substantial challenges when dealing with\nmultiple inputs, named Multi-View Learning (MVL). The methods used in the MVL\nscenario can be structured based on the encoder architecture, the fusion\nstrategy, and the optimization technique. The literature has primarily focused\non using specific encoder architectures for local regions, lacking a deeper\nexploration of other components in the MVL methodology. In contrast, we\ninvestigate the simultaneous selection of the fusion strategy and encoder\narchitecture, assessing global-scale cropland and crop-type classifications. We\nuse a range of five fusion strategies (Input, Feature, Decision, Ensemble,\nHybrid) and five temporal encoders (LSTM, GRU, TempCNN, TAE, L-TAE) as possible\nconfigurations in the MVL method. We use the CropHarvest dataset for\nvalidation, which provides optical, radar, weather time series, and topographic\ninformation as input data. We found that in scenarios with a limited number of\nlabeled samples, a unique configuration is insufficient for all the cases.\nInstead, a specialized combination should be meticulously sought, including an\nencoder and fusion strategy. To streamline this search process, we suggest\nidentifying the optimal encoder architecture tailored for a particular fusion\nstrategy, and then determining the most suitable fusion strategy for the\nclassification task. We provide a methodological framework for researchers\nexploring crop classification through an MVL methodology.\n", "link": "http://arxiv.org/abs/2403.16582v2", "date": "2024-09-04", "relevancy": 2.6468, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.53}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5292}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20the%20Search%20for%20Optimal%20Multi-view%20Learning%20Models%20for%20Crop%0A%20%20Classification%20with%20Global%20Remote%20Sensing%20Data&body=Title%3A%20In%20the%20Search%20for%20Optimal%20Multi-view%20Learning%20Models%20for%20Crop%0A%20%20Classification%20with%20Global%20Remote%20Sensing%20Data%0AAuthor%3A%20Francisco%20Mena%20and%20Diego%20Arenas%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Studying%20and%20analyzing%20cropland%20is%20a%20difficult%20task%20due%20to%20its%20dynamic%20and%0Aheterogeneous%20growth%20behavior.%20Usually%2C%20diverse%20data%20sources%20can%20be%20collected%0Afor%20its%20estimation.%20Although%20deep%20learning%20models%20have%20proven%20to%20excel%20in%20the%0Acrop%20classification%20task%2C%20they%20face%20substantial%20challenges%20when%20dealing%20with%0Amultiple%20inputs%2C%20named%20Multi-View%20Learning%20%28MVL%29.%20The%20methods%20used%20in%20the%20MVL%0Ascenario%20can%20be%20structured%20based%20on%20the%20encoder%20architecture%2C%20the%20fusion%0Astrategy%2C%20and%20the%20optimization%20technique.%20The%20literature%20has%20primarily%20focused%0Aon%20using%20specific%20encoder%20architectures%20for%20local%20regions%2C%20lacking%20a%20deeper%0Aexploration%20of%20other%20components%20in%20the%20MVL%20methodology.%20In%20contrast%2C%20we%0Ainvestigate%20the%20simultaneous%20selection%20of%20the%20fusion%20strategy%20and%20encoder%0Aarchitecture%2C%20assessing%20global-scale%20cropland%20and%20crop-type%20classifications.%20We%0Ause%20a%20range%20of%20five%20fusion%20strategies%20%28Input%2C%20Feature%2C%20Decision%2C%20Ensemble%2C%0AHybrid%29%20and%20five%20temporal%20encoders%20%28LSTM%2C%20GRU%2C%20TempCNN%2C%20TAE%2C%20L-TAE%29%20as%20possible%0Aconfigurations%20in%20the%20MVL%20method.%20We%20use%20the%20CropHarvest%20dataset%20for%0Avalidation%2C%20which%20provides%20optical%2C%20radar%2C%20weather%20time%20series%2C%20and%20topographic%0Ainformation%20as%20input%20data.%20We%20found%20that%20in%20scenarios%20with%20a%20limited%20number%20of%0Alabeled%20samples%2C%20a%20unique%20configuration%20is%20insufficient%20for%20all%20the%20cases.%0AInstead%2C%20a%20specialized%20combination%20should%20be%20meticulously%20sought%2C%20including%20an%0Aencoder%20and%20fusion%20strategy.%20To%20streamline%20this%20search%20process%2C%20we%20suggest%0Aidentifying%20the%20optimal%20encoder%20architecture%20tailored%20for%20a%20particular%20fusion%0Astrategy%2C%20and%20then%20determining%20the%20most%20suitable%20fusion%20strategy%20for%20the%0Aclassification%20task.%20We%20provide%20a%20methodological%20framework%20for%20researchers%0Aexploring%20crop%20classification%20through%20an%20MVL%20methodology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520the%2520Search%2520for%2520Optimal%2520Multi-view%2520Learning%2520Models%2520for%2520Crop%250A%2520%2520Classification%2520with%2520Global%2520Remote%2520Sensing%2520Data%26entry.906535625%3DFrancisco%2520Mena%2520and%2520Diego%2520Arenas%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Studying%2520and%2520analyzing%2520cropland%2520is%2520a%2520difficult%2520task%2520due%2520to%2520its%2520dynamic%2520and%250Aheterogeneous%2520growth%2520behavior.%2520Usually%252C%2520diverse%2520data%2520sources%2520can%2520be%2520collected%250Afor%2520its%2520estimation.%2520Although%2520deep%2520learning%2520models%2520have%2520proven%2520to%2520excel%2520in%2520the%250Acrop%2520classification%2520task%252C%2520they%2520face%2520substantial%2520challenges%2520when%2520dealing%2520with%250Amultiple%2520inputs%252C%2520named%2520Multi-View%2520Learning%2520%2528MVL%2529.%2520The%2520methods%2520used%2520in%2520the%2520MVL%250Ascenario%2520can%2520be%2520structured%2520based%2520on%2520the%2520encoder%2520architecture%252C%2520the%2520fusion%250Astrategy%252C%2520and%2520the%2520optimization%2520technique.%2520The%2520literature%2520has%2520primarily%2520focused%250Aon%2520using%2520specific%2520encoder%2520architectures%2520for%2520local%2520regions%252C%2520lacking%2520a%2520deeper%250Aexploration%2520of%2520other%2520components%2520in%2520the%2520MVL%2520methodology.%2520In%2520contrast%252C%2520we%250Ainvestigate%2520the%2520simultaneous%2520selection%2520of%2520the%2520fusion%2520strategy%2520and%2520encoder%250Aarchitecture%252C%2520assessing%2520global-scale%2520cropland%2520and%2520crop-type%2520classifications.%2520We%250Ause%2520a%2520range%2520of%2520five%2520fusion%2520strategies%2520%2528Input%252C%2520Feature%252C%2520Decision%252C%2520Ensemble%252C%250AHybrid%2529%2520and%2520five%2520temporal%2520encoders%2520%2528LSTM%252C%2520GRU%252C%2520TempCNN%252C%2520TAE%252C%2520L-TAE%2529%2520as%2520possible%250Aconfigurations%2520in%2520the%2520MVL%2520method.%2520We%2520use%2520the%2520CropHarvest%2520dataset%2520for%250Avalidation%252C%2520which%2520provides%2520optical%252C%2520radar%252C%2520weather%2520time%2520series%252C%2520and%2520topographic%250Ainformation%2520as%2520input%2520data.%2520We%2520found%2520that%2520in%2520scenarios%2520with%2520a%2520limited%2520number%2520of%250Alabeled%2520samples%252C%2520a%2520unique%2520configuration%2520is%2520insufficient%2520for%2520all%2520the%2520cases.%250AInstead%252C%2520a%2520specialized%2520combination%2520should%2520be%2520meticulously%2520sought%252C%2520including%2520an%250Aencoder%2520and%2520fusion%2520strategy.%2520To%2520streamline%2520this%2520search%2520process%252C%2520we%2520suggest%250Aidentifying%2520the%2520optimal%2520encoder%2520architecture%2520tailored%2520for%2520a%2520particular%2520fusion%250Astrategy%252C%2520and%2520then%2520determining%2520the%2520most%2520suitable%2520fusion%2520strategy%2520for%2520the%250Aclassification%2520task.%2520We%2520provide%2520a%2520methodological%2520framework%2520for%2520researchers%250Aexploring%2520crop%2520classification%2520through%2520an%2520MVL%2520methodology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20the%20Search%20for%20Optimal%20Multi-view%20Learning%20Models%20for%20Crop%0A%20%20Classification%20with%20Global%20Remote%20Sensing%20Data&entry.906535625=Francisco%20Mena%20and%20Diego%20Arenas%20and%20Andreas%20Dengel&entry.1292438233=%20%20Studying%20and%20analyzing%20cropland%20is%20a%20difficult%20task%20due%20to%20its%20dynamic%20and%0Aheterogeneous%20growth%20behavior.%20Usually%2C%20diverse%20data%20sources%20can%20be%20collected%0Afor%20its%20estimation.%20Although%20deep%20learning%20models%20have%20proven%20to%20excel%20in%20the%0Acrop%20classification%20task%2C%20they%20face%20substantial%20challenges%20when%20dealing%20with%0Amultiple%20inputs%2C%20named%20Multi-View%20Learning%20%28MVL%29.%20The%20methods%20used%20in%20the%20MVL%0Ascenario%20can%20be%20structured%20based%20on%20the%20encoder%20architecture%2C%20the%20fusion%0Astrategy%2C%20and%20the%20optimization%20technique.%20The%20literature%20has%20primarily%20focused%0Aon%20using%20specific%20encoder%20architectures%20for%20local%20regions%2C%20lacking%20a%20deeper%0Aexploration%20of%20other%20components%20in%20the%20MVL%20methodology.%20In%20contrast%2C%20we%0Ainvestigate%20the%20simultaneous%20selection%20of%20the%20fusion%20strategy%20and%20encoder%0Aarchitecture%2C%20assessing%20global-scale%20cropland%20and%20crop-type%20classifications.%20We%0Ause%20a%20range%20of%20five%20fusion%20strategies%20%28Input%2C%20Feature%2C%20Decision%2C%20Ensemble%2C%0AHybrid%29%20and%20five%20temporal%20encoders%20%28LSTM%2C%20GRU%2C%20TempCNN%2C%20TAE%2C%20L-TAE%29%20as%20possible%0Aconfigurations%20in%20the%20MVL%20method.%20We%20use%20the%20CropHarvest%20dataset%20for%0Avalidation%2C%20which%20provides%20optical%2C%20radar%2C%20weather%20time%20series%2C%20and%20topographic%0Ainformation%20as%20input%20data.%20We%20found%20that%20in%20scenarios%20with%20a%20limited%20number%20of%0Alabeled%20samples%2C%20a%20unique%20configuration%20is%20insufficient%20for%20all%20the%20cases.%0AInstead%2C%20a%20specialized%20combination%20should%20be%20meticulously%20sought%2C%20including%20an%0Aencoder%20and%20fusion%20strategy.%20To%20streamline%20this%20search%20process%2C%20we%20suggest%0Aidentifying%20the%20optimal%20encoder%20architecture%20tailored%20for%20a%20particular%20fusion%0Astrategy%2C%20and%20then%20determining%20the%20most%20suitable%20fusion%20strategy%20for%20the%0Aclassification%20task.%20We%20provide%20a%20methodological%20framework%20for%20researchers%0Aexploring%20crop%20classification%20through%20an%20MVL%20methodology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16582v2&entry.124074799=Read"},
{"title": "MMA-MRNNet: Harnessing Multiple Models of Affect and Dynamic Masked RNN\n  for Precise Facial Expression Intensity Estimation", "author": "Dimitrios Kollias and Andreas Psaroudakis and Anastasios Arsenos and Paraskevi Theofilou and Chunchang Shao and Guanyu Hu and Ioannis Patras", "abstract": "  This paper presents MMA-MRNNet, a novel deep learning architecture for\ndynamic multi-output Facial Expression Intensity Estimation (FEIE) from video\ndata. Traditional approaches to this task often rely on complex 3-D CNNs, which\nrequire extensive pre-training and assume that facial expressions are uniformly\ndistributed across all frames of a video. These methods struggle to handle\nvideos of varying lengths, often resorting to ad-hoc strategies that either\ndiscard valuable information or introduce bias. MMA-MRNNet addresses these\nchallenges through a two-stage process. First, the Multiple Models of Affect\n(MMA) extractor component is a Multi-Task Learning CNN that concurrently\nestimates valence-arousal, recognizes basic facial expressions, and detects\naction units in each frame. These representations are then processed by a\nMasked RNN component, which captures temporal dependencies and dynamically\nupdates weights according to the true length of the input video, ensuring that\nonly the most relevant features are used for the final prediction. The proposed\nunimodal non-ensemble learning MMA-MRNNet was evaluated on the Hume-Reaction\ndataset and demonstrated significantly superior performance, surpassing\nstate-of-the-art methods by a wide margin, regardless of whether they were\nunimodal, multimodal, or ensemble approaches. Finally, we demonstrated the\neffectiveness of the MMA component of our proposed method across multiple\nin-the-wild datasets, where it consistently outperformed all state-of-the-art\nmethods across various metrics.\n", "link": "http://arxiv.org/abs/2303.00180v4", "date": "2024-09-04", "relevancy": 2.6058, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5269}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5185}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMA-MRNNet%3A%20Harnessing%20Multiple%20Models%20of%20Affect%20and%20Dynamic%20Masked%20RNN%0A%20%20for%20Precise%20Facial%20Expression%20Intensity%20Estimation&body=Title%3A%20MMA-MRNNet%3A%20Harnessing%20Multiple%20Models%20of%20Affect%20and%20Dynamic%20Masked%20RNN%0A%20%20for%20Precise%20Facial%20Expression%20Intensity%20Estimation%0AAuthor%3A%20Dimitrios%20Kollias%20and%20Andreas%20Psaroudakis%20and%20Anastasios%20Arsenos%20and%20Paraskevi%20Theofilou%20and%20Chunchang%20Shao%20and%20Guanyu%20Hu%20and%20Ioannis%20Patras%0AAbstract%3A%20%20%20This%20paper%20presents%20MMA-MRNNet%2C%20a%20novel%20deep%20learning%20architecture%20for%0Adynamic%20multi-output%20Facial%20Expression%20Intensity%20Estimation%20%28FEIE%29%20from%20video%0Adata.%20Traditional%20approaches%20to%20this%20task%20often%20rely%20on%20complex%203-D%20CNNs%2C%20which%0Arequire%20extensive%20pre-training%20and%20assume%20that%20facial%20expressions%20are%20uniformly%0Adistributed%20across%20all%20frames%20of%20a%20video.%20These%20methods%20struggle%20to%20handle%0Avideos%20of%20varying%20lengths%2C%20often%20resorting%20to%20ad-hoc%20strategies%20that%20either%0Adiscard%20valuable%20information%20or%20introduce%20bias.%20MMA-MRNNet%20addresses%20these%0Achallenges%20through%20a%20two-stage%20process.%20First%2C%20the%20Multiple%20Models%20of%20Affect%0A%28MMA%29%20extractor%20component%20is%20a%20Multi-Task%20Learning%20CNN%20that%20concurrently%0Aestimates%20valence-arousal%2C%20recognizes%20basic%20facial%20expressions%2C%20and%20detects%0Aaction%20units%20in%20each%20frame.%20These%20representations%20are%20then%20processed%20by%20a%0AMasked%20RNN%20component%2C%20which%20captures%20temporal%20dependencies%20and%20dynamically%0Aupdates%20weights%20according%20to%20the%20true%20length%20of%20the%20input%20video%2C%20ensuring%20that%0Aonly%20the%20most%20relevant%20features%20are%20used%20for%20the%20final%20prediction.%20The%20proposed%0Aunimodal%20non-ensemble%20learning%20MMA-MRNNet%20was%20evaluated%20on%20the%20Hume-Reaction%0Adataset%20and%20demonstrated%20significantly%20superior%20performance%2C%20surpassing%0Astate-of-the-art%20methods%20by%20a%20wide%20margin%2C%20regardless%20of%20whether%20they%20were%0Aunimodal%2C%20multimodal%2C%20or%20ensemble%20approaches.%20Finally%2C%20we%20demonstrated%20the%0Aeffectiveness%20of%20the%20MMA%20component%20of%20our%20proposed%20method%20across%20multiple%0Ain-the-wild%20datasets%2C%20where%20it%20consistently%20outperformed%20all%20state-of-the-art%0Amethods%20across%20various%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00180v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMA-MRNNet%253A%2520Harnessing%2520Multiple%2520Models%2520of%2520Affect%2520and%2520Dynamic%2520Masked%2520RNN%250A%2520%2520for%2520Precise%2520Facial%2520Expression%2520Intensity%2520Estimation%26entry.906535625%3DDimitrios%2520Kollias%2520and%2520Andreas%2520Psaroudakis%2520and%2520Anastasios%2520Arsenos%2520and%2520Paraskevi%2520Theofilou%2520and%2520Chunchang%2520Shao%2520and%2520Guanyu%2520Hu%2520and%2520Ioannis%2520Patras%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520MMA-MRNNet%252C%2520a%2520novel%2520deep%2520learning%2520architecture%2520for%250Adynamic%2520multi-output%2520Facial%2520Expression%2520Intensity%2520Estimation%2520%2528FEIE%2529%2520from%2520video%250Adata.%2520Traditional%2520approaches%2520to%2520this%2520task%2520often%2520rely%2520on%2520complex%25203-D%2520CNNs%252C%2520which%250Arequire%2520extensive%2520pre-training%2520and%2520assume%2520that%2520facial%2520expressions%2520are%2520uniformly%250Adistributed%2520across%2520all%2520frames%2520of%2520a%2520video.%2520These%2520methods%2520struggle%2520to%2520handle%250Avideos%2520of%2520varying%2520lengths%252C%2520often%2520resorting%2520to%2520ad-hoc%2520strategies%2520that%2520either%250Adiscard%2520valuable%2520information%2520or%2520introduce%2520bias.%2520MMA-MRNNet%2520addresses%2520these%250Achallenges%2520through%2520a%2520two-stage%2520process.%2520First%252C%2520the%2520Multiple%2520Models%2520of%2520Affect%250A%2528MMA%2529%2520extractor%2520component%2520is%2520a%2520Multi-Task%2520Learning%2520CNN%2520that%2520concurrently%250Aestimates%2520valence-arousal%252C%2520recognizes%2520basic%2520facial%2520expressions%252C%2520and%2520detects%250Aaction%2520units%2520in%2520each%2520frame.%2520These%2520representations%2520are%2520then%2520processed%2520by%2520a%250AMasked%2520RNN%2520component%252C%2520which%2520captures%2520temporal%2520dependencies%2520and%2520dynamically%250Aupdates%2520weights%2520according%2520to%2520the%2520true%2520length%2520of%2520the%2520input%2520video%252C%2520ensuring%2520that%250Aonly%2520the%2520most%2520relevant%2520features%2520are%2520used%2520for%2520the%2520final%2520prediction.%2520The%2520proposed%250Aunimodal%2520non-ensemble%2520learning%2520MMA-MRNNet%2520was%2520evaluated%2520on%2520the%2520Hume-Reaction%250Adataset%2520and%2520demonstrated%2520significantly%2520superior%2520performance%252C%2520surpassing%250Astate-of-the-art%2520methods%2520by%2520a%2520wide%2520margin%252C%2520regardless%2520of%2520whether%2520they%2520were%250Aunimodal%252C%2520multimodal%252C%2520or%2520ensemble%2520approaches.%2520Finally%252C%2520we%2520demonstrated%2520the%250Aeffectiveness%2520of%2520the%2520MMA%2520component%2520of%2520our%2520proposed%2520method%2520across%2520multiple%250Ain-the-wild%2520datasets%252C%2520where%2520it%2520consistently%2520outperformed%2520all%2520state-of-the-art%250Amethods%2520across%2520various%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.00180v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMA-MRNNet%3A%20Harnessing%20Multiple%20Models%20of%20Affect%20and%20Dynamic%20Masked%20RNN%0A%20%20for%20Precise%20Facial%20Expression%20Intensity%20Estimation&entry.906535625=Dimitrios%20Kollias%20and%20Andreas%20Psaroudakis%20and%20Anastasios%20Arsenos%20and%20Paraskevi%20Theofilou%20and%20Chunchang%20Shao%20and%20Guanyu%20Hu%20and%20Ioannis%20Patras&entry.1292438233=%20%20This%20paper%20presents%20MMA-MRNNet%2C%20a%20novel%20deep%20learning%20architecture%20for%0Adynamic%20multi-output%20Facial%20Expression%20Intensity%20Estimation%20%28FEIE%29%20from%20video%0Adata.%20Traditional%20approaches%20to%20this%20task%20often%20rely%20on%20complex%203-D%20CNNs%2C%20which%0Arequire%20extensive%20pre-training%20and%20assume%20that%20facial%20expressions%20are%20uniformly%0Adistributed%20across%20all%20frames%20of%20a%20video.%20These%20methods%20struggle%20to%20handle%0Avideos%20of%20varying%20lengths%2C%20often%20resorting%20to%20ad-hoc%20strategies%20that%20either%0Adiscard%20valuable%20information%20or%20introduce%20bias.%20MMA-MRNNet%20addresses%20these%0Achallenges%20through%20a%20two-stage%20process.%20First%2C%20the%20Multiple%20Models%20of%20Affect%0A%28MMA%29%20extractor%20component%20is%20a%20Multi-Task%20Learning%20CNN%20that%20concurrently%0Aestimates%20valence-arousal%2C%20recognizes%20basic%20facial%20expressions%2C%20and%20detects%0Aaction%20units%20in%20each%20frame.%20These%20representations%20are%20then%20processed%20by%20a%0AMasked%20RNN%20component%2C%20which%20captures%20temporal%20dependencies%20and%20dynamically%0Aupdates%20weights%20according%20to%20the%20true%20length%20of%20the%20input%20video%2C%20ensuring%20that%0Aonly%20the%20most%20relevant%20features%20are%20used%20for%20the%20final%20prediction.%20The%20proposed%0Aunimodal%20non-ensemble%20learning%20MMA-MRNNet%20was%20evaluated%20on%20the%20Hume-Reaction%0Adataset%20and%20demonstrated%20significantly%20superior%20performance%2C%20surpassing%0Astate-of-the-art%20methods%20by%20a%20wide%20margin%2C%20regardless%20of%20whether%20they%20were%0Aunimodal%2C%20multimodal%2C%20or%20ensemble%20approaches.%20Finally%2C%20we%20demonstrated%20the%0Aeffectiveness%20of%20the%20MMA%20component%20of%20our%20proposed%20method%20across%20multiple%0Ain-the-wild%20datasets%2C%20where%20it%20consistently%20outperformed%20all%20state-of-the-art%0Amethods%20across%20various%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00180v4&entry.124074799=Read"},
{"title": "Automatic facial axes standardization of 3D fetal ultrasound images", "author": "Antonia Alomar and Ricardo Rubio and Laura Salort and Gerard Albaiges and Antoni Pay\u00e0 and Gemma Piella and Federico Sukno", "abstract": "  Craniofacial anomalies indicate early developmental disturbances and are\nusually linked to many genetic syndromes. Early diagnosis is critical, yet\nultrasound (US) examinations often fail to identify these features. This study\npresents an AI-driven tool to assist clinicians in standardizing fetal facial\naxes/planes in 3D US, reducing sonographer workload and facilitating the facial\nevaluation. Our network, structured into three blocks-feature extractor,\nrotation and translation regression, and spatial transformer-processes three\northogonal 2D slices to estimate the necessary transformations for\nstandardizing the facial planes in the 3D US. These transformations are applied\nto the original 3D US using a differentiable module (the spatial transformer\nblock), yielding a standardized 3D US and the corresponding 2D facial standard\nplanes. The dataset used consists of 1180 fetal facial 3D US images acquired\nbetween weeks 20 and 35 of gestation. Results show that our network\nconsiderably reduces inter-observer rotation variability in the test set, with\na mean geodesic angle difference of 14.12$^{\\circ}$ $\\pm$ 18.27$^{\\circ}$ and\nan Euclidean angle error of 7.45$^{\\circ}$ $\\pm$ 14.88$^{\\circ}$. These\nfindings demonstrate the network's ability to effectively standardize facial\naxes, crucial for consistent fetal facial assessments. In conclusion, the\nproposed network demonstrates potential for improving the consistency and\naccuracy of fetal facial assessments in clinical settings, facilitating early\nevaluation of craniofacial anomalies.\n", "link": "http://arxiv.org/abs/2409.02826v1", "date": "2024-09-04", "relevancy": 2.5957, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5232}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5232}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20facial%20axes%20standardization%20of%203D%20fetal%20ultrasound%20images&body=Title%3A%20Automatic%20facial%20axes%20standardization%20of%203D%20fetal%20ultrasound%20images%0AAuthor%3A%20Antonia%20Alomar%20and%20Ricardo%20Rubio%20and%20Laura%20Salort%20and%20Gerard%20Albaiges%20and%20Antoni%20Pay%C3%A0%20and%20Gemma%20Piella%20and%20Federico%20Sukno%0AAbstract%3A%20%20%20Craniofacial%20anomalies%20indicate%20early%20developmental%20disturbances%20and%20are%0Ausually%20linked%20to%20many%20genetic%20syndromes.%20Early%20diagnosis%20is%20critical%2C%20yet%0Aultrasound%20%28US%29%20examinations%20often%20fail%20to%20identify%20these%20features.%20This%20study%0Apresents%20an%20AI-driven%20tool%20to%20assist%20clinicians%20in%20standardizing%20fetal%20facial%0Aaxes/planes%20in%203D%20US%2C%20reducing%20sonographer%20workload%20and%20facilitating%20the%20facial%0Aevaluation.%20Our%20network%2C%20structured%20into%20three%20blocks-feature%20extractor%2C%0Arotation%20and%20translation%20regression%2C%20and%20spatial%20transformer-processes%20three%0Aorthogonal%202D%20slices%20to%20estimate%20the%20necessary%20transformations%20for%0Astandardizing%20the%20facial%20planes%20in%20the%203D%20US.%20These%20transformations%20are%20applied%0Ato%20the%20original%203D%20US%20using%20a%20differentiable%20module%20%28the%20spatial%20transformer%0Ablock%29%2C%20yielding%20a%20standardized%203D%20US%20and%20the%20corresponding%202D%20facial%20standard%0Aplanes.%20The%20dataset%20used%20consists%20of%201180%20fetal%20facial%203D%20US%20images%20acquired%0Abetween%20weeks%2020%20and%2035%20of%20gestation.%20Results%20show%20that%20our%20network%0Aconsiderably%20reduces%20inter-observer%20rotation%20variability%20in%20the%20test%20set%2C%20with%0Aa%20mean%20geodesic%20angle%20difference%20of%2014.12%24%5E%7B%5Ccirc%7D%24%20%24%5Cpm%24%2018.27%24%5E%7B%5Ccirc%7D%24%20and%0Aan%20Euclidean%20angle%20error%20of%207.45%24%5E%7B%5Ccirc%7D%24%20%24%5Cpm%24%2014.88%24%5E%7B%5Ccirc%7D%24.%20These%0Afindings%20demonstrate%20the%20network%27s%20ability%20to%20effectively%20standardize%20facial%0Aaxes%2C%20crucial%20for%20consistent%20fetal%20facial%20assessments.%20In%20conclusion%2C%20the%0Aproposed%20network%20demonstrates%20potential%20for%20improving%20the%20consistency%20and%0Aaccuracy%20of%20fetal%20facial%20assessments%20in%20clinical%20settings%2C%20facilitating%20early%0Aevaluation%20of%20craniofacial%20anomalies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520facial%2520axes%2520standardization%2520of%25203D%2520fetal%2520ultrasound%2520images%26entry.906535625%3DAntonia%2520Alomar%2520and%2520Ricardo%2520Rubio%2520and%2520Laura%2520Salort%2520and%2520Gerard%2520Albaiges%2520and%2520Antoni%2520Pay%25C3%25A0%2520and%2520Gemma%2520Piella%2520and%2520Federico%2520Sukno%26entry.1292438233%3D%2520%2520Craniofacial%2520anomalies%2520indicate%2520early%2520developmental%2520disturbances%2520and%2520are%250Ausually%2520linked%2520to%2520many%2520genetic%2520syndromes.%2520Early%2520diagnosis%2520is%2520critical%252C%2520yet%250Aultrasound%2520%2528US%2529%2520examinations%2520often%2520fail%2520to%2520identify%2520these%2520features.%2520This%2520study%250Apresents%2520an%2520AI-driven%2520tool%2520to%2520assist%2520clinicians%2520in%2520standardizing%2520fetal%2520facial%250Aaxes/planes%2520in%25203D%2520US%252C%2520reducing%2520sonographer%2520workload%2520and%2520facilitating%2520the%2520facial%250Aevaluation.%2520Our%2520network%252C%2520structured%2520into%2520three%2520blocks-feature%2520extractor%252C%250Arotation%2520and%2520translation%2520regression%252C%2520and%2520spatial%2520transformer-processes%2520three%250Aorthogonal%25202D%2520slices%2520to%2520estimate%2520the%2520necessary%2520transformations%2520for%250Astandardizing%2520the%2520facial%2520planes%2520in%2520the%25203D%2520US.%2520These%2520transformations%2520are%2520applied%250Ato%2520the%2520original%25203D%2520US%2520using%2520a%2520differentiable%2520module%2520%2528the%2520spatial%2520transformer%250Ablock%2529%252C%2520yielding%2520a%2520standardized%25203D%2520US%2520and%2520the%2520corresponding%25202D%2520facial%2520standard%250Aplanes.%2520The%2520dataset%2520used%2520consists%2520of%25201180%2520fetal%2520facial%25203D%2520US%2520images%2520acquired%250Abetween%2520weeks%252020%2520and%252035%2520of%2520gestation.%2520Results%2520show%2520that%2520our%2520network%250Aconsiderably%2520reduces%2520inter-observer%2520rotation%2520variability%2520in%2520the%2520test%2520set%252C%2520with%250Aa%2520mean%2520geodesic%2520angle%2520difference%2520of%252014.12%2524%255E%257B%255Ccirc%257D%2524%2520%2524%255Cpm%2524%252018.27%2524%255E%257B%255Ccirc%257D%2524%2520and%250Aan%2520Euclidean%2520angle%2520error%2520of%25207.45%2524%255E%257B%255Ccirc%257D%2524%2520%2524%255Cpm%2524%252014.88%2524%255E%257B%255Ccirc%257D%2524.%2520These%250Afindings%2520demonstrate%2520the%2520network%2527s%2520ability%2520to%2520effectively%2520standardize%2520facial%250Aaxes%252C%2520crucial%2520for%2520consistent%2520fetal%2520facial%2520assessments.%2520In%2520conclusion%252C%2520the%250Aproposed%2520network%2520demonstrates%2520potential%2520for%2520improving%2520the%2520consistency%2520and%250Aaccuracy%2520of%2520fetal%2520facial%2520assessments%2520in%2520clinical%2520settings%252C%2520facilitating%2520early%250Aevaluation%2520of%2520craniofacial%2520anomalies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20facial%20axes%20standardization%20of%203D%20fetal%20ultrasound%20images&entry.906535625=Antonia%20Alomar%20and%20Ricardo%20Rubio%20and%20Laura%20Salort%20and%20Gerard%20Albaiges%20and%20Antoni%20Pay%C3%A0%20and%20Gemma%20Piella%20and%20Federico%20Sukno&entry.1292438233=%20%20Craniofacial%20anomalies%20indicate%20early%20developmental%20disturbances%20and%20are%0Ausually%20linked%20to%20many%20genetic%20syndromes.%20Early%20diagnosis%20is%20critical%2C%20yet%0Aultrasound%20%28US%29%20examinations%20often%20fail%20to%20identify%20these%20features.%20This%20study%0Apresents%20an%20AI-driven%20tool%20to%20assist%20clinicians%20in%20standardizing%20fetal%20facial%0Aaxes/planes%20in%203D%20US%2C%20reducing%20sonographer%20workload%20and%20facilitating%20the%20facial%0Aevaluation.%20Our%20network%2C%20structured%20into%20three%20blocks-feature%20extractor%2C%0Arotation%20and%20translation%20regression%2C%20and%20spatial%20transformer-processes%20three%0Aorthogonal%202D%20slices%20to%20estimate%20the%20necessary%20transformations%20for%0Astandardizing%20the%20facial%20planes%20in%20the%203D%20US.%20These%20transformations%20are%20applied%0Ato%20the%20original%203D%20US%20using%20a%20differentiable%20module%20%28the%20spatial%20transformer%0Ablock%29%2C%20yielding%20a%20standardized%203D%20US%20and%20the%20corresponding%202D%20facial%20standard%0Aplanes.%20The%20dataset%20used%20consists%20of%201180%20fetal%20facial%203D%20US%20images%20acquired%0Abetween%20weeks%2020%20and%2035%20of%20gestation.%20Results%20show%20that%20our%20network%0Aconsiderably%20reduces%20inter-observer%20rotation%20variability%20in%20the%20test%20set%2C%20with%0Aa%20mean%20geodesic%20angle%20difference%20of%2014.12%24%5E%7B%5Ccirc%7D%24%20%24%5Cpm%24%2018.27%24%5E%7B%5Ccirc%7D%24%20and%0Aan%20Euclidean%20angle%20error%20of%207.45%24%5E%7B%5Ccirc%7D%24%20%24%5Cpm%24%2014.88%24%5E%7B%5Ccirc%7D%24.%20These%0Afindings%20demonstrate%20the%20network%27s%20ability%20to%20effectively%20standardize%20facial%0Aaxes%2C%20crucial%20for%20consistent%20fetal%20facial%20assessments.%20In%20conclusion%2C%20the%0Aproposed%20network%20demonstrates%20potential%20for%20improving%20the%20consistency%20and%0Aaccuracy%20of%20fetal%20facial%20assessments%20in%20clinical%20settings%2C%20facilitating%20early%0Aevaluation%20of%20craniofacial%20anomalies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02826v1&entry.124074799=Read"},
{"title": "SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments", "author": "Wenwu Guo and Jinlin Wu and Zhen Chen and Qingxiang Zhao and Miao Xu and Zhen Lei and Hongbin Liu", "abstract": "  Vision-based surgical navigation has received increasing attention due to its\nnon-invasive, cost-effective, and flexible advantages. In particular, a\ncritical element of the vision-based navigation system is tracking surgical\ninstruments. Compared with 2D instrument tracking methods, 3D instrument\ntracking has broader value in clinical practice, but is also more challenging\ndue to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models\nfor 3D registration. To solve these challenges, we propose the SurgTrack, a\ntwo-stage 3D instrument tracking method for CAD-free and robust real-world\napplications. In the first registration stage, we incorporate an Instrument\nSigned Distance Field (SDF) modeling the 3D representation of instruments,\nachieving CAD-freed 3D registration. Due to this, we can obtain the location\nand orientation of instruments in the 3D space by matching the video stream\nwith the registered SDF model. In the second tracking stage, we devise a\nposture graph optimization module, leveraging the historical tracking results\nof the posture memory pool to optimize the tracking results and improve the\nocclusion robustness. Furthermore, we collect the Instrument3D dataset to\ncomprehensively evaluate the 3D tracking of surgical instruments. The extensive\nexperiments validate the superiority and scalability of our SurgTrack, by\noutperforming the state-of-the-arts with a remarkable improvement. The code and\ndataset are available at https://github.com/wenwucode/SurgTrack.\n", "link": "http://arxiv.org/abs/2409.02598v1", "date": "2024-09-04", "relevancy": 2.5874, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5275}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5132}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgTrack%3A%20CAD-Free%203D%20Tracking%20of%20Real-world%20Surgical%20Instruments&body=Title%3A%20SurgTrack%3A%20CAD-Free%203D%20Tracking%20of%20Real-world%20Surgical%20Instruments%0AAuthor%3A%20Wenwu%20Guo%20and%20Jinlin%20Wu%20and%20Zhen%20Chen%20and%20Qingxiang%20Zhao%20and%20Miao%20Xu%20and%20Zhen%20Lei%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20Vision-based%20surgical%20navigation%20has%20received%20increasing%20attention%20due%20to%20its%0Anon-invasive%2C%20cost-effective%2C%20and%20flexible%20advantages.%20In%20particular%2C%20a%0Acritical%20element%20of%20the%20vision-based%20navigation%20system%20is%20tracking%20surgical%0Ainstruments.%20Compared%20with%202D%20instrument%20tracking%20methods%2C%203D%20instrument%0Atracking%20has%20broader%20value%20in%20clinical%20practice%2C%20but%20is%20also%20more%20challenging%0Adue%20to%20weak%20texture%2C%20occlusion%2C%20and%20lack%20of%20Computer-Aided%20Design%20%28CAD%29%20models%0Afor%203D%20registration.%20To%20solve%20these%20challenges%2C%20we%20propose%20the%20SurgTrack%2C%20a%0Atwo-stage%203D%20instrument%20tracking%20method%20for%20CAD-free%20and%20robust%20real-world%0Aapplications.%20In%20the%20first%20registration%20stage%2C%20we%20incorporate%20an%20Instrument%0ASigned%20Distance%20Field%20%28SDF%29%20modeling%20the%203D%20representation%20of%20instruments%2C%0Aachieving%20CAD-freed%203D%20registration.%20Due%20to%20this%2C%20we%20can%20obtain%20the%20location%0Aand%20orientation%20of%20instruments%20in%20the%203D%20space%20by%20matching%20the%20video%20stream%0Awith%20the%20registered%20SDF%20model.%20In%20the%20second%20tracking%20stage%2C%20we%20devise%20a%0Aposture%20graph%20optimization%20module%2C%20leveraging%20the%20historical%20tracking%20results%0Aof%20the%20posture%20memory%20pool%20to%20optimize%20the%20tracking%20results%20and%20improve%20the%0Aocclusion%20robustness.%20Furthermore%2C%20we%20collect%20the%20Instrument3D%20dataset%20to%0Acomprehensively%20evaluate%20the%203D%20tracking%20of%20surgical%20instruments.%20The%20extensive%0Aexperiments%20validate%20the%20superiority%20and%20scalability%20of%20our%20SurgTrack%2C%20by%0Aoutperforming%20the%20state-of-the-arts%20with%20a%20remarkable%20improvement.%20The%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/wenwucode/SurgTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgTrack%253A%2520CAD-Free%25203D%2520Tracking%2520of%2520Real-world%2520Surgical%2520Instruments%26entry.906535625%3DWenwu%2520Guo%2520and%2520Jinlin%2520Wu%2520and%2520Zhen%2520Chen%2520and%2520Qingxiang%2520Zhao%2520and%2520Miao%2520Xu%2520and%2520Zhen%2520Lei%2520and%2520Hongbin%2520Liu%26entry.1292438233%3D%2520%2520Vision-based%2520surgical%2520navigation%2520has%2520received%2520increasing%2520attention%2520due%2520to%2520its%250Anon-invasive%252C%2520cost-effective%252C%2520and%2520flexible%2520advantages.%2520In%2520particular%252C%2520a%250Acritical%2520element%2520of%2520the%2520vision-based%2520navigation%2520system%2520is%2520tracking%2520surgical%250Ainstruments.%2520Compared%2520with%25202D%2520instrument%2520tracking%2520methods%252C%25203D%2520instrument%250Atracking%2520has%2520broader%2520value%2520in%2520clinical%2520practice%252C%2520but%2520is%2520also%2520more%2520challenging%250Adue%2520to%2520weak%2520texture%252C%2520occlusion%252C%2520and%2520lack%2520of%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520models%250Afor%25203D%2520registration.%2520To%2520solve%2520these%2520challenges%252C%2520we%2520propose%2520the%2520SurgTrack%252C%2520a%250Atwo-stage%25203D%2520instrument%2520tracking%2520method%2520for%2520CAD-free%2520and%2520robust%2520real-world%250Aapplications.%2520In%2520the%2520first%2520registration%2520stage%252C%2520we%2520incorporate%2520an%2520Instrument%250ASigned%2520Distance%2520Field%2520%2528SDF%2529%2520modeling%2520the%25203D%2520representation%2520of%2520instruments%252C%250Aachieving%2520CAD-freed%25203D%2520registration.%2520Due%2520to%2520this%252C%2520we%2520can%2520obtain%2520the%2520location%250Aand%2520orientation%2520of%2520instruments%2520in%2520the%25203D%2520space%2520by%2520matching%2520the%2520video%2520stream%250Awith%2520the%2520registered%2520SDF%2520model.%2520In%2520the%2520second%2520tracking%2520stage%252C%2520we%2520devise%2520a%250Aposture%2520graph%2520optimization%2520module%252C%2520leveraging%2520the%2520historical%2520tracking%2520results%250Aof%2520the%2520posture%2520memory%2520pool%2520to%2520optimize%2520the%2520tracking%2520results%2520and%2520improve%2520the%250Aocclusion%2520robustness.%2520Furthermore%252C%2520we%2520collect%2520the%2520Instrument3D%2520dataset%2520to%250Acomprehensively%2520evaluate%2520the%25203D%2520tracking%2520of%2520surgical%2520instruments.%2520The%2520extensive%250Aexperiments%2520validate%2520the%2520superiority%2520and%2520scalability%2520of%2520our%2520SurgTrack%252C%2520by%250Aoutperforming%2520the%2520state-of-the-arts%2520with%2520a%2520remarkable%2520improvement.%2520The%2520code%2520and%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/wenwucode/SurgTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgTrack%3A%20CAD-Free%203D%20Tracking%20of%20Real-world%20Surgical%20Instruments&entry.906535625=Wenwu%20Guo%20and%20Jinlin%20Wu%20and%20Zhen%20Chen%20and%20Qingxiang%20Zhao%20and%20Miao%20Xu%20and%20Zhen%20Lei%20and%20Hongbin%20Liu&entry.1292438233=%20%20Vision-based%20surgical%20navigation%20has%20received%20increasing%20attention%20due%20to%20its%0Anon-invasive%2C%20cost-effective%2C%20and%20flexible%20advantages.%20In%20particular%2C%20a%0Acritical%20element%20of%20the%20vision-based%20navigation%20system%20is%20tracking%20surgical%0Ainstruments.%20Compared%20with%202D%20instrument%20tracking%20methods%2C%203D%20instrument%0Atracking%20has%20broader%20value%20in%20clinical%20practice%2C%20but%20is%20also%20more%20challenging%0Adue%20to%20weak%20texture%2C%20occlusion%2C%20and%20lack%20of%20Computer-Aided%20Design%20%28CAD%29%20models%0Afor%203D%20registration.%20To%20solve%20these%20challenges%2C%20we%20propose%20the%20SurgTrack%2C%20a%0Atwo-stage%203D%20instrument%20tracking%20method%20for%20CAD-free%20and%20robust%20real-world%0Aapplications.%20In%20the%20first%20registration%20stage%2C%20we%20incorporate%20an%20Instrument%0ASigned%20Distance%20Field%20%28SDF%29%20modeling%20the%203D%20representation%20of%20instruments%2C%0Aachieving%20CAD-freed%203D%20registration.%20Due%20to%20this%2C%20we%20can%20obtain%20the%20location%0Aand%20orientation%20of%20instruments%20in%20the%203D%20space%20by%20matching%20the%20video%20stream%0Awith%20the%20registered%20SDF%20model.%20In%20the%20second%20tracking%20stage%2C%20we%20devise%20a%0Aposture%20graph%20optimization%20module%2C%20leveraging%20the%20historical%20tracking%20results%0Aof%20the%20posture%20memory%20pool%20to%20optimize%20the%20tracking%20results%20and%20improve%20the%0Aocclusion%20robustness.%20Furthermore%2C%20we%20collect%20the%20Instrument3D%20dataset%20to%0Acomprehensively%20evaluate%20the%203D%20tracking%20of%20surgical%20instruments.%20The%20extensive%0Aexperiments%20validate%20the%20superiority%20and%20scalability%20of%20our%20SurgTrack%2C%20by%0Aoutperforming%20the%20state-of-the-arts%20with%20a%20remarkable%20improvement.%20The%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/wenwucode/SurgTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02598v1&entry.124074799=Read"},
{"title": "Large Scale Unsupervised Brain MRI Image Registration Solution for\n  Learn2Reg 2024", "author": "Yuxi Zhang and Xiang Chen and Jiazheng Wang and Min Liu and Yaonan Wang and Dongdong Liu and Renjiu Hu and Hang Zhang", "abstract": "  In this paper, we summarize the methods and experimental results we proposed\nfor Task 2 in the learn2reg 2024 Challenge. This task focuses on unsupervised\nregistration of anatomical structures in brain MRI images between different\npatients. The difficulty lies in: (1) without segmentation labels, and (2) a\nlarge amount of data. To address these challenges, we built an efficient\nbackbone network and explored several schemes to further enhance registration\naccuracy. Under the guidance of the NCC loss function and smoothness\nregularization loss function, we obtained a smooth and reasonable deformation\nfield. According to the leaderboard, our method achieved a Dice coefficient of\n77.34%, which is 1.4% higher than the TransMorph. Overall, we won second place\non the leaderboard for Task 2.\n", "link": "http://arxiv.org/abs/2409.00917v2", "date": "2024-09-04", "relevancy": 2.5658, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5298}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5067}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Scale%20Unsupervised%20Brain%20MRI%20Image%20Registration%20Solution%20for%0A%20%20Learn2Reg%202024&body=Title%3A%20Large%20Scale%20Unsupervised%20Brain%20MRI%20Image%20Registration%20Solution%20for%0A%20%20Learn2Reg%202024%0AAuthor%3A%20Yuxi%20Zhang%20and%20Xiang%20Chen%20and%20Jiazheng%20Wang%20and%20Min%20Liu%20and%20Yaonan%20Wang%20and%20Dongdong%20Liu%20and%20Renjiu%20Hu%20and%20Hang%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20summarize%20the%20methods%20and%20experimental%20results%20we%20proposed%0Afor%20Task%202%20in%20the%20learn2reg%202024%20Challenge.%20This%20task%20focuses%20on%20unsupervised%0Aregistration%20of%20anatomical%20structures%20in%20brain%20MRI%20images%20between%20different%0Apatients.%20The%20difficulty%20lies%20in%3A%20%281%29%20without%20segmentation%20labels%2C%20and%20%282%29%20a%0Alarge%20amount%20of%20data.%20To%20address%20these%20challenges%2C%20we%20built%20an%20efficient%0Abackbone%20network%20and%20explored%20several%20schemes%20to%20further%20enhance%20registration%0Aaccuracy.%20Under%20the%20guidance%20of%20the%20NCC%20loss%20function%20and%20smoothness%0Aregularization%20loss%20function%2C%20we%20obtained%20a%20smooth%20and%20reasonable%20deformation%0Afield.%20According%20to%20the%20leaderboard%2C%20our%20method%20achieved%20a%20Dice%20coefficient%20of%0A77.34%25%2C%20which%20is%201.4%25%20higher%20than%20the%20TransMorph.%20Overall%2C%20we%20won%20second%20place%0Aon%20the%20leaderboard%20for%20Task%202.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Scale%2520Unsupervised%2520Brain%2520MRI%2520Image%2520Registration%2520Solution%2520for%250A%2520%2520Learn2Reg%25202024%26entry.906535625%3DYuxi%2520Zhang%2520and%2520Xiang%2520Chen%2520and%2520Jiazheng%2520Wang%2520and%2520Min%2520Liu%2520and%2520Yaonan%2520Wang%2520and%2520Dongdong%2520Liu%2520and%2520Renjiu%2520Hu%2520and%2520Hang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520summarize%2520the%2520methods%2520and%2520experimental%2520results%2520we%2520proposed%250Afor%2520Task%25202%2520in%2520the%2520learn2reg%25202024%2520Challenge.%2520This%2520task%2520focuses%2520on%2520unsupervised%250Aregistration%2520of%2520anatomical%2520structures%2520in%2520brain%2520MRI%2520images%2520between%2520different%250Apatients.%2520The%2520difficulty%2520lies%2520in%253A%2520%25281%2529%2520without%2520segmentation%2520labels%252C%2520and%2520%25282%2529%2520a%250Alarge%2520amount%2520of%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520built%2520an%2520efficient%250Abackbone%2520network%2520and%2520explored%2520several%2520schemes%2520to%2520further%2520enhance%2520registration%250Aaccuracy.%2520Under%2520the%2520guidance%2520of%2520the%2520NCC%2520loss%2520function%2520and%2520smoothness%250Aregularization%2520loss%2520function%252C%2520we%2520obtained%2520a%2520smooth%2520and%2520reasonable%2520deformation%250Afield.%2520According%2520to%2520the%2520leaderboard%252C%2520our%2520method%2520achieved%2520a%2520Dice%2520coefficient%2520of%250A77.34%2525%252C%2520which%2520is%25201.4%2525%2520higher%2520than%2520the%2520TransMorph.%2520Overall%252C%2520we%2520won%2520second%2520place%250Aon%2520the%2520leaderboard%2520for%2520Task%25202.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Scale%20Unsupervised%20Brain%20MRI%20Image%20Registration%20Solution%20for%0A%20%20Learn2Reg%202024&entry.906535625=Yuxi%20Zhang%20and%20Xiang%20Chen%20and%20Jiazheng%20Wang%20and%20Min%20Liu%20and%20Yaonan%20Wang%20and%20Dongdong%20Liu%20and%20Renjiu%20Hu%20and%20Hang%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20summarize%20the%20methods%20and%20experimental%20results%20we%20proposed%0Afor%20Task%202%20in%20the%20learn2reg%202024%20Challenge.%20This%20task%20focuses%20on%20unsupervised%0Aregistration%20of%20anatomical%20structures%20in%20brain%20MRI%20images%20between%20different%0Apatients.%20The%20difficulty%20lies%20in%3A%20%281%29%20without%20segmentation%20labels%2C%20and%20%282%29%20a%0Alarge%20amount%20of%20data.%20To%20address%20these%20challenges%2C%20we%20built%20an%20efficient%0Abackbone%20network%20and%20explored%20several%20schemes%20to%20further%20enhance%20registration%0Aaccuracy.%20Under%20the%20guidance%20of%20the%20NCC%20loss%20function%20and%20smoothness%0Aregularization%20loss%20function%2C%20we%20obtained%20a%20smooth%20and%20reasonable%20deformation%0Afield.%20According%20to%20the%20leaderboard%2C%20our%20method%20achieved%20a%20Dice%20coefficient%20of%0A77.34%25%2C%20which%20is%201.4%25%20higher%20than%20the%20TransMorph.%20Overall%2C%20we%20won%20second%20place%0Aon%20the%20leaderboard%20for%20Task%202.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00917v2&entry.124074799=Read"},
{"title": "DNN-GDITD: Out-of-distribution detection via Deep Neural Network based\n  Gaussian Descriptor for Imbalanced Tabular Data", "author": "Priyanka Chudasama and Anil Surisetty and Aakarsh Malhotra and Alok Singh", "abstract": "  Classification tasks present challenges due to class imbalances and evolving\ndata distributions. Addressing these issues requires a robust method to handle\nimbalances while effectively detecting out-of-distribution (OOD) samples not\nencountered during training. This study introduces a novel OOD detection\nalgorithm designed for tabular datasets, titled Deep Neural Network-based\nGaussian Descriptor for Imbalanced Tabular Data (DNN-GDITD). The DNN-GDITD\nalgorithm can be placed on top of any DNN to facilitate better classification\nof imbalanced data and OOD detection using spherical decision boundaries. Using\na combination of Push, Score-based, and focal losses, DNN-GDITD assigns\nconfidence scores to test data points, categorizing them as known classes or as\nan OOD sample. Extensive experimentation on tabular datasets demonstrates the\neffectiveness of DNN-GDITD compared to three OOD algorithms. Evaluation\nencompasses imbalanced and balanced scenarios on diverse tabular datasets,\nincluding a synthetic financial dispute dataset and publicly available tabular\ndatasets like Gas Sensor, Drive Diagnosis, and MNIST, showcasing DNN-GDITD's\nversatility.\n", "link": "http://arxiv.org/abs/2409.00980v2", "date": "2024-09-04", "relevancy": 2.5456, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5218}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.521}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNN-GDITD%3A%20Out-of-distribution%20detection%20via%20Deep%20Neural%20Network%20based%0A%20%20Gaussian%20Descriptor%20for%20Imbalanced%20Tabular%20Data&body=Title%3A%20DNN-GDITD%3A%20Out-of-distribution%20detection%20via%20Deep%20Neural%20Network%20based%0A%20%20Gaussian%20Descriptor%20for%20Imbalanced%20Tabular%20Data%0AAuthor%3A%20Priyanka%20Chudasama%20and%20Anil%20Surisetty%20and%20Aakarsh%20Malhotra%20and%20Alok%20Singh%0AAbstract%3A%20%20%20Classification%20tasks%20present%20challenges%20due%20to%20class%20imbalances%20and%20evolving%0Adata%20distributions.%20Addressing%20these%20issues%20requires%20a%20robust%20method%20to%20handle%0Aimbalances%20while%20effectively%20detecting%20out-of-distribution%20%28OOD%29%20samples%20not%0Aencountered%20during%20training.%20This%20study%20introduces%20a%20novel%20OOD%20detection%0Aalgorithm%20designed%20for%20tabular%20datasets%2C%20titled%20Deep%20Neural%20Network-based%0AGaussian%20Descriptor%20for%20Imbalanced%20Tabular%20Data%20%28DNN-GDITD%29.%20The%20DNN-GDITD%0Aalgorithm%20can%20be%20placed%20on%20top%20of%20any%20DNN%20to%20facilitate%20better%20classification%0Aof%20imbalanced%20data%20and%20OOD%20detection%20using%20spherical%20decision%20boundaries.%20Using%0Aa%20combination%20of%20Push%2C%20Score-based%2C%20and%20focal%20losses%2C%20DNN-GDITD%20assigns%0Aconfidence%20scores%20to%20test%20data%20points%2C%20categorizing%20them%20as%20known%20classes%20or%20as%0Aan%20OOD%20sample.%20Extensive%20experimentation%20on%20tabular%20datasets%20demonstrates%20the%0Aeffectiveness%20of%20DNN-GDITD%20compared%20to%20three%20OOD%20algorithms.%20Evaluation%0Aencompasses%20imbalanced%20and%20balanced%20scenarios%20on%20diverse%20tabular%20datasets%2C%0Aincluding%20a%20synthetic%20financial%20dispute%20dataset%20and%20publicly%20available%20tabular%0Adatasets%20like%20Gas%20Sensor%2C%20Drive%20Diagnosis%2C%20and%20MNIST%2C%20showcasing%20DNN-GDITD%27s%0Aversatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNN-GDITD%253A%2520Out-of-distribution%2520detection%2520via%2520Deep%2520Neural%2520Network%2520based%250A%2520%2520Gaussian%2520Descriptor%2520for%2520Imbalanced%2520Tabular%2520Data%26entry.906535625%3DPriyanka%2520Chudasama%2520and%2520Anil%2520Surisetty%2520and%2520Aakarsh%2520Malhotra%2520and%2520Alok%2520Singh%26entry.1292438233%3D%2520%2520Classification%2520tasks%2520present%2520challenges%2520due%2520to%2520class%2520imbalances%2520and%2520evolving%250Adata%2520distributions.%2520Addressing%2520these%2520issues%2520requires%2520a%2520robust%2520method%2520to%2520handle%250Aimbalances%2520while%2520effectively%2520detecting%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520not%250Aencountered%2520during%2520training.%2520This%2520study%2520introduces%2520a%2520novel%2520OOD%2520detection%250Aalgorithm%2520designed%2520for%2520tabular%2520datasets%252C%2520titled%2520Deep%2520Neural%2520Network-based%250AGaussian%2520Descriptor%2520for%2520Imbalanced%2520Tabular%2520Data%2520%2528DNN-GDITD%2529.%2520The%2520DNN-GDITD%250Aalgorithm%2520can%2520be%2520placed%2520on%2520top%2520of%2520any%2520DNN%2520to%2520facilitate%2520better%2520classification%250Aof%2520imbalanced%2520data%2520and%2520OOD%2520detection%2520using%2520spherical%2520decision%2520boundaries.%2520Using%250Aa%2520combination%2520of%2520Push%252C%2520Score-based%252C%2520and%2520focal%2520losses%252C%2520DNN-GDITD%2520assigns%250Aconfidence%2520scores%2520to%2520test%2520data%2520points%252C%2520categorizing%2520them%2520as%2520known%2520classes%2520or%2520as%250Aan%2520OOD%2520sample.%2520Extensive%2520experimentation%2520on%2520tabular%2520datasets%2520demonstrates%2520the%250Aeffectiveness%2520of%2520DNN-GDITD%2520compared%2520to%2520three%2520OOD%2520algorithms.%2520Evaluation%250Aencompasses%2520imbalanced%2520and%2520balanced%2520scenarios%2520on%2520diverse%2520tabular%2520datasets%252C%250Aincluding%2520a%2520synthetic%2520financial%2520dispute%2520dataset%2520and%2520publicly%2520available%2520tabular%250Adatasets%2520like%2520Gas%2520Sensor%252C%2520Drive%2520Diagnosis%252C%2520and%2520MNIST%252C%2520showcasing%2520DNN-GDITD%2527s%250Aversatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNN-GDITD%3A%20Out-of-distribution%20detection%20via%20Deep%20Neural%20Network%20based%0A%20%20Gaussian%20Descriptor%20for%20Imbalanced%20Tabular%20Data&entry.906535625=Priyanka%20Chudasama%20and%20Anil%20Surisetty%20and%20Aakarsh%20Malhotra%20and%20Alok%20Singh&entry.1292438233=%20%20Classification%20tasks%20present%20challenges%20due%20to%20class%20imbalances%20and%20evolving%0Adata%20distributions.%20Addressing%20these%20issues%20requires%20a%20robust%20method%20to%20handle%0Aimbalances%20while%20effectively%20detecting%20out-of-distribution%20%28OOD%29%20samples%20not%0Aencountered%20during%20training.%20This%20study%20introduces%20a%20novel%20OOD%20detection%0Aalgorithm%20designed%20for%20tabular%20datasets%2C%20titled%20Deep%20Neural%20Network-based%0AGaussian%20Descriptor%20for%20Imbalanced%20Tabular%20Data%20%28DNN-GDITD%29.%20The%20DNN-GDITD%0Aalgorithm%20can%20be%20placed%20on%20top%20of%20any%20DNN%20to%20facilitate%20better%20classification%0Aof%20imbalanced%20data%20and%20OOD%20detection%20using%20spherical%20decision%20boundaries.%20Using%0Aa%20combination%20of%20Push%2C%20Score-based%2C%20and%20focal%20losses%2C%20DNN-GDITD%20assigns%0Aconfidence%20scores%20to%20test%20data%20points%2C%20categorizing%20them%20as%20known%20classes%20or%20as%0Aan%20OOD%20sample.%20Extensive%20experimentation%20on%20tabular%20datasets%20demonstrates%20the%0Aeffectiveness%20of%20DNN-GDITD%20compared%20to%20three%20OOD%20algorithms.%20Evaluation%0Aencompasses%20imbalanced%20and%20balanced%20scenarios%20on%20diverse%20tabular%20datasets%2C%0Aincluding%20a%20synthetic%20financial%20dispute%20dataset%20and%20publicly%20available%20tabular%0Adatasets%20like%20Gas%20Sensor%2C%20Drive%20Diagnosis%2C%20and%20MNIST%2C%20showcasing%20DNN-GDITD%27s%0Aversatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00980v2&entry.124074799=Read"},
{"title": "Boosting Certificate Robustness for Time Series Classification with\n  Efficient Self-Ensemble", "author": "Chang Dong and Zhengyang Li and Liangwei Zheng and Weitong Chen and Wei Emma Zhang", "abstract": "  Recently, the issue of adversarial robustness in the time series domain has\ngarnered significant attention. However, the available defense mechanisms\nremain limited, with adversarial training being the predominant approach,\nthough it does not provide theoretical guarantees. Randomized Smoothing has\nemerged as a standout method due to its ability to certify a provable lower\nbound on robustness radius under $\\ell_p$-ball attacks. Recognizing its\nsuccess, research in the time series domain has started focusing on these\naspects. However, existing research predominantly focuses on time series\nforecasting, or under the non-$\\ell_p$ robustness in statistic feature\naugmentation for time series classification~(TSC). Our review found that\nRandomized Smoothing performs modestly in TSC, struggling to provide effective\nassurances on datasets with poor robustness. Therefore, we propose a\nself-ensemble method to enhance the lower bound of the probability confidence\nof predicted labels by reducing the variance of classification margins, thereby\ncertifying a larger radius. This approach also addresses the computational\noverhead issue of Deep Ensemble~(DE) while remaining competitive and, in some\ncases, outperforming it in terms of robustness. Both theoretical analysis and\nexperimental results validate the effectiveness of our method, demonstrating\nsuperior performance in robustness testing compared to baseline approaches.\n", "link": "http://arxiv.org/abs/2409.02802v1", "date": "2024-09-04", "relevancy": 2.4871, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5233}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Certificate%20Robustness%20for%20Time%20Series%20Classification%20with%0A%20%20Efficient%20Self-Ensemble&body=Title%3A%20Boosting%20Certificate%20Robustness%20for%20Time%20Series%20Classification%20with%0A%20%20Efficient%20Self-Ensemble%0AAuthor%3A%20Chang%20Dong%20and%20Zhengyang%20Li%20and%20Liangwei%20Zheng%20and%20Weitong%20Chen%20and%20Wei%20Emma%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20the%20issue%20of%20adversarial%20robustness%20in%20the%20time%20series%20domain%20has%0Agarnered%20significant%20attention.%20However%2C%20the%20available%20defense%20mechanisms%0Aremain%20limited%2C%20with%20adversarial%20training%20being%20the%20predominant%20approach%2C%0Athough%20it%20does%20not%20provide%20theoretical%20guarantees.%20Randomized%20Smoothing%20has%0Aemerged%20as%20a%20standout%20method%20due%20to%20its%20ability%20to%20certify%20a%20provable%20lower%0Abound%20on%20robustness%20radius%20under%20%24%5Cell_p%24-ball%20attacks.%20Recognizing%20its%0Asuccess%2C%20research%20in%20the%20time%20series%20domain%20has%20started%20focusing%20on%20these%0Aaspects.%20However%2C%20existing%20research%20predominantly%20focuses%20on%20time%20series%0Aforecasting%2C%20or%20under%20the%20non-%24%5Cell_p%24%20robustness%20in%20statistic%20feature%0Aaugmentation%20for%20time%20series%20classification~%28TSC%29.%20Our%20review%20found%20that%0ARandomized%20Smoothing%20performs%20modestly%20in%20TSC%2C%20struggling%20to%20provide%20effective%0Aassurances%20on%20datasets%20with%20poor%20robustness.%20Therefore%2C%20we%20propose%20a%0Aself-ensemble%20method%20to%20enhance%20the%20lower%20bound%20of%20the%20probability%20confidence%0Aof%20predicted%20labels%20by%20reducing%20the%20variance%20of%20classification%20margins%2C%20thereby%0Acertifying%20a%20larger%20radius.%20This%20approach%20also%20addresses%20the%20computational%0Aoverhead%20issue%20of%20Deep%20Ensemble~%28DE%29%20while%20remaining%20competitive%20and%2C%20in%20some%0Acases%2C%20outperforming%20it%20in%20terms%20of%20robustness.%20Both%20theoretical%20analysis%20and%0Aexperimental%20results%20validate%20the%20effectiveness%20of%20our%20method%2C%20demonstrating%0Asuperior%20performance%20in%20robustness%20testing%20compared%20to%20baseline%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Certificate%2520Robustness%2520for%2520Time%2520Series%2520Classification%2520with%250A%2520%2520Efficient%2520Self-Ensemble%26entry.906535625%3DChang%2520Dong%2520and%2520Zhengyang%2520Li%2520and%2520Liangwei%2520Zheng%2520and%2520Weitong%2520Chen%2520and%2520Wei%2520Emma%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520issue%2520of%2520adversarial%2520robustness%2520in%2520the%2520time%2520series%2520domain%2520has%250Agarnered%2520significant%2520attention.%2520However%252C%2520the%2520available%2520defense%2520mechanisms%250Aremain%2520limited%252C%2520with%2520adversarial%2520training%2520being%2520the%2520predominant%2520approach%252C%250Athough%2520it%2520does%2520not%2520provide%2520theoretical%2520guarantees.%2520Randomized%2520Smoothing%2520has%250Aemerged%2520as%2520a%2520standout%2520method%2520due%2520to%2520its%2520ability%2520to%2520certify%2520a%2520provable%2520lower%250Abound%2520on%2520robustness%2520radius%2520under%2520%2524%255Cell_p%2524-ball%2520attacks.%2520Recognizing%2520its%250Asuccess%252C%2520research%2520in%2520the%2520time%2520series%2520domain%2520has%2520started%2520focusing%2520on%2520these%250Aaspects.%2520However%252C%2520existing%2520research%2520predominantly%2520focuses%2520on%2520time%2520series%250Aforecasting%252C%2520or%2520under%2520the%2520non-%2524%255Cell_p%2524%2520robustness%2520in%2520statistic%2520feature%250Aaugmentation%2520for%2520time%2520series%2520classification~%2528TSC%2529.%2520Our%2520review%2520found%2520that%250ARandomized%2520Smoothing%2520performs%2520modestly%2520in%2520TSC%252C%2520struggling%2520to%2520provide%2520effective%250Aassurances%2520on%2520datasets%2520with%2520poor%2520robustness.%2520Therefore%252C%2520we%2520propose%2520a%250Aself-ensemble%2520method%2520to%2520enhance%2520the%2520lower%2520bound%2520of%2520the%2520probability%2520confidence%250Aof%2520predicted%2520labels%2520by%2520reducing%2520the%2520variance%2520of%2520classification%2520margins%252C%2520thereby%250Acertifying%2520a%2520larger%2520radius.%2520This%2520approach%2520also%2520addresses%2520the%2520computational%250Aoverhead%2520issue%2520of%2520Deep%2520Ensemble~%2528DE%2529%2520while%2520remaining%2520competitive%2520and%252C%2520in%2520some%250Acases%252C%2520outperforming%2520it%2520in%2520terms%2520of%2520robustness.%2520Both%2520theoretical%2520analysis%2520and%250Aexperimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520demonstrating%250Asuperior%2520performance%2520in%2520robustness%2520testing%2520compared%2520to%2520baseline%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Certificate%20Robustness%20for%20Time%20Series%20Classification%20with%0A%20%20Efficient%20Self-Ensemble&entry.906535625=Chang%20Dong%20and%20Zhengyang%20Li%20and%20Liangwei%20Zheng%20and%20Weitong%20Chen%20and%20Wei%20Emma%20Zhang&entry.1292438233=%20%20Recently%2C%20the%20issue%20of%20adversarial%20robustness%20in%20the%20time%20series%20domain%20has%0Agarnered%20significant%20attention.%20However%2C%20the%20available%20defense%20mechanisms%0Aremain%20limited%2C%20with%20adversarial%20training%20being%20the%20predominant%20approach%2C%0Athough%20it%20does%20not%20provide%20theoretical%20guarantees.%20Randomized%20Smoothing%20has%0Aemerged%20as%20a%20standout%20method%20due%20to%20its%20ability%20to%20certify%20a%20provable%20lower%0Abound%20on%20robustness%20radius%20under%20%24%5Cell_p%24-ball%20attacks.%20Recognizing%20its%0Asuccess%2C%20research%20in%20the%20time%20series%20domain%20has%20started%20focusing%20on%20these%0Aaspects.%20However%2C%20existing%20research%20predominantly%20focuses%20on%20time%20series%0Aforecasting%2C%20or%20under%20the%20non-%24%5Cell_p%24%20robustness%20in%20statistic%20feature%0Aaugmentation%20for%20time%20series%20classification~%28TSC%29.%20Our%20review%20found%20that%0ARandomized%20Smoothing%20performs%20modestly%20in%20TSC%2C%20struggling%20to%20provide%20effective%0Aassurances%20on%20datasets%20with%20poor%20robustness.%20Therefore%2C%20we%20propose%20a%0Aself-ensemble%20method%20to%20enhance%20the%20lower%20bound%20of%20the%20probability%20confidence%0Aof%20predicted%20labels%20by%20reducing%20the%20variance%20of%20classification%20margins%2C%20thereby%0Acertifying%20a%20larger%20radius.%20This%20approach%20also%20addresses%20the%20computational%0Aoverhead%20issue%20of%20Deep%20Ensemble~%28DE%29%20while%20remaining%20competitive%20and%2C%20in%20some%0Acases%2C%20outperforming%20it%20in%20terms%20of%20robustness.%20Both%20theoretical%20analysis%20and%0Aexperimental%20results%20validate%20the%20effectiveness%20of%20our%20method%2C%20demonstrating%0Asuperior%20performance%20in%20robustness%20testing%20compared%20to%20baseline%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02802v1&entry.124074799=Read"},
{"title": "Hybrid Decentralized Optimization: Leveraging Both First- and\n  Zeroth-Order Optimizers for Faster Convergence", "author": "Matin Ansaripour and Shayan Talaei and Giorgi Nadiradze and Dan Alistarh", "abstract": "  Distributed optimization is the standard way of speeding up machine learning\ntraining, and most of the research in the area focuses on distributed\nfirst-order, gradient-based methods. Yet, there are settings where some\ncomputationally-bounded nodes may not be able to implement first-order,\ngradient-based optimization, while they could still contribute to joint\noptimization tasks. In this paper, we initiate the study of hybrid\ndecentralized optimization, studying settings where nodes with zeroth-order and\nfirst-order optimization capabilities co-exist in a distributed system, and\nattempt to jointly solve an optimization task over some data distribution. We\nessentially show that, under reasonable parameter settings, such a system can\nnot only withstand noisier zeroth-order agents but can even benefit from\nintegrating such agents into the optimization process, rather than ignoring\ntheir information. At the core of our approach is a new analysis of distributed\noptimization with noisy and possibly-biased gradient estimators, which may be\nof independent interest. Our results hold for both convex and non-convex\nobjectives. Experimental results on standard optimization tasks confirm our\nanalysis, showing that hybrid first-zeroth order optimization can be practical,\neven when training deep neural networks.\n", "link": "http://arxiv.org/abs/2210.07703v2", "date": "2024-09-04", "relevancy": 2.4802, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5403}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Decentralized%20Optimization%3A%20Leveraging%20Both%20First-%20and%0A%20%20Zeroth-Order%20Optimizers%20for%20Faster%20Convergence&body=Title%3A%20Hybrid%20Decentralized%20Optimization%3A%20Leveraging%20Both%20First-%20and%0A%20%20Zeroth-Order%20Optimizers%20for%20Faster%20Convergence%0AAuthor%3A%20Matin%20Ansaripour%20and%20Shayan%20Talaei%20and%20Giorgi%20Nadiradze%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Distributed%20optimization%20is%20the%20standard%20way%20of%20speeding%20up%20machine%20learning%0Atraining%2C%20and%20most%20of%20the%20research%20in%20the%20area%20focuses%20on%20distributed%0Afirst-order%2C%20gradient-based%20methods.%20Yet%2C%20there%20are%20settings%20where%20some%0Acomputationally-bounded%20nodes%20may%20not%20be%20able%20to%20implement%20first-order%2C%0Agradient-based%20optimization%2C%20while%20they%20could%20still%20contribute%20to%20joint%0Aoptimization%20tasks.%20In%20this%20paper%2C%20we%20initiate%20the%20study%20of%20hybrid%0Adecentralized%20optimization%2C%20studying%20settings%20where%20nodes%20with%20zeroth-order%20and%0Afirst-order%20optimization%20capabilities%20co-exist%20in%20a%20distributed%20system%2C%20and%0Aattempt%20to%20jointly%20solve%20an%20optimization%20task%20over%20some%20data%20distribution.%20We%0Aessentially%20show%20that%2C%20under%20reasonable%20parameter%20settings%2C%20such%20a%20system%20can%0Anot%20only%20withstand%20noisier%20zeroth-order%20agents%20but%20can%20even%20benefit%20from%0Aintegrating%20such%20agents%20into%20the%20optimization%20process%2C%20rather%20than%20ignoring%0Atheir%20information.%20At%20the%20core%20of%20our%20approach%20is%20a%20new%20analysis%20of%20distributed%0Aoptimization%20with%20noisy%20and%20possibly-biased%20gradient%20estimators%2C%20which%20may%20be%0Aof%20independent%20interest.%20Our%20results%20hold%20for%20both%20convex%20and%20non-convex%0Aobjectives.%20Experimental%20results%20on%20standard%20optimization%20tasks%20confirm%20our%0Aanalysis%2C%20showing%20that%20hybrid%20first-zeroth%20order%20optimization%20can%20be%20practical%2C%0Aeven%20when%20training%20deep%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.07703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Decentralized%2520Optimization%253A%2520Leveraging%2520Both%2520First-%2520and%250A%2520%2520Zeroth-Order%2520Optimizers%2520for%2520Faster%2520Convergence%26entry.906535625%3DMatin%2520Ansaripour%2520and%2520Shayan%2520Talaei%2520and%2520Giorgi%2520Nadiradze%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Distributed%2520optimization%2520is%2520the%2520standard%2520way%2520of%2520speeding%2520up%2520machine%2520learning%250Atraining%252C%2520and%2520most%2520of%2520the%2520research%2520in%2520the%2520area%2520focuses%2520on%2520distributed%250Afirst-order%252C%2520gradient-based%2520methods.%2520Yet%252C%2520there%2520are%2520settings%2520where%2520some%250Acomputationally-bounded%2520nodes%2520may%2520not%2520be%2520able%2520to%2520implement%2520first-order%252C%250Agradient-based%2520optimization%252C%2520while%2520they%2520could%2520still%2520contribute%2520to%2520joint%250Aoptimization%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520initiate%2520the%2520study%2520of%2520hybrid%250Adecentralized%2520optimization%252C%2520studying%2520settings%2520where%2520nodes%2520with%2520zeroth-order%2520and%250Afirst-order%2520optimization%2520capabilities%2520co-exist%2520in%2520a%2520distributed%2520system%252C%2520and%250Aattempt%2520to%2520jointly%2520solve%2520an%2520optimization%2520task%2520over%2520some%2520data%2520distribution.%2520We%250Aessentially%2520show%2520that%252C%2520under%2520reasonable%2520parameter%2520settings%252C%2520such%2520a%2520system%2520can%250Anot%2520only%2520withstand%2520noisier%2520zeroth-order%2520agents%2520but%2520can%2520even%2520benefit%2520from%250Aintegrating%2520such%2520agents%2520into%2520the%2520optimization%2520process%252C%2520rather%2520than%2520ignoring%250Atheir%2520information.%2520At%2520the%2520core%2520of%2520our%2520approach%2520is%2520a%2520new%2520analysis%2520of%2520distributed%250Aoptimization%2520with%2520noisy%2520and%2520possibly-biased%2520gradient%2520estimators%252C%2520which%2520may%2520be%250Aof%2520independent%2520interest.%2520Our%2520results%2520hold%2520for%2520both%2520convex%2520and%2520non-convex%250Aobjectives.%2520Experimental%2520results%2520on%2520standard%2520optimization%2520tasks%2520confirm%2520our%250Aanalysis%252C%2520showing%2520that%2520hybrid%2520first-zeroth%2520order%2520optimization%2520can%2520be%2520practical%252C%250Aeven%2520when%2520training%2520deep%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.07703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Decentralized%20Optimization%3A%20Leveraging%20Both%20First-%20and%0A%20%20Zeroth-Order%20Optimizers%20for%20Faster%20Convergence&entry.906535625=Matin%20Ansaripour%20and%20Shayan%20Talaei%20and%20Giorgi%20Nadiradze%20and%20Dan%20Alistarh&entry.1292438233=%20%20Distributed%20optimization%20is%20the%20standard%20way%20of%20speeding%20up%20machine%20learning%0Atraining%2C%20and%20most%20of%20the%20research%20in%20the%20area%20focuses%20on%20distributed%0Afirst-order%2C%20gradient-based%20methods.%20Yet%2C%20there%20are%20settings%20where%20some%0Acomputationally-bounded%20nodes%20may%20not%20be%20able%20to%20implement%20first-order%2C%0Agradient-based%20optimization%2C%20while%20they%20could%20still%20contribute%20to%20joint%0Aoptimization%20tasks.%20In%20this%20paper%2C%20we%20initiate%20the%20study%20of%20hybrid%0Adecentralized%20optimization%2C%20studying%20settings%20where%20nodes%20with%20zeroth-order%20and%0Afirst-order%20optimization%20capabilities%20co-exist%20in%20a%20distributed%20system%2C%20and%0Aattempt%20to%20jointly%20solve%20an%20optimization%20task%20over%20some%20data%20distribution.%20We%0Aessentially%20show%20that%2C%20under%20reasonable%20parameter%20settings%2C%20such%20a%20system%20can%0Anot%20only%20withstand%20noisier%20zeroth-order%20agents%20but%20can%20even%20benefit%20from%0Aintegrating%20such%20agents%20into%20the%20optimization%20process%2C%20rather%20than%20ignoring%0Atheir%20information.%20At%20the%20core%20of%20our%20approach%20is%20a%20new%20analysis%20of%20distributed%0Aoptimization%20with%20noisy%20and%20possibly-biased%20gradient%20estimators%2C%20which%20may%20be%0Aof%20independent%20interest.%20Our%20results%20hold%20for%20both%20convex%20and%20non-convex%0Aobjectives.%20Experimental%20results%20on%20standard%20optimization%20tasks%20confirm%20our%0Aanalysis%2C%20showing%20that%20hybrid%20first-zeroth%20order%20optimization%20can%20be%20practical%2C%0Aeven%20when%20training%20deep%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.07703v2&entry.124074799=Read"},
{"title": "Open Gaze: Open Source eye tracker for smartphone devices using Deep\n  Learning", "author": "Sushmanth reddy and Jyothi Swaroop Reddy", "abstract": "  Eye tracking has been a pivotal tool in diverse fields such as vision\nresearch, language analysis, and usability assessment. The majority of prior\ninvestigations, however, have concentrated on expansive desktop displays\nemploying specialized, costly eye tracking hardware that lacks scalability.\nRemarkably little insight exists into ocular movement patterns on smartphones,\ndespite their widespread adoption and significant usage. In this manuscript, we\npresent an open-source implementation of a smartphone-based gaze tracker that\nemulates the methodology proposed by a GooglePaper (whose source code remains\nproprietary). Our focus is on attaining accuracy comparable to that attained\nthrough the GooglePaper's methodology, without the necessity for supplementary\nhardware. Through the integration of machine learning techniques, we unveil an\naccurate eye tracking solution that is native to smartphones. Our approach\ndemonstrates precision akin to the state-of-the-art mobile eye trackers, which\nare characterized by a cost that is two orders of magnitude higher. Leveraging\nthe vast MIT GazeCapture dataset, which is available through registration on\nthe dataset's website, we successfully replicate crucial findings from previous\nstudies concerning ocular motion behavior in oculomotor tasks and saliency\nanalyses during natural image observation. Furthermore, we emphasize the\napplicability of smartphone-based gaze tracking in discerning reading\ncomprehension challenges. Our findings exhibit the inherent potential to\namplify eye movement research by significant proportions, accommodating\nparticipation from thousands of subjects with explicit consent. This\nscalability not only fosters advancements in vision research, but also extends\nits benefits to domains such as accessibility enhancement and healthcare\napplications.\n", "link": "http://arxiv.org/abs/2308.13495v3", "date": "2024-09-04", "relevancy": 2.4729, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5027}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4973}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Gaze%3A%20Open%20Source%20eye%20tracker%20for%20smartphone%20devices%20using%20Deep%0A%20%20Learning&body=Title%3A%20Open%20Gaze%3A%20Open%20Source%20eye%20tracker%20for%20smartphone%20devices%20using%20Deep%0A%20%20Learning%0AAuthor%3A%20Sushmanth%20reddy%20and%20Jyothi%20Swaroop%20Reddy%0AAbstract%3A%20%20%20Eye%20tracking%20has%20been%20a%20pivotal%20tool%20in%20diverse%20fields%20such%20as%20vision%0Aresearch%2C%20language%20analysis%2C%20and%20usability%20assessment.%20The%20majority%20of%20prior%0Ainvestigations%2C%20however%2C%20have%20concentrated%20on%20expansive%20desktop%20displays%0Aemploying%20specialized%2C%20costly%20eye%20tracking%20hardware%20that%20lacks%20scalability.%0ARemarkably%20little%20insight%20exists%20into%20ocular%20movement%20patterns%20on%20smartphones%2C%0Adespite%20their%20widespread%20adoption%20and%20significant%20usage.%20In%20this%20manuscript%2C%20we%0Apresent%20an%20open-source%20implementation%20of%20a%20smartphone-based%20gaze%20tracker%20that%0Aemulates%20the%20methodology%20proposed%20by%20a%20GooglePaper%20%28whose%20source%20code%20remains%0Aproprietary%29.%20Our%20focus%20is%20on%20attaining%20accuracy%20comparable%20to%20that%20attained%0Athrough%20the%20GooglePaper%27s%20methodology%2C%20without%20the%20necessity%20for%20supplementary%0Ahardware.%20Through%20the%20integration%20of%20machine%20learning%20techniques%2C%20we%20unveil%20an%0Aaccurate%20eye%20tracking%20solution%20that%20is%20native%20to%20smartphones.%20Our%20approach%0Ademonstrates%20precision%20akin%20to%20the%20state-of-the-art%20mobile%20eye%20trackers%2C%20which%0Aare%20characterized%20by%20a%20cost%20that%20is%20two%20orders%20of%20magnitude%20higher.%20Leveraging%0Athe%20vast%20MIT%20GazeCapture%20dataset%2C%20which%20is%20available%20through%20registration%20on%0Athe%20dataset%27s%20website%2C%20we%20successfully%20replicate%20crucial%20findings%20from%20previous%0Astudies%20concerning%20ocular%20motion%20behavior%20in%20oculomotor%20tasks%20and%20saliency%0Aanalyses%20during%20natural%20image%20observation.%20Furthermore%2C%20we%20emphasize%20the%0Aapplicability%20of%20smartphone-based%20gaze%20tracking%20in%20discerning%20reading%0Acomprehension%20challenges.%20Our%20findings%20exhibit%20the%20inherent%20potential%20to%0Aamplify%20eye%20movement%20research%20by%20significant%20proportions%2C%20accommodating%0Aparticipation%20from%20thousands%20of%20subjects%20with%20explicit%20consent.%20This%0Ascalability%20not%20only%20fosters%20advancements%20in%20vision%20research%2C%20but%20also%20extends%0Aits%20benefits%20to%20domains%20such%20as%20accessibility%20enhancement%20and%20healthcare%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13495v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Gaze%253A%2520Open%2520Source%2520eye%2520tracker%2520for%2520smartphone%2520devices%2520using%2520Deep%250A%2520%2520Learning%26entry.906535625%3DSushmanth%2520reddy%2520and%2520Jyothi%2520Swaroop%2520Reddy%26entry.1292438233%3D%2520%2520Eye%2520tracking%2520has%2520been%2520a%2520pivotal%2520tool%2520in%2520diverse%2520fields%2520such%2520as%2520vision%250Aresearch%252C%2520language%2520analysis%252C%2520and%2520usability%2520assessment.%2520The%2520majority%2520of%2520prior%250Ainvestigations%252C%2520however%252C%2520have%2520concentrated%2520on%2520expansive%2520desktop%2520displays%250Aemploying%2520specialized%252C%2520costly%2520eye%2520tracking%2520hardware%2520that%2520lacks%2520scalability.%250ARemarkably%2520little%2520insight%2520exists%2520into%2520ocular%2520movement%2520patterns%2520on%2520smartphones%252C%250Adespite%2520their%2520widespread%2520adoption%2520and%2520significant%2520usage.%2520In%2520this%2520manuscript%252C%2520we%250Apresent%2520an%2520open-source%2520implementation%2520of%2520a%2520smartphone-based%2520gaze%2520tracker%2520that%250Aemulates%2520the%2520methodology%2520proposed%2520by%2520a%2520GooglePaper%2520%2528whose%2520source%2520code%2520remains%250Aproprietary%2529.%2520Our%2520focus%2520is%2520on%2520attaining%2520accuracy%2520comparable%2520to%2520that%2520attained%250Athrough%2520the%2520GooglePaper%2527s%2520methodology%252C%2520without%2520the%2520necessity%2520for%2520supplementary%250Ahardware.%2520Through%2520the%2520integration%2520of%2520machine%2520learning%2520techniques%252C%2520we%2520unveil%2520an%250Aaccurate%2520eye%2520tracking%2520solution%2520that%2520is%2520native%2520to%2520smartphones.%2520Our%2520approach%250Ademonstrates%2520precision%2520akin%2520to%2520the%2520state-of-the-art%2520mobile%2520eye%2520trackers%252C%2520which%250Aare%2520characterized%2520by%2520a%2520cost%2520that%2520is%2520two%2520orders%2520of%2520magnitude%2520higher.%2520Leveraging%250Athe%2520vast%2520MIT%2520GazeCapture%2520dataset%252C%2520which%2520is%2520available%2520through%2520registration%2520on%250Athe%2520dataset%2527s%2520website%252C%2520we%2520successfully%2520replicate%2520crucial%2520findings%2520from%2520previous%250Astudies%2520concerning%2520ocular%2520motion%2520behavior%2520in%2520oculomotor%2520tasks%2520and%2520saliency%250Aanalyses%2520during%2520natural%2520image%2520observation.%2520Furthermore%252C%2520we%2520emphasize%2520the%250Aapplicability%2520of%2520smartphone-based%2520gaze%2520tracking%2520in%2520discerning%2520reading%250Acomprehension%2520challenges.%2520Our%2520findings%2520exhibit%2520the%2520inherent%2520potential%2520to%250Aamplify%2520eye%2520movement%2520research%2520by%2520significant%2520proportions%252C%2520accommodating%250Aparticipation%2520from%2520thousands%2520of%2520subjects%2520with%2520explicit%2520consent.%2520This%250Ascalability%2520not%2520only%2520fosters%2520advancements%2520in%2520vision%2520research%252C%2520but%2520also%2520extends%250Aits%2520benefits%2520to%2520domains%2520such%2520as%2520accessibility%2520enhancement%2520and%2520healthcare%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13495v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Gaze%3A%20Open%20Source%20eye%20tracker%20for%20smartphone%20devices%20using%20Deep%0A%20%20Learning&entry.906535625=Sushmanth%20reddy%20and%20Jyothi%20Swaroop%20Reddy&entry.1292438233=%20%20Eye%20tracking%20has%20been%20a%20pivotal%20tool%20in%20diverse%20fields%20such%20as%20vision%0Aresearch%2C%20language%20analysis%2C%20and%20usability%20assessment.%20The%20majority%20of%20prior%0Ainvestigations%2C%20however%2C%20have%20concentrated%20on%20expansive%20desktop%20displays%0Aemploying%20specialized%2C%20costly%20eye%20tracking%20hardware%20that%20lacks%20scalability.%0ARemarkably%20little%20insight%20exists%20into%20ocular%20movement%20patterns%20on%20smartphones%2C%0Adespite%20their%20widespread%20adoption%20and%20significant%20usage.%20In%20this%20manuscript%2C%20we%0Apresent%20an%20open-source%20implementation%20of%20a%20smartphone-based%20gaze%20tracker%20that%0Aemulates%20the%20methodology%20proposed%20by%20a%20GooglePaper%20%28whose%20source%20code%20remains%0Aproprietary%29.%20Our%20focus%20is%20on%20attaining%20accuracy%20comparable%20to%20that%20attained%0Athrough%20the%20GooglePaper%27s%20methodology%2C%20without%20the%20necessity%20for%20supplementary%0Ahardware.%20Through%20the%20integration%20of%20machine%20learning%20techniques%2C%20we%20unveil%20an%0Aaccurate%20eye%20tracking%20solution%20that%20is%20native%20to%20smartphones.%20Our%20approach%0Ademonstrates%20precision%20akin%20to%20the%20state-of-the-art%20mobile%20eye%20trackers%2C%20which%0Aare%20characterized%20by%20a%20cost%20that%20is%20two%20orders%20of%20magnitude%20higher.%20Leveraging%0Athe%20vast%20MIT%20GazeCapture%20dataset%2C%20which%20is%20available%20through%20registration%20on%0Athe%20dataset%27s%20website%2C%20we%20successfully%20replicate%20crucial%20findings%20from%20previous%0Astudies%20concerning%20ocular%20motion%20behavior%20in%20oculomotor%20tasks%20and%20saliency%0Aanalyses%20during%20natural%20image%20observation.%20Furthermore%2C%20we%20emphasize%20the%0Aapplicability%20of%20smartphone-based%20gaze%20tracking%20in%20discerning%20reading%0Acomprehension%20challenges.%20Our%20findings%20exhibit%20the%20inherent%20potential%20to%0Aamplify%20eye%20movement%20research%20by%20significant%20proportions%2C%20accommodating%0Aparticipation%20from%20thousands%20of%20subjects%20with%20explicit%20consent.%20This%0Ascalability%20not%20only%20fosters%20advancements%20in%20vision%20research%2C%20but%20also%20extends%0Aits%20benefits%20to%20domains%20such%20as%20accessibility%20enhancement%20and%20healthcare%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13495v3&entry.124074799=Read"},
{"title": "Benchmarking Spurious Bias in Few-Shot Image Classifiers", "author": "Guangtao Zheng and Wenqian Ye and Aidong Zhang", "abstract": "  Few-shot image classifiers are designed to recognize and classify new data\nwith minimal supervision and limited data but often show reliance on spurious\ncorrelations between classes and spurious attributes, known as spurious bias.\nSpurious correlations commonly hold in certain samples and few-shot classifiers\ncan suffer from spurious bias induced from them. There is an absence of an\nautomatic benchmarking system to assess the robustness of few-shot classifiers\nagainst spurious bias. In this paper, we propose a systematic and rigorous\nbenchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied\ndegrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates\nfew-shot evaluation tasks with biased attributes so that using them for\npredictions can demonstrate poor performance. To construct these tasks, we\npropose attribute-based sample selection strategies based on a pre-trained\nvision-language model, eliminating the need for manual dataset curation. This\nallows FewSTAB to automatically benchmark spurious bias using any existing test\ndata. FewSTAB offers evaluation results in a new dimension along with a new\ndesign guideline for building robust classifiers. Moreover, it can benchmark\nspurious bias in varied degrees and enable designs for varied degrees of\nrobustness. Its effectiveness is demonstrated through experiments on ten\nfew-shot learning methods across three datasets. We hope our framework can\ninspire new designs of robust few-shot classifiers. Our code is available at\nhttps://github.com/gtzheng/FewSTAB.\n", "link": "http://arxiv.org/abs/2409.02882v1", "date": "2024-09-04", "relevancy": 2.4579, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5134}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4826}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Spurious%20Bias%20in%20Few-Shot%20Image%20Classifiers&body=Title%3A%20Benchmarking%20Spurious%20Bias%20in%20Few-Shot%20Image%20Classifiers%0AAuthor%3A%20Guangtao%20Zheng%20and%20Wenqian%20Ye%20and%20Aidong%20Zhang%0AAbstract%3A%20%20%20Few-shot%20image%20classifiers%20are%20designed%20to%20recognize%20and%20classify%20new%20data%0Awith%20minimal%20supervision%20and%20limited%20data%20but%20often%20show%20reliance%20on%20spurious%0Acorrelations%20between%20classes%20and%20spurious%20attributes%2C%20known%20as%20spurious%20bias.%0ASpurious%20correlations%20commonly%20hold%20in%20certain%20samples%20and%20few-shot%20classifiers%0Acan%20suffer%20from%20spurious%20bias%20induced%20from%20them.%20There%20is%20an%20absence%20of%20an%0Aautomatic%20benchmarking%20system%20to%20assess%20the%20robustness%20of%20few-shot%20classifiers%0Aagainst%20spurious%20bias.%20In%20this%20paper%2C%20we%20propose%20a%20systematic%20and%20rigorous%0Abenchmark%20framework%2C%20termed%20FewSTAB%2C%20to%20fairly%20demonstrate%20and%20quantify%20varied%0Adegrees%20of%20robustness%20of%20few-shot%20classifiers%20to%20spurious%20bias.%20FewSTAB%20creates%0Afew-shot%20evaluation%20tasks%20with%20biased%20attributes%20so%20that%20using%20them%20for%0Apredictions%20can%20demonstrate%20poor%20performance.%20To%20construct%20these%20tasks%2C%20we%0Apropose%20attribute-based%20sample%20selection%20strategies%20based%20on%20a%20pre-trained%0Avision-language%20model%2C%20eliminating%20the%20need%20for%20manual%20dataset%20curation.%20This%0Aallows%20FewSTAB%20to%20automatically%20benchmark%20spurious%20bias%20using%20any%20existing%20test%0Adata.%20FewSTAB%20offers%20evaluation%20results%20in%20a%20new%20dimension%20along%20with%20a%20new%0Adesign%20guideline%20for%20building%20robust%20classifiers.%20Moreover%2C%20it%20can%20benchmark%0Aspurious%20bias%20in%20varied%20degrees%20and%20enable%20designs%20for%20varied%20degrees%20of%0Arobustness.%20Its%20effectiveness%20is%20demonstrated%20through%20experiments%20on%20ten%0Afew-shot%20learning%20methods%20across%20three%20datasets.%20We%20hope%20our%20framework%20can%0Ainspire%20new%20designs%20of%20robust%20few-shot%20classifiers.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gtzheng/FewSTAB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Spurious%2520Bias%2520in%2520Few-Shot%2520Image%2520Classifiers%26entry.906535625%3DGuangtao%2520Zheng%2520and%2520Wenqian%2520Ye%2520and%2520Aidong%2520Zhang%26entry.1292438233%3D%2520%2520Few-shot%2520image%2520classifiers%2520are%2520designed%2520to%2520recognize%2520and%2520classify%2520new%2520data%250Awith%2520minimal%2520supervision%2520and%2520limited%2520data%2520but%2520often%2520show%2520reliance%2520on%2520spurious%250Acorrelations%2520between%2520classes%2520and%2520spurious%2520attributes%252C%2520known%2520as%2520spurious%2520bias.%250ASpurious%2520correlations%2520commonly%2520hold%2520in%2520certain%2520samples%2520and%2520few-shot%2520classifiers%250Acan%2520suffer%2520from%2520spurious%2520bias%2520induced%2520from%2520them.%2520There%2520is%2520an%2520absence%2520of%2520an%250Aautomatic%2520benchmarking%2520system%2520to%2520assess%2520the%2520robustness%2520of%2520few-shot%2520classifiers%250Aagainst%2520spurious%2520bias.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520systematic%2520and%2520rigorous%250Abenchmark%2520framework%252C%2520termed%2520FewSTAB%252C%2520to%2520fairly%2520demonstrate%2520and%2520quantify%2520varied%250Adegrees%2520of%2520robustness%2520of%2520few-shot%2520classifiers%2520to%2520spurious%2520bias.%2520FewSTAB%2520creates%250Afew-shot%2520evaluation%2520tasks%2520with%2520biased%2520attributes%2520so%2520that%2520using%2520them%2520for%250Apredictions%2520can%2520demonstrate%2520poor%2520performance.%2520To%2520construct%2520these%2520tasks%252C%2520we%250Apropose%2520attribute-based%2520sample%2520selection%2520strategies%2520based%2520on%2520a%2520pre-trained%250Avision-language%2520model%252C%2520eliminating%2520the%2520need%2520for%2520manual%2520dataset%2520curation.%2520This%250Aallows%2520FewSTAB%2520to%2520automatically%2520benchmark%2520spurious%2520bias%2520using%2520any%2520existing%2520test%250Adata.%2520FewSTAB%2520offers%2520evaluation%2520results%2520in%2520a%2520new%2520dimension%2520along%2520with%2520a%2520new%250Adesign%2520guideline%2520for%2520building%2520robust%2520classifiers.%2520Moreover%252C%2520it%2520can%2520benchmark%250Aspurious%2520bias%2520in%2520varied%2520degrees%2520and%2520enable%2520designs%2520for%2520varied%2520degrees%2520of%250Arobustness.%2520Its%2520effectiveness%2520is%2520demonstrated%2520through%2520experiments%2520on%2520ten%250Afew-shot%2520learning%2520methods%2520across%2520three%2520datasets.%2520We%2520hope%2520our%2520framework%2520can%250Ainspire%2520new%2520designs%2520of%2520robust%2520few-shot%2520classifiers.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/gtzheng/FewSTAB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Spurious%20Bias%20in%20Few-Shot%20Image%20Classifiers&entry.906535625=Guangtao%20Zheng%20and%20Wenqian%20Ye%20and%20Aidong%20Zhang&entry.1292438233=%20%20Few-shot%20image%20classifiers%20are%20designed%20to%20recognize%20and%20classify%20new%20data%0Awith%20minimal%20supervision%20and%20limited%20data%20but%20often%20show%20reliance%20on%20spurious%0Acorrelations%20between%20classes%20and%20spurious%20attributes%2C%20known%20as%20spurious%20bias.%0ASpurious%20correlations%20commonly%20hold%20in%20certain%20samples%20and%20few-shot%20classifiers%0Acan%20suffer%20from%20spurious%20bias%20induced%20from%20them.%20There%20is%20an%20absence%20of%20an%0Aautomatic%20benchmarking%20system%20to%20assess%20the%20robustness%20of%20few-shot%20classifiers%0Aagainst%20spurious%20bias.%20In%20this%20paper%2C%20we%20propose%20a%20systematic%20and%20rigorous%0Abenchmark%20framework%2C%20termed%20FewSTAB%2C%20to%20fairly%20demonstrate%20and%20quantify%20varied%0Adegrees%20of%20robustness%20of%20few-shot%20classifiers%20to%20spurious%20bias.%20FewSTAB%20creates%0Afew-shot%20evaluation%20tasks%20with%20biased%20attributes%20so%20that%20using%20them%20for%0Apredictions%20can%20demonstrate%20poor%20performance.%20To%20construct%20these%20tasks%2C%20we%0Apropose%20attribute-based%20sample%20selection%20strategies%20based%20on%20a%20pre-trained%0Avision-language%20model%2C%20eliminating%20the%20need%20for%20manual%20dataset%20curation.%20This%0Aallows%20FewSTAB%20to%20automatically%20benchmark%20spurious%20bias%20using%20any%20existing%20test%0Adata.%20FewSTAB%20offers%20evaluation%20results%20in%20a%20new%20dimension%20along%20with%20a%20new%0Adesign%20guideline%20for%20building%20robust%20classifiers.%20Moreover%2C%20it%20can%20benchmark%0Aspurious%20bias%20in%20varied%20degrees%20and%20enable%20designs%20for%20varied%20degrees%20of%0Arobustness.%20Its%20effectiveness%20is%20demonstrated%20through%20experiments%20on%20ten%0Afew-shot%20learning%20methods%20across%20three%20datasets.%20We%20hope%20our%20framework%20can%0Ainspire%20new%20designs%20of%20robust%20few-shot%20classifiers.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gtzheng/FewSTAB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02882v1&entry.124074799=Read"},
{"title": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models", "author": "Quan Li and Tianxiang Zhao and Lingwei Chen and Junjie Xu and Suhang Wang", "abstract": "  Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.\n", "link": "http://arxiv.org/abs/2407.13989v3", "date": "2024-09-04", "relevancy": 2.4497, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5257}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Graph%20Neural%20Networks%20with%20Limited%20Labeled%20Data%20by%20Actively%0A%20%20Distilling%20Knowledge%20from%20Large%20Language%20Models&body=Title%3A%20Enhancing%20Graph%20Neural%20Networks%20with%20Limited%20Labeled%20Data%20by%20Actively%0A%20%20Distilling%20Knowledge%20from%20Large%20Language%20Models%0AAuthor%3A%20Quan%20Li%20and%20Tianxiang%20Zhao%20and%20Lingwei%20Chen%20and%20Junjie%20Xu%20and%20Suhang%20Wang%0AAbstract%3A%20%20%20Graphs%20are%20pervasive%20in%20the%20real-world%2C%20such%20as%20social%20network%20analysis%2C%0Abioinformatics%2C%20and%20knowledge%20graphs.%20Graph%20neural%20networks%20%28GNNs%29%20have%20great%0Aability%20in%20node%20classification%2C%20a%20fundamental%20task%20on%20graphs.%20Unfortunately%2C%0Aconventional%20GNNs%20still%20face%20challenges%20in%20scenarios%20with%20few%20labeled%20nodes%2C%0Adespite%20the%20prevalence%20of%20few-shot%20node%20classification%20tasks%20in%20real-world%0Aapplications.%20To%20address%20this%20challenge%2C%20various%20approaches%20have%20been%20proposed%2C%0Aincluding%20graph%20meta-learning%2C%20transfer%20learning%2C%20and%20methods%20based%20on%20Large%0ALanguage%20Models%20%28LLMs%29.%20However%2C%20traditional%20meta-learning%20and%20transfer%0Alearning%20methods%20often%20require%20prior%20knowledge%20from%20base%20classes%20or%20fail%20to%0Aexploit%20the%20potential%20advantages%20of%20unlabeled%20nodes.%20Meanwhile%2C%20LLM-based%0Amethods%20may%20overlook%20the%20zero-shot%20capabilities%20of%20LLMs%20and%20rely%20heavily%20on%20the%0Aquality%20of%20generated%20contexts.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%0Aintegrates%20LLMs%20and%20GNNs%2C%20leveraging%20the%20zero-shot%20inference%20and%20reasoning%0Acapabilities%20of%20LLMs%20and%20employing%20a%20Graph-LLM-based%20active%20learning%20paradigm%0Ato%20enhance%20GNNs%27%20performance.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20in%20improving%20node%20classification%20accuracy%20with%0Aconsiderably%20limited%20labeled%20data%2C%20surpassing%20state-of-the-art%20baselines%20by%0Asignificant%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Graph%2520Neural%2520Networks%2520with%2520Limited%2520Labeled%2520Data%2520by%2520Actively%250A%2520%2520Distilling%2520Knowledge%2520from%2520Large%2520Language%2520Models%26entry.906535625%3DQuan%2520Li%2520and%2520Tianxiang%2520Zhao%2520and%2520Lingwei%2520Chen%2520and%2520Junjie%2520Xu%2520and%2520Suhang%2520Wang%26entry.1292438233%3D%2520%2520Graphs%2520are%2520pervasive%2520in%2520the%2520real-world%252C%2520such%2520as%2520social%2520network%2520analysis%252C%250Abioinformatics%252C%2520and%2520knowledge%2520graphs.%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520great%250Aability%2520in%2520node%2520classification%252C%2520a%2520fundamental%2520task%2520on%2520graphs.%2520Unfortunately%252C%250Aconventional%2520GNNs%2520still%2520face%2520challenges%2520in%2520scenarios%2520with%2520few%2520labeled%2520nodes%252C%250Adespite%2520the%2520prevalence%2520of%2520few-shot%2520node%2520classification%2520tasks%2520in%2520real-world%250Aapplications.%2520To%2520address%2520this%2520challenge%252C%2520various%2520approaches%2520have%2520been%2520proposed%252C%250Aincluding%2520graph%2520meta-learning%252C%2520transfer%2520learning%252C%2520and%2520methods%2520based%2520on%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520However%252C%2520traditional%2520meta-learning%2520and%2520transfer%250Alearning%2520methods%2520often%2520require%2520prior%2520knowledge%2520from%2520base%2520classes%2520or%2520fail%2520to%250Aexploit%2520the%2520potential%2520advantages%2520of%2520unlabeled%2520nodes.%2520Meanwhile%252C%2520LLM-based%250Amethods%2520may%2520overlook%2520the%2520zero-shot%2520capabilities%2520of%2520LLMs%2520and%2520rely%2520heavily%2520on%2520the%250Aquality%2520of%2520generated%2520contexts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%250Aintegrates%2520LLMs%2520and%2520GNNs%252C%2520leveraging%2520the%2520zero-shot%2520inference%2520and%2520reasoning%250Acapabilities%2520of%2520LLMs%2520and%2520employing%2520a%2520Graph-LLM-based%2520active%2520learning%2520paradigm%250Ato%2520enhance%2520GNNs%2527%2520performance.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520model%2520in%2520improving%2520node%2520classification%2520accuracy%2520with%250Aconsiderably%2520limited%2520labeled%2520data%252C%2520surpassing%2520state-of-the-art%2520baselines%2520by%250Asignificant%2520margins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Graph%20Neural%20Networks%20with%20Limited%20Labeled%20Data%20by%20Actively%0A%20%20Distilling%20Knowledge%20from%20Large%20Language%20Models&entry.906535625=Quan%20Li%20and%20Tianxiang%20Zhao%20and%20Lingwei%20Chen%20and%20Junjie%20Xu%20and%20Suhang%20Wang&entry.1292438233=%20%20Graphs%20are%20pervasive%20in%20the%20real-world%2C%20such%20as%20social%20network%20analysis%2C%0Abioinformatics%2C%20and%20knowledge%20graphs.%20Graph%20neural%20networks%20%28GNNs%29%20have%20great%0Aability%20in%20node%20classification%2C%20a%20fundamental%20task%20on%20graphs.%20Unfortunately%2C%0Aconventional%20GNNs%20still%20face%20challenges%20in%20scenarios%20with%20few%20labeled%20nodes%2C%0Adespite%20the%20prevalence%20of%20few-shot%20node%20classification%20tasks%20in%20real-world%0Aapplications.%20To%20address%20this%20challenge%2C%20various%20approaches%20have%20been%20proposed%2C%0Aincluding%20graph%20meta-learning%2C%20transfer%20learning%2C%20and%20methods%20based%20on%20Large%0ALanguage%20Models%20%28LLMs%29.%20However%2C%20traditional%20meta-learning%20and%20transfer%0Alearning%20methods%20often%20require%20prior%20knowledge%20from%20base%20classes%20or%20fail%20to%0Aexploit%20the%20potential%20advantages%20of%20unlabeled%20nodes.%20Meanwhile%2C%20LLM-based%0Amethods%20may%20overlook%20the%20zero-shot%20capabilities%20of%20LLMs%20and%20rely%20heavily%20on%20the%0Aquality%20of%20generated%20contexts.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%0Aintegrates%20LLMs%20and%20GNNs%2C%20leveraging%20the%20zero-shot%20inference%20and%20reasoning%0Acapabilities%20of%20LLMs%20and%20employing%20a%20Graph-LLM-based%20active%20learning%20paradigm%0Ato%20enhance%20GNNs%27%20performance.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20in%20improving%20node%20classification%20accuracy%20with%0Aconsiderably%20limited%20labeled%20data%2C%20surpassing%20state-of-the-art%20baselines%20by%0Asignificant%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13989v3&entry.124074799=Read"},
{"title": "PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for\n  One-Shot Talking Head Generation", "author": "Jun Ling and Yiwen Wang and Han Xue and Rong Xie and Li Song", "abstract": "  While previous audio-driven talking head generation (THG) methods generate\nhead poses from driving audio, the generated poses or lips cannot match the\naudio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a\nTHG system that can freely generate lip-synchronized talking head videos with\nfree head poses conditioned on text prompts and audio. The core insight of our\nmethod is using head pose to connect visual, linguistic, and audio signals.\nFirst, we propose to generate poses from both audio and text prompts, where the\naudio offers short-term variations and rhythm correspondence of the head\nmovements and the text prompts describe the long-term semantics of head\nmotions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to\ngenerate motion latent from text prompts and audio cues in a pose latent space.\nSecond, we observe a loss-imbalance problem: the loss for the lip region\ncontributes less than 4\\% of the total reconstruction loss caused by both pose\nand lip, making optimization lean towards head movements rather than lip\nshapes. To address this issue, we propose a refinement-based learning strategy\nto synthesize natural talking videos using two cascaded networks, i.e.,\nCoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce\nanimated images in novel poses and the RefineNet focuses on learning finer lip\nmotions by progressively estimating lip motions from low-to-high resolutions,\nyielding improved lip-synchronization performance. Experiments demonstrate our\npose prediction strategy achieves better pose diversity and realness compared\nto text-only or audio-only, and our video generator model outperforms\nstate-of-the-art methods in synthesizing talking videos with natural head\nmotions. Project: https://junleen.github.io/projects/posetalk.\n", "link": "http://arxiv.org/abs/2409.02657v1", "date": "2024-09-04", "relevancy": 2.3924, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6215}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6211}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseTalk%3A%20Text-and-Audio-based%20Pose%20Control%20and%20Motion%20Refinement%20for%0A%20%20One-Shot%20Talking%20Head%20Generation&body=Title%3A%20PoseTalk%3A%20Text-and-Audio-based%20Pose%20Control%20and%20Motion%20Refinement%20for%0A%20%20One-Shot%20Talking%20Head%20Generation%0AAuthor%3A%20Jun%20Ling%20and%20Yiwen%20Wang%20and%20Han%20Xue%20and%20Rong%20Xie%20and%20Li%20Song%0AAbstract%3A%20%20%20While%20previous%20audio-driven%20talking%20head%20generation%20%28THG%29%20methods%20generate%0Ahead%20poses%20from%20driving%20audio%2C%20the%20generated%20poses%20or%20lips%20cannot%20match%20the%0Aaudio%20well%20or%20are%20not%20editable.%20In%20this%20study%2C%20we%20propose%20%5Ctextbf%7BPoseTalk%7D%2C%20a%0ATHG%20system%20that%20can%20freely%20generate%20lip-synchronized%20talking%20head%20videos%20with%0Afree%20head%20poses%20conditioned%20on%20text%20prompts%20and%20audio.%20The%20core%20insight%20of%20our%0Amethod%20is%20using%20head%20pose%20to%20connect%20visual%2C%20linguistic%2C%20and%20audio%20signals.%0AFirst%2C%20we%20propose%20to%20generate%20poses%20from%20both%20audio%20and%20text%20prompts%2C%20where%20the%0Aaudio%20offers%20short-term%20variations%20and%20rhythm%20correspondence%20of%20the%20head%0Amovements%20and%20the%20text%20prompts%20describe%20the%20long-term%20semantics%20of%20head%0Amotions.%20To%20achieve%20this%20goal%2C%20we%20devise%20a%20Pose%20Latent%20Diffusion%20%28PLD%29%20model%20to%0Agenerate%20motion%20latent%20from%20text%20prompts%20and%20audio%20cues%20in%20a%20pose%20latent%20space.%0ASecond%2C%20we%20observe%20a%20loss-imbalance%20problem%3A%20the%20loss%20for%20the%20lip%20region%0Acontributes%20less%20than%204%5C%25%20of%20the%20total%20reconstruction%20loss%20caused%20by%20both%20pose%0Aand%20lip%2C%20making%20optimization%20lean%20towards%20head%20movements%20rather%20than%20lip%0Ashapes.%20To%20address%20this%20issue%2C%20we%20propose%20a%20refinement-based%20learning%20strategy%0Ato%20synthesize%20natural%20talking%20videos%20using%20two%20cascaded%20networks%2C%20i.e.%2C%0ACoarseNet%2C%20and%20RefineNet.%20The%20CoarseNet%20estimates%20coarse%20motions%20to%20produce%0Aanimated%20images%20in%20novel%20poses%20and%20the%20RefineNet%20focuses%20on%20learning%20finer%20lip%0Amotions%20by%20progressively%20estimating%20lip%20motions%20from%20low-to-high%20resolutions%2C%0Ayielding%20improved%20lip-synchronization%20performance.%20Experiments%20demonstrate%20our%0Apose%20prediction%20strategy%20achieves%20better%20pose%20diversity%20and%20realness%20compared%0Ato%20text-only%20or%20audio-only%2C%20and%20our%20video%20generator%20model%20outperforms%0Astate-of-the-art%20methods%20in%20synthesizing%20talking%20videos%20with%20natural%20head%0Amotions.%20Project%3A%20https%3A//junleen.github.io/projects/posetalk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseTalk%253A%2520Text-and-Audio-based%2520Pose%2520Control%2520and%2520Motion%2520Refinement%2520for%250A%2520%2520One-Shot%2520Talking%2520Head%2520Generation%26entry.906535625%3DJun%2520Ling%2520and%2520Yiwen%2520Wang%2520and%2520Han%2520Xue%2520and%2520Rong%2520Xie%2520and%2520Li%2520Song%26entry.1292438233%3D%2520%2520While%2520previous%2520audio-driven%2520talking%2520head%2520generation%2520%2528THG%2529%2520methods%2520generate%250Ahead%2520poses%2520from%2520driving%2520audio%252C%2520the%2520generated%2520poses%2520or%2520lips%2520cannot%2520match%2520the%250Aaudio%2520well%2520or%2520are%2520not%2520editable.%2520In%2520this%2520study%252C%2520we%2520propose%2520%255Ctextbf%257BPoseTalk%257D%252C%2520a%250ATHG%2520system%2520that%2520can%2520freely%2520generate%2520lip-synchronized%2520talking%2520head%2520videos%2520with%250Afree%2520head%2520poses%2520conditioned%2520on%2520text%2520prompts%2520and%2520audio.%2520The%2520core%2520insight%2520of%2520our%250Amethod%2520is%2520using%2520head%2520pose%2520to%2520connect%2520visual%252C%2520linguistic%252C%2520and%2520audio%2520signals.%250AFirst%252C%2520we%2520propose%2520to%2520generate%2520poses%2520from%2520both%2520audio%2520and%2520text%2520prompts%252C%2520where%2520the%250Aaudio%2520offers%2520short-term%2520variations%2520and%2520rhythm%2520correspondence%2520of%2520the%2520head%250Amovements%2520and%2520the%2520text%2520prompts%2520describe%2520the%2520long-term%2520semantics%2520of%2520head%250Amotions.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520devise%2520a%2520Pose%2520Latent%2520Diffusion%2520%2528PLD%2529%2520model%2520to%250Agenerate%2520motion%2520latent%2520from%2520text%2520prompts%2520and%2520audio%2520cues%2520in%2520a%2520pose%2520latent%2520space.%250ASecond%252C%2520we%2520observe%2520a%2520loss-imbalance%2520problem%253A%2520the%2520loss%2520for%2520the%2520lip%2520region%250Acontributes%2520less%2520than%25204%255C%2525%2520of%2520the%2520total%2520reconstruction%2520loss%2520caused%2520by%2520both%2520pose%250Aand%2520lip%252C%2520making%2520optimization%2520lean%2520towards%2520head%2520movements%2520rather%2520than%2520lip%250Ashapes.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520refinement-based%2520learning%2520strategy%250Ato%2520synthesize%2520natural%2520talking%2520videos%2520using%2520two%2520cascaded%2520networks%252C%2520i.e.%252C%250ACoarseNet%252C%2520and%2520RefineNet.%2520The%2520CoarseNet%2520estimates%2520coarse%2520motions%2520to%2520produce%250Aanimated%2520images%2520in%2520novel%2520poses%2520and%2520the%2520RefineNet%2520focuses%2520on%2520learning%2520finer%2520lip%250Amotions%2520by%2520progressively%2520estimating%2520lip%2520motions%2520from%2520low-to-high%2520resolutions%252C%250Ayielding%2520improved%2520lip-synchronization%2520performance.%2520Experiments%2520demonstrate%2520our%250Apose%2520prediction%2520strategy%2520achieves%2520better%2520pose%2520diversity%2520and%2520realness%2520compared%250Ato%2520text-only%2520or%2520audio-only%252C%2520and%2520our%2520video%2520generator%2520model%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520synthesizing%2520talking%2520videos%2520with%2520natural%2520head%250Amotions.%2520Project%253A%2520https%253A//junleen.github.io/projects/posetalk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseTalk%3A%20Text-and-Audio-based%20Pose%20Control%20and%20Motion%20Refinement%20for%0A%20%20One-Shot%20Talking%20Head%20Generation&entry.906535625=Jun%20Ling%20and%20Yiwen%20Wang%20and%20Han%20Xue%20and%20Rong%20Xie%20and%20Li%20Song&entry.1292438233=%20%20While%20previous%20audio-driven%20talking%20head%20generation%20%28THG%29%20methods%20generate%0Ahead%20poses%20from%20driving%20audio%2C%20the%20generated%20poses%20or%20lips%20cannot%20match%20the%0Aaudio%20well%20or%20are%20not%20editable.%20In%20this%20study%2C%20we%20propose%20%5Ctextbf%7BPoseTalk%7D%2C%20a%0ATHG%20system%20that%20can%20freely%20generate%20lip-synchronized%20talking%20head%20videos%20with%0Afree%20head%20poses%20conditioned%20on%20text%20prompts%20and%20audio.%20The%20core%20insight%20of%20our%0Amethod%20is%20using%20head%20pose%20to%20connect%20visual%2C%20linguistic%2C%20and%20audio%20signals.%0AFirst%2C%20we%20propose%20to%20generate%20poses%20from%20both%20audio%20and%20text%20prompts%2C%20where%20the%0Aaudio%20offers%20short-term%20variations%20and%20rhythm%20correspondence%20of%20the%20head%0Amovements%20and%20the%20text%20prompts%20describe%20the%20long-term%20semantics%20of%20head%0Amotions.%20To%20achieve%20this%20goal%2C%20we%20devise%20a%20Pose%20Latent%20Diffusion%20%28PLD%29%20model%20to%0Agenerate%20motion%20latent%20from%20text%20prompts%20and%20audio%20cues%20in%20a%20pose%20latent%20space.%0ASecond%2C%20we%20observe%20a%20loss-imbalance%20problem%3A%20the%20loss%20for%20the%20lip%20region%0Acontributes%20less%20than%204%5C%25%20of%20the%20total%20reconstruction%20loss%20caused%20by%20both%20pose%0Aand%20lip%2C%20making%20optimization%20lean%20towards%20head%20movements%20rather%20than%20lip%0Ashapes.%20To%20address%20this%20issue%2C%20we%20propose%20a%20refinement-based%20learning%20strategy%0Ato%20synthesize%20natural%20talking%20videos%20using%20two%20cascaded%20networks%2C%20i.e.%2C%0ACoarseNet%2C%20and%20RefineNet.%20The%20CoarseNet%20estimates%20coarse%20motions%20to%20produce%0Aanimated%20images%20in%20novel%20poses%20and%20the%20RefineNet%20focuses%20on%20learning%20finer%20lip%0Amotions%20by%20progressively%20estimating%20lip%20motions%20from%20low-to-high%20resolutions%2C%0Ayielding%20improved%20lip-synchronization%20performance.%20Experiments%20demonstrate%20our%0Apose%20prediction%20strategy%20achieves%20better%20pose%20diversity%20and%20realness%20compared%0Ato%20text-only%20or%20audio-only%2C%20and%20our%20video%20generator%20model%20outperforms%0Astate-of-the-art%20methods%20in%20synthesizing%20talking%20videos%20with%20natural%20head%0Amotions.%20Project%3A%20https%3A//junleen.github.io/projects/posetalk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02657v1&entry.124074799=Read"},
{"title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency", "author": "Jianwen Jiang and Chao Liang and Jiaqi Yang and Gaojie Lin and Tianyun Zhong and Yanbo Zheng", "abstract": "  With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios.\n", "link": "http://arxiv.org/abs/2409.02634v1", "date": "2024-09-04", "relevancy": 2.3883, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6474}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5874}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loopy%3A%20Taming%20Audio-Driven%20Portrait%20Avatar%20with%20Long-Term%20Motion%0A%20%20Dependency&body=Title%3A%20Loopy%3A%20Taming%20Audio-Driven%20Portrait%20Avatar%20with%20Long-Term%20Motion%0A%20%20Dependency%0AAuthor%3A%20Jianwen%20Jiang%20and%20Chao%20Liang%20and%20Jiaqi%20Yang%20and%20Gaojie%20Lin%20and%20Tianyun%20Zhong%20and%20Yanbo%20Zheng%0AAbstract%3A%20%20%20With%20the%20introduction%20of%20diffusion-based%20video%20generation%20techniques%2C%0Aaudio-conditioned%20human%20video%20generation%20has%20recently%20achieved%20significant%0Abreakthroughs%20in%20both%20the%20naturalness%20of%20motion%20and%20the%20synthesis%20of%20portrait%0Adetails.%20Due%20to%20the%20limited%20control%20of%20audio%20signals%20in%20driving%20human%20motion%2C%0Aexisting%20methods%20often%20add%20auxiliary%20spatial%20signals%20to%20stabilize%20movements%2C%0Awhich%20may%20compromise%20the%20naturalness%20and%20freedom%20of%20motion.%20In%20this%20paper%2C%20we%0Apropose%20an%20end-to-end%20audio-only%20conditioned%20video%20diffusion%20model%20named%20Loopy.%0ASpecifically%2C%20we%20designed%20an%20inter-%20and%20intra-clip%20temporal%20module%20and%20an%0Aaudio-to-latents%20module%2C%20enabling%20the%20model%20to%20leverage%20long-term%20motion%0Ainformation%20from%20the%20data%20to%20learn%20natural%20motion%20patterns%20and%20improving%0Aaudio-portrait%20movement%20correlation.%20This%20method%20removes%20the%20need%20for%20manually%0Aspecified%20spatial%20motion%20templates%20used%20in%20existing%20methods%20to%20constrain%20motion%0Aduring%20inference.%20Extensive%20experiments%20show%20that%20Loopy%20outperforms%20recent%0Aaudio-driven%20portrait%20diffusion%20models%2C%20delivering%20more%20lifelike%20and%0Ahigh-quality%20results%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoopy%253A%2520Taming%2520Audio-Driven%2520Portrait%2520Avatar%2520with%2520Long-Term%2520Motion%250A%2520%2520Dependency%26entry.906535625%3DJianwen%2520Jiang%2520and%2520Chao%2520Liang%2520and%2520Jiaqi%2520Yang%2520and%2520Gaojie%2520Lin%2520and%2520Tianyun%2520Zhong%2520and%2520Yanbo%2520Zheng%26entry.1292438233%3D%2520%2520With%2520the%2520introduction%2520of%2520diffusion-based%2520video%2520generation%2520techniques%252C%250Aaudio-conditioned%2520human%2520video%2520generation%2520has%2520recently%2520achieved%2520significant%250Abreakthroughs%2520in%2520both%2520the%2520naturalness%2520of%2520motion%2520and%2520the%2520synthesis%2520of%2520portrait%250Adetails.%2520Due%2520to%2520the%2520limited%2520control%2520of%2520audio%2520signals%2520in%2520driving%2520human%2520motion%252C%250Aexisting%2520methods%2520often%2520add%2520auxiliary%2520spatial%2520signals%2520to%2520stabilize%2520movements%252C%250Awhich%2520may%2520compromise%2520the%2520naturalness%2520and%2520freedom%2520of%2520motion.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520end-to-end%2520audio-only%2520conditioned%2520video%2520diffusion%2520model%2520named%2520Loopy.%250ASpecifically%252C%2520we%2520designed%2520an%2520inter-%2520and%2520intra-clip%2520temporal%2520module%2520and%2520an%250Aaudio-to-latents%2520module%252C%2520enabling%2520the%2520model%2520to%2520leverage%2520long-term%2520motion%250Ainformation%2520from%2520the%2520data%2520to%2520learn%2520natural%2520motion%2520patterns%2520and%2520improving%250Aaudio-portrait%2520movement%2520correlation.%2520This%2520method%2520removes%2520the%2520need%2520for%2520manually%250Aspecified%2520spatial%2520motion%2520templates%2520used%2520in%2520existing%2520methods%2520to%2520constrain%2520motion%250Aduring%2520inference.%2520Extensive%2520experiments%2520show%2520that%2520Loopy%2520outperforms%2520recent%250Aaudio-driven%2520portrait%2520diffusion%2520models%252C%2520delivering%2520more%2520lifelike%2520and%250Ahigh-quality%2520results%2520across%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loopy%3A%20Taming%20Audio-Driven%20Portrait%20Avatar%20with%20Long-Term%20Motion%0A%20%20Dependency&entry.906535625=Jianwen%20Jiang%20and%20Chao%20Liang%20and%20Jiaqi%20Yang%20and%20Gaojie%20Lin%20and%20Tianyun%20Zhong%20and%20Yanbo%20Zheng&entry.1292438233=%20%20With%20the%20introduction%20of%20diffusion-based%20video%20generation%20techniques%2C%0Aaudio-conditioned%20human%20video%20generation%20has%20recently%20achieved%20significant%0Abreakthroughs%20in%20both%20the%20naturalness%20of%20motion%20and%20the%20synthesis%20of%20portrait%0Adetails.%20Due%20to%20the%20limited%20control%20of%20audio%20signals%20in%20driving%20human%20motion%2C%0Aexisting%20methods%20often%20add%20auxiliary%20spatial%20signals%20to%20stabilize%20movements%2C%0Awhich%20may%20compromise%20the%20naturalness%20and%20freedom%20of%20motion.%20In%20this%20paper%2C%20we%0Apropose%20an%20end-to-end%20audio-only%20conditioned%20video%20diffusion%20model%20named%20Loopy.%0ASpecifically%2C%20we%20designed%20an%20inter-%20and%20intra-clip%20temporal%20module%20and%20an%0Aaudio-to-latents%20module%2C%20enabling%20the%20model%20to%20leverage%20long-term%20motion%0Ainformation%20from%20the%20data%20to%20learn%20natural%20motion%20patterns%20and%20improving%0Aaudio-portrait%20movement%20correlation.%20This%20method%20removes%20the%20need%20for%20manually%0Aspecified%20spatial%20motion%20templates%20used%20in%20existing%20methods%20to%20constrain%20motion%0Aduring%20inference.%20Extensive%20experiments%20show%20that%20Loopy%20outperforms%20recent%0Aaudio-driven%20portrait%20diffusion%20models%2C%20delivering%20more%20lifelike%20and%0Ahigh-quality%20results%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02634v1&entry.124074799=Read"},
{"title": "HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical\n  MLLM Prompts", "author": "Xinyu Liu and Yingqing He and Lanqing Guo and Xiang Li and Bu Jin and Peng Li and Yan Li and Chi-Min Chan and Qifeng Chen and Wei Xue and Wenhan Luo and Qingfeng Liu and Yike Guo", "abstract": "  The potential for higher-resolution image generation using pretrained\ndiffusion models is immense, yet these models often struggle with issues of\nobject repetition and structural artifacts especially when scaling to 4K\nresolution and higher. We figure out that the problem is caused by that, a\nsingle prompt for the generation of multiple scales provides insufficient\nefficacy. In response, we propose HiPrompt, a new tuning-free solution that\ntackles the above problems by introducing hierarchical prompts. The\nhierarchical prompts offer both global and local guidance. Specifically, the\nglobal guidance comes from the user input that describes the overall content,\nwhile the local guidance utilizes patch-wise descriptions from MLLMs to\nelaborately guide the regional structure and texture generation. Furthermore,\nduring the inverse denoising process, the generated noise is decomposed into\nlow- and high-frequency spatial components. These components are conditioned on\nmultiple prompt levels, including detailed patch-wise descriptions and broader\nimage-level prompts, facilitating prompt-guided denoising under hierarchical\nsemantic guidance. It further allows the generation to focus more on local\nspatial regions and ensures the generated images maintain coherent local and\nglobal semantics, structures, and textures with high definition. Extensive\nexperiments demonstrate that HiPrompt outperforms state-of-the-art works in\nhigher-resolution image generation, significantly reducing object repetition\nand enhancing structural quality.\n", "link": "http://arxiv.org/abs/2409.02919v1", "date": "2024-09-04", "relevancy": 2.314, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5944}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5747}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiPrompt%3A%20Tuning-free%20Higher-Resolution%20Generation%20with%20Hierarchical%0A%20%20MLLM%20Prompts&body=Title%3A%20HiPrompt%3A%20Tuning-free%20Higher-Resolution%20Generation%20with%20Hierarchical%0A%20%20MLLM%20Prompts%0AAuthor%3A%20Xinyu%20Liu%20and%20Yingqing%20He%20and%20Lanqing%20Guo%20and%20Xiang%20Li%20and%20Bu%20Jin%20and%20Peng%20Li%20and%20Yan%20Li%20and%20Chi-Min%20Chan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Qingfeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20The%20potential%20for%20higher-resolution%20image%20generation%20using%20pretrained%0Adiffusion%20models%20is%20immense%2C%20yet%20these%20models%20often%20struggle%20with%20issues%20of%0Aobject%20repetition%20and%20structural%20artifacts%20especially%20when%20scaling%20to%204K%0Aresolution%20and%20higher.%20We%20figure%20out%20that%20the%20problem%20is%20caused%20by%20that%2C%20a%0Asingle%20prompt%20for%20the%20generation%20of%20multiple%20scales%20provides%20insufficient%0Aefficacy.%20In%20response%2C%20we%20propose%20HiPrompt%2C%20a%20new%20tuning-free%20solution%20that%0Atackles%20the%20above%20problems%20by%20introducing%20hierarchical%20prompts.%20The%0Ahierarchical%20prompts%20offer%20both%20global%20and%20local%20guidance.%20Specifically%2C%20the%0Aglobal%20guidance%20comes%20from%20the%20user%20input%20that%20describes%20the%20overall%20content%2C%0Awhile%20the%20local%20guidance%20utilizes%20patch-wise%20descriptions%20from%20MLLMs%20to%0Aelaborately%20guide%20the%20regional%20structure%20and%20texture%20generation.%20Furthermore%2C%0Aduring%20the%20inverse%20denoising%20process%2C%20the%20generated%20noise%20is%20decomposed%20into%0Alow-%20and%20high-frequency%20spatial%20components.%20These%20components%20are%20conditioned%20on%0Amultiple%20prompt%20levels%2C%20including%20detailed%20patch-wise%20descriptions%20and%20broader%0Aimage-level%20prompts%2C%20facilitating%20prompt-guided%20denoising%20under%20hierarchical%0Asemantic%20guidance.%20It%20further%20allows%20the%20generation%20to%20focus%20more%20on%20local%0Aspatial%20regions%20and%20ensures%20the%20generated%20images%20maintain%20coherent%20local%20and%0Aglobal%20semantics%2C%20structures%2C%20and%20textures%20with%20high%20definition.%20Extensive%0Aexperiments%20demonstrate%20that%20HiPrompt%20outperforms%20state-of-the-art%20works%20in%0Ahigher-resolution%20image%20generation%2C%20significantly%20reducing%20object%20repetition%0Aand%20enhancing%20structural%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiPrompt%253A%2520Tuning-free%2520Higher-Resolution%2520Generation%2520with%2520Hierarchical%250A%2520%2520MLLM%2520Prompts%26entry.906535625%3DXinyu%2520Liu%2520and%2520Yingqing%2520He%2520and%2520Lanqing%2520Guo%2520and%2520Xiang%2520Li%2520and%2520Bu%2520Jin%2520and%2520Peng%2520Li%2520and%2520Yan%2520Li%2520and%2520Chi-Min%2520Chan%2520and%2520Qifeng%2520Chen%2520and%2520Wei%2520Xue%2520and%2520Wenhan%2520Luo%2520and%2520Qingfeng%2520Liu%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520The%2520potential%2520for%2520higher-resolution%2520image%2520generation%2520using%2520pretrained%250Adiffusion%2520models%2520is%2520immense%252C%2520yet%2520these%2520models%2520often%2520struggle%2520with%2520issues%2520of%250Aobject%2520repetition%2520and%2520structural%2520artifacts%2520especially%2520when%2520scaling%2520to%25204K%250Aresolution%2520and%2520higher.%2520We%2520figure%2520out%2520that%2520the%2520problem%2520is%2520caused%2520by%2520that%252C%2520a%250Asingle%2520prompt%2520for%2520the%2520generation%2520of%2520multiple%2520scales%2520provides%2520insufficient%250Aefficacy.%2520In%2520response%252C%2520we%2520propose%2520HiPrompt%252C%2520a%2520new%2520tuning-free%2520solution%2520that%250Atackles%2520the%2520above%2520problems%2520by%2520introducing%2520hierarchical%2520prompts.%2520The%250Ahierarchical%2520prompts%2520offer%2520both%2520global%2520and%2520local%2520guidance.%2520Specifically%252C%2520the%250Aglobal%2520guidance%2520comes%2520from%2520the%2520user%2520input%2520that%2520describes%2520the%2520overall%2520content%252C%250Awhile%2520the%2520local%2520guidance%2520utilizes%2520patch-wise%2520descriptions%2520from%2520MLLMs%2520to%250Aelaborately%2520guide%2520the%2520regional%2520structure%2520and%2520texture%2520generation.%2520Furthermore%252C%250Aduring%2520the%2520inverse%2520denoising%2520process%252C%2520the%2520generated%2520noise%2520is%2520decomposed%2520into%250Alow-%2520and%2520high-frequency%2520spatial%2520components.%2520These%2520components%2520are%2520conditioned%2520on%250Amultiple%2520prompt%2520levels%252C%2520including%2520detailed%2520patch-wise%2520descriptions%2520and%2520broader%250Aimage-level%2520prompts%252C%2520facilitating%2520prompt-guided%2520denoising%2520under%2520hierarchical%250Asemantic%2520guidance.%2520It%2520further%2520allows%2520the%2520generation%2520to%2520focus%2520more%2520on%2520local%250Aspatial%2520regions%2520and%2520ensures%2520the%2520generated%2520images%2520maintain%2520coherent%2520local%2520and%250Aglobal%2520semantics%252C%2520structures%252C%2520and%2520textures%2520with%2520high%2520definition.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520HiPrompt%2520outperforms%2520state-of-the-art%2520works%2520in%250Ahigher-resolution%2520image%2520generation%252C%2520significantly%2520reducing%2520object%2520repetition%250Aand%2520enhancing%2520structural%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiPrompt%3A%20Tuning-free%20Higher-Resolution%20Generation%20with%20Hierarchical%0A%20%20MLLM%20Prompts&entry.906535625=Xinyu%20Liu%20and%20Yingqing%20He%20and%20Lanqing%20Guo%20and%20Xiang%20Li%20and%20Bu%20Jin%20and%20Peng%20Li%20and%20Yan%20Li%20and%20Chi-Min%20Chan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Qingfeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20The%20potential%20for%20higher-resolution%20image%20generation%20using%20pretrained%0Adiffusion%20models%20is%20immense%2C%20yet%20these%20models%20often%20struggle%20with%20issues%20of%0Aobject%20repetition%20and%20structural%20artifacts%20especially%20when%20scaling%20to%204K%0Aresolution%20and%20higher.%20We%20figure%20out%20that%20the%20problem%20is%20caused%20by%20that%2C%20a%0Asingle%20prompt%20for%20the%20generation%20of%20multiple%20scales%20provides%20insufficient%0Aefficacy.%20In%20response%2C%20we%20propose%20HiPrompt%2C%20a%20new%20tuning-free%20solution%20that%0Atackles%20the%20above%20problems%20by%20introducing%20hierarchical%20prompts.%20The%0Ahierarchical%20prompts%20offer%20both%20global%20and%20local%20guidance.%20Specifically%2C%20the%0Aglobal%20guidance%20comes%20from%20the%20user%20input%20that%20describes%20the%20overall%20content%2C%0Awhile%20the%20local%20guidance%20utilizes%20patch-wise%20descriptions%20from%20MLLMs%20to%0Aelaborately%20guide%20the%20regional%20structure%20and%20texture%20generation.%20Furthermore%2C%0Aduring%20the%20inverse%20denoising%20process%2C%20the%20generated%20noise%20is%20decomposed%20into%0Alow-%20and%20high-frequency%20spatial%20components.%20These%20components%20are%20conditioned%20on%0Amultiple%20prompt%20levels%2C%20including%20detailed%20patch-wise%20descriptions%20and%20broader%0Aimage-level%20prompts%2C%20facilitating%20prompt-guided%20denoising%20under%20hierarchical%0Asemantic%20guidance.%20It%20further%20allows%20the%20generation%20to%20focus%20more%20on%20local%0Aspatial%20regions%20and%20ensures%20the%20generated%20images%20maintain%20coherent%20local%20and%0Aglobal%20semantics%2C%20structures%2C%20and%20textures%20with%20high%20definition.%20Extensive%0Aexperiments%20demonstrate%20that%20HiPrompt%20outperforms%20state-of-the-art%20works%20in%0Ahigher-resolution%20image%20generation%2C%20significantly%20reducing%20object%20repetition%0Aand%20enhancing%20structural%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02919v1&entry.124074799=Read"},
{"title": "Revisiting Character-level Adversarial Attacks for Language Models", "author": "Elias Abad Rocamora and Yongtao Wu and Fanghui Liu and Grigorios G. Chrysos and Volkan Cevher", "abstract": "  Adversarial attacks in Natural Language Processing apply perturbations in the\ncharacter or token levels. Token-level attacks, gaining prominence for their\nuse of gradient-based methods, are susceptible to altering sentence semantics,\nleading to invalid adversarial examples. While character-level attacks easily\nmaintain semantics, they have received less attention as they cannot easily\nadopt popular gradient-based methods, and are thought to be easy to defend.\nChallenging these beliefs, we introduce Charmer, an efficient query-based\nadversarial attack capable of achieving high attack success rate (ASR) while\ngenerating highly similar adversarial examples. Our method successfully targets\nboth small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2,\nCharmer improves the ASR in 4.84% points and the USE similarity in 8% points\nwith respect to the previous art. Our implementation is available in\nhttps://github.com/LIONS-EPFL/Charmer.\n", "link": "http://arxiv.org/abs/2405.04346v2", "date": "2024-09-04", "relevancy": 2.3137, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5021}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4692}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Character-level%20Adversarial%20Attacks%20for%20Language%20Models&body=Title%3A%20Revisiting%20Character-level%20Adversarial%20Attacks%20for%20Language%20Models%0AAuthor%3A%20Elias%20Abad%20Rocamora%20and%20Yongtao%20Wu%20and%20Fanghui%20Liu%20and%20Grigorios%20G.%20Chrysos%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Adversarial%20attacks%20in%20Natural%20Language%20Processing%20apply%20perturbations%20in%20the%0Acharacter%20or%20token%20levels.%20Token-level%20attacks%2C%20gaining%20prominence%20for%20their%0Ause%20of%20gradient-based%20methods%2C%20are%20susceptible%20to%20altering%20sentence%20semantics%2C%0Aleading%20to%20invalid%20adversarial%20examples.%20While%20character-level%20attacks%20easily%0Amaintain%20semantics%2C%20they%20have%20received%20less%20attention%20as%20they%20cannot%20easily%0Aadopt%20popular%20gradient-based%20methods%2C%20and%20are%20thought%20to%20be%20easy%20to%20defend.%0AChallenging%20these%20beliefs%2C%20we%20introduce%20Charmer%2C%20an%20efficient%20query-based%0Aadversarial%20attack%20capable%20of%20achieving%20high%20attack%20success%20rate%20%28ASR%29%20while%0Agenerating%20highly%20similar%20adversarial%20examples.%20Our%20method%20successfully%20targets%0Aboth%20small%20%28BERT%29%20and%20large%20%28Llama%202%29%20models.%20Specifically%2C%20on%20BERT%20with%20SST-2%2C%0ACharmer%20improves%20the%20ASR%20in%204.84%25%20points%20and%20the%20USE%20similarity%20in%208%25%20points%0Awith%20respect%20to%20the%20previous%20art.%20Our%20implementation%20is%20available%20in%0Ahttps%3A//github.com/LIONS-EPFL/Charmer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Character-level%2520Adversarial%2520Attacks%2520for%2520Language%2520Models%26entry.906535625%3DElias%2520Abad%2520Rocamora%2520and%2520Yongtao%2520Wu%2520and%2520Fanghui%2520Liu%2520and%2520Grigorios%2520G.%2520Chrysos%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520in%2520Natural%2520Language%2520Processing%2520apply%2520perturbations%2520in%2520the%250Acharacter%2520or%2520token%2520levels.%2520Token-level%2520attacks%252C%2520gaining%2520prominence%2520for%2520their%250Ause%2520of%2520gradient-based%2520methods%252C%2520are%2520susceptible%2520to%2520altering%2520sentence%2520semantics%252C%250Aleading%2520to%2520invalid%2520adversarial%2520examples.%2520While%2520character-level%2520attacks%2520easily%250Amaintain%2520semantics%252C%2520they%2520have%2520received%2520less%2520attention%2520as%2520they%2520cannot%2520easily%250Aadopt%2520popular%2520gradient-based%2520methods%252C%2520and%2520are%2520thought%2520to%2520be%2520easy%2520to%2520defend.%250AChallenging%2520these%2520beliefs%252C%2520we%2520introduce%2520Charmer%252C%2520an%2520efficient%2520query-based%250Aadversarial%2520attack%2520capable%2520of%2520achieving%2520high%2520attack%2520success%2520rate%2520%2528ASR%2529%2520while%250Agenerating%2520highly%2520similar%2520adversarial%2520examples.%2520Our%2520method%2520successfully%2520targets%250Aboth%2520small%2520%2528BERT%2529%2520and%2520large%2520%2528Llama%25202%2529%2520models.%2520Specifically%252C%2520on%2520BERT%2520with%2520SST-2%252C%250ACharmer%2520improves%2520the%2520ASR%2520in%25204.84%2525%2520points%2520and%2520the%2520USE%2520similarity%2520in%25208%2525%2520points%250Awith%2520respect%2520to%2520the%2520previous%2520art.%2520Our%2520implementation%2520is%2520available%2520in%250Ahttps%253A//github.com/LIONS-EPFL/Charmer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Character-level%20Adversarial%20Attacks%20for%20Language%20Models&entry.906535625=Elias%20Abad%20Rocamora%20and%20Yongtao%20Wu%20and%20Fanghui%20Liu%20and%20Grigorios%20G.%20Chrysos%20and%20Volkan%20Cevher&entry.1292438233=%20%20Adversarial%20attacks%20in%20Natural%20Language%20Processing%20apply%20perturbations%20in%20the%0Acharacter%20or%20token%20levels.%20Token-level%20attacks%2C%20gaining%20prominence%20for%20their%0Ause%20of%20gradient-based%20methods%2C%20are%20susceptible%20to%20altering%20sentence%20semantics%2C%0Aleading%20to%20invalid%20adversarial%20examples.%20While%20character-level%20attacks%20easily%0Amaintain%20semantics%2C%20they%20have%20received%20less%20attention%20as%20they%20cannot%20easily%0Aadopt%20popular%20gradient-based%20methods%2C%20and%20are%20thought%20to%20be%20easy%20to%20defend.%0AChallenging%20these%20beliefs%2C%20we%20introduce%20Charmer%2C%20an%20efficient%20query-based%0Aadversarial%20attack%20capable%20of%20achieving%20high%20attack%20success%20rate%20%28ASR%29%20while%0Agenerating%20highly%20similar%20adversarial%20examples.%20Our%20method%20successfully%20targets%0Aboth%20small%20%28BERT%29%20and%20large%20%28Llama%202%29%20models.%20Specifically%2C%20on%20BERT%20with%20SST-2%2C%0ACharmer%20improves%20the%20ASR%20in%204.84%25%20points%20and%20the%20USE%20similarity%20in%208%25%20points%0Awith%20respect%20to%20the%20previous%20art.%20Our%20implementation%20is%20available%20in%0Ahttps%3A//github.com/LIONS-EPFL/Charmer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04346v2&entry.124074799=Read"},
{"title": "GenoCraft: A Comprehensive, User-Friendly Web-Based Platform for\n  High-Throughput Omics Data Analysis and Visualization", "author": "Yingzhou Lu and Minjie Shen and Ling Yue and Chenhao Li and Fan Meng and Xiao Wang and David Herrington and Yue Wang and Yue Zhao and Tianfan Fu and Capucine Van Rechem", "abstract": "  The surge in high-throughput omics data has reshaped the landscape of\nbiological research, underlining the need for powerful, user-friendly data\nanalysis and interpretation tools. This paper presents GenoCraft, a web-based\ncomprehensive software solution designed to handle the entire pipeline of omics\ndata processing. GenoCraft offers a unified platform featuring advanced\nbioinformatics tools, covering all aspects of omics data analysis. It\nencompasses a range of functionalities, such as normalization, quality control,\ndifferential analysis, network analysis, pathway analysis, and diverse\nvisualization techniques. This software makes state-of-the-art omics data\nanalysis more accessible to a wider range of users. With GenoCraft, researchers\nand data scientists have access to an array of cutting-edge bioinformatics\ntools under a user-friendly interface, making it a valuable resource for\nmanaging and analyzing large-scale omics data. The API with an interactive web\ninterface is publicly available at https://genocraft.stanford. edu/. We also\nrelease all the codes in https://github.com/futianfan/GenoCraft.\n", "link": "http://arxiv.org/abs/2312.14249v2", "date": "2024-09-04", "relevancy": 2.3119, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4826}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4523}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenoCraft%3A%20A%20Comprehensive%2C%20User-Friendly%20Web-Based%20Platform%20for%0A%20%20High-Throughput%20Omics%20Data%20Analysis%20and%20Visualization&body=Title%3A%20GenoCraft%3A%20A%20Comprehensive%2C%20User-Friendly%20Web-Based%20Platform%20for%0A%20%20High-Throughput%20Omics%20Data%20Analysis%20and%20Visualization%0AAuthor%3A%20Yingzhou%20Lu%20and%20Minjie%20Shen%20and%20Ling%20Yue%20and%20Chenhao%20Li%20and%20Fan%20Meng%20and%20Xiao%20Wang%20and%20David%20Herrington%20and%20Yue%20Wang%20and%20Yue%20Zhao%20and%20Tianfan%20Fu%20and%20Capucine%20Van%20Rechem%0AAbstract%3A%20%20%20The%20surge%20in%20high-throughput%20omics%20data%20has%20reshaped%20the%20landscape%20of%0Abiological%20research%2C%20underlining%20the%20need%20for%20powerful%2C%20user-friendly%20data%0Aanalysis%20and%20interpretation%20tools.%20This%20paper%20presents%20GenoCraft%2C%20a%20web-based%0Acomprehensive%20software%20solution%20designed%20to%20handle%20the%20entire%20pipeline%20of%20omics%0Adata%20processing.%20GenoCraft%20offers%20a%20unified%20platform%20featuring%20advanced%0Abioinformatics%20tools%2C%20covering%20all%20aspects%20of%20omics%20data%20analysis.%20It%0Aencompasses%20a%20range%20of%20functionalities%2C%20such%20as%20normalization%2C%20quality%20control%2C%0Adifferential%20analysis%2C%20network%20analysis%2C%20pathway%20analysis%2C%20and%20diverse%0Avisualization%20techniques.%20This%20software%20makes%20state-of-the-art%20omics%20data%0Aanalysis%20more%20accessible%20to%20a%20wider%20range%20of%20users.%20With%20GenoCraft%2C%20researchers%0Aand%20data%20scientists%20have%20access%20to%20an%20array%20of%20cutting-edge%20bioinformatics%0Atools%20under%20a%20user-friendly%20interface%2C%20making%20it%20a%20valuable%20resource%20for%0Amanaging%20and%20analyzing%20large-scale%20omics%20data.%20The%20API%20with%20an%20interactive%20web%0Ainterface%20is%20publicly%20available%20at%20https%3A//genocraft.stanford.%20edu/.%20We%20also%0Arelease%20all%20the%20codes%20in%20https%3A//github.com/futianfan/GenoCraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenoCraft%253A%2520A%2520Comprehensive%252C%2520User-Friendly%2520Web-Based%2520Platform%2520for%250A%2520%2520High-Throughput%2520Omics%2520Data%2520Analysis%2520and%2520Visualization%26entry.906535625%3DYingzhou%2520Lu%2520and%2520Minjie%2520Shen%2520and%2520Ling%2520Yue%2520and%2520Chenhao%2520Li%2520and%2520Fan%2520Meng%2520and%2520Xiao%2520Wang%2520and%2520David%2520Herrington%2520and%2520Yue%2520Wang%2520and%2520Yue%2520Zhao%2520and%2520Tianfan%2520Fu%2520and%2520Capucine%2520Van%2520Rechem%26entry.1292438233%3D%2520%2520The%2520surge%2520in%2520high-throughput%2520omics%2520data%2520has%2520reshaped%2520the%2520landscape%2520of%250Abiological%2520research%252C%2520underlining%2520the%2520need%2520for%2520powerful%252C%2520user-friendly%2520data%250Aanalysis%2520and%2520interpretation%2520tools.%2520This%2520paper%2520presents%2520GenoCraft%252C%2520a%2520web-based%250Acomprehensive%2520software%2520solution%2520designed%2520to%2520handle%2520the%2520entire%2520pipeline%2520of%2520omics%250Adata%2520processing.%2520GenoCraft%2520offers%2520a%2520unified%2520platform%2520featuring%2520advanced%250Abioinformatics%2520tools%252C%2520covering%2520all%2520aspects%2520of%2520omics%2520data%2520analysis.%2520It%250Aencompasses%2520a%2520range%2520of%2520functionalities%252C%2520such%2520as%2520normalization%252C%2520quality%2520control%252C%250Adifferential%2520analysis%252C%2520network%2520analysis%252C%2520pathway%2520analysis%252C%2520and%2520diverse%250Avisualization%2520techniques.%2520This%2520software%2520makes%2520state-of-the-art%2520omics%2520data%250Aanalysis%2520more%2520accessible%2520to%2520a%2520wider%2520range%2520of%2520users.%2520With%2520GenoCraft%252C%2520researchers%250Aand%2520data%2520scientists%2520have%2520access%2520to%2520an%2520array%2520of%2520cutting-edge%2520bioinformatics%250Atools%2520under%2520a%2520user-friendly%2520interface%252C%2520making%2520it%2520a%2520valuable%2520resource%2520for%250Amanaging%2520and%2520analyzing%2520large-scale%2520omics%2520data.%2520The%2520API%2520with%2520an%2520interactive%2520web%250Ainterface%2520is%2520publicly%2520available%2520at%2520https%253A//genocraft.stanford.%2520edu/.%2520We%2520also%250Arelease%2520all%2520the%2520codes%2520in%2520https%253A//github.com/futianfan/GenoCraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenoCraft%3A%20A%20Comprehensive%2C%20User-Friendly%20Web-Based%20Platform%20for%0A%20%20High-Throughput%20Omics%20Data%20Analysis%20and%20Visualization&entry.906535625=Yingzhou%20Lu%20and%20Minjie%20Shen%20and%20Ling%20Yue%20and%20Chenhao%20Li%20and%20Fan%20Meng%20and%20Xiao%20Wang%20and%20David%20Herrington%20and%20Yue%20Wang%20and%20Yue%20Zhao%20and%20Tianfan%20Fu%20and%20Capucine%20Van%20Rechem&entry.1292438233=%20%20The%20surge%20in%20high-throughput%20omics%20data%20has%20reshaped%20the%20landscape%20of%0Abiological%20research%2C%20underlining%20the%20need%20for%20powerful%2C%20user-friendly%20data%0Aanalysis%20and%20interpretation%20tools.%20This%20paper%20presents%20GenoCraft%2C%20a%20web-based%0Acomprehensive%20software%20solution%20designed%20to%20handle%20the%20entire%20pipeline%20of%20omics%0Adata%20processing.%20GenoCraft%20offers%20a%20unified%20platform%20featuring%20advanced%0Abioinformatics%20tools%2C%20covering%20all%20aspects%20of%20omics%20data%20analysis.%20It%0Aencompasses%20a%20range%20of%20functionalities%2C%20such%20as%20normalization%2C%20quality%20control%2C%0Adifferential%20analysis%2C%20network%20analysis%2C%20pathway%20analysis%2C%20and%20diverse%0Avisualization%20techniques.%20This%20software%20makes%20state-of-the-art%20omics%20data%0Aanalysis%20more%20accessible%20to%20a%20wider%20range%20of%20users.%20With%20GenoCraft%2C%20researchers%0Aand%20data%20scientists%20have%20access%20to%20an%20array%20of%20cutting-edge%20bioinformatics%0Atools%20under%20a%20user-friendly%20interface%2C%20making%20it%20a%20valuable%20resource%20for%0Amanaging%20and%20analyzing%20large-scale%20omics%20data.%20The%20API%20with%20an%20interactive%20web%0Ainterface%20is%20publicly%20available%20at%20https%3A//genocraft.stanford.%20edu/.%20We%20also%0Arelease%20all%20the%20codes%20in%20https%3A//github.com/futianfan/GenoCraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14249v2&entry.124074799=Read"},
{"title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture", "author": "Xidong Wang and Dingjie Song and Shunian Chen and Chen Zhang and Benyou Wang", "abstract": "  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n", "link": "http://arxiv.org/abs/2409.02889v1", "date": "2024-09-04", "relevancy": 2.2919, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6096}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5505}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongLLaVA%3A%20Scaling%20Multi-modal%20LLMs%20to%201000%20Images%20Efficiently%20via%0A%20%20Hybrid%20Architecture&body=Title%3A%20LongLLaVA%3A%20Scaling%20Multi-modal%20LLMs%20to%201000%20Images%20Efficiently%20via%0A%20%20Hybrid%20Architecture%0AAuthor%3A%20Xidong%20Wang%20and%20Dingjie%20Song%20and%20Shunian%20Chen%20and%20Chen%20Zhang%20and%20Benyou%20Wang%0AAbstract%3A%20%20%20Expanding%20the%20long-context%20capabilities%20of%20Multi-modal%20Large%20Language%0AModels~%28MLLMs%29%20is%20crucial%20for%20video%20understanding%2C%20high-resolution%20image%0Aunderstanding%2C%20and%20multi-modal%20agents.%20This%20involves%20a%20series%20of%20systematic%0Aoptimizations%2C%20including%20model%20architecture%2C%20data%20construction%20and%20training%0Astrategy%2C%20particularly%20addressing%20challenges%20such%20as%20%5Ctextit%7Bdegraded%0Aperformance%20with%20more%20images%7D%20and%20%5Ctextit%7Bhigh%20computational%20costs%7D.%20In%20this%0Apaper%2C%20we%20adapt%20the%20model%20architecture%20to%20a%20hybrid%20of%20Mamba%20and%20Transformer%0Ablocks%2C%20approach%20data%20construction%20with%20both%20temporal%20and%20spatial%20dependencies%0Aamong%20multiple%20images%20and%20employ%20a%20progressive%20training%20strategy.%20The%20released%0Amodel%20%5Ctextbf%7BLongLLaVA%7D~%28%5Ctextbf%7BLong%7D-Context%20%5Ctextbf%7BL%7Darge%0A%5Ctextbf%7BL%7Danguage%20%5Ctextbf%7Ba%7Dnd%20%5Ctextbf%7BV%7Dision%20%5Ctextbf%7BA%7Dssistant%29%20is%20the%20first%0Ahybrid%20MLLM%2C%20which%20achieved%20a%20better%20balance%20between%20efficiency%20and%0Aeffectiveness.%20LongLLaVA%20not%20only%20achieves%20competitive%20results%20across%20various%0Abenchmarks%2C%20but%20also%20maintains%20high%20throughput%20and%20low%20memory%20consumption.%0AEspecially%2C%20it%20could%20process%20nearly%20a%20thousand%20images%20on%20a%20single%20A100%2080GB%0AGPU%2C%20showing%20promising%20application%20prospects%20for%20a%20wide%20range%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongLLaVA%253A%2520Scaling%2520Multi-modal%2520LLMs%2520to%25201000%2520Images%2520Efficiently%2520via%250A%2520%2520Hybrid%2520Architecture%26entry.906535625%3DXidong%2520Wang%2520and%2520Dingjie%2520Song%2520and%2520Shunian%2520Chen%2520and%2520Chen%2520Zhang%2520and%2520Benyou%2520Wang%26entry.1292438233%3D%2520%2520Expanding%2520the%2520long-context%2520capabilities%2520of%2520Multi-modal%2520Large%2520Language%250AModels~%2528MLLMs%2529%2520is%2520crucial%2520for%2520video%2520understanding%252C%2520high-resolution%2520image%250Aunderstanding%252C%2520and%2520multi-modal%2520agents.%2520This%2520involves%2520a%2520series%2520of%2520systematic%250Aoptimizations%252C%2520including%2520model%2520architecture%252C%2520data%2520construction%2520and%2520training%250Astrategy%252C%2520particularly%2520addressing%2520challenges%2520such%2520as%2520%255Ctextit%257Bdegraded%250Aperformance%2520with%2520more%2520images%257D%2520and%2520%255Ctextit%257Bhigh%2520computational%2520costs%257D.%2520In%2520this%250Apaper%252C%2520we%2520adapt%2520the%2520model%2520architecture%2520to%2520a%2520hybrid%2520of%2520Mamba%2520and%2520Transformer%250Ablocks%252C%2520approach%2520data%2520construction%2520with%2520both%2520temporal%2520and%2520spatial%2520dependencies%250Aamong%2520multiple%2520images%2520and%2520employ%2520a%2520progressive%2520training%2520strategy.%2520The%2520released%250Amodel%2520%255Ctextbf%257BLongLLaVA%257D~%2528%255Ctextbf%257BLong%257D-Context%2520%255Ctextbf%257BL%257Darge%250A%255Ctextbf%257BL%257Danguage%2520%255Ctextbf%257Ba%257Dnd%2520%255Ctextbf%257BV%257Dision%2520%255Ctextbf%257BA%257Dssistant%2529%2520is%2520the%2520first%250Ahybrid%2520MLLM%252C%2520which%2520achieved%2520a%2520better%2520balance%2520between%2520efficiency%2520and%250Aeffectiveness.%2520LongLLaVA%2520not%2520only%2520achieves%2520competitive%2520results%2520across%2520various%250Abenchmarks%252C%2520but%2520also%2520maintains%2520high%2520throughput%2520and%2520low%2520memory%2520consumption.%250AEspecially%252C%2520it%2520could%2520process%2520nearly%2520a%2520thousand%2520images%2520on%2520a%2520single%2520A100%252080GB%250AGPU%252C%2520showing%2520promising%2520application%2520prospects%2520for%2520a%2520wide%2520range%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongLLaVA%3A%20Scaling%20Multi-modal%20LLMs%20to%201000%20Images%20Efficiently%20via%0A%20%20Hybrid%20Architecture&entry.906535625=Xidong%20Wang%20and%20Dingjie%20Song%20and%20Shunian%20Chen%20and%20Chen%20Zhang%20and%20Benyou%20Wang&entry.1292438233=%20%20Expanding%20the%20long-context%20capabilities%20of%20Multi-modal%20Large%20Language%0AModels~%28MLLMs%29%20is%20crucial%20for%20video%20understanding%2C%20high-resolution%20image%0Aunderstanding%2C%20and%20multi-modal%20agents.%20This%20involves%20a%20series%20of%20systematic%0Aoptimizations%2C%20including%20model%20architecture%2C%20data%20construction%20and%20training%0Astrategy%2C%20particularly%20addressing%20challenges%20such%20as%20%5Ctextit%7Bdegraded%0Aperformance%20with%20more%20images%7D%20and%20%5Ctextit%7Bhigh%20computational%20costs%7D.%20In%20this%0Apaper%2C%20we%20adapt%20the%20model%20architecture%20to%20a%20hybrid%20of%20Mamba%20and%20Transformer%0Ablocks%2C%20approach%20data%20construction%20with%20both%20temporal%20and%20spatial%20dependencies%0Aamong%20multiple%20images%20and%20employ%20a%20progressive%20training%20strategy.%20The%20released%0Amodel%20%5Ctextbf%7BLongLLaVA%7D~%28%5Ctextbf%7BLong%7D-Context%20%5Ctextbf%7BL%7Darge%0A%5Ctextbf%7BL%7Danguage%20%5Ctextbf%7Ba%7Dnd%20%5Ctextbf%7BV%7Dision%20%5Ctextbf%7BA%7Dssistant%29%20is%20the%20first%0Ahybrid%20MLLM%2C%20which%20achieved%20a%20better%20balance%20between%20efficiency%20and%0Aeffectiveness.%20LongLLaVA%20not%20only%20achieves%20competitive%20results%20across%20various%0Abenchmarks%2C%20but%20also%20maintains%20high%20throughput%20and%20low%20memory%20consumption.%0AEspecially%2C%20it%20could%20process%20nearly%20a%20thousand%20images%20on%20a%20single%20A100%2080GB%0AGPU%2C%20showing%20promising%20application%20prospects%20for%20a%20wide%20range%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02889v1&entry.124074799=Read"},
{"title": "Topological Methods in Machine Learning: A Tutorial for Practitioners", "author": "Baris Coskunuzer and C\u00fcneyt G\u00fcrcan Ak\u00e7ora", "abstract": "  Topological Machine Learning (TML) is an emerging field that leverages\ntechniques from algebraic topology to analyze complex data structures in ways\nthat traditional machine learning methods may not capture. This tutorial\nprovides a comprehensive introduction to two key TML techniques, persistent\nhomology and the Mapper algorithm, with an emphasis on practical applications.\nPersistent homology captures multi-scale topological features such as clusters,\nloops, and voids, while the Mapper algorithm creates an interpretable graph\nsummarizing high-dimensional data. To enhance accessibility, we adopt a\ndata-centric approach, enabling readers to gain hands-on experience applying\nthese techniques to relevant tasks. We provide step-by-step explanations,\nimplementations, hands-on examples, and case studies to demonstrate how these\ntools can be applied to real-world problems. The goal is to equip researchers\nand practitioners with the knowledge and resources to incorporate TML into\ntheir work, revealing insights often hidden from conventional machine learning\nmethods. The tutorial code is available at\nhttps://github.com/cakcora/TopologyForML\n", "link": "http://arxiv.org/abs/2409.02901v1", "date": "2024-09-04", "relevancy": 2.2876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4318}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Methods%20in%20Machine%20Learning%3A%20A%20Tutorial%20for%20Practitioners&body=Title%3A%20Topological%20Methods%20in%20Machine%20Learning%3A%20A%20Tutorial%20for%20Practitioners%0AAuthor%3A%20Baris%20Coskunuzer%20and%20C%C3%BCneyt%20G%C3%BCrcan%20Ak%C3%A7ora%0AAbstract%3A%20%20%20Topological%20Machine%20Learning%20%28TML%29%20is%20an%20emerging%20field%20that%20leverages%0Atechniques%20from%20algebraic%20topology%20to%20analyze%20complex%20data%20structures%20in%20ways%0Athat%20traditional%20machine%20learning%20methods%20may%20not%20capture.%20This%20tutorial%0Aprovides%20a%20comprehensive%20introduction%20to%20two%20key%20TML%20techniques%2C%20persistent%0Ahomology%20and%20the%20Mapper%20algorithm%2C%20with%20an%20emphasis%20on%20practical%20applications.%0APersistent%20homology%20captures%20multi-scale%20topological%20features%20such%20as%20clusters%2C%0Aloops%2C%20and%20voids%2C%20while%20the%20Mapper%20algorithm%20creates%20an%20interpretable%20graph%0Asummarizing%20high-dimensional%20data.%20To%20enhance%20accessibility%2C%20we%20adopt%20a%0Adata-centric%20approach%2C%20enabling%20readers%20to%20gain%20hands-on%20experience%20applying%0Athese%20techniques%20to%20relevant%20tasks.%20We%20provide%20step-by-step%20explanations%2C%0Aimplementations%2C%20hands-on%20examples%2C%20and%20case%20studies%20to%20demonstrate%20how%20these%0Atools%20can%20be%20applied%20to%20real-world%20problems.%20The%20goal%20is%20to%20equip%20researchers%0Aand%20practitioners%20with%20the%20knowledge%20and%20resources%20to%20incorporate%20TML%20into%0Atheir%20work%2C%20revealing%20insights%20often%20hidden%20from%20conventional%20machine%20learning%0Amethods.%20The%20tutorial%20code%20is%20available%20at%0Ahttps%3A//github.com/cakcora/TopologyForML%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Methods%2520in%2520Machine%2520Learning%253A%2520A%2520Tutorial%2520for%2520Practitioners%26entry.906535625%3DBaris%2520Coskunuzer%2520and%2520C%25C3%25BCneyt%2520G%25C3%25BCrcan%2520Ak%25C3%25A7ora%26entry.1292438233%3D%2520%2520Topological%2520Machine%2520Learning%2520%2528TML%2529%2520is%2520an%2520emerging%2520field%2520that%2520leverages%250Atechniques%2520from%2520algebraic%2520topology%2520to%2520analyze%2520complex%2520data%2520structures%2520in%2520ways%250Athat%2520traditional%2520machine%2520learning%2520methods%2520may%2520not%2520capture.%2520This%2520tutorial%250Aprovides%2520a%2520comprehensive%2520introduction%2520to%2520two%2520key%2520TML%2520techniques%252C%2520persistent%250Ahomology%2520and%2520the%2520Mapper%2520algorithm%252C%2520with%2520an%2520emphasis%2520on%2520practical%2520applications.%250APersistent%2520homology%2520captures%2520multi-scale%2520topological%2520features%2520such%2520as%2520clusters%252C%250Aloops%252C%2520and%2520voids%252C%2520while%2520the%2520Mapper%2520algorithm%2520creates%2520an%2520interpretable%2520graph%250Asummarizing%2520high-dimensional%2520data.%2520To%2520enhance%2520accessibility%252C%2520we%2520adopt%2520a%250Adata-centric%2520approach%252C%2520enabling%2520readers%2520to%2520gain%2520hands-on%2520experience%2520applying%250Athese%2520techniques%2520to%2520relevant%2520tasks.%2520We%2520provide%2520step-by-step%2520explanations%252C%250Aimplementations%252C%2520hands-on%2520examples%252C%2520and%2520case%2520studies%2520to%2520demonstrate%2520how%2520these%250Atools%2520can%2520be%2520applied%2520to%2520real-world%2520problems.%2520The%2520goal%2520is%2520to%2520equip%2520researchers%250Aand%2520practitioners%2520with%2520the%2520knowledge%2520and%2520resources%2520to%2520incorporate%2520TML%2520into%250Atheir%2520work%252C%2520revealing%2520insights%2520often%2520hidden%2520from%2520conventional%2520machine%2520learning%250Amethods.%2520The%2520tutorial%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/cakcora/TopologyForML%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Methods%20in%20Machine%20Learning%3A%20A%20Tutorial%20for%20Practitioners&entry.906535625=Baris%20Coskunuzer%20and%20C%C3%BCneyt%20G%C3%BCrcan%20Ak%C3%A7ora&entry.1292438233=%20%20Topological%20Machine%20Learning%20%28TML%29%20is%20an%20emerging%20field%20that%20leverages%0Atechniques%20from%20algebraic%20topology%20to%20analyze%20complex%20data%20structures%20in%20ways%0Athat%20traditional%20machine%20learning%20methods%20may%20not%20capture.%20This%20tutorial%0Aprovides%20a%20comprehensive%20introduction%20to%20two%20key%20TML%20techniques%2C%20persistent%0Ahomology%20and%20the%20Mapper%20algorithm%2C%20with%20an%20emphasis%20on%20practical%20applications.%0APersistent%20homology%20captures%20multi-scale%20topological%20features%20such%20as%20clusters%2C%0Aloops%2C%20and%20voids%2C%20while%20the%20Mapper%20algorithm%20creates%20an%20interpretable%20graph%0Asummarizing%20high-dimensional%20data.%20To%20enhance%20accessibility%2C%20we%20adopt%20a%0Adata-centric%20approach%2C%20enabling%20readers%20to%20gain%20hands-on%20experience%20applying%0Athese%20techniques%20to%20relevant%20tasks.%20We%20provide%20step-by-step%20explanations%2C%0Aimplementations%2C%20hands-on%20examples%2C%20and%20case%20studies%20to%20demonstrate%20how%20these%0Atools%20can%20be%20applied%20to%20real-world%20problems.%20The%20goal%20is%20to%20equip%20researchers%0Aand%20practitioners%20with%20the%20knowledge%20and%20resources%20to%20incorporate%20TML%20into%0Atheir%20work%2C%20revealing%20insights%20often%20hidden%20from%20conventional%20machine%20learning%0Amethods.%20The%20tutorial%20code%20is%20available%20at%0Ahttps%3A//github.com/cakcora/TopologyForML%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02901v1&entry.124074799=Read"},
{"title": "UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from\n  Endoscopic Sparse Views", "author": "Jiaxin Guo and Jiangliu Wang and Ruofeng Wei and Di Kang and Qi Dou and Yun-hui Liu", "abstract": "  Visualizing surgical scenes is crucial for revealing internal anatomical\nstructures during minimally invasive procedures. Novel View Synthesis is a\nvital technique that offers geometry and appearance reconstruction, enhancing\nunderstanding, planning, and decision-making in surgical scenes. Despite the\nimpressive achievements of Neural Radiance Field (NeRF), its direct application\nto surgical scenes produces unsatisfying results due to two challenges:\nendoscopic sparse views and significant photometric inconsistencies. In this\npaper, we propose uncertainty-aware conditional NeRF for novel view synthesis\nto tackle the severe shape-radiance ambiguity from sparse surgical views. The\ncore of UC-NeRF is to incorporate the multi-view uncertainty estimation to\ncondition the neural radiance field for modeling the severe photometric\ninconsistencies adaptively. Specifically, our UC-NeRF first builds a\nconsistency learner in the form of multi-view stereo network, to establish the\ngeometric correspondence from sparse views and generate uncertainty estimation\nand feature priors. In neural rendering, we design a base-adaptive NeRF network\nto exploit the uncertainty estimation for explicitly handling the photometric\ninconsistencies. Furthermore, an uncertainty-guided geometry distillation is\nemployed to enhance geometry learning. Experiments on the SCARED and Hamlyn\ndatasets demonstrate our superior performance in rendering appearance and\ngeometry, consistently outperforming the current state-of-the-art approaches.\nOur code will be released at \\url{https://github.com/wrld/UC-NeRF}.\n", "link": "http://arxiv.org/abs/2409.02917v1", "date": "2024-09-04", "relevancy": 2.2711, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5904}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5802}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UC-NeRF%3A%20Uncertainty-aware%20Conditional%20Neural%20Radiance%20Fields%20from%0A%20%20Endoscopic%20Sparse%20Views&body=Title%3A%20UC-NeRF%3A%20Uncertainty-aware%20Conditional%20Neural%20Radiance%20Fields%20from%0A%20%20Endoscopic%20Sparse%20Views%0AAuthor%3A%20Jiaxin%20Guo%20and%20Jiangliu%20Wang%20and%20Ruofeng%20Wei%20and%20Di%20Kang%20and%20Qi%20Dou%20and%20Yun-hui%20Liu%0AAbstract%3A%20%20%20Visualizing%20surgical%20scenes%20is%20crucial%20for%20revealing%20internal%20anatomical%0Astructures%20during%20minimally%20invasive%20procedures.%20Novel%20View%20Synthesis%20is%20a%0Avital%20technique%20that%20offers%20geometry%20and%20appearance%20reconstruction%2C%20enhancing%0Aunderstanding%2C%20planning%2C%20and%20decision-making%20in%20surgical%20scenes.%20Despite%20the%0Aimpressive%20achievements%20of%20Neural%20Radiance%20Field%20%28NeRF%29%2C%20its%20direct%20application%0Ato%20surgical%20scenes%20produces%20unsatisfying%20results%20due%20to%20two%20challenges%3A%0Aendoscopic%20sparse%20views%20and%20significant%20photometric%20inconsistencies.%20In%20this%0Apaper%2C%20we%20propose%20uncertainty-aware%20conditional%20NeRF%20for%20novel%20view%20synthesis%0Ato%20tackle%20the%20severe%20shape-radiance%20ambiguity%20from%20sparse%20surgical%20views.%20The%0Acore%20of%20UC-NeRF%20is%20to%20incorporate%20the%20multi-view%20uncertainty%20estimation%20to%0Acondition%20the%20neural%20radiance%20field%20for%20modeling%20the%20severe%20photometric%0Ainconsistencies%20adaptively.%20Specifically%2C%20our%20UC-NeRF%20first%20builds%20a%0Aconsistency%20learner%20in%20the%20form%20of%20multi-view%20stereo%20network%2C%20to%20establish%20the%0Ageometric%20correspondence%20from%20sparse%20views%20and%20generate%20uncertainty%20estimation%0Aand%20feature%20priors.%20In%20neural%20rendering%2C%20we%20design%20a%20base-adaptive%20NeRF%20network%0Ato%20exploit%20the%20uncertainty%20estimation%20for%20explicitly%20handling%20the%20photometric%0Ainconsistencies.%20Furthermore%2C%20an%20uncertainty-guided%20geometry%20distillation%20is%0Aemployed%20to%20enhance%20geometry%20learning.%20Experiments%20on%20the%20SCARED%20and%20Hamlyn%0Adatasets%20demonstrate%20our%20superior%20performance%20in%20rendering%20appearance%20and%0Ageometry%2C%20consistently%20outperforming%20the%20current%20state-of-the-art%20approaches.%0AOur%20code%20will%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/wrld/UC-NeRF%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUC-NeRF%253A%2520Uncertainty-aware%2520Conditional%2520Neural%2520Radiance%2520Fields%2520from%250A%2520%2520Endoscopic%2520Sparse%2520Views%26entry.906535625%3DJiaxin%2520Guo%2520and%2520Jiangliu%2520Wang%2520and%2520Ruofeng%2520Wei%2520and%2520Di%2520Kang%2520and%2520Qi%2520Dou%2520and%2520Yun-hui%2520Liu%26entry.1292438233%3D%2520%2520Visualizing%2520surgical%2520scenes%2520is%2520crucial%2520for%2520revealing%2520internal%2520anatomical%250Astructures%2520during%2520minimally%2520invasive%2520procedures.%2520Novel%2520View%2520Synthesis%2520is%2520a%250Avital%2520technique%2520that%2520offers%2520geometry%2520and%2520appearance%2520reconstruction%252C%2520enhancing%250Aunderstanding%252C%2520planning%252C%2520and%2520decision-making%2520in%2520surgical%2520scenes.%2520Despite%2520the%250Aimpressive%2520achievements%2520of%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%252C%2520its%2520direct%2520application%250Ato%2520surgical%2520scenes%2520produces%2520unsatisfying%2520results%2520due%2520to%2520two%2520challenges%253A%250Aendoscopic%2520sparse%2520views%2520and%2520significant%2520photometric%2520inconsistencies.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520uncertainty-aware%2520conditional%2520NeRF%2520for%2520novel%2520view%2520synthesis%250Ato%2520tackle%2520the%2520severe%2520shape-radiance%2520ambiguity%2520from%2520sparse%2520surgical%2520views.%2520The%250Acore%2520of%2520UC-NeRF%2520is%2520to%2520incorporate%2520the%2520multi-view%2520uncertainty%2520estimation%2520to%250Acondition%2520the%2520neural%2520radiance%2520field%2520for%2520modeling%2520the%2520severe%2520photometric%250Ainconsistencies%2520adaptively.%2520Specifically%252C%2520our%2520UC-NeRF%2520first%2520builds%2520a%250Aconsistency%2520learner%2520in%2520the%2520form%2520of%2520multi-view%2520stereo%2520network%252C%2520to%2520establish%2520the%250Ageometric%2520correspondence%2520from%2520sparse%2520views%2520and%2520generate%2520uncertainty%2520estimation%250Aand%2520feature%2520priors.%2520In%2520neural%2520rendering%252C%2520we%2520design%2520a%2520base-adaptive%2520NeRF%2520network%250Ato%2520exploit%2520the%2520uncertainty%2520estimation%2520for%2520explicitly%2520handling%2520the%2520photometric%250Ainconsistencies.%2520Furthermore%252C%2520an%2520uncertainty-guided%2520geometry%2520distillation%2520is%250Aemployed%2520to%2520enhance%2520geometry%2520learning.%2520Experiments%2520on%2520the%2520SCARED%2520and%2520Hamlyn%250Adatasets%2520demonstrate%2520our%2520superior%2520performance%2520in%2520rendering%2520appearance%2520and%250Ageometry%252C%2520consistently%2520outperforming%2520the%2520current%2520state-of-the-art%2520approaches.%250AOur%2520code%2520will%2520be%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/wrld/UC-NeRF%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UC-NeRF%3A%20Uncertainty-aware%20Conditional%20Neural%20Radiance%20Fields%20from%0A%20%20Endoscopic%20Sparse%20Views&entry.906535625=Jiaxin%20Guo%20and%20Jiangliu%20Wang%20and%20Ruofeng%20Wei%20and%20Di%20Kang%20and%20Qi%20Dou%20and%20Yun-hui%20Liu&entry.1292438233=%20%20Visualizing%20surgical%20scenes%20is%20crucial%20for%20revealing%20internal%20anatomical%0Astructures%20during%20minimally%20invasive%20procedures.%20Novel%20View%20Synthesis%20is%20a%0Avital%20technique%20that%20offers%20geometry%20and%20appearance%20reconstruction%2C%20enhancing%0Aunderstanding%2C%20planning%2C%20and%20decision-making%20in%20surgical%20scenes.%20Despite%20the%0Aimpressive%20achievements%20of%20Neural%20Radiance%20Field%20%28NeRF%29%2C%20its%20direct%20application%0Ato%20surgical%20scenes%20produces%20unsatisfying%20results%20due%20to%20two%20challenges%3A%0Aendoscopic%20sparse%20views%20and%20significant%20photometric%20inconsistencies.%20In%20this%0Apaper%2C%20we%20propose%20uncertainty-aware%20conditional%20NeRF%20for%20novel%20view%20synthesis%0Ato%20tackle%20the%20severe%20shape-radiance%20ambiguity%20from%20sparse%20surgical%20views.%20The%0Acore%20of%20UC-NeRF%20is%20to%20incorporate%20the%20multi-view%20uncertainty%20estimation%20to%0Acondition%20the%20neural%20radiance%20field%20for%20modeling%20the%20severe%20photometric%0Ainconsistencies%20adaptively.%20Specifically%2C%20our%20UC-NeRF%20first%20builds%20a%0Aconsistency%20learner%20in%20the%20form%20of%20multi-view%20stereo%20network%2C%20to%20establish%20the%0Ageometric%20correspondence%20from%20sparse%20views%20and%20generate%20uncertainty%20estimation%0Aand%20feature%20priors.%20In%20neural%20rendering%2C%20we%20design%20a%20base-adaptive%20NeRF%20network%0Ato%20exploit%20the%20uncertainty%20estimation%20for%20explicitly%20handling%20the%20photometric%0Ainconsistencies.%20Furthermore%2C%20an%20uncertainty-guided%20geometry%20distillation%20is%0Aemployed%20to%20enhance%20geometry%20learning.%20Experiments%20on%20the%20SCARED%20and%20Hamlyn%0Adatasets%20demonstrate%20our%20superior%20performance%20in%20rendering%20appearance%20and%0Ageometry%2C%20consistently%20outperforming%20the%20current%20state-of-the-art%20approaches.%0AOur%20code%20will%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/wrld/UC-NeRF%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02917v1&entry.124074799=Read"},
{"title": "MaDis-Stereo: Enhanced Stereo Matching via Distilled Masked Image\n  Modeling", "author": "Jihye Ahn and Hyesong Choi and Soomin Kim and Dongbo Min", "abstract": "  In stereo matching, CNNs have traditionally served as the predominant\narchitectures. Although Transformer-based stereo models have been studied\nrecently, their performance still lags behind CNN-based stereo models due to\nthe inherent data scarcity issue in the stereo matching task. In this paper, we\npropose Masked Image Modeling Distilled Stereo matching model, termed\nMaDis-Stereo, that enhances locality inductive bias by leveraging Masked Image\nModeling (MIM) in training Transformer-based stereo model. Given randomly\nmasked stereo images as inputs, our method attempts to conduct both image\nreconstruction and depth prediction tasks. While this strategy is beneficial to\nresolving the data scarcity issue, the dual challenge of reconstructing masked\ntokens and subsequently performing stereo matching poses significant\nchallenges, particularly in terms of training stability. To address this, we\npropose to use an auxiliary network (teacher), updated via Exponential Moving\nAverage (EMA), along with the original stereo model (student), where teacher\npredictions serve as pseudo supervisory signals to effectively distill\nknowledge into the student model. State-of-the-arts performance is achieved\nwith the proposed method on several stereo matching such as ETH3D and KITTI\n2015. Additionally, to demonstrate that our model effectively leverages\nlocality inductive bias, we provide the attention distance measurement.\n", "link": "http://arxiv.org/abs/2409.02846v1", "date": "2024-09-04", "relevancy": 2.2693, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5691}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaDis-Stereo%3A%20Enhanced%20Stereo%20Matching%20via%20Distilled%20Masked%20Image%0A%20%20Modeling&body=Title%3A%20MaDis-Stereo%3A%20Enhanced%20Stereo%20Matching%20via%20Distilled%20Masked%20Image%0A%20%20Modeling%0AAuthor%3A%20Jihye%20Ahn%20and%20Hyesong%20Choi%20and%20Soomin%20Kim%20and%20Dongbo%20Min%0AAbstract%3A%20%20%20In%20stereo%20matching%2C%20CNNs%20have%20traditionally%20served%20as%20the%20predominant%0Aarchitectures.%20Although%20Transformer-based%20stereo%20models%20have%20been%20studied%0Arecently%2C%20their%20performance%20still%20lags%20behind%20CNN-based%20stereo%20models%20due%20to%0Athe%20inherent%20data%20scarcity%20issue%20in%20the%20stereo%20matching%20task.%20In%20this%20paper%2C%20we%0Apropose%20Masked%20Image%20Modeling%20Distilled%20Stereo%20matching%20model%2C%20termed%0AMaDis-Stereo%2C%20that%20enhances%20locality%20inductive%20bias%20by%20leveraging%20Masked%20Image%0AModeling%20%28MIM%29%20in%20training%20Transformer-based%20stereo%20model.%20Given%20randomly%0Amasked%20stereo%20images%20as%20inputs%2C%20our%20method%20attempts%20to%20conduct%20both%20image%0Areconstruction%20and%20depth%20prediction%20tasks.%20While%20this%20strategy%20is%20beneficial%20to%0Aresolving%20the%20data%20scarcity%20issue%2C%20the%20dual%20challenge%20of%20reconstructing%20masked%0Atokens%20and%20subsequently%20performing%20stereo%20matching%20poses%20significant%0Achallenges%2C%20particularly%20in%20terms%20of%20training%20stability.%20To%20address%20this%2C%20we%0Apropose%20to%20use%20an%20auxiliary%20network%20%28teacher%29%2C%20updated%20via%20Exponential%20Moving%0AAverage%20%28EMA%29%2C%20along%20with%20the%20original%20stereo%20model%20%28student%29%2C%20where%20teacher%0Apredictions%20serve%20as%20pseudo%20supervisory%20signals%20to%20effectively%20distill%0Aknowledge%20into%20the%20student%20model.%20State-of-the-arts%20performance%20is%20achieved%0Awith%20the%20proposed%20method%20on%20several%20stereo%20matching%20such%20as%20ETH3D%20and%20KITTI%0A2015.%20Additionally%2C%20to%20demonstrate%20that%20our%20model%20effectively%20leverages%0Alocality%20inductive%20bias%2C%20we%20provide%20the%20attention%20distance%20measurement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaDis-Stereo%253A%2520Enhanced%2520Stereo%2520Matching%2520via%2520Distilled%2520Masked%2520Image%250A%2520%2520Modeling%26entry.906535625%3DJihye%2520Ahn%2520and%2520Hyesong%2520Choi%2520and%2520Soomin%2520Kim%2520and%2520Dongbo%2520Min%26entry.1292438233%3D%2520%2520In%2520stereo%2520matching%252C%2520CNNs%2520have%2520traditionally%2520served%2520as%2520the%2520predominant%250Aarchitectures.%2520Although%2520Transformer-based%2520stereo%2520models%2520have%2520been%2520studied%250Arecently%252C%2520their%2520performance%2520still%2520lags%2520behind%2520CNN-based%2520stereo%2520models%2520due%2520to%250Athe%2520inherent%2520data%2520scarcity%2520issue%2520in%2520the%2520stereo%2520matching%2520task.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Masked%2520Image%2520Modeling%2520Distilled%2520Stereo%2520matching%2520model%252C%2520termed%250AMaDis-Stereo%252C%2520that%2520enhances%2520locality%2520inductive%2520bias%2520by%2520leveraging%2520Masked%2520Image%250AModeling%2520%2528MIM%2529%2520in%2520training%2520Transformer-based%2520stereo%2520model.%2520Given%2520randomly%250Amasked%2520stereo%2520images%2520as%2520inputs%252C%2520our%2520method%2520attempts%2520to%2520conduct%2520both%2520image%250Areconstruction%2520and%2520depth%2520prediction%2520tasks.%2520While%2520this%2520strategy%2520is%2520beneficial%2520to%250Aresolving%2520the%2520data%2520scarcity%2520issue%252C%2520the%2520dual%2520challenge%2520of%2520reconstructing%2520masked%250Atokens%2520and%2520subsequently%2520performing%2520stereo%2520matching%2520poses%2520significant%250Achallenges%252C%2520particularly%2520in%2520terms%2520of%2520training%2520stability.%2520To%2520address%2520this%252C%2520we%250Apropose%2520to%2520use%2520an%2520auxiliary%2520network%2520%2528teacher%2529%252C%2520updated%2520via%2520Exponential%2520Moving%250AAverage%2520%2528EMA%2529%252C%2520along%2520with%2520the%2520original%2520stereo%2520model%2520%2528student%2529%252C%2520where%2520teacher%250Apredictions%2520serve%2520as%2520pseudo%2520supervisory%2520signals%2520to%2520effectively%2520distill%250Aknowledge%2520into%2520the%2520student%2520model.%2520State-of-the-arts%2520performance%2520is%2520achieved%250Awith%2520the%2520proposed%2520method%2520on%2520several%2520stereo%2520matching%2520such%2520as%2520ETH3D%2520and%2520KITTI%250A2015.%2520Additionally%252C%2520to%2520demonstrate%2520that%2520our%2520model%2520effectively%2520leverages%250Alocality%2520inductive%2520bias%252C%2520we%2520provide%2520the%2520attention%2520distance%2520measurement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaDis-Stereo%3A%20Enhanced%20Stereo%20Matching%20via%20Distilled%20Masked%20Image%0A%20%20Modeling&entry.906535625=Jihye%20Ahn%20and%20Hyesong%20Choi%20and%20Soomin%20Kim%20and%20Dongbo%20Min&entry.1292438233=%20%20In%20stereo%20matching%2C%20CNNs%20have%20traditionally%20served%20as%20the%20predominant%0Aarchitectures.%20Although%20Transformer-based%20stereo%20models%20have%20been%20studied%0Arecently%2C%20their%20performance%20still%20lags%20behind%20CNN-based%20stereo%20models%20due%20to%0Athe%20inherent%20data%20scarcity%20issue%20in%20the%20stereo%20matching%20task.%20In%20this%20paper%2C%20we%0Apropose%20Masked%20Image%20Modeling%20Distilled%20Stereo%20matching%20model%2C%20termed%0AMaDis-Stereo%2C%20that%20enhances%20locality%20inductive%20bias%20by%20leveraging%20Masked%20Image%0AModeling%20%28MIM%29%20in%20training%20Transformer-based%20stereo%20model.%20Given%20randomly%0Amasked%20stereo%20images%20as%20inputs%2C%20our%20method%20attempts%20to%20conduct%20both%20image%0Areconstruction%20and%20depth%20prediction%20tasks.%20While%20this%20strategy%20is%20beneficial%20to%0Aresolving%20the%20data%20scarcity%20issue%2C%20the%20dual%20challenge%20of%20reconstructing%20masked%0Atokens%20and%20subsequently%20performing%20stereo%20matching%20poses%20significant%0Achallenges%2C%20particularly%20in%20terms%20of%20training%20stability.%20To%20address%20this%2C%20we%0Apropose%20to%20use%20an%20auxiliary%20network%20%28teacher%29%2C%20updated%20via%20Exponential%20Moving%0AAverage%20%28EMA%29%2C%20along%20with%20the%20original%20stereo%20model%20%28student%29%2C%20where%20teacher%0Apredictions%20serve%20as%20pseudo%20supervisory%20signals%20to%20effectively%20distill%0Aknowledge%20into%20the%20student%20model.%20State-of-the-arts%20performance%20is%20achieved%0Awith%20the%20proposed%20method%20on%20several%20stereo%20matching%20such%20as%20ETH3D%20and%20KITTI%0A2015.%20Additionally%2C%20to%20demonstrate%20that%20our%20model%20effectively%20leverages%0Alocality%20inductive%20bias%2C%20we%20provide%20the%20attention%20distance%20measurement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02846v1&entry.124074799=Read"},
{"title": "Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in\n  Session-Based Social Recommendations", "author": "Chunyan An and Yunhan Li and Qiang Yang and Winston K. G. Seah and Zhixu Li and Conghao Yanga", "abstract": "  Session-based Social Recommendation (SSR) leverages social relationships\nwithin online networks to enhance the performance of Session-based\nRecommendation (SR). However, existing SSR algorithms often encounter the\nchallenge of ``friend data sparsity''. Moreover, significant discrepancies can\nexist between the purchase preferences of social network friends and those of\nthe target user, reducing the influence of friends relative to the target\nuser's own preferences. To address these challenges, this paper introduces the\nconcept of ``Like-minded Peers'' (LMP), representing users whose preferences\nalign with the target user's current session based on their historical\nsessions. This is the first work, to our knowledge, that uses LMP to enhance\nthe modeling of social influence in SSR. This approach not only alleviates the\nproblem of friend data sparsity but also effectively incorporates users with\nsimilar preferences to the target user. We propose a novel model named\nTransformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec),\nwhich includes the TEGAA module and the GAT-based social aggregation module.\nThe TEGAA module captures and merges both long-term and short-term interests\nfor target users and LMP users. Concurrently, the GAT-based social aggregation\nmodule is designed to aggregate the target users' dynamic interests and social\ninfluence in a weighted manner. Extensive experiments on four real-world\ndatasets demonstrate the efficacy and superiority of our proposed model and\nablation studies are done to illustrate the contributions of each component in\nTEGAARec.\n", "link": "http://arxiv.org/abs/2409.02702v1", "date": "2024-09-04", "relevancy": 2.2684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Like-Minded%20Peers%20to%20Overcome%20Friend%20Data%20Sparsity%20in%0A%20%20Session-Based%20Social%20Recommendations&body=Title%3A%20Incorporating%20Like-Minded%20Peers%20to%20Overcome%20Friend%20Data%20Sparsity%20in%0A%20%20Session-Based%20Social%20Recommendations%0AAuthor%3A%20Chunyan%20An%20and%20Yunhan%20Li%20and%20Qiang%20Yang%20and%20Winston%20K.%20G.%20Seah%20and%20Zhixu%20Li%20and%20Conghao%20Yanga%0AAbstract%3A%20%20%20Session-based%20Social%20Recommendation%20%28SSR%29%20leverages%20social%20relationships%0Awithin%20online%20networks%20to%20enhance%20the%20performance%20of%20Session-based%0ARecommendation%20%28SR%29.%20However%2C%20existing%20SSR%20algorithms%20often%20encounter%20the%0Achallenge%20of%20%60%60friend%20data%20sparsity%27%27.%20Moreover%2C%20significant%20discrepancies%20can%0Aexist%20between%20the%20purchase%20preferences%20of%20social%20network%20friends%20and%20those%20of%0Athe%20target%20user%2C%20reducing%20the%20influence%20of%20friends%20relative%20to%20the%20target%0Auser%27s%20own%20preferences.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%20the%0Aconcept%20of%20%60%60Like-minded%20Peers%27%27%20%28LMP%29%2C%20representing%20users%20whose%20preferences%0Aalign%20with%20the%20target%20user%27s%20current%20session%20based%20on%20their%20historical%0Asessions.%20This%20is%20the%20first%20work%2C%20to%20our%20knowledge%2C%20that%20uses%20LMP%20to%20enhance%0Athe%20modeling%20of%20social%20influence%20in%20SSR.%20This%20approach%20not%20only%20alleviates%20the%0Aproblem%20of%20friend%20data%20sparsity%20but%20also%20effectively%20incorporates%20users%20with%0Asimilar%20preferences%20to%20the%20target%20user.%20We%20propose%20a%20novel%20model%20named%0ATransformer%20Encoder%20with%20Graph%20Attention%20Aggregator%20Recommendation%20%28TEGAARec%29%2C%0Awhich%20includes%20the%20TEGAA%20module%20and%20the%20GAT-based%20social%20aggregation%20module.%0AThe%20TEGAA%20module%20captures%20and%20merges%20both%20long-term%20and%20short-term%20interests%0Afor%20target%20users%20and%20LMP%20users.%20Concurrently%2C%20the%20GAT-based%20social%20aggregation%0Amodule%20is%20designed%20to%20aggregate%20the%20target%20users%27%20dynamic%20interests%20and%20social%0Ainfluence%20in%20a%20weighted%20manner.%20Extensive%20experiments%20on%20four%20real-world%0Adatasets%20demonstrate%20the%20efficacy%20and%20superiority%20of%20our%20proposed%20model%20and%0Aablation%20studies%20are%20done%20to%20illustrate%20the%20contributions%20of%20each%20component%20in%0ATEGAARec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Like-Minded%2520Peers%2520to%2520Overcome%2520Friend%2520Data%2520Sparsity%2520in%250A%2520%2520Session-Based%2520Social%2520Recommendations%26entry.906535625%3DChunyan%2520An%2520and%2520Yunhan%2520Li%2520and%2520Qiang%2520Yang%2520and%2520Winston%2520K.%2520G.%2520Seah%2520and%2520Zhixu%2520Li%2520and%2520Conghao%2520Yanga%26entry.1292438233%3D%2520%2520Session-based%2520Social%2520Recommendation%2520%2528SSR%2529%2520leverages%2520social%2520relationships%250Awithin%2520online%2520networks%2520to%2520enhance%2520the%2520performance%2520of%2520Session-based%250ARecommendation%2520%2528SR%2529.%2520However%252C%2520existing%2520SSR%2520algorithms%2520often%2520encounter%2520the%250Achallenge%2520of%2520%2560%2560friend%2520data%2520sparsity%2527%2527.%2520Moreover%252C%2520significant%2520discrepancies%2520can%250Aexist%2520between%2520the%2520purchase%2520preferences%2520of%2520social%2520network%2520friends%2520and%2520those%2520of%250Athe%2520target%2520user%252C%2520reducing%2520the%2520influence%2520of%2520friends%2520relative%2520to%2520the%2520target%250Auser%2527s%2520own%2520preferences.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520introduces%2520the%250Aconcept%2520of%2520%2560%2560Like-minded%2520Peers%2527%2527%2520%2528LMP%2529%252C%2520representing%2520users%2520whose%2520preferences%250Aalign%2520with%2520the%2520target%2520user%2527s%2520current%2520session%2520based%2520on%2520their%2520historical%250Asessions.%2520This%2520is%2520the%2520first%2520work%252C%2520to%2520our%2520knowledge%252C%2520that%2520uses%2520LMP%2520to%2520enhance%250Athe%2520modeling%2520of%2520social%2520influence%2520in%2520SSR.%2520This%2520approach%2520not%2520only%2520alleviates%2520the%250Aproblem%2520of%2520friend%2520data%2520sparsity%2520but%2520also%2520effectively%2520incorporates%2520users%2520with%250Asimilar%2520preferences%2520to%2520the%2520target%2520user.%2520We%2520propose%2520a%2520novel%2520model%2520named%250ATransformer%2520Encoder%2520with%2520Graph%2520Attention%2520Aggregator%2520Recommendation%2520%2528TEGAARec%2529%252C%250Awhich%2520includes%2520the%2520TEGAA%2520module%2520and%2520the%2520GAT-based%2520social%2520aggregation%2520module.%250AThe%2520TEGAA%2520module%2520captures%2520and%2520merges%2520both%2520long-term%2520and%2520short-term%2520interests%250Afor%2520target%2520users%2520and%2520LMP%2520users.%2520Concurrently%252C%2520the%2520GAT-based%2520social%2520aggregation%250Amodule%2520is%2520designed%2520to%2520aggregate%2520the%2520target%2520users%2527%2520dynamic%2520interests%2520and%2520social%250Ainfluence%2520in%2520a%2520weighted%2520manner.%2520Extensive%2520experiments%2520on%2520four%2520real-world%250Adatasets%2520demonstrate%2520the%2520efficacy%2520and%2520superiority%2520of%2520our%2520proposed%2520model%2520and%250Aablation%2520studies%2520are%2520done%2520to%2520illustrate%2520the%2520contributions%2520of%2520each%2520component%2520in%250ATEGAARec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Like-Minded%20Peers%20to%20Overcome%20Friend%20Data%20Sparsity%20in%0A%20%20Session-Based%20Social%20Recommendations&entry.906535625=Chunyan%20An%20and%20Yunhan%20Li%20and%20Qiang%20Yang%20and%20Winston%20K.%20G.%20Seah%20and%20Zhixu%20Li%20and%20Conghao%20Yanga&entry.1292438233=%20%20Session-based%20Social%20Recommendation%20%28SSR%29%20leverages%20social%20relationships%0Awithin%20online%20networks%20to%20enhance%20the%20performance%20of%20Session-based%0ARecommendation%20%28SR%29.%20However%2C%20existing%20SSR%20algorithms%20often%20encounter%20the%0Achallenge%20of%20%60%60friend%20data%20sparsity%27%27.%20Moreover%2C%20significant%20discrepancies%20can%0Aexist%20between%20the%20purchase%20preferences%20of%20social%20network%20friends%20and%20those%20of%0Athe%20target%20user%2C%20reducing%20the%20influence%20of%20friends%20relative%20to%20the%20target%0Auser%27s%20own%20preferences.%20To%20address%20these%20challenges%2C%20this%20paper%20introduces%20the%0Aconcept%20of%20%60%60Like-minded%20Peers%27%27%20%28LMP%29%2C%20representing%20users%20whose%20preferences%0Aalign%20with%20the%20target%20user%27s%20current%20session%20based%20on%20their%20historical%0Asessions.%20This%20is%20the%20first%20work%2C%20to%20our%20knowledge%2C%20that%20uses%20LMP%20to%20enhance%0Athe%20modeling%20of%20social%20influence%20in%20SSR.%20This%20approach%20not%20only%20alleviates%20the%0Aproblem%20of%20friend%20data%20sparsity%20but%20also%20effectively%20incorporates%20users%20with%0Asimilar%20preferences%20to%20the%20target%20user.%20We%20propose%20a%20novel%20model%20named%0ATransformer%20Encoder%20with%20Graph%20Attention%20Aggregator%20Recommendation%20%28TEGAARec%29%2C%0Awhich%20includes%20the%20TEGAA%20module%20and%20the%20GAT-based%20social%20aggregation%20module.%0AThe%20TEGAA%20module%20captures%20and%20merges%20both%20long-term%20and%20short-term%20interests%0Afor%20target%20users%20and%20LMP%20users.%20Concurrently%2C%20the%20GAT-based%20social%20aggregation%0Amodule%20is%20designed%20to%20aggregate%20the%20target%20users%27%20dynamic%20interests%20and%20social%0Ainfluence%20in%20a%20weighted%20manner.%20Extensive%20experiments%20on%20four%20real-world%0Adatasets%20demonstrate%20the%20efficacy%20and%20superiority%20of%20our%20proposed%20model%20and%0Aablation%20studies%20are%20done%20to%20illustrate%20the%20contributions%20of%20each%20component%20in%0ATEGAARec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02702v1&entry.124074799=Read"},
{"title": "Hybrid Imitation-Learning Motion Planner for Urban Driving", "author": "Cristian Gariboldi and Matteo Corno and Beng Jin", "abstract": "  With the release of open source datasets such as nuPlan and Argoverse, the\nresearch around learning-based planners has spread a lot in the last years.\nExisting systems have shown excellent capabilities in imitating the human\ndriver behaviour, but they struggle to guarantee safe closed-loop driving.\nConversely, optimization-based planners offer greater security in short-term\nplanning scenarios. To confront this challenge, in this paper we propose a\nnovel hybrid motion planner that integrates both learning-based and\noptimization-based techniques. Initially, a multilayer perceptron (MLP)\ngenerates a human-like trajectory, which is then refined by an\noptimization-based component. This component not only minimizes tracking errors\nbut also computes a trajectory that is both kinematically feasible and\ncollision-free with obstacles and road boundaries. Our model effectively\nbalances safety and human-likeness, mitigating the trade-off inherent in these\nobjectives. We validate our approach through simulation experiments and further\ndemonstrate its efficacy by deploying it in real-world self-driving vehicles.\n", "link": "http://arxiv.org/abs/2409.02871v1", "date": "2024-09-04", "relevancy": 2.2531, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5639}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Imitation-Learning%20Motion%20Planner%20for%20Urban%20Driving&body=Title%3A%20Hybrid%20Imitation-Learning%20Motion%20Planner%20for%20Urban%20Driving%0AAuthor%3A%20Cristian%20Gariboldi%20and%20Matteo%20Corno%20and%20Beng%20Jin%0AAbstract%3A%20%20%20With%20the%20release%20of%20open%20source%20datasets%20such%20as%20nuPlan%20and%20Argoverse%2C%20the%0Aresearch%20around%20learning-based%20planners%20has%20spread%20a%20lot%20in%20the%20last%20years.%0AExisting%20systems%20have%20shown%20excellent%20capabilities%20in%20imitating%20the%20human%0Adriver%20behaviour%2C%20but%20they%20struggle%20to%20guarantee%20safe%20closed-loop%20driving.%0AConversely%2C%20optimization-based%20planners%20offer%20greater%20security%20in%20short-term%0Aplanning%20scenarios.%20To%20confront%20this%20challenge%2C%20in%20this%20paper%20we%20propose%20a%0Anovel%20hybrid%20motion%20planner%20that%20integrates%20both%20learning-based%20and%0Aoptimization-based%20techniques.%20Initially%2C%20a%20multilayer%20perceptron%20%28MLP%29%0Agenerates%20a%20human-like%20trajectory%2C%20which%20is%20then%20refined%20by%20an%0Aoptimization-based%20component.%20This%20component%20not%20only%20minimizes%20tracking%20errors%0Abut%20also%20computes%20a%20trajectory%20that%20is%20both%20kinematically%20feasible%20and%0Acollision-free%20with%20obstacles%20and%20road%20boundaries.%20Our%20model%20effectively%0Abalances%20safety%20and%20human-likeness%2C%20mitigating%20the%20trade-off%20inherent%20in%20these%0Aobjectives.%20We%20validate%20our%20approach%20through%20simulation%20experiments%20and%20further%0Ademonstrate%20its%20efficacy%20by%20deploying%20it%20in%20real-world%20self-driving%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Imitation-Learning%2520Motion%2520Planner%2520for%2520Urban%2520Driving%26entry.906535625%3DCristian%2520Gariboldi%2520and%2520Matteo%2520Corno%2520and%2520Beng%2520Jin%26entry.1292438233%3D%2520%2520With%2520the%2520release%2520of%2520open%2520source%2520datasets%2520such%2520as%2520nuPlan%2520and%2520Argoverse%252C%2520the%250Aresearch%2520around%2520learning-based%2520planners%2520has%2520spread%2520a%2520lot%2520in%2520the%2520last%2520years.%250AExisting%2520systems%2520have%2520shown%2520excellent%2520capabilities%2520in%2520imitating%2520the%2520human%250Adriver%2520behaviour%252C%2520but%2520they%2520struggle%2520to%2520guarantee%2520safe%2520closed-loop%2520driving.%250AConversely%252C%2520optimization-based%2520planners%2520offer%2520greater%2520security%2520in%2520short-term%250Aplanning%2520scenarios.%2520To%2520confront%2520this%2520challenge%252C%2520in%2520this%2520paper%2520we%2520propose%2520a%250Anovel%2520hybrid%2520motion%2520planner%2520that%2520integrates%2520both%2520learning-based%2520and%250Aoptimization-based%2520techniques.%2520Initially%252C%2520a%2520multilayer%2520perceptron%2520%2528MLP%2529%250Agenerates%2520a%2520human-like%2520trajectory%252C%2520which%2520is%2520then%2520refined%2520by%2520an%250Aoptimization-based%2520component.%2520This%2520component%2520not%2520only%2520minimizes%2520tracking%2520errors%250Abut%2520also%2520computes%2520a%2520trajectory%2520that%2520is%2520both%2520kinematically%2520feasible%2520and%250Acollision-free%2520with%2520obstacles%2520and%2520road%2520boundaries.%2520Our%2520model%2520effectively%250Abalances%2520safety%2520and%2520human-likeness%252C%2520mitigating%2520the%2520trade-off%2520inherent%2520in%2520these%250Aobjectives.%2520We%2520validate%2520our%2520approach%2520through%2520simulation%2520experiments%2520and%2520further%250Ademonstrate%2520its%2520efficacy%2520by%2520deploying%2520it%2520in%2520real-world%2520self-driving%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Imitation-Learning%20Motion%20Planner%20for%20Urban%20Driving&entry.906535625=Cristian%20Gariboldi%20and%20Matteo%20Corno%20and%20Beng%20Jin&entry.1292438233=%20%20With%20the%20release%20of%20open%20source%20datasets%20such%20as%20nuPlan%20and%20Argoverse%2C%20the%0Aresearch%20around%20learning-based%20planners%20has%20spread%20a%20lot%20in%20the%20last%20years.%0AExisting%20systems%20have%20shown%20excellent%20capabilities%20in%20imitating%20the%20human%0Adriver%20behaviour%2C%20but%20they%20struggle%20to%20guarantee%20safe%20closed-loop%20driving.%0AConversely%2C%20optimization-based%20planners%20offer%20greater%20security%20in%20short-term%0Aplanning%20scenarios.%20To%20confront%20this%20challenge%2C%20in%20this%20paper%20we%20propose%20a%0Anovel%20hybrid%20motion%20planner%20that%20integrates%20both%20learning-based%20and%0Aoptimization-based%20techniques.%20Initially%2C%20a%20multilayer%20perceptron%20%28MLP%29%0Agenerates%20a%20human-like%20trajectory%2C%20which%20is%20then%20refined%20by%20an%0Aoptimization-based%20component.%20This%20component%20not%20only%20minimizes%20tracking%20errors%0Abut%20also%20computes%20a%20trajectory%20that%20is%20both%20kinematically%20feasible%20and%0Acollision-free%20with%20obstacles%20and%20road%20boundaries.%20Our%20model%20effectively%0Abalances%20safety%20and%20human-likeness%2C%20mitigating%20the%20trade-off%20inherent%20in%20these%0Aobjectives.%20We%20validate%20our%20approach%20through%20simulation%20experiments%20and%20further%0Ademonstrate%20its%20efficacy%20by%20deploying%20it%20in%20real-world%20self-driving%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02871v1&entry.124074799=Read"},
{"title": "SITAR: Semi-supervised Image Transformer for Action Recognition", "author": "Owais Iqbal and Omprakash Chakraborty and Aftab Hussain and Rameswar Panda and Abir Das", "abstract": "  Recognizing actions from a limited set of labeled videos remains a challenge\nas annotating visual data is not only tedious but also can be expensive due to\nclassified nature. Moreover, handling spatio-temporal data using deep $3$D\ntransformers for this can introduce significant computational complexity. In\nthis paper, our objective is to address video action recognition in a\nsemi-supervised setting by leveraging only a handful of labeled videos along\nwith a collection of unlabeled videos in a compute efficient manner.\nSpecifically, we rearrange multiple frames from the input videos in row-column\nform to construct super images. Subsequently, we capitalize on the vast pool of\nunlabeled samples and employ contrastive learning on the encoded super images.\nOur proposed approach employs two pathways to generate representations for\ntemporally augmented super images originating from the same video.\nSpecifically, we utilize a 2D image-transformer to generate representations and\napply a contrastive loss function to minimize the similarity between\nrepresentations from different videos while maximizing the representations of\nidentical videos. Our method demonstrates superior performance compared to\nexisting state-of-the-art approaches for semi-supervised action recognition\nacross various benchmark datasets, all while significantly reducing\ncomputational costs.\n", "link": "http://arxiv.org/abs/2409.02910v1", "date": "2024-09-04", "relevancy": 2.251, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5771}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5601}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SITAR%3A%20Semi-supervised%20Image%20Transformer%20for%20Action%20Recognition&body=Title%3A%20SITAR%3A%20Semi-supervised%20Image%20Transformer%20for%20Action%20Recognition%0AAuthor%3A%20Owais%20Iqbal%20and%20Omprakash%20Chakraborty%20and%20Aftab%20Hussain%20and%20Rameswar%20Panda%20and%20Abir%20Das%0AAbstract%3A%20%20%20Recognizing%20actions%20from%20a%20limited%20set%20of%20labeled%20videos%20remains%20a%20challenge%0Aas%20annotating%20visual%20data%20is%20not%20only%20tedious%20but%20also%20can%20be%20expensive%20due%20to%0Aclassified%20nature.%20Moreover%2C%20handling%20spatio-temporal%20data%20using%20deep%20%243%24D%0Atransformers%20for%20this%20can%20introduce%20significant%20computational%20complexity.%20In%0Athis%20paper%2C%20our%20objective%20is%20to%20address%20video%20action%20recognition%20in%20a%0Asemi-supervised%20setting%20by%20leveraging%20only%20a%20handful%20of%20labeled%20videos%20along%0Awith%20a%20collection%20of%20unlabeled%20videos%20in%20a%20compute%20efficient%20manner.%0ASpecifically%2C%20we%20rearrange%20multiple%20frames%20from%20the%20input%20videos%20in%20row-column%0Aform%20to%20construct%20super%20images.%20Subsequently%2C%20we%20capitalize%20on%20the%20vast%20pool%20of%0Aunlabeled%20samples%20and%20employ%20contrastive%20learning%20on%20the%20encoded%20super%20images.%0AOur%20proposed%20approach%20employs%20two%20pathways%20to%20generate%20representations%20for%0Atemporally%20augmented%20super%20images%20originating%20from%20the%20same%20video.%0ASpecifically%2C%20we%20utilize%20a%202D%20image-transformer%20to%20generate%20representations%20and%0Aapply%20a%20contrastive%20loss%20function%20to%20minimize%20the%20similarity%20between%0Arepresentations%20from%20different%20videos%20while%20maximizing%20the%20representations%20of%0Aidentical%20videos.%20Our%20method%20demonstrates%20superior%20performance%20compared%20to%0Aexisting%20state-of-the-art%20approaches%20for%20semi-supervised%20action%20recognition%0Aacross%20various%20benchmark%20datasets%2C%20all%20while%20significantly%20reducing%0Acomputational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSITAR%253A%2520Semi-supervised%2520Image%2520Transformer%2520for%2520Action%2520Recognition%26entry.906535625%3DOwais%2520Iqbal%2520and%2520Omprakash%2520Chakraborty%2520and%2520Aftab%2520Hussain%2520and%2520Rameswar%2520Panda%2520and%2520Abir%2520Das%26entry.1292438233%3D%2520%2520Recognizing%2520actions%2520from%2520a%2520limited%2520set%2520of%2520labeled%2520videos%2520remains%2520a%2520challenge%250Aas%2520annotating%2520visual%2520data%2520is%2520not%2520only%2520tedious%2520but%2520also%2520can%2520be%2520expensive%2520due%2520to%250Aclassified%2520nature.%2520Moreover%252C%2520handling%2520spatio-temporal%2520data%2520using%2520deep%2520%25243%2524D%250Atransformers%2520for%2520this%2520can%2520introduce%2520significant%2520computational%2520complexity.%2520In%250Athis%2520paper%252C%2520our%2520objective%2520is%2520to%2520address%2520video%2520action%2520recognition%2520in%2520a%250Asemi-supervised%2520setting%2520by%2520leveraging%2520only%2520a%2520handful%2520of%2520labeled%2520videos%2520along%250Awith%2520a%2520collection%2520of%2520unlabeled%2520videos%2520in%2520a%2520compute%2520efficient%2520manner.%250ASpecifically%252C%2520we%2520rearrange%2520multiple%2520frames%2520from%2520the%2520input%2520videos%2520in%2520row-column%250Aform%2520to%2520construct%2520super%2520images.%2520Subsequently%252C%2520we%2520capitalize%2520on%2520the%2520vast%2520pool%2520of%250Aunlabeled%2520samples%2520and%2520employ%2520contrastive%2520learning%2520on%2520the%2520encoded%2520super%2520images.%250AOur%2520proposed%2520approach%2520employs%2520two%2520pathways%2520to%2520generate%2520representations%2520for%250Atemporally%2520augmented%2520super%2520images%2520originating%2520from%2520the%2520same%2520video.%250ASpecifically%252C%2520we%2520utilize%2520a%25202D%2520image-transformer%2520to%2520generate%2520representations%2520and%250Aapply%2520a%2520contrastive%2520loss%2520function%2520to%2520minimize%2520the%2520similarity%2520between%250Arepresentations%2520from%2520different%2520videos%2520while%2520maximizing%2520the%2520representations%2520of%250Aidentical%2520videos.%2520Our%2520method%2520demonstrates%2520superior%2520performance%2520compared%2520to%250Aexisting%2520state-of-the-art%2520approaches%2520for%2520semi-supervised%2520action%2520recognition%250Aacross%2520various%2520benchmark%2520datasets%252C%2520all%2520while%2520significantly%2520reducing%250Acomputational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SITAR%3A%20Semi-supervised%20Image%20Transformer%20for%20Action%20Recognition&entry.906535625=Owais%20Iqbal%20and%20Omprakash%20Chakraborty%20and%20Aftab%20Hussain%20and%20Rameswar%20Panda%20and%20Abir%20Das&entry.1292438233=%20%20Recognizing%20actions%20from%20a%20limited%20set%20of%20labeled%20videos%20remains%20a%20challenge%0Aas%20annotating%20visual%20data%20is%20not%20only%20tedious%20but%20also%20can%20be%20expensive%20due%20to%0Aclassified%20nature.%20Moreover%2C%20handling%20spatio-temporal%20data%20using%20deep%20%243%24D%0Atransformers%20for%20this%20can%20introduce%20significant%20computational%20complexity.%20In%0Athis%20paper%2C%20our%20objective%20is%20to%20address%20video%20action%20recognition%20in%20a%0Asemi-supervised%20setting%20by%20leveraging%20only%20a%20handful%20of%20labeled%20videos%20along%0Awith%20a%20collection%20of%20unlabeled%20videos%20in%20a%20compute%20efficient%20manner.%0ASpecifically%2C%20we%20rearrange%20multiple%20frames%20from%20the%20input%20videos%20in%20row-column%0Aform%20to%20construct%20super%20images.%20Subsequently%2C%20we%20capitalize%20on%20the%20vast%20pool%20of%0Aunlabeled%20samples%20and%20employ%20contrastive%20learning%20on%20the%20encoded%20super%20images.%0AOur%20proposed%20approach%20employs%20two%20pathways%20to%20generate%20representations%20for%0Atemporally%20augmented%20super%20images%20originating%20from%20the%20same%20video.%0ASpecifically%2C%20we%20utilize%20a%202D%20image-transformer%20to%20generate%20representations%20and%0Aapply%20a%20contrastive%20loss%20function%20to%20minimize%20the%20similarity%20between%0Arepresentations%20from%20different%20videos%20while%20maximizing%20the%20representations%20of%0Aidentical%20videos.%20Our%20method%20demonstrates%20superior%20performance%20compared%20to%0Aexisting%20state-of-the-art%20approaches%20for%20semi-supervised%20action%20recognition%0Aacross%20various%20benchmark%20datasets%2C%20all%20while%20significantly%20reducing%0Acomputational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02910v1&entry.124074799=Read"},
{"title": "Improved Single Camera BEV Perception Using Multi-Camera Training", "author": "Daniel Busch and Ido Freeman and Richard Meyes and Tobias Meisen", "abstract": "  Bird's Eye View (BEV) map prediction is essential for downstream autonomous\ndriving tasks like trajectory prediction. In the past, this was accomplished\nthrough the use of a sophisticated sensor configuration that captured a\nsurround view from multiple cameras. However, in large-scale production, cost\nefficiency is an optimization goal, so that using fewer cameras becomes more\nrelevant. But the consequence of fewer input images correlates with a\nperformance drop. This raises the problem of developing a BEV perception model\nthat provides a sufficient performance on a low-cost sensor setup. Although,\nprimarily relevant for inference time on production cars, this cost restriction\nis less problematic on a test vehicle during training. Therefore, the objective\nof our approach is to reduce the aforementioned performance drop as much as\npossible using a modern multi-camera surround view model reduced for\nsingle-camera inference. The approach includes three features, a modern masking\ntechnique, a cyclic Learning Rate (LR) schedule, and a feature reconstruction\nloss for supervising the transition from six-camera inputs to one-camera input\nduring training. Our method outperforms versions trained strictly with one\ncamera or strictly with six-camera surround view for single-camera inference\nresulting in reduced hallucination and better quality of the BEV map.\n", "link": "http://arxiv.org/abs/2409.02676v1", "date": "2024-09-04", "relevancy": 2.2358, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5845}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5746}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Single%20Camera%20BEV%20Perception%20Using%20Multi-Camera%20Training&body=Title%3A%20Improved%20Single%20Camera%20BEV%20Perception%20Using%20Multi-Camera%20Training%0AAuthor%3A%20Daniel%20Busch%20and%20Ido%20Freeman%20and%20Richard%20Meyes%20and%20Tobias%20Meisen%0AAbstract%3A%20%20%20Bird%27s%20Eye%20View%20%28BEV%29%20map%20prediction%20is%20essential%20for%20downstream%20autonomous%0Adriving%20tasks%20like%20trajectory%20prediction.%20In%20the%20past%2C%20this%20was%20accomplished%0Athrough%20the%20use%20of%20a%20sophisticated%20sensor%20configuration%20that%20captured%20a%0Asurround%20view%20from%20multiple%20cameras.%20However%2C%20in%20large-scale%20production%2C%20cost%0Aefficiency%20is%20an%20optimization%20goal%2C%20so%20that%20using%20fewer%20cameras%20becomes%20more%0Arelevant.%20But%20the%20consequence%20of%20fewer%20input%20images%20correlates%20with%20a%0Aperformance%20drop.%20This%20raises%20the%20problem%20of%20developing%20a%20BEV%20perception%20model%0Athat%20provides%20a%20sufficient%20performance%20on%20a%20low-cost%20sensor%20setup.%20Although%2C%0Aprimarily%20relevant%20for%20inference%20time%20on%20production%20cars%2C%20this%20cost%20restriction%0Ais%20less%20problematic%20on%20a%20test%20vehicle%20during%20training.%20Therefore%2C%20the%20objective%0Aof%20our%20approach%20is%20to%20reduce%20the%20aforementioned%20performance%20drop%20as%20much%20as%0Apossible%20using%20a%20modern%20multi-camera%20surround%20view%20model%20reduced%20for%0Asingle-camera%20inference.%20The%20approach%20includes%20three%20features%2C%20a%20modern%20masking%0Atechnique%2C%20a%20cyclic%20Learning%20Rate%20%28LR%29%20schedule%2C%20and%20a%20feature%20reconstruction%0Aloss%20for%20supervising%20the%20transition%20from%20six-camera%20inputs%20to%20one-camera%20input%0Aduring%20training.%20Our%20method%20outperforms%20versions%20trained%20strictly%20with%20one%0Acamera%20or%20strictly%20with%20six-camera%20surround%20view%20for%20single-camera%20inference%0Aresulting%20in%20reduced%20hallucination%20and%20better%20quality%20of%20the%20BEV%20map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Single%2520Camera%2520BEV%2520Perception%2520Using%2520Multi-Camera%2520Training%26entry.906535625%3DDaniel%2520Busch%2520and%2520Ido%2520Freeman%2520and%2520Richard%2520Meyes%2520and%2520Tobias%2520Meisen%26entry.1292438233%3D%2520%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520map%2520prediction%2520is%2520essential%2520for%2520downstream%2520autonomous%250Adriving%2520tasks%2520like%2520trajectory%2520prediction.%2520In%2520the%2520past%252C%2520this%2520was%2520accomplished%250Athrough%2520the%2520use%2520of%2520a%2520sophisticated%2520sensor%2520configuration%2520that%2520captured%2520a%250Asurround%2520view%2520from%2520multiple%2520cameras.%2520However%252C%2520in%2520large-scale%2520production%252C%2520cost%250Aefficiency%2520is%2520an%2520optimization%2520goal%252C%2520so%2520that%2520using%2520fewer%2520cameras%2520becomes%2520more%250Arelevant.%2520But%2520the%2520consequence%2520of%2520fewer%2520input%2520images%2520correlates%2520with%2520a%250Aperformance%2520drop.%2520This%2520raises%2520the%2520problem%2520of%2520developing%2520a%2520BEV%2520perception%2520model%250Athat%2520provides%2520a%2520sufficient%2520performance%2520on%2520a%2520low-cost%2520sensor%2520setup.%2520Although%252C%250Aprimarily%2520relevant%2520for%2520inference%2520time%2520on%2520production%2520cars%252C%2520this%2520cost%2520restriction%250Ais%2520less%2520problematic%2520on%2520a%2520test%2520vehicle%2520during%2520training.%2520Therefore%252C%2520the%2520objective%250Aof%2520our%2520approach%2520is%2520to%2520reduce%2520the%2520aforementioned%2520performance%2520drop%2520as%2520much%2520as%250Apossible%2520using%2520a%2520modern%2520multi-camera%2520surround%2520view%2520model%2520reduced%2520for%250Asingle-camera%2520inference.%2520The%2520approach%2520includes%2520three%2520features%252C%2520a%2520modern%2520masking%250Atechnique%252C%2520a%2520cyclic%2520Learning%2520Rate%2520%2528LR%2529%2520schedule%252C%2520and%2520a%2520feature%2520reconstruction%250Aloss%2520for%2520supervising%2520the%2520transition%2520from%2520six-camera%2520inputs%2520to%2520one-camera%2520input%250Aduring%2520training.%2520Our%2520method%2520outperforms%2520versions%2520trained%2520strictly%2520with%2520one%250Acamera%2520or%2520strictly%2520with%2520six-camera%2520surround%2520view%2520for%2520single-camera%2520inference%250Aresulting%2520in%2520reduced%2520hallucination%2520and%2520better%2520quality%2520of%2520the%2520BEV%2520map.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Single%20Camera%20BEV%20Perception%20Using%20Multi-Camera%20Training&entry.906535625=Daniel%20Busch%20and%20Ido%20Freeman%20and%20Richard%20Meyes%20and%20Tobias%20Meisen&entry.1292438233=%20%20Bird%27s%20Eye%20View%20%28BEV%29%20map%20prediction%20is%20essential%20for%20downstream%20autonomous%0Adriving%20tasks%20like%20trajectory%20prediction.%20In%20the%20past%2C%20this%20was%20accomplished%0Athrough%20the%20use%20of%20a%20sophisticated%20sensor%20configuration%20that%20captured%20a%0Asurround%20view%20from%20multiple%20cameras.%20However%2C%20in%20large-scale%20production%2C%20cost%0Aefficiency%20is%20an%20optimization%20goal%2C%20so%20that%20using%20fewer%20cameras%20becomes%20more%0Arelevant.%20But%20the%20consequence%20of%20fewer%20input%20images%20correlates%20with%20a%0Aperformance%20drop.%20This%20raises%20the%20problem%20of%20developing%20a%20BEV%20perception%20model%0Athat%20provides%20a%20sufficient%20performance%20on%20a%20low-cost%20sensor%20setup.%20Although%2C%0Aprimarily%20relevant%20for%20inference%20time%20on%20production%20cars%2C%20this%20cost%20restriction%0Ais%20less%20problematic%20on%20a%20test%20vehicle%20during%20training.%20Therefore%2C%20the%20objective%0Aof%20our%20approach%20is%20to%20reduce%20the%20aforementioned%20performance%20drop%20as%20much%20as%0Apossible%20using%20a%20modern%20multi-camera%20surround%20view%20model%20reduced%20for%0Asingle-camera%20inference.%20The%20approach%20includes%20three%20features%2C%20a%20modern%20masking%0Atechnique%2C%20a%20cyclic%20Learning%20Rate%20%28LR%29%20schedule%2C%20and%20a%20feature%20reconstruction%0Aloss%20for%20supervising%20the%20transition%20from%20six-camera%20inputs%20to%20one-camera%20input%0Aduring%20training.%20Our%20method%20outperforms%20versions%20trained%20strictly%20with%20one%0Acamera%20or%20strictly%20with%20six-camera%20surround%20view%20for%20single-camera%20inference%0Aresulting%20in%20reduced%20hallucination%20and%20better%20quality%20of%20the%20BEV%20map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02676v1&entry.124074799=Read"},
{"title": "(Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in\n  Large Models", "author": "Andreas Kirsch", "abstract": "  Epistemic uncertainty is crucial for safety-critical applications and\nout-of-distribution detection tasks. Yet, we uncover a paradoxical phenomenon\nin deep learning models: an epistemic uncertainty collapse as model complexity\nincreases, challenging the assumption that larger models invariably offer\nbetter uncertainty quantification. We propose that this stems from implicit\nensembling within large models. To support this hypothesis, we demonstrate\nepistemic uncertainty collapse empirically across various architectures, from\nexplicit ensembles of ensembles and simple MLPs to state-of-the-art vision\nmodels, including ResNets and Vision Transformers -- for the latter, we examine\nimplicit ensemble extraction and decompose larger models into diverse\nsub-models, recovering epistemic uncertainty. We provide theoretical\njustification for these phenomena and explore their implications for\nuncertainty estimation.\n", "link": "http://arxiv.org/abs/2409.02628v1", "date": "2024-09-04", "relevancy": 2.2165, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6043}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%28Implicit%29%20Ensembles%20of%20Ensembles%3A%20Epistemic%20Uncertainty%20Collapse%20in%0A%20%20Large%20Models&body=Title%3A%20%28Implicit%29%20Ensembles%20of%20Ensembles%3A%20Epistemic%20Uncertainty%20Collapse%20in%0A%20%20Large%20Models%0AAuthor%3A%20Andreas%20Kirsch%0AAbstract%3A%20%20%20Epistemic%20uncertainty%20is%20crucial%20for%20safety-critical%20applications%20and%0Aout-of-distribution%20detection%20tasks.%20Yet%2C%20we%20uncover%20a%20paradoxical%20phenomenon%0Ain%20deep%20learning%20models%3A%20an%20epistemic%20uncertainty%20collapse%20as%20model%20complexity%0Aincreases%2C%20challenging%20the%20assumption%20that%20larger%20models%20invariably%20offer%0Abetter%20uncertainty%20quantification.%20We%20propose%20that%20this%20stems%20from%20implicit%0Aensembling%20within%20large%20models.%20To%20support%20this%20hypothesis%2C%20we%20demonstrate%0Aepistemic%20uncertainty%20collapse%20empirically%20across%20various%20architectures%2C%20from%0Aexplicit%20ensembles%20of%20ensembles%20and%20simple%20MLPs%20to%20state-of-the-art%20vision%0Amodels%2C%20including%20ResNets%20and%20Vision%20Transformers%20--%20for%20the%20latter%2C%20we%20examine%0Aimplicit%20ensemble%20extraction%20and%20decompose%20larger%20models%20into%20diverse%0Asub-models%2C%20recovering%20epistemic%20uncertainty.%20We%20provide%20theoretical%0Ajustification%20for%20these%20phenomena%20and%20explore%20their%20implications%20for%0Auncertainty%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2528Implicit%2529%2520Ensembles%2520of%2520Ensembles%253A%2520Epistemic%2520Uncertainty%2520Collapse%2520in%250A%2520%2520Large%2520Models%26entry.906535625%3DAndreas%2520Kirsch%26entry.1292438233%3D%2520%2520Epistemic%2520uncertainty%2520is%2520crucial%2520for%2520safety-critical%2520applications%2520and%250Aout-of-distribution%2520detection%2520tasks.%2520Yet%252C%2520we%2520uncover%2520a%2520paradoxical%2520phenomenon%250Ain%2520deep%2520learning%2520models%253A%2520an%2520epistemic%2520uncertainty%2520collapse%2520as%2520model%2520complexity%250Aincreases%252C%2520challenging%2520the%2520assumption%2520that%2520larger%2520models%2520invariably%2520offer%250Abetter%2520uncertainty%2520quantification.%2520We%2520propose%2520that%2520this%2520stems%2520from%2520implicit%250Aensembling%2520within%2520large%2520models.%2520To%2520support%2520this%2520hypothesis%252C%2520we%2520demonstrate%250Aepistemic%2520uncertainty%2520collapse%2520empirically%2520across%2520various%2520architectures%252C%2520from%250Aexplicit%2520ensembles%2520of%2520ensembles%2520and%2520simple%2520MLPs%2520to%2520state-of-the-art%2520vision%250Amodels%252C%2520including%2520ResNets%2520and%2520Vision%2520Transformers%2520--%2520for%2520the%2520latter%252C%2520we%2520examine%250Aimplicit%2520ensemble%2520extraction%2520and%2520decompose%2520larger%2520models%2520into%2520diverse%250Asub-models%252C%2520recovering%2520epistemic%2520uncertainty.%2520We%2520provide%2520theoretical%250Ajustification%2520for%2520these%2520phenomena%2520and%2520explore%2520their%2520implications%2520for%250Auncertainty%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%28Implicit%29%20Ensembles%20of%20Ensembles%3A%20Epistemic%20Uncertainty%20Collapse%20in%0A%20%20Large%20Models&entry.906535625=Andreas%20Kirsch&entry.1292438233=%20%20Epistemic%20uncertainty%20is%20crucial%20for%20safety-critical%20applications%20and%0Aout-of-distribution%20detection%20tasks.%20Yet%2C%20we%20uncover%20a%20paradoxical%20phenomenon%0Ain%20deep%20learning%20models%3A%20an%20epistemic%20uncertainty%20collapse%20as%20model%20complexity%0Aincreases%2C%20challenging%20the%20assumption%20that%20larger%20models%20invariably%20offer%0Abetter%20uncertainty%20quantification.%20We%20propose%20that%20this%20stems%20from%20implicit%0Aensembling%20within%20large%20models.%20To%20support%20this%20hypothesis%2C%20we%20demonstrate%0Aepistemic%20uncertainty%20collapse%20empirically%20across%20various%20architectures%2C%20from%0Aexplicit%20ensembles%20of%20ensembles%20and%20simple%20MLPs%20to%20state-of-the-art%20vision%0Amodels%2C%20including%20ResNets%20and%20Vision%20Transformers%20--%20for%20the%20latter%2C%20we%20examine%0Aimplicit%20ensemble%20extraction%20and%20decompose%20larger%20models%20into%20diverse%0Asub-models%2C%20recovering%20epistemic%20uncertainty.%20We%20provide%20theoretical%0Ajustification%20for%20these%20phenomena%20and%20explore%20their%20implications%20for%0Auncertainty%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02628v1&entry.124074799=Read"},
{"title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and\n  Exploit Inaccurate Categorical Sampling", "author": "Kaiwen Zheng and Yongxin Chen and Hanzi Mao and Ming-Yu Liu and Jun Zhu and Qinsheng Zhang", "abstract": "  Masked diffusion models (MDMs) have emerged as a popular research topic for\ngenerative modeling of discrete data, thanks to their superior performance over\nother discrete diffusion models, and are rivaling the auto-regressive models\n(ARMs) for language modeling tasks. The recent effort in simplifying the masked\ndiffusion framework further leads to alignment with continuous-space diffusion\nmodels and more principled training and sampling recipes. In this paper,\nhowever, we reveal that both training and sampling of MDMs are theoretically\nfree from the time variable, arguably the key signature of diffusion models,\nand are instead equivalent to masked models. The connection on the sampling\naspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we\nshow that the FHS is theoretically equivalent to MDMs' original generation\nprocess while significantly alleviating the time-consuming categorical sampling\nand achieving a 20$\\times$ speedup. In addition, our investigation challenges\nprevious claims that MDMs can surpass ARMs in generative perplexity. We\nidentify, for the first time, an underlying numerical issue, even with the\n32-bit floating-point precision, which results in inaccurate categorical\nsampling. We show that the numerical issue lowers the effective temperature\nboth theoretically and empirically, leading to unfair assessments of MDMs'\ngeneration results in the previous literature.\n", "link": "http://arxiv.org/abs/2409.02908v1", "date": "2024-09-04", "relevancy": 2.2112, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6048}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5433}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Diffusion%20Models%20are%20Secretly%20Time-Agnostic%20Masked%20Models%20and%0A%20%20Exploit%20Inaccurate%20Categorical%20Sampling&body=Title%3A%20Masked%20Diffusion%20Models%20are%20Secretly%20Time-Agnostic%20Masked%20Models%20and%0A%20%20Exploit%20Inaccurate%20Categorical%20Sampling%0AAuthor%3A%20Kaiwen%20Zheng%20and%20Yongxin%20Chen%20and%20Hanzi%20Mao%20and%20Ming-Yu%20Liu%20and%20Jun%20Zhu%20and%20Qinsheng%20Zhang%0AAbstract%3A%20%20%20Masked%20diffusion%20models%20%28MDMs%29%20have%20emerged%20as%20a%20popular%20research%20topic%20for%0Agenerative%20modeling%20of%20discrete%20data%2C%20thanks%20to%20their%20superior%20performance%20over%0Aother%20discrete%20diffusion%20models%2C%20and%20are%20rivaling%20the%20auto-regressive%20models%0A%28ARMs%29%20for%20language%20modeling%20tasks.%20The%20recent%20effort%20in%20simplifying%20the%20masked%0Adiffusion%20framework%20further%20leads%20to%20alignment%20with%20continuous-space%20diffusion%0Amodels%20and%20more%20principled%20training%20and%20sampling%20recipes.%20In%20this%20paper%2C%0Ahowever%2C%20we%20reveal%20that%20both%20training%20and%20sampling%20of%20MDMs%20are%20theoretically%0Afree%20from%20the%20time%20variable%2C%20arguably%20the%20key%20signature%20of%20diffusion%20models%2C%0Aand%20are%20instead%20equivalent%20to%20masked%20models.%20The%20connection%20on%20the%20sampling%0Aaspect%20is%20drawn%20by%20our%20proposed%20first-hitting%20sampler%20%28FHS%29.%20Specifically%2C%20we%0Ashow%20that%20the%20FHS%20is%20theoretically%20equivalent%20to%20MDMs%27%20original%20generation%0Aprocess%20while%20significantly%20alleviating%20the%20time-consuming%20categorical%20sampling%0Aand%20achieving%20a%2020%24%5Ctimes%24%20speedup.%20In%20addition%2C%20our%20investigation%20challenges%0Aprevious%20claims%20that%20MDMs%20can%20surpass%20ARMs%20in%20generative%20perplexity.%20We%0Aidentify%2C%20for%20the%20first%20time%2C%20an%20underlying%20numerical%20issue%2C%20even%20with%20the%0A32-bit%20floating-point%20precision%2C%20which%20results%20in%20inaccurate%20categorical%0Asampling.%20We%20show%20that%20the%20numerical%20issue%20lowers%20the%20effective%20temperature%0Aboth%20theoretically%20and%20empirically%2C%20leading%20to%20unfair%20assessments%20of%20MDMs%27%0Ageneration%20results%20in%20the%20previous%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Diffusion%2520Models%2520are%2520Secretly%2520Time-Agnostic%2520Masked%2520Models%2520and%250A%2520%2520Exploit%2520Inaccurate%2520Categorical%2520Sampling%26entry.906535625%3DKaiwen%2520Zheng%2520and%2520Yongxin%2520Chen%2520and%2520Hanzi%2520Mao%2520and%2520Ming-Yu%2520Liu%2520and%2520Jun%2520Zhu%2520and%2520Qinsheng%2520Zhang%26entry.1292438233%3D%2520%2520Masked%2520diffusion%2520models%2520%2528MDMs%2529%2520have%2520emerged%2520as%2520a%2520popular%2520research%2520topic%2520for%250Agenerative%2520modeling%2520of%2520discrete%2520data%252C%2520thanks%2520to%2520their%2520superior%2520performance%2520over%250Aother%2520discrete%2520diffusion%2520models%252C%2520and%2520are%2520rivaling%2520the%2520auto-regressive%2520models%250A%2528ARMs%2529%2520for%2520language%2520modeling%2520tasks.%2520The%2520recent%2520effort%2520in%2520simplifying%2520the%2520masked%250Adiffusion%2520framework%2520further%2520leads%2520to%2520alignment%2520with%2520continuous-space%2520diffusion%250Amodels%2520and%2520more%2520principled%2520training%2520and%2520sampling%2520recipes.%2520In%2520this%2520paper%252C%250Ahowever%252C%2520we%2520reveal%2520that%2520both%2520training%2520and%2520sampling%2520of%2520MDMs%2520are%2520theoretically%250Afree%2520from%2520the%2520time%2520variable%252C%2520arguably%2520the%2520key%2520signature%2520of%2520diffusion%2520models%252C%250Aand%2520are%2520instead%2520equivalent%2520to%2520masked%2520models.%2520The%2520connection%2520on%2520the%2520sampling%250Aaspect%2520is%2520drawn%2520by%2520our%2520proposed%2520first-hitting%2520sampler%2520%2528FHS%2529.%2520Specifically%252C%2520we%250Ashow%2520that%2520the%2520FHS%2520is%2520theoretically%2520equivalent%2520to%2520MDMs%2527%2520original%2520generation%250Aprocess%2520while%2520significantly%2520alleviating%2520the%2520time-consuming%2520categorical%2520sampling%250Aand%2520achieving%2520a%252020%2524%255Ctimes%2524%2520speedup.%2520In%2520addition%252C%2520our%2520investigation%2520challenges%250Aprevious%2520claims%2520that%2520MDMs%2520can%2520surpass%2520ARMs%2520in%2520generative%2520perplexity.%2520We%250Aidentify%252C%2520for%2520the%2520first%2520time%252C%2520an%2520underlying%2520numerical%2520issue%252C%2520even%2520with%2520the%250A32-bit%2520floating-point%2520precision%252C%2520which%2520results%2520in%2520inaccurate%2520categorical%250Asampling.%2520We%2520show%2520that%2520the%2520numerical%2520issue%2520lowers%2520the%2520effective%2520temperature%250Aboth%2520theoretically%2520and%2520empirically%252C%2520leading%2520to%2520unfair%2520assessments%2520of%2520MDMs%2527%250Ageneration%2520results%2520in%2520the%2520previous%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Diffusion%20Models%20are%20Secretly%20Time-Agnostic%20Masked%20Models%20and%0A%20%20Exploit%20Inaccurate%20Categorical%20Sampling&entry.906535625=Kaiwen%20Zheng%20and%20Yongxin%20Chen%20and%20Hanzi%20Mao%20and%20Ming-Yu%20Liu%20and%20Jun%20Zhu%20and%20Qinsheng%20Zhang&entry.1292438233=%20%20Masked%20diffusion%20models%20%28MDMs%29%20have%20emerged%20as%20a%20popular%20research%20topic%20for%0Agenerative%20modeling%20of%20discrete%20data%2C%20thanks%20to%20their%20superior%20performance%20over%0Aother%20discrete%20diffusion%20models%2C%20and%20are%20rivaling%20the%20auto-regressive%20models%0A%28ARMs%29%20for%20language%20modeling%20tasks.%20The%20recent%20effort%20in%20simplifying%20the%20masked%0Adiffusion%20framework%20further%20leads%20to%20alignment%20with%20continuous-space%20diffusion%0Amodels%20and%20more%20principled%20training%20and%20sampling%20recipes.%20In%20this%20paper%2C%0Ahowever%2C%20we%20reveal%20that%20both%20training%20and%20sampling%20of%20MDMs%20are%20theoretically%0Afree%20from%20the%20time%20variable%2C%20arguably%20the%20key%20signature%20of%20diffusion%20models%2C%0Aand%20are%20instead%20equivalent%20to%20masked%20models.%20The%20connection%20on%20the%20sampling%0Aaspect%20is%20drawn%20by%20our%20proposed%20first-hitting%20sampler%20%28FHS%29.%20Specifically%2C%20we%0Ashow%20that%20the%20FHS%20is%20theoretically%20equivalent%20to%20MDMs%27%20original%20generation%0Aprocess%20while%20significantly%20alleviating%20the%20time-consuming%20categorical%20sampling%0Aand%20achieving%20a%2020%24%5Ctimes%24%20speedup.%20In%20addition%2C%20our%20investigation%20challenges%0Aprevious%20claims%20that%20MDMs%20can%20surpass%20ARMs%20in%20generative%20perplexity.%20We%0Aidentify%2C%20for%20the%20first%20time%2C%20an%20underlying%20numerical%20issue%2C%20even%20with%20the%0A32-bit%20floating-point%20precision%2C%20which%20results%20in%20inaccurate%20categorical%0Asampling.%20We%20show%20that%20the%20numerical%20issue%20lowers%20the%20effective%20temperature%0Aboth%20theoretically%20and%20empirically%2C%20leading%20to%20unfair%20assessments%20of%20MDMs%27%0Ageneration%20results%20in%20the%20previous%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02908v1&entry.124074799=Read"},
{"title": "Towards Edge-Based Data Lake Architecture for Intelligent Transportation\n  System", "author": "Danilo Fernandes and Douglas L. L. Moura and Gean Santos and Geymerson S. Ramos and Fabiane Queiroz and Andre L. L. Aquino", "abstract": "  The rapid urbanization growth has underscored the need for innovative\nsolutions to enhance transportation efficiency and safety. Intelligent\nTransportation Systems (ITS) have emerged as a promising solution in this\ncontext. However, analyzing and processing the massive and intricate data\ngenerated by ITS presents significant challenges for traditional data\nprocessing systems. This work proposes an Edge-based Data Lake Architecture to\nintegrate and analyze the complex data from ITS efficiently. The architecture\noffers scalability, fault tolerance, and performance, improving decision-making\nand enhancing innovative services for a more intelligent transportation\necosystem. We demonstrate the effectiveness of the architecture through an\nanalysis of three different use cases: (i) Vehicular Sensor Network, (ii)\nMobile Network, and (iii) Driver Identification applications.\n", "link": "http://arxiv.org/abs/2409.02808v1", "date": "2024-09-04", "relevancy": 2.1954, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4604}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4364}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Edge-Based%20Data%20Lake%20Architecture%20for%20Intelligent%20Transportation%0A%20%20System&body=Title%3A%20Towards%20Edge-Based%20Data%20Lake%20Architecture%20for%20Intelligent%20Transportation%0A%20%20System%0AAuthor%3A%20Danilo%20Fernandes%20and%20Douglas%20L.%20L.%20Moura%20and%20Gean%20Santos%20and%20Geymerson%20S.%20Ramos%20and%20Fabiane%20Queiroz%20and%20Andre%20L.%20L.%20Aquino%0AAbstract%3A%20%20%20The%20rapid%20urbanization%20growth%20has%20underscored%20the%20need%20for%20innovative%0Asolutions%20to%20enhance%20transportation%20efficiency%20and%20safety.%20Intelligent%0ATransportation%20Systems%20%28ITS%29%20have%20emerged%20as%20a%20promising%20solution%20in%20this%0Acontext.%20However%2C%20analyzing%20and%20processing%20the%20massive%20and%20intricate%20data%0Agenerated%20by%20ITS%20presents%20significant%20challenges%20for%20traditional%20data%0Aprocessing%20systems.%20This%20work%20proposes%20an%20Edge-based%20Data%20Lake%20Architecture%20to%0Aintegrate%20and%20analyze%20the%20complex%20data%20from%20ITS%20efficiently.%20The%20architecture%0Aoffers%20scalability%2C%20fault%20tolerance%2C%20and%20performance%2C%20improving%20decision-making%0Aand%20enhancing%20innovative%20services%20for%20a%20more%20intelligent%20transportation%0Aecosystem.%20We%20demonstrate%20the%20effectiveness%20of%20the%20architecture%20through%20an%0Aanalysis%20of%20three%20different%20use%20cases%3A%20%28i%29%20Vehicular%20Sensor%20Network%2C%20%28ii%29%0AMobile%20Network%2C%20and%20%28iii%29%20Driver%20Identification%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Edge-Based%2520Data%2520Lake%2520Architecture%2520for%2520Intelligent%2520Transportation%250A%2520%2520System%26entry.906535625%3DDanilo%2520Fernandes%2520and%2520Douglas%2520L.%2520L.%2520Moura%2520and%2520Gean%2520Santos%2520and%2520Geymerson%2520S.%2520Ramos%2520and%2520Fabiane%2520Queiroz%2520and%2520Andre%2520L.%2520L.%2520Aquino%26entry.1292438233%3D%2520%2520The%2520rapid%2520urbanization%2520growth%2520has%2520underscored%2520the%2520need%2520for%2520innovative%250Asolutions%2520to%2520enhance%2520transportation%2520efficiency%2520and%2520safety.%2520Intelligent%250ATransportation%2520Systems%2520%2528ITS%2529%2520have%2520emerged%2520as%2520a%2520promising%2520solution%2520in%2520this%250Acontext.%2520However%252C%2520analyzing%2520and%2520processing%2520the%2520massive%2520and%2520intricate%2520data%250Agenerated%2520by%2520ITS%2520presents%2520significant%2520challenges%2520for%2520traditional%2520data%250Aprocessing%2520systems.%2520This%2520work%2520proposes%2520an%2520Edge-based%2520Data%2520Lake%2520Architecture%2520to%250Aintegrate%2520and%2520analyze%2520the%2520complex%2520data%2520from%2520ITS%2520efficiently.%2520The%2520architecture%250Aoffers%2520scalability%252C%2520fault%2520tolerance%252C%2520and%2520performance%252C%2520improving%2520decision-making%250Aand%2520enhancing%2520innovative%2520services%2520for%2520a%2520more%2520intelligent%2520transportation%250Aecosystem.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520architecture%2520through%2520an%250Aanalysis%2520of%2520three%2520different%2520use%2520cases%253A%2520%2528i%2529%2520Vehicular%2520Sensor%2520Network%252C%2520%2528ii%2529%250AMobile%2520Network%252C%2520and%2520%2528iii%2529%2520Driver%2520Identification%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Edge-Based%20Data%20Lake%20Architecture%20for%20Intelligent%20Transportation%0A%20%20System&entry.906535625=Danilo%20Fernandes%20and%20Douglas%20L.%20L.%20Moura%20and%20Gean%20Santos%20and%20Geymerson%20S.%20Ramos%20and%20Fabiane%20Queiroz%20and%20Andre%20L.%20L.%20Aquino&entry.1292438233=%20%20The%20rapid%20urbanization%20growth%20has%20underscored%20the%20need%20for%20innovative%0Asolutions%20to%20enhance%20transportation%20efficiency%20and%20safety.%20Intelligent%0ATransportation%20Systems%20%28ITS%29%20have%20emerged%20as%20a%20promising%20solution%20in%20this%0Acontext.%20However%2C%20analyzing%20and%20processing%20the%20massive%20and%20intricate%20data%0Agenerated%20by%20ITS%20presents%20significant%20challenges%20for%20traditional%20data%0Aprocessing%20systems.%20This%20work%20proposes%20an%20Edge-based%20Data%20Lake%20Architecture%20to%0Aintegrate%20and%20analyze%20the%20complex%20data%20from%20ITS%20efficiently.%20The%20architecture%0Aoffers%20scalability%2C%20fault%20tolerance%2C%20and%20performance%2C%20improving%20decision-making%0Aand%20enhancing%20innovative%20services%20for%20a%20more%20intelligent%20transportation%0Aecosystem.%20We%20demonstrate%20the%20effectiveness%20of%20the%20architecture%20through%20an%0Aanalysis%20of%20three%20different%20use%20cases%3A%20%28i%29%20Vehicular%20Sensor%20Network%2C%20%28ii%29%0AMobile%20Network%2C%20and%20%28iii%29%20Driver%20Identification%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02808v1&entry.124074799=Read"},
{"title": "Scalable Glacier Mapping using Deep Learning and Open Earth Observation\n  Data Matches the Accuracy of Manual Delineation", "author": "Konstantin A. Maslov and Claudio Persello and Thomas Schellenberger and Alfred Stein", "abstract": "  Accurate global glacier mapping is critical for understanding climate change\nimpacts. Despite its importance, automated glacier mapping at a global scale\nremains largely unexplored. Here we address this gap and propose\nGlacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep\nlearning model, and five strategies for multitemporal global-scale glacier\nmapping using open satellite imagery. Assessing the spatial, temporal and\ncross-sensor generalisation shows that our best strategy achieves intersection\nover union >0.85 on previously unobserved images in most cases, which drops to\n>0.75 for debris-rich areas such as High-Mountain Asia and increases to >0.90\nfor regions dominated by clean ice. A comparative validation against human\nexpert uncertainties in terms of area and distance deviations underscores\nGlaViTU performance, approaching or matching expert-level delineation. Adding\nsynthetic aperture radar data, namely, backscatter and interferometric\ncoherence, increases the accuracy in all regions where available. The\ncalibrated confidence for glacier extents is reported making the predictions\nmore reliable and interpretable. We also release a benchmark dataset that\ncovers 9% of glaciers worldwide. Our results support efforts towards automated\nmultitemporal and global glacier mapping.\n", "link": "http://arxiv.org/abs/2401.15113v3", "date": "2024-09-04", "relevancy": 2.1912, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5678}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Glacier%20Mapping%20using%20Deep%20Learning%20and%20Open%20Earth%20Observation%0A%20%20Data%20Matches%20the%20Accuracy%20of%20Manual%20Delineation&body=Title%3A%20Scalable%20Glacier%20Mapping%20using%20Deep%20Learning%20and%20Open%20Earth%20Observation%0A%20%20Data%20Matches%20the%20Accuracy%20of%20Manual%20Delineation%0AAuthor%3A%20Konstantin%20A.%20Maslov%20and%20Claudio%20Persello%20and%20Thomas%20Schellenberger%20and%20Alfred%20Stein%0AAbstract%3A%20%20%20Accurate%20global%20glacier%20mapping%20is%20critical%20for%20understanding%20climate%20change%0Aimpacts.%20Despite%20its%20importance%2C%20automated%20glacier%20mapping%20at%20a%20global%20scale%0Aremains%20largely%20unexplored.%20Here%20we%20address%20this%20gap%20and%20propose%0AGlacier-VisionTransformer-U-Net%20%28GlaViTU%29%2C%20a%20convolutional-transformer%20deep%0Alearning%20model%2C%20and%20five%20strategies%20for%20multitemporal%20global-scale%20glacier%0Amapping%20using%20open%20satellite%20imagery.%20Assessing%20the%20spatial%2C%20temporal%20and%0Across-sensor%20generalisation%20shows%20that%20our%20best%20strategy%20achieves%20intersection%0Aover%20union%20%3E0.85%20on%20previously%20unobserved%20images%20in%20most%20cases%2C%20which%20drops%20to%0A%3E0.75%20for%20debris-rich%20areas%20such%20as%20High-Mountain%20Asia%20and%20increases%20to%20%3E0.90%0Afor%20regions%20dominated%20by%20clean%20ice.%20A%20comparative%20validation%20against%20human%0Aexpert%20uncertainties%20in%20terms%20of%20area%20and%20distance%20deviations%20underscores%0AGlaViTU%20performance%2C%20approaching%20or%20matching%20expert-level%20delineation.%20Adding%0Asynthetic%20aperture%20radar%20data%2C%20namely%2C%20backscatter%20and%20interferometric%0Acoherence%2C%20increases%20the%20accuracy%20in%20all%20regions%20where%20available.%20The%0Acalibrated%20confidence%20for%20glacier%20extents%20is%20reported%20making%20the%20predictions%0Amore%20reliable%20and%20interpretable.%20We%20also%20release%20a%20benchmark%20dataset%20that%0Acovers%209%25%20of%20glaciers%20worldwide.%20Our%20results%20support%20efforts%20towards%20automated%0Amultitemporal%20and%20global%20glacier%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15113v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Glacier%2520Mapping%2520using%2520Deep%2520Learning%2520and%2520Open%2520Earth%2520Observation%250A%2520%2520Data%2520Matches%2520the%2520Accuracy%2520of%2520Manual%2520Delineation%26entry.906535625%3DKonstantin%2520A.%2520Maslov%2520and%2520Claudio%2520Persello%2520and%2520Thomas%2520Schellenberger%2520and%2520Alfred%2520Stein%26entry.1292438233%3D%2520%2520Accurate%2520global%2520glacier%2520mapping%2520is%2520critical%2520for%2520understanding%2520climate%2520change%250Aimpacts.%2520Despite%2520its%2520importance%252C%2520automated%2520glacier%2520mapping%2520at%2520a%2520global%2520scale%250Aremains%2520largely%2520unexplored.%2520Here%2520we%2520address%2520this%2520gap%2520and%2520propose%250AGlacier-VisionTransformer-U-Net%2520%2528GlaViTU%2529%252C%2520a%2520convolutional-transformer%2520deep%250Alearning%2520model%252C%2520and%2520five%2520strategies%2520for%2520multitemporal%2520global-scale%2520glacier%250Amapping%2520using%2520open%2520satellite%2520imagery.%2520Assessing%2520the%2520spatial%252C%2520temporal%2520and%250Across-sensor%2520generalisation%2520shows%2520that%2520our%2520best%2520strategy%2520achieves%2520intersection%250Aover%2520union%2520%253E0.85%2520on%2520previously%2520unobserved%2520images%2520in%2520most%2520cases%252C%2520which%2520drops%2520to%250A%253E0.75%2520for%2520debris-rich%2520areas%2520such%2520as%2520High-Mountain%2520Asia%2520and%2520increases%2520to%2520%253E0.90%250Afor%2520regions%2520dominated%2520by%2520clean%2520ice.%2520A%2520comparative%2520validation%2520against%2520human%250Aexpert%2520uncertainties%2520in%2520terms%2520of%2520area%2520and%2520distance%2520deviations%2520underscores%250AGlaViTU%2520performance%252C%2520approaching%2520or%2520matching%2520expert-level%2520delineation.%2520Adding%250Asynthetic%2520aperture%2520radar%2520data%252C%2520namely%252C%2520backscatter%2520and%2520interferometric%250Acoherence%252C%2520increases%2520the%2520accuracy%2520in%2520all%2520regions%2520where%2520available.%2520The%250Acalibrated%2520confidence%2520for%2520glacier%2520extents%2520is%2520reported%2520making%2520the%2520predictions%250Amore%2520reliable%2520and%2520interpretable.%2520We%2520also%2520release%2520a%2520benchmark%2520dataset%2520that%250Acovers%25209%2525%2520of%2520glaciers%2520worldwide.%2520Our%2520results%2520support%2520efforts%2520towards%2520automated%250Amultitemporal%2520and%2520global%2520glacier%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15113v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Glacier%20Mapping%20using%20Deep%20Learning%20and%20Open%20Earth%20Observation%0A%20%20Data%20Matches%20the%20Accuracy%20of%20Manual%20Delineation&entry.906535625=Konstantin%20A.%20Maslov%20and%20Claudio%20Persello%20and%20Thomas%20Schellenberger%20and%20Alfred%20Stein&entry.1292438233=%20%20Accurate%20global%20glacier%20mapping%20is%20critical%20for%20understanding%20climate%20change%0Aimpacts.%20Despite%20its%20importance%2C%20automated%20glacier%20mapping%20at%20a%20global%20scale%0Aremains%20largely%20unexplored.%20Here%20we%20address%20this%20gap%20and%20propose%0AGlacier-VisionTransformer-U-Net%20%28GlaViTU%29%2C%20a%20convolutional-transformer%20deep%0Alearning%20model%2C%20and%20five%20strategies%20for%20multitemporal%20global-scale%20glacier%0Amapping%20using%20open%20satellite%20imagery.%20Assessing%20the%20spatial%2C%20temporal%20and%0Across-sensor%20generalisation%20shows%20that%20our%20best%20strategy%20achieves%20intersection%0Aover%20union%20%3E0.85%20on%20previously%20unobserved%20images%20in%20most%20cases%2C%20which%20drops%20to%0A%3E0.75%20for%20debris-rich%20areas%20such%20as%20High-Mountain%20Asia%20and%20increases%20to%20%3E0.90%0Afor%20regions%20dominated%20by%20clean%20ice.%20A%20comparative%20validation%20against%20human%0Aexpert%20uncertainties%20in%20terms%20of%20area%20and%20distance%20deviations%20underscores%0AGlaViTU%20performance%2C%20approaching%20or%20matching%20expert-level%20delineation.%20Adding%0Asynthetic%20aperture%20radar%20data%2C%20namely%2C%20backscatter%20and%20interferometric%0Acoherence%2C%20increases%20the%20accuracy%20in%20all%20regions%20where%20available.%20The%0Acalibrated%20confidence%20for%20glacier%20extents%20is%20reported%20making%20the%20predictions%0Amore%20reliable%20and%20interpretable.%20We%20also%20release%20a%20benchmark%20dataset%20that%0Acovers%209%25%20of%20glaciers%20worldwide.%20Our%20results%20support%20efforts%20towards%20automated%0Amultitemporal%20and%20global%20glacier%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15113v3&entry.124074799=Read"},
{"title": "CONDA: Condensed Deep Association Learning for Co-Salient Object\n  Detection", "author": "Long Li and Nian Liu and Dingwen Zhang and Zhongyu Li and Salman Khan and Rao Anwer and Hisham Cholakkal and Junwei Han and Fahad Shahbaz Khan", "abstract": "  Inter-image association modeling is crucial for co-salient object detection.\nDespite satisfactory performance, previous methods still have limitations on\nsufficient inter-image association modeling. Because most of them focus on\nimage feature optimization under the guidance of heuristically calculated raw\ninter-image associations. They directly rely on raw associations which are not\nreliable in complex scenarios, and their image feature optimization approach is\nnot explicit for inter-image association modeling. To alleviate these\nlimitations, this paper proposes a deep association learning strategy that\ndeploys deep networks on raw associations to explicitly transform them into\ndeep association features. Specifically, we first create hyperassociations to\ncollect dense pixel-pair-wise raw associations and then deploys deep\naggregation networks on them. We design a progressive association generation\nmodule for this purpose with additional enhancement of the hyperassociation\ncalculation. More importantly, we propose a correspondence-induced association\ncondensation module that introduces a pretext task, i.e. semantic\ncorrespondence estimation, to condense the hyperassociations for computational\nburden reduction and noise elimination. We also design an object-aware cycle\nconsistency loss for high-quality correspondence estimations. Experimental\nresults in three benchmark datasets demonstrate the remarkable effectiveness of\nour proposed method with various training settings.\n", "link": "http://arxiv.org/abs/2409.01021v2", "date": "2024-09-04", "relevancy": 2.1791, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5479}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5463}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CONDA%3A%20Condensed%20Deep%20Association%20Learning%20for%20Co-Salient%20Object%0A%20%20Detection&body=Title%3A%20CONDA%3A%20Condensed%20Deep%20Association%20Learning%20for%20Co-Salient%20Object%0A%20%20Detection%0AAuthor%3A%20Long%20Li%20and%20Nian%20Liu%20and%20Dingwen%20Zhang%20and%20Zhongyu%20Li%20and%20Salman%20Khan%20and%20Rao%20Anwer%20and%20Hisham%20Cholakkal%20and%20Junwei%20Han%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Inter-image%20association%20modeling%20is%20crucial%20for%20co-salient%20object%20detection.%0ADespite%20satisfactory%20performance%2C%20previous%20methods%20still%20have%20limitations%20on%0Asufficient%20inter-image%20association%20modeling.%20Because%20most%20of%20them%20focus%20on%0Aimage%20feature%20optimization%20under%20the%20guidance%20of%20heuristically%20calculated%20raw%0Ainter-image%20associations.%20They%20directly%20rely%20on%20raw%20associations%20which%20are%20not%0Areliable%20in%20complex%20scenarios%2C%20and%20their%20image%20feature%20optimization%20approach%20is%0Anot%20explicit%20for%20inter-image%20association%20modeling.%20To%20alleviate%20these%0Alimitations%2C%20this%20paper%20proposes%20a%20deep%20association%20learning%20strategy%20that%0Adeploys%20deep%20networks%20on%20raw%20associations%20to%20explicitly%20transform%20them%20into%0Adeep%20association%20features.%20Specifically%2C%20we%20first%20create%20hyperassociations%20to%0Acollect%20dense%20pixel-pair-wise%20raw%20associations%20and%20then%20deploys%20deep%0Aaggregation%20networks%20on%20them.%20We%20design%20a%20progressive%20association%20generation%0Amodule%20for%20this%20purpose%20with%20additional%20enhancement%20of%20the%20hyperassociation%0Acalculation.%20More%20importantly%2C%20we%20propose%20a%20correspondence-induced%20association%0Acondensation%20module%20that%20introduces%20a%20pretext%20task%2C%20i.e.%20semantic%0Acorrespondence%20estimation%2C%20to%20condense%20the%20hyperassociations%20for%20computational%0Aburden%20reduction%20and%20noise%20elimination.%20We%20also%20design%20an%20object-aware%20cycle%0Aconsistency%20loss%20for%20high-quality%20correspondence%20estimations.%20Experimental%0Aresults%20in%20three%20benchmark%20datasets%20demonstrate%20the%20remarkable%20effectiveness%20of%0Aour%20proposed%20method%20with%20various%20training%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCONDA%253A%2520Condensed%2520Deep%2520Association%2520Learning%2520for%2520Co-Salient%2520Object%250A%2520%2520Detection%26entry.906535625%3DLong%2520Li%2520and%2520Nian%2520Liu%2520and%2520Dingwen%2520Zhang%2520and%2520Zhongyu%2520Li%2520and%2520Salman%2520Khan%2520and%2520Rao%2520Anwer%2520and%2520Hisham%2520Cholakkal%2520and%2520Junwei%2520Han%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520Inter-image%2520association%2520modeling%2520is%2520crucial%2520for%2520co-salient%2520object%2520detection.%250ADespite%2520satisfactory%2520performance%252C%2520previous%2520methods%2520still%2520have%2520limitations%2520on%250Asufficient%2520inter-image%2520association%2520modeling.%2520Because%2520most%2520of%2520them%2520focus%2520on%250Aimage%2520feature%2520optimization%2520under%2520the%2520guidance%2520of%2520heuristically%2520calculated%2520raw%250Ainter-image%2520associations.%2520They%2520directly%2520rely%2520on%2520raw%2520associations%2520which%2520are%2520not%250Areliable%2520in%2520complex%2520scenarios%252C%2520and%2520their%2520image%2520feature%2520optimization%2520approach%2520is%250Anot%2520explicit%2520for%2520inter-image%2520association%2520modeling.%2520To%2520alleviate%2520these%250Alimitations%252C%2520this%2520paper%2520proposes%2520a%2520deep%2520association%2520learning%2520strategy%2520that%250Adeploys%2520deep%2520networks%2520on%2520raw%2520associations%2520to%2520explicitly%2520transform%2520them%2520into%250Adeep%2520association%2520features.%2520Specifically%252C%2520we%2520first%2520create%2520hyperassociations%2520to%250Acollect%2520dense%2520pixel-pair-wise%2520raw%2520associations%2520and%2520then%2520deploys%2520deep%250Aaggregation%2520networks%2520on%2520them.%2520We%2520design%2520a%2520progressive%2520association%2520generation%250Amodule%2520for%2520this%2520purpose%2520with%2520additional%2520enhancement%2520of%2520the%2520hyperassociation%250Acalculation.%2520More%2520importantly%252C%2520we%2520propose%2520a%2520correspondence-induced%2520association%250Acondensation%2520module%2520that%2520introduces%2520a%2520pretext%2520task%252C%2520i.e.%2520semantic%250Acorrespondence%2520estimation%252C%2520to%2520condense%2520the%2520hyperassociations%2520for%2520computational%250Aburden%2520reduction%2520and%2520noise%2520elimination.%2520We%2520also%2520design%2520an%2520object-aware%2520cycle%250Aconsistency%2520loss%2520for%2520high-quality%2520correspondence%2520estimations.%2520Experimental%250Aresults%2520in%2520three%2520benchmark%2520datasets%2520demonstrate%2520the%2520remarkable%2520effectiveness%2520of%250Aour%2520proposed%2520method%2520with%2520various%2520training%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CONDA%3A%20Condensed%20Deep%20Association%20Learning%20for%20Co-Salient%20Object%0A%20%20Detection&entry.906535625=Long%20Li%20and%20Nian%20Liu%20and%20Dingwen%20Zhang%20and%20Zhongyu%20Li%20and%20Salman%20Khan%20and%20Rao%20Anwer%20and%20Hisham%20Cholakkal%20and%20Junwei%20Han%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Inter-image%20association%20modeling%20is%20crucial%20for%20co-salient%20object%20detection.%0ADespite%20satisfactory%20performance%2C%20previous%20methods%20still%20have%20limitations%20on%0Asufficient%20inter-image%20association%20modeling.%20Because%20most%20of%20them%20focus%20on%0Aimage%20feature%20optimization%20under%20the%20guidance%20of%20heuristically%20calculated%20raw%0Ainter-image%20associations.%20They%20directly%20rely%20on%20raw%20associations%20which%20are%20not%0Areliable%20in%20complex%20scenarios%2C%20and%20their%20image%20feature%20optimization%20approach%20is%0Anot%20explicit%20for%20inter-image%20association%20modeling.%20To%20alleviate%20these%0Alimitations%2C%20this%20paper%20proposes%20a%20deep%20association%20learning%20strategy%20that%0Adeploys%20deep%20networks%20on%20raw%20associations%20to%20explicitly%20transform%20them%20into%0Adeep%20association%20features.%20Specifically%2C%20we%20first%20create%20hyperassociations%20to%0Acollect%20dense%20pixel-pair-wise%20raw%20associations%20and%20then%20deploys%20deep%0Aaggregation%20networks%20on%20them.%20We%20design%20a%20progressive%20association%20generation%0Amodule%20for%20this%20purpose%20with%20additional%20enhancement%20of%20the%20hyperassociation%0Acalculation.%20More%20importantly%2C%20we%20propose%20a%20correspondence-induced%20association%0Acondensation%20module%20that%20introduces%20a%20pretext%20task%2C%20i.e.%20semantic%0Acorrespondence%20estimation%2C%20to%20condense%20the%20hyperassociations%20for%20computational%0Aburden%20reduction%20and%20noise%20elimination.%20We%20also%20design%20an%20object-aware%20cycle%0Aconsistency%20loss%20for%20high-quality%20correspondence%20estimations.%20Experimental%0Aresults%20in%20three%20benchmark%20datasets%20demonstrate%20the%20remarkable%20effectiveness%20of%0Aour%20proposed%20method%20with%20various%20training%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01021v2&entry.124074799=Read"},
{"title": "CSGO: Content-Style Composition in Text-to-Image Generation", "author": "Peng Xing and Haofan Wang and Yanpeng Sun and Qixun Wang and Xu Bai and Hao Ai and Renyuan Huang and Zechao Li", "abstract": "  The diffusion model has shown exceptional capabilities in controlled image\ngeneration, which has further fueled interest in image style transfer. Existing\nworks mainly focus on training free-based methods (e.g., image inversion) due\nto the scarcity of specific data. In this study, we present a data construction\npipeline for content-style-stylized image triplets that generates and\nautomatically cleanses stylized data triplets. Based on this pipeline, we\nconstruct a dataset IMAGStyle, the first large-scale style transfer dataset\ncontaining 210k image triplets, available for the community to explore and\nresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer model\nbased on end-to-end training, which explicitly decouples content and style\nfeatures employing independent feature injection. The unified CSGO implements\nimage-driven style transfer, text-driven stylized synthesis, and text\nediting-driven stylized synthesis. Extensive experiments demonstrate the\neffectiveness of our approach in enhancing style control capabilities in image\ngeneration. Additional visualization and access to the source code can be\nlocated on the project page: \\url{https://csgo-gen.github.io/}.\n", "link": "http://arxiv.org/abs/2408.16766v2", "date": "2024-09-04", "relevancy": 2.1612, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5532}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5342}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSGO%3A%20Content-Style%20Composition%20in%20Text-to-Image%20Generation&body=Title%3A%20CSGO%3A%20Content-Style%20Composition%20in%20Text-to-Image%20Generation%0AAuthor%3A%20Peng%20Xing%20and%20Haofan%20Wang%20and%20Yanpeng%20Sun%20and%20Qixun%20Wang%20and%20Xu%20Bai%20and%20Hao%20Ai%20and%20Renyuan%20Huang%20and%20Zechao%20Li%0AAbstract%3A%20%20%20The%20diffusion%20model%20has%20shown%20exceptional%20capabilities%20in%20controlled%20image%0Ageneration%2C%20which%20has%20further%20fueled%20interest%20in%20image%20style%20transfer.%20Existing%0Aworks%20mainly%20focus%20on%20training%20free-based%20methods%20%28e.g.%2C%20image%20inversion%29%20due%0Ato%20the%20scarcity%20of%20specific%20data.%20In%20this%20study%2C%20we%20present%20a%20data%20construction%0Apipeline%20for%20content-style-stylized%20image%20triplets%20that%20generates%20and%0Aautomatically%20cleanses%20stylized%20data%20triplets.%20Based%20on%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20IMAGStyle%2C%20the%20first%20large-scale%20style%20transfer%20dataset%0Acontaining%20210k%20image%20triplets%2C%20available%20for%20the%20community%20to%20explore%20and%0Aresearch.%20Equipped%20with%20IMAGStyle%2C%20we%20propose%20CSGO%2C%20a%20style%20transfer%20model%0Abased%20on%20end-to-end%20training%2C%20which%20explicitly%20decouples%20content%20and%20style%0Afeatures%20employing%20independent%20feature%20injection.%20The%20unified%20CSGO%20implements%0Aimage-driven%20style%20transfer%2C%20text-driven%20stylized%20synthesis%2C%20and%20text%0Aediting-driven%20stylized%20synthesis.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20enhancing%20style%20control%20capabilities%20in%20image%0Ageneration.%20Additional%20visualization%20and%20access%20to%20the%20source%20code%20can%20be%0Alocated%20on%20the%20project%20page%3A%20%5Curl%7Bhttps%3A//csgo-gen.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSGO%253A%2520Content-Style%2520Composition%2520in%2520Text-to-Image%2520Generation%26entry.906535625%3DPeng%2520Xing%2520and%2520Haofan%2520Wang%2520and%2520Yanpeng%2520Sun%2520and%2520Qixun%2520Wang%2520and%2520Xu%2520Bai%2520and%2520Hao%2520Ai%2520and%2520Renyuan%2520Huang%2520and%2520Zechao%2520Li%26entry.1292438233%3D%2520%2520The%2520diffusion%2520model%2520has%2520shown%2520exceptional%2520capabilities%2520in%2520controlled%2520image%250Ageneration%252C%2520which%2520has%2520further%2520fueled%2520interest%2520in%2520image%2520style%2520transfer.%2520Existing%250Aworks%2520mainly%2520focus%2520on%2520training%2520free-based%2520methods%2520%2528e.g.%252C%2520image%2520inversion%2529%2520due%250Ato%2520the%2520scarcity%2520of%2520specific%2520data.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520data%2520construction%250Apipeline%2520for%2520content-style-stylized%2520image%2520triplets%2520that%2520generates%2520and%250Aautomatically%2520cleanses%2520stylized%2520data%2520triplets.%2520Based%2520on%2520this%2520pipeline%252C%2520we%250Aconstruct%2520a%2520dataset%2520IMAGStyle%252C%2520the%2520first%2520large-scale%2520style%2520transfer%2520dataset%250Acontaining%2520210k%2520image%2520triplets%252C%2520available%2520for%2520the%2520community%2520to%2520explore%2520and%250Aresearch.%2520Equipped%2520with%2520IMAGStyle%252C%2520we%2520propose%2520CSGO%252C%2520a%2520style%2520transfer%2520model%250Abased%2520on%2520end-to-end%2520training%252C%2520which%2520explicitly%2520decouples%2520content%2520and%2520style%250Afeatures%2520employing%2520independent%2520feature%2520injection.%2520The%2520unified%2520CSGO%2520implements%250Aimage-driven%2520style%2520transfer%252C%2520text-driven%2520stylized%2520synthesis%252C%2520and%2520text%250Aediting-driven%2520stylized%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520enhancing%2520style%2520control%2520capabilities%2520in%2520image%250Ageneration.%2520Additional%2520visualization%2520and%2520access%2520to%2520the%2520source%2520code%2520can%2520be%250Alocated%2520on%2520the%2520project%2520page%253A%2520%255Curl%257Bhttps%253A//csgo-gen.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSGO%3A%20Content-Style%20Composition%20in%20Text-to-Image%20Generation&entry.906535625=Peng%20Xing%20and%20Haofan%20Wang%20and%20Yanpeng%20Sun%20and%20Qixun%20Wang%20and%20Xu%20Bai%20and%20Hao%20Ai%20and%20Renyuan%20Huang%20and%20Zechao%20Li&entry.1292438233=%20%20The%20diffusion%20model%20has%20shown%20exceptional%20capabilities%20in%20controlled%20image%0Ageneration%2C%20which%20has%20further%20fueled%20interest%20in%20image%20style%20transfer.%20Existing%0Aworks%20mainly%20focus%20on%20training%20free-based%20methods%20%28e.g.%2C%20image%20inversion%29%20due%0Ato%20the%20scarcity%20of%20specific%20data.%20In%20this%20study%2C%20we%20present%20a%20data%20construction%0Apipeline%20for%20content-style-stylized%20image%20triplets%20that%20generates%20and%0Aautomatically%20cleanses%20stylized%20data%20triplets.%20Based%20on%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20IMAGStyle%2C%20the%20first%20large-scale%20style%20transfer%20dataset%0Acontaining%20210k%20image%20triplets%2C%20available%20for%20the%20community%20to%20explore%20and%0Aresearch.%20Equipped%20with%20IMAGStyle%2C%20we%20propose%20CSGO%2C%20a%20style%20transfer%20model%0Abased%20on%20end-to-end%20training%2C%20which%20explicitly%20decouples%20content%20and%20style%0Afeatures%20employing%20independent%20feature%20injection.%20The%20unified%20CSGO%20implements%0Aimage-driven%20style%20transfer%2C%20text-driven%20stylized%20synthesis%2C%20and%20text%0Aediting-driven%20stylized%20synthesis.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20enhancing%20style%20control%20capabilities%20in%20image%0Ageneration.%20Additional%20visualization%20and%20access%20to%20the%20source%20code%20can%20be%0Alocated%20on%20the%20project%20page%3A%20%5Curl%7Bhttps%3A//csgo-gen.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16766v2&entry.124074799=Read"},
{"title": "An Analysis of Linear Complexity Attention Substitutes with BEST-RQ", "author": "Ryan Whetten and Titouan Parcollet and Adel Moumen and Marco Dinarelli and Yannick Est\u00e8ve", "abstract": "  Self-Supervised Learning (SSL) has proven to be effective in various domains,\nincluding speech processing. However, SSL is computationally and memory\nexpensive. This is in part due the quadratic complexity of multi-head\nself-attention (MHSA). Alternatives for MHSA have been proposed and used in the\nspeech domain, but have yet to be investigated properly in an SSL setting. In\nthis work, we study the effects of replacing MHSA with recent state-of-the-art\nalternatives that have linear complexity, namely, HyperMixing, Fastformer,\nSummaryMixing, and Mamba. We evaluate these methods by looking at the speed,\nthe amount of VRAM consumed, and the performance on the SSL MP3S benchmark.\nResults show that these linear alternatives maintain competitive performance\ncompared to MHSA while, on average, decreasing VRAM consumption by around 20%\nto 60% and increasing speed from 7% to 65% for input sequences ranging from 20\nto 80 seconds.\n", "link": "http://arxiv.org/abs/2409.02596v1", "date": "2024-09-04", "relevancy": 2.1603, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4388}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.435}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Linear%20Complexity%20Attention%20Substitutes%20with%20BEST-RQ&body=Title%3A%20An%20Analysis%20of%20Linear%20Complexity%20Attention%20Substitutes%20with%20BEST-RQ%0AAuthor%3A%20Ryan%20Whetten%20and%20Titouan%20Parcollet%20and%20Adel%20Moumen%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20proven%20to%20be%20effective%20in%20various%20domains%2C%0Aincluding%20speech%20processing.%20However%2C%20SSL%20is%20computationally%20and%20memory%0Aexpensive.%20This%20is%20in%20part%20due%20the%20quadratic%20complexity%20of%20multi-head%0Aself-attention%20%28MHSA%29.%20Alternatives%20for%20MHSA%20have%20been%20proposed%20and%20used%20in%20the%0Aspeech%20domain%2C%20but%20have%20yet%20to%20be%20investigated%20properly%20in%20an%20SSL%20setting.%20In%0Athis%20work%2C%20we%20study%20the%20effects%20of%20replacing%20MHSA%20with%20recent%20state-of-the-art%0Aalternatives%20that%20have%20linear%20complexity%2C%20namely%2C%20HyperMixing%2C%20Fastformer%2C%0ASummaryMixing%2C%20and%20Mamba.%20We%20evaluate%20these%20methods%20by%20looking%20at%20the%20speed%2C%0Athe%20amount%20of%20VRAM%20consumed%2C%20and%20the%20performance%20on%20the%20SSL%20MP3S%20benchmark.%0AResults%20show%20that%20these%20linear%20alternatives%20maintain%20competitive%20performance%0Acompared%20to%20MHSA%20while%2C%20on%20average%2C%20decreasing%20VRAM%20consumption%20by%20around%2020%25%0Ato%2060%25%20and%20increasing%20speed%20from%207%25%20to%2065%25%20for%20input%20sequences%20ranging%20from%2020%0Ato%2080%20seconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Analysis%2520of%2520Linear%2520Complexity%2520Attention%2520Substitutes%2520with%2520BEST-RQ%26entry.906535625%3DRyan%2520Whetten%2520and%2520Titouan%2520Parcollet%2520and%2520Adel%2520Moumen%2520and%2520Marco%2520Dinarelli%2520and%2520Yannick%2520Est%25C3%25A8ve%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520proven%2520to%2520be%2520effective%2520in%2520various%2520domains%252C%250Aincluding%2520speech%2520processing.%2520However%252C%2520SSL%2520is%2520computationally%2520and%2520memory%250Aexpensive.%2520This%2520is%2520in%2520part%2520due%2520the%2520quadratic%2520complexity%2520of%2520multi-head%250Aself-attention%2520%2528MHSA%2529.%2520Alternatives%2520for%2520MHSA%2520have%2520been%2520proposed%2520and%2520used%2520in%2520the%250Aspeech%2520domain%252C%2520but%2520have%2520yet%2520to%2520be%2520investigated%2520properly%2520in%2520an%2520SSL%2520setting.%2520In%250Athis%2520work%252C%2520we%2520study%2520the%2520effects%2520of%2520replacing%2520MHSA%2520with%2520recent%2520state-of-the-art%250Aalternatives%2520that%2520have%2520linear%2520complexity%252C%2520namely%252C%2520HyperMixing%252C%2520Fastformer%252C%250ASummaryMixing%252C%2520and%2520Mamba.%2520We%2520evaluate%2520these%2520methods%2520by%2520looking%2520at%2520the%2520speed%252C%250Athe%2520amount%2520of%2520VRAM%2520consumed%252C%2520and%2520the%2520performance%2520on%2520the%2520SSL%2520MP3S%2520benchmark.%250AResults%2520show%2520that%2520these%2520linear%2520alternatives%2520maintain%2520competitive%2520performance%250Acompared%2520to%2520MHSA%2520while%252C%2520on%2520average%252C%2520decreasing%2520VRAM%2520consumption%2520by%2520around%252020%2525%250Ato%252060%2525%2520and%2520increasing%2520speed%2520from%25207%2525%2520to%252065%2525%2520for%2520input%2520sequences%2520ranging%2520from%252020%250Ato%252080%2520seconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Linear%20Complexity%20Attention%20Substitutes%20with%20BEST-RQ&entry.906535625=Ryan%20Whetten%20and%20Titouan%20Parcollet%20and%20Adel%20Moumen%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20proven%20to%20be%20effective%20in%20various%20domains%2C%0Aincluding%20speech%20processing.%20However%2C%20SSL%20is%20computationally%20and%20memory%0Aexpensive.%20This%20is%20in%20part%20due%20the%20quadratic%20complexity%20of%20multi-head%0Aself-attention%20%28MHSA%29.%20Alternatives%20for%20MHSA%20have%20been%20proposed%20and%20used%20in%20the%0Aspeech%20domain%2C%20but%20have%20yet%20to%20be%20investigated%20properly%20in%20an%20SSL%20setting.%20In%0Athis%20work%2C%20we%20study%20the%20effects%20of%20replacing%20MHSA%20with%20recent%20state-of-the-art%0Aalternatives%20that%20have%20linear%20complexity%2C%20namely%2C%20HyperMixing%2C%20Fastformer%2C%0ASummaryMixing%2C%20and%20Mamba.%20We%20evaluate%20these%20methods%20by%20looking%20at%20the%20speed%2C%0Athe%20amount%20of%20VRAM%20consumed%2C%20and%20the%20performance%20on%20the%20SSL%20MP3S%20benchmark.%0AResults%20show%20that%20these%20linear%20alternatives%20maintain%20competitive%20performance%0Acompared%20to%20MHSA%20while%2C%20on%20average%2C%20decreasing%20VRAM%20consumption%20by%20around%2020%25%0Ato%2060%25%20and%20increasing%20speed%20from%207%25%20to%2065%25%20for%20input%20sequences%20ranging%20from%2020%0Ato%2080%20seconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02596v1&entry.124074799=Read"},
{"title": "Few-shot Multi-Task Learning of Linear Invariant Features with Meta\n  Subspace Pursuit", "author": "Chaozhi Zhang and Lin Liu and Xiaoqun Zhang", "abstract": "  Data scarcity poses a serious threat to modern machine learning and\nartificial intelligence, as their practical success typically relies on the\navailability of big datasets. One effective strategy to mitigate the issue of\ninsufficient data is to first harness information from other data sources\npossessing certain similarities in the study design stage, and then employ the\nmulti-task or meta learning framework in the analysis stage. In this paper, we\nfocus on multi-task (or multi-source) linear models whose coefficients across\ntasks share an invariant low-rank component, a popular structural assumption\nconsidered in the recent multi-task or meta learning literature. Under this\nassumption, we propose a new algorithm, called Meta Subspace Pursuit\n(abbreviated as Meta-SP), that provably learns this invariant subspace shared\nby different tasks. Under this stylized setup for multi-task or meta learning,\nwe establish both the algorithmic and statistical guarantees of the proposed\nmethod. Extensive numerical experiments are conducted, comparing Meta-SP\nagainst several competing methods, including popular, off-the-shelf\nmodel-agnostic meta learning algorithms such as ANIL. These experiments\ndemonstrate that Meta-SP achieves superior performance over the competing\nmethods in various aspects.\n", "link": "http://arxiv.org/abs/2409.02708v1", "date": "2024-09-04", "relevancy": 2.151, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20Multi-Task%20Learning%20of%20Linear%20Invariant%20Features%20with%20Meta%0A%20%20Subspace%20Pursuit&body=Title%3A%20Few-shot%20Multi-Task%20Learning%20of%20Linear%20Invariant%20Features%20with%20Meta%0A%20%20Subspace%20Pursuit%0AAuthor%3A%20Chaozhi%20Zhang%20and%20Lin%20Liu%20and%20Xiaoqun%20Zhang%0AAbstract%3A%20%20%20Data%20scarcity%20poses%20a%20serious%20threat%20to%20modern%20machine%20learning%20and%0Aartificial%20intelligence%2C%20as%20their%20practical%20success%20typically%20relies%20on%20the%0Aavailability%20of%20big%20datasets.%20One%20effective%20strategy%20to%20mitigate%20the%20issue%20of%0Ainsufficient%20data%20is%20to%20first%20harness%20information%20from%20other%20data%20sources%0Apossessing%20certain%20similarities%20in%20the%20study%20design%20stage%2C%20and%20then%20employ%20the%0Amulti-task%20or%20meta%20learning%20framework%20in%20the%20analysis%20stage.%20In%20this%20paper%2C%20we%0Afocus%20on%20multi-task%20%28or%20multi-source%29%20linear%20models%20whose%20coefficients%20across%0Atasks%20share%20an%20invariant%20low-rank%20component%2C%20a%20popular%20structural%20assumption%0Aconsidered%20in%20the%20recent%20multi-task%20or%20meta%20learning%20literature.%20Under%20this%0Aassumption%2C%20we%20propose%20a%20new%20algorithm%2C%20called%20Meta%20Subspace%20Pursuit%0A%28abbreviated%20as%20Meta-SP%29%2C%20that%20provably%20learns%20this%20invariant%20subspace%20shared%0Aby%20different%20tasks.%20Under%20this%20stylized%20setup%20for%20multi-task%20or%20meta%20learning%2C%0Awe%20establish%20both%20the%20algorithmic%20and%20statistical%20guarantees%20of%20the%20proposed%0Amethod.%20Extensive%20numerical%20experiments%20are%20conducted%2C%20comparing%20Meta-SP%0Aagainst%20several%20competing%20methods%2C%20including%20popular%2C%20off-the-shelf%0Amodel-agnostic%20meta%20learning%20algorithms%20such%20as%20ANIL.%20These%20experiments%0Ademonstrate%20that%20Meta-SP%20achieves%20superior%20performance%20over%20the%20competing%0Amethods%20in%20various%20aspects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520Multi-Task%2520Learning%2520of%2520Linear%2520Invariant%2520Features%2520with%2520Meta%250A%2520%2520Subspace%2520Pursuit%26entry.906535625%3DChaozhi%2520Zhang%2520and%2520Lin%2520Liu%2520and%2520Xiaoqun%2520Zhang%26entry.1292438233%3D%2520%2520Data%2520scarcity%2520poses%2520a%2520serious%2520threat%2520to%2520modern%2520machine%2520learning%2520and%250Aartificial%2520intelligence%252C%2520as%2520their%2520practical%2520success%2520typically%2520relies%2520on%2520the%250Aavailability%2520of%2520big%2520datasets.%2520One%2520effective%2520strategy%2520to%2520mitigate%2520the%2520issue%2520of%250Ainsufficient%2520data%2520is%2520to%2520first%2520harness%2520information%2520from%2520other%2520data%2520sources%250Apossessing%2520certain%2520similarities%2520in%2520the%2520study%2520design%2520stage%252C%2520and%2520then%2520employ%2520the%250Amulti-task%2520or%2520meta%2520learning%2520framework%2520in%2520the%2520analysis%2520stage.%2520In%2520this%2520paper%252C%2520we%250Afocus%2520on%2520multi-task%2520%2528or%2520multi-source%2529%2520linear%2520models%2520whose%2520coefficients%2520across%250Atasks%2520share%2520an%2520invariant%2520low-rank%2520component%252C%2520a%2520popular%2520structural%2520assumption%250Aconsidered%2520in%2520the%2520recent%2520multi-task%2520or%2520meta%2520learning%2520literature.%2520Under%2520this%250Aassumption%252C%2520we%2520propose%2520a%2520new%2520algorithm%252C%2520called%2520Meta%2520Subspace%2520Pursuit%250A%2528abbreviated%2520as%2520Meta-SP%2529%252C%2520that%2520provably%2520learns%2520this%2520invariant%2520subspace%2520shared%250Aby%2520different%2520tasks.%2520Under%2520this%2520stylized%2520setup%2520for%2520multi-task%2520or%2520meta%2520learning%252C%250Awe%2520establish%2520both%2520the%2520algorithmic%2520and%2520statistical%2520guarantees%2520of%2520the%2520proposed%250Amethod.%2520Extensive%2520numerical%2520experiments%2520are%2520conducted%252C%2520comparing%2520Meta-SP%250Aagainst%2520several%2520competing%2520methods%252C%2520including%2520popular%252C%2520off-the-shelf%250Amodel-agnostic%2520meta%2520learning%2520algorithms%2520such%2520as%2520ANIL.%2520These%2520experiments%250Ademonstrate%2520that%2520Meta-SP%2520achieves%2520superior%2520performance%2520over%2520the%2520competing%250Amethods%2520in%2520various%2520aspects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20Multi-Task%20Learning%20of%20Linear%20Invariant%20Features%20with%20Meta%0A%20%20Subspace%20Pursuit&entry.906535625=Chaozhi%20Zhang%20and%20Lin%20Liu%20and%20Xiaoqun%20Zhang&entry.1292438233=%20%20Data%20scarcity%20poses%20a%20serious%20threat%20to%20modern%20machine%20learning%20and%0Aartificial%20intelligence%2C%20as%20their%20practical%20success%20typically%20relies%20on%20the%0Aavailability%20of%20big%20datasets.%20One%20effective%20strategy%20to%20mitigate%20the%20issue%20of%0Ainsufficient%20data%20is%20to%20first%20harness%20information%20from%20other%20data%20sources%0Apossessing%20certain%20similarities%20in%20the%20study%20design%20stage%2C%20and%20then%20employ%20the%0Amulti-task%20or%20meta%20learning%20framework%20in%20the%20analysis%20stage.%20In%20this%20paper%2C%20we%0Afocus%20on%20multi-task%20%28or%20multi-source%29%20linear%20models%20whose%20coefficients%20across%0Atasks%20share%20an%20invariant%20low-rank%20component%2C%20a%20popular%20structural%20assumption%0Aconsidered%20in%20the%20recent%20multi-task%20or%20meta%20learning%20literature.%20Under%20this%0Aassumption%2C%20we%20propose%20a%20new%20algorithm%2C%20called%20Meta%20Subspace%20Pursuit%0A%28abbreviated%20as%20Meta-SP%29%2C%20that%20provably%20learns%20this%20invariant%20subspace%20shared%0Aby%20different%20tasks.%20Under%20this%20stylized%20setup%20for%20multi-task%20or%20meta%20learning%2C%0Awe%20establish%20both%20the%20algorithmic%20and%20statistical%20guarantees%20of%20the%20proposed%0Amethod.%20Extensive%20numerical%20experiments%20are%20conducted%2C%20comparing%20Meta-SP%0Aagainst%20several%20competing%20methods%2C%20including%20popular%2C%20off-the-shelf%0Amodel-agnostic%20meta%20learning%20algorithms%20such%20as%20ANIL.%20These%20experiments%0Ademonstrate%20that%20Meta-SP%20achieves%20superior%20performance%20over%20the%20competing%0Amethods%20in%20various%20aspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02708v1&entry.124074799=Read"},
{"title": "Evaluating Environments Using Exploratory Agents", "author": "Bobby Khaleque and Mike Cook and Jeremy Gow", "abstract": "  Exploration is a key part of many video games. We investigate the using an\nexploratory agent to provide feedback on the design of procedurally generated\ngame levels, 5 engaging levels and 5 unengaging levels. We expand upon a\nframework introduced in previous research which models motivations for\nexploration and introduce a fitness function for evaluating an environment's\npotential for exploration. Our study showed that our exploratory agent can\nclearly distinguish between engaging and unengaging levels. The findings\nsuggest that our agent has the potential to serve as an effective tool for\nassessing procedurally generated levels, in terms of exploration. This work\ncontributes to the growing field of AI-driven game design by offering new\ninsights into how game environments can be evaluated and optimised for player\nexploration.\n", "link": "http://arxiv.org/abs/2409.02632v1", "date": "2024-09-04", "relevancy": 2.1433, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6327}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5479}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Environments%20Using%20Exploratory%20Agents&body=Title%3A%20Evaluating%20Environments%20Using%20Exploratory%20Agents%0AAuthor%3A%20Bobby%20Khaleque%20and%20Mike%20Cook%20and%20Jeremy%20Gow%0AAbstract%3A%20%20%20Exploration%20is%20a%20key%20part%20of%20many%20video%20games.%20We%20investigate%20the%20using%20an%0Aexploratory%20agent%20to%20provide%20feedback%20on%20the%20design%20of%20procedurally%20generated%0Agame%20levels%2C%205%20engaging%20levels%20and%205%20unengaging%20levels.%20We%20expand%20upon%20a%0Aframework%20introduced%20in%20previous%20research%20which%20models%20motivations%20for%0Aexploration%20and%20introduce%20a%20fitness%20function%20for%20evaluating%20an%20environment%27s%0Apotential%20for%20exploration.%20Our%20study%20showed%20that%20our%20exploratory%20agent%20can%0Aclearly%20distinguish%20between%20engaging%20and%20unengaging%20levels.%20The%20findings%0Asuggest%20that%20our%20agent%20has%20the%20potential%20to%20serve%20as%20an%20effective%20tool%20for%0Aassessing%20procedurally%20generated%20levels%2C%20in%20terms%20of%20exploration.%20This%20work%0Acontributes%20to%20the%20growing%20field%20of%20AI-driven%20game%20design%20by%20offering%20new%0Ainsights%20into%20how%20game%20environments%20can%20be%20evaluated%20and%20optimised%20for%20player%0Aexploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Environments%2520Using%2520Exploratory%2520Agents%26entry.906535625%3DBobby%2520Khaleque%2520and%2520Mike%2520Cook%2520and%2520Jeremy%2520Gow%26entry.1292438233%3D%2520%2520Exploration%2520is%2520a%2520key%2520part%2520of%2520many%2520video%2520games.%2520We%2520investigate%2520the%2520using%2520an%250Aexploratory%2520agent%2520to%2520provide%2520feedback%2520on%2520the%2520design%2520of%2520procedurally%2520generated%250Agame%2520levels%252C%25205%2520engaging%2520levels%2520and%25205%2520unengaging%2520levels.%2520We%2520expand%2520upon%2520a%250Aframework%2520introduced%2520in%2520previous%2520research%2520which%2520models%2520motivations%2520for%250Aexploration%2520and%2520introduce%2520a%2520fitness%2520function%2520for%2520evaluating%2520an%2520environment%2527s%250Apotential%2520for%2520exploration.%2520Our%2520study%2520showed%2520that%2520our%2520exploratory%2520agent%2520can%250Aclearly%2520distinguish%2520between%2520engaging%2520and%2520unengaging%2520levels.%2520The%2520findings%250Asuggest%2520that%2520our%2520agent%2520has%2520the%2520potential%2520to%2520serve%2520as%2520an%2520effective%2520tool%2520for%250Aassessing%2520procedurally%2520generated%2520levels%252C%2520in%2520terms%2520of%2520exploration.%2520This%2520work%250Acontributes%2520to%2520the%2520growing%2520field%2520of%2520AI-driven%2520game%2520design%2520by%2520offering%2520new%250Ainsights%2520into%2520how%2520game%2520environments%2520can%2520be%2520evaluated%2520and%2520optimised%2520for%2520player%250Aexploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Environments%20Using%20Exploratory%20Agents&entry.906535625=Bobby%20Khaleque%20and%20Mike%20Cook%20and%20Jeremy%20Gow&entry.1292438233=%20%20Exploration%20is%20a%20key%20part%20of%20many%20video%20games.%20We%20investigate%20the%20using%20an%0Aexploratory%20agent%20to%20provide%20feedback%20on%20the%20design%20of%20procedurally%20generated%0Agame%20levels%2C%205%20engaging%20levels%20and%205%20unengaging%20levels.%20We%20expand%20upon%20a%0Aframework%20introduced%20in%20previous%20research%20which%20models%20motivations%20for%0Aexploration%20and%20introduce%20a%20fitness%20function%20for%20evaluating%20an%20environment%27s%0Apotential%20for%20exploration.%20Our%20study%20showed%20that%20our%20exploratory%20agent%20can%0Aclearly%20distinguish%20between%20engaging%20and%20unengaging%20levels.%20The%20findings%0Asuggest%20that%20our%20agent%20has%20the%20potential%20to%20serve%20as%20an%20effective%20tool%20for%0Aassessing%20procedurally%20generated%20levels%2C%20in%20terms%20of%20exploration.%20This%20work%0Acontributes%20to%20the%20growing%20field%20of%20AI-driven%20game%20design%20by%20offering%20new%0Ainsights%20into%20how%20game%20environments%20can%20be%20evaluated%20and%20optimised%20for%20player%0Aexploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02632v1&entry.124074799=Read"},
{"title": "CLDA: Collaborative Learning for Enhanced Unsupervised Domain Adaptation", "author": "Minhee Cho and Hyesong Choi and Hayeon Jo and Dongbo Min", "abstract": "  Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a\nmodel trained on a labeled source domain and its deployment in an unlabeled\ntarget domain. However, current high-performance models demand significant\nresources, resulting in prohibitive deployment costs and highlighting the need\nfor small yet effective models. For UDA of lightweight models, Knowledge\nDistillation (KD) in a Teacher-Student framework can be a common approach, but\nwe find that domain shift in UDA leads to a significant increase in non-salient\nparameters in the teacher model, degrading model's generalization ability and\ntransferring misleading information to the student model. Interestingly, we\nobserved that this phenomenon occurs considerably less in the student model.\nDriven by this insight, we introduce Collaborative Learning, a method that\nupdates the teacher's non-salient parameters using the student model and at the\nsame time enhance the student's performance using the updated teacher model.\nExperiments across various tasks and datasets show consistent performance\nimprovements for both student and teacher models. For example, in semantic\nsegmentation, CLDA achieves an improvement of +0.7% mIoU for teacher and +1.4%\nmIoU for student compared to the baseline model in the GTA to Cityscapes. In\nthe Synthia to Cityscapes, it achieves an improvement of +0.8% mIoU for teacher\nand +2.0% mIoU for student.\n", "link": "http://arxiv.org/abs/2409.02699v1", "date": "2024-09-04", "relevancy": 2.1297, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5643}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLDA%3A%20Collaborative%20Learning%20for%20Enhanced%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20CLDA%3A%20Collaborative%20Learning%20for%20Enhanced%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Minhee%20Cho%20and%20Hyesong%20Choi%20and%20Hayeon%20Jo%20and%20Dongbo%20Min%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20endeavors%20to%20bridge%20the%20gap%20between%20a%0Amodel%20trained%20on%20a%20labeled%20source%20domain%20and%20its%20deployment%20in%20an%20unlabeled%0Atarget%20domain.%20However%2C%20current%20high-performance%20models%20demand%20significant%0Aresources%2C%20resulting%20in%20prohibitive%20deployment%20costs%20and%20highlighting%20the%20need%0Afor%20small%20yet%20effective%20models.%20For%20UDA%20of%20lightweight%20models%2C%20Knowledge%0ADistillation%20%28KD%29%20in%20a%20Teacher-Student%20framework%20can%20be%20a%20common%20approach%2C%20but%0Awe%20find%20that%20domain%20shift%20in%20UDA%20leads%20to%20a%20significant%20increase%20in%20non-salient%0Aparameters%20in%20the%20teacher%20model%2C%20degrading%20model%27s%20generalization%20ability%20and%0Atransferring%20misleading%20information%20to%20the%20student%20model.%20Interestingly%2C%20we%0Aobserved%20that%20this%20phenomenon%20occurs%20considerably%20less%20in%20the%20student%20model.%0ADriven%20by%20this%20insight%2C%20we%20introduce%20Collaborative%20Learning%2C%20a%20method%20that%0Aupdates%20the%20teacher%27s%20non-salient%20parameters%20using%20the%20student%20model%20and%20at%20the%0Asame%20time%20enhance%20the%20student%27s%20performance%20using%20the%20updated%20teacher%20model.%0AExperiments%20across%20various%20tasks%20and%20datasets%20show%20consistent%20performance%0Aimprovements%20for%20both%20student%20and%20teacher%20models.%20For%20example%2C%20in%20semantic%0Asegmentation%2C%20CLDA%20achieves%20an%20improvement%20of%20%2B0.7%25%20mIoU%20for%20teacher%20and%20%2B1.4%25%0AmIoU%20for%20student%20compared%20to%20the%20baseline%20model%20in%20the%20GTA%20to%20Cityscapes.%20In%0Athe%20Synthia%20to%20Cityscapes%2C%20it%20achieves%20an%20improvement%20of%20%2B0.8%25%20mIoU%20for%20teacher%0Aand%20%2B2.0%25%20mIoU%20for%20student.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLDA%253A%2520Collaborative%2520Learning%2520for%2520Enhanced%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DMinhee%2520Cho%2520and%2520Hyesong%2520Choi%2520and%2520Hayeon%2520Jo%2520and%2520Dongbo%2520Min%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520endeavors%2520to%2520bridge%2520the%2520gap%2520between%2520a%250Amodel%2520trained%2520on%2520a%2520labeled%2520source%2520domain%2520and%2520its%2520deployment%2520in%2520an%2520unlabeled%250Atarget%2520domain.%2520However%252C%2520current%2520high-performance%2520models%2520demand%2520significant%250Aresources%252C%2520resulting%2520in%2520prohibitive%2520deployment%2520costs%2520and%2520highlighting%2520the%2520need%250Afor%2520small%2520yet%2520effective%2520models.%2520For%2520UDA%2520of%2520lightweight%2520models%252C%2520Knowledge%250ADistillation%2520%2528KD%2529%2520in%2520a%2520Teacher-Student%2520framework%2520can%2520be%2520a%2520common%2520approach%252C%2520but%250Awe%2520find%2520that%2520domain%2520shift%2520in%2520UDA%2520leads%2520to%2520a%2520significant%2520increase%2520in%2520non-salient%250Aparameters%2520in%2520the%2520teacher%2520model%252C%2520degrading%2520model%2527s%2520generalization%2520ability%2520and%250Atransferring%2520misleading%2520information%2520to%2520the%2520student%2520model.%2520Interestingly%252C%2520we%250Aobserved%2520that%2520this%2520phenomenon%2520occurs%2520considerably%2520less%2520in%2520the%2520student%2520model.%250ADriven%2520by%2520this%2520insight%252C%2520we%2520introduce%2520Collaborative%2520Learning%252C%2520a%2520method%2520that%250Aupdates%2520the%2520teacher%2527s%2520non-salient%2520parameters%2520using%2520the%2520student%2520model%2520and%2520at%2520the%250Asame%2520time%2520enhance%2520the%2520student%2527s%2520performance%2520using%2520the%2520updated%2520teacher%2520model.%250AExperiments%2520across%2520various%2520tasks%2520and%2520datasets%2520show%2520consistent%2520performance%250Aimprovements%2520for%2520both%2520student%2520and%2520teacher%2520models.%2520For%2520example%252C%2520in%2520semantic%250Asegmentation%252C%2520CLDA%2520achieves%2520an%2520improvement%2520of%2520%252B0.7%2525%2520mIoU%2520for%2520teacher%2520and%2520%252B1.4%2525%250AmIoU%2520for%2520student%2520compared%2520to%2520the%2520baseline%2520model%2520in%2520the%2520GTA%2520to%2520Cityscapes.%2520In%250Athe%2520Synthia%2520to%2520Cityscapes%252C%2520it%2520achieves%2520an%2520improvement%2520of%2520%252B0.8%2525%2520mIoU%2520for%2520teacher%250Aand%2520%252B2.0%2525%2520mIoU%2520for%2520student.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLDA%3A%20Collaborative%20Learning%20for%20Enhanced%20Unsupervised%20Domain%20Adaptation&entry.906535625=Minhee%20Cho%20and%20Hyesong%20Choi%20and%20Hayeon%20Jo%20and%20Dongbo%20Min&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20endeavors%20to%20bridge%20the%20gap%20between%20a%0Amodel%20trained%20on%20a%20labeled%20source%20domain%20and%20its%20deployment%20in%20an%20unlabeled%0Atarget%20domain.%20However%2C%20current%20high-performance%20models%20demand%20significant%0Aresources%2C%20resulting%20in%20prohibitive%20deployment%20costs%20and%20highlighting%20the%20need%0Afor%20small%20yet%20effective%20models.%20For%20UDA%20of%20lightweight%20models%2C%20Knowledge%0ADistillation%20%28KD%29%20in%20a%20Teacher-Student%20framework%20can%20be%20a%20common%20approach%2C%20but%0Awe%20find%20that%20domain%20shift%20in%20UDA%20leads%20to%20a%20significant%20increase%20in%20non-salient%0Aparameters%20in%20the%20teacher%20model%2C%20degrading%20model%27s%20generalization%20ability%20and%0Atransferring%20misleading%20information%20to%20the%20student%20model.%20Interestingly%2C%20we%0Aobserved%20that%20this%20phenomenon%20occurs%20considerably%20less%20in%20the%20student%20model.%0ADriven%20by%20this%20insight%2C%20we%20introduce%20Collaborative%20Learning%2C%20a%20method%20that%0Aupdates%20the%20teacher%27s%20non-salient%20parameters%20using%20the%20student%20model%20and%20at%20the%0Asame%20time%20enhance%20the%20student%27s%20performance%20using%20the%20updated%20teacher%20model.%0AExperiments%20across%20various%20tasks%20and%20datasets%20show%20consistent%20performance%0Aimprovements%20for%20both%20student%20and%20teacher%20models.%20For%20example%2C%20in%20semantic%0Asegmentation%2C%20CLDA%20achieves%20an%20improvement%20of%20%2B0.7%25%20mIoU%20for%20teacher%20and%20%2B1.4%25%0AmIoU%20for%20student%20compared%20to%20the%20baseline%20model%20in%20the%20GTA%20to%20Cityscapes.%20In%0Athe%20Synthia%20to%20Cityscapes%2C%20it%20achieves%20an%20improvement%20of%20%2B0.8%25%20mIoU%20for%20teacher%0Aand%20%2B2.0%25%20mIoU%20for%20student.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02699v1&entry.124074799=Read"},
{"title": "Independence Constrained Disentangled Representation Learning from\n  Epistemological Perspective", "author": "Ruoyu Wang and Lina Yao", "abstract": "  Disentangled Representation Learning aims to improve the explainability of\ndeep learning methods by training a data encoder that identifies semantically\nmeaningful latent variables in the data generation process. Nevertheless, there\nis no consensus regarding a universally accepted definition for the objective\nof disentangled representation learning. In particular, there is a considerable\namount of discourse regarding whether should the latent variables be mutually\nindependent or not. In this paper, we first investigate these arguments on the\ninterrelationships between latent variables by establishing a conceptual bridge\nbetween Epistemology and Disentangled Representation Learning. Then, inspired\nby these interdisciplinary concepts, we introduce a two-level latent space\nframework to provide a general solution to the prior arguments on this issue.\nFinally, we propose a novel method for disentangled representation learning by\nemploying an integration of mutual information constraint and independence\nconstraint within the Generative Adversarial Network (GAN) framework.\nExperimental results demonstrate that our proposed method consistently\noutperforms baseline approaches in both quantitative and qualitative\nevaluations. The method exhibits strong performance across multiple commonly\nused metrics and demonstrates a great capability in disentangling various\nsemantic factors, leading to an improved quality of controllable generation,\nwhich consequently benefits the explainability of the algorithm.\n", "link": "http://arxiv.org/abs/2409.02672v1", "date": "2024-09-04", "relevancy": 2.1262, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5463}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5236}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Independence%20Constrained%20Disentangled%20Representation%20Learning%20from%0A%20%20Epistemological%20Perspective&body=Title%3A%20Independence%20Constrained%20Disentangled%20Representation%20Learning%20from%0A%20%20Epistemological%20Perspective%0AAuthor%3A%20Ruoyu%20Wang%20and%20Lina%20Yao%0AAbstract%3A%20%20%20Disentangled%20Representation%20Learning%20aims%20to%20improve%20the%20explainability%20of%0Adeep%20learning%20methods%20by%20training%20a%20data%20encoder%20that%20identifies%20semantically%0Ameaningful%20latent%20variables%20in%20the%20data%20generation%20process.%20Nevertheless%2C%20there%0Ais%20no%20consensus%20regarding%20a%20universally%20accepted%20definition%20for%20the%20objective%0Aof%20disentangled%20representation%20learning.%20In%20particular%2C%20there%20is%20a%20considerable%0Aamount%20of%20discourse%20regarding%20whether%20should%20the%20latent%20variables%20be%20mutually%0Aindependent%20or%20not.%20In%20this%20paper%2C%20we%20first%20investigate%20these%20arguments%20on%20the%0Ainterrelationships%20between%20latent%20variables%20by%20establishing%20a%20conceptual%20bridge%0Abetween%20Epistemology%20and%20Disentangled%20Representation%20Learning.%20Then%2C%20inspired%0Aby%20these%20interdisciplinary%20concepts%2C%20we%20introduce%20a%20two-level%20latent%20space%0Aframework%20to%20provide%20a%20general%20solution%20to%20the%20prior%20arguments%20on%20this%20issue.%0AFinally%2C%20we%20propose%20a%20novel%20method%20for%20disentangled%20representation%20learning%20by%0Aemploying%20an%20integration%20of%20mutual%20information%20constraint%20and%20independence%0Aconstraint%20within%20the%20Generative%20Adversarial%20Network%20%28GAN%29%20framework.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20method%20consistently%0Aoutperforms%20baseline%20approaches%20in%20both%20quantitative%20and%20qualitative%0Aevaluations.%20The%20method%20exhibits%20strong%20performance%20across%20multiple%20commonly%0Aused%20metrics%20and%20demonstrates%20a%20great%20capability%20in%20disentangling%20various%0Asemantic%20factors%2C%20leading%20to%20an%20improved%20quality%20of%20controllable%20generation%2C%0Awhich%20consequently%20benefits%20the%20explainability%20of%20the%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndependence%2520Constrained%2520Disentangled%2520Representation%2520Learning%2520from%250A%2520%2520Epistemological%2520Perspective%26entry.906535625%3DRuoyu%2520Wang%2520and%2520Lina%2520Yao%26entry.1292438233%3D%2520%2520Disentangled%2520Representation%2520Learning%2520aims%2520to%2520improve%2520the%2520explainability%2520of%250Adeep%2520learning%2520methods%2520by%2520training%2520a%2520data%2520encoder%2520that%2520identifies%2520semantically%250Ameaningful%2520latent%2520variables%2520in%2520the%2520data%2520generation%2520process.%2520Nevertheless%252C%2520there%250Ais%2520no%2520consensus%2520regarding%2520a%2520universally%2520accepted%2520definition%2520for%2520the%2520objective%250Aof%2520disentangled%2520representation%2520learning.%2520In%2520particular%252C%2520there%2520is%2520a%2520considerable%250Aamount%2520of%2520discourse%2520regarding%2520whether%2520should%2520the%2520latent%2520variables%2520be%2520mutually%250Aindependent%2520or%2520not.%2520In%2520this%2520paper%252C%2520we%2520first%2520investigate%2520these%2520arguments%2520on%2520the%250Ainterrelationships%2520between%2520latent%2520variables%2520by%2520establishing%2520a%2520conceptual%2520bridge%250Abetween%2520Epistemology%2520and%2520Disentangled%2520Representation%2520Learning.%2520Then%252C%2520inspired%250Aby%2520these%2520interdisciplinary%2520concepts%252C%2520we%2520introduce%2520a%2520two-level%2520latent%2520space%250Aframework%2520to%2520provide%2520a%2520general%2520solution%2520to%2520the%2520prior%2520arguments%2520on%2520this%2520issue.%250AFinally%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520disentangled%2520representation%2520learning%2520by%250Aemploying%2520an%2520integration%2520of%2520mutual%2520information%2520constraint%2520and%2520independence%250Aconstraint%2520within%2520the%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520framework.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520method%2520consistently%250Aoutperforms%2520baseline%2520approaches%2520in%2520both%2520quantitative%2520and%2520qualitative%250Aevaluations.%2520The%2520method%2520exhibits%2520strong%2520performance%2520across%2520multiple%2520commonly%250Aused%2520metrics%2520and%2520demonstrates%2520a%2520great%2520capability%2520in%2520disentangling%2520various%250Asemantic%2520factors%252C%2520leading%2520to%2520an%2520improved%2520quality%2520of%2520controllable%2520generation%252C%250Awhich%2520consequently%2520benefits%2520the%2520explainability%2520of%2520the%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Independence%20Constrained%20Disentangled%20Representation%20Learning%20from%0A%20%20Epistemological%20Perspective&entry.906535625=Ruoyu%20Wang%20and%20Lina%20Yao&entry.1292438233=%20%20Disentangled%20Representation%20Learning%20aims%20to%20improve%20the%20explainability%20of%0Adeep%20learning%20methods%20by%20training%20a%20data%20encoder%20that%20identifies%20semantically%0Ameaningful%20latent%20variables%20in%20the%20data%20generation%20process.%20Nevertheless%2C%20there%0Ais%20no%20consensus%20regarding%20a%20universally%20accepted%20definition%20for%20the%20objective%0Aof%20disentangled%20representation%20learning.%20In%20particular%2C%20there%20is%20a%20considerable%0Aamount%20of%20discourse%20regarding%20whether%20should%20the%20latent%20variables%20be%20mutually%0Aindependent%20or%20not.%20In%20this%20paper%2C%20we%20first%20investigate%20these%20arguments%20on%20the%0Ainterrelationships%20between%20latent%20variables%20by%20establishing%20a%20conceptual%20bridge%0Abetween%20Epistemology%20and%20Disentangled%20Representation%20Learning.%20Then%2C%20inspired%0Aby%20these%20interdisciplinary%20concepts%2C%20we%20introduce%20a%20two-level%20latent%20space%0Aframework%20to%20provide%20a%20general%20solution%20to%20the%20prior%20arguments%20on%20this%20issue.%0AFinally%2C%20we%20propose%20a%20novel%20method%20for%20disentangled%20representation%20learning%20by%0Aemploying%20an%20integration%20of%20mutual%20information%20constraint%20and%20independence%0Aconstraint%20within%20the%20Generative%20Adversarial%20Network%20%28GAN%29%20framework.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20method%20consistently%0Aoutperforms%20baseline%20approaches%20in%20both%20quantitative%20and%20qualitative%0Aevaluations.%20The%20method%20exhibits%20strong%20performance%20across%20multiple%20commonly%0Aused%20metrics%20and%20demonstrates%20a%20great%20capability%20in%20disentangling%20various%0Asemantic%20factors%2C%20leading%20to%20an%20improved%20quality%20of%20controllable%20generation%2C%0Awhich%20consequently%20benefits%20the%20explainability%20of%20the%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02672v1&entry.124074799=Read"},
{"title": "AdvSecureNet: A Python Toolkit for Adversarial Machine Learning", "author": "Melih Catal and Manuel G\u00fcnther", "abstract": "  Machine learning models are vulnerable to adversarial attacks. Several tools\nhave been developed to research these vulnerabilities, but they often lack\ncomprehensive features and flexibility. We introduce AdvSecureNet, a PyTorch\nbased toolkit for adversarial machine learning that is the first to natively\nsupport multi-GPU setups for attacks, defenses, and evaluation. It is the first\ntoolkit that supports both CLI and API interfaces and external YAML\nconfiguration files to enhance versatility and reproducibility. The toolkit\nincludes multiple attacks, defenses and evaluation metrics. Rigiorous software\nengineering practices are followed to ensure high code quality and\nmaintainability. The project is available as an open-source project on GitHub\nat https://github.com/melihcatal/advsecurenet and installable via PyPI.\n", "link": "http://arxiv.org/abs/2409.02629v1", "date": "2024-09-04", "relevancy": 2.1237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4403}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4235}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdvSecureNet%3A%20A%20Python%20Toolkit%20for%20Adversarial%20Machine%20Learning&body=Title%3A%20AdvSecureNet%3A%20A%20Python%20Toolkit%20for%20Adversarial%20Machine%20Learning%0AAuthor%3A%20Melih%20Catal%20and%20Manuel%20G%C3%BCnther%0AAbstract%3A%20%20%20Machine%20learning%20models%20are%20vulnerable%20to%20adversarial%20attacks.%20Several%20tools%0Ahave%20been%20developed%20to%20research%20these%20vulnerabilities%2C%20but%20they%20often%20lack%0Acomprehensive%20features%20and%20flexibility.%20We%20introduce%20AdvSecureNet%2C%20a%20PyTorch%0Abased%20toolkit%20for%20adversarial%20machine%20learning%20that%20is%20the%20first%20to%20natively%0Asupport%20multi-GPU%20setups%20for%20attacks%2C%20defenses%2C%20and%20evaluation.%20It%20is%20the%20first%0Atoolkit%20that%20supports%20both%20CLI%20and%20API%20interfaces%20and%20external%20YAML%0Aconfiguration%20files%20to%20enhance%20versatility%20and%20reproducibility.%20The%20toolkit%0Aincludes%20multiple%20attacks%2C%20defenses%20and%20evaluation%20metrics.%20Rigiorous%20software%0Aengineering%20practices%20are%20followed%20to%20ensure%20high%20code%20quality%20and%0Amaintainability.%20The%20project%20is%20available%20as%20an%20open-source%20project%20on%20GitHub%0Aat%20https%3A//github.com/melihcatal/advsecurenet%20and%20installable%20via%20PyPI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvSecureNet%253A%2520A%2520Python%2520Toolkit%2520for%2520Adversarial%2520Machine%2520Learning%26entry.906535625%3DMelih%2520Catal%2520and%2520Manuel%2520G%25C3%25BCnther%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520are%2520vulnerable%2520to%2520adversarial%2520attacks.%2520Several%2520tools%250Ahave%2520been%2520developed%2520to%2520research%2520these%2520vulnerabilities%252C%2520but%2520they%2520often%2520lack%250Acomprehensive%2520features%2520and%2520flexibility.%2520We%2520introduce%2520AdvSecureNet%252C%2520a%2520PyTorch%250Abased%2520toolkit%2520for%2520adversarial%2520machine%2520learning%2520that%2520is%2520the%2520first%2520to%2520natively%250Asupport%2520multi-GPU%2520setups%2520for%2520attacks%252C%2520defenses%252C%2520and%2520evaluation.%2520It%2520is%2520the%2520first%250Atoolkit%2520that%2520supports%2520both%2520CLI%2520and%2520API%2520interfaces%2520and%2520external%2520YAML%250Aconfiguration%2520files%2520to%2520enhance%2520versatility%2520and%2520reproducibility.%2520The%2520toolkit%250Aincludes%2520multiple%2520attacks%252C%2520defenses%2520and%2520evaluation%2520metrics.%2520Rigiorous%2520software%250Aengineering%2520practices%2520are%2520followed%2520to%2520ensure%2520high%2520code%2520quality%2520and%250Amaintainability.%2520The%2520project%2520is%2520available%2520as%2520an%2520open-source%2520project%2520on%2520GitHub%250Aat%2520https%253A//github.com/melihcatal/advsecurenet%2520and%2520installable%2520via%2520PyPI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdvSecureNet%3A%20A%20Python%20Toolkit%20for%20Adversarial%20Machine%20Learning&entry.906535625=Melih%20Catal%20and%20Manuel%20G%C3%BCnther&entry.1292438233=%20%20Machine%20learning%20models%20are%20vulnerable%20to%20adversarial%20attacks.%20Several%20tools%0Ahave%20been%20developed%20to%20research%20these%20vulnerabilities%2C%20but%20they%20often%20lack%0Acomprehensive%20features%20and%20flexibility.%20We%20introduce%20AdvSecureNet%2C%20a%20PyTorch%0Abased%20toolkit%20for%20adversarial%20machine%20learning%20that%20is%20the%20first%20to%20natively%0Asupport%20multi-GPU%20setups%20for%20attacks%2C%20defenses%2C%20and%20evaluation.%20It%20is%20the%20first%0Atoolkit%20that%20supports%20both%20CLI%20and%20API%20interfaces%20and%20external%20YAML%0Aconfiguration%20files%20to%20enhance%20versatility%20and%20reproducibility.%20The%20toolkit%0Aincludes%20multiple%20attacks%2C%20defenses%20and%20evaluation%20metrics.%20Rigiorous%20software%0Aengineering%20practices%20are%20followed%20to%20ensure%20high%20code%20quality%20and%0Amaintainability.%20The%20project%20is%20available%20as%20an%20open-source%20project%20on%20GitHub%0Aat%20https%3A//github.com/melihcatal/advsecurenet%20and%20installable%20via%20PyPI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02629v1&entry.124074799=Read"},
{"title": "Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN\n  Efficiency via Knowledge Distillation", "author": "Sangyeop Yeo and Yoojin Jang and Jaejun Yoo", "abstract": "  In this paper, we address the challenge of compressing generative adversarial\nnetworks (GANs) for deployment in resource-constrained environments by\nproposing two novel methodologies: Distribution Matching for Efficient\ncompression (DiME) and Network Interactive Compression via Knowledge Exchange\nand Learning (NICKEL). DiME employs foundation models as embedding kernels for\nefficient distribution matching, leveraging maximum mean discrepancy to\nfacilitate effective knowledge distillation. Simultaneously, NICKEL employs an\ninteractive compression method that enhances the communication between the\nstudent generator and discriminator, achieving a balanced and stable\ncompression process. Our comprehensive evaluation on the StyleGAN2 architecture\nwith the FFHQ dataset shows the effectiveness of our approach, with NICKEL &\nDiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and\n98.92%, respectively. Remarkably, our methods sustain generative quality even\nat an extreme compression rate of 99.69%, surpassing the previous\nstate-of-the-art performance by a large margin. These findings not only\ndemonstrate our methodologies' capacity to significantly lower GANs'\ncomputational demands but also pave the way for deploying high-quality GAN\nmodels in settings with limited resources. Our code will be released soon.\n", "link": "http://arxiv.org/abs/2405.11614v2", "date": "2024-09-04", "relevancy": 2.115, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5411}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nickel%20and%20Diming%20Your%20GAN%3A%20A%20Dual-Method%20Approach%20to%20Enhancing%20GAN%0A%20%20Efficiency%20via%20Knowledge%20Distillation&body=Title%3A%20Nickel%20and%20Diming%20Your%20GAN%3A%20A%20Dual-Method%20Approach%20to%20Enhancing%20GAN%0A%20%20Efficiency%20via%20Knowledge%20Distillation%0AAuthor%3A%20Sangyeop%20Yeo%20and%20Yoojin%20Jang%20and%20Jaejun%20Yoo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20compressing%20generative%20adversarial%0Anetworks%20%28GANs%29%20for%20deployment%20in%20resource-constrained%20environments%20by%0Aproposing%20two%20novel%20methodologies%3A%20Distribution%20Matching%20for%20Efficient%0Acompression%20%28DiME%29%20and%20Network%20Interactive%20Compression%20via%20Knowledge%20Exchange%0Aand%20Learning%20%28NICKEL%29.%20DiME%20employs%20foundation%20models%20as%20embedding%20kernels%20for%0Aefficient%20distribution%20matching%2C%20leveraging%20maximum%20mean%20discrepancy%20to%0Afacilitate%20effective%20knowledge%20distillation.%20Simultaneously%2C%20NICKEL%20employs%20an%0Ainteractive%20compression%20method%20that%20enhances%20the%20communication%20between%20the%0Astudent%20generator%20and%20discriminator%2C%20achieving%20a%20balanced%20and%20stable%0Acompression%20process.%20Our%20comprehensive%20evaluation%20on%20the%20StyleGAN2%20architecture%0Awith%20the%20FFHQ%20dataset%20shows%20the%20effectiveness%20of%20our%20approach%2C%20with%20NICKEL%20%26%0ADiME%20achieving%20FID%20scores%20of%2010.45%20and%2015.93%20at%20compression%20rates%20of%2095.73%25%20and%0A98.92%25%2C%20respectively.%20Remarkably%2C%20our%20methods%20sustain%20generative%20quality%20even%0Aat%20an%20extreme%20compression%20rate%20of%2099.69%25%2C%20surpassing%20the%20previous%0Astate-of-the-art%20performance%20by%20a%20large%20margin.%20These%20findings%20not%20only%0Ademonstrate%20our%20methodologies%27%20capacity%20to%20significantly%20lower%20GANs%27%0Acomputational%20demands%20but%20also%20pave%20the%20way%20for%20deploying%20high-quality%20GAN%0Amodels%20in%20settings%20with%20limited%20resources.%20Our%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNickel%2520and%2520Diming%2520Your%2520GAN%253A%2520A%2520Dual-Method%2520Approach%2520to%2520Enhancing%2520GAN%250A%2520%2520Efficiency%2520via%2520Knowledge%2520Distillation%26entry.906535625%3DSangyeop%2520Yeo%2520and%2520Yoojin%2520Jang%2520and%2520Jaejun%2520Yoo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenge%2520of%2520compressing%2520generative%2520adversarial%250Anetworks%2520%2528GANs%2529%2520for%2520deployment%2520in%2520resource-constrained%2520environments%2520by%250Aproposing%2520two%2520novel%2520methodologies%253A%2520Distribution%2520Matching%2520for%2520Efficient%250Acompression%2520%2528DiME%2529%2520and%2520Network%2520Interactive%2520Compression%2520via%2520Knowledge%2520Exchange%250Aand%2520Learning%2520%2528NICKEL%2529.%2520DiME%2520employs%2520foundation%2520models%2520as%2520embedding%2520kernels%2520for%250Aefficient%2520distribution%2520matching%252C%2520leveraging%2520maximum%2520mean%2520discrepancy%2520to%250Afacilitate%2520effective%2520knowledge%2520distillation.%2520Simultaneously%252C%2520NICKEL%2520employs%2520an%250Ainteractive%2520compression%2520method%2520that%2520enhances%2520the%2520communication%2520between%2520the%250Astudent%2520generator%2520and%2520discriminator%252C%2520achieving%2520a%2520balanced%2520and%2520stable%250Acompression%2520process.%2520Our%2520comprehensive%2520evaluation%2520on%2520the%2520StyleGAN2%2520architecture%250Awith%2520the%2520FFHQ%2520dataset%2520shows%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520with%2520NICKEL%2520%2526%250ADiME%2520achieving%2520FID%2520scores%2520of%252010.45%2520and%252015.93%2520at%2520compression%2520rates%2520of%252095.73%2525%2520and%250A98.92%2525%252C%2520respectively.%2520Remarkably%252C%2520our%2520methods%2520sustain%2520generative%2520quality%2520even%250Aat%2520an%2520extreme%2520compression%2520rate%2520of%252099.69%2525%252C%2520surpassing%2520the%2520previous%250Astate-of-the-art%2520performance%2520by%2520a%2520large%2520margin.%2520These%2520findings%2520not%2520only%250Ademonstrate%2520our%2520methodologies%2527%2520capacity%2520to%2520significantly%2520lower%2520GANs%2527%250Acomputational%2520demands%2520but%2520also%2520pave%2520the%2520way%2520for%2520deploying%2520high-quality%2520GAN%250Amodels%2520in%2520settings%2520with%2520limited%2520resources.%2520Our%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nickel%20and%20Diming%20Your%20GAN%3A%20A%20Dual-Method%20Approach%20to%20Enhancing%20GAN%0A%20%20Efficiency%20via%20Knowledge%20Distillation&entry.906535625=Sangyeop%20Yeo%20and%20Yoojin%20Jang%20and%20Jaejun%20Yoo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20compressing%20generative%20adversarial%0Anetworks%20%28GANs%29%20for%20deployment%20in%20resource-constrained%20environments%20by%0Aproposing%20two%20novel%20methodologies%3A%20Distribution%20Matching%20for%20Efficient%0Acompression%20%28DiME%29%20and%20Network%20Interactive%20Compression%20via%20Knowledge%20Exchange%0Aand%20Learning%20%28NICKEL%29.%20DiME%20employs%20foundation%20models%20as%20embedding%20kernels%20for%0Aefficient%20distribution%20matching%2C%20leveraging%20maximum%20mean%20discrepancy%20to%0Afacilitate%20effective%20knowledge%20distillation.%20Simultaneously%2C%20NICKEL%20employs%20an%0Ainteractive%20compression%20method%20that%20enhances%20the%20communication%20between%20the%0Astudent%20generator%20and%20discriminator%2C%20achieving%20a%20balanced%20and%20stable%0Acompression%20process.%20Our%20comprehensive%20evaluation%20on%20the%20StyleGAN2%20architecture%0Awith%20the%20FFHQ%20dataset%20shows%20the%20effectiveness%20of%20our%20approach%2C%20with%20NICKEL%20%26%0ADiME%20achieving%20FID%20scores%20of%2010.45%20and%2015.93%20at%20compression%20rates%20of%2095.73%25%20and%0A98.92%25%2C%20respectively.%20Remarkably%2C%20our%20methods%20sustain%20generative%20quality%20even%0Aat%20an%20extreme%20compression%20rate%20of%2099.69%25%2C%20surpassing%20the%20previous%0Astate-of-the-art%20performance%20by%20a%20large%20margin.%20These%20findings%20not%20only%0Ademonstrate%20our%20methodologies%27%20capacity%20to%20significantly%20lower%20GANs%27%0Acomputational%20demands%20but%20also%20pave%20the%20way%20for%20deploying%20high-quality%20GAN%0Amodels%20in%20settings%20with%20limited%20resources.%20Our%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11614v2&entry.124074799=Read"},
{"title": "On the Benefits of GPU Sample-Based Stochastic Predictive Controllers\n  for Legged Locomotion", "author": "Giulio Turrisi and Valerio Modugno and Lorenzo Amatucci and Dimitrios Kanoulas and Claudio Semini", "abstract": "  Quadrupedal robots excel in mobility, navigating complex terrains with\nagility. However, their complex control systems present challenges that are\nstill far from being fully addressed. In this paper, we introduce the use of\nSample-Based Stochastic control strategies for quadrupedal robots, as an\nalternative to traditional optimal control laws. We show that Sample-Based\nStochastic methods, supported by GPU acceleration, can be effectively applied\nto real quadruped robots. In particular, in this work, we focus on achieving\ngait frequency adaptation, a notable challenge in quadrupedal locomotion for\ngradient-based methods. To validate the effectiveness of Sample-Based\nStochastic controllers we test two distinct approaches for quadrupedal robots\nand compare them against a conventional gradient-based Model Predictive Control\nsystem. Our findings, validated both in simulation and on a real 21Kg Aliengo\nquadruped, demonstrate that our method is on par with a traditional Model\nPredictive Control strategy when the robot is subject to zero or moderate\ndisturbance, while it surpasses gradient-based methods in handling sustained\nexternal disturbances, thanks to the straightforward gait adaptation strategy\nthat is possible to achieve within their formulation.\n", "link": "http://arxiv.org/abs/2403.11383v2", "date": "2024-09-04", "relevancy": 2.1134, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.603}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5236}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Benefits%20of%20GPU%20Sample-Based%20Stochastic%20Predictive%20Controllers%0A%20%20for%20Legged%20Locomotion&body=Title%3A%20On%20the%20Benefits%20of%20GPU%20Sample-Based%20Stochastic%20Predictive%20Controllers%0A%20%20for%20Legged%20Locomotion%0AAuthor%3A%20Giulio%20Turrisi%20and%20Valerio%20Modugno%20and%20Lorenzo%20Amatucci%20and%20Dimitrios%20Kanoulas%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20Quadrupedal%20robots%20excel%20in%20mobility%2C%20navigating%20complex%20terrains%20with%0Aagility.%20However%2C%20their%20complex%20control%20systems%20present%20challenges%20that%20are%0Astill%20far%20from%20being%20fully%20addressed.%20In%20this%20paper%2C%20we%20introduce%20the%20use%20of%0ASample-Based%20Stochastic%20control%20strategies%20for%20quadrupedal%20robots%2C%20as%20an%0Aalternative%20to%20traditional%20optimal%20control%20laws.%20We%20show%20that%20Sample-Based%0AStochastic%20methods%2C%20supported%20by%20GPU%20acceleration%2C%20can%20be%20effectively%20applied%0Ato%20real%20quadruped%20robots.%20In%20particular%2C%20in%20this%20work%2C%20we%20focus%20on%20achieving%0Agait%20frequency%20adaptation%2C%20a%20notable%20challenge%20in%20quadrupedal%20locomotion%20for%0Agradient-based%20methods.%20To%20validate%20the%20effectiveness%20of%20Sample-Based%0AStochastic%20controllers%20we%20test%20two%20distinct%20approaches%20for%20quadrupedal%20robots%0Aand%20compare%20them%20against%20a%20conventional%20gradient-based%20Model%20Predictive%20Control%0Asystem.%20Our%20findings%2C%20validated%20both%20in%20simulation%20and%20on%20a%20real%2021Kg%20Aliengo%0Aquadruped%2C%20demonstrate%20that%20our%20method%20is%20on%20par%20with%20a%20traditional%20Model%0APredictive%20Control%20strategy%20when%20the%20robot%20is%20subject%20to%20zero%20or%20moderate%0Adisturbance%2C%20while%20it%20surpasses%20gradient-based%20methods%20in%20handling%20sustained%0Aexternal%20disturbances%2C%20thanks%20to%20the%20straightforward%20gait%20adaptation%20strategy%0Athat%20is%20possible%20to%20achieve%20within%20their%20formulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Benefits%2520of%2520GPU%2520Sample-Based%2520Stochastic%2520Predictive%2520Controllers%250A%2520%2520for%2520Legged%2520Locomotion%26entry.906535625%3DGiulio%2520Turrisi%2520and%2520Valerio%2520Modugno%2520and%2520Lorenzo%2520Amatucci%2520and%2520Dimitrios%2520Kanoulas%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520Quadrupedal%2520robots%2520excel%2520in%2520mobility%252C%2520navigating%2520complex%2520terrains%2520with%250Aagility.%2520However%252C%2520their%2520complex%2520control%2520systems%2520present%2520challenges%2520that%2520are%250Astill%2520far%2520from%2520being%2520fully%2520addressed.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520use%2520of%250ASample-Based%2520Stochastic%2520control%2520strategies%2520for%2520quadrupedal%2520robots%252C%2520as%2520an%250Aalternative%2520to%2520traditional%2520optimal%2520control%2520laws.%2520We%2520show%2520that%2520Sample-Based%250AStochastic%2520methods%252C%2520supported%2520by%2520GPU%2520acceleration%252C%2520can%2520be%2520effectively%2520applied%250Ato%2520real%2520quadruped%2520robots.%2520In%2520particular%252C%2520in%2520this%2520work%252C%2520we%2520focus%2520on%2520achieving%250Agait%2520frequency%2520adaptation%252C%2520a%2520notable%2520challenge%2520in%2520quadrupedal%2520locomotion%2520for%250Agradient-based%2520methods.%2520To%2520validate%2520the%2520effectiveness%2520of%2520Sample-Based%250AStochastic%2520controllers%2520we%2520test%2520two%2520distinct%2520approaches%2520for%2520quadrupedal%2520robots%250Aand%2520compare%2520them%2520against%2520a%2520conventional%2520gradient-based%2520Model%2520Predictive%2520Control%250Asystem.%2520Our%2520findings%252C%2520validated%2520both%2520in%2520simulation%2520and%2520on%2520a%2520real%252021Kg%2520Aliengo%250Aquadruped%252C%2520demonstrate%2520that%2520our%2520method%2520is%2520on%2520par%2520with%2520a%2520traditional%2520Model%250APredictive%2520Control%2520strategy%2520when%2520the%2520robot%2520is%2520subject%2520to%2520zero%2520or%2520moderate%250Adisturbance%252C%2520while%2520it%2520surpasses%2520gradient-based%2520methods%2520in%2520handling%2520sustained%250Aexternal%2520disturbances%252C%2520thanks%2520to%2520the%2520straightforward%2520gait%2520adaptation%2520strategy%250Athat%2520is%2520possible%2520to%2520achieve%2520within%2520their%2520formulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Benefits%20of%20GPU%20Sample-Based%20Stochastic%20Predictive%20Controllers%0A%20%20for%20Legged%20Locomotion&entry.906535625=Giulio%20Turrisi%20and%20Valerio%20Modugno%20and%20Lorenzo%20Amatucci%20and%20Dimitrios%20Kanoulas%20and%20Claudio%20Semini&entry.1292438233=%20%20Quadrupedal%20robots%20excel%20in%20mobility%2C%20navigating%20complex%20terrains%20with%0Aagility.%20However%2C%20their%20complex%20control%20systems%20present%20challenges%20that%20are%0Astill%20far%20from%20being%20fully%20addressed.%20In%20this%20paper%2C%20we%20introduce%20the%20use%20of%0ASample-Based%20Stochastic%20control%20strategies%20for%20quadrupedal%20robots%2C%20as%20an%0Aalternative%20to%20traditional%20optimal%20control%20laws.%20We%20show%20that%20Sample-Based%0AStochastic%20methods%2C%20supported%20by%20GPU%20acceleration%2C%20can%20be%20effectively%20applied%0Ato%20real%20quadruped%20robots.%20In%20particular%2C%20in%20this%20work%2C%20we%20focus%20on%20achieving%0Agait%20frequency%20adaptation%2C%20a%20notable%20challenge%20in%20quadrupedal%20locomotion%20for%0Agradient-based%20methods.%20To%20validate%20the%20effectiveness%20of%20Sample-Based%0AStochastic%20controllers%20we%20test%20two%20distinct%20approaches%20for%20quadrupedal%20robots%0Aand%20compare%20them%20against%20a%20conventional%20gradient-based%20Model%20Predictive%20Control%0Asystem.%20Our%20findings%2C%20validated%20both%20in%20simulation%20and%20on%20a%20real%2021Kg%20Aliengo%0Aquadruped%2C%20demonstrate%20that%20our%20method%20is%20on%20par%20with%20a%20traditional%20Model%0APredictive%20Control%20strategy%20when%20the%20robot%20is%20subject%20to%20zero%20or%20moderate%0Adisturbance%2C%20while%20it%20surpasses%20gradient-based%20methods%20in%20handling%20sustained%0Aexternal%20disturbances%2C%20thanks%20to%20the%20straightforward%20gait%20adaptation%20strategy%0Athat%20is%20possible%20to%20achieve%20within%20their%20formulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11383v2&entry.124074799=Read"},
{"title": "LADDER: Language Driven Slice Discovery and Error Rectification", "author": "Shantanu Ghosh and Rayan Syed and Chenyu Wang and Clare B. Poynton and Kayhan Batmanghelich", "abstract": "  Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).\n", "link": "http://arxiv.org/abs/2408.07832v3", "date": "2024-09-04", "relevancy": 2.0974, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5294}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5212}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LADDER%3A%20Language%20Driven%20Slice%20Discovery%20and%20Error%20Rectification&body=Title%3A%20LADDER%3A%20Language%20Driven%20Slice%20Discovery%20and%20Error%20Rectification%0AAuthor%3A%20Shantanu%20Ghosh%20and%20Rayan%20Syed%20and%20Chenyu%20Wang%20and%20Clare%20B.%20Poynton%20and%20Kayhan%20Batmanghelich%0AAbstract%3A%20%20%20Error%20slice%20discovery%20associates%20structured%20patterns%20with%20model%20errors.%0AExisting%20methods%20discover%20error%20slices%20by%20clustering%20the%20error-prone%20samples%0Awith%20similar%20patterns%20or%20assigning%20discrete%20attributes%20to%20each%20sample%20for%0Apost-hoc%20analysis.%20While%20these%20methods%20aim%20for%20interpretability%20and%20easier%0Amitigation%20through%20reweighting%20or%20rebalancing%2C%20they%20may%20not%20capture%20the%20full%0Acomplexity%20of%20error%20patterns%20due%20to%20incomplete%20or%20missing%20attributes.%20Contrary%0Ato%20the%20existing%20approach%2C%20this%20paper%20utilizes%20the%20reasoning%20capabilities%20of%20the%0ALarge%20Language%20Model%20%28LLM%29%20to%20analyze%20complex%20error%20patterns%20and%20generate%0Atestable%20hypotheses.%20This%20paper%20proposes%20LADDER%3A%20Language%20Driven%20slice%0ADiscovery%20and%20Error%20Rectification.%20It%20first%20projects%20the%20model%27s%20representation%0Ainto%20a%20language-aligned%20feature%20space%20%28eg%20CLIP%29%20to%20preserve%20semantics%20in%20the%0Aoriginal%20model%20feature%20space.%20This%20ensures%20the%20accurate%20retrieval%20of%20sentences%0Athat%20highlight%20the%20model%27s%20errors.%20Next%2C%20the%20LLM%20utilizes%20the%20sentences%20and%0Agenerates%20hypotheses%20to%20discover%20error%20slices.%20Finally%2C%20we%20mitigate%20the%20error%0Aby%20fine-tuning%20the%20classification%20head%20by%20creating%20a%20group-balanced%20dataset%0Ausing%20the%20hypotheses.%20Our%20entire%20method%20does%20not%20require%20any%20attribute%0Aannotation%2C%20either%20explicitly%20or%20through%20external%20tagging%20models.%20We%20validate%0Aour%20method%20with%20%5Ctextbf%7Bfive%7D%20image%20classification%20datasets.%20The%20code%20is%0Aavailable%20%28https%3A//github.com/batmanlab/Ladder%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07832v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLADDER%253A%2520Language%2520Driven%2520Slice%2520Discovery%2520and%2520Error%2520Rectification%26entry.906535625%3DShantanu%2520Ghosh%2520and%2520Rayan%2520Syed%2520and%2520Chenyu%2520Wang%2520and%2520Clare%2520B.%2520Poynton%2520and%2520Kayhan%2520Batmanghelich%26entry.1292438233%3D%2520%2520Error%2520slice%2520discovery%2520associates%2520structured%2520patterns%2520with%2520model%2520errors.%250AExisting%2520methods%2520discover%2520error%2520slices%2520by%2520clustering%2520the%2520error-prone%2520samples%250Awith%2520similar%2520patterns%2520or%2520assigning%2520discrete%2520attributes%2520to%2520each%2520sample%2520for%250Apost-hoc%2520analysis.%2520While%2520these%2520methods%2520aim%2520for%2520interpretability%2520and%2520easier%250Amitigation%2520through%2520reweighting%2520or%2520rebalancing%252C%2520they%2520may%2520not%2520capture%2520the%2520full%250Acomplexity%2520of%2520error%2520patterns%2520due%2520to%2520incomplete%2520or%2520missing%2520attributes.%2520Contrary%250Ato%2520the%2520existing%2520approach%252C%2520this%2520paper%2520utilizes%2520the%2520reasoning%2520capabilities%2520of%2520the%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520to%2520analyze%2520complex%2520error%2520patterns%2520and%2520generate%250Atestable%2520hypotheses.%2520This%2520paper%2520proposes%2520LADDER%253A%2520Language%2520Driven%2520slice%250ADiscovery%2520and%2520Error%2520Rectification.%2520It%2520first%2520projects%2520the%2520model%2527s%2520representation%250Ainto%2520a%2520language-aligned%2520feature%2520space%2520%2528eg%2520CLIP%2529%2520to%2520preserve%2520semantics%2520in%2520the%250Aoriginal%2520model%2520feature%2520space.%2520This%2520ensures%2520the%2520accurate%2520retrieval%2520of%2520sentences%250Athat%2520highlight%2520the%2520model%2527s%2520errors.%2520Next%252C%2520the%2520LLM%2520utilizes%2520the%2520sentences%2520and%250Agenerates%2520hypotheses%2520to%2520discover%2520error%2520slices.%2520Finally%252C%2520we%2520mitigate%2520the%2520error%250Aby%2520fine-tuning%2520the%2520classification%2520head%2520by%2520creating%2520a%2520group-balanced%2520dataset%250Ausing%2520the%2520hypotheses.%2520Our%2520entire%2520method%2520does%2520not%2520require%2520any%2520attribute%250Aannotation%252C%2520either%2520explicitly%2520or%2520through%2520external%2520tagging%2520models.%2520We%2520validate%250Aour%2520method%2520with%2520%255Ctextbf%257Bfive%257D%2520image%2520classification%2520datasets.%2520The%2520code%2520is%250Aavailable%2520%2528https%253A//github.com/batmanlab/Ladder%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07832v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LADDER%3A%20Language%20Driven%20Slice%20Discovery%20and%20Error%20Rectification&entry.906535625=Shantanu%20Ghosh%20and%20Rayan%20Syed%20and%20Chenyu%20Wang%20and%20Clare%20B.%20Poynton%20and%20Kayhan%20Batmanghelich&entry.1292438233=%20%20Error%20slice%20discovery%20associates%20structured%20patterns%20with%20model%20errors.%0AExisting%20methods%20discover%20error%20slices%20by%20clustering%20the%20error-prone%20samples%0Awith%20similar%20patterns%20or%20assigning%20discrete%20attributes%20to%20each%20sample%20for%0Apost-hoc%20analysis.%20While%20these%20methods%20aim%20for%20interpretability%20and%20easier%0Amitigation%20through%20reweighting%20or%20rebalancing%2C%20they%20may%20not%20capture%20the%20full%0Acomplexity%20of%20error%20patterns%20due%20to%20incomplete%20or%20missing%20attributes.%20Contrary%0Ato%20the%20existing%20approach%2C%20this%20paper%20utilizes%20the%20reasoning%20capabilities%20of%20the%0ALarge%20Language%20Model%20%28LLM%29%20to%20analyze%20complex%20error%20patterns%20and%20generate%0Atestable%20hypotheses.%20This%20paper%20proposes%20LADDER%3A%20Language%20Driven%20slice%0ADiscovery%20and%20Error%20Rectification.%20It%20first%20projects%20the%20model%27s%20representation%0Ainto%20a%20language-aligned%20feature%20space%20%28eg%20CLIP%29%20to%20preserve%20semantics%20in%20the%0Aoriginal%20model%20feature%20space.%20This%20ensures%20the%20accurate%20retrieval%20of%20sentences%0Athat%20highlight%20the%20model%27s%20errors.%20Next%2C%20the%20LLM%20utilizes%20the%20sentences%20and%0Agenerates%20hypotheses%20to%20discover%20error%20slices.%20Finally%2C%20we%20mitigate%20the%20error%0Aby%20fine-tuning%20the%20classification%20head%20by%20creating%20a%20group-balanced%20dataset%0Ausing%20the%20hypotheses.%20Our%20entire%20method%20does%20not%20require%20any%20attribute%0Aannotation%2C%20either%20explicitly%20or%20through%20external%20tagging%20models.%20We%20validate%0Aour%20method%20with%20%5Ctextbf%7Bfive%7D%20image%20classification%20datasets.%20The%20code%20is%0Aavailable%20%28https%3A//github.com/batmanlab/Ladder%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07832v3&entry.124074799=Read"},
{"title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models", "author": "Adam Ibrahim and Benjamin Th\u00e9rien and Kshitij Gupta and Mats L. Richter and Quentin Anthony and Timoth\u00e9e Lesort and Eugene Belilovsky and Irina Rish", "abstract": "  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n", "link": "http://arxiv.org/abs/2403.08763v4", "date": "2024-09-04", "relevancy": 2.0728, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&body=Title%3A%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20the%20final%20loss%0Aand%20the%20average%20score%20on%20several%20language%20model%20%28LM%29%20evaluation%20benchmarks.%0ASpecifically%2C%20we%20show%20this%20for%20a%20weak%20but%20realistic%20distribution%20shift%20between%0Atwo%20commonly%20used%20LLM%20pre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%0Astronger%20distribution%20shift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%0Aparameter%20model%20scale%20with%20large%20dataset%20sizes%20%28hundreds%20of%20billions%20of%0Atokens%29.%20Selecting%20the%20weak%20but%20realistic%20shift%20for%20larger-scale%20experiments%2C%0Awe%20also%20find%20that%20our%20continual%20learning%20strategies%20match%20the%20re-training%0Abaseline%20for%20a%2010B%20parameter%20LLM.%20Our%20results%20demonstrate%20that%20LLMs%20can%20be%0Asuccessfully%20updated%20via%20simple%20and%20scalable%20continual%20learning%20strategies%2C%0Amatching%20the%20re-training%20baseline%20using%20only%20a%20fraction%20of%20the%20compute.%0AFinally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%20alternatives%20to%20the%20cosine%0Alearning%20rate%20schedule%20that%20help%20circumvent%20forgetting%20induced%20by%20LR%20re-warming%0Aand%20that%20are%20not%20bound%20to%20a%20fixed%20token%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08763v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520and%2520Scalable%2520Strategies%2520to%2520Continually%2520Pre-train%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DAdam%2520Ibrahim%2520and%2520Benjamin%2520Th%25C3%25A9rien%2520and%2520Kshitij%2520Gupta%2520and%2520Mats%2520L.%2520Richter%2520and%2520Quentin%2520Anthony%2520and%2520Timoth%25C3%25A9e%2520Lesort%2520and%2520Eugene%2520Belilovsky%2520and%2520Irina%2520Rish%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520routinely%2520pre-trained%2520on%2520billions%2520of%2520tokens%252C%250Aonly%2520to%2520start%2520the%2520process%2520over%2520again%2520once%2520new%2520data%2520becomes%2520available.%2520A%2520much%250Amore%2520efficient%2520solution%2520is%2520to%2520continually%2520pre-train%2520these%2520models%252C%2520saving%250Asignificant%2520compute%2520compared%2520to%2520re-training.%2520However%252C%2520the%2520distribution%2520shift%250Ainduced%2520by%2520new%2520data%2520typically%2520results%2520in%2520degraded%2520performance%2520on%2520previous%2520data%250Aor%2520poor%2520adaptation%2520to%2520the%2520new%2520data.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520a%2520simple%2520and%250Ascalable%2520combination%2520of%2520learning%2520rate%2520%2528LR%2529%2520re-warming%252C%2520LR%2520re-decaying%252C%2520and%250Areplay%2520of%2520previous%2520data%2520is%2520sufficient%2520to%2520match%2520the%2520performance%2520of%2520fully%250Are-training%2520from%2520scratch%2520on%2520all%2520available%2520data%252C%2520as%2520measured%2520by%2520the%2520final%2520loss%250Aand%2520the%2520average%2520score%2520on%2520several%2520language%2520model%2520%2528LM%2529%2520evaluation%2520benchmarks.%250ASpecifically%252C%2520we%2520show%2520this%2520for%2520a%2520weak%2520but%2520realistic%2520distribution%2520shift%2520between%250Atwo%2520commonly%2520used%2520LLM%2520pre-training%2520datasets%2520%2528English%2524%255Crightarrow%2524English%2529%2520and%2520a%250Astronger%2520distribution%2520shift%2520%2528English%2524%255Crightarrow%2524German%2529%2520at%2520the%2520%2524405%2524M%250Aparameter%2520model%2520scale%2520with%2520large%2520dataset%2520sizes%2520%2528hundreds%2520of%2520billions%2520of%250Atokens%2529.%2520Selecting%2520the%2520weak%2520but%2520realistic%2520shift%2520for%2520larger-scale%2520experiments%252C%250Awe%2520also%2520find%2520that%2520our%2520continual%2520learning%2520strategies%2520match%2520the%2520re-training%250Abaseline%2520for%2520a%252010B%2520parameter%2520LLM.%2520Our%2520results%2520demonstrate%2520that%2520LLMs%2520can%2520be%250Asuccessfully%2520updated%2520via%2520simple%2520and%2520scalable%2520continual%2520learning%2520strategies%252C%250Amatching%2520the%2520re-training%2520baseline%2520using%2520only%2520a%2520fraction%2520of%2520the%2520compute.%250AFinally%252C%2520inspired%2520by%2520previous%2520work%252C%2520we%2520propose%2520alternatives%2520to%2520the%2520cosine%250Alearning%2520rate%2520schedule%2520that%2520help%2520circumvent%2520forgetting%2520induced%2520by%2520LR%2520re-warming%250Aand%2520that%2520are%2520not%2520bound%2520to%2520a%2520fixed%2520token%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08763v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&entry.906535625=Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20the%20final%20loss%0Aand%20the%20average%20score%20on%20several%20language%20model%20%28LM%29%20evaluation%20benchmarks.%0ASpecifically%2C%20we%20show%20this%20for%20a%20weak%20but%20realistic%20distribution%20shift%20between%0Atwo%20commonly%20used%20LLM%20pre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%0Astronger%20distribution%20shift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%0Aparameter%20model%20scale%20with%20large%20dataset%20sizes%20%28hundreds%20of%20billions%20of%0Atokens%29.%20Selecting%20the%20weak%20but%20realistic%20shift%20for%20larger-scale%20experiments%2C%0Awe%20also%20find%20that%20our%20continual%20learning%20strategies%20match%20the%20re-training%0Abaseline%20for%20a%2010B%20parameter%20LLM.%20Our%20results%20demonstrate%20that%20LLMs%20can%20be%0Asuccessfully%20updated%20via%20simple%20and%20scalable%20continual%20learning%20strategies%2C%0Amatching%20the%20re-training%20baseline%20using%20only%20a%20fraction%20of%20the%20compute.%0AFinally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%20alternatives%20to%20the%20cosine%0Alearning%20rate%20schedule%20that%20help%20circumvent%20forgetting%20induced%20by%20LR%20re-warming%0Aand%20that%20are%20not%20bound%20to%20a%20fixed%20token%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08763v4&entry.124074799=Read"},
{"title": "iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned\n  Adaptation", "author": "Hayeon Jo and Hyesong Choi and Minhee Cho and Dongbo Min", "abstract": "  Transfer learning based on full fine-tuning (FFT) of the pre-trained encoder\nand task-specific decoder becomes increasingly complex as deep models grow\nexponentially. Parameter efficient fine-tuning (PEFT) approaches using adapters\nconsisting of small learnable layers have emerged as an alternative to FFT,\nachieving comparable performance while maintaining high training efficiency.\nHowever, the inflexibility of the adapter with respect to input instances\nlimits its capability of learning task-specific information in diverse\ndownstream tasks. In this paper, we propose a novel PEFT approach,\ninput-Conditioned transFormer, termed iConFormer, that leverages a dynamic\nadapter conditioned on the input instances. To secure flexible learning ability\non input instances in various downstream tasks, we introduce an\ninput-Conditioned Network (iCoN) in the dynamic adapter that enables\ninstance-level feature transformation. To be specific, iCoN generates\nchannel-wise convolutional kernels for each feature and transform it using\nadaptive convolution process to effectively capture task-specific and\nfine-grained details tailor to downstream tasks. Experimental results\ndemonstrate that by tuning just 1.6% to 2.8% of the Transformer backbone\nparameters, iConFormer achieves performance comparable to FFT in monocular\ndepth estimation and semantic segmentation, while outperforming it in image\nclassification and instance segmentation. Also, the proposed method\nconsistently outperforms recent PEFT methods for all the tasks mentioned above.\n", "link": "http://arxiv.org/abs/2409.02838v1", "date": "2024-09-04", "relevancy": 2.0607, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5034}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iConFormer%3A%20Dynamic%20Parameter-Efficient%20Tuning%20with%20Input-Conditioned%0A%20%20Adaptation&body=Title%3A%20iConFormer%3A%20Dynamic%20Parameter-Efficient%20Tuning%20with%20Input-Conditioned%0A%20%20Adaptation%0AAuthor%3A%20Hayeon%20Jo%20and%20Hyesong%20Choi%20and%20Minhee%20Cho%20and%20Dongbo%20Min%0AAbstract%3A%20%20%20Transfer%20learning%20based%20on%20full%20fine-tuning%20%28FFT%29%20of%20the%20pre-trained%20encoder%0Aand%20task-specific%20decoder%20becomes%20increasingly%20complex%20as%20deep%20models%20grow%0Aexponentially.%20Parameter%20efficient%20fine-tuning%20%28PEFT%29%20approaches%20using%20adapters%0Aconsisting%20of%20small%20learnable%20layers%20have%20emerged%20as%20an%20alternative%20to%20FFT%2C%0Aachieving%20comparable%20performance%20while%20maintaining%20high%20training%20efficiency.%0AHowever%2C%20the%20inflexibility%20of%20the%20adapter%20with%20respect%20to%20input%20instances%0Alimits%20its%20capability%20of%20learning%20task-specific%20information%20in%20diverse%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20PEFT%20approach%2C%0Ainput-Conditioned%20transFormer%2C%20termed%20iConFormer%2C%20that%20leverages%20a%20dynamic%0Aadapter%20conditioned%20on%20the%20input%20instances.%20To%20secure%20flexible%20learning%20ability%0Aon%20input%20instances%20in%20various%20downstream%20tasks%2C%20we%20introduce%20an%0Ainput-Conditioned%20Network%20%28iCoN%29%20in%20the%20dynamic%20adapter%20that%20enables%0Ainstance-level%20feature%20transformation.%20To%20be%20specific%2C%20iCoN%20generates%0Achannel-wise%20convolutional%20kernels%20for%20each%20feature%20and%20transform%20it%20using%0Aadaptive%20convolution%20process%20to%20effectively%20capture%20task-specific%20and%0Afine-grained%20details%20tailor%20to%20downstream%20tasks.%20Experimental%20results%0Ademonstrate%20that%20by%20tuning%20just%201.6%25%20to%202.8%25%20of%20the%20Transformer%20backbone%0Aparameters%2C%20iConFormer%20achieves%20performance%20comparable%20to%20FFT%20in%20monocular%0Adepth%20estimation%20and%20semantic%20segmentation%2C%20while%20outperforming%20it%20in%20image%0Aclassification%20and%20instance%20segmentation.%20Also%2C%20the%20proposed%20method%0Aconsistently%20outperforms%20recent%20PEFT%20methods%20for%20all%20the%20tasks%20mentioned%20above.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiConFormer%253A%2520Dynamic%2520Parameter-Efficient%2520Tuning%2520with%2520Input-Conditioned%250A%2520%2520Adaptation%26entry.906535625%3DHayeon%2520Jo%2520and%2520Hyesong%2520Choi%2520and%2520Minhee%2520Cho%2520and%2520Dongbo%2520Min%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520based%2520on%2520full%2520fine-tuning%2520%2528FFT%2529%2520of%2520the%2520pre-trained%2520encoder%250Aand%2520task-specific%2520decoder%2520becomes%2520increasingly%2520complex%2520as%2520deep%2520models%2520grow%250Aexponentially.%2520Parameter%2520efficient%2520fine-tuning%2520%2528PEFT%2529%2520approaches%2520using%2520adapters%250Aconsisting%2520of%2520small%2520learnable%2520layers%2520have%2520emerged%2520as%2520an%2520alternative%2520to%2520FFT%252C%250Aachieving%2520comparable%2520performance%2520while%2520maintaining%2520high%2520training%2520efficiency.%250AHowever%252C%2520the%2520inflexibility%2520of%2520the%2520adapter%2520with%2520respect%2520to%2520input%2520instances%250Alimits%2520its%2520capability%2520of%2520learning%2520task-specific%2520information%2520in%2520diverse%250Adownstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520PEFT%2520approach%252C%250Ainput-Conditioned%2520transFormer%252C%2520termed%2520iConFormer%252C%2520that%2520leverages%2520a%2520dynamic%250Aadapter%2520conditioned%2520on%2520the%2520input%2520instances.%2520To%2520secure%2520flexible%2520learning%2520ability%250Aon%2520input%2520instances%2520in%2520various%2520downstream%2520tasks%252C%2520we%2520introduce%2520an%250Ainput-Conditioned%2520Network%2520%2528iCoN%2529%2520in%2520the%2520dynamic%2520adapter%2520that%2520enables%250Ainstance-level%2520feature%2520transformation.%2520To%2520be%2520specific%252C%2520iCoN%2520generates%250Achannel-wise%2520convolutional%2520kernels%2520for%2520each%2520feature%2520and%2520transform%2520it%2520using%250Aadaptive%2520convolution%2520process%2520to%2520effectively%2520capture%2520task-specific%2520and%250Afine-grained%2520details%2520tailor%2520to%2520downstream%2520tasks.%2520Experimental%2520results%250Ademonstrate%2520that%2520by%2520tuning%2520just%25201.6%2525%2520to%25202.8%2525%2520of%2520the%2520Transformer%2520backbone%250Aparameters%252C%2520iConFormer%2520achieves%2520performance%2520comparable%2520to%2520FFT%2520in%2520monocular%250Adepth%2520estimation%2520and%2520semantic%2520segmentation%252C%2520while%2520outperforming%2520it%2520in%2520image%250Aclassification%2520and%2520instance%2520segmentation.%2520Also%252C%2520the%2520proposed%2520method%250Aconsistently%2520outperforms%2520recent%2520PEFT%2520methods%2520for%2520all%2520the%2520tasks%2520mentioned%2520above.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iConFormer%3A%20Dynamic%20Parameter-Efficient%20Tuning%20with%20Input-Conditioned%0A%20%20Adaptation&entry.906535625=Hayeon%20Jo%20and%20Hyesong%20Choi%20and%20Minhee%20Cho%20and%20Dongbo%20Min&entry.1292438233=%20%20Transfer%20learning%20based%20on%20full%20fine-tuning%20%28FFT%29%20of%20the%20pre-trained%20encoder%0Aand%20task-specific%20decoder%20becomes%20increasingly%20complex%20as%20deep%20models%20grow%0Aexponentially.%20Parameter%20efficient%20fine-tuning%20%28PEFT%29%20approaches%20using%20adapters%0Aconsisting%20of%20small%20learnable%20layers%20have%20emerged%20as%20an%20alternative%20to%20FFT%2C%0Aachieving%20comparable%20performance%20while%20maintaining%20high%20training%20efficiency.%0AHowever%2C%20the%20inflexibility%20of%20the%20adapter%20with%20respect%20to%20input%20instances%0Alimits%20its%20capability%20of%20learning%20task-specific%20information%20in%20diverse%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20PEFT%20approach%2C%0Ainput-Conditioned%20transFormer%2C%20termed%20iConFormer%2C%20that%20leverages%20a%20dynamic%0Aadapter%20conditioned%20on%20the%20input%20instances.%20To%20secure%20flexible%20learning%20ability%0Aon%20input%20instances%20in%20various%20downstream%20tasks%2C%20we%20introduce%20an%0Ainput-Conditioned%20Network%20%28iCoN%29%20in%20the%20dynamic%20adapter%20that%20enables%0Ainstance-level%20feature%20transformation.%20To%20be%20specific%2C%20iCoN%20generates%0Achannel-wise%20convolutional%20kernels%20for%20each%20feature%20and%20transform%20it%20using%0Aadaptive%20convolution%20process%20to%20effectively%20capture%20task-specific%20and%0Afine-grained%20details%20tailor%20to%20downstream%20tasks.%20Experimental%20results%0Ademonstrate%20that%20by%20tuning%20just%201.6%25%20to%202.8%25%20of%20the%20Transformer%20backbone%0Aparameters%2C%20iConFormer%20achieves%20performance%20comparable%20to%20FFT%20in%20monocular%0Adepth%20estimation%20and%20semantic%20segmentation%2C%20while%20outperforming%20it%20in%20image%0Aclassification%20and%20instance%20segmentation.%20Also%2C%20the%20proposed%20method%0Aconsistently%20outperforms%20recent%20PEFT%20methods%20for%20all%20the%20tasks%20mentioned%20above.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02838v1&entry.124074799=Read"},
{"title": "SparQ Attention: Bandwidth-Efficient LLM Inference", "author": "Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr", "abstract": "  The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.\n", "link": "http://arxiv.org/abs/2312.04985v6", "date": "2024-09-04", "relevancy": 2.0517, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5524}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4959}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparQ%20Attention%3A%20Bandwidth-Efficient%20LLM%20Inference&body=Title%3A%20SparQ%20Attention%3A%20Bandwidth-Efficient%20LLM%20Inference%0AAuthor%3A%20Luka%20Ribar%20and%20Ivan%20Chelombiev%20and%20Luke%20Hudlass-Galley%20and%20Charlie%20Blake%20and%20Carlo%20Luschi%20and%20Douglas%20Orr%0AAbstract%3A%20%20%20The%20computational%20difficulties%20of%20large%20language%20model%20%28LLM%29%20inference%20remain%0Aa%20significant%20obstacle%20to%20their%20widespread%20deployment.%20The%20need%20for%20many%0Aapplications%20to%20support%20long%20input%20sequences%20and%20process%20them%20in%20large%20batches%0Atypically%20causes%20token-generation%20to%20be%20bottlenecked%20by%20data%20transfer.%20For%20this%0Areason%2C%20we%20introduce%20SparQ%20Attention%2C%20a%20technique%20for%20increasing%20the%20inference%0Athroughput%20of%20LLMs%20by%20utilising%20memory%20bandwidth%20more%20efficiently%20within%20the%0Aattention%20layers%2C%20through%20selective%20fetching%20of%20the%20cached%20history.%20Our%0Aproposed%20technique%20can%20be%20applied%20directly%20to%20off-the-shelf%20LLMs%20during%0Ainference%2C%20without%20requiring%20any%20modification%20to%20the%20pre-training%20setup%20or%0Aadditional%20fine-tuning.%20We%20show%20that%20SparQ%20Attention%20brings%20up%20to%208x%20savings%20in%0Aattention%20data%20transfers%20without%20substantial%20drops%20in%20accuracy%2C%20by%20evaluating%0ALlama%202%20and%203%2C%20Mistral%2C%20Gemma%20and%20Pythia%20models%20on%20a%20wide%20range%20of%20downstream%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04985v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparQ%2520Attention%253A%2520Bandwidth-Efficient%2520LLM%2520Inference%26entry.906535625%3DLuka%2520Ribar%2520and%2520Ivan%2520Chelombiev%2520and%2520Luke%2520Hudlass-Galley%2520and%2520Charlie%2520Blake%2520and%2520Carlo%2520Luschi%2520and%2520Douglas%2520Orr%26entry.1292438233%3D%2520%2520The%2520computational%2520difficulties%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520inference%2520remain%250Aa%2520significant%2520obstacle%2520to%2520their%2520widespread%2520deployment.%2520The%2520need%2520for%2520many%250Aapplications%2520to%2520support%2520long%2520input%2520sequences%2520and%2520process%2520them%2520in%2520large%2520batches%250Atypically%2520causes%2520token-generation%2520to%2520be%2520bottlenecked%2520by%2520data%2520transfer.%2520For%2520this%250Areason%252C%2520we%2520introduce%2520SparQ%2520Attention%252C%2520a%2520technique%2520for%2520increasing%2520the%2520inference%250Athroughput%2520of%2520LLMs%2520by%2520utilising%2520memory%2520bandwidth%2520more%2520efficiently%2520within%2520the%250Aattention%2520layers%252C%2520through%2520selective%2520fetching%2520of%2520the%2520cached%2520history.%2520Our%250Aproposed%2520technique%2520can%2520be%2520applied%2520directly%2520to%2520off-the-shelf%2520LLMs%2520during%250Ainference%252C%2520without%2520requiring%2520any%2520modification%2520to%2520the%2520pre-training%2520setup%2520or%250Aadditional%2520fine-tuning.%2520We%2520show%2520that%2520SparQ%2520Attention%2520brings%2520up%2520to%25208x%2520savings%2520in%250Aattention%2520data%2520transfers%2520without%2520substantial%2520drops%2520in%2520accuracy%252C%2520by%2520evaluating%250ALlama%25202%2520and%25203%252C%2520Mistral%252C%2520Gemma%2520and%2520Pythia%2520models%2520on%2520a%2520wide%2520range%2520of%2520downstream%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04985v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparQ%20Attention%3A%20Bandwidth-Efficient%20LLM%20Inference&entry.906535625=Luka%20Ribar%20and%20Ivan%20Chelombiev%20and%20Luke%20Hudlass-Galley%20and%20Charlie%20Blake%20and%20Carlo%20Luschi%20and%20Douglas%20Orr&entry.1292438233=%20%20The%20computational%20difficulties%20of%20large%20language%20model%20%28LLM%29%20inference%20remain%0Aa%20significant%20obstacle%20to%20their%20widespread%20deployment.%20The%20need%20for%20many%0Aapplications%20to%20support%20long%20input%20sequences%20and%20process%20them%20in%20large%20batches%0Atypically%20causes%20token-generation%20to%20be%20bottlenecked%20by%20data%20transfer.%20For%20this%0Areason%2C%20we%20introduce%20SparQ%20Attention%2C%20a%20technique%20for%20increasing%20the%20inference%0Athroughput%20of%20LLMs%20by%20utilising%20memory%20bandwidth%20more%20efficiently%20within%20the%0Aattention%20layers%2C%20through%20selective%20fetching%20of%20the%20cached%20history.%20Our%0Aproposed%20technique%20can%20be%20applied%20directly%20to%20off-the-shelf%20LLMs%20during%0Ainference%2C%20without%20requiring%20any%20modification%20to%20the%20pre-training%20setup%20or%0Aadditional%20fine-tuning.%20We%20show%20that%20SparQ%20Attention%20brings%20up%20to%208x%20savings%20in%0Aattention%20data%20transfers%20without%20substantial%20drops%20in%20accuracy%2C%20by%20evaluating%0ALlama%202%20and%203%2C%20Mistral%2C%20Gemma%20and%20Pythia%20models%20on%20a%20wide%20range%20of%20downstream%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04985v6&entry.124074799=Read"},
{"title": "Quantifying uncertainty in lung cancer segmentation with foundation\n  models applied to mixed-domain datasets", "author": "Aneesh Rangnekar and Nishant Nadkarni and Jue Jiang and Harini Veeraraghavan", "abstract": "  Medical image foundation models have shown the ability to segment organs and\ntumors with minimal fine-tuning. These models are typically evaluated on\ntask-specific in-distribution (ID) datasets. However, reliable performance on\nID dataset does not guarantee robust generalization on out-of-distribution\n(OOD) datasets. Importantly, once deployed for clinical use, it is impractical\nto have `ground truth' delineations to assess ongoing performance drifts,\nespecially when images fall into OOD category due to different imaging\nprotocols. Hence, we introduced a comprehensive set of computationally fast\nmetrics to evaluate the performance of multiple foundation models (Swin UNETR,\nSimMIM, iBOT, SMIT) trained with self-supervised learning (SSL). SSL\npretraining was selected as this approach is applicable for large, diverse, and\nunlabeled image sets. All models were fine-tuned on identical datasets for lung\ntumor segmentation from computed tomography (CT) scans. SimMIM, iBOT, and SMIT\nused identical architecture, pretraining, and fine-tuning datasets to assess\nperformance variations with the choice of pretext tasks used in SSL. Evaluation\nwas performed on two public lung cancer datasets (LRAD: n = 140, 5Rater: n =\n21) with different image acquisitions and tumor stage compared to training data\n(n = 317 public resource with stage III-IV lung cancers) and a public\nnon-cancer dataset containing volumetric CT scans of patients with pulmonary\nembolism (n = 120). All models produced similarly accurate tumor segmentation\non the lung cancer testing datasets. SMIT produced a highest F1-score (LRAD:\n0.60, 5Rater: 0.64) and lowest entropy (LRAD: 0.06, 5Rater: 0.12), indicating\nhigher tumor detection rate and confident segmentations. In the OOD dataset,\nSMIT misdetected least number of tumors, indicated by median volume occupancy\nof 5.67 cc compared to second best method SimMIM of 9.97 cc.\n", "link": "http://arxiv.org/abs/2403.13113v2", "date": "2024-09-04", "relevancy": 2.0464, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5759}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20uncertainty%20in%20lung%20cancer%20segmentation%20with%20foundation%0A%20%20models%20applied%20to%20mixed-domain%20datasets&body=Title%3A%20Quantifying%20uncertainty%20in%20lung%20cancer%20segmentation%20with%20foundation%0A%20%20models%20applied%20to%20mixed-domain%20datasets%0AAuthor%3A%20Aneesh%20Rangnekar%20and%20Nishant%20Nadkarni%20and%20Jue%20Jiang%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Medical%20image%20foundation%20models%20have%20shown%20the%20ability%20to%20segment%20organs%20and%0Atumors%20with%20minimal%20fine-tuning.%20These%20models%20are%20typically%20evaluated%20on%0Atask-specific%20in-distribution%20%28ID%29%20datasets.%20However%2C%20reliable%20performance%20on%0AID%20dataset%20does%20not%20guarantee%20robust%20generalization%20on%20out-of-distribution%0A%28OOD%29%20datasets.%20Importantly%2C%20once%20deployed%20for%20clinical%20use%2C%20it%20is%20impractical%0Ato%20have%20%60ground%20truth%27%20delineations%20to%20assess%20ongoing%20performance%20drifts%2C%0Aespecially%20when%20images%20fall%20into%20OOD%20category%20due%20to%20different%20imaging%0Aprotocols.%20Hence%2C%20we%20introduced%20a%20comprehensive%20set%20of%20computationally%20fast%0Ametrics%20to%20evaluate%20the%20performance%20of%20multiple%20foundation%20models%20%28Swin%20UNETR%2C%0ASimMIM%2C%20iBOT%2C%20SMIT%29%20trained%20with%20self-supervised%20learning%20%28SSL%29.%20SSL%0Apretraining%20was%20selected%20as%20this%20approach%20is%20applicable%20for%20large%2C%20diverse%2C%20and%0Aunlabeled%20image%20sets.%20All%20models%20were%20fine-tuned%20on%20identical%20datasets%20for%20lung%0Atumor%20segmentation%20from%20computed%20tomography%20%28CT%29%20scans.%20SimMIM%2C%20iBOT%2C%20and%20SMIT%0Aused%20identical%20architecture%2C%20pretraining%2C%20and%20fine-tuning%20datasets%20to%20assess%0Aperformance%20variations%20with%20the%20choice%20of%20pretext%20tasks%20used%20in%20SSL.%20Evaluation%0Awas%20performed%20on%20two%20public%20lung%20cancer%20datasets%20%28LRAD%3A%20n%20%3D%20140%2C%205Rater%3A%20n%20%3D%0A21%29%20with%20different%20image%20acquisitions%20and%20tumor%20stage%20compared%20to%20training%20data%0A%28n%20%3D%20317%20public%20resource%20with%20stage%20III-IV%20lung%20cancers%29%20and%20a%20public%0Anon-cancer%20dataset%20containing%20volumetric%20CT%20scans%20of%20patients%20with%20pulmonary%0Aembolism%20%28n%20%3D%20120%29.%20All%20models%20produced%20similarly%20accurate%20tumor%20segmentation%0Aon%20the%20lung%20cancer%20testing%20datasets.%20SMIT%20produced%20a%20highest%20F1-score%20%28LRAD%3A%0A0.60%2C%205Rater%3A%200.64%29%20and%20lowest%20entropy%20%28LRAD%3A%200.06%2C%205Rater%3A%200.12%29%2C%20indicating%0Ahigher%20tumor%20detection%20rate%20and%20confident%20segmentations.%20In%20the%20OOD%20dataset%2C%0ASMIT%20misdetected%20least%20number%20of%20tumors%2C%20indicated%20by%20median%20volume%20occupancy%0Aof%205.67%20cc%20compared%20to%20second%20best%20method%20SimMIM%20of%209.97%20cc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520uncertainty%2520in%2520lung%2520cancer%2520segmentation%2520with%2520foundation%250A%2520%2520models%2520applied%2520to%2520mixed-domain%2520datasets%26entry.906535625%3DAneesh%2520Rangnekar%2520and%2520Nishant%2520Nadkarni%2520and%2520Jue%2520Jiang%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Medical%2520image%2520foundation%2520models%2520have%2520shown%2520the%2520ability%2520to%2520segment%2520organs%2520and%250Atumors%2520with%2520minimal%2520fine-tuning.%2520These%2520models%2520are%2520typically%2520evaluated%2520on%250Atask-specific%2520in-distribution%2520%2528ID%2529%2520datasets.%2520However%252C%2520reliable%2520performance%2520on%250AID%2520dataset%2520does%2520not%2520guarantee%2520robust%2520generalization%2520on%2520out-of-distribution%250A%2528OOD%2529%2520datasets.%2520Importantly%252C%2520once%2520deployed%2520for%2520clinical%2520use%252C%2520it%2520is%2520impractical%250Ato%2520have%2520%2560ground%2520truth%2527%2520delineations%2520to%2520assess%2520ongoing%2520performance%2520drifts%252C%250Aespecially%2520when%2520images%2520fall%2520into%2520OOD%2520category%2520due%2520to%2520different%2520imaging%250Aprotocols.%2520Hence%252C%2520we%2520introduced%2520a%2520comprehensive%2520set%2520of%2520computationally%2520fast%250Ametrics%2520to%2520evaluate%2520the%2520performance%2520of%2520multiple%2520foundation%2520models%2520%2528Swin%2520UNETR%252C%250ASimMIM%252C%2520iBOT%252C%2520SMIT%2529%2520trained%2520with%2520self-supervised%2520learning%2520%2528SSL%2529.%2520SSL%250Apretraining%2520was%2520selected%2520as%2520this%2520approach%2520is%2520applicable%2520for%2520large%252C%2520diverse%252C%2520and%250Aunlabeled%2520image%2520sets.%2520All%2520models%2520were%2520fine-tuned%2520on%2520identical%2520datasets%2520for%2520lung%250Atumor%2520segmentation%2520from%2520computed%2520tomography%2520%2528CT%2529%2520scans.%2520SimMIM%252C%2520iBOT%252C%2520and%2520SMIT%250Aused%2520identical%2520architecture%252C%2520pretraining%252C%2520and%2520fine-tuning%2520datasets%2520to%2520assess%250Aperformance%2520variations%2520with%2520the%2520choice%2520of%2520pretext%2520tasks%2520used%2520in%2520SSL.%2520Evaluation%250Awas%2520performed%2520on%2520two%2520public%2520lung%2520cancer%2520datasets%2520%2528LRAD%253A%2520n%2520%253D%2520140%252C%25205Rater%253A%2520n%2520%253D%250A21%2529%2520with%2520different%2520image%2520acquisitions%2520and%2520tumor%2520stage%2520compared%2520to%2520training%2520data%250A%2528n%2520%253D%2520317%2520public%2520resource%2520with%2520stage%2520III-IV%2520lung%2520cancers%2529%2520and%2520a%2520public%250Anon-cancer%2520dataset%2520containing%2520volumetric%2520CT%2520scans%2520of%2520patients%2520with%2520pulmonary%250Aembolism%2520%2528n%2520%253D%2520120%2529.%2520All%2520models%2520produced%2520similarly%2520accurate%2520tumor%2520segmentation%250Aon%2520the%2520lung%2520cancer%2520testing%2520datasets.%2520SMIT%2520produced%2520a%2520highest%2520F1-score%2520%2528LRAD%253A%250A0.60%252C%25205Rater%253A%25200.64%2529%2520and%2520lowest%2520entropy%2520%2528LRAD%253A%25200.06%252C%25205Rater%253A%25200.12%2529%252C%2520indicating%250Ahigher%2520tumor%2520detection%2520rate%2520and%2520confident%2520segmentations.%2520In%2520the%2520OOD%2520dataset%252C%250ASMIT%2520misdetected%2520least%2520number%2520of%2520tumors%252C%2520indicated%2520by%2520median%2520volume%2520occupancy%250Aof%25205.67%2520cc%2520compared%2520to%2520second%2520best%2520method%2520SimMIM%2520of%25209.97%2520cc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20uncertainty%20in%20lung%20cancer%20segmentation%20with%20foundation%0A%20%20models%20applied%20to%20mixed-domain%20datasets&entry.906535625=Aneesh%20Rangnekar%20and%20Nishant%20Nadkarni%20and%20Jue%20Jiang%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Medical%20image%20foundation%20models%20have%20shown%20the%20ability%20to%20segment%20organs%20and%0Atumors%20with%20minimal%20fine-tuning.%20These%20models%20are%20typically%20evaluated%20on%0Atask-specific%20in-distribution%20%28ID%29%20datasets.%20However%2C%20reliable%20performance%20on%0AID%20dataset%20does%20not%20guarantee%20robust%20generalization%20on%20out-of-distribution%0A%28OOD%29%20datasets.%20Importantly%2C%20once%20deployed%20for%20clinical%20use%2C%20it%20is%20impractical%0Ato%20have%20%60ground%20truth%27%20delineations%20to%20assess%20ongoing%20performance%20drifts%2C%0Aespecially%20when%20images%20fall%20into%20OOD%20category%20due%20to%20different%20imaging%0Aprotocols.%20Hence%2C%20we%20introduced%20a%20comprehensive%20set%20of%20computationally%20fast%0Ametrics%20to%20evaluate%20the%20performance%20of%20multiple%20foundation%20models%20%28Swin%20UNETR%2C%0ASimMIM%2C%20iBOT%2C%20SMIT%29%20trained%20with%20self-supervised%20learning%20%28SSL%29.%20SSL%0Apretraining%20was%20selected%20as%20this%20approach%20is%20applicable%20for%20large%2C%20diverse%2C%20and%0Aunlabeled%20image%20sets.%20All%20models%20were%20fine-tuned%20on%20identical%20datasets%20for%20lung%0Atumor%20segmentation%20from%20computed%20tomography%20%28CT%29%20scans.%20SimMIM%2C%20iBOT%2C%20and%20SMIT%0Aused%20identical%20architecture%2C%20pretraining%2C%20and%20fine-tuning%20datasets%20to%20assess%0Aperformance%20variations%20with%20the%20choice%20of%20pretext%20tasks%20used%20in%20SSL.%20Evaluation%0Awas%20performed%20on%20two%20public%20lung%20cancer%20datasets%20%28LRAD%3A%20n%20%3D%20140%2C%205Rater%3A%20n%20%3D%0A21%29%20with%20different%20image%20acquisitions%20and%20tumor%20stage%20compared%20to%20training%20data%0A%28n%20%3D%20317%20public%20resource%20with%20stage%20III-IV%20lung%20cancers%29%20and%20a%20public%0Anon-cancer%20dataset%20containing%20volumetric%20CT%20scans%20of%20patients%20with%20pulmonary%0Aembolism%20%28n%20%3D%20120%29.%20All%20models%20produced%20similarly%20accurate%20tumor%20segmentation%0Aon%20the%20lung%20cancer%20testing%20datasets.%20SMIT%20produced%20a%20highest%20F1-score%20%28LRAD%3A%0A0.60%2C%205Rater%3A%200.64%29%20and%20lowest%20entropy%20%28LRAD%3A%200.06%2C%205Rater%3A%200.12%29%2C%20indicating%0Ahigher%20tumor%20detection%20rate%20and%20confident%20segmentations.%20In%20the%20OOD%20dataset%2C%0ASMIT%20misdetected%20least%20number%20of%20tumors%2C%20indicated%20by%20median%20volume%20occupancy%0Aof%205.67%20cc%20compared%20to%20second%20best%20method%20SimMIM%20of%209.97%20cc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13113v2&entry.124074799=Read"},
{"title": "ExpLLM: Towards Chain of Thought for Facial Expression Recognition", "author": "Xing Lan and Jian Xue and Ji Qi and Dongmei Jiang and Ke Lu and Tat-Seng Chua", "abstract": "  Facial expression recognition (FER) is a critical task in multimedia with\nsignificant implications across various domains. However, analyzing the causes\nof facial expressions is essential for accurately recognizing them. Current\napproaches, such as those based on facial action units (AUs), typically provide\nAU names and intensities but lack insight into the interactions and\nrelationships between AUs and the overall expression. In this paper, we propose\na novel method called ExpLLM, which leverages large language models to generate\nan accurate chain of thought (CoT) for facial expression recognition.\nSpecifically, we have designed the CoT mechanism from three key perspectives:\nkey observations, overall emotional interpretation, and conclusion. The key\nobservations describe the AU's name, intensity, and associated emotions. The\noverall emotional interpretation provides an analysis based on multiple AUs and\ntheir interactions, identifying the dominant emotions and their relationships.\nFinally, the conclusion presents the final expression label derived from the\npreceding analysis. Furthermore, we also introduce the Exp-CoT Engine, designed\nto construct this expression CoT and generate instruction-description data for\ntraining our ExpLLM. Extensive experiments on the RAF-DB and AffectNet datasets\ndemonstrate that ExpLLM outperforms current state-of-the-art FER methods.\nExpLLM also surpasses the latest GPT-4o in expression CoT generation,\nparticularly in recognizing micro-expressions where GPT-4o frequently fails.\n", "link": "http://arxiv.org/abs/2409.02828v1", "date": "2024-09-04", "relevancy": 2.0449, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpLLM%3A%20Towards%20Chain%20of%20Thought%20for%20Facial%20Expression%20Recognition&body=Title%3A%20ExpLLM%3A%20Towards%20Chain%20of%20Thought%20for%20Facial%20Expression%20Recognition%0AAuthor%3A%20Xing%20Lan%20and%20Jian%20Xue%20and%20Ji%20Qi%20and%20Dongmei%20Jiang%20and%20Ke%20Lu%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20%28FER%29%20is%20a%20critical%20task%20in%20multimedia%20with%0Asignificant%20implications%20across%20various%20domains.%20However%2C%20analyzing%20the%20causes%0Aof%20facial%20expressions%20is%20essential%20for%20accurately%20recognizing%20them.%20Current%0Aapproaches%2C%20such%20as%20those%20based%20on%20facial%20action%20units%20%28AUs%29%2C%20typically%20provide%0AAU%20names%20and%20intensities%20but%20lack%20insight%20into%20the%20interactions%20and%0Arelationships%20between%20AUs%20and%20the%20overall%20expression.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20method%20called%20ExpLLM%2C%20which%20leverages%20large%20language%20models%20to%20generate%0Aan%20accurate%20chain%20of%20thought%20%28CoT%29%20for%20facial%20expression%20recognition.%0ASpecifically%2C%20we%20have%20designed%20the%20CoT%20mechanism%20from%20three%20key%20perspectives%3A%0Akey%20observations%2C%20overall%20emotional%20interpretation%2C%20and%20conclusion.%20The%20key%0Aobservations%20describe%20the%20AU%27s%20name%2C%20intensity%2C%20and%20associated%20emotions.%20The%0Aoverall%20emotional%20interpretation%20provides%20an%20analysis%20based%20on%20multiple%20AUs%20and%0Atheir%20interactions%2C%20identifying%20the%20dominant%20emotions%20and%20their%20relationships.%0AFinally%2C%20the%20conclusion%20presents%20the%20final%20expression%20label%20derived%20from%20the%0Apreceding%20analysis.%20Furthermore%2C%20we%20also%20introduce%20the%20Exp-CoT%20Engine%2C%20designed%0Ato%20construct%20this%20expression%20CoT%20and%20generate%20instruction-description%20data%20for%0Atraining%20our%20ExpLLM.%20Extensive%20experiments%20on%20the%20RAF-DB%20and%20AffectNet%20datasets%0Ademonstrate%20that%20ExpLLM%20outperforms%20current%20state-of-the-art%20FER%20methods.%0AExpLLM%20also%20surpasses%20the%20latest%20GPT-4o%20in%20expression%20CoT%20generation%2C%0Aparticularly%20in%20recognizing%20micro-expressions%20where%20GPT-4o%20frequently%20fails.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpLLM%253A%2520Towards%2520Chain%2520of%2520Thought%2520for%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DXing%2520Lan%2520and%2520Jian%2520Xue%2520and%2520Ji%2520Qi%2520and%2520Dongmei%2520Jiang%2520and%2520Ke%2520Lu%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Facial%2520expression%2520recognition%2520%2528FER%2529%2520is%2520a%2520critical%2520task%2520in%2520multimedia%2520with%250Asignificant%2520implications%2520across%2520various%2520domains.%2520However%252C%2520analyzing%2520the%2520causes%250Aof%2520facial%2520expressions%2520is%2520essential%2520for%2520accurately%2520recognizing%2520them.%2520Current%250Aapproaches%252C%2520such%2520as%2520those%2520based%2520on%2520facial%2520action%2520units%2520%2528AUs%2529%252C%2520typically%2520provide%250AAU%2520names%2520and%2520intensities%2520but%2520lack%2520insight%2520into%2520the%2520interactions%2520and%250Arelationships%2520between%2520AUs%2520and%2520the%2520overall%2520expression.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520novel%2520method%2520called%2520ExpLLM%252C%2520which%2520leverages%2520large%2520language%2520models%2520to%2520generate%250Aan%2520accurate%2520chain%2520of%2520thought%2520%2528CoT%2529%2520for%2520facial%2520expression%2520recognition.%250ASpecifically%252C%2520we%2520have%2520designed%2520the%2520CoT%2520mechanism%2520from%2520three%2520key%2520perspectives%253A%250Akey%2520observations%252C%2520overall%2520emotional%2520interpretation%252C%2520and%2520conclusion.%2520The%2520key%250Aobservations%2520describe%2520the%2520AU%2527s%2520name%252C%2520intensity%252C%2520and%2520associated%2520emotions.%2520The%250Aoverall%2520emotional%2520interpretation%2520provides%2520an%2520analysis%2520based%2520on%2520multiple%2520AUs%2520and%250Atheir%2520interactions%252C%2520identifying%2520the%2520dominant%2520emotions%2520and%2520their%2520relationships.%250AFinally%252C%2520the%2520conclusion%2520presents%2520the%2520final%2520expression%2520label%2520derived%2520from%2520the%250Apreceding%2520analysis.%2520Furthermore%252C%2520we%2520also%2520introduce%2520the%2520Exp-CoT%2520Engine%252C%2520designed%250Ato%2520construct%2520this%2520expression%2520CoT%2520and%2520generate%2520instruction-description%2520data%2520for%250Atraining%2520our%2520ExpLLM.%2520Extensive%2520experiments%2520on%2520the%2520RAF-DB%2520and%2520AffectNet%2520datasets%250Ademonstrate%2520that%2520ExpLLM%2520outperforms%2520current%2520state-of-the-art%2520FER%2520methods.%250AExpLLM%2520also%2520surpasses%2520the%2520latest%2520GPT-4o%2520in%2520expression%2520CoT%2520generation%252C%250Aparticularly%2520in%2520recognizing%2520micro-expressions%2520where%2520GPT-4o%2520frequently%2520fails.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpLLM%3A%20Towards%20Chain%20of%20Thought%20for%20Facial%20Expression%20Recognition&entry.906535625=Xing%20Lan%20and%20Jian%20Xue%20and%20Ji%20Qi%20and%20Dongmei%20Jiang%20and%20Ke%20Lu%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Facial%20expression%20recognition%20%28FER%29%20is%20a%20critical%20task%20in%20multimedia%20with%0Asignificant%20implications%20across%20various%20domains.%20However%2C%20analyzing%20the%20causes%0Aof%20facial%20expressions%20is%20essential%20for%20accurately%20recognizing%20them.%20Current%0Aapproaches%2C%20such%20as%20those%20based%20on%20facial%20action%20units%20%28AUs%29%2C%20typically%20provide%0AAU%20names%20and%20intensities%20but%20lack%20insight%20into%20the%20interactions%20and%0Arelationships%20between%20AUs%20and%20the%20overall%20expression.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20method%20called%20ExpLLM%2C%20which%20leverages%20large%20language%20models%20to%20generate%0Aan%20accurate%20chain%20of%20thought%20%28CoT%29%20for%20facial%20expression%20recognition.%0ASpecifically%2C%20we%20have%20designed%20the%20CoT%20mechanism%20from%20three%20key%20perspectives%3A%0Akey%20observations%2C%20overall%20emotional%20interpretation%2C%20and%20conclusion.%20The%20key%0Aobservations%20describe%20the%20AU%27s%20name%2C%20intensity%2C%20and%20associated%20emotions.%20The%0Aoverall%20emotional%20interpretation%20provides%20an%20analysis%20based%20on%20multiple%20AUs%20and%0Atheir%20interactions%2C%20identifying%20the%20dominant%20emotions%20and%20their%20relationships.%0AFinally%2C%20the%20conclusion%20presents%20the%20final%20expression%20label%20derived%20from%20the%0Apreceding%20analysis.%20Furthermore%2C%20we%20also%20introduce%20the%20Exp-CoT%20Engine%2C%20designed%0Ato%20construct%20this%20expression%20CoT%20and%20generate%20instruction-description%20data%20for%0Atraining%20our%20ExpLLM.%20Extensive%20experiments%20on%20the%20RAF-DB%20and%20AffectNet%20datasets%0Ademonstrate%20that%20ExpLLM%20outperforms%20current%20state-of-the-art%20FER%20methods.%0AExpLLM%20also%20surpasses%20the%20latest%20GPT-4o%20in%20expression%20CoT%20generation%2C%0Aparticularly%20in%20recognizing%20micro-expressions%20where%20GPT-4o%20frequently%20fails.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02828v1&entry.124074799=Read"},
{"title": "A Hybrid Framework for Spatial Interpolation: Merging Data-driven with\n  Domain Knowledge", "author": "Cong Zhang and Shuyi Du and Hongqing Song and Yuhe Wang", "abstract": "  Estimating spatially distributed information through the interpolation of\nscattered observation datasets often overlooks the critical role of domain\nknowledge in understanding spatial dependencies. Additionally, the features of\nthese data sets are typically limited to the spatial coordinates of the\nscattered observation locations. In this paper, we propose a hybrid framework\nthat integrates data-driven spatial dependency feature extraction with\nrule-assisted spatial dependency function mapping to augment domain knowledge.\nWe demonstrate the superior performance of our framework in two comparative\napplication scenarios, highlighting its ability to capture more localized\nspatial features in the reconstructed distribution fields. Furthermore, we\nunderscore its potential to enhance nonlinear estimation capabilities through\nthe application of transformed fuzzy rules and to quantify the inherent\nuncertainties associated with the observation data sets. Our framework\nintroduces an innovative approach to spatial information estimation by\nsynergistically combining observational data with rule-assisted domain\nknowledge.\n", "link": "http://arxiv.org/abs/2409.00125v2", "date": "2024-09-04", "relevancy": 2.0385, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5205}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5105}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Framework%20for%20Spatial%20Interpolation%3A%20Merging%20Data-driven%20with%0A%20%20Domain%20Knowledge&body=Title%3A%20A%20Hybrid%20Framework%20for%20Spatial%20Interpolation%3A%20Merging%20Data-driven%20with%0A%20%20Domain%20Knowledge%0AAuthor%3A%20Cong%20Zhang%20and%20Shuyi%20Du%20and%20Hongqing%20Song%20and%20Yuhe%20Wang%0AAbstract%3A%20%20%20Estimating%20spatially%20distributed%20information%20through%20the%20interpolation%20of%0Ascattered%20observation%20datasets%20often%20overlooks%20the%20critical%20role%20of%20domain%0Aknowledge%20in%20understanding%20spatial%20dependencies.%20Additionally%2C%20the%20features%20of%0Athese%20data%20sets%20are%20typically%20limited%20to%20the%20spatial%20coordinates%20of%20the%0Ascattered%20observation%20locations.%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20framework%0Athat%20integrates%20data-driven%20spatial%20dependency%20feature%20extraction%20with%0Arule-assisted%20spatial%20dependency%20function%20mapping%20to%20augment%20domain%20knowledge.%0AWe%20demonstrate%20the%20superior%20performance%20of%20our%20framework%20in%20two%20comparative%0Aapplication%20scenarios%2C%20highlighting%20its%20ability%20to%20capture%20more%20localized%0Aspatial%20features%20in%20the%20reconstructed%20distribution%20fields.%20Furthermore%2C%20we%0Aunderscore%20its%20potential%20to%20enhance%20nonlinear%20estimation%20capabilities%20through%0Athe%20application%20of%20transformed%20fuzzy%20rules%20and%20to%20quantify%20the%20inherent%0Auncertainties%20associated%20with%20the%20observation%20data%20sets.%20Our%20framework%0Aintroduces%20an%20innovative%20approach%20to%20spatial%20information%20estimation%20by%0Asynergistically%20combining%20observational%20data%20with%20rule-assisted%20domain%0Aknowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Framework%2520for%2520Spatial%2520Interpolation%253A%2520Merging%2520Data-driven%2520with%250A%2520%2520Domain%2520Knowledge%26entry.906535625%3DCong%2520Zhang%2520and%2520Shuyi%2520Du%2520and%2520Hongqing%2520Song%2520and%2520Yuhe%2520Wang%26entry.1292438233%3D%2520%2520Estimating%2520spatially%2520distributed%2520information%2520through%2520the%2520interpolation%2520of%250Ascattered%2520observation%2520datasets%2520often%2520overlooks%2520the%2520critical%2520role%2520of%2520domain%250Aknowledge%2520in%2520understanding%2520spatial%2520dependencies.%2520Additionally%252C%2520the%2520features%2520of%250Athese%2520data%2520sets%2520are%2520typically%2520limited%2520to%2520the%2520spatial%2520coordinates%2520of%2520the%250Ascattered%2520observation%2520locations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hybrid%2520framework%250Athat%2520integrates%2520data-driven%2520spatial%2520dependency%2520feature%2520extraction%2520with%250Arule-assisted%2520spatial%2520dependency%2520function%2520mapping%2520to%2520augment%2520domain%2520knowledge.%250AWe%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520framework%2520in%2520two%2520comparative%250Aapplication%2520scenarios%252C%2520highlighting%2520its%2520ability%2520to%2520capture%2520more%2520localized%250Aspatial%2520features%2520in%2520the%2520reconstructed%2520distribution%2520fields.%2520Furthermore%252C%2520we%250Aunderscore%2520its%2520potential%2520to%2520enhance%2520nonlinear%2520estimation%2520capabilities%2520through%250Athe%2520application%2520of%2520transformed%2520fuzzy%2520rules%2520and%2520to%2520quantify%2520the%2520inherent%250Auncertainties%2520associated%2520with%2520the%2520observation%2520data%2520sets.%2520Our%2520framework%250Aintroduces%2520an%2520innovative%2520approach%2520to%2520spatial%2520information%2520estimation%2520by%250Asynergistically%2520combining%2520observational%2520data%2520with%2520rule-assisted%2520domain%250Aknowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Framework%20for%20Spatial%20Interpolation%3A%20Merging%20Data-driven%20with%0A%20%20Domain%20Knowledge&entry.906535625=Cong%20Zhang%20and%20Shuyi%20Du%20and%20Hongqing%20Song%20and%20Yuhe%20Wang&entry.1292438233=%20%20Estimating%20spatially%20distributed%20information%20through%20the%20interpolation%20of%0Ascattered%20observation%20datasets%20often%20overlooks%20the%20critical%20role%20of%20domain%0Aknowledge%20in%20understanding%20spatial%20dependencies.%20Additionally%2C%20the%20features%20of%0Athese%20data%20sets%20are%20typically%20limited%20to%20the%20spatial%20coordinates%20of%20the%0Ascattered%20observation%20locations.%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20framework%0Athat%20integrates%20data-driven%20spatial%20dependency%20feature%20extraction%20with%0Arule-assisted%20spatial%20dependency%20function%20mapping%20to%20augment%20domain%20knowledge.%0AWe%20demonstrate%20the%20superior%20performance%20of%20our%20framework%20in%20two%20comparative%0Aapplication%20scenarios%2C%20highlighting%20its%20ability%20to%20capture%20more%20localized%0Aspatial%20features%20in%20the%20reconstructed%20distribution%20fields.%20Furthermore%2C%20we%0Aunderscore%20its%20potential%20to%20enhance%20nonlinear%20estimation%20capabilities%20through%0Athe%20application%20of%20transformed%20fuzzy%20rules%20and%20to%20quantify%20the%20inherent%0Auncertainties%20associated%20with%20the%20observation%20data%20sets.%20Our%20framework%0Aintroduces%20an%20innovative%20approach%20to%20spatial%20information%20estimation%20by%0Asynergistically%20combining%20observational%20data%20with%20rule-assisted%20domain%0Aknowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00125v2&entry.124074799=Read"},
{"title": "Decentralized Intelligence Network (DIN)", "author": "Abraham Nash", "abstract": "  Decentralized Intelligence Network (DIN) is a theoretical framework designed\nto address challenges in AI development, particularly focusing on data\nfragmentation and siloing issues. It facilitates effective AI training within\nsovereign data networks by overcoming barriers to accessing diverse data\nsources, leveraging: 1) personal data stores to ensure data sovereignty, where\ndata remains securely within Participants' control; 2) a scalable federated\nlearning protocol implemented on a public blockchain for decentralized AI\ntraining, where only model parameter updates are shared, keeping data within\nthe personal data stores; and 3) a scalable, trustless cryptographic rewards\nmechanism on a public blockchain to incentivize participation and ensure fair\nreward distribution through a decentralized auditing protocol. This approach\nguarantees that no entity can prevent or control access to training data or\ninfluence financial benefits, as coordination and reward distribution are\nmanaged on the public blockchain with an immutable record. The framework\nsupports effective AI training by allowing Participants to maintain control\nover their data, benefit financially, and contribute to a decentralized,\nscalable ecosystem that leverages collective AI to develop beneficial\nalgorithms.\n", "link": "http://arxiv.org/abs/2407.02461v5", "date": "2024-09-04", "relevancy": 2.0362, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4156}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4053}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Intelligence%20Network%20%28DIN%29&body=Title%3A%20Decentralized%20Intelligence%20Network%20%28DIN%29%0AAuthor%3A%20Abraham%20Nash%0AAbstract%3A%20%20%20Decentralized%20Intelligence%20Network%20%28DIN%29%20is%20a%20theoretical%20framework%20designed%0Ato%20address%20challenges%20in%20AI%20development%2C%20particularly%20focusing%20on%20data%0Afragmentation%20and%20siloing%20issues.%20It%20facilitates%20effective%20AI%20training%20within%0Asovereign%20data%20networks%20by%20overcoming%20barriers%20to%20accessing%20diverse%20data%0Asources%2C%20leveraging%3A%201%29%20personal%20data%20stores%20to%20ensure%20data%20sovereignty%2C%20where%0Adata%20remains%20securely%20within%20Participants%27%20control%3B%202%29%20a%20scalable%20federated%0Alearning%20protocol%20implemented%20on%20a%20public%20blockchain%20for%20decentralized%20AI%0Atraining%2C%20where%20only%20model%20parameter%20updates%20are%20shared%2C%20keeping%20data%20within%0Athe%20personal%20data%20stores%3B%20and%203%29%20a%20scalable%2C%20trustless%20cryptographic%20rewards%0Amechanism%20on%20a%20public%20blockchain%20to%20incentivize%20participation%20and%20ensure%20fair%0Areward%20distribution%20through%20a%20decentralized%20auditing%20protocol.%20This%20approach%0Aguarantees%20that%20no%20entity%20can%20prevent%20or%20control%20access%20to%20training%20data%20or%0Ainfluence%20financial%20benefits%2C%20as%20coordination%20and%20reward%20distribution%20are%0Amanaged%20on%20the%20public%20blockchain%20with%20an%20immutable%20record.%20The%20framework%0Asupports%20effective%20AI%20training%20by%20allowing%20Participants%20to%20maintain%20control%0Aover%20their%20data%2C%20benefit%20financially%2C%20and%20contribute%20to%20a%20decentralized%2C%0Ascalable%20ecosystem%20that%20leverages%20collective%20AI%20to%20develop%20beneficial%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02461v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Intelligence%2520Network%2520%2528DIN%2529%26entry.906535625%3DAbraham%2520Nash%26entry.1292438233%3D%2520%2520Decentralized%2520Intelligence%2520Network%2520%2528DIN%2529%2520is%2520a%2520theoretical%2520framework%2520designed%250Ato%2520address%2520challenges%2520in%2520AI%2520development%252C%2520particularly%2520focusing%2520on%2520data%250Afragmentation%2520and%2520siloing%2520issues.%2520It%2520facilitates%2520effective%2520AI%2520training%2520within%250Asovereign%2520data%2520networks%2520by%2520overcoming%2520barriers%2520to%2520accessing%2520diverse%2520data%250Asources%252C%2520leveraging%253A%25201%2529%2520personal%2520data%2520stores%2520to%2520ensure%2520data%2520sovereignty%252C%2520where%250Adata%2520remains%2520securely%2520within%2520Participants%2527%2520control%253B%25202%2529%2520a%2520scalable%2520federated%250Alearning%2520protocol%2520implemented%2520on%2520a%2520public%2520blockchain%2520for%2520decentralized%2520AI%250Atraining%252C%2520where%2520only%2520model%2520parameter%2520updates%2520are%2520shared%252C%2520keeping%2520data%2520within%250Athe%2520personal%2520data%2520stores%253B%2520and%25203%2529%2520a%2520scalable%252C%2520trustless%2520cryptographic%2520rewards%250Amechanism%2520on%2520a%2520public%2520blockchain%2520to%2520incentivize%2520participation%2520and%2520ensure%2520fair%250Areward%2520distribution%2520through%2520a%2520decentralized%2520auditing%2520protocol.%2520This%2520approach%250Aguarantees%2520that%2520no%2520entity%2520can%2520prevent%2520or%2520control%2520access%2520to%2520training%2520data%2520or%250Ainfluence%2520financial%2520benefits%252C%2520as%2520coordination%2520and%2520reward%2520distribution%2520are%250Amanaged%2520on%2520the%2520public%2520blockchain%2520with%2520an%2520immutable%2520record.%2520The%2520framework%250Asupports%2520effective%2520AI%2520training%2520by%2520allowing%2520Participants%2520to%2520maintain%2520control%250Aover%2520their%2520data%252C%2520benefit%2520financially%252C%2520and%2520contribute%2520to%2520a%2520decentralized%252C%250Ascalable%2520ecosystem%2520that%2520leverages%2520collective%2520AI%2520to%2520develop%2520beneficial%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02461v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Intelligence%20Network%20%28DIN%29&entry.906535625=Abraham%20Nash&entry.1292438233=%20%20Decentralized%20Intelligence%20Network%20%28DIN%29%20is%20a%20theoretical%20framework%20designed%0Ato%20address%20challenges%20in%20AI%20development%2C%20particularly%20focusing%20on%20data%0Afragmentation%20and%20siloing%20issues.%20It%20facilitates%20effective%20AI%20training%20within%0Asovereign%20data%20networks%20by%20overcoming%20barriers%20to%20accessing%20diverse%20data%0Asources%2C%20leveraging%3A%201%29%20personal%20data%20stores%20to%20ensure%20data%20sovereignty%2C%20where%0Adata%20remains%20securely%20within%20Participants%27%20control%3B%202%29%20a%20scalable%20federated%0Alearning%20protocol%20implemented%20on%20a%20public%20blockchain%20for%20decentralized%20AI%0Atraining%2C%20where%20only%20model%20parameter%20updates%20are%20shared%2C%20keeping%20data%20within%0Athe%20personal%20data%20stores%3B%20and%203%29%20a%20scalable%2C%20trustless%20cryptographic%20rewards%0Amechanism%20on%20a%20public%20blockchain%20to%20incentivize%20participation%20and%20ensure%20fair%0Areward%20distribution%20through%20a%20decentralized%20auditing%20protocol.%20This%20approach%0Aguarantees%20that%20no%20entity%20can%20prevent%20or%20control%20access%20to%20training%20data%20or%0Ainfluence%20financial%20benefits%2C%20as%20coordination%20and%20reward%20distribution%20are%0Amanaged%20on%20the%20public%20blockchain%20with%20an%20immutable%20record.%20The%20framework%0Asupports%20effective%20AI%20training%20by%20allowing%20Participants%20to%20maintain%20control%0Aover%20their%20data%2C%20benefit%20financially%2C%20and%20contribute%20to%20a%20decentralized%2C%0Ascalable%20ecosystem%20that%20leverages%20collective%20AI%20to%20develop%20beneficial%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02461v5&entry.124074799=Read"},
{"title": "Regularized Multi-output Gaussian Convolution Process with Domain\n  Adaptation", "author": "Wang Xinming and Wang Chao and Song Xuan and Kirby Levi and Wu Jianguo", "abstract": "  Multi-output Gaussian process (MGP) has been attracting increasing attention\nas a transfer learning method to model multiple outputs. Despite its high\nflexibility and generality, MGP still faces two critical challenges when\napplied to transfer learning. The first one is negative transfer, which occurs\nwhen there exists no shared information among the outputs. The second challenge\nis the input domain inconsistency, which is commonly studied in transfer\nlearning yet not explored in MGP. In this paper, we propose a regularized MGP\nmodeling framework with domain adaptation to overcome these challenges. More\nspecifically, a sparse covariance matrix of MGP is proposed by using\nconvolution process, where penalization terms are added to adaptively select\nthe most informative outputs for knowledge transfer. To deal with the domain\ninconsistency, a domain adaptation method is proposed by marginalizing\ninconsistent features and expanding missing features to align the input domains\namong different outputs. Statistical properties of the proposed method are\nprovided to guarantee the performance practically and asymptotically. The\nproposed framework outperforms state-of-the-art benchmarks in comprehensive\nsimulation studies and one real case study of a ceramic manufacturing process.\nThe results demonstrate the effectiveness of our method in dealing with both\nthe negative transfer and the domain inconsistency.\n", "link": "http://arxiv.org/abs/2409.02778v1", "date": "2024-09-04", "relevancy": 2.0256, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Multi-output%20Gaussian%20Convolution%20Process%20with%20Domain%0A%20%20Adaptation&body=Title%3A%20Regularized%20Multi-output%20Gaussian%20Convolution%20Process%20with%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Wang%20Xinming%20and%20Wang%20Chao%20and%20Song%20Xuan%20and%20Kirby%20Levi%20and%20Wu%20Jianguo%0AAbstract%3A%20%20%20Multi-output%20Gaussian%20process%20%28MGP%29%20has%20been%20attracting%20increasing%20attention%0Aas%20a%20transfer%20learning%20method%20to%20model%20multiple%20outputs.%20Despite%20its%20high%0Aflexibility%20and%20generality%2C%20MGP%20still%20faces%20two%20critical%20challenges%20when%0Aapplied%20to%20transfer%20learning.%20The%20first%20one%20is%20negative%20transfer%2C%20which%20occurs%0Awhen%20there%20exists%20no%20shared%20information%20among%20the%20outputs.%20The%20second%20challenge%0Ais%20the%20input%20domain%20inconsistency%2C%20which%20is%20commonly%20studied%20in%20transfer%0Alearning%20yet%20not%20explored%20in%20MGP.%20In%20this%20paper%2C%20we%20propose%20a%20regularized%20MGP%0Amodeling%20framework%20with%20domain%20adaptation%20to%20overcome%20these%20challenges.%20More%0Aspecifically%2C%20a%20sparse%20covariance%20matrix%20of%20MGP%20is%20proposed%20by%20using%0Aconvolution%20process%2C%20where%20penalization%20terms%20are%20added%20to%20adaptively%20select%0Athe%20most%20informative%20outputs%20for%20knowledge%20transfer.%20To%20deal%20with%20the%20domain%0Ainconsistency%2C%20a%20domain%20adaptation%20method%20is%20proposed%20by%20marginalizing%0Ainconsistent%20features%20and%20expanding%20missing%20features%20to%20align%20the%20input%20domains%0Aamong%20different%20outputs.%20Statistical%20properties%20of%20the%20proposed%20method%20are%0Aprovided%20to%20guarantee%20the%20performance%20practically%20and%20asymptotically.%20The%0Aproposed%20framework%20outperforms%20state-of-the-art%20benchmarks%20in%20comprehensive%0Asimulation%20studies%20and%20one%20real%20case%20study%20of%20a%20ceramic%20manufacturing%20process.%0AThe%20results%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20dealing%20with%20both%0Athe%20negative%20transfer%20and%20the%20domain%20inconsistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Multi-output%2520Gaussian%2520Convolution%2520Process%2520with%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DWang%2520Xinming%2520and%2520Wang%2520Chao%2520and%2520Song%2520Xuan%2520and%2520Kirby%2520Levi%2520and%2520Wu%2520Jianguo%26entry.1292438233%3D%2520%2520Multi-output%2520Gaussian%2520process%2520%2528MGP%2529%2520has%2520been%2520attracting%2520increasing%2520attention%250Aas%2520a%2520transfer%2520learning%2520method%2520to%2520model%2520multiple%2520outputs.%2520Despite%2520its%2520high%250Aflexibility%2520and%2520generality%252C%2520MGP%2520still%2520faces%2520two%2520critical%2520challenges%2520when%250Aapplied%2520to%2520transfer%2520learning.%2520The%2520first%2520one%2520is%2520negative%2520transfer%252C%2520which%2520occurs%250Awhen%2520there%2520exists%2520no%2520shared%2520information%2520among%2520the%2520outputs.%2520The%2520second%2520challenge%250Ais%2520the%2520input%2520domain%2520inconsistency%252C%2520which%2520is%2520commonly%2520studied%2520in%2520transfer%250Alearning%2520yet%2520not%2520explored%2520in%2520MGP.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520regularized%2520MGP%250Amodeling%2520framework%2520with%2520domain%2520adaptation%2520to%2520overcome%2520these%2520challenges.%2520More%250Aspecifically%252C%2520a%2520sparse%2520covariance%2520matrix%2520of%2520MGP%2520is%2520proposed%2520by%2520using%250Aconvolution%2520process%252C%2520where%2520penalization%2520terms%2520are%2520added%2520to%2520adaptively%2520select%250Athe%2520most%2520informative%2520outputs%2520for%2520knowledge%2520transfer.%2520To%2520deal%2520with%2520the%2520domain%250Ainconsistency%252C%2520a%2520domain%2520adaptation%2520method%2520is%2520proposed%2520by%2520marginalizing%250Ainconsistent%2520features%2520and%2520expanding%2520missing%2520features%2520to%2520align%2520the%2520input%2520domains%250Aamong%2520different%2520outputs.%2520Statistical%2520properties%2520of%2520the%2520proposed%2520method%2520are%250Aprovided%2520to%2520guarantee%2520the%2520performance%2520practically%2520and%2520asymptotically.%2520The%250Aproposed%2520framework%2520outperforms%2520state-of-the-art%2520benchmarks%2520in%2520comprehensive%250Asimulation%2520studies%2520and%2520one%2520real%2520case%2520study%2520of%2520a%2520ceramic%2520manufacturing%2520process.%250AThe%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520dealing%2520with%2520both%250Athe%2520negative%2520transfer%2520and%2520the%2520domain%2520inconsistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Multi-output%20Gaussian%20Convolution%20Process%20with%20Domain%0A%20%20Adaptation&entry.906535625=Wang%20Xinming%20and%20Wang%20Chao%20and%20Song%20Xuan%20and%20Kirby%20Levi%20and%20Wu%20Jianguo&entry.1292438233=%20%20Multi-output%20Gaussian%20process%20%28MGP%29%20has%20been%20attracting%20increasing%20attention%0Aas%20a%20transfer%20learning%20method%20to%20model%20multiple%20outputs.%20Despite%20its%20high%0Aflexibility%20and%20generality%2C%20MGP%20still%20faces%20two%20critical%20challenges%20when%0Aapplied%20to%20transfer%20learning.%20The%20first%20one%20is%20negative%20transfer%2C%20which%20occurs%0Awhen%20there%20exists%20no%20shared%20information%20among%20the%20outputs.%20The%20second%20challenge%0Ais%20the%20input%20domain%20inconsistency%2C%20which%20is%20commonly%20studied%20in%20transfer%0Alearning%20yet%20not%20explored%20in%20MGP.%20In%20this%20paper%2C%20we%20propose%20a%20regularized%20MGP%0Amodeling%20framework%20with%20domain%20adaptation%20to%20overcome%20these%20challenges.%20More%0Aspecifically%2C%20a%20sparse%20covariance%20matrix%20of%20MGP%20is%20proposed%20by%20using%0Aconvolution%20process%2C%20where%20penalization%20terms%20are%20added%20to%20adaptively%20select%0Athe%20most%20informative%20outputs%20for%20knowledge%20transfer.%20To%20deal%20with%20the%20domain%0Ainconsistency%2C%20a%20domain%20adaptation%20method%20is%20proposed%20by%20marginalizing%0Ainconsistent%20features%20and%20expanding%20missing%20features%20to%20align%20the%20input%20domains%0Aamong%20different%20outputs.%20Statistical%20properties%20of%20the%20proposed%20method%20are%0Aprovided%20to%20guarantee%20the%20performance%20practically%20and%20asymptotically.%20The%0Aproposed%20framework%20outperforms%20state-of-the-art%20benchmarks%20in%20comprehensive%0Asimulation%20studies%20and%20one%20real%20case%20study%20of%20a%20ceramic%20manufacturing%20process.%0AThe%20results%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20dealing%20with%20both%0Athe%20negative%20transfer%20and%20the%20domain%20inconsistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02778v1&entry.124074799=Read"},
{"title": "Simultaneous Training of First- and Second-Order Optimizers in\n  Population-Based Reinforcement Learning", "author": "Felix Pfeiffer and Shahram Eivazi", "abstract": "  The tuning of hyperparameters in reinforcement learning (RL) is critical, as\nthese parameters significantly impact an agent's performance and learning\nefficiency. Dynamic adjustment of hyperparameters during the training process\ncan significantly enhance both the performance and stability of learning.\nPopulation-based training (PBT) provides a method to achieve this by\ncontinuously tuning hyperparameters throughout the training. This ongoing\nadjustment enables models to adapt to different learning stages, resulting in\nfaster convergence and overall improved performance. In this paper, we propose\nan enhancement to PBT by simultaneously utilizing both first- and second-order\noptimizers within a single population. We conducted a series of experiments\nusing the TD3 algorithm across various MuJoCo environments. Our results, for\nthe first time, empirically demonstrate the potential of incorporating\nsecond-order optimizers within PBT-based RL. Specifically, the combination of\nthe K-FAC optimizer with Adam led to up to a 10% improvement in overall\nperformance compared to PBT using only Adam. Additionally, in environments\nwhere Adam occasionally fails, such as the Swimmer environment, the mixed\npopulation with K-FAC exhibited more reliable learning outcomes, offering a\nsignificant advantage in training stability without a substantial increase in\ncomputational time.\n", "link": "http://arxiv.org/abs/2408.15421v2", "date": "2024-09-04", "relevancy": 2.0197, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Training%20of%20First-%20and%20Second-Order%20Optimizers%20in%0A%20%20Population-Based%20Reinforcement%20Learning&body=Title%3A%20Simultaneous%20Training%20of%20First-%20and%20Second-Order%20Optimizers%20in%0A%20%20Population-Based%20Reinforcement%20Learning%0AAuthor%3A%20Felix%20Pfeiffer%20and%20Shahram%20Eivazi%0AAbstract%3A%20%20%20The%20tuning%20of%20hyperparameters%20in%20reinforcement%20learning%20%28RL%29%20is%20critical%2C%20as%0Athese%20parameters%20significantly%20impact%20an%20agent%27s%20performance%20and%20learning%0Aefficiency.%20Dynamic%20adjustment%20of%20hyperparameters%20during%20the%20training%20process%0Acan%20significantly%20enhance%20both%20the%20performance%20and%20stability%20of%20learning.%0APopulation-based%20training%20%28PBT%29%20provides%20a%20method%20to%20achieve%20this%20by%0Acontinuously%20tuning%20hyperparameters%20throughout%20the%20training.%20This%20ongoing%0Aadjustment%20enables%20models%20to%20adapt%20to%20different%20learning%20stages%2C%20resulting%20in%0Afaster%20convergence%20and%20overall%20improved%20performance.%20In%20this%20paper%2C%20we%20propose%0Aan%20enhancement%20to%20PBT%20by%20simultaneously%20utilizing%20both%20first-%20and%20second-order%0Aoptimizers%20within%20a%20single%20population.%20We%20conducted%20a%20series%20of%20experiments%0Ausing%20the%20TD3%20algorithm%20across%20various%20MuJoCo%20environments.%20Our%20results%2C%20for%0Athe%20first%20time%2C%20empirically%20demonstrate%20the%20potential%20of%20incorporating%0Asecond-order%20optimizers%20within%20PBT-based%20RL.%20Specifically%2C%20the%20combination%20of%0Athe%20K-FAC%20optimizer%20with%20Adam%20led%20to%20up%20to%20a%2010%25%20improvement%20in%20overall%0Aperformance%20compared%20to%20PBT%20using%20only%20Adam.%20Additionally%2C%20in%20environments%0Awhere%20Adam%20occasionally%20fails%2C%20such%20as%20the%20Swimmer%20environment%2C%20the%20mixed%0Apopulation%20with%20K-FAC%20exhibited%20more%20reliable%20learning%20outcomes%2C%20offering%20a%0Asignificant%20advantage%20in%20training%20stability%20without%20a%20substantial%20increase%20in%0Acomputational%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Training%2520of%2520First-%2520and%2520Second-Order%2520Optimizers%2520in%250A%2520%2520Population-Based%2520Reinforcement%2520Learning%26entry.906535625%3DFelix%2520Pfeiffer%2520and%2520Shahram%2520Eivazi%26entry.1292438233%3D%2520%2520The%2520tuning%2520of%2520hyperparameters%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520critical%252C%2520as%250Athese%2520parameters%2520significantly%2520impact%2520an%2520agent%2527s%2520performance%2520and%2520learning%250Aefficiency.%2520Dynamic%2520adjustment%2520of%2520hyperparameters%2520during%2520the%2520training%2520process%250Acan%2520significantly%2520enhance%2520both%2520the%2520performance%2520and%2520stability%2520of%2520learning.%250APopulation-based%2520training%2520%2528PBT%2529%2520provides%2520a%2520method%2520to%2520achieve%2520this%2520by%250Acontinuously%2520tuning%2520hyperparameters%2520throughout%2520the%2520training.%2520This%2520ongoing%250Aadjustment%2520enables%2520models%2520to%2520adapt%2520to%2520different%2520learning%2520stages%252C%2520resulting%2520in%250Afaster%2520convergence%2520and%2520overall%2520improved%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aan%2520enhancement%2520to%2520PBT%2520by%2520simultaneously%2520utilizing%2520both%2520first-%2520and%2520second-order%250Aoptimizers%2520within%2520a%2520single%2520population.%2520We%2520conducted%2520a%2520series%2520of%2520experiments%250Ausing%2520the%2520TD3%2520algorithm%2520across%2520various%2520MuJoCo%2520environments.%2520Our%2520results%252C%2520for%250Athe%2520first%2520time%252C%2520empirically%2520demonstrate%2520the%2520potential%2520of%2520incorporating%250Asecond-order%2520optimizers%2520within%2520PBT-based%2520RL.%2520Specifically%252C%2520the%2520combination%2520of%250Athe%2520K-FAC%2520optimizer%2520with%2520Adam%2520led%2520to%2520up%2520to%2520a%252010%2525%2520improvement%2520in%2520overall%250Aperformance%2520compared%2520to%2520PBT%2520using%2520only%2520Adam.%2520Additionally%252C%2520in%2520environments%250Awhere%2520Adam%2520occasionally%2520fails%252C%2520such%2520as%2520the%2520Swimmer%2520environment%252C%2520the%2520mixed%250Apopulation%2520with%2520K-FAC%2520exhibited%2520more%2520reliable%2520learning%2520outcomes%252C%2520offering%2520a%250Asignificant%2520advantage%2520in%2520training%2520stability%2520without%2520a%2520substantial%2520increase%2520in%250Acomputational%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Training%20of%20First-%20and%20Second-Order%20Optimizers%20in%0A%20%20Population-Based%20Reinforcement%20Learning&entry.906535625=Felix%20Pfeiffer%20and%20Shahram%20Eivazi&entry.1292438233=%20%20The%20tuning%20of%20hyperparameters%20in%20reinforcement%20learning%20%28RL%29%20is%20critical%2C%20as%0Athese%20parameters%20significantly%20impact%20an%20agent%27s%20performance%20and%20learning%0Aefficiency.%20Dynamic%20adjustment%20of%20hyperparameters%20during%20the%20training%20process%0Acan%20significantly%20enhance%20both%20the%20performance%20and%20stability%20of%20learning.%0APopulation-based%20training%20%28PBT%29%20provides%20a%20method%20to%20achieve%20this%20by%0Acontinuously%20tuning%20hyperparameters%20throughout%20the%20training.%20This%20ongoing%0Aadjustment%20enables%20models%20to%20adapt%20to%20different%20learning%20stages%2C%20resulting%20in%0Afaster%20convergence%20and%20overall%20improved%20performance.%20In%20this%20paper%2C%20we%20propose%0Aan%20enhancement%20to%20PBT%20by%20simultaneously%20utilizing%20both%20first-%20and%20second-order%0Aoptimizers%20within%20a%20single%20population.%20We%20conducted%20a%20series%20of%20experiments%0Ausing%20the%20TD3%20algorithm%20across%20various%20MuJoCo%20environments.%20Our%20results%2C%20for%0Athe%20first%20time%2C%20empirically%20demonstrate%20the%20potential%20of%20incorporating%0Asecond-order%20optimizers%20within%20PBT-based%20RL.%20Specifically%2C%20the%20combination%20of%0Athe%20K-FAC%20optimizer%20with%20Adam%20led%20to%20up%20to%20a%2010%25%20improvement%20in%20overall%0Aperformance%20compared%20to%20PBT%20using%20only%20Adam.%20Additionally%2C%20in%20environments%0Awhere%20Adam%20occasionally%20fails%2C%20such%20as%20the%20Swimmer%20environment%2C%20the%20mixed%0Apopulation%20with%20K-FAC%20exhibited%20more%20reliable%20learning%20outcomes%2C%20offering%20a%0Asignificant%20advantage%20in%20training%20stability%20without%20a%20substantial%20increase%20in%0Acomputational%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15421v2&entry.124074799=Read"},
{"title": "CONClave -- Secure and Robust Cooperative Perception for CAVs Using\n  Authenticated Consensus and Trust Scoring", "author": "Edward Andert and Francis Mendoza and Hans Walter Behrens and Aviral Shrivastava", "abstract": "  Connected Autonomous Vehicles have great potential to improve automobile\nsafety and traffic flow, especially in cooperative applications where\nperception data is shared between vehicles. However, this cooperation must be\nsecured from malicious intent and unintentional errors that could cause\naccidents. Previous works typically address singular security or reliability\nissues for cooperative driving in specific scenarios rather than the set of\nerrors together. In this paper, we propose CONClave, a tightly coupled\nauthentication, consensus, and trust scoring mechanism that provides\ncomprehensive security and reliability for cooperative perception in autonomous\nvehicles. CONClave benefits from the pipelined nature of the steps such that\nfaults can be detected significantly faster and with less compute. Overall,\nCONClave shows huge promise in preventing security flaws, detecting even\nrelatively minor sensing faults, and increasing the robustness and accuracy of\ncooperative perception in CAVs while adding minimal overhead.\n", "link": "http://arxiv.org/abs/2409.02863v1", "date": "2024-09-04", "relevancy": 2.0161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5507}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CONClave%20--%20Secure%20and%20Robust%20Cooperative%20Perception%20for%20CAVs%20Using%0A%20%20Authenticated%20Consensus%20and%20Trust%20Scoring&body=Title%3A%20CONClave%20--%20Secure%20and%20Robust%20Cooperative%20Perception%20for%20CAVs%20Using%0A%20%20Authenticated%20Consensus%20and%20Trust%20Scoring%0AAuthor%3A%20Edward%20Andert%20and%20Francis%20Mendoza%20and%20Hans%20Walter%20Behrens%20and%20Aviral%20Shrivastava%0AAbstract%3A%20%20%20Connected%20Autonomous%20Vehicles%20have%20great%20potential%20to%20improve%20automobile%0Asafety%20and%20traffic%20flow%2C%20especially%20in%20cooperative%20applications%20where%0Aperception%20data%20is%20shared%20between%20vehicles.%20However%2C%20this%20cooperation%20must%20be%0Asecured%20from%20malicious%20intent%20and%20unintentional%20errors%20that%20could%20cause%0Aaccidents.%20Previous%20works%20typically%20address%20singular%20security%20or%20reliability%0Aissues%20for%20cooperative%20driving%20in%20specific%20scenarios%20rather%20than%20the%20set%20of%0Aerrors%20together.%20In%20this%20paper%2C%20we%20propose%20CONClave%2C%20a%20tightly%20coupled%0Aauthentication%2C%20consensus%2C%20and%20trust%20scoring%20mechanism%20that%20provides%0Acomprehensive%20security%20and%20reliability%20for%20cooperative%20perception%20in%20autonomous%0Avehicles.%20CONClave%20benefits%20from%20the%20pipelined%20nature%20of%20the%20steps%20such%20that%0Afaults%20can%20be%20detected%20significantly%20faster%20and%20with%20less%20compute.%20Overall%2C%0ACONClave%20shows%20huge%20promise%20in%20preventing%20security%20flaws%2C%20detecting%20even%0Arelatively%20minor%20sensing%20faults%2C%20and%20increasing%20the%20robustness%20and%20accuracy%20of%0Acooperative%20perception%20in%20CAVs%20while%20adding%20minimal%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCONClave%2520--%2520Secure%2520and%2520Robust%2520Cooperative%2520Perception%2520for%2520CAVs%2520Using%250A%2520%2520Authenticated%2520Consensus%2520and%2520Trust%2520Scoring%26entry.906535625%3DEdward%2520Andert%2520and%2520Francis%2520Mendoza%2520and%2520Hans%2520Walter%2520Behrens%2520and%2520Aviral%2520Shrivastava%26entry.1292438233%3D%2520%2520Connected%2520Autonomous%2520Vehicles%2520have%2520great%2520potential%2520to%2520improve%2520automobile%250Asafety%2520and%2520traffic%2520flow%252C%2520especially%2520in%2520cooperative%2520applications%2520where%250Aperception%2520data%2520is%2520shared%2520between%2520vehicles.%2520However%252C%2520this%2520cooperation%2520must%2520be%250Asecured%2520from%2520malicious%2520intent%2520and%2520unintentional%2520errors%2520that%2520could%2520cause%250Aaccidents.%2520Previous%2520works%2520typically%2520address%2520singular%2520security%2520or%2520reliability%250Aissues%2520for%2520cooperative%2520driving%2520in%2520specific%2520scenarios%2520rather%2520than%2520the%2520set%2520of%250Aerrors%2520together.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CONClave%252C%2520a%2520tightly%2520coupled%250Aauthentication%252C%2520consensus%252C%2520and%2520trust%2520scoring%2520mechanism%2520that%2520provides%250Acomprehensive%2520security%2520and%2520reliability%2520for%2520cooperative%2520perception%2520in%2520autonomous%250Avehicles.%2520CONClave%2520benefits%2520from%2520the%2520pipelined%2520nature%2520of%2520the%2520steps%2520such%2520that%250Afaults%2520can%2520be%2520detected%2520significantly%2520faster%2520and%2520with%2520less%2520compute.%2520Overall%252C%250ACONClave%2520shows%2520huge%2520promise%2520in%2520preventing%2520security%2520flaws%252C%2520detecting%2520even%250Arelatively%2520minor%2520sensing%2520faults%252C%2520and%2520increasing%2520the%2520robustness%2520and%2520accuracy%2520of%250Acooperative%2520perception%2520in%2520CAVs%2520while%2520adding%2520minimal%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CONClave%20--%20Secure%20and%20Robust%20Cooperative%20Perception%20for%20CAVs%20Using%0A%20%20Authenticated%20Consensus%20and%20Trust%20Scoring&entry.906535625=Edward%20Andert%20and%20Francis%20Mendoza%20and%20Hans%20Walter%20Behrens%20and%20Aviral%20Shrivastava&entry.1292438233=%20%20Connected%20Autonomous%20Vehicles%20have%20great%20potential%20to%20improve%20automobile%0Asafety%20and%20traffic%20flow%2C%20especially%20in%20cooperative%20applications%20where%0Aperception%20data%20is%20shared%20between%20vehicles.%20However%2C%20this%20cooperation%20must%20be%0Asecured%20from%20malicious%20intent%20and%20unintentional%20errors%20that%20could%20cause%0Aaccidents.%20Previous%20works%20typically%20address%20singular%20security%20or%20reliability%0Aissues%20for%20cooperative%20driving%20in%20specific%20scenarios%20rather%20than%20the%20set%20of%0Aerrors%20together.%20In%20this%20paper%2C%20we%20propose%20CONClave%2C%20a%20tightly%20coupled%0Aauthentication%2C%20consensus%2C%20and%20trust%20scoring%20mechanism%20that%20provides%0Acomprehensive%20security%20and%20reliability%20for%20cooperative%20perception%20in%20autonomous%0Avehicles.%20CONClave%20benefits%20from%20the%20pipelined%20nature%20of%20the%20steps%20such%20that%0Afaults%20can%20be%20detected%20significantly%20faster%20and%20with%20less%20compute.%20Overall%2C%0ACONClave%20shows%20huge%20promise%20in%20preventing%20security%20flaws%2C%20detecting%20even%0Arelatively%20minor%20sensing%20faults%2C%20and%20increasing%20the%20robustness%20and%20accuracy%20of%0Acooperative%20perception%20in%20CAVs%20while%20adding%20minimal%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02863v1&entry.124074799=Read"},
{"title": "Pseudo Replay-based Class Continual Learning for Online New Category\n  Anomaly Detection in Additive Manufacturing", "author": "Yuxuan Li and Tianxin Xie and Chenang Liu and Zhangyue Shi", "abstract": "  The incorporation of advanced sensors and machine learning techniques has\nenabled modern manufacturing enterprises to perform data-driven\nclassification-based anomaly detection based on the sensor data collected in\nmanufacturing processes. However, one critical challenge is that newly\npresented defect category may manifest as the manufacturing process continues,\nresulting in monitoring performance deterioration of previously trained machine\nlearning models. Hence, there is an increasing need for empowering machine\nlearning models to learn continually. Among all continual learning methods,\nmemory-based continual learning has the best performance but faces the\nconstraints of data storage capacity. To address this issue, this paper\ndevelops a novel pseudo replay-based continual learning framework by\nintegrating class incremental learning and oversampling-based data generation.\nWithout storing all the data, the developed framework could generate\nhigh-quality data representing previous classes to train machine learning model\nincrementally when new category anomaly occurs. In addition, it could even\nenhance the monitoring performance since it also effectively improves the data\nquality. The effectiveness of the proposed framework is validated in three\ncases studies, which leverages supervised classification problem for anomaly\ndetection. The experimental results show that the developed method is very\npromising in detecting novel anomaly while maintaining a good performance on\nthe previous task and brings up more flexibility in model architecture.\n", "link": "http://arxiv.org/abs/2312.02491v2", "date": "2024-09-04", "relevancy": 2.0012, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo%20Replay-based%20Class%20Continual%20Learning%20for%20Online%20New%20Category%0A%20%20Anomaly%20Detection%20in%20Additive%20Manufacturing&body=Title%3A%20Pseudo%20Replay-based%20Class%20Continual%20Learning%20for%20Online%20New%20Category%0A%20%20Anomaly%20Detection%20in%20Additive%20Manufacturing%0AAuthor%3A%20Yuxuan%20Li%20and%20Tianxin%20Xie%20and%20Chenang%20Liu%20and%20Zhangyue%20Shi%0AAbstract%3A%20%20%20The%20incorporation%20of%20advanced%20sensors%20and%20machine%20learning%20techniques%20has%0Aenabled%20modern%20manufacturing%20enterprises%20to%20perform%20data-driven%0Aclassification-based%20anomaly%20detection%20based%20on%20the%20sensor%20data%20collected%20in%0Amanufacturing%20processes.%20However%2C%20one%20critical%20challenge%20is%20that%20newly%0Apresented%20defect%20category%20may%20manifest%20as%20the%20manufacturing%20process%20continues%2C%0Aresulting%20in%20monitoring%20performance%20deterioration%20of%20previously%20trained%20machine%0Alearning%20models.%20Hence%2C%20there%20is%20an%20increasing%20need%20for%20empowering%20machine%0Alearning%20models%20to%20learn%20continually.%20Among%20all%20continual%20learning%20methods%2C%0Amemory-based%20continual%20learning%20has%20the%20best%20performance%20but%20faces%20the%0Aconstraints%20of%20data%20storage%20capacity.%20To%20address%20this%20issue%2C%20this%20paper%0Adevelops%20a%20novel%20pseudo%20replay-based%20continual%20learning%20framework%20by%0Aintegrating%20class%20incremental%20learning%20and%20oversampling-based%20data%20generation.%0AWithout%20storing%20all%20the%20data%2C%20the%20developed%20framework%20could%20generate%0Ahigh-quality%20data%20representing%20previous%20classes%20to%20train%20machine%20learning%20model%0Aincrementally%20when%20new%20category%20anomaly%20occurs.%20In%20addition%2C%20it%20could%20even%0Aenhance%20the%20monitoring%20performance%20since%20it%20also%20effectively%20improves%20the%20data%0Aquality.%20The%20effectiveness%20of%20the%20proposed%20framework%20is%20validated%20in%20three%0Acases%20studies%2C%20which%20leverages%20supervised%20classification%20problem%20for%20anomaly%0Adetection.%20The%20experimental%20results%20show%20that%20the%20developed%20method%20is%20very%0Apromising%20in%20detecting%20novel%20anomaly%20while%20maintaining%20a%20good%20performance%20on%0Athe%20previous%20task%20and%20brings%20up%20more%20flexibility%20in%20model%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo%2520Replay-based%2520Class%2520Continual%2520Learning%2520for%2520Online%2520New%2520Category%250A%2520%2520Anomaly%2520Detection%2520in%2520Additive%2520Manufacturing%26entry.906535625%3DYuxuan%2520Li%2520and%2520Tianxin%2520Xie%2520and%2520Chenang%2520Liu%2520and%2520Zhangyue%2520Shi%26entry.1292438233%3D%2520%2520The%2520incorporation%2520of%2520advanced%2520sensors%2520and%2520machine%2520learning%2520techniques%2520has%250Aenabled%2520modern%2520manufacturing%2520enterprises%2520to%2520perform%2520data-driven%250Aclassification-based%2520anomaly%2520detection%2520based%2520on%2520the%2520sensor%2520data%2520collected%2520in%250Amanufacturing%2520processes.%2520However%252C%2520one%2520critical%2520challenge%2520is%2520that%2520newly%250Apresented%2520defect%2520category%2520may%2520manifest%2520as%2520the%2520manufacturing%2520process%2520continues%252C%250Aresulting%2520in%2520monitoring%2520performance%2520deterioration%2520of%2520previously%2520trained%2520machine%250Alearning%2520models.%2520Hence%252C%2520there%2520is%2520an%2520increasing%2520need%2520for%2520empowering%2520machine%250Alearning%2520models%2520to%2520learn%2520continually.%2520Among%2520all%2520continual%2520learning%2520methods%252C%250Amemory-based%2520continual%2520learning%2520has%2520the%2520best%2520performance%2520but%2520faces%2520the%250Aconstraints%2520of%2520data%2520storage%2520capacity.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%250Adevelops%2520a%2520novel%2520pseudo%2520replay-based%2520continual%2520learning%2520framework%2520by%250Aintegrating%2520class%2520incremental%2520learning%2520and%2520oversampling-based%2520data%2520generation.%250AWithout%2520storing%2520all%2520the%2520data%252C%2520the%2520developed%2520framework%2520could%2520generate%250Ahigh-quality%2520data%2520representing%2520previous%2520classes%2520to%2520train%2520machine%2520learning%2520model%250Aincrementally%2520when%2520new%2520category%2520anomaly%2520occurs.%2520In%2520addition%252C%2520it%2520could%2520even%250Aenhance%2520the%2520monitoring%2520performance%2520since%2520it%2520also%2520effectively%2520improves%2520the%2520data%250Aquality.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520framework%2520is%2520validated%2520in%2520three%250Acases%2520studies%252C%2520which%2520leverages%2520supervised%2520classification%2520problem%2520for%2520anomaly%250Adetection.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520developed%2520method%2520is%2520very%250Apromising%2520in%2520detecting%2520novel%2520anomaly%2520while%2520maintaining%2520a%2520good%2520performance%2520on%250Athe%2520previous%2520task%2520and%2520brings%2520up%2520more%2520flexibility%2520in%2520model%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo%20Replay-based%20Class%20Continual%20Learning%20for%20Online%20New%20Category%0A%20%20Anomaly%20Detection%20in%20Additive%20Manufacturing&entry.906535625=Yuxuan%20Li%20and%20Tianxin%20Xie%20and%20Chenang%20Liu%20and%20Zhangyue%20Shi&entry.1292438233=%20%20The%20incorporation%20of%20advanced%20sensors%20and%20machine%20learning%20techniques%20has%0Aenabled%20modern%20manufacturing%20enterprises%20to%20perform%20data-driven%0Aclassification-based%20anomaly%20detection%20based%20on%20the%20sensor%20data%20collected%20in%0Amanufacturing%20processes.%20However%2C%20one%20critical%20challenge%20is%20that%20newly%0Apresented%20defect%20category%20may%20manifest%20as%20the%20manufacturing%20process%20continues%2C%0Aresulting%20in%20monitoring%20performance%20deterioration%20of%20previously%20trained%20machine%0Alearning%20models.%20Hence%2C%20there%20is%20an%20increasing%20need%20for%20empowering%20machine%0Alearning%20models%20to%20learn%20continually.%20Among%20all%20continual%20learning%20methods%2C%0Amemory-based%20continual%20learning%20has%20the%20best%20performance%20but%20faces%20the%0Aconstraints%20of%20data%20storage%20capacity.%20To%20address%20this%20issue%2C%20this%20paper%0Adevelops%20a%20novel%20pseudo%20replay-based%20continual%20learning%20framework%20by%0Aintegrating%20class%20incremental%20learning%20and%20oversampling-based%20data%20generation.%0AWithout%20storing%20all%20the%20data%2C%20the%20developed%20framework%20could%20generate%0Ahigh-quality%20data%20representing%20previous%20classes%20to%20train%20machine%20learning%20model%0Aincrementally%20when%20new%20category%20anomaly%20occurs.%20In%20addition%2C%20it%20could%20even%0Aenhance%20the%20monitoring%20performance%20since%20it%20also%20effectively%20improves%20the%20data%0Aquality.%20The%20effectiveness%20of%20the%20proposed%20framework%20is%20validated%20in%20three%0Acases%20studies%2C%20which%20leverages%20supervised%20classification%20problem%20for%20anomaly%0Adetection.%20The%20experimental%20results%20show%20that%20the%20developed%20method%20is%20very%0Apromising%20in%20detecting%20novel%20anomaly%20while%20maintaining%20a%20good%20performance%20on%0Athe%20previous%20task%20and%20brings%20up%20more%20flexibility%20in%20model%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02491v2&entry.124074799=Read"},
{"title": "Rethinking HTG Evaluation: Bridging Generation and Recognition", "author": "Konstantina Nikolaidou and George Retsinas and Giorgos Sfikas and Marcus Liwicki", "abstract": "  The evaluation of generative models for natural image tasks has been\nextensively studied. Similar protocols and metrics are used in cases with\nunique particularities, such as Handwriting Generation, even if they might not\nbe completely appropriate. In this work, we introduce three measures tailored\nfor HTG evaluation, $ \\text{HTG}_{\\text{HTR}} $, $ \\text{HTG}_{\\text{style}} $,\nand $ \\text{HTG}_{\\text{OOV}} $, and argue that they are more expedient to\nevaluate the quality of generated handwritten images. The metrics rely on the\nrecognition error/accuracy of Handwriting Text Recognition and Writer\nIdentification models and emphasize writing style, textual content, and\ndiversity as the main aspects that adhere to the content of handwritten images.\nWe conduct comprehensive experiments on the IAM handwriting database,\nshowcasing that widely used metrics such as FID fail to properly quantify the\ndiversity and the practical utility of generated handwriting samples. Our\nfindings show that our metrics are richer in information and underscore the\nnecessity of standardized evaluation protocols in HTG. The proposed metrics\nprovide a more robust and informative protocol for assessing HTG quality,\ncontributing to improved performance in HTR. Code for the evaluation protocol\nis available at: https://github.com/koninik/HTG_evaluation.\n", "link": "http://arxiv.org/abs/2409.02683v1", "date": "2024-09-04", "relevancy": 1.9931, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5016}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4962}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20HTG%20Evaluation%3A%20Bridging%20Generation%20and%20Recognition&body=Title%3A%20Rethinking%20HTG%20Evaluation%3A%20Bridging%20Generation%20and%20Recognition%0AAuthor%3A%20Konstantina%20Nikolaidou%20and%20George%20Retsinas%20and%20Giorgos%20Sfikas%20and%20Marcus%20Liwicki%0AAbstract%3A%20%20%20The%20evaluation%20of%20generative%20models%20for%20natural%20image%20tasks%20has%20been%0Aextensively%20studied.%20Similar%20protocols%20and%20metrics%20are%20used%20in%20cases%20with%0Aunique%20particularities%2C%20such%20as%20Handwriting%20Generation%2C%20even%20if%20they%20might%20not%0Abe%20completely%20appropriate.%20In%20this%20work%2C%20we%20introduce%20three%20measures%20tailored%0Afor%20HTG%20evaluation%2C%20%24%20%5Ctext%7BHTG%7D_%7B%5Ctext%7BHTR%7D%7D%20%24%2C%20%24%20%5Ctext%7BHTG%7D_%7B%5Ctext%7Bstyle%7D%7D%20%24%2C%0Aand%20%24%20%5Ctext%7BHTG%7D_%7B%5Ctext%7BOOV%7D%7D%20%24%2C%20and%20argue%20that%20they%20are%20more%20expedient%20to%0Aevaluate%20the%20quality%20of%20generated%20handwritten%20images.%20The%20metrics%20rely%20on%20the%0Arecognition%20error/accuracy%20of%20Handwriting%20Text%20Recognition%20and%20Writer%0AIdentification%20models%20and%20emphasize%20writing%20style%2C%20textual%20content%2C%20and%0Adiversity%20as%20the%20main%20aspects%20that%20adhere%20to%20the%20content%20of%20handwritten%20images.%0AWe%20conduct%20comprehensive%20experiments%20on%20the%20IAM%20handwriting%20database%2C%0Ashowcasing%20that%20widely%20used%20metrics%20such%20as%20FID%20fail%20to%20properly%20quantify%20the%0Adiversity%20and%20the%20practical%20utility%20of%20generated%20handwriting%20samples.%20Our%0Afindings%20show%20that%20our%20metrics%20are%20richer%20in%20information%20and%20underscore%20the%0Anecessity%20of%20standardized%20evaluation%20protocols%20in%20HTG.%20The%20proposed%20metrics%0Aprovide%20a%20more%20robust%20and%20informative%20protocol%20for%20assessing%20HTG%20quality%2C%0Acontributing%20to%20improved%20performance%20in%20HTR.%20Code%20for%20the%20evaluation%20protocol%0Ais%20available%20at%3A%20https%3A//github.com/koninik/HTG_evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520HTG%2520Evaluation%253A%2520Bridging%2520Generation%2520and%2520Recognition%26entry.906535625%3DKonstantina%2520Nikolaidou%2520and%2520George%2520Retsinas%2520and%2520Giorgos%2520Sfikas%2520and%2520Marcus%2520Liwicki%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520generative%2520models%2520for%2520natural%2520image%2520tasks%2520has%2520been%250Aextensively%2520studied.%2520Similar%2520protocols%2520and%2520metrics%2520are%2520used%2520in%2520cases%2520with%250Aunique%2520particularities%252C%2520such%2520as%2520Handwriting%2520Generation%252C%2520even%2520if%2520they%2520might%2520not%250Abe%2520completely%2520appropriate.%2520In%2520this%2520work%252C%2520we%2520introduce%2520three%2520measures%2520tailored%250Afor%2520HTG%2520evaluation%252C%2520%2524%2520%255Ctext%257BHTG%257D_%257B%255Ctext%257BHTR%257D%257D%2520%2524%252C%2520%2524%2520%255Ctext%257BHTG%257D_%257B%255Ctext%257Bstyle%257D%257D%2520%2524%252C%250Aand%2520%2524%2520%255Ctext%257BHTG%257D_%257B%255Ctext%257BOOV%257D%257D%2520%2524%252C%2520and%2520argue%2520that%2520they%2520are%2520more%2520expedient%2520to%250Aevaluate%2520the%2520quality%2520of%2520generated%2520handwritten%2520images.%2520The%2520metrics%2520rely%2520on%2520the%250Arecognition%2520error/accuracy%2520of%2520Handwriting%2520Text%2520Recognition%2520and%2520Writer%250AIdentification%2520models%2520and%2520emphasize%2520writing%2520style%252C%2520textual%2520content%252C%2520and%250Adiversity%2520as%2520the%2520main%2520aspects%2520that%2520adhere%2520to%2520the%2520content%2520of%2520handwritten%2520images.%250AWe%2520conduct%2520comprehensive%2520experiments%2520on%2520the%2520IAM%2520handwriting%2520database%252C%250Ashowcasing%2520that%2520widely%2520used%2520metrics%2520such%2520as%2520FID%2520fail%2520to%2520properly%2520quantify%2520the%250Adiversity%2520and%2520the%2520practical%2520utility%2520of%2520generated%2520handwriting%2520samples.%2520Our%250Afindings%2520show%2520that%2520our%2520metrics%2520are%2520richer%2520in%2520information%2520and%2520underscore%2520the%250Anecessity%2520of%2520standardized%2520evaluation%2520protocols%2520in%2520HTG.%2520The%2520proposed%2520metrics%250Aprovide%2520a%2520more%2520robust%2520and%2520informative%2520protocol%2520for%2520assessing%2520HTG%2520quality%252C%250Acontributing%2520to%2520improved%2520performance%2520in%2520HTR.%2520Code%2520for%2520the%2520evaluation%2520protocol%250Ais%2520available%2520at%253A%2520https%253A//github.com/koninik/HTG_evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20HTG%20Evaluation%3A%20Bridging%20Generation%20and%20Recognition&entry.906535625=Konstantina%20Nikolaidou%20and%20George%20Retsinas%20and%20Giorgos%20Sfikas%20and%20Marcus%20Liwicki&entry.1292438233=%20%20The%20evaluation%20of%20generative%20models%20for%20natural%20image%20tasks%20has%20been%0Aextensively%20studied.%20Similar%20protocols%20and%20metrics%20are%20used%20in%20cases%20with%0Aunique%20particularities%2C%20such%20as%20Handwriting%20Generation%2C%20even%20if%20they%20might%20not%0Abe%20completely%20appropriate.%20In%20this%20work%2C%20we%20introduce%20three%20measures%20tailored%0Afor%20HTG%20evaluation%2C%20%24%20%5Ctext%7BHTG%7D_%7B%5Ctext%7BHTR%7D%7D%20%24%2C%20%24%20%5Ctext%7BHTG%7D_%7B%5Ctext%7Bstyle%7D%7D%20%24%2C%0Aand%20%24%20%5Ctext%7BHTG%7D_%7B%5Ctext%7BOOV%7D%7D%20%24%2C%20and%20argue%20that%20they%20are%20more%20expedient%20to%0Aevaluate%20the%20quality%20of%20generated%20handwritten%20images.%20The%20metrics%20rely%20on%20the%0Arecognition%20error/accuracy%20of%20Handwriting%20Text%20Recognition%20and%20Writer%0AIdentification%20models%20and%20emphasize%20writing%20style%2C%20textual%20content%2C%20and%0Adiversity%20as%20the%20main%20aspects%20that%20adhere%20to%20the%20content%20of%20handwritten%20images.%0AWe%20conduct%20comprehensive%20experiments%20on%20the%20IAM%20handwriting%20database%2C%0Ashowcasing%20that%20widely%20used%20metrics%20such%20as%20FID%20fail%20to%20properly%20quantify%20the%0Adiversity%20and%20the%20practical%20utility%20of%20generated%20handwriting%20samples.%20Our%0Afindings%20show%20that%20our%20metrics%20are%20richer%20in%20information%20and%20underscore%20the%0Anecessity%20of%20standardized%20evaluation%20protocols%20in%20HTG.%20The%20proposed%20metrics%0Aprovide%20a%20more%20robust%20and%20informative%20protocol%20for%20assessing%20HTG%20quality%2C%0Acontributing%20to%20improved%20performance%20in%20HTR.%20Code%20for%20the%20evaluation%20protocol%0Ais%20available%20at%3A%20https%3A//github.com/koninik/HTG_evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02683v1&entry.124074799=Read"},
{"title": "CanvOI, an Oncology Intelligence Foundation Model: Scaling FLOPS\n  Differently", "author": "Jonathan Zalach and Inbal Gazy and Assaf Avinoam and Ron Sinai and Eran Shmuel and Inbar Gilboa and Christine Swisher and Naim Matasci and Reva Basho and David B. Agus", "abstract": "  The rapidly evolving field of digital oncopathology faces significant\nchallenges, including the need to address diverse and complex clinical\nquestions, often involving rare conditions, with limited availability of\nlabeled data. These limitations hinder the development of robust AI-driven\ntools in the biomedical space, where accuracy in probabilistic determinations\nis of utmost importance. To address this, digital pathology foundation models\nhave begun to emerge, typically developed with the size and diversity of the\npre-training dataset and model parameters in mind. Here, we present CanvOI, a\nViT-g/10-based foundation model designed to enhance the capabilities of digital\npathology by addressing these challenges through a different approach.\nConsidering the unique nature of oncologic histopathological images and the\nrequirements from the embeddings to provide meaningful representations for\nMultiple Instance Learning (MIL) downstream models, we chose to modify the\ninput image characteristics. By introducing larger tile sizes (380 x 380\npixels) and smaller patch sizes (10 x 10 pixels), we were able to optimize the\nmodel's performance, pushing computational resources in a new direction and\nachieving state-of-the-art performance on cancer-related benchmarks. CanvOI\ndemonstrated a 1.5-7.4% improvement in averaged AUC compared to other leading\nfoundation models built for digital pathology. Moreover, our results\ndemonstrate that CanvOI significantly outperformed the other models, with the\nperformance gap widening substantially when trained on just 10% of the initial\ncohort. This work highlights an alternative approach that, if integrated with\ntraditional development approaches, has the potential to advance Oncology\nIntelligence (OI), overcome some of the current barriers and ultimately improve\nthe clinical outcome of cancer patients.\n", "link": "http://arxiv.org/abs/2409.02885v1", "date": "2024-09-04", "relevancy": 1.9898, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5094}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4946}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CanvOI%2C%20an%20Oncology%20Intelligence%20Foundation%20Model%3A%20Scaling%20FLOPS%0A%20%20Differently&body=Title%3A%20CanvOI%2C%20an%20Oncology%20Intelligence%20Foundation%20Model%3A%20Scaling%20FLOPS%0A%20%20Differently%0AAuthor%3A%20Jonathan%20Zalach%20and%20Inbal%20Gazy%20and%20Assaf%20Avinoam%20and%20Ron%20Sinai%20and%20Eran%20Shmuel%20and%20Inbar%20Gilboa%20and%20Christine%20Swisher%20and%20Naim%20Matasci%20and%20Reva%20Basho%20and%20David%20B.%20Agus%0AAbstract%3A%20%20%20The%20rapidly%20evolving%20field%20of%20digital%20oncopathology%20faces%20significant%0Achallenges%2C%20including%20the%20need%20to%20address%20diverse%20and%20complex%20clinical%0Aquestions%2C%20often%20involving%20rare%20conditions%2C%20with%20limited%20availability%20of%0Alabeled%20data.%20These%20limitations%20hinder%20the%20development%20of%20robust%20AI-driven%0Atools%20in%20the%20biomedical%20space%2C%20where%20accuracy%20in%20probabilistic%20determinations%0Ais%20of%20utmost%20importance.%20To%20address%20this%2C%20digital%20pathology%20foundation%20models%0Ahave%20begun%20to%20emerge%2C%20typically%20developed%20with%20the%20size%20and%20diversity%20of%20the%0Apre-training%20dataset%20and%20model%20parameters%20in%20mind.%20Here%2C%20we%20present%20CanvOI%2C%20a%0AViT-g/10-based%20foundation%20model%20designed%20to%20enhance%20the%20capabilities%20of%20digital%0Apathology%20by%20addressing%20these%20challenges%20through%20a%20different%20approach.%0AConsidering%20the%20unique%20nature%20of%20oncologic%20histopathological%20images%20and%20the%0Arequirements%20from%20the%20embeddings%20to%20provide%20meaningful%20representations%20for%0AMultiple%20Instance%20Learning%20%28MIL%29%20downstream%20models%2C%20we%20chose%20to%20modify%20the%0Ainput%20image%20characteristics.%20By%20introducing%20larger%20tile%20sizes%20%28380%20x%20380%0Apixels%29%20and%20smaller%20patch%20sizes%20%2810%20x%2010%20pixels%29%2C%20we%20were%20able%20to%20optimize%20the%0Amodel%27s%20performance%2C%20pushing%20computational%20resources%20in%20a%20new%20direction%20and%0Aachieving%20state-of-the-art%20performance%20on%20cancer-related%20benchmarks.%20CanvOI%0Ademonstrated%20a%201.5-7.4%25%20improvement%20in%20averaged%20AUC%20compared%20to%20other%20leading%0Afoundation%20models%20built%20for%20digital%20pathology.%20Moreover%2C%20our%20results%0Ademonstrate%20that%20CanvOI%20significantly%20outperformed%20the%20other%20models%2C%20with%20the%0Aperformance%20gap%20widening%20substantially%20when%20trained%20on%20just%2010%25%20of%20the%20initial%0Acohort.%20This%20work%20highlights%20an%20alternative%20approach%20that%2C%20if%20integrated%20with%0Atraditional%20development%20approaches%2C%20has%20the%20potential%20to%20advance%20Oncology%0AIntelligence%20%28OI%29%2C%20overcome%20some%20of%20the%20current%20barriers%20and%20ultimately%20improve%0Athe%20clinical%20outcome%20of%20cancer%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanvOI%252C%2520an%2520Oncology%2520Intelligence%2520Foundation%2520Model%253A%2520Scaling%2520FLOPS%250A%2520%2520Differently%26entry.906535625%3DJonathan%2520Zalach%2520and%2520Inbal%2520Gazy%2520and%2520Assaf%2520Avinoam%2520and%2520Ron%2520Sinai%2520and%2520Eran%2520Shmuel%2520and%2520Inbar%2520Gilboa%2520and%2520Christine%2520Swisher%2520and%2520Naim%2520Matasci%2520and%2520Reva%2520Basho%2520and%2520David%2520B.%2520Agus%26entry.1292438233%3D%2520%2520The%2520rapidly%2520evolving%2520field%2520of%2520digital%2520oncopathology%2520faces%2520significant%250Achallenges%252C%2520including%2520the%2520need%2520to%2520address%2520diverse%2520and%2520complex%2520clinical%250Aquestions%252C%2520often%2520involving%2520rare%2520conditions%252C%2520with%2520limited%2520availability%2520of%250Alabeled%2520data.%2520These%2520limitations%2520hinder%2520the%2520development%2520of%2520robust%2520AI-driven%250Atools%2520in%2520the%2520biomedical%2520space%252C%2520where%2520accuracy%2520in%2520probabilistic%2520determinations%250Ais%2520of%2520utmost%2520importance.%2520To%2520address%2520this%252C%2520digital%2520pathology%2520foundation%2520models%250Ahave%2520begun%2520to%2520emerge%252C%2520typically%2520developed%2520with%2520the%2520size%2520and%2520diversity%2520of%2520the%250Apre-training%2520dataset%2520and%2520model%2520parameters%2520in%2520mind.%2520Here%252C%2520we%2520present%2520CanvOI%252C%2520a%250AViT-g/10-based%2520foundation%2520model%2520designed%2520to%2520enhance%2520the%2520capabilities%2520of%2520digital%250Apathology%2520by%2520addressing%2520these%2520challenges%2520through%2520a%2520different%2520approach.%250AConsidering%2520the%2520unique%2520nature%2520of%2520oncologic%2520histopathological%2520images%2520and%2520the%250Arequirements%2520from%2520the%2520embeddings%2520to%2520provide%2520meaningful%2520representations%2520for%250AMultiple%2520Instance%2520Learning%2520%2528MIL%2529%2520downstream%2520models%252C%2520we%2520chose%2520to%2520modify%2520the%250Ainput%2520image%2520characteristics.%2520By%2520introducing%2520larger%2520tile%2520sizes%2520%2528380%2520x%2520380%250Apixels%2529%2520and%2520smaller%2520patch%2520sizes%2520%252810%2520x%252010%2520pixels%2529%252C%2520we%2520were%2520able%2520to%2520optimize%2520the%250Amodel%2527s%2520performance%252C%2520pushing%2520computational%2520resources%2520in%2520a%2520new%2520direction%2520and%250Aachieving%2520state-of-the-art%2520performance%2520on%2520cancer-related%2520benchmarks.%2520CanvOI%250Ademonstrated%2520a%25201.5-7.4%2525%2520improvement%2520in%2520averaged%2520AUC%2520compared%2520to%2520other%2520leading%250Afoundation%2520models%2520built%2520for%2520digital%2520pathology.%2520Moreover%252C%2520our%2520results%250Ademonstrate%2520that%2520CanvOI%2520significantly%2520outperformed%2520the%2520other%2520models%252C%2520with%2520the%250Aperformance%2520gap%2520widening%2520substantially%2520when%2520trained%2520on%2520just%252010%2525%2520of%2520the%2520initial%250Acohort.%2520This%2520work%2520highlights%2520an%2520alternative%2520approach%2520that%252C%2520if%2520integrated%2520with%250Atraditional%2520development%2520approaches%252C%2520has%2520the%2520potential%2520to%2520advance%2520Oncology%250AIntelligence%2520%2528OI%2529%252C%2520overcome%2520some%2520of%2520the%2520current%2520barriers%2520and%2520ultimately%2520improve%250Athe%2520clinical%2520outcome%2520of%2520cancer%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CanvOI%2C%20an%20Oncology%20Intelligence%20Foundation%20Model%3A%20Scaling%20FLOPS%0A%20%20Differently&entry.906535625=Jonathan%20Zalach%20and%20Inbal%20Gazy%20and%20Assaf%20Avinoam%20and%20Ron%20Sinai%20and%20Eran%20Shmuel%20and%20Inbar%20Gilboa%20and%20Christine%20Swisher%20and%20Naim%20Matasci%20and%20Reva%20Basho%20and%20David%20B.%20Agus&entry.1292438233=%20%20The%20rapidly%20evolving%20field%20of%20digital%20oncopathology%20faces%20significant%0Achallenges%2C%20including%20the%20need%20to%20address%20diverse%20and%20complex%20clinical%0Aquestions%2C%20often%20involving%20rare%20conditions%2C%20with%20limited%20availability%20of%0Alabeled%20data.%20These%20limitations%20hinder%20the%20development%20of%20robust%20AI-driven%0Atools%20in%20the%20biomedical%20space%2C%20where%20accuracy%20in%20probabilistic%20determinations%0Ais%20of%20utmost%20importance.%20To%20address%20this%2C%20digital%20pathology%20foundation%20models%0Ahave%20begun%20to%20emerge%2C%20typically%20developed%20with%20the%20size%20and%20diversity%20of%20the%0Apre-training%20dataset%20and%20model%20parameters%20in%20mind.%20Here%2C%20we%20present%20CanvOI%2C%20a%0AViT-g/10-based%20foundation%20model%20designed%20to%20enhance%20the%20capabilities%20of%20digital%0Apathology%20by%20addressing%20these%20challenges%20through%20a%20different%20approach.%0AConsidering%20the%20unique%20nature%20of%20oncologic%20histopathological%20images%20and%20the%0Arequirements%20from%20the%20embeddings%20to%20provide%20meaningful%20representations%20for%0AMultiple%20Instance%20Learning%20%28MIL%29%20downstream%20models%2C%20we%20chose%20to%20modify%20the%0Ainput%20image%20characteristics.%20By%20introducing%20larger%20tile%20sizes%20%28380%20x%20380%0Apixels%29%20and%20smaller%20patch%20sizes%20%2810%20x%2010%20pixels%29%2C%20we%20were%20able%20to%20optimize%20the%0Amodel%27s%20performance%2C%20pushing%20computational%20resources%20in%20a%20new%20direction%20and%0Aachieving%20state-of-the-art%20performance%20on%20cancer-related%20benchmarks.%20CanvOI%0Ademonstrated%20a%201.5-7.4%25%20improvement%20in%20averaged%20AUC%20compared%20to%20other%20leading%0Afoundation%20models%20built%20for%20digital%20pathology.%20Moreover%2C%20our%20results%0Ademonstrate%20that%20CanvOI%20significantly%20outperformed%20the%20other%20models%2C%20with%20the%0Aperformance%20gap%20widening%20substantially%20when%20trained%20on%20just%2010%25%20of%20the%20initial%0Acohort.%20This%20work%20highlights%20an%20alternative%20approach%20that%2C%20if%20integrated%20with%0Atraditional%20development%20approaches%2C%20has%20the%20potential%20to%20advance%20Oncology%0AIntelligence%20%28OI%29%2C%20overcome%20some%20of%20the%20current%20barriers%20and%20ultimately%20improve%0Athe%20clinical%20outcome%20of%20cancer%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02885v1&entry.124074799=Read"},
{"title": "Learning-Based Error Detection System for Advanced Vehicle Instrument\n  Cluster Rendering", "author": "Cornelius B\u00fcrkle and Fabian Oboril and Kay-Ulrich Scholl", "abstract": "  The automotive industry is currently expanding digital display options with\nevery new model that comes onto the market. This entails not just an expansion\nin dimensions, resolution, and customization choices, but also the capability\nto employ novel display effects like overlays while assembling the content of\nthe display cluster. Unfortunately, this raises the need for appropriate\nmonitoring systems that can detect rendering errors and apply appropriate\ncountermeasures when required. Classical solutions such as Cyclic Redundancy\nChecks (CRC) will soon be no longer viable as any sort of alpha blending,\nwarping of scaling of content can cause unwanted CRC violations. Therefore, we\npropose a novel monitoring approach to verify correctness of displayed content\nusing telltales (e.g. warning signs) as example. It uses a learning-based\napproach to separate \"good\" telltales, i.e. those that a human driver will\nunderstand correctly, and \"corrupted\" telltales, i.e. those that will not be\nvisible or perceived correctly. As a result, it possesses inherent resilience\nagainst individual pixel errors and implicitly supports changing backgrounds,\noverlay or scaling effects. This is underlined by our experimental study where\nall \"corrupted\" test patterns were correctly classified, while no false alarms\nwere triggered.\n", "link": "http://arxiv.org/abs/2409.02647v1", "date": "2024-09-04", "relevancy": 1.9847, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4989}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-Based%20Error%20Detection%20System%20for%20Advanced%20Vehicle%20Instrument%0A%20%20Cluster%20Rendering&body=Title%3A%20Learning-Based%20Error%20Detection%20System%20for%20Advanced%20Vehicle%20Instrument%0A%20%20Cluster%20Rendering%0AAuthor%3A%20Cornelius%20B%C3%BCrkle%20and%20Fabian%20Oboril%20and%20Kay-Ulrich%20Scholl%0AAbstract%3A%20%20%20The%20automotive%20industry%20is%20currently%20expanding%20digital%20display%20options%20with%0Aevery%20new%20model%20that%20comes%20onto%20the%20market.%20This%20entails%20not%20just%20an%20expansion%0Ain%20dimensions%2C%20resolution%2C%20and%20customization%20choices%2C%20but%20also%20the%20capability%0Ato%20employ%20novel%20display%20effects%20like%20overlays%20while%20assembling%20the%20content%20of%0Athe%20display%20cluster.%20Unfortunately%2C%20this%20raises%20the%20need%20for%20appropriate%0Amonitoring%20systems%20that%20can%20detect%20rendering%20errors%20and%20apply%20appropriate%0Acountermeasures%20when%20required.%20Classical%20solutions%20such%20as%20Cyclic%20Redundancy%0AChecks%20%28CRC%29%20will%20soon%20be%20no%20longer%20viable%20as%20any%20sort%20of%20alpha%20blending%2C%0Awarping%20of%20scaling%20of%20content%20can%20cause%20unwanted%20CRC%20violations.%20Therefore%2C%20we%0Apropose%20a%20novel%20monitoring%20approach%20to%20verify%20correctness%20of%20displayed%20content%0Ausing%20telltales%20%28e.g.%20warning%20signs%29%20as%20example.%20It%20uses%20a%20learning-based%0Aapproach%20to%20separate%20%22good%22%20telltales%2C%20i.e.%20those%20that%20a%20human%20driver%20will%0Aunderstand%20correctly%2C%20and%20%22corrupted%22%20telltales%2C%20i.e.%20those%20that%20will%20not%20be%0Avisible%20or%20perceived%20correctly.%20As%20a%20result%2C%20it%20possesses%20inherent%20resilience%0Aagainst%20individual%20pixel%20errors%20and%20implicitly%20supports%20changing%20backgrounds%2C%0Aoverlay%20or%20scaling%20effects.%20This%20is%20underlined%20by%20our%20experimental%20study%20where%0Aall%20%22corrupted%22%20test%20patterns%20were%20correctly%20classified%2C%20while%20no%20false%20alarms%0Awere%20triggered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-Based%2520Error%2520Detection%2520System%2520for%2520Advanced%2520Vehicle%2520Instrument%250A%2520%2520Cluster%2520Rendering%26entry.906535625%3DCornelius%2520B%25C3%25BCrkle%2520and%2520Fabian%2520Oboril%2520and%2520Kay-Ulrich%2520Scholl%26entry.1292438233%3D%2520%2520The%2520automotive%2520industry%2520is%2520currently%2520expanding%2520digital%2520display%2520options%2520with%250Aevery%2520new%2520model%2520that%2520comes%2520onto%2520the%2520market.%2520This%2520entails%2520not%2520just%2520an%2520expansion%250Ain%2520dimensions%252C%2520resolution%252C%2520and%2520customization%2520choices%252C%2520but%2520also%2520the%2520capability%250Ato%2520employ%2520novel%2520display%2520effects%2520like%2520overlays%2520while%2520assembling%2520the%2520content%2520of%250Athe%2520display%2520cluster.%2520Unfortunately%252C%2520this%2520raises%2520the%2520need%2520for%2520appropriate%250Amonitoring%2520systems%2520that%2520can%2520detect%2520rendering%2520errors%2520and%2520apply%2520appropriate%250Acountermeasures%2520when%2520required.%2520Classical%2520solutions%2520such%2520as%2520Cyclic%2520Redundancy%250AChecks%2520%2528CRC%2529%2520will%2520soon%2520be%2520no%2520longer%2520viable%2520as%2520any%2520sort%2520of%2520alpha%2520blending%252C%250Awarping%2520of%2520scaling%2520of%2520content%2520can%2520cause%2520unwanted%2520CRC%2520violations.%2520Therefore%252C%2520we%250Apropose%2520a%2520novel%2520monitoring%2520approach%2520to%2520verify%2520correctness%2520of%2520displayed%2520content%250Ausing%2520telltales%2520%2528e.g.%2520warning%2520signs%2529%2520as%2520example.%2520It%2520uses%2520a%2520learning-based%250Aapproach%2520to%2520separate%2520%2522good%2522%2520telltales%252C%2520i.e.%2520those%2520that%2520a%2520human%2520driver%2520will%250Aunderstand%2520correctly%252C%2520and%2520%2522corrupted%2522%2520telltales%252C%2520i.e.%2520those%2520that%2520will%2520not%2520be%250Avisible%2520or%2520perceived%2520correctly.%2520As%2520a%2520result%252C%2520it%2520possesses%2520inherent%2520resilience%250Aagainst%2520individual%2520pixel%2520errors%2520and%2520implicitly%2520supports%2520changing%2520backgrounds%252C%250Aoverlay%2520or%2520scaling%2520effects.%2520This%2520is%2520underlined%2520by%2520our%2520experimental%2520study%2520where%250Aall%2520%2522corrupted%2522%2520test%2520patterns%2520were%2520correctly%2520classified%252C%2520while%2520no%2520false%2520alarms%250Awere%2520triggered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-Based%20Error%20Detection%20System%20for%20Advanced%20Vehicle%20Instrument%0A%20%20Cluster%20Rendering&entry.906535625=Cornelius%20B%C3%BCrkle%20and%20Fabian%20Oboril%20and%20Kay-Ulrich%20Scholl&entry.1292438233=%20%20The%20automotive%20industry%20is%20currently%20expanding%20digital%20display%20options%20with%0Aevery%20new%20model%20that%20comes%20onto%20the%20market.%20This%20entails%20not%20just%20an%20expansion%0Ain%20dimensions%2C%20resolution%2C%20and%20customization%20choices%2C%20but%20also%20the%20capability%0Ato%20employ%20novel%20display%20effects%20like%20overlays%20while%20assembling%20the%20content%20of%0Athe%20display%20cluster.%20Unfortunately%2C%20this%20raises%20the%20need%20for%20appropriate%0Amonitoring%20systems%20that%20can%20detect%20rendering%20errors%20and%20apply%20appropriate%0Acountermeasures%20when%20required.%20Classical%20solutions%20such%20as%20Cyclic%20Redundancy%0AChecks%20%28CRC%29%20will%20soon%20be%20no%20longer%20viable%20as%20any%20sort%20of%20alpha%20blending%2C%0Awarping%20of%20scaling%20of%20content%20can%20cause%20unwanted%20CRC%20violations.%20Therefore%2C%20we%0Apropose%20a%20novel%20monitoring%20approach%20to%20verify%20correctness%20of%20displayed%20content%0Ausing%20telltales%20%28e.g.%20warning%20signs%29%20as%20example.%20It%20uses%20a%20learning-based%0Aapproach%20to%20separate%20%22good%22%20telltales%2C%20i.e.%20those%20that%20a%20human%20driver%20will%0Aunderstand%20correctly%2C%20and%20%22corrupted%22%20telltales%2C%20i.e.%20those%20that%20will%20not%20be%0Avisible%20or%20perceived%20correctly.%20As%20a%20result%2C%20it%20possesses%20inherent%20resilience%0Aagainst%20individual%20pixel%20errors%20and%20implicitly%20supports%20changing%20backgrounds%2C%0Aoverlay%20or%20scaling%20effects.%20This%20is%20underlined%20by%20our%20experimental%20study%20where%0Aall%20%22corrupted%22%20test%20patterns%20were%20correctly%20classified%2C%20while%20no%20false%20alarms%0Awere%20triggered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02647v1&entry.124074799=Read"},
{"title": "When Does Visual Prompting Outperform Linear Probing for Vision-Language\n  Models? A Likelihood Perspective", "author": "Hsi-Ai Tsao and Lei Hsiung and Pin-Yu Chen and Tsung-Yi Ho", "abstract": "  Adapting pre-trained models to new tasks can exhibit varying effectiveness\nacross datasets. Visual prompting, a state-of-the-art parameter-efficient\ntransfer learning method, can significantly improve the performance of\nout-of-distribution tasks. On the other hand, linear probing, a standard\ntransfer learning method, can sometimes become the best approach. We propose a\nlog-likelihood ratio (LLR) approach to analyze the comparative benefits of\nvisual prompting and linear probing. By employing the LLR score alongside\nresource-efficient visual prompts approximations, our cost-effective measure\nattains up to a 100-fold reduction in run time compared to full training, while\nachieving prediction accuracies up to 91%. The source code is available at\nhttps://github.com/IBM/VP-LLR.\n", "link": "http://arxiv.org/abs/2409.01821v2", "date": "2024-09-04", "relevancy": 1.9802, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4932}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Does%20Visual%20Prompting%20Outperform%20Linear%20Probing%20for%20Vision-Language%0A%20%20Models%3F%20A%20Likelihood%20Perspective&body=Title%3A%20When%20Does%20Visual%20Prompting%20Outperform%20Linear%20Probing%20for%20Vision-Language%0A%20%20Models%3F%20A%20Likelihood%20Perspective%0AAuthor%3A%20Hsi-Ai%20Tsao%20and%20Lei%20Hsiung%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho%0AAbstract%3A%20%20%20Adapting%20pre-trained%20models%20to%20new%20tasks%20can%20exhibit%20varying%20effectiveness%0Aacross%20datasets.%20Visual%20prompting%2C%20a%20state-of-the-art%20parameter-efficient%0Atransfer%20learning%20method%2C%20can%20significantly%20improve%20the%20performance%20of%0Aout-of-distribution%20tasks.%20On%20the%20other%20hand%2C%20linear%20probing%2C%20a%20standard%0Atransfer%20learning%20method%2C%20can%20sometimes%20become%20the%20best%20approach.%20We%20propose%20a%0Alog-likelihood%20ratio%20%28LLR%29%20approach%20to%20analyze%20the%20comparative%20benefits%20of%0Avisual%20prompting%20and%20linear%20probing.%20By%20employing%20the%20LLR%20score%20alongside%0Aresource-efficient%20visual%20prompts%20approximations%2C%20our%20cost-effective%20measure%0Aattains%20up%20to%20a%20100-fold%20reduction%20in%20run%20time%20compared%20to%20full%20training%2C%20while%0Aachieving%20prediction%20accuracies%20up%20to%2091%25.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/IBM/VP-LLR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01821v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Does%2520Visual%2520Prompting%2520Outperform%2520Linear%2520Probing%2520for%2520Vision-Language%250A%2520%2520Models%253F%2520A%2520Likelihood%2520Perspective%26entry.906535625%3DHsi-Ai%2520Tsao%2520and%2520Lei%2520Hsiung%2520and%2520Pin-Yu%2520Chen%2520and%2520Tsung-Yi%2520Ho%26entry.1292438233%3D%2520%2520Adapting%2520pre-trained%2520models%2520to%2520new%2520tasks%2520can%2520exhibit%2520varying%2520effectiveness%250Aacross%2520datasets.%2520Visual%2520prompting%252C%2520a%2520state-of-the-art%2520parameter-efficient%250Atransfer%2520learning%2520method%252C%2520can%2520significantly%2520improve%2520the%2520performance%2520of%250Aout-of-distribution%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520linear%2520probing%252C%2520a%2520standard%250Atransfer%2520learning%2520method%252C%2520can%2520sometimes%2520become%2520the%2520best%2520approach.%2520We%2520propose%2520a%250Alog-likelihood%2520ratio%2520%2528LLR%2529%2520approach%2520to%2520analyze%2520the%2520comparative%2520benefits%2520of%250Avisual%2520prompting%2520and%2520linear%2520probing.%2520By%2520employing%2520the%2520LLR%2520score%2520alongside%250Aresource-efficient%2520visual%2520prompts%2520approximations%252C%2520our%2520cost-effective%2520measure%250Aattains%2520up%2520to%2520a%2520100-fold%2520reduction%2520in%2520run%2520time%2520compared%2520to%2520full%2520training%252C%2520while%250Aachieving%2520prediction%2520accuracies%2520up%2520to%252091%2525.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IBM/VP-LLR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01821v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Does%20Visual%20Prompting%20Outperform%20Linear%20Probing%20for%20Vision-Language%0A%20%20Models%3F%20A%20Likelihood%20Perspective&entry.906535625=Hsi-Ai%20Tsao%20and%20Lei%20Hsiung%20and%20Pin-Yu%20Chen%20and%20Tsung-Yi%20Ho&entry.1292438233=%20%20Adapting%20pre-trained%20models%20to%20new%20tasks%20can%20exhibit%20varying%20effectiveness%0Aacross%20datasets.%20Visual%20prompting%2C%20a%20state-of-the-art%20parameter-efficient%0Atransfer%20learning%20method%2C%20can%20significantly%20improve%20the%20performance%20of%0Aout-of-distribution%20tasks.%20On%20the%20other%20hand%2C%20linear%20probing%2C%20a%20standard%0Atransfer%20learning%20method%2C%20can%20sometimes%20become%20the%20best%20approach.%20We%20propose%20a%0Alog-likelihood%20ratio%20%28LLR%29%20approach%20to%20analyze%20the%20comparative%20benefits%20of%0Avisual%20prompting%20and%20linear%20probing.%20By%20employing%20the%20LLR%20score%20alongside%0Aresource-efficient%20visual%20prompts%20approximations%2C%20our%20cost-effective%20measure%0Aattains%20up%20to%20a%20100-fold%20reduction%20in%20run%20time%20compared%20to%20full%20training%2C%20while%0Aachieving%20prediction%20accuracies%20up%20to%2091%25.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/IBM/VP-LLR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01821v2&entry.124074799=Read"},
{"title": "Convolutional Neural Networks for Automated Cellular Automaton\n  Classification", "author": "Michiel Rollier and Aisling J. Daly and Jan M. Baetens", "abstract": "  The emergent dynamics in spacetime diagrams of cellular automata (CAs) is\noften organised by means of a number of behavioural classes. Whilst\nclassification of elementary CAs is feasible and well-studied, non-elementary\nCAs are generally too diverse and numerous to exhaustively classify manually.\nIn this chapter we treat the spacetime diagram as a digital image, and\nimplement simple computer vision techniques to perform an automated\nclassification of elementary cellular automata into the five Li-Packard\nclasses. In particular, we present a supervised learning task to a\nconvolutional neural network, in such a way that it may be generalised to\nnon-elementary CAs. If we want to do so, we must divert the algorithm's focus\naway from the underlying 'microscopic' local updates. We first show that\npreviously developed deep learning approaches have in fact been trained to\nidentify the local update rule, rather than directly focus on the mesoscopic\npatterns that are associated with the particular behavioural classes. By means\nof a well-argued neural network design, as well as a number of data\naugmentation techniques, we then present a convolutional neural network that\nperforms nearly perfectly at identifying the behavioural class, without\nnecessarily first identifying the underlying microscopic dynamics.\n", "link": "http://arxiv.org/abs/2409.02740v1", "date": "2024-09-04", "relevancy": 1.9705, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Neural%20Networks%20for%20Automated%20Cellular%20Automaton%0A%20%20Classification&body=Title%3A%20Convolutional%20Neural%20Networks%20for%20Automated%20Cellular%20Automaton%0A%20%20Classification%0AAuthor%3A%20Michiel%20Rollier%20and%20Aisling%20J.%20Daly%20and%20Jan%20M.%20Baetens%0AAbstract%3A%20%20%20The%20emergent%20dynamics%20in%20spacetime%20diagrams%20of%20cellular%20automata%20%28CAs%29%20is%0Aoften%20organised%20by%20means%20of%20a%20number%20of%20behavioural%20classes.%20Whilst%0Aclassification%20of%20elementary%20CAs%20is%20feasible%20and%20well-studied%2C%20non-elementary%0ACAs%20are%20generally%20too%20diverse%20and%20numerous%20to%20exhaustively%20classify%20manually.%0AIn%20this%20chapter%20we%20treat%20the%20spacetime%20diagram%20as%20a%20digital%20image%2C%20and%0Aimplement%20simple%20computer%20vision%20techniques%20to%20perform%20an%20automated%0Aclassification%20of%20elementary%20cellular%20automata%20into%20the%20five%20Li-Packard%0Aclasses.%20In%20particular%2C%20we%20present%20a%20supervised%20learning%20task%20to%20a%0Aconvolutional%20neural%20network%2C%20in%20such%20a%20way%20that%20it%20may%20be%20generalised%20to%0Anon-elementary%20CAs.%20If%20we%20want%20to%20do%20so%2C%20we%20must%20divert%20the%20algorithm%27s%20focus%0Aaway%20from%20the%20underlying%20%27microscopic%27%20local%20updates.%20We%20first%20show%20that%0Apreviously%20developed%20deep%20learning%20approaches%20have%20in%20fact%20been%20trained%20to%0Aidentify%20the%20local%20update%20rule%2C%20rather%20than%20directly%20focus%20on%20the%20mesoscopic%0Apatterns%20that%20are%20associated%20with%20the%20particular%20behavioural%20classes.%20By%20means%0Aof%20a%20well-argued%20neural%20network%20design%2C%20as%20well%20as%20a%20number%20of%20data%0Aaugmentation%20techniques%2C%20we%20then%20present%20a%20convolutional%20neural%20network%20that%0Aperforms%20nearly%20perfectly%20at%20identifying%20the%20behavioural%20class%2C%20without%0Anecessarily%20first%20identifying%20the%20underlying%20microscopic%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520Neural%2520Networks%2520for%2520Automated%2520Cellular%2520Automaton%250A%2520%2520Classification%26entry.906535625%3DMichiel%2520Rollier%2520and%2520Aisling%2520J.%2520Daly%2520and%2520Jan%2520M.%2520Baetens%26entry.1292438233%3D%2520%2520The%2520emergent%2520dynamics%2520in%2520spacetime%2520diagrams%2520of%2520cellular%2520automata%2520%2528CAs%2529%2520is%250Aoften%2520organised%2520by%2520means%2520of%2520a%2520number%2520of%2520behavioural%2520classes.%2520Whilst%250Aclassification%2520of%2520elementary%2520CAs%2520is%2520feasible%2520and%2520well-studied%252C%2520non-elementary%250ACAs%2520are%2520generally%2520too%2520diverse%2520and%2520numerous%2520to%2520exhaustively%2520classify%2520manually.%250AIn%2520this%2520chapter%2520we%2520treat%2520the%2520spacetime%2520diagram%2520as%2520a%2520digital%2520image%252C%2520and%250Aimplement%2520simple%2520computer%2520vision%2520techniques%2520to%2520perform%2520an%2520automated%250Aclassification%2520of%2520elementary%2520cellular%2520automata%2520into%2520the%2520five%2520Li-Packard%250Aclasses.%2520In%2520particular%252C%2520we%2520present%2520a%2520supervised%2520learning%2520task%2520to%2520a%250Aconvolutional%2520neural%2520network%252C%2520in%2520such%2520a%2520way%2520that%2520it%2520may%2520be%2520generalised%2520to%250Anon-elementary%2520CAs.%2520If%2520we%2520want%2520to%2520do%2520so%252C%2520we%2520must%2520divert%2520the%2520algorithm%2527s%2520focus%250Aaway%2520from%2520the%2520underlying%2520%2527microscopic%2527%2520local%2520updates.%2520We%2520first%2520show%2520that%250Apreviously%2520developed%2520deep%2520learning%2520approaches%2520have%2520in%2520fact%2520been%2520trained%2520to%250Aidentify%2520the%2520local%2520update%2520rule%252C%2520rather%2520than%2520directly%2520focus%2520on%2520the%2520mesoscopic%250Apatterns%2520that%2520are%2520associated%2520with%2520the%2520particular%2520behavioural%2520classes.%2520By%2520means%250Aof%2520a%2520well-argued%2520neural%2520network%2520design%252C%2520as%2520well%2520as%2520a%2520number%2520of%2520data%250Aaugmentation%2520techniques%252C%2520we%2520then%2520present%2520a%2520convolutional%2520neural%2520network%2520that%250Aperforms%2520nearly%2520perfectly%2520at%2520identifying%2520the%2520behavioural%2520class%252C%2520without%250Anecessarily%2520first%2520identifying%2520the%2520underlying%2520microscopic%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Neural%20Networks%20for%20Automated%20Cellular%20Automaton%0A%20%20Classification&entry.906535625=Michiel%20Rollier%20and%20Aisling%20J.%20Daly%20and%20Jan%20M.%20Baetens&entry.1292438233=%20%20The%20emergent%20dynamics%20in%20spacetime%20diagrams%20of%20cellular%20automata%20%28CAs%29%20is%0Aoften%20organised%20by%20means%20of%20a%20number%20of%20behavioural%20classes.%20Whilst%0Aclassification%20of%20elementary%20CAs%20is%20feasible%20and%20well-studied%2C%20non-elementary%0ACAs%20are%20generally%20too%20diverse%20and%20numerous%20to%20exhaustively%20classify%20manually.%0AIn%20this%20chapter%20we%20treat%20the%20spacetime%20diagram%20as%20a%20digital%20image%2C%20and%0Aimplement%20simple%20computer%20vision%20techniques%20to%20perform%20an%20automated%0Aclassification%20of%20elementary%20cellular%20automata%20into%20the%20five%20Li-Packard%0Aclasses.%20In%20particular%2C%20we%20present%20a%20supervised%20learning%20task%20to%20a%0Aconvolutional%20neural%20network%2C%20in%20such%20a%20way%20that%20it%20may%20be%20generalised%20to%0Anon-elementary%20CAs.%20If%20we%20want%20to%20do%20so%2C%20we%20must%20divert%20the%20algorithm%27s%20focus%0Aaway%20from%20the%20underlying%20%27microscopic%27%20local%20updates.%20We%20first%20show%20that%0Apreviously%20developed%20deep%20learning%20approaches%20have%20in%20fact%20been%20trained%20to%0Aidentify%20the%20local%20update%20rule%2C%20rather%20than%20directly%20focus%20on%20the%20mesoscopic%0Apatterns%20that%20are%20associated%20with%20the%20particular%20behavioural%20classes.%20By%20means%0Aof%20a%20well-argued%20neural%20network%20design%2C%20as%20well%20as%20a%20number%20of%20data%0Aaugmentation%20techniques%2C%20we%20then%20present%20a%20convolutional%20neural%20network%20that%0Aperforms%20nearly%20perfectly%20at%20identifying%20the%20behavioural%20class%2C%20without%0Anecessarily%20first%20identifying%20the%20underlying%20microscopic%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02740v1&entry.124074799=Read"},
{"title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for\n  PostNL", "author": "Mohammad Reshadati", "abstract": "  The developments in the field of generative AI has brought a lot of\nopportunities for companies, for instance to improve efficiency in customer\nservice and automating tasks. PostNL, the biggest parcel and E-commerce\ncorporation of the Netherlands wants to use generative AI to enhance the\ncommunication around track and trace of parcels. During the internship a\nMinimal Viable Product (MVP) is created to showcase the value of using\ngenerative AI technologies, to enhance parcel tracking, analyzing the parcel's\njourney and being able to communicate about it in an easy to understand manner.\nThe primary goal was to develop an in-house LLM-based system, reducing\ndependency on external platforms and establishing the feasibility of a\ndedicated generative AI team within the company. This multi-agent LLM based\nsystem aimed to construct parcel journey stories and identify logistical\ndisruptions with heightened efficiency and accuracy. The research involved\ndeploying a sophisticated AI-driven communication system, employing\nRetrieval-Augmented Generation (RAG) for enhanced response precision, and\noptimizing large language models (LLMs) tailored to domain specific tasks.\n  The MVP successfully implemented a multi-agent open-source LLM system, called\nSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of\nuser inquiries and improving internal knowledge handling. Results and\nevaluation demonstrated technological innovation and feasibility, notably in\ncommunication about the track and trace of a parcel, which exceeded initial\nexpectations. These advancements highlight the potential of AI-driven solutions\nin logistics, suggesting many opportunities for further refinement and broader\nimplementation within PostNL operational framework.\n", "link": "http://arxiv.org/abs/2409.02711v1", "date": "2024-09-04", "relevancy": 1.9553, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5071}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4803}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20a%20Gen-AI%20based%20Track%20and%20Trace%20Assistant%20MVP%20%28SuperTracy%29%20for%0A%20%20PostNL&body=Title%3A%20Creating%20a%20Gen-AI%20based%20Track%20and%20Trace%20Assistant%20MVP%20%28SuperTracy%29%20for%0A%20%20PostNL%0AAuthor%3A%20Mohammad%20Reshadati%0AAbstract%3A%20%20%20The%20developments%20in%20the%20field%20of%20generative%20AI%20has%20brought%20a%20lot%20of%0Aopportunities%20for%20companies%2C%20for%20instance%20to%20improve%20efficiency%20in%20customer%0Aservice%20and%20automating%20tasks.%20PostNL%2C%20the%20biggest%20parcel%20and%20E-commerce%0Acorporation%20of%20the%20Netherlands%20wants%20to%20use%20generative%20AI%20to%20enhance%20the%0Acommunication%20around%20track%20and%20trace%20of%20parcels.%20During%20the%20internship%20a%0AMinimal%20Viable%20Product%20%28MVP%29%20is%20created%20to%20showcase%20the%20value%20of%20using%0Agenerative%20AI%20technologies%2C%20to%20enhance%20parcel%20tracking%2C%20analyzing%20the%20parcel%27s%0Ajourney%20and%20being%20able%20to%20communicate%20about%20it%20in%20an%20easy%20to%20understand%20manner.%0AThe%20primary%20goal%20was%20to%20develop%20an%20in-house%20LLM-based%20system%2C%20reducing%0Adependency%20on%20external%20platforms%20and%20establishing%20the%20feasibility%20of%20a%0Adedicated%20generative%20AI%20team%20within%20the%20company.%20This%20multi-agent%20LLM%20based%0Asystem%20aimed%20to%20construct%20parcel%20journey%20stories%20and%20identify%20logistical%0Adisruptions%20with%20heightened%20efficiency%20and%20accuracy.%20The%20research%20involved%0Adeploying%20a%20sophisticated%20AI-driven%20communication%20system%2C%20employing%0ARetrieval-Augmented%20Generation%20%28RAG%29%20for%20enhanced%20response%20precision%2C%20and%0Aoptimizing%20large%20language%20models%20%28LLMs%29%20tailored%20to%20domain%20specific%20tasks.%0A%20%20The%20MVP%20successfully%20implemented%20a%20multi-agent%20open-source%20LLM%20system%2C%20called%0ASuperTracy.%20SuperTracy%20is%20capable%20of%20autonomously%20managing%20a%20broad%20spectrum%20of%0Auser%20inquiries%20and%20improving%20internal%20knowledge%20handling.%20Results%20and%0Aevaluation%20demonstrated%20technological%20innovation%20and%20feasibility%2C%20notably%20in%0Acommunication%20about%20the%20track%20and%20trace%20of%20a%20parcel%2C%20which%20exceeded%20initial%0Aexpectations.%20These%20advancements%20highlight%20the%20potential%20of%20AI-driven%20solutions%0Ain%20logistics%2C%20suggesting%20many%20opportunities%20for%20further%20refinement%20and%20broader%0Aimplementation%20within%20PostNL%20operational%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520a%2520Gen-AI%2520based%2520Track%2520and%2520Trace%2520Assistant%2520MVP%2520%2528SuperTracy%2529%2520for%250A%2520%2520PostNL%26entry.906535625%3DMohammad%2520Reshadati%26entry.1292438233%3D%2520%2520The%2520developments%2520in%2520the%2520field%2520of%2520generative%2520AI%2520has%2520brought%2520a%2520lot%2520of%250Aopportunities%2520for%2520companies%252C%2520for%2520instance%2520to%2520improve%2520efficiency%2520in%2520customer%250Aservice%2520and%2520automating%2520tasks.%2520PostNL%252C%2520the%2520biggest%2520parcel%2520and%2520E-commerce%250Acorporation%2520of%2520the%2520Netherlands%2520wants%2520to%2520use%2520generative%2520AI%2520to%2520enhance%2520the%250Acommunication%2520around%2520track%2520and%2520trace%2520of%2520parcels.%2520During%2520the%2520internship%2520a%250AMinimal%2520Viable%2520Product%2520%2528MVP%2529%2520is%2520created%2520to%2520showcase%2520the%2520value%2520of%2520using%250Agenerative%2520AI%2520technologies%252C%2520to%2520enhance%2520parcel%2520tracking%252C%2520analyzing%2520the%2520parcel%2527s%250Ajourney%2520and%2520being%2520able%2520to%2520communicate%2520about%2520it%2520in%2520an%2520easy%2520to%2520understand%2520manner.%250AThe%2520primary%2520goal%2520was%2520to%2520develop%2520an%2520in-house%2520LLM-based%2520system%252C%2520reducing%250Adependency%2520on%2520external%2520platforms%2520and%2520establishing%2520the%2520feasibility%2520of%2520a%250Adedicated%2520generative%2520AI%2520team%2520within%2520the%2520company.%2520This%2520multi-agent%2520LLM%2520based%250Asystem%2520aimed%2520to%2520construct%2520parcel%2520journey%2520stories%2520and%2520identify%2520logistical%250Adisruptions%2520with%2520heightened%2520efficiency%2520and%2520accuracy.%2520The%2520research%2520involved%250Adeploying%2520a%2520sophisticated%2520AI-driven%2520communication%2520system%252C%2520employing%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520for%2520enhanced%2520response%2520precision%252C%2520and%250Aoptimizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520tailored%2520to%2520domain%2520specific%2520tasks.%250A%2520%2520The%2520MVP%2520successfully%2520implemented%2520a%2520multi-agent%2520open-source%2520LLM%2520system%252C%2520called%250ASuperTracy.%2520SuperTracy%2520is%2520capable%2520of%2520autonomously%2520managing%2520a%2520broad%2520spectrum%2520of%250Auser%2520inquiries%2520and%2520improving%2520internal%2520knowledge%2520handling.%2520Results%2520and%250Aevaluation%2520demonstrated%2520technological%2520innovation%2520and%2520feasibility%252C%2520notably%2520in%250Acommunication%2520about%2520the%2520track%2520and%2520trace%2520of%2520a%2520parcel%252C%2520which%2520exceeded%2520initial%250Aexpectations.%2520These%2520advancements%2520highlight%2520the%2520potential%2520of%2520AI-driven%2520solutions%250Ain%2520logistics%252C%2520suggesting%2520many%2520opportunities%2520for%2520further%2520refinement%2520and%2520broader%250Aimplementation%2520within%2520PostNL%2520operational%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20a%20Gen-AI%20based%20Track%20and%20Trace%20Assistant%20MVP%20%28SuperTracy%29%20for%0A%20%20PostNL&entry.906535625=Mohammad%20Reshadati&entry.1292438233=%20%20The%20developments%20in%20the%20field%20of%20generative%20AI%20has%20brought%20a%20lot%20of%0Aopportunities%20for%20companies%2C%20for%20instance%20to%20improve%20efficiency%20in%20customer%0Aservice%20and%20automating%20tasks.%20PostNL%2C%20the%20biggest%20parcel%20and%20E-commerce%0Acorporation%20of%20the%20Netherlands%20wants%20to%20use%20generative%20AI%20to%20enhance%20the%0Acommunication%20around%20track%20and%20trace%20of%20parcels.%20During%20the%20internship%20a%0AMinimal%20Viable%20Product%20%28MVP%29%20is%20created%20to%20showcase%20the%20value%20of%20using%0Agenerative%20AI%20technologies%2C%20to%20enhance%20parcel%20tracking%2C%20analyzing%20the%20parcel%27s%0Ajourney%20and%20being%20able%20to%20communicate%20about%20it%20in%20an%20easy%20to%20understand%20manner.%0AThe%20primary%20goal%20was%20to%20develop%20an%20in-house%20LLM-based%20system%2C%20reducing%0Adependency%20on%20external%20platforms%20and%20establishing%20the%20feasibility%20of%20a%0Adedicated%20generative%20AI%20team%20within%20the%20company.%20This%20multi-agent%20LLM%20based%0Asystem%20aimed%20to%20construct%20parcel%20journey%20stories%20and%20identify%20logistical%0Adisruptions%20with%20heightened%20efficiency%20and%20accuracy.%20The%20research%20involved%0Adeploying%20a%20sophisticated%20AI-driven%20communication%20system%2C%20employing%0ARetrieval-Augmented%20Generation%20%28RAG%29%20for%20enhanced%20response%20precision%2C%20and%0Aoptimizing%20large%20language%20models%20%28LLMs%29%20tailored%20to%20domain%20specific%20tasks.%0A%20%20The%20MVP%20successfully%20implemented%20a%20multi-agent%20open-source%20LLM%20system%2C%20called%0ASuperTracy.%20SuperTracy%20is%20capable%20of%20autonomously%20managing%20a%20broad%20spectrum%20of%0Auser%20inquiries%20and%20improving%20internal%20knowledge%20handling.%20Results%20and%0Aevaluation%20demonstrated%20technological%20innovation%20and%20feasibility%2C%20notably%20in%0Acommunication%20about%20the%20track%20and%20trace%20of%20a%20parcel%2C%20which%20exceeded%20initial%0Aexpectations.%20These%20advancements%20highlight%20the%20potential%20of%20AI-driven%20solutions%0Ain%20logistics%2C%20suggesting%20many%20opportunities%20for%20further%20refinement%20and%20broader%0Aimplementation%20within%20PostNL%20operational%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02711v1&entry.124074799=Read"},
{"title": "UnLearning from Experience to Avoid Spurious Correlations", "author": "Jeff Mitchell and Jes\u00fas Mart\u00ednez del Rinc\u00f3n and Niall McLaughlin", "abstract": "  While deep neural networks can achieve state-of-the-art performance in many\ntasks, these models are more fragile than they appear. They are prone to\nlearning spurious correlations in their training data, leading to surprising\nfailure cases. In this paper, we propose a new approach that addresses the\nissue of spurious correlations: UnLearning from Experience (ULE). Our method is\nbased on using two classification models trained in parallel: student and\nteacher models. Both models receive the same batches of training data. The\nstudent model is trained with no constraints and pursues the spurious\ncorrelations in the data. The teacher model is trained to solve the same\nclassification problem while avoiding the mistakes of the student model. As\ntraining is done in parallel, the better the student model learns the spurious\ncorrelations, the more robust the teacher model becomes. The teacher model uses\nthe gradient of the student's output with respect to its input to unlearn\nmistakes made by the student. We show that our method is effective on the\nWaterbirds, CelebA, Spawrious and UrbanCars datasets.\n", "link": "http://arxiv.org/abs/2409.02792v1", "date": "2024-09-04", "relevancy": 1.9546, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4935}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnLearning%20from%20Experience%20to%20Avoid%20Spurious%20Correlations&body=Title%3A%20UnLearning%20from%20Experience%20to%20Avoid%20Spurious%20Correlations%0AAuthor%3A%20Jeff%20Mitchell%20and%20Jes%C3%BAs%20Mart%C3%ADnez%20del%20Rinc%C3%B3n%20and%20Niall%20McLaughlin%0AAbstract%3A%20%20%20While%20deep%20neural%20networks%20can%20achieve%20state-of-the-art%20performance%20in%20many%0Atasks%2C%20these%20models%20are%20more%20fragile%20than%20they%20appear.%20They%20are%20prone%20to%0Alearning%20spurious%20correlations%20in%20their%20training%20data%2C%20leading%20to%20surprising%0Afailure%20cases.%20In%20this%20paper%2C%20we%20propose%20a%20new%20approach%20that%20addresses%20the%0Aissue%20of%20spurious%20correlations%3A%20UnLearning%20from%20Experience%20%28ULE%29.%20Our%20method%20is%0Abased%20on%20using%20two%20classification%20models%20trained%20in%20parallel%3A%20student%20and%0Ateacher%20models.%20Both%20models%20receive%20the%20same%20batches%20of%20training%20data.%20The%0Astudent%20model%20is%20trained%20with%20no%20constraints%20and%20pursues%20the%20spurious%0Acorrelations%20in%20the%20data.%20The%20teacher%20model%20is%20trained%20to%20solve%20the%20same%0Aclassification%20problem%20while%20avoiding%20the%20mistakes%20of%20the%20student%20model.%20As%0Atraining%20is%20done%20in%20parallel%2C%20the%20better%20the%20student%20model%20learns%20the%20spurious%0Acorrelations%2C%20the%20more%20robust%20the%20teacher%20model%20becomes.%20The%20teacher%20model%20uses%0Athe%20gradient%20of%20the%20student%27s%20output%20with%20respect%20to%20its%20input%20to%20unlearn%0Amistakes%20made%20by%20the%20student.%20We%20show%20that%20our%20method%20is%20effective%20on%20the%0AWaterbirds%2C%20CelebA%2C%20Spawrious%20and%20UrbanCars%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnLearning%2520from%2520Experience%2520to%2520Avoid%2520Spurious%2520Correlations%26entry.906535625%3DJeff%2520Mitchell%2520and%2520Jes%25C3%25BAs%2520Mart%25C3%25ADnez%2520del%2520Rinc%25C3%25B3n%2520and%2520Niall%2520McLaughlin%26entry.1292438233%3D%2520%2520While%2520deep%2520neural%2520networks%2520can%2520achieve%2520state-of-the-art%2520performance%2520in%2520many%250Atasks%252C%2520these%2520models%2520are%2520more%2520fragile%2520than%2520they%2520appear.%2520They%2520are%2520prone%2520to%250Alearning%2520spurious%2520correlations%2520in%2520their%2520training%2520data%252C%2520leading%2520to%2520surprising%250Afailure%2520cases.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520approach%2520that%2520addresses%2520the%250Aissue%2520of%2520spurious%2520correlations%253A%2520UnLearning%2520from%2520Experience%2520%2528ULE%2529.%2520Our%2520method%2520is%250Abased%2520on%2520using%2520two%2520classification%2520models%2520trained%2520in%2520parallel%253A%2520student%2520and%250Ateacher%2520models.%2520Both%2520models%2520receive%2520the%2520same%2520batches%2520of%2520training%2520data.%2520The%250Astudent%2520model%2520is%2520trained%2520with%2520no%2520constraints%2520and%2520pursues%2520the%2520spurious%250Acorrelations%2520in%2520the%2520data.%2520The%2520teacher%2520model%2520is%2520trained%2520to%2520solve%2520the%2520same%250Aclassification%2520problem%2520while%2520avoiding%2520the%2520mistakes%2520of%2520the%2520student%2520model.%2520As%250Atraining%2520is%2520done%2520in%2520parallel%252C%2520the%2520better%2520the%2520student%2520model%2520learns%2520the%2520spurious%250Acorrelations%252C%2520the%2520more%2520robust%2520the%2520teacher%2520model%2520becomes.%2520The%2520teacher%2520model%2520uses%250Athe%2520gradient%2520of%2520the%2520student%2527s%2520output%2520with%2520respect%2520to%2520its%2520input%2520to%2520unlearn%250Amistakes%2520made%2520by%2520the%2520student.%2520We%2520show%2520that%2520our%2520method%2520is%2520effective%2520on%2520the%250AWaterbirds%252C%2520CelebA%252C%2520Spawrious%2520and%2520UrbanCars%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnLearning%20from%20Experience%20to%20Avoid%20Spurious%20Correlations&entry.906535625=Jeff%20Mitchell%20and%20Jes%C3%BAs%20Mart%C3%ADnez%20del%20Rinc%C3%B3n%20and%20Niall%20McLaughlin&entry.1292438233=%20%20While%20deep%20neural%20networks%20can%20achieve%20state-of-the-art%20performance%20in%20many%0Atasks%2C%20these%20models%20are%20more%20fragile%20than%20they%20appear.%20They%20are%20prone%20to%0Alearning%20spurious%20correlations%20in%20their%20training%20data%2C%20leading%20to%20surprising%0Afailure%20cases.%20In%20this%20paper%2C%20we%20propose%20a%20new%20approach%20that%20addresses%20the%0Aissue%20of%20spurious%20correlations%3A%20UnLearning%20from%20Experience%20%28ULE%29.%20Our%20method%20is%0Abased%20on%20using%20two%20classification%20models%20trained%20in%20parallel%3A%20student%20and%0Ateacher%20models.%20Both%20models%20receive%20the%20same%20batches%20of%20training%20data.%20The%0Astudent%20model%20is%20trained%20with%20no%20constraints%20and%20pursues%20the%20spurious%0Acorrelations%20in%20the%20data.%20The%20teacher%20model%20is%20trained%20to%20solve%20the%20same%0Aclassification%20problem%20while%20avoiding%20the%20mistakes%20of%20the%20student%20model.%20As%0Atraining%20is%20done%20in%20parallel%2C%20the%20better%20the%20student%20model%20learns%20the%20spurious%0Acorrelations%2C%20the%20more%20robust%20the%20teacher%20model%20becomes.%20The%20teacher%20model%20uses%0Athe%20gradient%20of%20the%20student%27s%20output%20with%20respect%20to%20its%20input%20to%20unlearn%0Amistakes%20made%20by%20the%20student.%20We%20show%20that%20our%20method%20is%20effective%20on%20the%0AWaterbirds%2C%20CelebA%2C%20Spawrious%20and%20UrbanCars%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02792v1&entry.124074799=Read"},
{"title": "Q-Seg: Quantum Annealing-Based Unsupervised Image Segmentation", "author": "Supreeth Mysore Venkatesh and Antonio Macaluso and Marlon Nuske and Matthias Klusch and Andreas Dengel", "abstract": "  We present Q-Seg, a novel unsupervised image segmentation method based on\nquantum annealing, tailored for existing quantum hardware. We formulate the\npixel-wise segmentation problem, which assimilates spectral and spatial\ninformation of the image, as a graph-cut optimization task. Our method\nefficiently leverages the interconnected qubit topology of the D-Wave Advantage\ndevice, offering superior scalability over existing quantum approaches and\noutperforming several tested state-of-the-art classical methods. Empirical\nevaluations on synthetic datasets have shown that Q-Seg has better runtime\nperformance than the state-of-the-art classical optimizer Gurobi. The method\nhas also been tested on earth observation image segmentation, a critical area\nwith noisy and unreliable annotations. In the era of noisy intermediate-scale\nquantum, Q-Seg emerges as a reliable contender for real-world applications in\ncomparison to advanced techniques like Segment Anything. Consequently, Q-Seg\noffers a promising solution using available quantum hardware, especially in\nsituations constrained by limited labeled data and the need for efficient\ncomputational runtime.\n", "link": "http://arxiv.org/abs/2311.12912v3", "date": "2024-09-04", "relevancy": 1.9277, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4994}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Seg%3A%20Quantum%20Annealing-Based%20Unsupervised%20Image%20Segmentation&body=Title%3A%20Q-Seg%3A%20Quantum%20Annealing-Based%20Unsupervised%20Image%20Segmentation%0AAuthor%3A%20Supreeth%20Mysore%20Venkatesh%20and%20Antonio%20Macaluso%20and%20Marlon%20Nuske%20and%20Matthias%20Klusch%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20We%20present%20Q-Seg%2C%20a%20novel%20unsupervised%20image%20segmentation%20method%20based%20on%0Aquantum%20annealing%2C%20tailored%20for%20existing%20quantum%20hardware.%20We%20formulate%20the%0Apixel-wise%20segmentation%20problem%2C%20which%20assimilates%20spectral%20and%20spatial%0Ainformation%20of%20the%20image%2C%20as%20a%20graph-cut%20optimization%20task.%20Our%20method%0Aefficiently%20leverages%20the%20interconnected%20qubit%20topology%20of%20the%20D-Wave%20Advantage%0Adevice%2C%20offering%20superior%20scalability%20over%20existing%20quantum%20approaches%20and%0Aoutperforming%20several%20tested%20state-of-the-art%20classical%20methods.%20Empirical%0Aevaluations%20on%20synthetic%20datasets%20have%20shown%20that%20Q-Seg%20has%20better%20runtime%0Aperformance%20than%20the%20state-of-the-art%20classical%20optimizer%20Gurobi.%20The%20method%0Ahas%20also%20been%20tested%20on%20earth%20observation%20image%20segmentation%2C%20a%20critical%20area%0Awith%20noisy%20and%20unreliable%20annotations.%20In%20the%20era%20of%20noisy%20intermediate-scale%0Aquantum%2C%20Q-Seg%20emerges%20as%20a%20reliable%20contender%20for%20real-world%20applications%20in%0Acomparison%20to%20advanced%20techniques%20like%20Segment%20Anything.%20Consequently%2C%20Q-Seg%0Aoffers%20a%20promising%20solution%20using%20available%20quantum%20hardware%2C%20especially%20in%0Asituations%20constrained%20by%20limited%20labeled%20data%20and%20the%20need%20for%20efficient%0Acomputational%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12912v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Seg%253A%2520Quantum%2520Annealing-Based%2520Unsupervised%2520Image%2520Segmentation%26entry.906535625%3DSupreeth%2520Mysore%2520Venkatesh%2520and%2520Antonio%2520Macaluso%2520and%2520Marlon%2520Nuske%2520and%2520Matthias%2520Klusch%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520We%2520present%2520Q-Seg%252C%2520a%2520novel%2520unsupervised%2520image%2520segmentation%2520method%2520based%2520on%250Aquantum%2520annealing%252C%2520tailored%2520for%2520existing%2520quantum%2520hardware.%2520We%2520formulate%2520the%250Apixel-wise%2520segmentation%2520problem%252C%2520which%2520assimilates%2520spectral%2520and%2520spatial%250Ainformation%2520of%2520the%2520image%252C%2520as%2520a%2520graph-cut%2520optimization%2520task.%2520Our%2520method%250Aefficiently%2520leverages%2520the%2520interconnected%2520qubit%2520topology%2520of%2520the%2520D-Wave%2520Advantage%250Adevice%252C%2520offering%2520superior%2520scalability%2520over%2520existing%2520quantum%2520approaches%2520and%250Aoutperforming%2520several%2520tested%2520state-of-the-art%2520classical%2520methods.%2520Empirical%250Aevaluations%2520on%2520synthetic%2520datasets%2520have%2520shown%2520that%2520Q-Seg%2520has%2520better%2520runtime%250Aperformance%2520than%2520the%2520state-of-the-art%2520classical%2520optimizer%2520Gurobi.%2520The%2520method%250Ahas%2520also%2520been%2520tested%2520on%2520earth%2520observation%2520image%2520segmentation%252C%2520a%2520critical%2520area%250Awith%2520noisy%2520and%2520unreliable%2520annotations.%2520In%2520the%2520era%2520of%2520noisy%2520intermediate-scale%250Aquantum%252C%2520Q-Seg%2520emerges%2520as%2520a%2520reliable%2520contender%2520for%2520real-world%2520applications%2520in%250Acomparison%2520to%2520advanced%2520techniques%2520like%2520Segment%2520Anything.%2520Consequently%252C%2520Q-Seg%250Aoffers%2520a%2520promising%2520solution%2520using%2520available%2520quantum%2520hardware%252C%2520especially%2520in%250Asituations%2520constrained%2520by%2520limited%2520labeled%2520data%2520and%2520the%2520need%2520for%2520efficient%250Acomputational%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12912v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Seg%3A%20Quantum%20Annealing-Based%20Unsupervised%20Image%20Segmentation&entry.906535625=Supreeth%20Mysore%20Venkatesh%20and%20Antonio%20Macaluso%20and%20Marlon%20Nuske%20and%20Matthias%20Klusch%20and%20Andreas%20Dengel&entry.1292438233=%20%20We%20present%20Q-Seg%2C%20a%20novel%20unsupervised%20image%20segmentation%20method%20based%20on%0Aquantum%20annealing%2C%20tailored%20for%20existing%20quantum%20hardware.%20We%20formulate%20the%0Apixel-wise%20segmentation%20problem%2C%20which%20assimilates%20spectral%20and%20spatial%0Ainformation%20of%20the%20image%2C%20as%20a%20graph-cut%20optimization%20task.%20Our%20method%0Aefficiently%20leverages%20the%20interconnected%20qubit%20topology%20of%20the%20D-Wave%20Advantage%0Adevice%2C%20offering%20superior%20scalability%20over%20existing%20quantum%20approaches%20and%0Aoutperforming%20several%20tested%20state-of-the-art%20classical%20methods.%20Empirical%0Aevaluations%20on%20synthetic%20datasets%20have%20shown%20that%20Q-Seg%20has%20better%20runtime%0Aperformance%20than%20the%20state-of-the-art%20classical%20optimizer%20Gurobi.%20The%20method%0Ahas%20also%20been%20tested%20on%20earth%20observation%20image%20segmentation%2C%20a%20critical%20area%0Awith%20noisy%20and%20unreliable%20annotations.%20In%20the%20era%20of%20noisy%20intermediate-scale%0Aquantum%2C%20Q-Seg%20emerges%20as%20a%20reliable%20contender%20for%20real-world%20applications%20in%0Acomparison%20to%20advanced%20techniques%20like%20Segment%20Anything.%20Consequently%2C%20Q-Seg%0Aoffers%20a%20promising%20solution%20using%20available%20quantum%20hardware%2C%20especially%20in%0Asituations%20constrained%20by%20limited%20labeled%20data%20and%20the%20need%20for%20efficient%0Acomputational%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12912v3&entry.124074799=Read"},
{"title": "Negation Blindness in Large Language Models: Unveiling the NO Syndrome\n  in Image Generation", "author": "Mohammad Nadeem and Shahab Saquib Sohail and Erik Cambria and Bj\u00f6rn W. Schuller and Amir Hussain", "abstract": "  Foundational Large Language Models (LLMs) have changed the way we perceive\ntechnology. They have been shown to excel in tasks ranging from poem writing\nand coding to essay generation and puzzle solving. With the incorporation of\nimage generation capability, they have become more comprehensive and versatile\nAI tools. At the same time, researchers are striving to identify the\nlimitations of these tools to improve them further. Currently identified flaws\ninclude hallucination, biases, and bypassing restricted commands to generate\nharmful content. In the present work, we have identified a fundamental\nlimitation related to the image generation ability of LLMs, and termed it The\nNO Syndrome. This negation blindness refers to LLMs inability to correctly\ncomprehend NO related natural language prompts to generate the desired images.\nInterestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found\nto be suffering from this syndrome. To demonstrate the generalization of this\nlimitation, we carried out simulation experiments and conducted entropy-based\nand benchmark statistical analysis tests on various LLMs in multiple languages,\nincluding English, Hindi, and French. We conclude that the NO syndrome is a\nsignificant flaw in current LLMs that needs to be addressed. A related finding\nof this study showed a consistent discrepancy between image and textual\nresponses as a result of this NO syndrome. We posit that the introduction of a\nnegation context-aware reinforcement learning based feedback loop between the\nLLMs textual response and generated image could help ensure the generated text\nis based on both the LLMs correct contextual understanding of the negation\nquery and the generated visual output.\n", "link": "http://arxiv.org/abs/2409.00105v2", "date": "2024-09-04", "relevancy": 1.9272, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4919}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4753}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Negation%20Blindness%20in%20Large%20Language%20Models%3A%20Unveiling%20the%20NO%20Syndrome%0A%20%20in%20Image%20Generation&body=Title%3A%20Negation%20Blindness%20in%20Large%20Language%20Models%3A%20Unveiling%20the%20NO%20Syndrome%0A%20%20in%20Image%20Generation%0AAuthor%3A%20Mohammad%20Nadeem%20and%20Shahab%20Saquib%20Sohail%20and%20Erik%20Cambria%20and%20Bj%C3%B6rn%20W.%20Schuller%20and%20Amir%20Hussain%0AAbstract%3A%20%20%20Foundational%20Large%20Language%20Models%20%28LLMs%29%20have%20changed%20the%20way%20we%20perceive%0Atechnology.%20They%20have%20been%20shown%20to%20excel%20in%20tasks%20ranging%20from%20poem%20writing%0Aand%20coding%20to%20essay%20generation%20and%20puzzle%20solving.%20With%20the%20incorporation%20of%0Aimage%20generation%20capability%2C%20they%20have%20become%20more%20comprehensive%20and%20versatile%0AAI%20tools.%20At%20the%20same%20time%2C%20researchers%20are%20striving%20to%20identify%20the%0Alimitations%20of%20these%20tools%20to%20improve%20them%20further.%20Currently%20identified%20flaws%0Ainclude%20hallucination%2C%20biases%2C%20and%20bypassing%20restricted%20commands%20to%20generate%0Aharmful%20content.%20In%20the%20present%20work%2C%20we%20have%20identified%20a%20fundamental%0Alimitation%20related%20to%20the%20image%20generation%20ability%20of%20LLMs%2C%20and%20termed%20it%20The%0ANO%20Syndrome.%20This%20negation%20blindness%20refers%20to%20LLMs%20inability%20to%20correctly%0Acomprehend%20NO%20related%20natural%20language%20prompts%20to%20generate%20the%20desired%20images.%0AInterestingly%2C%20all%20tested%20LLMs%20including%20GPT-4%2C%20Gemini%2C%20and%20Copilot%20were%20found%0Ato%20be%20suffering%20from%20this%20syndrome.%20To%20demonstrate%20the%20generalization%20of%20this%0Alimitation%2C%20we%20carried%20out%20simulation%20experiments%20and%20conducted%20entropy-based%0Aand%20benchmark%20statistical%20analysis%20tests%20on%20various%20LLMs%20in%20multiple%20languages%2C%0Aincluding%20English%2C%20Hindi%2C%20and%20French.%20We%20conclude%20that%20the%20NO%20syndrome%20is%20a%0Asignificant%20flaw%20in%20current%20LLMs%20that%20needs%20to%20be%20addressed.%20A%20related%20finding%0Aof%20this%20study%20showed%20a%20consistent%20discrepancy%20between%20image%20and%20textual%0Aresponses%20as%20a%20result%20of%20this%20NO%20syndrome.%20We%20posit%20that%20the%20introduction%20of%20a%0Anegation%20context-aware%20reinforcement%20learning%20based%20feedback%20loop%20between%20the%0ALLMs%20textual%20response%20and%20generated%20image%20could%20help%20ensure%20the%20generated%20text%0Ais%20based%20on%20both%20the%20LLMs%20correct%20contextual%20understanding%20of%20the%20negation%0Aquery%20and%20the%20generated%20visual%20output.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNegation%2520Blindness%2520in%2520Large%2520Language%2520Models%253A%2520Unveiling%2520the%2520NO%2520Syndrome%250A%2520%2520in%2520Image%2520Generation%26entry.906535625%3DMohammad%2520Nadeem%2520and%2520Shahab%2520Saquib%2520Sohail%2520and%2520Erik%2520Cambria%2520and%2520Bj%25C3%25B6rn%2520W.%2520Schuller%2520and%2520Amir%2520Hussain%26entry.1292438233%3D%2520%2520Foundational%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520changed%2520the%2520way%2520we%2520perceive%250Atechnology.%2520They%2520have%2520been%2520shown%2520to%2520excel%2520in%2520tasks%2520ranging%2520from%2520poem%2520writing%250Aand%2520coding%2520to%2520essay%2520generation%2520and%2520puzzle%2520solving.%2520With%2520the%2520incorporation%2520of%250Aimage%2520generation%2520capability%252C%2520they%2520have%2520become%2520more%2520comprehensive%2520and%2520versatile%250AAI%2520tools.%2520At%2520the%2520same%2520time%252C%2520researchers%2520are%2520striving%2520to%2520identify%2520the%250Alimitations%2520of%2520these%2520tools%2520to%2520improve%2520them%2520further.%2520Currently%2520identified%2520flaws%250Ainclude%2520hallucination%252C%2520biases%252C%2520and%2520bypassing%2520restricted%2520commands%2520to%2520generate%250Aharmful%2520content.%2520In%2520the%2520present%2520work%252C%2520we%2520have%2520identified%2520a%2520fundamental%250Alimitation%2520related%2520to%2520the%2520image%2520generation%2520ability%2520of%2520LLMs%252C%2520and%2520termed%2520it%2520The%250ANO%2520Syndrome.%2520This%2520negation%2520blindness%2520refers%2520to%2520LLMs%2520inability%2520to%2520correctly%250Acomprehend%2520NO%2520related%2520natural%2520language%2520prompts%2520to%2520generate%2520the%2520desired%2520images.%250AInterestingly%252C%2520all%2520tested%2520LLMs%2520including%2520GPT-4%252C%2520Gemini%252C%2520and%2520Copilot%2520were%2520found%250Ato%2520be%2520suffering%2520from%2520this%2520syndrome.%2520To%2520demonstrate%2520the%2520generalization%2520of%2520this%250Alimitation%252C%2520we%2520carried%2520out%2520simulation%2520experiments%2520and%2520conducted%2520entropy-based%250Aand%2520benchmark%2520statistical%2520analysis%2520tests%2520on%2520various%2520LLMs%2520in%2520multiple%2520languages%252C%250Aincluding%2520English%252C%2520Hindi%252C%2520and%2520French.%2520We%2520conclude%2520that%2520the%2520NO%2520syndrome%2520is%2520a%250Asignificant%2520flaw%2520in%2520current%2520LLMs%2520that%2520needs%2520to%2520be%2520addressed.%2520A%2520related%2520finding%250Aof%2520this%2520study%2520showed%2520a%2520consistent%2520discrepancy%2520between%2520image%2520and%2520textual%250Aresponses%2520as%2520a%2520result%2520of%2520this%2520NO%2520syndrome.%2520We%2520posit%2520that%2520the%2520introduction%2520of%2520a%250Anegation%2520context-aware%2520reinforcement%2520learning%2520based%2520feedback%2520loop%2520between%2520the%250ALLMs%2520textual%2520response%2520and%2520generated%2520image%2520could%2520help%2520ensure%2520the%2520generated%2520text%250Ais%2520based%2520on%2520both%2520the%2520LLMs%2520correct%2520contextual%2520understanding%2520of%2520the%2520negation%250Aquery%2520and%2520the%2520generated%2520visual%2520output.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Negation%20Blindness%20in%20Large%20Language%20Models%3A%20Unveiling%20the%20NO%20Syndrome%0A%20%20in%20Image%20Generation&entry.906535625=Mohammad%20Nadeem%20and%20Shahab%20Saquib%20Sohail%20and%20Erik%20Cambria%20and%20Bj%C3%B6rn%20W.%20Schuller%20and%20Amir%20Hussain&entry.1292438233=%20%20Foundational%20Large%20Language%20Models%20%28LLMs%29%20have%20changed%20the%20way%20we%20perceive%0Atechnology.%20They%20have%20been%20shown%20to%20excel%20in%20tasks%20ranging%20from%20poem%20writing%0Aand%20coding%20to%20essay%20generation%20and%20puzzle%20solving.%20With%20the%20incorporation%20of%0Aimage%20generation%20capability%2C%20they%20have%20become%20more%20comprehensive%20and%20versatile%0AAI%20tools.%20At%20the%20same%20time%2C%20researchers%20are%20striving%20to%20identify%20the%0Alimitations%20of%20these%20tools%20to%20improve%20them%20further.%20Currently%20identified%20flaws%0Ainclude%20hallucination%2C%20biases%2C%20and%20bypassing%20restricted%20commands%20to%20generate%0Aharmful%20content.%20In%20the%20present%20work%2C%20we%20have%20identified%20a%20fundamental%0Alimitation%20related%20to%20the%20image%20generation%20ability%20of%20LLMs%2C%20and%20termed%20it%20The%0ANO%20Syndrome.%20This%20negation%20blindness%20refers%20to%20LLMs%20inability%20to%20correctly%0Acomprehend%20NO%20related%20natural%20language%20prompts%20to%20generate%20the%20desired%20images.%0AInterestingly%2C%20all%20tested%20LLMs%20including%20GPT-4%2C%20Gemini%2C%20and%20Copilot%20were%20found%0Ato%20be%20suffering%20from%20this%20syndrome.%20To%20demonstrate%20the%20generalization%20of%20this%0Alimitation%2C%20we%20carried%20out%20simulation%20experiments%20and%20conducted%20entropy-based%0Aand%20benchmark%20statistical%20analysis%20tests%20on%20various%20LLMs%20in%20multiple%20languages%2C%0Aincluding%20English%2C%20Hindi%2C%20and%20French.%20We%20conclude%20that%20the%20NO%20syndrome%20is%20a%0Asignificant%20flaw%20in%20current%20LLMs%20that%20needs%20to%20be%20addressed.%20A%20related%20finding%0Aof%20this%20study%20showed%20a%20consistent%20discrepancy%20between%20image%20and%20textual%0Aresponses%20as%20a%20result%20of%20this%20NO%20syndrome.%20We%20posit%20that%20the%20introduction%20of%20a%0Anegation%20context-aware%20reinforcement%20learning%20based%20feedback%20loop%20between%20the%0ALLMs%20textual%20response%20and%20generated%20image%20could%20help%20ensure%20the%20generated%20text%0Ais%20based%20on%20both%20the%20LLMs%20correct%20contextual%20understanding%20of%20the%20negation%0Aquery%20and%20the%20generated%20visual%20output.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00105v2&entry.124074799=Read"},
{"title": "Decision Transformer for Enhancing Neural Local Search on the Job Shop\n  Scheduling Problem", "author": "Constantin Waubert de Puiseau and Fabian Wolz and Merlin Montag and Jannik Peters and Hasan Tercan and Tobias Meisen", "abstract": "  The job shop scheduling problem (JSSP) and its solution algorithms have been\nof enduring interest in both academia and industry for decades. In recent\nyears, machine learning (ML) is playing an increasingly important role in\nadvancing existing and building new heuristic solutions for the JSSP, aiming to\nfind better solutions in shorter computation times. In this paper we build on\ntop of a state-of-the-art deep reinforcement learning (DRL) agent, called\nNeural Local Search (NLS), which can efficiently and effectively control a\nlarge local neighborhood search on the JSSP. In particular, we develop a method\nfor training the decision transformer (DT) algorithm on search trajectories\ntaken by a trained NLS agent to further improve upon the learned\ndecision-making sequences. Our experiments show that the DT successfully learns\nlocal search strategies that are different and, in many cases, more effective\nthan those of the NLS agent itself. In terms of the tradeoff between solution\nquality and acceptable computational time needed for the search, the DT is\nparticularly superior in application scenarios where longer computational times\nare acceptable. In this case, it makes up for the longer inference times\nrequired per search step, which are caused by the larger neural network\narchitecture, through better quality decisions per step. Thereby, the DT\nachieves state-of-the-art results for solving the JSSP with ML-enhanced search.\n", "link": "http://arxiv.org/abs/2409.02697v1", "date": "2024-09-04", "relevancy": 1.922, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4925}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision%20Transformer%20for%20Enhancing%20Neural%20Local%20Search%20on%20the%20Job%20Shop%0A%20%20Scheduling%20Problem&body=Title%3A%20Decision%20Transformer%20for%20Enhancing%20Neural%20Local%20Search%20on%20the%20Job%20Shop%0A%20%20Scheduling%20Problem%0AAuthor%3A%20Constantin%20Waubert%20de%20Puiseau%20and%20Fabian%20Wolz%20and%20Merlin%20Montag%20and%20Jannik%20Peters%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen%0AAbstract%3A%20%20%20The%20job%20shop%20scheduling%20problem%20%28JSSP%29%20and%20its%20solution%20algorithms%20have%20been%0Aof%20enduring%20interest%20in%20both%20academia%20and%20industry%20for%20decades.%20In%20recent%0Ayears%2C%20machine%20learning%20%28ML%29%20is%20playing%20an%20increasingly%20important%20role%20in%0Aadvancing%20existing%20and%20building%20new%20heuristic%20solutions%20for%20the%20JSSP%2C%20aiming%20to%0Afind%20better%20solutions%20in%20shorter%20computation%20times.%20In%20this%20paper%20we%20build%20on%0Atop%20of%20a%20state-of-the-art%20deep%20reinforcement%20learning%20%28DRL%29%20agent%2C%20called%0ANeural%20Local%20Search%20%28NLS%29%2C%20which%20can%20efficiently%20and%20effectively%20control%20a%0Alarge%20local%20neighborhood%20search%20on%20the%20JSSP.%20In%20particular%2C%20we%20develop%20a%20method%0Afor%20training%20the%20decision%20transformer%20%28DT%29%20algorithm%20on%20search%20trajectories%0Ataken%20by%20a%20trained%20NLS%20agent%20to%20further%20improve%20upon%20the%20learned%0Adecision-making%20sequences.%20Our%20experiments%20show%20that%20the%20DT%20successfully%20learns%0Alocal%20search%20strategies%20that%20are%20different%20and%2C%20in%20many%20cases%2C%20more%20effective%0Athan%20those%20of%20the%20NLS%20agent%20itself.%20In%20terms%20of%20the%20tradeoff%20between%20solution%0Aquality%20and%20acceptable%20computational%20time%20needed%20for%20the%20search%2C%20the%20DT%20is%0Aparticularly%20superior%20in%20application%20scenarios%20where%20longer%20computational%20times%0Aare%20acceptable.%20In%20this%20case%2C%20it%20makes%20up%20for%20the%20longer%20inference%20times%0Arequired%20per%20search%20step%2C%20which%20are%20caused%20by%20the%20larger%20neural%20network%0Aarchitecture%2C%20through%20better%20quality%20decisions%20per%20step.%20Thereby%2C%20the%20DT%0Aachieves%20state-of-the-art%20results%20for%20solving%20the%20JSSP%20with%20ML-enhanced%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision%2520Transformer%2520for%2520Enhancing%2520Neural%2520Local%2520Search%2520on%2520the%2520Job%2520Shop%250A%2520%2520Scheduling%2520Problem%26entry.906535625%3DConstantin%2520Waubert%2520de%2520Puiseau%2520and%2520Fabian%2520Wolz%2520and%2520Merlin%2520Montag%2520and%2520Jannik%2520Peters%2520and%2520Hasan%2520Tercan%2520and%2520Tobias%2520Meisen%26entry.1292438233%3D%2520%2520The%2520job%2520shop%2520scheduling%2520problem%2520%2528JSSP%2529%2520and%2520its%2520solution%2520algorithms%2520have%2520been%250Aof%2520enduring%2520interest%2520in%2520both%2520academia%2520and%2520industry%2520for%2520decades.%2520In%2520recent%250Ayears%252C%2520machine%2520learning%2520%2528ML%2529%2520is%2520playing%2520an%2520increasingly%2520important%2520role%2520in%250Aadvancing%2520existing%2520and%2520building%2520new%2520heuristic%2520solutions%2520for%2520the%2520JSSP%252C%2520aiming%2520to%250Afind%2520better%2520solutions%2520in%2520shorter%2520computation%2520times.%2520In%2520this%2520paper%2520we%2520build%2520on%250Atop%2520of%2520a%2520state-of-the-art%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520agent%252C%2520called%250ANeural%2520Local%2520Search%2520%2528NLS%2529%252C%2520which%2520can%2520efficiently%2520and%2520effectively%2520control%2520a%250Alarge%2520local%2520neighborhood%2520search%2520on%2520the%2520JSSP.%2520In%2520particular%252C%2520we%2520develop%2520a%2520method%250Afor%2520training%2520the%2520decision%2520transformer%2520%2528DT%2529%2520algorithm%2520on%2520search%2520trajectories%250Ataken%2520by%2520a%2520trained%2520NLS%2520agent%2520to%2520further%2520improve%2520upon%2520the%2520learned%250Adecision-making%2520sequences.%2520Our%2520experiments%2520show%2520that%2520the%2520DT%2520successfully%2520learns%250Alocal%2520search%2520strategies%2520that%2520are%2520different%2520and%252C%2520in%2520many%2520cases%252C%2520more%2520effective%250Athan%2520those%2520of%2520the%2520NLS%2520agent%2520itself.%2520In%2520terms%2520of%2520the%2520tradeoff%2520between%2520solution%250Aquality%2520and%2520acceptable%2520computational%2520time%2520needed%2520for%2520the%2520search%252C%2520the%2520DT%2520is%250Aparticularly%2520superior%2520in%2520application%2520scenarios%2520where%2520longer%2520computational%2520times%250Aare%2520acceptable.%2520In%2520this%2520case%252C%2520it%2520makes%2520up%2520for%2520the%2520longer%2520inference%2520times%250Arequired%2520per%2520search%2520step%252C%2520which%2520are%2520caused%2520by%2520the%2520larger%2520neural%2520network%250Aarchitecture%252C%2520through%2520better%2520quality%2520decisions%2520per%2520step.%2520Thereby%252C%2520the%2520DT%250Aachieves%2520state-of-the-art%2520results%2520for%2520solving%2520the%2520JSSP%2520with%2520ML-enhanced%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision%20Transformer%20for%20Enhancing%20Neural%20Local%20Search%20on%20the%20Job%20Shop%0A%20%20Scheduling%20Problem&entry.906535625=Constantin%20Waubert%20de%20Puiseau%20and%20Fabian%20Wolz%20and%20Merlin%20Montag%20and%20Jannik%20Peters%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen&entry.1292438233=%20%20The%20job%20shop%20scheduling%20problem%20%28JSSP%29%20and%20its%20solution%20algorithms%20have%20been%0Aof%20enduring%20interest%20in%20both%20academia%20and%20industry%20for%20decades.%20In%20recent%0Ayears%2C%20machine%20learning%20%28ML%29%20is%20playing%20an%20increasingly%20important%20role%20in%0Aadvancing%20existing%20and%20building%20new%20heuristic%20solutions%20for%20the%20JSSP%2C%20aiming%20to%0Afind%20better%20solutions%20in%20shorter%20computation%20times.%20In%20this%20paper%20we%20build%20on%0Atop%20of%20a%20state-of-the-art%20deep%20reinforcement%20learning%20%28DRL%29%20agent%2C%20called%0ANeural%20Local%20Search%20%28NLS%29%2C%20which%20can%20efficiently%20and%20effectively%20control%20a%0Alarge%20local%20neighborhood%20search%20on%20the%20JSSP.%20In%20particular%2C%20we%20develop%20a%20method%0Afor%20training%20the%20decision%20transformer%20%28DT%29%20algorithm%20on%20search%20trajectories%0Ataken%20by%20a%20trained%20NLS%20agent%20to%20further%20improve%20upon%20the%20learned%0Adecision-making%20sequences.%20Our%20experiments%20show%20that%20the%20DT%20successfully%20learns%0Alocal%20search%20strategies%20that%20are%20different%20and%2C%20in%20many%20cases%2C%20more%20effective%0Athan%20those%20of%20the%20NLS%20agent%20itself.%20In%20terms%20of%20the%20tradeoff%20between%20solution%0Aquality%20and%20acceptable%20computational%20time%20needed%20for%20the%20search%2C%20the%20DT%20is%0Aparticularly%20superior%20in%20application%20scenarios%20where%20longer%20computational%20times%0Aare%20acceptable.%20In%20this%20case%2C%20it%20makes%20up%20for%20the%20longer%20inference%20times%0Arequired%20per%20search%20step%2C%20which%20are%20caused%20by%20the%20larger%20neural%20network%0Aarchitecture%2C%20through%20better%20quality%20decisions%20per%20step.%20Thereby%2C%20the%20DT%0Aachieves%20state-of-the-art%20results%20for%20solving%20the%20JSSP%20with%20ML-enhanced%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02697v1&entry.124074799=Read"},
{"title": "LLM-Assisted Visual Analytics: Opportunities and Challenges", "author": "Maeve Hutchinson and Radu Jianu and Aidan Slingsby and Pranava Madhyastha", "abstract": "  We explore the integration of large language models (LLMs) into visual\nanalytics (VA) systems to transform their capabilities through intuitive\nnatural language interactions. We survey current research directions in this\nemerging field, examining how LLMs are integrated into data management,\nlanguage interaction, visualisation generation, and language generation\nprocesses. We highlight the new possibilities that LLMs bring to VA, especially\nhow they can change VA processes beyond the usual use cases. We especially\nhighlight building new visualisation-language models, allowing access of a\nbreadth of domain knowledge, multimodal interaction, and opportunities with\nguidance. Finally, we carefully consider the prominent challenges of using\ncurrent LLMs in VA tasks. Our discussions in this paper aim to guide future\nresearchers working on LLM-assisted VA systems and help them navigate common\nobstacles when developing these systems.\n", "link": "http://arxiv.org/abs/2409.02691v1", "date": "2024-09-04", "relevancy": 1.9212, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5081}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.461}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Assisted%20Visual%20Analytics%3A%20Opportunities%20and%20Challenges&body=Title%3A%20LLM-Assisted%20Visual%20Analytics%3A%20Opportunities%20and%20Challenges%0AAuthor%3A%20Maeve%20Hutchinson%20and%20Radu%20Jianu%20and%20Aidan%20Slingsby%20and%20Pranava%20Madhyastha%0AAbstract%3A%20%20%20We%20explore%20the%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20visual%0Aanalytics%20%28VA%29%20systems%20to%20transform%20their%20capabilities%20through%20intuitive%0Anatural%20language%20interactions.%20We%20survey%20current%20research%20directions%20in%20this%0Aemerging%20field%2C%20examining%20how%20LLMs%20are%20integrated%20into%20data%20management%2C%0Alanguage%20interaction%2C%20visualisation%20generation%2C%20and%20language%20generation%0Aprocesses.%20We%20highlight%20the%20new%20possibilities%20that%20LLMs%20bring%20to%20VA%2C%20especially%0Ahow%20they%20can%20change%20VA%20processes%20beyond%20the%20usual%20use%20cases.%20We%20especially%0Ahighlight%20building%20new%20visualisation-language%20models%2C%20allowing%20access%20of%20a%0Abreadth%20of%20domain%20knowledge%2C%20multimodal%20interaction%2C%20and%20opportunities%20with%0Aguidance.%20Finally%2C%20we%20carefully%20consider%20the%20prominent%20challenges%20of%20using%0Acurrent%20LLMs%20in%20VA%20tasks.%20Our%20discussions%20in%20this%20paper%20aim%20to%20guide%20future%0Aresearchers%20working%20on%20LLM-assisted%20VA%20systems%20and%20help%20them%20navigate%20common%0Aobstacles%20when%20developing%20these%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Assisted%2520Visual%2520Analytics%253A%2520Opportunities%2520and%2520Challenges%26entry.906535625%3DMaeve%2520Hutchinson%2520and%2520Radu%2520Jianu%2520and%2520Aidan%2520Slingsby%2520and%2520Pranava%2520Madhyastha%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520integration%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520visual%250Aanalytics%2520%2528VA%2529%2520systems%2520to%2520transform%2520their%2520capabilities%2520through%2520intuitive%250Anatural%2520language%2520interactions.%2520We%2520survey%2520current%2520research%2520directions%2520in%2520this%250Aemerging%2520field%252C%2520examining%2520how%2520LLMs%2520are%2520integrated%2520into%2520data%2520management%252C%250Alanguage%2520interaction%252C%2520visualisation%2520generation%252C%2520and%2520language%2520generation%250Aprocesses.%2520We%2520highlight%2520the%2520new%2520possibilities%2520that%2520LLMs%2520bring%2520to%2520VA%252C%2520especially%250Ahow%2520they%2520can%2520change%2520VA%2520processes%2520beyond%2520the%2520usual%2520use%2520cases.%2520We%2520especially%250Ahighlight%2520building%2520new%2520visualisation-language%2520models%252C%2520allowing%2520access%2520of%2520a%250Abreadth%2520of%2520domain%2520knowledge%252C%2520multimodal%2520interaction%252C%2520and%2520opportunities%2520with%250Aguidance.%2520Finally%252C%2520we%2520carefully%2520consider%2520the%2520prominent%2520challenges%2520of%2520using%250Acurrent%2520LLMs%2520in%2520VA%2520tasks.%2520Our%2520discussions%2520in%2520this%2520paper%2520aim%2520to%2520guide%2520future%250Aresearchers%2520working%2520on%2520LLM-assisted%2520VA%2520systems%2520and%2520help%2520them%2520navigate%2520common%250Aobstacles%2520when%2520developing%2520these%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Assisted%20Visual%20Analytics%3A%20Opportunities%20and%20Challenges&entry.906535625=Maeve%20Hutchinson%20and%20Radu%20Jianu%20and%20Aidan%20Slingsby%20and%20Pranava%20Madhyastha&entry.1292438233=%20%20We%20explore%20the%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20visual%0Aanalytics%20%28VA%29%20systems%20to%20transform%20their%20capabilities%20through%20intuitive%0Anatural%20language%20interactions.%20We%20survey%20current%20research%20directions%20in%20this%0Aemerging%20field%2C%20examining%20how%20LLMs%20are%20integrated%20into%20data%20management%2C%0Alanguage%20interaction%2C%20visualisation%20generation%2C%20and%20language%20generation%0Aprocesses.%20We%20highlight%20the%20new%20possibilities%20that%20LLMs%20bring%20to%20VA%2C%20especially%0Ahow%20they%20can%20change%20VA%20processes%20beyond%20the%20usual%20use%20cases.%20We%20especially%0Ahighlight%20building%20new%20visualisation-language%20models%2C%20allowing%20access%20of%20a%0Abreadth%20of%20domain%20knowledge%2C%20multimodal%20interaction%2C%20and%20opportunities%20with%0Aguidance.%20Finally%2C%20we%20carefully%20consider%20the%20prominent%20challenges%20of%20using%0Acurrent%20LLMs%20in%20VA%20tasks.%20Our%20discussions%20in%20this%20paper%20aim%20to%20guide%20future%0Aresearchers%20working%20on%20LLM-assisted%20VA%20systems%20and%20help%20them%20navigate%20common%0Aobstacles%20when%20developing%20these%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02691v1&entry.124074799=Read"},
{"title": "A Systematic Bias of Machine Learning Regression Models and Its\n  Correction: an Application to Imaging-based Brain Age Prediction", "author": "Hwiyoung Lee and Shuo Chen", "abstract": "  Machine learning models for continuous outcomes often yield systematically\nbiased predictions, particularly for values that largely deviate from the mean.\nSpecifically, predictions for large-valued outcomes tend to be negatively\nbiased (underestimating actual values), while those for small-valued outcomes\nare positively biased (overestimating actual values). We refer to this linear\ncentral tendency warped bias as the \"systematic bias of machine learning\nregression\". In this paper, we first demonstrate that this systematic\nprediction bias persists across various machine learning regression models, and\nthen delve into its theoretical underpinnings. To address this issue, we\npropose a general constrained optimization approach designed to correct this\nbias and develop computationally efficient implementation algorithms.\nSimulation results indicate that our correction method effectively eliminates\nthe bias from the predicted outcomes. We apply the proposed approach to the\nprediction of brain age using neuroimaging data. In comparison to competing\nmachine learning regression models, our method effectively addresses the\nlongstanding issue of \"systematic bias of machine learning regression\" in\nneuroimaging-based brain age calculation, yielding unbiased predictions of\nbrain age.\n", "link": "http://arxiv.org/abs/2405.15950v2", "date": "2024-09-04", "relevancy": 1.9083, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5143}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4709}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Bias%20of%20Machine%20Learning%20Regression%20Models%20and%20Its%0A%20%20Correction%3A%20an%20Application%20to%20Imaging-based%20Brain%20Age%20Prediction&body=Title%3A%20A%20Systematic%20Bias%20of%20Machine%20Learning%20Regression%20Models%20and%20Its%0A%20%20Correction%3A%20an%20Application%20to%20Imaging-based%20Brain%20Age%20Prediction%0AAuthor%3A%20Hwiyoung%20Lee%20and%20Shuo%20Chen%0AAbstract%3A%20%20%20Machine%20learning%20models%20for%20continuous%20outcomes%20often%20yield%20systematically%0Abiased%20predictions%2C%20particularly%20for%20values%20that%20largely%20deviate%20from%20the%20mean.%0ASpecifically%2C%20predictions%20for%20large-valued%20outcomes%20tend%20to%20be%20negatively%0Abiased%20%28underestimating%20actual%20values%29%2C%20while%20those%20for%20small-valued%20outcomes%0Aare%20positively%20biased%20%28overestimating%20actual%20values%29.%20We%20refer%20to%20this%20linear%0Acentral%20tendency%20warped%20bias%20as%20the%20%22systematic%20bias%20of%20machine%20learning%0Aregression%22.%20In%20this%20paper%2C%20we%20first%20demonstrate%20that%20this%20systematic%0Aprediction%20bias%20persists%20across%20various%20machine%20learning%20regression%20models%2C%20and%0Athen%20delve%20into%20its%20theoretical%20underpinnings.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20general%20constrained%20optimization%20approach%20designed%20to%20correct%20this%0Abias%20and%20develop%20computationally%20efficient%20implementation%20algorithms.%0ASimulation%20results%20indicate%20that%20our%20correction%20method%20effectively%20eliminates%0Athe%20bias%20from%20the%20predicted%20outcomes.%20We%20apply%20the%20proposed%20approach%20to%20the%0Aprediction%20of%20brain%20age%20using%20neuroimaging%20data.%20In%20comparison%20to%20competing%0Amachine%20learning%20regression%20models%2C%20our%20method%20effectively%20addresses%20the%0Alongstanding%20issue%20of%20%22systematic%20bias%20of%20machine%20learning%20regression%22%20in%0Aneuroimaging-based%20brain%20age%20calculation%2C%20yielding%20unbiased%20predictions%20of%0Abrain%20age.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15950v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Bias%2520of%2520Machine%2520Learning%2520Regression%2520Models%2520and%2520Its%250A%2520%2520Correction%253A%2520an%2520Application%2520to%2520Imaging-based%2520Brain%2520Age%2520Prediction%26entry.906535625%3DHwiyoung%2520Lee%2520and%2520Shuo%2520Chen%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520for%2520continuous%2520outcomes%2520often%2520yield%2520systematically%250Abiased%2520predictions%252C%2520particularly%2520for%2520values%2520that%2520largely%2520deviate%2520from%2520the%2520mean.%250ASpecifically%252C%2520predictions%2520for%2520large-valued%2520outcomes%2520tend%2520to%2520be%2520negatively%250Abiased%2520%2528underestimating%2520actual%2520values%2529%252C%2520while%2520those%2520for%2520small-valued%2520outcomes%250Aare%2520positively%2520biased%2520%2528overestimating%2520actual%2520values%2529.%2520We%2520refer%2520to%2520this%2520linear%250Acentral%2520tendency%2520warped%2520bias%2520as%2520the%2520%2522systematic%2520bias%2520of%2520machine%2520learning%250Aregression%2522.%2520In%2520this%2520paper%252C%2520we%2520first%2520demonstrate%2520that%2520this%2520systematic%250Aprediction%2520bias%2520persists%2520across%2520various%2520machine%2520learning%2520regression%2520models%252C%2520and%250Athen%2520delve%2520into%2520its%2520theoretical%2520underpinnings.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520general%2520constrained%2520optimization%2520approach%2520designed%2520to%2520correct%2520this%250Abias%2520and%2520develop%2520computationally%2520efficient%2520implementation%2520algorithms.%250ASimulation%2520results%2520indicate%2520that%2520our%2520correction%2520method%2520effectively%2520eliminates%250Athe%2520bias%2520from%2520the%2520predicted%2520outcomes.%2520We%2520apply%2520the%2520proposed%2520approach%2520to%2520the%250Aprediction%2520of%2520brain%2520age%2520using%2520neuroimaging%2520data.%2520In%2520comparison%2520to%2520competing%250Amachine%2520learning%2520regression%2520models%252C%2520our%2520method%2520effectively%2520addresses%2520the%250Alongstanding%2520issue%2520of%2520%2522systematic%2520bias%2520of%2520machine%2520learning%2520regression%2522%2520in%250Aneuroimaging-based%2520brain%2520age%2520calculation%252C%2520yielding%2520unbiased%2520predictions%2520of%250Abrain%2520age.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15950v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Bias%20of%20Machine%20Learning%20Regression%20Models%20and%20Its%0A%20%20Correction%3A%20an%20Application%20to%20Imaging-based%20Brain%20Age%20Prediction&entry.906535625=Hwiyoung%20Lee%20and%20Shuo%20Chen&entry.1292438233=%20%20Machine%20learning%20models%20for%20continuous%20outcomes%20often%20yield%20systematically%0Abiased%20predictions%2C%20particularly%20for%20values%20that%20largely%20deviate%20from%20the%20mean.%0ASpecifically%2C%20predictions%20for%20large-valued%20outcomes%20tend%20to%20be%20negatively%0Abiased%20%28underestimating%20actual%20values%29%2C%20while%20those%20for%20small-valued%20outcomes%0Aare%20positively%20biased%20%28overestimating%20actual%20values%29.%20We%20refer%20to%20this%20linear%0Acentral%20tendency%20warped%20bias%20as%20the%20%22systematic%20bias%20of%20machine%20learning%0Aregression%22.%20In%20this%20paper%2C%20we%20first%20demonstrate%20that%20this%20systematic%0Aprediction%20bias%20persists%20across%20various%20machine%20learning%20regression%20models%2C%20and%0Athen%20delve%20into%20its%20theoretical%20underpinnings.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20general%20constrained%20optimization%20approach%20designed%20to%20correct%20this%0Abias%20and%20develop%20computationally%20efficient%20implementation%20algorithms.%0ASimulation%20results%20indicate%20that%20our%20correction%20method%20effectively%20eliminates%0Athe%20bias%20from%20the%20predicted%20outcomes.%20We%20apply%20the%20proposed%20approach%20to%20the%0Aprediction%20of%20brain%20age%20using%20neuroimaging%20data.%20In%20comparison%20to%20competing%0Amachine%20learning%20regression%20models%2C%20our%20method%20effectively%20addresses%20the%0Alongstanding%20issue%20of%20%22systematic%20bias%20of%20machine%20learning%20regression%22%20in%0Aneuroimaging-based%20brain%20age%20calculation%2C%20yielding%20unbiased%20predictions%20of%0Abrain%20age.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15950v2&entry.124074799=Read"},
{"title": "Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency\n  Discussions by Few-Shot Learning with Large Language Models", "author": "Moein Shahiki Tash and Zahra Ahani and Mohim Tash and Olga Kolesnikova and Grigori Sidorov", "abstract": "  This study performs analysis of Predictive statements, Hope speech, and\nRegret Detection behaviors within cryptocurrency-related discussions,\nleveraging advanced natural language processing techniques. We introduce a\nnovel classification scheme named \"Prediction statements,\" categorizing\ncomments into Predictive Incremental, Predictive Decremental, Predictive\nNeutral, or Non-Predictive categories. Employing GPT-4o, a cutting-edge large\nlanguage model, we explore sentiment dynamics across five prominent\ncryptocurrencies: Cardano, Binance, Matic, Fantom, and Ripple. Our analysis\nreveals distinct patterns in predictive sentiments, with Matic demonstrating a\nnotably higher propensity for optimistic predictions. Additionally, we\ninvestigate hope and regret sentiments, uncovering nuanced interplay between\nthese emotions and predictive behaviors. Despite encountering limitations\nrelated to data volume and resource availability, our study reports valuable\ndiscoveries concerning investor behavior and sentiment trends within the\ncryptocurrency market, informing strategic decision-making and future research\nendeavors.\n", "link": "http://arxiv.org/abs/2409.02836v1", "date": "2024-09-04", "relevancy": 1.9082, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4762}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Sentiment%20Dynamics%20and%20Predictive%20Behaviors%20in%20Cryptocurrency%0A%20%20Discussions%20by%20Few-Shot%20Learning%20with%20Large%20Language%20Models&body=Title%3A%20Exploring%20Sentiment%20Dynamics%20and%20Predictive%20Behaviors%20in%20Cryptocurrency%0A%20%20Discussions%20by%20Few-Shot%20Learning%20with%20Large%20Language%20Models%0AAuthor%3A%20Moein%20Shahiki%20Tash%20and%20Zahra%20Ahani%20and%20Mohim%20Tash%20and%20Olga%20Kolesnikova%20and%20Grigori%20Sidorov%0AAbstract%3A%20%20%20This%20study%20performs%20analysis%20of%20Predictive%20statements%2C%20Hope%20speech%2C%20and%0ARegret%20Detection%20behaviors%20within%20cryptocurrency-related%20discussions%2C%0Aleveraging%20advanced%20natural%20language%20processing%20techniques.%20We%20introduce%20a%0Anovel%20classification%20scheme%20named%20%22Prediction%20statements%2C%22%20categorizing%0Acomments%20into%20Predictive%20Incremental%2C%20Predictive%20Decremental%2C%20Predictive%0ANeutral%2C%20or%20Non-Predictive%20categories.%20Employing%20GPT-4o%2C%20a%20cutting-edge%20large%0Alanguage%20model%2C%20we%20explore%20sentiment%20dynamics%20across%20five%20prominent%0Acryptocurrencies%3A%20Cardano%2C%20Binance%2C%20Matic%2C%20Fantom%2C%20and%20Ripple.%20Our%20analysis%0Areveals%20distinct%20patterns%20in%20predictive%20sentiments%2C%20with%20Matic%20demonstrating%20a%0Anotably%20higher%20propensity%20for%20optimistic%20predictions.%20Additionally%2C%20we%0Ainvestigate%20hope%20and%20regret%20sentiments%2C%20uncovering%20nuanced%20interplay%20between%0Athese%20emotions%20and%20predictive%20behaviors.%20Despite%20encountering%20limitations%0Arelated%20to%20data%20volume%20and%20resource%20availability%2C%20our%20study%20reports%20valuable%0Adiscoveries%20concerning%20investor%20behavior%20and%20sentiment%20trends%20within%20the%0Acryptocurrency%20market%2C%20informing%20strategic%20decision-making%20and%20future%20research%0Aendeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Sentiment%2520Dynamics%2520and%2520Predictive%2520Behaviors%2520in%2520Cryptocurrency%250A%2520%2520Discussions%2520by%2520Few-Shot%2520Learning%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DMoein%2520Shahiki%2520Tash%2520and%2520Zahra%2520Ahani%2520and%2520Mohim%2520Tash%2520and%2520Olga%2520Kolesnikova%2520and%2520Grigori%2520Sidorov%26entry.1292438233%3D%2520%2520This%2520study%2520performs%2520analysis%2520of%2520Predictive%2520statements%252C%2520Hope%2520speech%252C%2520and%250ARegret%2520Detection%2520behaviors%2520within%2520cryptocurrency-related%2520discussions%252C%250Aleveraging%2520advanced%2520natural%2520language%2520processing%2520techniques.%2520We%2520introduce%2520a%250Anovel%2520classification%2520scheme%2520named%2520%2522Prediction%2520statements%252C%2522%2520categorizing%250Acomments%2520into%2520Predictive%2520Incremental%252C%2520Predictive%2520Decremental%252C%2520Predictive%250ANeutral%252C%2520or%2520Non-Predictive%2520categories.%2520Employing%2520GPT-4o%252C%2520a%2520cutting-edge%2520large%250Alanguage%2520model%252C%2520we%2520explore%2520sentiment%2520dynamics%2520across%2520five%2520prominent%250Acryptocurrencies%253A%2520Cardano%252C%2520Binance%252C%2520Matic%252C%2520Fantom%252C%2520and%2520Ripple.%2520Our%2520analysis%250Areveals%2520distinct%2520patterns%2520in%2520predictive%2520sentiments%252C%2520with%2520Matic%2520demonstrating%2520a%250Anotably%2520higher%2520propensity%2520for%2520optimistic%2520predictions.%2520Additionally%252C%2520we%250Ainvestigate%2520hope%2520and%2520regret%2520sentiments%252C%2520uncovering%2520nuanced%2520interplay%2520between%250Athese%2520emotions%2520and%2520predictive%2520behaviors.%2520Despite%2520encountering%2520limitations%250Arelated%2520to%2520data%2520volume%2520and%2520resource%2520availability%252C%2520our%2520study%2520reports%2520valuable%250Adiscoveries%2520concerning%2520investor%2520behavior%2520and%2520sentiment%2520trends%2520within%2520the%250Acryptocurrency%2520market%252C%2520informing%2520strategic%2520decision-making%2520and%2520future%2520research%250Aendeavors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Sentiment%20Dynamics%20and%20Predictive%20Behaviors%20in%20Cryptocurrency%0A%20%20Discussions%20by%20Few-Shot%20Learning%20with%20Large%20Language%20Models&entry.906535625=Moein%20Shahiki%20Tash%20and%20Zahra%20Ahani%20and%20Mohim%20Tash%20and%20Olga%20Kolesnikova%20and%20Grigori%20Sidorov&entry.1292438233=%20%20This%20study%20performs%20analysis%20of%20Predictive%20statements%2C%20Hope%20speech%2C%20and%0ARegret%20Detection%20behaviors%20within%20cryptocurrency-related%20discussions%2C%0Aleveraging%20advanced%20natural%20language%20processing%20techniques.%20We%20introduce%20a%0Anovel%20classification%20scheme%20named%20%22Prediction%20statements%2C%22%20categorizing%0Acomments%20into%20Predictive%20Incremental%2C%20Predictive%20Decremental%2C%20Predictive%0ANeutral%2C%20or%20Non-Predictive%20categories.%20Employing%20GPT-4o%2C%20a%20cutting-edge%20large%0Alanguage%20model%2C%20we%20explore%20sentiment%20dynamics%20across%20five%20prominent%0Acryptocurrencies%3A%20Cardano%2C%20Binance%2C%20Matic%2C%20Fantom%2C%20and%20Ripple.%20Our%20analysis%0Areveals%20distinct%20patterns%20in%20predictive%20sentiments%2C%20with%20Matic%20demonstrating%20a%0Anotably%20higher%20propensity%20for%20optimistic%20predictions.%20Additionally%2C%20we%0Ainvestigate%20hope%20and%20regret%20sentiments%2C%20uncovering%20nuanced%20interplay%20between%0Athese%20emotions%20and%20predictive%20behaviors.%20Despite%20encountering%20limitations%0Arelated%20to%20data%20volume%20and%20resource%20availability%2C%20our%20study%20reports%20valuable%0Adiscoveries%20concerning%20investor%20behavior%20and%20sentiment%20trends%20within%20the%0Acryptocurrency%20market%2C%20informing%20strategic%20decision-making%20and%20future%20research%0Aendeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02836v1&entry.124074799=Read"},
{"title": "A Fashion Item Recommendation Model in Hyperbolic Space", "author": "Ryotaro Shimizu and Yu Wang and Masanari Kimura and Yuki Hirakawa and Takashi Wada and Yuki Saito and Julian McAuley", "abstract": "  In this work, we propose a fashion item recommendation model that\nincorporates hyperbolic geometry into user and item representations. Using\nhyperbolic space, our model aims to capture implicit hierarchies among items\nbased on their visual data and users' purchase history. During training, we\napply a multi-task learning framework that considers both hyperbolic and\nEuclidean distances in the loss function. Our experiments on three data sets\nshow that our model performs better than previous models trained in Euclidean\nspace only, confirming the effectiveness of our model. Our ablation studies\nshow that multi-task learning plays a key role, and removing the Euclidean loss\nsubstantially deteriorates the model performance.\n", "link": "http://arxiv.org/abs/2409.02599v1", "date": "2024-09-04", "relevancy": 1.8904, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5436}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4671}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fashion%20Item%20Recommendation%20Model%20in%20Hyperbolic%20Space&body=Title%3A%20A%20Fashion%20Item%20Recommendation%20Model%20in%20Hyperbolic%20Space%0AAuthor%3A%20Ryotaro%20Shimizu%20and%20Yu%20Wang%20and%20Masanari%20Kimura%20and%20Yuki%20Hirakawa%20and%20Takashi%20Wada%20and%20Yuki%20Saito%20and%20Julian%20McAuley%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20fashion%20item%20recommendation%20model%20that%0Aincorporates%20hyperbolic%20geometry%20into%20user%20and%20item%20representations.%20Using%0Ahyperbolic%20space%2C%20our%20model%20aims%20to%20capture%20implicit%20hierarchies%20among%20items%0Abased%20on%20their%20visual%20data%20and%20users%27%20purchase%20history.%20During%20training%2C%20we%0Aapply%20a%20multi-task%20learning%20framework%20that%20considers%20both%20hyperbolic%20and%0AEuclidean%20distances%20in%20the%20loss%20function.%20Our%20experiments%20on%20three%20data%20sets%0Ashow%20that%20our%20model%20performs%20better%20than%20previous%20models%20trained%20in%20Euclidean%0Aspace%20only%2C%20confirming%20the%20effectiveness%20of%20our%20model.%20Our%20ablation%20studies%0Ashow%20that%20multi-task%20learning%20plays%20a%20key%20role%2C%20and%20removing%20the%20Euclidean%20loss%0Asubstantially%20deteriorates%20the%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fashion%2520Item%2520Recommendation%2520Model%2520in%2520Hyperbolic%2520Space%26entry.906535625%3DRyotaro%2520Shimizu%2520and%2520Yu%2520Wang%2520and%2520Masanari%2520Kimura%2520and%2520Yuki%2520Hirakawa%2520and%2520Takashi%2520Wada%2520and%2520Yuki%2520Saito%2520and%2520Julian%2520McAuley%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fashion%2520item%2520recommendation%2520model%2520that%250Aincorporates%2520hyperbolic%2520geometry%2520into%2520user%2520and%2520item%2520representations.%2520Using%250Ahyperbolic%2520space%252C%2520our%2520model%2520aims%2520to%2520capture%2520implicit%2520hierarchies%2520among%2520items%250Abased%2520on%2520their%2520visual%2520data%2520and%2520users%2527%2520purchase%2520history.%2520During%2520training%252C%2520we%250Aapply%2520a%2520multi-task%2520learning%2520framework%2520that%2520considers%2520both%2520hyperbolic%2520and%250AEuclidean%2520distances%2520in%2520the%2520loss%2520function.%2520Our%2520experiments%2520on%2520three%2520data%2520sets%250Ashow%2520that%2520our%2520model%2520performs%2520better%2520than%2520previous%2520models%2520trained%2520in%2520Euclidean%250Aspace%2520only%252C%2520confirming%2520the%2520effectiveness%2520of%2520our%2520model.%2520Our%2520ablation%2520studies%250Ashow%2520that%2520multi-task%2520learning%2520plays%2520a%2520key%2520role%252C%2520and%2520removing%2520the%2520Euclidean%2520loss%250Asubstantially%2520deteriorates%2520the%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fashion%20Item%20Recommendation%20Model%20in%20Hyperbolic%20Space&entry.906535625=Ryotaro%20Shimizu%20and%20Yu%20Wang%20and%20Masanari%20Kimura%20and%20Yuki%20Hirakawa%20and%20Takashi%20Wada%20and%20Yuki%20Saito%20and%20Julian%20McAuley&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20fashion%20item%20recommendation%20model%20that%0Aincorporates%20hyperbolic%20geometry%20into%20user%20and%20item%20representations.%20Using%0Ahyperbolic%20space%2C%20our%20model%20aims%20to%20capture%20implicit%20hierarchies%20among%20items%0Abased%20on%20their%20visual%20data%20and%20users%27%20purchase%20history.%20During%20training%2C%20we%0Aapply%20a%20multi-task%20learning%20framework%20that%20considers%20both%20hyperbolic%20and%0AEuclidean%20distances%20in%20the%20loss%20function.%20Our%20experiments%20on%20three%20data%20sets%0Ashow%20that%20our%20model%20performs%20better%20than%20previous%20models%20trained%20in%20Euclidean%0Aspace%20only%2C%20confirming%20the%20effectiveness%20of%20our%20model.%20Our%20ablation%20studies%0Ashow%20that%20multi-task%20learning%20plays%20a%20key%20role%2C%20and%20removing%20the%20Euclidean%20loss%0Asubstantially%20deteriorates%20the%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02599v1&entry.124074799=Read"},
{"title": "Hybrid-Segmentor: A Hybrid Approach to Automated Fine-Grained Crack\n  Segmentation in Civil Infrastructure", "author": "June Moh Goo and Xenios Milidonis and Alessandro Artusi and Jan Boehm and Carlo Ciliberto", "abstract": "  Detecting and segmenting cracks in infrastructure, such as roads and\nbuildings, is crucial for safety and cost-effective maintenance. In spite of\nthe potential of deep learning, there are challenges in achieving precise\nresults and handling diverse crack types. With the proposed dataset and model,\nwe aim to enhance crack detection and infrastructure maintenance. We introduce\nHybrid-Segmentor, an encoder-decoder based approach that is capable of\nextracting both fine-grained local and global crack features. This allows the\nmodel to improve its generalization capabilities in distinguish various type of\nshapes, surfaces and sizes of cracks. To keep the computational performances\nlow for practical purposes, while maintaining the high the generalization\ncapabilities of the model, we incorporate a self-attention model at the encoder\nlevel, while reducing the complexity of the decoder component. The proposed\nmodel outperforms existing benchmark models across 5 quantitative metrics\n(accuracy 0.971, precision 0.804, recall 0.744, F1-score 0.770, and IoU score\n0.630), achieving state-of-the-art status.\n", "link": "http://arxiv.org/abs/2409.02866v1", "date": "2024-09-04", "relevancy": 1.8859, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4745}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-Segmentor%3A%20A%20Hybrid%20Approach%20to%20Automated%20Fine-Grained%20Crack%0A%20%20Segmentation%20in%20Civil%20Infrastructure&body=Title%3A%20Hybrid-Segmentor%3A%20A%20Hybrid%20Approach%20to%20Automated%20Fine-Grained%20Crack%0A%20%20Segmentation%20in%20Civil%20Infrastructure%0AAuthor%3A%20June%20Moh%20Goo%20and%20Xenios%20Milidonis%20and%20Alessandro%20Artusi%20and%20Jan%20Boehm%20and%20Carlo%20Ciliberto%0AAbstract%3A%20%20%20Detecting%20and%20segmenting%20cracks%20in%20infrastructure%2C%20such%20as%20roads%20and%0Abuildings%2C%20is%20crucial%20for%20safety%20and%20cost-effective%20maintenance.%20In%20spite%20of%0Athe%20potential%20of%20deep%20learning%2C%20there%20are%20challenges%20in%20achieving%20precise%0Aresults%20and%20handling%20diverse%20crack%20types.%20With%20the%20proposed%20dataset%20and%20model%2C%0Awe%20aim%20to%20enhance%20crack%20detection%20and%20infrastructure%20maintenance.%20We%20introduce%0AHybrid-Segmentor%2C%20an%20encoder-decoder%20based%20approach%20that%20is%20capable%20of%0Aextracting%20both%20fine-grained%20local%20and%20global%20crack%20features.%20This%20allows%20the%0Amodel%20to%20improve%20its%20generalization%20capabilities%20in%20distinguish%20various%20type%20of%0Ashapes%2C%20surfaces%20and%20sizes%20of%20cracks.%20To%20keep%20the%20computational%20performances%0Alow%20for%20practical%20purposes%2C%20while%20maintaining%20the%20high%20the%20generalization%0Acapabilities%20of%20the%20model%2C%20we%20incorporate%20a%20self-attention%20model%20at%20the%20encoder%0Alevel%2C%20while%20reducing%20the%20complexity%20of%20the%20decoder%20component.%20The%20proposed%0Amodel%20outperforms%20existing%20benchmark%20models%20across%205%20quantitative%20metrics%0A%28accuracy%200.971%2C%20precision%200.804%2C%20recall%200.744%2C%20F1-score%200.770%2C%20and%20IoU%20score%0A0.630%29%2C%20achieving%20state-of-the-art%20status.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-Segmentor%253A%2520A%2520Hybrid%2520Approach%2520to%2520Automated%2520Fine-Grained%2520Crack%250A%2520%2520Segmentation%2520in%2520Civil%2520Infrastructure%26entry.906535625%3DJune%2520Moh%2520Goo%2520and%2520Xenios%2520Milidonis%2520and%2520Alessandro%2520Artusi%2520and%2520Jan%2520Boehm%2520and%2520Carlo%2520Ciliberto%26entry.1292438233%3D%2520%2520Detecting%2520and%2520segmenting%2520cracks%2520in%2520infrastructure%252C%2520such%2520as%2520roads%2520and%250Abuildings%252C%2520is%2520crucial%2520for%2520safety%2520and%2520cost-effective%2520maintenance.%2520In%2520spite%2520of%250Athe%2520potential%2520of%2520deep%2520learning%252C%2520there%2520are%2520challenges%2520in%2520achieving%2520precise%250Aresults%2520and%2520handling%2520diverse%2520crack%2520types.%2520With%2520the%2520proposed%2520dataset%2520and%2520model%252C%250Awe%2520aim%2520to%2520enhance%2520crack%2520detection%2520and%2520infrastructure%2520maintenance.%2520We%2520introduce%250AHybrid-Segmentor%252C%2520an%2520encoder-decoder%2520based%2520approach%2520that%2520is%2520capable%2520of%250Aextracting%2520both%2520fine-grained%2520local%2520and%2520global%2520crack%2520features.%2520This%2520allows%2520the%250Amodel%2520to%2520improve%2520its%2520generalization%2520capabilities%2520in%2520distinguish%2520various%2520type%2520of%250Ashapes%252C%2520surfaces%2520and%2520sizes%2520of%2520cracks.%2520To%2520keep%2520the%2520computational%2520performances%250Alow%2520for%2520practical%2520purposes%252C%2520while%2520maintaining%2520the%2520high%2520the%2520generalization%250Acapabilities%2520of%2520the%2520model%252C%2520we%2520incorporate%2520a%2520self-attention%2520model%2520at%2520the%2520encoder%250Alevel%252C%2520while%2520reducing%2520the%2520complexity%2520of%2520the%2520decoder%2520component.%2520The%2520proposed%250Amodel%2520outperforms%2520existing%2520benchmark%2520models%2520across%25205%2520quantitative%2520metrics%250A%2528accuracy%25200.971%252C%2520precision%25200.804%252C%2520recall%25200.744%252C%2520F1-score%25200.770%252C%2520and%2520IoU%2520score%250A0.630%2529%252C%2520achieving%2520state-of-the-art%2520status.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-Segmentor%3A%20A%20Hybrid%20Approach%20to%20Automated%20Fine-Grained%20Crack%0A%20%20Segmentation%20in%20Civil%20Infrastructure&entry.906535625=June%20Moh%20Goo%20and%20Xenios%20Milidonis%20and%20Alessandro%20Artusi%20and%20Jan%20Boehm%20and%20Carlo%20Ciliberto&entry.1292438233=%20%20Detecting%20and%20segmenting%20cracks%20in%20infrastructure%2C%20such%20as%20roads%20and%0Abuildings%2C%20is%20crucial%20for%20safety%20and%20cost-effective%20maintenance.%20In%20spite%20of%0Athe%20potential%20of%20deep%20learning%2C%20there%20are%20challenges%20in%20achieving%20precise%0Aresults%20and%20handling%20diverse%20crack%20types.%20With%20the%20proposed%20dataset%20and%20model%2C%0Awe%20aim%20to%20enhance%20crack%20detection%20and%20infrastructure%20maintenance.%20We%20introduce%0AHybrid-Segmentor%2C%20an%20encoder-decoder%20based%20approach%20that%20is%20capable%20of%0Aextracting%20both%20fine-grained%20local%20and%20global%20crack%20features.%20This%20allows%20the%0Amodel%20to%20improve%20its%20generalization%20capabilities%20in%20distinguish%20various%20type%20of%0Ashapes%2C%20surfaces%20and%20sizes%20of%20cracks.%20To%20keep%20the%20computational%20performances%0Alow%20for%20practical%20purposes%2C%20while%20maintaining%20the%20high%20the%20generalization%0Acapabilities%20of%20the%20model%2C%20we%20incorporate%20a%20self-attention%20model%20at%20the%20encoder%0Alevel%2C%20while%20reducing%20the%20complexity%20of%20the%20decoder%20component.%20The%20proposed%0Amodel%20outperforms%20existing%20benchmark%20models%20across%205%20quantitative%20metrics%0A%28accuracy%200.971%2C%20precision%200.804%2C%20recall%200.744%2C%20F1-score%200.770%2C%20and%20IoU%20score%0A0.630%29%2C%20achieving%20state-of-the-art%20status.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02866v1&entry.124074799=Read"},
{"title": "SNNAX -- Spiking Neural Networks in JAX", "author": "Jamie Lohoff and Jan Finkbeiner and Emre Neftci", "abstract": "  Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature.\n", "link": "http://arxiv.org/abs/2409.02842v1", "date": "2024-09-04", "relevancy": 1.8822, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4888}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4859}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNNAX%20--%20Spiking%20Neural%20Networks%20in%20JAX&body=Title%3A%20SNNAX%20--%20Spiking%20Neural%20Networks%20in%20JAX%0AAuthor%3A%20Jamie%20Lohoff%20and%20Jan%20Finkbeiner%20and%20Emre%20Neftci%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20simulators%20are%20essential%20tools%20to%20prototype%0Abiologically%20inspired%20models%20and%20neuromorphic%20hardware%20architectures%20and%0Apredict%20their%20performance.%20For%20such%20a%20tool%2C%20ease%20of%20use%20and%20flexibility%20are%0Acritical%2C%20but%20so%20is%20simulation%20speed%20especially%20given%20the%20complexity%20inherent%0Ato%20simulating%20SNN.%20Here%2C%20we%20present%20SNNAX%2C%20a%20JAX-based%20framework%20for%20simulating%0Aand%20training%20such%20models%20with%20PyTorch-like%20intuitiveness%20and%20JAX-like%20execution%0Aspeed.%20SNNAX%20models%20are%20easily%20extended%20and%20customized%20to%20fit%20the%20desired%20model%0Aspecifications%20and%20target%20neuromorphic%20hardware.%20Additionally%2C%20SNNAX%20offers%20key%0Afeatures%20for%20optimizing%20the%20training%20and%20deployment%20of%20SNNs%20such%20as%20flexible%0Aautomatic%20differentiation%20and%20just-in-time%20compilation.%20We%20evaluate%20and%20compare%0ASNNAX%20to%20other%20commonly%20used%20machine%20learning%20%28ML%29%20frameworks%20used%20for%0Aprogramming%20SNNs.%20We%20provide%20key%20performance%20metrics%2C%20best%20practices%2C%0Adocumented%20examples%20for%20simulating%20SNNs%20in%20SNNAX%2C%20and%20implement%20several%0Abenchmarks%20used%20in%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNNAX%2520--%2520Spiking%2520Neural%2520Networks%2520in%2520JAX%26entry.906535625%3DJamie%2520Lohoff%2520and%2520Jan%2520Finkbeiner%2520and%2520Emre%2520Neftci%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520simulators%2520are%2520essential%2520tools%2520to%2520prototype%250Abiologically%2520inspired%2520models%2520and%2520neuromorphic%2520hardware%2520architectures%2520and%250Apredict%2520their%2520performance.%2520For%2520such%2520a%2520tool%252C%2520ease%2520of%2520use%2520and%2520flexibility%2520are%250Acritical%252C%2520but%2520so%2520is%2520simulation%2520speed%2520especially%2520given%2520the%2520complexity%2520inherent%250Ato%2520simulating%2520SNN.%2520Here%252C%2520we%2520present%2520SNNAX%252C%2520a%2520JAX-based%2520framework%2520for%2520simulating%250Aand%2520training%2520such%2520models%2520with%2520PyTorch-like%2520intuitiveness%2520and%2520JAX-like%2520execution%250Aspeed.%2520SNNAX%2520models%2520are%2520easily%2520extended%2520and%2520customized%2520to%2520fit%2520the%2520desired%2520model%250Aspecifications%2520and%2520target%2520neuromorphic%2520hardware.%2520Additionally%252C%2520SNNAX%2520offers%2520key%250Afeatures%2520for%2520optimizing%2520the%2520training%2520and%2520deployment%2520of%2520SNNs%2520such%2520as%2520flexible%250Aautomatic%2520differentiation%2520and%2520just-in-time%2520compilation.%2520We%2520evaluate%2520and%2520compare%250ASNNAX%2520to%2520other%2520commonly%2520used%2520machine%2520learning%2520%2528ML%2529%2520frameworks%2520used%2520for%250Aprogramming%2520SNNs.%2520We%2520provide%2520key%2520performance%2520metrics%252C%2520best%2520practices%252C%250Adocumented%2520examples%2520for%2520simulating%2520SNNs%2520in%2520SNNAX%252C%2520and%2520implement%2520several%250Abenchmarks%2520used%2520in%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNNAX%20--%20Spiking%20Neural%20Networks%20in%20JAX&entry.906535625=Jamie%20Lohoff%20and%20Jan%20Finkbeiner%20and%20Emre%20Neftci&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20simulators%20are%20essential%20tools%20to%20prototype%0Abiologically%20inspired%20models%20and%20neuromorphic%20hardware%20architectures%20and%0Apredict%20their%20performance.%20For%20such%20a%20tool%2C%20ease%20of%20use%20and%20flexibility%20are%0Acritical%2C%20but%20so%20is%20simulation%20speed%20especially%20given%20the%20complexity%20inherent%0Ato%20simulating%20SNN.%20Here%2C%20we%20present%20SNNAX%2C%20a%20JAX-based%20framework%20for%20simulating%0Aand%20training%20such%20models%20with%20PyTorch-like%20intuitiveness%20and%20JAX-like%20execution%0Aspeed.%20SNNAX%20models%20are%20easily%20extended%20and%20customized%20to%20fit%20the%20desired%20model%0Aspecifications%20and%20target%20neuromorphic%20hardware.%20Additionally%2C%20SNNAX%20offers%20key%0Afeatures%20for%20optimizing%20the%20training%20and%20deployment%20of%20SNNs%20such%20as%20flexible%0Aautomatic%20differentiation%20and%20just-in-time%20compilation.%20We%20evaluate%20and%20compare%0ASNNAX%20to%20other%20commonly%20used%20machine%20learning%20%28ML%29%20frameworks%20used%20for%0Aprogramming%20SNNs.%20We%20provide%20key%20performance%20metrics%2C%20best%20practices%2C%0Adocumented%20examples%20for%20simulating%20SNNs%20in%20SNNAX%2C%20and%20implement%20several%0Abenchmarks%20used%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02842v1&entry.124074799=Read"},
{"title": "Complete and Efficient Covariants for 3D Point Configurations with\n  Application to Learning Molecular Quantum Properties", "author": "Hartmut Maennel and Oliver T. Unke and Klaus-Robert M\u00fcller", "abstract": "  When modeling physical properties of molecules with machine learning, it is\ndesirable to incorporate $SO(3)$-covariance. While such models based on low\nbody order features are not complete, we formulate and prove general\ncompleteness properties for higher order methods, and show that $6k-5$ of these\nfeatures are enough for up to $k$ atoms. We also find that the Clebsch--Gordan\noperations commonly used in these methods can be replaced by matrix\nmultiplications without sacrificing completeness, lowering the scaling from\n$O(l^6)$ to $O(l^3)$ in the degree of the features. We apply this to quantum\nchemistry, but the proposed methods are generally applicable for problems\ninvolving 3D point configurations.\n", "link": "http://arxiv.org/abs/2409.02730v1", "date": "2024-09-04", "relevancy": 1.8822, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.478}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4691}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complete%20and%20Efficient%20Covariants%20for%203D%20Point%20Configurations%20with%0A%20%20Application%20to%20Learning%20Molecular%20Quantum%20Properties&body=Title%3A%20Complete%20and%20Efficient%20Covariants%20for%203D%20Point%20Configurations%20with%0A%20%20Application%20to%20Learning%20Molecular%20Quantum%20Properties%0AAuthor%3A%20Hartmut%20Maennel%20and%20Oliver%20T.%20Unke%20and%20Klaus-Robert%20M%C3%BCller%0AAbstract%3A%20%20%20When%20modeling%20physical%20properties%20of%20molecules%20with%20machine%20learning%2C%20it%20is%0Adesirable%20to%20incorporate%20%24SO%283%29%24-covariance.%20While%20such%20models%20based%20on%20low%0Abody%20order%20features%20are%20not%20complete%2C%20we%20formulate%20and%20prove%20general%0Acompleteness%20properties%20for%20higher%20order%20methods%2C%20and%20show%20that%20%246k-5%24%20of%20these%0Afeatures%20are%20enough%20for%20up%20to%20%24k%24%20atoms.%20We%20also%20find%20that%20the%20Clebsch--Gordan%0Aoperations%20commonly%20used%20in%20these%20methods%20can%20be%20replaced%20by%20matrix%0Amultiplications%20without%20sacrificing%20completeness%2C%20lowering%20the%20scaling%20from%0A%24O%28l%5E6%29%24%20to%20%24O%28l%5E3%29%24%20in%20the%20degree%20of%20the%20features.%20We%20apply%20this%20to%20quantum%0Achemistry%2C%20but%20the%20proposed%20methods%20are%20generally%20applicable%20for%20problems%0Ainvolving%203D%20point%20configurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplete%2520and%2520Efficient%2520Covariants%2520for%25203D%2520Point%2520Configurations%2520with%250A%2520%2520Application%2520to%2520Learning%2520Molecular%2520Quantum%2520Properties%26entry.906535625%3DHartmut%2520Maennel%2520and%2520Oliver%2520T.%2520Unke%2520and%2520Klaus-Robert%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520When%2520modeling%2520physical%2520properties%2520of%2520molecules%2520with%2520machine%2520learning%252C%2520it%2520is%250Adesirable%2520to%2520incorporate%2520%2524SO%25283%2529%2524-covariance.%2520While%2520such%2520models%2520based%2520on%2520low%250Abody%2520order%2520features%2520are%2520not%2520complete%252C%2520we%2520formulate%2520and%2520prove%2520general%250Acompleteness%2520properties%2520for%2520higher%2520order%2520methods%252C%2520and%2520show%2520that%2520%25246k-5%2524%2520of%2520these%250Afeatures%2520are%2520enough%2520for%2520up%2520to%2520%2524k%2524%2520atoms.%2520We%2520also%2520find%2520that%2520the%2520Clebsch--Gordan%250Aoperations%2520commonly%2520used%2520in%2520these%2520methods%2520can%2520be%2520replaced%2520by%2520matrix%250Amultiplications%2520without%2520sacrificing%2520completeness%252C%2520lowering%2520the%2520scaling%2520from%250A%2524O%2528l%255E6%2529%2524%2520to%2520%2524O%2528l%255E3%2529%2524%2520in%2520the%2520degree%2520of%2520the%2520features.%2520We%2520apply%2520this%2520to%2520quantum%250Achemistry%252C%2520but%2520the%2520proposed%2520methods%2520are%2520generally%2520applicable%2520for%2520problems%250Ainvolving%25203D%2520point%2520configurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complete%20and%20Efficient%20Covariants%20for%203D%20Point%20Configurations%20with%0A%20%20Application%20to%20Learning%20Molecular%20Quantum%20Properties&entry.906535625=Hartmut%20Maennel%20and%20Oliver%20T.%20Unke%20and%20Klaus-Robert%20M%C3%BCller&entry.1292438233=%20%20When%20modeling%20physical%20properties%20of%20molecules%20with%20machine%20learning%2C%20it%20is%0Adesirable%20to%20incorporate%20%24SO%283%29%24-covariance.%20While%20such%20models%20based%20on%20low%0Abody%20order%20features%20are%20not%20complete%2C%20we%20formulate%20and%20prove%20general%0Acompleteness%20properties%20for%20higher%20order%20methods%2C%20and%20show%20that%20%246k-5%24%20of%20these%0Afeatures%20are%20enough%20for%20up%20to%20%24k%24%20atoms.%20We%20also%20find%20that%20the%20Clebsch--Gordan%0Aoperations%20commonly%20used%20in%20these%20methods%20can%20be%20replaced%20by%20matrix%0Amultiplications%20without%20sacrificing%20completeness%2C%20lowering%20the%20scaling%20from%0A%24O%28l%5E6%29%24%20to%20%24O%28l%5E3%29%24%20in%20the%20degree%20of%20the%20features.%20We%20apply%20this%20to%20quantum%0Achemistry%2C%20but%20the%20proposed%20methods%20are%20generally%20applicable%20for%20problems%0Ainvolving%203D%20point%20configurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02730v1&entry.124074799=Read"},
{"title": "Oops, I Sampled it Again: Reinterpreting Confidence Intervals in\n  Few-Shot Learning", "author": "Raphael Lafargue and Luke Smith and Franck Vermet and Mathias L\u00f6we and Ian Reid and Vincent Gripon and Jack Valmadre", "abstract": "  The predominant method for computing confidence intervals (CI) in few-shot\nlearning (FSL) is based on sampling the tasks with replacement, i.e.\\ allowing\nthe same samples to appear in multiple tasks. This makes the CI misleading in\nthat it takes into account the randomness of the sampler but not the data\nitself. To quantify the extent of this problem, we conduct a comparative\nanalysis between CIs computed with and without replacement. These reveal a\nnotable underestimation by the predominant method. This observation calls for a\nreevaluation of how we interpret confidence intervals and the resulting\nconclusions in FSL comparative studies. Our research demonstrates that the use\nof paired tests can partially address this issue. Additionally, we explore\nmethods to further reduce the (size of the) CI by strategically sampling tasks\nof a specific size. We also introduce a new optimized benchmark, which can be\naccessed at https://github.com/RafLaf/FSL-benchmark-again\n", "link": "http://arxiv.org/abs/2409.02850v1", "date": "2024-09-04", "relevancy": 1.871, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oops%2C%20I%20Sampled%20it%20Again%3A%20Reinterpreting%20Confidence%20Intervals%20in%0A%20%20Few-Shot%20Learning&body=Title%3A%20Oops%2C%20I%20Sampled%20it%20Again%3A%20Reinterpreting%20Confidence%20Intervals%20in%0A%20%20Few-Shot%20Learning%0AAuthor%3A%20Raphael%20Lafargue%20and%20Luke%20Smith%20and%20Franck%20Vermet%20and%20Mathias%20L%C3%B6we%20and%20Ian%20Reid%20and%20Vincent%20Gripon%20and%20Jack%20Valmadre%0AAbstract%3A%20%20%20The%20predominant%20method%20for%20computing%20confidence%20intervals%20%28CI%29%20in%20few-shot%0Alearning%20%28FSL%29%20is%20based%20on%20sampling%20the%20tasks%20with%20replacement%2C%20i.e.%5C%20allowing%0Athe%20same%20samples%20to%20appear%20in%20multiple%20tasks.%20This%20makes%20the%20CI%20misleading%20in%0Athat%20it%20takes%20into%20account%20the%20randomness%20of%20the%20sampler%20but%20not%20the%20data%0Aitself.%20To%20quantify%20the%20extent%20of%20this%20problem%2C%20we%20conduct%20a%20comparative%0Aanalysis%20between%20CIs%20computed%20with%20and%20without%20replacement.%20These%20reveal%20a%0Anotable%20underestimation%20by%20the%20predominant%20method.%20This%20observation%20calls%20for%20a%0Areevaluation%20of%20how%20we%20interpret%20confidence%20intervals%20and%20the%20resulting%0Aconclusions%20in%20FSL%20comparative%20studies.%20Our%20research%20demonstrates%20that%20the%20use%0Aof%20paired%20tests%20can%20partially%20address%20this%20issue.%20Additionally%2C%20we%20explore%0Amethods%20to%20further%20reduce%20the%20%28size%20of%20the%29%20CI%20by%20strategically%20sampling%20tasks%0Aof%20a%20specific%20size.%20We%20also%20introduce%20a%20new%20optimized%20benchmark%2C%20which%20can%20be%0Aaccessed%20at%20https%3A//github.com/RafLaf/FSL-benchmark-again%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOops%252C%2520I%2520Sampled%2520it%2520Again%253A%2520Reinterpreting%2520Confidence%2520Intervals%2520in%250A%2520%2520Few-Shot%2520Learning%26entry.906535625%3DRaphael%2520Lafargue%2520and%2520Luke%2520Smith%2520and%2520Franck%2520Vermet%2520and%2520Mathias%2520L%25C3%25B6we%2520and%2520Ian%2520Reid%2520and%2520Vincent%2520Gripon%2520and%2520Jack%2520Valmadre%26entry.1292438233%3D%2520%2520The%2520predominant%2520method%2520for%2520computing%2520confidence%2520intervals%2520%2528CI%2529%2520in%2520few-shot%250Alearning%2520%2528FSL%2529%2520is%2520based%2520on%2520sampling%2520the%2520tasks%2520with%2520replacement%252C%2520i.e.%255C%2520allowing%250Athe%2520same%2520samples%2520to%2520appear%2520in%2520multiple%2520tasks.%2520This%2520makes%2520the%2520CI%2520misleading%2520in%250Athat%2520it%2520takes%2520into%2520account%2520the%2520randomness%2520of%2520the%2520sampler%2520but%2520not%2520the%2520data%250Aitself.%2520To%2520quantify%2520the%2520extent%2520of%2520this%2520problem%252C%2520we%2520conduct%2520a%2520comparative%250Aanalysis%2520between%2520CIs%2520computed%2520with%2520and%2520without%2520replacement.%2520These%2520reveal%2520a%250Anotable%2520underestimation%2520by%2520the%2520predominant%2520method.%2520This%2520observation%2520calls%2520for%2520a%250Areevaluation%2520of%2520how%2520we%2520interpret%2520confidence%2520intervals%2520and%2520the%2520resulting%250Aconclusions%2520in%2520FSL%2520comparative%2520studies.%2520Our%2520research%2520demonstrates%2520that%2520the%2520use%250Aof%2520paired%2520tests%2520can%2520partially%2520address%2520this%2520issue.%2520Additionally%252C%2520we%2520explore%250Amethods%2520to%2520further%2520reduce%2520the%2520%2528size%2520of%2520the%2529%2520CI%2520by%2520strategically%2520sampling%2520tasks%250Aof%2520a%2520specific%2520size.%2520We%2520also%2520introduce%2520a%2520new%2520optimized%2520benchmark%252C%2520which%2520can%2520be%250Aaccessed%2520at%2520https%253A//github.com/RafLaf/FSL-benchmark-again%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oops%2C%20I%20Sampled%20it%20Again%3A%20Reinterpreting%20Confidence%20Intervals%20in%0A%20%20Few-Shot%20Learning&entry.906535625=Raphael%20Lafargue%20and%20Luke%20Smith%20and%20Franck%20Vermet%20and%20Mathias%20L%C3%B6we%20and%20Ian%20Reid%20and%20Vincent%20Gripon%20and%20Jack%20Valmadre&entry.1292438233=%20%20The%20predominant%20method%20for%20computing%20confidence%20intervals%20%28CI%29%20in%20few-shot%0Alearning%20%28FSL%29%20is%20based%20on%20sampling%20the%20tasks%20with%20replacement%2C%20i.e.%5C%20allowing%0Athe%20same%20samples%20to%20appear%20in%20multiple%20tasks.%20This%20makes%20the%20CI%20misleading%20in%0Athat%20it%20takes%20into%20account%20the%20randomness%20of%20the%20sampler%20but%20not%20the%20data%0Aitself.%20To%20quantify%20the%20extent%20of%20this%20problem%2C%20we%20conduct%20a%20comparative%0Aanalysis%20between%20CIs%20computed%20with%20and%20without%20replacement.%20These%20reveal%20a%0Anotable%20underestimation%20by%20the%20predominant%20method.%20This%20observation%20calls%20for%20a%0Areevaluation%20of%20how%20we%20interpret%20confidence%20intervals%20and%20the%20resulting%0Aconclusions%20in%20FSL%20comparative%20studies.%20Our%20research%20demonstrates%20that%20the%20use%0Aof%20paired%20tests%20can%20partially%20address%20this%20issue.%20Additionally%2C%20we%20explore%0Amethods%20to%20further%20reduce%20the%20%28size%20of%20the%29%20CI%20by%20strategically%20sampling%20tasks%0Aof%20a%20specific%20size.%20We%20also%20introduce%20a%20new%20optimized%20benchmark%2C%20which%20can%20be%0Aaccessed%20at%20https%3A//github.com/RafLaf/FSL-benchmark-again%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02850v1&entry.124074799=Read"},
{"title": "Open Implementation and Study of BEST-RQ for Speech Processing", "author": "Ryan Whetten and Titouan Parcollet and Marco Dinarelli and Yannick Est\u00e8ve", "abstract": "  Self-Supervised Learning (SSL) has proven to be useful in various speech\ntasks. However, these methods are generally very demanding in terms of data,\nmemory, and computational resources. BERT-based Speech pre-Training with\nRandom-projection Quantizer (BEST-RQ), is an SSL method that has shown great\nperformance on Automatic Speech Recognition (ASR) while being simpler than\nother SSL methods, such as wav2vec 2.0. Despite BEST-RQ's great performance,\ndetails are lacking in the original paper, such as the amount of GPU/TPU hours\nused in pre-training, and there is no official easy-to-use open-source\nimplementation. Furthermore, BEST-RQ has not been evaluated on other downstream\ntasks aside from ASR and speech translation. In this work, we describe a\nre-implementation of a Random-projection quantizer and perform a preliminary\nstudy with a comparison to wav2vec 2.0 on four downstream tasks. We discuss the\ndetails and differences of our implementation. We show that a random projection\nquantizer can achieve similar downstream performance as wav2vec 2.0 while\ndecreasing training time by over a factor of two.\n", "link": "http://arxiv.org/abs/2405.04296v2", "date": "2024-09-04", "relevancy": 1.8656, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4875}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4624}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Implementation%20and%20Study%20of%20BEST-RQ%20for%20Speech%20Processing&body=Title%3A%20Open%20Implementation%20and%20Study%20of%20BEST-RQ%20for%20Speech%20Processing%0AAuthor%3A%20Ryan%20Whetten%20and%20Titouan%20Parcollet%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20proven%20to%20be%20useful%20in%20various%20speech%0Atasks.%20However%2C%20these%20methods%20are%20generally%20very%20demanding%20in%20terms%20of%20data%2C%0Amemory%2C%20and%20computational%20resources.%20BERT-based%20Speech%20pre-Training%20with%0ARandom-projection%20Quantizer%20%28BEST-RQ%29%2C%20is%20an%20SSL%20method%20that%20has%20shown%20great%0Aperformance%20on%20Automatic%20Speech%20Recognition%20%28ASR%29%20while%20being%20simpler%20than%0Aother%20SSL%20methods%2C%20such%20as%20wav2vec%202.0.%20Despite%20BEST-RQ%27s%20great%20performance%2C%0Adetails%20are%20lacking%20in%20the%20original%20paper%2C%20such%20as%20the%20amount%20of%20GPU/TPU%20hours%0Aused%20in%20pre-training%2C%20and%20there%20is%20no%20official%20easy-to-use%20open-source%0Aimplementation.%20Furthermore%2C%20BEST-RQ%20has%20not%20been%20evaluated%20on%20other%20downstream%0Atasks%20aside%20from%20ASR%20and%20speech%20translation.%20In%20this%20work%2C%20we%20describe%20a%0Are-implementation%20of%20a%20Random-projection%20quantizer%20and%20perform%20a%20preliminary%0Astudy%20with%20a%20comparison%20to%20wav2vec%202.0%20on%20four%20downstream%20tasks.%20We%20discuss%20the%0Adetails%20and%20differences%20of%20our%20implementation.%20We%20show%20that%20a%20random%20projection%0Aquantizer%20can%20achieve%20similar%20downstream%20performance%20as%20wav2vec%202.0%20while%0Adecreasing%20training%20time%20by%20over%20a%20factor%20of%20two.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04296v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Implementation%2520and%2520Study%2520of%2520BEST-RQ%2520for%2520Speech%2520Processing%26entry.906535625%3DRyan%2520Whetten%2520and%2520Titouan%2520Parcollet%2520and%2520Marco%2520Dinarelli%2520and%2520Yannick%2520Est%25C3%25A8ve%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520proven%2520to%2520be%2520useful%2520in%2520various%2520speech%250Atasks.%2520However%252C%2520these%2520methods%2520are%2520generally%2520very%2520demanding%2520in%2520terms%2520of%2520data%252C%250Amemory%252C%2520and%2520computational%2520resources.%2520BERT-based%2520Speech%2520pre-Training%2520with%250ARandom-projection%2520Quantizer%2520%2528BEST-RQ%2529%252C%2520is%2520an%2520SSL%2520method%2520that%2520has%2520shown%2520great%250Aperformance%2520on%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520while%2520being%2520simpler%2520than%250Aother%2520SSL%2520methods%252C%2520such%2520as%2520wav2vec%25202.0.%2520Despite%2520BEST-RQ%2527s%2520great%2520performance%252C%250Adetails%2520are%2520lacking%2520in%2520the%2520original%2520paper%252C%2520such%2520as%2520the%2520amount%2520of%2520GPU/TPU%2520hours%250Aused%2520in%2520pre-training%252C%2520and%2520there%2520is%2520no%2520official%2520easy-to-use%2520open-source%250Aimplementation.%2520Furthermore%252C%2520BEST-RQ%2520has%2520not%2520been%2520evaluated%2520on%2520other%2520downstream%250Atasks%2520aside%2520from%2520ASR%2520and%2520speech%2520translation.%2520In%2520this%2520work%252C%2520we%2520describe%2520a%250Are-implementation%2520of%2520a%2520Random-projection%2520quantizer%2520and%2520perform%2520a%2520preliminary%250Astudy%2520with%2520a%2520comparison%2520to%2520wav2vec%25202.0%2520on%2520four%2520downstream%2520tasks.%2520We%2520discuss%2520the%250Adetails%2520and%2520differences%2520of%2520our%2520implementation.%2520We%2520show%2520that%2520a%2520random%2520projection%250Aquantizer%2520can%2520achieve%2520similar%2520downstream%2520performance%2520as%2520wav2vec%25202.0%2520while%250Adecreasing%2520training%2520time%2520by%2520over%2520a%2520factor%2520of%2520two.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04296v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Implementation%20and%20Study%20of%20BEST-RQ%20for%20Speech%20Processing&entry.906535625=Ryan%20Whetten%20and%20Titouan%20Parcollet%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20proven%20to%20be%20useful%20in%20various%20speech%0Atasks.%20However%2C%20these%20methods%20are%20generally%20very%20demanding%20in%20terms%20of%20data%2C%0Amemory%2C%20and%20computational%20resources.%20BERT-based%20Speech%20pre-Training%20with%0ARandom-projection%20Quantizer%20%28BEST-RQ%29%2C%20is%20an%20SSL%20method%20that%20has%20shown%20great%0Aperformance%20on%20Automatic%20Speech%20Recognition%20%28ASR%29%20while%20being%20simpler%20than%0Aother%20SSL%20methods%2C%20such%20as%20wav2vec%202.0.%20Despite%20BEST-RQ%27s%20great%20performance%2C%0Adetails%20are%20lacking%20in%20the%20original%20paper%2C%20such%20as%20the%20amount%20of%20GPU/TPU%20hours%0Aused%20in%20pre-training%2C%20and%20there%20is%20no%20official%20easy-to-use%20open-source%0Aimplementation.%20Furthermore%2C%20BEST-RQ%20has%20not%20been%20evaluated%20on%20other%20downstream%0Atasks%20aside%20from%20ASR%20and%20speech%20translation.%20In%20this%20work%2C%20we%20describe%20a%0Are-implementation%20of%20a%20Random-projection%20quantizer%20and%20perform%20a%20preliminary%0Astudy%20with%20a%20comparison%20to%20wav2vec%202.0%20on%20four%20downstream%20tasks.%20We%20discuss%20the%0Adetails%20and%20differences%20of%20our%20implementation.%20We%20show%20that%20a%20random%20projection%0Aquantizer%20can%20achieve%20similar%20downstream%20performance%20as%20wav2vec%202.0%20while%0Adecreasing%20training%20time%20by%20over%20a%20factor%20of%20two.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04296v2&entry.124074799=Read"},
{"title": "A hybrid FEM-PINN method for time-dependent partial differential\n  equations", "author": "Xiaodong Feng and Haojiong Shangguan and Tao Tang and Xiaoliang Wan and Tao Zhou", "abstract": "  In this work, we present a hybrid numerical method for solving evolution\npartial differential equations (PDEs) by merging the time finite element method\nwith deep neural networks. In contrast to the conventional deep learning-based\nformulation where the neural network is defined on a spatiotemporal domain, our\nmethodology utilizes finite element basis functions in the time direction where\nthe space-dependent coefficients are defined as the output of a neural network.\nWe then apply the Galerkin or collocation projection in the time direction to\nobtain a system of PDEs for the space-dependent coefficients which is\napproximated in the framework of PINN. The advantages of such a hybrid\nformulation are twofold: statistical errors are avoided for the integral in the\ntime direction, and the neural network's output can be regarded as a set of\nreduced spatial basis functions. To further alleviate the difficulties from\nhigh dimensionality and low regularity, we have developed an adaptive sampling\nstrategy that refines the training set. More specifically, we use an explicit\ndensity model to approximate the distribution induced by the PDE residual and\nthen augment the training set with new time-dependent random samples given by\nthe learned density model. The effectiveness and efficiency of our proposed\nmethod have been demonstrated through a series of numerical experiments.\n", "link": "http://arxiv.org/abs/2409.02810v1", "date": "2024-09-04", "relevancy": 1.8643, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4933}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4487}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20hybrid%20FEM-PINN%20method%20for%20time-dependent%20partial%20differential%0A%20%20equations&body=Title%3A%20A%20hybrid%20FEM-PINN%20method%20for%20time-dependent%20partial%20differential%0A%20%20equations%0AAuthor%3A%20Xiaodong%20Feng%20and%20Haojiong%20Shangguan%20and%20Tao%20Tang%20and%20Xiaoliang%20Wan%20and%20Tao%20Zhou%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20hybrid%20numerical%20method%20for%20solving%20evolution%0Apartial%20differential%20equations%20%28PDEs%29%20by%20merging%20the%20time%20finite%20element%20method%0Awith%20deep%20neural%20networks.%20In%20contrast%20to%20the%20conventional%20deep%20learning-based%0Aformulation%20where%20the%20neural%20network%20is%20defined%20on%20a%20spatiotemporal%20domain%2C%20our%0Amethodology%20utilizes%20finite%20element%20basis%20functions%20in%20the%20time%20direction%20where%0Athe%20space-dependent%20coefficients%20are%20defined%20as%20the%20output%20of%20a%20neural%20network.%0AWe%20then%20apply%20the%20Galerkin%20or%20collocation%20projection%20in%20the%20time%20direction%20to%0Aobtain%20a%20system%20of%20PDEs%20for%20the%20space-dependent%20coefficients%20which%20is%0Aapproximated%20in%20the%20framework%20of%20PINN.%20The%20advantages%20of%20such%20a%20hybrid%0Aformulation%20are%20twofold%3A%20statistical%20errors%20are%20avoided%20for%20the%20integral%20in%20the%0Atime%20direction%2C%20and%20the%20neural%20network%27s%20output%20can%20be%20regarded%20as%20a%20set%20of%0Areduced%20spatial%20basis%20functions.%20To%20further%20alleviate%20the%20difficulties%20from%0Ahigh%20dimensionality%20and%20low%20regularity%2C%20we%20have%20developed%20an%20adaptive%20sampling%0Astrategy%20that%20refines%20the%20training%20set.%20More%20specifically%2C%20we%20use%20an%20explicit%0Adensity%20model%20to%20approximate%20the%20distribution%20induced%20by%20the%20PDE%20residual%20and%0Athen%20augment%20the%20training%20set%20with%20new%20time-dependent%20random%20samples%20given%20by%0Athe%20learned%20density%20model.%20The%20effectiveness%20and%20efficiency%20of%20our%20proposed%0Amethod%20have%20been%20demonstrated%20through%20a%20series%20of%20numerical%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520hybrid%2520FEM-PINN%2520method%2520for%2520time-dependent%2520partial%2520differential%250A%2520%2520equations%26entry.906535625%3DXiaodong%2520Feng%2520and%2520Haojiong%2520Shangguan%2520and%2520Tao%2520Tang%2520and%2520Xiaoliang%2520Wan%2520and%2520Tao%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520hybrid%2520numerical%2520method%2520for%2520solving%2520evolution%250Apartial%2520differential%2520equations%2520%2528PDEs%2529%2520by%2520merging%2520the%2520time%2520finite%2520element%2520method%250Awith%2520deep%2520neural%2520networks.%2520In%2520contrast%2520to%2520the%2520conventional%2520deep%2520learning-based%250Aformulation%2520where%2520the%2520neural%2520network%2520is%2520defined%2520on%2520a%2520spatiotemporal%2520domain%252C%2520our%250Amethodology%2520utilizes%2520finite%2520element%2520basis%2520functions%2520in%2520the%2520time%2520direction%2520where%250Athe%2520space-dependent%2520coefficients%2520are%2520defined%2520as%2520the%2520output%2520of%2520a%2520neural%2520network.%250AWe%2520then%2520apply%2520the%2520Galerkin%2520or%2520collocation%2520projection%2520in%2520the%2520time%2520direction%2520to%250Aobtain%2520a%2520system%2520of%2520PDEs%2520for%2520the%2520space-dependent%2520coefficients%2520which%2520is%250Aapproximated%2520in%2520the%2520framework%2520of%2520PINN.%2520The%2520advantages%2520of%2520such%2520a%2520hybrid%250Aformulation%2520are%2520twofold%253A%2520statistical%2520errors%2520are%2520avoided%2520for%2520the%2520integral%2520in%2520the%250Atime%2520direction%252C%2520and%2520the%2520neural%2520network%2527s%2520output%2520can%2520be%2520regarded%2520as%2520a%2520set%2520of%250Areduced%2520spatial%2520basis%2520functions.%2520To%2520further%2520alleviate%2520the%2520difficulties%2520from%250Ahigh%2520dimensionality%2520and%2520low%2520regularity%252C%2520we%2520have%2520developed%2520an%2520adaptive%2520sampling%250Astrategy%2520that%2520refines%2520the%2520training%2520set.%2520More%2520specifically%252C%2520we%2520use%2520an%2520explicit%250Adensity%2520model%2520to%2520approximate%2520the%2520distribution%2520induced%2520by%2520the%2520PDE%2520residual%2520and%250Athen%2520augment%2520the%2520training%2520set%2520with%2520new%2520time-dependent%2520random%2520samples%2520given%2520by%250Athe%2520learned%2520density%2520model.%2520The%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520proposed%250Amethod%2520have%2520been%2520demonstrated%2520through%2520a%2520series%2520of%2520numerical%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20hybrid%20FEM-PINN%20method%20for%20time-dependent%20partial%20differential%0A%20%20equations&entry.906535625=Xiaodong%20Feng%20and%20Haojiong%20Shangguan%20and%20Tao%20Tang%20and%20Xiaoliang%20Wan%20and%20Tao%20Zhou&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20hybrid%20numerical%20method%20for%20solving%20evolution%0Apartial%20differential%20equations%20%28PDEs%29%20by%20merging%20the%20time%20finite%20element%20method%0Awith%20deep%20neural%20networks.%20In%20contrast%20to%20the%20conventional%20deep%20learning-based%0Aformulation%20where%20the%20neural%20network%20is%20defined%20on%20a%20spatiotemporal%20domain%2C%20our%0Amethodology%20utilizes%20finite%20element%20basis%20functions%20in%20the%20time%20direction%20where%0Athe%20space-dependent%20coefficients%20are%20defined%20as%20the%20output%20of%20a%20neural%20network.%0AWe%20then%20apply%20the%20Galerkin%20or%20collocation%20projection%20in%20the%20time%20direction%20to%0Aobtain%20a%20system%20of%20PDEs%20for%20the%20space-dependent%20coefficients%20which%20is%0Aapproximated%20in%20the%20framework%20of%20PINN.%20The%20advantages%20of%20such%20a%20hybrid%0Aformulation%20are%20twofold%3A%20statistical%20errors%20are%20avoided%20for%20the%20integral%20in%20the%0Atime%20direction%2C%20and%20the%20neural%20network%27s%20output%20can%20be%20regarded%20as%20a%20set%20of%0Areduced%20spatial%20basis%20functions.%20To%20further%20alleviate%20the%20difficulties%20from%0Ahigh%20dimensionality%20and%20low%20regularity%2C%20we%20have%20developed%20an%20adaptive%20sampling%0Astrategy%20that%20refines%20the%20training%20set.%20More%20specifically%2C%20we%20use%20an%20explicit%0Adensity%20model%20to%20approximate%20the%20distribution%20induced%20by%20the%20PDE%20residual%20and%0Athen%20augment%20the%20training%20set%20with%20new%20time-dependent%20random%20samples%20given%20by%0Athe%20learned%20density%20model.%20The%20effectiveness%20and%20efficiency%20of%20our%20proposed%0Amethod%20have%20been%20demonstrated%20through%20a%20series%20of%20numerical%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02810v1&entry.124074799=Read"},
{"title": "Configurable Foundation Models: Building LLMs from a Modular Perspective", "author": "Chaojun Xiao and Zhengyan Zhang and Chenyang Song and Dazhi Jiang and Feng Yao and Xu Han and Xiaozhi Wang and Shuo Wang and Yufei Huang and Guanyu Lin and Yingfa Chen and Weilin Zhao and Yuge Tu and Zexuan Zhong and Ao Zhang and Chenglei Si and Khai Hao Moo and Chenyang Zhao and Huimin Chen and Yankai Lin and Zhiyuan Liu and Jingbo Shang and Maosong Sun", "abstract": "  Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models.\n", "link": "http://arxiv.org/abs/2409.02877v1", "date": "2024-09-04", "relevancy": 1.8608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Configurable%20Foundation%20Models%3A%20Building%20LLMs%20from%20a%20Modular%20Perspective&body=Title%3A%20Configurable%20Foundation%20Models%3A%20Building%20LLMs%20from%20a%20Modular%20Perspective%0AAuthor%3A%20Chaojun%20Xiao%20and%20Zhengyan%20Zhang%20and%20Chenyang%20Song%20and%20Dazhi%20Jiang%20and%20Feng%20Yao%20and%20Xu%20Han%20and%20Xiaozhi%20Wang%20and%20Shuo%20Wang%20and%20Yufei%20Huang%20and%20Guanyu%20Lin%20and%20Yingfa%20Chen%20and%20Weilin%20Zhao%20and%20Yuge%20Tu%20and%20Zexuan%20Zhong%20and%20Ao%20Zhang%20and%20Chenglei%20Si%20and%20Khai%20Hao%20Moo%20and%20Chenyang%20Zhao%20and%20Huimin%20Chen%20and%20Yankai%20Lin%20and%20Zhiyuan%20Liu%20and%20Jingbo%20Shang%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Advancements%20in%20LLMs%20have%20recently%20unveiled%20challenges%20tied%20to%20computational%0Aefficiency%20and%20continual%20scalability%20due%20to%20their%20requirements%20of%20huge%0Aparameters%2C%20making%20the%20applications%20and%20evolution%20of%20these%20models%20on%20devices%0Awith%20limited%20computation%20resources%20and%20scenarios%20requiring%20various%20abilities%0Aincreasingly%20cumbersome.%20Inspired%20by%20modularity%20within%20the%20human%20brain%2C%20there%0Ais%20a%20growing%20tendency%20to%20decompose%20LLMs%20into%20numerous%20functional%20modules%2C%0Aallowing%20for%20inference%20with%20part%20of%20modules%20and%20dynamic%20assembly%20of%20modules%20to%0Atackle%20complex%20tasks%2C%20such%20as%20mixture-of-experts.%20To%20highlight%20the%20inherent%0Aefficiency%20and%20composability%20of%20the%20modular%20approach%2C%20we%20coin%20the%20term%20brick%20to%0Arepresent%20each%20functional%20module%2C%20designating%20the%20modularized%20structure%20as%0Aconfigurable%20foundation%20models.%20In%20this%20paper%2C%20we%20offer%20a%20comprehensive%0Aoverview%20and%20investigation%20of%20the%20construction%2C%20utilization%2C%20and%20limitation%20of%0Aconfigurable%20foundation%20models.%20We%20first%20formalize%20modules%20into%20emergent%20bricks%0A-%20functional%20neuron%20partitions%20that%20emerge%20during%20the%20pre-training%20phase%2C%20and%0Acustomized%20bricks%20-%20bricks%20constructed%20via%20additional%20post-training%20to%20improve%0Athe%20capabilities%20and%20knowledge%20of%20LLMs.%20Based%20on%20diverse%20functional%20bricks%2C%20we%0Afurther%20present%20four%20brick-oriented%20operations%3A%20retrieval%20and%20routing%2C%20merging%2C%0Aupdating%2C%20and%20growing.%20These%20operations%20allow%20for%20dynamic%20configuration%20of%20LLMs%0Abased%20on%20instructions%20to%20handle%20complex%20tasks.%20To%20verify%20our%20perspective%2C%20we%0Aconduct%20an%20empirical%20analysis%20on%20widely-used%20LLMs.%20We%20find%20that%20the%20FFN%20layers%0Afollow%20modular%20patterns%20with%20functional%20specialization%20of%20neurons%20and%0Afunctional%20neuron%20partitions.%20Finally%2C%20we%20highlight%20several%20open%20issues%20and%0Adirections%20for%20future%20research.%20Overall%2C%20this%20paper%20aims%20to%20offer%20a%20fresh%0Amodular%20perspective%20on%20existing%20LLM%20research%20and%20inspire%20the%20future%20creation%20of%0Amore%20efficient%20and%20scalable%20foundational%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfigurable%2520Foundation%2520Models%253A%2520Building%2520LLMs%2520from%2520a%2520Modular%2520Perspective%26entry.906535625%3DChaojun%2520Xiao%2520and%2520Zhengyan%2520Zhang%2520and%2520Chenyang%2520Song%2520and%2520Dazhi%2520Jiang%2520and%2520Feng%2520Yao%2520and%2520Xu%2520Han%2520and%2520Xiaozhi%2520Wang%2520and%2520Shuo%2520Wang%2520and%2520Yufei%2520Huang%2520and%2520Guanyu%2520Lin%2520and%2520Yingfa%2520Chen%2520and%2520Weilin%2520Zhao%2520and%2520Yuge%2520Tu%2520and%2520Zexuan%2520Zhong%2520and%2520Ao%2520Zhang%2520and%2520Chenglei%2520Si%2520and%2520Khai%2520Hao%2520Moo%2520and%2520Chenyang%2520Zhao%2520and%2520Huimin%2520Chen%2520and%2520Yankai%2520Lin%2520and%2520Zhiyuan%2520Liu%2520and%2520Jingbo%2520Shang%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Advancements%2520in%2520LLMs%2520have%2520recently%2520unveiled%2520challenges%2520tied%2520to%2520computational%250Aefficiency%2520and%2520continual%2520scalability%2520due%2520to%2520their%2520requirements%2520of%2520huge%250Aparameters%252C%2520making%2520the%2520applications%2520and%2520evolution%2520of%2520these%2520models%2520on%2520devices%250Awith%2520limited%2520computation%2520resources%2520and%2520scenarios%2520requiring%2520various%2520abilities%250Aincreasingly%2520cumbersome.%2520Inspired%2520by%2520modularity%2520within%2520the%2520human%2520brain%252C%2520there%250Ais%2520a%2520growing%2520tendency%2520to%2520decompose%2520LLMs%2520into%2520numerous%2520functional%2520modules%252C%250Aallowing%2520for%2520inference%2520with%2520part%2520of%2520modules%2520and%2520dynamic%2520assembly%2520of%2520modules%2520to%250Atackle%2520complex%2520tasks%252C%2520such%2520as%2520mixture-of-experts.%2520To%2520highlight%2520the%2520inherent%250Aefficiency%2520and%2520composability%2520of%2520the%2520modular%2520approach%252C%2520we%2520coin%2520the%2520term%2520brick%2520to%250Arepresent%2520each%2520functional%2520module%252C%2520designating%2520the%2520modularized%2520structure%2520as%250Aconfigurable%2520foundation%2520models.%2520In%2520this%2520paper%252C%2520we%2520offer%2520a%2520comprehensive%250Aoverview%2520and%2520investigation%2520of%2520the%2520construction%252C%2520utilization%252C%2520and%2520limitation%2520of%250Aconfigurable%2520foundation%2520models.%2520We%2520first%2520formalize%2520modules%2520into%2520emergent%2520bricks%250A-%2520functional%2520neuron%2520partitions%2520that%2520emerge%2520during%2520the%2520pre-training%2520phase%252C%2520and%250Acustomized%2520bricks%2520-%2520bricks%2520constructed%2520via%2520additional%2520post-training%2520to%2520improve%250Athe%2520capabilities%2520and%2520knowledge%2520of%2520LLMs.%2520Based%2520on%2520diverse%2520functional%2520bricks%252C%2520we%250Afurther%2520present%2520four%2520brick-oriented%2520operations%253A%2520retrieval%2520and%2520routing%252C%2520merging%252C%250Aupdating%252C%2520and%2520growing.%2520These%2520operations%2520allow%2520for%2520dynamic%2520configuration%2520of%2520LLMs%250Abased%2520on%2520instructions%2520to%2520handle%2520complex%2520tasks.%2520To%2520verify%2520our%2520perspective%252C%2520we%250Aconduct%2520an%2520empirical%2520analysis%2520on%2520widely-used%2520LLMs.%2520We%2520find%2520that%2520the%2520FFN%2520layers%250Afollow%2520modular%2520patterns%2520with%2520functional%2520specialization%2520of%2520neurons%2520and%250Afunctional%2520neuron%2520partitions.%2520Finally%252C%2520we%2520highlight%2520several%2520open%2520issues%2520and%250Adirections%2520for%2520future%2520research.%2520Overall%252C%2520this%2520paper%2520aims%2520to%2520offer%2520a%2520fresh%250Amodular%2520perspective%2520on%2520existing%2520LLM%2520research%2520and%2520inspire%2520the%2520future%2520creation%2520of%250Amore%2520efficient%2520and%2520scalable%2520foundational%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Configurable%20Foundation%20Models%3A%20Building%20LLMs%20from%20a%20Modular%20Perspective&entry.906535625=Chaojun%20Xiao%20and%20Zhengyan%20Zhang%20and%20Chenyang%20Song%20and%20Dazhi%20Jiang%20and%20Feng%20Yao%20and%20Xu%20Han%20and%20Xiaozhi%20Wang%20and%20Shuo%20Wang%20and%20Yufei%20Huang%20and%20Guanyu%20Lin%20and%20Yingfa%20Chen%20and%20Weilin%20Zhao%20and%20Yuge%20Tu%20and%20Zexuan%20Zhong%20and%20Ao%20Zhang%20and%20Chenglei%20Si%20and%20Khai%20Hao%20Moo%20and%20Chenyang%20Zhao%20and%20Huimin%20Chen%20and%20Yankai%20Lin%20and%20Zhiyuan%20Liu%20and%20Jingbo%20Shang%20and%20Maosong%20Sun&entry.1292438233=%20%20Advancements%20in%20LLMs%20have%20recently%20unveiled%20challenges%20tied%20to%20computational%0Aefficiency%20and%20continual%20scalability%20due%20to%20their%20requirements%20of%20huge%0Aparameters%2C%20making%20the%20applications%20and%20evolution%20of%20these%20models%20on%20devices%0Awith%20limited%20computation%20resources%20and%20scenarios%20requiring%20various%20abilities%0Aincreasingly%20cumbersome.%20Inspired%20by%20modularity%20within%20the%20human%20brain%2C%20there%0Ais%20a%20growing%20tendency%20to%20decompose%20LLMs%20into%20numerous%20functional%20modules%2C%0Aallowing%20for%20inference%20with%20part%20of%20modules%20and%20dynamic%20assembly%20of%20modules%20to%0Atackle%20complex%20tasks%2C%20such%20as%20mixture-of-experts.%20To%20highlight%20the%20inherent%0Aefficiency%20and%20composability%20of%20the%20modular%20approach%2C%20we%20coin%20the%20term%20brick%20to%0Arepresent%20each%20functional%20module%2C%20designating%20the%20modularized%20structure%20as%0Aconfigurable%20foundation%20models.%20In%20this%20paper%2C%20we%20offer%20a%20comprehensive%0Aoverview%20and%20investigation%20of%20the%20construction%2C%20utilization%2C%20and%20limitation%20of%0Aconfigurable%20foundation%20models.%20We%20first%20formalize%20modules%20into%20emergent%20bricks%0A-%20functional%20neuron%20partitions%20that%20emerge%20during%20the%20pre-training%20phase%2C%20and%0Acustomized%20bricks%20-%20bricks%20constructed%20via%20additional%20post-training%20to%20improve%0Athe%20capabilities%20and%20knowledge%20of%20LLMs.%20Based%20on%20diverse%20functional%20bricks%2C%20we%0Afurther%20present%20four%20brick-oriented%20operations%3A%20retrieval%20and%20routing%2C%20merging%2C%0Aupdating%2C%20and%20growing.%20These%20operations%20allow%20for%20dynamic%20configuration%20of%20LLMs%0Abased%20on%20instructions%20to%20handle%20complex%20tasks.%20To%20verify%20our%20perspective%2C%20we%0Aconduct%20an%20empirical%20analysis%20on%20widely-used%20LLMs.%20We%20find%20that%20the%20FFN%20layers%0Afollow%20modular%20patterns%20with%20functional%20specialization%20of%20neurons%20and%0Afunctional%20neuron%20partitions.%20Finally%2C%20we%20highlight%20several%20open%20issues%20and%0Adirections%20for%20future%20research.%20Overall%2C%20this%20paper%20aims%20to%20offer%20a%20fresh%0Amodular%20perspective%20on%20existing%20LLM%20research%20and%20inspire%20the%20future%20creation%20of%0Amore%20efficient%20and%20scalable%20foundational%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02877v1&entry.124074799=Read"},
{"title": "Unifying Causal Representation Learning with the Invariance Principle", "author": "Dingling Yao and Dario Rancati and Riccardo Cadei and Marco Fumero and Francesco Locatello", "abstract": "  Causal representation learning aims at recovering latent causal variables\nfrom high-dimensional observations to solve causal downstream tasks, such as\npredicting the effect of new interventions or more robust classification. A\nplethora of methods have been developed, each tackling carefully crafted\nproblem settings that lead to different types of identifiability. The folklore\nis that these different settings are important, as they are often linked to\ndifferent rungs of Pearl's causal hierarchy, although not all neatly fit. Our\nmain contribution is to show that many existing causal representation learning\napproaches methodologically align the representation to known data symmetries.\nIdentification of the variables is guided by equivalence classes across\ndifferent data pockets that are not necessarily causal. This result suggests\nimportant implications, allowing us to unify many existing approaches in a\nsingle method that can mix and match different assumptions, including\nnon-causal ones, based on the invariances relevant to our application. It also\nsignificantly benefits applicability, which we demonstrate by improving\ntreatment effect estimation on real-world high-dimensional ecological data.\nOverall, this paper clarifies the role of causality assumptions in the\ndiscovery of causal variables and shifts the focus to preserving data\nsymmetries.\n", "link": "http://arxiv.org/abs/2409.02772v1", "date": "2024-09-04", "relevancy": 1.8557, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4644}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4636}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Causal%20Representation%20Learning%20with%20the%20Invariance%20Principle&body=Title%3A%20Unifying%20Causal%20Representation%20Learning%20with%20the%20Invariance%20Principle%0AAuthor%3A%20Dingling%20Yao%20and%20Dario%20Rancati%20and%20Riccardo%20Cadei%20and%20Marco%20Fumero%20and%20Francesco%20Locatello%0AAbstract%3A%20%20%20Causal%20representation%20learning%20aims%20at%20recovering%20latent%20causal%20variables%0Afrom%20high-dimensional%20observations%20to%20solve%20causal%20downstream%20tasks%2C%20such%20as%0Apredicting%20the%20effect%20of%20new%20interventions%20or%20more%20robust%20classification.%20A%0Aplethora%20of%20methods%20have%20been%20developed%2C%20each%20tackling%20carefully%20crafted%0Aproblem%20settings%20that%20lead%20to%20different%20types%20of%20identifiability.%20The%20folklore%0Ais%20that%20these%20different%20settings%20are%20important%2C%20as%20they%20are%20often%20linked%20to%0Adifferent%20rungs%20of%20Pearl%27s%20causal%20hierarchy%2C%20although%20not%20all%20neatly%20fit.%20Our%0Amain%20contribution%20is%20to%20show%20that%20many%20existing%20causal%20representation%20learning%0Aapproaches%20methodologically%20align%20the%20representation%20to%20known%20data%20symmetries.%0AIdentification%20of%20the%20variables%20is%20guided%20by%20equivalence%20classes%20across%0Adifferent%20data%20pockets%20that%20are%20not%20necessarily%20causal.%20This%20result%20suggests%0Aimportant%20implications%2C%20allowing%20us%20to%20unify%20many%20existing%20approaches%20in%20a%0Asingle%20method%20that%20can%20mix%20and%20match%20different%20assumptions%2C%20including%0Anon-causal%20ones%2C%20based%20on%20the%20invariances%20relevant%20to%20our%20application.%20It%20also%0Asignificantly%20benefits%20applicability%2C%20which%20we%20demonstrate%20by%20improving%0Atreatment%20effect%20estimation%20on%20real-world%20high-dimensional%20ecological%20data.%0AOverall%2C%20this%20paper%20clarifies%20the%20role%20of%20causality%20assumptions%20in%20the%0Adiscovery%20of%20causal%20variables%20and%20shifts%20the%20focus%20to%20preserving%20data%0Asymmetries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Causal%2520Representation%2520Learning%2520with%2520the%2520Invariance%2520Principle%26entry.906535625%3DDingling%2520Yao%2520and%2520Dario%2520Rancati%2520and%2520Riccardo%2520Cadei%2520and%2520Marco%2520Fumero%2520and%2520Francesco%2520Locatello%26entry.1292438233%3D%2520%2520Causal%2520representation%2520learning%2520aims%2520at%2520recovering%2520latent%2520causal%2520variables%250Afrom%2520high-dimensional%2520observations%2520to%2520solve%2520causal%2520downstream%2520tasks%252C%2520such%2520as%250Apredicting%2520the%2520effect%2520of%2520new%2520interventions%2520or%2520more%2520robust%2520classification.%2520A%250Aplethora%2520of%2520methods%2520have%2520been%2520developed%252C%2520each%2520tackling%2520carefully%2520crafted%250Aproblem%2520settings%2520that%2520lead%2520to%2520different%2520types%2520of%2520identifiability.%2520The%2520folklore%250Ais%2520that%2520these%2520different%2520settings%2520are%2520important%252C%2520as%2520they%2520are%2520often%2520linked%2520to%250Adifferent%2520rungs%2520of%2520Pearl%2527s%2520causal%2520hierarchy%252C%2520although%2520not%2520all%2520neatly%2520fit.%2520Our%250Amain%2520contribution%2520is%2520to%2520show%2520that%2520many%2520existing%2520causal%2520representation%2520learning%250Aapproaches%2520methodologically%2520align%2520the%2520representation%2520to%2520known%2520data%2520symmetries.%250AIdentification%2520of%2520the%2520variables%2520is%2520guided%2520by%2520equivalence%2520classes%2520across%250Adifferent%2520data%2520pockets%2520that%2520are%2520not%2520necessarily%2520causal.%2520This%2520result%2520suggests%250Aimportant%2520implications%252C%2520allowing%2520us%2520to%2520unify%2520many%2520existing%2520approaches%2520in%2520a%250Asingle%2520method%2520that%2520can%2520mix%2520and%2520match%2520different%2520assumptions%252C%2520including%250Anon-causal%2520ones%252C%2520based%2520on%2520the%2520invariances%2520relevant%2520to%2520our%2520application.%2520It%2520also%250Asignificantly%2520benefits%2520applicability%252C%2520which%2520we%2520demonstrate%2520by%2520improving%250Atreatment%2520effect%2520estimation%2520on%2520real-world%2520high-dimensional%2520ecological%2520data.%250AOverall%252C%2520this%2520paper%2520clarifies%2520the%2520role%2520of%2520causality%2520assumptions%2520in%2520the%250Adiscovery%2520of%2520causal%2520variables%2520and%2520shifts%2520the%2520focus%2520to%2520preserving%2520data%250Asymmetries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Causal%20Representation%20Learning%20with%20the%20Invariance%20Principle&entry.906535625=Dingling%20Yao%20and%20Dario%20Rancati%20and%20Riccardo%20Cadei%20and%20Marco%20Fumero%20and%20Francesco%20Locatello&entry.1292438233=%20%20Causal%20representation%20learning%20aims%20at%20recovering%20latent%20causal%20variables%0Afrom%20high-dimensional%20observations%20to%20solve%20causal%20downstream%20tasks%2C%20such%20as%0Apredicting%20the%20effect%20of%20new%20interventions%20or%20more%20robust%20classification.%20A%0Aplethora%20of%20methods%20have%20been%20developed%2C%20each%20tackling%20carefully%20crafted%0Aproblem%20settings%20that%20lead%20to%20different%20types%20of%20identifiability.%20The%20folklore%0Ais%20that%20these%20different%20settings%20are%20important%2C%20as%20they%20are%20often%20linked%20to%0Adifferent%20rungs%20of%20Pearl%27s%20causal%20hierarchy%2C%20although%20not%20all%20neatly%20fit.%20Our%0Amain%20contribution%20is%20to%20show%20that%20many%20existing%20causal%20representation%20learning%0Aapproaches%20methodologically%20align%20the%20representation%20to%20known%20data%20symmetries.%0AIdentification%20of%20the%20variables%20is%20guided%20by%20equivalence%20classes%20across%0Adifferent%20data%20pockets%20that%20are%20not%20necessarily%20causal.%20This%20result%20suggests%0Aimportant%20implications%2C%20allowing%20us%20to%20unify%20many%20existing%20approaches%20in%20a%0Asingle%20method%20that%20can%20mix%20and%20match%20different%20assumptions%2C%20including%0Anon-causal%20ones%2C%20based%20on%20the%20invariances%20relevant%20to%20our%20application.%20It%20also%0Asignificantly%20benefits%20applicability%2C%20which%20we%20demonstrate%20by%20improving%0Atreatment%20effect%20estimation%20on%20real-world%20high-dimensional%20ecological%20data.%0AOverall%2C%20this%20paper%20clarifies%20the%20role%20of%20causality%20assumptions%20in%20the%0Adiscovery%20of%20causal%20variables%20and%20shifts%20the%20focus%20to%20preserving%20data%0Asymmetries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02772v1&entry.124074799=Read"},
{"title": "The Future of Open Human Feedback", "author": "Shachar Don-Yehiya and Ben Burtenshaw and Ramon Fernandez Astudillo and Cailean Osborne and Mimansa Jaiswal and Tzu-Sheng Kuo and Wenting Zhao and Idan Shenfeld and Andi Peng and Mikhail Yurochkin and Atoosa Kasirzadeh and Yangsibo Huang and Tatsunori Hashimoto and Yacine Jernite and Daniel Vila-Suero and Omri Abend and Jennifer Ding and Sara Hooker and Hannah Rose Kirk and Leshem Choshen", "abstract": "  Human feedback on conversations with language language models (LLMs) is\ncentral to how these systems learn about the world, improve their capabilities,\nand are steered toward desirable and safe behaviors. However, this feedback is\nmostly collected by frontier AI labs and kept behind closed doors. In this\nwork, we bring together interdisciplinary experts to assess the opportunities\nand challenges to realizing an open ecosystem of human feedback for AI. We\nfirst look for successful practices in peer production, open source, and\ncitizen science communities. We then characterize the main challenges for open\nhuman feedback. For each, we survey current approaches and offer\nrecommendations. We end by envisioning the components needed to underpin a\nsustainable and open human feedback ecosystem. In the center of this ecosystem\nare mutually beneficial feedback loops, between users and specialized models,\nincentivizing a diverse stakeholders community of model trainers and feedback\nproviders to support a general open feedback pool.\n", "link": "http://arxiv.org/abs/2408.16961v2", "date": "2024-09-04", "relevancy": 1.8437, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4605}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Future%20of%20Open%20Human%20Feedback&body=Title%3A%20The%20Future%20of%20Open%20Human%20Feedback%0AAuthor%3A%20Shachar%20Don-Yehiya%20and%20Ben%20Burtenshaw%20and%20Ramon%20Fernandez%20Astudillo%20and%20Cailean%20Osborne%20and%20Mimansa%20Jaiswal%20and%20Tzu-Sheng%20Kuo%20and%20Wenting%20Zhao%20and%20Idan%20Shenfeld%20and%20Andi%20Peng%20and%20Mikhail%20Yurochkin%20and%20Atoosa%20Kasirzadeh%20and%20Yangsibo%20Huang%20and%20Tatsunori%20Hashimoto%20and%20Yacine%20Jernite%20and%20Daniel%20Vila-Suero%20and%20Omri%20Abend%20and%20Jennifer%20Ding%20and%20Sara%20Hooker%20and%20Hannah%20Rose%20Kirk%20and%20Leshem%20Choshen%0AAbstract%3A%20%20%20Human%20feedback%20on%20conversations%20with%20language%20language%20models%20%28LLMs%29%20is%0Acentral%20to%20how%20these%20systems%20learn%20about%20the%20world%2C%20improve%20their%20capabilities%2C%0Aand%20are%20steered%20toward%20desirable%20and%20safe%20behaviors.%20However%2C%20this%20feedback%20is%0Amostly%20collected%20by%20frontier%20AI%20labs%20and%20kept%20behind%20closed%20doors.%20In%20this%0Awork%2C%20we%20bring%20together%20interdisciplinary%20experts%20to%20assess%20the%20opportunities%0Aand%20challenges%20to%20realizing%20an%20open%20ecosystem%20of%20human%20feedback%20for%20AI.%20We%0Afirst%20look%20for%20successful%20practices%20in%20peer%20production%2C%20open%20source%2C%20and%0Acitizen%20science%20communities.%20We%20then%20characterize%20the%20main%20challenges%20for%20open%0Ahuman%20feedback.%20For%20each%2C%20we%20survey%20current%20approaches%20and%20offer%0Arecommendations.%20We%20end%20by%20envisioning%20the%20components%20needed%20to%20underpin%20a%0Asustainable%20and%20open%20human%20feedback%20ecosystem.%20In%20the%20center%20of%20this%20ecosystem%0Aare%20mutually%20beneficial%20feedback%20loops%2C%20between%20users%20and%20specialized%20models%2C%0Aincentivizing%20a%20diverse%20stakeholders%20community%20of%20model%20trainers%20and%20feedback%0Aproviders%20to%20support%20a%20general%20open%20feedback%20pool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Future%2520of%2520Open%2520Human%2520Feedback%26entry.906535625%3DShachar%2520Don-Yehiya%2520and%2520Ben%2520Burtenshaw%2520and%2520Ramon%2520Fernandez%2520Astudillo%2520and%2520Cailean%2520Osborne%2520and%2520Mimansa%2520Jaiswal%2520and%2520Tzu-Sheng%2520Kuo%2520and%2520Wenting%2520Zhao%2520and%2520Idan%2520Shenfeld%2520and%2520Andi%2520Peng%2520and%2520Mikhail%2520Yurochkin%2520and%2520Atoosa%2520Kasirzadeh%2520and%2520Yangsibo%2520Huang%2520and%2520Tatsunori%2520Hashimoto%2520and%2520Yacine%2520Jernite%2520and%2520Daniel%2520Vila-Suero%2520and%2520Omri%2520Abend%2520and%2520Jennifer%2520Ding%2520and%2520Sara%2520Hooker%2520and%2520Hannah%2520Rose%2520Kirk%2520and%2520Leshem%2520Choshen%26entry.1292438233%3D%2520%2520Human%2520feedback%2520on%2520conversations%2520with%2520language%2520language%2520models%2520%2528LLMs%2529%2520is%250Acentral%2520to%2520how%2520these%2520systems%2520learn%2520about%2520the%2520world%252C%2520improve%2520their%2520capabilities%252C%250Aand%2520are%2520steered%2520toward%2520desirable%2520and%2520safe%2520behaviors.%2520However%252C%2520this%2520feedback%2520is%250Amostly%2520collected%2520by%2520frontier%2520AI%2520labs%2520and%2520kept%2520behind%2520closed%2520doors.%2520In%2520this%250Awork%252C%2520we%2520bring%2520together%2520interdisciplinary%2520experts%2520to%2520assess%2520the%2520opportunities%250Aand%2520challenges%2520to%2520realizing%2520an%2520open%2520ecosystem%2520of%2520human%2520feedback%2520for%2520AI.%2520We%250Afirst%2520look%2520for%2520successful%2520practices%2520in%2520peer%2520production%252C%2520open%2520source%252C%2520and%250Acitizen%2520science%2520communities.%2520We%2520then%2520characterize%2520the%2520main%2520challenges%2520for%2520open%250Ahuman%2520feedback.%2520For%2520each%252C%2520we%2520survey%2520current%2520approaches%2520and%2520offer%250Arecommendations.%2520We%2520end%2520by%2520envisioning%2520the%2520components%2520needed%2520to%2520underpin%2520a%250Asustainable%2520and%2520open%2520human%2520feedback%2520ecosystem.%2520In%2520the%2520center%2520of%2520this%2520ecosystem%250Aare%2520mutually%2520beneficial%2520feedback%2520loops%252C%2520between%2520users%2520and%2520specialized%2520models%252C%250Aincentivizing%2520a%2520diverse%2520stakeholders%2520community%2520of%2520model%2520trainers%2520and%2520feedback%250Aproviders%2520to%2520support%2520a%2520general%2520open%2520feedback%2520pool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Future%20of%20Open%20Human%20Feedback&entry.906535625=Shachar%20Don-Yehiya%20and%20Ben%20Burtenshaw%20and%20Ramon%20Fernandez%20Astudillo%20and%20Cailean%20Osborne%20and%20Mimansa%20Jaiswal%20and%20Tzu-Sheng%20Kuo%20and%20Wenting%20Zhao%20and%20Idan%20Shenfeld%20and%20Andi%20Peng%20and%20Mikhail%20Yurochkin%20and%20Atoosa%20Kasirzadeh%20and%20Yangsibo%20Huang%20and%20Tatsunori%20Hashimoto%20and%20Yacine%20Jernite%20and%20Daniel%20Vila-Suero%20and%20Omri%20Abend%20and%20Jennifer%20Ding%20and%20Sara%20Hooker%20and%20Hannah%20Rose%20Kirk%20and%20Leshem%20Choshen&entry.1292438233=%20%20Human%20feedback%20on%20conversations%20with%20language%20language%20models%20%28LLMs%29%20is%0Acentral%20to%20how%20these%20systems%20learn%20about%20the%20world%2C%20improve%20their%20capabilities%2C%0Aand%20are%20steered%20toward%20desirable%20and%20safe%20behaviors.%20However%2C%20this%20feedback%20is%0Amostly%20collected%20by%20frontier%20AI%20labs%20and%20kept%20behind%20closed%20doors.%20In%20this%0Awork%2C%20we%20bring%20together%20interdisciplinary%20experts%20to%20assess%20the%20opportunities%0Aand%20challenges%20to%20realizing%20an%20open%20ecosystem%20of%20human%20feedback%20for%20AI.%20We%0Afirst%20look%20for%20successful%20practices%20in%20peer%20production%2C%20open%20source%2C%20and%0Acitizen%20science%20communities.%20We%20then%20characterize%20the%20main%20challenges%20for%20open%0Ahuman%20feedback.%20For%20each%2C%20we%20survey%20current%20approaches%20and%20offer%0Arecommendations.%20We%20end%20by%20envisioning%20the%20components%20needed%20to%20underpin%20a%0Asustainable%20and%20open%20human%20feedback%20ecosystem.%20In%20the%20center%20of%20this%20ecosystem%0Aare%20mutually%20beneficial%20feedback%20loops%2C%20between%20users%20and%20specialized%20models%2C%0Aincentivizing%20a%20diverse%20stakeholders%20community%20of%20model%20trainers%20and%20feedback%0Aproviders%20to%20support%20a%20general%20open%20feedback%20pool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16961v2&entry.124074799=Read"},
{"title": "OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for\n  Effective Adversarial Text Generation", "author": "W\u0142odzimierz Lewoniewski and Piotr Stolarski and Milena Str\u00f3\u017cyna and Elzbieta Lewa\u0144ska and Aleksandra Wojewoda and Ewelina Ksi\u0119\u017cniak and Marcin Sawi\u0144ski", "abstract": "  This paper presents the experiments and results for the CheckThat! Lab at\nCLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial\nExamples (InCrediblAE). The primary objective of this task was to generate\nadversarial examples in five problem domains in order to evaluate the\nrobustness of widely used text classification methods (fine-tuned BERT, BiLSTM,\nand RoBERTa) when applied to credibility assessment issues.\n  This study explores the application of ensemble learning to enhance\nadversarial attacks on natural language processing (NLP) models. We\nsystematically tested and refined several adversarial attack methods, including\nBERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across\nvarious misinformation tasks. By developing modified versions of BERT-Attack\nand hybrid methods, we achieved significant improvements in attack\neffectiveness. Our results demonstrate the potential of modification and\ncombining multiple methods to create more sophisticated and effective\nadversarial attack strategies, contributing to the development of more robust\nand secure systems.\n", "link": "http://arxiv.org/abs/2409.02649v1", "date": "2024-09-04", "relevancy": 1.8383, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.477}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4505}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenFact%20at%20CheckThat%21%202024%3A%20Combining%20Multiple%20Attack%20Methods%20for%0A%20%20Effective%20Adversarial%20Text%20Generation&body=Title%3A%20OpenFact%20at%20CheckThat%21%202024%3A%20Combining%20Multiple%20Attack%20Methods%20for%0A%20%20Effective%20Adversarial%20Text%20Generation%0AAuthor%3A%20W%C5%82odzimierz%20Lewoniewski%20and%20Piotr%20Stolarski%20and%20Milena%20Str%C3%B3%C5%BCyna%20and%20Elzbieta%20Lewa%C5%84ska%20and%20Aleksandra%20Wojewoda%20and%20Ewelina%20Ksi%C4%99%C5%BCniak%20and%20Marcin%20Sawi%C5%84ski%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20experiments%20and%20results%20for%20the%20CheckThat%21%20Lab%20at%0ACLEF%202024%20Task%206%3A%20Robustness%20of%20Credibility%20Assessment%20with%20Adversarial%0AExamples%20%28InCrediblAE%29.%20The%20primary%20objective%20of%20this%20task%20was%20to%20generate%0Aadversarial%20examples%20in%20five%20problem%20domains%20in%20order%20to%20evaluate%20the%0Arobustness%20of%20widely%20used%20text%20classification%20methods%20%28fine-tuned%20BERT%2C%20BiLSTM%2C%0Aand%20RoBERTa%29%20when%20applied%20to%20credibility%20assessment%20issues.%0A%20%20This%20study%20explores%20the%20application%20of%20ensemble%20learning%20to%20enhance%0Aadversarial%20attacks%20on%20natural%20language%20processing%20%28NLP%29%20models.%20We%0Asystematically%20tested%20and%20refined%20several%20adversarial%20attack%20methods%2C%20including%0ABERT-Attack%2C%20Genetic%20algorithms%2C%20TextFooler%2C%20and%20CLARE%2C%20on%20five%20datasets%20across%0Avarious%20misinformation%20tasks.%20By%20developing%20modified%20versions%20of%20BERT-Attack%0Aand%20hybrid%20methods%2C%20we%20achieved%20significant%20improvements%20in%20attack%0Aeffectiveness.%20Our%20results%20demonstrate%20the%20potential%20of%20modification%20and%0Acombining%20multiple%20methods%20to%20create%20more%20sophisticated%20and%20effective%0Aadversarial%20attack%20strategies%2C%20contributing%20to%20the%20development%20of%20more%20robust%0Aand%20secure%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenFact%2520at%2520CheckThat%2521%25202024%253A%2520Combining%2520Multiple%2520Attack%2520Methods%2520for%250A%2520%2520Effective%2520Adversarial%2520Text%2520Generation%26entry.906535625%3DW%25C5%2582odzimierz%2520Lewoniewski%2520and%2520Piotr%2520Stolarski%2520and%2520Milena%2520Str%25C3%25B3%25C5%25BCyna%2520and%2520Elzbieta%2520Lewa%25C5%2584ska%2520and%2520Aleksandra%2520Wojewoda%2520and%2520Ewelina%2520Ksi%25C4%2599%25C5%25BCniak%2520and%2520Marcin%2520Sawi%25C5%2584ski%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520experiments%2520and%2520results%2520for%2520the%2520CheckThat%2521%2520Lab%2520at%250ACLEF%25202024%2520Task%25206%253A%2520Robustness%2520of%2520Credibility%2520Assessment%2520with%2520Adversarial%250AExamples%2520%2528InCrediblAE%2529.%2520The%2520primary%2520objective%2520of%2520this%2520task%2520was%2520to%2520generate%250Aadversarial%2520examples%2520in%2520five%2520problem%2520domains%2520in%2520order%2520to%2520evaluate%2520the%250Arobustness%2520of%2520widely%2520used%2520text%2520classification%2520methods%2520%2528fine-tuned%2520BERT%252C%2520BiLSTM%252C%250Aand%2520RoBERTa%2529%2520when%2520applied%2520to%2520credibility%2520assessment%2520issues.%250A%2520%2520This%2520study%2520explores%2520the%2520application%2520of%2520ensemble%2520learning%2520to%2520enhance%250Aadversarial%2520attacks%2520on%2520natural%2520language%2520processing%2520%2528NLP%2529%2520models.%2520We%250Asystematically%2520tested%2520and%2520refined%2520several%2520adversarial%2520attack%2520methods%252C%2520including%250ABERT-Attack%252C%2520Genetic%2520algorithms%252C%2520TextFooler%252C%2520and%2520CLARE%252C%2520on%2520five%2520datasets%2520across%250Avarious%2520misinformation%2520tasks.%2520By%2520developing%2520modified%2520versions%2520of%2520BERT-Attack%250Aand%2520hybrid%2520methods%252C%2520we%2520achieved%2520significant%2520improvements%2520in%2520attack%250Aeffectiveness.%2520Our%2520results%2520demonstrate%2520the%2520potential%2520of%2520modification%2520and%250Acombining%2520multiple%2520methods%2520to%2520create%2520more%2520sophisticated%2520and%2520effective%250Aadversarial%2520attack%2520strategies%252C%2520contributing%2520to%2520the%2520development%2520of%2520more%2520robust%250Aand%2520secure%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenFact%20at%20CheckThat%21%202024%3A%20Combining%20Multiple%20Attack%20Methods%20for%0A%20%20Effective%20Adversarial%20Text%20Generation&entry.906535625=W%C5%82odzimierz%20Lewoniewski%20and%20Piotr%20Stolarski%20and%20Milena%20Str%C3%B3%C5%BCyna%20and%20Elzbieta%20Lewa%C5%84ska%20and%20Aleksandra%20Wojewoda%20and%20Ewelina%20Ksi%C4%99%C5%BCniak%20and%20Marcin%20Sawi%C5%84ski&entry.1292438233=%20%20This%20paper%20presents%20the%20experiments%20and%20results%20for%20the%20CheckThat%21%20Lab%20at%0ACLEF%202024%20Task%206%3A%20Robustness%20of%20Credibility%20Assessment%20with%20Adversarial%0AExamples%20%28InCrediblAE%29.%20The%20primary%20objective%20of%20this%20task%20was%20to%20generate%0Aadversarial%20examples%20in%20five%20problem%20domains%20in%20order%20to%20evaluate%20the%0Arobustness%20of%20widely%20used%20text%20classification%20methods%20%28fine-tuned%20BERT%2C%20BiLSTM%2C%0Aand%20RoBERTa%29%20when%20applied%20to%20credibility%20assessment%20issues.%0A%20%20This%20study%20explores%20the%20application%20of%20ensemble%20learning%20to%20enhance%0Aadversarial%20attacks%20on%20natural%20language%20processing%20%28NLP%29%20models.%20We%0Asystematically%20tested%20and%20refined%20several%20adversarial%20attack%20methods%2C%20including%0ABERT-Attack%2C%20Genetic%20algorithms%2C%20TextFooler%2C%20and%20CLARE%2C%20on%20five%20datasets%20across%0Avarious%20misinformation%20tasks.%20By%20developing%20modified%20versions%20of%20BERT-Attack%0Aand%20hybrid%20methods%2C%20we%20achieved%20significant%20improvements%20in%20attack%0Aeffectiveness.%20Our%20results%20demonstrate%20the%20potential%20of%20modification%20and%0Acombining%20multiple%20methods%20to%20create%20more%20sophisticated%20and%20effective%0Aadversarial%20attack%20strategies%2C%20contributing%20to%20the%20development%20of%20more%20robust%0Aand%20secure%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02649v1&entry.124074799=Read"},
{"title": "Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon", "author": "Ramon Tavares", "abstract": "  This study presents a comprehensive methodology for modeling and forecasting\nthe historical time series of fire spots detected by the AQUA_M-T satellite in\nthe Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network\n(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit\n(GRU) architectures to predict monthly accumulations of daily detected fire\nspots. A summary of the data revealed a consistent seasonality over time, with\nannual maximum and minimum fire spot values tending to repeat at the same\nperiods each year. The primary objective is to verify whether the forecasts\ncapture this inherent seasonality through rigorous statistical analysis. The\nmethodology involved careful data preparation, model configuration, and\ntraining using cross-validation with two seeds, ensuring that the data\ngeneralizes well to the test and validation sets, and confirming the\nconvergence of the model parameters. The results indicate that the mixed LSTM\nand GRU model offers improved accuracy in forecasting 12 months ahead,\ndemonstrating its effectiveness in capturing complex temporal patterns and\nmodeling the observed time series. This research significantly contributes to\nthe application of deep learning techniques in environmental monitoring,\nspecifically in fire spot forecasting. In addition to improving forecast\naccuracy, the proposed approach highlights the potential for adaptation to\nother time series forecasting challenges, opening new avenues for research and\ndevelopment in machine learning and natural phenomenon prediction. Keywords:\nTime Series Forecasting, Recurrent Neural Networks, Deep Learning.\n", "link": "http://arxiv.org/abs/2409.02681v1", "date": "2024-09-04", "relevancy": 1.813, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4688}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4531}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Networks%20with%20LSTM%20and%20GRU%20in%20Modeling%20Active%20Fires%20in%20the%20Amazon&body=Title%3A%20Neural%20Networks%20with%20LSTM%20and%20GRU%20in%20Modeling%20Active%20Fires%20in%20the%20Amazon%0AAuthor%3A%20Ramon%20Tavares%0AAbstract%3A%20%20%20This%20study%20presents%20a%20comprehensive%20methodology%20for%20modeling%20and%20forecasting%0Athe%20historical%20time%20series%20of%20fire%20spots%20detected%20by%20the%20AQUA_M-T%20satellite%20in%0Athe%20Amazon%2C%20Brazil.%20The%20approach%20utilizes%20a%20mixed%20Recurrent%20Neural%20Network%0A%28RNN%29%20model%2C%20combining%20Long%20Short-Term%20Memory%20%28LSTM%29%20and%20Gated%20Recurrent%20Unit%0A%28GRU%29%20architectures%20to%20predict%20monthly%20accumulations%20of%20daily%20detected%20fire%0Aspots.%20A%20summary%20of%20the%20data%20revealed%20a%20consistent%20seasonality%20over%20time%2C%20with%0Aannual%20maximum%20and%20minimum%20fire%20spot%20values%20tending%20to%20repeat%20at%20the%20same%0Aperiods%20each%20year.%20The%20primary%20objective%20is%20to%20verify%20whether%20the%20forecasts%0Acapture%20this%20inherent%20seasonality%20through%20rigorous%20statistical%20analysis.%20The%0Amethodology%20involved%20careful%20data%20preparation%2C%20model%20configuration%2C%20and%0Atraining%20using%20cross-validation%20with%20two%20seeds%2C%20ensuring%20that%20the%20data%0Ageneralizes%20well%20to%20the%20test%20and%20validation%20sets%2C%20and%20confirming%20the%0Aconvergence%20of%20the%20model%20parameters.%20The%20results%20indicate%20that%20the%20mixed%20LSTM%0Aand%20GRU%20model%20offers%20improved%20accuracy%20in%20forecasting%2012%20months%20ahead%2C%0Ademonstrating%20its%20effectiveness%20in%20capturing%20complex%20temporal%20patterns%20and%0Amodeling%20the%20observed%20time%20series.%20This%20research%20significantly%20contributes%20to%0Athe%20application%20of%20deep%20learning%20techniques%20in%20environmental%20monitoring%2C%0Aspecifically%20in%20fire%20spot%20forecasting.%20In%20addition%20to%20improving%20forecast%0Aaccuracy%2C%20the%20proposed%20approach%20highlights%20the%20potential%20for%20adaptation%20to%0Aother%20time%20series%20forecasting%20challenges%2C%20opening%20new%20avenues%20for%20research%20and%0Adevelopment%20in%20machine%20learning%20and%20natural%20phenomenon%20prediction.%20Keywords%3A%0ATime%20Series%20Forecasting%2C%20Recurrent%20Neural%20Networks%2C%20Deep%20Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Networks%2520with%2520LSTM%2520and%2520GRU%2520in%2520Modeling%2520Active%2520Fires%2520in%2520the%2520Amazon%26entry.906535625%3DRamon%2520Tavares%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520comprehensive%2520methodology%2520for%2520modeling%2520and%2520forecasting%250Athe%2520historical%2520time%2520series%2520of%2520fire%2520spots%2520detected%2520by%2520the%2520AQUA_M-T%2520satellite%2520in%250Athe%2520Amazon%252C%2520Brazil.%2520The%2520approach%2520utilizes%2520a%2520mixed%2520Recurrent%2520Neural%2520Network%250A%2528RNN%2529%2520model%252C%2520combining%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520and%2520Gated%2520Recurrent%2520Unit%250A%2528GRU%2529%2520architectures%2520to%2520predict%2520monthly%2520accumulations%2520of%2520daily%2520detected%2520fire%250Aspots.%2520A%2520summary%2520of%2520the%2520data%2520revealed%2520a%2520consistent%2520seasonality%2520over%2520time%252C%2520with%250Aannual%2520maximum%2520and%2520minimum%2520fire%2520spot%2520values%2520tending%2520to%2520repeat%2520at%2520the%2520same%250Aperiods%2520each%2520year.%2520The%2520primary%2520objective%2520is%2520to%2520verify%2520whether%2520the%2520forecasts%250Acapture%2520this%2520inherent%2520seasonality%2520through%2520rigorous%2520statistical%2520analysis.%2520The%250Amethodology%2520involved%2520careful%2520data%2520preparation%252C%2520model%2520configuration%252C%2520and%250Atraining%2520using%2520cross-validation%2520with%2520two%2520seeds%252C%2520ensuring%2520that%2520the%2520data%250Ageneralizes%2520well%2520to%2520the%2520test%2520and%2520validation%2520sets%252C%2520and%2520confirming%2520the%250Aconvergence%2520of%2520the%2520model%2520parameters.%2520The%2520results%2520indicate%2520that%2520the%2520mixed%2520LSTM%250Aand%2520GRU%2520model%2520offers%2520improved%2520accuracy%2520in%2520forecasting%252012%2520months%2520ahead%252C%250Ademonstrating%2520its%2520effectiveness%2520in%2520capturing%2520complex%2520temporal%2520patterns%2520and%250Amodeling%2520the%2520observed%2520time%2520series.%2520This%2520research%2520significantly%2520contributes%2520to%250Athe%2520application%2520of%2520deep%2520learning%2520techniques%2520in%2520environmental%2520monitoring%252C%250Aspecifically%2520in%2520fire%2520spot%2520forecasting.%2520In%2520addition%2520to%2520improving%2520forecast%250Aaccuracy%252C%2520the%2520proposed%2520approach%2520highlights%2520the%2520potential%2520for%2520adaptation%2520to%250Aother%2520time%2520series%2520forecasting%2520challenges%252C%2520opening%2520new%2520avenues%2520for%2520research%2520and%250Adevelopment%2520in%2520machine%2520learning%2520and%2520natural%2520phenomenon%2520prediction.%2520Keywords%253A%250ATime%2520Series%2520Forecasting%252C%2520Recurrent%2520Neural%2520Networks%252C%2520Deep%2520Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20with%20LSTM%20and%20GRU%20in%20Modeling%20Active%20Fires%20in%20the%20Amazon&entry.906535625=Ramon%20Tavares&entry.1292438233=%20%20This%20study%20presents%20a%20comprehensive%20methodology%20for%20modeling%20and%20forecasting%0Athe%20historical%20time%20series%20of%20fire%20spots%20detected%20by%20the%20AQUA_M-T%20satellite%20in%0Athe%20Amazon%2C%20Brazil.%20The%20approach%20utilizes%20a%20mixed%20Recurrent%20Neural%20Network%0A%28RNN%29%20model%2C%20combining%20Long%20Short-Term%20Memory%20%28LSTM%29%20and%20Gated%20Recurrent%20Unit%0A%28GRU%29%20architectures%20to%20predict%20monthly%20accumulations%20of%20daily%20detected%20fire%0Aspots.%20A%20summary%20of%20the%20data%20revealed%20a%20consistent%20seasonality%20over%20time%2C%20with%0Aannual%20maximum%20and%20minimum%20fire%20spot%20values%20tending%20to%20repeat%20at%20the%20same%0Aperiods%20each%20year.%20The%20primary%20objective%20is%20to%20verify%20whether%20the%20forecasts%0Acapture%20this%20inherent%20seasonality%20through%20rigorous%20statistical%20analysis.%20The%0Amethodology%20involved%20careful%20data%20preparation%2C%20model%20configuration%2C%20and%0Atraining%20using%20cross-validation%20with%20two%20seeds%2C%20ensuring%20that%20the%20data%0Ageneralizes%20well%20to%20the%20test%20and%20validation%20sets%2C%20and%20confirming%20the%0Aconvergence%20of%20the%20model%20parameters.%20The%20results%20indicate%20that%20the%20mixed%20LSTM%0Aand%20GRU%20model%20offers%20improved%20accuracy%20in%20forecasting%2012%20months%20ahead%2C%0Ademonstrating%20its%20effectiveness%20in%20capturing%20complex%20temporal%20patterns%20and%0Amodeling%20the%20observed%20time%20series.%20This%20research%20significantly%20contributes%20to%0Athe%20application%20of%20deep%20learning%20techniques%20in%20environmental%20monitoring%2C%0Aspecifically%20in%20fire%20spot%20forecasting.%20In%20addition%20to%20improving%20forecast%0Aaccuracy%2C%20the%20proposed%20approach%20highlights%20the%20potential%20for%20adaptation%20to%0Aother%20time%20series%20forecasting%20challenges%2C%20opening%20new%20avenues%20for%20research%20and%0Adevelopment%20in%20machine%20learning%20and%20natural%20phenomenon%20prediction.%20Keywords%3A%0ATime%20Series%20Forecasting%2C%20Recurrent%20Neural%20Networks%2C%20Deep%20Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02681v1&entry.124074799=Read"},
{"title": "Task-Oriented Communication for Graph Data: A Graph Information\n  Bottleneck Approach", "author": "Shujing Li and Yanhu Wang and Shuaishuai Guo and Chenyuan Feng", "abstract": "  Graph data, essential in fields like knowledge representation and social\nnetworks, often involves large networks with many nodes and edges. Transmitting\nthese graphs can be highly inefficient due to their size and redundancy for\nspecific tasks. This paper introduces a method to extract a smaller,\ntask-focused subgraph that maintains key information while reducing\ncommunication overhead. Our approach utilizes graph neural networks (GNNs) and\nthe graph information bottleneck (GIB) principle to create a compact,\ninformative, and robust graph representation suitable for transmission. The\nchallenge lies in the irregular structure of graph data, making GIB\noptimization complex. We address this by deriving a tractable variational upper\nbound for the objective function. Additionally, we propose the VQ-GIB\nmechanism, integrating vector quantization (VQ) to convert subgraph\nrepresentations into a discrete codebook sequence, compatible with existing\ndigital communication systems. Our experiments show that this GIB-based method\nsignificantly lowers communication costs while preserving essential\ntask-related information. The approach demonstrates robust performance across\nvarious communication channels, suitable for both continuous and discrete\nsystems.\n", "link": "http://arxiv.org/abs/2409.02728v1", "date": "2024-09-04", "relevancy": 1.7976, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4739}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4341}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Oriented%20Communication%20for%20Graph%20Data%3A%20A%20Graph%20Information%0A%20%20Bottleneck%20Approach&body=Title%3A%20Task-Oriented%20Communication%20for%20Graph%20Data%3A%20A%20Graph%20Information%0A%20%20Bottleneck%20Approach%0AAuthor%3A%20Shujing%20Li%20and%20Yanhu%20Wang%20and%20Shuaishuai%20Guo%20and%20Chenyuan%20Feng%0AAbstract%3A%20%20%20Graph%20data%2C%20essential%20in%20fields%20like%20knowledge%20representation%20and%20social%0Anetworks%2C%20often%20involves%20large%20networks%20with%20many%20nodes%20and%20edges.%20Transmitting%0Athese%20graphs%20can%20be%20highly%20inefficient%20due%20to%20their%20size%20and%20redundancy%20for%0Aspecific%20tasks.%20This%20paper%20introduces%20a%20method%20to%20extract%20a%20smaller%2C%0Atask-focused%20subgraph%20that%20maintains%20key%20information%20while%20reducing%0Acommunication%20overhead.%20Our%20approach%20utilizes%20graph%20neural%20networks%20%28GNNs%29%20and%0Athe%20graph%20information%20bottleneck%20%28GIB%29%20principle%20to%20create%20a%20compact%2C%0Ainformative%2C%20and%20robust%20graph%20representation%20suitable%20for%20transmission.%20The%0Achallenge%20lies%20in%20the%20irregular%20structure%20of%20graph%20data%2C%20making%20GIB%0Aoptimization%20complex.%20We%20address%20this%20by%20deriving%20a%20tractable%20variational%20upper%0Abound%20for%20the%20objective%20function.%20Additionally%2C%20we%20propose%20the%20VQ-GIB%0Amechanism%2C%20integrating%20vector%20quantization%20%28VQ%29%20to%20convert%20subgraph%0Arepresentations%20into%20a%20discrete%20codebook%20sequence%2C%20compatible%20with%20existing%0Adigital%20communication%20systems.%20Our%20experiments%20show%20that%20this%20GIB-based%20method%0Asignificantly%20lowers%20communication%20costs%20while%20preserving%20essential%0Atask-related%20information.%20The%20approach%20demonstrates%20robust%20performance%20across%0Avarious%20communication%20channels%2C%20suitable%20for%20both%20continuous%20and%20discrete%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Oriented%2520Communication%2520for%2520Graph%2520Data%253A%2520A%2520Graph%2520Information%250A%2520%2520Bottleneck%2520Approach%26entry.906535625%3DShujing%2520Li%2520and%2520Yanhu%2520Wang%2520and%2520Shuaishuai%2520Guo%2520and%2520Chenyuan%2520Feng%26entry.1292438233%3D%2520%2520Graph%2520data%252C%2520essential%2520in%2520fields%2520like%2520knowledge%2520representation%2520and%2520social%250Anetworks%252C%2520often%2520involves%2520large%2520networks%2520with%2520many%2520nodes%2520and%2520edges.%2520Transmitting%250Athese%2520graphs%2520can%2520be%2520highly%2520inefficient%2520due%2520to%2520their%2520size%2520and%2520redundancy%2520for%250Aspecific%2520tasks.%2520This%2520paper%2520introduces%2520a%2520method%2520to%2520extract%2520a%2520smaller%252C%250Atask-focused%2520subgraph%2520that%2520maintains%2520key%2520information%2520while%2520reducing%250Acommunication%2520overhead.%2520Our%2520approach%2520utilizes%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520and%250Athe%2520graph%2520information%2520bottleneck%2520%2528GIB%2529%2520principle%2520to%2520create%2520a%2520compact%252C%250Ainformative%252C%2520and%2520robust%2520graph%2520representation%2520suitable%2520for%2520transmission.%2520The%250Achallenge%2520lies%2520in%2520the%2520irregular%2520structure%2520of%2520graph%2520data%252C%2520making%2520GIB%250Aoptimization%2520complex.%2520We%2520address%2520this%2520by%2520deriving%2520a%2520tractable%2520variational%2520upper%250Abound%2520for%2520the%2520objective%2520function.%2520Additionally%252C%2520we%2520propose%2520the%2520VQ-GIB%250Amechanism%252C%2520integrating%2520vector%2520quantization%2520%2528VQ%2529%2520to%2520convert%2520subgraph%250Arepresentations%2520into%2520a%2520discrete%2520codebook%2520sequence%252C%2520compatible%2520with%2520existing%250Adigital%2520communication%2520systems.%2520Our%2520experiments%2520show%2520that%2520this%2520GIB-based%2520method%250Asignificantly%2520lowers%2520communication%2520costs%2520while%2520preserving%2520essential%250Atask-related%2520information.%2520The%2520approach%2520demonstrates%2520robust%2520performance%2520across%250Avarious%2520communication%2520channels%252C%2520suitable%2520for%2520both%2520continuous%2520and%2520discrete%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Oriented%20Communication%20for%20Graph%20Data%3A%20A%20Graph%20Information%0A%20%20Bottleneck%20Approach&entry.906535625=Shujing%20Li%20and%20Yanhu%20Wang%20and%20Shuaishuai%20Guo%20and%20Chenyuan%20Feng&entry.1292438233=%20%20Graph%20data%2C%20essential%20in%20fields%20like%20knowledge%20representation%20and%20social%0Anetworks%2C%20often%20involves%20large%20networks%20with%20many%20nodes%20and%20edges.%20Transmitting%0Athese%20graphs%20can%20be%20highly%20inefficient%20due%20to%20their%20size%20and%20redundancy%20for%0Aspecific%20tasks.%20This%20paper%20introduces%20a%20method%20to%20extract%20a%20smaller%2C%0Atask-focused%20subgraph%20that%20maintains%20key%20information%20while%20reducing%0Acommunication%20overhead.%20Our%20approach%20utilizes%20graph%20neural%20networks%20%28GNNs%29%20and%0Athe%20graph%20information%20bottleneck%20%28GIB%29%20principle%20to%20create%20a%20compact%2C%0Ainformative%2C%20and%20robust%20graph%20representation%20suitable%20for%20transmission.%20The%0Achallenge%20lies%20in%20the%20irregular%20structure%20of%20graph%20data%2C%20making%20GIB%0Aoptimization%20complex.%20We%20address%20this%20by%20deriving%20a%20tractable%20variational%20upper%0Abound%20for%20the%20objective%20function.%20Additionally%2C%20we%20propose%20the%20VQ-GIB%0Amechanism%2C%20integrating%20vector%20quantization%20%28VQ%29%20to%20convert%20subgraph%0Arepresentations%20into%20a%20discrete%20codebook%20sequence%2C%20compatible%20with%20existing%0Adigital%20communication%20systems.%20Our%20experiments%20show%20that%20this%20GIB-based%20method%0Asignificantly%20lowers%20communication%20costs%20while%20preserving%20essential%0Atask-related%20information.%20The%20approach%20demonstrates%20robust%20performance%20across%0Avarious%20communication%20channels%2C%20suitable%20for%20both%20continuous%20and%20discrete%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02728v1&entry.124074799=Read"},
{"title": "AlignGroup: Learning and Aligning Group Consensus with Member\n  Preferences for Group Recommendation", "author": "Jinfeng Xu and Zheyu Chen and Jinze Li and Shuo Yang and Hewei Wang and Edith C. -H. Ngai", "abstract": "  Group activities are important behaviors in human society, providing\npersonalized recommendations for groups is referred to as the group\nrecommendation task. Existing methods can usually be categorized into two\nstrategies to infer group preferences: 1) determining group preferences by\naggregating members' personalized preferences, and 2) inferring group consensus\nby capturing group members' coherent decisions after common compromises.\nHowever, the former would suffer from the lack of group-level considerations,\nand the latter overlooks the fine-grained preferences of individual users. To\nthis end, we propose a novel group recommendation method AlignGroup, which\nfocuses on both group consensus and individual preferences of group members to\ninfer the group decision-making. Specifically, AlignGroup explores group\nconsensus through a well-designed hypergraph neural network that efficiently\nlearns intra- and inter-group relationships. Moreover, AlignGroup innovatively\nutilizes a self-supervised alignment task to capture fine-grained group\ndecision-making by aligning the group consensus with members' common\npreferences. Extensive experiments on two real-world datasets validate that our\nAlignGroup outperforms the state-of-the-art on both the group recommendation\ntask and the user recommendation task, as well as outperforms the efficiency of\nmost baselines.\n", "link": "http://arxiv.org/abs/2409.02580v1", "date": "2024-09-04", "relevancy": 1.7904, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4698}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4333}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignGroup%3A%20Learning%20and%20Aligning%20Group%20Consensus%20with%20Member%0A%20%20Preferences%20for%20Group%20Recommendation&body=Title%3A%20AlignGroup%3A%20Learning%20and%20Aligning%20Group%20Consensus%20with%20Member%0A%20%20Preferences%20for%20Group%20Recommendation%0AAuthor%3A%20Jinfeng%20Xu%20and%20Zheyu%20Chen%20and%20Jinze%20Li%20and%20Shuo%20Yang%20and%20Hewei%20Wang%20and%20Edith%20C.%20-H.%20Ngai%0AAbstract%3A%20%20%20Group%20activities%20are%20important%20behaviors%20in%20human%20society%2C%20providing%0Apersonalized%20recommendations%20for%20groups%20is%20referred%20to%20as%20the%20group%0Arecommendation%20task.%20Existing%20methods%20can%20usually%20be%20categorized%20into%20two%0Astrategies%20to%20infer%20group%20preferences%3A%201%29%20determining%20group%20preferences%20by%0Aaggregating%20members%27%20personalized%20preferences%2C%20and%202%29%20inferring%20group%20consensus%0Aby%20capturing%20group%20members%27%20coherent%20decisions%20after%20common%20compromises.%0AHowever%2C%20the%20former%20would%20suffer%20from%20the%20lack%20of%20group-level%20considerations%2C%0Aand%20the%20latter%20overlooks%20the%20fine-grained%20preferences%20of%20individual%20users.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20group%20recommendation%20method%20AlignGroup%2C%20which%0Afocuses%20on%20both%20group%20consensus%20and%20individual%20preferences%20of%20group%20members%20to%0Ainfer%20the%20group%20decision-making.%20Specifically%2C%20AlignGroup%20explores%20group%0Aconsensus%20through%20a%20well-designed%20hypergraph%20neural%20network%20that%20efficiently%0Alearns%20intra-%20and%20inter-group%20relationships.%20Moreover%2C%20AlignGroup%20innovatively%0Autilizes%20a%20self-supervised%20alignment%20task%20to%20capture%20fine-grained%20group%0Adecision-making%20by%20aligning%20the%20group%20consensus%20with%20members%27%20common%0Apreferences.%20Extensive%20experiments%20on%20two%20real-world%20datasets%20validate%20that%20our%0AAlignGroup%20outperforms%20the%20state-of-the-art%20on%20both%20the%20group%20recommendation%0Atask%20and%20the%20user%20recommendation%20task%2C%20as%20well%20as%20outperforms%20the%20efficiency%20of%0Amost%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignGroup%253A%2520Learning%2520and%2520Aligning%2520Group%2520Consensus%2520with%2520Member%250A%2520%2520Preferences%2520for%2520Group%2520Recommendation%26entry.906535625%3DJinfeng%2520Xu%2520and%2520Zheyu%2520Chen%2520and%2520Jinze%2520Li%2520and%2520Shuo%2520Yang%2520and%2520Hewei%2520Wang%2520and%2520Edith%2520C.%2520-H.%2520Ngai%26entry.1292438233%3D%2520%2520Group%2520activities%2520are%2520important%2520behaviors%2520in%2520human%2520society%252C%2520providing%250Apersonalized%2520recommendations%2520for%2520groups%2520is%2520referred%2520to%2520as%2520the%2520group%250Arecommendation%2520task.%2520Existing%2520methods%2520can%2520usually%2520be%2520categorized%2520into%2520two%250Astrategies%2520to%2520infer%2520group%2520preferences%253A%25201%2529%2520determining%2520group%2520preferences%2520by%250Aaggregating%2520members%2527%2520personalized%2520preferences%252C%2520and%25202%2529%2520inferring%2520group%2520consensus%250Aby%2520capturing%2520group%2520members%2527%2520coherent%2520decisions%2520after%2520common%2520compromises.%250AHowever%252C%2520the%2520former%2520would%2520suffer%2520from%2520the%2520lack%2520of%2520group-level%2520considerations%252C%250Aand%2520the%2520latter%2520overlooks%2520the%2520fine-grained%2520preferences%2520of%2520individual%2520users.%2520To%250Athis%2520end%252C%2520we%2520propose%2520a%2520novel%2520group%2520recommendation%2520method%2520AlignGroup%252C%2520which%250Afocuses%2520on%2520both%2520group%2520consensus%2520and%2520individual%2520preferences%2520of%2520group%2520members%2520to%250Ainfer%2520the%2520group%2520decision-making.%2520Specifically%252C%2520AlignGroup%2520explores%2520group%250Aconsensus%2520through%2520a%2520well-designed%2520hypergraph%2520neural%2520network%2520that%2520efficiently%250Alearns%2520intra-%2520and%2520inter-group%2520relationships.%2520Moreover%252C%2520AlignGroup%2520innovatively%250Autilizes%2520a%2520self-supervised%2520alignment%2520task%2520to%2520capture%2520fine-grained%2520group%250Adecision-making%2520by%2520aligning%2520the%2520group%2520consensus%2520with%2520members%2527%2520common%250Apreferences.%2520Extensive%2520experiments%2520on%2520two%2520real-world%2520datasets%2520validate%2520that%2520our%250AAlignGroup%2520outperforms%2520the%2520state-of-the-art%2520on%2520both%2520the%2520group%2520recommendation%250Atask%2520and%2520the%2520user%2520recommendation%2520task%252C%2520as%2520well%2520as%2520outperforms%2520the%2520efficiency%2520of%250Amost%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignGroup%3A%20Learning%20and%20Aligning%20Group%20Consensus%20with%20Member%0A%20%20Preferences%20for%20Group%20Recommendation&entry.906535625=Jinfeng%20Xu%20and%20Zheyu%20Chen%20and%20Jinze%20Li%20and%20Shuo%20Yang%20and%20Hewei%20Wang%20and%20Edith%20C.%20-H.%20Ngai&entry.1292438233=%20%20Group%20activities%20are%20important%20behaviors%20in%20human%20society%2C%20providing%0Apersonalized%20recommendations%20for%20groups%20is%20referred%20to%20as%20the%20group%0Arecommendation%20task.%20Existing%20methods%20can%20usually%20be%20categorized%20into%20two%0Astrategies%20to%20infer%20group%20preferences%3A%201%29%20determining%20group%20preferences%20by%0Aaggregating%20members%27%20personalized%20preferences%2C%20and%202%29%20inferring%20group%20consensus%0Aby%20capturing%20group%20members%27%20coherent%20decisions%20after%20common%20compromises.%0AHowever%2C%20the%20former%20would%20suffer%20from%20the%20lack%20of%20group-level%20considerations%2C%0Aand%20the%20latter%20overlooks%20the%20fine-grained%20preferences%20of%20individual%20users.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20group%20recommendation%20method%20AlignGroup%2C%20which%0Afocuses%20on%20both%20group%20consensus%20and%20individual%20preferences%20of%20group%20members%20to%0Ainfer%20the%20group%20decision-making.%20Specifically%2C%20AlignGroup%20explores%20group%0Aconsensus%20through%20a%20well-designed%20hypergraph%20neural%20network%20that%20efficiently%0Alearns%20intra-%20and%20inter-group%20relationships.%20Moreover%2C%20AlignGroup%20innovatively%0Autilizes%20a%20self-supervised%20alignment%20task%20to%20capture%20fine-grained%20group%0Adecision-making%20by%20aligning%20the%20group%20consensus%20with%20members%27%20common%0Apreferences.%20Extensive%20experiments%20on%20two%20real-world%20datasets%20validate%20that%20our%0AAlignGroup%20outperforms%20the%20state-of-the-art%20on%20both%20the%20group%20recommendation%0Atask%20and%20the%20user%20recommendation%20task%2C%20as%20well%20as%20outperforms%20the%20efficiency%20of%0Amost%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02580v1&entry.124074799=Read"},
{"title": "R2GQA: Retriever-Reader-Generator Question Answering System to Support\n  Students Understanding Legal Regulations in Higher Education", "author": "Phuc-Tinh Pham Do and Duy-Ngoc Dinh Cao and Khanh Quoc Tran and Kiet Van Nguyen", "abstract": "  In this article, we propose the R2GQA system, a Retriever-Reader-Generator\nQuestion Answering system, consisting of three main components: Document\nRetriever, Machine Reader, and Answer Generator. The Retriever module employs\nadvanced information retrieval techniques to extract the context of articles\nfrom a dataset of legal regulation documents. The Machine Reader module\nutilizes state-of-the-art natural language understanding algorithms to\ncomprehend the retrieved documents and extract answers. Finally, the Generator\nmodule synthesizes the extracted answers into concise and informative responses\nto questions of students regarding legal regulations. Furthermore, we built the\nViRHE4QA dataset in the domain of university training regulations, comprising\n9,758 question-answer pairs with a rigorous construction process. This is the\nfirst Vietnamese dataset in the higher regulations domain with various types of\nanswers, both extractive and abstractive. In addition, the R2GQA system is the\nfirst system to offer abstractive answers in Vietnamese. This paper discusses\nthe design and implementation of each module within the R2GQA system on the\nViRHE4QA dataset, highlighting their functionalities and interactions.\nFurthermore, we present experimental results demonstrating the effectiveness\nand utility of the proposed system in supporting the comprehension of students\nof legal regulations in higher education settings. In general, the R2GQA system\nand the ViRHE4QA dataset promise to contribute significantly to related\nresearch and help students navigate complex legal documents and regulations,\nempowering them to make informed decisions and adhere to institutional policies\neffectively. Our dataset is available for research purposes.\n", "link": "http://arxiv.org/abs/2409.02840v1", "date": "2024-09-04", "relevancy": 1.786, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4523}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4456}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R2GQA%3A%20Retriever-Reader-Generator%20Question%20Answering%20System%20to%20Support%0A%20%20Students%20Understanding%20Legal%20Regulations%20in%20Higher%20Education&body=Title%3A%20R2GQA%3A%20Retriever-Reader-Generator%20Question%20Answering%20System%20to%20Support%0A%20%20Students%20Understanding%20Legal%20Regulations%20in%20Higher%20Education%0AAuthor%3A%20Phuc-Tinh%20Pham%20Do%20and%20Duy-Ngoc%20Dinh%20Cao%20and%20Khanh%20Quoc%20Tran%20and%20Kiet%20Van%20Nguyen%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20propose%20the%20R2GQA%20system%2C%20a%20Retriever-Reader-Generator%0AQuestion%20Answering%20system%2C%20consisting%20of%20three%20main%20components%3A%20Document%0ARetriever%2C%20Machine%20Reader%2C%20and%20Answer%20Generator.%20The%20Retriever%20module%20employs%0Aadvanced%20information%20retrieval%20techniques%20to%20extract%20the%20context%20of%20articles%0Afrom%20a%20dataset%20of%20legal%20regulation%20documents.%20The%20Machine%20Reader%20module%0Autilizes%20state-of-the-art%20natural%20language%20understanding%20algorithms%20to%0Acomprehend%20the%20retrieved%20documents%20and%20extract%20answers.%20Finally%2C%20the%20Generator%0Amodule%20synthesizes%20the%20extracted%20answers%20into%20concise%20and%20informative%20responses%0Ato%20questions%20of%20students%20regarding%20legal%20regulations.%20Furthermore%2C%20we%20built%20the%0AViRHE4QA%20dataset%20in%20the%20domain%20of%20university%20training%20regulations%2C%20comprising%0A9%2C758%20question-answer%20pairs%20with%20a%20rigorous%20construction%20process.%20This%20is%20the%0Afirst%20Vietnamese%20dataset%20in%20the%20higher%20regulations%20domain%20with%20various%20types%20of%0Aanswers%2C%20both%20extractive%20and%20abstractive.%20In%20addition%2C%20the%20R2GQA%20system%20is%20the%0Afirst%20system%20to%20offer%20abstractive%20answers%20in%20Vietnamese.%20This%20paper%20discusses%0Athe%20design%20and%20implementation%20of%20each%20module%20within%20the%20R2GQA%20system%20on%20the%0AViRHE4QA%20dataset%2C%20highlighting%20their%20functionalities%20and%20interactions.%0AFurthermore%2C%20we%20present%20experimental%20results%20demonstrating%20the%20effectiveness%0Aand%20utility%20of%20the%20proposed%20system%20in%20supporting%20the%20comprehension%20of%20students%0Aof%20legal%20regulations%20in%20higher%20education%20settings.%20In%20general%2C%20the%20R2GQA%20system%0Aand%20the%20ViRHE4QA%20dataset%20promise%20to%20contribute%20significantly%20to%20related%0Aresearch%20and%20help%20students%20navigate%20complex%20legal%20documents%20and%20regulations%2C%0Aempowering%20them%20to%20make%20informed%20decisions%20and%20adhere%20to%20institutional%20policies%0Aeffectively.%20Our%20dataset%20is%20available%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR2GQA%253A%2520Retriever-Reader-Generator%2520Question%2520Answering%2520System%2520to%2520Support%250A%2520%2520Students%2520Understanding%2520Legal%2520Regulations%2520in%2520Higher%2520Education%26entry.906535625%3DPhuc-Tinh%2520Pham%2520Do%2520and%2520Duy-Ngoc%2520Dinh%2520Cao%2520and%2520Khanh%2520Quoc%2520Tran%2520and%2520Kiet%2520Van%2520Nguyen%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520propose%2520the%2520R2GQA%2520system%252C%2520a%2520Retriever-Reader-Generator%250AQuestion%2520Answering%2520system%252C%2520consisting%2520of%2520three%2520main%2520components%253A%2520Document%250ARetriever%252C%2520Machine%2520Reader%252C%2520and%2520Answer%2520Generator.%2520The%2520Retriever%2520module%2520employs%250Aadvanced%2520information%2520retrieval%2520techniques%2520to%2520extract%2520the%2520context%2520of%2520articles%250Afrom%2520a%2520dataset%2520of%2520legal%2520regulation%2520documents.%2520The%2520Machine%2520Reader%2520module%250Autilizes%2520state-of-the-art%2520natural%2520language%2520understanding%2520algorithms%2520to%250Acomprehend%2520the%2520retrieved%2520documents%2520and%2520extract%2520answers.%2520Finally%252C%2520the%2520Generator%250Amodule%2520synthesizes%2520the%2520extracted%2520answers%2520into%2520concise%2520and%2520informative%2520responses%250Ato%2520questions%2520of%2520students%2520regarding%2520legal%2520regulations.%2520Furthermore%252C%2520we%2520built%2520the%250AViRHE4QA%2520dataset%2520in%2520the%2520domain%2520of%2520university%2520training%2520regulations%252C%2520comprising%250A9%252C758%2520question-answer%2520pairs%2520with%2520a%2520rigorous%2520construction%2520process.%2520This%2520is%2520the%250Afirst%2520Vietnamese%2520dataset%2520in%2520the%2520higher%2520regulations%2520domain%2520with%2520various%2520types%2520of%250Aanswers%252C%2520both%2520extractive%2520and%2520abstractive.%2520In%2520addition%252C%2520the%2520R2GQA%2520system%2520is%2520the%250Afirst%2520system%2520to%2520offer%2520abstractive%2520answers%2520in%2520Vietnamese.%2520This%2520paper%2520discusses%250Athe%2520design%2520and%2520implementation%2520of%2520each%2520module%2520within%2520the%2520R2GQA%2520system%2520on%2520the%250AViRHE4QA%2520dataset%252C%2520highlighting%2520their%2520functionalities%2520and%2520interactions.%250AFurthermore%252C%2520we%2520present%2520experimental%2520results%2520demonstrating%2520the%2520effectiveness%250Aand%2520utility%2520of%2520the%2520proposed%2520system%2520in%2520supporting%2520the%2520comprehension%2520of%2520students%250Aof%2520legal%2520regulations%2520in%2520higher%2520education%2520settings.%2520In%2520general%252C%2520the%2520R2GQA%2520system%250Aand%2520the%2520ViRHE4QA%2520dataset%2520promise%2520to%2520contribute%2520significantly%2520to%2520related%250Aresearch%2520and%2520help%2520students%2520navigate%2520complex%2520legal%2520documents%2520and%2520regulations%252C%250Aempowering%2520them%2520to%2520make%2520informed%2520decisions%2520and%2520adhere%2520to%2520institutional%2520policies%250Aeffectively.%2520Our%2520dataset%2520is%2520available%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R2GQA%3A%20Retriever-Reader-Generator%20Question%20Answering%20System%20to%20Support%0A%20%20Students%20Understanding%20Legal%20Regulations%20in%20Higher%20Education&entry.906535625=Phuc-Tinh%20Pham%20Do%20and%20Duy-Ngoc%20Dinh%20Cao%20and%20Khanh%20Quoc%20Tran%20and%20Kiet%20Van%20Nguyen&entry.1292438233=%20%20In%20this%20article%2C%20we%20propose%20the%20R2GQA%20system%2C%20a%20Retriever-Reader-Generator%0AQuestion%20Answering%20system%2C%20consisting%20of%20three%20main%20components%3A%20Document%0ARetriever%2C%20Machine%20Reader%2C%20and%20Answer%20Generator.%20The%20Retriever%20module%20employs%0Aadvanced%20information%20retrieval%20techniques%20to%20extract%20the%20context%20of%20articles%0Afrom%20a%20dataset%20of%20legal%20regulation%20documents.%20The%20Machine%20Reader%20module%0Autilizes%20state-of-the-art%20natural%20language%20understanding%20algorithms%20to%0Acomprehend%20the%20retrieved%20documents%20and%20extract%20answers.%20Finally%2C%20the%20Generator%0Amodule%20synthesizes%20the%20extracted%20answers%20into%20concise%20and%20informative%20responses%0Ato%20questions%20of%20students%20regarding%20legal%20regulations.%20Furthermore%2C%20we%20built%20the%0AViRHE4QA%20dataset%20in%20the%20domain%20of%20university%20training%20regulations%2C%20comprising%0A9%2C758%20question-answer%20pairs%20with%20a%20rigorous%20construction%20process.%20This%20is%20the%0Afirst%20Vietnamese%20dataset%20in%20the%20higher%20regulations%20domain%20with%20various%20types%20of%0Aanswers%2C%20both%20extractive%20and%20abstractive.%20In%20addition%2C%20the%20R2GQA%20system%20is%20the%0Afirst%20system%20to%20offer%20abstractive%20answers%20in%20Vietnamese.%20This%20paper%20discusses%0Athe%20design%20and%20implementation%20of%20each%20module%20within%20the%20R2GQA%20system%20on%20the%0AViRHE4QA%20dataset%2C%20highlighting%20their%20functionalities%20and%20interactions.%0AFurthermore%2C%20we%20present%20experimental%20results%20demonstrating%20the%20effectiveness%0Aand%20utility%20of%20the%20proposed%20system%20in%20supporting%20the%20comprehension%20of%20students%0Aof%20legal%20regulations%20in%20higher%20education%20settings.%20In%20general%2C%20the%20R2GQA%20system%0Aand%20the%20ViRHE4QA%20dataset%20promise%20to%20contribute%20significantly%20to%20related%0Aresearch%20and%20help%20students%20navigate%20complex%20legal%20documents%20and%20regulations%2C%0Aempowering%20them%20to%20make%20informed%20decisions%20and%20adhere%20to%20institutional%20policies%0Aeffectively.%20Our%20dataset%20is%20available%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02840v1&entry.124074799=Read"},
{"title": "Different Victims, Same Layout: Email Visual Similarity Detection for\n  Enhanced Email Protection", "author": "Sachin Shukla and Omid Mirzaei", "abstract": "  In the pursuit of an effective spam detection system, the focus has often\nbeen on identifying known spam patterns either through rule-based detection\nsystems or machine learning (ML) solutions that rely on keywords. However, both\nsystems are susceptible to evasion techniques and zero-day attacks that can be\nachieved at low cost. Therefore, an email that bypassed the defense system once\ncan do it again in the following days, even though rules are updated or the ML\nmodels are retrained. The recurrence of failures to detect emails that exhibit\nlayout similarities to previously undetected spam is concerning for customers\nand can erode their trust in a company. Our observations show that threat\nactors reuse email kits extensively and can bypass detection with little\neffort, for example, by making changes to the content of emails. In this work,\nwe propose an email visual similarity detection approach, named Pisco, to\nimprove the detection capabilities of an email threat defense system. We apply\nour proof of concept to some real-world samples received from different\nsources. Our results show that email kits are being reused extensively and\nvisually similar emails are sent to our customers at various time intervals.\nTherefore, this method could be very helpful in situations where detection\nengines that rely on textual features and keywords are bypassed, an occurrence\nour observations show happens frequently.\n", "link": "http://arxiv.org/abs/2408.16945v3", "date": "2024-09-04", "relevancy": 1.773, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4327}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Different%20Victims%2C%20Same%20Layout%3A%20Email%20Visual%20Similarity%20Detection%20for%0A%20%20Enhanced%20Email%20Protection&body=Title%3A%20Different%20Victims%2C%20Same%20Layout%3A%20Email%20Visual%20Similarity%20Detection%20for%0A%20%20Enhanced%20Email%20Protection%0AAuthor%3A%20Sachin%20Shukla%20and%20Omid%20Mirzaei%0AAbstract%3A%20%20%20In%20the%20pursuit%20of%20an%20effective%20spam%20detection%20system%2C%20the%20focus%20has%20often%0Abeen%20on%20identifying%20known%20spam%20patterns%20either%20through%20rule-based%20detection%0Asystems%20or%20machine%20learning%20%28ML%29%20solutions%20that%20rely%20on%20keywords.%20However%2C%20both%0Asystems%20are%20susceptible%20to%20evasion%20techniques%20and%20zero-day%20attacks%20that%20can%20be%0Aachieved%20at%20low%20cost.%20Therefore%2C%20an%20email%20that%20bypassed%20the%20defense%20system%20once%0Acan%20do%20it%20again%20in%20the%20following%20days%2C%20even%20though%20rules%20are%20updated%20or%20the%20ML%0Amodels%20are%20retrained.%20The%20recurrence%20of%20failures%20to%20detect%20emails%20that%20exhibit%0Alayout%20similarities%20to%20previously%20undetected%20spam%20is%20concerning%20for%20customers%0Aand%20can%20erode%20their%20trust%20in%20a%20company.%20Our%20observations%20show%20that%20threat%0Aactors%20reuse%20email%20kits%20extensively%20and%20can%20bypass%20detection%20with%20little%0Aeffort%2C%20for%20example%2C%20by%20making%20changes%20to%20the%20content%20of%20emails.%20In%20this%20work%2C%0Awe%20propose%20an%20email%20visual%20similarity%20detection%20approach%2C%20named%20Pisco%2C%20to%0Aimprove%20the%20detection%20capabilities%20of%20an%20email%20threat%20defense%20system.%20We%20apply%0Aour%20proof%20of%20concept%20to%20some%20real-world%20samples%20received%20from%20different%0Asources.%20Our%20results%20show%20that%20email%20kits%20are%20being%20reused%20extensively%20and%0Avisually%20similar%20emails%20are%20sent%20to%20our%20customers%20at%20various%20time%20intervals.%0ATherefore%2C%20this%20method%20could%20be%20very%20helpful%20in%20situations%20where%20detection%0Aengines%20that%20rely%20on%20textual%20features%20and%20keywords%20are%20bypassed%2C%20an%20occurrence%0Aour%20observations%20show%20happens%20frequently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferent%2520Victims%252C%2520Same%2520Layout%253A%2520Email%2520Visual%2520Similarity%2520Detection%2520for%250A%2520%2520Enhanced%2520Email%2520Protection%26entry.906535625%3DSachin%2520Shukla%2520and%2520Omid%2520Mirzaei%26entry.1292438233%3D%2520%2520In%2520the%2520pursuit%2520of%2520an%2520effective%2520spam%2520detection%2520system%252C%2520the%2520focus%2520has%2520often%250Abeen%2520on%2520identifying%2520known%2520spam%2520patterns%2520either%2520through%2520rule-based%2520detection%250Asystems%2520or%2520machine%2520learning%2520%2528ML%2529%2520solutions%2520that%2520rely%2520on%2520keywords.%2520However%252C%2520both%250Asystems%2520are%2520susceptible%2520to%2520evasion%2520techniques%2520and%2520zero-day%2520attacks%2520that%2520can%2520be%250Aachieved%2520at%2520low%2520cost.%2520Therefore%252C%2520an%2520email%2520that%2520bypassed%2520the%2520defense%2520system%2520once%250Acan%2520do%2520it%2520again%2520in%2520the%2520following%2520days%252C%2520even%2520though%2520rules%2520are%2520updated%2520or%2520the%2520ML%250Amodels%2520are%2520retrained.%2520The%2520recurrence%2520of%2520failures%2520to%2520detect%2520emails%2520that%2520exhibit%250Alayout%2520similarities%2520to%2520previously%2520undetected%2520spam%2520is%2520concerning%2520for%2520customers%250Aand%2520can%2520erode%2520their%2520trust%2520in%2520a%2520company.%2520Our%2520observations%2520show%2520that%2520threat%250Aactors%2520reuse%2520email%2520kits%2520extensively%2520and%2520can%2520bypass%2520detection%2520with%2520little%250Aeffort%252C%2520for%2520example%252C%2520by%2520making%2520changes%2520to%2520the%2520content%2520of%2520emails.%2520In%2520this%2520work%252C%250Awe%2520propose%2520an%2520email%2520visual%2520similarity%2520detection%2520approach%252C%2520named%2520Pisco%252C%2520to%250Aimprove%2520the%2520detection%2520capabilities%2520of%2520an%2520email%2520threat%2520defense%2520system.%2520We%2520apply%250Aour%2520proof%2520of%2520concept%2520to%2520some%2520real-world%2520samples%2520received%2520from%2520different%250Asources.%2520Our%2520results%2520show%2520that%2520email%2520kits%2520are%2520being%2520reused%2520extensively%2520and%250Avisually%2520similar%2520emails%2520are%2520sent%2520to%2520our%2520customers%2520at%2520various%2520time%2520intervals.%250ATherefore%252C%2520this%2520method%2520could%2520be%2520very%2520helpful%2520in%2520situations%2520where%2520detection%250Aengines%2520that%2520rely%2520on%2520textual%2520features%2520and%2520keywords%2520are%2520bypassed%252C%2520an%2520occurrence%250Aour%2520observations%2520show%2520happens%2520frequently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Different%20Victims%2C%20Same%20Layout%3A%20Email%20Visual%20Similarity%20Detection%20for%0A%20%20Enhanced%20Email%20Protection&entry.906535625=Sachin%20Shukla%20and%20Omid%20Mirzaei&entry.1292438233=%20%20In%20the%20pursuit%20of%20an%20effective%20spam%20detection%20system%2C%20the%20focus%20has%20often%0Abeen%20on%20identifying%20known%20spam%20patterns%20either%20through%20rule-based%20detection%0Asystems%20or%20machine%20learning%20%28ML%29%20solutions%20that%20rely%20on%20keywords.%20However%2C%20both%0Asystems%20are%20susceptible%20to%20evasion%20techniques%20and%20zero-day%20attacks%20that%20can%20be%0Aachieved%20at%20low%20cost.%20Therefore%2C%20an%20email%20that%20bypassed%20the%20defense%20system%20once%0Acan%20do%20it%20again%20in%20the%20following%20days%2C%20even%20though%20rules%20are%20updated%20or%20the%20ML%0Amodels%20are%20retrained.%20The%20recurrence%20of%20failures%20to%20detect%20emails%20that%20exhibit%0Alayout%20similarities%20to%20previously%20undetected%20spam%20is%20concerning%20for%20customers%0Aand%20can%20erode%20their%20trust%20in%20a%20company.%20Our%20observations%20show%20that%20threat%0Aactors%20reuse%20email%20kits%20extensively%20and%20can%20bypass%20detection%20with%20little%0Aeffort%2C%20for%20example%2C%20by%20making%20changes%20to%20the%20content%20of%20emails.%20In%20this%20work%2C%0Awe%20propose%20an%20email%20visual%20similarity%20detection%20approach%2C%20named%20Pisco%2C%20to%0Aimprove%20the%20detection%20capabilities%20of%20an%20email%20threat%20defense%20system.%20We%20apply%0Aour%20proof%20of%20concept%20to%20some%20real-world%20samples%20received%20from%20different%0Asources.%20Our%20results%20show%20that%20email%20kits%20are%20being%20reused%20extensively%20and%0Avisually%20similar%20emails%20are%20sent%20to%20our%20customers%20at%20various%20time%20intervals.%0ATherefore%2C%20this%20method%20could%20be%20very%20helpful%20in%20situations%20where%20detection%0Aengines%20that%20rely%20on%20textual%20features%20and%20keywords%20are%20bypassed%2C%20an%20occurrence%0Aour%20observations%20show%20happens%20frequently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16945v3&entry.124074799=Read"},
{"title": "$\u03bc$GUIDE: a framework for quantitative imaging via generalized\n  uncertainty-driven inference using deep learning", "author": "Ma\u00ebliss Jallais and Marco Palombo", "abstract": "  This work proposes $\\mu$GUIDE: a general Bayesian framework to estimate\nposterior distributions of tissue microstructure parameters from any given\nbiophysical model or MRI signal representation, with exemplar demonstration in\ndiffusion-weighted MRI. Harnessing a new deep learning architecture for\nautomatic signal feature selection combined with simulation-based inference and\nefficient sampling of the posterior distributions, $\\mu$GUIDE bypasses the high\ncomputational and time cost of conventional Bayesian approaches and does not\nrely on acquisition constraints to define model-specific summary statistics.\nThe obtained posterior distributions allow to highlight degeneracies present in\nthe model definition and quantify the uncertainty and ambiguity of the\nestimated parameters.\n", "link": "http://arxiv.org/abs/2312.17293v4", "date": "2024-09-04", "relevancy": 1.602, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5835}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CE%BC%24GUIDE%3A%20a%20framework%20for%20quantitative%20imaging%20via%20generalized%0A%20%20uncertainty-driven%20inference%20using%20deep%20learning&body=Title%3A%20%24%CE%BC%24GUIDE%3A%20a%20framework%20for%20quantitative%20imaging%20via%20generalized%0A%20%20uncertainty-driven%20inference%20using%20deep%20learning%0AAuthor%3A%20Ma%C3%ABliss%20Jallais%20and%20Marco%20Palombo%0AAbstract%3A%20%20%20This%20work%20proposes%20%24%5Cmu%24GUIDE%3A%20a%20general%20Bayesian%20framework%20to%20estimate%0Aposterior%20distributions%20of%20tissue%20microstructure%20parameters%20from%20any%20given%0Abiophysical%20model%20or%20MRI%20signal%20representation%2C%20with%20exemplar%20demonstration%20in%0Adiffusion-weighted%20MRI.%20Harnessing%20a%20new%20deep%20learning%20architecture%20for%0Aautomatic%20signal%20feature%20selection%20combined%20with%20simulation-based%20inference%20and%0Aefficient%20sampling%20of%20the%20posterior%20distributions%2C%20%24%5Cmu%24GUIDE%20bypasses%20the%20high%0Acomputational%20and%20time%20cost%20of%20conventional%20Bayesian%20approaches%20and%20does%20not%0Arely%20on%20acquisition%20constraints%20to%20define%20model-specific%20summary%20statistics.%0AThe%20obtained%20posterior%20distributions%20allow%20to%20highlight%20degeneracies%20present%20in%0Athe%20model%20definition%20and%20quantify%20the%20uncertainty%20and%20ambiguity%20of%20the%0Aestimated%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17293v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CE%25BC%2524GUIDE%253A%2520a%2520framework%2520for%2520quantitative%2520imaging%2520via%2520generalized%250A%2520%2520uncertainty-driven%2520inference%2520using%2520deep%2520learning%26entry.906535625%3DMa%25C3%25ABliss%2520Jallais%2520and%2520Marco%2520Palombo%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520%2524%255Cmu%2524GUIDE%253A%2520a%2520general%2520Bayesian%2520framework%2520to%2520estimate%250Aposterior%2520distributions%2520of%2520tissue%2520microstructure%2520parameters%2520from%2520any%2520given%250Abiophysical%2520model%2520or%2520MRI%2520signal%2520representation%252C%2520with%2520exemplar%2520demonstration%2520in%250Adiffusion-weighted%2520MRI.%2520Harnessing%2520a%2520new%2520deep%2520learning%2520architecture%2520for%250Aautomatic%2520signal%2520feature%2520selection%2520combined%2520with%2520simulation-based%2520inference%2520and%250Aefficient%2520sampling%2520of%2520the%2520posterior%2520distributions%252C%2520%2524%255Cmu%2524GUIDE%2520bypasses%2520the%2520high%250Acomputational%2520and%2520time%2520cost%2520of%2520conventional%2520Bayesian%2520approaches%2520and%2520does%2520not%250Arely%2520on%2520acquisition%2520constraints%2520to%2520define%2520model-specific%2520summary%2520statistics.%250AThe%2520obtained%2520posterior%2520distributions%2520allow%2520to%2520highlight%2520degeneracies%2520present%2520in%250Athe%2520model%2520definition%2520and%2520quantify%2520the%2520uncertainty%2520and%2520ambiguity%2520of%2520the%250Aestimated%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17293v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CE%BC%24GUIDE%3A%20a%20framework%20for%20quantitative%20imaging%20via%20generalized%0A%20%20uncertainty-driven%20inference%20using%20deep%20learning&entry.906535625=Ma%C3%ABliss%20Jallais%20and%20Marco%20Palombo&entry.1292438233=%20%20This%20work%20proposes%20%24%5Cmu%24GUIDE%3A%20a%20general%20Bayesian%20framework%20to%20estimate%0Aposterior%20distributions%20of%20tissue%20microstructure%20parameters%20from%20any%20given%0Abiophysical%20model%20or%20MRI%20signal%20representation%2C%20with%20exemplar%20demonstration%20in%0Adiffusion-weighted%20MRI.%20Harnessing%20a%20new%20deep%20learning%20architecture%20for%0Aautomatic%20signal%20feature%20selection%20combined%20with%20simulation-based%20inference%20and%0Aefficient%20sampling%20of%20the%20posterior%20distributions%2C%20%24%5Cmu%24GUIDE%20bypasses%20the%20high%0Acomputational%20and%20time%20cost%20of%20conventional%20Bayesian%20approaches%20and%20does%20not%0Arely%20on%20acquisition%20constraints%20to%20define%20model-specific%20summary%20statistics.%0AThe%20obtained%20posterior%20distributions%20allow%20to%20highlight%20degeneracies%20present%20in%0Athe%20model%20definition%20and%20quantify%20the%20uncertainty%20and%20ambiguity%20of%20the%0Aestimated%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17293v4&entry.124074799=Read"},
{"title": "The future of cosmological likelihood-based inference: accelerated\n  high-dimensional parameter estimation and model comparison", "author": "Davide Piras and Alicja Polanska and Alessio Spurio Mancini and Matthew A. Price and Jason D. McEwen", "abstract": "  We advocate for a new paradigm of cosmological likelihood-based inference,\nleveraging recent developments in machine learning and its underlying\ntechnology, to accelerate Bayesian inference in high-dimensional settings.\nSpecifically, we combine (i) emulation, where a machine learning model is\ntrained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii)\ndifferentiable and probabilistic programming, e.g. JAX and NumPyro,\nrespectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling\ntechniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv)\ndecoupled and scalable Bayesian model selection techniques that compute the\nBayesian evidence purely from posterior samples, e.g. the learned harmonic mean\nimplemented in harmonic. This paradigm allows us to carry out a complete\nBayesian analysis, including both parameter estimation and model selection, in\na fraction of the time of traditional approaches. First, we demonstrate the\napplication of this paradigm on a simulated cosmic shear analysis for a Stage\nIV survey in 37- and 39-dimensional parameter spaces, comparing $\\Lambda$CDM\nand a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contours\nand evidence estimates that are in excellent agreement with those computed by\nthe traditional nested sampling approach while reducing the computational cost\nfrom 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint\nanalysis between three simulated next-generation surveys, each performing a\n3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.\nStandard nested sampling techniques are simply unlikely to be feasible in this\nhigh-dimensional setting, requiring a projected 12 years of compute time on 48\nCPU cores; on the other hand, the proposed approach only requires 8 days of\ncompute time on 24 GPUs. All packages used in our analyses are publicly\navailable.\n", "link": "http://arxiv.org/abs/2405.12965v2", "date": "2024-09-04", "relevancy": 1.4996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5533}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20future%20of%20cosmological%20likelihood-based%20inference%3A%20accelerated%0A%20%20high-dimensional%20parameter%20estimation%20and%20model%20comparison&body=Title%3A%20The%20future%20of%20cosmological%20likelihood-based%20inference%3A%20accelerated%0A%20%20high-dimensional%20parameter%20estimation%20and%20model%20comparison%0AAuthor%3A%20Davide%20Piras%20and%20Alicja%20Polanska%20and%20Alessio%20Spurio%20Mancini%20and%20Matthew%20A.%20Price%20and%20Jason%20D.%20McEwen%0AAbstract%3A%20%20%20We%20advocate%20for%20a%20new%20paradigm%20of%20cosmological%20likelihood-based%20inference%2C%0Aleveraging%20recent%20developments%20in%20machine%20learning%20and%20its%20underlying%0Atechnology%2C%20to%20accelerate%20Bayesian%20inference%20in%20high-dimensional%20settings.%0ASpecifically%2C%20we%20combine%20%28i%29%20emulation%2C%20where%20a%20machine%20learning%20model%20is%0Atrained%20to%20mimic%20cosmological%20observables%2C%20e.g.%20CosmoPower-JAX%3B%20%28ii%29%0Adifferentiable%20and%20probabilistic%20programming%2C%20e.g.%20JAX%20and%20NumPyro%2C%0Arespectively%3B%20%28iii%29%20scalable%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20sampling%0Atechniques%20that%20exploit%20gradients%2C%20e.g.%20Hamiltonian%20Monte%20Carlo%3B%20and%20%28iv%29%0Adecoupled%20and%20scalable%20Bayesian%20model%20selection%20techniques%20that%20compute%20the%0ABayesian%20evidence%20purely%20from%20posterior%20samples%2C%20e.g.%20the%20learned%20harmonic%20mean%0Aimplemented%20in%20harmonic.%20This%20paradigm%20allows%20us%20to%20carry%20out%20a%20complete%0ABayesian%20analysis%2C%20including%20both%20parameter%20estimation%20and%20model%20selection%2C%20in%0Aa%20fraction%20of%20the%20time%20of%20traditional%20approaches.%20First%2C%20we%20demonstrate%20the%0Aapplication%20of%20this%20paradigm%20on%20a%20simulated%20cosmic%20shear%20analysis%20for%20a%20Stage%0AIV%20survey%20in%2037-%20and%2039-dimensional%20parameter%20spaces%2C%20comparing%20%24%5CLambda%24CDM%0Aand%20a%20dynamical%20dark%20energy%20model%20%28%24w_0w_a%24CDM%29.%20We%20recover%20posterior%20contours%0Aand%20evidence%20estimates%20that%20are%20in%20excellent%20agreement%20with%20those%20computed%20by%0Athe%20traditional%20nested%20sampling%20approach%20while%20reducing%20the%20computational%20cost%0Afrom%208%20months%20on%2048%20CPU%20cores%20to%202%20days%20on%2012%20GPUs.%20Second%2C%20we%20consider%20a%20joint%0Aanalysis%20between%20three%20simulated%20next-generation%20surveys%2C%20each%20performing%20a%0A3x2pt%20analysis%2C%20resulting%20in%20157-%20and%20159-dimensional%20parameter%20spaces.%0AStandard%20nested%20sampling%20techniques%20are%20simply%20unlikely%20to%20be%20feasible%20in%20this%0Ahigh-dimensional%20setting%2C%20requiring%20a%20projected%2012%20years%20of%20compute%20time%20on%2048%0ACPU%20cores%3B%20on%20the%20other%20hand%2C%20the%20proposed%20approach%20only%20requires%208%20days%20of%0Acompute%20time%20on%2024%20GPUs.%20All%20packages%20used%20in%20our%20analyses%20are%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520future%2520of%2520cosmological%2520likelihood-based%2520inference%253A%2520accelerated%250A%2520%2520high-dimensional%2520parameter%2520estimation%2520and%2520model%2520comparison%26entry.906535625%3DDavide%2520Piras%2520and%2520Alicja%2520Polanska%2520and%2520Alessio%2520Spurio%2520Mancini%2520and%2520Matthew%2520A.%2520Price%2520and%2520Jason%2520D.%2520McEwen%26entry.1292438233%3D%2520%2520We%2520advocate%2520for%2520a%2520new%2520paradigm%2520of%2520cosmological%2520likelihood-based%2520inference%252C%250Aleveraging%2520recent%2520developments%2520in%2520machine%2520learning%2520and%2520its%2520underlying%250Atechnology%252C%2520to%2520accelerate%2520Bayesian%2520inference%2520in%2520high-dimensional%2520settings.%250ASpecifically%252C%2520we%2520combine%2520%2528i%2529%2520emulation%252C%2520where%2520a%2520machine%2520learning%2520model%2520is%250Atrained%2520to%2520mimic%2520cosmological%2520observables%252C%2520e.g.%2520CosmoPower-JAX%253B%2520%2528ii%2529%250Adifferentiable%2520and%2520probabilistic%2520programming%252C%2520e.g.%2520JAX%2520and%2520NumPyro%252C%250Arespectively%253B%2520%2528iii%2529%2520scalable%2520Markov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520sampling%250Atechniques%2520that%2520exploit%2520gradients%252C%2520e.g.%2520Hamiltonian%2520Monte%2520Carlo%253B%2520and%2520%2528iv%2529%250Adecoupled%2520and%2520scalable%2520Bayesian%2520model%2520selection%2520techniques%2520that%2520compute%2520the%250ABayesian%2520evidence%2520purely%2520from%2520posterior%2520samples%252C%2520e.g.%2520the%2520learned%2520harmonic%2520mean%250Aimplemented%2520in%2520harmonic.%2520This%2520paradigm%2520allows%2520us%2520to%2520carry%2520out%2520a%2520complete%250ABayesian%2520analysis%252C%2520including%2520both%2520parameter%2520estimation%2520and%2520model%2520selection%252C%2520in%250Aa%2520fraction%2520of%2520the%2520time%2520of%2520traditional%2520approaches.%2520First%252C%2520we%2520demonstrate%2520the%250Aapplication%2520of%2520this%2520paradigm%2520on%2520a%2520simulated%2520cosmic%2520shear%2520analysis%2520for%2520a%2520Stage%250AIV%2520survey%2520in%252037-%2520and%252039-dimensional%2520parameter%2520spaces%252C%2520comparing%2520%2524%255CLambda%2524CDM%250Aand%2520a%2520dynamical%2520dark%2520energy%2520model%2520%2528%2524w_0w_a%2524CDM%2529.%2520We%2520recover%2520posterior%2520contours%250Aand%2520evidence%2520estimates%2520that%2520are%2520in%2520excellent%2520agreement%2520with%2520those%2520computed%2520by%250Athe%2520traditional%2520nested%2520sampling%2520approach%2520while%2520reducing%2520the%2520computational%2520cost%250Afrom%25208%2520months%2520on%252048%2520CPU%2520cores%2520to%25202%2520days%2520on%252012%2520GPUs.%2520Second%252C%2520we%2520consider%2520a%2520joint%250Aanalysis%2520between%2520three%2520simulated%2520next-generation%2520surveys%252C%2520each%2520performing%2520a%250A3x2pt%2520analysis%252C%2520resulting%2520in%2520157-%2520and%2520159-dimensional%2520parameter%2520spaces.%250AStandard%2520nested%2520sampling%2520techniques%2520are%2520simply%2520unlikely%2520to%2520be%2520feasible%2520in%2520this%250Ahigh-dimensional%2520setting%252C%2520requiring%2520a%2520projected%252012%2520years%2520of%2520compute%2520time%2520on%252048%250ACPU%2520cores%253B%2520on%2520the%2520other%2520hand%252C%2520the%2520proposed%2520approach%2520only%2520requires%25208%2520days%2520of%250Acompute%2520time%2520on%252024%2520GPUs.%2520All%2520packages%2520used%2520in%2520our%2520analyses%2520are%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20future%20of%20cosmological%20likelihood-based%20inference%3A%20accelerated%0A%20%20high-dimensional%20parameter%20estimation%20and%20model%20comparison&entry.906535625=Davide%20Piras%20and%20Alicja%20Polanska%20and%20Alessio%20Spurio%20Mancini%20and%20Matthew%20A.%20Price%20and%20Jason%20D.%20McEwen&entry.1292438233=%20%20We%20advocate%20for%20a%20new%20paradigm%20of%20cosmological%20likelihood-based%20inference%2C%0Aleveraging%20recent%20developments%20in%20machine%20learning%20and%20its%20underlying%0Atechnology%2C%20to%20accelerate%20Bayesian%20inference%20in%20high-dimensional%20settings.%0ASpecifically%2C%20we%20combine%20%28i%29%20emulation%2C%20where%20a%20machine%20learning%20model%20is%0Atrained%20to%20mimic%20cosmological%20observables%2C%20e.g.%20CosmoPower-JAX%3B%20%28ii%29%0Adifferentiable%20and%20probabilistic%20programming%2C%20e.g.%20JAX%20and%20NumPyro%2C%0Arespectively%3B%20%28iii%29%20scalable%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20sampling%0Atechniques%20that%20exploit%20gradients%2C%20e.g.%20Hamiltonian%20Monte%20Carlo%3B%20and%20%28iv%29%0Adecoupled%20and%20scalable%20Bayesian%20model%20selection%20techniques%20that%20compute%20the%0ABayesian%20evidence%20purely%20from%20posterior%20samples%2C%20e.g.%20the%20learned%20harmonic%20mean%0Aimplemented%20in%20harmonic.%20This%20paradigm%20allows%20us%20to%20carry%20out%20a%20complete%0ABayesian%20analysis%2C%20including%20both%20parameter%20estimation%20and%20model%20selection%2C%20in%0Aa%20fraction%20of%20the%20time%20of%20traditional%20approaches.%20First%2C%20we%20demonstrate%20the%0Aapplication%20of%20this%20paradigm%20on%20a%20simulated%20cosmic%20shear%20analysis%20for%20a%20Stage%0AIV%20survey%20in%2037-%20and%2039-dimensional%20parameter%20spaces%2C%20comparing%20%24%5CLambda%24CDM%0Aand%20a%20dynamical%20dark%20energy%20model%20%28%24w_0w_a%24CDM%29.%20We%20recover%20posterior%20contours%0Aand%20evidence%20estimates%20that%20are%20in%20excellent%20agreement%20with%20those%20computed%20by%0Athe%20traditional%20nested%20sampling%20approach%20while%20reducing%20the%20computational%20cost%0Afrom%208%20months%20on%2048%20CPU%20cores%20to%202%20days%20on%2012%20GPUs.%20Second%2C%20we%20consider%20a%20joint%0Aanalysis%20between%20three%20simulated%20next-generation%20surveys%2C%20each%20performing%20a%0A3x2pt%20analysis%2C%20resulting%20in%20157-%20and%20159-dimensional%20parameter%20spaces.%0AStandard%20nested%20sampling%20techniques%20are%20simply%20unlikely%20to%20be%20feasible%20in%20this%0Ahigh-dimensional%20setting%2C%20requiring%20a%20projected%2012%20years%20of%20compute%20time%20on%2048%0ACPU%20cores%3B%20on%20the%20other%20hand%2C%20the%20proposed%20approach%20only%20requires%208%20days%20of%0Acompute%20time%20on%2024%20GPUs.%20All%20packages%20used%20in%20our%20analyses%20are%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12965v2&entry.124074799=Read"},
{"title": "An incremental preference elicitation-based approach to learning\n  potentially non-monotonic preferences in multi-criteria sorting", "author": "Zhuolin Li and Zhen Zhang and Witold Pedrycz", "abstract": "  This paper introduces a novel incremental preference elicitation-based\napproach to learning potentially non-monotonic preferences in multi-criteria\nsorting (MCS) problems, enabling decision makers to progressively provide\nassignment example preference information. Specifically, we first construct a\nmax-margin optimization-based model to model potentially non-monotonic\npreferences and inconsistent assignment example preference information in each\niteration of the incremental preference elicitation process. Using the optimal\nobjective function value of the max-margin optimization-based model, we devise\ninformation amount measurement methods and question selection strategies to\npinpoint the most informative alternative in each iteration within the\nframework of uncertainty sampling in active learning. Once the termination\ncriterion is satisfied, the sorting result for non-reference alternatives can\nbe determined through the use of two optimization models, i.e., the max-margin\noptimization-based model and the complexity controlling optimization model.\nSubsequently, two incremental preference elicitation-based algorithms are\ndeveloped to learn potentially non-monotonic preferences, considering different\ntermination criteria. Ultimately, we apply the proposed approach to a credit\nrating problem to elucidate the detailed implementation steps, and perform\ncomputational experiments on both artificial and real-world data sets to\ncompare the proposed question selection strategies with several benchmark\nstrategies.\n", "link": "http://arxiv.org/abs/2409.02760v1", "date": "2024-09-04", "relevancy": 1.3801, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4866}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4557}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20incremental%20preference%20elicitation-based%20approach%20to%20learning%0A%20%20potentially%20non-monotonic%20preferences%20in%20multi-criteria%20sorting&body=Title%3A%20An%20incremental%20preference%20elicitation-based%20approach%20to%20learning%0A%20%20potentially%20non-monotonic%20preferences%20in%20multi-criteria%20sorting%0AAuthor%3A%20Zhuolin%20Li%20and%20Zhen%20Zhang%20and%20Witold%20Pedrycz%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20incremental%20preference%20elicitation-based%0Aapproach%20to%20learning%20potentially%20non-monotonic%20preferences%20in%20multi-criteria%0Asorting%20%28MCS%29%20problems%2C%20enabling%20decision%20makers%20to%20progressively%20provide%0Aassignment%20example%20preference%20information.%20Specifically%2C%20we%20first%20construct%20a%0Amax-margin%20optimization-based%20model%20to%20model%20potentially%20non-monotonic%0Apreferences%20and%20inconsistent%20assignment%20example%20preference%20information%20in%20each%0Aiteration%20of%20the%20incremental%20preference%20elicitation%20process.%20Using%20the%20optimal%0Aobjective%20function%20value%20of%20the%20max-margin%20optimization-based%20model%2C%20we%20devise%0Ainformation%20amount%20measurement%20methods%20and%20question%20selection%20strategies%20to%0Apinpoint%20the%20most%20informative%20alternative%20in%20each%20iteration%20within%20the%0Aframework%20of%20uncertainty%20sampling%20in%20active%20learning.%20Once%20the%20termination%0Acriterion%20is%20satisfied%2C%20the%20sorting%20result%20for%20non-reference%20alternatives%20can%0Abe%20determined%20through%20the%20use%20of%20two%20optimization%20models%2C%20i.e.%2C%20the%20max-margin%0Aoptimization-based%20model%20and%20the%20complexity%20controlling%20optimization%20model.%0ASubsequently%2C%20two%20incremental%20preference%20elicitation-based%20algorithms%20are%0Adeveloped%20to%20learn%20potentially%20non-monotonic%20preferences%2C%20considering%20different%0Atermination%20criteria.%20Ultimately%2C%20we%20apply%20the%20proposed%20approach%20to%20a%20credit%0Arating%20problem%20to%20elucidate%20the%20detailed%20implementation%20steps%2C%20and%20perform%0Acomputational%20experiments%20on%20both%20artificial%20and%20real-world%20data%20sets%20to%0Acompare%20the%20proposed%20question%20selection%20strategies%20with%20several%20benchmark%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520incremental%2520preference%2520elicitation-based%2520approach%2520to%2520learning%250A%2520%2520potentially%2520non-monotonic%2520preferences%2520in%2520multi-criteria%2520sorting%26entry.906535625%3DZhuolin%2520Li%2520and%2520Zhen%2520Zhang%2520and%2520Witold%2520Pedrycz%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520incremental%2520preference%2520elicitation-based%250Aapproach%2520to%2520learning%2520potentially%2520non-monotonic%2520preferences%2520in%2520multi-criteria%250Asorting%2520%2528MCS%2529%2520problems%252C%2520enabling%2520decision%2520makers%2520to%2520progressively%2520provide%250Aassignment%2520example%2520preference%2520information.%2520Specifically%252C%2520we%2520first%2520construct%2520a%250Amax-margin%2520optimization-based%2520model%2520to%2520model%2520potentially%2520non-monotonic%250Apreferences%2520and%2520inconsistent%2520assignment%2520example%2520preference%2520information%2520in%2520each%250Aiteration%2520of%2520the%2520incremental%2520preference%2520elicitation%2520process.%2520Using%2520the%2520optimal%250Aobjective%2520function%2520value%2520of%2520the%2520max-margin%2520optimization-based%2520model%252C%2520we%2520devise%250Ainformation%2520amount%2520measurement%2520methods%2520and%2520question%2520selection%2520strategies%2520to%250Apinpoint%2520the%2520most%2520informative%2520alternative%2520in%2520each%2520iteration%2520within%2520the%250Aframework%2520of%2520uncertainty%2520sampling%2520in%2520active%2520learning.%2520Once%2520the%2520termination%250Acriterion%2520is%2520satisfied%252C%2520the%2520sorting%2520result%2520for%2520non-reference%2520alternatives%2520can%250Abe%2520determined%2520through%2520the%2520use%2520of%2520two%2520optimization%2520models%252C%2520i.e.%252C%2520the%2520max-margin%250Aoptimization-based%2520model%2520and%2520the%2520complexity%2520controlling%2520optimization%2520model.%250ASubsequently%252C%2520two%2520incremental%2520preference%2520elicitation-based%2520algorithms%2520are%250Adeveloped%2520to%2520learn%2520potentially%2520non-monotonic%2520preferences%252C%2520considering%2520different%250Atermination%2520criteria.%2520Ultimately%252C%2520we%2520apply%2520the%2520proposed%2520approach%2520to%2520a%2520credit%250Arating%2520problem%2520to%2520elucidate%2520the%2520detailed%2520implementation%2520steps%252C%2520and%2520perform%250Acomputational%2520experiments%2520on%2520both%2520artificial%2520and%2520real-world%2520data%2520sets%2520to%250Acompare%2520the%2520proposed%2520question%2520selection%2520strategies%2520with%2520several%2520benchmark%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20incremental%20preference%20elicitation-based%20approach%20to%20learning%0A%20%20potentially%20non-monotonic%20preferences%20in%20multi-criteria%20sorting&entry.906535625=Zhuolin%20Li%20and%20Zhen%20Zhang%20and%20Witold%20Pedrycz&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20incremental%20preference%20elicitation-based%0Aapproach%20to%20learning%20potentially%20non-monotonic%20preferences%20in%20multi-criteria%0Asorting%20%28MCS%29%20problems%2C%20enabling%20decision%20makers%20to%20progressively%20provide%0Aassignment%20example%20preference%20information.%20Specifically%2C%20we%20first%20construct%20a%0Amax-margin%20optimization-based%20model%20to%20model%20potentially%20non-monotonic%0Apreferences%20and%20inconsistent%20assignment%20example%20preference%20information%20in%20each%0Aiteration%20of%20the%20incremental%20preference%20elicitation%20process.%20Using%20the%20optimal%0Aobjective%20function%20value%20of%20the%20max-margin%20optimization-based%20model%2C%20we%20devise%0Ainformation%20amount%20measurement%20methods%20and%20question%20selection%20strategies%20to%0Apinpoint%20the%20most%20informative%20alternative%20in%20each%20iteration%20within%20the%0Aframework%20of%20uncertainty%20sampling%20in%20active%20learning.%20Once%20the%20termination%0Acriterion%20is%20satisfied%2C%20the%20sorting%20result%20for%20non-reference%20alternatives%20can%0Abe%20determined%20through%20the%20use%20of%20two%20optimization%20models%2C%20i.e.%2C%20the%20max-margin%0Aoptimization-based%20model%20and%20the%20complexity%20controlling%20optimization%20model.%0ASubsequently%2C%20two%20incremental%20preference%20elicitation-based%20algorithms%20are%0Adeveloped%20to%20learn%20potentially%20non-monotonic%20preferences%2C%20considering%20different%0Atermination%20criteria.%20Ultimately%2C%20we%20apply%20the%20proposed%20approach%20to%20a%20credit%0Arating%20problem%20to%20elucidate%20the%20detailed%20implementation%20steps%2C%20and%20perform%0Acomputational%20experiments%20on%20both%20artificial%20and%20real-world%20data%20sets%20to%0Acompare%20the%20proposed%20question%20selection%20strategies%20with%20several%20benchmark%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02760v1&entry.124074799=Read"},
{"title": "SDE-based Multiplicative Noise Removal", "author": "An Vuong and Thinh Nguyen", "abstract": "  Multiplicative noise, also known as speckle or pepper noise, commonly affects\nimages produced by synthetic aperture radar (SAR), lasers, or optical lenses.\nUnlike additive noise, which typically arises from thermal processes or\nexternal factors, multiplicative noise is inherent to the system, originating\nfrom the fluctuation in diffuse reflections. These fluctuations result in\nmultiple copies of the same signal with varying magnitudes being combined.\nConsequently, despeckling, or removing multiplicative noise, necessitates\ndifferent techniques compared to those used for additive noise removal.\n  In this paper, we propose a novel approach using Stochastic Differential\nEquations based diffusion models to address multiplicative noise. We\ndemonstrate that multiplicative noise can be effectively modeled as a Geometric\nBrownian Motion process in the logarithmic domain. Utilizing the Fokker-Planck\nequation, we derive the corresponding reverse process for image denoising. To\nvalidate our method, we conduct extensive experiments on two different\ndatasets, comparing our approach to both classical signal processing techniques\nand contemporary CNN-based noise removal models. Our results indicate that the\nproposed method significantly outperforms existing methods on perception-based\nmetrics such as FID and LPIPS, while maintaining competitive performance on\ntraditional metrics like PSNR and SSIM.\n", "link": "http://arxiv.org/abs/2408.10283v2", "date": "2024-09-04", "relevancy": 1.6374, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5582}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5531}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDE-based%20Multiplicative%20Noise%20Removal&body=Title%3A%20SDE-based%20Multiplicative%20Noise%20Removal%0AAuthor%3A%20An%20Vuong%20and%20Thinh%20Nguyen%0AAbstract%3A%20%20%20Multiplicative%20noise%2C%20also%20known%20as%20speckle%20or%20pepper%20noise%2C%20commonly%20affects%0Aimages%20produced%20by%20synthetic%20aperture%20radar%20%28SAR%29%2C%20lasers%2C%20or%20optical%20lenses.%0AUnlike%20additive%20noise%2C%20which%20typically%20arises%20from%20thermal%20processes%20or%0Aexternal%20factors%2C%20multiplicative%20noise%20is%20inherent%20to%20the%20system%2C%20originating%0Afrom%20the%20fluctuation%20in%20diffuse%20reflections.%20These%20fluctuations%20result%20in%0Amultiple%20copies%20of%20the%20same%20signal%20with%20varying%20magnitudes%20being%20combined.%0AConsequently%2C%20despeckling%2C%20or%20removing%20multiplicative%20noise%2C%20necessitates%0Adifferent%20techniques%20compared%20to%20those%20used%20for%20additive%20noise%20removal.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20using%20Stochastic%20Differential%0AEquations%20based%20diffusion%20models%20to%20address%20multiplicative%20noise.%20We%0Ademonstrate%20that%20multiplicative%20noise%20can%20be%20effectively%20modeled%20as%20a%20Geometric%0ABrownian%20Motion%20process%20in%20the%20logarithmic%20domain.%20Utilizing%20the%20Fokker-Planck%0Aequation%2C%20we%20derive%20the%20corresponding%20reverse%20process%20for%20image%20denoising.%20To%0Avalidate%20our%20method%2C%20we%20conduct%20extensive%20experiments%20on%20two%20different%0Adatasets%2C%20comparing%20our%20approach%20to%20both%20classical%20signal%20processing%20techniques%0Aand%20contemporary%20CNN-based%20noise%20removal%20models.%20Our%20results%20indicate%20that%20the%0Aproposed%20method%20significantly%20outperforms%20existing%20methods%20on%20perception-based%0Ametrics%20such%20as%20FID%20and%20LPIPS%2C%20while%20maintaining%20competitive%20performance%20on%0Atraditional%20metrics%20like%20PSNR%20and%20SSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDE-based%2520Multiplicative%2520Noise%2520Removal%26entry.906535625%3DAn%2520Vuong%2520and%2520Thinh%2520Nguyen%26entry.1292438233%3D%2520%2520Multiplicative%2520noise%252C%2520also%2520known%2520as%2520speckle%2520or%2520pepper%2520noise%252C%2520commonly%2520affects%250Aimages%2520produced%2520by%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%252C%2520lasers%252C%2520or%2520optical%2520lenses.%250AUnlike%2520additive%2520noise%252C%2520which%2520typically%2520arises%2520from%2520thermal%2520processes%2520or%250Aexternal%2520factors%252C%2520multiplicative%2520noise%2520is%2520inherent%2520to%2520the%2520system%252C%2520originating%250Afrom%2520the%2520fluctuation%2520in%2520diffuse%2520reflections.%2520These%2520fluctuations%2520result%2520in%250Amultiple%2520copies%2520of%2520the%2520same%2520signal%2520with%2520varying%2520magnitudes%2520being%2520combined.%250AConsequently%252C%2520despeckling%252C%2520or%2520removing%2520multiplicative%2520noise%252C%2520necessitates%250Adifferent%2520techniques%2520compared%2520to%2520those%2520used%2520for%2520additive%2520noise%2520removal.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520using%2520Stochastic%2520Differential%250AEquations%2520based%2520diffusion%2520models%2520to%2520address%2520multiplicative%2520noise.%2520We%250Ademonstrate%2520that%2520multiplicative%2520noise%2520can%2520be%2520effectively%2520modeled%2520as%2520a%2520Geometric%250ABrownian%2520Motion%2520process%2520in%2520the%2520logarithmic%2520domain.%2520Utilizing%2520the%2520Fokker-Planck%250Aequation%252C%2520we%2520derive%2520the%2520corresponding%2520reverse%2520process%2520for%2520image%2520denoising.%2520To%250Avalidate%2520our%2520method%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520two%2520different%250Adatasets%252C%2520comparing%2520our%2520approach%2520to%2520both%2520classical%2520signal%2520processing%2520techniques%250Aand%2520contemporary%2520CNN-based%2520noise%2520removal%2520models.%2520Our%2520results%2520indicate%2520that%2520the%250Aproposed%2520method%2520significantly%2520outperforms%2520existing%2520methods%2520on%2520perception-based%250Ametrics%2520such%2520as%2520FID%2520and%2520LPIPS%252C%2520while%2520maintaining%2520competitive%2520performance%2520on%250Atraditional%2520metrics%2520like%2520PSNR%2520and%2520SSIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDE-based%20Multiplicative%20Noise%20Removal&entry.906535625=An%20Vuong%20and%20Thinh%20Nguyen&entry.1292438233=%20%20Multiplicative%20noise%2C%20also%20known%20as%20speckle%20or%20pepper%20noise%2C%20commonly%20affects%0Aimages%20produced%20by%20synthetic%20aperture%20radar%20%28SAR%29%2C%20lasers%2C%20or%20optical%20lenses.%0AUnlike%20additive%20noise%2C%20which%20typically%20arises%20from%20thermal%20processes%20or%0Aexternal%20factors%2C%20multiplicative%20noise%20is%20inherent%20to%20the%20system%2C%20originating%0Afrom%20the%20fluctuation%20in%20diffuse%20reflections.%20These%20fluctuations%20result%20in%0Amultiple%20copies%20of%20the%20same%20signal%20with%20varying%20magnitudes%20being%20combined.%0AConsequently%2C%20despeckling%2C%20or%20removing%20multiplicative%20noise%2C%20necessitates%0Adifferent%20techniques%20compared%20to%20those%20used%20for%20additive%20noise%20removal.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20using%20Stochastic%20Differential%0AEquations%20based%20diffusion%20models%20to%20address%20multiplicative%20noise.%20We%0Ademonstrate%20that%20multiplicative%20noise%20can%20be%20effectively%20modeled%20as%20a%20Geometric%0ABrownian%20Motion%20process%20in%20the%20logarithmic%20domain.%20Utilizing%20the%20Fokker-Planck%0Aequation%2C%20we%20derive%20the%20corresponding%20reverse%20process%20for%20image%20denoising.%20To%0Avalidate%20our%20method%2C%20we%20conduct%20extensive%20experiments%20on%20two%20different%0Adatasets%2C%20comparing%20our%20approach%20to%20both%20classical%20signal%20processing%20techniques%0Aand%20contemporary%20CNN-based%20noise%20removal%20models.%20Our%20results%20indicate%20that%20the%0Aproposed%20method%20significantly%20outperforms%20existing%20methods%20on%20perception-based%0Ametrics%20such%20as%20FID%20and%20LPIPS%2C%20while%20maintaining%20competitive%20performance%20on%0Atraditional%20metrics%20like%20PSNR%20and%20SSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10283v2&entry.124074799=Read"},
{"title": "Convolutional L2LFlows: Generating Accurate Showers in Highly Granular\n  Calorimeters Using Convolutional Normalizing Flows", "author": "Thorsten Buss and Frank Gaede and Gregor Kasieczka and Claudius Krause and David Shih", "abstract": "  In the quest to build generative surrogate models as computationally\nefficient alternatives to rule-based simulations, the quality of the generated\nsamples remains a crucial frontier. So far, normalizing flows have been among\nthe models with the best fidelity. However, as the latent space in such models\nis required to have the same dimensionality as the data space, scaling up\nnormalizing flows to high dimensional datasets is not straightforward. The\nprior L2LFlows approach successfully used a series of separate normalizing\nflows and sequence of conditioning steps to circumvent this problem. In this\nwork, we extend L2LFlows to simulate showers with a 9-times larger profile in\nthe lateral direction. To achieve this, we introduce convolutional layers and\nU-Net-type connections, move from masked autoregressive flows to coupling\nlayers, and demonstrate the successful modelling of showers in the ILD\nElectromagnetic Calorimeter as well as Dataset 3 from the public CaloChallenge\ndataset.\n", "link": "http://arxiv.org/abs/2405.20407v3", "date": "2024-09-04", "relevancy": 1.497, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5749}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20L2LFlows%3A%20Generating%20Accurate%20Showers%20in%20Highly%20Granular%0A%20%20Calorimeters%20Using%20Convolutional%20Normalizing%20Flows&body=Title%3A%20Convolutional%20L2LFlows%3A%20Generating%20Accurate%20Showers%20in%20Highly%20Granular%0A%20%20Calorimeters%20Using%20Convolutional%20Normalizing%20Flows%0AAuthor%3A%20Thorsten%20Buss%20and%20Frank%20Gaede%20and%20Gregor%20Kasieczka%20and%20Claudius%20Krause%20and%20David%20Shih%0AAbstract%3A%20%20%20In%20the%20quest%20to%20build%20generative%20surrogate%20models%20as%20computationally%0Aefficient%20alternatives%20to%20rule-based%20simulations%2C%20the%20quality%20of%20the%20generated%0Asamples%20remains%20a%20crucial%20frontier.%20So%20far%2C%20normalizing%20flows%20have%20been%20among%0Athe%20models%20with%20the%20best%20fidelity.%20However%2C%20as%20the%20latent%20space%20in%20such%20models%0Ais%20required%20to%20have%20the%20same%20dimensionality%20as%20the%20data%20space%2C%20scaling%20up%0Anormalizing%20flows%20to%20high%20dimensional%20datasets%20is%20not%20straightforward.%20The%0Aprior%20L2LFlows%20approach%20successfully%20used%20a%20series%20of%20separate%20normalizing%0Aflows%20and%20sequence%20of%20conditioning%20steps%20to%20circumvent%20this%20problem.%20In%20this%0Awork%2C%20we%20extend%20L2LFlows%20to%20simulate%20showers%20with%20a%209-times%20larger%20profile%20in%0Athe%20lateral%20direction.%20To%20achieve%20this%2C%20we%20introduce%20convolutional%20layers%20and%0AU-Net-type%20connections%2C%20move%20from%20masked%20autoregressive%20flows%20to%20coupling%0Alayers%2C%20and%20demonstrate%20the%20successful%20modelling%20of%20showers%20in%20the%20ILD%0AElectromagnetic%20Calorimeter%20as%20well%20as%20Dataset%203%20from%20the%20public%20CaloChallenge%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20407v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520L2LFlows%253A%2520Generating%2520Accurate%2520Showers%2520in%2520Highly%2520Granular%250A%2520%2520Calorimeters%2520Using%2520Convolutional%2520Normalizing%2520Flows%26entry.906535625%3DThorsten%2520Buss%2520and%2520Frank%2520Gaede%2520and%2520Gregor%2520Kasieczka%2520and%2520Claudius%2520Krause%2520and%2520David%2520Shih%26entry.1292438233%3D%2520%2520In%2520the%2520quest%2520to%2520build%2520generative%2520surrogate%2520models%2520as%2520computationally%250Aefficient%2520alternatives%2520to%2520rule-based%2520simulations%252C%2520the%2520quality%2520of%2520the%2520generated%250Asamples%2520remains%2520a%2520crucial%2520frontier.%2520So%2520far%252C%2520normalizing%2520flows%2520have%2520been%2520among%250Athe%2520models%2520with%2520the%2520best%2520fidelity.%2520However%252C%2520as%2520the%2520latent%2520space%2520in%2520such%2520models%250Ais%2520required%2520to%2520have%2520the%2520same%2520dimensionality%2520as%2520the%2520data%2520space%252C%2520scaling%2520up%250Anormalizing%2520flows%2520to%2520high%2520dimensional%2520datasets%2520is%2520not%2520straightforward.%2520The%250Aprior%2520L2LFlows%2520approach%2520successfully%2520used%2520a%2520series%2520of%2520separate%2520normalizing%250Aflows%2520and%2520sequence%2520of%2520conditioning%2520steps%2520to%2520circumvent%2520this%2520problem.%2520In%2520this%250Awork%252C%2520we%2520extend%2520L2LFlows%2520to%2520simulate%2520showers%2520with%2520a%25209-times%2520larger%2520profile%2520in%250Athe%2520lateral%2520direction.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520convolutional%2520layers%2520and%250AU-Net-type%2520connections%252C%2520move%2520from%2520masked%2520autoregressive%2520flows%2520to%2520coupling%250Alayers%252C%2520and%2520demonstrate%2520the%2520successful%2520modelling%2520of%2520showers%2520in%2520the%2520ILD%250AElectromagnetic%2520Calorimeter%2520as%2520well%2520as%2520Dataset%25203%2520from%2520the%2520public%2520CaloChallenge%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20407v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20L2LFlows%3A%20Generating%20Accurate%20Showers%20in%20Highly%20Granular%0A%20%20Calorimeters%20Using%20Convolutional%20Normalizing%20Flows&entry.906535625=Thorsten%20Buss%20and%20Frank%20Gaede%20and%20Gregor%20Kasieczka%20and%20Claudius%20Krause%20and%20David%20Shih&entry.1292438233=%20%20In%20the%20quest%20to%20build%20generative%20surrogate%20models%20as%20computationally%0Aefficient%20alternatives%20to%20rule-based%20simulations%2C%20the%20quality%20of%20the%20generated%0Asamples%20remains%20a%20crucial%20frontier.%20So%20far%2C%20normalizing%20flows%20have%20been%20among%0Athe%20models%20with%20the%20best%20fidelity.%20However%2C%20as%20the%20latent%20space%20in%20such%20models%0Ais%20required%20to%20have%20the%20same%20dimensionality%20as%20the%20data%20space%2C%20scaling%20up%0Anormalizing%20flows%20to%20high%20dimensional%20datasets%20is%20not%20straightforward.%20The%0Aprior%20L2LFlows%20approach%20successfully%20used%20a%20series%20of%20separate%20normalizing%0Aflows%20and%20sequence%20of%20conditioning%20steps%20to%20circumvent%20this%20problem.%20In%20this%0Awork%2C%20we%20extend%20L2LFlows%20to%20simulate%20showers%20with%20a%209-times%20larger%20profile%20in%0Athe%20lateral%20direction.%20To%20achieve%20this%2C%20we%20introduce%20convolutional%20layers%20and%0AU-Net-type%20connections%2C%20move%20from%20masked%20autoregressive%20flows%20to%20coupling%0Alayers%2C%20and%20demonstrate%20the%20successful%20modelling%20of%20showers%20in%20the%20ILD%0AElectromagnetic%20Calorimeter%20as%20well%20as%20Dataset%203%20from%20the%20public%20CaloChallenge%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20407v3&entry.124074799=Read"},
{"title": "Neural timescales from a computational perspective", "author": "Roxana Zeraati and Anna Levina and Jakob H. Macke and Richard Gao", "abstract": "  Timescales of neural activity are diverse across and within brain areas, and\nexperimental observations suggest that neural timescales reflect information in\ndynamic environments. However, these observations do not specify how neural\ntimescales are shaped, nor whether particular timescales are necessary for\nneural computations and brain function. Here, we take a complementary\nperspective and synthesize three directions where computational methods can\ndistill the broad set of empirical observations into quantitative and testable\ntheories: We review (i) how data analysis methods allow us to capture different\ntimescales of neural dynamics across different recording modalities, (ii) how\ncomputational models provide a mechanistic explanation for the emergence of\ndiverse timescales, and (iii) how task-optimized models in machine learning\nuncover the functional relevance of neural timescales. This integrative\ncomputational approach, combined with empirical findings, would provide a more\nholistic understanding of how neural timescales capture the relationship\nbetween brain structure, dynamics, and behavior.\n", "link": "http://arxiv.org/abs/2409.02684v1", "date": "2024-09-04", "relevancy": 1.2708, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4246}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4241}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20timescales%20from%20a%20computational%20perspective&body=Title%3A%20Neural%20timescales%20from%20a%20computational%20perspective%0AAuthor%3A%20Roxana%20Zeraati%20and%20Anna%20Levina%20and%20Jakob%20H.%20Macke%20and%20Richard%20Gao%0AAbstract%3A%20%20%20Timescales%20of%20neural%20activity%20are%20diverse%20across%20and%20within%20brain%20areas%2C%20and%0Aexperimental%20observations%20suggest%20that%20neural%20timescales%20reflect%20information%20in%0Adynamic%20environments.%20However%2C%20these%20observations%20do%20not%20specify%20how%20neural%0Atimescales%20are%20shaped%2C%20nor%20whether%20particular%20timescales%20are%20necessary%20for%0Aneural%20computations%20and%20brain%20function.%20Here%2C%20we%20take%20a%20complementary%0Aperspective%20and%20synthesize%20three%20directions%20where%20computational%20methods%20can%0Adistill%20the%20broad%20set%20of%20empirical%20observations%20into%20quantitative%20and%20testable%0Atheories%3A%20We%20review%20%28i%29%20how%20data%20analysis%20methods%20allow%20us%20to%20capture%20different%0Atimescales%20of%20neural%20dynamics%20across%20different%20recording%20modalities%2C%20%28ii%29%20how%0Acomputational%20models%20provide%20a%20mechanistic%20explanation%20for%20the%20emergence%20of%0Adiverse%20timescales%2C%20and%20%28iii%29%20how%20task-optimized%20models%20in%20machine%20learning%0Auncover%20the%20functional%20relevance%20of%20neural%20timescales.%20This%20integrative%0Acomputational%20approach%2C%20combined%20with%20empirical%20findings%2C%20would%20provide%20a%20more%0Aholistic%20understanding%20of%20how%20neural%20timescales%20capture%20the%20relationship%0Abetween%20brain%20structure%2C%20dynamics%2C%20and%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520timescales%2520from%2520a%2520computational%2520perspective%26entry.906535625%3DRoxana%2520Zeraati%2520and%2520Anna%2520Levina%2520and%2520Jakob%2520H.%2520Macke%2520and%2520Richard%2520Gao%26entry.1292438233%3D%2520%2520Timescales%2520of%2520neural%2520activity%2520are%2520diverse%2520across%2520and%2520within%2520brain%2520areas%252C%2520and%250Aexperimental%2520observations%2520suggest%2520that%2520neural%2520timescales%2520reflect%2520information%2520in%250Adynamic%2520environments.%2520However%252C%2520these%2520observations%2520do%2520not%2520specify%2520how%2520neural%250Atimescales%2520are%2520shaped%252C%2520nor%2520whether%2520particular%2520timescales%2520are%2520necessary%2520for%250Aneural%2520computations%2520and%2520brain%2520function.%2520Here%252C%2520we%2520take%2520a%2520complementary%250Aperspective%2520and%2520synthesize%2520three%2520directions%2520where%2520computational%2520methods%2520can%250Adistill%2520the%2520broad%2520set%2520of%2520empirical%2520observations%2520into%2520quantitative%2520and%2520testable%250Atheories%253A%2520We%2520review%2520%2528i%2529%2520how%2520data%2520analysis%2520methods%2520allow%2520us%2520to%2520capture%2520different%250Atimescales%2520of%2520neural%2520dynamics%2520across%2520different%2520recording%2520modalities%252C%2520%2528ii%2529%2520how%250Acomputational%2520models%2520provide%2520a%2520mechanistic%2520explanation%2520for%2520the%2520emergence%2520of%250Adiverse%2520timescales%252C%2520and%2520%2528iii%2529%2520how%2520task-optimized%2520models%2520in%2520machine%2520learning%250Auncover%2520the%2520functional%2520relevance%2520of%2520neural%2520timescales.%2520This%2520integrative%250Acomputational%2520approach%252C%2520combined%2520with%2520empirical%2520findings%252C%2520would%2520provide%2520a%2520more%250Aholistic%2520understanding%2520of%2520how%2520neural%2520timescales%2520capture%2520the%2520relationship%250Abetween%2520brain%2520structure%252C%2520dynamics%252C%2520and%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20timescales%20from%20a%20computational%20perspective&entry.906535625=Roxana%20Zeraati%20and%20Anna%20Levina%20and%20Jakob%20H.%20Macke%20and%20Richard%20Gao&entry.1292438233=%20%20Timescales%20of%20neural%20activity%20are%20diverse%20across%20and%20within%20brain%20areas%2C%20and%0Aexperimental%20observations%20suggest%20that%20neural%20timescales%20reflect%20information%20in%0Adynamic%20environments.%20However%2C%20these%20observations%20do%20not%20specify%20how%20neural%0Atimescales%20are%20shaped%2C%20nor%20whether%20particular%20timescales%20are%20necessary%20for%0Aneural%20computations%20and%20brain%20function.%20Here%2C%20we%20take%20a%20complementary%0Aperspective%20and%20synthesize%20three%20directions%20where%20computational%20methods%20can%0Adistill%20the%20broad%20set%20of%20empirical%20observations%20into%20quantitative%20and%20testable%0Atheories%3A%20We%20review%20%28i%29%20how%20data%20analysis%20methods%20allow%20us%20to%20capture%20different%0Atimescales%20of%20neural%20dynamics%20across%20different%20recording%20modalities%2C%20%28ii%29%20how%0Acomputational%20models%20provide%20a%20mechanistic%20explanation%20for%20the%20emergence%20of%0Adiverse%20timescales%2C%20and%20%28iii%29%20how%20task-optimized%20models%20in%20machine%20learning%0Auncover%20the%20functional%20relevance%20of%20neural%20timescales.%20This%20integrative%0Acomputational%20approach%2C%20combined%20with%20empirical%20findings%2C%20would%20provide%20a%20more%0Aholistic%20understanding%20of%20how%20neural%20timescales%20capture%20the%20relationship%0Abetween%20brain%20structure%2C%20dynamics%2C%20and%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02684v1&entry.124074799=Read"},
{"title": "Hypothesizing Missing Causal Variables with LLMs", "author": "Ivaxi Sheth and Sahar Abdelnabi and Mario Fritz", "abstract": "  Scientific discovery is a catalyst for human intellectual advances, driven by\nthe cycle of hypothesis generation, experimental design, data evaluation, and\niterative assumption refinement. This process, while crucial, is expensive and\nheavily dependent on the domain knowledge of scientists to generate hypotheses\nand navigate the scientific cycle. Central to this is causality, the ability to\nestablish the relationship between the cause and the effect. Motivated by the\nscientific discovery process, in this work, we formulate a novel task where the\ninput is a partial causal graph with missing variables, and the output is a\nhypothesis about the missing variables to complete the partial graph. We design\na benchmark with varying difficulty levels and knowledge assumptions about the\ncausal graph. With the growing interest in using Large Language Models (LLMs)\nto assist in scientific discovery, we benchmark open-source and closed models\non our testbed. We show the strong ability of LLMs to hypothesize the mediation\nvariables between a cause and its effect. In contrast, they underperform in\nhypothesizing the cause and effect variables themselves. We also observe\nsurprising results where some of the open-source models outperform the closed\nGPT-4 model.\n", "link": "http://arxiv.org/abs/2409.02604v1", "date": "2024-09-04", "relevancy": 0.908, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4538}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypothesizing%20Missing%20Causal%20Variables%20with%20LLMs&body=Title%3A%20Hypothesizing%20Missing%20Causal%20Variables%20with%20LLMs%0AAuthor%3A%20Ivaxi%20Sheth%20and%20Sahar%20Abdelnabi%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Scientific%20discovery%20is%20a%20catalyst%20for%20human%20intellectual%20advances%2C%20driven%20by%0Athe%20cycle%20of%20hypothesis%20generation%2C%20experimental%20design%2C%20data%20evaluation%2C%20and%0Aiterative%20assumption%20refinement.%20This%20process%2C%20while%20crucial%2C%20is%20expensive%20and%0Aheavily%20dependent%20on%20the%20domain%20knowledge%20of%20scientists%20to%20generate%20hypotheses%0Aand%20navigate%20the%20scientific%20cycle.%20Central%20to%20this%20is%20causality%2C%20the%20ability%20to%0Aestablish%20the%20relationship%20between%20the%20cause%20and%20the%20effect.%20Motivated%20by%20the%0Ascientific%20discovery%20process%2C%20in%20this%20work%2C%20we%20formulate%20a%20novel%20task%20where%20the%0Ainput%20is%20a%20partial%20causal%20graph%20with%20missing%20variables%2C%20and%20the%20output%20is%20a%0Ahypothesis%20about%20the%20missing%20variables%20to%20complete%20the%20partial%20graph.%20We%20design%0Aa%20benchmark%20with%20varying%20difficulty%20levels%20and%20knowledge%20assumptions%20about%20the%0Acausal%20graph.%20With%20the%20growing%20interest%20in%20using%20Large%20Language%20Models%20%28LLMs%29%0Ato%20assist%20in%20scientific%20discovery%2C%20we%20benchmark%20open-source%20and%20closed%20models%0Aon%20our%20testbed.%20We%20show%20the%20strong%20ability%20of%20LLMs%20to%20hypothesize%20the%20mediation%0Avariables%20between%20a%20cause%20and%20its%20effect.%20In%20contrast%2C%20they%20underperform%20in%0Ahypothesizing%20the%20cause%20and%20effect%20variables%20themselves.%20We%20also%20observe%0Asurprising%20results%20where%20some%20of%20the%20open-source%20models%20outperform%20the%20closed%0AGPT-4%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypothesizing%2520Missing%2520Causal%2520Variables%2520with%2520LLMs%26entry.906535625%3DIvaxi%2520Sheth%2520and%2520Sahar%2520Abdelnabi%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Scientific%2520discovery%2520is%2520a%2520catalyst%2520for%2520human%2520intellectual%2520advances%252C%2520driven%2520by%250Athe%2520cycle%2520of%2520hypothesis%2520generation%252C%2520experimental%2520design%252C%2520data%2520evaluation%252C%2520and%250Aiterative%2520assumption%2520refinement.%2520This%2520process%252C%2520while%2520crucial%252C%2520is%2520expensive%2520and%250Aheavily%2520dependent%2520on%2520the%2520domain%2520knowledge%2520of%2520scientists%2520to%2520generate%2520hypotheses%250Aand%2520navigate%2520the%2520scientific%2520cycle.%2520Central%2520to%2520this%2520is%2520causality%252C%2520the%2520ability%2520to%250Aestablish%2520the%2520relationship%2520between%2520the%2520cause%2520and%2520the%2520effect.%2520Motivated%2520by%2520the%250Ascientific%2520discovery%2520process%252C%2520in%2520this%2520work%252C%2520we%2520formulate%2520a%2520novel%2520task%2520where%2520the%250Ainput%2520is%2520a%2520partial%2520causal%2520graph%2520with%2520missing%2520variables%252C%2520and%2520the%2520output%2520is%2520a%250Ahypothesis%2520about%2520the%2520missing%2520variables%2520to%2520complete%2520the%2520partial%2520graph.%2520We%2520design%250Aa%2520benchmark%2520with%2520varying%2520difficulty%2520levels%2520and%2520knowledge%2520assumptions%2520about%2520the%250Acausal%2520graph.%2520With%2520the%2520growing%2520interest%2520in%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ato%2520assist%2520in%2520scientific%2520discovery%252C%2520we%2520benchmark%2520open-source%2520and%2520closed%2520models%250Aon%2520our%2520testbed.%2520We%2520show%2520the%2520strong%2520ability%2520of%2520LLMs%2520to%2520hypothesize%2520the%2520mediation%250Avariables%2520between%2520a%2520cause%2520and%2520its%2520effect.%2520In%2520contrast%252C%2520they%2520underperform%2520in%250Ahypothesizing%2520the%2520cause%2520and%2520effect%2520variables%2520themselves.%2520We%2520also%2520observe%250Asurprising%2520results%2520where%2520some%2520of%2520the%2520open-source%2520models%2520outperform%2520the%2520closed%250AGPT-4%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypothesizing%20Missing%20Causal%20Variables%20with%20LLMs&entry.906535625=Ivaxi%20Sheth%20and%20Sahar%20Abdelnabi%20and%20Mario%20Fritz&entry.1292438233=%20%20Scientific%20discovery%20is%20a%20catalyst%20for%20human%20intellectual%20advances%2C%20driven%20by%0Athe%20cycle%20of%20hypothesis%20generation%2C%20experimental%20design%2C%20data%20evaluation%2C%20and%0Aiterative%20assumption%20refinement.%20This%20process%2C%20while%20crucial%2C%20is%20expensive%20and%0Aheavily%20dependent%20on%20the%20domain%20knowledge%20of%20scientists%20to%20generate%20hypotheses%0Aand%20navigate%20the%20scientific%20cycle.%20Central%20to%20this%20is%20causality%2C%20the%20ability%20to%0Aestablish%20the%20relationship%20between%20the%20cause%20and%20the%20effect.%20Motivated%20by%20the%0Ascientific%20discovery%20process%2C%20in%20this%20work%2C%20we%20formulate%20a%20novel%20task%20where%20the%0Ainput%20is%20a%20partial%20causal%20graph%20with%20missing%20variables%2C%20and%20the%20output%20is%20a%0Ahypothesis%20about%20the%20missing%20variables%20to%20complete%20the%20partial%20graph.%20We%20design%0Aa%20benchmark%20with%20varying%20difficulty%20levels%20and%20knowledge%20assumptions%20about%20the%0Acausal%20graph.%20With%20the%20growing%20interest%20in%20using%20Large%20Language%20Models%20%28LLMs%29%0Ato%20assist%20in%20scientific%20discovery%2C%20we%20benchmark%20open-source%20and%20closed%20models%0Aon%20our%20testbed.%20We%20show%20the%20strong%20ability%20of%20LLMs%20to%20hypothesize%20the%20mediation%0Avariables%20between%20a%20cause%20and%20its%20effect.%20In%20contrast%2C%20they%20underperform%20in%0Ahypothesizing%20the%20cause%20and%20effect%20variables%20themselves.%20We%20also%20observe%0Asurprising%20results%20where%20some%20of%20the%20open-source%20models%20outperform%20the%20closed%0AGPT-4%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02604v1&entry.124074799=Read"},
{"title": "Accelerating Model Predictive Control for Legged Robots through\n  Distributed Optimization", "author": "Lorenzo Amatucci and Giulio Turrisi and Angelo Bratta and Victor Barasuol and Claudio Semini", "abstract": "  This paper presents a novel approach to enhance Model Predictive Control\n(MPC) for legged robots through Distributed Optimization. Our method focuses on\ndecomposing the robot dynamics into smaller, parallelizable subsystems, and\nutilizing the Alternating Direction Method of Multipliers (ADMM) to ensure\nconsensus among them. Each subsystem is managed by its own Optimal Control\nProblem, with ADMM facilitating consistency between their optimizations. This\napproach not only decreases the computational time but also allows for\neffective scaling with more complex robot configurations, facilitating the\nintegration of additional subsystems such as articulated arms on a quadruped\nrobot. We demonstrate, through numerical evaluations, the convergence of our\napproach on two systems with increasing complexity. In addition, we showcase\nthat our approach converges towards the same solution when compared to a\nstate-of-the-art centralized whole-body MPC implementation. Moreover, we\nquantitatively compare the computational efficiency of our method to the\ncentralized approach, revealing up to a 75% reduction in computational time.\nOverall, our approach offers a promising avenue for accelerating MPC solutions\nfor legged robots, paving the way for more effective utilization of the\ncomputational performance of modern hardware.\n", "link": "http://arxiv.org/abs/2403.11742v4", "date": "2024-09-04", "relevancy": 1.5449, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5686}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.501}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Model%20Predictive%20Control%20for%20Legged%20Robots%20through%0A%20%20Distributed%20Optimization&body=Title%3A%20Accelerating%20Model%20Predictive%20Control%20for%20Legged%20Robots%20through%0A%20%20Distributed%20Optimization%0AAuthor%3A%20Lorenzo%20Amatucci%20and%20Giulio%20Turrisi%20and%20Angelo%20Bratta%20and%20Victor%20Barasuol%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20enhance%20Model%20Predictive%20Control%0A%28MPC%29%20for%20legged%20robots%20through%20Distributed%20Optimization.%20Our%20method%20focuses%20on%0Adecomposing%20the%20robot%20dynamics%20into%20smaller%2C%20parallelizable%20subsystems%2C%20and%0Autilizing%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20to%20ensure%0Aconsensus%20among%20them.%20Each%20subsystem%20is%20managed%20by%20its%20own%20Optimal%20Control%0AProblem%2C%20with%20ADMM%20facilitating%20consistency%20between%20their%20optimizations.%20This%0Aapproach%20not%20only%20decreases%20the%20computational%20time%20but%20also%20allows%20for%0Aeffective%20scaling%20with%20more%20complex%20robot%20configurations%2C%20facilitating%20the%0Aintegration%20of%20additional%20subsystems%20such%20as%20articulated%20arms%20on%20a%20quadruped%0Arobot.%20We%20demonstrate%2C%20through%20numerical%20evaluations%2C%20the%20convergence%20of%20our%0Aapproach%20on%20two%20systems%20with%20increasing%20complexity.%20In%20addition%2C%20we%20showcase%0Athat%20our%20approach%20converges%20towards%20the%20same%20solution%20when%20compared%20to%20a%0Astate-of-the-art%20centralized%20whole-body%20MPC%20implementation.%20Moreover%2C%20we%0Aquantitatively%20compare%20the%20computational%20efficiency%20of%20our%20method%20to%20the%0Acentralized%20approach%2C%20revealing%20up%20to%20a%2075%25%20reduction%20in%20computational%20time.%0AOverall%2C%20our%20approach%20offers%20a%20promising%20avenue%20for%20accelerating%20MPC%20solutions%0Afor%20legged%20robots%2C%20paving%20the%20way%20for%20more%20effective%20utilization%20of%20the%0Acomputational%20performance%20of%20modern%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11742v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Model%2520Predictive%2520Control%2520for%2520Legged%2520Robots%2520through%250A%2520%2520Distributed%2520Optimization%26entry.906535625%3DLorenzo%2520Amatucci%2520and%2520Giulio%2520Turrisi%2520and%2520Angelo%2520Bratta%2520and%2520Victor%2520Barasuol%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520enhance%2520Model%2520Predictive%2520Control%250A%2528MPC%2529%2520for%2520legged%2520robots%2520through%2520Distributed%2520Optimization.%2520Our%2520method%2520focuses%2520on%250Adecomposing%2520the%2520robot%2520dynamics%2520into%2520smaller%252C%2520parallelizable%2520subsystems%252C%2520and%250Autilizing%2520the%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528ADMM%2529%2520to%2520ensure%250Aconsensus%2520among%2520them.%2520Each%2520subsystem%2520is%2520managed%2520by%2520its%2520own%2520Optimal%2520Control%250AProblem%252C%2520with%2520ADMM%2520facilitating%2520consistency%2520between%2520their%2520optimizations.%2520This%250Aapproach%2520not%2520only%2520decreases%2520the%2520computational%2520time%2520but%2520also%2520allows%2520for%250Aeffective%2520scaling%2520with%2520more%2520complex%2520robot%2520configurations%252C%2520facilitating%2520the%250Aintegration%2520of%2520additional%2520subsystems%2520such%2520as%2520articulated%2520arms%2520on%2520a%2520quadruped%250Arobot.%2520We%2520demonstrate%252C%2520through%2520numerical%2520evaluations%252C%2520the%2520convergence%2520of%2520our%250Aapproach%2520on%2520two%2520systems%2520with%2520increasing%2520complexity.%2520In%2520addition%252C%2520we%2520showcase%250Athat%2520our%2520approach%2520converges%2520towards%2520the%2520same%2520solution%2520when%2520compared%2520to%2520a%250Astate-of-the-art%2520centralized%2520whole-body%2520MPC%2520implementation.%2520Moreover%252C%2520we%250Aquantitatively%2520compare%2520the%2520computational%2520efficiency%2520of%2520our%2520method%2520to%2520the%250Acentralized%2520approach%252C%2520revealing%2520up%2520to%2520a%252075%2525%2520reduction%2520in%2520computational%2520time.%250AOverall%252C%2520our%2520approach%2520offers%2520a%2520promising%2520avenue%2520for%2520accelerating%2520MPC%2520solutions%250Afor%2520legged%2520robots%252C%2520paving%2520the%2520way%2520for%2520more%2520effective%2520utilization%2520of%2520the%250Acomputational%2520performance%2520of%2520modern%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11742v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Model%20Predictive%20Control%20for%20Legged%20Robots%20through%0A%20%20Distributed%20Optimization&entry.906535625=Lorenzo%20Amatucci%20and%20Giulio%20Turrisi%20and%20Angelo%20Bratta%20and%20Victor%20Barasuol%20and%20Claudio%20Semini&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20enhance%20Model%20Predictive%20Control%0A%28MPC%29%20for%20legged%20robots%20through%20Distributed%20Optimization.%20Our%20method%20focuses%20on%0Adecomposing%20the%20robot%20dynamics%20into%20smaller%2C%20parallelizable%20subsystems%2C%20and%0Autilizing%20the%20Alternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20to%20ensure%0Aconsensus%20among%20them.%20Each%20subsystem%20is%20managed%20by%20its%20own%20Optimal%20Control%0AProblem%2C%20with%20ADMM%20facilitating%20consistency%20between%20their%20optimizations.%20This%0Aapproach%20not%20only%20decreases%20the%20computational%20time%20but%20also%20allows%20for%0Aeffective%20scaling%20with%20more%20complex%20robot%20configurations%2C%20facilitating%20the%0Aintegration%20of%20additional%20subsystems%20such%20as%20articulated%20arms%20on%20a%20quadruped%0Arobot.%20We%20demonstrate%2C%20through%20numerical%20evaluations%2C%20the%20convergence%20of%20our%0Aapproach%20on%20two%20systems%20with%20increasing%20complexity.%20In%20addition%2C%20we%20showcase%0Athat%20our%20approach%20converges%20towards%20the%20same%20solution%20when%20compared%20to%20a%0Astate-of-the-art%20centralized%20whole-body%20MPC%20implementation.%20Moreover%2C%20we%0Aquantitatively%20compare%20the%20computational%20efficiency%20of%20our%20method%20to%20the%0Acentralized%20approach%2C%20revealing%20up%20to%20a%2075%25%20reduction%20in%20computational%20time.%0AOverall%2C%20our%20approach%20offers%20a%20promising%20avenue%20for%20accelerating%20MPC%20solutions%0Afor%20legged%20robots%2C%20paving%20the%20way%20for%20more%20effective%20utilization%20of%20the%0Acomputational%20performance%20of%20modern%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11742v4&entry.124074799=Read"},
{"title": "Decision-Focused Learning: Foundations, State of the Art, Benchmark and\n  Future Opportunities", "author": "Jayanta Mandi and James Kotary and Senne Berden and Maxime Mulamba and Victor Bucarey and Tias Guns and Ferdinando Fioretto", "abstract": "  Decision-focused learning (DFL) is an emerging paradigm that integrates\nmachine learning (ML) and constrained optimization to enhance decision quality\nby training ML models in an end-to-end system. This approach shows significant\npotential to revolutionize combinatorial decision-making in real-world\napplications that operate under uncertainty, where estimating unknown\nparameters within decision models is a major challenge. This paper presents a\ncomprehensive review of DFL, providing an in-depth analysis of both\ngradient-based and gradient-free techniques used to combine ML and constrained\noptimization. It evaluates the strengths and limitations of these techniques\nand includes an extensive empirical evaluation of eleven methods across seven\nproblems. The survey also offers insights into recent advancements and future\nresearch directions in DFL.\n  Code and benchmark: https://github.com/PredOpt/predopt-benchmarks\n", "link": "http://arxiv.org/abs/2307.13565v4", "date": "2024-09-04", "relevancy": 1.5173, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision-Focused%20Learning%3A%20Foundations%2C%20State%20of%20the%20Art%2C%20Benchmark%20and%0A%20%20Future%20Opportunities&body=Title%3A%20Decision-Focused%20Learning%3A%20Foundations%2C%20State%20of%20the%20Art%2C%20Benchmark%20and%0A%20%20Future%20Opportunities%0AAuthor%3A%20Jayanta%20Mandi%20and%20James%20Kotary%20and%20Senne%20Berden%20and%20Maxime%20Mulamba%20and%20Victor%20Bucarey%20and%20Tias%20Guns%20and%20Ferdinando%20Fioretto%0AAbstract%3A%20%20%20Decision-focused%20learning%20%28DFL%29%20is%20an%20emerging%20paradigm%20that%20integrates%0Amachine%20learning%20%28ML%29%20and%20constrained%20optimization%20to%20enhance%20decision%20quality%0Aby%20training%20ML%20models%20in%20an%20end-to-end%20system.%20This%20approach%20shows%20significant%0Apotential%20to%20revolutionize%20combinatorial%20decision-making%20in%20real-world%0Aapplications%20that%20operate%20under%20uncertainty%2C%20where%20estimating%20unknown%0Aparameters%20within%20decision%20models%20is%20a%20major%20challenge.%20This%20paper%20presents%20a%0Acomprehensive%20review%20of%20DFL%2C%20providing%20an%20in-depth%20analysis%20of%20both%0Agradient-based%20and%20gradient-free%20techniques%20used%20to%20combine%20ML%20and%20constrained%0Aoptimization.%20It%20evaluates%20the%20strengths%20and%20limitations%20of%20these%20techniques%0Aand%20includes%20an%20extensive%20empirical%20evaluation%20of%20eleven%20methods%20across%20seven%0Aproblems.%20The%20survey%20also%20offers%20insights%20into%20recent%20advancements%20and%20future%0Aresearch%20directions%20in%20DFL.%0A%20%20Code%20and%20benchmark%3A%20https%3A//github.com/PredOpt/predopt-benchmarks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.13565v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision-Focused%2520Learning%253A%2520Foundations%252C%2520State%2520of%2520the%2520Art%252C%2520Benchmark%2520and%250A%2520%2520Future%2520Opportunities%26entry.906535625%3DJayanta%2520Mandi%2520and%2520James%2520Kotary%2520and%2520Senne%2520Berden%2520and%2520Maxime%2520Mulamba%2520and%2520Victor%2520Bucarey%2520and%2520Tias%2520Guns%2520and%2520Ferdinando%2520Fioretto%26entry.1292438233%3D%2520%2520Decision-focused%2520learning%2520%2528DFL%2529%2520is%2520an%2520emerging%2520paradigm%2520that%2520integrates%250Amachine%2520learning%2520%2528ML%2529%2520and%2520constrained%2520optimization%2520to%2520enhance%2520decision%2520quality%250Aby%2520training%2520ML%2520models%2520in%2520an%2520end-to-end%2520system.%2520This%2520approach%2520shows%2520significant%250Apotential%2520to%2520revolutionize%2520combinatorial%2520decision-making%2520in%2520real-world%250Aapplications%2520that%2520operate%2520under%2520uncertainty%252C%2520where%2520estimating%2520unknown%250Aparameters%2520within%2520decision%2520models%2520is%2520a%2520major%2520challenge.%2520This%2520paper%2520presents%2520a%250Acomprehensive%2520review%2520of%2520DFL%252C%2520providing%2520an%2520in-depth%2520analysis%2520of%2520both%250Agradient-based%2520and%2520gradient-free%2520techniques%2520used%2520to%2520combine%2520ML%2520and%2520constrained%250Aoptimization.%2520It%2520evaluates%2520the%2520strengths%2520and%2520limitations%2520of%2520these%2520techniques%250Aand%2520includes%2520an%2520extensive%2520empirical%2520evaluation%2520of%2520eleven%2520methods%2520across%2520seven%250Aproblems.%2520The%2520survey%2520also%2520offers%2520insights%2520into%2520recent%2520advancements%2520and%2520future%250Aresearch%2520directions%2520in%2520DFL.%250A%2520%2520Code%2520and%2520benchmark%253A%2520https%253A//github.com/PredOpt/predopt-benchmarks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.13565v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision-Focused%20Learning%3A%20Foundations%2C%20State%20of%20the%20Art%2C%20Benchmark%20and%0A%20%20Future%20Opportunities&entry.906535625=Jayanta%20Mandi%20and%20James%20Kotary%20and%20Senne%20Berden%20and%20Maxime%20Mulamba%20and%20Victor%20Bucarey%20and%20Tias%20Guns%20and%20Ferdinando%20Fioretto&entry.1292438233=%20%20Decision-focused%20learning%20%28DFL%29%20is%20an%20emerging%20paradigm%20that%20integrates%0Amachine%20learning%20%28ML%29%20and%20constrained%20optimization%20to%20enhance%20decision%20quality%0Aby%20training%20ML%20models%20in%20an%20end-to-end%20system.%20This%20approach%20shows%20significant%0Apotential%20to%20revolutionize%20combinatorial%20decision-making%20in%20real-world%0Aapplications%20that%20operate%20under%20uncertainty%2C%20where%20estimating%20unknown%0Aparameters%20within%20decision%20models%20is%20a%20major%20challenge.%20This%20paper%20presents%20a%0Acomprehensive%20review%20of%20DFL%2C%20providing%20an%20in-depth%20analysis%20of%20both%0Agradient-based%20and%20gradient-free%20techniques%20used%20to%20combine%20ML%20and%20constrained%0Aoptimization.%20It%20evaluates%20the%20strengths%20and%20limitations%20of%20these%20techniques%0Aand%20includes%20an%20extensive%20empirical%20evaluation%20of%20eleven%20methods%20across%20seven%0Aproblems.%20The%20survey%20also%20offers%20insights%20into%20recent%20advancements%20and%20future%0Aresearch%20directions%20in%20DFL.%0A%20%20Code%20and%20benchmark%3A%20https%3A//github.com/PredOpt/predopt-benchmarks%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.13565v4&entry.124074799=Read"},
{"title": "Comprehensive Review and Empirical Evaluation of Causal Discovery\n  Algorithms for Numerical Data", "author": "Wenjin Niu and Zijun Gao and Liyan Song and Lingbo Li", "abstract": "  Causal analysis has become an essential component in understanding the\nunderlying causes of phenomena across various fields. Despite its significance,\nexisting literature on causal discovery algorithms is fragmented, with\ninconsistent methodologies, i.e., there is no universal classification standard\nfor existing methods, and a lack of comprehensive evaluations, i.e., data\ncharacteristics are often ignored to be jointly analyzed when benchmarking\nalgorithms. This study addresses these gaps by conducting an exhaustive review\nand empirical evaluation for causal discovery methods on numerical data, aiming\nto provide a clearer and more structured understanding of the field. Our\nresearch begins with a comprehensive literature review spanning over two\ndecades, analyzing over 200 academic articles and identifying more than 40\nrepresentative algorithms. This extensive analysis leads to the development of\na structured taxonomy tailored to the complexities of causal discovery,\ncategorizing methods into six main types. To address the lack of comprehensive\nevaluations, our study conducts an extensive empirical assessment of 29 causal\ndiscovery algorithms on multiple synthetic and real-world datasets. We\ncategorize synthetic datasets based on size, linearity, and noise distribution,\nemploying five evaluation metrics, and summarize the top-3 algorithm\nrecommendations, providing guidelines for users in various data scenarios. Our\nresults highlight a significant impact of dataset characteristics on algorithm\nperformance. Moreover, a metadata extraction strategy with an accuracy\nexceeding 80% is developed to assist users in algorithm selection on unknown\ndatasets. Based on these insights, we offer professional and practical\nguidelines to help users choose the most suitable causal discovery methods for\ntheir specific dataset.\n", "link": "http://arxiv.org/abs/2407.13054v2", "date": "2024-09-04", "relevancy": 0.8047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4129}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3978}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Review%20and%20Empirical%20Evaluation%20of%20Causal%20Discovery%0A%20%20Algorithms%20for%20Numerical%20Data&body=Title%3A%20Comprehensive%20Review%20and%20Empirical%20Evaluation%20of%20Causal%20Discovery%0A%20%20Algorithms%20for%20Numerical%20Data%0AAuthor%3A%20Wenjin%20Niu%20and%20Zijun%20Gao%20and%20Liyan%20Song%20and%20Lingbo%20Li%0AAbstract%3A%20%20%20Causal%20analysis%20has%20become%20an%20essential%20component%20in%20understanding%20the%0Aunderlying%20causes%20of%20phenomena%20across%20various%20fields.%20Despite%20its%20significance%2C%0Aexisting%20literature%20on%20causal%20discovery%20algorithms%20is%20fragmented%2C%20with%0Ainconsistent%20methodologies%2C%20i.e.%2C%20there%20is%20no%20universal%20classification%20standard%0Afor%20existing%20methods%2C%20and%20a%20lack%20of%20comprehensive%20evaluations%2C%20i.e.%2C%20data%0Acharacteristics%20are%20often%20ignored%20to%20be%20jointly%20analyzed%20when%20benchmarking%0Aalgorithms.%20This%20study%20addresses%20these%20gaps%20by%20conducting%20an%20exhaustive%20review%0Aand%20empirical%20evaluation%20for%20causal%20discovery%20methods%20on%20numerical%20data%2C%20aiming%0Ato%20provide%20a%20clearer%20and%20more%20structured%20understanding%20of%20the%20field.%20Our%0Aresearch%20begins%20with%20a%20comprehensive%20literature%20review%20spanning%20over%20two%0Adecades%2C%20analyzing%20over%20200%20academic%20articles%20and%20identifying%20more%20than%2040%0Arepresentative%20algorithms.%20This%20extensive%20analysis%20leads%20to%20the%20development%20of%0Aa%20structured%20taxonomy%20tailored%20to%20the%20complexities%20of%20causal%20discovery%2C%0Acategorizing%20methods%20into%20six%20main%20types.%20To%20address%20the%20lack%20of%20comprehensive%0Aevaluations%2C%20our%20study%20conducts%20an%20extensive%20empirical%20assessment%20of%2029%20causal%0Adiscovery%20algorithms%20on%20multiple%20synthetic%20and%20real-world%20datasets.%20We%0Acategorize%20synthetic%20datasets%20based%20on%20size%2C%20linearity%2C%20and%20noise%20distribution%2C%0Aemploying%20five%20evaluation%20metrics%2C%20and%20summarize%20the%20top-3%20algorithm%0Arecommendations%2C%20providing%20guidelines%20for%20users%20in%20various%20data%20scenarios.%20Our%0Aresults%20highlight%20a%20significant%20impact%20of%20dataset%20characteristics%20on%20algorithm%0Aperformance.%20Moreover%2C%20a%20metadata%20extraction%20strategy%20with%20an%20accuracy%0Aexceeding%2080%25%20is%20developed%20to%20assist%20users%20in%20algorithm%20selection%20on%20unknown%0Adatasets.%20Based%20on%20these%20insights%2C%20we%20offer%20professional%20and%20practical%0Aguidelines%20to%20help%20users%20choose%20the%20most%20suitable%20causal%20discovery%20methods%20for%0Atheir%20specific%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Review%2520and%2520Empirical%2520Evaluation%2520of%2520Causal%2520Discovery%250A%2520%2520Algorithms%2520for%2520Numerical%2520Data%26entry.906535625%3DWenjin%2520Niu%2520and%2520Zijun%2520Gao%2520and%2520Liyan%2520Song%2520and%2520Lingbo%2520Li%26entry.1292438233%3D%2520%2520Causal%2520analysis%2520has%2520become%2520an%2520essential%2520component%2520in%2520understanding%2520the%250Aunderlying%2520causes%2520of%2520phenomena%2520across%2520various%2520fields.%2520Despite%2520its%2520significance%252C%250Aexisting%2520literature%2520on%2520causal%2520discovery%2520algorithms%2520is%2520fragmented%252C%2520with%250Ainconsistent%2520methodologies%252C%2520i.e.%252C%2520there%2520is%2520no%2520universal%2520classification%2520standard%250Afor%2520existing%2520methods%252C%2520and%2520a%2520lack%2520of%2520comprehensive%2520evaluations%252C%2520i.e.%252C%2520data%250Acharacteristics%2520are%2520often%2520ignored%2520to%2520be%2520jointly%2520analyzed%2520when%2520benchmarking%250Aalgorithms.%2520This%2520study%2520addresses%2520these%2520gaps%2520by%2520conducting%2520an%2520exhaustive%2520review%250Aand%2520empirical%2520evaluation%2520for%2520causal%2520discovery%2520methods%2520on%2520numerical%2520data%252C%2520aiming%250Ato%2520provide%2520a%2520clearer%2520and%2520more%2520structured%2520understanding%2520of%2520the%2520field.%2520Our%250Aresearch%2520begins%2520with%2520a%2520comprehensive%2520literature%2520review%2520spanning%2520over%2520two%250Adecades%252C%2520analyzing%2520over%2520200%2520academic%2520articles%2520and%2520identifying%2520more%2520than%252040%250Arepresentative%2520algorithms.%2520This%2520extensive%2520analysis%2520leads%2520to%2520the%2520development%2520of%250Aa%2520structured%2520taxonomy%2520tailored%2520to%2520the%2520complexities%2520of%2520causal%2520discovery%252C%250Acategorizing%2520methods%2520into%2520six%2520main%2520types.%2520To%2520address%2520the%2520lack%2520of%2520comprehensive%250Aevaluations%252C%2520our%2520study%2520conducts%2520an%2520extensive%2520empirical%2520assessment%2520of%252029%2520causal%250Adiscovery%2520algorithms%2520on%2520multiple%2520synthetic%2520and%2520real-world%2520datasets.%2520We%250Acategorize%2520synthetic%2520datasets%2520based%2520on%2520size%252C%2520linearity%252C%2520and%2520noise%2520distribution%252C%250Aemploying%2520five%2520evaluation%2520metrics%252C%2520and%2520summarize%2520the%2520top-3%2520algorithm%250Arecommendations%252C%2520providing%2520guidelines%2520for%2520users%2520in%2520various%2520data%2520scenarios.%2520Our%250Aresults%2520highlight%2520a%2520significant%2520impact%2520of%2520dataset%2520characteristics%2520on%2520algorithm%250Aperformance.%2520Moreover%252C%2520a%2520metadata%2520extraction%2520strategy%2520with%2520an%2520accuracy%250Aexceeding%252080%2525%2520is%2520developed%2520to%2520assist%2520users%2520in%2520algorithm%2520selection%2520on%2520unknown%250Adatasets.%2520Based%2520on%2520these%2520insights%252C%2520we%2520offer%2520professional%2520and%2520practical%250Aguidelines%2520to%2520help%2520users%2520choose%2520the%2520most%2520suitable%2520causal%2520discovery%2520methods%2520for%250Atheir%2520specific%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Review%20and%20Empirical%20Evaluation%20of%20Causal%20Discovery%0A%20%20Algorithms%20for%20Numerical%20Data&entry.906535625=Wenjin%20Niu%20and%20Zijun%20Gao%20and%20Liyan%20Song%20and%20Lingbo%20Li&entry.1292438233=%20%20Causal%20analysis%20has%20become%20an%20essential%20component%20in%20understanding%20the%0Aunderlying%20causes%20of%20phenomena%20across%20various%20fields.%20Despite%20its%20significance%2C%0Aexisting%20literature%20on%20causal%20discovery%20algorithms%20is%20fragmented%2C%20with%0Ainconsistent%20methodologies%2C%20i.e.%2C%20there%20is%20no%20universal%20classification%20standard%0Afor%20existing%20methods%2C%20and%20a%20lack%20of%20comprehensive%20evaluations%2C%20i.e.%2C%20data%0Acharacteristics%20are%20often%20ignored%20to%20be%20jointly%20analyzed%20when%20benchmarking%0Aalgorithms.%20This%20study%20addresses%20these%20gaps%20by%20conducting%20an%20exhaustive%20review%0Aand%20empirical%20evaluation%20for%20causal%20discovery%20methods%20on%20numerical%20data%2C%20aiming%0Ato%20provide%20a%20clearer%20and%20more%20structured%20understanding%20of%20the%20field.%20Our%0Aresearch%20begins%20with%20a%20comprehensive%20literature%20review%20spanning%20over%20two%0Adecades%2C%20analyzing%20over%20200%20academic%20articles%20and%20identifying%20more%20than%2040%0Arepresentative%20algorithms.%20This%20extensive%20analysis%20leads%20to%20the%20development%20of%0Aa%20structured%20taxonomy%20tailored%20to%20the%20complexities%20of%20causal%20discovery%2C%0Acategorizing%20methods%20into%20six%20main%20types.%20To%20address%20the%20lack%20of%20comprehensive%0Aevaluations%2C%20our%20study%20conducts%20an%20extensive%20empirical%20assessment%20of%2029%20causal%0Adiscovery%20algorithms%20on%20multiple%20synthetic%20and%20real-world%20datasets.%20We%0Acategorize%20synthetic%20datasets%20based%20on%20size%2C%20linearity%2C%20and%20noise%20distribution%2C%0Aemploying%20five%20evaluation%20metrics%2C%20and%20summarize%20the%20top-3%20algorithm%0Arecommendations%2C%20providing%20guidelines%20for%20users%20in%20various%20data%20scenarios.%20Our%0Aresults%20highlight%20a%20significant%20impact%20of%20dataset%20characteristics%20on%20algorithm%0Aperformance.%20Moreover%2C%20a%20metadata%20extraction%20strategy%20with%20an%20accuracy%0Aexceeding%2080%25%20is%20developed%20to%20assist%20users%20in%20algorithm%20selection%20on%20unknown%0Adatasets.%20Based%20on%20these%20insights%2C%20we%20offer%20professional%20and%20practical%0Aguidelines%20to%20help%20users%20choose%20the%20most%20suitable%20causal%20discovery%20methods%20for%0Atheir%20specific%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13054v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


